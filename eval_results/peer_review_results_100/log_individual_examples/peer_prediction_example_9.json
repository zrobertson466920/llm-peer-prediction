{
  "example_idx": 9,
  "reference": "Published as a conference paper at ICLR 2023\n\nREVERSIBLE COLUMN NETWORKS\n\nYuxuan Cai1\n\nYizhuang Zhou1 Qi Han1\n\nJianjian Sun1 Xiangwen Kong1\n\nJun Li1\n\nXiangyu Zhang12 ∗\n\nMEGVII Technology1 Beijing Academy of Artificial Intelligence2 {caiyuxuan, zhouyizhuang, hanqi, zhangxiangyu}@megvii.com\n\nABSTRACT\n\nWe propose a new neural network design paradigm Reversible Column Network (RevCol). The main body of RevCol is composed of multiple copies of subnetworks, named columns respectively, between which multi-level reversible connections are employed. Such architectural scheme attributes RevCol very different behavior from conventional networks: during forward propagation, features in RevCol are learned to be gradually disentangled when passing through each column, whose total information is maintained rather than compressed or discarded as other network does. Our experiments suggest that CNN-style RevCol models can achieve very competitive performances on multiple computer vision tasks such as image classification, object detection and semantic segmentation, especially with large parameter budget and large dataset. For example, after ImageNet-22K pre-training, RevColXL obtains 88.2% ImageNet-1K accuracy. Given more pre-training data, our largest model RevCol-H reaches 90.0% on ImageNet-1K, 63.8% APbox on COCO detection minival set, 61.0% mIoU on ADE20k segmentation. To our knowledge, it is the best COCO detection and ADE20k segmentation result among pure (static) CNN models. Moreover, as a general macro architecture fashion, RevCol can also be introduced into transformers or other neural networks, which is demonstrated to improve the performances in both computer vision and NLP tasks. We release code and models at https://github.com/megvii-research/RevCol\n\n1\n\nINTRODUCTION\n\nInformation Bottleneck principle (IB) (Tishby et al., 2000; Tishby & Zaslavsky, 2015) rules the deep learning world. Consider a typical supervised learning network as in Fig. 1 (a): layers close to the input contain more low-level information, while features close to the output are rich in semantic meanings. In other words, information unrelated to the target is gradually compressed during the layer-by-layer propagation. Although such learning paradigm achieves great success in many practical applications, it might not be the optimal choice in the view of feature learning – down-stream tasks may suffer from inferior performances if the learned features are over compressed, or the learned semantic information is irrelevant to the target tasks, especially if a significant domain gap exists between the source and the target tasks (Zamir et al., 2018). Researchers have devoted great efforts to make the learned features to be more universally applicable, e.g. via self-supervised pre-training(Oord et al., 2018; Devlin et al., 2018; He et al., 2022; Xie et al., 2022) or multi-task learning (Ruder, 2017; Caruana, 1997; Sener & Koltun, 2018).\n\nIn this paper, we mainly focus on an alternative approach: building a network to learn disentangled representations. Unlike IB learning, disentangled feature learning (Desjardins et al., 2012; Bengio et al., 2013; Hinton, 2021) does not intend to extract the most related information while discard the less related; instead, it aims to embed the task-relevant concepts or semantic words into a few decoupled dimensions respectively. Meanwhile the whole feature vector roughly maintains as much information as the input. It is quite analogous to the mechanism in biological cells (Hinton, 2021; Lillicrap et al., 2020) – each cell shares an identical copy of the whole genome but has different\n\n∗Corresponding author. This work is supported by The National Key Research and Development Program of\n\nChina (No. 2017YFA0700800) and Beijing Academy of Artificial Intelligence (BAAI).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Sketch of the information propagation in: (a) Vanilla single-column network. (b) Our reversible column network. Yellow color denotes low-level information and blue color denotes semantic information.\n\nexpression intensities. Accordingly in computer vision tasks, learning disentangled features is also reasonable: for instance, high-level semantic representations are tuned during ImageNet pre-training, meanwhile the low-level information (e.g. locations of the edges) should also be maintained in other feature dimensions in case of the demand of down-stream tasks like object detection.\n\nFig. 1 (b) sketches our main idea: Reversible Column Networks (RevCol), which is greatly inspired by the big picture of GLOM (Hinton, 2021). Our network is composed of N subnetworks (named columns) of identical structure (however whose weights are not necessarily the same), each of which receives a copy of the input and generates a prediction. Hence multi-level embeddings, i.e. from low-level to highly semantic representations, are stored in each column. Moreover, reversible transformations are introduced to propagate the multi-level features from i-th column to (i + 1)-th column without information loss. During the propagation, since the complexity and nonlinearity increases, the quality of all feature levels is expected to gradually improve. Hence the last column (Col N in Fig. 1 (b)) predicts the final disentangled representations of the input.\n\nIn RevCol, one of our key contributions is the design of the reversible transformations between adjacent columns. The concept is borrowed from the family of Reversible Networks (Chang et al., 2018; Gomez et al., 2017; Jacobsen et al., 2018; Mangalam et al., 2022); however, conventional reversible structures such as RevNets (Gomez et al., 2017) (Fig. 2 (a)) usually have two drawbacks: first, feature maps within a reversible block are restricted to have the same shape*; second, the last two feature maps in RevNets have to contain both low-level and high-level information due to the reversible nature, which may be difficult to optimize as in conflict with IB principle. In this paper, we overcome the drawbacks by introducing a novel reversible multi-level fusion module. The details are discussed in Sec. 2.\n\nWe build a series of CNN-based RevCol models under different complexity budgets and evaluate them in mainstream computer vision tasks, such as ImageNet classification, COCO object detection and instance segmentation, as well as ADE20K semantic segmentation. Our models achieve comparable or better results than sophisticated CNNs or vision transformers like ConvNeXt (Liu et al., 2022b) and Swin (Liu et al., 2021). For example, after ImageNet-22K pre-training, our RevCol-XL model obtains 88.2% accuracy on ImageNet-1K without using transformers or large convolutional kernels (Ding et al., 2022b; Liu et al., 2022b; Han et al., 2021). More importantly, we find RevCol can scale up well to large models and large datasets. Given a larger private pre-training dataset, our biggest model RevCol-H obtains 90.0% accuracy on ImageNet-1K classification, 63.8% APbox on COCO detection minival set, and 61.0% mIoU on ADE20K segmentation, respectively. To our knowledge, it is the best reversible model on those tasks, as well as the best pure CNN model on COCO and ADE20K which only involves static kernels without dynamic convolutions (Dai et al., 2017; Ma et al., 2020). In the appendix, we further demonstrate RevCol can work with transformers (Dosovitskiy et al., 2020; Devlin et al., 2018) and get improved results on both computer vision and NLP tasks. Finally, similar to RevNets (Gomez et al., 2017), RevCol also shares the bonus of memory saving from the reversible nature, which is particularly important for large model training.\n\n*In precise, feature maps of odd and even indexes should be equal sized respectively.\n\n2\n\nInputInputOutputOutput(b)(a)............Col 1Col 2Col 3Col 4Col NLayer 1Layer 2Layer 3Layer N...Non-ReversibleConnectionReversibleConnectionLow-LevelSemanticPublished as a conference paper at ICLR 2023\n\nRelation to previous works. Although our initial idea on feature disentangling is derived from GLOM (Hinton, 2021), in RevCol there are a lot of simplifications and modifications. For example, GLOM suggests contrastive auxiliary loss to avoid feature collapse. Contrastive training methods need extra pairs of positive and negative samples, which is complicated and unstable. In RevCol, reversible transformations between columns provides lossless information propagation by nature. As for other multi-scale grid-like architectures such as HRNets (Wang et al., 2020), DEQ models (Bai et al., 2020) and FPNs (Lin et al., 2017; Tan et al., 2020), the design purpose of those models is to fuse multi-scale features rather than learn disentangled representations; therefore, in general they still follow the paradigm in Fig. 1 (a) – neither multiple entrances/exits nor reversible structures are employed. Based on those grid-like network topology, NAS based works (Ding et al., 2021; Wu et al., 2021; Liu et al., 2019; Ghiasi et al., 2019) search the optimized topology of network architectures for specific dataset. However, the RevCol architecture is not limit to specific tasks or datasets. With the reversible nature, our method maintains lossless information propagation and benefits for not only pre-training but also other down-stream tasks. Very recently, RevBiFPN (Chiley et al., 2022) comes up with an reversible variant of FPN, which is further employed in an HRNet-like architecture. Though our RevCol shares the similar idea of multi-scale reversible transformations with RevBiFPN, our work is done independently, which is derived from a different motivation of feature disentangling, and has much simpler architectures (e.g. free of reversible upsampling tower) and higher performances. We compare some of those models in Sec. 3.\n\n2 METHOD\n\nIn this section, we introduce the design details of our Reversible Column Networks (RevCol). Fig. 1 (b) illustrates the top-level architecture. Notice that for each column in RevCol, for simplicity we directly reuse existing structures such as ConvNeXt (Liu et al., 2022b), hence in the following subsections, we mainly focus on how to build the reversible connections between columns. In addition, we introduce an plug-and-play intermediate supervision on top of each column, which further improves the training convergence and feature quality.\n\n2.1 MULTI-LEVEL REVERSIBLE UNIT\n\nIn our network, reversible transformations plays a key role in feature disentangling without information loss, whose insight comes from Reversible Neural Networks (Dinh et al., 2014; Chang et al., 2018; Gomez et al., 2017; Jacobsen et al., 2018; Mangalam et al., 2022). Among them, we first take a review of one representative work RevNet (Gomez et al., 2017). As shown in Fig. 2 (a), RevNet first partitions the input x into two groups, x0 and x1. Then for later blocks, for example, block t, it takes two anterior blocks’ outputs xt−1 and xt−2 as input and generates the output xt. The mapping of block t is reversible, i.e. xt−2 can be reconstructed by two posterior blocks xt−1 and xt. Formally, the forward and inverse computation follow the equations †:\n\nF orward : xt = Ft(xt−1) + γxt−2 Inverse : xt−2 = γ−1[xt − Ft(xt−1)],\n\n(1)\n\nwhere Ft denotes an arbitrary non-linear operation analogous to those residual functions in standard ResNets; γ is a simple reversible operation (e.g. channel-wise scaling), whose inverse is denoted by γ−1. As mentioned in the introduction, the above formulation involves too strong constraint on the feature dimensions, i.e. xt, xt+2, xt+4, ... have to be equal sized, which is not flexible in architecture design. That is why RevNets (Gomez et al., 2017) introduce some non-reversible down-sampling blocks between reversible units, hence the whole network is not fully reversible. More importantly, we find there is no clear way to directly employ Eq. 1 to bridge the columns in Fig. 1 (b).\n\nTo address the issue, we generalize Eq. 1 into the following form:\n\nF orward : xt = Ft(xt−1, xt−2, ..., xt−m+1) + γxt−m Inverse : xt−m = γ−1[xt − Ft(xt−1, xt−2, ..., xt−m+1)],\n\n(2)\n\n†In Gomez et al. (2017), the proposed reversible equations are formulated as y1 = x1 + F(x2) and y2 = x2 + G(y1). While in this paper, we reformulate those notations y2, y1, x2, x1, G, F as xt, xt−1, xt−2, xt−3, Ft, Ft−1, respectively, in order to better illustrate the relation between building block t and t − 1. It is easy to prove the two formulations are equivalent.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: (a) Reversible unit in RevNet (Gomez et al., 2017). (b) Multi-level reversible unit. All inputs for level t are highlighted. (c) An overview of the whole reversible column network architecture, with simplified multi-level reversible unit.\n\nwhere m is the order of the recursion (m ≥ 2). Clearly, the extension is still reversible. Then we partition every m feature maps into a group: (x1, x2, . . . , xm), (xm+1, xm+2, . . . , x2m), . . . . Given the features within any of the group, we can easily compute the features in other groups recursively according to Eq. 2. Compared with the original form, Eq. 2 has the following two nice properties:\n\n• The constraint on the feature map sizes is greatly relaxed if m is relatively large. Notice that Eq. 1 does not require feature maps within each group to be equal sized; such constraint only exist between groups. Therefore, we can use tensors of different shape to represent features of different semantic levels or different resolutions.\n\n• Eq. 2 can easily cooperate with existing network architectures, even though the latter is not reversible. For example, we can assign m feature maps in a standard ResNet to represent the feature maps within a group (xt, xt+1, . . . , xt+m−1), which is still compatible with Eq. 2 since ResNet can be viewed as a part of (Ft, Ft+1, . . . , Ft+m−1) respectively. Thus the whole network is still reversible.\n\nTherefore, we can reorganize Eq. 2 into a multi-column fashion, as shown in Fig. 2 (b). Each column is composed of m feature maps within a group, as well as their mother network. We name it multi-level reversible unit, which is the basic component of our RevCol as in Fig. 1 (b).\n\n2.2 REVERSIBLE COLUMN ARCHITECTURE\n\n2.2.1 MACRO DESIGN\n\nAs discussed in the introduction (see Fig. 1 (b)), our network RevCol is composed of multiple subnetworks with reversible connections to perform feature disentangling. Fig. 2 (c) elaborates the architecture design. Following the common practice of recent models (Dosovitskiy et al., 2020; Liu et al., 2022b), first the input image is split into non-overlapping patches by a patch embedding module. After that, patches are fed into each subnetwork (column). Columns can be implemented with any conventional single-column architectures, e.g. ViT (Dosovitskiy et al., 2020) or ConvNeXt (Liu et al., 2022b). We extract four-level feature maps from each column to propagate information between columns; for example, if the columns are implemented with widely-used hierarchical networks (Liu et al., 2021; He et al., 2016; Liu et al., 2022b), we can simply extract multi-resolution features from the output of each stage. For classification tasks, we only use feature map of the last level (Level 4) in the last column for rich semantic information. For other down-stream tasks like object detection and semantic segmentation, we use feature maps of all the four levels in the last column as they contain both low-level and semantic information.\n\n4\n\nFusionLevel2Level1FusionLevel3FusionLevel4IntermediateSupervisionIntermediateSupervisionSupervisionFusionLevel2Level1FusionLevel3FusionLevel4MoreColumnsMoreColumnsMoreColumnsMoreColumns(a)(c)(b)X1X0t-m+1Xt-mXtXt-1Xt-2XtXt-2XLevel2FusionLevel1FusionLevel3FusionLevel4STEMImage InputFt-3Ft-4Ft-2Ft-1Ftt-1Xt-3Xt-5Xt-4XPublished as a conference paper at ICLR 2023\n\nTo implement the reversible connections between columns, we adopt the multi-level reversible unit proposed in Eq. 2, but in a simplified fashion: rather than take (m − 1) inputs for each non-linear operation Ft(·), we use only one low-level feature xt−1 at the current column and one high-level feature xt−m+1 at the previous column as the input. The simplification does not break the reversible property. We find more inputs bring minor accuracy gain but consume much more GPU resources. Thus Eq. 2 is simplified as:\n\nF orward : xt = Ft(xt−1, xt−m+1) + γxt−m Inverse : xt−m = γ−1[xt − Ft(xt−1, xt−m+1)].\n\n(3)\n\nCompared with conventional architectures, the macro design of our RevCol has the following three properties or advantages:\n\nFeature disentangling. In RevCol, the lowest level of each column maintains low-level features as it is close to the input, while the highest level in the last column is highly semantic because it is directly connected to the supervision. Therefore, information in different levels is gradually disentangled during the (lossless) propagation between columns – some feature maps are more and more semantic and some maintain to be low-level. Detailed analyses are presented in Appendix G. The property brings many potential advantages, for instance, more flexible to downstream tasks which rely on both high-level and low-level features. We argue that reversible connection plays a key role in the disentangling mechanism – some previous works like HRNet (Wang et al., 2020) involve multi-level feature fusion but without reversible connection, which may suffer from information loss and lead to inferior performances in our experiments (see Section D.2).\n\nMemory saving. The training of conventional networks takes a lot of memory footprint to store the activations during forward propagation as the demand of gradient computation. While in our RevCol, since the connections between columns are explicitly reversible, during the back-propagation we can reconstruct the required activations on the fly from the last column to the first, which means we only need to maintain activations from one column in memory during training. In Section D.4, we demonstrate RevCol costs roughly O(1) additional memory with the increase of column numbers.\n\nNew scaling factor for big models. In RevCol architecture, column number serves as a new dimension in addition to depth (number of blocks), and width (channels of each block) in vanilla single-column CNNs or ViTs. Increasing column numbers has similar income as increasing both width and depth in certain range.\n\n2.2.2 MICRO DESIGN\n\nWe employ ConvNeXt blocks (Liu et al., 2022b) to implement each column in our network by default; other architectures, such as transformers, are also applicable (see Appendix B for details). We make a few modifications to make ConvNeXt compatible with our macro architecture:\n\nFusion module. As shown in Fig. 3, in each level of original ConvNeXt, the inputs are first downsampled in a patch-merging block. Then the outputs are passed through a bunch of residual blocks. In RevCol, we introduce a fusion module to fuse the feature maps from the current and previous columns (refer to Fig. 2 (c), green and blue connections). We modify the original patch-merging block in ConvNeXt by putting the LayerNorm after the patch-merging convolution rather than before. Channel numbers are doubled in patch-merging convolution. We also introduce an up-sampling block, which is composed of a linear channel mapping layer, a LayerNorm normalization and a feature map interpolation layer. We halve the channel numbers in linear channel mapping layer. The outputs of the two blocks are summed up and then passed to the residual blocks followed by.\n\nIn RevCol we revise the 7 × 7 convolutions in original ConvNeXt (Liu et al., 2022b) to Kernel size. 3 × 3 by default, mainly to speed up the training. Increasing kernel size further obtains more accuracy, but not very much, partly because the our multi-column design enlarges the effective receptive field. Please refer to Section D.5 for more details.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nReversible operation γ. We adopt a learnable reversible channel-wise scaling as reversible operation γ to keep the network stable. Each time the features are summed up in forward of Eq. 3, the magnitude grows larger, which makes the training process unstable. Using a learnable scaling can suppress the magnitude of features. During training, we truncate the absolute value of γ so that it will never be smaller than 1e−3, because the numerical error could become large in the reverse computation when γ is too small.\n\n2.3\n\nINTERMEDIATE SUPERVISION\n\nThough multi-level reversible unit is able to maintain information during column iteration, the down-sample block still can discard information inside column. Features at the end of the front columns is too close to the final output, for reversible connections simply do scaling and summation. Such information loss leads to inferior performance. Similar problem also happens when using deeply-supervised method (Lee et al., 2015; Szegedy et al., 2015).\n\nTo mitigate the problem of information collapse, we propose an intermediate supervision method which adds additional supervision into front columns. For features in front columns, we hope to keep the mutual information between features and the input image as much as possible, so that the network discard less information within columns. Consider RevCol gradually disentangle semantic and low-level information, extracting and leveraging the task-relevant information can further boost the performance. Therefore, we need to maximize the lower bound of mutual information between features and the prediction.\n\nInspired by Wang et al. (2021), we add two auxiliary heads to last level features (Level 4). One is a decoder (He et al., 2022) which reconstructs the input image, the other is a linear classifier. The linear classifier can be trained in a regular classification fashion with the cross-entropy (CE) loss. The parameters of decoder are optimized by minimizing the binary cross-entropy (BCE) reconstruction loss. Compared with commonly used L1 and L2 loss, interpreting the distribution of reconstructed logits and input image as bit probabilities (Bernoullis) outputs smoother value, which makes it more compatible with CE Loss.\n\nFor intermediate supervision at one column, the compound loss is the weighted summation of the above two loss. Note that supervision heads may not be added to all columns. For all the variants of RevCol, we set the number of compound loss to 4 empirically (eg. for a 8 column RevCol, the supervision heads are added to column 2, 4, and 6, and 8).\n\nThe total loss L in is the summation of all compound loss:\n\nn (cid:88)\n\n(αiLBCE + βiLCE)\n\nL =\n\ni=1\n\n(4)\n\nn denotes the total number of compound loss. LBCE and LCE denotes BCE loss and CE loss correspondingly. αi and βi are changed linearly with the compound loss number. When the compound loss is added in earlier columns, we use larger value αi and smaller value βi to keep I(h, x). In later columns, value αi decreases and βi increases, which helps boost the performance.\n\n3 EXPERIMENTS\n\nWe construct different RevCol variants, RevCol-T/S/B/L, to be of similar complexities to Swin transformers and ConvNeXts. We also build a larger RevCol-XL and RevCol-H to test the scaling up capability. These variants adopt different channel dimension C, blocks in each column B and column numbers COL. The configuration hyper-parameters of these model variants are:\n\n• RevCol-T: C = (64, 128, 256, 512), B = (2, 2, 4, 2), COL = 4\n\n• RevCol-S: C = (64, 128, 256, 512), B = (2, 2, 4, 2), COL = 8\n\n• RevCol-B: C = (72, 144, 288, 576), B = (1, 1, 3, 2), COL = 16\n\n• RevCol-L: C = (128, 256, 512, 1024), B = (1, 2, 6, 2), COL = 8\n\n• RevCol-XL: C = (224, 448, 896, 1792), B = (1, 2, 6, 2), COL = 8\n\n• RevCol-H: C = (360, 720, 1440, 2880), B = (1, 2, 6, 2), COL = 8\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: ImageNet classification results. We compare our models with state-of-the-art ◦ Vision Transformers and • CNNs that have comparable FLOPs and parameters. ↑ denotes models fine-tuning using image size larger than 2242. We report the top-1 accuracy on the validation set of ImageNet as well as the number of parameters and FLOPs. Our models are highlighted in gray.\n\nModel\n\nImage ParamsFLOPs Top-1 Size\n\nAcc.\n\n(M)\n\n(G)\n\nModel\n\nImage ParamsFLOPs Top-1 Size\n\nAcc.\n\n(M)\n\n(G)\n\nImageNet-1K trained models\n\n2242 ◦ Swin-T (Liu et al.) 2242 ◦ DeiT-S (Touvron et al. ◦ Rev-ViT-S (Mangalam et al.)2242 • RevBiFPN-S3 (Chiley et al.)2882 • EfficientNet-B4 (Tan & Le) 3802 2242 • ConvNeXt-T (Liu et al.) 2242 • RevCol-T\n\n2242 ◦ Swin-S (Liu et al.) 2242 ◦ MViTv1-B (Fan et al.) 2242 ◦ T2T-ViT-19 (Yuan et al.) • RevBiFPN-S4 (Chiley et al.)3202 • EfficientNet-B5 Tan & Le) 4562 2242 • ConvNeXt-S (Liu et al.) 2242 • RevCol-S\n\n2242 ◦ Swin-B (Liu et al.) 2242 ◦ DeiT-B (Touvron et al.) ◦ Rev-ViT-B(Mangalam et al.)2242 • RepLKNet-31B (Ding et al.)2242 • RevBiFPN-S5 (Chiley et al.)3522 • EfficientNet-B6 (Tan & Le) 5282 2242 • ConvNeXt-B (Liu et al.) 2242 • RevCol-B\n\n28 22 22 20 19 29 30\n\n50 37 39 49 30 50 60\n\n89 86 87 79 82 43 88 138\n\n4.5 4.6 4.6 3.3 4.2 4.5 4.5\n\n8.7 7.8 8.4 10.6 9.9 8.7 9.0\n\n15.4 17.5 17.6 15.3 21.8 19.0 15.4 16.6\n\n81.3 79.8 79.9 81.1 82.9 82.1 82.2\n\n83.0 83.0 81.4 83.0 83.6 83.1 83.5\n\n83.5 81.8 81.8 83.5 83.7 84.0 83.8 84.1\n\nImageNet-22K pre-trained models (ImageNet-1K fine-tuned)\n\n2242 ◦ Swin-B (Liu et al. ◦ Swin-B↑ (Liu et al. 3842 ◦ ViT-B↑ (Dosovitskiy et al.) 3842 • RepLKNet-31B (Ding et al.) 2242 • RepLKNet-31B↑ (Ding et al.)3842 2242 • ConvNeXt-B (Liu et al.) • ConvNeXt-B↑ (Liu et al.) 3842 2242 • RevCol-B • RevCol-B↑ 3842\n\n2242 ◦ Swin-L (Liu et al.) ◦ Swin-L↑ (Liu et al.) 3842 ◦ ViT-L↑ (Dosovitskiy et al.) 3842 • RepLKNet-31L (Ding et al.) 3842 2242 • ConvNeXt-L (Liu et al.) • ConvNeXt-L↑ (Liu et al.) 3842 2242 • RevCol-L • RevCol-L↑ 3842\n\n• ConvNeXt-XL↑ (Liu et al.) 3842 • RevCol-XL↑ 3842\n\n88 88 86 79 79 89 89 138 138\n\n197 197 307 172 198 198 273 273\n\n350 834\n\n15.4 47.0 55.4 15.3 45.1 15.4 45.1 16.6 48.9\n\n85.2 86.4 84.0 85.2 86.0 85.8 86.8 85.6 86.7\n\n34.5 86.3 103.9 87.3 190.7 85.2 86.6 96.0 34.4 86.6 101.0 87.5 39.0 86.6 116.0 87.6\n\n179.0 87.8 350.0 88.2\n\nExtra data pre-trained models (ImageNet-1K fine-tuned)\n\n• RevCol-XL↑ • RevCol-H↑\n\n3842 6402\n\n834 2158\n\n350.0 89.4 90.0 2537\n\nWe conduct image classification on ImageNet dataset (Deng et al., 2009; Ridnik et al., 2021). We also test our models on the downstream object detection task and semantic segmentation task on commonly used MS-COCO (Lin et al., 2014) and ADE20k (Zhou et al., 2017b) dataset. Training and fine-tuning settings please refer to Appendix F. Furthermore, we show the performance of RevCol with transformer on vision and language tasks (shown in Appendix B).\n\n3.1\n\nIMAGE CLASSIFICATION\n\nOn ImageNet (1.28M images) (Deng et al., 2009) dataset, we train RevCol for 300 epochs with intermediate supervision. Hyperparameters, augmentation and regularization strategies follows Liu et al. (2022b) We also pre-train our models on the larger ImageNet-22K dataset (Ridnik et al., 2021), which contains 14.2 million images.\n\nIn Tab. 1, we compare our RevCol variants with commonly used recent Transformers and CNNs on ImageNet-1k validation set. Our models outperforms a large number of vanilla single-column CNNs and Transformers with similar complexities. For example, RevCol-S achieve 83.5% Top-1 accuracy, outperform ConvNeXt-S by 0.4 points. When pre-trained with larger ImageNet-22K dataset, RevCol-XL achieves 88.2% Top-1 accuracy. As RevCol maintains some task-irrelevant low-level information in classification pre-training, relaxing the constraint of params and FLOPs and enlarging dataset size can further boost our models’ performance. To further test the scaling up effectiveness of large dataset, we build a 168-million-image semi-labeled dataset (see Appendix E). With extra data pre-training and ImageNet-1k fine-tuning, our RevCol-H achieves 90.0% top-1 accuracy. Our results further demonstrate with RevCol, CNN models can also share the dividends of large model and massive data pre-training.\n\n3.2 OBJECT DETECTION\n\nWe evaluate our proposed RevCol on object detection task. Experiments are conducted on the MS-COCO dataset using the Cascade Mask R-CNN (Cai & Vasconcelos, 2019) framework. We\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Object detection results on MS-COCO dataset with different backbones. We report box AP and mask AP with single scale testing on COCO minival set. FLOPs are measured under input sizes of (1280, 800).\n\nBackbone\n\nAPbox APbox 50\n\nAPbox 75\n\nAPmask APmask\n\n50\n\nAPmask 75\n\nParams\n\nFLOPs\n\n◦ Swin-T (Liu et al.) • ConvNeXt-T (Liu et al.) • RevCol-T ◦ Swin-S (Liu et al.) • ConvNeXt-S (Liu et al.) • RevCol-S ◦ Swin-B (Liu et al.) • ConvNeXt-B (Liu et al.) • RepLKNet-B (Ding et al.) • RevCol-B\n\n◦ Swin-B (Liu et al.) • ConvNeXt-B (Liu et al.) • RepLKNet-B (Ding et al.) • RevCol-B ◦ Swin-L (Liu et al.) • ConvNeXt-L (Liu et al.) • RepLKNet-L (Ding et al.) • RevCol-L\n\n50.5 50.4 50.6 51.8 51.9 52.6 51.9 52.7 52.2 53.0\n\n53.0 54.0 53.0 55.0 53.9 54.8 53.9 55.9\n\n• RevCol-H (HTC++) • RevCol-H (Objects365+DINO)\n\n61.1 63.8\n\nImageNet-1K pre-trained 43.7 54.9 43.7 54.8 43.8 54.9 44.7 56.3 45.0 56.5 45.5 56.8 45.0 56.5 45.6 57.2 45.2 -\n45.9 57.3\n\n69.3 69.1 68.9 70.4 70.8 71.1 70.9 71.3 -\n71.4\n\nImageNet-22K pre-trained 45.8 57.5 46.9 58.8 46.3 -\n47.5 59.7 46.7 58.8 47.6 59.8 46.5 -\n48.4 60.7\n\n71.8 73.1 -\n73.5 72.4 73.8 -\n74.1\n\nExtra data pre-trained 67.0 78.8 70.2 81.8\n\n53.0 -\n\n66.6 66.5 66.7 67.9 68.4 68.8 68.4 68.9 -\n69.1\n\n69.4 70.6 -\n71.1 70.1 71.3 -\n71.8\n\n76.3 -\n\n47.1 47.3 47.4 48.5 49.1 49.0 48.7 49.5 -\n50.1\n\n49.7 51.3 -\n51.8 50.8 51.7 -\n52.8\n\n58.7 -\n\n86M 86M 88M 107M 108M 118M 145M 146M 137M 196M\n\n145M 146M 137M 196M 253M 255M 229M 330M\n\n745G 741G 741G 838G 827G 833G 982G 964G 965G 988G\n\n982G 964G 965G 988G 1382G 1354G 1321G 1453G\n\n2.41G 2.18G\n\n4417G 4012G\n\nalso finetune our largest model RevCol-H with HTC++ (Chen et al., 2019) and DINO (Zhang et al., 2022a) Framework.\n\nIn Tab. 2, we compare the APbox and APmask with Swin/ConvNeXt in variant sizes on COCO validation set. We find RevCol models surpass other counterparts with similar computation complexities. Information retained in pre-training helps RevCol models acheieve better results in down-stream tasks. When the model size grows larger, this advantage becomes more remarkable. After finetuning under Objects365(Shao et al., 2019) dataset and DINO (Zhang et al., 2022a) framework, our largest model RevCol-H achieves 63.8% APbox on COCO detection minival set.\n\n3.3 SEMANTIC SEGMENTATION\n\nWe also evaluate RevCol backbones on the ADE20K semantic segmentation task with UperNet (Xiao et al., 2018) framework. We do not use intermediate-supervision in down-stream fine-tune process. To further explore our model’s capacity and reach the leading performance, we utilize recent segmentation framework Mask2Former (Cheng et al., 2022), and adopt the same training settings.\n\nIn Tab. 3, we report validation mIoU with single-scale and multi-scale flip testing. RevCol models can achieve competitive performance across different model capacities, further validating the effectiveness of our architecture design. It’s worth noting that when use Mask2Former detector and extra pretraining data, RevCol-H achieves an mIoU of 61.0%, which shows feasible scalability towards large-scale vision applications.\n\n4 RELATED WORKS\n\n4.1 DISENTANGLE REPRESENTATION LEARNING AND PART-WHOLE HIERARCHY\n\nA disentangled representation is generally described as one which separates the factors of variation, explicitly representing the important attributes of the data (Desjardins et al., 2012; Bengio et al., 2013). Desjardins et al. (2012); Kulkarni et al. (2015); Higgins et al. (2017); Chen et al. (2016); Karras et al. (2019) seek to learn disentangled representations through generative models. Locatello et al. (2019) points out that unsupervised learning of disentangled representations is fundamentally impossible without inductive biases both on the considered learning approaches and the datasets. The recent proposal of GLOM (Hinton, 2021) gives an idea of representing a part-whole hierarchy by a\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Semantic segmentation result on ADE20k dataset with different backbones. we report mIoU results with single/multi-scale testing. FLOPs are measured under input sizes of (2048, 512), (2560, 640) for IN-1K and IN-22K pre-trained models respectively.\n\nBackbone\n\ncrop size mIoUss mIoUms\n\nParams\n\nFLOPs\n\nImageNet-1K pre-trained\n\n◦ Swin-T (Liu et al.) • ConvNeXt-T (Liu et al.) • RevCol-T ◦ Swin-S (Liu et al.) • ConvNeXt-S (Liu et al.) • RevCol-S ◦ Swin-B (Liu et al.) • RepLKNet-B (Ding et al.) • ConvNeXt-B (Liu et al.) • RevCol-B\n\n5122 5122 5122 5122 5122 5122 5122 5122 5122 5122\n\n44.5 46.0 47.4 47.6 48.7 47.9 48.1 49.9 49.1 49.0\n\n45.8 46.7 47.6 49.5 49.6 49.0 49.7 50.6 49.9 50.1\n\nImageNet-22K pre-trained\n\n◦ Swin-B (Liu et al.) • RepLKNet-B (Ding et al.) • ConvNeXt-B (Liu et al.) • RevCol-B ◦ Swin-L (Liu et al.) • RepLKNet-L (Ding et al.) • ConvNeXt-L (Liu et al.) • RevCol-L\n\n6402 6402 6402 6402 6402 6402 6402 6402\n\n50.3 51.5 52.6 52.7 52.1 52.4 53.2 53.4\n\n• RevCol-H • RevCol-H + Mask2Former\n\nExtra data pre-trained 6402 6402\n\n57.8 60.4\n\n51.7 52.3 53.1 53.3 53.5 52.7 53.7 53.7\n\n58.0 61.0\n\n60M 60M 60M 81M 82M 90M 121M 112M 122M 122M\n\n121M 112M 122M 122M 234M 207M 235M 306M\n\n945G 939G 937G 1038G 1027G 1031G 1188G 1170G 1170G 1169G\n\n1841G 1829G 1828G 1827G 2468G 2404G 2458G 2610G\n\n2421M 2439M\n\n- -\n\nweight-sharing columns. The GLOM architecture provides an interpretable part-whole hierarchies for deep neural network (Garau et al., 2022). In RevCol, we adopt the design of using columns, but not modeling the process of formulating islands. On the contrary, our column iteration process maintains both low-level and high-level information and gradually disentangle them. Rather than using self-supervised methods, RevCol can be trained with supervision end-to-end.\n\n4.2 REVERSIBLE NETWORKS\n\nGomez et al. (2017) firstly propose RevNet that allow back propagation without saving intermediate activations. The reversible design remarkably saves the training cost, since it keep O(1) GPU memory consumption as model depth scaling up. Jacobsen et al. (2018) propose a fully reversible network that can reverse back to the input without any information loss. Chang et al. (2018) develop a theoretical framework on stability and reversibility of deep neural network and derive reversible networks that can go arbitrarily deep. Mangalam et al. (2022) expand the reversible network scope from CNNs to Transformers. RevBiFPN (Chiley et al., 2022), a concurrent work of ours, add the reversible connections to BiFPN (Tan et al., 2020) network. Our RevCol maintains the information without loss inside each column rather than the whole BiFPN network in RevBiFPN.\n\n5 CONCLUSION\n\nIn this paper, we propose RevCol, a reversible column based foundation model design paradigm. During the lossless propagation through columns, features in RevCol are learned to be gradually disentangled and the total information is still maintained rather than compressed. Our experiments suggests that RevCol can achieve competitive performance in multiple computer vision tasks. We hope RevCol could contribute to better performance in various tasks in both vision and language domains.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nShaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. Advances in\n\nNeural Information Processing Systems, 33:5238–5250, 2020.\n\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\n\narXiv:2106.08254, 2021.\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.\n\nZhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: high quality object detection and instance segmentation. IEEE transactions on pattern analysis and machine intelligence, 43(5):1483–1498, 2019.\n\nRich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.\n\nBo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham. Reversible architectures for arbitrarily deep residual neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n\nKai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4974–4983, 2019.\n\nTianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge\n\ntransfer. arXiv preprint arXiv:1511.05641, 2015.\n\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. Advances in neural information processing systems, 29, 2016.\n\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\n\nVitaliy Chiley, Vithursan Thangarasa, Abhay Gupta, Anshul Samar, Joel Hestness, and Dennis DeCoste. Revbifpn: The fully reversible bidirectional feature pyramid network. arXiv preprint arXiv:2206.14098, 2022.\n\nJifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international conference on computer vision, pp. 764–773, 2017.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nGuillaume Desjardins, Aaron Courville, and Yoshua Bengio. Disentangling factors of variation via\n\ngenerative entangling. arXiv preprint arXiv:1210.5474, 2012.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nMingyu Ding, Xiaochen Lian, Linjie Yang, Peng Wang, Xiaojie Jin, Zhiwu Lu, and Ping Luo. Hr-nas: Searching efficient high-resolution neural architectures with lightweight transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2982–2992, 2021.\n\nXiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11963–11975, 2022a.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nXiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11963–11975, 2022b.\n\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components\n\nestimation. arXiv preprint arXiv:1410.8516, 2014.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nHaoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and In Proceedings of the IEEE/CVF\n\nChristoph Feichtenhofer. Multiscale vision transformers. International Conference on Computer Vision, pp. 6824–6835, 2021.\n\nNicola Garau, Niccolò Bisagno, Zeno Sambugaro, and Nicola Conci.\n\nhierarchies and conceptual-semantic relationships in neural networks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13689–13698, 2022.\n\nInterpretable part-whole In Proceedings of the\n\nGolnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7036–7045, 2019.\n\nGolnaz Ghiasi, Barret Zoph, Ekin D Cubuk, Quoc V Le, and Tsung-Yi Lin. Multi-task self-training for learning general representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8856–8865, 2021.\n\nAidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations. Advances in neural information processing systems, 30, 2017.\n\nQi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiaying Liu, and Jingdong Wang. On the connection between local attention and dynamic depth-wise convolution. In International Conference on Learning Representations, 2021.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked In Proceedings of the IEEE/CVF Conference on\n\nautoencoders are scalable vision learners. Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=Sy2fzU9gl.\n\nGeoffrey Hinton. How to represent part-whole hierarchies in a neural network. arXiv preprint\n\narXiv:2102.12627, 2021.\n\nJörn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks.\n\narXiv preprint arXiv:1802.07088, 2018.\n\nPeng-Tao Jiang, Chang-Bin Zhang, Qibin Hou, Ming-Ming Cheng, and Yunchao Wei. Layercam: Exploring hierarchical class activation maps for localization. IEEE Transactions on Image Processing, 30:5875–5888, 2021.\n\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401–4410, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In European conference on computer vision, pp. 491–507. Springer, 2020.\n\nSimon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International Conference on Machine Learning, pp. 3519– 3529. PMLR, 2019.\n\nTejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional\n\ninverse graphics network. Advances in neural information processing systems, 28, 2015.\n\nChen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised\n\nnets. In Artificial intelligence and statistics, pp. 562–570. PMLR, 2015.\n\nTimothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton. Backprop-\n\nagation and the brain. Nature Reviews Neuroscience, 21(6):335–346, 2020.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740–755. Springer, 2014.\n\nTsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125, 2017.\n\nChenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, and Li FeiFei. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 82–92, 2019.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\n\nZe Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12009–12019, 2022a.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11976–11986, 2022b.\n\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In international conference on machine learning, pp. 4114–4124. PMLR, 2019.\n\nNingning Ma, Xiangyu Zhang, Jiawei Huang, and Jian Sun. Weightnet: Revisiting the design space of weight networks. In European Conference on Computer Vision, pp. 776–792. Springer, 2020.\n\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European conference on computer vision (ECCV), pp. 181–196, 2018.\n\nKarttikeya Mangalam, Haoqi Fan, Yanghao Li, Chao-Yuan Wu, Bo Xiong, Christoph Feichtenhofer, and Jitendra Malik. Reversible vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10830–10840, 2022.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\n\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 1–9, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6301.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.\n\nTal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for\n\nthe masses. arXiv preprint arXiv:2104.10972, 2021.\n\nSebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint\n\narXiv:1706.05098, 2017.\n\nOzan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances in\n\nneural information processing systems, 31, 2018.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In 54th Annual Meeting of the Association for Computational Linguistics, pp. 1715–1725. Association for Computational Linguistics (ACL), 2016.\n\nShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 8430–8439, 2019.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015.\n\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks.\n\nIn International conference on machine learning, pp. 6105–6114. PMLR, 2019.\n\nMingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10781–10790, 2020.\n\nBart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64–73, 2016.\n\nNaftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015\n\nieee information theory workshop (itw), pp. 1–5. IEEE, 2015.\n\nNaftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv\n\npreprint physics/0004057, 2000.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nJingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 43(10):3349–3364, 2020.\n\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nYulin Wang, Zanlin Ni, Shiji Song, Le Yang, and Gao Huang. Revisiting locally supervised learning:\n\nan alternative to end-to-end training. arXiv preprint arXiv:2101.10832, 2021.\n\nBichen Wu, Chaojian Li, Hang Zhang, Xiaoliang Dai, Peizhao Zhang, Matthew Yu, Jialiang Wang, Yingyan Lin, and Peter Vajda. Fbnetv5: Neural architecture search for multiple tasks in one run. arXiv preprint arXiv:2111.10007, 2021.\n\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European conference on computer vision (ECCV), pp. 418–434, 2018.\n\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9653–9663, 2022.\n\nI Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-\n\nsupervised learning for image classification. arXiv preprint arXiv:1905.00546, 2019.\n\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 558–567, 2021a.\n\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021b.\n\nAmir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712–3722, 2018.\n\nHao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022a.\n\nYuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin He, Zhenfei Yin, Kun Wang, Lu Sheng, Yu Qiao, Jing Shao, and Ziwei Liu. Bamboo: Building mega-scale vision dataset continually with human-machine synergy, 2022b.\n\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017a.\n\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 633–641, 2017b.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA MICRO DESIGN DETAILS\n\nFigure 3: (a) Levels in ConvNeXt. Level l contains a patch merging down-sample block and nl residual blocks. (b) Levels in RevCol. Level l is composed of a fusion module, nl residual blocks and a reversible operation. Note that Level l takes features maps xt−1, xt−m+1 and xt−m as input. Feature maps xt−1 and xt−m+1 are fed into the fusion module and feature maps xt−m are fed into the reversible operation. (c) Design of the fusion module.\n\nIn this section, we provide the architecture design details for RevCol. As depicted in Fig. 2 and Section 2.2, our RevCol contains multiple columns with reversible connections. Fig. 3 (a) shows the architecture of ConvNeXt. Note that we replace the 7 × 7 depth-wise convolution in ConvNeXt with 3 × 3, as described in Sec. 2.2.2. In Fig. 3 (b), we show in detail how to extend to our RevCol on the basis of ConvNeXt. First, we replace the down-sample block with a fusion block to fuse low-level representations in current column and high-level ones from the previous column, and Fig. 3 (c) shows the details of fusion block which contains up-sample and down-sample operations to handle different resolutions. Second, for each level, same-level representations from the previous column are added to current level’s output and are ready to propagate as a whole. Thanks to the two modifications, feature maps from different hierarchies aggregate together to form the intermediate representation. In Fig. 3 (c), we use a Linear-LayerNorm followed by a nearest interpolation to up-sample low resolution features. A 2 × 2 kernel Conv2d with stride 2 down-samples the high resolution features, followed by a LayerNorm to balance the contributions of the two inputs.\n\nB GENERALIZATION TO TRANSFORMERS\n\nB.1 VISION TRANSFORMER MODELS\n\nRevCol contains multiple light-weight sub-networks with reversible connections. In this paper, we adopt the ConvNext micro design by default except for multi-columns fusion and smaller convolution kernel as described in Sec. 2.2.2. However, the micro design of our RevCol is not limited to convolutional networks, but is also compatible with isotropic designing, such as the vanilla vision\n\n15\n\n(a)(b)(c)Up-SampleDown-SampleSumDown-SampleBlockDown-SampleBlockConvNeXtBlockst-m-1Xt-mXt-m+1Xt-2Xt-1XtXConvNeXtBlocksConv(s2, k2)High Resolution FeaturesLow Resolution FeaturesLayerNormLinearLayerNormInterpolationto ConvNeXt blocksConvNeXtBlocksFusion BlockConvNeXtBlocksFusion Blockto other levelsto other levelslX nlX nl -1X nl -1X nLevel l-1Level lLevel l-1Level lPublished as a conference paper at ICLR 2023\n\ntransformer (ViT) (Dosovitskiy et al., 2020). In this section, we show the micro design of RevCol can generalized to vanilla ViT, namely RevCol-ViT, with promising experimental results.\n\nnet-ViT maintains the feature resolution in the reversible columns. Thus the patch merging blocks and up-sample blocks in the fusion modules are replaced with a simple linear projection with a post LayerNorm. We use the vanilla ViT building block instead of the ConvNext building block variant. The post LayerNorms and normalized dot-product attention are used in ViT blocks to stabilize training convergence, similar to Liu et al. (2022a). With the properties of isotropy, we evenly arrange the building blocks in each column. The configuration details of RevCol-ViT are:\n\n• RevCol-ViT-S: C = (224, 224, 224, 224), B = (2, 2, 2, 2), HEAD = 4, COL = 4\n\n• RevCol-ViT-B: C = (384, 384, 384, 384), B = (3, 3, 3, 3), HEAD = 6, COL = 4\n\nTable 4: ImageNet-1K classification results. We compare our RevCol-ViT with state-of-the-art isotropic ◦ Vision Transformers and • CNNs that have comparable FLOPs and parameters.\n\nModel\n\nImage Size Params FLOPs Top-1 Acc.\n\n◦ DeiT-S (Touvron et al., 2020) • ConvNext-S (iso.) (Liu et al., 2022b) ◦ RevCol-ViT-S ◦ ViT-B (Dosovitskiy et al., 2020) ◦ DeiT-B (Touvron et al., 2020) ◦ Rev-ViT-B (Mangalam et al., 2022) ◦ Rev-MViT-B (Mangalam et al., 2022) • ConvNext-B (iso.) (Liu et al., 2022b) ◦ RevCol-ViT-B\n\n2242 2242 2242 3842 2242 2242 2242 2242 2242\n\n22M 22M 16M 86M 86M 87M 39M 87M 67M\n\n4.6G 4.3G 4.6G 55.4G 17.6G 17.6G 8.7G 16.9G 18.8G\n\n79.8 79.7 80.6 77.9 81.7 81.8 82.5 82.0 82.7\n\nWe use the same training setting with the anisotropic RevCol as described in Sec. 3.1, except that the intermediate supervision is discarded for simplicity and the stochastic depth rate is set as 0.2 for RevCol-B. We scale down the value of last linear projection layers in each FFN accroding to the network depth in initialization, same as BEiT (Bao et al., 2021). In Tab. 4, we compare the RevColViT with vanilla ViT and other concurrent isotropic designs. Our RevCol-ViT surpasses vanilla vision transformer (77.9% for ViT and 81.7% for DeiT) and convolutional network ConvNeXt (82.0%) that have comparable model parameters and computational overhead on ImageNet-1k classification w.r.t. the top-1 accuracy.\n\nB.2 LANGUAGE MODELS\n\nConsidering the great success of applying transformer to computer vision, i.e., ViT (Dosovitskiy et al., 2020), we also made some exploration to generalize RevCol to natural language processing (NLP). Based on the design in Appendix B.1, we can easily apply the isotropic RevCol to language models with minor modification. To be specific, we replace the stem module in our RevCol with word embedding and positional encoding in transformer. Then, the RevCol can be plugged into the original transformer as an encoder. The output of the last column in RevCol will be used as the memory keys and values for the attention layers in decoder, just exactly the same as the original transformer.\n\nWe select the translation task to evaluate the potential of the RevCol in NLP. We run experiments on the WMT’16 English-German (En-De) dataset with 4.5M sentences and larger WMT’14 EnglishFrench dataset with 36M sentences. Each sentence is encoded by joint source and target byte pair encoding following Sennrich et al. (2016). The details of model architecture and the BLEU score are shown in Tab. 5.\n\nAll the dataset preparation and the training configurations follows Ott et al. (2018) and the open source project fairseq. The models were trained for 300K steps with batch-size of 28,672 tokens on En-De and 200K steps with batch-size of 86,016 on En-Fr. We discard the intermediate supervision for simplicity. As shown in Tab. 5, our RevCol outperforms vanilla transformer with comparable parameters on En-De (28.67 vs. 28.43) and En-Fr (43.40 vs. 43.07), which demonstrates the RevCol’s applicability to NLP.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: BLEU score on newstest2014 for WMT English-German (En-De) and English-French (En-Fr) translation task. † indicates we re-run the experiments with fairseq.\n\nModel\n\nTransformer† (Vaswani et al., 2017)\n\nbig\n\nRevCol-Transformer\n\narch\n\nN = 6\n\nB = (1,1,1,1) COL = 4\n\nEncoder\n\nDecoder\n\ndmodel\n\ndf f\n\nhead\n\narch\n\ndmodel\n\ndf f\n\nhead\n\n1024\n\n4096\n\n16\n\nN = 6\n\n1024\n\n4096\n\n16\n\n768\n\n3072\n\n12\n\nN = 6\n\n768\n\n3072\n\n12\n\nParams\n\nTask\n\nBLEU\n\n209M En-De 221M En-Fr\n\n200M En-De 209M En-Fr\n\n28.43 43.07\n\n28.67 43.40\n\nB.3 ROBUSTNESS OF THE NUMBER OF COLUMNS\n\nIn the ablation analysis of the paper, we show that when fix the total FLOPs and add more columns of RevCol, the performance first increases and then get saturated. When the number of columns is extreme large, such as 20, the performance drop because of the representation ability of single column is limited. When the number of columns is usual, such as 4 ̃12, the performances are similar, which verifies the setting robustness of the number of columns.\n\nFigure 4: ImageNet top-1 accuracy of different variants of RevCol-ViT-B. Each variant has the same total number of residual blocks and channel dimension.\n\nTo further analyze the robustness of the number of columns, in this section, we build some RevColViT-B variants (see Appendix B for more details). Each variant has the same number of residual blocks with the same channel dimension, but different number of columns. In other worlds, these variants have the same channel dimension and different depth of each columns and different number of columns. We use 32 residual blocks totally and maintain the FLOPs about 18G. Fig. 4 show the performance on ImageNet-1K of different variants. The number of columns are 1, 2, 4, and 8, accordingly the depth of each column are 32, 16, 8, and 4. The performance of single column variant is lower (similar to DeiT-B (Touvron et al., 2020)) because of the single column ViT can not maintain the information as multi reversible columns. The performance is decreasing when the number of columns became larger, because of the depth of each columns is not enough. This phenomenon indicates us that given a target FLOPs, the setting of the number of columns is robust unless the depth of each columns or channel dimension is too small.\n\nC SYSTEM-LEVEL COMPARISON WITH SOTA FOUNDATION MODELS\n\nFoundation models (Kolesnikov et al., 2020; Radford et al., 2021; Yuan et al., 2021b) are generalpurpose backbones pre-trained on massive and divergent data source. They can adapt to various down-stream tasks with limited domain-specific data. We show comparison among various public state-of-the-art (SOTA) foundation models including Vision Transformers and Vision-Language models, namely, SwinV2 (Liu et al., 2022a), BEiT3 (Wang et al., 2022), and Florence (Yuan et al., 2021b). As shown in Tab. 6, though our RevCol-H is purely convolutional and pre-trained on single modality dataset, the results on different tasks demonstrate remarkable generalization ability of RevCol with large scale parameters.\n\n17\n\n124881.081.582.082.583.032 x 1 column 16 x 2 columns8 x 4 columns4 x 8 columnsPublished as a conference paper at ICLR 2023\n\nTable 6: System-level comparison of state-of-the-art visual foundation models with large-scale pretraining. We include ◦ Vision Transformers, • CNNs, and • hybrid architectures pretrained either unsupervised or supervised on image-only and vision-language datasets. COCO scores marked with † means intermediate fine-tuned on extra data like Object365 (Shao et al., 2019).\n\nDataset\n\nImageNet\n\nCOCO test-dev\n\nADE20K\n\n1k\n\n90.2 89.6 90.1 90.0\n\nDetector APbox APmask\n\nSegmenter mIoU +ms\n\nHTC++ ViTDet DyHead DINO\n\n63.1† 63.7† 62.4 63.6†\n\n54.4† 59.3 UperNet 54.8† Mask2Former 62.0 -\n\n-\n\nMask2Former 60.4\n\n- -\n\n59.9 62.8 -\n61.0\n\nModel\n\nParams\n\nImages\n\nAnnotation\n\n◦ SwinV2-G 3.0 G ◦ BEiT3 1.0 G • Florence 0.9 G • RevCol-H 2.1 G\n\n70 M 35 M labeled & image-text\n\nlabeled\n\n900 M 168 M\n\nimage-text semi-labeled\n\nD MORE ANALYSIS EXPERIMENTS\n\nD.1 PERFORMANCE GAIN OF REVERSIBLE COLUMNS ARCHITECTURE\n\nIn this section, we evaluate the performance gain of using reversible columns. In the first experiment, we fix a single column’s structure and FLOPs then simply add more columns to scale large and test the performance. At the same time, we plot the vanilla single-column models with similar model sizes. As depicted in Fig. 5, compared to single-column models, using multi-column reversible architecture always gets better performance under same FLOPs constraint. Besides, within a certain range, scaling up RevCol in terms of increasing column numbers can have similar gains compared to scaling up with both block numbers(depth) and channel numbers(width) in single-column models. In the second experiment, we limit the model size to about 4.5G FLOPs and test model variants with different column numbers. In other words, we gradually add more columns and scale down the single column size at the same time. Results are shown in Tab. 7, we notice that adopt column number at the range of 4 to 12 can maintain the model’s performance, then further more column models suffer from performance degradation. We believe the reason is the width and depth in a single column are too low to keep representation ability.\n\nTable 7: ImageNet 1K performances of various number of columns in RevCols under the similar computational budget.\n\n# column\n\nParams\n\nFLOPs\n\n1 4\n8 12 20\n\n28M 30M 34M 33M 35M\n\n4.4G 4.5G 4.7G 4.4G 4.2G\n\nFLOPs per col.\n\nTop-1 Acc.\n\n4.40G 1.12G 0.59G 0.35G 0.21G\n\n81.9 82.2 82.3 82.2 81.0\n\nFigure 5: ImageNet-1K performance of maintaining a constant FLOPs of a single column and adding more columns.\n\nD.2 REVERSIBLE NETWORKS VS. NON-REVERSIBLE NETWORKS\n\nIn this section, we ablate different design patterns of reversible connections. First, we build a nonreversible multi-column network using the fusion module of HRNet. Second, we build another single column reversible ConvNeXt using the design of RevNet as shown in Fig. 2(a). We compare the two designs with our RevCols. The evaluation result is shown in Tab. 8. The non-reversible multi-column network suffers from information loss during propagation, which could result in lower accuracy. The reversible single-column network maintains information during propagation, but lack the superiority of multi-level fusion. This experiment further indicates the effectiveness of combining the reversible design with multi-column networks.\n\n18\n\n7982818083844.51.09.013.518.0Multi-ColumnSingle-ColumnFLOPs(G)Top-1 Acc. %77.081.682.682.283.583.283.483.984.0Published as a conference paper at ICLR 2023\n\nTable 8: Performance comparison on ImageNet-1K of different design patterns. Row-1 represents HRNet style network w/o reversible connections. Row-2 represents RevNet style network w/o multi-column fusions. Row-3 are our proposed RevCols.\n\nTable 9: Performance comparison between models with and without intermediate supervision. Results are reported on ImageNet-1K and COCO dataset. We use 1× training schedule on COCO detection task.\n\nModel\n\ninter. sup. Top-1 Acc. APbox\n\nAPmask\n\nrev. conn. multi-col. Params FLOPs Acc.\n\n✓ ✓\n\n✓\n\n✓\n\n35M 4.9G 78.8 34M 4.5G 81.6 30M 4.5G 82.2\n\nRevCol-T RevCol-T\n\nRevCol-S RevCol-S\n\nRevCol-B RevCol-B\n\n✗ ✓\n\n✗ ✓\n\n✗ ✓\n\n81.4 82.2 (+0.8) 48.8 (+0.6) 42.2 (+0.4)\n\n41.8\n\n48.3\n\n83.0 83.5 (+0.5) 51.1 (+0.4) 43.8 (+0.0)\n\n43.8\n\n50.7\n\n83.2 84.1 (+0.9) 51.6 (+0.4) 44.2 (+0.0)\n\n44.2\n\n51.2\n\nD.3 PERFORMANCE GAIN OF USING INTERMEDIATE SUPERVISION\n\nIn this section, we evaluate the performance of RevCol-T/S/B with and without intermediate supervision on ImageNet-1K. We also evaluate the object detection task performance using 1× training schedule on MS-COCO dataset. Other settings remain the same. From the validation results in Tab. 9, models trained with intermediate supervision achieves 0.5% to 0.9% better top-1 accuracy. Besides, intermediate supervision also benefits down-stream tasks, which further demonstrates its effectiveness.\n\nTable 10: Performance of models with larger kernel convolution.\n\nKernel Size\n\nFLOPs\n\nTop-1 Acc\n\nAPbox 1×\n\nAPmask 1×\n\n3\n\n5\n\n7\n\n11\n\n4.5G\n\n4.5G\n\n4.6G\n\n4.6G\n\n82.2\n\n82.5\n\n82.5\n\n82.5\n\n48.8\n\n49.5\n\n49.3\n\n49.9\n\n42.2\n\n42.6\n\n42.4\n\n42.7\n\nFigure 6: GPU Memory Consumption vs. Model size\n\nD.4 GPU MEMORY CONSUMPTION VS MODEL SIZE\n\nFig. 6 plots the GPU memory consumption with the scaling of model size. We fix the computation complexity of a single column to 1G FLOPs and increase column number. Meanwhile, we measure the memory consumption in training process which includes the forward and backward propagation. Our experiments are conducted on Nvidia Tesla V100 GPU under batch-size 64, FP16 precision and PyTorch implementation. With the increment of column number, we can see RevCol keeps an O(1) GPU memory consumption, while non-reversible architecture’s memory consumption increase linearly with column number. Note that our RevCol does not keep strictly the same GPU memory consumption as column number increase, as reversible networks need to back-up the operation weights in need for calculating gradients and the re-construction of feature maps in backward propagation.\n\nD.5 ABLATION OF KERNEL SIZE IN CONVOLUTIONS\n\nIn original ConvNeXt, large kernel convolution achieves in better performance. We conduct experiments in RevCol-T. As shown in Tab. 10, for 4 column models, using 5 × 5 convolution increase the ImageNet-1k Top-1 accuracy by 0.3% and the COCO APbox by 0.7 for RevCol-T model. Further increasing kernel size obtains more accuracy in down-stream tasks, but not too much. We consider the RevCol design already enlarges the effective receptive field and this limit the accuracy gain of using large kernel convolution. On the other hand, 3 × 3 convolution enjoys the merits of efficiency and stability in (pre)training. Therefore, we adopt kernel 3 in all RevCol models.\n\n19\n\n02,0004,0006,0008,00010,00012,00014,00016,00018,00020,00012345678910GPUMemory (MB)# ColumnReversibleNon-ReversiblePublished as a conference paper at ICLR 2023\n\nE SEMI-LABELED PRIVATELY COLLECTED DATASET FOR LARGE MODELS\n\nE.1 DATA COLLECTION AND PSEUDO LABEL SYSTEM\n\nThe dataset consists of around 168 million(M) images, 50M of which labeled and the remaining 118M unlabeled. The majority of labeled images come from public datasets, e.g. ImageNet, Places365 (Zhou et al., 2017a), and Bamboo (Zhang et al., 2022b). The others are web-crawled images annotated by in-door employees. Unlabeled images come from weakly-annotated image-text datasets like YFCC-100M (Thomee et al., 2016). We do not use text annotations.\n\nIn order to utilize images of different label domains and the massive unlabeled images, we employ a multi-target label system similar to Ding et al. (2022a) and Ghiasi et al. (2021). We adopt a semisupervised learning strategy with ViTs, thus generating pseudo labels with continuously increased quality. We only store soft predictions with confidence higher than 1% to save storage. The final version of pseudo label we use are generated by a multi-head ViT-Huge teacher, which has an 89.0% ImageNet-1k accuracy.\n\nE.2\n\nIMAGE DEDUPLICATION\n\nSince the dataset contains large amount of unverified web-crawled images, there are probably validation or test images sneaking into our training dataset. Works like Mahajan et al. (2018) and Yalniz et al. (2019) all regard image deduplication an important procedure for fair experiments.\n\nWe first iterate over the entire dataset to filter out suspicious duplicates together with the corresponding test images based on their pseudo label distance. This brings more than 10,000 images with high suspicion. We look at these image pairs and finally find about 1,200 exact-duplicates and nearduplicates. Fig. 7 shows some examples of the near-duplicates, which are difficult to detect. Never the less, training a model without removing these duplicates gives less than 0.1% accuracy gain on ImageNet-1k in our experiments. We attribute this to the absence of true labels from these duplicates.\n\nFigure 7: Top: Near duplicates found in unlabeled images. Bottom: ImageNet-1k validation images.\n\nF MORE TRAINING DETAILS\n\nThis section gives more training details on ImageNet classification, COCO detection, and ADE20K segmentation.\n\nF.1\n\nINTERMEDIATE SUPERVISION SETTINGS\n\nWe add intermediate supervision in ImageNet-1k training, ImageNet-22k and extra data pre-training. We used a 3-block decoder with gradually up-sampled feature maps in ImageNet-1k training. The block setting remains the same as Sec. 2.2 We use a single layer decoder in ImageNet-22k and extra data pre-training. For all the variants of RevCol, we set the number of compound loss n to 3 empirically (eg. for a 8 column RevCol, the intermediate supervision is added to column 2, 4, and 6, and the original classification CE loss is also added to column 8). αi is set to 3, 2, 1, 0 and βi is set to 0.18, 0.35, 0.53, 1.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nF.2 HYPERPARAMETERS USED FOR TRAINING AND PRE-TRAINING\n\nThis section introduces the training details for main experiments, the supervised training on ImageNet and extra data. We show this setting in Tab. 11. All experiments in ablation studies are superivised trained on ImageNet-1K except additional descriptions and also follow settings described in this section.\n\nTable 11: Hyperparameters for training and pre-training RevCol.\n\nHyperparameters\n\nInput resolution Training epochs Warmup epochs Batch size Peak learning rate Learning rate schedule Layer-wise learning rate decay AdamW momentum Weight decay Gradient clipping Drop path EMA\n\nLabel smoothing ε Data augment Mixup CutMix Random erase\n\nImageNet-1K ImageNet-22K 168M Extra Data\n\nT/S/B\n\nB/L/XL\n\nXL/H\n\n300 20\n\n4e-3\n\n0.05\n\n2242\n\n4096\n\ncosine ✗\n(0.9, 0.999)\n\n✗\n\n0.1/0.3/0.4 0.9999\n\n90 5\n\n5e-4\n\n0.1\n\n0.3 ✗\n\n0.1 RandAug (9, 0.5) 0.8 1.0 0.25\n\n2242 10 0.15 5120 6.25e-4 cosine ✗\n(0.9, 0.999) 0.05 1.0 (element-wise) 0.2 ✗\n\n0.1 RandAug (9, 0.5) ✗\n✗ ✗\n\nF.3 HYPERPARAMETERS USED FOR FINE-TUNING\n\nThis section gives the hyperparameters used for fine-tuning on ImageNet-1K and downstrea COCO object detection and instance segmentation, ADE20K semantic segmentation tasks, as shown in Tab. 12, Tab. 13 and Tab. 14.\n\nTable 12: Hyperparameters for fine-tuning RevCol on ImageNet-1K classification.\n\nHyperparameters\n\nInput resolution Fine-tuning epochs Warmup epochs Batch size Peak learning rate Layer-wise learning rate decay AdamW momentum Weight decay Learning rate schedule Head init scale Drop path EMA Gradient clipping\n\nLabel smoothing ε Data augment Mixup CutMix Random erase\n\nImageNet-1K\n\nB/L/XL/H\n\n3842/3842/3842/6402 30 0\n512 5e-5 0.9/0.8/0.8/0.8 (0.9, 0.999) 1e-8 cosine 0.001 0.2/0.3/0.4/0.5 ✗/✗/✗/0.9999 10.0 (norm)\n\n0.1 RandAug (9, 0.5) ✗\n✗ 0.25\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nTable 13: Hyperparameters for fine-tuning RevCol on object detection with Cascade Mask R-CNN detector.\n\nHyperparameters\n\nFine-tuning epochs Batch size Peak learning rate Warmup steps Layer-wise learning rate decay\n\nAdamW momentum Weight decay Drop path\n\nIN-1K Pre-trained\n\nIN-22K Pre-trained\n\nRevCol-T/S/B\n\nRevCol-B/L\n\n36 16\n\n1500\n\n1e-4\n\n0.9/0.8\n\n2e-4\n\n0.85/0.8/0.8\n\n(0.9, 0.999) 0.05\n\n0.3/0.4/0.4\n\n0.5/0.6\n\nTable 14: Hyperparameters for fine-tuning RevCol on ADE20K semantic segmentation with UperNet segmentation framework.\n\nHyperparameters\n\nInput resolution Fine-tuning steps Batch size Peak learning rate Warmup steps Layer-wise learning rate decay\n\nAdamW momentum Weight decay Drop path\n\nIN-1K Pre-trained\n\nIN-22K Pre-trained\n\nRevCol-T/S/B\n\nRevCol-B/L\n\n5122\n\n6402\n\n80k 16 4e-5 1500\n\n1.0\n\n0.9\n\n(0.9, 0.999) 0.01 0.3\n\nF.3.1 CONVOLUTION KERNEL PADDING TRICK IN DOWN-STREAM TASKS\n\nAccording the results shown in Section D.5, larger kernel convolution perform better especially in down-stream tasks. To save the pre-training cost meanwhile achieve better performance, we pad the small 3 × 3 convolution kernel in pre-trained model weights to larger size then fine-tune in detection and segmentation tasks. Inspired by Net2net (Chen et al., 2015) method, we pad the pre-trained 3 × 3 kernel in convolution layer with Gaussian initialized values. To protect the pre-trained kernel from being disturbed by the new padded values, we initialize the padded values with 0 mean and extremely small standard deviations (1e-7). We use this trick only with our largest model RevCol-H. We pad the 3 × 3 kernel in pre-trained model to 7 × 7 kernel size in COCO detection task and 13 × 13 in ADE20k sementatic segmentation task, then fine-tune on corresponding dataset to get the final result. In general, the kernel padding trick leads to 0.5∼0.8 APbox improvement and 0.7∼1.0 mIoU improvement for RevCol-H model.\n\nG VISUALIZATIONS OF FEATURE DISENTANGLEMENT\n\nIn this section, we show our RevCol can disentangle features with stacked columns, which is different from the conventional sequential networks. We use RevCol-S pre-trained on ImageNet-1K for analysis. First, we visualize the class activation maps (CAMs) for outputs of each last layer of a level. We adopt LayerCAM (Jiang et al., 2021) technology to generate the CAMs with the predicted classes. Fig. 8 show the heatmaps of activation. With the levels and columns going deeper, the features focus on the regions with more semantics. The outputs of RevCol-S are the different levels of last column. These features with high level semantics focus on different parts of the image and the whole part of the object, achieving disentanglement of features for task-relevant and task-irrelevant.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: Visualizations of class activation maps using LayerCAM (Jiang et al., 2021) for different levels and columns.\n\nFigure 9: CKA similarities (Kornblith et al., 2019) of features and images/labels for different levels and columns.\n\nTo quantify the disentanglement, we use Centered Kernel Alignment (CKA) similarity metric (Kornblith et al., 2019) to measure the similarity between representations in RevCol-S. We calculate the CKA similarities between intermediate features in different levels and columns and images or labels of each category in the ImageNet val set. Then we plot the similarities of the category with\n\n23\n\nLevel 1Level 2Level 3Level 4Column 1Column 2Column 3Column 4Column 5Column 6Column 7Column 8Level 1Level 2Level 3Level 4Column 1Column 2Column 3Column 4Column 5Column 6Column 7Column 8Level 1Level 2Level 3Level 4Level 1Level 2Level 3Level 4Col 1Col 2Col 3Col 4Col 5Col 6Col 7Col 8Col 1Col 2Col 3Col 4Col 5Col 6Col 7Col 8Published as a conference paper at ICLR 2023\n\nthe highest label similarity in Fig. 9. As shown in the figure, the similarities between images and intermediate features are not clearly distinguished at different levels in Column 2-5, while the features with higher levels have lower similarity to the images in Column 6-8. The similarities between labels and intermediate features are also more distinct in higher columns.\n\n24",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a new neural network paradigm, which aims at gradually disentangling features in the forward propagation. The whole network is constructed with multiple copies of sub-networks. The total information can be maintained rather than compressed during propagation. The proposed method is also evaluated on several typical computer vision tasks and shows competitive performance.\n\n# Strength And Weaknesses\n\n**Strength**  \n* It is important to explore strong and disentangled representations by designing new neural architectures. The proposed reversible column manner is reasonable to maintain both high- and low-level information.\n* The proposed method shows promising experimental performance.\n\n**Weaknesses**\n* It is expected to explain the intrinsic difference between the proposed method and previous ones, including but not limited to HRNet and NAS (neural architecture searched) networks. Especially, networks [1,2] searched in the cell-based/-like search space have many in common with the proposed one. A more comprehensive analysis is suggested, not only the provided demonstration from the motivation perspective.\n* The shown experimental performance is competitive but the promotion over previous ones is not so evident. I admit the number is almost saturated, but any other advantage this method could bring needs to be demonstrated.\n* Real throughput/latency needs to be measured to more accurately validate the model budget, not just FLOPs or #Params. The introduced connections seem to introduce larger latency on real hardware which is not so related to FLOPs numbers.\n\n[1] Liu C, Chen L C, Schroff F, et al. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 82-92.   \n[2] Xie S, Kirillov A, Girshick R, et al. Exploring randomly wired neural networks for image recognition[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 1284-1293.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n* **Clarity**: most contents are clear and easy to follow.\n* **Quality**: The proposed method is evaluated and studied on substantial benchmarks and settings.\n* **Novelty**: The design principles have some in common with previous methods, but some details are new.\n* **Reproducibility**: Most implementation details are provided but the code is not available.\n\n# Summary Of The Review\n\nThe overall idea is good while the main concerns lie in the demonstration of advantages or differences with previous methods and speed analysis.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper introduces the Reversible Column Network (RevCol), a novel neural network architecture designed to enhance feature learning through reversible connections among subnetworks, termed columns. The methodology emphasizes disentangling features while retaining comprehensive task-relevant information, inspired by biological systems. Key contributions include a Reversible Multi-Level Fusion Module, memory-efficient operations, and scalability through the flexible integration of columns. Experimental results demonstrate RevCol's competitive performance across various computer vision tasks, achieving state-of-the-art results in image classification, object detection, and semantic segmentation.\n\n# Strength And Weaknesses\nThe strengths of RevCol lie in its innovative approach to feature disentanglement and the effective use of reversible connections, which address memory limitations and improve scalability. The architecture's ability to integrate with existing models (e.g., ConvNeXt, ViT) is a significant advantage, allowing for widespread applicability. However, the paper could have benefited from a more detailed discussion on the limitations of the current architecture and potential challenges in implementation. Additionally, while the paper presents strong empirical results, a more extensive comparison with a broader range of architectures could solidify its claims of superiority.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations behind the RevCol design, along with its theoretical foundations. The quality of the writing is high, with clear explanations of the methodology and experimental setup. The novelty of the approach is notable, particularly in the context of reversible networks and disentangled representation learning. The authors have made their code and models publicly available, enhancing reproducibility, although additional details on hyperparameters and training protocols would further assist researchers in replicating the results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to neural network design through the introduction of RevCol, which effectively enhances feature disentanglement while maintaining memory efficiency. The empirical results are promising, demonstrating the architecture's versatility and performance across various tasks in computer vision.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents Reversible Column Networks (RevCol), a novel architecture designed for lossless information propagation and disentangled feature learning in deep learning applications. RevCol is constructed from multiple subnetworks (columns) that utilize reversible multi-level fusion units, allowing for flexible feature processing without the constraints of traditional reversible networks. The authors demonstrate the effectiveness of RevCol through extensive experiments, achieving state-of-the-art results in image classification, object detection, and semantic segmentation tasks across multiple benchmarks such as ImageNet-1K, COCO, and ADE20K. The architecture shows strong scalability with increasing data volume and model size.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include its innovative approach to maintaining information integrity through lossless propagation, resulting in high performance across various benchmarks. The architecture's flexibility allows it to adapt to multiple tasks beyond vision, and its memory efficiency provides advantages for large model training. However, the complexity of the design may complicate the training process. Additionally, the architecture's dependence on large datasets could limit its utility in scenarios with limited data. Sensitivity to hyperparameter tuning and a lack of comprehensive comparisons with a broader range of contemporary architectures are also notable weaknesses.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The novelty of the RevCol architecture is significant, introducing a new paradigm in reversible networks. The experimental design is robust, utilizing substantial datasets and various model variants, which enhances reproducibility. However, the complexity of the architecture may pose challenges for replication without careful attention to hyperparameter settings.\n\n# Summary Of The Review\nOverall, the paper introduces a highly innovative and effective architecture in RevCol, demonstrating strong performance across multiple tasks while maintaining information integrity. While the complexity and data dependency present challenges, the contributions to the field are substantial and warrant attention.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces the **Reversible Column Network (RevCol)**, a novel neural network architecture designed to disentangle features during forward propagation while preserving total information. The methodology involves the utilization of multi-level reversible transformations that allow for the gradual separation of task-relevant concepts across multiple subnetworks (columns). The findings demonstrate that RevCol achieves competitive performance across various computer vision tasks, including image classification, object detection, and semantic segmentation, with notable results such as 90.0% accuracy on ImageNet-1K for RevCol-H and 63.8% APbox on COCO. The architecture's flexibility is highlighted by its compatibility with existing frameworks, including transformers, and the availability of code and models for reproducibility.\n\n# Strengths And Weaknesses\nStrengths of the paper include its innovative approach to feature disentanglement, which counters the common pitfalls of over-compression observed in traditional deep learning architectures. The empirical results are impressive, with RevCol outpacing existing models in accuracy and performance across multiple datasets. However, the paper could benefit from a more comprehensive analysis of the computational complexity and memory consumption associated with the reversible operations, as understanding these factors is crucial for practical implementation. Additionally, while the theoretical underpinnings of the reversible transformations are well-articulated, further empirical validation of the proposed benefits would strengthen the argument.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The use of figures to illustrate complex concepts, such as the architecture and its reversible connections, aids in comprehension. The novelty of the approach lies in its unique feature disentanglement strategy and the integration of reversible connections, which distinguishes it from existing models. The authors have provided code and models, enhancing the reproducibility of their results, which is commendable. However, additional details regarding hyperparameter tuning and training configurations would further facilitate reproducibility by other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to neural network design through the Reversible Column Network. Its contributions to feature disentanglement and the preservation of information are significant, with strong empirical validation across various tasks. Despite minor weaknesses in computational analysis, the paper adds valuable insights to the field of deep learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Reversible Column Network (RevCol), a novel neural network architecture designed with multi-level reversible connections that aim to maintain comprehensive information flow while achieving competitive performance in various computer vision tasks. The authors demonstrate RevCol's state-of-the-art results on well-known datasets such as ImageNet, COCO, and ADE20K, highlighting its memory efficiency during training and scalability with larger models. Additionally, the paper explores the benefits of implementing intermediate supervision and the architecture's potential for disentangled representations, which enhances its applicability across different tasks.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative architecture that promises significant memory savings and scalable performance. The competitive results across multiple benchmarks underscore the utility of RevCol in advancing current methodologies. However, the complexity associated with the design may hinder reproducibility and practical implementation for those less familiar with advanced architectures. Furthermore, while the empirical results are compelling, the lack of a strong theoretical foundation for the architecture's performance could limit its broader acceptance. The focus on visual tasks may also restrict the exploration of RevCol's effectiveness in other domains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and results. However, the complexity of the RevCol architecture could pose challenges for reproducibility, particularly for practitioners without extensive experience in advanced neural network designs. The novelty of the architecture is significant, as it introduces new concepts in neural network design, although the empirical findings require further theoretical backing to fully understand the underlying dynamics. Overall, while the quality of the work is high, reproducibility may be a concern due to the intricate nature of the proposed architecture.\n\n# Summary Of The Review\nIn summary, the paper presents an innovative architecture that achieves competitive results in computer vision tasks while addressing memory efficiency and scalability. However, the complexity of implementation and limited exploration of non-visual tasks may hinder broader applicability and reproducibility. The findings are promising but would benefit from stronger theoretical underpinnings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents **Modular Disentangled Network (MoDiNet)**, a novel architecture designed to enhance feature disentanglement and information flow in neural networks. By employing a columnar structure of modular subnetworks with reversible connections, MoDiNet allows for more efficient feature representation and generalization across various tasks. The authors assert that their model achieves state-of-the-art results in critical computer vision benchmarks, including image classification, object detection, and semantic segmentation, with notable scalability and versatility for integration into existing frameworks.\n\n# Strength And Weaknesses\nThe paper's primary strengths include its innovative modular design, which effectively preserves and disentangles features, and the introduction of reversible connections that significantly reduce memory usage during training. The empirical results are compelling, demonstrating clear advantages over established architectures in both performance and adaptability. However, the paper could benefit from a more in-depth discussion of the potential limitations of the modular architecture and the implications of its complexity on training dynamics. Additionally, while the empirical results are impressive, further exploration of the impact of hyperparameter tuning or specific architectural choices on performance could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology, experimental setup, and results. The writing quality is high, making it accessible to readers with varying levels of expertise. The novelty is significant, as the combination of modular architectures and reversible connections is relatively unexplored in the literature. The authors' commitment to releasing their code and models supports reproducibility, which is a crucial aspect of contemporary research.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in neural network architecture through the MoDiNet design, effectively combining modularity and reversible connections to improve feature disentanglement and performance. While the contributions are substantial, a deeper discussion of limitations and further empirical analysis could enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel adversarial training approach named Reversible Column Networks (RevCol), which aims to enhance the robustness of neural networks against adversarial attacks through a unique multi-column architecture featuring reversible connections. This architecture facilitates the gradual disentangling of features while preserving the integrity of input data, contrasting with traditional methods that often compress information during training. The authors provide empirical evidence of RevCol's effectiveness, demonstrating competitive performance on several benchmarks, indicating its potential to improve both adversarial robustness and standard task accuracy.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Architecture**: The multi-column structure with reversible connections is a novel contribution to the field of adversarial training, allowing for a more sophisticated understanding of features and enhancing resistance to adversarial perturbations.\n2. **Information Preservation**: RevCol maintains a comprehensive representation of input data, which may lead to improved generalization and robustness in real-world applications.\n3. **Empirical Performance**: The architecture exhibits competitive results across various benchmarks, suggesting its dual benefits of enhanced adversarial robustness and improved accuracy.\n4. **Scalability**: The design scales effectively with both model size and dataset size, showing significant improvements in performance with larger models and more training data.\n\n**Weaknesses:**\n1. **Increased Complexity**: The introduction of a multi-column architecture adds complexity to model design and training, potentially requiring more computational resources than traditional methods.\n2. **Limited Comparative Analysis**: The paper lacks a comprehensive comparison with state-of-the-art adversarial training techniques, which could strengthen the argument for RevCol's effectiveness.\n3. **Insufficient Theoretical Insights**: A deeper theoretical exploration of how reversible connections contribute to adversarial robustness is needed to provide clarity on the architecture's advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear descriptions of the methodology and findings. The innovative nature of the RevCol architecture stands out, contributing to its novelty. However, the complexity of the model may pose challenges for reproducibility, particularly for those unfamiliar with the intricacies of multi-column architectures. The empirical results are promising, but the lack of extensive comparative analysis and theoretical justification may hinder the overall clarity regarding its advantages over existing methods.\n\n# Summary Of The Review\nThe paper introduces a promising new approach to adversarial training with Reversible Column Networks, showcasing an innovative architecture that maintains information flow and demonstrates competitive empirical results. However, the complexity of the model and the need for a more comprehensive comparative analysis with existing techniques are notable concerns that should be addressed for broader acceptance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Reversible Column Networks (RevCol), a novel neural network architecture that emphasizes feature learning through reversible columns. The architecture comprises multiple identical subnetworks interconnected via multi-level reversible connections, claiming to preserve information without compression. The authors report remarkable performance metrics, achieving 90.0% accuracy on ImageNet-1K and leading results on COCO detection and ADE20K segmentation tasks. The paper also discusses the potential for disentangled representations, training efficiency through memory savings, and scalability to larger datasets, while proposing a new intermediate supervision technique. The authors assert RevCol's superiority over existing models, positioning it as a transformative advancement in deep learning.\n\n# Strength And Weaknesses\nThe main strength of this work is its innovative approach to reversible architectures, which allows for better information retention and potentially improved performance in various tasks. The reported accuracy on ImageNet-1K and other benchmarks is impressive, suggesting that RevCol could set new standards in computer vision. However, the claims regarding its superiority over existing architectures like GLOM and RevNets seem exaggerated; the enhancements may not be as revolutionary as suggested, and the importance of the intermediate supervision technique could be overstated. Additionally, the generalization of RevCol to other domains, such as NLP, lacks sufficient empirical backing.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making it accessible to readers in the field. The methodology is presented clearly, with sufficient detail for reproducibility. However, some claims about the novel features of RevCol, particularly regarding its performance and efficiency benefits, could benefit from more rigorous empirical validation. The novelty of the proposed architecture is significant but may not fully meet the bar for groundbreaking contributions, given the incremental nature of improvements compared to existing architectures.\n\n# Summary Of The Review\nRevCol presents an interesting advancement in neural network design with its reversible columns and impressive performance metrics. While the contributions are notable, some claims regarding its revolutionary nature and superiority over prior work may be overstated, necessitating a more cautious interpretation of its impact on the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Reversible Column Networks (RevCol), a novel architectural framework designed to enhance feature propagation while maintaining information integrity. The authors propose a structure composed of multiple subnetworks (columns) interconnected through multi-level reversible connections, aiming to learn disentangled representations as opposed to adhering to the traditional Information Bottleneck principle. Experimental results demonstrate that RevCol achieves competitive performance across various vision tasks, including image classification on ImageNet-1K, object detection on COCO, and semantic segmentation on ADE20K, with reported accuracy figures indicating its effectiveness in large-scale settings.\n\n# Strength And Weaknesses\nRevCol's primary strength lies in its innovative architecture that facilitates effective feature disentangling, potentially leading to better performance across a range of vision tasks. The methodology of using multi-level reversible connections is particularly noteworthy, as it addresses memory efficiency while ensuring robust feature propagation. However, the reported performance metrics show slight discrepancies from the original values, suggesting a need for cautious interpretation of its advantages. Additionally, the performance degradation observed with an excessive number of columns raises questions about scalability and optimal configuration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to readers with varying levels of expertise in the field. The quality of the experimental design and results appears high, with comprehensive comparisons to existing models. The novelty of the RevCol architecture is significant, offering a fresh perspective on feature disentangling in deep learning. However, reproducibility could be improved by providing more detailed information about implementation specifics and hyperparameter settings, which would aid other researchers in validating the findings.\n\n# Summary Of The Review\nRevCol presents a compelling advancement in neural network architecture that leverages reversible connections for enhanced feature disentangling, showcasing competitive performance across multiple vision tasks. While the innovative approach and experimental results are promising, minor discrepancies in reported metrics and the impact of column scaling warrant further investigation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper critiques the Information Bottleneck (IB) principle in neural network design, proposing a novel architecture called RevCol that emphasizes maintaining disentangled representations and utilizing reversible connections. The authors argue that these elements enhance feature learning and generalization across varied tasks in computer vision and natural language processing. Key findings suggest that RevCol achieves competitive performance with fewer parameters and FLOPs, although its performance is contingent on large datasets for pre-training.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its bold challenge to the IB principle and its innovative proposal of RevCol, which offers a new perspective on architectural design. The introduction of intermediate supervision as a means to counteract information collapse is a noteworthy contribution. However, weaknesses include a lack of empirical validation of the generalizability of the architecture across diverse tasks, potential implementation complexity, and the dependence on large datasets. The analysis of comparative performance is also insufficient, as it does not fully address potential weaknesses relative to existing models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written but could benefit from clearer explanations of complex concepts, particularly regarding reversible connections and disentangled representations. The novelty of challenging the IB principle is significant, yet the reproducibility of results may be questionable due to the reliance on intermediate supervision and large datasets. More detailed methodology and evaluation metrics would enhance the clarity and reproducibility of the findings.\n\n# Summary Of The Review\nThe paper presents a compelling critique of the Information Bottleneck principle and proposes a novel architecture that emphasizes disentangled representations and reversible connections. While the proposed model shows promise, it requires further empirical validation and exploration of its limitations in diverse scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces Reversible Column Networks (RevCol), a novel neural network architecture designed to enhance feature disentanglement through the use of multiple interconnected subnetworks (columns) linked via reversible connections. The methodology involves creating multi-level reversible units that facilitate lossless information propagation, thereby improving feature quality across layers. Experimental results demonstrate that RevCol achieves state-of-the-art performance in various computer vision tasks, including classification, object detection, and segmentation, particularly excelling with larger models and datasets.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to addressing the limitations of traditional networks under the Information Bottleneck principle, offering a compelling alternative that focuses on disentangled representations. The architecture's design effectively allows for efficient information flow, which is well-supported by empirical results showcasing its competitive performance against established models. However, one potential weakness is the complexity introduced by the multi-level reversible units, which may pose challenges in terms of implementation and understanding for practitioners less familiar with reversible networks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings, making it accessible to readers. The quality of the writing is high, and the figures and tables effectively illustrate key concepts and results. In terms of novelty, RevCol presents a significant advancement in the architecture of neural networks, particularly in disentangled representation learning. The reproducibility of the results is enhanced by detailed descriptions of the architecture and experimental setups, although the complexity of the network may still require careful consideration during replication.\n\n# Summary Of The Review\nOverall, the paper presents a novel and well-executed approach to improving neural network performance through reversible connections and disentangled representations. While it demonstrates significant empirical advancements, the architectural complexity may present challenges for broader adoption.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework for enhancing feature extraction in deep learning models, particularly focusing on the integration of attention mechanisms with convolutional neural networks (CNNs). The authors propose a hybrid architecture that combines traditional CNNs with adaptive attention layers to improve the model's ability to focus on relevant features in data. Empirical results demonstrate that this approach outperforms existing state-of-the-art models on several benchmark datasets, indicating its effectiveness in various machine learning tasks.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by addressing the limitations of conventional CNN architectures in feature representation through the innovative use of attention mechanisms. The proposed framework is flexible and can be adapted to different types of data, which is a considerable advantage. However, the paper could benefit from a more comprehensive discussion of the computational trade-offs associated with the added complexity of the attention layers. Additionally, while the empirical results are promising, further validation across a wider range of datasets would strengthen the claims of generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe writing is generally clear and well-structured, allowing for easy comprehension of the main ideas and methodology. The use of figures and tables effectively supports the textual content, although some figures would benefit from clearer legends and explanations. The novelty of integrating attention mechanisms with CNNs is commendable, providing a fresh perspective on feature extraction. The methodology is presented in a reproducible manner, with sufficient details provided for implementation, although more explicit discussions on the experimental setup would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the application of attention mechanisms within CNNs, demonstrating both theoretical and practical significance. While the contributions are noteworthy, the authors could improve the paper by addressing the computational implications of their approach and expanding the scope of empirical validation. The work is recommended for acceptance with minor revisions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Reversible Column Networks (RevCol), a novel neural network architecture that incorporates multiple subnetworks (columns) with multi-level reversible connections. Unlike traditional neural networks, which often compress information through the Information Bottleneck principle, RevCol aims to preserve total information and disentangle features during forward propagation. The authors demonstrate that RevCol achieves competitive performance in various computer vision tasks, including image classification, object detection, and semantic segmentation, particularly when applied to large datasets. Additionally, the RevCol architecture's flexibility allows for integration with transformers and other neural networks, enhancing performance across both vision and natural language processing (NLP) tasks.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to feature learning, which mitigates the drawbacks associated with information compression in traditional networks. The ability to maintain lossless information and disentangle features may provide significant advantages in complex tasks requiring detailed representation. Furthermore, the empirical results indicate that RevCol performs competitively in multiple domains, showcasing its versatility. However, a potential weakness lies in evaluating the scalability and computational efficiency of the architecture, which may be a concern for practical implementations. Additionally, the paper could benefit from more extensive comparisons with state-of-the-art architectures to fully contextualize its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, presenting a clear outline of the proposed architecture and its theoretical underpinnings. The methodology is described in a manner that facilitates understanding, allowing readers to grasp the novel aspects of RevCol. The quality of the experimental results supports the claims made regarding the architecture's performance. However, the reproducibility of the findings would be enhanced by providing detailed information about the datasets, hyperparameters, and training protocols used in the experiments.\n\n# Summary Of The Review\nOverall, RevCol presents a compelling advancement in neural network design by enabling lossless feature propagation and disentangling information, which could lead to improved performance in various applications. While the architecture shows promise and achieves competitive results, further investigation into its scalability and more exhaustive comparisons with existing models would strengthen the paper's contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Reversible Column Networks (RevCol), a novel neural network paradigm designed to enhance feature learning by maintaining information through reversible connections among multiple subnetworks, termed columns. By leveraging a multi-level reversible unit, RevCol facilitates disentangled feature learning and effectively scales with larger datasets and models. The authors demonstrate the effectiveness of RevCol across various computer vision tasks, achieving state-of-the-art performance on datasets such as ImageNet, COCO, and ADE20K, while providing code and models for reproducibility.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to feature learning, emphasizing information retention through reversible transformations. This methodology not only enhances performance on standard benchmarks but also addresses the challenges associated with disentangled feature representation. However, the paper could benefit from a more in-depth exploration of the theoretical implications of reversible architectures and their limitations. Additionally, while the results are impressive, the experiments could be expanded to include comparisons with a broader range of existing architectures to better contextualize the advantages of RevCol.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear structure that effectively communicates the proposed methodology and results. The quality of the experiments is high, with comprehensive evaluations across multiple tasks, although more extensive comparisons could improve the overall robustness of the findings. The novelty of RevCol is significant, introducing a new architectural paradigm that could influence future research in neural networks. The authors have made their code and models publicly available on GitHub, enhancing reproducibility and encouraging further exploration of their work by the community.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to neural network design through its Reversible Column Networks, achieving state-of-the-art results in several tasks while prioritizing information retention and feature disentangling. While the contributions are noteworthy, further exploration of theoretical implications and broader experimental comparisons could strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel neural architecture called the Reversible Column Network (RevCol), which is designed to enhance feature disentanglement while maintaining information propagation in deep learning models. The RevCol architecture comprises multiple subnetworks (columns) with reversible connections, allowing for improved performance across various tasks, including image classification, object detection, and semantic segmentation. The authors report that RevCol achieves state-of-the-art accuracy for pure convolutional neural network (CNN) models on benchmarks such as COCO and ADE20K, emphasizing its robust performance and scalability.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to neural architecture design, specifically the focus on reversible connections and feature disentanglement, which addresses limitations of traditional information compression methods. The comprehensive experimental evaluation showcases the effectiveness of RevCol against numerous state-of-the-art models, providing a strong validation of its claims. However, a potential weakness is the complexity of the architecture, which may pose challenges for practical implementation and reproducibility. Additionally, while the paper discusses various configurations, further exploration of the model's applicability beyond computer vision tasks could enhance its impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear sections that guide the reader through the introduction of the RevCol architecture, its methodology, and experimental results. The mathematical formulations and detailed explanations contribute to the technical quality of the work. The novelty of the proposed architecture is significant, as it introduces a new paradigm in neural network design. However, the reproducibility of the results may be hindered by the complexity of the model and the need for extensive experimentation details, which the authors should further clarify.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in neural architecture through the introduction of the Reversible Column Network, demonstrating strong empirical performance across multiple tasks. While the paper is well-written and offers significant contributions, attention to reproducibility and practical implementation aspects should be considered.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Reversible Column Network (RevCol), a novel architecture designed to maintain total information integrity during forward propagation through the use of multiple identical subnetworks called columns. This architecture allows for gradual disentanglement of feature representations, improving the learning of task-relevant information. Empirical evaluations demonstrate that RevCol achieves competitive performance on various tasks, notably reaching an accuracy of 88.2% on ImageNet-1K following pre-training on ImageNet-22K, and 90.0% with additional data. The versatility of RevCol is further established through its application in transformers and natural language processing tasks.\n\n# Strength And Weaknesses\nThe primary strength of this work lies in its innovative architectural design that prioritizes information preservation, contrasting with traditional models that may suffer from information bottlenecks. The introduction of reversible transformations and hierarchical feature extraction enhances model robustness across multiple tasks. However, a potential weakness is the complexity of the architecture, which may pose challenges in terms of implementation and understanding for practitioners unfamiliar with reversible networks. Additionally, while the empirical results are strong, further exploration into the scalability and efficiency of RevCol under diverse conditions would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations, methodologies, and results. The mathematical formulations provided lend credibility to the architectural innovations presented. The novelty of the RevCol framework is significant, particularly in its approach to feature disentanglement and information preservation. Reproducibility appears to be supported through comprehensive details about model configurations and experimental setups, although supplementary materials or code availability would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in neural network architecture through the introduction of RevCol, effectively addressing information preservation and feature disentanglement. While the contributions are significant and well-justified through empirical results, the complexity of the architecture may hinder broader adoption without further simplification or clarification.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel architectural framework called Reversible Column Networks (RevCol), which claims to improve information retention during feature extraction through a multi-level architecture with reversible connections. The authors propose that this framework can enhance performance while reducing memory usage by avoiding information compression. They employ a method of intermediate supervision in training and evaluate RevCol against existing models using metrics such as Average Precision (APbox) and mean Intersection over Union (mIoU). The results indicate competitive performance on various benchmarks, although the authors do not convincingly establish that these results significantly exceed those of existing architectures.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to reversible connections and the introduction of intermediate supervision, which aims to maintain information flow. However, the complexity of the RevCol architecture may hinder clarity and practical implementation. The reliance on multiple identical subnetworks raises concerns about redundancy and inefficiency, potentially complicating the training process without substantial performance gains. Furthermore, the paper fails to thoroughly justify the selection of performance metrics and lacks comprehensive comparisons with a wider range of models. The absence of ablation studies leaves questions about the contributions of individual components and the overall robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel architecture, the clarity of the proposed methodology is obscured by its complexity. The authors do not effectively communicate how their concepts translate into practical advantages, such as \"disentangled representations.\" The quality of the empirical results is somewhat undermined by a selective presentation of comparisons and the lack of detailed ablation studies, which affects reproducibility. Additionally, the claims regarding memory savings and compatibility with other architectures lack sufficient empirical backing, further complicating the evaluation of their contributions.\n\n# Summary Of The Review\nOverall, the paper presents a potentially interesting approach to feature extraction through Reversible Column Networks. However, the complexity of the architecture, coupled with insufficient empirical validation and clarity in its claims, raises concerns about its practical applicability and the significance of its contributions. The lack of comprehensive comparisons and ablation studies further detracts from the robustness of the findings.\n\n# Correctness\nRating: 3\n\n# Technical Novelty And Significance\nRating: 3\n\n# Empirical Novelty And Significance\nRating: 2",
    "# Summary Of The Paper\nThe paper introduces RevCol, an innovative neural network architecture that utilizes multiple subnetworks (columns) with multi-level reversible connections to enhance feature disentanglement. The methodology leverages scalability and memory efficiency, showcasing competitive performance across various computer vision tasks, including image classification, object detection, and semantic segmentation. Notably, RevCol achieves state-of-the-art results on benchmark datasets such as ImageNet, COCO, and ADE20K, while also promoting improved training convergence through intermediate supervision.\n\n# Strength And Weaknesses\nRevCol's primary strength lies in its groundbreaking design, which not only enhances performance but also supports scalability and memory efficiency. The ability to integrate with transformers and other neural networks broadens its applicability across domains, making it a versatile tool for researchers. However, the paper could benefit from more detailed analysis on the computational costs associated with training these large models and potential limitations of the architecture in various contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the innovations introduced by RevCol, making the contributions easy to understand for both practitioners and researchers. The methodology is presented in a structured manner, enhancing the overall quality of the work. The novelty of RevCol is significant, as it proposes a unique approach to neural network design. Furthermore, the authors have made their code and models available on GitHub, promoting reproducibility and encouraging further exploration of their work.\n\n# Summary Of The Review\nRevCol represents a substantial advancement in neural network architecture, combining innovative design with practical performance improvements. Its versatility and memory efficiency make it a promising tool for various applications in deep learning. Overall, this paper is a noteworthy contribution to the field and has the potential to influence future research directions.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces Reversible Column Networks (RevCol), a novel neural network architecture designed to facilitate disentangled feature learning while preserving information integrity throughout the network. The methodology leverages reversible transformations and multi-level reversible units to allow lossless information propagation, enhancing both memory efficiency and the retention of low- and high-level features. Key findings indicate that RevCol outperforms traditional architectures in terms of memory consumption and the ability to maintain relevant information across layers, offering a significant theoretical advancement in deep learning design.\n\n# Strength And Weaknesses\nRevCol's strengths lie in its innovative approach to reversible transformations and the emphasis on disentangled feature learning, setting it apart from existing architectures such as GLOM and HRNets. The memory efficiency achieved through O(1) consumption during training is particularly noteworthy, making it suitable for diverse computational environments. However, the paper could benefit from empirical validation across a wider range of tasks to fully demonstrate the advantages of its unique design. Additionally, while the theoretical framework is compelling, practical implementation challenges and the impact of intermediate supervision on various datasets remain underexplored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making complex concepts accessible to the reader. The quality of the writing is high, with a coherent narrative that effectively communicates the significance of the proposed architecture. The novelty of the work is substantial, introducing a paradigm shift in feature learning, but reproducibility may be a concern if specific implementation details are not sufficiently detailed in future work. The reliance on theoretical frameworks without extensive empirical validation could also hinder reproducibility.\n\n# Summary Of The Review\nRevCol presents a significant advancement in neural network architecture by focusing on disentangled feature learning through reversible transformations, leading to enhanced memory efficiency and information retention. While the theoretical contributions are robust, further empirical validation is needed to confirm the advantages of this approach across diverse tasks.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces Reversible Column Networks (RevCol), a novel network architecture designed to enhance feature propagation and reduce memory consumption during training. The methodology incorporates multiple identical subnetworks, referred to as columns, which leverage reversible connections for effective feature disentanglement. The design includes a multi-level reversible unit that facilitates the propagation of low-level and high-level features, and the network architecture can be implemented using popular frameworks like ConvNeXt or transformers. The findings suggest that RevCol achieves improved performance metrics while maintaining lower memory requirements compared to traditional architectures, demonstrating scalability with increased dataset size and model complexity.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative use of reversible connections to enhance feature propagation without loss of information, as well as its focus on memory efficiency during training. The methodology is detailed and provides clear implementation insights, making it reproducible. However, the paper lacks a broader discussion on the implications of the results and their significance within the context of existing literature. While the architecture shows promise, the evaluation of results is somewhat superficial, focusing more on implementation rather than establishing a strong comparative significance against state-of-the-art models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly, making it accessible for readers who may want to replicate the work. The quality of the writing is generally high, with sufficient technical detail to understand the architecture and its components. The novelty lies primarily in the reversible connection design; however, the paper could benefit from more robust empirical validation and exploration of the broader significance of its findings. The reproducibility of the results is supported by the availability of code and models, which is a positive aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling architecture in the form of RevCol, showcasing innovative design choices that address memory efficiency and feature disentanglement. However, the lack of deeper empirical analysis and broader contextualization of the results limits its impact. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces Reversible Column Networks (RevCol) as a novel architecture aimed at improving feature propagation and efficiency in convolutional neural networks (CNNs). The methodology emphasizes reversible connections to enhance information retention during propagation. The authors claim that RevCol achieves competitive performance across various tasks, asserting that it can scale effectively to larger models and datasets. However, the paper lacks comprehensive comparisons with established architectures, leading to questions about the significance of its reported findings.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of reversible connections, which is a relevant and interesting concept in the context of CNNs. However, the paper fails to address prior works that have explored similar ideas, such as GLOM and RevNets, which diminishes the novelty of RevCol. Additionally, while the authors claim competitive performance, the results are not sufficiently contextualized against strong existing baselines, which raises concerns about the validity of their assertions. The mention of intermediate supervision as a performance enhancer is not substantiated with thorough comparisons, further weakening the impact of their findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by vague assertions and insufficient comparisons with other models. The methodology is presented in a manner that is understandable, but the lack of rigorous evaluation against state-of-the-art architectures undermines the overall quality of the work. The novelty is questionable, given the lack of direct engagement with existing literature on reversible networks. As for reproducibility, the paper does not provide enough detail on experiment setups and comparisons, which may hinder other researchers from validating the results.\n\n# Summary Of The Review\nOverall, while RevCol presents an intriguing approach to feature propagation in CNNs, the lack of thorough comparisons with existing architectures and the failure to engage with prior work significantly undermine its contributions. The paper's claims of superiority and innovation are not convincingly supported, leading to a cautious assessment of its findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents \"REVERSIBLE COLUMN NETWORKS,\" a novel architecture designed to enhance the performance of Convolutional Neural Networks (CNNs) for various downstream tasks. The authors introduce the RevCol architecture, which incorporates a reversible design to facilitate efficient training and inference while minimizing memory usage. The methodology includes extensive experiments on benchmark datasets, demonstrating that RevCol outperforms existing models in terms of accuracy and resource efficiency, particularly in low-resource settings.\n\n# Strength And Weaknesses\nThe main strengths of the paper are its innovative approach to network architecture and the thorough empirical evaluation conducted across multiple datasets. The reversible design is well-explained and offers significant advantages in terms of efficiency. However, the paper could benefit from a more detailed discussion on the limitations of the proposed architecture and its applicability to real-world scenarios. Additionally, while the results are promising, further analysis on the model's behavior in edge cases would enhance the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical flow of ideas. However, some terminological inconsistencies (e.g., \"down-stream tasks\" vs. \"downstream tasks\") and formatting issues (e.g., inconsistent citation styles) detract from its overall clarity. The novelty of the proposed architecture is evident, and the empirical results support the claims made by the authors. While the methodology is reproducible, clearer definitions and consistent formatting would improve accessibility for readers attempting to replicate the experiments.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of neural network architectures through its reversible design, demonstrating potential for improved efficiency in CNNs. While the findings are promising, addressing minor clarity and formatting issues could enhance the paper's impact and readability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel architecture called RevCol, which focuses on disentangled representations and employs reversible connections to improve memory efficiency. The authors present promising results on standard benchmark datasets, showcasing RevCol's potential for various downstream tasks. However, the paper does not thoroughly explore the implications of applying RevCol to non-vision tasks, such as audio processing, and lacks a comprehensive discussion on the challenges of scalability and training on distributed systems.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to disentangled representations and the potential memory-saving advantages of reversible connections. However, it has several weaknesses. The authors fail to address how RevCol could be adapted for complex multimodal tasks or the implications of using it with different transformer architectures. Additionally, the exploration of intermediate supervision is limited, and the paper lacks a thorough examination of RevCol's limitations in edge cases or specific datasets. The breadth of experiments is also restricted, focusing primarily on standard benchmarks without considering noisy or less structured data.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its findings clearly. However, some sections, especially those discussing the integration of RevCol with transformer architectures, are underdeveloped and lack depth. While the proposed architecture exhibits technical novelty, the overall reproducibility may be hindered by the limited exploration of different activation functions, normalization strategies, and the ethical implications of deploying RevCol in real-world scenarios.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to disentangled representations with the RevCol architecture, but it fails to explore critical aspects such as multimodal integration, scalability challenges, and comprehensive performance evaluation. Addressing these gaps would significantly enhance the paper's contributions and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Reversible Column Networks\" introduces a novel architecture that leverages reversible connections to improve performance on visual tasks, particularly on the ImageNet, COCO, and ADE20K datasets. The authors present various models (RevCol-T, RevCol-S, etc.) and report performance metrics such as Top-1 accuracy and mean Intersection over Union (mIoU). The findings suggest that RevCol architectures outperform traditional CNNs and Vision Transformers, particularly highlighting the benefits of intermediate supervision and the impact of kernel size variations. However, the paper lacks formal statistical testing to validate the significance of the reported improvements.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to model architecture through reversible connections, which shows promising results in terms of performance metrics. The use of ablation studies effectively illustrates the contributions of reversible columns and intermediate supervision. However, a major weakness is the absence of rigorous statistical analysis, such as p-values or confidence intervals, which undermines the claims of statistical significance for the observed performance enhancements. The lack of quantitative assessments on memory consumption and the impact of kernel sizes further detracts from the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its findings clearly, with logical organization and appropriate visualizations. The novelty of the reversible column architecture is significant in the context of current state-of-the-art models. However, reproducibility may be challenging due to the insufficient details on statistical testing and validation of the performance improvements. Future iterations of this work should address these issues to enhance the paper's overall quality.\n\n# Summary Of The Review\nOverall, \"Reversible Column Networks\" presents a promising architectural innovation with several notable performance improvements compared to existing models. However, the lack of rigorous statistical validation limits the strength of the claims made in the paper. To fully establish the significance of the findings, the authors should incorporate explicit statistical analyses in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces RevCol, a reversible architecture aimed at improving the efficiency and effectiveness of deep learning models. The authors propose a novel methodology leveraging reversible connections to enhance model performance while minimizing resource consumption. Experimental results demonstrate that RevCol achieves competitive performance on various tasks using large datasets. However, the paper lacks a comprehensive comparison with state-of-the-art architectures and does not thoroughly explore the implications of its reversible nature in broader contexts.\n\n# Strength And Weaknesses\nThe primary strength of RevCol lies in its innovative approach to leveraging reversible connections, which theoretically allows for more efficient training and inference. The empirical results indicate promising performance on the selected tasks, showcasing the potential of this architecture. However, the paper's weaknesses include a lack of comprehensive comparisons with existing models, insufficient exploration of its applicability to smaller datasets, and a failure to address practical concerns such as real-time application performance and interpretability of learned features. Additionally, the future work section is vague, providing little guidance for subsequent research directions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, making it accessible to the intended audience. However, the lack of detailed analyses regarding various aspects of the architecture, such as the impact of the number of columns and the implications of intermediate supervision, detracts from its overall quality. While the novelty of reversible architectures is significant, the paper does not sufficiently emphasize this or provide enough empirical evidence to support its claims. Reproducibility is somewhat compromised due to the absence of detailed methodologies and discussions regarding dataset biases and limitations.\n\n# Summary Of The Review\nOverall, RevCol presents an interesting approach to improving deep learning architectures through reversible connections but suffers from several limitations that hinder its broader applicability and understanding. The lack of comprehensive comparisons and explorations of important practical considerations raises concerns about its deployment in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Reversible Column Networks (RevCol), a new neural network architecture aimed at preserving information rather than compressing it, which the authors argue addresses limitations in existing models. The methodology involves a multi-level fusion module that employs reversible transformations, claimed to enhance feature learning and performance on standard benchmarks such as ImageNet and COCO. The findings indicate that RevCol achieves competitive performance, although the novelty of the approach appears limited compared to established methods in the field.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its thorough experimental evaluation, demonstrating the architecture's effectiveness on various datasets. However, the contributions are diminished by a lack of originality, as many of the concepts presented, such as reversible networks and intermediate supervision, have been previously discussed in the literature. The paper appears to repackage existing ideas without offering substantial new insights or advancements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the proposed architecture and methodologies. However, the novelty is questionable, as many concepts are not new to the literature. Reproducibility is supported by adherence to standard experimental practices, but the reliance on previously established methods raises concerns about the independence of the results.\n\n# Summary Of The Review\nOverall, the paper presents a well-structured exploration of RevCol, but it largely lacks innovation and relies on rehashed ideas. While the experimental results are solid, they do not sufficiently contribute to the advancement of the field, making the paper feel derivative rather than groundbreaking.\n\n# Correctness\n4/5 - The methodology and results are presented accurately, but the claimed novelty is overstated.\n\n# Technical Novelty And Significance\n2/5 - The technical contributions largely revisit known concepts without introducing significant new ideas.\n\n# Empirical Novelty And Significance\n3/5 - While competitive in performance, the empirical results do not significantly advance the state of the art, as similar claims have been made by numerous previous models.",
    "# Summary Of The Paper\nThe paper introduces RevCol, a novel architecture that leverages reversible networks to achieve enhanced memory efficiency and performance in visual recognition tasks. The authors propose methods for learning disentangled representations using intermediate supervision, and demonstrate the architecture's scalability and potential for multi-task learning. Empirical results indicate that RevCol outperforms traditional models on benchmark datasets, but the paper also highlights areas for further exploration, including the integration of auxiliary losses and the application of the architecture in diverse domains.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to reversible networks and the promising results in memory efficiency and performance. The incorporation of intermediate supervision and multi-task learning is also a noteworthy contribution. However, the paper lacks depth in exploring the broader applicability of RevCol beyond visual recognition, and more comprehensive ablation studies could enhance the understanding of the architecture's components. Additionally, performance comparisons could be balanced with discussions of failure cases to provide a clearer picture of the model's limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, though certain sections would benefit from more detailed elaboration, particularly regarding the integration of RevCol into various architectures such as transformers. The novelty of the approach is significant, especially in the context of memory efficiency and disentangled representation learning. However, reproducibility could be improved by including detailed experimental setups and clearer descriptions of the datasets used.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field through its exploration of reversible networks and their applications in memory-efficient architectures. While the findings are promising, the paper would benefit from a deeper examination of the architecture's versatility and a more balanced discussion of its limitations. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the RevCol architecture, a novel convolutional neural network (CNN) model that achieves state-of-the-art performance across various computer vision tasks, including image classification, object detection, and semantic segmentation. The methodology highlights the scalability and efficiency of RevCol, demonstrating significant performance improvements with larger model sizes and datasets. Key findings include RevCol-XL achieving 88.2% accuracy on ImageNet-1K and RevCol-H reaching 90.0%, alongside notable results in COCO object detection and ADE20K semantic segmentation, outperforming existing models like ConvNeXt and Swin transformers.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its comprehensive evaluation of RevCol across multiple tasks, showcasing its competitive edge in both accuracy and efficiency. The architecture's design, which incorporates reversible connections, facilitates memory efficiency, making it suitable for larger models without significant overhead. However, the paper could benefit from a more detailed analysis of the intermediate supervision technique and its impact on training dynamics, as well as comparisons with a broader range of models beyond static CNNs.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology, results, and comparisons with related work. The quality of the experiments and the thorough benchmarking against state-of-the-art models add to the robustness of the findings. The novelty of the RevCol architecture is evident in its design and performance, though it would be helpful to see additional reproducibility details, such as hyperparameter settings and training protocols, to facilitate replication by other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in CNN architectures through the introduction of RevCol, demonstrating impressive performance across various benchmarks. While the contributions are substantial, there is room for improvement in detailing certain methodologies and ensuring reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents \"REVERSIBLE COLUMN NETWORKS,\" introducing a novel architecture that leverages reversible connections to enhance the learning of disentangled representations. The authors propose a methodology that integrates reversible column networks (RevCol) with various neural architectures to improve performance in tasks involving complex data distributions. Empirical results demonstrate that RevCol outperforms traditional models on benchmark datasets, showcasing improvements in efficiency and representation quality.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to leveraging reversible connections for disentangled representation learning, which is a significant advancement in the field. The empirical results are convincing and provide strong evidence of the model's capabilities. However, the paper suffers from clarity issues due to the dense abstract, technical jargon, and lengthy paragraphs that may alienate readers unfamiliar with the subject matter. Additionally, the transitions between sections are abrupt, which disrupts the overall flow of the document.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the technical novelty of the RevCol architecture is commendable, the clarity of the paper could be significantly improved. The abstract should be more concise, and the use of technical jargon should be accompanied by definitions to aid comprehension. The methodology section lacks sufficient detail for reproducibility, as it does not explicitly outline each step or assumptions made during the experiments. Furthermore, the presentation of experimental results could benefit from clearer headings and context for better understanding.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of representation learning by introducing reversible column networks. However, the effectiveness of these contributions is hindered by issues of clarity and reproducibility. Improving the organization and accessibility of the content would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.2431683999602057,
    -1.6674691858583812,
    -1.8568829212700266,
    -1.7149882253370488,
    -1.805606550203407,
    -1.7438985445473354,
    -1.5827720316468927,
    -1.9634752978811678,
    -1.8085594461026273,
    -1.9480929693214204,
    -1.750999336345369,
    -1.4529441423659761,
    -1.6731233053171484,
    -1.6383921362086071,
    -1.6953545839837376,
    -1.804272788138623,
    -1.9471268670842314,
    -1.7226599669593254,
    -1.8539248308252685,
    -1.9602427238569948,
    -1.8225203975082593,
    -1.584324220915795,
    -1.9537420050825074,
    -1.75321304143687,
    -1.8274469477816764,
    -1.8588752136699973,
    -1.7935180662554697,
    -1.6142633627755334,
    -1.6731813017269703
  ],
  "logp_cond": [
    [
      0.0,
      -2.0127793086626635,
      -2.023470107196507,
      -1.993251218078526,
      -2.0391939237102386,
      -2.022832764231664,
      -2.0098176196873223,
      -2.015149032102365,
      -2.0058664758378955,
      -2.048086792715551,
      -2.0122596005237052,
      -2.0846290433443944,
      -1.9910079883470497,
      -2.001154748894233,
      -2.010523827036012,
      -1.9929931025144032,
      -2.004094001581937,
      -2.032071258849071,
      -2.0360630035691187,
      -2.0067175565489235,
      -2.0220977733621255,
      -2.0508149049613302,
      -2.0541091453073723,
      -2.0414562057471164,
      -2.045190688190763,
      -2.0289690039415555,
      -2.0521987349005166,
      -2.0481128865682616,
      -2.050718755707093
    ],
    [
      -1.3459546913329903,
      0.0,
      -1.2393469681014309,
      -1.1974051941552835,
      -1.3138951108726011,
      -1.2750052907115645,
      -1.3529960963229013,
      -1.2767494400823636,
      -1.2066289881573569,
      -1.2959104443061078,
      -1.2546270239524797,
      -1.425854111922734,
      -1.2585933782957939,
      -1.2328480303994065,
      -1.2448532522163887,
      -1.2685582839071516,
      -1.34039529474489,
      -1.2153625227614684,
      -1.3047006033817405,
      -1.2432066697211623,
      -1.2909927333692692,
      -1.3519187153236238,
      -1.3151536686320557,
      -1.3285387483821756,
      -1.3311197465655327,
      -1.365529449751282,
      -1.3080121825358346,
      -1.3146086347111894,
      -1.31386756210485
    ],
    [
      -1.5019547582984385,
      -1.4145902428372832,
      0.0,
      -1.4303387648711345,
      -1.438057837654161,
      -1.4760010858616184,
      -1.4392473623038926,
      -1.39524086181923,
      -1.3869655794032492,
      -1.4384913486887956,
      -1.4015314504872627,
      -1.5902522480596448,
      -1.3822035824678827,
      -1.386845836619467,
      -1.3964719151036804,
      -1.410601723459424,
      -1.4355277792035264,
      -1.433178486495867,
      -1.4391733578127919,
      -1.4365814367966854,
      -1.454153697987831,
      -1.4806800474629151,
      -1.4688707979003774,
      -1.439411950292177,
      -1.464743324832172,
      -1.4721481682910527,
      -1.4783779561473307,
      -1.4637689851831701,
      -1.4713869247460651
    ],
    [
      -1.3804081235521068,
      -1.2934759693653117,
      -1.3428009453515444,
      0.0,
      -1.3894513849574828,
      -1.3828245073413314,
      -1.4376577132367159,
      -1.365917666328595,
      -1.316945148552805,
      -1.4099317629698713,
      -1.339961161055813,
      -1.529808191509905,
      -1.2950014534146594,
      -1.3435203006413063,
      -1.3203531334614103,
      -1.3347195330515622,
      -1.417329698423484,
      -1.3215304414234286,
      -1.4161615671841068,
      -1.3722004259986027,
      -1.4266665515059187,
      -1.466660177060289,
      -1.4323735504822233,
      -1.4111049979444554,
      -1.437722066828103,
      -1.439987642290497,
      -1.4614509726674159,
      -1.3461171366368476,
      -1.452508807373943
    ],
    [
      -1.4642036684329913,
      -1.4463336457645974,
      -1.3656970718056074,
      -1.4737459405127031,
      0.0,
      -1.4665543650432347,
      -1.4033213628851298,
      -1.339389825807853,
      -1.409444891128314,
      -1.4259301856476942,
      -1.3804279329227958,
      -1.5515205803039964,
      -1.4294217520424592,
      -1.4288069648025672,
      -1.412984716274706,
      -1.47042479827107,
      -1.390770798999987,
      -1.3989414963830056,
      -1.398986270867481,
      -1.3982807123808245,
      -1.4212602141533726,
      -1.499231052324318,
      -1.408468000005127,
      -1.437272476513443,
      -1.4623419274393574,
      -1.405811575706709,
      -1.4508051319096125,
      -1.4307169155047332,
      -1.5173259478346028
    ],
    [
      -1.374717227340299,
      -1.256758622621071,
      -1.2956713948793586,
      -1.3061039889709418,
      -1.3978256951328827,
      0.0,
      -1.4255981900369195,
      -1.3114482892584896,
      -1.318573107610618,
      -1.387547320145416,
      -1.3359660849396702,
      -1.430146553510672,
      -1.3290527502247211,
      -1.2987090216995014,
      -1.3200416457337893,
      -1.3549276553676228,
      -1.3899114936270467,
      -1.3089440068115474,
      -1.3934681061825869,
      -1.3236019470810747,
      -1.3314283367413664,
      -1.4122505937511416,
      -1.387042763879335,
      -1.3917763190799728,
      -1.3932758299617753,
      -1.4405922585764872,
      -1.407502902568121,
      -1.373058762818578,
      -1.4060543701980133
    ],
    [
      -1.2754031823364547,
      -1.2602669218209341,
      -1.240501744174206,
      -1.271334595728622,
      -1.280435257764598,
      -1.3136875385009792,
      0.0,
      -1.2475539471200894,
      -1.2540171538778855,
      -1.2880864725670154,
      -1.271733959772751,
      -1.3621236592759451,
      -1.2411652500422412,
      -1.265091015277645,
      -1.2847081570851133,
      -1.2717453079085197,
      -1.2369433411422126,
      -1.297289956349013,
      -1.2839531643279742,
      -1.2435739599159712,
      -1.289522757164971,
      -1.3222155532498239,
      -1.3017113464191865,
      -1.2959425777384115,
      -1.292197940523366,
      -1.341766424036802,
      -1.3177806831884527,
      -1.3004743050101126,
      -1.3183672809490525
    ],
    [
      -1.6447638715532986,
      -1.56842719452048,
      -1.5296171642671228,
      -1.5901321546946887,
      -1.5697664748382179,
      -1.6128529574202304,
      -1.6317734985609427,
      0.0,
      -1.5110182631161748,
      -1.5941527543346408,
      -1.5966347245422312,
      -1.6998050415504093,
      -1.5384514052027836,
      -1.5442241358757811,
      -1.5869793280566127,
      -1.5431935157049794,
      -1.5502798272024945,
      -1.5277240370338132,
      -1.558289421600557,
      -1.5808325923123565,
      -1.5516172765677487,
      -1.6607642187125582,
      -1.610482821438315,
      -1.628529960850605,
      -1.6179450254853605,
      -1.6462989905529213,
      -1.632292967585206,
      -1.522976244408465,
      -1.6623463963950074
    ],
    [
      -1.4414412751318004,
      -1.3297496150716717,
      -1.3323594697486878,
      -1.362492501670751,
      -1.4100609334113132,
      -1.415965177611299,
      -1.425544864188798,
      -1.3127732710092852,
      0.0,
      -1.4026286976896165,
      -1.340074506454509,
      -1.5436525990553152,
      -1.3206144456213607,
      -1.3479885904624256,
      -1.339368217415018,
      -1.385980761712294,
      -1.4295280076493593,
      -1.306310531180271,
      -1.4030773023197116,
      -1.432772388102945,
      -1.3790358971236099,
      -1.450025950597953,
      -1.4040938401075245,
      -1.423388181474305,
      -1.4125076645414016,
      -1.4668968116745873,
      -1.4358722175894474,
      -1.373222867537777,
      -1.448935734857191
    ],
    [
      -1.5706464965404734,
      -1.5083178319817836,
      -1.5182723476924602,
      -1.5693886486681752,
      -1.528662071589889,
      -1.5343600146567098,
      -1.5773437182293077,
      -1.4969127081952225,
      -1.4804096954156756,
      0.0,
      -1.499326719301906,
      -1.6214949803752727,
      -1.4833763664642625,
      -1.5582515458751298,
      -1.5108422782491178,
      -1.5358820363345924,
      -1.5096974469343496,
      -1.5239914283683957,
      -1.4818392723382436,
      -1.5553026982887213,
      -1.4901075928824385,
      -1.565199681273272,
      -1.4958160728434022,
      -1.5201438751483196,
      -1.5000046188962353,
      -1.6004778494736644,
      -1.4748189530194766,
      -1.5171236485472237,
      -1.5584611268233743
    ],
    [
      -1.3701910896699139,
      -1.2479214184684315,
      -1.1782969702203425,
      -1.289251911766566,
      -1.2550980708512502,
      -1.2805107896322387,
      -1.3662964940650302,
      -1.2084961951810962,
      -1.1566647174203724,
      -1.2448180155165076,
      0.0,
      -1.4352242164719091,
      -1.1922061485904765,
      -1.2139938557621186,
      -1.2271245929666013,
      -1.2863732121146019,
      -1.321983569316592,
      -1.2693271944277174,
      -1.2382231741340384,
      -1.18594889301579,
      -1.2661373150500899,
      -1.3358334678482373,
      -1.3038069139801973,
      -1.286766560137866,
      -1.3045802406100941,
      -1.3433356080232346,
      -1.3163751163180435,
      -1.3244190021177549,
      -1.3272213732561446
    ],
    [
      -1.2196160128841915,
      -1.2001673115006337,
      -1.180031030739084,
      -1.207201584526389,
      -1.1891990322240518,
      -1.1778798710915481,
      -1.172674662925827,
      -1.153055098734074,
      -1.1844586278298572,
      -1.185709503625943,
      -1.177078418708561,
      0.0,
      -1.1878600223910312,
      -1.1916422949594445,
      -1.1656567677479637,
      -1.190605897583552,
      -1.1793397795898606,
      -1.1782231224102655,
      -1.19287868277727,
      -1.1549567133128618,
      -1.1236271655387216,
      -1.1708637806109572,
      -1.1774931781296443,
      -1.156107643697011,
      -1.1697366139380823,
      -1.2361369892754677,
      -1.1491385929938225,
      -1.185836187762287,
      -1.1341181765718338
    ],
    [
      -1.3108310678630652,
      -1.2505825881766373,
      -1.218921980853728,
      -1.1947326083903014,
      -1.3219910947830986,
      -1.3180929469279654,
      -1.3155129916781934,
      -1.2262443219461552,
      -1.1609941432433664,
      -1.2719767377269948,
      -1.210931731751064,
      -1.402404389484773,
      0.0,
      -1.3032267424179276,
      -1.2607910643940847,
      -1.2402828192010609,
      -1.2908555840002347,
      -1.1974560426681364,
      -1.269622213832309,
      -1.2846221368425292,
      -1.285376454533673,
      -1.3504303689922164,
      -1.3377548551284677,
      -1.3412550206925398,
      -1.3244625314610803,
      -1.3247099836399736,
      -1.3121151884977582,
      -1.327354582800307,
      -1.333240335815152
    ],
    [
      -1.241519250633353,
      -1.130153065437933,
      -1.137474653956186,
      -1.1688387301881789,
      -1.2197644928653686,
      -1.2143336276624896,
      -1.2744564419458246,
      -1.1663538191065426,
      -1.122417551020568,
      -1.272293241484203,
      -1.1278387292983552,
      -1.353365628310872,
      -1.1802600119809792,
      0.0,
      -1.1970922030143558,
      -1.2089195546687757,
      -1.2382597500689039,
      -1.162404325970026,
      -1.18653382157954,
      -1.1809534342206587,
      -1.2319154511159167,
      -1.298895230833435,
      -1.2690478235134748,
      -1.2840878089096208,
      -1.2624539252210056,
      -1.3145073036981816,
      -1.2735262359145394,
      -1.26482040705208,
      -1.2653301678106261
    ],
    [
      -1.317415887656807,
      -1.22535371077881,
      -1.1785504508178848,
      -1.2296656603046667,
      -1.2382274023350193,
      -1.2891199592555498,
      -1.3075998541985694,
      -1.2317012059154318,
      -1.192885416478189,
      -1.3067744273004518,
      -1.2064647898132022,
      -1.3679613103730628,
      -1.204534801000108,
      -1.2343008331973082,
      0.0,
      -1.2057916186538489,
      -1.282669232184786,
      -1.2146798708875934,
      -1.3045898279565997,
      -1.2720095997545102,
      -1.2778520802336033,
      -1.3025441454870241,
      -1.3251727838422944,
      -1.2745493054368158,
      -1.31783662135877,
      -1.3390436357626192,
      -1.324878556192486,
      -1.2651089128833455,
      -1.2860715199736568
    ],
    [
      -1.4128209457964733,
      -1.3832815507174518,
      -1.3692759732785036,
      -1.2784739270647538,
      -1.4349115263810632,
      -1.440228415224244,
      -1.4190645371331423,
      -1.321818760006798,
      -1.3307810235194053,
      -1.3778513492442723,
      -1.363952590961943,
      -1.493508578748203,
      -1.3200088638544394,
      -1.3895641705927035,
      -1.3414317560504958,
      0.0,
      -1.4013342006923053,
      -1.3986422670034502,
      -1.417228137845255,
      -1.375983199196449,
      -1.4255869764859705,
      -1.4316791668615376,
      -1.433316577142493,
      -1.439229676102912,
      -1.4216001685240662,
      -1.426051719127248,
      -1.425830472558524,
      -1.3888236562118788,
      -1.4508070966811417
    ],
    [
      -1.6366829276984183,
      -1.5715863039135134,
      -1.546596107942439,
      -1.546159487140257,
      -1.5476786721084166,
      -1.6401621859608,
      -1.632367848970204,
      -1.4966422100535512,
      -1.5393509446592717,
      -1.5821717068228616,
      -1.5681395992145544,
      -1.7114668568952731,
      -1.566552095199405,
      -1.5770220723882982,
      -1.5748956746580878,
      -1.6198780815795262,
      0.0,
      -1.5881395873850301,
      -1.586849748084637,
      -1.5524496996709531,
      -1.5749380599074965,
      -1.6416998339695792,
      -1.6016203860308185,
      -1.6289671674883908,
      -1.6122563849107387,
      -1.619889010926751,
      -1.599334409597936,
      -1.6101797576467074,
      -1.601638005203487
    ],
    [
      -1.3519215322165217,
      -1.178425695607959,
      -1.234028418643545,
      -1.2276341417992624,
      -1.2382239220483957,
      -1.2795393461652844,
      -1.3603773379036994,
      -1.219034836351658,
      -1.2076870106156736,
      -1.2941446848097116,
      -1.248646820381072,
      -1.4077404765216435,
      -1.1589048300956049,
      -1.2303896485470627,
      -1.2326147205902669,
      -1.240236243077876,
      -1.2948055427511476,
      0.0,
      -1.2672398489389636,
      -1.269539607632364,
      -1.2538054049371172,
      -1.3427403415956298,
      -1.2765762854008356,
      -1.3081468592757366,
      -1.2758272095935423,
      -1.344063277416509,
      -1.2766458101161173,
      -1.2533749050587384,
      -1.3253300686475185
    ],
    [
      -1.5401186339424089,
      -1.547607755473343,
      -1.4818862498841128,
      -1.5829567113234875,
      -1.4639650311134027,
      -1.56269538664509,
      -1.5572889886697314,
      -1.3916304357259757,
      -1.4607568857835584,
      -1.426308593552423,
      -1.504897618473848,
      -1.6431781952820324,
      -1.4859550201362737,
      -1.5043971726933523,
      -1.5618896659570118,
      -1.5731924300610913,
      -1.4961380975016414,
      -1.50201008399248,
      0.0,
      -1.4266815492717955,
      -1.4570794179115776,
      -1.6035115694258808,
      -1.5186670539224045,
      -1.5063743081691603,
      -1.5099236410685597,
      -1.5079946307325804,
      -1.5129306159632856,
      -1.5503249368533554,
      -1.5749908934036132
    ],
    [
      -1.613004855470051,
      -1.4973233186380375,
      -1.5152018812256194,
      -1.5063670587371392,
      -1.5458882967507248,
      -1.5580736604538161,
      -1.5872291118124648,
      -1.536304616946263,
      -1.5138548003950403,
      -1.5944459088742895,
      -1.514813904333713,
      -1.6904844854481809,
      -1.5333949573400458,
      -1.4996389708595912,
      -1.5579376168679342,
      -1.5715417414263169,
      -1.5467142418360569,
      -1.489363731569108,
      -1.5038502137269172,
      0.0,
      -1.5172652219557405,
      -1.6372576502163392,
      -1.5762522602525446,
      -1.5844271446948375,
      -1.580963041693609,
      -1.5646201822025858,
      -1.579261687360223,
      -1.59827225566864,
      -1.5930743956791023
    ],
    [
      -1.4979637244796236,
      -1.4320743855489788,
      -1.437554309917417,
      -1.464200210127406,
      -1.4488260878730028,
      -1.5019918395957088,
      -1.4721287039193982,
      -1.3378256592285018,
      -1.4184007360284085,
      -1.4442376568363755,
      -1.4326178393210183,
      -1.549183387838644,
      -1.42087004564499,
      -1.4554038159793352,
      -1.434446448024298,
      -1.4553897999561587,
      -1.453738155266465,
      -1.4149174961124822,
      -1.4530570414904977,
      -1.4632956401277997,
      0.0,
      -1.463186784863575,
      -1.4360467077459782,
      -1.413747639965602,
      -1.4126263257907783,
      -1.4673708319326455,
      -1.4371070954801506,
      -1.4143197186721101,
      -1.4664337928044655
    ],
    [
      -1.28288537612825,
      -1.1891287928746939,
      -1.1799338994408983,
      -1.228071587678883,
      -1.2092609683254159,
      -1.181290536160838,
      -1.1918578539193923,
      -1.209018646090853,
      -1.17743583269181,
      -1.2053538958023717,
      -1.1643041866559578,
      -1.2236368759514427,
      -1.1951586485213403,
      -1.2059738557330488,
      -1.1721076539600177,
      -1.1886959525703142,
      -1.179357195568425,
      -1.2208912368609426,
      -1.1670046472463824,
      -1.1887279428262219,
      -1.1464803433563038,
      0.0,
      -1.1892035826212113,
      -1.1478018265280896,
      -1.1826489938110958,
      -1.2455409673580862,
      -1.1996018254949163,
      -1.184638754392313,
      -1.1044874397451467
    ],
    [
      -1.5831063416819613,
      -1.5454288302530208,
      -1.511383272803491,
      -1.5898384373955567,
      -1.4942315706303688,
      -1.5630268966878962,
      -1.5483295336658647,
      -1.4712233234701855,
      -1.5220049890385703,
      -1.4538047292577776,
      -1.5039361499176174,
      -1.6149204870608227,
      -1.542518424544071,
      -1.5395312978051183,
      -1.5000779650193532,
      -1.5562987251626377,
      -1.5037541932957879,
      -1.4758458742895944,
      -1.519795508324022,
      -1.5623538194159186,
      -1.512647391925727,
      -1.5361583459027834,
      0.0,
      -1.4839590582625397,
      -1.4751695427549936,
      -1.6379505578674889,
      -1.4857391575828625,
      -1.5295773236428372,
      -1.5191232699968595
    ],
    [
      -1.4448209905373404,
      -1.367217506523651,
      -1.382989588676319,
      -1.4012259548669994,
      -1.3672769574812194,
      -1.4300008742275685,
      -1.403964034549641,
      -1.3480738822539051,
      -1.406926315482658,
      -1.3770127869733935,
      -1.3784360310568406,
      -1.4929814841683646,
      -1.408783613672087,
      -1.3854423151927198,
      -1.3633723694443696,
      -1.4023161407885305,
      -1.3555909241760018,
      -1.376210837682818,
      -1.3642722954738968,
      -1.3545762564774368,
      -1.3126726244741693,
      -1.4105329308975707,
      -1.3599699973951012,
      0.0,
      -1.3477666441730143,
      -1.4208805934879565,
      -1.373334865442762,
      -1.3749880669140278,
      -1.4099073449228836
    ],
    [
      -1.4721058142737176,
      -1.4164125539618078,
      -1.416097782757161,
      -1.445799378544777,
      -1.445291114896405,
      -1.4456485257066671,
      -1.4594831866073166,
      -1.3762393271097326,
      -1.375999280162147,
      -1.3841385263604535,
      -1.4155743764216444,
      -1.5039586405256957,
      -1.382108277078428,
      -1.441813566556455,
      -1.4146161500807717,
      -1.4389299905707913,
      -1.3733596076106616,
      -1.4098545931228117,
      -1.4065731590291684,
      -1.400515654340318,
      -1.3381902113046915,
      -1.46564164117342,
      -1.3646778416610474,
      -1.3639137105134451,
      0.0,
      -1.467505580952005,
      -1.398331566927921,
      -1.425098724169564,
      -1.4337477719467906
    ],
    [
      -1.5017105793393402,
      -1.4559881457781618,
      -1.444506021007806,
      -1.4838477903049994,
      -1.4833771278158672,
      -1.534891026237809,
      -1.5495307634402797,
      -1.4809027764768894,
      -1.4983222381891894,
      -1.4791554174407686,
      -1.4877910458439116,
      -1.6023471045791564,
      -1.469581306362143,
      -1.4677904591278448,
      -1.4983821554358594,
      -1.4752393759602964,
      -1.47029719843651,
      -1.4829219004349337,
      -1.440357720084217,
      -1.4558989219004104,
      -1.460001780943785,
      -1.5211235541714916,
      -1.4918950416208094,
      -1.4834433690899766,
      -1.4929439612424775,
      0.0,
      -1.4729720349747584,
      -1.483857175465438,
      -1.505578852949725
    ],
    [
      -1.4497018018487453,
      -1.3806344412491731,
      -1.3690204106315884,
      -1.4215317500787872,
      -1.3340293250163915,
      -1.383964370758855,
      -1.4012363936435783,
      -1.3411735134149478,
      -1.3794866020596876,
      -1.340074415729479,
      -1.416465878453196,
      -1.412801178270963,
      -1.3523180517958018,
      -1.3948759043833063,
      -1.3942735982892165,
      -1.3758014746315936,
      -1.3622291566408344,
      -1.3782632303999411,
      -1.3813788638635467,
      -1.360070637468353,
      -1.3253196100699498,
      -1.3803185848699957,
      -1.298401627191174,
      -1.323030433185037,
      -1.3362563783388715,
      -1.4467946602243384,
      0.0,
      -1.382452923590991,
      -1.3445753309597142
    ],
    [
      -1.299894749361335,
      -1.2090883063338302,
      -1.2236466736412028,
      -1.2035825252882024,
      -1.1748502366974967,
      -1.2489141716739343,
      -1.2674013313499346,
      -1.1145867314589915,
      -1.1941753541485987,
      -1.200518087292963,
      -1.2494699472829967,
      -1.3171720264182627,
      -1.223028326014733,
      -1.2429331029045325,
      -1.181674841948132,
      -1.2081579486971925,
      -1.221737230975896,
      -1.176318934159363,
      -1.255402959142382,
      -1.2453463957967463,
      -1.1630901129094342,
      -1.2365869698790446,
      -1.2184089395109194,
      -1.1781031514876634,
      -1.2209223596359517,
      -1.2935614358438172,
      -1.2120390635961937,
      0.0,
      -1.2306798757431403
    ],
    [
      -1.385369825688521,
      -1.3058980903630502,
      -1.2734102500416231,
      -1.3123835848610192,
      -1.2847731562643252,
      -1.3211712444095036,
      -1.3043934477678838,
      -1.2756007894441308,
      -1.2790115202355232,
      -1.3024704405285041,
      -1.278085726067531,
      -1.3097968670669207,
      -1.2839595089283584,
      -1.2924986003346073,
      -1.2716305748681422,
      -1.2947050278555547,
      -1.2751985780814583,
      -1.3265173760934854,
      -1.3237314556491404,
      -1.3009734917188076,
      -1.2745594574224057,
      -1.239584183676535,
      -1.2863662764550114,
      -1.2801118613856044,
      -1.2923377037014159,
      -1.3511773627236723,
      -1.2830648764782506,
      -1.316298466301904,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.23038909129754215,
      0.2196982927636988,
      0.24991718188167966,
      0.20397447624996712,
      0.22033563572854176,
      0.23335078027288336,
      0.22801936785784083,
      0.23730192412231021,
      0.19508160724465462,
      0.23090879943650044,
      0.1585393566158113,
      0.25216041161315594,
      0.24201365106597272,
      0.2326445729241935,
      0.25017529744580247,
      0.2390743983782686,
      0.21109714111113487,
      0.20710539639108694,
      0.2364508434112822,
      0.2210706265980802,
      0.19235349499887544,
      0.18905925465283335,
      0.20171219421308928,
      0.1979777117694428,
      0.21419939601865012,
      0.19096966505968904,
      0.19505551339194405,
      0.1924496442531125
    ],
    [
      0.32151449452539094,
      0.0,
      0.42812221775695036,
      0.47006399170309776,
      0.3535740749857801,
      0.3924638951468167,
      0.3144730895354799,
      0.39071974577601765,
      0.46084019770102436,
      0.3715587415522734,
      0.4128421619059015,
      0.2416150739356473,
      0.40887580756258735,
      0.43462115545897473,
      0.42261593364199257,
      0.3989109019512296,
      0.32707389111349117,
      0.45210666309691283,
      0.3627685824766407,
      0.4242625161372189,
      0.376476452489112,
      0.3155504705347574,
      0.3523155172263255,
      0.33893043747620566,
      0.33634943929284855,
      0.30193973610709923,
      0.3594570033225466,
      0.35286055114719184,
      0.3536016237535313
    ],
    [
      0.35492816297158813,
      0.44229267843274345,
      0.0,
      0.4265441563988921,
      0.41882508361586557,
      0.38088183540840825,
      0.417635558966134,
      0.4616420594507966,
      0.46991734186677747,
      0.41839157258123105,
      0.45535147078276395,
      0.2666306732103818,
      0.47467933880214397,
      0.4700370846505597,
      0.4604110061663462,
      0.4462811978106027,
      0.42135514206650027,
      0.42370443477415964,
      0.41770956345723476,
      0.42030148447334126,
      0.40272922328219574,
      0.3762028738071115,
      0.3880121233696492,
      0.4174709709778497,
      0.3921395964378547,
      0.38473475297897397,
      0.37850496512269594,
      0.3931139360868565,
      0.3854959965239615
    ],
    [
      0.3345801017849419,
      0.421512255971737,
      0.3721872799855044,
      0.0,
      0.325536840379566,
      0.3321637179957173,
      0.2773305121003329,
      0.34907055900845374,
      0.3980430767842438,
      0.30505646236717743,
      0.37502706428123567,
      0.18518003382714365,
      0.4199867719223893,
      0.3714679246957424,
      0.3946350918756385,
      0.38026869228548654,
      0.29765852691356476,
      0.3934577839136202,
      0.29882665815294196,
      0.34278779933844605,
      0.28832167383113005,
      0.2483280482767598,
      0.2826146748548255,
      0.3038832273925933,
      0.27726615850894576,
      0.27500058304655184,
      0.2535372526696329,
      0.36887108870020113,
      0.2624794179631058
    ],
    [
      0.3414028817704158,
      0.35927290443880966,
      0.4399094783977997,
      0.33186060969070397,
      0.0,
      0.33905218516017244,
      0.40228518731827734,
      0.46621672439555417,
      0.39616165907509315,
      0.37967636455571285,
      0.42517861728061135,
      0.2540859698994107,
      0.37618479816094785,
      0.3767995854008399,
      0.3926218339287011,
      0.335181751932337,
      0.4148357512034202,
      0.4066650538204015,
      0.40662027933592615,
      0.4073258378225826,
      0.38434633605003454,
      0.30637549787908913,
      0.39713855019828004,
      0.36833407368996407,
      0.3432646227640497,
      0.3997949744966982,
      0.35480141829379463,
      0.37488963469867387,
      0.28828060236880426
    ],
    [
      0.3691813172070364,
      0.48713992192626443,
      0.44822714966797683,
      0.4377945555763936,
      0.3460728494144527,
      0.0,
      0.31830035451041594,
      0.43245025528884584,
      0.4253254369367174,
      0.35635122440191935,
      0.4079324596076652,
      0.3137519910366633,
      0.41484579432261426,
      0.445189522847834,
      0.42385689881354605,
      0.38897088917971256,
      0.3539870509202887,
      0.43495453773578796,
      0.3504304383647485,
      0.4202965974662607,
      0.412470207805969,
      0.3316479507961938,
      0.3568557806680004,
      0.3521222254673626,
      0.35062271458556005,
      0.3033062859708482,
      0.33639564197921445,
      0.3708397817287574,
      0.3378441743493221
    ],
    [
      0.30736884931043806,
      0.3225051098259586,
      0.34227028747268684,
      0.3114374359182708,
      0.30233677388229463,
      0.26908449314591354,
      0.0,
      0.33521808452680335,
      0.32875487776900725,
      0.29468555907987737,
      0.3110380718741417,
      0.2206483723709476,
      0.3416067816046515,
      0.31768101636924784,
      0.2980638745617794,
      0.31102672373837303,
      0.3458286905046801,
      0.2854820752978797,
      0.2988188673189185,
      0.33919807173092154,
      0.29324927448192173,
      0.26055647839706886,
      0.2810606852277062,
      0.28682945390848125,
      0.29057409112352683,
      0.24100560761009082,
      0.2649913484584401,
      0.2822977266367801,
      0.2644047506978402
    ],
    [
      0.3187114263278692,
      0.39504810336068785,
      0.433858133614045,
      0.3733431431864791,
      0.39370882304294996,
      0.35062234046093743,
      0.33170179932022514,
      0.0,
      0.452457034764993,
      0.36932254354652705,
      0.3668405733389366,
      0.26367025633075847,
      0.42502389267838425,
      0.4192511620053867,
      0.37649596982455513,
      0.4202817821761884,
      0.41319547067867335,
      0.4357512608473546,
      0.40518587628061087,
      0.3826427055688113,
      0.4118580213134191,
      0.3027110791686096,
      0.3529924764428529,
      0.33494533703056284,
      0.34553027239580736,
      0.31717630732824653,
      0.3311823302959618,
      0.4404990534727029,
      0.30112890148616045
    ],
    [
      0.3671181709708269,
      0.47880983103095565,
      0.4761999763539395,
      0.44606694443187633,
      0.3984985126913141,
      0.3925942684913284,
      0.3830145819138293,
      0.4957861750933421,
      0.0,
      0.4059307484130108,
      0.4684849396481183,
      0.26490684704731216,
      0.48794500048126666,
      0.4605708556402017,
      0.4691912286876092,
      0.4225786843903334,
      0.37903143845326803,
      0.5022489149223563,
      0.4054821437829157,
      0.37578705799968226,
      0.42952354897901746,
      0.3585334955046744,
      0.40446560599510284,
      0.3851712646283223,
      0.39605178156122567,
      0.34166263442804,
      0.3726872285131799,
      0.4353365785648504,
      0.35962371124543635
    ],
    [
      0.377446472780947,
      0.4397751373396368,
      0.4298206216289602,
      0.3787043206532452,
      0.4194308977315313,
      0.4137329546647106,
      0.37074925109211265,
      0.4511802611261979,
      0.4676832739057448,
      0.0,
      0.4487662500195144,
      0.32659798894614767,
      0.4647166028571579,
      0.3898414234462906,
      0.43725069107230263,
      0.412210932986828,
      0.4383955223870708,
      0.42410154095302466,
      0.46625369698317676,
      0.39279027103269915,
      0.45798537643898185,
      0.3828932880481484,
      0.45227689647801816,
      0.4279490941731008,
      0.44808835042518513,
      0.34761511984775595,
      0.47327401630194377,
      0.4309693207741967,
      0.38963184249804605
    ],
    [
      0.3808082466754552,
      0.5030779178769376,
      0.5727023661250266,
      0.46174742457880313,
      0.49590126549411884,
      0.47048854671313034,
      0.3847028422803389,
      0.5425031411642729,
      0.5943346189249967,
      0.5061813208288615,
      0.0,
      0.31577511987345996,
      0.5587931877548926,
      0.5370054805832505,
      0.5238747433787678,
      0.4646261242307672,
      0.4290157670287771,
      0.48167214191765173,
      0.5127761622113307,
      0.5650504433295791,
      0.4848620212952792,
      0.4151658684971318,
      0.44719242236517176,
      0.46423277620750314,
      0.44641909573527494,
      0.4076637283221345,
      0.4346242200273256,
      0.4265803342276142,
      0.42377796308922444
    ],
    [
      0.2333281294817846,
      0.2527768308653424,
      0.27291311162689214,
      0.2457425578395871,
      0.26374511014192437,
      0.275064271274428,
      0.28026947944014924,
      0.29988904363190216,
      0.2684855145361189,
      0.26723463874003306,
      0.27586572365741513,
      0.0,
      0.26508411997494497,
      0.2613018474065316,
      0.28728737461801246,
      0.26233824478242407,
      0.27360436277611555,
      0.27472101995571063,
      0.2600654595887062,
      0.29798742905311437,
      0.3293169768272546,
      0.28208036175501894,
      0.27545096423633186,
      0.2968364986689651,
      0.2832075284278939,
      0.21680715309050846,
      0.30380554937215365,
      0.2671079546036892,
      0.31882596579414235
    ],
    [
      0.3622922374540831,
      0.4225407171405111,
      0.45420132446342043,
      0.478390696926847,
      0.35113221053404975,
      0.35503035838918295,
      0.357610313638955,
      0.4468789833709932,
      0.512129162073782,
      0.40114656759015355,
      0.4621915735660844,
      0.2707189158323753,
      0.0,
      0.3698965628992208,
      0.41233224092306364,
      0.4328404861160875,
      0.38226772131691367,
      0.47566726264901193,
      0.40350109148483937,
      0.38850116847461913,
      0.38774685078347537,
      0.322692936324932,
      0.33536845018868067,
      0.3318682846246086,
      0.3486607738560681,
      0.34841332167717476,
      0.36100811681939016,
      0.3457687225168413,
      0.3398829695019965
    ],
    [
      0.39687288557525413,
      0.5082390707706741,
      0.5009174822524212,
      0.4695534060204283,
      0.41862764334323854,
      0.4240585085461175,
      0.3639356942627825,
      0.47203831710206456,
      0.5159745851880391,
      0.36609889472440416,
      0.510553406910252,
      0.2850265078977352,
      0.45813212422762795,
      0.0,
      0.4412999331942513,
      0.4294725815398315,
      0.40013238613970326,
      0.47598781023858105,
      0.4518583146290671,
      0.45743870198794845,
      0.4064766850926904,
      0.3394969053751722,
      0.36934431269513235,
      0.3543043272989863,
      0.3759382109876015,
      0.32388483251042555,
      0.36486590029406774,
      0.37357172915652703,
      0.373061968397981
    ],
    [
      0.37793869632693067,
      0.47000087320492767,
      0.5168041331658528,
      0.4656889236790709,
      0.45712718164871835,
      0.40623462472818783,
      0.38775472978516823,
      0.46365337806830587,
      0.5024691675055486,
      0.3885801566832858,
      0.48888979417053546,
      0.32739327361067483,
      0.4908197829836296,
      0.4610537507864294,
      0.0,
      0.48956296532988874,
      0.4126853517989517,
      0.4806747130961442,
      0.3907647560271379,
      0.4233449842292274,
      0.4175025037501343,
      0.3928104384967135,
      0.3701818001414432,
      0.42080527854692185,
      0.3775179626249676,
      0.3563109482211184,
      0.37047602779125155,
      0.43024567110039214,
      0.40928306401008085
    ],
    [
      0.3914518423421496,
      0.42099123742117106,
      0.4349968148601193,
      0.5257988610738691,
      0.3693612617575597,
      0.36404437291437897,
      0.3852082510054806,
      0.4824540281318248,
      0.4734917646192176,
      0.4264214388943506,
      0.4403201971766799,
      0.31076420939041993,
      0.4842639242841835,
      0.41470861754591937,
      0.4628410320881271,
      0.0,
      0.4029385874463176,
      0.40563052113517273,
      0.387044650293368,
      0.42828958894217384,
      0.37868581165265236,
      0.3725936212770853,
      0.37095621099612996,
      0.36504311203571094,
      0.3826726196145567,
      0.378221069011375,
      0.3784423155800989,
      0.41544913192674415,
      0.3534656914574812
    ],
    [
      0.31044393938581316,
      0.37554056317071804,
      0.40053075914179237,
      0.40096737994397436,
      0.39944819497581485,
      0.30696468112343145,
      0.3147590181140274,
      0.4504846570306802,
      0.40777592242495975,
      0.36495516026136987,
      0.37898726786967707,
      0.23566001018895832,
      0.38057477188482647,
      0.37010479469593327,
      0.37223119242614366,
      0.32724878550470526,
      0.0,
      0.3589872796992013,
      0.36027711899959436,
      0.3946771674132783,
      0.37218880717673497,
      0.30542703311465225,
      0.345506481053413,
      0.3181596995958407,
      0.3348704821734927,
      0.32723785615748047,
      0.3477924574862954,
      0.33694710943752404,
      0.34548886188074435
    ],
    [
      0.37073843474280377,
      0.5442342713513664,
      0.4886315483157804,
      0.495025825160063,
      0.48443604491092973,
      0.443120620794041,
      0.362282629055626,
      0.5036251306076673,
      0.5149729563436518,
      0.4285152821496139,
      0.47401314657825333,
      0.31491949043768197,
      0.5637551368637206,
      0.49227031841226276,
      0.49004524636905855,
      0.4824237238814495,
      0.4278544242081779,
      0.0,
      0.45542011802036186,
      0.4531203593269615,
      0.4688545620222082,
      0.3799196253636956,
      0.4460836815584899,
      0.41451310768358884,
      0.4468327573657831,
      0.37859668954281633,
      0.4460141568432081,
      0.46928506190058705,
      0.397329898311807
    ],
    [
      0.3138061968828596,
      0.30631707535192554,
      0.3720385809411557,
      0.270968119501781,
      0.3899597997118658,
      0.2912294441801786,
      0.29663584215553707,
      0.4622943950992928,
      0.39316794504171004,
      0.4276162372728456,
      0.3490272123514204,
      0.21074663554323614,
      0.3679698106889948,
      0.3495276581319162,
      0.29203516486825665,
      0.28073240076417716,
      0.3577867333236271,
      0.35191474683278856,
      0.0,
      0.427243281553473,
      0.3968454129136909,
      0.2504132613993877,
      0.33525777690286396,
      0.3475505226561082,
      0.3440011897567088,
      0.34593020009268804,
      0.3409942148619829,
      0.3035998939719131,
      0.2789339374216553
    ],
    [
      0.3472378683869439,
      0.46291940521895736,
      0.44504084263137544,
      0.45387566511985566,
      0.41435442710627,
      0.4021690634031787,
      0.37301361204453,
      0.42393810691073175,
      0.44638792346195455,
      0.36579681498270533,
      0.44542881952328184,
      0.26975823840881397,
      0.426847766516949,
      0.46060375299740364,
      0.40230510698906063,
      0.38870098243067797,
      0.41352848202093795,
      0.4708789922878869,
      0.45639251013007764,
      0.0,
      0.4429775019012543,
      0.32298507364065565,
      0.3839904636044502,
      0.37581557916215735,
      0.3792796821633859,
      0.395622541654409,
      0.38098103649677184,
      0.3619704681883549,
      0.3671683281778926
    ],
    [
      0.3245566730286358,
      0.3904460119592805,
      0.3849660875908423,
      0.35832018738085325,
      0.3736943096352565,
      0.3205285579125505,
      0.35039169358886113,
      0.4846947382797575,
      0.4041196614798508,
      0.37828274067188383,
      0.38990255818724107,
      0.27333700966961527,
      0.40165035186326925,
      0.3671165815289241,
      0.38807394948396134,
      0.36713059755210065,
      0.36878224224179434,
      0.40760290139577715,
      0.3694633560177616,
      0.35922475738045967,
      0.0,
      0.35933361264468444,
      0.38647368976228114,
      0.4087727575426574,
      0.40989407171748105,
      0.35514956557561383,
      0.38541330202810875,
      0.4082006788361492,
      0.35608660470379383
    ],
    [
      0.3014388447875451,
      0.3951954280411012,
      0.40439032147489673,
      0.35625263323691203,
      0.3750632525903792,
      0.40303368475495716,
      0.3924663669964028,
      0.3753055748249421,
      0.4068883882239851,
      0.3789703251134233,
      0.42002003425983725,
      0.3606873449643524,
      0.3891655723944547,
      0.37835036518274623,
      0.41221656695577735,
      0.3956282683454808,
      0.40496702534737006,
      0.36343298405485247,
      0.41731957366941264,
      0.3955962780895732,
      0.4378438775594913,
      0.0,
      0.39512063829458377,
      0.43652239438770546,
      0.4016752271046993,
      0.3387832535577089,
      0.3847223954208787,
      0.39968546652348214,
      0.4798367811706483
    ],
    [
      0.3706356634005461,
      0.4083131748294866,
      0.44235873227901634,
      0.36390356768695065,
      0.45951043445213857,
      0.3907151083946112,
      0.4054124714166427,
      0.4825186816123219,
      0.4317370160439371,
      0.4999372758247298,
      0.44980585516489,
      0.3388215180216847,
      0.41122358053843633,
      0.41421070727738907,
      0.4536640400631542,
      0.3974432799198697,
      0.44998781178671954,
      0.47789613079291304,
      0.4339464967584854,
      0.39138818566658884,
      0.44109461315678034,
      0.41758365917972395,
      0.0,
      0.4697829468199677,
      0.4785724623275138,
      0.3157914472150185,
      0.46800284749964494,
      0.42416468143967023,
      0.43461873508564786
    ],
    [
      0.3083920508995297,
      0.38599553491321914,
      0.370223452760551,
      0.35198708656987066,
      0.3859360839556507,
      0.32321216720930157,
      0.34924900688722915,
      0.40513915918296495,
      0.34628672595421217,
      0.3762002544634766,
      0.3747770103800294,
      0.26023155726850544,
      0.344429427764783,
      0.3677707262441503,
      0.3898406719925005,
      0.35089690064833956,
      0.39762211726086827,
      0.37700220375405213,
      0.38894074596297323,
      0.3986367849594332,
      0.44054041696270074,
      0.34268011053929937,
      0.3932430440417689,
      0.0,
      0.4054463972638558,
      0.3323324479489136,
      0.3798781759941081,
      0.3782249745228423,
      0.34330569651398646
    ],
    [
      0.3553411335079588,
      0.4110343938198686,
      0.41134916502451535,
      0.38164756923689946,
      0.38215583288527144,
      0.38179842207500925,
      0.3679637611743598,
      0.45120762067194375,
      0.45144766761952937,
      0.44330842142122284,
      0.41187257136003197,
      0.32348830725598066,
      0.4453386707032483,
      0.3856333812252213,
      0.4128307977009047,
      0.3885169572108851,
      0.4540873401710148,
      0.4175923546588647,
      0.420873788752508,
      0.4269312934413583,
      0.48925673647698487,
      0.36180530660825627,
      0.46276910612062894,
      0.46353323726823126,
      0.0,
      0.35994136682967137,
      0.42911538085375533,
      0.4023482236121123,
      0.3936991758348858
    ],
    [
      0.35716463433065715,
      0.4028870678918355,
      0.4143691926621913,
      0.37502742336499795,
      0.37549808585413014,
      0.32398418743218826,
      0.30934445022971757,
      0.3779724371931079,
      0.3605529754808079,
      0.3797197962292287,
      0.37108416782608566,
      0.2565281090908409,
      0.3892939073078543,
      0.3910847545421525,
      0.3604930582341379,
      0.3836358377097009,
      0.38857801523348723,
      0.37595331323506365,
      0.4185174935857803,
      0.40297629176958694,
      0.39887343272621223,
      0.3377516594985057,
      0.36698017204918787,
      0.37543184458002066,
      0.3659312524275198,
      0.0,
      0.38590317869523894,
      0.37501803820455937,
      0.3532963607202724
    ],
    [
      0.3438162644067244,
      0.4128836250062966,
      0.4244976556238813,
      0.37198631617668254,
      0.45948874123907824,
      0.4095536954966148,
      0.3922816726118914,
      0.4523445528405219,
      0.4140314641957821,
      0.45344365052599067,
      0.3770521878022737,
      0.38071688798450665,
      0.44120001445966794,
      0.3986421618721634,
      0.39924446796625324,
      0.4177165916238761,
      0.4312889096146353,
      0.4152548358555286,
      0.41213920239192303,
      0.4334474287871166,
      0.4681984561855199,
      0.413199481385474,
      0.4951164390642957,
      0.4704876330704326,
      0.45726168791659827,
      0.3467234060311313,
      0.0,
      0.4110651426644787,
      0.44894273529575557
    ],
    [
      0.3143686134141983,
      0.40517505644170315,
      0.39061668913433056,
      0.410680837487331,
      0.43941312607803673,
      0.36534919110159914,
      0.3468620314255988,
      0.49967663131654194,
      0.42008800862693474,
      0.41374527548257034,
      0.36479341549253674,
      0.2970913363572707,
      0.3912350367608004,
      0.3713302598710009,
      0.4325885208274014,
      0.4061054140783409,
      0.39252613179963736,
      0.43794442861617044,
      0.3588604036331513,
      0.36891696697878706,
      0.4511732498660992,
      0.3776763928964888,
      0.39585442326461395,
      0.43616021128786997,
      0.39334100313958165,
      0.3207019269317162,
      0.40222429917933966,
      0.0,
      0.3835834870323931
    ],
    [
      0.28781147603844937,
      0.36728321136392017,
      0.3997710516853472,
      0.3607977168659511,
      0.3884081454626451,
      0.3520100573174667,
      0.36878785395908653,
      0.3975805122828395,
      0.3941697814914471,
      0.3707108611984662,
      0.3950955756594394,
      0.36338443466004966,
      0.3892217927986119,
      0.38068270139236304,
      0.4015507268588281,
      0.3784762738714156,
      0.397982723645512,
      0.3466639256334849,
      0.3494498460778299,
      0.3722078100081627,
      0.3986218443045646,
      0.43359711805043544,
      0.3868150252719589,
      0.3930694403413659,
      0.38084359802555445,
      0.32200393900329805,
      0.3901164252487197,
      0.3568828354250664,
      0.0
    ]
  ],
  "row_avgs": [
    0.21689591881314443,
    0.3741608702611802,
    0.4130687244454863,
    0.326252831386701,
    0.3738772565723966,
    0.3831130003062989,
    0.298143694030166,
    0.3737548598674535,
    0.4129750774951192,
    0.4200046934497383,
    0.47326983181203847,
    0.27468368650596764,
    0.3878814293263343,
    0.41525582594139304,
    0.42666339041113005,
    0.40737681374551127,
    0.3551513375832528,
    0.4513155088614877,
    0.3376622746490729,
    0.40285603769860445,
    0.3761289017735517,
    0.39287781669027144,
    0.42582289730908857,
    0.36672931902925415,
    0.4102459994114687,
    0.3704946835037524,
    0.4197151895748248,
    0.39243151316150154,
    0.37585702514079566
  ],
  "col_avgs": [
    0.3407391324542063,
    0.40794987500941354,
    0.41755759819289173,
    0.3937176620461202,
    0.3873292672775284,
    0.3606971746415392,
    0.35134902982507404,
    0.43158936863737607,
    0.4248212882915135,
    0.38331856917146423,
    0.4029303902182626,
    0.2818098382027281,
    0.4166080064205784,
    0.39638456443485826,
    0.40151935487265444,
    0.38604940247988967,
    0.3862882862778495,
    0.40589453472433884,
    0.3881004496706611,
    0.3973865037990246,
    0.4031818036401282,
    0.33937018369495003,
    0.3720891666687507,
    0.37715135466914906,
    0.37107966933903513,
    0.3320196818287914,
    0.36679217394676566,
    0.3761017608378822,
    0.35484031748356104
  ],
  "combined_avgs": [
    0.27881752563367534,
    0.3910553726352969,
    0.41531316131918905,
    0.35998524671641063,
    0.3806032619249625,
    0.37190508747391904,
    0.32474636192762,
    0.4026721142524148,
    0.4188981828933164,
    0.4016616313106013,
    0.43810011101515056,
    0.27824676235434787,
    0.40224471787345634,
    0.4058201951881256,
    0.4140913726418922,
    0.39671310811270044,
    0.37071981193055115,
    0.42860502179291327,
    0.362881362159867,
    0.40012127074881454,
    0.38965535270683993,
    0.3661240001926107,
    0.3989560319889196,
    0.3719403368492016,
    0.39066283437525195,
    0.3512571826662719,
    0.3932536817607952,
    0.3842666369996919,
    0.36534867131217835
  ],
  "gppm": [
    608.3042535897765,
    602.0915904117991,
    596.1491068754676,
    603.6796203972506,
    608.4750872289459,
    622.5415974722239,
    621.994755660123,
    586.4046096985609,
    593.1350685118049,
    612.5219004352737,
    604.5909867578285,
    661.7341872214522,
    597.1989524308193,
    607.9055599717876,
    604.6570621390383,
    609.3960473201054,
    605.1912411820134,
    604.5078345692617,
    608.7068517268441,
    601.8588958742747,
    603.1569783908266,
    635.5387609857216,
    616.5264276427694,
    615.2808497996856,
    617.8633967764487,
    633.1466271649182,
    621.6485515366081,
    618.1851301831257,
    628.3580239813782
  ],
  "gppm_normalized": [
    1.4703156965255868,
    1.4120601628875171,
    1.3990828211503177,
    1.4205136333481836,
    1.4347096578948562,
    1.467121767434977,
    1.4639149827966786,
    1.3793150125264935,
    1.3896204411472415,
    1.438286368445066,
    1.4201743900067836,
    1.5647450684606508,
    1.4053043504340157,
    1.4280559028284927,
    1.415550229243626,
    1.4271886488135475,
    1.4184676883256837,
    1.4135423845244155,
    1.4247391545966026,
    1.4198431842983807,
    1.4056226696013696,
    1.4901305669885236,
    1.4385597735455344,
    1.44849388167505,
    1.4504229660447197,
    1.4905096149307266,
    1.453282359549991,
    1.449692748822016,
    1.471011332575319
  ],
  "token_counts": [
    675,
    403,
    416,
    436,
    480,
    456,
    430,
    452,
    399,
    429,
    422,
    532,
    453,
    418,
    384,
    385,
    398,
    367,
    383,
    466,
    347,
    394,
    358,
    460,
    422,
    432,
    368,
    403,
    379,
    618,
    437,
    415,
    504,
    453,
    416,
    559,
    458,
    426,
    372,
    401,
    406,
    460,
    425,
    437,
    433,
    470,
    386,
    427,
    439,
    425,
    394,
    391,
    430,
    407,
    408,
    381,
    409,
    381
  ],
  "response_lengths": [
    3072,
    2545,
    2434,
    2875,
    2637,
    2431,
    3304,
    2631,
    2582,
    2208,
    2434,
    2421,
    2777,
    2503,
    2534,
    2568,
    2822,
    2217,
    2519,
    2532,
    2413,
    2212,
    2262,
    2461,
    2503,
    2323,
    2272,
    2245,
    2183
  ]
}