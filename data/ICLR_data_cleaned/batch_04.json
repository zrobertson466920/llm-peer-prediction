[
  {
    "input": "Published as a conference paper at ICLR 2023\n\nMETA-LEARNING IN GAMES\n\nKeegan Harris∗ Carnegie Mellon University keeganh@cs.cmu.edu\n\nIoannis Anagnostides∗ Carnegie Mellon University ianagnos@cs.cmu.edu\n\nGabriele Farina FAIR, Meta AI gfarina@meta.com\n\nMikhail Khodak Carnegie Mellon University mkhodak@cs.cmu.edu\n\nZhiwei Steven Wu Carnegie Mellon University zhiweiw@cs.cmu.edu\n\nTuomas Sandholm Carnegie Mellon University Strategy Robot, Inc. Optimized Markets, Inc. Strategic Machine, Inc. sandholm@cs.cmu.edu\n\nABSTRACT\n\nIn the literature on game-theoretic equilibrium finding, focus has mainly been on solving a single game in isolation. In practice, however, strategic interactions— ranging from routing problems to online advertising auctions—evolve dynamically, thereby leading to many similar games to be solved. To address this gap, we introduce meta-learning for equilibrium finding and learning to play games. We establish the first meta-learning guarantees for a variety of fundamental and well-studied classes of games, including two-player zero-sum games, general-sum games, and Stackelberg games. In particular, we obtain rates of convergence to different game-theoretic equilibria that depend on natural notions of similarity between the sequence of games encountered, while at the same time recovering the known single-game guarantees when the sequence of games is arbitrary. Along the way, we prove a number of new results in the single-game regime through a simple and unified framework, which may be of independent interest. Finally, we evaluate our meta-learning algorithms on endgames faced by the poker agent Libratus against top human professionals. The experiments show that games with varying stack sizes can be solved significantly faster using our meta-learning techniques than by solving them separately, often by an order of magnitude.\n\n1\n\nINTRODUCTION\n\nResearch on game-theoretic equilibrium computation has primarily focused on solving a single game in isolation. In practice, however, there are often many similar games which need to be solved. One use-case is the setting where one wants to find an equilibrium for each of multiple game variations— for example poker games where the players have various sizes of chip stacks. Another use-case is strategic interactions that evolve dynamically: in online advertising auctions, the advertiser’s value for different keywords adapts based on current marketing trends (Nekipelov et al., 2015); routing games— be it Internet routing or physical transportation—reshape depending on the topology and the cost functions of the underlying network (Hoefer et al., 2011); and resource allocation problems (Johari and Tsitsiklis, 2004) vary based on the values of the goods/services. Successful agents in such complex decentralized environments must effectively learn how to incorporate past experience from previous strategic interactions in order to adapt their behavior to the current and future tasks.\n\nMeta-learning, or learning-to-learn (Thrun and Pratt, 1998), is a common formalization for machine learning in dynamic single-agent environments. In the meta-learning framework, a learning agent faces a sequence of tasks, and the goal is to use knowledge gained from previous tasks in order to improve performance on the current task at hand. Despite rapid progress in this line of work, prior results have not been tailored to tackle multiagent settings. This begs the question: Can players obtain provable performance improvements when meta-learning across a sequence of games? We answer this\n\n∗Equal contribution.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nquestion in the affirmative by introducing meta-learning for equilibrium finding and learning to play games, and providing the first performance guarantees in a number of fundamental multiagent settings.\n\n1.1 OVERVIEW OF OUR RESULTS\n\nOur main contribution is to develop a general framework for establishing the first provable guarantees for meta-learning in games, leading to a comprehensive set of results in a variety of well-studied multiagent settings. In particular, our results encompass environments ranging from two-player zero-sum games with general constraint sets (and multiple extensions thereof), to general-sum games and Stackelberg games. See Table 1 for a summary of our results. Our refined guarantees are parameterized based on natural similarity metrics between the sequence of games. For example, in zero-sum games we obtain last-iterate rates that depend on the variance of the Nash equilibria (Theorem 3.2); in potential games based on the deviation of the potential functions (Theorem 3.4); and in Stackelberg games our regret bounds depend on the similarity of the leader’s optimal commitment in hindsight (Theorem 3.8). All of these measures are algorithm-independent, and tie naturally to the underlying game-theoretic solution concepts.\n\nImportantly, our algorithms are agnostic to how similar the games are, but are nonetheless specifically designed to adapt to the similarity. Our guarantees apply under a broad class of no-regret learning algorithms, such as optimistic mirror descent (OMD) (Chiang et al., 2012; Rakhlin and Sridharan, 2013b), with the important twist that each player employs an additional regret minimizer for meta-learning the parameterization of the base-learner; the latter component builds on the meta-learning framework of Khodak et al. (2019). For example, in zero-sum games we leverage an initialization-dependent RVU bound (Syrgkanis et al., 2015) in order to meta-learn the initialization of OMD across the sequences of games, leading to per-game convergence rates to Nash equilibria that closely match our refined lower bound (Theorem 3.3). More broadly, in the worst-case—i.e., when the sequence of games is arbitrary— we recover the near-optimal guarantees known for static games, but as the similarity metrics become more favorable we establish significant gains in terms of convergence to different notions of equilibria.\n\nAlong the way, we also obtain new insights and results even from a single-game perspective, including convergence rates of OMD and the extra-gradient method in Hölder continuous variational inequalities (Rakhlin and Sridharan, 2013a), and certain nonconvex-nonconcave problems such as those considered by (Diakonikolas et al., 2021) and stochastic games. Further, our analysis is considerably simpler than prior techniques and unifies several prior results. Finally, in Section 4 we evaluate our techniques on a series of poker endgames faced by the poker agent Libratus (Brown and Sandholm, 2018) against top human professionals. The experiments show that our meta-learning algorithms offer significant gains compared to solving each game in isolation, often by an order of magnitude.\n\nTable 1: A summary of our key theoretical results on meta-learning in games.\n\nClass of games\n\nSpecific problem\n\nKey results\n\nLocation\n\nZero-sum games\n\nBilinear saddle-point problems Hölder continuous VIs Lower bound\n\nTheorems 3.1 and 3.2 Theorems C.17 and C.34 Appendices C.2 and C.6 Theorem 3.3\n\nSection 3.1\n\nSection 3.1\n\nGeneral-sum games\n\nPotential games (Coarse) Correlated equilibria Approximately optimal welfare\n\nTheorem 3.4 Theorems D.7 and D.10 Theorem 3.6\n\nSection 3.2 Appendices D.2 and D.3 Section 3.2\n\nStackelberg games\n\nSecurity games\n\nTheorem 3.8\n\nSection 3.3\n\n1.2 RELATED WORK\n\nWhile most prior work on learning in games posits that the underlying game remains invariant, there is ample motivation for studying games that gradually change over time, such as online advertising (Nekipelov et al., 2015; Lykouris et al., 2016; Nisan and Noti, 2017) or congestion games (Hoefer et al., 2011; Bertrand et al., 2020; Meigs et al., 2017). Indeed, a number of prior works study the performance of learning algorithms in time-varying zero-sum games (Zhang et al., 2022b; Fiez et al., 2021b; Duvocelle et al., 2022; Cardoso et al., 2019); there, it is natural to espouse dynamic notions of regret (Yang et al., 2016; Zhao et al., 2020). A work closely related to ours is the recent paper by Zhang et al. (2022b), which provides regret bounds in time-varying bilinear saddle-point problems parameter-\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nized by the similarity of the payoff matrices and the equilibria of those games. In contrast to our metalearning setup, they study a more general setting in which the game can change arbitrarily from roundto-round. While our problem can be viewed a special type of a time-varying game in which the boundaries between different games are fixed and known, algorithms designed for generic time-varying games will not perform as well in our setting, as they do not utilize this extra information. As a result, we view these results as complementary to ours. For a more detailed discussion, see Appendix A.\n\nAn emerging paradigm for modeling such considerations is meta-learning, which has gained increasing popularity in the machine learning community in recent years; for a highly incomplete set of pointers, we refer to (Balcan et al., 2015b; Al-Shedivat et al., 2018; Finn et al., 2017; 2019; Balcan et al., 2019; Li et al., 2017; Chen et al., 2022), and references therein. Our work constitutes the natural coalescence of meta-learning with the line of work on (decentralized) online learning in games. Although, as we pointed out earlier, learning in dynamic games has already received considerable attention, we are the first (to our knowledge) to formulate and address such questions within the metalearning framework; c.f., see (Kayaalp et al., 2020; 2021; Li et al., 2022). Finally, our methods may be viewed within the algorithms with predictions paradigm (Mitzenmacher and Vassilvitskii, 2020): we speed up equilibrium computation by learning to predict equilibria across multiple games, with the task-similarity being the measure of prediction quality. For further related work, see Appendix A.\n\n2 OUR SETUP: META-LEARNING IN GAMES\n\nNotation We use boldface symbols to represent vectors and matrices. Subscripts are typically reserved to indicate the player, while superscripts usually correspond to the iteration or the index of the task. We let N := {1, 2, . . . , } be the set of natural numbers. For T ∈ N, we use the shorthand notation [[T ]] := {1, 2, . . . , T }. For a nonempty convex and compact set X , we denote by ΩX its l2-diameter: ΩX := maxx,x′∈X ∥x − x′∥2. Finally, to lighten the exposition we use the O(·) notation to suppress factors that depend polynomially on the natural parameters of the problem.\n\nThe general setup We consider a setting wherein players interact in a sequence of T repeated games (or tasks), for some N ∋ T ≫ 1. Each task itself consists of m ∈ N iterations. Any fixed task t corresponds to a multiplayer game G(t) between a set [[n]] of players, with n ≥ 2; it is assumed for simplicity in the exposition that n remains invariant across the games, but some of our results apply more broadly. Each player k ∈ [[n]] selects a strategy xk from a convex and compact set of strategies Xk ⊆ Rdk with nonempty relative interior. For a given joint strategy profile x := (x1, . . . , xn) ∈ ×n k=1 Xk, there is a multilinear utility function uk : x (cid:55)→ ⟨xk, uk(x−k)⟩ for each player k, where x−k := (x1, . . . , xk−1, xk+1, . . . , xn). We will also let L > 0 be a Lipschitz parameter of each −k ∈×k′̸=k Xk′, game, in the sense that for any player k ∈ [[n]] and any two strategy profiles x−k, x′\n\n∥uk(x−k) − uk(x′\n\n−k)∥2 ≤ L∥x−k − x′\n\n−k∥2.\n\n(1)\n\nHere, we use the l2-norm for convenience in the analysis; (1) can be translated to any equivalent norm. Finally, for a joint strategy profile x ∈ ×n k=1 Xk, the social welfare is defined as SW(x) := (cid:80)n k=1 Xk SW(x) denotes the optimal social welfare. A concrete example encompassed by our setup is that of extensive-form games. More broadly, it captures general games with concave utilities (Rosen, 1965; Hsieh et al., 2021).\n\nk=1 uk(x), so that OPT := maxx∈×n\n\nOnline learning in games Learning proceeds in an online fashion as follows. At every iteration i ∈ [[m]] of some underlying game t, each player k ∈ [[n]] has to select a strategy x(t,i) ∈ Xk. Then, in the full information setting, the player observes as feedback the utility corresponding to the other players’ strategies at iteration i; namely, u(t,i) −k ) ∈ Rdk . For convenience, we will assume that ∥uk(x(t,i) −k )∥∞ ≤ 1. The canonical measure of performance in online learning is that of external regret, comparing the performance of the learner with that of the optimal fixed strategy in hindsight: Definition 2.1 (Regret). Fix a player k ∈ [[n]] and some game t ∈ [[T ]]. The (external) regret of player k is defined as\n\n:= uk(x(t,i)\n\nk\n\nk\n\nReg(t,m)\n\nk\n\n:= max ̊x(t)\n\nk ∈Xk\n\n(cid:40) m (cid:88)\n\n⟨ ̊x(t)\n\nk , u(t,i)\n\nk\n\n(cid:41)\n\n⟩\n\n− ⟨x(t,i)\n\nk\n\n, u(t,i)\n\nk\n\n⟩.\n\ni=1\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nWe will let ̊x(t) k be an optimum-in-hindsight strategy for player k in game t; ties are broken arbitrarily, but according to a fixed rule (e.g., lexicographically). In the meta-learning setting, our goal will be to optimize the average performance—typically measured in terms of convergence to different game-theoretic equilibria—across the sequence of games.\n\nOptimistic mirror descent Suppose that Rk : Xk → R is a 1-strongly convex regularizer with respect to a norm ∥ · ∥. We let BRk (xk ∥ x′ k⟩ denote the Bregman divergence induced by Rk, where x′ k is in the relative interior of Xk. Optimistic mirror descent (OMD) (Chiang et al., 2012; Rakhlin and Sridharan, 2013b) is parameterized by a prediction m(t,i)\n\n∈ Rdk and a learning rate η > 0, and is defined at every iteration i ∈ N as follows.\n\nk) := Rk(xk) − Rk(x′\n\nk) − ⟨∇Rk(x′\n\nk), xk − x′\n\nk\n\nx(t,i)\n\nk\n\nˆx(t,i)\n\nk\n\n:= arg max xk∈Xk\n\n:= arg max ˆxk∈Xk\n\n(cid:26)\n\n(cid:26)\n\n⟨xk, m(t,i)\n\nk\n\n⟩ −\n\n1 η\n\nBRk (xk ∥ ˆx(t,i−1)\n\nk\n\n(cid:27) )\n\n,\n\n⟨ ˆxk, u(t,i)\n\nk\n\n⟩ −\n\n1 η\n\nBRk ( ˆxk ∥ ˆx(t,i−1)\n\nk\n\n)⟩\n\n(cid:27)\n\n.\n\nFurther, ˆx(1,0) regularization, Rk(xk) := 1\n\n:= arg min ˆxk∈Xk\n\nk\n\n2 ∥xk∥2\n\nRk( ˆxk) =: x(1,0) 2, we will refer to OMD as optimistic gradient descent (OGD).\n\n:= uk(x(t,0)\n\n, and m(t,1)\n\n−k ). Under Euclidean\n\nk\n\nk\n\n3 META-LEARNING HOW TO PLAY GAMES\n\nIn this section, we present our main theoretical results: provable guarantees for online and decentralized meta-learning in games. We commence with zero-sum games in Section 3.1, and we then transition to general-sum games (Section 3.2) and Stackelberg (security) games (Section 3.3).\n\n3.1 ZERO-SUM GAMES\n\nWe first highlight our results for bilinear saddle-point problems (BSPPs), which take the form minx∈X maxy∈Y x⊤Ay, where A ∈ Rdx×dy is the payoff matrix of the game. A canonical application for this setting is on the solution of zero-sum imperfect-information extensive-form games (Romanovskii, 1962; Koller and Megiddo, 1992), as we explore in our experiments (Section 4). Next we describe a number of extensions to gradually more general settings, and we conclude with our lower bound (Theorem 3.3). The proofs from this subsection are included in Appendix C.\n\nx := 1\n\nT minx∈X\n\nWe first derive a refined meta-learning convergence guarantee for the average of the players’ strategies. Below, we denote by V 2 2 the task similarity metric for player x, written in terms of the optimum-in-hindsight strategies; analogous notation is used for player y. Theorem 3.1 (Informal; Detailed Version in Corollary C.2). Suppose that both players employ OGD with a suitable (fixed) learning rate and follow the leader over previous optimum-in-hindsight strategies for the initialization. Then, the game-average duality gap of the players’ average strategies is bounded by\n\nt=1 ∥ ̊x(t) − x∥2\n\n(cid:80)T\n\n1 T\n\nT (cid:88)\n\nt=1\n\n(cid:16)\n\n1 m\n\nReg(t,m)\n\nx\n\n+ Reg(t,m)\n\ny\n\n(cid:17)\n\n≤\n\n2L m\n\n(cid:0)V 2\n\nx + V 2\n\ny\n\n(cid:1) +\n\n8L(1 + log T ) mT\n\n(cid:0)Ω2\n\nX + Ω2\n\nY\n\n(cid:1) .\n\n(2)\n\nHere, the second term in the right-hand side of (2) becomes negligible for a large number of games T , while the first term depends on the task similarity measures. For any sequence of games, Theorem 3.1 nearly matches the lower bound in the single-task setting (Daskalakis et al., 2015), but our guarantee can be significantly better when V 2 y ≪ 1. To achieve this, the basic idea is to use—on top of OGD—a “meta” regret minimization algorithm that, for each player, learns a sequence of initializations by taking the average of the past optima-in-hindsight, which is equivalent to follow the leader (FTL) over the regret upper-bounds of the within-task algorithm; see Algorithm 1 (in Appendix B) for pseudocode of the meta-version of OGD we consider. Similar results can be obtained more broadly for OMD (c.f., see Appendices D.2 and D.3). We also obtain analogous refined bounds for the individual regret of each player (Corollary C.4).\n\nx , V 2\n\nOne caveat of Theorem 3.1 is that the underlying task similarity measure could be algorithmdependent, as the optimum-in-hindsight for each player could depend on the other player’s\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nbehavior. To address this, we show that if the meta-learner can initialize using Nash equilibria (NE) (recall Definition C.5) from previously seen games, the game-average last-iterate rates gracefully decrease with the similarity of the Nash equilibria of those games. More precisely, if z := (x, y) ∈ X × Y =: Z, we let V 2 2, where z(t,⋆) is any Nash equilibrium of the t-th game. As we point out in the sequel, we also obtain results under a more favorable notion of task similarity that does not depend on the worst sequence of NE. Theorem 3.2 (Informal; Detailed Version in Theorem C.8). When both players employ OGD with a suitable (fixed) learning rate and FTL over previous NE strategies for the initialization, then\n\nT maxz(1,⋆),...,z(T ,⋆) minz∈Z\n\nt=1 ∥z(t,⋆) − z∥2\n\nNE := 1\n\n(cid:80)T\n\n ̄m ≤\n\nNE\n\n2V 2 ε2 +\n\n8(1 + log T ) T ε2\n\n(cid:0)Ω2\n\nX + Ω2\n\nY\n\n(cid:1)\n\niterations suffice to reach an O(ε)-approximate Nash equilibrium on average across the T games.\n\nδ(Ω2\n\nX + Ω2\n\nTheorem 3.2 recovers the optimal m−1/2 rates for OGD (Golowich et al., 2020a;b) under an arbitrary sequence of games, but offers substantial gains in terms of the average iteration complexity when the Nash equilibria of the games are close. For example, when they lie within a ball of l2-diameter (cid:113) Y ), for some δ ∈ (0, 1], Theorem 3.2 improves upon the rate of OGD by at least a multiplicative factor of 1/δ as T → ∞. While generic—roughly speaking, randomly perturbed—zero-sum (normal-form) games have a unique Nash equilibrium (van Damme, 1987), the worst-case NE similarity metric used in Theorem 3.2 can be loose under multiplicity of equilibria. For that reason, in Appendix C.1.2 we further refine Theorem 3.2 using the most favorable sequence of Nash equilibria; this requires that players know each game after its termination, which is arguably a well-motivated assumption in some applications. We further remark that Theorem 3.2 can be cast in terms of the similarity V 2 NE, using the parameterization of Theorem 3.1. Finally, since the base-learner can be viewed as an algorithm with predictions—the number of iterations to compute an approximate NE is smaller if the Euclidean error of a prediction of it (the initialization) is small—Theorem 3.2 can also be viewed as learning these predictions (Khodak et al., 2022) by targeting that error measure.\n\ny , instead of V 2\n\nx + V 2\n\nExtensions Moving beyond bilinear saddle-point problems, we extend our results to gradually broader settings. First, in Appendix C.2 we apply our techniques to general variational inequality problems under a Lipschitz continuous operator for which the so-called MVI property (Mertikopoulos et al., 2019) holds. Thus, Theorems 3.1 and 3.2 are extended to settings such as smooth convex-concave games and zero-sum polymatrix (multiplayer) games (Cai et al., 2016). Interestingly, extensions are possible even under the weak MVI property (Diakonikolas et al., 2021), which captures certain “structured” nonconvex-nonconcave games. In a similar vein, we also study the challenging setting of Shapley’s stochastic games (Shapley, 1953) (Appendix C.5). There, we show that there exists a time-varying—instead of constant—but non-vanishing learning rate schedule for which OGD reaches minimax equilibria, thereby leading to similar extensions in the meta-learning setting. Next, we relax the underlying Lipschitz continuity assumption underpinning the previous results by instead imposing only α-Hölder continuity (recall Definition C.32). We show that in such settings OGD enjoys a rate of m−α/2 (Theorem C.34), which is to our knowledge a new result; in the special case where α = 1, we recover the recently established m−1/2 rates. Finally, while we have focused on the OGD algorithm, our techniques apply to other learning dynamics as well. For example, in Appendix C.7 we show that the extensively studied extra-gradient (EG) algorithm (Korpelevich, 1976) can be analyzed in a unifying way with OGD, thereby inheriting all of the aforementioned results under OGD; this significantly broadens the implications of (Mokhtari et al., 2020), which only applied in certain unconstrained problems. Perhaps surprisingly, although EG is not a no-regret algorithm, our analysis employs a regret-based framework using a suitable proxy for the regret (see Theorem C.35).\n\nLower bound We conclude this subsection with a lower bound, showing that our guarantee in Theorem 3.1 is essentially sharp under a broad range of our similarity measures. Our result significantly refines the single-game lower bound of Daskalakis et al. (2015) by constructing an appropriate distribution over sequences of zero-sum games. Theorem 3.3 (Informal; Precise Version in Theorem C.39). For any ε > 0, there exists a distribution over sequences of T zero-sum games, with a sufficiently large T = T (ε), such that\n\n1 T\n\nT (cid:88)\n\nt=1\n\nE[Reg(t,m)\n\nx\n\n+ Reg(t,m)\n\ny\n\n] ≥\n\n1 2\n\n5\n\n(cid:0)V 2\n\nx + V 2\n\ny\n\n(cid:1) − ε =\n\n1 2\n\nV 2\n\nNE − ε.\n\nPublished as a conference paper at ICLR 2023\n\n3.2 GENERAL-SUM GAMES\n\nIn this subsection, we switch our attention to general-sum games. Here, unlike zero-sum games, no-regret learning algorithms are instead known to generally converge—in a time-average sense—to correlated equilibrium concepts, which are more permissive than the Nash equilibrium. Nevertheless, there are structured classes of general-sum games for which suitable dynamics do reach Nash equilibria; perhaps the most notable example being that of potential games. In this context, we first obtain meta-learning guarantees for potential games, parameterized by the similarity of the potential functions. Then, we derive meta-learning algorithms with improved guarantees for convergence to correlated and coarse correlated equilibria. Finally, we conclude this subsection with improved guarantees of convergence to near-optimal—in terms of social welfare—equilibria. Proofs from this subsection are included in Appendices B and D.\n\nPotential games A potential game is endowed with the additional property of admitting a potential: a player-independent function that captures the player’s benefit from unilaterally deviating from any given strategy profile (Definition D.2). In our meta-learning setting, we posit a sequence of potential games (Φ(t))1≤t≤t, each described by its potential function. Unlike our approach in Section 3.1, a central challenge here is that the potential function is in general nonconcave/nonconvex, precluding standard regret minimization approaches. Instead, we find that by initializing at the previous last-iterate the dynamics still manage to adapt based on the similarity V∆ := 1 t=1 ∆(Φ(t), Φ(t+1)), where ∆(Φ, Φ′) := maxx(Φ(x) − Φ′(x)), which captures the deviation of the potential functions. This initialization has the additional benefit of being agnostic to the boundaries of different tasks. Unlike our results in Section 3.1, the following guarantee applies even for vanilla (i.e., non-optimistic) projected gradient descent (GD). Theorem 3.4 (Informal; Detailed Version in Corollary D.5). GD with suitable parameterization requires O (cid:0) V∆ (cid:1) iterations to reach an ε-approximate Nash equilibrium on average across the T potential games, where maxx,t |Φ(t)(x)| ≤ Φmax.\n\nε2 + Φmax\n\n(cid:80)T −1\n\nε2T\n\nT\n\nTheorem 3.4 matches the known rate of GD for potential games in the worst case, but offers substantial gains in terms of the average iteration complexity when the games are similar. For example, if |Φ(t)(x) − Φ(t−1)(x)| ≤ α, for all x ∈ ×n k=1 Xk and t ≥ 2, then O(α/ε2) iterations suffice to reach an ε-approximate NE on an average game, as T → +∞. Such a scenario may arise in, e.g., a sequence of routing games if the cost functions for each edge change only slightly between games.\n\nConvergence to correlated equilibria In contrast, for general games the best one can hope for is to obtain improved rates for convergence to correlated or coarse correlated equilibria (Hart and MasColell, 2000; Blum and Mansour, 2007). It is important to stress that learning correlated equilibria is fundamentally different than learning Nash equilibria—which are product distributions. For example, for the former any initialization—which is inevitably a product distribution in the case of uncoupled dynamics—could fail to exploit the learning in the previous task (Proposition D.1): unlike Nash equilibria, correlated equilibria (in general) cannot be decomposed for each player, thereby making uncoupled methods unlikely to adapt to the similarity of CE. Instead, our task similarity metrics depend on the optima-in-hindsight for each player. Under this notion of task similarity, we obtain task-average guarantees for CCE by meta-learning the initialization (by running FTL) and the learning rate (by running the EWOO method of Hazan et al. (2007) over a sequence of regret upper bounds) of optimistic hedge (Daskalakis et al., 2021) (Theorem D.7)—OMD with entropic regularization. Similarly, to obtain guarantees for CE, we use the no-swap-regret construction of Blum and Mansour (2007) in conjuction with the logarithmic barrier (Anagnostides et al., 2022a) (Theorem D.10).\n\n3.2.1 SOCIAL WELFARE GUARANTEES\n\nWe conclude this subsection with meta-learning guarantees for converging to near-optimal equilibria (Theorem 3.6). Let us first recall the following central definition. Definition 3.5 (Smooth games (Roughgarden, 2015)). A game G is (λ, μ)-smooth, with λ, μ > 0, if there exists a strategy profile x⋆ ∈×n\n\nk=1 Xk such that for any x ∈×n\n\nk=1 Xk,\n\nn (cid:88)\n\nk=1\n\nuk(x⋆\n\nk, x−k) ≥ λOPT − μSW(x),\n\n(3)\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nwhere OPT is the optimal social welfare and SW(x) is the social welfare of joint strategy profile x.\n\nSmooth games capture a number of important applications, including network congestion games (Awerbuch et al., 2013; Christodoulou and Koutsoupias, 2005) and simultaneous auctions (Christodoulou et al., 2016; Roughgarden et al., 2017) (see Appendix B for additional examples); both of those settings are oftentimes non-static in real-world applications, thereby motivating our meta-learning considerations. In this context, we assume that there is a sequence of smooth games (G(t))1≤t≤T , each of which is (λ(t), μ(t))-smooth (Definition 3.5). Theorem 3.6 (Informal; Detailed Version in Theorem B.11). If all players use OGD with suitable parameterization in a sequence of T games (G(t))1≤t≤T , each of which is (λ(t), μ(t))-smooth, then\n\n1 mT\n\nT (cid:88)\n\nm (cid:88)\n\nt=1\n\ni=1\n\nSW(x(t,i)) ≥\n\n1 T\n\nT (cid:88)\n\nt=1\n\nλ(t)\n\n1 + μ(t) OPT(t) −\n\n√\n\n2L\n\nn − 1 m\n\nn (cid:88)\n\nk=1\n\nV 2\n\nk − (cid:101)O\n\n(cid:18) 1 mT\n\n(cid:19)\n\n,\n\n(4)\n\nwhere OPT(t) is the optimal social welfare attainable at game G(t) and (cid:101)O(·) hides logarithmic terms.\n\nThe first term in the right-hand side of (4) is the average robust PoA in the sequence of games, while the third term vanishes as T → ∞. The orchestrated learning dynamics reach approximately optimal equilibria much faster when the underlying task similarity is small; without meta-learning one would instead obtain the m−1 rate known from the work of Syrgkanis et al. (2015). Theorem 3.6 is established by first providing a refined guarantee for the sum of the players regrets (Theorem B.3), and then translating that guarantee in terms of the social welfare using the smoothness condition for each game (Proposition B.10). Our guarantees are in fact more general, and apply for any suitable linear combination of players’ utilities (see Corollary B.12).\n\n3.3 STACKELBERG (SECURITY) GAMES\n\nTo conclude our theoretical results, we study meta-learning in repeated Stackelberg games. Following the convention of Balcan et al. (2015a), we present our results in terms of Stackelberg security games, although our results apply to general Stackelberg games as well (see (Balcan et al., 2015a, Section 8) for details on how such results extend).\n\nStackelberg security games A repeated Stackelberg security game is a sequential interaction between a defender and m attackers. In each round, the defender commits to a mixed strategy over d targets to protect, which induces a coverage probability vector x ∈ ∆d over targets. After having observed coverage probability vector, the attacker best responds by attacking some target b(x) ∈ [[d]] in order to maximize their utility in expectation. Finally, the defender’s utility is some function of their coverage probability vector x and the target attacked b(x).\n\nIt is a well-known fact that no-regret learning in repeated Stackelberg games is not possible without any prior knowledge about the sequence of followers (Balcan et al., 2015a, Section 7), so we study the setting in which each attacker belongs to one of k possible attacker types. We allow sequence of attackers to be adversarially chosen from the k types, and assume the attacker’s type is revealed to the leader after each round. We adapt the methodology of Balcan et al. (2015a) to our setting by meta-learning the initialization and learning rate of the multiplicative weights update (henceforth MWU) run over a finite (but exponentially-large) set of extreme points E ⊂ ∆d.1 Each point x ∈ E corresponds to a leader mixed strategy, and E can be constructed in such a way that it will always contain a mixed strategy which is arbitrarily close to the optima-in-hindsight for each task.2\n\nOur results are given in terms of guarantees on the task-average Stackelberg regret, which measures the difference in utility between the defender’s deployed sequence of mixed strategies and the optima-in-hindsight, given that the attacker best responds. Definition 3.7 (Stackelberg Regret). Denote attacker f (t,i)’s best response to mixed strategy x as bf (t,i) (x). The Stackelberg regret of the attacker in a repeated Stackelberg security game t is\n\nStackReg(t,m)( ̊x(t)) =\n\nm (cid:88)\n\n⟨ ̊x(t), u(t)(bf (t,i) ( ̊x(t)))⟩ − ⟨x(t,i), u(t)(bf (t,i) (x(t,i)))⟩.\n\n1This is likely unavoidable, as Li et al. (2016) show computing a Stackelberg strategy is strongly NP-Hard. 2For a precise definition of how to construct E, we point the reader to (Balcan et al., 2015a, Section 4).\n\ni=1\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nIn contrast to the standard notion of regret (Definition 2.1), Stackelberg regret takes into account the extra structure in the defender’s utility in Stackelberg games; namely that it is a function of the defender’s current mixed strategy (through the attacker’s best response).\n\nTheorem 3.8 (Informal; Detailed Version in Theorem E.1). Given a sequence of T repeated Stackelberg security games with d targets, k attacker types, and within-game time-horizon m, running MWU over the set of extreme points E as defined in Balcan et al. (2015a) with suitable initialization and sequence of learning rates achieves task-averaged expected Stackelberg regret\n\n1 T\n\nT (cid:88)\n\nt=1\n\nE[StackReg(t,m)] = O(\n\n(cid:112)H( ̄y)m) + oT (poly(m, |E|)),\n\nwhere the sequence of attackers in each task can be adversarially chosen, the expectation is with respect to the randomness of MWU, ̄y := 1 t=1 ̊y(t), where ̊y(t) is the optimum-in-hindsight distribution over mixed strategies in E for game t, H( ̄y) is the Shannon entropy of ̄y, and oT (1) suppresses terms which decay with T .\n\n(cid:80)T\n\nT\n\n(cid:112)m log |E|) H( ̄y) ≤ log |E|, so in the worst-case our algorithm asymptotically matches the O( performance of the algorithm of Balcan et al. (2015a). Entropy H( ̄y) is small whenever the same small set of mixed strategies are optimal for the sequence of T Stackelberg games. For example, if in each task the adversary chooses from s ≪ k attacker types who are only interested in attacking u ≪ d targets (unbeknownst to the meta-learner), H( ̄y) = O(s2u log(su)). In Stackelberg security games |E| = O((2d + kd2)ddk), so log |E| = O(d2k log(dk)). Finally, the distance between the set of optimal strategies does not matter, as ̄y is a categorical distribution over a discrete set of mixed strategies.\n\n4 EXPERIMENTS\n\nIn this section, we evaluate our meta-learning techniques in two River endgames that occurred in the Brains vs AI competition (Brown and Sandholm, 2018). We use the two public endgames that were released by the authors,3 denoted ‘Endgame A’ and ‘Endgame B,’ each corresponding to a zero-sum extensive-form game. For each of these endgames, we produced T := 200 individual tasks by varying the size of the stacks of each player according to three different task sequencing setups:4\n\n1. (random stacks) In each task we select stack sizes for the players by sampling uniformly at\n\nrandom a multiple of 100 in the range [1000, 20000].\n\n2. (sorted stacks) Task t ∈ {1, . . . , 200} corresponds to solving the endgame where the stack sizes\n\nare set to the amount t × 100 for each player.\n\n3. (alternating stacks) We sequence the stack amounts of the players as follows: in task 1, the stacks are set to 100; in task 2 to 200, 000; in task 3 to 200; in task 4 to 199, 900; and so on.\n\nFor each endgame, we tested the performance when both players (1) employ OGD while meta-learning the initialization (Theorem 3.1) with m(t,1) x = 0dx and m(t,1) y = 0dy , (2) employ OGD while setting the initialization equal to the last iterate of the previous task (see Remark B.8), and (3) use the vanilla initialization of OGD—i.e., the players treat each game separately. For each game, players run m := 1000 iterations. The l2 projection to the sequence-form polytope (Romanovskii, 1962; Koller and Megiddo, 1992)—the strategy set of each player in extensive-form games—required for the steps of OGD is implemented via an algorithm originally described by Gilpin et al. (2012), and further clarified in (Farina et al., 2022, Appendix B). We tried different learning rates for the players selected from the set {0.1, 0.01, 0.001}. Figure 1 illustrates our results for η := 0.01, while the others are deferred to Appendix F. In the table at the top of Figure 1 we highlight several parameters of the endgames including the board configuration, the dimensions of the players’ strategy sets—i.e., the sequences—and the number of nonzero elements in each payoff matrix. Because of the scale of the games, we used the Kronecker sparsification algorithm of Farina and Sandholm (2022, Technique A) in order to accelerate the training.\n\n3Obtained from https://github.com/Sandholm-Lab/LibratusEndgames. 4While in the general meta-learning setup it is assumed that the number of tasks is large but per-task data is\n\nlimited (i.e., T ≫ m), we found that setting T := 200 was already sufficient to see substantial benefits.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nBoard\n\nGame Endgame A J♠ K♠ 5♣ Q♠ 7♦ Endgame B 4♠ 8♥ 10♣ 9♥ 2♠\n\nPot\n\n3,700 500\n\nSequences Pl. 1 18,789 46,875\n\nPl. 2 19,237 47,381\n\nDecision Points Pl. 2 6,870 16,480\n\nPl. 1 6,710 16,304\n\nPayoff Matrix num. nonzeros 14,718,298 62,748,525\n\nMeta learning (avg. best-in-hindsight)\n\nMeta learning (last iterate)\n\nNo meta learning\n\nFigure 1: (Top) Parameters of each endgame. (Bottom) The task-averaged NE gap of the players’ average strategies across 200 tasks, 2 endgames, and 3 different stack orderings. Both players use OGD with η := 0.01. For the random stacks, we repeated each experiment 10 times with different random seeds. The plots show the mean (thick line) as well as the minimum and maximum values. We see that across all task sequencing setups, meta-learning the initialization (using either technique) leads to up to an order of magnitude better performance compared to vanilla OGD. When stacks are sorted, initializing to the last iterate of the previous game obtains the best performance, whereas when stacks are alternated or random, initializing according to Theorem 3.1 performs best.\n\n5 CONCLUSIONS AND FUTURE RESEARCH\n\nIn this paper, we introduced the study of meta-learning in games. In particular, we considered many of the most central game classes—including zero-sum games, potential games, general-sum multi-player games, and Stackelberg security games—and obtained provable performance guarantees expressed in terms of natural measures of similarity between the games. Experiments on several sequences of poker endgames that were actually played in the Brains vs AI competition (Brown and Sandholm, 2018) show that meta-learning the initialization improves performance even by an order of magnitude.\n\nOur results open the door to several exciting directions for future research, including meta-learning in other settings for which single-game results are known, such as general nonconvex-nonconcave min-max problems (Suggala and Netrapalli, 2020), the nonparametric regime (Daskalakis and Golowich, 2022), and partial feedback (such as bandit) models (Wei and Luo, 2018; Hsieh et al., 2022; Balcan et al., 2022; Osadchiy et al., 2022). Another interesting, yet challenging, avenue for future research would be to consider strategy sets that can vary across tasks.\n\n9\n\n150100150200Task20406080100Task-averageNashgapEndgameA[randomstacks]150100150200Task0204060EndgameA[sortedstacks]150100150200Task50100EndgameA[altern.stacks]1150100150200Task50100150Task-averageNashgapEndgameB[randomstacks]150100150200Task0255075100EndgameB[sortedstacks]150100150200Task50100EndgameB[altern.stacks]1Published as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nWe are grateful to the anonymous ICLR reviewers for valuable feedback. KH is supported by a NDSEG Fellowship. IA and TS are supported by NSF grants IIS-1901403 and CCF-1733556, and the ARO under award W911NF2010081. MK is supported by a Meta Research PhD Fellowship. ZSW is supported in part by the NSF grant FAI-1939606, a Google Faculty Research Award, a J.P. Morgan Faculty Award, a Meta Research Award, and a Mozilla Research Grant. The authors would like to thank Nina Balcan for helpful discussions throughout the course of the project. IA is grateful to Ioannis Panageas for insightful discussions regarding Appendix C.5.\n\nREFERENCES\n\nJacob D. Abernethy, Kevin A. Lai, Kfir Y. Levy, and Jun-Kun Wang. Faster rates for convex-concave games. In Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018, volume 75 of Proceedings of Machine Learning Research, pages 1595–1625. PMLR, 2018.\n\nAlekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. J. Mach. Learn. Res., 22: 98:1–98:76, 2021.\n\nMaruan Al-Shedivat, Trapit Bansal, Yura Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. In 6th International Conference on Learning Representations, ICLR 2018. OpenReview.net, 2018.\n\nIoannis Anagnostides, Gabriele Farina, Christian Kroer, Chung-Wei Lee, Haipeng Luo, and Tuomas Sandholm. Uncoupled learning dynamics with O(log T ) swap regret in multiplayer games. arXiv preprint arXiv:2204.11417, 2022a.\n\nIoannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On last-iterate convergence beyond zero-sum games. In International Conference on Machine Learning, ICML 2022, volume 162 of Proceedings of Machine Learning Research, pages 536–581. PMLR, 2022b.\n\nRobert J. Aumann. Subjectivity and correlation in randomized strategies. Journal of Mathematical\n\nEconomics, 1(1):67–96, 1974.\n\nBaruch Awerbuch, Yossi Azar, and Amir Epstein. The price of routing unsplittable flow. SIAM J.\n\nComput., 42(1):160–177, 2013.\n\nWaïss Azizian, Franck Iutzeler, Jérôme Malick, and Panayotis Mertikopoulos. The last-iterate convergence rate of optimistic mirror descent in stochastic variational inequalities. In Mikhail Belkin and Samory Kpotufe, editors, Conference on Learning Theory, COLT 2021, volume 134 of Proceedings of Machine Learning Research, pages 326–358. PMLR, 2021.\n\nMaria-Florina Balcan, Avrim Blum, Nika Haghtalab, and Ariel D Procaccia. Commitment without regrets: Online learning in stackelberg security games. In Proceedings of the sixteenth ACM conference on economics and computation, pages 61–78, 2015a.\n\nMaria-Florina Balcan, Avrim Blum, and Santosh S. Vempala. Efficient representations for lifelong learning and autoencoding. In Proceedings of The 28th Conference on Learning Theory, COLT 2015, volume 40 of JMLR Workshop and Conference Proceedings, pages 191–210. JMLR.org, 2015b.\n\nMaria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-based meta-learning. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, volume 97 of Proceedings of Machine Learning Research, pages 424–433. PMLR, 2019.\n\nMaria-Florina Balcan, Keegan Harris, Mikhail Khodak, and Zhiwei Steven Wu. Meta-learning\n\nadversarial bandits. arXiv preprint arXiv:2205.14128, 2022.\n\nArindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, and Joydeep Ghosh. Clustering with bregman\n\ndivergences. J. Mach. Learn. Res., 6:1705–1749, 2005.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nHeinz H. Bauschke, Walaa M. Moursi, and Xianfu Wang. Generalized monotone operators and their\n\naveraged resolvents. Math. Program., 189:55–74, 2021.\n\nNathalie Bertrand, Nicolas Markey, Suman Sadhukhan, and Ocan Sankur. Dynamic network congestion games. In 40th IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science, FSTTCS 2020, volume 182 of LIPIcs, pages 40:1–40:16. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2020.\n\nBenjamin E. Birnbaum, Nikhil R. Devanur, and Lin Xiao. Distributed algorithms via gradient descent for fisher markets. In Proceedings 12th ACM Conference on Electronic Commerce (EC-2011), 2011, pages 127–136. ACM, 2011.\n\nAdam Block, Yuval Dagan, Noah Golowich, and Alexander Rakhlin. Smoothed online learning is as easy as statistical learning. In Conference on Learning Theory, 2-5 July 2022, volume 178 of Proceedings of Machine Learning Research, pages 1716–1786. PMLR, 2022.\n\nAvrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning\n\nResearch, 8(6), 2007.\n\nNoam Brown and Tuomas Sandholm. Regret transfer and parameter optimization. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014, pages 594–601. AAAI Press, 2014.\n\nNoam Brown and Tuomas Sandholm. Regret-based pruning in extensive-form games. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, pages 1972–1980, 2015a.\n\nNoam Brown and Tuomas Sandholm. Simultaneous abstraction and equilibrium finding in games. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, pages 489–496. AAAI Press, 2015b.\n\nNoam Brown and Tuomas Sandholm. Strategy-based warm starting for regret minimization in games. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages 432–438. AAAI Press, 2016.\n\nNoam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top\n\nprofessionals. Science, 359(6374):418–424, 2018.\n\nNoam Brown and Tuomas Sandholm. Solving imperfect-information games via discounted regret minimization. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, 2019, pages 1829–1836. AAAI Press, 2019.\n\nYang Cai and Weiqiang Zheng. Accelerated single-call methods for constrained min-max optimization.\n\nCoRR, abs/2210.03096, 2022.\n\nYang Cai, Ozan Candogan, Constantinos Daskalakis, and Christos H. Papadimitriou. Zero-sum\n\npolymatrix games: A generalization of minmax. Math. Oper. Res., 41(2):648–655, 2016.\n\nOzan Candogan, Asuman E. Ozdaglar, and Pablo A. Parrilo. Dynamics in near-potential games.\n\nGames Econ. Behav., 82:66–90, 2013.\n\nAdrian Rivera Cardoso, Jacob D. Abernethy, He Wang, and Huan Xu. Competing against nash equilibria in adversarially changing zero-sum games. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, volume 97 of Proceedings of Machine Learning Research, pages 921–930. PMLR, 2019.\n\nNicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University\n\nPress, 2006.\n\nNicolò Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for predic-\n\ntion with expert advice. Mach. Learn., 66(2-3):321–352, 2007.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nXi Chen, Xiaotie Deng, and Shang-Hua Teng. Settling the complexity of computing two-player nash\n\nequilibria. J. ACM, 56(3):14:1–14:57, 2009.\n\nXi Chen, Christos H. Papadimitriou, and Binghui Peng. Memory bounds for continual learning.\n\nCoRR, abs/2204.10830, 2022.\n\nChao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In COLT 2012 - The 25th Annual Conference on Learning Theory, 2012, volume 23 of JMLR Proceedings, pages 6.1–6.20. JMLR.org, 2012.\n\nGeorge Christodoulou and Elias Koutsoupias. The price of anarchy of finite congestion games. In Proceedings of the 37th Annual ACM Symposium on Theory of Computing, Baltimore, MD, USA, May 22-24, 2005, pages 67–73. ACM, 2005.\n\nGeorge Christodoulou, Annamária Kovács, and Michael Schapira. Bayesian combinatorial auctions.\n\nJ. ACM, 63(2):11:1–11:19, 2016.\n\nPatrick L. Combettes and Teemu Pennanen. Proximal methods for cohypomonotone operators. SIAM\n\nJ. Control. Optim., 43(2):731–742, 2004.\n\nConstantinos Daskalakis and Noah Golowich. Fast rates for nonparametric online learning: from realizability to learning in games. In STOC ’22: 54th Annual ACM SIGACT Symposium on Theory of Computing, 2022, pages 846–859. ACM, 2022.\n\nConstantinos Daskalakis, Paul W. Goldberg, and Christos H. Papadimitriou. The complexity of\n\ncomputing a nash equilibrium. SIAM J. Comput., 39(1):195–259, 2009.\n\nConstantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms\n\nfor zero-sum games. Games Econ. Behav., 92:327–348, 2015.\n\nConstantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. In 6th International Conference on Learning Representations, ICLR 2018. OpenReview.net, 2018.\n\nConstantinos Daskalakis, Dylan J. Foster, and Noah Golowich. Independent policy gradient methods for competitive reinforcement learning. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020.\n\nConstantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning in general games. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, pages 27604–27616, 2021.\n\nJelena Diakonikolas, Constantinos Daskalakis, and Michael I. Jordan. Efficient methods for structured nonconvex-nonconcave min-max optimization. In The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, volume 130 of Proceedings of Machine Learning Research, pages 2746–2754. PMLR, 2021.\n\nJohn C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning\n\nand stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, 2011.\n\nBenoit Duvocelle, Panayotis Mertikopoulos, Mathias Staudigl, and Dries Vermeulen. Multiagent\n\nonline learning in time-varying games. Mathematics of Operations Research, 2022.\n\nGabriele Farina and Tuomas Sandholm. Fast payoff matrix sparsification techniques for structured\n\nextensive-form games. In AAAI Conference on Artificial Intelligence, 2022.\n\nGabriele Farina, Ioannis Anagnostides, Haipeng Luo, Chung-Wei Lee, Christian Kroer, and Tuomas Sandholm. Near-optimal no-regret learning for general convex games. CoRR, abs/2206.08742, 2022.\n\nTanner Fiez, Lillian J. Ratliff, Eric Mazumdar, Evan Faulkner, and Adhyyan Narang. Global convergence to local minmax equilibrium in classes of nonconvex zero-sum games. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, pages 29049–29063, 2021a.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nTanner Fiez, Ryann Sim, Stratis Skoulakis, Georgios Piliouras, and Lillian J. Ratliff. Online learning in periodic zero-sum games. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, pages 10313–10325, 2021b.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, volume 70 of Proceedings of Machine Learning Research, pages 1126–1135. PMLR, 2017.\n\nChelsea Finn, Aravind Rajeswaran, Sham M. Kakade, and Sergey Levine. Online meta-learning. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, volume 97 of Proceedings of Machine Learning Research, pages 1920–1930. PMLR, 2019.\n\nYoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an\n\napplication to boosting. J. Comput. Syst. Sci., 55(1):119–139, 1997.\n\nYuan Gao, Christian Kroer, and Donald Goldfarb. Increasing iterate averaging for solving saddlepoint problems. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, pages 7537–7544. AAAI Press, 2021.\n\nAngeliki Giannou, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Panayotis Mertikopoulos. Survival of the strictest: Stable and unstable equilibria under regularized learning with partial information. In Conference on Learning Theory, COLT 2021, volume 134 of Proceedings of Machine Learning Research, pages 2147–2148. PMLR, 2021.\n\nGauthier Gidel, Hugo Berard, Gaëtan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality perspective on generative adversarial networks. In 7th International Conference on Learning Representations, ICLR 2019. OpenReview.net, 2019.\n\nAndrew Gilpin, Javier Peña, and Tuomas Sandholm. First-order algorithm with O(ln(1/ε)) convergence for ε-equilibrium in two-person zero-sum games. Math. Program., 133(1-2):279–298, 2012.\n\nNoah Golowich, Sarath Pattathil, and Constantinos Daskalakis. Tight last-iterate convergence rates for no-regret learning in multi-player games. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, 2020a.\n\nNoah Golowich, Sarath Pattathil, Constantinos Daskalakis, and Asuman E. Ozdaglar. Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems. In Conference on Learning Theory, COLT 2020, volume 125 of Proceedings of Machine Learning Research, pages 1758–1784. PMLR, 2020b.\n\nNika Haghtalab, Yanjun Han, Abhishek Shetty, and Kunhe Yang. Oracle-efficient online learning for\n\nbeyond worst-case adversaries. CoRR, abs/2202.08549, 2022.\n\nSergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.\n\nEconometrica, 68(5):1127–1150, 2000.\n\nSergiu Hart and David Schmeidler. Existence of correlated equilibria. Mathematics of Operations\n\nResearch, 14(1):18–25, 1989.\n\nJason D. Hartline, Vasilis Syrgkanis, and Éva Tardos. No-regret learning in bayesian games. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, pages 3061–3069, 2015.\n\nElad Hazan and Satyen Kale. Extracting certainty from uncertainty: regret bounded by variation in\n\ncosts. Mach. Learn., 80(2-3):165–188, 2010.\n\nElad Hazan and Satyen Kale. Better algorithms for benign bandits. J. Mach. Learn. Res., 12:\n\n1287–1311, 2011.\n\nElad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex\n\noptimization. Machine Learning, 69(2):169–192, 2007.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nMartin Hoefer, Vahab S. Mirrokni, Heiko Röglin, and Shang-Hua Teng. Competitive routing over\n\ntime. Theor. Comput. Sci., 412(39):5420–5432, 2011.\n\nJosef Hofbauer and William H. Sandholm. On the global convergence of stochastic fictitious play.\n\nEconometrica, 70(6):2265–2294, 2002.\n\nYu-Guan Hsieh, Franck Iutzeler, Jérôme Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, pages 6936–6946, 2019.\n\nYu-Guan Hsieh, Kimon Antonakopoulos, and Panayotis Mertikopoulos. Adaptive learning in continuous games: Optimal regret bounds and convergence to nash equilibrium. In Conference on Learning Theory, COLT 2021, volume 134 of Proceedings of Machine Learning Research, pages 2388–2422. PMLR, 2021.\n\nYu-Guan Hsieh, Kimon Antonakopoulos, Volkan Cevher, and Panayotis Mertikopoulos. No-regret learning in games with noisy feedback: Faster rates and adaptivity via learning rate separation. CoRR, abs/2206.06015, 2022.\n\nRamesh Johari and John N. Tsitsiklis. Efficiency loss in a network resource allocation game.\n\nMathematics of Operations Research, 29(3):407–435, 2004.\n\nMert Kayaalp, Stefan Vlaski, and Ali H. Sayed. Dif-maml: Decentralized multi-agent meta-learning.\n\nCoRR, abs/2010.02870, 2020.\n\nMert Kayaalp, Stefan Vlaski, and Ali H Sayed. Distributed meta-learning with networked agents. In 2021 29th European Signal Processing Conference (EUSIPCO), pages 1361–1365. IEEE, 2021.\n\nMikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based metalearning methods. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nMikhail Khodak, Maria-Florina Balcan, Ameet Talwalkar, and Sergei Vassilvitskii. Learning predictions for algorithms with predictions. In Advances in Neural Information Processing Systems, 2022. To appear.\n\nRobert Kleinberg, Georgios Piliouras, and Éva Tardos. Multiplicative updates outperform generic no-regret learning in congestion games: extended abstract. In Proceedings of the 41st Annual ACM Symposium on Theory of Computing, STOC 2009, pages 533–542. ACM, 2009.\n\nDaphne Koller and Nimrod Megiddo. The complexity of two-person zero-sum games in extensive\n\nform. Games and Economic Behavior, 4(4):528–552, 1992.\n\nGalina M Korpelevich. The extragradient method for finding saddle points and other problems.\n\nMatecon, 12:747–756, 1976.\n\nChristian Kroer and Tuomas Sandholm. A unified framework for extensive-form game abstraction with bounds. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, pages 613–624, 2018.\n\nStefanos Leonardos, Will Overman, Ioannis Panageas, and Georgios Piliouras. Global convergence of multi-agent policy gradient in markov potential games. In The Tenth International Conference on Learning Representations, ICLR 2022. OpenReview.net, 2022.\n\nShuangtong Li, Tianyi Zhou, Xinmei Tian, and Dacheng Tao. Learning to collaborate in decentralized learning of personalized models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9766–9775, 2022.\n\nYuqian Li, Vincent Conitzer, and Dmytro Korzhyk. Catcher-evader games. arXiv preprint\n\narXiv:1602.01896, 2016.\n\nZhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few\n\nshot learning. CoRR, abs/1707.09835, 2017.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nBrendan Lucier and Allan Borodin. Price of anarchy for greedy auctions. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2010, pages 537–553. SIAM, 2010.\n\nHaipeng Luo and Robert E. Schapire. Achieving all with no parameters: Adanormalhedge. In Proceedings of The 28th Conference on Learning Theory, COLT 2015, volume 40 of JMLR Workshop and Conference Proceedings, pages 1286–1304. JMLR.org, 2015.\n\nThodoris Lykouris, Vasilis Syrgkanis, and Éva Tardos. Learning and efficiency in games with dynamic population. In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2016, pages 120–129. SIAM, 2016.\n\nH. Brendan McMahan. Follow-the-regularized-leader and mirror descent: Equivalence theorems and L1 regularization. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, volume 15 of JMLR Proceedings, pages 525–533. JMLR.org, 2011.\n\nEmily Meigs, Francesca Parise, and Asuman E. Ozdaglar. Learning dynamics in stochastic routing games. In 55th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2017, pages 259–266. IEEE, 2017.\n\nPanayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In 7th International Conference on Learning Representations, ICLR 2019. OpenReview.net, 2019.\n\nPaul Milgrom and John Roberts. Rationalizability, learning, and equilibrium in games with strategic\n\ncomplementarities. Econometrica, 58(6):1255–1277, 1990.\n\nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. In Tim Roughgarden, editor, Beyond the Worst-Case Analysis of Algorithms, pages 646–662. Cambridge University Press, 2020.\n\nAryan Mokhtari, Asuman E. Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, volume 108 of Proceedings of Machine Learning Research, pages 1497–1507. PMLR, 2020.\n\nDenis Nekipelov, Vasilis Syrgkanis, and Éva Tardos. Econometrics for learning agents. In Proceedings of the Sixteenth ACM Conference on Economics and Computation, EC ’15, pages 1–18. ACM, 2015.\n\nJ. V. Neumann. A model of general economic equilibrium. The Review of Economic Studies, 13(1):\n\n1–9, 1945.\n\nNoam Nisan and Gali Noti. An experimental evaluation of regret-based econometrics. In Proceedings of the 26th International Conference on World Wide Web, WWW 2017, pages 73–81. ACM, 2017.\n\nIlya Osadchiy, Kfir Y Levy, and Ron Meir. Online meta-learning in adversarial multi-armed bandits.\n\narXiv preprint arXiv:2205.15921, 2022.\n\nGeorgios Piliouras, Ryann Sim, and Stratis Skoulakis. Optimal no-regret learning in general games: Bounded regret with unbounded step-sizes via clairvoyant MWU. CoRR, abs/2111.14737, 2021.\n\nAlexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In COLT 2013 - The 26th Annual Conference on Learning Theory, 2013, volume 30 of JMLR Workshop and Conference Proceedings, pages 993–1019. JMLR.org, 2013a.\n\nAlexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In Christopher J. C. Burges, Léon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, pages 3066–3074, 2013b.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nI. Romanovskii. Reduction of a game with complete memory to a matrix game. Soviet Mathematics,\n\n3, 1962.\n\nJ. B. Rosen. Existence and uniqueness of equilibrium points for concave n-person games. Economet-\n\nrica, 33(3):520–534, 1965.\n\nTim Roughgarden. Intrinsic robustness of the price of anarchy. J. ACM, 62(5):32:1–32:42, 2015.\n\nTim Roughgarden, Vasilis Syrgkanis, and Éva Tardos. The price of anarchy in auctions. J. Artif.\n\nIntell. Res., 59:59–101, 2017.\n\nL. S. Shapley. Stochastic games. Proceedings of the National Academy of Sciences, 39(10):1095–\n\n1100, 1953.\n\nArun Sai Suggala and Praneeth Netrapalli. Follow the perturbed leader: Optimism and fast parallel algorithms for smooth minimax games. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, 2020.\n\nVasilis Syrgkanis and Éva Tardos. Composable and efficient mechanisms. In Dan Boneh, Tim Roughgarden, and Joan Feigenbaum, editors, Symposium on Theory of Computing Conference, STOC’13, 2013, pages 211–220. ACM, 2013.\n\nVasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E. Schapire. Fast convergence of regularized learning in games. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, pages 2989–2997, 2015.\n\nOskari Tammelin. Solving large imperfect information games using CFR+. CoRR, abs/1407.5042,\n\n2014.\n\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 1998.\n\nEric van Damme. Stability and Perfection of Nash Equilibria. Springer-Verlag, Berlin, Heidelberg,\n\n1987.\n\nAdrian Vetta. Nash equilibria in competitive societies, with applications to facility location, traffic routing and auctions. In 43rd Symposium on Foundations of Computer Science (FOCS 2002), page 416. IEEE Computer Society, 2002.\n\nJun-Kun Wang, Jacob D. Abernethy, and Kfir Y. Levy. No-regret dynamics in the fenchel game: A\n\nunified framework for algorithmic convex optimization. CoRR, abs/2111.11309, 2021.\n\nChen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Conference On Learning Theory, COLT 2018, volume 75 of Proceedings of Machine Learning Research, pages 1263–1291. PMLR, 2018.\n\nChen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained saddle-point optimization. In 9th International Conference on Learning Representations, ICLR 2021. OpenReview.net, 2021.\n\nAndre Wibisono, Molei Tao, and Georgios Piliouras. Alternating mirror descent for constrained\n\nmin-max games. CoRR, abs/2206.04160, 2022.\n\nTianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant: Optimal In Proceedings of the 33nd dynamic regret of online learning with true and noisy gradient. International Conference on Machine Learning, ICML 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 449–457. JMLR.org, 2016.\n\nHugh Zhang, Adam Lerer, and Noam Brown. Equilibrium finding in normal-form games via greedy regret minimization. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, ThirtyFourth Conference on Innovative Applications of Artificial Intelligence, 2022, pages 9484–9492. AAAI Press, 2022a.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nMengxiao Zhang, Peng Zhao, Haipeng Luo, and Zhi-Hua Zhou. No-regret learning in time-varying zero-sum games. In International Conference on Machine Learning, ICML 2022, volume 162 of Proceedings of Machine Learning Research, pages 26772–26808. PMLR, 2022b.\n\nPeng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. Dynamic regret of convex and smooth functions. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, 2020.\n\n17",
    "reference": "# Summary Of The Paper\n\nThis paper applies the well-known idea of meta learning to learning in games, across various game types and settings including zero-sum games, potential games, general-sum games and Stackelberg games. The authors show several theoretical results for convergence to relevant performance metrics in these games, depending on natural notions of similarity between games encountered by the learner. In general, meta-learning serves to improve the performance of standard algorithms, recovering the known guarantees in static games. The authors also experiment with meta learning poker (zero-sum) endgames, with meta learning used to determine appropriate initializations for the learners. The experiments show that the technique greatly improves performance compared to vanilla methods.\n\n# Strength And Weaknesses\n\nStrengths:\n- This paper is very thorough in its exposition of related work and in extending the results in each of the settings studied. It is very ambitious to analyze so many different game types and settings, but I feel that the authors managed to do so in a way which still feels digestible and natural.\n-  As far as I can tell, despite the breadth of the paper's analyses, the theoretical results are presented clearly and the proofs (while typically derivative of the standard techniques in the field) contain some non-trivial ideas which would certainly be of independent interest in static game settings. A primary example of this is the analysis of the extragradient algorithm using a regret-proxy, allowing for an RVU-type bound to be written even though extragradient is not a no-regret algorithm.\n\nWeaknesses:\n- A somewhat minor weakness of the paper is that a lot of the results seem unsurprising, in the sense that by selecting natural similarity metrics and having agents learn initializations based on a 'stacking'-type setup with no-regret algorithms, one would certainly expect the performance to improve over time. It is of course still useful and interesting to formally analyze the performance improvements, but I would have liked to see more focus on the empirical results.\n- The experimental results in the paper are quite restricted, in the sense that despite a broad range of theoretical results, the authors only show experimental improvements for zero-sum games. It would have been interesting to see a visual representation of the improvements in potential, general-sum and even stackelberg games using meta-learning. \n- Finally, the choice of similarity metric in each setting, while usually appropriately motivated, leaves some room open for debate. In particular it would have been interesting to empirically compare different similarity metrics in settings where multiple metrics can be devised.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written and the framework for meta learning in games is clear and easy to understand. Moreover the technical results are all well-argued for and overall the paper has a nice flow to it. I feel the paper is less novel in the sense that it applies a well-known technique in machine learning to learning in games, and the resulting analyses, while interesting, are mostly extensions of known results. Overall, however, the central idea is well motivated and potentially practically useful for future work.\n\n# Summary Of The Review\n\nIn summary, this paper introduces a framework for meta learning in games which seems to be a useful and interesting idea to me. Many theoretical results in different game settings are described, all of which improve upon vanilla bounds and some new techniques are even shown in the supplementary material. The paper is well written enough to not feel bloated, but there is a lack of empirical evidence for the meta learning techniques in games beyond zero-sum. Overall I would recommend the paper for acceptance.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nGENERALIZED BELIEF TRANSPORT\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nHuman learners have ability to adopt appropriate learning approaches depending on constraints such as prior on the hypothesis and urgency of decision. However, existing learning models are typically considered individually rather than in relation to one and other. To build agents that have the ability to move between different modes of learning over time, it is important to understand how learning models are related as points in a broader space of possibilities. We introduce a mathematical framework, Generalized Belief Transport (GBT), that unifies and generalizes prior models, including Bayesian inference, cooperative communication and classification, as parameterizations of three learning constraints within Unbalanced Optimal Transport (UOT). We visualize the space of learning models encoded by GBT as a cube which includes classic learning models as special points. We derive critical properties of this parameterized space including proving continuity and differentiability which is the basis for model interpolation, and study limiting behavior of the parameters, which allows attaching learning models on the boundaries. Moreover, we investigate the long-run behavior of GBT, explore convergence properties of models in GBT mathematical and computationally, and formulate conjectures about general behavior. We conclude with open questions and implications for more unified models of learning.\n\n1\n\nINTRODUCTION\n\nLearning and inference are subject to internal and external constraints. Internal constraints include the availability of relevant prior knowledge, which may be brought to bear on inferences based on data. External constraints include the availability of time to accumulate evidence or make the best decision now. Human learners appear to be capable of moving between these constraints as necessary. However, standard models of machine learning tend to view constraints as different problems, which impedes development of unified view of learning agents.\n\nIndeed, internal and external constraints on learning map onto classic dichotomies in machine learning. Internal constraints such as the availability of prior knowledge maps onto the Frequentist-Bayesian dichotomy in which the latter uses prior knowledge as a constraint on posterior beliefs, while the former does not. Within Bayesian theory, a classic debate pertains to uninformative, or minimally informative, settings of priors (Jeffreys, 1946; Robert et al., 2009). External constraints such as availability of time to accumulate evidence versus the need to make the best possible decision now informs the use of generative versus discriminative approaches (Ng and Jordan, 2001). Despite the fundamental nature of these debates, and the usefulness of all approaches in the appropriate contexts, we are unaware of prior efforts to unify these perspectives and study the full space of possible models.\n\nWe introduce Generalized Belief Transport (GBT), based on Unbalanced Optimal Transport (Sec. 2), which paramterizes and interpolates between known reasoning modes (Sec. 3.2), with four major contributions. First, we prove continuity in the parameterization and differentiability on the interior of the parameter space (Sec. 3.1). Second, we analyze the behavior under variations in the parameter space (Sec. 3.3). Third, we consider sequential learning, where learners may (not) track the empirically observed data frequencies. And finally we state our theoretical results, simulations and conjectures about the sequential behaviors for various parameters for generic costs and priors (Sec. 4.2). Notations. R≥0 denotes the non-negative reals. Vector 1 = (1, . . . , 1). The i-th component of vector v is v(i). P(A) is the set of probability distributions over A. For a matrix M , Mij represents its (i, j)-th entry, M(i,_) denotes its i-th row, and M(_,j) denotes its j-th column. Probability is P( · ).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n2 LEARNING AS A PROBLEM OF UNBALANCED OPTIMAL TRANSPORT\n\nConsider a general learning setting: an agent, which we call a learner, updates their belief about the world based on observed data subject to constraints. There is a finite set D = {d1, . . . , dn} of all possible data, that defines the interface between the learner and the world. The world is defined by a true hypothesis h∗, whose meaning is captured by a probability mapping P(d|h∗) onto observable data. For instance, the world can either be the environment in classic Bayesian inference (Murphy, 2012) or a teacher in cooperative communication (Wang et al., 2020b).\n\nA learner is equipped with a set of hypotheses H = {h1, . . . , hm} which may not contain h∗; an initial belief on the hypotheses set, denoted by θ0 ∈ P(H); and a non-negative cost matrix C = (Cij)m×n, where Cij measures the underlying cost of mapping di into hj 1. The cost matrix can be derived from other matrices that record the relation between D and H, such as likelihood matrices in classic Bayesian inference or consistency matrices in cooperative communication (see details in Section 3.2).This setting reflects an agent’s learning constraints: pre-selected hypotheses, and the relations between them and the communication interface (data set).\n\nA learner observes data in sequence. At round k, the learner observes a data dk that is sampled from D by the world according to P(d|h∗). Then the learner updates their beliefs over H from θk−1 to θk through a learning scheme, where θk−1, θk ∈ P(H). For instance, in Bayesian inference, the learning scheme is defined by Bayes rule; while in discriminative models, the learning scheme is prescribed by a code book.\n\nThe learner transforms the observed data into a belief on hypotheses h ∈ H with a minimal cost, subject to appropriate constraints, with the goal of learning the exact map P(d|h∗). We can naturally cast this learning problem as Unbalanced Optimal Transport.\n\n2.1 UNBALANCED OPTIMAL TRANSPORT\n\nUnbalanced optimal transport is a generalization of (entropic) Optimal Transport. Optimal transport infers a coupling that minimizes the cost of transporting between two marginal probability distributions (Monge, 1781; Kantorovich, 2006; Villani, 2008). Entropic Optimal Transport adds a regularization term based on the entropy of the inferred coupling, which has desirable computational consequences (Cuturi, 2013; Peyré and Cuturi, 2019). Unbalanced OT further relaxes the problem by allowing one to approximately match marginal probability distributions.\n\nLet η = (η(1), . . . , η(n)) and θ = (θ(1), . . . , θ(m)) be two probability distributions. A joint distribution matrix P = (Pij)n×m is called a transport plan or coupling between η and θ if P has η and θ as its marginals. Given a cost matrix C = (Cij)n×m ∈ (R≥0)m×n, Entropy regularized optimal transport (EOT) (Cuturi, 2013) solves the optimal transport plan P εP that minimizes the entropy regularized cost of transporting η into θ. Thus for a parameter εP > 0: P εP = arg minP ∈U (η,θ) ⟨C, P ⟩ − εP H(P ), where U (η, θ) is the set of all transport plans between η and θ, ⟨C, P ⟩ = (cid:80) i,j CijPij is the inner product between C and P , and H(P ) = − (cid:80)\n\nij Pij log Pij + Pij is the entropy of P .\n\nUnbalanced Optimal Transport (UOT), introduced by Liero et al. (2018), is a generalization of EOT that relaxes the marginal constraints. The degree of relaxation is controlled by two regularization terms. Formally, for non-negative scalar parameters ε = (εP , εη, εθ), the UOT plan is,\n\nP ε(C, η, θ) = arg min\n\nP ∈(R≥0)n×m\n\n{⟨C, P ⟩ − εP H(P ) + εηKL(P 1|η) + εθKL(P T 1|θ)}.\n\n(1)\n\nHere KL(a|b) := (cid:80) i ai log(ai/bi) − ai + bi is the Kullback–Leibler divergence between vectors. UOT differs from EOT in relaxing the hard constraint that P satisfy the given marginals η and θ, to soft constraints that penalize the marginals being far from η or θ 2. In particular, as εη and εθ → ∞, we recover the EOT problem.\n\n1To guarantee the hypotheses are distinguishable, we assume that C does not contain two columns that are\n\nonly differ by an additive scalar.\n\n2UOT also generalizes to measures of arbitrary mass, i.e. the total mass of η does not need to equal to θ.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nProposition 1. The UOT problem with cost matrix C, marginals θ, η and parameters ε = (εP , εη, εθ) generates the same UOT plan as the UOT problem with tC, θ, η, tε = (tεP , tεη, tεθ) for any t ∈ (0, ∞). Therefore, the analysis on ε and tε are the same for general cost C.\n\nThe objective function in Eq. (1) is linear on C, εP , εη, εθ, so a positive common factor does not affect the solution. In the discussion under general cost matrix C, properties that hold for ε are also valid for all tε (t > 0).\n\nUOT plans can be solved efficiently via a Sinkhorn-style algorithm (Sinkhorn and Knopp, 1967). Roughly speaking, (η, θ, ε)-unbalanced Sinkhorn scaling of a matrix M is iterated alternation of row and column normalizations of M with respect to (η, θ, ε) (see Algorithm 2). Chizat et al. (2018) shows that: given a cost C, the UOT plan P ε can be obtained by applying (η, θ, ε)-unbalanced Sinkhorn scaling on K ε := e− 1\n\nCij )m×n, with convergence rate ̃O( mn\n\n) (Pham et al., 2020).\n\nC = (e− 1\n\nεP\n\nεP\n\nεP\n\nGeneralized Belief Transport. Learning, efficiently transport one’s belief with constraints, is naturally a UOT problem. Each round, a learner, defined by a choice of ε = (εP , εη, εθ), updates their beliefs as follows. Let ηk−1, θk−1 be the learner’s estimations of the data distribution and the belief over hypotheses H after round k − 1, respectively. At round k, the learner first improves their estimation of the mapping between D and H, denoted by Mk, through solving the UOT plan Eq. (1) with (C, ηk−1, θk−1), i.e. Mk = P ε(C, ηk−1, θk−1). Then with data observation dk, the learner updates their beliefs over H using corresponding row of Mk, i.e. suppose dk = di for some di ∈ D, the learner’s belief θk is defined to be the row normalization of the i-th row of Mk. Finally, the learner updates their data distribution to ηk by increment of the i-th element of ηk−1, see Algorithm 1.\n\nAlgorithm 1 Generalized Belief Transport\n\nAlgorithm 2 Unbalanced Sinkhorn Scaling\n\ninput: C, θ0, η0, h∗, N , data sampler τ based on P(d|h∗), stopping condition ω output: M , θ initialize: k ← 1 while k < N and not ω(θ) do M ← P ε(C, ηk−1, θk−1) get data di sampled from τ ηk ← update(ηk−1, di) via update rule v ← M(i,_) θk ← v/(cid:80) k ← k + 1\n\nh∈H v(h)\n\nend while\n\ninput: C, θ, η, ε = (εP , εη, εθ), N stopping condition ω output: P ε(C, η, θ) initialize: K = exp(−εP C), v(0) = 1m while k < N and not ω do (cid:17) εη\n\n(cid:16)\n\nu(k) ←\n\nη Kv(k−1)\n\nεη +εP ,\n\n(cid:19) εθ\n\nεθ +εP\n\nv(k) ←\n\n(cid:18)\n\nθ KT u(k)\n\nend while P ε(C, η, θ) = diag(u)Kdiag(v)\n\n3 GENERALIZED BELIEF TRANSPORT (GBT)\n\nMany learning models with different constraints—including Bayesian inference, Frequentist inference, Cooperative learning, and Discriminative learning—are unified under our GBT framework by varying the choice of ε. In this section, we focus on the single-round behavior of the GBT model, i.e., given a pair of marginals (θ, η), how different learners update beliefs. We first visualize the entire learner set as a cube (in terms of parameters), see Figure 1. Then, we study the topological properties of the learner set through continuous deformations of parameters ε. In particular, we show that existing models including Bayesian inference, cooperative inference and discriminative learning are learners with parameters (1, 0, ∞), (1, ∞, ∞) and (0, ∞, ∞) respectively in our UOT framework.\n\n3.1 THE PARAMETER SPACE OF GBT MODEL\n\nThe space of constrained belief-updating learners in GBT are parameterized by three regularizers for the underlying UOT problem (1): εP , εη and εθ, each ranges in [0, ∞). Therefore, the constraint space for GBT is R3 + ), the map ε = (εP , εη, εθ) (cid:55)→ (P ε) bears continuous properties:\n\n≥0, with the standard topology. When C, θ and η are fixed (assume η ∈ Rm\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nProposition 2. 3 The UOT plan P in Equation (1), as a function of ε, is continuous in (0, ∞)×[0, ∞)2. Furthermore, P is differentiable with respect to ε in the interior.\n\nContinuity on ε provides the basis for interpolation between different learning agents. The proof of Proposition 2 also implies the continuity on η and θ. Further, towards the boundaries of the parameter space (where theories like Bayesian, Cooperative Communication live in), we show: Proposition 3. For any finite sP , sη, sθ ≥ 0, the limit of P ε exists as ε approaches (∞, sη, sθ). In fact, limε→(∞,sη,sθ) P ε\n\nij = 1 for all i, j. Moreover, P ε converges to the solution to\n\nmin⟨C, P ⟩ − sP H(P ) + sθKL(P T 1|θ), with constraint P 1 = η,\n\nas ε → (sP , ∞, sθ). Similarly, P ε converges to the solution to\n\nmin⟨C, P ⟩ − sP H(P ) + sηKL(P 1|η), with constraint P T 1 = θ,\n\nas ε → (sP , sη, ∞). And when ε → (sP , ∞, ∞), the matrix P ε converges to the EOT solution:\n\nmin⟨C, P ⟩ − sP H(P ), with constraints P T 1 = θ and P 1 = η.\n\nWhen ε → (∞, ∞, sθ), (∞, sη, ∞) or (∞, ∞, ∞), the limit does not exist, but the directional limits can be calculated.\n\nFigure 1: The parameter space S of GBT. Parameters ε = (εP , εη, εθ) can take the value ∞, rendering the corresponding regularization to a strict constraint. The two dashed edges with εP = ∞ are not generally well-defined since the limits do not exist. The vertices corresponding to θ ⊗ η, Frequentist (η ⊗ 1) and 1 ⊗ θ are the limits taken along the vertical edges. Given (C, θ, η) as shown in the left corner, each colored map plots each GBT learner (differ by constraints)’s estimation of the mapping between hypotheses and data (UOT plan).\n\nThe generalized parameter space for UOT with its boundaries can be visualized in Fig. 1. Function sigmoid(log(x)) maps segment [0, ∞) to [0, 1) smoothly. Then we can add boundaries to the image cube [0, 1)3. The dashed lines in the figure indicates limits that do not exist. The parameter space is then S = [0, ∞]3\\({(∞, ∞, x) : x ∈ [0, ∞]} ∪ {(∞, x, ∞) : x ∈ [0, ∞]}). Later, we may still mention (∞, ∞, εθ) and (∞, εη, ∞), only for case where the direction is vertical (along axis of εP ).\n\n3.2 SOME SPECIAL POINTS IN THE PARAMETER SPACE\n\nBayesian Inference. Given a data observation, a Bayesian learner (BI) (Murphy, 2012) derives posterior belief P(h|d) based on prior belief P(h) and likelihood matrix P(d|h), according to the Bayes rule. Intuitively, due to soft time constraint (εP = 1), a Bayesian learner is a generative agent who puts a hard internal constraint on their prior belief (εθ = ∞), and omits the estimated data distribution η in the learning process, (εη = 0). As a direct application of Prop 3, we show: Corollary 4. Consider a UOT problem with cost C = − log P(d|h), marginals θ = P(h), η ∈ P(D). The optimal UOT plan P (1,εη,εθ) converges to the posterior P(h|d) as εη → 0 and εθ → ∞. Bayesian inference is a special case of GBT with ε = (1, 0, ∞).\n\nMoreover, by relaxing the constraint on the prior (i.e., 0 < εθ < ∞), one obtains a parameterized family of less informative priors.\n\n3Proofs of all claims are included in the Appendices.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFrequentist Inference. A frequentist updates their belief from data observations by increasing the corresponding frequencies of datum. Intuitively, a frequentist is an agent who puts a hard constraint on the data distribution η (εη = ∞), and omits prior knowledge θ (εθ = 0) in a learning process without time constraint (εP = ∞). Formally we show: Corollary 5. Consider a UOT problem with θ ∈ P(H), η = P(d). The optimal UOT plan P (εP ,∞,0) converges to η ⊗ 1 as εP → ∞. Frequentist Inference is a special case of GBT with ε = (∞, ∞, 0).\n\nCooperative Communication. Two cooperative agents, a teacher and a learner, are considered in Yang et al. (2018); Wang et al. (2020b); Shafto et al. (2021). Cooperative learners (CI) draw inferences about hypotheses based on which data would be most effective for the teacher to choose (see a brief model summary in the Appendix A). Given a data observation, a cooperative learner derives an optimal plan L = P(H, D) based on a prior belief P(h), a shared data distribution P(d) and a matrix M specifies the consistency between data and hypotheses (such as Mij records the co-occurrence of di and hj). Intuitively, a cooperative learner is also a generative agent who puts hard constraints on both data and hypotheses (εη = ∞, εθ = ∞), and aims to align with the true belief asymptotically, (εP = 1). Thus as a direct application of Proposition 3 we show: Corollary 6. Let cost C = − log M , marginals θ = P(h) and η = P(d). The optimal UOT plan P (1,εη,εθ) converges to the optimal plan L as εη → ∞ and εθ → ∞. Cooperative Inference is a special case of GBT with ε = (1, ∞, ∞), which is exactly entropic Optimal Transport (Cuturi, 2013).\n\nDiscriminative learning. A discriminative learner decodes an uncertain, possibly noise corrupted, encoded message, which is a natural bridge to information theory (Cover, 1999; Wang et al., 2020b). A discriminative learner builds an optimal map to hypotheses H conditioned on observed data D. The map is perfect when, for all messages, encodings are uniquely and correctly decoded. Intuitively, a discriminative learner aims to quickly build a deterministic code book (implies εP = 0) that matches the marginals on H and D. Thus, discriminative learner is GBT with ε = (0, ∞, ∞): Corollary 7. Consider a UOT problem with cost C = − log P(d, h), m = n, and marginals θ = η are uniform. The optimal UOT plan P (εP ,εη,εθ) approaches to a diagonal matrix as εη, εθ → ∞ and εP → 0. In particular, discriminative learner is a special case of GBT with ε = (0, ∞, ∞), which is exactly classical Optimal Transport (Villani, 2008).\n\nMany other interesting models are unified under GBT framework as well. GBT with ε = (0, ∞, 0) denotes Row Greedy learner which is widely used in Reinforcement learning community (Sutton and Barto, 2018); ε = (∞, ∞, ∞) yields η ⊗ θ which is independent coupling used in χ2 (Fienberg et al., 1970); ε = (εP , εθ, ∞) is used for adaptive color transfer studied in (Rabin et al., 2014); and ε = (0, εθ, εη) is UOT without entropy regularizer developed in (Chapel et al., 2021). Other points in the GBT parameter space are also of likely interest, past or future.\n\n3.3 GENERAL PROPERTIES ON THE TRANSPORTATION PLANS\n\nThe general GBT framework builds a connection between the above theories, and the behavior of theory varies according to the change of parameters. In particular, each factor of ε = (εP , εη, εθ) expresses different constraints of the learner. Given (C, θ, η) as shown in the top-left corner of Fig. 1, we plot each learner’s UOT plan with darker color representing larger elements.\n\nεP controls a learner’s learning horizon. When εP → 0, agents are under the time pressure of making immediate decision, hence GBT converges a discriminative learner, or Row Greedy learner on the bottom of the cube (Fig. 1). Their UOT plans have a clear leading diagonal which allows them to make fast decisions. Most of the time, one datum is enough to identify the true hypothesis and convergence is achieved within every data observation. When εP → ∞, GBT converges to a reticent learner, such as learners on the top of the cube. Data do not constrain the true hypothesis, and learners draw their conclusions independent of the data. In between, GBT provides a generative (probabilistic) learner. When εP = 1, we have Bayesian learner and Cooperative learner, for whom data accumulate to identify the true hypothesis in a manner broadly consistent with probabilistic inference, and consistency is asymptotic.\n\nεη controls a learner’s knowledge on the data distribution η. When εη → ∞, GBT converges to a learner who is aware of the data distribution and reasons about the observed data according to the probabilities/costs of possible outcomes. Examples include the Discriminative and Cooperative\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nlearners on the front of the cube. When εη → 0, GBT converges to a learner who updates their belief without taking η into consideration, such as Bayesian learners on the back of the cube, and the Tyrant who does not care about data nor cost and is impossible to be changed by anybody.\n\nεθ controls the strength of availability of prior knowledge for the learner. When εθ → ∞, GBT converges to a learner who enforces a prior over the hypotheses, such as Bayesian, Cooperative and Discriminative learners on the right of the cube. Actually, BGT follows Bayes rule when εθ = ∞ (Prop 8). When εθ → 0, GBT converges to learners who utilizes no prior knowledge. Hence they do NOT maintain beliefs over H, and draws their conclusions purely on the data distribution, such as a Frequentist learner η ⊗ 1 on the left of the cube. Proposition 8. In GBT with εθ = ∞, cost C and current belief θ. The learner updates θ with UOT plan in the same way as applying Bayes rule with likelihood from P ε(C, η, θ), and prior θ.\n\n4 SEQUENTIAL GBT: ASYMPTOTIC BEHAVIOR\n\nOne interesting difference between the one-shot case considered above and the sequential case is the possibility of observing many data points. In addition to the learning models in the GBT parameter space, in this section, we consider whether the learner’s marginal on the data is fixed a priori, or accumulates evidence based on experience.\n\n4.1 BASICS\n\nThe sequential GBT model consists of a teacher and a learner. The teacher samples data from a probability distribution η (not necessarily related to some h ∈ H), and the learner follows GBT with cost C, and parameter ε. The learner starts with a prior θ0, and applies in each round k GBT with ηk−1 and θk−1 to generate θk through the UOT solution Mk. In the Preliminary sequential model (PS), we assume ηk = η for all k. However, in practice, a learner does not have access η = P(d|h∗). Instead, in each round the learner may choose to use the current statistical distribution in data as an estimation of η, i.e., ηk(d) ∝ |{i : i < k, di = d}| + n0(d) according to the observed data sequence, where n0(d) > 0 (e.g., 1 as in add-one smoothing (Murphy, 2012)) is the prior counts to avoid a.s−−→ η. It is easy to see that the zero in η. Thus we have the Real sequential model (RS) where ηk sequence of posteriors form a time-homogeneous Markov chain on P(H).\n\nIn statistics, a model is said to be consistent (strongly-consistent) when, for every fixed hypothesis h ∈ H, the model’s belief θ over the hypotheses set H converges to δh in probability (almost surely) as more and more data are sampled from η = P(d|h), when θ’s are considered random variables. The consistency has been well studied for Bayesian Inference since Bernstein and von Mises and Doob (Doob, 1949), and recently demonstrated for Cooperative Communication (Wang et al., 2020a). The challenging problem arises when one tries to learn a h∗ that is not contained in the pre-selected hypothesis space H. It is not clear which h ∈ H is the ‘correct’ target to converge to. Thus consistency does not fit the situation in sequential GBT.\n\nFor sequential GBT models, we state the properties directly in the language of posterior sequence (Θk)∞ k=1 as random variables, and name them if necessary. We focus on whether the sequence converges (and in which sense), and how conclusive (how likely to provide a stable, fixed h ∈ H as the result) the sequence is. We provide some theoretical conclusions, and fill the gaps with empirical results and conjectures.\n\n4.2 RESULTS AND CONJECTURES\n\nResults in this section are stated on different εθ values. According to Prop. 1, we could focus on εP = 1 for generic cost matrix C, and general result of (εP , εη, εθ) becomes the same as the (1, εη/εP , εθ/εP ) case. So we choose εP = 1 in simulations.\n\nεθ = ∞: Conclusive and Bayesian-style. These are located on the right side of Fig. 1, and contain many well-studied learners: Bayesian, Cooperative, Discriminative, Row Greedy etc. According to Prop 8, learners in this class perform “Bayesian” style learning.\n\nThere are two theoretical results: εη = 0 (Bayesian) and εη = ∞ (SCBI learner (Wang et al., 2020a)). Others are explored in simulations.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem 9 ((Doob, 1949),(Wang et al., 2020a)). In GBT sequential model (both (PS) and (RS)) with ε = (εP , 0, ∞) where εP ∈ (0, ∞), the sequence Θk converges to some δh almost surely, h is the closest column of e−C/εP to η in the sense of KL-divergence, when θ is positive (on each entry).\n\nWhen εη = εθ = ∞, the models (PS) and (RS) have slightly different behaviors. Lemma 10. For ε = (εP , ∞, ∞), εP ∈ (0, ∞), given cost C with initial belief θ0 ∈ P(H) and fixed teaching and learning distribution ηk = η ∈ P(D) for all k, i.e., model (PS), then the belief random variables (Θk)k∈N have the same expectation on h: EΘk [θ(h)] = θ0(h).\n\nFigure 2: Evidence of general consistency: we plot the percentage of episodes that reaches a threshold (0.999) by round number (in colors of the bars). Each bar represents a size of matrix, for each bar 100 matrices were randomly sampled, and 1000 rounds were simulated per matrix. “exact” means learner uses ηk = η, (PS), “update” means learner uses statistics on current data in the episode (RS). “uot” takes ε = (1, 40, 40) and “ot” comes with exact and ε = (1, ∞, ∞).\n\nTheorem 11 (PS). Consider a learning problem with initial belief θ0 ∈ P(H), and the true hypothesis h∗ defined by η ∈ P(D). If the learner’s data distribution ηk = η, then belief random variables (Θk)k∈N converge to the random variable Y in probability, where Y = (cid:80) h∈H θ0(h)δh and Y is supported on {δh}h∈H with P(Y = δh) = θ0(h) for εη = εθ = ∞ and εP ∈ (0, ∞). Corollary 12. Given a fixed data sequence di sampled from η, if θk converges to δhj , then the j-th column of Mk converges to η.\n\nThus a GBT learner, with access to the data distribution and using strict marginal constraints, converges to a distribution on D same as η with probability 1. Moreover, the probability of which column h is shaped into η is determined by their prior θ0. That is, GBT learners converge to the truth by changing one of their original hypotheses into the true hypothesis.\n\nFor the (RS) model, the result is similar, but Lemma 10 fails to hold:\n\nProposition 13. Consider a learning problem with cost C, initial belief θ0 ∈ P(H), the true hypothesis h∗ defined by η ∈ P(D). For the (RS) problem, the belief random variables (Θk)k∈N P(Θ(h) > 1 − s) = 1. As a consequence, Mk as the transport satisfies that for any s > 0,\n\n(cid:80)\n\nlim k→∞\n\nh∈H\n\nplan has a dominant column (hj) with total weights > 1 − s, and |(Mk)ij − ηk(i)| < s.\n\nIn fact, as long as the sequence of ηk as random variables converges to η in probability, the above proposition holds. The limit\n\nP(Θ(h) > 1 − s) measures how conclusive the model is.\n\n(cid:80)\n\nlim k→∞\n\nh∈H\n\nIn contrast with standard Bayesian or other inductive learners, Proposition 13 shows that a GBT learner is able to learn any hypothesis mapping η = P(d|h∗) up to a given threshold s with probability 1. In addition to unifying disparate models of learning, GBT enables a fundamentally more powerful approach to learning by empirically monitoring the data marginal.\n\nFig. 2 illustrates convergence over learning problems and episodes. In each bar, we sample 100 learning problem (C, θ0, h∗) from Dirichlet distribution with hyperparameters the vector 1. Then we sample 1000 data sequences (episodes) of maximal length N = 10000. The learner learns with Algorithm 1 where the stopping condition ω is set to be maxh∈H θ(h) > 1 − s with s = 0.001. The y-axis in the plots represents the percentage of total episode converged. The color indicates in how many rounds the episode converges. For instance, in the bar corresponding to ‘10 × 10_update_uot’, with 10 data points (yellow portion), about 50% episodes satisfy the stopping condition.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nIn Figure 2, the first plot shows results for 10 × 10 and 5 × 3 matrices. The second plot shows results for rectangular matrices of dimension m × 10 with m ranges in [5, 10, 25, 50, 100]. The third plot shows results for square matrices of dimension m × m with m ranges in [10, 25, 50, 100]. Here ‘exact’ and ‘update’ indicate the problem is (PS) or (RS), respectively. For parameters, uot represents the parameter choice (εP = 1, εθ = εη = 40) vs. ot represents the parameter choice (εP = 1, εθ = εη = ∞). The first plot illustrates that learners that do not have access to the true hypothesis (empirically builds estimation of η) learn faster than learners who have full access. The second plot indicates with a fixed number of hypotheses, learning is faster when the dimension of D increases. The third plot shows that the GBT learner scales well with the dimension of the problem.\n\nFigure 3: Left: Behavior of models spanning the line segment between BI and CI. With εP = 1 and εθ = ∞, when εη varies from 0 to ∞, the theory changes from BI to CI. Each bar graphs the Monte-Carlo result of 400,000 teaching sequences, we empirically observe that the coefficients a(h) of the limit in terms of (cid:80) h∈H a(h)δh changes from BI to CI continuously from δ(h3) by Bernstein-von Mises to θ0(h) by Theorem 11. Right: the Euclidean distances of each coefficient a(h) to BI result (blue crosses), and to CI result (orange dots).\n\nThen we study the learners that interpolate between Bayesian and Cooperative learners (located on the line connecting CI and BI in Fig 1). Consider a fixed learning problem (C, θ0, h∗). Consistency of Bayesian inference states that asymptotically, the learner Bayesian converges to a particular hypothesis hb ∈ H almost surely where hb is the hypothesis closest to h∗ under KL divergence. Theorem 11 indicates that a GBT cooperative learner modifies one of the hypotheses into h∗ in probability 1. The probability of hj converges to h∗ is determined by θ0(hj).\n\nIn Fig. 3, we study the asymptotic behavior of the learners corresponding to ε = (1, εη, ∞), with εη ∈ {0, 0.02, 0.2, 0.5, 1, 2, 5, 50, ∞}. We sample a learning problem with a dimension 5 × 5 from Dirichlet distribution with hyperparameters the vector 1. Each learner ε = (1, εη, ∞) is equipped with a fixed C, θ0 and ηk = η for all k. We run 400, 000 learning episodes per learner, and plot their convergence summary in the bar graph. A continuous transition from a Bayesian learner to a cooperative learner can be empirically observed: the coefficients a(h) of the limit in terms of (cid:80)\n\nh∈H a(h)δh changes from δ(h3) by Bernstein-von Mises to θ0(h) by Theorem 11.\n\nFrom the previous empirical results, we conclude the following conjecture:\n\nConjecture 14. When ε = (εP , εη, ∞), where εP ∈ (0, ∞), the sequence of posteriors Θk from generic C, η, θ and ε as random variables satisfy lim k→∞\n\nP(|Θk(h) − 1| < e) = 1 for any e > 0.\n\n(cid:80)\n\nh∈H\n\nWe further report an empirical property observed in simulation, which suggests a possible rate of convergence. For given C, θ0 and η, fix εP = 1 and εθ = ∞, as εη changes from 0 to ∞, we pick out those episodes with θN (h) > 0.95 and plot the values EθN (h)>0.95[ln θk(h) − ln(1 − θk(h))] for each h against k (Fig. 4 bottom). Near linear relations are observed away from the first several rounds and before the values reaches the precision threshold. These are empirical estimates of the rate of convergence.\n\nThere is a special case on the boundary, the Independent Coupling (∞, ∞, ∞), whose limit is taken vertically along εP -axis, see Sec. 3.1. Independent Coupling has a fixed posterior, where Law(Θk) = δθ0, as the normalization of each row of P (∞,∞,∞) is θ0.\n\nεθ = 0: Inconclusive and independent. The following holds for both (PS) and (RS):\n\nProposition 15. For ε = (εP , εη, 0) with εP ∈ (0, ∞), as ηk → η almost surely, the sequence Θk of posteriors as a sequence of random variables converges in probability to variable Θ, where\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Top: For a learning problem C, behaviors of 9 different learners with εP = 1, εθ = ∞ and various εη (denoted in figure) on conclusion distributions, a(h) in bar graph, plots below bars are estimated convergence rates E ln(θk(h)/(1 − θk(h))) averaged on episodes converging to h, one curve per hypothesis.\n\nP(Θ = vi) = η(i) and vi = P(i,_)/ limk→∞\n\nand P = P ε(C, η, θ). Therefore, for any s > 0, P(|Θk(h) − 1| < s) = 0 for generic (for all but in a closed subset) cost C and η, θ.\n\nj=1 Pij\n\n(cid:80)\n\nh∈H\n\n(cid:16)(cid:80)m\n\n(cid:17)\n\nWith εθ = 0, the constraint on column-sum (εη-term) fails to affect the transport plan, thus the Θk’s in the sequence are independent from each other, in contrast that in all other cases the adjacent ones are correlated via a nondegenerate transition distribution. The independence makes the sequence of posterior-samples in one episode behave totally random, thus rarely converge as points in P(H). Furthermore, when consider the natural coupling (Θk−1, Θk) from Markov transition measure for εθ = 0 (which is independent), E (cid:0)|Θk−1 − Θk|2(cid:1) converges to the variance V ar(η). In contrast, for εθ = ∞, E (cid:0)|Θk−1 − Θk|2(cid:1) converges to 0 if Conj. 14 holds.\n\nεθ ∈ (0, ∞): partially conclusive. From Conj. 14 and Prop. 15, together with the continuity of the transition distribution on ε, we conjecture the following continuity on conclusiveness when εP ∈ (0, ∞).\n\nConjecture 16. For both (PS) and (RS) models, when ε = (εP , εη, εθ) with εP , εθ ∈ (0, ∞), the posP(|Θk(h)− terior sequence Θk from generated from generic C, η, θ and ε satisfy that limk→∞ 1| < s) = L exists, and L ∈ (0, 1), for any s > 0.\n\nh∈H\n\n(cid:80)\n\n5 RELATED WORK\n\nPrior work defines and outlines basic properties of Unbalanced Optimal Transport (Liero et al., 2018; Chizat et al., 2018; Pham et al., 2020). Bayesian approaches are prominent in machine learning (Murphy, 2012) and beyond (Jaynes, 2003; Gelman et al., 1995). There is also research on cooperative learning (Wang et al., 2019; 2020b;a) see also (Liu et al., 2021; Yuan et al., 2021; Zhu, 2015; Liu et al., 2017; Shafto and Goodman, 2008; Shafto et al., 2014; Frank and Goodman, 2012; Goodman and Frank, 2016; Fisac et al., 2017; Ho et al., 2018; Laskey et al., 2017). Discriminative learning is the reciprocal problem in which one sees data and asks which hypothesis best explains it (Ng and Jordan, 2001; Mandler, 1980). We are unaware of any work that attempts to unify and analyze the general problem of learning in which each of these are instances.\n\n6 CONCLUSIONS\n\nWe have introduced Generalized Belief Transport (GBT), which unifies and parameterizes classic instances of learning including Bayesian inference, Cooperative Inference, and Discrimination, as Unbalanced Optimal Transport (UOT). We show that each instance is a point in a continuous, differentiable on the interior, 3-dimensional space defined by the regularization parameters of UOT. In addition to supporting generalized learning, we prove and illustrate asymptotic consistency and estimate rates of convergence, including convergence to hypotheses with zero prior support. In summary, GBT unifies very different modes of learning, yielding a powerful, general framework for modeling learning agents.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nETHIC STATEMENT\n\nThe main contributions of this paper are theoretical, rather than practical, in nature. While understanding learning and inference in more unified and generalized ways may have broad impact including causing any ethic problems, nothing is likely to be realized as a direct consequence of this work.\n\nREFERENCES\n\nLaetitia Chapel, Rémi Flamary, Haoran Wu, Cédric Févotte, and Gilles Gasso. Unbalanced optimal transport through non-negative penalized linear regression. Advances in Neural Information Processing Systems, 34:23270–23282, 2021.\n\nLenaic Chizat, Gabriel Peyré, Bernhard Schmitzer, and François-Xavier Vialard. Scaling algorithms for unbalanced optimal transport problems. Mathematics of Computation, 87(314):2563–2609, 2018.\n\nThomas M Cover. Elements of information theory. John Wiley & Sons, 1999.\n\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in\n\nneural information processing systems, pages 2292–2300, 2013.\n\nJoseph L Doob. Application of the theory of martingales. Le calcul des probabilites et ses applications,\n\npages 23–27, 1949.\n\nStephen E Fienberg et al. An iterative procedure for estimation in contingency tables. The Annals of\n\nMathematical Statistics, 41(3):907–917, 1970.\n\nJaime F Fisac, Monica A Gates, Jessica B Hamrick, Chang Liu, Dylan Hadfield-Menell, Malayandi Palaniappan, Dhruv Malik, S Shankar Sastry, Thomas L Griffiths, and Anca D Dragan. Pragmaticpedagogic value alignment. arXiv preprint arXiv:1707.06354, 2017.\n\nMichael C Frank and Noah D Goodman. Predicting pragmatic reasoning in language games. Science,\n\n336(6084):998–998, 2012.\n\nAndrew Gelman, John B Carlin, Hal S Stern, and Donald B Rubin. Bayesian data analysis. Chapman\n\nand Hall/CRC, 1995.\n\nNoah D Goodman and Michael C Frank. Pragmatic language interpretation as probabilistic inference.\n\nTrends in cognitive sciences, 20(11):818–829, 2016.\n\nMark K Ho, Michael L Littman, Fiery Cushman, and Joseph L Austerweil. Effectively learning from pedagogical demonstrations. In Proceedings of the Annual Conference of the Cognitive Science Society, 2018.\n\nEdwin T Jaynes. Probability theory: The logic of science. Cambridge university press, 2003.\n\nHarold Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences, 186(1007):453–461, 1946.\n\nLeonid V Kantorovich. On the translocation of masses. Journal of Mathematical Sciences, 133(4):\n\n1381–1382, 2006.\n\nMichael Laskey, Caleb Chuck, Jonathan Lee, Jeffrey Mahler, Sanjay Krishnan, Kevin Jamieson, Anca Dragan, and Ken Goldberg. Comparing human-centric and robot-centric sampling for robot deep learning from demonstrations. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 358–365. IEEE, 2017.\n\nMatthias Liero, Alexander Mielke, and Giuseppe Savaré. Optimal entropy-transport problems and a new hellinger–kantorovich distance between positive measures. Inventiones mathematicae, 211(3): 969–1117, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWeiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, James M Rehg, and Le Song. Iterative machine teaching. In International Conference on Machine Learning, pages 2149–2158. PMLR, 2017.\n\nWeiyang Liu, Zhen Liu, Hanchen Wang, Liam Paull, Bernhard Schölkopf, and Adrian Weller. Iterative\n\nteaching by label synthesis. Advances in Neural Information Processing Systems, 34, 2021.\n\nGeorge Mandler. Recognizing: The judgment of previous occurrence. Psychological review, 87(3):\n\n252, 1980.\n\nGaspard Monge. Memory on the theory of excavations and embankments. History of the Royal\n\nAcademy of Sciences of Paris, 1781.\n\nKevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.\n\nAndrew Ng and Michael Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Advances in neural information processing systems, 14, 2001.\n\nGabriel Peyré and Marco Cuturi. Computational optimal transport. Foundations and Trends in\n\nMachine Learning, 11(5-6):355–607, 2019.\n\nKhiem Pham, Khang Le, Nhat Ho, Tung Pham, and Hung Bui. On unbalanced optimal transport: An analysis of sinkhorn algorithm. In International Conference on Machine Learning, pages 7673–7682. PMLR, 2020.\n\nJulien Rabin, Sira Ferradans, and Nicolas Papadakis. Adaptive color transfer with relaxed optimal transport. In 2014 IEEE international conference on image processing (ICIP), pages 4852–4856. IEEE, 2014.\n\nChristian P Robert, Nicolas Chopin, and Judith Rousseau. Harold jeffreys’s theory of probability\n\nrevisited. Statistical Science, 24(2):141–172, 2009.\n\nPatrick Shafto and Noah D. Goodman. Teaching games: Statistical sampling assumptions for learning in pedagogical situations. In Proceedings of the 30th annual conference of the Cognitive Science Society, Austin, TX, 2008. Cognitive Science Society.\n\nPatrick Shafto, Noah D Goodman, and Thomas L Griffiths. A rational account of pedagogical reasoning: Teaching by, and learning from, examples. Cognitive Psychology, 71:55–89, 2014.\n\nPatrick Shafto, Junqi Wang, and Pei Wang. Cooperative communication as belief transport. Trends in\n\ncognitive sciences, 25(10):826–828, 2021.\n\nRichard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matrices.\n\nPacific Journal of Mathematics, 21(2):343–348, 1967.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nCédric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,\n\n2008.\n\nJunqi Wang, Pei Wang, and Patrick Shafto. Sequential cooperative bayesian inference. In International\n\nConference on Machine Learning, pages 10039–10049. PMLR, 2020a.\n\nPei Wang, Pushpi Paranamana, and Patrick Shafto. Generalizing the theory of cooperative inference.\n\nAIStats, 2019.\n\nPei Wang, Junqi Wang, Pushpi Paranamana, and Patrick Shafto. A mathematical theory of cooperative\n\ncommunication. Advances in Neural Information Processing Systems, 33, 2020b.\n\nScott Cheng-Hsin Yang, Yue Yu, Arash Givchi, Pei Wang, Wai Keen Vong, and Patrick Shafto. Optimal cooperative inference. In AISTATS, volume 84 of Proceedings of Machine Learning Research, pages 376–385. PMLR, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nLuyao Yuan, Dongruo Zhou, Junhong Shen, Jingdong Gao, Jeffrey L Chen, Quanquan Gu, Ying Nian Wu, and Song-Chun Zhu. Iterative teacher-aware learning. Advances in Neural Information Processing Systems, 34:29231–29245, 2021.\n\nXiaojin Zhu. Machine teaching: An inverse problem to machine learning and an approach toward\n\noptimal education. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA ADDITIONAL MATERIALS\n\nAlgorithm 3 Unbalanced Sinkhorn Scaling\n\ninput: C, θ, η, ε = (εP , εη, εθ), N stopping condition ω initialize: K = exp(−εP C), v(0) = 1m while k < N and not ω do\n\nu(k) ← (\n\nη\n\nKv(k−1) )\n\nεη +εP , v(k) ← (\n\nθ\n\nKT u(k) )\n\nεη\n\nεθ εθ +εP\n\nend while output: M = diag(u)Kdiag(v)\n\nCooperative Communication. Cooperative communication formalizes a single problem comprised of interactions between two processes: teaching and learning. The teacher and learner have beliefs about hypotheses, which are represented as probability distributions. The process of teaching is to select data that move the learner’s beliefs from some initial state, to a final desired state. The process of learning is then, given the data selected by the teacher, infer the beliefs of the teacher. The teacher’s selection and learner’s inference incur costs. The agents minimize the cost to achieve their goals. Communication is successful when the learner’s belief, given the teacher’s data, is moved to the target distribution. Formally, denote the common ground between agents: the shared priors on H and D by P(h) and P(d), the shared initial matrix over D and H by M of size |D| × |H|. In general, up to normalization, M is simply a non-negative matrix which also specifies the consistency between data and hypotheses4\n\nij)|M|×|H| is defined as C L\n\nIn cooperative communication, a learner’s goal is to minimize the cost of transforming the observed data distribution P(D) to the shared prior over hypotheses P(H). A learner’s cost matrix C L = (C L ij = − log M . A learning plan is a joint distribution L = (Lij), where Lij = PL(di, hj) represents the probability of the learner inferring hj given di. It is proved in (Wang et al., 2019) that: Proposition 17. Optimal cooperative communication plans, L, is the EOT plan with cost C L and marginals being η = P(d) and θ = P(h).\n\nB PROOFS\n\nProposition 1. The UOT problem with cost matrix C, marginals θ, η and parameters ε = (εP , εη, εθ) generates the same UOT plan as the UOT problem with tC, θ, η, tε = (tεP , tεη, tεθ) for any t ∈ (0, ∞).\n\nProof. Consider that the UOT problem solution is\n\nP ε(C, η, θ) = arg min\n\nP ∈(R≥0)n×m\n\n{⟨C, P ⟩ − εP H(P ) + εηKL(P 1|η) + εθKL(P T 1|θ)}.\n\n(2)\n\nwhere the objective function is linear on C and ε.\n\nP tε(tC, η, θ) =\n\narg min P ∈(R≥0)n×m\n\n{⟨tC, P ⟩ − tεP H(P ) + tεηKL(P 1|η) + tεθKL(P T 1|θ)}\n\n=\n\narg min P ∈(R≥0)n×m = P ε(C, η, θ).\n\nt · {⟨C, P ⟩ − εP H(P ) + εηKL(P 1|η) + εθKL(P T 1|θ)}\n\n(3)\n\nProposition 2. The UOT plan P in Equation (1), as a function of ε, is continuous in (0, ∞) × [0, ∞)2. Furthermore, P is differentiable with respect to ε in the interior.\n\n4Data, di, are consistent with a hypothesis, hj, when Mij > 0.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nProof. For simplicity, in this proof, for a vector v, we use both vi and v(i) to represent a component of v.\n\nBy definition, the UOT plan P minimizes the objective function Ω(P ; ε) = ⟨C, P ⟩ − εP H(P ) + εηKL(P 1|η) + εθKL(P T 1|θ). Since Ω is a strict convex function on P , there is only one minimal P . So the UOT plan P is the solution to ∇P Ω = 0. From a direct calculation,\n\n(∇P Ω)ij = Cij + εP ln Pij + εη(ln(\n\nm (cid:88)\n\nPik) − ln η(i)) + εθ(ln(\n\nn (cid:88)\n\nPkj) − ln θ(j))\n\nk=1\n\nk=1\n\nand\n\n(∇2\n\nP Ω)ijkl =\n\nεP δikδjl Pij\n\n+\n\nεηδik t=1 Pit\n\n(cid:80)m\n\n+\n\nεθδjl t=1 Ptj\n\n(cid:80)n\n\n.\n\nAs we assume that Pij > 0 for all i, j, all the terms above are well-defined. Besides, ∇P Ω is C 1 on η, θ and ε. Therefore, we can show P ε(C, η, θ) is continuous not only on ε but also on η and θ after checking Hessian. From implicit function theorem, if we show the above Hessian is invertible for εP > 0, then the results of the proposition are true. Equivalently, it suffices to show that det H ̸= 0 where matrix H is the flattened ∇2\n\nP Ω by mapping (i, j, k, l) (cid:55)→ (im + j, km + l).\n\nInvertibility of H. Let r be the vector of reciprocals of row sums of P , i.e., ri = 1/ and similarly, let c be the vector of reciprocals of column sums of P , i.e., cj = 1/ ((cid:80)\n\n(cid:17)\n\n,\n\n(cid:16)(cid:80)\n\nj Pij i Pij). Then\n\n(∇2\n\nP Ω)ijkl =\n\nεP δikδjl Pij\n\n+ εηδikri + εθδjlcj.\n\nLet φ be the map (i, j) (cid:55)→ (im + j), then φ induces a reshaping of P to a vector of size mn, denoted by P φ. When there is no ambiguity, we may omit the φ superscript.\n\nFurther define pφ as a vector of dimension mn where pφ k . By definition, H φ = εP (diag(pφ)) + εη1m ⊗ (diag(r)) + εθ(diag(c)) ⊗ 1n where 1k is the k × k matrix of ones, and A ⊗ B is Kronecker product (tensor product of matrices). Decompose H = D + G where D = εP (diag(pφ)) and G = εη1m ⊗ (diag(r)) + εθ(diag(c)) ⊗ 1n.\n\nk = εP /P φ\n\nFrom now on, we may use P -row, P -column to represent i, j style indices, and G-row, G-column or simply row/column to represent those of G, or the ones in range [1, mn]. D is diagonal, and det G = 0. Furthermore,\n\n(∗)\n\nany row or column of G with index k can be represented by an entry position (i, j) of P by inverse of φ, and any rows of indices k1, k2, k3, k4 corresponding to (i1, j1), (i1, j2), (i2, j1), (i2, j2) (i.e., determined as intersections of two P -rows and two P -columns) is linearly dependent: G(k1,_) + G(k4,_) − G(k2,_) − G(k3,_) = 0, we denote this property as (∗).\n\nStructure of det H: Let D = diag(p1, p2, . . . , pmn), then det H is a polynomial on pk’s with constant term 0. Each term in det H is of form f (I) (cid:0)(cid:81) (cid:1) for each subset I ⊆ {1, 2, . . . , mn}, and the coefficient f (I) = det G(I,I) where G(I,I) is the submatrix with lines of indices not in I, i.e., the entries of G(I,I) are of the form Gij with i ∈ I and j ∈ I.\n\nk /∈I pk\n\nNext we show that f (I) is nonnegative for all I, then with pk > 0 for all k, we can conclude that det H > 0. Since I ⊆ {1, 2, . . . , mn}, φ−1(I) ⊆ {1, 2, . . . , n} × {1, 2, . . . , m}, and φ is a bijection, we may not distinguish I from φ−1(I), in order to make the statement neater.\n\n1. [Operation-(∗) on I]: We want to investigate the operations on I producing a subset J such that f (I) = f (J ). By the properties of determinant, (∗) induces one operation: when I containing 4 integer pairs which can form the vertices of a rectangle, f (I) = 0. Moreover, for any k1, k2, k3, k4 such indices in (∗), we can generate row G(k4,_) by G(k4,_) = G(k2,_)+G(k3,_)−G(k1,_), then if {k1, k2, k3} ⊆ I, we can build G(k4,_) on any G(ki,_), thus the determinant det Grow (I,I) = ± det G(I,I) (positive for k2 and k3, negative for k1 ). Similarly, if we follow the same operation on columns, we have det Gcol (I,I) = det G(I,I).\n\n(I,I) = ± det G(I,I). And when doing both, det Gcol·row\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTherefore, we know that if k1, k2, k3 ∈ I, and J = {k4} ∪ I\\{ki} for any i = 1, 2, 3, then f (I) = f (J ). Such operations changing I to J is denoted by operation-∗. In short, an operation-∗ moves an end of a small “L-shaped” set of 3 pairs along a P -row or a P -column, producing another L-shaped set of 3 pairs.\n\n2. [Regularized form of I, and decomposition of nondegenerate regularized form I ♯ into L-shaped subsets]: Once I or any J equivalent to I via operations-∗ contains 4 pairs satisfying condition (∗), f (I) = 0, then we call I degenerate. In decomposing I, when we find it degenerate, we stop since f (I) is known.\n\nWe decompose I as set of pairs inductively in the following way before stopping. Start with any (i, j) ∈ I, we look for pairs of form (i, l) and (k, j) in I, adding them into the subset A(i,j) containing (i, j). Then check the degeneracy, by looking for whether I contains a point (k, l) with (i, l), (j, k) ∈ A(i,j), whenever I is degenerate, we stop since f (I) = 0. Next we enlarge A(i,j) by changing the set I to a regularized form using operation-∗’s. For each (k, l) with (i, l) ∈ A(i,j), then (k, j) can be constructed on (k, l) via an operation-∗ with (i, j) and (i, l). Thus we modify I into J = (i, l) ∪ I\\(k, l) that f (I) = f (J ), and adding (i, l) into set A(i,j). Similar process can be done for those (k, l) ∈ I with (k, j) ∈ A(i,j).\n\nAfter regularizing I and enlarging A(i,j) to maximum about (i, j), we get a regularized form J of I, with f (I) = f (J ), and a component A(i,j) of L-shape. The set of J \\A(i,j) has no elements of form (k, l) with (i, l) ∈ A(i,j) or (k, j) ∈ A(i,j), as they are already moved to A(i,j) by operation- ∗. Therefore, J \\A(i,j) is supported on a rectangular region by deleting all P -rows (k, _)’s and P -columns (_, l)’s where k, l’s occur in A(i,j).\n\nRepeating the L-shaped component construction above for J \\A(i,j), we can transform I into a regularized form (not unique or standard) I ♯ and we have a decomposition I ♯ = (cid:83) A(it,jt) into L-shaped components, which do not intersect with each other. The name “regularized form” is given to the transformed set with a L-shaped decomposition, and since only operation-∗ is applied, f (I) = f (I ♯).\n\n3. [Properties between the L-shaped subsets:] For each I which we did not conclude f (I) = 0 in the last step, we get I ♯ and a decomposition I ♯ = (cid:83)\n\nt∈T At into L-shaped subsets.\n\nThe construction of components At induces such a property: for two distinct components At there is no elements (i, j) ∈ At and (k, l) ∈ As, in normal words, the At occupies certain P -rows and P -columns which is distinct from those of As.\n\nFor (i, j) and (k, l) with i ̸= k and j ̸= l, Gim+j,km+l = 0 from the formula that Gim+j,km+l = εηriδik + εθcjδjl. Therefore, the decomposition I ♯ = (cid:83) t∈T At induces a decomposition of matrix G(I♯,I♯) into blockwise diagonal matrix\n\n\n\n \n\n\nGA1,A1 0\n\n...\n\n0\n\n0 GA2,A2\n\n0\n\n0 0\n\n. . . . . . . . . . . . GAt,At\n\n...\n\n\n\n \n\n\n(4)\n\nSo for a decomposition I ♯ = (cid:83)\n\nt∈T At, we haves f (I ♯) = (cid:81)\n\nt∈T f (At)\n\n4. [f (A) for an L-shaped component]: The last part is to show f (A) > 0 for all L-shaped components. Recall that Gim+j,km+l = εηriδik + εθcjδjl, so for A an L-shaped component with s P -rows and t P -columns, G(A,A) in general is of form\n\nG(A,A) =\n\n\n\n \n \n \n \n \n\nr1 + c1 ... r1 r1 0\n\n...\n\n0\n\n. . . . . . . . . . . . . . . . . . . . .\n\nr1 ... r1 + ct−1 r1 0\n\n...\n\n0\n\nr1 ... r1 r1 + ct ct ... ct\n\n0\n\n...\n\n0 ct ct + r2 ... ct\n\n. . . . . . . . . . . . . . . . . . . . .\n\n\n\n \n \n \n \n \n\n0\n\n...\n\n0 ct ct ... ct + rs\n\n(5)\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nRecall the formula det\n\n(cid:21)\n\n(cid:20) E B C D\n\n= det(E) det(D − CE−1B) and the matrix determinant lemma\n\ndet(diag(c) + r11T ) = (1 + r1T diag(c)−11) det(diag(c)) =\n\n(cid:89)\n\nci(1 +\n\n(cid:88)\n\n(r/ci)).\n\nIf s = 1 or t = 1, the determinant of G(A,A) can be calculated directly by the matrix determinant lemma above.\n\nIf s > 1 and t > 1, we cut Eq. (5) into 4 blocks\n\n(cid:21)\n\n(cid:20) E B C D\n\nwhere E contains the upper left t × t\n\npart, B is zero but the last row, C is zero but the last column, D is a matrix in a similar form as E.\n\nt,t 1T which is an s × s-matrix. The entry E−1\n\nAccording to the characters of B, C stated above, t 1E−1 c2 E(1:t−1,1:t−1) (cid:16)(cid:81)t−1\n\nis the matrix E without\n\n(cid:16)(cid:81)t\n\nit can be found that CE−1B = t,t = det E(1:t−1,1:t−1)/ det E where t,t =\n\nthe last row and last column, moreover, E−1 1 + (cid:80)t−1 ct(1 + (cid:80)t\n\n(r1/ci) 1(r1/ci))\n\n1(r1/ci))\n\n1 ci(1 + (cid:80)t\n\nci(1 + (cid:80)t−1\n\n< 1/ct. There-\n\nfore, CE−1B = λ11T with λ < ct and D − CE−1B = diag(r2:s) + (ct − λ)11T , whose determinant is positive according to the matrix determinant lemma.\n\n(r1/ci))\n\n=\n\n(cid:17)\n\n(cid:17)\n\n/\n\n1\n\n1\n\n1\n\nAs a consequence, det G(A,A) > 0 for each L-shaped components A. So combining the discussions in [1-4], we have det H = det(D + G) > 0.\n\nThen the implicit function theorem implies the differentiability of P ε on ε.\n\nProposition 3. For any finite sP , sη, sθ ≥ 0, the limit of P ε exists as ε approaches to (∞, sη, sθ). In fact, limε→(∞,sη,sθ) P ε\n\nij = 1 for all i, j (Limit 1). Moreover, P ε converges to the solution to\n\nmin⟨C, P ⟩ − sP H(P ) + sθKL(P T 1|θ), with constraint P 1 = η,\n\nas ε → (sP , ∞, sθ) (Limit 2). Similarly, P ε converges to the solution to\n\nmin⟨C, P ⟩ − sP H(P ) + sηKL(P 1|η), with constraint P T 1 = θ,\n\n(6)\n\n(7)\n\nas ε → (sP , sη, ∞) (Limit 3). And in the case when ε → (sP , ∞, ∞), the matrix P ε converges to the EOT solution (Limit 4):\n\nmin⟨C, P ⟩ − sP H(P ), with constraints P T 1 = θ and P 1 = η.\n\n(8)\n\nWhen ε → (∞, ∞, sθ), (∞, sη, ∞) or (∞, ∞, ∞), the limit does not exist, but the directional limits can be calculated..\n\nProof. Recall that H(P ) = − (cid:80) ij(Pij ln Pij − Pij), (∇P H)ij = − ln Pij, and H(P ) is strictly concave, therefore H has a unique maximum mn at Pij = 1, denoted by 1. Similarly, KL(a|b) = (cid:80) i(ai(ln ai − ln bi) − ai + bi), ∇aKL(a|b)i = ln ai − ln bi, KL is strictly convex, therefore KL\n\nhas a minimum 0 at ai = bi for all i. Limit 1. Shown by contradiction: When ε → (∞, sη, sθ), suppose the limit limε→(∞,sη,sθ) P ε ij for some (i, j) does not exist, or is not 1. Thus there is e > 0 that, for any δ > 0 and N > 0, there exists a parameter ε1 = (εP , εη, εθ) such that εP > N , |εη − sη| < δ and |εθ − sθ| < δ, satisfying |P ε\n\nij − 1| > e.\n\nHowever, for any 0 < e < 1/2, let δ = 1, let E = (1+e) ln(1+e)−(1+e)+1 > 0, min Ω(P ; ε) ≤ Ω(1; ε) < C for some G > 0 where (1)ij = 1 for all (i, j), and any ε ∈ {(εP , εη, εθ) : sη/2 < εη < 3sη/2, sθ/2 < εθ < 3sθ/2, }. So there is a N > 0 such that N E > G + maxij Cij + mn + L where L = − inf{εηKL(P 1|η) + εθKL(P t1|θ)}, meaning those P with |Pij − 1| > e for some (i, j) is not minimizing Ω.\n\nThe contradiction indicates that limε→(∞,sη,sθ) P ε Limit 2 & 3: The situation of εθ → ∞ and εη → ∞ are similar, so we only prove for εθ → ∞ case. Let ˆP denote the solution to Eq. (7).\n\nij = 1 for all i, j.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nˆP be the solution to the optimization with constraints.\n\nWe first\n\nshow that\n\nLet limε→(sP ,sη,∞)\n\n(cid:80)n\n\nk=1 P ε\n\nkj = θj.\n\nThis is similar to limit 1. Suppose the limit either does not exist or is not θj, then there exists an e > 0 such that for any N > 0, δ > 0, there exists εθ > N , |εη − sη| < δ and |εP − sP | < δ, such that\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nn (cid:88)\n\nk=1\n\nP ε\n\nkj − θj\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n> e\n\n(9)\n\nfor some j. Thus KL((P ε)T 1|θ) > E for some E > 0. Consider that ⟨C, P ⟩ ≥ 0, H(P ) ≥ −mn and KL(P 1|η) ≥ 0 are lower bounded, we can take sufficiently large N such that the P ε satisfying Eq. (9) satisfy Ω(P ε; ε) > Ω( ˆP ; ε), making P ε fail to optimize Ω(·; ε), which is a contradiction. Thus we have limε→(sP ,sη,∞)\n\nkj = θj.\n\nk=1 P ε\n\n(cid:80)n\n\nFor each ε = (εP , εη, εθ), let θε denote the (P ε)T 1, then for any ε, the solution P ε is also the solution to\n\n⟨C, P ⟩ + εP H(P ) + εηKL(P 1|η), with constraint P T 1 = θε.\n\n(10)\n\nmin P\n\nDenote Φ(P, εP , εη) := ⟨C, P ⟩ + εP H(P ) + εηKL(P 1|η) When εP ∈ (0, ∞), the new objective function Φ(P, εP , εη) is continuous on P and εP ,εη, and each minimization problem gets a unique solution since the objective function is strictly convex. Therefore, the limit limε→(sP ,sη,∞)P ε = (cid:98)P . We show this via contradiction: Suppose the opposite, there exists some ξ > 0 such that ||P ε − ˆP ||2 > ξ for ε arbitrarily close to (sP , sη, ∞). Let\n\nα :=\n\ninf P T e=θ,||P − ˆP ||2>ξ\n\nΦ(P, sP , sη) − Φ( ˆP , sP , sη),\n\nα > 0 since the minimum ˆP is unique and the objective is strictly convex. The sets P T e = θε is compact since it is closed and bounded, so there exists bounds b = (b1, b2, b3) for ε = (εP , εη, εθ) such that in the bound where |εP − sP | < b1, |εη − sη| < b2 and εθ > b3, max Φ(P, sP , sη) − Φ(P ♯, εP , εη) < α/3 for P with P T e = θ and P ♯ its Euclidean projection to {P T e = θε}, and max Φ(P, εP , εη) − Φ(P ♭, sP , sη) < α/3 for P with P T e = θε and P ♭ its Euclidean projection to {P T e = θ}.\n\nLet ε be a parameter in the above bound b to (sP , sη, ∞), where P = argminP T e=θε Φ(P, εP , εη) is ξ far from ˆP . Then Φ(P, εP , εη) > Φ(P ♭, sP , sη) − α/3 > Φ( ˆP , sP , sη) + 2/3α > Φ( ˆP ♯, εP , εη) + α/3 > Φ( ˆP ♯, εP , εη), which is a contradiction to the assumption that P is the argmin.\n\nLimit 4: Similar to the previous two limits, we can say that limε→(sP ,∞,∞) limε→(sP ,∞,∞) solution.\n\nkj = θj and ik = ηi. Then the problem becomes the EOT problem, which has a unique\n\nk=1 P ε\n\n(cid:80)m\n\n(cid:80)n\n\nk=1 P ε\n\nBoundaries at εη = 0 or εθ = 0: It is simple to check the continuity when εη → 0 or εθ → 0. From Prop. 2, the continuity and differentiability hold for εη → 0 or εθ → 0 when εP > 0.\n\nNonexistence of the limits when εP , εη → ∞, and directional limits: Let a sequence ε1, ε2, . . . where εi = (εi P ) = t, then the limit P of P ε satisfy Pij = t(ln cj − ln n)/(t + 1), since the limit minimizes the following objective function\n\nη = ∞ and lim(εi\n\nθ) satisfy lim εi\n\nP = lim εi\n\nP , εi\n\nη/εi\n\nη, εi\n\nH(P ) + tKL(P 1|η).\n\nThe reason is, as (cid:80) ηi = 1, H(P ) and KL(P 1|η) cannot vanish for the same P , thus the minima of objective function approaches to infinity, therefore the finite terms ⟨C, P ⟩ and εθKL(P T 1|θ) tend to have no effect on the minimal point P as εP , εη increases to infinity.\n\nA direct consequence of the above discussion is, when t changes, the limits P of those sequences changes, which indicates that the limit of P ε as ε → (∞, ∞, sθ) fails to exist. And similar situation happens when ε → (∞, sη, ∞)\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nNonexistence of the limits when εP , εη, εθ → ∞, and directional limits : Similar to the discussions above, let the sequence ε1, ε2, . . . where εi = (εi θ) satisfy limi→∞ εi = (∞, ∞, ∞). P ) = u, lim(εi Further let lim(εi\n\nP ) = w, then P εi converges to the solution to the problem\n\nP , εi\n\nη/εi\n\nη, εi\n\nθ/εi\n\nH(P ) + uKL(P 1|η) + wKL(P T 1|θ),\n\nwhich could be considered as another UOT problem with cost function constantly 0.\n\nCorollary 4. Consider a UOT problem with cost C = − log P(d|h), marginals θ = P(h), η ∈ P(D). The optimal UOT plan P (1,εη,εθ) converges to the posterior P(h|d) as εη → 0 and εθ → ∞. Bayesian inference is a special case of GBT with ε = (1, 0, ∞).\n\nProof. As direct application of Limit 3 of Proposition 3, we only need to show that the optimal plan P (1,0,∞) is propositional to the posterior P (h|d).\n\nP (1,0,∞) = arg min\n\nP ∈U (θ)\n\nK(P ) := arg min P ∈U (θ)\n\n{⟨C, P ⟩ − H(P )}.\n\n(11)\n\nwhere U (θ) = {P ∈ M(D × H)|P T 1 = θ}. Let λ ∈ R+m, consider the corresponding Lagrangian problem:\n\nL(P, λ) := ⟨C, P ⟩ − H(P ) + ⟨λ, (P T 1 − θ)⟩\n\nPartial derivatives ∂Pij = 0 and ∂λj L = 0 result the following system of equations:\n\nlog Pij − log P (di|hj) + λj = 0\n\n(cid:88)\n\ni\n\nPij − P (hj) = 0\n\n(12)\n\nCalculation shows that the solution to Equation 12 is Pij = P (di|hj )P (hj ) P (hj|di). Hence the proof is completed.\n\n(cid:80)\n\ni P (di|hj ) = P (di|hj)P (hj) ∝\n\nCorollary 5. Consider a UOT problem with θ ∈ P(H), η = P(d). The optimal UOT plan P (εP ,∞,0) converges to η ⊗ 1 as εP → ∞. Frequentist Inference is a special case of GBT with ε = (∞, ∞, 0).\n\nProof. As direct application of Proposition 3, we only need to show that P (∞,∞,0) = η ⊗ 1. Notice that Eq 1 is equivalent to\n\nP (∞,∞,0) = arg min\n\nP ∈(R≥0)n×m\n\nH(P ), with constraint P 1 = η\n\n(13)\n\nHence P (∞,∞,0) = η ⊗ 1.\n\nCorollary 6. Let cost C = − log M , marginals θ = P(h) and η = P(d). The optimal UOT plan P (1,εη,εθ) converges to the optimal plan L as εη → ∞ and εθ → ∞. Cooperative Inference is a special case of GBT with ε = (1, ∞, ∞), which is exactly entropic Optimal Transport (Cuturi, 2013).\n\nProof. According to proposition 17, L = P (1,∞,∞), and the convergence result is a direct application of Limit 4 of Proposition 3\n\nCorollary 7. Consider a UOT problem with cost C = − log P(d, h), m = n, and marginals θ = η are uniform. The optimal UOT plan P (εP ,εη,εθ) approaches to a diagonal matrix as εη, εθ → ∞ and εP → 0. In particular, discriminative learner is a special case of GBT with ε = (0, ∞, ∞), which is exactly classical Optimal Transport (Villani, 2008).\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Limit 4 of Proposition 3 implies the convergence of P (εP ,εη,εθ) → P (0,∞,∞) as εη, εθ → ∞ and εP → 0. When m = n, P (0,∞,∞) is a permutation matrix is the result of Wang et al. (2020b)[Proposition 8].\n\nProposition 8. In GBT with εθ = ∞, cost C and current belief θ. The learner updates θ with UOT plan in the same way as applying Bayes rule with likelihood from P ε(C, η, θ), and prior θ.\n\nProof. From Algorithm 1, for a general data point di chosen, the GBT takes the vector normalization of some row P ε, i.e., θ′ = P ε\n\n(i,_)/((cid:80)\n\nj P ε\n\nij).\n\nOn the other hand, when we apply Bayes rule to P ε, prior is θ = P(h), likelihood P(d|h) is the column normalization of P ε, satisfying P(di|hj) = P ε ij/θj. The last equality is because θ(i) = (cid:80) ij when εθ = ∞. So the posterior P(h|di) is the vector normalization of P(di|h)P(h), by P(di|hj)P(hj) = P ε\n\nij. Therefore, P(hj|di) = θ′(hj).\n\nij/θj ∗ θj = P ε\n\nij) = P ε\n\nij/((cid:80)\n\nj P ε\n\ni P ε\n\nNow, we introduce some notations will be used in the following proofs.\n\nNotations. Denote the set of all possible belief by ∆ = P(H). Distribution of Θk is denoted by μk. We only consider the case where no two hypotheses are the same in H. Hence we make the following assumption that columns of exp(−εP C) are not differ by a multiplicative scalar, i.e. columns of C are not differ by an additive scalar.\n\nLemma 10. For ε = (εP , ∞, ∞), εP ∈ (0, ∞), given cost C with initial belief θ0 ∈ P(H) and fixed teaching and learning distribution ηk = η ∈ P(D) for all k, then the belief random variables (Θk)k∈N have the same expectation on h: EΘk [θ(h)] = θ0(h).\n\nProof. We start the proof by showing EΘk [θ(h)] = EΘk−1 [θ(h)] for k ≥ 1. Notice that given cost C and data marginal η, an observed data d ∈ D and UOT planning uniquely determines a map from a learner’s initial belief θk−1 to one’s posterior belief θk. Denote this map by Td : θk−1 (cid:55)→ θk. Let the distribution of Θk−1 over P(H) be μk−1, denote its support by Sk−1. Then the following holds:\n\nEΘk [θ(hj)] =\n\n(cid:88)\n\nμk−1(θ)\n\n=\n\nθ∈Sk−1 (cid:88)\n\nθ∈Sk−1\n\nμk−1(θ)\n\n(cid:88)\n\ndi∈D (cid:88)\n\ndi∈D\n\nηiTdi(θ)(hj) =\n\n(cid:88)\n\nθ∈Sk−1\n\nμk−1(θ)\n\n(cid:88)\n\ndi∈D\n\nηi Mk(i, j) ηi\n\nMk(i, j) =\n\n(cid:88)\n\nθ∈Sk−1\n\nμk−1(θ)θ(hj) = EΘk−1 [θ(h)]\n\nHence EΘk [θ(h)] = EΘk−1 [θ(h)] = · · · = EΘ0[θ(h)] = θ0(h).\n\nTheorem 11. Consider a learning problem with initial belief θ0 ∈ P(H), and the true hypothesis h∗ defined by η ∈ P(D). If the learner’s data distribution ηk = η, then belief random variables (Θk)k∈N converge to the random variable Y in probability, where Y = (cid:80) h∈H θ0(h)δh and Y is supported on {δh}h∈H with P (Y = δh) = θ0(h) for εη = εθ = ∞ and εP ∈ (0, ∞).\n\nProof. Step 1: First, we show the following claim inspired the proof proposition 5.1 in Wang et al. (2020a)\n\nClaim: limk→∞ μk(∆ε) = 0, for any ε > 0, where ∆ε := {θ ∈ ∆ : θ(h) ≤ 1 − ε, ∀h ∈ H}.\n\nAssume the claim does not hold, then there exists α > 0 and a subsequence (μki)i∈N such that μki(∆ε) > α for all i. Let the center of ∆ be u, we define L(μ) := Eμf (θ), where f (θ) = ∥θ − u∥2 as entropy H(θ)). Then L(μk+1) = Eμk (Ed∼ηf (Td(θ))).\n\n2, (f may also be chosen\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nNotice that f is strictly convex, by Jensen’s inequality,\n\nEd∼ηf (Td(θ))\n\n(a)\n\n≥ f (Ed∼ηTd(θ))\n\n(b) = f (θ)\n\nHere (b) holds because:\n\nEd∼ηTd(θ)\n\n(c) =\n\n(cid:88)\n\ndi∈D\n\nηi · (Mk(i, _)\\ηi) =\n\n(cid:88)\n\nMk(i, _)\n\n(d) = θ\n\ndi∈D\n\n(14)\n\n(15)\n\n(c), (d) hold since Mk has marginals η, θ.\n\nMoreover, equality holds in (a) if and only if Td(θ) = θ for all d ∈ D. Thus rows of Mk are the same up to a scalar. This implies either (1) only one column of Mk is none zero, thus Θk ≡ δh for some h or (2) Mk has at least two columns are differed by a scalar.\n\nIn the case of (1), if θ0 ̸= δh, Θk ≡ δh is contradict to Lemma 10. Otherwise, Y = δh, the result holds. In the case of (2), according to Wang et al. (2019), Mk is cross-ratio equivalent to exp(−εP C), hence exp(−εP C) has two columns differ by a multiplicative scalar, contradict to the assumption. Thus for any θ ∈ ∆ε, Ed∼ηf (Td(θ)) > f (θ). Therefore L(μk+1) > L(μk) for any k. Moreover, notice that ∆ε is compact, there is a lower bound β > 0, such that Ed∼ηf (Td(θ))−f (θ) > β for all θ ∈ ∆ε. Therefore:\n\nL(μki+1) = Eθki+1∈∆ε(Ed∼ηf (Td(θ))) + Eθki+1∈∆\\∆ε (Ed∼ηf (Td(θ)))\n\n> Eθki ∈∆ε(f (θ)) + Eθki ∈∆\\∆ε(f (θ)) + α ∗ β = L(μki) + α ∗ β.\n\n(16)\n\nThus L(μki+s) > L(μki) + s ∗ α ∗ β → ∞ as s → ∞. On the other hand, by definition, f (θ) is bounded above by the diameter of ∆ under l2 norm, so L(μ) is also bounded above. Contradiction! Therefore, the Claim holds.\n\nStep 2. We show limk→∞ P (Θk ∈ ∆h ∆h\n\n1−ε := {θ ∈ ∆ : θ(h) > 1 − ε}.\n\n1−ε) = limk→∞ μk(∆h\n\n1−ε) = θ0(h), for all h ∈ H where\n\nFor a fixed h ∈ H, we have:\n\nθ0(h)\n\n(a)\n\n= EΘk (θ(h))\n\n(b)\n\n= E\n\nθk∈∆h\n\n1−ε\n\n(θ(hj)) + Eθk∈∆u\n\n1−ε\n\n(θ(h)) + Eθk∈∆ε(θ(h))\n\n(c)\n\n≤ μk(∆h = μk(∆h\n\n1−ε) · 1 + μk(∆u 1−ε) + ε + μk(∆ε)\n\n1−ε) · ε + μk(∆ε) · 1\n\n1−ε denotes the union of all the other corners of ∆, i.e. ∆u\n\nwhere ∆u direct application of Lemma 10; (b) holds since ∆ = ∆h θ(hj) < 1, and θ(hj) < ε for any θ ∈ ∆u k → ∞ hold for any choice of ε. Pick a sequence of ε → 0, we have that limk→∞ μk(∆h\n\n1−ε. Here (a) is 1−ε ∪∆ε. (c) holds because in general 1−ε) ≤ ε+μk(∆ε) → ε as 1−ε) = θ0(h).\n\n1−ε. Therefore, 0 ≤ θ0(h)−μk(∆h\n\n1−ε := ∪h′∈H\\h∆h′\n\n1−ε ∪∆u\n\nHence combining results from Step 1 and Step 2, we have shown Θk converges to Y in probability: P (|Θk − Y | > ε) ≤ μk(∆ε) + (cid:80) 1−ε)) → 0 as k → ∞. Hence the proof is completed.\n\nh∈H(θ0(h) − μk(∆h\n\nCorollary 12. Given a fixed data sequence di sampled from η, if θk converges to δhj , then the j-th column of Mk converges to η.\n\nProof. For ε > 0, there exists N > 0 such that θk(hj) > 1−ε for any k > N . So (cid:80) ε for any di ∈ D, on the other hand (cid:80) so Mk(i, j) → ηi as ε → 0. Therefore the j-th column of Mk converges to η.\n\nj′̸=j Mk(i, j′) < j′ Mk(i, j′) = ηi. This implies that ηi − ε < Mk(i, j) < ηi,\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nProposition 13. Consider a learning problem with cost C, initial belief θ0 ∈ P(H), the true hypothesis h∗ defined by η ∈ P(D). If the learner updates the estimation ηk with observed data (sampled from η) as stated above, then belief random variables (Θk)k∈N satisfies that for any s > 0, h∈H P (Θ(h) > 1 − s) = 1. As a consequence, Mk as the transport plan has a dominant limk→∞ column (hj) with total weights > 1 − s, and |(Mk)ij − ηk(i)| < s. In fact, as long as the sequence of ηk as random variables converges to η in probability, the above proposition holds.\n\n(cid:80)\n\nProof. The proof is similar to Step 1 of Theorem 11. The major difference is that data are sampled from η in each step, whereas the learner only has an estimation ηk at round k. Therefore, under current condition, equality (b) of Eq 14 need to be modified as following:\n\nEd∼ηTd(θk) =\n\n(cid:88)\n\ndi∈D\n\nηi · (Mk(i, _)\\ηi\n\nk) =\n\n(cid:88)\n\ndi∈D\n\nMk(i, _) ·\n\nηi ηi k\n\n= θk ⊙ vk.\n\n(17)\n\nηi k\n\nwhere vk = ( ηi ) is a vector of the size of the data set D, and ⊙ represents element-wise product. Hence Ed∼ηf (Td(θk)) = f (θk ⊙ vk) holds for all θk ∈ ∆. Since ηk → η as k → ∞. For any α∗β > 0, there exists N > 0 such that for k > N , |1− ηi\n\n2n . Hence: |f (θk ⊙vk)−f (θk)| ≤\n\n(cid:113) α∗β\n\n| <\n\nα∗β\n\nηi k\n2 . Then corresponding to Eq 16, for ki > N , we have:\n\nL(μki+1) = Eθki+1∈∆ε(Ed∼ηf (Td(θ))) + Eθki+1∈∆\\∆ε (Ed∼ηf (Td(θ)))\n\n> Eθki ∈∆ε(f (θk ⊙ vk)) + Eθki ∈∆\\∆ε (f (θk ⊙ vk)) + α ∗ β\n\n> Eθki ∈∆ε(f (θk)) + Eθki ∈∆\\∆ε(f (θk)) −\n\nα ∗ β 2\n\n+ α ∗ β\n\n= L(μki) +\n\nα ∗ β 2\n\n.\n\nHence the contradiction on the upper bound of L(μki+1) still holds, which shows the claim that: limk→∞ μk(∆ε) = 0. So limk→∞ h∈H P (Θ(h) > 1 − s) = 1. The proof for the second part of the proposition follows exactly as Corollary 12.\n\n(cid:80)\n\nProposition 15. For ε = (εP , εη, 0) with εP ∈ (0, ∞), as ηk → η almost surely, the sequence Θk of posteriors as a sequence of random variables converges in probability to variable Θ, where (cid:17) P(Θ = vi) = η(i) and vi = P(i,_)/ and P = P ε(C, η, θ). Therefore, for any s > 0, P(|Θk(h) − 1| < s) = 0 for generic (for all but in a closed subset) cost C and η, θ. limk→∞\n\nj=1 Pij\n\n(cid:16)(cid:80)m\n\n(cid:80)\n\nh∈H\n\nProof. First, εθ = 0 means that P ε(C, η, θ) is independent of θ. Therefore, Mk = P ε(C, ηk, θ) and has a limit P ε(C, η, θ), regardless of the concrete posterior θk. From construction of GBT, the posterior Θk is determined by P(Θk = wi j=1(Mk)ij. Given the coupling (Θk, Θ) by setting only P(Θk = wi k, Θ = vi) = η(i) for each i, we may calculate P(|Θk − Θ| < s) converge to 1 as Mk converge to P ε(C, η, θ). For generic C, η, θ, the probability of P ε(C, η, θ) having a row with only one nonzero entry is 0.\n\nk = (Mk)(i,_)/ (cid:80)m\n\nk) = η(i) where wi\n\nRemark: As ηk → η almost surely, for any e > 0, there exists N > 0, such that, when k > N , the probability of having ηk e-close to η is 1. Thus in almost all episodes, with generic C, η, θ, when e is small enough, for any ||η′ − η|| < e (using p − ∞ norm, same for below), the row-normalized (to 1n) UOT plans\n\nmax i\n\n||P ε\n\nr (C, η′, θ)(i,_) − P ε\n\nr (C, η′, θ)(i,_)|| <\n\n1 4\n\nmin i,j\n\nwhere P ε\n\nr is the row normalization of P ε.\n\n21\n\n||P ε\n\nr (C, η, θ)(i,_) − P ε\n\nr (C, η, θ)(j,_)||\n\nUnder review as a conference paper at ICLR 2023\n\nTherefore, for such e, we may find an N > 0 such that for any k, k′ > N , P ε P ε k, θ). However, for generic η, say, no entry of η is 0, ||θk − θ′ dk ̸= dk′. Thus the posterior sequence of almost every episode fails to converge.\n\nr (C, ηk, θ) ̸= k|| < when k, k′ > N and\n\nr (C, η′\n\nC ADDITIONAL SIMULATIONS\n\nInterpolation between learning models can be investigated properly under GBT. Human learners appear to be capable of moving between different learning models gradually. Consider an individual at a carnival who is playing a game. At each of 10 trials, a bit of information is provided, but the available reward decreases. The individual has a pool of tickets with which they can bet on the outcome at each trial. The question is how the individual should update their beliefs in order to maximize their rewards. On the first trial, their belief update, in order to accurately reflect the evidence, should follow Bayes rule. However, for the last trial, one should focus bets on the most probable outcome in order to maximize chances for rewards, that is, their beliefs should be optimized for discriminating among the possible outcomes. GBT offers a coherent way of interpolating between these two approaches to provide candidate strategies on the intermediate steps. Such situations are common where there is an explicit constraint on the time horizon after which point no further evidence can be obtained, and there are incentives to act early, rather than to wait until evidence has fully accumulated; for example, identifying dangerous situations (tiger or not? poisonous or not?).\n\nWe now demonstrate how continuity of GBT (section 3.1) allows one to gradually interpolate between Bayesian and discriminative learning over steps (rather than a sharp switch).\n\nC.1 SIMULATION SETUP\n\nSuppose a learner who observes data sampled from a true hypothesis P(d|h∗), and needs to make a conclusion on whether h∗ is one of the hypotheses in H within a fixed number N of observations.\n\nHere we compare a baseline learner who utilizes Bayesian inference (ε = (1, 0, ∞)) on the first N − 1 observations, and switch to discriminative learning (ε = (0, ∞, ∞)) on the last observation, against learners who interpolate from Bayesian to discriminative learning gradually along a sequence of models on curves in GBT. Two curves along with intermediate models are shown red and orange in Figure 5.\n\nWe take a random sampled M of shape 4 × 4 as an example,\n\nM =\n\n\n\n \n\n0.225779 0.613779 0.069799 0.090643\n\n0.014886 0.322347 0.620178 0.042588\n\n0.433787 0.172658 0.29083 0.102725\n\n0.050735 0.109262 0.243635 0.596368\n\n\n\n  .\n\nThus |H| = |D| = 4. Set N = 10 and start from uniform θ = (0.25, 0.25, 0.25, 0.25).\n\nSimulation details: We perform 40000 trials in total. For each trial s (or say each episode), we uniformly sample Xs ∈ P(H), and let the true hypothesis h∗ be the covex combination of elements in H with coefficients given by Xs. While teaching the episode, in each round, we sample a hypothesis h ∈ H following Xs, then sample a data d following the column of M corresponding to d. During inference, we set ηk by counting the frequency of each d ∈ D (starting from 1 to avoid 0 in ηk) and then normalize, as stated in (RS) model in Sec. 4.1.\n\nC.2 RESULTS\n\nFollowing paths shown in Fig. 5, for baseline (blue, left), path 1 (orange, middle), and path 2 (red, right), the distribution of maximal component of each posterior at round 10 are shown in histograms of 30, and the entropy of these posteriors are plotted in the lower three figures.\n\nIn the upper figures, comparing to the baseline (blue), weights are concentrated more on the right bars for the gradual interpolations (orange and red). Thus learning tends to be more conclusive along these paths. Here conclusiveness means that the ability of getting a conclusion (one component of the posterior eventually becoming dominant). Furthermore, the entropy distributions shown in the lower figures also illustrate this point, as compare to baseline, gradual interpolations have lower entropy.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Baseline (sharp change) and two paths we follow on the parameter space of GBT.\n\nNumerical results: entropy of baseline: mean 0.1888, standard deviation 0.2858, entropy along path 1: mean 0.0097, standard deviation 0.0686 entropy along path 2: mean 0.0571, standard deviation 0.1584.\n\nIt is necessary to consider that, the two paths and interpolations are chosen for demonstration purpose, by no means they are optimal. However, we believe GBT is capable of facilitating exploration of such optimization.\n\nFigure 6: Results. Upper: distribution of maximal component of posterior. Lower: Entropy distribution of posteriors. Left: baseline. Middle: along path 1. Right, along path 2.\n\n23",
    "reference": "# Summary Of The Paper\n\nThis paper introduces a new unified formalism where Bayesian\n inference, Discriminate learning, Cooperative learning, and Maximum\n Likelihood Estimation are all expressible using a single algorithm\n inspired by the Sinkhorn algorithm from the optimal transport literature.\n\n The algorithm's generality is expressed with a cube where one could in\n principle express not just any of the above algorithms but linear combinations\n of them.\n\n# Strength And Weaknesses\n\nThe formalism introduced in the paper is novel, but I'm not particularly sure\n that it is all that useful. While I can see how exploring a Bayesian inference where\n the prior is made progressively more or less informative has some value, I don't\n see what benefits come from some of the other mixtures.\n\n I found the paper very difficult to follow. Large amounts of time is spent on\n introducing the work with fairly basic ideas being defined that likely reader\n would already be familiar with. I also am unsure some of the terminology is being\n used precisely enough. I don't know what it means to do Frequentist inference. From\n context it seems to be maximum likelihood estimation. Bayesian inference seems\n to be restricted to discrete space of parameters. None of these limitations\n are explicitly mentioned, I just don't see how the algorithm presented could\n generalize otherwise. It would be very helpful if the authors clarified precisely\n which methods their formalism can represent instead of just saying \"Bayesian inference\"\n and \"Discriminate Learning\".\n\nThe author's response did help clarify some of what these interpolations could mean, but I\nremain unsure all pairs of interpolations within the cube yields an explainable learning algorithm.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nWhile the contribution is novel, I'm not sure it's particularly significant. As mentioned in\nthe strengths and weaknesses section, the clarity of the work could be greatly improved.\nSince the work has stated it's mostly theoretical and the experiment in the paper seems\nstraightforward, I don't have reproducibility concerns.\n\n# Summary Of The Review\n\nThis is an original idea that would greater benefit from additional clarity and better\n exposition of the value of the presented formalism.\n\n# Correctness\n\n1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nADAPTIVE UPDATE DIRECTION RECTIFICATION FOR UNSUPERVISED CONTINUAL LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRecent works on continual learning have shown that unsupervised continual learning (UCL) methods rival or even beat supervised continual learning methods. However, most UCL methods typically adopt fixed learning strategies with predefined objectives and ignore the influence of the constant shift of data distributions on the newer training process. This non-adaptive paradigm tends to achieve sub-optimal performance, since the optimal update direction (to ensure the tradeoff between old and new tasks) keeps changing during training over sequential tasks. In this work, we thus propose a novel UCL framework termed AUDR to adaptively rectify the update direction by a policy network (i.e., the Actor) at each training step based on the reward predicted by a value network (i.e., the Critic). Concretely, different from existing Actor-Critic based reinforcement learning works, there are three vital designs that make our AUDR applicable to the UCL setting: (1) A reward function to measure the score/value of the currently selected action, which provides the ground-truth reward to guide the Critic’s predictions; (2) An action space for the Actor to select actions (i.e., update directions) according to the reward predicted by the Critic; (3) A multinomial sampling strategy with a lower-bound on the sampling probability of each action, which is designed to improve the variance of the Actor’s selected actions for more diversified exploration. Extensive experiments show that our AUDR achieves state-of-the-art results under both the in-dataset and cross-dataset UCL settings. Importantly, our AUDR also shows superior performance when combined with other UCL methods, which suggests that our AUDR is highly extensible and versatile.\n\n1\n\nINTRODUCTION\n\nContinual learning has recently drawn great attention, for it can be applied to learning on a sequence of tasks without full access to the historical data (Rusu et al., 2016; Rebuffi et al., 2017; Lopez-Paz & Ranzato, 2017; Fernando et al., 2017; Kirkpatrick et al., 2017; Zenke et al., 2017). Most of existing methods focus on supervised continual learning (SCL), and only a few (Rao et al., 2019; Madaan et al., 2021; Fini et al., 2022) pay attention to unsupervised continual learning (UCL). UCL is an important yet more challenging task which requires a model to avoid forgetting previous knowledge after being trained on a sequence of tasks without labeled data.\n\nRecent UCL methods (Rao et al., 2019; Madaan et al., 2021; Fini et al., 2022) have achieved promising results, and even outperform the SCL methods. However, these UCL methods are still limited by the fixed learning strategies with pre-defined objectives. For instance, LUMP (Madaan et al., 2021) proposed a fixed lifelong mixup strategy that integrates current and memory data in a random ratio sampled from a Beta distribution regardless of the shift in data distributions. This non-adaptive paradigm is not ideal for UCL, since the optimal update direction of achieving the best performance on all learned tasks is continuously changing during training. Therefore, a new adaptive paradigm for UCL to model the process of selecting the optimal update direction is needed.\n\nIn this work, we thus devise a new UCL framework termed AUDR that can adaptively rectify the update direction (see Figure 1), where a policy network (i.e., the Actor) is proposed to select the best action for current data batch and a value network (i.e., the Critic) is designed for predicting the action’s latent value. The Actor is trained to maximize the Critic’s prediction reward, and the Critic is trained to more precisely predict the reward for the Actor’s selected action. Different from the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustration of the traditional UCL method LUMP and our AUDR. Their main difference lies in that our AUDR adopts an Actor-Critic architecture to rectify the update direction (i.e., adaptive mixup strategy) with three core designs while LUMP has a fixed mixup strategy.\n\nshort-sighted approaches (e.g., directly using a learnable parameter) that can only adjust the update direction based on current batch of data/loss, our AUDR predicts the total future rewards which is more and more precise/reliable during training. This is inspired by the Actor-Critic learning, a combination of policy-based and value-based methods, in the reinforcement learning field (Sutton et al., 1999; Haarnoja et al., 2018; Yarats et al., 2020; Mu et al., 2022). Actor-Critic learning enables the policy network to be updated at each training step (instead of after completing each task) with sampled transitions (i.e., from one state to next state), and thus it is possible to be transferred to UCL. However, we still have difficulty in directly deploying existing Actor-Critic methods under the UCL setting, because: (1) there is no environment (or reward function) that could give feedback rewards to all input states; (2) the action space for the Actor is not explicitly defined; (3) how to alleviate the problem of the trade-off between the old and new tasks remains unclear.\n\nTo address these problems, our AUDR thus has three core designs: (1) A reward function defining the ground-truth reward to guide the Critic’s training. It is based on two UCL losses of the next model (after one-step gradient descent) by respectively using the current and memory data. Thus, this reward can represent the changes of model performance on old and new tasks with the selected action, which is then utilized to conduct a continual TD-error loss to train the Critic. (2) An action space containing different actions (i.e., different update directions) for the Actor to select. To be more specific, an action with larger value (e.g., 0.99) represents that the memory data accounts for higher percentage when mixed with current data, and thus the update direction is more oriented towards improving the model performance on old tasks. (3) A multinomial sampling strategy to sample the action based on the action probability distributions predicted by the Actor. Concretely, for each input feature, the Actor outputs a probability distribution where each action is associated with a probability holding a lower-bound above zero to improve the variance of the Actor’s selected actions. We then multinomially sample one action per feature and all samples vote for the final action. This strategy is designed to explore more diverse actions to avoid the model falling into a local optimal update direction. Note that the Actor-Critic module of our AUDR is only employed for training and we only use the backbone network for testing as in LUMP (Madaan et al., 2021).\n\nFurthermore, we combine the proposed adaptive paradigm with another representative method DER (Buzzega et al., 2020) for UCL to verify the extensibility of our AUDR. Specifically, we use different coefficients of the penalty loss as the new action space to replace the action space mentioned above, which is a key factor that affects the update direction in DER. Other settings remain the same as in our original AUDR. We find that our AUDR+DER outperforms DER for UCL by a large margin. This demonstrates that our AUDR is highly generalizable/versatile. We believe that our work could bring some inspirations to the continual learning community.\n\nOur main contributions are four-fold: (1) We are the first to deploy an adaptive learning paradigm for UCL, i.e., we propose a novel UCL framework AUDR with an Actor-Critic module. (2) We devise three core designs in our AUDR to ensure that the Actor-Critic architecture is seamlessly transferred to UCL, including a reward function, an action space, and a multinomial sampling strategy. (3) Extensive experiments on three benchmarks demonstrate that our AUDR achieves new state-of-theart results on UCL. (4) Further analysis on combining our proposed adaptive paradigm with another UCL method shows that our AUDR is highly generalizable and has great potential in UCL.\n\n2\n\nImagesEncoderEncoderCurrentMemory(a) Traditional UCL method (LUMP)UCL LossFeaturesMixupLossdynamicstrategyActor-Critic for UCLRevise(b) AUDR (Ours)Fixedmixup strategyCurrentMemoryaAdaptivemixup strategyActor-CriticRectificationReward functionAction spaceSampling strategy···FeaturesMixed FeaturesUCL LossMixed FeaturesUnder review as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\nContinual Learning. Existing continual learning methods can be mainly categorized into three groups: (1) Expansion-based methods have dynamic architectures which add extra extended networks for in-coming new tasks (Rusu et al., 2016; Fernando et al., 2017; Alet et al., 2018; Chang et al., 2018; Li et al., 2019). (2) Regularization-based methods deploy either regularization constraints (Li & Hoiem, 2017; Aljundi et al., 2017; Hou et al., 2018; Rannen et al., 2017; Hou et al., 2019) or penalty losses (Kirkpatrick et al., 2017; Zenke et al., 2017; Farajtabar et al., 2020; Saha et al., 2021) to align the old and new models. (3) Rehearsal-based methods adopt a memory buffer to restore the memory data of previous tasks (Rebuffi et al., 2017; Lopez-Paz & Ranzato, 2017; Riemer et al., 2018; Chaudhry et al., 2019; Buzzega et al., 2020; Aljundi et al., 2019; Chaudhry et al., 2020; Cha et al., 2021). In contrast to these supervised continual learning works, (Rao et al., 2019; Madaan et al., 2021; Fini et al., 2022; Davari et al., 2022) have started to study the unsupervised continual learning (UCL) setting in a fixed learning paradigm by combining continual learning methods with unsupervised methods (Chen et al., 2020; Chen & He, 2021; Zbontar et al., 2021). Our AUDR is basically a rehearsal-based method for UCL but with an adaptive training paradigm which rectifies the update direction at each training step. Note that the adaptive paradigm of our AUDR is different from those methods that utilize a learnable parameter to adjust the update direction or even learn to prompt (Wang et al., 2022), since our AUDR is long-sighted (predicting future rewards) and the adaptively rectified action is only used for training (the Actor-Critic module is dropped at the test phase). We provide more discussions about their differences in Appendix B.\n\nActor-Critic Learning. Actor-Critic is a widely-used architecture in recent reinforcement learning (RL) works. (Peters & Schaal, 2008) builds the Actor-Critic algorithm on standard policy gradient formulation to update the Actor; (Schulman et al., 2017; Mnih et al., 2016; Gruslys et al., 2017; Haarnoja et al., 2018) choose to maximize or regularize the entropy of the policy; CURL (Laskin et al., 2020) combines the unsupervised learning with Actor-Critic reinforcement learning; DrQ (Yarats et al., 2020) designs a data-regularized Q to improve the Actor-Critic method; CtrlFormer (Mu et al., 2022) proposes a control transformer to tackle the forgetting problem in visual control. Our AUDR is the first work to apply Actor-Critic learning to the UCL setting. It consists of a similar Actor-Critic module as in CURL, DrQ, and CtrlFormer, but also has vital differences: a reward function for UCL to replace the environment of RL, a new action space designed for the Actor, and a multinomial sampling strategy to ensure the diversity of actions.\n\n3 METHODOLOGY\n\n3.1 UNSUPERVISED CONTINUAL LEARNING\n\nUnsupervised continual learning (UCL) requires the model to be trained on a sequence of tasks without labeled data. We follow the learning protocol proposed in LUMP (Madaan et al., 2021) to conduct our study on UCL. Concretely, let D = [D1, D2, · · · , DN ] denotes a dataset with N tasks. For each task t, it has Dt = {xt,i, yt,i}nt i=1 with nt samples, where xt,i is the input image and yt,i is the ground-truth label (which is utilized only during the validation and test phases). For simplicity, we omit t and only use xi, yi in the following subsections. For each input sample xi, we first i , x2 randomly augment it into two views x1 i . We then consider fθ with parameters θ as the backbone i )} ∈ Rd. (encoder) and employ it to obtain d-dimensional feature representations {fθ(x1 Formally, based on a widely-used contrastive learning framework SimSiam (Chen & He, 2021), the main training loss for UCL can be defined as:\n\ni ), fθ(x2\n\nLSimSiam(fθ(x1\n\ni ), fθ(x2\n\ni )) =\n\n1 2\n\nD(MLP(fθ(x1\n\ni )), fθ(x2\n\ni )) +\n\n1 2\n\nD(MLP(fθ(x2\n\ni )), fθ(x1\n\ni )),\n\n(1)\n\nwhere D(p, z) = − cos(p, stop_gradient(z)) = − p , and MLP(·) denotes a multi-layer perception. After training, the model is then evaluated by a K-nearest neighbor (KNN) classifier (Wu et al., 2018) following the setup in (Chen et al., 2020; Chen & He, 2021; Madaan et al., 2021).\n\nz ∥z∥2\n\n∥p∥2\n\n·\n\nDirectly applying the SimSiam-based learning loss mentioned above is difficult to obtain a wellperformed model due to the catastrophic forgetting problem that the model performance on previous tasks drops significantly during sequential training. To tackle this forgetting problem, rehearsalbased methods with a memory buffer to store limited previous data are most popular solutions in the\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Schematic illustration of our AUDR. At each training step s, the Actor selects the best action abest and the Critic predicts the reward of taking this action. We train the Actor with the predicted reward Qs. A reward function Rgt with current loss Lcur and memory loss Lmem of the next-step encoder fθs+1 is proposed to train the Critic (along with the Target Critic).\n\npast few years. For instance, LUMP (Madaan et al., 2021) proposes a life-long mixup strategy to integrate the memory data with current data as follows:\n\nˆx1\n\ni = λ · x1\n\nm,i + (1 − λ) · x1 i ,\n\nˆx2\n\ni = λ · x2\n\nm,i + (1 − λ) · x2 i ,\n\nLmixup =\n\n1 B\n\nB (cid:88)\n\ni=1\n\nLSimSiam(fθ(ˆx1\n\ni ), fθ(ˆx1\n\ni )),\n\n(2)\n\n(3)\n\nm,i, x2\n\nwhere B is the mini-batch size, x1 m,i denote two augmentations of the memory data xm,i, and λ ∼ Beta(α, α), α ∈ (0, ∞). This strategy is fixed across all tasks and thus it easily leads to sub-optimal performance. In addition, randomly changing the update direction at each training step is not intuitive. In our opinion, whether to use more memory data (i.e., larger λ) or more current data (i.e., smaller λ) should be measured by an appropriate pattern. Therefore, in the following, we propose to adaptively rectify the update direction by an Actor-Critic architecture.\n\n3.2 ACTION PREDICTION WITH THE ACTOR\n\nWe present the schematic illustration of our AUDR in Figure 2. In contrast to randomly sampling the mixup ratio from a fixed Beta distribution, we utilize a policy network (i.e., the Actor) to predict the score distribution of all actions. Concretely, we first define an action space A = {aj|j = 1, 2, · · · , Nact} where each action aj denotes different values of λ and Nact denotes the number of actions. Further, we adopt an Actor πφ : Rd → RNact to predict and select a best action ai best based on the representations of the current data xi:\n\nai\n\nbest = aarg maxj [softmax(πφ(fθ(xi)))]j ,\n\n(4)\n\nwhere [πφ(xi)]j denotes the j-th element of πφ(xi). For a mini-batch with B data samples, we first predict the best action of each sample by Eq. (4) and then vote for the action that appears the most frequently as abest. We take the selected best action abest as the learning strategy of the current training step s to compute the UCL loss:\n\n ̃x1\n\ni = abest · x1\n\nm,i + (1 − abest) · x1 i ,\n\n ̃x2\n\ni = abest · x2\n\nm,i + (1 − abest) · x2 i ,\n\nLucl =\n\n1 B\n\nB (cid:88)\n\ni=1\n\nLSimSiam(fθs( ̃x1\n\ni ), fθs( ̃x2\n\ni )),\n\n(5)\n\n(6)\n\nwhere θs denotes the parameters of the encoder f at the current training step s. Note that the abest is detached (i.e., without gradient) in Lucl. Therefore, this UCL loss is only used for training the encoder fθ. In this paper, we follow recent Actor-Critic learning works (Laskin et al., 2020; Yarats et al., 2020; Mu et al., 2022) to train the Actor for better predicting the actions, whose learning objective is to maximize the reward predicted by a value network (i.e., the Critic) Qφ : Rd+Nact →\n\n4\n\nQQss(LLaaaaaaaaaa)Current DataMemory Data+ActorEncoderEncoderffθθss+1ffθθs······⨁⨁······CriticTarget CriticMomentum UpdateQQss+1xx=aabbbbssaa⋅xxmmbbmm+(1−aabbbbssaa)⋅xxaaccaaThe Best Action aabbbbssaaAction ScoreAction ScoreLLccaauuRRggaa=LLaaccaa+LLmmbbmmLLaaaaccaaccaaContinual TD-errorcurrent data xxaaccaamemory data xxmmbbmmQQss,QQss+1predicted rewardsUnder review as a conference paper at ICLR 2023\n\nR1 . Formally, for each training step s, we define the actor loss as:\n\nLactor = −\n\n1 B\n\nB (cid:88)\n\ni=1\n\nQs,i = −\n\n1 B\n\nB (cid:88)\n\ni=1\n\nQφ(concat(fθs (xi), πφ(fθs (xi)))),\n\n(7)\n\nwhere concat(·, ·) is to concatenate the input vectors as shown in Figure 2 and Qs,i denotes the predicted reward at the current training step s. Training the Actor with Lactor requires the Critic’s prediction to be precise and reliable. However, there is no existing environment which provides the ground-truth rewards of input representations. Therefore, we are supposed to design a dedicated environment (or reward function) for our AUDR.\n\n3.3 REWARD FUNCTION FOR THE CRITIC\n\nThe widely-used concept “environment” in recent Actor-Critic learning works (Laskin et al., 2020; Yarats et al., 2020; Mu et al., 2022) refers to a pre-defined interactive system, which can provide the ground-truth reward to any input state for the agent. Without the environment, the basis for measuring the values of actions is missing. Therefore, in our AUDR, we desvise a reward function Rgt to compensate for the lack of the environment in UCL. Rgt measures the model performance on both current data (i.e., Lcur) and memory data (i.e., Lmem) after taking the action abest:\n\nLcur = LSimSiam(fθs+1(x1 Lmem = LSimSiam(fθs+1(x1\n\ni ), fθs+1 (x2 i )), m,i), fθs+1(x2\n\nm,i)),\n\nRs,i\n\ngt = −(Lcur + Lmem),\n\n(8)\n\n(9)\n\n(10)\n\nwhere fθs+1 denotes the updated model. Note that the training data is still from the same batch. In other words, Rs,i gt evaluates how well the model can perform on the same data after updating. Further, we maintain a target Critic Qφt to help train the Critic Qφ. It is first proposed by TD3 (Fujimoto et al., 2018) to stablize the training process and has been adopted in many recent works (Laskin et al., 2020; Yarats et al., 2020; Mu et al., 2022). Concretely, Qφt is initialized by Qφ and then updated by momentum update strategy (i.e., slowly updated by small part of the parameters of Qφ), which is also known as exponential moving average (EMA) (Holt, 2004):\n\nφt = m · φ + (1 − m) · φt,\n\n(11)\n\nwhere m is the momentum coefficient. Similar to the Critic, the target Critic can predict the reward of input features and actions. Differently, the Critic predicts the reward Qs,i for encoder fθs while the target Critic predicts the reward Qs+1,i for encoder fθs+1:\n\nQs+1,i = Qφt(concat(fθs+1(xi), πφ(fθs+1(xi)))).\n\n(12)\n\nAs directly predicting the whole future reward is difficult, we could measure the difference between two training steps by temporal difference learning (i.e., TD-error) (Watkins, 1989), which is an update rule based on Bellman equation (Bellman, 1966). Since we have re-designed the groundtruth reward for UCL, we thus call the learning objective of the Critic as “continual TD-error”. Formally, we define the continual TD-error as follows:\n\nLcritic =\n\n1 B\n\nB (cid:88)\n\ni=1\n\n(Qs,i − (Rs,i\n\ngt + γ · Qs+1,i))2,\n\n(13)\n\nwhere γ denotes the discounted factor. During training, the predicted reward will be closer to the ground-truth reward and thus the selected action becomes more reliable.\n\n3.4 RESTRICTIONS ON SAMPLING PROBABILITY\n\nAlthough we can predict the best action of each training step by our AUDR framework introduced above, it still can not guarantee that the predictions are completely accurate, nor that taking other actions will definitely lead to worse performance. In fact, the predicted optimal model updating direction is possibly not global optimal. Therefore, completely relying on the predictions and ignoring other choices will hinder the model’s exploration of a better update direction. To address this problem, we propose to sample each action according to the predicted action score (each score has a\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nrange through clamping) by multinomial sampling. Formally, for each sample xi and each training step s, we adjust Eq. (4) and predict the best action as follows:\n\nˆπφ(xi) = clamp(softmax(πφ(fθs(xi))), pmin, pmax),\n\nabest = aMS(softmax(ˆπφ(xi)),1),\n\n(14)\n\n(15)\n\nwhere clamp(·, pmin, pmax) denotes clamping each element of the input vector into the interval [pmin, pmax], and MS(·, 1) denotes a multinomial sampling strategy that samples one action according to the input score ˆπφ(xi) and returns the subscript of the selected action. To be more specific, each element of softmax(πφ(fθs (xi))) represents an action’s score which will be set to pmin if it is smaller than pmin and will be set to pmax if it is larger than pmax. Overall, we can obtain the lower-bound sampling probability Plow for each action as follows:\n\nPlow =\n\nexp(pmin) exp(pmin) + (Nact − 1) · exp(pmax)\n\n,\n\n(16)\n\nwhich means that each action has at least the probability of Plow to be sampled. It is calculated by assuming the most extreme case, where only one action’s weight (i.e., one element of ˆπφ(xi)) is pmin and the others are pmax after clamping.\n\n3.5 FULL ALGORITHM\n\nThere are three main modules in our AUDR framework: the encoder, the Actor, and the Critic. These modules are updated separately at the training phase and promote each other. Concretely, the encoder is learned by minimizing the SimSiam-based UCL loss with an adaptively adjusted mixup strategy. The Actor aims to maximize the prediction score of the Critic. The Critic improves its prediction preciseness through a continual TD-error. For easier and clearer understanding, we summarize the full training process of our AUDR as follows: (1) The Actor selects the best action from the action space by Eq. (14) and Eq. (15); (2) The encoder is updated with the selected action by Eq. (6); (3) The Actor is updated with the predicted reward of the Critic by Eq. (7); (4) The Critic is updated with the continual TD-error by Eq. (13). Note that the Actor-Critic architecture is removed at the test phase and only the encoder is evaluated with a KNN classifier as in other UCL methods. We present the pseudocode of the full algorithm of our AUDR in Appendix A.\n\nFurthermore, it is worth noting that defining the update direction (or the action space) in the mixup style as we did above is not the only way to construct our AUDR. In fact, this adaptive paradigm could be integrated into many other UCL works. For instance, DER (Buzzega et al., 2020) is a representative work of regularization-based methods, which maintains a penalty function to align the representations of old and new models. Let the weight of the penalty function in the final loss be the action, we could define a new action space for our AUDR. We conduct experiments and provide more details in Sec. 4.4 to show that our AUDR+DER obtains great performance improvements over DER, which demonstrates that our AUDR is indeed extensible and versatile.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETUP\n\nDatasets. We train and evaluate our model on three benchmarks for in-dataset UCL: (1) Split CIFAR-10 (Krizhevsky, 2009) contains 5 tasks, with 2 classes (randomly sampled from 10 classes) per task. (2) Split CIFAR-100 (Krizhevsky, 2009) consists of 20 tasks, with 5 classes (randomly sampled from 100 classes) per task. (3) Split Tiny-ImageNet is a subset of ImageNet (Deng et al., 2009) which has 20 tasks with 5 classes per task. We also evaluate our models (pre-trained on Split CIFAR-10 or Split CIFAR-100) on three benchmarks for cross-dataset UCL: MNIST (LeCun, 1998), Fashion-MNIST (FMNIST) (Xiao et al., 2017), and SVHN (Netzer et al., 2011).\n\nEvaluation Protocol. We follow recent works (Mirzadeh et al., 2020; Madaan et al., 2021) to establish the evaluation protocol of UCL. Formally, let At,i denote the test accuracy of the model on task i after trained on task t, where the total number of tasks is N . We define two metrics: (1) Average Accuracy denotes the average classification accuracy of the model on all learned tasks after sequential training: Accuracy = 1\n\n(cid:80)N\n\nN\n\ni=1 AN,i.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparison results with state-of-the-art methods under the in-dataset UCL setting on three benchmarks: Split CIFAR-10, Split CIFAR-100 and Split Tiny-ImageNet. “Accuracy” denotes average accuracy and “Forgetting” denotes average forgetting. “Multi-Task” is the upper-bound method which is based on multi-task learning. Standard deviation results are shown in brackets.\n\nMethod\n\nSplit CIFAR-10\n\nSplit CIFAR-100\n\nSplit Tiny-ImageNet\n\nAccuracy (↑) Forgetting (↓) Accuracy (↑) Forgetting (↓) Accuracy (↑) Forgetting (↓)\n\nMulti-Task\n\n95.76 (±0.08)\n\n–\n\n90.11 (±0.12) Finetune 90.93 (±0.22) PNN (Rusu et al., 2016) 92.75 (±0.06) SI (Zenke et al., 2017) DER (Buzzega et al., 2020) 91.22 (±0.30) LUMP (Madaan et al., 2021) 91.00 (±0.40)\n\n5.42 (±0.08) –\n1.81 (±0.21) 4.63 (±0.26) 2.92 (±0.53)\n\n86.31 (±0.38)\n\n75.42 (±0.78) 66.58 (±1.00) 80.08 (±1.30) 77.27 (±0.30) 82.30 (±1.35)\n\n–\n\n10.19 (±0.37) –\n5.54 (±1.30) 9.31 (±0.09) 4.71 (±1.52)\n\n82.89 (±0.49)\n\n71.07 (±0.20) 62.15 (±1.35) 72.34 (±0.42) 71.90 (±1.44) 76.66 (±2.39)\n\n–\n\n9.48 (±0.56) –\n8.26 (±0.64) 8.36 (±2.06) 3.54 (±1.04)\n\nOurs\n\n93.29 (±0.21)\n\n1.72 (±0.12)\n\n84.04 (±0.11)\n\n3.35 (±0.16)\n\n77.67 (±0.12)\n\n3.48 (±0.11)\n\nTable 2: Comparison results with the state-of-the-art methods under the cross-dataset UCL setting. All models are pre-trained on Split CIFAR-10 (or Split CIFAR-100), and then directly evaluated on MNIST, FMNIST, SVHN, and Split CIFAR-100 (or Split CIFAR-10).\n\nMethod\n\nSplit CIFAR-10\n\nSplit CIFAR-100\n\nMNIST\n\nFMNIST\n\nSVHN CIFAR-100 MNIST\n\nFMNIST\n\nSVHN CIFAR-10\n\nMulti-Task\n\n90.69 (±0.13) 80.65 (±0.42) 47.67 (±0.45) 39.55 (±0.18) 90.35 (±0.24) 81.11 (±1.86) 52.20 (±0.61) 70.19 (±0.15)\n\n89.23 (±0.99) 80.05 (±0.34) 49.66 (±0.81) 34.52 (±0.12) 85.99 (±0.86) 76.90 (±0.11) 50.09 (±1.41) 57.15 (± 0.96) Finetune 93.72 (±0.58) 82.50 (±0.51) 57.88 (±0.16) 36.21 (±0.69) 91.50 (±1.26) 80.57 (±0.93) 54.07 (±2.73) 60.55 (±2.54) SI (Zenke et al., 2017) DER (Buzzega et al., 2020) 88.35 (±0.82) 79.33 (±0.62) 48.83 (±0.55) 30.68 (±0.36) 87.96 (±2.04) 76.21 (±0.63) 47.70 (±0.94) 56.26 (±0.16) LUMP (Madaan et al., 2021) 91.03 (±0.22) 80.78 (±0.88) 45.18 (±1.57) 31.17 (±1.83) 91.76 (±1.17) 81.61 (±0.45) 50.13 (±0.71) 63.00 (±0.53)\n\nOurs\n\n93.98 (±0.33) 83.78 (±0.13) 55.95 (±1.76) 39.77 (±0.53) 94.34 (±0.34) 83.09 (±0.43) 55.29 (±0.56) 69.33 (±0.62)\n\n(2) Average Forgetting is the average performance decrease of the model on each task between its maximum accuracy and the final accuracy: Forgetting = 1 i=1 maxt∈{1,··· ,N }(At,i − AN,i).\n\n(cid:80)N −1\n\nN −1\n\nImplementation Details. All baseline methods and our AUDR are implemented based on SimSiam (Chen & He, 2021) with ResNet-18 (He et al., 2016) as the backbone encoder. The Actor and the Critic are both MLP-based structures. Concretely, the Actor has 4 linear layers and the Critic has a similar architecture with one more 3-layer MLP head which is adopted for clipped double Q-learning (Van Hasselt et al., 2016; Fujimoto et al., 2018). We provide more details for the Actor-Critic architecture in Appendix B. To obtain our main results and make fair comparison to recent competitors, we follow LUMP (Madaan et al., 2021) to average the evaluation results over three independent runs with different random seeds. More details are given as follows: (1) At the training phase, all images are randomly augmented into two views by horizontal-flip, color-jitter, gaussian-blur, and gray-scale. (2) We train our model for 200 epochs per task (the same for all baseline methods). (3) The learning rate is set to 0.03 and the memory buffer size is set to 256 (as in all baseline methods). Our action space has 10 actions which are uniformly sampled from [0, 1] while the minimum action score pmin is set to 0.08. The source code will be released soon.\n\n4.2 COMPARISON TO STATE-OF-THE-ART METHODS\n\nIn-Dataset UCL. Table 1 shows the accuracy and forgetting results of our AUDR under the in-dataset UCL setting on three datasets: Split CIFAR-10, Split CIFAR-100, and Split TinyImageNet. We compare our AUDR model with recent representative methods including expansionbased method PNN (Rusu et al., 2016), regularization-based methods SI (Zenke et al., 2017) and DER (Buzzega et al., 2020), and rehearsal-based method LUMP (Madaan et al., 2021). Note that Finetune denotes the lower-bound of UCL, which means directly finetuning the model across all the tasks without any continual learning strategies. The upper-bound of UCL is Multi-Task, which is to simultaneously train the model on all tasks and thus it has no forgetting results. All the results for the baseline methods are directly copied from LUMP (Madaan et al., 2021). It can be clearly seen that our AUDR achieves new state-of-the-art results on all three datasets. Particularly, our AUDR outperforms the second-best method LUMP by 1.68% on accuracy and 0.87% on forgetting (com-\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Ablation study results for our AUDR. Three groups of experiments are constructed to show the impact of different (a) reward functions, (b) action spaces, and (c) samping strategies.\n\n(a) Different reward functions.\n\n(b) Different number of actions.\n\n(c) Different sampling strategies.\n\nSplit CIFAR-10\n\nSplit CIFAR-10\n\nSplit CIFAR-10\n\nMethod\n\nAccuracy Forgetting\n\nMethod\n\nAccuracy Forgetting\n\nMethod Accuracy Forgetting\n\n91.00 None Lcur 91.17 Lmem 89.83 Lcur+Lmem 93.29\n\n2.92 2.59 0.55 1.72\n\nNact = 0 Nact = 5 Nact = 10 Nact = 20\n\n91.00 92.92 93.29 92.62\n\n2.92 2.32 1.72 2.94\n\nrandom learnable w/o MS w/ MS\n\n91.11 91.30 91.94 93.29\n\n3.92 3.14 2.97 1.72\n\nparing average results on all three datasets). Note that the performance of our AUDR is remarkable since it is quite close to the upper-bound (e.g., 84.04 vs. 86.31 on Split CIFAR-100).\n\nCross-Dataset UCL. Table 2 shows the accuracy and forgetting results of our AUDR under the cross-dataset UCL setting. We first train our model on the training set of Split CIFAR-10 (or Split CIFAR-100) and then directly evaluate it on MNIST, FMNIST, SVHN, and Split CIFAR-100 (or Split CIFAR-10). We can observe that: (1) Our AUDR beats the second-best method SI (Zenke et al., 2017) in 7 out of 8 cases and outperforms it by 2.32% in average, which demonstrates that our AUDR has stronger generalization ability. (2) The results of multi-task learning (i.e., Multi-Task) are not the upper-bound under cross-dataset UCL. This suggests that the model’s generalization ability can be better enhanced by UCL (e.g., AUDR and SI) than by multi-task learning.\n\n4.3 ABLATION STUDIES\n\nThere are three core designs in our AUDR to help transfer the Actor-Critic architecture to the UCL setting: (1) a reward function, (2) an action space, and (3) the multinomial sampling strategy. We thus separately conduct experiments to analyze the contributions of them in Table 3.\n\nImpact of Different Reward Functions. Table 3a shows the results of our AUDR with different reward functions. Note that we have defined the original reward function in Eq. (10), which has two components: Lcur and Lmem. We can observe that the results obtained by AUDR with Lcur +Lmem are better than those obtained by AUDR with none reward function (i.e., 1st row) or with only one of Lcur and Lmem (i.e., 2nd and 3rd rows). Therefore, the reward function should balance the model performance on both old and new tasks for UCL. The reason why the forgetting results of Lmem is smaller than Lcur + Lmem (0.55 vs. 1.72) lies in the much lower accuracy results of Lmem, which means that it doesn’t learn the old knowledge well and thus it has little knowledge to forget.\n\nImpact of Different Action Spaces. We present the results of our AUDR with different action spaces (i.e., different number of actions) in Table 3b. We compare models with Nact ∈ {0, 5, 10, 20} and find that: (1) Learning to select actions with our AUDR is better than randomly sampling them from a fixed distribution (i.e., 1st row vs. others). (2) More actions do not always lead to better performance (i.e., 3rd row vs. 4th row). In our opinion, learning with more actions means having a more difficult training process for the Actor-Critic module, especially with limited training iterations (i.e., 200 epochs per task for fair comparison with baseline methods).\n\nImpact of Different Sampling Strategies. Table 3c shows the results of our AUDR with different sampling strategies. We compare our multinomial sampling (MS) strategy with three other alternatives: random sampling (1st row), a learnable parameter to directly predict the action’s value (2nd row), and without our MS strategy (3rd row). We have the following observations: (1) The ActorCritic module achieves less satisfactory performance without multinomial sampling (MS) strategy (i.e., 3rd row vs. 4th row), which demonstrates the effectiveness of MS. (2) Models with discrete action space are easier to be trained than those with continuous action space (i.e., 2nd row vs. 3rd row). Note that the 2nd row of Table 3c contains the results of our AUDR with a learnable parameter to represent the action. In other words, it has a continuous action space [0, 1], which can be regarded as a space with infinite actions. Training the parameter in this way, its value will have little change between two training steps (e.g., 0.96 and 0.90). However, the optimal actions between these two steps may have a large margin (e.g., 0.96 and 0.16). This hypothetical situation is possible (and common) due to the random sampling and the distribution shift in UCL. Therefore, a discrete action space is more suitable than a continuous action space for our AUDR.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Further analysis for our AUDR. (a) The probability distributions of sampled actions for two different tasks (task 1 and task 5) on Split CIFAR-10. (b) Accuracy and forgetting results for our re-designed AUDR with DER (i.e., AUDR+DER) on Split CIFAR-10.\n\n4.4 FURTHER ANALYSIS\n\nAction Distributions of Different Tasks. To validate the effectiveness of the Actor-Critic module in our AUDR, we draw sampled action distributions of different tasks from Split CIFAR-10. Concretely, we present the results for the first task (task 1) and the last task (task 5) in Figure 3a. Note that the larger the action serial number is, the closer the action value is to 1 (i.e., the coefficient of memory data is higher in Eq. (5)). We can observe that: (1) When training on the last task, our AUDR tends to choose more-higher-value actions than training on the first task (e.g., 8% more actions of [8, 9, 10] and 6.5% less actions of [1, 2, 3, 4]). This demonstrates that the model needs to pay more attention to reviewing the old knowledge in the later tasks. (2) Every action on all tasks has chance to be sampled by our AUDR (more than 0.05 in Figure 3a), which is mainly due to our multinomial sampling strategy that holds a lower-bound probability for each action.\n\nRe-design AUDR with DER. To demonstrate that our AUDR is extensible in UCL, we change the mixup strategy to another UCL method DER (Buzzega et al., 2020). Note that DER was first proposed for SCL and then re-implemented for UCL in (Madaan et al., 2021). Formally, the UCL loss for DER is defined as follows:\n\nLDER =\n\n1 B\n\nB (cid:88)\n\ni=1\n\n(LSimSiam(fθ(x1\n\ni ), fθ(x2\n\ni )) + α · ∥fθ(xm,i) − Fm,i∥2),\n\n(17)\n\nwhere α is a fixed coefficient and Fm,i denotes the stored features of memory data. The second term of this formula is a penalty function to align the outputs of new and old models but with a fixed ratio α. We thus replace α with our predicted action abest to formulate our AUDR+DER:\n\nLAU DR+DER =\n\n1 B\n\nB (cid:88)\n\ni=1\n\n(LSimSiam(fθ(x1\n\ni ), fθ(x2\n\ni )) + abest · ∥fθ(xm,i) − Fm,i∥2).\n\n(18)\n\nFigure 3b shows the results for our AUDR+DER on Split CIFAR-10. More results on Split CIFAR100 and Split Tiny-ImageNet are also presented in Appendix C. It can be seen that our AUDR+DER outperforms DER by a large margin on both accuracy and forgetting. Such success of AUDR+DER demonstrates that our proposed adaptive paradigm (i.e., AUDR) is highly extensible/versatile and has great potential in the continual learning field.\n\n5 CONCLUSION\n\nIn this paper, we propose an Actor-Critic framework with adaptive update direction rectification (AUDR) for unsupervised continual learning. We devise three novel designs for our AUDR: (1) A reward function considering the model performance on both current and memory data, which provides reliable ground-truth reward for training the Critic; (2) An action space with discrete actions for the Actor to select; (3) A multinomial sampling strategy to ensure the variance of sampled actions. We conduct extensive experiments on three benchmarks to show that our AUDR achieves new state-of-the-art results for both in-dataset and cross-dataset UCL. Further analysis on action distribution and AUDR+DER demonstrate that our AUDR is indeed effective and extensible.\n\n9\n\n123456888990919293FinetuneDERAUDR+DERForgettingAccuracyMethodsAccuracyForgetting0.050.070.090.110.130.1512345678910ProbabilityActionstask1task5(a) Probability distributions of different actions.(b) Results for our AUDR+DER on Split CIFAR-10.+8%−6.5%Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nFerran Alet, Tomás Lozano-Pérez, and Leslie P Kaelbling. Modular meta-learning. In CoRL, pp.\n\n856–868, 2018.\n\nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a\n\nnetwork of experts. In CVPR, pp. 3366–3375, 2017.\n\nRahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection\n\nfor online continual learning. NeurIPS, 32:11817–11826, 2019.\n\nRichard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.\n\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. NeurIPS, 33:15920–15930, 2020.\n\nHyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning.\n\nIn ICCV, pp.\n\n9516–9525, 2021.\n\nMichael B Chang, Abhishek Gupta, Sergey Levine, and Thomas L Griffiths. Automatically arXiv preprint\n\ncomposing representation transformations as a means for generalization. arXiv:1807.04640, 2018.\n\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019.\n\nArslan Chaudhry, Albert Gordo, Puneet K Dokania, Philip Torr, and David Lopez-Paz. Using hindsight to anchor past knowledge in continual learning. arXiv preprint arXiv:2002.08165, 3, 2020.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n\ncontrastive learning of visual representations. In ICML, pp. 1597–1607, 2020.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning.\n\nIn CVPR, pp.\n\n15750–15758, 2021.\n\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\n\nLearning augmentation strategies from data. In CVPR, pp. 113–123, 2019.\n\nMohammadReza Davari, Nader Asadi, Sudhir Mudur, Rahaf Aljundi, and Eugene Belilovsky. Probing representation forgetting in supervised and unsupervised continual learning. In CVPR, pp. 16712–16721, 2022.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\n\nhierarchical image database. In CVPR, pp. 248–255, 2009.\n\nMehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for contin-\n\nual learning. In AISTATS, pp. 3762–3773, 2020.\n\nChrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.\n\nEnrico Fini, Victor G Turrisi da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal. Self-supervised models are continual learners. In CVPR, pp. 9621–9630, 2022.\n\nScott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-\n\ncritic methods. In ICML, pp. 1587–1596, 2018.\n\nAudrunas Gruslys, Mohammad Gheshlaghi Azar, Marc G Bellemare, and Remi Munos. The reactor:\n\nA sample-efficient actor-critic architecture. arXiv preprint arXiv:1704.04651, 5, 2017.\n\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In CVPR, pp. 770–778, 2016.\n\nCharles C Holt. Forecasting seasonals and trends by exponentially weighted moving averages.\n\nInternational Journal of Forecasting, 20(1):5–10, 2004.\n\nSaihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Lifelong learning via\n\nprogressive distillation and retrospection. In ECCV, pp. 437–452, 2018.\n\nSaihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier\n\nincrementally via rebalancing. In CVPR, pp. 831–839, 2019.\n\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):3521–3526, 2017.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University\n\nof Tront, 2009.\n\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representa-\n\ntions for reinforcement learning. In ICML, pp. 5639–5650, 2020.\n\nYann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.\n\nXilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In ICML, pp. 3925–3934, 2019.\n\nZhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 40(12):2935–2947, 2017.\n\nDavid Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.\n\nNeurIPS, 30:6470–6479, 2017.\n\nDougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-\n\ntion through reversible learning. In ICML, pp. 2113–2122, 2015.\n\nDivyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung Ju Hwang. Representational\n\ncontinuity for unsupervised continual learning. In ICLR, 2021.\n\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, and Hassan Ghasemzadeh. Under-\n\nstanding the role of training regimes in continual learning. NeurIPS, 33:7308–7320, 2020.\n\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, pp. 1928–1937, 2016.\n\nYao Mu, Shoufa Chen, Mingyu Ding, Jianyu Chen, Runjian Chen, and Ping Luo. Ctrlformer: Learning transferable state representation for visual control via transformer. arXiv preprint arXiv:2206.08883, 2022.\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.\n\nJan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural\n\nNetworks, 21(4):682–697, 2008.\n\nAmal Rannen, Rahaf Aljundi, Matthew B Blaschko, and Tinne Tuytelaars. Encoder based lifelong\n\nlearning. In ICCV, pp. 1320–1328, 2017.\n\nDushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pascanu, Yee Whye Teh, and Raia Hadsell.\n\nContinual unsupervised representation learning. NeurIPS, 32:7647–7657, 2019.\n\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.\n\nicarl:\n\nIncremental classifier and representation learning. In CVPR, pp. 2001–2010, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nMatthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018.\n\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.\n\nGobinda Saha, Isha Garg, and Kaushik Roy. Gradient projection memory for continual learning.\n\narXiv preprint arXiv:2103.09762, 2021.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods\n\nfor reinforcement learning with function approximation. NeurIPS, 12:1057–1063, 1999.\n\nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-\n\nlearning. In AAAI, pp. 2094–2100, 2016.\n\nZifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In CVPR, pp. 139–149, 2022.\n\nChristopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, University\n\nof Cambridge, 1989.\n\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-\n\nparametric instance discrimination. In CVPR, pp. 3733–3742, 2018.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-\n\ning machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n\nDenis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing\n\ndeep reinforcement learning from pixels. In ICLR, 2020.\n\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\n\nlearning via redundancy reduction. In ICML, pp. 12310–12320, 2021.\n\nFriedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.\n\nIn ICML, pp. 3987–3995, 2017.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA FULL ALGORITHM OF AUDR\n\nWe provide the pseudocode of the full algorithm for our AUDR in Algorithm 1.\n\nAlgorithm 1 Pseudocode of AUDR.\n\nInput: Encoder fθ with parameters θ;\n\nActor πφ with parameters φ; Critic Qφ with parameters φ; Target Critic Qφt with parameters φt; An action space A = {aj|j = 1, 2, · · · , Nact} of Nact actions; A dataset D = [D1, D2, · · · , DN ] of N tasks.\n\nOutput: The learned f ∗ θ\n\n1: Initialize the Target Critic Qφt = Qφ; 2: for all task = 1, 2, · · · , N do 3: 4: 5: 6: 7: 8: 9:\n\nfor all iteration s = 1, 2, · · · , M axIteration do Sample a mini-batch with B images {xi}B i=1; Obtain the best action abest by the Actor with Eqs. (14–15); Obtain UCL loss Lucl with Eqs. (5–6); Update fθ using SGD and obtain fθs , fθs+1; Obtain the predicted reward Rs,i with Eq. (7); Update the Actor πφ using SGD; Obtain ground-truth reward Rs,i Obtain the target reward Rs+1,i with Eq. (12); Obtain the continual TD-error Lcritic with Eq. (13); Update the Critic Qφ using SGD; Update the target Critic Qφt using EMA with Eq. (11);\n\ngt with Eqs. (8–10);\n\n10: 11: 12: 13: 14: 15: 16: end for 17: return the found best f ∗ θ .\n\nend for\n\nB MORE IMPLEMENTATION DETAILS AND DISCUSSIONS\n\nDetails of Actor and Critic. The structures of our Actor and Critic networks are the same as in DrQv2 (Yarats et al., 2021), where the Actor has a 1-layer trunk network (Linear+Layernorm+Tanh) with one 3-layer MLP head and the Critic has a 1-layer trunk network with two 3-layer MLP heads. The Critic is trained by clipped double Q-learning (Van Hasselt et al., 2016; Fujimoto et al., 2018) to alleviate the over-estimation problem, where two MLP heads represent two Q-functions Qφ1 , Qφ2 which separately predict the rewards R1 s,i. Then the final predicted reward Rs,i of Eq. (7) is obtained by: Rs,i = min{R1\n\ns,i, R2\n\ns,i, R2\n\ns,i}.\n\nDiscussions on Comparing AUDR with Other Possible Methods. The core idea of our AUDR is to adaptively rectify the update direction during training. In this work, the instantiated “update direction” is based on the mixup ratio (of AUDR) or the penalty loss weight (of AUDR+DER), which is a pre-defined hyper-parameter of the original method (LUMP or DER). In addition to our AUDR, there are three possible approaches to adjusting the update direction during training: (1) Directly using a learnable parameter (through a MLP layer) to represent the update directions. The main drawback of this method lies in the slight change of the action value between two steps, which has already been discussed in Sec. 4.3. (2) Finding the best hyper-parameters by meta-learning (Maclaurin et al., 2015) or reinforcement learning (Cubuk et al., 2019). Different from their objectives of finding the best hyper-parameter combination by training the models several times (each has a whole training process), our AUDR is an online method to adaptively rectify the hyper-parameter (i.e., the update direction) and thus the hyper-parameter is continuously changing during training instead of fixed. Concretely, those hyper-parameter search methods focus on finding an optimal policy for a neural network to solve a specific task (i.e., the best hyper-parameter has fixed value once found). When transferring them to the UCL setting, they would search the action space to find the best (but fixed) ratio. Since each exploration step requires a whole training process, the computation cost of their schema is enormous\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Results for our AUDR+DER on the other two datasets.\n\nFigure 5: Visualization of Actor Loss during training.\n\n(e.g., 10 trials will lead to 10 times training complexity). The online schema of our AUDR with the Actor-Critic architecture is instead only in need of one whole training process to adjust the mixup ratio at each step, which is significantly more efficient. (3) Learning task-specific prompts (Wang et al., 2022) to be used in the test phase. Differently, the extra Actor-Critic module of our AUDR is removed during testing which is more applicable and resource-saving in real-world application scenarios (e.g., large-scale pre-training).\n\nC MORE EXPERIMENTAL RESULTS\n\nWe provide a re-designed version of our AUDR in our main paper and present the experimental results on Split CIFAR-10 (see Figure 3b). For a more comprehensive study, we provide the results of our AUDR+DER on Split CIFAR-100 and Split Tiny-ImageNet in Figure 4. We can observe that our AUDR+DER achieves better performance than the competitors in all cases.\n\n14\n\n1357911727476788082FinetuneDERAUDR+DERForgettingAccuracyMethodAccuracyForgetting123456888990919293FinetuneDERAUDR+DERForgettingAccuracyMethodsAccuracyForgetting(a) Results for our AUDR+DER on Split CIFAR-10.(a) Results for our AUDR+DER on Split CIFAR-100.1357911697071727374FinetuneDERAUDR+DERForgettingAccuracyMethodAccuracyForgetting(b) Results for our AUDR+DER on Split Tiny-ImageNet.training iterationsLossLactorLossLossSplit CIFAR-10Split CIFAR-100Split Tiny-ImageNetUnder review as a conference paper at ICLR 2023\n\nTable 4: More results for our AUDR over Split-CIFAR-10. ∗ denotes that our model is trained with 300 epochs per task (otherwise 200 epochs by default).\n\n(a) Different Actor-Critic layers.\n\n(b) Longer epochs for 20 actions.\n\n(c) Training with BarlowTwins.\n\nSplit CIFAR-10\n\nSplit CIFAR-10\n\nSplit CIFAR-10\n\nMethod Accuracy Forgetting\n\nMethod Accuracy Forgetting\n\nMethod\n\nAccuracy Forgetting\n\nNone 2-layer 4-layer 6-layer\n\n91.00 91.47 93.29 93.69\n\n2.92 3.55 1.72 1.61\n\nNact=5 Nact=10 Nact=20 Nact=20∗\n\n92.92 93.29 92.62 93.60\n\n2.32 1.72 2.94 2.61\n\n87.72 Finetune 88.67 DER LUMP 90.31 AUDR-BT 91.53\n\n4.08 2.41 1.13 1.98\n\nTable 5: Detailed ablation studies. “Max Accuracy” denotes the maximum accuracy on each task over Spilt-CIFAR-10 during training.\n\nMethod\n\nMax Accuracy\n\nOverall\n\nTask 1\n\nTask 2\n\nTask 3\n\nTask 4\n\nLmem Lcur + Lmem\n\n92.90 97.25\n\n83.10 91.60\n\n88.45 92.81\n\n92.45 95.60\n\nTask5\n\n94.50 96.08\n\nAccuracy\n\nForgetting\n\n89.83 93.29\n\n0.55 1.72\n\nD VISUALIZATION OF ACTOR LOSS\n\nTo verify the training stability of the Actor-Critic architecture applied in our AUDR, we present the plot of the training loss of the Actor in Figure 5. It can be seen that minimizing the actor loss Lactor is generally stable during training and finally converges on all datasets. Note that there are relatively large fluctuations during training, which is normal since the distribution of data is constantly changing across sequential tasks.\n\nE FURTHER EVALUATION\n\nDifferent Actor-Critic Architectures. We present more results for different Actor-Critic architectures in Table 4a. The MLP-based modules of both Actor and Critic are set to have the same number of layers (0, 2, 4, or 6). We can observe that larger MLP used for Actor/Critic indeed has stronger learning ability and thus leads to better results.\n\nTraining AUDR with More Epochs. As shown in Table 3b of our main paper, when the number of epochs is limited (e.g., 200 epochs per task), training our AUDR with a larger action space becomes more difficult and thus suffers from performance degradation. To make further verification, we train our model with 20 actions for more (i.e., 300) epochs and present the new results in Table 4b. We find that it leads to better performance, which confirms our assertion.\n\nTraining AUDR with Barlow-Twins. To show the generalization ability of our AUDR, we change the SimSiam loss with another unsupervised learning loss Barlow-Twins (Zbontar et al., 2021), and then denote our method as AUDR-BT. Experimental results are shown in Table 4c. The results of the competitors are directly copied from LUMP (Madaan et al., 2021). We can observe that our AUDR-BT still achieves the best accuracy on Split CIFAR-10.\n\nMore Detailed Ablation Study. We provide more detailed results about the ablation study for training our AUDR with only Lmem in Table 5. It achieves a much lower overall forgetting rate due to the lower maximum accuracy (i.e., Max Accuracy) on each task. However, it performs significantly worse than training our AUDR with Lcur + Lmem in terms of overall accuracy.\n\n15",
    "reference": "# Summary Of The Paper\n\nThe paper focuses on Unsupervised continual learning (UCL) and proposes Adaptive Update Direction Rectification (AUDR), an adaptive learning paradigm for the UCL setting. Mainly, the paper uses an Actor-critical approach, where the actor selects the best action and is updated with the predicted reward by the Critic. It proposes a reward function based on the current task and replay-buffer performance to guide the Critic’s training, which is updated using the continual TD error. During the evaluation, the paper compares the learned encoder with prior methods, showing superior performance on multiple benchmark datasets.\n\n# Strength And Weaknesses\n\nI am well-familiar with the literature and read the full paper in detail. Accordingly, I'll describe the strengths and weaknesses of the paper in the order of originality + quality, clarity, and reproducibility. \n\n## Originality and Quality\n### Strengths \n* The choice of hyper-parameters plays a vital role in most rehearsal-based methods. The paper proposes an interesting approach for modeling the hyper-parameter search for UCL as an actor-critic framework and would interest the CL community.\n* The proposed AUDR framework is flexible and applicable to various prior CL methods, as also demonstrated in the empirical evaluation of the paper.\n\n### Weaknesses\n* The paper's objective is primarily a hyper-parameter search for UCL, focusing on the mixup ratio or penalty loss weight. The paper includes a discussion of various hyper-parameter search methods in the appendix. Still, the efficiency of the proposed method over the prior methods needs to be clarified in the current form. The paper claims that it is an \"online\" method and more efficient than other approaches; however, in this case, it should show a comparison with these methods and highlight the efficiency of the proposed method.\n* The proposed framework has also been restricted to two hyper-parameters and can be further strengthened by incorporating other hyper-parameters into AUDR.\n* AUDR also requires additional MLP-based architectures for both Actor and Critics. The paper should include ablation and a discussion on the choice of these architectures in the UCL setting. \n\n---\n\n## Clarity\nThe paper was well-written and easy to follow. I have a few suggestions and clarifying questions:\n* While the paper focuses on LUMP, I suggest updating the notations and figures to the proposed AUDR as a general framework applicable to adapt the buffer hyper-parameters of prior CL methods to strengthen the proposed method.\n* The paper highlights that more actions only sometimes lead to better performance, possibly due to limited training iterations. I recommend increasing the training epochs for each task to check if that makes the training process more efficient and improves performance.\n* The paper should also compare the generated mixup examples using AUDR and LUMP. Additionally, it would be beneficial to include a discussion on the selected actions with high rewards for both LUMP and DER compared to the hyper-parameters used in prior works.\n* The paragraph before subsection 3.2 - In addition, it is not … -> The sentence is incomplete.\n* Section 4.2, last line of paragraph 1, on on -> on\n* The paper mentions that forgetting for $L_{mem}$ is smaller than the combination of losses because it did not learn the old knowledge well. I suggest supporting this statement with an accuracy comparison of prior tasks during training in the appendix.\n* The references are inconsistent, NIPS and NeurIPS are randomly used interchangeably, and few articles don't use the conference bibliographies.\n\n---\n\n## Reproducibility\n The code is not provided with the submission. Since the paper is empirical, it is necessary to provide the code to aid the reproducibility of future works.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI elaborate on all these aspects in the above section.\n\n# Summary Of The Review\n\nThe paper proposes a novel and interesting approach to continual learning; however, the paper can be strengthened further. I am happy to increase my score if the authors address the above concerns. Notably, It lacks a comparison to prior hyper-parameter search approaches and is limited to two choices of hyper-parameters.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSWIFT: RAPID DECENTRALIZED FEDERATED LEARNING VIA WAIT-FREE MODEL COMMUNICATION\n\nMarco Bornstein, Tahseen Rabbani, Evan Wang, Amrit Singh Bedi, & Furong Huang Department of Computer Science, University of Maryland {marcob, trabbani, ezw, amritbd, furongh}@umd.edu\n\nABSTRACT\n\nThe decentralized Federated Learning (FL) setting avoids the role of a potentially unreliable or untrustworthy central host by utilizing groups of clients to collaboratively train a model via localized training and model/gradient sharing. Most existing decentralized FL algorithms require synchronization of client models where the speed of synchronization depends upon the slowest client. In this work, we propose SWIFT: a novel wait-free decentralized FL algorithm that allows clients to conduct training at their own speed. Theoretically, we prove that SWIFT matches the gold-standard iteration convergence rate O(1/ T ) of parallel stochastic gradient descent for convex and non-convex smooth optimization (total iterations T ). Furthermore, we provide theoretical results for IID and non-IID settings without any bounded-delay assumption for slow clients which is required by other asynchronous decentralized FL algorithms. Although SWIFT achieves the same iteration convergence rate with respect to T as other state-of-the-art (SOTA) parallel stochastic algorithms, it converges faster with respect to run-time due to its wait-free structure. Our experimental results demonstrate that SWIFT’s run-time is reduced due to a large reduction in communication time per epoch, which falls by an order of magnitude compared to synchronous counterparts. Furthermore, SWIFT produces loss levels for image classification, over IID and non-IID data settings, upwards of 50% faster than existing SOTA algorithms. Code for SWIFT can be found on GitHub at https://github.com/umd-huang-lab/SWIFT.\n\n√\n\n1\n\nINTRODUCTION\n\nFederated Learning (FL) is an increasingly popular setting to train powerful deep neural networks with data derived from an assortment of clients. Recent research (Lian et al., 2017; Li et al., 2019; Wang & Joshi, 2018) has focused on constructing decentralized FL algorithms that overcome speed and scalability issues found within classical centralized FL (McMahan et al., 2017; Savazzi et al., 2020). While decentralized algorithms have eliminated a major bottleneck in the distributed setting, the central server, their scalability potential is still largely untapped. Many are plagued by high communication time per round (Wang et al., 2019). Shortening the communication time per round allows more clients to connect and then communicate with one another, thereby increasing scalability.\n\nDue to the synchronous nature of current decentralized FL algorithms, communication time per round, and consequently run-time, is amplified by parallelization delays. These delays are caused by the slowest client in the network. To circumvent these issues, asynchronous decentralized FL algorithms have been proposed (Lian et al., 2018; Luo et al., 2020; Liu et al., 2022; Nadiradze et al., 2021). However, these algorithms still suffer from high communication time per round. Furthermore, their communication protocols either do not propagate models well throughout the network (via gossip algorithms) or require partial synchronization. Finally, these asynchronous algorithms rely on a deterministic bounded-delay assumption, which ensures that the slowest client in the network updates at least every τ iterations. This assumption is satisfied only under certain conditions (Abbasloo & Chao, 2020), and worsens the convergence rate by adding a sub-optimal reliance on τ .\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm Iteration Convergence Rate Client (i) Comm-Time Complexity Neighborhood Avg. Asynchronous O(T maxj∈Ni O(|Cs| maxj∈Ni O(|Cs| maxj∈Ni\n\nCj) Cj) Cj)\n\nD-SGD PA-SGD LD-SGD AD-PSGD SWIFT\n\n√ √\n√ √\n√\n\nT ) T ) T ) T ) T )\n\nO(1/ O(1/ O(1/ O(τ / O(1/\n\n✗ ✗\n✗ ✓\n✓\n\nPrivate Memory ✓\n✓ ✓\n✗ ✓\n\nO(T Ci) O(|Cs|Ci)\n\n✓ ✓\n✓ ✗\n✓\n\n(1) Notation: total iterations T , communication set Cs (|Cs| < T ), client i’s neighborhood Ni, maximal bounded delay τ , and client i’s communication time per round Ci. (2) As compared to AD-PSGD, SWIFT does not have a τ convergence rate term due to using an expected client delay in analysis.\n\nTable 1: Rate and complexity comparisons for decentralized FL algorithms.\n\n√\n\nTo remedy these drawbacks, we propose the Shared WaIt-Free Transmission (SWIFT) algorithm: an efficient, scalable, and high-performing decentralized FL algorithm. Unlike other decentralized FL algorithms, SWIFT obtains minimal communication time per round due to its wait-free structure. Furthermore, SWIFT is the first asynchronous decentralized FL algorithm to obtain an optimal T ) convergence rate (aligning with stochastic gradient descent) without a bounded-delay O(1/ assumption. Instead, SWIFT leverages the expected delay of each client (detailed in our remarks within Section 5). Experiments validate SWIFT’s efficiency, showcasing a reduction in communication time by nearly an order of magnitude and run-times by upwards of 35%. All the while, SWIFT remains at state-of-the-art (SOTA) global test/train loss for image classification compared to other decentralized FL algorithms. We summarize our main contributions as follows.\n\n▷ Propose a novel wait-free decentralized FL algorithm (called SWIFT) and prove its theoretical\n\nconvergence without a bounded-delay assumption.\n\n▷ Implement a novel pre-processing algorithm to ensure non-symmetric and non-doubly stochastic\n\ncommunication matrices are symmetric and doubly-stochastic under expectation.\n\n▷ Provide the first theoretical client-communication error bound for non-symmetric and non-doubly\n\nstochastic communication matrices in the asynchronous setting.\n\n▷ Demonstrate a significant reduction in communication time and run-time per epoch for CIFAR-10\n\nclassification in IID and non-IID settings compared to synchronous decentralized FL.\n\n2 RELATED WORKS\n\nAsynchronous Learning. HOGWILD! (Recht et al., 2011), AsySG-Con (Lian et al., 2015), and AD-PSGD (Lian et al., 2017) are seminal examples of asynchronous algorithms that allow clients to proceed at their own pace. However, these methods require a shared memory/oracle from which clients grab the most up-to-date global parameters (e.g. the current graph-averaged gradient). By contrast, SWIFT relies on a message passing interface (MPI) to exchange parameters between neighbors, rather than interfacing with a shared memory structure. To circumvent local memory overload, common in IoT clusters (Li et al., 2018), clients in SWIFT access neighbor models sequentially when averaging. The recent works of (Koloskova et al., 2022; Mishchenko et al., 2022) have improved asynchronous SGD convergence guarantees which no longer rely upon the largest gradient delay. Like these works, SWIFT similarly proves convergence without a bounded-delay assumptions. However, SWIFT differs as it functions in the decentralized domain as well as the FL setting. Decentralized Stochastic Gradient Descent (SGD) algorithms are reviewed in Appendix D.\n\nCommunication Under Expectation. Few works in FL center on communication uncertainty. In (Ye et al., 2022), a lightweight, yet unreliable, transmission protocol is constructed in lieu of slow heavyweight protocols. A synchronous algorithm is developed to converge under expectation of an unreliable communication matrix (probabilistic link reliability). SWIFT also convergences under expectation of a communication matrix, yet in a different and asynchronous setting. SWIFT is already lightweight and reliable, and our use of expectation does not regard link reliability.\n\nCommunication Efficiency. Minimizing each client i’s communication time per round Ci is a challenge in FL, as the radius of information exchange can be large (Kairouz et al., 2021). MATCHA (Wang et al., 2019) decomposes the base network into m disjoint matchings. Every epoch, a random sub-graph is generated from a combination of matchings, each having an activation probability pk. Clients then exchange parameters along this sub-graph. This requires a total communicationtime complexity of O(T (cid:80)m Cj), where Ni are client i’s neighbors. LD-SGD (Li\n\nk=1 pk maxj∈Ni\n\n2\n\nPublished as a conference paper at ICLR 2023\n\net al., 2019) and PA-SGD (Wang & Joshi, 2018) explore how reducing the number of neighborhood parameter exchanges affects convergence. Both algorithms create a communication set Cs (defined in Appendix D) that dictate when clients communicate with one another. The communication-time complexities are listed in Table 1. These methods, however, are synchronous and their communicationCj. SWIFT improves upon this, time complexities depend upon the slowest neighbor maxj∈Ni achieving a complexity depending on a client’s own communication-time per round. Unlike ADPSGD Lian et al. (2018), which achieves a similar communication-time complexity, SWIFT allows for periodic communication, uses only local memory, and does not require a bounded-delay assumption.\n\n3 PROBLEM FORMULATION\n\nDecentralized FL. In the FL setting, we have n clients represented as vertices of an arbitrary communication graph G with vertex set V = {1, . . . , n} and edge set E ⊆ V × V. Each client i communicates with one-hop neighboring clients j such that (i, j) ∈ E. We denote the neighborhood for client i as Ni, and clients work in tandem to find the global model parameters x by solving:\n\nmin x∈Rd\n\nf (x) :=\n\nn (cid:88)\n\ni=1\n\nfi(x),\n\nfi(x) := Eξi∼Di\n\n(cid:2)l(x, ξ)(cid:3),\n\nn (cid:88)\n\ni=1\n\npi = 1,\n\npi ≥ 0.\n\n(1)\n\nThe global objective function f (x) is the weighted average of all local objective functions fi(x). In Equation 1, pi, ∀i ∈ [n] denotes the client influence score. This term controls the influence of client i on the global consensus model, forming the client influence vector p = {pi}n i=1. These scores also reflect the sampling probability of each client. We note that each local objective function fi(x) is the expectation of loss function l with respect to potentially different local data ξi = {ξi,j}M j=1 from each client i’s distribution Di, i.e., ξi,j ∼ Di. The total number of iterations is denoted as T .\n\nExisting Inter-Client Communication in Decentralized FL. All clients balance their individual training with inter-client communications in order to achieve consensus while operating in a decentralized manner. The core idea of decentralized FL is that each client communicates with its neighbors (connected clients) and shares local information. Balancing individual training with inter-client communication ensures individual client models are well-tailored to personal data while remaining (i) robust to other client data, and (ii) able to converge to an optimal consensus model.\n\nPeriodic Averaging. Algorithms such as Periodic Averaging SGD (PA-SGD) (Wang & Joshi, 2018) and Local Decentralized SGD (LD-SGD) reduce communication time by performing multiple local updates before synchronizing. This process is accomplished through the use of a communication set Cs, which defines the set of iterations a client must perform synchronization,\n\nCs = {t ∈ N | t mod (s + 1) = 0, t ≤ T }. (2) We adopt this communication set notation, although synchronization is unneeded in our algorithm.\n\n4 SHARED WAIT-FREE TRANSMISSION (SWIFT) FEDERATED LEARNING\n\nIn this section, we present the Shared WaIt-Free Transmission (SWIFT) Algorithm. SWIFT is an asynchronous algorithm that allows clients to work at their own speed. Therefore, it removes the dependency on the slowest client which is the major drawback of synchronous settings. Moreover, unlike other asynchronous algorithms, SWIFT does not require a bound on the speed of the slowest client in the network and allows for neighborhood averaging and periodic communication.\n\nA SWIFT Overview. Each client i runs SWIFT in parallel, first receiving an initial model xi, communication set Cs, and counter ci ← 1. SWIFT is concisely summarized in the following steps: (0) Determine client-communication weights wi via Algorithm 2 in Appendix B.2. (1) Broadcast the local model to all neighboring clients. (2) Sample a random local data batch of size M . (3) Compute the gradient update of the loss function l with the sampled local data. (4) Fetch and store neighboring local models, and average them with one’s own local model if ci ∈ Cs. (5) Update the local model with the computed gradient update, as well as the counter ci ← ci + 1. (6) Repeat steps (1)-(5) until convergence. A diagram and algorithmic block of SWIFT are depicted in Figure 1 and Algorithm 1 respectively.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nActive Clients, Asynchronous Iterations, and the Local-Model Matrix. Each time a client finishes a pass through steps (1)-(5), one global iteration is performed. Thus, the global iteration t is increased after the completion of any client’s averaging and local gradient update. The client that performs the t-th iteration is called the active client, and is designated as it (Line 6 of Algorithm 1). There is only one active client per global iteration. All other client models remain unchanged during the t-th iteration (Line 16 of Algorithm 1). In synchronous algorithms, the global iteration t increases only after all clients finish an update. SWIFT, which is asynchronous, increases the global iteration t after any client finishes an update. In our analysis, we define local-model matrix X t ∈ Rd×n as the concatenation of all local client models at iteration t for ease of notation,\n\nFigure 1: SWIFT schematic with Cs = C1 (i.e., clients communicate every two local update steps).\n\n1, . . . , xt (3) Inspired by PA-SGD (Wang & Joshi, 2018), SWIFT handles multiple local gradient steps before averaging models amongst neighboring clients (Line 10 of Algorithm 1). Periodic averaging for SWIFT, governed by a dynamic client-communication matrix, is detailed below.\n\nX t := [xt\n\nn] ∈ Rd×n.\n\nAlgorithm 1: Shared WaIt-Free Transmission (SWIFT) Input\n\n:Vertex set V, Total steps T , Step-size γ, Client Influence Vector p, Distributions of client data Di, Communication set Cs, Batch size M , Loss function l, and Initial model x0\n\nOutput :Consensus model 1\n\nn 1 Initialize each client’s local update counter ci ← 1, ∀i ∈ V 2 Obtain each client’s new communication vector wt 3 for t = 1, . . . , T do\n\n(cid:80)n\n\ni=1 xT\n\ni\n\ni using Algorithm 2\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\nif network topology changes then\n\nRenew each client’s communication vector wt\n\ni using Algorithm 2\n\nRandomly select an active client it according to Client Influence Probability Vector p Broadcast active client’s model xt Sample a batch of active client it’s local data ξt it) ← 1 Compute the gradient update: g(xt if current step falls in the predefined communication set, i.e., cit ∈ Cs then\n\nit to all its neighbors {k | wt\n\nm=1 from distribution Dit\n\nit,m}M m=1 ∇l(xt\n\nit,k ̸= 0, k ̸= it}\n\nit := {ξt (cid:80)M\n\nit, ξt\n\nit, ξt\n\nit,m)\n\nM\n\nFetch and store the latest models {xt Model average for the active client: xt+1/2\n\nk} from it’s neighbors {k | wt k wt\n\nk + wt\n\n← (cid:80)\n\nit,kxt\n\nit\n\nit,itxt\n\nit\n\nit,k ̸= 0, k ̸= it}\n\nelse\n\nActive client model remains the same: xt+1/2\n\nit\n\n← xt it\n\nModel update for the active client: xt+1 Update other clients: xt+1 Update the active client’s counter cit ← cit + 1\n\nj ← xt j,\n\n∀ j ̸= it\n\nit\n\n← xt+1/2\n\nit\n\n− γg(xt\n\nit; ξt\n\nit)\n\nWait Free. The backbone of SWIFT is its wait-free structure. Unlike any other decentralized FL algorithms, SWIFT does not require simultaneous averaging between two clients or a neighborhood of clients. Instead, each client fetches the latest models its neighbors have sent it and performs averaging with those available (Lines 11-12 of Algorithm 1). There is no pause in local training waiting for a neighboring client to finish computations or average with, making SWIFT wait-free.\n\nUpdate Rule. communication simultaneously. Collectively, the update rule can be written in matrix form as\n\nSWIFT runs in parallel with all clients performing local gradient updates and model\n\n− γG(xt\n\nit, ξt\n\nit),\n\n(4)\n\nX t+1 = X tW t it\n\n4\n\n1 ∉ C1Local Gradient UpdateWait-Free Model Communication2 ∈ C13∉ C14 ∈ C1Fetch & AverageFetch & AverageFetch & AverageFetch & AverageFetch & AverageClient 1Client 2Client 3Client 4Fetch & AveragePublished as a conference paper at ICLR 2023\n\nwhere γ denotes the step size parameter and the matrix G(xt gradient of the active model xt contains the active gradient g(xt\n\nit. The entries of G(xt\n\nit, ξt\n\nit) ∈ Rd×n is the zero-padded it) are zero except for the it-th column, which\n\nit, ξt\n\nit, ξt\n\nit). Next, we describe the client-communication matrix W t it.\n\nClient-Communication Matrix. The backbone of decentralized FL algorithms is the clientcommunication matrix W (also known as the weighting matrix). To remove all forms of synchronization and to become wait-free, SWIFT relies upon a novel client-communication matrix W t it that is neither symmetric nor doubly-stochastic, unlike other algorithms in FL (Wang & Joshi, 2018; Lian et al., 2018; Li et al., 2019; Koloskova et al., 2020). The result of a non-symmetric and non-doubly stochastic client-communication matrix, is that averaging occurs for a single active client it and not over a pair or neighborhood of clients. This curbs superfluous communication time.\n\nWithin SWIFT, a dynamic client-communication matrix is implemented to allow for periodic averaging. We will now define the active client-communication matrix W t it in SWIFT, where it is the active client which performs the t-th global iteration. W t it can be one of two forms: (1) an identity matrix W t\n\nit = In if cit /∈ Cs or (2) a communication matrix if cit ∈ Cs with structure,\n\nW t\n\nit := In + (wt\n\nit\n\n− eit)e\n\nit, wt\n\nit := [wt\n\n1,it, . . . , wt\n\nn,it]\n\n⊺\n\nn (cid:88)\n\n⊺ ∈ Rn,\n\nwj,i = 1, wi,i ≥ 1/n ∀i. (5)\n\n∈ Rn denotes the active client-communication vector at iteration t, which contains The vector wt it the communication coefficients between client it and all clients (including itself). The clientcommunication coefficients induce a weighted average of local neighboring models. We note that wt it is often sparse because clients are connected to few other clients only in most decentralized settings.\n\nj=1\n\nNovel Client-Communication Weight Selection. While utilizing a non-symmetric and nondoubly-stochastic client-communication matrix decreases communication time, there are technical difficulties when it comes to guaranteeing the convergence. One of the novelties of our work is that we carefully design a client-communication matrix W t it such that it is symmetric and doubly-stochastic under expectation of all potential active clients it and has diagonal values greater than or equal to 1/n. Specifically, we can write\n\nn (cid:88)\n\nn (cid:88)\n\nEit\n\n(cid:2)W t\n\nit\n\n(cid:3) =\n\n(cid:2)In + (wt\n\ni − ei)e\n\n⊺ i\n\n(cid:3) = In +\n\npi\n\npi(wt\n\ni − ei)e\n\n⊺\n\ni =: ̄W t,\n\n(6)\n\ni=1\n\ni=1\n\nwhere, we denote ̄W t as the expected client-communication matrix with the following form,\n\n[ ̄W t]i,i = 1 + pi(wt\n\n(7) Note that ̄W t is column stochastic as the entries of any column sum to one. If we ensure that ̄W t is symmetric, then it will become doubly-stochastic. By Equation 7, ̄W t becomes symmetric if, i,j = piwt\n\nj,i ∀i, j ∈ V.\n\ni,i − 1), and [ ̄W t]i,j = pjwt\n\ni,j, for i ̸= j.\n\npjwt\n\n(8)\n\nTo achieve the symmetry of Equation 8, SWIFT deploys a novel pre-processing algorithm: the Communication Coefficient Selection (CCS) Algorithm. Given any client-influence vector pi, CCS determines all client-communication coefficients such that Equations 5 and 8 hold for every global iteration t. Unlike other algorithms, CCS focuses on the expected client-communication matrix, ensuring its symmetry. CCS only needs to run once, before running SWIFT. In the event that the underlying network topology changes, CCS can be run again during the middle of training. In Appendix B.2, we detail how CCS guarantees Equations 5 and 8 to hold.\n\nThe CCS Algorithm, presented in Appendix B.2, is a waterfall method: clients receive coefficients from their larger-degree neighbors. Every client runs CCS concurrently, with the following steps: (1) Receive coefficients from larger-degree neighbors. If the largest, or tied, skip to (2). (2) Calculate the total coefficients already assigned sw as well as the sum of the client influence scores for the unassigned clients sp. (3) Assign the leftover coefficients 1 − sw to the remaining unassigned neighbors (and self) in a manner proportional to each unassigned client i’s percentage of the leftover influence scores pi/sp. (4) If tied with neighbors in degree size, ensure assigned coefficients won’t sum to larger than one. (5) Send coefficients to smaller-degree neighbors.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n5 SWIFT THEORETICAL ANALYSIS\n\nMajor Message from Theoretical Analysis. As summarized in Table 1, the efficiency and effectiveness of decentralized FL algorithms depend on both the iteration convergence rate and communicationtime complexity; their product roughly approximates the total time for convergence. In this section, we will prove that SWIFT improves the SOTA convergence time of decentralized FL as it obtains SOTA iteration convergence rate (Theorem 1) and outperforms SOTA communication-time complexity.\n\nBefore presenting our theoretical results, we first detail standard assumptions (Kairouz et al., 2021) required for the analysis. Assumption 1 (L-smooth global and local objective functions). ∥∇f (x) − ∇f (y)∥ ≤ L ∥x − y∥ . Assumption 2 (Unbiased stochastic gradient). Eξ∼Di Assumption 3 (Bounded inter-client gradient variance). The variance of the stochastic gradient is bounded for any x with client i sampled from probability vector p and local client data ξ sampled from Di. This implies there exist constants σ, ζ ≥ 0 (where ζ = 0 in IID settings) such that: Ei ∥∇f (x) − ∇fi(x)∥2 ≤ ζ 2 ∀i, ∀x, Eξ∼Di ∥∇fi(x) − ∇l(x; ξ)∥2 ≤ σ2, ∀x.\n\n(cid:2)∇l(x; ξ)(cid:3) = ∇fi(x) for each i ∈ V.\n\nAs mentioned in Section 4, the use of a non-symmetric, non-doubly-stochastic matrix W t it causes issues in analysis. In Appendix C, we discuss properties of stochastic matrices, including symmetric and doubly-stochastic matrices, and formally define ρν, a constant related to the connectivity of the network. Utilizing our symmetric and doubly-stochastic expected client-communication matrix (constructed via Algorithm 2), we reformulate Equation 4 by adding and subtracting out ̄W t,\n\nX t+1 = X t ̄W t + X t(W t\n\nit\n\n− ̄W t) − γG(xt\n\nit, ξt\n\nit).\n\n(9)\n\nNext, we present our first main result in Lemma 1, establishing a client-communication error bound. Lemma 1 (Client-Communication Error Bound). Following Algorithm 2, the product of the difference between the expected and actual client communication matrices is bounded as follows:\n\nt (cid:88)\n\nE\n\nj=0\n\n(cid:13) (cid:13)G(xj (cid:13)\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n(W q\n\niq\n\n− ̄W q)(\n\nq=j+1\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n= O\n\n(cid:16) σ2 M\n\n+ E\n\nt (cid:88)\n\nj=0\n\n∥∇fij (xj\n\nij )∥2(cid:17)\n\n.\n\n(10)\n\nRemark. One novelty of our work is that we are the first to bound the client-communication error in the asynchronous decentralized FL setting. The upper bound in Lemma 1 is unique to our analysis because other decentralized works do not incorporate wait-free communication (Lian et al., 2017; Li et al., 2019; Wang & Joshi, 2018; Lian et al., 2018). Now, we are ready to present our main theorem, which establishes the convergence rate of SWIFT: Theorem 1 (Convergence Rate of SWIFT). Under assumptions 1, 2 and 3 (with Algorithm 2), let ∆f := f ( ̄x0) − f ( ̄x∗), step-size γ, total iteration T , and average model ̄xt be defined as n\n(cid:88)\n\n(cid:114)\n\n, T ≥ 1932LM ∆f ρ2\n\nνn4p2\n\nmax,\n\n ̄xt :=\n\n(cid:112)M n2∆f √\n√ M\nT L + Then, for the output of Algorithm 1, it holds that\n\nM n2∆f T L\n\nγ :=\n\n≤\n\n1 n\n\nxt i.\n\ni=1\n\n1 T\n\nT −1 (cid:88)\n\nt=0\n\nE (cid:13)\n\n(cid:13)∇f ( ̄xt)(cid:13) (cid:13)\n\n2 ≤\n\n2(cid:112)L∆f\n\n2(cid:112)∆f T\n\n+\n\n(cid:18)\n\n(cid:0)σ2 + 6ζ 2\n\n(cid:19)\n\n√\n\nM (cid:1)\n\n1 + 1921 20 ρν √\n\nT M\n\n.\n\n(11)\n\n(1) We prove that SWIFT obtains a O(1/\n\nIteration Convergence Rate Remarks. T ) iteration convergence rate, matching the optimal rate for SGD (Dekel et al., 2012; Ghadimi & Lan, 2013; Lian et al., 2017; 2018). (2) Unlike existing asynchronous decentralized SGD algorithms, SWIFT’s iteration convergence rate does not depend on the maximal bounded delay. Instead, we bound any delays by taking the expectation over the active client. The probability of each client i being the active client is simply its sampling probability pi. We therefore assume that each client i is expected to perform updates at its prescribed sampling probability rate pi. Clients which are often delayed in practice can be dealt with by lowering their inputted sampling probability. (3) SWIFT converges in fewer total iterations T with respect to n total clients compared to other asynchronous methods (Lian et al., 2018) (T = Ω(n4p2 max) in SWIFT versus T = Ω(n4) in AD-PSGD). Similar to AD-PSGD, SWIFT achieves a linear speed-up in computational complexity as the number of clients increase.\n\n√\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nCommunication-Time Complexity Remarks. (1) Due to its asynchronous nature, SWIFT achieves a communication-time complexity that relies only on each client’s own communication time per round Ci. This improves upon synchronous decentralized SGD algorithms, which rely upon the Cj. (2) Unlike AD-PSGD communication time per round of the slowest neighboring client maxj∈Ni (Lian et al., 2018), which also achieves a communication-time complexity reliant on Ci, SWIFT incorporates periodic averaging which further reduces the communication complexity from T rounds of communication to |Cs|. Furthermore, SWIFT allows for entire neighborhood averaging, and not just one-to-one gossip averaging. This increases neighborhood information sharing, improving model robustness and reducing model divergence. Corollary 1 (Convergence under Uniform Client Influence). In the common scenario where client influences are uniform, pi = 1/n ∀i =⇒ pmax = 1/n, SWIFT obtains convergence improvements: total iterations T with respect to the number of total clients n improves to T = Ω(n2) as compared to T = Ω(n4) for AD-PSGD under the same conditions.\n\n6 EXPERIMENTS\n\nBelow, we perform image classification experiments for a range of decentralized FL algorithms (Krizhevsky et al., 2009). We compare the results of SWIFT to the following decentralized baselines: • The most common synchronous decentralized FL algorithm: D-SGD (Lian et al., 2017). • Synchronous decentralized FL communication reduction algorithms: PA-SGD (Wang & Joshi, 2018) and LD-SGD (Li et al., 2019). • The most prominent asynchronous decentralized FL algorithm: AD-PSGD (Lian et al., 2018).\n\nFiner details of the experimental setup are in Appendix A. Throughout our experiments we use two network topologies: standard ring and ring of cliques (ROC). ROC-xC signifies a ring of cliques with x clusters. The ROC topology is more reflective of a realistic network, as networks usually have pockets of connected clients. These topologies are visualized in Figures 7 and 8 respectively.\n\n6.1 BASELINE COMPARISON\n\nTo compare the performance of SWIFT to all other algorithms listed above, we reproduce an experiment within (Lian et al., 2018). With no working code for AD-PSGD to run on anything but an extreme supercomputing cluster (Section 5.1.2 of (Lian et al., 2018)), reproducing this experiment allows us to compare the relative performance of SWIFT to AD-PSGD.\n\n16 Client Ring Epoch (s) % Change Comm. (s) % Change\n\nDecentralized FL Algorithms SWIFT (C0 ) D-SGD (C0 ) AD-PSGD∗ (C0 ) SWIFT (C1) LD-SGD (C1) PA-SGD (C1)\n\n0.086 0.627 —\n0.064 0.428 0.358 * AD-PSGD results come from Table 4 in (Lian et al., 2018).\n\n-34.60 —\n-15.86 -34.79 -15.28 -17.78\n\n1.019 1.558 —\n1.016 1.320 1.281\n\n-86.28 —- —\n-89.79 -31.74 -42.90\n\n(a) Average epoch and communication times.\n\n(b) Average test loss.\n\n(c) Average train loss.\n\nFigure 2: Baseline performance comparison on CIFAR-10 for 16 client ring.\n\nTable in Figure 2a showcases that SWIFT reduces the average epoch time, relative to D-SGD, by 35% (C0 and C1). This far outpaces AD-PSGD (as well as the other synchronous algorithms), with AD-PSGD only reducing the average epoch time by 16% relative to D-SGD. Finally, Figure 2 displays how much faster SWIFT achieves optimal train and test loss values compared to other decentralized baseline algorithms. SWIFT outperforms all other baseline algorithms even without any slow-down (which we examine in Section 6.2), where wait-free algorithms like SWIFT especially shine.\n\n6.2 VARYING HETEROGENEITIES\n\nVarying Degrees of Non-IIDness Our second experiment evaluates SWIFT’s efficacy at converging to a well-performing optima under varying degrees of non-IIDness. We vary the degree (percentage) of each client’s data coming from one label. The remaining percentage of data is randomly sampled (IID) data over all labels. A ResNet-18 model is trained by 10 clients in a 3-cluster ROC network topology. We chose 10 clients to make the label distribution process easier: CIFAR-10 has 10 labels.\n\n7\n\n00.511.522.530.511.52Wall-clockTime(Minutes)TestLossD-SGDLD-SGDPA-SGDSWIFTSWIFT(2-SGD)00.511.522.530.511.52Wall-clockTime(Minutes)TrainingLossD-SGDLD-SGDPA-SGDSWIFTSWIFT(2-SGD)Published as a conference paper at ICLR 2023\n\nAs expected, when data becomes more non-IID, the test loss becomes higher and the overall accuracy lower (Table 2). We do see, however, that SWIFT converges faster, and to a lower average loss, than all other synchronous baselines (Figure 3). In fact, SWIFT with C1 converges much quicker than the synchronous algorithms. This is an important result: SWIFT converges both quicker and to a smaller loss than synchronous algorithms in the non-IID setting.\n\nDecentralized FL Algorithms SWIFT (C0) D-SGD (C0) SWIFT (C1) LD-SGD (C1) PA-SGD (C1)\n\n10 Client Ring of Cliques - 3 Cluster Topology Epoch Time (s) 1.709 2.116 1.517 1.973 1.929\n\nCommunication Time (s) 0.197 0.705 0.110 0.575 0.421\n\nTable 2: Average epoch and communication times for non-IID setting on CIFAR-10.\n\n(a) 1/4 degree non-IID data. (d) 9/10 degree non-IID data. Figure 3: Average test loss for varying degrees of non-IIDness on CIFAR-10, 10 client ROC-3C.\n\n(c) 7/10 degree non-IID data.\n\n(b) 1/2 degree non-IID data.\n\nVarying Heterogeneity of Clients In this experiment, we investigate the performance of SWIFT under varying heterogeneity, or speed, of our clients (causing different delays). This is done with 16 clients in a ring topology. We add an artificial slowdown, suspending execution of one of the clients such that it takes a certain amount of time longer (slowdown) to perform the computational portions of the training process. We perform tests in the case where a client is two times (2x) and four times (4x) as slow as usual. We then compare how the decentralized algorithms fare under these circumstances compared to the normal setting with no added slowdown.\n\nDecentralized FL Algorithms SWIFT (C0) D-SGD (C0) SWIFT (C1) LD-SGD (C1) PA-SGD (C1)\n\nTotal (s) 2.69 3.110 2.44 3.323 3.183\n\nNo Slowdown\n\nEpoch (s) Comm. (s)\n\n1.033 1.45 0.996 1.353 1.270\n\n0.087 0.564 0.061 0.413 0.331\n\nTotal (s) 2.451 3.972 2.152 3.762 3.557\n\n2x Slowdown Epoch (s) Comm. (s)\n\n0.964 1.571 0.928 1.389 1.262\n\n0.064 0.616 0.040 0.428 0.309\n\nTotal (s) 3.054 6.137 2.847 5.917 5.743\n\n4x Slowdown Epoch (s) Comm. (s)\n\n1.117 1.666 1.074 1.412 1.270\n\n0.091 0.651 0.065 0.448 0.318\n\nTable 3: Average epoch and communication times on CIFAR-10 for 16 client ring with slowdown.\n\nIn Table 3, the average epoch, communication, and total time is displayed. Average total time includes computations, communication, and any added slowdown (wait time). SWIFT avoids large average total times as the slowdown grows larger. The wait-free structure of SWIFT allows all non-slowed clients to finish their work at their own speed. All other algorithms require clients to wait for the slowest client to finish a mini-batch before proceeding. At large slowdowns (4x), the average total time for SWIFT is nearly half of those for synchronous algorithms. Thus, SWIFT is very effective at reducing the run-time when clients are slow within the network.\n\nFigure 4: SWIFT vs. D-SGD for CIFAR-10 in 16 client ring with varying slowdown.\n\n(b) Train loss slowdown.\n\n(a) Test loss slowdown.\n\nFigure 4 shows how SWIFT is able to converge faster to an equivalent, or smaller, test loss than D-SGD for all slowdowns. In the case of large slowdowns (4x), SWIFT significantly outperforms D-SGD, finishing in better than half the run-time. We do not include the other baseline algorithms to avoid overcrowding of the plotting space. However, SWIFT also performs much better than PA-SGD and LD-SGD as shown in Table 3. These results show that the wait-free structure of SWIFT allows it to be efficient under client slowdown.\n\n8\n\n02468100246Wall-clockTime(Minutes)TestLossD-SGDLD-SGDPA-SGDSWIFTSWIFT(2-SGD)02468100246Wall-clockTime(Minutes)TestLossD-SGDLD-SGDPA-SGDSWIFTSWIFT(2-SGD)02468100246Wall-clockTime(Minutes)TestLossD-SGDLD-SGDPA-SGDSWIFTSWIFT(2-SGD)02468100246Wall-clockTime(Minutes)TestLossD-SGDLD-SGDPA-SGDSWIFTSWIFT(2-SGD)02468100.511.52Wall-clockTime(Minutes)TestLossD-SGDSWIFTSWIFT(2-SGD)D-SGD(2x)SWIFT(2x)SWIFT(2-SGD,2x)D-SGD(4x)SWIFT(4x)SWIFT(2-SGD,4x)02468100.511.52Wall-clockTime(Minutes)TrainingLossD-SGDSWIFTSWIFT(2-SGD)D-SGD(2x)SWIFT(2x)SWIFT(2-SGD,2x)D-SGD(4x)SWIFT(4x)SWIFT(2-SGD,4x)Published as a conference paper at ICLR 2023\n\n6.3 VARYING NUMBERS OF CLIENTS & NETWORK TOPOLOGIES\n\nfourth experiment, we determine how SWIFT clients. the\n\nas we\n\nnumber\n\nvary\n\nof\n\nalgorithms\n\nversus\n\nIn our\n\nbaseline\n\nVarying Numbers of Clients performs other In Table 4, the time per epoch for SWIFT drops by nearly the optimal factor of 2 as the number of clients is doubled. For all algorithms, there is a bit of parallel overhead when the number of clients is small, however this becomes minimal as the number of clients grow to be large (greater than 4 clients). In comparison to the synchronous algorithms, SWIFT actually decreases its communication time as the number of clients increases. This allows the parallel performance to be quite efficient, as shown in Figure 5.\n\n(a) Communication time.\n\n(b) Epoch time.\n\nFigure 5: Average communication and epoch times for increasing numbers of clients.\n\nDecentralized FL Algorithms SWIFT (C0) D-SGD (C0) SWIFT (C1) LD-SGD (C1) PA-SGD (C1)\n\n16 Client Ring Epoch (s) Comm. (s)\n\n8 Client Ring Epoch (s) Comm. (s)\n\n4 Client Ring Epoch (s) Comm. (s)\n\n2 Client Ring Epoch (s) Comm. (s)\n\n1.019 1.558 1.016 1.320 1.281\n\n0.086 0.627 0.064 0.428 0.358\n\n2.003 2.459 1.970 2.217 2.093\n\n0.172 0.525 0.112 0.345 0.285\n\n3.964 3.970 3.862 3.946 3.871\n\n0.314 0.439 0.232 0.300 0.220\n\n6.971 7.62 6.83 6.712 7.303\n\n0.251 0.393 0.176 0.179 0.180\n\nTable 4: Average epoch and communication times on CIFAR-10 with varying clients in ring topology.\n\nVarying Topologies Our fifth experiment analyzes the effectiveness of SWIFT versus other baseline decentralized algorithms under different, and more realistic, network topologies. In this experiment setting, we train 16 clients on varying network topologies (Table 5 and Figure 6).\n\nDecentralized FL Algorithms SWIFT (C0) D-SGD (C0) SWIFT (C1) LD-SGD (C1) PA-SGD (C1)\n\n16 Client ROC-2C Epoch (s) Comm. (s)\n\n16 Client ROC-4C Epoch (s) Comm. (s)\n\n16 Client Ring Epoch (s) Comm. (s)\n\n1.793 2.799 1.611 2.408 2.639\n\n0.416 1.479 0.295 0.987 0.765\n\n1.291 2.813 1.494 2.525 2.216\n\n0.124 1.464 0.174 1.105 0.708\n\n1.367 2.241 1.348 2.172 1.982\n\n0.121 0.962 0.085 0.517 0.500\n\nTable 5: Average epoch and communication times on CIFAR-10 for varying network topologies.\n\n(a) ROC-2C (n = 16).\n\n(b) ROC-4C (n = 16). Figure 6: Average test loss for varying network topologies on CIFAR-10.\n\n(c) Ring (n = 16).\n\n7 CONCLUSION\n\nSWIFT delivers on the promises of decentralized FL: a low communication and run-time algorithm (scalable) which attains SOTA loss (high-performing). As a wait-free algorithm, SWIFT is wellsuited to rapidly solve large-scale distributed optimization problems. Empirically, SWIFT reduces communication time by almost 90% compared to baseline decentralized FL algorithms. In future work, we aim to add protocols for selecting optimal client sampling probabilities. We would like to show how varying these values can: (i) boost convergence both theoretically and empirically, and (ii) improve robustness under local client data distribution shift.\n\n9\n\n24681012141600.10.20.30.40.50.60.7NumberofClientsAverageCommunicationTime(s)D-SGDLD-SGDPA-SGDSWIFTSWIFT(2-SGD)246810121416100100.2100.4100.6100.8101NumberofClientsAverageEpochTime(s)D-SGDLD-SGDPA-SGDSWIFTSWIFT(2-SGD)024680.511.52Wall-clockTime(Minutes)TestLossD-SGDLD-SGDPA-SGDSWIFTSWIFT(2-SGD)024680.511.52Wall-clockTime(Minutes)TestLossD-SGDLD-SGDPA-SGDSWIFTSWIFT(2-SGD)02460.511.52Wall-clockTime(Minutes)TestLossD-SGDLD-SGDPA-SGDSWIFTSWIFT(2-SGD)Published as a conference paper at ICLR 2023\n\n8 ETHICS STATEMENT\n\nWe propose a novel wait-free decentralized Federated Learning algorithm with strong theoretical guarantees and improved empirical results. Our contributions add to the sparse foundational literature in asynchronous decentralized Federated Learning. Therefore, our work does not have direct societal or ethical consequences. We would like to note that it is imperative for users of Federated Learning algorithms in real-world distributed learning applications to respect data privacy.\n\n9 REPRODUCIBILITY\n\nOur code can be found on GitHub at https://github.com/umd-huang-lab/SWIFT. We ran five trials for SWIFT and each baseline algorithm we compared it to with random seeds over each experiment (Sections 6.1, 6.2, 6.2, 6.3). Our plots include error bars from these five trials for each experiment. As stated in Section 6, we perform image classification experiments on the CIFAR-10 dataset (Krizhevsky et al., 2009). In Table 6 we describe the hyperparameters we use in all experiments. Appendix A describes further experimental setup details (computational resources used, how we partition data, and more). Finally we provide pseudocode for SWIFT in Algorithm 1.\n\n10 ACKNOWLEDGMENTS\n\nBornstein, Rabbani and Huang acknowledge support by the National Science Foundation NSF-IISFAI program, DOD-ONR-Office of Naval Research, DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness against Deception (GARD), Adobe, Capital One and JP Morgan faculty fellowships. Rabbani is additionally supported by NSF DGE-1632976. Bedi acknowledges the support by Army Cooperative Agreement W911NF2120076. Bornstein additionally thanks Michael Blankenship for both helpful discussions regarding parallel code implementation and inspiration for SWIFT’s name.\n\nREFERENCES\n\nSoheil Abbasloo and H Jonathan Chao. Sharpedge: An asynchronous and core-agnostic solution to\n\nguarantee bounded-delays. CCF Transactions on Networking, 3(1):35–50, 2020.\n\nAlekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. Advances in neural\n\ninformation processing systems, 24, 2011.\n\nTuncer Can Aysal, Mehmet Ercan Yildiz, Anand D Sarwate, and Anna Scaglione. Broadcast gossip algorithms for consensus. IEEE Transactions on Signal processing, 57(7):2748–2761, 2009.\n\nAmrit Singh Bedi, Alec Koppel, and Ketan Rajawat. Asynchronous online learning in multi-agent systems with proximity constraints. IEEE Transactions on Signal and Information Processing over Networks, 5(3):479–494, 2019a.\n\nAmrit Singh Bedi, Alec Koppel, and Ketan Rajawat. Asynchronous saddle point algorithm for stochastic optimization in heterogeneous networks. IEEE Transactions on Signal Processing, 67 (7):1742–1757, 2019b.\n\nAurélien Bellet, Anne-Marie Kermarrec, and Erick Lavoie. D-cliques: Compensating for data\n\nheterogeneity with topology in decentralized federated learning. 2021.\n\nStephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Gossip algorithms: Design, In Proceedings IEEE 24th Annual Joint Conference of the IEEE\n\nanalysis and applications. Computer and Communications Societies., volume 3, pp. 1653–1664. IEEE, 2005.\n\nStephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms.\n\nIEEE transactions on information theory, 52(6):2508–2530, 2006.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nXuanyu Cao and Tamer Ba ̧sar. Decentralized multi-agent stochastic optimization with pairwise constraints and quantized communications. IEEE Transactions on Signal Processing, 68:3296– 3311, 2020. doi: 10.1109/TSP.2020.2997394.\n\nOfer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction\n\nusing mini-batches. Journal of Machine Learning Research, 13(1), 2012.\n\nSaeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic\n\nprogramming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.\n\nSaeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1):267–305, 2016.\n\nIstván Heged ̋us, Gábor Danner, and Márk Jelasity. Decentralized learning works: An empirical comparison of gossip learning and federated learning. Journal of Parallel and Distributed Computing, 148:109–124, 2021.\n\nBeomyeol Jeon, SM Ferdous, Muntasir Raihan Rahman, and Anwar Walid. Privacy-preserving decentralized aggregation for federated learning. In IEEE INFOCOM 2021-IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS), pp. 1–6. IEEE, 2021.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210, 2021.\n\nAnastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified theory of decentralized sgd with changing topology and local updates. In International Conference on Machine Learning, pp. 5381–5393. PMLR, 2020.\n\nAnastasia Koloskova, Sebastian U Stich, and Martin Jaggi. Sharper convergence guarantees for asynchronous sgd for distributed and federated learning. Advances in Neural Information Processing Systems 35 (NeurIPS 2022), 2022.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nHe Li, Kaoru Ota, and Mianxiong Dong. Learning iot in edge: Deep learning for the internet of\n\nthings with edge computing. IEEE network, 32(1):96–101, 2018.\n\nXiang Li, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Communication-efficient local decentral-\n\nized sgd methods. arXiv preprint arXiv:1910.09126, 2019.\n\nXiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for\n\nnonconvex optimization. Advances in Neural Information Processing Systems, 28, 2015.\n\nXiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. Advances in Neural Information Processing Systems, 30, 2017.\n\nXiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic gradient descent. In International Conference on Machine Learning, pp. 3043–3052. PMLR, 2018.\n\nQi Liu, Bo Yang, Zhaojian Wang, Dafeng Zhu, Xinyi Wang, Kai Ma, and Xinping Guan. Asynchronous decentralized federated learning for collaborative fault diagnosis of pv stations. IEEE Transactions on Network Science and Engineering, 2022.\n\nQinyi Luo, Jiaao He, Youwei Zhuo, and Xuehai Qian. Prague: High-performance heterogeneity-aware asynchronous decentralized training. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, pp. 401–416, 2020.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 1273–1282. PMLR, 2017.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nKonstantin Mishchenko, Francis Bach, Mathieu Even, and Blake Woodworth. Asynchronous sgd beats minibatch sgd under arbitrary delays. In NeurIPS 2022-Thirty-sixth Conference on Neural Information Processing Systems, 2022.\n\nGiorgi Nadiradze, Amirmojtaba Sabour, Peter Davies, Shigang Li, and Dan Alistarh. Asynchronous decentralized sgd with quantized and local updates. Advances in Neural Information Processing Systems, 34:6829–6842, 2021.\n\nAngelia Nedi ́c and Alex Olshevsky. Distributed optimization over time-varying directed graphs.\n\nIEEE Transactions on Automatic Control, 60(3):601–615, 2014.\n\nAngelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization.\n\nIEEE Transactions on Automatic Control, 54(1):48–61, 2009.\n\nYurii Nesterov. Introductory lectures on convex programming volume i: Basic course, 1998.\n\nBenjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. Advances in neural information processing systems, 24, 2011.\n\nStefano Savazzi, Monica Nicoli, and Vittorio Rampa. Federated learning with cooperating devices: A consensus approach for massive iot networks. IEEE Internet of Things Journal, 7(5):4641–4654, 2020.\n\nJianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of\n\ncommunication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576, 2018.\n\nJianyu Wang, Anit Kumar Sahu, Zhouyi Yang, Gauri Joshi, and Soummya Kar. Matcha: Speeding up decentralized sgd via matching decomposition sampling. In 2019 Sixth Indian Control Conference (ICC), pp. 299–300. IEEE, 2019.\n\nHao Ye, Le Liang, and Geoffrey Ye Li. Decentralized federated learning with unreliable communica-\n\ntions. IEEE Journal of Selected Topics in Signal Processing, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nSupplementary Material\n\nA ADITIONAL EXPERIMENTAL DETAILS AND SETUP\n\nA.1 COMPUTATIONAL SPECIFICATIONS\n\nWe train our consensus model on a network of nodes. Each node has an NVIDIA GeForce RTX 2080 Ti GPU. All algorithms are built in Python and communicate via Open MPI, using MPI4Py (Python bindings for MPI). The training is also done in Python, leveraging Pytorch.\n\nA.2 DATA PARTITIONING\n\nFor all experiments, the training set is evenly partitioned amongst the number of clients training the consensus model. While the size of each client’s partition is equal, we perform testing with data that is both (1) independent and identically distributed (iid) among all clients and (2) sorted by class and is thus non-iid. For the iid setting, each client is assigned data uniformly at random over all classes. In the non-iid setting, each client is assigned a subset of classes from which it will receive data exclusively. The c classes are assigned in the following steps: (1) The class subset size nc, for all clients, is determined as the ceiling of the number of classes per client nc = ⌈ c n ⌉. Each class k within the class subset will take up 1/nc of the client’s total data partition if possible. (2) The classes within each client’s class subset are assigned cyclically, starting with the first client. The first client selects the first nc classes, the second client selects the next nc classes, and so on. Classes can, in some cases, be assigned to multiple clients. If the final class has been assigned, and more clients have yet to be assigned any classes, classes can be re-assigned starting at the first class. (3) Each client is assigned data from the classes in their class subset cyclically (1/nc of its partition for each class), starting with the first client. If no more data is available from a specific class, the required data to fill its fraction of the partition is replaced by data from the next class.\n\nSince we follow this data partitioning process within our experiments, each client is assigned equal partitions of data. Therefore, following the works (Lian et al., 2018; Wang et al., 2019; Ye et al., 2022; Li et al., 2019), we set the client influence scores to be uniform for all clients pi = 1/n ∀i ∈ V.\n\nA.3 EXPERIMENTAL SETUP\n\nBelow we provide information into the hyperparameters we select for our experiments in Section 6.\n\nModel Experiment Type ResNet-18 Baseline Vary non-IIDness ResNet-18 Vary Heterogeneity ResNet-18 ResNet-18 ResNet-50\n\nVary # of Clients Vary Topology\n\nEpochs E 200 300 100 200 200\n\nγ 0.1 0.8 0.1 0.1 0.1\n\nγ Decay (Rate, E, Freq.) M Weight Decay Momentum (1/10, 81 & 122, Single) (1/2, 200, 10) (1/2, 50, 10) (1/10, 81 & 122, Single) (1/2, 100, 10)\n\n10−4 10−4 10−4 10−4 10−4\n\n0.9 0.9 0.9 0.9 0.9\n\n32 32 32 32 64\n\nTable 6: Hyperparameters for all experiments.\n\nIn Table 6, one can see that the step-size decay column is split into the following sections: rate, E, and frequency. The rate is the decay rate for the step-size. For example, in the Baseline row, the step-size decays by 1/10. The term E is the epoch at which decay begins during training. For example, in the Baseline row, the step-size decays at E = 81 and 122. Frequency simply is how often the step-size decays. For example, in the Vary Topology row, the step-size decays every 10 epochs.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA.4 NETWORK TOPOLOGIES\n\nRing Topology. Please refer to Figure 7.\n\nFigure 7: An 8 Client Ring.\n\nRing of Cliques Topology. Please refer to Figure 8.\n\nFigure 8: (Left) 10 Client, 3-Cluster. (Middle) 16 Client, 2-Cluster. (Right) 16 Client, 4-Cluster.\n\nLike the works of (Jeon et al., 2021; Bellet et al., 2021), we believe that the ring of cliques network topology is a realistic topology in the decentralized setting. In many real-world setting, like one’s home (smart appliances, smart speakers/displays, phones, etc.), devices are connected together in a small clique. Only a small amount of these devices have connections to other devices outside the cluster (like a phone or router). We wanted to utilize this network topology due to this realistic nature. Furthermore, (Bellet et al., 2021) shows that a ring of clique topology can be used in the decentralized setting to reduce the impact of label distribution skew.\n\nB ALGORITHM DETAILS AND NOTATION\n\nB.1 NOTATION TABLE\n\nDefinition\n\nNotation\n\nDefinition\n\nNotation\n\nGlobal Iteration Active Client at t Client (i, j) Communication Coefficient at t\n\nLocal Model of Client i at t Local Model of Active Client at t Mini-Batch Data from Active Client at t Gradient of the Active Model at t Client i Communication Vector at t\n\nLocal Model Matrix at t\n\nZero-Padded Gradient of the Active Model at t G(xt\n\nExpected Local Model Gradients at t Active Client Communication Matrix at t Expected Client Communication Matrix at t\n\ni,j\n\nt it wt i ∈ Rd xt ∈ Rd xt it ∈ Rf ξt it it) ∈ Rd it, ξt wt\n\ng(xt\n\ni ∈ Rn X t ∈ Rd×n it, ξt\n\nit) ∈ Rd×n ̄G(X t, ξt) ∈ Rd×n\n\n∈ Rn×n W t it ̄W t ∈ Rn×n\n\nNumber of Clients Parameter Dimension Data Dimension\n\nStep-Size Total SWIFT Iterations Mini-Batch Size (Uniform) Communication Set Global Data Distribution\n\nn d\nf\n\nγ T\nM Cs D\n\nGlobal Objective Client Influence Score Client Influence Vector One-Hot Vector Identity Matrix\n\nf (x) pi p ∈ Rn ei ∈ Rn In ∈ Rn×n\n\n14\n\nClient 5Client 6Client 8Client 7Client 4Client 1Client 3Client 2Client 1Client 2Client 3Client 4Client 5Client 6Client 8Client 9 Client 7Client 10Client 10Client 11Client 13Client 12Client 9Client 14Client16 Client 15Client 5Client 6Client 8Client 7Client 4Client 1Client 3Client 2Client 2Client 3Client 1Client 4Client 5Client 6Client 8Client 7Client 9Client 10Client 12Client 11Client 16Client 15Client 14Client 13Published as a conference paper at ICLR 2023\n\nAlgorithm 2: Communication Coefficient Selection (CCS) Input\n\n:Client Influence Score (CIS) pi ∈ R, Client Degree di ∈ R, Client Neighbor Set Ji = {∀j : client j is a one-hop neighbor of client i}, ∀i\n\nOutput :Client-Communication Vector wi ∈ Rn, ∀i ∈ [n]\n\n1 for i = 1 : n in parallel do\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\nif CIS are non-uniform then\n\nInitialize Client-Communication Vector wi = [w1,i, w2,i, · · · , wn,i] ← (1/n)ei\n\nelse\n\nInitialize Client-Communication Vector wi = [w1,i, w2,i, · · · , wn,i] ← 0\n\nExchange CIS and degree with all neighbors Store Neighbor CIS Vector P J ← [{pj}j∈Ji] ∈ Rdi Construct Neighbor Subsets J L, J SE, J E ⊂ Ji as subsets of i’s neighbors with degree larger than, no larger than and equal to di respectively\n\nfor ∀j ∈ J L do\n\nWait to fetch wj,i from neighbor client j with a degree larger than di\n\nw ← (cid:80)n\n\nm=1 wm,i\n\np ← (cid:80)\n\nj∈J SE P J\n\nj\n\nDetermine the sum of the total coefficients assigned (TCA) si if |J SE| > 0 then\n\nDetermine the sum of all remaining neighbors’ CIS si if |J E| > 0 then Exchange si Store s∗ Set wj,i ← (1−s∗ Recompute si\n\nw and si w ← max{si w)P J s∗ p\n\nw} and s∗ ∀j ∈ J E\n\np ← max{si\n\nw = (cid:80)n\n\np = (cid:80)\n\nj\n\np with all neighbors j ∈ J E, storing all exchanged sj p, sj w, sj\n\np} ∀j ∈ J E\n\nw, sj\n\np\n\nj∈{J SE∪i}\\J E P J\n\nm=1 wm,i and si (1−si w)P J si p\nto all waiting neighbors j ∈ J SE \\ J E\n\nj\n\nj\n\n∀j ∈ {J SE ∪ i} \\ J E for all remaining neighbors\n\nSet wj,i ← wj,i + Send wi,j = (1−si\n\nw)P J si p\n\ni\n\nelse\n\nwi,i = 1 − si\n\nw\n\nB.2 SWIFT PRE-PROCESSING: SETTING CLIENT-COMMUNICATION WEIGHTS\n\nBelow we present the the algorithmic pseudocode here for our novel client-communication selection algorithm in Algorithm 2. As a note, we include different client-communication vector initializations if the client influence scores are uniform versus non-uniform. The reason for this is to ensure that the selfweight for each client i, wi,i, has a value greater than 1/n. This naturally occurs when the CIS are uniform, however is not so when they are non-uniform.\n\nIn Algorithm 2, the terms sw and sp play a pivotal role in satisfying Equations 5 and 8 respectively. Equation 8 is satisfied by assigning weights amongst client i and its neighbors j as follows\n\npj\n\n(1 − sw)pi sp\n\n= pi\n\n(1 − sw)pj sp\n\n.\n\n(12)\n\nAssigning weights proportionally with respect to neighboring client influence scores and total neighbors ensures higher influence clients receive higher weighting during averaging.\n\nB.3 OPTIMAL STEP-SIZE UNDER UNIFORM CLIENT INFLUENCE\n\nThe defined step-size γ and total iterations T for SWIFT is (cid:112)M n2∆f √\n√ M\nT L +\n\nM n2∆f T L\n\nγ :=\n\n(cid:114)\n\n≤\n\n, T ≥ 1932LM ∆f ρ2\n\nνn4p2\n\nmax.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTherefore, γ can be rewritten as\n\n(cid:115)\n\nγ ≤\n\nM n2∆f\n\n(cid:115)\n\n=\n\n1 νn2p2 1932L2ρ2\n\n.\n\nmax)L When the client influence scores are uniform (i.e, pi = 1/n ∀i ∈ V), one can see that our step-size becomes\n\n(1932LM ∆f ρ2\n\nνn4p2\n\nmax\n\nγ = O\n\n(cid:19)\n\n.\n\n(cid:18) 1 L\n\nThis mirrors the optimal step-size in analysis of gradient descent convergence to first-order stationary points O(1/L) (Nesterov, 1998).\n\nB.4\n\nINSTANTANEOUS COMMUNICATION AND OVERCOMING COMPUTATIONAL DELAY\n\nSimilar to AD-PSGD (Lian et al., 2018), we assume that model transmission is instantaneous between clients. Within (Lian et al., 2018), two algorithms (Algorithms 2 and 3) are provided within the appendix to provide realistic multi-thread implementation of AD-PSGD. The computational thread of Algorithm 2 is explicitly told to wait at Line 5 until the gradient buffer is empty. Unless a gradient queue is utilized, which alters the effective batch-size from the constant M , the computational thread must wait for model transmission if there exists communication delay. Important future work for SWIFT, and within the field of asynchronous decentralized FL in general, is to remove this instantaneous delay assumption and build protocols to overcome it (and theoretically guarantee convergence in its presence).\n\nWe would like to mention that eliminating computational delay is a key novelty of SWIFT. Delayed computations are handled by SWIFT through its communication matrix. Unlike AD-PSGD, no model overwriting can occur while a client is performing gradient computations (Line 9 of Algorithm 1). This only occurs due to our novel non-doubly stochastic communication matrix. Therefore, client will never be computing gradients with a delayed model. This, of course, assumes that model transmission is instantaneous.\n\nSWIFT deals with a slow client i (having delayed computations) by allowing a faster client j to reuse a stored model of client i until client i finally finishes its model update. The stored model is still up to date (and not delayed) since client i has not finished its gradient computations, updated its model, and sent out its updated model to neighboring clients (as it experiences delayed computations). If a client has not received a message from a neighbor, then their stored model for that neighbor is still its most up-to-date model.\n\nC PROPERTIES OF COMMUNICATION MATRICES\n\nStochastic Matrices. Within our work, we use a non-symmetric, non-doubly-stochastic matrix W t it for client averaging. Utilizing W t it comes with some analysis issues (it is non-symmetric), however it provides the wait-free nature of SWIFT. Interestingly, W t it does have some unique properties: it is column-stochastic. Lemma 3 proves that the product of stochastic matrices converges exponentially to a stochastic vector with common ratio ν ∈ [0, 1).\n\nSymmetric and Doubly-Stochastcic Matrices. As mentioned in Section 5, we utilize Algorithm 2 to select client weights such that we have a symmetric and doubly-stochastic communication matrix ̄W t under expectation. By Lemma 2, there exists a scalar ρ ∈ [0, 1) such that (cid:0)( ̄W t)⊺ ̄W t(cid:1)|} ≤ ρ, ∀t. This parameter ρ reflects the connectivity of the max{|λ2 underlying graph topology. The value of ρ is inversely proportionate to how fast information spreads in the client network. A small value of ρ results in information spreading faster (ρ = 0 in centralized settings).\n\n(cid:0)( ̄W t)⊺ ̄W t(cid:1)|, |λn\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nWithin our analysis, we denote the parameter ρν as a combination of ρ and ν:\n\nρν :=\n\nn − 1 n\n\n(\n\n7 2(1 − ρ)\n\n+\n\n(1 −\n\nρ)2 +\n\n384 (1 − ν2)\n\n√\n\nρ √\n\n)\n\n(13)\n\nD REVIEW OF EXISTING INTER-CLIENT COMMUNICATION IN\n\nDECENTRALIZED FL\n\nThe predecessor to decentralized FL is gossip learning (Boyd et al., 2006; Heged ̋us et al., 2021). Gossip learning was first introduced by the control community to assist with mean estimation of decentrally-hosted data distributions (Aysal et al., 2009; Boyd et al., 2005). Now, SGD-based gossip algorithms are used to solve large-scale machine learning tasks (Lian et al., 2015; 2018; Ghadimi et al., 2016; Nedic & Ozdaglar, 2009; Recht et al., 2011; Agarwal & Duchi, 2011). A key feature of gossip learning is the presence of a globally shared oracle/memory with whom clients exchange parameters at the end of training rounds (Boyd et al., 2006). While read/write-accessible shared memory is well-suited for a single-organization ecosystem (i.e. all clients are controllable and trusted), this is unrealistic for more general edge-based paradigms. Below, we review newer decentralized SGD algorithms, such as D-SGD (Lian et al., 2017), PA-SGD (Wang & Joshi, 2018), and LD-SGD Li et al. (2019). These algorithms theoretically and empirically outperform their centralized counterparts, especially under heterogeneous client data distribution.\n\nDecentralized SGD (D-SGD) (Lian et al., 2017) One of the foundational decentralized Federated Learning algorithms is Decentralized SGD. In order to minimize Equation 1, D-SGD orchestrates a local gradient step for all clients before performing synchronous neighborhood averaging. The D-SGD process for a single client i is defined as:\n\nn (cid:88)\n\nxt+1\n\ni =\n\nWij\n\n(cid:2)xt\n\nj − g(xt\n\nj, ξt\n\nj)(cid:3).\n\n(14)\n\nj=1\n\nj, ξt\n\nj with mini-batch data ξt\n\nj) denotes the stochastic gradient of xt\n\nThe term g(xt j sampled from the local data distribution of client j. The matrix W is a weighting matrix, where Wij is the amount of xt+1 i which will be made up of client j’s local model after one local gradient step (e.g. if Wij = 1/2, then half of xt+1 i will have been composed of client j’s model after its local gradient step). The weighting matrix only has a zero value Wij = 0 if clients i and j are not connected (they are not within the same neighborhood). The values of Wij are generally selected ahead of time by a central host, with the usual weighting scheme being uniform. In D-SGD, model communication occurs only after all local gradient updates are finished. These gradient updates are computed in parallel.\n\nPeriodic Averaging SGD (PA-SGD) (Wang & Joshi, 2018) The Periodic Averaging SGD algorithm is an extension of D-SGD. In order to save communication costs when the number of clients grows to be large, PA-SGD performs model averaging after an additional I1 local gradient steps. Thus, the communication set for PA-SGD is defined as:\n\nCI1 = {t ∈ N| t mod (I1 + 1) = 0}. The special case of I1 = 0 reduces to D-SGD. The PA-SGD process for a single client i is defined as:\n\nxt+1\n\ni =\n\n(cid:40)(cid:80)w\n\nj=1 Wij i − g(xt\n\n(cid:2)xt j − g(xt i, ξt i ),\n\nxt\n\nj, ξt\n\nj)(cid:3),\n\nt ∈ CI1 otherwise.\n\n(15)\n\nCompared with D-SGD, PA-SGD still suffers from the inefficiency of having to wait for the slowest client for each update. However, PA-SGD saves communication costs by reducing the frequency of communication.\n\nLocal Decentralized SGD (LD-SGD) (Li et al., 2019) Continuing to generalize the foundational decentralized Federated Learning algorithms is Local Decentralized SGD. LD-SGD generalizes PA-SGD by allowing multiple chunks of singular D-SGD updates, as described in Equation 14, in between the increased local gradient steps seen in PA-SGD. The number of D-SGD chunks is dictated by a new parameter I2.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 9: I1, I2 Depiction (from (Li et al., 2019)).\n\nIn this case, the communication set for LD-SGD is defined as\n\nCI1,I2 =\n\n(cid:40)(cid:83) (cid:80)I1+I2 {t ∈ N}\n\ni=I1\n\n{t ∈ N| t mod (i + 1) = 0}\n\nif I1 > 0, if I1 = 0.\n\nFor example, in the case I1 = 3, I2 = 2, LD-SGD will take three local gradient steps and then perform two D-SGD updates (which consists of a local gradient step and then averaging) as shown in Figure 9. The special case of I2 = 1 reduces to PA-SGD. The LD-SGD process for a single client i is defined as:\n\nxt+1\n\ni =\n\n(cid:40)\n\nPerform (cid:80)w i, ξt i − g(xt xt\n\nj=1 Wij i ),\n\n(cid:2)xt\n\nj − g(xt\n\nj, ξt\n\nj)(cid:3),\n\nt ∈ CI1,I2 otherwise\n\n(16)\n\nIn the literature, there also exist other asynchronous decentralized learning methods such as in Cao & Ba ̧sar (2020); Bedi et al. (2019a;b), but they are limited to convex objectives and hence not applicable to the setting in our work.\n\nE PROOF OF THE MAIN THEOREM\n\nBefore beginning, we quickly define the expected gradient Eit n\n(cid:88)\n\n ̄G(X t, ξt) := Eit\n\n(cid:2)G(xt\n\nit, ξt\n\nit )(cid:3) =\n\n(cid:2)G(xt\n\nit, ξit\n\n∗,t)(cid:3) as\n\npiG(xt\n\nit, ξt\n\nit).\n\ni=1\n\n(17)\n\nProof of Theorem 1. In this theorem, we characterize the convergence of the average of all local models. Using the Gradient Lipschitz assumption with Equation 9 yields (cid:28) − ̄W t)1n n\n\n(cid:16) X t+11n n\n\n(cid:16) X t1n n\n\nit, ξit n\n\nX t(W t it\n\n∗,t)1n\n\nG(xt\n\n− γ\n\n∇f\n\n≤f\n\n+\n\n(cid:29)\n\n(cid:17)\n\n(cid:17)\n\n(cid:17)\n\nf\n\n,\n\nX t(W t it\n\nit, ξit n\nWe first denote the average over all local models as ̄xt := X t1n to the updating client it yields\n\n− ̄W t)1n n\n\nG(xt\n\n− γ\n\n+\n\n∗,t)1n\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n.\n\n(18)\n\nn . Taking the expectation with respect\n\n(cid:16) X t1n n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nL 2\n\nf ( ̄xt+1) ≤f ( ̄xt) +\n\n+\n\nL 2\n\nEit\n\n=f ( ̄xt) +\n\n(cid:28)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:28)\n\n∇f ( ̄xt), X t( ̄W t − ̄W t)\n\n1n n\n\n− γ ̄G(X t, ξ∗,t)\n\n(cid:29)\n\n1n n\n\nX t(W t it\n\n− ̄W t)\n\n1n n\n\n− γG(xt\n\n∗,t)\n\nit, ξit (cid:29)\n\n∇f ( ̄xt), −γ ̄G(X t, ξ∗,t)\n\n1n n\n\n+\n\nL 2\n\nEit\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nX t(W t it\n\n− ̄W t)\n\n1n n\n\n18\n\n− γG(xt\n\nit, ξit\n\n∗,t)\n\n1n n\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n1n n\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n(19)\n\n(20)\n\nPublished as a conference paper at ICLR 2023\n\n=f ( ̄xt) +\n\n(cid:28)\n\n∇f ( ̄xt), −\n\nγ M n\n\nn (cid:88)\n\nM (cid:88)\n\ni=1\n\nm=1\n\n(cid:29)\n\npi∇l(xt\n\ni, ξi\n\nm,t)\n\n+\n\nL 2\n\nEit\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nX t(W t it\n\n− ̄W t)\n\n1n n\n\n− γG(xt\n\nit, ξit\n\n∗,t)\n\n1n n\n\n=f ( ̄xt) − γ\n\n(cid:28)\n\n∇f ( ̄xt),\n\n1 M n\n\nn (cid:88)\n\nM (cid:88)\n\ni=1\n\nm=1\n\npi∇l(xt\n\ni, ξi\n\nm,t)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n(cid:29)\n\n1n +\nn Taking the expectation over all local data Eξ∼Di yields\n\nX t(W t it\n\n− ̄W t)\n\nEit\n\nL 2\n\n− γG(xt\n\nit, ξit\n\n∗,t)\n\n1n n\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n.\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:28)\n\nn (cid:88)\n\n∇f ( ̄xt),\n\npi∇fi(xt i)\n\n(cid:29)\n\nf ( ̄xt+1) − f ( ̄xt) ≤ −\n\n+\n\nγ n\n\nL 2\n\nEξ∼Di,it\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nX t(W t it\n\n− ̄W t)\n\n1n n\n\n− γG(xt\n\nit, ξit\n\n∗,t)\n\n(21)\n\n(22)\n\n(23)\n\n1n n\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n.\n\nBy properties of the inner product\n\nf ( ̄xt+1) − f ( ̄xt) ≤ −\n\n(cid:18)\n\nγ 2n\n\n(cid:13) (cid:13)∇f ( ̄xt)\n\n(cid:13) 2\n(cid:13)\n\n+\n\nn (cid:88)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npi∇fi(xt i)\n\n∇f ( ̄xt) −\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n−\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\npi∇fi(xt i)\n\n2 (cid:19)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\nL 2\n\nEξ∼Di,it\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nX t(W t it\n\ni=1\n\n− ̄W t)\n\n1n n\n\n− γG(xt\n\nit, ξit\n\n∗,t)\n\ni=1 (cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n.\n\n1n n\n\n(cid:125) Bounding Term A: Given the update equation, term A can be transformed into\n\n(cid:123)(cid:122) :=A\n\n(cid:124)\n\nEξ∼Di,it\n\nX t(W t it\n\n− ̄W t) − γG(xt\n\nit, ξit\n\n∗,t)\n\n= Eξ∼Di,it\n\nX t+1 − X t ̄W t\n\n(cid:19) 1n n\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n(cid:18)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:18)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nDue to the symmetric and doubly stochastic property of ̄W t this reduces to (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) ̄xt+1 − ̄xt(cid:13) (cid:13) 2\n(cid:13)\n\n= Eξ∼Di,it\n\n= Eξ∼Di,it\n\ni − xt\n\nEξ∼Di,it\n\n(cid:0)xt+1\n\nn (cid:88)\n\n1 n\n\n(cid:1)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\ni=1\n\n(cid:0)xt+1\n\nit\n\n− xt it\n\n1 n\n\n(cid:1)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n(24)\n\n(cid:19) 1n n\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n. (25)\n\n=\n\n≤\n\n1 n2 3\nn2\n\nEξ∼Di,it\n\nEξ∼Di,it\n\n(cid:13) (cid:13)xt+1 (cid:18)\n\nit\n\n(cid:13) (cid:13)xt+1\n\nit\n\n− xt it\n\n(cid:13) 2\n(cid:13)\n\n− ̄xt+1(cid:13) 2\n(cid:13)\n\n+ (cid:13)\n\n(cid:13) ̄xt − xt\n\nit\n\n(cid:13) 2\n(cid:13)\n\n+ (cid:13)\n\n(cid:13) ̄xt+1 − ̄xt(cid:13) 2\n(cid:13)\n\n(cid:19)\n\n.\n\nCombining like terms yields\n\n(1 −\n\n3\n\nn2 )Eξ∼Di,it\n\n(cid:13) ̄xt+1 − ̄xt(cid:13) (cid:13) (cid:13)\n\n2 ≤\n\n3 n2\n\n(cid:18)\n\nEξ∼Di,it\n\n(cid:13) (cid:13)xt+1\n\nit\n\n− ̄xt+1(cid:13) 2\n(cid:13)\n\n+ (cid:13)\n\n(cid:13) ̄xt − xt\n\nit\n\n(cid:19)\n\n.\n\n(cid:13) 2\n(cid:13)\n\nSince n ≥ 2 (we assume at least 2 devices are running the algorithm) we find the following result\n\nEξ∼Di,it\n\n(cid:13) ̄xt+1 − ̄xt(cid:13) (cid:13) (cid:13)\n\n2 ≤\n\n3 (n2 − 3)\n\nEξ∼Di,it\n\n(cid:13) (cid:13)xt+1\n\nit\n\n− ̄xt+1(cid:13) 2\n(cid:13)\n\n+ (cid:13)\n\n(cid:13) ̄xt − xt\n\nit\n\n(cid:13) 2\n(cid:13)\n\n(cid:19)\n\n=\n\n3 (n2 − 3)\n\nEξ∼Di\n\nn (cid:88)\n\n(cid:18)\n\npi\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\ni\n\n(cid:13) 2\n(cid:13)\n\n+ (cid:13)\n\n(cid:13) ̄xt − xt\n\ni\n\n(cid:19)\n\n(cid:13) 2\n(cid:13)\n\n(30)\n\nThus, we have bounded Term A. Substituting this back into Equation 24 results in\n\n≤ −\n\n(cid:18)\n\nγ 2n\n\n(cid:13) (cid:13)∇f ( ̄xt)(cid:13) 2\n(cid:13)\n\n+\n\n+\n\n3L 2(n2 − 3)\n\nEξ∼Di\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1 (cid:18)\n\npi\n\nn (cid:88)\n\ni=1\n\npi∇fi(xt i)\n\n−\n\n∇f ( ̄xt) −\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\npi∇fi(xt i)\n\n2 (cid:19)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n]\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\ni\n\n(cid:13) 2\n(cid:13)\n\n+ (cid:13)\n\n(cid:13) ̄xt − xt\n\ni\n\n(cid:19)\n\n.\n\n(cid:13) 2\n(cid:13)\n\n(31)\n\nTaking the sum from t = 0 to t = T − 1 yields\n\nf ( ̄xT ) − f ( ̄x0) ≤ −\n\nγ 2n\n\n(cid:18) T −1 (cid:88)\n\nt=0\n\n(cid:13)∇f ( ̄xt)(cid:13) (cid:13) (cid:13)\n\n2 −\n\nT −1 (cid:88)\n\nt=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇f ( ̄xt) −\n\nn (cid:88)\n\ni=1\n\npi∇fi(xt i)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(cid:18)\n\ni=1\n\n19\n\n(32)\n\n(33)\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\n(39)\n\n(40)\n\nPublished as a conference paper at ICLR 2023\n\n+\n\nT −1 (cid:88)\n\nt=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\npi∇fi(xt i)\n\n2 (cid:19)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n3L 2(n2 − 3)\n\nEξ∼Di\n\nT −1 (cid:88)\n\nn (cid:88)\n\n(cid:18)\n\npi\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\ni\n\n(cid:13) 2\n(cid:13)\n\n+\n\n(cid:13) (cid:13) ̄xt − xt\n\ni\n\n(cid:13) 2\n(cid:13)\n\n(cid:19) .\n\nt=0\n\ni=1\n\nUsing the Lipschitz Gradient assumption, the following term is bounded as\n\nT −1 (cid:88)\n\nt=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇f ( ̄xt) −\n\nn (cid:88)\n\ni=1\n\npi∇fi(xt i)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nT −1 (cid:88)\n\nt=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\npi∇fi( ̄xt) −\n\nn (cid:88)\n\ni=1\n\npi∇fi(xt i)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n=\n\n≤\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt=0\n\ni=1\n\np2\n\ni\n\n(cid:13) (cid:13)∇fi( ̄xt) − ∇fi(xt i)\n\n(cid:13) 2\n(cid:13)\n\n≤ L2\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt=0\n\ni=1\n\np2\n\ni\n\n(cid:13) (cid:13) ̄xt − xt\n\ni\n\n(cid:13) 2\n(cid:13)\n\n≤ L2pmax\n\nT −1 (cid:88)\n\nn (cid:88)\n\n(cid:13) (cid:13) ̄xt − xt\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\nt=0\n\ni=1\n\nPlacing this back into Equation 32, and rearranging, yields T −1 (cid:88)\n\nT −1 (cid:88)\n\nf ( ̄xT ) − f ( ̄x0) ≤ −\n\n(cid:13)∇f ( ̄xt)(cid:13) (cid:13) (cid:13)\n\n2 −\n\nγ 2n\n\nt=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\npi∇fi(xt i)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nγ 2n\n\nt=0 (cid:18)\n\npi\n\n+\n\n3L 2(n2 − 3)\n\nEξ∼Di\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt=0\n\ni=1\n\n+\n\nγL2pmax 2n\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt=0\n\ni=1\n\n(cid:13) (cid:13) ̄xt − xt\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\ni\n\n(cid:13) 2\n(cid:13)\n\n+ (cid:13)\n\n(cid:13) ̄xt − xt\n\ni\n\n(cid:19)\n\n(cid:13) 2\n(cid:13)\n\nGiven that ̄x0 = x0\n\ni for all clients i, one can see that\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt=0\n\ni=1\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n=\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n+\n\nn (cid:88)\n\ni=1\n\n(cid:13) (cid:13) ̄x0 − x0\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt=0\n\ni=1\n\nT −1 (cid:88)\n\nn (cid:88)\n\n=\n\nt=0 T −1 (cid:88)\n\ni=1 n\n(cid:88)\n\n≥\n\nt=0\n\ni=1\n\n(cid:13) (cid:13) ̄xt − xt\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n+\n\nn (cid:88)\n\ni=1\n\n(cid:13) (cid:13) ̄xT − xT\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n(cid:13) (cid:13) ̄xt − xt\n\ni\n\n(cid:13) 2\n(cid:13)\n\n.\n\npi\n\nUsing the result of Equation 38 condenses Equation 37 into T −1 (cid:88)\n\nT −1 (cid:88)\n\nf ( ̄xT ) − f ( ̄x0) ≤ −\n\n(cid:13) (cid:13)∇f ( ̄xt)(cid:13) (cid:13)\n\n2 −\n\nγ 2n\n\nt=0\n\nγ 2n\n\nn (cid:88)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npi∇fi(xt i)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nt=0\n\ni=1\n\n(cid:18) 3L\n\n(n2 − 3)\n\n(cid:19)\n\n+\n\nγL2pmax 2n\n\n+\n\nEξ∼Di\n\n(cid:124)\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt=0\n\ni=1\n\nBounding Term B. The recursion of our update rule can be written as\n\nX t+1 = X 0 − γ\n\nt (cid:88)\n\nj=0\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\nq=j+1\n\n ̄W q − γ\n\nt (cid:88)\n\nj=0\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nq=j+1\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n(41)\n\n(cid:123)(cid:122) :=B\n\nt (cid:89)\n\n(cid:125)\n\n(W q\n\niq\n\n− ̄W q)\n\n(42)\n\nThe recursion equation for the expected consensus model ̄xt and expected local model for client i can be computed by multiplying by 1n n and ei respectively 1n n\n\n ̄xt+1 = ̄x0 − γ\n\n− ̄W q)\n\nij , ξij\n\nij , ξij\n\n1n n\n\nG(xj\n\nG(xj\n\n(W q\n\nt (cid:88)\n\nt (cid:88)\n\nt (cid:89)\n\n∗,j)\n\n∗,j)\n\n− γ\n\n(43)\n\niq\n\nj=0\n\nj=0\n\nq=j+1\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nxt+1\n\ni = x0\n\ni − γ\n\nt (cid:88)\n\nj=0\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\nq=j+1\n\n ̄W qei − γ\n\nt (cid:88)\n\nj=0\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n(W q\n\niq\n\n− ̄W q)ei\n\n(44)\n\nq=j+1\n\nUsing the recursive equations above transforms the bound on term B\n\nEξ∼Di\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt=0\n\ni=1\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n− ̄W q)( 1n\n\nn − ei)\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(45)\n\n(46)\n\n= γ2Eξ∼Di\n\n(cid:80)T −1 t=0\n\n(cid:80)n\n\ni=1 pi\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n− (cid:80)t\n\nj=0 G(xj\n\nij , ξij\n\n∗,j)\n\n(cid:18)\n\n1n\n\nn − (cid:81)t\n\nq=j+1\n\n(cid:19)\n\n ̄W qei\n\n− (cid:80)t\n\nj=0 G(xj\n\nij , ξij\n\n∗,j) (cid:81)t\n\nq=j+1(W q\n\niq\n\n≤2γ2Eξ∼Di\n\nT −1 (cid:88)\n\nn (cid:88)\n\n(cid:18)\n\npi\n\nt=0\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nt (cid:88)\n\nj=0\n\nG(xj\n\nij , ξij\n\n∗,j)\n\n(cid:18) 1n n\n\nt (cid:89)\n\n−\n\nq=j+1\n\n ̄W qei\n\n(cid:19)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nt (cid:88)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n(W q\n\niq\n\n− ̄W q)(\n\n+\n\nj=0\n\nq=j+1\n\n1n n\n\n(cid:19)\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n=2γ2Eξ∼Di\n\nT −1 (cid:88)\n\nn (cid:88)\n\n(cid:18) t\n\n(cid:88)\n\npi\n\nt=0\n\ni=1\n\nj=0 (cid:124)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\n(cid:18) 1n n\n\nt (cid:89)\n\n−\n\nq=j+1\n\n ̄W qei\n\n(cid:19)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:125)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\n+\n\nt (cid:88)\n\nj=0 (cid:124)\n\nt (cid:89)\n\n(W q\n\niq\n\n− ̄W q)(\n\nq=j+1\n\n(cid:123)(cid:122) :=B3\n\n(cid:123)(cid:122) :=B1\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:125)\n\nt (cid:88)\n\nt (cid:88)\n\nj=0\n\nj′=j+1\n\n⟨G(xj\n\nij , ξij\n\n∗,j)(cid:0) 1n\n\nn\n\nt (cid:89)\n\n−\n\nq=j+1\n\n ̄W qei\n\n(cid:123)(cid:122) :=B2\n\n(cid:1), G(xj′\n\nij′ , ξ\n\nij′\n\n∗,j′)(cid:0) 1n\n\nn\n\nt (cid:89)\n\n−\n\nq=j′+1\n\n ̄W qei\n\n(cid:1)⟩\n\nt (cid:88)\n\nt (cid:88)\n\n⟨G(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n(W q\n\niq\n\n− ̄W q)(\n\nj=0\n\nj′=j+1\n\nq=j+1\n\n1n n\n\n− ei), G(xj′\n\nij′ , ξ\n\nij′ ∗,j′)\n\n(cid:123)(cid:122) :=B4\n\nBounding Term B1. Using Lemma 2,\n\n(cid:125)\n\n(cid:19)\n\n(47)\n\nt (cid:89)\n\nq=j′+1\n\n(W q\n\niq\n\n− ̄W q)(\n\n1n n\n\n− ei)⟩\n\n(cid:125)\n\n+ 2\n\n(cid:124)\n\n+ 2\n\n(cid:124)\n\nEξ∼Di\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ Eξ∼Di\n\n= Eξ∼Di\n\n≤ Eξ∼Di\n\nt (cid:88)\n\nj=0\n\nt (cid:88)\n\nj=0\n\nt (cid:88)\n\nj=0\n\nG(xj\n\nij , ξij\n\n∗,j)\n\n(cid:18) 1n n\n\nt (cid:89)\n\n−\n\nq=j+1\n\n ̄W qei\n\n(cid:19)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13)G(xj (cid:13)\n\nij , ξij\n\n∗,j)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:18) 1n (cid:13) (cid:13) n\n(cid:13) (cid:13)\n\nt (cid:89)\n\n−\n\n ̄W qei\n\n(cid:19)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇l(xj\n\nij ; ξij\n\nm,j)\n\nq=j+1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n2 (cid:13) (cid:13) (cid:18) 1n (cid:13) (cid:13) n\n(cid:13) (cid:13)\n\nt (cid:89)\n\n−\n\nq=j+1\n\n ̄W qei\n\n(cid:19)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇l(xj\n\nij ; ξij\n\nm,j)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(\n\nn − 1 n\n\n)ρt−j\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 M\n\n1 M\n\nM (cid:88)\n\nm=1\n\nM (cid:88)\n\nm=1\n\nUsing Lemma 4, the equation above becomes\n\n= 2\n\nt (cid:88)\n\nj=0\n\n(cid:18) σ2 M\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n+\n\n2 (cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n(\n\nn − 1 n\n\n)ρt−j\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:19)\n\n)ρt−j\n\n(\n\nn − 1 n\n\n(cid:18) t\n\n(cid:88)\n\n= 2\n\nj=0\n\nσ2 M\n\n(\n\nn − 1 n\n\n)ρt−j +\n\nt (cid:88)\n\nj=0\n\n21\n\n(48)\n\n(49)\n\n(50)\n\n(51)\n\n(52)\n\nPublished as a conference paper at ICLR 2023\n\n≤\n\n2(n − 1)σ2 (1 − ρ)M n\n\n+ 2\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(\n\nn − 1 n\n\n)ρt−j\n\n(53)\n\nTaking the expectation over worker ij yields the desired bound (cid:20) 2(n − 1)σ2 (cid:13) 2\n(cid:13) (cid:13) (1 − ρ)M n\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nt (cid:88)\n\nEij\n\n+ 2\n\nij )\n\nj=0\n\n(cid:21)\n\n)ρt−j\n\n(\n\nn − 1 n\n\n=\n\n2(n − 1)σ2 (1 − ρ)M n\n\n+\n\n2(n − 1) n\n\nt (cid:88)\n\nj=0\n\nEij\n\nBounding Term B2. Using Lemma 2,\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nρt−j\n\n(54)\n\nt (cid:88)\n\nt (cid:88)\n\n2\n\nj=0\n\nj′=j+1\n\n⟨G(xj\n\nij , ξij\n\n∗,j)(cid:0) 1n\n\nn\n\nt (cid:89)\n\n−\n\nq=j+1\n\n ̄W qei\n\n(cid:1), G(xj′\n\nij′ , ξ\n\nij′\n\n∗,j′)(cid:0) 1n\n\nn\n\nt (cid:89)\n\n−\n\n ̄W qei\n\n(cid:1)⟩\n\nij , ξij\n\n∗,j)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:0) 1n\n\nn − (cid:81)t\n\nq=j+1\n\n ̄W qei\n\n(cid:1)(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13)\n\n(cid:13)G(xj′\n\nij′ , ξ\n\nij′ ∗,j′)\n\nq=j′+1 (cid:13) (cid:13) (cid:13)\n\n(cid:0) 1n\n\n(cid:13) (cid:13) (cid:13)\n\nn − (cid:81)t\n\nq=j′+1\n\n ̄W qei\n\n(cid:1)(cid:13) (cid:13) (cid:13) (55)\n\nj=0\n\n(cid:80)t\n\nj′=j+1\n\n= 2 (cid:80)t\n\n(cid:13) (cid:13)G(xj (cid:13) For any αj,j′ > 0 we find (cid:13) (cid:13)G(xj (cid:13)\n\nt (cid:88)\n\nt (cid:88)\n\n(cid:18)\n\n≤2\n\nij , ξij\n\n∗,j)\n\n(cid:13)G(xj′\n\nij′ , ξ\n\nij′ ∗,j′)\n\n2 (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) 2αj,j′\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nj=0\n\nj′=j+1 (cid:13) (cid:13) (cid:13)\n\nαj,j′\n\n+\n\n(cid:0) 1n\n\nn − (cid:81)t\n\nq=j+1\n\n ̄W qei\n\n(cid:0) 1n\n\nn − (cid:81)t\n\nq=j′+1\n\n2 (cid:13) (cid:13) (cid:13)\n\n(cid:1)(cid:13) (cid:13) (cid:13) 2\n\n ̄W qei\n\n(cid:1)(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:19)\n\n(56)\n\nt (cid:88)\n\n(cid:18)\n\n≤\n\nj̸=j′\n\n(cid:13) (cid:13)G(xj (cid:13)\n\nij , ξij\n\n∗,j)\n\n(cid:13)G(xj′\n\nij′ , ξ\n\nij′ ∗,j′)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) 2αj,j′\n\n+\n\nαj,j′ρt−min{j,j′} 2\n\n(\n\nn − 1 n\n\n)2\n\n(cid:19)\n\n, αj,j′ = αj′,j\n\n(57)\n\nBy applying inequality of arithmetic and geometric means to the term in the last step, we can choose αj,j′ > 0 s.t.\n\n≤\n\nn − 1 n\n\nt (cid:88)\n\nj̸=j′\n\n(cid:18) (cid:13) (cid:13)G(xj (cid:13)\n\nij , ξij\n\n∗,j)\n\n(cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13)G(xj′\n\nij′ , ξ\n\nij′ ∗,j′)\n\n(cid:13) (cid:13) (cid:13) ρ\n\nt−min{j,j′} 2\n\n(cid:19)\n\n≤\n\nn − 1 n\n\nt (cid:88)\n\n(cid:18)\n\n(cid:13) (cid:13)G(xj (cid:13)\n\nij , ξij\n\n∗,j)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n2\n\n(cid:13) (cid:13)\n\n(cid:13)G(xj′\n\nij′ , ξ\n\nij′ ∗,j′)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nt−min{j,j′ } 2\n\nρ\n\n(cid:19)\n\nj̸=j′\n\nt (cid:88)\n\nj̸=j′\n\nt (cid:88)\n\nn − 1 n\n\nn − 1 n\n\n=\n\n=\n\n=\n\n(cid:13) (cid:13)G(xj (cid:13)\n\nij , ξij\n\n∗,j)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nt−min{j,j′ } 2\n\nρ\n\nt (cid:88)\n\n(cid:13) (cid:13)G(xj (cid:13)\n\nij , ξij\n\n∗,j)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nt−j 2\n\nρ\n\nj=0\n\nj′=j+1\n\nn − 1 n\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13)G(xj (cid:13)\n\nij , ξij\n\n∗,j)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n2(t − j)ρ\n\nt−j 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Using Lemma 4 (and the expectation Eξ∼Di that was omitted above but is present) yields\n\nn − 1 n\n\n2(t − j)ρ\n\nij ; ξij\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n∇l(xj\n\n1 M\n\nm,j)\n\nM (cid:88)\n\nt (cid:88)\n\nm=1\n\nt−j 2\n\nj=0\n\n=\n\nt (cid:88)\n\n2(t − j)ρ\n\nt−j 2\n\n(cid:18) σ2 M\n\n+\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) (cid:13) (cid:13)\n\n2 (cid:19)\n\n=\n\n≤\n\n2(n − 1) n\n\n4(n − 1) M n(1 −\n\nj=0 √\n\n√\n\nρσ2 ρ)2 +\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n2(t − j)ρ\n\nt−j 2\n\n2(n − 1) n\n\nt (cid:88)\n\nj=0\n\n22\n\n(58)\n\n(59)\n\n(60)\n\n(61)\n\n(62)\n\n(63)\n\n(64)\n\n(65)\n\nPublished as a conference paper at ICLR 2023\n\nTaking the expectation over worker ij yields the desired bound √\n\nEij\n\n(cid:20) 4(n − 1) M n(1 −\n\n√\n\nρσ2 ρ)2 +\n\n2(n − 1) n\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:21)\n\nt−j 2\n\n2(t − j)ρ\n\n=\n\n4(n − 1) M n(1 −\n\n√\n\n√\n\nρσ2 ρ)2 +\n\n2(n − 1) n\n\nt (cid:88)\n\nj=0\n\nEij\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n2(t − j)ρ\n\nt−j 2\n\n(66)\n\nBounding Term B3.\n\nEξ∼Di\n\n=Eξ∼Di\n\nt (cid:88)\n\nj=0\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n=Eξ∼Di\n\n(cid:80)t\n\nj=0\n\n≤2Eξ∼Di\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\n(cid:20)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\n(W q\n\niq\n\n− ̄W q)(\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(W q\n\niq\n\n⊺\n\n− φ1\n\n+ φ1\n\n⊺ − ̄W q)(\n\nt (cid:89)\n\nq=j+1\n\nt (cid:89)\n\nq=j+1\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:81)t\n\nq=j+1(W q\n\niq\n\n− φ1⊺)( 1n\n\nn − ei) − (cid:81)t\n\nq=j+1( ̄W q − φ1⊺)( 1n\n\nn − ei)\n\n(cid:21)(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\nt (cid:89)\n\n(W q\n\niq\n\n⊺\n\n− φ1\n\n)(\n\nq=j+1\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13)G(xj (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\n+ 2Eξ∼Di\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n( ̄W q − φ1\n\n⊺\n\n)(\n\nq=j+1\n\n1n n\n\n− ei)\n\nDue to the structure of φ1⊺, multiplying this matrix by 1n well as the double stochasticity of ̄W , we find\n\nn or ei yields the same result. Using this, as\n\n= 2Eξ∼Di\n\n(cid:80)t\n\nj=0\n\n= 2Eξ∼Di\n\n(cid:80)t\n\nj=0\n\n(cid:20) (cid:13) (cid:13)G(xj (cid:13)\n\nij , ξij\n\n∗,j) (cid:81)t\n\nq=j+1(W q\n\niq\n\n− φ1⊺)( 1n\n\nn − ei)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nij , ξij\n\n∗,j) (cid:81)t\n\nq=j+1\n\n ̄W q( 1n\n\nn − ei)\n\n2 (cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:20) (cid:13) (cid:13)G(xj (cid:13)\n\nij , ξij\n\n∗,j) (cid:81)t\n\nq=j+1(W q\n\niq\n\n− φ1⊺)( 1n\n\nn − ei)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nG(xj\n\nij , ξij\n\n∗,j)(\n\n1n n\n\n−\n\nt (cid:89)\n\nq=j+1\n\n ̄W qei)\n\n(cid:123)(cid:122) =B1\n\n(cid:21)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:125)\n\nUsing the result of Lemma 3, as our communication graph G is uniformly strongly connected and [W t\n\nit]i,i ≥ 1/n by construction, we see that\n\nt (cid:89)\n\n|(cid:2)\n\nq=j+1\n\nW q\n\niq\n\n⊺(cid:3)\n\n− φ1\n\nh,k\n\n| ≤ 4νt−j−1 ∀ h, k.\n\nUsing this result, we find that\n\nt (cid:89)\n\n|(cid:2)\n\nq=j+1\n\n(W q\n\niq\n\n⊺\n\n− φ1\n\n)(\n\n1n n\n\n− ei)(cid:3)\n\nh\n\n| ≤ 8(\n\nn − 1 n\n\n)νt−j−1 ∀ h.\n\nWe can remove the -1 exponent by doubling the constant out front\n\nt (cid:89)\n\n|(cid:2)\n\n(W q\n\niq\n\n⊺\n\n− φ1\n\n)(\n\nq=j+1\n\n1n n\n\n− ei)(cid:3)\n\nh\n\n| ≤ 16(\n\nn − 1 n\n\n)νt−j ∀ h.\n\n(72)\n\n(73)\n\n(74)\n\n∗,j) is all zeros except for one column, the ij-th column, yields\n\nFinally, using the fact that G(xj the desired result\n\nij , ξij\n\n(67)\n\n(68)\n\n(69)\n\n(70)\n\n(71)\n\nEξ∼Di\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n(W q\n\niq\n\n⊺\n\n− φ1\n\n)(\n\nq=j+1\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇l(xj\n\nij , ξij\n\n∗,j)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n.\n\n(75)\n\n≤\n\nt (cid:88)\n\nj=0\n\n256(\n\nn − 1 n\n\n)2ν2(t−j)Eξ∼Di\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 M\n\nM (cid:88)\n\nm=1\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nUtilizing Lemma 4 yields\n\n≤ 256(\n\nn − 1 n\n\nt (cid:88)\n\n)2\n\nν2(t−j)\n\n(cid:18) σ2 M\n\n(cid:13) (cid:13) (cid:13)∇fij (xt\n\nij )\n\n+\n\n2 (cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n.\n\nj=0 By properties of geometric series, and taking the expectation over worker ij, we find\n\n≤\n\n256σ2 (1 − ν2)M\n\n(\n\nn − 1 n\n\n)2 + 256(\n\nn − 1 n\n\n)2\n\nt (cid:88)\n\nj=0\n\nEij\n\n(cid:13) (cid:13) (cid:13)∇fij (xt\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nν2(t−j).\n\nUsing the bound of B1 in the main proof above, we arrive at the final bound of B3\n\nEξ∼Di\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\nq=j+1\n\n(W q\n\niq\n\n− ̄W q)(\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤\n\n512σ2 (1 − ν2)M\n\n(\n\nn − 1 n\n\n)2 +\n\n4(n − 1)σ2 (1 − ρ)M n\n\n+ 512(\n\nn − 1 n\n\n)2\n\nt (cid:88)\n\nj=0\n\nEij\n\n(cid:13) (cid:13) (cid:13)∇fij (xt\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nν2(t−j)\n\n+\n\n4(n − 1) n\n\nt (cid:88)\n\nj=0\n\nEij\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nρt−j\n\n≤\n\n4(n − 1)σ2 M n\n\n+\n\n4(n − 1) n\n\n(cid:18) 1\n\n(1 − ρ) t\n(cid:88)\n\nEij\n\nj=0\n\n(cid:19)\n\n+\n\n128 (1 − ν2)\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) (cid:13) (cid:13)\n\n2 (cid:18)\n\nρt−j + 128ν2(t−j)\n\n(cid:19)\n\nBounding Term B4. Following similar steps as bounding Term B2 we find\n\n2Eξ∼Di\n\n(cid:80)t\n\n(cid:80)t\n\nj=0\n\n∗,j) (cid:81)t\n\nq=j+1(W q\n\niq\n\n− ̄W q)( 1n\n\nn − ei), G(xj′\n\nij′ , ξ\n\nij′\n\n∗,j′) (cid:81)t\n\nq=j′+1(W q\n\niq\n\n(76)\n\n(77)\n\n(78)\n\n(79)\n\nij , ξij j′=j+1⟨G(xj (cid:13) (cid:13)G(xj (cid:13)\n\nt (cid:88)\n\n(cid:18)\n\nt (cid:88)\n\n≤2Eξ∼Di\n\nij , ξij\n\n∗,j) (cid:81)t\n\nq=j+1(W q\n\niq\n\n− ̄W q)( 1n\n\nn − ei)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nj=0\n\nj′=j+1\n\n(cid:13) (cid:13)\n\n(cid:13)G(xj′\n\nij′ , ξ\n\nij′\n\n∗,j′) (cid:81)t\n\nq=j′+1(W q\n\niq\n\n2\n\n− ̄W q)( 1n\n\nn − ei)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:19)\n\n+\n\n≤2 Eξ∼Di\n\nt (cid:88)\n\nj=0\n\n(cid:124)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\n2\n\nt (cid:89)\n\nq=j+1 (cid:123)(cid:122) =B3\n\n(W q\n\niq\n\n− ̄W q)(\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:125)\n\n− ̄W q)( 1n\n\nn − ei)⟩\n\n(80)\n\n(81)\n\nOnce again, we can use the proof of Lemma 1 to bound this result.\n\nFinishing Bound of Term B. Putting all terms together, we find that Term B is bounded above by\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt=0\n\ni=1\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\nT −1 (cid:88)\n\nn (cid:88)\n\n≤2γ2\n\n(cid:18) 2(n − 1)σ2 (1 − ρ)M n\n\npi\n\n+\n\n2(n − 1) n\n\nt (cid:88)\n\nj=0\n\nEij\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nρt−j\n\n2(n − 1) n\n\nt (cid:88)\n\nj=0\n\nEij\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n2(t − j)ρ\n\nt−j 2\n\nt=0\n\ni=1\n\n4(n − 1) M n(1 −\n\n√\n\n√\n\nρσ2 ρ)2 + (cid:18) 1\n\n12(n − 1)σ2 M n\n\n+\n\n(1 − ρ)\n\n12(n − 1) n\n\nt (cid:88)\n\nj=0\n\nEij\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:19)\n\n128 (1 − ν2) 2 (cid:18)\n\n+\n\n+\n\n+\n\nρt−j + 128ν2(t−j)\n\n(cid:19)(cid:19)\n\n(82)\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nSimplifying results in T −1 (cid:88)\n\nn (cid:88)\n\n≤2γ2\n\n(cid:18) 4(n − 1)σ2 M n\n\n(cid:18)\n\n7 2(1 − ρ)\n\n+\n\npi\n\n√\n\nρ √\n\nρ)2 +\n\n384 (1 − ν2)\n\n(cid:19)\n\n(1 −\n\nt=0\n\ni=1\n\n+\n\n4(n − 1) n\n\nt (cid:88)\n\nj=0\n\nEij\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) (cid:13) (cid:13)\n\n2 (cid:18) 7 2\n\nρt−j + (t − j)ρ\n\nt−j\n\n2 + 384ν2(t−j)\n\n(cid:19)(cid:19)\n\n(83)\n\n≤\n\n8γ2σ2T M\n\n(cid:18) n − 1 n\n\n(\n\n7 2(1 − ρ)\n\n+\n\n(cid:124)\n\n√\n\nρ √\n\n(1 − (cid:123)(cid:122) :=ρν\n\nρ)2 +\n\n384 (1 − ν2)\n\n(cid:19)\n\n)\n\n(cid:125)\n\n+\n\n8(n − 1)γ2 n\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt (cid:88)\n\npi\n\nEij\n\nt=0\n\ni=1\n\nj=0\n\nUsing Lemma 5 results in\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) (cid:13) (cid:13)\n\n2 (cid:18) 7 2\n\nρt−j + (t − j)ρ\n\nt−j\n\n2 + 384ν2(t−j)\n\n(cid:19)\n\n(84)\n\n≤\n\n8γ2σ2T ρν M\n\n+\n\n8(n − 1)γ2 n\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt (cid:88)\n\n(cid:18)\n\n2\n\npi\n\nt=0\n\ni=1\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\npi∇fi(xj i )\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+ 12L2\n\nn (cid:88)\n\n(cid:13) (cid:13) ̄xj − xj (cid:13)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\npi\n\n+ 6ζ 2\n\n(cid:19)(cid:18) 7 2\n\nρt−j + (t − j)ρ\n\nt−j\n\n2 + 384ν2(t−j)\n\n(cid:19)\n\n(85)\n\n≤\n\n8γ2σ2T ρν M\n\n+\n\ni=1 16(n − 1)γ2 n\n\nT −1 (cid:88)\n\nt (cid:88)\n\nt=0\n\nj=0\n\n(cid:18) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\npi∇fi(xj i )\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+ 6L2\n\nn (cid:88)\n\ni=1\n\n(cid:13) (cid:13) ̄xj − xj (cid:13)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\npi\n\n+ 3ζ 2\n\n(cid:19)(cid:18) 7 2\n\nρt−j + (t − j)ρ\n\nt−j\n\n2 + 384ν2(t−j)\n\n≤\n\n8γ2σ2T ρν M\n\n+\n\n16(n − 1)γ2 n\n\nT −1 (cid:88)\n\n∞ (cid:88)\n\nj=0\n\nt=j+1\n\n(cid:18) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\npi∇fi(xj i )\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+ 6L2\n\nn (cid:88)\n\ni=1\n\n(cid:13) (cid:13) ̄xj − xj (cid:13)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\npi\n\n+ 3ζ 2\n\n(cid:19)(cid:18) 7 2\n\nρt−j + (t − j)ρ\n\nt−j\n\n2 + 384ν2(t−j)\n\n(cid:19)\n\n(cid:19)\n\n≤\n\n8γ2σ2T ρν M\n\n+\n\n16(n − 1)γ2 n\n\nT −1 (cid:88)\n\nj=0\n\n+ 6L2\n\nn (cid:88)\n\ni=1\n\n(cid:13) (cid:13) ̄xj − xj (cid:13)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\npi\n\n+ 3ζ 2\n\nn (cid:88)\n\n(cid:18) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\ni=1 (cid:19)(cid:18) ∞ (cid:88)\n\nh=0\n\npi∇fi(xj i )\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nρh + hρ\n\nh\n\n2 + 384ν2h\n\n(cid:19)\n\n7 2\n\n≤\n\n8γ2σ2T ρν M\n\n+ 16ρνγ2\n\nT −1 (cid:88)\n\nj=0\n\nUsing Equation 38 ends with\n\n(cid:18) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\npi∇fi(xj i )\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+ 6L2\n\nn (cid:88)\n\ni=1\n\n(cid:13) (cid:13) ̄xj − xj (cid:13)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\npi\n\n(cid:19)\n\n+ 3ζ 2\n\n≤\n\n8γ2σ2T ρν M\n\n+ 48T ρνγ2ζ 2 + 16ρνγ2\n\nT −1 (cid:88)\n\nt=0\n\n+ 96L2ρνγ2\n\nT −1 (cid:88)\n\nn (cid:88)\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\nt=0\n\ni=1\n\nNow subtract the final term on the right hand side from both sides 8γ2σ2T ρν M\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\nT −1 (cid:88)\n\nn (cid:88)\n\n(cid:13) 2\n(cid:13)\n\npi\n\n≤\n\n(cid:18)\n\n(cid:19)\n\ni\n\n1 − 96L2ρνγ2 (cid:125) (cid:123)(cid:122) (cid:124) :=z\n\nt=0\n\ni=1\n\npi∇fi(xt i)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+ 48T ρνγ2ζ 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\npi∇fi(xt i)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+ 16ρνγ2\n\nT −1 (cid:88)\n\nt=0\n\n25\n\n(86)\n\n(87)\n\n(88)\n\n(89)\n\n(90)\n\n(91)\n\nPublished as a conference paper at ICLR 2023\n\nBy Lemma 6, we can divide z from both sides\n\nT −1 (cid:88)\n\nn (cid:88)\n\n(cid:13) (cid:13) ̄xt+1 − xt+1\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n≤\n\nt=0\n\ni=1\n\n8γ2σ2T ρν M z\n\n+\n\n48T ρνγ2ζ 2 z\n\n+\n\n16ρνγ2 z\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt=0\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npi∇fi(xt i)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(92)\n\nFinishing Bound of Term A. Substituting the bound of B above into Equation 41 yields\n\nf ( ̄xT ) − f ( ̄x0) ≤ −\n\n+\n\nn (cid:88)\n\nT −1 (cid:88)\n\n(cid:13) (cid:13) γ\n(cid:13) (cid:13) 2n (cid:13) (cid:19)(cid:18) 8γ2σ2T ρν\n\nt=0\n\ni=1\n\npi∇fi(xt i)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) 48T ρνγ2ζ 2 z\n\n+\n\nM z\n\nT −1 (cid:88)\n\nγ 2n (cid:18) 3L\n\nt=0\n\n(n2 − 3) (cid:124)\n\nE (cid:13)\n\n(cid:13)∇f ( ̄xt)(cid:13) (cid:13)\n\n2 −\n\n+\n\nγL2pmax 2n\n\n(cid:123)(cid:122) :=φ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nT −1 (cid:88)\n\nn (cid:88)\n\nt=0\n\ni=1\n\n(cid:125)\n\npi∇fi(xt i)\n\n2 (cid:19)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n16ρνγ2 z\n\nRearranging terms simplifies the inequality above to\n\nf ( ̄xT ) − f ( ̄x0) ≤ −\n\nγ 2n\n\nT −1 (cid:88)\n\nt=0\n\nE (cid:13)\n\n(cid:13)∇f ( ̄xt)(cid:13) 2\n(cid:13)\n\n+\n\nγ 2n\n\n(cid:18) 32ρνφγn z\n\n(cid:19) T −1 (cid:88)\n\n− 1\n\nt=0\n\n+\n\n8φγ2σ2T ρν M z\n\n+\n\n48φT ρνγ2ζ 2 z\n\nFrom Lemma 8, we find that (1 − 32ρν φγn above can be removed.\n\nz\n\n) ≥ 0. Therefore, the second term of the right hand side\n\npi∇fi(xt i)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(93)\n\n(94)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\nf ( ̄xT ) − f ( ̄x0) ≤ −\n\nγ 2n\n\nE (cid:13)\n\n(cid:13)∇f ( ̄xt)(cid:13) 2\n(cid:13)\n\n+\n\n8φγ2σ2T ρν M z\n\n+\n\n48φT ρνγ2ζ 2 z\n\nT −1 (cid:88)\n\nt=0\n\nRearranging the inequality above and dividing by T yields\n\n1 T\n\nT −1 (cid:88)\n\nt=0\n\nE (cid:13)\n\n(cid:13)∇f ( ̄xt)\n\n(cid:13) (cid:13)\n\n2 ≤\n\n=\n\n2n(cid:0)f ( ̄x0) − f ( ̄xT )(cid:1) T γ 2n(cid:0)f ( ̄x0) − f ( ̄xT )(cid:1) T γ\n\n+\n\n+\n\n16nφγσ2ρν M z\n\n+\n\n96nφρνγζ 2 z\n\n16γφnρν z\n\n(cid:0) σ2 M\n\n+ 6ζ 2(cid:1)\n\nFrom Lemmas 6 and 7, the inequality above becomes\n\nT −1 (cid:88)\n\nE (cid:13)\n\n(cid:13)∇f ( ̄xt)\n\n(cid:13) (cid:13)\n\n2 ≤\n\n1 T\n\nt=0\n\n2n(cid:0)f ( ̄x0) − f ( ̄xT )(cid:1) T γ\n\n+\n\n1921\n\n10 Lγρν n\n\n(cid:0) σ2 M\n\n+ 6ζ 2(cid:1)\n\nSubstituting in the defined step-size γ (as well as its bound) yields\n\n1 T\n\nT −1 (cid:88)\n\nt=0\n\nE (cid:13)\n\n(cid:13)∇f ( ̄xt)(cid:13) (cid:13)\n\n2 ≤\n\n=\n\n(cid:18)\n\n2n(cid:0)f ( ̄x0) − f ( ̄xT )(cid:1) T\n(cid:18) (cid:112)M n2∆f √\n\n1921\n\n+\n\n10 L n\n2(cid:112)∆f T\n\n+\n\nT L 2(cid:112)L∆f √\nT M\n\n+\n\n(cid:19)\n\n√\n\n√\n\nT L + M\n(cid:112)M n2∆f (cid:19) (cid:0) σ2 M\n(cid:112)L∆f ρν √\n\n1921 10\n\nρν\n\n+ 6ζ 2(cid:1)\n\n√\n\nM (cid:1)\n\n(cid:0)σ2 + 6ζ 2 T M\n\n(95)\n\n(96)\n\n(97)\n\n(98)\n\n(99)\n\n(100)\n\n(101)\n\nThe final desired result is shown as\n\n1 T\n\nT −1 (cid:88)\n\nt=0\n\nE (cid:13)\n\n(cid:13)∇f ( ̄xt)(cid:13) (cid:13)\n\n2 ≤\n\n(cid:0)σ2 + 6ζ 2\n\n(cid:19)\n\n√\n\nM (cid:1)\n\n1 + 1921 20 ρν √\n\nT M\n\n(cid:18)\n\n2(cid:112)L∆f\n\n2(cid:112)∆f T\n\n+\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nF ADDITIONAL LEMMAS\n\nLemma 2 (From Lemma 3 in Lian et al. 2018 (Lian et al., 2018)). Let W t be a symmetric doubly stochastic matrix for each iteration t. Then\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1n n\n\n−\n\nT (cid:89)\n\nt=1\n\nW tei\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤\n\nn − 1 n\n\nρT , ∀T ≥ 0.\n\nLemma 3 (From Corollary 2 in Nedic and Olshevsky 2014 (Nedi ́c & Olshevsky, 2014)). Let the communication graph G be uniformly strongly connected (otherwise known as B-strongly-connected for some integer B > 0), and A(t) ∈ Rn×n be a column stochastic matrix with [A(t)]i,i ≥ 1/n ∀i, t. Define the product of matrices A(t) through A(s) (for t ≥ s ≥ 0) as A(t : s) := A(t) . . . A(s). Then, there exists a stochastic vector φ(t) ∈ Rn such that\n\n|[A(t : s)]i,j − φi(t)| ≤ Cνt−s\n\nwill always hold for the following values of C and ν,\n\nC = 4,\n\nν = (1 − 1/nnB)1/B < 1.\n\nLemma 4. Under Assumption 1, the following inequality holds\n\nEξ∼Di\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 M\n\nM (cid:88)\n\nm=1\n\n∇l(xt\n\ni, ξi\n\nm,t)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤\n\nσ2 M\n\n+ (cid:13)\n\n(cid:13)∇fi(xt\n\ni)(cid:13)\n\n2 (cid:13)\n\n.\n\nLemma 5. Under Assumption 1, the following inequality holds n\n(cid:88)\n\nn (cid:88)\n\nEi\n\n(cid:13) (cid:13)∇fi(xt\n\ni)(cid:13)\n\n(cid:13)\n\n2 ≤ 2\n\npi∇fi(xt i)\n\n+ 12L2\n\n(cid:13) (cid:13) ̄xt − xt\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n+ 6ζ 2.\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\ni=1\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ni=1\n\nLemma 6. Given the defined step-size γ and total iterations T in Theorem 1, the term z is and bounded by\n\n1 > z := 1 − 96L2ρνγ2 ≥ 1 −\n\n384 1932(775)\n\n.\n\nLemma 7. Given the defined step-size γ and total iterations T in Theorem 1, the term φ is and bounded by\n\nφ :=\n\n3L (n2 − 3)\n\n+\n\nL2γpmax 2n\n\n≤\n\nL\n\nn2 (12 +\n\n2 775(193)\n\n).\n\nLemma 8. Given the defined step-size γ and total iterations T in Theorem 1, we find the following bound\n\n(cid:0)1 −\n\n32ρνφγn z\n\n(cid:1) ≥ 0\n\nG LEMMA PROOFS\n\nProof of Lemma 1. These steps are shown in the Main Theorem proof. Due to the symetry and double stochasticity of ̄W we find\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n(W q\n\niq\n\n− ̄W q)(\n\nq=j+1\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n(W q\n\niq\n\n⊺\n\n− φ1\n\n+ φ1\n\n⊺ − ̄W q)(\n\nq=j+1\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\n(cid:20)\n\n(cid:81)t\n\nq=j+1(W q\n\niq\n\n− φ1⊺)( 1n\n\nn − ei) − (cid:81)t\n\nq=j+1( ̄W q − φ1⊺)( 1n\n\nn − ei)\n\n(cid:21)(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n(102)\n\n(103)\n\n(104)\n\nEξ∼Di\n\n=Eξ∼Di\n\nt (cid:88)\n\nj=0\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n=Eξ∼Di\n\n(cid:80)t\n\nj=0\n\n≤2Eξ∼Di\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n(W q\n\niq\n\n− φ1\n\n⊺\n\n)(\n\nq=j+1\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n27\n\nPublished as a conference paper at ICLR 2023\n\n+ 2Eξ∼Di\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n( ̄W q − φ1\n\n⊺\n\n)(\n\nq=j+1\n\n1n n\n\n− ei)\n\nDue to the structure of φ1⊺, multiplying this matrix by 1n well as the double stochasticity of ̄W , we find (cid:20) (cid:13) (cid:13)G(xj (cid:13)\n\nq=j+1(W q\n\n= 2Eξ∼Di\n\n− φ1⊺)( 1n\n\n∗,j) (cid:81)t\n\nn − ei)\n\nij , ξij\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:80)t\n\nj=0\n\niq\n\n+\n\n= 2Eξ∼Di\n\n(cid:80)t\n\nj=0\n\n(cid:20) (cid:13) (cid:13)G(xj (cid:13)\n\nij , ξij\n\n∗,j) (cid:81)t\n\nq=j+1(W q\n\niq\n\n− φ1⊺)( 1n\n\nn − ei)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nG(xj\n\nij , ξij\n\n∗,j)(\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13)G(xj (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\n(105)\n\nn or ei yields the same result. Using this, as\n\nij , ξij\n\n∗,j) (cid:81)t\n\nq=j+1\n\n ̄W q( 1n\n\nn − ei)\n\n2 (cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\n(106)\n\n1n n\n\n−\n\nt (cid:89)\n\nq=j+1\n\n(cid:123)(cid:122) =B1\n\n(cid:21)\n\n(107)\n\n ̄W qei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:125)\n\nUsing the result of Lemma 3, as our communication graph G is uniformly strongly connected and [W t\n\nit]i,i ≥ 1/n by construction, we see that\n\nt (cid:89)\n\n|(cid:2)\n\nq=j+1\n\nW q\n\niq\n\n⊺(cid:3)\n\n− φ1\n\nh,k\n\n| ≤ 4νt−j−1 ∀ h, k.\n\nUsing this result, we find that\n\nt (cid:89)\n\n|(cid:2)\n\nq=j+1\n\n(W q\n\niq\n\n⊺\n\n− φ1\n\n)(\n\n1n n\n\n− ei)(cid:3)\n\nh\n\n| ≤ 8(\n\nn − 1 n\n\n)νt−j−1 ∀ h.\n\nWe can remove the -1 exponent by doubling the constant out front\n\nt (cid:89)\n\n|(cid:2)\n\n(W q\n\niq\n\n⊺\n\n− φ1\n\n)(\n\nq=j+1\n\n1n n\n\n− ei)(cid:3)\n\nh\n\n| ≤ 16(\n\nn − 1 n\n\n)νt−j ∀ h.\n\n(108)\n\n(109)\n\n(110)\n\n∗,j) is all zeros except for one column, the ij-th column, yields\n\nFinally, using the fact that G(xj the desired result\n\nij , ξij\n\nEξ∼Di\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n(W q\n\niq\n\n⊺\n\n− φ1\n\n)(\n\nq=j+1\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nt (cid:88)\n\n≤\n\n256(\n\nj=0 Utilizing Lemma 4 yields\n\nn − 1 n\n\n)2ν2(t−j)Eξ∼Di\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 M\n\nM (cid:88)\n\nm=1\n\n∇l(xj\n\nij , ξij\n\n∗,j)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n.\n\n≤ 256(\n\nn − 1 n\n\nt (cid:88)\n\n)2\n\nν2(t−j)\n\n(cid:18) σ2 M\n\n(cid:13) (cid:13) (cid:13)∇fij (xt\n\nij )\n\n+\n\n2 (cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n.\n\nj=0 By properties of geometric series, and taking the expectation over worker ij, we find\n\n≤\n\n256σ2 (1 − ν2)M\n\n(\n\nn − 1 n\n\n)2 + 256(\n\nn − 1 n\n\n)2\n\nt (cid:88)\n\nj=0\n\nEij\n\n(cid:13) (cid:13) (cid:13)∇fij (xt\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nν2(t−j).\n\nUsing the bound Equation 54 (term B1) in the main proof above, we arrive at the final bound\n\nEξ∼Di\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\n(W q\n\niq\n\n− ̄W q)(\n\nq=j+1\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤\n\n512σ2 (1 − ν2)M\n\n(\n\nn − 1 n\n\n)2 +\n\n4(n − 1)σ2 (1 − ρ)M n\n\n+ 512(\n\nn − 1 n\n\n)2\n\nt (cid:88)\n\nj=0\n\nEij\n\n+\n\n4(n − 1) n\n\nt (cid:88)\n\nj=0\n\nEij\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nρt−j\n\n(cid:13) (cid:13) (cid:13)∇fij (xt\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nν2(t−j)\n\n≤\n\n4(n − 1)σ2 M n\n\n+\n\n4(n − 1) n\n\n(cid:18) 1\n\n(1 − ρ) t\n(cid:88)\n\nEij\n\n(cid:19)\n\n+\n\n128 (1 − ν2)\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) (cid:13) (cid:13)\n\n2 (cid:18)\n\nρt−j + 128ν2(t−j)\n\n(cid:19)\n\nj=0\n\n28\n\n(111)\n\n(112)\n\n(113)\n\n(114)\n\n(115)\n\nPublished as a conference paper at ICLR 2023\n\nThus, we have our desired result\n\nt (cid:88)\n\nE\n\nj=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nG(xj\n\nij , ξij\n\n∗,j)\n\nt (cid:89)\n\nq=j+1\n\n(W q\n\niq\n\n− ̄W q)(\n\n1n n\n\n− ei)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ O(\n\nσ2 M\n\n+ E\n\nt (cid:88)\n\nj=0\n\n(cid:13) (cid:13)∇fij (xj (cid:13)\n\nij )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n).\n\n(116)\n\n∇l(xt\n\ni, ξi\n\nm,t)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nProof of Lemma 4.\n\nEξ∼Di\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 M\n\n=\n\n1 M 2\n\nEξ∼Di\n\n=\n\n1 M 2\n\nEξ∼Di\n\nM (cid:88)\n\nM (cid:88)\n\nm=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nM (cid:88)\n\nm=1\n\nm=1\n\n∇l(xt\n\ni, ξi\n\nm,t) − ∇fi(xt\n\ni) + ∇fi(xt i)\n\n∇l(xt\n\ni, ξi\n\nm,t) − ∇fi(xt i)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+ Eξ∼Di\n\nM (cid:88)\n\nm=1\n\n∇fi(xt i)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n2 M 2\n\nEξ∼Di\n\nM (cid:88)\n\n(cid:10)\n\nm=1\n\n(cid:0)∇l(xt\n\ni, ξi\n\nm,t) − ∇fi(xt\n\ni)(cid:1),\n\nM (cid:88)\n\nm=1\n\n∇fi(xt\n\ni)(cid:11)\n\n=\n\n1 M 2\n\nM (cid:88)\n\nm=1\n\nEξ∼Di\n\n(cid:13) (cid:13)∇l(xt\n\ni, ξi\n\nm,t) − ∇fi(xt i)\n\n(cid:13) + M 2 (cid:13) (cid:13)\n\n(cid:13)∇fi(xt i)\n\n(cid:13) 2\n(cid:13)\n\n≤\n\nσ2 M\n\n+\n\n(cid:13) (cid:13)∇fi(xt i)\n\n(cid:13) 2\n(cid:13)\n\nProof of Lemma 5.\n\nEi\n\n(cid:13) (cid:13)∇fi(xt\n\ni)(cid:13)\n\n2 (cid:13)\n\n= Ei\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇fi(xt\n\ni) −\n\nn (cid:88)\n\ni′=1\n\npi′∇fi(xt\n\ni′) +\n\nn (cid:88)\n\npi′∇fi(xt\n\ni′)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:18)\n\n≤ 2Ei\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇fi(xt\n\ni) −\n\nn (cid:88)\n\nj=1\n\npj∇fj(xt\n\nj)\n\ni′=1 (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\nj=1\n\npj∇fj(xt\n\nj)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:19)\n\nThe first term on the right hand side can be bounded by\n\nEi\n\n∇fi(xt\n\ni) −\n\npj∇fj(xt\n\nj)\n\nn (cid:88)\n\nj=1\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n=3Ei\n\n(cid:18) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇fi(xt\n\ni) − ∇fi(\n\nX t1n n\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n)\n\n+\n\n+\n\n∇fi(\n\nX t1n n\n\n) −\n\nn (cid:88)\n\nj=1\n\npj∇fj(\n\nX t1n n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:18)\n\nn (cid:88)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:19)\n\nj=1 (cid:13) 2\n(cid:13) (cid:13) )\n(cid:13) (cid:13) (cid:13)\n\npj∇fj(\n\nX t1n n\n\n) −\n\nn (cid:88)\n\nj=1\n\npj∇fj(xt\n\nj)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤3Ei\n\nL2 (cid:13)\n\n(cid:13)xt\n\ni − ̄xt(cid:13)\n\n2 (cid:13)\n\n(cid:18)\n\n≤3Ei\n\nL2 (cid:13)\n\n(cid:13)xt\n\ni − ̄xt(cid:13)\n\n2 (cid:13)\n\n(cid:13) (cid:13)∇fj( ̄xt) − ∇fj(xt j)\n\n(cid:13) 2\n(cid:13)\n\npj\n\n(cid:13) (cid:13)∇fi( ̄xt) − ∇f ( ̄xt)\n\n(cid:13) 2\n(cid:13)\n\n+\n\n(cid:19)\n\n+\n\nn (cid:88)\n\nj=1\n\n+ L2\n\nn (cid:88)\n\nj=1\n\n(cid:13) (cid:13) ̄xt − xt\n\nj\n\n(cid:13) 2\n(cid:13)\n\npj\n\n(cid:19)\n\n+ 3ζ 2\n\n=6L2\n\nn (cid:88)\n\ni=1\n\n(cid:13) (cid:13) ̄xt − xt\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n+ 3ζ 2\n\n29\n\n(117)\n\n(118)\n\n(119)\n\n(120)\n\n(121)\n\n(122)\n\n(123)\n\n(124)\n\n(125)\n\n(126)\n\n(127)\n\n(129)\n\n(130)\n\n(131)\n\n(132)\n\n(133)\n\n(134)\n\n(135)\n\n(136)\n\nPublished as a conference paper at ICLR 2023\n\nCombining all terms yields the final result\n\nEi\n\n(cid:13) (cid:13)∇fi(xt i)\n\n(cid:13) (cid:13)\n\n2 ≤ 2\n\npi∇fi(xt i)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+ 12L2\n\nn (cid:88)\n\ni=1\n\n(cid:13) (cid:13) ̄xt − xt\n\ni\n\n(cid:13) 2\n(cid:13)\n\npi\n\n+ 6ζ 2\n\n(128)\n\nProof of Lemma 6. It is trivial to see that z < 1. We now determine the lower bound of z given n ≥ 2, pmax ≥ 1/n, and ρν ≥ 775/4\n\nz = 1 − 96L2ρνγ2 = 1 − 96L2ρν(\n\nM n2∆f T L\n\n) = 1 − 96L2ρν(\n\n1 νn2p2 1932L2ρ2\n\nmax\n\n)\n\n= 1 −\n\n96\n\n1932ρνn2p2\n\nmax\n\n≤ 1 −\n\n384 1932(775)\n\nProof of Lemma 7. Given n ≥ 2 and ρν ≥ 775/4, and the definition of γ and T , one can see\n\nφ = (\n\n3L (n2 − 3)\n\n+\n\nL2γpmax 2n\n\n) =\n\n≤\n\n≤\n\n=\n\nL\n\nn2 (\n\nL\n\nn2 (\n\nL\n\n+\n\n3 (1 − 3/n2) 3\n(1 − 3/n2) 2\n775(193)\n\n+\n\nLnpmax 2\nLnpmax 2\n\n)\n\nn2 (12 +\n\nL\n\n3 (1 − 3/n2)\n\nn2 ( (cid:112)M n2∆f T L 1\n193Lρνnpmax\n\n√\n\n))\n\n(\n\n(\n\n+\n\nLγnpmax 2\n\n)\n\n)) ≤\n\nL\n\nn2 (\n\n3 (1 − 3/4)\n\n+\n\n1 386ρν\n\n)\n\nProof of Lemma 8. Given n ≥ 2, pmax ≥ 1/n Lemma 6, and Lemma 7, one can see\n\n1 − 1 −\n\n32ρνφγn z\n\n= 1 −\n\n32(12 +\n\n≥ 1 −\n\n32ρνφn z\n2 775(193) )\n\n(\n\n1 193Lρνnpmax\n\n) = 1 −\n\n32nφ 193Lz\n\n≥ 1 −\n\n16(12 +\n\n2\n\n775(193) ) 1932(775) )\n\n≥ 0\n\n193(1 − 384\n\n193zn\n\n30",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a new asynchronous decentralized training algorithm with local updates, named SWIFT. Compared to previous works in this direction, the authors removed the bounded delay assumption in analysis, and hence, obtained better theoretical results. In order to ensure the convergence, a new technique is introduced to allow non-symmetric non-doubly stochastic mixing matrices. At the end, experiments on CIFAR10 dataset with a cluster of machines validate the superiority of the proposed algorithm.\n\n# Strength And Weaknesses\n\n## Strength\n1. The authors removed the bounded delay assumption in nearly all previous works when analyzing asynchronous decentralized training algorithms.\n1. In order to ensure convergence, several new techniques are introduced to deal with non-symmetric and non-doubly stochastic mixing matrices.\n1. The authors are able to run experiments on a real cluster of machines, instead of just simulating runtimes on a single machine. So the experimental results are more convincing.\n\n## Weakness\n1. Asynchronous decentralized training is already a mature technique. As the authors mentioned in the paper, several papers already proposed, such as AD-PSGD, and [1]. After reading the paper, it is unclear why do we need a new asynchronous algorithm. I understand that it is an important contribution to remove the bounded delay assumption. But it should be clear whether the algorithmic changes or new analysis techniques enable this theory advancement. The authors are supposed to highlight the insights on how and why. \n2. The authors mentioned several places (e.g. in the abstract) that SWIFT reduces the communication time per round. But this statement is not accurate. As an asynchronous algorithm, SWIFT does not change the communication time but just reduce/remove the idle time in waiting for slow workers. I suggest the authors to refine the related statements.\n3. I did not get why SWIFT needs to randomly select an active client at each round. If so, there must be some idle time at some clients, as they are not selected but may already finish computation. In practice, the active clients should be determined by the randomness in the client computation time. Once a client finishes local computation, it can trigger a communication and be labeled as the active clients. Server cannot control this process. The authors are supposed to provide clarifications on this.\n4. If the above point is true, then that means the actual running SWIFT is different from what the authors analyze. The authors model the randomness in computing time by randomly selecting active clients. This can be a very rough approximation. The authors should justify why this theoretical approximation make sense and why this assumption is better than bounded delay assumption. Also, it should be made clear in the paper that their analysis is applied to a variant (possibly simplified version) of SWIFT.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nOverall the paper is well-written but I found that some details are unclear. While asynchronous decentralized training is not new, it is interesting to see a new variant, which does not need bounded delay assumption.\n\n# Summary Of The Review\n\nI believe this paper make a good contribution in advancing asynchronous decentralized training algorithm, but some technical details are unclear such that I cannot accurately evaluate its significance.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDIFFERENTIALLY PRIVATE BIAS-TERM ONLY FINE-TUNING OF FOUNDATION MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe study the problem of differentially private (DP) fine-tuning of large pre-trained models — a recent privacy-preserving approach suitable for solving downstream tasks with sensitive data. Existing work has demonstrated that high accuracy is possible under strong privacy constraint, yet requires significant computational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), which matches the state-of-the-art accuracy for DP algorithms and the efficiency of the standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the network architecture), parameter efficient (only training about 0.1% of the parameters), and computation efficient (almost removing the overhead caused by DP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiT is 2 ∼ 30× faster and uses 2 ∼ 8× less memory than DP full fine-tuning, even faster than the standard full fine-tuning. This amazing efficiency enables us to conduct DP finetuning on language and vision tasks with long-sequence texts and high-resolution images, which were computationally difficult using existing methods.\n\n1\n\nINTRODUCTION\n\nFine-tuning from large pre-trained neural networks is one of the most critical technique in deep learning, yielding strong performance in a variety of domains (Pan & Yang, 2009; Kenton & Toutanova, 2019; Goyal et al., 2017). Among different methods, full fine-tuning is the most prevalent one, which trains all the model parameters on the downstream tasks and achieves high accuracy within a small number of training epochs. However, full fine-tuning on large models, from hundreds of millions (He et al., 2016; Chen et al., 2016) to billions of parameters (Brown et al., 2020), can be burdensome in terms of the computation and the deployment, since a full copy of fine-tuned model parameters is needed for each task.\n\nTo alleviate this issue, the parameter efficient fine-tuning only trains a substantially small portion of the model parameters, in contrast to the full fine-tuning. At a high level, the parameter efficient finetuning methods can be divided into two categories. ⟨1⟩ Model-aware methods, meaning a relatively small number of parameters are introduced into the neural network architecture and only the new parameters are optimized. Examples include LoRA (Hu et al., 2021), Adapter (Houlsby et al., 2019), and Compacter (Mahabadi et al., 2021). ⟨2⟩ Model-agnostic methods, meaning that only a subset of existing parameters are trainable. Examples include training only the output linear layer (linear probing, (Kornblith et al., 2019)), only the layer normalization layer (Houlsby et al., 2019) and biasterm fine-tuning (BiTFiT) (Zaken et al., 2022). We illustrate the differences in Equation (1): W0, b0 are the pre-trained weights and biases, ‘ ˆ ’ indicates trainable parameters, and θ is the additional parameters.\n\nf (x; W0, b0) (cid:124) (cid:125) (cid:123)(cid:122) pre-trained model\n\n−→ f (x; ˆW, ˆb) (cid:125) (cid:123)(cid:122) (cid:124) full fine-tuning\n\nor\n\nf (x; W0, b0, ˆθ) (cid:125) (cid:123)(cid:122) (cid:124) model-aware fine-tuning\n\nor\n\nf (x; W0, ˆb) (cid:125) (cid:123)(cid:122) (cid:124) bias-term fine-tuning\n\n(1)\n\nEmpirically, these parameter efficient fine-tuning methods have achieved high accuracy that is comparable to the full fine-tuning in the standard non-private setting. For instance, linear probing of ResNet (He et al., 2016) and Vision Transformer (ViT, (Dosovitskiy et al., 2020)) achieves 80% accuracy on the ImageNet dataset (Sun et al., 2017; Kornblith et al., 2019); LoRA and BiTFiT\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nof RoBERTa (Liu et al., 2019) and BERT (Kenton & Toutanova, 2019) achieve about 94% on SST2, 87% on MNLI, and on average 85% across the General Language Understanding Evaluation (GLUE) datasets (He et al., 2021; Hu et al., 2021). In addition, parameter efficient methods are faster than full fine-tuning and save the communication cost significantly in the distributed learning.\n\nParallel to these developments, the success of deep learning models relies on the availability of large datasets, which may contain sensitive information to be protected rigorously. This privacy issue is well-known for neural networks can be vulnerable to privacy attacks: membership information can be leaked from the purchase records via Google and Amazon online services (Shokri et al., 2017); sensitive texts can be reconstructed by specifically designed prefix on GPT2 (Carlini et al., 2021) and so can images in CIFAR10 and MNIST (Haim et al., 2022). To protect against such privacy risks, the standard technique is differential privacy (DP, formally stated in Definition 2.1), which randomizes the standard optimizers by updating with the private gradient in Equation (2).\n\nA recent line of work has extensively studied the DP fine-tuning in both computer vision and language tasks, often achieving less than 3% accuracy drop across different settings via full fine-tuning (De et al., 2022; Li et al., 2021; Bu et al., 2022b;a), linear probing (Mehta et al., 2022), LoRA, Adapter, or Compacter (Yu et al., 2021a). In fact, fine-tuning or pre-training from large dataset is considered necessary in the DP deep learning literature. As a matter of fact, full fine-tuning DPGPT2 only achieves 24.2 BLEU score (ε = 8) on E2E dataset if randomly initialized (Li et al., 2021), in starking contrast to 63.2 BLEU if pre-trained; similarly, state-of-the-art (SOTA) DP accuracy on ImageNet is 48% (ε = 10) without pre-training (Kurakin et al., 2022) but 86.7% accuracy if pre-trained (De et al., 2022). Specifically, parameter efficient DP fine-tuning has empirically demonstrated strong accuracy (see our Table 4) with 3 ∼ 4× memory saving and 2 ∼ 3× speedup compared to DP full fine-tuning by Opacus (c.f. Figure 3 and Yu et al., 2021a, Table 3). Although previous works have shed light on various DP fine-tuning methods, we are the first to study DP-BiTFiT specifically and to show two distinctive advantages of it.\n\nFigure 1: Performance of different fine-tuning methods on MNLI dataset with RoBERTa-large.\n\nFirstly, DP-BiTFiT is model-agnostic and remains its parameter efficiency around 0.1% across models by Table 1. While linear probing is also model-agnostic, the parameter efficiency can be as high as 8% in ResNet50. Other methods like LoRA, Adapter and Compacter are architecture-dependent and possibly parameter inefficient, making them difficult to directly apply on arbitrary neural networks: LoRA and Adapter may need to train more than 12% on BART-large (Lewis et al., 2020) to achieve high accuracy by He et al. (2021, Figure 1& 4).\n\nSecondly, DP-BiTFiT is computationally efficient, almost as much as the standard BiTFiT and significantly more efficient than DP full fine-tuning, particularly with large models and highdimensional input data. For examples of DP full fine-tuning, Li et al. (2021) have reported 2 ∼ 4× slowdown on large language models for four advanced private codebases and up to 5× memory overhead, compared to the standard fine-tuning; even on small networks, 11 codebases across Tensorflow, JAX, and Pytorch have demonstrated 0.2 ∼ 5× slowdown and 3 ∼ 100× reduction in maximum batch size in Subramani et al. (2021). See more discussion in Section 3.3.\n\nContributions. In this work, we develop DP-BiTFiT, a fine-tuning method that is model-agnostic, accurate, privacy-preserving, parameter efficient, and computationally efficient.\n\n1. Algorithmically, we propose the Differentially Private Bias-Term Fine-Tuning (DP-BiTFiT) in Algorithm 1 that is highly accurate under DP constraint, on par with SOTA in Section 4. Specially, we propose a two-phase training in Section 4.4 to close the accuracy gap between DPBiTFiT and DP full-finetuning.\n\n2\n\n0.000.250.500.751.001.251.501.752.00Fine-tuned parameters (%)87.087.588.088.589.089.590.0Test accuracy (%)full (non-DP)DP full (GhostClip/Opacus)BiTFiT (non-DP)DP-BiTFiTDP LoRADP AdapterDP Compacter0.000.250.500.751.001.251.501.752.00Fine-tuned parameters (%)0255075100125150175200Throughputfull (non-DP)DP full (GhostClip)DP full (Opacus)BiTFiT (non-DP)DP-BiTFiTDP LoRADP AdapterDP Compacter0.000.250.500.751.001.251.501.752.00Fine-tuned parameters (%)152025303540Memory (GB)full (non-DP)DP full (GhostClip)DP full (Opacus)BiTFiT (non-DP)DP-BiTFiTDP LoRADP AdapterDP CompacterUnder review as a conference paper at ICLR 2023\n\n2. DP-BiTFiT is model-agnostic and only optimizes 0.1% of the model parameters on BERT, RoBERTa, GPT2, ViT, ResNet, and so on (see Table 1). Thus DP-BiTFiT is one of the most parameter efficient fine-tuning methods among DP LoRA, Adapter, linear probing, etc.\n\n3. We design a computationally efficient implementation of DP-BiTFiT, whose time and space complexity is almost the same as the standard non-DP BiTFiT, while being faster than non-DP full fine-tuning and other DP fine-tuning (see Figure 1). This advantage is analyzed in Table 2, and demonstrated via the substantial speedup and memory-saving in Figure 3 and Figure 4.\n\n4. DP-BiTFiT is the unique DP algorithm that has a computation overhead independent of the feature dimension T 1. This is due to the activation-free forward pass that only happens in the no-weight training2. Therefore, DP-BiTFiT enjoys a special advantage to work efficiently on long-sequence texts and high-resolution images (see Figure 3).\n\nNovelty. At a glance, our results may appear to be incremental as we are merely adding differential privacy to an existing method (BiTFiT) through a standard mechanism (DP-SGD). This is not true! Computationally, our implementation of DP-BiTFiT involves substantial algorithmic innovation that exploits the special structures in the forward and backward passes of the per-example gradient computation, hence removing the computational and memory overhead in DP-SGD. Statistically, it is quite surprising to us that one can achieve nearly the same accuracy in DP-fine tuning in both vision and language tasks when optimizing only 0.1% of the parameters.\n\nAlgorithm 1 Bias-Term Fine-Tuning (BiTFiT) v.s. DP-BiTFiT Parameters: l-th layer’s bias bl, subsampling probability p, number of iterations T , number of layers L, noise scale σ, clipping threshold R, clipping factor Ci = 1 (no clipping).\n\n1: for iteration t = 1, · · · , T do 2: 3: 4:\n\nSubsample a batch Bt ⊆ {1, . . . , n} from training set with probability p for layer l ∈ L, L − 1, · · · , 1 do Get output gradient ∂L ∂sl\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n10:\n\nCompute per-example gradient and its norm: ∂Li ∂bl\n\n= ∂L ∂sl,i\n\n⊤\n\n1 =⇒ ∥ ∂Li ∂bl\n\n∥2 F\n\nAggregate gradient norms across all layers: ∥ ∂Li\n\n∂b ∥2\n\nF = (cid:80)\n\nl ∥ ∂Li\n\n∂bl\n\n∥2 F\n\nCompute clipping factor: Ci = C(∥ ∂Li Compute sum of clipped gradients G = (cid:80) Add Gaussian noise G = G + σR · N (0, I) Descend on bias terms with the gradient G by SGD/Adam/...\n\n∂b ∥F ; R) i Ci\n\n∂Li ∂b\n\n2 PRELIMINARIES\n\nFine-tuning methods. Fine-tuning, i.e. training a model on a large dataset for a sufficiently long time, and then continuing to train (or transferring) onto the downstream datasets, is the standard paradigm to achieve high accuracy in both the standard and the DP regimes. In DP deep learning, the pre-training takes place on a public dataset using regular optimizers like SGD, and the finetuning takes place on a private dataset which requires privacy protection, using DP optimizers like DP-SGD in Section 2.\n\nIn a long line of research, various fine-tuning methods have been proposed. One of the most popular method is the full fine-tuning, which simply runs gradient descents on all trainable weights and biases, thus can be inefficient when the model is large. To improve the efficiency, Li & Liang (2021) proposes the prefix tuning that only optimizes the prompts or the input layer activation (Lester et al., 2021; Liu et al., 2021). However, as pointed out in Hu et al. (2021) and Li et al. (2021), the prefix tuning can be difficult to optimize and thus sub-optimal on large models. Another approach is to\n\n1As summarized in Table 2 and Table 8, the computation overhead to get the per-sample weight gradient norm is linear (by instantiating per-sample gradints) or quadratic in T (if using the ghost norm trick (Goodfellow, 2015; Li et al., 2021)), for DP full and parameter efficient fine-tuning.\n\n2We distinguish the weight training and bias training in Section 2 using the chain rules. Note that activation-\n\nfree means memory-saving, which is not leveraged by DP full, LoRA, Adapter, Compacter, etc.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nreduce the number of trainable parameters. For example, LoRA (Hu et al., 2021), Adapter (Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2021; R ̈uckl ́e et al., 2021; Lin et al., 2020) and Compacter (Mahabadi et al., 2021) insert small ‘adapter’ layers (usually 1-10% of total parameters) between existing layers, and only the newly added adapters are optimized. We describe the forms of LoRA and Adapter in Appendix C and analyze their complexity.\n\nIn addition to the aforementioned methods, BiTFiT is a special parameter-efficient method that rivals the full fine-tuning (Zaken et al., 2022; Cai et al., 2020; He et al., 2021). Firstly, BiTFiT optimizes a subset of original parameters – the bias terms, which usually constitute less than 1/1000 of all parameters as demonstrated in Table 1. Therefore, BiTFiT can be readily deployed to any network in a model-agnostic manner. Secondly, BiTFiT is fundamentally different to other parameter efficient methods such as LoRA, since the bias gradients are computed differently than the weight gradients on the computation graph. We will elaborate on this in Equation (4).\n\nDeep learning with differential privacy. We recall the classic (ε, δ)-DP, under which we train deep neural networks with provably privacy guarantees.\n\nDefinition 2.1 ((Dwork et al., 2006)). A randomized algorithm M is (ε, δ)-differentially private if, for any two neighboring datasets S, S′ that differ by one datapoint and for any event E, we have P[M (S) ∈ E] ⩽ eεP [M (S′) ∈ E] + δ.\n\nIn deep learning, DP can be achieved through applying an off-the-shelf optimizer (SGD or Adam) with a privately released stochastic gradient in place of the regular (cid:80) i gi. The private stochastic gradient is computed by first getting a minibatch I via Poisson sampling, then compute\n\nPrivate gradient\n\n(cid:88)\n\ni∈I\n\ngi · C(∥gi∥; R) + σR · N (0, I),\n\n(2)\n\nwhere C is any function3 R+ → R subject to C(x) ≤ R/x, gi is the i-th per-sample gradient, R is the clipping threshold, and σ is the noise multiplier. The private gradient is guaranteed to be DP through the sampled-Gaussian mechanism and the associated tight privacy accounting to compose over the iterations (see, e.g., Abadi et al., 2016; Wang et al., 2019; Mironov et al., 2019; Koskela et al., 2020; Bu et al., 2020; Gopi et al., 2021, and the references therein.).\n\nBackward propagation. We briefly introduce the back-propagation, which reveals a simple yet important difference between the gradients of weights and those of biases. We consider a linear layer, indexed as the l-th layer, with weight Wl ∈ Rd×p and bias as bl ∈ Rp. We leave the derivation of other layers such as normalization and convolution in Appendix A. We denote the mini-batched input of this layer as al ∈ RB×T ×d and the immediate output as sl ∈ RB×T ×p, where B is the batch size and T is the feature dimension4: al+1 = φ(sl), sl = alWl + bl. Here φ is any non-parametric inter-layer operation, e.g. the non-linear activation (like ReLU), pooling, padding, and so on. We write L = (cid:80)n i=1 Li as the total loss and Li as the per-sample loss of the i-th sample. During a standard back-propagation of L layers, the chain rule keeps track of the output gradient at each layer in a just-in-time fashion:\n\n∂L ∂sl\n\n=\n\n∂L ∂aL\n\n◦\n\n∂aL ∂sL−1\n\n·\n\n∂sL−1 ∂aL−1\n\n◦ · · ·\n\n∂al+1 ∂sl\n\n=\n\n∂L ∂sl+1\n\nWl+1 ◦ φ′(sl).\n\nThis output gradient ∂L ∂sl\n\nis used to compute per-sample gradient of weights and biases,\n\n⊤\n\n∂Li ∂Wl\n\n(cid:88)\n\n=\n\nj\n\n∂Li ∂sl,j\n\n⊤ ∂sl,j ∂Wl\n\n=\n\n⊤\n\n∂L ∂sl,i\n\nal,i,\n\n⊤\n\n∂Li ∂bl\n\n(cid:88)\n\n=\n\nj\n\n∂Li ∂sl,j\n\n⊤ ∂sl,j ∂bl\n\n=\n\n⊤\n\n1.\n\n∂L ∂sl,i\n\n(3)\n\n(4)\n\nNotably, the weight gradient needs the activation tensor al to compute an expensive O(BT pd) tensor multiplication. Memory-wise, {al}l across all layers is very costly to store (taking more than 95% memory across VGG, ResNet, DenseNet, RoBERTa, etc. by Jain et al. (2020, Figure 3)). In\n\n3Examples of gradient clipping include but not limited to Abadi’s clipping min(R/∥gi∥, 1) (Abadi et al.,\n\n2016) and automatic clipping (AUTO-S) R/(∥gi∥ + 0.01) (Bu et al., 2022b; Yang et al., 2022).\n\n4In sequential data such as text, T is the sequence length; in vision data, T is the product of input dimensions (e.g. for images, T is the product of height and width). We refer to a high-dimensional input when T is large.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nsharp contrast, the computation of bias gradient does not need al, and the multiplication with 1 in Equation (4) is actually a cheap O(BT p) summation on ∂L ∂sl\n\n: B × T × p → B × p.\n\nForward propagation and the hook. During the forward propagation, all Pytorch-based codebases for DP algorithms such as Private Transformers, Opacus, FastGradClip, and others (Yu et al., 2021a; Bu et al., 2022a) register the forward hooks to extract the activation tensors {al}l of all layers from the computation graph, where al is computed and stored. Hence, the majority of memory burden is on the activation that grows extremely large for huge models like GPT3 (Brown et al., 2020) with 175B parameters: the activation tensors consume more than 3600GB of memory while the parameters and gradients only consume 300GB (Rajbhandari et al., 2020). On one hand, this issue can be alleviated by the activation recomputation or checkpointing technique (Chen et al., 2016; Jain et al., 2020), whose memory cost reduces from O(L) to O( L) with an extra 33% slowdown. Alternatively, we note that the activation tensors are not necessary in the forward propagation, if we only optimize the bias terms.\n\n√\n\n3 DIFFERENTIALLY PRIVATE BIAS-TERM FINE-TUNING\n\nWe propose DP-BiTFiT, to privately train only the bias terms in a neural network by combining Equation (4) and Equation (2). We use shaded lines to represent the additional DP operations in Algorithm 1, and add DP-related variables and operations in red in the computation graph by Figure 2.\n\nFigure 2: Back-propagation for DP (red&black) and non-DP (black) algorithms. Left: full finetuning with GhostClip (ghost clipping; (Goodfellow, 2015; Li et al., 2021; Bu et al., 2022a)). Upper right: full fine-tuning with Opacus (Yousefpour et al., 2021). Lower right: BiTFiT.\n\nImplementation-wise, DP-BiTFiT is different from all existing DP algorithms (including full, LoRA, Adapter, etc.) that optimize weights, since it does not apply a Pytorch forward hook to store the activation al for all layers. We provide the implementation details of DP-BiTFiT in Appendix B. To give a concrete example, we apply DP-BiTFiT to the RoBERTa-large model on QQP dataset, following the same setting as Li et al. (2021) and using one 40GB A100 GPU. This is the most timeconsuming text classification task in our work, taking 119 minutes per epoch for a training batch size 20 using the fastest DP full fine-tuning implementation – GhostClip (Li et al., 2021). To conduct a simple ablation study, setting all weights to not require gradients (but forward hooks are still operating) reduces the training time by 50% to to 80 minutes; removing the forward hooks further reduces the training time by 30% to 63 minutes; finally, using the maximum batch size allowed by the memory-saving DP-BiTFiT reduces to 43 minutes.\n\n3.1 PARAMETER EFFICIENCY\n\nDP-BiTFiT enjoys exactly the same parameter efficiency as the standard BiTFiT, training merely about 0.1% of the total parameters in large models. We demonstrate that DP-BiTFiT is one of the most parameter-efficient fine-tuning through a list of models in Table 1, extended in Table 12.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAn advantage of this parameter efficiency is reflected in the computation efficiency, given that most parameters do not require gradients to be computed: we show in Table 2 and Section 3.3 that DP-BiTFiT is much more efficient than full fine-tuning (DP and even non-DP). Additionally, the parameter efficiency also translates to the communication efficiency in the distributed learning. For example, the 64-bit communication cost of DP full fine-tuning is 64M D where M is the number of worker and D is the total number of parameters, which can be improved to 0.064M D by DPBiTFiT.\n\nDataset\n\nImageNet\n\nE2E\n\nGLUE\n\nModel VGG16 ResNet18 ResNet50 ViT-small-patch16 ViT-base-patch16 ViT-large-patch16 GPT2-small GPT2-medium GPT2-large RoBERTa-base RoBERTa-large\n\n# of params % of params\n\n138M 11.7M 25.6M 21.7M 85.8M 303M 124M 355M 774M 125M 355M\n\n0.009 0.043 0.113 0.238 0.120 0.090 0.082 0.076 0.066 0.083 0.077\n\nTable 1: Parameter efficiency of (DP) BiTFiT.\n\n3.2 COMPLEXITY OF WEIGHT AND BIAS TRAINING\n\nWe present in Table 2 the complexity of DP training on weights and biases, for one layer mapping B × Tl × dl to B × Tl × pl. To elaborate on Footnote 4, for text data, Tl is the sequence length, dl is input dimension, and pl is output dimension; for image data and specially in a convolution layer, Tl is height times width, dl is the input channels times kernel sizes, pl is the output channels (c.f. Bu et al., 2022a, Section 2.3). Notice that the total complexity of training a network is summed across l Tlpldl, DP full fine-tuning all layers, e.g. is over 8B (cid:80) l Tlpldl. Therefore, our complexity analysis indicates that DP-BiTFiT is 6/4 = 1.5× faster than non-private full fine-tuning and over 8/4 = 2× faster than DP full fine-tuning.\n\nthe time complexity of standard full training is 6B (cid:80)\n\nl Tlpldl, and DP-BiTFiT is about 4B (cid:80)\n\nforward &output grad\n\nnon-DP Opacus\n\nweight training GhostClip +2BT pd +2BT pd\n\n4BT pd\n\n2BT pd\n\nMixGhostClip +2BT pd\n\n+2BT 2(p + d) +⟨2BT 2(p + d), 2BT pd⟩\n\npd+ BT (p + d)\n\nBT (p + d) +Bpd\n\n+2BT 2\n\n+ min{2BT 2, 2Bpd}\n\n1 ✗\n\n1 ✓\n\n2 ✓\n\n2 ✓\n\nbias training non-DP DP (ours)\n\nBT p +3Bp\n\np\n\n1 ✗\n\n+Bp\n\n1 ✗\n\nTime complexity Space complexity # back-prop forward hook\n\nTable 2: Per-layer time and space complexity of training on weights (full fine-tuning) and biases. ‘+’ means additional overhead to non-DP training, and ‘⟨⟩’ means between two values.\n\nHere, the DP weight training (full fine-tuning) uses three efficient implementations that are equivalent mathematically but have different complexity: Opacus (Yousefpour et al., 2021), GhostClip (Goodfellow, 2015; Li et al., 2021), and MixGhostClip (Bu et al., 2022a). The first two implementations are illustrated in Figure 2, of which MixGhostClip is a hybridization that reduces to GhostClip when T is small. These implementations have been thoroughly analyzed in (Bu et al., 2022a, Appendix C), and we take the complexity result from Bu et al. (2022a, Table 1). For the complexity of bias training in Table 2, it suffices to analyze Line 5 of Algorithm 1. We refer the interested readers to Table 8 for details, where we also apply the complexity analysis of weight training on other methods beyond full fine-tuning, including DP LoRA and DP Adapter.\n\n3.3 SCALABILITY OF DP ALGORITHMS\n\nFrom the complexity analysis in Table 2, we observe that DP training on weights can be memory costly, especially when the models are large and the data is high-dimensional. As an example of the large modelling issue, Li et al. (2021) shows that Opacus cannot fit even a single datapoint into a 16GB GPU using GPT2-large (Radford et al.) with 774M parameters, due to its O(B (cid:80) l pldl) space complexity where the number of parameters is (cid:80) l pldl; for high-dimensional data, GhostClip cannot fit a single 400 × 400 image into the same GPU using ResNet18 with 11.7M parameters, due to its O(B (cid:80) l ) space complexity. Although MixGhostClip (Bu et al., 2022a) significantly alleviates the memory issue in both cases, it does so at a cost of roughly 2× slowdown than the standard full fine-tuning (c.f. Bu et al., 2022a, Figure 4). In sharp contrast, DP-BiTFiT is amazingly scalable since its computational overhead is negligible and independent of T (though the total complexity, mainly due to forward and output gradient, is still linear in T ).\n\nl T 2\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nEfficiency of DP training v.s. feature dimension To empirically evaluate the computation efficiency of DP fine-tuning methods, we measure the time and GPU memory for a fixed batch size. We depict the high-dimensional data issue in Figure 3, in which the memory saving and speedup by DP-BiTFiT is substantial. We expect to observe greater efficiency advantage of DP-BiTFiT on higher dimensional data, e.g. in document-level language tasks with T ≈ 20000 by Beltagy et al. (2020), and in high-resolution image tasks, such as 1024 × 1024 CelebA-HQ (Karras et al., 2018) and Flickr-Faces-HQ (Karras et al., 2019) where T can be of order 105 in the convolution layers.\n\nFigure 3: Memory and speed by different fine-tuning methods. Left two: SST2 dataset (sequence length T ; MixGhostClip is equivalent to GhostClip for this small T ) with RoBERTa-base and batch size 20. Right two: 50000 images of\n\nT pixels with ResNet50 and batch size 200.\n\nT ×\n\n√\n\n√\n\nEfficiency of DP training v.s. model size To stress-test the computation efficiency of DP-BiTFiT with large models, we apply the maximum batch size with respect to each fine-tuning method, instead of using a fixed one across different methods. Therefore, DP-BiTFiT can further leverage its memory efficiency to achieve the best throughput. Here we consider a setting of high-dimensional data (T = 5122) but small ResNet (11.7 ∼ 58.2M parameters) and the other setting of lowdimensional data (T = 100) but large GPT2 (125 ∼ 774M parameters).\n\nFigure 4: Maximum throughput and batch size by different fine-tuning methods. Left two: E2E dataset with GPT2-small/medium/large (MixGhostClip is equivalent to GhostClip for this small T ). Right two: 50000 images of 512 × 512 pixels with ResNet 50/101/152.\n\n4 EXPERIMENTS\n\nWe now test the accuracy of DP-BiTFiT on natural language and computer vision tasks, with the settings in Appendix D. For DP full fine-tuning algorithms, we use GhostClip (Li et al., 2021) on texts, and MixedGhostClip (Bu et al., 2022a) on images, which achieve SOTA efficiency and accuracy on these datasets respectively. We compute ε using a conversion from RDP though tighter privacy accountants in Section 2 are feasible. We illustrate in Table 3 that tuning the learning rate for BiTFiT is not difficult. And we observe in all experiments that, with or without DP, the optimal learning rate for BiTFiT is larger than that for full fine-tuning.\n\n5e-4\n\n1e-3\n\nlearning rate\n\n1e-4 RoBERTa-base 90.94 91.28 91.74 92.43 90.94 91.51 91.97 92.43 91.28 93.92 94.38 94.49 93.35 RoBERTa-large 94.38 95.07 94.38 94.50 94.04 94.84 94.72 94.61 92.66 95.76 96.21 96.21 95.99 Table 3: Test accuracy on SST2 under ε = 8, using DP-Adam with AUTO-S clipping.\n\n5e-3\n\n5e-4\n\n1e-3\n\n2e-4\n\n1e-4\n\n1e-2\n\n1e-5\n\nDP full\n\nnon-DP full 5e-5 2e-5\n\nDP-BiTFiT 2e-3\n\n4.1 TEXT CLASSIFICATION\n\nWe experiment on MNLI-m(mismatch) (Williams et al., 2018), QQP (Iyer et al., 2017), QNLI (Rajpurkar et al., 2016), and SST2 datasets (Socher et al., 2013). Competitive algorithms include reparameterized gradient perturbation (RGP, (Yu et al., 2021c)), LoRA, Adapter and Compacter (Yu et al., 2021a). We use the same setup as Li et al. (2021) on RoBERTa models, only increasing the learning rate for DP-BiTFiT. Additional results with different clipping functions and under a stronger privacy guarantee ε = 3 can be found in Table 13.\n\n7\n\n100200300400500Input dimension T51015202530Memory (GB)100200300400500Input dimension T2004006008001000Time (sec) per epochnon-DP BiTFiTDP BiTFiTnon-DP fullOpacus(Mix)GhostClip50^2100^2128^2150^2200^2Input dimension T020406080Memory (GB)non-DP BiTFiTDP BiTFiTnon-DP fullOpacusGhostClipMixedGhostClip50^2100^2128^2150^2200^2Input dimension T20406080100120140Time (sec) per epoch0100200300400Maximum throughput of DP-BiTFiT0100200300400Maximum throughput of algorithmsnon-DP BiTFiTDP-BiTFiTnon-DP fullOpacusGhostClip0255075100125150175200Maximum batch size of DP-BiTFiT0255075100125150175200Maximum batch size of algorithmsnon-DP BiTFiTDP-BiTFiTnon-DP fullOpacusGhostClip020406080100120140160Maximum throughput of DP-BiTFiT0255075100125150Maximum throughput of algorithmsnon-DP BiTFiTDP-BiTFiTnon-DP fullOpacusGhostClipMixGhostClip020406080Maximum batch size of DP-BiTFiT020406080Maximum batch size of algorithmsnon-DP BiTFiTDP-BiTFiTnon-DP fullOpacusGhostClipMixGhostClipUnder review as a conference paper at ICLR 2023\n\nFull\n\nRGP\n\nAdapter\n\nLoRA\n\nAdditional params to networks Forward caching activations\n\n✗ ✓\n\n✗ ✓\n\n✓ ✓\n\n✓ ✓\n\n(Li et al., 2021) (Yu et al., 2021a) (Yu et al., 2021a) (Yu et al., 2021a)\n\nBiTFiT Ours ✗\n✗\n\nCompacter (Yu et al., 2021a) ✓\n✓\n\n% of trainable params\n\nAccuracy SST2 Accuracy QNLI Accuracy QQP Accuracy MNLI-m\n\n% of trainable params\n\nAccuracy SST2 Accuracy QNLI Accuracy QQP Accuracy MNLI-m\n\n100% standard DP 92.1 87.9 86.1 83.2\n\n94.5 91.4 87.3 85.9\n\n100% standard DP 93.8 91.1 87.5 87.0\n\n96.2 93.6 87.9 90.3\n\nRoBERTa-base (125M) 1.4% 100% DP DP 92.5 91.6 87.5 87.2 85.6 85.5 80.1 83.4 RoBERTa-large (355M) 100% DP 93.0 90.0 86.7 86.1\n\n1.4% DP 93.9 90.7 86.3 87.7\n\n0.94%\n\nstandard DP 92.2 87.3 85.7 83.5\n\n95.1 93.3 90.8 87.5\n\n0.083% standard DP 93.5 92.4 87.3 86.5 86.1 83.4 83.4 82.6\n\n0.94%\n\nstandard DP 95.3 90.8 87.4 87.8\n\n96.2 94.9 91.6 90.6\n\n0.077% standard DP 95.5 94.5 92.2 91.0 87.9 86.5 89.3 87.6\n\n0.055% DP 92.3 85.1 84.7 82.6\n\n0.053% DP 94.2 90.2 86.2 87.5\n\nTable 4: Accuracy of fine-tuning methods with RoBERTa, under ε = 8. More non-private finetuning results (similar to here) can be found in (Yu et al., 2021a; Hu et al., 2021; Zaken et al., 2022). Note that linear probing of RoBERTa-base only gets 87.2% on SST2 and 77.3% on QNLI.\n\nIn Table 4, DP-BiTFiT is highly parameter efficiency and on-par with other DP fine-tuning in terms of accuracy. As indicated by Figure 1 and Figure 3, over 2× speedup and over 3× memory saving is observed, when switching from DP full fine-tuning to DP-BiTFiT across datasets.\n\nRemark 4.1. It is encouraging to observe that the gap between the full fine-tuning and BiTFiT, with or without DP, tends to decrease as the model size increases. For instance on QNLI, this gap without privacy reduces from 4.1% to 1.4%, and with privacy reduces from 1.4% to 0.1%. This scaling pattern is consistently observed on different tasks, e.g. in Table 5 and Table 6.\n\n4.2 NATURAL LANGUAGE GENERATION\n\nWe compare DP-BiTFiT with DP LoRA, full fine-tuning, and prefix tuning (Li & Liang, 2021) on E2E dataset (Dusek et al., 2020), in order to train GPT2 that generates texts to evaluate a restaurant. The performance measures are BLEU (Papineni et al., 2002), ROGUE-L (Lin, 2004), NIST (Sadjadi et al., 2018), METEOR (Banerjee & Lavie, 2005), CIDEr (Vedantam et al., 2015) and perplexity. We use the same setup as Bu et al. (2022b) with automatic clipping, only increasing the learning rate for DP-BiTFiT. More results under a stronger privacy guarantee ε = 3 can be found in Table 14.\n\nModel\n\nFine-tuning % of params Privacy↓ Perplexity↓ BLEU↑ ROGUE-L↑ NIST↑ METEOR↑ CIDEr↑\n\nGPT2-small (124M)\n\nfull\n\n100%\n\nLoRA\n\nprefix\n\n—\n\n—\n\nBiTFiT\n\n0.082%\n\nGPT2-medium (355M)\n\nfull\n\n100%\n\nBiTFiT\n\n0.076%\n\nGPT2-large (774M)\n\nfull\n\n100%\n\nBiTFiT\n\n0.066%\n\nstandard DP (ε = 8) standard DP (ε = 8) standard DP (ε = 8) standard DP (ε = 8) standard DP (ε = 8) standard DP (ε = 8) standard DP (ε = 8) standard DP (ε = 8)\n\n2.91 2.33 —\n— —\n— 3.19 2.89 2.08 2.25 2.85 2.67 1.79 2.26 2.79 2.59\n\n69.46 63.60 69.68 63.39 68.85 49.26 64.46 60.13 68.50 64.22 64.48 61.02 66.84 64.64 65.79 65.21\n\n71.36 67.07 71.71 67.53 70.81 60.73 63.67 64.96 71.46 67.53 67.81 66.13 70.38 68.97 67.61 67.88\n\n8.78 7.71 8.82 7.45 8.72 5.53 4.25 6.14 8.63 8.17 8.50 7.18 8.73 8.30 8.55 8.43\n\n0.46 0.40 0.46 0.41 0.45 0.36 0.36 0.37 0.45 0.42 0.43 0.39 0.46 0.42 0.43 0.42\n\n2.42 1.94 2.49 1.95 2.35 1.57 1.36 1.62 2.14 2.08 2.11 1.80 2.36 2.16 2.21 2.15\n\nTable 5: Performance of fine-tuning methods with GPT2, under ε = 8. LoRA and prefix results are documented in Li et al. (2021). Best performance in each model is in bold text.\n\nIn Table 5, DP-BiTFiT has shown strong performance, even outperforming DP full fine-tuning on GPT2-large, as well as both the computation and parameter efficiency (see Figure 4). Similar to Remark 4.1, the gap of BLEU score between DP-BiTFiT and DP full fine-tuning reduces from - 3.06/-3.20 (GPT2-small/medium) to +0.57 (GPT2-large), as the model size increases. We refer to Table 14 for a more significant pattern when ε = 3.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n4.3\n\nIMAGE CLASSIFICATION\n\nWe further experiment with DP-BiTFiT on CIFAR10/CIFAR100 (32 × 32 pixels, resized to 224 × 224) and CelebA (218 × 178 pixels, not resized) after pre-training on ImageNet (224 × 224 pixels). Unlike language tasks, DP-BiTFiT can be less satisfactory, e.g. inducing 30% test accuracy gap in CelebA [Smiling] classification in Table 6, though this gap can often be closed by increasing the model size, similar to Remark 4.1. Alternatively, we can leverage a two-phase training to interpolate between full fine-tuning and BiTFiT in the next section.\n\nDataset\n\nModel\n\nFine-tuning Accuracy\n\n(Yu et al., 2021b) (Tramer & Boneh, 2020)\n\n(De et al., 2022)\n\nCIFAR10 (ε = 2, δ =1e-5)\n\n(Bu et al., 2022a)\n\nOurs\n\n(Bu et al., 2022b)\n\nOurs\n\n(Bu et al., 2022b)\n\nOurs\n\n(Bu et al., 2022b)\n\nOurs\n\nCelebA [Smiling] (ε = 8, δ =5e-6)\n\nCelebA [Male] (ε = 8, δ =5e-6)\n\nCelebA [Multi-label] (ε = 8, δ =5e-6)\n\nSIMCLRv2\n\nResNet152 (GEP) linear probing linear probing Wide-ResNet28 linear probing Wide-ResNet28 crossvit-base-240 vit-base-patch16 vit-large-patch16 crossvit-base-240 vit-base-patch16 vit-large-patch16 ResNet9 ResNet18 ResNet9 ResNet18 ResNet9 ResNet18 ResNet9 ResNet18 ResNet9 ResNet18 ResNet9 ResNet18\n\nfull full full full BiTFiT BiTFiT BiTFiT full full BiTFiT BiTFiT full full BiTFiT BiTFiT full full BiTFiT BiTFiT\n\n94.8 92.7 93.6 95.4 96.1 94.4 98.0 79.3 90.7 95.5 91.08 91.02 61.15 90.83 95.70 95.15 70.10 93.34 87.58 88.38 82.04 86.24\n\nCIFAR10 DP-BiTFiT 1+BiTFiT 2+BiTFiT DP full\n\nε = 1 ε = 2 ε = 4 ε = 8\n\n11.7 10.0 13.8 10.1\n\n98.2 98.3 98.2 98.5\n\n97.9 98.0 98.0 98.0\n\n97.2 97.3 97.5 97.8\n\nCIFAR100 DP-BiTFiT 1+BiTFiT 2+BiTFiT DP full\n\nε = 1 ε = 2 ε = 4 ε = 8\n\n1.0 1.0 1.0 1.0\n\n86.9 88.7 89.7 90.3\n\n87.8 89.3 89.7 90.7\n\n87.0 88.7 89.6 90.0\n\nTable 7: Accuracy of two-phase training (ours) and full finetuning by Bu et al. (2022a) with BEiT-large (Bao et al., 2021).\n\nTable 6: Accuracy of DP fine-tuning methods on CIFAR10 and CelebA. More results under different ε and network architectures can be found in Appendix E.3.\n\nFigure 5: Accuracy by epochs with BEiT-large on CIFAR100.\n\n4.4 TWO-PHASE TRAINING: INTERPOLATING BETWEEN FULL FINE-TUNING AND BITFIT\n\nWe introduce the two-phase training, denoted as X+BiTFiT, which firstly applies DP full finetuning for X epochs then DP-BiTFiT for the rest of training. Hence, X+BiTFiT becomes DP full fine-tuning when X equals total epochs, and reduces to DP-BiTFiT when X = 0. Empirically speaking, it suffices to use X ≤ 2 to achieve comparable accuracy to full fine-tuning, while still enjoying the speedup. The effectiveness of two-phase training is verified in Table 7 and Appendix E.3. 1+BiTFiT outperforms previous SOTA by DP full fine-tuning (Bu et al., 2022a) that used BEiT-large: CIFAR10 97.1% → 98.8%; CIFAR100 86.2% → 88.7%, under ε = 2. 2+BiTFiT is comparable to previous SOTA, 87.05/87.58% → 86.54/86.71% on CelebA in Table 17, under ε = 3/8.\n\n5 DISCUSSION\n\nIn this work, we study DP-BiTFiT to privately train only the bias terms of neural networks. The highlight of DP-BiTFiT is the accuracy, the parameter efficiency and the computation efficiency, which is realized by not forward caching the activation tensors, and not back-propagating the gradient of weights. This consequently allows DP-BiTFiT to be as fast and memory-saving as its non-private counterpart, thus particularly suitable for large models and high-dimension data.\n\nFor future directions, DP-BiTFiT can be readily combined with prefix-based tuning and weightsbased fine-tuning, e.g. DP Adapter+BiTFiT and DP LoRA+BiTFiT, via f (x; W0, ˆb, ˆθ) using the notation in Equation (1). Specifically, such combination can decide whether weight and/or bias should be fine-tuned in a layer-wise manner. For instance, we can optimize only the embedding layer (which has no bias terms) and all bias terms in other layers. We expect this interpolating approach between full fine-tuning and BiTFiT, in parallel to our two-phase training, to circumvent the limitation that DP-BiTFiT is sometimes sub-optimal on small models or difficult tasks.\n\n9\n\n85.087.590.092.595.00.000.250.500.751.001.251.501.752.00Epoch0.02.55.07.510.0Test accuracy (=2)non-DP fullDP fullnon-DP BiTFiT1+ DP-BiTFiTDP-BiTFiTUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and In Proceedings of the 2016 ACM SIGSAC\n\nLi Zhang. Deep learning with differential privacy. conference on computer and communications security, pp. 308–318, 2016.\n\nSatanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with In Proceedings of the ACL Workshop on Inimproved correlation with human judgments. trinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 65–72, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W05-0909.\n\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers.\n\nIn International Conference on Learning Representations, 2021.\n\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n\narXiv preprint arXiv:2004.05150, 2020.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nZhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J Su. Deep learning with gaussian differential privacy.\n\nHarvard data science review, 2020(23), 2020.\n\nZhiqi Bu, Jialin Mao, and Shiyun Xu. Scalable and efficient training of large convolutional neural\n\nnetworks with differential privacy. arXiv preprint arXiv:2205.10683, 2022a.\n\nZhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis. Automatic clipping: Differentially\n\nprivate deep learning made easier and stronger. arXiv preprint arXiv:2206.07136, 2022b.\n\nHan Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for efficient on-device learning. Advances in Neural Information Processing Systems, 33:11285– 11297, 2020.\n\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp. 2633–2650, 2021.\n\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\n\nmemory cost. arXiv preprint arXiv:1604.06174, 2016.\n\nSoham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. UnlockarXiv preprint\n\ning high-accuracy differentially private image classification through scale. arXiv:2204.13650, 2022.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.\n\nOndrej Dusek, Jekaterina Novikova, and Verena Rieser. Evaluating the State-of-the-Art of End-toEnd Natural Language Generation: The E2E NLG Challenge. Computer Speech & Language, 59:123–156, January 2020. doi: 10.1016/j.csl.2019.06.009.\n\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pp. 265–284. Springer, 2006.\n\nIan Goodfellow. Efficient per-example gradient computations. arXiv preprint arXiv:1510.01799,\n\n2015.\n\nSivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy.\n\nAdvances in Neural Information Processing Systems, 34, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nPriya Goyal, Piotr Doll ́ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n\nNiv Haim, Gal Vardi, Gilad Yehudai, Ohad Shamir, and Michal Irani. Reconstructing training data\n\nfrom trained neural networks. arXiv preprint arXiv:2206.07758, 2022.\n\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2021.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint\n\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021.\n\nShankar\n\nIyer,\n\nNikhil Dandekar,\n\nand Kornel Csernai.\n\nrelease: First-Quora-Dataset-Release-Question-Pairs.\n\nQuestion\n\n2017.\n\npairs,\n\nURL\n\ndataset https://data.quora.com/\n\nquora\n\nFirst\n\nParas Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. Checkmate: Breaking the memory wall with optimal tensor rematerialization. Proceedings of Machine Learning and Systems, 2:497–511, 2020.\n\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In International Conference on Learning Representations, 2018.\n\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401–4410, 2019.\n\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171– 4186, 2019.\n\nSimon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2661– 2671, 2019.\n\nAntti Koskela, Joonas J ̈alk ̈o, and Antti Honkela. Computing tight differential privacy guarantees using fft. In International Conference on Artificial Intelligence and Statistics, pp. 2560–2569. PMLR, 2020.\n\nAlexey Kurakin, Steve Chien, Shuang Song, Roxana Geambasu, Andreas Terzis, and Abhradeep arXiv preprint\n\nimagenet scale with differential privacy.\n\nToward training at\n\nThakurta. arXiv:2201.12328, 2022.\n\nJaewoo Lee and Daniel Kifer. Scaling up differentially private deep learning with fast per-example\n\ngradient clipping. arXiv preprint arXiv:2009.03106, 2020.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, 2020.\n\nQuentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Cl ́ement Delangue, Th ́eo Matussi`ere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Franc ̧ois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community In Proceedings of the 2021 Conference on Empirical library for natural language processing. Methods in Natural Language Processing: System Demonstrations, pp. 175–184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.emnlp-demo.21.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\n\npreprint arXiv:2101.00190, 2021.\n\nXuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be\n\nstrong differentially private learners. arXiv preprint arXiv:2110.05679, 2021.\n\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W04-1013.\n\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 441–459, 2020.\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\n\nunderstands, too. arXiv preprint arXiv:2103.10385, 2021.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank\n\nhypercomplex adapter layers. arXiv preprint arXiv:2106.04647, 2021.\n\nHarsh Mehta, Abhradeep Thakurta, Alexey Kurakin, and Ashok Cutkosky. Large scale transfer learning for differentially private image classification. arXiv preprint arXiv:2205.02973, 2022.\n\nIlya Mironov, Kunal Talwar, and Li Zhang. R ́enyi differential privacy of the sampled gaussian mechanism. arXiv preprint arXiv:1908.10530, 2019. URL http://arxiv.org/abs/1908. 10530.\n\nSinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge\n\nand data engineering, 22(10):1345–1359, 2009.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. Bleu: a method for automatic\n\nevaluation of machine translation. pp. 311–318, 2002.\n\nJonas Pfeiffer, Aishwarya Kamath, Andreas R ̈uckl ́e, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. In 16th Conference of the European Chapter of the Associationfor Computational Linguistics, EACL 2021, pp. 487–503. Association for Computational Linguistics (ACL), 2021.\n\nBoris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.\n\nSIAM journal on control and optimization, 30(4):838–855, 1992.\n\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Micro-batch training with batch-\n\nchannel normalization and weight standardization. arXiv preprint arXiv:1903.10520, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners.\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–16. IEEE, 2020.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\n\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\n\nresidual adapters. Advances in neural information processing systems, 30, 2017.\n\nAndreas R ̈uckl ́e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and In Proceedings Iryna Gurevych. Adapterdrop: On the efficiency of adapters in transformers. of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7930–7946, 2021.\n\nSeyed Omid Sadjadi, Timothee Kheyrkhah, Audrey Tong, Craig S Greenberg, Douglas A Reynolds, Elliot Singer, Lisa P Mason, Jaime Hernandez-Cordero, et al. The 2017 nist language recognition evaluation. In Odyssey, pp. 82–89, 2018.\n\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pp. 3–18. IEEE, 2017.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013.\n\nPranav Subramani, Nicholas Vadivelu, and Gautam Kamath. Enabling fast differentially private sgd via just-in-time compilation and vectorization. Advances in Neural Information Processing Systems, 34, 2021.\n\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pp. 843–852, 2017.\n\nFlorian Tramer and Dan Boneh. Differentially private learning needs better features (or much more\n\ndata). arXiv preprint arXiv:2011.11660, 2020.\n\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4566–4575, 2015.\n\nYu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled r ́enyi differential privacy and analytical moments accountant. In International Conference on Artificial Intelligence and Statistics, pp. 1226–1235. PMLR, 2019.\n\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112–1122. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101.\n\nXiaodong Yang, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Normalized/clipped sgd with perturbation for differentially private non-convex optimization. arXiv preprint arXiv:2206.13033, 2022.\n\nAshkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya Mironov. Opacus: User-friendly differential privacy library in PyTorch. arXiv preprint arXiv:2109.12298, 2021.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500, 2021a.\n\nDa Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Do not let privacy overbill utility: Gradient embedding perturbation for private learning. In International Conference on Learning Representations, 2021b. URL https://openreview.net/forum?id=7aogOj_VYO0.\n\nDa Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via lowIn International Conference on Machine Learning, pp. 12208–12218.\n\nrank reparametrization. PMLR, 2021c.\n\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 1–9, 2022.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA DETAILED ANALYSIS OF BACK-PROPAGATION\n\nWe rigorously analyze the neural network represented in Section 2: for sample index i ∈ [B],\n\nal+1,i (cid:124) (cid:123)(cid:122) (cid:125) RT ×d′\n\n= φ( sl,i (cid:124)(cid:123)(cid:122)(cid:125) RT ×p\n\n),\n\nsl,i = al,i (cid:124)(cid:123)(cid:122)(cid:125) RT ×d\n\nWl (cid:124)(cid:123)(cid:122)(cid:125) Rd×p\n\n+ 1\n\n(cid:124)(cid:123)(cid:122)(cid:125) RT ×1\n\n,\n\n· bl\n\n(cid:124)(cid:123)(cid:122)(cid:125) R1×p\n\n(5)\n\nThen the per-sample weight gradient is given by the chain rule as\n\n⊤\n\n∂Li ∂Wl\n\n(cid:88)\n\n=\n\nj\n\n∂Li ∂sl,j\n\n⊤ ∂sl,j ∂Wl\n\n=\n\n∂Li ∂sl,i\n\n⊤ ∂sl,i ∂Wl\n\n=\n\n⊤\n\n∂Li ∂sl,i\n\nal,i =\n\n⊤\n\n∂L ∂sl,i\n\nal,i\n\nin which the second equality holds when there is no parameter sharing (so that each per-sample loss only depends on i-th input and output). The last equality holds for the same reason.\n\nSimilarly, we have the per-sample bias gradient as\n\n⊤\n\n∂Li ∂bl\n\n(cid:88)\n\n=\n\nj\n\n∂Li ∂sl,j\n\n⊤ ∂sl,j ∂bl\n\n=\n\n∂Li ∂sl,i\n\n⊤ ∂sl,i ∂bl\n\n=\n\n⊤\n\n∂Li ∂sl,i\n\n1 =\n\n⊤\n\n1.\n\n∂L ∂sl,i\n\nWe additionally demonstrate that bias gradient is independent of the input al, on the convolution (1d/2d/3d) and the normalization layers. For the convolution, sl is the inversely folded output and al is the unfolded input, then the forward pass is the same as that of linear layer in Equation (5). Notice that T is the product of hidden feature dimension (c.f. Bu et al. (2022a)), which depends on the padding, kernel sizes, strides, etc. For the batch, layer, group, and instance normalization, the forward pass is\n\nal,i − E(al) (cid:112)Var(al) + 0.00001 which can be analyzed similarly to that of Equation (5).\n\nsl,i =\n\n· Wl + 1 · bl\n\nB IMPLEMENTATION OF DP-BITFIT\n\nIn this section we describe the implementation of DP-BiTFiT, which only uses Pytorch backward hook but not the forward hook, and thus is different from existing packages such as FastGradClip Lee & Kifer (2020), Opacus Yousefpour et al. (2021), Private Transformers Li et al. (2021), Private CNN Bu et al. (2022a). Notice that in these packages, the forward hook is used to store the activation tensor al for all layers, which incurs huge memory burden as discussed in Section 2.\n\nThe Pytorch backward hook is a function, to be registered on a torch Module (or a layer in the neural network), that will be executed in the backward propagation. The backward hook automatically extracts the input gradient ∂L ∂al\n\nand the output gradient ∂L ∂sl\n\nof the layer.\n\nIn DP-BiTFiT, we call register backward hook to register a backward hook for Line 5 of Algorithm 1. An example for a linear layer: RB×T ×d → RB×T ×p looks like\n\ndef hook(linear_layer, grad_input, grad_output):\n\nlinear_layer.bias.grad_sample = grad_output.sum(dim=1) linear_layer.bias.norm_sample = linear_layer.bias.grad_sample.norm(2,dim=1)\n\nHere the attribute norm sample stores the per-sample gradient norm grad sample stores the RB×p per-sample gradient of bias.\n\nThen the implementation of DP-BiTFiT for one iteration looks like\n\n(cid:13) (cid:13) (cid:13)\n\n∂Li ∂bl\n\n(cid:13) (cid:13) (cid:13)F\n\n, and the attribute\n\noutput=model(input) loss=F.cross_entropy()(output,label) torch.autograd.grad(loss,biases) all_layer_norm_sample = torch.stack([param.norm_sample for param in biases],dim=0).norm(2, dim=0) clipping_factor=1/(all_layer_norm_sample+0.01) for layer in model.modules():\n\nlayer.bias.grad=torch.einsum(\"i,i...->...\", clipping_factor,layer.bias.grad_sample)\n\noptimizer.step() optimizer.zero_grad()\n\nwhere biases is the collection of all bias terms in all layers.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nC COMPLEXITY ANALYSIS\n\nWe provide more details on analyzing the time and space complexity. The analysis for full fine-tuning has been presented in (Bu et al., 2022a, Appendix C) and is adapted here for the for example, Adapter Houlsby et al. (2019) uses two matrices parameter efficient fine-tuning: Wdown ∈ Rp×r, Wup ∈ Rr×p that constitute\n\nx ←− x + GeLU(x · Wdown)Wup\n\nHence the complexity, in comparison to full-finetuning, changes by replacing d → 2r. LoRA Hu et al. (2021) also uses two matrices Wdown ∈ Rd×r, Wup ∈ Rr×p that constitute\n\nx ←− x · W + x · WdownWup\n\nHence the complexity, in comparison to full-finetuning, changes by replacing pd → r(p + d).\n\nforward &output grad\n\nnon-DP DP full (Opacus)\n\nDP LoRA\n\nDP Adapter non-DP DP (ours)\n\nweight training\n\nbias training\n\n4BT pd\n\n2BT pd\n\n+2BT pd\n\n+2BT (pr + dr) +4BT pr BT p +3Bp\n\npd + BT d BT (p + d)\n\n+Bpd\n\n+B(pr + dr)\n\n+2Bpr\n\n1 ✗\n\n1 ✓\n\n1 ✓\n\n1 ✓\n\np\n\n1 ✗\n\n+Bp\n\n1 ✗\n\nTime complexity Space complexity # back-prop forward hook\n\nTable 8: Per-layer time and space complexity of training on weights (full and parameter efficient fine-tuning) and biases. ‘+’ means additional overhead to non-DP training.\n\nFor per-sample bias gradient clipping, we need ∂Li 1 in Equation (4), which consists of ∂bl the per-sample gradient instantiation (i.e. summation along the feature dimension, from RT p → Rp, ∂L ), and computing the per-sample gradient norm (i.e. taking the square at each index ∂sl,i and summing all indices). Here each operation in italic takes Bp time complexity, meaning the total time complexity is 3Bp, but the space complexity is Bp if operated in-place.\n\n→ ∂Li ∂bl\n\n= ∂L ∂sl,i\n\n⊤\n\n⊤\n\nD EXPERIMENT DETAILS\n\nD.1 LANGUAGE TASKS\n\nThroughout this work, the text datasets are processed and loaded from Huggingface Lhoest et al. (2021). We follow the same setup as Li et al. (2021); Bu et al. (2022b), such as δ = 0.5×sample size. The full fine-tuning is implemented by Private Transformers codebase, version 0.2.0 (i.e. GhostClip algorithm Li et al. (2021)).\n\nFor text classification, we experiment on four datasets: MNLI(m), the matched splits from MultiGenre Natural Language Inference Corpus; QQP, the Quora Question Pairs2 dataset; QNLI The Stanford Question Answering dataset; SST2 The Stanford Sentiment Treebank dataset.\n\nTo give a fair comparison, we use the same optimizer as in Li et al. (2021), i.e. DP-Adam with Abadi’s clipping.\n\nDataset epoch batch size clipping threshold R DP learning rate non-DP learning rate max sequence length\n\nMNLI QQP QNLI 18 6000\n\n6 2000\n\n18 6000\n\nSST2 3\n1000\n\n0.1 full 5e-4 / BiTFiT 5e-3 full 5e-5 / BiTFiT 1e-3 256\n\nTable 9: Hyperparameters of text classification in Table 4 and Table 13, using RoBERTa (base/large).\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFor E2E generation task, we experiment GPT2 models using the same optimizer as in Bu et al. (2022b), using DP-AdamW with automatic clipping.\n\nModel epoch batch size DP learning rate (full) non-DP learning rate (full) DP learning rate (BiTFiT) non-DP learning rate (BiTFiT) learning rate decay max sequence length\n\n2e-3 2e-4\n\nGPT2-small GPT2-medium GPT2-large 10 1024 2e-3 1e-4 1e-2 2e-3 No 100\n\n2e-3 1e-4\n\nTable 10: Hyperparameters of E2E generation task in Table 5 and Table 14, using GPT2.\n\nD.2\n\nIMAGE TASKS\n\nWe give the experiments settings for image classification. For CIFAR10 and CIFAR100, we use the same setting as Bu et al. (2022a), e.g. 5 epochs for CrossViT/ViT and 3 epochs for BEiT-large. For CelebA, we use the same setting as Bu et al. (2022b), e.g. 10 epochs.\n\nWe use DP-Adam with Abadi’s clipping. We do not apply tricks such as random data augmentation, weight standardization Qiao et al. (2019), or parameter averaging Polyak & Juditsky (1992). Our experiments are heavily based on Private CNN (i.e. MixGhostClip algorithm Bu et al. (2022a)) and TIMM codebases.\n\nDataset Model epoch batch size clipping threshold DP learning rate (full) DP learning rate (BiTFiT) learning rate decay normalizing data\n\nCIFAR10 CrossViT BEiT-large BEiT-large ResNet18\n\nCIFAR100\n\nCIFAR10\n\nCelebA\n\n5 1000\n\n3 1000\n\n3 1000\n\n10 500\n\n0.1 1e-3 5e-3 No\n\nYes\n\nYes\n\nYes\n\nNo\n\nTable 11: Hyperparameters of image classification task in Section 4.3,Table 15,Table 16,Table 17.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nE ADDITIONAL TABLES AND FIGURES\n\nE.1 PARAMETER EFFICIENCY OF DP-BITFIT\n\nModel VGG11 VGG16 VGG19 ResNet18 ResNet34 ResNet50 ResNet101 ResNet152 wide resnet50 2 wide resnet101 2 convnext base convnext large ViT-small-patch16 ViT-base-patch16 ViT-large-patch16 beit base patch16 224 deit base patch16 224 GPT2-small GPT2-medium GPT2-large RoBERTa-base RoBERTa-large BERT-base-uncased BERT-large-uncased BART-large longformer-base-4096 longformer-large-4096\n\nNumber of params % of params\n\n133M 138M 144M 11.7M 21.8M 25.6M 44.5M 60.2M 68.9M 126.9M 88.6M 197.8M 22.0M 86.6M 304M 86.5M 86.4M 124M 355M 774M 125M 355M 109M 335M 406M 149M 435M\n\n0.009 0.009 0.010 0.043 0.044 0.113 0.121 0.127 0.051 0.055 0.148 0.099 0.238 0.120 0.090 0.088 0.120 0.082 0.076 0.066 0.083 0.077 0.094 0.081 0.082 0.088 0.080\n\nTable 12: Parameter efficiency of (DP) BiTFiT on various models.\n\nE.2 MORE RESULTS ON DP-BITFIT AND LANGUAGE TASKS\n\nfull (Li et al., 2021; Bu et al., 2022b) RoBERTa-base\n\nBiTFiT (ours)\n\nAccuracy SST2 Accuracy QNLI Accuracy QQP\n\n94.5 91.4 87.3 Accuracy MNLI-m 85.9\n\nstandard DPAbadi DPAUTO DPAbadi DPAUTO standard DPAbadi DPAUTO DPAbadi DPAUTO ε = ∞ ε = 8 92.1 87.9 86.1 83.2\n\nε = ∞ ε = 8 92.4 86.5 83.4 82.6\n\nε = 8 92.4 87.9 86.6 83.8\n\nε = 8 92.4 86.7 84.0 82.6\n\nε = 3 92.0 86.4 83.0 81.5\n\nε = 3 92.0 86.1 83.8 82.0\n\n93.5 87.3 86.1 83.4\n\nε = 3 91.9 87.4 85.6 82.5\n\nε = 3 92.3 86.9 85.8 83.2 RoBERTa-large\n\nAccuracy SST2 Accuracy QNLI Accuracy QQP\n\nstandard DPAbadi DPAUTO DPAbadi DPAUTO standard DPAbadi DPAUTO DPAbadi DPAUTO ε = ∞ ε = 8 93.8 91.1 86.9 87.0\n\n96.2 93.6 87.9 Accuracy MNLI-m 90.3 Table 13: Accuracy of full fine-tuning and BiTFiT with RoBERTa, under different per-sample clipping functions (indicated as subscript, Abadi Abadi et al. (2016) and AUTO-S Bu et al. (2022b)). Same setting as Appendix D.\n\nε = ∞ ε = 8 94.5 91.0 86.5 87.6\n\nε = 3 93.9 91.0 86.8 86.3\n\nε = 8 94.6 91.5 87.5 87.1\n\nε = 8 94.7 91.1 87.1 87.7\n\nε = 3 94.5 90.3 86.3 87.2\n\nε = 3 94.6 90.8 86.5 87.2\n\nε = 3 93.0 90.8 86.6 86.4\n\n95.5 92.2 87.9 89.3\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nFine-tuning % of params\n\nfull\n\n100%\n\nGPT2-small (124M)\n\nLoRA\n\nprefix\n\n—\n\n—\n\nBiTFiT\n\n0.082%\n\nGPT2-medium (355M)\n\nfull\n\n100%\n\nBiTFiT\n\n0.076%\n\nGPT2-large (774M)\n\nfull\n\n100%\n\nBiTFiT\n\n0.066%\n\nPrivacy↓ standard DP (ε = 8) DP (ε = 3) standard DP (ε = 8) DP (ε = 3) standard DP (ε = 8) DP (ε = 3) standard DP (ε = 8) DP (ε = 3) standard DP (ε = 8) DP (ε = 3) standard DP (ε = 8) DP (ε = 3) standard DP (ε = 8) DP (ε = 3) standard DP (ε = 8) DP (ε = 3)\n\nPerplexity↓ BLEU↑ ROGUE-L↑ NIST↑ METEOR↑ CIDEr↑\n\n2.91 2.33 2.36 —\n— —\n— —\n— 3.19 2.89 3.00 2.08 2.25 2.62 2.85 2.67 2.67 1.79 2.26 2.65 2.79 2.59 2.61\n\n69.46 63.60 61.34 69.68 63.39 58.15 68.85 49.26 47.77 64.46 60.13 54.78 68.50 64.22 63.85 64.48 61.02 57.11 66.84 64.64 64.18 65.79 65.21 65.18\n\n71.36 67.07 65.87 71.71 67.53 65.77 70.81 60.73 58.96 63.67 64.96 63.55 71.46 67.53 67.07 67.81 66.13 66.16 70.38 68.97 67.86 67.61 67.88 67.90\n\n8.78 7.71 7.07 8.82 7.45 5.46 8.72 5.53 5.25 4.25 6.14 4.78 8.63 8.17 7.11 8.50 7.18 5.07 8.73 8.30 7.94 8.55 8.43 8.34\n\n0.46 0.40 0.39 0.46 0.41 0.37 0.45 0.36 0.36 0.36 0.37 0.34 0.45 0.42 0.39 0.43 0.39 0.37 0.46 0.42 0.40 0.43 0.42 0.42\n\n2.42 1.94 1.80 2.49 1.95 1.58 2.35 1.57 1.51 1.36 1.62 1.31 2.14 2.08 1.75 2.11 1.80 1.47 2.36 2.16 2.01 2.21 2.15 2.12\n\nTable 14: Accuracy of fine-tuning with GPT2 on E2E dataset. LoRA and prefix results are taken from Li et al. (2021). Same setting as Appendix D.\n\nE.3 MORE RESULTS ON TWO-PHASE TRAINING\n\nCIFAR10\n\nModel\n\nbeit large patch16 224 ε = 1 ε = 2 ε = 4 ε = 8 beit base patch16 224 ε = 1 ε = 2 ε = 4 ε = 8 deit base patch16 224 ε = 1 ε = 2 ε = 4 ε = 8 ε = 1 ε = 2 ε = 4 ε = 8 vit large patch16 224 ε = 1 ε = 2 ε = 4 ε = 8 vit base patch16 224 ε = 1 ε = 2 ε = 4 ε = 8\n\nPrivacy DP-BiTFiT 1+BiTFiT 2+BiTFiT DP full 98.2 98.3 98.2 98.5 96.6 97.1 97.2 97.2 94.4 95.4 95.8 96.1 92.4 93.6 94.9 94.8 98.9 98.8 98.9 99.0 95.2 97.7 97.7 97.6\n\n97.2 97.3 97.5 97.8 95.4 96.0 96.2 96.3 95.4 95.6 96.0 96.3 95.2 95.3 95.7 96.2 98.9 98.9 99.0 99.0 96.8 97.1 97.2 97.4\n\n97.9 98.0 98.0 98.0 96.0 96.4 96.6 96.5 95.2 95.2 95.9 96.0 94.3 95.0 95.8 95.8 98.7 98.9 98.8 98.9 97.0 97.1 97.2 97.2\n\n11.7 10.0 13.8 10.1 10.0 10.7 14.0 10.0 78.2 75.0 72.9 71.2 74.3 80.4 81.0 78.2 89.7 90.6 93.2 93.9 86.7 89.3 88.3 88.7\n\ncrossvit base 240\n\nTable 15: Accuracy of two-phase fine-tuning on CIFAR10. Same setting as Appendix D.2 except ViT uses the following learning rate: DP full fine-tuning 5e-4, DP-BiTFiT 5e-3.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nCIFAR100\n\nModel\n\nbeit large patch16 224 ε = 1 ε = 2 ε = 4 ε = 8 beit base patch16 224 ε = 1 ε = 2 ε = 4 ε = 8 deit base patch16 224 ε = 1 ε = 2 ε = 4 ε = 8 ε = 1 ε = 2 ε = 4 ε = 8 vit large patch16 224 ε = 1 ε = 2 ε = 4 ε = 8 vit base patch16 224 ε = 1 ε = 2 ε = 4 ε = 8\n\nPrivacy DP-BiTFiT 1+BiTFiT 2+BiTFiT DP full 86.9 88.7 89.7 90.3 81.4 83.4 84.6 84.9 49.1 58.1 64.5 69.7 49.2 56.8 61.6 63.4 73.5 82.4 87.5 89.0 64.3 77.0 83.0 83.8\n\n87.0 88.7 89.6 90.0 80.9 83.1 84.8 85.2 69.1 74.3 77.1 77.9 67.6 71.6 73.1 74.2 87.7 90.1 91.0 91.3 83.9 85.5 87.2 87.1\n\n87.8 89.3 89.7 90.7 82.2 83.4 85.1 85.6 65.9 71.5 73.9 75.7 61.7 65.3 70.4 72.8 86.0 89.0 89.9 90.7 79.5 83.8 85.2 86.5\n\n1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 10.9 13.6 15.7 16.6 12.2 12.3 17.2 20.9 14.0 19.4 24.3 23.9 16.0 22.9 21.2 26.2\n\ncrossvit base 240\n\nTable 16: Accuracy of two-phase fine-tuning on CIFAR100. Same setting as Appendix D.2 except ViT uses the following learning rate: DP full fine-tuning 5e-4, DP-BiTFiT 5e-3.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nAttributes\n\n5 o Clock Shadow Arched Eyebrows Attractive Bags Under Eyes Bald Bangs Big Lips Big Nose Black Hair Blond Hair Blurry Brown Hair Bushy Eyebrows Chubby Double Chin Eyeglasses Goatee Gray Hair Heavy Makeup High Cheekbones Male Mouth Slightly Open Mustache Narrow Eyes No Beard Oval Face Pale Skin Pointy Nose Receding Hairline Rosy Cheeks Sideburns Smiling Straight Hair Wavy Hair Wearing Earrings Wearing Hat Wearing Lipstick Wearing Necklace Wearing Necktie Young Average Total time\n\nDP-BiTFiT 1+BiTFiT 2+BiTFiT DP full DP-BiTFiT 1+BiTFiT 2+BiTFiT DP full\n\nε = 3\n\nε = 8\n\n90.01 71.56 68.71 79.74 97.88 84.43 67.30 78.80 72.84 89.54 94.94 82.03 87.05 94.70 95.43 93.54 95.42 96.81 76.51 62.13 80.37 54.03 96.13 85.13 85.37 70.44 95.79 71.43 91.51 92.83 95.36 60.07 79.01 71.24 79.34 95.80 80.61 86.21 92.99 75.71 82.97 10:30\n\n90.01 73.12 73.98 79.76 97.88 84.43 67.30 78.95 74.86 93.00 94.94 82.02 87.05 94.70 95.43 93.54 95.42 96.81 82.76 68.20 88.47 59.32 96.13 85.13 85.87 70.94 95.79 71.51 91.51 92.83 95.36 66.32 79.01 73.09 79.34 95.80 87.90 86.21 92.99 79.33 84.42 12:02\n\n90.14 76.01 75.99 81.27 97.88 84.80 67.30 80.08 82.37 93.28 94.94 82.87 87.21 94.70 95.43 93.54 95.42 96.85 85.71 81.63 91.52 77.61 96.13 85.13 87.56 71.50 95.79 71.63 91.51 92.86 95.36 85.85 79.02 76.22 80.37 95.80 89.81 86.21 93.03 81.23 86.54 13:34\n\n91.32 77.33 79.22 81.73 97.93 94.06 67.78 81.19 85.84 94.17 95.05 85.44 88.26 94.84 95.49 94.30 95.96 97.44 88.48 83.77 94.73 86.75 96.10 85.14 92.94 73.11 95.79 71.89 91.59 93.07 96.44 89.34 79.65 77.35 83.24 96.01 91.59 86.21 93.58 83.69 88.20 25:50\n\n90.01 71.56 69.70 79.74 97.88 84.43 67.30 78.80 73.02 89.13 94.94 82.03 87.05 94.70 95.43 93.54 95.42 96.81 77.22 61.43 82.04 55.26 96.13 85.13 85.37 70.44 95.79 71.43 91.51 92.87 95.36 58.92 79.01 70.86 79.34 95.80 80.35 86.21 92.99 75.71 83.01 10:30\n\n90.01 73.74 73.61 79.74 97.88 84.44 67.30 78.92 78.71 92.62 94.94 82.37 87.05 94.70 95.43 93.54 95.42 96.81 83.03 67.27 88.52 60.70 96.13 85.13 85.88 71.48 95.79 71.47 91.51 92.83 95.36 65.97 79.01 73.62 79.34 95.80 87.20 86.21 92.99 78.52 84.52 12:02\n\n90.51 75.49 76.20 80.69 97.88 86.51 67.29 79.23 83.33 93.88 94.96 83.49 87.15 94.70 95.43 93.54 95.42 97.12 85.86 81.33 92.14 79.42 96.13 85.13 88.59 71.92 95.79 71.77 91.51 92.86 95.36 85.55 79.13 77.11 80.71 95.80 89.56 86.21 93.11 80.66 86.71 13:34\n\n91.64 78.82 78.08 82.62 97.91 94.22 68.34 81.86 86.47 94.34 95.10 85.04 89.02 94.78 95.39 95.85 95.89 97.45 89.05 84.20 95.19 90.24 96.12 85.16 93.59 71.77 95.79 72.87 91.61 93.33 96.63 89.11 78.60 72.73 84.36 97.02 91.94 86.21 93.57 83.11 88.38 25:50\n\nTable 17: Accuracy on CelebA dataset with settings in Appendix D.2 from one run. DP full finetuning is implemented with the most efficient MixGhostClip algorithm Bu et al. (2022a).\n\n21",
    "reference": "# Summary Of The Paper\n\nThe paper proposed to do DP fine-tuning on only the bias terms of the model. It analyzes the computational efficiency and demonstrated the empirical advantage of the proposed method.\n\n# Strength And Weaknesses\n\nStrength:\nThe idea seems interesting and the result on large models seem encouraging.\n\nWeakness:\nMaybe the authors can investigate and elaborate more on when and why the proposed method would work.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: The paper is quite easy to follow.\n\nQuality: The analyses of the efficiency looks correct and the empirical evaluation seems pretty throughout.\n\nNovelty: There are quite some work on fine-tuning certain part of the parameters under DP but the idea and the analyses of fine-tuning the bias with DP seems novel.\n\n# Summary Of The Review\n\nThe idea is simple, yet it seems to work pretty well in some cases and the computational efficiency is also good.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nEXPLORING LOW-RANK PROPERTY IN MULTIPLE INSTANCE LEARNING FOR WHOLE SLIDE IMAGE CLASSIFICATION\n\nJinxi Xiang, Xiyue Wang, Jun Zhang∗, Sen Yang, Xiao Han, Wei Yang Tencent AI Lab {jinxixang,junejzhang,haroldhan,willyang}@tencent.com {xiyue.wang.scu,sen.yang.scu}@gmail.com\n\nABSTRACT\n\nThe classification of gigapixel-sized whole slide images (WSIs) with slide-level labels can be formulated as a multiple-instance-learning (MIL) problem. State-ofthe-art models often consist of two decoupled parts: local feature embedding with a pre-trained model followed by a global feature aggregation network for classification. We leverage the properties of the apparent similarity in high-resolution WSIs, which essentially exhibit low-rank structures in the data manifold, to develop a novel MIL with a boost in both feature embedding and feature aggregation. We extend the contrastive learning with a pathology-specific Low-Rank Constraint (LRC) for feature embedding to pull together samples (i.e., patches) belonging to the same pathological tissue in the low-rank subspace and simultaneously push apart those from different latent subspaces. At the feature aggregation stage, we introduce an iterative low-rank attention MIL (ILRA-MIL) model to aggregate features with low-rank learnable latent vectors. We highlight the importance of cross-instance correlation modeling but refrain from directly using the transformer encoder considering the O(n2) complexity. ILRA-MIL with LRC pre-trained features achieves strong empirical results across various benchmarks, including (i) 96.49% AUC on the CAMELYON16 for binary metastasis classification, (ii) 97.63% AUC on the TCGA-NSCLC for lung cancer subtyping, and (iii) 0.6562 kappa on the large-scale PANDA dataset for prostate cancer classification. Code is available at https://github.com/jinxixiang/low_rank_wsi.\n\n1\n\nINTRODUCTION\n\nRecent artificial intelligence in digital pathology has presented the potential to analyze gigapixel whole-slide images (WSIs). However, some challenges remain unsolved, including limited samples for training deep learning models and the extremely high resolution of WSI images (Lu et al., 2021c; Campanella et al., 2019; Shao et al., 2021; Sharma et al., 2021; Lu et al., 2021b).\n\nSince the relationship between input images and target labels is highly ill-posed, e.g., on CAMELYON16, 1.5 million 224×224 input image tiles against 270 WSI-level labels, one has to decompose the model into two separate stages, local feature embedding and global feature aggregation. Biological tissues in WSIs exhibit a wide variation, and there are still high semantic and background similarities among different image patches from the same type of tissue. Therefore, one fundamental challenge is performing feature embedding that only captures relevant biological information and allows for quantitative comparison, categorization, and interpretation. After embedding, the standard MIL uses non-parametric max-/mean-pooling to perform slide-level classification. Such simplified schemes might lead to sub-optimal feature aggregation for WSI classification, and the models cannot learn cross-instance correlation due to the weak supervision signal.\n\nAs consistent with the findings in natural images (Cong et al., 2013; Zhou et al., 2014; Zhang et al., 2013; Liu et al., 2012), we empirically find that gigapixel WSIs exhibit essentially low-rank properties in the data manifold (see evidence in Appendix A). We aim to harness the low-rank property\n\n∗corresponding author\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: The proposed pipeline. WSI is cropped into patches and then embedded into vectors for classification. We design LRC for feature embedding and ILRA-MIL for feature aggregation.\n\nfor WSI classification. The first intention is to learn a low-dimensional feature embedding in a discriminative way by extending contrastive loss with a low-rank constraint. For global feature aggregation, it would be beneficial for MIL to learn potential cross-instance correlation, which may help the model become more context-aware (Lu et al., 2021c). To this end, the second intention is to introduce self-attention with a low-rank matrix that forms an attention bottleneck with which all instances must interact, allowing it to handle large-scale bag sizes with a small computation overhead. It resolves the quadratic complexity O(n2) caused by global self-attention.\n\nOur main contributions: (1) We extend contrastive learning with a low-rank constraint (LRC) to learn feature embedding using unlabeled WSI data; (2) We use iterative low-rank attention MIL (ILRA-MIL) to process a large bag of instances, allowing it to encode cross-instance correlation naturally; (3) Extensive experiments on public benchmarks are conducted. Remarkably, ILRA-MIL improves over baselines, including attention-pooling and transformer-based MIL, by a large margin.\n\n2 RELATED WORK\n\n2.1 LOCAL FEATURE EMBEDDING IN MIL\n\nMost methods conduct feature embedding with the ResNet50 pre-trained on ImageNet (Lu et al., 2021c; Campanella et al., 2019). However, there is a significant domain deviation between pathological and natural images, which might lead to sub-optimal patch features for WSI classification. Contrastive learning paves a way for pathology-specific image pre-training (Lu et al., 2019; Li et al., 2021; Chen et al., 2020a; Ciga et al., 2022; Stacke et al., 2021). The fundamental idea is to pull together an anchor and a “positive” sample in embedding space and push apart the anchor from many “negative” samples. Nevertheless, it is infeasible for pathology images since they usually consist of multiple positive instances (Li et al., 2022). SupCon extends the self-supervised contrastive approach to the fully-supervised setting, allowing us to leverage label information (Khosla et al., 2020) effectively. Nevertheless, fine-grained local annotations for WSIs are hardly available; thus, we cannot adapt SupCon directly. We exploit the low-rank properties to generalize the supervised contrastive loss for WSIs without patch-level label information.\n\n2.2 GLOBAL FEATURE AGGREGATION IN MIL\n\nTraditional poolings are robust to noisy data and unbalanced distribution. MIL-RNN (Campanella et al., 2019) built on recurrent network achieved clinical grade using more than 10,000 slides, but it is data-hungry and constrained for binary classification. The local attention method, i.e. ABMIL (Ilse et al., 2018) uses the attention weights to allow us to find key instances, bringing significant improvements and robustness. CLAM (Lu et al., 2021c) further improves ABMIL with a clustering constraint by pulling the most and least attended instances apart. As concluded by (Lu et al., 2021c), one limitation of CLAM and MIL-based approaches is that they typically treat different patches in the slide as independent and do not learn the potential cross-interactions, which may help the model become context-aware. To this end, global attention-based networks (Li et al., 2021; Shao et al., 2021; Lu et al., 2021a), are introduced with non-local pooling or transformer encoder to compensate for the shortness of local attention MIL that considers no cross-instance correlations. We aim to improve the global attention model with ILRA-MIL further.\n\n2\n\nwhole slide imagepatchesfeature extractor with LRCiterative low-rank attention MIL (ILRA-MIL)QKQKQKPublished as a conference paper at ICLR 2023\n\n3 METHOD\n\nThe proposed pipeline is boosted with the low-rank property of WSI, consisting of a local feature embedding module and a global feature aggregation module, as illustrated in Fig. 1.\n\n3.1 LOCAL FEATURE EMBEDDING\n\n3.1.1 PRELIMINARY\n\nContrastive learning implements the heuristic to discern positive samples from negative samples (Chen et al., 2020a;b;c;c; Grill et al., 2020; Gao et al., 2021). Given a randomly sampled minibatch of N images, we get pairs of projected feature vectors from augmented examples {zi}i∈I , I = {1, · · · , 2N }. The self-supervised contrastive loss is (Chen et al., 2020a):\n\nLCon = −\n\nlog\n\n(cid:88)\n\ni∈I\n\nexp (cid:0)sim(zi, zj(i))/τ (cid:1) a∈N (i) exp (sim(zi, za)/τ )\n\n(cid:80)\n\n(1)\n\nwhere sim(u, v) = u⊤v/∥u∥∥v∥ is the dot product between l2 normalized u and v; N (i) = I\\{i}; j(i) is the index of the other augmented sample from the same image; τ is a temperature parameter. For each anchor zi, there is one positive sample zj(i) and 2(N − 1) negative samples.\n\n3.1.2 EXTENSION OF CONTRASTIVE LOSS\n\nMost pathology cases have high semantic and background similarity, thus resulting in multiple positives in a batch, introducing estimation errors in (1). One straightforward approach is the generalization of supervised contrastive learning (i.e., SupCon (Khosla et al., 2020)) to an arbitrary number of positives by extending:\n\nLSupCon = −\n\n1 |P(i)|\n\n(cid:88)\n\ni∈I\n\n(cid:88)\n\nlog\n\np∈P(i)\n\n(cid:80)\n\nexp (sim(zi, zp)/τ )\n\na∈N (i) exp (sim(zi, za)/τ )\n\n(2)\n\nwhere P(i) is the set of indices of all positive samples in the minibatch given anchor zi; |P(i)| is its cardinality. For images with labels, it is intuitive to constitute positive samples with the same labels.\n\n3.1.3 PATHOLOGY SPECIFIC LOW-RANK LOSS\n\nSupCon in (2) extents vanilla contrastive loss by leveraging label information. But we refrain from adopting SupCon for WSIs because no patch-level labels are available. We thus propose a new self-supervised learning loss named LRC tailored for pathology images, which is shown to be a generalization of SupCon to unlabeled scenarios.\n\nGiven a set of feature samples, each of which can be represented as a linear combination of the bases in a dictionary, we aim at finding the representations that have a low-rank similarity matrix between two sets of augmented representations:\n\nR(T⊤ ̃T) =\n\n(cid:110)\n\nT⊤ ̃T ∈ RN ×N : rank(T⊤ ̃T) = r, r ≪ N\n\n(cid:111)\n\n(3)\n\nwhere T⊤ ̃T is a similarity matrix of T = [t1, · · · , tN ], ̃T = [ ̃t1, · · · , ̃tN ]; ̃ti and ti are two augmented representations of the same image. A low-rank matrix can be decomposed as the product of a dictionary D and a block-diagonal B such that (Liu et al., 2012; Wright & Ma, 2022):\n\nT⊤ ̃T = DB + E = [D1, D2, · · · , Dr]\n\n\n\n \n\n\n0 B1 0 B2\n\n0 0\n\n0 0\n\n\n\n \n\n\n0 0\n\n0 0\n. . . 0\n0 Br\n\n+ E\n\n(4)\n\nwhere E is an error matrix which should be minimized; Db ∈ RN ×sb , Bb ∈ Rsb×qb with b = 1, · · · , r; sb, qb represent the shape of subspace Bb.\n\nIntuitively, pairs belonging to the same subspace are more semantically similar than randomly sampled ones. This has also been recognized as latent classes (Chuang et al., 2020; Saunshi et al.,\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n2019). For self-supervised contrastive loss LCon with only one positive pair for each anchor, T⊤ ̃T is considered to be a full-rank diagonal matrix, i.e., all entries are zeros except for the diagonal ones. SupCon loss LSupCon further leverages label information to access more positive samples, enforcing T⊤ ̃T to explore the semantic similarity in the embedding space. In this way, SupCon loss could make low-rank constraints implicitly on T⊤ ̃T, where r is the rank of the matrix corresponding to the total number of classes in SupCon. This observation recognizes the connections of contrastive loss with the low-rank property.\n\nSince the low-rank decomposition of (3) is not tractable for online learning, as an alternative, we use SupCon loss in (2) as a surrogate by accessing more positives belonging to the same subspace Bb. Suppose we get a set of descendingly sorted indices based on their similarity to the anchor:\n\nC(a) = {A(1), · · · , A(N )| if i < j, then sim(ta, ̃tA(i)) ≥ sim(ta, ̃tA(j))}.\n\n(5)\n\nGiven an anchor ta, we get r subspace Cb(a), b = 1, · · · , r as stated in the low-rank representation Eq. (4). We can intuitively consider that each subspace corresponds to a latent class, where C1(a) = {A(1), · · · , A(q1)}, C2(a) = {A(q1 + 1), · · · , A(q1 + q2)}, · · · , Cr(a) = {A(N − qr + 1), · · · , A(N )}. Note that q1, q2, · · · qr, is the column dimension of B1, B2, · · · , Br. Instead of partitioning all samples to get all subspace, which is computationally infeasible without solving (4), we only optimize the objective over the least- and most-distant subspace C1(a), Cr(a) with respect to the anchor. For any positive sample p ∈ C1(a) and negative sample n ∈ Cr(a), we would like to achieve the following:\n\nsim(ta, ̃tp) ≥ sim(ta, ̃tn) + ξ,\n\n(6)\n\nwhere ξ = 0.5 is a constant margin for all pairs of negative. We should add a threshold ξ rather than just ensure sim(ta, ̃tp) ≥ sim(ta, ̃tn) to avoid trivial solution where features collapse together, i.e. sim(ta, ̃tp) = sim(ta, ̃tn).\n\nWe can incorporate low-rank constraint loss with margin into the supervised contrastive loss function in (2) by adding it after the cosine similarity term, giving us:\n\nLLRC = −\n\n(cid:88)\n\na=1···N\n\n1 |C1(a)|\n\n(cid:88)\n\np∈C1(a)\n\nlog\n\n(cid:80)\n\nexp (cid:0)sim(ta, ̃tp)(cid:1) j∈ {C1(a)∪Cr(a)}\\a exp (cid:0)sim(ta, ̃tj) + ξj\n\n(cid:1) .\n\n(7)\n\nwhere ξj = 0 if j ∈ C1(a), otherwise ξj = ξ; |C1(a)| = q1 is the number of elements in C1(a). The loss (7) is minimized when all positive pairs are correctly identified with condition (6) satisfied, thus enforcing our low-rank constraints. We set the top 5% of instances in a training batch as C1(a) and the bottom 5% as Cr(a). The derivation and analysis of (7) is provided in the Appendix A, C.\n\nThe total loss for self-supervised learning for feature embedding is:\n\nL = λLcon + (1 − λ)LLRC.\n\n(8)\n\nWithout the self-supervised contrastive loss (1), there is a chicken-and-egg issue that good features will not be learned and low-rank loss in (28) is not sufficiently good. Incorporating contrastive loss Lcon with LLRC is an incremental self-updating learning process. In our default setting where λ = 0.5, no unstable training is observed.\n\n3.2 GLOBAL FEATURE AGGREGATION\n\n3.2.1 PRELIMINARY\n\nWithout loss of generality, we take the binary MIL classification as an example. The learning task is to learn a nonlinear function from feature space X to label space Y = {1, 0} using the training data set {(X1, y1), · · · , (Xm, ym)}, where Xi = {xi,1, · · · , xi,mi} is a WSI; mi is the bag size of Xi; xi,j is an instance. The corresponding instance labels {yi,1, · · · yi,mi} are unknown, i.e.\n\nyi =\n\n(cid:26) 0, iff (cid:80)\n\n1, otherwise .\n\nj yi,j = 0; yi,j ∈ {0, 1}, j = 1, · · · mi\n\n(9)\n\nMIL processes a bag of instances with permutation invariance property, stating that the label of the bag remains unchanged regardless of the order of input instances on the bag (Ilse et al., 2018; Li\n\n4\n\nPublished as a conference paper at ICLR 2023\n\net al., 2021). A simple example of a permutation invariant model is a network that performs pooling over embedding extracted from the patches of a bag. Mathematically:\n\nlogits(Xi) = ρ (pool ({φ (xi,1) , · · · , φ (xi,mi )})) ,\n\n(10)\n\nwhere ’pool’ is a pooling operation; φ and ρ denote instance-level network and bag-level classifier, respectively. Attention-based pooling is commonly used (Ilse et al., 2018; Lu et al., 2021c; Tomita et al., 2019; Hashimoto et al., 2020):\n\nφ(xi,j) =\n\nexp (cid:8)W⊤ (tanh (Vxi,j) ⊙ sigm (Uxi,j))(cid:9) j=1 exp {W⊤ (tanh (Vxi,j) ⊙ sigm (Uxi,j))}\n\n(cid:80)mi\n\n,\n\n(11)\n\nwith learnable parameters W, V, and U.\n\n3.2.2 TRANSFORMER-BASED MIL\n\n(11) is a local attention network where the score φ(xi,j) of instance xi,j only depends on Eq. the instance itself. We aim to explore the dependence and interaction among all instances. One straightforward approach is the application of transformer.\n\nThe transformer encoder consists of alternating layers of multi-headed attention and MLP blocks. Here, we denote the feature matrix of the feature bag Xi as X1 i = [xi,1, · · · , xi,mi]⊤. An attention head maps queries Q ∈ Rmi×d to outputs using mi key-value pairs K ∈ Rmi×d, V ∈ Rmi×d, and d is the query/key dimension:\n\n(cid:40)\n\nheadh(Xl where Qh = Xl\n\ni ) = Attention(Qh, Kh, Vh) = softmax h,l, Vh = Xl\n\nh,l, Kh = Xl\n\niWQ\n\niWK\n\n(cid:16)\n\n√\n\nQhK⊤\n\nh /\n\n(cid:17)\n\nd\n\nVh\n\niWV\n\nh,l, h = 1, · · · , H,\n\n(12)\n\nh,l, WK\n\nwhere WQ h,l are learnable; l = 1, · · · , k is the index of the transformer layer; k is the total number of layers. Transformer uses multi-head attention to project Q; K; V onto H different vectors and then concatenate all attention outputs:\n\nh,l, WV\n\n(cid:26) MHA(Xl\n\ni = MHA (cid:0)LN (cid:0)Xl\n\ni ) = concat (head1, · · · , headH ) (cid:1)(cid:1) + Xl i ,\n\ni\n\nˆXl\n\nwhere LN is the layer norm. The output layer is MLP with a skip connection:\n\nXl+1\n\ni = MLP\n\n(cid:16)\n\nLN\n\n(cid:17)(cid:17)\n\n(cid:16) ˆXl\n\ni\n\n+ ˆXl i .\n\n(13)\n\n(14)\n\nConsidering a large number of instances in each bag (hundreds of thousands), one obstacle with (cid:1). Despite the linear thetransformer for MIL is the quadratic time and memory complexity O (cid:0)m2 oretical complexity with some approximations like Nystromformer (Xiong et al., 2021), Linformer (Wang et al., 2020a), or Performer (Choromanski et al., 2020), it overlooks the innate characteristic of input instances.\n\ni\n\n3.2.3\n\nITERATIVE LOW-RANK ATTENTION MIL\n\nMedical image including WSI is extensively high-dimensional in its raw form. As such, it is effective to explore the hidden structures in the forms of low-rank matrices of high-dimensional data (Wang et al., 2020b; Li et al., 2018; 2020). We thus introduce a learnable low-rank latent matrix L ∈ Rr×d to interact with all input instances as the proposed ILRA-MIL shown in Fig. 2. One basic module of the network is the cross-attention (CAtt), defined as:\n\n(cid:40)\n\nCAtt(L, Xl where Q = LWQ\n\ni ) = Attention(Q, K, V) = softmax iWV iWK l .\n\nl , V = Xl\n\nl , K = Xl\n\n(cid:16)\n\n√\n\nQK⊤/\n\n(cid:17)\n\nd\n\nV\n\n(15)\n\nNote that L is a unified matrix for all layers to keep the low-rank consistency for different layers. As shown in the right-hand side of Fig. 2, we also use a unified layer with cross-attention and Gated Linear United (GLU), named Gated Attention Block (GAB):\n\n(cid:26) GAB(L, Xl\n\ni ) = (U (cid:74) ˆV)WO\n\nl\n\nU = φU (LWU\n\nl ), ˆV = CAtt(L, Xl i )\n\n(16)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: ILRA-MIL iterates over k IRLA layers. Each layer consists of two GAB blocks. The first i ∈ Rmi×d to low-rank space by attending to the latent vectors GAB block projects input instance X1 L ∈ Rr×d, r < mi and the second GAB recovers the input dimension. ω represents softmax. The output layer uses non-local pooling to make predictions. Layer normalization is omitted for brevity.\n\nwhere (cid:74) stands for element-wise multiplication ; φU is Sigmoid Linear Units SiLU (Elfwing et al., 2018; Hua et al., 2022); WO l are linear transforms. The inputs of GAB are not permutation invariant as the first is the query and the second is the key-value. An ILRA block consists of:\n\nl , WU\n\nP = GABf(L, Xl\n\ni ), Xl+1\n\ni = GABb(Xl\n\ni , P).\n\n(17)\n\nEq. (17) is analogous to low-rank projection or auto-encoder models. GABf first projects the highi to the low-rank space L. Then the projection result P ∈ Rr×d is reconstructed to dimensional Xl high-dimensional space Xl+1\n\ni with GABb where the query is Xl\n\ni and key-value is P.\n\nThere are some desirable properties of ILRA-MIL. (i) The latent vectors L encode global features that help to explain input instances. For example, in the cancer subtyping problem for computational pathology, the latent vectors could be approximately some mutual and universal information of key cancerous regions so that the ILRA module can compare instances in the query indirectly through L to all inputs. (ii) The Q-K-V pair is not longer symmetric as in MHA because for the shapes L ∈ Rr×d, K ∈ Rmi×d, V ∈ Rmi×d, r ≪ mi. Thus, the complexity of cross-attention operation significantly is reduced from quadratic O(m2\n\ni ) to linear O(rmi). We set r = 64 by default.\n\nConstraining the latent vectors to be low-rank may restrict the network’s ability to capture all of the necessary details from the input instances. To improve expressivity, the model stacks k (k = 4 by default) ILRA layers to extract information from the input instances:\n\n ̃Xi = ILRA(ILRA(X1\n\n(cid:124)\n\n(cid:123)(cid:122) k layers\n\ni ) · · · ) (cid:125)\n\n,\n\n(18)\n\nwhere LN should be applied before the input of each layer. ̃Xi = { ̃x1, ̃x2, · · · , ̃xmi} encodes cross-instance correlations in the bag. A bag feature xb ∈ R1×d is obtained through max pooling over ̃Xi. Then, a trainable linear classifier ρ is used to conduct non-local pooling at the output layer:\n\nlogits( ̃Xi) = ρ(\n\nmi(cid:88)\n\nj=1\n\nwj · ̃xj), wj =\n\nexp (xb · ̃xj) q=1 exp (xb · ̃xq)\n\n(cid:80)mi\n\n.\n\n(19)\n\n4 EXPERIMENTS\n\n4.1 DATASET\n\nCAMELYON16 is a public dataset for metastasis detection in breast cancer (binary classification), including 270 training slides and 130 test slides. A total of about 1.5 million patches at ×10 magnification are obtained. TCGA-NSCLC includes two subtype projects (binary classification), i.e., Lung Squamous Cell Carcinoma (TGCA-LUSC) and Lung Adenocarcinoma (TCGA-LUAD), for a total of 993 diagnostic WSIs, including 507 LUAD slides from 444 cases and 486 LUSC slides from 452 cases. We obtain 3.4 million patches in total at ×10 magnification. PANDA is the largest prostate biopsy public dataset to date (Bulten et al., 2022). We use 4369 slides from Karolinska Institute for training. The independent test set from Radboud University has 2591 slides. A total of 1.1 million patches at ×10 magnification are obtained. More details are introduced in the Appendix.\n\n6\n\nNon-localPoolingLogitsinput instance Xi1latent matrix L213123VKQωQKTVUDense21GABILRAklayersCAttGAB!GAB\"Published as a conference paper at ICLR 2023\n\nTable 1: Classification Results on Benchmarks.\n\nMean-pooling Max-pooling ABMIL MIL-RNN CLAM-SB CLAM-MB DSMIL DSMIL+SimCLR TransMIL DTFD-MIL (MaxS) DTFD-MIL (AFS) ILRA-MIL (ours) ILRA-MIL + LRC (ours)\n\nCAMELYON16 AUC Acc 0.6755 0.6511 0.8169 0.7674 0.8503 0.8527 0.8580 0.8449 0.8709 0.8682 0.8779 0.8604 0.8944 0.8759 0.9175 0.8867 0.8769 0.8449 0.9103 0.8543 0.9010 0.9401 0.9278 0.8992 0.9649 0.9218\n\nTCGA-NSCLC AUC Acc 0.8401 0.7282 0.9263 0.8593 0.9205 0.8384 0.9107 0.8619 0.9307 0.8632 0.9377 0.8492 0.9439 0.8690 0.9551 0.9048 0.9303 0.8565 0.9097 0.8701 0.9612 0.8941 0.9592 0.9004 0.9763 0.9213\n\nPANDA\n\nAcc 0.5691 0.6100 0.6834 NA 0.6648 0.6760 0.6737 0.7017 0.6720 0.6334 0.6573 0.7094 0.7287\n\nkappa 0.4422 0.5830 0.5998 NA 0.5782 0.6067 0.5562 0.5837 0.5638 0.5462 0.5437 0.6236 0.6562\n\n4.2\n\nIMPLEMENTATION DETAILS\n\nTraining. In CAMELYON16, the 270 training WSIs are split into approximately 90% training and 10% validation and tested on the official test set. In PANDA, we split the 4219 slides from Karolinska into 80% training and 20% validation and tested on the 2591 slides from Radboud. For TCGA datasets, we first ensured that different slides from one patient do not exist in both the training and test sets, and split the data in the ratio of training:validation:test = 60:15:25. For self-supervised learning, we use ResNet50 to encode 224 × 224 images into 1024-dimensional vectors. The same training data is used to develop feature embedding with LRC and feature aggregator ILRA-MIL.\n\nEvaluation. For the evaluation metrics, we used accuracy and area under the curve (AUC) scores to evaluate the classification performance, where the accuracy was calculated with a threshold of 0.5 in all experiments. The multi-class PANDA is scored based on Cohen’s kappa.\n\nBaseline methods include mean/max-pooling and deep MIL models, i.e., ABMIL (Ilse et al., 2018), DSMIL (Li et al., 2021), CLAM-SB / CLAM-MB (Lu et al., 2021c), MIL-RNN (Campanella et al., 2019), transMIL (Shao et al., 2021), and DTFD-MIL (Zhang et al., 2022).\n\n5 RESULTS\n\n5.1 RESULTS ON CLASSIFICATION\n\nAll results are provided in Table 6. ’DSMIL+SimCLR’ denotes DSMIL with SimCLR features as reported in (Li et al., 2021). Other baselines use ImageNet pre-trained features without notice. In all cases, ILRA with LRC feature embedding consistently improves over ImageNet pre-trained feature embedding, as the statistic in the last row shows.\n\nIn CAMELYON16, tumors are minor regions in positive slides (averagely < 10% per slide), resulting in a highly imbalanced distribution of positive and negative instances in a bag. Attention-based methods all outperform the traditional mean or max pooling operators. Nonlocal poolings, including DSMIL and TransMIL outperform attention pooling with a nonlocal operator that models the crossinstance correlation. DTFD-MIL is the best-performed competing method which is particularly designed to address the small sample cohorts. The proposed ILRA-MIL processes cross-instance correlation and the AUC score was at least 4.99% higher than CLAM-MB, which only local instance for aggregation.\n\nIn TCGA-NSCLC, positive slides contain relatively large areas of the tumour region (average total cancer area per slide > 80%). As a result, both the max pooling and attention pooling operators work pretty well in this scenario. Non-local pooling methods are consistently stable, and ILRAMIL performed better than all the other competing methods, achieving 1.53% improvement in AUC and 1.51% in accuracy, compared with the best competing results.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Ablation on Different Pretrained Models.\n\nImageNet SimCLR (Chen et al., 2020a) BYOL (Grill et al., 2020) SimSiam (Chen & He, 2021) MoCov3 (Chen et al., 2020c) LRC (ours)\n\nILRA-MIL\n\nCLAM-SB\n\nAcc 0.8992 0.9082 0.9002 0.9032 0.9158 0.9218\n\nAUC 0.9278 0.9392 0.9330 0.9354 0.9490 0.9649\n\nAcc 0.8682 0.8895 0.8701 0.8837 0.9021 0.9088\n\nAUC 0.8709 0.9106 0.8902 0.8962 0.9123 0.9377\n\nFigure 3: Probability distribution with MoCo-V3 and LRC.\n\nFor PANDA, as MIL-RNN does not work for multi-classification problems, we exclude it from the comparison result. PANDA is unbalanced distributed in cancer subtypes, and it is challenging to differentiate glandular patterns with intermediate morphological structures (Nagpal et al., 2020).\n\nILRA-MIL can also be applied to multi-class problems with unbalanced data, and it can be observed that the best results are achieved in both accuracy and kappa. For comparison, existing clinicalgrade AI system trained pixel-wise annotations from highly urological pathologists scores from 0.62 (Bulten et al., 2020) to 0.66 (Tolkach et al., 2020).\n\n5.2 ABLATIONS ON LRC\n\nTo demonstrate the effectiveness of the proposed clustering-constrained contrastive loss, we compare its performance with alternative contrastive learning: SimCLR (Chen et al., 2020a), BYOL (Grill et al., 2020), SimSiam (Chen & He, 2021), and MoCoV3 (Chen et al., 2020c), and an ImageNet pre-trained model, as shown in Table 2. The same ILRA-MIL model is used to evaluate the ACC and AUC performance. Unsurprisingly, all self-supervised features significantly bootstrap the performance against the ImageNet pre-trained features.\n\nMoCo-V3 and SimCLR outperform BYOL and SimSiam without negative samples. The proposed LRC achieves 1.59% AUC improvement over MoCo-v3. Similar results also apply to CLAM-SB, as shown in the table. Fig. 3 shows the predicted probability on the CAMELYON16 test set using ILRA-MIL trained with MoCo-v3 and LRC features. As we set 0.5 as the classification threshold, we can observe that with LRC features, there are fewer false positives and false negatives samples compared with the probability distribution of MoCo-V3.\n\n5.3 PARAMETER ANALYSIS AND ABLATIONS ON ILRA-MIL\n\nWe conduct some ablations on ILRA-MIL in terms of some key modules: (i) low-rank latent vectors attention in (15); (ii) non-local pooling in (19); (iii) iterative attention mechanism in (18). Ablation studies are performed on the CAMELYON16 dataset with ImageNet features; see Table 3.\n\n(1) The default rank of L in Eq. (15) is r = 64. We adjust the rank from 32 to 128, and the result demonstrates that with a large-enough vector rank, it can attend all input instances with negligible loss of information. Then, we compare it with the self-attention module.\n\n8\n\n0.00.20.40.60.81.0Prob0246810121416CountMoCo-v3negativepositive0.00.20.40.60.81.0Prob02468101214CountLRCnegativepositivePublished as a conference paper at ICLR 2023\n\nTable 3: Parameter Analysis and Ablations on ILRA-MIL\n\nSettings rank r = 16 rank r = 32 rank r = 64 rank r = 128 full self-attention low-rank attention max pooling local att. Pooling nonlocal att. pooling\n\n# params AUC 2.97 M 0.9205 2.99 M 0.9231 3.02 M 0.9278 3.09 M 0.9279 2.64 M 0.8127 3.02 M 0.9278 2.76 M 0.8061 5.01 M 0.8612 3.02 M 0.9278\n\n(i)\n\n(ii)\n\n(iii)\n\n(iv)\n\nSettings iteration k = 1 iteration k = 2 iteration k = 4 iteration k = 6 iteration k = 8\n\n# params AUC 1.39 M 0.9102 1.94 M 0.9221 3.02 M 0.9278 4.11 M 0.8947 5.19 M 0.8418\n\nFigure 4: Heatmap visualization of ”test 075” and ”test 026” for CLAM with and without LRC.\n\n(2) We cannot directly apply the full self-attention considering the large bag size, and instead, we use the Nystrom transformer as an approximation. The same number of heads and layers are used for evaluation. The results indicated that low-rank attention achieves 0.9278 AUC, outperforming 0.8127 AUC of full self-attention by a large margin. Although with linear approximation, full self-attention involves excessively redundant and task-irrelevant interactions among instances and is challenging to optimize where only a tiny amount of slide-level labels are available.\n\n(3) After ILRA iteration in (17), non-local pooling is used with (19) to aggregate global feature. We ablate it with the commonly used max pooling and local attention pooling in (11). Remarkably, nonlocal pooling can improve max pooling and local attention pooling by 12.17% and 6.66% AUC.\n\n(4) ILRA-MIL can make a deeper network through iterative attention. As the number of iterations increases, the model performance growth tends to level off, which indicates that it is sufficient to characterize cross-instance correlations in the dataset. The iteration number greater than k = 4 leads to a significant decrease in performance caused by the over-fitting dataset.\n\n5.4\n\nINTERPRETABILITY\n\nOur feature embedding LRC boosts the performance of CLAM-SB (see Table 2), and it can also enhance interpretability. We use the trained CLAM-SB model with LRC features to draw the predicted heatmap as shown in Fig. 4. The heatmaps show remarkable consistency with expert annotation, especially for ”test 075” where the ROIs only occupied a small area; the most significant regions are located and identified. We show more visual comparisons in the Appendix.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n6 CONCLUSION\n\nIn this paper, we address the problem of WSI classification by optimizing the feature embedding and feature aggregation with low-rank properties. We improve the vanilla contrastive loss with additional low-rank constraints to collect more positive samples for contrast. We also devise an iterative lowrank attention feature aggregator to make efficient cross-instance correlations. All these designs boost the performance across various benchmarks, as the results show. One limitation of our model is that it has not been validated on multi-center larger-scale clinical datasets. In addition, ILRA-MIL cannot directly provide a local attention score for each instance, which might hinder an intuitive clinical analysis of each patch image.\n\nREFERENCES\n\nStephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends® in Machine learning, 3(1):1–122, 2011.\n\nWouter Bulten, Hans Pinckaers, Hester van Boven, Robert Vink, Thomas de Bel, Bram van Ginneken, Jeroen van der Laak, Christina Hulsbergen-van de Kaa, and Geert Litjens. Automated deep-learning system for gleason grading of prostate cancer using biopsies: a diagnostic study. The Lancet Oncology, 21(2):233–241, 2020.\n\nWouter Bulten, Kimmo Kartasalo, Po-Hsuan Cameron Chen, Peter Str ̈om, Hans Pinckaers, Kunal Nagpal, Yuannan Cai, David F Steiner, Hester van Boven, Robert Vink, et al. Artificial intelligence for diagnosis and gleason grading of prostate cancer: the panda challenge. Nature medicine, pp. 1–10, 2022.\n\nGabriele Campanella, Matthew G Hanna, Luke Geneslaw, Allen Miraflor, Vitor Werneck Krauss Silva, Klaus J Busam, Edi Brogi, Victor E Reuter, David S Klimstra, and Thomas J Fuchs. Clinical-grade computational pathology using weakly supervised deep learning on whole slide images. Nature medicine, 25(8):1301–1309, 2019.\n\nEmmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?\n\nJournal of the ACM (JACM), 58(3):1–37, 2011.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020a.\n\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:22243–22255, 2020b.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021.\n\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\n\nImproved baselines with momentum\n\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020c.\n\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.\n\nChing-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. Advances in neural information processing systems, 33:8765–8775, 2020.\n\nOzan Ciga, Tony Xu, and Anne Louise Martel. Self supervised contrastive learning for digital\n\nhistopathology. Machine Learning with Applications, 7:100198, 2022.\n\nYang Cong, Ji Liu, Junsong Yuan, and Jiebo Luo. Self-supervised online metric learning with low rank constraint for scene categorization. IEEE Transactions on Image Processing, 22(8):3179– 3191, 2013.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network\n\nfunction approximation in reinforcement learning. Neural Networks, 107:3–11, 2018.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence\n\nembeddings. arXiv preprint arXiv:2104.08821, 2021.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271–21284, 2020.\n\nNoriaki Hashimoto, Daisuke Fukushima, Ryoichi Koga, Yusuke Takagi, Kaho Ko, Kei Kohno, Masato Nakaguro, Shigeo Nakamura, Hidekata Hontani, and Ichiro Takeuchi. Multi-scale domain-adversarial multiple-instance cnn for cancer subtype classification with unannotated histopathological images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3852–3861, 2020.\n\nWeizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv\n\npreprint arXiv:2202.10447, 2022.\n\nMaximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learn-\n\ning. In International conference on machine learning, pp. 2127–2136. PMLR, 2018.\n\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661–18673, 2020.\n\nBin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14318–14328, 2021.\n\nHuafeng Li, Xiaoge He, Dapeng Tao, Yuanyan Tang, and Ruxin Wang. Joint medical image fusion, denoising and enhancement via discriminative low-rank sparse dictionaries learning. Pattern Recognition, 79:130–146, 2018.\n\nHuafeng Li, Xiaoge He, Zhengtao Yu, and Jiebo Luo. Noise-robust image fusion with low-rank sparse decomposition guided by external patch prior. Information Sciences, 523:14–37, 2020.\n\nZhaowen Li, Yousong Zhu, Fan Yang, Wei Li, Chaoyang Zhao, Yingying Chen, Zhiyang Chen, Jiahao Xie, Liwei Wu, Rui Zhao, et al. Univip: A unified framework for self-supervised visual pre-training. arXiv preprint arXiv:2203.06965, 2022.\n\nGuangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, and Yi Ma. Robust recovery IEEE transactions on pattern analysis and\n\nof subspace structures by low-rank representation. machine intelligence, 35(1):171–184, 2012.\n\nMengkang Lu, Yongsheng Pan, Dong Nie, Feihong Liu, Feng Shi, Yong Xia, and Dinggang Shen. Smile: Sparse-attention based multiple instance contrastive learning for glioma sub-type clasIn MICCAI Workshop on Computational Pathology, pp. sification using pathological images. 159–169. PMLR, 2021a.\n\nMing Y Lu, Richard J Chen, Jingwen Wang, Debora Dillon, and Faisal Mahmood. Semi-supervised histology classification using deep multiple instance learning and contrastive predictive coding. arXiv preprint arXiv:1910.10825, 2019.\n\nMing Y Lu, Tiffany Y Chen, Drew FK Williamson, Melissa Zhao, Maha Shady, Jana Lipkova, and Faisal Mahmood. Ai-based pathology predicts origins for cancers of unknown primary. Nature, 594(7861):106–110, 2021b.\n\nMing Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J Chen, Matteo Barbieri, and Faisal Mahmood. Data-efficient and weakly supervised computational pathology on whole-slide images. Nature biomedical engineering, 5(6):555–570, 2021c.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nKunal Nagpal, Davis Foote, Fraser Tan, Yun Liu, Po-Hsuan Cameron Chen, David F Steiner, Naren Manoj, Niels Olson, Jenny L Smith, Arash Mohtashamian, et al. Development and validation of a deep learning algorithm for gleason grading of prostate cancer from biopsy specimens. JAMA oncology, 6(9):1372–1380, 2020.\n\nNikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In International Conference on Machine Learning, pp. 5628–5637. PMLR, 2019.\n\nZhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian Zhang, Xiangyang Ji, et al. Transmil: Transformer based correlated multiple instance learning for whole slide image classification. Advances in Neural Information Processing Systems, 34, 2021.\n\nYash Sharma, Aman Shrivastava, Lubaina Ehsan, Christopher A Moskaluk, Sana Syed, and Donald Brown. Cluster-to-conquer: A framework for end-to-end multi-instance learning for whole slide image classification. In Medical Imaging with Deep Learning, pp. 682–698. PMLR, 2021.\n\nKarin Stacke, Jonas Unger, Claes Lundstr ̈om, and Gabriel Eilertsen.\n\nLearning representations with contrastive self-supervised learning for histopathology applications. arXiv preprint arXiv:2112.05760, 2021.\n\nPeter Str ̈om, Kimmo Kartasalo, Henrik Olsson, Leslie Solorzano, Brett Delahunt, Daniel M Berney, David G Bostwick, Andrew J Evans, David J Grignon, Peter A Humphrey, et al. Artificial intelligence for diagnosis and grading of prostate cancer in biopsies: a population-based, diagnostic study. The Lancet Oncology, 21(2):222–232, 2020.\n\nYuri Tolkach, Tilmann Dohmg ̈orgen, Marieta Toma, and Glen Kristiansen. High-accuracy prostate\n\ncancer pathology using deep learning. Nature Machine Intelligence, 2(7):411–418, 2020.\n\nNaofumi Tomita, Behnaz Abdollahi, Jason Wei, Bing Ren, Arief Suriawinata, and Saeed Hassanpour. Attention-based deep neural networks for detection of cancerous and precancerous esophagus tissue on histopathological slides. JAMA network open, 2(11):e1914645–e1914645, 2019.\n\nMadeleine Udell, Corinne Horn, Reza Zadeh, Stephen Boyd, et al. Generalized low rank models.\n\nFoundations and Trends® in Machine Learning, 9(1):1–118, 2016.\n\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\n\nwith linear complexity. arXiv preprint arXiv:2006.04768, 2020a.\n\nYongjun Wang, Baiying Lei, Ahmed Elazab, Ee-Leng Tan, Wei Wang, Fanglin Huang, Xuehao Gong, and Tianfu Wang. Breast cancer image classification via multi-network features and dualnetwork orthogonal low-rank learning. IEEE Access, 8:27779–27792, 2020b.\n\nJohn Wright and Yi Ma. High-dimensional data analysis with low-dimensional models: Principles,\n\ncomputation, and applications. Cambridge University Press, 2022.\n\nYunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr ̈omformer: A nyst ̈om-based algorithm for approximating self-attention. In Proceedings of the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, volume 35, pp. 14138. NIH Public Access, 2021.\n\nHongrun Zhang, Yanda Meng, Yitian Zhao, Yihong Qiao, Xiaoyun Yang, Sarah E Coupland, and Yalin Zheng. Dtfd-mil: Double-tier feature distillation multiple instance learning for histopathology whole slide image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18802–18812, 2022.\n\nYangmuzi Zhang, Zhuolin Jiang, and Larry S Davis. Learning structured low-rank representations for image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 676–683, 2013.\n\nXiaowei Zhou, Can Yang, Hongyu Zhao, and Weichuan Yu. Low-rank modeling and its applications\n\nin image analysis. ACM Computing Surveys (CSUR), 47(2):1–33, 2014.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA LOW-RANK PROPERTY OF WSI\n\nHigh-dimensional WSI data bring great challenges to data analysis. But fortunately, the highdimensional WSI data often lie in low-dimensional subspace, consistent with findings including natural images in computer vision, documents in natural language processing(Udell et al., 2016; Zhang et al., 2013; Zhou et al., 2014; Wright & Ma, 2022). Pursuing the low-rank property of highdimensional data is to identify the intrinsic manifold or physical mechanisms from which the data are generated.\n\nGiven a bag of feature embedding from a WSI, it can be formulated as a data matrix X = [X1, · · · , Xr]⊤ where Xi corresponds to latent class i, r is the total number of latent classes. Ideally, X can be decomposed into a low-rank component DB and a sparse error component E, i.e., X = DB + E with respect to dictionary, the optimal representation matrix B for X should be block-diagonal:\n\n\n\n\n\n \n\n\nB1 0\n0 B2\n\n0 0\n\n0 0\n\n0 0\n\n0 0\n. . . 0\n0 Br\n\n \n\n\n(20)\n\nThe space matrix D = [D1, D2, ...Dr] contains r sub-space. An example of optimal decomposition for feature embedding is illustrated in Fig. (5). For example, data X = [X1, X2, X3] contains features from 3 classes, where X1 contains 3 samples x1, x2, x3, X2 contains 4 samples x4, x5, x6, x7, and X3 contains 3 samples x8, x9, x10. D has 3 sub-space, and each has 2 support items.\n\nFigure 5: Example of an optimal low-rank decomposition.\n\nMathematically, the decomposition is achieved by optimizing: minB,E ∥B∥∗ + λ∥E∥1 s.t X = DB + E\n\n(21)\n\nWe construct 270 data matrices with CAMLEYON16 training WSIs for low-rank property analysis. The size of data matrix is Rm×d, where m is the bag size of a WSI and d is the fix-dimension of embedding depending on the encoder, e.g., d = 1024 for ResNet50 backbone. The low-rank decomposition problem in (21) can be optimized by ADMM algorithm (Alternating Direction Method of Multipliers) (Cand`es et al., 2011; Boyd et al., 2011).\n\nWe plot the histogram of the rank of all matrices in Fig. (6. The average rank of ImageNet feature embedding is 349, much smaller than the full-rank 1024. Remarkably, the average rank of self-supervised learning feature BYOL (Grill et al., 2020), MoCo (Chen & He, 2021), and the proposed LRC can further reduce to 218, 195, and 181, respectively. As the Table 1 in the main paper shows, the classification performance AUC with the same ILRA-MIL model using ImageNet, BYOL, MoCo, and LRC features is 0.9278, 0.9330, 0.9490, 0.9649, respectively, i.e.:\n\navg. rank: classification AUC:\n\nImageNet > BYOL > MoCo > LRC ImageNet < BYOL < MoCo < LRC\n\n(22)\n\nEven without low-rank constraints, BYOL and MoCo tend to produce features with lower ranks than ImageNet. Also, Fig. (6) (d) indicates that the distribution of all WSIs feature embedding is more\n\n13\n\nPublished as a conference paper at ICLR 2023\n\ncompact than ImageNet. This empirical evidence implicitly shows that low-rank features are very likely to be beneficial to WSI representation.\n\n(a) ImageNet features, avg. rank = 349.\n\n(b) BYOL features, avg. rank = 218.\n\n(c) MoCo features, avg. rank =195.\n\n(d) LRC features (proposed), avg. rank=181.\n\nFigure 6: The histogram of the rank of feature embedding. Each sample in the histogram is a data matrix by embedding each patch in the WSI as a feature vector.\n\nB DERIVATION OF LLRC\n\nThe derivation of LLRC incorporates the contrastive margin into the standard SupCon loss LSupCon for positive pair classification. Starting from a triplet example with an anchor sample ta, positive sample ̃tp, and negative sample ̃tn. The sigmoid function to identify the positive pairs is:\n\nexp (cid:0)sim(ta, ̃tp)(cid:1) exp (cid:0)sim(ta, ̃tp)(cid:1) + exp (cid:0)sim(ta, ̃tn)(cid:1)\n\nThe positive pair is correctly classified if:\n\nsim(ta, ̃tp) ≥ sim(ta, ̃tn)\n\n(23)\n\n(24)\n\nWe incorporate the margin constraint into the classification boundary so that the positive pair is correctly classified only if:\n\nsim(ta, ̃tp) ≥ sim(ta, ̃tn) + ξ.\n\nThe sigmoid in (23) is modified accordingly:\n\nexp (cid:0)sim(ta, ̃tp)(cid:1) exp (cid:0)sim(ta, ̃tp)(cid:1) + exp (cid:0)sim(ta, ̃tn) + ξ(cid:1)\n\n14\n\n(25)\n\n(26)\n\nPublished as a conference paper at ICLR 2023\n\nTherefore, the cross-entropy loss to identify the positive is:\n\n− log\n\nexp (cid:0)sim(ta, ̃tp)(cid:1) exp (cid:0)sim(ta, ̃tp)(cid:1) + exp (cid:0)sim(ta, ̃tn) + ξ(cid:1)\n\n(27)\n\nGiven an anchor ta, we get r subspace Cb(a), b = 1, · · · , r as stated in the low-rank representation Eq. (4). We can intuitively consider that each subspace corresponds to a latent class. We thus would like to discriminate between different subspaces, e.g. C1(a), Cr(a), which are the least and most-distant subspaces to anchor ta.\n\nWe extends (27) to C1(a) positive and Cr(a) negative pairs in a sample batch with SupCon (Khosla et al., 2020), giving us:\n\nLLRC = −\n\n(cid:88)\n\na=1···N\n\n1 |C1(a)|\n\n(cid:88)\n\nlog\n\np∈C1(a)\n\nexp (cid:0)sim(ta, ̃tp)(cid:1) j∈ {C1(a)∪Cr(a)}\\a exp (cid:0)sim(ta, ̃tj) + ξj\n\n(cid:80)\n\n(cid:1) .\n\n(28)\n\nwhere ξj = 0 if j ∈ C1(a), otherwise ξj = ξ.\n\nC HOW SENSITIVE IS THE HYPER-PARAMETER IN LLRC?\n\nEq. (28) aims to push apart two subspaces. We set the top 5% of instances in a training batch as C1(a) and the bottom 5% as Cr(a) by default. The percentage of C1(a) controls the estimated positive samples in a minibatch given anchor ta, and helps strike a balance between the benefits it brings with more true positive samples and the inverse effects of using false positive samples.\n\nTable 4 shows the WSI classification evaluations on CAMELYON16 and TCGA-NSCLC datasets for ILRA-MIL models trained with different choices of C1(a), ranging from 1% to 10%. The best result is highlighted in bold and the second best is underlined. The results show that 5% achieves relatively optimal performance on both datasets. A larger percentage of 10% or a smaller percentage of 1% generally leads to worse performance. We find the sensitivity of the hyperparameter is reduced in the range of 3% to 7%.\n\nTable 4: WSI classification evaluations for models trained with different choices of C1(a)\n\nCAMELYON16\n\nTCGA-NSCLC\n\nAccuracy 0.8899 ± 0.0365 0.9287 ± 0.0090 0.9218 ± 0.0113 0.9084 ± 0.0215 0.8884 ± 0.0105\n\nAUC 0.9260 ± 0.0136 0.9556 ± 0.0098 0.9649 ± 0.0844 0.9521 ± 0.0095 0.9137 ± 0.0139\n\nAccuracy 0.8897 ± 0.0207 0.9205 ± 0.0224 0.9213 ± 0.0173 0.9200 ± 0.0204 0.9113 ± 0.0197\n\nAUC 0.9551 ± 0.0191 0.9710 ± 0.0185 0.9763 ± 0.0149 0.9780 ± 0.0234 0.9663 ± 0.0188\n\n1% 3% 5% 7% 10%\n\nD TRAINING DETAILS\n\nThe training data splits are described in Section E. For each dataset, we use the same training data to first develop a self-supervised learning model to conduct local feature embedding and then train MIL models to implement global feature aggregation for classification.\n\nOne should pay attention not to exposing test datasets for the development of a feature embedding model, although no labels are used. For example, if the test set of CAMELYON16 is used by MoCov3 for feature embedding pretraining, we can achieve 0.9885 AUC classification performance on the test set with CLAM-MB. This exceptionally high performance is caused by data leakage.\n\nD.1 SELF-SUPERVISED TRAINING DETAILS\n\nWe train self-supervised learning models to conduct local feature embedding. We closely follow MoCo-V3 (Chen et al., 2020c) and use the same training hyper-parameters. The data augmentation setting: a 224×224-pixel crop is taken from a randomly resized image, and then undergoes random\n\n15\n\nPublished as a conference paper at ICLR 2023\n\ncolor jittering, random horizontal flip, and random grayscale conversion. For all methods, we use an initial learning rate of 1.5e−4. We use AdamW as the optimizer and adopt a learning rate warmup for 20 epochs to alleviate instability. Each model is optimized on 16 Nvidia V100 GPUs with a cosine learning rate decay schedule and a mini-batch size of 4096. We train for 200 epochs for CAMELYON16, TCGA-NSCLC, and PANDA. The training takes about 4 days.\n\nD.2 MIL TRAINING DETAILS\n\nMIL models, including baseline and ours, are trained on a single Nvidia V100 GPU. The training of ILRA-MIL take less than 1 hour for all datasets with an Nvidia V100 GPU. They are optimized end-to-end with Adam optimizer with a batch size of 1 and a learning rate of 1e−4 for 200 epochs. The Adam optimizer has parameters β1 = 0.9, β2 = 0.95, and ε = 1e−8.\n\nE DATASET\n\nEach WSI is cropped into a series of 224 × 224 non-overlapping patches using a binary mask for the tissue regions which is computed based on thresholding the saturation channel of the image.\n\nCAMELYON161 is a public dataset for metastasis detection in breast cancer (2-level classification), including 270 training sets and 130 test sets. A total of about 1.5 million patches at ×10 magnification are obtained after prep-process.\n\nTCGA-NSCLC2 includes two subtype projects (2-level classification), i.e., Lung Squamous Cell Carcinoma (TGCA-LUSC) and Lung Adenocarcinoma (TCGA-LUAD), for a total of 993 diagnostic WSIs, including 507 LUAD slides from 444 cases and 486 LUSC slides from 452 cases. We obtain 3.4 million patches in total at ×10 magnification.\n\nPANDA3 is the largest prostate biopsy public dataset to date (Bulten et al., 2022). We only use slides with pure and unequivocal patterns (from 0+0, 3+3, 4+4, or 5+5 slides) where the interobserver variability was normally low (Tolkach et al., 2020; Bulten et al., 2022; Str ̈om et al., 2020), making it a 4-level classification problem. We use 4369 slides (1924 slides of 0+0, 1813 slides of 3+3, 466 slides of 4+4, 166 slides of 5+5) from Karolinska Institute for training, and 2591 slides (962 slides of 0+0, 852 slides of 3+3, 660 slides of 4+4, 111 slides of 5+5) from Radboud University for testing. A total of 1.1 million patches at ×10 magnification are obtained.\n\nF INFERENCE EFFICIENCY\n\nWe evaluate the inference runtime and MACs (multiply-accumulate operations) of the proposed model. We use the CAMELYON16 test set that contains 130 WSIs and the average bag size is 1600 at ×10 magnification. The evaluation involves data preprocessing including segmentation and patching, feature embedding using the LRC pre-trained ResNet50, and slide-level prediction with ILRA-MIL. The average inference run time for each WSI is represented in Table 5. The data preprocessing consume most of the time cost and this module is not accelerated by GPU. Feature embedding module converts 224 × 224 images into 1024-dimensional vectors and its MACs are relatively high. The feature aggregation module operating on embedding vectors is efficient and only takes about 4.4 ms.\n\nTable 5: Average Runtime Per Slide on CAMELYON16 Using a P40 GPU.\n\nModules Data Preprocessing (non-parametric) Feature Embedding (RestNet50) Feature Aggregation (ILRA-MIL)\n\nruntime MACs 183.3 s 3.6 s 4.4 ms\n\n- 13.22 T 2.89 G\n\n1https://camelyon16.grand-challenge.org/Data/ 2https://portal.gdc.cancer.gov/ 3https://www.kaggle.com/competitions/prostate-cancer-grade-assessment/data\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nG CLASSIFICATION RESULTS ON BENCHMARKS\n\nThe following Table 6 and Table 7 are extensions of Table 1 in the main paper with the standard deviation. Each experiment is conducted for 5 runs with respect to different random startup seeds on the same data splits.\n\nTable 6: Results on Benchmarks CAMELYON16 and TCGA-NSCLC.\n\nMean-pooling Max-pooling ABMIL MIL-RNN CLAM-SB CLAM-MB DSMIL DSMIL + SimCLR TransMIL DTFD-MIL (MaxS) DTFD-MIL (AFS) ILRA-MIL ILRA-MIL + LRC\n\nCAMELYON16\n\nTCGA-NSCLC\n\nAccuracy 0.6511 ± 0.015 0.7674 ± 0.004 0.8527 ± 0.0252 0.8449 ± 0.0257 0.8682 ± 0.0133 0.8604 ± 0.0215 0.8759 ± 0.0231 0.8867 ± 0.0201 0.8449 ± 0.0381 0.8543 ± 0.0236 0.9010 ± 0.0341 0.8992 ± 0.0184 0.9218 ± 0.0113\n\nAUC 0.6755 ± 0.0415 0.8169 ± 0.0254 0.8503 ± 0.0293 0.8580 ± 0.0314 0.8709 ± 0.0135 0.8779 ± 0.0193 0.8944 ± 0.0184 0.9175 ± 0.0139 0.8669 ± 0.0273 0.9103 ± 0.0312 0.9401 ± 0.0272 0.9278 ± 0.0121 0.9649 ± 0.0096\n\nAccuracy 0.7282 ± 0.0181 0.8593 ± 0.0451 0.8384 ± 0.0260 0.8619 ± 0.0404 0.8632 ± 0.0267 0.8492 ± 0.0294 0.8690 ± 0.0277 0.9048 ± 0.0225 0.8565 ± 0.0178 0.8701 ± 0.0294 0.8941 ± 0.0331 0.9004 ± 0.0218 0.9213 ± 0.0173\n\nAUC 0.8551 ± 0.0429 0.9263 ± 0.0585 0.9205 ± 0.0259 0.9107 ± 0.0430 0.9307 ± 0.0162 0.9377 ± 0.0139 0.9439 ± 0.0215 0.9551 ± 0.0187 0.9303 ± 0.0154 0.9097 ± 0.0185 0.9612 ± 0.0223 0.9592 ± 0.0176 0.9763 ± 0.0149\n\nTable 7: Results on PANDA test set.\n\nMean-pooling Max-pooling ABMIL MIL-RNN CLAM-SB CLAM-MB DSMIL DSMIL + SimCLR TransMIL DTFD-MIL (MaxS) DTFD-MIL (AFS) ILRA-MIL ILRA-MIL + LRC\n\nPANDA\n\nAccuracy 0.5691 ± 0.0493 0.6100 ± 0.0255 0.6834 ± 0.0177 NA 0.6648 ± 0.0368 0.6760 ± 0.0441 0.6737 ± 0.0468 0.7017 ± 0.0530 0.6720 ± 0.0434 0.6334 ± 0.0329 0.6573 ± 0.0218 0.7094 ± 0.0309 0.7287 ± 0.0210\n\nkappa 0.4422 ± 0.0248 0.5830 ± 0.0460 0.5998 ± 0.0155 NA 0.5782 ± 0.0182 0.6067 ± 0.0161 0.5562 ± 0.0427 0.5837 ± 0.0231 0.5638 ± 0.0135 0.5462 ± 0.0237 0.5437 ± 0.0193 0.6236 ± 0.0143 0.6562 ± 0.0244\n\nH HEATMAPS\n\nFigure 7 shows 4 diverse examples on the CAMLEYON16 test set. In the ”raw image” column, the tumor area is delineated by the blue line. In the ”CLAM” and ”CLAM+LRC” columns, brighter red indicates that the higher attention score is the tumor at the corresponding location. ”CLAM” is the original CLAM-SB method (Lu et al., 2021c) whereas ”CLAM+LRC” incorporates CLAM-SB with our proposed LRC feature embedding method.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Four examples of high-resolution heatmaps of the CAMELYON16 test set, namely test 016, test 073, test 117, and test 092 from the top row to the bottom row. We compare the heatmap of CLAM in the second column with our proposed LRC method in the third column. Our method is more consistent with the ground truth annotations, indicating superior performance.\n\n18",
    "reference": "# Summary Of The Paper\n\nThe paper tackles the problem of multiple instance learning in pathology imaging where annotations are available only at slide level and the goal is to predict it at instance level. The author exploits the low-rank structure in the data to design a new contrastive learning approach. LRC is proposed as an extension SupCon approach for low-rank data by defining the loss function on the most- and least-distant subspaces. At the feature aggregation level, an iterative approach is proposed using transformers. A learnable low-rank latent matrix L is used across the layers to encode global features. The transformer model named ILRA is based on two transformer modules GABf which reduces the space dimension and GABb restores it. The ILRA transformer layer is applied k times. The proposed approaches are benchmarked on three datasets, showing the improved performance\n\n# Strength And Weaknesses\n\nStrengths\nLow-rank property provides good inductive bias in applications such as in computational pathology\n\nWeakness\nNo validation with multi-center datasets\nThe computation time has not been discussed and could be prohibitive for real-world usage where inference time needs to be reduced.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is not so easy to read. Its clarity could be further improved.\n\n# Summary Of The Review\n\nI think the main drive for improvements in the proposed model is the contrastive learning loss LRC rather than the proposed global features. ILRA-MIL alone does not do better than the baseline methods. This would require further investigation and limits the novelty of the paper.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLEARN LOW-DIMENSIONAL SHORTEST-PATH REPRESENTATION OF LARGE-SCALE AND COMPLEX GRAPHS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nEstimation of shortest-path (SP) distance lies at the heart of network analysis tasks. Along with the rapid emergence of large-scale and complex graphs, approximate SP-representing algorithms that transform a graph into compact and low-dimensional representations are critical for fast and scalable online analysis. Among different approaches, learning-based representation methods have made a breakthrough both in response time and accuracy. Several competitive works in learning-based methods heuristically leverage truncated random walk and optimization on the arbitrary linkage for SP representation learning. However, they have limitations on both exploration range and distance preservation. We propose in this paper an efficient and interpretable SP representation method called Betweenness Centrality-based Distance Resampling (BCDR). First, we prove that betweenness centrality-based random walk can occupy a wider exploration range of distance due to its awareness of high-order path structures. Second, we leverage distance resampling to simulate random shortest paths from original paths and prove that the optimization on such shortest paths preserves distance relations via implicitly decomposing SP distance-based similarity matrix. BCDR yields an average improvement of 25% accuracy and 25-30% query speed, compared to all existing approximate methods when evaluated on a broad class of real-world and synthetic graphs with diverse sizes and structures.\n\n1\n\nINTRODUCTION\n\nEstimation of shortest-path (SP) distance lies at the heart of many network analysis tasks, such as centrality computation (Sch ̈onfeld & Pfeffer, 2021), node separation (Houidi et al., 2020), community detection (Zhang et al., 2020; Asif et al., 2022), which also directly contributes to enormous downstream applications, including point of interest (POI) search (Qi et al., 2020; Chen et al., 2021a) social relationship analysis (Carlton, 2020; Melkonian et al., 2021), biomedical structure prediction (Yue et al., 2019; Sokolowski & Wasserman, 2021), learning theory (Yang et al., 2021; Yuan et al., 2021), optimization (Rahmad Syah et al., 2021; Jiang et al., 2021b), etc. Nowadays, a key challenge of computing SP distance is the prohibitive complexity in very large and complex graphs. e.g., for a sparse undirected graph with N nodes and k queries, the time complexity of A* (Hart et al., 1968) and Dijkstra algorithm (Thorup & Zwick, 2004) are up to O(kN ) and O(kN log N ) for unweighted and weighted graph, respectively.\n\nRegarding this issue, various methods (Cohen et al., 2003; Fu et al., 2013; Akiba et al., 2013; Delling et al., 2014; Farhan et al., 2019; Liu et al., 2021) attempt answering exact distance in microseconds online via indexing or compressing techniques, which suffer huge storage costs on all pair SP distance representations and fail to reflect latent sub-structures in graphs for scalable queries (see Figure 1). Highly concise SP representation for large-scale and complex graphs remains to be studied yet. Regarding this, a surging number of approximate SP-representing algorithms that transform a graph into compact and low-dimensional representations are thus critical for fast and scalable online analysis. They can be categorized into oracle-based (Thorup & Zwick, 2004; Baswana & Kavitha, 2006), landmark-based (Potamias et al., 2009; Sarma et al., 2010; Gubichev et al., 2010) and learning-based (Rizi et al., 2018; Schl ̈otterer et al., 2019; Qi et al., 2020; Jiang et al., 2021a) SP representation methods. Among these categories, learning-based methods are of high accuracy and short response time (see Table 1), owing much to flexible node embeddings in a metric space.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 1: Differences between approximate (ours.) and exact (PLL (Akiba et al., 2013), efficient implementation of the hub-labeling method) SP representation methods regarding storage cost (megabytes, MB) and response time (nanoseconds, ns). We simulate a group of Bernoulli random graphs with |V | nodes, and each edge is filled independently with probability p. (a) and (c) show the storage cost of exact representations increases dramatically relative to the graph size. (b) and (d) reflect longer response time of exact methods, induced by random access to massive information.\n\nTable 1: Overall comparison of approaches to SP representation on DBLP dataset (A.8.4). PTC: preprocessing time complexity, PSC: preprocessing space complexity, RTC: response time complexity, TSC: the total storage cost for answering online distance queries, RT: real response time, AL: accuracy loss which is measured by mRE (see Equation 1). N : the number of nodes in the graph, ̄L(N ): the average label size of each node which increases along with N , ̄D: the amortized degree on each node. α0, L, n, d, w, l, c and β are hyperparameters in corresponded models.\n\nCategories Hub-labeling\n\nOracle-based Landmark-based Learning-based\n\nMethod PLL (Akiba et al., 2013)\n\nADO (Thorup & Zwick, 2004) LS (Potamias et al., 2009) Orion (Xiaohan et al., 2010) Rigel (Xiaohan et al., 2011) DADL (Rizi et al., 2018) Path2Vec (Kutuzov et al., 2019) HALK (Schl ̈otterer et al., 2019) CatBoost-SDP (Jiang et al., 2021a) O((|L|N ) BCDR (ours.) BCDR-FQ (ours.)\n\nO((|L| + wl O((|L| + wl\n\nα0 )\n\nPSC O(N ̄L(N )) O(α0N 1+ 1 O(lN ) O(dN ) O(dN )\n\nPTC O(N 1+log ̄L(N )) O(α0N 1+ 1 O(lN ) O(n2 + nN ) O(n2 + nN ) O((|L| + wl)N ) O(dN + c) O((|L| + wl)N ) O(dN + c) O((|L| + wl)N ) O(dN + c)\n\nO(|L|N + c) β )N ) O(|L|N + c) β )N ) O(dN + c)\n\nRTC O( ̄L(N ))\n\nα0 ) O(α0)\n\nO(l) O(d) O(d + ̄D) O(d + ̄D) O(d + ̄D) O(d + ̄D) O(|L|c) O(|L|c + ̄D) O(d)\n\nTSC 611.2 MB 5, 980 MB 334.6 MB 19.35 MB 35.37 MB 35.37 MB 35.37 MB 35.37 MB 44.16 MB 39.19 MB 19.35 MB\n\nRT 2104.4 ns 8, 598 ns 12, 094 ns 82.25 ns 5, 657 ns 7, 562 ns 7, 700 ns 7, 704 ns 9, 270 ns 7, 247 ns 58.82 ns\n\nAL -\n0.4985 0.3939 1.1897 1.0662 0.2016 0.6097 0.3077 0.0890 0.0798 0.1840\n\nSeveral competitive works in learning-based methods (Rizi et al., 2018; Schl ̈otterer et al., 2019) heuristically leverage truncated random walk and optimization of node-cooccurrence likelihood on the arbitrary linkage to learn SP representations, which once achieved the state-of-the-art performance on approximation quality. However, they are not without limitations on efficiency and interpretability. On one side, a random walk is an unstrained node sequence from the root, possessing a limited exploration range of distance, thus resulting in uncaught distance relations with remote nodes. This is because each transition on nodes is not implied for a specific direction to move towards or beyond the root, especially after several walk steps, which restricts it from visiting remote nodes under limited walk steps (see Figure 2a). On the other side, the optimization on arbitrary linkage reflects excessively versatile local similarity among nodes, which preserves inaccurate distance relations from original graphs to the embedding space. In fact, it exerts a too-general metric over nodes’ correlation, wherein the more edges or paths exist between two nodes, the stronger correlation they share. That means there are many ways to simulate a strong correlation for two nodes (e.g., add mutual edges, delete an edge to other nodes) even if some of the operations do not influence their actual SP distance (see Figures 2c and 2d). A detailed statement of related works on SP representation and motivation for estimating accurate SP distance can be found in Appendix A.1.\n\nIn this paper, we address the above shortcomings by proposing an efficient and interpretable SP representation method called Betweenness Centrality-based Distance Resampling (BCDR). It improves the approximation quality of SP representations with two components. The first is betweenness centrality (BC)-based random walk which explores a wider range of distance correlation on the graph due to its awareness of high-order path structures. To our best knowledge, there is no existing method that combines betweenness centrality and random walk to learn SP representations. We prove that BC-based transition is prone to jump out of local neighborhoods compared to random\n\n2\n\np0.20.40.60.81.0log |V|24681012storage cost (MB)012345graph sizeexact representationapproximate representationp0.20.40.60.81.0log |V|24681012response time (ns)50100150200250exact representationapproximate representationp0.20.40.60.81.0log |V|2468101214storage cost (MB)050100150200250300350400graph sizeexact representationapproximate representationp0.20.40.60.81.0log |V|2468101214response time (ns)200040006000800010000120001400016000exact representationapproximate representationUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(a) random walk from va Figure 2: Distance confusion in previous SP representation learning. has much difficulty in exploring beyond current community to vc. (b): node similarity on random paths misleads the measurement of SP distance since the walk from va is prone to steer clear of vb for starters and back to vb as the end, causing an extremely weak correlation between va and vb even though they have an immediate edge. (c): a sufficient number of 2-hop links between vc and va induce a shorter distance in embedding space than that of vb and va. (d): vb and vc sharing substantial connection are mapped closed to each other even if they have a large SP distance gap, while the divergence of distance between vb, vc and va is also plagued with extraction.\n\ntransition. The second is distance resampling which preserves accurate SP distance relations via implicitly decomposing an SP distance-based similarity matrix. In essence, it simulates the observation of random SPs from original walk paths and exerts desirable constraints on node representations to preserve distance relations over the graph.\n\nWe summarize the major contributions as follows: i) We propose BC-based random walk as an efficient strategy for exploring a wider range of SP distance within limited walk steps (see Section ii) We propose distance resampling to preserve accurate distance relations among nodes to 3.1). iii) We evaluate BCDR with a broad learn an interpretable SP representation (see Section 3.2). class of real-world and synthetic graphs, and it yields an average improvement of 25% accuracy and 25-30% query speed compared to all existing methods (see Section 4).\n\n2 PRELIMINARY\n\nNotation: G = (V, E) denotes an undirected graph, with V = {vi} being the set of nodes and E = {(vi, vj)} being the set of undirected edges, and N = |V |, M = |E|. We use ZN ×d to represent a matrix comprising embedded vectors of nodes, where d is the embedding size, and the i-th row of Z is corresponded with vi. A path pij of length l ∈ N+ on graph G is an ordered sequence of nodes (vi, va1 , · · · , val−1, vj), where each node except the last one has an edge with the subsequent node. The shortest path ̊pij is one of the paths with the minimum length Dij between vi and vj. Also, the SP distance matrix D comprises {Dij}. A node vi’s neighborhood Ni is a set of nodes with an edge with vi, i.e., Ni = {vj|(vi, vj) ∈ E}. For high-order neighborhoods of vi, N (h) is defined as a set of nodes h-hop away from vi, i.e., {vj|Dij = h}. To avoid confusion with the symbol of paths, we use ̃P (·) to represent a probability distribution in this paper. A truncated i (cid:105), where W k random walk Wi rooted at node vi of length l is a random vector of (cid:104)W 1 i\nis a node chosen from the neighborhood of node W k−1 for k = 1, ..., l, with the initial probability ̃P (W 0 i = vi) ≡ 1. Wi is a categorical distribution of nodes on Wi, and the probability of each node in Wi represents the frequency of occurrence on the sampled paths.\n\ni , · · · , W l\n\ni , W 2\n\ni\n\ni\n\nProblem Definition & Metrics: The evaluation of approximate SP representation methods is divided into two stages. For the offline stage, the processing time and memory usage when constructing SP representations are evaluated, and the storage size of such representations is considered. For the online stage, the query speed, memory usage, and approximation quality are evaluated. Thereinto, to evaluate query speed and memory usage, a million times of query requests for arbitrary node pairs are performed, then the memory and average response time for each node pair are recorded. For approximation quality, the commonly used metrics are mean of relative error (mRE) and mean of absolute error (mAE). For a group of SP distance queries Q = {(vi, vj)}, mRE is defined as the relative loss of the estimated value ̃Dij with respect to the real value Dij, while mAE measures the\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nabsolute gap between them:\n\nmRE :=\n\n1 |Q|\n\n(cid:88)\n\n(vi,vj )∈Q\n\n| ̃Dij − Dij| Dij\n\nmAE :=\n\n1 |Q|\n\n(cid:88)\n\n| ̃Dij − Dij|\n\n(vi,vj )∈Q\n\n(1)\n\n3 METHOD\n\nAlthough random walk (RW) is universally accepted as an efficient serialization strategy of similarity measurement on graphs (Grover & Leskovec, 2016; Zhuang & Ma, 2018), we argue that the intuitive practice of RW in representing SP structures has several limitations. Consider a walk path p = (va, va1 , va2, · · · , val ) ∈ Pa sampled by stochastic selection on neighborhoods from root node va. Distance measured along p (i.e., the order on the walk) is not consistent with that on the graph (see Figure 2b) since the node sequence is unstrained, i.e., for vai, vaj ∈ p, i ≤ j (cid:60) Daai ≤ Daaj , where i and j are indices of node vai and vaj on p, and 1 ≤ i, j ≤ l. Therefore, optimizing node co-occurrence likelihood on such walk paths incurs two problems.\n\n1. Problem 1: Limited exploration range of distance. The exploration range of rooted random walk is not in proportion to its length since each transition on the walk has an agnostic tendency to move towards or beyond the root after a few steps (see Figure 2a).\n\n2. Problem 2: Intractability of distance relations on paths. The distance measured on walk paths may not actually reflect the SP distance on the graph because of the unbalanced number of edges between different nodes (see Figure 2c and 2d).\n\nIn this section, we describe in detail our method as a decent way of representing SP structures. We discuss two techniques named BC-based random walk and distance resampling to address the above problems, respectively, and present the corresponding theoretical analysis for their interpretability. A time and space-efficient implementation of BCDR to integrate these techniques is available in Appendix A.2.\n\n3.1 BC-BASED RANDOM WALK FOR WIDER EXPLORATION RANGE OF DISTANCE\n\nDefinition 1. (Betweenness Centrality) Define G = (V, E) as undirected graph. vi, vs, vt are arbitrary nodes in V . σst(vi) represents the number of shortest paths between vs and vt that pass vi, and σst is the total number of shortest paths between vs and vt. Then we say that BC of vi is\n\nBC(vi) =\n\n(cid:88)\n\ns(cid:54)=i(cid:54)=t\n\nσst(vi) σst\n\n(2)\n\nTo address Problem 1, we propose BC-based random walk. As defined in Definition 1, BC(vi) determines the probability of vi located on SPs of arbitrary node pairs. Thus, we consider a node with a large BC value vitally significant to drive the walk to move away from the root node, since it reveals an easy way of traveling to some other nodes with minimal steps. And to leverage this property, in BC-based random walk Wa = (cid:104)W 1 a, ...(cid:105) on node va, we prefer choosing nodes with the largest BC values among their neighborhoods when simulating walk paths, i.e.,\n\na, ...W j\n\na, W 2\n\n ̃P (W j\n\na = vm|W j−1\n\na = vn) =\n\nBC(vm)\n\n(cid:80)\n\nvk∈Nn\n\nBC(vk)\n\n, vm ∈ Nn\n\n(3)\n\na\n\na\n\nto N (h+1)\n\nTheorem 2, proved in Appendix A.3, indicates that BC-based random walk is prone to transit from N (h) , leading to a wider exploration range measured by the intrinsic graph’s SP distance. Specifically, for each node va, N (h) comprises two components, i.e., final nodes fh(va) and connective nodes eh(va). Thereinto, nodes in fh(va) have no edge with N (h+1) , while nodes in eh(va) have several edges with N (h+1) , as illustrated in Figure 3a. Our method significantly improves the performance when the number of final nodes |fh(va)| is larger or there are more edges from fh(va) to eh(va) (see analysis in Remark 1).\n\na\n\na\n\na\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 3: (a): high-order neighborhoods’ structure of va. BC-based random walk enhances the transition from fh to eh and eh to eh+1, prone to jump out of local neighborhoods. (b): comparison between general RW-based graph learning and BCDR. Distance resampling transforms the observation into random shortest paths, which exerts desirable constraints on learning SP representations.\n\nPractically, pre-computing BC value on each node is time-consuming, which takes at least O(N M ) time. To address this problem, we estimate BC by leveraging breadth-first search (BFS) from a fixed number of landmarks, since nodes with large BC values tend to be visited first by BFS from any landmark. We show in Algorithm 1 and Appendix A.2.1 that this process could be involved in the simulation of distance triplets without introducing extra time complexity.\n\nFinally, we conclude that BC-based random walk is a competitive walking pattern regarding exploration range of SP distance, since it possesses a strong tendency to jump out of local neighborhoods. We further verify our conclusion by comparing it with existing RW techniques in Section 4.2.\n\n3.2 DISTANCE RESAMPLING FOR SP DISTANCE PRESERVATION\n\nTo address Problem 2, we propose distance resampling. We first illustrate a general RW-based graph learning paradigm in Figure 3b and clarify the differences between ours and other approaches. The basic idea of RW-based methods is to learn node-level embeddings Z from pieces of observation (i.e., walk paths), and Z thus reflects the structural bias on graphs. Specifically, for the naive RW strategy and its variants utilized in other approaches, the observation is a set of stochastic paths reflecting the property of arbitrary linkage between nodes, which asks Z to preserve point-wise mutual information (PMI) similarity (proved in Levy & Goldberg (2014); Shaosheng et al. (2015)). Unfortunately, the PMI similarity shares no direct connection with SP distance and causes the problems depicted in Figure 2c and 2d. To fit Z with correct information about SP structures, we intend to observe random shortest paths instead. This practice is feasible since the SP problem always has optimal substructures, i.e., the subpath between two nodes on any SP could also be extracted as an SP between these nodes. However, the prohibitive complexity of computing all pairs of SPs forbids us from performing such sufficient observation (see both technical and empirical comparisons between utilizing BCDR and directly sampling SPs for optimization in Appendix A.7). By way of an alternative, we propose a resampling strategy to transform BC random paths into approximate random SPs with efficient linear processing time and better performance.\n\nInitially, we formulate the SP representation problem from the RW-based learning perspective. we refer to random SP walk ̊Wi as an ideal walking pattern whose transition reflects the probability of each shortest path passing through vi. It means paths sampled from ̊Wi are prone to be an SP rooted at vi. For sufficient observation on SPs, we thus have an optimization objective on ̊Wi, i.e., L(Z) = E . To reduce optimization complexity, we replace the intrinsic probability normalization by negative sampling, according to Mikolov et al. (2013a;b), i.e.,\n\nlog ̃P ̊Wi|Zi\n\n( ̊Wi|Zi)\n\nvi∈ ̃P (V )\n\n(cid:105)\n\n(cid:104)\n\nLn(Z) =\n\n ̃P (vi)\n\n(cid:88)\n\nvi∈V\n\n(cid:110)\n\nE\n\nvj ∼ ̃P ̊Wi\n\n(V )[log ˆσ(ZiZT\n\nj )] + λE\n\nvk∼ ̃Pn(V )[log ˆσ(−ZiZT\n\nk )]\n\n(cid:111)\n\n(4)\n\n, since we prefer an informative Z instead of the accurate probability. Thereinto, ̃Pn is the distribution of negative sampling over the graph, λ denotes the number of negative samples, and\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nˆσ(x) = (1 + e−x)−1. It is notable that ̊Wi on each node vi is backbreaking to extract, since it requires a traversal on all SPs passing through vi. To address this, we revisit the node distribution ̃PWi on BC-based random walk Wi and construct a distribution ̃QWi resampled from ̃PWi, as an efficient approximation to ̃P ̊Wi\n\n, i.e.,\n\n ̃QWi(vj) =\n\n(cid:80)\n\nαDij BC(vj)\n\nvk∈Wi\n\nαDik BC(vk)\n\n(5)\n\nwhere α is a hyper-parameter controlling the weight decay by the distance, 0 < α < 1.\n\nFinally, we maximize the following approximate objective ˆLn on Wi (instead of Ln on ̊Wi), i.e.,\n\nˆLn(Z) =\n\n ̃P (vi)\n\n(cid:88)\n\nvi∈V\n\n(cid:110)\n\nE\n\nvj ∼ ̃QWi (V )[log ˆσ(ZiZT\n\nj )] + λE\n\nvk∼ ̃Pn(V )[log ˆσ(−ZiZT\n\nk )]\n\n(6)\n\n(cid:111)\n\nWe show in Proposition 1 that optimization of Equation 6 conforms to implicitly decompose an SP distance-based similarity matrix where for any va and vb located far away from each other under the SP metric (i.e., a small Dab) should be mapped with low similarity in the embedding space (i.e., a large | ˆDab|). Also, further discussion in Remark 2 shows that such similarity matrix ˆD shares strong connections with the real SP distance matrix D on graphs. Proposition 1. Let G be an undirected graph, Wi be the categorical distribution of nodes on the paths sampled by BC-based random walk. Negative sampling on each node vi takes a uniform distribution on Wi. Then, for sufficient observation of W1, · · · , WN , maximizing ˆLn defined by Equation 6 with embeddings Z is equivalent to decomposing an SP distance-based similarity matrix ˆD = ZZ T , where for any va and vb, the distance between them in the embedding space varies linearly with respect to distance Dab, namely, ˆDab = ZaZ T\n\n(7) where (cid:15) is a small constant related to the negative samples, which is independent of vb, and α is the hyper-parameter defined in Equation 5, where 0 < α < 1.\n\nb = − log (cid:15) + Dab log α\n\nProposition 1 is proved in Appendix A.4 by deriving the extreme point of ˆLn regarding Z.\n\nThen, we consider the preservation of SP distance relations. Some studies on metric learning (Hermans et al., 2017; Zeng et al., 2020) have revealed that a triplet of samples (va, vb, vc) being easy to learn means if vb shares strong correlation with va, the distance between vb and va in the embedding space should be shorter than that of vc and va. With this property, we have the following theorem (proved in A.6 by directly applying Proprosition 1), which indicates that our method is consistent with distance relations under the intrinsic SP metric. Theorem 1. Each symbol here follows the definition in Proposition 1. Let D be a global distance matrix defined on graph G and Dab be graph’s SP distance between node va and vb. Then for any nodes va, vb, vc ∈ G,\n\n(Dab − Dac)( ˆDab − ˆDac) ≤ 0\n\n(8)\n\nIn conclusion, we discuss here the significance of distance resampling for preserving accurate distance relations. It exerts two implicit constraints on Z to learn an interpretable SP representation. First, as stated in Proposition 1, the distance measured in the embedding space shares a strong negative correlation with that measured on the graph. Second, for any node triplet, the distance relation between any two of them is preserved according to Theorem 1. The two constraints are further verified in Section 4.3 against existing techniques.\n\n3.3 EFFICIENT IMPLEMENTATION OF BCDR ALGORITHM\n\nWe also provide a time and space-efficient implementation of BCDR to integrate the above techniques in Algorithm 1. Like previous learning-based SP representation methods (Rizi et al., 2018), we first transform the graph into low-dimensional embeddings (i.e., ZN ×d) and learn a distance predictor gφ : (Rd, Rd) → R by observed distance triplets {(Za, Zb, Dab)}. Then, the predictor gφ will be involved in answering online distance queries. In addition, similar to Jiang et al. (2021a), we also improve the prediction results via gradient boosting techniques. The detailed designs of these procedures are described in Appendix A.2, and an ablation study to evaluate their impact on performance is provided in Appendix A.11.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4 EXPERIMENTAL EVALUATION\n\nIn this section, we show the comprehensive performance of BCDR with 5 real-world graphs of different sizes and 6 synthetic graphs of different structures. Specifically, we evaluate BCDR on 3 small graphs (i.e., Cora, Facebook, and GrQc) and 2 large graphs (i.e., DBLP and Youtube) for its scalability (see Section 4.1), and evaluate it on 6 synthetic graphs for its representational capacity of complex structures (see Section 4.2 and 4.3). Our method is compared with strong baselines from both approximate SP representation and general graph representation learning (GRL). In the experiments, we also provide two variants of BCDR, i.e., BCDR-FC and BCDR-FQ, for accelerating the construction and querying process, respectively. A detailed description of the datasets, including statistics and visualization, is thoroughly provided in Appendix A.8.\n\n4.1 PERFORMANCE OF APPROXIMATE SP DISTANCE QUERY\n\nWe compare BCDR with other learning-based SP representation methods (i.e., Orion (Xiaohan et al., 2010), Rigel (Xiaohan et al., 2011), DADL (Rizi et al., 2018), Path2Vec (Kutuzov et al., 2019), HALK (Schl ̈otterer et al., 2019)) and CatBoost-SDP (Jiang et al., 2021a) as well as other approximate methods, including landmark-based (i.e., LS (Potamias et al., 2009) and oracle-based (i.e., ADO (Thorup & Zwick, 2004)) techniques. All of the above models are run with six 3.50GHz Intel Xeon(R) CPUs and 128GB memory, and the precomputed representations of each model are serialized by Pickle. Each baseline generally follows the default parameters discussed in its paper with some trivial changes, so that its performance can be evaluated in a unified way. The detailed parameter setups of each model are provided in Appendix A.9. Like previous works, we initially compute all pairs of SP distance on each graph by BFS and take a uniform sampling to select 1, 000, 000 distance triplets {(va, vb, Dab)} as test samples. All of the baselines, including ours, are purely implemented in Python 3.9 and evaluated under the same environment. Since only unweighted graphs are considered, the outputs of each model are quantized to integer when evaluating accuracy loss. Some of the experimental results are shown in Table 2 (see Appendix A.10 for extended comparisons with GRL models). We can see from the table that our model not only outperforms previous models regarding accuracy loss for all graphs but also shares competitive results on other metrics.\n\nIn detail, for accuracy loss (mAE and mRE), BCDR answers arbitrary queries with the minimum error due to a wider exploration range of distance and distance-preserved optimization. Notably, the variants of BCDR without boosting module (i.e., BCDR-FQ and BCDR-FC) also achieve the highest accuracy against other RW-based learning approaches (i.e., DADL and HALK) within almost the least storage cost. For offline processing time (PT), memory usage (PMU), and storage cost (SC), the results show BCDR possesses powerful scalability against the growth of graph scale. Even for a graph with millions of nodes, the offline processing could be completed within several hours, and the memory usage is close to the graph size. This is because we perform BC-based random walks with a fixed length on each graph, and the size of walk data is further reduced by distance resampling. In addition, although CatBoost-SDP seems to achieve strong scalability on these metrics, we need to point out that this method does not learn any representation of nodes and completely optimizes all pairs of distance in a boosting way, which subsequently suffers higher time and space cost for online queries. For response time (RT) and memory usage (QMU) in querying, we see BCDR-FC and most other learning-based models share similar low memory overhead since each distance query could be answered by checking the node embeddings and graph adjacency matrices. Furthermore, BCDR-FQ and Orion could answer such a query within tens of nanoseconds due to the absence of double-checking on adjacency matrices.\n\nBesides, to evaluate the impact of critical components and hyper-parameters in BCDR, we further conduct an ablation study in Appendix A.11 where we discuss 6 different modifications to BCDR as well as an investigation on 9 critical parameters to show their impacts on different metrics.\n\n4.2 EXPLORATION RANGE OF DISTANCE\n\nAs stated in Section 3.1, the exploration range of distance could be widened by BC-based random walk (BC-RW), since the latter helps to jump out of local neighborhoods. Here, we compare BCRW with existing renowned walk strategies, including naive random walk (NRW) (Perozzi et al., 2014; Zhuang & Ma, 2018), second-order random walk (SORW) (Grover & Leskovec, 2016), and\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Performance comparison of approximate methods on SP distance queries. PT: processing time when constructing SP representations, PMU: processing memory usage, SC: space cost on storing SP representations, RT: average response time of answering a distance query, QMU: querying memory usage. mAE and mRE are the accuracy metrics. DNF means it did not finish in one day. We bold the top three performances, and highlight the top one with an underline.\n\nDataset\n\nCora\n\nFacebook\n\nGrQc\n\nDBLP\n\nYoutube\n\nModel ADO LS Orion Rigel DADL Path2Vec HALK CatBoost-SDP BCDR (ours.) BCDR-FQ (ours.) BCDR-FC (ours.) ADO LS Orion Rigel DADL Path2Vec HALK CatBoost-SDP BCDR (ours.) BCDR-FQ (ours.) BCDR-FC(ours.) ADO LS Orion Rigel DADL Path2Vec HALK CatBoost-SDP BCDR (ours.) BCDR-FQ (ours.) BCDR-FC (ours.) ADO LS Orion Rigel DADL Path2Vec HALK CatBoost-SDP BCDR (ours.) BCDR-FQ (ours.) BCDR-FC (ours.) ADO LS Orion Rigel DADL Path2Vec HALK CatBoost-SDP BCDR (ours.) BCDR-FQ (ours.) BCDR-FC (ours.)\n\nPT 0.3179 s 0.6355 s 61.34 s 61.36 s 68.22 s 172.9 s 30.44s 9.582 s 40.85 s 39.51 s 35.60 s 1.991 s 3.382 s 80.53 s 80.31 s 176.3 s 284.5 s 75.95 s 12.84 s 142.7 s 160.6 s 91.71 s 1.322 s 2.098 s 98.14 s 98.00 s 103.0 s 333.0 s 48.30 s 11.03 s 69.61 s 67.98 s 58.30 s 37, 029 s 5, 838 s 5, 531 s 5, 523 s 2, 650 s 27, 453 s 1, 270 s 244.4 s 1, 099 s 1,026 s 999.8 s DNF 87, 902 s 50, 484 s 51, 246 s 28, 351 s DNF 5,971 s 3,030 s 9, 247 s 7, 893 s 6,000 s\n\nPMU 6.205 MB 2.952 MB 15.06 MB 15.06 MB 27.36 MB 2.870 MB 6.715 MB 0.4444 MB 2.979 MB 2.728 MB 2.728 MB 13.10 MB 4.390 MB 21.00 MB 21.00 MB 42.44 MB 5.459 MB 21.28 MB 1.874 MB 6.804 MB 5.213 MB 5.213 MB 17.75 MB 5.722 MB 21.33 MB 21.33 MB 53.09 MB 5.632 MB 13.80 MB 0.9305 MB 5.907 MB 5.351 MB 5.351 MB 8, 899 MB 349.0 MB 321.8 MB 321.8 MB 1, 958 MB 140.9 MB 649.6 MB 56.47 MB 89.04 MB 53.52 MB 53.52 MB −\n1, 258 MB 1, 251 MB 1, 251 MB 6, 983 MB −\n2, 187 MB 190.4 MB 295.2 MB 179.8 MB 179.8 MB\n\nSC 0.9905 MB 0.6791 MB 0.1655 MB 0.1655 MB 0.1696 MB 0.1696 MB 0.1696 MB 0.4711 MB 0.4079 MB 0.1696 MB 0.1696 MB 1.783 MB 0.9929 MB 0.2419 MB 0.2419 MB 0.2459 MB 0.2459 MB 0.2459 MB 0.6896 MB 1.210 MB 0.2459 MB 0.2459 MB 2.922 MB 1.315 MB 0.3202 MB 0.3202 MB 0.3242 MB 0.3242 MB 0.3242 MB 0.7846 MB 0.7916 MB 0.3242 MB 0.3242 MB 199.8 MB 80.02 MB 19.36 MB 19.36 MB 19.36 MB 19.36 MB 19.36 MB 40.34 MB 41.82 MB 19.36 MB 19.36 MB −\n286.7 MB 69.26 MB 69.26 MB 69.27 MB −\n69.27 MB 141.7 MB 141.6 MB 69.27 MB 69.27 MB\n\nRT 9, 813 ns 11, 537 ns 63.87 ns 4,587 ns 6, 798 ns 6, 820 ns 6, 802 ns 8, 987 ns 6, 376 ns 50.85 ns 6, 830 ns 8, 105 ns 11, 664 ns 60.81 ns 4,976 ns 8, 601 ns 8, 582 ns 8, 777 ns 8, 955 ns 8, 298 ns 53.78 ns 8, 713 ns 13, 555 ns 11, 825 ns 62.80 ns 4,872 ns 6, 912 ns 6, 938 ns 6, 955 ns 9, 015 ns 6, 452 ns 51.90 ns 6, 965 ns 8, 598 ns 12, 094 ns 82.25 ns 5,657 ns 7, 562 ns 7, 700 ns 7, 704 ns 9, 270 ns 7, 247 ns 58.82 ns 7, 617 ns −\n16, 672 ns 163.9 ns 13, 808 ns 7, 708 ns −\n7, 764 ns 9, 921 ns 7, 393 ns 63.78 ns 6,156 ns\n\nQMU 11.89 MB 10.92 MB 0.1654 MB 0.2482 MB 0.2486 MB 0.2486 MB 0.2486 MB 4.285 MB 4.0623 MB 0.1657 MB 0.2486 MB 16.94 MB 12.28 MB 0.2418 MB 1.588 MB 1.588 MB 1.588 MB 1.588 MB 4.504 MB 5.402 MB 0.2421 MB 1.588 MB 19.19 MB 13.60 MB 0.3201 MB 0.5519 MB 0.5523 MB 0.5523 MB 0.5523 MB 4.599 MB 4.367 MB 0.3204 MB 0.5523 MB 5, 980 MB 344.6 MB 19.35 MB 35.37 MB 35.37 MB 35.37 MB 35.37 MB 44.16 MB 39.19 MB 19.35 MB 35.37 MB −\n1, 217 MB 69.26 MB 114.9 MB 114.9 MB −\n114.9 MB 145.5 MB 118.7 MB 69.27 MB 114.9 MB\n\nmAE 2.1070 1.0599 3.0542 3.0464 1.0822 3.2020 1.7702 0.8907 0.8046 0.7249 0.8243 1.1842 0.9566 1.7770 1.7531 0.2250 1.4263 0.9004 0.0203 0.0106 0.0978 0.1463 1.8747 1.1538 3.0532 3.0493 0.9812 4.1239 1.1695 0.7815 0.7043 0.8743 0.8776 3.0691 2.5060 3.5044 3.5043 1.2753 3.9474 1.8477 0.5492 0.4923 1.3018 1.1014 −\n2.0159 2.8642 2.8642 1.1144 −\n1.9035 0.4022 0.3297 0.9004 0.9083\n\nmRE 0.4266 0.2068 0.5242 0.5164 0.1862 0.6066 0.3293 0.1585 0.1411 0.1247 0.1384 0.5080 0.3924 0.6864 0.6625 0.0792 0.5489 0.3517 0.0159 0.0044 0.0385 0.0478 0.3582 0.2097 0.4849 0.4810 0.1659 0.7231 0.2063 0.1416 0.1274 0.1501 0.1442 0.4985 0.3939 0.5165 0.5164 0.2016 0.6097 0.3077 0.0890 0.0798 0.1840 0.1580 −\n0.4091 0.5473 0.5473 0.2163 −\n0.3860 0.0724 0.0676 0.1704 0.1745\n\nrandom surfing (RS) (Cao et al., 2016). We also consider a DFS-like random walk (DFS-RW) as a strong baseline by setting a very small q in Node2Vec for deep exploration. The methods are tested on 6 synthetic graphs with divergent structures. We randomly sample 20 root nodes and, for each root, simulate 10 walks to show how many nodes with different distance are explored at each step of the walk. The ideal situation for rooted walk paths with a fixed length l is to cover up to nodes l-hop away from the current root. The results on circle graphs are shown in Figure 4. We can see from the results that our proposed BC-RW is much more competitive in exploring a wider range of SP distance. Further results and analysis on different graph structures are presented in Appendix A.12.\n\n4.3 PRESERVATION OF DISTANCE RELATIONS\n\nAs discussed in Section 3.2, distance resampling is proposed to preserve accurate distance relations via implicitly decomposing an SP distance-based similarity matrix. Here, we show its interpretability for SP representations by visualizing the properties of embedded vectors Z when compared with\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Exploration range of distance of different walk strategies on circle graphs. Column from left to right: different walk strategies, i.e, NRW, SORW, RS, DFS-RW, BC-RW (ours.).\n\nthe maximum likelihood optimization on other biased random walks. Environment configuration follows the previous section.\n\nFirst, we evaluate the relation of distance measured on graphs and embedding spaces for each node pair. Specifically, distance on the embedding space is measured by inner product ZiZT j for given nodes vi and vj, and that on the graph is measured by SP distance. Initially, we learn embedded vectors from walk paths simulated by each walk strategy and randomly sample 100 source nodes with 100 destinations for each source. The results on circle graphs are shown in Figure 5 (refer to Appendix A.13 for extended results on other graphs), which indicates that embeddings enhanced by distance resampling have a better tendency to maintain a linear relationship on the distance metric between the original graph and embedding space. The results also verified our analysis in Remark 2 regarding relations between the SP distance-based similarity matrix ˆD and the distance matrix D.\n\nFigure 5: Row 1: measured distance from the embedding space and the original graph. Row 2: whether distance relations are violated in the embedding space. Columns from left to right: embeddings learned by different walk strategies,i.e., NRW, SORW, RS, DFS-RW, and BC-RW (ours.), respectively. For ours, walk paths are further simulated by distance resampling.\n\nSecond, we try to find out how much the probability distance relation is violated in the embedding space, i.e., whether a pair of nodes with larger SP distance is corresponded with less embedded similarity as described in Theorem 1. We randomly take 10, 000 node triplets {(va, vb, vc)}, and record if Equation 8 is satisfied. The results on circle graphs are shown in Figure 5 (refer to Appendix A.13 for extended results on other graphs). The figure confirms that our model is much more satisfactory in preserving distance relation than existing methods. This is because BC-RW provides sufficient observation on each node by locating many remote nodes with a sequence of centralized nodes on a graph, and thus distance resampling based on such observation could preserve distance relations of each node within its exploration range.\n\n5 CONCLUSION\n\nIn this paper, we propose a novel graph SP representation method called Betweenness Centralitybased Distance Resampling (BCDR) and discuss two significant techniques for an efficient and interpretable SP representation. The experimental evaluation indicates that BCDR improves the approximation quality with a shorter response time for SP distance queries and possesses strong scalability to large-scale and complex graphs. Notably, the produced node representations by our method also reflect the highly-efficient paths for high-order message passing in GNNs, which appears to be helpful for structural graph pooling and inference. We leave it for our future work.\n\n9\n\n051015202530Walk Length051015202530Exploration Range of Distance051015202530Walk Length051015202530Exploration Range of Distance051015202530Walk Length051015202530Exploration Range of Distance051015202530Walk Length051015202530Exploration Range of Distance051015202530Walk Length051015202530Exploration Range of Distance010203040SP Distance on the graph0246810Distance in the embedding space010203040SP Distance on the graph0246810Distance in the embedding space010203040SP Distance on the graph0246810Distance in the embedding space010203040SP Distance on the graph0246810Distance in the embedding space010203040SP Distance on the graph0246810Distance in the embedding space0200040006000800010000sampled node triples-400-300-200-1000100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-400-300-200-1000100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-600-500-400-300-200-1000100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-300-200-1000100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-80-60-40-200value of distance expressionviolatedpreservedUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nA. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed largescale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, pp. 37–48, 2013.\n\nTakuya Akiba, Yoichi Iwata, and Yuichi Yoshida. Fast exact shortest-path distance queries on large networks by pruned landmark labeling. In Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data, pp. 349–360, 2013.\n\nMuhammad Asif, Hassan Raza, and Muhammad Imran Manzoor. A novel approach for tenuous community detection in social networks. International Journal of Data Analytics (IJDA), 3(1): 1–12, 2022.\n\nGr ́egoire Aujay, Franck H ́etroy, Francis Lazarus, and Christine Depraz. Harmonic skeleton for realistic character animation. In SCA 2007-ACM-SIGGRAPH/Eurographics Symposium on Computer Animation, pp. 151–160. Eurographics Association, 2007.\n\nEgon Balas. Tutorial paper x: A class of location, distribution and scheduling problems: modelling and solution methods. JORBEL-Belgian Journal of Operations Research, Statistics, and Computer Science, 22(2):36–69, 1982.\n\nS. Baswana and T. Kavitha. Faster algorithms for approximate distance oracles and all-pairs small In 2006 47th Annual IEEE Symposium on Foundations of Computer Science\n\nstretch paths. (FOCS’06), pp. 591–602, Oct 2006. doi: 10.1109/FOCS.2006.29.\n\nShaosheng Cao, Wei Lu, and Qiongkai Xu. Deep neural networks for learning graph representations.\n\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.\n\nJason Carlton. The shortest distance between two people is a story: Storytelling best practices in digital and social media marketing. Journal of Digital & Social Media Marketing, 8(2):108–115, 2020.\n\nSudhanshu Chanpuriya, Cameron Musco, Konstantinos Sotiropoulos,\n\nand Charalampos Tsourakakis. Node embeddings and exact low-rank representations of complex networks. Advances in Neural Information Processing Systems, 33, 2020.\n\nHaochen Chen, Bryan Perozzi, Yifan Hu, and Steven Skiena. Harp: Hierarchical representation In Proceedings of the AAAI conference on artificial intelligence, vol-\n\nlearning for networks. ume 32, 2018.\n\nMingming Chen, Ning Wang, Guofeng Lin, and Jedi S Shang. Network-based trajectory search over\n\ntime intervals. Big Data Research, 25:100221, 2021a.\n\nWei Chen, Christian Sommer, Shang-Hua Teng, and Yajun Wang. Compact routing in power-law graphs. In International Symposium on Distributed Computing, pp. 379–391. Springer, 2009.\n\nYu Chen, Hanchao Ku, and Mingwu Zhang. Pp-ocq: A distributed privacy-preserving optimal closeness query scheme for social networks. Computer Standards & Interfaces, 74:103484, 2021b.\n\nEdith Cohen, Eran Halperin, Haim Kaplan, and Uri Zwick. Reachability and distance queries via\n\n2-hop labels. SIAM Journal on Computing, 32(5):1338–1355, 2003.\n\nManuel Costa, Miguel Castro, R Rowstron, and Peter Key. Pic: Practical internet coordinates for distance estimation. In 24th International Conference on Distributed Computing Systems, 2004. Proceedings., pp. 178–187. IEEE, 2004.\n\nFrank Dabek, Russ Cox, Frans Kaashoek, and Robert Morris. Vivaldi: A decentralized network coordinate system. ACM SIGCOMM Computer Communication Review, 34(4):15–26, 2004.\n\nDaniel Delling, Andrew V Goldberg, Thomas Pajor, and Renato F Werneck. Robust distance queries\n\non massive networks. In European Symposium on Algorithms, pp. 321–333. Springer, 2014.\n\nAnna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. Catboost: gradient boosting with\n\ncategorical features support. arXiv preprint arXiv:1810.11363, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMihaela Enachescu, Mei Wang, and Ashish Goel. Reducing maximum stretch in compact routing. In IEEE INFOCOM 2008-The 27th Conference on Computer Communications, pp. 336–340. IEEE, 2008.\n\nMuhammad Farhan, Qing Wang, Yu Lin, and Brendan McKay. A highly scalable labelling approach\n\nfor exact distance queries in complex networks. Network, 400(3B):8B, 2019.\n\nAda Wai-Chee Fu, Huanhuan Wu, James Cheng, and Raymond Chi-Wing Wong.\n\nIs-label: an independent-set based labeling scheme for point-to-point distance querying. Proceedings of the VLDB Endowment, 6(6):457–468, 2013.\n\nLucia Galoviˇcov ́a, Petra Borotov ́a, Veronika Valkov ́a, Nenad L Vukovic, Milena Vukic, Jana ˇStef ́anikov ́a, Hana ˇD ́uranov ́a, Przemysław Łukasz Kowalczewski, Nat ́alia ˇCmikov ́a, and Miroslava Kaˇc ́aniov ́a. Thymus vulgaris essential oil and its biological activity. Plants, 10(9): 1959, 2021.\n\nA. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In the 22nd ACM\n\nSIGKDD International Conference, 2016.\n\nA. Gubichev, S. Bedathur, S. Seufert, and G. Weikum. Fast and accurate estimation of shortest paths In Proceedings of the 19th ACM Conference on Information and Knowledge\n\nin large graphs. Management, CIKM 2010, Toronto, Ontario, Canada, October 26-30, 2010, 2010.\n\nPeter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100–107, 1968.\n\nAlexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-\n\nidentification. arXiv preprint arXiv:1703.07737, 2017.\n\nOmar Houidi, Oussama Soualah, Wajdi Louati, and Djamal Zeghlache. Dynamic vnf forwarding IEEE Transactions on Network and Service Management, 17(3):\n\ngraph extension algorithms. 1389–1402, 2020.\n\nLiying Jiang, Yongxuan Lai, Quan Chen, Wenhua Zeng, Fan Yang, and Fan Yi. Shortest path distance prediction based on catboost. In International Conference on Web Information Systems and Applications, pp. 133–143. Springer, 2021a.\n\nShijie Jiang, Yang Wang, Guang Lu, and Chuanwen Li. Dlsm: Distance label based subgraph matching on gpu. In Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data, pp. 194–200. Springer, 2021b.\n\nAndrey Kutuzov, Mohammad Dorgham, Oleksiy Oliynyk, Chris Biemann, and Alexander Panchenko. Making fast graph-based algorithms with graph metric embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3349–3355, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1325. URL https://aclanthology.org/P19-1325.\n\nJames R Lee. Volume distortion for subsets of euclidean spaces. Discrete & Computational Geom-\n\netry, 41(4):590–615, 2009.\n\nSanghwan Lee, Zhi-Li Zhang, Sambit Sahu, and Debanjan Saha. On suitability of euclidean embedding of internet hosts. ACM SIGMETRICS Performance Evaluation Review, 34(1):157–168, 2006.\n\nJure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http:\n\n//snap.stanford.edu/data, June 2014.\n\nOmer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. Advances\n\nin neural information processing systems, 27:2177–2185, 2014.\n\nZiyi Liu, Lei Li, Mengxuan Zhang, Wen Hua, Pingfu Chao, and Xiaofang Zhou. Efficient constrained shortest path query answering with forest hop labeling. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), pp. 1763–1774. IEEE, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYun Mao, Lawrence K Saul, and Jonathan M Smith. Ides: An internet distance estimation service for large networks. IEEE Journal on Selected Areas in Communications, 24(12):2273–2284, 2006.\n\nVardges Melkonian et al. Mathematical models for a social partitioning problem. American Journal\n\nof Computational Mathematics, 11(01):1, 2021.\n\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-\n\ntations in vector space. arXiv preprint arXiv:1301.3781, 2013a.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111–3119, 2013b.\n\nTS Eugene Ng and Hui Zhang. Predicting internet network distance with coordinates-based apIn Proceedings. Twenty-First Annual Joint Conference of the IEEE Computer and\n\nproaches. Communications Societies, volume 1, pp. 170–179. IEEE, 2002.\n\nTS Eugene Ng and Hui Zhang. A network positioning system for the internet. In USENIX Annual\n\nTechnical Conference, General Track, pp. 141–154, 2004.\n\nJo ̃ao Nogueira. A large-scale and decentralised applicationlevel multicast infrastructure, 2014.\n\nRuksar Parveen and N Sandeep Varma. Friend’s recommendation on social media using different\n\nalgorithms of machine learning. Global Transitions Proceedings, 2021.\n\nB. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 03 2014. doi: 10.1145/2623330.2623732.\n\nBryan Perozzi, Vivek Kulkarni, Haochen Chen, and Steven Skiena. Don’t walk, skip! online learnIn Proceedings of the 2017 IEEE/ACM International\n\ning of multi-scale network embeddings. Conference on Advances in Social Networks Analysis and Mining 2017, pp. 258–265, 2017.\n\nMartin Poirier and Eric Paquette. Rig retargeting for 3d animation.\n\nIn Graphics interface, pp.\n\n103–110, 2009.\n\nMichalis Potamias, Francesco Bonchi, Carlos Castillo, and Aristides Gionis. Fast shortest path distance estimation in large networks. In Proceedings of the 18th ACM conference on Information and knowledge management, pp. 867–876, 2009.\n\nJianzhong Qi, Wei Wang, Rui Zhang, and Zhuowei Zhao. A learning based approach to predict shortest-path distances. In Angela Bonifati, Yongluan Zhou, Marcos Antonio Vaz Salles, Alexander B ̈ohm, Dan Olteanu, George H. L. Fletcher, Arijit Khan, and Bin Yang (eds.), Proceedings of the 23rd International Conference on Extending Database Technology, EDBT 2020, Copenhagen, Denmark, March 30 - April 02, 2020, pp. 367–370. OpenProceedings.org, 2020. doi: 10.5441/002/edbt.2020.34. URL https://doi.org/10.5441/002/edbt.2020.34.\n\nJiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Proceedings of the eleventh ACM international conference on web search and data mining, pp. 459–467, 2018.\n\nBayu Rahmad Syah, Mahyuddin Nasution, Erna Nababan, and Syahril Efendi. Sensitivity of shortest distance search in the ant colony algorithm with varying normalized distance formulas. TELKOMNIKA Indonesian Journal of Electrical Engineering, 19:1251–1259, 08 2021. doi: 10.12928/TELKOMNIKA.v19i4.18872.\n\nSatish Rao. Small distortion and volume preserving embeddings for planar and euclidean metrics. In Proceedings of the fifteenth annual symposium on Computational geometry, pp. 300–306, 1999.\n\nSylvia Ratnasamy, Mark Handley, Richard Karp, and Scott Shenker. Topologically-aware overlay construction and server selection. In Proceedings. Twenty-First Annual Joint Conference of the IEEE Computer and Communications Societies, volume 3, pp. 1190–1199. IEEE, 2002.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nSean C Rhea, Patrick R Eaton, Dennis Geels, Hakim Weatherspoon, Ben Y Zhao, and John Kubia-\n\ntowicz. Pond: The oceanstore prototype. In FAST, volume 3, pp. 1–14, 2003.\n\nFatemeh Salehi Rizi, Joerg Schloetterer, and Michael Granitzer. Shortest path distance approximation using deep learning techniques. In 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), pp. 1007–1014. IEEE, 2018.\n\nS. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding.\n\nscience, 290(5500):2323–2326, 2000.\n\nA. D. Sarma, S. Gollapudi, M. Najork, and R. Panigrahy. A sketch-based distance oracle for webscale graphs. In Proceedings of the Third International Conference on Web Search and Web Data Mining, WSDM 2010, New York, NY, USA, February 4-6, 2010, 2010.\n\nJ ̈org Schl ̈otterer, Martin Wehking, Fatemeh Salehi Rizi, and Michael Granitzer. Investigating exIn 2019 IEEE International Conference on\n\ntensions to random walk based graph embedding. Cognitive Computing (ICCC), pp. 81–89. IEEE, 2019.\n\nMirco Sch ̈onfeld and J ̈urgen Pfeffer. Shortest path-based centrality metrics in attributed graphs with\n\nnode-individual context constraints. Social Networks, 2021.\n\nC. Shaosheng, L. Wei, and X. Qiongkai. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM international on conference on information and knowledge management, pp. 891–900, 2015.\n\nYuval Shavitt and Tomer Tankel. Hyperbolic embedding of internet graph for distance estimation\n\nand overlay construction. IEEE/ACM Transactions on Networking, 16(1):25–36, 2008.\n\nMarcus Sokolowski and Danuta Wasserman. A candidate biological network formed by genes from\n\ngenomic and hypothesis-free scans of suicide. Preventive Medicine, 152:106604, 2021.\n\nLiying Tang and Mark Crovella. Virtual landmarks for the internet. In Proceedings of the 3rd ACM\n\nSIGCOMM conference on Internet measurement, pp. 143–152, 2003.\n\nM. Thorup and U. Zwick. Approximate distance oracles. Journal of the Acm, 52(1), 2004.\n\nKonstantin Tretyakov, Abel Armas-Cervantes, Luciano Garc ́ıa-Ba ̃nuelos, Jaak Vilo, and Marlon Dumas. Fast fully dynamic landmark-based estimation of shortest path distances in very large graphs. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management, CIKM ’11, pp. 1785–1794, New York, NY, USA, 2011. Association for Computing ISBN 9781450307178. doi: 10.1145/2063576.2063834. URL https://doi. Machinery. org/10.1145/2063576.2063834.\n\nAnton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel M ̈uller. Verse: Versatile graph embeddings from similarity measures. In Proceedings of the 2018 World Wide Web Conference, pp. 539–548, 2018.\n\nZ. Xiaohan, A. Sala, C. Wilson, Z. Haitao, and Z. Ben Y. Orion: Shortest path estimation for large social graphs. In Proceedings of the 3rd Wonference on Online Social Networks, WOSN’10, pp. 9, USA, 2010. USENIX Association.\n\nZ. Xiaohan, A. Sala, Z. Haitao, and Z. Ben. Fast and scalable analysis of massive social graphs.\n\nCoPR, 07 2011.\n\nYiding Yang, Xinchao Wang, Mingli Song, Junsong Yuan, and Dacheng Tao. Spagan: Shortest path\n\ngraph attention network. arXiv preprint arXiv:2101.03464, 2021.\n\nHuilin Yuan, Jianlu Hu, Yufan Song, Yanke Li, and Jie Du. A new exact algorithm for the shortest path problem: An optimized shortest distance matrix. Computers & Industrial Engineering, 158: 107407, 2021.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nXiang Yue, Zhen Wang, Jingong Huang, Srinivasan Parthasarathy, Soheil Moosavinasab, Yungui Huang, Simon M Lin, Wen Zhang, Ping Zhang, and Huan Sun. Graph embedding on biomedical networks: methods, applications and evaluations. Bioinformatics, 36(4):1241–1251, 10 2019. ISSN 1367-4803. doi: 10.1093/bioinformatics/btz718. URL https://doi.org/10.1093/ bioinformatics/btz718.\n\nAbeer A Zaki, Nesma A Saleh, and Mahmoud A Mahmoud. Performance comparison of some centrality measures used in detecting anomalies in directed social networks. Communications in Statistics-Simulation and Computation, pp. 1–15, 2021.\n\nKaiwei Zeng, Munan Ning, Yaohua Wang, and Yang Guo. Hierarchical clustering with hard-batch triplet loss for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13657–13665, 2020.\n\nWeitong Zhang, Ronghua Shang, and Licheng Jiao. Complex network graph embedding method based on shortest path and moea/d for community detection. Applied Soft Computing, 97:106764, 2020.\n\nHan Zheng, Eng Keong Lua, Marcelo Pias, and Timothy G Griffin. Internet routing policies and round-trip-times. In International Workshop on Passive and Active Network Measurement, pp. 236–250. Springer, 2005.\n\nChenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-supervised\n\nclassification. In Proceedings of the 2018 World Wide Web Conference, pp. 499–508, 2018.\n\nA APPENDIX\n\nA.1 RELATED WORK\n\nA.1.1 ESTIMATION OF ACCURATE SP DISTANCE\n\nAs an important global measurement on graphs, SP distance reflects the minimum travelling cost from node to node, similar to the geodesic distance on manifolds. Along the rapid emergence of large-scale graphs in many areas, space- and time-efficient estimation of accurate SP distance is urgently required in many downstream applications. In this part, we investigate the direct impact of SP distance estimation in different fields by discussing several real-world scenerios.\n\nCase 1: find nearest points of interest in road and social networks Points of interest (POIs) (Chen et al., 2021a) are specific point locations that someone may find useful or interesting, e.g., hotels, campsites, fuel stations, etc. A real road network may contain millions of nodes, while thousands of users may issue SP distance queries simultaneously for searching the nearest POI from their location, like ’finding restaurants within 5 km distance’ or ’ranking restaurant search results by distance’. To achieve such demands, learning to accurately and fast answer SP distance with limited computing resources is of high significance. Specifically, utilizing limited computing resources means the algorithm should be space- and time-efficient. Thereinto, less storage overhead enables the representations to be stored in users’ mobile devices instead of centrally computing SP on the server. And less query time ensures that the computation of SP distance can be processed in real-time (since some POIs may change their positions frequently over time).\n\nCase 2: construct skeleton graph from mesh for 3D animation In the literature of 3D animation, animating an articulated character requires constructing a skeleton graph to control the movement of the surface, i.e., place the skeleton joints inside the character and specify which parts of the surface are attached to which bone. A critical technique (Aujay et al., 2007; Poirier & Paquette, 2009) to automatically embed a skeleton into a character relies on computing a harmonic function under the SP metric on mesh graphs. This requires finding a group of nodes that locally maximize SP distance with the user-defined node. Since the mesh of a delicate-described character may have tens or hundreds of vertices, estimating and finding such nodes with the longest SP distance accurately and fast are also well-motivated.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nCase 3: estimate latencies in communication networks In large-scale communication networks, the latencies between Internet hosts are defined as a round-trip measurement from one to another (i.e., SP distance), which is utilized for performance optimization in many network applications such as content distribution networks (Ratnasamy et al., 2002), multicast systems (Nogueira, 2014), distributed file system (Rhea et al., 2003), etc.\n\nA.1.2 APPROXIMATE SP REPRESENTATION\n\nHard-coding Perspective Compared with exact SP representations that improve query speed at the expense of huge storage costs, approximate methods are designed to find a compact and scalable representation of high performance both in time and space. The basic idea of these methods is to reduce the complexity of SP distance matrices. Thorup and Zwick (Thorup & Zwick, 2004) initially observe that a hierarchical sparse sampling of nodes could significantly reduce the number of elements in the distance matrix, and all pairs of SP distance are thus approximately represented by the distance relations on those nodes with a bounded error. They also provide a time-efficient algorithm to compute the pruned distance relations. Several later extensions (Baswana & Kavitha, 2006; Enachescu et al., 2008; Chen et al., 2009) are proposed to improve the processing time and space on specific graphs. However, these methods still have limitations on space complexity and accuracy. First, the sampled distance relations take O(α0N 1+α0) space which is not linear to the number of nodes N , thus inducing scalable problems on large graphs. Second, the bounded error is often unacceptable on graphs with smaller diameters since even the most accurate model (with α0 = 2) allows three times the error of real distance.\n\nAddressing these issues, landmark-based distance estimation methods (Potamias et al., 2009; Gubichev et al., 2010; Sarma et al., 2010) are proposed. Instead of sampling hierarchical sets of nodes, landmark-based methods only preserve distance relations between a fixed number of nodes (called landmarks) to others on the graph, and all pairs of SP distance could be bounded by their distance related to landmarks according to triangle inequality (Zheng et al., 2005; Lee et al., 2006; Mao et al., 2006), i.e., for any nodes va and vb,\n\nmax vi∈L\n\n|Dai − Dib| ≤ Dab ≤ min vj ∈L\n\n|Dai + Dib|\n\n(9)\n\nwhere Dab denotes the SP distance between va and vb, L denotes the set of landmarks. The average accuracy could be optimized by selecting proper landmarks that covers as many SPs as possible. Unfortunately, finding the optimal finite set of landmarks with the minimal size has been proved to be NP-hard, which is mapping to a set-cover problem (Balas, 1982). Therefore, several heuristic selection strategies are discussed to tight Equation 9 by leading Dab almost near to its upper bound (Potamias et al., 2009). Other efforts (Gubichev et al., 2010; Tretyakov et al., 2011) are made to store SP trees for each landmark instead of distances at the cost of extra storage and response time. Nevertheless, the approximation performance in these models relies highly on graph structures, since less-centralized graphs (e.g., a grid-like graph) and graphs of large diameters (e.g., a large planar graph) require a large number of distributed landmarks to cover remote pairs of nodes.\n\nLearning Perspective Instead of the hard-coding techniques mentioned above, our work steps forward from a learning perspective of SP distance estimation, which constructs general and scalable representations for arbitrary graphs. Under the low-rank assumption of SP distance matrices, the basic idea of learning-based methods is to transform the graph into a metric space while preserving the distance between pairs of nodes. As the embedding space is low-dimensional and continuous, extracting distance from learning-based SP representations is fast and scalable. However, directly optimizing the distance between all pairs is time-consuming, which takes at least O(N 2) time for computing distance and subsequent optimization. Towards this, many graph coordinate systems (Ng & Zhang, 2002; Tang & Crovella, 2003; Costa et al., 2004; Dabek et al., 2004; Ng & Zhang, 2004) have been studied in the past years. To reduce processing complexity, a feasible learning procedure for very large graphs later proposed in Orion (Xiaohan et al., 2010) contains three steps. First, perform breadth-first search (BFS) from a small landmark set L and record node pairs as well as their distance as training triplets {(cid:104)vl, va, Dla(cid:105)} where vl ∈ L, va ∈ V . Second, create a graph coordinate system M by preserving distance relations among nodes in L, i.e.,\n\narg\n\nmin\n\nLM ={vM\n\ni |vi∈L}\n\n(cid:88)\n\nvi,vj ∈L\n\n|DM\n\nij − Dij|\n\n(10)\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nand vM\n\ni denotes the embedded vector corresponding to the node vi, and DM\n\nwhere vM ij denotes the geodesic distance between vM j measured on M . Third, fix LM and calibrate distance between other nodes and landmarks iteratively. Among these steps, the metric tensor defined on M significantly affects the accuracy of distance estimation, and models regarding embedding in euclidean space (Rao, 1999; Lee, 2009; Xiaohan et al., 2010) and hyperbolic space (Shavitt & Tankel, 2008; Xiaohan et al., 2011) are well studied respectively.\n\ni\n\nInspired by the great success in graph representation learning (GRL), further work (Rizi et al., 2018; Schl ̈otterer et al., 2019) including ours treats M as an agnostic but definite manifold learned by GRL techniques and estimates distance based on learnable metric criteria (usually a multi-layer perceptron). Therefore, the learning task here is converted from ”calibrate the position of each node” to ”learn powerful metric criteria to extract distance everywhere.” This novel paradigm achieves higher accuracy with reduced training time despite the fact that we are unsure about whether general GRL models could embed sufficient information to infer all pairs of SPs. In this paper, we thus discuss an interpretable SP representation learning method and improve the comprehensive performance of SP distance estimation.\n\nA.1.3 GRAPH REPRESENTATION LEARNING\n\nGraph representation learning (GRL) organizes symbolic objects (such as nodes, edges, and clusters) in a way such that their similarities on the graph are well-preserved in the low-dimensional embedding space. Currently, most of these methods focus on preserving arbitrary linkage on graphs by considering high-order adjacency matrices, and are categorized into matrix factorization (MF) and random walk (RW) approaches. Thereinto, our work shares strong connections with general RW approaches, which embed remote nodes’ correlation within linear complexity compared with MF methods. The basic idea of RW-based learning methods proposed in Deepwalk (Perozzi et al., 2014) is to dump complicated linkage structure on graphs into a few fixed-length node sequences in a statistical view and learn node embeddings to reflect their co-occurrence on walk paths using a skip-gram algorithm (Mikolov et al., 2013a;b). The learning process is to solve a maximum likelihood optimization problem based on observed sequences, i.e., for any nodes va and vb,\n\narg max Za,Zb\n\nlog ˆσ(ZaZT\n\nb ) + λE\n\nvk∼ ̃Pn(V )[log ˆσ(−ZaZT\n\nk )])]\n\n(11)\n\nwhere Za denotes the embedded vector of va, λ denotes the number of negative samples, ˆσ(·) denotes the sigmoid function where ˆσ(x) = (1 + e−x)−1, and ̃Pn(·) denotes a probability distribution of the negative sampling. Several practical strategies are proposed in the past few years to simulate structure-aware traversal on graphs in RW-based methods (Grover & Leskovec, 2016; Cao et al., 2016; Perozzi et al., 2017; Chen et al., 2018). In detail, to enhance sensibility on divergent structures, Node2Vec (Grover & Leskovec, 2016) exploits a biased random walk strategy to perform combinatorial traversal on graphs, including breadth-first search (BFS) and depth-first search (DFS), which explores both local-neighborhood linkage and correlation with remote nodes simultaneously. To reflect the locality around each node, a random walk with restarting mechanism (called random surfing) is applied in learning point-wise mutual information (PMI) representations (Cao et al., 2016). For capturing multi-scale representations of different-order neighborhoods, hierarchical random walks by skipping some of the nodes on paths are also proposed (Perozzi et al., 2017; Chen et al., 2018). Recently, Schlotterer et al. (Schl ̈otterer et al., 2019) observed that RW-based methods perform better than others in exploring a wide range of distance and evaluated these methods as being helpful for SP distance estimation. However, a specific and insightful investigation of RW-based SP representation remains to be studied. In this paper, we discuss a novel biased random walk strategy toward high-order SP exploration and provide an explicit optimization algorithm for distance-preserved representation.\n\nA.2 EFFICIENT IMPLEMENTATION OF BCDR ALGORITHM\n\nWe discuss here an efficient implementation to integrate the techniques mentioned in Section 3.1 and 3.2. Our algorithm is presented in Algorithm 1, including constructing SP representations and answering online distance queries. The description and analysis of these procedures are provided as follows.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: construct SP representations & answer distance queries\n\nInput: input graph G = (V, E), set of landmarks L, distance queries of node pairs Q, dimension of embeddings d, number\n\nof walk paths on each node win, length of walk paths lin, BC decay coefficient ζ, number of resampled walk paths on each node wout, length of resampled walk paths lout, distance decay coefficient α, training epochs m, learning rate ηr, usage of fast query τ , usage of boosting χ.\n\nOutput: SP representation Z, predictor gφ, estimated distance D. (optional: regressors b1, b2, global representation Z (cid:48))\n\n36 Def cons BCDR(G, d, L, win, lin, ζ, wout, lout, α, m, η):\n\n1 Def sim DT with BC(G, L):\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\ndistance triplets T := list[] approximate BCs γ := dict{vi : 0, ∀vi ∈ V } for each landmark vi ∈ L do\n\nγ[vi] ← 1 for each node vj ∈ V reached by BFS from vi do\n\nappend (vi, vj, Dij) to T γ[vj] ← γ[vj] + 1 Dij\n\nend\n\nend return T , γ\n\n12 Def sim BC Walk(G, vi, win, lin, γ):\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\n35\n\ndistance map Di := dict{vi : 0} visit counter B := dict{vj : 0, ∀vj ∈ V } for walk k from 0 to win do visit sign set Si := {vi} current node vc := vi length ci := 0 while ci < lin do\n\nprobabilities of the next candidate nodes ̃P k for vj ∈ Nc ∧ vj /∈ Si do\n\nc := dict{}\n\nc [vj] ← γ[vi] × (2 − tanh(ζ − B[vj]))\n\n ̃P k if vj ∈ Di then\n\nDi[vj] = min{Di[vj], Di[vc] + 1}\n\nelse\n\nDi[vn] = Di[vc] + 1\n\nend\n\nend sample next vn from normalized ̃P k vc ← vn, Si ← vn ci ← ci + 1 B[vn] ← B[vn] + 1\n\nc\n\nend\n\nend return Di\n\nT, γ ← sim DT with BC(G, L), P = list[] for vi ∈ V do\n\nDi ← sim BC Walk(G, vi, win, lin, γ) probabilities of candidates nodes ̃Pi = dict{} for vj ∈ Di.keys do\n\n ̃Pi[vj] := αDi[vj ] · γ[vj]\n\nend for walk k from 0 to wout do\n\nsample walk path pi of length lout by normalized ̃Pi append pi to P\n\nend\n\nend maximize Equation 6 with ZN ×d and P by skip-gram. define learnable distance predictor gφ := (Rd, Rd) → R for each epoch from 0 to m do\n\nLM SE(va, vb) := [gφ(Za, Zb) − Dab]2 , ∀(va, vb, Dab) ∈ T minimize LM SE with φ at the learning rate ηr by SGD.\n\nend if χ then\n\ndefine CatBoostRegressor b1, b2. Z (cid:48) := {Z(cid:48) Train b1 by (Z(cid:48) Train b2 by (Z(cid:48)\n\ni := list[Dij]|L| b) → Dab b, b1(Z(cid:48)\n\na, Z(cid:48) a, Z(cid:48)\n\na, Z(cid:48)\n\ni|Z(cid:48)\n\nj=0, ∀vj ∈ L}\n\nb), gφ(Za, Zb)) → Dab\n\nend return Z, gφ [, Z (cid:48), b1, b2]\n\n62 Def query BCDR(Q, Z, gφ, τ [, E, Z (cid:48)]):\n\nestimated distance D[va, vb] = gφ(Za, Zb), ∀(va, vb) ∈ Q if χ then\n\nD[va, vb] = b2(Z(cid:48)\n\na, Z(cid:48)\n\nb, b1(Z(cid:48)\n\na, Z(cid:48)\n\nb), D[va, vb])\n\nend if not τ then\n\nD[va, vb] ← 1, ∀(va, vb) ∈ Q ∩ E\n\nend return D\n\n37\n\n38\n\n39\n\n40\n\n41\n\n42\n\n43\n\n44\n\n45\n\n46\n\n47\n\n48\n\n49\n\n50\n\n51\n\n52\n\n53\n\n54\n\n55\n\n56\n\n57\n\n58\n\n59\n\n60\n\n61\n\n63\n\n64\n\n65\n\n66\n\n67\n\n68\n\n69\n\n70\n\nA.2.1 SIMULATION OF DISTANCE TRIPLETS & BC WALK\n\nTo simulate distance triplets (line 1 to 11), we perform BFS from a fixed number of landmarks L and record their distance to each node on the graph (line 6 and 7). L comprises nodes that are selected by heuristic strategies (e.g., by their degrees in descending order or randomly), and the simulated triplets with linear complexity reflect a sufficient group of distance relations among V ×V for metric learning. Before simulating BC walk paths, pre-computed BC of each node is also required at first, which takes at least O(N M ) time on unweighted graphs for an exact solution. To reduce the time complexity, we consider a time-efficient approximation by integrating this process into the above Intuitively, BC(va) measures some kind of relationship simulation of distance triplets (line 8). between va and the centers of the graph, and nodes with larger BC values possess a shorter average SP distance to any node on the graph. Since BFS visits each node just in ascending order of distance, we thus estimate BC on each node by the average distance to all landmarks in L without introducing extra time complexity.\n\nThen, in the simulation of BC walk paths (line 12 to 35), we are interested in ”nodes on these walk paths” instead of the full paths themselves, and the former with their distance to the root vi (i.e., Di) will be passed to feed subsequent construction (line 39). Therefore, we enlarge the node coverage by a decay mechanism on BC to diverge different walks. Note that BC-based random walk possessing the ability to explore remote nodes tends to choose the paths that are prone to travel further, which causes ignorance of nodes on some dead ends. Addressing this, as stated in line 22, the probability of transiting to a node with large BC will be saturated after a sufficient number of walks passing through it, which means some rare paths are getting much easier to be visited later.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nA.2.2 LEARNING EMBEDDINGS & DISTANCE PREDICTOR\n\nTo learn node embeddings Z on SP structures (line 37 to 49), we initially sample a group of BC walk paths as distance maps {Di} (line 39), and then resample from them to feed the skip-gram algorithm by considering distance decay and BC (line 42 and line 45), which preserves the property of distance relations discussed in Theorem 1. Besides, the resampling process also provides a shape transform of observed node sequences from win × lin to wout × lout. Let β = (winlin)/(woutlout) > 1, and this property leads the actual time cost of learning embeddings to be reduced to its 1/β, compared with learning directly on the original paths.\n\nFor the learning of the distance predictor (line 37, and line 50 to 54), we utilize a two-layer fully connected neural network as the predictor, which takes the concatenation of two nodes’ embeddings as input, and outputs a scalar to indicate the distance (line 50). The predictor model is learned from distance triplets T by minimizing a mean squared error (MSE) between the predicted value and the real distance using the stochastic gradient descent (SGD) technique. Finally, the parameter of neural network φ and node embeddings Z are stored as graph SP representations.\n\nTo improve the prediction results, we further integrate the distance predictor with CatBoost techniques (Dorogush et al., 2018). Initially, we treat the feature of nodes as a combination of global features and local features. Thereinto, local features are already represented by Z, since we have constructed SP representations on each node locally. For the global feature of any node vi, we directly leverage the distance to each landmark as its global embedding Z(cid:48) i. Then, we train two CatBoost regressors (i.e., b1, b2) in turn. The first regressor b1 takes global features of two nodes as input and predicts their distance as output (line 58), while the second regressor b2 takes as input not only such global features, but the distance predicted from both global (i.e., b1(Z(cid:48) b)) and local (i.e., gφ(Za, Zb)) features. Finally, the outputs of b2 are regarded as the final prediction results of SP distance.\n\na, Z(cid:48)\n\nA.2.3 ANSWERING DISTANCE QUERIES\n\nFor distance queries, We provide two versions with different properties based on the same SP representations. As revealed in previous studies of learning-based SP representation methods, the distance relations in local neighborhoods are really hard to converge, which causes a decline in average accuracy (Rizi et al., 2018; Kutuzov et al., 2019). For an input query pair (va, vb), it is helpful to alleviate such decline if we perform an extra search among first-order neighbors (i.e., search in the adjacency matrix) to judge if (va, vb) is an edge in the graph. Since this practice prolongs the total response time, we also preserve a fast version (BCDR-FQ) without neighbors searching for some potential applications.\n\nA.2.4 PARALLELISM\n\nThe implementation of BCDR is easy to be highly parallelized. In detail, the construction of SP representations could be divided into three parts, including simulating distance triplets, performing BC-based random walk, training embeddings, as well as the distance predictor. First, the BFS from each landmark could be parallelized at a thread level up to the size of the landmark set. Second, for the simulation of BC walk paths, the paths from different roots could also be simulated simultaneously. Third, the training process in the skip-gram algorithm and neural network could be locally parallelized by matrix computations.\n\nA.3 THEOREM FOR CLARIFYING THE SIGNIFICANCE OF BC-BASED RANDOM WALK\n\na\n\na = |N (h)\n\na | as the number of nodes in the set. Nodes in N (h)\n\nTheorem 2. Define N (h) a = {vj|Daj = h} as a set of nodes that are h-hops away from va and N (h) could be divided into two sets, i.e., eh(va) and fh(va). Thereinto, eh(va) comprises nodes that have an edge with the nodes in N (h+1) (called connective nodes), and fh(va) comprises the other nodes (called final nodes). The BC value of each node is approximated by considering only the shortest path of nodes within a range of k-hops locally. Let ̃PR(N (h) to N (h+1) ) represent that by a BC-based random walk. Let ̃PR(fh(va) → eh(va)) be the probability of transition from fh(va) to eh(va). E(va) is the\n\n) represent the probability to transit from N (h)\n\nby a naive random walk, and ̃PB(N (h)\n\na → N (h+1)\n\na → N (h+1)\n\na\n\na\n\na\n\na\n\na\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\neccentricity of va, E(va) = maxvb∈G Dab. Then, for sufficient large E(va), any node va ∈ G and any h > 1,\n\n ̃PB(N (h) ̃PR(N (h)\n\na → N (h+1) a → N (h+1)\n\na\n\na\n\n)\n\n)\n\n= 1 + B(k) + C\n\nlim k→E(va)−1−h\n\nB(k) + C =\n\nA2 − 1 A1\n\n+ C > 0\n\nA1 =\n\n|eh(va)| |fh(va)|\n\n, A2 =\n\n1 ̃PR(fh(va) → eh(va))\n\n, C ≥ 0.\n\nProof. We simplify symbols N (h)\n\na , N (h)\n\na , eh(va), fh(va) as Nh, Nh, eh, fh for short.\n\n→ N h = #(\n\nmin{E(va),h+k−1} (cid:91)\n\n{vj|vj ∈ N (i)\n\na })\n\n← N h = #(\n\ni=h\n\nh (cid:91)\n\n{vj|vj ∈ N (i)\n\na })\n\ni=max{0,h−k+1}\n\n(12)\n\n(13)\n\n(14)\n\nwhere #(·) is a counting function indicating the number of occurrence times of specified nodes, i.e., the cardinality of a sampled set.\n\nAccording to the definition in Theorem 2, we have Nh = |eh| + |fh|. Since only nodes in eh could travel from Nh to Nh+1, We firstly consider ̃PR(eh → eh+1|eh → Nh+1) and ̃PB(eh → eh+1|eh → Nh+1).\n\nFor general random walks, the choice of destination is based on uniform sampling, thus causing\n\n ̃PR(eh → eh+1|eh → Nh+1) =\n\n|eh+1| Nh+1\n\n(15)\n\nFor BC-based random walk, we need calculate BC value of nodes of eh+1 and fh+1 for starters. Let BC(eh+1) and BC(fh+1) represent the BC value of nodes in eh+1 and fh+1 respectively, and the ←\nN h → Nh+1}, {Nh+1 → correspond legal SPs counts come from 4 sources as { →\nN h+2} and {Nh+1 → Nh+1}. And we use BC({· → ·}) as the BC gain from the specified source, then\n\n→ N h+2}, {\n\n← N h →\n\nBC({\n\nBC({\n\n→ N h+2}) =\n\n← N h → ←\nN h → Nh+1}) =\n\n← N h ←\n\n→ N h+2\n\nN h(|fh+1| · 0 + |eh+1|β(1) e )\n\n→ N h+2}) =\n\nBC({Nh+1 → BC({Nh+1 → Nh+1}) = N 2\n\n→\n\nh+1β(2)\n\ne\n\nN h+2(|fh+1| · 1 + |eh+1|β(1) e )\n\nwhere β(1) gain between nodes both in eh+1, which are constantly related with G. Therefore, we have\n\ne means average BC gain between nodes in eh+1 and\n\nN h+2, and β(2)\n\ne means average BC\n\n→\n\nBC(eh+1) =\n\n← N h\n\n→ N h+2 + (\n\n← N h +\n\nLikewise, we calculate\n\n→\n\nN h+2)|eh+1|β(1)\n\ne +\n\n→\n\nN h+2|fh+1| + N 2\n\nh+1β(2)\n\ne\n\n(17)\n\nBC(fh+1) =\n\n←\n\n→ N h+2 · 0 + →\n\n← N h ←\nN h +\n\nN h+2)β(1)\n\n=(\n\nf + N 2\n\nh+1β(2)\n\nf\n\nN h(|fh+1|β(1)\n\nf + |eh+1| · 0) +\n\n→\n\nN h+2(|fh+1|β(1)\n\nf + eh+1 · 0) + N 2\n\nh+1β(2)\n\ne\n\nTo compare the above BC(eh+1) and BC(fh+1),\n\nBC(fh+1) BC(eh+1)\n\n=\n\n← N h\n\n→ N h+2 + (\n\n(\n\n← N h + →\n\n→\n\nN h+2)β(1)\n\nf + N 2\n\n← N h +\n\nN h+2)|eh+1|β(1)\n\ne +\n\nf\n\nh+1β(2) N h+2|fh+1| + N 2\n\n→\n\nh+1β(2)\n\ne\n\n19\n\n(16)\n\n(18)\n\n(19)\n\nUnder review as a conference paper at ICLR 2023\n\nnote that for any Nj and N(x,y) =\n\n→ N 0 −\n\n← N x −\n\n→ N y where j, x, y ∈ {0, E(va)},\n\nlim k→E(va)−1−h\n\nNj N(x,y)\n\n=\n\nlim k→E(va)−1−h\n\n(cid:15)(k) = 0\n\n(20)\n\nEquation 18 is reduced to\n\nBC(fh+1) BC(eh+1)\n\n=\n\n2(cid:15)(k)β(1) 1 + 2(cid:15)(k)n|eh+1|β(1)\n\nf + (cid:15)(k2)β(2) e + (cid:15)(k)|fh+1| + (cid:15)(k2)β(2)\n\nf\n\ne\n\n= 2β(1)\n\nf (cid:15)(k)\n\n(21)\n\nThen, we perform weighted random sampling based on BC and get\n\n ̃PB(eh → eh+1|eh → Nh+1) =\n\n|eh+1|\n\n|eh+1| + 2|fh+1|β(1)\n\nf (cid:15)(k)\n\n(22)\n\nNow, we consider the relation between ̃PR(Nh → eh+1|Nh → Nh+1) and ̃PB(Nh → eh+1|Nh → Nh+1).\n\n ̃PB(Nh → Nh+1) ̃PR(Nh → Nh+1)\n\n=\n\n ̃PB(eh) ̃PB(eh → Nh+1) ̃PR(eh) ̃PR(eh → Nh+1)\n\n=\n\n ̃PB(Nh−1 → eh) + ̃PB(Nh−1 → fh) ̃PB(fh → eh) ̃PR(Nh−1 → eh) + ̃PR(Nh−1 → fh) ̃PR(fh → eh) (cid:104) ̃PB(Nh−1 → eh) − ̃PR(Nh−1 → eh)\n\n(cid:105)\n\n[1 − ̃PR(fh → eh)]\n\n= 1 +\n\n ̃PR(Nh−1 → eh) + ̃PR(Nh−1 → fh) ̃PR(fh → eh)\n\n+\n\n ̃PR(fh → eh)(cid:15) ̃PR(Nh−1 → eh) + ̃PR(Nh−1 → fh) ̃PR(fh → eh)\n\n= 1 +\n\n|fh|[1 − 2|fh|β(1)\n\nf (cid:15)(k)][1 − ̃PR(fh → eh)]\n\n(1 + |fh|\n\n|eh| )[|eh| + 2|fh|β(1)\n\nf (cid:15)(k)] ̃PR(fh → eh)\n\n+\n\n ̃PR(fh → eh)(cid:15) ̃PR(Nh−1 → eh) + ̃PR(Nh−1 → fh) ̃PR(fh → eh)\n\n(23)\n\nLet C =\n\n ̃PR(fh→eh)(cid:15) ̃PR(Nh−1→eh)+ ̃PR(Nh−1→fh) ̃PR(fh→eh)\n\n, B(k) =\n\nSince C is a non-negative value independent of k, finally we get\n\n|fh|[1−2|fh|β(1)\n\nf (cid:15)(k)][1− ̃PR(fh→eh)]\n\n(1+ |fh|\n\n|eh | )[|eh|+2|fh|β(1)\n\nf (cid:15)(k)] ̃PR(fh→eh)\n\n.\n\nwhere\n\n ̃PB(Nh → Nh+1) ̃PR(Nh → Nh+1)\n\n= 1 + B(k) + C\n\nlim k→E(va)−1−h\n\nB(k) + C =\n\nA2 − 1 A1\n\n+ C.\n\n(24)\n\n(25)\n\nRemark 1. Equations 12 and 13 in Theorem 2 give definite conditions under which BC-based random walk travels further than random walk. Since nodes in va’s h-order neighborhood N (h) could always be divided into eh(va) and fh(va) like Figure 3a, BC-based random walk improves exploration distance beyond local loops and dead ends in contrast with naive random walk by two aspects. On one side, even if N (h) comprises a larger number of nodes in fh(va) than eh(va) and thus A1 decreases, BC-based random walk tends to transit in eh(va) to get near with the next desired set N (h+1) . On the other side, even if nodes in fh(va) have fewer links to those in eh(va) and thus A2 increases, BC-based random walk tends to transit between fh(va) and eh(va) instead of looping within fh(va) as well as N (h−1)\n\na\n\na\n\na\n\na\n\n.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nA.4 PROOF OF PROPOSITION 1\n\nProof. In Proposition 1, we optimize an approximate objective ˆLn instead of Ln, i.e.,\n\nˆLn(Z) =\n\n ̃P (vi)\n\n(cid:88)\n\nvi∈V\n\n(cid:110)\n\nE\n\nvj ∼ ̃QWi (V )[log ˆσ(ZiZT\n\nj )] + λE\n\nvk∼ ̃Pn(V )[log ˆσ(−ZiZT\n\nk )]\n\n(cid:111)\n\nHere, we rewrite the negative sampling item of Equation 26 as\n\nE\n\nvk∼ ̃Pn(Wi)[log ˆσ(−ZiZT\n\nk )]) =\n\n= ̃Pn(vj|vi)[log ˆσ(−ZiZT\n\nj )] +\n\n(cid:88)\n\n ̃Pn(vk|vi)[log ˆσ(−ZiZT\n\nk )]\n\nvk∈Wi\n\n(cid:88)\n\nvk∈Wi\\{vj }\n\n ̃Pn(vk|vi)[log ˆσ(−ZiZT\n\nk )]\n\n(26)\n\n(27)\n\nThen, for each pair of vi ∈ V and vj ∈ Wi, we get independent objective by combing similar items from the total likelihood ˆLn, and reach\n\nˆLn(Z) =\n\n(cid:88)\n\n(cid:88)\n\nL(cid:48)(Zi, Zj)\n\nvi∈V\n\nvj ∈Wi\n\nL(cid:48)(Zi, Zj) = ̃P (vi, vj) log ˆσ(−ZiZT\n\nj ) + λ ̃P (vi) ̃Pn(vj|vi) log ˆσ(−ZiZT j )\n\nLet ˆD = ZZ T , for each node pair (va, vb) with SP distance ˆDab, consider\n\nˆDab = ZaZT\n\nb = arg max Za,Zb\n\nL(cid:48)(Za, Zb)\n\n(28)\n\n(29)\n\nRemember that ̃QWi is a distribution resampled from ̃PWi, as an efficient approximation to ̃P ̊Wi i.e.,\n\n,\n\n ̃QWi(vj) =\n\n(cid:80)\n\nαDij BC(vj)\n\nαDik BC(vk)\n\nvk∈Wi Denote BC(vb) by γb, according to Equation 30, the joint distribution of each pair (va, vb) could be formulated as\n\n ̃P (va, vb) = ̃P (va) · ̃P (vb|va) = ̃P (va) · αDab γb\n\n(30)\n\n(31)\n\nThen, consider the formualtion of ̃Pn(vb|va). Recall that the negative sampling is a uniform distribution on Wa which are simulated by BC-based random walk, and the probability of vb’s occurrence relies on γb and Wa. Thus there holds\n\n ̃Pn(vb|va) =\n\nγb κ(va)\n\n(32)\n\nwhere κ(va) is in proportion with the number of nodes covered by Wa on the graph.\n\nFurthermore, Equation 29 could be rewritten as\n\nˆDab = arg max ˆDab\n\n ̃P (va)αDabγb log ˆσ( ˆDab) +\n\nλ κ(va)\n\n ̃P (va)γb log ˆσ(− ˆDab)\n\n(33)\n\nSolve the above problem by just let ∂L(va,vb)\n\n∂ ˆDab\n\nbe equal to zero, i.e.,\n\n∂L(va, vb) ∂ ˆDab After some simplification, we get\n\n= ̃P (va)αnγb ˆσ(1 − ˆDab) −\n\nλ κ(va)\n\np(va)γb ˆσ(1 + ˆDab) = 0\n\n(34)\n\nLet (cid:15) = λκ−1(va) and there holds\n\nˆDab = Dab log α − log\n\nλ κ(va)\n\nˆDab = − log (cid:15) + Dab log α\n\n21\n\n(35)\n\n(36)\n\nUnder review as a conference paper at ICLR 2023\n\nA.5 REMARK FOR THE RELATIONSHIP BETWEEN DISTANCE MATRIX & DISTANCE-BASED\n\nSIMILARITY MATRIX\n\nRemark 2. Equation 7 in Proposition 1 reveals the linear projections between elements in ˆD and D. Thereinto, − log (cid:15) is a big positive constant with respect to |log|Wa|| − | log λ|, and Dab log α is a negative value that decreases linearly with SP distance Dab. It indicates that there exists a finite distance range n ∈ N+, for each node vb ∈ {vx|Dax ≤ n}, the distance relation between va and b → ˆDab > 0. It also reveals that the significance of vb could be well-optimized by converging ZaZ T distance resampling is to preserve the SP distance relations between nodes which are well-observed on given arbitrary walk paths. Besides, although the similarity matrix ˆD could not directly tell the absolute distance, it also shares similar properties with D, i.e., if we fix va as the source node in a path, the comparison between the similarities of (va, vb) and (va, vc) just reflects the SP distance relations between them. This practical property is discussed in Theorem 1.\n\nA.6 PROOF OF THEOREM 1\n\nProof. In terms of node pair (va, vb), as proved in Proposition 1, their similarity ˆDab in the embedding space varies linear with respect to the SP distance Dab on the graph, i.e.,\n\nLikewise, for (va, vc), there holds\n\nˆDab = − log (cid:15) + Dab log α\n\n| ˆDac| = − log (cid:15) + Dac log α\n\n(37)\n\n(38)\n\nwhere 0 < α < 1 and (cid:15) relies on Wa which is independent of vb and vc.\n\nThen, consider the distance relation of va, vb and vc, there holds\n\n(Dab − Dac)( ˆDab − ˆDbc) = (Dab − Dac)((Dab − Dac)) log α = log α · (Dab − Dac)2 ≤ 0.\n\n(39)\n\nA.7 MOTIVATION OF BCDR PROCEDURE AGAINST DIRECTLY SAMPLING SPS\n\nWe further clarify in this section the motivation for leveraging BCDR instead of directly sampling SPs. As a prerequisite, it should be acknowledged that we need sampled SPs as observation to optimize node embeddings Z. However, to perform sufficient observation on all pairs of shortest paths is time-consuming, which takes at least O(N 2) time on sparse unweighted graphs. Towards this, an intuitive idea is to sample a limited number of paths that starts only at a few nodes (landmarks). But it will introduce strong bias on the landmarks and ignore many shortest paths far away from them. To alleviate this bias, in BCDR, we hope to observe shortest paths rooted at all nodes on the graph (instead of the landmarks only). Therefore, we need some strategies to overcome the huge complexity of directly sampling these paths (since it requires performing BFS on all nodes). The proposed strategy is BC-based random walk where we intend to equip ’random walk’ with the awareness of high-order SP structure and make the sampled walk paths much more likely to be certain shortest paths. This strategy is comparatively efficient since the sampling complexity is proportional to its path length l. Then the subsequent module DR further resampled from these paths for implicitly preserving SP distance relations on Z.\n\nAccording to the above discussion, a brief procedure of BCDR with its motivation could be summarized as follows.\n\n• estimate BC just by BFS from only a few nodes (landmarks).\n\nmotivation: determine which node is prone to trigger high-order explorations of SP distances.\n\n• perform BC-based random walk.\n\nmotivation: observe the potential shortest paths rooted at each node sufficiently.\n\n• leverage DR for resampling approximate random shortest paths.\n\nmotivation: implicitly preserving distance relations on observed paths.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\n• optimize Z from the observation of the resampled paths.\n\nmotivation: reflect the SP structure on the graph instead of arbitrary linkage.\n\nEach step above possesses linear complexity with respect to N (number of nodes in the graph).\n\nBesides, we are convinced of the necessity of BCDR procedure and would like to explain it carefully from both technical and empirical perspectives.\n\nFrom a technical perspective, directly leveraging shortest paths as observation to optimize Z has a few shortcomings.\n\n• Prohibitive Complexity of Sufficient Observation. Observing all pairs of SP distance requires at least O(N 2) time for sparse unweighted graphs. Alternatively, an insufficient observation with linear complexity will cause a loss in accuracy (see experimental results below).\n\n• Inflexible Path Length for Optimization. Since we leverage the skip-gram algorithm for optimizing Z, it should be clear how long the sliding window size is, serving to reconstruct the distance relations between nodes. But shortest paths rooted at a certain node factually possess significantly divergent path lengths, which causes difficulty in determining proper sliding window size on different paths, i.e., a longer window helps to capture long-distance correlation but causes indistinguishable in shorter paths and vice versa. Alternatively, if we only select shortest paths of a certain fixed length, paths shorter than this length will be ignored, thus impairing the performance.\n\nCorrespondingly, the BCDR procedure overcomes the above shortcomings as follows.\n\n• Linear Complexity of Such Observation. Instead of directly simulating shortest paths, we sample paths by BC-based random walk and transform the paths into approximate random shortest paths by DR. Both of these operation share linear time complexity. Also, the optimization on such resampled paths is proved to share similar properties with that on real shortest paths by Proposition 1 and Theorem 2.\n\n• Flexible Path Length for Optimization. Since the paths are resampled from random paths, the number and length of them (i.e., wout, lout) could be customized. We are thus able to fix them at a certain proper length for subsequent optimization.\n\nFrom an empirical perspective, we further construct and evaluate 6 competitive baselines which have the same architecture and hyper-parameters as BCDR, but adopt different intuitive strategies to directly optimize on shortest paths. The basic description of these baselines is stated as follows.\n\n• Shortest Paths on Landmarks only (SPoL) Since we need anyway perform BFS from landmarks to acquire distance triplet for learning distance predictor, we intuitively retrieve the shortest paths starting from the landmarks. This operation introduces little extra time cost. The size of landmark set is the same as BCDR (i.e., |L| = 80)\n\n• Shortest Paths on Landmarks only with Fixed Length (SPoL-F) This is similar to SPoL but\n\nrestricts the output walk length at a certain level (the same as BCDR, i.e., lout = 10).\n\n• Shortest Paths on All Nodes (SPoN) In BCDR, we perform BC random walk on each node va to locate its position on the graph. Here, we directly sample shortest paths from va to any other nodes instead. Specifically, for each source node va, we take a uniform sampling over V to acquire the destination nodes, and retrieve the shortest paths between them. The number and max length of shortest paths on each node is the same as BCDR (i.e., wout = 40, lout = 10).\n\n• Shortest Paths on All Nodes with Fixed Length (SPoN-F) This is similar to SPoN but restricts\n\nthe output walk length (the same as BCDR, i.e., lout = 10).\n\n• Shortest Paths on Arbitrary Node Pair (SPoANP) We randomly select a group of node pairs (vs, vt) and retrieve one of the shortest paths between them by BFS. The number of paths is the same as the total number of walk paths on all nodes in BCDR (i.e., N × wout).\n\n• Shortest Paths on Arbitrary Node Pair (SPoANP-F) This is similar to SPoANP but restricts\n\nthe output walk length (the same as BCDR, i.e., lout = 10).\n\nAll of the above baselines are evaluated on GrQc dataset, and the experimental results are presented in Table 3.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Comparisons between BCDR and directly sampling SPs by different intuitive strategies. PT: pre-processing time, ST: time of sampling paths, mAE: mean of Absolute Error, mRE: mean of Relative Error.\n\nModel SPoL SPoL-F SPoN SPoN-F SPoANP SPoANP-F BCDR\n\nPT 60.71 s 60.46s 283.0 s 2,702 s 282.1 s 6,341 s 69.61 s\n\nST 15.36s 15.63 s 235.5 s 2,656 s 234.4 s 6,290 s 27.66 s\n\nmAE 0.9703 1.2874 1.0411 1.2961 1.3564 1.3294 0.7043\n\nmRE 0.1641 0.1961 0.1645 0.1969 0.2047 0.2003 0.1274\n\nWe see from the table that BCDR outperforms all the baselines in approximation quality (i.e., mAE and mRE) within proper time. Specifically, SPoL possesses desirable pre-processing time since only the shortest paths rooted at landmarks are considered. But they are plagued with insufficient observation of other shortest paths that do not pass through landmarks. SPoN and SPoANP suffer huge complexity when retrieving shortest paths on the whole graph, and perform even worse due to the uncertainty of reasonable sliding window size. From SPoL-F, SPoN-F, and SPoANP, we see that even if the path length is fixed, some uncaptured shorter paths will also cause a loss in accuracy.\n\nA.8 DATASETS\n\nTo thoroughly evaluate our proposed method, we conduct experiments on real-world graphs and synthetic graphs with divergent properties on sizes, structures, diameters, etc. Thereinto, real-world graphs are extracted from Stanford Large Network Dataset Collection (Leskovec & Krevl, 2014), and synthetic graphs are simulated according to specific rules described in A.8.6. The visualization results of each graph are illustrated in Figure 6, and the corresponding statistics are presented in Table 4. In the experiments, we show the efficiency and scalability of BCDR on real-world graphs of different sizes and test on smaller synthetic graphs with different structures for further analysis of exploration range and distance preservation. Here are brief descriptions of these graphs:\n\nA.8.1 CORA\n\nThis is a graph that describes the citation relationship of papers, which contains 2708 nodes and 10556 directed edges among them. Each node also has a predefined feature with 1433 dimensions.\n\nA.8.2 FACEBOOK\n\nThis is a graph that describes the relationship among Facebook users by their social circles (or friend lists), which is collected from a group of test users. Facebook has also encoded each user with a reindexed user ID to protect their privacy.\n\nA.8.3 GRQC\n\nThis is a graph recorded from the e-print arXiv in the period from January 1993 to April 2003, which represents co-author relationships based on their submission. Each undirected edge (vi, vj) represents that an author vi is co-authored a paper with another author vj. If one paper is owned by k authors, a complete subgraph of k nodes is generated correspondingly.\n\nA.8.4 DBLP\n\nThis is a graph collected as a computer science bibliography that provides a comprehensive list of research papers in computer science. As an undirected collaboration network, each edge reflects the corresponding two authors who have at least one paper together.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\n(g)\n\n(h)\n\n(f)\n\n(i)\n\n(j)\n\n(k)\n\nFigure 6: Visualization results of the graphs used for evaluation. (a): Cora. (b): Facebook. (c): GrQc. (d): DBLP. (e): YouTube. (f): CG. (g): TG. (h): TCG. (i): TRG. (j): SG. (k): NG.\n\nA.8.5 YOUTUBE\n\nThis is a graph constructed from users’ social relations on a video-sharing website Youtube. Each node represents a user, and each edge indicates a friendship between two users.\n\nA.8.6 SYNTHETIC GRAPHS\n\nWe also construct some smaller graphs reflecting one or some of the typical sub-structures which are frequently occurred in complex graphs. The simulation rules of each graph are listed as follows.\n\n• Circle Graph (CG): this is a graph that contains several circles of different sizes. The simulation of circle graphs takes an iterative process where for each newly introduced circle, there are a limited number of nodes (called exit nodes) connected to the previous circles.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Statistics of the graphs used for evaluation. N denotes the number of nodes, M denotes the number of edges, RoBC denotes the range of BC, mBC denotes the average BC on nodes, D denotes the diameter of a graph, BFS denotes the average processing time of breadth-first search on all nodes. Each graph is considered an undirected and unweighted graph in the SP representation problem.\n\nCora Facebook GrQc DBLP YouTube CG TG TCG TRG SG NG\n\nN 2, 708 4, 039 5, 242 317, 080 1, 134, 890 197 384 242 134 550 200\n\nM M/N 3.9834 21.846 5.7310 3.3110 5.2650 1.0964 2.5625 2.4545 0.9925 1.0636 1.8200\n\n10, 787 176, 437 30, 042 1, 049, 866 5, 975, 248 216 984 594 133 585 364\n\nD 21 8\n17 21 20 46 100 31 12 85 28\n\nRoBC 375.20 1, 306.9 148.43 1, 140.9 -\n0.6058 0.4987 0.4891 0.6416 0.3068 0.2914\n\nmBC 2.7174 0.8993 2.7219 2.0795 -\n0.1032 0.0928 0.0551 0.0630 0.0528 0.1215\n\nBFS 0.0074 s 0.0191 s 0.0148 s 46.0907 s 691.0986 s -\n- -\n- -\n-\n\n• Triangle Graph (TG): this is a graph possessing several cliques which are linearly connected\n\nmutually.\n\n• Tri-circle Graph (TCG): this is a graph that combines the properties of circle graphs and triangle graphs. Here, each circle is simulated by connecting triangle sub-graphs end to end.\n\n• Tree Graph (TRG): this is a graph that is generated from one root to several leaves recursively. There is no cycle in tree graphs. To control the tree structure, we define a splitting probability that is decayed exponentially with current depth.\n\n• Spiral Graph (SG): this is a graph shaped like a spiral line. We first simulate a line graph and add edges between nodes with exponentially increased distances by their indices on the line.\n\n• Net Graph (NG): this is a graph containing grid-like connections between nodes. We define\n\na small probability of dropping those edges stochastically.\n\nA.9 BASELINE & PARAMETER SETUP\n\nThe parameter setups of each baseline are listed as follows. For the oracle-based method, α0 is set to 2 for the best accuracy, as discussed in the previous work. For the landmark-based method, we choose a sufficient size of the landmark set as |L| = 128 and take the constrained strategy, i.e., for each landmark selected, nodes within two hops are discarded from consideration. For learningbased methods, the embedding size d is fixed at 16. In addition, the number of selected landmarks in learning-based methods is up to 80 for small graphs (i.e., Cora, Facebook, and GrQc) and 24 for large graphs (i.e., DBLP and Youtube). Other hyper-parameters of each model follow the default configurations discussed in their works. For the baselines proposed in road networks, the coordinaterelated features are omitted in their models, since there’s no coordinate assumption in our graph datasets. For general GRL methods, all of the baselines follow the default configurations and are further trained by linear regression to extract the distance between any two nodes.\n\nFor our proposed method, we simulate win = 20 walks on each node, and each walk is truncated at a length of lin = 40. Each landmark is selected randomly up to the size of a landmark set |L| = 80. The number of negative samples n is set to 1. The process of distance resampling outputs wout = 40 walks with each walk at a length of lout = 10. The decay coefficients of BC and distance are fixed as ζ = 10, α = 0.35. We train the distance predictor using a two-layer perceptron with a learning rate (cid:15)r = 0.01 for 15 epochs and train the CatBoost regressors with a grid search for their best parameters at the offline stage. For large graphs (i.e., DBLP and YouTube), we adjust the above parameters by |L| = 5, win = 2, ζ = 1.0. For BCDR-FC, the number of walks is reduced by a half for every graph. For BCDR-FQ, we take the raw outputs of the distance predictor without searching\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Parameter settings of evaluation on 5 real-world graphs. Smaller graphs include Cora, FaceBook, and GrQc. Larger Graphs include DBLP and YouTube.\n\nParameters d\n|L| win lin wout lout ζ\nα epochs\n\nfor Smaller Graphs 16 80 20 40 40 10 10 0.35 15\n\nfor Larger Graphs 16 5\n2 40 40 10 1\n0.35 15\n\nfirst-order neighborhoods on the graph (i.e., set τ = True). The boosting module is only utilized in BCDR and disabled in BCDR-FQ and BCDR-FC (i.e. set χ = False). We summarized the critical parameter setting to reproduce results in Table 2 as follows.\n\nA.10 EXTENDED COMPARISONS WITH GRL MODELS ON APPROXIMATION QUALITY\n\nWe present in Table 6 the experimental results of comparisons to general GRL models. Here, only the approximation quality (i.e., mAE and mRE) is evaluated, and the metrics are exerted directly on the representations without quantizing the outputs to integers or checking adjacency matrices. We see from Table 6 that although general embeddings by GRL methods could preserve some local SP structures, our proposed method with explicit SP constraint possesses better approximation quality for the SP distance queries.\n\nTable 6: Extended comparison to general GRL models on approximation quality\n\nModel\n\nLLE (Roweis & Saul, 2000) LE (Roweis & Saul, 2000) GF (Ahmed et al., 2013) DeepWalk (Perozzi et al., 2014) GraRep (Shaosheng et al., 2015) Node2Vec (Grover & Leskovec, 2016) NetMF (Qiu et al., 2018) VERSE (Tsitsulin et al., 2018) LPCA (Chanpuriya et al., 2020) BCDR (ours.)\n\nCora\n\nFacebook\n\nGrQc\n\nmAE 5.6265 5.6393 5.6249 1.5183 2.6206 1.3072 4.1736 2.8895 2.2813 0.9768\n\nmRE 0.8445 0.8455 0.8440 0.2425 0.3830 0.2115 0.6025 0.4049 0.3337 0.1605\n\nmAE 1.9921 2.0312 1.8743 0.9323 2.8702 0.8541 1.6982 1.1092 2.1373 0.4804\n\nmRE 0.6841 0.6998 0.6383 0.3289 1.0479 0.2993 0.6163 0.3729 0.8611 0.1770\n\nmAE 4.8849 5.0046 4.8562 2.8002 4.2445 1.5156 3.8799 3.3436 2.4526 1.0490\n\nmRE 0.7105 0.7366 0.7125 0.4169 0.6292 0.2278 0.5779 0.4689 0.3475 0.1684\n\nA.11 FURTHER INVESTIGATION ON BCDR FRAMEWORK\n\nA.11.1 ABLATION STUDY OF BCDR FRAMEWORK ON APPROXIMATION QUALITY\n\nWe discuss here the impact on approximation quality of different components in BCDR framework. In addition to those plausible post-processing operations described in Algorithm 1 (i.e., enable τ, χ or not), we also explore other operations that influnces approximation quality when pre-processing graphs. The modifications to BCDR are stated as follows, and the corresponding results evaluated on Facebook and DBLP are shown in Table 7.\n\n• no checks on adjacency. The outputs of BCDR are accepted as predictions of SP distance without checking if there is any immediate edge between each node pair (i.e., set τ = True).\n\n• no global features. SP distances are solely predicted by the two-layer neural network, and the boosting module based on global distances to landmarks is omitted (i.e., set χ = False).\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\n• no local features. SP distances are solely predicted by the boosting module, and the learn-\n\ning process of the two-layer neural network on local features is omitted.\n\n• no BC. Node representations are learned without BC-based random walk. For any nodes, each transition in simulating walks considers its first-order neighbors equally, ignoring their BC values.\n\n• no DR. Node representations are learned without distance resampling. The resampling rule\n\n(i.e. Equation 5) is replaced by a uniform sampling.\n\n• degree selection. In simulation of distance triplets and BC values, the landmarks are se-\n\nlected in descending order of degree, instead of random selection.\n\nWe see from the table that the full approach of BCDR achieves the best performance on approximation quality. It also reveals that all of the components significantly improve the prediction results. Specifically, for the checks on adjacency, it is notable that learning-based methods on SP representation show much difficulty in catching distance to first-order neighbors. Checking adjacency of input node pairs is necessary for accurate SP prediction. For global and local features, SP distance predicted from global features performs better than that from local features. It means leveraging global distances to each landmark helps a lot in locating a node on the graph. Furthermore, BCDR combines both global and local features for prediction and shows superior performance compared to either of them. For representing local features (i.e., BC and DR), we see that both BC-based random walk and distance resampling help to enhance the node representations with high-order SP structures, making it easier to extract distances to remote nodes. For landmark selection, we find that random selection of landmarks is more necessary for BCDR than other existing strategies. This is because we need to estimate BC values by performing BFS from these landmarks, and any assumption on landmark distribution will lead to unfair numerical estimation. If only landmarks with large degrees are selected, the BC value of nodes located in dense regions will be over-estimated, which impairs the efficiency of BC-based random walk.\n\nTable 7: Ablation study of BCDR framework on approximation quality\n\nModel\n\nBCDR - no checks on adjacency BCDR - no global features BCDR - no local features BCDR - no BC BCDR - no DR BCDR - degree selection BCDR - full approach\n\nFacebook\n\nDBLP\n\nmAE 0.0253 0.1138 0.0453 0.0210 0.0285 0.3820 0.0106\n\nmRE 0.0180 0.0378 0.0171 0.0086 0.0142 0.1139 0.0044\n\nmAE 0.5677 1.0070 0.5385 0.5437 0.5093 1.1524 0.4923\n\nmRE 0.0907 0.1484 0.0855 0.0839 0.0808 0.1611 0.0798\n\nA.11.2 FURTHER INVESTIGATION ON CRITICAL PARAMETER SETTING OF BCDR\n\nThen, we further investigate the parameter settings of BCDR and discuss 9 critical parameters for their impacts on performance. Notably, although we describe rather detailed settings of parameters in Appendix A.9, the proposed method BCDR is factually robust and effective, and its performance does not sensitively rely on any one of them. Here, we show the impacts of these parameters on related metrics and how to easily tune them in any unweighted graphs, both conceptually and practically. The next discussion and evaluation of each parameter follow its order in Table 5.\n\nthe dimension of node-level embeddings (i.e., Z).\n\nIn our experiment, d is not a fined: tuned parameter but fixed at a certain value (i.e., d = 16) among different models to fairly evaluate their performance. This parameter could improve the performance on accuracy since a large size of embeddings could dump more valuable information about SP structures at the expense of higher storage cost and deficiency in query speed. To verify this, We test BCDR with different d = {2, 4, 16, 64, 128, 256} on Facebook and GrQc to evaluate their performance under these metrics.\n\nFrom the Table 8 and 9, we see that the accuracy loss could be cut down by increasing d, but it will lead to significant deterioration in storage cost and query speed. As we discuss a low-dimensional\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nand accurate SP representation in this paper, the results also reveal that even at a rather lower dimension of embeddings (like d = 4), the distance relations on the graph could be well-preserved.\n\nTable 8: Impacts of d on the performance of BCDR evaluated on FaceBook\n\nd Storage mAE mRE Query Time\n\n4\n\n2\n\n256 0.1307 MB 0.1924 MB 1.210 MB 3.160 MB 5.944 MB 8.579 MB 0.0130 0.0064 81,188 ns\n\n0.0499 0.0212 22,430 ns\n\n0.0310 0.0122 41,142 ns\n\n0.0902 0.0347 4,089 ns\n\n0.0150 0.0075 4,664 ns\n\n0.0202 0.0091 8,334 ns\n\n128\n\n16\n\n64\n\nTable 9: Impacts of d on the performance of BCDR evaluated on GrQc\n\nd Storage mAE mRE Query Time\n\n2\n\n4\n\n64 0.2240 MB 0.3023 MB 0.7916 MB 2.750 MB 0.6867 0.1227 20,837 ns\n\n0.7089 0.1259 6, 570 ns\n\n0.8719 0.1548 2,421 ns\n\n0.8954 0.1555 3,011 ns\n\n16\n\n128\n\n256 5.415 MB 10.86 MB 0.677 0.1209 79,023 ns\n\n0.6746 0.1201 39, 479 ns\n\n|L|: the number of landmarks for constructing distance triplet and estimating BC. This parameter mainly affects accuracy and pre-processing time since involving more landmarks helps to alleviate harmful inductive bias on a certain part of the graph but suffers higher computing overhead. It is also observed in the previous works (Rizi et al., 2018) that for large graphs with strong centrality on a few nodes, the number of landmarks could be reduced without much loss of accuracy. We evaluate BCDR with a group of landmarks (|L| = {10, 20, 40, 80, 160}) on Facebook and GrQc, to see their impacts on the two metrics.\n\nTable 10: Impacts of |L| on the performance of BCDR evaluated on FaceBook\n\n|L| Pre-processing Time mAE mRE\n\n10 127.8 s 0.0342 0.0134\n\n20 134.3 s 0.0297 0.0108\n\n40 142.5 s 0.0148 0.0063\n\n80 157.5 s 0.0193 0.0096\n\n160 187.5 s 0.0124 0.0062\n\nTable 11: Impacts of |L| on the performance of BCDR evaluated on GrQc\n\n|L| Pre-processing Time mAE mRE\n\n10 47.47 s 0.9922 0.1591\n\n20 52.95 s 0.6837 0.1185\n\n40 64.35 s 0.7383 0.1266\n\n80 83.75 s 0.7112 0.1217\n\n160 123.7 s 0.7065 0.1231\n\nThe results in Table 10 and 11 show that the pre-processing time on graphs increases linearly with |L| since performing BFS from the added landmarks needs extra traversal on the whole graph for O(N + M ) time. It is also interesting to see that the number of landmarks large enough for the best performance diverges for dense and sparse graphs, i.e., it generally takes more than 40 landmarks for Facebook but only 20 landmarks necessary for GrQc. Specifically, for relatively dense graphs (i.e., Facebook), each node shares weaker centrality due to the enriched links, which means we need to observe more landmarks to cover more SPs on the graphs (according to the hub-labeling theory in (Cohen et al., 2003)). But for sparse graphs (i.e., GrQc), as long as several nodes with strong centrality are well-observed, SP distance between most node pairs could be preserved, resulting in tolerance of reduced landmarks.\n\nwin, lin: the number and length of sampled BC walks on each node. These parameters affect the accuracy and pre-processing time. When we simluate BC walks rooted at a certain node, a large win makes it sufficient to observe the local structure of each node (like BFS), while a large\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nlin allows wider exploration on the graph to let the distance with remote nodes be seen (like DFS). Like the previous evaluation, we test win = {5, 10, 20, 30, 40} and lin = {5, 10, 20, 40, 60, 80} to investigate their impacts, respectively.\n\nTable 12: Impacts of win on the performance of BCDR evaluated on FaceBook\n\nwin Pre-processing Time mAE mRE\n\n5 128.5 s 0.0136 0.0061\n\n10 143.7 s 0.0454 0.0182\n\n20 186.3 s 0.0113 0.0056\n\n30 228.2 s 0.0188 0.0093\n\n40 273.5 s 0.0062 0.0027\n\nTable 13: Impacts of lin on the performance of BCDR evaluated on FaceBook\n\nlin Pre-processing Time mAE mRE\n\n5 126.9 s 0.0133 0.0065\n\n10 131.6 s 0.0145 0.0067\n\n20 145.9 s 0.0725 0.0304\n\n40 182.8 s 0.0081 0.0037\n\n60 215.4 s 0.0320 0.0132\n\n80 114.0 s 0.0370 0.0184\n\nTable 14: Impacts of win on the performance of BCDR evaluated on GrQc\n\nwin Pre-processing Time mAE mRE\n\n5 112.2 s 0.7227 0.1250\n\n10 114.5 s 0.6671 0.1195\n\n20 118.8 s 0.6581 0.1166\n\n30 125.1 s 0.6930 0.1245\n\n40 130.2 s 0.7115 0.1259\n\nTable 15: Impacts of lin on the performance of BCDR evaluated on GrQc\n\nlin Pre-processing Time mAE mRE\n\n5 114.0 s 0.7460 0.1280\n\n10 115.7 s 0.6765 0.1146\n\n20 117.8 s 0.7074 0.1217\n\n40 119.3 s 0.6643 0.1171\n\n60 119.7 s 0.6895 0.1227\n\n80 120.8 s 0.6926 0.1230\n\nThe experimental results from Table 12 to 15 show the accuracy of BCDR is not sensitive to these parameters, owing much to the efficiency of BC walk and well-preserved distance relations by DR. Intuitively, we recommend setting lin proportional to the diameter of the graph, which makes the whole graph observed from any nodes. Also, win could be reduced when the connectivity on the graph is relatively weak since the local structures are quite simple to explore.\n\nwout, lout: the number and length of resampled paths (by DR) on each node. These parameters control the shape of output node sequences to subsequently optimize Z under a skip-gram procedure. To avoid much loss of information and preserve the correlation in BC walks, we intend to keep the scale of outputs similar to that of inputs, i.e., woutlout = Ω(winlin). To accelerate the optimization process, we could further shorten lout and keep this scale (by correspondingly expanding wout). Note that this reshaping operation does not apparently change the locality nor impair the performance since DR resamples nodes from high-order neighborhoods with respect to their distance from the root, thus resulting in well-defined convergence, as shown in Prop. 1. In the experiment, we fix the scale of output node sequences as half of the scale of BC walks (i.e., woutlout = winlin/2 = 400), and test different combinations of their settings as (wout, lin) = {(200, 2), (100, 4), (50, 8), (40, 10), (25, 16), (16, 25), (10, 40), (8, 50), (4, 100), (2, 200)}.\n\nThe results in Table 16 and 17 reveal that the pre-processing time dramatically increases along with lout. This is because we utilize the whole sequence to optimize co-occurrence likelihood between the root and nodes in this sequence, which requires joint training with a large number of node embeddings proportional to lout. It is also shown that the accuracy does not significantly fluctuate as pre-processing time, indicating a relatively small lout will help to reduce the off-line time cost.\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nTable 16: Impacts of (wout, lout) on the performance of BCDR evaluated on FaceBook\n\n(200,2)\n\n(wout, lout)\n\n(2,200) Pre-processing Time 176.3 s 154.1 s 148.6 s 150.6 s 156.8 s 170.5 s 206.3 s 236.8 s 501.0 s 1,521 s 0.0237 0.0217 0.0249 0.0258 0.0303 0.0128 0.0127 0.0360 0.0083 0.0237 0.0097 0.0093 0.0107 0.0108 0.0136 0.0059 0.0043 0.0145 0.0041 0.0098\n\nmAE mRE\n\n(25,16)\n\n(4,100)\n\n(10,40)\n\n(16,25)\n\n(40,10)\n\n(100,4)\n\n(8,50)\n\n(50,8)\n\nTable 17: Impacts of (wout, lout) on the performance of BCDR evaluated on GrQc\n\n(200,2)\n\n(wout, lout)\n\n(2,200) Pre-processing Time 73.00 s 75.65 s 77.78 s 80.03 s 88.09 s 106.2 s 151.1 s 191.4 s 512.2 s 1,709 s 0.7829 0.7107 0.6811 0.6780 0.7170 0.7129 0.6970 0.6751 0.7340 0.6986 0.1389 0.1225 0.1209 0.1225 0.1274 0.1204 0.1240 0.1212 0.1295 0.1263\n\nmAE mRE\n\n(25,16)\n\n(16,25)\n\n(4,100)\n\n(10,40)\n\n(40,10)\n\n(100,4)\n\n(8,50)\n\n(50,8)\n\nζ, α: the decay coefficient of BC values and distance weights. These parameters mainly affect the performance on accuracy by dominating the intrinsic behaviors of BC walk and DR, respectively. Thereinto, ζ determines how frequently a node could be enrolled in the current BC walk, which helps to diverge the direction of different walks from one root. Likewise, α determines how frequently a node with more hops from the root could be selected into resampled paths, which helps to distinguish neighbors of different orders. Like the previous evaluation, we test BCDR with ζ = {−1, 0, 1, 2, 4, 10, 20} and α = {0.1, 0.2, 0.3, 0.4, 0.5, 0.9, 0.98} to show their impacts.\n\nTable 18: Impacts of ζ and α on the performance of BCDR evaluated on FaceBook\n\nζ mAE mRE\n\nα mAE mRE\n\n-1 0.0522 0.0186\n\n0.1 0.0104 0.0046\n\n0 0.0146 0.0069\n\n0.2 0.0418 0.0204\n\n1 0.0061 0.0026\n\n0.3 0.0506 0.0178\n\n2 0.0243 0.0099\n\n0.4 0.0096 0.0046\n\n4 0.0137 0.0053\n\n0.5 0.0252 0.0125\n\n10 0.0143 0.0056\n\n0.9 0.0197 0.0096\n\n20 0.0131 0.0052\n\n0.98 0.0341 0.0159\n\nTable 19: Impacts of ζ and α on the performance of BCDR evaluated on GrQc\n\nζ mAE mRE\n\nα mAE mRE\n\n-1 0.6419 0.1146\n\n0.1 0.7216 0.1237\n\n0 0.6865 0.1234\n\n0.2 0.6780 0.1202\n\n1 0.6844 0.1209\n\n0.3 0.7036 0.1239\n\n2 0.6717 0.1214\n\n0.4 0.7441 0.1295\n\n4 0.7219 0.1235\n\n0.5 0.7006 0.1234\n\n10 0.6734 0.1175\n\n0.9 0.7281 0.1267\n\n20 0.6879 0.1209\n\n0.98 0.7258 0.1290\n\nFrom the Table 18 and 19, we see the accuracy of BCDR is not sensitive to these parameters, but a fine-tuning process could improve the performance on specific graphs.\n\nFor choices of ζ, it depends on the fluctuation of centrality on neighbor nodes. Specifically, for relatively dense graphs (like Facebook) with flattened centrality on neighbors, a larger ζ resists the frequency decaying of most preferred walk paths, leading to efficient exploration for high-order distance relations. On the contrary, a quick BC decaying (smaller ζ) makes the priority of neighbor nodes indistinguishable, dragging down the performance like a naive random walk, since many neighbors possess similar centrality on such graphs.\n\nFor choices of α, as discussed in Remark 2, it reflects a trade-off between quality (i.e., preserves accurate distance relations) and quantity (i.e., embeds more relations with a widened range of nodes). In detail, a smaller α slows down the process ˆDab → 0, allowing relations between node pairs with b → ˆDab > 0, but it causes nodes possessing similar larger distance Dab to converge, i.e., ZaZT distance from the root indistinguishable due to the noise in the embedding space, and vice versa.\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nNumber of epochs. The number of epochs determines if it is sufficient to learn a NN distance predictor. To produce the results of Table 2, we just leverage the empirical value as discussed in (Rizi et al., 2018). Here, we evaluate its impact on accuracy loss and pre-processing time.\n\nTable 20: Impacts of the number of epochs on the performance of BCDR evaluated on FaceBook\n\nNum. of epochs Pre-processing Time mAE mRE\n\n1 121.2 s 0.0174 0.0087\n\n2 125.3 s 0.0136 0.0067\n\n5 130.7 s 0.0167 0.0083\n\n10 139.7 s 0.0121 0.0057\n\n15 150.1 s 0.0107 0.0048\n\n20 160.0 s 0.0176 0.0070\n\n40 194.7 s 0.0259 0.0104\n\nTable 21: Impacts of the number of epochs on the performance of BCDR evaluated on GrQc\n\nNum. of epochs Pre-processing Time mAE mRE\n\n1 44.67 s 0.6888 0.1225\n\n2 47.92 s 0.6913 0.1257\n\n5 54.69 s 0.7071 0.1263\n\n10 68.03 s 0.6786 0.1158\n\n15 79.85 s 0.6803 0.1197\n\n20 92.09 s 0.6884 0.1194\n\n40 141.5 s 0.7047 0.1225\n\nThe results in Table 20 and 21 show that learning with 15 epochs is generally appropriate for many real-world graphs. It also reflects that training the distance predictor with more iterations may cause an over-fitting problem since the training data (distance triplets) are extracted from a few landmarks, which induces harmful inductive bias on a certain part of the graph.\n\nA.12 EXTENDED RESULTS ON EXPLORATION RANGE OF DISTANCE\n\nThe extended results on all synthetic graphs are shown in Figure 7. We analyze the significance of utilizing BC-RW for a wider range of exploration on different structures as follows.\n\n• For CGs and TCGs, BC-RW tends to choose the exit nodes of each circle since they provide a large BC gain by splitting all SPs between inner nodes and outer nodes regarding the current circle.\n\n• For TGs, transitions on every triangle clique tend to move forward along the trunk road since the number of nodes beyond the current clique is often larger than that of inner nodes, contributing to more SPs.\n\n• For TRGs, each transition from the root to leaves appears to be biased since subtrees with\n\nmore descendants contribute to more SPs and possess larger BC values.\n\n• For SGs, there are many shortcuts that link some nodes on the trunk road, and the BC values of shortcut nodes and other nodes are usually on par. BC-RW possesses a slight advantage by keeping a relatively good balance on these nodes.\n\n• For NGs, most of the nodes are passed through by SPs with similar probabilities, and BCRW is hard to tell the proper direction for deeper exploration like other walk strategies.\n\nA.13 EXTENDED RESULTS ON PRESERVATION OF DISTANCE RELATIONS\n\nThe extended results of distance preservation are shown in Figure 8 and 9. These figures confirm that our model is much more satisfactory in preserving distance relation than existing methods except for TGs. For most graphs, BC walk paths provide sufficient observation on each node by locating many remote nodes with a sequence of center nodes on a graph, and thus the resampling process based on such observation could preserve distance relations in the exploration range. For TGs, however, there are many final nodes (i.e., leaves) possessing trivial significance on BC walks which are insufficiently observed for calibrating their distance relations well.\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Exploration range of distance when taking different walk strategies tested on six synthetic graphs. Row from top to bottom: different synthetic graphs including CG, TG, TCG, TRG, SG, and NG, respectively. Column from left to right: different walk strategies including NRW, SORW, RS, DFS-RW, BC-RW (ours.), respectively.\n\n33\n\n051015202530Walk Length051015202530Exploration Range of Distance051015202530Walk Length051015202530Exploration Range of Distance051015202530Walk Length051015202530Exploration Range of Distance051015202530Walk Length051015202530Exploration Range of Distance051015202530Walk Length051015202530Exploration Range of Distance0102030405060Walk Length01020304050607080Exploration Range of Distance010203040506070Walk Length01020304050607080Exploration Range of Distance0102030405060Walk Length01020304050607080Exploration Range of Distance01020304050607080Walk Length01020304050607080Exploration Range of Distance01020304050607080Walk Length01020304050607080Exploration Range of Distance0.02.55.07.510.012.515.017.5Walk Length0.02.55.07.510.012.515.017.520.0Exploration Range of Distance0.02.55.07.510.012.515.017.5Walk Length0.02.55.07.510.012.515.017.520.0Exploration Range of Distance0.02.55.07.510.012.515.017.5Walk Length0.02.55.07.510.012.515.017.520.0Exploration Range of Distance0.02.55.07.510.012.515.017.5Walk Length0.02.55.07.510.012.515.017.520.0Exploration Range of Distance0.02.55.07.510.012.515.017.5Walk Length0.02.55.07.510.012.515.017.520.0Exploration Range of Distance01234567Walk Length012345678Exploration Range of Distance01234567Walk Length012345678Exploration Range of Distance0123456Walk Length012345678Exploration Range of Distance01234567Walk Length012345678Exploration Range of Distance01234567Walk Length012345678Exploration Range of Distance0102030405060Walk Length0102030405060Exploration Range of Distance0102030405060Walk Length0102030405060Exploration Range of Distance0102030405060Walk Length0102030405060Exploration Range of Distance0102030405060Walk Length0102030405060Exploration Range of Distance0102030405060Walk Length0102030405060Exploration Range of Distance0.02.55.07.510.012.515.017.5Walk Length0.02.55.07.510.012.515.017.520.0Exploration Range of Distance0.02.55.07.510.012.515.017.5Walk Length0.02.55.07.510.012.515.017.520.0Exploration Range of Distance0.02.55.07.510.012.515.017.5Walk Length0.02.55.07.510.012.515.017.520.0Exploration Range of Distance0.02.55.07.510.012.515.017.5Walk Length0.02.55.07.510.012.515.017.520.0Exploration Range of Distance0.02.55.07.510.012.515.017.5Walk Length0.02.55.07.510.012.515.017.520.0Exploration Range of DistanceUnder review as a conference paper at ICLR 2023\n\nFigure 8: Measured distance from the embedding space and the original graph. Row from top to bottom: different graphs including CG, TG, TCG, TRG, SG, and NG, respectively. Column from left to right: embeddings learned by different walk strategies,i.e., NRW, SORW, RS, DFS-RW, and BC-RW (ours.), respectively. For ours, walk paths are further simulated by distance resampling.\n\n34\n\n010203040SP Distance on the graph0246810Distance in the embedding space010203040SP Distance on the graph0246810Distance in the embedding space010203040SP Distance on the graph0246810Distance in the embedding space010203040SP Distance on the graph0246810Distance in the embedding space010203040SP Distance on the graph0246810Distance in the embedding space020406080100SP Distance on the graph0246810Distance in the embedding space020406080100SP Distance on the graph0246810Distance in the embedding space020406080100SP Distance on the graph0246810Distance in the embedding space020406080100SP Distance on the graph0246810Distance in the embedding space020406080100SP Distance on the graph0246810Distance in the embedding space051015202530SP Distance on the graph0246810Distance in the embedding space051015202530SP Distance on the graph0246810Distance in the embedding space051015202530SP Distance on the graph0246810Distance in the embedding space051015202530SP Distance on the graph0246810Distance in the embedding space051015202530SP Distance on the graph0246810Distance in the embedding space024681012SP Distance on the graph0246810Distance in the embedding space024681012SP Distance on the graph0246810Distance in the embedding space024681012SP Distance on the graph0246810Distance in the embedding space024681012SP Distance on the graph0246810Distance in the embedding space024681012SP Distance on the graph0246810Distance in the embedding space010203040506070SP Distance on the graph0246810Distance in the embedding space010203040506070SP Distance on the graph0246810Distance in the embedding space010203040506070SP Distance on the graph0246810Distance in the embedding space0102030405060SP Distance on the graph0246810Distance in the embedding space010203040506070SP Distance on the graph0246810Distance in the embedding space0510152025SP Distance on the graph0246810Distance in the embedding space0510152025SP Distance on the graph0246810Distance in the embedding space0510152025SP Distance on the graph0246810Distance in the embedding space0510152025SP Distance on the graph0246810Distance in the embedding space0510152025SP Distance on the graph0246810Distance in the embedding spaceUnder review as a conference paper at ICLR 2023\n\nFigure 9: Distance preservation among node triplets in the embedding spaces. Row from top to bottom: different graphs including CG, TG, TCG, TRG, SG, and NG, respectively. Column from left to right: embeddings learned by different walk strategies,i.e., NRW, SORW, RS, DFS-RW, and BC-RW (ours.), respectively. For ours, walk paths are further simulated by distance resampling.\n\n35\n\n0200040006000800010000sampled node triples-400-300-200-1000100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-400-300-200-1000100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-600-500-400-300-200-1000100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-300-200-1000100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-80-60-40-200value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-1500-1000-5000500value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-1250-1000-750-500-2500250500750value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-800-600-400-2000200400value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-1500-1000-500050010001500value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-400-300-200-1000value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-250-200-150-100-50050value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-400-300-200-1000100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-300-200-1000100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-300-200-1000100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-40-30-20-100value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-6-4-2024value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-6-4-2024value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-120-100-80-60-40-20020value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-8-6-4-2024value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-8-6-4-20246value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-1500-1000-5000500value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-1500-1250-1000-750-500-2500250500value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-800-600-400-2000200value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-1000-800-600-400-2000200400value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-150-125-100-75-50-25025value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-200-150-100-50050value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-250-200-150-100-50050value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-200-150-100-50050value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-200-150-100-50050value of distance expressionviolatedpreserved0200040006000800010000sampled node triples-80-60-40-20020value of distance expressionviolatedpreserved",
    "reference": "# Summary Of The Paper\n\nThe paper presents an algorithm to learn shortest-path (SP) representation of nodes in a graph. Existing strategies to learn SP are based on random-walks that, based on the structure of the graph, have limitations in terms of performance as well as distance preservation. The authors propose a method called Between Centrality-based Distance Resampling or BCDR that accomplishes two things. First, by using a centrality-based random walk approach, it ensures a better exploration of the possible paths and thus yield better distance preservation. Second, it uses a distance resampling strategy to improve the performance of the learning task. Authors provide theoretical proofs to support the algorithmic choices in the BCDR algorithm. A comparison with several existing methods on several benchmark data sets is provided to show that BCDR allows for improvements, both in terms of distance preservation accuracy and performance (speed).\n\n# Strength And Weaknesses\n\nStrengths:\n- The paper addresses an intellectually challenging and important problem\n- The proposed method is novel and is supported by theoretical arguments and empirical evaluation\n\nWeaknesses:\n- It is not clear if the implementation will be made public which could impact the reproducibility of the results\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written and the ideas are presented clearly. The organization is good. Based on the related work presented in the paper and on my limited literature search, I believe that the idea of using between centrality and distance resampling for the task of shortest path based representation learning is novel.\n\n# Summary Of The Review\n\nOverall, the paper seems to make significant contributions and presents and easy to understand paper that will be of interest to the conference audience and the community.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPROSAMPLER: IMPROVING CONTRASTIVE LEARNING BY BETTER MINI-BATCH SAMPLING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nIn-batch contrastive learning has emerged as a state-of-the-art self-supervised learning solution, with the philosophy of bringing semantically similar instances closer while pushing dissimilar instances apart within a mini-batch. However, the in-batch negative sharing strategy is limited by the batch size and falls short of prioritizing the informative negatives (i.e., hard negatives) globally. In this paper, we propose to sample mini-batches with hard negatives on a proximity graph in which the instances (nodes) are connected according to the similarity measurement. Sampling on the proximity graph can better exploit the hard negatives globally by bridging in similar instances from the entire dataset. The proposed method can flexibly explore the negatives by modulating two parameters, and we show that such flexibility is the key to better exploit hard negatives globally. We evaluate the proposed method on three representative contrastive learning algorithms, each of which corresponds to one modality: image, text, and graph. Besides, we also apply it to the variants of the InfoNCE objective to verify its generality. Results show that our method can consistently boost the performance of contrastive methods, with a relative improvement of 2.5% for SimCLR on ImageNet-100, 1.4% for SimCSE on the standard STS task, and 1.2% for GraphCL on the COLLAB dataset.\n\n1\n\nINTRODUCTION\n\nContrastive learning has been the dominant approach in current self-supervised representation learning, which is applied in many areas, such as MoCo (Chen et al., 2020) and SimCLR (He et al., 2020) in computer vision, GCC (Qiu et al., 2020) and GraphCL (You et al., 2020) in graph representation learning and SimCSE (Gao et al., 2021) in natural language processing. The basic idea is to decrease the distance between the embeddings of the same instances (positive pair) while increase that of the other instances (negative pair). These important works in contrastive learning generally follow or slightly modify the framework of in-batch contrastive learning as follows:\n\n\n\nminimize E\n\n{\n\nx1... xB\n\n\n\n−\n\n}⊂D\n\nB (cid:88)\n\ni=1\n\nlog\n\nef (xi)T f (x+ i ) i ) + (cid:80)\n\nef (xi)T f (x+\n\n=i ef (xi)T f (xj )\n\nj\n\n\n\n ,\n\n(1)\n\n{\n\nx1... xB}\n\nis a mini-batch of samples (usually) sequentially loaded from the dataset\n\n, and x+ where i\nis an augmented version of xi. The encoder f ( ) learns to discriminate instances by mapping different ·\ndata-augmentation versions of the same instance (positive pair) to similar embeddings, and mapping different instances in the mini-batch (negative pair) to dissimilar embeddings. The key to efficiency is in-batch negative sharing strategy that any instance within the mini-batch is the other instances’ 1) pairs of instances in a mini-batch while negative, which means we learn to discriminate all B(B encoding each instance only once. Its advantages of simplicity and efficiency make it more popular than pairwise (Mikolov et al., 2013) or triple-based methods (Schroff et al., 2015; Harwood et al., 2017), gradually becoming the dominating framework for contrastive learning.\n\n−\n\nD\n\nHowever, the performance of in-batch contrastive learning is closely related to batch size. It is the target of many important contrastive learning methods to obtain a large (equivalent) batch size under limited computation and memory budget. For example, Memory Bank (Wu et al., 2018) stores the encoded embeddings from previous mini-batches as extra negative samples, and MoCo (He et al., 2020) improves the consistency of the stored negative samples via a momentum encoder. SimCLR\n\n1\n\n̸ Under review as a conference paper at ICLR 2023\n\nFigure 1: A motivating example of ProSampler. The generated image representations form an embedding space where Uniform Sampler randomly samples a mini-batch with easy negatives and ProSampler samples a mini-batch with hard negatives based on proximity graph.\n\nshows that simply increasing the batch size of the plain in-batch contrastive learning to 8,192 outperforms previous carefully designed methods. Although many works highlight the importance of batch size, a further question arises — which instances in the mini-batch contribute the most?\n\nHard negative pair contributes the most — a clear answer to this question is well-supported by many efforts in some related studies on negative sampling (Ying et al., 2018; Yang et al., 2020; Huang et al., 2021; Kalantidis et al., 2020; Robinson et al., 2021). An intuitive explanation is that the ef (xi)T f (xj ) for easy-to-discriminate negative pairs will become very small after the early period of training, and thus the hard negative pairs contribute the majority of the loss and gradients.\n\nThe hard negative sampling already made great success in many real-world applications, e.g., 8% improvement of Facebook search recall (Huang et al., 2020) and 15% relative gains of Microsoft retrieval engine (Xiong et al., 2020). The key to these methods is to globally select negatives that are similar to the query one across the whole dataset. However, previous methods for negative sampling of in-batch contrastive learning (Robinson et al., 2021; Chuang et al., 2020) focus on identifying negative samples within the current mini-batch, which is insufficient to mine the meaningful negatives from the entire dataset. Meanwhile, previous global negative samplers apply triplet loss and explore the negatives in pairs (Karpukhin et al., 2020; Xiong et al., 2020), which is inapplicable to in-batch negative sharing strategy, since it cannot guarantee the similarity between every instance pair within a mini-batch.\n\nIn this paper, we focus on designing a global hard negative sampler for in-batch contrastive learning. Since every instance serves as the negative to the other instances in the same batch, the desired sampling strategy should be the one with more hard-to-distinguish pairs in each sampled batch. This objective can be considered as sampling a batch of similar instances from the dataset. But how can we identify such batch globally over the dataset?\n\nPresent Work. Here we propose Proximity Graph-based Sampler (ProSampler), a global hard negative sampling strategy that can be plugged into any in-batch contrastive learning method. Proximity graph breaks the independence between different instances and captures the relevance among instances to better perform global negative sampling. As shown in Figure 1, similar instances form a local neighborhood in the proximity graph where ProSampler performs negative sampling as short random walks to effectively draw hard negative pairs. Besides, ProSampler can flexibly control the hardness of the sampled mini-batch by modulating two parameters. In practice, we build the proximity graph per fixed iterations, and then apply Random Walk with Restart (RWR) per iteration to sample a mini-batch for training.\n\nOur experiments show that ProSampler consistently improves top-performing contrastive learning algorithms in different domains, including SimCLR (Chen et al., 2020) and MoCo v3 (Chen et al., 2021) in CV, SimCSE (Gao et al., 2021) in NLP, and GraphCL (You et al., 2020) in graph learning by merely changing the mini-batch sampling step. To the best of our knowledge, ProSampler is the first algorithm to optimize the mini-batch sampling step for better negative sampling in the current in-batch contrastive learning framework.\n\n2\n\nImage Representation SpaceProSamplerProSampler for Global Hard NegativesUniform Sampler for Easy NegativesPositiveGlobal Hard NegativesUniform SamplerEncoderProSamplerUniform SamplerPositiveEasy NegativesEncoderContrastive LossContrastive LossUnder review as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\nContrastive learning in different modalities. Contrastive learning follows a similar paradigm that contrasts similar and dissimilar observations based on noise contrastive estimation (NCE) (Gutmann and Hyvärinen, 2010; Oord et al., 2018). The primary distinction between contrastive methods of different modalities is how they augment the data. As for computer vision, MoCo (He et al., 2020), SimCLR (Chen et al., 2020), SwAV (Caron et al., 2020), and BYOL (Grill et al., 2020) augment data with geometric transformation and appearance transformation. Besides simply using data augmentation, WCL (Zheng et al., 2021) additionally utilizes an affinity graph to construct positive pairs for each example within the mini-batch. As for language, CLEAR (Wu et al., 2020b) and COCO-LM (Meng et al., 2021) augment the text data through word deletion, reordering, and substitution, while SimCSE (Gao et al., 2021) obtains the augmented instances by applying the standard dropout twice. As for graph, DGI (Petar et al., 2018) and InfoGraph (Sun et al., 2019) treat the node representations and corresponding graph representations as positive pairs. Besides, GCC (Qiu et al., 2020) and GraphCL (You et al., 2020) augment the graph data by graph sampling or proximity-oriented methods. Zhu et al. (2021) compares different kinds of graph augmentation strategy. Our proposed ProSampler is a general mini-batch sampler which can directly be applied to any in-batch contrastive learning framework with different modalities.\n\nNegative sampling in contrastive learning. Previous studies about negative sampling in contrastive learning roughly fall into two categories: (1) Memory-based negative sampling strategy, such as MoCo (He et al., 2020), maintains a fixed-size memory bank to store negatives which are updated regularly during the training process. MoCHI (Kalantidis et al., 2020) proposes to mix the hard negative candidates at the feature level to generate more challenging negative pairs. MoCoRing (Wu et al., 2020a) samples hard negatives from a defined conditional distribution which keeps a lower bound on the mutual information. (2) In-batch negative sharing strategy, such as SimCLR (Chen et al., 2020) and MoCo v3 (Chen et al., 2021), adopts different instances in the current mini-batch as negatives. To mitigate the false negative issue, DCL (Chuang et al., 2020) modifies the original InfoNCE objective to reweight the contrastive loss. Huynh et al. (2022) identifies the false negatives within a mini-batch by comparing the similarity between negatives and the anchor image’s multiple support views. Additionally, HCL (Robinson et al., 2021) revises the original InfoNCE objective by assigning higher weights for hard negatives among the mini-batch. However, such locally sampled hard negatives cannot exploit the hard negatives sufficiently from the dataset.\n\nGlobal hard negative sampling methods on triplet loss have been widely investigated, which aim to globally sample hard negatives for a given positive pair. For example, Wang et al. (2021) proposes to take rank-k hard negatives from some randomly sampled negatives. Xiong et al. (2020) globally samples hard negatives by an asynchronously-updated approximate nearest neighbor (ANN) index for dense text retrieval. Different from the abovementioned methods which are applied to a triplet loss for a given pair, our ProSampler samples mini-batches with hard negatives for InfoNCE loss.\n\nSelf-supervised learning without negative sampling. Recently, some attempts on learning without negative sampling achieve promising results, such as BYOL (Grill et al., 2020), SwAV (Caron et al., 2020), SimSiam (Chen and He, 2021), and DINO (Caron et al., 2021). These methods apply Siamese network structure, and contrast the output of an online network and a target network with different augmented views. The main difference between them is how they prevent model from collapsing.\n\n3 LEARNING WITH PROSAMPLER\n\n3.1 GLOBAL HARD NEGATIVE SAMPLING FOR IN-BATCH CONTRASTIVE LEARNING\n\nContrastive learning aims to learn a proper transformation that maps two semantically similar instances xi, xj to two close points in the embedding space. It applies NCE objective (Gutmann and Hyvärinen, 2010; Oord et al., 2018) and in-batch negative sharing strategy to boost the training efficiency, which means that every instance serves as a negative to the other instances within the mini-batch. How to sample a mini-batch with hard negatives for contrastive learning remains an open problem, and previous methods achieve this by sampling within the mini-batch (Chuang et al., 2020; Robinson et al., 2021; Karpukhin et al., 2020). However, the batch size is far smaller than the dataset\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nsize and sampling within the mini-batch cannot effectively explore the hard negatives from the whole dataset (Xiong et al., 2020; Zhang et al., 2013).\n\nIn this work, we delve deeper into learning with the global hard negative sampler, which picks a batch of instances containing considerable hard-to-distinguish pairs. Besides, a desired mini-batch sampling strategy should be general and adaptable to the datasets with various modalities and different scales. We formulate the problem as:\n\nProblem 1. Given a set of data instances independent sampler g(D) = instance pair are hard to distinguish across the dataset.\n\n, x(i+B)}\n\nxi,\n\n· · ·\n\n=\n\nD\n\n{\n\n, our goal is to design a modalityx1, {\nto sample a mini-batch of instances where any\n\n, xN }\n\n· · ·\n\n3.2 TWO EXTREME STRATEGIES: UNIFORM SAMPLER AND KNN SAMPLER\n\nHere we discuss two extreme mini-batch sampling strategies for in-batch contrastive learning: Uniform Sampler and kNN Sampler, which represent extreme scenarios in terms of the hardness of a mini-batch they construct.\n\nUniform Sampler is the most common strategy used in contrastive learning (Chen et al., 2020; Gao et al., 2021; You et al., 2020), which is general, easy to implement, and model-independent. The overall pipeline is to first randomly sample a batch of instances for each training step, then feed them into the objective function.\n\nkNN Sampler can globally sample a mini-batch with many hard negative examples. As its name indicates, kNN Sampler would pick an instance at random and retrieve a set of nearest neighbors to construct a batch. Figure 4 shows that the mini-batch sampled by kNN Sampler has a high percentage of similar instance pairs.\n\nHowever, the abovementioned two methods suffer from the following limitations:\n\n• Uniform Sampler neglects the effect of hard negatives (Kalantidis et al., 2020; Robinson et al., 2021), and will select negatives with low gradients that contribute little to optimization. As shown in Figure 4, Uniform Sampler results in a low percentage of similar instance pairs in a mini-batch. Yang et al. (2020) and Xiong et al. (2020) also theoretically prove that the suggested sampled negative should be similar to the query instance since it can provide a meaningful gradient to the model.\n\n• During the self-supervised training, the instances of the same class will cluster together in the embedding space (Chen et al., 2020; Caron et al., 2020). Hence the kNN Sampler can first retrieve the hard negatives but they will be replaced by false negatives (FN) as the training epochs increase. Figure 4 also demonstrates that kNN Sampler exhibits a very high percentage of FN in a mini-batch.\n\nIn conclusion, Uniform Sampler cannot leverage hard negatives to guide the optimization of the model; whereas kNN Sampler explicitly samples hard negatives but suffers from the false negative issue. Both of them will result in sub-optimal performance. A better global hard negative sampler for in-batch contrastive learning should trade-off these two sampling styles, and balance the exploitation of hard negatives and the FN issue. Building on the above observations, we propose ProSampler, a flexible global mini-batch sampler which allows us to smoothly interpolate between the kNN Sampler and the Uniform Sampler.\n\n3.3 PROSAMPLER\n\nAs discussed in Section 3.1, a desired mini-batch should be the one where any example is the hard negative of the other examples. This objective can also be seen as sampling a group of instances which are close to one another in the embedding space. But how to identify such groups globally from the dataset? As shown in Figure 2, we propose to capture similarity relationships among instances by proximity graph. Proximity graph connects the instances by the similarity measurement, and in this way, instances that appear to be close to each other form a local community in the graph. We perform the mini-batch sampling as a walk in the proximity graph, which collects the visited instances as sampling results. To modulate the hardness of a sampled batch, we introduce two parameters M and α to control the behaviors of proximity graph construction and sampling respectively.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The framework of ProSampler. The proximity graph is first constructed based on generated image representations and will be updated every t training steps. Next, a proximity graph-based negative sampler is applied to generate a batch with hard negatives for in-batch contrastive learning.\n\nProximity graph construction. Recall that in a training dataset, we have N observations 1, }\nencoder f ( ·\n\nand their corresponding representations ). We formulate the proximity graph as:\n\ni = generated by the current\n\ni = 1,\n\nei|\n\nvi|\n\n, N\n\n, N\n\n· · ·\n\n· · ·\n\n{\n\n{\n\n}\n\nG = (\n\n,\n\n),\n\nV\n\nE\n\n(2)\n\nCi =\n\nvm}\n\n{\n\n=\n\nwhere the node set a collection of node pairs. Let construct M (M\n\nNi, we first form a candidate set\n\nv1,\n\n· · ·\n\nV\n\n{\n\ndenotes the data examples and\n\n, vN } Ni be the neighbor set of instances vi in the proximity graph. To for each instance vi by uniformly picking\n\nvi, vj ∈ V}\n\n(vi, vj) |\n\nE ⊆ {\n\nis\n\n≪\n\nN ) neighbor candidates. Then we select the K nearest ones from the candidate set: (ei · is the inner product operation. M controls the similarity between the center node and its\n\nwhere immediate neighbor nodes, which can be demonstrated by the following proposition: Proposition 1. Given an observation vi with the corresponding representation ei, assume that there are at least S observations whose inner product similarity with vi is larger than s, i.e.,\n\nNi = TopK\n\nem) ,\n\n(3)\n\nvm\n\n∈C\n\n·\n\ni\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:8)vj ∈ V |\n\nei ·\n\nej > s(cid:9)(cid:12)\n\n(cid:12) (cid:12) ≥\n\nS.\n\n(4)\n\nThen in the proximity graph G, the similarity between vi and its neighbors is larger than s with proximate probability at least:\n\nP\n\nei · N , and K is the number of neighbors.\n\nvk ∈ Ni}\n\nek > s,\n\n∀\n\n{\n\n−\n\nS\n\nwhere p = N\n\n⪆\n\n(cid:16)\n\n1\n\n−\n\npM (cid:17)K\n\n,\n\n(5)\n\nThe proof is deferred to Appendix B. The insight of Proposition 1 is to relate candidate set size M to the similarity between a node pair. Higher M indicates a greater probability that two adjacent nodes are similar, and proximity graph will be more like the kNN graph. On the other hand, if M is low, some randomly chosen instances are more likely to be neighbors, improving the diversity of the negatives around the center node.\n\nProximity graph sampling. Breadth-first Sampling (BFS) and Depth-first Sampling (DFS) are two straightforward graph sampling methods (Grover and Leskovec, 2016), representing extreme scenarios in terms of the search space:\n\n• Breadth-first Sampling (BFS) collects all of the current node’s immediate neighbors, then moves to its neighbors and repeats the procedure until the number of collected instances reaches batch size.\n\n• Depth-first Sampling (DFS) randomly explores the node branch as far as possible before the number\n\nof the visited nodes reaches batch size.\n\nTo flexibly explore the negatives in proximity graph, we propose to apply Random Walk with Restart (RWR) which can exhibit a mixture of both. As shown in Algorithm 3, beginning at a node,\n\n5\n\nProximity Graph Construction......BatchProximity Graph Sampling1345789101126In-batch Contrastive Learning<latexit sha1_base64=\"f5w0av1p96bMiZvHCjrN4DH5FtQ=\">AAACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVkUxGUL9gG1SJJOa2hezEzEUvQH3Oq3iX+gf+GdcQpqEZ2Q5My595yZe6+fRaGQjvNasObmFxaXisulldW19Y3y5lZLpDkPWDNIo5R3fE+wKExYU4YyYp2MMy/2I9b2R2cq3r5lXIRpcinHGevF3jAJB2HgSaIad9flilN19LJngWtABWbV0/ILrtBHigA5YjAkkIQjeBD0dOHCQUZcDxPiOKFQxxnuUSJtTlmMMjxiR/Qd0q5r2IT2ylNodUCnRPRyUtrYI01KeZywOs3W8Vw7K/Y374n2VHcb0983XjGxEjfE/qWbZv5Xp2qRGOBE1xBSTZlmVHWBccl1V9TN7S9VSXLIiFO4T3FOONDKaZ9trRG6dtVbT8ffdKZi1T4wuTne1S1pwO7Pcc6C1kHVPaq6jcNK7dSMuogd7GKf5nmMGi5QR1N7P+IJz9a5FVnCyj9TrYLRbOPbsh4+AG0Uj4M=</latexit>xInput<latexit sha1_base64=\"9JPprEm6JUphUbgsW62YWavPmdc=\">AAACznicjVHLTsJAFD3UF+ILdemmkZi4Iq0x6pLoxiUmAiZASDsMOKG0zXRKJIS49Qfc6mcZ/0D/wjtjSVRidJq2Z849587ce/04EIlynNectbC4tLySXy2srW9sbhW3d+pJlErGaywKInnjewkPRMhrSqiA38SSe0M/4A1/cKHjjRGXiYjCazWOeXvo9UPRE8xTRDVbSgRdPrmbdkSnWHLKjln2PHAzUEK2qlHxBS10EYEhxRAcIRThAB4Seppw4SAmro0JcZKQMHGOKQrkTUnFSeERO6Bvn3bNjA1pr3Mmxs3olIBeSU4bB+SJSCcJ69NsE09NZs3+lnticuq7jenvZ7mGxCrcEvuXb6b8r0/XotDDmalBUE2xYXR1LMuSmq7om9tfqlKUISZO4y7FJWFmnLM+28aTmNp1bz0TfzNKzeo9y7Qp3vUtacDuz3HOg/pR2T0pu1fHpcp5Nuo89rCPQ5rnKSq4RBU10/FHPOHZqloja2rdf0qtXObZxbdlPXwA67mUFQ==</latexit> ̃xi<latexit sha1_base64=\"/joh2qOuBAO9P6W9jJZThj0x4bc=\">AAACznicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl047KCfUBbSpJO69i8SCbFUopbf8Ctfpb4B/oX3hmnoBbRCUnOnHvOnbn3urHPU2FZrzljYXFpeSW/Wlhb39jcKm7v1NMoSzxW8yI/SpqukzKfh6wmuPBZM06YE7g+a7jDCxlvjFiS8ii8FuOYdQJnEPI+9xxBVKstuN9jk7tp97ZbLFllSy1zHtgalKBXNSq+oI0eInjIEIAhhCDsw0FKTws2LMTEdTAhLiHEVZxhigJ5M1IxUjjEDuk7oF1LsyHtZc5UuT06xac3IaeJA/JEpEsIy9NMFc9UZsn+lnuicsq7jenv6lwBsQI3xP7lmyn/65O1CPRxpmrgVFOsGFmdp7Nkqivy5uaXqgRliImTuEfxhLCnnLM+m8qTqtplbx0Vf1NKycq9p7UZ3uUtacD2z3HOg/pR2T4p21fHpcq5HnUee9jHIc3zFBVcooqa6vgjnvBsVI2RMTXuP6VGTnt28W0ZDx/uGZQW</latexit> ̃xjViews<latexit sha1_base64=\"ymbPSV1QlrfslKD8yM5OZQH8q/g=\">AAACy3icjVHLSsNAFD3GV62vqks3wSLUTUlE1GXRjRuhgn1AWySZTuvQNBOSiVCrS3/Arf6X+Af6F94ZU1CL6IQkZ849587ce/0oEIlynNcZa3ZufmExt5RfXlldWy9sbNYTmcaM15gMZNz0vYQHIuQ1JVTAm1HMvaEf8IY/ONXxxg2PEyHDSzWKeGfo9UPRE8xTRDV7pTbrSrV3VSg6Zccsexq4GSgiW1VZeEEbXUgwpBiCI4QiHMBDQk8LLhxExHUwJi4mJEyc4x558qak4qTwiB3Qt0+7VsaGtNc5E+NmdEpAb0xOG7vkkaSLCevTbBNPTWbN/pZ7bHLqu43o72e5hsQqXBP7l2+i/K9P16LQw7GpQVBNkWF0dSzLkpqu6JvbX6pSlCEiTuMuxWPCzDgnfbaNJzG16956Jv5mlJrVe5ZpU7zrW9KA3Z/jnAb1/bJ7WHYvDoqVk2zUOWxjByWa5xEqOEMVNTPHRzzh2Tq3EuvWuvuUWjOZZwvflvXwAeNIkg4=</latexit>f(·)<latexit sha1_base64=\"ymbPSV1QlrfslKD8yM5OZQH8q/g=\">AAACy3icjVHLSsNAFD3GV62vqks3wSLUTUlE1GXRjRuhgn1AWySZTuvQNBOSiVCrS3/Arf6X+Af6F94ZU1CL6IQkZ849587ce/0oEIlynNcZa3ZufmExt5RfXlldWy9sbNYTmcaM15gMZNz0vYQHIuQ1JVTAm1HMvaEf8IY/ONXxxg2PEyHDSzWKeGfo9UPRE8xTRDV7pTbrSrV3VSg6Zccsexq4GSgiW1VZeEEbXUgwpBiCI4QiHMBDQk8LLhxExHUwJi4mJEyc4x558qak4qTwiB3Qt0+7VsaGtNc5E+NmdEpAb0xOG7vkkaSLCevTbBNPTWbN/pZ7bHLqu43o72e5hsQqXBP7l2+i/K9P16LQw7GpQVBNkWF0dSzLkpqu6JvbX6pSlCEiTuMuxWPCzDgnfbaNJzG16956Jv5mlJrVe5ZpU7zrW9KA3Z/jnAb1/bJ7WHYvDoqVk2zUOWxjByWa5xEqOEMVNTPHRzzh2Tq3EuvWuvuUWjOZZwvflvXwAeNIkg4=</latexit>f(·)Representations<latexit sha1_base64=\"0N80wFRysxEMXq224HhqaW4iEAc=\">AAACz3icjVHLSsNAFD3GV62vqks3wSK4KomIuiy6cdmCfUBbSpJO69C8mEzUUipu/QG3+lfiH+hfeGdMQS2iE5KcOfeeM3PvdWOfJ9KyXueM+YXFpeXcSn51bX1js7C1XU+iVHis5kV+JJqukzCfh6wmufRZMxbMCVyfNdzhuYo3rplIeBReylHMOoEzCHmfe44kqt2W7Fa6/TGbdHm3ULRKll7mLLAzUES2KlHhBW30EMFDigAMISRhHw4SelqwYSEmroMxcYIQ13GGCfKkTSmLUYZD7JC+A9q1MjakvfJMtNqjU3x6BSlN7JMmojxBWJ1m6niqnRX7m/dYe6q7jejvZl4BsRJXxP6lm2b+V6dqkejjVNfAqaZYM6o6L3NJdVfUzc0vVUlyiIlTuEdxQdjTymmfTa1JdO2qt46Ov+lMxaq9l+WmeFe3pAHbP8c5C+qHJfu4ZFePiuWzbNQ57GIPBzTPE5RxgQpq5B3jEU94NqrGjXFn3H+mGnOZZgfflvHwAQ14lIc=</latexit>ei<latexit sha1_base64=\"kJFn3Ym7nk6/yqLL+uSXkWHFdzI=\">AAACz3icjVHLTsJAFD3UF+ILdemmkZi4Iq0x6pLoxiUk8kiAkHYYsFLapp2qhGDc+gNu9a+Mf6B/4Z1xSFRidJq2Z86958zce93I9xJhWa8ZY25+YXEpu5xbWV1b38hvbtWSMI0Zr7LQD+OG6yTc9wJeFZ7weSOKuTN0fV53B2cyXr/mceKFwYUYRbw9dPqB1/OYI4hqtQS/FW5vzCedq06+YBUttcxZYGtQgF7lMP+CFroIwZBiCI4AgrAPBwk9TdiwEBHXxpi4mJCn4hwT5EibUhanDIfYAX37tGtqNqC99EyUmtEpPr0xKU3skSakvJiwPM1U8VQ5S/Y377HylHcb0d/VXkNiBS6J/Us3zfyvTtYi0MOJqsGjmiLFyOqYdklVV+TNzS9VCXKIiJO4S/GYMFPKaZ9NpUlU7bK3joq/qUzJyj3TuSne5S1pwPbPcc6C2kHRPiralcNC6VSPOosd7GKf5nmMEs5RRpW8IzziCc9Gxbgx7oz7z1QjozXb+LaMhw8P2JSI</latexit>ejContrastiveLoss<latexit sha1_base64=\"YDw2Kt7qSvKmPItHJTYRtGStLAk=\">AAAC5HicjVHLSsNAFD3G97vq0oXBItRNSUTUZdGNS4W2Fmwpk+m0HcyLZCKU0qU7d+LWH3Cr3yL+gf6Fd8YIPhCdkOTMufecmXuvF/syVY7zPGaNT0xOTc/Mzs0vLC4tF1ZW62mUJVzUeORHScNjqfBlKGpKKl804kSwwPPFmXdxpONnlyJJZRRW1SAWrYD1QtmVnCmi2oWNbnvIst6o1OSdSG3bzVQGdjNgqs+ZP6yO2oWiU3bMsn8CNwdF5OskKjyhiQ4icGQIIBBCEfbBkNJzDhcOYuJaGBKXEJImLjDCHGkzyhKUwYi9oG+Pduc5G9Jee6ZGzekUn96ElDa2SBNRXkJYn2abeGacNfub99B46rsN6O/lXgGxCn1i/9J9ZP5Xp2tR6OLA1CCpptgwujqeu2SmK/rm9qeqFDnExGncoXhCmBvlR59to0lN7bq3zMRfTKZm9Z7nuRle9S1pwO73cf4E9Z2yu1d2T3eLlcN81DNYxyZKNM99VHCME9TI+wr3eMCj1bWurRvr9j3VGss1a/iyrLs32VCcIA==</latexit>faug(·)⇠T<latexit sha1_base64=\"sfyc+ko3Zt11xf1Wzo9t724D2co=\">AAAC5XicjVHLSsNAFD3GV31XXboJFrFuSiKiLotuXFZoa8FKmUyndTAvkolQSrfu3Ilbf8Ct/or4B/oX3hkj+EB0QpIz595zZu69XuzLVDnO85g1PjE5NV2YmZ2bX1hcKi6vNNMoS7ho8MiPkpbHUuHLUDSUVL5oxYlggeeLE+/iUMdPLkWSyiisq0EszgLWD2VPcqaI6hTt3mZnyLL+qNzm3Uht2e1UBnY7YOqcM39YH3WKJafimGX/BG4OSshXLSo+oY0uInBkCCAQQhH2wZDScwoXDmLizjAkLiEkTVxghFnSZpQlKIMRe0HfPu1OczakvfZMjZrTKT69CSltbJAmoryEsD7NNvHMOGv2N++h8dR3G9Dfy70CYhXOif1L95H5X52uRaGHfVODpJpiw+jqeO6Sma7om9ufqlLkEBOncZfiCWFulB99to0mNbXr3jITfzGZmtV7nudmeNW3pAG738f5EzS3K+5uxT3eKVUP8lEXsIZ1lGmee6jiCDU0yPsK93jAo9W3rq0b6/Y91RrLNav4sqy7N2ESnFE=</latexit>f0aug(·)⇠TUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: In-batch Contrastive Framework with ProSampler\n\nInput: Dataset\n\ni = 1, Modality-specific augmentation functions\n\n, Encoder f ( ·\n.\n\nxi|\n\n, N\n\n· · ·\n\n=\n\nD\n\n{\n\n}\n\nT\n\n0, 1,\n\ndo\n\n· · ·\n\n← // ProSampler if iter%t == 0 then\n\n), Batchsize B, Graph update step t,\n\n// Proximity Graph Construction Build the proximity graph G by Algorithm 2.\n\nend // Proximity Graph Sampling Randomly select a start node and get the mini-batch (xi, x+ Obtain positive pairs }B by augmentation functions faug( i ) {\n(ei, e+ Generate representations i ) {\nCompute the loss by Eq. 1, where Update the parameters of f ( ·\n\n}B by Encoder f ( (ei, ej)\n\n=j i\nB(B }\n\n). ·\n\n).\n\n{\n\n{\n\n−\n\nxi}B by Algorithm 3.\n\n) ·\n\n∼ T\n\n.\n\n1) are treated as negative pairs.\n\nfor iter\n\nend\n\nthe sampler iteratively teleports back to the start point with probability α or travels to a neighbor of the current position with the probability proportional to the edge weight. The process continues until it collects a fixed number of vertices which will be taken as the sampled batch.\n\nThe key insight of using RWR is that it can modulate the probability of sampling within a neighborhood by adjusting α, which can be demonstrated by the following proposition:\n\nProposition 2. For all 0 < α starting from a node u stationary distribution, and Φ(\n\n∈ S\n\n1 and ≤\nescapes\n\n, the probability that a Lazy Random Walk with Restart 1\n), where pu is the\n\nα\n\n2α Φ(\n\n−\n\nsatisfies (cid:80) ) is the graph conductance of\n\nS ⊂ V S\n\nV−S\n\n) pu(v) .\n\n∈\n\nv\n\n(\n\n≤\n\nS\n\nS\n\nS\n\nThe proof is deferred to Appendix B. Proposition 2 indicates that the probability of RWR escaping from a local cluster (Andersen et al., 2006; Spielman and Teng, 2013) can be bounded by the graph conductance (Šíma and Schaeffer., 2006) and the restart probability α. In other words, higher α indicates that the walker will approximate BFS behavior and sample within a small locality. Lower α encourages the walker to visit the nodes which are further away from the center node.\n\nProSampler pipeline. As shown in Algorithm 1, ProSampler serves as a mini-batch sampler and can be easily plugged into any in-batch contrastive learning method, such as SimCLR (Chen et al., 2020), MoCo v3 (Chen et al., 2021), SimCSE (Gao et al., 2021) and GraphCL (You et al., 2020). Specifically, during the training process, ProSampler first constructs the proximity graph, which will be updated after t training steps, then selects a start node at random and samples a mini-batch on proximity graph by RWR. ProSampler is orthogonal to the contrastive methods.\n\nAs shown in Figure 3, the number of candidates M and the restart probability α are the key to flexibly control the hardness of a sampled batch. When we set M as the size of dataset and α as 1, proximity graph is equivalent to kNN graph and graph sampler will only collect the immediate neighbors around a center node, which behaves similarly to a kNN Sampler. On the other hand, if M is set to 1 and α is set to 0, the RWR degenerates into the DFS and chooses the neighbors that are linked at random, which indicates that ProSampler performs as a Uniform Sampler. We provide an empirical criterion of choosing M and α in Section 4.3.\n\nComplexity. The time complexity of building a proximity graph is O(N M d) where N is the dataset size, M is the candidate set size and d denotes the embedding size. It is practically efficient since usually M is much smaller than N , and the process can be accelerated by embedding retrieval libraries such as Faiss (Johnson et al., 2019). More analysis on efficiency can be found in Appendix F.5. Besides, the space cost of ProSampler mainly comes from graph construction and graph storage. The total space complexity of ProSampler is O(N d + N K) where K is the number of neighbors in the proximity graph.\n\n6\n\n̸ Under review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\nTo show the effectiveness of ProSampler in a variety of scenarios, we apply it to the representative contrastive learning algorithms on three data modalities, including image, text, and graph. Furthermore, to investigate ProSampler’s generality, we equip two variants of InfoNCE objective with our model, including DCL (Chuang et al., 2020) and HCL (Robinson et al., 2021). InfoNCE objective and its variants are described in Appendix C. The statistics of the datasets are summarized in Appendix D, and the detailed experimental setting can be found in Appendix E.\n\n4.1 BENCHMARKING RESULTS\n\nTable 1: Top-1 accuracy under the linear evaluation with the ResNet-50 backbone on ImageNet.\n\nResults on Image Modality. We first adopt SimCLR (Chen et al., 2020) and MoCo v3 (Chen et al., 2021) as the backbone based on ResNet-50 (He et al., 2016). We start with training the model for 800 epochs with a batch size of 2048 for SimCLR and 4096 for MoCo v3, respectively. We then use linear probing to evaluate the representations on ImageNet. We also compare the ProSampler with the two state-of-the-art self-supervised learning methods without negative sampling, including SwAV (Caron et al., 2020) and BYOL (Grill et al., 2020). As shown in Table 1, our proposed model can consistently boost the performance of original SimCLR and MoCo v3, and outperforms all the baselines without negatives, demonstrating the superiority of ProSampler. Besides, we evaluate ProSampler on the other benchmark datasets, which can be found in Appendix F.1.\n\nSwAV* BYOL\n\nMethod\n\n66.5 66.5\n\n100 ep\n\nSimCLR w/ ProSampler 64.7 (↑ 0.7) 69.2 (↑ 0.5)\n\n64.0\n\n68.7\n\nMoCo v3 w/ ProSampler 69.5 (↑ 0.6) 74.2 (↑ 0.4)\n\n68.9\n\n73.8\n\n* without multi-crop augmentations.\n\n800 ep\n\n71.8 74.3\n\n∼\n\nResults on Text Modality. We evaluate ProSampler on learning the sentence representations by SimCSE (Gao et al., 2021) framework with pretrained BERT (Devlin et al., 2018) as backbone. The results of Table 2 suggest that ProSampler consistently improves the baseline models with an absolute 2.91% on 7 semantic textual similarity (STS) tasks (Agirre et al., 2012; 2013; 2014; gain of 1.09% 2015; 2016; Tian et al., 2017; Marelli et al., 2014). Specifically, we observe that when applying DCL and HCL, the performance of the self-supervised language model averagely drops by 2.45% and 3.08% respectively. As shown in Zhou et al. (2022) and Appendix F.2, the pretrained language model offers a prior distribution over the sentences, leading to a high cosine similarity of both positive pairs and negative pairs. So DCL and HCL, which leverage the similarity of positive and negative scores to tune the weight of negatives, are inapplicable because the high similarity scores of positives and negatives will result in homogeneous weighting. However, the hard negatives explicitly sampled by our proposed ProSampler can alleviate it, with an absolute improvement of 1.64% on DCL and 2.64% on HCL. The results of RoBERTa (Liu et al., 2019) are reported in Appendix F.3.\n\nTable 2: Overall performance comparison with different negative sampling methods on STS tasks.\n\nMethod\n\nSTS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg.\n\nSimCSE-BERTbase w/ ProSampler\n\nDCL-BERTbase w/ ProSampler\n\nHCL-BERTbase w/ ProSampler\n\n68.62 72.37\n\n65.22 69.55\n\n62.57 66.87\n\n80.89 82.08\n\n77.89 82.66\n\n79.12 81.38\n\n73.74 75.24\n\n68.94 73.37\n\n69.70 72.96\n\n80.88 83.10\n\n79.88 80.40\n\n78.00 80.11\n\n77.66 78.43\n\n76.72 75.37\n\n75.11 77.99\n\n77.79 77.54\n\n73.89 75.43\n\n73.38 75.95\n\n69.64 68.05\n\n69.54 66.76\n\n69.74 70.89\n\n75.60 76.69\n\n73.15 74.79\n\n72.52 75.16\n\nResults on Graph Modality. We test ProSampler with graph-level classification task based on GraphCL (You et al., 2020) framework, which uses GIN (Xu et al., 2018) as the backbone. Table 3 reports the detailed performance comparison on four benchmark datasets: IMDB-B, IMDB-M, COLLAB, and REDDIT-B (Yanardag and Vishwanathan, 2015). The result shows that ProSampler can consistently boost the performance of GraphCL, with an average absolute improvement of\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n0.89% across all the datasets. Besides, equipped with ProSampler, DCL and HCL can achieve better performance in 6 out of 8 cases. It can also be found that ProSampler can reduce variance in most cases, demonstrating that the hard negatives exploited by ProSampler can enforce the model to learn more robust representations.\n\nTable 3: Accuracy on graph classification task under LIBSVM (Chang and Lin, 2011) classifier.\n\nMethod\n\nIMDB-B\n\nIMDB-M COLLAB REDDIT-B\n\n70.90 GraphCL w/ ProSampler 71.90\n\nDCL 71.07 w/ ProSampler 71.32\n\n71.24 HCL w/ ProSampler 71.20\n\n± ±\n\n± ±\n\n± ±\n\n0.53 48.48 0.46 48.93\n\n0.38 70.62 0.28 71.48\n\n0.23 90.54 0.28 90.88\n\n0.36 48.93 0.17 48.96\n\n0.32 71.06 0.25 70.44\n\n0.51 90.66 0.35 90.73\n\n0.36 48.54 0.38 48.76\n\n0.51 71.03 0.39 71.70\n\n0.45 90.40 0.35 91.25\n\n± ±\n\n± ±\n\n± ±\n\n± ±\n\n± ±\n\n± ±\n\n0.25 0.16\n\n0.29 0.34\n\n0.42 0.25\n\n± ±\n\n± ±\n\n± ±\n\n4.2 WHY PROSAMPLER PERFORMS BETTER?\n\nIn this section, we apply SimCLR on CIFAR10 and CIFAR100, and compare the Uniform Sampler, kNN Sampler and ProSampler in terms of performance and the false negatives to deepen the understanding of ProSampler. We show the performance of ProSampler with different M and α settings in Figure 3, and use the name convention ProSampler (M, α). Besides, we illustrate the histogram of cosine similarity for all pairs from a sampled batch, and the percentage of false negatives within the mini-batch during training in Figure 4. We can observe that although kNN Sampler can explicitly draw a data batch with similar pairs, it introduces a substantially higher number of false negatives, degrading performance significantly. Uniform Sampler is independent of the model so the percentage of FN within the sampled batch remains consistent during training. However, ProSampler can modulate M, α to find the best balance between these two sampling methods. We can observe that ProSampler can sample hard mini-batch but only exhibits a slightly higher percentage of false negatives than Uniform Sampler with optimal parameter setting, which enables ProSampler to achieve the best performance. Similar phenomenon on CIFAR100 can be found in Appendix F.4.\n\nFigure 3: Performance comparison of different in-batch samplers on image classification task.\n\nFigure 4: Cosine similarity and false negative ratio on CIFAR10 dataset.\n\n4.3 EMPIRICAL CRITERION FOR PROSAMPLER\n\nTo analyze the impact of the size of neighbor candidate M and the random walk restart probability α, we vary the M and α in range of respectively, and apply SimCLR, SimCSE and GraphCL as backone. We summarize the results in Table 4 and Table 5. Table 4 shows that in most cases, the performance of the model peaks when M = 1000 but plumbs quickly with the increasing of M . Such phenomena are consistent with the intuition that higher M raises the probability of selecting similar instances as neighbors, but the sampler will be more likely to draw the mini-batch with false negatives, degrading the performance.\n\n500, 1000, 2000, 4000, 6000\n\n0.1, 0.3, 0.5, 0.7\n\nand\n\n{\n\n}\n\n}\n\n{\n\nTable 5 shows the performance of ProSampler with different α. Besides, to better understand the effect of α, we illustrate the histograms of cosine similarity for all pairs from a sampled batch after training in Figure 5, and plot the percentage of false negatives in the mini-batch during training in Figure 6. We can observe that α moving from 0.1 through 0.2 to 0.7 causes cosine similarities to gradually skew left, but introduces more false negatives in the batch, creating a trade-off. This phenomenon indicates\n\n8\n\nUniform SamplerProSampler(500,0.2~0.05)ProSampler(1000,0.5)ProSampler(6000,0.1)kNN SamplerUniform SamplerProSampler(1000,0.2~0.05)ProSampler(1000,0.5)ProSampler(6000,0.1)kNN Sampler0.00.20.40.60.81.0Cosine Similarity0.00.51.01.52.02.53.03.54.0FrequencykNN SamplerProSamplerUniform Sampler050100150200Training Step(x1000)0.100.120.140.160.180.200.220.24FN ratiokNN SamplerProSamplerUniform SamplerUnder review as a conference paper at ICLR 2023\n\nTable 4: Impact of neighbor candidates M .\n\nTable 5: Impact of restart probability α.\n\nM\n\n500\n\n1000 2000 4000 6000\n\nα\n\n0.1\n\n0.3\n\n0.5\n\n0.7\n\n0.2∼0.05\n\nCIFAR10 CIFAR100 STL10\n\n92.54 92.49 91.83 91.72 91.43 67.92 68.68 67.05 66.19 65.55 84.16 84.38 82.80 81.91 80.92 58.4 60.1\n\n60.8\n\n59.1\n\nCIFAR10 CIFAR100 STL10\n\n92.41 92.26 92.12 92.06 68.31 67.98 68.20 68.00 83.01 80.69 83.93 82.56 57.7\n\n59.6\n\n58.1\n\n92.54 68.68 84.38 60.8\n\nImageNet-100 60.8\n\nImageNet-100 59.6\n\nWikipedia\n\n71.36 76.69 76.09 75.76 75.11\n\nWikipedia\n\n71.74 72.13 72.41 76.69\n\n–\n\nCOLLAB\n\n70.47 71.48 70.93 70.46 70.24\n\nCOLLAB\n\n70.36 70.63 70.63 70.31\n\n71.48\n\nthat the sampler with a higher α sample more frequently within a local neighborhood, which is more likely to yield similar pairs. However, as training progresses, the instances of the same class tend to group together, increasing the probability of collecting false negatives. To find the best balance, we linearly decay α from 0.2 to 0.05 as the training epoch increases, which is presented as 0.2 0.05 in Table 5. It can be found that this dynamic strategy achieves the best performance in all cases except SimCSE which only trains for one epoch. Interestingly, SimCSE achieves the best performance by a large margin when α = 0.7 since hard negatives can alleviate the distribution issue brought by the pre-trained language model. More analysis can be found in Section 4.1 and Appendix F.2.\n\n∼\n\nTo sum up, the suggested M would be 500 for the small-scale dataset, and 1000 for the larger dataset. The suggested α should be relatively high, e.g., 0.7, for the pre-trained language model-based method. Besides, dynamic decay α, e.g., 0.2 to 0.05, is the best strategy for the other methods.\n\n(a) CIFAR10\n\n(b) CIFAR100\n\n(a) CIFAR10\n\n(b) CIFAR100\n\nFigure 5: Cosine similarities between the pairs.\n\nFigure 6: Percentage of false negatives.\n\n4.4 DISCUSSIONS\n\nDue to the page limit, some additional experiments are reported in Appendix F. Appendix F.5 studies the efficiency of ProSampler. Appendix F.6 compares different graph sampling methods in terms of performance, cosine similarity, and false negatives. Appendix F.7 compares the performance of ProSampler with proximity graph and kNN graph. Appendix F.8 discusses the influence of some parameters, including batchsize B, neighbor number K, and proximity graph update interval t. Appendix F.9 presents the training curves. Appendix F.10 includes case studies where we show some real cases of the mini-batch sampled by ProSampler and Uniform Sampler.\n\n5 CONCLUSION\n\nIn this paper, we study the problem of global hard negative sampling for in-batch contrastive learning. We reformulate the original mini-batch sampling problem to the proximity graph sampling problem. Based on this, we propose a proximity graph-based sampling framework, ProSampler, which can sample a mini-batch with hard negative pairs for in-batch contrastive learning at each training step. Besides, we conduct experiments on three state-of-the-art contrastive methods with different modalities and two variants of InfoNCE objective to evaluate our proposed ProSampler, which shows that ProSampler can consistently improve the these models.\n\n9\n\n0.00.20.40.60.8Cosine Similarity0.00.51.01.52.02.53.03.54.0Frequency=0.1=0.3=0.5=0.70.00.20.40.60.81.0Cosine Similarity0.00.51.01.52.02.53.03.54.0Frequency=0.1=0.3=0.5=0.7050100150200Training Step(x1000)0.110.120.130.140.150.160.170.18FN ratio=0.1=0.3=0.5=0.7050100150200Training Step(x1000)0.0110.0120.0130.0140.0150.0160.0170.018FN ratio=0.1=0.3=0.5=0.7Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First Joint Conference on Lexical and Computational Semantics– Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385–393, 2012.\n\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. * sem 2013 shared task: Semantic textual similarity. In Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main conference and the shared task: semantic textual similarity, pages 32–43, 2013.\n\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel M Cer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. Semeval-2014 task 10: Multilingual semantic textual similarity. In SemEval@ COLING, pages 81–91, 2014.\n\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, et al. Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability. In Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pages 252–263, 2015.\n\nEneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez Agirre, Rada Mihalcea, German Rigau Claramunt, and Janyce Wiebe. Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In SemEval-2016. 10th International Workshop on Semantic Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p. 497-511. ACL (Association for Computational Linguistics), 2016.\n\nReid Andersen, Fan Chung, and Kevin Lang. Local graph partitioning using pagerank vectors. 2006.\n\nKonstantin Avrachenkov, Remco van der Hofstad, and Marina Sokol. Personalized pagerank with node-dependent\n\nrestart. In International Workshop on Algorithms and Models for the Web-Graph, pages 23–33, 2014.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Advances in Neural Information Processing Systems, pages 9912–9924, 2020.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9650–9660, 2021.\n\nChih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector. ACM transactions on intelligent\n\nsystems and technology, pages 1–27, 2011.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive\n\nlearning of visual representations. arXiv preprint arXiv:2002.05709, 2020.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning.\n\nIn Proceedings of the\n\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15750–15758, 2021.\n\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9640–9649, 2021.\n\nChing-Yao Chuang, Joshua Robinson, Lin Yen-Chen Antonio Torralba, and Stefanie Jegelka. Debiased\n\ncontrastive learning. In Advances in neural information processing systems, 2020.\n\nFan Chung and Alexander Tsiatas. Finding and visualizing graph clusters using pagerank optimization. In\n\nInternational Workshop on Algorithms and Models for the Web-Graph, pages 86–97, 2010.\n\nFan Chung and Wenbo Zhao. Pagerank and random walks on graphs. In Fete of combinatorics and computer\n\nscience, pages 43–62, 2010.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\n\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings.\n\narXiv preprint arXiv:2104.08821, 2021.\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, and Carl Doersch et al. Bootstrap your own latent-a new approach to self-supervised learning. In Advances in Neural Information Processing Systems, pages 21271–21284, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855–864, 2016.\n\nMichael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of i=International Conference on Artificial Intelligence and Statistics, pages 297–304, 2010.\n\nBen Harwood, Vijay Kumar BG, Gustavo Carneiro, Ian Reid, and Tom Drummond. Smart mining for deep metric learning. In Proceedings of the IEEE International Conference on Computer Vision, pages 2821–2829, 2017.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. Proceedings of the IEEE conference on computer vision and pattern recognition, 2020.\n\nJui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. Embedding-based retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2553–2561, 2020.\n\nTinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, and Jie Tang. MixGCF: An improved training method for graph neural network-based recommender systems. In Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 665–674, 2021.\n\nTri Huynh, Simon Kornblith, Matthew R Walter, Michael Maire, and Maryam Khademi. Boosting contrastive self-supervised learning with false negative cancellation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2785–2795, 2022.\n\nJeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions\n\non Big Data, 7(3):535–547, 2019.\n\nYannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative\n\nmixing for contrastive learning. In Advances in neural information processing systems, 2020.\n\nVladimir Karpukhin, Barlas O ̆guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\n\nDiederik P. Kingma and Jimmy Lei Ba. Colorful image colorization. In International Conference on Learning\n\nRepresentations, 2015.\n\nDonald Ervin Knuth. The art of computer programming: Fundamental Algorithms, volume 1. Pearson Education,\n\n1997.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. A sick cure for the evaluation of compositional distributional semantic models. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 216–223, 2014.\n\nYu Meng, Chenyan Xiong, Payal Bajaj, Paul Bennett, Jiawei Han, and Xia Song. Coco-lm: Correcting and contrasting text sequences for language model pretraining. In Advances in Neural Information Processing Systems, 2021.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed representations of words\n\nand phrases and their compositionality. In Advances in neural information processing systems, 2013.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding.\n\narXiv preprint arXiv:1807.03748, 2018.\n\nLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing\n\norder to the web. Stanford University Technical Report,, 1999.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nVeliˇckovi ́c Petar, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, and R. Devon Hjelm. Deep\n\ngraph infomax. arXiv preprint arXiv:1809.10341, 2018.\n\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1150–1160, 2020.\n\nJoshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative\n\nsamples. In International Conference on Learning Representations, 2021.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Imagenet large scale visual recognition challenge.\n\nKarpathy, Aditya Khosla, Michael Bernstein, et al. International journal of computer vision, 115(3):211–252, 2015.\n\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815–823, 2015.\n\nDaniel A Spielman and Shang-Hua Teng. A local clustering algorithm for massive graphs and its application to\n\nnearly linear time graph partitioning. SIAM Journal on computing, 2013.\n\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. arXiv preprint arXiv:1908.01000, 2019.\n\nJunfeng Tian, Zhiheng Zhou, Man Lan, and Yuanbin Wu. Ecnu at semeval-2017 task 1: Leverage kernel-based traditional nlp features and neural networks to build a universal model for multilingual and cross-lingual In Proceedings of the 11th international workshop on semantic evaluation semantic textual similarity. (SemEval-2017), pages 191–197, 2017.\n\nHanghang Tong, Christos Faloutsos, and Jia-Yu Pan. Fast random walk with restart and its applications. In Sixth\n\ninternational conference on data mining, 2006.\n\nGuangrun Wang, Keze Wang, Guangcong Wang, Philip HS Torr, and Liang Lin. Solving inefficiency of selfsupervised representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9505–9515, 2021.\n\nMike Wu, Milan Mosse, Chengxu Zhuang, Daniel Yamins, and Noah Goodman. Conditional negative sampling\n\nfor contrastive learning of visual representations. arXiv preprint arXiv:2010.02037, 2020a.\n\nZhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733–3742, 2018.\n\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. Clear: Contrastive learning for\n\nsentence representation. arXiv preprint arXiv:2012.15466, 2020b.\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv\n\npreprint arXiv:1810.00826, 2018.\n\nPinar Yanardag and SVN Vishwanathan. Deep graph kernels. Proceedings of the 21th ACM SIGKDD interna-\n\ntional conference on Knowledge discovery and data mining, 2015.\n\nZhen Yang, Ming Ding, Chang Zhou, Hongxia Yang, Jingren Zhou, and Jie Tang. Understanding negative sampling in graph representation learning. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1666–1676, 2020.\n\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery and data mining, 2018.\n\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in neural information processing systems, pages 5812–5823, 2020.\n\nWeinan Zhang, Tianqi Chen, Jun Wang, and Yong Yu. Optimizing top-n collaborative filtering via dynamic negative item sampling. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, pages 785–788, 2013.\n\nMingkai Zheng, Fei Wang, Shan You, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang Xu. Weakly supervised contrastive learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10042–10051, 2021.\n\nKun Zhou, Beichen Zhang, Wayne Xin Zhao, and Ji-Rong Wen. Debiased contrastive learning of unsupervised\n\nsentence representations. arXiv preprint arXiv:2205.00656, 2022.\n\nYanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. An empirical study of graph contrastive learning. arXiv\n\npreprint arXiv:2109.01116, 2021.\n\nJiˇrí Šíma and Satu Elisa Schaeffer. On the np-completeness of some graph cluster measures. In International\n\nConference on Current Trends in Theory and Practice of Computer Science, 2006.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA ALGORITHM DETAIL\n\nHere we present the detail of Proximity Graph Construction and Random Walk with Restart, as shown in Algorithm 2 and Algorithm 3 respectively.\n\nAlgorithm 2: Proximity Graph Construction\n\n, Candidate set size M , Neighbor number K;\n\nInput: Dataset D\nOutput: A proximity graph G; for v in\n\nxi}\n\ndo\n\n=\n\n{\n\nD\n\nRandomly select M neighbor candidates from Select the K closest candidates G[v]\n\nNv by Eq. 3;\n\n;\n\nD\n\n← Nv;\n\nend return G\n\nAlgorithm 3: Random Walk with Restart(RWR)\n\n, seed node u, restart probability α, number of sampled\n\n,\n\nE}\n\n{V\n\n;\n\nS\n\nInput: Proximity graph G =\n\nnode B;\n\nOutput: A sampled node set u;\n\n, v S ← {} while len(\n\nif v not in\n\n← ) < B do S\nthen .insert(v)\n\nS\n\nend Sample r from Uniform distribution U (0, 1); if r < α then\n\nS\n\nv\n\nu;\n\n←\n\nend else\n\nRandomly sample ˆv from v’s neighbors; v\n\nˆv;\n\nend\n\n←\n\nend return\n\nS\n\nB THEORETICAL PROOF\n\nProposition 1. Given an observation vi with the corresponding representation ei, assume that there are at least S observations whose inner product similarity with vi is larger than s, i.e.,\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:8)vj ∈ V |\n\nei ·\n\nej > s(cid:9)(cid:12)\n\n(cid:12) (cid:12) ≥\n\nS.\n\n(1)\n\nThen in the proximity graph G, the similarity between vi and its neighbors is larger than s with proximate probability at least:\n\nP\n\nei · N , and K is the number of neighbors.\n\nvk ∈ Ni}\n\nek > s,\n\n∀\n\n{\n\n−\n\nS\n\nwhere p = N\n\n⪆\n\n(cid:16)\n\n1\n\n−\n\npM (cid:17)K\n\n,\n\n(2)\n\nProof. Since M case, we have\n\n≪\n\nN , we can approximately assume that the sampling is with replacement. In this\n\nP\n\nei ·\n\n{\n\nek > s,\n\nvk ∈ Ni}\n\n∀\n\n= 1\n\n−\n\n1 K\n(cid:88) −\n\nk=0\n\n(cid:19)\n\n(cid:18)M k\n\npM\n\nk (1\n\n−\n\np)k .\n\n−\n\n(3)\n\nThen let us prove (2) by induction. When K = 1, the conclusion clearly holds.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAssuming that the conclusion holds when K = L have\n\n−\n\n1, let us consider the case when K = L. We\n\n1 L\n(cid:88) −\n\nk=0\n\n(cid:19)\n\n(cid:18)M k\n\n1\n\n−\n\npM\n\nk(1\n\n−\n\n−\n\np)k ⪆\n\n(cid:16)\n\n1\n\npM (cid:17)L\n\n1\n\n−\n\n−\n\n(cid:18) M L\n\n−\n\n(cid:19)\n\n1\n\n−\n\npM\n\nL+1 (1\n\n−\n\n−\n\np)L\n\n−\n\n1 .\n\n(4)\n\nTo prove the conclusion, we only need to show\n\n(cid:16)\n\n1\n\n−\n\npM (cid:17)L\n\n1\n\n−\n\npM ⪆\n\n(cid:18) M L\n\n−\n\n(cid:19)\n\n1\n\npM\n\nL+1 (1\n\n−\n\np)L\n\n−\n\n1 ,\n\n−\n\nor equivalently\n\n(cid:16)\n\n1\n\n−\n\npM (cid:17)L\n\n−\n\n1\n\npL\n\n−\n\n1 =\n\n⪆\n\n(cid:16)\n\n1\n\n− (cid:18) M L\n\n−\n\npM (cid:17)L\n\n−\n\n1 (cid:18) N\n\n(cid:19)L\n\n−\n\n1\n\nS\n\n− N\n1\n\n=\n\n(cid:19)L\n\n−\n\n(cid:19) (cid:18) S 1\nN\n\n(cid:18) M L\n\n−\n\n(cid:19)\n\n(1\n\n−\n\n1\n\np)L\n\n1 .\n\n−\n\nOn the other hand, according to Knuth (1997), we have\n\nwhere e denotes the Euler’s number. Substituting (7) into (6), we only need to show\n\n(cid:18) M L\n\n(cid:19)\n\n1\n\n≤\n\n(cid:18) eM L\n\n(cid:19)L\n\n−\n\n1\n\n,\n\n1\n\n−\n\n−\n\n(N\n\n−\n\nS)(L\n\n−\n\n(cid:16)\n\n1\n\n1)\n\n−\n\npM (cid:17)\n\n⪆ eM S.\n\n(5)\n\n(6)\n\n(7)\n\n(8)\n\nThe above relation holds depending on the choices of M , S and L, which can be approximately satisfied in our scenario.\n\nProposition 2. For all 0 < α starting from a node u stationary distribution, and Φ(\n\n∈ S\n\n1 and ≤\nescapes\n\n, the probability that a Lazy Random Walk with Restart 1\n), where pu is the\n\nα\n\n2α Φ(\n\n−\n\nsatisfies (cid:80) ) is the graph conductance of\n\nS ⊂ V S\n\nV−S\n\n) pu(v) .\n\n∈\n\nv\n\n(\n\n≤\n\nS\n\nS\n\nS\n\nProof. We first introduce the definition of graph conductance (Šíma and Schaeffer., 2006) and Lazy Random Walk (Spielman and Teng, 2013):\n\nGraph Conductance . For an undirected graph G = ( ) = (cid:80) is defined as vol( v\nnode set is defined to be ∂( followed:\n\n∈S S\n\n(x, y)\n\n∈ E|\n\n∈ S\n\n) =\n\nS\n\n{\n\nS ⊂ V d(v), where d(v) is the degree of node v. The edge boundary of a is calculated as\n\n. The conductance of\n\n, y /\n\nV\n\nx\n\nE\n\n,\n\n), the graph volume of a node set\n\n∈ S}\n\nS\n\n) =\n\nΦ(\n\nS\n\nmin(vol(\n\n∂( )\n| |\nS ), vol( S\n\nV − S\n\n))\n\n(9)\n\nLazy Random Walk . Lazy Random Walk (LRW) is a variant of Random Walk, which first starts at a node, then stays at the current position with a probability 1/2 or travels to a neighbor. The transition matrix of a lazy random walk is M ≜ (I + AD− 1)/2, where the I denotes the identity matrix, A is the adjacent matrix, and D is the degree matrix. The K-th step Lazy Random Walk distribution starting from a node u is defined as q(K) MK1u.\n\nWe then present a theorem which relates the Lazy Random Walk to graph conductance, which has been proved in Spielman and Teng (2013):\n\n←\n\nTheorem 1. For all K escapes u\n\n0 and ≥\nsatisfies q(K)(\n\n∈ S\n\nS\n\n, the probability that a K-step Lazy Random Walk starting at S ⊂ V )\nV − S\n\nKΦ(\n\n)/2.\n\n≤\n\nS\n\n, the Lazy and a start node u Theorem 1 guarantees that given a non-empty node set Random Walker will be more likely stuck at . Here we extend the LRW to Lazy Random Walk with Restart (LRWR) which will return to the start node with probability α or perform Lazy Random\n\nS ⊂ V\n\n∈ S\n\nS\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nWalk. According to the previous studies (Page et al., 1999; Avrachenkov et al., 2014; Chung and Zhao, 2010; Tong et al., 2006), we can obtain a stationary distribution pu by recursively performing Lazy Random Walk with Restart, which can be formulated as a linear system:\n\npu = α1u + (1\n\nα)Mpu\n\n−\n\n(10)\n\nwhere α denotes the restart probability. The element pu(v) represents the probability of the walker starting at u and ending at v. pu can be expressed by a geometric sum of Lazy Random Walk (Chung and Tsiatas, 2010):\n\npu = α\n\n∞(cid:88)\n\n(1\n\nl=0\n\n−\n\nα)lMl1u = α\n\n∞(cid:88)\n\n(1\n\nl=0\n\n−\n\nα)lq(l)\n\nu\n\n(11)\n\nApplying the Theorem 1, we have:\n\n(cid:88)\n\npu(v) = α\n\n∞(cid:88)\n\n(cid:88)\n\n(1\n\nv\n\n∈\n\n(\n\nV−S\n\n)\n\nl=0\n\nv\n\n(\n\nV−S\n\n)\n\n∈\n\nα)lq(l)\n\nu (v)\n\n−\n\nα\n\n≤\n\n∞(cid:88)\n\nl=0\n\nl(1\n\n−\n\nα)lΦ(\n\nS\n\n)/2 =\n\n1\n\nα\n\n− 2α\n\nΦ(\n\n)\n\nS\n\n(12)\n\nThe desired result is obtained by comparing the two sides of (12).\n\nIn particular, the only difference between Lazy Random with Restart and Random Walk with Restart is that the former has a probability of remaining in the current position without taking any action. They are equivalent when sampling a predetermined number of nodes.\n\nC INFONCE OBJECTIVE AND ITS VARIANTS\n\nHere we describe in detail the objective functions of three in-batch contrastive learning methods, including SimCLR (Chen et al., 2020), GraphCL (You et al., 2020) and SimCSE (Gao et al., 2021). Besides, we cover two variants, i.e., DCL (Chuang et al., 2020) and HCL (Robinson et al., 2021), which are also applied in the experiments.\n\nC.1 SIMCLR\n\n, then SimCLR (Chen et al., 2020) first uniformly draws a mini-batch of instances x1... xB} ⊂ D augments the instances by two randomly sampled augmentation strategies faug( ,\n), f ′aug( ∼ T ·\n· resulting in 2B data points. Two augmented views (xi, xi+B) of the same image are treated as a positive pair, while the other 2(B 1) examples are negatives. The objective function applied in SimCLR for a positive pair (xi, xi+B) is formulated as:\n\n−\n\n{\n\n)\n\nli,i+B =\n\nlog\n\n−\n\nef (xi)T f (xi+B )/τ\n\n(cid:80)2B j\n\n=i ef (xi)T f (xj )/τ\n\n,\n\n(13)\n\nwhere τ is the temperature and f ( ·\nmini-batch, including (xi, xi+B) and (xi+B, xi). It can be found that SimCLR takes all 2(B augmented instances within a mini-batch as negatives.\n\n) is the encoder. The loss is calculated for all positive pairs in a 1)\n\n−\n\nC.2 GRAPHCL AND SIMCSE\n\nSimilar as SimCLR, the objective function of GraphCL (You et al., 2020) and SimCSE (Gao et al., 2021) is defined on the augmented instance pairs within a mini-batch. Given a sampled minibatch , both GraphCL and SimCSE apply data augmentation to obtain positive pairs, and the loss function for a positive pair (xi, x+\n\nx1... xB} ⊂ D\n\ni ) can be formulated as:\n\n{\n\n16\n\n̸ Under review as a conference paper at ICLR 2023\n\nli =\n\nlog\n\n−\n\n(cid:80)B\n\ni )/τ\n\nef (xi)T f (x+ j=1 ef (xi)T f (x+\n\nj )/τ\n\n.\n\n(14)\n\nCompared with the SimCLR, GraphCL and SimCSE only take the other B as negatives.\n\n−\n\n1 augmented instances\n\nC.3 DCL AND HCL\n\nDCL (Robinson et al., 2021) and HCL (Robinson et al., 2021) are two variants of InfoNCE objective function, which aim to alleviate the false negative issue or mine the hard negatives by reweighting the negatives in the objective. The main idea behind them is using the positive distribution to correct for the negative distribution.\n\nFor simplicity, we annotate the positive score ef (xi)T f (x+ ef (xi)T f (x+ tive distribution proposed in DCL and HCL are:\n\nj )/τ as negij. Given a mini-batch and a positive pair (xi, x+\n\ni )/τ as pos, and negative score i ), the reweighting nega-\n\nmax\n\n\n\n\n\nB (cid:88)\n\nj=1\n\nNneg ×\n\n−\n\nτ +\n\npos + λij × τ +\n\n× 1\n\n−\n\nnegij\n\n\n\n1/τ\n\n, e−\n\n,\n\n(15)\n\nnegij\n\nwhere Nneg is the number of the negatives in mini-batch, τ + is the class probability, τ is the temperature, and λij is concentration parameter which is simply set as 1 in DCL or calculated as λij = β in HCL. All of τ +, τ, β are tunable hyperparameters. The insight of Eq.15 is that the negative pair with the score closer to positive score will be assigned lower weight in loss function. In other words, the similarity difference between positive and negative pairs dominates the weighting function.\n\n(cid:80) negij /Nneg\n\n×\n\nD DATASET DETAILS\n\nFor image representation learning, we adopt five benchmark datasets, comprising of CIFAR10, CIFAR100, STL10, ImageNet-100 and ImageNet ILSVRC-2012 (Russakovsky et al., 2015). Information on the statistics of these datasets is summarized in Table 6. For graph-level representation learning, we conduct experiments on IMDB-B, IMDB-M, COLLAB and REDDIT-B (Yanardag and Vishwanathan, 2015), the details of which are presented in Table 7. For text representation learning, we evaluate the method on a one-million English Wikipedia dataset which is used in the SimCSE and can be downloaded from HuggingFace repository1.\n\nTable 6: Statistics of datasets for image classification task.\n\nDatasets CIFAR10 CIFAR100 STL10 ImageNet-100 ImageNet\n\n#Train #Test #Classes\n\n50,000 10,000 10\n\n50,000 10,000 100\n\n105,000 8,000 10\n\n130,000 50,00 100\n\n1,281,167 50,000 1,000\n\nE EXPERIMENTAL DETAILS\n\nE.1\n\nIMAGE REPRESENTATIONS\n\nIn image domain, we apply SimCLR (Chen et al., 2020) and MoCo v3 (Chen et al., 2021) as the baseline method, with ResNet-50 (He et al., 2016) as an encoder to learn image representations. The\n\n1https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/\n\nresolve/main/wiki1m_for_simcse.txt\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: Statistics of datasets for graph-level classification task.\n\nDatasets\n\nIMDB-B IMDB-M COLLAB REDDIT-B\n\n#Graphs #Classes Avg. #nodes\n\n1,000 2\n19.8\n\n1,500 3\n13.0\n\n5,000 3\n74.5\n\n2,000 2\n429.7\n\nfeature map generated by ResNet-50 block is projected to a 128-D image embedding via a two-layer MLP (2048-D hidden layer with ReLU activation function). Besides, the output vector is normalized by l2 normalization (Wu et al., 2018). We employ two sampled data augmentation strategies to generate positive pairs, and implicitly use other examples in the same mini-batch as negative samples.\n\nFor CIFAR10, CIFAR100 and STL10, all models are trained for 1000 epochs with the default batch size B of 256. We use the Adam optimizer (Kingma and Ba, 2015) with learning rate of 0.001 for optimization. The temperature parameter is set as 0.5 and the dimension of image embedding is set as 128. For ImageNet-100 and ImageNet, we train the models with 100 and 400 epochs respectively, B/256 and weight decay and use LARS optimizer (You et al., 2019) with learning rate of 0.3 6. Here, the batch size is set as 2048 for ImageNet and 512 for ImageNet-100, respectively. of 10− We fix the temperature parameter as 0.1 and the image embedding dimension as 128. After the unsupervised learning, we train a supervised linear classifier for 100 epochs on the top of the frozen learned representations.\n\n×\n\nAs for ProSampler, we update the proximity graph per 100 training iterations. We fix the number of neighbors K as 100 for CIFAR10, CIFAR100 and STL10. The size of neighbor candidate set M is set as 1000 for CIFAR100 and STL10, and 500 for CIFAR10. Besides, the initial restart probability α of RWR (Random Walk with Restart) is set to 0.2 and decays linearly to 0.05 with the training process. For ImageNet-100 and ImageNet, we keep M as 1000 and K as 500. The restart probability α is fixed as 0.1.\n\nE.2 GRAPH REPRESENTATIONS\n\nIn graph domain, we use the GraphCL (You et al., 2020) framework as the baseline and GIN (Xu et al., 2018) as the backone. We run ProSampler 5 times with different random seeds and report the mean 10-fold cross-validation accuracy with variance. We apply Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.01, and 3-layer GIN with a fixed hidden size of 32. We set the temperature as 0.2 and gradually decay the restart probability of RWR (0.2 0.05). Proximity graph will be updated after t iterations. The overall hyperparameter settings on different datasets are summarized in Table 8.\n\n∼\n\nTable 8: Hyperparameter settings for graph-level representation learning.\n\nDatasets IMDB-B IMDB-M COLLAB REDDIT-B\n\nBatchsize Epoch t\nM K\n\n256 100 25 300 100\n\n128 50 25 300 100\n\n128 20 50 1,000 100\n\n128 50 50 500 100\n\nE.3 TEXT REPRESENTATIONS\n\nIn text domain, we use SimCSE (Gao et al., 2021) as baseline method and adopt the pretrained BERT and RoBERTa provided by HuggingFace2 for sentence embedding learning. Following the training setting of SimCSE, we train the model for one epoch in an unsupervised manner and evaluate it\n\n2https://huggingface.co/models\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\non 7 STS tasks. Proximity graph will be only built once based on the pretrained language models 5. For RoBERTa, before training. For BERT, we set the batch size to 64 and learning rate to 3 10− 5. We keep the temperature as 0.05, the batch size is set as 512 and learning rate is fixed as 10− the number of neighbor candidates M as 1000, the number of neighbors K as 500, and the restart probability α as 0.7 for both BERT and RoBERTa.\n\n×\n\nF ADDITIONAL EXPERIMENTS\n\nF.1 EXTENSIVE STUDIES ON COMPUTER VISION\n\nHere we evaluate the ProSampler on two small-scale (CIFAR10,CIFAR100) and two mediumscale (STL10,ImageNet-100) benchmark datasets, and equip DCL (Chuang et al., 2020) and HCL (Robinson et al., 2021)3 with ProSampler to investigate its generality. Experimental results in Table 9 show that ProSampler can consistently improve SimCLR and its variants on all the datasets, with an absolute gain of 0.3% 2.5%. We also can observe that the improvement is greater on medium-scale datasets than on small-scale datasets. Specifically, the model equipped with HCL and ProSampler achieves a significant improvement (6.23%) on STL10 over the original SimCLR.\n\n∼\n\nTable 9: Overall performance comparison on image classification task in term of Top-1 Accuracy.\n\nMethod\n\nCIFAR10 CIFAR100\n\nSTL10\n\nImageNet-100\n\nSimCLR w/ ProSampler\n\nDCL w/ ProSampler\n\nHCL w/ ProSampler\n\n92.13 92.54\n\n92.28 92.74\n\n92.39 92.41\n\n68.14 68.68\n\n68.52 68.91\n\n68.92 69.13\n\n83.26 84.38\n\n84.92 86.39\n\n88.20 89.49\n\n59.30 60.80\n\n59.90 60.14\n\n60.60 61.50\n\nF.2 SIMILARITY COMPARISON BETWEEN POSITIVE AND NEGATIVE PAIRS\n\nTo explain the performance degradation of DCL and HCL objectives, we select 12 representative mini-batches and plot the cosine similarity histogram of positive and negative pairs on BERT (top) and RoBERTa (bottom) in Figure 7. We observe the following: (1) At the start of and throughout the training, the positive pairs are assigned a high cosine similarity (around 0.9) by the pretrained language model; (2) The negative similarities begin with a relative high score and gradually skew left because of the self-supervised learning. Such phenomenon is consistent to Zhou et al. (2022). DCL and HCL which leverage the difference between positive and negative similarity to reweight the negative scores are inapplicable since the low distribution gap between positive and negative similarities will lead to homogeneous weighting in the objective.\n\nF.3 TEXT REPRESENTATIONS WITH ROBERTA\n\nWe also apply ProSampler to the SimCSE with the pretrained RoBERTa, and present the results in Table 10. Similar as the results of BERT, ProSampler can consistently improve the performance of the baseline model. Besides, as discussed in Section 4.1 and Section F.2, the hard negative sampled by ProSampler explicitly can alleviate the low distribution gap between positive score and negative score distribution caused by the pretrained language model, alleviating the performance degradation of DCL and HCL.\n\n3DCL and HCL are more like variants of InfoNCE loss, which adjust the weights of negative samples in the\n\noriginal InfoNCE loss.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Histograms of cosine similarity on BERT (top) and RoBERTa (bottom).\n\nTable 10: Performance comparison for sentence embedding learning based on RoBERTa.\n\nMethod\n\nSTS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg.\n\nSimCSE-RoBERTabase 67.90 80.91 73.14 80.58 80.74 80.26 68.29 81.96 73.86 82.16 80.94 80.77 w/ ProSampler 66.60 79.16 71.05 80.40 77.76 77.94 DCL-RoBERTabase 65.53 80.09 71.00 80.64 78.35 77.75 w/ ProSampler 67.20 80.47 72.44 80.88 80.57 78.79 HCL-RoBETabase 66.01 80.79 73.58 81.25 80.66 79.22 w/ ProSampler\n\n69.87 76.20 69.30 76.75 67.57 74.35 67.52 74.41 67.98 75.49 68.52 75.72\n\nF.4 COSINE SIMILARITY AND FALSE NEGATIVE RATIO ON CIFAR100\n\nHere we compare the Uniform Sampler, kNN Sampler, and ProSampler in terms of cosine similarity and false negatives on CIFAR100. Specifically, we show the histogram of cosine similarity for all pairs in a sampled batch, and the false negative ratio of the mini-batch in Figure 8. It can be found that ProSampler exhibits a balance of Uniform Sampler and kNN Sampler, which can sample the hard negative pair but only brings slightly greater number of false negatives than Uniform Sampler. More analysis can be found in Section 4.2.\n\nFigure 8: Cosine similarity and false negative ratio on CIFAR100.\n\nF.5 EFFICIENCY ANALYSIS\n\nTo further investigate the efficiency of ProSampler, we analyze the wall-clock time performance. Here, we introduce three metrics to analyze the time cost of mini-batch sampling by ProSampler: (1) Batch Sampling Cost (CostS) is the average time of RWR taken to sample a mini-batch from a proximity graph; (2) Proximity Graph Construction Cost (CostG) refers to the time consumption of ProSampler for constructing a proximity graph; (3) Batch Training Cost (CostT ) is the average time taken by the encoder to forward and backward; (4) Proximity Graph Construction Amortized\n\n20\n\n0.00.20.40.60.81.0Cosine Similarity0.00.51.01.52.02.53.03.54.0FrequencykNN SamplerProSamplerUniform Sampler050100150200Training Step(x1000)0.0100.0150.0200.0250.0300.035FN ratiokNN SamplerProSamplerUniform SamplerUnder review as a conference paper at ICLR 2023\n\nCost (CostG/t) is the ratio of CostG to the graph update interval t. The time cost of ProSampler is shown in Table 11, from which we make the following observations: (1) Sampling a mini-batch CostS takes an order of magnitude less time than training with a batch CostT at most cases. (2) Although it takes 100s for ProSampler to construct a proximity graph in ImageNet, the cost shares across t training steps, which take only CostG/t = 0.2 per batch. A similar phenomenon can be found in the other datasets as well. In particular, SimCSE only trains for one epoch, and proximity graph is only built once.\n\nTable 11: Time cost of mini-batch sampling by ProSampler on a NVIDIA V100 GPU.\n\nMetric\n\nSTL10\n\nImageNet-100\n\nWikipedia\n\nImageNet\n\nCostS CostG CostT CostG/t\n\n0.013s 2s 0.55s 0.02(t = 100)\n\n0.015s 3s 1.1s 0.03(t = 100)\n\n0.005 79s 0.08s 0.005(t = 15625)\n\n0.15s 100s 1.1s 0.2(t = 500)\n\nF.6 COMPREHENSIVE ANALYSIS ABOUT STRATEGIES OF PROXIMITY GRAPH SAMPLING\n\nWe conduct an experiment to explore different choices of graph sampling methods, including (1) Depth First Search (DFS); (2) Breadth First Search (BFS); (3) Random Walk (RW); (4) Random Walk with Restart (RWR). Table 12 presents an overall performance comparison with different graph sampling methods. Besides, we illustrate the histograms of cosine similarity for all pairs from a sampled batch after finishing training and plot the percentage of false negatives in the mini-batch during training in Figure 9. It can be observed that although BFS brings the most similar pairs in the mini-batch, it performs worse than the original SimCLR since it introduces substantial false negatives. While having a slightly lower percentage of false negatives than RWR, DFS and RW do not exhibit higher performance since they are unable to collect the hard negatives in the mini-batch. The restart property allows RWR to exhibit a mixture of DFS and BFS, which can flexibly modulate the hardness of the sampled batch and find the best balance between hard negatives and false negatives. Benefiting from it, RWR achieves the best performance over the other sampling methods.\n\nTable 12: Overall performance comparison with different graph sampling methods.\n\nTable 13: The performance comparison of different batchsize B.\n\nMethod\n\nBFS DFS RW RWR\n\nB\n\n16\n\n32\n\n64\n\n128\n\n256\n\nCIFAR10 91.03 92.14 92.28 92.54 CIFAR100 65.15 68.29 68.33 68.68 77.08 83.05 83.54 84.38\n\nSTL10\n\nCIFAR10 79.36 84.64 89.09 91.03 92.54 CIFAR100 46.59 56.24 61.30 65.96 68.68 56.31 68.61 74.24 82.56 84.38\n\nSTL10\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 9: (a,b) Percentage of false negative in a batch sampled from CIFAR10 and CIFAR100 over the training step using different graph sampling methods. (c,d) Histograms of cosine similarity of all pairs in a batch for embeddings trained on CIFAR10 and CIFAR100 using different sampling methods.\n\n21\n\n050100150200Training Step(x1000)0.100.120.140.160.180.200.220.24FN ratioBFSDFSRWRWR050100150Training Step(x1000)0.0100.0120.0140.0160.0180.0200.0220.0240.026FN ratioBFSDFSRWRWR0.00.20.40.60.81.0Cosine Similarity01234FrequencyBFSDFSRWRWR0.00.20.40.60.81.0Cosine Similarity01234FrequencyBFSDFSRWRWRUnder review as a conference paper at ICLR 2023\n\nF.7 COMPARISON BETWEEN PROXIMITY GRAPH AND KNN GRAPH\n\nTo demonstrate the effectiveness of proximity graph, we do an ablation study by replacing proximity graph with kNN graph which directly selects k neighbors with the highest scores for each instance from the whole dataset. The neighbor number k is 100 by default. The comparison results are shown in Table 14, from which we can observe that proximity graph outperforms the kNN graph by a margin. ProSampler with kNN graph even performs worse than the original contrastive learning method because of the false negatives.\n\nMethod\n\nSimCLR kNN graph\n\nproximity graph\n\nCIFAR-10 CIFAR-100\n\n92.13 68.14\n\n90.47 62.67\n\n92.54 68.68\n\nTable 14: Performance comparison of different graph construction methods.\n\nTo develop a intuitive understanding of how Proximity graph alleviates the false negative issue, Figure 10 plots the changing curve of false negative ratio in a batch. The results show that Proximity graph could discard the false negative significantly: by the end of the training, kNN will introduce more than 22% false negatives in a batch, while Proximity graph brings about 13% on the CIFAR10 dataset. Similar phenomenon can also be found on CIFAR100 dataset.\n\n(a) CIFAR10\n\n(b) CIFAR100\n\nFigure 10: Percentage of false negative using different graph building methods over the training step.\n\nF.8 PARAMETER ANALYSIS\n\nF.8.1 BATCHSIZE B\n\nTo analyze the impact of the batchsize B, we vary B in the range of {16, 32, 64, 128, 256} and summarize the results in Table 13. It can be found that a larger batchsize leads to better results, which is consistent with the previous studies (Chen et al., 2020; He et al., 2020; Kalantidis et al., 2020).\n\nF.8.2\n\nIMPACT OF NEIGHBOR NUMBER K\n\nIn Figure 11, we investigate the impact of the neighbor number K on ImageNet-100 dataset with the default ProSampler setting. We observe that an absolute improvement of 1.1% with the increasing size of neighbors. Specifically, model achieves an absolute performance gain of 0.9% from K = 100 to K = 300, while only obtains 0.2% from K = 300 to K = 500. Such experimental results are consistent with our prior philosophy, in which sampling more neighbors always increase the scale of proximity graph and urging ProSampler to explore smaller-scope local cluster (i.e. sample harder negatives within a batch), leading to a significant improvement in performance at first. However, performance degrades after reaching the optimum, because larger K introduces more easy negatives.\n\n22\n\n050100150200Training Step(x1000)0.100.120.140.160.180.200.220.24FN ratiokNN graphProximity graph050100150200Training Step(x1000)0.0100.0150.0200.0250.0300.035FN ratiokNN graphProximity graphUnder review as a conference paper at ICLR 2023\n\nFigure 11: Impact of neighbor number K.\n\nF.8.3 PROXIMITY GRAPH UPDATE INTERVAL t\n\nProximity graph will be updated per t training iterations, and to analyze the impact of t, we vary t in the range of {50,100,200,400} and summarize the results in Table 15. It can be observed that update intervals that are too short (t = 50) or too long (t = 400) will degrade the performance. The possible reason is that sampling on a proximity graph that is frequently updated results in unstable learning of the model. Besides, the distribution of instances in the embedding space will change during the training process, resulting in a shift in hard negatives. As a result, after a few iterations, the lazy-updated graph cannot adequately capture the similarity relationship.\n\nTable 15: Performance comparison with different update interval t on CIFAR10 and CIFAR100.\n\nUpdate Interval t\n\n50\n\n100\n\n200\n\n400\n\nCIFAR10 CIFAR100\n\n92.29 68.37\n\n92.54 68.68\n\n92.34 67.83\n\n92.26 68.59\n\nF.9 TRAINING CURVE\n\nWe plot the training curves on STL10 and ImageNet-100 respectively. As shown in Figure 12, on STL10 dataset, ProSampler takes only about 600 epochs to achieve the similar performance as the original SimCLR, which takes 1000 epochs. A similar phenomenon can be seen on ImageNet-100. All these results manifest that ProSampler can bring model better and faster learning.\n\n(a) STL10\n\n(b) ImageNet-100\n\nFigure 12: Training curves for image classification task on STL10 and ImageNet-100.\n\n23\n\n100200300400500600Neighbor Number K59.059.560.060.561.0Top-1 Accuracy59.759.860.660.860.860.5Under review as a conference paper at ICLR 2023\n\nF.10 CASE STUDY\n\nTo give an intuitive impression of the mini-batch sampled by ProSampler, we show some real cases of the negatives sampled by ProSampler and Uniform Sampler in Figure 13. For a given anchor (a cat or a dog), we apply ProSampler and Uniform Sampler to draw a mini-batch of images, and pick the images with the highest inner product with anchor. Obviously, compared with Uniform Sampler, the images sampled by ProSampler are more semantically relevant to the anchor in terms of texture, background or appearance.\n\n(a) Uniform Sampler\n\n(b) ProSampler\n\n(c) Uniform Sampler\n\n(d) ProSampler\n\nFigure 13: Case study of the negatives sampled by ProSampler and Uniform Sampler based on the encoder trained for 100 epochs on ImageNet. Given an anchor image (Cat or Dog), (a,c) select 10 images with the highest similarity from a mini-batch sampled by ProSampler, and (b,d) randomly select 10 images from a mini-batch sampled by Uniform Sampler.\n\n24",
    "reference": "# Summary Of The Paper\n\nThis paper presents a negative sampling strategy for contrastive learning, which many contrastive learning-related frameworks can incorporate to improve performance. Instead of the uniform sampler and kNN sampler, the proposed proximity graph could well capture the similarity relationships among instances. Random walks among the proximity graph can ﬂexibly explore the negatives. Experiments on several datasets demonstrate the superiority.\n\n# Strength And Weaknesses\n\nStrengths:\n\n(1) The proposed method is simple and effective.\n\n(2) The authors provide a detailed theoretical analysis.\n\n(3) The authors conduct extensive experiments for validation, including several large datasets and different modalities.\n\nWeaknesses:\n\n(1) The proposed method is very simple, which is similar to searching hard negative samples from a top knn graph constructed by different similarity computation strategies. The overall contribution is incremental from this point. I wonder why the authors pick M neighbors by distance and select K nearest ones by inner product operation. And why do not you directly select the K nearest neighbors?\n\n(2) To a certain extent, the construction of a mini-batch with hard negative samples is similar to filtering out the positive samples from the original mini-batch. There are already several methods [1,2] that adopt affinity graphs to select positive pairs from the original mini-batch and achieve much better results on contrastive feature learning and clustering tasks. In this case, the novelty of this paper is unsatisfying. \n\n(3) The main motivation of this paper lies in the influence of batch size on existing contrastive learning methods. According to MoCo v2 and MoCo V2+, the influence of batch size is marginal. I wonder whether the proposed sampling strategy still works for such a situation and please give more explanations.\n\n(4) According to Table 1, the authors only present results of 100 and 400 epochs. I wonder whether the final results can be improved by the ProSampler, such as the results after 800 or 1,000 epochs. Besides, compared with SimCLR, SwAV, and BYOL, the improvement on ImageNet by [1] is much more significant than the proposed method. Please compare the results with [1] in detail.\n\n(5) According to Table 9, the improvement is marginal on these small datasets.\n\n[1] Weakly Supervised Contrastive Learning, ICCV 2021\n[2] Graph Contrastive Clustering, ICCV 2021\n\n# Clarity, Quality, Novelty And Reproducibility\n\nPlease refer to my detailed comments above.\n\n# Summary Of The Review\n\nThe proposed method is quite simple and is highly related to existing graph-based positive sample selection methods. The overall novelty and contribution are incremental. Besides, the experimental improvement is also marginal.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nFP AINET: FUSION PROTOTYPE WITH ADAPTIVE INDUCTION NETWORK FOR FEW-SHOT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nA prototypical network treats all samples equally and does not consider the noisy samples, which leads to a biased class representation. In this paper, we propose a novel fusion prototype with an adaptive induction network (FP AINet) for fewshot learning that can learn representative prototypes from a few support samples. Specifically, to address the problem of noisy samples, an adaptive induction network is developed, which can learn different class representations for queries and assign adaptive scores for support samples according to their relative significance. Moreover, FP AINet can generate a more accurate prototype than comparison methods by considering the query-related samples. With an increasing number of samples, the prototypical network is more expressive since the adaptive induction network ignores the relative local features. As a result, a Gaussian fusion algorithm is designed to learn more representative prototypes. Extensive experiments are conducted on three datasets: miniImageNet, tieredImageNet, and CIFAR FS. The experimental results compared with the state-of-the-art few-shot learning methods demonstrate the superiority of FP AINet.\n\n1\n\nINTRODUCTION\n\nFew-shot learning aims to learn classifiers for novel classes with limited data. Prototypical network (PN) (Snell et al. (2017)) averages the support features as the prototype. While most of the previous research has achieved promising results, those methods generally assume that the samples used for training were carefully selected to represent their class. The expected prototype should have the smallest distance from all other samples in its class (Liu et al. (2020)), and each sample significantly contributes to the final performance when training from a few labeled samples. Unfortunately, the existing dataset frequently contains mislabeled samples because of weakly automated supervised annotation, ambiguity, or human error (Liang et al. (2022)). In addition, since some images have multiple objects and unrelated background information, the accuracy can be affected by a single noisy example. As illustrated in Figure 1 (a), the PN is easily affected by noisy samples. Metalearning approaches have become the dominant paradigm for few-shot learning (Chen et al. (2020); Tian et al. (2020); Yao et al. (2021)).\n\nMeta-learning approaches can be roughly summarized into two categories: optimization-based methods (Antoniou et al. (2019); Kao et al. (2022)) and metric-based methods (Vinyals et al. (2016); Sung et al. (2018)). Optimization-based methods readily learn the model’s parameters to adapt to each task using gradient descent. However, these methods need to be fine-tuned for the target tasks. Metric-based methods are more efficient and applicable than optimization-based methods. Metricbased methods learn a good metric to calculate the similarity between query and the support samples using a pre-defined distance function, such as cosine similarity (Vinyals et al. (2016)), euclidean distance (Snell et al. (2017); Koch et al. (2015)), earth mover’s distance (Zhang et al. (2020)), or a distance parameterized by a neural network (Sung et al. (2018); Zhang et al. (2018)), which has achieved remarkable success due to its fewer parameters.\n\nTo obtain more representative prototypes, many methods correct the prototype by using similar samples (Yang et al. (2021); Liu et al. (2020)) or additional knowledge (Zhang et al. (2021)), but since it is easy to introduce sample noise or class differences, a novel method of fusion prototype with an adaptive induction network (FP AINet) is proposed to solve the issue. The induction network (Geng et al. (2019)) designs a non-linear mapping from sample vector to class vector to diminish the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Prototype with noisy samples.\n\n(b) Test accuracy on miniImageNet.\n\nFigure 1: Different prototype models. (a) shows the sample is misclassified by the PN. Different colors represent different classes. The orange circle denotes the sample to be classified. (b) illustrates the test accuracy of different prototypes on the 5-way k-shot.\n\nprototype bias. But since the model has not seen query samples before extracting support features, some inappropriate features may be extracted, resulting in a significant deviation in prototype estimation. An adaptive induction network (AINet) is proposed to extract more reliable prototypes for each class. The AINet does not take into account the local relative importance of different regions in a sample, while the prototype generated by the PN becomes more discriminative and expressive as the number of support samples increases, as shown in Figure 1 (b). To solve the problem that the calculation of a single prototype is not comprehensive, we assume the estimated prototype follow a multivariate Gaussian distribution (Zhang et al. (2021)). Specifically, the features in the target task are transformed using the Yeo-Johnson transformation, and then two kinds of prototypes are combined, which are generated by AINet and PN, respectively. Finally, the performance of FP AINet is evaluated on the miniImageNet, tieredImageNet, and CIFAR FS. Besides, the ablation experiments validate the effectiveness of the FP AINet. Experimental results show that the FP AINet can generate a more representative prototype and improve the accuracy of few-shot learning.\n\nThe main contributions are summarized as follows: (1) A novel method of AINet is proposed to assign scores to support samples based on their relevance automatically. (2) A modified Gaussian-based fusion algorithm is employed to aggregates prototypes from PN and AINet by exploring the unlabeled samples. (3) Extensive experiments on three datasets demonstrate the effectiveness of the FP AINet.\n\n2 RELATED WORK\n\nUnlike conventional machine learning, which provides abundant training examples, few-shot learning requires a classifier that can quickly adapt to novel classes with limited examples. Many efforts have been made to address the issue of data efficiency.\n\nMetric-based methods. To boost the performance of PN, task dependent adaptive metric (TADAM) (Oreshkin et al. (2018)) proposes metric scaling and task conditioning. It is difficult to represent the distribution of a class with limited samples, so many methods have been proposed to correct bias in prototype estimations (Hou & Sato (2021); Yang et al. (2021)). BD-CSPN (Liu et al. (2020)) modifies prototypes by diminishing intra-class and cross-class bias. A pseudo-label is used to reduce intra-class bias, but it is easy to introduce noise. Rather than relying on a pre-defined metric to calculate similarity (Vinyals et al. (2016)), relation network (Sung et al. (2018)) and a deep comparison network (Zhang et al. (2018)) train deep neural networks to compare each query-support image pair. While previous methods adopted the conceptual representation of the first moment (Snell et al. (2017)), CovaMNet (Li et al. (2019)) adopts the second moment rather than the first moment for feature description. Unlike the above methods, multi-level metric learning (Chen et al. (2022)) measures the similarity at three different feature levels. According to the above analysis, most existing methods ignore the noisy samples, resulting in biased class representations. To solve this issue, this paper proposes a more accurate prototype estimate method to improve the few-shot image classification performance.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nTransductive few-shot learning. In general, inductive few-shot is employed when data acquisition is expensive, and transductive few-shot is applied when data labeling is expensive (Bendou et al. (2022)). Some studies have tackled the problem by utilizing the additional knowledge from the query dataset or extra unlabeled examples in a transductive setting (Wang et al. (2020); Nichol et al. (2018)). However, they share knowledge between query datasets via batch normalization rather than explicitly modeling the transductive setting as in (Flennerhag et al. (2020)). Task-adaptive feature sub-space learning (TAFSSL) (Lichtenstein et al. (2020)) looks for the discriminative feature sub-spaces for few-shot classification tasks. In contrast to unidirectional label propagation, mutual centralized learning (MCL) considers query and support dataset features as bipartite data and avoids self-reinforcements (Liu et al. (2022)). Inspired by transductive few-shot learning, unlabeled samples are employed to estimate the prototype and enrich the feature representation.\n\n3 METHOD\n\n3.1 PROBLEM DEFINITION\n\nt\n\n, xbase t\n\n(cid:9)N base t=1\n\nrepresents the image sampled from the base class C base, ybase\n\nthe base class dataset Dbase with abunA few-shot classification setting includes two datasets: dant labeled images and the novel class dataset Dnovel with few labeled data. Suppose Dbase = (cid:8)xbase , ybase ∈ C base t\nis the label of xbase , there is no intersection between the base class and novel class, that is C base ∩ C novel = ∅, C base ∪ C novel = C. In each iteration process, one of the episodes means that N classes are selected at random, and each class contains K labeled samples as a support dataset s=1 with a few labeled samples. The query set Q = {(xq, yq)}N ×K′ S = {(xs, ys)}N ×K q=N ×K+1 contains examples of the same N classes in S, K ′ is the quantity of each class in Q. The model needs to predict a class label for a query sample given N support classes, each containing K support samples.\n\nt\n\nt\n\n3.2 OVERALL ARCHITECTURE\n\nThe FP AINet consists of three stages, including the pre-training stage, the meta-training stage, and the meta-testing stage. An overview of the FP AINet is provided in Figure 2.\n\nFigure 2: An overview of FP AINet.\n\nPre-training stage. During the pre-training stage, an embedding model is trained on the base class dataset Dbase, the last Softmax layer is removed, and the classifier is transformed into a feature extractor fθf () with parameters θf , allowing the model to learn task-agnostic knowledge from base classes and then apply this knowledge to novel classes to produce more reliable prototypes. Then, the feature extractor fθf () is frozen.\n\nMeta-training stage. An N-way K-shot classification task is constructed through episode few-shot learning using the base class dataset Dbase. In each episode, class C is sampled from Dbase, K\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nsamples of each class are used as support set S, K ′ are selected as query dataset Q from the remaining samples in class C. Then the fθf () can be fine-tuned on query dataset. During each episode, estimating the mean-based prototype ˆpi by averaging the labeled support features. Furthermore, AINet is proposed to learn the class prototype p′ i, which is derived from the features fθf (x) of the support and query samples. To obtain more mutual information, the fusion prototype pi is calculated using the Gaussian-based fusion method. Finally, the cosine similarity of features fθf (x) and pi is calculated to determine the probability that each sample x ∈ Q belongs to class i.\n\nMeta-testing stage. The same as the meta-training, and classification task is performed on Dnovel.\n\n3.2.1 ADAPTIVE INDUCTION NETWORK\n\nThe induction module (IM) (Geng et al. (2019)) learns the class-level relationship by considering features and classes to be local-global relationships, but because of the diversity and incompleteness of the support sample, every support sample contributes differently to the class representation when it faces different target query samples. In order to learn a more representative class vector and reduce sample noise, we propose an AINet that pays more attention to effective instances for current query samples. The details of the AINet are shown in Algorithm 1. Using the multi-head self-attention ij and query vector zq are concatenated to calculate the relationship mechanism, the support vector zs score; each support vector has its weight attached to the current query vector. Then, we apply dynamic routing to obtain a class vector. The process adjusts the connection’s strength dynamically and makes sure that the sum of the coupling coefficients di between class i and all of its support samples is 1. The difference is that when adjusting the logits of coupling coefficients in the last step of every iteration, we consider not only the consistency of class candidate vectors and sample prediction vectors but also the relationship between query and support vectors.\n\nAlgorithm 1 Adaptive Induction Network Require: sample vector zs\n\nlogits of coupling coefficients: bij = 0\n\nEnsure: Class vector p′ i\n\nfor all samples j = 1, ..., K in class i:\n\nij in support dataset S and a vector zq in query dataset Q, initialize the\n\nzij = Concat(zs\n\nsij = sof tmax( ij = squash(Wszs ˆzs\n\nfor r iterations do\n\nij, zq), zij is equivalent to concatenate the vector zs zij zT ij√ d\n\n)zij, where d is the dimension of zij\n\nij and zq\n\nij + bs), where Ws is transformation weights, bs denotes bias\n\nij is the prediction vector, p′\n\ni is the class candidate vector i∥ , where squash is a non-linear squashing function\n\np′ i\n∥p′\n\ndi = softmax(bi) i = (cid:80) ij, where ˆzs j dij · ˆzs p′ i) = ∥p′ p′ i = squash(p′ for all sample j = 1, ..., K in class i:\n\ni∥2 1+∥p′\n\ni∥2\n\nbij = bij + sij · tanh(ˆzs\n\nij · p′ i)\n\nend for return p′ i\n\n3.2.2 PROTOTYPE FUSION\n\nWhen the number of training samples is limited, p′ i is more accurate because the model needs to focus on more representative features, and ˆpi is more representative as the number of samples increases because the model only considers global features and ignores local features. This means that ˆpi and p′ i can learn mutual affiliations with each other(Zhang et al. (2021)). In order to address the aforementioned issues, a prototype fusion algorithm is proposed to reduce the prototype bias. We assume that the estimated prototype has a Gaussian distribution, and the distributions are independent of each other because samples in the pre-trained space are continuous and clustered. Algorithm 2 describes the Gaussian-based prototype fusion.\n\nTo follow a multivariate normal distribution(Yang et al. (2021)), the input features are preprocessed using the Yeo-Johnson transformation(Weisberg (2001)). The Yeo-Johnson transformation can reduce the heteroskedasticity of random variables and increase their normality, resulting in a probabil-\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nity density function with a similarity to the normal distribution. At the same time, the Yeo-Johnson transformation can be applied to samples with zero and negative features, making it suitable for statistical analysis of random variables based on the normal assumption, as follows in Equation 1.\n\nfθf (x) =\n\n\n\n\n\n\n\n[(fθf (x)+1)λ−1] λ\n\n,\n\nlog(fθf (x)) + 1),\n\n[(−fθf (x)+1)2−λ−1] −\n2−λ − log(−fθf (x) + 1),\n\nλ ̸= 0, fθf (x) ≥ 0 λ = 0, fθf (x) ≥ 0\n\n,\n\nλ ̸= 2, fθf (x) < 0 λ = 2, fθf (x) < 0\n\n(1)\n\nwhere fθf (x) is the feature to be transformed and the λ is employed to correct the distribution. Then, the mean-based prototype of ˆpi should be estimated by averaging the features of the support labeled samples, it can be calculated by Equation 2.\n\nˆpi =\n\n1 |Si|\n\n(cid:88)\n\nx∈Si\n\nfθf (x)\n\n(2)\n\ni ), and p′\n\ni, diag(σ′2\n\nwhere Si represents the support dataset extracted for the class i, and fθf (x) is the feature of support dataset. We assume the ˆpi follows a Gaussian distribution with a mean ˆμi and diagonal covariance diag( ˆσ2 i is a sample from N (μ′ i )). To improve the class representation of the i and diagonal covariance diag( ˆσ2 model, learn a Gaussian distribution with mean ˆμi + μ′ i ), then the mean is used to calculate the fusion prototype pi, as shown in Equation 3. i, diag( ˆσ2 i )), θ ∼ N ( ˆμi + μ′ i ), θ′ ∼ N (μ′ Transductive few-shot learning method is used to calculate the ˆμi and μ′ i(Liu et al. (2020)) by leveraging the unlabeled samples. When the class prototype is ˆpi or p′ i, the Equations 4 and 5 can be used to calculate the probability of x ∈ S ∪ Q, where S is the support dataset with a few labeled samples and Q is the query dataset with unlabeled samples.\n\nˆθ ∼ N ( ˆμi, diag( ˆσ2\n\ni, diag(σ′2\n\ni + σ′2 i )\n\ni + σ′2\n\n(3)\n\nˆP (y = i | x) =\n\nP ′(y = i | x) =\n\ned(fθf (x)), ˆpi) c ed(fθf (x), pc)\n\n(cid:80)\n\ni) ed(fθf (x), p′ c ed(fθf (x)), pc)\n\n(cid:80)\n\n(4)\n\n(5)\n\nwhere d() is the cosine similarity. Then, ˆμi and μ′ P ′(y = i | x) as the weights, as shown in Equation 6 and 7.\n\ni can be calculated by regarding ˆP (y = i | x) and\n\nˆμi =\n\nμ′\n\ni =\n\n1\n\n(cid:80)\n\nx∈S∪Q P (i | x)\n\n1\n\n(cid:80)\n\nx∈S∪Q P (i | x)\n\n(cid:88)\n\nx∈S∪Q\n\n(cid:88)\n\nx∈S∪Q\n\nˆP (i | x)fθf (x)\n\nP ′(i | x)fθf (x)\n\nFinally, the fusion prototype of pi can be obtained by ˆμi and μi, as shown in Equation 8.\n\npi = μi = ˆμi + μ′\n\ni\n\n(6)\n\n(7)\n\n(8)\n\n4 EXPERIMENTAL SETUP\n\n4.1 DATASETS AND SETTINGS\n\nThe method of FP AINet is evaluated on the miniImageNet, tieredImageNet and CIFAR FS. miniImageNet (Ravi & Larochelle (2017)) contains 100 classes with 600 samples per class. The\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Prototype Fusion Require: Support samples S = {(xs, ys)}N ×K Ensure: Fusion prototype pi\n\ns=1\n\nfor each episode iteration do\n\n, query samples Q = {(xq, yq)}N ×K′\n\nq=N ×K+1\n\nCreate the episodic tasks using S and Q, fine-tuned the feature extractor fθf () Estimate the mean-based prototype ˆpi with Equation 2 Calculate the class vector p′ Use ˆpi and p′ Calculate ˆμi and μ′ Estimate the fusion prototype pi by ˆμi and μ′\n\ni to calculate the probability of x ∈ S ∪ Q with Equation 4 and 5, respectively i by ˆP (y = i | x) and P ′(y = i | x) with Equation 6 and 7, respectively\n\ni with Algorithm 1\n\ni\n\nend for return pi\n\ndataset is divided into 64, 16, and 20 classes for training, validation, and testing. tieredImageNet (Ren et al. (2018)) consists of a total of 608 classes, which are divided into 34 higher-level classes. The training dataset contains 20 higher-level classes, 351 fine-grained classes; 6 higher-level classes, 97 fine-grained classes as validation sets; 8 higher-level classes, and 160 fine-grained classes as the test datasets. The image size of miniImageNet and tieredImageNet is 84 × 84 × 3 . CIFAR FS (Bertinetto et al. (2019)) contains 100 classes and 600 images in each class, including 64 classes of training datasets, 16 classes of validation datasets, and 20 classes of test datasets. The image size is unified to 32 × 32 × 3.\n\nThe classical 5-way 1/5-shot episodic in few-shot task settings are adopted. The query dataset contains 6 images per class during the meta-training stage, 15 test samples during the meta-testing stage, and 10,000 tasks are randomly constructed. Then test the task and calculate the average classification accuracy of top-1 and the 95% confidence interval as the final result.\n\n4.2\n\nIMPLEMENTATION DETAILS\n\nThe experiment is conducted on the feature extractor of ResNet-12 with 640-dimensional for the tieredImageNet. Each residual block contains three 3 × 3 convolutional layers and a shortcut connection. The WRN-28-10 with a layer number of 28 and a width of 10 is used for tieredImageNet and the extracted features are 512-dimensional. Average pooling is applied at the last block of each architecture to get feature vectors (Mangla et al. (2020)). In the pre-training stage, the base class dataset is trained on 100 epochs with a batch size of 128. SGD with a momentum of 0.9 and weight decay of 0.0005 is adopted as the optimizer to train the feature extractor of ResNet-12, while the Adam optimizer is used for WRN-28-10. In the meta-training stage, data augmentation techniques are used, including random cropping, color jittering, and horizontal flipping. The model is metatrained for 60 epochs, with each epoch containing 1000 episodes and an initial learning rate of 0.1. When the epochs are set to 20, 40, and 50, the learning rate changes to 0.006, 0.0012, and 0.00024, respectively. λ is set to 0.5 in the Yeo-Johnson transform, and 3 iterations were used for the AINet.\n\n4.3 EXPERIMENTAL RESULTS\n\n4.3.1 COMPARISON WITH STATE-OF-THE-ART METHODS\n\nTables 1 and 2 show the 5-way 1/5-shot classification results of the FP AINet and state-of-theart few-shot learning methods on the miniImageNet and tieredImageNet, respectively. Table 1 shows that the FP AINet achieves better performance on miniImageNet compared with comparison methods. In the 5-way 1/5-shot settings, the accuracy of the FP AINet reaches 72.13% and 84.29%, respectively. Compared to the suboptimal methods Curvature Generation(Gao et al. (2021)) and UniSiam (Lu et al. (2022)), it increased by about 0.34% and 0.89%, respectively. On the tieredImageNet, the accuracy of FP AINet on 1-shot is higher than 0.49% of the second-best models of BD-CSPN (Liu et al. (2020)), and higher than 0.29% EPNet(Rodr ́ıguez et al. (2020)) on a 5-shot setting. The FP AINet has such an improvement attributed to considering the more important samples. Moreover, the Gaussian-based fusion algorithm alleviates the prototype error and facilitates learning the optimal prototype by exploring the unlabeled samples.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: 5-way 1/5-shot accuracy (%) on miniImageNet with 95% confidence intervals.The best two results are highlighted and underlined.\n\nMethod\n\nBackbone\n\nSetting\n\n5-way 1-shot\n\n5-way 5-shot\n\nminiImageNet\n\nMatching Network (Vinyals et al. (2016)) Relation Network (Sung et al. (2018)) R2D2 (Bertinetto et al. (2019)) Baseline++ (Chen et al. (2019)) TADAM (Oreshkin et al. (2018)) PN (Snell et al. (2017)) B+@EST+L2-N (Hou & Sato (2021)) MetaOptNet (Lee et al. (2019)) MetaBaseline (Chen et al. (2021b)) S2M2 (Mangla et al. (2020)) UniSiam (Lu et al. (2022)) DeepEMD (Zhang et al. (2020)) ICI (Wang et al. (2020)) DC (Yang et al. (2021)) BD-CSPN (Liu et al. (2020)) AIM (Lee et al. (2021)) Curvature Generation(Gao et al. (2021)) FP AINet (OURS)\n\n64-64-64-64 64-96-128-256 96-192-384-512 ResNet-18 ResNet-12 ResNet-12 ResNet-18 ResNet-12 ResNet12 ResNet-18 ResNet-34 ResNet-12 ResNet-12 WRN-28-10 WRN-28-10 WRN-28-10 ResNet-12 WRN-28-10\n\nInductive Inductive Inductive Inductive Transductive Inductive Inductive Inductive Inductive Inductive Inductive Inductive Transductive Inductive Transductive Transductive Transductive Transductive\n\n43.56 ± 0.84 50.44 ± 0.82 51.20 ± 0.60 51.87 ± 0.77 58.50 ± 0.30 60.37 ± 0.83 62.44 62.64 ± 0.61 63.17 ± 0.23 64.06 ± 0.18 65.55 ± 0.36 65.91 ± 0.82 66.8 68.57 ± 0.55 70.31 ± 0.93 71.22 ± 0.57 71.79 ± 0.23 72.13 ± 0.73\n\n55.31 ± 0.73 65.32 ± 0.70 68.80 ± 0.10 75.68 ± 0.63 76.70 ± 0.30 78.02 ± 0.57 77.13 78.63 ± 0.46 79.26 ± 0.17 80.58 ± 0.12 83.40 ± 0.24 82.41 ± 0.56 79.26 82.88 ± 0.42 81.89 ± 0.60 82.25 ± 0.34 83.00 ± 0.17 84.29 ± 0.44\n\nTable 3 shows the comparison results of the FP AINet with the main few-shot learning methods on the CIFAR-FS. In the 5-way 1-shot setting, the accuracy of the FP AINet reaches 81.92%, 0.32% higher than the suboptimal method of SSR (Shen et al. (2021)), which proves that FP AINet can handle extremely few-shot classification tasks better. In the 5-way 5-shot setting, the accuracy of FP AINet is 89.38%, which is 0.38% higher than the suboptimal method EASY (Bendou et al. (2022)). The FP AINet has the highest accuracy with the same backbone, and accurate prototypes are more effective than fully extracted features. Furthermore, accuracy on the 5-shot setting is significantly higher than on the 1-shot setting. The main reason is that fewer annotated samples result in inaccurate prototype estimation, whereas a 5-shot can yield a more representative prototype estimation. It is verified that the FP AINet can better handle the few-shot learning task with a limited amount of data. The prototype features of the novel class are expressed more abundantly and accurately by fusing the prototypes.\n\nTable 2: 5-way 1-shot/5-shot accuracy (%) on tieredImageNet with 95% confidence intervals.\n\nMethod\n\nBackbone\n\nSetting\n\n5-way 1-shot\n\n5-way 5-shot\n\ntieredImageNet\n\nRelation Network (Sung et al. (2018)) B+@EST+L2-N (Hou & Sato (2021)) PN (Snell et al. (2017)) MetaOptNet (Lee et al. (2019)) MetaBaseline (Chen et al. (2021b)) DeepEMD (Zhang et al. (2020)) Meta DeepBDC (Xie et al. (2022)) SIB (Hu et al. (2020)) ECKPN (Chen et al. (2021a)) Curvature Generation(Gao et al. (2021)) ICI v2 (Wang et al. (2021)) EPNet (Rodr ́ıguez et al. (2020)) BD-CSPN (Liu et al. (2020)) FP AINet (OURS)\n\n64-96-128-256 ResNet-18 ResNet-12 ResNet-12 ResNet-12 ResNet-12 ResNet-12 WRN-28-10 ResNet-12 ResNet-12 ResNet-12 WRN-28-10 WRN-28-10 WRN-28-10\n\nInductive Inductive Inductive Inductive Inductive Inductive Inductive Transductive Transductive Transductive Transductive Transductive Transductive Transductive\n\n54.48 ± 0.93 60.87 65.65 ± 0.92 65.99 ± 0.72 68.62 ± 0.27 71.16 ± 0.87 72.34 ± 0.49 72.9 73.59 ± 0.45 77.19 ± 0.24 77.48 ± 0.62 78.50 ± 0.91 78.74 ± 0.95 79.23 ± 0.70\n\n71.32 ± 0.78 81.80 83.85 ± 0.36 81.56 ± 0.53 83.74 ± 0.18 86.03 ± 0.58 87.31 ± 0.32 82.8 88.13 ± 0.28 86.18 ± 0.15 86.84 ± 0.36 88.36 ± 0.57 86.92 ± 0.63 88.65 ± 0.49\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: 5-way 1-shot/5-shot accuracy (%) on CIFAR FS with 95% confidence intervals.\n\nCIFAR FS\n\nMethod\n\nBackbone\n\nSetting\n\n5-way 1-shot\n\n5-way 5-shot\n\nRelation Network (Sung et al. (2018)) MAML (Finn et al. (2017)) B+@EST+L2-N (Hou & Sato (2021)) BD-CSPN (Liu et al. (2020)) PN (Snell et al. (2017)) MetaOptNet (Lee et al. (2019)) S2M2 (Mangla et al. (2020)) EASY (Bendou et al. (2022)) Fine-tuning (Dhillon et al. (2020)) ICI v2 (Wang et al. (2021)) SIB (Hu et al. (2020)) SSR (Shen et al. (2021)) FP AINet (OURS)\n\n64-96-128-256 32-32-32-32 ResNet-18 WRN-28-10 ResNet-12 ResNet-12 ResNet-18 3xResNet-12 WRN-28-10 ResNet-12 WRN-28-10 WRN-28-10 WRN-28-10\n\nInductive Inductive Inductive Transductive Inductive Inductive Inductive Transductive Transductive Transductive Transductive Transductive Transductive\n\n55.00 ± 1.00 58.90 ± 1.90 63.00 72.13 ± 1.01 72.20 ± 0.70 72.80 ± 0.70 74.81 ± 0.19 76.20 ± 0.20 76.58 ± 0.68 79.19 ± 0.63 80.00 ± 0.60 81.60 ± 0.60 81.92 ± 0.69\n\n69.30 ± 0.80 71.50 ± 1.00 77.99 82.28 ± 0.69 83.50 ± 0.50 85.00 ± 0.50 87.47 ± 0.13 89.00 ± 0.14 85.79 ± 0.50 86.66 ± 0.36 85.30 ± 0.40 86.00 ± 0.40 89.38 ± 0.44\n\n4.3.2 ABLATION STUDY\n\nTable 4 summarizes the results of FP AINet and shows that each component is important in few-shot image classification, giving improvements over the state-of-the-art on the miniImageNet. Among them, (i) represents classification using only PN, (ii) is classification result of induction module, (iii) denotes classification using only AINet, and (iv) represents the Gaussian-based fusion algorithm. Obviously, in the 5-way 1-shot setting, if neither module is used, the accuracy drops by more than 10%. The prototype fusion algorithm of FP AINet achieves better performance than AINet.\n\nAdaptive Induction Network. It can be seen from (iii) in Table 4 that in the 5-way 1-shot, the classification result of AINet is better than the PN, and the main reason is that the module calculates the prototype by using query samples and selection. At the same time, the induction prototype method obtains class-level information and automatically adjusts the coupling coefficient according to the input, which is suitable for few-shot learning and can achieve good results in the presence of noise. In 5-way 5-shot, with the increase of samples, the mean-based prototype obtains better class representation. The results demonstrate that paying more attention to effective support samples is an important factor in the few-shot classification problem.\n\nPrototype fusion. The accuracy of the AINet is improved by about 9% and 6%, respectively, in the 5-way 1/5-shot settings, as shown by the model of (iv) in Table 4. The results indicate that fusion prototypes can improve model performance and alleviate bias in prototype estimates. The primary argument is that prototype fusion utilizes more samples, which can more effectively address the issues of sample noise and incompleteness in few-shot learning. The results show the necessity and effectiveness of learning an optimal class prototype.\n\nTable 4: Ablation studies of 5-way 1/5-shot on miniImageNet.\n\nIM AINet PN Fusion\n\n5-way 1-shot\n\n5-way 5-shot\n\n(i) (ii) ✓ (iii) (iv)\n\n✓\n\n✓\n\n✓ ✓\n\n61.47 ± 0.66 63.85 ± 0.68 63.88 ± 0.66 72.13 ± 0.73\n\n79.33 ± 0.48 78.23 ± 0.48 78.49 ± 0.49 84.29 ± 0.44\n\n✓\n\nThe prototypes generated by the FP AINet are visualized using t-distributed stochastic neighbor embedding (t-SNE). A 5-way 1-shot task of miniImagenet is shown in Figure 3, where circles represent query samples and different colors denote different classes, stars represent PN features, pentagons are AINet features, and squares represent FP AINet. The prototypes generated by FP AINet are\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nmuch closer to the class center, which can effectively learn the representation of a prototype and improve the capacity of the support dataset.\n\nFigure 3:\n\nt-SNE visualization of different prototype.\n\n4.3.3 DIFFERENT PROTOTYPE METHODS\n\nFigure 4 shows the results of different prototype methods on the 5-way k-shot task. On the miniImageNet, the prototype based on AINet is more accurate in 1/2-shot tasks. The PN achieves better performance on the 3/4/5-shot tasks; on the CIFAR FS, the AINet outperforms the PN on the 1/2/3-shot classification tasks, while the PN is better at classification on the 4/5-shot tasks. The main reason is that mean-based prototypes may be far from the expected class center when given very few labeled samples. But the mean-based prototype obtains more training samples and achieves better classification performance as the number of shots increases. The advantages of the two prototypes are fused to create a more representative prototype through Gaussian-based prototype fusion. Meanwhile, when the shot of the support dataset setting on the 5-way k-shot task is increased to 5, the accuracy of the three prototype models on miniImageNet and CIFAR FS improves.\n\n(a) miniImageNet\n\n(b) CIFAR FS\n\nFigure 4: The accuracy of different prototype methods on different datasets.\n\n5 CONCLUSION\n\nTo address the problem of noisy samples in few-shot learning, we propose a new method based on Gaussian fusion with an adaptive induction network. Firstly, it is significant to exploit different samples for obtaining the class representation, and the AINet can evaluate the significance of different samples adaptively. Secondly, a single prototype method is not comprehensive enough, and a Gaussian-based fusion algorithm is employed to obtain more accurate prototypes. Experiments show that the FP AINet achieves consistent improvements on three datasets, which verifies the effectiveness of the FP AINet in few-shot image classification.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAntreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your maml. International\n\nConference on Learning Representations, 2019.\n\nYassir Bendou, Yuqing Hu, Raphael Lafargue, Giulia Lioi, Bastien Pasdeloup, St ́ephane Pateux, and Vincent Gripon. Easy: Ensemble augmented-shot y-shaped learning: State-of-the-art fewshot classification with simple ingredients. arXiv preprint arXiv:2201.09699, 2022.\n\nLuca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differ-\n\nentiable closed-form solvers. International Conference on Learning Representations, 2019.\n\nChaofan Chen, Xiaoshan Yang, Changsheng Xu, Xuhui Huang, and Zhe Ma. Eckpn: Explicit class knowledge propagation network for transductive few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6596–6605, 2021a.\n\nHaoxing Chen, Huaxiong Li, Yaohui Li, and Chunlin Chen. Multi-level metric learning for fewshot image recognition. In International Conference on Artificial Neural Networks, pp. 243–254, 2022.\n\nWeiYu Chen, YenCheng Liu, Zsolt Kira, YuChiang Frank Wang, and Jia-Bin Huang. A closer look\n\nat few-shot classification. International Conference on Learning Representations, 2019.\n\nYinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, and Trevor Darrell. A new meta-baseline\n\nfor few-shot learning. International Conference on Learning Representations, 2020.\n\nYinbo Chen, Zhuang Liu, Huijuan Xu, Trevor Darrell, and Xiaolong Wang. Meta-baseline: Exploring simple meta-learning for few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9062–9071, 2021b.\n\nGuneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for\n\nfew-shot image classification. International Conference on Learning Representations, 2020.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\n\nof deep networks. In International conference on machine learning, pp. 1126–1135, 2017.\n\nSebastian Flennerhag, Andrei A Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, and Raia International Conference on Learning\n\nHadsell. Meta-learning with warped gradient descent. Representations, 2020.\n\nZhi Gao, Yuwei Wu, Yunde Jia, and Mehrtash Harandi. Curvature generation in curved spaces for few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8691–8700, 2021.\n\nRuiying Geng, Binhua Li, Yongbin Li, Xiaodan Zhu, Ping Jian, and Jian Sun. Induction networks for few-shot text classification. Conference on Empirical Methods in Natural Language ProcessingInternational Joint Conference on Natural Language Processing, pp. 3895–3904, 2019.\n\nMingcheng Hou and Issei Sato. A closer look at prototype classifier for few-shot image classifica-\n\ntion. International Conference on Learning Representations, pp. 721–731, 2021.\n\nShell Xu Hu, Pablo G Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, Neil D Lawrence, and Andreas Damianou. Empirical bayes transductive meta-learning with synthetic gradients. International Conference on Learning Representations, 2020.\n\nChia Hsiang Kao, WeiChen Chiu, and PinYu Chen. Maml is a noisy contrastive learner in classifi-\n\ncation. In International Conference on Learning Representations, 2022.\n\nGregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. In International Conference on Machine Learning Deep Learning Workshop, volume 2, pp. 1–8, 2015.\n\nEugene Lee, Cheng-Han Huang, and Chen-Yi Lee. Few-shot and continual learning with attentive independent mechanisms. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9455–9464, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable convex optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10657–10665, 2019.\n\nWenbin Li, Jinglin Xu, Jing Huo, Lei Wang, Yang Gao, and Jiebo Luo. Distribution consistency based covariance metric networks for few-shot learning. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 8642–8649, 2019.\n\nKevin J Liang, Samrudhdhi B Rangrej, Vladan Petrovic, and Tal Hassner. Few-shot learning with noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9089–9098, 2022.\n\nMoshe Lichtenstein, Prasanna Sattigeri, Rogerio Feris, Raja Giryes, and Leonid Karlinsky. Tafssl: Task-adaptive feature sub-space learning for few-shot classification. In European Conference on Computer Vision, pp. 522–539, 2020.\n\nJinlu Liu, Liang Song, and Yongqiang Qin. Prototype rectification for few-shot learning. In Euro-\n\npean Conference on Computer Vision, pp. 741–756, 2020.\n\nYang Liu, Weifeng Zhang, Chao Xiang, Tu Zheng, Deng Cai, and Xiaofei He. Learning to affiliate: Mutual centralized learning for few-shot classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14411–14420, 2022.\n\nYuning Lu, Liangjian Wen, Jianzhuang Liu, Yajing Liu, and Xinmei Tian. Self-supervision can be\n\na good few-shot learner. arXiv preprint arXiv:2207.09176, 2022.\n\nPuneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank Singh, Balaji Krishnamurthy, and Vineeth N Balasubramanian. Charting the right manifold: Manifold mixup for few-shot learning. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 2218– 2227, 2020.\n\nAlex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv\n\npreprint arXiv:1803.02999, 2018.\n\nBoris Oreshkin, Pau Rodr ́ıguez L ́opez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning. Advances in Neural Information Processing Systems, 31: 721–731, 2018.\n\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. International\n\nConference on Learning Representations, 2017.\n\nMengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. International Conference on Learning Representations, 2018.\n\nPau Rodr ́ıguez, Issam Laradji, Alexandre Drouin, and Alexandre Lacoste. Embedding propagation: Smoother manifold for few-shot classification. In European Conference on Computer Vision, pp. 121–138. Springer, 2020.\n\nXi Shen, Yang Xiao, Shell Xu Hu, Othman Sbai, and Mathieu Aubry. Re-ranking for image retrieval and transductive few-shot classification. Advances in Neural Information Processing Systems, 34: 25932–25943, 2021.\n\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Ad-\n\nvances in Neural Information Processing Systems, 30:4077–4087, 2017.\n\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1199–1208, 2018.\n\nYonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking few-shot image classification: a good embedding is all you need? In European Conference on Computer Vision, pp. 266–282, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, and Daan Wierstra. Matching networks for one\n\nshot learning. Advances in Neural Information Processing Systems, 29:3637–3645, 2016.\n\nYikai Wang, Chengming Xu, Chen Liu, Li Zhang, and Yanwei Fu. Instance credibility inference for few-shot learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12836–12845, 2020.\n\nYikai Wang, Li Zhang, Yuan Yao, and Yanwei Fu. How to trust unlabeled data instance credibility inference for few-shot learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n\nSanford Weisberg. Yeo-johnson power transformations. Department of Applied Statistics, University\n\nof Minnesota. Retrieved June, 1:2003, 2001.\n\nJiangtao Xie, Fei Long, Jiaming Lv, Qilong Wang, and Peihua Li. Joint distribution matters: Deep brownian distance covariance for few-shot classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7972–7981, 2022.\n\nShuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. Inter-\n\nnational Conference on Learning Representations, 2021.\n\nHuaxiu Yao, Yu Wang, Ying Wei, Peilin Zhao, Mehrdad Mahdavi, Defu Lian, and Chelsea Finn. Meta-learning with an adaptive task scheduler. Advances in Neural Information Processing Systems, 34:7497–7509, 2021.\n\nBaoquan Zhang, Xutao Li, Yunming Ye, Zhichao Huang, and Lisai Zhang. Prototype completion with primitive knowledge for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3754–3762, 2021.\n\nChi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classification with differentiable earth mover’s distance and structured classifiers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12203–12213, 2020.\n\nXueting Zhang, Flood Sung, Yuting Qiang, Yongxin Yang, and Timothy M Hospedales. Deep comparison: Relation columns for few-shot learning. arXiv preprint arXiv:1811.07100, 2018.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA EFFECT OF YEO-JOHNSON TRANSFORMATION\n\nFigure 5 shows the 5-way 1-shot accuracy when choosing different λ for the Yeo-Johnson transform in Equation 1. It can be found λ equals 0.5 is the optimum choice, and different values have a significant impact on the classification accuracy. With the Yeo-Johnson transformation, the distribution of features becomes more aligned with the calibrated Gaussian distribution, which favors the classifier that is trained on features from the calibrated distribution.\n\nFigure 5: Accuracy of different values of λ on miniImageNet.\n\nThe query features before and after the Yeo-Johnson transformation are shown in Figure 6. Different colors represent categories. It is observed that the distribution before transformation is more skewed. The distribution after Yeo-Johnson transformation can very well satisfy the Gaussian assumption. It provides a powerful means of reducing skewness.\n\n(a) Before transformation\n\n(b) After transformation\n\nFigure 6: Feature transformation.\n\nB COMPARISON OF COMPUTATION COST\n\nTable 5 shows a detailed analysis of different modules in our method and classification results on miniImageNet. Compared with the baseline of PN, FLOPs increased by 16M. The main reason is that the attention mechanism introduces some attention parameters. The operation of fusion almost without additional calculations. The parameter of ours has increased by 410.9 K.\n\nC COMPARISON OF DIFFERENT BACKBONES\n\nIn order to explore the influence of feature embedding vectors, the depth of the backbone network is changed and the same settings were used for the three models. It can be seen from Table 6 that the\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Comparison of the FLOPs, Params, and Accuracy on miniImageNet.\n\n5-way 1-shot\n\nFLOPs\n\nParams\n\nAccuracy\n\nBackbone (WRN-28-10) PN AINet FP AINet(OURS)\n\n36.19 G + 0\n\n36.47 M + 0\n\n- 61.47 ± 0.66 + 16.0 M + 410.9 K 63.88 ± 0.66 + 16.0 M + 410.9 K 72.13 ± 0.73\n\nminiImageNet has achieved the best results on the WRN-28-10 backbone network. The classification results are constantly improving as the number of network layers increases. In few-shot learning, the feature embedding vector is the key factor affecting the classification results. A better backbone network can bring better test performance in few-shot learning.\n\nTable 6: Accuracy (%) on miniImageNet with 95% confidence intervals of different backbone.\n\nminiImageNet\n\nBackbone\n\n5-way 1-shot\n\n5-way 5-shot\n\nResNet-10 ResNet-12 WRN-28-10\n\n60.61 ± 0.76 66.63 ± 0.76 72.13 ± 0.73\n\n74.80 ± 0.53 79.64 ± 0.51 84.29 ± 0.44\n\n14",
    "reference": "# Summary Of The Paper\n\nThe paper propse a method to assign scores to support samples and use Gaussian-based fusion algorithm to aggregate prototypes, in order to imigate the effect of the noise samples. in few-shot learning.\n\n# Strength And Weaknesses\n\nStrength:\n(1) The issue of noise labled data in the few-shot learning is very important and necessary to research further.\n(2) The method of FR_AINet which can generate a more representative prototype makes sense and achieve a higher performance.\n\nWeakness:\n(1) The similar idea has already been adopted in recent works, such as \n(2) The computation cost should be reported in the experimental results.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe quality of the work is mediate, and the clarity of the paper is satisfied, and the originality of the work is incremental.\n\n# Summary Of The Review\n\nBased on the limited originality of the work, I recommend to weakly reject the paper.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nNone."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nREVISITING HIGHER-ORDER GRADIENT METHODS FOR MULTI-AGENT REINFORCEMENT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThis paper revisits Higher-Order Gradient (HOG) methods for Multi-Agent Reinforcement Learning (MARL). HOG methods are algorithms in which agents use higher-order gradient information to account for other agents’ anticipated learning, and are shown to improve coordination in games with self-interested agents. So far, however, HOG methods are only applied to games with low-dimensional state spaces due to inefficient computation and preservation of higher-order gradient information. In this work, we solve these limitations and propose a HOG framework that can be applied to games with higher-dimensional state spaces. Moreover, we show that current HOG methods, when applied to games with common-interested agents, i.e., team games, can lead to miscoordination among the agents. To solve this, we propose Hierarchical Reasoning (HR) to improve coordination in team games, and we experimentally show that our proposed HR significantly outperforms state-of-the-art methods in standard multi-agent games. With our contributions, we greatly improve the applicability of HOG methods for MARL. For reproducibility, the code used for our work will be shared after the reviewing process.\n\n1\n\nINTRODUCTION\n\nIn multi-agent systems, the paradigm of agents’ reasoning about other agents has been explored and researched extensively (Goodie et al., 2012; Liu & Lakemeyer, 2021). Recently, this paradigm is being studied in the subfield of Multi-Agent Reinforcement Learning (MARL) (Wen et al., 2019; 2020; Konan et al., 2022). Generally speaking, MARL deals with several agents simultaneously learning and interacting in an environment. In the context of MARL, reasoning can be interpreted as accounting for the anticipated learning of other agents (Zhang & Lesser, 2010). As MARL uses gradient-based optimization, learning anticipation naturally leads to the usage of higher-order gradient information (Letcher et al., 2019). The so-called Higher-Order Gradient (HOG) methods use this extra gradient information to predict and, in some cases, shape the learning of other agents (Letcher et al., 2019). The importance of prediction and shaping has been frequently shown for various games, such as the Iterated Prisoner’s Dilemma (IPD), where shaping ensures cooperation among the agents (Foerster et al., 2018a). However, current HOG methods have clear limitations, as they can only work for specific types of games, and become inefficient when the dimensionality of the game increases. In this paper, we explore these limitations and propose a framework that can extend the application scope of HOG methods to a broader range of problem settings in MARL.\n\nThe vast majority of existing HOG methods focus only on games with low-dimensional state spaces, e.g., matrix games (Foerster et al., 2018a;b; Willi et al., 2022). There are two challenges that limit HOG methods from being applied to games with high-dimensional state spaces: inefficient computation and preservation of higher-order gradient information. Specifically, current implementations of HOG methods require multiple data sampling stages to compute higher-order gradient information (Foerster et al., 2018b). Moreover, the higher-order gradient information is applied and, more importantly, preserved in the policy network’s parameter space. As a result, existing HOG methods become very inefficient when applied to games that have high-dimensional state spaces, and therefore require high-dimensional parameter spaces. In this paper, to solve this, we propose an HOG framework where the higher-order gradient information are computed and preserved more efficiently. By comparing our proposed framework to existing HOG methods in well-controlled studies, we demonstrate that the overall performance and efficiency of our proposed framework stay\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nconsistent with increased dimensionality, unlike for existing HOG methods, where they get drastically worse.\n\nIn addition to dimensionality limitations, the generalizability of HOG methods to various types of games is questionable. Originally, HOG methods are proposed to improve cooperation in games with self-interested agents (Zhang & Lesser, 2010; Foerster et al., 2018a). So far, however, it is unclear how HOG methods perform when agents are fully cooperative, i.e., for common-interested agents in team games. We demonstrate that existing HOG methods have the tendency to lead to miscoordination among common-interested agents, causing a sub-optimal overall reward. To solve this, and improve the applicability of HOG methods to team games, we propose Hierarchical Reasoning (HR), a new HOG methodology explicitly developed for improving coordination in games with common-interested agents. Below, we summarize our contributions.\n\n• We propose HOG-MADDPG, a framework to make existing HOG methodologies, e.g., LA and LOLA, applicable to games with higher-dimensional state spaces by solving the limitations in computation and preservation of higher-order gradient information. With our framework, we develop two novel HOG methods, LA-MADDPG and LOLA-MADDPG, which apply the principles of LA and LOLA, respectively.\n\n• We demonstrate theoretically, in a two-agent two-action coordination game, and empirically, in a two-agent three-action coordination game, that the existing HOG methodologies can suffer from miscoordination among common-interested agents. To solve this, we propose the HR methodology and show, theoretically and empirically, that it overcomes miscoordination in the coordination games.\n\n• We apply the HR principle to our HOG-MADDPG framework and develop HR-MADDPG, a HOG method for common-interested agents. We show that HR-MADDPG outperforms the existing state-of-the-art methods on standard multi-agent games.\n\n2 RELATED WORKS\n\nWhen direct communication among agents is not possible, the standard tool for MARL agents to apply reasoning is Agents Modeling Agents (AMA) (Albrecht & Stone, 2018). Although agents traditionally use AMA to only predict the behavior of others (He et al., 2016; Hong et al., 2018), recent studies have extended AMA to further consider multiple levels of reasoning over the predicted behaviors (Wen et al., 2019; 2020). However, these approaches do not explicitly account for the other agents’ anticipated learning, which has shown to be beneficial in games where interaction among self-interested agents naturally leads to worst-case outcomes (Foerster et al., 2018a).\n\nHOG methods, on the other hand, are a range of methods that use higher-order gradient information to predict and, in some cases, shape the anticipated learning of other agents directly. This includes Learning with Opponent-Learning Awareness (LOLA), proposed to shape opponents for better coordination in Iterated Prisoner’s Dilemma (IPD) by Foerster et al. (2018a), Look-Ahead (LA), proposed to guarantee convergence in cyclic games by Zhang & Lesser (2010), Stable Opponent Shaping (SOS), developed by Letcher et al. (2019) as an interpolation between LOLA and LA to inherit the benefits of both, and other methods such as Consensus Optimization (CO) and Symplectic Gradient Adjustment (SGA), that are proposed to improve cooperation by Bertsekas (2014) and Balduzzi et al. (2018), respectively. However, as we explain in Section 4, these methods have only been applied to simple games due to the challenges in computation and preservation of higher-order gradient information. Furthermore, the impact of current HOG methods on coordination among common-interested agents has not yet been fully investigated. Current investigations are limited to convergence and non-convergence to stable and unstable fixed points in differential games, respectively (Letcher et al., 2019). However, we demonstrate in Section 5.1 that in the case of a two-agent, two-action coordination game with unstable fixed points, HOG methods can converge to miscoordination points. The focus of this work is to extend current HOG methodology so that it can be used for games with higher-dimensional state spaces and common-interested agents.\n\n3 BACKGROUND\n\nWe formulate the MARL setup as a Markov Game (MG) (Littman, 1994). An MG is a tuple (N , S, {Ai}i∈N , {Ri}i∈N , T , ρ, γ), where N is the set of agents (|N | = n), S is the set of states,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nand Ai is the set of possible actions for agent i ∈ N . Agent i chooses its action ai ∈ Ai through the policy network πθi : S × Ai → [0, 1] parameterized by θi conditioning on the given state s ∈ S. Given the actions of all agents, each agent i obtains a reward ri according to its reward function Ri : S × A1 × ... × An → R. Given an initial state, the next state is produced according to the state transition function T : S × A1 × ... × An → S. We denote an episode of horizon T as τ = ({s0, a0 n, r0 n }), and the discounted return for l=t γl−tri where γ is a predefined diseach agent i at time step t ≤ T is defined by Gt count factor. The expected return given the agents’ policy parameters approximates the state value function for each agent Vi(s, θ1, ..., θn) = E[Gt i(τ |st = s)]. Each agent i aims to maximize the expected return given the distribution of the initial state ρ(s), denoted by the performance objective Ji = Eρ(s)Vi(s, θ1, ..., θn). A na ̈ıve agent updates its policy parameters in the direction of the objective’s gradient: ∇θiJi = Eρ(s)∇θiVi(s, θ1, ..., θn).\n\nn}, ..., {sT , aT\n\ni(τ ) = (cid:80)T\n\n1 , ..., aT\n\n1 , ..., rT\n\n1, ..., a0\n\n1, ..., r0\n\nn , rT\n\nLearning With Opponent-Learning Awareness (LOLA). Unlike na ̈ıve agents, LOLA agents modify their learning objectives by differentiating through the anticipated learning steps of the opponents (Foerster et al., 2018a). Given n = 2 for simplicity, a first-order LOLA agent assumes a na ̈ıve opponent and optimizes V LOLA (s, θ1, θ2 + ∆θ2) where ∆θ2 = η∇θ2V2(s, θ1, θ2) and η is the prediction length. Using first-order Taylor expansion, and by differentiating with respect to θ1, the gradient adjustment for the first LOLA agent (Foerster et al., 2018a) is given by\n\n1\n\n∇θ1V LOLA\n\n1\n\n(s, θ1, θ2 + ∆θ2) ≈ ∇θ1 V1 + (∇θ2θ1 V1)⊺∆θ2 + (∇θ1 ∆θ2)⊺∇θ2V1 ,\n(cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) shaping\n\n(1)\n\nwhere V1 = V1(s, θ1, θ2). The rightmost term in the LOLA update allows for active shaping of the opponent’s learning. This term has been proven effective in enforcing cooperation in various games, including IPD (Foerster et al., 2018a;b). The LOLA update can be further extended to non-na ̈ıve opponents, resulting in higher-order LOLA agents (Foerster et al., 2018a; Willi et al., 2022).\n\nLook Ahead (LA). LA agents assume that the opponents’ learning steps cannot be influenced, i.e., cannot be shaped (Zhang & Lesser, 2010; Letcher et al., 2019). In other words, agent 1 assumes that the prediction step, ∆θ2, is independent of the current optimization, i.e., ∇θ1∆θ2 = 0. Therefore, the shaping term disappears and the gradient adjustment for the first LA agent will be\n\n∇θ1V LA\n\n1\n\n(s, θ1, θ2 + ∆θ2) ≈ ∇θ1 V1 + (∇θ2θ1 V1)⊺∆θ2.\n\n(2)\n\n4 A HOG FRAMEWORK FOR HIGH-DIMENSIONAL STATE SPACES\n\nExisting HOG methods like LOLA and LA are only applied to games with low-dimensional state spaces, e.g., matrix games (Zhang & Lesser, 2010; Foerster et al., 2018b;a; Letcher et al., 2019; Willi et al., 2022). When applied to games with higher-dimensional state spaces, they become very inefficient, due to the way the higher-order gradient information is computed and preserved. In this section, we analyze these problems, and we propose a framework that makes HOG practical for application to games with high-dimensional state spaces.\n\n4.1 LIMITATIONS OF EXISTING HOG APPROACHES\n\nComputation of higher-order gradient. Existing HOG methods are implemented in the stochastic policy gradient framework, and optimize non-differentiable objectives. Furthermore, the learning step for one agent in the standard stochastic policy gradient theorem is independent of other agents’ parameters. Therefore, higher-order mixed partial derivatives (among multiple agents) cannot be easily computed. Foerster et al. (2018b) proposed an infinitely differentiable Monte Carlo estimator, referred to as DiCE, to correctly optimize the stochastic objectives with any order of gradients. Similarly to meta-learning frameworks, the agents reason about and predict the learning process of the opponents using inner learning loops and update their parameters in outer learning loops. However, each learning loop for each agent requires a sampling stage which is very inefficient for high-order reasoning and games with higher-dimensional state spaces, i.e., beyond matrix games.\n\nPreservation of higher-order gradient information. HOG methods should constantly compute and update the higher-order gradient values and computation graphs that keep track of how the\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ngradients should flow. In the implementation of existing HOG methods, the higher-order gradient information is computed and preserved in the parameter spaces of the agents’ policy networks (Foerster et al., 2018a;b). As a result, the agents should either have access to other agents’ exact parameters or infer other agents’ parameters from state-action trajectories (Foerster et al., 2018a). In many game settings, these parameters are obscured. Moreover, when the dimensionality of state spaces increases, e.g., when having images as input, the dimensionality of the parameter spaces increases as well, making the parameter inference problem computationally expensive. Furthermore, computing and storing the higher-order gradient information in high-dimensional parameter spaces is inefficient, whether the parameters are inferred or exact.\n\n4.2 HOG-MADDPG\n\nTo efficiently compute any-order mixed partial derivatives, we need to optimize differentiable objectives, which are directly dependent on all agents’ parameters and can be efficiently estimated. The only platform that meets the above requirements and can deal with both discrete and continuous action spaces is Multi-Agent Deep Deterministic Policy Gradient (MADDPG) (Lowe et al., 2017). In this platform, decentralized policies are trained to optimize centralized, differentiable objectives, i.e., state-action value functions, that are estimated efficiently from trajectories sampled from a distinct behavior policy, i.e., the off-policy approach. Therefore, we propose to build our HOG framework on top of the MADDPG platform. Similarly to MADDPG, we follow the Centralized Training and Decentralized Execution (CTDE) setting in our work. However, differently from MADDPG, we better exploit the available information in CTDE by accounting for the agents’ anticipated learning.\n\nThe only unsolved problem is the preservation of higher-order gradient information, as the centralized learning of MADDPG does not grant access to agents’ parameters. To solve this, we propose to project the anticipated gradient information from the policies’ parameter spaces to the action spaces. This way, 1) we avoid additional constraints above the centralized state-action value functions where the agents have access to all actions, and 2) we improve efficiency as the action spaces have significantly lower dimensionality than the policies’ parameter spaces. In Appendix B we theoretically analyze the influence of the proposed projection concept on the overall performance (Appendix B.1), and the time complexity of gradient anticipation (Appendix B.2). In the following sections, we apply our proposed framework to two HOG methods and explain the details of their update rules for policy parameters. For all proposed methods in the HOG-MADDPG frameworks, the centralized state-action value functions are updated in a way identical to MADDPG (Lowe et al., 2017).\n\n4.2.1 LA-MADDPG\n\nIn the MADDPG platform, a deterministic policy μθi for agent i is defined as μθi : S → Ai, parameterized by θi. By denoting a centralized state-action function for each agent i as Qi(s, a1, ..., an) = E[Gt i = ai∀i ∈ N )], the gradient of the MADDPG performance objective Ji with respect to θi can be approximated as:\n\ni(τ |st = s, at\n\n∇θiJi ≈ Eρβ (s,ˆa)∇θiQi(s, ˆa1, ..., ai, ..., ˆan)|ai=μθi (s),\n\n(3)\n\nwhere ρβ(s, ˆa) is the state-action distribution of the behavior policy and ˆa = {ˆai∀i ∈ N } are the actions sampled from the behavior policy during the exploration stage. Given n = 2 and a na ̈ıve opponent for simplicity, the gradient adjustment for parameters of the LA-MADDPG agent (θ1) is computed by accounting for the anticipated policy parameters of the opponent, i.e., ˆθ2 + ∆ˆθ2(s):\n\n∇θ1J LA\n\n1 ≈ Eρβ (s)∇θ1 Q1(s, a1, ̃a2)|a1=μθ1 (s), ̃a2=μ ˆθ2+∆ ˆθ2\n\n(s),\n\n(4)\n\n(s), and ˆθ1 and ˆθ2 are the behavior policy paramwhere ∆ˆθ2 = η∇ˆθ2 eters. As the agents cannot have access to these parameters, we propose to project the anticipated gradients to the action space (see Appendix A.1):\n\nQ2(s, ˆa1, ˆa2)|ˆa1=μ ˆθ1\n\n(s),ˆa2=μ ˆθ2\n\n∇θ1J LA\n\n1 ≈ Eρβ (s,ˆa)∇θ1μθ1(s)∇a1Q1(s, a1, ˆa2 + ∆ˆa2)|a1=μθ1 (s),\n\n(5)\n\nwhere ∆ˆa2 = ˆη∇ˆa2 Q(s, ˆa1, ˆa2) and ˆη is the projected prediction length (see Alg. 1).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nMethod\n\nLA-DiCE LA-MADDPG (ours)\n\nIRG (DtE ↓)\n\n0.09±0.07 0.03±0.02\n\nMethod\n\nIPD (AER ↑)\n\nLOLA-DiCE LOLA-MADDPG (ours)\n\n-2.16±0.12 -2.08±0.02\n\nTable 1: Comparisons of the LA and LOLA methods in the frameworks of DiCE and our proposed HOG-MADDPG.\n\nFigure 1: Learning curves in two matrix games. Left: Iterated Rotational Game in terms of the distance to the equilibrium point (↓). Right: Iterated Prisoner’s Dilemma in terms of the normalized averages return (↑).\n\n4.2.2 LOLA-MADDPG\n\nIn the standard MADDPG, it is assumed that the opponents’ actions are fixed during the optimization steps for the agents. This assumption is problematic for LOLA agents that require shaping on the opponents’ learning steps. To make these dependencies possible, we employ the Centralized Policy Gradient (CPG) (Peng et al., 2021) to derive the learning step of the LOLA agents. In CPGMADDPG, the gradient update in Eq. (3) is modified to:\n\n∇θi Ji ≈ Eρβ (s)∇θiQi(s, a1, ..., an)|ai=μθi (s) ∀i∈N . Again, we propose to project the anticipated gradient information to the action space. Given n = 2, the first-order LOLA-MADDPG agent updates the policy parameters through (see Appendix A.2):\n\n(6)\n\n∇θ1J LOLA\n\n1\n\n≈ Eρβ (s)∇θ1 μθ1 (s)∇a1Q1(s, a1, a2 + ∆a2)|a1=μθ1 (s),a1=μθ2 (s),\n\n(7)\n\nwhere ∆a2 = ˆη∇a2Q(s, a1, a2), which, unlike for LA-MADDPG, is a function of a1 (see Alg. 2).\n\n4.3 EXPERIMENTS\n\nIn this section, we first verify our proposed HOG-MADDPG methods on simple matrix games, to show how they work in the same situations as the original HOG methods. Second, we apply our proposed methods to games with high-dimensional state spaces, which is the envisioned use case for our methods, and evaluate the performance and efficiency. As the primary baselines, we apply HOG methods on the DiCE framework (Foerster et al., 2018b), i.e., LA-DiCE and LOLA-DiCE. We further compare our proposed methods with the standard MADDPG configured with three stateof-the-art update rules: 1) standard update rule (Lowe et al., 2017), referred to as MADDPG, 2) CPG update rule (Peng et al., 2021), referred to as CPG-MADDPG, and 3) Probabilistic Recursive Reasoning (PR2) update rule (Wen et al., 2019), referred to as PR2-MADDPG. For a fair comparison, we have employed identical architectures with the same number of policy and value function parameters for all the baseline (see Appendix D).\n\n4.3.1 MATRIX GAMES\n\nWe evaluate the methods on the following commonly-used, two-agent matrix games: 1) Iterated Rotational Game (IRG) (Zhang & Lesser, 2010), a one-state game with a 1-Dimensional (1-D) continuous action space between 0 and 1 representing the probability of taking two discrete actions, and 2) Iterated Prisoner’s Dilemma (IPD) (Foerster et al., 2018a), a five-state game with two discrete actions and T = 150. The games are developed to highlight the strengths of specific HOG methods (IRG for LA-based methods and IPD for LOLA). Further details about these games are provided in Appendix D.1. We evaluate the performances of methods based on the Distance to Equilibrium (DtE) in IRG (the equilibrium point in IRG is reached when a1 = a2 = 0.5) and the Averaged Episode Reward (AER) in IPD. In Figure 1, we depict the learning curves for our methods and other, state-of-the-art MADDPG-based algorithms. From this figure, we find that our HOG methods are the only MADDPG-based networks that can effectively solve these games, with LA-MADDPG for IRG and LOLA-MADDPG for IPD. This highlights the importance of using higher-order gradient information. To further show its effectiveness, we compare with current DiCE-based HOG methods (Foerster et al., 2018b) which are designed for these matrix games, in Table 1, and we find that we even achieve slightly better results.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Exit-Room level one.\n\n(b) Exit-Room level two.\n\n(c) Exit-Room level three.\n\nFigure 2: Learning curves in different complexity levels of the exit-room game in terms of the normalized average return. Higher values are better.\n\n↑NAER in Exit-Room game\n\n↓NTTI in Exit-Room game\n\nMethods\n\nl = 1\n\nl = 2\n\nl = 3\n\nNa ̈ıve\n\n1st-order\n\n2nd-order\n\n3rd-order\n\n4th-order\n\nLOLA-DiCE LOLA-MADDPG (ours)\n\n0.91±0.04 1.00±0.00\n\n0.68±0.06 0.99±0.01\n\n0.56±0.12 0.93±0.03\n\n1 1\n\n2.39 1.03\n\n3.74 1.05\n\n5.12 1.08\n\n6.41 1.12\n\nTable 2: Comparisons of DiCE with our proposed HOG-MADDPG in the Exit-Room game, in terms of performance (normalized average return in different game levels) and efficiency (training time per iteration in different reasoning levels).\n\n4.3.2 MULTI-LEVEL EXIT-ROOM GAME\n\nInspired by Vinitsky et al. (2019), we propose an Exit-Room game with three levels of complexity (see Figure 3). The Exit-Room game is a grid-world variant of the IPD, with two agents (blue and red), and is specifically developed to highlight the strength of LOLA. The agents should cooperate and move towards the exit doors on the right. However, they are tempted to exit the left doors, and in some cases, not exiting at all. In level 1, the agents have three possible actions (move-left, move-right, or do nothing), and in levels 2 and 3, they have additional move-up and move-down actions. Additionally, in level 3, the door positions are randomly located, resulting in more complex interactions among the agents. For more details about the game, see Appendix D.2.\n\nFigure 2 compares the learning curves of LOLAMADDPG with the state-of-the-art, MADDPG-based methods in terms of Normalized Average Episode Reward (NAER) which is the AER value that is normalized between the highest and lowest episode rewards in each game level. In Figure 2, we can clearly see that our LOLA-MADDPG significantly outperforms the other methods, similarly as for the matrix games. To highlight the benefits of our proposed method with respect to existing HOG methods, we compare our LOLA-MADDPG with LOLA-DiCE in terms of performance (by comparing NAER) and training efficiency, in Table 2. For training efficiency, we use the average Training Time per Iteration (TTI) for various opponents’ reasoning levels. For a fair comparison, we report Normalized TTI (NTTI) for both methods, which are the normalized TTI values with respect to na ̈ıve (zero-order) version of each method. Observing Table 2, it is apparent that LOLA-DiCE fails to acquire the highest rewards, particularly for the second and third levels of the game, where complexity is increased. Moreover, our proposed LOLAMADDPG performs better in all levels of the game in terms of NAER, and scaling from na ̈ıve to higher-order opponents is significantly more efficient for LOLA-MADDPG than LOLA-DiCE. This emphasizes that we have overcome the limitations of HOG methods described in Section 4.1.\n\nFigure 3: State observation in the ExitRoom game, level one (left), level two (middle), and level three (right).\n\n5 A HOG METHODOLOGY FOR COMMON-INTERESTED AGENTS\n\nCurrent HOG methods are proposed to enforce coordination in games with self-interested agents. In many applications, however, agents should cooperate to increase a common reward function, i.e., as a team game. As multiple agents should interact and cooperate, anticipating the learning of other agents, which is the core idea of HOG methods, has the potential to work well in these types of games too. In this section, we first show that standard HOG methods do not work well for team games, because they suffer from miscoordination. Subsequently, we propose a method to overcome this limitation, and evaluate it for several different games.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n5.1 MISCOORDINATION ANALYSIS IN COOPERATIVE SCENARIOS\n\nTo investigate the effectiveness of HOG methods on the coordination among common-interested agents, we consider a two-agent, two-action coordination game (Claus & Boutilier, 1998) with an (cid:3), added miscoordination penalty. The game is defined by a common reward matrix R = (cid:2) a k where a > 0 is coordination reward and k ≤ 0 is the miscoordination penalty. We further define g = a − k > 0 as the miscoordination regret. The agents are parameterized by θ1 ∈ [0, 1] and θ2 ∈ [0, 1], denoting the probability of choosing the first action by agent one and two, respectively. Similarly to Singh et al. (2000); Zhang & Lesser (2010), we analyze the dynamics of θ1 and θ2 for LOLA, LA, and na ̈ıve agents to investigate the coordination behaviors.\n\nk a\n\nTheorem 1 If, in the previously defined two-person, two-action coordination game with a miscoordination regret g, the agents are updated following the LA method and a fixed prediction length η, then they are subject to miscoordination for g > 1/2η. If the agents are updated following the LOLA method and a fixed prediction length η, then they are subject to miscoordination for g > 1/4η. If the agents follow the na ̈ıve updates, then they are never subject to miscoordination for any value of g. Proof – See Appendix C.1.\n\nA closer inspection of the HOG methods reveals two important aspects about their fundamental ideas. First, anticipating other agents’ learning is only effective when it is close to their true future learning. Existing HOG methods assume a reasoning level for other agents. If this assumption is wrong, it can negatively affect the coordination among the cooperative agents. Second, the idea of shaping other agents’ learning can be misleading if the other agents do not follow, making agents more likely to suffer from miscoordination. We hypothesize that by addressing these two aspects of current HOG methods, miscoordination among agents can be avoided.\n\n5.2 HIERARCHICAL REASONING\n\nBased on our hypothesis, we propose Hierarchical Reasoning (HR), an HOG methodology especially designed for cooperative agents. In contrast to standard HOG methods, HR determines a hierarchy among the agents in each training iteration, which determines the reasoning orders of the agents. Concretely, if n = 2, and we assume that the first agent is the leader and the second agent is the follower, the gradient adjustment for the leader is similar to first-order LOLA agents, and is:\n\n∇θ1V Leader(s, θ1, θ2 + ∆θ2) ≈ ∇θ1V + (∇θ2θ1V )⊺∆θ2 + (∇θ1 ∆θ2)⊺∇θ2V, where V = V (s, θ1, θ2) is the common value function, and ∆θ2 = η∇θ2V . However, unlike LOLA agents, the leader knows the reasoning level of the follower, which is a na ̈ıve agent. The plan for the leader is to change its parameters ̄θ1 = θ1 + ∇θ1V Leader(s, θ1, θ2 + ∆θ2) in such a way that an optimal increase in the common value is achieved, after its new parameters are taken into account by the follower. Therefore, the follower must follow the plan and adjust its parameters through\n\n(8)\n\n∇θ2 V Follower(s, ̄θ1, θ2) ≈ ∇θ2V + (∇θ1θ2V )⊺∇θ1V Leader(s, θ1, θ2 + ∆θ2),\n\n(9)\n\nTheorem 2 If, in the previously defined two-person, two-action coordination game with a miscoordination regret g, the agents are updated following the HR methodology, then they are not subject to miscoordination for any value of g. Proof – See Appendix C.2.\n\nWith this, we have shown that HR naturally avoids miscoordination and therefore, our hypothesis is correct. However, the main goal is to demonstrate that HR improves coordination among commonto na ̈ıve learning, interested agents with respect which does not take into account higher-order gradients at all. If HR does not improve the coordination, there is no clear benefit over the na ̈ıve learners, as they also avoid miscoordination. To show the benefits of HR, we employ a standard two-agent, three-action coordination game (Claus & Boutilier, 1998). The game has a common reward matrix R = (cid:2) 10 0 k (cid:3), and we define g = 10 − k as the miscoordination regret. Each agent is parameterized with three parameters: θ1, θ2, and θ3 (θi > 0 ∀i ∈ {1, 2, 3} and (cid:80)3 i θi = 1), representing\n\nFigure 4: Converged results for various values of miscoordination regret.\n\n0 2 0 k 0 10\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: An example of the parameter update stages in HR-MADDPG, for a game with three common-interested agents m = 3, assigned to three hierarchy levels.\n\nthe probability of taking the actions a1, a2, and a3, respectively. The game has two global equilibrium points (if θ1 = 1 or θ3 = 1 for all agents), and one local equilibrium point (if θ2 = 1 for all agents). In Figure 4, we depict the converged results for this game for na ̈ıve, LA, LOLA, and HR agents, for various values of miscoordination regret g. The experiments are run 500 times until convergence, with random initializations. From 4, we find that both LA and LOLA agents are subject to miscoordination for high values of g, which is consistent with our findings for the two-action coordination game. However, the most interesting aspect of this experiment is that by increasing the value of g, the coordination among the na ̈ıve agents reduces, leading them to the local equilibrium point, whereas our HR agents consistently achieve the highest reward, independently of the miscoordination regret. This shows the benefit of HR agents over na ̈ıve agents.\n\n5.2.1 HR-MADDPG\n\nIn this section, we propose HR-MADDPG, an extension of HOG-MADDPG for games with common-interested agents and high-dimensional state spaces. We first define M ⊆ N , as a set of common-interested agents such that Ri = Rj ∀i, j ∈ M. Without the loss of generality, we consider M = N , i.e., team games with a common state-action value function Q(s, a1, ..., am).\n\nHierarchy level assignment. In the policy update step of HR-MADDPG, each agent is first assigned to one of m = |M| levels of hierarchy based on the amount of influence that it has on other agents, i.e., its shaping capacity, in each training iteration. The shaping capacity of the ith agent, fi, is the sum of l2-norms of the shaping term in Eq. 1 with respect to all other agents j:\n\n(cid:88)\n\nfi =\n\n(cid:13)(∇ai∆aj)⊺∇aj Q(a1, ..., am)(cid:13) (cid:13) (cid:13) ,\n\nj̸=i & ∈M\n\n(10)\n\nwhere ∆aj = ∇aj Q(a1, ..., am). In each hierarchy level, the assigned agent is a leader of the lower hierarchy levels and a follower of the higher ones, with two reasoning rules: 1) a leader knows the reasoning levels of the followers and is one level higher, and 2) a follower cannot reason about the leaders and only follows their shaping plans. As HR-MADDPG benefits from centralized learning, the only constraint for these reasoning rules remains the centralized state-action value function.\n\nParameter update. After the hierarchy level assignment, the agents update their policy parameters in m update stages, i.e., one for each agent, and in a top-down fashion: the agent in the highest hierarchy level updates its policy parameters first. In each update stage, the corresponding agent 1) reasons about the followers (if any) in a bottom-up fashion, i.e., it reasons about the agent in the lowest hierarchy level first, 2) updates its policy parameters, and 3) updates its action for the next update stage (if any). Figure 5 demonstrates an example with the update stages for three commoninterested agents 1, 2 and 3, that are assigned to hierarchy levels h1, h2, h3, where agent 3, assigned to h3 is the leader, etc. For the case of m agents, see the HR-MADDPG update rules in Alg. 3.\n\n5.3 EXPERIMENTS\n\nIn this section, we aim to demonstrate the advantages of our proposed HR-MADDPG for games with common-interested agents, compared to 1) LA-MADDPG and LOLA-MADDPG, and 2) state-ofthe-art methods: MADDPG (Lowe et al., 2017), CPG-MADDPG (Peng et al., 2021), and PR2MADDPG (Wen et al., 2019). For this purpose, we first develop the Particle Coordination game to assess the coordination capability of the methods carefully. Then, we compare the general performance of all the methods in standard multi-agent games (Lowe et al., 2017; Peng et al., 2021). See Appendix D.3 and D.4 for details on the experiments.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nMethods\n\nDDPG (LB) C-MADDPG (UB)\n\nMADDPG CPG-MADDPG PR2-MADDPG LA-MADDPG (ours) LOLA-MADDPG (ours) HR-MADDPG (ours)\n\n↑NAER in Particle Environment\n\n↑NAER in Mujoco Environment\n\nCooperative Navigation\n\nPhysical Deception\n\nPredator-Prey Half-Cheetah Walker Reacher\n\n0.00 1.00\n\n0.77 0.78 0.78 0.78 0.77 0.88\n\n0.00 1.00\n\n0.61 0.67 0.54 0.63 0.56 0.83\n\n0.00 1.00\n\n0.21 0.18 0.08 0.13 0.13 0.44\n\n0.00 1.00\n\n0.86 0.88 0.85 0.85 0.83 0.94\n\n0.00 1.00\n\n0.45 0.46 0.45 0.43 0.42 0.67\n\n0.00 1.00\n\n0.02 0.05 0.01 0.04 0.01 0.42\n\nTable 3: Comparisons of methods in terms of the Normalized Average Episode Reward (NAER) for common-interested agents. LB: Lower Bound. UB: Upper Bound.\n\n5.3.1 PARTICLE COORDINATION GAME\n\nOur proposed game is a variant of the Cooperative Navigation game (Lowe et al., 2017) with two agents and three landmarks. The agents should select and approach one of the landmarks, and the landmark closest to an agent is considered to be the selected landmark. If the agents select and approach the same landmark, they receive global or local optimal rewards based on the selected landmark. They will receive an assigned miscoordination penalty if In Figthey select and approach different landmarks. ure 6, we depict the learning curves for our method and other MADDPG-based algorithms. From this figure, it is clear that our HR-MADDPG is the only method that consistently converges to the global optimum of the game, which is consistent with our previous results coordination games. Further experiments regarding the sensitivity of HOG-MADDPG methods to the prediction length are provided in Appendix D.3.\n\nFigure 6: Learning curves in the Particle Coordination game. Higher values are better.\n\n5.3.2 STANDARD MULTI-AGENT GAMES\n\nWe evaluate the methods in three Particle environment games (Lowe et al., 2017): 1) Cooperative Navigation with three common-interested agents, 2) Physical Deception with two commoninterested and one self-interested agent, and 3) Predator-Prey with two common-interested (predator) and one self-interested (prey) agents. Furthermore, we compare the methods in three games within the multi-agent Mujoco environment (Peng et al., 2021): 1) two-agent Half-Cheetah, 2) two-agent Walker, and 3) two-agent Reacher. In the mixed environments (Physical Deception and PredatorPrey), we have employed the MADDPG method for the self-interested agents. We report the Normalized Average Episode Reward for the common-interested agents in Table 3, where the normalization is done between the single-agent variant of MADDPG (DDPG (Lillicrap et al., 2016)) and a fully centralized (in learning and execution) variant of MADDPG, referred to as C-MADDPG. In Table 3, we observe that our proposed HOG-MADDPG consistently and significantly outperforms all the state-of-the-art MADDPG-based methods. Again, these results confirm that our proposed HR-MADDPG improves coordination among common-interested agents, leading to better results.\n\n6 DISCUSSION\n\nIn this paper, we proposed the HOG-MADDPG framework to make HOG methods applicable to games with high-dimensional state spaces. As a result, the benefits of HOG can now be used in many more MARL problems. As a first case study, we investigated the applicability of current HOG methodologies to team games, and found that they suffer from miscoordination, which we then solved with our proposed HR methodology. Like this solution, there are numerous other possibilities for extending HOG-MADDPG, e.g., factorizing the centralized value functions, which is essential for many-agent games. With our work, we aim to spark such new ideas for HOG methods in MARL, and we provide the framework to realize them.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nStefano V Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive\n\nsurvey and open problems. Artificial Intelligence, 258:66–95, 2018.\n\nDavid Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. In International Conference on Machine Learning, pp. 354–363. PMLR, 2018.\n\nDimitri P Bertsekas. Constrained optimization and Lagrange multiplier methods. Academic press,\n\n2014.\n\nCaroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multia-\n\ngent systems. AAAI/IAAI, 1998(746-752):2, 1998.\n\nThomas Degris, Martha White, and Richard Sutton. Off-Policy Actor-Critic. In International Con-\n\nference on Machine Learning, 2012.\n\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network\n\nfunction approximation in reinforcement learning. Neural Networks, 107:3–11, 2018.\n\nJakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with Opponent-Learning Awareness. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 122–130, 2018a.\n\nJakob Foerster, Gregory Farquhar, Maruan Al-Shedivat, Tim Rockt ̈aschel, Eric Xing, and Shimon Whiteson. Dice: The infinitely differentiable monte carlo estimator. In International Conference on Machine Learning, pp. 1529–1538. PMLR, 2018b.\n\nAdam S Goodie, Prashant Doshi, and Diana L Young. Levels of theory-of-mind reasoning in com-\n\npetitive games. Journal of Behavioral Decision Making, 25(1):95–108, 2012.\n\nHe He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daum ́e III. Opponent modeling in deep reinforcement learning. In International conference on machine learning, pp. 1804–1813. PMLR, 2016.\n\nZhang-Wei Hong, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang, and Chun-Yi Lee. A Deep Policy Inference Q-Network for Multi-Agent Systems. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1388–1396, 2018.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical Reparametrization with Gumble-Softmax. In International Conference on Learning Representations (ICLR 2017). OpenReview. net, 2017.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nSachin Konan, Esmaeil Seraj, and Matthew Gombolay. Iterated Reasoning with Mutual Information in Cooperative and Byzantine Decentralized Teaming. arXiv preprint arXiv:2201.08484, 2022.\n\nAlistair Letcher, Jakob Foerster, David Balduzzi, Tim Rockt ̈aschel, and Shimon Whiteson. Stable Opponent Shaping in Differentiable Games. In International Conference on Learning Representations, 2019.\n\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR (Poster), 2016.\n\nRaymond Lister and James V Stone. An empirical study of the time complexity of various error In Proceedings of ICNN’95-International\n\nfunctions with conjugate gradient backpropagation. Conference on Neural Networks, volume 1, pp. 237–241. IEEE, 1995.\n\nMichael L Littman. Markov games as a framework for multi-agent reinforcement learning.\n\nIn\n\nMachine learning proceedings 1994, pp. 157–163. Elsevier, 1994.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDaxin Liu and Gerhard Lakemeyer. Reasoning about Beliefs and Meta-Beliefs by Regression in an Expressive Probabilistic Action Logic. In international joint conference on artificial intelligence. IJCAI, 2021.\n\nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-\n\ncritic for mixed cooperative-competitive environments. In NIPS, 2017.\n\nBei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin Boehmer, and Shimon Whiteson. FACMAC: Factored Multi-Agent Centralised Policy Gradients. NeurIPS, 2021.\n\nSatinder P Singh, Michael J Kearns, and Yishay Mansour. Nash Convergence of Gradient Dynamics\n\nin General-Sum Games. In UAI, pp. 541–548, 2000.\n\nEugene Vinitsky, Natasha Jaques, Joel Leibo, Antonio Castenada, and Edward Hughes. An open source implementation of sequential social dilemma games. https://github.com/ eugenevinitsky/sequential_social_dilemma_games/issues/182, 2019. GitHub repository.\n\nYing Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for In 7th International Conference on Learning Representa-\n\nmulti-agent reinforcement learning. tions, ICLR 2019, 2019.\n\nYing Wen, Yaodong Yang, and Jun Wang. Modelling Bounded Rationality in Multi-Agent Interac-\n\ntions by Generalized Recursive Reasoning. In IJCAI, 2020.\n\nTimon Willi, Alistair Hp Letcher, Johannes Treutlein, and Jakob Foerster. COLA: consistent learning with opponent-learning awareness. In International Conference on Machine Learning, pp. 23804–23831. PMLR, 2022.\n\nChongjie Zhang and Victor Lesser. Multi-agent learning with policy prediction. In Proceedings of\n\nthe AAAI Conference on Artificial Intelligence, volume 24, 2010.\n\nAPPENDIX\n\nA MORE DETAILS ON THE HOG-MADDPG METHODS\n\nA.1 LA-MADDPG\n\nIn the standard MADDPG, the performance objective is defined as:\n\nJi = Eρβ (s,ˆa)Qi(s, ˆa1, ..., ai, ..., ˆan, θ1, ..., θn)|ai=μθi (s), where ρβ(s, ˆa) is the state-action distribution of the behavior policy with ˆa = {ˆai∀i ∈ N }. The gradient of Ji with respect to θi can be approximated as:\n\n(11)\n\n∇θiJi ≈ Eρβ (s,ˆa)∇θiQi(s, ˆa1, ..., ai, ..., ˆan)|ai=μθi (s), where the direct dependencies of state-action value function to policy parameters can be dropped based on the proofs presented in Degris et al. (2012). Given n = 2 and a na ̈ıve opponent for simplicity, the gradient adjustment for parameters of the LA-MADDPG agent (θ1) is computed by accounting for the anticipated policy parameters of the opponent, i.e., ˆθ2 + ∆ˆθ2(s):\n\n(12)\n\n∇θ1J LA\n\n1 ≈ Eρβ (s)∇θ1 Q1(s, a1, ̃a2)|a1=μθ1 (s), ̃a2=μ ˆθ2+∆ ˆθ2\n\n(s),\n\n(13)\n\nwhere ∆ˆθ2 = η∇ˆθ2 (s), and ˆθ1 and ˆθ2 are the behavior policy parameters. As the agents cannot have access to these parameters, we propose to project the anticipated gradient information to the action space using first-order Taylor expansion:\n\nQ2(s, ˆa1, ˆa2)|ˆa1=μ ˆθ1\n\n(s),ˆa2=μ ˆθ2\n\n(s)\n\n ̃a2 = μˆθ2+∆ˆθ2 ≈ μˆθ2\n\n(s) + (∆ˆθ2)⊺∇ˆθ2\n\nμˆθ2\n\n(s)\n\n(14)\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: LA-MADDPG for a set of n self-interested agents (N ). i ∀i ∈ N\n\nInitialize μθi , Qi, μ′ for episode = 1 to max-num-episodes do\n\ni, and Q′\n\nReceive initial state s for t = 1 to max-episode-length do\n\nSelect action ai from μθi(s) and the exploration strategy ∀i ∈ N Execute actions a = {ai}∀i∈N and observe rewards r = {ri}∀i∈N and new state s′ Store the tuple (s, a, r, s′) in replay buffer D Set s = s′\n\nSample a random K tuples {(sk, ak, rk, s′k)}k∈{1,...,K} from D for agent i = 1 to n do i = rk\n\nSet yk Update state-action value function Qi by minimizing:\n\nh(s′k), for k ∈ {1, ..., K}\n\ni + γQ′\n\ni(s′k, a′\n\n1, ..., a′\n\nn)|a′\n\nh=μ′\n\nL =\n\n1 K\n\n(cid:88)\n\nk∈{1,...,K}\n\nend for for agent i = 1 to n do\n\nfor agent j = 1 to n do\n\n[(Qi(sk, ak\n\n1, ..., ak\n\nn) − yk\n\ni )2]\n\nProject the anticipated gradients if j = i then continue j = η ∂ Set ∆ak\n\nQj(sk, ak\n\n1, ..., ak\n\n∂ak j\n\nend for Update policy parameters θi via: ∇θi J LA\n\ni ≈\n\nn) for k ∈ {1, ..., K}\n\n1 K\n\n(cid:88)\n\n∇θiμθi(sk)\n\nk∈{1,...,K}\n\n∂ ∂ak i\n\nQi(sk, ak\n\n1 + ∆ak\n\n1, ..., ak\n\ni , ..., ak\n\nn + ∆ak\n\nn)|ak\n\ni =μθi (sk)\n\nend for Update Q′\n\ni and μ′\n\ni ∀i ∈ N\n\nend for\n\nend for\n\nGiven that:\n\nwe have:\n\n∆ˆθ2 = η∇ˆθ2\n\nQ2(s, ˆa1, ˆa2)\n\n= η∇ˆθ2\n\nμˆθ2\n\n(s)\n\n ̃a2 ≈ μˆθ2\n\n(s) +\n\n(cid:16)\n\nη∇ˆθ2\n\nμˆθ2\n\n(cid:16)\n\n∇ˆa2 Q2(s, ˆa1, ˆa2)|ˆa2=μ ˆθ2\n\n(s)\n\n(cid:17)⊺\n\n,\n\n⊺(cid:17)⊺\n\n∇ ˆθ2\n\nμˆθ2\n\n(s)\n\n(s) + ∇ˆa2 Q2(s, ˆa1, ˆa2)\n\nμˆθ2\n\n(s)\n\n∇ ˆθ2\n\nμˆθ2\n\n(s)\n\n(s) (∇ˆa2Q2(s, ˆa1, ˆa2)) (cid:17)⊺\n\n(cid:16)\n\nη∇ˆθ2 (cid:13) (cid:13) (cid:13)∇ˆθ2\n\nμˆθ2(s)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n= μˆθ2\n\n= μˆθ2 = μˆθ2\n\n(s) + ∇ˆa2Q2(s, ˆa1, ˆa2)η\n\n(s) + ∇ˆa2Q2(s, ˆa1, ˆa2)ˆη,\n\nwhere ∥.∥ is the l2-norm and we have defined the projected prediction length ˆη = η\n\nsince\n\n(cid:13) (cid:13) (cid:13)∇ˆθ2\n\nμˆθ2(s)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nis a positive number and independent of θ1. Therefore:\n\n ̃a2 ≈ μˆθ2\n\n(s) + ˆη∇ˆa2Q2(s, ˆa1, ˆa2)\n\n= ˆa2 + ∆ˆa2.\n\n12\n\n(15)\n\n(16)\n\n(cid:13) (cid:13) (cid:13)∇ˆθ2\n\nμˆθ2(s)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(17)\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2: LOLA-MADDPG for a set of n self-interested agents (N ).\n\nInitialize μθi , Qi, μ′ for episode = 1 to max-num-episodes do\n\ni, and Q′\n\ni ∀i ∈ N\n\nReceive initial state s for t = 1 to max-episode-length do\n\nSelect action ai from μθi(s) and the exploration strategy ∀i ∈ N Execute actions a = {ai}∀i∈N and observe rewards r = {ri}∀i∈N and new state s′ Store the tuple (s, a, r, s′) in replay buffer D Set s = s′\n\nSample a random K tuples {(sk, ak, rk, s′k)}k∈{1,...,K} from D for agent i = 1 to n do i = rk\n\nSet yk Update state-action value function Qi by minimizing:\n\nh(s′k), for k ∈ {1, ..., K}\n\ni + γQ′\n\ni(s′k, a′\n\n1, ..., a′\n\nn)|a′\n\nh=μ′\n\nL =\n\n1 K\n\n(cid:88)\n\nk∈{1,...,K}\n\n[(Qi(sk, ak\n\n1, ..., ak\n\nn) − yk\n\ni )2]\n\nend for Set ak for agent i = 1 to n do\n\ni = μθi(sk), for k ∈ {1, ..., K} and i ∈ N\n\nfor agent j = 1 to n do\n\nProject the anticipated gradients if j = i then continue j = η ∂ Set ∆ak\n\nQj(sk, ak\n\n1, ..., ak\n\n∂ak j\n\nend for Update policy parameters θi via: ∇θi J LA\n\ni ≈\n\nn) for k ∈ {1, ..., K}\n\n1 K\n\n(cid:88)\n\n∇θiμθi(sk)\n\nk∈{1,...,K}\n\n∂ ∂ak i\n\nQi(sk, ak\n\n1 + ∆ak\n\n1, ..., ak\n\ni , ..., ak\n\nn + ∆ak n)\n\nend for Update Q′\n\ni and μ′\n\ni ∀i ∈ N\n\nend for\n\nend for\n\nReplacing Eq. 17 in Eq. 13 yields Eq. 5:\n\n∇θ1J LA\n\n1 ≈ Eρβ (s,ˆa)∇θ1μθ1(s)∇a1Q1(s, a1, ˆa2 + ∆ˆa2)|a1=μθ1 (s).\n\n(18)\n\nIn the case of n agents, the agent i ∈ N first anticipates the gradient information of all agents j ∈ N in the action space as:\n\n∆ˆaj = ˆη∇ˆa2 Q2(s, ˆa1, ..., ˆan)\n\nThen, agent i updates its parameters θi by the following gradient adjustment:\n\n∇θiJ LA\n\ni ≈ Eρβ (s,ˆa)∇θiμθi(s)∇aiQi(s, ˆa1 + ∆ˆa1, ..., ai, ..., ˆan + ∆ˆan)|ai=μθi (s).\n\nPlease refer to Alg. 1 for more details on the LA-MADDPG optimization framework.\n\n(19)\n\n(20)\n\nA.2 LOLA-MADDPG\n\nIn CPG-MADDPG, the gradient update is:\n\n∇θiJi ≈ Eρβ (s)∇θiQi(s, a1, ..., an)|ai=μθi (s) ∀i∈N . Given n = 2 and a na ̈ıve opponent for simplicity, the gradient adjustment for parameters of the LOLA-MADDPG agent (θ1) is computed by accounting for the anticipated policy parameters of the\n\n(21)\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 3: HR-MADDPG for a set of m common-interested agents (M).\n\nInitialize μθi , Qi, μ′ for episode = 1 to max-num-episodes do\n\ni ∀i ∈ M\n\ni, and Q′\n\nReceive initial state s for t = 1 to max-episode-length do\n\nSelect action ai from μθi(s) and the exploration strategy ∀i ∈ M Execute actions a = {ai}∀i∈M and observe common reward r and new state s′ Store the tuple (s, a, r, s′) in replay buffer D Set s = s′\n\nSample a random K tuples {(sk, ak, rk, s′k)}k∈{1,...,K} from D Set yk = rk + γQ′(s′k, a′ Update state-action value function Q by minimizing:\n\nh(s′k), for k ∈ {1, ..., K}\n\n1, ..., a′\n\nm)|a′\n\nh=μ′\n\nL =\n\n1 K\n\n(cid:88)\n\nk∈{1,...,K}\n\n[(Q(sk, ak\n\n1, ..., ak\n\nm) − yk)2]\n\nAssign the agents into m hierarchy levels using Eq. 10 Set ak i = μθi(sk), for k ∈ {1, ..., K} and i ∈ M for agent i = m to 1 do\n\nfor agent j = 1 to i do\n\nProject the anticipated gradients Compute ∆ak if\n\nj = 1 & i ̸= m then ∆ak\n\nj , for k ∈ {1, ..., K}:\n\nelif j ̸= 1 & i = m then ∆ak\n\nelif j = 1 & i = m then ∆ak\n\n∂ak 1\n\n1 = η ∂ j = η ∂ 1 = η ∂\n\n∂ak j\n\n∂ak 1\n\nQ(sk, ak\n\nQ(sk, ak\n\nQ(sk, ak\n\ni , ̄ak 1, ..., ak 1, ..., ak 1 + ∆ak 1, ..., ak m) j−1, ak\n\nj , ̄ak\n\ni+1, ..., ̄ak\n\nm) j−1 + ∆ak\n\nj−1, ak\n\nj , ..., ak\n\nm)\n\nelse ∆ak\n\nj = η ∂\n\nQ(sk, ak\n\n1 + ∆ak\n\n1, ..., ak\n\nj−1 + ∆ak\n\nj+1, ... ̄ak\n\nm)\n\n∂ak j\n\nend for Update policy parameters θi via:\n\n∇θiJ HR\n\ni ≈\n\n1 K\n\n(cid:88)\n\n∇θiμθ1(sk)∆ak\n\ni\n\nk∈{1,...,K}\n\ni = detach(ak\n\ni + ∆ak\n\ni ), for k ∈ {1, ..., K}\n\nSet ̄ak end for Update Q′\n\nend for\n\nend for\n\ni and μ′\n\ni ∀i ∈ M\n\nopponent, i.e., θ2 + ∆θ2(s):\n\n∇θ1J LOLA\n\n1\n\n≈ Eρβ (s)∇θ1Q1(s, a1, ̃a2)|a1=μθ1 (s), ̃a2=μθ2+∆θ2 (s),\n\n(22)\n\nwhere ∆θ2 = η∇θ2 Q2(s, a1, a2)|a1=μθ1 (s),a2=μθ2 (s), and unlike for LA-MADDPG, is a function of θ1. Again, we propose to project the anticipated gradient information to the action space. Given that:\n\n∆θ2 = η∇θ2 μθ2 (s)\n\n(cid:16)\n\n∇a2 Q2(s, a1, a2)|a1=μθ1 (s),a2=μθ2 (s)\n\n(cid:17)⊺\n\n,\n\nwe have:\n\n ̃a2 = μθ2+∆θ2 (s)\n\n≈ μθ2 (s) + (∆θ2)⊺∇θ2μθ2 (s) = μθ2 (s) + ∇a2 Q2(s, a1, a2)η (cid:13) = μθ2 (s) + ∇a2 Q2(s, a1, a2)ˆη,\n\n(cid:13)∇θ2μθ2(s)\n\n(cid:13) 2\n(cid:13)\n\n14\n\n(23)\n\n(24)\n\nUnder review as a conference paper at ICLR 2023\n\nwhere ∥.∥ is the l2-norm and we have defined the projected prediction length ˆη = η (cid:13) since (cid:13)\n\nis a positive number and independent of θ1. Therefore:\n\n(cid:13)∇θ2 μθ2(s)\n\n(cid:13) 2\n(cid:13)\n\n(cid:13)∇θ2 μθ2(s)\n\n(cid:13) 2\n(cid:13)\n\nReplacing Eq. 25 in Eq. 22 yields Eq. 7:\n\n ̃a2 ≈ a2 + ∆a2.\n\n∇θ1J LOLA\n\n1\n\n≈ Eρβ (s)∇θ1 μθ1(s)∇a1 Q1(s, a1, a2 + ∆a2)|a1=μθ1 (s),a1=μθ2 (s).\n\n(25)\n\n(26)\n\nIn the case of n agents, the agent i ∈ N first anticipates the gradient information of all agents j ∈ N in the action space as:\n\n∆aj = ˆη∇a2Q2(s, a1, ..., an)\n\nThen, agent i updates its parameters θi by the following gradient adjustment:\n\n∇θiJ LA\n\ni ≈ Eρβ (s,a)∇θiμθi(s)∇ai Qi(s, a1 + ∆a1, ..., ai, ..., an + ∆an)|ai=μθi (s). Please refer to Alg. 2 for more details on the LOLA-MADDPG optimization framework.\n\n(27)\n\n(28)\n\nA.3 HR-MADDPG\n\nFor the general cases of m agents, we provide the HR-MADDPG algorithm in Alg. 3.\n\nB THEORETICAL ANALYSES ON THE PROJECTION ESTIMATION\n\nB.1\n\nINFLUENCE OF THE PROJECTION ESTIMATION\n\nAs discussed in Appendices A.1 and A.2, we project the anticipated gradient information to the action space using first-order Taylor expansion. This section studies the influence of this projection estimation on the performance of the HOG methods. Without the loss of generality, we consider LOLA-MADDPG with two agents (na ̈ıve opponents). Using first-order Taylor expansion, we showed in Appendix A.2 that the anticipated gradient of the second agent on the parameter space is projected to the action space as:\n\nμθ2+∆θ2(s) ≈ μθ2(s) + ˆη1st∇a2 Q2(s, a1, a2), (29) where ∆θ2 = η∇θ2Q2(s, a1, a2), η ∈ R+ is the prediction length, and we denote ˆη1st ∈ R+ as the projected prediction length via first-order Taylor expansion, which is obtained as (see Appendix A.2):\n\nˆη1st = η (cid:13)\n\n(cid:13)∇θ2 μθ2(s)\n\n(cid:13) 2\n(cid:13)\n\n,\n\n(30)\n\nTheorem 3 If the anticipated gradients are projected using first-order Taylor expansion, for sufficiently small ˆη1st, there exists η′ ∈ R+ such that\n\nwhere\n\nμθ2+∆θ′\n\n2(s) = μθ2 (s) + ˆη1st∇a2Q2(s, a1, a2),\n\n2 = η′∇θ2Q2(s, a1, a2)\n\n(cid:13)∇θ2μθ2(s)\n\n(cid:13) 2\n(cid:13)\n\n∆θ′ ˆη1st = η (cid:13) η ∈ R+\n\n(31)\n\n(32)\n\nIn order to prove Theorem 3, we first need to show that:\n\nLemma 1 If the anticipated gradients are projected using full-order Taylor expansion, there exists ˆηfull ∈ R such that\n\nwhere\n\nμθ2+∆θ2 (s) = μθ2(s) + ˆηfull∇a2Q2(s, a1, a2),\n\n∆θ2 = η∇θ2 Q2(s, a1, a2) η ∈ R+\n\n15\n\n(33)\n\n(34)\n\nUnder review as a conference paper at ICLR 2023\n\nProof – The full-order Taylor expansion of the anticipated gradient yields:\n\nμθ2+∆θ2(s) = μθ2(s) + (∆θ2)⊺∇θ2μθ2(s) +\n\n1 2\n\n(∆θ2)⊺Hμθ2\n\n(s)∆θ2 + O(∥∆θ2∥3),\n\n(35)\n\nwhere Hμθ2\n\n(s) denotes the Hessian of μθ2 at s. Given that:\n\n∆θ2 = η∇θ2Q2(s, a1, a2)\n\n= η∇θ2μθ2(s) (∇a2 Q2(s, a1, a2))\n\n⊺\n\n,\n\n(36)\n\nwe have\n\nμθ2+∆θ2(s) =μθ2(s) + ∇a2 Q2(s, a1, a2)η (cid:13)\n\n(cid:13)∇θ2μθ2(s)\n\n(cid:13) 2\n(cid:13)\n\n+\n\n1 2\n\n∇a2Q2(s, a1, a2)η2 (∇θ2μθ2 (s))\n\n⊺\n\nHμθ2\n\n(s)∇θ2μθ2(s) (∇a2 Q2(s, a1, a2))\n\n⊺\n\n+ O(η3).\n\nBy defining\n\nwe have:\n\nC1(s) = (cid:13) (cid:13)∇θ2μθ2(s) 1\n2\n\nC2(s) =\n\n(∇θ2 μθ2 (s))\n\n(cid:13) 2\n(cid:13)\n\n⊺\n\nHμθ2\n\n(s)∇θ2μθ2(s) (∇a2Q2(s, a1, a2))\n\n⊺\n\n,\n\n(37)\n\n(38)\n\nμθ2+∆θ2(s) =μθ2(s) + ∇a2 Q2(s, a1, a2)(ηC1(s) + η2C2(s) + O(η3)), (39) Given the definition of C2(s) and the dimension constraint implied by ∇a2Q2(s, a1, a2), it can be concluded that C1(s) ∈ R+ and C2(s) ∈ R. Therefore:\n\nμθ2+∆θ2(s) =μθ2(s) + ˆηfull∇a2 Q2(s, a1, a2),\n\n(40)\n\nwhere ˆηfull = ηC1(s) + η2C2(s) + O(η3) ∈ R. Consequently, we have proved Lemma 1 If we now project the anticipated gradients, with a prediction length η′ ∈ R+, to the action space using full-order Taylor expansion, we have:\n\n(41) full = η′C1(s) + η′2C2(s) + O(η′3). In order to prove Theorem 3, we need to find the\n\nfull∇a2Q2(s, a1, a2),\n\nμθ2+∆θ′\n\n2(s) = μθ2(s) + ˆη′\n\nwhere ˆη′ values of ˆη1st that yields:\n\nˆη1st = ˆη′\n\nfull\n\n= η′C1(s) + η′2C2(s) + O(η′3),\n\n(42)\n\nand at the same time η′ ∈ R+. By neglecting O(η′3) and given that ˆη1st ∈ R+, there are two cases to be considered:\n\n• if C2(s) is non-negative, then for any value of ˆη1st ∈ R+, there exists η ∈ R+. • if C2(s) is negative, then for ˆη1st < C1(s)2\n\n4|C2(s)| , there exists η ∈ R+.\n\nTherefore, for sufficiently small ˆη1st, i.e., ˆη1st < C1(s)2 quently, the Theorem 3 is proved.\n\n4|C2(s)| , there always exists η ∈ R+, and conse-\n\nBased on Theorem 3, the projection estimation via first-order Taylor expansion and sufficiently small ˆη1st only scales the prediction length as both η and η′ are non-negative numbers. The amount of this scaling depends on both values of C1(s) and C2(s).\n\nThe general theoretical analyses on single-sate games reveal that scaling the prediction length directly influences the HOG methods’ convergence speed (Letcher et al., 2019; Zhang & Lesser, 2010). However, by directly changing the projected prediction length, i.e., ˆη1st, we can tune the resulting prediction length in the state space, i.e., η′, and consequently improve the convergence behavior. To empirically show this, we conducted an experimental study to analyze the influence of ˆη1st on the convergence behavior of LOLA-MADDPG in the Exit-room game (See Figure 7). The experiments are repeated four times, and the mean results are reported in the form of normalized average episode reward in Figure 7. It is clear from Figure 7 that increasing ˆη1st improves the convergence behavior of LOLA-MADDPG. However, high values of the projected prediction length (ˆη1st = 1.3 in Figure 7) can lead to instability of the algorithm which can be attributed to our findings in Theorem 3.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: The influence of ˆη1st on the convergence behavior of LOLA-MADDPG in the Exit-room game.\n\nB.2 TIME COMPLEXITY OF THE PROJECTION ESTIMATION\n\nIn this section, we first study the time complexity of gradient anticipation in the parameter space. Then, we discuss the time complexity reduction we gain by projecting the anticipated gradients to the action space.\n\nAs both policy and state-action value functions are approximated via neural networks, the time complexity of the gradient anticipation follows the time complexity of backpropagation in neural networks. Without the loss of the generality, we assume (as done in our experiments) that policy and state-action value networks have the same number of hidden layers, H, and neurons in each hidden layer, N . Therefore, the backpropagation time complexity of the networks for an input state of size Ns and action of size Na is (Lister & Stone, 1995):\n\n• Backpropagation time complexity in the policy network: O(NsN + (H − 1)N 2 + N Na) • Backpropagation time complexity in the state-action value network: O((Ns + Na)N +\n\n(H − 1)N 2 + N )\n\nGiven that N > Ns + Na, the time complexity of both networks can be upper bounded by O(LN 2) where we defined L = H + 1. As discussed in Appendix A.2, the anticipated gradient in the state space for the case of the two-agent LOLA-MADDPG is:\n\n∆θ2 = η∇θ2 μθ2 (s)\n\n(cid:16)\n\n∇a2 Q2(s, a1, a2)|a1=μθ1 (s),a2=μθ2 (s)\n\n(cid:17)⊺\n\n.\n\n(43)\n\nTherefore, the time complexity of gradient anticipation in the state space is O(LN 2) × O(LN 2), or in other words, O(L2N 4). This is while the projected anticipated gradient in the action space is:\n\nwhich has the complexity of O(LN 2). Consequently, by projecting the anticipated gradient to the action space, the time complexity is reduced by O(LN 2).\n\n∆a2 = ˆη∇a2 Q2(s, a1, a2),\n\n(44)\n\nC MORE DETAILS ON THE MISCOORDINATION ANALYSES\n\nThe two-agent, two-action coordination game (Claus & Boutilier, 1998) is defined by a common reward matrix R = (cid:2) a k (cid:3), where a > 0 is coordination reward and k ≤ 0 is the miscoordination penalty. We further define g = a − k > 0 as the miscoordination regret. Let θ1 ∈ [0, 1] and θ2 ∈ [0, 1] denote the probability of choosing the first action by first and second agents, respectively.\n\nk a\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Phase planes of the LA and LOLA dynamics. Left: unstable fixed point. Right: unstable saddle fixed point.\n\nThe common value function V (θ1, θ2) = 2g(θ1θ2) − g(θ1 + θ2) + a is the expected reward, given θ1 and θ2. The game has two equivalent Nash equilibria: θ1 = θ2 = 0 and θ1 = θ2 = 1. We further define two miscoordination points: (θ1 = 1, θ2 = 0) and\n\nC.1 PROOF OF THEOREM 1\n\nGiven Eq. 2 the unconstrained dynamics of LA agents can be defined by the following differential equations :\n\n(cid:21) (cid:20)∂θ1/∂t ∂θ2/∂t\n\n=\n\n(cid:20)4ηg2 2g\n\n2g 4ηg2\n\n(cid:21)\n\n(cid:21) (cid:20)θ1 θ2\n\n(cid:21)\n\n(cid:20)2ηg2 + g 2ηg2 + g\n\n−\n\n(45)\n\nThis system of equations has a unique fixed point (zero gradients) at θ1 = θ2 = 0.5 (see Figure 8). The eigenvalue analysis of the coefficient matrix yields two real eigenvalues, λ1 = 4ηg2 + 2g and λ2 = 4ηg2 − 2g, and two respective diagonal and off-diagonal eigenvectors. While λ1 is always positive, the sign of λ2 depends on the values of both η and g. For a fixed prediction length, nonpositive values of λ2 are reached by g ≤ 1 2η . In this case, the fixed point is an unstable saddle point (or unstable line in case of λ2 = 0), and the agents, with any initial values of θ1 and θ2 (except on the fixed point itself), converge to the equilibrium points (see Figure 8-Left). However, when the miscoordination regret increases, g > 1 2η , the fixed point becomes an unstable (source) point (see Figure 8-Right). Therefore, some initial values of θ1 and θ2 naturally lead to the miscoordination points (θ1 = 0, θ2 = 1).\n\nIn the case of LoLA agents, the unconstrained dynamics can be defined as:\n\n(cid:21) (cid:20)∂θ1/∂t ∂θ2/∂t\n\n=\n\n(cid:20)8ηg2 2g\n\n2g 8ηg2\n\n(cid:21)\n\n(cid:21) (cid:20)θ1 θ2\n\n(cid:21)\n\n(cid:20)4ηg2 + g 4ηg2 + g\n\n−\n\n(46)\n\nThis system of equations has a unique fixed point, again at θ1 = θ2 = 0.5 (see Figure 8). The eigenvalue analysis of the coefficient matrix yields two real eigenvalues, λ1 = 8ηg2 + 2g and λ2 = 8ηg2 − 2g, and two respective diagonal and off-diagonal eigenvectors. Similar to the case of LA agents, λ1 is always positive and the sign of λ2 depends on the values of both η and g. For a fixed prediction length, non-positive values of λ2 are reached by g ≤ 1 4η . In this case, the fixed point is an unstable saddle point (or unstable line in case of λ2 = 0), and the agents, with any initial values of θ1 and θ2 (except on the fixed point itself), converge to the equilibrium points. However, when the miscoordination regret increases, g > 1 4η , the fixed point becomes an unstable (source) point. Therefore, some initial values of θ1 and θ2 naturally lead to the miscoordination points.\n\nIn the case of the na ̈ıve agents, we have: (cid:21) (cid:20)∂θ1/∂t ∂θ2/∂t\n\n(cid:20) 0 2g\n\n=\n\n(cid:21)\n\n2g 0\n\n(cid:21) (cid:20)θ1 θ2\n\n(cid:21)\n\n(cid:20)g g\n\n−\n\n(47)\n\nSimilar to the case of LOLA and LA, this system of equations has a unique fixed point (zero gradients) at θ1 = θ2 = 0.5. The eigenvalue analysis of the coefficient matrix yields two real eigenvalues, λ1 = 2g and λ2 = −2g, and two respective diagonal and off-diagonal eigenvectors. This time, however, the eigenvalues are of opposite signs for any values of g and the fixed point is always an\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nunstable saddle point. Therefore, any initial values of θ1 and θ2 (except on the fixed point itself) naturally lead to the equilibrium points.\n\nBased on these results, we hypothesize that 1) wrong reasoning level assumptions and 2) shaping plans that are not followed can lead to miscoordination points. Both LOLA and LA agents assume that other agents are na ̈ıve learners, which is obviously a wrong assumption since all agents conduct first-order reasoning. For self-interested agents, it is natural for the agents to don’t unveil their reasoning order to each other. as they have different goals. However, common-interested agents can benefit more from this reasoning information to achieve their common goal. Furthermore, LOLA agents constantly underestimate other LOLA agents and try to shape them. This is while other LOLA agents do not follow the plan, and each tries to show that it is smarter than the others. Letcher et al. (2019) shows that these arrogant behaviors lead to outcomes that are strictly worse for everyone. It is also clear from Theorem 1 that the range of g that leads to miscoordination in LOLA (g > 1\n\n4η ) is larger than the range of g in LA (g > 1\n\n2η ).\n\nIn the above coordination game, one general solution to reduce the possibility of miscoordination is decreasing the prediction length’s values. However, in non-tabular settings, with large state spaces, it is infeasible to estimate the miscoordination regret and adjust the prediction length accordingly. Furthermore, the prediction length directly affects the usage of higher-order gradient information, and further reducing the prediction length (η → 0) leads to the na ̈ıve learners.\n\nC.2 PROOF OF THEOREM 2\n\nGiven Eqs. 9 and 8, the unconstrained dynamics of the HR agents can be defined by the following differential equations :\n\n(cid:21) (cid:20)∂θ1/∂t ∂θ2/∂t\n\n=\n\n(cid:20)\n\n8ηg2 2g + 16η2g3\n\n2g 4ηg2\n\n(cid:21)\n\n(cid:21) (cid:20)θ1 θ2\n\n−\n\n(cid:20)\n\n4ηg2 + g 8η2g3 + 4ηg2 + g\n\n(cid:21)\n\n(48)\n\nresulting in a unique fixed point at θ1 = θ2 = 0.5 and two real eigenvalues, λ = 6ηg2 ± 2p(cid:112)9eta2g2 + 1. Unlike the case of LOLA and LA, the eigenvalues are now of opposite signs for any values of g, and the fixed point is always an unstable saddle point. Therefore, any initial values of θ1 and θ2 (except on the fixed point itself) naturally lead to the equilibrium points.\n\nD MORE DETAILS ON THE EXPERIMENTS AND THE IMPLEMENTATIONS\n\nIn this section, we describe all the experiments and the implementations in detail. To ease the reproducibility of our work, the code of our methods and experiments are shared with the community at [to comply with the double-blind policy, the link will be inserted in the final version].\n\nA note on partial observability. So far, we have formulated the MARL setup as an MG, where it is assumed that the agents have access to the state space. However, in many games, the agents only receive a private state observation of the current state. In this case, the MARL setup can be formulated as a Partially Observable Markov Game (PO-MG) (Littman, 1994). A PO-MG is a tuple (N , S, {Ai}i∈N , {Oi}i∈N , {Ri}i∈N , T , {Ωi}i∈N , ρ, γ), where Oi is the set of sate observations for agent i ∈ N . Each agent i chooses its action ai ∈ Ai through the policy πθi : Oi × Ai → [0, 1] parameterized by θi conditioning on the given state observation oi ∈ Oi. After transition to a new state, each agent i receives a private state observation through its observation function Ωi : S → Oi. In this case, the centralized state-action value function for each agent i is defined as Qi(o1, ..., on, a1, ..., an) = E[Gt i = ai∀i ∈ N )]. Therefore, the proposed HOG-MADDPG framework can be modified accordingly.\n\ni(τ |st = s, oi = Ωi(s) & at\n\nD.1 MATRIX GAMES\n\nIterated Rotational Game (IRG)(Zhang & Lesser, 2010) is a one-state, two-agent, one-action matrix game with the reward matrices depicted in Table 4 (for two discrete actions). Each agent must choose a 1-D continuous action (a1 for agent one and a2 for agent two) representing the probability of taking two discrete actions. The game has a unique equilibrium point at a1 = a2 = 0.5, which is also the fixed point of the game. IRG is originally proposed to demonstrate the circular behavior that\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\ndiscrete action 1\n\ndiscrete action 2\n\nCooperate\n\nDefect\n\n(−3, 0)\n\ndiscrete action 1\n\ndiscrete action 2\n\n(0, 3)\n\n(1, 0)\n\n(3, 2)\n\n(2, 1)\n\nCooperate\n\n(−1, −1)\n\nDefect\n\n(0, −3)\n\n(−3, −3)\n\nTable 4: Reward matrix in IRG.\n\nTable 5: Reward matrix in IPD.\n\ncan emerge if the agents follow the na ̈ıve gradient updates. LA agents, on the other hand, can quickly converge to the equilibrium point by considering their opponent’s parameter adjustment. As LOLA agents cannot preserve the fixed point of the game (Letcher et al., 2019), they converge to nonequilibrium points. We evaluate the performances of methods based on the Distance to Equilibrium (DtE).\n\nIterated Prisoner’s Dilemma (IPD) (Foerster et al., 2018a) is a five-state, two-agent, two-action game with the reward matrices depicted in Table 5. Each agent must choose between two discrete actions (cooperate or defect). The game is played for 150 time steps (T = 150). In the one-shot version of the game, there is only one Nash equilibrium for the agents (Defect, Defect). In the iterated games, (Defect, Defect) is also a Nash equilibrium. However, a better equilibrium is Tit-For-Tat (TFT), where the players start by cooperating and then repeat the previous action of the opponents. The LOLA agents can shape the opponent’s learning to encourage cooperation and, therefore, converge to TFT (Letcher et al., 2019). We evaluate the methods’ performances based on the Averaged Episode Reward (AER).\n\nImplementation details. We used policies and state-action value functions with the same neural network architecture in all methods. We employed Multi-Layer Perceptron (MLP) networks with two hidden layers of dimension 64 for policies and state-action value functions. In order to make the value functions any-order differentiable, we used SiLU nonlinear function (Elfwing et al., 2018) in between the hidden layers. For IRG, we used the Sigmoid function in the policies to output 1D continues action, and for IPD, we used the Gumble-softmax function (Jang et al., 2017) in the policies to output two discrete actions. The algorithms are trained for 900 (in IRG) and 50 (in IPD) episodes by running Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.01. The prediction length, η, for LOLA and LA agents in both HOG-MADDPG and DiCE frameworks are fixed to 1 in all experiments. We reported the best methods’ performances in Table 1. All experiments are repeated five times, and the results are reported in terms of mean and standard deviation in Figure 1 and Table 1.\n\nD.2 EXIT-ROOM GAME\n\nThe Exit-Room game is a grid-world variant of the IPD, with two agents (blue and red) and 152l states where l ∈ {1, 2, 3} is the complexity level of the game. The agents should cooperate and move towards the exit doors on the right. However, they are tempted to exit the left doors and, in some cases, not exiting at all. In level 1, the agents have three possible actions (move-left, move-right, or do nothing), and the reward is computed as Vinitsky et al. (2019):\n\nrewardC = λC(cooperationself + cooperationopponent) rewardD = λD(1 − cooperationself ) reward = rewardC + rewardD,\n\n(49)\n\nwhere λC and λD are some constants, and cooperationself and cooperationopponent are the normalized distances of the agent and its opponent to the right door, respectively. In levels 2 and 3, the agents have additional move-up and move-down actions. In level 3, the door positions are randomly located, resulting in more complex interactions among the agents. In addition to the reward in Eq. (49), the agents receive an additional reward for approaching the doors in levels 2 and 3. Each agent receives four 90 × 90 RGB images representing the state observations of the last four time steps.\n\nImplementation details. As before, we used policies and state-action value functions with the same neural network architecture in all methods. Both policy and value networks consist of two parts: encoder and decoder. The encoders are CNN networks with three convolutional layers (12 × 90 × 90 → 32 × 21 × 21 → 64 × 9 × 9 → 64 × 7 × 7 ) and two fully connected layers (3136 → 512 → 128 ), with SiLU nonlinear functions (Elfwing et al., 2018) in between. The decoders are MLP\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n(a) η = 1\n\n(b) η = 0.1\n\n(c) η = 0.01\n\nFigure 9: Learning curves in the particle coordination game with different values of the prediction length, η, for HOG-MADDPG methods.\n\nnetworks with two hidden layers of dimension 64 for policies and state-action value functions. We used the Gumble-softmax function in the policies (Jang et al., 2017) to output the discrete actions. The algorithms are trained for 450 (in level one) and 4500 (in levels two and three) episodes by running Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.01. The prediction length, η, for LOLA and LA agents in both HOG-MADDPG and DiCE frameworks are fixed to 1 in all experiments. All experiments are repeated five times, and the results are reported in terms of mean and standard deviation in Figure 2 and Table 2. The methods are evaluated in terms of the Normalized Average Episode Reward (NAER), where the normalization is done between the highest and lowest episode rewards in each game level. We reported the best methods’ performances in Table 2.\n\nD.3 PARTICLE COORDINATION GAME\n\nOur proposed game is a variant of the Cooperative Navigation game in the Particle environment (Lowe et al., 2017). As shown in Figure 10, each one of the two agents (purple circles) should select and approach one of the three landmarks (one gray and two green circles). Landmarks are selected based on the closest distance between the agent and the landmarks. Suppose the agents select and approach the same landmark. In that case, they receive global (by selecting the green landmarks) or local (by selecting the gray landmark) optimal rewards. They will receive an assigned miscoordination penalty if they select and approach different landmarks. Each agents receive a 14-D state observation vector and select a 5-D, one-hot vector, representing one of the five discrete actions: move-right, move-left, move-up, move-down, and stay. The horizon is set to 25, T = 25. The agents receive a Landmark Selection (LS) reward indicated by the matrix: RLS = (cid:2) 2 (cid:3), where the rows and columns indicate the selected landmarks by the first and second agents, respectively. Furthermore, the agents receive an additional reward for approaching the landmarks.\n\nFigure 10: Particle coordination game with two agents and three landmarks.\n\n0 0.4 0 2\n\n0 −20\n\n−20 0\n\nImplementation details. We used policies and state-action value functions with the same neural network architecture in all methods. We employed MLP networks with two hidden layers of dimension 64 for policies and state-action value functions with SiLU nonlinear functions (Elfwing et al., 2018) in between. We used the Gumble-softmax function (Jang et al., 2017) in the policies to output the discrete actions. The algorithms are trained for 100k episodes by running Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.01. We set the prediction length, η, to 0.1 for HR agents and 0.01 for LOLA and LA agents. All experiments are repeated five times, and the results are reported in terms of mean and standard deviation in Figure 6.\n\nEffect of the prediction length. Additionally, we illustrate the effect of various prediction lengths on the performance of HR, LOLA, and LA agents in Figure 9. As shown, the HR-MADDPG method consistently converges to the global optimum of the game. This is while both LOLA-MADDPG and LA-MADDPG demonstrate weak performances, especially by increasing the prediction length.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nParticle Environment\n\nMujoco Environment\n\nCooperative Navigation\n\nPhysical Deception\n\nPredator-Prey Half-Cheetah\n\nWalker\n\nReacher\n\nObservation Observation type # Discrete observations Action Action type Policy parameter Horizon (step)\n\n18-D continuous infinite 5-D discrete 11K-D 25\n\n10-D (8-D) continuous infinite 5-D (5-D) discrete 11K-D (8-D) 25\n\n14-D (12-D) continuous infinite 5-D (5-D) discrete 10K-D (12-D) 25\n\n11-D continuous infinite 3-D continuous 70K-D 100\n\n11-D continuous infinite 3-D continuous 70K-D 300\n\n8-D continuous infinite 1-D continuous 70K-D 50\n\nTable 6: Specifications in the standard multi-agent games. In the mixed environments, the dimensions are reported as ”d1 (d2)” where d1 is the dimension for common-interested agents and d2 is the dimension for self-interested ones.\n\nD.4 STANDARD MULTI-AGENT GAMES\n\nWe evaluate the methods in three Particle environment games (Lowe et al., 2017): 1) Cooperative Navigation with three common-interested agents, 2) Physical Deception with two commoninterested and one self-interested agent, and 3) Predator-Prey with two common-interested (predator) and one self-interested (prey) agents. Furthermore, we compare the methods in three games within the multi-agent Mujoco environment (Peng et al., 2021): 1) two-agent Half-Cheetah, 2) two-agent Walker, and 3) two-agent Reacher. In the mixed environments (Physical Deception and PredatorPrey), we have employed the MADDPG method for the self-interested agents. Games’ specifications are reported in Table 6. We created separate validation and test sets for each game that included 100 and 300 randomly generated scenarios, respectively. In each game, we save the models that have the best performance on the validation set and test them on the test set to report the results. All experiments are repeated five times, and the mean results are reported in Table 3 in terms of the Normalized Average Episode Reward (NAER). The normalization is done between the single-agent variant of MADDPG (DDPG (Lillicrap et al., 2016)) and a fully centralized (in learning and execution) variant of MADDPG, referred to as C-MADDPG. The non-normalized data are reported in Table 8 in terms of the Collective Average Episode Reward (CAER) for the common-interested agents.\n\nImplementation details. As before, we used policies and state-action value functions with the same neural network architecture in all methods. We employed MLP networks with two hidden layers (of dimension 64 for the Particle environment and 256 for the Mujoco environment) for policies and state-action value functions with SiLU nonlinear functions (Elfwing et al., 2018). In the Particle environment, We used the Gumble-softmax function (Jang et al., 2017) in the policies to output the discrete actions and trained the algorithms for 100k episodes by running Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.01. In the Mujoco environment, we used the Tanh function in the policies to output the continuous actions and train the algorithms for 10k episodes by running Adam optimizer (Kingma & Ba, 2015) with a fixed learning rate of 0.001. The prediction lengths, η, in HR-MADDPG, LA-MADDPG, and LOLA-MADDPG are optimized between 0.001 − 0.1 in all games. We avoid considering any smaller value than 0.001 for the prediction length as it makes the algorithms similar to the na ̈ıve learners. The optimized prediction lengths are reported in Table 7.\n\nAblation study. We have additionally conducted an ablation study on the hierarchy level assignments in the HR-MADDPG. Rather than iteratively sorting the agents based on their shaping capacities through Eq. (10), we randomly assigned the agents to hierarchy levels in the beginning and fixed the hierarchy levels throughout the optimization. This variant of the HR-MADDPG, referred to as HR-MADDPG (F), is evaluated and compared in Table 9. As can be seen, using the proposed sorting strategy based on the shaping capacities of the agents, as done in our HR-MADDPG, constantly improves performance.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nη in Particle Environment\n\nη in Mujoco Environment\n\nCooperative Navigation\n\nPhysical Deception\n\nPredator-Prey Half-Cheetah Walker Reacher\n\nLA-MADDPG LOLA-MADDPG HR-MADDPG\n\n0.002 0.001 0.003\n\n0.003 0.003 0.01\n\n0.01 0.008 0.04\n\n0.001 0.001 0.004\n\n0.002 0.001 0.004\n\n0.003 0.002 0.007\n\nTable 7: The optimized prediction lengths for HOG-MADDPG methods.\n\nMethods\n\nCooperative Navigation\n\nPhysical Deception\n\nPredator-Prey Half-Cheetah Walker\n\nReacher\n\n↑CAER in Particle Environment\n\n↑CAER in Mujoco Environment\n\nDDPG (LB) C-MADDPG (UB)\n\nMADDPG CPG-MADDPG PR2-MADDPG LA-MADDPG LOLA-MADDPG HR-MADDPG\n\n-189.18 -130.91\n\n-144.02 -143.96 -143.67 -143.72 -144.21 -137.63\n\n13.04 20.82\n\n17.82 18.29 17.26 17.92 17.43 19.48\n\n10.20 20.87\n\n12.46 12.08 11.08 11.58 11.54 14.85\n\n611.74 1564.60\n\n1435.93 1454.62 1418.69 1424.39 1406.61 1503.07\n\n2925.17 6362.36\n\n4482.31 4511.08 4473.14 4408.56 4369.13 5226.26\n\n-16.19 -8.89\n\n-16.02 -15.84 -16.09 -15.89 -16.13 -13.16\n\nTable 8: Comparisons of methods in terms of the Collective Average Episode Reward (CAER) for common-interested agents, corresponding to the normalized data in Table 3.\n\n↑NAER in Particle Environment\n\n↑NAER in Mujoco Environment\n\nMethods\n\nCooperative Navigation\n\nPhysical Deception\n\nPredator-Prey Half-Cheetah Walker Reacher\n\nHR-MADDPG (F) HR-MADDPG\n\n0.85 0.88\n\n0.80 0.83\n\n0.38 0.44\n\n0.92 0.94\n\n0.63 0.67\n\n0.38 0.42\n\nTable 9: Ablation study on the hierarchy level assignments in our HR-MADDPG method.\n\n23",
    "reference": "# Summary Of The Paper\n\nThis paper studies the use of higher order gradient methods for multi-agent RL with high dimensional state space. It shows that existing methods can lead to miscoordination among agents. A hierarchical reasoning algorithm is proposed. Experimental results are presented to show the applicability of the method.\n\n# Strength And Weaknesses\n\nStrength\n- The problem in consideration is interesting. \n- The proposed algorithm performs well in the experiments. \n\nWeakness\n- The paper could make it clear what the exact setting is. This will help clarify some potential confusion. For example, the issue raised in the intro does not seem to exist in the CTDE framework (not a game setting). \n- What are the forms of (1) and (2) when there are multiple agents? So are (3)-(7). Right now everything is done with n=2. \n- Following the above comment, it appears that most of the derivation and most experiments are done for a 2-user case (or two coordinating agents with others). While the reviewer understands the tractability of this case, it seems a bit limited that even in the experiments only two agents are considered. \n- Where is the projection step in the algorithms? Also, what is the complexity for doing so? \n- The presentation is a bit confusing. In Fig. 2,  LA-MADDPG and LOLA-MADDPG are both marked as “ours”. But in Table 3, only HR-MADDPG has this label. From the text, it seems that the paper applies the ideas to LA-MADDPG and LOLA-MADDPG. Please clarify. \n- It would be useful to highlight the key difference between the proposed algorithms and MADDPG. Right now the novelty of the algorithm is not clear. \n- Minor: “between the agents” should be “among the agents”\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing and presentation of the paper could be improved. The authors should also further highlight the key novelty of the proposed algorithms and explain more about their advantages compared to existing methods.\n\n# Summary Of The Review\n\nOverall, the paper considers an interesting problem and the experimental results show that the algorithms perform well. However, the presentation of the paper could be improved, also, the contributions need to be better explained.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION∗\n\nBowen Zhao1,2, Chen Chen3,(cid:12), Shu-Tao Xia1,4,(cid:12) 1Tsinghua University, 2Tencent TEG AI, 3OPPO research institute, 4Peng Cheng Laboratory zbw18@mails.tsinghua.edu.cn, chen1634chen@gmail.com, xiast@sz.tsinghua.edu.cn\n\nABSTRACT\n\nFully test-time adaptation aims at adapting a pre-trained model to the test stream during real-time inference, which is urgently required when the test distribution differs from the training distribution. Several efforts have been devoted to improving adaptation performance. However, we find that two unfavorable defects are concealed in the prevalent adaptation methodologies like test-time batch normalization (BN) and self-learning. First, we reveal that the normalization statistics in test-time BN are completely affected by the currently received test samples, resulting in inaccurate estimates. Second, we show that during test-time adaptation, the parameter update is biased towards some dominant classes. In addition to the extensively studied test stream with independent and class-balanced samples, we further observe that the defects can be exacerbated in more complicated test environments, such as (time) dependent or class-imbalanced data. We observe that previous approaches work well in certain scenarios while show performance degradation in others due to their faults. In this paper, we provide a plug-in solution called DELTA for Degradation-freE fuLly Test-time Adaptation, which consists of two components: (i) Test-time Batch Renormalization (TBR), introduced to improve the estimated normalization statistics. (ii) Dynamic Online re-weighTing (DOT), designed to address the class bias within optimization. We investigate various test-time adaptation methods on three commonly used datasets with four scenarios, and a newly introduced real-world dataset. DELTA can help them deal with all scenarios simultaneously, leading to SOTA performance.\n\n1\n\nINTRODUCTION\n\nModels suffer from performance decrease when test and training distributions are mismatched (Quinonero-Candela et al., 2008). Numerous studies have been conducted to narrow the performance gap based on a variety of hypotheses/settings. Unsupervised domain adaptation methods (Ganin et al., 2016) necessitate simultaneous access to labeled training data and unlabeled target data, limiting their applications. Source-free domain adaptation approaches (Liang et al., 2020) only need a trained model and do not require original training data when performing adaptation. Nonetheless, in a more difficult and realistic setting, known as fully test-time adaptation (Wang et al., 2021), the model must perform online adaptation to the test stream in real-time inference. The model is adapted in a single pass on the test stream using a pre-trained model and continuously arriving test data (rather than a prepared target set). Offline iterative training or extra heavy computational burdens beyond normal inference do not meet the requirements.\n\nThere have been several studies aimed at fully test-time adaptation. Test-time BN (Nado et al., 2020) / BN adapt (Schneider et al., 2020) directly uses the normalization statistics derived from test samples instead of those inherited from the training data, which is found to be beneficial in reducing the performance gap. Entropy-minimization-based methods, such as TENT (Wang et al., 2021), further optimize model parameters during inference. Contrastive learning (Chen et al., 2022), data augmentation (Wang et al., 2022a) and uncertainty-aware optimization (Niu et al., 2022) have been introduced to enhance adaptation performance. Efforts have also been made to address test-time adaptation in more complex test environments, like LAME (Boudiaf et al., 2022).\n\n∗work done by Bowen Zhao (during internship) and Chen Chen at Tencent.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n−→\n\nIS+CB DS+CB IS+CI DS+CI\n\nFigure 1: IS+CB / DS+CB: the test stream which is independently / dependently sampled from a class-balanced test distribution; IS+CI/ DS+CI: independently / dependently drawn from a class-imbalanced test distribution. Each bar represents a sample, each color represents a category.\n\nTable 1: Comparison of fully test-time adaptation methods against the pretrained model on CIFAR100-C. DELTA achieves improvement in all scenarios.\n\nScenario TENT LAME DELTA (Ours)\n\nIS+CB DS+CB IS+CI DS+CI\n\nDespite the achieved progress, we find that there are non-negligible defects hidden in the popular methods. First, we take a closer look at the normalization statistics within inference (Section 3.2). We observe that the statistics used in BN adapt is inaccurate in per batch compared to the actual population statistics. Second, we reveal that the prevalent test-time model updating is biased towards some dominant categories (Section 3.3). We notice that the model predictions are extremely imbalanced on out-of-distribution data, which can be exacerbated by the self-learning-based adaptation methods. Besides the most common independent and class-balanced test samples considered in existing studies, following Boudiaf et al. (2022), we investigate other three test scenarios as illustrated in Figure 1 (please see details in Section 3.1) and find when facing the more intricate test streams, like dependent samples or class-imbalanced data, the prevalent methods would suffer from severe performance degradation, which limits the usefulness of these test-time adaptation strategies.\n\nTo address the aforementioned issues, we propose two powerful tools. Specifically, to handle the inaccurate normalization statistics, we introduce test-time batch renormalization (TBR) (Section 3.2), which uses the test-time moving averaged statistics to rectify the normalized features and considers normalization during gradient optimization. By taking advantage of the observed test samples, the calibrated normalization is more accurate. We further propose dynamic online re-weighting (DOT) (Section 3.3) to tackle the biased optimization, which is derived from cost-sensitive learning. To balance adaptation, DOT assigns low/high weights to the frequent/infrequent categories. The weight mapping function is based on a momentum-updated class-frequency vector that takes into account multiple sources of category bias, including the pre-trained model, the test stream, and the adaptation methods (the methods usually do not have an intrinsic bias towards certain classes, but can accentuate existing bias). TBR can be applied directly to the common BN-based pre-trained models and does not interfere with the training process (corresponding to the fully test-time adaptation setting), and DOT can be easily combined with other adaptation approaches as well.\n\nTable 1 compares our method to others on CIFAR100-C across various scenarios. The existing test-time adaptation methods behave differently across the four scenarios and show performance degradation in some scenarios. While our tools perform well in all four scenarios simultaneously without any prior knowledge of the test data, which is important for real-world applications. Thus, the whole method is named DELTA (Degradation-freE fuLly Test-time Adaptation).\n\nThe major contributions of our work are as follows. (i) We expose the defects in commonly used test-time adaptation methods, which ultimately harm adaptation performance. (ii) We demonstrate that the defects will be even more severe in complex test environments, causing performance degradation. (iii) To achieve degradation-free fully test-time adaptation, we propose DELTA which comprises two components: TBR and DOT, to improve the normalization statistics estimates and mitigate the bias within optimization. (iv) We evaluate DELTA on three common datasets with four scenarios and a newly introduced real-world dataset, and find that it can consistently improve the popular test-time adaptation methods on all scenarios, yielding new state-of-the-art results.\n\n2 RELATED WORK\n\nUnsupervised domain adaptation (UDA). In reality, test distribution is frequently inconsistent with the training distribution, resulting in poor performance. UDA aims to alleviate the phenomenon with the collected unlabeled samples from the target distribution. One popular approach is to align the statistical moments across different distributions (Gretton et al., 2006; Zellinger et al., 2017; Long et al., 2017). Another line of studies adopts adversarial training to achieve adaptation (Ganin et al., 2016;\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nLong et al., 2018). UDA has been developed for many tasks including object classification (Saito et al., 2017)/detection (Li et al., 2021) and semantic segmentation (Hoffman et al., 2018).\n\nSource-free domain adaptation (SFDA). SFDA deals with domain gap with only the trained model and the prepared unlabeled target data. To be more widely used, SFDA methods should be built on a common source model trained by a standard pipeline. SHOT (Liang et al., 2020) freezes the source model’s classifier and optimizes the feature extractor via entropy minimization, diversity regularization, and pseudo-labeling. SHOT incorporates weight normalization, 1D BN, and labelsmoothing into backbones and training, which do not exist in most off-the-shelf trained models, but its other ideas can be used. USFDA (Kundu et al., 2020) utilizes synthesized samples to achieve compact decision boundaries. NRC (Yang et al., 2021b) encourages label consistency among local target features with the same network architecture as SHOT. GSFDA (Yang et al., 2021a) further expects the adapted model performs well not only on target data but also on source data.\n\nFully test-time adaptation (FTTA). FTTA is a more difficult and realistic setting. In the same way that SFDA does not provide the source training data, only the trained model is provided. Unlike SFDA, FTTA cannot access the entire target dataset; however, the methods should be capable of doing online adaptation on the test stream and providing instant predictions for the arrived test samples. BN adapt (Nado et al., 2020; Schneider et al., 2020) replaces the normalization statistics estimated during training with those derived from the test mini-batch. On top of it, TENT (Wang et al., 2021) optimizes the affine parameters in BN through entropy minimization during test. EATA (Niu et al., 2022) and CoTTA (Wang et al., 2022a) study long-term test-time adaptation in continually changing environments. ETA (Niu et al., 2022) excludes unreliable and redundant samples from the optimization. AdaContrast (Chen et al., 2022) resorts to contrastive learning to promote feature learning along with a pseudo label refinement mechanism. Both AdaContrast and CoTTA utilize heavy data augmentation during test, which will increase inference latency. Besides, AdaContrast modifies the model architecture as in SHOT. Different from them, LAME (Boudiaf et al., 2022) does not rectify the model’s parameters but only the model’s output probabilities via the introduced unsupervised objective laplacian adjusted maximum-likelihood estimation.\n\nClass-imbalanced learning. Training with class-imbalanced data has attracted widespread attention (Liu et al., 2019). Cost-sensitive learning (Elkan, 2001) and resampling (Wang et al., 2020) are the classical strategies to handle this problem. Ren et al. (2018) designs a meta-learning paradigm to assign weights to samples. Class-balanced loss (Cui et al., 2019) uses the effective number of samples when performing re-weighting. Decoupled training (Kang et al., 2020b) learns the feature extractor and the classifier separately. Menon et al. (2021) propose logit adjustment from a statistical perspective. Other techniques such as weight balancing (Alshammari et al., 2022; Zhao et al., 2020), contrastive learning (Kang et al., 2020a), knowledge distillation (He et al., 2021), etc. have also been applied to solve this problem.\n\n3 DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION\n\n3.1\n\nPROBLEM DEFINITION\n\nAssume that we have the training data Dtrain = {(xi, yi)}N train i=1 ∼ P train(x, y), where x ∈ X is the input and y ∈ Y = {1, 2, · · · , K} is the target label; f{θ0,a0} denotes the model with parameters θ0 and normalization statistics a0 learned or estimated on Dtrain. Without loss of generality, we denote the test stream as Dtest = {(xj, yj)}N test j=1 ∼ P test(x, y), where {yj} are not available actually, the subscript j also indicates the sample position within the test stream. When P test(x, y) ̸= P train(x, y) (the input/output space X /Y is consistent between training and test data), f{θ0,a0} may perform poorly on Dtest. Under fully test-time adaptation scheme (Wang et al., 2021), during inference step t ≥ 1, the model f{θt−1,at−1} receives a mini-batch of test data {xmt+b}B b=1 with B batch size (mt is the number of test samples observed before inference step t), and then elevates itself to f{θt,at} based b=1 (p ∈ RK). Finally, on current test mini-batch and outputs the real-time predictions {pmt+b}B the evaluation metric is calculated based on the online predictions from each inference step. Fully test-time adaptation emphasizes performing adaptation during real-time inference entirely, i.e., the training process cannot be interrupted, the training data is no longer available during test, and the adaptation should be accomplished in a single pass over the test stream.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nThe most common hypothesis is that Dtest is independently sampled from P test(x, y). However, in real environment, the assumption does not always hold, e.g., samples of some classes may appear more frequently in a certain period of time, leading to another hypothesis: the test samples are dependently sampled. Most studies only considered the scenario with class-balanced test samples, while in real-world, the test stream can be class-imbalanced1. We investigate fully test-time adaptation under the four scenarios below, considering the latent sampling strategies and the test class distribution. For convenience, we denote the scenario where test samples are independently/dependently sampled from a class-balanced test distribution as IS+CB / DS+CB; denote the scenario where test samples are independently/dependently sampled from a class-imbalanced test distribution as IS+CI/ DS+CI, as shown in Figure 1. Among them, IS+CB is the most common scenario within FTTA studies, and the other three scenarios also frequently appear in real-world applications.\n\n3.2 A CLOSER LOOK AT NORMALIZATION STATISTICS\n\nWe revisit BN (Ioffe & Szegedy, 2015) briefly. Let v ∈ RB×C×S×S′ be a mini-batch of features with C channels, height S and width S′. BN normalizes v with the normalization statistics μ, σ ∈ RC: v∗ = v−μ σ , v⋆ = γ · v∗ + β, where γ, β ∈ RC are the learnable affine parameters, {γ, β} ⊂ θ. We mainly focus on the first part v → v∗ (all the discussed normalization methods adopt the affine parameters). In BN, during training, μ, σ are set to the empirical mean μbatch and standard deviation σbatch calculated for each channel c: μbatch[c] = 1 b,s,s′ v[b, c, s, s′], σbatch[c] = b,s,s′(v[b, c, s, s′] − μbatch[c])2 + ε, where ε is a small value to avoid division by zero. During inference, μ, σ are set to μema, σema which are the exponential-moving-average (EMA) estimates over training process (a0 is formed by the EMA statistics of all BN modules). However, when P test(x, y) ̸= P train(x, y), studies found that replacing μema, σema with the statistics of the test mini-batch: ˆμbatch, ˆσbatch can improve model accuracy (Nado et al., 2020) (for clarify, statistics estimated on test samples are denoted with ‘ ˆ ’). The method is also marked as “BN adapt” (Schneider et al., 2020).\n\n(cid:113) 1\n\nBSS′\n\nBSS′\n\n(cid:80)\n\n(cid:80)\n\nDiagnosis I: Normalization statistics are inaccurate within each test mini-batch. We conduct experiments on CIFAR100-C. From Figure 2 we can see that the statistics ˆμbatch, ˆσbatch used in BN adapt fluctuate dramatically during adaptation, and are inaccurate in most test mini-batches. It should be noted that for BN adapt, predictions are made online based on real-time statistics, so poor estimates can have a negative impact on performance. More seriously, the estimates in the DS+CB scenario are worse. In Table 2, though BN adapt and TENT can improve accuracy compared to Source (test with the fixed pre-trained model f{θ0,a0}) in IS+CB scenario, they suffer from degradation in the DS+CB cases. Overall, we can see that the poor statistics severely impede test-time adaptation because they are derived solely from the current small mini-batch.\n\nTreatment I: Test-time batch renormalization (TBR) is a simple and powerful tool to improve the normalization. It is natural to simply employ the test-time moving averages ˆμema, ˆσema to perform normalization during adaptation, referring to as TEMA, where ˆμema t−1 + (1 − α) · sg(ˆμbatch t = α · ˆσema ), ˆσema ), sg(·) stands for the operation of stopping gradient, e.g., the Tensor.detach() function in PyTorch, α is a smoothing coef-\n\nt−1 + (1 − α) · sg(ˆσbatch\n\nt = α · ˆμema\n\nt\n\nt\n\n(a) μ, IS+CB\n\n(b) μ, DS+CB\n\n(c) σ, IS+CB\n\n(d) σ, DS+CB\n\nFigure 2: Normalization statistics in different scenarios on CIFAR100-C.\n\nTable 2: Average accuracy (%) of 15 corrupted sets on CIFAR100-C.\n\nMethod\n\nIS+CB\n\nDS+CB\n\n53.5±0.00 53.5±0.00 Source BN adapt 64.3±0.05 27.3±1.12 BN adapt+TEMA 64.8±0.04 63.5±0.51 68.5±0.13 23.7±1.04 TENT 21.8±0.84 26.2±1.27 TENT+TEMA 68.8±0.13 64.1±0.57 TENT+TBR\n\n1Regarding training class distribution, in experiments, we primarily use models learned on balanced training data following the benchmark of previous studies. Furthermore, when P train(y) is skewed, some techniques are commonly used to bring the model closer to the one trained on balanced data, such as on YTBB-sub (Section 4), where the trained model is learned with logit adjustment on class-imbalanced training data.\n\n4\n\n01020304050607080Test mini-batch0.0720.0710.0700.0690.0680.0670.0660.0650.064 for normalizationGlobalBN adaptBN adapt+TEMA01020304050607080Test mini-batch0.0720.0710.0700.0690.0680.0670.0660.0650.064 for normalizationGlobalBN adaptBN adapt+TEMA01020304050607080Test mini-batch0.0620.0640.0660.0680.0700.0720.074 for normalizationGlobalBN adaptBN adapt+TEMA01020304050607080Test mini-batch0.0620.0640.0660.0680.0700.0720.074 for normalizationGlobalBN adaptBN adapt+TEMAPublished as a conference paper at ICLR 2023\n\nficient. TEMA can consistently improve BN adapt: the normalization statistics in Figure 2 become more stable and accurate, and the test accuracy in Table 2 is improved as well.\n\nHowever, for TENT which involves parameters update, TEMA can destroy the trained model as shown in Table 2. As discussed in Ioffe & Szegedy (2015), simply employing the moving averages would neutralize the effects of gradient optimization and normalization, as the gradient descent optimization does not consider the normalization, leading to unlimited growth of model parameters. Thus, we introduce batch renormalization (Ioffe, 2017) into test-time adaptation, leading to TBR, which is formulated by\n\nv∗ =\n\nv − ˆμbatch ˆσbatch\n\n· r + d, where\n\nr =\n\nsg(ˆσbatch) ˆσema\n\n,\n\nd =\n\nsg(ˆμbatch) − ˆμema ˆσema\n\n,\n\n(1)\n\nWe present a detailed algorithm description in Appendix A.2. Different from BN adapt, we use the test-time moving averages to rectify the normalization (through r and d). Different from the TEMA, TBR is well compatible with gradient-based adaptation methods (e.g., TENT) and can improve them as summarised in Table 2. For BN adapt, TEMA is equal to TBR. Different from the original batch renormalization used in the training phase, TBR is employed in the inference phase which uses the statistics and moving averages derived from test batches. Besides, as the adaptation starts with a trained model f{θ0,a0}, TBR discards the warm-up and truncation operation to r and d, thus does not introduce additional hyper-parameters. TBR can be applied directly to a common pre-trained model with BN without requiring the model to be trained with such calibrated normalization.\n\n3.3 A CLOSER LOOK AT TEST-TIME PARAMETER OPTIMIZATION\n\nBuilding on BN adapt, TENT (Wang et al., 2021) further optimizes the affine parameters γ, β through entropy minimization and shows that test-time parameter optimization can yield better results compared to employing BN adapt alone. We further take a closer look at this procedure.\n\nII: is\n\nFigure 3: Per-class number of predictions under combinations of [data, scenario, method].\n\nbiased classes. We\n\nTable 3: Standard Deviation (STD), Range (R) of per-class number of predictions and accuracy (Acc, %) on Gauss data.\n\nthe test-time opDiagnosis towards timization dominant evaluate the model on IS+CB and DS+CB gaussian-noise-corrupted test data (Gauss) of CIFAR100-C. We also Source 158.3±0.0 956.0±0.0 27.0±0.0 158.3±0.0 956.0±0.0 27.0±0.0 test the model on the original clean BN adapt+TEMA 18.4±0.2 121.6±3.7 58.0±0.2 19.8±1.1 130.0±13.6 56.7±0.5 test set of CIFAR100 for comparison. TENT+TBR 35.8±2.9 269.8±44.0 62.2±0.4 52.4±9.1 469.2±104.2 57.1±0.8 Figure 3 depicts the per-class number TENT+TBR+DOT 20.4±1.1 122.0±15.2 63.9±0.2 25.5±2.1 164.6±43.0 60.4±0.5 of predictions, while Table 3 shows the corresponding standard deviation, range (maximum subtract minimum), and accuracy. We draw the following five conclusions.\n\nDS+CB\n\nMethod\n\nIS+CB\n\nSTD\n\nSTD\n\nAcc\n\nAcc\n\nR\n\nR\n\n• Predictions are imbalanced, even for a model trained on class-balanced training data and tested on a class-balanced test set with P test(x, y) = P train(x, y): the “clean” curve in Figure 3 (left) with standard deviation 8.3 and range 46. This phenomenon is also studied in Wang et al. (2022b). • Predictions becomes more imbalanced when P test(x, y) ̸= P train(x, y) as shown in Figure 3 (left):\n\nthe ranges are 46 and 956 on the clean and corrupted test set respectively.\n\n• BN adapt+TEMA improves accuracy (from 27.0% to 58.0%) and alleviates the prediction imbal-\n\nance at the same time (the range dropped from 956 to 121.6).\n\n• Though accuracy is further improved with TENT+TBR (from 58.0% to 62.2%), the predictions become more imbalanced inversely (the range changed from 121.6 to 269.8). The entropy minimization loss focuses on data with low entropy, while samples of some classes may have relatively lower entropy owing to the trained model, thus TENT would aggravate the prediction imbalance. • On dependent test streams, not only the model accuracy drops, but also the predictions become more imbalanced (range 269.8 / range 469.2 on independent/dependent samples for TENT+TBR), as the model may be absolutely dominated by some classes over a period of time in DS+CB scenario.\n\n5\n\n050100Sorted classes050100150200250300# Predictions050100Sorted classes050100150200250300# Predictions050100Sorted classes050100150200250300# Predictions050100Sorted classes050100150200250300# Predictions[Clean, IS+CB, Source][Gauss, IS+CB, Source][Gauss, IS+CB, BN adapt+TEMA][Gauss, IS+CB, TENT+TBR+DOT][Gauss, IS+CB, TENT+TBR][Gauss, DS+CB, TENT+TBR]Published as a conference paper at ICLR 2023\n\nAlgorithm 1: Dynamic Online reweighTing (DOT) Input: inference step t := 0; test stream samples {xj}; pre-trained model f{θ0,a0}; class-frequency\n\nvector z0; loss function L; smooth coefficient λ.\n\n1 while the test mini-batch {xmt+b}B\n\nb=1 arrives do\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\nt = t + 1 {pmt+b}B for b = 1 to B do\n\nb=1, f{θt−1,at} ← Forward({xmt+b}B\n\nb=1, f{θt−1,at−1}) // output predictions\n\nmt+b = arg maxk∈[1,K] pmt+b[k] // predicted label\n\nk∗ wmt+b = 1/(zt−1[k∗ ̄wmt+b = B · wmt+b/ (cid:80)B l = 1 f{θt,at} ← Backward & Update(l, f{θt−1,at}) // update θ zt ← λzt−1 + (1−λ)\n\nb=1 pmt+b // update z\n\n(cid:80)B\n\n(cid:80)B\n\nB\n\nB\n\nb=1 ̄wmt+b · L(pmt+b) // combine sample weight with loss\n\nmt+b]+ε) // assign sample weight b′=1 wmt+b′ , b = 1, 2, · · · , B // normalize sample weight\n\nThe imbalanced data is harmful during the normal training phase, resulting in biased models and poor overall accuracy (Liu et al., 2019; Menon et al., 2021). Our main motivation is that the test-time adaptation methods also involve gradient-based optimization which is built on the model predictions; however, the predictions are actually imbalanced, particularly for dependent or class-imbalanced streams and the low-entropy-emphasized adaptation methods. Therefore, we argue that the test-time optimization is biased towards some dominant classes actually, resulting in inferior performance. A vicious circle is formed by skewed optimization and imbalanced predictions.\n\nTreatment II: Dynamic online re-weighting (DOT) can alleviate the biased optimization. Many methods have been developed to deal with class imbalance during the training phase, but they face several challenges when it comes to fully test-time adaptation: (i) Network architectures are immutable. (ii) Because test sample class frequencies are dynamic and agnostic, the common constraint of making the output distribution uniform (Liang et al., 2020) is no longer reasonable. (iii) Inference and adaptation must occur in real-time when test mini-batch arrived (only a single pass through test data, no iterative learning).\n\nGiven these constraints, we propose DOT as presented in Algorithm 1. DOT is mainly derived from class-wise re-weighting (Cui et al., 2019). To tackle the dynamically changing and unknown class frequencies, we use a momentum-updated class-frequency vector z ∈ RK instead (Line 10 of Algorithm 1), which is initiated with z[k] = 1 K , k = 1, 2, · · · , K. For each inference step, we assign weights to each test sample based on its pseudo label and the current z (Line 5,6 of Algorithm 1). Specifically, when z[k] is relatively large, during the subsequent adaptation, DOT will reduce the contributions of the kth class samples (pseudo label) and emphasize others. It is worth noting that DOT can alleviate the biased optimization caused by the pre-trained model (e.g., inter-class similarity), test stream (e.g., class-imbalanced scenario) simultaneously.\n\nDOT is a general idea to tackle the biased optimization, some parts in Algorithm 1 have multiple options, so it can be combined with different existing test-time adaptation techniques. For the “Forward (·)” function (Line 3 of Algorithm 1), the discussed BN adapt and TBR can be incorporated. For the loss function L(·) (Line 8 of Algorithm 1), studies usually employ the entropy minimization loss: L(pb) = − (cid:80)K k=1 pb[k] log pb[k] or the cross-entropy loss with pseudo labels: L(pb) = −Ipb[k∗ b ]≥τ · log pb[k∗ b ] (commonly, only samples with high prediction confidence are utilized, τ is a pre-defined threshold). Similarly, for entropy minimization, EntW (Niu et al., 2022) also discards the high-entropy samples and emphasizes the low-entropy ones: L(pb) = −IHb<τ · eτ −Hb · (cid:80)K\n\nk=1 pb[k] log pb[k], where Hb is the entropy of sample xb.\n\n4\n\nEXPERIMENTS\n\nDatasets and models. We conduct experiments on common datasets CIFAR100-C, ImageNetC (Hendrycks & Dietterich, 2019), ImageNet-R (Hendrycks et al., 2021), and a newly introduced video (segments) dataset: the subset of YouTube-BoundingBoxes (YTBB-sub) (Real et al., 2017). CIFAR100-C / ImageNet-C contains 15 corruption types, each with 5 severity levels; we use the\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nhighest level unless otherwise specified. ImageNet-R contains various styles (e.g., paintings) of ImageNet categories. Following Wang et al. (2022a); Niu et al. (2022), for evaluations on CIFAR100-C, we adopt the trained ResNeXt-29 (Xie et al., 2017) model from Hendrycks et al. (2020) as f{θ0,a0}; for ImageNet-C / -R, we use the trained ResNet-50 model from Torchvision. The models are trained on the corresponding original training data. For YTBB-sub, we use a ResNet-18 trained on the related images of COCO. Details of the tasks, datasets and examples are provided in Appendix A.1.\n\nMetrics. Unless otherwise specified, we report the mean accuracy over classes (Acc, %) (Liu et al., 2019); results are averaged over 15 different corruption types for CIFAR100-C and ImageNet-C in the main text, please see detailed performance on each corruption type in Appendix A.5, A.6.\n\nImplementation. The configurations are mainly followed previous work Wang et al. (2021; 2022a); Niu et al. (2022) for comparison, details are listed in Appendix A.3. Code is available online.\n\nBaselines. We adopt the following SOTA methods as baselines: pseudo label (PL) (Lee et al., 2013), test-time augmentation (TTA) (Ashukha et al., 2020), BN adaptation (BN adapt) (Schneider et al., 2020; Nado et al., 2020), test-time entropy minimization (TENT) (Wang et al., 2021), marginal entropy minimization with one test point (MEMO) (Zhang et al., 2021), efficient test-time adaptation (ETA) (Niu et al., 2022), entropy-based weighting (Ent-W) (Niu et al., 2022), laplacian adjusted maximum-likelihood estimation (LAME) (Boudiaf et al., 2022), continual test-time adaptation (CoTTA/CoTTA*: w/wo resetting) (Wang et al., 2022a). We combine DELTA with PL, TENT, and Ent-W in this work.\n\nTable 4: Acc in IS+CB scenario.\n\nMethod CIFAR100-C ImageNet-C\n\n53.5±0.00 –\n\nSource TTA BN adapt 64.6±0.03 MEMO – 69.3±0.14 ETA LAME 50.8±0.06 CoTTA 65.5±0.04 CoTTA* 67.3±0.13 68.0±0.13 PL +DELTA 68.7±0.12\n\n+0.7 TENT 68.7±0.16 +DELTA 69.5±0.03\n\n18.0±0.00 17.7 31.5±0.02 23.9 48.0±0.06 17.2±0.01 34.4±0.11 34.8±0.53 40.2±0.11 41.8±0.03 +1.6 42.7±0.03 45.1±0.03 +2.4 44.3±0.41 49.9±0.05 +5.6\n\nEvaluation in IS+CB scenario. The results on CIFAR100-C are reported in Table 4. As can be seen, the proposed DELTA consistently improves the previous adaptation approaches PL (gain 0.7%), TENT (gain 0.8%), and Ent-W (gain 0.8%), achieving new state-of-the-art performance. The results also indicate that current test-time adaptation methods indeed suffer from the discussed drawbacks, and the proposed methods can help them obtain superior performance. Then we evaluate the methods on the more challenging dataset ImageNet-C. Consistent with the results on CIFAR100C, DELTA remarkably improves the existing methods. As the adaptation batch size (64) is too small compared to the class number (1,000) on ImageNet-C, the previous methods undergo more severe damage than on CIFAR100-C. Consequently, DELTA achieves greater gains on ImageNet-C: 1.6% gain over PL, 2.4% gain over TENT, and 5.6% gain over Ent-W.\n\nEnt-W 69.3±0.15 +DELTA 70.1±0.05\n\n+0.8\n\n+0.8\n\n0.5\n\n1.0\n\n0.1\n\n0.5\n\n1.0\n\n0.1\n\nMethod\n\nImageNet-C\n\nCIFAR100-C\n\nTable 5: Acc in DS+CB scenario with varying ρ.\n\nEvaluation in DS+CB scenario. To simulate dependent streams, following Yurochkin et al. (2019), we arrange the samples via the Dirichlet distribution with a concentration factor ρ > 0 (the smaller ρ is, the more concentrated the same-class samples will be, which is detailed in Appendix A.1). We test models with ρ ∈ {1.0, 0.5, 0.1}. The experimental results are provided in Table 5 (we provide the results of more extreme cases with ρ = 0.01 in Appendix A.4). The representative test-time adaptation methods suffer from performance degradation in the dependent scenario, especially on data sampled with small ρ. DELTA successfully helps models adapt to environments across different concentration factors. It is worth noting that DELTA’s DS+CB results are close to the IS+CB results, e.g., TENT+DELTA achieves 69.5% and 68.5% accuracy on IS+CB and DS+CB (ρ = 0.5) test streams from CIFAR100-C.\n\nSource 53.5±0.00 53.5±0.00 53.5±0.00 18.0±0.00 18.0±0.00 18.0±0.00 BN adapt 53.0±0.48 49.0±0.32 35.2±0.64 21.8±0.12 19.2±0.09 12.1±0.13 55.4±0.63 50.5±0.34 34.5±0.83 27.6±0.31 22.4±0.20 9.7±0.24 ETA LAME 60.3±0.25 61.8±0.26 65.4±0.41 21.9±0.03 22.7±0.05 24.7±0.03 CoTTA 53.8±0.51 50.0±0.23 36.3±0.63 23.4±0.15 20.5±0.05 12.6±0.15 CoTTA* 54.1±0.65 50.2±0.23 36.1±0.71 23.5±0.27 20.3±0.55 12.8±0.26 PL 54.9±0.54 50.1±0.29 34.8±0.76 25.9±0.18 22.5±0.14 13.0±0.09 +DELTA 68.0±0.25 67.5±0.30 66.0±0.45 40.5±0.05 39.9±0.07 37.3±0.10 +31.2\n\nTENT 54.6±0.52 49.7±0.40 33.7±0.70 26.0±0.20 22.1±0.12 12.1±0.10 +DELTA 68.9±0.20 68.5±0.40 67.1±0.47 43.7±0.06 43.1±0.07 40.3±0.06 +33.4 Ent-W 55.4±0.63 50.5±0.35 34.5±0.83 17.4±0.40 13.0±0.22 4.1±0.22 +DELTA 69.4±0.22 68.8±0.35 67.1±0.45 48.3±0.12 47.4±0.04 43.2±0.11 +32.6\n\n+18.3\n\n+28.2\n\n+17.7\n\n+39.1\n\n+14.6\n\n+21.0\n\n+17.4\n\n+34.4\n\n+17.4\n\n+24.3\n\n+14.0\n\n+13.1\n\n+30.9\n\n+18.8\n\n+14.3\n\nEvaluation in IS+CI and DS+CI scenarios. Following Cui et al. (2019), we resample the test samples with an imbalance factor π (the smaller π is, the more imbalanced the test data will be,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Mean acc in IS+CI, DS+CI scenarios with different π.\n\nIS+CI\n\nDS+CI (ρ = 0.5)\n\nMethod\n\nCIFAR100-C\n\nImageNet-C\n\nCIFAR100-C\n\nImageNet-C\n\n0.1\n\n0.05\n\n0.1\n\n0.05\n\n0.1\n\n0.05\n\n0.1\n\n0.05\n\n53.3±0.00 53.3±0.00 17.9±0.00 17.9±0.00 53.3±0.00 53.3±0.00 17.9±0.00 17.9±0.00 Source BN adapt 64.3±0.16 64.2±0.48 31.5±0.24 31.4±0.19 49.8±0.47 49.9±0.63 20.0±0.22 20.5±0.22 68.2±0.24 68.2±0.59 47.4±0.23 47.1±0.18 51.1±0.45 51.0±0.54 21.7±0.52 21.0±0.40 ETA LAME 50.6±0.18 50.8±0.39 17.2±0.10 17.2±0.07 60.4±0.34 59.6±0.43 21.8±0.12 21.5±0.07 CoTTA 65.1±0.13 65.1±0.58 34.2±0.26 34.2±0.16 50.5±0.47 50.5±0.60 21.4±0.21 22.0±0.26 CoTTA* 67.0±0.17 66.9±0.66 34.6±0.78 34.3±0.51 50.7±0.52 50.6±0.63 21.6±0.56 22.1±0.24 67.2±0.21 67.3±0.57 39.4±0.21 39.3±0.18 50.7±0.41 50.6±0.53 22.8±0.35 23.1±0.25 PL +DELTA 67.6±0.36 67.6±0.46 40.9±0.26 40.7±0.22 66.6±0.39 66.3±0.57 38.8±0.27 38.5±0.21\n\nTable 7: Results on in-distribution test set of CIFAR100.\n\nMethod\n\nAccuracy\n\n78.9±0.00 76.1±0.15 78.5±0.16\n\nSource BN adapt TENT +DELTA 78.9±0.03 (+0.4) Ent-W +DELTA 79.1±0.09 (+0.5)\n\n78.6±0.19\n\n+0.4 TENT 67.7±0.29 67.7±0.58 42.2±0.26 42.0±0.21 50.3±0.41 50.2±0.56 22.3±0.25 22.5±0.23 +DELTA 68.5±0.31 68.6±0.60 44.4±0.25 44.2±0.22 67.7±0.41 67.5±0.70 42.1±0.28 41.9±0.24\n\n+15.4\n\n+15.9\n\n+15.7\n\n+16.0\n\n+1.4\n\n+0.3\n\n+1.5\n\n+0.8\n\n+0.9\n\n+2.2\n\n+2.2\n\n+17.4\n\n+17.3\n\n+19.8\n\n+19.4\n\nEnt-W 68.3±0.26 68.2±0.58 40.8±0.76 39.5±0.82 51.1±0.44 51.0±0.53 11.3±0.81 10.8±0.40 +DELTA 69.1±0.25 69.2±0.53 48.4±0.31 47.7±0.21 68.0±0.30 67.8±0.60 45.4±0.53 44.8±0.24\n\n+0.8\n\n+1.0\n\n+7.6\n\n+8.2\n\n+16.9\n\n+16.8\n\n+34.1\n\n+34.0\n\nFigure 4: Across architecture.\n\nwhich is detailed in Appendix A.1). We test models with π ∈ {0.1, 0.05} (similarly, we show the extreme experiments with π = 0.001 in Appendix A.4). Table 6 summarizes the results in IS+CI and DS+CI scenarios, with the following observations: (i) Under class-imbalanced scenario, the performance degradation is not as severe as under dependent data. This is primarily because the imbalanced test data has relatively little effect on the normalization statistics. DELTA works well on the imbalanced test stream. (ii) The hybrid DS+CI scenario can be more difficult than the individual scenarios. DELTA can also boost baselines in the hybrid scenario. (iii) Though the low-entropyemphasized method Ent-W improves TENT in IS+CB scenario (Table 4), it can be inferior to TENT in dependent or class-imbalanced scenarios (the results on ImageNet-C in Table 5,6). The reason is that Ent-W leads to a side effect — amplifying the class bias, which would neutralize or even overwhelm its benefits. DELTA eliminates Ent-W’s side effects while retaining its benefits, so EntW+DELTA always significantly outperforms TENT+DELTA.\n\nEvaluation on realistic out-of-distribution datasets ImageNet-R and YTBB-sub. ImageNet-R is inherently class-imbalanced and consists of mixed variants such as cartoon, art, painting, sketch, toy, etc. As shown in Table 8, DELTA also leads to consistent improvement on it. While compared to ImageNet-C, ImageNet-R is collected individually, which consists of more hard cases that are still difficult to recognize for DELTA, the gain is not as great as on ImageNet-C. For YTBB-sub, dependent and class-imbalanced samples are encountered naturally. We see that classical methods suffer from severe degradation, whereas DELTA assists them in achieving good performance.\n\nTable 8: Mean acc on ImageNet-R and YTBB-sub.\n\nMethod\n\nImageNet-R YTBB-sub\n\nSource 38.4±0.00 BN adapt 41.9±0.15 48.3±0.37 ETA TENT 44.7±0.23 +DELTA 45.3±0.08\n\n+0.6\n\nEnt-W 48.3±0.26 +DELTA 49.6±0.09\n\n+1.3\n\n74.0±0.00 51.4±0.29 51.5±0.32 51.7±0.27 75.7±0.21 +24.0 51.5±0.28 76.2±0.23 +24.7\n\nEvaluation on in-distribution test data. A qualified FTTA method should be “safe” on indistribution datasets, i.e., P test(x, y) = P train(x, y). According to Table 7, (i) DELTA continues to improve performance, albeit slightly; (ii) most adaptation methods can produce comparable results to Source, and the combination with DELTA even outperforms Source on in-distribution data.\n\nEvaluation with different architectures. Figure 4 indicates that DELTA can help improve previous test-time adaptation methods with different model architectures. More analyses (e.g., evaluations with small batch size, different severity levels) are provided in Appendix A.4.\n\nContribution of each component of DELTA. DELTA consists of two tools: TBR and DOT. In Table 9, we analyze their contributions on the basis of TENT with four scenarios and two datasets. Row #1 indicates the results of TENT. Applying either TBR or DOT alone on TENT brings gain in most scenarios and datasets. While, we find that TBR achieves less improvement when the test stream is IS+CB and the batch size is large (e.g., performing adaptation with TBR alone on the IS+CB data of CIFAR100-C with batch size of 200 does not improve TENT). However, when the batch size is relatively small (e.g., ImageNet-C, batch size of 64), the benefits of TBR will become apparent. More importantly, TBR is extremely effective and necessary for dependent samples.\n\n8\n\nResNet18ResNet50ResNet101ResNet152WideResNet50ResNeXt5035.037.540.042.545.047.550.052.555.0Accuracy (%)TENTTENT+DELTAEnt-WEnt-W+DELTAPublished as a conference paper at ICLR 2023\n\nDOT can consistently promote TENT or TENT+TBR in all scenarios, especially when the class number is large. These results demonstrate that both the inaccurate normalization statistics and the biased optimization are detrimental, TBR and DOT can effectively alleviate them.\n\nTable 9: Ablation on the effectiveness of each component (on top of TENT) measured in various scenarios: IS+CB, DS+CB (ρ=0.5), IS+CI (π=0.1), DS+CI (ρ=0.5, π=0.05).\n\n# TBR DOT\n\nCIFAR100-C\n\nImageNet-C\n\nIS+CB DS+CB IS+CI DS+CI\n\nIS+CB DS+CB IS+CI DS+CI\n\n68.7±0.16 49.7±0.40 67.7±0.29 50.2±0.56 42.7±0.03 22.1±0.12 42.0±0.21 22.5±0.23 1\n2 ✓ 68.9±0.03 67.4±0.41 67.9±0.27 66.6±0.72 43.4±0.05 40.9±0.11 42.8±0.25 39.6±0.24 ✓ 69.1±0.07 50.6±0.37 68.1±0.27 51.0±0.60 44.3±0.02 23.7±0.17 43.9±0.25 24.8±0.26 3\n4 ✓ ✓ 69.5±0.03 68.5±0.40 68.5±0.31 67.5±0.70 45.1±0.03 43.1±0.07 44.2±0.22 41.9±0.24\n\n– –\n\n– –\n\n– –\n\n– –\n\nMethod\n\nImageNet-C\n\nCIFAR100-C\n\nIS+CB DS+CB IS+CI DS+CI\n\nIS+CB DS+CB IS+CI DS+CI\n\nTable 10: Ablation on different techniques for class imbalance (on top of Ent-W+TBR) measured in various scenarios (same as in Table 9).\n\nDiv-W (0.05) 67.5±0.12 68.1±0.30 66.8±0.31 67.1±0.59 48.8±0.02 45.1±0.25 47.9±0.25 41.0±0.49 Div-W (0.1) 69.3±0.09 68.6±0.34 68.3±0.30 67.6±0.53 48.4±0.08 43.0±0.28 47.7±0.29 39.6±0.56 Div-W (0.2) 69.7±0.10 68.2±0.37 68.6±0.28 67.4±0.61 46.4±0.46 40.3±0.18 46.5±0.38 37.5±0.48 Div-W (0.4) 69.7±0.08 68.0±0.41 68.4±0.23 67.2±0.63 43.6±0.54 37.5±0.35 44.1±0.47 35.1±0.54 70.0±0.06 66.9±0.36 69.0±0.27 66.4±0.63 42.2±0.73 28.6±0.57 43.1±0.73 27.5±0.86 LA 47.6±1.11 39.9±0.94 46.6±0.62 36.5±1.21 KL-div (1e2) KL-div (1e3) 48.9±0.07 27.7±0.36 43.1±0.30 22.5±0.60 Sample-drop 70.1±0.08 68.7±0.34 69.0±0.26 67.5±0.55 49.5±0.06 46.9±0.09 48.2±0.34 42.6±0.28 70.1±0.05 68.8±0.35 69.1±0.25 67.8±0.60 49.9±0.05 47.4±0.04 48.4±0.31 44.8±0.24 DOT\n\nComparing DOT with other techniques for class imbalance. On the basis of Ent-W+TBR, Table 10 compares DOT against the following strategies for solving class imbalance. Diversity-based weight (DivW) (Niu et al., 2022) computes the cosine similarity between the arrived test samples’ prediction and a moving average one like z, then only employs the samples with low similarity to update model. Although the method is proposed to reduce redundancy, we find it can resist class imbalance too. The method relies on a predefined similarity threshold to determine whether to use a sample. We report the results of Div-W with varying thresholds (shown in parentheses). We observe that the threshold is very sensitive and the optimal value varies greatly across datasets. Logit adjustment (LA) (Menon et al., 2021) shows strong performance when training on imbalanced data. Following Wang et al. (2022b), we can perform LA with the estimated class-frequency vector z in test-time adaptation tasks. While we find that LA does not show satisfactory results here. We speculate that this is because the estimated class distribution is not accurate under the one-pass adaptation and small batch size, while LA requires a high-quality class distribution estimate. KL divergence regularizer (KL-div) (Mummadi et al., 2021) augments loss function to encourage the predictions of test samples to be uniform. While, this is not always reasonable for TTA, e.g., for the class-imbalanced test data, forcing the outputs to be uniform will hurt the performance conversely. We examine multiple regularization strength options (shown in parentheses) and report the best two. The results show that KL-div is clearly inferior in dependent or class-imbalanced scenarios. We further propose another strategy called Sample-drop. It records the (pseudo) categories of the test samples that have been employed, then Sample-drop will directly discard a newly arrived test sample (i.e., not use the sample to update the model) if its pseudo category belongs to the majority classes among the counts. This simple strategy is valid but inferior to DOT, as it completely drops too many useful samples.\n\nImpacts of α in TBR and λ in DOT. Similar to most exponential-moving-average-based methods, when the smoothing coefficient α (or λ) is too small, the adaptation may be unstable; when α (or λ) is too large, the adaptation would be slow. Figure 5 provides the ablation studies of α (left) and λ (right) on the DS+CB (ρ = 0.5) samples of CIFAR100-C (from the validation set). We find that TBR and DOT perform reasonably well under a wide range of α and λ.\n\nFigure 5: Impacts of α and λ.\n\n5 CONCLUSION\n\nIn this paper, we expose the defects in test-time adaptation methods which cause suboptimal or even degraded performance, and propose DELTA to mitigate them. First, the normalization statistics used in BN adapt are heavily influenced by the current test mini-batch, which can be one-sided and highly fluctuant. We introduce TBR to improve it using the (approximate) global statistics. Second, the optimization is highly skewed towards dominant classes, making the model more biased. DOT alleviates this problem by re-balancing the contributions of each class in an online manner. The combination of these two powerful tools results in our plug-in method DELTA, which achieves improvement in different scenarios (IS+CB, DS+CB, IS+CI, and DS+CI) at the same time.\n\n9\n\n0.800.850.900.951.005560657075Accuracy (%)SourceTENTTENT+DELTA0.800.850.900.951.005560657075Accuracy (%)SourceTENTTENT+DELTAPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work is supported in part by the National Natural Science Foundation of China under Grant 62171248, the R&D Program of Shenzhen under Grant JCYJ20220818101012025, the PCNL KEY project (PCL2021A07), and Shenzhen Science and Technology Innovation Commission (Research Center for Computer Network (Shenzhen) Ministry of Education).\n\nREFERENCES\n\nShaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed recognition via weight balancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6897–6907, 2022.\n\nArsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=BJxI5gHKDr.\n\nMalik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online testtime adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8344–8353, 2022.\n\nDian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In\n\nCVPR, 2022.\n\nYin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9268–9277, 2019.\n\nCharles Elkan. The foundations of cost-sensitive learning.\n\nIn International joint conference on\n\nartificial intelligence, volume 17, pp. 973–978. Lawrence Erlbaum Associates Ltd, 2001.\n\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ̧ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016.\n\nArthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Sch ̈olkopf, and Alex Smola. A In B. Sch ̈olkopf, J. Platt, and T. HoffInformation Processing Systems, volume 19. MIT URL https://proceedings.neurips.cc/paper/2006/file/\n\nkernel method for the two-sample-problem. man (eds.), Advances Press, 2006. e9fb2eda3d9c55a0d89c98d6c54b5b3e-Paper.pdf.\n\nin Neural\n\nYin-Yin He, Jianxin Wu, and Xiu-Shen Wei. Distilling virtual examples for long-tailed recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 235–244, 2021.\n\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HJz6tiCqYm.\n\nDan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. Proceedings of the International Conference on Learning Representations (ICLR), 2020.\n\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical In Proceedings of the IEEE/CVF International analysis of out-of-distribution generalization. Conference on Computer Vision, pp. 8340–8349, 2021.\n\nJudy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International conference on machine learning, pp. 1989–1998. Pmlr, 2018.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nSergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized\n\nmodels. Advances in neural information processing systems, 30, 2017.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448–456. PMLR, 2015.\n\nBingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi Feng. Exploring balanced feature spaces for\n\nrepresentation learning. In International Conference on Learning Representations, 2020a.\n\nBingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. In International Conference on Learning Representations, 2020b. URL https://openreview.net/ forum?id=r1gRTCVFvB.\n\nJogendra Nath Kundu, Naveen Venkat, Rahul M V, and R. Venkatesh Babu. Universal source-free\n\ndomain adaptation. June 2020.\n\nDong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896, 2013.\n\nLei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xiaoyue Mi, Zhengze Yu, Xiaoya Li, and Boyang Xia. Progressive domain expansion network for single domain generalization. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 224–233, 2021.\n\nJian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning, pp. 6028–6039. PMLR, 2020.\n\nZiwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Largescale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2537–2546, 2019.\n\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint adaptation networks. In International conference on machine learning, pp. 2208–2217. PMLR, 2017.\n\nMingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial\n\ndomain adaptation. Advances in neural information processing systems, 31, 2018.\n\nAditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=37nvvqkCo5.\n\nChaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-time adaptation to distribution shift by confidence maximization and input transformation. arXiv preprint arXiv:2106.14999, 2021.\n\nZachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020.\n\nShuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 16888–16905. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr. press/v162/niu22a.html.\n\nJoaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence.\n\nDataset shift in machine learning. Mit Press, 2008.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nEsteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, and Vincent Vanhoucke. Youtubeboundingboxes: A large high-precision human-annotated data set for object detection in video. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7464–7473, 2017.\n\nMengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In International conference on machine learning, pp. 4334–4343. PMLR, 2018.\n\nKuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric tri-training for unsupervised domain adaptation. In International Conference on Machine Learning, pp. 2988–2997. PMLR, 2017.\n\nSteffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Improving robustness against common corruptions by covariate shift adaptation. Ad-\n\nBethge. vances in Neural Information Processing Systems, 33:11539–11551, 2020.\n\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c.\n\nQin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022a.\n\nTao Wang, Yu Li, Bingyi Kang, Junnan Li, Junhao Liew, Sheng Tang, Steven Hoi, and Jiashi Feng. The devil is in classification: A simple framework for long-tail instance segmentation. In European conference on computer vision, pp. 728–744. Springer, 2020.\n\nXudong Wang, Zhirong Wu, Long Lian, and Stella X Yu. Debiased learning from naturally imbalanced pseudo-labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14647–14657, 2022b.\n\nSaining Xie, Ross Girshick, Piotr Doll ́ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1492–1500, 2017.\n\nShiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 8958–8967, 2021a. doi: 10.1109/ICCV48922.2021.00885.\n\nShiqi Yang, Yaxing Wang, Joost van de weijer, Luis Herranz, and SHANGLING JUI. Exploiting the intrinsic neighborhood structure for source-free domain adaptation. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021b. URL https://openreview.net/forum?id=ueGDv64HmO.\n\nMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference on Machine Learning, pp. 7252–7261. PMLR, 2019.\n\nWerner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschl ̈ager, and Susanne Saminger-Platz. Central moment discrepancy (CMD) for domain-invariant representation learnIn International Conference on Learning Representations, 2017. URL https:// ing. openreview.net/forum?id=SkB-_mcel.\n\nM. Zhang, S. Levine, and C. Finn. MEMO: Test time robustness via adaptation and augmentation.\n\n2021.\n\nBowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13208–13217, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nArt\n\nCartoon\n\nDeviantart\n\nGraffiti\n\nGraphic\n\nMisc\n\nOrigami\n\nPainting\n\nSculpture\n\nSketch\n\nSticker\n\nTattoo\n\nToy\n\nVideogame\n\nFigure 6: Different renditions of class n01694178 (African chameleon) from ImageNet-R.\n\nA APPENDIX\n\nA.1 DATASETS\n\nExamples of ImageNet-R and ImageNet-C are shown in Figure 6 and Figure 7 respectively. ImageNet-R Hendrycks et al. (2021) holds a variety of renditions (sketches, graphics, paintings, plastic objects, cartoons, graffiti, origami, patterns, deviantart, plush objects, sculptures, art, tattoos, toys, embroidery, video game) of 200 ImageNet classes, resulting in 30,000 images. CIFAR100-C and ImageNet-C are established in Hendrycks & Dietterich (2019). CIFAR100-C contains 10,000 images with 15 corruption types: Gaussian Noise (abbr. Gauss), Shot Noise (Shot), Impulse Noise (Impul), Defocus Blur (Defoc), Frosted Glass Blur (Glass), Motion Blur (Motion), Zoom Blur (Zoom), Snow, Frost, Fog, Brightness (Brit), Contrast (Contr), Elastic, Pixelate (Pixel), JPEG. There are 50,000 images for each corruption type in ImageNet-C, others are the same as CIFAR100-C.\n\nFor the real-word applications with dependent and class-imbalanced test samples, we consider an automatic video content moderation task (e.g., for the short-video platform), which needs to recognize the categories of interest from the extracted frames. It is exactly a natural DS+CI scenario. We collect 1686 test videos from YouTube, which are annotated in YouTube-BoundingBoxes dataset. 49006 video segments are extracted from these videos and form the test stream in this experiment, named YTBB-sub here. We consider 21 categories. For the trained model, we adopt a model (ResNet18) trained on the related images from COCO dataset. Thus, there is a natural difference between the training domain and test domain. The consecutive video segments form the natural dependent samples (an object usually persists over several frames) as shown in Figure 8. Moreover, the test class distribution is also skewed naturally as shown in Figure 8. To simulate dependent test samples, for each class, we sample qk ∼ DirJ (ρ), qk ∈ RJ and allocate a qk,j proportion of the kth class samples to piece j, then the J pieces are concatenated to form a test stream in our experiments (J is set to 10 for all experiments); ρ > 0 is a concentration factor, when ρ is small, samples belong to the same category will concentrate in test stream.\n\nTo simulate class-imbalanced test samples, we re-sample data points with an exponential decay in frequencies across different classes. We control the degree of imbalance through an imbalance factor π, which is defined as the ratio between sample sizes of the least frequent class and the most frequent class.\n\nFor DS+CI scenario, we mimic a class-imbalanced test set first, then the final test samples are dependently sampled from it.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nBrightness\n\nJPEG\n\nPixelate\n\nElastic\n\nContrast\n\nFog\n\nFrost\n\nSnow\n\nZoom Blur\n\nMotion Blur\n\nFrosted Glass Blur\n\nDefocus Blur\n\nShot Noise\n\nImpulse Noise\n\nGaussian Noise\n\nFigure 7: Different corruption types of class n01694178 (African chameleon) from ImageNet-C.\n\n−→\n\n(a) The natural dependent samples in YTBB-sub. Each bar represents a sample, each color represents a category. The videos can be found at “https://www.youtube.com/watch?v={the above video ID}”.\n\n(b) The test class distribution.\n\nFigure 8: Characters of YTBB-sub dataset.\n\nA.2\n\nTHE ALGORITHM DESCRIPTION OF TBR\n\nWe present the detailed algorithm description of TBR in Algorithm 2.\n\n14\n\nboatpotted planttruckcartraincowboatbuscow36uMLT9BKYA1WsbZj-NsfQ0Wigb079iMk0N7yCdf7DPs3u7iTx8CViY1ceprZO-VEU0Neg9vT08to08u9yvYwrTc17Z_zMVLeqU3ZyPIcwx_n82yDeK7WyDUM0vK_6B2ikEAhttps://www.youtube.com/watch?v={theabovevideoID}videoID:category:01020Class ID01000200030004000# SamplesPublished as a conference paper at ICLR 2023\n\nAlgorithm 2: Test-time Batch Renormalization (TBR) module Input: mini-batch test features v ∈ RB×C×S×S′\n\nwith batch size B, C channels, height S and width S′;\n\nlearnable affine parameters γ ∈ RC , β ∈ RC ; current test-time moving mean ˆμema ∈ RC and standard deviation ˆσema ∈ RC ; smoothing coefficient α.\n\n1 ˆμbatch[c] = 1\n\n2 ˆσbatch[c] =\n\n(cid:80)\n\nBSS′ (cid:113) 1\n\nBSS′\n\nb,s,s′ v[b, c, s, s′], c = 1, 2, · · · , C // get mean (for each channel) (cid:80) b,s,s′ (v[b, c, s, s′] − ˆμbatch[c])2 + ε, c = 1, 2, · · · , C // get standard\n\ndeviation (for each channel)\n\nˆσema\n\nˆσema\n\n// get r\n\n// get d\n\n3 r = sg(ˆσbatch) 4 d = sg( ˆμbatch)− ˆμema 5 v∗ = v− ˆμbatch 6 v⋆ = γ · v∗ + β // scale and shift 7 ˆμema ← α · ˆμema + (1 − α) · sg(ˆμbatch) // update ˆμema 8 ˆσema ← α · ˆσema + (1 − α) · sg(ˆσbatch) // update ˆσema\n\n· r + d // normalize\n\nˆσbatch\n\nOutput: v⋆, ˆμema, ˆσema\n\nA.3\n\nIMPLEMENTATIONS\n\nWe use Adam optimizer with learning rate of 1e-3, batch size of 200 for CIFAR100-C; SGD optimizer with learning rate of 2.5e-4, batch size of 64 for ImageNet-C/-R; SGD optimizer with learning rate of 2.5e-4, batch size of 200 for YTBB-sub. For DELTA, the hyper-parameters α and λ are roughly selected from {0.9, 0.95, 0.99, 0.999} on validation sets, e.g., the extra sets with corruption types outside the 15 types used in the benchmark. The smoothing coefficient α in TBR is set to 0.95 for CIFAR100-C and ImageNet-C/-R, 0.999 for YTBB-sub, λ in DOT is set to 0.95 for ImageNet-C/-R and 0.9 for CIFAR100-C / YTBB-sub.\n\nThen, we summarize the implementation details of the compared methods here, including BN adapt, PL, TENT, LAME, ETA, Ent-W, and CoTTA (CoTTA*). Unless otherwise specified, the optimizer, learning rate, and batch size are the same as those described in the main paper. For BN adapt, we follow the operation in Nado et al. (2020) and the official code of TENT (https://github.com/ DequanWang/tent), i.e., using the test-time normalization statistics completely. Though one can introduce a hyper-parameter to adjust the trade-off between current statistics and those inherited from the trained model (a0) (Schneider et al., 2020), we find this strategy does not lead to significant improvement and its effect varies from dataset to dataset. For PL and TENT, besides the normalization statistics, we update the affine parameters in BN modules. The confidence threshold in PL is set to 0.4, which can produce acceptable results in most cases. We adopt/modify the official implementation https://github.com/DequanWang/tent to produce the results of TENT/PL. For LAME, we use the k-NN affinity matrix with 5 nearest neighbors following Boudiaf et al. (2022) and the official implementation https://github.com/fiveai/LAME. For ETA, the entropy constant threshold is set to 0.4 × ln K (K is the number of task classes), and the similarity threshold is set to 0.4/0.05 for CIFAR/ImageNet experiments following the authors’ suggestion and official implementation https://github.com/mr-eggplant/EATA. For Ent-W, the entropy constant threshold is set to 0.4 or 0.5 times ln K. For CoTTA, the used random augmentations include color jitter, random affine, gaussian blur, random horizontal flip, and gaussian noise. 32 augmentations are employed in this method. The learning rate is set to 0.01 for ImageNet experiments following official implementation https://github.com/qinenergy/cotta. The restoration probability is set to 0.01 for CIFAR experiments and 0.001 for ImageNet experiments. The augmentation threshold is set to 0.72 for CIFAR experiments and 0.1 for ImageNet experiments. The exponential-moving-average factor is set to 0.999 for all experiments. CoTTA optimizes all learnable parameters during adaptation.\n\nA.4 ADDITIONAL ANALYSIS\n\nFully test-time adaptation with small (test) batch size. In the main paper, we report results with the default batch size following previous studies. Here, we study test-time adaptation with a much smaller batch size. The small batch size brings two serious challenges: the normalization statistics can be inaccurate and fluctuate dramatically; the gradient-based optimization can be noisy. Previ-\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nous study (Niu et al., 2022) employs a sliding window with L samples in total (including L − B previous samples, assuming L > B, L%B = 0 here) to perform adaptation. However, this strategy significantly increases the computational cost: L B × forward and backward, e.g., 64× when B = 1, L = 64. We employ another strategy, called “fast-inference and slow-update”. When the samples arrive, infer them instantly with the current model but do not perform adaptation; the model is updated with the recent L samples every L B mini-batches. Thus, this strategy only needs 2× forward and 1× backward. Note that the two strategies both need to cache some recent test samples, which may be a bit against the “online adaptation”. We evaluate TENT and DELTA on the IS+CB test stream of CIFAR100-C with batch sizes 128, 16, 8, and 1. The results are listed in Table 11. We find that TENT suffers from severe performance degeneration when the batch size is small, which is due to TENT always using the normalization statistics derived from the test mini-batches, thus it is still affected by the small batch size during “fast-inference”. With the assistance of DELTA, the performance degradation can be significantly alleviated: it only drops by 0.7% (from 69.8% to 69.1%) when B = 1.\n\nTable 11: Results (classification accuracy, %) with different batch sizes on IS+CB test stream of CIFAR100-C.\n\nMethod\n\n128\n\n16\n\n8\n\n53.5 Source TENT 68.7 TENT+DELTA 69.8\n\n53.5 64.9 69.4\n\n53.5 59.9 69.0\n\n1\n\n53.5 1.6 69.1\n\n1\n\n, ˆσbatch 1\n\nThe initialization of TBR’s normalization statistics. As described in Section 3.2, TBR keeps the moving normalization statistics ˆμema, ˆσema, we usually have two ways to initialize them: using the statistics ˆμbatch derived from the first test mini-batch (First); using the statistics μema, σema inherited from the trained model (Inherit). In the main paper, we use the “First” initialization strategy. However, it is worth noting that “First” is not reasonable for too small batch size. We perform TENT+DELTA with the above two initialization strategies and different batch sizes on the IS+CB test stream of CIFAR100-C. Figure 9 summaries the results, we can see that when the batch size is too small, using the inherited normalization statistics as initialization is better; when the batch size is acceptable (just > 8 for CIFAR100-C), using the “First” initialization strategy is superior.\n\nFigure 9: Comparison of two TBR initialization strategies on top of TENT+DELTA in IS+CB scenario on CIFAR100-C.\n\nPerformance under different severity levels on CIFAR100-C and ImageNet-C. In the main paper, for CIFAR100-C and ImageNet-C, we report the results with the highest severity level 5 following previous studies. Here, we investigate DELTA on top of TENT with different severity levels on CIFAR100-C (IS+CB scenario). Figure 10 presents the results. We observe that (i) as the corruption level increases, the model accuracy decreases; (ii) DELTA works well under all severity levels.\n\nPerformance in extreme cases. We examine the performance of DELTA with more extreme conditions: DS+CB with ρ = 0.01, IS+CI with π = 0.001. Table 12 shows DELTA can manage the intractable cases.\n\nInfluence of random seeds. As fully test-time adaptation is established based on a pre-trained model, i.e., does not need random initialization; methods like PL, TENT, Ent-W, and our DELTA\n\n16\n\n1281681Batch size60.062.565.067.570.072.5Accuracy (%)InheritFirstPublished as a conference paper at ICLR 2023\n\nFigure 10: Comparison under different severity levels on CIFAR100-C.\n\nTable 12: Performance in extreme cases.\n\nDS+CB (ρ = 0.01)\n\nIS+CI (π = 0.001)\n\nSource BN adapt ETA LAME CoTTA CoTTA* PL +DELTA TENT +DELTA Ent-W +DELTA\n\n18.0 6.8 3.3 26.0 7.0 7.2 6.6 34.2 6.0 36.7 1.4 36.5\n\n17.9 31.1 44.1 17.4 33.5 33.6 37.9 38.9 39.8 41.8 39.9 45.1\n\nalso do not bring random initialization. As a result, the adaptation results are always the same on one fixed test stream. However, the random seeds can affect sample order in our experiments. We study the influence of random seeds on Gauss and Shot data (IS+CB scenario) of ImageNet-C with seeds {2020, 2021, 2022, 2023}. The results of TENT and DELTA are summarized in Table 13, from which one can see the methods are not greatly affected by the sample order within the same scenario. For fair comparison, all methods are investigated under the same sample order for each specific scenario in our experiments.\n\nTable 13: Influence of random seeds. Classification accuracies (%) are reported on two kinds of corrupted data (IS+CB) of ImageNet-C under four random seeds (2020, 2021, 2022, and 2023).\n\nData\n\nTENT\n\nTENT+DELTA\n\n2020\n\n2021\n\n2022\n\n2023\n\n2020\n\n2021\n\n2022\n\n2023\n\nGauss Shot\n\n28.672 30.536\n\n28.434 30.496\n\n28.774 30.370\n\n28.796 30.458\n\n31.186 33.146\n\n30.916 33.140\n\n31.270 33.124\n\n31.208 32.994\n\nAblation on DOT. We examine the performance of DOT with another way to get the sample weights (Line 5,6 in Algorithm 1). One can discard line 5 and modify line 6 to adopt the original soft probabilities: ωmt+b = (cid:80)K k=1 1/(zt−1[k] + ε) · pmt+b[k]. We compare the hard label strategy (Algorithm 1) with the soft one in Table 14 (on the basis of Enw-W+TBR, on ImageNet-C). We find that both strategies work well in all scenarios, demonstrating the effectiveness of the idea of DOT. The performance of the soft strategy is slightly worse than the hard strategy in some scenarios. However, we think it is difficult to say “hard labels are necessarily better than soft labels” or “soft labels are necessarily better than hard labels”, for example, the two strategies both exist in recent semi-supervised methods: hard label in FixMatch, soft label in UDA.\n\n17\n\n12345Severity Level70727476Accuracy (%)TENTTENT+DELTAPublished as a conference paper at ICLR 2023\n\nTable 14: Ablation on DOT.\n\nIS+CB DS+CB DS+CB DS+CB\n\nIS+CI\n\nIS+CI\n\nDS+CI\n\nDS+CI\n\nρ = 1.0\n\nρ = 0.5\n\nρ = 0.1 π = 0.1 π = 0.05 ρ = 0.5, π = 0.1 ρ = 0.5, π = 0.05\n\nHard Soft\n\n49.9 49.7\n\n48.3 48.0\n\n47.4 47.3\n\n43.2 43.0\n\n48.4 48.3\n\n47.7 47.5\n\n45.4 45.1\n\n44.8 44.5\n\nA.5 RESULTS OF EACH CORRUPTION TYPE ON CIFAR100-C.\n\nTable 2 has compared the usages of different normalization statistics, we further provide the detailed results of all corruption types in Table 15.\n\nTable 16 presents the results of all corruption types under different batch sizes and the two initialization strategies for normalization statistics in TBR, the averaged results have been illustrated in Table 11 and Figure 9 respectively.\n\nTable 17 summarises the detailed performance on IS+CB test stream with different severity levels.\n\nTable 18 compares the test-time adaptation methods in IS+CB scenario; Table 19 for DS+CB test stream (ρ = 1.0), Table 20 for DS+CB test stream (ρ = 0.5), Table 21 for DS+CB test stream (ρ = 0.1); Table 22, 23 for IS+CI data with π = 0.1, π = 0.05; Table 24 / Table 25 for DS+CI test data with ρ = 0.5 and π = 0.1 / π = 0.05.\n\nA.6 RESULTS OF EACH CORRUPTION TYPE ON IMAGENET-C.\n\nTable 26 compares the test-time adaptation methods in IS+CB scenario and Table 27 further compares them with different model architectures; Table 28, Table 29, and Table 30 for DS+CB test streams with ρ = 1.0, ρ = 0.5 and ρ = 0.1, respectively; Table 31, 32 for IS+CI data with π = 0.1, π = 0.05; Table 33 / Table 34 for DS+CI test data with ρ = 0.5 and π = 0.1 / π = 0.05. The results in Table 15-Table 34 are obtained with seed 2020.\n\nTable 15: Comparison of the normalization statistics on IS+CB and DS+CB test streams of CIFAR100-C with B = 128 in terms of classification accuracy (%).\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n27.0 32.0 60.6 70.7 45.9\n\nSource IS+CB scenario BN adapt 57.6 59.0 56.9 72.3 58.0 BN adapt+TEMA 58.0 59.7 57.1 72.5 58.6 62.4 64.7 67.3 74.3 62.5 TENT 19.4 14.9 16.4 31.9 14.8 TENT+TEMA 62.1 64.7 67.7 74.6 62.0 TENT+TBR DS+CB scenario BN adapt 24.1 24.7 23.4 30.2 23.2 BN adapt+TEMA 56.0 57.8 55.2 70.7 56.7 21.2 22.7 21.9 26.6 20.0 TENT 18.0 17.3 15.2 34.2 18.6 TENT+TEMA 55.8 60.0 58.8 70.7 57.2 TENT+TBR\n\n69.2\n\n71.2 60.5 54.2 49.7 70.5 44.9 62.8 25.3 58.8 53.5\n\n70.3 70.3 72.4 25.0 72.6\n\n29.9 68.6 25.5 26.3 67.4\n\n71.8 64.8 64.8 58.1 73.3 69.7 64.0 66.7 58.4 64.4 72.5 65.3 65.5 58.3 74.1 70.2 64.4 67.0 59.2 64.9 74.2 69.4 67.6 66.8 75.6 71.8 66.9 71.3 62.6 68.7 28.9 24.3 25.0 19.3 31.1 24.3 25.5 26.0 18.2 23.0 74.0 69.7 67.9 67.8 76.2 71.6 67.1 71.8 63.3 68.9\n\n29.8 26.6 27.2 24.2 30.0 28.6 25.7 27.8 23.8 26.6 70.2 63.2 63.6 56.6 71.7 67.8 62.2 64.8 57.2 62.8 26.6 23.0 22.2 21.7 26.3 21.6 21.7 24.7 20.3 23.1 36.6 18.9 27.2 24.6 36.2 25.8 26.5 28.6 20.4 25.0 69.7 64.4 62.8 60.2 71.5 64.0 60.9 67.1 56.4 63.1\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTable 16: Comparison of different batch sizes and the initialization strategies for TBR’s normalization statistics on IS+CB test stream of CIFAR100-C in terms of classification accuracy (%).\n\nMethod\n\nInit\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n– –\n– –\n–\n\n27.0 32.0 60.6 70.7 45.9 Source TENT, B=128 62.4 64.7 67.3 74.3 62.5 TENT, B=16 58.7 61.0 63.8 70.8 58.7 TENT, B=8 54.0 56.1 58.6 65.9 53.0 TENT, B=1 1.6 1.6 1.5 TENT+DELTA, B=128 Inherit 62.4 63.9 69.0 75.3 63.2 TENT+DELTA, B=128 First 64.0 66.0 69.1 75.3 63.3 TENT+DELTA, B=16 Inherit 62.3 64.0 69.1 75.2 63.1 TENT+DELTA, B=16 First 63.5 65.5 68.2 74.8 63.2 TENT+DELTA, B=8 Inherit 62.4 64.0 69.0 75.2 63.1 TENT+DELTA, B=8 First 63.1 65.1 67.1 74.8 62.4 TENT+DELTA, B=1 Inherit 62.2 64.0 68.9 75.3 63.1 TENT+DELTA, B=1 60.0 62.0 64.4 71.4 59.5 First\n\n1.5\n\n1.6\n\n69.2 72.4 68.8 64.1 1.8 73.2 73.0 73.3 72.7 73.3 72.6 73.2 69.0\n\n1.8\n\n1.6 1.5 1.6 1.6\n\n71.2 60.5 54.2 49.7 70.5 44.9 62.8 25.3 58.8 53.5 74.2 69.4 67.6 66.8 75.6 71.8 66.9 71.3 62.6 68.7 70.3 65.8 64.1 63.3 72.2 66.9 62.7 67.6 59.4 64.9 65.4 61.0 58.6 57.8 67.1 62.9 58.1 62.8 53.8 59.9 1.7 1.6 1.6 74.8 69.8 69.2 66.6 76.0 71.3 67.4 69.7 64.3 69.1 74.6 70.3 69.4 68.1 76.7 72.9 67.6 72.3 64.6 69.8 74.8 69.6 69.3 66.7 76.0 70.8 67.3 69.7 64.3 69.0 74.6 70.2 69.3 67.7 76.2 72.4 67.5 71.9 63.9 69.4 74.8 69.7 69.4 66.6 75.9 71.2 67.3 69.6 64.2 69.0 74.3 69.9 69.2 67.2 75.7 71.2 67.0 71.6 63.0 68.9 74.7 69.7 69.4 66.6 76.1 71.6 67.4 69.6 64.4 69.1 71.4 65.6 65.7 62.9 72.6 64.0 63.6 68.6 59.8 65.4\n\n1.5\n\n1.8\n\nTable 17: Classification accuracy (%) on IS+CB test stream of CIFAR100-C with different severity levels (B = 128).\n\nMethod\n\nLevel Gauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\nTENT\n\nSource\n\n1 2\n3 4\n5 1\n2 3\n4 5\nTENT+DELTA 1 2\n3 4\n5\n\n64.2 70.9 77.6 78.9 54.4 49.3 63.6 75.3 78.1 56.6 36.5 47.2 73.1 76.8 60.6 31.2 40.6 68.0 75.2 39.5 27.0 32.0 60.6 70.7 45.9 73.8 75.8 77.2 77.9 69.4 70.2 73.7 76.0 77.9 69.6 66.7 70.0 74.5 77.4 68.7 64.4 68.3 70.9 76.3 62.6 62.4 64.7 67.3 74.3 62.5 74.4 76.1 78.0 78.7 70.3 70.9 74.7 76.4 78.4 70.3 67.8 70.2 75.3 77.9 69.8 65.6 69.2 72.5 76.9 63.4 64.0 66.0 69.1 75.3 63.3\n\n77.0 75.1 72.3 72.4 69.2 76.6 75.6 73.7 74.2 72.4 77.2 75.8 74.5 74.9 73.0\n\n76.8 76.5 74.2 78.4 78.7 78.2 74.4 76.5 70.5 73.8 76.6 69.9 69.7 76.1 77.7 75.2 75.0 72.3 65.8 70.4 75.4 69.6 62.1 72.3 76.6 71.9 73.7 69.1 64.1 66.8 74.0 65.2 61.1 65.8 74.9 65.7 68.9 52.3 62.5 61.2 71.2 60.5 54.2 49.7 70.5 44.9 62.8 25.3 58.8 53.5 77.1 76.3 75.8 78.0 77.9 77.3 73.3 76.3 71.3 75.6 76.8 73.4 74.0 76.2 77.8 75.2 74.8 76.0 68.5 74.4 76.4 72.1 71.4 74.9 77.3 74.4 73.9 75.6 66.3 72.9 75.3 70.5 70.4 72.2 76.9 74.1 70.5 74.3 65.2 71.1 74.2 69.4 67.6 66.8 75.6 71.8 66.9 71.3 62.6 68.7 77.7 77.1 76.6 78.6 78.5 78.3 74.5 77.1 72.0 76.3 77.4 74.5 74.8 76.9 78.4 76.9 75.4 77.0 69.8 75.2 76.8 73.1 72.5 75.6 78.1 76.6 74.8 76.5 67.6 73.8 76.0 71.0 71.3 73.2 77.9 75.8 71.3 75.1 66.4 72.0 74.6 70.3 69.4 68.1 76.7 72.9 67.6 72.3 64.6 69.8\n\nTable 18: Classification accuracy (%) on IS+CB test stream of CIFAR100-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n27.0 32.0 60.6 70.7 45.9 Source 57.9 59.3 57.3 72.4 58.2 BN adapt 63.2 65.3 66.9 75.1 63.2 ETA 24.1 29.0 59.2 69.0 42.8 LAME 60.0 61.8 60.1 72.6 60.2 CoTTA 60.0 62.2 60.8 73.2 62.3 CoTTA* 61.8 64.5 65.0 74.6 62.0 PL 62.8 64.8 66.3 74.3 62.7 PL+DELTA 62.8 65.4 66.3 74.8 62.3 TENT 62.5 64.9 67.0 74.8 62.1 TENT+TBR 63.6 65.7 66.9 75.1 63.0 TENT+DOT 63.5 65.7 67.8 75.1 63.3 TENT+DELTA Ent-W 63.5 65.5 67.2 75.1 63.2 Ent-W+TBR+Div-W(0.05) 60.3 63.5 63.8 73.5 60.8 63.5 65.3 67.0 75.2 62.7 Ent-W+TBR+Div-W(0.1) 63.8 65.6 68.1 75.3 63.1 Ent-W+TBR+Div-W(0.2) 63.6 65.4 68.2 75.3 63.1 Ent-W+TBR+Div-W(0.4) Ent-W+TBR+LA 64.0 65.9 68.4 75.4 63.5 Ent-W+TBR+Sample-drop 64.1 66.2 68.6 75.8 63.8 64.2 66.1 68.5 75.6 63.6 Ent-W+DELTA\n\n69.2 70.3 73.1 67.0 70.5 71.9 72.1 72.7 72.8 72.9 73.1 73.1 73.1 71.8 72.8 73.4 73.3 73.6 73.5 73.5\n\n19\n\n71.2 60.5 54.2 49.7 70.5 44.9 62.8 25.3 58.8 53.5 72.1 65.1 65.0 58.5 73.5 69.7 64.3 67.1 58.8 64.6 74.9 70.0 69.7 66.9 76.5 73.6 67.7 72.0 64.0 69.5 68.9 58.3 50.7 46.5 67.6 39.2 60.3 21.4 56.7 50.7 72.3 64.8 65.5 56.7 73.6 69.9 64.3 68.4 62.6 65.5 73.7 67.0 67.9 59.8 75.4 72.9 67.5 72.0 66.6 67.5 74.2 68.9 68.4 64.8 75.5 72.0 66.8 70.8 61.9 68.2 74.6 69.4 68.5 65.7 75.5 72.8 66.8 71.3 62.7 68.7 74.6 69.6 68.6 66.8 76.1 72.3 67.3 71.6 63.5 69.0 74.3 69.8 68.3 66.8 76.6 72.0 67.1 71.9 63.0 68.9 74.8 69.8 69.0 67.1 76.2 73.2 67.6 71.8 63.8 69.4 74.7 70.3 69.3 67.4 76.8 72.8 67.8 72.3 63.6 69.6 74.8 70.1 69.8 67.1 76.6 73.5 67.7 72.0 64.1 69.6 73.7 68.6 66.2 63.8 74.9 71.8 66.7 69.9 61.7 67.4 74.7 70.0 69.4 66.7 76.1 73.2 67.1 71.7 63.8 69.3 75.0 70.7 70.0 67.4 77.0 73.5 67.3 72.5 64.1 69.8 75.0 70.8 69.9 67.3 76.9 73.6 67.1 72.6 64.0 69.7 75.1 71.0 70.2 67.6 77.0 73.8 67.6 72.8 64.5 70.0 75.5 70.9 70.2 67.7 77.0 73.9 68.2 72.8 64.4 70.2 75.2 71.2 70.3 68.0 77.1 74.0 68.0 72.8 64.7 70.2\n\nPublished as a conference paper at ICLR 2023\n\nTable 19: Classification accuracy (%) on DS+CB (ρ = 1.0) test stream of CIFAR100-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n27.0 32.0 60.6 70.7 45.9 Source 47.2 47.8 46.2 59.5 47.2 BN adapt 50.1 51.2 52.6 60.3 49.4 ETA 28.0 34.2 68.6 80.1 52.1 LAME 49.1 51.2 49.7 57.7 49.3 CoTTA 49.1 51.3 49.5 57.4 49.8 CoTTA* 49.9 50.5 51.5 60.0 48.3 PL 61.3 62.9 64.4 73.9 61.8 PL+DELTA TENT 49.3 50.7 52.6 59.9 48.7 TENT+DELTA 62.3 64.4 66.7 74.5 62.6 50.0 51.3 52.9 60.3 49.3 Ent-W Ent-W+DELTA 62.7 64.9 67.4 74.6 62.7\n\n69.2 57.4 58.7 78.6 56.8 56.6 58.2 71.7 57.8 72.0 58.9 72.6\n\n71.2 60.5 54.2 49.7 70.5 44.9 62.8 25.3 58.8 53.5 58.8 52.2 53.2 46.7 59.9 57.4 51.9 54.4 46.9 52.4 60.2 55.2 54.7 51.2 61.0 58.0 53.2 56.8 50.1 54.9 80.7 70.5 61.1 57.4 79.3 49.2 73.3 26.1 68.7 60.5 58.6 52.8 53.6 46.6 60.0 53.6 52.6 57.3 50.9 53.3 58.4 53.1 54.1 46.9 59.1 54.2 53.3 57.1 52.8 53.5 60.4 54.2 54.6 50.4 60.7 57.4 53.0 56.5 49.2 54.3 74.0 68.1 68.0 63.9 74.9 71.2 66.2 70.1 62.2 67.6 59.5 53.8 53.5 50.7 60.2 56.8 52.7 56.1 49.4 54.1 74.3 68.9 68.5 65.8 75.6 72.0 66.8 71.4 63.4 68.6 60.3 54.9 54.9 51.1 61.0 57.8 53.1 56.7 50.0 54.8 74.4 69.6 69.2 66.1 75.7 72.4 66.8 71.7 64.2 69.0\n\nTable 20: Classification accuracy (%) on DS+CB (ρ = 0.5) test stream of CIFAR100-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n27.0 32.0 60.6 70.7 45.9 Source 43.8 45.2 43.9 56.2 44.5 BN adapt 45.8 47.5 48.9 56.4 45.3 ETA 28.5 34.8 69.7 80.8 53.5 LAME 46.9 48.3 46.5 55.1 46.6 CoTTA 46.9 48.4 46.5 54.5 47.2 CoTTA* 45.4 47.0 47.8 56.0 45.7 PL 61.3 62.5 63.2 73.1 61.3 PL+DELTA 44.8 46.7 48.4 55.9 45.5 TENT 59.7 62.4 64.6 73.3 60.7 TENT+TBR 45.9 47.5 49.3 56.8 46.4 TENT+DOT 61.3 63.5 65.5 73.9 62.2 TENT+DELTA Ent-W 45.8 47.5 49.0 56.3 45.5 Ent-W+TBR+Div-W(0.05) 61.5 64.0 64.1 73.8 60.7 62.4 63.9 65.7 74.3 61.9 Ent-W+TBR+Div-W(0.1) 61.0 63.5 65.5 73.6 60.8 Ent-W+TBR+Div-W(0.2) Ent-W+TBR+Div-W(0.4) 60.5 63.4 65.2 73.4 60.3 60.0 62.8 64.2 72.3 59.5 Ent-W+TBR+LA Ent-W+TBR+Sample-drop 61.9 64.2 65.6 74.2 61.8 61.9 64.2 66.0 74.3 61.9 Ent-W+DELTA\n\n69.2 54.7 54.5 79.6 54.2 53.8 54.3 70.8 54.0 70.7 54.8 71.5 54.5 71.7 71.8 71.2 71.2 69.9 71.7 71.9\n\n71.2 60.5 54.2 49.7 70.5 44.9 62.8 25.3 58.8 53.5 55.5 49.1 50.0 43.9 57.0 54.2 48.7 51.2 45.0 49.5 55.8 51.2 51.2 48.1 57.4 53.8 49.4 53.1 45.9 50.9 81.8 71.9 62.7 58.6 81.1 50.6 74.5 26.9 69.5 61.6 55.2 49.3 50.6 43.4 56.9 50.8 49.3 54.2 48.3 50.4 54.4 50.0 51.2 43.9 56.1 51.5 50.3 53.9 49.4 50.5 55.7 50.8 51.3 47.2 57.1 52.6 49.4 52.7 45.9 50.6 73.6 68.0 67.0 63.3 74.5 70.0 65.7 69.7 61.2 67.0 55.2 50.0 50.1 47.3 56.6 52.2 48.4 52.6 45.6 50.2 72.9 67.3 66.6 64.2 74.2 68.9 65.0 69.5 61.0 66.7 55.8 50.8 51.1 48.2 57.4 53.6 49.6 53.0 46.4 51.1 73.8 68.3 67.5 65.6 74.8 70.8 66.1 70.4 62.0 67.8 55.6 51.6 51.1 48.3 57.2 53.8 49.3 53.0 45.9 51.0 73.5 67.6 68.2 64.2 74.8 71.0 66.4 70.3 62.2 67.6 73.8 68.3 68.5 65.0 75.0 71.1 66.2 70.5 62.4 68.1 72.9 68.0 67.9 65.1 74.5 70.7 65.7 70.2 62.2 67.5 72.9 67.6 67.9 65.0 74.4 70.6 65.3 69.9 61.8 67.3 71.7 66.8 66.8 63.9 73.3 69.4 64.7 69.1 61.0 66.4 73.8 68.3 68.4 65.5 74.9 71.4 66.2 70.7 62.7 68.1 73.9 68.3 68.5 65.9 74.9 71.5 66.4 70.9 62.9 68.2\n\nTable 21: Classification accuracy (%) on DS+CB (ρ = 0.1) test stream of CIFAR100-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n27.0 32.0 60.6 70.7 45.9 Source 31.5 32.8 31.4 40.4 31.0 BN adapt 31.2 32.2 32.6 38.4 30.4 ETA 30.3 36.9 73.2 84.3 57.2 LAME 33.7 34.8 34.1 39.5 33.2 CoTTA 33.7 35.0 33.7 39.0 33.2 CoTTA* 32.2 32.0 32.5 39.2 30.7 PL 59.2 61.0 61.6 72.0 58.8 PL+DELTA TENT 29.9 31.1 32.1 37.8 30.0 TENT+DELTA 60.3 62.7 63.1 72.7 60.2 Ent-W 31.0 32.1 32.7 38.3 30.1 Ent-W+DELTA 60.2 62.3 63.5 72.3 59.6\n\n69.2 39.3 37.7 83.3 39.3 38.7 37.8 70.1 36.6 70.7 37.7 70.0\n\n71.2 60.5 54.2 49.7 70.5 44.9 62.8 25.3 58.8 53.5 40.0 35.3 35.7 31.5 40.7 38.0 34.5 36.7 31.7 35.4 38.4 34.6 34.7 32.2 39.4 36.3 33.2 36.3 31.2 34.6 85.0 76.7 66.4 63.1 84.7 54.3 79.2 28.6 73.9 65.1 40.1 36.3 36.8 31.8 39.9 36.8 35.8 39.6 35.3 36.5 39.4 35.8 36.6 31.8 39.1 36.1 35.6 38.8 35.5 36.1 39.2 35.0 35.0 32.1 39.5 36.8 33.5 36.9 31.2 34.9 72.2 66.2 65.2 61.6 72.8 69.2 63.5 67.4 59.6 65.4 37.6 33.6 33.3 31.4 38.1 34.4 32.0 36.0 30.1 33.6 72.1 66.7 65.9 63.4 73.6 69.8 64.5 68.5 60.2 66.3 38.5 34.5 34.5 32.0 39.3 36.0 32.9 36.2 30.6 34.4 72.3 67.3 66.3 63.2 73.7 70.3 64.2 69.2 60.5 66.3\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nTable 22: Classification accuracy (%) on IS+CI (π = 0.1) test stream of CIFAR100-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n26.2 31.7 60.1 70.3 45.7 Source 58.0 58.8 56.7 71.6 58.3 BN adapt 62.6 63.7 65.2 73.6 62.9 ETA 23.5 28.6 59.4 68.8 43.3 LAME 59.8 61.3 59.7 71.8 59.8 CoTTA 59.8 61.9 60.1 72.0 61.7 CoTTA* 61.7 62.3 62.8 73.1 61.7 PL 62.4 63.0 63.2 73.4 61.3 PL+DELTA 61.7 63.3 63.9 73.0 62.3 TENT 61.6 63.8 64.4 73.3 62.2 TENT+TBR 62.4 63.6 64.7 73.1 62.6 TENT+DOT 62.5 64.3 65.3 73.8 62.4 TENT+DELTA Ent-W 62.5 63.8 65.2 73.6 62.9 Ent-W+TBR+Div-W(0.05) 61.1 62.0 62.6 73.0 60.8 62.5 63.5 64.8 73.7 62.8 Ent-W+TBR+Div-W(0.1) 63.3 64.1 66.2 73.9 63.2 Ent-W+TBR+Div-W(0.2) Ent-W+TBR+Div-W(0.4) 62.7 63.7 65.7 73.5 62.9 63.6 64.6 66.4 74.2 63.7 Ent-W+TBR+LA Ent-W+TBR+Sample-drop 63.3 64.6 65.8 73.8 63.6 63.9 64.8 66.4 74.1 63.7 Ent-W+DELTA\n\n69.5 69.6 71.6 67.1 69.6 70.9 71.1 71.9 71.4 71.5 71.6 71.3 71.7 71.1 72.0 72.0 71.8 72.1 72.2 72.2\n\n71.2 60.1 53.9 49.7 69.7 45.1 62.5 25.6 58.9 53.3 71.5 64.9 65.1 58.6 72.9 68.7 64.4 66.3 58.5 64.3 73.8 68.6 68.9 65.6 75.2 72.1 65.9 70.7 62.7 68.2 68.8 58.2 50.9 46.6 67.1 39.4 60.4 21.6 56.7 50.7 71.6 64.4 65.3 56.5 73.1 68.5 64.2 68.2 62.5 65.1 72.6 66.2 67.4 59.1 74.5 71.3 67.3 71.5 66.3 66.8 73.6 67.2 68.1 63.7 74.3 71.3 65.5 69.7 61.3 67.2 73.5 67.2 68.3 64.0 75.0 71.5 65.6 70.1 62.2 67.5 73.1 67.6 68.1 65.1 74.9 71.4 65.5 70.7 62.5 67.6 73.6 68.0 68.0 64.9 74.8 71.4 65.5 71.0 63.0 67.8 73.7 68.0 68.6 65.3 74.7 71.8 66.1 70.7 63.0 68.0 73.6 68.3 69.0 66.1 75.1 71.6 66.2 71.1 63.9 68.3 73.7 68.5 68.9 65.5 75.3 72.0 66.3 70.7 62.9 68.2 73.1 66.9 66.9 63.4 74.1 70.2 65.6 68.6 60.5 66.7 74.2 68.5 68.7 65.5 75.2 71.7 66.7 70.7 62.4 68.2 73.8 68.9 69.5 65.8 75.7 72.5 66.8 71.2 62.9 68.7 74.2 68.3 69.5 65.5 75.6 73.1 66.5 70.9 62.9 68.5 74.2 69.0 70.1 66.0 76.0 73.3 67.2 71.8 63.4 69.0 74.0 69.5 69.7 66.4 75.6 72.5 67.0 71.5 63.1 68.8 74.4 69.2 70.5 66.2 75.6 73.3 67.0 71.6 63.3 69.1\n\nTable 23: Classification accuracy (%) on IS+CI (π = 0.05) test stream of CIFAR100-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n26.2 31.8 60.5 70.5 46.4 Source 56.7 58.0 55.5 71.4 57.5 BN adapt 61.3 63.2 64.6 73.6 61.5 ETA 23.2 28.9 59.0 67.9 43.8 LAME 58.4 60.6 58.8 71.6 58.2 CoTTA 58.4 60.9 59.1 72.0 59.9 CoTTA* 60.4 62.1 62.9 72.8 60.8 PL 61.0 63.1 62.8 73.2 61.8 PL+DELTA TENT 61.0 63.4 64.0 73.3 60.6 TENT+DELTA 61.7 64.8 65.6 73.5 62.1 Ent-W 61.4 63.2 64.7 73.7 61.5 Ent-W+DELTA 62.8 64.4 65.6 74.4 62.5\n\n68.9 69.5 72.2 66.7 69.4 70.9 71.4 71.6 71.7 71.2 72.1 72.3\n\n70.6 59.8 53.7 50.3 70.4 44.9 61.8 24.7 58.2 53.3 71.1 64.7 64.1 57.5 72.5 69.0 63.1 66.2 58.0 63.6 73.3 68.1 67.7 65.0 74.4 71.4 65.6 70.2 62.8 67.7 67.8 58.2 50.5 47.1 67.7 39.8 59.7 20.6 56.9 50.5 71.2 63.5 64.2 55.6 72.5 68.6 62.4 67.9 61.0 64.3 71.8 65.2 66.5 58.6 73.9 71.0 65.7 70.5 65.2 66.0 72.7 67.7 67.1 62.6 73.5 71.3 65.4 69.4 61.4 66.8 73.2 67.9 67.6 63.5 74.2 71.4 65.3 69.6 62.0 67.2 73.2 68.7 66.9 64.9 73.9 71.0 65.1 70.0 62.0 67.3 73.4 69.0 68.6 65.4 74.6 71.1 66.1 70.6 63.1 68.1 73.2 68.4 67.8 64.9 74.4 71.3 65.6 70.1 62.7 67.7 74.1 69.1 68.9 66.2 75.5 73.0 66.1 71.7 63.0 68.6\n\nTable 24: Classification accuracy (%) on DS+CI (ρ = 0.5, π = 0.1) test stream of CIFAR100-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n26.2 31.7 60.1 70.3 45.7 Source 44.9 45.6 44.7 56.1 44.8 BN adapt 46.5 47.0 48.6 56.2 46.1 ETA 27.2 34.0 68.5 80.0 52.5 LAME 47.0 48.3 47.5 55.0 47.0 CoTTA 47.0 48.3 47.6 54.7 48.0 CoTTA* 46.0 45.9 47.7 56.0 45.8 PL 60.3 62.1 62.9 72.6 60.9 PL+DELTA TENT 46.3 46.1 47.6 55.8 45.2 TENT+DELTA 62.5 63.7 64.9 73.5 62.2 Ent-W 46.7 46.9 48.7 56.1 46.1 Ent-W+DELTA 62.4 63.9 65.0 73.5 61.9\n\n69.5 54.4 55.1 78.9 54.6 54.1 55.4 70.9 54.7 70.8 55.0 71.4\n\n71.2 60.1 53.9 49.7 69.7 45.1 62.5 25.6 58.9 53.3 56.4 49.4 50.8 44.3 56.5 53.4 49.2 51.7 46.0 49.9 56.5 51.4 51.9 47.1 56.7 53.5 49.3 53.4 46.7 51.1 80.5 70.3 60.5 56.6 78.2 49.9 72.7 26.4 68.6 60.3 55.5 49.7 51.6 44.0 55.9 50.3 50.5 55.2 49.0 50.7 54.7 49.7 51.7 45.1 55.2 50.1 50.6 54.7 50.6 50.8 56.4 50.7 50.7 46.4 56.3 53.5 48.8 53.2 46.8 50.6 72.4 66.6 67.4 62.2 73.5 69.9 65.6 69.3 62.5 66.6 55.6 49.8 50.5 47.4 56.7 51.3 48.6 52.4 45.2 50.2 72.1 67.6 68.0 65.7 75.0 70.5 66.6 69.9 63.4 67.8 56.3 51.2 51.9 47.7 57.1 53.5 49.2 53.2 46.6 51.1 73.5 68.1 68.8 65.4 74.7 70.7 66.2 70.4 63.3 67.9\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nTable 25: Classification accuracy (%) on DS+CI (ρ = 0.5, π = 0.05) test stream of CIFAR100-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n26.2 31.8 60.5 70.5 46.4 Source 43.0 45.0 42.3 55.4 44.0 BN adapt 45.4 46.4 46.8 56.2 45.3 ETA 27.1 33.3 67.6 78.7 51.7 LAME 46.5 47.3 45.3 54.5 45.5 CoTTA 46.5 47.8 45.5 54.1 46.2 CoTTA* 44.3 45.8 46.4 55.8 45.2 PL 59.3 61.1 62.2 71.6 59.4 PL+DELTA 44.7 46.7 45.7 55.3 44.6 TENT 58.8 61.6 62.5 72.2 58.6 TENT+TBR 45.2 47.1 46.7 55.6 45.4 TENT+DOT 60.3 62.3 63.7 72.9 60.3 TENT+DELTA Ent-W 45.6 46.4 47.0 56.0 45.4 Ent-W+TBR+Div-W(0.05) 60.9 62.3 62.9 73.0 59.5 61.2 62.8 64.5 73.5 59.9 Ent-W+TBR+Div-W(0.1) 60.4 62.4 63.6 73.5 59.4 Ent-W+TBR+Div-W(0.2) 59.7 62.3 63.3 72.9 59.4 Ent-W+TBR+Div-W(0.4) Ent-W+TBR+LA 59.2 61.6 62.3 71.9 58.9 Ent-W+TBR+Sample-drop 60.9 62.6 63.7 73.2 60.0 61.2 62.9 64.0 73.7 60.4 Ent-W+DELTA\n\n68.9 54.2 54.7 77.1 53.7 53.4 54.2 70.3 53.8 70.3 54.3 70.3 54.9 70.7 71.3 70.7 70.6 69.5 70.6 71.1\n\n70.6 59.8 53.7 50.3 70.4 44.9 61.8 24.7 58.2 53.3 54.9 49.1 49.4 43.8 56.2 53.1 48.5 51.3 44.3 49.0 54.8 50.7 50.1 46.5 56.4 52.2 48.8 53.0 45.5 50.2 78.9 68.5 59.7 56.0 77.5 49.3 70.1 25.1 66.6 59.2 55.0 48.6 49.9 42.4 56.0 49.0 49.1 53.5 47.2 49.6 54.2 48.8 50.6 43.5 54.2 49.4 49.6 52.8 48.7 49.7 54.8 50.7 49.3 45.8 56.5 52.4 49.1 52.0 45.5 49.9 70.8 66.3 65.5 61.4 74.0 69.0 64.5 67.5 59.8 65.5 53.7 50.0 48.6 46.1 55.5 50.0 48.9 52.0 44.6 49.3 70.9 67.0 64.8 62.5 73.5 68.1 63.4 68.5 59.4 65.5 54.3 50.9 49.7 47.3 56.1 51.7 49.2 52.9 45.6 50.1 71.3 67.8 66.2 64.1 74.2 68.7 64.3 69.1 60.7 66.4 54.9 50.7 50.1 46.8 56.3 52.2 48.6 53.1 45.1 50.2 72.0 67.0 66.2 62.4 74.5 69.8 64.9 69.0 60.7 66.4 71.8 67.4 66.4 63.7 74.5 70.6 65.2 69.5 61.0 66.9 72.0 67.1 65.8 63.4 74.4 70.1 64.5 69.5 60.4 66.5 71.9 67.0 65.8 63.1 74.3 69.6 63.9 69.4 60.3 66.2 71.3 65.7 65.1 62.8 73.2 69.0 63.3 68.2 60.0 65.5 72.0 66.9 66.6 64.1 74.9 69.4 64.6 69.9 61.1 66.7 72.3 67.4 67.0 64.2 74.7 70.2 64.7 69.8 61.0 67.0\n\nTable 26: Classification accuracy (%) on IS+CB test stream of ImageNet-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n8.7\n\n2.4\n\n17.6\n\n2.9 4.9\n\n17.9 12.5\n\n9.8 1.9 2.2 Source 4.1 8.2 4.5 TTA 15.2 15.8 15.8 15.0 15.3 BN adapt 7.5 19.7 13.0 9.0 MEMO 35.6 37.5 36.2 33.7 33.1 ETA 9.1 1.3 1.6 LAME 17.6 18.0 17.4 15.6 18.2 CoTTA 17.6 22.1 24.3 19.8 22.7 CoTTA* 26.2 26.2 27.0 25.2 24.3 PL 27.7 29.4 28.5 27.0 26.1 PL+DELTA 28.7 30.5 30.1 28.0 27.2 TENT 29.5 31.4 30.9 28.8 28.0 TENT+TBR 30.5 32.3 31.6 29.6 29.3 TENT+DOT 31.2 33.1 32.1 30.5 30.2 TENT+DELTA Ent-W 34.5 29.0 33.1 29.6 26.3 Ent-W+TBR+Div-W(0.05) 36.1 37.9 37.8 34.4 33.5 35.3 37.3 36.3 33.6 32.2 Ent-W+TBR+Div-W(0.1) 32.5 35.4 33.5 26.7 25.8 Ent-W+TBR+Div-W(0.2) 28.7 32.8 31.7 20.3 19.3 Ent-W+TBR+Div-W(0.4) Ent-W+TBR+LA 26.7 22.4 29.6 20.3 20.0 Ent-W+TBR+Sample-drop 37.0 38.9 38.2 35.8 35.4 38.1 39.6 39.0 36.3 36.5 Ent-W+DELTA\n\n22.5 16.9 23.3 24.4 58.9 5.4 17.0 20.6 31.6 18.0 25.8 14.0 19.1 21.3 53.0 12.4 14.6 24.6 33.6 17.7 38.8 34.3 33.1 47.8 65.3 16.8 43.9 48.9 39.7 31.5 27.6 25.3 28.8 32.1 61.0 11.0 23.8 33.0 37.5 23.9 52.5 51.9 45.8 60.0 67.8 44.7 57.8 60.9 55.2 48.0 15.2 19.9 31.1 17.2 21.9 15.6 22.5 22.8 58.6 5.2 43.6 36.6 35.1 53.0 66.5 19.5 46.3 54.9 42.6 34.4 38.1 36.0 37.2 45.2 60.1 26.4 46.6 53.4 46.8 35.1 46.5 43.3 39.5 55.0 66.7 30.2 51.2 55.7 49.1 40.2 47.9 44.1 40.7 55.9 67.4 34.1 52.9 56.6 50.3 41.8 49.4 47.2 41.2 57.4 67.4 26.5 54.6 58.5 52.5 42.7 50.3 47.7 41.8 58.3 68.1 26.9 55.4 59.3 53.3 43.5 49.9 47.8 42.2 57.5 67.5 37.5 55.4 58.8 52.9 44.4 50.9 48.2 43.0 58.5 68.1 37.9 56.2 59.5 53.6 45.1 52.2 51.9 45.6 59.9 67.8 17.8 57.8 60.9 55.0 44.6 53.3 53.2 46.7 60.9 68.5 45.1 58.9 61.7 56.0 48.9 53.4 53.1 46.6 61.0 68.4 43.1 58.7 61.7 55.9 48.4 53.0 52.9 46.2 60.9 68.4 31.1 58.7 61.7 56.0 46.1 53.0 52.7 46.2 60.8 68.4 13.9 58.7 61.7 56.0 43.5 53.4 52.9 46.7 60.7 68.0 10.1 58.8 61.5 56.0 42.4 53.8 53.3 47.4 61.0 68.5 46.4 59.1 62.0 56.4 49.5 54.0 53.5 47.6 61.1 68.4 46.9 59.2 61.9 56.6 49.9\n\n14.8 12.9 26.4 20.7 47.7 13.9 31.2 29.7 37.2 38.1 41.4 41.9 42.5 42.9 47.4 49.1 49.1 48.9 48.9 49.2 49.6 49.9\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nTable 27: Classification accuracy (%) on IS+CB test stream of ImageNet-C with different architectures.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n2.9\n\n1.8\n\n4.3\n\n11.4\n\n17.9\n\nResNet18 1.2 8.7 1.0 Source 22.3 24.7 22.2 20.3 21.1 TENT TENT+DELTA 24.5 26.8 24.4 22.6 23.7 Ent-W 27.1 30.7 24.3 22.3 17.5 Ent-W+DELTA 31.7 33.8 32.0 29.0 30.3 ResNet50 9.8 1.9 2.2 Source TENT 28.7 30.5 30.1 28.0 27.2 TENT+DELTA 31.2 33.1 32.1 30.5 30.2 Ent-W 34.5 29.0 33.1 29.6 26.3 Ent-W+DELTA 38.1 39.6 39.0 36.3 36.5 ResNet101 21.9 13.1 3.5 3.5 Source TENT 32.6 34.0 33.2 32.2 32.4 TENT+DELTA 35.1 37.4 35.6 34.9 35.1 Ent-W 36.1 20.8 37.3 33.6 31.7 Ent-W+DELTA 40.9 43.0 41.9 39.8 40.1 ResNet152 3.6 22.1 11.9 3.3 Source TENT 34.0 36.8 35.3 34.1 34.0 TENT+DELTA 36.6 39.2 37.7 36.7 36.3 Ent-W 38.7 33.4 34.6 36.6 33.2 Ent-W+DELTA 42.6 45.4 44.5 42.0 42.2 WideResNet50 TENT 34.5 37.2 34.7 30.6 31.6 TENT+DELTA 36.7 39.6 37.2 33.5 34.6 Ent-W 34.0 37.1 33.6 25.0 27.7 Ent-W+DELTA 41.1 44.9 42.9 38.6 39.3 ResNeXt50 TENT 33.3 36.2 34.2 32.3 30.9 TENT+DELTA 35.3 38.5 36.1 34.5 33.5 Ent-W 31.4 37.5 34.7 34.0 25.2 Ent-W+DELTA 40.7 43.6 42.0 39.5 39.1\n\n4.4\n\n11.2 32.2 34.0 37.6 40.2\n\n14.8 41.4 42.9 47.4 49.9\n\n19.2 45.1 46.8 50.3 53.1\n\n24.8 46.9 48.7 52.9 55.5\n\n45.2 47.4 51.0 53.4\n\n45.5 46.6 51.0 53.1\n\n17.6 10.9 16.5 14.3 51.3 3.4 16.8 23.1 29.6 14.6 41.1 37.8 33.7 49.0 59.2 19.5 46.9 50.6 45.8 35.1 42.7 38.9 35.4 50.2 60.3 27.5 48.5 51.9 47.0 37.2 44.2 42.5 37.8 51.5 59.9 5.5 49.5 52.9 48.5 36.8 46.1 44.2 39.7 53.1 60.9 36.9 51.5 54.7 49.8 42.3\n\n17.0 20.6 31.6 18.0 22.5 16.9 23.3 24.4 58.9 5.4 49.4 47.2 41.2 57.3 67.4 26.7 54.6 58.5 52.5 42.7 50.9 48.2 43.0 58.5 68.1 37.9 56.2 59.5 53.6 45.1 52.2 51.9 45.6 59.9 67.8 17.8 57.8 60.9 55.0 44.6 54.0 53.5 47.6 61.1 68.4 46.9 59.2 61.9 56.6 49.9\n\n24.3 35.0 42.3 22.5 26.5 21.0 26.7 28.1 61.4 7.2 53.0 50.8 45.0 59.6 69.1 33.8 58.6 61.1 55.8 46.4 54.6 51.8 46.7 60.7 69.9 42.6 60.1 62.3 57.2 48.7 55.6 54.9 46.8 62.4 69.8 19.7 61.1 63.2 58.2 46.8 57.4 56.5 50.8 63.4 70.2 50.6 62.3 64.2 59.8 53.0\n\n25.5 22.1 28.9 27.7 63.1 5.2 24.9 27.1 42.2 22.5 54.0 52.4 47.0 61.3 70.7 35.5 59.9 62.4 57.2 48.1 55.6 54.0 48.4 62.4 71.2 44.0 61.3 63.3 58.4 50.2 57.4 56.9 46.5 64.2 71.0 29.3 62.7 64.8 60.0 49.5 58.9 58.5 52.7 65.5 71.4 51.9 63.7 65.8 61.2 54.8\n\n52.0 51.1 45.8 60.5 69.9 38.4 58.3 61.7 54.9 47.1 54.5 53.0 47.6 62.2 71.2 44.1 60.3 63.4 56.9 49.5 54.7 55.5 49.9 62.8 70.4 24.9 60.7 63.9 57.6 47.3 57.3 57.6 51.8 64.7 71.4 52.0 62.4 65.7 59.8 53.5\n\n52.2 51.1 45.9 59.6 69.3 39.0 57.1 61.5 53.8 46.8 53.7 52.1 47.0 60.5 69.9 43.9 58.4 62.4 55.0 48.5 54.6 55.1 49.1 62.2 70.0 49.1 60.3 64.3 57.1 49.0 56.7 56.6 51.1 63.2 70.4 50.7 61.5 64.9 58.2 52.8\n\nTable 28: Classification accuracy (%) on DS+CB (ρ = 1.0) test stream of ImageNet-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n2.9\n\n17.9\n\n2.2 9.8 1.9 Source 10.6 10.9 10.9 10.2 10.3 BN adapt 17.0 19.2 18.2 14.1 12.0 ETA 22.4 11.3 1.8 1.5 2.7 LAME 12.2 12.5 12.8 11.2 9.5 CoTTA 12.2 14.9 16.2 12.3 14.2 CoTTA* 15.9 15.6 16.4 14.4 13.9 PL 26.3 27.4 27.1 25.5 25.1 PL+DELTA TENT 16.1 16.8 16.8 15.1 14.1 TENT+DELTA 29.6 31.7 30.4 29.1 28.6 3.6 Ent-W Ent-W+DELTA 35.6 37.9 36.0 34.4 34.4\n\n2.9\n\n4.2\n\n2.8\n\n3.1\n\n14.8 17.5 25.9 17.2 19.7 18.9 23.1 37.4 23.3 41.5 11.3 47.9\n\n22.5 16.9 23.3 24.4 58.9 5.4 17.0 20.6 31.6 18.0 25.8 23.5 23.1 33.0 46.5 11.3 30.2 33.3 27.0 21.6 31.1 30.9 26.8 38.6 46.1 18.9 36.0 38.7 33.5 27.1 28.4 19.8 28.4 29.8 74.4 5.9 20.0 25.6 40.4 22.0 28.4 24.7 23.9 35.9 47.4 12.8 31.1 37.0 28.4 23.2 24.2 24.4 25.2 30.0 41.5 15.3 30.8 35.5 31.2 23.1 29.8 28.1 26.2 37.3 47.2 12.8 34.2 37.2 32.2 25.6 46.5 43.0 39.8 54.8 66.6 32.7 51.4 55.6 48.6 40.5 30.2 28.8 24.9 37.5 46.7 9.3 34.9 37.8 33.0 25.7 49.8 47.0 42.1 57.6 67.5 35.7 54.9 58.5 52.0 43.7 32.0 37.1 21.5 16.8 20.2 20.0 12.5 34.4 44.7 1.7 52.8 51.9 46.5 60.1 67.8 44.2 57.9 60.8 55.4 48.3\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nTable 29: Classification accuracy (%) on DS+CB (ρ = 0.5) test stream of ImageNet-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n2.9 9.9\n\n17.9 8.8\n\n9.8 1.9 2.2 Source 9.6 9.1 9.8 BN adapt 13.9 15.5 13.3 11.1 10.3 ETA 23.6 11.7 1.9 1.6 2.8 LAME 10.8 11.0 11.0 10.4 7.8 CoTTA 10.8 13.3 14.3 11.0 12.9 CoTTA* 14.2 13.4 14.5 12.5 11.5 PL 25.6 27.3 26.2 25.2 24.6 PL+DELTA 13.9 14.6 14.5 12.6 11.7 TENT 27.3 28.5 28.2 26.0 25.4 TENT+TBR 15.5 16.5 15.9 14.2 14.0 TENT+DOT 29.1 30.9 29.7 28.2 27.8 TENT+DELTA 1.0 3.5 2.9 Ent-W Ent-W+TBR+Div-W(0.05) 32.4 34.6 33.3 27.2 28.3 30.1 33.4 31.1 25.5 21.7 Ent-W+TBR+Div-W(0.1) 23.7 30.5 26.5 19.7 12.2 Ent-W+TBR+Div-W(0.2) 4.8 17.1 15.3 22.2 11.2 Ent-W+TBR+Div-W(0.4) Ent-W+TBR+LA 5.0 5.1 10.9 7.2 14.3 Ent-W+TBR+Sample-drop 33.7 36.4 35.1 31.9 30.8 34.9 37.5 35.8 32.7 32.3 Ent-W+DELTA\n\n1.4\n\n2.5\n\n14.8 15.8 21.4 17.8 17.4 17.1 20.0 36.0 19.0 38.9 20.9 40.3 7.1 45.2 44.8 44.3 43.7 35.0 46.7 46.7\n\n17.0 20.6 31.6 18.0 22.5 16.9 23.3 24.4 58.9 5.4 22.8 21.0 20.8 29.5 41.9 10.3 26.7 29.5 24.2 19.3 26.2 26.1 22.9 33.4 40.5 13.4 30.9 33.2 29.3 22.8 20.8 26.4 41.5 22.7 29.4 20.4 29.4 30.5 76.1 6.2 25.0 22.0 21.6 32.0 42.6 9.9 27.9 32.5 25.8 20.5 22.0 21.8 22.8 27.6 38.0 14.8 27.8 32.6 28.0 21.0 26.3 24.9 23.4 33.2 42.4 11.1 30.5 33.1 28.5 22.6 45.7 42.9 39.3 54.4 66.5 31.0 50.6 55.0 47.9 39.9 26.1 25.2 21.5 33.2 41.6 6.5 30.5 33.1 28.9 22.2 48.5 46.0 39.6 57.1 67.3 18.5 53.6 57.6 51.2 40.9 27.1 25.9 23.5 33.7 41.8 15.2 31.4 33.5 29.5 23.9 49.0 46.7 41.5 57.3 67.3 33.9 54.4 58.1 51.6 43.1 22.2 31.1 20.1 12.9 11.9 15.1 8.5 27.7 37.0 1.3 51.3 50.5 44.3 59.3 67.4 36.0 57.0 60.1 54.3 45.4 51.1 50.4 43.4 59.3 67.4 16.3 56.9 60.2 54.3 43.1 56.8 60.2 54.2 40.3 51.1 50.5 41.1 59.4 67.4 7.0 56.7 60.3 54.2 37.1 51.1 50.2 36.5 59.5 67.4 6.1 39.9 39.3 23.2 46.8 53.6 4.1 44.6 47.2 42.4 27.9 52.2 51.2 45.6 60.0 67.6 40.4 57.3 60.6 54.7 46.9 52.3 51.5 46.0 59.7 67.3 42.8 57.3 60.4 54.9 47.5\n\nTable 30: Classification accuracy (%) on DS+CB (ρ = 0.1) test stream of ImageNet-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n2.9 6.4 5.2 2.9 7.0 8.2 7.6\n\n9.8 17.9 1.9 2.2 Source 5.6 5.6 6.2 6.3 BN adapt 2.3 2.8 5.1 4.6 ETA 26.2 12.8 1.6 1.9 LAME 6.2 5.0 7.1 7.1 CoTTA 7.3 6.3 8.7 7.1 CoTTA* 7.7 6.1 6.4 8.3 PL 23.4 24.6 24.0 22.0 21.5 PL+DELTA TENT 5.9 7.8 7.4 TENT+DELTA 26.7 28.2 27.3 25.0 24.8 Ent-W 0.8 Ent-W+DELTA 30.4 33.1 31.4 26.8 28.1\n\n1.5\n\n7.8\n\n1.1\n\n6.2\n\n0.6\n\n1.4\n\n14.8 9.8 7.0 19.8 10.5 10.3 10.8 33.3 8.9 37.1 2.3 42.2\n\n17.0 20.6 31.6 18.0 22.5 16.9 23.3 24.4 58.9 5.4 16.5 18.1 15.0 12.1 13.9 13.6 13.4 18.4 26.1 6.4 14.5 17.1 14.4 10.0 12.3 11.6 10.9 17.5 22.5 2.3 22.5 29.0 45.5 24.7 32.7 22.8 32.5 33.8 80.0 6.6 17.1 19.6 15.8 12.7 15.0 14.1 13.7 19.3 26.5 6.4 16.9 19.4 17.3 12.9 13.4 14.3 14.5 17.9 23.7 7.9 15.4 15.0 14.0 20.0 25.9 5.0 17.5 19.4 17.0 13.1 43.4 40.0 37.3 52.2 65.0 26.1 47.8 52.5 45.4 37.2 14.7 12.5 11.6 19.0 24.5 3.0 16.8 18.5 16.5 12.1 46.6 43.6 39.6 55.1 65.7 27.2 51.6 55.6 49.0 40.2 4.6 5.7 4.4 48.9 48.2 42.6 56.9 65.4 31.5 54.4 57.8 51.5 43.3\n\n3.1 8.4 15.5 0.5\n\n4.4\n\n9.7\n\n7.0\n\nTable 31: Classification accuracy (%) on IS+CI (π = 0.1) test stream of ImageNet-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n3.0\n\n2.5\n\n17.5\n\n17.8\n\n2.4 9.7 1.9 Source 15.0 15.8 15.4 14.7 15.1 BN adapt 34.6 36.7 35.7 33.1 32.5 ETA 1.8 9.0 1.5 LAME 17.1 17.8 17.5 15.9 16.7 CoTTA 17.1 22.0 24.1 19.0 22.2 CoTTA* 24.9 24.8 25.9 24.3 23.4 PL 26.4 27.7 27.0 26.3 24.9 PL+DELTA 27.8 29.3 29.2 28.1 26.6 TENT 28.5 30.1 29.7 28.7 27.3 TENT+TBR 29.8 31.6 30.9 29.4 28.8 TENT+DOT 30.7 32.5 31.3 30.3 29.3 TENT+DELTA Ent-W 23.2 21.7 29.4 19.1 19.6 Ent-W+TBR+Div-W(0.05) 34.1 37.4 36.4 32.5 32.9 34.5 36.1 35.9 32.4 32.0 Ent-W+TBR+Div-W(0.1) 32.5 34.1 35.3 30.0 29.7 Ent-W+TBR+Div-W(0.2) 29.2 27.5 34.3 27.4 25.1 Ent-W+TBR+Div-W(0.4) Ent-W+TBR+LA 24.8 23.5 34.6 25.1 20.4 Ent-W+TBR+Sample-drop 36.1 37.8 37.3 33.7 33.2 36.6 38.6 37.8 34.9 34.4 Ent-W+DELTA\n\n22.4 16.5 23.1 24.2 58.9 5.5 16.9 20.4 31.5 17.9 39.1 34.4 33.2 47.8 65.1 17.5 44.4 48.8 39.8 31.5 51.9 51.3 45.4 59.5 67.6 44.8 57.3 60.9 55.1 47.5 21.8 15.1 22.3 22.6 58.5 5.3 14.9 19.8 30.9 17.2 43.2 36.8 35.7 51.9 66.4 17.7 47.1 54.0 42.8 34.1 35.7 35.2 35.8 42.8 57.8 22.9 44.8 50.4 45.3 33.5 45.7 42.3 39.5 54.6 66.5 28.6 49.9 55.5 48.5 39.4 46.9 43.3 40.2 55.3 66.8 33.3 52.1 56.5 49.8 40.9 48.7 46.5 41.0 57.2 67.3 25.7 53.6 58.2 51.9 42.1 49.9 47.0 41.7 57.6 67.9 25.1 54.5 59.0 52.9 42.7 49.4 47.0 42.1 57.3 67.3 36.8 54.9 58.6 52.4 43.9 50.5 47.5 42.9 57.8 67.7 36.4 55.7 59.2 53.1 44.4 51.7 51.0 39.0 58.9 67.5 10.1 57.2 60.5 54.9 40.7 52.9 52.1 45.7 60.0 67.9 42.6 57.8 61.7 55.7 47.8 52.9 52.1 45.8 59.8 68.0 40.2 57.9 61.5 55.7 47.5 52.7 51.9 45.5 59.7 68.0 30.2 57.9 61.5 55.7 46.1 58.0 61.4 55.8 43.3 52.8 51.8 44.7 59.5 68.0 6.1 52.9 52.2 45.0 59.7 67.3 4.2 58.0 61.3 55.8 42.2 52.9 52.1 46.0 59.7 68.0 43.7 57.9 61.5 55.5 48.2 52.6 51.9 46.1 59.5 67.4 44.6 57.9 60.9 55.4 48.4\n\n14.7 25.6 46.5 13.9 30.2 28.0 36.2 37.5 40.8 41.3 41.7 42.0 46.7 47.7 48.0 47.6 47.8 48.2 47.3 47.7\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nTable 32: Classification accuracy (%) on IS+CI (π = 0.05) test stream of ImageNet-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n2.9\n\n2.4\n\n17.7\n\n2.2 18.0 10.0 1.9 Source 15.1 15.2 15.6 14.9 15.8 BN adapt 34.2 36.1 35.0 32.0 32.0 ETA 1.6 9.1 1.4 LAME 17.3 17.4 17.8 15.4 17.1 CoTTA 17.3 21.6 23.8 19.9 22.9 CoTTA* 24.2 24.6 25.8 24.7 23.5 PL 26.1 27.3 27.1 25.8 25.3 PL+DELTA TENT 27.1 29.0 28.8 27.7 27.1 TENT+DELTA 30.1 32.3 31.2 29.6 29.6 17.2 13.4 25.6 15.8 12.1 Ent-W Ent-W+DELTA 35.7 38.2 37.1 34.1 33.8\n\n14.6 25.6 46.1 13.9 29.8 29.3 36.2 36.2 40.3 41.4 45.9 46.5\n\n22.5 16.6 23.1 24.4 58.4 5.5 16.9 20.6 31.5 17.9 39.0 34.6 33.2 47.8 64.7 17.2 44.1 48.2 39.9 31.4 52.0 50.6 45.0 59.4 67.3 43.4 57.0 60.3 54.5 47.0 21.9 15.4 22.3 22.7 58.0 5.2 15.1 19.8 30.9 17.2 43.1 37.5 35.4 51.9 65.8 19.3 46.8 53.3 42.5 34.0 37.4 35.7 36.6 44.5 59.0 24.2 45.5 51.8 45.9 34.4 45.8 42.7 38.9 54.3 65.9 27.0 49.0 55.0 48.0 39.0 46.8 43.2 39.9 54.8 66.4 32.6 51.1 55.4 48.8 40.5 49.1 46.4 40.7 57.1 66.6 24.8 53.1 57.8 51.3 41.8 50.0 47.4 42.4 57.6 67.2 35.3 55.1 58.5 52.6 44.0 51.0 50.4 44.6 59.3 66.9 10.0 56.5 60.0 54.1 38.9 51.7 51.1 45.6 58.4 66.0 43.5 57.0 59.3 54.5 47.5\n\nTable 33: Classification accuracy (%) on DS+CI (ρ = 0.5, π = 0.1) test stream of ImageNet-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n9.7 17.8 2.4 1.9 3.0 Source 9.5 9.3 9.7 10.1 10.1 BN adapt 8.6 7.3 11.9 12.7 12.7 ETA 22.4 11.4 2.0 1.5 2.8 LAME 10.4 9.8 11.4 11.6 11.7 CoTTA 11.4 13.9 14.9 11.7 13.3 CoTTA* 14.4 12.5 14.0 12.6 11.8 PL 24.8 25.5 25.4 23.6 23.0 PL+DELTA TENT 12.9 13.9 14.3 12.8 11.7 TENT+DELTA 28.3 30.1 29.1 27.5 27.2 1.3 Ent-W Ent-W+DELTA 32.2 35.0 34.1 30.5 29.4\n\n1.6\n\n2.3\n\n2.4\n\n1.6\n\n14.7 16.1 18.8 17.1 17.9 17.9 20.2 34.9 18.5 39.3 5.6 44.8\n\n22.4 16.5 23.1 24.2 58.9 5.5 16.9 20.4 31.5 17.9 24.2 21.7 21.4 30.8 43.5 11.2 27.0 30.9 25.7 20.1 30.7 34.9 30.3 21.8 27.1 25.9 22.8 34.0 41.9 7.9 27.5 19.4 28.0 29.5 73.1 6.0 19.8 24.9 40.0 21.7 26.4 22.9 22.6 33.3 44.4 11.6 28.6 33.8 27.3 21.6 22.8 22.7 23.5 29.2 39.4 14.6 28.2 33.1 29.6 21.7 27.2 25.3 24.1 34.1 43.9 10.7 29.8 34.2 29.8 23.0 44.8 41.0 38.8 53.2 65.9 29.6 49.7 54.1 47.3 38.8 27.0 25.0 21.7 34.1 42.9 6.6 30.1 34.5 30.1 22.4 48.3 45.2 41.2 56.3 66.8 31.0 53.6 57.2 51.2 42.2 16.8 17.4 16.6 10.8 12.9 13.5 11.1 16.7 40.4 1.1 50.7 49.5 44.5 58.1 66.6 36.6 55.7 58.4 53.7 45.3\n\nTable 34: Classification accuracy (%) on DS+CI (ρ = 0.5, π = 0.05) test stream of ImageNet-C.\n\nMethod\n\nGauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Brit Contr Elastic Pixel JPEG Avg\n\n18.0 10.0 2.9 1.9 2.2 Source 9.5 9.6 9.8 9.9 10.5 BN adapt 3.9 7.8 9.1 11.2 12.5 ETA 21.9 11.5 1.8 1.6 2.7 LAME 11.3 11.4 12.1 10.0 8.9 CoTTA 11.3 13.6 14.9 12.1 13.4 CoTTA* 13.3 11.2 14.7 12.4 12.4 PL 23.8 25.0 25.1 23.4 22.6 PL+DELTA 12.6 13.6 14.3 12.6 11.4 TENT 24.7 26.6 26.9 24.8 24.7 TENT+TBR 15.4 16.6 16.4 14.6 14.5 TENT+DOT 27.5 29.4 28.9 26.3 27.2 TENT+DELTA Ent-W 1.4 3.6 0.9 Ent-W+TBR+Div-W(0.05) 27.0 28.5 29.4 21.3 23.3 24.3 28.8 28.8 16.5 22.0 Ent-W+TBR+Div-W(0.1) 15.0 20.6 22.6 24.4 Ent-W+TBR+Div-W(0.2) 12.5 10.7 15.0 Ent-W+TBR+Div-W(0.4) 13.4 6.4 7.6 13.0 7.5 Ent-W+TBR+LA Ent-W+TBR+Sample-drop 27.9 32.2 30.9 24.3 27.0 30.8 34.4 33.0 28.7 29.3 Ent-W+DELTA\n\n9.4 7.4 3.6\n\n0.8\n\n1.5\n\n22.5 16.6 23.1 24.4 58.4 5.5 16.9 20.6 31.5 17.9 23.8 22.1 21.8 31.4 43.9 11.2 26.9 30.6 25.8 20.2 30.7 33.7 29.7 20.9 26.3 25.6 22.4 34.4 41.9 6.4 27.3 19.3 28.3 29.3 71.4 5.8 20.2 24.6 39.1 21.5 26.3 23.4 23.0 33.9 44.6 10.1 29.0 34.1 27.6 21.6 23.1 23.3 23.5 29.1 40.0 13.9 28.4 33.1 29.8 21.8 27.1 25.7 24.2 34.7 44.4 8.4 29.7 34.0 30.0 22.8 44.3 41.2 38.7 53.1 65.4 27.9 48.9 53.6 46.6 38.2 26.6 25.2 21.7 34.6 43.0 6.0 29.6 34.2 30.3 22.2 47.0 44.4 39.0 55.5 66.2 15.5 51.0 56.3 50.0 39.3 27.7 26.5 24.1 35.4 43.3 13.6 31.4 35.2 31.3 24.4 47.7 45.3 40.8 56.0 66.4 29.1 52.7 56.8 50.5 41.5 11.7 10.8 8.9 23.0 36.2 0.5 18.0 23.5 13.9 10.7 48.5 48.1 42.1 57.3 66.1 13.4 54.4 58.3 52.6 40.7 54.7 58.5 52.5 39.6 48.6 48.1 41.5 57.1 66.2 6.9 54.7 58.7 52.5 37.6 49.2 48.5 42.4 57.2 66.4 3.2 49.3 48.5 37.9 57.1 66.4 2.1 54.9 58.6 52.7 35.2 44.3 47.3 42.7 27.8 39.7 39.3 30.4 46.3 54.1 1.7 48.9 48.2 41.9 56.4 65.8 29.3 54.0 58.0 52.2 42.5 49.7 49.1 43.9 57.2 65.3 36.7 54.9 58.6 52.7 44.5\n\n14.6 15.8 18.2 16.9 17.9 17.8 19.6 33.6 17.2 37.1 20.1 38.4 5.9 40.1 40.0 39.9 41.0 33.8 40.8 42.8\n\n25",
    "reference": "# Summary Of The Paper\n\nThis paper presents a method named Debiased Fully Test-time Adaptation (DELTA) to address the biased issue in test-time adaptation. Specifically, the authors conduct experiments to verify the claims that 1) the normalization statistics tend to fit the current test mini-batch, and 2) the test-time adaptation optimization would bias to some dominant classes. The authors first adopt the renormalization technique to alleviate the biases in the normalization statistics of batch normalization, and then devise a re-weighting module to assign different weights for test samples with different pseudo labels to address the optimization issue. Extensive experiments on the ImageNet-C, ImageNet-R, and CIFAR 100-C demonstrate the effectiveness of the proposed method. However, I have some concerns about this paper. My detailed comments are as follows.\n\n# Strength And Weaknesses\n\nStrength:\n\n1.\tThis paper introduces a new setting, namely the class imbalance issue during the test-time process, in the test-time adaptation.\n\n2.\tThe authors dig out the issues of existing test-time adaptation methods (i.e., the bias issue) from a new perspective.\n\n3.\tExtensive experiments on the ImageNet-C, ImageNet-R, and CIFAR 100-C demonstrate the effectiveness of the proposed method.\n\nWeakness:\n\n1.\tThe technical contribution is not very significant. For example, the TBR just adopts the batch renormalization technique from the initial Batch Normalization paper, and the re-weighting technique is derived from class-wise re-weighting.\n\n2.\tAs referred to the Treatment II, some components of DOT have multiple options. However, the ablation studies about the components of DOT and the selection of the components of DOT may be missing.\n\n3.\tIn Table 8, when conducting experiments on the real-world out-of-distribution dataset ImageNet-R, the improvement of DELTA is marginal compared with that on the ImageNet-C dataset, more discussions are required.\n\n4.\tIn Table 5, how about the performance of DELTA with \\rho<0.1 (e.g., 0.01?), i.e., totally concentrate on one same class during a period. Similarly, could the authors provide more results regarding smaller \\pi<0.05 (e.g., 0.001)? I am curious about these results since any value of \\pi and \\rho may appear in practice.\n\n5.\tIn Table 10, could the authors further provide the results of Div-W+Fisher regularization (namely EATA, this is the full version of ETA in Niu et al, 2022)?\n\n6.\tFigure 1 is somewhat confusing. It is hard to distinguish the difference between IID and CI, non-IID and CI & non-IID.\n\n7.\tIn Section 3.1, it would be better to detail describe each scenario and the difference between the previous TTA setting.\n\n8.\tIn Treatment II of Section 3.3, it would be better to extend the “L x” to “Line x of Algorithm 1” to improve the readability of the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe class imbalance settings of TTA and the perspective of addressing the issue of TTA are novel.\n\n# Summary Of The Review\n\nThis paper proposes a new setting of test-time adaptation, namely class imbalance in the mini-batch, and proposes to address this problem from the de-bias perspective, which is interesting. Though the pure technical contribution is not very significant, it is new in the area of TTA. Some results are still missing to convince me regarding its effectiveness.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nIMPROVING ASPECT RATIO DISTRIBUTION FAIRNESS IN FEW-SHOT DETECTOR PRETRAINING VIA COOPERATING RPN’S\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRegion proposal networks (RPN) are a key component of modern object detectors. An RPN identifies image boxes likely to contain objects, and so worth further investigation. An RPN false negative is unrecoverable, so the performance of an object detector can be significantly affected by RPN behavior, particularly in low-data regimes. The RPN for a few shot detector is trained on base classes. Our experiments demonstrate that, if the distribution of box aspect ratios for base classes is different from that for novel classes, errors caused by RPN failure to propose a good box become significant. This is predictable: for example, an RPN trained on base classes that are mostly square will tend to miss short wide boxes. It has not been noticed to date because the (relatively few) standard base/novel class splits on current datasets do not display this effect. But changing the base/novel split highlights the problem. We describe datasets where the distribution shift is severe using PASCAL VOC, COCO, and LVIS datasets. We show that the effect can be mitigated by training multiple distinct but cooperating specialized RPNs. Each specializes in a different aspect ratio, but cooperation constraints reduce the extent to which the RPNs are tuned. This means that if a box is missed by one RPN, it has a good chance of being picked up by another. Experimental evaluation confirms this approach results in substantial improvements in performance on the ARShift benchmarks, while remaining comparable to SOTA on conventional splits. Our approach applies to any few-shot detector and consistently improves performance of detectors.\n\n1\n\nINTRODUCTION\n\nMost state-of-the-art object detectors follow a two-stage detection paradigm. A region proposal network (RPN) finds promising locations, and these are passed through a classifier to determine what, if any, object is present. In this architecture, if an RPN makes no proposal around an object, the object will not be detected. For a few-shot detector, one splits the classes into base and novel, then trains the RPN and classifier on base classes, fixes the RPN, and finally fine-tunes the classifier on novel classes using the RPN’s predictions.\n\nObjects in large-scale object detection datasets (e.g. COCO (Lin et al., 2014); LVIS (Gupta et al., 2019)) have typical aspect ratio that varies somewhat from instance to instance, and often differs sharply from category to category. As a result, the few-shot training procedure has a built-in problem with distribution shift. This phenomenon is illustrated in Figure 1. Imagine all base classes are roughly square, and all novel classes are either short and wide, or tall and narrow. The RPN trained on the base classes should miss some novel class boxes. These boxes will have two effects: the training data the classifier sees will be biased against the correct box shape; and, at run time, the detector may miss objects because of RPN failures. We refer to this problem as the bias (the RPN does not deal fairly with different aspect ratios). The bias occurs because the RPN sees few or no examples of the novel classes during training (Kang et al., 2019; Wang et al., 2020; Yan et al., 2019).\n\nTo date, this bias has not been remarked on. This is an accident of dataset construction: the standard base/novel splits in standard datasets do not result in a distribution shift. But other base/novel splits do result in a distribution shift large enough to have notable effects, and Section 3 shows our evidence\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: RPN is severely affected by the distribution shift of object aspect ratios from base to novel classes, leading to degenerated few-shot detection performance. After training on the base classes which are mostly boxy objects (bike, chair, table, tv, animals, etc.), (Left) state-of-the-art few-shot detector DeFRCN (Qiao et al., 2021) built on the conventional RPN misses the elongated novel class (train) object at the proposal stage and generates no proposal with IoUgt > 0.7 – this is a disaster (the classifier of DeFRCN will not see a train box proposal, and so it cannot detect the train). By contrast, (Right) our CoRPN’s remedy this issue and thus improve few-shot detection for DeFRCN. Red boxes are the groundtruth box of the novel class train object; green boxes are box proposals output by the model. We plot positive proposals with IoUgt > 0.7 following Qiao et al. (2021).\n\nthat this effect occurs in practice. In particular, we describe ARShift benchmarks that simulate the real-world scenario where the aspect ratio distribution shift is severe. RPNs in state-of-the-art fewshot detectors are heavily biased towards familiar aspect ratio distributions, and so have weaker than necessary performance on non-standard splits because their RPNs are unfair. Evaluation practice should focus on performance under hard splits.\n\nIn few-shot detection applications, a more robust RPN will be more reliable, because applications typically offer no guarantees about the aspect ratio of novel classes. We show how to build a more robust RPN by training multiple RPN classifiers to be specialized but cooperative. Our CoRPN’s can specialize (and a degree of specialization emerges naturally), but our cooperation constraints discourage individual RPN classifiers from overspecializing and so face generalization problems. CoRPN’s are competitive with SOTA on widely used conventional benchmarks of few-shot detection, using conventional splits. But on our ARShift benchmarks with hard splits based on PASCAL VOC, MS-COCO, and LVIS (Everingham et al., 2010; Lin et al., 2014; Kang et al., 2019; Wang et al., 2020), they beat SOTA, because they are more robust to shifts in aspect ratio distribution.\n\nOur contributions: (1) We show the bias has severe effects on detector performance, and describe ARShift benchmarks that evaluate these effects. (2) We describe a general approach to improving RPN robustness to distribution shifts. Our CoRPN construction works with many types of few-shot detectors. (3) We show that performance improvements resulting from CoRPN’s results from improved fairness. (4) Our CoRPN’s are competitive with SOTA on widely used conventional benchmarks. But on the hard splits in ARShift, they beat SOTA, because they are fair.\n\n2 RELATED WORK\n\nObject Detection with Abundant Data. There are two families of detector architecture, both relying on the fact that one can quite reliably tell whether an image region contains an object independent of category (Endres & Hoiem, 2010; van de Sande et al., 2011). In serial detection, a proposal process (RPN in what follows) offers the classifier a selection of locations likely to contain objects, and the classifier labels them. This family includes R-CNN and its variants (Girshick, 2015; Girshick et al., 2014; He et al., 2017; Ren et al., 2015) In parallel detection, there is no explicit proposal step; these methods can be faster but the accuracy may be lower. This family includes YOLO and its variants (Bochkovskiy et al., 2020; Redmon & Farhadi, 2017; Redmon et al., 2016; Redmon & Farhadi, 2018), SSD (Liu et al., 2016), point-based detectors such as CornerNet (Law & Deng, 2018) and ExtremeNet (Zhou et al., 2019), and emerging transformer-based methods exemplified by DETR (Carion et al., 2020). This paper identifies an issue with the proposal process that can impede strong performance when there is very little training data (the few-shot case). The effect is described in the context of two-stage detection, but likely occurs in one-stage detection too.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFew-Shot Object Detection. Few-shot detection involves detecting objects for which there are very few training examples (Chen et al., 2018; Kang et al., 2019; Schwartz et al., 2019), and state-of-theart methods are usually serial (Wang et al., 2019; Yan et al., 2019; Wang et al., 2020; Fan et al., 2020; Wu et al., 2020; Xiao & Marlet, 2020; Yang et al., 2020; Li et al., 2021a; Hu et al., 2021; Zhang et al., 2021; Li et al., 2021b; Zhu et al., 2021). There is a rich few-shot classification literature (roots in Thrun (1998); Fei-Fei et al. (2006)). Dvornik et al. (2019) uses ensemble procedures for few-shot classification. As to detection, TFA (Wang et al., 2020) shows that a simple twostage fine-tuning approach outperforms other complex methods. Much work seeks improvements by applying different techniques, such as meta-learning (Wang et al., 2019; Yan et al., 2019; Hu et al., 2021; Zhang et al., 2021), metric learning (Han et al., 2021; Wu et al., 2021; Yang et al., 2020), refinement (Wu et al., 2020; Li et al., 2021b), feature reweighting (Kang et al., 2019), semantic relations (Zhu et al., 2021), augmentation (Li & Li, 2021; Zhang & Wang, 2021), and margin loss (Li et al., 2021a). Other work (Fan et al., 2021) alleviates forgetting of base classes. In particular, Qiao et al. (2021) achieves state-of-the-art performance by decoupling the gradient of the backbone and other components of the detector, as well as adding a prototypical calibration module. Here we focus on the two most representative methods – the state-of-the-art DeFRCN (Qiao et al., 2021) and the widely used TFA (Wang et al., 2020) – as our main baselines.\n\nFew-Shot Detection Benchmarks. The existing literature can be seen as variations on a standard detection framework, where one splits data into two sets of categories: base classes Cb (which have many training examples) and novel classes Cn (which have few). The RPN and classifier are trained with instances from the base classes, and then fine-tuned with the few-shot novel class data. While the choice of the split can be important in revealing different aspects of few-shot detection, existing benchmarks (Kang et al., 2019; Wang et al., 2020) have only focused on a few fixed, rather arbitrary splits. However, we explore the scenario where there exists a notable distribution shift between base and novel classes, and investigate the behavior of RPNs accordingly.\n\nProposal Process in Few-Shot Detection. Relatively little work adjusts the proposal process, which (Sun et al., is usually seen as robust to few-shot issues because there are many base examples. 2021) introduces contrastive-aware object proposal encodings to facilitate classification. Attention mechanisms are also introduced that feed category-aware features instead of plain image features into the proposal process (Hsieh et al., 2019; Fan et al., 2020; Xiao & Marlet, 2020; Osokin et al., 2020), as well as re-ranking proposals based on similarity with query images (Hsieh et al., 2019; Fan et al., 2020). Making the RPN category-aware improves the quality of novel class proposals, but at inference time the model suffers from catastrophic forgetting of base categories – current category-aware features cannot summarize the very large number of base class examples efficiently or accurately. An RPN that is generally well-behaved can still create serious trouble in the few-shot case by missing important proposals for the novel classes during fine-tuning. We show that the proposal process can be improved by a carefully constructed cooperating RPN’s without substantial loss of performance for the base classes.\n\n3 OUR APPROACH\n\nWe believe that improving the population of RPN boxes seen by the classifier in training will always tend to improve a detector, and so focus on finding and fixing the effect within a standard few-shot object detection framework. Our proposed strategy is general and can work with different types of few-shot detectors. Here we consider the two most representative methods: the state-of-the-art DeFRCN (Qiao et al., 2021) and the widely used TFA (Wang et al., 2020). We first observe the box aspect ratio distribution shift problem through our pilot study, and show that na ̈ıve ensemble of RPN experts does not sufficiently solve this problem on our hard ARShift splits. Then we introduce our CoRPN’s that effectively tackles the aspect ratio distribution shift problem via the cooperation and diversity losses.\n\n3.1 BACKGROUND\n\nWe use the few-shot detection setting introduced in Kang et al. (2019). We split the dataset into two sets of categories: base classes Cb and novel classes Cn. The training process is two-phase: (1) base classes training, and (2) fine-tuning with novel classes. In phase 1, the model is trained with base class instances which results in a |Cb|-way detector. After base classes training, weights\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 2: The choice of base and novel class split can have a strong effect on the distribution of box aspect ratios. This histogram on COCO (a) and LVIS (b) shows: (blue) the distribution of aspect ratios for base classes; (pink) a randomly selected set of novel classes — note the bimodal structure, suggesting that different classes might have quite different bounding box distributions; and (yellow) for each of our selected hard set of novel classes in COCO-ARShift and LVIS-ARShift. Note that these classes are “rare” classes in LVIS so the histogram appears sparse. Experimental evidence (Tables 1, 2 and 3) suggests that a standard RPN trained on the base set performs poorly on our novel classes, while our CoRPN’s address the issue and substantially improve the performance. We believe that our hard splits in ARShift are relatively easy to generate and examples are provided in the appendix.\n\nMethod TFA w/ cos + CoRPN’s DeFRCN + CoRPN’s TFA w/ cos (Wang et al., 2020) DeFRCN (Qiao et al., 2021) DeFRCN Ensemble of Experts\n\nshot=1 28.1 37.0 19.1 31.0 29.7\n\n2 31.7 49.3 27.2 44.6 42.7\n\n3 36.0 55.1 28.0 47.5 50.9\n\n5 38.6 56.7 34.4 55.2 55.3\n\n10 44.7 59.0 42.1 57.2 56.0\n\nTable 1: Our proposed split VOC-ARShift causes serious problems for current state-of-the-art fewshot detectors, which are resolved by using CoRPN’s. CoRPN’s bring significant improvements (AP50) to all baseline models under distribution shift of base/novel class box aspect ratios. Note that na ̈ıve ensemble of RPN experts sometimes leads to even worse results, potentially due to the overspecialization of its individual RPNs. Results in bold are the better result between ours and the baselines. Results in red are the best.\n\nfor novel classes are randomly initialized, making the classifier a (|Cb| + |Cn|)-way classifier. In phase 2, the model is fine-tuned using either a set of few novel class instances or a balanced dataset containing both novel and base classes. The classifier sees the ground truth box and RPN boxes; it is typically trained to regard RPN boxes with IoU≥0.7 as positive, and with IoU<0.3 as negative. After the fine-tuning phase, we evaluate our model by average precision (AP) on novel and base categories. Although the focus of few-shot detection is the novel classes, since most test images contain instances from both base and novel classes, it is essential to maintain good performance on base classes.\n\nWe adopt the widely-used Faster R-CNN (Ren et al., 2015) as our base model. Faster R-CNN is a two-stage detector, which consists of a backbone image feature extractor, an RPN, followed by the region of interest (ROI) pooling layer, and a bounding box classifier and a bounding box regressor on top of the model. The RPN determines if a box is a foreground or a background box. Following the RPN is non-maximum suppression (NMS) which ranks and selects top proposal boxes. In phase 1, the whole model is trained on many-shot base class instances. Phase 2 fine-tunes part of the model on novel class instances with other parts frozen. Specifically, for TFA (Wang et al., 2020), only the top layer of the bounding box classifier and regressor are fine-tuned. For DeFRCN (Qiao et al., 2021), the whole model is fine-tuned except for the convolutions in the bounding box classifier and regressor.\n\n3.2 PILOT STUDY: BOX DISTRIBUTION SHIFT\n\nIn the real world, the bounding box distribution of these novel categories often differs from the base categories, resulting in unfair few-shot performances. Namely, the difference in the distribution of\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nOurs\n\nBaselines\n\nMethod DeFRCN + CoRPN’s DeFRCN (Qiao et al., 2021) DeFRCN Ensemble of Experts\n\n1-shot AP AP50 AP75 8.7 7.1 8.0\n\n15.3 13.0 14.3\n\n8.7 7.0 7.9\n\n2-shot AP50 AP75 13.4 20.5 12.5 20.3 20.6 13.0\n\nAP 14.7 14.3 14.2\n\n3-shot AP50 AP75 15.9 24.4 24.4 15.1 15.0 23.5\n\nAP 12.4 11.5 12.3\n\nTable 2: Our proposed split COCO-ARShift causes serious problems for current state-of-the-art few-shot detectors, which are resolved by using CoRPN’s. CoRPN’s consistently outperform the DeFRCN (Qiao et al., 2021) baseline in all cases (novel split in 1, 2 and 3 shots) and are superior to the ensemble of experts method overall. Results in red are the best.\n\nOurs\n\nBaseline\n\nMethod DeFRCN + CoRPN’s DeFRCN (Qiao et al., 2021) DeFRCN Ensemble of Experts\n\nAP 12.3 9.6 11.5\n\nSplit 1 AP50 AP75 31.0 22.9 26.0\n\n9.4 7.0 8.4\n\nAP 16.0 8.0 14.8\n\nSplit 2 AP50 AP75 18.8 27.5 1.7 19.1 12.1 24.3\n\nAP 13.7 9.0 12.4\n\nSplit 3 AP50 AP75 16.8 20.4 6.3 18.8 14.7 20.1\n\nEntire test set\n\nAP 15.3 15.0 14.8\n\nAP50 AP75 15.6 26.4 26.6 14.2 13.9 25.0\n\nTable 3: Our proposed three splits on LVIS (LVIS-ARShift) cause serious problems for current stateof-the-art few-shot detectors, which are resolved by using CoRPN’s. As a reference, we also provide the results on all rare classes in the entire LVIS test set. Our CoRPN’s outperform the baseline by large margins on all the splits with aspect ratio distribution shift, especially in AP75, suggesting that CoRPN’s improve the quality of detection under aspect ratio shift. CoRPN’s also marginally outperform the baseline on the entire test set in mAP and AP75. Results in red are the best.\n\nbox scale, aspect ratio, and center location all inhibit a successful transfer. Previous work tries to alleviate the scale issues with multiscale features and the location issues with translation invariance of convolution. However, these approaches fail to solve all of these problems, especially when the distribution between base and novel class box aspect ratios has a significant shift. We simulate this scenario by proposing new splits: our ARShift benchmark, on the PASCAL VOC, COCO and LVIS datasets that emphasize this distribution shift and the fairness to different aspect ratio distributions.\n\nWe manually select a set of classes that will likely have a different box distribution to the base categories. Figure 2 shows that our split has a more significant shift in the box aspect ratio distribution. As an na ̈ıve approach to alleviating this issue, we modified the RPN classifier to be an ensemble of expert RPN classifiers. Instead of using one RPN classifier for all proposals, we use 3 RPN classifiers for the anchors of 3 different aspect ratios (0.5, 1, and 2.0 respectively). The 3 RPN classifiers independently output their prediction for their respective anchors, which are combined as the final prediction. Tables 1, 2 and 3 show that compared to the baseline DeFRCN (Qiao et al., 2021), this approach represented by ‘DeFRCN Ensemble of Experts’ performs comparably and thus cannot fully address the issue. Intuitively, in this ensemble of experts method, individual RPN classifiers might be overspecializing and so facing generalization problems. Instead, we propose a method where we do not explicitly enforce each RPN classifier to specialize in an aspect ratio, but let the specialization emerge in the learning process. This method, named CoRPN’s, shows a large improvement over the baselines in Tables 1, 2 and 3.\n\n3.3 LEARNING COOPERATING RPN’S (CORPN’S)\n\nWe would like to alter the RPN to improve the population of boxes reported, especially on our ARShift benchmark where novel classes has a large box distribution shift. We expect that doing so affects mainly the few-shot case. As illustrated in Figure 3, we use multiple redundant RPN classifiers, but our goals imply that these RPN classifiers need to be trained to cooperate (i.e., they should not be a pure ensemble of experts). In what follows, we use the term RPN and RPN classifier interchangeably unless otherwise noted. In particular, we train and evaluate our RPN classifiers using an OR strategy – a box is classified with the label reported by the most confident RPN, which gets the gradient during training. This has two effects. First, the RPN’s can specialize to a degree, though we do not allow the RPN’s to drift too far apart. In turn, if one RPN misses a positive box, the other might find it. Second, the training strategy may improve the variation of proposals, which is especially essential when dealing with a different proposal box distribution in the few-shot case. Both effects may bring improvements to the model’s fairness with respect to aspect ratio distribution.\n\nFaster R-CNN’s RPN consists of a feature extractor, a binary classifier (which decides whether a box is foreground or background), and a bounding box regressor (which is not relevant to our current purpose). We do not intend for our RPN’s to use distinct sets of features, since it would introduce a large number of additional parameters, so we construct redundant classifiers while keeping both the feature extractor and the bounding box regressor shared between all RPN’s.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Illustration for our CoRPN’s network. Left is the original structure for the RPN in Faster R-CNN; right is our CoRPN’s, consisting of cooperating bounding box classifiers. Note that we only add multiple classifier heads that share features, so the model complexity of CoRPN’s does not significantly exceeds a regular RPN. For convenience, we use the term RPN and RPN classifier interchangeably.\n\nAn RPN with a single classifier is trained with a cross-entropy loss Lcls = LCE and produces a single prediction. In our case, we train N different binary classifiers simultaneously, and must determine (1) what prediction is made at test time and (2) what gradient goes to what classifier at training time. At test time, a given box gets the score from the most confident RPN. If the highest foreground probability is closer to one than the highest background probability, the box is predicted to be foreground; otherwise, it is predicted to be the background. In training time, merely taking the gradient of the best RPN score is not good enough, because the model may collapse to the trivial solution where one RPN scores all boxes, and the others do nothing interesting. For any foreground box, we want at least one RPN to have a very confident foreground prediction and all others to have good foreground scores too (so that no foreground box is missed).\n\nWe use the following strategy. For a specific anchor box i, each RPN j (of the N RPN’s) outputs a raw score rj i , r2 i ]. After applying a sigmoid, the jth RPN produces the foreground probability f j i ) for anchor box i. We choose the score from the j∗th RPN such that\n\ni , indicating if the box is a foreground box or not: ri = [r1 i = σ(rj\n\ni , . . . , rN\n\nj∗ = argminj min{f j\n\ni , 1 − f j\n\ni },\n\n(1)\n\nnamely the most certain RPN which produces probability closest to the edge of the [0, 1] interval. At training time, only the chosen j∗th RPN gets the gradient from anchor box i. The RPN selection procedure is per-box, and even adjacent boxes can pass through different RPN’s.\n\nOther than the standard cross-entropy loss, we use two additional loss terms: a diversity loss Ldiv encourages RPN’s to be distinct, and a cooperation loss Lcoop encourages cooperation and suppresses foreground false negatives. The final loss is Lcls := Lj∗\n\nCE + λdLdiv + λcLcoop,\n\n(2)\n\nwhere λd and λc are trade-off hyperparameters.\n\n3.4 ENFORCING DIVERSITY\n\nWe do not want our RPN’s to be too similar and prefer their specialization. For each positive anchor box, RPN responses should be different because we want different RPN’s to cover different types of proposal boxes. To this end, we propose a loss function to enforce diversity among RPN’s. Given a set of NA anchor boxes, the N RPN’s produce an N by NA matrix of probabilities F = [f 1, f 2, . . . , f N ]T . The covariance matrix is\n\nΣjk(F) = E[(f j − E[f j])(f k − E[f k])T ].\n\nWe define the diversity loss Ldiv by the log determinant loss\n\nLdiv := − log(det(Σ(F))).\n\n(3)\n\n(4)\n\nThis log determinant loss has been widely used in previous work (Boyd & Vandenberghe, 2008; Dhillon, 2008) to encourage diversity. By this diversity loss, we encourage the probability matrix to have rank N , so each RPN is reacting differently on the collection of NA boxes. This procedure ensures each RPN to be the most certain RPN for some boxes, so that every RPN is being selected and trained. Omitting this loss can cause some RPN classifier to receive little training.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nMethod TFA w/ cos + CoRPN’s DeFRCN + CoRPN’s TFA w/ cos (Wang et al., 2020) DeFRCN (Qiao et al., 2021) MPSR** (Wu et al., 2020) FsDetView (Xiao & Marlet, 2020) FSOD* (Fan et al., 2020)\n\n1-shot\n\nNovel Class 2-shot\n\nAP AP50 AP75 AP AP50 AP75 4.1 5.0 3.4 4.8 2.3 3.2 2.4\n\n9.6 16.1 8.3 16.3 6.3 13.3 5.9\n\n5.6 7.8 4.8 7.8 3.4 2.3 2.7\n\n4.4 4.8 3.8 4.4 2.3 1.4 2.0\n\n7.2 9.7 5.8 9.5 4.1 8.9 4.8\n\n5.4 8.6 4.6 8.5 3.5 4.9 2.9\n\n3-shot AP50 AP75 13.2 20.0 12.1 20.0 9.5 18.6 7.2\n\n7.2 10.4 6.5 10.3 5.1 2.9 3.3\n\nAP 7.1 10.9 6.6 10.7 5.2 6.7 3.7\n\n1-shot AP50 AP75 36.5 55.1 33.2 45.6 36.4 54.7 33.4 45.7 14.2 17.1 1.0 7.0 12.5 20.3\n\nAP 34.1 30.3 34.1 30.3 12.1 2.4 11.9\n\nBase Class 2-shot AP50 AP75 55.3 37.3 47.9 34.2 37.6 55.1 34.5 47.1 16.9 20.7 2.2 11.9 17.2 24.4\n\nAP 34.7 31.2 34.7 31.2 14.4 4.4 15.6\n\n3-shot AP50 AP75 55.2 37.6 35.1 48.1 37.9 54.8 35.1 47.9 18.3 23.3 2.2 13.6 19.0 27.3\n\nAP 34.8 32.0 34.7 31.8 15.8 4.9 17.4\n\nTable 4: CoRPN’s not only improve performance on our hard split, but also perform comparably with state of the art on more traditional splits without forgetting the base classes, hence improving model fairness for aspect ratio distribution. CoRPN’s mostly beat strong baselines in few-shot detection performance on the (extremely difficult) COCO novel classes task for 1, 2, and 3-shot cases. Results in bold are the better result between ours and the baselines. All approaches are evaluated following the standard procedure in Xiao & Marlet (2020). *Model re-evaluated using the standard procedure (with base and novel classes joint space) for a fair comparison. **Model evaluated using public code and pre-trained base classes detector.\n\nRatio 0.5 1.0 2.0\n\nNovel Set 1 RPN2 99.68% RPN3 99.68% RPN1 99.67%\n\nNovel Set 2 RPN5 99.72% RPN2 99.67% RPN1 99.59%\n\nNovel Set 3 RPN2 98.64% RPN2 99.83% RPN1 10.75%, RPN2 89.25%\n\nTable 5: Aspect ratio coverage of different RPN classifiers on VOC standard splits. For example, RPN1 refers to the first RPN classifier in the CoRPN’s. This provides evidance that CoRPN’s indeed learn to specialize in box aspect ratios, despite no direct supervision.\n\n3.5 LEARNING TO COOPERATE\n\nWe also want the RPN’s to cooperate so that they all agree to a certain extent for foreground boxes. We propose a cooperation loss to prevent any RPN from firmly rejecting any foreground box. For foreground box i, with the jth RPN, we define the cooperation loss\n\nLi,j\n\ncoop := max{0, φ − f j\n\ni },\n\n(5)\n\nwhere φ is a constant parameter (usually less than 0.5), acting as a lower bound for each RPN’s probability assigning to a foreground box. If an RPN’s response is below φ, that RPN is going to be penalized. The final cooperation loss is an average of cooperation losses over all foreground boxes and all RPN’s.\n\n4 EXPERIMENTS\n\nBenchmarks. We propose a new base/novel split on both the PASCAL VOC (Everingham et al., 2010), MS-COCO (Lin et al., 2014) and LVIS (Gupta et al., 2019) datasets to simulate the realworld scenario, where the novel class box distribution deviates from the base class counterpart. Our proposed split VOC-ARShift is similar to the conventional few-shot detection VOC split (Kang et al., 2019; Wang et al., 2020). We use the images and annotations of VOC (07 + 12) and select 15 classes as base classes, and leave the rest as novel classes. In our proposed COCO-ARShift split, we use the images and annotation of COCO 2014, select the 20 VOC classes as training, and select 10 out of the other 60 classes as novel classes. We select these classes to explicitly produce a distribution shift in the box aspect ratios, as shown in Figure 2. In our LVIS-ARShift benchmark, we use LVIS v0.5 and 10 shots following Wang et al. (2020). In each setting, the base classes are the 20 VOC classes in COCO, while the novel classes are 10 rare classes manually picked that have an aspect ratio distribution shift from the base classes. The detailed classes of our proposed splits are in Section H in the appendix.\n\nApart from our proposed ARShift setting, we also evaluate on two widely-used few-shot detection benchmarks (Kang et al., 2019; Wang et al., 2020) based on PASCAL VOC and COCO. For a fair comparison, we use the same train/test splits and novel class instances as in Kang et al. (2019); Wang et al. (2020) to train and evaluate all models. On COCO, we report base/novel classes AP, AP50, and AP75 under shots 1, 2, 3, 5, 10, and 30. On PASCAL VOC, we report AP50 for three different base/novel class splits under shots 1, 2, 3, 5, and 10. Following Wang et al. (2020) and Qiao et al. (2021), we use Faster R-CNN as our base model and use an ImageNet pre-trained (Russakovsky et al., 2015) ResNet-101 as the backbone, unless otherwise noted.\n\nTraining Procedure. Our training and fine-tuning procedures are consistent with previous work TFA (Wang et al., 2020) and DeFRCN (Qiao et al., 2021). On PASCAL VOC, at phase 1 base\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Qualitative results of our CoRPN’s + DeFRCN on COCO-ARShift on 5-shots, compared with the DeFRCN baseline. The results illustrate that our approach discovers novel objects missed by the baseline, and also has less misclassification.\n\nclasses training, each model is trained on the union set of VOC 07+12 trainval data. Evaluation is on the VOC 07 test set. At the fine-tuning phase, each model is fine-tuned with a balanced few-shot dataset sampled from VOC 07+12 that contains both base classes and novel classes. On COCO, following Wang et al. (2020), the fine-tuning phase is two-stage: at stage 1, we fine-tune the model on novel classes; at stage 2, we then fine-tune the model with a balanced few-shot dataset containing both base and novel classes. Please refer to Section C of the appendix for our implementation details and hyperparameters.\n\nBaselines and Evaluation Procedure. We mainly focus on comparing against the state-of-the-art baseline DeFRCN (Qiao et al., 2021), and a widely-used previous work TFA (Wang et al., 2020). Our approach incorporates CoRPN’s into the baseline models, while keeping other model components and design choices unchanged. In addition, we thoroughly compare with a variety of recent few-shot detectors, including FSOD (Fan et al., 2020), MPSR (Wu et al., 2020), FSDetView (Xiao & Marlet, 2020). These baselines address other aspects of few-shot detection which are different from us (Section 2), and their modifications are thus largely orthogonal to our effort here. Note that our evaluation follows the standard procedure in Wang et al. (2020). This standard procedure computes AP separately for novel and base categories for a detector that is engineered to detect both novel and base classes ((|Cb| + |Cn|)-way). We focus on the novel class performance, and also report the base class performance in Table 4. For work (Fan et al., 2020) with different evaluation procedures, we re-evaluate their methods with the standard procedure, so that results in Tables 4 can be different from the original reported results.\n\n4.1 MAIN RESULTS\n\nOur evaluation mainly focuses on our proposed ARShift splits, but also includes the conventional splits. Also, we focus on the extremely few-shot regime, which is the most challenging scenario for few-shot detection. Tables 1, 2 and 3 show the detection performance on our proposed ARShift splits where a bounding box aspect ratio distribution shift is present between base and novel classes. Table 4 summarize the detection results for base and novel classes in low shots on the conventional COCO benchmark, respectively. For completeness, the results for base and novel classes in higher shots on PASCAL VOC and COCO are summarized in Section B of the appendix, where our model also performs comparably. We present our qualitative results in Figures 4 and 6.\n\nCoRPN’s consistently outperform the baselines. Tables 1, 2 and 3 show that CoRPN’s greatly outperform the baseline TFA (Wang et al., 2020), DeFRCN (Qiao et al., 2021) and an aspect ratio ensemble modified from the DeFRCN model, especially in very low shots. Especially, we provide the results on three additional base and novel class splits on the LVIS dataset (Gupta et al., 2019) in Table 3. LVIS is a much more challenging long-tail recognition dataset, containing a large amount of rare classes, hence the fairness for aspect ratio is more crucial. We also provide the results on all rare classes in the LVIS test set as a reference. Our CoRPN’s outperform the state-of-the-art\n\n8\n\nBaselineOursBaselineOursBaselineOursBaselineOursBaselineUnder review as a conference paper at ICLR 2023\n\nMethod DeFRCN Qiao et al. (2021) CoRPN’s No Cooperation Loss No Diversity Loss\n\nThreshold\n\nφ = 0.1 φ = 0.4 φ = 0.7 φ = 0.9\n\nAP 13.6 18.8 14.7 13.8 14.2 16.8 14.7 14.8\n\nAP50 AP75 31.0 37.0 30.9 30.2 31.0 35.2 31.5 33.6\n\n9.6 17.2 11.7 10.0 11.1 13.5 11.6 10.1\n\nTable 6: Our diversity loss and cooperation loss are both required for CoRPN’s to obtain the largest improvement. A sub-optimal threshold in the cooperation loss also has an adverse effect on performance. The table shows 1-shot novel class performance of all models under our proposed VOCARShift novel split.\n\nDeFRCN baseline by large margins on all the splits with aspect ratio distribution shift, especially in AP75, suggesting that CoRPN’s improve the quality of detection in such scenarios. CoRPN’s also marginally outperform the DeFRCN baseline on the entire set of rare classes in mAP and AP75 on LVIS. On the conventional splits, as shown in Tables 4, CoRPN’s consistently improve over TFA for all shots, and also marginally outperform DeFRCN on the challenging COCO dataset. The combination of both results shows a significant improvement in aspect ratio distribution fairness in our model.\n\nCoRPN’s beat other state of the art. With our simple modification on RPN, we also outperform other sophisticated approaches in the very low-shot regime on the more challenging COCO dataset. In particular, we significantly outperform baselines that introduce attention mechanisms for adjusting proposal generation (Hsieh et al., 2019; Fan et al., 2020) under the standard evaluation procedure. We believe CoRPN’s could be combined with other approaches with improvements from different perspectives, such as exploiting better multi-scale representation (Wu et al., 2020), incorporating metric learning (Yang et al., 2020), or adding feature aggregation module (Xiao & Marlet, 2020) for further improvements.\n\nCoRPN’s don’t forget base classes. While improving detection on novel classes through finetuning, we maintain strong performance on base classes – there is no catastrophic forgetting (Table 4). By contrast, the base class performance of some state-of-the-art baselines dramatically drops, demonstrating that they cannot fairly detect novel and base classes.\n\n4.2 ABLATION STUDY\n\nWe investigate how the proposals of CoRPN’s change and conduct a series of ablations that evaluate the contribution of each loss component and different design choices. Specifically, we find that: (1) CoRPN’s specialize in different aspect ratios without explicit supervision; (2) Our cooperation loss and diversity loss are both necessary for CoRPN’s to improve fairness; (3) (in Section F of the appendix) CoRPN’s outperform other baselines such as with larger RPN sub-networks, an existing cosine loss based diversity, and bootstrapping.\n\nSpecialization for aspect ratios emerges in CoRPN’s training. Table 5 shows that the boxes of three different aspect ratios are handled by different RPN classifiers in CoRPN’s. Instead of explicitly training different RPN classifiers to handle different aspect ratios, CoRPN’s learn a more flexible specialization, improving the performance.\n\nCoRPN’s need both diversity and cooperation losses. Table 6 shows that after removing either loss, CoRPN’s does not improve the performance over the baseline. Also, when the threshold hyperparameter is suboptimal, the performance surpasses the baseline but still substantially underperforms the CoRPN’s with the optimal hyperparameter.\n\n5 CONCLUSION\n\nWe identify the bias of few-shot detectors towards familiar aspect ratio distribution. As illustrated in our ARShift benchmark, a substantial improvement on under-represented aspect ratio distribution can be obtained by our proposed CoRPN’s which produce more informative proposals. Our method achieves a new state of the art on both our proposed settings with hard base and novel splits and widely-used benchmarks in the very few-shot regime. This is achieved by training CoRPN’s with diversity and cooperation losses.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. YOLOv4: Optimal speed and\n\naccuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. 2\n\nStephen Boyd and Lieven Vandenberghe. Convex optimization - interior-point methods. 2008. 6\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\n\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 2\n\nHao Chen, Yali Wang, Guoyou Wang, and Yu Qiao. LSTD: A low-shot transfer detector for object\n\ndetection. In AAAI, 2018. 3\n\nInderjit S. Dhillon. The log-determinant divergence and its applications. 2008. 6\n\nNikita Dvornik, Cordelia Schmid, and Julien Mairal. Diversity with cooperation: Ensemble methods\n\nfor few-shot classification. In ICCV, 2019. 3, 17, 18\n\nIan Endres and Derek Hoiem. Category independent object proposals. In ECCV, 2010. 2\n\nMark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman.\n\nThe PASCAL visual object classes (VOC) challenge. IJCV, 88(2):303–338, 2010. 2, 7\n\nQi Fan, Wei Zhuo, Chi-Keung Tang, and Yu-Wing Tai. Few-shot object detection with attention-rpn and multi-relation detector. In CVPR, 2020. https://github.com/fanq15/FewX. 3, 7, 8, 9, 14\n\nZhibo Fan, Yuchen Ma, Zeming Li, and Jian Sun. Generalized few-shot object detection without\n\nforgetting. In CVPR, 2021. 3\n\nLi Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE TPAMI, 28\n\n(4):594–611, 2006. 3\n\nRoss Girshick. Fast R-CNN. In ICCV, 2015. 2\n\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accu-\n\nrate object detection and semantic segmentation. In CVPR, 2014. 2\n\nAgrim Gupta, Piotr Doll ́ar, and Ross Girshick. LVIS: A dataset for large vocabulary instance seg-\n\nmentation. In CVPR, 2019. 1, 7, 8\n\nGuangxing Han, Yicheng He, Shiyuan Huang, Jiawei Ma, and Shih-Fu Chang. Query adaptive few-shot object detection with heterogeneous graph convolutional networks. In ICCV, 2021. 3\n\nKaiming He, Georgia Gkioxari, Piotr Doll ́ar, and Ross Girshick. Mask R-CNN. In ICCV, 2017. 2\n\nTing-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, and Tyng-Luh Liu. One-shot object detection with In NeurIPS, 2019. https://github.com/timy90022/\n\nco-attention and co-excitation. One-Shot-Object-Detection. 3, 9, 14\n\nHanzhe Hu, Shuai Bai, Aoxue Li, Jinshi Cui, and Liwei Wang. Dense relation distillation with\n\ncontext-aware aggregation for few-shot object detection. In CVPR, 2021. 3\n\nBingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, and Trevor Darrell. Few-shot object\n\ndetection via feature reweighting. In ICCV, 2019. 1, 2, 3, 7\n\nHei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints. In ECCV, 2018. 2\n\nAoxue Li and Zhenguo Li. Transformation invariant few-shot object detection. In CVPR, 2021. 3\n\nBohao Li, Boyu Yang, Chang Liu, Feng Liu, Rongrong Ji, and Qixiang Ye. Beyond max-margin:\n\nClass margin equilibrium for few-shot object detection. In CVPR, 2021a. 3, 14, 17\n\nYiting Li, Haiyue Zhu, Yu Cheng, Wenxin Wang, Chek Sing Teo, Cheng Xiang, Prahlad Vadakkepat, and Tong Heng Lee. Few-shot object detection via classification refinement and distractor retreatment. In CVPR, 2021b. 3\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ́ar, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 1, 2, 7\n\nWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and\n\nAlexander C. Berg. SSD: Single shot multibox detector. In ECCV, 2016. 2\n\nAnton Osokin, Denis Sumin, and Vasily Lomakin. OS2D: One-stage one-shot object detection by\n\nmatching anchor features. In ECCV, 2020. 3\n\nJuan-Manuel P ́erez-R ́ua, Xiatian Zhu, Timothy Hospedales, and Tao Xiang. Incremental few-shot\n\nobject detection. In CVPR, 2020. 14\n\nLimeng Qiao, Yuxuan Zhao, Zhiyuan Li, Xi Qiu, Jianan Wu, and Chi Zhang. Defrcn: Decoupled faster r-cnn for few-shot object detection. In ICCV, 2021. 2, 3, 4, 5, 7, 8, 9, 13, 14, 15, 16, 17\n\nJoseph Redmon and Ali Farhadi. YOLO9000: better, faster, stronger. In CVPR, 2017. 2\n\nJoseph Redmon and Ali Farhadi. YOLOv3: An incremental improvement.\n\narXiv preprint\n\narXiv:1804.02767, 2018. 2\n\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,\n\nreal-time object detection. In CVPR, 2016. 2\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object\n\ndetection with region proposal networks. In NeurIPS, 2015. 2, 4\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 115(3):211–252, 2015. 7\n\nEli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias Marder, Sharathchandra Pankanti, Rogerio Feris, Abhishek Kumar, Raja Giries, and Alex M. Bronstein. RepMet: Representative-based metric learning for classification and one-shot object detection. In CVPR, 2019. 3\n\nBo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, and Chi Zhang. Fsce: Few-shot object detection via\n\ncontrastive proposal encoding. In CVPR, 2021. 3, 13, 14, 16, 17\n\nSebastian Thrun. Lifelong learning algorithms. Learning to learn, 8:181–209, 1998. 3\n\nKoen van de Sande, Jasper Uijlings, Theo Gevers, and Arnold Smeulders. Segmentation as selective\n\nsearch for object recognition. In ICCV, 2011. 2\n\nXin Wang, Thomas E. Huang, Trevor Darrell, Joseph E Gonzalez, and Fisher Yu. Frustratingly\n\nsimple few-shot object detection. In ICML, 2020. 1, 2, 3, 4, 7, 8, 13, 14, 15, 16, 17, 18\n\nYu-Xiong Wang, Deva Ramanan, and Martial Hebert. Meta-learning to detect rare objects. In ICCV,\n\n2019. 3\n\nAming Wu, Yahong Han, Linchao Zhu, and Yi Yang. Universal-prototype enhancing for few-shot\n\nobject detection. In ICCV, 2021. 3\n\nJiaxi Wu, Songtao Liu, Di Huang, and Yunhong Wang. Multi-scale positive sample refinement for few-shot object detection. In ECCV, 2020. https://github.com/jiaxi-wu/MPSR. 3, 7, 8, 9, 14, 17\n\nYang Xiao and Renaud Marlet. Few-shot object detection and viewpoint estimation for objects in\n\nthe wild. In ECCV, 2020. 3, 7, 8, 9, 17\n\nXiaopeng Yan, Ziliang Chen, Anni Xu, Xiaoxi Wang, Xiaodan Liang, and Liang Lin. Meta R-CNN:\n\nTowards general solver for instance-level low-shot learning. In ICCV, 2019. 1, 3\n\nYukuan Yang, Fangyun Wei, Miaojing Shi, and Guoqi Li. Restoring negative information in few-\n\nshot object detection. In NeurIPS, 2020. 3, 9, 14, 17\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nLu Zhang, Shuigeng Zhou, Jihong Guan, and Ji Zhang. Accurate few-shot object detection with\n\nsupport-query mutual guidance and hybrid loss. In CVPR, 2021. 3\n\nWeilin Zhang and Yu-Xiong Wang. Hallucination improves few-shot object detection. In CVPR,\n\n2021. 3\n\nXingyi Zhou, Jiacheng Zhuo, and Philipp Krahenbuhl. Bottom-up object detection by grouping\n\nextreme and center points. In CVPR, 2019. 2\n\nChenchen Zhu, Fangyi Chen, Uzair Ahmed, and Marios Savvides. Semantic relation reasoning for\n\nshot-stable few-shot object detection. In CVPR, 2021. 3, 14, 17\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nThis appendix provides additional experimental results and details that support the approach in the main paper and are not included there due to limited space. The seven sections include (1) additional results for higher-shots on the COCO-ARShift, comparisons with additional baselines for ARShift, and results on base classes before and after finetuning; (2) explainations for additional evaluation and experimental details; (3) discussion of the training and inference time and memory; (4) results on conventional splits; (5) analysis with additional ablation study; (6) addtional qualitative results; and (7) details of the proposed base and novel class splits on ARShift for PASCAL VOC, COCO, and LVIS.\n\nB ADDITIONAL RESULTS\n\nHigher-Shot Results on COCO-ARShift\n\nIn Table 2 of the main paper, we show that CoRPN’s outperform the DeFRCN (Qiao et al., 2021) baseline consistently on 1, 2, and 3 shots. Table 10 shows that this performance improvement persists in 5, 10, and 30 shots. In this scenario where more support instances are available, CoRPN’s consistently outperform the DeFRCN (Qiao et al., 2021) baseline.\n\nAdditional Baseline on ARShift\n\nIn Table 1 of the main paper, we mainly compare our method against the state-of-the-art method DeFRCN (Qiao et al., 2021) on our VOC-ARShift. In Table 11 we also evaluate an additional baseline FSCE (Sun et al., 2021). Note that other recently published methods represented by FSCE underperform our DeFRCN baseline by significant margins on our ARShift splits as well, so we do not include these other methods in our main paper.\n\nResults on Base classes\n\nHere we provide the detection results on base classes after base classes training (stage 1) and after fine-tuning (stage 2). As shown in Table 9, the performance of our CoRPN’s + DeFRCN on base classes is comparable with DeFRCN (Qiao et al., 2021), while we achieve large improvements on novel test classes as shown in Table 1 of the main paper. After fine-tuning, our CoRPN’s also do not forget base classes. Table 9 shows that the performance of our CoRPN’s + DeFRCN after fine-tuning is still comparable with the baseline.\n\nAverage Recall\n\nWe present the average recall (AR) result on the three splits in our LVIS-ARShift benchmark in Table 7. Here we show AR1000 by the convention of COCO. Our CoRPN’s also improve fewshot detection in the AR, meaning that our CoRPN’s miss fewer novel objects under aspect ratio distribution shift.\n\nPer-Category Result\n\nIn Table 8 we present the 1-shot AP50 for each novel class on our VOC-ARShift benchmark. Our CoRPN’s improve upon the vanilla DeFRCN baseline by large amounts in 4 of the 5 categories, while the naive ensemble of experts only achieves marginal improvements.\n\nAnalysis of Proposal Aspect Ratio\n\nFigure 5 shows the aspect ratio distribution of proposals from the baseline RPN and our CoRPN’s. Our CoRPN’s produce more diverse proposals that are robust to the aspect ratio distribution shift.\n\nC IMPLEMENTATION AND EVALUATION DETAILS\n\nImplementation Details and Hyperparameters. For ease of comparison, we use the same values for all shared training and fine-tuning hyperparameters (batch size, learning rate, momentum, weight decay, etc.) as the baselines Wang et al. (2020) and Qiao et al. (2021). CoRPN’s have the following additional hyperparameters: the number of RPN’s, the cooperation loss threshold φ, the diversity loss trade-off λd, and the cooperation loss trade-off λc. For COCO and LVIS, we directly used\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nOurs\n\nBaselines\n\nMethod DeFRCN + CoRPN’s DeFRCN (Qiao et al., 2021) DeFRCN Ensemble of Experts\n\nSplit 1 18.4 17.6 18.1\n\nSplit 2 24.8 18.5 23.4\n\nSplit 3 28.3 13.9 25.0\n\nTable 7: Average recall (AR1000) on our LVIS-ARShift benchmark. We achieve performance improvements over both baselines in recall.\n\nOurs\n\nBaselines\n\nMethod DeFRCN + CoRPN’s DeFRCN (Qiao et al., 2021) DeFRCN Ensemble of Experts\n\nTrain Bottle Aeroplane Horse 46.2 10.5 50.6 27.5 54.7 27.6\n\n43.4 23.2 24.3\n\n4.5 1.8 3.1\n\nBus 80.4 51.7 38.6\n\nTable 8: Per-Category 1-shot AP50 result on our VOC-ARShift benchmark. Our method achieves large improvement in 4 of the 5 classes.\n\nhyperparameter sets that worked well on PASCAL VOC. In Table 4, we report CoRPN’s detection results. We find that hyperparameters selected from PASCAL VOC are generalizable to the more challenging COCO and LVIS benchmark. Results reported in Table 4 are obtained with 5 RPNs, φ = 0.3, λc = 1, λd = 0.025 for TFA(Wang et al., 2020), and λc = 2, λd = 0.05 for DeFRCN (Qiao et al., 2021).\n\nSelection Procedure – Cumulative Variance. We summarized the hyperparameter selection criteria for PASCAL VOC in the experiment section in the main paper. The second criterion is the cumulative variance in the RPN’s response matrix to foreground boxes. Specifically, given a set of M anchor boxes, the N RPN’s produce an N by M matrix of probabilities F = [f 1, f 2, . . . , f N ]T . We run a principal component analysis (PCA) on F with N components and compare the cumulative percentage of variance explained by each component. We would like the variance to be distributed across components.\n\nEvaluation. As mentioned in the main paper, for a fair comparison, we use the standard evaluation procedure for all the compared models. We also compare against other approaches with the same novel class instances and test images. Authors of methods compared in this submission have helped us ensure the performance we report is a proper reflection of their methods, and we will acknowledge properly in any final version. In the standard evaluation procedure, when a test image comes in, the model has no assumption on what category the image contains (Wang et al., 2020). The detector’s classifier is (|Cb|+|Cn|)-way, detecting objects from a joint space of both base and novel categories. In the main paper, we marked results with * if they were re-evaluated under the standard procedure, and with ** if the original work used the standard procedure, but the results were not reported and so they were evaluated by us.\n\nSpecifically, below we include the details on how we obtained the results (with special marks) in the main paper; other results (without special marks) (Wang et al., 2020; P ́erez-R ́ua et al., 2020; Yang et al., 2020; Sun et al., 2021; Li et al., 2021a; Zhu et al., 2021) including the concurrent work (Sun et al., 2021; Li et al., 2021a; Zhu et al., 2021) are from the original papers.\n\n• We fine-tuned and evaluated MPSR using the publicly released code and the pre-trained\n\ndetection model for base classes (Wu et al., 2020).\n\n• We re-evaluated FSOD (Fan et al., 2020) and CoAE (Hsieh et al., 2019) under the standard procedure, using the publicly released code and the pre-trained model (Fan et al., 2020; Hsieh et al., 2019). FSOD uses a class-agnostic 2-way classifier that determines if an object is foreground or background. At inference time, FSOD produces a balanced number of proposals per novel category and collects these proposals for NMS. We adapt FSOD under the standard procedure such that there are a balanced number of proposals for all base and novel categories. For a fair comparison, we also fine-tuned FSOD with the same novel category instance(s) as in TFA (Wang et al., 2020).\n\n• We re-evaluated CoAE under the standard evaluation, using the publicly released code and a pre-trained model (Hsieh et al., 2019). For each test image containing a certain category, CoAE samples support image(s) from this category and collects boxes based on\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: The aspect ratio distribution of proposals from the baseline RPN and our CoRPN’s. Our CoRPN’s produce more diverse proposals robust to the aspect ratio distribution shift.\n\nMethod DeFRCN (Qiao et al., 2021) DeFRCN + CoRPN’s (Ours)\n\nAfter Phase 1 w/o fine-tuning 81.4 82.8\n\n1-shot 76.5 75.6\n\nAfter Phase 2 3-shot 76.9 75.7\n\n5-shot 77.0 75.5\n\n2-shot 77.0 75.8\n\n10-shot 76.7 75.4\n\nTable 9: Base classes AP50 on VOC-ARShift after base class training (phase 1) and after fine-tuning (phase 2). The same hyperparameter setting applies to both models. Notice that ours is higher after phase 1 and slightly lower but comparable with the results of DeFRCN after phase 2, but our method achieves significantly better performance on test classes as shown in Table 1 of the main paper (e.g., 7.3AP on 1-shot and 6.6 AP on 2-shot). Neither our method nor the baseline forgets base classes significantly.\n\nthe support image(s). At inference time, instead of providing each test image with support feature(s) from this category, we provide each test image support feature(s) from all base and novel categories. We then collect boxes from all categories and evaluate them. For a fair comparison, we also fine-tuned CoAE with the same novel category instance(s) as in TFA (Wang et al., 2020).\n\nD TRAINING & INFERENCE TIME AND MEMORY\n\nThe only architectural difference between CoRPN’s and the baseline RPNs of TFA (Wang et al., 2020) and DeFRCN(Qiao et al., 2021) is that CoRPN’s have multiple RPN classifiers. Note that we only duplicate the RPN classifiers, but not the entire RPN. In the baselines, the RPN’s classifier is a 1×1 convolutional layer, with input channels as the number of feature channels (256) and output channels as the number of cell anchors (3). Compared with the baselines, CoRPN’s with 5 RPN’s have four additional RPN classifiers and thus consist of 256×12 additional parameters. In our experiments, we find that CoRPN’s with 5 RPN’s increase the training time by only 3%, with roughly the same inference time and same memory footprint, compared with the baselines.\n\nE RESULTS ON CONVENTIONAL SPLITS\n\nIn Table 4 of the main paper, we show that CoRPN’s perform comparably or even better on conventional splits on 1, 2, and 3 shots on COCO. In this section we present more conventional split results. Table 12 shows PASCAL VOC conventional split results where our CoRPN’s perform comparably with the baselines. Table 13 shows the results on COCO 5, 10, and 30 shots, where our method also obtains similar performance to the baseline.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nMethod DeFRCN (Qiao et al., 2021) DeFRCN + CoRPN’s (Ours)\n\nAP 16.9 17.2\n\n5-shot AP50 AP75 27.4 18.0 18.3 27.0\n\n10-shot AP50 AP75 20.2 30.8 21.0 31.0\n\nAP 18.9 19.3\n\n30-shot AP50 AP75 22.2 34.5 34.9 22.0\n\nAP 21.2 21.4\n\nTable 10: When there are many shots (our proposed COCO-ARShift in 5, 10 and 30 shots), while the impact of using CoRPN’s is reduced, CoRPN’s still outperform the DeFRCN (Qiao et al., 2021) baseline in most cases. The reduced improvement is likely because missing objects in the RPN stage becomes a less important effect. Results in red are the best.\n\nMethod TFA w/ cos + CoRPN’s DeFRCN + CoRPN’s TFA w/ cos (Wang et al., 2020) FSCE (Sun et al., 2021) DeFRCN (Qiao et al., 2021) DeFRCN Ensemble\n\nshot=1 28.1 37.0 19.1 23.7 31.0 29.7\n\n2 31.7 49.3 27.2 33.5 44.6 42.7\n\n3 36.0 55.1 28.0 35.8 47.5 50.9\n\n5 38.6 56.7 34.4 41.5 55.2 55.3\n\n10 44.7 59.0 42.1 48.1 57.2 56.0\n\nTable 11: In Table 1 of the main paper, we mainly compare our method against the state-of-theart method DeFRCN (Qiao et al., 2021) on our VOC-ARShift. Here we evaluate the result of an additional baseline FSCE (Sun et al., 2021) (marked in green). Note that other recently published methods represented by FSCE underperform our DeFRCN baseline by significant margins on our ARShift splits as well, so we do not include these methods in our main paper.\n\nFigure 6: Additional qualitative results of our CoRPN’s + DeFRCN on COCO-ARShift on 5-shots, compared with the DeFRCN baseline.\n\nF ADDITIONAL ANALYSIS\n\nWe present some additional ablation studies for our CoRPN’s. Note that these ablations are performed on the conventional split. Even on conventional splits, our CoRPN’s outperform these naive strategies. Our CoRPN’s will likely achieve more performance improvement on our ARShift splits.\n\n16\n\nBaselineOursBaselineOursBaselineOursBaselineOursBaselineOursBaselineOursUnder review as a conference paper at ICLR 2023\n\nOurs\n\nMain baselines\n\nOther baselines\n\nMethod TFA w/ fc + CoRPN’s TFA w/ cos + CoRPN’s DeFRCN + CoRPN’s TFA w/ fc (Wang et al., 2020) TFA w/ cos (Wang et al., 2020) DeFRCN (Qiao et al., 2021) FRCN+ft-full (Wang et al., 2020) MPSR (Wu et al., 2020) NP-RepMet (Yang et al., 2020) CME (Li et al., 2021a) SRR-FSD (Zhu et al., 2021)\n\nshot=1 40.8 44.4 44.1 36.8 39.8 43.8 15.2 41.7 37.8 41.5 47.8\n\nNovel Set 1 2\n44.8 38.5 56.8 29.1 36.1 57.5 20.3 42.5 40.3 47.5 50.5\n\n3 45.7 46.4 62.2 43.6 44.7 61.4 29.0 51.4 41.7 50.4 51.3\n\n5 53.1 54.1 66.0 55.7 55.7 65.3 40.1 55.2 47.3 58.2 55.2\n\n10 54.8 55.7 65.5 57.0 56.0 67.0 45.5 61.8 49.4 60.9 56.8\n\nshot=1 20.4 25.7 31.8 18.2 23.5 31.5 13.4 24.4 41.6 27.2 32.5\n\nNovel Set 2 2\n29.2 29.5 41.2 29.0 26.9 40.9 20.6 29.3 43.0 30.2 35.3\n\n3 36.3 37.3 46.1 33.4 34.1 45.6 28.6 39.2 43.4 41.4 39.1\n\n5 36.5 36.2 49.9 35.5 35.1 50.1 32.4 39.9 47.4 42.5 40.8\n\n10 41.5 41.3 53.3 39.0 39.1 52.9 38.8 47.8 49.1 46.8 43.8\n\nshot=1 29.4 35.8 38.9 27.7 30.8 38.2 19.6 35.6 33.3 34.3 40.1\n\nNovel Set 3 2\n40.4 41.8 51.3 33.6 34.8 50.9 20.8 41.8 38.0 39.6 41.5\n\n3 44.7 44.6 54.6 42.5 42.8 54.1 28.7 42.3 39.8 45.1 44.3\n\n5 51.7 51.6 59.4 48.7 49.5 59.2 42.2 48.0 41.5 48.3 46.9\n\n10 49.9 49.6 61.5 50.2 49.8 61.9 42.1 49.7 44.8 51.5 46.4\n\nTable 12: CoRPN’s not only improve performance on our ARShift split, but also perform comparably with state of the art on more traditional splits: Few-shot detection (AP50) on PASCAL VOC novel classes under three base/novel splits in the generalized few-shot learning setting. CoRPN’s outperform the main baselines TFA and DeFRCN mostly in the very low shots, with comparable performance in the higher shots, regardless of classifier choice. Note that these other baselines address different aspects of few-shot detection, and could be combined with them for further improvements. All models are based on Faster R-CNN with a ResNet-101 backbone, and follow the evaluation procedure in Wang et al. (2020). Results in bold are the better result between ours and the main baselines.\n\nOurs\n\nMain baselines\n\nOther baselines\n\nMethod TFA w/ fc + CoRPN’s TFA w/ cos + CoRPN’s DeFRCN + CoRPN’s TFA w/ fc (Wang et al., 2020) TFA w/ cos (Wang et al., 2020) DeFRCN (Qiao et al., 2021) FsDetView (Xiao & Marlet, 2020) FSCE (Sun et al., 2021) CME (Sun et al., 2021) SRR-FSD (Zhu et al., 2021)\n\nBackbone ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101\n\n5-shot\n\n30-shot\n\n10-shot AP AP50 AP75 AP AP50 AP75 AP AP50 AP75 20.2 8.9 12.9 13.9 19.9 8.8 29.7 14.1 20.9 13.2 19.2 8.4 13.4 19.1 8.3 21.4 29.6 13.5 15.1 25.6 8.1 16.2 –\n– 17.8 24.6 –\n13.5 23.0 –\n\n16.9 16.4 25.2 16.0 15.3 24.7 20.1 –\n– –\n\n8.6 8.7 13.6 8.4 8.0 13.0 4.4 –\n– –\n\n9.8 10.1 16.8 9.2 9.3 16.7 6.5 10.5 16.4 9.8\n\n25.0 25.1 39.6 24.7 24.9 36.7 31.7 –\n28.0 29.2\n\n10.5 10.6 16.7 10.0 10.0 16.7 10.7 11.9 15.1 11.3\n\n13.5 13.9 20.8 13.4 13.7 21.0 15.9 16.4 16.9 14.7\n\nTable 13: When there are many shots, the impact of using CoRPN’s on the conventional split is reduced, likely because missing objects in the RPN stage becomes a less important effect. CoRPN’s perform acceptably for few-shot detection performance on the difficult COCO novel classes task for 5-shot, 10-shot, and 30-shot. *Model re-evaluated using the standard procedure (with base and novel classes joint space) for a fair comparison. ‘–’ denotes that numbers are not reported in the corresponding paper. CoRPN’s consistently outperform the main baseline DeFRCN except for the 30-shot case.\n\nBootstrapping. Table 1 in the main paper shows that our CoRPN’s outperform naive ensembles. Here we further compare CoRPN’s with a bootstrapping baseline in Table 14. We construct the bootstrapping baseline by using multiple RPN classifiers, the number of which is the same as CoRPN’s. Instead of selecting the most confident RPN to get gradients, we randomly select an RPN to get gradients during training. There are no additional loss terms in training these RPN classifiers. In the fine-tuning stage and the inference time, we use the same selection procedure as CoRPN’s (i.e., using the most certain RPN).\n\nDiversity loss in Dvornik et al. (2019). In Table 14, we also compare our CoRPN’s with (Dvornik et al., 2019), which utilizes a pairwise cosine similarity based diversity loss. Our CoRPN’s also outperform this baseline with a considerable margin.\n\nLarger RPN. We compare CoRPN’s with a baseline with larger RPN sub-networks in Table 15. We find that using larger RPN sub-networks does not improve performance, suggesting that the advantage of CoRPN’s is not simply the result of using more parameters. In CoRPN’s, all RPN classifiers share the same RPN feature extractor, as shown in Figure 4 (main paper). We enlarge the feature dimension in the RPN. We implemented two options: a large RPN where the RPN feature extractor’s output channels increase from 256 to 272, and a larger RPN where the output channels double to 512. We also modified the RPN classifier and bounding box regressor to take in larger features for both options. Table 15 shows that simply enlarging the model capacity and the anchor density of the original RPN cannot improve the few-shot detection performance.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nMethod\n\n2 RPN’s, Bootstrapping 2 RPN’s, Dvornik et al. (2019) 2 RPN’s, CoRPN’s (Ours)\n\nAP50\n\n31.8 32.4 35.8\n\nTable 14: Our diversity term – the log-determinant loss – offers improvements over a bootstrapping baseline and a pairwise cosine similarity based diversity loss in Dvornik et al. (2019). The table shows novel class AP50 under PASCAL VOC novel split 3, shot 1.\n\nMethod TFA, original TFA, Large RPN TFA, Larger RPN TFA + CoRPN’s, 2 RPN’s TFA + CoRPN’s, 5 RPN’s TFA + CoRPN’s, 10 RPN’s\n\n# param added AP50 30.8 25.9 27.8 35.8 34.8 35.7\n\n0 28×256 268×256 3×256 12×256 27×256\n\nTable 15: Performance improvements of CoRPN’s are not due to the increased number of parameters in CoRPN’s. This table shows the novel class AP50 of TFA (Wang et al., 2020) with large RPN and even larger RPN, and compares with CoRPN’s with different numbers of RPN’s. All results are with PASCAL VOC novel split 3, shot 1. The second column presents how many additional parameters are introduced to the original TFA model. Using larger RPN sub-networks does not improve the performance, while CoRPN’s significantly improve with fewer added parameters.\n\nG ADDITIONAL QUALITATIVE RESULTS\n\nIn Figure 6 we show some additional qualitative result of our CoRPN’s model on COCO. Notably, our model could successfully discover novel objects omitted by the baseline, and also successfully classify some novel objects that cause confusion for the baseline method.\n\nH DATA SPLITS\n\nPASCAL VOC Split. For our proposed PASCAL VOC-ARShift split, the base classes include “bicycle”, “boat”, “car”, “cat”, “chair”, “diningtable”, “dog”, “person”, “sheep”, “tvmonitor”, “bird”, “pottedplant”, “cow”, “motorbike”, and “sofa”; the novel classes include “train”, “bottle”, “aeroplane”, “horse”, and “bus”.\n\nCOCO Split. For our proposed COCO-ARShift split, the base classes include the 20 VOC classes in COCO; the novel classes include “hot dog”, “tennis racket”, “fire hydrant”, “laptop”, “suitcase”, “frisbee”, “teddy bear”, “bowl”, “kite”, and “elephant”.\n\nLVIS Splits. For our proposed LVIS-ARShift splits, the base classes include the 20 VOC classes in LVIS; the novel classes of the first split include “Loafer (type of shoe)”, “batter (food)”, “cabin car”, “cylinder”, “egg roll”, “liquor”, “nailfile”, “plow (farm equipment)”, “vinegar”, and “yoke (animal equipment)”; the novel classes of the second split include “broach”, “burrito”, “cargo ship”, “crayon”, “incense”, “peeler (tool for fruit and vegetables)”, “pin (non jewelry)”, “roller skate”, “tinsel”, and “vodka”; the novel classes of the third split include “ax”, “beaker”, “ferry”, “fish (food)”, “funnel”, “incense”, “needle”, “space shuttle”, “stepladder”, and “vulture”.\n\n18",
    "reference": "# Summary Of The Paper\n\nThis paper claims that in few-shot detection tasks, performance can be degraded due to the lack of RPN ability to find object proposals for new categories with aspect ratios significantly different from those of the base categories. To overcome the limitations of RPN in few-shot detection, this paper uses multiple RPNs that have been trained in the direction of enhancing diversity among RPNs and cooperating with each other. To adequately validate this benefit of the proposed method, base/novel categories split of datasets are modified so that the (few-shot) images of the novel categories have very different aspect ratios from images of the base categories, which are referred ARShift. Object detectors trained using the proposed method successfully increases the accuracy for the novel categories, avoiding the performance degradation caused by the catastrophic forgetting issue for the base categories in the few-shot detection task on the three datasets, PASCAL VOC, COCO, and LVIS datasets.\n\n# Strength And Weaknesses\n\n* Strengths\n1. For the test environment using the ARShift benchmark, it was effective to significantly increase the few-shot detection accuracy using the proposed method on the three detection datasets. It also prevented performance degradation for the base categories, which is often caused by the catastrophic forgetting problem that is common in a few-shot detections.\n\n* Weaknesses\n1. The problem in a specific environment of the few-shot detection problem, which has a large difference in the aspect ratio distribution of the basic category and the novel category, as claimed in this paper, is of interest to only a few researchers, and the applicability/generality of the problem is very Limited. Thus, the environment claimed in this paper did not appear in the general dataset, but only in the dataset that was modified to emphasize the environment.\n\n2. In addition, the proposed method using multiple RPNs is not designed to address a claimed problem. Experimentally, several RPNs were specialized in generating region proposals of specific aspect ratios (Tab 5), but this function was not considered at all in the CoRPN design. \n\n3. Paper presentation is immature yet.\n- Figure 1 is not mentioned anywhere in the manuscript's content.\n- The same explanation about using Faster R-CNN are given in the last paragraph of Section 3.1 and the second paragraph of Section 3.3.\n- In Figure 2, it's hard to figure out what 'density' is as the sum of the densities over the entire aspect ratio range seems to be greater than 1.\n- In the top row of Figure 3, there are no explanations about the difference betwee the gray and blue box. In addition, non-maximum suppression is applied after performing bouding box regression so the illustrations in the bottom row of Figure 3 may bring incorrect information. Most of all, this figure does not deliver important information and nor is hard to understand as it has to occupy the large space of the manuscript.\n- In eq 3, Sigma may be Sigma_jk\n- Important details of experimental setup is missing such as the number of RPNs. According to Tab. 5, the number of RPNs may be larger than five.\n\n4. Some questionable impact of the proposed method based on the experimental results\n- In Tab. 5, only two or three RPNs appear to be activated. Does it mean the proposed method uses multiple RPNs inefficiently.\n- What is the version of CoRPN's with the best accuracy? Is it the model trained with the Cooperation loss with phi=0.5 and the diversity loss? If so, the CoRPN seems to be very sensitive to the phi.\n\n5. Similar problems and similar method designs have already been claimed in [a]. [a] is applied to general object detection and more experts than RPN are used for the classifier, but since it seems very simple to apply it to RPN, comparison with [a] will be necessary.\n\n[a] H. Lee, Multi-expert R-CNN for Object Detection, In IEEE TIP.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- As mentioned in the weaknesses, the manuscript must be revised thoroughly to improve its clarity and presentation quality.\n- As for novelty, it doesn't seem to meet the ICLR acceptance criteria.\n- Since the important implementation details (i.e., the number of RPNs) is missing, it is difficult to be reproduced.\n\n# Summary Of The Review\n\nMy rating was based on the points presented in Weaknesses. My biggest concern is that there is no consistency between the problem to be claimed and the method design to solve the problem.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSEMPPL: PREDICTING PSEUDO-LABELS FOR BETTER CONTRASTIVE REPRESENTATIONS\n\nMatko Boˇsnjak, Pierre H. Richemond, Nenad Tomasev, Florian Strub, Jacob C. Walker Felix Hill, Lars Holger Buesing, Razvan Pascanu, Charles Blundell, Jovana Mitrovic DeepMind {matko, richemond, mitrovic}@deepmind.com\n\nABSTRACT\n\nLearning from large amounts of unsupervised data and a small amount of supervision is an important open problem in computer vision. We propose a new semisupervised learning method, Semantic Positives via Pseudo-Labels (SEMPPL), that combines labelled and unlabelled data to learn informative representations. Our method extends self-supervised contrastive learning—where representations are shaped by distinguishing whether two samples represent the same underlying datum (positives) or not (negatives)—with a novel approach to selecting positives. To enrich the set of positives, we leverage the few existing ground-truth labels to predict the missing ones through a k-nearest neighbours classifier by using the learned embeddings of the labelled data. We thus extend the set of positives with datapoints having the same pseudo-label and call these semantic positives. We jointly learn the representation and predict bootstrapped pseudolabels. This creates a reinforcing cycle. Strong initial representations enable better pseudo-label predictions which then improve the selection of semantic positives and lead to even better representations. SEMPPL outperforms competing semisupervised methods setting new state-of-the-art performance of 68.5% and 76% top-1 accuracy when using a ResNet-50 and training on 1% and 10% of labels on ImageNet, respectively. Furthermore, when using selective kernels, SEMPPL significantly outperforms previous state-of-the-art achieving 72.3% and 78.3% top-1 accuracy on ImageNet with 1% and 10% labels, respectively, which improves absolute +7.8% and +6.2% over previous work. SEMPPL also exhibits stateof-the-art performance over larger ResNet models as well as strong robustness, out-of-distribution and transfer performance. We release the checkpoints and the evaluation code at https://github.com/deepmind/semppl.\n\n1\n\nINTRODUCTION\n\nIn recent years, self-supervised learning has made significant strides in learning useful visual features from large unlabelled datasets [Oord et al., 2018; Chen et al., 2020a; Mitrovic et al., 2021; Grill et al., 2020; Caron et al., 2021]. Moreover, self-supervised representations have matched the performance of historical supervised baselines on the ImageNet-1k benchmark [Russakovsky et al., 2015] in like-for-like comparisons as well as outperformed supervised learning in many transfer settings [Tomasev et al., 2022]. While such results show exciting progress in the field, in many real-wold applications often there exists a small amount of ground-truth labelled datapoints making the problem of representation learning semi-supervised.\n\nIn this work we propose a novel approach to semi-supervised learning called Semantic Positives via Pseudo-Labels (SEMPPL) which incorporates supervised information during the representation learning stage within a self-supervised loss. Unlike previous work which uses the available supervision as targets within a cross-entropy objective, we propose to use the supervised information to help inform which points should have similar representations. We propose to learn representations using a contrastive approach, i.e. we learn the representation of a datapoint (anchor) by maximizing the similarity of the embedding of that datapoint with a set of similar points (positives), while simultaneously minimizing the similarity of that embedding with a set of dissimilar points (negatives). As such, the appropriate construction of these sets of positives and negatives is crucial to the success of\n\n1\n\nPublished as a conference paper at ICLR 2023\n\ncontrastive learning methods. While strategies for sampling negatives have been extensively studied in the literature [Schroff et al., 2015; Harwood et al., 2017; Ge et al., 2018; Wang et al., 2019a; He et al., 2020; Chen et al., 2020c], the sampling of positives has received far less attention.\n\nWe propose a novel approach to selecting positives which leverages supervised information. Specifically, we propose using the small amount of available ground-truth labels in order to nonparametrically predict the missing labels (pseudo-labels) for the unlabelled data. Note that many previous semi-supervised approaches use pseudo-labels as targets within a cross-entropy-based objective [Van Engelen & Hoos, 2020; Yang et al., 2021]. In SEMPPL we use pseudo-labels in a very different way, i.e. we use them to select positives based on whether two datapoints (we call these semantic positives) share the same (pseudo-)label. By maximizing the similarity of a datapoint with its semantic positives we expect to learn representations that are more semantically aligned and as a consequence encode more abstract, higher-level features which should generalise better. To predict informative pseudo-labels, we compare the representations of the unlabelled data with those of the labelled subset and use a k-nearest neighbours (k-NN) classifier to impute the missing labels.\n\nWe simultaneously learn the representation, predict pseudo-labels and select semantic positives. This creates a virtuous cycle: better representations enable better pseudo-label prediction which in turn enables better selection of semantic positives and thus helps us learn better representations. Importantly, as the prediction of pseudo-labels and selection of semantic positives does not depend on the exact form of the contrastive objective employed, SEMPPL is compatible with and complements all contrastive losses, e.g. [Chen et al., 2020a;b; Caron et al., 2020; He et al., 2020; Mitrovic et al., 2021] and may even be extended to non-contrastive losses [Grill et al., 2020; Chen & He, 2021].\n\nWe evaluate the representations learned with SEMPPL across a varied set of tasks and datasets. In particular, SEMPPL sets new state-of-the-art in semi-supervised learning on ImageNet with 1% and 10% of labels on the standard ResNet-50 (1×) architecture with respectively 68.5% and 76.0% top1 performance and across larger architectures. When combined with Selective Kernels [Li et al., 2019b], we achieve 72.3% and 78.3% top-1 performance with 1% and 10% labels, respectively, significantly outperforming previous state-of-the-art by absolute +7.8% and +6.2% in top-1 performance. We also outperform previous state-of-the-art on robustness and out-of-distribution (OOD) generalisation benchmarks while retaining competitive performance in transfer learning.\n\nOur main contributions are:\n\n• We extend contrastive learning to the semi-supervised setting by introducing the idea of estimating pseudo-labels for selecting semantic positives as a key component especially in the low-label regime,\n\n• We propose a novel semi-supervised method SEMPPL that jointly estimates pseudo-labels, selects semantic positives and learns representations which creates a virtuous cycle and enables us to learn more informative representations,\n\n• We extensively evaluate SEMPPL and achieve a new state-of-the-art in semi-supervised learning, robustness and out-of-distribution generalisation, and competitive performance in transfer.\n\n2 SEMANTIC POSITIVES VIA PSEUDO-LABELS\n\nThe selection of appropriate positive and negative examples are the cornerstone of contrastive learning. Though the research community has mainly focused on the selection of negatives, positives are equally important as they play a vital role in learning semantic similarity. We thus leverage labelled information as it encodes semantic information to improve the selection of informative positives. Specifically, we expand a self-supervised model to use this labelled data to non-parametrically predict pseudo-labels for the remaining unlabelled data. Using both ground-truth labels and the predicted pseudo-labels, we expand the set of positives with semantic positives.\n\nNotations Let D = Dl ∪ Du be a dataset consisting of labelled training data Dl = {(xi, yi)}N i=1 and unlabelled training data Du = {(xj)}M j=N +1 with M ≫ N . Let B be a batch of data of size B with B = {(xi, yi)}b j=b+1 where (xi, yi) ∈ Dl and xj ∈ Du, where the indices i, j and m to denote labelled, unlabelled, and all datapoints, respectively. Following established self-supervised learning practices [Chen et al., 2020a;b; Caron et al., 2020; Mitrovic et al., 2021; Dwibedi et al., 2021; Tomasev et al., 2022], we create different views of the data by applying pairs of randomly\n\ni=1 ∪ {xj}B\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Sketch of SEMPPL. (Left) Standard contrastive pipelines. (Middle) Unlabelled data are tagged with pseudo-labels by using a k-NN over projected labelled data. (Right) Semantic positives are queried from the queue and processed to compute an additional contrastive loss.\n\nsampled augmentations a1, a2 ∼ A from the augmentation distribution A proposed in Chen et al. [2020a]. For every datapoint xm ∈ D we denote the corresponding augmentations as xa1\n\nm , xa2 m .\n\nm ) and za2\n\nm,t = ft(xa2\n\nm with a target encoder network ft, i.e. we get latent representations za1\n\nAugmentation positives We embed one data view xa1 m via an online encoder network f and embed the other data view xa2 m = f (xa1 m ). Note that the weights of ft are an exponential moving average of the weights of f . Next, we pass these latent representations through projection and prediction multilayer perceptrons. Specifically, we use an online projector g and target projector gt, as well as an online predictor h, to further transform za1 m,t; again, the weights of gt are an exponential moving average of the weights of g. We then get ˆza1 m,t) and l2normalise these; we use ˆza1\n\nm,t onward as the normalised latent embeddings.\n\nm,t = gt(za2\n\nm = h(g(za1\n\nm )) and ̃za2\n\nm and za2\n\nm , ̃za2\n\nIn order to learn the representation of ˆza1 ̃za2 m,t as well as against negatives. For this, we use the contrastive loss:\n\nm , we contrast it against the augmentation-based positive\n\nLAUGM = −\n\nB (cid:88)\n\nm=1\n\nlog\n\nφ(ˆza1\n\nm , ̃za2\n\nm,t) + (cid:80)\n\nφ(ˆza1\n\nm,t)\n\nm , ̃za2 xn∈N (xm) φ(ˆza1\n\nm , ̃za2\n\nn,t)\n\n(1)\n\nwhere N (xk) is the set of negatives, randomly uniformly sampled from the current batch, ̃za2 n,t = gt(ft(xn)) the target network projection of the negative sample; φ(x1, x2) = τ · exp(⟨x1, x2⟩/τ ) is the scoring function, τ > 0 is a scalar temperature, and ⟨·, ·⟩ denotes the Euclidean dot product. Since the representations we contrast are l2-normalised, the dot product effectively turns into cosine similarity.\n\nPseudo-label prediction and semantic positives Since we have access to a small labelled dataset, we can use the label information to select more informative positives beyond just augmentations of the original image. Specifically, we can associate images with the same label as positives and we call these semantic positives. We want to select semantic positives for all the data, not just the labelled subset. For this purpose, we propose to compute pseudo-labels for the unlabelled data and use this to select semantic positives. To compute pseudo-labels we compare the current latent embeddings of the unlabelled data to those of the labelled data. Specifically, we propose to use a first-in-first-out queue Q with capacity C for storing labelled embeddings which we use for computing the pseudolabels. At the start of training, we simply initialise the queue with random vectors, and use the queue\n\n3\n\nEmbeddingProjectionSemantic positiveLossPredictionProjectionProjectionProjectionProjectionK-NNQueryPseudo labelsEmbeddingProjectionTarget NetworkNegativesLossTarget NetworkOnline NetworkOnline NetworkEmbeddingProjectionPredictionOnline NetworkPredictionQueuePublished as a conference paper at ICLR 2023\n\nfrom the first step. For each batch B, we add the target projection of only the labelled data to the queue, i.e. Q ← ( ̃za2 i,t, yi). To predict a pseudo-label for an unlabelled datapoint xj, we first compute j , before retrieving its k-nearest neighbours {( ̃za2 the online predictor output ˆza1 s=1 in cosine similarity from the queue Q.1 Finally, we compute the pseudo-label ̄yj of xj as:\n\ns,t, ys)}k\n\n ̄yj = mode\n\n{( ̃za2\n\ns,t, ys)}k\n\ns=1\n\nys\n\n(2)\n\nwhere mode is the mode of the set, tasked with obtaining the most frequent class in the k-nearest neighbours. We use the ground-truth labels (for the labelled data) or the computed pseudo-labels (for the unlabelled data) to select semantic positives for every datapoint in B. For each xm ∈ B, we uniformly sample over all the embeddings in Q that share the same (pseudo-) label as xm to get a semantic positive ̃za2,+ l,t , yl) ∈ Q | yl = pl(xm)}), where pl(xm) = ym if xm is labelled and pl(xm) = ̄ym if xm is unlabelled. Next, we include these semantic positives within our representation learning process through the contrastive objective\n\nm,t ∼ U ({( ̃za2\n\nLSEMPOS = −\n\nB (cid:88)\n\nm=1\n\nlog\n\nφ(ˆza1 m,t ) + (cid:80)\n\nφ(ˆza1\n\nm , ̃za2,+\n\nm , ̃za2,+ m,t )\n\nxn∈N (xm) φ(ˆza1\n\nm , ̃za2\n\nn,t)\n\n(3)\n\nTaking these two losses (1) and (3) together, we propose to learn representations in our method SemPPL by minimising the following total loss\n\nLSEMPPL = LAUGM + αLSEMPOS\n\n(4)\n\nwhere α controls the ratio between these sub-losses.\n\n2.1\n\nIMPLEMENTATION DETAILS\n\nArchitecture We use Residual Networks [He et al., 2016] (v1; pre-activation as customary in the literature) for f and ft and use either 50 or 200 layers deep networks and with a width multiplier ranging from 1× to 4×. As in [Grill et al., 2020; Tomasev et al., 2022], we use multi-layer perceptrons with 2 layers of size 4096 and 256, with batch normalisation [Ioffe & Szegedy, 2015] and rectified linear activation.\n\nSelf-supervised learning method We use RELICv2 [Tomasev et al., 2022] as our default selfsupervised training objective due to its competitive performance. Therefore, we add an invariance penalty on top of Equation 4 to further enforce the similarity constraints and regularize the learning process as detailed in Appendix B. We also explore other self-supervised learning objectives in Section 4.\n\nAlgorithm parameters We use a queue of capacity C = 20B, with batch size B = 4096, and temperature τ = 0.2 while randomly sampling negatives from the current batch; we take |N (x)| = 10 negatives in total. For augmentations, we use the standard SIMCLR augmentations [Chen et al., 2020a] and the RELICV2 multi-crop and saliency-based masking [Tomasev et al., 2022]; we use 4 large views and 2 small views for augmentation positives and 3 semantic positives. The semantic positives are computed with a k-NN with k = 1 (see the analysis section in Appendix D); we build a single k-NN instance per augmentation a queried with all the augmentations where |a| = 4. This produces |a|2 = 16 k-NN induced pseudo-labels in total for each unlabelled image among which we then perform majority voting to compute the final pseudo-label.\n\nOptimisation Our networks are optimized with LARS [You et al., 2017]. Our base learning rate is 0.3 and we train our models for 300 epochs with a learning rate warm-up period of 10 epochs and cosine decay schedule thereafter. We use a weight decay of 10−6 and batch size B = 4096. We exclude the biases and batch normalisation parameters both from LARS adaptation and weight decay. The exponential moving average parameter for target networks is 0.996. Our pseudo-code is described in the appendix along with precise architectural and implementation details. Pretrained model checkpoints and code will be made available on GitHub.\n\n1We use the cosine similarity as the embeddings are normalised.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n3 EXPERIMENTAL RESULTS\n\nTo evaluate SEMPPL, we pre-train representations using 1% and 10% labelled data from the ImageNet dataset [Russakovsky et al., 2015] based on the splits from Chen et al. [2020a]. We then test SEMPPL in semi-supervised classification, robustness and out-of-distribution generalisation tasks. Lastly, we probe the transfer capabilities of the representations to other image classification datasets. For a complete set of results and experimental details, please see the Appendix A.\n\n3.1 SEMI-SUPERVISED LEARNING\n\nIn Table 1, we report top-1 accuracy on the ImageNet test set when either 1% or 10% of the data is labelled for the ResNet-50 architecture as well as deeper and wider ResNets. SEMPPL achieves top-1 accuracy of 68.5% with 1% of labels, significantly outperforming the previous state-of-the-art SimMatch [Zheng et al., 2022] by an absolute +1.3% in ImageNet test accuracy. With 10% of label data, our top-1 accuracy on ResNet-50 reaches 76.0%, outperforming the previous state-of-the-art PAWS [Assran et al., 2021] in semi-supervised learning. SEMPPL outperforms competing representation learning methods across the board, achieving state-of-the-art performance on all ResNet-50 2×, ResNet-50 4× and , in both the 1% and 10% labelled settings. SEMPPL does not use, and therefore excludes from comparison, distillation from larger networks as in [Chen et al., 2020b; Pham et al., 2021].\n\nSimilar to [Chen et al., 2020b], we also tested SEMPPL on ResNets with Selective Lernels (SK) [Li et al., 2019b]. This increases the encoder parameter count to 27.5M. We thus achieve a new absolute state-of-the-art of 72.3% and 78.3% top-1 accuracies, respectively, when using 1% and 10% of labelled data. Finally, SEMPPL reaches a new state-of-the-art using 76.0 and 80.5 on 1% and 10% of labels without self-distillation with a ResNet-200 2× + SK architecture.\n\nFor implementation details of the semi-supervised results and additional results, see the Appendix A.1.\n\nTable 1: Top-1 accuracy (in %) for ResNet encoders with different depth and width.\n\nResNet-50 1× ResNet-50 2× ResNet-50 4× ResNet-200 2×\n\nMethod\n\nTop-1\n\nTop-1\n\nTop-1\n\nTop-1\n\nSimCLR [Chen et al., 2020a] BYOL [Grill et al., 2020] RELICv2 [Tomasev et al., 2022] SimCLRv2 [Chen et al., 2020b] CoMatch [Li et al., 2021a] PAWS [Assran et al., 2021] SimMatch [Zheng et al., 2022] SemPPL (ours)\n\nSimCLRv2 + SK [Chen et al., 2020b] SemPPL + SK (ours)\n\n1%\n\n48.3 53.2 58.1 57.9 66.0 66.5 67.2 68.5\n\n64.5 72.3\n\n10%\n\n65.6 68.8 72.4 68.4 73.7 75.5 74.4 76.0\n\n72.1 78.3\n\n1%\n\n58.5 62.2 64.7 66.3 -\n69.6 -\n71.9\n\n70.6 74.5\n\n10%\n\n71.7 73.5 73.7 73.9 -\n77.8 -\n78.6\n\n77.0 79.8\n\n1%\n\n63.0 69.1 69.5 -\n- 69.9 -\n72.5\n\n- -\n\n10%\n\n74.4 75.7 74.6 -\n- 79.0 -\n79.3\n\n- -\n\n1%\n\n- 71.2 72.1 -\n- -\n- 74.8\n\n- 76.0\n\n10%\n\n- 77.7 76.4 -\n- -\n- 80.4\n\n- 80.5\n\n3.2 ROBUSTNESS AND OOD GENERALISATION\n\nWe evaluate the robustness and generalisation abilities of SEMPPL on ImageNetV2 [Recht et al., 2019], ImageNet-C [Hendrycks & Dietterich, 2019], ImageNet-R [Hendrycks et al., 2021] and ObjectNet [Barbu et al., 2019] which have all been purposefully constructed to test different robustness and generalisation aspects. We evaluate all three variants on ImageNetV2: matched frequency (MF), Threshold 0.7 (T-0.7) and Top Images (TI). When evaluating PAWS, we used the publicly available checkpoints. Table 2 shows good robustness and generalisation ability of the representations learned with SEMPPL. SEMPPL sets the new state-of-the-art performance (outperforming even the supervised baseline) on 4 out of 5 datasets, while outperforming PAWS across all datasets. SEMPPL also outperforms SimMatch on 4 out of 5 datasets. For more details on the evaluation protocols and results for ImageNet-C see the Appendix A.2.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Top-1 accuracy (in %) for ImageNetV2, ImageNet-R and ObjectNet.\n\nRobustness\n\nOOD generalization\n\nMethod\n\nSupervised (100% labels) [Lim et al., 2019]\n\nSemi-supervised (10% labels) PAWS [Assran et al., 2021] SimMatch [Zheng et al., 2022] SemPPL (ours)\n\nMF\n\n65.1\n\n64.5 63.8 65.4\n\nTi\n\nImageNet-R ObjectNet\n\nT-0.7\n\n73.9\n\n78.4\n\n24.0\n\n73.7 73.2 74.1\n\n78.9 78.3 79.6\n\n23.5 25.0 24.4\n\n26.6\n\n23.8 24.5 25.3\n\nTable 3: Top-1 accuracy (in %) on the full suite of transfer tasks.\n\nMethod\n\nFood101 CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft DTD Pets Caltech101 Flowers\n\nSupervised-IN [Chen et al., 2020a]\n\n72.3\n\n93.6\n\nSemi-supervised (10% labels) PAWS [Assran et al., 2021] SimMatch [Zheng et al., 2022] SEMPPL (ours)\n\n79.1 71.7 80.2\n\n92.3 93.6 92.5\n\n78.3\n\n76.3 78.4 77.6\n\n53.7\n\n61.9\n\n66.7\n\n61.0\n\n74.9 91.5\n\n94.5\n\n94.7\n\n62.0 –\n64.2\n\n66.1 –\n66.3\n\n75.7 69.7 75.5\n\n61.4 –\n63.9\n\n77.0 92.2 75.1 92.8 77.8 92.5\n\n91.9 –\n93.0\n\n96.5 93.2 96.3\n\n3.3 TRANSFER LEARNING\n\nWe evaluate the generality of SEMPPL representations by testing whether the features learned on ImageNet are useful across different datasets and tasks. Specifically, we evaluate the transfer performance of SEMPPL on a set of 11 image classification datasets commonly used in the contrastive literature under the linear protocol [Grill et al., 2020; Chen et al., 2020a; Dwibedi et al., 2021; Mitrovic et al., 2021; Tomasev et al., 2022]. For the linear protocol, the pretrained encoder is frozen and a randomly initialized linear classifier is trained on top using the training data from the target dataset. We report standard metrics for each dataset as well as performance on a held-out test set. For more details on the evaluation protocol see the Appendix A.3. Table 3 compares the transfer performance of representations pretrained using the supervised baseline [Chen et al., 2020a], PAWS [Assran et al., 2021], SimMatch [Zheng et al., 2022] and our method SEMPPL. SEMPPL outperforms the supervised baseline on 8 out of 11 datasets, PAWS on 9 out of 11 datasets, while showing competitive performance to SimMatch, outperforming it on 4 out of 7 datasets.\n\n3.4 FULL LABELLED DATASET\n\nParams\n\nMethod\n\nSupervised (ResNet-50)\n\nFigure 2: Top-1 accuracy for ResNet50 with 100% of the labels across augmentations, initializations and networks.\n\nWe also assess how SEMPPL behaves in a fully supervised setting. For this purpose, we select semantic positives based on the ground-truth labels and fine-tune the learned representations with the full ImageNet dataset. We compare against strong supervised baselines on ResNets as well as against recent performant network architectures that are extensions of the ResNet, e.g. [Liu et al., 2021b; 2022]. Our method reaches 79.7% top-1 accuracy on a ResNet 50 outperforming a number of strong supervised baselines. When we add selective kernels to a ResNet 50, we achieve 82% top-1 accuracy outperforming recent transformers architecture [Liu et al., 2021b], and matching highly tuned ConvNext [Liu et al., 2022]. Therefore, SEMPPL may also be considered as a promising pretraining method in the supervised learning setting.\n\nSEMPPL (SimCLR base) SEMPPL (BYOL base) SEMPPL (ReLICv2 base; ours)\n\nSwin-T [Liu et al., 2021b] ConvNeXt [Liu et al., 2022] SEMPPL + SK (ours)\n\n+ AutoAugment [Cubuk et al., 2019] + MaxUp [Gong et al., 2021]\n\nRepresentation Learning (ResNet-50)\n\nOther Architectures\n\n27M 27M 27M\n\n29M 29M 29M\n\n76.0 77.7 79.7\n\n81.3 82.1 82.0\n\n27M 27M\n\n77.6 78.9\n\nTop-1\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n4 ANALYSIS\n\nWe analyse the impact of different design choices in SEMPPL on downstream performance. In this section, we focus the behaviour and impact of pseudo-labels and semantic positives on learning representations. For further analyses and experimental details, please see Appendix D.\n\nSemantic positives across self-supervised learning objectives With SEMPPL we extend the set of positives to include semantic positives based on predicted pseudo-labels; we can combine these ideas with other self-supervised methods. In Table 4a, we additionally evaluate SEMPPL on the noncontrastive self-supervised method BYOL [Grill et al., 2020]. BYOL replaces the contrastive loss in Equation 4 with an l2 loss. Importantly, we follow the training pipeline (e.g. augmentation, hyperparameters etc.) from [Grill et al., 2020] to fairly highlight the impact of SEMPPL. We observe a drastic improvement when adding semantic positives. With 1% labels on ImageNet BYOL improves by absolute +3.9% and by absolute +3.6% when using 10% labels. For completeness we have also highlighted the contribution of SEMPPL when using RELICv2 as the base self-supervised objective which is our default implementation. For 1% labeled data, we see an absolute improvement of +10.4% in top-1 accuracy, while for 10% labels we see a gain of absolute +3.6% in top-1 accuracy. In summary, we see that SEMPPL can be easily combined with other self-supervised objectives to yield significant improvements and can be used a plug-and-play module in semi-supervised learning.\n\nThe contribution of pseudo-labels and semantic positives We examine the impact of omitting pseudo-label prediction and semantic positives from learning representations. Specifically, we ablate the use of pseudo-labels when selecting semantic positives for unlabelled datapoints, i.e. we only use labelled images when retrieving semantic positives. In Table 4b (middle row), removing pseudo-label prediction significantly decreases performance both in the 1% and 10% label settings. In addition, the low-label regime (1% labels) suffers a stronger performance decrease −6.6% than the 10% labels regime, −4.9%. This underscores the importance of pseudo-label estimation and subsequent selection of semantic positives for unlabelled data especially in the low-data regime. Going a step further, we remove semantic positives even for the labelled data, falling back to vanilla RELICv2. In Table 4b (bottom row), we see again a significant drop in performance for both the 1% and 10% label settings with a sharper drop for the low-label regime. Together these highlights the importance of both including semantic positives for labelled data as well as using pseudo-label prediction for selecting semantic positives for unlabelled data in order to learn informative representations in a semi-supervised setting in a label-efficient way.\n\nPrecision and Recall of pseudo-labels. In Figure 3, we analyse the behaviour of pseudo-labels by looking at the precision and recall as training progresses. We train a ResNet-50 for 100 epochs using 10% labels with SEMPPL on ImageNet. As we have 4 large views there will be in total 16 votes cast and then the pseudo-label will be estimated using majority voting. We want to measure how often these 16 votes agree or disagree; we denote as voting threshold the number k where at least k votes have been cast for one class. We see that as training progresses the precision across all thresholds increases as expected. This means that the pseudo-label prediction is bootstrapping itself to become more accurate, which enables us to select better semantic positives and thus learn more informative representations as training progresses, i.e. we have a virtuous cycle of representation learning and pseudo-label prediction. Furthermore, precision is an increasing function of the voting\n\nTable 4: Top-1 test accuracy (in %) with a ResNet50 pretrained on ImageNet with 1% and 10% labels.\n\nBYOL [Grill et al., 2020] SEMPPL with BYOL\n\nReLICv2 [Tomasev et al., 2022] SEMPPL with RELICv2 (ours)\n\nTop-1\n\n1%\n\n53.2 57.1\n\n58.1 68.5\n\n10%\n\n68.8 72.4\n\n72.4 76.0\n\n(a) Trained on a different self-supervised objective.\n\n1% labels\n\n10% labels\n\nSEMPPL\n\n- Pseudo-labels - Semantic Positives\n\n68.5 61.9 58.1\n\n76.0 71.1 72.4\n\n(b) Removing pseudo-labelling and semantic positives in SEMPPL.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: Top-1 test accuracy (in %) with a ResNet50 pretrained on ImageNet 10% labels for 100 epoches when using ground truth labels instead of pseudo-labels while retrieving semantic positives (oracle); PL accuracy holds for pseudo-label accuracy.\n\n10% labels\n\nTop-1\n\nPL accuracy\n\nSEMPPL SEMPPL (+oracle)\n\n69.9 71.6\n\n69.7 76.9\n\nthreshold throughout training and is highest for the biggest voting threshold. This indicates how confident we can be in the accuracy of pseudo-label prediction, and thus how confident we can be that an appropriate semantic positive has been selected. Yet, we see that the recall for individual thresholds is also increasing as training progresses but that the recall decreases as we increase the voting threshold. This is expected as there is always a trade-off between precision and recall.\n\nFigure 3: Precision and recall for pseudo-labels computed based on k-nearest neighbours when trained on ImageNet with 10% labels over 100 epoches.\n\nNoise in pseudo-label prediction In Figure 3, we observe that the proportion of correctly predicted pseudo-labels at the end of training is reasonably high (60% accuracy of voting threshold 0). Yet, it also means that 40% of pseudo labels are still incorrectly predicted. As the incorrect prediction results in suboptimal semantic positives selection, i.e., SEMPPL does not select semantic positives from the same class as the datapoint, this behavior may ultimately worsen the quality of extracted features. To quantify this phenomenon, we train the representation with SEMPPL where an oracle replaces the pseudo-label prediction with ground-truth labels and those are used for selecting semantic positives. In Table 5, we train the representations for 100 epochs on ImageNet with 10% labels. There, the oracle increases the top-1 performance of 1.7% in the test set with 10%. Besides, the pseudo-label accuracy also gets 6.2% higher. It thus confirms that incorrect pseudo-label predictions, and incorrect semantic positives retrieval, hurts learning informative representations and the downstream performance. Yet, the oracle performance remains close to the actual performance of SEMPPL, illustrating the method’s robustness.\n\nFurther ablations on other design choices, such as the number of semantic positives, the use of view voting, the choice of k in k-NN, queue length and training duration can be found in Appendix D.2.\n\n5 RELATED WORK\n\nSemi-supervised learning In the semi-supervised regime [Cheplygina et al., 2019; Van Engelen & Hoos, 2020; Yang et al., 2021; Alizadehsani et al., 2021], one can either pre-train a model on unlabelled data and subsequently fine-tune it on labelled data, or train both jointly. Joint training on labelled and unlabelled data often involves combining the two losses [Grandvalet & Bengio, 2004; Miyato et al., 2018; Zhai et al., 2019; Verma et al., 2019; Berman et al., 2019; Xie et al., 2020a]. Pseudo-label self-training approaches [Zoph et al., 2020] present an important alternative, first inferring approximate pseudo-labels for the unlabelled examples, and then incorporating them in supervised losses. Pseudo-labels can either be generated prior to a subsequent supervised learning\n\n8\n\n50001000015000200002500030000Steps0.00.20.40.60.81.0Precision50001000015000200002500030000Steps0.00.20.40.60.81.0RecallVoting threshold0257101215Published as a conference paper at ICLR 2023\n\nphase [Yarowsky, 1995; Riloff, 1996; Lee et al., 2013] or jointly in an online fashion [Berthelot et al., 2019; 2020; Sohn et al., 2020]. These methods may benefit from pseudo-label confidence measures [Sohn et al., 2020; Rizve et al., 2021; Zhang et al., 2021] as well as thresholding [Xu et al., 2021], temporal ensembling [Laine & Aila, 2017], or stronger regularization to mitigate bias in early model training [Sajjadi et al., 2016; Arazo et al., 2020]. The use of pseudo-labels with rebalancing has shown improvements, both in class-imbalanced problems [Wei et al., 2021] and in a general context [Wang et al., 2022]. Teacher-student network configurations for generating and utilising pseudo-labels have also shown promise [Tarvainen & Valpola, 2017; Luo et al., 2018; Ke et al., 2019; Xie et al., 2020b; Cai et al., 2021; Pham et al., 2021]. Co-training uses different feature extractors for different data views and alternates between pseudo-labelling and training phases [Blum & Mitchell, 1998; Qiao et al., 2018]. Good performance has been reached by using consistency losses between pseudo-labels of different inputs [Verma et al., 2019; Hu et al., 2021].\n\nPredicting view assignments with support samples [Assran et al., 2021] (PAWS) has resulted in substantial performance improvements, with the idea that the assigned pseudo-labels ought to be similar across multiple views of the same image. Recent work has shown that incorporating label information in positive selection in contrastive methods is highly promising, compared to the crossentropy loss in the fully supervised case [Khosla et al., 2020]. Our method demonstrates a similar utility of pseudo-labels for semi-supervised problems, and differs from competing ones in the following ways. Unlike DebiasPL [Wang et al., 2022] that uses an adaptive margin loss, SemPPL does not seek to directly address or re-shape the distribution of pseudo-labels. Unlike SimCLRv2 [Chen et al., 2020b], we do not rely on self-distillation procedures. In contrast with PAWS [Assran et al., 2021], we fully leverage the contrastive approach for semi-supervised learning; not using positives only for training means SEMPPL does not require specific care like pseudo-labels sharpening to stabilize learning and avoid representational collapse. SEMPPL is more closely related to CoMatch [Li et al., 2021a] that also uses bootstrapping to improve pseudo-labels representational quality, but is conceptually much simpler, avoiding phases of distributional alignment and of performing graphbased contrastive learning. In a similar vein, SimMatch [Zheng et al., 2022] also uses a memory buffer to propagate pseudo-labels, but has a more complex objective than SEMPPL and equally requires additional phases of pseudo-labels unfolding and aggregation to function.\n\nSelf-supervised learning Major advances in learning useful representations from unlabelled data [Liu et al., 2021a; Goyal et al., 2021] can be seen as a paradigm shift, since these methods have recently been competitive with supervised training baselines [Tomasev et al., 2022]. A number of self-supervised learning methods involve contrasting multiple views of the data [Oord et al., 2018; Bachman et al., 2019; Chen et al., 2020a; He et al., 2020; Grill et al., 2020; Dwibedi et al., 2021]. Similar performance were also achieved by bootstrapping-based multi-view learning [Grill et al., 2020; Richemond et al., 2020; Chen & He, 2021; Zbontar et al., 2021; Wang et al., 2021], or involving explicit clustering steps [Caron et al., 2020; Asano et al., 2020; Li et al., 2021b]. An explicit causally-motivated invariance loss, when used in conjunction with the contrastive objective, has been shown to lead to more compact representations, and desirable generalisation properties [Mitrovic et al., 2021; Tomasev et al., 2022]. Contrastive approaches are not always used in self-supervised methods [He et al., 2021; Ermolov et al., 2021; Chen et al., 2022]. Transformer-specific methods have been devised [Caron et al., 2021; Chen et al., 2021; Zhai et al., 2022].\n\n6 CONCLUSION\n\nIn this work, we propose SEMPPL, a novel semi-supervised learning method to incorporate semantic positives in self-supervised objectives by taking advantage of pseudo-labels. Through extensive empirical evaluation, we demonstrated that our approach achieves state-of-the-art semi-supervised performance on ImageNet across several ResNet architectures as well as on the robustness, out-ofdistribution generalization and transfer tasks. We also show that SEMPPL can be easily combined with other existing self-supervised methods and is a promising direction to pre-train networks also in a fully supervised learning regime. Our analyses suggest that the role of pseudo-labels in selecting positives for semi-supervised contrastive methods might be underappreciated. Despite widespread use in semi-supervised applications, pseudo-labels are less understood and have been explored far less in the context of self-supervised methods. We hope this study, which shows empirically that prior work has under-utilized pseudo-labels, may help bridge that gap.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nWe documented the model and experimental evaluation in the main body of the paper and added further details in the appendix. Concretely, we explain implementation details and sweep parameters in Appendix A and Appendix D.1, the invariance loss in Appendix B and give details on data augmentations in Appendix E. The model pseudo-code is in Appendix C. The datasets used in the experiments are freely available from their respective sources. We also open source SemPPL pretrained checkpoints from our experiments, namely ResNet-50 1×, 2× and 4× as well as ResNet-200 2× together with the evaluation code at https://github.com/deepmind/semppl.\n\nREFERENCES\n\nRoohallah Alizadehsani, Danial Sharifrazi, Navid Hoseini Izadi, Javad Hassannataj Joloudari, Afshin Shoeibi, Juan M Gorriz, Sadiq Hussain, Juan E Arco, Zahra Alizadeh Sani, Fahime Khozeimeh, et al. Uncertaintyaware semi-supervised method using large unlabeled and limited labeled covid-19 data. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 17(3s):1–24, 2021.\n\nEric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In Proc. of the International Joint Conference on Neural Networks (IJCNN), 2020.\n\nYuki M. Asano, C. Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representa-\n\ntion learning. In Proc. of International Conference on Learning Representations (ICLR), 2020.\n\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, Nicolas Ballas, and Michael Rabbat. Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\nAndrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\nIrwan Bello, William Fedus, Xianzhi Du, Ekin Dogus Cubuk, A. Srinivas, Tsung-Yi Lin, Jonathon Shlens, and Barret Zoph. Revisiting resnets: Improved training and scaling strategies. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2021.\n\nThomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander, David W Jacobs, and Peter N Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2014.\n\nMaxim Berman, Herv ́e J ́egou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain: a unified\n\nimage embedding for classes and instances. arxiv preprint arXiv:1902.05509, 2019.\n\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\nDavid Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In Proc. of International Conference on Learning Representations (ICLR), 2020.\n\nAvrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proc. of the Annual\n\nConference on Computational Learning Theory, 1998.\n\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative components with\n\nrandom forests. In Proc. of the European conference on computer vision (ECCV), 2014.\n\nZhaowei Cai, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes, Zhuowen Tu, and Stefano Soatto. Exponential moving average normalization for self-supervised and semi-supervised learning. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive In Proc. of International Conference on Machine Learning (ICML),\n\nlearning of visual representations. 2020a.\n\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised models are strong semi-supervised learners. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2020b.\n\nXiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation learning. arxiv preprint arXiv:2202.03026, 2022.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proc. of Conference on\n\nComputer Vision and Pattern Recognition (CVPR), 2021.\n\nXinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive\n\nlearning. arxiv preprint arXiv:2003.04297, 2020c.\n\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers.\n\nIn Proc. of the International Conference on Computer Vision (ICCV), 2021.\n\nVeronika Cheplygina, Marleen de Bruijne, and Josien PW Pluim. Not-so-supervised: a survey of semisupervised, multi-instance, and transfer learning in medical image analysis. Medical image analysis, 54: 280–296, 2019.\n\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing tex-\n\ntures in the wild. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2014.\n\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning In Proc. of Conference on Computer Vision and Pattern Recognition\n\naugmentation policies from data. (CVPR), 2019.\n\nEkin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proc. of Conference on Computer Vision and Pattern Recognition Workshop (CVPRW), 2020.\n\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In Proc. of the International Conference on Computer Vision (ICCV), 2021.\n\nAleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for self-supervised\n\nrepresentation learning. In Proc. of International Conference on Machine Learning (ICML), 2021.\n\nLi Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In Proc. of Conference on Computer Vision and Pattern Recognition Workshop (CVPRW), 2004.\n\nWeifeng Ge, Weilin Huang, Dengke Dong, and Matthew R. Scott. Deep metric learning with hierarchical triplet\n\nloss. In Proc. of European Conference on Computer Vision (ECCV), 2018.\n\nChengyue Gong, Tongzheng Ren, Mao Ye, and Qiang Liu. Maxup: Lightweight adversarial training with data augmentation improves neural network training. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nPriya Goyal, Piotr Doll ́ar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arxiv preprint arXiv:1706.02677, 2017.\n\nPriya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, and Piotr Bojanowski. Self-supervised pretraining of visual features in the wild. arxiv preprint arXiv:2103.01988, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Proc. of Advances\n\nin Neural Information Processing Systems (NeurIPS), 2004.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nBo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Wai-Hung Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nBen Harwood, B. V. Kumar, G. Carneiro, Ian D. Reid, and Tom Drummond. Smart mining for deep metric\n\nlearning. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\n\nProc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked autoencoders are\n\nscalable vision learners. arxiv preprint arXiv:2111.06377, 2021.\n\nTong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classification with convolutional neural networks. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and\n\nperturbations. In Proc. of International Conference on Learning Representations (ICLR), 2019.\n\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-ofdistribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340–8349, 2021.\n\nZijian Hu, Zhengyu Yang, Xuefeng Hu, and Ram Nevatia. Simple: Similar pseudo label exploitation for semiIn Proc. of Conference on Computer Vision and Pattern Recognition (CVPR),\n\nsupervised classification. 2021.\n\nS. Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal\n\ncovariate shift. In Proc. of International Conference on Machine Learning (ICML), 2015.\n\nJeff Johnson, Matthijs Douze, and Herv ́e J ́egou. Billion-scale similarity search with gpus. IEEE Transactions\n\non Big Data, 7(3):535–547, 2019.\n\nZhanghan Ke, Daoye Wang, Qiong Yan, Jimmy Ren, and Rynson W.H. Lau. Dual student: Breaking the limits of the teacher in semi-supervised learning. In Proc. of the International Conference on Computer Vision (ICCV), 2019.\n\nMahmoud Khairy.\n\nTpu vs gpu vs cerebras vs graphcore: A fair comparison between ml hardhttps://khairy2011.medium.com/tpu-vs-gpu-vs-cerebras-vs-graphcore-a-fair-comparison-between-\n\nware. ml-hardware-3f5a19d89e38, 2020. Accessed: 2022-11-16.\n\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Proc. of Advances in Neural Information Processing Systems (NeurIPS), 33:18661–18673, 2020.\n\nYoungdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy labels. In Proc.\n\nof the International Conference on Computer Vision (ICCV), 2019.\n\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained catego-\n\nrization. In Proc of the International Conference on Computer Vision Workshops, 2013.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical\n\nreport, Toronto, 2009.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In Proc. of International\n\nConference on Learning Representations (ICLR), 2017.\n\nDong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural\n\nnetworks. In Proc. of the ICML Workshop on Challenges in Representation Learning, 2013.\n\nKuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive visual repre-\n\nsentations. Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2021.\n\nJunnan Li, Yongkang Wong, Qi Zhao, and Mohan S Kankanhalli. Learning to learn from noisy labeled data. In\n\nProc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2019a.\n\nJunnan Li, Caiming Xiong, and Steven C. H. Hoi. Comatch: Semi-supervised learning with contrastive graph\n\nregularization. In Proc. of the International Conference on Computer Vision (ICCV), 2021a.\n\nJunnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven C. H. Hoi. Prototypical contrastive learning of unsupervised representations. In Proc. of International Conference on Learning Representations (ICLR), 2021b.\n\nXiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In Proc. of Conference on\n\nComputer Vision and Pattern Recognition (CVPR), 2019b.\n\nSungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Proc. of\n\nAdvances in Neural Information Processing Systems (NeurIPS), 32, 2019.\n\nSheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nXiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-supervised\n\nlearning: Generative or contrastive. IEEE Transactions on Knowledge and Data Engineering, 2021a.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proc. of the International Conference on Computer Vision (ICCV), 2021b.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet\n\nfor the 2020s. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n\nYucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for semisupervised learning. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual clas-\n\nsification of aircraft. arxiv preprint arXiv:1306.5151, 2013.\n\nJovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, and Charles Blundell. Representation learnIn Proc. of International Conference on Learning Representations\n\ning via invariant causal mechanisms. (ICLR), 2021.\n\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. Proc. of Transactions on Pattern Analysis and Machine Intelligence, 2018.\n\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes.\n\nIn Proc. of Indian Conference on Computer Vision, Graphics & Image Processing, 2008.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding.\n\narxiv preprint arXiv:1807.03748, 2018.\n\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In Proc. of Conference\n\non Computer Vision and Pattern Recognition (CVPR), 2012.\n\nHieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le. Meta pseudo labels. In Proc. of Conference on Computer\n\nVision and Pattern Recognition (CVPR), 2021.\n\nSiyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, and Alan Loddon Yuille. Deep co-training for semi-\n\nsupervised image recognition. In Proc. of European Conference on Computer Vision (ECCV), 2018.\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize\n\nto imagenet? In Proc. of International Conference on Machine Learning (ICML), 2019.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nPierre H Richemond, Jean-Bastien Grill, Florent Altch ́e, Corentin Tallec, Florian Strub, Andrew Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, et al. Byol works even without batch statistics. In Proc. of Advances in Neural Information Processing Systems Workshops, 2020.\n\nEllen Riloff. Automatically generating extraction patterns from untagged text. In Proc. of the National Con-\n\nference on Artificial Intelligence, 1996.\n\nMamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah.\n\nlabeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. preprint arXiv:2101.06329, 2021.\n\nIn defense of pseudoarxiv\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.\n\nMehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2016.\n\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition\n\nand clustering. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets In Proc. of Advances in Neural Information Processing\n\nimprove semi-supervised deep learning results. Systems (NeurIPS), 2017.\n\nNenad Tomasev, Ioana Bica, Brian McWilliams, Lars Buesing, Razvan Pascanu, Charles Blundell, and Jovana Mitrovic. Pushing the limits of self-supervised resnets: Can we outperform supervised learning without labels on imagenet? arxiv preprint arXiv:2201.05119, 2022.\n\nJesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning. Machine Learning, 109(2):\n\n373–440, 2020.\n\nVikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz.\n\nInterpolation consistency training for semi-supervised learning. In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI), 2019.\n\nXiang Wang, Xinlei Chen, Simon Shaolei Du, and Yuandong Tian. Towards demystifying representation\n\nlearning with non-contrastive self-supervision. arxiv preprint arXiv:2110.04947, 2021.\n\nXudong Wang, Zhirong Wu, Long Lian, and Stella X Yu. Debiased learning from naturally imbalanced pseudo-\n\nlabels. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n\nXun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R. Scott. Multi-similarity loss with general pair weighting for deep metric learning. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2019a.\n\nYisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for robust learning with noisy labels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 322–330, 2019b.\n\nChen Wei, Kihyuk Sohn, Clayton Mellina, Alan Yuille, and Fan Yang. Crest: A class-rebalancing self-training framework for imbalanced semi-supervised learning. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2010.\n\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2020a.\n\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR), 2020b.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nYi Xu, Lei Shang, Jinxing Ye, Qi Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong Jin. Dash: Semi-supervised learning with dynamic thresholding. In Proc. of International Conference on Machine Learning (ICML)g (ICML), 2021.\n\nXiangli Yang, Zixing Song, Irwin King, and Zenglin Xu. A survey on deep semi-supervised learning. arxiv\n\npreprint arXiv:2103.00550, 2021.\n\nDavid Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of Annual\n\nMeeting of the Association for Computational Linguistics (ACL), 1995.\n\nYang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arxiv preprint\n\narXiv:1708.03888, 2017.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St ́ephane Deny. Barlow twins: Self-supervised learning\n\nvia redundancy reduction. In Proc. of International Conference on Machine Learning (ICML), 2021.\n\nShuangfei Zhai, Navdeep Jaitly, Jason Ramapuram, Dan Busbridge, Tatiana Likhomanenko, Joseph Yitan Cheng, Walter Talbott, Chen Huang, Hanlin Goh, and Joshua Susskind. Position prediction as an effective pretraining strategy. In Proc. of International Conference on Machine Learning (ICML), 2022.\n\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semi-supervised\n\nlearning. In Proc. of the International Conference on Computer Vision (ICCV), 2019.\n\nBowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2021.\n\nZhilu Zhang and Mert Rory Sabuncu. Generalized cross entropy loss for training deep neural networks with\n\nnoisy labels. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nMingkai Zheng, Shan You, Lang Huang, Fei Wang, Chen Qian, and Chang Xu. Simmatch: Semi-supervised In Proc. of Conference on Computer Vision and Pattern Recognition\n\nlearning with similarity matching. (CVPR), 2022.\n\nBarret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-training and self-training. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nA ADDITIONAL RESULTS AND IMPLEMENTATION DETAILS\n\nA.1 SEMI-SUPERVISED DETAILS AND RESULTS\n\nImplementation details. In this work, we follow the protocol of Chen et al. [2020b]; Assran et al. [2021] for fine-tuning from the first layer of the projector and initialize both the encoder and the first layer of the projector with the parameters of the pretrained model. We add the randomly initialized classifier on top of the first layer of the projector (after the non-linearity). We train all the weights (pretrained and classifier weights) using either 1% or 10% of the ImageNet-1k training data, and we use the splits introduced in Chen et al. [2020a] and used in all the methods to compare to Grill et al. [2020]; Caron et al. [2020]; Dwibedi et al. [2021]; Lee et al. [2021]; Mitrovic et al. [2021]; Tomasev et al. [2022]; Assran et al. [2021].\n\nAt training time we randomly crop the image, resize it to 224 × 224, and then randomly apply a horizontal flip. At test time we resize images to 256 pixels along the shorter side with bicubic resampling and apply a 224 × 224 center crop to it. Both at training and testing times we subtract from the color channels the average channel value and divide it by the standard deviation of the channel value (as computed on ImageNet-1k).\n\nWe use a cross entropy loss and stochastic gradient descent with Nesterov momentum of 0.9 to fine-tune the model. For both 1% and 10% settings, we train for 30 epochs and decay the initial learning rate by a factor 0.2 at 18 and 24 epochs. Following the approach of Caron et al. [2020], we pick different learning rates for the encoder (and the first projector layer) and for the classifier weights. We do not use any weight decay or other regularization techniques. We sweep over batch sizes values in {512, 1024, 2048}, encoder base learning rate values in {0.005, 0.0035, 0.003, 0.0025, 0.002, 0.001}, and linear layer base learning rate values in {0.5, 0.3, 0.2, 0.1, 0.05, 0.025}.\n\nTable 6: Top-1 and Top-5 accuracies (in %), after semi-supervised fine-tuning with a fraction of ImageNet labels, for a ResNet-50 encoder across a number of representation learning methods.\n\nMethod\n\nSupervised [Zhai et al., 2019]\n\nPseudo labels in classification: MPL [Pham et al., 2021]\n\nRepresentation learning methods: SimCLRv2 [Chen et al., 2020b] SimCLRv2 + self distillation [Chen et al., 2020b] CoMatch [Li et al., 2021a] PAWS [Assran et al., 2021] DebiasPL [Wang et al., 2022] SimMatch [Zheng et al., 2022] SEMPPL (ours) SimCLRv2 + Selective Kernels [Li et al., 2019b] SEMPPL (ours) + Selective Kernels\n\nTop-1\n\nTop-5\n\n1% 10% 1% 10%\n\n25.4\n\n56.4\n\n48.4\n\n80.4\n\n-\n\n73.9\n\n-\n\n-\n\n57.9 60.0 66.0 66.5 67.1 67.2 68.5 64.5 72.3\n\n68.4 70.5 73.7 75.5 -\n74.4 76.0 72.1 78.2\n\n- -\n86.4 -\n85.8 87.1 88.2 86.7 90.6\n\n- -\n91.6 -\n- 91.6 92.7 91.4 93.9\n\nAdditional results and larger networks. When the architecture of the ResNet-50 is modified to include selective kernels [Li et al., 2019b], we see significant gains in performance at the expense of additional weights. Our implementation of selective kernels is standard and follows rigorously Li et al. [2019b] for a total of 27.5 million weights instead of of 25.5 million for a regular ResNet-50. Specifically, we use 2 channels, two convolution kernels of (3, 3) and (5, 5) with the latter implemented as a (3, 3) dilated convolution with rate 2, and 32 grouped convolutions. Unlike SimCLRv2 [Chen et al., 2020b], we implement our group convolutions explicitly, and do not use the additional ResNet-D architectural modification from He et al. [2019]. When using selective kernels our performance after finetuning with 1% of labels is the same as that of SimCLRv2 after finetuning with 10% of labels.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nAdditionally, in order to investigate the robustness and scalability of these results, we further test the generality of SEMPPL by learning representations on larger (both deeper and wider) ResNet encoders. Table 1 testifies to SEMPPL outperforming the competing representation learning methods across all the architectures, both in the 1% and the 10% labelled settings. Also, as our flagship result we reach 80.4% top-1 accuracy on ResNet-200 2× with 10% of ImageNet-1k labels. Just as in the ResNet-50 1× case this figure is comparable with the fully supervised accuracy attained by historical methods. 80.1% top-1 is defined as in Grill et al. [2020] with standard RandAugment [Cubuk et al., 2020] data augmentation. However it’s certainly a few percentage accuracy points away from results obtained with optimal current training protocols [Bello et al., 2021]. We also note that SEMPPL is pre-trained for 300 epochs in all cases. This, rather than the 1000 epochs used as standard by most other representation learning methods, again compares with a typical figure of 200 epochs used in supervised learning. Overall this hints at SEMPPL having achieved close to an order of magnitude gain in label efficiency (compared to supervised learning) at a similar epochs budget.\n\nOur final networks were optimized using tranches of between 128 (for a ResNet-50) and 512 (for the largest ResNets) Cloud TPUv3s all during 300 epochs each irrespective of size. This required around a day of computation time per run and tranche for a ResNet-50 on 128 devices, time which scaled approximately linearly with the number of parameters on larger networks, depending on the actual network.\n\nA.2 ROBUSTNESS AND OOD GENERALIZATION\n\nWe test the robustness and out-of-distribution (OOD) generalization abilities of representations learned via SEMPPL on several detasets. We use ImageNetV2 [Recht et al., 2019] and ImageNetC [Hendrycks & Dietterich, 2019] datasets to evaluate robustness and the datasets ObjectNet [Barbu et al., 2019] and ImageNet-R [Hendrycks et al., 2021] to evaluate the OOD generalization.\n\nThe ImageNetV2 dataset [Recht et al., 2019] has three sets of 10000 images (matched frequency (MF), Threshold 0.7 (T-0.7) and Top Images (TI)) that were collected to have a similar distribution to the ImageNet test set. The ImageNet-C dataset [Hendrycks & Dietterich, 2019] consists of 15 synthetically generated corruptions of 5 different severities (e.g. blur, noise) that are applied to the ImageNet validation set. The ImageNet-R dataset [Hendrycks et al., 2021] consists of 30000 different renditions (e.g. paintings, cartoons) of 200 ImageNet classes; the aim of this dataset is to test the generalization ability to different textures and other naturally occurring style changes that are out-of-distribution to the ImageNet training data. The ObjectNet dataset [Barbu et al., 2019] has 18574 images from differing viewpoints and backgrounds compared to the ImageNet training set.\n\nOn all datasets we evaluate the representations learned on a standard ResNet50 encoder under a linear evaluation protocol. We freeze the pretrained representations (no gradient updates) and train a linear classifier on top of the output of the ResNet-50 encoder using the full labelled ImageNet training set. We perform the test evaluation zero-shot, i.e the above datasets are not seen during the training of the representation or classifier.\n\nWe provide a detailed breakdown across the different ImageNet-C corruptions in Table 7. Our proposed approach SEMPPL outperforms both the supervised baseline, on 12 out of 15 corruptions, as well as the competing semi-supervised representation learning model PAWS, on 12 out of 15 corruptions (notably, over all Blur, Weather and Digital corruptions).\n\nTable 7: Top-1 accuracies (in %) for OOD generalisation on Gauss, Shot, Impulse, Blur, Weather, and Digital corruption types of ImageNet-C.\n\nBlur\n\nWeather\n\nDigital\n\nMethod\n\nSupervised [Lim et al., 2019] Semi-supervised representations:\n\nPAWS [Assran et al., 2021] SEMPPL(ours)\n\nGauss 37.1\n\n43.5 41.3\n\nShot 35.1\n\n40.6 39.1\n\nA.3 TRANSFER\n\nImpulse Defocus Glass Motion Zoom Snow Frost 40.7\n\n38.1\n\n34.5\n\n36.8\n\n25.9\n\n34.9\n\n30.8\n\nFog Bright Contrast Elastic 56.9\n\n45.6\n\n40.6\n\n68.1\n\nPixel 32.6\n\nJPEG 56.0\n\n33.5 30.0\n\n38.7 41.9\n\n19.7 23.2\n\n34.1 37.5\n\n32.8 34.0\n\n40.3 40.5\n\n44.7 45.5\n\n64.0 64.4\n\n70.5 71.9\n\n59.7 60.6\n\n42.4 44.2\n\n38.5 45.1\n\n55.1 57.7\n\nTo further evaluate the usefulness of the learned representations, we evaluate how well they transfer across datasets. For this, we follow the standard evaluation protocol outlined in Grill et al. [2020];\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nChen et al. [2020a]. We evaluate SEMPPL across the linear evaluation protocol which consists of freezing the encoder and only training a randomly initialized linear classifier on top of the encoder. In line with prior work [Chen et al., 2020a; Grill et al., 2020; Dwibedi et al., 2021], we test SEMPPL representations on the following datasets: Food101 [Bossard et al., 2014], CIFAR10 [Krizhevsky et al., 2009], CIFAR100 [Krizhevsky et al., 2009], Birdsnap [Berg et al., 2014], SUN397 (split 1) [Xiao et al., 2010], DTD (split 1) [Cimpoi et al., 2014], Cars [Krause et al., 2013] Aircraft [Maji et al., 2013], Pets [Parkhi et al., 2012], Caltech101 [Fei-Fei et al., 2004], and Flowers [Nilsback & Zisserman, 2008], where we compare the downstream performance of SEMPPL to that of other reported semi-supervised methods on 10% of labels. Across these datasets there are differences in terms of metrics used for selecting the best hyper-parameters as well the reporting of the final results. In line with prior work [Chen et al., 2020a; Grill et al., 2020; Dwibedi et al., 2021], for Food101 [Bossard et al., 2014], CIFAR10 [Krizhevsky et al., 2009], CIFAR100 [Krizhevsky et al., 2009], Birdsnap [Berg et al., 2014], SUN397 (split 1) [Xiao et al., 2010], DTD (split 1) [Cimpoi et al., 2014], and Cars [Krause et al., 2013] we report the Top-1 accuracy on the test set, and for Aircraft [Maji et al., 2013], Pets [Parkhi et al., 2012], Caltech101 [Fei-Fei et al., 2004], and Flowers [Nilsback & Zisserman, 2008] we report the mean per-class accuracy. For DTD and SUN397 we only use the first split of the 10 provided splits in the dataset as per Chen et al. [2020a]; Grill et al. [2020]; Dwibedi et al. [2021].\n\nIn these experiments, models are initially trained on the training sets of the individual datasets, and the validation sets are used to select the best hyperparameters from the executed hyperparameter sweeps. Once the best hyperparameters have been selected, the final models are trained on a merged dataset containing both the training and the validation split and evaluated on the held-out test split. The final results of the transfer experiments are reported in Table 3. The performed hyperparameter sweeps involved sweeping over the learning rates {.001, .01, 0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 1., 2.}, batch sizes {128, 256, 512, 1024, 2048}, weight decay {1e−6, 1e−5, 1e−4, 1e−3, 0.01, 0.1}, warmup epochs {0, 10}, momentum {0.9, 0.99}, Nesterov {True, False}, and the number of training epochs. For the linear transfer protocol we considered setting epochs among {20, 30, 60, 80, 100}. Models were trained by stochastic gradient descent with momentum.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nB INVARIANCE REGULARIZATION\n\nWe define the short-hands\n\np(ˆza1\n\nm ; ̃za2,+\n\nm,t ) =\n\nand\n\np(ˆza1\n\nm ; ̃za2\n\nm,t) =\n\nφ(ˆza1 m,t ) + (cid:80)\n\nφ(ˆza1\n\nm , ̃za2,+\n\nm , ̃za2,+ m,t )\n\nxn∈N (xm) φ(ˆza1\n\nm , ̃za2\n\nn,t)\n\nφ(ˆza1\n\nm , ̃za2\n\nm,t) + (cid:80)\n\nφ(ˆza1\n\nm,t)\n\nm , ̃za2 xn∈N (xm) φ(ˆza1\n\nm , ̃za2\n\nn,t)\n\n(5)\n\n(6)\n\nwhere N (xk) is the set of negatives, randomly uniformly sampled from the current batch, ̃za2 n,t = gt(ft(xn)) is the target network projection of the negative sample; φ(x1, x2) = τ · exp(⟨x1, x2⟩/τ ) is the scoring function, τ > 0 is a scalar temperature, and ⟨·, ·⟩ denotes the standard Euclidean dot product.\n\nWe can now rewrite the components of the overall loss\n\nas\n\nand\n\nLSEMPPL = LAUGM + αLSEMPOS\n\nLSEMPOS = −\n\nB (cid:88)\n\nm=1\n\nlog p(ˆza1\n\nm ; ̃za2,+ m,t )\n\nLAUGM = −\n\nB (cid:88)\n\nm=1\n\nlog p(ˆza1\n\nm ; ̃za2\n\nm,t).\n\n(7)\n\n(8)\n\n(9)\n\nAs discussed in the main text we add the invariance penalty introduced in Mitrovic et al. [2021] to further increase the similarity between the anchor and positives and regularize the learning process. We add this invariance penalty both for augmentation positives and semantic positives. In particular, we compute\n\nIaugm =DKL(p(ˆza1 =sg[E\n\np(ˆza1\n\nm ; ̃za2\n\nm,t) ∥ p(ˆza2\n\nm ; ̃za2 m,t) log p(ˆza1\n\nm ; ̃za2\n\nm ; ̃za1\n\nm,t)) m,t)] − E\n\np(ˆza1\n\nm ; ̃za2\n\nm,t) log p(ˆza2\n\nm ; ̃za1\n\nm,t)\n\nand\n\nIsempos =DKL(p(ˆza1 =sg[E\n\nm,t ) ∥ p(ˆza2,+\n\nm ; ̃za2,+ m,t ) log p(ˆza1\n\nm ; ̃za2,+\n\nm,t)) m,t )] − E\n\nm ; ̃za1\n\np(ˆza1\n\nm ; ̃za2,+\n\np(ˆza1\n\nm ; ̃za2,+\n\nm,t ) log p(ˆza2,+\n\nm ; ̃za1\n\nm,t)\n\nwhere sg denotes the stop-gradient operation. Taking all this together, this gives the final form of the loss as\n\nLSEMPPL = c(LAUGM + αLSEMPOS) + λ(Iaugm + Isempos) with λ the invariance scale and c is the contrastive scale. We use λ = 5 and c = 0.3 in all our experiments irrespective of encoder size or training time as our method is robust to the choice of these hyperparameters.\n\n(10)\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nC PSEUDO-CODE OF SEMPPL\n\nListing 1 provides PyTorch-like pseudo-code for SEMPPL detailing how we compute pseudo-labels and use them to select the additional semantic positives, which are then used in the contrastive loss, along the augmentation positives.\n\ni n r a n g e ( n u m l a r g e v i e w s ) :\n\ni q u e u e i = q u e u e . i n i t ( q u e u e s i z e , FIFO )\n\n# Load b a t c h o f B s a m p l e s e a c h w i t h d a t a x and ( maybe )\n\nl a b e l y .\n\nf o : o n l i n e n e t w o r k : E n c o d e r + c o m p a r i s o n n e t . t a r g e t n e t w o r k : E n c o d e r + c o m p a r i s o n n e t .\n\ni n k−NN when c o m p u t i n g p s e u d o l a b e l s .\n\n’ ’ ’\n\nf o r\n\n’ ’ ’ f o r\n\nf o r x , y i n b a t c h :\n\nx m = m a s k b a c k g r o u n d ( x ) f o r\n\n1 2 k : The number o f n e i g h b o r s 3\n4 g t : 5 gamma : T a r g e t EMA c o e f f i c i e n t . 6 n e : Number o f n e g a t i v e s . 7 p m : Mask a p p l y p r o b a b i l i t y . 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\nl o s s = 0 # Compute t h e f o r\n\n# Pseudo − l a b e l i f y i s None :\n\n# Compute t h e f o r\n\ni f o r\n\ni f o r\n\ni n r a n g e ( n u m l a r g e v i e w s ) :\n\ni # S t o c h a s t i c a l l y a p p l y b a c k g r o u n d r e m o v a l . x = B e r n o u l l i ( p m ) ? x m : x # C r e a t e an a u g m e n t e d l a r g e view . i = augment ( c r o p l a r g e ( x ) ) x l i = f o ( x l o l i = g t ( x l t l # Enqueue t h e i f y i s n o t None :\n\ni ) i ) l a b e l e d i m a g e s\n\ni n t h e b a t c h .\n\nq u e u e i . e n q u e u e ( ( t l\n\ni\n\n, y i ) )\n\ni i n r a n g e ( n u m s m a l l v i e w s ) : x s i = augment ( c r o p s m a l l ( x ) ) # S m a l l v i e w s o n l y go t h r o u g h t h e o n l i n e n e t w o r k o s i = f o ( x s i )\n\nc o m p u t a t i o n f o r u n l a b e l l e d e x a m p l e s . # M i s s i n g l a b e l . v o t e s = [ knn ( k , q u e u e i , o l y = mode ( v o t e s )\n\nf o r\n\nj )\n\ni ,\n\nj\n\ni n a l l p a i r s ( n u m l a r g e v i e w s ) ]\n\nl o s s b e t w e e n a l l\n\nt h e p a i r s o f\n\nl a r g e v i e w s .\n\ni n r a n g e ( n u m l a r g e v i e w s ) :\n\ni n r a n g e ( n u m l a r g e v i e w s ) :\n\nj l o s s += c o n t r a s t i v e l o s s ( o l f o r\n\ni n r a n g e ( n u m s e m a n t i c p o s i t i v e s ) :\n\n# Sample s e m a n t i c p o s i t i v e s z = s a m p l e ( q u e u e j . f i l t e r ( y ) ) l o s s += c o n t r a s t i v e l o s s ( o l\n\ni , z , n e )\n\ni ,\n\nt l\n\nj\n\n, n e )\n\n# A u g m e n t a t i o n p o s i t i v e s .\n\nfrom t h e queue , and add t o t h e\n\nl o s s .\n\nl o s s b e t w e e n t h e s m a l l and l a r g e v i e w s .\n\ni n r a n g e ( n u m s m a l l v i e w s ) :\n\nj i n r a n g e ( n u m l a r g e v i e w s ) : l o s s += c o n t r a s t i v e l o s s ( o s i , f o r\n\ni n r a n g e ( n u m s e m a n t i c p o s i t i v e s ) :\n\n# Sample s e m a n t i c p o s i t i v e s z = s a m p l e ( q u e u e j . f i l t e r ( y ) ) l o s s += c o n t r a s t i v e l o s s ( o l\n\ni , z , n e )\n\nt l\n\nj\n\n, n e )\n\n# A u g m e n t a t i o n p o s i t i v e s .\n\nfrom t h e queue , and add t o t h e\n\nl o s s .\n\nl o s s / = ( ( n u m l a r g e v i e w s + n u m s m a l l v i e w s ) * n u m l a r g e v i e w s * ( 1 + n u m s e m a n t i c p o s i t i v e s ) )\n\n# Compute t h e g r a d i e n t s , and u p d a t e t h e o n l i n e and t a r g e t n e t w o r k . l o s s . b a c k w a r d ( ) u p d a t e ( f o ) g t = gamma * g t + ( 1 − gamma ) * f o\n\nListing 1 Pseudo-code for SEMPPL.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nD ANALYSIS\n\nD.1\n\nIMPLEMENTATION DETAILS\n\nWe perform all the ablation experiments using 10% of labelled data and train a standard ResNet50 encoder with SEMPPL for 100 epochs (except in the training duration ablation). We report the top-1 accuracies on the ImageNet test set after fine-tuning from the first layer of the projector. As in Grill et al. [2020] and for the main results in this paper, we use multi-layer perceptrons for the projector and predictor with 2 linear layers—the first one followed by batch normalization [Ioffe & Szegedy, 2015] and rectified linear activation with output sizes 4096 and 256 for the two layers respectively. We use the same augmentations as for the experiments in the main paper—the standard SIMCLR augmentations [Chen et al., 2020a] and the RELICv2multi-crop and saliency-based masking [Tomasev et al., 2022]. Following the hyperparameter settings of the main results, we use\n\n• batch size: B = 4096\n\n• queue capacity C = 20B (unless specifically ablated)\n\n• number of nearest neighbours k = 1 (unless specifically ablated)\n\n• view voting is used (unless specifically ablated)\n\n• weight decay: 1e − 6 (exclude biases and batch normalization parameters)\n\n• optimizer: LARS (exclude biases and batch normalization parameters from adaptation)\n\n• base learning rate: 0.3 (scaled linearly with batch size [Goyal et al., 2017])\n\n• warm-up: 10 epochs\n\n• cosine decay schedule for learning rate\n\n• exponential moving average parameter: 0.996\n\n• views: 4 large views of size 224 × 224 and 2 small views of size 96 × 96\n\n• temperature: τ = 0.2\n\n• number of semantic positives: 3 (unless specifically ablated)\n\n• 10 randomly subsampled negatives per anchor\n\n• α = 1/5 (unless specifically ablated), λ = 5 and c = 0.3.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nD.2 ADDITIONAL ANALYSES\n\nNumber of semantic positives We study the effect of varying the number of semantic positives in SEMPPL. Table 8a shows that increasing this number from 1 to 3 only has an effect on the amount of correctly predicted pseudo-labels, but no effect on downstream performance. On the other hand, using 5 or 10 semantic positives significantly improves performance and also yields much more accurate pseudo-labels prediction.\n\nTraining duration Next, we vary the length of training representations and examine downstream performance. As can be seen from Table 8b, both the proportion of correctly predicted pseudolabels and downstream performance improve with longer training up to 300 epochs but decrease if we continue training up to 500 epochs. This indicates with training longer than 300 epochs SEMPPL is starting to overfit, an observation consistent with phenomena reported elsewhere in the literature involving noisy labels [Li et al., 2019a; Kim et al., 2019; Liu et al., 2020; Han et al., 2018; Zhang & Sabuncu, 2018; Wang et al., 2019b].\n\nTable 8: Top-1 accuracy (in %) on the ImageNet-1k test set, and accuracy (in %) of correctly predicted pseudo-labels at the end of training for semantic positives and training length experiments.\n\nNum. positives\n\nTop-1\n\nPseudo-label acc.\n\n1 2\n3 5\n10\n\n69.9 69.9 69.9 71.0 70.7\n\n68.6 70.9 69 72.8 72.9\n\nTraining time (epochs)\n\nTop-1\n\nPseudo-label acc.\n\n100 200 300 500\n\n69.9 72.4 72.7 72.3\n\n69.2 76.8 77.9 75.6\n\n(a) Varying the number of semantic positives.\n\n(b) Varying the length of training.\n\nView voting SEMPPL generates multiple views from a single instance image in order to learn representations. Those different views can be leveraged towards better pseudo-labels prediction. Rather than only picking one randomly selected data view to compute a single pseudo-label, we perform majority voting over (noisy) pseudo-labels computed from all available image views. Specifically, we compare the online predictor embedding of one view with the queue of the target projector embeddings of the same data view from previous batches in the first setting; in the second setting we compare the online predictor embedding of each view with the queue of the target projector embeddings of each other data view from previous batches.\n\nSince SEMPPL relies on 4 large views, this yields up to 16 different pairs of views to compare and compute pseudo-labels from, i.e. we get 16 pseudo-label predictions; this setting we call view voting. Table 9 shows that using all available views to compute pseudo-labels significantly increases pseudo-labels accuracy which in turn significantly improves downstream performance.\n\nTable 9: Top-1 accuracy (in %) on the ImageNet-1k test set, and accuracy (in %) of correctly predicted pseudo-labels at the end of training for view voting experiments (using all views to compute pseudo-labels vs using just a single view).\n\nView voting\n\nTop-1\n\nPseudo-label acc.\n\nOn Off\n\n69.9 69.0\n\n68.6 62.5\n\nIn order to compute pseudo-labels we use k-nearest neighbour Number of nearest neighbours lookup on the queue. While in the main results Section 3 we consistently assume k = 1 here we ablate the effect of varying k on downstream performance. As can be seen Table 10a, increasing k actually leads to a small decrease in performance. This is attributable to the decrease in the proportion of correctly predicted pseudo-labels as k increases.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nQueue length How long should the queue of target projector embeddings for computing pseudolabels be? As the queue grows in size, it contains increasingly stale embeddings and threatens to hamper the accuracy of predicted pseudo-labels. On the other hand, increasing queue size increases the amount and diversity of labelled embeddings available which we would expect to be beneficial. Those two opposing forces—staleness and increasing coverage—govern the accuracy with which we can correctly predict pseudo-labels in turn directly affecting the selection of semantic positives and their quality. We resolve this ambiguity empirically. As seen in Table 10b, increasing the coverage and diversity of labelled embeddings has a strong positive effect on representation learning and downstream performance. Staleness of embeddings is far less of a problem at practical (i.e., not very large) queue sizes, showing diversity and coverage to be the dominant factor.\n\nTable 10: Top-1 accuracy (in %) on the ImageNet-1k test set, and accuracy (in %) of correctly predicted pseudo-labels at the end of training for k-NN and queue length experiments.\n\nk-nn\n\nTop-1\n\nPseudo-label acc.\n\n1 2\n3 5\n10\n\n70.0 69.9 69.8 69.8 69.1\n\n70.5 69.5 70.0 70.0 68.0\n\nQueue size (C)\n\nTop-1\n\nPseudo-label acc.\n\n4000 8000 12000 20000 40000 80000 200000\n\n67.9 68.6 68.8 69.2 69.6 69.9 69.8\n\n53.1 60.4 62.0 64.3 66.9 68.8 69.5\n\n(a) Varying the number of nearest neighbours.\n\n(b) Varying queue size.\n\nThe effect of the loss weight α We use α in Equation 4 to weight the contribution of the loss coming from semantic positives against the loss coming from augmentation positives. In SEMPPL we use multiple semantic positives for learning and thus we need to adjust α to appropriately weigh the contribution of the individual semantic positives. In our main experiments, we use 3 semantic positives and for this reason α is not equal to 1.\n\nIn Table 11 we vary α from 0.0 to 1.0, with α = 0.0 effectively recovering RELICv2. The results indicate that the SEMPPL loss notably improves over RELICv2, but that the exact choice of α should be treated as a hyperparameter. Note that the value 0.2 is very close to the 1 3 which is exactly 1/number of semantic positives. Thus, the optimal choice of α is very close to that fraction.\n\nTable 11: Top-1 accuracy (in %) on the ImageNet-1k test set for varied values of the loss weight α\n\nLoss weight (α)\n\nTop-1\n\n0.0 (RELICv2) 0.2 1.0\n\n72.4 76.0 74.6\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nE AUGMENTATIONS\n\nIn this work, we follow the established data augmentations protocols and pipelines of Chen et al. [2020a]; Grill et al. [2020]; Caron et al. [2020]; Mitrovic et al. [2021]; Chen et al. [2020b]. Specifically, SEMPPL uses a set of augmentations to generate different views of the original image which has three channels, red r, green g and blue b with r, g, b ∈ [0, 1].\n\nThe augmentations we use are generated by applying the following sequence of operations in the following order\n\n1. Crop the image: Randomly select a patch of the image, between a minimum and maximum crop area of the image, with aspect ratio sampled log-uniformly in [3/4, 4/3]. Upscale the patch, via bicubic interpolation, to a square image of size s × s.\n\n2. Flip the image horizontally.\n\n3. Colour jitter: randomly adjust brightness, contrast, saturation and hue of the image, in a random order, uniformly by a value in [−a, a] where a is the maximum adjustment (specified below).\n\n4. Grayscale the image, such that the channels are combined into one channel with value\n\n0.2989r + 0.5870g + 0.1140b.\n\n5. Randomly blur. Apply a 23 × 23 Gaussian kernel with standard deviation sampled uni-\n\nformly in [0.1, 2.0].\n\n6. Randomly solarize:\n\nthreshold each channel value such that all values less than 0.5 are\n\nreplaced by 0 and all values above or equal to 0.5 are replaced with 1.\n\nApart from the initial step of image cropping, each subsequent step is applied with a certain probability to generate the augmented view of the original image. These probabilities and other augmentation parameters are given in Table 12. SEMPPL uses 4 large views of size 224 × 224 pixels and 2 small views of 96 × 96 pixels; to get the first and third large views and the first small view we use the parameters listed below for odd views, while for the second and fourth large view and the second small view we use the parameters for even views. Note that these are the same augmentations used also in Chen et al. [2020a]; Grill et al. [2020]; Caron et al. [2020]; Mitrovic et al. [2021]; Chen et al. [2020b].\n\nIn addition to these augmentations, we also randomly apply the saliency masking augmentation proposed in Tomasev et al. [2022] which enables us to remove a large part of the background. We follow the protocol described in Tomasev et al. [2022] for computing the saliency masking for an image and we apply this augmentation with probability 0.1 to the 4 large views. In keeping with Tomasev et al. [2022], we fill out the removed background of the image with homogeneous grayscale noise with the grayscale level randomly sampled for each view. We only apply the saliency masking when the remaining foreground covers at least 20% of the total image.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nParameter\n\nEven views\n\nOdd views\n\nProbability of randomly cropping Probability of horizontal flip Probability of colour jittering Probability of grayscaling Probability of blurring Probability of solarization Maximum adjustment a of brightness Maximum adjustment a of contrast Maximum adjustment a of saturation Maximum adjustment a of hue Crop size s Crop minimum area Crop maximum area\n\n50% 50% 80% 20% 100% 0% 0.4 0.4 0.2 0.1 224 8% 100%\n\n50% 50% 80% 20% 10% 20% 0.4 0.4 0.2 0.1 96 (small), 224 (large) 5% (small), 14% (large) 14% (small), 100% (large)\n\nTable 12: Parameters of data augmentation scheme. Small/large indicates small or large crop.\n\nF COMPUTATIONAL COST OF SEMPPL\n\nAdded cost of SEMPPL As SEMPPL is based on RELICv2, the computational overhead of SEMPPL over RELICv2 comes from three factors: i) the queue maintenance, ii) the k-NN execution, and iii) the computation of the added loss term. We carefully analyzed this overhead, and concluded the following:\n\n• The k-NN and the queue maintenance take 5.6% of the step time, with the k-NN itself\n\ntaking 4.5%.\n\n• The total loss (both forward and backward passes of loss terms coming from both augmentation and semantic positives) takes 3.9% of the step time, with 2.9% belonging to the additional loss coming from semantic positives.\n\n• In total, SEMPPL takes 8.5% of the total step time.\n\nNote that in our work we use a naive implementation of the k-NN. If needed, the k-NN can be further sped up, at a fraction of the accuracy, with a fast approximate k-NN model such as FAISS [Johnson et al., 2019]. This would further reduce the computation cost of using the k-NN to predict pseudolabels and select semantic positives. Additionally, since our code is not heavily optimised, there is room for additionally lowering these numbers with targeted optimisations.\n\nComparing SEMPPL to related models The direct comparison of training times of SEMPPL to related models can be deceiving, since there are three different sources of difference that influence the comparison. These summarise to differences in: i) accelerator architectures used, ii) deep learning frameworks used, and iii) concrete code, which can be assumed, is not optimised for any of the models, given their research nature. Being aware of these sources of difference, we contrast the running times of pre-training for SEMPPL and related models:\n\n• PAWS [Assran et al., 2021] report 8.5 hours per 100 epochs on 64 V100s. Assuming\n\ncontinuity in speed, this is equivalent to 25.5 hours for 300 epochs.\n\n• SimMatch [Zheng et al., 2022] report 2.34 hours per epoch on 8 V100s. If we (generously) assume perfect scaling (which is difficult in reality) to 64 V100s, this is equivalent to 87.75 hrs for 300 epochs.\n\n• SEMPPL trains in 13 hours on 64 TPUv3 cores.\n\nGiven empirical evidence that TPUv3 is roughly 23% faster than V100 on ResNet-50 training [Khairy, 2020], one could infer that the implementation of our model is significantly more computationally efficient. However, given that we cannot control all the aforementioned differences mentioned above, more careful analysis is needed to establish the right cost for each of these methods.\n\n25",
    "reference": "# Summary Of The Paper\n\nThe paper proposes an elegant way of combining pseudo labels (semantic similarity) and instance similarity. This method achieves state of the art performance on 1% and 10% ImageNet settings using ResNet-50 architecture. Transfer learning and OOD generalization experiments show the usefulness of this approach.\n\n# Strength And Weaknesses\n\n**Strengths**  \n_S1_. The paper provides an elegant solution to combine pseudo labels (semantic similarity) and instance similarity while learning visual representations. This approach is more elegant than SimMatch (Zheng et al, 2022), beating their performance with lesser number of training epochs.   \n_S2_. It achieves state of the art on 1% and 10% ImageNet settings using ResNet-50 architecture.  \n_S3_. The paper beats the transfer learning performance w.r.t. SimMatch (Zheng et al, 2022).\n\n**Weaknesses**  \nAs such the paper is well written with a good set of ablation studies. But certain things aren't clear.  \n_W1_. The paper says that is uses the Relic objective (Mitrovich et. al., 2021). It is not clear how important this objective would be in terms of the performance gain. This objective could be used for the competing approaches as well.  \n_W2_. It would be good to have the precision and recall plots of the pseudo label quality (Fig. 2) for the competing approaches. That would give good insight into whether there is a correlation with this elegant way of optimizing to the quality of pseudo labels generated. It would also justify the point the authors are marking about the virtuous cycle of learning better representations while improving quality of pseudo labels. Currently, that point has not been justified empirically. We know the approach helps learning better representations. \n_W3_. It would be good to also show how this approach does on limited annotation settings introduced in PAWS (Assran et. al., 2021) and Masked Siamese Networks (Assran et. al., ECCV 2022) with 1-10 annotations per class.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper has written in a clear and precise manner. \n\nWhile this idea of combining the pseudo labels while learning visual representations during contrastive learning has been introduced in SimMatch (Zheng et al, 2022), the method discussed in this paper is more elegant and original.\n\n# Summary Of The Review\n\nThe paper provides an elegant solution to the idea introduced by SimMatch (Zheng et. al. 2022). It is well written and there is extensive evaluation done w.r.t. semi-supervised learning approaches in the literature, along with transfer learning and OOD generalization benchmarks. The novelty of the idea is not large. But it is an interesting contribution to the literature.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nCO3: COOPERATIVE UNSUPERVISED 3D REPRESENTATION LEARNING FOR AUTONOMOUS DRIVING\n\npluo.lhi@gmail.com\n\nRunjian Chen 1, Yao Mu 1, Runsen Xu 4, Wenqi Shao 2∗, Chenhan Jiang 5, Hang Xu 3, Yu Qiao 2, Zhenguo Li 3, Ping Luo 1∗ {rjchen, muyao}@connect.hku.hk {shaowenqi, qiaoyu}@pjlab.org.cn li.zhenguo@huawei.com runsenxu@connect.cuhk.edu.hk 1 The University of Hong Kong 2 Shanghai AI Laboratory 3 Huawei Noah’s Ark Lab 4 The Chinese University of Hong Kong 5 Hong Kong University of Science and Technology\n\nxbjxh@live.com\n\ncjiangao@connect.hkust.hk\n\nABSTRACT\n\nUnsupervised contrastive learning for indoor-scene point clouds has achieved great successes. However, unsupervised representation learning on outdoor-scene point clouds remains challenging because previous methods need to reconstruct the whole scene and capture partial views for the contrastive objective. This is infeasible in outdoor scenes with moving objects, obstacles, and sensors. In this paper, we propose CO3, namely Cooperative Contrastive Learning and Contextual Shape Prediction, to learn 3D representation for outdoor-scene point clouds in an unsupervised manner. CO3 has several merits compared to existing methods. (1) It utilizes LiDAR point clouds from vehicle-side and infrastructure-side to build views that differ enough but meanwhile maintain common semantic information for contrastive learning, which are more appropriate than views built by previous methods. (2) Alongside the contrastive objective, we propose contextual shape prediction to bring more task-relevant information for unsupervised 3D point cloud representation learning and we also provide a theoretical analysis for this pretraining goal. (3) As compared to previous methods, representation learned by CO3 is able to be transferred to different outdoor scene datasets collected by different type of LiDAR sensors. (4) CO3 improves current state-of-the-art methods on both Once, KITTI and NuScenes datasets by up to 2.58 mAP in 3D object detection task and 3.54 mIoU in LiDAR semantic segmentation task. Codes and models will be released here. We believe CO3 will facilitate understanding LiDAR point clouds in outdoor scene.\n\n1\n\nINTRODUCTION\n\nLiDAR is an important sensor for autonomous driving in outdoor environments and both of the machine learning and computer vision communities have shown strong interest on perception tasks on LiDAR point clouds, including 3D object detection, segmentation and tracking. Up to now, randomly initializing and directly training from scratch on detailed annotated data still dominates this field. On the contrary, recent research efforts (He et al., 2020; Tian et al., 2019; Caron et al., 2020; Grill et al., 2020; Wang et al., 2021) in image domain focus on unsupervised representation learning with contrastive objective on different views built from different augmentation of images. They pre-train the 2D backbone with a large-scale dataset like ImageNet (Deng et al., 2009) in an unsupervised manner and use the pre-trained backbone to initialize downstream neural networks on different datasets and tasks, which achieve significant performance improvement in 2D object detection and semantic segmentation (Girshick et al., 2014; Lin et al., 2017; Ren et al., 2015). Inspired by these\n\n∗corresponding authors are Wenqi Shao and Ping Luo\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Example views built by different methods in contrastive learning, including (a) previous indoor-scene methods (b) previous outdoor-scene methods and (c) the proposed CO3. Compared to previous methods, CO3 can build two views that differ a lot and share adequate common semantics.\n\nsuccesses, we explore unsupervised representation learning for outdoor scene point clouds to learn general representations for different architectures on various downstream datasets and tasks.\n\nIn the past decade, learning 3D representation from unlabelled data has achieved great success in indoor-scene point clouds. PointContrast (Xie et al., 2020) is the pioneering work and proposes to reconstruct the whole indoor scenes, collect partial point clouds from two different poses and utilize them as two views in contrastive learning to learn dense (point-level or voxel-level) representation. More recent works such as (Hou et al., 2021) and (Liu et al., 2020) also need the reconstruction and this naturally assumes that the environment is static. Fig. 1 (a) shows an example of views in PointContrast (Xie et al., 2020). We can see that the views differ a lot because they are captured from different poses but meanwhile, they still contain enough common semantic information such as the same sofa and table. These are demonstrated important properties of views in contrastive learning in (Tian et al., 2020).\n\nHowever, outdoor scenes are dynamic and large-scale, making it impossible to reconstruct the whole scenes for building views. Thus, methods in (Xie et al., 2020; Hou et al., 2021; Liu et al., 2020) cannot be directly transferred but there exists two possible alternatives to build views. The first idea, embraced by (Liang et al., 2021; Yin et al., 2022), is to apply data augmentation to single frame of point cloud and treat the original and augmented versions as different views, which are indicated by the first and second pictures in Fig. 1 (b). However, all the augmentation of point clouds, including random drop, rotation and scaling, can be implemented in a linear transformation and views constructed in this way do not differ enough. The second idea is to consider point clouds at different timestamps as different views, represented by (Huang et al., 2021). Yet the moving objects would make it hard to find correct correspondence for contrastive learning. See the first and third pictures in Fig. 1 (b), while the autonomous vehicle is waiting at the crossing, other cars and pedestrians are moving around. The autonomous vehicle has no idea about how they move and is not able to find correct correspondence (common semantics). Due to these limitations, it is still challenging when transferring the pre-trained 3D encoders to datasets collected by different LiDAR sensors. Could we find better views to learn general representations for outdoor-scene LiDAR point clouds?\n\nIn this paper, we propose COoperative COntrastive Learning and COntextual Shape Prediction, namely CO3, to explore the potential of utilizing vehicle-infrastructure cooperation dataset for building adequate views in unsupervised 3D representation learning. As shown in (c) in Fig. 1, a recently released infrastructure-vehicle-cooperation dataset called DAIR-V2X (Yu et al., 2022) is utilized to learn general 3D representations. Point clouds from both vehicle and infrastructure sides are captured at the same timestamp thus views share adequate common semantic. Meanwhile infrastructure-side and vehicle-side point clouds differ a lot. These properties make views constructed in this way appropriate in contrastive learning. Besides, as proposed in (Wang et al., 2022), representations learned by pure contrastive learning lack task-relevant information. Thus we further add a pre-training goal\n\n2\n\nView1View2Sampled sceneView 1View 2(a) Indoor SceneSampled frameLinearaugmentationT+10 frameVehicle(b) Previous Methods inOutdoor SceneVehicle-sideInfrastructure-sideIllustration of the real sceneView from infrastructureView from vehicle(c) Our CO3: Cooperative View from Infrastructure and VehicleVehicletocollect dataVehicle to collect dataVehicleVehiclePublished as a conference paper at ICLR 2023\n\nFigure 2: (a) shows the unsupervised 3D representation learning pipeline. (b) presents the performance changes after pre-training with different methods. Our CO3 achieves consistent improvement.\n\ncalled contextual shape prediction to reconstruct local point distribution, encouraging our proposed CO3 to capture more task-relevant information.\n\n(1) CO3 is proposed to utilize the vehicleOur contributions can be summarized as follows. infrastructure-cooperation dataset to build adequate views for unsupervised contrastive 3D representation learning on outdoor-scene point clouds. (2) A shape-context prediction task is proposed alongside to inject task-relevant information, which is beneficial for downstream 3D detection and LiDAR semantic segmentation tasks. (3) The learned 3D representations is generic enough to be well transferred to datasets collected by different LiDAR sensors. (4) Extensive experiments demonstrate the effectiveness of CO3. For example, on 3D object detection task, CO3 improves Second, CenterPoints on Once dataset by 1.07 and 2.58 mAPs respectively. As for LiDAR semantic segmentation task, CO3 improves Cylinder3D on NuScenes dataset by 3.54 mIoUs.\n\n2 RELATED WORKS 3D Perception Tasks. 3D object detection aims to predict 3D boundary boxes for different objects in the LiDAR point clouds. Current 3D detectors can be divided into three main streams due to the 3D backbones they use: (1) point-based methods (Shi et al., 2019; Chen et al., 2017; Yang et al., 2018) use point-based 3D backbone. (2) voxel-based methods (Zhou & Tuzel, 2018; Lang et al., 2019; Su et al., 2015; Shi et al., 2020b; Yin et al., 2021; Fan et al., 2021) generally transform point cloud into voxel grids and process them using 3D volumetric convolutions. (3) point-voxel-combined methods (Shi et al., 2020a; 2021; Deng et al., 2021) utilize features from both (1) and (2). LiDAR Semantic Segmentation aims to predict per-point label for LiDAR point clouds. Cylinder3D (Zhu et al., 2021) and PVKD (Hou et al., 2022) are current SOTA methods for LiDAR Semantic Segmentation.\n\nUnsupervised 3D Representation Learning. As shown in (a) of Fig. 2, unsupervised 3D representation learning aims to pre-train only for one time and downstream to different architectures on various datasets and tasks to achieve performance gain. PointContrast (Xie et al., 2020) is the pioneering work for unsupervised contrastive learning on indoor-scene point clouds, which relies on the reconstructed point clouds for constructing adequate views. To extend their ideas to outdoor-scene point clouds, GCC-3D and ProposalContrast (Liang et al., 2021; Yin et al., 2022) augment single frame of point cloud to build views and STRL (Huang et al., 2021) utilizes point clouds at different timestamps as views for contrastive learning. Alongside contrastive learning, authors in (Hu et al., 2021) propose to use safe space prediction as a pretext task for self-supervised representation learning. However, as discussed in Sec. 1, previous works fail to build suitable views and their learned representations are unable to transfer to datasets collected by different LiDAR sensors. In this paper, we propose to use point clouds from vehicle and infrastructure to construct views in contrastive learning and also extend the idea of shape context to introduce task-relevant information with a local-distribution prediction goal. The performance change after initialized with different methods are shown in (b) of Fig. 2. CO3 generally improve the performance and achieve more performance gains than other pre-training methods for different detectors on different datasets.\n\n3 METHODS\n\nIn this section, we introduce the proposed CO3 for unsupervised representation learning on LiDAR point clouds in outdoor scenes. As detailed in Fig. 3, CO3 has two pre-training objectives: (a) a\n\n3\n\nPre-train only once on DAIR-V2X dataset with different methods3D BackboneLidar Point CloudsPoints/Voxels with featuresCO3Downstream to various architectures for different datasets and tasks3DObject detectionOnceSecondPV-RCNNCenterPoint3DObject detectionKITTISecondPV-RCNNLiDARsemantic segmentationNuScenesCylinder3D(a) Pipeline for unsupervised 3D representation learning(b) Performance change after initialized by different methodsProposal ContrastSTRLPublished as a conference paper at ICLR 2023\n\nFigure 3: The pipeline of CO3. With vehicle-side and fusion point clouds as inputs, we first process them with the 3D backbone and propose two pre-training objectives: (a) Cooperative Contrastive Loss (b) Contextual Shape Prediction Loss\n\ncooperative contrastive learning goal on dense (point-level or voxel-level) representations between vehicle-side and fusion point clouds, which provides adequate views for contrastive learning. (b) a contextual shape prediction loss to bring in more task-relevant information. To start with, we discuss the problem formulation and overall pipeline in Sec. 3.1. Then we respectively introduce the cooperative contrastive objective and contextual shape prediction goal in Sec. 3.2 and Sec. 3.3.\n\n3.1 PROBLEM FORMULATION AND PIPELINE\n\nNotation. To begin with, we denote LiDAR point clouds P ∈ RN ×(3+d) as the concatenation of the xyz-coordinate C ∈ RN ×3 and the point features F ∈ RN ×d, which leads to P = [C, F]. Here N denotes the number of points (or voxels) and d is the number of point feature channels. For raw LiDAR point clouds, d = 1 represents the intensity of each point. Moreover, we use the subscripts (e.g. ‘v’ and ‘i’) to indicate point clouds from different sources. For example, Pv = [Cv, Fv] ∈ RNv×(3+d) and Pi = [Ci, Fi] ∈ RNi×(3+d) respectively represent Nv and Ni points (or voxels) in vehicle and infrastructure sides. Besides, each pair of vehicle-side and infrastructure-side point clouds is associated with a transformation T mapping infrastructure-side coordinate to that of vehicle-side.\n\ni = [C′\n\ni, Fi] where C′\n\nPre-processing and Encoding. For cooperative learning, we need to first align the point clouds from both sides in the same coordinate. Hence, we transform the infrastructure-side point clouds to P′ i = T (Ci). However, we empirically find that the contrastive learning built upon vehicle-side point cloud Pv and transformed infrastructure-side point clouds P′ i only achieve marginal performance gains than training from scratch on downstream ONCE dataset (0.53 mAPs vs 2.58 mAPs, also see Table 7 in Appendix D). We believe this stems from the sparsity of LiDAR point clouds, which sometimes make it difficult to find good positive pairs to perform contrastive learning. To mitigate this problem, we concatenate the LiDAR point clouds from both sides to fusion point i] ∈ R(Nv+Ni)×(3+d), which is used as the contrastive view of vehicle-side point clouds Pf = [Pv, P′ clouds Pv as shown in Fig.3. By default of notations, we can express the coordinate and feature of fusion point clouds as Cf = [Cv, C′ i] and Ff = [Fv, Fi], respectively. Then Pv and Pf are embedded by the 3D encoder f enc to obtain their 3D representations. We use subscript ‘v/f’ to indicate that the same operation is applied respectively on both pointclouds. ˆPv/f = f enc(Pv/f) (1) where ˆPv/f ∈ R ˆNv/f×(3+ ˆd) indicates the vehicle point cloud and fusion point cloud respectively. Here we use ˆNv/f to denote the number of points after encoding because pooling operation often exists in 3D encoders (Graham et al., 2018) and changes the number of voxels/points. Moreover, ˆd is the number of feature channels after encoding.\n\nLoss Function. To guide the 3D encoder to learn good representations in an unsupervised manner, our proposed CO3 consists of a cooperative contrastive loss LCO2 and a contextual shape prediction loss LCSP. The overall loss function is given by:\n\nL =\n\n1 |Pv/f|\n\n(cid:88)\n\nPv/f∈{Pv/f}\n\nLCO2{ ˆPv, ˆPf} + w × LCSP{ ˆPv, ˆPf, Cf}\n\n(2)\n\n4\n\nFusion Point Cloud / VoxelsFusion Points/Voxels with features3D Backbone3D Backbone(-) negative pair(+) positive pair(a) Cooperative Contrastive LossSampled Vehicle Points/VoxelsSampled Fusion Points/Voxels(+)(-)MLP1MLP1Vehicle-side Point Cloud / VoxelsVehicle-side Points/Voxels with features(b) Contextual Shape Prediction LossSampled Points/VoxelsPredicted Shape Context“Ground-truth”KLD LossMLP2Shared WeightsPublished as a conference paper at ICLR 2023\n\nwhere Pv/f denote a batch of vehicle and fusion point clouds and |Pv/f| indicates the batch size. LCO2 applies contrastive learning on the encoded vehicle and fusion pointclouds. Meanwhile, LCSP introduces more task-relevant information into f enc by using the encoded features to predict contextual shape obtained by the coordinate of fusion point clouds Cf. w is a weight to balance the losses.\n\n3.2 COOPERATIVE CONTRASTIVE OBJECTIVE\n\nUnsupervised contrastive learning has been demonstrated successful in image domain (He et al., 2020; Tian et al., 2019) and indoor-scene point clouds (Xie et al., 2020). However, when it turns to outdoor-scene LiDAR point clouds, building adequate views, which share common semantics while differing enough, for contrastive learning is difficult. To tackle this challenge, we utilize a recently released vehicle-infrastructure-cooperation dataset called DAIR-V2X (Yu et al., 2022) and use vehicle-side point clouds and fusion point clouds as views for contrastive representation learning. Views built in this way differ a lot because they are captured at different positions and they share enough information because they are captured at the same timestamp. More details about view building in contrastive learning can be found in Appendix A.\n\nContrastive Head. Following BYOL (Grill et al., 2020), we construct contrastive head of cooperative contrastive objective by a Multi-Layer-Perceptron (MLP) layer and a l2-normalization. Specifically, the embedded features of vehicle and fusion point clouds, ˆFv and ˆFf, are first projected by a MLP layer denoted as MLP1. Then an l2-normalization is applied along feature dimension. This process is described below, where Zv/f ∈ R ˆNv/f×dz and dz is the output feature dimension.\n\nˆZv/f = MLP1( ˆFv/f), Zv/f = ˆZv/f/∥ ˆZv/f∥2\n\n(3)\n\nCooperative Contrastive Loss. For cooperative contrastive learning, we obtain positive pairs by exploring the correspondence between coordinates ˆCv and ˆCf. In detail, we uniformly sample N1 points from the vehicle point cloud and find their corresponding points in the fusion point cloud to v/f ∈ R1×dz . We form N1 pairs of features ({zn treat corresponding points (or voxels) as positive pairs and otherwise negative pairs for contrastive learning. Example pairs are shown in Fig. 3. The final loss function is formulated below,\n\nn=1) from Zv/f for contrastive learning, where zn\n\nv/f}N1\n\nLCO2 =\n\n1 N1\n\nN1(cid:88)\n\nn=1\n\n− log(\n\nexp(zn v · zn i=1 exp(zn\n\nf /τ ) v · zi\n\n(cid:80)N1\n\nf /τ )\n\n)\n\n(4)\n\nwhere τ is the temperature parameter. The bottom right box in Fig. 3 shows examples for this loss. As ground points only contain background information that is not related to perception tasks, we mark those points with height value lower than a threshold zthd as ground points and filter them out when sampling.\n\n3.3 CONTEXTUAL SHAPE PREDICTION\n\nCO3 aims to learn representations applicable to various downstream datasets. However, as demonstrated in (Wang et al., 2022), pure contrastive loss in Eqn. (4) would result in the lack of task-relevant information in the learned representations, making it hard to generalize across different architectures and datasets. Meanwhile, an additional reconstruction objective could increase the mutual information between the representations and the input views, which would bring in task-relevant information. Please refer to detailed explanations about this in Appendix B. For outdoor-scene point clouds, it is difficult to reconstruct the whole scene with point/voxel-level representations. To mitigate this issue, we propose to reconstruct the neighborhood of each point/voxel with its representation.\n\nLocal Distribution. Shape context has been demonstrated as useful point feature descriptor in previous works (Hou et al., 2021; Belongie et al., 2002; Körtgen et al., 2003; Xie et al., 2018). Fig. 4 shows two examples of shape context with 8 bins (the number of bins can be changed) around the query point, which\n\nFigure 4: Two examples of shape context.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nis marked as a larger black point. Previous works use the number of points in each bin as the descriptor for the query point. However, it can be difficult for the neural net to learn to regress the exact number of points in each bin. Thus, we propose to use the representation to predict a local distribution built upon shape context. In practice, we divide the neighborhood of each point in fusion point clouds into Nbin = 32 bins along xyz-plane with R1 = 0.5m and R2 = 4m. Then we compute the “raw” shape context Q′ ∈ RNf×Nbin, which describe the number of points in each bin around every fusion point. Next, we apply l2-normalization to Q′ and a consecutive softmax to obtain the ground-truth shape context Q ∈ RNf×Nbin as described below, where Qi,∗ ∈ R1×Nbin describes the ground-truth local distribution for i-th point.\n\nQi,∗ = softmax(Q′\n\ni,∗/∥Q′\n\ni,∗∥2)\n\n(5)\n\nPrediction Loss. We use the encoded features of both vehicle and fusion point clouds, i.e. ˆFv/f, to predict the local distribution. To be specific, ˆFv/f is first passed through a MLP layer MLP2 and softmax operation is applied on the projected features to get the predicted local distribution.\n\nPv/f = softmax(MLP2( ˆFv/f))\n\n(6)\n\nwhere Pv/f ∈ R ˆNv/f×Nbin. Then we uniformly sample N2 predictions ({pn their corresponding “ground truth” in Q by coordinate correspondence, where we have {qn dimensions of each prediction and “ground truth” are pn the prediction loss with KL-divergence is given by,\n\nn=1) from Pv/f and find n=1. The v/f ∈ R1×Nbin . Finally,\n\nv/f ∈ R1×Nbin and qn\n\nv/f}N2\n\nv/f}N2\n\nLCSP =\n\n1 N2\n\nN2(cid:88)\n\nNbin(cid:88)\n\n(pn,m\n\nv\n\nlog\n\nn=1\n\nm=1\n\n4 EXPERIMENTS\n\nv\n\npn,m qn,m\n\nv\n\n+ pn,m\n\nf\n\nlog\n\nf\n\npn,m qn,m\n\nf\n\n)\n\n(7)\n\nThe goal for unsupervised representation learning is to learn general representation that can benefit different downstream architectures on different downstream datasets and tasks. In this section, we design experiments to answer the question whether CO3 learns such representation as compared to previous methods. We first provide experiment setups in Sec. 4.1 and then discuss main results in Sec. 4.2. We also conduct ablation study and qualitative visualization in Sec. 4.3 and 4.4.\n\n4.1 EXPERIMENT SETUP\n\nPre-training Dataset. We utilize the recently released vehicle-infrastructure-cooperation dataset called DAIR-V2X (Yu et al., 2022) to pre-train the 3D encoder. DAIR-V2X is the first real-world autonomous dataset for vehicle-infrastructure-cooperative task. The LiDAR sensor at vehicle-side is 40-beam while a 120-beam LiDAR is utilized at infrastructure-side. There are 38845 LiDAR frames (10084 in vehicle-side and 22325 in infrastructure-side) for cooperative-detection task. The dataset contains around 7000 synchronized cooperative samples in total.\n\nImplementation Details of CO3. We use Sparse-Convolution as the 3D encoder which is a 3D convolutional network because it is widely used as 3D encoders in current state-of-the-art methods (Zhou & Tuzel, 2018; Yin et al., 2021; Shi et al., 2020a). We set the number of feature channels denc = 64, the temperature parameter in contrastive learning τ = 0.07, the dimension of common feature space of vehicle-side and fusion point clouds dz = 256 and the sample number in cooperative contrastive loss N1 = 2048. For contextual shape prediction, we set the number of bins Nbin = 32, the sample number N2 = 2048 and the weighting constant w = 10. The threshold for ground point filtering is zthd = 1.6m. We empirically find that freezing the parameters of MLP2 brings better results in detection task thus we fix them.\n\nBaselines. We implement STRL (Huang et al., 2021) and use the official code of ProposalContrast (Yin et al., 2022) as baselines. Besides, as proposed in (Mao et al., 2021), several methods in image domain and indoor-scene point clouds can be transferred to outdoor scene point clouds, including Swav (Caron et al., 2020), Deep Cluster (Caron et al., 2018), BYOL (Grill et al., 2020) and Point Contrast (Xie et al., 2020). Note that in order to make fair comparisons, all the pre-training methods are pre-trained on DAIR-V2X. Thus there might exist number discrepancy between the results and previous benchmarks.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nInit.\n\nRand Swav Deep Cluster BYOL PointContrast ProposalContrast STRL Ours\n\nOverall 52.21 53.03 52.30 45.24 47.64 50.70 51.57 53.28+1.07\n\nSecond\n\n0-30m 60.71 61.38 61.03 53.84 56.91 59.42 59.80 62.16\n\n30-50m 47.31 48.34 47.00 40.22 42.54 46.09 46.59 49.33\n\nOverall 54.55 54.89 54.91 49.41 50.49 52.79 54.25 55.17+0.61\n\nPV-RCNN 0-30m 63.05 63.41 63.84 58.04 58.94 60.97 63.03 63.94\n\n30-50m 50.00 50.31 50.29 45.19 46.30 48.91 49.31 50.29\n\nOverall 55.92 57.00 57.65 52.17 54.17 56.02 57.44 58.50+2.58\n\nCenterPoint 0-30m 66.39 67.54 68.05 63.53 65.57 66.26 67.90 69.09\n\n30-50m 50.16 50.60 51.04 45.00 46.53 48.70 50.47 51.51\n\nTable 1: Results of 3D object detection on Once dataset (Mao et al., 2021). We conduct experiments on 3 different detectors: Second (Zhou & Tuzel, 2018) (short as Sec.), PV-RCNN (Shi et al., 2021) (short as PV) and CenterPoint (Yin et al., 2021) (short as Cen.) and 8 different initialization methods including random (short as Rand, i.e. training from scratch), Swav (Caron et al., 2020), Deep Cluster (short as D. Cl.) (Caron et al., 2018), BYOL (Grill et al., 2020), Point Contrast (short as P.C.) (Xie et al., 2020), GCC-3D (Liang et al., 2021) and STRL (Huang et al., 2021). Results are mAPs in %. “0-30m” and “30-50m” respectively indicate results for objects in 0 to 30 meters and 30 to 50 meters. The “Overall” metric highlighted in red is the overall mAP, which serves as major metric for comparisons. We use bold font for the best overall mAP of each detector for better understanding.\n\nDownstream Tasks. Two downstream tasks are selected for evaluation: 3D object detection and LiDAR semantic segmentation. 3D object detection task takes raw 3D point clouds as inputs and aims to output 3D boundary boxes of different objects in the scene. LiDAR semantic segmentation assign each 3D point a category label, including Car, Pedestrian, Bicycle, Truck and so on.\n\n3D Object Detection. We select two downstream datasets: Once (Mao et al., 2021) and KITTI (Geiger et al., 2012). Once has 15k fully annotated frames of LiDAR scans. A 40-beam LiDAR is used to collect the point cloud data. We adopt common practice, including point cloud range and voxel size, in their public code repository. mAPs (mean accurate precisions) in different ranges and overall mAP are presented. KITTI is a widely used self-driving dataset, where point clouds are collected by LiDAR with 64 beams. It contains around 7k samples for training and another 7k for evaluation. All the results are evaluated by mAPs with three difficulty levels: Easy, Moderate and Hard. We select several current state-of-the-art methods implemented in the public repository of Once dataset (Mao et al., 2021)1 and OpenPCDet2 to evaluate the quality of representations learned by CO3, including Second (Zhou & Tuzel, 2018), CenterPoint (Yin et al., 2021) and PV-RCNN (Shi et al., 2020a).\n\nLiDAR Semantic Segmentation. We select NuScenes (Caesar et al., 2020) as downstream dataset. There are 1000 scenes each of which lasts 20 seconds in NuScenes. It is collected with a 32beam LiDAR sensor and the total number of frames is 40,000. We select Cylinder3D (Zhu et al., 2021) as downstream architecture. The full training is time-consuming (at least 4 days for one training) and we use a 1/8 training schedule setting to evaluate whether CO3 is able to speed up training. We follow the conventional evaluation metrics. mAPs for detailed categories and mean intersection-over-union (mIoU) for overall evaluation are presented. Per-class mIoU is first computed as mIoUi = , where TPi, FPi and FNi respectively represent true positive, false positive and false negative for class i. We then average over classes and get the final mIoU.\n\nTPi TPi+FPi+FNi\n\n4.2 MAIN RESULTS\n\nOnce Detection. As shown in Table 1, when initialized by CO3 , all the three detectors achieve the best performance on the overall mAPs, which we value the most, and CenterPoint (Yin et al., 2021) achieves the highest overall mAP (58.50) with 2.58 improvement. The improvement on PVRCNN (Shi et al., 2021) is 0.62 in mAP (similar lower improvement with other pre-training methods) because PV-RCNN (Shi et al., 2021) has both the point-based and voxel-based 3D backbones, among which CO3 only pre-trains the voxel-based branch. It can be found that other baselines are not able to learn general representation for different architectures. For example, STRL achieves +1.52 mAPs improvements on CenterPoint but degrades the performance of Second and PV-RCNN. On the contrary, CO3 achieve consistent improvement over different detectors.\n\nKITTI Detection. As shown in Table 2, when initialized by CO3 , PV-RCNN (Shi et al., 2021) achieves the best performance on Easy and Hard (+1.19) level and third place on Moderate level.\n\n1https://github.com/PointsCoder/Once_Benchmark 2https://github.com/open-mmlab/OpenPCDet\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nInitialization\n\nRandom Swav Deep Cluster BYOL PointContrast ProposalContrast STRL Ours\n\nEasy 73.29 73.23 73.19 71.05 72.67 74.23 73.95 74.40+1.11\n\nSecond Moderate 63.16 64.05 63.37 60.39 62.74 64.22 63.90 64.38+1.22\n\nHard 60.34 60.90 60.08 56.98 59.21 60.88 60.90 60.90+0.56\n\nEasy 78.54 78.43 77.05 77.96 77.62 77.28 77.10 78.84+0.30\n\nPV-RCNN Moderate 67.23 67.91 67.06 67.50 67.79 66.47 66.21 67.75+0.52\n\nHard 63.68 64.60 64.50 64.42 63.31 63.22 62.90 64.87+1.09\n\nTable 2: Results of 3D object detection on KITTI dataset (Geiger et al., 2012). Results are mAPs in %. “Easy”, “Moderate” and “Hard” respectively indicate difficulty levels defined in KITTI dataset. We use bold font for the best mAP of each detector in each difficulty level for better understanding.\n\nInitialization Random P.C. STRL Ours\n\nmIoU 63.34 64.31 64.71 66.88+3.54\n\nCar 84.32 84.70 84.66 85.52\n\nTruck 70.25 74.36 76.65 77.00\n\nCon. Veh. 28.29 30.81 27.30 36.00\n\nPed. 64.46 63.52 63.29 66.93\n\nTrailer 41.35 47.13 52.76 53.13\n\nBic. 0.00 10.16 12.79 19.51\n\nM.C. 60.46 55.55 60.11 70.65\n\nS.W. 69.76 70.38 70.27 70.40\n\nTerrain 71.21 71.43 71.70 72.43\n\nVeg. 83.32 84.65 84.60 84.88\n\nTable 3: LiDAR semantic segmentation on NuScenes (Caesar et al., 2020). Results are mIoU and mAPs in %. We shows details results in each category. “Con. Veh.”, “Ped.”, “Bic.”, “M.C.”, “S. W.” and “Veg.” are abbreviations respectively for Construction Vehicle, Pedestrian, Bicycle, MotoCycle, Sidewalk and Vegetation. We use bold font for the best results in each column for better understanding.\n\nInit. Random Sup. CO3\n\nOverall 55.92 57.50 58.50\n\nVehicle 62.85 63.86 64.60\n\nPedestrian 45.52 46.96 48.83\n\nCyclist 59.39 61.68 62.17\n\nInit. From Scratch CO3 w. ground CO3\n\nOverall 55.92 57.37 58.50\n\nVehicle 62.85 63.19 64.60\n\nPedestrian 45.52 48.16 48.83\n\nCyclist 59.39 60.76 62.17\n\nTable 4: Comparison to supervised initialization\n\nTable 5: Ablation study on filtering out ground points.\n\nMeanwhile, when Second (Zhou & Tuzel, 2018) is equipped with CO3, it achieves the highest mAPs on Easy level (+1.11), Moderate level (+1.22) and Hard level (+0.56). The lower gains on the KITTI dataset (Geiger et al., 2012) stem from the smaller number of training samples (half of that in Once (Mao et al., 2021)), which makes the detectors easily reach their capacity and improvement is hard to achieve. Consistent results across different initialization methods demonstrate this.\n\nNuScene Semantic Segmentation. As shown in Table 3, CO3 improve Cylinder3D by 3.54 in mIoU and also achieves the best performance among the four initialization methods. Meanwhile, when initialized by CO3, Cylinder3D achieves the best mAPs among all the categories. On truck and construction vehicle, CO3 improve the performance of random initialization by 6.75 and 7.71 mAPs, which is very important in autonomous driving because correct segmentation of different vehicles can help benefit control and avoid accidents.\n\nComparison to Supervised Pre-training. We find backbone pre-trained on 3D detection task from the official codebase of DAIR-V2X dataset and use it to initialize CenterPoint, after which we fine-tune it on Once dataset. Results are shown in Table 4. It can be found that although supervised pre-training achieve improvement as compared to training from scratch, CO3 leads to the best results in all categories and the overall mAP. This is because supervised pre-training overfit to DAIR-V2X dataset, making the improvement lower than CO3 in 3D detection task on Once dataset.\n\nOverall Evaluation. To summarize, CO3 achieves consistent performance gains over different architectures on different tasks (3D object detection and LiDAR semantic segmentation) and datasets (Once, KITTI and NuScenes) when pre-trained only on DAIR-V2X dataset. In comparison, other baseline pre-training methods only occasionally improve the performance and sometimes even bring degradation. These demonstrate CO3 learns general 3D representations.\n\n4.3 ABLATION STUDY\n\nThe influence of ground points in pre-training. We first conduct ablation experiments on filtering out ground points. We use CO3 to pre-train 3D backbone without filtering out ground points and downstream it to 3D object detection with CenterPoint on Once. Results are shown in Table 5. When pre-training without filtering out ground points, the performance of CO3 drop in each category and\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nInit.\n\nRandom Contextual Shape Prediction Only Cooperative Contrastive Only CO3\n\nOverall 55.92 57.30 57.53 58.50\n\nOnce (CenterPoint) Vehicle 62.85 62.86 63.39 64.50\n\nPedestrian 45.52 49.17 48.14 48.83\n\nCyclist 59.39 59.86 61.05 62.17\n\nOverall 63.16 63.36 63.41 64.38\n\nKITTI (Second)\n\nVehicle 77.45 77.75 77.40 77.95\n\nPedestrian 48.71 49.16 47.78 49.59\n\nCyclist 63.32 63.18 65.06 65.60\n\nTable 6: Results of ablation study on Once (Mao et al., 2021) and KITTI (Geiger et al., 2012). We use CenterPoint (Yin et al., 2021) on Once and Second (Zhou & Tuzel, 2018) on KITTI. Results are mAPs in %. For Once, results are average across different ranges. For KITTI, results are all in moderate level. We highlight the best performance in each column for better understanding.\n\nFigure 5: Visualization for detection results. Green boxes are predicted ones and red boxes are the ground truth.\n\nthe overall mAP. This demonstrate the effectiveness of filtering out ground points because ground points contain background information that is not useful for 3D perception tasks.\n\nThe effect of each component in CO3. We conduct ablation experiments to analyze the effectiveness of different components. We respectively pre-train the 3D encoder with cooperative contrastive objective and contextual shape prediction objective. As shown in Table 6, it can be found that each objective alone can achieve slight improvement, which demonstrates the effectiveness of either part. Besides, when pre-trained by CO3, we achieve the best performance on the overall mAPs. A more detailed discussion on the ablation study is provided in Appendix G\n\n4.4 QUALITATIVE EXPERIMENT\n\nWe use the 3D backbone pre-trained by CO3 and PointContrast to initialize CenterPoint and train it on Once dataset. Then we visualize the detection results in Fig. 5, where predicted boxes are marked as green and the ground truth boxes are red. In Case 1 and 2, when the detector is initialized by CO3, the detection results are more correct in headings as shown in the zoom-in area. Correct heading prediction is important especially for control in autonomous driving. In Case 3 and 4, it can be found that CO3 helps the CenterPoint detect object with only a few points captured by the LiDAR sensor and meanwhile, PointContrast initialization fails to detect them. This is also essential in autonomous driving because detection failure can sometimes lead to disaster.\n\n5 CONCLUSION AND FUTURE WORK\n\nIn this paper, we propose CO3, namely Cooperative Contrastive Learning and Contextual Shape Prediction, for unsupervised 3D representation learning in outdoor scenes. The recently released vehicle-infrastructure-cooperation dataset DAIR-V2X is utilized to build views for cooperative contrastive learning. Meanwhile the contextual shape prediction objective provides task-relevant information for the 3D encoders. Our experiments demonstrate that the representation learned by CO3 can be transferred to various architectures and different downstream datasets and tasks to achieve performance gain. Currently the size of the real cooperation dataset is relatively small and it will be interesting if larger cooperative datasets can be collected for pre-training in the future.\n\n9\n\n(a) Case 1Pre-trained byCO3Pre-trained by PointContrast(c) Case 3Pre-trained byCO3Pre-trained by PointContrast(b) Case 2Pre-trained byCO3Pre-trained by PointContrast(d) Case 4Pre-trained by CO3Pre-trained by PointContrastPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nPing Luo is supported by the General Research Fund of HK No.27208720, No.17212120, and No.17200622.\n\nREFERENCES\n\nSerge Belongie, Jitendra Malik, and Jan Puzicha. Shape matching and object recognition using shape contexts. IEEE transactions on pattern analysis and machine intelligence, 24(4):509–522, 2002.\n\nHolger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11621–11631, 2020.\n\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Proceedings of the European conference on computer vision (ECCV), pp. 132–149, 2018.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9912–9924. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 70feb62b69f16e0238f741fab228fec2-Paper.pdf.\n\nXiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object detection network for autonomous driving. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1907–1915, 2017.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.\n\nJiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards high performance voxel-based 3d object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 1201–1209, 2021.\n\nLue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang. Embracing single stride 3d object detector with sparse transformer. arXiv preprint arXiv:2112.06375, 2021.\n\nKeinosuke Fukunaga. Introduction to statistical pattern recognition. Elsevier, 2013.\n\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\n\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 580–587, 2014.\n\nBenjamin Graham, Martin Engelcke, and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21271–21284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729–9738, 2020.\n\nJi Hou, Benjamin Graham, Matthias Nießner, and Saining Xie. Exploring data-efficient 3d scene understanding with contrastive scene contexts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15587–15597, 2021.\n\nYuenan Hou, Xinge Zhu, Yuexin Ma, Chen Change Loy, and Yikang Li. Point-to-voxel knowledge distillation for lidar semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 8479–8488, 2022.\n\nPeiyun Hu, Aaron Huang, John Dolan, David Held, and Deva Ramanan. Safe local motion planning In Proceedings of the IEEE/CVF Conference on\n\nwith self-supervised freespace forecasting. Computer Vision and Pattern Recognition, pp. 12732–12741, 2021.\n\nSiyuan Huang, Yichen Xie, Song-Chun Zhu, and Yixin Zhu. Spatio-temporal self-supervised In Proceedings of the IEEE/CVF International\n\nrepresentation learning for 3d point clouds. Conference on Computer Vision, pp. 6535–6545, 2021.\n\nMarcel Körtgen, Gil-Joo Park, Marcin Novotni, and Reinhard Klein. 3d shape matching with 3d shape contexts. In The 7th central European seminar on computer graphics, volume 3, pp. 5–17. Citeseer, 2003.\n\nJ. Kreer. A question of terminology. IRE Transactions on Information Theory, 3(3):208–208, 1957.\n\ndoi: 10.1109/TIT.1957.1057418.\n\nAlex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12697–12705, 2019.\n\nHanxue Liang, Chenhan Jiang, Dapeng Feng, Xin Chen, Hang Xu, Xiaodan Liang, Wei Zhang, Zhenguo Li, and Luc Van Gool. Exploring geometry-aware contrast and clustering harmonization for self-supervised 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3293–3302, 2021.\n\nTsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125, 2017.\n\nYunze Liu, Li Yi, Shanghang Zhang, Qingnan Fan, Thomas Funkhouser, and Hao Dong. P4contrast: Contrastive learning with pairs of point-pixel pairs for rgb-d scene understanding. arXiv preprint arXiv:2012.13089, 2020.\n\nJiageng Mao, Minzhe Niu, Chenhan Jiang, Xiaodan Liang, Yamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Jie Yu, Chunjing Xu, et al. One million scenes for autonomous driving: Once dataset. 2021.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n\nClaude Elwood Shannon. A mathematical theory of communication. ACM SIGMOBILE mobile\n\ncomputing and communications review, 5(1):3–55, 2001.\n\nShaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generation and detection from point cloud. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nShaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020a.\n\nShaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. IEEE transactions on pattern analysis and machine intelligence, 43(8):2647–2664, 2020b.\n\nShaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection. arXiv preprint arXiv:2102.00463, 2021.\n\nHang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pp. 945–953, 2015.\n\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint\n\narXiv:1906.05849, 2019.\n\nYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 6827–6839. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf.\n\nHaoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. Rethinking minimal sufficient representation\n\nin contrastive learning. arXiv preprint arXiv:2203.07004, 2022.\n\nXinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2021.\n\nWikipedia contributors. Mutual information — Wikipedia, the free encyclopedia, 2022. URL https://en.wikipedia.org/w/index.php?title=Mutual_information& oldid=1089343634. [Online; accessed 21-June-2022].\n\nSaining Xie, Sainan Liu, Zeyu Chen, and Zhuowen Tu. Attentional shapecontextnet for point cloud recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4606–4615, 2018.\n\nSaining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. In European Conference on Computer Vision, pp. 574–591. Springer, 2020.\n\nBin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-time 3d object detection from point clouds. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 7652–7660, 2018.\n\nJunbo Yin, Dingfu Zhou, Liangjun Zhang, Jin Fang, Cheng-Zhong Xu, Jianbing Shen, and Wenguan Wang. Proposalcontrast: Unsupervised pre-training for lidar-based 3d object detection. 2022.\n\nTianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11784–11793, 2021.\n\nHaibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang, Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui Yuan, and Zaiqing Nie. Dair-v2x: A large-scale dataset for vehicle-infrastructure cooperative 3d object detection. In IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), June 2022.\n\nYin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4490–4499, 2018.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nXinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin Ma, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical and asymmetrical 3d convolution networks for lidar segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 9939–9948, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA BACKGROUND ABOUT VIEW BUILDING IN CONTRASTIVE LEARNING\n\nIn this section, we discuss how to build proper views in contrastive learning. Firstly, we introduce the formulation of contrastive learning and some important properties of good views in contrastive learning as proposed in (Tian et al., 2020). Then we discuss view building for LiDAR point clouds.\n\nView Building Contrastive Learning. Unsupervised representation learning aims to pre-train 2D/3D backbones on a dataset without labels, which can be transferred to downstream datasets and tasks to achieve performance improvement over training from scratch (random initialization). Recently, unsupervised contrastive learning achieves great success in image domain (He et al., 2020; Tian et al., 2019; Caron et al., 2020; Grill et al., 2020; Wang et al., 2021). Given a batch of images X as inputs, these works first apply two kinds of random augmentations for each image xn ∈ X (n = 1, 2, ..., N where N is the number of images in the batch) to get augmented images xn 2 , which are called different views of xn. The main objective of contrastive learning is to pull together the representations of views of the same image in the feature space while pushing away representations of different images, as indicated in the equation below:\n\n1 and xn\n\nLcon =\n\n1 N\n\nzn 1,2 = f enc\n\nI\n\n(xn\n\nN (cid:88)\n\nn=1\n\n− log(\n\n1 · zn exp(zn i=1 exp(zn 1,2) n = 1, 2, ..., N\n\n(cid:80)N\n\n2 /τ ) 1 · zi\n\n2/τ )\n\n) with\n\n(8)\n\nI\n\nis the 2D backbone used to extract representations. zn where f enc 1,2 is the encoded representations for views xn 1,2. To apply contrastive loss in the first line in Eqn. (8), views of the same image are considered as positive pairs and other pairs are negative ones. The numerator indicates the similarity of positive pairs while the denominator sums up the positive similarity and the sum of similarity of negative pairs. τ is the temperature parameter. Minimizing this loss equals to maximize the similarity of positive pairs and minimize the similarity of negative pairs.\n\n1 ; xn\n\nAuthors in (Tian et al., 2020) discuss what property views should have to benefit contrastive learning via Information Theory and propose that mutual information (Shannon, 2001; Kreer, 1957; Wikipedia contributors, 2022) of views ,I(xn 2 ), can indicate the quality of the learned representations. Mutual information formally quantifies “how much information about one random variable we can obtain when observing the other one”. Experiments on images suggest that there exists a “sweet spot” for I(xn 2 ) where the pre-trained backbone can achieve the most significant performance improvement in downstream tasks. This means the mutual information between views can neither be too low (sharing little semantics) nor too high (differing little). Further experiments in (Tian et al., 2020) indicate that the mutual information of different augmented images is high and reducing I(xn 2 ) by applying stronger augmentations is effective. The performance on downstream tasks increases at the beginning and then decrease when the augmentations are too strong.\n\n1 ; xn\n\n1 ; xn\n\nViews Building for LiDAR Point Clouds. As discussed in the main paper, it is impossible for us to reconstruct the whole outdoor-scene for constrative learning, which is demonstrated useful in indoor-scene (Xie et al., 2020; Hou et al., 2021; Liu et al., 2020). And there exists two alternatives to build views for outdoor-scene LiDAR point clouds. The first one (Liang et al., 2021; Yin et al., 2022) is to apply data augmentation to single frame of point cloud and treat the original and augmented versions as different views, which is similar to what previous works do in image domain. However, the augmentations in image domain are highly non-linear while all the augmentation of point clouds, including random drop, rotation and scaling, can be implemented in a linear transformation. As claimed in (Tian et al., 2020), the highly non-linear augmentations on images already bring high mutual information between views. Thus views of LiDAR point clouds built in this way would have higher mutual information, which is not adequate for learning representations. The second intuitive idea to build views is to utilize point clouds at different timestamps, embraced by (Huang et al., 2021). However, outdoor-scenes are dynamic and the autonomous driving vehicle has no idea about how other objects (cars, pedestrians, etc.) move. Thus observing one view (timestamp t) bring in little information about the other one (timestamp t+10 for example), meaning that I(xn 2 ) can be extremely low and this can be harmful to the learned representations. Due to these limitations, pretrained 3D encoders in (Liang et al., 2021; Huang et al., 2021) cannot achieve noticeable improvement when transferring to datasets collected by different LiDAR sensors. Thus, in this paper, we propose to utilize the vehicle-infrastructure-cooperation dataset (Yu et al., 2022), which capture the same scene from different view-point at the same timestamp, for contrastive representation learning. Views built\n\n1 ; xn\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nwith this dataset neither share too much information (captured from different view-points) nor share too little information (captured at the same time, easy to find correspondence), which is adequate for contrastive learning.\n\nB RECONSTRUCTION OBJECTIVE FOR TASK-RELEVANT INFORMATION\n\nIn this section, we borrow the ideas from (Wang et al., 2022) to explain why pure contrastive learning bring less improvements as shown in Table 3 in the main paper. Firstly, we give the definition of sufficient representation and minimal sufficient representation in contrastive learning. Then, we present analysis on image classification problem as downstream task and we refer readers to (Wang et al., 2022) for other downstream tasks. Finally, we propose our pre-training objective for LiDAR point clouds.\n\n2 , minimal sufficient representation zn\n\n1 contains all the information that is shared by xn\n\nSufficient Representation and Minimal Sufficient Representation. Sufficient Representation zn of xn 1,suf can be used to express common semantics shared by these two views. Among all the sufficient representations for xn 1 . The learned representation in contrastive learning is sufficient and almost minimal. Assuming that Z1,suf is the set of all possible sufficient representations of view xn Definition 1. zn\n\n1,min contains the least information about xn\n\n1 , we can define these two concepts as belows,\n\n2 , which means zn\n\n1 and xn\n\n2 if and only if I(zn\n\n1 is sufficient for xn\n\n1,suf of view xn\n\n2 ) = I(xn\n\n1,suf, xn\n\n1 , xn\n\n2 ).\n\n1,suf\n\nDefinition 2. zn I(zn 1 ), ∀zn\n\n1,suf, xn\n\n1,min ∈ Z1,suf of view xn 1,suf ∈ Z1,suf.\n\n1 is minimal sufficient if and only if I(zn\n\n1,min, xn\n\n1 ) ≤\n\nTheorem. (1) z1,suf provides more information about the downstream task T than z1,min. (2) The upper bound of error rates in downstream tasks using minimal sufficient representations are higher than that of sufficient representations. That is,\n\nI(z1,suf, T ) ≥ I(z1,min, T )\n\nsup{P e\n\nsuf} ≤ sup{P e\n\nmin}\n\n(9)\n\nThis gap stems from the missing task-relevant information in z1,min. To prevent this problem, authors in (Wang et al., 2022) propose to add a reconstruction objective (reconstruct x1 using z1) alongside the contrastive loss to increase I(z1, x1), which indirectly increases I(z1, T |x2) and brings improvement in downstream classification problem over pure contrastive learning.\n\nProof for Image Classification Problem. We denote the downstream classification task as T and the task-relevant information in minimal sufficient representation z1,min can be described as below:\n\nI(z1,suf, T ) = I(z1,min, T ) + [I(x1, T |z1,min) − I(x1, T |z1,suf)]\n\n≥ I(z1,min, T )\n\n(10)\n\nTo begin with, z1,suf and z1,min are sufficient representations and they contain two parts of information: shared information between x1 and x2, and extra information about x1. Thus the mutual information I(z1,suf, T ) can be decomposed into I(z1,min, T ) and [I(x1, T |z1,min) − I(x1, T |z1,suf)], where I(x1, T |z1,min) indicates the information about T we can obtain by observing x1 when z1,min is known. As z1,suf contains more information about x1 than z1,min, the second term is larger than zero and the right-hand-side of the first line in Eqn. (10) is larger than I(z1,min, T ). This indicates that z1,suf contains more task-relevant information than z1,min and thus would have better performance in T . Then we consider using Bayes error rate P e (Fukunaga, 2013), which is the lower-bound of achievable error for the classifier, to analyze performance of z1,suf and z1,min on downstream classification problem. We have\n\nP e suf ≤ 1 − exp[−H(T ) + I(x1, x2, T ) + I(z1,suf, T |x2)] P e min ≤ 1 − exp[−H(T ) + I(x1, x2, T )]\n\n(11)\n\nwhere H(T ) is the entropy of the task. Since 1−exp[−(H(T )+I(x1, x2, T )+I(z1,suf, T |x2)] ≤ 1− exp[−H(T ) + I(x1, x2, T )], the upper bound of Bayes error rate of minimal sufficient representation is larger than that of sufficient representations. This indicates that ideally z1,suf can achieve better performance than z1,min in classification problem.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nContextual Shape Prediction Objective. As it is impossible to reconstruct the whole scene point cloud with point-level or voxel-level representations. We propose an additional pre-training objective to predict distribution of local neighborhood using point/voxel-level representation. We use shape context to describe the local neighborhood distribution of a point/voxel, which has been demonstrated as a useful local distribution descriptor in previous works (Hou et al., 2021; Belongie et al., 2002; Körtgen et al., 2003; Xie et al., 2018). As demonstrated in our ablation study (Table 3 in main paper), this additional pre-training objective bring more significant performance improvement over pre-trained by pure contrastive loss. We also provide python-style code for computing shape context as followings\n\nAlgorithm 1 Implementation of Contextual Shape Computation in Python Style. class Contextual_Shape(object):\n\ndef __init__(self, r1=0.125, r2=2, nbins_xy=2, nbins_zy=2):\n\nself.r1 = r1 self.r2 = r2 self.nbins_xy = nbins_xy self.nbins_zy = nbins_zy self.partitions = nbins_xy * nbins_zy * 2\n\ndef pdist_batch(rel_trans):\n\nD2 = torch.sum(rel_trans.pow(2), 3) return torch.sqrt(D2 + 1e-7)\n\ndef compute_rel_trans_batch(A, B):\n\nreturn A.unsqueeze(1) - B.unsqueeze(2)\n\ndef hash_batch(A, B, seed):\n\nmask = (A >= 0) & (B >= 0) C = torch.zeros_like(A) - 1 C[mask] = A[mask] * seed + B[mask] return C\n\ndef compute_angles_batch(rel_trans):\n\nangles_xy = torch.atan2(rel_trans[:, :, :, 1], rel_trans[:, :, :, 0]) angles_xy = torch.fmod(angles_xy + 2 * math.pi, 2 * math.pi) angles_zy = torch.atan2(rel_trans[:, :, :, 1], rel_trans[:, :, :, 2]) angles_zy = torch.fmod(angles_zy + 2 * math.pi, math.pi) return angles_xy, angles_zy\n\ndef compute_partitions_batch(self, xyz_batch):\n\nrel_trans_batch = ShapeContext.compute_rel_trans_batch(xyz_batch,\n\n# compute angles from different points to the query one angles_xy_batch, angles_zy_batch = ShapeContext.compute_angles_batch(\n\nangles_xy_bins_batch = torch.floor(angles_xy_batch / (2 * math.pi /\n\nangles_zy_bins_batch = torch.floor(angles_zy_batch / (math.pi / self.\n\nxyz_batch)\n\nrel_trans_batch)\n\nself.nbins_xy))\n\nnbins_zy))\n\nangles_bins_batch = ShapeContext.hash_batch(angles_xy_bins_batch,\n\nangles_zy_bins_batch, self.nbins_zy)\n\n# compute distances between different points and the query one distance_matrix_batch = ShapeContext.pdist_batch(rel_trans_batch) dist_bins_batch = torch.zeros_like(angles_bins_batch) - 1 # generate partitions for each points mask_batch = (distance_matrix_batch >= self.r1) & (\n\ndistance_matrix_batch < self.r2)\n\ndist_bins_batch[mask_batch] = 0 mask_batch = distance_matrix_batch >= self.r2 dist_bins_batch[mask_batch] = 1 bins_batch = ShapeContext.hash_batch(dist_bins_batch, angles_bins_batch, self.nbins_xy * self.nbins_zy)\n\nreturn bins_batch\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nC DATASETS DETAILS\n\nIn this section, we introduce details about different datasets used in the main paper for evaluation and also two more datasets in additional experiments.\n\nDAIR-V2X. DAIR-V2X (Yu et al., 2022) is the first real-world autonomous dataset for vehicleinfrastructure-cooperative detection task. It covers various scenes, including cities and highways, and different whether condition including sunny, rainy and foggy days. A virtual world coordinate is used to align the vehicle LiDAR coordinate and infrastructure LiDAR coordinate. There are 38845 LiDAR frames (10084 in vehicle-side and 22325 in infrastructure-side) for cooperative-detection task. The dataset contains around 7000 synchronized cooperative samples in total and we utilize them to pre-train 3D encoder in an unsupervised manner via the proposed CO3. The LiDAR sensor at vehicle-side is 40-beam while a 120-beam LiDAR is utilized at infrastructure-side.\n\nOnce. Once (Mao et al., 2021) is a large-scale autonomous dataset for evaluating self-supervised methods with 1 Million LiDAR frames and only 15k fully annotated frames with 3 classes (Vehicle, Pedestrian, Cyclist). A 40-beam LiDAR is used in (Mao et al., 2021) to collect the point cloud data. We adopt common practice, including point cloud range and voxel size, in their public code repository3. As for the evaluation metrics, IoU thresholds 0.7, 0.3, 0.5 are respectively adopted for vehicle, pedestrian, cyclist. Then 50 score thresholds with the recall rates ranging from 0.02 to 1.00 (step size if 0.02) are computed and the 50 corresponding values are used to draw a PR curve, resulting in the final mAPs (mean accurate precisions) for each category. We also further overage over the three categories and compute an ’Overall’ mAP for evaluations.\n\nKITTI. KITTI (Geiger et al., 2012) is a widely used self-driving dataset, where point clouds are collected by LiDAR with 64 beams. It contains around 7k samples for training and another 7k for evaluation. For point cloud range and voxel size, we adopt common practice in current popular codebase like MMDetection3D4 and OpenPCDet5. All the results are evaluated by mAPs with three difficulty levels: Easy, Moderate and Hard. These three results are further average and an ’Overall’ mAP is generated for comparisons.\n\nD ADDITIONAL EXPERIMENT RESULTS\n\nD.1\n\nINFRASTRUCTURE AS VIEW IN CONTRASTIVE LEARNING\n\nWe also conduct ablation study on the fusion view. Instead of using fusion point clouds as view in contrastive learning, we directly use infrastructure side point cloud as another view. Results are shown in Table 7. It can be found that if we directly use infrastructure-side point cloud for contrastive learning, the performance improvement is very marginal. This stems from the sparsity of LiDAR point clouds, which sometimes make it difficult to find good positive pairs to perform contrastive learning.\n\nInit. Random Inf-view CO3\n\nOverall Vehicle 62.85 55.92 62.71 56.45 64.60 58.50\n\nPedestrian Cyclist 59.39 60.02 62.17\n\n45.52 46.63 48.83\n\nTable 7: Resuls of directly using infrastructure-side point cloud as view in contrastive learning.\n\nE IMPLEMENTATION DETAILS\n\nIn this section, we introduce some details about implementation in both pre-training stage and finetuning stage. Common settings of pre-training and fine-tuning are listed in Table 8 and we discuss other settings that vary in different detectors later.\n\n3https://github.com/PointsCoder/Once_Benchmark 4https://github.com/open-mmlab/mmdetection3d 5https://github.com/open-mmlab/OpenPCDet\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nConfiguration optimizer base learning rate weight decay batch size learning rate schedule GPU numbers training epochs\n\nPre-training KITTI Once Adam Adam 0.003 0.003 0.01 0.01 -\n- cyclic cyclic 4\n4 80 80\n\nAdamW 0.0001 0.01 16 cyclic 8\n20\n\nTable 8: Details about implementations. “Pre-training” means settings in DAIR-V2X (Yu et al., 2022). “KITTI” and “Once” respectively indicate settings for detection tasks in KITTI (Geiger et al., 2012) and Once (Mao et al., 2021). We list all common settings and discuss those vary in different detectors below, which are marked as “-” in this table.\n\nOther Pre-training Settings. To accelerate the pre-training process, we utilize the “repeated dataset” in MMDetection3D and the schedule is set to 10 epochs, which equals to 20 epochs without “repeated dataset”. Thus the number 20 for training epochs in Table 8 is tilt.\n\nOther Fine-tuning Settings. Batch size settings for different detectors on different datasets are shown in Table 9. We use different types of GPUs, different number of GPUs and different version of PyTorch (Paszke et al., 2019) as compared to those used in the codebases, which may lead to degrading when training from scratch. Thus these parameters are tuned based on the original settings from the codebases to make the performance of training from scratch match or even surpass the results they published.\n\nDetectors Second PV-RCNN CenterPoint\n\nKITTI Once\n\n48 16 -\n\n48 48 32\n\nTable 9: Details about batchsize settings for different detectors on different datasets. “-” means there is no configuration for the detector on the exact dataset in the codebase or we do not conduct the downstream experiments.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nF TABLE OF NOTATION\n\nVariable Description Pv/f ˆPv/f\n\nRaw vehicle/fusion LiDAR points. Point/Voxel-level features after embedded by the 3D backbone. Projected point/voxel-level features in the common feature space.\n\nˆZv/f\n\nZv/f\n\nN1 τ\n\nQ′\n\nQ\n\nP\n\nN2\n\nNormalized point/voxel-level features in the common feature space.\n\nThe number of samples for contrastive learning. Temperature parameter in contrastive learning. A matrix in dimension RNf×Nbin indicating the number of points in each bin in the neighborhood of each fusion point. Apply l2-normalization and softmax to Q′ then we get Q ∈ RNf×Nbin , which describes a local geometry distribution in the neighborhood of each fusion point.\n\nPredicted local geometry distribution from Penc v/f .\n\nThe number of samples for contextual shape prediction.\n\nNote\n\nIndicating same process applied to vehicle/fusion point cloud.\n\nAn L-2 normalization is applied on the feature dimension of ˆZv/f. zn v/f is sampled point/voxel feature for contrastive learning.\n\nqn v/f is sampled “ground truth” distribution from Q\n\nv/f is sampled distribution prediction\n\npn from P.\n\nTable 10: Detailed description of variables\n\nG DISCUSSION ABOUT THE ABLATION STUDY\n\nFigure 6: Example samples of pedestrian and cyclist. LiDAR points are shown in red. It can be found that pedestrian category has consistent shape while the shape of cyclist varies across time and identities.\n\nIt can be found in Table 6 that using contextual shape prediction loss only brings more improvement on Pedestrian class while cooperative contrastive loss introduce more gains on the Cyclist class. In this section, we provide a discussion on this phenomenon.\n\nFirst of all, the \"ground truth\" shape context is computed with point clouds and the contextual shape prediction goal is to predict the local point distribution with voxel-level representations. This\n\n19\n\n(a) Different Cases of Pedestrian Class(b) Different Cases of Cyclist ClassPublished as a conference paper at ICLR 2023\n\nenables the voxel-level representation to predict the structure inside a voxel. Meanwhile, cooperative contrastive only focus on contrast different voxel representation. For pedestrian class, there are usually only one or two voxels for one pedestrian. Thus, with cooperative contrastive loss only, the representation fail to recognize the inner structure of the voxel and thus cannot improve upon the pedestrian category. On the contrary, when we pre-train the encoder with contextual shape prediction loss only, the learned representation is able to express the structure inside the voxel and this helps with the downstream detection on the pedestrian category.\n\nSecond, as the contextual shape prediction goal aims to capture local shape distribution, it fails to learn good representation with varying shape. Compared to pedestrians whose shape is always cylinder-like, cyclists with their bicycles are usually captured with different poses, leading to different shapes. A visualization of different cases of the two categories are provided in Fig. 6. Thus the representation fails to learn knowledge when predicting varying shape of the same semantic. When we look at the cyclist category, it can be found that pre-training with contextual shape loss only brings little improvement.\n\nBesides, as discussed in Appendix B, the two pre-training objectives are complementary. Pure cooperative contrastive learning makes the representation minimal sufficient, which lacks of taskrelevant information. The contextual shape prediction loss brings more task-relevant information by increasing the mutual information between the representations and the inputs. Thus combining them leads to better performance.\n\nH MORE EXPERIMENTS.\n\nH.1 ABLATION EXPERIMENTS ON SHAPE CONTEXT.\n\nWe conduct a ablation experiment on the contextual shape prediction loss. Previous works (Belongie et al., 2002; Körtgen et al., 2003) use the number of points in each bin as the feature of the query point. However, we think it is difficult for the network to directly regress the exact number of points in the bins. Thus we propose to construction a local distribution and use the representation to predict it. In this part, we conduct ablation study on this design. We use the exact number of points as prediction goal and keep other parts of our method the same. The experiment results are shown in Table 11 and it can be found that directly transfer the idea of shape context only brings little improvement (0.94mAP) as compared to the proposed CO3 (2.58 mAP). Thus our insight can also be transferred to indoor scene point clouds.\n\nInitialization Random CO3 with exact number of points prediction CO3\n\nOverall mAP Vehicle 62.85 62.79 64.60\n\n55.92 56.86 (+0.94) 58.50 (+2.58)\n\nPedestrian Cyclist 59.39 60.15 62.17\n\n45.52 47.63 48.83\n\nTable 11: Ablation experiments on shape context.\n\nH.2 EXPERIMENTS COMPARED TO SAFETY SPACE FORECASTING.\n\nWe also conduct experiments where freespace forecasting (Hu et al., 2021) is used as pre-training goal. The pre-training setting is kept the same as all the other initialization methods. The results are shown in Table 12. It can be found that pre-training with safety space forecasting (Hu et al., 2021) brings minor performance gain. We think this might stems from the objective of the pre-training goal. The pre-training goal of (Hu et al., 2021) is to predict freespace for safe driving and their downstream task is motion planning. However, 3D perception task requires semantic information and simply predicting freespace does not help to distinguish the representation of different objects. This is why pre-training with freespace forecasting bring lower performance improvement on 3D perception task.\n\nH.3 PARAMETER SENSITIVITY EXPERIMENTS\n\nIn this part, we conduct parameter sensitivity experiments on temperature parameter and radius in contextual shape prediction loss.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nInitialization From Scratch Freespace forecasting CO3\n\nOverall mAP Vehicle 62.85 63.33 64.60\n\n55.92 56.18 (+0.26) 58.50 (+2.58)\n\nPedestrian Cyclist 59.39 59.01 62.17\n\n45.52 46.19 48.83\n\nTable 12: Experiment results on freespace forecasting pre-training.\n\nWe first fix all the other parameters in the main paper and change R1 and R2 in the contextual shape prediction loss. We pre-train the 3D encoder on the DAIR-V2X dataset and downstream it to Once dataset with CenterPoint as detector. Results are shown in Table 13. It can be found that for the same R2, an increasing R1 brings more improvement on the downstream detection tasks. This might stem from that a larger inner radius can help capture more neighborhood information, making the contextual shape prediction goal more meaningful. It can also be found that when R2 is relatively small, for example R2=3.5m and R1=1m, the performance drops. This might stem from the relatively small R2 which might fail to capture the local shape distribution of a larger object like cars or trucks. Also, we did not search these parameters before and we surprisingly find that using R1=1.5m and R2=4m brings the best performance (58.86 mAP).\n\nR1/m\n\nR2/m\n\n3.5 4.0\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\n57.55 57.53\n\n57.31 58.50\n\n58.00 58.86\n\n58.42 58.48\n\nTable 13: Results on parameter sensitivity experiments on temperature.\n\nIn the experiment on temperature, all other parameters are kept the same as those in the main experiment and we only change the temperature in the cooperative contrastive loss. We pre-train the 3D backbone on DAIR-V2X dataset with different parameter settings and fine-tune it on Once dataset with CenterPoint. The results are shown in Table 14. It can be found that as the temperature increases to 0.15, the downstream performance drop to 57.34 mAP. This is because a higher temperature brings smaller gap between negative pairs and this makes the representations hard to separate different objects. Also, as we keep decreasing the temperature, the overall downstream performance drop to 57.74 mAP. This is because too small temperature push representation of different objects too far away but ignore the similar semantic meaning for the same category. For example, it will make representation of two different cars far away and this will harm the performance of downstream detection task. It can also be found that using adequate temperature brings comparable performance (58.50 mAP vs 58.10 mAP).\n\nTemperature Overall mAP\n\n0.02 57.74\n\n0.07 58.50\n\n0.1 58.10\n\n0.15 57.34\n\nTable 14: Results on parameter sensitivity experiments on radius in contextual shape prediction loss.\n\nI DISCUSSION ON THE INFLUENCE TO V2X COMMUNITY.\n\nThe V2X community is developing rapidly and the original motivation of V2X setting is to alleviate occlusion and long-range sensing problems (Yu et al., 2022). There are several V2X settings including vehicle-to-infrastructure and vehicle-to-vehicle.\n\nIn this paper, we propose to use cooperation dataset for 3D unsupervised representation learning and achieve performance improvement on perception task using vehicle-side point clouds only. Although our experiments are conducted on vehicle-infrastructure dataset, our method can also be applied on other V2X settings without any label. As labeling is the most intensive and time-consuming part in the collection of cooperation dataset, we believe larger scale of unlabeled cooperation dataset will be collected in the future for unsupervised 3D representation learning to introduce more performance gain. Also, it is expensive and difficult to deploy V2X settings everywhere. In our work, we pretrain on such cooperation dataset and achieve improvements on downstream tasks with vehicle-side\n\n21\n\nPublished as a conference paper at ICLR 2023\n\npoint clouds as inputs, which is a new exciting finding about the V2X research. We believe the promising results will encourage more attempts in cooperative unsupervised representation learning and accelerate the development of the V2X community.\n\n22",
    "reference": "# Summary Of The Paper\n\nThis paper focuses on self-supervised point cloud representation learning. The authors combine the contrastive loss with KL divergence between the predicted feature and manually crafted 3D shape context. \n\nTo construct better views for contrastive loss,  the authors utilize a recently proposed dataset where point clouds are captured from both vehicle sensors and infrastructure sensors. \n\nTo demonstrate the effectiveness of the proposed methods, the authors finetuned the pre-trained model on  3D object detection and segmentation tasks and have achieved better performance than other self-supervised learning methods or even supervised pre-trained models.\n\n# Strength And Weaknesses\n\nStrength\n+ The motivation of construct views from infrastructure sensors and vehicle sensors is well introduced.\n+ The performance gain is significant on various downstream benchmarks.\n+ The evaluation is extensive.\n\nWeakness:\n+ The paper converts the view construction of outdoor scene to a case similar to indoor by using the new dataset. So the proposed solution should also works for indoor, but which is not demonstrated in the paper.\n+ The novelty and the insight of the method itself is limited.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written. The novelty of the method itself is limited.\nHowever, the reproducibility is high since most of the components are off-the-shelf.\n\n# Summary Of The Review\n\nAlthough the performance gain is significant, I prefer to reject the paper because it resolves the (positive) view construction of 3D point clouds by using an existing dataset, and the novelty and insights of the method itself are insufficient.\n\n[post-rebuttal] I appreciate the efforts the authors did on proving the effectiveness of the proposed method on indoor data. Although I maintain my original opinion that the paper is more about construct data on a very specific dataset and is less inspiring for further researches in general case, I updated the rating due to the additional effort the authors have made.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nMODELING SEQUENTIAL SENTENCE RELATION TO IMPROVE CROSS-LINGUAL DENSE RETRIEVAL\n\nShunyu Zhang1,∗, Yaobo Liang1, Ming Gong2, Daxin Jiang2, Nan Duan1 1Microsoft Research Asia, 2Microsoft STC Asia shunyuzh@foxmail.com, {yalia, migon, djiang, nanduan}@microsoft.com\n\nABSTRACT\n\nRecently multi-lingual pre-trained language models (PLM) such as mBERT and XLM-R have achieved impressive strides in cross-lingual dense retrieval. Despite its successes, they are general-purpose PLM while the multilingual PLM tailored for cross-lingual retrieval is still unexplored. Motivated by an observation that the sentences in parallel documents are approximately in the same order, which is universal across languages, we propose to model this sequential sentence relation to facilitate cross-lingual representation learning. Specifically, we propose a multilingual PLM called masked sentence model (MSM), which consists of a sentence encoder to generate the sentence representations, and a document encoder applied to a sequence of sentence vectors from a document. The document encoder is shared for all languages to model the universal sequential sentence relation across languages. To train the model, we propose a masked sentence prediction task, which masks and predicts the sentence vector via a hierarchical contrastive loss with sampled negatives. Comprehensive experiments on four cross-lingual retrieval tasks show MSM significantly outperforms existing advanced pre-training models, demonstrating the effectiveness and stronger cross-lingual retrieval capabilities of our approach.\n\n1\n\nINTRODUCTION\n\nCross-lingual retrieval (also including multi-lingual retrieval) is becoming increasingly important as new texts in different languages are being generated every day, and people query and search for the relevant documents in different languages (Zhang et al., 2021b; Asai et al., 2021a). This is a fundamental and challenging task and plays an essential part in real-world search engines, for example, Google and Bing search which serve hundreds of countries across diverse languages. In addition, it’s also a vital component to solve many cross-lingual downstream problems, such as open-domain question answering (Asai et al., 2021a) or fact checking (Huang et al., 2022).\n\nWith the rapid development of deep neural models, cross-lingual retrieval has progressed from translation-based methods (Nie, 2010), cross-lingual word embeddings (Sun & Duh, 2020), and now to dense retrieval built on the top of multi-lingual pre-trained models (Devlin et al., 2019; Conneau et al., 2019). Dense retrieval models usually adopt pretrained models to encode queries and passages into low-dimensional vectors, so its performance relies on the representation quality of pretrained models, and for multilingual retrieval it also calls for cross-lingual capabilities.\n\nModels like mBERT (Devlin et al., 2019), XLMR (Conneau et al., 2019) pre-trained with masked language model task on large multilingual corpora, have been applied widely in cross-lingual retrieval (Asai et al., 2021a;b; Shi et al., 2021) and achieved promising performance improvements. However, they are general pre-trained models and not tailored for dense retrieval. Except for the direct application, there are some pre-trained methods tailored for monolingual retrieval. Lee et al. (2019) and Gao & Callan (2021) propose to perform contrastive learning with synthetic querydocument pairs to pre-train the retriever. They generate pseudo pairs either by selecting a sentence and its context or by cropping two sentences in a document. Although showing improvements, these approaches have only been applied in monolingual retrieval and the generated pairs by hand-crafted\n\n∗Work done during internship at Microsoft Research Asia.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nrules may be low-quality and noisy. In addition, learning universal sentence representations across languages is more challenging and crucial than monolingual, so better multilingual pre-training for retrieval needs to be explored.\n\nIn this paper, we propose a multilingual PLM to leverage sequential sentence relation across languages to improve cross-lingual retrieval. We start from an observation that the parallel documents should each contain approximately the same sentence-level information. Specifically, the sentences in parallel documents are approximately in the same order, while the words in parallel sentences are usually not. It means the sequential relation at sentence-level are similar and universal across languages. This idea has been adopted for document alignment (Thompson & Koehn, 2020; Resnik, 1998) which incorporates the order information of sentences. Motivated by it, we propose a novel Masked Sentence Encoder (MSM) to learn this universal relation and facilitate the isomorphic sentence embeddings for cross-lingual retrieval. It consists of a sentence encoder to generate sentence representations, and a document encoder applied to a sequence of sentences in a document. The document encoder is shared for all languages and can learn the sequential sentence relation that is universal across languages. In order to train MSM, we adopt a sentence-level masked prediction task, which masks the selected sentence vector and predicts it using the output of the document encoder. Distinct from MLM predicting tokens from pre-built vocabulary, we propose a hierarchical contrastive loss with sampled negatives for sentence-level prediction.\n\nWe conduct comprehensive experiments on 4 cross-lingual dense retrieval tasks including Mr. TyDi, XOR Retrieve, Mewsli-X and LAReQA. Experimental results show that our approach achieves state-of-the-art retrieval performance compared to other advanced models, which validates the effectiveness of our MSM model in cross-lingual retrieval. Our in-depth analysis demonstrates that the cross-lingual transfer ability emerges for MSM can learn the universal sentence relation across languages, which is beneficial for cross-lingual retrieval. Furthermore, we perform ablations to motivate our design choices and show MSM works better than other counterparts.\n\n2 RELATED WORK\n\nMulti-lingual Pre-trained Models. Recently the multilingual pre-trained models (Lample & Conneau, 2019; Conneau et al., 2019; Huang et al., 2019) have empowered great success in different multilingual tasks (Liang et al., 2020; Hu et al., 2020). Multilingual BERT (Devlin et al., 2019) is a transformer model pre-trained on Wikipedia using the multi-lingual masked language model (MMLM) task. XLM-R (Conneau et al., 2019) further extends the corpus to a magnitude more web data with MMLM. XLM (Lample & Conneau, 2019) proposes the translation language model (TLM) task to achieve cross-lingual token alignment. Unicoder (Huang et al., 2019) presents several pre-training tasks upon parallel corpora and ERNIE-M (Ouyang et al., 2021) learns semantic alignment by leveraging back translation. XLM-K (Jiang et al., 2022) leverages the multi-lingual knowledge base to improve cross-lingual performance on knowledge-related tasks. InfoXLM (Chi et al., 2021) and HiCTL (Wei et al., 2020) encourage bilingual alignment via InfoNCE based contrastive loss. These models usually focus on cross-lingual alignment leveraging bilingual data, while it’s not fit for cross-lingual retrieval that calls for semantic relevance between query and passage. There is few explore on how to improve pre-training tailored for cross-lingual retrieval, which is exactly what our model addresses.\n\nCross-lingual Retrieval. Cross-lingual (including multi-lingual) retrieval is becoming increasingly important in the community and impacting our lives in real-world applications. In the past, multilingual retrieval relied on community-wide datasets at TREC, CLEF, and NCTIR, such as CLEF 2000-2003 collection (Ferro & Silvello, 2015). They usually comprise a small number of queries (at most a few dozen) with relevance judgments and only for evaluation, which are insufficient for dense retrieval. Recently, more large scale cross-lingual retrieval datasets (Zhang et al., 2021b; Ruder et al., 2021) have been proposed to promote cross-lingual retrieval research, such as Mr. TyDi (Asai et al., 2021a) proposed in open-QA domain, Mewsli-X (Ruder et al., 2021) for news entity retrieval, etc.\n\nThe technique of the cross-lingual retrieval field has progressed from translation-based methods (Nie, 2010; Shi et al., 2021) to cross-lingual word embeddings by neural models (Sun & Duh, 2020), and now to dense retrieval built on the top of multi-lingual pre-trained models (Devlin et al., 2019; Conneau et al., 2019). Asai et al. (2021a;b) modify the bi-encoder retriever to be equipped with mBERT, which plays an essential part in the open-QA system, and Zhang et al. (2022b) explore\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: The general framework of masked sentence model (MSM), which has a hierarchical model architecture including the sentence encoder and the document encoder. The masked sentence prediction task predicts the masked sentence vector pt, given the original vector ht as the positive anchor, via a hierarchical contrastive loss.\n\nthe impact of data and model. However, most of the existing work focuses on fine-tuning a specific task, while ours targets pre-training and conducts evaluations on diverse benchmarks. There also exist some similarity-specialized multi-lingual models (Litschko et al., 2021), trained with parallel or labeled data supervision. LASER (Artetxe & Schwenk, 2019) train a seq2seq model on largescale parallel data and LaBSE (Feng et al., 2022) encourage bilingual alignment via contrastive loss. m-USE (Yang et al., 2019) is trained with mined QA pairs, translation pairs and SNLI corpus. Some others also utilize distillation (Reimers & Gurevych, 2020; Li et al., 2021), adapter (Pfeiffer et al., 2020; Litschko et al., 2022), siamese learning (Zhang et al., 2021c). Compared to them, MSM is unsupervised without any parallel data, which is more simple and effective (Artetxe et al., 2020b), and can also be continually trained with these bi-lingual tasks.\n\nDense Retrieval. Dense retrieval (Karpukhin et al., 2020; Lee et al., 2019; Xiong et al., 2020; Zhang et al., 2022a) (usually monolingual here) typically utilizes bi-encoder model to encode queries and passages into low-dimensional representations. Recently there have been several directions explored in the pre-training tailored for dense retrieval: utilizing the hyperlinks between the Wikipedia pages (Ma et al., 2021; Zhou et al., 2022), synthesizing query-passage datasets for pre-training (O ̆guz et al., 2021; Reddy et al., 2021), and auto-encoder-based models that force the better representations (Lu et al., 2021; Ma et al., 2022). Among them, there is a popular direction that leverage the correlation of intra-document text pairs for the pre-training. Lee et al. (2019) and Chang et al. (2020) propose Inverse Close Task (ICT) to treat a sentence as pseudo-query and the concatenated context as the pseudo-passage for contrastive pre-training. Another way is cropping two sentence spans (we call them CROP in short) from a document as positive pairs (Giorgi et al., 2021; Izacard et al., 2021a), including Wu et al. (2022a); Iter et al. (2020) that use two sentences and Gao & Callan (2021) that adopts two non-overlapping spans. The most relevant to ours are ICT and CROP, which generate two views of a document for contrastive learning. However, the correlation of the pseudo pair is coarse-granular and even not guaranteed. In contrast, ours utilizes a sequence of sentences in a document and models the universal sentence relation across languages via an explicit document encoder, resulting in better representation for cross-lingual retrieval.\n\n3 METHODOLOGY\n\n3.1 HIERARCHICAL MODEL ARCHITECTURE\n\nIn this section, we first present the hierarchical model architecture. As illustrated in Figure.1, our Masked Sentence Encoder (MSM) has a hierarchical architecture that contains the Sentence Encode and the Document Encoder. The document encoder is applied to the sentence vectors generated by the sentence encoder from a sequence of sentences in a document.\n\nSentence Encoder. Given a document containing a sequence of sentences D = (S1, S2, ..., SN ) in which Si denote a sentence in document, and each sentence contains a list of words. As shown\n\n3\n\nDocument EncoderSentenceEncoderSentthtSentenceEncoderSentt+1ht+1ht+1SentenceEncoderSentt−1ht−1ht−1[mask]ptpt+1pt−1HierarchicalContrastive LossProjection Layerht±kIhCIntra-Doc NegativesProjection LayerCross-Doc NegativesSentenceEncoderSentt+2ht+2ht+2pt+2Published as a conference paper at ICLR 2023\n\nin 1, the sentence encoder extracts the sentence representations for the sequence in a document, and the document encoder is to model the sentence relation and predict the masked sentences. First, we adopt a transformer-based encoder as our sentence encoder. Then as usual the sentence is passed through the embedding layer and transformer layers, and we take the last hidden state of the CLS token as the sentence representation. Note that all the sentence encoders share the parameters and we can get N sentence vectors DH = (h1, h2, ..., hN ) respectively. In this task, we just encoder the complete sentences without the mask to get thorough sentence representations.\n\nDocument Encoder. Then the sentence-level vectors run through the document encoder, which has similar transformer-based architecture. Considering the sentences have sequential order in a document, the sentence position is also taken into account and it doesn’t have token type embedding. After equipped with sentence position embeddings, we encode them through document encoder layers to get document-level context aware embeddings DP = (p1, p2, ..., pN ). In order to train our model, we apply the sentence-level mask to the sentence vectors for our masked sentence prediction task. Specifically, DH = (h1, h2, ..., hN ) are the original sentence vectors, and we mask selected sentence vector ht to [mask] token and keep other the original ones. The original sentence vector ht is seen as the positive anchor for the output vector pt of document encoder corresponding to [mask]. Considering a document that contains N sentences, we mask each sentence in turn and keep the others the original, to get N pairs of pt and ht. It is effective to get as many samples as possible at the same time for efficient training. Since the length of document encoder’s input is not long (for the number of sentences in a document is not long) and our document encoder is also shallow, it makes our approach efficient without much computation.\n\nThere are some models also adopting a hierarchical transformer-based model (Santra et al., 2021). For example, HiBERT (Zhang et al., 2019) uses a multi-level transformers for document summarization, while it applies the mask to the words with a decoder for autoregressive pre-training. Poolingformer (Zhang et al., 2021a) proposes a two-level pooling attention schema for long document but can’t be applied for retrieval. They mainly adopt token-level MLM and targets document understanding, while ours focuses on masked sentence prediction and is directed at cross-lingual retrieval.\n\n3.2 MASKED SENTENCE PREDICTION TASK\n\nIn order to model the sentence relation, we propose a masked sentence prediction task that aligns masked sentence vectors pt with corresponding original ht via the hierarchical contrastive loss. Distinct from Masked Language Model which can directly compute cross-entropy loss between masked tokens and pre-built vocabulary, our model lacks a sentence-level vocabulary. Here we propose a novel hierarchical contrastive loss on sentence vectors to address it. Contrastive learning has been shown effective in sentence representation learning (Karpukhin et al., 2020; Gao et al., 2021), and our model modifies the typical InfoNCE loss (Oord et al., 2018) to a hierarchical contrastive loss for the masked sentence prediction task. As shown in Figure.1, for masked sentence vectors pt, the positive anchor is original ht and we collect two categories of negatives: (a) Cross-Doc Negatives are the sentence vectors from different documents, i.e. hC k, which can be seen as random negatives as usual. (b) Intra-Doc Negatives are the sentence vectors in a same document generated by sentence encoder, i.e. hI j , j̸=t. Then the masked sentence vectors pt with them are passed through the projection layer, and the output vectors are involved in the hierarchical contrastive loss as:\n\nLmsm(pt, {hI\n\nt , hI\n\n1 , . . . , hI\n\n= − log\n\nesim(pt,hI\n\nt )− + (cid:80)|I|\n\n|I|, hC\n\n1 , . . . , hC\n\n|C|}) t ) esim(pt,hI j=1,j̸=t esim(pt,hI\n\nj )−μα+ (cid:80)|C|\n\nk) k=1 esim(pt,hC\n\n(1)\n\nIn the previous study (Gao & Callan, 2021), two sampled views or sentences of the same document are often seen as a positive pair to leverage their correlation. However, it limits the representation capability for it encourages the alignment between two views, just as a coarse-grained topic model (Yan et al., 2013). In contrast, we treat them as Intra-Doc Negatives, which could help the model to distinguish sentences from the same document to learn fine-grained representations. The intra-doc samples usually have closer semantic relation than cross-doc ones and directly treating them as negatives could hurt the uniformity of embedding space. To prevent this negative impact,\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nwe set the dynamic bias subtracted from their similarity scores. As seen in Eq.1, the dynamic bias is −μα in which μ is a hyper-parameter and α is computed as:\n\nα =\n\n(cid:32) (cid:80)|I|\n\nj=1,j̸=t sim (cid:0)pt, hI |I| − 1\n\nj\n\n(cid:1)\n\n−\n\n(cid:80)|C|\n\nk=1 sim (cid:0)pt, hC |C|\n\nk\n\n(cid:33)\n\n(cid:1)\n\n.detach()\n\n(2)\n\nIt represents the gap between the average similarity score of Intra-Doc Negatives and them from Cross-Doc Negatives. Subtracting the dynamic bias can tune the high similarity of intra-doc negatives to the level of cross-doc negatives, which can also be seen as interpolation to generate soft samples. Note that we only use the value but do not pass the gradient, so we adopt the detach function after computation. Our experimental result in Sec.5.4 validates that the hierarchical contrastive loss is beneficial for representation learning in our model.\n\nConsidering the expensive cost of pre-training from scratch, we initialize the sentence encoder with pre-trained XLM-R weight and solely the document encoder from scratch. To prevent gradient back propagated from the randomly initialized document encoder from damaging sentence encoder weight, we adopt MLM task to impose a semantic constraint. Therefore our total loss consists of a token-level MLM loss and a sentence-level contrastive loss:\n\nL = Lmsm + Lmlm\n\n(3)\n\nAfter pre-training, we discard the document encoder and leave the sentence encoder for fine-tuning. In fact, the document encoder in our MSM plays as a bottleneck (Li et al., 2020): the sentence encoder press the sentence semantics into sentence vectors, and the document encoder leverage the limited information to predict the masked sentence vector, thus enforcing an information bottleneck on the sentence encoder for better representations. It also coincides with the recent works utilizing similar bottleneck theory for better text encoders (Lu et al., 2021; Liu & Shao, 2022). By the way, the sentence encoder has the same architecture as XLMR, which ensures a fair comparison.\n\n4 EXPERIMENTS SETUP\n\n4.1 EVALUATION DATASETS\n\nWe evaluate our model with other counterparts on 4 popular datasets: Mr. TyDi is for query-passage retrieval, XOR Retrieve is cross-lingual retrieval for open-domain QA, Mewsli-X and LAReQA are for language-agnostic retrieval. Mr. TyDi (Zhang et al., 2021b) aims to evaluate cross-lingual passage retrieval with dense representations. Given a question in language L, the model should retrieve relevant texts in language L that can respond to the query. XOR Retrieve (Asai et al., 2021a) is proposed for multilingual open-domain QA, and we take its sub-task XOR-Retrieve: given a question in L (e.g., Korean), the task is to retrieve English passages that can answer the query. Mewsli-X is built on top of Mewsli (Botha et al., 2020) and we follow the setting of XTREMER (Ruder et al., 2021), in which it consisting of 15K mentions in 11 languages. LAReQA (Roy et al., 2020) is a retrieval task that each query has target answers in multiple languages, and models require retrieving all correct answers regardless of language. More details refer to Appendix A.1.\n\n4.2\n\nIMPLEMENTATION DETAILS\n\nFor the pre-training stage, we adopt transformer-based sentence encoder initialized from the XLMR weight, and a 2-layers transformer document encoder trained from scratch. We use a learning rate of 4e-5 and Adam optimizer with a linear warm-up. Following Wenzek et al. (2019), we collect a clean version of Common Crawl (CC) including a 2,500GB multi-lingual corpus covering 108 languages, which adopt the same pre-processing method as XLMR (Conneau et al., 2019). Note that we only train on CC without any bilingual parallel data, in an unsupervised manner. To limit the memory consumption during training, we limit the length of each sentence to 64 words (longer parts are truncated) and split documents with more than 32 sentences into smaller with each containing at most 32 sentences. The rest settings mainly follow the original XLMR’s in FairSeq. We conduct pre-training on 8 A100 GPUs for about 200k steps.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Retrieval performance comparison on four benchmark datasets. The best performing models are marked bold and the results unavailable are left blank. * means the results are borrowed from published papers: † from Zhang et al. (2022b), ‡ from Asai et al. (2021a), § from Ruder et al. (2021), while others are evaluated using the same pipeline by us for a fair comparison.\n\nModel\n\nMr. TyDi\n\nXOR Retrieve\n\nMewsli-X\n\nLAReQA\n\nMRR@100\n\nR@100\n\nR@2k\n\nR@5k\n\nmAP@20\n\nmAP@20\n\nCross-lingual zero-shot transfer (models are fine-tuned on English data)\n\nmBERT* mBERT XLMR MSM\n\n34.4† 33.0 37.7 44.7\n\n73.4† 69.7 72.7 78.6\n\n- 31.6 30.6 34.9\n\n- 42.3 39.3 44.7\n\n38.6‡ 39.0 39.7 41.5\n\n21.6‡ 24.5 29.3 33.5\n\nMulti-lingual fine-tune (models are fine-tuned on the multi-lingual data if available)\n\nmBERT* mBERT XLMR MSM\n\n59.1† 57.6 58.5 60.5\n\n87.1† 87.9 87.8 89.0\n\n38.8§ 48.3 45.1 48.6\n\n48.0§ 57.0 53.9 57.3\n\n- -\n- -\n\n- -\n- -\n\nFor the fine-tuning stage, we mainly follow the hyper-parameters of the original paper for the Mr. TyDi and XOR Retrieve tasks separately. And for Mewsli-X and LAReQA tasks, we mainly follow the settings of XTREME-R using its open-source codebase. Note that we didn’t tune the hyper-parameters and mainly adopted the original settings using the same pipeline for a fair comparison. More details of fine-tuning hyper-parameters refer to Appendix.A.1.\n\n5 EXPERIMENT RESULTS\n\n5.1 EVALUATION SETTINGS\n\nCross-lingual Zero-shot Transfer. This setting is most widely adopted for the evaluation of multilingual scenarios with English as the source language, as many tasks only have labeled train data available in English. Concretely, the models are fine-tuned on English labeled data and then evaluated on the test data in the target languages. It also facilitates evaluation as models only need to be trained once and can be evaluated on all other languages. For Mr. TyDi dataset, the original paper adopt the Natural Questions data (Kwiatkowski et al., 2019) for fine-tuning while later Zhang et al. (2022b) suggests fine-tuning on MS MARCO for better results, so we fine-tune on MARCO when compared with best-reported results and on NQ otherwise. For XOR Retrieve, we fine-tune on NQ dataset as the original paper (Asai et al., 2021a). For Mewsli-X and LAReQA, we follow the settings in XTREME-R, where Mewsli-X on a predefined set of English-only mention-entity pairs and LAReQA on the English QA pairs from SQuAD v1.1 train set.\n\nMulti-lingual Fine-tune. For the tasks where multi-lingual training data is available, we additionally compare the performance when jointly trained on the combined training data of all languages. Following the setting of Mr. TyDi and XOR Retrieve, we pre–fine-tune models as in Cross-lingual Zero-shot Transfer and then fine-tune on multi-lingual data if available. For the Mewsli-X and LAReQA, there is no available multi-lingual labeled data.\n\n5.2 MAIN RESULTS\n\nIn this section, we evaluate our model on diverse cross-lingual and multi-lingual retrieval tasks and compare it with other strong pre-training baselines. Multilingual BERT (Devlin et al., 2019) pre-trained on multilingual Wikipedia using MLM and XLMR (Conneau et al., 2019) extend to a magnitude more web data for 100 languages.\n\nWe report the main results in Table.1, and provide more detailed results in Appendix A.2. The effectiveness of our multilingual pre-training method appears clearly as MSM achieves significantly better performance than its counterparts. (1) No matter whether in the cross-lingual zero-shot transfer or multi-lingual fine-tune settings, the performance trend is consistent and MSM achieves impressive\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Performance comparison on Mr. TyDi across languages in the cross-lingual zero-shot transfer setting, where all the models are fine-tuned on MS MARCO data. - means it doesn’t support BN and TE languages and the average is for the supported languages.\n\nMethod\n\nMetrics\n\nAR\n\nBN\n\nEN\n\nFI\n\nID\n\nJA\n\nKO\n\nRU\n\nSW TE\n\nTH\n\nAVG\n\nSimilarity-specialized multi-lingual encoders (with parallel data or labeled data supervision)\n\nDistilmBERT\n\nInfoXLM\n\nLaBSE\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\n40.8 79.7\n\n48.2 81.2\n\n50.1 83.0\n\n- -\n\n50.6 83.8\n\n52.3 85.6\n\n29.9 71.0\n\n30.1 72.2\n\n29.7 71.4\n\n26.7 64.1\n\n29.0 65.8\n\n41.3 80.2\n\n39.7 79.7\n\n39.9 75.9\n\n48.3 86.1\n\nUnsupervised pre-train with multi-lingual corpus (Wiki or CC)\n\nmBERT\n\nXLMR\n\nXLMR-Long\n\nCROP\n\nMSM\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\n45.3 77.8\n\n43.8 77.4\n\n43.9 75.8\n\n46.2 80.3\n\n51.6 83.0\n\n38.8 84.7\n\n41.2 81.5\n\n44.7 82.0\n\n39.9 84.2\n\n53.0 83.8\n\n29.1 73.8\n\n29.3 68.5\n\n27.2 68.5\n\n27.7 70.9\n\n31.6 73.9\n\n28.7 65.8\n\n33.2 72.2\n\n31.6 71.1\n\n34.0 73.8\n\n39.4 77.9\n\n35.5 72.9\n\n45.4 81.8\n\n44.2 81.6\n\n47.0 85.5\n\n50.5 85.7\n\n27.0 65.0\n\n30.1 68.1\n\n27.6 63.2\n\n29.6 68.4\n\n27.0 61.2\n\n28.5 62.9\n\n26.2 63.1\n\n32.0 67.5\n\n32.2 64.4\n\n34.8 70.0\n\n33.4 67.0\n\n29.8 59.9\n\n33.4 66.5\n\n34.1 64.9\n\n32.1 68.1\n\n36.8 70.3\n\n29.4 62.6\n\n35.0 72.7\n\n37.3 74.3\n\n32.6 71.1\n\n32.2 66.4\n\n30.9 64.7\n\n31.8 70.4\n\n37.2 71.4\n\n22.0 48.2\n\n38.9 69.3\n\n54.6 86.7\n\n27.8 56.4\n\n35.3 63.9\n\n31.0 58.9\n\n41.5 72.0\n\n43.4 73.0\n\n- -\n\n51.7 81.0\n\n56.7 89.4\n\n40.8 76.8\n\n44.5 75.5\n\n49.5 80.0\n\n55.9 86.3\n\n62.6 89.8\n\n26.5 60.9\n\n50.9 87.1\n\n43.6 81.9\n\n25.3 59.2\n\n49.7 85.4\n\n48.2 85.9\n\n46.3 85.9\n\n53.5 88.2\n\n30.5 66.2\n\n39.9 75.2\n\n43.2 79.0\n\n33.0 69.7\n\n37.7 72.8\n\n37.6 72.4\n\n38.9 76.4\n\n44.7 78.6\n\nTable 3: Cross-lingual zero-shot transfer results on XOR Retrieve task. We report R@2k and R@5 metrics on the test sets. All compared methods are unsupervised pre-trained models.\n\nMethod Metrics AR\n\nmBERT\n\nXLMR\n\nCROP\n\nICT\n\nMSM\n\nR@2k R@5k\n\nR@2k R@5k\n\nR@2k R@5k\n\nR@2k R@5k\n\nR@2k R@5k\n\n31.1 44.1\n\n39.9 49.6\n\n45.4 53.8\n\n41.6 52.5\n\n47.9 58.4\n\nBN\n\n26.6 36.2\n\n25.7 34.9\n\n32.9 42.8\n\n35.9 46.1\n\n32.6 41.8\n\nFI\n\n38.5 48.1\n\n41.1 50.0\n\n42.4 49.4\n\n43.9 51.6\n\n44.9 51.9\n\nJA\n\n32.4 41.5\n\n27.8 34.9\n\n23.7 34.0\n\n27.8 39.0\n\n24.1 36.9\n\nKO\n\n38.6 48.1\n\n31.9 41.8\n\n33.3 42.1\n\n34.0 42.8\n\n35.8 44.6\n\nRU\n\n24.9 38.4\n\n22.4 30.4\n\n24.9 31.6\n\n24.5 32.9\n\n25.3 37.6\n\nTE\n\nAVG\n\n29.1 39.5\n\n25.2 34.0\n\n30.1 38.8\n\n27.8 38.5\n\n34.0 42.1\n\n31.6 42.3\n\n30.6 39.3\n\n33.2 41.8\n\n33.6 43.3\n\n34.9 44.7\n\nimprovements on all the tasks, which demonstrates the effectiveness of our proposed MSM. (2) Under the setting of cross-lingual zero-shot transfer, it shows that our MSM outperforms other models by a large margin. Compared to strong XLM-R, MSM improves 7% MRR@100 on Mr. TyDi, 4.3% R@2k on XOR Retrieve, 1.8% mAP@20 of Mewsli-X, and 4.2% mAP@20 of LAReQA. (3) Under the setting of multi-lingual fine-tuning, the performance can be further improved by fine-tuning on multi-lingual train data and MSM can achieve the best results. However, there usually doesn’t exist available multi-lingual data (such as Mewsli-X and LAReQA), especially for low-resource languages, and in this case MSM can achieve more gains for its stronger cross-lingual ability.\n\n5.3 RESULTS ACROSS LANGUAGES\n\nTable.2 show the results across languages on Mr. TyDi. The compared models are categorized into two kinds: mBERT, XLMR, CROP and MSM are unsupervised pre-trained models without any supervised data, while others are specialized multilingual encoders (Litschko et al., 2021) trained with parallel data or labeled data. DistilmBERT (Reimers & Gurevych, 2020) distills knowledge from the m-USE (Yang et al., 2019) teacher into a multilingual DistilBERT. InfoXLM (Chi et al., 2021) and LaBSE (Feng et al., 2022) encourage bilingual alignment with parallel data via ranking loss. They all use additional supervised data while our MSM only needs multi-lingual data. And in Appendix.A.4, we provide more comparison with these multilingual models. Through the detailed results in Tab.2, we demonstrate that MSM has consistent improvements over others on average\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Performance comparison of zero-shot cross-lingual retrieval when setting individual document encoder (and projection head) for English and other languages. OTHS means the average of languages except for EN.\n\nMethod\n\nXLMR\n\nShare All\n\nSep Doc\n\nSep Doc + Head\n\nMetrics\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nAR\n\n30.8 72.3\n\n37.9 77.3\n\n35.5 74.7\n\n35.8 75.4\n\nBN\n\n30.2 78.4\n\n39.4 82.9\n\n38.2 82.4\n\n39.7 76.1\n\nFI\n\n23.5 67.0\n\n27.7 70.5\n\n26.2 68.0\n\n28.8 70.2\n\nID\n\n33.5 79.8\n\n38.3 83.5\n\n38.3 81.7\n\n36.5 81.7\n\nJA\n\n23.5 66.5\n\n28.0 67.9\n\n24.9 65.2\n\n24.7 63.5\n\nKO\n\n26.6 64.7\n\n26.8 61.8\n\n27.9 59.8\n\n25.1 59.5\n\nRU\n\n25.7 65.0\n\n28.5 68.6\n\n28.3 66.5\n\n26.9 65.6\n\nSW TE\n\n24.0 57.2\n\n32.1 69.9\n\n32.6 66.9\n\n32.0 69.1\n\n26.6 72.7\n\n43.1 83.5\n\n41.3 87.2\n\n20.8 68.2\n\nTH\n\n36.3 84.6\n\n42.0 84.9\n\n42.6 83.6\n\n38.8 82.2\n\nEN OTHS AVG\n\n27.1 74.3\n\n29.9 73.6\n\n28.1 73.0\n\n28.0 72.4\n\n28.1 70.8\n\n34.4 75.1\n\n33.6 73.6\n\n30.9 71.2\n\n28.0 71.1\n\n34.0 74.9\n\n33.1 73.5\n\n30.6 71.3\n\nresults and most languages. LaBSE is slightly better on Recall@100 for it extends to a 500k vocabulary which is twice ours, and it utilizes parallel data. Interestingly though only fine-tuning on English data, MSM also achieves more gains in other languages. For example, MSM improves more than 5% on recall@100 on AR, FI, RU, and SW compared to XLMR, which clearly shows that MSM leads to better cross-lingual transfer compared to other baselines.\n\nIn Table.3, we mainly compare MSM with unsupervised pre-trained models. We reproduced two strong baselines proposed for monolingual retrieval, i.e. ICT and CROP, by extending them to multilingual corpora. We follow the original setting (Lee et al., 2019) for ICT and for CROP follow Wu et al. (2022a). It indicates that learning from two views of the same document (i.e. ICT and CROP) can achieve competitive results. Yet compared to them, MSM achieves more gains especially in low-resource languages, which indicates modeling sequential sentence relation across languages is indeed beneficial for cross-lingual transfer. More detailed results across different languages on other tasks can be seen in Appendix A.2 and there provides more analysis on multilinguality.\n\n5.4 ANALYSIS OF CROSS-LINGUAL ABILITY\n\nTo investigate why MSM can advantage cross-lingual retrieval and how the cross-lingual ability emerges, we design some analytical experiments. Recall that in the zero-shot transfer setting, the pre-trained model is first fine-tuned on English data and then evaluated on all languages, which relies on the cross-lingual transfer ability from en to others. So in this experiment, we set individual the document encoder for en language and other languages to break the sentence relation shared between en and others, to see how it impacts the retrieval performance.\n\nIn Table.4, we report the results of different settings: Share All mean the original MSM where the document encoder is shared for all languages, Sep Doc sets two separate document encoder for EN and others, and Sep Doc + Head separates both encoder and projection head. The results clearly show that if EN and others don’t share the same document encoder, the cross-lingual transfer ability drops rapidly. The previous works on multi-lingual BERTology (Conneau et al., 2020; Artetxe et al., 2020a; Rogers et al., 2020) found the shared model can learn the universal word embeddings across languages. Similar to it, our findings indicate that the shared document encoder benefits universal sentence embedding. This experiment further demonstrates that the sequential sentence relation is universal across languages, and modeling this relation is helpful for cross-lingual retrieval.\n\n5.5 ABLATION STUDY\n\nIn this section, we conduct the ablation study on several components in our model. Considering computation efficiency and for a fair comparison, we fine-tune all the pre-trained models on NQ data and evaluate them on the target data.\n\nAblation of Loss Function. We first study the effectiveness of the hierarchical contrastive loss proposed in Eq.1. As shown in Table.5, Cross-doc means only using cross-doc negatives without the intra-doc negatives. It results in poor performance due to the lack of utilization of intra-doc samples’ information. When w/o bias, it leads to a significant decrease for it regards intra-doc sentences as negatives, which would harm the representation space as we stated in Sec.3.2. We can\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: Comparison of different settings for the hierarchical contrastive loss.\n\nTable 6: Impact of contrastive negatives number. Best are marked bold.\n\nSetting\n\nCross-doc w/o bias α μ = 0.3 μ = 0.5 μ = 0.7\n\nMr. TyDi\n\nXOR Retrieve\n\nMRR@100 R@100 R@2k R@5k\n\n32.9 31.1 31.7 34.0 32.5\n\n73.9 73.1 73.3 74.9 74.2\n\n33.2 29.8 31.0 34.9 34.7\n\n41.6 39.9 41.5 44.7 44.2\n\nSize\n\n256 512 1024 4096\n\nMr. TyDi\n\nXOR Retrieve\n\nMRR@100 R@100 R@2k R@5k\n\n31.4 34.0 32.0 31.8\n\n73.6 74.9 73.9 72.8\n\n33.2 34.9 32.1 32.3\n\n42.9 44.7 42.4 43.2\n\nTable 7: Impact of the projector settings. Best are marked bold.\n\nTable 8: Comparison of different document encoder layers. Best are marked bold.\n\nSetting\n\nMr. TyDi\n\nXOR Retrieve\n\nLayers\n\nMr. TyDi\n\nXOR Retrieve\n\nMRR@100 R@100 R@2k R@5k\n\nMRR@100 R@100 R@2k R@5k\n\nw/o PL Shared PL Asymmetric PL\n\n27.0 32.1 34.0\n\n66.1 71.0 74.9\n\n30.3 34.5 34.9\n\n39.1 43.8 44.7\n\n1 2\n4 6\n\n29.7 34.0 33.3 30.9\n\n71.4 74.9 74.4 71.4\n\n33.1 34.9 34.3 34.2\n\n42.2 44.7 44.9 43.9\n\nchange the hyper μ to tune the impact of intra-doc negatives, and it gets the best results when setting μ at an appropriate value, which indicates ours can contribute to better representation learning.\n\nImpact of Contrastive Negative Size. We analyze how the number of negatives influences performance and range it from 256 to 4096. As shown in Table.6, the performance increase as the negative size become larger and it has diminishing gain if the batch is sufficiently large. Interestingly the model performance does not improve when continually increasing batch size, which has been also observed in some work (Cao et al., 2022; Chen et al., 2021) on contrastive learning. In our work, it may be due to when the total negative number increases to a large number, the impact of intradocument negatives would be diminished and hurt the performance. By the way, the performance would be harmed by the instability when the batch size is too large (Chen et al., 2021).\n\nImpact of Projector Setting. Existing work has shown the projection layers (Dong et al., 2022; Cao et al., 2022) between the representation and the contrastive loss affect the quality of learned representations. We explore the impact of different projection layer under different settings in Tab.7. Referring to Fig.1, Shared PL means the two projection layers share the same parameters, and Asymmetric PL means not sharing the layers. The results show that using an Asymmetric PL performs better than others and the removal of projection layers badly degrades the performance. One possible reason is that the projection layer can bridge the semantic gap between the representation of different samples in the embedding spaces.\n\nImpact of Decoder Layers. We explore the impact of the size of document encoder layers in Table.8 and find a two-layer document encoder can achieve the best results. When the document encoder only has one layer, its capability is not enough to model sequential sentence relation, resulting in inefficient utilization of information. When the layer increase to larger, the masked sentence prediction task may depend more on the document encoder’s ability and causes degradation of sentence representations, which is also in line with the findings of (Wu et al., 2022b; Lu et al., 2021).\n\n6 CONCLUSION\n\nIn this paper, we propose a novel masked sentence model which leverages sequential sentence relation for pre-training to improve cross-lingual retrieval. It contains a two-level encoder in which the document encoder applied to the sentence vectors generated by the sentence encoder from a sequence of sentences. Then we propose a masked sentence prediction task to train the model, which masks and predicts the selected sentence vector via a hierarchical contrastive loss with sampled negatives. Through comprehensive experiments on 4 cross-lingual retrieval benchmark datasets, we demonstrate that MSM significantly outperforms existing advanced pre-training methods. Our further analysis and detailed ablation study clearly show the effectiveness and stronger cross-lingual retrieval capabilities of our approach.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nMikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zero-shot\n\ncross-lingual transfer and beyond. TACL, 2019.\n\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of mono-\n\nlingual representations. In ACL, 2020a.\n\nMikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka, and Eneko Agirre. A call for more\n\nrigor in unsupervised cross-lingual learning. In ACL, pp. 7375–7388, 2020b.\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learning to retrieve reasoning paths over wikipedia graph for question answering. arXiv preprint arXiv:1911.10470, 2019.\n\nAkari Asai, Jungo Kasai, Jonathan Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. XOR\n\nQA: Cross-lingual open-retrieval question answering. In NAACL, 2021a.\n\nAkari Asai, Xinyan Yu, Jungo Kasai, and Hanna Hajishirzi. One question answering model for many languages with cross-lingual dense passage retrieval. Advances in Neural Information Processing Systems, 34:7547–7560, 2021b.\n\nJan A. Botha, Zifei Shan, and Daniel Gillick. Entity Linking in 100 Languages. In EMNLP, 2020.\n\nRui Cao, Yihao Wang, Yuxin Liang, Ling Gao, Jie Zheng, Jie Ren, and Zheng Wang. Exploring the impact of negative samples of contrastive learning: A case study of sentence embedding. In Findings of ACL, 2022.\n\nWei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. Pre-training\n\ntasks for embedding-based large-scale retrieval. In ICLR, 2020.\n\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\n\ntransformers. In ICCV, 2021.\n\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. InfoXLM: An information-theoretic framework for crosslingual language model pre-training. In NAACL, 2021.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm ́an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019.\n\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. Emerging cross-\n\nlingual structure in pretrained language models. In ACL, 2020.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n\nbidirectional transformers for language understanding. In NAACL, 2019.\n\nZhe Dong, Jianmo Ni, Dan Bikel, Enrique Alfonseca, Yuan Wang, Chen Qu, and Imed Zitouni. Exploring dual encoder architectures for question answering. arXiv preprint arXiv:2204.07120, 2022.\n\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-\n\nagnostic BERT sentence embedding. In ACL, 2022.\n\nNicola Ferro and Gianmaria Silvello. Clef 2000-2014: Lessons learnt from ad hoc retrieval. In IIR,\n\n2015.\n\nLuyu Gao and Jamie Callan. Unsupervised corpus aware language model pre-training for dense\n\npassage retrieval. arXiv preprint arXiv:2108.05540, 2021.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence\n\nembeddings. In EMNLP, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJohn Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. DeCLUTR: Deep contrastive learning for\n\nunsupervised textual representations. In ACL, 2021.\n\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In ICML, pp. 4411–4421. PMLR, 2020.\n\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. Unicoder: A universal language encoder by pre-training with multiple cross-lingual tasks. arXiv preprint arXiv:1909.00964, 2019.\n\nKung-Hsiang Huang, ChengXiang Zhai, and Heng Ji. Concrete: Improving cross-lingual fact-\n\nchecking with cross-lingual retrieval. arXiv preprint arXiv:2209.02071, 2022.\n\nDan Iter, Kelvin Guu, Larry Lansing, and Dan Jurafsky. Pretraining with contrastive sentence ob-\n\njectives improves discourse performance of language models. In ACL, 2020.\n\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Towards unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021a.\n\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021b.\n\nXiaoze Jiang, Yaobo Liang, Weizhu Chen, and Nan Duan. Xlm-k: Improving cross-lingual language In Proceedings of the AAAI Conference on\n\nmodel pre-training with multilingual knowledge. Artificial Intelligence, pp. 10840–10848, 2022.\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP, 2020.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. TACL, 2019.\n\nGuillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint\n\narXiv:1901.07291, 2019.\n\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open\n\ndomain question answering. In ACL, 2019.\n\nChunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, and Jianfeng Gao. Optimus: Organizing sentences via pre-trained modeling of a latent space. In EMNLP, November 2020.\n\nYulong Li, Martin Franz, Md Arafat Sultan, Bhavani Iyer, Young-Suk Lee, and Avirup Sil. Learning\n\ncross-lingual ir from an english retriever. arXiv preprint arXiv:2112.08185, 2021.\n\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation. In EMNLP, 2020.\n\nRobert Litschko, Ivan Vuli ́c, Simone Paolo Ponzetto, and Goran Glavaˇs. Evaluating multilingual text encoders for unsupervised cross-lingual retrieval. In ECIR, pp. 342–358. Springer, 2021.\n\nRobert Litschko, Ivan Vuli ́c, and Goran Glavaˇs. Parameter-efficient neural reranking for cross-\n\nlingual and multilingual retrieval. arXiv preprint arXiv:2204.02292, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.\n\nZheng Liu and Yingxia Shao. Retromae: Pre-training retrieval-oriented transformers via masked\n\nauto-encoder. arXiv preprint arXiv:2205.12035, 2022.\n\nShuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold Overwijk. Less is more: Pretrain a strong Siamese encoder for dense text retrieval using a weak decoder. In EMNLP, 2021.\n\nXinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, and Xueqi Cheng. Pre-train a discriminative text encoder for dense retrieval via contrastive span prediction. arXiv preprint arXiv:2204.10641, 2022.\n\nZhengyi Ma, Zhicheng Dou, Wei Xu, Xinyu Zhang, Hao Jiang, Zhao Cao, and Ji-Rong Wen. Pre-\n\ntraining for ad-hoc retrieval: hyperlink is also you need. In CIKM, pp. 1212–1221, 2021.\n\nJian-Yun Nie. Cross-language information retrieval. Synthesis Lectures on Human Language Tech-\n\nnologies, 3(1):1–125, 2010.\n\nBarlas O ̆guz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Wen-tau Yih, Sonal Gupta, et al. Domain-matched pre-training tasks for dense retrieval. arXiv preprint arXiv:2107.13602, 2021.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nXuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE-M: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora. In EMNLP, 2021.\n\nJonas Pfeiffer, Ivan Vuli ́c, Iryna Gurevych, and Sebastian Ruder. Mad-x: An adapter-based frame-\n\nwork for multi-task cross-lingual transfer. In EMNLP, pp. 7654–7673, 2020.\n\nRevanth Gangi Reddy, Vikas Yadav, Md Arafat Sultan, Martin Franz, Vittorio Castelli, Heng Ji, and Avirup Sil. Towards robust neural retrieval models with synthetic pre-training. arXiv preprint arXiv:2104.07800, 2021.\n\nNils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using\n\nknowledge distillation. In EMNLP, 2020.\n\nPhilip Resnik. Parallel strands: a preliminary investigation into mining the web for bilingual text. In Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers, 1998.\n\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about\n\nhow bert works. Transactions of the Association for Computational Linguistics, 2020.\n\nUma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua, Aaron Phillips, and Yinfei Yang. LAReQA: Language-agnostic answer retrieval from a multilingual pool. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.\n\nSebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, and Melvin Johnson. XTREME-R: Towards more challenging and nuanced multilingual evaluation. In EMNLP, 2021.\n\nMarkus Sagen. Large-context question answering with cross-lingual transfer. Master’s thesis, Upp-\n\nsala University, Department of Information Technology, 2021.\n\nBishal Santra, Potnuru Anusha, and Pawan Goyal. Hierarchical transformer for task oriented dialog\n\nsystems. In NAACL, June 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nPeng Shi, Rui Zhang, He Bai, and Jimmy Lin. Cross-lingual training of dense retrievers for document retrieval. In Proceedings of the 1st Workshop on Multilingual Representation Learning, 2021.\n\nShuo Sun and Kevin Duh. CLIRMatrix: A massively large collection of bilingual and multilingual\n\ndatasets for cross-lingual information retrieval. In EMNLP, 2020.\n\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. Ernie 2.0: A continual pre-training framework for language understanding. In AAAI, pp. 8968–8975, 2020.\n\nBrian Thompson and Philipp Koehn. Exploiting sentence order in document alignment. In EMNLP,\n\n2020.\n\nXiangpeng Wei, Rongxiang Weng, Yue Hu, Luxi Xing, Heng Yu, and Weihua Luo. On learning\n\nuniversal representations across languages. arXiv preprint arXiv:2007.15960, 2020.\n\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm ́an, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359, 2019.\n\nNing Wu, Yaobo Liang, Houxing Ren, Linjun Shou, Nan Duan, Ming Gong, and Daxin Jiang. Unsupervised context aware sentence representation pretraining for multi-lingual dense retrieval. arXiv preprint arXiv:2206.03281, 2022a.\n\nXing Wu, Guangyuan Ma, Meng Lin, Zijia Lin, Zhongyuan Wang, and Songlin Hu. Contextual\n\nmask auto-encoder for dense passage retrieval. arXiv preprint arXiv:2208.07670, 2022b.\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. CoRR, abs/2007.00808, 2020.\n\nXiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. A biterm topic model for short texts. In Proceedings of the 22nd international conference on World Wide Web, pp. 1445–1456, 2013.\n\nPeilin Yang, Hui Fang, and Jimmy Lin. Anserini: Enabling the use of lucene for information retrieval In Proceedings of the 40th International ACM SIGIR Conference on Research and\n\nresearch. Development in Information Retrieval, pp. 1253–1256, 2017.\n\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, et al. Multilingual universal sentence encoder for semantic retrieval. arXiv preprint arXiv:1907.04307, 2019.\n\nHang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu Chen. Poolingformer: Long document modeling with pooling attention. In International Conference on Machine Learning, pp. 12437–12446. PMLR, 2021a.\n\nShunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, and Nan Duan. Multi-view document\n\nrepresentation learning for open-domain dense retrieval. In ACL, pp. 5990–6000, 2022a.\n\nXingxing Zhang, Furu Wei, and Ming Zhou. HIBERT: Document level pre-training of hierarchical\n\nbidirectional transformers for document summarization. In ACL, 2019.\n\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. Mr. TyDi: A multi-lingual benchmark for dense retrieval. In Proceedings of the 1st Workshop on Multilingual Representation Learning, 2021b.\n\nXinyu Zhang, Kelechi Ogueji, Xueguang Ma, and Jimmy Lin. Towards best practices for training\n\nmultilingual dense retrieval models. arXiv preprint arXiv:2204.02363, 2022b.\n\nYan Zhang, Ruidan He, Zuozhu Liu, Lidong Bing, and Haizhou Li. Bootstrapped unsupervised\n\nsentence representation learning. In ACL, 2021c.\n\nJiawei Zhou, Xiaoguang Li, Lifeng Shang, Lan Luo, Ke Zhan, Enrui Hu, Xinyu Zhang, Hao Jiang, Zhao Cao, Fan Yu, Xin Jiang, Qun Liu, and Lei Chen. Hyperlink-induced pre-training for passage retrieval in open-domain question answering. In ACL, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 DETAILS OF DATASETS AND HYPERPARAMETERS\n\nCC-108. Since Common Crawl (Wenzek et al., 2019) is not a public dataset, so we need to preprocess it by ourselves. Our reserved 108 languages are the union of the languages that XLMR and mBERT support. And we followed the processing method adopted by XLMR (Conneau et al., 2019) to pre-process CommonCrawl and retain 108 languages.\n\nMr. TyDi. (Zhang et al., 2021b) It aims to evaluate cross-lingual passage retrieval with dense representations. Mr. TyDi is a multi-lingual benchmark dataset for mono-lingual query passage retrieval in eleven typologically distinct languages. Given a question in language L, the model should retrieve relevant texts in language L that can respond to the query. As the original paper suggests, we take MRR@100 and Recall@100 for evaluation.\n\nWe mainly follow the open-source codebase DPR (Karpukhin et al., 2020) with minor modifications for multi-lingual models. When fine-tuned on MS MARCO in the zero-shot setting, we use AdamW optimizer with a learning rate of 2e-5. The model is trained for up to 3 epochs with a mini-batch size of 64. When fine-tuning on the NQ dataset, it is up to 40 epochs with a mini-batch size of 128. When further fine-tuning on Mr. TyDi’s in-language data, it is 40 epochs, mini-batch size 128, and 1e-5 learning rate. Note all of them mainly follow what the original papers (Zhang et al., 2021b; 2022b) suggest. All these experiments are conducted on 8 NVIDIA Tesla A100 GPUs.\n\nXOR Retrieve. (Asai et al., 2021a) XOR QA is proposed for multilingual open-domain QA, and we take its sub-task XOR-Retrieve for our evaluation: given a question in L (e.g., Korean), the task is to retrieve English passages that can answer the query. Following Asai et al. (2021a), we calculate the recall by computing the fraction of the questions for which the minimal answer is included in the top n tokens selected, and take R@2kt and R@5kt (kilo-tokens) as the metrics.\n\nSimilar to the settings of the previous one, we use AdamW Optimizer with a learning rate of 2e-5. The model is trained up to 40 epochs with a mini-batch size of 128 when fine-tuning on NQ dataset. And when further tuning on XOR’s data, the hyper-parameter remains the same. We evaluate all the compared pre-trained models in the same pipelines on 8 NVIDIA Tesla A100 GPUs.\n\nMewsli-X. (Ruder et al., 2021) Mewsli-X is built on top of Mewsli (Botha et al., 2020). We follow the setting of XTREME-R (Ruder et al., 2021), which builds Mewsli-X consisting of 15K mentions in 11 languages: given a mention in context, it is to retrieve the correct target entity description from a candidate pool ranging over 1M candidates across 50 languages.\n\nWe mainly follow the setting of XTREME-R (Hu et al., 2020). It adopts a 2e-5 learning rate, and it is trained for up to 2 epochs with batch size of 16. As the original paper suggests, all the evaluations are conducted on NVIDIA Tesla V100 GPU for a fair comparison.\n\nLAReQA. (Roy et al., 2020) Language Agnostic Retrieval Question Answering is a retrieval task in which each query has target answers in multiple languages, and models require retrieving all correct answers from the candidate pool regardless of language. Following Ruder et al. (2021), we use the LAReQA XQuAD-R dataset which consists of 13,090 questions, each of which has 11 target answers (in 11 different languages) within 13,014 candidate answer sentences.\n\nFollowing Ruder et al. (2021), we use the LAReQA XQuAD-R dataset which consists of 13,090 questions, each of which has 11 target answers (in 11 different languages) within 13,014 candidate answer sentences. It also follows the setting proposed by XTREME-R (Hu et al., 2020). It adopts a 2e-5 learning rate and it is trained up to 3 epochs with batch size of 4. All the evaluations are conducted on NVIDIA Tesla V100 GPU.\n\nA.2 DETAILED RESULTS ACROSS LANGUAGES\n\nWe show the detailed results for each task across different languages corresponding to Table.1. Specifically, the results of Mr. TyDi are as shown in Table.9, XOR Retrieve in Table.10, Mewsli-X in Table.11, and LAReQA in Table.12.\n\nThrough the detailed results across languages, there are some findings on multilinguality: (1) MSM can achieve more gains in low-resource language. For example, in zero-shot setting of Mr. TyDi\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTable 9: Mr. TyDi results across languages. We report MRR@100 and Recall@100 on the test sets of Mr. TyDi in the two settings.\n\nMethod\n\nMetrics\n\nAR\n\nBN\n\nEN\n\nFI\n\nID\n\nJA\n\nKO\n\nRU\n\nSW TE\n\nTH\n\nAVG\n\nCross-lingual zero-shot transfer (models are fine-tuned on English data)\n\nmBERT\n\nXLMR\n\nMSM\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\n45.3 77.8\n\n43.8 77.4\n\n51.6 83.0\n\n38.8 84.7\n\n41.2 81.5\n\n53.0 83.8\n\n29.1 73.8\n\n29.3 68.5\n\n31.6 73.9\n\n28.7 65.8\n\n33.2 72.2\n\n39.4 77.9\n\n35.5 72.9\n\n45.4 81.8\n\n50.5 85.7\n\n29.6 68.4\n\n27.0 61.2\n\n32.0 67.5\n\n29.8 59.9\n\n33.4 66.5\n\n36.8 70.3\n\nMulti-lingual fine-tune (models are fine-tuned on the multi-lingual data)\n\nmBERT\n\nXLMR\n\nMSM\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\n66.8 90.3\n\n66.4 89.4\n\n67.7 90.5\n\n63.3 95.0\n\n66.0 93.2\n\n69.9 95.9\n\n50.8 89.0\n\n48.5 84.3\n\n49.6 86.5\n\n55.0 85.8\n\n53.7 86.6\n\n55.1 88.0\n\n55.7 89.3\n\n58.3 90.0\n\n61.2 90.1\n\n47.8 81.6\n\n45.4 82.3\n\n48.2 82.0\n\n43.3 80.2\n\n44.3 79.3\n\n49.4 81.6\n\n32.6 71.1\n\n32.2 66.4\n\n37.2 71.4\n\n47.3 85.6\n\n47.4 82.8\n\n47.5 82.5\n\n27.8 56.4\n\n35.3 63.9\n\n43.4 73.0\n\n60.5 86.7\n\n61.4 86.4\n\n63.9 88.9\n\n40.8 76.8\n\n44.5 75.5\n\n62.6 89.8\n\n85.8 96.3\n\n86.2 97.4\n\n85.2 97.6\n\n25.3 59.2\n\n49.7 85.4\n\n53.5 88.2\n\n57.7 87.2\n\n65.7 93.8\n\n67.7 95.0\n\n33.0 69.7\n\n37.7 72.8\n\n44.7 78.6\n\n57.6 87.9\n\n58.5 87.8\n\n60.5 89.0\n\nTable 10: XOR Retrieve results across languages. We report R@2k and R@5 metrics on the test sets in the two settings.\n\nMethod Metrics AR\n\nBN\n\nFI\n\nJA\n\nKO\n\nRU\n\nTE\n\nAVG\n\nCross-lingual zero-shot transfer (models are fine-tuned on English data)\n\nmBERT\n\nXLMR\n\nMSM\n\nR@2k R@5k\n\nR@2k R@5k\n\nR@2k R@5k\n\n31.1 44.1\n\n39.9 49.6\n\n47.9 58.4\n\n26.6 36.2\n\n25.7 34.9\n\n32.6 41.8\n\n38.5 48.1\n\n41.1 50.0\n\n44.9 51.9\n\n32.4 41.5\n\n27.8 34.9\n\n24.1 36.9\n\n38.6 48.1\n\n31.9 41.8\n\n35.8 44.6\n\n24.9 38.4\n\n22.4 30.4\n\n25.3 37.6\n\n29.1 39.5\n\n25.2 34.0\n\n34.0 42.1\n\nMulti-lingual fine-tune (models are fine-tuned on the multi-lingual data)\n\nmBERT\n\nXLMR\n\nMSM\n\nR@2k R@5k\n\nR@2k R@5k\n\nR@2k R@5k\n\n51.7 58.4\n\n59.2 67.2\n\n61.8 68.9\n\n52.7 61.5\n\n48.7 58.6\n\n55.3 63.8\n\n52.2 58.6\n\n46.8 53.5\n\n51.3 59.9\n\n41.9 50.6\n\n36.1 45.6\n\n36.9 48.1\n\n51.9 63.2\n\n46.0 56.1\n\n50.5 56.1\n\n47.5 54.9\n\n35.4 45.1\n\n41.4 52.7\n\n40.5 51.8\n\n43.4 51.5\n\n43.0 51.5\n\n31.6 42.3\n\n30.6 39.3\n\n34.9 44.7\n\n48.3 57.0\n\n45.1 53.9\n\n48.6 57.3\n\n(Tab.9) it improves + 8.1 MRR@100 for SW and + 11.8 for BN compared to XLMR, and for XOR Retrieve (Tab.10) + 8.8 R@5k for TE. Though there are limited data in low-resource languages and the model suffers from the curse of multilinguality, our MSM can lead to better transfer to them, benefiting from modeling the sequential sentence relation across languages. (2) The target languages closer to pivot language (i.e. EN in our experiment) usually perform better and achieve more improvements. On Mewsli-X task (Tab.11) MSM can improve + 5.3 MAP for language DE while only + 1.3 for UK, for German (DE) is more similar to English in both scripts and language family (Ruder et al., 2021). Similar observations also exist in LAReQA (Tab.12) that MSM performs better and improves more on DE, EL, ES and poorer on ZH and TH. (3) The multi-lingual data lead to better cross-lingual retrieval performance. It can be seen in Tab.9 and Tab.10, the performance can be further improved after fine-tuning multi-lingual train data. It indicates that the target languages can benefit from the other languages’ data, and also shows that fine-tuning on multi-lingual data is necessary if available. It is worth mentioning that in this setting MSM can also achieve more gains, which demonstrates better cross-lingual transfer ability.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTable 11: Mewsli-X results across different input languages. We report the mean average precision@20 (mAP@20) results.\n\nMethod\n\nAR\n\nDE\n\nEN\n\nES\n\nFA\n\nJA\n\nPL\n\nRO TA TR\n\nUK AVG\n\nCross-lingual zero-shot transfer (models are fine-tuned on English data)\n\n14.2 mBERT XLMR 15.5 MSM 18.6\n\n65.5 62.7 68.0\n\n57.4 57.2 58.7\n\n55.7 53.5 57.3\n\n11.3 12.5 13.7\n\n44.8 46.2 46.3\n\n57.2 59.3 60.2\n\n38.2 34.1 36.3\n\n5.8 7.5 7.6\n\n42.7 51.8 52.8\n\n36.2 36.0 37.3\n\n39.0 39.7 41.5\n\nTable 12: LAReQA results across different question languages. We report the mean average precision@20 (mAP@20) results.\n\nMethod\n\nAR\n\nDE\n\nEL\n\nEN\n\nES\n\nHI\n\nRU\n\nTH\n\nTR\n\nVI\n\nZH\n\nAVG\n\nCross-lingual zero-shot transfer (models are fine-tuned on English data)\n\n20.1 mBERT XLMR 21.4 MSM 28.0\n\n32.2 31.7 36.1\n\n19.9 26.7 31.9\n\n34.8 36.7 40.6\n\n33.5 34.3 38.3\n\n15.3 25.2 29.4\n\n29.6 32.3 36.0\n\n7.2 27.8 30.2\n\n23.3 29.0 34.3\n\n27.6 29.1 33.2\n\n26.2 28.5 29.7\n\n24.5 29.3 33.4\n\nA.3 COMPARISON WITH SEVERAL EXISTING METHODS\n\nTable 13 shows the comparison of MSM and several existing retrieval approaches. DistilmBERT (Reimers & Gurevych, 2020) distills knowledge from m-USE (Yang et al., 2019) trained on labeled pair data into mBERT. LaBSE (Feng et al., 2022) and InfoXLM (Chi et al., 2021) encourage bilingual alignment via a translation ranking loss, and also trained with MLM and TLM tasks (Lample & Conneau, 2019). InfoXLM adopts the momentum contrast and LaBSE proposed additive margin softmax for contrastive learning. They all use additional parallel corpora, while our MSM only needs multi-lingual data without relying on any parallel or labeled data.\n\nAmong unsupervised pre-trained models, mBERT (Devlin et al., 2019) and XLMR (Conneau et al., 2019) are general-purpose multilingual text encoders trained with MLM. XLMR-Long (Sagen, 2021) (or XLMR Longformer) is an XLMR model that has been extended to allow sequence lengths up to 4096 tokens. mContriever (Izacard et al., 2021b) and CCP (Wu et al., 2022a) similarly mine positive pairs by cropping two spans in a document. The former proposes random cropping while the latter utilizes two sentences. However, the quality of the cropped pairs is not guaranteed. In contrast, MSM utilizes a sequence of sentences in a document and models the universal sentence relation across languages via an explicit document encoder, which results in better cross-lingual retrieval capability.\n\nTable 13: Comparison with existing approaches. For the training objective, BI means bi-lingual pair alignment task, and CROP means contrastive learning with cropped spans. For the corpora, CC means CommonCrawl, mWiki means multi-lingual data from Wikipedia, and Bi-lingual may include MultiUN, OPUS, WikiMatrix, etc (Chi et al., 2021), which depends on models.\n\nModel\n\n#lg Objective Function\n\nCorpora\n\nDistilmBERT (Reimers & Gurevych, 2020) LaBSE (Feng et al., 2022) InfoXLM (Chi et al., 2021)\n\nDistillation\n\n53 109 MLM + TLM + BI CC + mWiki + Bi-lingual 94 MLM + TLM + BI\n\nCC + Bi-lingual\n\nBi-lingual\n\nmBERT (Devlin et al., 2019) XLMR (Conneau et al., 2019) XLMR-Long (Sagen, 2021) mContriever (Izacard et al., 2021b) CCP (Wu et al., 2022a)\n\nMSM\n\n104 100 100 29 108\n\n108\n\nMLM MLM MLM CROP MLM + CROP\n\nMLM + MSM\n\nmWiki CC CC + Wiki CC + mWiki CC\n\nCC\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 14: Cross-lingual zero-shot transfer retrieval performance for different size models. We report MRR@100 and Recall@100 on the test sets of Mr. TyDi after finetuning on NQ dataset.\n\nMethod\n\nMetrics\n\nmBERT\n\nXLMR Base\n\nXLMR Large\n\nInfoXLM Large\n\nCCP Large\n\nMSM Base\n\nMSM Large\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nMRR@100 Recall@100\n\nAR\n\n28.7 68.3\n\n30.8 72.3\n\n36.5 81.3\n\n37.2 76.2\n\n42.6 82.0\n\n37.9 77.3\n\n45.6 83.3\n\nBN\n\n33.4 79.3\n\n30.2 78.4\n\n37.4 84.2\n\n50.4 91.0\n\n45.7 88.3\n\n39.4 82.9\n\n55.7 90.1\n\nEN\n\n28.1 76.1\n\n27.1 74.3\n\n27.5 77.6\n\n31.4 78.3\n\n35.9 80.1\n\n29.9 73.6\n\n33.8 78.5\n\nFI\n\n24.0 65.0\n\n23.5 67.0\n\n31.8 78.2\n\n30.9 76.0\n\n37.2 78.7\n\n27.7 70.5\n\n39.1 79.9\n\nID\n\n32.0 74.1\n\n33.5 79.8\n\n39.5 88.6\n\n37.6 85.2\n\n46.2 87.5\n\n38.3 83.5\n\n46.8 85.9\n\nJA\n\n24.3 65.3\n\n23.5 66.5\n\n29.9 78.5\n\n27.1 66.9\n\n37.7 80.0\n\n28.0 67.9\n\n40.3 78.1\n\nKO\n\n21.8 57.9\n\n26.6 64.7\n\n30.4 72.7\n\n30.9 64.4\n\n34.6 73.2\n\n26.8 61.8\n\n38.8 73.1\n\nRU\n\n29.0 69.2\n\n25.7 65.0\n\n30.6 77.4\n\n32.5 74.4\n\n36.0 77.2\n\n28.5 68.6\n\n37.9 77.1\n\nSW TE\n\nTH\n\nAVG\n\n18.7 51.7\n\n24.0 57.2\n\n27.4 63.3\n\n39.4 75.0\n\n39.2 75.1\n\n32.1 69.9\n\n41.8 76.4\n\n14.5 46.1\n\n26.6 72.7\n\n34.6 87.5\n\n46.5 88.9\n\n47.0 88.8\n\n43.1 83.5\n\n46.5 86.9\n\n19.3 54.0\n\n36.3 84.6\n\n40.1 88.2\n\n37.4 83.4\n\n48.9 88.9\n\n42.0 84.9\n\n50.8 88.9\n\n24.9 64.3\n\n28.0 71.1\n\n33.3 79.8\n\n36.5 78.2\n\n41.0 81.8\n\n34.0 74.9\n\n43.4 81.6\n\nA.4 PERFORMANCE OF DIFFERENT MODEL SIZE\n\nMotivated by the recent progress of giant models, we also increase the model capability. Considering expensive computation, it is initialized with XLMR-large and other settings keep the same as the base model. As shown in Tab.14, we report the zero-shot transfer results on Mr. TyDi after fine-tuning on NQ data. It clearly shows that as the model capacity increase, the performance on the downstream task can be consistently improved. We also report several strong large-size pre-trained models, including InfoXLM and CCP which are both initialized with XLM-R Large. Compared to them, MSM-Large achieves outperforming results on MRR@100 and comparable results on Recall@100, which further demonstrates the effectiveness of MSM in different model capabilities.\n\nA.5 COMPARISON TO MT-BASED CROSS-LINGUAL RETRIEVAL\n\nIn this section, we compare the model performance to the MT-based (machine translation based) cross-lingual retrieval. As shown in Table.15, we provide four MT-based baselines borrowed from (Asai et al., 2021a), all of which first translate the query to the document language using a translation system and then perform monolingual retrieval. For translation systems, GMT means Google’s online machine translation service and White-MT means white-box translation model based on autoregressive transformers. For the monolingual retrieval model, PATH means Path Retriever (Asai et al., 2019), a graph-based recurrent retrieval approach, and DPR (Karpukhin et al., 2020) is a typical bi-encoder based retriever. Both of them are trained on the human translated queries with the annotated gold paragraph data of XOR Retrieve.\n\nResults in Table.15 clearly shows that MSM can outperform While-box MT-based methods by a large margin, which demonstrates the effectiveness of MSM. Moreover, upgrading the MT system to GMT achieve even better results, due to the superiority of industrial MT systems (large parallel data, models and pipelines, etc.) (Asai et al., 2021a). These observations indicate that the performance of MT-base methods heavily depends on the quality of MT system. However, GMT is a black-box system so it’s difficult to be analyzed. By the way, the MT-based method relies on the two-stage pipeline that first translates and then retrieves, which may lead to cumulative errors. In contrast, our MSM is a universal pre-trained model and can be easily applied in bi-encoder retrievers, which have more advantages in terms of deployments and diagnosis.\n\nA.6 MONOLINGUAL EXPERIMENT\n\nWe adapt our MSM pre-training method to the monolingual domain, in which we narrow the train corpus and model to English only. We initialize the sentence encoder with ERNIE-2.0-Base model and others adopt the same setting to our multi-lingual experiment. In Table.16, we report the performance on the Natural Question (Kwiatkowski et al., 2019) test set after fine-tuning. It shows that our proposed unsupervised model achieves better performance than advanced baselines including\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable 15: Performance comparison to MT-Based Cross-lingual Retrieval on XOR Retrieve across languages. We report R@2k and R@5k results.\n\nMethod\n\nMetrics AR\n\nGMT + PATH\n\nGMT + DPR\n\nWhite-MT + PATH\n\nWhite-MT + DPR\n\nXLM-R\n\nMSM\n\nR@2k R@5k\n\nR@2k R@5k\n\nR@2k R@5k\n\nR@2k R@5k\n\nR@2k R@5k\n\nR@2k R@5k\n\n59.1 63.3\n\n61.7 67.5\n\n45.0 51.6\n\n48.3 52.5\n\n59.2 67.2\n\n61.8 68.9\n\nBN\n\n58.2 78.9\n\n72.0 83.2\n\n60.9 64.8\n\n54.4 63.2\n\n48.7 58.6\n\n55.3 63.8\n\nFI\n\n60.3 64.1\n\n60.6 68.1\n\n56.6 59.5\n\n56.7 65.9\n\n46.8 53.5\n\n51.3 59.9\n\nJA\n\n50.0 52.3\n\n52.1 60.1\n\n36.7 41.7\n\n41.8 52.1\n\n36.1 45.6\n\n36.9 48.1\n\nKO\n\n50.3 54.0\n\n57.9 66.3\n\n33.8 37.6\n\n39.4 46.5\n\n46.0 56.1\n\n50.5 56.1\n\nRU\n\n54.1 56.5\n\n51.2 60.4\n\n34.7 38.1\n\n39.6 47.3\n\n35.4 45.1\n\n41.4 52.7\n\nTE\n\nAVG\n\n58.0 62.5\n\n59.4 65.0\n\n15.7 18.1\n\n18.7 22.7\n\n43.4 51.5\n\n43.0 51.5\n\n58.2 61.7\n\n59.3 67.2\n\n40.5 44.5\n\n42.7 50.0\n\n45.1 53.9\n\n48.6 57.3\n\nTable 16: Monolingual Retrieval performance on Natural Question test. The best performing models are marked bold and the results unavailable are left blank.\n\nMethod\n\nR@5 R@20 R@100\n\nBM25 (Yang et al., 2017) BERT (Karpukhin et al., 2020) RoBERTa (Liu et al., 2019) ERNIE2.0 (Sun et al., 2020)\n\nMSM (ours)\n\n- -\n64.9 68.7\n\n69.3\n\n59.1 78.4 76.8 79.8\n\n80.3\n\n73.7 85.3 84.7 86.1\n\n86.5\n\nBERT and ERNIE2.0 (Sun et al., 2020). It further proves our MSM is a general pre-training method tailored for dense retrieval including monolingual and multi-lingual domains.\n\n18",
    "reference": "# Summary Of The Paper\n\nThis work proposes a new method for cross-lingual and multilingual dense retrieval, focusing on applications of query-passage and sentence retrieval in multilingual setups. The approach relies on parallel document-aligned and assumes that the sentences in such documents are roughly in the same order; the idea is then to combine the standard MLM objective with its bilingual sentence-level variant, termed Masked Sentence Modeling (MSM), hoping to guess the correct sentence (once the sentence is masked in the document), where the document encoder is shared across languages. \n\nThe experiments are conducted on four retrieval tasks from prior work, and gains over (mostly) weak baselines are reported.\n\n# Strength And Weaknesses\n\nStrengths:\n- The idea of MSM is simple and neat, although it is very similar to the idea of next sentence prediction - the difference here is that the model performs masked sentence prediction.\n- The ablation study shows the usefulness of introducing the MSM objective.\n\nThe paper is simply not at a state to be considered for publication, with a series of major flaws as follows.\nWeaknesses:\n- Only a partial awareness of very related work on cross-lingual information retrieval (CLIR), with many strong reference works and baselines omitted from the paper completely and omitted from the comparisons. Here, only a few directly relevant papers are mentioned:\n-- https://arxiv.org/pdf/2112.09118.pdf\n-- https://arxiv.org/abs/2204.02292\n-- https://arxiv.org/abs/2004.09813\n-- https://arxiv.org/pdf/2101.08370.pdf\n-- There is also work on multilingual Longformers (e.g., https://huggingface.co/markussagen/xlm-roberta-longformer-base-4096)\n- Related to the point above, those papers provide much stronger baselines - the baselines in this submission are simply weak and inadequate. When doing sentence retrieval, the paper should compare against strong multilingual sentence encoders and not the original off-the-shelf models. \n- The paper also does conflate query-passage and sentence retrieval, and does not evaluate on document retrieval at all. There are huge differences on how to approach each 'granularity of information' when doing retrieval, and the paper does not seem to pay attention to that: e.g., check this work for further details: https://arxiv.org/pdf/2101.08370.pdf\n- The paper also critically requires parallel data to work -> if one has parallel data, one of the must-have baselines are also MT-based query-translate or passage-translate approaches which sometimes/often work better than standard encoder-based approaches.\n- There are no discussions on how different target languages might impact the results: are all the languages equally difficult, which ones might cause major drops of performance and, most importantly, why? The paper treats multilinguality very superficially. \n\nThere are other (minor) weaknesses, including problems with language and presentation, but the major ones are mostly listed above.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper will not have any major impact as it omits many major baselines and a lot of very relevant work, offering only basic comparisons and lacking insightful side analyses. It makes a minor methodological contribution by combining document-level and sentence-level masked language modeling, which is not evaluated against cutting-edge CLIR methods.\n\nIt should be possible to reproduce the main results in the paper - it does not mention whether the results are average over several random seeds or not (and which random seed was used).\n\n# Summary Of The Review\n\nThe paper lacks strong baselines, shows only partial awareness of the current cutting-edge CLIR methodology. and it is difficult to contextualise its results (does it really bring any major improvement for CLIR?). It also does not delve deeper into intricacies of multilinguality and differences between sentence/passage/document retrieval. There are also presentation problems which make the paper seem incomplete and half-finished\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nLONG-TAILED PARTIAL LABEL LEARNING VIA DYNAMIC REBALANCING\n\nFeng Hong1\n\nJiangchao Yao1,2,† Zhihan Zhou1 Ya Zhang1,2 Yanfeng Wang1,2,†\n\n1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University 2Shanghai AI Laboratory\n\nfeng.hong, Sunarker, zhihanzhou, ya zhang, wangyanfeng\n\n}\n\n{\n\n@sjtu.edu.cn\n\nABSTRACT\n\nReal-world data usually couples the label ambiguity and heavy imbalance, challenging the algorithmic robustness of partial label learning (PLL) and long-tailed learning (LT). The straightforward combination of LT and PLL, i.e., LT-PLL, suffers from a fundamental dilemma: LT methods build upon a given class distribution that is unavailable in PLL, and the performance of PLL is severely influenced in long-tailed context. We show that even with the auxiliary of an oracle class prior, the state-of-the-art methods underperform due to an adverse fact that the constant rebalancing in LT is harsh to the label disambiguation in PLL. To overcome this challenge, we thus propose a dynamic rebalancing method, termed as RECORDS, without assuming any prior knowledge about the class distribution. Based on a parametric decomposition of the biased output, our method constructs a dynamic adjustment that is benign to the label disambiguation process and theoretically converges to the oracle class prior. Extensive experiments on three benchmark datasets demonstrate the significant gain of RECORDS compared with a range of baselines. The code is publicly available.\n\n1\n\nINTRODUCTION\n\nPartial label learning (PLL) origins from the real-world scenarios, where the annotation for each sample is an ambiguous set containing the groundtruth and other confusing labels. This is common when we gather annotations of samples from news websites with several tags (Luo & Orabona, 2010), videos with several characters of interest (Chen et al., 2018), or labels from multiple annotators (Gong et al., 2018). The ideal assumption behind PLL is that the collected data is approximately uniformly distributed regarding classes. However, a natural distribution assumption in above realworld applications should be imbalance, especially follows the long-tailed law, which should be considered if we deploy the PLL methods into online systems. This thereby poses a new challenge about the robustness of algorithms to both category imbalance and label ambiguity in PLL studies.\n\nExisting efforts, partial label learning and long-tailed learning, independently study the partial aspect of this problem in the past decades. The standard PLL requires the label disambiguation from candidate sets along with the training of an ordinary classifier (Feng et al., 2020). The mainstream to solve this problem is estimating label-wise confidence to implicitly or explicitly re-weight the classification loss, e.g., PRODEN (Lv et al., 2020), LW (Wen et al., 2021), CAVL (Fei et al., 2022) and CORR (Wu et al., 2022), which have achieved the state-of-the-art performance in PLL. When it comes to the long-tailed learning, the core difficulty lies on diminishing the inherent bias induced by the heavy class imbalance (Chawla et al., 2002; Menon et al., 2013). The simple but fairly effective method is the logit adjustment (Menon et al., 2021; Ren et al., 2020), which has been demonstrated very powerful in a range of recent studies (Cui et al., 2021; Narasimhan & Menon, 2021).\n\nNevertheless, considering a more practical long-tailed partial label learning (LT-PLL) problem, several dilemma remains based on the above two paradigms. One straightforward concern is that the skewed long-tailed distribution exacerbates the bias to the head classes in the label disambiguation,\n\n†\n\nCorresponding to: Jiangchao Yao (Sunarker@sjtu.edu.cn) and Yanfeng Wang (wangyanfeng@sjtu.edu.cn).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Average classifier prediction (on the CIFAR-100 test set) of different methods during training (on LT-PLL training set CIFAR-100-LT with imbalance ratio ρ = 100 and ambiguity q = 0.05). “PRODEN” (Lv et al., 2020) is a popular PLL method. “PRODEN + Oracle-LA” denotes PRODEN with the state-of-the-art logit adjustment (Menon et al., 2021; Hong et al., 2021) in LT under the oracle prior. “PRODEN + RECORDS” is PRODEN with our proposed calibration. “Uniform” characterizes the expected average confidence on different classes.\n\neasily resulting in the trivial solution that are excessively confident to the head classes. More importantly, most state-of-the-art long-tailed learning methods cannot be directly used in LT-PLL, since they require the class distribution available that is agnostic in PLL due to the label ambiguities. In addition, we discover that even after applying an oracle class distribution prior in the training, existing techniques underperform in LT-PLL and even fail in some cases.\n\nIn Figure 1, we trace the average prediction of a PLL model PRODEN (Lv et al., 2020), on a uniform test set. Normally, the backbone PLL method PRODEN exhibits the biased prediction towards head classes shown by the blue curve, and ideally, we expect with the intervention of the state-of-the-art logit adjustment in LT, the prediction for all classes will be equally confident, namely, the purple curve. However, as can be seen, PRODEN calibrated by the oracle prior actually performs worse and is prone to over-adjusting towards the tail classes as shown in the orange curve. This is because logit adjustment in LT leverages a constant class distribution prior to rebalance the training and does not consider the dynamic of label disambiguation. Specially, at the early stage where the true label is very ambiguous from the candidate set, over-adjusting the logit only leads to the strong confusion of the classifier, which is negative to the overall training. Thus, we can see in Figure 1, the average prediction on the tail classes becomes too high along with the training.\n\nBased on the above analysis, compared with the previous constant rebalancing methods in PLL, a dynamic rebalancing mechanism friendly to the training dynamic will be more preferred. To this intuition, we propose a novel method, termed as REbalanCing fOR Dynamic biaS (RECORDS) for LT-PLL. Specifically, we perform a parametric decomposition of the biased model output and implement a dynamic adjustment by maintaining a prototype feature with momentum updates during training. The empirical and theoretical analysis demonstrate that our dynamic parametric class distribution is asymmetrically approaching to the statistical prior but benign to the overall training. A quick glance at the performance of RECORDS is the red curve in Figure 1, which approximately fits the expected purple curve in the whole training progress.\n\nThe contribution can be summarized as follows,\n\n1. We delve into a more practical but under-explored LT-PLL scenario, and identify its several challenges in this task that cannot be addressed and even lead to failure by the straightforward combination of the current long-tailed learning and partial label learning.\n\n2. We propose a novel RECORDS for LT-PLL that conducts the dynamic adjustment to rebalance the training without requiring any prior about the class distribution. The theoretical and empirical analysis show that the dynamic parametric class distribution is asymmetrically approaching to the oracle class distribution but more friendly to label disambiguation.\n\n3. Our method is orthogonal to existing PLL methods and can be easily plugged into the current PLL methods in an end-to-end manner. Extensive experiments on three benchmark datasets under the long-tailed setting and a range of PLL methods demonstrate the effectiveness of the proposed RECORDS. Specially, we show a 32.03% improvement in classification performance compared to the best CORR on the Pascal VOC dataset.\n\n2\n\ntraining stepdisambiguation confidence = 0.2disambiguation confidence = 0.5disambiguation confidence = 0.8Published as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\nPartial Label Learning (PLL). In PLL, each training sample is associated with a candidate label set containing the ground-truth. Early explorations is mainly average-based (H ̈ullermeier & Beringer, 2006; Zhang & Yu, 2015; Cour et al., 2011b), which treat all candidate labels equally during model training. Their drawback is that the model training is easily misled by the false positive labels that co-occur with the ground truth. Some other identification-based methods consider the truth label as a latent variable and optimize the objective under some criterions (Jin & Ghahramani, 2002; Liu & Dietterich, 2014; Nguyen & Caruana, 2008; Yu & Zhang, 2015). Recently, self-training methods (Feng et al., 2020; Wen et al., 2021; Lv et al., 2020; Fei et al., 2022) that gradually disambiguate the candidate label sets during training have achieved better results. PiCO (Wang et al., 2022b) introduces a contrastive learning branch that improves PLL performance by enhancing representation.\n\nLong-Tailed Learning (LT). In LT, several methods have been proposed to consider the extremely skewed distribution during training (Cui et al., 2019; Zhou et al., 2022; Chawla et al., 2002). Resampling (Kubat & Matwin, 1997; Wallace et al., 2011; Han et al., 2005) is one of the most widely used paradigm by down-sampling samples of head classes or up-sampling samples of tail classes. Re-weighting (Morik et al., 1999; Menon et al., 2013) adjusts the sample weights in the loss function during training. Transfer learning (Chu et al., 2020; Wang et al., 2021; Kim et al., 2020) seeks to transfer knowledge from head classes to tail classes to obtain a more balanced performance. Recently, logit adjustment techniques (Menon et al., 2021; Ren et al., 2020; Tian et al., 2020) that modify the output logits of the model by an offset term log Ptrain(y) have been the state-of-the-art.\n\nLong-Tailed Partial Label Learning (LT-PLL). A few works approximately explore the LT-PLL problem. Liu et al. (2021) implicitly alleviates data imbalance in the non-deep PLL by constraining the parameter space, which is hard to apply in deep learning context due to the complex optimization. Concurrently, SoLar (Wang et al., 2022a) improves the label disambiguation process in LT-PLL through the optimal transport technique. However, it requires an extra outer-loop to refine the label prediction via sinkhorn-knopp iteration, which increases the algorithm complexity. Different from these works, this paper tries to solve the LT-PLL problem from the perspective of rebalancing.\n\n3 PRELIMINARIES\n\n3.1 PROBLEM FORMULATION\n\nN\n\n{\n\n{\n\nY\n\nX\n\n=\n\nS (\n\n∈ Y\n\ni=1 ∈ }\n\n1, 2, ..., C\n\nDtrain =\n\nbe the input space and\n\nand its ground truth yi ∈\n\nbe the class space. The candidate label set space\n\nLet the powerset of denoted as\n\nY without the empty set: (xi, yi, Si)\n\nis }\n= 2Y . A long-tailed partial label training set can be )N , where any xi is associated with a candidate ,\n, Si is invisible. The sample number Nc of each class label set Si ⊂ Y in the descending order exhibits a long-tailed distribution. Let f (x; θ) denote a deep model c\nparameterized by θ, transforming x into an embedding vector. Then, the final output logits of x are given by z(x) = g(f (x; θ); W ) = W ⊤f (x; θ) where g is a linear classifier with the parameter matrix W . We leverage Θ = [θ, W ] to denote all parameters of the deep network. For evaluation, a is used (Brodersen et al., 2010). In a nutshell, the goal of class-balanced test set LT-PLL is to learn Θ on\n\nDtrain that minimizes the following balanced error rate (BER) on\n\nDuni =\n\nDuni:\n\n− ∅ S\n\n(xi, yi)\n\nX\n\nY\n\nS\n\n{\n\n}\n\nmin Θ\n\nBER(Θ)\n\nPuni(y)= 1 C=\n\nP(x,y)∈Duni(y\n\n= argmax y′∈Y\n\nzy′\n\n(x)).\n\n(1)\n\n3.2 SELF-TRAINING PLL METHODS\n\nSelf-training PLL methods maintain class-wise confidence weights w for each sample during training and formulate the loss function as a weighted summation of the classification loss by w. w is updated in each training round based on the model output and gradually converges to the one-hot labels, converting PLL to ordinary classification. Specially, PRODEN (Lv et al., 2020) assigns higher confidence weights to labels with larger output logit; LW (Wen et al., 2021) additionally considers the loss of non-partial labels and assigns the corresponding confidence weights to non-partial labels as well; CAVL (Fei et al., 2022) designs a identification strategy based on the idea of CAM and assigns hard confidence weights to labels; and CORR (Wu et al., 2022) applies consistency regularization in the disambiguation strategy. Please refer to Appendix B.1 for more details.\n\n3.3 LOGIT ADJUSTMENT FOR LONG-TAILED LEARNING\n\nWe briefly review logit adjustment (LA) (Menon et al., 2021; Hong et al., 2021), a powerful technique for supervised long-tailed learning. First, according to the Bayes rule, we have the underlying\n\n3\n\n̸ Published as a conference paper at ICLR 2023\n\nx)\n\nclass-probability P(y timal has softmax(zy(x)) = Ptrain(y zuni, it is about the balanced data, which has softmax(zy and Puni(y) = 1\n\nx) C (Menon et al., 2013). Then, we have the following relations\n\nP(y). Directly minimizing the cross-entropy loss to reach the opx) (Yu et al., 2018). However, for a BER-optimal output logit |\nPuni(y)\n\nuni(x)) = Puni(y\n\ny) |\n\ny) |\n\nP(x\n\nP(x\n\n∝\n\n∝\n\n·\n\n|\n\n|\n\n·\n\nPuni(y\n\nx) |\n\n∝\n\n∝ ∝\n\nP(x y) ·\n| Ptrain(y softmax(zy(x)\n\n|\n\nPtrain(y) / Ptrain(y) x) / Ptrain(y)\n\nlog Ptrain(y)).\n\n−\n\n(2)\n\nuni(x) = zy(x)\n\nThat is to say, if we have the logit zy(x) by training on standard cross-entropy loss, a BER-optimal logit zy log Ptrain(y) can be obtained by subtracting an offset term log Ptrain(y). Using zy y) of the output and removes the |\nimbalanced part Ptrain(y) in a statistical sense. LA has demonstrated its effectiveness on a range of recent long-tailed learning methods (Zhu et al., 2022; Cui et al., 2021).\n\nuni(x) as the test logit preserves the balanced part P(x\n\n−\n\n4 METHOD\n\nIn LT-PLL, the model output is biased to head classes, and using biased output to update confidence weights will result in biased re-weighting, namely, a tendency to identify a more frequent label in the candidate set as the ground truth. In turn, biased re-weighting on the loss function will further lead to a more severe imbalance on the model. In this section, we seek to propose a universal rebalancing algorithm that obtains unbiased output zuni from biased output z in self-training PLL methods.\n\n4.1 MOTIVATION\n\nPrevious long-tailed learning techniques assume no ambiguity in supervision and focus only on the model bias caused by the training set distribution, which we term as constant rebalancing. As shown in Figure 1, the constant rebalancing (Oracle-LA) fails to rebalance model in LT-PLL. This is because in LT-PLL, not only the skewed training set distribution, but also the ambiguous supervision can affect the training, since the inferred label changes continuously along with the label disambiguation process. Specially, at the early stage, the prediction imbalance of PRODEN is not significant. Using the more skewed training set distribution instead leads to a model that is heavily biased towards the tail classes, inducing the difficulty in label disambiguation. Therefore, a dynamic rebalancing method that considers the label disambiguation process can be intuitively more effective, e.g., RECORDS in Figure 1. In the following, we will concretely present our method.\n\n4.2 RECORDS: REBALANCING FOR DYNAMIC BIAS\n\nThe above analysis empirically suggests that the constant calibration by LA cannot match the dynamic of label disambiguation in LT-PLL. Formally, Equation 2 fails because the label disambiguation dynamically affects model optimization during training, which induces the mismatch between the ground truth Ptrain(y) and the training dynamics of the label disambiguation. To mediate the conflict, we propose a parametric decomposition on the original rebalancing paradigm:\n\nPuni(y\n\nx; Θ) |\n\n∝\n\n∝ ∝\n\nPtrain(y\n\nP(x y; Θ) |\nPtrain(y softmax(zy(x)\n\n·\n\n|\n\nx; Θ) / Ptrain(y\n\nΘ) / Ptrain(y |\nΘ) |\nlog Ptrain(y\n\nΘ)), |\n\n−\n\nΘ) |\n\n(3)\n\n|\n\nx; Θ) is the parametric class-probabilities under a uniform class prior. Here, we start where Puni(y a perspective of dynamic rebalancing: a dynamic Ptrain(y Θ) adapted to the training process is required instead of the constant prior Ptrain(y) to rebalance the model in LT-PLL. Existing PLL methods work to achieve better disambiguation results or improve the quality of the representation, i.e., learning P(x y). Thus, our study is orthogonal to existing PLL methods |\nand can be combined together to improve the performance (see Section 5.2). Also, the rebalanced output can boost the accuracy of label disambiguation and a better P(x\n\ny; Θ) that is closer to P(x |\n\n|\n\ny; Θ) can be learned. |\n\nHere, our effective and lightweight design is the estimation of the dynamic class distribution x; Θ). First, Ptrain(y\n\nΘ) by the model via the training set, i.e., Ptrain(y |\n\nΘ) = Exi∈DtrainPtrain(y |\n\n|\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Illustration for RECORDS. The class-wise confidence weights w are updating by the “disambiguation” module for each sample and used as soft/hard pseudo labels in PLL Loss. The main differences between the PLL baselines are the “PLL Loss” and the “disambiguation” module (see Tax; Θ). A balble 6). The “debias” module dynamically rebalances Ptrain(y x; Θ) helps tail samples to disambiguate labels more accurately and avoid being overanced Puni(y |\nwhelmed by head classes. A momentum-updated prototype feature is used to estimate Ptrain(y Θ), |\nwhich is benign to label disambiguation and asymmetrically approaching the oracle prior Ptrain(y). In comparison, constant rebalancing does not consider the dynamic of the label disambiguation.\n\nx; Θ) to obtain Puni(y |\n\n|\n\nwe use the Normalized Weighted Geometric Mean (NWGM) approximation (Baldi & Sadowski, 2013) to put the expectation operation inside the softmax as follows:\n\nPtrain(y\n\n|\n\nΘ) = Exi∈Dtrainsoftmax(zy(xi))\n\n≈\n\nNW GM\n\nsoftmax(Exi∈Dtrainzy(xi))\n\n= softmax(gy(Exi∈Dtrainf (xi; θ); W )).\n\n(4)\n\nNote that, NWGM approximation is widely used in dropout understanding (Baldi & Sadowski, 2014), caption generation (Xu et al., 2015), and causal inference (Wang et al., 2020). The intuition behind Equation 4 is to capture more stable feature statistics by means of NWGM and combine with Θ). Nevertheless, with Equation 4, we do not the latest updated linear classifier to estimate Ptrain(y |\nreach the final form, since directly estimating Ptrain(y Θ) requires to consider the whole dataset via |\nan EM alternation. To improve the efficiency, we design a momentum mechanism to accumulatively compute the expectation of features along with the training. Concretely, we maintain a prototype feature F for the entire training set, using each batch’s feature expectation for momentum updates:\n\nwhere m F yields the final implementation of our method:\n\n∈\n\n[0, 1) is a momentum coefficient. Then, replacing Exi∈Dtrainf (xi; θ) in Equation 4 by\n\nF\n\nmF + (1\n\nm)Exi∈Batchf (xi; θ),\n\n(5)\n\n←\n\n−\n\nzy uni(x) = zy(x)\n\nlog Ptrain(y\n\n|\n\n−\n\nΘ) = zy(x)\n\n−\n\nlog softmax(gy(F ; W )).\n\n(6)\n\nOur RECORDS is lightweight and can be easily plugged into existing PLL methods in an end-to-end manner. As illustrated in Figure 2, we insert a “debias” module before label disambiguation of each training iteration, converting Ptrain(y\n\nx; Θ) to Puni(y |\n\nx; Θ) via Equation 6. |\n\n4.3 RELATION BETWEEN DYNAMIC REBALANCING AND CONSTANT REBALANCING\n\nIn previous sections, we design a parametric class distribution to calibrate training in LT-PLL. HowΘ) and Ptrain(y). In this section, we theoretically ever, it is not clear about the relation of Ptrain(y |\npoint out their connection. First, let Ptrain(yj) = E(x,y)∼(X ,Y)1(y = yj), yj ∈ Y denote the oracle prior, where 1( ) denotes the indicator function. Considering a hypothesis space H where each ·\nf in Figure 2), we define the is a multiclass classifier parameterized by Θ (h = g hΘ . AsS during training. Then, the em- (cid:80)N =\n\nparametric class distribution for hΘ as Ptrain(yj| sume the disambiguated label for (x, y, S) in LT-PLL is ̃y(x, S) pirical risk on basis of the disambiguated label under\n\nΘ) = E(x,y)∼(X ,Y)1(hΘ(x) = yj), yj ∈ Y\n\nX → Y\n\n1(hΘ(xi)\n\nH :\n\n∈\n\n∈\n\n◦\n\nDtrain is RDtrain(Θ) = 1\n\nN\n\ni=1\n\n5\n\napproachingbatchupdatingAvgfeatureEMAacrossbatchtrainingstepshareweightLT-PLLDynamicRebalancingConstantRebalancingConstantPriorPLLLossdisambiguationdebiasDynamicEstimationofPtrain(y|Θ)̸ Published as a conference paper at ICLR 2023\n\nFigure 4: Visualization of the toy study. The first column illustrates the groud truth distribution of four classes on the training set and on the test set, marked by color. The right three columns exhibit the label identification from the candidate set for training samples in the first row and the predictions on the test set in the second row w.r.t. three methods.\n\n ̃y(xi, Si)). Motivated by the theoretical study on the learnability of PLL (Liu & Dietterich, 2014), we propose Proposition 1 to discuss the relation between dynamic and constant rebalancing.\n\nS) denote the ambiguity degree, Proposition 1. Let η = sup(x,y)∈X ×Y,yj ∈Y,yj ̸=y PS|(x,y)(yj ∈ dH be the Natarajan dimension of the hypothesis space H, ̃h = h ̃Θ be the optimal classifier on the basis of the label disambiguation, where ̃Θ = arg minΘ RDtrain(Θ). If the small ambiguity degree condition (Cour et al., 2011a; Liu & Dietterich, 2014)) satisfies, namely, η δ > 0, ̃Θ) given ̃h is bounded as the L2 distance between Ptrain(y) and Ptrain(y |\n\n[0, 1), then for\n\n∈\n\n∀\n\n(cid:17)\n\n(cid:16) ̃h\n\nL2\n\n4\n\n<\n\n(ln 2\n\n−\n\n−\n\nln(1 + η))N\n\n(dH (ln 2N + 2 ln C)\n\nln δ + ln 2)\n\n−\n\nwith probability at least 1\n\nδ, where N is the sample number and C is the category number.\n\n|\n\nProposition 1 yields an important implication that alongside the label disambiguation, the dynamic estimation can progressively approach to the oracle class distribution under small ambiguity degree. We kindly refer the readers to Appendix D for the complete proof. To further verify this, we trace the L2 distance between the estimation of Ptrain(y Θ) and the oracle Ptrain(y) during training in each epoch and visualize it in Figure 3(a). It can be observed that the distance between two distributions are gradually minimized, i.e.,, our parametric class distribution gradually converges to the statistical oracle class prior. Besides, as shown in Figure 3(b), the final estimated class distribution is very close to the oracle class prior. In total, Figure 3 indicates that our dynamic rebalancing is not only benign to the label disambiguation at the early stages, but also finally approach to the constant rebalancing.\n\nFigure 3: (a) L2 distance between the estimated class distribution and the oracle class prior during training. (b) The final estimated class distribution. Experiment is conducted by CORR + RECORDS on CIFAR-100-LT with imbalance ratio ρ = 100 and ambiguity q = 0.03.\n\n(a) L2 distance during training\n\n(b) Estimated class distribution\n\n5 EXPERIMENTS\n\n5.1 TOY STUDY\n\nWe simulate a four-class LT-PLL task where each class is distributed in different regions and their samples distribute uniformly in the corresponding regions. The label set of each sample consists of a true label and negative labels having a 0.6 probability flipped from other classes. In the first column of Figure 4, we visualize the training set and the test set: the training set is highly imbalanced among classes, with the number of samples from each class being 30 (red), 100 (yellow), 500 (blue) and 1000 (green), while the test set is balanced. In the right three columns of Figure 4, we show the results of identifying true labels from the candidate sets on the training set and the prediction results on the test set for different methods respectively. As can be seen, PRODEN results in the minority category (red) being completely overwhelmed by the majority categories (blue and green), and the\n\n6\n\nTrainingSetTestSetGroundTruthPRODENPRODEN+TP-LAdebiasPRODEN+RECORDSGround TruthPRODENPRODEN + Oracle-LAPRODEN + RECORDSTraining SetTest Set0100200300400500600700800Epoch0.040.050.060.070.080.09L2 distance020406080100Class index0.000.010.020.030.04ProbabilityOracle class priorDynamic estimationPublished as a conference paper at ICLR 2023\n\nTable 1: Top-1 accuracy on three benchmark datasets. Bold indicates the superior results.\n\nImbalance ratio ρ\n\nAmbiguity q\n\nCORR + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. CORR\n\nPRODEN + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. PRODEN\n\nLW + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. LW\n\nCAVL + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. CAVL\n\n0.3\n\n76.12 80.70 36.27 82.57 +6.45\n\n73.12 77.41 27.18 79.48 +6.36\n\n70.11 74.34 41.90 76.02 +5.91\n\n56.73 55.23 22.16 67.27 +10.54\n\n50\n\n0.5\n\n56.45 58.49 17.61 80.28 +23.83\n\n54.45 57.14 16.97 76.73 +22.28\n\n37.67 40.27 21.36 57.39 +19.72\n\n40.27 39.76 14.97 61.23 +20.96\n\nCIFAR-10-LT\n\n0.7\n\n0.3\n\n41.56 43.44 12.77 67.24 +25.68\n\n41.37 42.91 11.52 65.31 +23.94\n\n22.73 25.34 15.28 40.28 +17.55\n\n18.52 18.34 11.50 40.71 +22.19\n\n66.38 72.96 29.97 77.66 +11.28\n\n63.55 70.71 19.51 72.15 +8.60\n\n64.78 69.60 25.75 71.18 +6.40\n\n54.28 51.37 18.29 64.35 +10.07\n\nCIFAR-100-LT\n\n100\n\n0.5\n\n50.09 54.64 15.80 72.90 +22.81\n\n47.37 48.79 14.11 65.22 +17.85\n\n39.57 42.34 20.35 57.23 +17.66\n\n38.97 37.28 14.23 58.27 +19.30\n\n0.7\n\n38.11 41.66 11.75 57.46 +19.35\n\n38.06 41.38 11.17 52.26 +14.2\n\n23.54 27.35 14.24 41.24 +17.70\n\n17.28 14.58 10.67 37.38 +20.1\n\n50\n\n0.05\n\n38.03 40.76 5.59 45.56 +7.53\n\n35.45 38.64 4.09 41.31 +5.86\n\n29.50 28.80 14.43 31.67 +2.17\n\n0.07\n\n36.59 39.07 3.12 42.51 +5.92\n\n33.90 35.82 2.64 39.26 +5.36\n\n27.86 27.27 4.79 29.39 +1.53\n\n0.03\n\n42.29 46.94 22.56 48.06 +5.77\n\n39.23 43.40 12.37 44.56 +5.33\n\n35.54 35.47 30.37 36.56 +1.02\n\n29.63 29.65 17.31 42.25 +12.62\n\n17.31 14.86 4.36 36.53 +19.22\n\n8.34 5.76 2.83 29.13 +14.27\n\n100\n\n0.05\n\n34.09 36.79 3.32 40.59 +6.40\n\n32.04 35.20 2.73 37.23 +5.19\n\n28.09 26.96 5.08 28.85 +0.76\n\n25.39 26.27 2.55 31.49 +6.10\n\n0.07\n\n31.05 33.32 1.98 38.65 +7.60\n\n29.40 31.92 1.98 35.26 +5.86\n\n24.65 23.20 2.70 25.64 +0.99\n\n8.20 5.80 2.03 24.98 +16.78\n\n0.03\n\n38.39 41.49 11.37 42.25 +3.86\n\n34.52 38.40 6.79 39.13 +4.61\n\n31.58 31.03 30.30 33.00 +1.42\n\n28.29 28.34 7.24 36.93 +8.64\n\nPASCAL VOC\n\n24.43 34.12 52.51 56.46 +32.03\n\n22.39 31.53 48.33 52.65 +30.26\n\n19.41 21.06 51.53 53.09 +33.68\n\n17.25 22.27 50.78 53.07 +35.82\n\nTable 2: Fine-grained analysis on CIFAR-100-LT with ρ = 100 and q Many/Medium/Few corresponds to three partitions on the long-tailed data.\n\n0.03, 0.05, 0.07\n\n∈ {\n\n. }\n\nMethod\n\nq = 0.03\n\nq = 0.05\n\nq = 0.07\n\nMany Medium\n\nFew\n\nOverall Many Medium\n\nFew\n\nOverall Many Medium\n\nFew\n\nOverall\n\nCORR + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. CORR\n\n68.43 70.37 11.03 66.37 -2.06\n\n37.40 41.89 12.34 42.54 +5.14\n\n4.50 7.33 10.63 13.77 +9.27\n\n38.39 41.49 11.37 42.25 +3.86\n\n67.51 70.46 0.34 68.49 +0.98\n\n29.60 33.40 4.46 40.20 +10.60\n\n0.33 1.47 5.47 8.50 +8.17\n\n34.09 36.79 3.32 40.59 +6.50\n\n68.86 69.77 0.00 69.97 +1.11\n\n19.80 24.86 0.71 36.71 +16.91\n\n0.07 0.67 5.77 4.37 +4.30\n\n31.05 33.32 1.98 38.65 +7.60\n\nyellow category being dominated mostly. After calibration with the oracle class prior, PRODEN + Oracle-LA instead predicts all data in the test set as the minority class (red). This suggests that constant rebalancing is prone to being over-adjusted towards tail classes coupling with the label disambiguation dynamic. In comparison, when using RECORDS as a dynamic rebalancing mechanism in LT-PLL, we achieve the desired results on both the training set and the test set.\n\n5.2 BENCHMARK RESULTS\n\nLong-tailed partial label datasets. We evaluate RECORDS on three datasets: CIFAR-10-LT (Liu et al., 2019), CIFAR-100-LT (Liu et al., 2019) and PASCAL VOC. For CIFAR-10-LT and CIFAR- .\n100-LT, we build the long-tailed version of of CIFAR-10/100 with imbalanced ratio ρ }\nFollowing Lv et al. (2020); Wen et al. (2021), we adopt the uniform setting in CIFAR-10-LT and CIFAR-100-LT, i.e., P(y = y) = q to generate candidate label set of each sample. PASCAL VOC is a real-world LT-PLL dataset constructed from PASCAL VOC 2007 (Everingham et al.). Specifically, we crop objects in images as instances and all objects appearing in the same original image are regarded as the labels of a candidate set and empirically we can observe the significant class imbalance. Note that, here we are the first to conduct experiments based on deep models on real-world datasets, while previous PLL real-world datasets are tabular and only suitable for linear models or shallow MLPs (Fei et al., 2022). Please refer to Appendix E.1 for more details.\n\n50, 100\n\n∈ {\n\ny |\n\n∈\n\nS\n\nBaselines. We consider the state-of-the-art PLL algorithms including PRODEN (Lv et al., 2020), LW (Wen et al., 2021), CAVL (Fei et al., 2022), and CORR (Wu et al., 2022) and their combination with the state-of-the-art rebalancing method logit adjustment (LA) (Menon et al., 2021; Hong et al., 2021). We use two rebalancing variants: (1) Oracle-LA: rebalance a PLL model during training with the oracle class prior by LA; (2) Oracle-LA post-hoc: rebalance a pre-trained PLL model with the oracle prior using LA in a post-hoc way. Note that, for our methods, we directly apply RECORDS into PLL baselines but without requiring the oracle class prior. For comparisons with concurrent LT-PLL work SoLar (Wang et al., 2022a), please refer to Appendix E.7.\n\nImplementation details. We use 18-layer ResNet as the backbone. The standard data augmentations are applied as in Cubuk et al. (2020). The mini-batch size is set to 256 and all the methods are trained using SGD with momentum of 0.9 and weight decay of 0.001 as the optimizer. The hyper-parameter m in Equation 5 is set to 0.9 constantly. The initial learning rate is set to 0.01. We train the model for 800 epochs with the cosine learning rate scheduling.\n\n7\n\n̸ Published as a conference paper at ICLR 2023\n\nFigure 5: The per-class accuracy on CIFAR-10-LT with ρ = 100 and q\n\nTable 3: Fine-grained analysis on CIFAR-100-LT-NU with ρ = 100 and q\n\nq = 0.03\n\nq = 0.05\n\n∈ {\n\n∈ {\n\n0.3, 0.5, 0.7\n\n.\n\n}\n\n0.03, 0.05, 0.07\n\nq = 0.07\n\n.\n\n}\n\nMethod\n\nMany Medium\n\nFew\n\nOverall Many Medium\n\nFew\n\nOverall Many Medium\n\nFew\n\nOverall\n\nCORR + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. CORR\n\n66.71 69.80 3.26 65.60 -1.11\n\n34.57 40.43 9.97 41.26 +6.69\n\n3.13 4.57 13.10 11.77 +8.44\n\n36.39 39.95 8.56 40.93 +4.54\n\n70.23 71.20 0.86 66.40 -3.83\n\n21.57 28.77 5.34 38.71 +17.14\n\n0.13 1.30 12.27 8.57 +8.44\n\n32.17 35.38 5.85 39.36 +7.19\n\n65.37 68.71 0.00 62.09 -3.28\n\n16.11 20.17 0.25 37.40 +21.29\n\n0.13 1.10 16.87 7.47 +7.34\n\n28.56 31.44 5.92 37.06 +8.50\n\nTable 4: Comparision with more baselines for imbalanced learning.\n\nImbalance ratio ρ\n\nambiguity q\n\nCORR + Oracle-BCL + Oracle-PaCO + LDD + SFN + RECORDS\n\n50\n\n0.5\n\n56.45 25.24 26.34 58.02 57.13 80.28\n\n0.3\n\n76.12 43.76 42.45 75.23 75.98 82.57\n\nCIFAR-10-LT\n\n0.7\n\n0.3\n\n41.56 17.34 16.23 42.14 41.77 67.24\n\n66.38 33.23 34.21 67.59 66.91 77.66\n\n100\n\n0.5\n\n50.09 18.34 18.23 51.38 50.23 72.90\n\nCIFAR-100-LT\n\n50\n\n0.7\n\n0.03\n\n0.05\n\n0.07\n\n0.03\n\n38.11 15.72 14.23 39.28 38.71 57.46\n\n42.29 27.23 27.37 42.91 43.14 48.06\n\n38.03 10.34 11.23 39.05 38.62 45.56\n\n36.59 8.34 7.23 36.98 37.03 42.51\n\n38.39 14.34 15.82 39.20 39.31 42.25\n\n100\n\n0.05\n\n34.09 5.82 5.62 34.87 33.91 40.59\n\n0.07\n\n31.05 4.10 4.72 32.15 31.67 38.65\n\nOverall performance. In Table 1, we summarize the Top-1 accuracy on three benchmark LT-PLL datasets. Our method clearly exhibits superior performance on all datasets with different experimental settings under CORR, PRODEN, LW and CAVL. Specially, compared to the best PLL baseline CORR, RECORDS significantly improve the accuracy by 6.45%-25.68% on CIFAR-10-LT, 3.86%- 7.60% on CIFAR-100-LT, and 32.03% on PASCAL VOC. When Oracle-LA is applied into PLL baselines, it induces severe performance degradation on CIFAR but improves significantly on PASCAL VOC. In comparison, Oracle-LA post-hoc can better alleviates the class imbalance on CIFAR. However, both of them cannot address the adverse effect of constant rebalancing on label disambiguation. In comparison, our RECORDS that solves this problem through dynamic rebalancing during training, achieves the consistent and the best improvements among all methods.\n\nFine-grained analysis. In Figure 5, we visualize the per-class accuracy on CIFAR-10-LT with ρ = 100 under the best PLL baseline CORR. As expected, dominant classes generally exhibits a higher accuracy in CORR. Oracle-LA post-hoc can improve CORR performance on medium classes, but accuracy remains poor on tail classes, especially when label ambiguity is high. Oracle-LA performs very poorly on head and medium classes due to over-adjustment towards tail classes. Our RECORDS systematically improves performance over CORR, particularly on rare classes. In Table 2, we show the Many-Medium-Few1 accuracies of different methods on CIFAR-100-LT with ρ = 100. Our RECORDS shows the significant and consistent gains on Medium and Few classes when combined with CORR. As demonstrated in many long-tailed learning literatures (Kang et al., 2020; Menon et al., 2021), there might be a head-to-tail accuracy tradeoff in LT-PLL, and our method achieve the best overall accuracy in this tradeoff. More results are summarized in Appendix E.4.\n\n5.3 FURTHER ANALYSIS\n\nNon-uniform candidates generation. Real-world annotation ambiguity often occurs among semantically close labels. To evaluate the performance of RECORDS in practical scenarios, we conduct experiments on a more challenging dataset, namely CIFAR-100-LT-NU. To build CIFAR100-LT-NU, we generate candidates from the ground truth of CIFAR-100-LT using a Non-uniform setting. Specifically, labels in the same superclass of the ground truth have a higher probability to be selected into the candidate set, i.e., P(y = y, D(y) = D(y)) = 8q, where D(y) denotes the superclass to which y belongs. In Table 3, we show the fine- . Combined with grained analysis on CIFAR-100-LT-NU with ρ = 100 and q\n\n= D(y)) = q, P(y\n\n0.03, 0.05, 0.07\n\n= y, D(y)\n\ny |\n\n∈\n\n∈\n\nS\n\nS\n\ny\n\n|\n\n∈ {\n\n}\n\n1Many/Medium/Few corresponds to classes with >100, [20, 100], and <20 images (Kang et al., 2020).\n\n8\n\n0123456789Class Index0.00.20.40.60.81.0Accuracyq=0.30123456789Class Index0.00.20.40.60.81.0Accuracyq=0.50123456789Class Index0.00.20.40.60.81.0Accuracyq=0.7CORR+ Oracle-LA post-hoc+ Oracle-LA+ RECORDSCORR+ Oracle-LA post-hoc+ Oracle-LA+ RECORDSCORR+ Oracle-LA post-hoc+ Oracle-LA+ RECORDS̸ ̸\n̸ Published as a conference paper at ICLR 2023\n\nTable 5: Comparison with other dynamic strategies on CIFAR-10-LT and CIFAR-100-LT.\n\nImbalance ratio ρ\n\nAmbiguity q\n\nCORR + Temp Oracle-LA + Epoch RECORDS + RECORDS\n\n50\n\n0.5\n\n56.45 43.62 70.27 80.28\n\n0.3\n\n76.12 81.37 75.43 82.57\n\nCIFAR-10-LT\n\n0.7\n\n0.3\n\n41.56 18.10 59.50 67.24\n\n66.38 76.09 69.38 77.66\n\n100\n\n0.5\n\n50.09 25.88 63.12 72.90\n\nCIFAR-100-LT\n\n50\n\n0.7\n\n0.03\n\n0.05\n\n0.07\n\n0.03\n\n38.11 16.11 47.85 57.46\n\n42.29 47.44 46.54 48.06\n\n38.03 43.46 43.07 45.56\n\n36.59 29.75 38.28 42.51\n\n38.39 41.78 41.58 42.25\n\n100\n\n0.05\n\n34.09 39.19 37.14 40.59\n\n0.07\n\n31.05 33.69 34.38 38.65\n\n(a) Linear Probing\n\n(b) Ablation on m\n\nFigure 6: (a) Top-1 accuracy (left) and top-5 accuracy (right) under different shots of linear probing for different methods pretrained on CIFAR-100-LT (ρ = 100, q = 0.05). CORR + Oracle-LA posthoc is same to PRODEN in terms of features and thus is not plotted. (b) Performance of CORR + RECORDS with varying m on CIFAR-100-LT (ρ = 100, q = 0.05).\n\nour RECORDS, the best baseline CORR achieves significant gains, demonstrating the robustness of RECORDS in different scenarios. More results are summarized in Appendix E.3.\n\nLinear probing performance. Following the literatures of self-supervised learning (Chen et al., 2020; He et al., 2020), we conduct linear probing on CIFAR-100-LT with ρ = 100 and q = 0.05 to quantitatively evaluate the representation quantity of different methods. To eliminate the class imbalance effect, the linear classifier is trained on a balanced dataset. From Figure 6(a), our RECORDS It indicates that our consistently outperforms baselines in this evaluation under different shots. RECORDS that considers the label disambiguation dynamic can help extract better representation.\n\nOther dynamic strategies. To verify the effectiveness of RECORD, we setup two other straightforward dynamic strategies: (1) Temp Oracle-LA: temperature scaling Ptrain(y) from 0 to 1 during training; (2) Epoch RECORDS: use the prediction of the latest epoch to estimate the class distribution and rebalance. In Table 5, we can find that although simper solutions might be effective, RECORDS outperforms them significantly, confirming the effectiveness of our design.\n\nMore baselines for imbalanced learning. We additionally compare our RECORDS with two contrastive-based SOTA long-tailed learning baselines, BCL (Zhu et al., 2022) and PaCO (Cui et al., 2021), and two regularization methods for mitigating imbalance in PLL, LDD (Liu et al., 2021) and SFN (Liu et al., 2021). Similar to Oracle-LA, we use the oracle class distributions for BCL and PaCO, denoted as Oracle-BCL and Oracle-PaCO. In Table 4, RECORDS still significantly outperforms these methods, showing the effectiveness of dynamic rebalancing.\n\nEffect of the momentum coefficient m. In Figure 6(b), we explore the effect of the momentum update factor m on the performance of RECORDS. As can be seen, the best result is achieved at m = 0.9 and the performance decreases when m takes smaller values or a larger value. Specially, when m = 0, it still maintains a competitive result, showing the robustness of RECORDS. At the other extreme value, i.e., m = 1.0, CORR + RECORDS actually degenerates to CORR.\n\n6 CONCLUSION\n\nIn this paper, we focus on a more practical LT-PLL scenario and identify several critical challenges in this task based on previous independent research paradigms LT and PLL. To avoid the drawback of their combination, we propose a novel method for LT-PLL, RECORDS, without requiring the prior of the oracle class distribution. Both empirical and theoretical analysis show that our proposed parametric class distribution is asymmetrically approach the static oracle class prior during training and is more friendly to label disambiguation. Our method is orthogonal to existing PLL methods and can be easily plugged into current PLL methods in an end-to-end manner. Extensive experiments demonstrate the effectiveness of our proposed RECORDS. In the future, the extension of RECORDS to other weakly supervised learning scenarios can be explored in a more general scope.\n\n9\n\nFull-shot100-shot50-shotEvaluation Setting0.150.200.250.300.350.400.450.500.55Top-1 AccuracyFull-shot100-shot50-shotEvaluation Setting0.400.450.500.550.600.650.700.750.80Top-5 AccuracyCORR+ Oracle-LA+ RECORDSCORR+ Oracle-LA+ RECORDS0200400600800Epoch0.100.150.200.250.300.350.40Accuracym=0.0m=0.3m=0.6m=0.9m=1.0Published as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nThis paper does not raise any ethics concerns. This study does not involve any human subjects, practices to data set releases, potentially harmful insights, methodologies and applications, potential conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security issues, legal compliance, and research integrity issues.\n\nREPRODUCIBILITY STATEMENT\n\nTo ensure the reproducibility of experimental results, our code is available at https://github. com/MediaBrain-SJTU/RECORDS-LTPLL. We provide experimental setups and implementation details in Section 5 and Appendix E. The proof of Proposition 1 is given in Appendix D.\n\nACKNOWLEDGMENTS\n\nThis work is supported by STCSM (No. 22511106101, No. 18DZ2270700, No. 21DZ1100100), 111 plan (No. BP0719010), and State Key Laboratory of UHD Video and Audio Production and Presentation.\n\nREFERENCES\n\nPierre Baldi and Peter J. Sadowski. Understanding dropout.\n\nIn Christopher J. C. Burges, L ́eon\n\nBottou, Zoubin Ghahramani, and Kilian Q. Weinberger (eds.), NeurIPS, pp. 2814–2822, 2013.\n\nPierre Baldi and Peter J. Sadowski. The dropout learning algorithm. Artif. Intell., 210:78–122, 2014.\n\nKay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M. Buhmann. The balanced accuracy and its posterior distribution. In ICPR, pp. 3121–3124. IEEE Computer Society, 2010.\n\nNitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. SMOTE:\n\nsynthetic minority over-sampling technique. J. Artif. Intell. Res., 16:321–357, 2002.\n\nChing-Hui Chen, Vishal M. Patel, and Rama Chellappa. Learning from ambiguously labeled face\n\nimages. IEEE Trans. Pattern Anal. Mach. Intell., 40(7):1653–1667, 2018.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, volume 119 of Proceedings of Machine Learning Research, pp. 1597–1607. PMLR, 2020.\n\nPeng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for long-tailed In ECCV, volume 12374 of Lecture Notes in Computer Science, pp. 694–710. Springer,\n\ndata. 2020.\n\nTimothee Cour, Ben Sapp, and Ben Taskar. Learning from partial labels. The Journal of Machine\n\nLearning Research, 12:1501–1536, 2011a.\n\nTimoth ́ee Cour, Benjamin Sapp, and Ben Taskar. Learning from partial labels. J. Mach. Learn. Res.,\n\n12:1501–1536, 2011b.\n\nEkin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Practical automated\n\ndata augmentation with a reduced search space. In NeurIPS, 2020.\n\nJiequan Cui, Zhisheng Zhong, Shu Liu, Bei Yu, and Jiaya Jia. Parametric contrastive learning. In\n\nICCV, pp. 695–704. IEEE, 2021.\n\nYin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge J. Belongie. Class-balanced loss based on effective number of samples. In CVPR, pp. 9268–9277. Computer Vision Foundation / IEEE, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nM. Everingham, L. Van Gool, C. K.\n\nI. Williams,\n\nPASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. network.org/challenges/VOC/voc2007/workshop/index.html.\n\nJ. Winn, and A. Zisserman.\n\nThe http://www.pascal-\n\nZhang Fei, Feng Lei, Han Bo, Liu Tongliang, Niu Gang, Qin Tao, and Sugiyama Masashi. Exploit-\n\ning class activation value for partial-label learning. In ICLR. OpenReview.net, 2022.\n\nLei Feng, Jiaqi Lv, Bo Han, Miao Xu, Gang Niu, Xin Geng, Bo An, and Masashi Sugiyama. Prov-\n\nably consistent partial-label learning. In NeurIPS, 2020.\n\nChen Gong, Tongliang Liu, Yuanyan Tang, Jian Yang, Jie Yang, and Dacheng Tao. A regularization approach for instance-based superset label learning. IEEE Trans. Cybern., 48(3):967–978, 2018.\n\nHui Han, Wenyuan Wang, and Binghuan Mao. Borderline-smote: A new over-sampling method in imbalanced data sets learning. In ICIC, volume 3644 of Lecture Notes in Computer Science, pp. 878–887. Springer, 2005.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pp. 9726–9735. Computer Vision Foundation / IEEE, 2020.\n\nYoungkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, and Buru Chang. In CVPR, pp. 6626–6636.\n\nDisentangling label distribution for long-tailed visual recognition. Computer Vision Foundation / IEEE, 2021.\n\nEyke H ̈ullermeier and J ̈urgen Beringer. Learning from ambiguously labeled examples. Intell. Data\n\nAnal., 10(5):419–439, 2006.\n\nRong Jin and Zoubin Ghahramani. Learning with multiple labels. In NeurIPS, pp. 897–904. MIT\n\nPress, 2002.\n\nBingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and YanIn ICLR.\n\nnis Kalantidis. Decoupling representation and classifier for long-tailed recognition. OpenReview.net, 2020.\n\nJaehyung Kim, Jongheon Jeong, and Jinwoo Shin. M2m: Imbalanced classification via major-to-\n\nminor translation. In CVPR, pp. 13893–13902. Computer Vision Foundation / IEEE, 2020.\n\nMiroslav Kubat and Stan Matwin. Addressing the curse of imbalanced training sets: One-sided\n\nselection. In ICML, pp. 179–186. Morgan Kaufmann, 1997.\n\nLi-Ping Liu and Thomas G. Dietterich. A conditional multinomial mixture model for superset label\n\nlearning. In NeurIPS, pp. 557–565, 2012.\n\nLi-Ping Liu and Thomas G. Dietterich. Learnability of the superset label learning problem.\n\nIn ICML, volume 32 of JMLR Workshop and Conference Proceedings, pp. 1629–1637. JMLR.org, 2014.\n\nWenpeng Liu, Li Wang, Jie Chen, Yu Zhou, Ruirui Zheng, and Jianjun He. A partial label metric learning algorithm for class imbalanced data. In Vineeth N. Balasubramanian and Ivor W. Tsang (eds.), ACML, volume 157 of Proceedings of Machine Learning Research, pp. 1413–1428. PMLR, 2021.\n\nZiwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Largescale long-tailed recognition in an open world. In CVPR, pp. 2537–2546. Computer Vision Foundation / IEEE, 2019.\n\nJie Luo and Francesco Orabona. Learning from candidate labeling sets. In NeurIPS, pp. 1504–1512.\n\nCurran Associates, Inc., 2010.\n\nJiaqi Lv, Miao Xu, Lei Feng, Gang Niu, Xin Geng, and Masashi Sugiyama. Progressive identification of true labels for partial-label learning. In ICML, volume 119 of Proceedings of Machine Learning Research, pp. 6500–6510. PMLR, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAditya Krishna Menon, Harikrishna Narasimhan, Shivani Agarwal, and Sanjay Chawla. On the In ICML,\n\nstatistical consistency of algorithms for binary classification under class imbalance. volume 28 of JMLR Workshop and Conference Proceedings, pp. 603–611. JMLR.org, 2013.\n\nAditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and\n\nSanjiv Kumar. Long-tail learning via logit adjustment. In ICLR. OpenReview.net, 2021.\n\nKatharina Morik, Peter Brockhausen, and Thorsten Joachims. Combining statistical learning with a knowledge-based approach - A case study in intensive care monitoring. In ICML, pp. 268–277. Morgan Kaufmann, 1999.\n\nHarikrishna Narasimhan and Aditya Krishna Menon. Training over-parameterized models with non-\n\ndecomposable objectives. In NeurIPS, pp. 18165–18181, 2021.\n\nB. K. Natarajan. On learning sets and functions. Mach. Learn., 4:67–97, 1989.\n\nNam Nguyen and Rich Caruana. Classification with partial labels. In SIGKDD, pp. 551–559. ACM,\n\n2008.\n\nJiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Bal-\n\nanced meta-softmax for long-tailed visual recognition. In NeurIPS, 2020.\n\nJunjiao Tian, Yen-Cheng Liu, Nathaniel Glaser, Yen-Chang Hsu, and Zsolt Kira. Posterior re-\n\ncalibration for imbalanced datasets. In NeurIPS, 2020.\n\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine\n\nLearning Research, 9(86):2579–2605, 2008.\n\nByron C. Wallace, Kevin Small, Carla E. Brodley, and Thomas A. Trikalinos. Class imbalance,\n\nredux. In ICDM, pp. 754–763. IEEE Computer Society, 2011.\n\nHaobo Wang, Mingxuan Xia, Yixuan Li, Yuren Mao, Lei Feng, Gang Chen, and Junbo Zhao. Solar:\n\nSinkhorn label refinery for imbalanced partial-label learning. In NeurIPS, 2022a.\n\nHaobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, and Junbo Zhao. Pico: Contrastive label disambiguation for partial label learning. In ICLR. OpenReview.net, 2022b.\n\nJianfeng Wang, Thomas Lukasiewicz, Xiaolin Hu, Jianfei Cai, and Zhenghua Xu. RSG: A simple but effective module for learning imbalanced datasets. In CVPR, pp. 3784–3793. Computer Vision Foundation / IEEE, 2021.\n\nTan Wang, Jianqiang Huang, Hanwang Zhang, and Qianru Sun. Visual commonsense R-CNN. In\n\nCVPR, pp. 10757–10767. Computer Vision Foundation / IEEE, 2020.\n\nHongwei Wen, Jingyi Cui, Hanyuan Hang, Jiabin Liu, Yisen Wang, and Zhouchen Lin. Leveraged weighted loss for partial label learning. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 11091–11100. PMLR, 2021.\n\nDong-Dong Wu, Deng-Bao Wang, and Min-Ling Zhang. Revisiting consistency regularization for deep partial label learning. In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 24212–24225. PMLR, 2022.\n\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In Francis R. Bach and David M. Blei (eds.), ICML, volume 37 of JMLR Workshop and Conference Proceedings, pp. 2048–2057. JMLR.org, 2015.\n\nFei Yu and Min-Ling Zhang. Maximum margin partial label learning. In ACML, volume 45 of JMLR\n\nWorkshop and Conference Proceedings, pp. 96–111. JMLR.org, 2015.\n\nXiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary labels. In ECCV, volume 11205 of Lecture Notes in Computer Science, pp. 69–85. Springer, 2018.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nZinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, and Yi Ma. Learning In CVPR, pp. 708–715. IEEE Computer Society,\n\nby associating ambiguously labeled images. 2013.\n\nHongyi Zhang, Moustapha Ciss ́e, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n\nMin-Ling Zhang and Fei Yu. Solving the partial label learning problem: An instance-based ap-\n\nproach. In IJCAI, pp. 4048–4054. AAAI Press, 2015.\n\nZhihan Zhou, Jiangchao Yao, Yan-Feng Wang, Bo Han, and Ya Zhang. Contrastive learning with boosted memorization. In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 27367–27377. PMLR, 2022.\n\nJianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang Jiang. Balanced contrastive learning for long-tailed visual recognition. In CVPR, pp. 6898–6907. IEEE, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\nCONTENTS\n\nAppendix A: Pracatical Background of Long-Tailed Partial Label Learning.\n\nAppendix B: Preliminaries (Section 3).\n\nAppendix C: Pseudo-Code of RECORDS (Section 4.2).\n\nAppendix D: Proof of Proposition 1 (Section 4.3).\n\nAppendix E: Detailed Supplement for Experiments (Section 5).\n\nA LONG-TAILED PARTIAL LABEL LEARNING: BACKGROUND\n\nThe remarkable success of deep learning is built on a large amount of labeled data. Data annotation in real-world scenarios often suffers from annotation ambiguity. To address annotation ambiguity, partial label learning allows multiple candidate labels to be annotated for each training instance, which can be widely used in web mining (Luo & Orabona, 2010), automatic image annotations (Zeng et al., 2013; Chen et al., 2018), ecoinformatics (Liu & Dietterich, 2012), and crowdsourcing (Gong et al., 2018).\n\nFor example, a movie clip may contain several characters talking to each other, with some of them appearing in a screenshot. Although we can obtain scripts and dialogues that indicate the names of the characters, we cannot directly confirm the real name of each face in the screenshot (see Figure 7(a)). A similar scenario arises for recognizing faces from news images, where we can obtain the names of the people from the news descriptions but cannot establish a one-to-one correspondence with the face images (see Figure 7(b)). Partial label learning problem also appears in crowdsourcing, where each instance may be given multiple labels by different annotators. However, some labels may be incorrect or biased due to differences in expertise or cultural background of different annotators, so it is necessary to find the most appropriate label for each instance from candidate labels (see Figure 7(c)).\n\nMeanwhile, in the real world, the data naturally exhibit a long-tailed distribution. Corresponding to the three examples in Figure 7, the main characters in movies tend to take up most of the time, while the supporting roles appear much less frequently; in sports news, superstars get most of the exposure, while many role players only get few appearances; and the number of different species in nature also shows a clear long-tailed distribution. However, such common long-tailed distribution of real data has been ignored by most existing PLL methods. Further, the invisibility of ground truth in PLL makes it difficult to even manually balance the dataset. Therefore, we focus on the more difficult but common LT-PLL scenario where label ambiguity and category imbalance co-occur.\n\nWhen facing LT-PLL, directly combining the paradigms of partial label learning and long-tailed learning poses some dilemmas. One immediate problem is that the skewed long-tailed distribution exacerbates the bias toward head classes in label disambiguation and tends to lead to trivial solutions with overconfidence in head classes. More importantly, most state-of-the-art long-tailed learning methods cannot be directly used in LT-PLL because they require available class distributions, which are agnostic in PLL due to label ambiguity. Moreover, we find that existing techniques perform poorly in LT-PLL and even fail in some cases even after applying an oracle class distribution prior in the training.\n\nB PRELIMINARIES\n\nB.1 SELF-TRAINING PLL METHODS\n\nThe self-training PLL methods (Feng et al., 2020; Wen et al., 2021; Lv et al., 2020; Fei et al., 2022) remove annotation ambiguities from model outputs and gradually identify true labels during training, achieving state-of-the-art results. These methods maintain class-wise confidence weights w for each sample during training and formulate the loss function as a weighted summation of the classification\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Some practical applications of PLL. (a) A screenshot of the movie “The Amazing SpiderMan” and the corresponding dialogue script. (b) An image from NBA news with the caption “The History of LeBron James and Stephen Curry’s Rivalrous Friendship”. (c) an image of “Russian Blue”, which is very similar to “Korat”.\n\nloss by w. They update the confidence weights w from the output, then use w as soft/hard pseudolabels to learn classifiers from the data. Model training and weight updates are performed iteratively in each round. In the late training period, w converges to stable one-hot labels, and PLL is converted to an ordinary classification problem that learns from the disambiguated labels. The differences among these self-training methods are the form of loss function and how the weights are updated. In Tabel 6, we summarize four state-of-the-art self-training methods, PRODEN (Lv et al., 2020), LW (Wen et al., 2021), CAVL (Fei et al., 2022), and CORR (Wu et al., 2022). Specially, PRODEN assigns higher confidence weights to labels with larger output logit; LW additionally considers the loss of non-partial labels and assigns the corresponding confidence weights to non-partial labels as well; CAVL designs a identification strategy based on the idea of CAM and assigns hard confidence weights to labels; and CORR applies consistency regularization in the disambiguation strategy.\n\nTable 6: The loss functions and confidence weights updating strategies for self-training PLL methods.\n\nMethods\n\nPer-Sample Loss\n\nUpdating w\n\nPRODEN (cid:80)\n\nj∈Y wi,jl(zj(xi))\n\nwi,j =\n\nexp(zj (xi))\n\nexp(zk(xi)) , if j\n\n(cid:80)\n\nk∈Si\n\nSi\n\n∈\n\nwi,j = 0, if j / ∈\n\nSi\n\nLW\n\n(cid:80)\n\nj∈Si\n\nwi,jl(zj(xi))+\n\nwi,j =\n\nβ (cid:80)\n\nj /∈Si\n\nwi,jl(\n\n−\n\nzj(xi)) wi,j =\n\nexp(zj (xi))\n\nexp(zk(xi)) , if j\n\n(cid:80)\n\nk∈Si\n\n∈\n\nexp(zj (xi))\n\n(cid:80)\n\nk /∈Si\n\nexp(zk(xi)) , if j / ∈\n\nSi\n\nSi\n\nCAVL\n\n(cid:80)\n\nj∈Y wi,jl(zj(xi))\n\nwi,j = 1, if j = argmaxj∈Si\n\n(cid:12) (cid:12)zj(xi)\n\n−\n\n(cid:12) (cid:12) zj(xi) 1\n\nwi,j = 0, else\n\nCORR\n\nλDKL(z(xi)\n\nwi)+\n\n∥\n\nwi,j =\n\n((cid:81)\n\nx′ ∈A(xi) exp(zj (x′)))\n\n1 |A(xi)|\n\n(cid:80)\n\nk∈Y ((cid:81)\n\nx′ ∈A(xi) exp(zk(x′)))\n\n1 |A(xi)|\n\n, if j\n\nSi\n\n∈\n\n(cid:80)\n\nj /∈Si l(\n\nzj(xi))\n\n−\n\nwi,j = 0, if j / ∈\n\nSi\n\nB.2 LOGIT ADJUSTMENT FOR LONG-TAILED LEARNING\n\nHere we give a review on the successful logit adjustment (LA) (Menon et al., 2021; Hong et al., 2021) in long-tailed learning. LA uses the Bayes Rule to remove the adverse effect of class imbalance by performing a offset term on the output logit. In the long-tailed setting with a highly skewed\n\n15\n\nThe History of LeBron James and Stephen Curry’s Rivalrous FriendshipLeBron James?Stephen Curry?Parker:Morning,Flash.Thompson:Goodmorning,Parker.Parker?Thompson?Annotator1:KoratAnnotator2:Russian BlueKorat?Russian Blue?(a)(b)(c)Published as a conference paper at ICLR 2023\n\ntraining category distribution Ptrain(y), a trivial classifier that classifies all instances as majority labels can achieve high training accuracy. To cope with it, a class-balanced test set is used (Brodersen et al., 2010). We donate Puni(y under uniform distribution Puni(y) = 1 that minimizes the following balanced error rate (BER) on\n\n} y) as the underlying class probability Dtrain\n\nC . The of long-tailed learning is to learn a model on\n\nDuni =\n\n(xi, yi)\n\nP(x |\n\nx)\n\n∝\n\n{\n\n|\n\nDuni:\n\nmin Θ\n\nBER(Θ)\n\nPuni(y)= 1 C=\n\nP(x,y)∈Duni(y\n\n= argmax y′∈Y\n\nzy′\n\n(x)).\n\nFor an optimal model trained by minimizing the traditional softmax cross-entropy loss, the output logit z satisfies softmax(zy(x)) = Ptrain(y Ptrain(y) (Yu et al., 2018). Recall that ∝\nthe goal is to minimize BER. For a BER-optimal model, also known as Bayes-optimal model, its output logit zuni satisfies softmax(zy x) (Menon et al., 2013). LA attempts to convert the output z obtained from traditional cross-entropy training into a Bayes-optimal output as follows:\n\ny) |\nuni(x)) = Puni(y\n\nP(x\n\nx)\n\n|\n\n·\n\n|\n\nsoftmax(zy(x)) = Ptrain(y Puni(y\n\nP(x y) |\nsoftmax(zy(x)\n\nx) |\n\nP(x\n\n∝\n\nPtrain(y)\n\n· log Ptrain(y)).\n\nx) |\n\n∝\n\ny) |\n\n∝\n\n−\n\nuni(x) = zy(x)\n\nThat is to say, if we have the logit zy(x) by training on standard cross-entropy loss, a Bayes-optimal logit zy log Ptrain(y) can be obtained by subtracting an offset term log Ptrain(y). Using zy y) of the output and removes the |\nimbalanced part Ptrain(y) in a statistical sense. A number of recent studies (Zhu et al., 2022; Cui et al., 2021) have demonstrated the powerful effects of logit adjustment in long-tailed learning.\n\nuni(x) as the test logit preserves the balanced part P(x\n\n−\n\nC PSEUDO-CODE OF RECORDS\n\nWe summarize the complete procedure of our RECORDS in Algorithm 1.\n\nAlgorithm 1: Our proposed RECORDS. Input: Training dataset\n\nnumber of epochs T .\n\nDtrain, deep model f , classifier g, a self-training PLL algorithm\n\n, A\n\nOutput: Parameter θ for f, parameter W for g.\n\n1 Initialize uniform weights w; 2 Initialize F with 0; 3 for t = 1, 2, . . . , T do Shuffle training set for k = 1, 2, . . . , B do\n\n4\n\n5\n\nDtrain into B mini-batches;\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\nDk;\n\nCompute output z for mini-batch Update F according to Equation 5; Compute loss LA according to algorithm ;\nA Compute debiased output zuni according to Equation 6; Update w using zuni according to algorithm Update θ and W by minimizing LA;\n\n; A\n\nend\n\n12 13 end\n\nD PROOF OF PROPOSITION 1\n\nLemma D.1. Let L2(h) for h parameterized by Θ. We have\n\n∈\n\nH be L2 distance between Ptrain(y) and Ptrain(y\n\nΘ), where h is |\n\nL2(h)\n\n≤\n\n2E(x,y)∼(X ,Y)1(h(x)\n\n= y)).\n\n16\n\n̸ ̸\nPublished as a conference paper at ICLR 2023\n\nProof.\n\nL2(h) =\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nC (cid:88)\n\nyj =1\n\n(cid:12) (cid:12)E(x,y)∼(X ,Y)(1(y = yj)\n\n1(h(x) = yj))\n\n(cid:12) 2\n(cid:12)\n\n−\n\nC (cid:88)\n\n≤\n\nyj =1\n\nE(x,y)∼(X ,Y) |\n\n1(y = yj)\n\n1(h(x) = yj) |\n\n−\n\n(7)\n\nFor any instance (x, y), if h(x) = y, i.e., h correctly classifies x, then for have\n\n= 0. Otherwise if h(x)\n\n= y, we have\n\n1(y = yj)\n\n1(h(x) = yj) |\n\n−\n\n|\n\nyj ∈ {\n\n∀\n\n1, 2, . . . , C\n\n, we\n\n}\n\n1(y = yj)\n\n|\n\n1(h(x) = yj) |\n\n−\n\n=\n\n(cid:26)1, 0,\n\nyj = y or yj = h(x) else\n\nThus, we can bound L2(h) by\n\nL2(h)\n\n≤\n\n2E(x,y)∼(X ,Y)1(h(x)\n\n= y))\n\n(8)\n\n(9)\n\n′\n\nLemma D.2. Define R as set of all empirical risk: R = ,\n{Dtrain ∈ training set ,\n, (\nS D\nX there exists a h with zero empirical risk on εN 4 classification errors on train, i.e., M = ε, RDtrain(h) = 0, (cid:80)N\n\nD 1(h(x′)\n\ntrain ∈\n\n= y′)\n\n)N :\n\nεN\n\nY\n\n′\n\ni=1\n\n,\n\nh, L2(h) (\nX )N . Define M to be set of all pairs (\n\nDtrain for which there exists an ε-L2 Distance h with zero ε, RDtrain(h) = 0 . Introduce another train) for which Dtrain that makes ε-L2 Distance and make at least\n\nDtrain,\n\n≥\n\nY\n\nS\n\n∃\n\n}\n\n′\n\n′\n\nD )2N :\n\nDtrain, (\n{ . If N > 16 ln 2\n\ntrain)\n\n( ,\nX , then we have\n\nY\n\n∈\n\n,\n\nS\n\nD ε\n\nh, L2(h)\n\n∃\n\n≥\n\n≥\n\n4 }\n\nP(\n\nDtrain ∈\n\nR) < 2P((\n\nDtrain,\n\n′\n\ntrain)\n\nD\n\nM ).\n\n∈\n\nProof. This lemma is used in many learnability proofs. Apply the chain rule of probability:\n\nP(( =P((\n\nDtrain, Dtrain,\n\nD\n\nD\n\n′\n\ntrain) train)\n\n′\n\nM )\n\nM\n\n|Dtrain ∈\n\n∈\n\n∈\n\n=P(\n\nh, L2(h)\n\n∃\n\n≥\n\nε, RDtrain(h) = 0,\n\nR)\n\nN (cid:88)\n\ni=1\n\nP(\n\nDtrain ∈\n\n·\n\nR)\n\n1(h(x′)\n\n= y′)\n\nεN 4 |Dtrain ∈\n\n≥\n\nR)\n\nP(\n\n·\n\nDtrain ∈\n\nR)\n\nN (cid:88)\n\ni=1\n\nP(\n\n≥\n\n1(h(x′)\n\n= y′)\n\nεN 4 |\n\nL2(h)\n\n≥\n\n≥\n\nε, RDtrain(h) = 0)\n\nP(\n\nDtrain ∈\n\n·\n\nR)\n\nGiven L2(h) bound, when N > 16 ln 2\n\n≥\n\nε\n\n, we can get\n\nε, we can get E(x,y)∼(X ,Y)1(h(x)\n\nP(\n\nN (cid:88)\n\ni=1\n\n1(h(x′)\n\n= y′)\n\nεN 4\n\n)\n\n≥\n\n= y)\n\n≥\n\nε\n\n2 (Lemma D.1). Using the Chernoff\n\n(10)\n\n=1\n\nP(\n\n−\n\n1\n\n≥\n\n−\n\nP(\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1 N E\n\n1(h(x′)\n\n= y′) <\n\nεN 4\n\n)\n\n1(h(x′)\n\n= y′) < (1\n\n1 2\n\n−\n\n)E(x′,y′)∼(X ,Y)\n\nN (cid:88)\n\ni=1\n\n1(h(x′)\n\n= y′))\n\n(11)\n\n1\n\n1\n\n≥\n\n≥\n\n−\n\n−\n\ne−\n\n(x,y)∼(X ,Y)\n\n8\n\ne− N ε\n\n16 >\n\n1 2\n\n1(h(x)̸=y)\n\n17\n\n̸ ̸\n̸ ̸\n̸ ̸\n̸ ̸\n̸ ̸\nPublished as a conference paper at ICLR 2023\n\nThus we can bound P(\n\nR) by\n\nDtrain ∈ P(\n\nDtrain ∈\n\nR) < 2P((\n\nDtrain,\n\n′\n\ntrain)\n\nD\n\nM )\n\n∈\n\n(12)\n\nLemma D.3. (Liu & Dietterich, 2014) If the hypothesis space H has Natarajandimension dH and η < 1, then\n\nP((\n\nDtrain,\n\nD\n\n′\n\ntrain)\n\n∈\n\nM ) < (2N )dH C 2dH (\n\n1 + η 2\n\nεN 4\n\n)\n\n.\n\nProof. Expand (x′N , y′N , S′N )\n\nDtrain and\n\n,\n\n( X\n\n∈\n\n,\n\nY ′\n\nS\n\n′\n\ntrain as )N . We can get\n\nD\n\nDtrain = (xN , yN , SN )\n\n,\n\n(\n\nX\n\nY\n\n,\n\nS\n\n)N ,\n\nD\n\n′\n\ntrain =\n\n∈\n\nP((\n\nDtrain,\n\nDtrain, Where the expectation E is taken with respect to (xN , yN , x′N , y′N ) and the probability P comes from (SN , S′N ) given (xN , yN , x′N , y′N ).\n\nxN , yN , x′N , y′N ) |\n\nM ) = E P((\n\ntrain)\n\ntrain)\n\n(13)\n\nM\n\nD\n\nD\n\n∈\n\n∈\n\n′\n\nLet H we can get\n\n(xN , x′N ) be the set of hypothesis making different classifications for |\n\nDtrain and\n\n′\n\ntrain,\n\nD\n\n′\n\ntrain)\n\nD\n\nM\n\n|\n\n∈\n\nxN , yN , x′N , y′N )\n\nP(RDtrain(h) = 0,\n\nN (cid:88)\n\ni=1\n\n1(h(x′)\n\n= y′)\n\nεN 4 |\n\n≥\n\nxN , yN , x′N , y′N )\n\n(14)\n\n′\n\ntrain)\n\nD\n\nM\n\n|\n\n∈\n\nxN , yN , x′N , y′N )\n\nP(RDtrain(h) = 0,\n\nN (cid:88)\n\ni=1\n\n1(h(x′)\n\n= y′)\n\nεN 4 |\n\n≥\n\nxN , yN , x′N , y′N )\n\n(15)\n\nRandomly match the instances in we define a group G of swaps. A swap σ\n\nDtrain and\n\n1, 2, . . . , n\n\nG {\n| |\nwhich h classifies classifies both incorrectly, and only one incorrectly.\n\n= 2N , σ( Dtrain incorrectly, and\n\nDtrain,\n\n. }\n\nD\n\nD\n\n′\n\n′\n\nD ∈\ntrain) = (\n\ntrain into n pairs. Following Liu & Dietterich (2014), G swaps the instances in pairs indexed by Jσ ⊆ ′σ train). Let a1, a2 be the number for train incorrectly. Let b1, b2 be the number for which h\n\ntrain,\n\nD\n\nD\n\nσ\n\n′\n\nP(RDtrain(h) = 0,\n\nN (cid:88)\n\ni=1\n\n1(h(x′)\n\n= y′)\n\n≥\n\nεN 4 |\n\nxN , yN , x′N , y′N )\n\nN (cid:88)\n\n=1(\n\ni=1\n\n1(h(x′)\n\n= y′)\n\nεN 4\n\n)\n\n·\n\n≥\n\nN (cid:89)\n\ni=1\n\nP(h(xi) = ̃y(xi, Si) |\n\nxN , yN , x′N , y′N )\n\n(16)\n\n=1(a2\n\nεN 4\n\n)\n\n·\n\n≥\n\nN (cid:89)\n\ni=1\n\nxN , yN , x′N , y′N ) P(h(xi) = ̃y(xi, Si) |\n\nWe then give the bound for (cid:81)N\n\ni=1 P(h(xi) = ̃y(xi, Si)\n\nxN , yN , x′N , y′N ). |\n\nN (cid:89)\n\ni=1\n\nP(h(xi) = ̃y(xi, Si)\n\nxN , yN , x′N , y′N ) |\n\n≤\n\nN (cid:89)\n\ni=1\n\nP(h(xi)\n\n∈\n\nxN , yN , x′N , y′N )\n\nSi|\n\nηa1\n\n(17)\n\n≤\n\n18\n\nP((\n\nDtrain,\n\n(cid:88)\n\n≤\n\nh∈H|(xN ,x′N )\n\nP((\n\nDtrain,\n\n(cid:88)\n\n≤\n\nh∈H|(xN ,x′N )\n\n̸ ̸\n̸ ̸\nPublished as a conference paper at ICLR 2023\n\nCombining Equation 13-17, we can get\n\nM )\n\nM\n\nxN , yN , x′N , y′N ) |\n\n∈\n\nDtrain,\n\n′\n\ntrain)\n\nD\n\nM\n\nxN , yN , x′N , y′N ) |\n\n∈\n\n′\n\ntrain)\n\n′\n\n∈ train)\n\nDtrain,\n\nD\n\nDtrain,\n\n(cid:88)\n\nD P(σ(\n\nP(( =E P(( 1\n2N\n\n=E\n\n1 2N\n\nE\n\n≤\n\nσ∈G\n\n(cid:88)\n\nσ∈G\n\nh∈H|σ(x,x′)\n\nE (2N )dH C 2dH 1\n\n(cid:88)\n\n1(aσ\n\n2 ≥\n\nεN 4\n\n)ηaσ\n\n1\n\n≤\n\n≤\n\n2N\n\n2N\n\nσ∈G\n\nb1+b2(cid:88)\n\naσ\n\n1 =b1\n\nE (2N )dH C 2dH 1\n\n1(b1 + b2\n\nεN 4\n\n≥\n\n1 −b1\n\n)C aσ\n\nb2\n\n2n−b2ηaσ\n\n1\n\n(cid:88)\n\nP(RDtrain(h) = 0,\n\nN (cid:88)\n\ni=1\n\n1(h(x′)\n\n= y′)\n\n≥\n\nεN 4 |\n\nxN , yN , x′N , y′N )\n\n(18)\n\n=E (2N )dH C 2dH 1(b1 + b2\n\nεN 4\n\n≥\n\n)ηb1 (\n\n1 + η 2\n\n)b2\n\nwhere the third line uses the inequality (cid:12) b1 = 0, b2 = εN\n\n(cid:12)H\n\n(xN , x′N )(cid:12) (cid:12) |\n\n≤\n\n4 , the right side reaches the maximum of (2N )dH C 2dH( 1+η 2 )\n\n(2N )dH C 2dH (Natarajan, 1989). When 4 .\n\nεN\n\nFor ease of reading, here we restate Proposition 1 in the main text.\n\nProposition D.1. Let η = sup(x,y)∈X ×Y,yj ∈Y,yj ̸=y PS|(x,y)(yj ∈ S) denote the ambiguity degree, dH be the Natarajan dimension of the hypothesis space H, ̃h = h ̃Θ be the optimal classifier on the basis of the label disambiguation, where ̃Θ = arg minΘ RDtrain(Θ). If the small ambiguity degree condition (Cour et al., 2011a; Liu & Dietterich, 2014)) satisfies, namely, η δ > 0, the L2 distance between Ptrain(y) and Ptrain(y\n\n[0, 1), then for\n\n∈\n\n∀\n\n ̃Θ) for ̃h is bounded as |\n\nL2( ̃h) <\n\n4\n\n(ln 2\n\n−\n\nln(1 + η))N\n\n(dH (ln 2N + 2 ln C)\n\nln δ + ln 2)\n\n−\n\nwith probability at least 1\n\n−\n\nδ, where N is the sample number and C is the category number.\n\nProof. Recall the definition of R:\n\nR =\n\n{Dtrain ∈\n\n,\n\n( X\n\nY\n\n,\n\nS\n\n)N :\n\nh, L2(h)\n\n∃\n\n≥\n\nε, RDtrain(h) = 0\n\n}\n\n(19)\n\nHere we need to prove the sufficient condition for P( P( Dtrain, P((\n\nM ) by (2N )dH C 2dH( 1+η\n\nR) < 2P(( 2 )\n\nDtrain ∈\n\nR) by P(\n\ntrain)\n\nDtrain ∈ Dtrain, D\n\n∈\n\nεN\n\n′\n\nDtrain ∈ train)\n\n′\n\nR)\n\nδ. Lemma D.2 bounds ≤\nM ). Lemma D.3 bounds\n\n∈ 4 . Taking it into Lemma D.2, we get\n\nD\n\nP(\n\nDtrain ∈\n\nR) < 2P((\n\nDtrain,\n\n′\n\ntrain)\n\nD\n\nM )\n\n∈\n\n≤\n\n2(2N )dH C 2dH(\n\n1 + η 2\n\nεN 4\n\n)\n\nThen we can prove the sufficient condition for P(\n\nDtrain ∈\n\nR)\n\nδ:\n\n≤\n\nP(\n\nDtrain ∈\n\nR)\n\nδ\n\n≤\n\n= ln 2 + dH(ln 2 + ln N ) + 2dH ln C\n\n⇐\n\nεN 4\n\n−\n\nln(\n\n1 + η 2\n\n)\n\n≤\n\nln δ\n\nε\n\n⇐⇒\n\n≥\n\n(ln 2\n\n4\n\nln(1 + η))N\n\n−\n\n(dH (ln 2N + 2 ln C)\n\nln δ + ln 2)\n\n−\n\n(20)\n\n(21)\n\nThat is, when ε = probability at least 1\n\n4\n\n(ln 2−ln(1+η))N (dH (ln 2N + 2 ln C)\n\nln δ + ln 2), we have L2\n\n−\n\n(cid:17)\n\n(cid:16) ̃h\n\n< ε with\n\nδ.\n\n−\n\n19\n\n̸ Published as a conference paper at ICLR 2023\n\nTable 7: Characteristics of PASCAL VOC\n\n#Classes\n\n#Train\n\n#Test\n\nImbalance ratio Avg #Candidate Labels\n\n20\n\n11706\n\n4000\n\n118.8\n\n2.46\n\nTable 8: Sample size for each class of PASCAL VOC\n\nClass Number\n\nClass Number\n\nperson 5702\n\ntvmonitor 277\n\nchair 1285\n\ndog 251\n\ncar 884\n\nbus 193\n\nbottle 452\n\ncow 134\n\nsofa 364\n\nboat 125\n\nbicycle 354\n\nhorse 333\n\npottedplant 319\n\nbird 117\n\ncat 111\n\ntrain 87\n\ndiningtable motorbike\n\n304\n\nsheep 66\n\n300\n\naeroplane 48\n\nE ADDITIONAL EXPERIMENTAL SETUP AND RESULTS\n\nE.1 DATASETS\n\nCIFAR-10-LT and CIFAR-100-LT. The original versions of CIFAR-10 and CIFAR-100 contain 50,000 training images and 10,000 validation images in 32 32 size with 10 and 100 categories, respectively. The 100 categories in CIFAR-100 form 20 superclasses, each with 5 classes. Following Liu et al. (2019), we conduct the long-tailed version of CIFAR-10 and CIFAR-100 by an exponential decay across sample sizes of different classes, namely CIFAR-10-LT and CIFAR-100-LT. The imbalance rate ρ denotes the ratio of the sample sizes of the most frequent and least frequent classes, i.e., ρ = NC N1\n\n, where the sample number Nc of each class c\n\nis in the descending order.\n\n×\n\n∈ Y\n\nCandidate label sets generation. Following Lv et al. (2020); Wen et al. (2021), we adopt the uniform setting in CIFAR-10-LT and CIFAR-100-LT, i.e., P(y = y) = q to generate candidate label set of each sample. That is, all the C 1 negative labels have the same probability of q to be selected into the candidate set along with the ground truth. Besides, for CIFAR-100-LT, we also use a more challenging non-uniform setting where other labels in the same superclass of the ground truth have a higher probability to be selected into the candidate set, i.e., P(y = D(y)) = = y, D(y) = D(y)) = 8q, where D(y) denotes the superclass to which y belongs. q, P(y The non-uniform setting is more practical and challenging for the LT-PLL algorithms because the semantics of the categories in the same superclass are closer. We refer to CIFAR-100-LT with candidates generated by the non-uniform setting as CIFAR-100-LT-NU.\n\n= y, D(y)\n\ny |\n\ny |\n\n−\n\n∈\n\n∈\n\n∈\n\nS\n\nS\n\nS\n\ny\n\n|\n\nDataset partitioning. To demonstrate the performance of the algorithm on categories with different frequencies, we partition the dataset according to the sample size. Following Kang et al. (2020), We split the dataset into three partitions: Many-shot (classes with more than 100 images), Medium-shot (classes with 20-100 images), and Few-shot (classes with less than 20 images).\n\nPASCAL VOC. We conduct the real-world LT-PLL dataset PASCAL VOC from PASCAL VOC 2007 (Everingham et al.). Specifically, we crop objects in images as instances and all objects appearing in the same original image are regarded as the labels of a candidate set. Note that, here we are the first to conduct experiments based on deep models on real-world datasets, while previous PLL real-world datasets are tabular and only suitable for linear models or shallow MLPs (Fei et al., 2022). Empirically we can observe the significant class imbalance as shown in Table 8. We manually balanced the test set for fairness of the evaluation. In Table 7, we summarize the basic characteristics of PASCAL VOC.\n\nE.2 ADDITIONAL IMPLEMENTATION DETAILS\n\nToy study. (Figure 4) We simulated a four-class LT-PLL task with each class distributed in a different region and their samples uniformly distributed in the corresponding region. The four uniform 1, 0], [0, 1]), and U([0, 0], [1, 1]), redistributions are U([ spectively. The candidate label set for each sample consists of the ground truth and negative labels with 0.6 probability of being flipped from the other classes e.g., P(y = y) = 0.6. The sample size of each class in the training set is 30 (red), 100 (yellow), 500 (blue) and 1000 (green), respectively. The test set is balanced, with 100 samples per class. We use a two-layer MLP with 10 hidden\n\n1], [0, 0]), U([0,\n\n1], [1, 0]), U([\n\n−\n\n−\n\n−\n\n−\n\n1,\n\n∈\n\nS\n\ny\n\n|\n\n20\n\n̸ ̸\n̸ ̸\n̸ Published as a conference paper at ICLR 2023\n\nTable 9: Top-1 accuracy on CIFAR-100-LT-NU with imbalance ratio ρ q\n\n0.03, 0.05, 0.07\n\n. Bold indicates superior results. }\nImbalance ratio ρ\n\n50\n\n∈ {\n\n50, 100\n\n∈ {\n\n}\n\nand ambiguity\n\nambiguity q\n\nCORR + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. CORR\n\nPRODEN + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. PRODEN\n\nLW + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. LW\n\nCAVL + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. CAVL\n\n0.03\n\n40.73 45.57 16.52 46.98 +6.25\n\n38.00 42.69 11.44 43.83 +5.83\n\n33.76 33.53 28.22 34.93 +1.17\n\n0.05\n\n35.76 39.07 10.25 44.93 +9.17\n\n33.03 35.86 7.28 41.39 +8.36\n\n28.73 28.39 12.13 30.78 +2.05\n\n27.30 20.56 11.66 39.71 +12.41\n\n20.85 14.96 7.11 31.02 +10.17\n\n0.07\n\n32.21 34.75 8.61 43.37 +11.16\n\n28.29 30.42 6.50 39.88 +11.59\n\n26.11 25.83 11.11 27.79 +1.68\n\n13.18 9.88 6.47 18.42 +5.24\n\n0.03\n\n36.39 39.95 8.56 40.93 +4.54\n\n32.97 36.83 6.65 37.99 +5.02\n\n30.10 29.31 14.36 31.51 +1.41\n\n27.26 20.52 6.86 34.77 +7.51\n\n100\n\n0.05\n\n32.17 35.38 5.85 39.36 +7.19\n\n29.98 32.72 5.23 35.76 +5.78\n\n25.56 24.56 7.73 26.65 +1.09\n\n20.53 13.56 5.16 26.36 +5.83\n\n0.07\n\n28.56 31.44 5.92 37.06 +8.50\n\n26.45 29.07 4.96 34.19 +7.74\n\n22.58 21.84 6.85 25.06 +2.48\n\n7.34 4.93 4.80 20.45 +13.11\n\n. Table 10: Fine-grained analysis on CIFAR-100-LT with ρ = 100 and q }\nMany/Medium/Few corresponds to three partitions on the long-tailed data. Bold indicates superior results.\n\n0.03, 0.05, 0.07\n\n∈ {\n\nMethod\n\nCORR + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. CORR\n\nPRODEN + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. PRODEN\n\nLW + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. LW\n\nCAVL + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. CAVL\n\nq = 0.03\n\nq = 0.05\n\nq = 0.07\n\nMany Medium\n\nFew\n\nOverall Many Medium\n\nFew\n\nOverall Many Medium\n\nFew\n\nOverall\n\n68.43 70.37 11.03 66.37 -2.06\n\n64.23 66.63 2.40 64.91 +0.68\n\n64.37 59.89 50.37 61.66 -2.71\n\n59.54 51.51 5.29 62.66 +3.12\n\n37.40 41.89 12.34 42.54 +5.14\n\n32.49 38.03 9.31 38.40 +5.91\n\n25.86 25.57 29.60 29.29 +3.43\n\n20.80 27.26 9.00 35.43 +14.63\n\n4.50 7.33 10.63 13.77 +9.27\n\n2.23 5.93 8.97 9.90 +7.67\n\n0.00 3.73 7.70 4.42 +4.42\n\n0.57 2.57 7.47 8.67 +8.10\n\n38.39 41.49 11.37 42.25 +3.86\n\n34.52 38.40 6.79 39.13 +4.61\n\n31.58 31.03 30.30 33.00 +1.42\n\n28.29 28.34 7.24 36.93 +8.64\n\n67.51 70.46 0.34 68.49 +0.98\n\n62.60 66.86 0.06 65.94 +3.34\n\n63.85 58.03 4.29 63.17 -0.68\n\n58.57 57.85 0.00 54.20 -4.37\n\n29.60 33.40 4.46 40.20 +10.60\n\n28.66 31.80 1.74 36.54 +7.88\n\n16.40 16.26 4.63 16.26 -0.14\n\n12.97 16.51 2.03 31.43 +18.46\n\n0.33 1.47 5.47 8.50 +8.17\n\n0.33 2.23 7.00 4.53 +4.20\n\n0.00 3.20 6.53 3.45 +3.45\n\n0.00 0.80 6.13 5.07 +5.07\n\n34.09 36.79 3.32 40.59 +6.50\n\n32.04 35.20 2.73 37.23 +5.19\n\n28.09 26.96 5.08 28.85 +0.76\n\n25.39 26.27 2.55 31.49 +6.10\n\n68.86 69.77 0.00 69.97 +1.11\n\n65.49 66.60 0.03 66.14 +0.65\n\n62.31 55.14 0.00 58.51 -3.80\n\n16.71 7.60 0.00 44.66 +27.95\n\n19.80 24.86 0.71 36.71 +16.91\n\n18.51 24.17 0.20 33.03 +14.52\n\n8.11 8.49 1.54 11.74 +3.63\n\n6.65 5.54 0.29 25.49 +18.84\n\n0.07 0.67 5.77 4.37 +4.30\n\n0.00 0.50 6.33 1.83 +1.83\n\n0.00 3.10 7.20 3.25 +3.25\n\n0.13 3.97 6.43 1.43 +1.30\n\n31.05 33.32 1.98 38.65 +7.60\n\n29.40 31.92 1.98 35.26 +5.86\n\n24.65 23.20 2.70 25.64 +0.99\n\n8.20 5.80 2.03 24.98 +16.78\n\nneurons as the model for the toy study. The batch size is set to 512. The toy models are trained using SGD with momentum of 0.9. We train the models for 50 epochs with initial learning rate 2.0.\n\nLinear Probing. (Figure 6(a)) We train a linear classifier on a frozen pre-trained backbone and measure the quality of the representation through the test accuracy. To eliminate the effect of the longtailed distribution in the fine-tuning phase, the classifier is trained on a balanced dataset. Specifically, the performance of the classifier is reported on the basis of pre-trained representations for different amounts of data, including full-shot, 100-shot, and 50-shot. In the fine-tuning phase, we train the linear classifier for 500 epochs with SGD of momentum 0.7 and weight decay 0.0005. The batch size is set to 1000. The learning rate decays exponentially from 10−2 to 10−6. The loss function is set to the ordinary cross-entropy loss.\n\nE.3 ADDITIONAL RESULTS FOR NON-UNIFORM CANDIDATES GENERATION.\n\nIn Table 9, we show the complete experimental results on CIFAR-100-LT-NU. Under a more practical and challenging candidate generation setting, our RECORDS achieves a consistent and signifi-\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nTable 11: Fine-grained analysis on CIFAR-100-LT-NU with ρ = 100 and q .\n} Many/Medium/Few corresponds to three partitions on the long-tailed data. Bold indicates superior results.\n\n0.03, 0.05, 0.07\n\n∈ {\n\nMethod\n\nCORR + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. CORR\n\nPRODEN + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. PRODEN\n\nLW + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. LW\n\nCAVL + Oracle-LA post-hoc + Oracle-LA + RECORDS vs. CAVL\n\nq = 0.03\n\nq = 0.05\n\nq = 0.07\n\nMany Medium\n\nFew\n\nOverall Many Medium\n\nFew\n\nOverall Many Medium\n\nFew\n\nOverall\n\n66.71 69.80 3.26 65.60 -1.11\n\n63.94 66.09 1.91 62.91 -1.03\n\n63.69 58.60 14.03 61.41 -2.28\n\n54.94 25.03 2.20 56.89 +1.95\n\n34.57 40.43 9.97 41.26 +6.69\n\n29.74 36.37 8.11 37.77 +8.03\n\n22.31 25.51 17.37 24.39 +2.08\n\n21.29 25.06 6.63 35.09 +13.80\n\n3.13 4.57 13.10 11.77 +8.44\n\n0.6 3.23 10.47 9.17 +8.57\n\n0.00 3.07 11.23 4.45 +4.45\n\n1.93 9.97 12.57 8.60 +6.67\n\n36.39 39.95 8.56 40.93 +4.54\n\n32.97 36.83 6.65 37.99 +5.02\n\n30.10 29.31 14.36 31.51 +1.41\n\n27.26 20.52 6.86 34.77 +7.51\n\n70.23 71.20 0.86 66.40 -3.83\n\n65.54 65.94 0.20 59.20 -6.34\n\n62.94 57.89 1.34 61.54 -1.40\n\n42.17 15.37 0.00 43.26 +1.09\n\n21.57 28.77 5.34 38.71 +17.14\n\n20.09 26.83 2.57 36.89 +16.80\n\n10.09 9.37 6.74 10.60 +0.51\n\n14.74 14.80 2.77 25.34 +10.60\n\n0.13 1.30 12.27 8.57 +8.44\n\n0.03 0.83 14.20 7.10 +7.07\n\n0.00 3.40 16.33 4.33 +4.33\n\n2.03 10.00 13.97 7.83 +5.80\n\n32.17 35.38 5.85 39.36 +7.19\n\n29.98 32.72 5.23 35.76 +5.78\n\n25.56 24.56 7.73 26.65 +1.09\n\n20.53 13.56 5.16 26.36 +5.83\n\n65.37 68.71 0.00 62.09 -3.28\n\n62.89 64.17 0.00 57.29 -5.60\n\n58.31 53.40 0.00 59.02 +0.71\n\n18.37 8.49 0.00 32.40 +14.03\n\n16.11 20.17 0.25 37.40 +21.29\n\n12.69 18.89 2.43 34.43 +21.74\n\n6.20 6.37 6.57 9.51 +3.31\n\n2.06 0.60 1.20 19.23 +17.17\n\n0.13 1.10 16.87 7.47 +7.34\n\n0.00 0.00 13.70 6.97 +6.97\n\n0.00 3.06 5.17 3.26 +3.26\n\n0.63 5.83 14.60 7.87 +7.24\n\n28.56 31.44 5.92 37.06 +8.50\n\n26.45 29.07 4.96 34.19 +7.74\n\n22.58 21.84 6.85 25.06 +2.48\n\n7.34 4.93 4.80 20.45 +13.11\n\n(a) PRODEN\n\n(b) PRODEN + Oracle-LA\n\n(c) PRODEN + RECORDS\n\nFigure 8: T-SNE of image representation on CIFAR-10-LT with ρ = 100 and q = 0.5. Note that, PRODEN + Oracle-LA post-hoc does not alter feature and shares the same figure as PRODEN.\n\ncant improvement over the PLL baselines for varying imbalance ratio ρ q\npotential candidate generation settings.\n\nand ambiguity . It demonstrates the stable enhancement by our RECORDS under different\n\n0.03, 0.05, 0.07\n\n50, 100\n\n∈ {\n\n∈ {\n\n}\n\n}\n\nE.4 FINE-GRAINED ANALYSIS ON MORE PLL BASELINES\n\n0.03, .0.05, 0.07\n\nIn Table 10 and 11, we show the additional results of fine-grained analysis on CIFAR-100-LT (ρ = 100, q ). Our RECORDS shows consistent gains on Medium and Few classes when conbined with all four PLL baselines. As a result of head-to-tail tradeoff, the accuracies of Many classes improve less or decrease slightly, which can be observed in many long-tailed learning literatures (Kang et al., 2020; Menon et al., 2021). Our RECORDS consistently improves the overall accuracies in this tradeoff.\n\n) and CIFAR-100-LT-NU (ρ = 100, q\n\n0.03, .0.05, 0.07\n\n∈ {\n\n∈ {\n\n}\n\n}\n\nE.5 FEATURE VISUALIZATION\n\nIn Figure 8, We visualize the image representation produced by the feature encoder f using tSNE (van der Maaten & Hinton, 2008) on CIFAR-10-LT with ρ = 100 and q = 0.5. We can observe that there is some overlap in the representations of different classes of PRODEN due to the heavy class imbalance and the label ambiguity. PRODEN + Oracle-LA basically fails to learn discriminative representations due to the tough constant rebalancing. In contrast, our dynamic rebalancing method RECORDS produces more distinguishable representations.\n\n22\n\nt-SNEdimension1t-SNEdimension2t-SNEdimension1t-SNEdimension2t-SNEdimension1t-SNEdimension2Published as a conference paper at ICLR 2023\n\nTable 12: Top-1 accuracy on three benchmark datasets. Bold indicates the superior results.\n\nImbalance ratio ρ\n\nAmbiguity q\n\nCORR + Oracle-LA post-hoc + Oracle-LA + Oracle-BCL + Oracle-PaCO + LDD + SFN + RECORDS vs. CORR\n\nPRODEN + Oracle-LA post-hoc + Oracle-LA + Oracle-BCL + Oracle-PaCO + LDD + SFN + RECORDS vs. PRODEN\n\nLW + Oracle-LA post-hoc + Oracle-LA + Oracle-BCL + Oracle-PaCO + LDD + SFN + RECORDS vs. LW\n\nCAVL + Oracle-LA post-hoc + Oracle-LA + Oracle-BCL + Oracle-PaCO + LDD + SFN + RECORDS vs. CAVL\n\n0.3\n\n76.12 80.70 36.27 43.76 42.45 75.23 75.98 82.57 +6.45\n\n73.12 77.41 27.18 31.66 31.48 72.89 73.00 79.48 +6.36\n\n70.11 74.34 41.90 45.89 45.02 70.20 69.34 76.02 +5.91\n\n56.73 55.23 22.16 24.15 24.60 56.35 56.74 67.27 +10.54\n\n50\n\n0.5\n\n56.45 58.49 17.61 25.24 26.34 58.02 57.13 80.28 +23.83\n\n54.45 57.14 16.97 21.92 21.09 54.89 54.73 76.73 +22.28\n\n37.67 40.27 21.36 23.38 23.10 37.87 37.04 57.39 +19.72\n\n40.27 39.76 14.97 17.82 18.42 40.28 41.29 61.23 +20.96\n\nCIFAR-10-LT\n\n0.7\n\n0.3\n\n41.56 43.44 12.77 17.34 16.23 42.14 41.77 67.24 +25.68\n\n41.37 42.91 11.52 14.97 14.58 41.84 41.72 65.31 +23.94\n\n22.73 25.34 15.28 18.46 18.61 22.42 22.82 40.28 +17.55\n\n18.52 18.34 11.50 12.71 13.51 18.40 18.33 40.71 +22.19\n\n66.38 72.96 29.97 33.23 34.21 67.59 66.91 77.66 +11.28\n\n63.55 70.71 19.51 24.14 23.82 63.88 64.16 72.15 +8.60\n\n64.78 69.60 25.75 28.77 28.65 64.90 64.00 71.18 +6.40\n\n54.28 51.37 18.29 20.02 21.55 54.33 54.87 64.35 +10.07\n\nCIFAR-100-LT\n\n100\n\n0.5\n\n50.09 54.64 15.80 18.34 18.23 51.38 50.23 72.90 +22.81\n\n47.37 48.79 14.11 18.51 19.00 47.73 47.89 65.22 +17.85\n\n39.57 42.34 20.35 23.95 23.66 39.49 39.59 57.23 +17.66\n\n38.97 37.28 14.23 15.45 16.04 39.15 39.80 58.27 +19.30\n\n0.7\n\n38.11 41.66 11.75 15.72 14.23 39.28 38.71 57.46 +19.35\n\n38.06 41.38 11.17 13.85 15.76 38.08 38.02 52.26 +14.2\n\n23.54 27.35 14.24 15.51 15.25 23.44 23.52 41.24 +17.70\n\n17.28 14.58 10.67 13.10 12.02 16.49 16.89 37.38 +20.1\n\n50\n\n0.05\n\n38.03 40.76 5.59 10.34 11.23 39.05 38.62 45.56 +7.53\n\n35.45 38.64 4.09 8.45 8.05 35.36 36.04 41.31 +5.86\n\n29.50 28.80 14.43 17.75 17.80 29.05 28.87 31.67 +2.17\n\n0.07\n\n36.59 39.07 3.12 8.34 7.23 36.98 37.03 42.51 +5.92\n\n33.90 35.82 2.64 7.63 7.06 34.03 34.19 39.26 +5.36\n\n27.86 27.27 4.79 8.04 7.91 27.20 27.14 29.39 +1.53\n\n0.03\n\n42.29 46.94 22.56 27.23 27.37 42.91 43.14 48.06 +5.77\n\n39.23 43.40 12.37 16.42 16.34 39.68 39.32 44.56 +5.33\n\n35.54 35.47 30.37 33.84 33.28 35.23 35.46 36.56 +1.02\n\n29.63 29.65 17.31 19.56 17.97 29.06 29.31 42.25 +12.62\n\n17.31 14.86 4.36 26.96 6.33 17.26 17.46 36.53 +19.22\n\n8.34 5.76 2.83 5.79 6.00 8.11 9.33 29.13 +14.27\n\n100\n\n0.05\n\n34.09 36.79 3.32 5.82 5.62 34.87 33.91 40.59 +6.40\n\n32.04 35.20 2.73 7.58 7.52 32.64 32.60 37.23 +5.19\n\n28.09 26.96 5.08 8.83 7.81 28.27 27.50 28.85 +0.76\n\n25.39 26.27 2.55 3.37 4.12 25.14 25.39 31.49 +6.10\n\n0.07\n\n31.05 33.32 1.98 4.10 4.72 32.15 31.67 38.65 +7.60\n\n29.40 31.92 1.98 6.01 7.40 29.81 29.51 35.26 +5.86\n\n24.65 23.20 2.70 6.18 6.02 24.07 24.54 25.64 +0.99\n\n8.20 5.80 2.03 3.68 4.00 7.60 9.00 24.98 +16.78\n\n0.03\n\n38.39 41.49 11.37 14.34 15.82 39.20 39.31 42.25 +3.86\n\n34.52 38.40 6.79 10.28 11.66 34.49 35.13 39.13 +4.61\n\n31.58 31.03 30.30 33.55 33.83 31.00 31.19 33.00 +1.42\n\n28.29 28.34 7.24 10.70 9.64 28.19 28.34 36.93 +8.64\n\nPASCAL VOC\n\n24.43 34.12 52.51 53.23 53.19 25.45 25.63 56.46 +32.03\n\n22.39 31.53 48.33 49.71 50.04 22.79 22.94 52.65 +30.26\n\n19.41 21.06 51.53 52.06 51.54 19.71 18.68 53.09 +33.68\n\n17.25 22.27 50.78 51.69 51.80 16.65 18.31 53.07 +35.82\n\nTable 13: Results with the addition of Mixup on benchmark datasets. The best and second best results are respectively in bold and underline.\n\nImbalance ratio ρ\n\nAmbiguity q\n\nLW CAVL PRODEN CORR SoLar\n\nLW+RECORDS CAVL+RECORDS PRODEN+RECORDS CORR+RECORDS\n\n50\n\n0.5\n\n39.69 41.68 56.25 58.35 76.55\n\n59.88 63.59 78.82 82.5\n\n0.3\n\n71.31 57.77 74.22 77.53 83.88\n\n77.22 68.93 81.23 84.25\n\nCIFAR-10-LT\n\n0.7\n\n0.3\n\n25.23 21.32 45.11 45.66 54.61\n\n43.98 44.61 68.03 71.24\n\n66.08 54.28 65.72 69.12 75.38\n\n72.2 66.65 73.92 79.79\n\n100\n\n0.5\n\n41.2 39.47 49.23 50.96 70.63\n\n59.87 59.62 66.21 74.07\n\nCIFAR-100-LT\n\n50\n\n0.7\n\n0.03\n\n0.05\n\n0.07\n\n0.03\n\n26.59 20.67 42.26 42.87 53.15\n\n43.63 41.88 55.73 62.25\n\n37.34 23.01 43.02 45.95 47.93\n\n38.16 46.13 48.12 52.08\n\n31.59 21.29 40.15 42.83 46.85\n\n33.6 41.93 46.01 50.58\n\n30.6 14.23 39.1 41.72 45.1\n\n31.53 34.01 44.19 47.91\n\n33.78 31.65 38.66 41.79 42.51\n\n35.33 41.33 42.98 46.57\n\n100\n\n0.05\n\n30.69 28.77 36.23 38.47 41.71\n\n31.55 35.79 41.03 45.22\n\n0.07\n\n27.91 13.65 35.52 36.96 39.15\n\n28.8 31.29 40.25 44.73\n\nPASCAL VOC\n\n20.73 18.33 24.47 26.72 56.49\n\n55.19 55.03 54.78 58.45\n\nE.6 COMPARISON WITH MORE BASELINES FOR IMBALANCED LEARNING\n\nWe additionally compare our RECORDS with two contrastive-based state-of-the-art long-tailed learning baselines, BCL (Zhu et al., 2022) and PaCO (Cui et al., 2021), and two regularization methods for mitigating imbalance in PLL, LDD (Liu et al., 2021) and SFN (Liu et al., 2021). Similar to Oracle-LA, we use the otherwise invisible oracle category distributions for BCL and PaCO, denoted as Oracle-BCL and Oracle-PaCO. We use pseudo-labels to construct positive and negative sample pairs for the contrastive learning part of BCL and PaCO, because the true labels are not visible during training. In Table 12, we show the top-1 accuracy on CIFAR-10-LT and CIFAR-100-LT under the best PLL baseline CORR. Both BCL and PaCO use an LA-like form for the supervised part. Despite the performance gains from contrastive learning (compared to Oracle-LA), they also experience performance degradation similar to that of LA. LDD and SFN implicitly mitigate the imbalance in PLL by regularization constraints on the parameter space, and achieve a slight improvement over the PLL baseline CORR. However, they do not explicitly address the long-tailed distribution problem of LT-PLL. Our method achieves a significant improvement over all baselines. We should also note that, in these baselines except LDD and SFN, they use the oracle class distribution, which our method also does not use. Therefore, our method is more general than them.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nE.7 COMPARISON WITH THE CONCURRENT LT-PLL WORK SOLAR\n\nSoLar (Wang et al., 2022a) is a concurrent LT-PLL work published in NeuIPS 2022. At the time of our submission, we did not have access to this work and make comparisons with it. It improves the label disambiguation process in LT-PLL through the optimal transport technique. However, it requires an extra outer-loop to refine the label prediction via sinkhorn-knopp iteration, which increases the computational complexity. Different from SoLar, this paper tries to solve the LT-PLL problem from the perspective of rebalancing in a lightweight and effective manner.\n\nNote that compared to other PLL baselines and previous experiments in this paper, SoLar additionally applies Mixup (Zhang et al., 2018) in the experiments. To align the experimental setup, we show the results with the addition of Mixup in Table 13. All other settings remain the same as previous sections. Compared to SoLar, which designed for the LT-PLL problem, our method still offers consistent improvements. The code implementation of comparisons with Solar can be found here.\n\n24",
    "reference": "# Summary Of The Paper\n\nThis paper describes a new method for rebalancing of class probabilities for longtailed learning under the setting of partial label learning. No others followed the idea of logit adjustment, which factories P(y|x) as P(x|y) P(y). The second component is estimated on a per class basis in the training algorithm.\n\nThe authors show their experimental results on CIFAR-100 LT and CIFAR 10 LT data sets.\n\n# Strength And Weaknesses\n\nStrength:\n- Proposed dynamic re-balancing technique is interesting.\n- The problem formulation is theoretically interesting.\n\nWeaknesses:\n- The experimental results are not convincing. The main issue with this paper is that it does not compare its performance with the state of the art methods for longtailed learning. For example the following paper reports in excess of 52% accuracy on CIFAR-100 LT. \n\nJianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, Yu-Gang Jiang. Balanced Contrastive Learning for Long-Tailed Visual Recognition. CVPR 2022\n\n- Another major problem is about the practical motivation for the proposed problem. Authors almost assume that LT+PLL is an important problem setting to learning with without giving any motivating examples. This is also reflected in the experimental set up on benchmark data sets where for the PLL setting the authors simply use uniform probability to generate partial label sets from CIFAR 100 LT. This might generate more partial labels for the tail classes.\n\n- Finally, the paper is very difficult to read. This is because, the exposition in the paper assumes that the author is familiar with the literature on long-tailed learning and partial label learning. However many of these settings are not mainstream and the approaches should be introduced properly.\n\nFor example, prior to equation 2 the authors do not define p_uni(y) but refer to a previous paper. It is hard for a reader not familiar with the literature to understand the definition from the previous paper.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nPlease see above.\n\n# Summary Of The Review\n\nOverall the paper is well written and dwells on topical problems. However the current problem is poorly motivated and seems almost like a combination of two other problems. The proposed method is logical and simple. However the exposition of the proposed methods in my opinion needs to be more lucid. Finally while lot of experiments have been performed showing improvement over existing PLL techniques, the baselines for long tail learning have not been incorporated.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCROSS-PROTEIN WASSERSTEIN TRANSFORMER FOR PROTEIN-PROTEIN INTERACTIONS\n\nPaper ID: 3822\n\nABSTRACT\n\nPrevious studies reveal intimate relationships between the structure and function of proteins. Motivated by this, for protein-protein interactions (PPIs), we hypothesize that cross-protein structural correspondence, including both global correlation and local co-occurrence, poses a great influence. Accordingly, a novel deep learning framework named Cross-Protein Wasserstein Transformer (CPWT) is proposed to predict PPI sites through fine-grained cross-graph structural modeling. Considering the irregular architecture of acid sequences, for a pair of proteins, graphs are constructed to describe them. Then, a core Cross-Graph Transformer (CGT) module of two branches (e.g. ligand and receptor branches) is proposed for cross-protein structural modeling. Specifically, in this module, Wasserstein affinity across graphs is calculated through cross-graph query (i.e. ligand (query) - receptor (key) or the converse), based on which the multi-head attention is derived to adaptively mine fine-grained cues of PPI sites. By stacking CGT modules, the two branches in CGT are co-evolved in a deep architecture during forward inference, hence being powerful and advantageous in cross-protein structural representation and fine-grained learning. We verify the effectiveness of our CPWT framework by conducting comprehensive experiments on multiple PPI datasets, and further visualize the learned fine-grained saliencies for intuitive understanding.\n\n1\n\nINTRODUCTION\n\nProteins are chains of amino acids, and physical interaction between proteins is essential to life. The protein-protein interaction (PPI) (Figure 1 shows an example) determines molecular and cellular mechanisms, and thus plays a crucial role in biological processes including the gene expression, proliferation of cells, etc. Moreover, as proteins are predominant drug targets, characterizing PPIs at the fine-grained level, e.g. identifying protein interaction sites for PPIs, would provide significant insight into biological mechanisms, and has important application in drug design, disease treatment (Ryan & Matthews, 2005), and target discovery. For this reason, the research on PPI prediction has drawn increasing attention.\n\nIn the early stage, sundry experimental assays have been widely applied to PPI identification, such as nuclear magnetic resonance(NMR) (Wuthrich, 1989), X-ray crystallography (Svergun et al., 2001) and high-throughput screening methods (Song et al., 2011). However, identifying the binding sites based on these methods is often time-consuming and resource-expensive. Then, with more and more protein structure data available (Berman et al., 2000), computational methods are developed to predict protein-protein interaction sites, which can be divided into two categories, i.e. protein-protein docking and data-driven methods. For the protein-protein docking (Porter et al., 2019; Halperin et al., 2002), the fundamental principle is the steric complementarity at protein-protein interfaces. However, it suffers from the tremendous search space and requirement of expert-defined scoring functions in the searching and scoring processes for predicting complex structures.\n\nFor data-driven methods, a number of machine learning algorithms with shallow structures are first proposed for PPIs in the early stage. Generally, they can be divided into three categories, i.e. sequencebased methods (Murakami & Mizuguchi, 2010; Zhang et al., 2019; Jurtz et al., 2017; Haberal & O ̆gul, 2017; Zheng et al., 2018), structure-based methods (Du et al., 2016; Bradford & Westhead, 2005; Neuvirth et al., 2004) and those ones based on mixed information (Afsar Minhas et al., 2014; Li et al., 2012; Northey et al., 2018; Porollo & Meller, 2007). Then, inspired by the success of deep learning in vision tasks, deep neural networks are employed for protein science, e.g. the success of DeepMind’s Alphafold2 (Jumper et al., 2021) for structure prediction. Specifically for PPIs, to utilize the powerful structure modeling ability, recent works (Bryant et al., 2022; Gao et al., 2022; Evans et al., 2021) including AF2Complex (Gao et al., 2022) and AlphaFold-Multimer (Evans et al., 2021) propose to\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Ball and stick view of the protein complex named 2HRK, bound from a receptor protein (green) and a ligand protein (cyan). The interactive residues are marked as pink in the receptor and orange in the ligand. The zoomed-up area in the red box shows detailed interactions (red dotted lines) of amino acids between asparagine in the receptor (pink) and arginine in the ligand (orange).\n\npredict the structure of multimeric protein complexes through the Alphafold model (Jumper et al., 2021), based on which interactions between protein sequences can be further identified. Moreover, multiple deep learning methods (Amidi et al., 2018; Torng & Altman, 2017; Jiménez et al., 2017; Skalic et al., 2019) attempt to map irregular atomic coordinates to regular representations of 3D grids, or just to 2D images which are then fed into Convolutional Neural Networks(CNN) (Gainza et al., 2020). Recently, graph neural networks (GNNs) (Fout et al., 2017; Liu et al., 2020; Morehead et al., 2022) are applied to deal with PPI prediction and achieve state-of-the-art performance.\n\nConsiderable progress has been made in PPI prediction, especially by GNNs. However, considering the intimate structure-function relationship, two main issues are remaining to be solved: (1) insufficient cross-protein structural modeling: the often used post-fusion of vectorized protein features fails to well characterize the cross-protein structural correlation; (2) unsophisticated fine-grained learning for PPI sites: interactive pairs of sites always take a pretty small proportion of residues (as shown in Figure 1) in proteins so that fine-grained modeling is rather necessary for PPI prediction, which is not adequately investigated in the previous study.\n\nTo tackle these issues, we propose a deep learning framework named Cross-Protein Wasserstein Transformer (CPWT) for PPI prediction. For a given pair of proteins, e.g. ligand and receptor, the target is to detect those interactive pairs of sites. Considering the irregular architecture of proteins and intrinsic relations among residues, graphs are constructed to describe proteins with residues as nodes and spatial relationship for defining edges, then fed to corresponding graph encoders for feature learning. Furthermore, a core Cross-Graph Transformer (CGT) module, consisting of two branches (e.g. ligand and receptor branches), is proposed to model structural correlation across proteins with fine-grained learning on PPI sites. In this process, cross-graph modeling is required, which is rather non-trivial. Theoretically, as irregular graphs don’t lie in the Euclidean space, the common metric like cosine similarity, cannot well describe the cross-graph structural relationship. To address this issue, the Wasserstein metric is specifically introduced, based on which cross-protein query operations (i.e. ligand (query) - receptor (key) or the converse) are first conducted in the CGT by calculating Wasserstein affinities across graphs. Furthermore, multi-head attention is derived based on Wasserstein affinities to adaptively highlight salient pairs of sites and update the Transformer values.With the optimal transport principle of Waserstein affinity, the CGT is advantagous in characterizing two irregular (cross-graph) point/residue sets and exploring both global and local cross-graph structural information. Moreover, the CGT can be stacked in multiple layers so that its two branches can be effectively co-evolved in a deep architecture, hence being powerful in cross-protein structural expression and advantageous in fine-grained learning. We verify the effectiveness of our CPWT framework by conducting comprehensive experiments on PPI datasets, then visualize the fine-grained saliencies and compare them with the ground truth interaction for intuitive understanding.\n\nThe contributions are summarized as follows: (1) We propose a new Cross-Protein Wasserstein Transformer framework to promote the PPI prediction from the perspective of sophisticated crossprotein structural modeling based on Wasserstein affinities; (2) We propose a novel CGT module in which the multi-head attention is derived to mine fine-grained cues of PPI sites. Moreover, this module can be stacked into a deep architecture with two branches co-evolved, which are powerful\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nin cross-protein structural expression with sophisticated fine-grained learning; (3) We report the state-of-the-art performance on multiple PPI datasets with comprehensive experiments.\n\n2 RELATED WORK\n\nThe PPI prediction problem has two flavors (Murakami & Mizuguchi, 2010; Zhang et al., 2019; Jurtz et al., 2017; Haberal & O ̆gul, 2017; Zheng et al., 2018; Afsar Minhas et al., 2014; Li et al., 2012; Northey et al., 2018; Porollo & Meller, 2007): the partner-independent prediction and the partner-specific one. The former is to predict whether a single residue of a protein could interact with residues from any other protein, while the latter is to detect interaction between residues from two proteins. Here, we focus on those works of partner-specific prediction, which is more relevant to our task. Generally, they can be roughly divided into two categories named surface geometry-based and graph-based methods, respectively.\n\nSurface geometry-based methods. In order to encode and exploit surface geometry and interface complementarity in a deep learning framework, several methods have been proposed (Townshend et al., 2019; Behler & Parrinello, 2007; Sverrisson et al., 2021; Gainza et al., 2020; Sverrisson et al., 2021). Townshend et al. (Townshend et al., 2019) employs voxelization method which voxelizes local atomic environments or \"surface patches\", surrounding each of them, and then applies a 3D convolutional neural network to extract its latent features. Sverrisson et al. (Sverrisson et al., 2021) operates directly on the large set of atoms that compose the protein, generates a point cloud representation for the protein surface, learns task-specific geometric and chemical features on the surface point cloud and finally applies a new convolutional operator that approximates geodesic coordinates in the tangent space. Dai et al. (Dai & Bailey-Kellogg, 2021) constructs pairs of point clouds encoding the structures of two partner proteins, in order to predict their structural regions mediating interaction. It extracts local surface features and global protein features and then concatenated them to predict the interface regions on both interacting protein.\n\nGraph-based methods. As one of the geometric deep learning methods, Graph Neural Networks (GNN) have been enthusiastically sought and have been proposed for protein interface region prediction (Fout et al., 2017; Duvenaud et al., 2015; Liu et al., 2020; Schütt et al., 2017; Pittala & Bailey-Kellogg, 2020). Font et al. (Fout et al., 2017) represents the protein as graph, which amino acids as nodes and affinities between nodes as edges. Then it employs GNN to learn node features, and classify each amino acid pair by a classifier. For better incorporating more informations. Liu et al. (Liu et al., 2020) proposes high-order interactions for protein interface. Same as (Fout et al., 2017), it learns node features by GNN. It proposes the sequential modeling method to incorporate the sequential information. Then it incorporates high-order pairwise interactions to generate a 3D tensor containing different pairwise interactions, and employs Convolutional Neural Networks(CNNs) to perform 2D dense predictions. Pittala et al. (Pittala & Bailey-Kellogg, 2020) combines the advantages of GNN and attention mechanisms in an integrated model for predicting both epitopes and paratopes. The former aggregate properties across local regions in a protein and the latter explicitly encode the context of the partner. Recently, Morehead et al. (Morehead et al., 2022) proposes the geometric transformer for rotation-invariant protein interface contact prediction. Moreover, the PepNN (Abdin et al., 2022) framework is proposed for protein-peptide interaction, where the reciprocal attention is employed for cross-graph measurement.\n\nDifferent from all previous works, we propose a new CPWT framework from the perspective of cross-protein structural modeling. Specifically, we propose a novel CGT module consisting of two branches with multi-head attention based on the Wasserstein affinity, which is advantageous in cross-protein structural expression and fine-grained learning on PPI sites.\n\n3 THE PROPOSED FRAMEWORK\n\n3.1 OVERVIEW\n\nThe architecture of our CPWT framework with a one-layer CGT is shown in Figure 2, which takes a pair of ligand and receptor proteins as the input. Generally, the CPWT framework consists of three main learning processes including graph encoding, cross-graph transform, and PPI site prediction. Considering the irregular structure of proteins, graphs are constructed to describe them by taking residues as nodes with corresponding edges defined according to the spatial relationship. To learn robust features from these graphs, graph encoders such as GNNs are then applied. Furthermore, these encoded graphs are fed into the specifically designed CGT module to characterize structural\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The architecture of our CPWT framework. Three main learning processes are involved in the CPWT framework, including graph encoding, cross-Wasserstein transformer, and prediction. The input pairs of receptor and ligand proteins are first modeled as graphs, which takes residues in amino acid sequences as nodes with edges existed in the k-nearest neighbors. Then, multi-layer GNN can be stacked as the encoder for robust feature learning. The encoded graphs are both fed into the CGT module for further fine-grained cross-protein learning. In this process, multiple CGT layers can be stacked into a deep architecture (only one layer shown here) with the two branches in CGT co-evolved. More details could be found in Section 3. Finally, the learned features of residues from two input proteins are combined for PPI site prediction.\n\ncorrelation across two proteins and meantime achieve fine-grained learning. As shown in Figure 2, the CGT module consists of two branches that are mutually interacted and thus co-evolved. For each branch, two main operations, i.e. cross-graph query and multi-head attention, are involved. Specifically, the cross-graph query calculates the Wasserstein affinity across proteins where the sinkhorn algorithm is applied for cross-graph structural optimization. Based on query-resulted Wasserstein affinities, the multi-head attention is derived to update CGT values so as to adaptively detect salient pairs of sites. Finally, the learned features of residues in ligand and receptor proteins are combined to fulfill the prediction of those PPI sites. In the following parts, we descibe the learning processes in detail.\n\n3.2 GRAPH ENCODING\n\nFor an input pair of ligand and receptor proteins, we first construct graphs for them which are denoted as Gl = {(V l, E l)|xl ij ∈ E r}, respectively. Here, V l and V r are the node sets of ligand and receptor graphs, while E l and E r are the edge sets. xl i\ndenotes the feature vector of the i-th node while el ij denotes the relationship (edge) feature between the i-th and j-th nodes in the ligand graph.\n\nij ∈ E l} and Gr = {(V r, E r)|xr\n\ni ∈ V r, er\n\ni ∈ V l, el\n\nWe adopt GNNs to learn nodes’ representation based on its local neighbors. Considering that neighbours always contribute differently to the center node, different weights are distributed to them according to their distances from the center node, with larger weights for closer neighbors. Taking a node xl\n\ni in the ligand graph as an example, the graph encoding process is as follows:\n\nhl\n\nj = σ(Wl\n\n1xl\n\nj + Wl\n\n2el\n\nij + bl), αj = sof tmax(hl\n\njpl),\n\ni = σ(Wl zl\n\n3xi +\n\n1 |N l i |\n\n(cid:88)\n\nj\n\nαjhl\n\nj),\n\n(1)\n\n(2)\n\n1, Wl\n\nis neighbour set of the i-th node and |N l\n\nwhere j ∈ N l i | denotes neighbour node number, i\nWl 3 are learnable projection matrices, bl is the learnable bias, and pl is an optimizable projection vector. σ(·) is a non-linear activation function, e.g. Relu, which results in the encoded feature denoted as zl i.\n\n2, Wl\n\n4\n\nLigand ProteinLigand GraphEncoded Ligand GraphGraph EncoderGraph EncoderEncoded Receptor GraphReceptor GraphReceptor ProteinSampling StrategyCross-Graph TransformerCross-Graph TransformerWasserstein AffinityWasserstein AffinityLigand Graph EmbeddingReceptor Graph EmbeddingPredictionCombinated RepresentationfoutputfoutputQlQrKrKlVlVrComplex ProteinLossUnder review as a conference paper at ICLR 2023\n\n3.3 THE CROSS-GRAPH TRANSFORMER\n\nThe CGT module models structural correlation across proteins and meantime achieves fine-grained learning on PPI sites. It consists two branches, i.e. ligand and receptor branches, that are co-evolved during forward inference. For each branch, two main operations named cross-graph query and multi-head attention are involed. Specifically, the Wasserstein affinity based on the optimal transport is used for cross-graph modeling in the cross-graph query. We introduce them in detail below.\n\nCross-graph query. The cross-query operation aims to measure the structual relationship between two graphs. Considering that irregular graphs don’t lie in the Euclidean space, we apply the Wasserstein affinity to characterize cross-graph structural information through the optimal transport. Here, ] ∈ Rnl×d represent the encoded 1, · · · , zl taking the ligand branch as an example, let Zl = [zl ] ∈ Rnr×d for the receptor graph, then the features of the ligand graph while Zr = [zr 1, · · · , zr Wasserstein affinity of the cross ligand-receptor query, denoted as Al,r ∈ Rnl×nr , can be written as:\n\nnr\n\nnl\n\nAl,r = ql(Zl, Zr, Θl) = Tλ\n\nl,r (cid:12) Ml,r,\n\n(3)\n\nwhere Tl,r is the optimal transport matrix, (cid:12) represents elementwise production, and Θl denotes the parameter set involved. Ml,r ∈ Rnl×nr is a pairwise affinity matrix with the element in the i-th row and j-th column calculated as:\n\nMl,r(i, j) = cosine(Wl\n\nQzl\n\ni, Wr\n\nKzr\n\nj ).\n\n(4)\n\nWl\n\nQ, Wr\n\nK ∈ Θl are learnable matrices corresponding to the ligand and receptor.\n\nSimlar with the cross ligand-receptor query, the cross receptor-ligand query qr(Zr, Zl, Θr) calculates the Wasserstein affinity Ar,l ∈ Rnr×nl using with corresponding parameters, e.g. Wr\n\nQ, Wl\n\nK.\n\nMulti-head attention. Based on those cross-graph query-resulted Wasserstein affinities Al,r, Ar,l, we further derive the multi-head attetion to adaptively highlight those salient pairs of residues. For simplication, we take the ligand branch with one-head attention as an example, which can be easily extended to the multi-head case. Formally, for the ligand branch:\n\nFl = fl(Zl, Zr, Φl) = sof tmax(Al,r)Vl.\n\n(5)\n\nHere, Φl denotes the parameter set involved in the multi-head attention, sof tmax(·) applies softmax operation along the rows of Al,r and results in a nl × nr dimension matrix for adaptively weighting V , Vl ∈ Rnr×d(cid:48) the nodes in the value graph Vl = ZrWl The above cross-graph query and Multi-head attention operations fulfill the cross-graph tranformer with one-head attention. And the multi-head attention Fm r can be achieved by conducting parallel multi-channel transformation f m r (·) based on Eqn. (3)-(5), and then aggregating all those channels. Moreover, multi-layer CGT modules can be stacked in a deep architecture with the ligand and receptor branches co-evoled, where the output features of the previous layer are concatenated with those of the current layer. The whole process is shown in Algorithm 1.\n\n. Fl ∈ Rnl×d(cid:48)\n\nis the output.\n\nl (·), f m\n\nl , Fm\n\nThe optimal transport. In the learning process above, one crucial step is to learn the matrix Tλ l,r in Eqn. (3) for calculating the Wasserstein affinity between two cross-graph node sets. Formally, Tλ\n\nis the solution of an entropy-smoothed optimal transport problem:\n\nl,r ∈ Rnl×nr\n\n+\n\nTλ\n\nl,r = argmin λ (cid:104)Tl,r, Ml,r(cid:105) − Ω(Tl,r),\n\n(6)\n\nwhere (cid:104)A, B(cid:105) = tr(AT B), Ω(Tl,r) is a discrete joint probability distribution calculating the entropy of Tl,r. Specifically, the optimization problem in Eqn.(6) can be efficiently solved through Sinkhorn’s fixed point iterations(Cuturi, 2013), and the solution can be written as:\n\nTl,r = diag(ul,r)Kl,rdiag(vl,r) = ul,r1T\n\nnr\n\n(cid:12) Kl,r (cid:12) 1nl vT\n\nl,r,\n\n(7)\n\nwhere Kl,r is calculated based on the distance matrix Ml,r with Kl,r = e−λMl,r . In Sinkhorn iterations, ul,r and vl,r are kept alternately update. Taking the k-th iteration as an example, the update takes the following form:\n\nvk\n\nl,r =\n\nwith u0\n\nl,r = 1nl as an initialization.\n\n1nr /nr l,ruk−1\n\nl,r − 1\n\nKT\n\n, uk\n\nl,r =\n\n1nl /nl Kl,rvk\n\nl,r\n\n,\n\n(8)\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Stacking multi-layer CGT modules. Input: Encoded ligand and receptor graph features Zl, Zr; number of stacked layers K\n\n1 for k ← 1 to K do\n\n// Conducting cross-graph query according to Eqn.(3-8), then calculating multi-head attention through applying the multi-channel version of Eqn.(5) based on A(k)\n\nl,r , A(k)\n\nr,l ;\n\nif k = 1 then F(k) F(k)\n\nl = f m r = f m\n\nl (Zl, Zr, Φl); r (Zr, Zl, Φr);\n\nelse\n\nF(k) F(k)\n\nl = f m r = f m\n\nl (Ok−1 r (Ok−1\n\nr\n\nl\n\n, Ok−1 r\n, Ok−1\n\nl\n\n, Φl); , Φr);\n\nend // node feature update; O(k) O(k)\n\nl = foutput(Concatenate(O(k−1) r = foutput(Concatenate(O(k−1)\n\nr\n\nl\n\n));\n\n, F(k) , F(k)\n\nl r ));\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10 11 end\n\nOutput: The output graph features O(k)\n\nl\n\n, O(k)\n\nr of the K-layer CGT module\n\n4 EXPERIMENT\n\nIn this section, we first introduce the used datasets and the experiment setup including the implementation details, parameter setting and the evaluation metircs. Then we compare our model with other baseline methods. Finally, we analyze our method through ablation experiments.\n\n4.1 DATASET\n\nThe employed datasets are DB3 (Hwang et al., 2008), DB4 (Hwang et al., 2010), DB5 (Vreven et al., 2015), and DB5.5 1 from the family of Docking Benchmark (DB) datasets. Each dataset is split into training and test sets. DB5.5 is the latest and largest dataset consisting of totally 271 complexes, which provides the bound state of two given proteins as well as the unbound state of each protein. The DB5 is the subset of the DB5.5 with 230 complexes, which is most widely used. The DB3 and DB4 are subsets of DB5, which are relatively small. Specially, the DB3 contains 119 complexes and DB4 has 171 complexes totally.\n\n4.2 EXPERIMENT SETUP\n\nData Preprossing Considering the irregular structure of proteins, graphs are constructed to describe them by taking residues as nodes with their edges defined according to the spatial relationship. On all datasets, we extract the same features and labels by following the work in (Fout et al., 2017). Generally, the features are divided into two types, i.e. sequence features and structural features. Specially, the Position Specific Scoring Matrix(PSSM) is extracted from amino acid sequence as sequence features. For structural features, we extract Relative Accessible Surface Area(rASA), residue depth, protrusion index, hydrophobicity and half sphere amino acid composition. Finally, we combine all the aforementioned features, which result in 70-dimension features for nodes. Edge features are composed of average atomic distance and the angle between two residues (Fout et al., 2017). To define the label, a pair of amino acids are treated to be interactive if any two non-hydrogen atoms, each from one amino acid, are within 6 ̊A, which is also adopted by (Fout et al., 2017; Townshend et al., 2019).\n\nImplementation Details For inputs of ligand–receptor pairs, we employ graph encoders of siameselike networks and each graph encoder is a two-layer GNN module, resulting in features of 512. For the CGT module, we stack it into a two-layer architecture, and foutput in Algorithm 1 is set as a 1-layer GNN. The output 512-dimension features of the two-layer CGT are further passed through two layers of MLP for prediction. For a protein, the number of positive is far less than that of negative\n\n1https://zlab.umassmed.edu/benchmark/\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nsamples, so we employ the weighted cross-entropy loss to train our model in an end-to end manner. Specially, the weight for positive samples is 0.1 while 1 for negative ones. We train our model with stochastic gradient descent optimizer (SGD) for 200 epochs on 2080Ti GPU cards, where the learning rate is 0.1.\n\nProtocol For the Docking Benchmark (DB) datasets, training sets are generally complexes from previous version dataset and test sets are those updated in the current version. For example, we split DB5.5 into a training set corresponding to DB5 and a test set composed of the complexes added in the update from DB5 to DB5.5. We use the same training set and test set as in all baseline methods (Fout et al., 2017; Townshend et al., 2019; Liu et al., 2020; Schütt et al., 2017; Duvenaud et al., 2015) for the DB5 dataset. For the DB3, DB4 and DB5.5 datasets, we use the fixed training examples and test examples shown on the website1. Following the previous works (Fout et al., 2017; Townshend et al., 2019; Liu et al., 2020), we evaluate the performance by median area under the receiver operating characteristic curve (MedAUC) across all complexes in the test set. The experiments are repeated 10 times and we use the average of the ten runs as the final performance. For each method we report the mean and standard deviation of MedAUC across 10 runs.\n\n4.3 EXPERIMENTAL RESULTS\n\nWe compare the CPWT framework with state-of-the-arts methods, including Node Average (NA) (Fout et al., 2017), Node and Edge Average(NEA) (Fout et al., 2017), Deep Tensor Neural Networks (DTNNs) (Schütt et al., 2017), Single Weight Matrix (MFN) (Duvenaud et al., 2015), Siamese Atomic Surfacelet Network (SASNet) (Townshend et al., 2019) and High-Order Pairwise Interactions (HOPI) (Liu et al., 2020). The results are shown in Table 1. Specifically on the DB5 dataset, we compare our performance with all baseline methods reported in (Fout et al., 2017; Townshend et al., 2019; Liu et al., 2020), while the results on DB3 and DB4 datasets of the work (Liu et al., 2020) are not compared here because different protocols are used. According to the results in Table 1, our proposed CPWT method outperforms all baselines on all DB datasets. Specifically, in the most widely used DB5 dataset, our method achieves approximately 0.041 performance gain comparing with the previous work (Fout et al., 2017). It is worth noting that the recent work (Liu et al., 2020) reports relatively high performance among those compared methods on the DB5 dataset. However, it conducts additional data augmentation by incorporating in-protein pairwise interactions as positive pairs of samples. Comparing with this method, we still obtain better performance without the data augmentation. On the DB3 and DB4 datasets with less protein complexes, SASNet and several graphbased methods get poor performance. However, our method still achieve good performance. For the DB5.5 dataset, which is the largest one, our method still get the best result, with the performance gain of 0.013 comparing with NA (Fout et al., 2017).All the results above demonstrate the effectiveness of our proposed CPWT framework and the robustness against variation of dataset size.\n\nTable 1: Comparsion with state-of-the-art-methods.\n\nMethods\n\nDB3\n\nDB4\n\nDB5\n\nDB5.5\n\nNA (Fout et al., 2017) NEA (Fout et al., 2017) DTNNs (Schütt et al., 2017) MFN (Duvenaud et al., 2015) SASNet (Townshend et al., 2019) NeiA+HOPI (Liu et al., 2020) NeiWA+HOPI (Liu et al., 2020)\n\n0.895 (0.015) 0.881 (0.009) 0.853 (0.017) 0.875 (0.019) 0.800 (0.012) -\n-\n\n0.902 (0.005) 0.898 (0.005) 0.870 (0.015) 0.878 (0.009) 0.802 (0.014) -\n-\n\n0.882 (0.007) 0.898 (0.005) 0.880 (0.007) 0.871 (0.013) 0.876 (0.037) 0.919 (0.015) 0.930 (0.016)\n\n0.931 (0.007) 0.917 (0.008) 0.924 (0.013) 0.929 (0.012) 0.910 (0.008) -\n-\n\nCPWT\n\n0.917 (0.001)\n\n0.913 (0.005)\n\n0.939 (0.002)\n\n0.944 (0.008)\n\n4.4 ABLATION STUDY\n\nAs our CPWT framework has achieved promising performance compared to the existing state-of-theart methods, it is interesting and meaningful to make clear how the modules or parameters setting, e.g. the number of CGT layers and graph encoder layers, influence the performance of protein interaction prediction. For this purpose we conduct several additional experiments to explore our framework based on the DB5 dataset as follows.\n\nInfluence of the CGT layer number. To investigate the impact of CGT module on our framework, we stack multiple CGT layers and compare the performance between them. The results are shown in\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: The evaluation of hyperparameters.(a) the number of CGT layers (denoted as N), (b) λ in Wasserstein affinity, (c)K in Wasserstein affinity)\n\nFig. 3 (a). When the number of CGT layers is set to 0, it means no CGT module in our framework. It can be observed that our designed CGT module effectively promote the performance, as 1.3% performance gain is obtained by using a one-layer CGT module comparing with the framework without any CGT module. Moreover,the performance varies with the number of CGT layers. And the best result is obtained when the number is set to 2.\n\nEffectiveness of multi-head attention module. We compare the performance between model with or without multi-head attention. We remove multi-head attention module and use just the Wasserstein affinity (named CPWT_no_MH) to implement cross-graph function. For comprehensive evaluations, we evaluate the result in multiple setting with different GNN layers used in the graph encoder. The results are shown in Table 3. It is observed that the multi-head attention also improves the performance because the performance decreases from 93.9% to 92.3% without using the multi-head attention, which demonstrates the meaningness of conducting fine-grained learning on residues.\n\nComparsion with the cross-protein reciprocal attention. We compare the performance between the models using the Wasserstein affinity and reciprocal attention in PepNN (Abdin et al., 2022), where the cosine similarity is employed for cross-graph measurement. We replace the Wasserstein affinity with the reciprocal attention in our model, which is named as CPRA. The performaces are evaluated on all datasets and the results are shown in Table 2. As it is shown, performance improvement is obtained on all datasets, which verifies the effectiveness of our CPWT by introducing the Wasserstein affinity.\n\nTable 2: Comparsion with reciprocal attention.\n\nDatasets DB3\n\nDB4\n\nDB5\n\nDB5.5\n\nCPRA CPWT\n\n0.887 (0.003) 0.917 (0.001)\n\n0.900 (0.007) 0.913 (0.005)\n\n0.928 (0.003) 0.939 (0.002)\n\n0.935 (0.009) 0.944 (0.008)\n\nInfluence of the hyperparameter in Wasserstein affinity. In Wasserstein affinity module, there are two key hyperparameter λ and K. λ is the regularization coefficient, which controls the strength of the regularization and K controls the iterative update steps. We set different values and compare them. The results are shown in Fig. 3 (c) and Fig. 3 (d). Specifically, λ controls local information between the nodes across two graphs. For too large values of λ, the OT matrix would become little sensitive to the local correlation between two graphs and therefore would degrade the ability of capturing patterns of protein complex. Even though two parameters are varied in a pretty large range of values, the performance of the model stays above 90%.\n\nInfluence of multiple GNN layers in graph encoder. For comprehensive comparison, we conduct experiments with multiple numbers of GNN layers in the graph encoder, and compare the results with those baselines based on graph. The results are shown in Table 3. As shown, more GNN layers lead to the performance reduction because of the over-smoothing. Moreover, we can observe that our proposed CPWT framework can always achieve the best performance even though the number GNN layer varies, which demonstrates the effectiveness of our proposed method.\n\nVisualization of salient pairs of residues. To intuitively understand the effect of our fine-grained learning, we further visualize the factor values of multi-head attention using protein complex 3V6Z as an example in Figure 4. According to Figure 4, intuitively, our proposed CGT module can endow large weights to most interactive pairs comparing with the ground truth. Specifically, compared with\n\n8\n\n(a)(b)(c)MedAUCMedAUCMedAUCNλKUnder review as a conference paper at ICLR 2023\n\nFigure 4: The visualization of the ground truth(a), (b), learned saliencies (c), (d) and reciprocal attention (e), (f) separately in the heatmap and protein cartoon model upon protein complex 3V6Z. The red parts in (b) are the binding sites of the ground truth in the complex and the red parts in (d) and (f) denotes the learned salient pairs of sites.\n\nTable 3: Comparsion with graph-based methods on the DB5 dataset.\n\nGNN Layers\n\n1\n\n2\n\n3\n\n4\n\nNA (Fout et al., 2017) NEA (Fout et al., 2017) DTNNs (Schütt et al., 2017) MFN (Duvenaud et al., 2015) NeiA+HOPI (Liu et al., 2020) NeiWA+HOPI (Liu et al., 2020)\n\n0.864 (0.007) 0.876 (0.005) 0.867 (0.007) 0.865 (0.007) 0.902 (0.012) 0.908 (0.019)\n\n0.882 (0.007) 0.898 (0.005) 0.880 (0.007) 0.871 (0.013) 0.919 (0.015) 0.930 (0.016)\n\n0.891 (0.005) 0.895 (0.006) 0.882 (0.008) 0.873 (0.017) 0.921 (0.009) 0.924 (0.011)\n\n0.889 (0.005) 0.889 (0.007) 0.873 (0.012) 0.869 (0.017) 0.915 (0.009) 0.914 (0.013)\n\nCPWT_no_MH CPWT\n\n0.912 (0.006) 0.923 (0.003)\n\n0.923 (0.005) 0.939 (0.002)\n\n0.921 (0.006) 0.932 (0.002)\n\n0.915 (0.005) 0.928 (0.005)\n\nthe reciprocal attention in (Abdin et al., 2022), our framework achieves more fine-grained learning as it has less red area but generally covers the ground truth. Hence, Table 2 and Fig. 4 jointly verify the effectiveness of our CPWT framewrok by introducing Wasserstein affinity.\n\n5 CONCLUTION\n\nIn this paper, a novel CPWT framework was proposed for the PPI prediction task. Based on the hypothesis of intimate structure-function relationship, the CPWT framework focuses on cross-protein structural modeling as well as fine-grained learning on pairs of residues. Considering the irregular structure of proteins, graphs were constructed to describe them, with graph encoders then applied for robust features. Then, a core Cross-Graph Transformer (CGT) module was proposed for cross-protein structural modeling, where the cross-graph query was conducted based on Wasserstein affinities across graphs. Moreover, the multi-head attention was accordingly derived for mining fine-grained cues. For better feature learning ability, the CGT was stacked into a multi-layer structure with the ligand and receptor branches co-evolved during forward inference. Comprehensive experiments were conducted for performance evaluation, and the learned fine-grained saliencies were also visualized. All these above verified the effectiveness of our CPWT framework.\n\n9\n\n(a) GroundTruth(b) GroundTruthin cartoon(c) Wasserstein affinity(d) Wasserstein affinity in cartoon(e) Reciprocal attention(f) Reciprocal attention in cartoonUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nOsama Abdin, Satra Nim, Han Wen, and Philip M Kim. Pepnn: a deep attention model for the\n\nidentification of peptide binding sites. Communications Biology, 5(1):1–10, 2022.\n\nFayyaz ul Amir Afsar Minhas, Brian J Geiss, and Asa Ben-Hur. Pairpred: partner-specific prediction of interacting residues from sequence and structure. Proteins: Structure, Function, and Bioinformatics, 82(7):1142–1155, 2014.\n\nAfshine Amidi, Shervine Amidi, Dimitrios Vlachakis, Vasileios Megalooikonomou, Nikos Paragios, and Evangelia I Zacharaki. Enzynet: enzyme classification using 3d convolutional neural networks on spatial representation. PeerJ, 6:e4750, 2018.\n\nJörg Behler and Michele Parrinello. Generalized neural-network representation of high-dimensional\n\npotential-energy surfaces. Physical review letters, 98(14):146401, 2007.\n\nHelen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig, Ilya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic acids research, 28(1): 235–242, 2000.\n\nJames R Bradford and David R Westhead. Improved prediction of protein–protein binding sites using\n\na support vector machines approach. Bioinformatics, 21(8):1487–1494, 2005.\n\nPatrick Bryant, Gabriele Pozzati, and Arne Elofsson. Improved prediction of protein-protein interac-\n\ntions using alphafold2. Nature communications, 13(1):1–11, 2022.\n\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural\n\ninformation processing systems, 26, 2013.\n\nBowen Dai and Chris Bailey-Kellogg. Protein interaction interface region prediction by geometric\n\ndeep learning. Bioinformatics, 37(17):2580–2588, 2021.\n\nXiuquan Du, Shiwei Sun, Changlin Hu, Xinrui Li, and Junfeng Xia. Prediction of protein–protein interaction sites by means of ensemble learning and weighted feature descriptor. Journal of Biological Research-Thessaloniki, 23(1):23–28, 2016.\n\nDavid K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. Advances in neural information processing systems, 28, 2015.\n\nRichard Evans, Michael O’Neill, Alexander Pritzel, Natasha Antropova, Andrew W Senior, Timothy Green, Augustin Žídek, Russell Bates, Sam Blackwell, Jason Yim, et al. Protein complex prediction with alphafold-multimer. BioRxiv, 2021.\n\nAlex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph\n\nconvolutional networks. Advances in neural information processing systems, 30, 2017.\n\nPablo Gainza, Freyr Sverrisson, Frederico Monti, Emanuele Rodola, D Boscaini, MM Bronstein, and BE Correia. Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. Nature Methods, 17(2):184–192, 2020.\n\nMu Gao, Davi Nakajima An, Jerry M Parks, and Jeffrey Skolnick. Af2complex predicts direct physical interactions in multimeric proteins with deep learning. Nature communications, 13(1): 1–13, 2022.\n\n ̇Ismail Haberal and Hasan O ̆gul. Deepmbs: Prediction of protein metal binding-site using deep learning networks. In 2017 Fourth International Conference on Mathematics and Computers in Sciences and in Industry (MCSI), pp. 21–25. IEEE, 2017.\n\nInbal Halperin, Buyong Ma, Haim Wolfson, and Ruth Nussinov. Principles of docking: An overview of search algorithms and a guide to scoring functions. Proteins: Structure, Function, and Bioinformatics, 47(4):409–443, 2002.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nHowook Hwang, Brian Pierce, Julian Mintseris, Joël Janin, and Zhiping Weng. Protein–protein docking benchmark version 3.0. Proteins: Structure, Function, and Bioinformatics, 73(3):705–709, 2008.\n\nHowook Hwang, Thom Vreven, Joël Janin, and Zhiping Weng. Protein–protein docking benchmark\n\nversion 4.0. Proteins: Structure, Function, and Bioinformatics, 78(15):3111–3114, 2010.\n\nJosé Jiménez, Stefan Doerr, Gerard Martínez-Rosell, Alexander S Rose, and Gianni De Fabritiis. Deepsite: protein-binding site predictor using 3d-convolutional neural networks. Bioinformatics, 33(19):3036–3042, 2017.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\nVanessa Isabell Jurtz, Alexander Rosenberg Johansen, Morten Nielsen, Jose Juan Almagro Armenteros, Henrik Nielsen, Casper Kaae Sønderby, Ole Winther, and Søren Kaae Sønderby. An introduction to deep learning on biological sequence data: examples and solutions. Bioinformatics, 33(22):3685–3690, 2017.\n\nBi-Qing Li, Kai-Yan Feng, Lei Chen, Tao Huang, and Yu-Dong Cai. Prediction of protein-protein\n\ninteraction sites by random forest algorithm with mrmr and ifs. 2012.\n\nYi Liu, Hao Yuan, Lei Cai, and Shuiwang Ji. Deep learning of high-order interactions for protein In Proceedings of the 26th ACM SIGKDD international conference on\n\ninterface prediction. knowledge discovery & data mining, pp. 679–687, 2020.\n\nAlex Morehead, Chen Chen, and Jianlin Cheng. Geometric transformers for protein interface contact\n\nprediction. ICLR, 2022.\n\nYoichi Murakami and Kenji Mizuguchi. Applying the naïve bayes classifier with kernel density estimation to the prediction of protein–protein interaction sites. Bioinformatics, 26(15):1841–1848, 2010.\n\nHani Neuvirth, Ran Raz, and Gideon Schreiber. Promate: a structure based prediction program to identify the location of protein–protein binding sites. Journal of molecular biology, 338(1): 181–199, 2004.\n\nThomas C Northey, Anja Bareši ́c, and Andrew CR Martin. Intpred: a structure-based predictor of\n\nprotein–protein interaction sites. Bioinformatics, 34(2):223–229, 2018.\n\nSrivamshi Pittala and Chris Bailey-Kellogg. Learning context-aware structural representations to\n\npredict antigen and antibody binding interfaces. Bioinformatics, 36(13):3996–4003, 2020.\n\nAleksey Porollo and Jarosław Meller. Prediction-based fingerprints of protein–protein interactions.\n\nProteins: Structure, Function, and Bioinformatics, 66(3):630–645, 2007.\n\nKathryn A Porter, Israel Desta, Dima Kozakov, and Sandor Vajda. What method to use for protein–\n\nprotein docking? Current opinion in structural biology, 55:1–7, 2019.\n\nDaniel P Ryan and Jacqueline M Matthews. Protein–protein interactions in human disease. Current\n\nopinion in structural biology, 15(4):441–446, 2005.\n\nKristof T Schütt, Farhad Arbabzadah, Stefan Chmiela, Klaus R Müller, and Alexandre Tkatchenko. Quantum-chemical insights from deep tensor neural networks. Nature communications, 8(1):1–8, 2017.\n\nMiha Skalic, Alejandro Varela-Rial, José Jiménez, Gerard Martínez-Rosell, and Gianni De Fabritiis. Ligvoxel: inpainting binding pockets using 3d-convolutional neural networks. Bioinformatics, 35 (2):243–250, 2019.\n\nYang Song, Vipul Madahar, and Jiayu Liao. Development of fret assay into quantitative and highthroughput screening technology platforms for protein–protein interactions. Annals of biomedical engineering, 39(4):1224–1234, 2011.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDmitri I Svergun, Maxim V Petoukhov, and Michel HJ Koch. Determination of domain structure of\n\nproteins from x-ray solution scattering. Biophysical journal, 80(6):2946–2953, 2001.\n\nFreyr Sverrisson, Jean Feydy, Bruno E Correia, and Michael M Bronstein. Fast end-to-end learning on protein surfaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15272–15281, 2021.\n\nWen Torng and Russ B Altman. 3d deep convolutional neural networks for amino acid environment\n\nsimilarity analysis. BMC bioinformatics, 18(1):1–23, 2017.\n\nRaphael Townshend, Rishi Bedi, Patricia Suriana, and Ron Dror. End-to-end learning on 3d protein structure for interface prediction. Advances in Neural Information Processing Systems, 32, 2019.\n\nThom Vreven, Iain H Moal, Anna Vangone, Brian G Pierce, Panagiotis L Kastritis, Mieczyslaw Torchala, Raphael Chaleil, Brian Jiménez-García, Paul A Bates, Juan Fernandez-Recio, et al. Updates to the integrated protein–protein interaction benchmarks: docking benchmark version 5 and affinity benchmark version 2. Journal of molecular biology, 427(19):3031–3041, 2015.\n\nKurt Wuthrich. Protein structure determination in solution by nuclear magnetic resonance spec-\n\ntroscopy. Science, 243(4887):45–50, 1989.\n\nBuzhong Zhang, Jinyan Li, Lijun Quan, Yu Chen, and Qiang Lü. Sequence-based prediction of protein-protein interaction sites by simplified long short-term memory network. Neurocomputing, 357:86–100, 2019.\n\nJinfang Zheng, Xiaoli Zhang, Xunyi Zhao, Xiaoxue Tong, Xu Hong, Juan Xie, and Shiyong Liu. Deep-rbppred: Predicting rna binding proteins in the proteome scale based on deep learning. Scientific reports, 8(1):1–9, 2018.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe paper proposes Cross-Protein Wasserstein Transformer (CWPT) for protein-protein interaction site detection. For this it proposes an elaborate architecture, composed of three main parts: (i) A graph network as the encoder of the individual proteins, (ii) a (cross-graph) transformer with cross-attention (from one protein to another) using a Wasserstein affinity, and (iii) a final module to operate on the cross-protein embeddings to detect the interaction sites.\n\n# Strength And Weaknesses\n\n- The ablation studies with regards to the importance of different components are informative.\n\n- Given the recent advances in protein-complex folding, what is the benefit of PPI site prediction?\n- The results should have been compared with the recent methods that do structure prediction for a pair or a complex of proteins.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is not entirely clear and raises quite a few questions:\n\n- what is meant by irregular graphs when referring to protein graphs?\n- why should the weights $W_Q$ and $W_K$ be different for the ligand and receptor?\n- what is the formal motivation / rationale for using Wasserstein affinity when doing cross attention? \n- what is the computational burden of the model with Wasserstein affinity and how does it compare with the chosen baselines?\n- how is the final site prediction done? The method section ends by describing the cross-attention (second) module and has no description of the third (last) detection module.\n\n# Summary Of The Review\n\nThe work does not have a technical machine learning contribution and hence should be refereed as an application paper. Given that there are quite a few recent works on protein complexes, it is unclear what application benefits the results of this work bring about. Furthermore, the presentation requires important clarifications. These together suggest that the paper may not be ready for publication.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSOCIAL AND ENVIRONMENTAL IMPACT OF RECENT DEVELOPMENTS IN MACHINE LEARNING ON BIOLOGY\n\nAND CHEMISTRY RESEARCH\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nPotential societal and environmental effects such as the rapidly increasing resource use and the associated environmental impact, reproducibility issues, and exclusivity, the privatization of ML research leading to a public research brain-drain, a narrowing of the research effort caused by a focus on deep learning, and the introduction of biases through a lack of sociodemographic diversity in data and personnel caused by recent developments in machine learning are a current topic of discussion and scientific publications. However, these discussions and publications focus mainly on computer science-adjacent fields, including computer vision and natural language processing or basic ML research. Using bibliometric analysis of the complete and full-text analysis of the open-access literature, we show that the same observations can be made for applied machine learning in chemistry and biology. These developments can potentially affect basic and applied research, such as drug discovery and development, beyond the known issue of biased data sets.\n\n1\n\nINTRODUCTION\n\nThe unprecedented progress of machine learning during the past two decades has been catalysed and remains driven by the development of increasingly powerful computer hardware. This progress is enabled by the ability of deep neural networks to scale exceptionally well with increasing data availability and model complexity compared to other approaches. Thus, they can be trained for linear regression on small data sets and, with conceptually simple changes to the network architecture, for language translation or image generation on immense text corpora and image collections. While comparatively exceptional, deep neural networks are understood to still only scale linearly at an exponential cost (Schwartz et al., 2020), leading to diminishing returns (Thompson et al., 2021). Among the machine learning community, this has raised concern over the future direction of the field and a growing exclusivity driven by ever-increasing hardware and energy costs (Schwartz et al., 2020; Thompson et al., 2021; Jurowetzki et al., 2021). After a discourse on the intertwined recent history of deep learning and hardware advances, we will analyse the applicability of the most prominent concerns raised in machine learning research to applied machine learning research in biology and chemistry. We have categorised these concerns under socioeconimic, scientific, and environmental considerations.\n\nThe hard- and software that catalysed rapid developments in machine learning In late 2002 and early 2003, the release of the Radeon 9700 and GeForce FX video cards introduced a fully programmable graphics pipeline, extending and later replacing the existing fixed function pipelines. Unlike the fixed function pipeline, which allowed the user to only supply input matrices and parameters to built-in operations, the programmable pipeline introduced the execution of user-written shader programs on the GPU (Contributors, 2015). This fundamental change allowed programmers and researchers to exploit the intrinsic parallelism of GPUs 2 years before Intel would introduce its first dual-core CPU. Within months of the availability of this new hardware and the accompanying APIs, researchers implemented linear algebra methods on GPUs and introduced programming frameworks to use GPUs for general-purpose computations (Thompson et al., 2002; Kr ̈uger & West-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nermann, 2003). This rapid development marked the dawn of general-purpose computing on graphics processing units (GPGPU). In a presentation at ICS ’08, Harris presented the successes of GPGPU by highlighting a speed-up in molecular docking, N-body simulations, HD video stream transcoding, or image processing—applications in machine learning were not discussed. However, just one year later, the introduction of GPUs as general-purpose processors catalysed the deep learning explosion of the early 2010s by allowing deep learning algorithms pioneered by Alexey Ivakhnenko in 1971 to be run within practical time on widely available consumer hardware when Rajat et al. showed that GPUs outperform CPUs by an order of magnitude in large-scale deep unsupervised learning tasks (Ivakhnenko, 1971; Raina et al., 2009).\n\nHardware and energy requirements increase in machine learning research In 2010, Ciresan et al. (2010) introduced a multi-layer perceptron (MLP) with up to 12.11 million free parameters where forward and backward propagation were implemented on a GPU using NVIDIA’s proprietary CUDA API introduced by Harris at ICS ’08 two years before, speeding up the routines by a factor of 40. In their arXiv paper, they also report the computer’s hardware specifications as ”Core2 Quad 9450 2.66GHz processor, 3GB of RAM, and a GTX280 graphics card”. The GTX 280 graphics card by NVIDIA was, at the time of the paper’s writing, two years old and cost USD 893 when first released (adjusted for inflation). Equipped with this 2-year-old hardware that cost well below USD 1,000, Cires ̧an et al. were able to improve upon the state-of-the-art performance on the MNIST classification benchmark set four years prior by Ranzato et al. (2006). As they not only reported the hardware used but also the time it took to train the model, the power usage of the GPU and CPU, with a thermal design power (TDP) of 236 and 95 Watt, respectively, can be calculated as 114.5h × (236 + 95)W = 37.9kWh. Seven years later, Vaswani et al. (2017) introduced the transformer architecture. The training used 8 NVIDIA Tesla P100 GPUs, whose price was ∼USD 55,100 at the time, and took 84 hours, resulting in overall energy usage of 84h × 8 × 250W = 168kWh.\n\nHardware and energy requirements explode in applied machine learning Applying the novel transformer architecture, NVIDIA reportedly trained the 345 million parameter BERT model in 2019, which was previously introduced by Google in the same year, on 4 DGX-2H servers (64 Tesla V100s) in 79.2 hours, with a maximum power usage of 12,000 Watt, resulting in a total power use of 3.8 MWh (79.2h × 4 × 12kW) (Devlin et al., 2018). The cost of this system at the time of training was USD 1,596,000. Alternatively, the BERT model could be trained on on-demand Google Cloud GPUs for USD 2.48 per GPU hour, resulting in total costs of USD 12,570 (2.48 × 64 × 79.2). The MT-NLG model presented by NVIDIA and Microsoft in 2021 represents the acceleration of hardware and energy cost in the field (Smith et al., 2022). The 530 billion parameter model was trained on 560 DGX A100 servers—a total of 4,480 NVIDIA A100 80GB Tensor Core GPUs—for 2,160 hours (Rajbhandari et al., 2022). The power usage of a cluster of 560 DGX A100 servers during 2,160 hours is 7.862 GWh (2,160 × 560 × 6.5kW). Taking the world average electricity price of USD 0.131 per kWh during December 2021, the total electricity bill for training MT-NLG was USD 1,029,922. The total cost of the hardware is hard to estimate as specialised network hardware is required to build such a cluster; however, the DGX A100 was priced at USD 199,000 on release, resulting in a minimum total cost of USD 111,440,000 (199,000 × 560). Training the model on on-demand Google Cloud GPUs for USD 2,141.82 per GPU month results in a total cost of USD 28,786,060.8 (3 × 4,480 × 2,141.82).\n\nHardware and energy costs drive the de-democratization of machine learning The examples discussed above represent an increasing hardware and energy cost in conducting basic and applied deep learning-based machine learning research. The resulting diminishing returns and the environmental impact have previously been discussed by Thompson et al. (2021) and Schwartz et al. (2020). The development of increasing costs following a potential breakthrough stands in contrast to similar or even more disruptive changes in other fields, such as CRISPR-Cas9 lowering costs in molecular biology or the ever-decreasing costs of genome and RNA sequencing (Ledford, 2015; Wetterstrand, 2021; Gierahn et al., 2017). While CRISPR-Cas9 and affordable sequencing has led to what has been called the democratization of access to sequencing and genome editing (Guernet & Grumolato, 2017; McPherson, 2014; Srivathsan et al., 2019), cutting-edge machine learning research is becoming potentially increasingly expensive and exclusive (Ahmed & Wahed, 2020). Indeed, in a 2020 article on the most cited research articles, all mentioned machine learning research was conducted by, or in collaboration with, OpenAI, Microsoft, and Alphabet (Kingma & Ba, 2014; Ren et al.,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2015; Mnih et al., 2015; Vaswani et al., 2017; Silver et al., 2016; Crew, 2020), suggesting a need for resources not available within academia. While the involvement of these companies, whose R&D budgets exceed those of most nations (Bajpai, 2021; Ballard, 2021; Uis, 2022), contributed greatly to the advancement of machine learning, a potential technological dependency of academia on this commercially driven progress combined with a brain-train from academia to industry may result in a narrowing of machine learning research (Jurowetzki et al., 2021; Klinger et al., 2020; Hagendorff & Meding, 2020). In the following paragraph we will discuss these developments and the concerns raised with a focus on applied machine learning in chemistry and biology.\n\nFigure 1: Fraction of applied machine learning publications in biology and chemistry. After an increase in the early 2000s and a plateau lasting for 10 years, the introduction of general-purpose GPUs and a renewed interest in deep neural networks making use of this capable hardware sparked a unprecedented growth of machine learning research in biology and chemistry in the mid-2010s.\n\n2 SOCIOECONOMIC CONSIDERATIONS\n\nAvailable research funding differs greatly between countries and institutions. While discussing the reasons and possible solutions to this situation are far beyond the scope of this research, it is important to note that cost of research being driven by ever increasing hardware and electricity requirements has the potential to further drive inequity between nations, institutions, research groups, and individual researchers based on available funding (Gerhards et al., 2018; Nielsen & Andersen, 2021; Wahls, 2018). In addition to direct funding, collaborations with leading big tech companies, which led to the most cited recent publications (Crew, 2020), are equally unevenly distributed among countries and institutions, further concentrating funding. Furthermore, the availability of necessary hardware to conduct state-of-the-art deep learning research can depend on the current geopolitical situation, possibly disproportionately affecting researchers in low- and middle-income countries negatively (Nellis & Lee, 2022). In this section, we discuss social and economic concerns raised within the machine learning community, including machine learning in a divided society and that diminishing returns limit participation.\n\nMachine learning in a divided society In a wider social context, the concentration of resources and talent needed to develop and operate machine learning systems in high-income countries form the basis of what has been labelled colonialist AI (Birhane, 2020; Mohamed et al., 2020; Adams, 2021). The effects of this process recently came to light through the discovery of biases in models in healthcare and policing, among others (Obermeyer et al., 2019; Hildebrandt et al., 2020). Causes of these biases have been shown to go beyond biased data (Schwartz et al., 2022; Mehrabi et al., 2021).\n\nThese effects introduced above are of interest for the natural sciences, concretely biology and chemistry, as deep learning-based applied machine learning plays an ever more critical role in both fields and issues being caused by said effects may affect crucial research such as drug development or genetics. The increasing importance of machine learning in biology and chemistry is reflected by an increasing share of machine learning-related publications, which have approximately doubled within the past six years compared to the overall published literature in the respective fields (Figure 1). Here, the potential for the described societal and environmental effects within the scientific\n\n3\n\n20002005201020152020Year0.100.150.200.250.300.350.400.45Fraction of ML Publications [%]BiologyChemistryUnder review as a conference paper at ICLR 2023\n\nfields of biology and chemistry will be explored based on quantitative bibliometric analysis. The utilised data was downloaded from the OpenAlex index of scholarly works (Priem et al., 2022). The final data set contained 27,861 biology and 16,301 chemistry works filtered for machine learning topics. For a list of filter keywords, see Appendix A.\n\nFigure 2: R&D funding of nation states and ”big tech” companies. (a) R&D funding per researcher and year correlates well with a countries GDP. (b) The amount of R&D funding per researcher and year differs greatly between countries. Comparing the R&D budgets per employee (includes nonresearch employees) of ”big tech” companies places them among the top funded nations.\n\nDiminishing returns that limit participation As current and future diminishing returns of deep learning in basic and applied machine learning research threaten to drive costs up, the financial barrier to participating in the respective fields also increases (Thompson et al., 2021; Schwartz et al., 2020). Given the extreme disparities in R&D funding across countries (Figure 2) and institutions has great potential to also increase the exclusiveness of participating in state-of-the-art research. Indeed, potential downstream effects of such disparities have recently come in the focus in fields such as health care, bioinformatics, and biometrics, where machine learning models are often biased towards persons of European descent (Obermeyer et al., 2019; Gupta et al., 2020; Celi et al., 2022; Dias & Torkamani, 2019; Kiseleva & Quinn, 2021; Drozdowski et al., 2020; Hazirbas et al., 2022). While these issues are being tackled by increasing participation of students, developers, and researchers of non-European descent and by debiasing data sets, it is being questioned whether these efforts alone can adequately solve the bias-problem and avert a future long-term dependence of the global south on the global north (Birhane, 2020; Bon et al., 2021).\n\nIn respect to machine learning in biology and chemistry, we make the assumption that an increase in the correlation between available funding and the share of machine learning literature published in each field would suggest a potential for exclusiveness and a measure for inequity. In order to identify a possible increase in correlation between available R&D funding and applied machine learning in biology and chemistry, as well as the impact of deep learning-based machine learning, 137,506 affiliations from bioinformatics/computational biology and 80,206 affiliations from cheminformatics/computational chemistry publications were analysed for the periods between 2000 to 2016 and 2017 to 2021 (Figure 3). Only countries with at least 100 publications per year during the specified periods were included, and outliers were removed using a z-score cut-off of ±3. Plotting the fraction of machine learning publications amongst the entire published literature for the chosen topics from biology and chemistry for each country against the countries’ available funding per researcher shows an increasing, however as of yet insignificant, correlation. While the average fraction of machine learning literature increased by 19.3% (21.7% to 41%) and 9% (20.4% to 29.4%) in biology and chemistry, respectively, the Gini coefficient of the fraction of machine learning publications remained stable in biology (0.287 to 0.287) and decreased in chemistry (0.343 to 0.304). The fund-\n\n4\n\n020000400006000080000100000120000GDP Per Capita [USD]0246Funding Researcher/Year[USD]1e5r=0.716, p=0.0aSYRMDGAZEUZBPAKGEOLAOMDAGMBSENMicrosoftAppleNvidiaAlphabetDEUAUTLUXAUSBELAmazonGRLASMFacebookUSACHEQATCountry/Company104105Funding Researcher/YearEmployee/Year [USD]bUnder review as a conference paper at ICLR 2023\n\nFigure 3: Fraction of applied machine learning papers in biology and chemistry as a function of R&D funding per researcher and year. (a) Biology 2000–2016, (b) chemistry 2000–2016, (c) biology 2017–2021, (d) chemistry 2017–2021. While not significantly so, a potential trend is emerging where the fraction of applied machine learning publications (of each fields total) correlates with the funding per researcher and year.\n\ning per researcher and year is an average of the available data between 2000 and 2021 for each country—with an average Gini coefficient of 0.479 (0.452–0.497) over all stratifications.\n\n3 SCIENTIFIC CONSIDERATIONS\n\nDifferent concerns with a potential effect on the scientific community have been raised within the machine learning field. In this section, we analyse the most prominent of these concerns including the privatization of machine learning, a possible academic brain-drain, citation inequality, and a narrowing of research.\n\nThe privatization of machine learning Beyond Universities, technology companies are major contributors to basic and applied deep learning-based machine learning research across multiple fields. Prominent examples are AlphaFold by DeepMind, BERT by Google, 3D human pose estimation by Facebook and Google, or DALL·E by OpenAI (Jumper et al., 2021; Devlin et al., 2018; Ramesh et al., 2021). In addition, the hardware company NVIDIA, which GPUs and CUDA framework power most basic and applied deep learning research (with the notable exception of research conducted by Google and DeepMind, both using Google’s TPUs), is active in both basic research and deep learning applied to computer graphics (Karras et al., 2020; Wang et al., 2019). The involvement of these corporations in research is significant, as their respective R&D budgets are higher than those of most countries (Figure 2b). For 2020 (2021 for NVIDIA), the R&D spending of Google (now Alphabet) was USD 27.57B, that of Facebook USD 18.45B, and that of NVIDIA USD 3.92B and were higher than that of 120, 116, and 101 countries, respectively (Bajpai, 2021; Ballard, 2021). For Google and Facebook, this includes countries such as Belgium, Spain, India, or the Russian Federation (Uis, 2022). The availability of this large amounts of funding for machine learning research in industry can have three significant effects on public university research:\n\n(i) During the past two decades, there has been an unprecedented brain drain of professors from university to industry, a development that has been termed the ”privatization of AI research(-ers)” (Gofman & Jin, 2019; Jurowetzki et al., 2021).\n\n(ii) Industry collaborations that provide access to significant amounts of compute are often limited to elite institutions in countries where funding is already comparatively high, further increasing the divide in resource availability between institutions within a country and globally (Jurowetzki et al., 2021; Ahmed & Wahed, 2020).\n\n(iii) A narrowing of research driven by universities aligning with commercial interest through collaborations, a reliance of universities on models, methods, and hardware developed in industry, and the above mentioned brain-drain. (Klinger et al., 2020; Gofman & Jin, 2019).\n\nPotential consequences of these developments have been shown to include stagnation of (methodical) diversity in machine learning research with a preference for compute-intensive and dataIn addition, an analysis of research prehungry deep learning methods (Klinger et al., 2020).\n\n5\n\n0246Researcher/Year [USD]1e50.00.20.40.60.81.0Fraction of ML Publications [%]r=0.154, p=0.194a0246Researcher/Year [USD]1e5r=0.01, p=0.93b0246Researcher/Year [USD]1e5r=0.172, p=0.1c0246Researcher/Year [USD]1e5r=0.179, p=0.107dUnder review as a conference paper at ICLR 2023\n\nsented at NeurIPS, CVPR, and ICML showed that researchers in the field often fail to report conflicts of interest, that publications from the industry gather significantly higher citations than those from academia, and that industry publishes on trending machine learning topics two years before academia on average (Hagendorff & Meding, 2020).\n\nFigure 4: Transitions by researchers between education and industry. (a) In biology, the overall transitions between academia and industry remain largely balanced over the observed period. (b) In chemistry, we observe a recent steady increase of transitions from academia to industry. (c) Starting in 2016, there is a steady net flow from acadamia to Tech, spiking in 2021. (d) A similar but less pronounced spike can be observed in chemistry.\n\nIn order to assess the existence of such effects in biology and chemistry, the available data was labelled by affiliation-type. Industry affiliations were labelled with either Tech, Pharma, Chem, and Other. Tech includes the large technology companies (Alphabet, Microsoft, Meta, Nvidia, Amazon, IBM, Tencent, Baidu, and Twitter) and their subsidiaries. Pharma includes pharmaceutical companies such as Novartis, Pfizer, and Bayer. Chem includes chemical and petrochemical companies such as BASF, Exxon, and Lonza. Other includes companies not falling into the other categories, such as Schr ̈odinger, General Electrics, or Siemens. All other affiliations were labelled with None.\n\nAcademic brain-drain Following the approach by Jurowetzki et al. (2021), transitions between academia and industry were defined as a year-over-year change in affiliation (if there were industry as well as academic affiliations within the same year, the mode was chosen as the overall affiliation). In the results, based on 92,496 and 54,243 authors of biology and chemistry articles, respectively, trends that are similar to those observed by Jurowetzki et al. (2021) can be noticed (Figure 4). Over the past six years, chemistry saw an unprecedented increase in net transitions of machine learning practitioners from academia to industry (Figure 4b), while overall transitions in biology remain slightly skewed towards industry (Figure 4a). However, biology, and to lesser degree chemistry, saw a significant jump in academia to Tech transitions in 2021 (Figure 4c,d). Interestingly, the data shows a lower baseline of academia to industry transitions compared to the data presented by Jurowetzki et al. (2021), suggesting either fewer transitions from academia to industry overall or, more likely, a tendency in the fields to stop publishing after leaving academia.\n\nCitation inequality The observation by Hagendorff & Meding (2020) of significantly higher rates of citations of papers authored by industry-affiliated authors can be replicated for machine learning in chemistry and partially for machine learning in biology. In biology, citation rates by Pharma, Tech, and Other are significantly higher than those in academia (with a mean of 6.31, 21.64, and 4.13 versus 3.03 citations per year, respectively. Figure 5a). In chemistry, the citations per year are significantly higher for Pharma, Chem, Tech, and Other, compared to academia (with a mean of 4.46, 4.00, 8.71, and 3.62 versus 2.99, respectively. Figure 5g). Breaking down the affiliations\n\n6\n\n20012004200720102013201620192022Year050100150200TransitionsaEducation to IndustryIndustry to EducationNet Flow to Industry20012004200720102013201620192022Year−250255075100125TransitionsbEducation to IndustryIndustry to EducationNet Flow to Industry20012004200720102013201620192022Year051015TransitionscEducation to IndustryIndustry to EducationNet Flow to Industry2015201620172018201920202021Year−2024TransitionsdEducation to IndustryIndustry to EducationNet Flow to IndustryUnder review as a conference paper at ICLR 2023\n\nFigure 5: Industry affiliations and the effect on citations. For both biology (a) and chemistry (g), publications with industry affiliations gather a higher amount of citations per year. This effect is, in both fields, most pronounced for Tech affiliations. Breaking down the industry affiliations by country (Figure 5b–f for biology, h–l for chemistry), it is little surprising that private research generally takes place in countries with high ranking universities and highly developed industries.\n\nby country, it is little surprising that private research generally takes place in countries with highranking universities and highly developed industries (Figure 5b–f for biology, h–l for chemistry).\n\nA narrowing of research A narrowing of machine learning research would certainly have the potential to lead to negative downstream effects in the natural sciences. In addition, there is also the potential for a narrowing of applied methods independently from developments in machine learning research. Such a narrowing of applied methods can be driven by the same factors as a narrowing in basic machine learning research. These factors include demand-side economies of scale or a bandwagon effect, early fortuitous events that result in lock-in to a certain method, the availability of and previous investments in suitable resources such as hardware or personnel, and social dilemmas (Klinger et al., 2020). To explore a potential narrowing of machine learning methods utilised in biology and chemistry, articles have been labelled with the following broad categories: Dimensionality Reduction/Feature Selection, Genetic/Evolutionary, Neural Networks, Other/Unspecified, Regression, Statistic/Probabilistic, and SVM. Analysing the temporal development, the results show that the largely deep learning-based Neural Networks in biology (Figure 6a) and Neural Networks together with RF/Boosting in chemistry (Figure 6b) are mainly responsible for the recent explosive growth of machine learning literature in the two fields. However, the continuous and roughly linear growth of the other categories implies that the fast-growing methods do not seem to grow at the expense of methods from other categories. Plotting the mean of the citations per year for each category shows that the publication of highly cited biology articles utilising Neural Networks predates and is more sustained (Figure 6c) than the publication of highly cited chemistry articles utilising methods from the same category (Figure 6d).\n\n7\n\nNonePharmaChemTechOtherIndustry051015Citations/Year****ns*******aUSGBCNCHITDECA??JPILCountry0255075100125Tech AffiliationsbUSDEGBCHSEDKFRJPBE??Country020406080100Pharma AffiliationscUSGBJPFRCH??DESACNJEUAKRRUTWNLCountry051015Chem AffiliationsdUSCNGBDEFRINAUCAJPITCountry01000200030004000Total AffiliationsefNonePharmaChemTechOtherIndustry0102030Citations/Year*************gUSCNCHGBCAIT??DKKRIECountry010203040Tech AffiliationshUSCHGBDESEJPBEFRHUCNCountry050100150Pharma AffiliationsiUSDECNNLGBFRBRJPCHJECABEKRITUASA??Country05101520Chem AffiliationsjUSCNIRGBDEINESJPFRRUCountry0500100015002000Total Affiliationskl100101102Tech and Pharma Affiliations100101102Tech and Pharma AffiliationsUnder review as a conference paper at ICLR 2023\n\nFigure 6: Number of publications by machine learning method category. (a) Number of publications by machine learning method category in biology. (b) Number of publications by machine learning category in chemistry. (c) Mean citations/year by year and machine learning category in biology. (d) Mean citations/year by year and machine learning category in chemistry.\n\n4 ENVIRONMENTAL CONSIDERATIONS\n\nIn their publication ”Green AI”, Schwartz et al. (2020) showed that the carbon footprint of deep learning has been growing continuously and suggest making efficiency an evaluation criterion in addition to accuracy. They argue that this ever-increasing need for more compute would make deep learning-based machine learning research not only environmentally problematic but also less inclusive due to the previously discussed unequal distribution of access to compute.\n\nFigure 7: Hardware and energy use of applied machine learning in biology and chemistry. (a) GPU models used by biology and chemistry researchers combined, showing an increase in the fraction of machine learning-dedicated Tesla series GPUs used, while the share of consumer and enthusiast oriented GPU series is shrinking. (b) The data on training times and exact GPU model that could be extracted from open access literature shows a general increase in electricity use per publication.\n\nThe quantitative exploration of the environmental impact of applied machine learning in biology and chemistry has been complicated by the low availability of information regarding the hardware used and the time spent to train the models. From a total of 27,861 biology and 16,301 chemistry articles, 14,365 (51.6%) and 4,184 (25.7%) were downloadable as open access articles. From these downloaded articles, the GPU model could be extracted from 646 (4.5%) biology articles and from\n\n8\n\n20002005201020152020Year050010001500PublicationsaOther/UnspecifiedNeural NetworksRF/BoostingDimesionality Reduction/Feature SelectionStatistic/ProbabilisticSVMRegressionGenetic/Evolutionary20002005201020152020Year0200400600Publicationsb20002005201020152020Year024681012Mean Citations/Yearc20002005201020152020Year024681012Mean Citations/Yeard2018(n=61)2019(n=108)2020(n=149)2021(n=214)Year010203040Fraction of Publications [%]aLegacyGeForce 10GeForce 20GeForce 30QuadroTitanTesla2018(n=5)2019(n=12)2020(n=10)2021(n=14)2022(n=9)Year020406080100Power Usage / Publication [kWh]bUnder review as a conference paper at ICLR 2023\n\n243 (5.8%) chemistry articles, and the GPU model as well as the training time from 56 (0.4%) and 14 (0.3%), respectively. Based on this sparse data, the statistical power of the analysis is relatively low. However, certain trends in the GPU model-use can be observed between 2018 and 2021 (Figure 7a): The fraction of Nvidia Titan GPUs has been steadily declining during the entire period. Consumerfocused GPUs from the GeForce 10, 20, and 30 series continue to be used in research, with the fraction of GeForce 20 GPUs increasing. In contrast, GeForce 10 series cards experienced a decline starting in 2019. However, the most important trend is the increase in the fraction of Tesla GPUs.\n\nAs newer GPU models are becoming more energy efficient ( ˇSpeˇtko et al., 2021), efficiency remains a second-class metric compared to accuracy in applied machine learning publications in biology and chemistry. This situation is reflected in the lack of an increase in reported hardware use or training times (Figure 7b). Indeed, the available data shows a general increase in power usage per publication based on the reported GPU model, the number of GPUs used, and the training time. The values are an estimation and only repreent the training time of the final model.\n\n5 CONCLUSION\n\nThe societal and environmental impacts of recent developments in machine learning are actively being discussed in machine learning research and closely adjoined fields such as natural language processing or image processing. However, the effects driven by and experienced within applied machine learning in the natural sciences received little attention. This exploratory study into the social and environmental impact of recent developments in machine learning on biology and chemistry research brings to light the following insights:\n\n(i) The introduction of deep learning methods has caused a rapid increase in the share of\n\nmachine learning-related articles in biology and chemistry literature.\n\n(ii) There is a potential emerging trend towards an increase in inequity in applied machine learning research conducted in biology and chemistry based on available funding when corrected for overall publications in the respective field. While not yet statistically significant, the trend should be monitored and countered. There is, however, a significant difference between citation metrics of education vs industry affiliated researchers, with university-affiliated research receiving fewer citations.\n\n(iii) Based on transition patterns of researchers between academia and industry, big tech companies seem to get increasingly involved in conducting applied machine learning research in biology and chemistry. Given the vast resources and R&D spending of these corporations (even compared to nations), the concerns of a potential brain-drain away from academia and a narrowing of the research conducted are also warranted in biology and chemistry.\n\n(iv) As of yet and based on the number of publications, the growth of neural networks-based deep learning research did not happen at the expanse of other, generally less computeintensive, categories of machine learning methods. However, citation metrics for neural network-based deep learning methods have recently spiked compared to other categories. While this data does not show a narrowing of the use of methods, continuous monitoring of these metrics to avoid a potential method and technology lock-in, especially as investments in specialized hardware continue, may be warranted.\n\n(v) Data that allows an estimation of the environmental impact of applied machine learning in biology and chemistry is scarce due to a failure to report used hardware and spent training time in the vast majority of articles. This scarcity shows the need for an effort by both authors and publishers to introduce measures concerning reporting these metrics, especially as their importance goes beyond monitoring the environmental impact. Nevertheless, based on the available data, a trend towards more specialized hardware and an overall increase in energy use per paper could be observed.\n\nOverall, these insights are similar to the ones made in machine learning research in general and warrant action, given that research topics with a high societal impact, such as genetics or drug development, can be affected. While each topic may be discussed in detail in future publications, this overview provides the basis for discussion and draws attention to the crucial interfaces between society, the economy, the natural sciences, and machine learning.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n6 REPRODUCIBILITY STATEMENT\n\nAll data used in this study is freely accessible. The code to reproduce that data as well as the figures can be found in the following anonymised repository: https:// anonymous.4open.science/r/anon aichem-83F8\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRachel Adams. Can artificial intelligence be decolonized? Interdisciplinary Science Reviews, 46\n\n(1-2):176–197, 2021. ISSN 0308-0188. doi: 10.1080/03080188.2020.1840225.\n\nNur Ahmed and Muntasir Wahed. The De-democratization of AI: Deep Learning and the Compute\n\nDivide in Artificial Intelligence Research. arXiv, 2020.\n\nPrableen Bajpai. Which companies spend the most\n\nin research and development (r&d)?, Jun 2021. URL https://www.nasdaq.com/articles/which-companies-spendthe-most-in-research-and-development-rd-2021-06-21.\n\nJohn Ballard. Nvidia’s usd 24 billion advantage over advanced micro devices, May 2021. URL https://www.fool.com/investing/2021/05/20/nvidias-24-billionadvantage-over-amd/.\n\nAbeba Birhane. Algorithmic Colonization of Africa. SCRIPT-ed, 17(2):389–409, 2020. doi:\n\n10.2966/scrip.170220.389.\n\nAnna Bon, Francis Dittoh, Gossa Lˆo, M ́onica Pini, Robert Bwana, Cheah WaiShiang, Narayanan Kulathuramaiyer, and Andr ́e Baart. Perspectives on Digital Humanism. Springer Cham, 2021. doi: 10.1007/978-3-030-86144-5\\ 9.\n\nLeo Anthony Celi, Jacqueline Cellini, Marie-Laure Charpignon, Edward Christopher Dee, Franck Dernoncourt, Rene Eber, William Greig Mitchell, Lama Moukheiber, Julian Schirmer, Julia Situ, Joseph Paguio, Joel Park, Judy Gichoya Wawira, and Seth Yao. Sources of bias in artificial intelligence that perpetuate healthcare disparities—A global review. PLOS Digital Health, 1(3): e0000022, 2022. doi: 10.1371/journal.pdig.0000022.\n\nDan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, and Juergen Schmidhuber. Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition. arXiv, 2010. doi: 10.1162/neco\\ a\\ 00052.\n\nWiki Contributors. Fixed function pipeline, Apr 2015. URL https://www.khronos.org/\n\nopengl/wiki/Fixed Function Pipeline.\n\nBec Crew. Google scholar reveals its most influential papers for 2020, Jul 2020. URL https: //www.nature.com/nature-index/news-blog/google-scholar-revealsmost-influential-papers-research-citations-twenty-twenty.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\n\nBidirectional Transformers for Language Understanding. arXiv, 2018.\n\nRaquel Dias and Ali Torkamani. Artificial intelligence in clinical and genomic diagnostics. Genome\n\nMedicine, 11(1):70, 2019. doi: 10.1186/s13073-019-0689-8.\n\nPawel Drozdowski, Christian Rathgeb, Antitza Dantcheva, Naser Damer, and Christoph Busch. Demographic Bias in Biometrics: A Survey on an Emerging Challenge. IEEE Transactions on Technology and Society, 1(2):89–103, 2020. ISSN 2637-6415. doi: 10.1109/tts.2020.2992344.\n\nJ ̈urgen Gerhards, Silke Hans, and Daniel Drewski. Global inequality in the academic system: effects of national and university symbolic capital on international academic mobility. Higher Education, 76(4):669–685, 2018. ISSN 0018-1560. doi: 10.1007/s10734-018-0231-8.\n\nTodd M Gierahn, Marc H Wadsworth, Travis K Hughes, Bryan D Bryson, Andrew Butler, Rahul Satija, Sarah Fortune, J Christopher Love, and Alex K Shalek. Seq-Well: portable, low-cost RNA sequencing of single cells at high throughput. Nature Methods, 14(4):395–398, 2017. ISSN 1548-7091. doi: 10.1038/nmeth.4179.\n\nMichael Gofman and Zhao Jin. Artificial Intelligence, Human Capital, and Innovation. SSRN\n\nElectronic Journal, 2019. doi: 10.2139/ssrn.3449440.\n\nAlexis Guernet and Luca Grumolato. CRISPR/Cas9 editing of the genome for cancer modeling.\n\nMethods, 121:130–137, 2017. ISSN 1046-2023. doi: 10.1016/j.ymeth.2017.03.007.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nRajesh Gupta, Yan Liu, Mohak Shah, Suju Rajan, Jiliang Tang, B Aditya Prakash, Muhammad Aurangzeb Ahmad, Arpit Patel, Carly Eckert, Vikas Kumar, and Ankur Teredesai. Fairness in Machine Learning for Healthcare. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3529–3530, 2020. doi: 10.1145/3394486.3406461.\n\nThilo Hagendorff and Kristof Meding. Ethical Considerations and Statistical Analysis of Industry Involvement in Machine Learning Research. arXiv, 2020. doi: 10.1007/s00146-021-01284-z.\n\nCaner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacqueline Pan, Albert Gordo, and Cristian Canton Ferrer. Towards Measuring Fairness in AI: The Casual Conversations Dataset. IEEE Transactions on Biometrics, Behavior, and Identity Science, 4(3):324–332, 2022. ISSN 2637-6407. doi: 10.1109/tbiom.2021.3132237.\n\nMireille Hildebrandt, Carlos Castillo, Elisa Celis, Salvatore Ruggieri, Linnet Taylor, Gabriela Zanfir-Fortuna, Vidushi Marda, and Shivangi Narayan. Data in New Delhi’s predictive policing system. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 317–324, 2020. doi: 10.1145/3351095.3372865.\n\nA. G. Ivakhnenko. Polynomial Theory of Complex Systems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-1(4):364–378, 1971. ISSN 0018-9472. doi: 10.1109/tsmc.1971.4308320.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino RomeraParedes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583–589, 2021. ISSN 0028-0836. doi: 10.1038/s41586-021-03819-2.\n\nRoman Jurowetzki, Daniel Hain, Juan Mateos-Garcia, and Konstantinos Stathoulopoulos. The Privatization of AI Research(-ers): Causes and Potential Consequences – From university-industry interaction to public research brain-drain? arXiv, 2021.\n\nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training\n\nGenerative Adversarial Networks with Limited Data. arXiv, 2020.\n\nDiederik P Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv, 2014.\n\nAnastasiya Kiseleva and Paul Quinn. Are You AI’s Favorite? EU Legal Implications of Biased AI Systems in Clinical Genetics and Genomics. SSRN Electronic Journal, 2021. doi: 10.2139/ ssrn.4039764.\n\nJoel Klinger, Juan C Mateos-Garcia, and Konstantinos Stathoulopoulos. A Narrowing of AI Re-\n\nsearch? SSRN Electronic Journal, 2020. doi: 10.2139/ssrn.3698698.\n\nJens Kr ̈uger and R ̈udiger Westermann. Linear algebra operators for GPU implementation of numerical algorithms. ACM Transactions on Graphics (TOG), 22(3):908–916, 2003. ISSN 0730-0301. doi: 10.1145/882262.882363.\n\nHeidi Ledford. CRISPR, the disruptor. Nature, 522(7554):20–24, 2015. ISSN 0028-0836. doi:\n\n10.1038/522020a.\n\nJohn D McPherson. A defining decade in DNA sequencing. Nature Methods, 11(10):1003–1005,\n\n2014. ISSN 1548-7091. doi: 10.1038/nmeth.3106.\n\nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A Survey on Bias and Fairness in Machine Learning. ACM Computing Surveys, 54(6):1–35, 2021. ISSN 0360-0300. doi: 10.1145/3457607.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015. ISSN 0028-0836. doi: 10.1038/nature14236.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nShakir Mohamed, Marie-Therese Png, and William Isaac. Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence. Philosophy & Technology, 33(4):659–684, 2020. ISSN 2210-5433. doi: 10.1007/s13347-020-00405-8.\n\nStephen Nellis and Jane Lanhee Lee. U.S. officials order Nvidia to halt sales of top AI chips to China, 9 2022. URL https://www.reuters.com/technology/nvidia-says-ushas-imposed-new-license-requirement-future-exports-china-202208-31/.\n\nMathias Wullum Nielsen and Jens Peter Andersen. Global citation inequality is on the rise. Proceedings of the National Academy of Sciences, 118(7):e2012208118, 2021. ISSN 0027-8424. doi: 10.1073/pnas.2012208118.\n\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias in an algorithm used to manage the health of populations. Science, 366(6464):447–453, 2019. ISSN 0036-8075. doi: 10.1126/science.aax2342.\n\nJason Priem, Heather Piwowar, and Richard Orr. OpenAlex: A fully-open index of scholarly works,\n\nauthors, venues, institutions, and concepts. arXiv, 2022. doi: 10.48550/arxiv.2205.01833.\n\nRajat Raina, Anand Madhavan, and Andrew Y. Ng. Large-scale deep unsupervised learning using graphics processors. Proceedings of the 26th Annual International Conference on Machine Learning - ICML ’09, pp. 873–880, 2009. doi: 10.1145/1553374.1553486.\n\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing Mixture-ofExperts Inference and Training to Power Next-Generation AI Scale. arXiv, 2022.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\n\nand Ilya Sutskever. Zero-Shot Text-to-Image Generation. arXiv, 2021.\n\nMarc’Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann Cun. Efficient learning of sparse representations with an energy-based model. Advances in neural information processing systems, 19, 2006.\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards Real-Time Object\n\nDetection with Region Proposal Networks. arXiv, 2015.\n\nReva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, Andrew Burt, and Patrick Hall. Towards a standard for identifying and managing bias in artificial intelligence. Technical report, National Institute of Standards and Technology, March 2022. URL https://doi.org/ 10.6028/nist.sp.1270.\n\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. Communications of the\n\nACM, 63(12):54–63, 2020. ISSN 0001-0782. doi: 10.1145/3381831.\n\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016. ISSN 0028-0836. doi: 10.1038/nature16961.\n\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. arXiv, 2022.\n\nAmrita Srivathsan, Emily Hartop, Jayanthi Puniamoorthy, Wan Ting Lee, Sujatha Narayanan Kutty, Olavi Kurina, and Rudolf Meier. Rapid, large-scale species discovery in hyperdiverse taxa using 1D MinION sequencing. BMC Biology, 17(1):96, 2019. doi: 10.1186/s12915-019-0706-9.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nChris J. Thompson, Sahngyun Hahn, and Mark Oskin. Using modern graphics architectures for general-purpose computing: A framework and analysis. In 35th Annual IEEE/ACM International Symposium on Microarchitecture, 2002. (MICRO-35). Proceedings, pp. 306–317, 2002.\n\nNeil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. Deep Learning’s ISSN 0018-9235. doi: 10.1109/\n\nIEEE Spectrum, 58(10):50–55, 2021.\n\nDiminishing Returns. mspec.2021.9563954.\n\nUis.\n\nUnesco institute for statistics, 2022.\n\nURL http://data.uis.unesco.org/\n\nIndex.aspx?DataSetCode=SCN DS&amp;lang=en.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\nLukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. arXiv, 2017.\n\nWayne P. Wahls. High cost of bias: Diminishing marginal returns on NIH grant funding to institu-\n\ntions. bioRxiv, pp. 367847, 2018. doi: 10.1101/367847.\n\nTing-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. Few-\n\nshot Video-to-Video Synthesis. arXiv, 2019.\n\nKris Wetterstrand.\n\nThe cost of\n\nsequencing a human genome, Nov 2021.\n\nURL\n\nhttps://www.genome.gov/about-genomics/fact-sheets/SequencingHuman-Genome-cost.\n\nMatej ˇSpeˇtko, Ondˇrej Vysock ́y, Branislav Jans ́ık, and Lubom ́ır ˇR ́ıha. DGX-A100 Face to Face DGX-2—Performance, Power and Thermal Behavior Evaluation. Energies, 14(2):376, 2021. doi: 10.3390/en14020376.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 OPENALEX QUERIES\n\nThe OpenAlex database was queried for works of type journal-article and proceedings-article and filtered on machine learning concepts and biology or chemistry concepts.\n\nThe query for machine learning works in biology was https://api.openalex.org/ works?per-page=200&cursor=*&filter=concepts.id:C154945302| C119857082|C108583219|C50644808|C8880873|C159149176|C126980161| C58328972|C97541855|C12267149|C179717631|C81363708|C147168706| C46686674|C169258074,concepts.id:C74187038|C64903051|C32909587| C177801218|C99726746|C63222358|C103697762|C192071366|C164126121| C69366308|C185592680|C147597530|C59593255|C161790260|C178790620| C98274493,type:journal-article|proceedings-article.\n\nThe query for machine learning works in chemistry was https://api.openalex.org/ works?per-page=200&cursor=*&filter=concepts.id:C154945302| C119857082|C108583219|C50644808|C8880873|C159149176|C126980161| C58328972|C97541855|C12267149|C179717631|C81363708|C147168706| C46686674|C169258074,concepts.id:C86803240|C60644358|C54355233| C70721500|C140556311|C153911025|C78458016|C95444343|C89423630| C203014093|C21565614|C46111723|C157585117|C191120209|C55493867| C204328495,type:journal-article|proceedings-article.\n\nThe query for all biology works was: page=200&cursor=*&filter=concepts.id:C74187038|C64903051| C32909587|C177801218|C99726746|C63222358|C103697762|C192071366| C164126121|C69366308|C185592680|C147597530|C59593255|C161790260| C178790620|C98274493,publication year:<year>,type:journal-article| proceedings-article&group by=authorships.institutions.country code\n\nhttps://api.openalex.org/works?per-\n\nThe query for all chemistry works was: https://api.openalex.org/works?perpage=200&cursor=*&filter=concepts.id:C86803240|C60644358| C54355233|C70721500|C140556311|C153911025|C78458016|C95444343| C89423630|C203014093|C21565614|C46111723|C157585117|C191120209| C55493867|C204328495,publication year:<year>,type:journal-article| proceedings-article&group by=authorships.institutions.country code\n\n• Machine learning concepts\n\n– Artificial Intelligence: C154945302 – Machine Learning: C119857082 – Deep Learning: C108583219 – Artificial Neural Network: C50644808 – Genetic Algorithm: C8880873 – Evolutionary Algorithm: C159149176 – Simulated Annealing: C126980161 – Expert System: C58328972 – Reinforcement Learning: C97541855 – Support Vector Machine: C12267149 – Multilayer Perceptron: C179717631 – Convolutional Neural Network: C81363708 – Recurrent Neural Network: C147168706 – Boosting: C46686674 – Random Forest: C169258074\n\n• Biology concepts\n\n– Biology: C86803240\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n– Bioinformatics: C60644358 – Genetics: C54355233 – Computational Biology: C70721500 – Biostatistics: C140556311 – Molecular Biology: C153911025 – Evolutionary Biology: C78458016 – Cell Biology: C95444343 – Microbiology: C89423630 – Immunology: C203014093 – Metabolomics: C21565614 – Proteomics: C46111723 – Omics: C157585117 – Structural Biology: C191120209 – Biochemistry: C55493867 – Protein Folding: C204328495\n\n• Chemistry concepts\n\n– Drug Discovery: C74187038 – Drug Design: C64903051 – Molecule: C32909587 – Chemical Reaction: C177801218 – Chemical Space: C99726746 – chEMBL: C63222358 – Virtual screening: C103697762 – Drug Design: C192071366 – QSAR: C164126121 – ADME: C69366308 – Chemistry: C185592680 – Computational Chemistry: C147597530 – Molecular Dynamics: C59593255 – Catalysis: C161790260 – Organic Chemistry: C178790620 – Pharmacology: C98274493\n\n16",
    "reference": "# Summary Of The Paper\n\nThe authors go over trends in the research and use of machine learning\nand in particular deep learning in the applied sciences, via analysis\nof the open-access literature (bibliometrics and/or full-text analyses),\nand highlight areas for concern. The findings are somewhat similar to\nprior results of the analyses of recent trends in machine learning R&D \nand its related fields.\n\n# Strength And Weaknesses\n\nStrengths:\n\n-- A variety of useful insights and takeaways into how  ml (in particular large-scale deep ml) is impacting other\n   scientific fields, in particular biology and chemistry, and several\n   trends are estimated from analysis of published research  (societal/environmental/energy impact).  \n\nWeaknesses:\n\n-- No or very little technical contributions.  The ICLR venue may not to be\n   relevant for this report.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity is good, although the introduction abruptly dives into the\nhistory of hardware development.  The paper uses existing techniques\nbut the novelty mainly lies in applying the analyses to research in \nbiology and chemistry.\n\n# Summary Of The Review\n\nThe paper reports on trends in ml and how it has been developing in\nthat past decade, specially in biology and chemistry.  Several of\nthese trends are certainly concerning (the de-democratizatin of ml\nwork).\n\nHowever, I don't think that ICLR is the venue for this publication,\njust as literature reviews are highly useful, but not relevant in this\nconference. The paper does not have a technical contribution.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nA DIFFERENTIABLE LOSS FUNCTION FOR LEARNING HEURISTICS IN A*\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nOptimization of heuristic functions for the A* algorithm, realized by deep neural networks, is usually done by minimizing square root loss of estimate of the cost to goal values. This paper argues that this does not necessarily lead to a faster search of A* algorithm since its execution relies on relative values instead of absolute ones. As a mitigation, we propose the L∗ loss, which upper-bounds the number of excessively expanded states inside the A* search. The L∗ loss, when used in the optimization of state-of-the-art deep neural networks for automated planning in maze domains like Sokoban and maze with teleports, significantly improves the fraction of solved problems, the quality of founded plans, and reduces the number of expanded states to approximately 50%.\n\n1\n\nINTRODUCTION\n\nAutomated planning aims to find a sequence of actions that will reach a goal in a model of the environment provided by the user. Planning is considered to be one of the core problems in Artificial intelligence and it is behind some of its successful applications Samuel (1967); Knuth & Moore (1975); Silver et al. (2017). Early analysis of planning tasks McDermott (1996) indicated that optimising the heuristic function steering the search for a given problem domain can dramatically improve the performance of the search.\n\nLearning in planning means optimizing heuristic functions from plans of already solved problems and their instances. This definition includes selection of proper heuristics in a set of pattern databases Franco et al. (2017); Haslum et al. (2007); Moraru et al. (2019); Edelkamp (2006), a selection of a planner from a portfolio Katz et al. (2018), learning planning operators from instances M ́enager et al. (2018); Wang (1994), and learning for macro-operators and entanglements Chrpa (2010); Korf (1985). Recent years observe a renewed interest in learning heuristic functions and this is fuelled by the success of deep learning and reinforcement learning in the same area Shen et al. (2020); Groshev et al. (2018); Ferber et al. (2020); Bhardwaj et al. (2017).\n\nIn this work, we are interested in optimising the heuristic function for A* Hart et al. (1968), which despite the popularity of Monte Carlo tree search Coulom (2006); Silver et al. (2017) is interesting due to its guarantees on optimal solution. A* is also optimally efficient in the sense that it expands the minimal number of states. Majority of prior art Shen et al. (2020); Toyer et al. (2020); Groshev et al. (2018); Ferber et al. (2020); Bhardwaj et al. (2017) optimises the heuristic function by minimizing the error of the predicted cost to the goal on a training set of problem instances,1 where the error is measured by the L2 error function or its variant. L2 = 0 does not guarantee the optimal efficiency of A* , hence it gives a false sense of security.\n\nWe propose a L∗ loss function tailored for A* , which minimizes an upper bound on the number of expanded states. This is achieved by stimulating states on an optimal path to have a smaller cost function f = g + h than those off the optimal path. By this, L∗ effectively utilizes all the states generated during the exploration of A* , providing much more information to the learner. If L∗ on a given problem instance is equal to zero, it is guaranteed that A* will expand only states on the optimal path, which under conditions on the training set as detailed below, implies optimal efficiency of A* . We emphasize that the optimal efficiency is retained even on problems with\n\n1The training set contains solved problem instances, where the solution should be ideally found by a search\n\nfinding optimal solution, such as A* with ideally admissible heuristic function.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nexponentially many optimal paths Helmert & R ̈oger (2008), therefore the heuristic function has to learn a tie-breaking mechanism.\n\nThe proposed L∗ is compared to state of the art on seven domains: Sokoban, Maze with teleports, Sliding tile puzzle, Blockworld, Ferry, Grippe, and N-Puzzle and on all of them it consistently outperforms heuristic functions optimizing L2.\n\n2 PRELIMINARIES\n\nWe define a search problem instance by a directed weighted graph Γ = ⟨S, E, w⟩, a distinct node s0 ∈ S and a distinct set of nodes S ∗ ⊆ S. The nodes S denote all possible states s ∈ S of the underlying transition system representing the graph. The set of edges E contains all possible transitions e ∈ E between the states in the form e = (s, s′). s0 ∈ S is the initial state of the problem instance and S ∗ ⊆ S is a set of allowed goal states. Problem instance graph weights (alias action costs) are mappings w : E → R≥\n\n0.\n\nLet π = (e1, e2, . . . , el), we call π a path (alias a plan) of length l solving a task Γ with s0 and S ∗ 1, sl)) and sl ∈ S ∗. An optimal path is defined as a minimal iff π = ((s0, s1), (s1, s2), . . . , (sl cost of a problem instance Γ, s0, S ∗ and is denoted as π∗ together with its value f ∗ = w(π∗) = w(e1)+w(e2)+. . . , +w(el). We often minimize the cost of solution of a problem instance Γ, s0, S ∗, namely π∗, together with its length l∗ = |π∗|.\n\n−\n\n2.1 A* ALGORITHM\n\nLet’s briefly recall how the A* algorithm works. For consistent heuristics, where h(s) − h(s′) ≤ w(s, s′) for all edges (s, s′) in the w-weighted state space graph, it mimics the working of Dijkstra’s shortest-path algorithm Dijkstra (1959) and maintains the set of generated but not expanded nodes in O (the Open list) and the set of already expanded nodes in C (the Closed list). It works as follows.\n\n1. Add the start node s0 to the Open list O0. 2. Set g(s0) = 0 3. Initiate the Closed list to empty, i.e. C0 = ∅. 4. For i ∈ 1, . . . until Oi ̸= ∅\n\ni−1 g(s) + h(s)\n\n(a) Select the state si = arg mins (b) Remove si from Oi (c) If si ∈ S ∗, i.e. it is a goal state, go to 5. (d) Insert the state si to Ci −\n(e) Expand the state si into states s′ for which hold (si, s′) ∈ E and for each\n\n∈O 1, Oi = Oi\n\n1, Ci = Ci\n\n1 ∪ {si}\n\n1 \\ {si}\n\n−\n\n−\n\n−\n\ni. set g(s′) = g(si) + w(si, s′) ii. if s′ is in the Closed list as sc and g(s′) < g(sc) then sc is reopened (i.e., moved\n\nfrom the Closed to the Open list), else continue with (e)\n\niii. if s′ is in the Open list as so and g(s′) < g(so) then so is updated (i.e., removed from the Open list and re-added in next step with updated g(·)), else continue with (e)\n\niv. add s′ into the Open list\n\n5. Walk back to retrieve the optimal path.\n\nIn the above algorithm, g(s) denotes a function assigning an accumulated cost w for moving from the initial state (s0) to a given state s. Consistent heuristics are called monotone because the estimated cost of a partial solution f (s) = g(s) + h(s) is monotonically non-decreasing along the best path to the goal. More than this, f is monotone on all edges (s, s′), if and only if h is consistent as we have f (s′) = g(s′) + h(s′) ≥ g(s) + w(s, s′) + h(s) − w(s, s′) = f (s) and h(s) − h(s′) = f (s) − g(s) − (f (s′) − g(s′)) = f (s) − f (s′) + w(s, s′) ≤ w(s, s′). For the case of consistent heuristics, no reopening (moving back nodes from Closed to Open) is needed, as we essentially traverse a state-space graph with edge weights w(s, s′) + h(s′) − h(s) ≥ 0. For the trivial heuristic h0, we have h0(s) = 0 and for perfect heuristic h∗, we have f (s) = f ∗ = g(s) + h∗(s) for all nodes s. Both heuristics h0 and h∗ are consistent.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 1: A visualization of a search space of an A* algorithm. In sub-figure (a), path s0 → s1 → s2 represents the optimal plan, states {s3, s4, s5}/s6 are off the optimal path but have / have not been generated by the A* . In sub-figure (b), path s0 → s1 → s2 → s4 and s0 → s1 → s3 → s4 represents the optimal plan, states s5/s6 are off the optimal path but have / have not been generated.\n\nEven if the heuristic is not consistent, algorithms like A* even without the reopening, remain complete i.e. they find a plan if there is one. Plans might not be provably optimal but are often very good in planning practice.\n\n2.2 OPTIMIZING THE HEURISTIC\n\nWe consider heuristic function hθ : S → R≥ 0 mapping a state s ∈ S to a real non-negative value, where θ ∈ Rm holds parameters of hθ. Using a set of problem instances T (further called training set), we want to optimize parameters θ of hθ such that an A* search algorithm would find an (optimal) solution by expanding the least number of states. 2 This, in practice, means to solve the optimization problem\n\narg min\n\nθ\n\n(cid:88)\n\nL(hθ, S),\n\n(1)\n\nS∈T where the loss function L should be such that smaller values imply better heuristic function hθ as perceived by A* .\n\n2.3 WEAKNESS OF L2 LOSS FUNCTION\n\ni (hθ(si) − yi)2 , where the training set S consists of pairs {(si, yi)}n\n\nMany prior art on optimizing heuristic function Shen et al. (2020); Groshev et al. (2018); Ferber et al. (2020); Bhardwaj et al. (2017); Toyer et al. (2020) minimize the L2 loss function3 L2(hθ, S) = (cid:80) i=1 , where si is some state and yi is the length of the plan from si to the goal state. We argue that zero L2 loss on a given problem instance for states on the optimal path does not guarantee that A* will be optimally efficient in the sense that it can expand more states than needed.\n\nL2 does not utilize states off the optimal path. Imagine a problem instance shown in Figure 1a, where s0 and s2 are the initial and goal states respectively and (s0, s1, s2) is the optimal path. When L2 loss is optimized, one needs to know the exact cost-to-go values which are obtained by solving the problem instance. Thus, by solving the instance in Figure 1, one obtains heuristic values for {s0, s1, s2}. But if L2 loss is equal to zero on them, it does not say anything about estimates for states off the optimal path states {s3, s4, s5, s6}. This means that it can happen that f (s3) < f (s1), which would lead to expanding the state s3 in A* algorithm and hence to sub-optimality.\n\nThis issue can be fixed, if the training set is extended to contain heuristic values for all states off the optimal path ({s3, s4, . . . , s6} in our example), which in practice requires solving all possible variants of the problem instances. This has been suggested in Bhardwaj et al. (2017) but is infeasible due to excessive computational requirements. Therefore, in practice, it is assumed the training set to be large, thereby mitigating this problem.\n\n2Here it is assumed that the number of expanded states is directly proportional to the time taken to find the\n\nsolution, as the time to compute the value of hθ is independent of the value of θ.\n\n3While some works use L1, the properties discussed here for L2 holds for L1 as well.\n\n3\n\ns3s5s4s6s0s1s2s5s6s0s3s4s1s2Under review as a conference paper at ICLR 2023\n\nL2 loss provides a false sense of optimality. Some problems can have large (even exponential) number of optimal solutions with the same cost Helmert & R ̈oger (2008). In this case, minimizing estimate of cost-to-go of all states in the problem instance (to fix the problem mentioned above) does not guarantee that A* will be optimally efficient. Consider an example in Figure 1b with unit cost on edges. The algorithm starts by expanding state s0 to s1 and s2. The heuristic is perfectly estimated and f (s1) = f (s2) = 3. Hence there are two states with the same value f which means A* has to decide, how to the break this tie. The situation repeats after A* expands either s1 or s3, since the open set will now contain state s3 with f (s3) = 3 and A* needs to resolve ties again. See Helmert & R ̈oger (2008) for more examples.\n\nHeuristic value for unreachable (dead-end) states should be infinite to ensure that they are never selected. An infinity in the L2 loss would always lead to an infinite loss which would then result in an infinite gradient. Hence, in practice, a sufficiently large value for dead-end states has to be used.\n\n3 L∗ LOSS\n\nWe explain the proposed L∗loss function on a single problem instance Γ = ⟨S, E, w⟩ (the extension to a set of plan is trivial through Equation equation 1). We assume to have a (preferably optimal and shortest) plan π = ((s0, s1), (s1, s2), . . . , (sn 1, sn)) with states from this optimal plan denoted as S o = {s0, s1, s2, . . . , sn}. This plan can be found by A* with some (admissible) heuristic function h, which does not have to coincide with the heuristic function hθ that we are optimizing. We denote states off the optimal plan as S n ⊂ S \\ S o, where the subset exists because, in practice, S n contains states generated by A* while solving the problem instance Γ. In the visualization in Figure 1a, grey states are on the optimal path S o, pink states are off the optimal path, and yellow states were not generated in the course of solving the problem instance. Hence, S = {si}6 i=1 and S n = {si}5\n\ni=3. The training sample for a L∗is defined as a tuple ̄Γ = ⟨S, E, w⟩, S o.\n\ni=1, S o = {si}2\n\n−\n\nL∗ aims to minimize the number of expanded states in the A* algorithm. Recall that A* always expands a state from an open list with smallest fθ(s) = g(s) + hθ(s). To be optimally efficient, states on the optimal path s′ ∈ S o should have always smaller f (s′) than states off the optimal path s′′ ∈ S n, i.e.\n\n(∀s′ ∈ S o)(∀s′′ ∈ S n)(g(s′) + hθ(s′) < g(s′′) + hθ(s′′))\n\nOn the optimal path, we might also impose monotonicity as\n\n(∀si, sj ∈ S o)(i < j)(g(si) + hθ(si) ≤ g(sj) + hθ(sj),\n\n(2)\n\n(3)\n\nthough it does not affect the optimality of A* . We do this, since monotonic heuristic function implies A* returning optimal solution. In Constraint equation 2, states not generated by A* are ignored. But S n will always contain all states of distance one from the optimal path, which is sufficient to show that a loss equalling zero implies expanding states only on an optimal path (in the training set). To prevent confusion, we emphasize that conditions are designed for the heuristic hθ that is to be optimized, and not for the heuristic h that has generated the training set in the first place.\n\nWhile Constraint equation 3 is true for every consistent heuristic, Constraint equation 2 is true only for perfect heuristics. Otherwise, we could have some earlier states in the exploration off the optimal path that have a smaller f -value than later ones in the optimal path. What seems to be over-restrictive, such that almost no heuristic function will ever fulfill, Constraint equation 2, is intentional.\n\nThe proposed L∗ loss minimizes the number of times each of the above constraints are violated as\n\ng(s′) + hθ(s′) ≥ g(s′′) + hθ(s′′)\n\n+\n\n(cid:75)\n\nn(cid:74)\n\n1 |S o||S n|\n\n(cid:88)\n\n(cid:88)\n\ns′\n\n∈S\n\no\n\ns′′\n\n∈S o\n\n1 |S o|(|S o| − 1)\n\n|(cid:88) |S\n\ni=2\n\ni (cid:88)\n\nj=1(cid:74)\n\ng(si) + hθ(si) > g(sj) + hθ(sj)\n\n, (cid:75)\n\n(4)\n\n·\n\n(cid:74)\n\n(cid:75)\n\nwhere is an Iverson bracket, which is equal to one if the argument is true and zero otherwise. The first part of the loss function loosely upper bounds the number of non-optimal states the A* expands while the second part ensures the monotonicity of the heuristic function along the optimal plan. In other words, the conditions equation 2 and equation 3 encode the aim of a consistent and\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nperfect heuristic. During training, we iterate over many samples of A* explorations which enlarges the scope of L∗.\n\nTo set constraints for heuristic learning, we only need the partitioning of the set of explored nodes into the sets S0 and Sn, computed via an optimal plan and a set of all generated nodes, together with their g-values. Given the optimal heuristic, A* will always find an optimal solution. Up to tie-breaking, it is optimally efficient and will expand only nodes with optimal merit f ∗.\n\nLoss function L∗ does not distinguish between the Open and Closed lists in the exploration of A* as long as it has access to the combined set of explored nodes. This way, we can take any optimal planner and not just the heuristic search planners for training.\n\n3.1 HOW L∗ ADDRESSES THE DEFICIENCIES OF L2\n\nL∗ utilizes all states generated during the A* search used to create the training sample(s), which is in sharp contrast to L2 estimating cost-to-go. This propagates to better utilization of states in the training set. The experimental results show that given a fixed and small number of training problems, models minimizing L∗ achieves higher performance.\n\nL∗ = 0 implies optimality. We state a following theorem:\n\nTheorem 1 (Upper Bound) For a problem instance with states S = S n ∪ S o, denote\n\nRn = (cid:8)s′′ ∈ S n | ∃s′ ∈ S 0 ∧ (g(s′) + hθ(s′) ≥ g(s′′) + hθ(s′′))(cid:9) ,\n\n(5)\n\nthe quantity |Rn| is an upper bound on the number of non-optimal states A* expands during its search.\n\nThe proof is straightforward and it is included in supplementary for completeness. The quantity |Rn| is exactly the quantity minimized by the L∗ as defined in Equation equation 4. The following theorem is a trivial consequence of this property.\n\nTheorem 2 (Optimal efficiency) Let for a given training sample ̄Γ, and a heuristic function hθ L∗( ̄Γ, hθ) = 0. Then A* with heuristic function hθ will expand only states on the optimal path S o. If S o in ̄Γ is optimal and shortest, A* will be optimally efficient.\n\nThe proof is a consequence of the property that L∗( ̄Γ, hθ) = 0 implies that |Rn| = 0. The above theorem holds even on problems with multiple optimal solutions. In this case, L∗ would be either equal to zero, which means hθ includes tie-breaking mechanism and it will be optimal, or it will be greater than zero. Thus and unlike L2, its zero value implies optimal efficiency.\n\nL∗does not require heuristic value of unreachable (dead-end) states, which is caused by the fact that L∗requires satisfaction of inequalities instead of estimation of some value. L∗ loss does not force the heuristic to be goal aware, since as discussed in Supplementary, this is not needed for the optimal efficiency of A* .\n\n4 RELATED WORK\n\nIn potential heuristics Seipp et al. (2015), parameters of the heuristic functions are optimized by linear programming for a particular problem instance to satisfy constraints similar to those stated in this paper. The optimization assumes a particular structure of the heuristic, unlike here, where no structure is assumed. Ref. Takahashi et al. (2019) admits that the symmetry of L2 (and of L1) loss does not promote admissibility of the heuristic. It recommends asymmetric L1 with different weights on the left and right parts, but this does not completely solve the problems identified above in Section 2.3.\n\nIn Ferber et al. (2020), neural networks estimate the number of expansions of a GBFS search, though the results are comparable to an estimation of cost-to-goal. In Bhardwaj et al. (2017) A* is viewed as a Markov Decision Process with value function being equal to the number of steps of A* till it reaches the solution. While this detaches the heuristic values from cost-to-goal cost, it does not solve the problem with dead-ends, state efficiency, and ties.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nRefs. Vlastelica et al. (2021); Yonetani et al. (2021) combine neural networks with discrete search algorithms, which become an inseparable part of the architecture. Orseau & Lelis (2021) proposes a new search algorithm that uses both a heuristic function and policy networks to minimize the search loss. Our setting is more classical where the heuristic is optimized for A* search but the execution of the search is independent of the training. This has the advantage that one (costly) execution of A* search algorithm is used many times during training to optimize weights.\n\nA large corpus of literature Silver et al. (2017); Guez et al. (2018); Feng et al. (2020); Anthony et al. (2017) is devoted to improvements to Monte Carlo Tree Search. Since this work is concerned exclusively to A* algorithm, we view these works independent to this. Nevertheless, we compare to some of them in the experimental section. Similarly, a lot of works Shen et al. (2020); Toyer et al. (2020); Zhang & Geißer (2021); Groshev et al. (2018); Chrestien et al. (2021); Ferber et al. (2020); Bhardwaj et al. (2017) investigate architectures of neural networks for learning a heuristic function, ideally for arbitrary planning problems. These works are perpendicular to this one, which investigate how to optimize these neural networks to perform well inside A* .\n\n5 EXPERIMENTAL EVALUATION\n\nHeuristic functions optimized with respect to L∗ loss function are compared to those optimized with respect to L2 loss on seven domains. Due to lack of space, only Sokoban and Mazes with teleports are detailed below, and the remaining five are in supplementary material. This is supplemented by the comparison with domain-independent planners: (1) SymBA* Torralba et al. (2014), a cost-optimal planner from International Planning Competition (IPC) 2014; (2) Delfi Katz et al. (2018); (3) Mercury14 Katz & Hoffmann (2014), a satisfycing planner from IPC 2014; (4) Stone soupe Seipp & R ̈oger (2018), and by solutions based on Monte Carlo Tree Search Guez et al. (2018) and reinforcement learning Racani`ere et al. (2017); Guez et al. (2019). Ref. Toyer et al. (2020) admit it does not work on Sokoban and Shen et al. (2020) works only on small Sokoban problems with two boxes, we do not compare to these works. All experiments involving neural networks have been repeated three times.\n\nThe Neural Network Neural networks (NN) implementing heuristic functions were adopted from Groshev et al. (2018) and Chrestien et al. (2021), where the latter is, to our best knowledge the state-of-the-art architecture for maze domains. It contains seven convolution layers P1, . . . P7 followed by four convolution-attention-position blocks, which allow correlating information from distant parts of the maze. The output tensor of the fourth CoAt block is ”flattened” by global average pooling over the x and y dimension to a vector, which is then fed to a fully connected layer (FC) which outputs a scalar estimating the heuristic value. More details on the network can be found Chrestien et al. (2021). This network is further called ”CoAt” after the CoAt blocks. We also studied the network of Groshev et al. (2018) (without policy head), which has a similar structure, but instead of four CoAt blocks it has seven CNN layers. We refer to this network as to CNN. Both networks are by design scale-free, which means that they can be used on mazes of various sizes, as is shown below on the mazes with teleport domain. Since the L∗loss as defined in Equation 4 is not differentiable, the Iverson bracket is replaced by a logistic loss function Ll(x) = log(1 + exp(−x)).\n\nOur experiments were implemented in Keras-2.4.3 with Tensorflow-2.3.1 as the backend. For training the neural networks, we used an NVIDIA Tesla GPU model V100-SXM2-32GB; the evaluation was performed on the CPU to ensure a fair comparison to domain independent planners. The neural networks were trained by the Adam optimizer Kingma & Ba (2014) with a default learning rate of 0.001. Each mini-batch contained all states from one problem instance. Scripts reproducing our experiments together with mazes and solutions will be made available upon acceptance.\n\n5.1 TRAINING FROM SOLVED MAZES\n\nWe generate mazes for the training set by running the A* algorithm using the heuristic function from Groshev et al. (2018) on a set of problem instances to identify a set of states generated during the A* search. All these sets form the training set. Since optimizing the heuristic by L2 loss requires knowing the true heuristic value (cost to reach the goal), we have used SymBA* Torralba et al. (2014) to find the optimal plan from each state in the training set. For states for which SymBA* doesn’t find a solution (dead-end states), the h value is replaced by a very large value. This con-\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nCNN #b SBA* Delfi Merc FDSS L2 L∗ 81 87 3\n74 80 4\n5 72 82 61 71 6\n51 59 7\n- -\n8 -\n- 9\n\n100 81 67 49 31 -\n-\n\n100 100 97 55 46 -\n-\n\n100 100 91 55 44 -\n-\n\n100 100 94 56 42 -\n-\n\n(a)\n\nCoAt L2 L∗ CL w.L∗ 91 94 89 93 85 89 73 80 63 77 32 -\n12 -\n\n95 94 90 85 83 59 38\n\nmodel MCTSNet I2A DRC (3,3) 10k DRC (3,3) 900k CoAt − L∗ CL CoAt − L∗ SBA*\n\ncoverage 84 84 93 99 97 100 100\n\n(b)\n\nTable 1: Left: Fraction of solved mazes (in percents) of S(ym)BA* , Delfi(1), Merc(ury14), FDSS (Fast Downward Stone Soup), CoAt and CoAt* on test data sets containing variable number of boxes. Column captioned #b indicates the number of boxes in different categories. The standard deviation of all repeated experiments was between 0.004 and 0.008 and it is not shown to save space. Right: Fraction of solved mazes (in percents) from Boxoban dataset. DRC 900k / DRC 10k optimizes on 10k levels. Results of MCTSNet Guez et al. (2018), I2A Racani`ere et al. (2017), and DRC Guez et al. (2019) have been copied from Table 2 of Guez et al. (2019).\n\nstruction, albeit very expensive, allows a fair comparison, since the training of heuristic by L2 loss will also use states off the optimal path of the original problem instance for which the states were generated.\n\nSokoban’s training set contained 10000 Sokoban mazes of size 10 × 10 with 3 boxes created using gym-sokoban Schrader (2018). The testing set contained 2000 mazes of the same size 10 × 10 but with 3, 4, 5, 6, 7, 8, 9 boxes. The complexity increases with the addition of more boxes.4, and therefore we can evaluate the ability to generalize outside training environments. We go a step further and implement curriculum learning Bengio et al. (2009) by training from those mazes that are solved by our network during evaluation. We create a new training set containing all the solved mazes and re-train our network in an effort to improve the coverage of our network.\n\nMaze-with-teleports’s training set contained 5000 randomly generated mazes of size n × n = 15 × 15 with the agent in the upper-left corner and the goal in the lower-right corner. The mazes were generated using an open-source maze generator 5, where walls were randomly broken and 4 pairs of teleports were randomly added to the maze structure. The testing contained 2000 mazes generated by the same algorithm but (i) were bigger by up to 60 × 60 and (ii) were rotated by 90, 180, and 270 degrees which moved the start and goal states to positions not occurring in the training set.\n\n5.2 RESULTS\n\nSokoban Table 1a shows the percentage of solved mazes of all compared planners on problem instances with a various number of boxes (recall that the NNs were optimized on instances with only three boxes). All planners were given a time limit of 10 minutes to solve each Sokoban instance. On mazes with 3 and 4 boxes, the optimal planners SymBA* (SBA* ) and Delfi were able to solve all problem instances while the best performing architecture among the NNs, which is CoAt optimized with respect to L∗ (CoAt − L∗), could solve 94% and 93% of the mazes respectively. On increasing the number of boxes, the A* with NNs start outperforming classical planners. A* with NNs optimizing L∗ is consistently better than those optimizing L2. The CoAt architecture proposed in Chrestien et al. (2021) with the proposed L∗ is the only solver that can solve some mazes with 8 and 9 boxes. Solutions found by CoAt optimizing L∗were close to optimum, on average by 1 step longer, which is likely because the learnt heuristic is most of the times admissible (see Supplementary for details). The average number of expanded states in Figure 2a shows that NNs optimizing L∗indeed expand a smaller number of states than those optimizing L2.\n\nSince networks have never seen mazes with four or more boxes, extrapolation to eight and nine boxes is impressive. To evaluate the potential for self-improvement, the training set of CoAt − L∗was\n\n4This is of course just an approximation, as we can have simple problems with a large number of boxes 5https://github.com/ravenkls/Maze-Generator-and-Solver\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Sokoban\n\n(b) Maze with teleports\n\nFigure 2: The average number of expanded states in A* for Sokoban (left) and Maze problems (right).\n\nCNN n SBA* Delfi Merc FDSS L2 L∗ 50 75 55 -\n60 -\n\nCoAt L2 L∗ 100 100 100 100 100 86 88 100 76 79 100\n\n85 85 73 74\n\n90 50 -\n\n92 52 -\n\nTable 2: Fraction of solved mazes with teleports of S(ym)BA* , Delfi(1), Merc(ury14), and A* algorithm with heuristic function implemented by CNN and CoAt networks optimized with respect to L2 and L∗. All solvers have solved all mazes of size 15–40, hence they are not shown Standard deviations of repeated experiments were between 0.005 and 0.008 are shown in Supplementary.\n\nextended with mazes from the testing set it has already solved for fine-tuning. We refer to this as curriculum learning (CL w. L∗) and the results are shown in the last column of Table 1a. It shows marginal improvement on mazes with 3-5 boxes but records a significant improvement in performance over the vanilla L∗on mazes exceeding 5 boxes.\n\nBoxoban On unfiltered ”Boxoban” levels from Guez et al. (2019), A* with heuristic implemented by CoAt network and optimized with respect to the proposed L∗is compared to MCTSNet Guez et al. (2018), Imagination augmented agent (I2A) Racani`ere et al. (2017), and to DRC (3,3) network Guez et al. (2019), with possible discrepancies, as results on the competing methods were taken from Guez et al. (2019). CoAt−L∗was optimized on 10k mazes with 3 boxes (it is the same network as reported in the previous paragraph), whereas others were optimized on 900k mazes with 4 boxes. CoAt − L∗and MCTSNet knows the model, whereas DRC and I2A do not. The fraction of solved mazes shown in Table 1b shows that CoAt − L∗trained on 10k mazes is second best behind DRC (3,3) that is trained on 900k mazes, but DRC (3,3) trained on 10k mazes is already inferior. CoAt − L∗with one iteration of curriculum learning where the training set is extended to contain previously solved mazes (from set used in previous paragraph) solves 100% of boxoban mazes. Needless to say that during optimization, DRC (3,3) used 1G iterations of SGD, MCTSNets allowed 10M iterations of SGD, whereas CoAt optimized L∗allowed just for 120k iterations, which is few magnitudes less.\n\nMaze-with-teleports Fraction of solved mazes with teleports is shown in Table 2. A* with heuristic implemented by NN was optimized on mazes of size 15 × 15 and has solved all mazes up to size 40 × 40 (not shown in the table) and beyond. The results mimic the results on Sokoban in the sense that A* with CoAt networks optimizing L∗ is consistently outperforming those optimizing L2.\n\nAll 2000 mazes in the training set were created such that the agent starts in the top left corner and the goal is in the bottom right corner. When mazes are rotated by 90◦, 180◦ and 270◦, the agent has to solve mazes with distributions very different to that on the training set, yet the fraction of solved mazes for CoAt − L∗decreases by at most 5% (see Table 3 in Supplementary). Average number of generated states in A* with different heuristics is shown in Figure 2a. Again, heuristics optimized with respect to L∗expand smaller number of states during the search.\n\n8\n\n3456701,0002,0003,000NoofboxesAvgnoofexpandedstatesAvgnoofexpandedstatesvs.NoofboxesCNN-L2CNN-L∗CoAt-L2CoAt-L∗1520304050556001,0002,0003,000GridsizeAvgnoofexpandedstatesAvgnoofexpandedstatesvs.GridsizeCNN-L2CNN-L∗CoAt-L2CoAt-L∗Under review as a conference paper at ICLR 2023\n\nepoch 0\n1 2\n3 4\n5 6\n7\n\n3000 L2 L∗ 7.4 7.4 11 11 15 18 29 33 34 62 62 75 67 77 63 76\n\n4000 L2 L∗ 7.7 7.7 9.8 11 11 16 20 31 27 69 44 75 51 80 62 80\n\n5000 L2 L∗ 8.5 8.5 10 10 12 18 21 24 36 49 57 73 61 80 74 82\n\n(a) Sokoban\n\n6000 L2 L∗ 8.3 8.3 9.6 10 14 16 20 32 31 48 34 66 49 77 50 86\n\nepoch 0\n1 2\n3 4\n\n1000 L2 L∗ 25 25 42 45 45 68 69 83 84 90\n\n3000 L2 L∗ 19 19 29 35 39 51 59 67 78 83\n\n5000 L2 L∗ 15 15 18 20 34 34 41 59 66 75\n\n(b) Maze with teleports\n\nTable 3: Fraction of solved mazes (in percents) when the networks are optimized only on mazes they have previously solved. First row corresponds to A* with untrained network.\n\n5.3 TRAINING FROM UNSOLVED MAZES\n\nLet’s now consider a bootstrap protocol, where in each epoch, a heuristic function implemented by the NN is first used in A* to try to solve mazes from an available set of mazes (recall that we set a 10min time limit for solving a maze) and then to optimize its parameters on a set of mazes it has solved. Similarly to reinforcement learning, if an un-optimized (which means uninformed) heuristics solves at least a few mazes, it can boot the learning.\n\nIn this experiment, the training set of unsolved mazes is fixed. In the optimization of our NN over solved mazes, we perform one epoch. Hence the number of iteration and epoch coincide. Table 3 shows percentages of solved mazes on Sokoban and Maze with teleports for the first seven and four epochs (epoch number 0 means that the network is untrained) for different sizes of the training set. The set of Sokoban mazes contained problems with three, four, and five boxes; the set of mazes with teleports contained problems of size 40 × 40.\n\nWe observe that the fraction of solved mazes increases with epochs and the speed of this growth is significantly faster for heuristics optimized with respect to the proposed L∗. To our surprise, the fraction of solved mazes does not grow faster when the number of initially unsolved set of mazes is bigger. Yet we have observed that the fraction of solved unfiltered boxoban mazes increases as expected. A* with the network optimized on the set of 6000 mazes could solve 96% of levels of unfiltered boxoban mazes after seven epochs. This agent has performed just 20.5k gradient descend steps, which is comparatively smaller than 1G steps of DRC (3,3) agent from Guez et al. (2018). Sections 5 and 6 in Supplementary contains additional evaluation of five more domains (Sliding tile puzzle, Blockworld, Gripper, Ferry, and N-Puzzle) with the same conclusion: L∗is always better than L2.\n\n6 CONCLUSION\n\nThis work has proposed L∗ loss function for imitation learning in planning;it has been designed specifically to maximize the efficiency of the A* algorithm. L∗ is zero, if and only if the basic monotonicity requirements on the f-value in A* are satisfied, so that the heuristic trained on this loss function is consistent and perfect. This enables A* to find the optimal solution at an optimal time. It has been shown that L2 does not have these guarantees. The experiments have verified the promises that A* with heuristic functions optimized with respect to L∗ always solve a much higher number of problems, generate up to 50% lesser states than those optimized with respect to the usual L2, and return nearly optimal solutions. By comparison to MCTSNets, we have shown that A* with well trained heuristic can be competitive to Monte Carlo Tree Search. The training is also more efficient, as we use a much lesser number of SGD steps.\n\nThe proposed L∗loss well complements contemporary research, which pays a lot of attention to the network architectures, as it can be used as a drop-in replacement for L2. It inspires us to design loss functions for other types of search algorithm and research neglected aspects, such as the construction of a representative training set. We see them as a limiting factor in the further endeavour to solve more difficult problem instances.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nThomas Anthony, Zheng Tian, and David Barber.\n\nThinking fast and slow with deep learning and tree search. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5360–5370, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ d8e1344e27a5b08cdfd5d027d9b8d6de-Abstract.html.\n\nYoshua Bengio, J ́erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In\n\nICML, pp. 41–48, 2009.\n\nMohak Bhardwaj, Sanjiban Choudhury, and Sebastian Scherer. Learning heuristic search via imita-\n\ntion. In Conference on Robot Learning, pp. 271–280. PMLR, 2017.\n\nLeah Chrestien, Tom ́aˇs Pevn ́y, Anton ́ın Komenda, and Stefan Edelkamp. Heuristic search planning with deep neural networks using imitation, attention and curriculum learning. arXiv:2112.01918, 2021.\n\nLuk ́as Chrpa. Combining learning techniques for classical planning: Macro-operators and entangle-\n\nments. In ICTAI, pp. 79–86. IEEE Computer Society, 2010.\n\nR ́emi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International\n\nconference on computers and games, pp. 72–83. Springer, 2006.\n\nEdsger W. Dijkstra. A note on two problems in connexion with graphs. Numerische Mathematik, 1:\n\n269–271, 1959.\n\nStefan Edelkamp. Automated creation of pattern database search heuristics. In Model Checking and\n\nArtificial Intelligence, pp. 35–50, 2006.\n\nDieqiao Feng, Carla P. Gomes, and Bart Selman. Solving hard AI planning instances using curriculum-driven deep reinforcement learning. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pp. 2198– 2205. ijcai.org, 2020. doi: 10.24963/ijcai.2020/304. URL https://doi.org/10.24963/ ijcai.2020/304.\n\nPatrick Ferber, Malte Helmert, and J ̈org Hoffmann. Neural network heuristics for classical planning:\n\nA study of hyperparameter space. In ECAI, pp. 2346–2353. IOS Press, 2020.\n\nSantiago Franco, ́Alvaro Torralba, Levi H. S. Lelis, and Mike Barley. On creating complementary\n\npattern databases. In IJCAI, pp. 4302–4309, 2017.\n\nEdward Groshev, Aviv Tamar, Maxwell Goldstein, Siddharth Srivastava, and Pieter Abbeel. Learn-\n\ning generalized reactive policies using deep neural networks. 2018.\n\nArthur Guez, Th ́eophane Weber, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals, Daan Wierstra, R ́emi Munos, and David Silver. Learning to search with mctsnets. In International conference on machine learning, pp. 1822–1831. PMLR, 2018.\n\nArthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, S ́ebastien Racani`ere, Theophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, Greg Wayne, David Silver, and Timothy P. Lillicrap. An investigation of model-free planning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2464–2473. PMLR, 2019. URL http://proceedings.mlr. press/v97/guez19a.html.\n\nPeter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100–107, 1968.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nPatrik Haslum, Adi Botea, Malte Helmert, Blai Bonet, and Sven Koenig. Domain-independent construction of pattern database heuristics for cost-optimal planning. In AAAI, pp. 1007–1012, 2007.\n\nMalte Helmert and Gabriele R ̈oger. How good is almost perfect? In AAAI, pp. 944–949. AAAI\n\nPress, 2008.\n\nMichael Katz and Joerg Hoffmann. Mercury planner: Pushing the limits of partial delete relaxation.\n\nIPC 2014 planner abstracts, pp. 43–47, 2014.\n\nMichael Katz, Shirin Sohrabi, Horst Samulowitz, and Silvan Sievers. Delfi: Online planner selection\n\nfor cost-optimal planning. IPC-9 planner abstracts, pp. 57–64, 2018.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980,\n\n2014.\n\nDonald E Knuth and Ronald W Moore. An analysis of alpha-beta pruning. Artificial intelligence, 6\n\n(4):293–326, 1975.\n\nRichard E Korf. Macro-operators: A weak method for learning. Artificial intelligence, 26(1):35–77,\n\n1985.\n\nDrew V. McDermott. A heuristic estimator for means-ends analysis in planning. In AIPS, pp. 142–\n\n149. AAAI, 1996.\n\nDavid M ́enager, Dongkyu Choi, Mark Roberts, and David W. Aha. Learning planning operators\n\nfrom episodic traces. In 2018 AAAI Spring Symposia. AAAI Press, 2018.\n\nIonut Moraru, Stefan Edelkamp, Santiago Franco, and Mois ́es Mart ́ınez. Simplifying automated\n\npattern selection for planning with symbolic pattern databases. In KI, pp. 249–263, 2019.\n\nLaurent Orseau and Levi HS Lelis. Policy-guided heuristic search with guarantees. In Proceedings\n\nof the AAAI Conference on Artificial Intelligence, volume 35, pp. 12382–12390, 2021.\n\nS ́ebastien Racani`ere, Th ́eophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adri`a Puigdom`enech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented agents for deep reinforcement learning. Advances in neural information processing systems, 30:5690–5701, 2017.\n\nArthur L Samuel. Some studies in machine learning using the game of checkers. ii—recent progress.\n\nIBM Journal of research and development, 11(6):601–617, 1967.\n\nMax-Philipp B. Schrader. gym-sokoban, 2018.\n\ngym-sokoban.\n\nhttps://github.com/mpSchrader/\n\nJendrik Seipp and Gabriele R ̈oger. Fast downward stone soup 2018. IPC2018–Classical Tracks, pp.\n\n72–74, 2018.\n\nJendrik Seipp, Florian Pommerening, and Malte Helmert. New optimization functions for potential\n\nheuristics. In ICAPS, 2015.\n\nWilliam Shen, Felipe Trevizan, and Sylvie Thi ́ebaux. Learning domain-independent planning\n\nheuristics with hypergraph networks. In ICAPS, volume 30, pp. 574–584, 2020.\n\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354–359, 2017.\n\nTakeshi Takahashi, He Sun, Dong Tian, and Yebin Wang. Learning heuristic functions for mobile\n\nrobot path planning using deep neural networks. In ICAPS, volume 29, pp. 764–772, 2019.\n\nAlvaro Torralba, Vidal Alc ́azar, Daniel Borrajo, Peter Kissmann, and Stefan Edelkamp. Symba*: A symbolic bidirectional a* planner. In International Planning Competition, pp. 105–108, 2014.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSam Toyer, Sylvie Thi ́ebaux, Felipe Trevizan, and Lexing Xie. Asnets: Deep learning for gener-\n\nalised planning. Journal of Artificial Intelligence Research, 68:1–68, 2020.\n\nMarin Vlastelica, Michal Rol ́ınek, and Georg Martius. Neuro-algorithmic policies enable fast com-\n\nbinatorial generalization. arXiv:2102.07456, 2021.\n\nXuemei Wang. Learning planning operators by observation and practice. In Kristian J. Hammond\n\n(ed.), AIPS, pp. 335–340. AAAI, 1994.\n\nRyo Yonetani, Tatsunori Taniai, Mohammadamin Barekatain, Mai Nishimura, and Asako Kanezaki. Path planning using neural A* search. In International Conference on Machine Learning, pp. 12029–12039. PMLR, 2021.\n\nZiqi Zhang and Florian Geißer. Extending graph neural networks for generalized stochastic plan-\n\nning. In Bridging the Gap Between AI Planning and Reinforcement Learning. 2021.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe paper introduces a novel loss function L* for learning heuristic functions that attempts to shrink the size of the the A* search tree. While previous methods for learning heuristic functions relied on minimizing the L2 loss, this paper argues that L2 loss doesn't necessarily reduce the search tree size. Empirical results of Sokoban-like puzzles show that L* might indeed reduce the A* search effort.\n\n# Strength And Weaknesses\n\nStrength\n\nThis paper tackles one of the key weaknesses of learning heuristic functions to guide the A* search, which is the lack of a proper loss function. The L* loss seems to be a good step in the direction of having a loss function that is more correlated with the A* search effort.  \n\nWeaknesses\n\nWhile the problem the paper tackles is important and the idea for solving this problem is promising, the paper has several important weaknesses that might prevent it from being published at this point. \n\n1. The paper presents two types empirical evaluations. In the first, one uses classical planners to generate a training set from which a heuristic function is learned. In the second, the algorithm learns from the problems it is able to solve. The first setting is hardly interesting at all because it assumes the existence of a planner that is able to solve the problems. The second setting is the interesting one because it is general and it doesn't assume prior knowledge or a system for generating training data. I would have preferred an extensive evaluation on the second setting than the current mix of evaluations. \n2. The evaluation is somewhat week because the system is evaluated only on grid-based puzzles (Sokoban and Maze with teleports). Instead of showing results on the setting where classical planners generate a training set, I would have preferred to see results on more domains. \n3. The comparison of optimal classical planners with A* with a learned heuristic function is problematic as the planners are finding optimal solutions, while A* is finding suboptimal ones. Even if the solution costs are near optimal, the problems can be substantially easier if you lift the optimality requirement. MCTS and model-free methods aren't good baselines as they tend to be weak. MCTS suffers when planning on long horizons such as those of Sokoban (see [2]). Model-free methods are in disadvantage as they don't use the model to search, so one should expect that they perform rather poorly compared to search algorithms. \n4. A more fair comparison would be with satisficing planners, which don't have the guarantee of finding optimal solutions. Another baseline that should be included is WA* with a learned heuristic function with the L2 loss. WA* is surprisingly effective in the learning setting (see [5]). Levin tree search (LTS) [1][2] uses a policy to guide its search and it defines a loss function that is an upper bound on the size of the tree. One can then learn a policy that minimizes this loss. This is exactly what is done in this paper, but for a policy. It is natural to wonder how A* with the L* loss compares to LTS.   \n5. The paper misses important citations. Whenever talking about optimality of A*, the paper seems to be referring to [3]. The procedure described in Section 5.3 is called Bootstrap [4].\n\nReferences\n\n[1] Orseau, L.; Lelis, L.; Lattimore, T.; and Weber, T. 2018. Single-Agent Policy Tree Search With Guarantees. In Advances in Neural Information Processing Systems 31, 3201–3211. Curran Associates, Inc.\n\n[2] Orseau, Laurent and Levi H. S. Lelis. “Policy-Guided Heuristic Search with Guarantees.” AAAI (2021).\n\n[3] Rina Dechter and Judea Pearl. 1985. Generalized best-first search strategies and the optimality of A*. J. ACM 32, 3 (July 1985), 505–536. https://doi.org/10.1145/3828.3830 \n3.  \n\n[4] Arfaee, Shahab & Zilles, Sandra & Holte, Robert. (2011). Learning heuristic functions for large state spaces. Artif. Intell.. 175. 2075-2098. 10.1016/j.artint.2011.08.001. \n\n[5] Solving the Rubik's Cube with Deep Reinforcement Learning and Search. Forest Agostinelli*, Stephen McAleer*, Alexander Shmakov*, Pierre Baldi. Nature Machine Intelligence, Volume 1, 2019\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity\n\nThe paper is mostly easy to understand. However, I do miss a more formal treatment with respect to A*'s optimality. I don't understand what it means to have a \"false sense of optimality\", for some definition of optimality. \n\nQuality\n\nWhile the idea is promising and interesting, the execution is lacking (see my comments above).\n\nNovelty\n\nThe loss function is novel, to the best of my knowledge. \n\nReproducibility\n\nThe idea is simple enough (which is something very positive!) that should be easy for someone to reproduce the results of this paper.\n\n# Summary Of The Review\n\nThe paper introduces a novel and interesting loss function for learning heuristic functions for guiding the A* search. The results are somewhat preliminary and that is why I recommend rejection at this point. I do strongly encourage the authors to resubmit the paper if it is indeed rejected from this conference. The issues related to writing, lack of formalism, and missing important citations should all be easy to fix. The evaluation is the problem. The evaluation will be much stronger once L* is tested on more problem domains and compared with proper baselines.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "QUANTIFYING AND MITIGATING THE IMPACT OF LABEL ERRORS ON MODEL DISPARITY METRICS\n\nJulius Adebayo Prescient Design / Genentech\n\nMelissa Hall Meta Inc.\n\nBowen Yu Meta Inc.\n\nBobbie Chern Meta Inc.\n\nABSTRACT\n\nErrors in labels obtained via human annotation adversely affect a model’s performance. Existing approaches propose ways to mitigate the effect of label error on a model’s downstream accuracy, yet little is known about its impact on a model’s disparity metrics1. Here we study the effect of label error on a model’s disparity metrics. We empirically characterize how varying levels of label error, in both training and test data, affect these disparity metrics. We find that group calibration and other metrics are sensitive to train-time and test-time label error—particularly for minority groups. This disparate effect persists even for models trained with noise-aware algorithms. To mitigate the impact of training-time label error, we present an approach to estimate the influence of a training input’s label on a model’s group disparity metric. We empirically assess the proposed approach on a variety of datasets and find significant improvement, compared to alternative approaches, in identifying training inputs that improve a model’s disparity metric. We complement the approach with an automatic relabel-and-finetune scheme that produces updated models with, provably, improved group calibration error.\n\n1\n\nINTRODUCTION\n\nLabel error (noise) — mistakes associated with the label assigned to a data point — is a pervasive problem in machine learning (Northcutt et al., 2021). For example, 30 percent of a random 1000 samples from the Google Emotions dataset (Demszky et al., 2020) had label errors (Chen, 2022). Similarly, an analysis of the MS COCO dataset found that up to 37 percent (273,834 errors) of all annotations are erroneous (Murdoch, 2022). Yet, little is known about the effect of label error on a model’s group-based disparity metrics like equal odds (Hardt et al., 2016), group calibration (Pleiss et al., 2017), and false positive rate (Barocas et al., 2019).\n\nIt is now common practice to conduct ‘fairness’ audits (see: (Buolamwini and Gebru, 2018; Raji and Buolamwini, 2019; Bakalar et al., 2021)) of a model’s predictions to identify data subgroups where the model underperforms. Label error in the test data used to conduct a fairness audit renders the results unreliable. Similarly, label error in the training data, especially if the error is systematically more prevalent in certain groups, can lead to models that associate erroneous labels to such groups. The reliability of a fairness audit rests on the assumption that labels are accurate; yet, the sensitivity of a model’s disparity metrics to label error is still poorly understood. Towards such end, we ask:\n\nwhat is the effect of label error on a model’s disparity metric?\n\nWe address the high-level question in a two-pronged manner via the following questions:\n\n1. Research Question 1: What is the sensitivity of a model’s disparity metric to label errors in\n\ntraining and test data? Does the effect of label error vary based on group size?\n\n2. Research Question 2: How can a practitioner identify training points whose labels have the\n\nmost influence on a model’s group disparity metric?\n\n1Group-based disparity metrics like subgroup calibration, false positive rate, false negative rate, equalized odds, and equal opportunity are more often known, colloquially, as fairness metrics in the literature. We use the term group-based disparity metrics in this work.\n\n1\n\nCONTRIBUTIONS & SUMMARY OF FINDINGS\n\nFigure 1: A schematic of the test and train-time empirical sensitivity tests. Here we show the model training and fairness audit pipeline. Our proposed sensitivity tests capture the effect of label error, in both stages, on the disparity metric. In the Test-time sensitivity test, we flip the label of a portion of the test data and then compare the corresponding disparity metric (group calibration for example) for the flipped dataset to the metrics for a standard model where the test labels were not flipped. In the Train-time sensitivity test, we flip the labels of a portion of the training set, and then measure the change in disparity metric to a standard model.\n\nIn addressing these questions, we make two broad contributions:\n\nEmpirical Sensitivity Tests. We assess the sensitivity of model disparity metrics to label errors with a label flipping experiment. First, we iteratively flip the labels of samples in the test set, for a fixed model, and then measure the corresponding change in the model disparity metric compared to an unflipped test set. Second, we fix the test set for the fairness audit but flip the labels of a proportion of the training samples. We then measure the change in the model disparity metrics for a model trained on the data with flipped labels. We perform these tests across a datasets and model combinations.\n\nTraining Point Influence on Disparity Metric. We propose an approach, based on a modification to the influence of a training example on a test example’s loss, to identify training points whose labels have undue effects on any disparity metric of interest on the test set. We empirically assess the proposed approach on a variety of datasets and find a 10-40% improvement, compared to alternative approaches that focus solely on model’s loss, in identifying training inputs that improve a model’s disparity metric.\n\n2 SETUP & BACKGROUND\n\nIn this section, we discuss notation, and set the stage for our contributions by discussing the disparity metrics that we focus on. We also provide an overview of the datasets and models used in the experimental portions of the paper.2\n\nOverview of Notation. We consider prediction problems, i.e, settings where the task is to learn a mapping, θ : X × A → Y, where X ∈ Rd is the feature space, Y ∈ {0, 1} is the output space, and A is a group identifier that partitions the population into disjoint sets e.g. race, gender. We can represent the tuple (xi, ai, yi) as zi. Consequently, the n training points can be written as: {zi}n i=1. Throughout this work, we will only consider learning via empirical risk minimization (ERM), which corresponds to: ˆθ := arg minθ∈Θ i l(zi, θ). Similar to Koh and Liang (2017), we will assume that the ERM objective is twice-differentiable and strictly convex in the parameters. We focus on binary classification tasks, however, our analysis can be easily generalized.\n\n(cid:80)n\n\n1 n\n\nDisparity Metrics. We define a group disparity metric to be a function, GD, that gives a performance score given a model’s probabilistic predictions (θ outputs the probability of belonging to the positive class) and ‘ground-truth’ labels. We consider the following metrics (We refer readers to the Appendix for a detailed overview of these metrics):\n\n2We refer readers to the longer version of this work on the arxiv. Code to replicate our findings is available\n\nat: https://github.com/adebayoj/influencedisparity\n\n2\n\nDataset CivilComments ACSIncome ACSEmployment ACSPublic Coverage Credit Dataset\n\nClasses 2\n2 2\n2 2\n\nn 1, 820, 000 195, 665 378, 817 138, 554 405, 032\n\nd 768 10 16 19 6\n\nGroup\n\nSource\n\nSex Koh and Liang (2017)\n\nSex, Race Ding et al. (2021) Sex, Race Ding et al. (2021) Sex, Race Ding et al. (2021)\n\nSex De Montjoye et al. (2015)\n\nTable 1: Overview of dataset characteristics for the datasets considered in this work.\n\n1. Calibration: defined as P (ˆy = y|ˆp = p) , ∀p ∈ [0, 1]. In this work, we measure calibration with two different metrics: 1) Expected Calibration Error (ECE) (Naeini et al., 2015; Pleiss et al., 2017), and 2) the Brier Score (Rufibach, 2010) (BS). (Generalized) False Positive Rate (FPR): is GDfpr(θ) = E[θ(xi) | yi = 0] (see Guo et al. (2017)), (Generalized) False Negative Rate (FNR): is GDfnr(θ) = E[(1 − θ(xi)) | yi = 1],\n\n2.\n\n3.\n\n4. Error Rate (ER): is the GDer(θ) = 1 − acc(θ).\n\nWe consider these metrics separately for each group as opposed to relative differences. For each dataset, we consider the protected data subgroup with the largest size as the majority group, and the group the smallest size is the minority group.\n\nDatasets. We consider datasets across different modalities: 4 tabular, and a text dataset. A description of these datasets along with test accuracy is provided in Table 2. Each dataset contains annotations with a group label for both training and test data, so we are able to manipulate these labels for our empirical sensitivity tests. For the purposes of this work, we assume that the provided labels are the ground-truth—a strong assumption that nevertheless does not impact the interpretation of our findings.\n\nModel. We consider three kinds of model classes in this work: 1) a logistic regression model, 2) a Gradient-boosted Tree (GBT) classifier for the tabular datasets, and 3) a ResNet-18 model. We only consider the logistic regression and GBT models for tabular data, while we fine-tune a ResNet-18 model on embeddings for the text data.\n\n3 EMPIRICAL ASSESSMENT OF LABEL SENSITIVITY\n\nIn this section, we perform empirical sensitivity tests to quantify the impact of label error on test group disparity metrics. We conduct tests on data from two different stages of the ML pipeline: 1) Test-time (test dataset) and 2) Training-time (training data). We use as our primary experimental tool: label flipping, i.e., we flip the labels of a percentage of the samples, uniformly at random in either the test or training set, and then measure the concomitant change in the model disparity metric. We assume that each dataset’s labels are the ground truth and that flipping the label results in label error for the samples whose labels have been overturned. Recent literature has termed this setting synthetic noise, i.e., the label flipping simulates noise that might not be representative of real-world noise in labels (Arpit et al., 2017; Zhang et al., 2021; Jiang et al., 2020).\n\n3.1 SENSITIVITY TO TEST-TIME LABEL ERROR\n\nOverview & Experimental Setup. The goal of the test-time empirical test is to measure the impact of label error on the group calibration error of a fixed model. Consider the setting where a model has been trained, and a fairness assessment is to be conducted on the model. What impact does label error, in the test set used to conduct the audit, have on the calibration error on the test data? The test-time empirical tests answer this question. Given a fixed model, we iteratively flip a percentage of the labels, uniformly at random, ranging from zero to 30 percent in the test data. We then estimate the model’s calibration using the modified dataset. Critically, we keep the model fixed while performing these tests across each dataset.\n\nResults. In Figure 2, we report results of the label flipping experiments across 6 tasks. On the horizontal axis, we have the percentage of labels flipped in the test dataset, while on the vertical\n\n3\n\nFigure 2: Test-time Label Flipping Results across. For each dataset, we plot the percent change in calibration error versus the corresponding percentage change in label error. Here, we plot the minority (smallest) group as well as the majority (largest) group. These two groups represent two ends of the spectrum for the impact of label error. We observe that across all datasets, the minority group incurs higher percentage change in group calibration compared to the majority group.\n\naxis, we have the percentage change in the model’s calibration. For each dataset, we compute model calibration for two demographic groups in the dataset, the majority and the minority—in size–groups. We do this since these two groups constitute the two ends of the spectrum in the dataset. As shown, we observe a more distinctive effect for the minority group across all datasets. This is to be expected since flipping even a small number samples in the minority group can have a dramatic effect on test and training accuracy within this group. For both groups, we observe a changes to the calibration error. For example, for the Income prediction task on the Adult dataset, a 10 percent label error induces at least a 20 percent change in the model’s test calibration error. These results suggest that test-time label error has more pronounced effects for minority groups. Similarly, we observe for other disparity metrics (See Appendix) across all model classes that increases in percentage of labels flipped disproportionately affects the minority group.\n\n3.2 SENSITIVITY TO TRAINING LABEL ERROR\n\nOverview & Experimental Setup. The goal of the training-time empirical tests is to measure the impact of label error on a trained model. More specifically, given a training set in which a fraction of the samples’ labels have been flipped, what effect does the label error have on the calibration error compared to a model trained on data without label error? We simulate this setting by creating multiple copies of each of the datasets where a percentage of the training labels have been flipped uniformly at random. We then assess the model calibration of these different model using the same fixed test dataset. Under similar experimental training conditions for these models, we are then able to quantify the effect of training label error on a model’s test calibration error. We conduct this analysis across all dataset-model task pairs.\n\nResults & Implications. We show the results of the training-time experiments in Figure 3. Similar to the test-time experiments, we find minority groups are more sensitive to label error than larger groups. Specifically, we find that even a 5 percent label error can induce significant changes in the disparity metrics, of a model trained on such data, for these groups.\n\nA conjecture for the higher sensitivity to extreme training-time error is that a model trained on significant label error might have a more difficult time learning patterns in the minority class where there are not enough samples to begin with. Consequently, the generalization performance of this model worsens for inputs that belong to the minority group. Alternatively, in the majority group, the proportion of corrupted labels due to label error is smaller. This might mean that uniform flipping does not affect the proportion of true labels compared to the minority group. Even though the majority group exhibits label error, there still exists enough samples with true labels such that a model can learn the underlying signal for the majority class.\n\n4\n\nFigure 3: Training-time Label Flipping Results. For each dataset, we plot the percent change in calibration error versus the corresponding percentage change in label error for the training set. Here, we plot the minority (smallest) group as well as the majority (largest) groups by size. Similar to the test-time setting, we observe that across all datasets, the minority group incurs higher percentage change in group calibration compared to the majority group. However, we observe a larger magnitude change for the minority groups.\n\nA second important finding is that overparameterization seems to confer more resilience to training label error. We find that for the same levels of training label error, an overparametrized model is less sensitive to such change compared to a model with a smaller number of parameters. Recent work suggests that models that learn functions that are more aligned with the underlying target function of the data generation process are more resilient to training label error (Li et al., 2021). It might be that compared to linear and tree-based models, an overparametrized deep net is more capable of learning an aligned function.\n\n3.3 NOISE-AWARE ROBUST LEARNING HAS DISPARATE IMPACT\n\nOverview & Experimental Setup. We now assess whether training models with noise-aware algorithmic interventions (e.g. robust loss functions (Ma et al., 2020; Ghosh et al., 2017)) results in models whose disparity metrics have reduced sensitivity to label error in the training set. We test this hypothesis on a modified Cifar-10 dataset following the setting of Hall et al. (2022). Specifically, the Cifar10 dataset is modified to a binary classification setting along with group labels by inverting a subset of each class’s examples. Given a specified parameter ε ∈ [0, 1/2], a 1 2 − ε of the negative class is inverted, while a 1 2 + ε of the positive class is inverted leading to 2ε fraction of one group of samples and 1 − 2ε of the other group. In all experiments we set ε = 0.15 for a 30 percent minority group membership. We replicate the label flipping experiment on the task with a Resnet-18 model. We test the MEIDTM (Cheng et al., 2022), DivideMix (Li et al., 2020), and a robust loss approach (Ghosh et al., 2017).\n\nFigure 4: Effect of Noiseaware algorithms on group calibration.\n\nResults. At a high level, for the majority group, we find that group calibration remains resilient to low rates of label error (below 25 percent). At high rates (>30 percent label error), we start to see increased sensitivity. However, for the minority group (30 percent of the dataset), we observe group calibration remains sensitive to label error even at low levels. This finding suggests that noise-aware methods show are more effective for larger groups in the data. A similar observation has also been made for other algorithmic interventions like Pruning (Tran et al., 2022; Hooker et al., 2019), differential privacy (Bagdasaryan et al., 2019), selective classification (Jones et al., 2020) and adversarial training (Xu et al., 2021).\n\n5\n\n4\n\nINFLUENCE OF TRAINING LABEL ON TEST DISPARITY METRIC\n\nWe now present an approach for estimating the ‘influence’ of perturbing a training point’s label on a disparity metric of interest. We consider: 1) up-weighting a training point, and 2) perturbing the training label.\n\nUpweighting a training point. Let ˆθ−zi be the ERM solution when a model is trained on all data points, {zj}n j=1, except zi. The influence, Iup,params, of datapoint, zi, on the model parameters is then defined: ˆθ−zi − ˆθ. This measure indicates how much the parameters change when the model is ‘refit’ on all training data points except zi. Koh and Liang (2017) give a closed-form estimate of this quantity as:\n\nIup,params\n\ndef=\n\ndˆθε, zi dε\n\n(cid:12) (cid:12) (cid:12) (cid:12)ε=0\n\n= −H −1\n\nˆθ\n\n∇θl(zi, ˆθ),\n\n(1)\n\nwhere H is the hessian, i.e., Hˆθ θl(zi, θ). The loss on a test example, l(zt, ˆθ), is a function of the model parameters, so using the chain-rule, we can estimate the influence, Iup,loss(zi, zt), of a training point, zi, on l(zt, ˆθ) as:\n\nn\n\ndef= 1\n\n(cid:80)n\n\ni=1 ∇2\n\nIup,loss(zi, zt) def=\n\ndl(zt, ˆθε, zi ) dε\n\n(cid:12) (cid:12) (cid:12) (cid:12)ε=0\n\n= −∇θl(zt, ˆθ)⊤H −1\n\nˆθ\n\n∇θl(zi, ˆθ).\n\n(2)\n\nPerturbing a training point’s label. A second notion of influence that Koh and Liang (2017) study is how perturbing a training point leads to changes in the model parameters. Specifically, given a training input, zi, that is a tuple (xi, yi), how would the perturbation, zi → zi,δ, which is defined as (xi, yi) → (xi, yi + δ), change the model’s predictions? Koh and Liang (2017) give a closed-form estimate of this quantity as:\n\nIpert,loss,y(zj, zt) ≈ −∇θl(zt, ˆθzj,δ,−zj )⊤H −1\n\nˆθ\n\n∇y∇θl(zj, ˆθ).\n\n(3)\n\nAdapting influence functions to group disparity metrics. We now propose modifications that allow us to compute the influence of a training point on a test group disparity metric (See Appendix D for longer discussion). Let St be a set of test examples. We can then denote GD(St, ˆθ) as the group disparity metric of interest, e.g., the estimated ECE for the set St given parameter setting ˆθ.\n\nInfluence of upweighting a training point on a test group disparity metric. A group disparity metric on the test set is a function of the model parameters; consequently, we can apply the chain rule to Iup,params (from Equation 1) to estimate the influence, Iup,disparity, of up-weighting a training point on the disparity metric as follows:\n\nIup,disparity(zi, St) def=\n\ndGD(St, ˆθε, zi) dε\n\n(cid:12) (cid:12) (cid:12) (cid:12)ε=0\n\n= −∇θGD(St, ˆθ)⊤ dˆθε, zi\n\ndε\n\n(cid:12) (cid:12) (cid:12) (cid:12)ε=0\n\n,\n\nWe now have a closed-form expression for a training point’s influence on a test group disparity metric.\n\n= −∇θGD(St, ˆθ)⊤H −1\n\nˆθ\n\n∇θl(zi, ˆθ).\n\n(4)\n\nInfluence of perturbing a training point’s label on a test group disparity metric. We now consider the influence of a training label perturbation on a group disparity metric of interest. To do this, we simply consider the group disparity metric function as the quantity of interest instead of the test loss. Consequently, the closed-form expression for the influence of a modification to the training label on disparity for a given test set is:\n\nIpert,disparity,y(zj, St) ≈ −∇θGD(St, ˆθ)⊤H −1\n\nˆθ\n\n∇y∇θl(zj, ˆθ).\n\n(5)\n\nWith Equations 4 and 5, we have the key quantities of interest that allows us to rank training points, in terms of influence, on the test group disparity metric.\n\n6\n\n5\n\nIDENTIFYING AND CORRECTING TRAINING LABEL ERROR\n\nIn this section, we empirically assess the modified influence expressions for calibration across these datasets for prioritizing mislabelled samples. We find that the prioritization scheme shows improvement, compared to alternative approaches. In addition, we propose an approach to automatically correct the labels identified by our proposed approach.\n\n5.1\n\nIDENTIFYING LABEL ERROR\n\nOverview & Experimental Question. We are interested in surfacing training points whose change in label will induce a concomitant change in a test disparity metric like group calibration. Specifically, we ask: When the training points are ranked by influence on test calibration, are the most highly influential training points most likely to have the wrong labels? We conduct our experiments to directly measure a method’s ability to answer this question.\n\nExperimental Setup. For each dataset, we randomly flip the labels of 10 − 30 percent of the training samples. We then train on this modified dataset. In this task, we have direct access to the ground-truth of the exact samples whose labels were flipped. This allows us to directly compare the performance of our proposed methods to each of the baselines on this task. We then rank training points using a number of baseline approaches as well as the modified influence approaches. For the top 50 examples, we consider what fraction of these examples had flipped labels in the training set. We discuss additional experimental details in the Appendix.\n\nApproaches & Baselines. We consider the following methods: 1) IF-Calib: The closed-form approximation to the influence of a training point on the test calibration; 2) IF-Calib-Label: The closed-form approximation to the influence of a training point’s label on the test calibration; 3) Loss: A baseline method which is the training loss evaluated at each data point in the training set. The intuition is that, presumably, more difficult training samples will have higher training loss. We also consider several additional baselines that we discuss in the Appendix.\n\nFigure 5: Empirical Results for Training Point Ranking Across 6 datasets. For the top 50 most influential examples, we show the proportion of samples whose labels were flipped in the training data.\n\nResults: Prioritizing Samples. In Figure 5, we show the performance of the two approximations that we consider in this work as well as two baselines. We plot the fraction of inputs, out of the top ranked 50 ranked training points, whose labels were flipped in the training set. The higher this proportion, then the more effective an approach is in identifying the samples that likely have wrong labels. In practice, the goal is to surface these training samples and have a domain expert inspect them. If a larger proportion of the items to be inspected are mislabeled, then a higher proportion of training set mistakes, i.e. label error, can be fixed. Across the different datasets, we find a 10-40 percent improvement, compared to baseline approaches, in identifying critical training data points whose labels need to be reexamined.\n\n7\n\nProportion of Flipped LabelsProportion of Flipped LabelsWe find the loss baseline to be ineffective for ranking in our experiments. A possible reason is that modern machine learning models can typically be trained to ‘memorize’ the training data; resulting in settings where a model has low loss even on outliers or mislabeled examples. In such a case, ranking by training loss for a sample is an ineffective ranking strategy. We find that the noise-aware approaches perform similarly to the IF-Norm baseline. We defer the results of the uncertainty-based baselines and the noise-aware methods to Appendix (Section F). We find that these baselines also underperform our proposed approaches.\n\n5.2 CORRECTING LABEL ERROR\n\nWe take label error identification one step further to automatically relabelling inputs that have identified as critical. We restrict our focus to binary classification where the label set is {0, 1}, and the corresponding relabelling function is simply 1 − yi, where yi is the predicted label.\n\nSetup & Experiment: We consider the logistic regression model across all tasks for a setting with 20 percent training label error. We consider calibration as the disparity function of interest. We then rank the top 20 percent of training points by label-disparity influence, our proposed approach. For these points, we apply the relabelling function, and then fine-tune the model for an additional epoch with the modified labels.\n\nResults: First, we observe an improvement, in group calibration, across all groups, with larger improvement coming from the smallest group. As expected, we also observe a decrease in the average loss for the overall training set. These results point to increasing promise of automatic relabeling.\n\nTheoretical Justification. We now present a theorem that suggests that the influence priorization and relabeling scheme described above provably leads to better calibrated models.\n\nTheorem 1. Given a κ-strongly convex loss function l(., .), with κ > 0, a training dataset, D, where A indexes the data groups, and a model, ˆθ : xi → yi, optimized via l(., .) that maps inputs to labels. Let Q be a set of test examples all belonging to group A = a, where ECalQ(ˆθ) is the expected calibration error of ˆθ on the set Q. In addition, let DA=a be the set of problematic training examples, belonging to group a, prioritized based on influence, i.e., Ipert,calib,yi(xi a, Q) > 0. We term a model trained on a different training set (D+) where the problematic examples have been relabeled to be ˆθR. Analogously, the expected calibration error of this new model on the set Q is ECalQ(ˆθR). We have that:\n\nECalQ(ˆθR) ≤ ECalQ(ˆθ).\n\nWe defer the proof to the Appendix. Theorem 1 suggests that when a model is trained on a relabeled dataset, following the influence prioritization scheme, the expected group calibration of the retrained model should be lower than that of a model trained on a dataset that has not been relabeled.\n\n6 RELATED WORK\n\nWe discuss directly related work here, and defer a longer discussion to Section A of the Appendix.\n\nImpact of Label Error/Noise on Model Accuracy. Learning under label error falls under the category more commonly known as learning under noise (Frénay and Verleysen, 2013; Natarajan et al., 2013; Bootkrajang and Kabán, 2012). Noise in learning can come from different either input features or the labels. In this work, we focus on label error—categorization mistakes associated with the label in both the test and training data. Previous work focused primarily on the effect of label error in the training data; however, we advance this line of work to investigate the effect of label error in the test data used to conduct a fairness audit on the reliability of the audit. Model resilience to training label error has been studied for both synthetic (Arpit et al., 2017; Zhang et al., 2021; Rolnick et al., 2017) and real-world noise settings (Jiang et al., 2020). A major line of inquiry is the development of algorithmic approaches to learn accurate models given a training set with noisy labels. These approaches include model regularization (Srivastava et al., 2014; Zhang et al., 2017), bootstrap (Reed et al., 2014), knowledge distillation (Jiang et al., 2020), instance weighting (Ren et al., 2018; Jiang and Nachum, 2020), robust loss functions (Ma et al., 2020; Ghosh et al., 2017), or trusted data (Hendrycks et al., 2018), joint training (Wei et al., 2020), mixture models in semi-supervised learning (Li et al.,\n\n8\n\n2020), and methods to learn a transition matrix that captures noise dependencies (Cheng et al., 2022). In contrast to this line of work, we primarily seek to identify the problematic instances that need to be relabelled, often by a human labeler, and not automatically learn a model that is robust to label error.\n\nImpact of Label Error on Model ‘Fairness’. This work contributes to the burgeoning area that studies the impact of label error on a model’s ‘fairness’ (termed ‘group-based disparity’ in this paper) metrics. Fogliato et al. (2020) studied a setting in which the labels used for model training are a noisy proxy for the true label of interest, e.g., predicting rearrest as a proxy for rearrest. Wang et al. (2021) considers an ERM problem subject to group disparity constraints with group-dependent label noise, and provides theoretical results along with a scheme to obtain classifiers that are robust to noise. Different from their setting, we consider unconstrained ERM (no fairness constraints during learning). Similarly, Konstantinov and Lampert (2021) study the effect of adversarial data corruptions on fair learning in a PAC model. Jiang and Nachum (2020) propose a re-weighting scheme that is able to correct for label noise.\n\nInfluence Functions & Their Uses. Influence functions originate from robust statistics where it is used as a tool to identify outliers (Cook and Weisberg, 1982; Cook, 1986; Hampel, 1974). Koh and Liang (2017) introduced influence functions for modern machine learning models, and used them for various model debugging tasks. Most similar to our work, Sattigeri et al. (2022) and Li and Liu (2022) also consider the influence of a training point on model’s disparity metric, and present intriguing results that demonstrate that reweighting training samples can improve a model’s disparity metrics. Here, we focus specifically on the role of mislabeled examples; however, our goal aligns with theirs. Similarly, Kong et al. (2021) propose RDIA, a relabelling scheme based on the influence function that is able to provably correct for label error in the training data. RDIA identifies training samples that have a high influence on the test loss for a validation set; however, we focus on identifying training samples that influence a group-disparity metric on a test/audit set. We also rely on their technical results to prove Theorem 1.\n\nIn recent work, De-Arteaga et al. (2021) study expert consistency in data labeling and use influence functions to estimate the impact of labelers on a model’s predictions. Along similar direction, Brunet et al. (2019) adapt the influence function approach to measure how removing a small part of a training corpus, in a word embedding task, affects test bias as measured by the word embedding association test Caliskan et al. (2017). Feldman and Zhang (2020) use influence functions to estimate how likely a training point is to have been memorized by a model. More generally, influence functions are gaining widespread use as a tool for debugging model predictions (Barshan et al., 2020; Han et al., 2020; Yeh et al., 2018; Pruthi et al., 2020). Different from these uses of influence functions, here we isolate the effect of a training point’s label on a model’s disparity metric on a audit data.\n\n7 CONCLUSION\n\nIn this paper, we sought to address two key questions: 1) What is the impact of label error on a model’s group disparity metric, especially for smaller groups in the data; and 2) How can a practitioner identify training samples whose labels would also lead to a significant change in the test disparity metric of interest? We find that disparity metrics are, indeed, sensitive to test and training time label error particularly for minority groups in the data. In addition, we present an approach for estimating the ‘influence’ of perturbing a training point’s label on a disparity metric of interest, and find a 10-40% improvement, compared to alternative approaches, in identifying training inputs that improve a model’s disparity metric. We present an approach to estimate the effect of a training input’s label on a model’s group disparity metric. Lastly, perform a simple automatic relabel-and-finetune scheme that produces updated models with, provably, improved group calibration error.\n\nOur findings come with certain limitations. In this work, we focused on the influence of label error on disparity metrics. However, other components of the ML pipeline can also impact downstream model performance. The proposed empirical tests simulate the impact of label error; however, it might be the case that real-world label error is less pernicious to model learning dynamics than the synthetic flipping results suggest. Ultimately, we see our work as helping to provide insight and as an additional tool for practitioners seeking to address the challenge of label error particularly in relation to a disparity metric of interest.\n\n9\n\nREFERENCES\n\nDevansh Arpit, Stanisław Jastrz ̨ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International Conference on Machine Learning, pages 233–242. PMLR, 2017.\n\nEugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate\n\nimpact on model accuracy. Advances in neural information processing systems, 32, 2019.\n\nChloé Bakalar, Renata Barreto, Stevie Bergman, Miranda Bogen, Bobbie Chern, Sam Corbett-Davies, Melissa Hall, Isabel Kloumann, Michelle Lam, Joaquin Quiñonero Candela, et al. Fairness on the ground: Applying algorithmic fairness approaches to production systems. arXiv preprint arXiv:2103.06172, 2021.\n\nSolon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning. fairml-\n\nbook.org, 2019. http://www.fairmlbook.org.\n\nElnaz Barshan, Marc-Etienne Brunet, and Gintare Karolina Dziugaite. Relatif: Identifying explanatory training samples via relative influence. In International Conference on Artificial Intelligence and Statistics, pages 1899–1909. PMLR, 2020.\n\nJakramate Bootkrajang and Ata Kabán. Label-noise robust logistic regression and its applications. In Joint European conference on machine learning and knowledge discovery in databases, pages 143–158. Springer, 2012.\n\nMarc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. Understanding the origins of bias in word embeddings. In International Conference on Machine Learning, pages 803–811. PMLR, 2019.\n\nJoy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pages 77–91. PMLR, 2018.\n\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from\n\nlanguage corpora contain human-like biases. Science, 356(6334):183–186, 2017.\n\nEdwin\n\nChen.\n\n30%\n\nof\n\ngoogle’s\n\nlabeled, URL 30-percent-of-googles-reddit-emotions-dataset-is-mislabeled.\n\n2022.\n\nemotions\n\nmishttps://www.surgehq.ai/blog/\n\ndataset\n\nis\n\nDe Cheng, Tongliang Liu, Yixiong Ning, Nannan Wang, Bo Han, Gang Niu, Xinbo Gao, and Masashi Sugiyama. Instance-dependent label-noise learning with manifold-regularized transition matrix estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16630–16639, 2022.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019.\n\nR Dennis Cook. Assessment of local influence. Journal of the Royal Statistical Society: Series B\n\n(Methodological), 48(2):133–155, 1986.\n\nR Dennis Cook and Sanford Weisberg. Residuals and influence in regression. New York: Chapman\n\nand Hall, 1982.\n\nElliot Creager, Jörn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In International Conference on Machine Learning, pages 2189–2200. PMLR, 2021.\n\nMaria De-Arteaga, Artur Dubrawski, and Alexandra Chouldechova. Leveraging expert consistency\n\nto improve algorithmic decision support. arXiv preprint arXiv:2101.09648, 2021.\n\nYves-Alexandre De Montjoye, Laura Radaelli, Vivek Kumar Singh, and Alex “Sandy” Pentland. Unique in the shopping mall: On the reidentifiability of credit card metadata. Science, 347(6221): 536–539, 2015.\n\n10\n\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, and Sujith Ravi. Goemotions: A dataset of fine-grained emotions. arXiv preprint arXiv:2005.00547, 2020.\n\nFrances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair\n\nmachine learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nVitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long\n\ntail via influence estimation. arXiv preprint arXiv:2008.03703, 2020.\n\nRiccardo Fogliato, Alexandra Chouldechova, and Max G’Sell. Fairness evaluation in presence of biased noisy labels. In International Conference on Artificial Intelligence and Statistics, pages 2325–2336. PMLR, 2020.\n\nBenoît Frénay and Michel Verleysen. Classification in the presence of label noise: a survey. IEEE\n\ntransactions on neural networks and learning systems, 25(5):845–869, 2013.\n\nAritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321–1330. PMLR, 2017.\n\nMelissa Hall, Laurens van der Maaten, Laura Gustafson, and Aaron Adcock. A systematic study of\n\nbias amplification. arXiv preprint arXiv:2201.11706, 2022.\n\nFrank R Hampel. The influence curve and its role in robust estimation. Journal of the american\n\nstatistical association, 69(346):383–393, 1974.\n\nXiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling data artifacts through influence functions. arXiv preprint arXiv:2005.06676, 2020.\n\nMoritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances\n\nin neural information processing systems, 29:3315–3323, 2016.\n\nDan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise. Advances in neural information processing systems, 31, 2018.\n\nSara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. What do com-\n\npressed deep neural networks forget? arXiv preprint arXiv:1911.05248, 2019.\n\nHeinrich Jiang and Ofir Nachum. Identifying and correcting label bias in machine learning. In International Conference on Artificial Intelligence and Statistics, pages 702–712. PMLR, 2020.\n\nLu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on controlled noisy labels. In International Conference on Machine Learning, pages 4804–4815. PMLR, 2020.\n\nErik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, and Percy Liang. Selective classification\n\ncan magnify disparities across groups. arXiv preprint arXiv:2010.14134, 2020.\n\nMichael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 247–254, 2019.\n\nPang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In\n\nInternational Conference on Machine Learning, pages 1885–1894. PMLR, 2017.\n\nShuming Kong, Yanyan Shen, and Linpeng Huang. Resolving training biases via influence-based\n\ndata relabeling. In International Conference on Learning Representations, 2021.\n\nNikola Konstantinov and Christoph H Lampert. Fairness-aware pac learning from corrupted data.\n\narXiv preprint arXiv:2102.06004, 2021.\n\n11\n\nFabian Küppers, Jan Kronenberger, Amirhossein Shantia, and Anselm Haselhoff. Multivariate confidence calibration for object detection. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2020.\n\nJingling Li, Mozhi Zhang, Keyulu Xu, John Dickerson, and Jimmy Ba. How does a neural network’s architecture impact its robustness to noisy labels? Advances in Neural Information Processing Systems, 34:9788–9803, 2021.\n\nJunnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-\n\nsupervised learning. arXiv preprint arXiv:2002.07394, 2020.\n\nPeizhao Li and Hongfu Liu. Achieving fairness at no utility cost via data reweighing with influence.\n\nIn International Conference on Machine Learning, pages 12917–12930. PMLR, 2022.\n\nLydia T Liu, Max Simchowitz, and Moritz Hardt. The implicit fairness criterion of unconstrained learning. In International Conference on Machine Learning, pages 4051–4060. PMLR, 2019.\n\nXingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Normalized loss functions for deep learning with noisy labels. In International Conference on Machine Learning, pages 6543–6553. PMLR, 2020.\n\nEdwin Murdoch. 2022.\n\ncoco, how-i-found-nearly-300-000-errors-in-ms-coco-79d382edf22b.\n\ni URL\n\nfound\n\nnearly\n\nms https://medium.com/@jamie_34747/\n\n300,000\n\nerrors\n\nin\n\nHow\n\nMahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\n\nNagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with\n\nnoisy labels. Advances in neural information processing systems, 26:1196–1204, 2013.\n\nCurtis G Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize\n\nmachine learning benchmarks. arXiv preprint arXiv:2103.14749, 2021.\n\nGeoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and\n\ncalibration. arXiv preprint arXiv:1709.02012, 2017.\n\nRomila Pradhan, Jiongli Zhu, Boris Glavic, and Babak Salimi. Interpretable data-based explanations\n\nfor fairness debugging. arXiv preprint arXiv:2112.09745, 2021.\n\nGarima Pruthi, Frederick Liu, Mukund Sundararajan, and Satyen Kale. Estimating training data\n\ninfluence by tracing gradient descent. arXiv preprint arXiv:2002.08484, 2020.\n\nInioluwa Deborah Raji and Joy Buolamwini. Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 429–435, 2019.\n\nScott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.\n\nMengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In International conference on machine learning, pages 4334–4343. PMLR, 2018.\n\nDavid Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive\n\nlabel noise. arXiv preprint arXiv:1705.10694, 2017.\n\nElan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and Zico Kolter. Certified robustness to labelflipping attacks via randomized smoothing. In International Conference on Machine Learning, pages 8230–8241. PMLR, 2020.\n\nKaspar Rufibach. Use of brier score to assess binary predictions. Journal of clinical epidemiology,\n\n63(8):938–939, 2010.\n\n12\n\nPrasanna Sattigeri, Soumya Ghosh, Inkit Padhi, Pierre Dognin, and Kush R Varshney. Fair infinitesimal jackknife: Mitigating the influence of biased training data points without refitting. arXiv preprint arXiv:2212.06803, 2022.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.\n\nCuong Tran, Ferdinando Fioretto, Jung-Eun Kim, and Rakshit Naidu. Pruning has a disparate impact\n\non model accuracy. arXiv preprint arXiv:2205.13574, 2022.\n\nJialu Wang, Yang Liu, and Caleb Levy. Fair classification with group-dependent label noise. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 526–536, 2021.\n\nHongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A joint training method with co-regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13726–13735, 2020.\n\nHuiyu Wu and Diego Klabjan. Logit-based uncertainty measure in classification. In 2021 IEEE\n\nInternational Conference on Big Data (Big Data), pages 948–956. IEEE, 2021.\n\nHan Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang. To be robust or to be fair: Towards fairness in adversarial training. In International Conference on Machine Learning, pages 11492–11501. PMLR, 2021.\n\nChih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection for explaining deep neural networks. Advances in Neural Information Processing Systems, 31, 2018.\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\n13",
    "reference": "# Summary Of The Paper\n\nThis paper studies the effect of label error on the model’s disparity metrics (e.g., calibration, FPR, FNR) on both the training and test set. Empirically, the authors have found that label errors have a larger influence on minority groups than on majority groups. To mitigate the impact of label errors, The authors have proposed a method to estimate the influence of changing a single training input’s label on a model’s group disparity metric.\n\n# Strength And Weaknesses\n\nStrength:\n+ The research problems are important and may have many practical applications. The real-world machine learning dataset can easily contain label errors. Improving the robustness of learning models trained on noisy data is important. Existing methods mainly focus on downstream accuracy, but group-based disparity metrics have been ignored which are also important for designing a robust algorithm. \n+ The proposed method is well-motivated. Estimating the influence of a single training input on a model’s group disparity metric is important for confident example selection and dataset purification.\n\nWeakness:\n+ The technical insight may not be enough. The authors have empirically illustrated that minority groups are more sensitive to label errors than majority groups. To make the conclusion more meaningful and practical, I think it would be great to add some theoretical analysis on the influence of label errors with different minority and majority group sizes.\n\n+ The proposed method for estimating the ‘influence’ of perturbing a training point’s label on a disparity metric may not practical. The computational cost of the method seems very expensive and needs a lot of retraining processes to detect the effect of all training inputs, which can be hard to apply to a dataset with high-dimensional features. In addition, to demonstrate the performance of the proposed methods, some SOTA methods should be compared (e.g., JoCoR, CVPR’20; DivideMix, CVPR’20; MEIDTM, CVPR’22). The benchmark datasets such as CIFAR10 and CIFAR100 with different types of synthetic noise should also be compared.\n\n+ The experiment setting is not clear to me. For example, it is not clear how the minority group and majority group in Fig. 1 and Fig.2 are obtained. I think the authors may also need to discuss that how to apply the convolutional network Resnet-18 to tabular and text datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper generally is well-written and easy to follow, but most discussions are based on experimental results obtained from a few datasets. The experimental settings and comparison should be more detailed and comprehensive.\n\n# Summary Of The Review\n\nFor me, the motivation and research problems of this paper are strong and important. My major concerns are that the technical contribution may not that strong, and the proposed method may not practical and hard to be applied to real-world machine learning datasets.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Details Of Ethics Concerns\n\nI have not found any ethics concerns."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nIMPOSING CONSERVATION PROPERTIES IN DEEP DYNAMICS MODELING VIA CONTRASTIVE LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDeep neural networks (DNN) has shown great capacity of modeling a dynamical system, but these DNN-based dynamical models usually do not obey conservation laws. To impose the learned DNN dynamical models with key physical properties such as conservation laws, this paper proposes a two-step approach to endow the invariant priors into the simulations. We first establish a contrastive learning framework to capture the system invariants along the trajectory observations. During the dynamics modeling, we design a projection layer of DNNs to preserve the system invariance. Through experiments, we show our method consistently outperforms the baseline in both coordinate error and conservation metrics and can be further extended to complex and large dynamics by leveraging autoencoder. Notably, a byproduct of our framework is the automated conservation law discovery for dynamical systems with single conservation property.\n\n1\n\nINTRODUCTION\n\nWith the quick growth of computational resources and massive prediction power of neural networks, recent times have seen great success of artificial intelligence in a wide range of applications such as image classification (He et al., 2016), natural language processing (Vaswani et al., 2017; Devlin et al., 2018) and reinforcement learning (Mnih et al., 2013). Despite the scalability and diversity of modern machine learning tasks, extracting the underlying latent mechanism from training data and deploying the knowledge toward the new occurrence have always been the heart of artificial intelligence. The idea to work with a compressed representation or prior information has been historically entangled with the development of machine learning, from earlier tools like clustering and principal component analysis (PCA) to more contemporary autoencoders or embeddings. In recent years, there is a surge in interest to discover knowledge from larger or even different domains leveraging techniques like representation learning (Bengio et al., 2013; Chen & He, 2021) and transfer learning (Weiss et al., 2016) to more general meta-learning (Rusu et al., 2018; Santoro et al., 2016) and foundation models (Bommasani et al., 2021) which are capable of handling a wide range of tasks.\n\nMany critical discoveries in the world of physics were driven by distilling the invariants from observations. For instance, the Kepler laws were found by analyzing and fitting parameters for the astronomical observations, and the mass conservation law was first carried out by a series of experiments. However, such discovery usually requires extensive human insights and customized strategies for specific problems. This naturally raises a question, can we learn certain conservation laws from real-world data in an automated fashion? On the other hand, data-driven dynamical modeling is prone to violation of physics laws or instability issues (Greydanus et al., 2019; Kolter & Manek, 2019), since the model only statistically learns the data or system state function without knowing physics prior.\n\nIn this paper, we provide a novel contrastive perspective to find one or more distinguishing features (i.e. conservation values) of physics-based system trajectories. By comparing the latent space distance of the system state observations, we aim to learn a low-dimensional representation potentially serving as the invariant term for the system. With such inspiration, we propose ConCerNet consisting of two neural networks. The first network contrastively learns the trajectory invariants, the second network captures the nominal system dynamical behavior which will be corrected by the first network to preserve certain properties of the simulation in the long term prediction. The correction is implemented by projecting the dynamical neural network output on the learned conservation man-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Pipeline to learn the dynamical system conservation and enforce it in simulation. A contrastive learning framework is proposed to extract the invariants across trajectory observations, then the dynamical model is projected to the invariant manifold to guarantee the conservation property.\n\nifold learned by the first module, and therefore enforcing the trajectory conservation for the learned invariants.\n\nWe summarize our main contributions as follows:\n\n• We provide a novel contrastive learning perspective of dynamical system trajectory data to capture the invariants of dynamical systems. One byproduct of this method is that the learned invariant functions discover physical conservation laws in certain cases. To the best of the authors’ knowledge, this is the first work that studies the discovery of conservation laws for general dynamical systems through contrastive learning.\n\n• We propose a projection layer to impose any invariant function for dynamical system tra-\n\njectory prediction, guaranteeing the conservation property during simulation.\n\n• Based on the above two components, we establish a generic learning framework for dynamical system modeling named ConCerNet (CONtrastive ConsERved Network) which provides robustness in prediction outcomes and flexibility to be applied to a wide range of dynamical systems that mandate conservation properties. We conducted extensive experiments to demonstrate the efficacy of ConCerNet, especially its remarkable improvement over a generic neural network in prediction error and conservation violation metrics.\n\n• We draw inferences on the relationship between the contrastively learned function, the exact conservation law, and the logistics of contrastive invariant learning. Further, potential improvements to the proposed method are illustrated that can improve the automated scientific discovery process.\n\n2 BACKGROUND AND RELATED WORK\n\n2.1 CONTRASTIVE LEARNING\n\nUnlike discriminative models that explicitly learn the data mappings, contrastive learning aims to extract the data representation implicitly by comparing among examples. The early idea dates back to the 1990s (Bromley et al., 1993) and has been widely adopted in many areas. One related field to\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nour work is metric learning (Chopra et al., 2005; Harwood et al., 2017; Sohn, 2016), where the goal is to learn a distance function or latent space to cluster similar examples and separate the dis-similar ones.\n\nContrastive learning has been a popular choice for self-supervised learning (SSL) tasks recently, as it demonstrated its performance in many applications such as computer vision (Chen et al., 2020a; He et al., 2020; Ho & Nvasconcelos, 2020; Tian et al., 2020) and natural language processing (Wu et al., 2020b; Gao et al.). There are many existing works related to contrastive learning, covering the design of contrastive loss (Oord et al., 2018a; Chen et al., 2020a;b), memory bank (Wu et al., 2018), sample bias (Chuang et al., 2020; Arora et al., 2019) and momentum encoder (Chen et al., 2020c; He et al., 2020).\n\n2.2 DEEP LEARNING BASED DYNAMICAL SYSTEM MODELING\n\nConstructing dynamical system models from observed data is a long-standing research problem with numerous applications such as forecasting, inference and control. System identification (SYSID) (Ljung, 1998; Keesman & Keesman, 2011) was introduced a few decades ago and designed to fit the system input-output behavior with choice of lightweight basis functions. In recent years, neural networks became increasingly popular in dynamical system modeling due to its representation power. In this paper, we consider the following neural network based learning task to model an (autonomous and continuous time) dynamical system:\n\nfθ(x) ∼ ̇x ≡\n\ndx(t) dt\n\n(1)\n\nwhere x ∈ Rn is the system state and ̇x is its time derivative. fθ : Rn → Rn denotes the neural network model f with parameter θ to approximate ground truth dynamics.\n\nThe vanilla neural networks learn the physics through data by minimizing the step prediction error, without a purposely designed feature to honor other metrics such as conservation laws. One path to address this issue is to include an additional loss in the training (Singh et al., 2021; Wu et al., 2020a; Richards et al., 2018; Wang et al., 2020); however, the soft Lagrangian treatment does not guarantee the model performance during testing. Imposing hard constraints upon the neural network structures is a more desirable approach, where the built-in design naturally respect certain property regardless of input data. Existing work includes: Kolter & Manek (2019) learns the dynamical system and a Lyapunov function to ensure the exponential stability of predicted system; Hamiltonian neural network (HNN, Greydanus et al. (2019)) targets at the Hamiltonian mechanics, directly learns the Hamiltonian and uses the symplectic vector field to approximate the dynamics; Lagrangian neural network (LNN, Cranmer et al. (2020)) extends the work of HNN to Lagrangian mechanics. Although the above models are able to capture certain conservation laws under specific problem formulations, they are not applicable to general conserved dynamical systems (e.g. mass conservation). This motivates our work in this paper to propose a contrastive learning framework in a more generic form that is compatible with arbitrary conservation.\n\n2.3 LEARNING WITH CONSERVED PROPERTIES\n\nAutomated scientific discovery from data without prior knowledge has attracted great interest to both communities in physics and machine learning. Besides the above-mentioned HNN and LNN, a few recent works (Zhang et al., 2018; Liu & Tegmark, 2021; Ha & Jeong, 2021; Liu et al., 2022) have explored automated approaches to extract the invariants or conservation laws from data. Despite the promising results, the existing methods usually suffer from limitations including poor data sample efficiency and reliance on artificial pre-processing, and hereby difficult to extend to larger and more general systems. Note that the aim of our proposed framework is to provide a genuinely adaptable and highly automated tool for trustworthy data-driven dynamical systems modeling, rather than simply solving the above limitations. The other line of physics invariant learning focuses on discovering the hidden symmetries in physical systems (Liu & Tegmark, 2022; Mototake, 2021). To the best of our knowledge, this is the first time conservation law discovery for general dynamical systems is studied through the lens of contrastive learning.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3 PROPOSED METHODS\n\n3.1 CONTRASTIVE LEARNING FOR CONSERVATION PROPERTY FROM SYSTEM\n\nTRAJECTORIES\n\nt ∈ Rn}N,T\n\nIn the practice of dynamical system learning, the dynamics data is usually observed as a set of trajectories of system state {xi i=1,t=1, where i denotes the trajectory index of total trajectory number N and t is the time step with total time step number T . Hθc : Rn → Rm is the neural network parameterization to map the original state to latent representation with dimension m. We let the trajectory set {Ci}N i=1 be the simulation history starting from initial conditions drawn from a distribution D, and assume the initial conditions have various conservation values. Inspired by the concept of Neighborhood Component Analysis (NCA, Goldberger et al. (2004)) and Neighborhood analysis Contrastive loss (NaCl, Ko et al. (2022)), we analogize each trajectory as a neighbor class where the individual points are drawn from and try to solve a N class classification problem by comparing the latent representation of every pair of points. Since the invariants are well conserved along the trajectory and differ among different trajectories, we aim to find the conservation laws which naturally serve as the latent representation of each class. We consider the nearest neighbor selection as a random event, where the probability of point xi t belonging to the trajectory Ck is defined as p(Ck|xi t) in the following design:\n\n(2)\n\n(3)\n\np(Ck|xi\n\nt) :=\n\n(cid:80)N\n\nj=1\n\n(cid:80)T\n\nt2=1 exp(−∥Hθc(xi t2=1 exp(−∥Hθc(xi\n\nt) − Hθc(xi t) − Hθc(xj\n\nt2\n\n(cid:80)T\n\n)∥2)1(t2 ̸= t)\n\nt2 )∥2)1(i ̸= j or t ̸= t2)\n\nBy traversing all pairs of points, we formulate the contrastive loss function:\n\nLcon = E x∼C,C∼D, x+∈C\\x, x−∈C−,C−∼D\\C\n\n\n\n− log\n\n \n \n\n≈\n\n1 N T\n\nN (cid:88)\n\nT (cid:88)\n\ni=1\n\nt1=1\n\n− log\n\n(cid:80)N\n\nj=1\n\n(cid:80)\n\nk exp(−∥Hθc(x) − Hθc(x+\n\nk )∥2)\n\n(cid:80)\n\nk exp(−∥Hθc(x) − Hθc (x+ + (cid:80)\n\nk )∥2) k exp(−∥Hθc(x) − Hθc (x− ) − Hθc (xi t1 ) − Hθc(xj\n\nt2=1 exp(−∥Hθc(xi t2=1 exp(−∥Hθc(xi\n\nt1\n\nt2\n\n(cid:80)T\n\n(cid:80)T\n\nk )∥2)\n\n\n\n \n \n\n)∥2)1(t1 ̸= t2)\n\nt2)∥2)1(i ̸= j or t1 ̸= t2)\n\n(4)\n\nSimilar to NCA, we choose squared Euclidean norm as the distance metric between point pairs1 and the Softmax like function, ensuring the nice property of probabilistic distribution. To notice, in common contrastive learning setting like SimCLR (Chen et al., 2020a) or NCA, the classification target group only contains one element. In our setup, we consider the classification problem as assigning one point to a group of points in the same class, therefore we have the additional summation loop on both denominator and numerator. This setup is similar to the NaCl loss in Ko et al. (2022) with many positive pairs.\n\n3.2 ENFORCING CONSERVATION LAWS IN DYNAMICAL SYSTEM PREDICTION\n\nAfter the contrastive learning of the m conservation terms Hθc(x) ∈ Rm, we attempt to enforce the predicted trajectory along the learned conservation manifold in the simulation stage, s.t. dHθc (x) dt = 0. In the continuous dynamical system like Equation (1), we can project the nominal neural network output fθd (x) onto the conservation manifold by eliminating its parallel component to the normal direction of the invariant planes (i.e. ∇xHθc(x)). We define the projected dynamical model ̃fθd (x) as following:\n\n1Positive pair denotes correlated views of the same example in contrastive learning literature (Chen et al., 2020a), in this paper we define positive pair as two system states from the same trajectory therefore they are assumed with same conservation property and considered in the same class.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n ̃fθd (x) := Projection (fθd (x), {f : ⟨f, ∇xHθc(x)⟩ = 0})\n\n= fθd (x) −\n\nm (cid:88)\n\n⟨fθd (x), (∇xH i\n\nθc\n\n(x))⊥⟩(∇xH i θc\n\n(x))⊥\n\n(5)\n\n(6)\n\ni=1\n\nwhere {(∇xH i (x))⊥} denotes the orthonormalized set of vectors from {∇xHθc(x)} by θc Gram–Schmidt process. The summation symbol indicates projections to each of the conservation terms and vanishes if only 1 conservation term is learned (i.e. m = 1). The projected model dynamics naturally satisfies ⟨ ̃fθd (x), ∇xHθc(x)⟩ = 0 and therefore guarantees Hθc (x) being constant along the simulated trajectory. The intuitive diagram of projection is shown in Figure 1.\n\nFor dynamical system learning, the loss function is simply the mean square loss between the neural network prediction and the observed system time derivative.\n\nLdyn = E\n\nx∼C∼D\n\n(cid:104)\n\n∥ ̃fθd (x) − ̇x∥2(cid:105)\n\n(7)\n\n4 EXPERIMENTS\n\nTo demonstrate the power of our method, we first illustrate the procedure through two simple conservation examples, then we show technical details to overcome a more complex problem. In the end, we highlight the method is extendable to high-dimensional problems by leveraging an autoencoder. All the system and experiment details are listed in Appendix A.\n\nFigure 2: Learned conservation function vs ground truth, left: ideal spring mass, right: chemical kinetics\n\n4.1 SIMPLE CONSERVATION EXAMPLES\n\nIn this section, we introduce two simple examples: Ideal spring mass system under energy conservation (x[1]2 + x[2]2) and Chemical reaction under mass conservation (x[1] + x[2]). Both systems have 2D state space for easier visualization of the learned conservation function. Figure 2 shows the learned conservation compared with ground truth. The contrastive learning process captures the quadratic and linear functions, as the contour lines are drawn in circles and affine functions. To notice, the learned conservation here is approximately the exact conservation differing by some constant coefficient, as the conservation is a relative quantify instead of an absolute value. For further relationship between contrastively learning invariants and actual conservation, we delay the discussion to Section 5.1. In Figure 3, we compare the two methods by showing the trajectory, conservation and coordinate error to the ground truth. The vanilla neural network is likely to quickly diverge from the conserved trajectory, and the error grows faster than our proposed method.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Simulation comparisons of two simple examples: upper row: ideal spring mass system, lower row: chemical kinematics. 1st column: state trajectories, 2nd column: violation of conservation laws to ground truth, 3rd column: mean square error to ground truth.\n\nFigure 4: Kepler system results: First rows (contrastive learning) learned invariants for 6 sampled trajectories. Last row (dynamics simulation): state trajectories, violation of conservation laws to ground truth, mean square error to ground truth.\n\n4.2 COMPLEX CONSERVATION FUNCTIONS\n\nIn this section, we tackle a more complex system with more than one conservation laws with complicated representations. The Kepler system describes a planet orbiting around a star with elliptical trajectories. The planet has four dimensional states, including both coordinates in the 2D plane and the corresponding velocity. The system has two conservation terms (energy conservation x[3]2+x[4]2 x[1]2+x[2]2 and angular momentum conservation x[1]x[4] − x[2]x[3]). 2\n\n1√\n\n−\n\nThe major challenge for Kepler system conservation learning is data requirement and the representation power of the simple neural network to capture the complex energy conservation function including the square root on the denominator. Besides, if we use the standard contrastive loss from Equation (3), we found it is possible that the learned conservation might converge to a trivial solution as the loss function might encourage more of “similarity” within trajectories than “discrepancy” be-\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\ntween trajectories. To address this issue, we propose to use batch normalized latent function during training by replacing Hθc(x) with H θc(x) with\n\nH θc (xj) =\n\nHθc(x) − μbatch σbatch\n\n(8)\n\nwhere μbatch = 1 i Hθc(xi) and σbatch = mean and standard deviation of the mini-batch.\n\nn\n\n(cid:80)n\n\n(cid:113) 1\n\nn\n\n(cid:80)n\n\ni (Hθc (xi) − μbatch)2 are the element-wise\n\nFrom Figure 4, we use different dimensions (dim= 2 and 1) for the latent space and plot the learned conservation along 6 trajectories. We found that for both dimensions, the learned conservations are distinguishable but struggling to follow the flat line, where the periodic pattern from the orbit loops challenges the prediction power of the neural network to find the correct law. In terms of the relationship between learned and actual conservation, it is difficult to draw any conclusions other than the ordering of the trajectories. Despite not learning the exact conservation law, in the simulation stage, our method still outperforms the vanilla neural network by a large margin in both metrics with the learned invariant. Interestingly, the conservation laws are better enforced with dim(Hθc(x)) = 2 comparing to dim(Hθc (x)) = 1, indicating the system naturally follows 2 conservation laws.\n\n4.3 LARGER SYSTEM: HEAT EQUATION\n\nTo further extend our model to larger systems, we test our method on solving the Heat Equation on a 1D rod. The 1D rod is given some initial temperature distribution and insulated boundary condition on both ends. The temperature U (y, t), as a function of coordinate and time, gradually evens up following the heat equation ∂U ∂y2 . The total internal energy along the rod does not vary because the heat flow is blocked by the boundary. We use system state x consisting of overall 101 nodes to discretize y ∈ [−5, 5] and compress system states to a 9 dimension latent space with an autoencoder pair (EθE , DθD ). For both contrastive conservation learning and dynamical system learning, original space state and time derivative (x, ̇x) are mapped to the autoencoder latent space (z, ̇z) by\n\n∂t = ∂2U\n\nz = EθE (x)\n\n(9)\n\n∂EθE (x) ∂x where × denotes the matrix multiplication by chain rule, the partial derivative from latent space to original space can be calculated by auto-differentiation package. After simulation, the latent space trajectory will be mapped back to the original space by DθD .\n\n(10)\n\n× ̇x\n\n ̇z =\n\nFigure 5: Heat Equation Simulation. Left column: vanilla neural network coordinate error and conservation violation to ground truth. Mid-column: our method. Right column: initial and final temperature distribution\n\nFigure 5 shows the simulation result and conservation metric comparison between vanilla neural network and our method. For both methods, the initial conservation violation error was introduced\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nby autoencoder. In general, our method conforms to ground truth trajectory and conservation laws much better than the vanilla method.\n\nWe generalize the quantitative results for all the experiments above in Table 1. Each of the experiments is performed over three random seeds. During testing, we integrate the trajectory with RungeKutta method and compare the trajectory state coordinate and conservation error to the ground truth. Our method outperforms the baseline neural network by a large margin, and the error is often multiple times smaller. One may notice that the standard deviation is comparable with the error metric in the experiments, this is due to the instability of the dynamical system. The trajectory tracking error will exponentially grow as a function of time, and the cases with outlier initialization are likely to dominate the averaged results and lead to large variance. We append the same results in log scale in Appendix A to help clarification. In practice, our method is capable to control the tracking deviation better than the baseline method across almost all the cases.\n\nTable 1: Simulation error over the tasks\n\nMean square error\n\nTask\n\nBaseline NN Ideal spring mass system 0.209 ± 0.172 0.064 ± 0.006 0.854 ± 0.103 0.133 ± 0.054\n\nChemical kinematics Kepler system Heat equation\n\nConCerNet 0.076 ± 0.063 0.031 ± 0.033 0.328 ± 0.026 0.098 ± 0.073\n\nViolation of conservation laws ConCerNet Baseline NN 0.002 ± 0.002 0.096 ± 0.080 0.003 ± 0.001 0.025 ± 0.012 0.009 ± 0.012 0.060 ± 0.011 0.178 ± 0.045 0.686 ± 0.546\n\n5 DISCUSSIONS\n\n5.1 LEARNED INVARIANTS VS EXACT CONSERVATION LAWS\n\nTo notice, the learned invariants are different from exact physics conservation laws. From Figure 2, we can tell the learned quantity is approximately a linear function of the conserved quantity. We will give a glimpse of the intuition of why linear function reaches the minimal loss with one-dimensional conservation assumption in Section 5.2. In fact, the linear coefficient is not always positive, therefore the sign of the conservation function is not guaranteed. This is the natural result for a perfectly conserved system, lacking of time evolution information for conservation quantities. In real world cases, many systems are dissipative, with directional time derivative on nominal conservation. We design a ranking loss function to utilize the directional information and encourage the correct sign of the learn invariants, the result is delayed to Appendix B.\n\nDespite the similarity between learned invariants and conservation laws for the above cases, for a general system with more than one conservation terms, the learned invariant can be a non-linear function of the conservation terms. Therefore, it might not preserve the linear relationship. In this paper, we focus on improving the conservation performance for dynamical modeling and leave the task to find the multiple conservation laws to future work. Regardless of the mapping between the learned function and the exact conservation function, the simulation is guaranteed to preserve the conservation property if the mapping is bijection.\n\n5.2 CONTRASTIVELY LEARNED LATENT SPACE FOR CONTINUOUS LABELS\n\nAs the original contrastive learning framework is designed for discrete labels, we would like to investigate how the learned latent space looks like when a continuous conservation function implicitly labels the observations. Let a system with state x has distribution D on a compact set X ⊂ Rn, there exists a continuous scalar function g : Rn → R denoting the unknown conservation function mapping X to another compact set Z = {z|z = g(x), x ∈ X }. Consider Z as a continuous label space, where close examples in the Z space are considered noisy observations to each other and belong to the same class. Formally, (x, x′) ∈ {positive pairs}, ∀|g(x′) − g(x)| ≤ ε. Then we can write the “continuous” version of the contrastive loss function Equation (3).\n\nLcontinuous = −\n\n(cid:82)\n\npD(x)log(\n\n(cid:90)\n\nX\n\n{x′||g(x′)−g(x)|≤ε} pD(x′)exp(−(hθ(x) − hθ(x′))2)dx′ (cid:82) X pD(x′′)exp(−(hθ(x) − hθ(x′′))2)dx′′\n\n)dx (11)\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nwhere hθ() is the parameterized conservation function and pD() is the probability over the distribution D. The numerator traverses the similarity on the neighborhood x′ assumed in the same class of x, the denominator calculates the same integral over the entire space. As we attempt to analytically solve the optimization problem for Equation (11) but only come up with a trivial maximum solution, we conduct numerical experiments and make the following conjecture:\n\nConjecture 1. Let D be a uniform distribution, optimize Lcontinuous over a bounded set of θ, then hθ(x) is an affine function of g(x) (e.g. hθ(x) = c1g(x) + c2 ) when Lcontinuous achieves a minimum point.\n\nWe illustrate the conjecture through the following numerical example. We consider a 1D uniform distribution of x over the set X = [−1, 1] and g(x) = x for simplicity. We parameterize hθ(x) with a quadratic function and show the numerical results of Lcontinuous in Figure 6. The trivial maximum point is when hθ(x) being constant (blue). The minimum point reaches when hθ(x) is a linear function with the largest absolute coefficient (green). Interestingly, the red solution performs worse than green with the same starting and ending points, indicating a deviation from the linear transform will increase Lcontinuous. For more generality, we provide another numerical experiments with g(x) = x2 without bijection relationship between X and Z in Appendix C, the result is consistent with our conjecture.\n\nFigure 6: Right: hθ(x) = ax2 + bx + c on different parameterization. Left: Contrastive loss on 1D continuous label space as function of (a, b) ∈ [−10, 10]2, ε = 0.05.\n\n5.3 CONTRASTIVE LEARNING TRAINING LOGISTICS\n\nDuring the training of trajectory invariants with contrastive learning, we observe certain interesting phenomenon and would like to involve the discussion here for readers’ reference. We use the linear regression error to the exact conservation function as metric and delay the experiment results under different hyper-parameters to Appendix D. We found the fitting error decays with O(N −1/2), where N is the training trajectory number. This echoes with the supervised deep learning generalization bound in literature (Yehudai & Shamir, 2019; Cao & Gu, 2019; 2020). In practical contrastive training, we also use small sizes (10-50) as large sizes slightly compromise the performance. Our intuitive explanation is that a large batch is likely to involve similar but less distinguishable trajectories.\n\n6 CONCLUSION\n\nIn this paper, we propose ConCerNet, a generic framework to learn the dynamical system with designed features to preserve the invariant properties along the simulation trajectory. We firstly learn the conservation manifold in the state space with contrastive view over the trajectory observation, then purposely enforce the dynamical system to stay in the desired subspace by appending a projection layer after the nominal neural network. We show the advantage of our proposed method in both simulation error and conservation metrics and extendibility to be incorporated into larger models. Despite the paper presents an end-to-end approach, both contrastive learning on system invariants and projected dynamical system learning can be seen as an independent procedure and open up a different direction. We believe these ideas represent a promising route in automated system property discovery and practical dynamical system modeling.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. A theoretical analysis of contrastive unsupervised representation learning. In 36th International Conference on Machine Learning, ICML 2019, pp. 9904–9923. International Machine Learning Society (IMLS), 2019.\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.\n\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard S ̈ackinger, and Roopak Shah. Signature verification using a” siamese” time delay neural network. Advances in neural information processing systems, 6, 1993.\n\nYuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and\n\ndeep neural networks. Advances in neural information processing systems, 32, 2019.\n\nYuan Cao and Quanquan Gu. Generalization error bounds of gradient descent for learning overparameterized deep relu networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 3349–3356, 2020.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020a.\n\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:22243–22255, 2020b.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021.\n\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\n\nImproved baselines with momentum\n\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020c.\n\nSumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with application to face verification. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), volume 1, pp. 539–546. IEEE, 2005.\n\nChing-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. Advances in neural information processing systems, 33:8765–8775, 2020.\n\nMiles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho.\n\nLagrangian neural networks. arXiv preprint arXiv:2003.04630, 2020.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence\n\nembeddings.\n\nJacob Goldberger, Geoffrey E Hinton, Sam Roweis, and Russ R Salakhutdinov. Neighbourhood\n\ncomponents analysis. Advances in neural information processing systems, 17, 2004.\n\nSamuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. Advances\n\nin neural information processing systems, 32, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMichael Gutmann and Aapo Hyv ̈arinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 297–304. JMLR Workshop and Conference Proceedings, 2010.\n\nSeungwoong Ha and Hawoong Jeong. Discovering conservation laws from trajectories via machine\n\nlearning. arXiv preprint arXiv:2102.04008, 2021.\n\nBen Harwood, Vijay Kumar BG, Gustavo Carneiro, Ian Reid, and Tom Drummond. Smart mining In Proceedings of the IEEE International Conference on Computer\n\nfor deep metric learning. Vision, pp. 2821–2829, 2017.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on\n\nunsupervised visual representation learning. computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nChih-Hui Ho and Nuno Nvasconcelos.\n\nContrastive learning with adversarial examples. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 17081–17093. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ c68c9c8258ea7d85472dd6fd0015f047-Paper.pdf.\n\nKarel J Keesman and Karel J Keesman. System identification: an introduction, volume 2. Springer,\n\n2011.\n\nChing-Yun Ko, Jeet Mohapatra, Sijia Liu, Pin-Yu Chen, Luca Daniel, and Lily Weng. Revisiting contrastive learning through the lens of neighborhood component analysis: an integrated framework. In International Conference on Machine Learning, pp. 11387–11412. PMLR, 2022.\n\nJ Zico Kolter and Gaurav Manek. Learning stable deep dynamics models. Advances in neural\n\ninformation processing systems, 32, 2019.\n\nKookjin Lee and Kevin T Carlberg. Deep conservation: a latent-dynamics model for exact satIn Proceedings of the AAAI Conference on Artificial\n\nisfaction of physical conservation laws. Intelligence, volume 35, pp. 277–285, 2021.\n\nZiming Liu and Max Tegmark. Machine learning conservation laws from trajectories. Physical\n\nReview Letters, 126(18):180604, 2021.\n\nZiming Liu and Max Tegmark. Machine learning hidden symmetries. Physical Review Letters, 128\n\n(18):180201, 2022.\n\nZiming Liu, Varun Madhavan, and Max Tegmark. Ai poincar ́e 2.0: Machine learning conservation\n\nlaws from differential equations. arXiv preprint arXiv:2203.12610, 2022.\n\nLennart Ljung. System identification. In Signal analysis and prediction, pp. 163–173. Springer,\n\n1998.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\nYoh-ichi Mototake. Interpretable conservation law estimation by deriving the symmetries of dynam-\n\nics from trained deep neural networks. Physical Review E, 103(3):033303, 2021.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv preprint arXiv:1807.03748, 2018a.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv preprint arXiv:1807.03748, 2018b.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSpencer M Richards, Felix Berkenkamp, and Andreas Krause. The lyapunov neural network: Adaptive stability certification for safe learning of dynamical systems. In Conference on Robot Learning, pp. 466–476. PMLR, 2018.\n\nAndrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint arXiv:1807.05960, 2018.\n\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Metalearning with memory-augmented neural networks. In International conference on machine learning, pp. 1842–1850. PMLR, 2016.\n\nSumeet Singh, Spencer M Richards, Vikas Sindhwani, Jean-Jacques E Slotine, and Marco Pavone. Learning stabilizable nonlinear dynamics with contraction-based regularization. The International Journal of Robotics Research, 40(10-11):1123–1150, 2021.\n\nKihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. Advances in\n\nneural information processing systems, 29, 2016.\n\nNaoya Takeishi and Yoshinobu Kawahara. Learning dynamics models with stable invariant sets. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 9782–9790, 2021.\n\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding.\n\nIn European\n\nconference on computer vision, pp. 776–794. Springer, 2020.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nRui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physicsinformed deep learning for turbulent flow prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1457–1466, 2020.\n\nKarl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of\n\nBig data, 3(1):1–40, 2016.\n\nJin-Long Wu, Karthik Kashinath, Adrian Albert, Dragos Chirila, Heng Xiao, et al. Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems. Journal of Computational Physics, 406:109209, 2020a.\n\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via nonparametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3733–3742, 2018.\n\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. Clear: Contrastive\n\nlearning for sentence representation. arXiv preprint arXiv:2012.15466, 2020b.\n\nGilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding\n\nneural networks. Advances in Neural Information Processing Systems, 32, 2019.\n\nPengfei Zhang, Huitao Shen, and Hui Zhai. Machine learning topological invariants with neural\n\nnetworks. Physical review letters, 120(6):066401, 2018.\n\n12",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors propose a method to impose an acquired conservation law on a learned time-evolving model such as a DNN.\nThe proposed method consists of a two-step approach. The first step is invariant learning, in which underlying conservation laws are estimated from trajectory data, and the second step is dynamic learning, in which the time evolution model is subjected to preservation of conservation laws. Numerical experiments show that the proposed method consistently outperforms the baseline method in both coordinate error and conservation measures and can be further extended to complex, large-scale dynamics by leveraging autoencoders. The proposed method may also be useful in discovering conservation laws in unknown dynamical systems.\n\n# Strength And Weaknesses\n\nStrengths\nThe framework of estimating hidden conservation laws from data and then improving simulation accuracy based on the estimated conservation laws is highly novel.\n\nWeaknesses\n\nA weakness of the research is that the effectiveness of the proposed method has not been verified because there is no theoretical analysis to guarantee the effectiveness of the method and the numerical experiments are limited to simple systems for which the conservation laws are known.\nConcrete weaknesses are that the following two points have not been validated.\n\n1. the effectiveness of the proposed method for learning the energy function\n\nThe denominator of the \"invariant learning\" loss function (Eq. (3)) seems to be based on the good properties of the energy terrain.\nFor example, it is questionable whether it would work effectively in the case of a complex energy function with multiple peaks, such that starting from different initial conditions would result in the same energy value.\nIn addition, the behavior of the model when the number of data is small relative to the degrees of freedom the system has is unclear.\nNumerical experiments with more complex energy landscapes and a discussion of the relationship between the number of data and learning performance would be needed. \n\n2. effectiveness of the proposed method as a method for estimating unknown conservation laws\nThe authors mention the application of the proposed method to the estimation of unknown conservation laws, but its effectiveness is questionable in terms of interpretability and feasibility of estimating complex conservation laws.\n\n2-1. Interpretability\nAll of the previous studies cited in section \"2.3 LEARNING WITH CONSERVED PROPERTIES\" seem to achieve interpretable conservation law estimation.\nOn the other hand, the proposed method does not seem to be able to achieve interpretable conservation law estimation.\nMethods for estimating interpretable conservation laws from DNNs trained on dynamical system data have been proposed [1,2].\nI think that an additional discussion based on such previous studies seems necessary.\n\n2-2. Possibility of Estimating Complex Conservation Laws\nThe authors claim that the proposed method can estimate complex conservation laws.\nAs an example, they demonstrate the estimation of an angular momentum conservation law for the Kepler system.\nHowever, this task is not so difficult.\nConservation law estimation for the Kepler system has been realized in many studies [1,2].\nIn order to claim the effectiveness of the proposed method as a conservation law estimation method, it is necessary to at least realize conservation law estimation corresponding to symmetries for nonlinear transformations such as Runge-Lenz vectors.\nNon-linear conservation estimation is also mentioned in [1][2]. \nAlso, [3] achieves it using Hamiltonian Neural Networks by estimating the mass tensor.\n\n[1]Ziming Liu and Max Tegmark, \"Machine Learning Hidden Symmetries,\" PRL, 128, 180201, 2022.\n\n[2]Yoh-ichi Mototake, \"Interpretable conservation law estimation by deriving the symmetries of dynamics from trained deep neural networks,\" PRE, 103, 033303, 2021.\n\n[3]Nate Gruver, Marc Finzi, Samuel Stanton, Andrew Gordon Wilson, \"Deconstructing the indirect biases of hamiltonian neural networks,\" ICLR 2022.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nquality:\n\nThe manuscript was of high quality with no typographical errors or omissions.\n\nclarity:\n\nThe paper was clear except for the following two points\nThe meaning of x∼C∼D in equation (3) was unclear.\nIn section 4.3, I did not understand why the reduced space was created.\nIf we bring all possible initial states, does the system still have 9 dimensions of freedom?\n\noriginality:\n\nOriginality is considered to be high enough to be published.\n\nreproducibility:\n\nPublication of the source code for the numerical experiments is desired.\n\n# Summary Of The Review\n\nThe approach of estimating hidden conservation laws from data and improving the accuracy of simulations based on these conservation laws is an innovative research, and the novelty of this research fully satisfies the conditions for acceptance for presentation at the conference.\nOn the other hand, the evaluation of the effectiveness of the proposed method is weak, and the validity of the study is questionable.\nI believe that additional discussion and numerical experiments to improve these points would be a condition for the conference to accept the study.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nADVERSARY-AWARE PARTIAL LABEL LEARNING WITH LABEL DISTILLATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nTo ensure that the data collected from human subjects is entrusted with a secret, rival labels are introduced to conceal the information provided by the participants on purpose. The corresponding learning task can be formulated as a noisy partiallabel learning problem. However, conventional partial-label learning (PLL) methods are still vulnerable to the high ratio of noisy partial labels, especially in a large labelling space. To learn a more robust model, we present Adversary-Aware Partial Label Learning and introduce the rival, a set of noisy labels, to the collection of candidate labels for each instance. By introducing the rival label, the predictive distribution of PLL is factorised such that a handy predictive label is achieved with less uncertainty coming from the transition matrix, assuming the rival generation process is known. Nonetheless, the predictive accuracy is still insufficient to produce an sufficiently accurate positive sample set to leverage the clustering effect of the contrastive loss function. Moreover, the inclusion of rivals also brings an inconsistency issue for the classifier and risk function due to the intractability of the transition matrix. Consequently, an adversarial teacher within momentum (ATM) disambiguation algorithm is proposed to cope with the situation, allowing us to obtain a provably consistent classifier and risk function. In addition, our method has shown high resiliency to the choice of the label noise transition matrix. Extensive experiments demonstrate that our method achieves promising results on the CIFAR10, CIFAR100 and CUB200 datasets.\n\n1\n\nINTRODUCTION\n\nDeep learning algorithms depend heavily on a large-scale, true annotated training dataset. Nonetheless, the costs of accurately annotating a large volume of true labels to the instances are exorbitant, not to mention the time invested in the labelling procedures. As a result, weakly supervised labels such as partial labels that substitute true labels for learning have proliferated and gained massive popularity in recent years. Partial-label learning (PLL) is a special weakly-supervised learning problem associated with a set of candidate labels ⃗Y for each instance, in which only one true latent label y is in existence. Nonetheless, without an appropriately designed learning algorithm, the limitations of the partial label are evident since deep neural networks are still vulnerable to the ambiguous issue rooted in the partial label problem because of noisy labels Zhou (2018); Patrini et al. (2017); Han et al. (2018). As a result, there have had many partial label learning works (PLL)Cour et al. (2011); H ̈ullermeier & Beringer (2006); Feng & An (2019); Feng et al. (2020) successfully solved the ambiguity problem where there is a set of candidate labels for each instance, and only a true label exists. Apart from the general partial label, we have also seen a variety of partial label generations evolved, simulating different real-life scenarios. The independently and uniformly drawing is the one have seen the most Lv et al. (2020); Feng & An (2019). The other problem settings include the instance dependent partial label learning, where each partial label set is generated depending on the instance as well as the true label Xu et al. (2021). Furthermore, Lv et al. (2020) has introduced label specific partial label learning, where the uniform flipping probability of similar instances differs from dissimilar group instances. Overall, the learning objective of the previous works is all about disambiguation. More specifically, the goal is to design a classifier training with partial labels, aiming to correctly label the testing dataset, hoping the classification performance will be as close as the full supervised learning.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nOn the contrary, there is a lack of discussion on previous works that shed light on the data privacyenhancing techniques in general partial label learning. The privacy risk is inescapable; thus, privacypreserving techniques need to be urgently addressed. Recently, we have seen surging data breach cases worldwide. These potential risks posed by the attacker are often overlooked and pose a detrimental threat to society. For instance, it is most likely for the adversary to learn from stolen or leaked partially labelled data for illegal conduct using the previous proposed partial-label learning methods. Subsequently, it has become an inherent privacy concerns in conventional partial label learning. In this paper, the Adversary-Aware partial label learning is proposed to address and mitigate the ramification of the data breach. In a nutshell, we propose an affordable and practical approach to manually corrupt the collected dataset to prevent the adversary from obtaining high-quality, confidential information meanwhile ensure the trustee has full access to the useful information. However, we have observed that adversary-aware partial label learning possesses some intrinsic learnability issues. Firstly, the intractability is raised from the transition matrix. Secondly, the classifier and risk inconsistency problem has been raised. Hence, we propose an the Adversarial teacher within momentum (ATM)(In section 2.1), adversary-aware loss function equation 19, and a new ambiguity condition equation 1 to counter the issues.\n\nUnder the adversary-aware partial label problem setting, the rival is added to a candidate set of labels. To achieve that, we extend the original partial label generation equation 2 by factorisation to add the rival Y ′. Subsequently, we have the adversary-aware partial label generation established as equation 3. Then, we decompose the second equation of equation 3 into the rival embedded intractable transition matrix term Q∗ and class instance-dependent transition matrix Ty,y′, which is P(Y ′ = y′ | Y = y, X = x). In our problem setting, ̄Ty,y′, the class instance-independent transition matrix is utilised, which is defined as P(Y ′ = y′ | Y = y), with the assumption the rival is generated depending only on Y but instance X. Under the assumption, the class instanceindependent transition matrix is simplified and mathematically identifiable. Since all the instances share the same class instance-independent transition matrix in practice, such encryption is more affordable to implement. The rival variable serves as controllable randomness to enhance privacy against the potential adversary and information leakage. In contrast, the previous methods can not guarantee the privacy protection property.\n\nHowever, a fundamental problem has been raised, inclusion of the rival implies an inconsistent classifier according to the adversary-aware label generation equation equation 3. Learning a consistent partial label classifier is vital, but in our problem setting, the consistency classifier may not be obtained due to the intractability of Q∗(details are described in section 1.2). As a consequence, the Adversarial teacher within momentum (ATM) is proposed, which is designed to identify the term P(⃗Y | Y, Y ′, X) which is denoted as Q∗. The Moco-style dictionary technique He et al. (2020) and Wang et al. (2022) have inspired us to explore exploiting the the soft label from instance embedding, leveraging ̄Ty,y′ to identify or reduce the uncertainty of the Q∗ due to the property of informational preservation and tractability. Therefore, a consistent partial label learner is obtained if the uncertainty raised from the transition matrix is reduced greatly. Specifically, we transform the inference of label generation in Adversary-Aware PLL as an approximation for the transition matrix Q∗. Ultimately, a tractable solution to the unbiased estimate of P(⃗Y | Y, Y ′, X) can be derived. Lastly, we have rigorously proven that a consistent Adversary-Aware PLL classifier can be obtained if P(⃗Y | Y, Y ′, X) and P(Y ′ | Y ) are approximated accurately according to equation 3.\n\nIn this work, we are mainly focusing on identifying the transition matrix term P(⃗Y | Y, Y ′, X). The rival is generated manually for privacy enhancement. Thus the P(Y ′ | Y ) is given by design. Overall, our proposed method has not only solved the ambiguity problem in Adversary-Aware PLL but also addressed the potential risks from the data breach by using a rival as the encryption. Our proposed label generation bears some resemblance to local differential privacy Kairouz et al. (2014); Warner (1965), which aims to randomise the responses. The potential application is to randomise survey responses, a survey technique for improving the reliability of responses to confidential interviews or private questions. Depending on the sophistication of the adversary, our method offers a dynamic mechanism for privacy encryption that is more resilient and flexible to face the potential adversary or privacy risk. By learning from the previous attacks, we can design different levels of protection by adjusting the ̄T term. The main contributions of the work are summarized:\n\n• We propose a novel problem setting named adversary-aware partial label learning.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n• We propose a novel Adversary-Aware loss function and the Adversarial teacher within momentum (ATM) disambiguation algorithm. Our proposed paradigm and loss function can be applied universally to other related partial label learning methods to enhance the privacy protection.\n\n• A new ambiguity condition (equation 1) for Adversary-Aware Partial Label Learning is derived. Theoretically, we proven that the method is a Classifier-Consistent Risk Estimator.\n\n1.1 RELATED WORK\n\nPartial Label Learning (PLL) trains an instance associated with a candidate set of labels in which the true label is included. Many frameworks are designed and proposed to solve the label ambiguity issue in partial label learning. The probabilistic graphical model-based methodsZhang et al. (2016); Wang & Isola (2020); Xu et al. (2019); Lyu et al. (2019) as well as the clustering-based or unsupervised approaches Liu & Dietterich (2012) are proposed by leveraging the graph structure and prior information of feature space to do the label disambiguation. The average-based perspective methods H ̈ullermeier & Beringer (2006); Cour et al. (2011); Zhang et al. (2016) are designed based on the assumption of uniform treatment of all candidates; however, it is vulnerable to the false positive label, leading to misled prediction. Identification perspective-based methods Jin & Ghahramani (2002) tackle disambiguation by treating the true label as a latent variable. The representative perspective approach uses the maximum margin method Nguyen & Caruana (2008); Wang et al. (2020; 2022) to do the label disambiguation. Most recently, self-training perspective methodsFeng & An (2019); Wen et al. (2021); Feng et al. (2020) have emerged and shown promising performance. In Contrastive Learning He et al. (2020); Oord et al. (2018), the augmented input is applied to learns from feature of the unlabeled sample data. The learning objective is to differentiate the similar and dissimilar parts of the input, in turn, maximise the learning of the high-quality representations. CL has been studied in unsupervised representation fashion Chen et al. (2020); He et al. (2020), which treats the same classes as the positive set to boost the performance. The weakly supervised learning has also borrowed the concepts of CL to tackle the partial label problem Wang et al. (2022). The CL has also been applied to semi-supervised learning Li et al. (2020).\n\n1.2 ADVERSARY-AWARE PARTIAL LABEL PROBLEM SETTING\n\nGiven the input space X ∈ Rd and label space is defined as Y = [c] ∈ {1 · · · c} with the number of c > 2 classes. Under adversary-aware partial labels, each instance X ∈ X has a candidate set of adversary-aware partial labels ⃗Y ∈ ⃗Y. The adversary-aware partial label set has space of ⃗Y := {⃗y | ⃗y ⊂ Y}=2[c], in which there is total 2[c] selection of subsets in [c]. The objective is to learn a classifier with the adversary-aware partially labelled sample n, which was i.i.d drawn from the ⃗D = {(X1, ⃗Y1), . . . , (Xn, ⃗Yn)}, aiming that it is able to assign the true labels for the testing dataset. Given instance and the adversary-aware partial label ⃗Y the adversary-aware partial label dataset distribution ⃗D is defined as (X, ⃗Y ) ∈ X × ⃗Y. The class instance-independent transition matrix P (Y ′ | Y ) is denoted as ̄T ∈ Rc×c. ̄Ty,y′ = P (Y ′ = y′ | Y = y) where ̄Ty,y = 0, ∀y′, y ∈ [c]. The adversary-aware means the designed paradigm can prevent the adversary from efficiently and reliably inferring certain information from the database without the ̄T , even if the data was leaked. The rival is the controllable randomness added to the partial label set to enhance privacy.\n\n1.2.1 ASSERTION CONDITIONS IN LABEL GENERATION SET\n\nThe following conditions describe the learning condition for adversary-aware partial label. According to Cour et al. (2011) there needs to be certain degrees of ambiguity for the partial label learning. Lemma 1 is the new ERM learnability condition which is proposed as follows\n\nPy′, ̄y := P(y′, ̄y ∈ ⃗Y | Y ′ = y′, ̄Y = ̄y, X = x). (1) The y′ is the rival, and ̄y is the false positive label that exists in the partial label set. It has to be met to ensure the Adversary-Aware PLL problem is learnable with y′ ̸= y and ̄y ̸= y, these conditions ensure the ERM learnability Liu & Dietterich (2014) of the adversary-aware PLL problem if there is small ambiguity degree condition. In our case which is that, Py′, ̄y < 1. The y is the true label corresponding to each instance x. And Py:= P(y ∈ ⃗Y | Y = y, X = x), where Py = 1 to ensure that the ground truth label is in the partial label set with respect to each instance.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n1.2.2 LABEL GENERATION\n\nIn the previous works of partial label generation procedure, only a candidate of the partial label was generated as such. The Standard Partial Label Generation:\n\nP(⃗Y = ⃗y, Y = y | X = x) =\n\n(cid:88)\n\ny∈Y\n\n(cid:88)\n\ny∈Y\n\nP(⃗Y = ⃗y | Y = y, X = x)P(Y = y | X = x).\n\n=\n\n(cid:88)\n\ny∈Y\n\nP(⃗Y = ⃗y | Y = y)P(Y = y | X = x),\n\n(2)\n\nwhere P(⃗Y = ⃗y | Y = y, X = x) is the label generation for the class instance-dependent partial label and P(⃗Y = ⃗y | Y = y) is the standard partial label learning framework. Then we present the difference between the general partial labels and the adversary-aware partial label. The Adversary-Aware Partial Label Generation:\n\nP(⃗Y = ⃗y | X = x) =\n\n(cid:88)\n\ny∈Y\n\n(cid:88)\n\n(cid:88)\n\ny∈Y\n\ny′∈Y ′\n\nP(⃗Y = ⃗y, Y = y, Y ′ = y′ | X = x)\n\n(cid:88)\n\n(cid:88)\n\n=\n\ny∈Y\n\ny′∈Y ′\n\nP(⃗Y = ⃗y | Y = y, Y ′ = y′, X = x) (cid:125) (cid:123)(cid:122) (cid:124) Adversary-Aware transition matrix\n\n ̄Ty,y′P(Y = y | X = x).\n\n(3) In the adversary-aware partial label problem setting, the transition matrix of the adversary-aware partial label is defined as P(⃗Y | Y, Y ′, X) and denoted as Q∗ ∈ Rc×(2c−2). The partial label transition matrix P(⃗Y | Y ) is denotes as ̄Q ∈ Rc×(2c−2). Theoretically, if the true label Y of the vector ⃗Y is unknown given an instance X, where ⃗y ∈ ⃗Y and there are 2c −2 candidate label sets.The εx is the instance-dependent rival label noise for each instance where εx ∈ R1×c. The entries of the adversary-aware transition matrix for each instance is defined as follows\n\n2c−2 (cid:88)\n\nj=1\n\nQ∗[:, j] =\n\n2c−2 (cid:88)\n\n([ ̄Q[:, j]T + εx] ̄T )T =\n\n2c−2 (cid:88)\n\n(A[:, j]T ̄T )T ,\n\nj=1\n\nj=1\n\n(4)\n\nwhere A[:, j]T = ̄Q[:, j]T + εx and the conditional distribution of the adversary-aware partial label set ⃗Y based on Wen et al. (2021) is derived as belows\n\nP(⃗Y = ⃗y | Y = y, Y ′ = y′, X = x) =\n\n(cid:89)\n\n(cid:89)\n\npb′ ·\n\n(1 − pt′) ,\n\nb′∈⃗y,b′̸=y\n\nt′ /∈⃗y\n\n(5)\n\nwhere pt′ and pb′ are defined as\n\npt′ := P(t ∈ ⃗Y | Y = y, Y ′ = y′, X = x) < 1, pb′ := P(b ∈ ⃗Y | Y = y, Y ′ = y′, X = x) < 1.\n\n(6) We summarize the equation 3 as a matrix form in equation 7. The inverse problem is to identify a sparse approximation matrix A to use equation 8 to estimate the true posterior probability.\n\nP (⃗Y | X = x) (cid:125) (cid:123)(cid:122) (cid:124) Adversary-aware PLL\n\n= Q∗ P (Y | X = x) (cid:125)\n\n(cid:123)(cid:122) True posterior probability\n\n(cid:124)\n\nQ∗−1 P (⃗Y | X = x) (cid:125) (cid:123)(cid:122) (cid:124) Adversary-aware PLL\n\n= P (Y | X = x) (cid:125)\n\n(cid:123)(cid:122) True posterior probability\n\n(cid:124)\n\n,\n\n,\n\n ̄T −1A−1 P (⃗Y | X = x) (cid:124) (cid:125) (cid:123)(cid:122) Adversary-aware PLL\n\n≈ P (Y | X = x) (cid:125)\n\n(cid:123)(cid:122) True posterior probability\n\n(cid:124)\n\n(7)\n\n(8)\n\n.\n\nIn reality, due to the computational complexity of the transition matrix, it would be a huge burden to estimate Q∗ accurately for each instance. The 2c − 2 is an extremely large figure and increases\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nexponentially as the label space increase. Therefore, we are no longer required to estimate the true transition matrix P(⃗Y | Y, Y ′, X). Instead, we resort to using instance embedding in the form of a soft label to identify the adversary-aware partial label transition matrix Q∗. Specifically, we proposed to use a soft pseudo label from the instance embedding (Prototype) to approximate the adversary-aware transition matrix for each instance. The reason is that we can not achieve the true transition matrix Q∗ directly due to the nature of the practical partial label problem. Therefore, we have used the self-attention prototype learning to approximate the true transition matrix. The detail is described in section 2.1. Since the Adversary-aware partial label is influenced by the rival label noise, it is challenging to accurately estimate both the class instance-independent transition matrix ̄T and the sparse matrix A simultaneously to estimate the true posterior. Considering that the ̄T is private and given, it is easier for us just to approximate A to estimate the posterior probability than the adversary. The equation 8 is implemented as the loss function in equation 17.\n\n1.3 POSITIVE SAMPLE SET\n\nThe construction of a positive sample is used for contrastive learning to identify the transition matrix P (⃗Y | Y ′, Y, X) via the label disambiguation. Nonetheless, the performance of the contrastive learning erodes drastically due to the introduced rival, which is manifest in the poorly constructed positive sample set, resulting in the degenerated classification performance (See Figure 2). Subsequently, the adversary-aware loss function is proposed in conjunction the contrastive learning to prevent classification performance degeneration. To start with, we define L2 norm embedding of u and k as the query and key latent feature from the feature extraction network fΘ and key neural netΘ respectively. Correspondingly, we have the output u ∈ R1×d where ui = fΘ(Augq(x)) work f ′ and z ∈ R1×d where zi=f ′ Θ(Augk(xi)). The construction of a positive sample set is shown as follows. In each mini-batch, we have ⃗Db where ⃗Db ∈ ⃗D. The f (xi) is the function of a neural network with a projection head of 128 feature dimensionality. The outputs of Dq and Dk are defined as follows,\n\nDq = {ui = f (cid:0)Augq (xi)(cid:1) | xi ∈ ⃗Db}, Dk = {zi = f ′ (Augk (xi)) | xi ∈ ⃗Db}, (10) where ̄S(x) is the sample set excluding the query set q and is defined as ̄S(x) = ̄C\\{q}, in which ̄C = Dq ∪ Dk ∪ queue . The Dq and Dk are vectorial embedding with respect to the query and key views given the current mini-batch. The queue size is determined accordingly depending on the input. The instances from the current mini-batch with the prediction label ̄y′ equal to (ˆyi = c) from the ̄S(x). is chosen to be the positive sample set. Ultimately, the N (x) is acquired, and it is denoted as\n\n(9)\n\nN+(xi) = (cid:8)z′ | z′ ∈ ̄S (xi) , ̄y′ = (ˆyi = c)(cid:9) . The N+(x) is the positive sample set. The construction of sufficiently accurate positive sample set N+(x) is vital as it underpins the clustering effect of the latent embedding in the contrastive learning procedure. The quality of the clustering effect relies on the precision of prototype vj corresponding to j ∈ {1, ..., C}. Our method helps maintain the precision of prototypes using the ̄T to render better label disambiguation module performance for contrastive learning when introduced the rival. where the query embedding u multiplies the key embedding z and then divides with the remaining pool ̄C. Overall, the S+(x) is used to facilitate the representation learning of the contrastive learning and the self-attention prototype learning to do the label disambiguation or a more accurate pseudolabelling procedure. Our proposed loss ensures the prototype and contrastive learning are working systematically and benefit mutually when the rival is introduced. The pseudo label generation is according to equation 16. We have followed Wang et al. (2022) for the positive sample selection.\n\n(11)\n\n2 METHODOLOGY\n\nThe main task of partial label learning is label disambiguation, which targets identifying the true label among candidate label sets. Thus, we present an adversarial teacher within momentum (ATM). The equation 17 is developed to do the debiasing from the prediction of f (x) given the adversaryaware partial label via the class instance dependent transition matrix ̄T + I. The unbiased prediction induces the identification of a more accurate positive sample set which allows Equation 18 to\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: An overview of the proposed method. General partial label can be disclosed to adversary. The initial training is about positive sample selection. Moreover, we have assumed ̄T is given.\n\nleverage the high-quality presentation power of a positive sample set to improve the classification performance.\n\n2.1\n\nPSEUDO LABEL LEARNERS VIA ADVERSARIAL TEACHER WITHIN MOMENTUM (ATM)\n\nUnlike Wang et al. (2022), we present an adversarial teacher strategy with momentum update (ATM) to guide the learning of pseudo labels using Equation 17. Just like a tough teacher who teaches the subject using harsh contents to test students’ understanding of the subject. In our case, the rival is like the subject which is purposely generated by us, at the same time Equation 17 is introduced to check the understanding of the student (classifier) given the scope of testing content which is the ̄T . Specifically, the spherically margin between prototype vector vi ∈ Sd−1 and prototype vector vj ∈ Sd−1 is defined as\n\nFor prototype vi, we define the normalized margin between vi and vj as\n\nmij = exp (−v⊤\n\ni vj).\n\n ̄mij =\n\nexp (−v⊤ i vj) j̸=i exp (−v⊤\n\ni vj)\n\n(cid:80)\n\n.\n\n(12)\n\n(13)\n\nFor each vi, i ∈ {1, · · · , K}, we perform momentum updating with the normalized margin between vj and vi for all j ̸= i as an regularization. The resulted new update rule is given as\n\nwhere the gradient g is given as\n\n(cid:112)\n\nvt+1\n\ni =\n\n1 − α2vt\n\ni + α\n\ng ∥g∥2\n\n,\n\ng = u − β\n\n ̄mt\n\nijvt j,\n\n(cid:88)\n\nj̸=i\n\n(14)\n\n(15)\n\nwhere u is the query embedding whose prediction is class i, ̄mt prototype vectors at step t (i.e., vt\n\nj, j ̸= i). The vc is the prototype corresponding to each class.\n\nij is the normalized margin between\n\n ̄q = φ ̄q + (1 − φ)v,\n\nvc =\n\n(cid:26)1 0\n\nif c = arg maxj∈Y u⊤v otherwise ,\n\n.\n\n(16)\n\nwhere ̄q is the target prediction and subsequently used in the equation 17. It was initialised as the uniform probability ̄q = 1 |c| 1 and updated accordingly to the equation 16. The φ is the hyperparameter controlling for the updating of ̄q.\n\n2.2 ADVERSARY AWARE LOSS FUNCTION.\n\nThe goal is to build a risk consistent loss function, hoping it can achieve the same generalization error as the supervised classification risk R(f ) with the same classifier f . To train the classifier,\n\n6\n\nAdversaryAware PartialLabel Trainingsample SoftMax Neural NetworkAdversary-AwareLoss FunctionDisambiguatedLabelsAdversaryAwarePartialLabelAdversaryAware PartialLabel TrainingsampleMomentumEncoderMLPMLPSelf-Attention PrototypesContrastive LossUpdatingApproximatingStop GradientLower Embedding Augmented Key ViewPositive Sample CandidatesAugmented Query ViewDisambiguated LabelsqueryUnder review as a conference paper at ICLR 2023\n\nwe minimize the following modified loss function estimator by leveraging the updated pseudo label from the Adversarial teacher within momentum (ATM) distillation method and transition plus identity matrix, Ii,j ∈ [0, 1]c×c, Ii,i = 1, for ∀i=j ∈ [c], Ii,j = 0, for ∀i̸=j ∈ [c]: where f (X) ∈ R|c|,\n\n⃗L(f (X), ⃗Y ) = −\n\nc (cid:88)\n\n( ̄qi) log (cid:0)(( ̄T + I)f (X))i\n\n(cid:1) .\n\ni=1\n\n(17) The proof for the modified loss function is shown in the appendix lemma 4. In our case, given sufficiently accurate positive sample set of the contrastive learning is utilised to incorporate with equation 17 to identify the transition matrix of the adversary-aware partial label. The contrastive loss is defined as follows\n\nL(f (x), τ, C) =\n\n1 |Dq|\n\n(cid:88)\n\n{−\n\nu∈Dq\n\n1 N+(x)\n\n(cid:88)\n\nlog\n\nz+∈N+(x)\n\nexp(u⊤z/τ )\n\n(cid:80)\n\nz′∈ ̄C(x) exp(u⊤z/τ )\n\n}.\n\n(18)\n\nFinally, we have the Adversary-Aware Loss expressed as\n\nAdversary-Aware Loss = λL(f (xi), τ, C) + ⃗L(f (X), ⃗Y ). There are two terms of the proposed loss function (equation 19), which are the equation 17 and equation 18 correspondingly. equation 17 is developed to lessen prediction errors from f (x) given the adversary-aware partial label. The debiasing is achieved via the class instance dependent transition matrix ̄T + I by down-weighting the false prediction. The unbiased prediction induces the identification of a more accurate positive sample set. equation 18 is the contrastive loss. It leverages the high-quality representation power of positive sample set to improve the classification performance further.\n\n(19)\n\n3 THEORETICAL ANALYSIS\n\nThe section introduces the concepts of classifier consistency and risk consistency Xia et al. (2019) Zhang (2004), which are crucial in weakly supervised learning. Risk consistency is achieved if the risk function of weak supervised learning is the same as the risk of fully supervised learning with the same hypothesis. The risk consistency implies classifier consistency, meaning classifier trained with partial labels is consistent as the optimal classifier of the fully supervised learning.\n\nLets denote f (X) = Classifier-Consistent Risk Estimator Learning with True labels. (g1(x), . . . , gK(x)) as the classifier, in which gc(x) is the classifier for label c ∈ [K]. The prediction of the classifier fc(x) is P (Y = c | x). We want to obtain a classifier f (X) =arg maxi∈[K] gi(x). The loss function is to measure the loss given classifier f (X). To this end, the true risk can be denoted as\n\nR(f ) = E(X,Y )[L (f (X) , Y )]. (20) The ultimate goal is to learn the optimal classifier f ∗=arg minf ∈F R(f ) for all loss functions, for instance to enable the empirical risk ̄Rpn(f ) to be converged to true risk R(h). To obtain the optimal classifier, we need to prove that the modified loss function is risk consistent as if it can converge to the true loss function.\n\nLearning with adversary-aware Partial Label. An input X ∈ X has a candidate set of ⃗Y ∈ ⃗Y but a only true label Y ∈ ⃗Y. Given the adversary-aware partial label ⃗Y ∈ ⃗Y and instance X ∈ X that the objective of the loss function is denoted as\n\nˆR(f ) = E\n\n(X,⃗Y )\n\n(cid:16)\n\n⃗L\n\nf (X) , ⃗Y\n\n(cid:17)\n\n.\n\n(21)\n\nSince the true adversary-aware partial label distribution ̄D is unknown, our goal is approximate the optimal classifier with sample distribution ̄Dpn by minimising the empirical risk function, namely\n\n⃗L (f (xi) , ⃗yi) .\n\n(22)\n\nˆRpn(f ) =\n\n1 n\n\nn (cid:88)\n\ni=1\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nAssumption 1. According to Yu et al. (2018) that the minimization of the expected risk R(f ) given clean true population implies that the optimal classifier is able to do the mapping of f ∗ i (X) = P (Y = i | X), ∀i ∈ [c]. Under the assumption 1, we are able to draw conclusion that ˆf ∗ = f ∗ applying the theorem 2 in the following.\n\nTheorem 1. Assume that the Adversary-Aware matrix Ty,y′ is fully ranked and the Assumption 1 is met, the the minimizer of ˆf ∗ of ˆR(f ) will be converged to f ∗ of R(f ), meaning ˆf ∗ = f ∗.\n\nRemark. If the Q∗ and Ty,y′ is estimated correctly the empirical risk of the designed algorithm trained with adversary-aware partial label will converge to the expected risk of the optimal classifier trained with the true label. If the number of sample is reaching infinitely large that given the adversary-aware partial labels, ˆfn is going to converged to ˆf ∗ theoretically. Subsequently, ˆfn will converge to the optimal classifier f ∗ as claimed in the theorem 1. With the new generation procedure, the loss function risk consistency theorems are introduced.\n\nTheorem 2. The adversary-aware loss function proposed is risk consistent estimator if it can asymptotically converge to the expected risk given sufficiently good approximate of ̄Q and the adversaryaware matrix.The proof is in appendix lemma 4.\n\nL(y, f (x)) =\n\n(cid:88)\n\nC (cid:88)\n\n(cid:88)\n\n⃗y∈ ⃗Yy\n\ny=1\n\ny′∈Y ′\n\n(P(Y = y | X = x) =\n\n(cid:89)\n\nb′∈⃗y\n\npb′ ·\n\n(cid:89)\n\nt′ /∈⃗y\n\n(1 − pt′) ̄Tyy′ ⃗L(⃗y, f (x))) = ⃗L(⃗y, f (x)).\n\n3.0.1 GENERALISATION ERROR\n\nDefine ˆR and ˆRpn as the true risk the empirical risk respectively given the adversary-aware partial label dataset. The empirical loss classifier is obtained as ˆfpn = arg minf ∈F ˆRpn(f ). Suppose a set of real hypothesis F⃗yk with fi(X) ∈ F, ∀i ∈ [c] . Also, assume it’s loss function ⃗L(f (X), ⃗Y ) is L-Lipschitz continuous with respect to f (X) for all ⃗yk ∈ ⃗Y and upper-bounded by M , i.e., M = ⃗L (f (x), ⃗yk). The expected Rademacher complexity of F k is denoted as Rn(F⃗yk ) supx∈X ,f ∈F ,yk∈⃗Y Bartlett & Mendelson (2002)\n\nTheorem 3. For any δ > 0, with probability at least 1 − δ,\n\n(cid:17)\n\n(cid:16) ˆfpn\n\nˆR\n\n− ˆR\n\n(cid:16) ˆf ⋆(cid:17)\n\n√\n\n≤ 4\n\n2L\n\nc (cid:88)\n\nk=1\n\nRn (F⃗yk ) + M\n\n(cid:115)\n\nlog 2 δ\n2n\n\n.\n\n(23)\n\nAs the number of samples reaches to infinity n → ∞, Rn (F⃗yk ) → 0 with a bounded norm. Subsequently, ̄R( ˆf ) → ̄R as the number of training data reach to infinitely large. The proof is given in Appendix Theorem 3.\n\n(cid:16) ˆf ⋆(cid:17)\n\n4 EXPERIMENTS\n\nDatasets We evaluate the proposed method on three benchmarks-CIFAR10, CIFAR100 Krizhevsky et al. (2009), and fine-grained CUB200 Wah et al. (2011) with general partial label and adversaryaware partial label datasets. Main Empirical Results for CIFAR10. All the classification accuracy is shown in Table 1. We have compared classification results on CIFAR-10 with previous works Wang et al. (2022); Lv et al. (2020); Wen et al. (2021) using the Adversarial teacher within momentum (ATM). The method has shown consistently superior results in all learning scenarios where q = {0.3, 0.5} for the adversary-aware partial label learning. More specifically, the proposed method achieves 8.17% superior classification performance at a 0.5 partial rate than the previous state of art work Wang et al. (2022). Moreover, our proposed method has achieved comparable results at 0.1 and 0.3 partial rates. The experiments for CIFAR-10 have been repeated four times with four random seeds. Main Empirical Results for CUB200 and CIFAR100. The proposed method has shown superior results for the Adversary-Aware Partial Label, especially in more challenging learning tasks like the 0.1 partial rate of the dataset cub200 and CIFAR100, respectively. On the cub200 dataset, we have shown 5.95% improvement at partial rates 0.1 and 1.281% and 0.37% where the partial rate is at 0.05 and 0.03. On the CIFAR100 dataset, the method has shown 6.06% and 0.4181%, 0.5414% higher classification margin at partial rate 0.1, 0.05 and 0.03.The experiments have been repeated five times with five random seeds.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Benchmark datasets for accuracy comparisons. Superior results are indicated in bold. Our proposed methods have shown comparable results to fully supervised learning and outperform previous methods in a more challenging learning scenario, such as the partial rate at 0.5(CIFAR10) and 0.1(CIFAR100, CUB200). The hyper-parameter α is set to 0.1 for our method. (The symbol ∗ indicates Adversary-Aware partial label dataset).\n\nDataset\n\nMethod\n\n(ATM)(Without T)(Our) PiCO LWS PRODEN Full Supervised\n\nq = 0.01\n\n73.43 ±0.11 73.28 ±0.24 65.78 ±0.02 62.60±0.02\n\nq = 0.05\n\n72.63±0.27 72.90 ±0.27 59.56 ±0.33 60.73±0.03 73.56 ±0.10\n\nq = 0.1\n\n72.35±0.22 71.77±0.14 53.53 ±0.08 56.80±0.29\n\nMethod\n\n(ATM)(Our)∗ PiCO∗ LWS∗ PRODEN∗\n\nq∗ = 0.03 ± 0.02\n\nq∗ = 0.05 ± 0.02\n\nq∗ = 0.1 ± 0.02\n\n73.36 ±0.32 72.87 ±0.26 46.8±0.06 59.33±0.48\n\n72.76±0.14 72.53±0.37 24.82±0.17 41.20±0.27\n\n54.09 ±1.88 48.03±3.32 4.53±0.47 13.44±0.41\n\nCIFAR100\n\nDataset\n\nCIFAR100\n\nDataset\n\nMethod\n\nq = 0.01\n\nq = 0.05\n\nq = 0.1\n\nCUB200\n\n(ATM) (Without T)(Our) PiCO LWS PRODEN Full Supervised\n\n74.43±0.876 74.11±0.37 73.74±0.23 72.34±0.04\n\n72.30±0.521 71.75±0.56 39.74±0.47 62.56±0.10 76.02±0.19\n\n66.87±0.98 66.12±0.99 12.30±0.77 35.89±0.05\n\nDataset\n\nMethod\n\nq∗ = 0.03±0.02\n\nq∗ = 0.05 ±0.02\n\nq∗ =0.1±0.02\n\nCUB200\n\n(ATM) (Our)∗ PiCO∗ LWS∗ PRODEN∗\n\n72.22±1.36 71.85 ±0.53 9.6 ±0.62 18.71±0.45\n\n72.43±0.86 71.15 ±0.41 4.02 ±0.03 17.63 ±0.89\n\n56.26±0.70 50.31 ±1.01 1.44 ±0.06 17.99 ±0.62\n\nDataset\n\nMethod\n\nq = 0.1\n\nq = 0.3\n\nq = 0.5\n\nCIFAR10\n\n(ATM)(Without T)(Our) PiCO LWS PRODEN Full Supervised\n\n93.57±0.16 93.74±0.24 90.30 ±0.60 90.24±0.32\n\n93.17±0.09 93.25±0.32 88.99 ±1.43 89.38±0.31 94.91±0.07\n\n92.22±0.40 92.46±0.38 86.16 ±0.85 87.78±0.07\n\nDataset\n\nMethod\n\nq∗ = 0.1 ± 0.02\n\nq∗ = 0.3 ± 0.02\n\nq∗ = 0.5 ± 0.02\n\nCIFAR10\n\n(ATM) (Our) ∗ PiCO∗ LWS∗ PRODEN∗\n\n93.52 ±0.11 93.64±0.24 87.34±0.87 88.80±0.14\n\n92.98±0.51 92.85±0.43 39.9±0.72 81.88±0.51\n\n89.62±0.79 81.45±0.57 9.89±0.55 20.32±3.43\n\n4.1 ABLATION STUDY\n\nFigure 2 shows the experimental result comparisons for CUB200 between the adversary-aware loss function and previous loss function before and after the momentum updating. Given equation 17, the uncertainty of the transition matrix ̄Q is reduced, leading to a good initialisation for the positive set selection, which is a warm start and plays a vital role in improving the performance of contrastive learning. After we have a good set of positive samples, the prototype’s accuracy is enhanced. Subsequently, leveraging the clustering effect and the high-quality representation power of the positive sample set of contrastive loss function to improve the classification performance.\n\nFigure 2: The Top1 and Prototype Accuracy of the Proposed Method and the Method in Wang et al. (2022) on CUB200 Adversary-Aware Loss Comparison.\n\n(a)\n\n(b)\n\n5 CONCLUSION AND FUTURE WORKS\n\nThis paper introduces a novel Adversary-Aware partial label learning problem. The new problem setting has taken local data privacy protection into account. Specifically, we have added the rival to the partial label candidate set as encryption for the dataset. Nonetheless, the generation process has made the intractable transition matrix even more complicated, leading to an inconsistency issue. Therefore, the novel adversary-aware loss function and the self-attention prototype are proposed. The method is proven to be a provable classifier and has shown superior performance. Future work will use variational inference methods to approximate the intractable transition matrix.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nPeter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and\n\nstructural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.\n\nBrian Chen, Bo Wu, Alireza Zareian, Hanwang Zhang, and Shih-Fu Chang. General partial label learning via dual bipartite graph autoencoder. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 10502–10509, 2020.\n\nTimothee Cour, Ben Sapp, and Ben Taskar. Learning from partial labels. The Journal of Machine\n\nLearning Research, 12:1501–1536, 2011.\n\nLei Feng and Bo An. Partial label learning with self-guided retraining. In Proceedings of the AAAI\n\nConference on Artificial Intelligence, volume 33, pp. 3542–3549, 2019.\n\nLei Feng, Jiaqi Lv, Bo Han, Miao Xu, Gang Niu, Xin Geng, Bo An, and Masashi Sugiyama. Provably consistent partial-label learning. Advances in Neural Information Processing Systems, 33: 10948–10960, 2020.\n\nBo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. Advances in neural information processing systems, 31, 2018.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on\n\nunsupervised visual representation learning. computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nEyke H ̈ullermeier and J ̈urgen Beringer. Learning from ambiguously labeled examples. Intelligent\n\nData Analysis, 10(5):419–439, 2006.\n\nRong Jin and Zoubin Ghahramani. Learning with multiple labels. Advances in neural information\n\nprocessing systems, 15, 2002.\n\nPeter Kairouz, Sewoong Oh, and Pramod Viswanath. Extremal mechanisms for local differential\n\nprivacy. Advances in neural information processing systems, 27, 2014.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nJunnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. Prototypical contrastive learning of\n\nunsupervised representations. arXiv preprint arXiv:2005.04966, 2020.\n\nLiping Liu and Thomas Dietterich. A conditional multinomial mixture model for superset label\n\nlearning. Advances in neural information processing systems, 25, 2012.\n\nLiping Liu and Thomas Dietterich. Learnability of the superset label learning problem. In Interna-\n\ntional Conference on Machine Learning, pp. 1629–1637. PMLR, 2014.\n\nJiaqi Lv, Miao Xu, Lei Feng, Gang Niu, Xin Geng, and Masashi Sugiyama. Progressive identification of true labels for partial-label learning. In Hal Daum ́e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 6500–6510. PMLR, 13–18 Jul 2020.\n\nGengyu Lyu, Songhe Feng, Tao Wang, Congyan Lang, and Yidong Li. Gm-pll: graph matching IEEE Transactions on Knowledge and Data Engineering, 33(2):\n\nbased partial label learning. 521–535, 2019.\n\nNam Nguyen and Rich Caruana. Classification with partial labels. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 551–559, 2008.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nGiorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1944–1952, 2017.\n\nC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-ucsd birds-200-2011. Techni-\n\ncal report, 2011.\n\nHaobo Wang, Yuzhou Qiang, Chen Chen, Weiwei Liu, Tianlei Hu, Zhao Li, and Gang Chen. Online partial label learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 455–470. Springer, 2020.\n\nHaobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, and Junbo Zhao. Pico:\n\nContrastive label disambiguation for partial label learning. ICLR, 2022.\n\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pp. 9929–9939. PMLR, 2020.\n\nStanley L Warner. Randomized response: A survey technique for eliminating evasive answer bias.\n\nJournal of the American Statistical Association, 60(309):63–69, 1965.\n\nHongwei Wen, Jingyi Cui, Hanyuan Hang, Jiabin Liu, Yisen Wang, and Zhouchen Lin. Leveraged weighted loss for partial label learning. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 11091–11100. PMLR, 18–24 Jul 2021.\n\nXiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? Advances in Neural Information Processing Systems, 32:6838–6849, 2019.\n\nNing Xu, Jiaqi Lv, and Xin Geng. Partial label learning via label enhancement. In Proceedings of\n\nthe AAAI Conference on Artificial Intelligence, volume 33, pp. 5557–5564, 2019.\n\nNing Xu, Congyu Qiao, Xin Geng, and Min-Ling Zhang. Instance-dependent partial label learning.\n\nAdvances in Neural Information Processing Systems, 34, 2021.\n\nXiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary\n\nlabels. In ECCV, pp. 68–83, 2018.\n\nMin-Ling Zhang, Bin-Bin Zhou, and Xu-Ying Liu. Partial label learning via feature-aware disambiguation. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1335–1344, 2016.\n\nTong Zhang. Statistical analysis of some multi-category large margin classification methods. Journal\n\nof Machine Learning Research, 5(Oct):1225–1251, 2004.\n\nZhi-Hua Zhou. A brief introduction to weakly supervised learning. National science review, 5(1):\n\n44–53, 2018.\n\n11",
    "reference": "# Summary Of The Paper\n\nThis paper introduces a the adversary-aware partial label learning problem to protect the data privacy. The novel adversary-aware loss function, together with an immature teacher within momentum disambiguation algorithm, has achieved state of-the-art performance and proven to be a provable classifier.\n\n# Strength And Weaknesses\n\nPros:\n* This paper proposes to take the data privacy into account in partial label learning, which is novel and significant in practice.\n\n* The proposed method is smart and easy to follow, referring the idea of self-supervised learning. \n\nCons:\n* The theoretical analysis is based on the fully rank transition matrix. I have a concern about the existence of this condition especially when the noise is very regular.\n\n* I think the rival is essentially a noisy label for PLL. Thus, incorporating the label noise learning methods into the existing PLL methods will be an important baseline.\n\n* Some typos. For example, in the caption of Figure 2, \"the Method inWang et al. (2022)\".\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity, quality and originality is fine.\n\n# Summary Of The Review\n\nPlease refer to the Strength And Weaknesses.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTOPOZERO: DIGGING INTO TOPOLOGY ALIGNMENT ON ZERO-SHOT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nCommon space learning, associating semantic and visual domains in a common latent space, is essential to transfer knowledge from seen classes to unseen ones on Zero-Shot Learning (ZSL) realm. Existing methods for common space learning rely heavily on structure alignment due to the heterogeneous nature between semantic and visual domains, but the existing design is sub-optimal. In this paper, we utilize persistent homology to investigate geometry structure alignment, and observe two following issues: (i) The sampled mini-batch data points present a distinct structure gap compared to global data points, thus the learned structure alignment space inevitably neglects abundant and accurate global structure information. (ii) The latent visual and semantic space fail to preserve multiple dimensional geometry structure, especially high dimensional structure information. To address the first issue, we propose a Topology-guided Sampling Strategy (TGSS) to mitigate the gap between sampled and global data points. Both theoretical analyses and empirical results guarantee the effectiveness of the TGSS. To solve the second issue, we introduce a Topology Alignment Module (TAM) to preserve multi-dimensional geometry structure in latent visual and semantic space, respectively. The proposed method is dubbed TopoZero. Empirically, our TopoZero achieves superior performance on three authoritative ZSL benchmark datasets.\n\n1\n\nINTRODUCTION\n\nGiven a large amount of training data, deep learning has exhibited excellent performance on various vision tasks, e.g., image recognition He et al. (2016); Dosovitskiy et al. (2020), object detection Lin et al. (2017); Liu et al. (2021), and instance segmentation He et al. (2017); Bolya et al. (2019). However, when considering a more realistic situation, e.g., the testing class does not appear at the training stage, the deep learning model fails to give a prediction on these novel classes. To remedy this, some pioneering researchers Lampert et al. (2014); Mikolov et al. (2013) point out that the auxiliary semantic information (sentence embeddings and attribute vectors) is available for both seen and unseen classes. Thus, by employing this common semantic representation, Zero-Shot Learning (ZSL) was proposed to transfer knowledge from seen classes to unseen ones.\n\nCommon space learning, enabling a significant alignment between semantic and visual information on the common embedding space, is a mainstream algorithm for ZSL. Existing approaches for common space learning can be divided into two categories: algorithms with 1) distribution alignment and 2) structure and distribution alignment. Typical methods in the first category employ various encoding networks to directly align the distribution between visual and semantic domains, e.g., variational autoencoder in Sch ̈onfeld et al. (2019), bidirectional latent embedding framework in Wang & Chen (2017), and deep visual-semantic embedding network in Tsai et al. (2017). Even though these methods encourage distribution alignment between visual and semantic domains, the alignment on the geometry structure is usually neglected. Note that the structure gap naturally exists in these two domains due to their heterogeneous nature Chen et al. (2021c). To mitigate the structure gap for promoting alignment between visual and semantic domains, HSVA Chen et al. (2021c) was proposed and become a pioneering work in the second category. Inspired by the successful structure alignment work Lee et al. (2019) in unsupervised domain adaptation, HSVA introduces a novel hierarchical semantic-visual adaptation framework to align the structure and distribution progressively.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Motivation Illustration. (a)-(c) Based on the same random sampled data points X (m) in (a), the sampled batch data points from our Topological-guided Sampling Strategy (TGSS) are closer to the global data points compared to those sampled from random sampling strategy (DH (X, X (m+1) )). Combining this illustrator example with our theoretical analysis guarantees that our TGSS can mitigate the structure gap between mini-batch and global data points. (d)-(f) Compared to the input space, HSVA latent space can only preserve 0-dimensional topological features, indicating some high dimensional structure representation is lost during the dimension reduction phase. In contrast, our TopoZero latent space can preserve more accurate topological features by taking advantage of our proposed Topology Alignment Module.\n\n) < DH (X, X (m+1)\n\nR\n\nT\n\nAlthough HSVA empirically works well, we discover that there exist two issues in HSVA’s structure alignment module. To clarify our findings clearly, we first introduce some background information in terms of Persistent Homology Zomorodian & Carlsson (2005). Persistent homology is a tool for computing topological features1 of a data set at different spatial resolutions. More persistent features can be found over a wide range of spatial scales and represent true features of the underlying geometry space. We first introduce the concept of simplicial homology. For a simplicial complex R, i.e. a generalised graph with higher-order connectivity information such as cliques, simplicial homology employs matrix reduction algorithms to assign R a family of groups, namely homology groups. The d-th homology group Hd(R) of R contains d-dimensional topological features, such as connected components (d = 0), cycles/tunnels (d = 1), and voids (d = 2). Homology groups are typically summarised by their ranks, thereby obtaining a simple invariant signature of a manifold. For example, a circle in R2 has one feature with d = 1 (a cycle), and one feature with d = 0 (a connected component). Based on these background knowledge, we further introduce how to compute a Persistent Homology when given a point cloud X. Firstly, we denote the Vietoris-Rips complex Vietoris (1927) of X at scale ε as Vε(X). Then, we can obtain the Persistent Homology PH(Vε(X))of a Vietoris-Rips complex Vε(X), which consists of persistence diagrams {D1, D2, ...} and persistence pairs {π1, π2, ...}. The d-dimensional persistence diagram Dd contains coordinates with the form (a, b), where a refers to a threshold ε at which a d-dimensional topological feature appears and b refers to a threshold ε′ at which it disappears. The d-dimensional persistence pairs contains indices (i, j) corresponding to simplices si, sj ∈ Vε(X), which create and destroy the corresponding topological features determined by (a, b) ∈ Dd. Note that more detailed background knowledge (e.g., simplex, Vietoris-Rips complex) is introduced in Section A.\n\n1Connectivity-based features, e.g., connected components in 0-dimensional, cycles in 1-dimensional, and\n\nvoids in 2-dimensional topological features\n\n2\n\n(b)Randomincrementaldatapoint(c)TGSSincrementaldatapoint(a)RandomsampledmdatapointsXX(m)DH(X,X(m))XX%&’( =X(m)+xDH(X,X%&’( )XDH(X,X*&’( )(e)Alive1-dimtopologicalfeature(f) Alive2-dimtopologicalfeature(d)Alive0-dimtopologicalfeatureXGlobaldatapointsSampledmdatapointsX(m)RandomincrementaldatapointTGSSincrementaldatapointxX*&’( =X(m)+xxxxUnder review as a conference paper at ICLR 2023\n\nBased on the powerful geometry feature analysis ability of Persistent Homology, we discover 2 problems in the existing state-of-the-art (sota) structure alignment module Chen et al. (2021c): (i) Due to the limitation of batch size, the underlying geometry structure of mini-batch samples can not represent global samples’. Thus, when applying structure alignment metric (i.e., sliced Wasserstein discrepancy Lee et al. (2019)) on random sampled 2 mini-batch visual and semantic data points, we can only achieve a local-level structure alignment, indicating the accurate global geometry information is lost inevitably. (ii) HSVA utilizes sliced Wasserstein discrepancy to align latent visual and semantic space for bridging structure alignment. Actually, this implementation requires an assumption that the latent visual and semantic space can represent their underlying geometry structure adequately. To verify the correctness of this assumption, we adopt persistent homology to visualize the underlying geometry structure of input space and latent space on the visual domain. As shown in Fig. 1 (d) - (f), there is a distinct gap between the blue dash line and the orange dash line, which is further expanded in the latter two images, representing that the HSVA latent visual space loses abundant geometry structure, especially for 1-dimensional and 2-dimensional topological features. The rationale is that after dimensionality reduction (namely curse of dimensionality Wang & Chen (2017) ), the topological structure is difficult to maintain.\n\nR\n\nare constructed by our TGSS and random sampling strategy, respectively. Obviously, the\n\nIn this paper, we devise a TopoZero framework to achieve a more desirable structure alignment by solving 2 aforementioned issues. Concretely, our TopoZero adopts CADA-VAE Sch ̈onfeld et al. (2019) as the distribution alignment module and develops a Topology Alignment Module (TAM) with 2 following novelties. (i) To alleviate the structure gap between the sampled mini-batch data points and global data points, we propose a Topology-guided Sampling Strategy (TGSS) to explicitly and progressively mine the topology-preserving data point into the sampled mini-batch data point. Moreover, the theoretical analysis illustrated in Section A guarantees the advantage of our TGSS. Besides, as shown in Fig. 1 (b) - (c), we further visualize the advantage of our TGSS in an illustrator example: based on the same random sampled data points X (m), X (m+1) and X (m+1) Hausdorff Distance 3 DH (X, X (m+1) DH (X, X (m+1) ), indicating our TGSS can alleviate the gap between sampled data points and global data points compared to random sampling strategy. (ii) To preserve the topological structure for visual and semantic latent space, we develop a dual topological-aware branch as well as a topologicalpreserving loss to learn a topological-invariant latent representation. Moreover, based on the opensource tool Ripser 4, we compute the persistent homology to analyze the multi-dimensional topological features from input space, HSVA latent structure space, and TopoZero latent structure space on the visual domain. Given a set of data points, ripser can compute the corresponding persistent homology, which consists of persistence diagrams {π1, π2, ...} and persistence pairs {D1, D2, ...}. Thus based on the obtained persistence diagrams and persistence pairs, we can calculate the number of alive 0/1/2-dimensional topological features under different threshold ε. As such, we draw the Fig. ·1 (d)-(f), where the line represents the trend of the number of alive topological features under different threshold ε. As revealed from these visualization results, by taking advantage of our proposed TAM, the multi-dimensional topology feature gap between our TopoZero latent space and input space is negligible.\n\nand global data points X is bounded by\n\n) between X (m+1)\n\nR\n\nT\n\nT\n\nT\n\n2 RELATED WORKS\n\nZero-Shot Learning. In recent years, the ZSL realm has attracted many researchers’ attention Zhang & Saligrama (2016); Li et al. (2017); Zhu et al. (2019a); Fu et al. (2015); Ye & Guo (2017); Yu & Lee (2019b); Chen et al. (2018). One typical branch to solve the ZSL problem is learning a common embedding space for aligning semantic and visual domains, termed common space learning. Early common space learning methods focus on framework designation for better distribution alignment. Wang et al. Wang & Chen (2017) have proposed a bidirectional latent embedding framework with two subsequent learning stages. Liu et al. (2018) maps visual features and semantic representations of class prototypes into a common embedding space to guarantee the seen data is compatible with seen and unseen classes. CADA-VAE Liu et al. (2018) have demonstrated that\n\n2Existing methods all adopt random sampling strategy to generate mini-batch data points. 3A metric that can measure the bounded distance between two persistence diagrams. 4Available at https://github.com/Ripser/ripser.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nonly two variational autoencoders as well as a distribution alignment loss, can achieve a significant distribution alignment in a common space. However, as pointed out from HSVA Chen et al. (2021c), due to the heterogeneous nature of the feature representations in semantic and visual domains, the distribution and structure variation intrinsically exists. Motivated by this, Chen et al. Chen et al. (2021c) propose a hierarchical semantic-visual adaptation framework for aligning structure and distribution progressively. Thus, the structure alignment in ZSL emerges with a new state-of-the-art performance on the task of common space learning.\n\nPersistent Homology. Persistent homology, a tool for topological data analysis, is used for understanding topological features at different dimension. Concretely, persistent homology can detect multi-dimensional topological features (holes, circles, connected components) under various dimensions for the underlying manifold of a set of sampled data points. Based on this property, persistent homology has been applied to a vast body of scenarios, e.g., characterizing graphs in Archambault et al. (2007); Carri`ere et al. (2020); Li et al. (2012), analysing underlying manifolds in Bae et al. (2017); Futagami et al. (2019), topological preserving autoencoder in Moor et al. (2020). In this paper, by leveraging persistent homology, we discover that the latent visual and semantic space can not preserve multi-dimensional topological features. Furthermore, to improve the geometry representation of latent space in both domains, we propose a Topology Alignment Module for encoding multi-dimensional topological representation explicitly.\n\n3 METHODOLOGY\n\nTo begin with, we formulate the task of ZSL. Assume we have a set of seen samples S for training, and a set of unseen samples U for testing only, where S = {(xs, ys, as) | xs ∈ X s, ys ∈ Y s, as ∈ A} be a training set. xs is seen image feature, which is extracted from the pre-trained CNN backbone (ResNet-101 He et al. (2016) is adopted in this paper). ys and as are xs corresponding class label and semantic vector, respectively. Analogously, let U = {(xu, yu) | xu ∈ X u, yu ∈ Y u}. Note that Y s ∩ Y u = ∅. The objectiveness of conventional ZSL (CZSL) is to learn a classifier for mapping unseen image features into unseen categories, i.e., Fczsl : X u → Y u, while the challenging generalized ZSL (GZSL) focus on learning a classifier to map image features to both seen and unseen categories, i.e., Fgzsl : X → Y u ∪ Y s.\n\nAs shown in Fig. 2, our TopoZero contains two parallel alignment modules, Distribution Alignment Module and Topology Alignment Module Specifically, we directly adopt the architecture of CADAVAE Sch ̈onfeld et al. (2019) as our Distribution Alignment Module. While for our TAD, topologyguided sampling strategy and dual topological-aware branch are proposed to mitigate the geometry structure gap between mini-batch and global data points and preserve multi-dimensional topological structure on both visual and semantic domains, respectively.\n\n3.1 TOPOLOGY-GUIDED SAMPLING STRATEGY\n\nTo bridge a structure gap between mini-batch and global data points, we propose a Topology-guided Sampling Strategy (TGSS) as well as a theoretical analysis to guarantee its superiority.\n\n3.1.1 DESCRIPTION\n\nAlgorithm 1 describes how our TGSS samples mini-batch samples from global data points. First, we random sample b/2 5 data points (Xb2) from global training samples (X). After that, we select the incremental data point xmax according to Equ. 1. Then, we construct a set of candidate set (C) by Equ. 2 and random sample b/2 − 1 data points from C to form Cmini. Finally, the mini-batch sampled data points are constructed by integrating Xb2, xmax and Cmini. The advantage of our TGSS relies heavily on the selection of xmax, which is proved by the following theoretical analysis.\n\n∃ xmax ∈ X, x′\n\nmax ∈ Xb2, s.t. dist(xmax, x′\n\nmax) = dH(X, Xb2)\n\nC(xmax, d) = {{x0, ..., xk}, xi ∈ X, xi /∈ T || dist(xi, xmax) < d}\n\n5b represents the size of batch training samples\n\n4\n\n(1)\n\n(2)\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The proposed TopoZero framework. Based on our proposed TGSS sampling strategy, we can obtain a batch of visual features x and corresponding semantic embeddings a. Then x and a are fed into the parallel topology alignment module and distribution alignment module. For TAD, x and Et the encoder Et x and topological semantic representation zt x and a. to reconstruct visual and semantic feature, which is optimized by reconstuction loss La zt AE and AE. The visual and semantic latent topological representation is optimized by Lx Lx to preserve multi-dimensional structure information. LT A is also applied to align zt a. For the distribution alignment module, we adopt the framework of CADA-VAE, which consists of two variational autoencoders and optimized by LBCE, LKL, LDA, and LCA.\n\na first encode x and a to get latent topological visual representation zt a decode zt\n\na, respectively. Then the decoder Dt\n\nT P and La x and zt\n\nx and Dt\n\nT P\n\nwhere T denotes a set of sampled data points from X and dist represents distance metric (Euclidean Distance in this paer). dH refers to the Hausdorff distance Huttenlocher et al. (1993) between X and Xb2. Then, we revisit the definition of Hausdorff Distance that dH(X, Y ) = max (cid:8) supx∈X d(x, Y ), supy∈Y d(X, y) (cid:9), which measures how far two subsets of a metric space are from each other. Informally speaking, the xmax represents the farthest data point in X to the sampled Xb2 when adopting Hausdorff Distance metric. Thus, by integrating the xmax into Xb2 , the Hausdorff Distance between X and Xb2 can be reduced, indicating the gap between sampled and global data points is also mitigated according to Theorem 1. Moreover, considering that the advantage of our TGSS relies heavily on the selection of xmax, we provide a theoretical analysis to guarantee its superiority. Besides, the introduction of Cmini is to maintain the representation of local topology structure surrounding from xmax.\n\n3.1.2 THEORETICAL ANALYSIS FOR TGSS\n\nThe core design of our TGSS is the procedure of selection xmax (line 4 in Algorithm 1), which can eliminate the structure gap compared to random sampling strategy. Here, we further provide a theoretical analysis to guarantee the advantage of this selection procedure. Before we carry out our analysis, we define a few important definitions and notations. For a point cloud X := {x1, . . . , xm} ⊆ Rd, denote X (m) be a subsample of X with cardinality m. Based on X (m) and the procedure of TGSS’s selection xmax, the constructed set is denoted as X (m+1) . While for random sampling strategy, we have X (m+1)\n\n. Thus, we have:\n\nT\n\nR\n\nT\n\nX (m+1) X (m+1)\n\nR\n\n= {X (m) ∪ x, x = xmax}\n\n= {X (m) ∪ x, x ∈ X\\X (m)}\n\n(3)\n\n(4)\n\nwhere xmax is defined in Equ. 1 Theorem 1. Moor et al. (2020). Let X be a point cloud of cardinality n and X (m) be one subsample of X of cardinality m, i.e. X (m) ⊆ X, sampled without replacement. We can bound the probability of the persistence diagrams of X (m) exceeding a threshold in terms of the bottleneck distance as\n\nP(db(DX , DX (m)\n\n) > ε) ≤ P(dH(X, X (m)) > 2ε)\n\n(5)\n\n5\n\nRedLegsGrayWingsWhiteHeadLongWingShapeCNNxaExtzxtDxtEatzatDatExdzxdDxdEadzadDadaxaxxaxaUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Topology-guided Sampling Strategy Input:\n\nX is a set of whole training samples. b is the size of batch training samples.\n\nOutput:\n\nT is prepared mini-batch samples for an epoch.\n\nrandom sample b / 2 training data from X\\T : Xb2; compute the incremental data point xmax by Equ. 1; construct a set of data points by Equ. 2: C = C(xmax, dH (Xb2, X));\n\nM ← random select b/2 − 1 − len(C) data points from X/Xb2; Xb2 = Xb2 ∪ C ∪ M;\n\nM ← random select b/2 − 1 data points from C; Xb2 = Xb2 ∪ M;\n\nif len(C) < b/2 − 1 then\n\n1: init T : T ← ∅ 2: for each iteration in epoch do 3: 4: 5: 6: Xb2 = Xb2 ∪ xmax; 7: 8: 9: 10: 11: 12: 13: 14: 15: end for 16: return T ;\n\nend if T = T ∪ Xb2\n\nelse\n\nT\n\nR\n\nT\n\nT\n\nand X (m+1)\n\n, and AX,X (m+1)\n\n∈ Rn×(m+1) be the distance matrix between samples of X and ∈ Rn×(m+1) be the distance matrix between samples of X and X (m+1)\n\nTheorem 2. Let AX,X (m+1) X (m+1) The X (m+1) are both sorted to ensure that the first (m+1) rows correspond to the columns of the m subsampled points with diagonal elements aii = 0. Assume that the entries aij in both matrix are independent and follow a same distance distribution FD when i > (m + 1). For i for rows with i > (m + 1) follow a distribution F∆′ .Letting AX,X (m+1) Z ′ := max1≤i≤n δ , the minimal distances\n\ni with a corresponding distribution F\n\n, the minimal distances δ\n\nZ. For AX,X (m+1)\n\nR\n\nR\n\n.\n\nT\n\n′\n\n′\n\n′\n\nδ\n\n′′ i\n\nfor rows with i > (m + 1) follow a distribution F∆′′ . Letting Z\n\n:= max1≤i≤n δ Z , the expected Hausdorff distance between X and X (m+1)\n\n′′\n\ni with a coris bounded\n\nT\n\nR\n\n′′\n\n′′\n\nresponding distribution F by:\n\nE[dH(X, X (m+1)\n\nT\n\n)] ≤ E[dH(X, X (m+1)\n\nR\n\n)]\n\n(6)\n\nR\n\n), the sampled batch data points (X (m+1)\n\nWe include its proof in Section A. Theorem. 2 illustrates that compared to random sampling strategy (X (m+1) ) from our TGSS are closer to the global data points X with Hausdorff Distance metric, which constitutes the upper bound of bottleneck distance between two persistence diagrams (Theorem 1). Thus, since bottleneck distance is usually used to measure the distance between two persistence diagrams in the topological space Beketayev et al. (2014); Bubenik et al. (2010), we can conclude that compared to random sampling strategy, the sampled batch data points from our TGSS are closer to the global data points in the topological space.\n\nT\n\n3.2 TOPOLOGY ALIGNMENT MODULE\n\nAs shown in Fig. 1 (a)-(c), HSVA, a state-of-the-art common space learning method by taking structure alignment into account, fails to preserve multi-dimensional topological features. Specifically, the terrible structure representation in the latent space inevitably leads to a sub-optimal structure alignment. To remedy this, we propose a Topology Alignment Module, consisting of a dual topology-aware branch and a topology-preserving loss, to encode multi-dimensional topological information into latent visual and semantic space for conducting a more desirable structure alignment.\n\nOur Dual Topology-aware Branch is illustrated in Fig. 2, which contains two autoencoders for obtaining topological-aware latent representation in visual and semantic domains. Specifically, the encoder Et a encodes image feature (x) / semantic vector (a) into latent space and obtain visual\n\nx / Et\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nand Z (m) . After that, the decoder Dt x\nfor reconstructing the latent representation into x / a. We first apply\n\nv\n\na\n\n/ Z (m)\n\na decodes Z (m)\n\nand semantic topological-aware latent representation Z (m) / Dt reconstruction loss to optimize our Dual Topology-aware Branch: Lx La\n\nAE = LREC = ∥Dt AE = LREC = ∥Dt\n\nx(x)) − x∥2 a(a)) − a∥2\n\nx(Et a(Et\n\na\n\nv\n\n(7)\n\n(8)\n\nv\n\nand semantic embeddings X (m)\n\nThen we utilize the topology-preserving loss proposed by Moor et al. (2020) to preserve multiple dimensional topological features on the latent visual and semantic space, which is calculated by the following steps: 1) Given a batch of visual feature X (m) , our dual topology-aware branch can obtain corresponding latent representation, Z (m) and Z (m) ; 2) We calculate the distance matrix between samples of X (m) . The corresponding persistent homology of X (m) ). Analogously, for X (m) and AZ(m) , AZ(m) ,\npersistence pairings πX (m) ; 3) Finally, we retrieve the value of 0-dimensional / 1-dimensional / 2-dimensional persistence diagram 6 from distance matrix with indices provided by the persistence pairings, namely DX (m) [πX (m) get the 0/1/2 -dimensional persistence diagrams in X (m) mized by the following topology-preserving loss:\n\n, termed AX (m) , πX (m) , we can obtain corresponding distance matrix AX (m)\n\n]. Through this computation process, we\n\nis recorded as PH(Vε(X (m)\n\n)) = (DX (m)\n\n, which are opti-\n\nv X (m)\n\nand πZ(m)\n\nand X (m)\n\nand Z (m)\n\nand Z (m)\n\n≃ AX (m)\n\n, πZ(m)\n\n, Z (m)\n\n, Z (m)\n\na\n\na\n\na\n\na\n\na\n\na\n\nv\n\nv\n\nv\n\nv\n\nv\n\nv\n\nv\n\n0\n\n0\n\na\n\na\n\na\n\na\n\nv\n\nv\n\nv\n\nv\n\nv\n\nv\n\nv\n\nv\n\nLx\n\nT P =\n\nLa\n\nT P =\n\n2 (cid:88)\n\ni=0\n\n2 (cid:88)\n\ni=0\n\n∥ DX (m)\n\nv\n\ni\n\n− DZ(m)\n\nv\n\ni\n\n∥ DX (m)\n\na\n\ni\n\n− DZ(m)\n\na\n\ni\n\n∥2,\n\n∥2\n\n(9)\n\n(10)\n\nFinally, to encourage interaction between visual and semantic domains in the topological space, we directly minimize the L2 distance between latent visual topological representation and latent semantic topological representation:\n\nLT A = ∥Z (m)\n\nv − Z (m)\n\na\n\n∥2\n\n(11)\n\n(12)\n\n3.3 DISTRIBUTION ALIGNMENT MODULE\n\nSince CADA-VAE Schonfeld et al. (2019) serves as our distribution alignment module, we directly revisit it in our framework. Our distribution alignment module adopts two variational autoencoders Kingma & Welling (2014) to obtain latent representation in visual and semantic domains, respectively. Concretely, the encoder Ed a encodes image feature (x) / semantic vector (a) into latent space and obtain visual and semantic latent representation zd x / Dd decodes zd a for reconstructing the latent representation into x / a. We apply standard VAE loss to optimize:\n\na. Then, the decoder Dd\n\nx and zd\n\nx / Ed\n\nx / zd\n\na\n\nLa\n\nLx\n\nV AE = LBCE − βLKL = EEd V AE = LBCE − βLKL = EEd\n\nx(x)[log Dd a (a)[log Dd where DKL represents the Kullback-Leibler divergence and p(z) is a prior distribution (standard Gaussian distribution N (0, 1) in this paper). The binary cross-entropy loss LBCE is served as the reconstruction loss. Following Schonfeld et al. (2019), β serves as the balanced weight to measure the importance of DKL.\n\nx)] − βDKL(Ed a)] − βDKL(Ed\n\nx(zd a(zd\n\nx(x)||p(z))\n\na(a)||p(z))\n\n(14)\n\n(13)\n\nDistribution alignment loss is formulated as:\n\nLDA =\n\n(cid:18)\n\n∥μx − μa∥2\n\n2 +\n\n(cid:13) (cid:13)(δx) (cid:13)\n\n1\n\n2 − (δa)\n\n1 2\n\n(cid:19) 1\n\n2\n\n(cid:13) 2\n(cid:13) (cid:13) F\n\n(15)\n\n6Due to the page limited, we provide a more detailed computation process in Section A\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nwhere ∥ · ∥2\n\nF is the squared matrix Frobenius norm, and cross-alignment loss is formulated as:\n\nLx La\n\nCA = |x − Dd CA = |a − Dd\n\nx(Ed a(Ed\n\na(a))| x(x))|\n\n3.4 TOPOZERO OBJECTIVE FUNCTION\n\nOur TopoZero is optimized by the following objective function:\n\nLT opoZero = Lx\n\nAE + La\n\n+ λ3 ∗ (Lx\n\nAE + λ1 ∗ (Lx CA + Lx\n\nCA + La\n\nT P + La V AE + La\n\nT P ) + λ2 ∗ LT A V AE + LDA)\n\n(16)\n\n(17)\n\n(18)\n\nIn the branch of TAM, Lx\n\nwhere λ1, λ2, and λ3 are the balanced weight to measure the importance of each module in our TopoZero. AE aim to obtain the latent visual and semantic representation. Lx T P assist the latent visual and semantic representation to preserve multidimensional topology structure. LT A associates semantic and visual latent representation in a common space. While for the branch of distribution alignment module, all the objective functions keep the same with those in CADA-VAE Zhu et al. (2019a).\n\nAE and La\n\nT P and La\n\n3.5 ZERO-SHOT PREDICTION\n\nAfter the optimization of TopoZero, we need to train Fgzsl and Fczsl for predicting unseen or seen samples. Given a seen image features xs, we can obtain the latent distribution representation x(xs) with reparametrization trick Kingma & Welling (2014) and topological representation zd xs = Ed au and zt x(xs). Analoguously, for unseen image semantic vector au, we have zd zt xs = Et au . Then we au , zt concatenate zd au]) for unseen one. After training Fgzsl and Fczsl, we use [zd\n\nxs ]) to serve as seen training data and ([zd xu ] to inference.\n\nxs ] and [zd\n\nxs and zt\n\nxs ([zd\n\nxu, zt\n\nxs , zt\n\nxs, zt\n\n4 EXPERIMENTS\n\nIn this section, we first elaborate on implementation details and 3 authoritative benchmark datasets in the field of ZSL. Then we compare our TopoZero with existing state-of-the-art ZSL methods. Finally, we provide some qualitative and quantitative analysis to illustrate the advantage of our TopoZero. Due to the limitation of page size, several parts are placed on Appendix A.\n\n4.1 DATASETS AND IMPLEMENTATION\n\nDatasets. We verify our TopoZero on 3 popular ZSL benchmark datasets, including CUB Welinder et al. (2010), SUN Patterson & Hays (2012), and AWA2 Xian et al. (2018a). CUB contains 11788 images of 200 bird classes (seen/unseen classes = 150/50) with 312 attributes. SUN consists of 14340 images from 717 classes (seen/unseen classes = 645/72) with 102 attributes. AWA2 includes 37322 images of 50 animal classes (seen/unseen classes = 40/10) with 85 attributes. Finally, we adopt the “split version 2.0” mode Xian et al. (2018b) to conduct data splits on CUB, SUN, and AWA2.\n\nNetwork Architecture. As illustrated in Fig. 2, our TopoZero contains 2 Encoders and 2 Decoders, which are basic Multi-Layer Perceptions with 2 fully connected (FC) layers and 4096 hidden units. The dimension of latent variable in the distribution alignment and topology alignment module are both set 64. The architecture of CZSL and GZEL classifier is a single FC layer.\n\nOptimization Details. Our TopoZero is optimized by Adam optimizer with an initial learning rate 10−4 . The total training epoch of TopoZero is set 100 with a batch size 50. For training final CZSL and GZSL classifiers, the training epoch, batch size, and initial learning rate are set 25, 28, 10−3 respectively.\n\nEvaluation Protocols. Following the standard evaluation protocol Xian et al. (2018a), our TopoZero is evaluated by the top-1 accuracy. For CZSL, we only compute the accuracy on unseen classes. While for GZSL, we both calculate the accuracy of seen and unseen classes.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Results (%) of the state-of-the-art models on CUB, SUN and, AWA2 datasets. The best result is masked in bold. The symbol “–” indicates no available result.\n\nMethods\n\nCZSL acc\n\nNon Common Space QFSL Song et al. (2018) LDF Li et al. (2018) SGMA Zhu et al. (2019b) AREN Xie et al. (2019) LFGAA Liu et al. (2019) SP-AEN Chen et al. (2018) PQZSL Li et al. (2019) CRNet Zhang & Shi (2019) IIR Cacheux et al. (2019) DVBE Min et al. (2020) FREE Chen et al. (2021b) Common Space DeViSE Frome et al. (2013) ReViSE Tsai et al. (2017) DCN Liu et al. (2018) SGAL Yu & Lee (2019a) CADA-VAE Sch ̈onfeld et al. (2019) DOE-ZEL Geng et al. (2022) VGSE Xu et al. (2022) HSVA Chen et al. (2021c) TopoZero (Ours)\n\n58.8 67.5 71.0 71.8 67.6 55.4 –\n– 63.8 –\n–\n\n- -\n- –\n57.9 -\n56.8 62.8 64.3\n\nCUB\n\nSUN\n\nAWA2\n\nGZSL S\n\n48.1 81.6 71.3 78.7 80.9 70.6 51.4 56.8 65.8 60.2 59.9\n\n53.0 28.3 60.7 47.1 53.5 -\n45.7 58.3 59.9\n\nU\n\n33.3 26.4 36.7 38.9 36.2 34.7 43.2 45.4 30.4 53.2 55.7\n\n23.8 37.6 28.4 44.7 51.6 -\n24.1 52.7 54.9\n\nH\n\n39.4 39.9 48.5 52.1 50.0 46.6 46.9 50.5 41.2 56.5 57.7\n\n32.8 32.3 38.7 45.9 52.4 -\n31.5 55.3 57.3\n\nCZSL acc\n\n56.2 –\n– 60.6 61.5 59.2 –\n– 63.5 –\n–\n\n- -\n- –\n61.6 -\n41.1 63.8 64.7\n\nGZSL S\n\n18.5 –\n– 38.8 40.0 38.6 35.3 36.5 34.1 37.2 37.2\n\n27.4 20.1 37.0 42.9 35.7 -\n35.7 39.0 40.9\n\nH\n\n23.1 –\n– 25.5 25.3 30.3 35.2 35.3 26.7 40.7 41.7\n\n20.9 22.0 30.2 36.1 40.6 -\n29.8 43.3 44.7\n\nU\n\n30.9 –\n– 19.0 18.5 24.9 35.1 34.1 22.0 45.0 47.4\n\n16.9 24.3 25.5 31.2 47.2 -\n25.5 48.6 49.4\n\nCZSL acc\n\n63.5 65.5 68.8 67.9 68.1 58.5 –\n– 67.9 –\n–\n\n- -\n- –\n62.6 66.4 66.7 70.6 70.6\n\nGZSL S\n\n72.8 87.4 87.1 92.9 93.4 90.9 70.9 –\n87.0 70.8 75.4\n\n74.7 39.7 -\n55.1 53.5 -\n66.7 76.6 80.0\n\nU\n\n52.1 9.8 37.6 15.6 27.0 23.3 31.7 –\n17.6 63.6 60.4\n\n17.1 46.4 -\n81.2 51.6 -\n45.7 59.3 59.1\n\nH\n\n60.7 17.6 52.5 26.7 41.9 37.1 43.8 –\n28.9 67.0 67.1\n\n27.8 42.8 -\n65.6 52.4 57.6 54.2 66.8 68.0\n\nFor determining the performance of GZSL in a unified criterion, the harmonic mean (defined as H = (2 × S × U )/(S + U )) is adopted in this paper.\n\n4.2 COMPARISON WITH STATE-OF-THE-ARTS.\n\nResults on Conventional Zero-Shot Learning. Tab. 1 reports the CZSL results of our TopoZero and recent state-of-the-art (sota) methods on 3 ZSL datasets. Considering that attribute-based sota methods Huynh & Elhamifar (2020); Chen et al. (2021a) exploit the advantage of pre-trained NLP models GloVE and generation-based sota methods Xian et al. (2018b); Yu et al. (2020) take advantage of data augmentation, methods involving these 2 branches are not taken into account in this part. Compared to methods only with distribution alignment, our TopoZero illustrates a significant improvement of 6.4%, 3.1%, and 8.0% on CUB, SUN, and AWA2 datasets at least. While compared to HSVA Chen et al. (2021c) with distribution and structure alignment, our TopoZero also achieves a great improvement of 1.5%, 0.9% on CUB and SUN datasets, respectively. Such a significant performance directly verifies the effectiveness of topology alignment for the ZSL task.\n\nResults on Generalized Zero-Shot Learning. By looking at the challenging GZSL results in Tab. 2, our TopoZero also achieves a dominant harmonic mean performance of 57.3%, 44.7%, and 68.0% on CUB, SUN, and AWA2 datasets, respectively. Both superiority results of TopoZero on CZSL and GZSL settings demonstrate that our TopoZero is better than HSVA on structure alignment.\n\n5 CONCLUSION\n\nIn this paper, we propose a TopoZero framework to improve structure alignment for common space learning methods. To begin with, we discover that existing structure alignment approaches confront two challenging issues: 1) sampled mini-batch data points present a distinct gap compared to global ones; 2) latent visual and semantic space lose some high-dimensional structure information due to the ’curse of dimensionality.’ To solve these two problems, Topology-guided sampling strategy and Topology Alignment Module are proposed to construct our TopoZero. Furthermore, we provide a theoretical analysis as well as visualization results to guarantee the advantage of our TopoZero, namely excellent multi-dimensional topology-preserving and topology-alignment ability. Finally, The extensive and superior experiment results demonstrate that our TopoZero has a great potential to advance the ZSL community.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDaniel Archambault, Tamara Munzner, and David Auber. Topolayout: Multilevel graph layout by topological features. IEEE transactions on visualization and computer graphics, 13(2):305–317, 2007.\n\nWoong Bae, Jaejun Yoo, and Jong Chul Ye. Beyond deep residual learning for image restoration: Persistent homology-guided manifold simplification. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 145–153, 2017.\n\nKenes Beketayev, Damir Yeliussizov, Dmitriy Morozov, Gunther H Weber, and Bernd Hamann. Measuring the distance between merge trees. In Topological Methods in Data Analysis and Visualization III, pp. 151–165. Springer, 2014.\n\nDaniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9157–9166, 2019.\n\nPeter Bubenik, Gunnar Carlsson, Peter T Kim, and Zhi-Ming Luo. Statistical topology via morse theory persistence and nonparametric estimation. Algebraic methods in statistics and probability II, 516:75–92, 2010.\n\nYannick Le Cacheux, H. Borgne, and M. Crucianu. Modeling inter and intra-class relations in the\n\ntriplet loss for zero-shot learning. In ICCV, 2019.\n\nMathieu Carri`ere, Fr ́ed ́eric Chazal, Yuichi Ike, Th ́eo Lacombe, Martin Royer, and Yuhei Umeda. Perslay: A neural network layer for persistence diagrams and new graph topological signatures. In International Conference on Artificial Intelligence and Statistics, pp. 2786–2796. PMLR, 2020.\n\nLong Chen, Hanwang Zhang, Jun Xiao, W. Liu, and S. Chang. Zero-shot visual recognition using\n\nsemantics-preserving adversarial embedding networks. In CVPR, pp. 1043–1052, 2018.\n\nShiming Chen, Ziming Hong, Guo-Sen Xie, Jian Zhao, Xinge You, Shuicheng Yan, and Ling Shao. Transzero++: Cross attribute-guided transformer for zero-shot learning. arXiv preprint arXiv:2112.08643, 2021a.\n\nShiming Chen, Wenjie Wang, Beihao Xia, Qinmu Peng, Xinge You, Feng Zheng, and Ling Shao.\n\nFree: Feature refinement for generalized zero-shot learning. In ICCV, 2021b.\n\nShiming Chen, Guo-Sen Xie, Yang Yang Liu, Qinmu Peng, Baigui Sun, Hao Li, Xinge You, and Ling Shao. Hsva: Hierarchical semantic-visual adaptation for zero-shot learning. In NeurIPS, 2021c.\n\nShiming Chen, Ziming Hong, Yang Liu, Guo-Sen Xie, Baigui Sun, Hao Li, Qinmu Peng, Ke Lu, and Xinge You. Transzero: Attribute-guided transformer for zero-shot learning. In AAAI, volume 2, pp. 3, 2022a.\n\nShiming Chen, Ziming Hong, Guo-Sen Xie, Wenhan Yang, Qinmu Peng, Kai Wang, Jian Zhao, and Xinge You. Msdn: Mutually semantic distillation network for zero-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7612–7621, 2022b.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nAndrea Frome, G. S. Corrado, Jonathon Shlens, S. Bengio, J. Dean, Marc’Aurelio Ranzato, and\n\nTomas Mikolov. Devise: A deep visual-semantic embedding model. In NeurIPS, 2013.\n\nZhenyong Fu, Elyor Kodirov, Tao Xiang, and S. Gong. Zero-shot object recognition by semantic\n\nmanifold distance. In CVPR, pp. 2635–2644, 2015.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRentaro Futagami, Noritaka Yamada, and Takeshi Shibuya. Inferring underlying manifold of data by the use of persistent homology analysis. In International Workshop on Computational Topology in Image Context, pp. 40–53. Springer, 2019.\n\nYuxia Geng, Jiaoyan Chen, Wen Zhang, Yajing Xu, Zhuo Chen, Jeff Z. Pan, Yufeng Huang, Feiyu In ProXiong, and Huajun Chen. Disentangled ontology embedding for zero-shot learning. ceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 443–453, 2022.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In CVPR, 2016.\n\nKaiming He, Georgia Gkioxari, Piotr Doll ́ar, and Ross Girshick. Mask r-cnn. In Proceedings of the\n\nIEEE international conference on computer vision, pp. 2961–2969, 2017.\n\nDaniel P Huttenlocher, Gregory A. Klanderman, and William J Rucklidge. Comparing images using the hausdorff distance. IEEE Transactions on pattern analysis and machine intelligence, 15(9): 850–863, 1993.\n\nD. Huynh and E. Elhamifar. Fine-grained generalized zero-shot learning via dense attribute-based\n\nattention. In CVPR, pp. 4482–4492, 2020.\n\nDiederik P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014.\n\nChristoph H. Lampert, H. Nickisch, and S. Harmeling. Attribute-based classification for zero-shot visual object categorization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36:453–465, 2014.\n\nChen-Yu Lee, Tanmay Batra, M. H. Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy for\n\nunsupervised domain adaptation. In CVPR, pp. 10277–10287, 2019.\n\nGeng Li, Murat Semerci, B ̈ulent Yener, and Mohammed J Zaki. Effective graph classification based on topological and label attributes. Statistical Analysis and Data Mining: The ASA Data Science Journal, 5(4):265–283, 2012.\n\nJ. Li, X. Lan, Y. Liu, L. Wang, and N. Zheng. Compressing unknown images with product quantizer\n\nfor efficient zero-shot classification. In CVPR, pp. 5458–5467, 2019.\n\nY. Li, D. Wang, Huanhang Hu, Yuetan Lin, and Yueting Zhuang. Zero-shot recognition using dual\n\nvisual-semantic mapping paths. In CVPR, pp. 5207–5215, 2017.\n\nY. Li, Junge Zhang, Jianguo Zhang, and Kaiqi Huang. Discriminative learning of latent features for\n\nzero-shot recognition. In CVPR, 2018.\n\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ́ar. Focal loss for dense object\n\ndetection. In ICCV, 2017.\n\nShichen Liu, Mingsheng Long, J. Wang, and Michael I. Jordan. Generalized zero-shot learning with\n\ndeep calibration network. In NeurIPS, 2018.\n\nYang Liu, Jishun Guo, Deng Cai, and X. He. Attribute attention for semantic disambiguation in\n\nzero-shot learning. In ICCV, 2019.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, G. S. Corrado, and J. Dean. Distributed representations\n\nof words and phrases and their compositionality. In NeurIPS, pp. 3111–3119, 2013.\n\nShaobo Min, Hantao Yao, Hongtao Xie, Chaoqun Wang, Z. Zha, and Yongdong Zhang. DomainIn CVPR, pp. 12661–12670,\n\naware visual bias eliminating for generalized zero-shot learning. 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nMichael Moor, Max Horn, Bastian Rieck, and Karsten Borgwardt. Topological autoencoders. In\n\nInternational conference on machine learning, pp. 7045–7054. PMLR, 2020.\n\nG. Patterson and J. Hays. Sun attribute database: Discovering, annotating, and recognizing scene\n\nattributes. In CVPR, pp. 2751–2758, 2012.\n\nJeffrey Pennington, R. Socher, and Christopher D. Manning. Glove: Global vectors for word repre-\n\nsentation. In EMNLP, 2014.\n\nEdgar Sch ̈onfeld, S. Ebrahimi, Samarth Sinha, Trevor Darrell, and Zeynep Akata. Generalized zero-\n\nand few-shot learning via aligned variational autoencoders. In CVPR, pp. 8239–8247, 2019.\n\nEdgar Schonfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell, and Zeynep Akata. Generalized\n\nzero-and few-shot learning via aligned variational autoencoders. In CVPR, 2019.\n\nJie Song, Chengchao Shen, Yezhou Yang, Y. Liu, and Mingli Song. Transductive unbiased embed-\n\nding for zero-shot learning. CVPR, 2018.\n\nYao-Hung Hubert Tsai, Liang-Kang Huang, and R. Salakhutdinov. Learning robust visual-semantic\n\nembeddings. In ICCV, pp. 3591–3600, 2017.\n\nLeopold Vietoris. ̈Uber den h ̈oheren zusammenhang kompakter r ̈aume und eine klasse von zusam-\n\nmenhangstreuen abbildungen. Mathematische Annalen, 97(1):454–472, 1927.\n\nQ. Wang and Ke Chen. Zero-shot visual recognition via bidirectional latent embedding. Interna-\n\ntional Journal of Computer Vision, 124:356–383, 2017.\n\nP. Welinder, S. Branson, T. Mita, C. Wah, Florian Schroff, Serge J. Belongie, and P. Perona. Caltech-\n\nucsd birds 200. Technical Report CNS-TR-2010-001, Caltech,, 2010.\n\nYongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning—a\n\ncomprehensive evaluation of the good, the bad and the ugly. TPAMI, 2018a.\n\nYongqin Xian, T. Lorenz, B. Schiele, and Zeynep Akata. Feature generating networks for zero-shot\n\nlearning. In CVPR, pp. 5542–5551, 2018b.\n\nGuo-Sen Xie, L. Liu, Xiaobo Jin, F. Zhu, Zheng Zhang, J. Qin, Yazhou Yao, and L. Shao. Attentive\n\nregion embedding network for zero-shot learning. In CVPR, pp. 9376–9385, 2019.\n\nWenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, and Zeynep Akata. Vgse: Visually-grounded In Proceedings of the IEEE/CVF Conference on\n\nsemantic embeddings for zero-shot learning. Computer Vision and Pattern Recognition, pp. 9316–9325, 2022.\n\nMeng Ye and Yuhong Guo. Zero-shot classification with discriminative semantic representation\n\nlearning. In CVPR, 2017.\n\nH. Yu and B. Lee. Zero-shot learning via simultaneous generating and learning. In NeurIPS, 2019a.\n\nHyeonwoo Yu and Beomhee Lee. Zero-shot learning via simultaneous generating and learning.\n\narXiv:1910.09446, 2019b.\n\nY. Yu, Zhong Ji, J. Han, and Z. Zhang. Episode-based prototype generating network for zero-shot\n\nlearning. In CVPR, pp. 14032–14041, 2020.\n\nF. Zhang and G. Shi. Co-representation network for generalized zero-shot learning. In ICML, 2019.\n\nZiming Zhang and Venkatesh Saligrama. Zero-shot learning via joint latent similarity embedding.\n\nIn CVPR, pp. 6034–6042, 2016.\n\nPengkai Zhu, Hanxiao Wang, and Venkatesh Saligrama. Generalized zero-shot recognition based\n\non visually semantic embedding. In CVPR, pp. 2990–2998, 2019a.\n\nYizhe Zhu, Jianwen Xie, Z. Tang, Xi Peng, and A. Elgammal. Semantic-guided multi-attention\n\nlocalization for zero-shot learning. In NeurIPS, 2019b.\n\nAfra Zomorodian and Gunnar Carlsson. Computing persistent homology. Discrete & Computational\n\nGeometry, 33(2):249–274, 2005.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nB PROOF OF THEOREM 2\n\nProof. First, we derive the distribution F∆′ (y) and F∆′′ (y):\n\nF∆′ (y) = P(δ\n\ni ≤ y) = 1 − P(δ\n\ni > y) = 1 − P( min\n\naij > y)\n\n′\n\n′\n\n1≤j≤m+1\n\n= 1 − P(\n\n(cid:92)\n\nj\n\naij > y) = 1 − (1 − FD(y))m+1\n\n(cid:26)\n\n=\n\n1 − (1 − FD(y))m+1 1\n\n, y < E[dH (X, X (m))] , else\n\nAnalogously, we have:\n\nF∆′′ (y) = P(δ\n\n′′\n\ni ≤ y) = 1 − P(δ\n\n= 1 − (1 − FD(y))m+1\n\n′′\n\ni > y) = 1 − P(\n\naij > y)\n\n(cid:92)\n\nj\n\nFor convenience, we denote 1 − (1 − FD(y))m+1 as Fδ(y). Next, we derive the distribution (FZ′(z) and FZ′′ (z) ) of Z ′ and Z\n\n, respectively:\n\n′′\n\nFZ′(z) = P (Z ′ ≤ z) = P ( max\n\nm+1<i≤n\n\nδi ≤ z) = P (\n\n(cid:92)\n\nδi ≤ z)\n\nm+1<i≤n\n\n(cid:26)\n\n=\n\nF∆(z)(n−m+1) 1\n\n, z < E[dH (X, X (m))] , else\n\nAnalogously,\n\nFZ′′ (z) = P (Z\n\n′′\n\n≤ z) = P ( max\n\nm+1<i≤n\n\nδi ≤ z) = P (\n\n(cid:92)\n\nδi ≤ z)\n\nm+1<i≤n\n\n(cid:26)\n\n=\n\nF∆(z)(n−m+1) F∆(z)\n\n, z < E[dH (X, X (m))] , else\n\nThus, we have:\n\nEZ′∼FZ′ [Z ′] =\n\n=\n\n=\n\n=\n\n=\n\n+∞ (cid:90)\n\n(1 − FZ′(z)) dz −\n\n0\n\n+∞ (cid:90)\n\n(1 − FZ′(z)) dz\n\n(cid:90) 0\n\n−∞\n\nFZ′(z) dz\n\n0 (cid:90) E[dH (X,X (m)]\n\n0\n\n(cid:90) E[dH (X,X (m)]\n\n0\n\n(cid:90) E[dH (X,X (m)]\n\n0\n\n(1 − FZ′(z)) dz +\n\n(cid:90) +∞\n\nE[dH (X,X (m)]\n\n(1 − FZ′(z)) dz\n\n(31)\n\n(1 − F∆(z)n−m) dz +\n\n(cid:90) +∞\n\nE[dH (X,X (m)]\n\n(1 − 1) dz\n\n(32)\n\n(1 − F∆(z)n−m) dz\n\n(33)\n\n13\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\nUnder review as a conference paper at ICLR 2023\n\nand:\n\nEZ′′ ∼F\n\nZ\n\n′′ [Z\n\n′′\n\n] =\n\n+∞ (cid:90)\n\n(1 − FZ′′ (z)) dz\n\n(34)\n\n0 (cid:90) E[dH (X,X (m)]\n\n0\n\n(cid:90) E[dH (X,X (m)]\n\n=\n\n=\n\n0\n\n(1 − FZ′′ (z)) dz +\n\n(cid:90) +∞\n\nE[dH (X,X (m)]\n\n(1 − FZ′′ (z)) dz\n\n(35)\n\n(1 − F∆(z)n−m−1) dz +\n\n(cid:90) +∞\n\nE[dH (X,X (m)]\n\n(1 − F∆(z)) dz\n\n(36)\n\nFinally,\n\nEZ′∼F\n\nZ\n\n′ [Z\n\n=> EZ′∼F\n\nZ\n\n′ [Z\n\n′\n\n′\n\n] − EZ′′ ∼F\n\n′′ [Z\n\nZ\n\n′′\n\n] =\n\n] ≤ EZ′′ ∼F\n\n′′ [Z\n\n′′\n\n]\n\nZ\n\n(cid:90) +∞\n\nE[dH (X,X (m)]\n\n(F∆(z) − 1) dz ≤ 0\n\n=> E[dH(X, X (m+1)\n\nT\n\n)] ≤ E[dH(X, X (m+1)\n\nR\n\n)]\n\n(37)\n\n(38)\n\n(39)\n\n(40)\n\nC PERSISTENT HOMOLOGY\n\nHere, we further provide several explanations on the definition of simplex, simplicial complex, ab- (a) Simplex: In geometry, a simplex is a stract simplicial complex and Vietoris-Rips complex. generalization of the notion of a triangle or tetrahedron to arbitrary dimensions. The simplex is so-named because it represents the simplest possible polytope made with line segments in any given dimension. For example, a 0-simplex is a point, a 1-simplex is a line segment, and a 2-simplex is a triangle. (b) Simplicial Complex: In topology, it is common to ”glue together” simplices to form a simplicial complex. A simplicial complex is a set composed of points, line segments, triangles, and their n-dimensional counterparts. The strict definition of a simplicial complex is that A simplicial complex K is a set of simplices that satisfies the following conditions: 1) Every face of a simplex from K is also in K; 2) The non-empty intersection of any two simplices σ1, σ2 ∈ K is a face of both σ1 and σ2. (c) Abstract Simplicial Complex The purely combinatorial counterpart to a simplicial complex is an abstract simplicial complex. (d) Vietoris–Rips complex: In topology, the Vietoris–Rips complex, also called the Vietoris complex or Rips complex, is a way of forming a topological space from distances in a set of points. It is an abstract simplicial complex that can be defined from any metric space M and distance δ by forming a simplex for every finite set of points that has a diameter at most δ. That is, it is a family of finite subsets of M, in which we think of a subset of k points as forming a (k1)-dimensional simplex (an edge for two points, a triangle for three points, a tetrahedron for four points, etc.); if a finite set S has the property that the distance between every pair of points in S is at most δ, then we include S as a simplex in the complex. As illustrated in Moor et al. (2020), we can compute the persistent homology of a set of data points X based on this background information.\n\nD COMPUTATION PROCEDURE OF TOPOLOGY-PRESERVING LOSS\n\nv\n\nHere, we further introduce how to retrieve the value of 0-dimensional / 1-dimensional / 2dimensional persistence diagram from distance matrix with indices provided by the persistence pairings, namely DX (m) In essence, this retrieving procedure equals to how to select retreival indices from 0-dimensional / 1-dimensional / 2-dimensional persistence pairings. Concretely, for 0-dimensional topological features , we select the ”destroyer” simplices in the 0-dimensional persistence pairings. For 1-dimensional topological features and 1-dimensional topological features , we regard the maximum edge of the ”destroyer simplices” in corresponding persistence pairings as retrieval indices.\n\n≃ AX (m)\n\n[πX (m)\n\n].\n\n0\n\n0\n\nv\n\nv\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nE EXPERIMENTS.\n\nE.1 ABLATION STUDY\n\nBased on the CADA-VAE Schonfeld et al. (2019), we conduct ablative experiments on CUB, SUN, and AWA2 datasets to verify the effectiveness of our proposed Topology-guided Sampling Strategy and Topology Alignment Module. We first clarify the notations in Tab. 2. TAD denotes our Topology Alignment Module. TAD0 / TAD0−1 represents our Topology Alignment Module with preserving 0-dimensional/ 0-dimensional and 1-dimensional topological features. We can see the 4th row with TAD performs a better result than the 2nd row with T AD0 and the 3rd row with TAD0−1, indicating the effectiveness of multi-dimensional (especially high dimensional) structure alignment. Then, with the addition of TGSS, the performance is further enhanced, demonstrating the TGSS can achieve better structure alignment. This experiment result is highly compatible with our provided theoretical analysis on TGSS.\n\nTable 2: Ablation studies of TGSS and TAD on CUB, SUN, and AWA2 datasets. AWA2\n\nSUN\n\nCUB\n\nMethod\n\nCADA-VAE CADA-VAE + TAD0 CADA-VAE + TAD0−1 CADA-VAE + TAD TopoZero (CADA-VAE + TAD + TGSS)\n\nCZSL acc 57.9 59.2 60.3 62.2 64.3\n\nGZSL S\n53.5 58.8 59.2 58.7 59.9\n\nH 52.4 54.8 55.8 56.1 57.3\n\nU 51.6 51.3 52.7 53.7 54.9\n\nCZSL acc 61.6 61.8 61.9 62.5 64.7\n\nGZSL S\n35.7 37.2 38.1 39.2 40.9\n\nH 40.6 42.0 42.7 43.5 44.7\n\nU 47.2 48.3 48.5 48.8 49.4\n\nCZSL acc 62.6 67.4 68.2 68.8 70.6\n\nGZSL S\n53.5 71.3 77.3 76.9 80.0\n\nH 52.4 63.6 65.1 66.6 68.0\n\nU 51.6 55.5 56.2 58.6 59.1\n\nE.2 ANALYSIS\n\nThe effectiveness of TAM. To verify the effectiveness of single Topology Alignment Module, we disentangle it from our overall TopoZero framework. As reported in Tab. 3, although a single TAM can achieve great performance, there exactly exists a distinct performance gap compared with recent sota methods Chen et al. (2021c;b). This is why we introduce an off-the-shelf distribution alignment module into our TopoZero framework.\n\nTable 3: The effectiveness of single TAM. CUB SUN AWA 64.1 60.7 58.4 66.4 63.1 60.5\n\nMethod TAM TAM + TGSS\n\nCompatibility with sota ZSL frameworks. To verify the compatibility between our TAM branch with sota ZSL framework, we implement our proposed TGSS and TAD on the sota open-source method TransZero Chen et al. (2022a). The CZSL results are listed in Tab. 4. After applying the proposed TGSS and TAM, the performance of TransZero increases to 77.6%, 67.2% and 72.6% on CUB, SUN and AWA datasets respectively. This improvement indicates the effectiveness of the proposed method on the sota attribute-based ZSL framework. Note that the superiority of TransZero benefits from it utilizes semantic attribute vectors of each attribute learned by GloVe Pennington et al. (2014) to improve semantic representation. Through this extra knowledge, recent attributebased ZSL methods Chen et al. (2021a; 2022b) perform better than others without extra knowledge. Thus, we only verify the compatibility between our TopoZero and Transzero rather than comparing performance directly.\n\nTable 4: Compatibility with sota ZSL framework TransZero.\n\nMethod TransZero TransZero + Ours\n\nCUB SUN AWA 70.1 65.6 76.8 72.6 67.2 77.6\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: The coarse effects of λ1, λ2 and λ3 on the CUB dataset.\n\nFigure 4: The fine effects of λ1, λ2 and λ3 on the CUB dataset.\n\nModel Complexity Analysis. Our TopoZero has a clear intuition of leveraging parallel structure and distribution for advancing ZSL. Such design thus inevitably leads to the first 5 terms in Eq. 18 for multi-dimensional structure alignment and the last 5 terms in Eq. 18 for distribution alignment. Although TopoZero has 4 autoencoders in total, the entire training process is simultaneous and loss weights of all terms in Eq. 18 are the same for all datasets. The consistently significant results on all datasets show that our model is robust and easy to train. Additionally, several losses are formulated with similar forms, which are cooperated for easy optimization, i.e. Lx AE, Lx T P . Finally, TAM and DAM are parallel and such disentangle design can make the learning curve smooth and maximize the role of each branch, respectively. Benefiting from this disentangled design, our TopoZero is easy to train compared to HSVA, where the latter adopts coupled framework.\n\nAE and La\n\nCA and La\n\nT P and La\n\nCA, Lx\n\nHyper-parameter Analysis. In this part, we further verify the sensitivity of hyper-parameter in our TopoZero by conducting experiments on the CUB dataset, including λ1, λ2, and λ3. As shown in Fig. 3, the performance of TopoZero is of great robustness when varying hyper-parameter from {0.01, 0.05, 0.1, 0.25, 0.5, 1.0}. Finally, λ1, λ2 and λ3 are set 0.05, 0.05, and 1 in this paper for the better result.\n\nAlthough this hyper-parameter configuration achieves a great performance on 3 ZSL benchmark datasets, it also raises an interesting question: given these 3 hyper-parameters play distinct role in our TopoZero framework, why their effects are so consistent? For instance, the green lines in Fig. 3 almost present a consistent trending. The reason for this question is that the configuration in the hyperparameter selection setting is unreasonable, where the candidate range of λ3 is small. This hides the role of each term in the objective function since the value of 4-th term (controlled by λ3) is far larger than that of 2-nd (controlled by λ1) and 3rd (controlled by λ2) terms, where the value of La V AE and La V AE in 4-th term is extraordinarily large. Thus, to conduct a detailed hyper-parameter analysis, we extend the range of λ3 into {0.0001, 0.0005, 0.001, 0.0050.01, 0.05, 0.1, 0.25, 0.5, 1.0}. Based on this revision, the individual effects of the three hyper-parameters are expanded remarkably, that is illustrated in Fig. 4. Simultaneously, our TopoZero achieves a higher CZSL accuracy of 64.9% on the cub dataset via this step. In our opinion, this improvement benefits from this more reasonable hyper-parameter selection procedure, which is conducive to getting rid of ”hyper-parameter overfitting” via mining the role of each item accurately. Considering this step involves some tricks of hyper-parameter tuning, we only discuss this situation rather than adopting this hyper-parameter configuration for better results.\n\nVisualization Result. As shown in Fig. 1 (a) - (c), we utilize persistent homology to visualize the multi-dimensional topological features of TopoZero and HSVA latent structure space. We can\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nsee that our TopoZero topological latent space presents an almost consistent trend in terms of input topological space while HSVA fails, indicating that our TopoZero can preserve more geometry information than HSVA when handling with ’curse of dimensionality’ Wang & Chen (2017).\n\n17",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a topology-guided sampling strategy (TGSS) to mitigate the gap between sampled data within a mini-batch and global data. The proposed model, which is called TopoZero, consists of a topology alignment module (TAM) and a distribution alignment module. TAM is capable of preserving multidimensional geometric structures in each of the latent visual and semantic spaces. Experimental results show that TopoZero performs well on standard ZSL benchmark datasets.\n\n# Strength And Weaknesses\n\n- Strength\n\n-- A topology guided sampling strategy (TGSS) is proposed to mitigate the gap between sampled data in a mini-batch and global data.\n\n-- A theoretical analysis is presented for TGSS.\n\nTopoZero shows good performance on standard ZSL benchmark datasets.\n\n- Weaknesses\n\n-- The paper is not self-contained, which hinders understanding of the paper.\n\n-- The paper is poorly written, making it difficult to read the paper smoothly.\n\n-- There are some typos.\n\n-- Evaluations of the proposed method are insufficient.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nTo mitigate the gap between sampled data within a mini-batch and global data, a topology-guided sampling strategy (TGSS) is proposed, which has certain novelties.\n\nOn the other hand, the paper is not self-contained, which hinders understanding of the contents of the paper. For example, the explanation of Persistent Homology on page 2 is not familiar to many readers, so the reviewer would like to see an intuitive and qualitative explanation in the text and a detailed explanation in the Appendix. The lack of explanations makes the middle part of the description in 3.2 difficult to understand.\n\nThe paper is not well written, and it is difficult to read through. For example, the description in 3.1.1 does not explain more than that the calculations are performed using Equations 1 and 2. Qualitative and intuitive descriptions of each equation should be provided. \n\nThe description of equation 2 is particularly insufficient. At the time of reading this text, it would be more appropriate to write $\\mathcal{C}$ instead of $\\mathcal{U}$, and it would be more helpful to write $x$ as $x_{max}$.\n\nFigure 2 is difficult to understand. In particular, the explanation of (d)-(f) is insufficient.\n\nThe evaluation of the proposed method is insufficient. The model is a combination of CADA-VAE and TAP, thus the performance of TAP alone should be presented.\n\nAlso, each batch must be sampled according to Algorithm 1. The reviewer would like to know how long this computation takes, and would like to see some experiments on the order of computation and actual computation time.\n\nWhat is the problem of \"The latent visual and semantic space fails to preserve multiple dimensional geometry structure, especially high dimensional structure information.\" The reviewer would like to see a detailed explanation as to whether this problem has been solved along with the basis for the experiment.\n\nThere are some spelling errors.\n- In equation 2, the $($ in $(x_0$ is not necessary.\n- In equation 2, it is $X$, not $\\mathcal{X}$.\n- In the definition of $X$ in 3.1.2, it is $x_m$, not $x_n$.\n\n# Summary Of The Review\n\nAlthough the proposed method, called TGSS, has a certain degree of novelty and shows improvement in performance from the baselines, it is not considered to have reached the stage of publishing at this time due to a lack of experiments and insufficient explanation due to the low quality of the writing.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "ROBUST ALGORITHMS ON ADAPTIVE INPUTS FROM BOUNDED ADVERSARIES\n\nYeshwanth Cherapanamjeri UC Berkeley yeshwanth@berkeley.edu\n\nSandeep Silwal MIT silwal@mit.edu\n\nDavid P. Woodruff Carnegie Mellon University dwoodruf@andrew.cmu.edu\n\nFred Zhang UC Berkeley z0@berkeley.edu\n\nQiuyi (Richard) Zhang Google Research qiuyiz@google.com\n\nSamson Zhou UC Berkeley and Rice University samsonzhou@gmail.com\n\nABSTRACT\n\n√\n\nWe study dynamic algorithms robust to adaptive input generated from sources with bounded capabilities, such as sparsity or limited interaction. For example, we consider robust linear algebraic algorithms when the updates to the input are sparse but given by an adversary with access to a query oracle. We also study robust algorithms in the standard centralized setting, where an adversary queries an algorithm in an adaptive manner, but the number of interactions between the adversary and the algorithm is bounded. We first recall a unified framework of (Hassidim et al., 2020; Beimel et al., 2022; Attias et al., 2023) for answering Q adaptive queries that incurs (cid:101)O( Q) overhead in space, which is roughly a quadratic improvement over the na ̈ıve implementation, and only incurs a logarithmic overhead in query time. Although the general framework has diverse applications in machine learning and data science, such as adaptive distance estimation, kernel density estimation, linear regression, range queries, point queries, and serves as a preliminary benchmark, we demonstrate even better algorithmic improvements for (1) reducing the preprocessing time for adaptive distance estimation and (2) permitting an unlimited number of adaptive queries for kernel density estimation. Finally, we complement our theoretical results with additional empirical evaluations.\n\n1\n\nINTRODUCTION\n\nRobustness to adaptive inputs or adversarial attacks has recently emerged as an important desirable characteristic for algorithm design. An adversarial input can be created using knowledge of the model to induce incorrect outputs on widely used models, such as neural networks (Biggio et al., 2013; Szegedy et al., 2014; Goodfellow et al., 2015; Carlini & Wagner, 2017; Madry et al., 2018). Adversarial attacks against machine learning algorithms in practice have also been documented in applications such as network monitoring (Chandola et al., 2009), strategic classification (Hardt et al., 2016), and autonomous navigation (Papernot et al., 2016; Liu et al., 2017; Papernot et al., 2017). The need for sound theoretical understanding of adversarial robustness is also salient in situations where successive inputs to an algorithm can be possibly correlated; even if the input is not adversarially generated, a user may need to repeatedly interact with a mechanism in a way such that future updates may depend on the outcomes of previous interactions (Mironov et al., 2011; Gilbert et al., 2012; Bogunovic et al., 2017; Naor & Yogev, 2019; Avdiukhin et al., 2019). Motivated by both practical needs and a lack of theoretical understanding, there has been a recent flurry of theoretical studies of adversarial robustness. The streaming model of computation has especially received significant attention Ben-Eliezer et al. (2021); Hassidim et al. (2020); Woodruff & Zhou (2021); Kaplan et al. (2021); Braverman et al. (2021); Chakrabarti et al. (2022); Ajtai et al. (2022); Chakrabarti et al. (2022); Ben-Eliezer et al. (2022); Assadi et al. (2022); Attias et al. (2023); Dinur et al. (2023); Woodruff et al. (2023). More recently, there have also been a few initial results for dynamic algorithms on adaptive inputs for graph algorithms (Wajc, 2020; Beimel et al., 2021; Bernstein et al., 2022). These works explored the capabilities and limits of algorithms for adversaries that were freely able to choose the input based on previous outputs by the algorithm.\n\n1\n\nHowever, in many realistic settings, an adversary is limited in its abilities. For example, adversarial attacks in machine learning are often permitted to only alter the “true” input by a small amount bounded in norm. For the L0 norm, this restriction means that the adversary can only add a sparse noise to the true input. More generally, it seems reasonable to assume that adversarial input is generated from a source that has bounded computation time or bounded interactions with an honest algorithm.\n\n1.1 OUR CONTRIBUTIONS\n\nIn this paper, we study algorithms robust to adaptive/adversarial input generated from sources with bounded capabilities. We first study dynamic algorithms for adaptive inputs from a source that is restricted in sparsity. Namely, we consider robust linear algebraic algorithms when the updates to the label can be adversarial but are restricted in sparsity. We then study robust algorithms in the standard centralized setting, where an adversary queries an algorithm in an adaptive manner, but the number of interactions between the adversary and the algorithm is bounded. We first show that combining novel subroutines for each of these problems in conjunction with a simple but elegant idea of using differential privacy to hide the internal randomness of various subroutines previously used by Hassidim et al. (2020); Beimel et al. (2022); Attias et al. (2023) suffices to achieve robust algorithms across these different settings.\n\nDynamic algorithms on adaptive input for regression. Motivated by the problem of label shift in machine learning, we consider a dynamic version of least-squares regression, where the labels get updated. In this model, we are given a fixed design matrix and a target label that receives a sequence of updates. After each one, the algorithm is asked to output an estimate of the optimal least-squares objective. The goal of the algorithm is to maintain the objective value within a multiplicative factor (1 + ε) to the optimal. More specifically, the algorithm is given a fixed design matrix A ∈ Rn×d with n ≥ d and an initial response vector (i.e., label) b(1), which receives updates over time. We are interested in estimating 2 as the target label b undergoes updates. the least-squares objective value F (A, b) = min x∈Rd The updates to b are adaptively chosen by an adversary but can only affect at most K entries of b per step. Formally, on the i-th round:\n\n∥Ax − b∥2\n\n(1) The adversary provides an update to K entries of b(i−1), possibly depending on all previous\n\noutputs of the algorithm.\n\n(2) The algorithm updates its data structure and outputs an estimate (cid:98)Fi of Fi = F (cid:0)A, b(i)(cid:1).\n\n(3) The adversary observes and records the output (cid:98)Fi.\n\nThe goal of the adversary is to create a sequence of labels (cid:0)b(i)(cid:1)T i=1 that induces the algorithm to output an inaccurate estimate. To deal with adaptivity, a na ̈ıve idea is to treat each step as an independent least-squares regression problem. However, this approach uses a completely new approximation of the objective value for each update, which seems potentially wasteful. On the other hand, any randomness that is shared by computations over multiple updates can potentially be leveraged by the adversary to induce an incorrect output.\n\nOur main result is an algorithm that beats the na ̈ıve algorithm in this challenging, adaptively adversarial setting. We provide a general result with run-time dependence on n, d, K, and the number of nonzero entries in A, nnz(A).\n\nTheorem 1.1 (Informal; see Theorem 2.1). Let κ(A) = O(1) and ε ∈ (0, 1). There exists a dynamic algorithm that given adaptively chosen K-sparse updates to b and a fixed design matrix A ∈ Rn×d, outputs a (1 + ε) approximation to the least-squares objective F (A, b(i)) every round with high\n\nprobability. The algorithm uses (cid:101)O\n\n(cid:16)(cid:112)K nnz(A)/ε3(cid:17)\n\namortized time per step of update.\n\nSpecifically, the update time is d1.5 when K ≤ d and n = O(d) and square root of the input sparsity when K = O(1). Notice that this significantly betters the na ̈ıve approach of treating each step independently and solving for the least-square objective, which requires nnz(A) + poly(d) time by sketching (Woodruff (2014)).\n\n2\n\nWe mention that a recent work by Jiang et al. (2022) considers a row-arrival model for dynamic linear regression. Our setting is different since we allow arbitrary updates to the target label, whereas in their setting the design matrix undertakes incremental change. We note that their algorithm maintains a solution vector, while we focus on the cost only.\n\nAdaptive query framework. We then consider robust algorithms in the standard centralized setting, where an adversary queries an algorithm in an adaptive manner. In many key algorithmic applications, randomization is necessary to achieve fast query time and efficient storage. This necessitates the need for robust versions of these algorithm which can efficiently employ the power of randomness while also being accurate across multiple possibly correlated inputs. Our main parameters of interest are query time and the space used by a robust algorithm compared to their na ̈ıve, non-robust, counterparts.\n\nFormally, we define the model as a two-player game between an algorithm HonestAlg over a data set X and an adversary A that makes adversarial queries about X to HonestAlg. At the beginning of the game, HonestAlg uses pre-processing time to compute a data structure D from X to answer future queries from A. The game then proceeds in at most Q rounds for some predetermined Q, so that in the t-th round, where t ∈ [Q]:\n\n(1) A computes a query qt on X, which depends on all previous responses from HonestAlg. (2) HonestAlg uses D to output a response dt to query qt. (3) A observes and records the response dt.\n\nThe goal of A is to formulate a query qt for which the algorithm HonestAlg produces an incorrect response dt. We remark that the algorithm may not have access to X, after constructing D, to respond to the query qt. On the other hand, A can use previous outputs to possibly determine the internal randomness of the data structure D and make future queries accordingly. In this case, the analysis of many randomized algorithms fails because it assumes that the randomness of the algorithm is independent of the input. Consequently, it does not seem evident how to handle Q adaptive queries without implementing Q instances of a non-adaptive data structure, i.e., each instance handles a separate query. Thus, a natural question to ask is whether a space overhead of Ω(Q) is necessary.\n\nAs a preliminary benchmark, we show that a space overhead of Ω(Q) is unnecessary by giving a\n\nunified framework with only an (cid:101)O (cid:0)√ Theorem 1.2. Given a data structure D that answers a query q with probability at least 3 4 using space S and query time T , there exists a data structure that answers Q adaptive queries, with high probabilQ log(nQ)(cid:1) and query time (cid:101)O (cid:0)T log(nQ) + log3(nQ)(cid:1). ity, i.e., 1 −\n\npoly(n,Q) , using space O (cid:0)S\n\nQ(cid:1) space overhead.\n\n√\n\n1\n\nTheorem 1.2 invokes the framework of Hassidim et al. (2020); Beimel et al. (2022); Attias et al. (2023) to the centralized setting, where a number of queries are made only after the data structure is created.\n\nTo concretely instantiate our framework and state an example, we consider the adaptive distance estimation problem defined as follows. In the adaptive distance estimation problem, there exists a set X = {x(1), . . . , x(n)} of n points in Rd. We are also given an accuracy parameter ε > 0. A query is of the form q, and the algorithm must output a (1 + ε)-approximation to ∥x(i) − q∥p for all i. The trivial solution of storing all n points and computing all n distances to a query point uses space and query time O (nd). Cherapanamjeri & Nelson (2020) improved the query time to (cid:101)O (cid:0) n+d (cid:1) at the (cid:17) cost of using (cid:101)O number of queries. By comparison, our data structure handles Q queries of approximate distances from a specified point in X, using query time (cid:101)O (cid:0) n+d ε2 √\n\npre-processing time, while permitting an arbitrary\n\n(cid:1), pre-processing time (cid:101)O\n\n(cid:16) (n+d)d ε2\n\nspace and (cid:101)O\n\n, and space\n\n(cid:16) nd2\n\n(cid:16) nd\n\n(cid:17)\n\n(cid:17)\n\nε2\n\nε2\n\nε2\n\n√\n\n√\n\nQ\n\nQ, our data structure already improves on the\n\n(cid:16) (n+d) ε2\n\n(cid:17)\n\nQ\n\n(cid:101)O work of Cherapanamjeri & Nelson (2020).\n\n. Thus, in the regime where d ≫ n\n\nA noticeable weakness of our construction is that the Q queries return only the approximate distance between a query point and a single point in X, whereas Cherapanamjeri & Nelson (2020) outputs approximate distances to all points in X. Moreover, Cherapanamjeri & Nelson (2022) subsequently (cid:1). Thus we open up our framework to (1) show that it can improve the pre-processing time to (cid:101)O (cid:0) nd\n\nε2\n\n3\n\nbe further improved to handle the case where we return the approximate distances of all points in X from Q adaptive query points and (2) achieve pre-processing time (cid:101)O (cid:0) nd Theorem 1.3. There is a data structure which, when instantiated with dataset X = {xi}i∈[n] ⊂ Rd and query bound Q ≤ d, answers any sequence of Q adaptively chosen distance estimation queries correctly with probability at least 0.99. Furthermore, the space complexity of the data structure is Q) and the setup and query times are (cid:101)O(ε−2 · nd) and (cid:101)O(ε−2 · (n + d)), respectively. (cid:101)O(ε−2 · n\n\n(cid:1).\n\n√\n\nε2\n\nn\n\n(cid:80)\n\nAnother application of our framework is the adaptive kernel density estimation problem, where there exists a set X = {x(1), . . . , x(n)} of n points in Rd and the goal is to output a (1 + ε)-approximation to the quantity 1 i∈[n] k(x(i), q), for an accuracy parameter ε > 0, a query q, and a kernel function k, under the promise that the output is at least some threshold τ > 0. Backurs et al. (2019) give an algorithm for kernel density estimation that uses O (cid:0) 1 query time, improving (cid:1) query time to output over the standard algorithm that samples O (cid:0) 1 the empirical kernel density. However, the analysis for both of these algorithms fails for the adaptive setting, where there can be dependencies between the query and the data structure. By using the data structure of Backurs et al. (2019) as a subroutine, our framework immediately implies an algorithm (cid:16) √ for adaptive kernel density estimation that uses (cid:101)O each of Q adaptive queries. In this case, we are again able to go beyond our framework and give a data structure that handles an unlimited number of adaptive kernel density queries:\n\n(cid:1) space and O (cid:1) points and then uses O (cid:0) d\n\nquery time to answer\n\nspace and O\n\n(cid:16) d log Q√\n\n(cid:16) d√\n\nQ τ ε2\n\nτ ε2\n\nτ ε2\n\nτ ε2\n\nτ ε2\n\nτ ε2\n\n(cid:17)\n\n(cid:17)\n\n(cid:17)\n\nTheorem 1.4. Suppose the kernel function k is L-Lipschitz in the second variable for some L > 0, i.e., |k(x, y) − k(x, z)| ≤ L∥y − z∥2 for all x, y, z ∈ Rd. Moreover, suppose that for all ∥x − y∥2 ≤ ρ, we have k(x, y) ≤ τ 3 . Then an algorithm that produces a kernel density estimation data structure D that is L-Lipschitz over a set X of points with diameter at most ∆ and outputs a (1+ε)-approximation to KDE queries with value at least τ with probability at least 1 − δ using space S(n, ε, τ, log δ) and query time T (n, ε, τ, log δ), then there exists a KDE data structure that with probability at least 0.99, outputs a (1 + ε)-approximation to any number of KDE queries with value at least τ using space\n\n(cid:16)\n\nS\n\nn, O (ε) , O (τ ) , O\n\n(cid:16)\n\nd log (∆+ρ)L\n\nετ\n\n(cid:17)(cid:17)\n\nand query time T\n\nn, O (ε) , O (τ ) , O\n\n(cid:16)\n\n(cid:16)\n\nd log (∆+ρ)L\n\nετ\n\n(cid:17)(cid:17) .\n\nAdditionally, we show that our framework guarantees adversarial robustness for a number of other important problems such as nearest neighbor search, range queries, point queries, matrix-vector norm queries, and linear regression. Finally, we supplement our theoretical results with a number of empirical evaluations, which are in the appendix.\n\n1.2 OUR TECHNIQUES\n\nDynamic regression on adaptive inputs. Our dynamic algorithm for dynamic maintenance of least-squares objective exploits two main ideas. First, standard results in sketching and sampling show that it suffices to solve for the sketched objective of minx∈Rd ∥SAx − Sb∥2 2, where S is an l2 subspace embedding for A. Here, we exploit several techniques from numerical linear algebra and in particular use leverage score sampling to obtain a subspace embedding S of A. By standard results in sketching, a (1 + ε) optimal solution is given by x∗ = (SA)†Sb. Moreover, since the goal is to output the objective value instead of the solution vector, we may take a Johnson-Lindenstrauss (JL) sketch to further reduce dimensionality and run-time. This allows us to focus on ∥GAx∗ − Gb∥2 2, where G ∈ RO(log d)×n is a JL sketch.\n\nAs a result, our algorithm dynamically maintains a solution GA(SA)†b in this sketched space. To achieve that, we first explicitly solve GA(SA)† in pre-processing. Since GA has few rows, this reduces to a small number of linear solves and can be computed fast via conjugate gradient-type methods. To handle the updates, we leverage their sparsity to efficiently maintain the solution and show that each round takes roughly O (K) time. Amortizing the pre-processing with the update costs over all iterations yields our desired run-time.\n\nFinally, we apply techniques from differential privacy to ensure adversarial robustness, by aggregating independent copies of the algorithm via a private median mechanism. Intuitively, the private mechanism hides the internal randomness of the algorithm and therefore prevents the adversary from otherwise choosing a “bad” input based on knowledge of internal parameters.\n\n4\n\nAdaptive query framework. Our framework maintains (cid:101)O (cid:0)√\n\nQ(cid:1) instances of the non-adaptive data structure and crucially uses differential privacy (DP) to protect the internal randomness of the data structures. In addition to our previous results for dynamic regression, the technique of using DP to hide randomness has recently been used in the streaming model (Hassidim et al., 2020; Kaplan et al., 2021) and the dynamic model (Beimel et al., 2021). These works elegantly use the advanced composition property of DP to bound the number of simultaneous algorithms that must be used in terms of the number of times the output changes “significantly” over the course of the stream. In the streaming model, the robust algorithms proceed by instantiating many “hidden” copies of a standard randomized algorithm. As the stream arrives, the algorithms are updated and an answer, aggregated using DP, is reported. Crucially, many of these results exploit the fact that the output answer is monotonic in the stream so that there is a known upper bound on the final output. Thus, the reported answers can only increase by a multiplicative factor at most a logarithmic number of times, which is used to bound the initial number of algorithms which are initialized. In our centralized setting, this can be imagined as setting the parameter Q. The main parameter of interest in the streaming literature is the space used by the streaming algorithms, whereas we are concerned with both space usage and query times. Furthermore, stream elements are only accessed one at a time and cannot be processed together unless memory is used. In our case, the dataset is given to us upfront and we can pre-process it to construct a data structure towards solving a centralized problem.\n\nThe work by Beimel et al. (2021) shares many of these ideas: the authors are concerned with dynamic graph algorithms where an adversary can update the graph in an adaptive fashion. Similar tools such as multiple randomized initialization and aggregated responses using DP are utilized. The main difference is their parameters of interest: the goal of Beimel et al. (2021) is to have a fast amortized update time across many queries. This necessitates the need to “throw away” existing algorithms and start with fresh randomness at intermittent points. In contrast, we study a centralized setting where the underlying dataset is not updated but we wish to answer Q adaptive queries on the dataset.\n\nof maintaining (cid:101)O (cid:0)√\n\nsmall subset, i.e., a subset of size O (log Q), of these (cid:101)O (cid:0)√ only incurs a logarithmic overhead in query time and an (cid:101)O (cid:0)√\n\nInspired by these works, our main framework also uses advanced composition to show the sufficiency Q(cid:1) data structures to answer Q adaptive queries in the centralized setting, which gives a rich set of applications. Moreover, to improve the query time of our algorithms, we further invoke the privacy amplification of sampling to show that it suffices to output the private median of a Q(cid:1) data structures. Thus our framework Q(cid:1) overhead in space. Surprisingly, our simple framework gives diverse applications for adaptive algorithms on a number of important problems, including estimating matrix-vector norms, adaptive range query search, adaptive nearest neighbor search, and adaptive kernel density estimation, to name a few. These applications are discussed in depth in Section C.\n\nAdaptive distance estimation. To achieve better pre-processing time for adaptive distance estimation, our main technique is to sample groups of rows from a Hadamard transform and argue that an interaction with a separate group should be considered in separate privacy budgets, effectively arguing that outputting n approximate distances to a single adaptive query only uses one unit of privacy budget. By contrast, our black-box framework charges one unit of privacy budget per approximate distance, so that outputting n approximate distances would use n units of privacy budget.\n\nAdaptive kernel density estimation. Theorem 1.4 is based on showing that with constant probability, our data structure is accurate on all possible queries in Rd. In particular, we first show that our data structure is accurate on a sufficiently fine net of points through a standard union bound argument, which incurs the d overhead compared to the space required to handle a single query. We then show that if the algorithm and the kernel function are both Lipschitz, which is true for sampling-based algorithms and a number of standard kernel functions, then accuracy on the net implies accuracy on all possible points in Rd.\n\n2 DYNAMIC REGRESSION UNDER LABEL UPDATES\n\nIn this section, we consider the dynamic problem of maintaining the cost of the least-squares regression, where the labels receive adaptively chosen updates. Let A ∈ Rn×d be the design matrix and b ∈ Rn be the target label. A classic problem in numerical linear algebra and optimization is to\n\n5\n\nsolve the l2 least-squares regression objective\n\nF (A, b) = min x∈Rd\n\n∥Ax − b∥2\n\n2 = (cid:13)\n\n(cid:13)AA†b − b(cid:13) 2\n(cid:13)\n\n2 .\n\n(2.1)\n\nWe consider a dynamic version of the problem, where the label receives adaptively chosen updates. We assume that each update can only affect K entries of the label vector. In this setting, we show:\n\nTheorem 2.1 (Main theorem; dynamic maintenance of regression cost). Let ε ∈ (0, 1/4) be an error parameter and b(1) be the initial target label. Given ε, A, b(1), a stream of T adaptively chosen, K-sparse updates to the label, Algorithm 4 outputs an estimate (cid:98)Fi such that (cid:98)Fi = (1 ± ε)F (A, b(i)) for all i with high probability.\n\nFurthermore, the algorithm requires a preprocessing step in time (cid:101)O (nnz(A) + poly(d)). The amortized update time of the algorithm is\n\n(cid:16)(cid:112)K nnz(A)\n\n(cid:16)(cid:112)κ(A) + ε−3(cid:17)(cid:17)\n\n.\n\n(cid:101)O\n\nWe defer the technical details to Appendix 3. Here, we describe the main ideas of the algorithm.\n\nAt a high-level, our algorithm implements a sketch-and-solve strategy. First, in pre-processing, the algorithm samples a leverage score sketching matrix S ∈ Rk×n, where k = O (cid:0)d log d/ε2(cid:1). This provides a (1 + ε) l2 subspace embedding for A. Standard results in sketching imply that it suffices to solve for the sketched objective of minx∈Rd ∥SAx − Sb∥2 2 (Sarlos (2006); Clarkson & Woodruff (2013; 2017); Woodruff (2014)). Let (cid:98)A = SA. A (1 + ε) optimal solution is thus given by (cid:98)A†b. Moreover, our goal is to maintain the regression cost, rather than this solution vector. Hence, we can apply the Johnson–Lindenstrauss lemma and focus on\n\n∥SAx − Sb∥2\n\n2 ≈ (cid:13)\n\n(cid:13)GA(SA)†Sb − Gb(cid:13) 2\n(cid:13)\n\n2 ,\n\nmin x∈Rd\n\n(2.2)\n\nwhere G ∈ RO(log n/ε2)×n is a JL sketch.\n\nTo track the cost value dynamically, the algorithm first computes and stores M = GA(SA)† in pre-processing. In the 1-st step, given the initial target label b(1), the algorithm computes Sb(i), M (cid:0)Sb(1)(cid:1) and Gb(1). Then it outputs (cid:98)F1 = (cid:13) 2 as an estimate of the regression cost. For the later steps, we show how to maintain Gb(i), Sb(i) efficiently, by exploiting the sparsity of the updates. Finally, to remain robust under adaptive inputs, we aggregate multiple copies using private median and carefully balance the parameters to achieve the run-time guarantee.\n\n(cid:13)MSb(1) − Gb(1)(cid:13) 2\n(cid:13)\n\n3 DETAILS ON DYNAMIC REGRESSION\n\nIn this section, we consider the dynamic problem of maintaining the cost of the least-squares regression, where the labels receive adaptively chosen updates.\n\nWe first introduce the basic setting of the problem in Section 3.1. In Section 3.2, we design a key subroutine under non-adaptive updates. The data structure enjoys a nearly linear update time. This allows us to aggregate multiple copies of the procedure and thereby efficiently ensure adversarial robustness against an adaptive adversary. The argument is via an application of differential privacy and detailed subsequently in Section 3.3.\n\n3.1 BASIC SETTING\n\nLet A ∈ Rn×d be the design matrix and b ∈ Rn be the target label. A classic problem in numerical linear algebra and optimization is to solve the l2 least-squares regression objective\n\nF (A, b) = min x∈Rd\n\n∥Ax − b∥2\n\n2 = (cid:13)\n\n(cid:13)AA†b − b(cid:13) 2\n(cid:13)\n\n2 .\n\n(3.1)\n\nWe consider a dynamic version of the problem, where the design matrix A remains unchanged. However, at each step (at most) K entries of b undergo an update. Moreover, we assume that the updates are chosen adaptively by an adversary in the following manner.\n\n6\n\n• The algorithm starts by receiving the input A ∈ Rn×d and b(1) ∈ Rn.\n\n• In the i-th step, the algorithm outputs an estimate (cid:98)Fi of the cost F (A, b(i)), where b(i) is\n\nthe target label corresponding to the step.\n\n• The adversary observes (cid:98)Fi and updates at most K labels to form b(i).\n\nLet b(1), b(2), . . . , b(T ) ∈ Rn be the resulting sequence of labels over T steps. The goal of the algorithm is to output a (1 + ε) approximation to the optimal cost at every step, while minimizing the update time.\n\n3.2 DYNAMIC ALGORITHM FOR OBLIVIOUS INPUTS\n\nIn this section, we provide a key subroutine that maintains a data structure under oblivious updates. On a high-level, the data structure aims to enable a sketch-and-solve strategy dynamically. The main ideas are two fold: (1) apply randomized sketching to reduce dimensionality and therefore the run-time, and (2) exploit the sparsity of the updates to argue that the regression costs can be maintained efficiently.\n\nBefore delving into the technical details, we give an overview of the algorithm.\n\nOverview of the algorithm. We start by assuming that the algorithm has access to DLS (via Lemma A.11), the row leverage score sampling data structure for A. In preprocessing, the algorithm samples a leverage score sketching matrix S ∈ Rk×n from DLS, where k = O(d log d/ε2). This provides a (1 + ε) l2 subspace embedding for A. Standard results in sketching imply that it suffices to solve for the sketched objective of minx∈Rd ∥SAx − Sb∥2 2 Sarlos (2006); Clarkson & Woodruff (2013; 2017); Woodruff (2014). Let (cid:98)A = SA. Then a (1 + ε) optimal solution is thus given by (cid:98)A†b. Moreover, our goal is to maintain the regression cost, rather than this solution vector. Hence, we can apply Johnson–Lindenstrauss lemma and focus on\n\n∥SAx − Sb∥2\n\n2 ≈ (cid:13)\n\n(cid:13)GA(SA)†Sb − Gb(cid:13) 2\n(cid:13)\n\n2 ,\n\nmin x∈Rd\n\n(3.2)\n\nwhere G ∈ RO(log n/ε2)×n is a JL sketch.\n\nNext, we describe how to track the cost value dynamically. We stress that the sketching matrices S and G are sampled upfront in the preprocessing stage and remain fixed afterwards. The algorithm stores G and M = GA(SA)†, both computed in preprocessing. Meanwhile, it maintains Gb(i), Sb(i), initialized at i = 1. In the first step, given the initial target label b(1), the algorithm computes Sb(i), M (cid:0)Sb(1)(cid:1) and Gb(1). Then it outputs (cid:98)F1 = (cid:13) 2 as an estimate of the regression cost.\n\n(cid:13)MSb(1) − Gb(1)(cid:13) 2\n(cid:13)\n\nLet’s consider the i-th step, where the label is updated to b(i). First, we read the K labels that get changed and update Sb(i−1) to Sb(i) accordingly. This can be done in O(K) time. Finally, we simply compute M(Sb(i)) and Gb(i) and output (cid:98)Fi = (cid:13) (cid:13)MSb(i) − Gb(i)(cid:13) 2\n2. We store Gb(i) for (cid:13) the next iteration.\n\nWe now describe the algorithm formally, followed by an analysis of its run-time and accuracy.\n\nFormal description of the algorithm. We assume DLS for A is given. The data structure is initialized by drawing the sketching matrices G and S. We also compute M = GSA(SA)† in preprocessing. This matrix is stored explicitly throughout.\n\nAlgorithm 1 Initialize the data structure, i.e., preprocessing\n\nInput: Design matrix A ∈ Rn×d, initial label b(1) ∈ Rn, DLS, ε ∈ (0, 1) 1: Let k = Θ (cid:0)d log d/ε2(cid:1) 2: Sample a (1 + ε/2) l2 leverage score row sampling matrix S ∈ Rk×n for A from DLS. 3: Sample a JL sketch matrix G ∈ RCε−2 log n×n, for a sufficiently large C, by Theorem A.6. 4: Compute and store M = GA(SA)†.\n\n7\n\nAt each step, the algorithm computes Sb(i) by reading all K entries of bi−1 that are updated in the step. After that, compute M(Sb(i)) and Gb(i) and output (cid:13) 2. The algorithm is formally given by Algorithm 2.\n\n(cid:13)Mb(i) − Gb(i)(cid:13) 2\n(cid:13)\n\nAlgorithm 2 Update data structure and maintain regression cost\n\nInput: Matrices G ∈ RCε−2 log n×n, S ∈ Rk×n, M ∈ R (cid:101)O(1/ε2)×k and the label b(i) Output: Estimate of the regression cost F (cid:0)A, b(i)(cid:1) 1: Compute Sb(i) by reading all K entries of b(i−1) that are updated. 2: Compute M (cid:0)Sb(i)(cid:1) and Gb(i). (cid:13)MSb(i) − Gb(i)(cid:13) 3: Output (cid:98)Fi = (cid:13) 2\n(cid:13)\n\n2.\n\n▷Store MSb(i), Sb(i), Gb(i) for the next round.\n\nAnalysis of the algorithm. We now analyze the run-time of the algorithm. First, consider the preprocessing stage performed by Algorithm 1. Lemma 3.1 (Preprocessing time). Assuming access to the leverage score sampling data structure DLS, the preprocessing time of Algorithm 1 is (cid:18)\n\n(cid:19)\n\nO\n\n(cid:112)κ(A) nnz(A) log\n\n1 ε\n\n+\n\nnnz(A) ε2\n\nd\n\nε2 log n\n\nlog n +\n\n.\n\n(3.3)\n\nProof. By Lemma A.11, the guarantee of the sampling data structure DLS, it takes O(k log(nd)) time to obtain a leverage score sample S of size k. Drawing the JL sketch is straightforward, and standard constructions such as i.i.d. Gaussian entries require O(k log n/ε2) times to form G.\n\nFinally, we need to compute M. Computing GA requires O (cid:16) log n ε2\n\nmultiplication. Moreover, since GA is a matrix of O\n\n(cid:17)\n\n(cid:16) nnz(A) ε2\n\n(cid:17)\n\nlog n\n\ntime by sparse matrix\n\nrows, then computing (GA)(SA)†\n\nreduces to O\n\nnumber of linear system solves with respect to SA ∈ Rk×d. By conjugate gradient type methods, since κ(SA) = (1 ± ε)κ(A), each solve can be achieved to high accuracy\n\n(cid:17)\n\n(cid:16) log n ε2\n\n(cid:16)(cid:112)κ(A) log(1/ε)\n\n(cid:17)\n\nin O\n\nnumber of matrix-vector products with respect to A Golub & Van Loan\n\n(2013). In total, this gives a run-time of O\n\n(cid:16)(cid:112)κ(A) nnz(A) log(1/ε)\n\n(cid:17)\n\n.\n\nLemma 3.2 (Update time). The update time of Algorithm 2 is O (cid:0) K\n\nε2 log n(cid:1) per step.\n\nProof. First, the algorithm reads the K entries that are updated and compute the Sb(i) from Sb(i−1). This step takes O(K) time, since we just need to update the entries that lie in the support of the row sampling matrix S. Similarly, in step 2 of Algorithm 2 we can update Gb(i−1) to Gb(i) in O(K log n/ε2) time. Since S is a row sampling matrix and b(i) only has K entries updated, then Sb(i) has at most K entries updated as well. It follows that given M (cid:0)Sb(i−1)(cid:1) from the prior round, M (cid:0)Sb(i)(cid:1) can be updated in O (cid:0) K\n\nε2 log n(cid:1) time.\n\nLemma 3.3 (Accuracy). Given a stream of T = O(d2) non-adaptive updates and error parameter ε ∈ (0, 1/4), Algorithm 2 outputs an estimate (cid:98)Fi of the regression cost F (A, b(i)) such that (cid:98)Fi = (1 ± ε)F (A, b(i)) for all i with high probability.\n\nProof. First, we apply the subspace embedding property of S. This implies that with high probability,\n\n(cid:13) (cid:13)\n\n(cid:13)SAx − Sb(i)(cid:13)\n\n2 (cid:13) (cid:13) 2\n\n= (1 ± ε/2) min\n\nx\n\n(cid:13) (cid:13)\n\n(cid:13)Ax − b(i)(cid:13)\n\n2 (cid:13) (cid:13) 2\n\n.\n\nmin x\n\nApply the JL lemma (Theorem A.6), where we consider the collection of O (cid:0)d2(cid:1) (1 + ε) optimal predictions {y∗ i = A(SA)†b(i). Via union bound, we have that with high probability for all i ∈ [T ]\n\ni=1 with y∗\n\ni }T\n\n(cid:13) (cid:13)Gy∗ (cid:13)\n\ni − Gb(i)(cid:13)\n\n2 (cid:13) (cid:13) 2\n\n(cid:13) (cid:13)y∗ (cid:13)\n\ni − b(i)(cid:13)\n\n2 (cid:13) (cid:13) 2\n\n.\n\n= (1 ± ε/2)\n\n8\n\nOur algorithm precisely solves for y∗ proof.\n\ni each iteration. Combining the two equations above finishes the\n\n3.3 DYNAMIC ALGORITHM WITH ADVERSARIAL ROBUSTNESS\n\n(cid:16)√\n\n(cid:17)\n\n(cid:16) nnz(A) ε2K\n\n(cid:17)\n\nTo put everything together and ensure adversarial robustness, we use a standard approach of Hassidim et al. (2020); Beimel et al. (2022); Attias et al. (2023). Our full algorithm maintains\n\nΓ = O\n\nT log(nT )\n\nindependent copies of the key subroutine for T = O\n\n. Then at each\n\nstep, we output the private median of the outputs of these copies. Advanced composition of DP ensures robustness up to T rounds. Afterwards, the algorithm reboots by rebuilding the copies, using fresh randomness independently for sampling and computing the sketching matrices.\n\nAlgorithm 3 Preprocessing step for Algorithm 4\n\nInput: A design matrix A ∈ Rn×d, an approximation factor ε ∈ (0, 1). Output: The leverage score sampling data structure DLS for A.\n\n1: Compute the approximate row leverage scores of A. 2: Build and output the data structure DLS\n\n▷Lemma A.11\n\nAlgorithm 4 Dynamic algorithm for maintaining regression cost under adaptive updates Input: A sequence of target labels (cid:8)b(i)(cid:9)m\n\ni=1 and a fixed design matrix A ∈ Rn×d, an approximation\n\nfactor ε ∈ (0, 1), the leverage score sampling data structure DLS for A.\n\nOutput: Estimates of the regression cost F (A, b(i)) under adaptively chosen updates to b.\n\n1: for every epoch of T = O\n\nupdates do\n\n(cid:16) nnz(A) ε2K\n\n(cid:17)\n\nT log (nT )\n\n(cid:17)\n\nindependent instances of the data structure in Section 3.2\n\n(cid:16)√\n\nInitialize Γ = O\n\nvia Algorithm 1.\n\nRun PrivMed on the Γ instances with privacy parameter ε′ = O\n\nprobability δ =\n\n1\n\npoly(m,T ) .\n\nFor each query, return the output of PrivMed.\n\n(cid:16)\n\n(cid:17)\n\n1√\n\nT log(nT )\n\nwith failure\n\n2:\n\n3:\n\n4:\n\nTheorem 3.4. [Main theorem; dynamic maintenance of regression cost] Let ε ∈ (0, 1/4) be an error parameter and b(1) be the initial target label. Given ε, A, b(1), a stream of T adaptively chosen, K-sparse updates to the label, Algorithm 4 outputs an estimate (cid:98)Fi such that (cid:98)Fi = (1 ± ε)F (A, b(i)) for all i with high probability.\n\nFurthermore, the algorithm requires a preprocessing step in time (cid:101)O (nnz(A) + poly(d)). The amortized update time of the algorithm is\n\n(cid:16)(cid:112)K nnz(A)\n\n(cid:16)(cid:112)κ(A) + ε−3(cid:17)(cid:17)\n\n(cid:101)O\n\nper round.\n\nWe defer the proof of Theorem 3.4 and a discussion on a deterministic algorithm to Section B.\n\nACKNOWLEDGEMENTS\n\nSandeep Silwal is supported by an NSF Graduate Research Fellowship under Grant No. 1745302, and NSF TRIPODS program (award DMS-2022448), NSF award CCF-2006798, and Simons Investigator Award (via Piotr Indyk). This work was done in part while David P. Woodruff was at Google Research and supported in part by a Simons Investigator Award and by the National Science Foundation under Grant No. CCF-1815840. Fred Zhang is supported by ONR grant N00014-18-1-2562. This work was done in part while Samson Zhou was at Carnegie Mellon University and supported in part by a Simons Investigator Award and by the National Science Foundation under Grant No. CCF-1815840.\n\n9\n\nREFERENCES\n\nMikl ́os Ajtai, Vladimir Braverman, T. S. Jayram, Sandeep Silwal, Alec Sun, David P. Woodruff, and Samson Zhou. The white-box adversarial data stream model. In PODS ’22: International Conference on Management of Data, pp. 15–27, 2022. 1\n\nJosh Alman and Huacheng Yu. Faster update time for turnstile streaming algorithms. In Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA, pp. 1803–1813, 2020. 20\n\nAlexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and optimal lsh for angular distance. Advances in neural information processing systems, 28, 2015. 27\n\nSepehr Assadi, Amit Chakrabarti, Prantar Ghosh, and Manuel Stoeckl. Coloring in graph streams via\n\ndeterministic and adversarially robust algorithms. CoRR, abs/2212.10641, 2022. 1\n\nIdan Attias, Edith Cohen, Moshe Shechner, and Uri Stemmer. A framework for adversarial streaming via differential privacy and difference estimators. In 14th Innovations in Theoretical Computer Science Conference, ITCS, pp. 8:1–8:19, 2023. 1, 2, 3, 9, 17\n\nDmitrii Avdiukhin, Slobodan Mitrovic, Grigory Yaroslavtsev, and Samson Zhou. Adversarially robust submodular maximization under knapsack constraints. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD, pp. 148–156, 2019. 1\n\nArturs Backurs, Moses Charikar, Piotr Indyk, and Paris Siminelakis. Efficient density evaluation for smooth kernels. In 59th IEEE Annual Symposium on Foundations of Computer Science, FOCS, pp. 615–626, 2018. 24\n\nArturs Backurs, Piotr Indyk, and Tal Wagner. Space and time efficient kernel density estimation in high dimensions. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, NeurIPS, pp. 15773–15782, 2019. 4, 24, 25\n\nAinesh Bakshi, Piotr Indyk, Praneeth Kacham, Sandeep Silwal, and Samson Zhou. Sub-quadratic algorithms for kernel matrices via kernel density estimation. CoRR, abs/2212.00642, 2022. 24\n\nRaef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan R. Ullman.\n\nAlgorithmic stability for adaptive data analysis. SIAM J. Comput., 50(3), 2021. 14\n\nAmos Beimel, Haim Kaplan, Yishay Mansour, Kobbi Nissim, Thatchaphol Saranurak, and Uri Stemmer. Dynamic algorithms against an adaptive adversary: Generic constructions and lower bounds. CoRR, abs/2111.03980, 2021. 1, 5\n\nAmos Beimel, Haim Kaplan, Yishay Mansour, Kobbi Nissim, Thatchaphol Saranurak, and Uri Stemmer. Dynamic algorithms against an adaptive adversary: generic constructions and lower bounds. In STOC ’22: 54th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1671–1684, 2022. 1, 2, 3, 9, 17\n\nOmri Ben-Eliezer, Rajesh Jayaram, David P. Woodruff, and Eylon Yogev. A framework for adversari-\n\nally robust streaming algorithms. SIGMOD Rec., 50(1):6–13, 2021. 1\n\nOmri Ben-Eliezer, Talya Eden, and Krzysztof Onak. Adversarially robust streaming via dense-sparse trade-offs. In 5th Symposium on Simplicity in Algorithms, SOSA@SODA, pp. 214–227, 2022. 1\n\nAaron Bernstein, Jan van den Brand, Maximilian Probst Gutenberg, Danupon Nanongkai, Thatchaphol Saranurak, Aaron Sidford, and He Sun. Fully-dynamic graph sparsifiers against an adaptive adversary. In 49th International Colloquium on Automata, Languages, and Programming, ICALP, pp. 20:1–20:20, 2022. 1\n\nBattista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD Proceedings,, 2013. 1\n\n10\n\nIlija Bogunovic, Slobodan Mitrovic, Jonathan Scarlett, and Volkan Cevher. Robust submodular maximization: A non-uniform partitioning approach. In Proceedings of the 34th International Conference on Machine Learning, ICML, pp. 508–516, 2017. 1\n\nVladimir Braverman, Avinatan Hassidim, Yossi Matias, Mariano Schain, Sandeep Silwal, and Samson Zhou. Adversarial robustness of streaming algorithms through importance sampling. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS, pp. 3544–3557, 2021. 1\n\nHerv ́e Br ̈onnimann, Bernard Chazelle, and J ́anos Pach. How hard is half-space range searching?\n\nDiscrete & Computational Geometry, 10(2):143–155, 1993. 19\n\nMark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Differentially private release and learning of threshold functions. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS, pp. 634–649, 2015. 13\n\nNicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy, SP, pp. 39–57. IEEE Computer Society, 2017. 1\n\nAmit Chakrabarti, Prantar Ghosh, and Manuel Stoeckl. Adversarially robust coloring for graph streams. In 13th Innovations in Theoretical Computer Science Conference, ITCS, pp. 37:1–37:23, 2022. 1\n\nVarun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM Comput.\n\nSurv., 41(3):15:1–15:58, 2009. 1\n\nMoses Charikar and Paris Siminelakis. Hashing-based-estimators for kernel density in high dimensions. In 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS, pp. 1032–1043, 2017. 24, 25\n\nMoses Charikar, Kevin C. Chen, and Martin Farach-Colton. Finding frequent items in data streams.\n\nTheor. Comput. Sci., 312(1):3–15, 2004. 20\n\nMoses Charikar, Michael Kapralov, Navid Nouri, and Paris Siminelakis. Kernel density estimation through density constrained near neighbor search. In 61st IEEE Annual Symposium on Foundations of Computer Science, FOCS, pp. 172–183, 2020. 24\n\nBernard Chazelle. The discrepancy method: randomness and complexity. Cambridge University\n\nPress, 2000. 19\n\nBernard Chazelle, Ding Liu, and Avner Magen. Approximate range searching in higher dimension.\n\nComputational Geometry, 39(1):24–29, 2008. 19\n\nNadiia Chepurko, Kenneth Clarkson, Lior Horesh, Honghao Lin, and David Woodruff. Quantuminspired algorithms from randomized numerical linear algebra. In International Conference on Machine Learning (ICML), 2022. 15\n\nYeshwanth Cherapanamjeri and Jelani Nelson. On adaptive distance estimation. In Advances in\n\nNeural Information Processing Systems 33: NeurIPS, 2020. 3, 20, 22, 27, 28\n\nYeshwanth Cherapanamjeri and Jelani Nelson. Uniform approximations for randomized hadamard transforms with applications. In STOC ’22: 54th Annual ACM SIGACT Symposium on Theory of Computing, pp. 659–671, 2022. 3, 20, 22, 27, 28\n\nKenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input sparsity\n\ntime. In Symposium on Theory of Computing Conference, STOC, pp. 81–90, 2013. 6, 7, 19\n\nKenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input sparsity\n\ntime. Journal of the ACM (JACM), 63(6):1–45, 2017. 6, 7\n\nItai Dinur, Uri Stemmer, David P. Woodruff, and Samson Zhou. On differential privacy and adaptive\n\ndata analysis with bounded space. CoRR, abs/2302.05707, 2023. 1\n\n11\n\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography, Third Theory of Cryptography Conference, TCC, Proceedings, pp. 265–284, 2006. 13\n\nCynthia Dwork, Guy N. Rothblum, and Salil P. Vadhan. Boosting and differential privacy. In 51th\n\nAnnual IEEE Symposium on Foundations of Computer Science, FOCS, pp. 51–60, 2010. 14\n\nCynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Leon Roth. Preserving statistical validity in adaptive data analysis. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC, pp. 117–126. ACM, 2015. 14\n\nAnna C. Gilbert, Brett Hemenway, Martin J. Strauss, David P. Woodruff, and Mary Wootters. Reusable low-error compressive sampling schemes through privacy. In IEEE Statistical Signal Processing Workshop, SSP, pp. 536–539, 2012. 1\n\nGene H Golub and Charles F Van Loan. Matrix computations. Johns Hopkins University Press, 2013.\n\n8\n\nIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In 3rd International Conference on Learning Representations, ICLR, Conference Track Proceedings, 2015. 1\n\nMoritz Hardt, Nimrod Megiddo, Christos H. Papadimitriou, and Mary Wootters. Strategic classification. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, pp. 111–122, 2016. 1\n\nAvinatan Hassidim, Haim Kaplan, Yishay Mansour, Yossi Matias, and Uri Stemmer. Adversarially robust streaming algorithms via differential privacy. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS, 2020. 1, 2, 3, 5, 9, 14, 16, 17\n\nPiotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation.\n\nJ. ACM, 53(3):307–323, 2006. 18\n\nShunhua Jiang, Binghui Peng, and Omri Weinstein. Dynamic least-squares regression. arXiv preprint\n\narXiv:2201.00228, 2022. 3\n\nHaim Kaplan, Yishay Mansour, Kobbi Nissim, and Uri Stemmer. Separating adaptive streaming from oblivious streaming using the bounded storage model. In Advances in Cryptology - CRYPTO 2021 - 41st Annual International Cryptology Conference, CRYPTO, Proceedings, Part III, pp. 94–121, 2021. 1, 5\n\nPing Li. Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable random projections. In Shang-Hua Teng (ed.), Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA, pp. 10–19, 2008. 18\n\nYanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In 5th International Conference on Learning Representations, ICLR, Conference Track Proceedings, 2017. 1\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR, 2018. 1\n\nMichael W Mahoney. Randomized algorithms for matrices and data. Foundations and Trends® in\n\nMachine Learning, 3(2):123–224, 2011. 14\n\nIlya Mironov, Moni Naor, and Gil Segev. Sketching in adversarial environments. SIAM J. Comput.,\n\n40(6):1845–1870, 2011. 1\n\nMoni Naor and Eylon Yogev. Bloom filters in adversarial environments. ACM Trans. Algorithms, 15\n\n(3):35:1–35:30, 2019. 1\n\n12\n\nNicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. CoRR, abs/1605.07277, 2016. 1\n\nNicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, AsiaCCS, pp. 506–519, 2017. 1\n\nTamas Sarlos. Improved approximation algorithms for large matrices via random projections. In 47th annual IEEE symposium on foundations of computer science (FOCS), pp. 143–152, 2006. 6, 7\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In 2nd International Conference on Learning Representations, ICLR, Conference Track Proceedings, 2014. 1\n\nCsaba D Toth, Joseph O’Rourke, and Jacob E Goodman. Handbook of discrete and computational\n\ngeometry. CRC press, 2017. 19\n\nDavid Wajc. Rounding dynamic matchings against an adaptive adversary. In Proccedings of the 52nd\n\nAnnual ACM SIGACT Symposium on Theory of Computing, STOC, pp. 194–207, 2020. 1\n\nDavid P Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends® in\n\nTheoretical Computer Science, 10(1–2):1–157, 2014. 2, 6, 7, 14, 15\n\nDavid P. Woodruff and Samson Zhou. Tight bounds for adversarially robust streams and sliding windows via difference estimators. In 62nd IEEE Annual Symposium on Foundations of Computer Science, FOCS, pp. 1183–1196, 2021. 1\n\nDavid P. Woodruff, Fred Zhang, and Samson Zhou. Streaming algorithms for learning with experts:\n\nDeterministic versus robust, 2023. 1\n\nVladimir M Zolotarev. One-dimensional stable distributions, volume 65. American Mathematical\n\nSoc., 1986. 19\n\nA PRELIMINARIES\n\nIn this paper, we use [n] for a positive integer n > 0 to denote the set {1, . . . , n}. We Notations use poly(n) to denote a fixed polynomial in n. We say an event occurs with high probability if it occurs with probability 1 − 1 poly(n) . For real numbers a, b and positive ε, we say a = (1 ± ε)b if (1 − ε)b ≤ a ≤ (1 + ε)b. Let ei ∈ Rn be the i’th standard basis vector. Let X+ denote the Moore-Penrose pseudo-inverse of matrix X. Let ∥X∥ denote the operator norm of X. Let κ(X) = ∥X+∥ ∥X∥ denote the condition number of X.\n\nA.1 DIFFERENTIAL PRIVACY\n\nMuch of our technical results leverage tools from DP. We recall its definition and several key statements.\n\nDefinition A.1 (Differential privacy, Dwork et al. (2006)). Given ε > 0 and δ ∈ (0, 1), a randomized algorithm A : X ∗ → Y is (ε, δ)-differentially private if, for every neighboring datasets S and S′ and for all E ⊆ Y,\n\nPr [A(S) ∈ E] ≤ eε · Pr [A(S′) ∈ E] + δ.\n\nTheorem A.2 (Amplification via sampling, e.g., Bun et al. (2015)). Let A be an (ε, δ)-differentially private algorithm for ε ≤ 1, δ ∈ (0, 1). Given a database S of size n, let A′ be the algorithm that constructs a database T ⊂ S by subsampling (with replacement) s ≤ n 2 rows of S and outputs A(T ). Then A′ is (ε′, δ′)-differentially private for\n\nε′ =\n\n6εk n\n\n,\n\nδ′ = exp(6εk/n)\n\n4kδ n\n\n.\n\n13\n\nTheorem A.3 (Private median, e.g., Hassidim et al. (2020)). Given a database D ∈ X ∗, there exists an (ε, 0)-differentially private algorithm PrivMed that outputs an element x ∈ X such that with probability at least 1 − δ, there are at least |S| 2 − k elements in S that are at least x, and at least\n\n|S|\n\n2 − k elements in S in S that are at most x, for k = O Theorem A.4 (Advanced composition, e.g., Dwork et al. (2010)). Let ε, δ′ ∈ (0, 1] and let δ ∈ [0, 1]. Any mechanism that permits k adaptive interactions with mechanisms that preserve (ε, δ)-differential\n\nδ\n\n.\n\n(cid:16) 1\n\nε log |X|\n\n(cid:17)\n\nprivacy guarantees (ε′, kδ + δ′)-differential privacy, where ε′ =\n\n(cid:113)\n\n2k ln 1\n\nδ′ · ε + 2kε2.\n\nTheorem A.5 (Generalization of DP, e.g., Dwork et al. (2015); Bassily et al. (2021)). Let ε ∈ (0, 1/3), δ ∈ (0, ε/4), and n ≥ 1 δ . Suppose A : X n → 2X is an (ε, δ)-differentially private algorithm that curates a database of size n and produces a function h : X → {0, 1}. Suppose D is a distribution over X and S is a set of n elements drawn independently and identically distributed from D. Then\n\nε2 log 2ε\n\nPr S∼D,h←A(S)\n\n(cid:34)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 |S|\n\n(cid:88)\n\nx∈S\n\nh(x) − E\n\nx∼D\n\n(cid:12) (cid:12) (cid:12) [h(x)] (cid:12) (cid:12)\n\n(cid:35)\n\n≥ 10ε\n\n<\n\nδ ε\n\n.\n\nA.2 NUMERICAL LINEAR ALGEBRA\n\nOur results on dynamic regression relies upon some tools in numerical linear algebra. We first recall the dimensionality reduction techniques.\n\nTheorem A.6 (Johnson-Lindenstrauss transformation, ε-JL). Given ε > 0, there exists a family of random maps Πm,d ∈ Rm×d with m = O (cid:0) 1\n\n(cid:1) such that for any x ∈ Rd, we have\n\nε2\n\nPr Π∼Πm,d\n\n[(1 − ε)∥x∥2 ≤ ∥Πx∥2 ≤ (1 + ε)∥x∥2] ≥\n\n3 4\n\n.\n\nMoreover, Πx takes O (cid:0) d Theorem A.7 (Fast JL). Given ε > 0, there exists a family of random maps Πm,d ∈ Rm×d with m = O\n\nsuch that for any x ∈ Rd, we have\n\n(cid:1) time to compute.\n\n(cid:17)\n\nε2\n\n(cid:16) log d ε2\n\nPr Π∼Πm,d\n\n[(1 − ε)∥x∥2 ≤ ∥Πx∥2 ≤ (1 + ε)∥x∥2] ≥\n\n3 4\n\n.\n\nMoreover, Πx takes O\n\nε2 + d log d\n\ntime to compute.\n\n(cid:16) log d\n\n(cid:17)\n\nA row sampling matrix S has rows that are multiples of natural basis vectors, so that SA is a (weighted) sample of the rows of A. A column sampling matrix is defined similarly. The size of a row/column sampling matrix is defined as the number of rows/columns it samples. The leverage score of the ith row a⊤\n\ni of A is\n\nτi(A) def= a⊤\n\ni\n\n(cid:0)A⊤A(cid:1)+\n\nai.\n\nFor a survey on leverage score and applications, we refer the reader to Mahoney (2011).\n\nDefinition A.8 (Leverage score sampling). Let u be a vector of leverage score overestimates, i.e., τi(A) ≤ ui. Let α be a sampling rate parameter and c be a fixed positive constant. For each row, we define a sampling probability pi = min {1, α · uic log d}. The leverage score sampling matrix is a row sampling matrix S with independently chosen entries such that Sii = 1√ with probability pi pi and 0 otherwise.\n\nDefinition A.9 (Subspace embedding). A (1 ± ε) l2 subspace embedding for the column space of an n × d matrix A is a matrix S for which for all x ∈ Rd\n\n∥SAx∥2\n\n2 = (1 ± ε)∥Ax∥2 2.\n\nTheorem A.10 (Leverage sampling implies subspace embedding, Theorem 17 of Woodruff (2014)). Let α = ε−2 and c be a sufficiently large constant. With high probability, the leverage score sampling matrix is a (1 ± ε) l2 subspace embedding. Furthermore, it has size O (cid:0)d log d/ε2(cid:1).\n\n14\n\nThe approximate leverage scores can be computed in input-sparsity time. Afterwards, repeated sampling from the leverage score distribution can be done efficiently using the binary tree data structure in quantum-inspired numerical linear algebra.\n\nLemma A.11 (Leverage score computation and sampling data structure; see Woodruff (2014); Chepurko et al. (2022)). Let A ∈ Rn×d. There exists an algorithm that given A outputs a vector of row leverage score overestimates with high probability and in run-time (cid:101)O (nnz(A) + poly(d)).\n\nFurthermore, there exists a sampling data structure DLS that stores the row leverage scores of A such that given a positive integer m ≤ n, returns a leverage score sample of A of size m in O (m log(mn)) time. In total, the pre-processing takes O (nnz(A) + poly(d)) time.\n\nB ADDITIONAL DETAILS ON DYNAMIC REGRESSION\n\nTheorem 3.4. [Main theorem; dynamic maintenance of regression cost] Let ε ∈ (0, 1/4) be an error parameter and b(1) be the initial target label. Given ε, A, b(1), a stream of T adaptively chosen, K-sparse updates to the label, Algorithm 4 outputs an estimate (cid:98)Fi such that (cid:98)Fi = (1 ± ε)F (A, b(i)) for all i with high probability.\n\nFurthermore, the algorithm requires a preprocessing step in time (cid:101)O (nnz(A) + poly(d)). The amortized update time of the algorithm is\n\n(cid:16)(cid:112)K nnz(A)\n\n(cid:16)(cid:112)κ(A) + ε−3(cid:17)(cid:17)\n\n(cid:101)O\n\nper round.\n\nProof. We focus on any fixed epoch of T iterations. Let {Ai}Γ i=1 be the collection of Γ data structures maintained by the Algorithm 4 and Ti be the transcript between Algorithm 4 and the adversary at round i, consisting of the algorithm’s output and the update requested by the adversary.\n\nTo handle a sequence of T adaptive queries, consider the transcript T (R) = {T1, . . . , TT }, where R denotes the internal randomness of Algorithm 4. Note that for a fixed iteration, Ti is (cid:16) -differentially private with respect to the algorithms A1, . . . , AΓ, since the pri-\n\n1√\n\n, 0\n\nO\n\n(cid:16)\n\n(cid:17)\n\n(cid:17)\n\nT log(nT )\n\nvate median algorithm PrivMed is\n\n(cid:16)\n\n(cid:16)\n\nO\n\n1√\n\nT log(nT )\n\n(cid:17)\n\n, 0\n\n(cid:17)\n\n-differentially private. By the advanced com-\n\nposition of differential privacy, i.e., Theorem A.4, the transcript T is private with respect to the algorithms A1, . . . , AΓ.\n\n(cid:16)\n\nO (1) ,\n\n1 poly(n)\n\n(cid:17)\n\n-differentially\n\nAlgorithm 4 runs Γ instances of the data structure with error parameter ε. For any given round i ∈ [T ], we say that an instance j ∈ [Γ] is correct if its output fi,j is within a (1 ± ε) factor of F (A, b(i)) and incorrect otherwise. For a fixed i, let Yj be the indicator variable for whether fi,j is correct.\n\nFrom the generalization properties of differential privacy, i.e., Theorem A.5, we have that for any fixed iteration i,\n\n(cid:12) (cid:12) (cid:12) Yj − E [Y ] (cid:12) (cid:12) (cid:12) where Y denotes the indicator random variable for whether a random instance of the algorithm A (not necessarily restricted to the m instances maintained by the algorithm) is correct at the given round i. Since a random instance A has randomness that is independent of the adaptive update, then E [Y ] ≥ 3\n\n4 . Therefore, by a union bound over all T rounds, we have\n\n1 poly(m, T )\n\n(cid:12) \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\n1 10\n\n <\n\n1 Γ\n\n(cid:88)\n\nj∈[Γ]\n\nPr\n\n≥\n\n\n\n,\n\n\n\n\n\n1 Γ\n\nPr\n\n(cid:88)\n\ni∈[Γ]\n\n\n\nYi > 0.6\n\n > 1 −\n\n1 poly(m, T )\n\n,\n\nwhich implies that the output on the ith round is correct with probability at least 1 − poly(m,T ) , since T = d. Then by a union bound over i ∈ [T ] for all T rounds within an epoch, we have that the data structure answers all T queries with probability 1 − 1 m2 , under the adaptively chosen updates. Finally,\n\n1\n\n15\n\n(cid:16)√\n\n(cid:17)\n\nby a union bound over all m updates, we have that the algorithm succeeds with probability at least 1 − 1\n\nm .\n\nWe now analyze the run-time of the algorithm. The preprocessing time follows from the guarantee of Lemma A.11. For update time, we amortize over each epoch. Within an epoch, we invoke Γ =\n\nO\n\nT log(nT )\n\ncopies of the data structure in Section 3.2, and so we consider the preprocessing\n\nand update time from there and amortize over the epoch length T . By Lemma 3.1, each copy takes\n\n(cid:16)(cid:112)κ(A) nnz(A) log 1 β = O update, each copy takes O (cid:0) K\n\nevery epoch of length T = O\n\nε + nnz(A)\n\nε2\n\nlog n + d\n\nε2 log n\n\n(cid:17)\n\ntime to pre-process. For every step of\n\nε2 log n(cid:1) time by Lemma 3.2. Therefore, the amortized update time for (cid:16) nnz(A) ε2K\n\nis\n\n(cid:17)\n\nO\n\n(cid:18) 1 T\n\n(cid:18)\n\nΓβ + ΓT\n\n(cid:19)(cid:19)(cid:19)\n\n(cid:18) K\n\nε2 log n\n\n= (cid:101)O\n\n(cid:16)(cid:112)K nnz(A)\n\n(cid:16)(cid:112)κ(A) + ε−3(cid:17)(cid:17)\n\n.\n\nThis completes the proof.\n\nB.1 AN EXACT AND DETERMINISTIC ALGORITHM\n\nWe now give a simple deterministic algorithm for the dynamic regression problem based on an SVD trick. Let A = UΣV⊤ be the SVD of A, where U ∈ Rn×d, Σ ∈ Rd×d and V ∈ Rd×d. The starting observation is that for any solution vector x, we can write the regression cost as (cid:13)UΣV⊤x − b(cid:13)\n\n(cid:13)ΣV⊤x − U⊤b(cid:13) (cid:13) ,\n\n∥Ax − b∥ = (cid:13)\n\n(cid:13) = (cid:13)\n\n(B.1)\n\nsince U is orthonormal. The goal is the maintain the solution vector x = A†b and the associated right-side quantity (cid:13)\n\n(cid:13)ΣV⊤x − U⊤b(cid:13) (cid:13).\n\nNow suppose we compute A† ∈ Rd×n and U⊤ ∈ Rd×n in pre-processing, and A†b(1) and U⊤b(1) in the first round. Then since all subsequent updates to b are all K-sparse, we only pay O(dK) time per step to maintain A†b(i) and U⊤b(i).\n\nAlgorithm 5 A simple SVD-based algorithm for dynamic regression\n\nInput: Design matrix A ∈ Rn×d, its pseudoinverse A† ∈ Rd×n and its SVD A = UΣV⊤, a\n\nsequence of labels b(i) ∈ Rn\n\n1: Compute and store SVD A = UΣV⊤, where U ∈ Rn×d, Σ ∈ Rd×d, V ∈ Rd×d 2: Compute and store A† from the SVD. 3: for each update b(i) do 4: 5:\n\nUpdate and store x(i) = A†b(i) Update and store U⊤b(i) Output Fi = (cid:13)\n\n(cid:13)ΣV⊤x(i) − U⊤b(i)(cid:13) 2\n(cid:13) 2\n\n6:\n\n▷In the 1st-round, compute and store A†b(1), U⊤b(1).\n\nThe algorithm is formally given by Algorithm 5. Observe that the algorithm always maintains the exact optimal regression cost. Moreover, the procedure does not require any randomness, and therefore it is adversarially robust to adaptive inputs. We formally claim the following guarantees of the algorithm. Theorem B.1 (Deterministic maintenance of regression costs). Given A, b(1) and a stream of adaptively chosen, K-sparse updates to the label, Algorithm 5 takes O(dK) time to update and maintain the exact regression cost F (A, b(i)) at all iterations i. The pre-processing requires an SVD of A, in O(n2d) time.\n\nC A FRAMEWORK FOR ADVERSARIAL ROBUSTNESS\n\nstructure by using (cid:101)O (cid:0)√\n\nIn this section, we describe the benchmark framework that enables Q adaptive queries to a data Q(cid:1) copies of a non-adaptive data structure. The framework and corresponding analysis of correctness are simply compartmentalizations of the techniques in Hassidim et al. (2020);\n\n16\n\nBeimel et al. (2022); Attias et al. (2023). For the sake of completeness, we include them here and discuss additional applications. Namely, we show that through advanced composition of differential Q(cid:1) copies protects the internal randomness of each non-adaptive data structure while still adding sufficiently small noise to guarantee accuracy. Moreover, we use Q(cid:1) non-adaptive\n\nprivacy, the private median of (cid:101)O (cid:0)√ amplification of privacy by sampling to only consider a small subset of the (cid:101)O (cid:0)√\n\ndata structures to further improve the runtime.\n\nAlgorithm 6 Adaptive Algorithm Interaction\n\n1: r ← O (cid:0)√\n\nQ log2(nQ)(cid:1), k ← O (log(nQ))\n\nImplement data structure Di on the input\n\n2: for i ∈ [r] do 3: 4: for each query qi, i ∈ [Q] do 5: 6: 7:\n\nLet S be a set of k indices sampled (with replacement) from [r] For each j ∈ [k], let di,j be the output of DSj on query qi di ← PrivMed({di,j}j∈[k]), where PrivMed is (1, 0)-DP\n\nWe first argue that Algorithm 6 maintains accuracy against Q rounds of interaction with an adaptive adversary. Let R = {R(0), R(1), . . . , R(r)}, where R(1), . . . , R(r) denotes the random strings used by the oblivious data structures D1, . . . , Dr and R(0) denotes the additional randomness used by Algorithm 6, such as in the private median subroutine PrivMed. Consider a transcript T (R) = {T1, . . . , TQ} such that for each i ∈ [Q], we define Ti = (qi, di) to be the ordered pair consisting of the query qi and the corresponding answer di by Algorithm 6 using the random string R(0), as well as the oblivious data structures D1, . . . , Dr with random strings R(1), . . . , R(r). We remark that di is a random variable due to the randomness of each data structure, as well as the randomness of the private median subroutine PrivMed.\n\nWe will first argue that the transcript TR is differentially private with respect to R. We emphasize that similar arguments were made in the streaming model by Hassidim et al. (2020) and in the dynamic model Beimel et al. (2022); Attias et al. (2023).\n\nLemma C.1. For a fixed iteration, Ti is R.\n\n(cid:16)\n\n(cid:16)\n\nO\n\n1√\n\nQ log(nQ)\n\n(cid:17)\n\n(cid:17)\n\n, 0\n\n-differentially private with respect to\n\nO (cid:0)√\n\nProof. We first observe that PrivMed is (1, 0)-differentially private on the outputs of the r = Q log2(nQ)(cid:1) data structures. Algorithm 6 samples k = O (log(nQ)) groups of data structures from the r total data structures. Thus by amplification via sampling, i.e., Theorem A.2, PrivMed\n\n(cid:16)\n\n(cid:16)\n\nis\n\nO\n\n1√\n\nQ log(nQ)\n\n(cid:17)\n\n(cid:17)\n\n, 0\n\n-differentially private. Therefore, Ti is\n\n(cid:16)\n\n(cid:16)\n\nO\n\n1√\n\nQ log(nQ)\n\n(cid:17)\n\n(cid:17)\n\n, 0\n\n-differentially\n\nprivate with respect to R.\n\nWe next argue that the entire transcript is differentially private with respect to the randomness R.\n\nLemma C.2. T is\n\n(cid:16)\n\nO (1) ,\n\n1 poly(nQ)\n\n(cid:17)\n\n-differentially private with respect to R.\n\nProof. By Lemma C.1, for each fixed iteration i ∈ [Q], the transcript Ti is differentially private with respect to R. Note that the transcript T is an adaptive composition of the transcripts T1, . . . , TQ. Thus, by the advanced composition of differential privacy, i.e., Theorem A.4,\n\nQ log(nQ)\n\n, 0\n\nO\n\n-\n\n(cid:16)\n\n(cid:16)\n\n1√\n\n(cid:17)\n\n(cid:17)\n\nthe transcript T is\n\nO (1) ,\n\n-differentially private with respect to R.\n\n(cid:16)\n\n(cid:17)\n\n1 poly(nQ)\n\nWe now prove the correctness of our unifying framework.\n\nProof of Theorem 1.2: For a fixed query qi with i ∈ [Q], let S be the corresponding set of k indices sampled from [r]. Let V be the set of valid answers on query qi. Let Ij be an indicator variable\n\n17\n\nfor whether the output di,j on query qi by DSj is correct, so that Ij = 1 if di,j ∈ V and Ij = 0 if di,j /∈ V. By assumption, we have that for each j ∈ [k],\n\nPr [Ij = 1] ≥\n\nso that E [Ij] ≥ 3 E [I] = 1\n\n(cid:80)\n\nj∈[k]\n\nk\n\nE [Ij] ≥ 3 4 .\n\n4 . We define the random variable I = 1\n\nk\n\n,\n\n3 4\n(cid:80)\n\nj∈[k] Ij so that by linearity of expectation,\n\nTo handle a sequence of Q adaptive queries, we consider the transcript T (R) = {T1, . . . , TQ} for the randomness R = {R(0), R(1), . . . , R(r)} previously defined, i.e., for each i ∈ [Q], Ti = (qi, di) is the ordered pair consisting of the query qi and the corresponding answer di by Algorithm 6 using the random string R(0), as well as the oblivious data structures D1, . . . , Dr with random strings R(1), . . . , R(r). By Lemma C.2, we have that T is -differentially private with respect to R.\n\n1 poly(nQ)\n\nO (1) ,\n\n(cid:16)\n\n(cid:17)\n\nFor j ∈ [k], we define the function success(R(Sj )) to be the indicator variable for whether the output di,Sj by data structure DSj is successful on query qi. For example, if D is supposed to answer queries within (1 + α)-approximation, then we define success(R(Sj )) to be one if di,Sj is within a (1 + α)-approximation to the true answer on query qi, and zero otherwise. From the generalization properties of differential privacy, i.e., Theorem A.5, we have\n\nPr\n\n(cid:12) \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\n1 k\n\n(cid:88)\n\nj∈[k]\n\nsuccess(R(Sj )) − E\n\nR\n\n(cid:12) (cid:12) (cid:12) (cid:2)success(R)(cid:3) (cid:12) (cid:12) (cid:12)\n\n\n\n <\n\n≥\n\n1 10\n\n1 poly(n, Q)\n\n,\n\nfor sufficiently small O (1). Therefore, by a union bound over all Q queries, we have\n\n\n\n\n\n1 k\n\nPr\n\n(cid:88)\n\ni∈[k]\n\n\n\nIi > 0.6\n\n > 1 −\n\n1 poly(n, Q)\n\n,\n\nwhich implies that di is correct on query qi. Then by a union bound over i ∈ [Q] for all Q adaptive □\nqueries, we have that the data structure answers all Q adaptive queries with high probability.\n\nD APPLICATIONS OF OUR FRAMEWORK\n\nTheorem 1.2 has applications to a number of central problems in data science and machine learning. In this section, we formally describe the range queries, point queries, matrix-vector norm queries, and linear regression problems; we defer discussion of adaptive distance estimation, kernel density estimation, and nearest neighbor search to the the appendix.\n\nD.1 APPLICATION: MATRIX-VECTOR NORM QUERIES\n\nIn the matrix-vector norm query problem, we are given a matrix A ∈ Rn×d and we would like to handle Q adaptive queries x(1), . . . , x(Q) for an approximation parameter ε > 0 by outputting a (1 + ε)-approximation to ∥Ax(i)∥p for each query x(i) ∈ Rd with i ∈ [Q]. Here we define i∈[d] |vi|p for a vector v ∈ Rd. Observe that computing Ax(i) explicitly and then ∥v∥p computing its p-norm requires O (nd) time. Thus for n ≫ d, a much faster approach is to produce a subspace embedding, i.e., to compute a matrix M ∈ Rm×d with m ≪ n, such that for all x ∈ Rd,\n\np = (cid:80)\n\n(1 − ε)∥Ax∥p ≤ ∥Mx∥p ≤ (1 + ε)∥Ax∥p.\n\n(cid:1) due to requiring correctness over an ε-net.\n\nHowever, because subspace embeddings must be correct over all possible queries, the number of rows of M is usually m = Ω (cid:0) d Theorem D.1 (Indyk (2006); Li (2008)). Given A ∈ Rn×d, p ∈ (0, 2], and an accuracy parameter ε > 0, there exists an algorithm that creates a data structure that uses O (cid:0) 1 ε2 log n(cid:1) bits of space and outputs a (1 + ε)-approximation to ∥Ax∥p for a query x ∈ Rd, with high probability, in time O (cid:0) d\n\nε2\n\nε2 log n(cid:1).\n\n18\n\nTheorem D.1 essentially creates a matrix R ∈ Rm×n of random variables sampled from p-stable distribution (Zolotarev, 1986) and then stores the matrix RA. Once the query x arrives, the data structure then outputs a (1 + ε)-approximation to ∥Ax∥p by computing a predetermined function on RAx. The restriction on p ∈ (0, 2] is due to the fact that p-stable distributions only exist for p ∈ (0, 2]. From Theorem D.1 and Theorem 1.2, we have the following: Theorem D.2. Given A ∈ Rn×d, p ∈ (0, 2], and an accuracy parameter ε > 0, there exists an\n\nalgorithm that creates a data structure that uses O bits of space and outputs a (1 + ε)-approximation to ∥Ax(i)∥p with i ∈ [Q] for Q adaptive queries x(1), . . . , x(Q) ∈ Rd, with high probability, in time (cid:101)O (cid:0) d\n\nε2 log2(nQ) + log3(nQ)(cid:1).\n\nQ\n\nε2 log2(nQ)\n\n(cid:16) √\n\n(cid:17)\n\nD.2 APPLICATION: LINEAR REGRESSION\n\nIn the linear regression problem, we are given a fixed matrix A ∈ Rn×d and we would like to handle Q adaptive queries b(1), . . . , b(Q), for an approximation parameter ε > 0, by outputting a (1 + ε)-approximation to minx∈Rd ∥Ax − b(i)∥2 for each query b(i) ∈ Rn with i ∈ [Q]. For linear regression, we can again compute a subspace embedding M = SA ∈ Rm×n and answer a query b(i) by approximately solving minx∈Rd ∥SAx − Sb(i)∥2, where S is a sketching matrix (Clarkson & Woodruff, 2013). Theorem D.3 (Clarkson & Woodruff (2013)). Given A ∈ Rn×d, b ∈ Rn, and an accuracy (cid:17) parameter ε > 0, there exists an algorithm that creates a data structure that uses O\n\nε2 log2(nQ) bits of space and outputs a (1 + ε)-approximation to minx∈Rd ∥Ax − b∥2 with high probability.\n\n(cid:16) d2\n\nHowever, this may fail for multiple interactions with the data structure. For example, suppose the adversary learns the kernel of S. Then the adversary could query some vector b(i) in the kernel of S so that Sb(i) is the all zeros vector, so that the output is the all zeros vector of dimension d, which could be arbitrarily bad compared to the actual minimizer. Thus the na ̈ıve approach is to maintain Q subspace embeddings, one for each query, resulting in a data structure with space (cid:101)O comparison, Theorem D.3 and Theorem 1.2 yield the following: Theorem D.4. Given A ∈ Rn×d and an accuracy parameter ε > 0, there exists an algorithm bits of space and with high probability, that creates a data structure that uses O\n\nlog3(nQ)\n\n(cid:16) Qd ε2\n\n. By\n\n(cid:16) √\n\n(cid:17)\n\n(cid:17)\n\nQd2 ε2\n\noutputs (1 + ε)-approximations to minx∈Rd ∥Ax − b(i)∥2 for Q adaptive queries b(1), . . . , b(Q).\n\nD.3 APPLICATION: HALF-SPACE QUERIES\n\nGiven a set P of n points in Rd, the range query or search problem asks us to pre-process P so that given a region R, chosen from a predetermined family, one can quickly count or return the points in P ∩ R. This is an extremely well-studied class of problems in computational geometry Toth et al. (2017) and the case where the regions R are hyperplanes (also called half-spaces) is of special interest since many algebraic constraints can be “lifted” to be hyperplanes in a higher dimension.\n\nUnfortunately, exact versions of the problem are known to have the “curse of dimensionality” and suffer from exponential dependence on d in the query time (Br ̈onnimann et al., 1993; Chazelle, 2000). Nonetheless, Chazelle et al. (2008) gave a data structure capable of answering hyperplane queries approximately with polynomial query time. Their notion of approximation is as follows: given a set of points P in the unit l2 ball, hyperplane R, and ε > 0, we return the number of points that are on a given side of the hyperplane R up to additive error equal to the number of points in P which lie within distance ε of the boundary of R. We will refer to this query as an ε-approximate hyperplane query. Chazelle et al. (2008) proved the following theorem. Theorem D.5 (Chazelle et al. (2008)). Given a set of points P that lie in the unit l2 ball, there such that any ε-approximate exists a data structure that pre-processes P using space (cid:101)O hyperplane range query is answered correctly with high probability. The query time is (cid:101)O (cid:0)d/ε2(cid:1).\n\ndnO(ε−2)(cid:17)\n\n(cid:16)\n\nThe data structure of Chazelle et al. (2008) is randomized and in particular employs randomized dimensionality reduction. Thus, it is feasible that queries might fail for multiple adaptive interactions\n\n19\n\nwith the data structure. By utilizing our framework of Section C and Theorem 1.2, we can obtain the following robust guarantee. Theorem D.6. Given a set of points P that lie in the unit l2 ball, there exists a data structure which such that Q adaptive ε-hyperplane range queries pre-processes P using space (cid:101)O are answered correctly with high probability. The query time is (cid:101)O (cid:0)d/ε2(cid:1).\n\nQdnO(ε−2)(cid:17)\n\n(cid:16)√\n\nD.4 APPLICATION: POINT QUERIES ON TURNSTILE STREAMS\n\nIn the problem of point queries on turnstile streams, there exists a stream of m updates. Each update specifies a coordinate i ∈ [n] of an underlying frequency vector f ∈ Rn and changes fi by some amount between ∆i ∈ [−∆, ∆], where ∆ = poly(n). Given any constant accuracy parameter ε > 0 any time t ∈ [m], we define f (t) to be the frequency vector implicitly defined after the first t updates. Then the point query problem is to output f (t) for various choices of t ∈ [m] and i ∈ [n] within an additive error of ε∥f (t)∥1. Theorem D.7 (Alman & Yu (2020)). There exists an algorithm that uses space O (cid:0)log2 n(cid:1) bits, worst-case update time O (cid:0)log0.582 n(cid:1), and query time O (cid:0)log1.582 n(cid:1), that supports point queries with ε = 0.1 with high probability.\n\ni\n\nAn important quality of Theorem D.7 is that it significantly improves the update time over previous data structures, e.g., Charikar et al. (2004), at a cost in query time. By applying Theorem 1.2, we can avoid a blow-up in query time while still enjoying the update time improvements:\n\nTheorem D.8. There exists an algorithm that uses space O (cid:0)√ update time O (cid:0)√\n\nQ log3(nQ)(cid:1) bits, has worst-case Q log1.582(nQ)(cid:1) and query time (cid:101)O (cid:0)log3(nQ)(cid:1), and supports Q adaptive point\n\nqueries with ε = 0.1 and with high probability.\n\nE ADAPTIVE DISTANCE ESTIMATION\n\nIn the adaptive distance estimation problem, there exists a set X = {x(1), . . . , x(n)} of n points in Rd. Given an accuracy parameter ε > 0, the goal is to output a (1 + ε)-approximation to ∥x(i) − q∥p for each query q across all points x(i) ∈ X, while minimizing the space, query time, or pre-processing time for the corresponding data structures. The trivial solution stores all n points and computes all n distances to each query point and thus can handle an unlimited number of queries. Since each point has dimension d, the trivial solution uses space and query time O (nd). Cherapanamjeri & Nelson (2020) first improved the query time to (cid:101)O (cid:0) n+d\n\n(cid:1) at the cost of using (cid:101)O\n\nspace and\n\n(cid:16) (n+d)d ε2\n\n(cid:17)\n\nε2\n\n(cid:16) nd2\n\n(cid:17)\n\n(cid:101)O (2020) also permits an arbitrary number of queries.\n\nε2\n\npre-processing time. Like the trivial solution, the algorithm of Cherapanamjeri & Nelson\n\nIn this section, we first apply our framework to show a data structure that can handle Q queries of (cid:1), pre-processing time approximate distances from a specified point in X, using query time (cid:101)O (cid:0) n+d (cid:17) (cid:101)O the work of Cherapanamjeri & Nelson (2020).\n\nQ, our data structure already improves on\n\n. Hence for d ≫ n\n\n, and space (cid:101)O\n\n(cid:16) (n+d) ε2\n\n(cid:16) nd\n\n√\n\n(cid:17)\n\nε2\n\nε2\n\n√\n\n√\n\nQ\n\nQ\n\nHowever in this setting, each of the Q queries returns only the approximate distance between a query point and a single point in X. By comparison, Cherapanamjeri & Nelson (2020) outputs approximate distances to all points in X and moreover, follow-up work by Cherapanamjeri & Nelson (2022) (cid:1). Therefore, we address these two shortcomings of our improved the pre-processing time to (cid:101)O (cid:0) nd framework by giving a data structure that (1) handles the case where we return the approximate distances of all points in X from Q adaptive query points and (2) achieves pre-processing time (cid:101)O (cid:0) nd For completeness, we now show correctness of our algorithm across all Q adaptive queries, though we remark that the proof can simply be black-boxed into Theorem 1.2. Theorem E.1. With high probability, we have\n\n(cid:1).\n\nε2\n\nε2\n\n(1 − ε)∥xiq − yq∥2 ≤ di ≤ (1 + ε)∥xiq − yq∥2,\n\n20\n\nAlgorithm 7 Adaptive Distance Estimation\n\nQ log2(nQ)(cid:1), k ← O (log(nQ))\n\n1: r ← O (cid:0)√ 2: Let Π1, . . . , Πr ∈ Rm×d be a JL transformation matrix (see Theorem A.6 or Theorem A.7) 3: for j ∈ [r] do 4: 5: for each query (y, i) with y ∈ Rd, i ∈ [n] do 6: 7: 8:\n\nLet S be a set of k indices sampled (with replacement) from [r] for j ∈ [k] do\n\nCompute Πjxi\n\ndi,j ← ∥ΠSj (xi − y)∥2\n\n▷Adaptive queries\n\n9: 10:\n\ndi ← PrivMed({di,j}j∈[k]), where PrivMed is (1, 0)-DP. return di\n\nfor all q ∈ [Q].\n\nProof. Fix query (yq, iq) with q ∈ [Q] and iq ∈ [n]. Let S be a set of k indices sampled (with replacement) from [r]. By Theorem A.6 or Theorem A.7, then we have for each j ∈ [k],\n\nPr (cid:2)(1 − ε)∥xiq − yq∥2 ≤ ∥ΠSj (xiq − yq)∥2 ≤ (1 + ε)∥xiq − yq∥2\n\n(cid:3) ≥\n\n3 4\n\n.\n\nLet Ij be an indicator variable so that Ij = 1 if (1 − ε)∥xiq − yq∥2 ≤ ∥ΠSj (xiq − yq)∥2 ≤ (1 + ε)∥xiq − yq∥2 and Ij = 0 otherwise, so that we have Pr [Ij = 1] ≥ 3 4 , or equivalently, (cid:80) E [Ij] ≥ 3 E [Ij] ≥ 3 4 .\n\nj∈[k] Ij so that by linearity of expectation, E [I] = 1\n\n4 . Let I = 1\n\nj∈[k]\n\n(cid:80)\n\nk\n\nk\n\nthe r Fast JL transforms. Since we sample k = O (log(nQ)) groups from the r = O (cid:0)√\n\nTo address adaptive queries, we first note that PrivMed is (1, 0)-differentially private on the outputs of Q log2(nQ)(cid:1) groups with replacement, then by amplification via sampling, i.e., Theorem A.2, PrivMed is (cid:16)\n\n(cid:16)\n\n(cid:17)\n\n(cid:17)\n\n-differentially private. Thus, by the advanced composition of differential (cid:17)\n\n(cid:16)\n\nO\n\n1√\n\nQ log(nQ)\n\n, 0\n\nprivacy, i.e., Theorem A.4, the mechanism permits Q adaptive queries and is\n\nO (1) ,\n\n1 poly(nQ)\n\n-\n\ndifferentially private. By the generalization properties of differential privacy, i.e., Theorem A.5, we have\n\nPr\n\n(cid:12) \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\n1 k\n\n(cid:88)\n\nj∈[k]\n\nIj − E [I]\n\n≥\n\n1 10\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n <\n\n1 poly(Q, n)\n\n,\n\nfor sufficiently small O (1). Thus we have\n\n\n\n\n\n1 k\n\nPr\n\n(cid:88)\n\ni∈[k]\n\n\n\nIi > 0.6\n\n > 1 −\n\n1 poly(Q, n)\n\n,\n\nwhich implies that (1−ε)∥xiq −yq∥2 ≤ di ≤ (1+ε)∥xiq −yq∥2. Therefore, by a union bound across Q adaptive queries (yq, xiq ) with q ∈ [Q], we have that (1−ε)∥xiq −yq∥2 ≤ di ≤ (1+ε)∥xiq −yq∥2 for all q ∈ [Q] with high probability.\n\nTheorem E.2. There exists an algorithm that answers Q adaptive distance estimation queries within (cid:17)\n\n(cid:17)\n\n(cid:17)\n\n√\n\n(cid:16) n\n\nQ log3(nQ)\n\n(cid:16)(cid:16) log d\n\nε2 + d log d\n\nlog(nQ)\n\nquery time, it stores O\n\nε2\n\nε2 log(nQ)(cid:1) query time, it stores O\n\n√\n\n(cid:16) n\n\nQ log2(nQ)\n\n(cid:17)\n\nε2\n\nwords of space.\n\na factor of (1 + ε). For O words of space. For O (cid:0) d\n\nProof. By Theorem A.7, each fast JL transform uses O\n\n(cid:17)\n\n(cid:16) log d ε2\n\nO\n\nrows. On the other hand, by Theorem A.6, each JL transform uses O (cid:0) d\n\nruntime and stores m = ε2 + d log d(cid:1)\n\n(cid:16) log d\n\nε2 + d log d\n\n(cid:17)\n\nruntime and stores m = O\n\n(cid:17)\n\n(cid:16) log d ε2\n\nrows.\n\n21\n\nBy comparison, Cherapanamjeri & Nelson (2020) uses O\n\n(cid:16) nd log n ε2\n\n(cid:17)\n\nwords of space and O (cid:0) d\n\nε2\n\n(cid:1)\n\nquery time.\n\nE.1 FASTER PRE-PROCESSING TIME FOR ADAPTIVE DISTANCE ESTIMATION\n\nof Cherapanamjeri & Nelson (2020) with an improved space complexity of O(ε−2√\n\nIn this section, we present an improved algorithm for Adaptive Distance Estimation, which allows the release of distances to all n points in the dataset for a single query, matching the query time Qn). Our results utilize a class of structured randomized linear transformations based on Hadamard matrices recursively defined below:\n\nH1 = [1]\n\nHd =\n\n(cid:20)Hd/2 Hd/2 Hd/2 −Hd/2\n\n(cid:21)\n\n.\n\nThe associated class of randomized linear transformations are now defined below:\n\n{Dj}j∈[m] ⊂ Rd×d s.t Dj\n\nk,l\n\niid∼\n\n(cid:26)N (0, I)\n\n0\n\nif k = l otherwise\n\n∀z ∈ Rd : h(z) =\n\n\n\n \n\n\n· z.\n\n(SRHT)\n\n\n\n \n\n\nHdD1 HdD2 ... HdDm\n\nNote that for any vector z, h(z) may be computed in time O(md log d) due to the recursive definition of the Hadamard transform. We now let φ and Φ denote the pdf and cdf of a standard normal random variable, Quantα({ai}i∈[l]) the αth quantile of a multi-set of real numbers {ai}i∈[l] for any l ∈ N and define ψr as follows:\n\n∀r > 0, a ∈ R : ψr(a) := min(|a|, r).\n\nThrough the remainder of the section, we condition on the event defined in the following lemma: Lemma E.3 (Claims 5.1 and 5.2 Cherapanamjeri & Nelson (2022)). For any δ ∈ (cid:0)0, 1 probability at least 1 − δ:\n\n2\n\n(cid:1), with\n\n∀z s.t ∥z∥ = 1 : 2 ≤ Quantα−β/4\n\n∀z s.t ∥z∥ = 1, r ≥ 4(cid:112)log(1/ε) :\n\n(cid:1) ≤ Quantα+β/4 (cid:0){h(z)i}i∈[md] (cid:17) (cid:16) 1\nε md 2\n\n(cid:114) π 2\n\n1 −\n\n(cid:88)\n\n≤\n\n·\n\n·\n\ni∈[md]\n\n(cid:0){h(z)i}i∈[md]\n\n(cid:1) ≤ 4\n\nψr(hi(z)) ≤\n\n(cid:16)\n\n1 +\n\n(cid:17)\n\nε 2\n\nas long as m ≥ Cε−2 log(2/δ) log5(d/ε) for some absolute constant C > 0.\n\nWe will additionally require the following technical result from Cherapanamjeri & Nelson (2022), where for any vector v ∈ Rd and multiset S = {ij}j∈[k] with ij ∈ [d], vS denotes the vector [vi1 , . . . , vik ]: Lemma E.4 (Theorem 1.4 Cherapanamjeri & Nelson (2022)). Assume h : Rd → Rmd (SRHT) satisfies the conclusion of Lemma E.3. Then, there is an algorithm, RetNorm, which satisfies for all x ∈ Rd:\n\nPS {(1 − ε) · ∥x∥ ≤ RetNorm(h(x)S) ≤ (1 + ε) · ∥x∥} ≥ 1−δ for S = {ij}j∈[k] with ij\n\niid∼ Unif([md])\n\nwhen k ≥ Cε−2 log(2/ε) log(2/δ) for some C > 0. Furthermore, RetNorm runs in time O(k).\n\nWith these primitives, we will construct our data structure for adaptive distance estimation. Our constructions is formally described in Algorithm 8.\n\n22\n\nAlgorithm 8 Adaptive Distance Estimation with SRHTs\n\nQ log3(nd), k ← Cε−2 log(2/ε) log(2nd)\n\n√\n\n1: m ← Cε−2 log6(2dn/ε) 2: Let h be an SRHT as defined in SRHT 3: r ← C 4: for i ∈ [n] do 5: 6: 7: 8: l ← C log(nd) 9: for j ∈ 1 : Q do\n\nCompute yi = h(xi) for j ∈ [r] do\n\nLet Si,j be a set of k indices sampled with replacement from [md]\n\n▷Revealed to analyst\n\n▷Adaptive queries\n\n10: 11: 12: 13: 14: 15:\n\n16:\n\n17:\n\nReceive query qj vj ← h(qj) for i ∈ [n] do\n\nLet {ti,j,p}p∈[l] be a set of l indices sampled (with replacement) from [r] for p ∈ [l] do\n\ndi,j,p ← RetNorm((vj − yi)Si,ti,j,p\n\n)\n\ndi,j ← PrivMed({di,j,p}p∈[l]), where PrivMed is (O (1) , 0)-DP.\n\nreturn {di,j}i∈[n]\n\nThe proof of correctness of Algorithm 8 will follow along similar lines to that of Algorithm 6 with a more refined analysis of the privacy loss incurred due to the adaptivity of the data analyst. In particular, each input query results in n different queries made to a differentially private mechanism PrivMed leading to a total of nQ queries. A na ̈ıve application of Theorem 1.2 would thus result in a data structure with space complexity scaling as (cid:101)O(n3/2√ Q) and query complexity (cid:101)O(ε−2nd). The key insight yielding the improved result is the privacy loss incurred by a single query is effectively amortized across n independent differentially private algorithms each capable of answering Q adaptively chosen queries correctly with high probability.\n\nQ) as opposed to the desired (cid:101)O(n\n\n√\n\nTo start, we first condition on the event in Lemma E.3 and assume public access to the correspondingly defined SRHT h. We now use R to denote the randomness used to instantiate the multisets, Si,j, in Algorithm 8 and decompose it as follows R = {Ri}i∈[n] with Ri = {Ri,j}j∈[r] where Ri,j corresponds to the randomness used to generate the set Si,j and the random elements ti,p. As in the proof of Theorem 1.2, we define a transcript T = {Tj}j∈[Q] with Tj = (qj, {di,j}i∈[n]) denoting the jth query and the responses returned by Algorithm 8 as a single transaction.\n\nLemma E.5. For all i ∈ [n], j ∈ [Q], Tj is\n\n(cid:16)\n\n(cid:16)\n\no\n\n1√\n\nQ log(nQ)\n\n(cid:17)\n\n(cid:17)\n\n, 0\n\n-differentially private with Ri.\n\nProof. The proof is identical to that of Lemma C.1 with the observation that each transaction Tj only results in a single query to a differentially private mechanism operating on Ri.\n\nLemma E.6. For all i ∈ [n], T is\n\n(cid:16)\n\no(1),\n\n1 poly(nQ)\n\n(cid:17)\n\n-differentially private with respect to Ri.\n\nProof. The proof is identical to Lemma C.2 and follows from Theorem A.4 and Lemma E.5.\n\nWe now prove the correctness of our improved procedure for adaptive distance estimation.\n\nProof of Theorem 1.3: We condition on the event in the conclusion of Lemma E.3 start by bounding the failure probability of a single query. The bound for the whole sequence of adaptively chosen queries follows by a union bound. Now, fixing i ∈ [n] and j ∈ [Q], note that the sub-transcript T (j) = {Tp}p∈[j−1] is the indicator random variables:\n\n-differentially private with respect to Ri. Furthermore, define\n\n1 poly(nQ)\n\no(1),\n\n(cid:17)\n\n(cid:16)\n\n∀p ∈ [l] : Wp := 1\n\n(cid:110)\n\n(1 − ε) · ∥qj − xi∥ ≤ RetNorm\n\n(cid:16)\n\n(vj − yi)Si,ti,j,p\n\n(cid:17)\n\n≤ (1 + ε) · ∥qj − xi∥\n\n(cid:111)\n\n23\n\nAdditionally, defining W := (cid:80)l Lemma E.4 and Theorem A.5:\n\np=1 Wp, we get by the differential privacy of the sub-transcript, T (j),\n\n(cid:26)\n\nP\n\nW ≤\n\n(cid:27)\n\n· l\n\n≤\n\n3 4\n\n1\n\n400 · (nQ)2 .\n\nConsequently, we get from Theorem A.3 and another union bound:\n\nP {(1 − ε) · ∥qj − xi∥ ≤ di,j ≤ (1 + ε) · ∥qj − xi∥} ≥ 1 −\n\n1\n\n200 · (nQ)2 .\n\nA subsequent union bound over all i ∈ [n], j ∈ [Q] yields:\n\nP {∀i ∈ [n], j ∈ [Q] : (1 − ε) · ∥qj − xi∥ ≤ di,j ≤ (1 + ε) · ∥qj − xi∥} ≥ 1 −\n\n1 200 · (nQ)\n\n.\n\nA final union bound over the conclusion of Lemma E.3 concludes the proof. The runtime guarantees follow from the fact that for all z ∈ Rd, h(z) is computable in time O(md log d) and the runtime □\nguarantees of RetNorm.\n\nF ADAPTIVE KERNEL DENSITY ESTIMATION\n\n(cid:16) d√\n\n(cid:17)\n\n(cid:17)\n\n(cid:16) d\n\n√\n\nε2\n\nτ\n\nKernel density estimation is an important problem in learning theory and statistics that has recently attracted significant interest, e.g., (Charikar & Siminelakis, 2017; Backurs et al., 2018; Charikar et al., 2020; Bakshi et al., 2022). In the adaptive kernel density estimation problem, the input is a set X = {x(1), . . . , x(n)} of n points in Rd. Given an accuracy parameter ε > 0 and a threshold parameter τ > 0, the goal is to output a (1 + ε)-approximation to the quantity 1 i∈[n] k(x(i), q), for a kernel function k under the promise that the output is at least τ . A standard approach is to sample O (cid:0) 1 (cid:1) query time to output the empirical kernel density for a (cid:1) specific query. Backurs et al. (2019) give an algorithm for kernel density estimation that uses O (cid:0) 1\n\n(cid:1) points and then use O (cid:0) d\n\n(cid:80)\n\nτ ε2\n\nτ ε2\n\nn\n\nspace and O Theorem F.1. Backurs et al. (2019) Given ε, τ > 0, there exists a data structure D that uses O (cid:0) 1\n\nquery time, improving over the standard sampling approach.\n\nτ ε2\n\nτ ε2\n\n(cid:1)\n\nτ ε2\n\nspace and O\n\nquery time that outputs a (1 + ε)-approximation D(y) to a kernel density\n\nestimation query y that has value at least τ , i.e.,\n\nPr [|D(y) − KDE(X, y)| ≤ ε · KDE(X, y)] ≥\n\n3 4\n\n.\n\nHowever, the analysis for both these algorithms fails for the adaptive setting, where there can be dependencies between the query and the data structure. By using the data structure of Backurs et al. (2019) as a subroutine, our framework immediately implies an algorithm for adaptive kernel density estimation that uses (cid:101)O queries.\n\nquery time to answer each of Q adaptive\n\nspace and O\n\n(cid:16) d log Q√\n\nQ τ ε2\n\n(cid:16) √\n\nτ ε2\n\n(cid:17)\n\n(cid:17)\n\nAlgorithm 9 Adaptive Kernel Density Estimation\n\nInput: Number Q of queries, accuracy ε, threshold τ\n\n1: r ← O (cid:0)√\n\nQ log2 Q(cid:1)\n\n2: for i ∈ [r] do Let Ti be a KDE data structure 3: 4: for each query yq ∈ Rd with q ∈ [Q] do 5: 6: 7:\n\nLet Di be the output of TSi on query yq\n\nLet S be a set of k indices sampled (with replacement) from [r] for i ∈ [k] do\n\n▷Pre-processing\n\n▷Adaptive queries\n\n8:\n\nreturn dq = PrivMed({Di}i∈[k]), where PrivMed is (1, 0)-DP.\n\nFor completeness, we now show adversarial robustness of our algorithm across Q adaptive queries. Again we remark that the proof can simply be black-boxed into Theorem 1.2, though we include the specific kernel density details in the following proof as a warm-up for the following section.\n\n24\n\nLemma F.2. Algorithm 9 answers Q adaptive kernel density estimation queries within a factor of (1 + ε), provided each query has value at least τ .\n\nProof. Fix query yq ∈ Rd with q ∈ [Q]. Let S be a set of k indices sampled (with replacement) from [r]. Then by Theorem F.1, we have that for each j ∈ [k],\n\nPr (cid:2)(cid:12)\n\n(cid:12)DSj (y) − KDE(X, y)(cid:12)\n\nLet Ij be an indicator variable so that Ij = 1 if (cid:12) Ij = 0 otherwise, so that we have Pr [Ij = 1] ≥ 3 so that E [I] = 1\n\nE [Ij] ≥ 3 4 .\n\nj∈[k]\n\n(cid:80)\n\nk\n\n(cid:12) ≤ ε · KDE(X, y)(cid:3) ≥\n\n3 4\n(cid:12)DSj (y) − KDE(X, y)(cid:12) 4 or equivalently, E [Ij] ≥ 3\n\n.\n\n(cid:12) ≤ ε · KDE(X, y) and j∈[k] Ij\n\n4 . Let I = 1\n\n(cid:80)\n\nk\n\nTo handle adaptive queries, we first note that PrivMed is (1, 0)-differentially private on the outputs of the r kernel density estimation data structures. We sample k = O (log Q) indices from Q log2 Q(cid:1) data structures with replacement. Thus by amplification via sampling,\n\nthe r = O (cid:0)√\n\n(cid:16)\n\n(cid:16)\n\n(cid:17)\n\n(cid:17)\n\n1√\n\nQ log Q\n\ni.e., Theorem A.2, PrivMed is\n\nO\n\n, 0\n\n-differentially private. By the advanced compo-\n\nsition of differential privacy, i.e., Theorem A.4, our algorithm can answer Q adaptive queries with (cid:16)\n\n-differentially privacy. By the generalization properties of differential privacy, i.e.,\n\nO (1) ,\n\n(cid:17)\n\n1 poly(Q)\n\nTheorem A.5, we have\n\nPr\n\n(cid:12) \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\n1 k\n\n(cid:12) (cid:12) (cid:12) Ij − E [I] (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nj∈[k]\n\n≥\n\n1 10\n\n\n\n < 0.01,\n\nfor sufficiently small constant O (1) in the private median algorithm PrivMed. Therefore,\n\n\n\n\n\n1 k\n\nPr\n\n(cid:88)\n\ni∈[k]\n\n\n\nIi > 0.6\n\n > 0.99,\n\nso that |dq − KDE(X, yq)| ≤ ε · KDE(X, y) across Q queries yq with q ∈ [Q].\n\nTheorem F.3. There exists an algorithm that uses O\n\n(cid:16) √\n\nQ log2 Q\n\n(cid:17)\n\nτ ε2\n\nspace and answers Q adaptive\n\nkernel density estimation queries within a factor of (1 + ε), provided each query has value at least τ .\n\nEach query uses O\n\n(cid:16) d log(nQ) ε2\n\n√\n\nτ\n\n(cid:17)\n\nruntime.\n\nBy comparison, random sampling, e.g., Charikar & Siminelakis (2017), uses Q Q queries and each query uses O (cid:0) d\n\nτ ε2 samples to answer (cid:1) runtime and using Q copies of the data structure by Backurs\n\net al. (2019) uses O\n\nspace and O\n\nruntime.\n\n(cid:17)\n\n(cid:16) Q τ ε2\n\nτ ε2\n\n(cid:17)\n\n(cid:16) d\n\n√\n\nε2\n\nτ\n\nF.1 UNLIMITED ADAPTIVE QUERIES FOR KERNEL DENSITY ESTIMATION\n\nIn this section, we go beyond the limits of our framework and analyze the case where there may be an unbounded number of adversarial queries.\n\nTheorem 1.4. Suppose the kernel function k is L-Lipschitz in the second variable for some L > 0, i.e., |k(x, y) − k(x, z)| ≤ L∥y − z∥2 for all x, y, z ∈ Rd. Moreover, suppose that for all ∥x − y∥2 ≤ ρ, we have k(x, y) ≤ τ 3 . Then an algorithm that produces a kernel density estimation data structure D that is L-Lipschitz over a set X of points with diameter at most ∆ and outputs a (1+ε)-approximation to KDE queries with value at least τ with probability at least 1 − δ using space S(n, ε, τ, log δ) and query time T (n, ε, τ, log δ), then there exists a KDE data structure that with probability at least 0.99, outputs a (1 + ε)-approximation to any number of KDE queries with value at least τ using space\n\n(cid:16)\n\nS\n\nn, O (ε) , O (τ ) , O\n\n(cid:16)\n\nd log (∆+ρ)L\n\nετ\n\n(cid:17)(cid:17)\n\nand query time T\n\nn, O (ε) , O (τ ) , O\n\n(cid:16)\n\n(cid:16)\n\nd log (∆+ρ)L\n\nετ\n\n(cid:17)(cid:17) .\n\nProof. Given a set X ⊆ Rd of n points with diameter ∆, let N be an ετ L -net over a ball of radius ∆ + ρ that contains X. More formally, let B be a ball of radius (∆ + ρ) that contains X and for\n\n25\n\nevery y ∈ B, there exists a point z ∈ N such that ∥y − z∥2 ≤ ετ\n\nL . We can construct the net greedily\n\nso that |N | ≤\n\n(cid:16) 2(∆+ρ)L ετ\n\n(cid:17)d\n\n.\n\nτ\n\nWe implement a data structure D that answers each (non-adaptive) kernel density estimation query (cid:1) for any kernel density estimation query with value at least with multiplicative approximation (cid:0)1 + ε 2 , with probability at least 1 − δ, where δ ≤ 1 100|N | . Then by a union bound, D correctly answers each kernel density estimation query in N with probability at least 0.99. Let q ∈ Rd be an arbitrary query such that KDE(X, q) ≥ τ . By assumption, we have that ∥q − x∥2 ≤ ρ for some x ∈ X and thus q ∈ B. By the definition of N , there exists some y ∈ N such that ∥q − y∥2 ≤ ετ\n\n3L . Then since k is L-Lipschitz in the second variable, we have\n\n3\n\n| KDE(X, q) − KDE(X, y)| =\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 n\n\n(cid:88)\n\nx∈X\n\nk(x, q) −\n\n1 n\n\n(cid:88)\n\nx∈X\n\nk(x, y)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\nL n\n\n∥q − y∥2 ≤\n\nετ 3n\n\n.\n\nHence, KDE(X, q) ≥ τ implies that KDE(X, y) ≥ τ D on query y. Then by correctness of D on N for any query with threshold at least τ\n\n2 . Let Ky be the output of the data structure\n\n2 , we have\n\n|Ky − KDE(X, y)| ≤\n\nε 3\n\nKDE(X, y).\n\nLet Kq be the output of the data structure D on query y. Since the algorithm itself is L-Lipschitz, then\n\nTherefore by the triangle inequality, we have that\n\n|Kq − Ky| ≤ L∥q − y∥2 ≤\n\nετ 3\n\n.\n\n|Kq − KDE(X, q)| ≤ |Kq − Ky| − |Ky − KDE(X, y)| − | KDE(X, y) − KDE(X, q)|\n\n≤\n\nετ 3\n\n+\n\nε 3\n\nKDE(X, y) +\n\nετ 3n\n\n.\n\nSince KDE(X, y) ≤ KDE(X, q) + ετ\n\n3n , then it follows that\n\n|Kq − KDE(X, q)| ≤\n\nετ 3\n\n+\n\nε 3\n\nKDE(X, q) +\n\nε2τ n\n\n+\n\nετ 3n\n\n≤ ε KDE(X, q),\n\nfor n ≥ 6.\n\nIn particular, sampling-based algorithms for kernels that are Lipschitz are also Lipschitz. Thus to apply Theorem 1.4, it suffices to identify kernels that are L-Lipschitz and use the data structure of Theorem F.1. To that end, we note that the kernels k(x, y) = for C > 0 and k(x, y) = Ce−∥x−y∥2 are both Lipschitz for some function of C. In particular, we have\n\nC C+∥x−y∥2\n\n|k(x, y) − k(x, z)| =\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nC C + ∥x − y∥2\n\n−\n\nC C + ∥x − z∥2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n=\n\n≤\n\nC|∥x − z∥2 − ∥x − y∥2| (C + ∥x − y∥2)(C + ∥x − z∥2) ∥y − z∥2 C\n\n,\n\nso k(x, y) =\n\nC C+∥x−y∥2\n\nis 1\n\nC -Lipschitz. Similarly, since e−x is 1-Lipschitz, then\n\n|k(x, y) − k(x, z)| = Ce−∥x−y∥2 − Ce−∥x−z∥2\n\n≤ C|∥x − z∥2 − ∥x − y∥2| ≤ C∥y − z∥2,\n\nso k(x, y) = Ce−∥x−y∥2 is C-Lipschitz.\n\n26\n\nG EMPIRICAL EVALUATION\n\nWe empirically demonstrate the space and query time efficiency of our framework of Section C. We consider the problem of l2 norm estimation where queries q1, q2, . . . are generated in an adaptive fashion and our goal is to output an estimate of ∥qi∥2 for all i. This setting is a special case of adaptive distance estimation and captures the essence of our adversarial robustness framework. In addition, this same setting was investigated empirically in prior works Cherapanamjeri & Nelson (2020).\n\nExperimental Setup. Consider the setting of Algorithm 6: it creates r copies of an underlying randomized data structure and upon a query, it subsamples k of them and outputs an answer aggregated via the private median. In our setting, the underlying algorithm will be the fast Johnson-Lindenstrauss (JL) transform which is defined as follows: it is the matrix P HD : Rd → Rm where D is a diagonal matrix with uniformly random ±1 entries, H is the Hadamard transform, and P is a sampling matrix uniformly samples m rows of HD. Our algorithm will initialize r copies of this matrix where the sampling matrix P and diagonal D will be the randomness which is “hidden” from the adversary. Upon query q, we sample k different Fast JL data structures, input q to all of them, and proceed as in Algorithm 6. Note that this setting exactly mimics the theoretical guarantees of Section C and is exactly Algorithm 7 of Section E. In our experiments, d = 4096, m = 250, r = 200, and k = 5. These are exactly the parameters chosen in prior works Cherapanamjeri & Nelson (2020). We will have 5000 adaptive queries qi which are described shortly. Our experiments are done on a 2021 M1 Macbook Pro with 32 gigabytes of RAM. We implemented all algorithms in Python 3.5 using Numpy. The Hadamard transform code is from Andoni et al. (2015)1 and we use Google’s differential privacy library2 for the private median implementation.\n\nBaselines. We will consider three baselines. JL will denote a standard (Gaussian) JL map from dimension 4096 to 250. Baseline 1 will denote the algorithm of Cherapanamjeri & Nelson (2020). At a high level, it instantiates many independent copies of the standard Gaussian JL map and only feeds an incoming query into a select number of subsampled data structures. Note that our experimental setting is mimicking exactly that of Cherapanamjeri & Nelson (2020) where the same parameters r (number of different underlying data structures) and k (number of subsampled data structures to use for a query) were used. This ensures that both our algorithm and theirs have access to the same number of different JL maps and thus allows us to compare the two approaches on an equal footing. The last baseline, denoted as Baseline 2, is the main algorithm of Cherapanamjeri & Nelson (2022) which is the optimized version of Cherapanamjeri & Nelson (2020). At a high level, their algorithm proceeds similarly to that of Cherapanamjeri & Nelson (2020), except they employ Hadamard transforms (after multiplying the query entry-wise by random Gaussians), rather than using Gaussian JL maps. Furthermore, instead of subsampling, their algorithm feeds an incoming query into all the different copies of the Hadamard transform, and subsamples the coordinates of the concatenated output for norm estimation. We again set the parameters of their algorithm to match that of our algorithm and Baseline 1 by using r copies of their Hadamard transform and subsampling mk total coordinates. We refer to the respective papers for full details of their algorithms.\n\nSummary of adaptive queries. Our input queries are the same adaptive queries used in Cherapanamjeri & Nelson (2020). To summarize, let Π denote the map used in the JL benchmark stated above. The i-th query for 1 ≤ i ≤ 5000 will be of the form qi = (cid:80)i j=1(−1)Wizi, which we then normalize to have unit norm. The zi are standard Gaussian vectors. Wi is the indicator variable for the event ∥Π(zi − e1)∥2 ≤ ∥Π(zi + e1)∥2 where e1 is the first standard basis vector. Intuitively, the queries become increasingly correlated with the matrix Π since we successively “augment” the queries in a biased fashion. See Section 5 of Cherapanamjeri & Nelson (2020) for a more detailed discussion of the adaptive inputs.\n\nResults. Our results are shown in Figure 1. In Figure 1a, we plot the norm estimated by each of the algorithms in each of the queries across iterations. We see that the na ̈ıve JL map increasingly deviates from the true value of 1.0. This is intuitive as the adaptive queries are increasingly correlated\n\n1available in https://github.com/FALCONN-LIB/FFHT 2available in https://github.com/google/differential-privacy\n\n27\n\n(a)\n\n(b)\n\n(c)\n\nFig. 1: Figures for our experiments.\n\nwith the map Π. The performance of all other algorithms are indistinguishable in Figure 1a. Thus, we only zoom into the performances of our algorithm and Baseline 1 and Baseline 2, shown in Figure 1b. For these three algorithms, we plot a histogram of answers outputted by the respective algorithms across all iterations. We see that the algorithm of Cherapanamjeri & Nelson (2020), shown in the blue shaded histogram, is the most accurate as it has the smallest deviations from the true answer of 1.0. Our algorithm, shown in green, is noisier than Baseline 1 since it has a wider range of variability. This may be due to the fact that we use a differentially private median algorithm, which naturally incurs additional noise. Lastly, Baseline 2 is also noisier than Baseline 1 and comparable to our algorithm. This may be due to the fact that the algorithm of Cherapanamjeri & Nelson (2022) requires very fine-tuned constants in their theoretical bounds, which naturally deviate in practice. Lastly, Figure 1c shows the cumulative runtime of all three algorithms across all iterations. Our algorithm, shown in green, is the fastest while Baseline 2 is the slowest. This is explained by the fact that Baseline 2 calculates many more Hadamard transforms than our algorithm does.\n\n28\n\n010002000300040005000Iteration1.01.52.02.53.0Norm EstimateBaseline 1Baseline 2OursJL0.850.900.951.001.051.101.15Norm Estimate02004006008001000120014001600CountBaseline 1Baseline 2Ours010002000300040005000Iteration012345678Cumulative Runtime (s)Baseline 1Baseline 2Ours",
    "reference": "# Summary Of The Paper\n\nThis paper considers the setting where an algorithm/data structure has to give responses to a sequence of adversarially chosen inputs.\nThe paper considers tow separate settings:\n1. The consider linear regression $min_x \\|Ax =b\\|_2^2$ where the target $b$ can be updated in $K$ locations at each step. The goal is approximate the squared error.\n2. They consider a setting where you have a non-adaptive data structure for a problem. They show that in order to answer $Q$ adversarial queries,  rather than use $Q$ independent copies of the. data structure, one can use privacy amplification techniques to only maintain $\\sqrt{Q}$ copies.\n\n# Strength And Weaknesses\n\nI am not quite convinced by the first problem. I think the problem of how to not resolve linear regression under updates is natural, but I am not sure why you would only care about approximating the squared error rather computing a near-optimal solution. Typically, the loss is a means to the end of finding a good $x$, not the end in itself.\n\nI found the second result much more interesting and general. The use of differential privacy is nice (and is somewhat reminiscent of the adaptive data analysis paper of Dwork et. al). It gives them  black box results for distance and kernel density estimation, which they are able to improve on in some settings. Similar techniques seem to have been used earlier in work on the streaming model.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nGood.\n\n# Summary Of The Review\n\nI think this paper is nice and ought to be accpted.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTIADA: A TIME-SCALE ADAPTIVE ALGORITHM FOR NONCONVEX MINIMAX OPTIMIZATION\n\nXiang Li, Department of Computer Science, ETH Zurich, Switzerland\n\nJunchi Yang, Niao He\n\nxiang.li,junchi.yang,niao.he\n\n{\n\n@inf.ethz.ch\n\n}\n\nABSTRACT\n\nAdaptive gradient methods have shown their ability to adjust the stepsizes on the fly in a parameter-agnostic manner, and empirically achieve faster convergence for solving minimization problems. When it comes to nonconvex minimax optimization, however, current convergence analyses of gradient descent ascent (GDA) combined with adaptive stepsizes require careful tuning of hyper-parameters and the knowledge of problem-dependent parameters. Such a discrepancy arises from the primal-dual nature of minimax problems and the necessity of delicate timescale separation between the primal and dual updates in attaining convergence. In this work, we propose a single-loop adaptive GDA algorithm called TiAda for nonconvex minimax optimization that automatically adapts to the time-scale separation. Our algorithm is fully parameter-agnostic and can achieve near-optimal complexities simultaneously in deterministic and stochastic settings of nonconvexstrongly-concave minimax problems. The effectiveness of the proposed method is further justified numerically for a number of machine learning applications.\n\n1\n\nINTRODUCTION\n\nAdaptive gradient methods, such as AdaGrad (Duchi et al., 2011), Adam (Kingma & Ba, 2015) and AMSGrad (Reddi et al., 2018), have become the default choice of optimization algorithms in many machine learning applications owing to their robustness to hyper-parameter selection and fast empirical convergence. These advantages are especially prominent in nonconvex regime with success in training deep neural networks (DNN). Classic analyses of gradient descent for smooth functions require the stepsize to be less than 2/l, where l is the smoothness parameter and often unknown for complicated models like DNN. Many adaptive schemes, usually with diminishing stepsizes based on cumulative gradient information, can adapt to such parameters and thus reducing the burden of hyper-parameter tuning (Ward et al., 2020; Xie et al., 2020). Such tuning-free algorithms are called parameter-agnostic, as they do not require any prior knowledge of problem-specific parameters, e.g., the smoothness or strong-convexity parameter.\n\nIn this work, we aim to bring the benefits of adaptive stepsizes to solving the following problem:\n\nmin x∈Rd1\n\nmax y∈Y\n\nf (x, y) = Eξ∈P [F (x, y; ξ)] ,\n\n(1)\n\n×\n\n→\n\nRd2\n\nRd2 is closed where P is an unknown distribution from which we can drawn i.i.d. samples, and convex, and f : Rd1 R is nonconvex in x. We call x the primal variable and y the dual variable. This minimax formulation has found vast applications in modern machine learning, notably generative adversarial networks (Goodfellow et al., 2014; Arjovsky et al., 2017), adversarial learning (Goodfellow et al., 2015; Miller et al., 2020), reinforcement learning (Dai et al., 2017; Modi et al., 2021), sharpness-aware minimization (Foret et al., 2021), domain-adversarial training (Ganin et al., 2016), etc. Albeit theoretically underexplored, adaptive methods are widely deployed in these applications in combination with popular minimax optimization algorithms such as (stochastic) gradient descent ascent (GDA), extragradient (EG) (Korpelevich, 1976), and optimistic GDA (Popov, 1980; Rakhlin & Sridharan, 2013); see, e.g., (Gulrajani et al., 2017; Daskalakis et al., 2018; Mishchenko et al., 2020; Reisizadeh et al., 2020), just to list a few.\n\nY ⊂\n\nWhile it seems natural to directly extend adaptive stepsizes to minimax optimization algorithms, a recent work by Yang et al. (2022a) pointed out that such schemes may not always converge without\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) trajectory\n\n(b) effective stepsize ratio\n\n(c) convergence\n\nt and ηy\n\nFigure 1: Comparison between TiAda and vanilla GDA with AdaGrad stepsizes (labeled as AdaGrad) on the quadratic function (2) with L = 2 under a poor initial stepsize ratio, i.e., ηx/ηy = 5. Here, ηx t are the effective stepsizes respectively for x and y, and κ is the condition number1. (a) shows the trajectory of the two algorithms and the background color demonstrates the function value f (x, y). In (b), while the effective stepsize ratio stays unchanged for AdaGrad, TiAda adapts to the desired time-scale separation 1/κ, which divides the training process into two stages. In (c), after entering Stage II, TiAda converges fast, whereas AdaGrad diverges.\n\nknowing problem-dependent parameters. Unlike the case of minimization, convergent analyses of GDA and EG for nonconvex minimax optimization are subject to time-scale separation (Bot ̧ & B ̈ohm, 2020; Lin et al., 2020a; Sebbouh et al., 2022; Yang et al., 2022b) — the stepsize ratio of primal and dual variables needs to be smaller than a problem-dependent threshold — which is recently shown to be necessary even when the objective is strongly concave in y with true gradients (Li et al., 2022). Moreover, Yang et al. (2022a) showed that GDA with standard adaptive stepsizes, that chooses the stepsize of each variable based only on the (moving) average of its own past gradients, fails to adapt to the time-scale separation requirement. Take the following nonconvex-stronglyconcave function as a concrete example:\n\nf (x, y) =\n\n−\n\n1 2\n\ny2 + Lxy\n\n−\n\nL2 2\n\nx2,\n\n(2)\n\nwhere L > 0 is a constant. Yang et al. (2022a) proved that directly using adaptive stepsizes like AdaGrad, Adam and AMSGrad will fail to converge if the ratio of initial stepsizes of x and y (denoted as ηx and ηy) is large. We illustrate this phenomenon in Figures 1(a) and 1(c), where AdaGrad diverges. To sum up, adaptive stepsizes designed for minimization, are not time-scale adaptive for minimax optimization and thus not parameter-agnostic.\n\nTo circumvent this time-scale separation bottleneck, Yang et al. (2022a) introduced an adaptive algorithm, NeAda, for problem (1) with nonconvex-strongly-concave objectives. NeAda is a two-loop algorithm built upon GDmax (Lin et al., 2020a) that after one primal variable update, updates the dual variable for multiple steps until a stopping criterion is satisfied in the inner loop. Although the algorithm is agnostic to the smoothness and strong-concavity parameters, there are several limitations that may undermine its performance in large-scale training: (a) In the stochastic setting, it gradually increases the number of inner loop steps (k steps for the k-th outer loop) to improve the inner maximization problem accuracy, resulting in a possible waste of inner loop updates if the maximization problem is already well solved; (b) NeAda needs a large batchsize of order Ω (cid:0)ε−2(cid:1) to achieve the near-optimal convergence rate in theory; (c) It is not fully adaptive to the gradient noise, since it deploys different strategies for deterministic and stochastic settings.\n\nIn this work, we address all of the issues above by proposing TiAda (Time-scale Adaptive Algorithm), a single-loop algorithm with time-scale adaptivity for minimax optimization. Specifically, one of our major modifications is setting the effective stepsize, i.e., the scale of (stochastic) gradient used in the updates, of the primal variable to the reciprocal of the maximum between the primal and dual variables’ second moments, i.e., the sums of their past gradient norms. This ensures the effective stepsize ratio of x and y being upper bounded by a decreasing sequence, which eventually reaches the desired time-scale separation. Taking the test function (2) as an example, Figure 1 illustrates the time-scale adaptivity of TiAda: In Stage I, the stepsize ratio quickly decreases below the threshold; in Stage II, the ratio is stabilized and the gradient norm starts to converge fast.\n\nWe focus on the minimax optimization (1) that is strongly-concave in y, since other nonconvex regimes are far less understood even without adaptive stepsizes. Moreover, near stationary point\n\n1Please refer to Section 2 for formal definitions of initial stepsize and effective stepsize. Note that the initial\n\nstepsize ratio, ηx/ηy, does not necessarily equal to the first effective stepsize ratio, ηx\n\n0 /ηy 0 .\n\n2\n\n010203040x020406080yTiAdaAdaGradstationarypointsinitialpoint−3.5−3.0−2.5−2.0−1.5−1.0−0.50.0×10302000400060008000#gradientcalls0.250.501.002.50ηxt/ηytStageIStageIITiAdaAdaGradηxt/ηyt=1/κ02000400060008000#gradientcalls5102050k∇xf(x,y)kStageIStageIIwhenηxt/ηytreaches1/κTiAdaAdaGradPublished as a conference paper at ICLR 2023\n\nmay not exist in nonconvex-nonconcave (NC-NC) problems and finding first-order local minimax point is already PPAD-complete (Daskalakis et al., 2021). We consider a constraint for the dual variable, which is common in convex optimization with adaptive stepsizes (Levy, 2017; Levy et al., 2018) and in the minimax optimization with non-adaptive stepsizes (Lin et al., 2020a). In summary, our contributions are as follows:\n\n• We introduce the first single-loop and fully parameter-agnostic adaptive algorithm, TiAda, for nonconvex-strongly-concave (NC-SC) minimax optimization. It adapts to the necessary timescale separation without large batchsize or any knowledge of problem-dependant parameters or (cid:0)ε−2(cid:1) in the target accuracy. TiAda finds an ε-stationary point with an optimal complexity of (cid:0)ε−(4+δ)(cid:1) for any small δ > 0 deterministic case, and a near-optimal sample complexity of in the stochastic case. It shaves off the extra logarithmic terms in the complexity of NeAda with AdaGrad stepsize for both primal and dual variables (Yang et al., 2022a). TiAda is proven to be noise-adaptive, which is the first of its kind among nonconvex minimax optimization algorithms.\n\nO\n\nO\n\n• While TiAda is based on AdaGrad stepsize, we generalize TiAda with other existing adaptive schemes, and conduct experiments on several tasks. The tasks include 1) test functions by Yang et al. (2022a) for showing the nonconvergence of GDA with adaptive schemes under poor initial stepsize ratios, 2) distributional robustness optimization (Sinha et al., 2018) on MNIST dataset with a NC-SC objective, and 3) training the NC-NC generative adversarial networks on CIFAR10 dataset. In all tasks, we show that TiAda converges faster and is more robust compared with NeAda or GDA with other existing adaptive stepsizes.\n\n1.1 RELATED WORK\n\nAdaptive gradient methods. AdaGrad brings about an adaptive mechanism for gradient-based optimization algorithm that adjusts its stepsize by keeping the averaged past gradients. The original AdaGrad was introduced for online convex optimization and maintains coordinate-wise stepsizes. In nonconvex stochastic optimization, AdaGrad-Norm with one learning rate for all directions is shown to achieve the same complexity as SGD (Ward et al., 2020; Li & Orabona, 2019), even with the high probability bound (Kavis et al., 2022; Li & Orabona, 2020). In comparison, RMSProp (Hinton et al., 2012) and Adam (Kingma & Ba, 2015) use the decaying moving average of past gradients, but may suffer from divergence (Reddi et al., 2018). Many variants of Adam are proposed, and a wide family of them, including AMSGrad, are provided with convergence guarantees (Zhou et al., 2018; Chen et al., 2018; D ́efossez et al., 2020; Zhang et al., 2022b). One of the distinguishing traits of adaptive algorithms is that they can achieve order-optimal rates without knowledge about the problem parameters, such as smoothness and variance of the noise, even in nonconvex optimization (Ward et al., 2020; Levy et al., 2021; Kavis et al., 2019).\n\nAdaptive minimax optimization algorithms. The adaptive stepsize schemes are naturally extended to minimax optimization, both in theory and practice, notably in the training of GANs (Goodfellow, 2016; Gidel et al., 2018). In the convex-concave regime, several adaptive algorithms are designed based on EG and AdaGrad stepsize, and they inherit the parameter-agnostic characteristic (Bach & Levy, 2019; Antonakopoulos et al., 2019). In sharp contrast, when the objective function is nonconvex about one variable, most existing adaptive algorithms require knowledge of the problem parameters (Huang & Huang, 2021; Huang et al., 2021; Guo et al., 2021). Very recently, it was proved that a parameter-dependent ratio between two stepsizes is necessary for GDA in NC-SC minimax problems with non-adaptive stepsize (Li et al., 2022) and most existing adaptive stepsizes (Yang et al., 2022a). Heusel et al. (2017) shows the two-time-scaled GDA with non-adaptive stepsize or Adam will converge, but assuming the existence of an asymptotically stable attractor.\n\nOther NC-SC minimax optimization algorithms. In the NC-SC setting, the most popular algorithms are GDA and GDmax, in which one primal variable update is followed by one or multiple (ε−2) complexity in the deterministic steps of dual variable updates. Both of them can achieve (ε−4) sample complexity in the stochastic setting (Lin et al., 2020a; Chen et al., 2021; setting and Nouiehed et al., 2019; Yang et al., 2020), which are not improvable in the dependency on ε given the existing lower complexity bounds (Zhang et al., 2021; Li et al., 2021). Later, several works further improved the dependency on the condition number with more complicated algorithms in deterministic (Yang et al., 2022b; Lin et al., 2020b) and stochastic settings (Zhang et al., 2022a). All of the algorithms above do not use adaptive stepsizes and rely on knowledge of the problem parameters.\n\nO\n\nO\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n1.2 NOTATIONS\n\nWe denote l as the smoothness parameter, μ as the strong-concavity parameter, whose formal definitions will be introduced in Assumptions 3.1 and 3.2, and κ := l/μ as the condition number. We yF (x, y; ξ)]. For the miniassume access to stochastic gradient oracle returning [ ∇\nmax problem (1), we denote y∗(x) := arg maxy∈Y f (x, y) as the solution of the inner maximization problem, Φ(x) := f (x, y∗(x)) as the primal function, and .\nY For notational simplicity, we will use the name of an existing adaptive algorithm to refer to the simple combination of GDA and it, i.e., setting the stepsize of GDA to that adaptive scheme separately for both x and y. For instance “AdaGrad” for minimax problems stands for the algorithm that uses AdaGrad stepsizes separately for x and y in GDA.\n\n) as projection operator onto set ·\n\nxF (x, y; ξ),\n\nY (\n\n∇\n\nP\n\n2 METHOD\n\nWe formally introduce the TiAda method in Algorithm 1, and the major difference with AdaGrad lies in line 5. Like AdaGrad, TiAda stores the accumulated squared (stochastic) gradient norm of the primal and dual variables in vx t , respectively. We refer to hyper-parameters ηx and ηy as the initial stepsizes, and the actual stepsizes for updating in line 5 as effective stepsizes which are denoted by ηx t = t+1 ηy/ (cid:0)vy . In Section 3, our theoretical analysis suggests to choose α > 1/2 > β. We will also illustrate in the next subsection that the max structure and different α, β make our algorithm adapt to the desired time-scale separation.\n\nt and ηy , while AdaGrad uses ηx/ (cid:0)vx\n\nt . TiAda adopts effective stepsizes ηx\n\nt = ηx/ max (cid:8)vx\n\nand ηy/ (cid:0)vy\n\nt and vy\n\nt+1, vy\n\nand ηy\n\n(cid:1)1/2\n\n(cid:1)1/2\n\n(cid:9)α\n\nt+1\n\nt+1\n\nt+1\n\n(cid:1)β\n\nFor simplicity of analysis, similar to AdaGrad-Norm (Ward et al., 2020), we use the norms of gradients for updating the effective stepsizes. A more practical coordinate-wise variant that can be used for high-dimensional models is presented in Section 4.1.\n\nAlgorithm 1 TiAda (Time-scale Adaptive Algorithm) 0 > 0, vy\n\n0 > 0, ηx > 0, ηy > 0, α > 0, β > 0 and α > β.\n\n1: Input: (x0, y0), vx 2: for t = 0, 1, 2, ... do 3:\n\n4:\n\n5:\n\nsample i.i.d. ξx t+1 = vx vx xt+1 = xt\n\nt +\n\n∥\n\nt and ξy gx t ∥\n\nt , and let gx t+1 = vy α gx\n\n2 and vy ηx t+1,vy vx\n\nxF (xt, yt; ξx gy t ∥\n\nt = t +\n\n∇ ∥\nt and yt+1 =\n\n(cid:18)\n\nmax\n\n2\n\nY\n\nP\n\n−\n\n{\n\nt+1}\n\n6: end for\n\nt ) and gy\n\nt =\n\nyF (xt, yt; ξy t )\n\n∇\n\n(cid:19)\n\nyt + ηy\n\nt+1)β gy\n\nt\n\n(vy\n\n2.1 THE TIME-SCALE ADAPTIVITY OF TIADA\n\nt /ηy\n\nCurrent analyses of GDA with non-adaptive stepsizes require the time-scale, ηx t , to be smaller than a threshold depending on problem constants such as the smoothness and the strong-concavity parameter (Lin et al., 2020a; Yang et al., 2022b). The intuition is that we should not aggressively update x if the inner maximization problem has not yet been solved accurately, i.e., we have not found a good approximation of y∗(x). Therefore, the effective stepsize of x should be small compared with that of y. It is tempting to expect adaptive stepsizes to automatically find a suitable time-scale separation. However, the quadratic example (2) given by Yang et al. (2022a) shattered the illusion. In this example, the effective stepsize ratio stays the same along the run of existing adaptive algorithms, including AdaGrad (see Figure 1(b)), Adam and AMSGrad, and they fail to converge if the initial stepsizes are not carefully chosen (see Yang et al. (2022a) for details). As vx t only separately contain the gradients of x and y, the effective stepsizes of two variables in these adaptive methods depend on their own history, which prevents them from cooperating to adjust the ratio.\n\nt and vy\n\nNow we explain how TiAda adapts to both the required time-scale separation and small enough stepsizes. First, the ratio of our modified effective stepsizes is upper bounded by a decreasing sequence when α > β:\n\nt\n\nηx ηy\n\nt\n\n=\n\nηx/ max (cid:8)vx ηy/ (cid:0)vy\n\nt+1, vy (cid:1)β\n\nt+1\n\n(cid:9)α\n\nt+1\n\nηx/ (cid:0)vy ηy/ (cid:0)vy\n\nt+1\n\nt+1\n\n(cid:1)α (cid:1)β =\n\nηx\n\nηy (cid:0)vy\n\nt+1\n\n(cid:1)α−β ,\n\n(3)\n\n≤\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nas vy t is the sum of previous gradient norms and is increasing. Regardless of the initial stepsize ratio ηx/ηy, we expect the effective stepsize ratio to eventually drop below the desirable threshold for convergence. On the other hand, the effective stepsizes for the primal and dual variables are also and ηy/ (cid:0)vy upper bounded by decreasing sequences, ηx/ (cid:0)vx AdaGrad, such adaptive stepsizes will reduce to small enough, e.g.,\n\n, respectively. Similar to (1/l), to ensure convergence.\n\n(cid:1)α\n\nt+1\n\nt+1\n\n(cid:1)β\n\nO\n\nAnother way to look at the effective stepsize of x is\n\nηx\n\nt =\n\nηx t+1, vy\n\nt+1\n\nmax (cid:8)vx\n\n(cid:9)α =\n\n(cid:1)α\n\n(cid:0)vx max (cid:8)vx\n\nt+1\n\nt+1, vy\n\nt+1\n\nηx\n\n(cid:9)α ·\n\n(cid:0)vx\n\nt+1\n\n(cid:1)α .\n\n(4)\n\nIf the gradients of y are small (i.e., vy t+1), meaning the inner maximization problem is well solved, then the first factor becomes 1 and the effective stepsize of x is just the second factor, similar to the AdaGrad updates. If the term vy t+1, the first factor would be smaller than 1, allowing to slow down the update of x and waiting for a better approximation of y∗(x).\n\nt+1 dominates over vx\n\nt+1 < vx\n\nTo demonstrate the time-scale adaptivity of TiAda, we conducted experiments on the quadratic minimax example (2) with L = 2. As shown in Figure 1(b), while the effective stepsize ratio of AdaGrad stays unchanged for this particular function, TiAda progressively decreases the ratio. According to Lemma 2.1 of Yang et al. (2022a), 1/κ is the threshold where GDA starts to converge. We label the time period before reaching this threshold as Stage I, during which as shown in Figure 1(c), the gradient norm for TiAda increases. However, as soon as it enters Stage II, i.e., when the ratio drops below 1/κ, TiAda converges fast to the stationary point. In contrast, since the stepsize ratio of AdaGrad never reaches this threshold, the gradient norm keeps growing.\n\n3 THEORETICAL ANALYSIS OF TIADA\n\nIn this section, we study the convergence of TiAda under NC-SC setting with both deterministic and stochastic gradient oracles. We make the following assumptions to develop our convergence results.\n\nAssumption 3.1 (smoothness). Function f (x, y) is l-smooth (l > 0) in both x and y, that is, for any x1, x2\n\nRd1 and y1, y2\n\n, we have\n\n∈\n\n∈ Y xf (x2, y2)\n\n, ∥\n\nmax\n\nxf (x1, y1)\n\n{∥∇\n\n∥ Assumption 3.2 (strong-concavity in y). Function f (x, y) is μ-strongly-concave (μ > 0) in y, that is, for any x\n\nRd1 and y1, y2\n\n, we have\n\n∥} ≤\n\n− ∇\n\n− ∇\n\n∥∇\n\n−\n\n−\n\n∥\n\n∥\n\n∥\n\nyf (x1, y1)\n\nyf (x2, y2)\n\nl (\n\nx1\n\nx2\n\n+\n\ny1\n\ny2\n\n) .\n\n∈\n\n∈ Y\n\nf (x, y1)\n\n≥\n\nf (x, y2) +\n\n⟨∇\n\nyf (x, y1), y1\n\ny2\n\n⟩\n\n−\n\n+\n\nμ 2 ∥\n\ny1\n\ny2\n\n2.\n\n∥\n\n−\n\nAssumption 3.3 (interior optimal point). For any x\n\nRd1 , y∗(x) is in the interior of\n\nyf (x, y∗(x)) = 0, which is important for AdaGrad-like Remark 3.1. The last assumption ensures stepsizes that use the sum of squared norms of past gradients in the denominator. If the gradient about y is not 0 at y∗(x), the stepsize will keep decreasing even near the optimal point, leading to slow convergence. This assumption could be potentially alleviated by using generalized AdaGrad stepsizes (Bach & Levy, 2019).\n\n∇\n\n.\n\nY\n\n∈\n\n∥∇\n\n2 ∥\n\nxf (x, y)\n\nxf (x, y)\n\n∥∇ ε2 and E\n\nWe aim to find a near stationary point for the minimax problem (1). Here, (x, y) is defined to be an ε stationary point if ε in the deterministic setting, or E\nε2 in the stochastic setting, where the expectation is taken over all the randomness in the algorithm. This stationarity notion can be easily translated to the near-stationarity of the primal function Φ(x) = maxy∈Y (x, y) (Yang et al., 2022b). Under our (cid:0)ε−2(cid:1) complexity in the deterministic setting and a analyses, TiAda is able to achieve the optimal near-optimal\n\n(cid:0)ε−(4+δ)(cid:1) sample complexity for any small δ > 0 in the stochastic setting.\n\nε and ∥ ≤ 2\nyf (x, y) ∥\n\nyf (x, y)\n\n∥ ≤\n\n∥∇\n\n∥∇\n\nO\n\n≤\n\n≤\n\nO\n\n3.1 DETERMINISTIC SETTING\n\nIn this subsection, we assume to have access to the exact gradients of f ( ·\nyF (xt, yt; ξy replace\n\nxF (xt, yt; ξx\n\nxf (xt, yt) and\n\n), and therefore we can ·\n\nyf (xt, yt) in Algorithm 1.\n\n,\n\nt ) and\n\nt ) by\n\n∇\n\n∇\n\n∇\n\n∇\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 3.1 (deterministic setting). Under Assumptions 3.1 to 3.3, Algorithm 1 with deterministic gradient oracles satisfies that for any 0 < β < α < 1, after T iterations,\n\n1 T\n\nT −1 (cid:88) t=0 ∥∇\n\nxf (xt, yt)\n\n2 + ∥\n\n1 T\n\nT −1 (cid:88) t=0 ∥∇\n\nyf (xt, yt)\n\n2\n\n≤ O\n\n(cid:19)\n\n.\n\n(cid:18) 1 T\n\n∥\n\n(ε−2) This theorem implies that for any initial stepsizes, TiAda finds an ε-stationary point within iterations. Such complexity is comparable to that of nonadaptive methods, such as vanilla GDA (Lin et al., 2020a), and is optimal in the dependency of ε (Zhang et al., 2021). Like NeAda (Yang et al., 2022a), TiAda does not need any prior knowledge about μ and l, but it improves over NeAda by removing the logarithmic term in the complexity. Notably, we provide a unified analysis for a wide range of α and β, while most existing literature on AdaGrad-like stepsizes only validates a specific hyper-parameter, e.g., α = 1/2 in minimization problems (Ward et al., 2020; Kavis et al., 2019).\n\nO\n\n3.2 STOCHASTIC SETTING\n\n∈\n\n∇\n\n∇\n\n∥ ≤\n\n∈ { ∥∇\n\nzf (x, y).\n\nzF (x, y, ξ)] =\n\nxF (x, y; ξ) and\n\n∇ G for any x\n\nRd1, Φ(x) is upper bounded by Φmax.\n\nIn this subsection, we assume the access to a stochastic gradient oracle, that returns unbiased noisy gradients,\n\nyF (x, y; ξ). Also, we make the following additional assumptions. , we have Eξ [ x, y }\nzF (x, y, ξ)\n\nAssumption 3.4 (stochastic gradients). For z In addition, there exists a constant G such that ∈\nAssumption 3.5 (bounded primal function value). There exists a constant Φmax any x Remark 3.2. The bounded gradients and function value are assumed in many works on adaptive algorithms (Kavis et al., 2022; Levy et al., 2021). This implies the domain of y is bounded, which is also assumed in the analyses of AdaGrad (Levy, 2017; Levy et al., 2018). In neural networks with rectified activations, because of its scale-invariance property (Dinh et al., 2017), imposing boundedness of y does not affect the expressiveness. Wasserstein GANs (Arjovsky et al., 2017) also use projections on the critic to restrain the weights on a small cube around the origin. Assumption 3.6 (second order Lipschitz continuity for y). For any x1, x2 there exists constant L such that (cid:13) 2\nL ( xyf (x1, y1) (cid:13) ∇\nyyf (x2, y2)(cid:13) and (cid:13) L ( (cid:13) (cid:13)\n\nxyf (x2, y2)(cid:13) (cid:13) y2 y1 +\n\nRd1 and y1, y2 +\n\nR such that for\n\n∇ Rd1 and y\n\n2 yyf (x1, y1)\n\n, ∈ Y )\ny2 ∥\n\n∈ Y\n\n≤ ).\n\n∈ x1\n\nx1\n\nx2\n\ny1\n\n−\n\n−\n\n∈\n\n∥\n\n∥\n\n∥\n\n2\n\n2\n\n.\n\n(cid:0)ε−4(cid:1) comRemark 3.3. Chen et al. (2021) also impose this assumption to achieve the optimal plexity for GDA with non-adaptive stepsizes for solving NC-SC minimax problems. Together with Assumption 3.3, we can show that y∗( ) is smooth. Nevertheless, without this assumption, Lin et al. ·\n(2020a) only show a worse complexity of\n\n(cid:0)ε−5(cid:1) for GDA without large batchsize.\n\nO\n\n− ∇ x2 ∥\n\n−\n\n∥\n\n−\n\n∥\n\n≤\n\n∥\n\n− ∇\n\n∇\n\nTheorem 3.2 (stochastic setting). Under Assumptions 3.1 to 3.6, Algorithm 1 with stochastic gradient oracles satisfies that for any 0 < β < α < 1, after T iterations,\n\nE\n\n1 T\n\n(cid:34)T −1 (cid:88) t=0 ∥∇\n\nxf (xt, yt)\n\n2 + ∥\n\nT −1 (cid:88) t=0 ∥∇\n\nyf (xt, yt)\n\n(cid:35)\n\n2 ∥\n\n≤ O\n\n(cid:0)T α−1 + T −α + T β−1 + T −β(cid:1) .\n\nO\n\nO\n\n(cid:0)ε−(4+δ)(cid:1) for any small δ > 0 if we set α = 0.5 + δ/(8 + 2δ) and β = 0.5\n\n(cid:0)ε−4(cid:1) (Li TiAda can achieve the complexity arbitrarily close to the optimal sample complexity, et al., 2021), by choosing α and β arbitrarily close to 0.5. Specifically, TiAda achieves a complexity δ/(8 + 2δ). of Notably, this matches the complexity of NeAda with AdaGrad stepsizes for both variables (Yang et al., 2022a). NeAda may attain (cid:101) O\n\n(ε−4) complexity with more complicated subroutines for y.\n\nTheorem 3.2 implies that TiAda is fully agnostic to problem parameters, e.g., μ, l and σ. GDA with non-adaptive stepsizes (Lin et al., 2020a) and vanilla single-loop adaptive methods (Huang & Huang, 2021), such as AdaGrad and AMSGrad, all require knowledge of these parameters. Compared with the only parameter-agnostic algorithm, NeAda, our algorithm has several advantages. First, TiAda is a single-loop algorithm, while NeAda (Yang et al., 2022a) needs increasing inner-loop steps and a huge batchsize of order Ω (cid:0)ε−2(cid:1) to achieve its best complexity. Second, our stationary guarantee is for E ε guarantee in NeAda. Last but not least, although NeAda does not need to know the exact value of variance σ in the stochastic setting when σ > 0, NeAda uses a different stopping criterion for the inner loop in the deterministic\n\nε2, which is stronger than E\n\nxf (x, y)\n\nxf (x, y)\n\n∥ ≤\n\n∥∇\n\n∥∇\n\n−\n\n≤\n\n∥\n\n2\n\n6\n\nO\n\nPublished as a conference paper at ICLR 2023\n\nsetting when σ = 0, so it still needs partial information about σ. In comparison, TiAda achieves the (near) optimal complexity in both settings with the same strategy.\n\nt+1\n\n(cid:1)α−β\n\nConsistent with the intuition of time-scale adaptivity in Section 2.1, the convergence result can be derived in two stages. In Stage I, according to the upper bound of the ratio in Equation (3), we expect the term 1/ (cid:0)vy reduces to a constant c, a desirable time-scale separation. This means that vy t+1 has to grow to nearly (1/c)1/(α−β). In Stage II, when the time-scale separation is satisfied, TiAda converges at a speed specified in Theorem 3.2. This indicates that the proximity between α and β affects the speed trade-off between Stage I and II. When α and β are close, we have a faster overall convergence rate close to the optimality, but suffer from a longer transition phase in Stage I, albeit by only a constant term. We also present an empirical ablation study on the convergence behavior with different choices of α and β in Appendix A.2. Remark 3.4. In TiAda, the update of x requires to know the gradients of y (or vy t+1). However, in some applications that concern about privacy, one player might not access the information about the other player (Koller & Pfeffer, 1995; Foster & Young, 2006; He et al., 2016). Therefore, we also consider a variant of TiAda without taking the maximum of gradient norms, i.e., setting the effective stepsize of x in Algorithm 1 to ηx/ (cid:0)vx . This variant achieves a sub-optimal com- (cid:0)ε−6(cid:1). This result further justifies the importance of coordination between adaptive plexity of (cid:101) O\nstepsizes of two players for achieving faster convergence in minimax optimization. The algorithm and convergence results are presented in Appendix C.4.\n\n(cid:1)α\n\nt+1\n\n4 EXPERIMENTS\n\nIn this section, we first present extensions of TiAda that accommodate other adaptive schemes besides AdaGrad and are more practical in deep models. Then we present empirical results of TiAda and compare it with (i) simple combinations of GDA and adaptive stepsizes, which are commonly used in practice, and (ii) NeAda with different adaptive mechanisms (Yang et al., 2022a). Our experiments include test functions proposed by Yang et al. (2022a), the NC-SC distributional robustness optimization (Sinha et al., 2018), and training the NC-NC Wasserstein GAN with gradient penalty (Gulrajani et al., 2017). We believe that this not only validates our theoretical results but also shows the potential of our algorithm in real-world scenarios. To show the strength of being parameter-agnostic of TiAda, in all the experiments, we merely select α = 0.6 and β = 0.4 without further tuning those two hyper-parameters. All experimental details including the neural network structure and hyper-parameters are described in Appendix A.1.\n\n4.1 EXTENSIONS TO OTHER ADAPTIVE STEPSIZES AND HIGH-DIMENSIONAL MODELS\n\nAlthough we design TiAda upon AdaGrad-Norm, it is easy and intuitive to apply other adaptive t and schemes like Adam and AMSGrad. To do so, for z vz\n\n, we replace the definition of gz\n\nt+1 in line 3 and 4 of Algorithm 1 to\n\n∈ {\n\nx, y\n\nt gz\n\nt−1 + (1\n\nβz t )\n\n∇\n\n−\n\nzF (xt, yt; ξz\n\nt ),\n\nvz t+1 = ψ\n\nt = βz gz βz\n\n{\n\nt }\n\nwhere is the momentum parameters and ψ is the second moment function. Some common stepsizes that fit in this generalized framework can be seen in Table 1 in the appendix. Since Adam is widely used in many deep learning tasks, we also implement generalized TiAda with Adam stepsizes in our experiments for real-world applications, and we label it “TiAda-Adam”.\n\n} (cid:18)\n\nv0,\n\n(cid:110)\n\n∥∇\n\nzF (xi, yi; ξz i )\n\n∥\n\n2(cid:111)t\n\ni=0\n\n(cid:19)\n\n,\n\nBesides generalizing TiAda to accommodate different stepsize schemes, for high-dimensional models, we also provide a coordinate-wise version of TiAda. Note that we cannot simply change everything in Algorithm 1 to be coordinate-wise, because we use the gradients of y in the stepsize of x and there are no corresponding relationships between the coordinates of x and y. Therefore, in light of our intuition in Equation (4), we use the global accumulated gradient norms to dynamically adjust the stepsize of x. Denote the second moment (analogous to vx t+1 in Algorithm 1) for the i-th t+1 := (cid:80)d1 i=1 vx coordinate of x at the t-th step as vx t+1,i. We also use similar notations for y. Then, the update for the i-th parameter, i.e., xi and yi, can be written as\n\nt+1,i and globally vx\n\n \n\n\n\nxi\n\nt+1 = xi t+1 = yi yi\n\nt − t +\n\nt+1)α (vx t+1,vy vx\n\nmax\n\n{ ηy\n\nt+1} t+1,i)β ∇yif (xt, yt).\n\n(vy\n\nηx t+1,i)α\n\n(vx\n\nα\n\n·\n\n∇xif (xt, yt)\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n(a) r = 1\n\n(b) r = 1/2\n\n(c) r = 1/4\n\n(d) r = 1/8\n\n(e) r = 1/0.01\n\n(f) r = 1/0.03\n\n(g) r = 1/0.05\n\n(h) ablation study\n\nFigure 2: Comparison of algorithms on test functions. r = ηx/ηy is the initial stepsize ratio. In the first row, we use the quadratic function (2) with L = 2 under deterministic gradient oracles. For the second row, we test the methods on the McCormick function with noisy gradients.\n\nOur results in the following subsections provide strong empirical evidence for the effectiveness of these TiAda variants, and developing convergence guarantees for them would be an interesting future work. We believe our proof techniques for TiAda, together with existing convergence results for coordinate-wise AdaGrad and AMSGrad (Zhou et al., 2018; Chen et al., 2018; D ́efossez et al., 2020), can shed light on the theoretical analyses of these variants.\n\n4.2 TEST FUNCTIONS\n\nFirstly, we examine TiAda on the quadratic function (2) that shows the non-convergence of simple combinations of GDA and adaptive stepsizes (Yang et al., 2022a). Since our TiAda is based on AdaGrad, we compare it to GDA with AdaGrad stepsize and NeAda-AdaGrad (Yang et al., 2022a). The results are shown in the first row of Figure 2. When the initial ratio is poor, TiAda and NeAdaAdaGrad always converge while AdaGrad diverges. NeAda also suffers from slow convergence when the initial ratio is poor, e.g., 1 and 1/2 after 2000 iterations. In contrast, TiAda automatically balances the stepsizes and converges fast under all ratios.\n\n1\n\nFor the stochastic case, we follow Yang et al. (2022a) and conduct experiments on the McCormick function which is more complicated and 2-dimensional: f (x, y) = sin(x1+x2)+(x1 2 x1+ 5\n2). TiAda consistently outperforms AdaGrad and NeAda-AdaGrad 2 x2 +1+x1y1 +x2y2 as demonstrated in the second row of Figure 2 regardless of the initial ratio. In this function, we also run an ablation study on the effect of our design that uses max-operator in the update of x. We compare TiAda with and its variant without the max-operator, TiAda without MAX (Algorithm 2 in the appendix) whose effective stepsizes of x are ηx/ (cid:0)vx . According to Figure 2(h), TiAda converges to smaller gradient norms under all configurations of α and β.\n\n1 +y2\n\n2 (y2\n\nx2)2\n\n(cid:1)α\n\nt+1\n\n−\n\n−\n\n−\n\n3\n\n4.3 DISTRIBUTIONAL ROBUSTNESS OPTIMIZATION\n\nIn this subsection, we consider the distributional robustness optimization (Sinha et al., 2018). We target training the model weights, the primal variable x, to be robust to the perturbations in the image inputs, the dual variable y. The problem can be formulated as:\n\nmin x\n\nmax y=[y1,...,yn]\n\n1 n\n\nn (cid:88)\n\ni=1\n\nfi(x, yi)\n\nγ\n\nyi\n\n∥\n\n−\n\n−\n\nvi\n\n2, ∥\n\n(5)\n\nwhere fi is the loss function of the i-th sample, vi is the i-th input image, and yi is the corresponding perturbation. There are a total of n samples and γ is a trade-off hyper-parameter between the original loss and the penalty of the perturbations. If γ is large enough, the problem is NC-SC.\n\nWe conduct the experiments on the MNIST dataset (LeCun, 1998). In the left two plots of Figure 3, we compare TiAda with AdaGrad and NeAda-AdaGrad in terms of convergence. Since it is common in practice to update y 15 times after each x update (Sinha et al., 2018) for better generalization error,\n\n8\n\n02000400060008000#gradientcalls10−510−310−1101103k∇xf(x,y)kTiAdaAdaGradNeAda-AdaGrad02000400060008000#gradientcalls10−810−610−410−2100102TiAdaAdaGradNeAda-AdaGrad0500100015002000#gradientcalls10−510−310−1101TiAdaAdaGradNeAda-AdaGrad010002000300040005000#gradientcalls10−1410−1110−810−510−2101TiAdaAdaGradNeAda-AdaGrad010000200003000040000#gradientcalls10−310−210−1100101k∇xf(x,y)kTiAdaAdaGradNeAda-AdaGrad0250050007500100001250015000#gradientcalls10−310−210−1100101TiAdaAdaGradNeAda-AdaGrad05000100001500020000#gradientcalls10−310−210−1100101TiAdaAdaGradNeAda-AdaGrad05000100001500020000#gradientcalls10−310−210−1α=0.5,β=0.5(TiAda)α=0.5,β=0.5(TiAdawithoutMAX)α=0.6,β=0.4(TiAda)α=0.6,β=0.4(TiAdawithoutMAX)α=0.7,β=0.3(TiAda)α=0.7,β=0.3(TiAdawithoutMAX)Published as a conference paper at ICLR 2023\n\n(a) ηx = 0.1, ηy = 0.05\n\n(b) ηx = 0.1, ηy = 0.1\n\n(c) ηx = 0.001, ηy = 0.1\n\n(d) ηx = 0.001, ηy = 0.001\n\nFigure 3: Comparison of the algorithms on distributional robustness optimization (5). We use i in the legend to indicate the number of inner loops. Here we present two sets of stepsize configurations for the comparisons of AdaGrad-like and Adam-like algorithms. Please refer to Appendix A.3 for extensive experiments on larger ranges of stepsizes, and it will be shown that TiAda is the best among all stepsize combinations in our grid.\n\nwe implement AdaGrad using both single and 15 iterations of inner loop (update of y). In order to show that TiAda is more robust to the initial stepsize ratio, we compare two sets of initial stepsize configurations with two different ratios. In both cases, TiAda outperforms NeAda and AdaGrad, especially when ηx = ηy = 0.1, the performance gap is large. In the right two plots of Figure 3, the Adam variants are compared. In this case, we find that TiAda is not only faster, but also more stable comparing to Adam with one inner loop iteration.\n\n4.4 GENERATIVE ADVERSARIAL NETWORKS\n\nAnother successful and popular application of minimax optimization is generative adversarial networks. In this task, a discriminator (or critic) is trained to distinguish whether an image is from the dataset. At the same time, a generator is mutually trained to synthesize samples with the same distribution as the training dataset so as to fool the discriminator. We use WGAN-GP loss (Gulrajani et al., 2017), which imposes the discriminator to be a 1-Lipschitz function, with CIFAR-10 dataset (Krizhevsky et al., 2009) in our experiments.\n\nSince TiAda is a single-loop algorithm, for fair comparisons, we also update the discriminator only once for each generator update in Adam. In Figure 4, we plot the inception scores (Salimans et al., 2016) of TiAda-Adam and Adam under different initial stepsizes. We use the same color for the same initial stepsizes, and different line styles to distinguish the two methods, i.e., solid lines for TiAda-Adam and dashed lines for Adam. For all the three initial stepsizes we consider, TiAda-Adam achieves higher inception scores. Also, TiAda-Adam is more robust to initial stepsize selection, as the gap between different solid lines at the end of training is smaller than the dashed lines.\n\nFigure 4: Inception score on WGAN-GP.\n\n5 CONCLUSION\n\nIn this work, we bring in adaptive stepsizes to nonconvex minimax problems in a parameter-agnostic manner. We designed the first time-scale adaptive algorithm, TiAda, which progressively adjusts the effective stepsize ratio and reaches the desired time-scale separation. TiAda is also noise adaptive and does not require large batchsizes compared with the existing parameter-agnostic algorithm for nonconvex minimax optimization. Furthermore, TiAda is able to achieve optimal and near-optimal complexities respectively wtih deterministic and stochastic gradient oracles. We also empirically showcased the advantages of TiAda over NeAda and GDA with adaptive stepsizes on several tasks, including simple test functions, as well as NC-SC and NC-NC real-world applications. It remains an interesting problem to study whether TiAda can escape stationary points that are not local optimum, like adaptive methods for minimization problems (Staib et al., 2019).\n\n9\n\n020000400006000080000#gradientcalls10−310−210−1100101k∇xf(x,y,ξ)kTiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−310−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−410−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam0500010000150002000025000300003500040000#gradientcalls1.52.02.53.03.54.04.55.05.5TiAda-Adam(ηx,ηy=0.01)Adam(ηx,ηy=0.01)TiAda-Adam(ηx,ηy=0.005)Adam(ηx,ηy=0.005)TiAda-Adam(ηx,ηy=0.001)Adam(ηx,ηy=0.001)Published as a conference paper at ICLR 2023\n\nACKNOWLEDGMENT\n\nWe thank Dr. Anas Barakat and anonymous reviewers for their valuable feedback. The work is supported by ETH research grant and Swiss National Science Foundation (SNSF) Project Funding No. 200021-207343.\n\nREFERENCES\n\nKimon Antonakopoulos, Veronica Belmega, and Panayotis Mertikopoulos. An adaptive mirror-prox\n\nmethod for variational inequalities with singular operators. NeurIPS, 32, 2019.\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein generative adversarial networks.\n\nIn ICML, pp. 214–223. PMLR, 2017.\n\nFrancis Bach and Kfir Y Levy. A universal algorithm for variational inequalities adaptive to smooth-\n\nness and noise. In Conference on Learning Theory, pp. 164–194. PMLR, 2019.\n\nRadu Ioan Bot ̧ and Axel B ̈ohm. Alternating proximal-gradient steps for (stochastic) nonconvex-\n\nconcave minimax problems. arXiv preprint arXiv:2007.13605, 2020.\n\nTianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating stochastic\n\ngradient methods for bilevel problems. 2021.\n\nXiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type\n\nalgorithms for non-convex optimization. arXiv preprint arXiv:1808.02941, 2018.\n\nBo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from conditional distributions\n\nvia dual embeddings. In AISTATS, pp. 1458–1467. PMLR, 2017.\n\nConstantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with\n\noptimism. In ICLR, 2018.\n\nConstantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained min-max optimization. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pp. 1466–1478, 2021.\n\nAlexandre D ́efossez, L ́eon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence proof\n\nof adam and adagrad. arXiv preprint arXiv:2003.02395, 2020.\n\nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize\n\nfor deep nets. In ICML, pp. 1019–1028. PMLR, 2017.\n\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\n\nstochastic optimization. Journal of machine learning research, 12(7), 2011.\n\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimiza-\n\ntion for efficiently improving generalization. In ICLR, 2021.\n\nDean Foster and Hobart Peyton Young. Regret testing: Learning to play nash equilibrium without\n\nknowing you have an opponent. Theoretical Economics, 1(3):341–367, 2006.\n\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ̧ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016.\n\nGauthier Gidel, Hugo Berard, Ga ̈etan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. arXiv preprint inequality perspective on generative adversarial networks.\n\nA variational arXiv:1802.10551, 2018.\n\nIan Goodfellow.\n\nNips 2016 tutorial: Generative adversarial networks.\n\narXiv preprint\n\narXiv:1701.00160, 2016.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\n\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 27, 2014.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. In ICLR, 2015.\n\nGreen9. Pytorch code for gan models. https://github.com/Zeleni9/pytorch-wgan,\n\n2018.\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-\n\nproved training of wasserstein gans. NeurIPS, 30, 2017.\n\nZhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis for\n\nalgorithms of the adam family. arXiv preprint arXiv:2112.03459, 2021.\n\nHe He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daum ́e III. Opponent modeling in deep rein-\n\nforcement learning. In ICML, pp. 1804–1813. PMLR, 2016.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\nGeoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning\n\nlecture 6a overview of mini-batch gradient descent. Cited on, 14(8):2, 2012.\n\nFeihu Huang and Heng Huang. Adagda: Faster adaptive gradient descent ascent methods for mini-\n\nmax optimization. arXiv preprint arXiv:2106.16101, 2021.\n\nFeihu Huang, Xidong Wu, and Heng Huang. Efficient mirror descent ascent methods for nonsmooth\n\nminimax problems. NeurIPS, 34:10431–10443, 2021.\n\nAli Kavis, Kfir Y Levy, Francis Bach, and Volkan Cevher. Unixgrad: A universal, adaptive algorithm\n\nwith optimal guarantees for constrained optimization. NeurIPS, 32, 2019.\n\nAli Kavis, Kfir Levy, and Volkan Cevher. High probability bounds for a class of nonconvex algo-\n\nrithms with adagrad stepsize. In ICLR, 2022.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nDaphne Koller and Avi Pfeffer. Generating and solving imperfect information games. In IJCAI, pp.\n\n1185–1193. Citeseer, 1995.\n\nGalina M Korpelevich. The extragradient method for finding saddle points and other problems.\n\nMatecon, 12:747–756, 1976.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nYann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.\n\nKfir Levy. Online to offline conversions, universality and adaptive minibatch sizes. NeurIPS, 30,\n\n2017.\n\nKfir Levy, Ali Kavis, and Volkan Cevher. Storm+: Fully adaptive sgd with recursive momentum for\n\nnonconvex optimization. NeurIPS, 34:20571–20582, 2021.\n\nKfir Y Levy, Alp Yurtsever, and Volkan Cevher. Online adaptive methods, universality and acceler-\n\nation. NeurIPS, 31, 2018.\n\nHaochuan Li, Yi Tian, Jingzhao Zhang, and Ali Jadbabaie. Complexity lower bounds for nonconvex-\n\nstrongly-concave min-max optimization. NeurIPS, 34:1792–1804, 2021.\n\nHaochuan Li, Farzan Farnia, Subhro Das, and Ali Jadbabaie. On convergence of gradient descent\n\nascent: A tight local analysis. In ICML, pp. 12717–12740. PMLR, 2022.\n\nXiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. In The 22nd international conference on artificial intelligence and statistics, pp. 983– 992. PMLR, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nXiaoyu Li and Francesco Orabona. A high probability analysis of adaptive sgd with momentum.\n\narXiv preprint arXiv:2007.14294, 2020.\n\nTianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave mini-\n\nmax problems. In ICML, pp. 6083–6093. PMLR, 2020a.\n\nTianyi Lin, Chi Jin, and Michael I Jordan. Near-optimal algorithms for minimax optimization. In\n\nConference on Learning Theory, pp. 2738–2779. PMLR, 2020b.\n\nLouis Lv. cipled Reproducing-certifiable-distributional-robustness, 2019.\n\nprinhttps://github.com/Louis-udm/\n\nrobustness with\n\ndistributional\n\nReproducing\n\nadversarial\n\n”certifying\n\ntraining”.\n\nsome\n\nDavid J Miller, Zhen Xiang, and George Kesidis. Adversarial learning targeting deep neural network classification: A comprehensive review of defenses against attacks. Proceedings of the IEEE, 108 (3):402–433, 2020.\n\nKonstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richt ́arik, and Yura Malitsky. Revisiting stochastic extragradient. In International Conference on Artificial Intelligence and Statistics, pp. 4573–4582. PMLR, 2020.\n\nAditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free representation learning and exploration in low-rank mdps. arXiv preprint arXiv:2102.07035, 2021.\n\nYurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer\n\nScience & Business Media, 2003.\n\nMaher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving a class of non-convex min-max games using iterative first order methods. NeurIPS, 32, 2019.\n\nLeonid Denisovich Popov. A modification of the arrow-hurwicz method for search of saddle points.\n\nMathematical notes of the Academy of Sciences of the USSR, 28(5):845–848, 1980.\n\nAlexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Confer-\n\nence on Learning Theory, pp. 993–1019. PMLR, 2013.\n\nSashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In\n\nICLR, 2018.\n\nAmirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated learning: The case of affine distribution shifts. Advances in Neural Information Processing Systems, 33:21554–21565, 2020.\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\n\nImproved techniques for training gans. NeurIPS, 29, 2016.\n\nOthmane Sebbouh, Marco Cuturi, and Gabriel Peyr ́e. Randomized stochastic gradient descent as-\n\ncent. In AISTATS, pp. 2941–2969. PMLR, 2022.\n\nAman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with\n\nprincipled adversarial training. In ICLR, 2018.\n\nMatthew Staib, Sashank Reddi, Satyen Kale, Sanjiv Kumar, and Suvrit Sra. Escaping saddle points with adaptive gradient methods. In International Conference on Machine Learning, pp. 5956– 5965. PMLR, 2019.\n\nRachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex\n\nlandscapes. The Journal of Machine Learning Research, 21(1):9047–9076, 2020.\n\nYuege Xie, Xiaoxia Wu, and Rachel Ward. Linear convergence of adaptive stochastic gradient\n\ndescent. In AISTATS, pp. 1475–1485. PMLR, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nJunchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems. Advances in Neural Information Processing Systems, 33:1153–1165, 2020.\n\nJunchi Yang, Xiang Li, and Niao He. Nest your adaptive algorithm for parameter-agnostic noncon-\n\nvex minimax optimization. NeurIPS, 2022a.\n\nJunchi Yang, Antonio Orvieto, Aurelien Lucchi, and Niao He. Faster single-loop algorithms for minimax optimization without strong concavity. In AISTATS, pp. 5485–5517. PMLR, 2022b.\n\nSiqi Zhang, Junchi Yang, Crist ́obal Guzm ́an, Negar Kiyavash, and Niao He. The complexity of nonconvex-strongly-concave minimax optimization. In Uncertainty in Artificial Intelligence, pp. 482–492. PMLR, 2021.\n\nXuan Zhang, Necdet Serhat Aybat, and Mert Gurbuzbalaban. Sapd+: An accelerated stochastic method for nonconvex-concave minimax problems. arXiv preprint arXiv:2205.15084, 2022a.\n\nYushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can converge\n\nwithout any modification on update rules. arXiv preprint arXiv:2208.09632, 2022b.\n\nDongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu. On arXiv preprint\n\nthe convergence of adaptive gradient methods for nonconvex optimization. arXiv:1808.05671, 2018.\n\nA SUPPLEMENTARY TO EXPERIMENTS\n\nTable 1: Stepsize schemes fit in generalized TiAda. See also Yang et al. (2022a).\n\nAlgorithms\n\nfirst moment parameter βt\n\nAdaGrad (TiAda) GDA Adam AMSGrad\n\nβt = 0 βt = 0 0 < βt < 1 0 < βt < 1\n\ni=0 u2\n\nsecond moment function ψ (cid:0)v0, v0 + (cid:80)t 1\nγt+1v0 + (1 −\nmaxm=0,...,t γm+1v0 + (1\n\ni=0 γt−iu2\n\nγ) (cid:80)t\n\ni\n\ni\n\n(cid:1)\n\nu2\n\ni }\n\n{\n\nt i=0\n\nγ) (cid:80)m\n\ni=0 γm−iu2\n\ni\n\n−\n\nA.1 EXPERIMENTAL DETAILS\n\nIn this section, we will summarize the experimental settings and hyper-parameters used. As we mentioned, since we try to develop a parameter-agnostic algorithm without tuning the hyper-parameters much, if not specified, we simply use α = 0.6 and β = 0.4 for all experiments. For fair comparisons, we used the same hyper-parameters when comparing our TiAda with other algorithms.\n\nTest Functions For Figure 1 and the first row of Figure 2, we conduct experiments on problem (2) with L = 2. We use initial stepsize ηy = 0.2 and initial point (1, 0.01) for all runs. As for the McCormick function used in the second row of Figure 2, we chose ηy = 0.01, and the noises added to the gradients are from zero-mean Gaussian distribution with variance 0.01.\n\nDistributional Robustness Optimization For results shown in Figures 3, 6 and 7, we adapt code from Lv (2019), and used the same hyper-parameter setting as Sinha et al. (2018); Sebbouh et al. (2022), i.e., γ = 1.3. The model we used is a three layer convolutional neural network (CNN) with a final fully-connected layer. For each layer, batch normalization and ELU activation are used. The width of each layer is (32, 64, 128, 512). The setting is the same as Sinha et al. (2018); Yang et al. (2022a). We set the batchsize as 128, and for the Adam-like optimizers, including Adam, NeAdaAdam and TiAda-Adam, we use β1 = 0.9, β2 = 0.999 for the first moment and second moment parameters.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nGenerative Adversarial Networks For this part, we use the code adapted from Green9 (2018). To produce the results in Figure 4, a four layer CNN and a four layer CNN with transpose convolution layers are used respectively for the discriminator and generator. Following a similar setting as Daskalakis et al. (2018), we set batchsize as 512, the dimension of latent variable as 50 and the weight of gradient penalty term as 10−4. For the Adam-like optimizers, we set β1 = 0.5, β2 = 0.9. To get the inception score, we feed the pre-trained inception network with 8000 synthesized samples.\n\nA.2 ABLATION STUDY ON CONVERGENCE BEHAVIOR WITH DIFFERENT α AND β\n\nWe conduct experiments on the quadratic minimax problem (2) with L = 2 to study the effect of hyper-parameters α and β on the convergence behavior of TiAda. As discussed in Sections 1 and 3.2, we refer to the period before the stepsize ratio reduce to the convergence threshold as Stage I, and the period after that as Stage II. In order to accentuate the difference between these two stages, we pick a large initial stepsize ratio ηx/ηy = 20. We compare 4 different pairs of α and β: α. From Figure 5, we observed that as soon as TiAda enters α\nStage II, the norm of gradients start to drop. Moreover, the closer α and β are to 0.5, the more time TiAda remains in Stage I, which confirms the intuitions behind our analysis in Section 3.2.\n\n0.59, 0.6, 0.61, 0.62\n\nand β = 1\n\n∈ {\n\n−\n\n}\n\n(a) effective stepsize ratio\n\n(b) convergence\n\nFigure 5: Illustration of the effect of α and β on the two stages in TiAda’s time-scale adaptation process. We set β = 1 α. The dashed line on the right plot represents the first iteration when the effective stepsize ratio is below 1/κ.\n\n−\n\nA.3 ADDITIONAL EXPERIMENTS ON DISTRIBUTIONAL ROBUSTNESS OPTIMIZATION\n\nWe use a grid of stepsize combinations to evaluate TiAda and compare it with NeAda and GDA with corresponding adaptive stepsizes. For AdaGrad-like algorithms, we use }\nfor both ηx and ηy, and the results are reported in Figure 6. For Adam-like algorithms, we use for ηy, and the results are shown in 0.1, 0.05, 0.005, 0.001 {\nFigure 7. We note that since Adam uses the reciprocal of the moving average of gradient norms, it is extremely unstable when the gradients are small. Therefore, Adam-like algorithms often experience instability when they are near stationary points.\n\n0.1, 0.05, 0.01, 0.0005\n\n0.001, 0.0005, 0.0001\n\nfor ηx and\n\n}\n\n}\n\n{\n\n{\n\nB HELPER LEMMAS\n\nLemma B.1 (Lemma A.2 in Yang et al. (2022a)). Let x1, ..., xT be a sequence of non-negative real numbers, x1 > 0 and 0 < α < 1. Then we have\n\n(cid:33)1−α\n\n(cid:32) T\n\n(cid:88)\n\nt=1\n\nxt\n\nT (cid:88)\n\nt=1\n\n≤\n\nxt\n\n(cid:16)(cid:80)t\n\nk=1 xk\n\n(cid:17)α ≤\n\n1\n\n(cid:32) T\n\n(cid:88)\n\nt=1\n\n1\n\n−\n\nα\n\n(cid:33)1−α\n\nxt\n\n.\n\n14\n\n0100002000030000400005000060000#gradientcalls0.250.501.002.50ηxt/ηytα=0.59α=0.6α=0.61α=0.62ηxt/ηyt=1/κ0100002000030000400005000060000#gradientcalls050100150200250300350400k∇xf(x,y)kα=0.59α=0.6α=0.61α=0.62Published as a conference paper at ICLR 2023\n\n(a) ηx = 0.1, ηy = 0.1\n\n(b) ηx = 0.1, ηy = 0.05\n\n(c) ηx = 0.1, ηy = 0.01\n\n(d) ηx = 0.1, ηy = 0.005\n\n(e) ηx = 0.05, ηy = 0.1\n\n(f) ηx = 0.05, ηy = 0.05\n\n(g) ηx = 0.05, ηy = 0.01\n\n(h) ηx = 0.05, ηy = 0.005\n\n(i) ηx = 0.01, ηy = 0.1\n\n(j) ηx = 0.01, ηy = 0.05\n\n(k) ηx = 0.01, ηy = 0.01\n\n(l) ηx = 0.01, ηy = 0.005\n\n(m) ηx = 0.005, ηy = 0.1\n\n(n) ηx = 0.005, ηy = 0.05\n\n(o) ηx = 0.005, ηy = 0.01 (p) ηx = 0.005, ηy = 0.005\n\nFigure 6: Gradient norms in x of AdaGrad-like algorithms on distributional robustness optimization (5). We use i in the legend to indicate the number of inner loops.\n\nWhen α = 1, we have\n\nT (cid:88)\n\nt=1\n\nxt\n\n(cid:16)(cid:80)t\n\nk=1 xk\n\n1 + log\n\n(cid:17)α ≤\n\n(cid:32) (cid:80)t\n\nt=1 xt x1\n\n(cid:33)\n\n.\n\nLemma B.2 (smoothness of Φ( Under Assumptions 3.1 and 3.2, we have Φ( and y∗( ) is κ-Lipschitz. ·\n\n) and Lipschitzness of y∗( ·\n\n) is (l + κl)-smooth with ·\n\n∇\n\n). Lemma 4.3 in Lin et al. (2020a)). ·\nxf (x, y∗(x)),\n\nΦ(x) =\n\nLemma B.3 (smoothness of y∗( and 3.6, we have that with (cid:98)L = L+Lκ\n\n). Lemma 2 in Chen et al. (2021)). Under Assumptions 3.1, 3.2 ·\nμ + l(L+Lκ)\n\nμ2\n\n,\n\n∇\n\ny∗(x1)\n\n∥∇\n\n− ∇\n\ny∗(x2)\n\n∥ ≤\n\nC PROOFS\n\n(cid:98)L\n\nx1\n\n∥\n\nx2\n\n.\n\n∥\n\n−\n\nFor notational convenience in the proofs, we denote the stochastic gradient as α , γt =\n\ny (cid:101)f (xt, yt). Also denote y∗\n\nt = y∗(xt), ηt =\n\nΦ∗. We use 1 as the indicator function.\n\nηx t+1,vy vx\n\nt+1}\n\nmax\n\n{\n\n∇ minx∈Rd1 Φ(x), and ∆Φ = Φmax\n\n−\n\nx (cid:101)f (xt, yt) and ηy t+1)β , Φ∗ =\n\n∇ (vy\n\n15\n\n020000400006000080000#gradientcalls10−310−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−310−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−310−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGrad020000400006000080000#gradientcalls10−210−1100101TiAdaAdaGrad(i=1)AdaGrad(i=15)NeAda-AdaGradPublished as a conference paper at ICLR 2023\n\n(a) ηx = 0.001, ηy = 0.1\n\n(b) ηx = 0.001, ηy = 0.05\n\n(c) ηx = 0.001, ηy = 0.005\n\n(d) ηx = 0.001, ηy = 0.001\n\n(e) ηx = 0.0005, ηy = 0.1\n\n(f) ηx = 0.0005, ηy = 0.05\n\n(g) ηx = 0.0005, ηy = 0.005\n\n(h) ηx = 0.0005, ηy = 0.001\n\n(i) ηx = 0.0001, ηy = 0.1\n\n(j) ηx = 0.0001, ηy = 0.05\n\n(k) ηx = 0.0001, ηy = 0.005\n\n(l) ηx = 0.0001, ηy = 0.001\n\nFigure 7: Gradient norms in x of Adam-like algorithms on distributional robustness optimization (5). We use i in the legend to indicate the number of inner loops.\n\nC.1 PROOF OF THEOREM 3.1\n\nWe present a formal version of Theorem 3.1.\n\nTheorem C.1 (deterministic setting). Under Assumptions 3.1 to 3.3, Algorithm 1 with deterministic gradient oracles satisfies that for any 0 < β < α < 1, after T iterations,\n\nT −1 (cid:88) t=0 ∥∇\n\nxf (xt, yt)\n\n2\n\nmax\n\n{\n\n≤\n\n5C1, 2C2\n\n,\n\n}\n\n∥\n\nwhere\n\nC1 = vx\n\n0 +\n\n(cid:18) 2∆Φ ηx\n\n(cid:19) 1\n\n1−α\n\n(cid:32)\n\n+\n\n(cid:19) 1\n\n1−α\n\n(cid:32)\n\n+\n\n+\n\n(cid:18) c1c5 ηx (cid:34) (cid:32)\n\nC2 = vx\n\n0 +\n\n4κle(1−α)(1−log vx e(1\n\nα) (vx\n\n0 )2α−1\n\n0 )/2\n\n−\n\n2c1c4ηxe(1−α)(1−log vx 0 )2α−β−1 α) (vx\n\ne(1\n\n0 )/2\n\n−\n\n(cid:33) 2\n\n1−α\n\n12α≥1 +\n\n(cid:18) 2κl 1\n\n2α\n\n(cid:19) 1\n\nα\n\n12α<1\n\n(cid:33) 2\n\n1−α\n\n12α−β≥1 +\n\n(cid:19) 1\n\nα−β\n\n12α−β<1\n\n− (cid:18) c1c4ηx\n\n2α + β\n\n1\n\n−\n\n+\n\n2κle(1−2α+β)(1−log vx 0 ) 2α + β) (vx e(1\n\n0 )2α−1 12α≥1 +\n\n−\n\n1\n\n(1\n\n−\n\n(cid:33)\n\n2κl 2α) (vx\n\n0 )β 12α<1\n\n(cid:32)\n\n+\n\n(cid:32)\n\n2∆Φ + c1c5\n\nηx (vx\n\n0 )1−2α+β + c4 (ηx)2\n\nc1c4ηx\n\n1\n\n2α + β\n\n− (cid:33) α\n\n1−β (cid:35)\n\nc5\n\n0 )1−2α+β +\n\n(vx (cid:34) (cid:32)\n\n2α + β\n\n1\n\n−\n\n2∆Φ + c1c5 ηx (vx\n\n0 )1/4 +\n\n8κle(1−log vx\n\n0 )/4\n\ne (vx\n\n0 )2α−1 + 4c4α (ηx)2 e(1−β)(1−log vx 0 )2α−β−1\n\nβ) (vx\n\ne(1\n\n−\n\nc5\n\n(1−β) 4α\n\n(vx 0 )\n\n+\n\n1−(1−2α+β)(1+ α\n\n1−β )\n\n12α−β<1\n\n(cid:33)\n\n0 )/4\n\n4c1c4ηxe(1−log vx 0 )2α−β−1 1−β (cid:35)2\n\ne (vx\n\n(cid:33) α\n\n0 )/(4α)\n\n12α≥1,\n\n16\n\n020000400006000080000#gradientcalls10−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−410−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−410−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−410−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−610−410−2100TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−510−410−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−410−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−410−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-Adam020000400006000080000#gradientcalls10−310−210−1100101TiAda-AdamAdam(i=1)Adam(i=15)NeAda-AdamPublished as a conference paper at ICLR 2023\n\nwith ∆Φ = Φ(x0)\n\nΦ∗,\n\nc1 =\n\n−\n\nηxκ2\n\n(cid:1)α−β ,\n\nc2 = max\n\n(cid:26) 4ηyμl μ + l\n\n(cid:27)\n\n, ηy(μ + l)\n\n,\n\nc1/β\n\n2\n\n,\n\nc4 = (μ + l)\n\n(cid:18) 2κ2 (vy\n\n0 )α +\n\n(μ + l)κ2 ηyμl\n\n(cid:19)\n\n,\n\nηy (cid:0)vy (cid:33)\n\nt0\n\nc3 = 4(μ + l)\n\n(cid:32)\n\n1\n\nμ2 +\n\nc5 = c3 +\n\n0\n\nηyvy 0 )β + (vy\n\n(cid:1)β\n\nηy (cid:0)vy\n\nt0 1−β β\n\n.\n\nβ\n\n2\n\nηyc 1\n\n−\n\nIn addition, denoting the above upper bound for (cid:80)T −1\n\nxf (xt, yt)\n\n2 as C3, we have\n\nt=0 ∥∇\n\n∥\n\nT −1 (cid:88) t=0 ∥∇\n\nyf (xt, yt)\n\n(cid:32)\n\n(cid:32)\n\nc5 + c4 (ηx)2\n\n2 ∥\n\n≤\n\n1 + log C3 (vx\n\n− 0 )2α−β−1\n\nlog vx\n\n0\n\n12α−β≥1 +\n\n3\n\nC 1−2α+β 2α + β 1\n\n−\n\n(cid:33)(cid:33) 1\n\n1−β\n\n12α−β<1\n\n.\n\nProof. Let us start from the smoothness of the primal function Φ(\n\n). By Lemma B.2, ·\n\nΦ(xt+1)\n\nΦ(xt)\n\n≤ = Φ(xt)\n\nΦ(xt)\n\n≤\n\n= Φ(xt)\n\n= Φ(xt)\n\nΦ(xt)\n\nΦ(xt)\n\nΦ(xt)\n\n≤\n\n≤\n\n≤\n\n−\n\n−\n\n−\n\n−\n\n−\n\n−\n\n−\n\n−\n\nηt\n\nηt\n\nηt\n\nΦ(xt+1), ⟨\n\n∇ xf (xt, yt)\n\n∥∇\n\nxf (xt, yt)\n\n+ klη2\n\nt ∥∇ xf (xt, yt)\n\nxf (xt, yt)\n\n⟨∇\n\nxf (xt, yt) ⟩\n2 + ηt ∥\nηt 2 + 2 ∥∇ ∥\n2 + klη2 ∥\n2 + klη2 ∥\n\nt ∥∇\n\nt ∥∇\n\nxf (xt, yt)\n\n2\n\n− ∇ 2 + ∥\n\n∥ Φ(xt), ηt 2 ∥∇ ηt 2 ∥∇\n\n2 + ∥\n2 + ∥\n\nxf (xt, yt)\n\nxf (xt, yt)\n\nxf (xt, yt)\n\nxf (xt, yt)\n\n∥∇\n\nηt 2 ∥∇ ηt 2 ∥∇\n\nηt 2 ∥∇\n\nxf (xt, yt)\n\n2 + klη2 ∥\n\nt ∥∇\n\nxf (xt, yt)\n\n2 + ∥\n\nηt 2 ∥∇\n\nxf (xt, yt)\n\n2 + klη2 ∥\n\nt ∥∇\n\nxf (xt, yt)\n\n2 + ∥\n\nηt 2 ∥∇\n\nxf (xt, yt)\n\n2 + klη2 ∥\n\nt ∥∇\n\nxf (xt, yt)\n\n2 + ∥\n\nxf (xt, yt)\n\n∇ xf (xt, yt)\n\n⟩\n\n+ klη2\n\nt ∥∇\n\nxf (xt, yt)\n\nΦ(xt)\n\n− ∇\n\n2 + klη2 ∥\n\nt ∥∇\n\n2 ∥\nxf (xt, yt)\n\n2\n\n∥\n\nxf (xt, yt)\n\nΦ(xt)\n\n2 ∥\n\n− ∇\n\nt+1\n\nηx 2 max (cid:8)vx t+1, vy ηx (cid:1)α−β (cid:0)vy ηxκ2 (cid:1)α−β (cid:0)vy\n\n2 (cid:0)vy\n\n2 (cid:0)vy\n\nt0\n\nt+1\n\nt+1\n\nt0 ηxκ2\n\n2ηy (cid:0)vy\n\nt0\n\n(cid:1)α−β ·\n\nxf (xt, yt)\n\n(cid:9)α ∥∇\n\nΦ(xt)\n\n2 ∥\n\n− ∇\n\nxf (xt, yt)\n\n(cid:1)β ∥∇\n\nΦ(xt)\n\n2 ∥\n\n− ∇\n\nyf (xt, yt)\n\n2 ∥\n\n(cid:1)β ∥∇\n\nγt\n\n∥∇\n\nyf (xt, yt)\n\n2, ∥\n\nwhere in the second to last inequality, we used the strong-concavity of f (x,\n\nl\n\nyt\n\n∥\n\n−\n\ny∗ t ∥ ≤\n\nκ\n\n∥∇\n\nyf (xt, yt)\n\n.\n\n): ·\n\n∥\n\n∥ ≤ Telescoping and rearranging the terms, we have\n\n− ∇\n\n∥∇\n\nxf (xt, yt)\n\nΦ(xt)\n\nT −1 (cid:88)\n\nt=0\n\nηt\n\n∥∇\n\nxf (xt, yt)\n\n2\n\n∥\n\n≤\n\n2 (Φ(x0) (cid:123)(cid:122) ∆Φ\n\n(cid:124)\n\n−\n\n+2κl\n\nΦ∗) (cid:125)\n\nT −1 (cid:88)\n\nt=0\n\nη2 t ∥∇\n\nxf (xt, yt)\n\n∥\n\n2 +\n\nηxκ2\n\n(cid:1)α−β\n\nT −1 (cid:88)\n\nt=0\n\nγt\n\n∥∇\n\nyf (xt, yt)\n\n2\n\n∥\n\nηy (cid:0)vy t0 (cid:124) (cid:123)(cid:122) c1\n\n(cid:125)\n\nγt\n\nyf (xt, yt)\n\n∥∇\n\n2\n\n∥\n\n= 2∆Φ +\n\n2∆Φ +\n\nT −1 (cid:88)\n\nt=0\n\nT −1 (cid:88)\n\nt=0\n\n≤\n\n≤\n\n2κlηx t+1, vy\n\nt+1\n\nmax (cid:8)vx\n\nxf (xt, yt)\n\n(cid:9)2α ∥∇\n\n2 + c1\n\nT −1 (cid:88)\n\nt=0\n\n∥\n\n2κlηx\n\n(cid:0)vx t+1 (cid:32)\n\n(cid:1)2α ∥∇\n\nxf (xt, yt)\n\n2 + c1 ∥\n\nT −1 (cid:88)\n\nt=0\n\nγt\n\n∥∇\n\nyf (xt, yt)\n\n2 ∥\n\n2∆Φ + 2κlηx\n\n1 + log vx\n\nlog vx\n\n0\n\nT − 0 )2α−1\n\n(vx\n\nT )1−2α (vx 1\n\n2α ·\n\n−\n\n(cid:33)\n\n12α<1\n\n+ c1\n\nT −1 (cid:88)\n\nt=0\n\nγt\n\n∥∇\n\nyf (xt, yt)\n\n2. ∥\n(6)\n\n12α≥1 +\n\n·\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nWe proceed to bound (cid:80)T −1 (cid:110) 4ηyμl μ+l , ηy(μ + l)\n\nc2 := max\n\nt=0 γt (cid:111)\n\nyf (xt, yt) ∥∇ . We have vy\n\n2. Let t0 be the first iteration such that (cid:0)vy ∥\nt0, t0 ≤\n\n, and for t\n\nc1/β\n\n≥\n\n2\n\n(cid:1)β\n\n>\n\nt0+1\n\n(cid:13) (cid:13)yt+1\n\n−\n\ny∗\n\nt+1\n\n(cid:13) 2\n(cid:13)\n\n≤\n\n≤\n\n(1 + λt)\n\nyt+1\n\n∥ (cid:32)\n\n2 +\n\ny∗ t ∥\n\n−\n\n(1 + λt)\n\nyt\n\n∥\n\n−\n\n2 +\n\ny∗ t ∥\n\n(cid:124)\n\n(cid:19)\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n(cid:18)\n\n1 +\n\n1 λt (ηy)2\n\n(cid:0)vy\n\nt+1\n\n(cid:1)2β ∥∇\n\nyf (xt, yt)\n\n(cid:123)(cid:122) (A)\n\n2 +\n\n2ηy\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β ⟨\n\ny∗ t ,\n\nyt\n\n−\n\nyf (xt, yt) ⟩\n\n∇\n\n(cid:33)\n\n(cid:125)\n\n∥\n\n(cid:18)\n\n+\n\n1 +\n\n(cid:19)\n\n1 λt\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n,\n\nwhere λt > 0 will be determined later. For l-smooth and μ-strongly convex function g(x), according to Theorem 2.1.12 in Nesterov (2003), we have\n\ng(x)\n\n⟨∇\n\n− ∇\n\ng(y), x\n\ny\n\n−\n\n⟩ ≥\n\nμl μ + l ∥\n\nx\n\ny\n\n2 + ∥\n\n−\n\n1\n\nμ + l ∥∇\n\ng(x)\n\n2.\n\ng(y)\n\n∥\n\n− ∇\n\nTherefore,\n\nTerm (A)\n\n(cid:32)(cid:32)\n\n(1 + λt)\n\n≤\n\n1\n\n−\n\n(cid:33)\n\n2ηyμl (μ + l) (cid:0)vy\n\nt+1\n\n(cid:1)β\n\nyt\n\n∥\n\n−\n\n2 +\n\ny∗ t ∥\n\n(cid:32)\n\n(ηy)2\n\n2ηy\n\n(cid:33)\n\n(cid:0)vy\n\nt+1\n\n(cid:1)2β −\n\n(μ + l) (cid:0)vy\n\nt+1\n\n(cid:1)β\n\nLet λt =\n\nηyμl t+1)β\n\n(μ+l)(vy\n\n−2ηyμl\n\n. Note that λt > 0 after t0. Then\n\nyf (xt, yt)\n\n∥∇\n\n(cid:33)\n\n2\n\n.\n\n∥\n\nTerm (A) (cid:32)\n\n1\n\n−\n\n≤\n\nηyμl (μ + l) (cid:0)vy\n\nt+1\n\n(cid:33)\n\n(cid:1)β\n\nyt\n\n∥\n\n−\n\n2 + (1 + λt)\n\ny∗ t ∥\n\n(cid:32)\n\n(ηy)2\n\n(cid:1)2β −\n\n(cid:0)vy\n\nt+1 (cid:33)\n\n2ηy\n\n(cid:33)\n\n(μ + l) (cid:0)vy\n\nt+1\n\n(cid:1)β\n\nyf (xt, yt)\n\n∥∇\n\n2\n\n∥\n\nyt\n\n≤ ∥\n\ny∗ t ∥\n\n−\n\n2 + (1 + λt)\n\n(cid:124)\n\n(cid:32)\n\n(ηy)2\n\n(cid:0)vy\n\nt+1\n\n(cid:1)2β − (cid:123)(cid:122) (B)\n\n2ηy\n\n(μ + l) (cid:0)vy\n\nt+1\n\n(cid:1)β\n\n∥∇\n\n(cid:125)\n\nyf (xt, yt)\n\n2. ∥\n\nηy(μ + l), we have term (B)\n\nηy (μ+l)(vy\n\nt+1)β . Putting them back,\n\n≤ −\n\nAs 1 + λt\n\nwe can get\n\n≥\n\n1 and (cid:0)vy\n\nt+1\n\n(cid:1)β\n\n≥\n\nηy\n\ny∗\n\nt+1\n\n(cid:13) 2\n(cid:13)\n\n(cid:13) (cid:13)yt+1\n\nyt\n\n≤ ∥\n\n−\n\n− y∗ t ∥\n\n2\n\n−\n\n(μ + l) (cid:0)vy\n\nt+1\n\nηy\n\n−\n\n(μ + l) (cid:0)vy\n\nt+1\n\nηy\n\n−\n\n(μ + l) (cid:0)vy\n\nt+1\n\nηy\n\n−\n\n(μ + l) (cid:0)vy\n\nt+1\n\nyt\n\n≤ ∥\n\n2\n\ny∗ t ∥\n\n−\n\nyt\n\n≤ ∥\n\n2\n\ny∗ t ∥\n\n−\n\n=\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\nyf (xt, yt)\n\n(cid:1)β ∥∇\n\n(cid:18)\n\n1 +\n\n2 +\n\n(cid:19)\n\n1 λt\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n∥\n\nyf (xt, yt)\n\n(cid:1)β ∥∇\n\nyf (xt, yt)\n\n(cid:1)β ∥∇\n\nyf (xt, yt)\n\n(cid:1)β ∥∇\n\n(cid:1)β\n\n(μ + l) (cid:0)vy ηyμl\n\nt+1\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n(μ + l)κ2 (cid:0)vy\n\nt+1\n\n(cid:1)β\n\nηyμl\n\nxx+1\n\n∥\n\n2\n\nxt\n\n∥\n\n−\n\n(μ + l)κ2 (cid:0)vy ηyμl\n\nt+1\n\n(cid:1)β\n\nη2\n\nt\n\n∥∇\n\nxf (xt, yt)\n\n2.\n\n∥\n\n2 +\n\n2 +\n\n2 +\n\n∥\n\n∥\n\n∥\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nThen, by telescoping, we have\n\nT −1 (cid:88)\n\nt=t0\n\nηy\n\n(μ + l) (cid:0)vy\n\nt+1\n\nyf (xt, yt)\n\n(cid:1)β ∥∇\n\n2\n\n(cid:13)\n\n(cid:13)yt0 −\n\n≤\n\ny∗ t0\n\n(cid:13) 2\n(cid:13)\n\n+\n\nT −1 (cid:88)\n\nt=t0\n\n(μ + l)κ2 (cid:0)vy ηyμl\n\nt+1\n\n(cid:1)β\n\nη2\n\nt\n\n∥∇\n\nxf (xt, yt)\n\n2. ∥\n\n(7)\n\n∥\n\nFor the first term in the RHS, using Young’s inequality with τ to be determined later, we have\n\n(cid:13)\n\n(cid:13)yt0 −\n\ny∗ t0\n\n(cid:13) 2\n(cid:13)\n\n+ 2(cid:13)\n\n(cid:13)y∗\n\nY (yt0−1 + γt0−1\n\n(cid:13) 2\n(cid:13)\n\ny∗ t0−1\n\n2(cid:13) (cid:13)yt0 − ≤\n(cid:13) = 2 (cid:13) 2(cid:13) (cid:13)yt0−1 + γt0−1 (cid:16)(cid:13)\n\n≤\n\nP\n\n(cid:13)yt0−1\n\n∇\n\ny∗ t0−1\n\ny∗ t0−1\n\n(cid:13) 2\n(cid:13)\n\nt0 − yf (xt0−1, yt0−1)) y∗ t0−1\n\n−\n\n∇\n\nyf (xt0−1, yt0−1) (cid:13) 2\n(cid:13)\n\n+ γ2\n\nt0−1∥∇\n\n−\n\n−\n\nyf (xt0−1, yt0−1)\n\n∥\n\ny∗ t0−1 (cid:13) 2\n(cid:13)\n\n(cid:13) 2\n(cid:13) + 2(cid:13) (cid:13)y∗ 2(cid:17)\n\n+ 2\n\n(cid:13) (cid:13)y∗ t0 − y∗ t0−1 t0 − + 2(cid:13)\n\n(cid:13) 2\n(cid:13)\n\ny∗ t0−1 (cid:13) 2\n(cid:13)\n\ny∗ t0−1\n\n(cid:13) 2\n(cid:13)\n\n(cid:18) 1\n\nμ2 ∥∇\n\nyf (xt0−1, yt0−1)\n\n2 + γ2\n\nt0−1∥∇\n\n∥\n\nyf (xt0−1, yt0−1)\n\n∥\n\n+ 2(cid:13)\n\n(cid:13)y∗\n\nt0 −\n\ny∗ t0−1\n\n(cid:13) 2\n(cid:13)\n\n(cid:13)y∗ t0 − (cid:19)\n\n2\n\n4\n\n4\n\n≤\n\n≤\n\n= 4\n\n4\n\n4\n\n4\n\n4\n\n4\n\n≤\n\n≤\n\n≤\n\n≤\n\n≤\n\n(cid:18) 1\n\nμ2 + γ2\n\nt0−1\n\n(cid:19)\n\nyf (xt0−1, yt0−1)\n\n∥∇\n\n2 + 2(cid:13) ∥\n\n(cid:13)y∗\n\nt0 −\n\ny∗ t0−1\n\n(cid:13) 2\n(cid:13)\n\n(cid:18) 1\n\nμ2 + γ2\n\n0\n\n(cid:19)\n\nvy t0 + 2\n\n(cid:13) (cid:13)y∗\n\nt0 −\n\ny∗ t0−1\n\n(cid:13) 2\n(cid:13)\n\n(cid:32)\n\n(cid:32)\n\n(cid:32)\n\n(cid:32)\n\n1\n\nμ2 +\n\n1\n\nμ2 +\n\n1\n\nμ2 +\n\n1\n\nμ2 +\n\n(cid:33)\n\n(cid:33)\n\n(cid:33)\n\n(cid:33)\n\nηy (cid:0)vy\n\nt0 ηy (cid:0)vy\n\nt0 ηy (cid:0)vy\n\nt0\n\n(cid:1)β\n\n(cid:1)β\n\n(cid:1)β\n\nηy (cid:0)vy\n\nt0\n\n(cid:1)β\n\n2 + 2(cid:13) c1/β\n\n(cid:13)y∗\n\nt0 −\n\ny∗ t0−1\n\n(cid:13) 2\n(cid:13)\n\nc1/β 2 + 2κ2\n\nxt0 −\n\n∥\n\nxt0−1\n\n2 ∥\n\nc1/β 2 + 2κ2η2\n\nt0−1∥∇\n\nxf (xt0−1, yt0−1)\n\n2 ∥\n\nc1/β 2 +\n\n(cid:1)β\n\n2κ2 (cid:0)vy 0 )β (vy\n\nt+1\n\nη2 t0−1∥∇\n\nxf (xt0−1, yt0−1)\n\n2. ∥\n\nCombined with Equation (7), we have\n\nyf (xt, yt)\n\n2 ∥\n\n(cid:1)β ∥∇\n\nT −1 (cid:88)\n\nt=t0\n\nηy\n\n(cid:0)vy\n\nt+1 (cid:32)\n\n4(μ + l)\n\n≤\n\n(cid:124)\n\nηy (cid:0)vy\n\nt0\n\n(cid:1)β\n\n(cid:33)\n\nc1/β\n\n2\n\n+ (μ + l)\n\n(cid:124)\n\n(cid:125)\n\n1\n\nμ2 +\n\n(cid:123)(cid:122) c3\n\n(cid:18) 2κ2 (vy\n\n0 )α +\n\n(cid:123)(cid:122) c4\n\n(μ + l)κ2 ηyμl\n\n(cid:19)\n\nT −1 (cid:88)\n\nt=t0−1\n\n(cid:125)\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β\n\nη2 t ∥∇\n\nxf (xt, yt)\n\n2.\n\n∥\n\nBy adding terms from 0 to t0\n\n−\n\n1 and ηyvy\n\n0\n\n0 )β from both sides, we have\n\n(vy\n\nηy\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β ∥∇\n\nyf (xt, yt)\n\n2\n\n∥\n\n0\n\n0\n\nc3 +\n\nt=0\n\nT −1 (cid:88)\n\nηyvy 0 )β + (vy ηyvy 0 )β + c4 (vy ηyvy 0 )β + c4 (vy ηyvy 0 )β + c4 (vy\n\nc3 +\n\nc3 +\n\n0\n\n0\n\n≤\n\n≤\n\n≤\n\nT −1 (cid:88)\n\nt=0\n\nT −1 (cid:88)\n\nt=0\n\nT −1 (cid:88)\n\nt=0\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β\n\nη2 t ∥∇\n\nxf (xt, yt)\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β\n\nη2 t ∥∇\n\nxf (xt, yt)\n\n2 + ∥\n\n2 + ∥\n\nt0−1 (cid:88)\n\nηy\n\nt=t=0\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β ∥∇\n\nyf (xt, yt)\n\n2 ∥\n\n0\n\nηyvy 0 )β + (vy\n\nt0−1 (cid:88)\n\nt=t=0\n\nηy\n\n(cid:0)vy\n\nt+1\n\nyf (xt, yt)\n\n(cid:1)β ∥∇\n\n2\n\n∥\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β\n\nη2 t ∥∇\n\nxf (xt, yt)\n\n2 + ∥\n\n1\n\nηy\n\n−\n\nβ\n\nv1−β\n\nt0\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nc3 +\n\n≤\n\n= c3 +\n\n= c3 +\n\n(cid:124)\n\n0\n\n0\n\nηyvy 0 )β + c4 (vy ηyvy 0 )β + (vy ηyvy 0 )β + (vy\n\n0\n\n2\n\nηyc 1\n\n−\n\n2\n\nηyc 1\n\n−\n\n(cid:123)(cid:122) c5\n\n1−β β\n\nβ\n\n1−β β\n\nβ\n\n(cid:125)\n\nT −1 (cid:88)\n\nt=0\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β\n\nη2 t ∥∇\n\nxf (xt, yt)\n\n2 + ∥\n\nηyc 1\n\n1−β β\n\n2\n\nβ\n\n−\n\n+ c4 (ηx)2\n\nT −1 (cid:88)\n\n+c4 (ηx)2\n\nt=0\n\nT −1 (cid:88)\n\nt=0\n\n(cid:1)β\n\n(cid:0)vy max (cid:8)vx\n\nt+1\n\nt+1, vy\n\nt+1\n\n(cid:9)2α ∥∇\n\nxf (xt, yt)\n\n2 ∥\n\n1 (cid:1)2α−β ∥∇\n\nxf (xt, yt)\n\n2\n\n∥\n\n(cid:0)vx\n\nt+1\n\n(cid:32)\n\nc5 + c4 (ηx)2\n\n≤\n\n1 + log vx (vx\n\nT − 0 )2α−β−1\n\nlog vx\n\n0\n\n12α−β≥1 +\n\n·\n\nT )1−2α+β\n\n2α + β ·\n\n(vx 1\n\n−\n\n(cid:33)\n\n12α−β<1\n\n.\n\nThe LHS can be bounded by (vy above:\n\nT )1−β by Lemma B.1. Then we get two useful inequalities from\n\n \n\n(cid:80)T −1\n\nt=0 γt\n\n(cid:18)\n\n\n\nvy T ≤\n\nyf (xt, yt)\n\n∥∇\n\n2 ∥\n(cid:18)\n\nc5 + c4 (ηx)2\n\n(cid:18)\n\nc5 + c4 (ηx)2\n\n≤\n\nT −log vx\n\n0\n\n1+log vx (vx\n\n0 )2α−β−1\n\n·\n\n12α−β≥1 + (vx\n\nT )1−2α+β 1−2α+β (cid:19)(cid:19) 1\n\n1−β\n\n(cid:19)\n\n12α−β<1\n\n·\n\nT −log vx\n\n0\n\n1+log vx (vx\n\n0 )2α−β−1\n\n12α−β≥1 + (vx\n\nT )1−2α+β 1−2α+β\n\n·\n\n12α−β<1\n\n·\n\n.\n\n(8)\n\n(cid:33)\n\n12α−β<1\n\n.\n\nNow bring it back to Equation (6), we get\n\nT −1 (cid:88)\n\nt=0\n\nηt\n\n∥∇\n\nxf (xt, yt)\n\n2 ∥\n\n(cid:32)\n\n2∆Φ + 2κlηx\n\n≤\n\n+ c1c5 + c1c4 (ηx)2\n\nFor the LHS, we have\n\n1 + log vx\n\nlog vx\n\n0\n\n(vx (cid:32)\n\nT − 0 )2α−1 1 + log vx (vx\n\n12α≥1 +\n\n·\n\n(cid:33)\n\n12α<1\n\nT )1−2α (vx 1\n\n2α ·\n\n−\n\nlog vx\n\n0\n\nT − 0 )2α−β−1\n\n12α−β≥1 +\n\n·\n\nT )1−2α+β\n\n2α + β ·\n\n(vx 1\n\n−\n\nT −1 (cid:88)\n\nt=0\n\nηt\n\n∥∇\n\nxf (xt, yt)\n\n2 = ∥\n\nT −1 (cid:88)\n\nt=0\n\nηx t+1, vy\n\nt+1\n\nmax (cid:8)vx\n\nxf (xt, yt)\n\n(cid:9)α ∥∇\n\n2 ∥\n\n≥\n\nmax\n\nηx T , vy vx\n\nT }\n\n{\n\nT −1 (cid:88) t=0 ∥∇\n\nxf (xt, yt)\n\n2 ∥\n\nα\n\nFrom here, by combining two inequalites above and noting that (cid:80)T −1 can already conclude that (cid:80)T −1 We consider two cases:\n\nxf (xt, yt)\n\n2 = ∥\n\nt=0 ∥∇\n\nT , we (1). Now we will provide an explicit bound.\n\nxf (xt, yt)\n\nt=0 ∥∇\n\n2 ∥\n\nvx\n\nO\n\n≤\n\n(1)\n\nIf vy\n\nT ≤\n\nvx\n\nT , then\n\nxf (xt, yt)\n\nT −1 (cid:88) t=0 ∥∇ 2∆Φ (vx\n\nT )α\n\n∥\n\n2\n\n(cid:32)\n\n≤\n\nηx\n\n+ 2κl\n\n+\n\nc1c5 (vx\n\nT )α\n\nηx\n\n+ c1c4ηx\n\n(vx\n\nlog vx 0 )\n\nT −\n\nT )α (1 + log vx 0 )2α−1 (vx T )α (1 + log vx (vx\n\n(vx\n\n(cid:32)\n\nT − 0 )2α−β−1\n\n12α≥1 +\n\n·\n\n(cid:33)\n\n12α<1\n\nT )1−α\n\n(vx 1\n\n−\n\n2α ·\n\nlog vx 0 )\n\n12α−β≥1 +\n\n·\n\nT )1−α+β\n\n2α + β ·\n\n(vx 1\n\n−\n\n(cid:33)\n\n12α−β<1\n\n20\n\nPublished as a conference paper at ICLR 2023\n\n=\n\n2∆Φ (vx\n\nT )α\n\nηx\n\n+ 2κl\n\n(cid:32)\n\n(vx\n\nT )α (vx T )\n\n1−α 2\n\nα−1 2\n\n(vx\n\nT ) (vx\n\n0 )2α−1\n\n(1 + log vx\n\nT −\n\nlog vx 0 )\n\n12α≥1 +\n\n·\n\n(vx 1\n\nT )1−α\n\n2α ·\n\n−\n\n(cid:33)\n\n12α<1\n\n+\n\nc1c5 (vx\n\nT )α\n\nηx\n\n+ c1c4ηx\n\n(cid:32)\n\n(vx\n\nT )α (vx T )\n\n1−α 2\n\nα−1 2\n\n(1 + log vx\n\n(vx\n\nT ) (vx\n\n0 )2α−β−1\n\nT −\n\nlog vx 0 )\n\n12α−β≥1 +\n\n·\n\nT )1−α+β\n\n2α + β ·\n\n(vx 1\n\n−\n\n(cid:33)\n\n12α−β<1\n\n2∆Φ (vx\n\nT )α\n\n≤\n\nηx\n\n(cid:32)\n\n+ 2κl\n\n2e(1−α)(1−log vx\n\n0 )/2 (vx T ) 0 )2α−1\n\nα) (vx\n\n1+α 2\n\n12α≥1 +\n\n·\n\n(vx 1\n\nT )1−α\n\n2α ·\n\ne(1\n\n−\n\n−\n\n(cid:33)\n\n12α<1\n\n+\n\nc1c5 (vx\n\nT )α\n\nηx\n\n(cid:32)\n\n+ c1c4ηx\n\n2e(1−α)(1−log vx α) (vx\n\ne(1\n\n0 )/2 (vx T ) 0 )2α−β−1\n\n1+α 2\n\n−\n\n12α−β≥1 +\n\n·\n\nT )1−α+β\n\n2α + β ·\n\n(vx 1\n\n−\n\n(cid:33)\n\n12α−β<1\n\n,\n\n(9)\n\nwhere we used x−m(c + log x) 0 < αi < 1 and bi are positive constants, and x Now consider vx vx T −\n\nR in the last inequality. Also, if n (cid:80)n i=1 b1/(1−αi) .\nT as the x in the previous statement, and note that the LHS of Equation (9) equals to\n\n∈ i=1 bixαi, then we get x\n\necm em for x > 0, m > 0 and c\n\n0 . Then we can get\n\n(cid:80)n\n\nvx\n\n≤\n\n≤\n\n≤\n\ni\n\nvx T ≤\n\n5vx\n\n0 + 5\n\n(cid:19) 1\n\n1−α\n\n(cid:18) 2∆Φ ηx\n\n+ 5\n\n+ 5\n\n(cid:18) c1c5 ηx\n\n(cid:19) 1\n\n1−α\n\n(cid:32)\n\n+ 5\n\n(cid:32)\n\n4κle(1−α)(1−log vx e(1\n\nα) (vx\n\n0 )2α−1\n\n0 )/2\n\n− 2c1c4ηxe(1−α)(1−log vx 0 )2α−β−1 α) (vx\n\ne(1\n\n0 )/2\n\n(cid:33) 2\n\n1−α\n\n(cid:33) 2\n\n1−α\n\n·\n\n·\n\n−\n\n12α≥1 + 5\n\n12α−β≥1 + 5\n\n(cid:19) 1\n\nα\n\n(cid:18) 2κl 1\n\n2α\n\n− (cid:18) c1c4ηx\n\n12α<1\n\n·\n\n2α + β\n\n1\n\n−\n\n(cid:19) 1\n\nα−β\n\n·\n\n(10)\n\n12α−β<1.\n\nNote that the RHS is a constant and also an upper bound for (cid:80)T −1\n\nt=0 ∥∇\n\nxf (xt, yt)\n\n2. ∥\n\n(2) cases:\n\nIf vy\n\nT ≤\n\nT , then we can use the upper bound for vy vx\n\nT from Equation (8). We now discuss two\n\n1. 2α < 1 + β. Then we have\n\nT −1 (cid:88) t=0 ∥∇ (cid:32)\n\nxf (xt, yt)\n\n2\n\n∥\n\n2∆Φ + c1c5 ηx\n\n≤\n\n(cid:32)\n\n+ 2κl\n\n(cid:32)\n\nc5 +\n\nc4 (ηx)2 (vx 1\n\n2α + β\n\nT )1−2α+β\n\n(cid:32)\n\n≤\n\n− 2∆Φ + c1c5\n\nηx (vx\n\n0 )1−2α+β + 2κl\n\n(cid:32)\n\n(cid:32)\n\n(cid:32)\n\nc5\n\n0 )1−2α+β +\n\n(vx\n\nc4 (ηx)2\n\n1\n\n−\n\n2α + β (cid:32)\n\n1 + log vx\n\nlog vx\n\n0\n\nT − 0 )2α−1\n\n(vx (cid:33) α\n\n1−β\n\n12α≥1 +\n\n·\n\nT )1−2α (vx 1\n\n2α ·\n\n−\n\n(cid:33)\n\n12α<1\n\n+\n\n(cid:33)\n\nc1c4ηx (vx 1\n\nT )1−2α+β 2α + β\n\n−\n\n1 + log vx T − 0 )2α−1 (vx (cid:33) α\n\n1−β\n\n(vx\n\nlog vx T )1−2α+β ·\n\n0\n\n12α≥1 +\n\n1\n\n2α) (vx\n\n0 )β ·\n\n(1\n\n−\n\n(cid:33)\n\n12α<1\n\n+\n\n(cid:33)\n\nc1c4ηx\n\n2α + β\n\n1\n\n−\n\n(vx\n\nT )1−2α+β+ (1−2α+β)α\n\n1−β\n\n·\n\n2∆Φ + c1c5\n\nηx (vx\n\n0 )1−2α+β + 2κl\n\n≤\n\ne(1−2α+β)(1−log vx 0 ) 2α + β) (vx\n\n0 )2α−1 ·\n\ne(1\n\n12α≥1 +\n\n1\n\n2α) (vx\n\n0 )β ·\n\n(1\n\n−\n\n(cid:33)\n\n12α<1\n\n+\n\n(cid:33)\n\nc1c4ηx\n\n2α + β\n\n1\n\n−\n\n(cid:32)\n\nc5\n\n0 )1−2α+β +\n\n(vx\n\nc4 (ηx)2\n\n2α + β\n\n1\n\n−\n\n− (cid:33) α\n\n1−β\n\n(vx\n\nT )1−2α+β+ (1−2α+β)α\n\n1−β\n\n,\n\n·\n\nNote that since α > β, we have\n\n2α + β +\n\n1\n\n−\n\n(1\n\n−\n\n2α + β)α β\n1\n\n−\n\nα)α β\n\n(1 −\n1 −\n\n≤\n\n+ 1\n\n−\n\nα = 1 +\n\n21\n\nα(β 1\n\nα) β\n\n− −\n\n< 1.\n\n2κle(1−2α+β)(1−log vx 0 ) 2α + β) (vx e(1\n\n0 )2α−1 ·\n\n−\n\n1\n\n12α≥1 +\n\n2κl 2α) (vx\n\n0 )β ·\n\n(1\n\n−\n\n(cid:33)\n\n12α<1\n\n(cid:33) α\n\n1−β (cid:35)\n\n1−(1−2α+β)(1+ α\n\n1−β )\n\n+ 2vx 0 ,\n\nPublished as a conference paper at ICLR 2023\n\nTherefore, with the same reasoning as Equation (10),\n\nT −1 (cid:88) t=0 ∥∇ (cid:34) (cid:32)\n\n2\n\n≤\n\n(cid:32)\n\nxf (xt, yt)\n\n2\n\nvx\n\nT\n\n≤\n\n∥\n\n2∆Φ + c1c5\n\nηx (vx\n\n0 )1−2α+β +\n\n1\n\nc1c4ηx\n\n2α + β\n\n+\n\nc5\n\n0 )1−2α+β +\n\n(vx\n\n− c4 (ηx)2\n\n2α + β\n\n1\n\n−\n\nwhich gives us constant RHS.\n\n2. 2α\n\n≥\n\n1 + β. Then we have\n\nxf (xt, yt)\n\n2\n\n∥\n\nT −1 (cid:88) t=0 ∥∇ (cid:32)\n\n(cid:32)\n\nc5 +\n\n2∆Φ + c1c5 ηx\n\n≤\n\n+\n\n2κl (1 + log vx\n\nT − 0 )2α−1\n\n(vx\n\nlog vx 0 )\n\n+\n\nc1c4ηx (1 + log vx (vx\n\nT − 0 )2α−β−1\n\n(cid:33)\n\nlog vx 0 )\n\n(cid:33) α\n\n1−β\n\nlog vx 0 )\n\nc4 (ηx)2 (1 + log vx 0 )2α−β−1 (vx 2κl (1 + log vx\n\nT −\n\nlog vx 0 )\n\n+\n\nc1c4ηx (1 + log vx\n\nT − 0 )2α−β−1 (vx\n\nT )1/4\n\n(vx\n\n(cid:33)\n\nlog vx 0 )\n\n(cid:32)\n\n≤\n\n2∆Φ + c1c5 ηx (vx\n\n0 )1/4 +\n\n(cid:32)\n\nc5\n\n+\n\n(1−β) 4α\n\n(vx 0 )\n\n(cid:32)\n\n≤\n\n2∆Φ + c1c5 ηx (vx\n\n0 )1/4 +\n\n(cid:32)\n\nc5\n\n(1−β) 4α\n\n(vx 0 )\n\n+\n\nwhich implies\n\nT )1/4\n\n(vx\n\nT − 0 )2α−1 (vx c4 (ηx)2 (1 + log vx T − 0 )2α−β−1 (vx T )\n\n(vx\n\n(1−β) 4α\n\n(cid:33) α\n\n1−β\n\nlog vx 0 )\n\n8κle(1−log vx\n\n0 )/4\n\ne (vx\n\n0 )2α−1 + 4c4α (ηx)2 e(1−β)(1−log vx 0 )2α−β−1\n\nβ) (vx\n\ne(1\n\n4c1c4ηxe(1−log vx 0 )2α−β−1 (cid:33) α\n\ne (vx\n\n1−β\n\n0 )/(4α)\n\n−\n\n(vx\n\nT )1/2\n\n·\n\n(cid:33)\n\n0 )/4\n\n(vx\n\nT )1/2 ,\n\n·\n\nxf (xt, yt)\n\n2 ∥\n\nvx\n\nT\n\n≤\n\nT −1 (cid:88) t=0 ∥∇ (cid:34) (cid:32)\n\n2\n\n≤\n\n(cid:32)\n\n2∆Φ + c1c5 ηx (vx\n\n0 )1/4 +\n\n8κle(1−log vx\n\n0 )/4\n\n0 )2α−1 +\n\ne (vx\n\nc5\n\n(1−β) 4α\n\n(vx 0 )\n\n+\n\n4c4α (ηx)2 e(1−β)(1−log vx 0 )2α−β−1\n\nβ) (vx\n\ne(1\n\n(cid:33)\n\n0 )/4\n\n4c1c4ηxe(1−log vx 0 )2α−β−1 (cid:33) α 1−β (cid:35)2\n\ne (vx\n\n0 )/(4α)\n\n− Now we also get only a constant on the RHS.\n\nSummarizing all the cases, we finish the proof.\n\nC.2\n\nINTERMEDIATE LEMMAS FOR THEOREM 3.2\n\nLemma C.1. Under the same setting as Theorem 3.2, if for t = t0 to t1\n\n(cid:13) (cid:13)yt+1\n\n−\n\ny∗\n\nt+1\n\n(cid:13) 2\n(cid:13)\n\n(1 + λt)\n\nyt+1\n\n∥\n\n−\n\n≤\n\n22\n\n− 2 + St,\n\ny∗ t ∥\n\n+ 2vx 0 .\n\n1 and any λt > 0, St,\n\nPublished as a conference paper at ICLR 2023\n\nthen we have\n\n(cid:34)t1−1 (cid:88)\n\nE\n\nt=t0\n\n(cid:35)\n\n(f (xt, y∗ t )\n\n−\n\nf (xt, yt))\n\nE\n\n≤\n\n(cid:34) t1−1 (cid:88)\n\n(cid:18) 1\n\nt=t0+1 (cid:34)t1−1 (cid:88)\n\n+ E\n\nt=t0\n\nγt 2\n\n(cid:13) (cid:13) (cid:13)∇\n\nγtμ\n\n− 2γt\n\nyt\n\n∥\n\n−\n\ny (cid:101)f (xt, yt)\n\n2\n\ny∗ t ∥ 2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n1 2γt(1 + λt)\n\n−\n\n(cid:13) (cid:13)yt+1\n\n−\n\n2(cid:19)(cid:35) (cid:13) (cid:13)\n\ny∗\n\nt+1\n\n+ E\n\n(cid:34)t1−1 (cid:88)\n\nt=t0\n\nSt 2γt(1 + λt)\n\n(cid:35)\n\n.\n\nProof. Letting λt := μηy\n\nt+1)β , we have\n\n(cid:13) (cid:13)yt+1\n\n− (1 + λt)\n\n≤ = (1 + λt)\n\n(1 + λt)\n\n≤\n\n2(vy (cid:13) 2\n(cid:13)\n\ny∗\n\nt+1\n\nY\n\nyt+1 (cid:16)\n\n∥ (cid:13) (cid:13) (cid:13)P (cid:13) (cid:13) (cid:13)yt + γt (cid:18)\n\n= (1 + λt)\n\n(cid:18)\n\n= (1 + λt)\n\nyt\n\n∥\n\n−\n\nyt\n\n∥\n\n−\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ St\n\n(cid:17)\n\n− (cid:13) 2\n(cid:13) (cid:13)\n\n2 + St\n\ny∗ t ∥ −\nyt + γt\n\ny (cid:101)f (xt, yt)\n\n∇\n\ny (cid:101)f (xt, yt)\n\n+ St\n\ny∗\n\nt\n\n−\n\n2 + γ2\n\nt\n\ny (cid:101)f (xt, yt)\n\n∇\n\ny∗ t ∥\n\n(cid:13) (cid:13) (cid:13)∇ (cid:13) (cid:13) (cid:13)∇\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ 2γt\n\n(cid:68)\n\n∇\n\ny (cid:101)f (xt, yt), yt\n\n(cid:69)(cid:19)\n\n+ St\n\ny∗\n\nt\n\n−\n\n2 + γ2\n\nt\n\ny∗ t ∥\n\ny (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ 2γt\n\n(cid:68)\n\n∇\n\ny (cid:101)f (xt, yt), yt\n\n(cid:69)\n\ny∗\n\nt\n\n−\n\n+ γtμ\n\n∥\n\nyt\n\n2\n\ny∗ t ∥\n\n−\n\nγtμ\n\n∥\n\nyt\n\n2\n\ny∗ t ∥\n\n−\n\n−\n\n(cid:19)\n\n+ St\n\nBy multiplying\n\n1\n\nγt(1+λt) and rearranging the terms, we can get\n\n(cid:68)\n\n2\n\n∇\n\ny (cid:101)f (xt, yt), y∗\n\nt −\n\n(cid:69)\n\nyt\n\n−\n\nμ\n\nyt\n\n∥\n\n1\n\n≤\n\nγtμ\n\n−\n\nγt\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\n−\n\n1 γt(1 + λt)\n\n2\n\ny∗ t ∥\n\n− (cid:13) (cid:13)yt+1\n\ny∗\n\nt+1\n\n(cid:13) 2\n(cid:13)\n\n+ γt\n\n−\n\n(cid:13) (cid:13) (cid:13)∇\n\ny (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nSt γt(1 + λt)\n\n.\n\nBy telescoping from t = t0 to t1\n\n1, we have\n\n−\n\nt1−1 (cid:88)\n\n(cid:16)(cid:68)\n\n∇\n\nt=t0\n\ny (cid:101)f (xt, yt), y∗\n\nt −\n\n(cid:69)\n\nyt\n\nμ 2 ∥\n\nyt\n\n−\n\n−\n\n2(cid:17)\n\ny∗ t ∥\n\nt1−1 (cid:88)\n\n(cid:18) 1\n\n≤\n\nt=t0+1\n\nγtμ\n\n− 2γt\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\n−\n\n1 2γt(1 + λt)\n\n(cid:13) (cid:13)yt+1\n\n−\n\n2(cid:19)\n\n(cid:13) (cid:13)\n\n+\n\ny∗\n\nt+1\n\nt1−1 (cid:88)\n\nt=t0\n\nγt 2\n\n(cid:13) (cid:13) (cid:13)∇\n\ny (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nt1−1 (cid:88)\n\nt=t0\n\nSt 2γt(1 + λt)\n\n.\n\nNow we take the expectation and get\n\nE [LHS]\n\n≥\n\n(cid:34)t1−1 (cid:88)\n\nE\n\n(cid:104)(cid:16)(cid:68)\n\nEξy\n\nt\n\ny (cid:101)f (xt, yt), y∗\n\nt −\n\n∇\n\n(cid:69)\n\nyt\n\nμ 2 ∥\n\nyt\n\n−\n\n−\n\n2(cid:17)(cid:105)\n\ny∗ t ∥\n\n(cid:35)\n\nt=t0 (cid:34)t1−1 (cid:88)\n\n(cid:16)\n\n= E\n\nE\n\n≥\n\nt=t0 (cid:34)t1−1 (cid:88)\n\nt=t0\n\nyf (xt, yt), y∗\n\n⟨∇\n\nyt\n\n⟩ −\n\nμ 2 ∥\n\nyt\n\n−\n\n2(cid:17)\n\ny∗ t ∥\n\n(cid:35)\n\nt −\n\n(cid:35)\n\n(f (xt, y∗ t )\n\n−\n\nf (xt, yt))\n\n,\n\nwhere we used strong-concavity in the last inequality.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nLemma C.2. Under the same setting as Theorem 3.2, if vy have\n\nt+1 ≤\n\nC for t = 0, ..., t0\n\n1, then we\n\n−\n\n(cid:34)t0−1 (cid:88)\n\n(f (xt, y∗ t )\n\n(cid:35)\n\nf (xt, yt))\n\n−\n\nE\n\nE\n\n≤\n\nt=0 (cid:34)t0−1 (cid:88)\n\n(cid:18) 1\n\nt=0\n\nγtμ\n\n− 2γt\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\n−\n\n1 γt(2 + μγt) (cid:34)\n\n1 + log vx\n\n+\n\nκ2 (cid:0)μηyC β + 2C 2β(cid:1) (ηx)2 2μ (ηy)2\n\nE\n\nlog vx\n\n0\n\nt0 − 0 )2α−1\n\n(vx\n\n1α≥0.5 +\n\n·\n\n(cid:13) (cid:13)yt+1\n\ny∗\n\nt+1\n\n−\n\n2(cid:19)(cid:35) (cid:13) (cid:13)\n\n+ E\n\n(cid:34)t0−1 (cid:88)\n\ny (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\nγt 2\n\n(cid:13) (cid:13) (cid:13)∇\n\nt=0 (cid:1)1−2α\n\n2α ·\n\n(cid:0)vx t0 1\n\n−\n\n(cid:35)\n\n1α<0.5\n\n.\n\nProof. By Young’s inequality, we have\n\n(cid:13) (cid:13)yt+1\n\n−\n\ny∗\n\nt+1\n\n(cid:13) 2\n(cid:13)\n\n≤\n\n(1 + λt)\n\nyt+1\n\n∥\n\n2 +\n\ny∗ t ∥\n\n−\n\n(cid:18)\n\n1 +\n\n(cid:19)\n\n1 λt\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n.\n\nThen letting λt = μγt\n\n2 and by Lemma C.1, we have\n\n(cid:34)t0−1 (cid:88)\n\n(f (xt, y∗ t )\n\n(cid:35)\n\nf (xt, yt))\n\n−\n\nγtμ\n\n− 2γt\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\n−\n\nE\n\nE\n\n≤\n\nt=0 (cid:34)t0−1 (cid:88)\n\n(cid:18) 1\n\nt=0\n\n+ E\n\n(cid:34)t0−1 (cid:88)\n\nt=0\n\nγt 2\n\n(cid:13) (cid:13) (cid:13)∇\n\ny (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\nWe now remain to bound the last term:\n\n(cid:13) (cid:13)yt+1\n\n2(cid:19)(cid:35) (cid:13) (cid:13)\n\ny∗\n\nt+1\n\n1 γt(2 + μγt) (cid:16)\n\n\n\n− (cid:17)\n\n+ E\n\n\n\nt0−1 (cid:88)\n\n1 + 2 μγt\n\nγt(2 + μγt)\n\nt=0\n\n\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n .\n\n(cid:16)\n\n1 + 2 μγt\n\n(cid:17)\n\nγt(2 + μγt)\n\n\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n\n\n(cid:16)\n\n1 + 2 μγt\n\n(cid:17)\n\n2γt\n\n\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n\n\nμηy (cid:0)vy\n\nt+1\n\n(cid:1)β\n\n+ 2 (cid:0)vy\n\nt+1\n\n(cid:1)2β\n\nE\n\nE\n\n≤\n\n= E\n\n\n\n\n\nt0−1 (cid:88)\n\nt=0\n\n\n\n\n\nt0−1 (cid:88)\n\nt=0 (cid:34)t0−1 (cid:88)\n\nt=0\n\n(cid:35)\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n(cid:13) (cid:13)y∗\n\nt+1 − (cid:35)\n\n2μ (ηy)2 (cid:34)t0−1 (cid:88)\n\nE\n\nt=0\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n.\n\nμηyC β + 2C 2β 2μ (ηy)2\n\n≤\n\nBy Lemma B.2 we have\n\nt0−1 (cid:88)\n\nt=0\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\nκ2\n\n≤\n\n= κ2\n\nt0−1 (cid:88) t=0 ∥\n\nxt+1\n\nt0−1 (cid:88)\n\nt=0\n\nη2\n\nt\n\n(cid:13) (cid:13) (cid:13)∇\n\nxt\n\n2 ∥\n\n−\n\nx (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n= κ2 (ηx)2\n\nκ2 (ηx)2\n\n≤\n\nt0−1 (cid:88)\n\nt=0\n\nt0−1 (cid:88)\n\nt=0\n\nmax (cid:8)vx\n\n1 t+1, vy\n\nt+1\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:9)2α\n\nx (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n1\n\n(cid:0)vx\n\nt+1\n\n(cid:1)2α\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n24\n\nPublished as a conference paper at ICLR 2023\n\n(cid:32)\n\n(cid:32)\n\nκ2 (ηx)2\n\nκ2 (ηx)2\n\n≤\n\n≤\n\nvx\n\n0 (vx\n\n0 )2α +\n\nt0−1 (cid:88)\n\nt=0\n\n1\n\n(cid:0)vx\n\nt+1\n\n(cid:1)2α\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:33)\n\n(cid:13) (cid:13) (cid:13)\n\n1 + log vx\n\nlog vx\n\n0\n\nt0 − 0 )2α−1\n\n(vx\n\n1α≥0.5 +\n\n·\n\n(cid:1)1−2α\n\n(cid:33)\n\n1α<0.5\n\n2α ·\n\n(cid:0)vx t0 1\n\n−\n\nwhere we applied Lemma B.1 in the last inequality. Bringing back this result, we finish the proof.\n\nLemma C.3. Under the same setting as Theorem 3.2, if t0 is the first iteration such that vy then we have\n\nt0+1 > C,\n\n(cid:34)T −1 (cid:88)\n\nt=t0 (cid:34)T −1 (cid:88)\n\nE\n\nE\n\nt=t0 (cid:32)\n\n≤\n\n(cid:35)\n\n(f (xt, y∗ t )\n\n−\n\nf (xt, yt))\n\n(cid:18) 1\n\nγtμ\n\n− 2γt\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥ (cid:33)\n\n1 γt(2 + μγt)\n\n−\n\n(cid:13) (cid:13)yt+1\n\n−\n\n2(cid:19)(cid:35) (cid:13) (cid:13)\n\ny∗\n\nt+1\n\n+ E\n\n(cid:34)T −1 (cid:88)\n\nt=t0\n\nγt 2\n\n(cid:13) (cid:13) (cid:13)∇\n\ny (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n+\n\nκ2 +\n\n(cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy\n\n+\n\n2κ2 (ηx)2 μ (ηy)2 C 2α−2β\n\nE\n\n(cid:34)T −1 (cid:88)\n\n∥∇\n\nt=t0\n\n(ηx)2 α)ηy (vy\n\n0 )α−β (cid:32) (cid:35)\n\n2(1\n\n−\n\nxf (xt, yt)\n\n2 ∥\n\n+\n\nE\n\n(cid:104) (vx\n\nT )1−α(cid:105)\n\n(cid:33)\n\n1 μ\n\n+\n\nηy 0 )β (vy\n\n4κηxG2 ηy (vy\n\n0 )α E\n\n(cid:104)\n\n(vy\n\nT )β(cid:105)\n\n.\n\nProof. By the Lipschitzness of y∗( ·\n2 +\n\n(cid:13) (cid:13)yt+1\n\nyt+1\n\n(cid:13) 2\n(cid:13)\n\ny∗\n\n=\n\nt+1\n\n−\n\n) as in Lemma B.2, we have\n\n∥\n\n−\n\nyt+1\n\n≤ ∥\n\nyt+1\n\n≤ ∥\n\n−\n\n−\n\ny∗ t ∥ y∗ t ∥ y∗ t ∥\n\n(cid:13) (cid:13)y∗\n\ny∗\n\nt+1\n\n(cid:13) 2\n(cid:13)\n\n2 + κ2η2\n\nx (cid:101)f (xt, yt)\n\nt , y∗ y∗\n\n− + 2(cid:10)yt+1\n\n(cid:11)\n\ny∗\n\nt+1\n\nt − t , y∗ y∗\n\n−\n\nt −\n\n(cid:11)\n\ny∗\n\nt+1\n\n2 + κ2η2\n\nx (cid:101)f (xt, yt)\n\n2 (yt+1\n\n+ 2(cid:10)yt+1 (cid:13) 2\n(cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13)\n\n− (cid:124)\n\nt\n\nt − (cid:13) (cid:13) (cid:13)∇ (cid:13) (cid:13) (cid:13)∇\n\nt\n\n⊺ y∗ t )\n\n−\n\ny∗(xt) (xt+1 ∇\n(cid:123)(cid:122) (C)\n\nxt) (cid:125)\n\n−\n\n+ 2 (yt+1\n\n(cid:124)\n\n−\n\n⊺ (cid:0)y∗\n\ny∗ t )\n\nt −\n\ny∗ t+1 + (cid:123)(cid:122) (D)\n\n∇\n\ny∗(xt) (xt+1\n\n.\n\nxt)(cid:1) (cid:125)\n\n−\n\nFor Term (C), by the Cauchy-Schwarz and Lipschitzness of y∗( ·\n\n),\n\n2 (yt+1\n\n−\n\n−\n\n= 2ηt (yt+1\n\n−\n\n⊺ y∗ t ) ⊺\ny∗ t )\n\n∇ y∗(xt)\n\n∇\n\n∇\n\ny∗(xt) (xt+1\n\nxt)\n\n−\n\nxf (xt, yt) + 2ηt (yt+1\n\n2ηt\n\nyt+1\n\n∥\n\n2\n\nyt+1\n\n∥\n\n−\n\ny∗ t ∥∥∇ κηt\n\n− y∗ t ∥\n\n∥∇\n\ny∗(xt)\n\n∥∥∇\n\nxf (xt, yt)\n\n∥\n\n+ 2ηt (yt+1\n\nxf (xt, yt)\n\n+ 2ηt (yt+1\n\n∥\n\nλt\n\nyt+1\n\n∥\n\n2 +\n\ny∗ t ∥\n\n−\n\nκ2η2\n\nt λt ∥∇\n\nxf (xt, yt)\n\n⊺\n\ny∗ t )\n\n−\n\n(cid:16)\n\ny∗(xt)\n\n∇ ⊺\ny∗ t )\n\n∇ y∗(xt)\n\n− y∗(xt)\n\n∇ (cid:16)\n\n⊺ y∗ t )\n\nx (cid:101)f (xt, yt) (cid:16)\n\nx (cid:101)f (xt, yt)\n\n∇\n\n(cid:17)\n\nxf (xt, yt)\n\n− ∇\n\n(cid:17)\n\n− ∇\n\nxf (xt, yt) (cid:17)\n\nx (cid:101)f (xt, yt)\n\n− ∇\n\n∇\n\nxf (xt, yt)\n\n⊺\n\ny∗ t )\n\n−\n\n∇\n\ny∗(xt)\n\n(cid:16)\n\n∇\n\nx (cid:101)f (xt, yt)\n\n− ∇\n\n(cid:17)\n\nxf (xt, yt)\n\n,\n\n∇\n\n− 2 + 2ηt (yt+1 ∥\n\n≤\n\n≤\n\n≤\n\nwhere we used Young’s inequality in the last step and λt > 0 will be determined later.\n\nFor Term (D), according to Cauchy-Schwarz and the smoothness of y∗(\n\n) as shown in Lemma B.3, ·\n\n⊺ (cid:0)y∗ (cid:13) (cid:13)y∗\n\nt −\n\ny∗ t+1 +\n\ny∗ t+1 +\n\ny∗(xt) (xt+1\n\n∇ y∗(xt) (xt+1\n\nxt)(cid:1) −\nxt)(cid:13) (cid:13)\n\n∇\n\n−\n\n2 (yt+1\n\n2\n\n∥\n\n2\n\n∥\n\n≤\n\n≤\n\nyt+1\n\nyt+1\n\n−\n\n−\n\n−\n\ny∗ t ) y∗ t ∥ y∗ t ∥ ·\n\nt − (cid:98)L 2 ∥\n\nxt+1\n\nxt\n\n2 ∥\n\n−\n\n25\n\nPublished as a conference paper at ICLR 2023\n\n(cid:13) (cid:13) (cid:13)∇ G\n\n·\n\ny∗ t ∥ y∗ t ∥\n\n−\n\n−\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nx (cid:101)f (xt, yt) (cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n(cid:13) (cid:13) (cid:13)\n\n= (cid:98)Lη2\n\nyt+1\n\nt ∥\n\n≤\n\nyt+1\n\n(cid:98)Lη2\n\nt ∥ τ (cid:98)LG2η2 2\n\nt\n\n(cid:13) (cid:13) (cid:13)∇ where in the last step we used Young’s inequality and τ > 0.\n\ny∗ t ∥\n\n(cid:98)Lη2 t\n2τ\n\n2 +\n\nyt+1\n\n−\n\n≤\n\n∥\n\nx (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n,\n\nTherefore, in total, we have\n\n(cid:13) (cid:13)yt+1\n\n−\n\ny∗\n\nt+1\n\n(cid:13) 2\n(cid:13)\n\n≤\n\n(cid:32)\n\n1 + λt +\n\n(cid:33)\n\nτ (cid:98)LG2η2 2\n\nt\n\nyt+1\n\n∥\n\ny∗ t ∥\n\n−\n\n2 +\n\n(cid:32)\n\nκ2 +\n\n(cid:33)\n\n(cid:98)L 2τ\n\nη2\n\nt\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nκ2η2\n\nt λt ∥∇\n\nxf (xt, yt)\n\n2 + 2ηt (yt+1 ∥\n\n⊺ y∗ t )\n\n−\n\n∇\n\ny∗(xt)\n\n(cid:16)\n\n∇\n\nx (cid:101)f (xt, yt)\n\n− ∇\n\nxf (xt, yt)\n\n(cid:17)\n\n.\n\nNote that we can upper bound ηt by\n\nηt =\n\nηx t+1, vy\n\nt+1\n\nmax (cid:8)vx\n\nηx\n\n(cid:9)α ≤\n\n(cid:0)vy\n\nt+1\n\n(cid:1)α ≤\n\nηx 0 )α , (vy\n\nand\n\nηt\n\nηx\n\n≤\n\n(cid:0)vy\n\nt+1\n\n(cid:1)α =\n\nηx (cid:1)α−β (cid:0)vy\n\nt+1\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β ≤\n\nwhich, plugged into the previous result, implies\n\nηx 0 )α−β (cid:0)vy\n\n(vy\n\nt+1\n\n(cid:1)β ,\n\n(cid:13) (cid:13)yt+1\n\n−\n\ny∗\n\nt+1\n\n(cid:13) 2\n(cid:13)\n\n≤\n\n(cid:32)\n\n1 + λt +\n\nτ (cid:98)LG2 (ηx)2 0 )2α−β (cid:0)vy\n\nt+1\n\n2 (vy\n\n(cid:33)\n\n(cid:1)β\n\nyt+1\n\n∥\n\n2 +\n\ny∗ t ∥\n\n−\n\n(cid:32)\n\nκ2 +\n\n(cid:33)\n\n(cid:98)L 2τ\n\nη2\n\nt\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nκ2η2\n\nt λt ∥∇\n\nxf (xt, yt)\n\n2 + 2ηt (yt+1 ∥\n\n⊺ y∗ t )\n\n−\n\n∇\n\ny∗(xt)\n\n(cid:16)\n\n∇\n\nx (cid:101)f (xt, yt)\n\n− ∇\n\nxf (xt, yt)\n\n(cid:17)\n\n.\n\nNow we choose λt = μηy\n\nt+1)β and τ =\n\n4(vy\n\n0 )2α−β\n\nμηy(vy 2(cid:98)LG2(ηx)2 , and get\n\n(cid:13) (cid:13)yt+1 (cid:32)\n\n−\n\ny∗\n\nt+1\n\n(cid:13) 2\n(cid:13)\n\nμηy\n\n(cid:33)\n\n(cid:1)β\n\nt+1\n\n1 +\n\n2 (cid:0)vy 4κ2 (cid:0)vy\n\n+\n\n≤\n\nyt+1\n\n∥\n\ny∗ t ∥\n\n−\n\n2 +\n\n(cid:32)\n\nκ2 +\n\n(cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy\n\n(cid:33)\n\nη2\n\nt\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:1)β\n\nη2\n\nt\n\nxf (xt, yt)\n\n∥\n\n∥∇\n\n2 + 2ηt (yt+1\n\n⊺ y∗ t )\n\n−\n\n∇\n\ny∗(xt)\n\n(cid:16)\n\n∇\n\nx (cid:101)f (xt, yt)\n\n− ∇\n\n(cid:17)\n\nxf (xt, yt)\n\n.\n\nt+1 μηy\n\nThen Lemma C.1 gives us\n\n(cid:34)T −1 (cid:88)\n\nt=t0 (cid:34)T −1 (cid:88)\n\nE\n\nE\n\nt=t0\n\n≤\n\n(cid:35)\n\n(f (xt, y∗ t )\n\n−\n\nf (xt, yt))\n\n(cid:18) 1\n\nγtμ\n\n− 2γt\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\n−\n\n1 γt(2 + μγt)\n\n(cid:13) (cid:13)yt+1\n\n−\n\n2(cid:19)(cid:35) (cid:13) (cid:13)\n\ny∗\n\nt+1\n\n+ E\n\n(cid:34)T −1 (cid:88)\n\nt=t0\n\nγt 2\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:34)T −1 (cid:88)\n\nt=t0\n\n(cid:34)T −1 (cid:88)\n\nt=t0\n\n+ E\n\n(cid:124)\n\n+ E\n\n(cid:124)\n\n1 γt(2 + μγt)\n\n(cid:32)\n\nκ2 +\n\n(cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy (cid:123)(cid:122) (E)\n\n(cid:33)\n\nη2\n\nt\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:125)\n\n(cid:1)β\n\n4κ2 (cid:0)vy γt(2 + μγt)μηy ∥∇\n\nη2\n\nt+1\n\nt\n\n(cid:123)(cid:122) (F)\n\nxf (xt, yt)\n\n2 ∥\n\n(cid:35)\n\n(cid:125)\n\n26\n\ny (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\nPublished as a conference paper at ICLR 2023\n\n(cid:34)T −1 (cid:88)\n\nt=t0\n\n+ E\n\n(cid:124)\n\n2ηt γt(2 + μγt)\n\n(yt+1\n\n−\n\n⊺ y∗ t )\n\ny∗(xt)\n\n∇\n\n(cid:16)\n\n∇\n\nx (cid:101)f (xt, yt)\n\n− ∇\n\n(cid:17)\n\nxf (xt, yt)\n\n(cid:123)(cid:122) (G)\n\n(cid:35)\n\n(cid:125)\n\nNow we proceed to bound each term.\n\nTerm (E)\n\nTerm (E)\n\n≤\n\n=\n\n≤\n\n≤\n\n≤\n\n(cid:32)\n\nκ2 +\n\n(cid:32)\n\nκ2 +\n\n(cid:32)\n\nκ2 +\n\n(cid:32)\n\nκ2 +\n\n(cid:32)\n\nκ2 +\n\n(cid:32)\n\nκ2 +\n\n(cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy (cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy (cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy (cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy (cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy (cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy\n\n(cid:33)\n\n(cid:33)\n\n(cid:33)\n\n(cid:33)\n\nE\n\nE\n\nE\n\nE\n\n(cid:34)T −1 (cid:88)\n\nt=t0 (cid:34)T −1 (cid:88)\n\nt=t0 (cid:34)T −1 (cid:88)\n\nt=t0 (cid:34)T −1 (cid:88)\n\nt=t0\n\n(cid:33)\n\n(cid:34)\n\nE\n\n(cid:33)\n\n2(1\n\n≤\n\n− where we used Lemma B.1 in the last step.\n\nη2 t\n2γt\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n(ηx)2 (cid:0)vy\n\nt+1\n\n(cid:1)β\n\n2ηy max (cid:8)vx\n\nt+1, vy (ηx)2 (cid:0)vy (cid:1)β (cid:0)vy\n\nt+1\n\nt+1\n\nt+1\n\n(ηx)2 0 )α−β (cid:0)vx (cid:32)\n\nt+1\n\n2ηy (vy\n\n(ηx)2\n\n2ηy (cid:0)vy\n\nt+1\n\n(cid:1)α−β (cid:0)vx\n\nt+1\n\nx (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:9)2α\n\n(cid:1)β\n\nx (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:1)α\n\nx (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:1)α\n\n2ηy (vy\n\n0 )α−β\n\nvx 0\n0 )α + (vx\n\nT −1 (cid:88)\n\nt=0\n\n1 (cid:0)vx\n\nt+1\n\n(cid:1)α\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:33)(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n(ηx)2 α)ηy (vy\n\n0 )α−β\n\nE\n\n(cid:104) (vx\n\nT )1−α(cid:105)\n\n,\n\nTerm (F)\n\nTerm (F)\n\n≤\n\n=\n\n≤\n\n≤\n\n≤\n\n(cid:34)T −1 (cid:88)\n\nE\n\nt=t0\n\n2κ2 (cid:0)vy\n\n(cid:1)β\n\nη2\n\nt\n\nt+1 γtμηy\n\nxf (xt, yt)\n\n∥∇\n\n(cid:35)\n\n2\n\n∥\n\n(cid:1)2β\n\nt+1\n\nt+1, vy\n\nt+1\n\n(cid:35)\n\n(cid:9)2α ∥∇\n\nxf (xt, yt)\n\n2 ∥\n\nxf (xt, yt)\n\n(cid:1)2α ∥∇\n\n(cid:35)\n\n2\n\n∥\n\n(cid:0)vy max (cid:8)vx (cid:0)vy (cid:1)2β (cid:0)vy\n\nt+1\n\nt+1\n\n(cid:34)T −1 (cid:88)\n\nt=t0 (cid:34)T −1 (cid:88)\n\nt=t0\n\n(cid:34)\n\n2κ2 (ηx)2 μ (ηy)2\n\nE\n\n2κ2 (ηx)2 μ (ηy)2\n\nE\n\n2κ2 (ηx)2 μ (ηy)2\n\nE\n\n1 (cid:1)2α−2β\n\nT −1 (cid:88)\n\nt=t0\n\n∥∇\n\n(cid:0)vy\n\nt0+1\n\nxf (xt, yt)\n\n(cid:35)\n\n2\n\n∥\n\n2κ2 (ηx)2 μ (ηy)2 C 2α−2β\n\nE\n\n(cid:34)T −1 (cid:88)\n\n∥∇\n\nt=t0\n\nxf (xt, yt)\n\n(cid:35)\n\n2 ∥\n\nTerm (G) For simplicity, denote mt := Since y∗(\n\n) is κ-Lipschitz as in Lemma B.2, ·\n\nmt |\n\n|\n\n2\n\nγt(2+μγt) (yt+1\n\n⊺ y∗(xt) y∗ t ) can be upper bounded as\n\n∇\n\n−\n\nx (cid:101)f (xt, yt)\n\n− ∇\n\n(cid:17)\n\nxf (xt, yt)\n\n(cid:16)\n\n∇\n\n(cid:17)\n\nmt |\n\n| ≤\n\n≤\n\n1 γt ∥ κ\nγt ∥\n\nyt+1\n\nyt+1\n\ny∗ t ∥∥∇ (cid:16)(cid:13) y∗ (cid:13) (cid:13)∇ t ∥\n\n−\n\n−\n\ny∗(xt) ∥\n\n(cid:16)(cid:13) (cid:13) (cid:13)∇ (cid:13) (cid:13) (cid:13) +\n\nx (cid:101)f (xt, yt)\n\nx (cid:101)f (xt, yt)\n\n(cid:13) (cid:13) (cid:13) +\n\nxf (xt, yt)\n\n∥\n\n∥∇\n\n(cid:17)\n\n∥\n\nxf (xt, yt)\n\n∥∇\n\n27\n\nPublished as a conference paper at ICLR 2023\n\n(cid:17)\n\ny (cid:101)f (xt, yt)\n\nyt + γt\n\n∇\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:16)(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n(cid:13) (cid:13) (cid:13) +\n\n∥∇\n\nxf (xt, yt)\n\ny∗\n\nt\n\n(cid:13) (cid:13) (cid:13)\n\n−\n\nx (cid:101)f (xt, yt)\n\nxf (xt, yt)\n\n(cid:13) (cid:13) (cid:13) +\n\n(cid:17)\n\n∥\n\ny (cid:101)f (xt, yt)\n\nx (cid:101)f (xt, yt)\n\nxf (xt, yt)\n\n∥∇ (cid:13) (cid:13) (cid:13) +\n\n∥∇\n\n(cid:17)\n\n∥\n\n(cid:17)\n\n∥\n\ny∗\n\nt\n\n− (cid:16)(cid:13) (cid:13) (cid:13)∇ (cid:13) (cid:13) (cid:13)\n\n(cid:17) (cid:16)(cid:13) (cid:13) (cid:13)∇ (cid:13) (cid:13) (cid:13)\n\ny (cid:101)f (xt, yt)\n\n(cid:19) (cid:16)(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n(cid:13) (cid:13) (cid:13) +\n\n∥∇\n\nxf (xt, yt)\n\n(cid:17)\n\n∥\n\nκ γt κ\nγt κ\nγt κ\nγt\n\nY\n\n(cid:16)\n\n(cid:13) (cid:13) (cid:13)P (cid:13) (cid:13) (cid:13)yt + γt (cid:16)\n\n∥ (cid:18) 1\n\nμ ∥∇ (cid:32)\n\n≤\n\n≤\n\n≤\n\n≤\n\n≤\n\ny (cid:101)f (xt, yt)\n\n∇\n\nyt\n\ny∗ t ∥\n\n−\n\n(cid:13) (cid:13) (cid:13)γt\n\n+\n\n∇\n\nyf (xt, yt)\n\n(cid:13) (cid:13) (cid:13)γt\n\n+\n\n∇\n\n2Gκ γT −1 (cid:124)\n\nηyG 0 )β (vy\n\nG μ\n\n+\n\n(cid:123)(cid:122) M\n\n∥ (cid:33)\n\n.\n\n(cid:125)\n\nAlso note that γt and yt+1 does not depend on ξx\n\nt , so Eξx\n\nt [mt] = 0. Next, we look at Term (G).\n\nTerm (G) = E\n\n(cid:35)\n\nηtmt\n\n(cid:34)T −1 (cid:88)\n\nt=t0\n\n(cid:34)\n\n= E\n\nηt0mt0 +\n\nT −1 (cid:88)\n\nηt−1mt +\n\nT −1 (cid:88)\n\nt=t0+1\n\nt=t0+1\n\n(cid:35)\n\n(ηt\n\n−\n\nηt−1) mt\n\n(cid:35)\n\n(ηt−1\n\nηt) (\n\n−\n\n−\n\nmt)\n\nT −1 (cid:88)\n\nt=t0+1\n\nT −1 (cid:88)\n\nt=t0+1\n\nηt−1Eξx\n\nt [mt] +\n\nT −1 (cid:88)\n\nt=t0+1\n\n(cid:35)\n\n(ηt−1\n\n−\n\nηt) M\n\nE\n\nE\n\nE\n\n(cid:32)\n\n≤\n\n≤\n\n≤\n\n=\n\n(cid:34)\n\nηx 0 )α M + (vy\n\n(cid:34)\n\nηx 0 )α M + (vy (cid:20) 2ηx (vy\n\n(cid:21)\n\n0 )α M ηy 0 )β (vy\n\n+\n\n1 μ\n\n(cid:33)\n\n4κηxG2 ηy (vy\n\n0 )α E\n\n(cid:104)\n\n(vy\n\nT )β(cid:105)\n\n.\n\nSummarizing all the results, we finish the proof.\n\nLemma C.4. Under the same setting as Theorem 3.2, we have\n\n(cid:34)T −1 (cid:88)\n\n(cid:18) 1\n\nE\n\nt=0\n\nγtμ\n\n− 2γt\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\n−\n\n1 γt(2 + μγt)\n\n(cid:13) (cid:13)yt+1\n\n−\n\n2(cid:19)(cid:35) (cid:13) (cid:13)\n\ny∗\n\nt+1\n\n0 )β G2 (vy 2μ2ηy +\n\n≤\n\n4μ\n\nProof.\n\n(2βG) 1−β +3 (ηy)\n\n1\n\n1\n\n1−β +2 G2 1−β +2 (vy\n\n1\n\n0 )2−2β .\n\n(cid:34)T −1 (cid:88)\n\n(cid:18) 1\n\nt=0\n\nγtμ\n\n− 2γt\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\n−\n\n1 γt(2 + μγt)\n\n(cid:13) (cid:13)yt+1\n\n−\n\n2(cid:19)(cid:35) (cid:13) (cid:13)\n\ny∗\n\nt+1\n\nE\n\n(cid:32)\n\n≤\n\n≤\n\n(cid:33)\n\n0 )β (vy 2ηy −\n\nμ 2\n\ny0\n\n∥\n\n−\n\ny∗ 0∥\n\n2 +\n\n1 2ηy\n\n0 )β G2 (vy 2μ2ηy +\n\n1 2ηy\n\nT −1 (cid:88)\n\n(cid:18)\n\nt=1 (cid:124)\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β\n\n−\n\n(cid:32)\n\nT −1 (cid:88)\n\nt=1\n\nμηy\n\n2 − (cid:123)(cid:122) (H)\n\n(cid:19)\n\n(vy\n\nt )β\n\nyt\n\n∥\n\n−\n\n.\n\n2\n\ny∗ t ∥ (cid:125)\n\n28\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β\n\nμηy\n\n−\n\n2 −\n\n(vy\n\nt )β\n\n(cid:33)\n\nμ2 (ηy)2 t )β + 2μηy\n\n4 (vy\n\n−\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\nPublished as a conference paper at ICLR 2023\n\nFor Term (H),we will bound it using the same strategy as in (Yang et al., 2022a). The general idea is to show that (cid:0)vy t )β is positive for only a constant number of times. If the term (vy t+1 is positive at iteration t, then we have\n\n2 −\n\nμηy\n\n(cid:1)β\n\n−\n\n0 < (cid:0)vy\n\nt+1\n\n(cid:1)β\n\n(vy\n\nt )β\n\n−\n\n−\n\nμηy 2\n2(cid:19)β (cid:13) (cid:13) (cid:13)\n\n(cid:18)\n\n=\n\nvy t +\n\n(cid:13) (cid:13) (cid:13)∇ \n\n= (vy\n\nt )β\n\n 1 +\n\ny (cid:101)f (xt, yt)\n\n(vy\n\nt )β\n\n−\n\nμηy 2\n\n−\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:13) 2\n(cid:13) (cid:13)\n\ny (cid:101)f (xt, yt) vy\n\nt\n\nβ\n\n\n\n \n\n(vy\n\nt )β\n\n−\n\nμηy 2\n\n−\n\n\n\n(vy\n\nt )β\n\n 1 +\n\n≤\n\nβ\n\n(cid:13) (cid:13) (cid:13)∇\n\ny (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nvy\n\nt\n\n\n\n(vy\n\nt )β\n\n  −\n\n−\n\nμηy 2\n\n(cid:13) (cid:13) (cid:13)∇\n\nβ\n\n=\n\n(cid:13) 2\n(cid:13) (cid:13)\n\ny (cid:101)f (xt, yt) t )1−β\n\n(vy\n\nμηy 2\n\n,\n\n−\n\n(11)\n\nwhere in the last inequality we used Bernoulli’s inequality. By rearranging the terms, we have the two following conditions\n\n \n\n\n\n(cid:13) (cid:13) (cid:13)∇ (vy\n\n(cid:13) 2\n(cid:13) y (cid:101)f (xt, yt) (cid:13) t )1−β < 2β\n\nμηy\n\n2β (vy\n\n> μηy (cid:13) (cid:13) (cid:13)∇\n\nt )1−β (cid:13) 2\n(cid:13) (cid:13)\n\nμηy\n\n2β (vy\n\n0 )1−β\n\n2βG\n\nμηy ,\n\n≥\n\n≤\n\ny (cid:101)f (xt, yt)\n\nThis indicates that at each time the term is positive, the gradient norm must be large enough and the accumulated gradient norm, i.e., vy\n\nt+1, must be small enough. Therefore, we can have at most\n\n(cid:17) 1\n\n1−β\n\n(cid:16) 2βG μηy\n\nμηy\n\n2β (vy\n\n0 )1−β\n\nconstant number of iterations when the term is positive. When the term is positive, it is also upper bounded by using the result from Equation (11):\n\n(cid:18)\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β\n\nμηy\n\n−\n\n2 −\n\n(cid:19)\n\n(vy\n\nt )β\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\n≤\n\nβ\n\n(cid:13) (cid:13) (cid:13)∇\n\ny (cid:101)f (xt, yt) t )1−β\n\n(vy\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\n≤\n\n(vy\n\nβG2 0 )1−β ∥ βG2\n\nyt\n\n2\n\ny∗ t ∥\n\n−\n\nyf (xt, yt)\n\n2 ∥\n\n≤\n\nμ2 (vy\n\n0 )1−β ∥∇\n\nβG4\n\n≤\n\nμ2 (vy\n\n0 )1−β\n\nwhich is a constant. In total, Term (H) is bounded by\n\n(2βG) 1−β +3 (ηy)\n\n1\n\n1\n\n1−β +2 G2 1−β +1 (vy\n\n1\n\n0 )2−2β .\n\n2μ\n\nBringing it back, we get the desired result.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nLemma C.5. Under the same setting as Theorem 3.2, for any constant C, we have\n\n(cid:34)T −1 (cid:88)\n\nE\n\nt=0\n\n(cid:35)\n\n(f (xt, y∗ t )\n\n−\n\nf (xt, yt))\n\n2κ2 (ηx)2 μ (ηy)2 C 2α−2β\n\n≤\n\nE\n\n(cid:33)\n\n(cid:34)T −1 (cid:88) t=0 ∥∇ 4κηxG2 ηy (vy\n\n(cid:32)\n\n1 μ\n\nηy 0 )β (vy\n\n+\n\n0 )α E κ2 (cid:0)μηyC β + 2C 2β(cid:1) (ηx)2 2μ (ηy)2 (cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy\n\nκ2 +\n\n(cid:33)\n\n(cid:32)\n\n+\n\n+\n\n+\n\n+\n\n(cid:35)\n\n2\n\n+\n\nηy\n\n2(1\n\nβ)\n\n−\n\nE\n\n(cid:104)\n\n(vy\n\nT )1−β(cid:105)\n\n∥\n\nxf (xt, yt)\n\n(cid:104)\n\n(vy\n\nT )β(cid:105)\n\n(cid:34)\n\nE\n\n1 + log vx\n\nlog vx\n\n0\n\nT − 0 )2α−1\n\n(vx\n\n1α≥0.5 +\n\n·\n\nT )1−2α (vx 1\n\n2α ·\n\n−\n\n(ηx)2 α)ηy (vy\n\n0 )α−β\n\nE\n\n(cid:104) (vx\n\nT )1−α(cid:105)\n\n2(1\n\n−\n\n0 )β G2 (vy 2μ2ηy +\n\n4μ\n\n(2βG) 1−β +3 (ηy)\n\n1\n\n1\n\n1−β +2 G2 1−β +2 (vy\n\n1\n\n0 )2−2β .\n\nProof. By Lemma C.2 and Lemma C.3, we have for any constant C,\n\n(cid:34)T −1 (cid:88)\n\n(f (xt, y∗ t )\n\n(cid:35)\n\nf (xt, yt))\n\n−\n\nE\n\nE\n\n≤\n\nt=0 (cid:34)T −1 (cid:88)\n\n(cid:18) 1\n\nt=0\n\n− 2(cid:35)\n\n+ E\n\n(cid:34)T −1 (cid:88)\n\nt=0\n\n(cid:13) (cid:13) (cid:13)\n\nγt 2\n\ny (cid:101)f (xt, yt)\n\n(cid:13) (cid:13) (cid:13)∇ κ2 (cid:0)μηyC β + 2C 2β(cid:1) (ηx)2 2μ (ηy)2 (cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy (cid:33)\n\nκ2 +\n\n(cid:33)\n\n(cid:32)\n\n(cid:32)\n\n2(1\n\n1 μ\n\n+\n\nηy 0 )β (vy\n\n4κηxG2 ηy (vy\n\n0 )α E\n\n+\n\n+\n\n+\n\nγtμ\n\n− 2γt\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\n1 γt(2 + μγt)\n\n(cid:13) (cid:13)yt+1\n\n−\n\n2(cid:19)(cid:35) (cid:13) (cid:13)\n\ny∗\n\nt+1\n\n+\n\n(cid:34)\n\nE\n\n2κ2 (ηx)2 μ (ηy)2 C 2α−2β\n\nE\n\n(cid:34)T −1 (cid:88) t=0 ∥∇\n\n(cid:35)\n\nxf (xt, yt)\n\n2 ∥\n\n1 + log vx\n\nlog vx\n\n0\n\nT − 0 )2α−1\n\n(vx\n\n1α≥0.5 +\n\n·\n\nT )1−2α (vx 1\n\n2α ·\n\n−\n\nE\n\n(cid:104) (vx\n\nT )1−α(cid:105)\n\n(ηx)2 α)ηy (vy T )β(cid:105)\n\n.\n\n− (vy\n\n(cid:104)\n\n0 )α−β\n\n(cid:35)\n\n1α<0.5\n\n(cid:35)\n\n1α<0.5\n\nThe first term can be bounded by Lemma C.4. For the second term, we have\n\n(cid:34)T −1 (cid:88)\n\nE\n\nt=0\n\nγt 2\n\n(cid:13) (cid:13) (cid:13)∇\n\ny (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:34)T −1 (cid:88)\n\n= E\n\n(cid:13) (cid:13) (cid:13)\n\nηy 2 (cid:0)vy\n\nt+1\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:1)β\n\ny (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\nηy 2\n\n≤\n\nt=0 (cid:34)\n\nE\n\nηy\n\n≤\n\n2(1\n\n−\n\nβ)\n\n0\n\nvy 0 )β + (vy (vy\n\nE\n\n(cid:104)\n\nT −1 (cid:88)\n\nt=0\n\n(cid:0)vy T )1−β(cid:105)\n\n,\n\n1\n\nt+1\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:1)β\n\ny (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\nwhere the last inequality follows from Lemma B.1. Then the proof is completed.\n\nC.3 PROOF OF THEOREM 3.2\n\nWe present a formal version of Theorem 3.2.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nTheorem C.2 (stochastic setting). Under Assumptions 3.1 to 3.6, Algorithm 1 with stochastic gradient oracles satisfies that for any 0 < β < α < 1, after T iterations,\n\n(cid:34)\n\nE\n\n1 T\n\nT −1 (cid:88) t=0 ∥∇\n\n(cid:32)\n\n4∆ΦG2α ηxT 1−α +\n\n≤\n\nxf (xt, yt)\n\n(cid:35)\n\n2 ∥\n(cid:32)\n\n+\n\nκ2 +\n\n4lκηx α\n1\n\n−\n\n(cid:32)\n\n+\n\n(1\n\n1 μ\n\n2lκηyG2(1−β)\n\nβ)T β +\n\nηy 0 )β (vy (cid:32) 2κ4 (cid:0)μηyC β + 2C 2β(cid:1) (ηx)2 (ηy)2\n\n−\n\n+\n\n+\n\n+\n\nand\n\n2κ2 (vy\n\n0 )β G2\n\nμηyT\n\n+\n\nlκ (2βG)\n\n1\n\n1−β +2 G2\n\n1\n\n1−β +3 (ηy)\n\nμ\n\n1\n\n1−β +2 (vy\n\n0 )2−2β T\n\n,\n\n2lκ (ηx)2 α)ηy (vy\n\n0 )α−β\n\n(cid:33)\n\nG2(1−α) T α\n\n(cid:33)\n\n(cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy (cid:33)\n\n(1\n\n− 16lκ2ηxG2(1+β) 0 )α T 1−β ηy (vy\n\n1 + log(G2T ) (vx\n\n− 0 )2α−1 T\n\nlog vx\n\n0\n\n1α≥0.5 +\n\n·\n\nG2(1−2α)\n\n2α)T 2α ·\n\n(1\n\n−\n\n(cid:33)\n\n1α<0.5\n\n(cid:34)\n\nE\n\n1 T\n\nT −1 (cid:88) t=0 ∥∇\n\nyf (xt, yt)\n\n(cid:35)\n\n2 ∥\n\n4κ3 (ηx)2 (ηy)2 C 2α−2β\n\n≤\n\n(cid:34)\n\nE\n\n1 T\n\nT −1 (cid:88) t=0 ∥∇\n\n(cid:35)\n\n+\n\nxf (xt, yt)\n\n2 ∥\n\n+\n\nκ3 (cid:0)μηyC β + 2C 2β(cid:1) (ηx)2 (ηy)2\n\n(cid:32)\n\n1 + log T G2 (vx\n\n− 0 )2α−1 T\n\n(cid:33)\n\n(cid:32)\n\n1 μ\n\n+\n\nηy 0 )β (vy\n\nlηyG2−2β (1\n\nβ)T β +\n\n− log vx\n\n0\n\n1α≥0.5 +\n\n·\n\nG2−4α\n\n2α)T 2α ·\n\n(1\n\n−\n\n8lκηxG2+2β 0 )α T 1−β ηy (vy (cid:33)\n\n1α<0.5\n\n(cid:32)\n\n+\n\nκ2 +\n\n(cid:33)\n\n(cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy\n\n(1\n\nl (ηx)2 G2−2α α)ηy (vy\n\n0 )α−β T α\n\n−\n\n+\n\nκ (vy\n\n0 )β G2 μηyT\n\n+\n\n2l (2βG)\n\n1\n\n1−β +2 G2\n\n4μ\n\n1\n\n1−β +3 (ηy)\n\n1\n\n1−β +2 (vy\n\n0 )2−2β T\n\n.\n\nProof. By smoothness of the primal function, we have\n\nΦ(xt+1)\n\nΦ(xt)\n\nηt\n\n≤ −\n\n−\n\n(cid:68)\n\n∇\n\nΦ(xt),\n\n∇\n\n(cid:69)\n\nx (cid:101)f (xt, yt)\n\n+ lκη2\n\nt\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n.\n\nBy multiplying 1 ηt have\n\non both sides and taking the expectation w.r.t. the noise of current iteration, we\n\nE\n\n(cid:20) Φ(xt+1) ηt\n\n−\n\nΦ(xt)\n\n(cid:21)\n\nΦ(xt),\n\n≤ −⟨∇\n\nxf (xt, yt) ⟩\n\n∇\n\n+ lκE\n\n(cid:20)\n\nηt\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\n=\n\n−∥∇\n\nxf (xt, yt)\n\n2 + ∥\n\n⟨∇\n\nxf (xt, yt)\n\nΦ(xt),\n\nxf (xt, yt) ⟩\n\n∇\n\n− ∇\n\n+ lκE\n\n(cid:20)\n\nηt\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\nxf (xt, yt)\n\n≤ −∥∇\n\n2 + ∥\n\n1 2 ∥∇\n\nxf (xt, yt)\n\nΦ(xt)\n\n− ∇\n\n2 + ∥\n\n1 2 ∥∇\n\nxf (xt, yt)\n\n2 + lκE ∥\n\n(cid:20)\n\nηt\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\n=\n\n1 2 ∥∇\n\n−\n\nxf (xt, yt)\n\n∥\n\n2 +\n\n1 2 ∥∇\n\nxf (xt, yt)\n\nΦ(xt)\n\n2 + lκE ∥\n\n− ∇\n\n(cid:20)\n\nηt\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\nSumming over t = 0 to T\n\n1, rearranging and taking total expectation, we get\n\n− (cid:35)\n\nE\n\n(cid:34)T −1 (cid:88) t=0 ∥∇\n\nxf (xt, yt)\n\n2 ∥\n\n31\n\nPublished as a conference paper at ICLR 2023\n\n≤\n\n2E\n\n(cid:124)\n\n(cid:34)T −1 (cid:88)\n\nΦ(xt)\n\nt=0\n\n(cid:123)(cid:122) (I)\n\n(cid:35)\n\nΦ(xt+1)\n\n−\n\nηt\n\n+ 2lκE\n\n(cid:34)T −1 (cid:88)\n\nt=0\n\nηt\n\n(cid:13) (cid:13) (cid:13)∇ (cid:123)(cid:122) (J)\n\nx (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n+ E\n\n(cid:125)\n\n(cid:124)\n\n(cid:34)T −1 (cid:88) t=0 ∥∇\n\nxf (xt, yt)\n\n(cid:123)(cid:122) (K)\n\n2 ∥\n\n.\n\n(cid:35)\n\n(cid:125)\n\nΦ(xt)\n\n− ∇\n\n(12)\n\n(cid:125)\n\n(cid:124)\n\n(cid:35)\n\nΦ(xt+1)\n\n−\n\nηt\n\nTerm (I)\n\n(cid:34)T −1 (cid:88)\n\nΦ(xt)\n\n2E\n\nt=0\n\n(cid:34)\n\n(cid:34)\n\n2E\n\n2E\n\n≤\n\n≤\n\nΦ(x0)\n\nη0 −\n\nΦ(xT ) ηT −1\n\n+\n\nT −1 (cid:88)\n\nt=1\n\nΦ(xt)\n\n(cid:18) 1\n\nηt −\n\nΦmax\n\nη0 −\n\nΦ∗ ηT −1\n\n+\n\nT −1 (cid:88)\n\nt=1\n\nΦmax\n\n(cid:18) 1\n\nηt −\n\n= 2E\n\n(cid:21)\n\n(cid:20) ∆Φ ηT −1\n\n= 2E\n\n(cid:20) ∆Φ\n\nηx max\n\n{\n\nT , vy vx\n\nT }\n\nα\n\n1 ηt−1 (cid:21)\n\n.\n\n(cid:19)(cid:35)\n\n1 ηt−1 (cid:19)(cid:35)\n\nTerm (J)\n\nT −1 (cid:88)\n\nE\n\n(cid:20)\n\nηt\n\n2lκ\n\nt=0\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:34)T −1 (cid:88)\n\n= 2lκE\n\nηx t+1, vy\n\nt+1\n\nmax (cid:8)vx\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:9)α\n\nx (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n1 (cid:0)vx\n\nt+1\n\n(cid:1)α\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\nT −1 (cid:88)\n\nt=0\n\n1 (cid:0)vx\n\nt+1\n\n(cid:1)α\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:33)(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\nt=0 (cid:34)T −1 (cid:88)\n\nt=0\n\n2lκηxE\n\n(cid:34)(cid:32)\n\n2lκηxE\n\n(cid:104)\n\nE\n\n2lκηx α\n1\n\n−\n\n≤\n\n≤\n\n≤\n\nvx 0\n0 )α + (vx T )1−α(cid:105)\n\n.\n\n(vx\n\nTerm (K) According to the smoothness of f (xt,\n\nE\n\n(cid:34)T −1 (cid:88) t=0 ∥∇\n\nxf (xt, yt)\n\nΦ(xt)\n\n− ∇\n\n(cid:35)\n\n2 ∥\n\nl2E\n\n≤\n\n(cid:34)T −1 (cid:88) t=0 ∥\n\n), we have ·\n\n(cid:35)\n\nyt\n\n2\n\ny∗ t ∥\n\n−\n\n≤\n\n2lκE\n\n(cid:34)T −1 (cid:88)\n\nt=0\n\n(cid:35)\n\n(f (xt, y∗ t )\n\n−\n\nf (xt, yt))\n\n,\n\nwhere the last inequality follows the strong-concavity of y. Now we let\n\nC =\n\n(cid:32)\n\n8lκ3 (ηx)2 μ (ηy)2\n\n(cid:33) 1\n\n2α−2β\n\n,\n\nand apply Lemma C.5, in total, we have\n\nxf (xt, yt)\n\n(cid:35)\n\n2 ∥\n\n(cid:34)T −1 (cid:88) t=0 ∥∇ (cid:34)T −1 (cid:88) t=0 ∥∇\n\nE\n\nE\n\n1 2\n\n≤\n\nxf (xt, yt)\n\n∥\n\n(cid:35)\n\n2\n\n+ 2E\n\n(cid:20) ∆Φ\n\nηx max\n\n(cid:21)\n\nα\n\nT , vy vx\n\nT }\n\n{\n\n(cid:32)\n\n+\n\n+\n\n+\n\nE\n\n+\n\n−\n\n(cid:104) (vy\n\nT )1−β(cid:105)\n\nlκηy 1\n1 β\nμ κ4 (cid:0)μηyC β + 2C 2β(cid:1) (ηx)2 (ηy)2 (cid:98)L2G2 (ηx)2 0 )2α−β μηy (vy\n\nκ2 +\n\n(1\n\n(cid:33)\n\n(cid:32)\n\n+\n\nE\n\n−\n\n(cid:33)\n\nηy 0 )β (vy (cid:34) 1 + log vx\n\n8lκ2ηxG2 ηy (vy\n\n0 )α E\n\n(cid:104) (vy\n\nT )β(cid:105)\n\nlog vx\n\n0\n\nT − 0 )2α−1\n\n(vx\n\n1α≥0.5 +\n\n·\n\nlκ (ηx)2 α)ηy (vy\n\n0 )α−β\n\nE\n\n(cid:104)\n\nT )1−α(cid:105)\n\n(vx\n\n+\n\n32\n\n+\n\n2lκηx α\n1\n\n−\n\nE\n\n(cid:104) (vx\n\nT )1−α(cid:105)\n\n(cid:35)\n\n1α<0.5\n\nT )1−2α (vx 1\n\n2α ·\n\nκ2 (vy\n\n− 0 )β G2 μηy\n\nPublished as a conference paper at ICLR 2023\n\nlκ (2βG) 1−β +3 (ηy)\n\n1\n\n1\n\n1−β +2 G2 1−β +2 (vy\n\n1\n\n0 )2−2β .\n\n+\n\n2μ\n\nIt remains to bound (vz\n\nT )m for z\n\n∈ {\n\nx, y\n\nand m\n\n} (vz\n\nT )m\n\n≤\n\n0: ≥\n(cid:0)T G2(cid:1)m\n\n.\n\nBringing it back, we conclude our proof.\n\nC.4 TIADA WITHOUT ACCESSING OPPONENT’S GRADIENTS\n\nThe effective stepsize of x requires the knowledge of gradients of y, i.e., vy t+1. At the end of Section 3, we discussed the situation when such information is not available. Now we formally introduce the algorithm and present the convergence result.\n\nAlgorithm 2 TiAda without MAX\n\n1: Input: (x0, y0), vx 2: for t = 0, 1, 2, ... do\n\n0 > 0, vy\n\n0 > 0, ηx > 0, ηy > 0, α > 0, β > 0 and α > β.\n\n3:\n\n4:\n\n5:\n\nsample i.i.d. ξx t+1 = vx vx\n\nt +\n\nt , and let gx t+1 = vy\n\nt = t +\n\n2 and vy\n\nt and ξy gx t ∥ ηx t+1)α gx\n\n(vx\n\n∥\n\nt and yt+1 =\n\nxF (xt, yt; ξx gy t ∥ (cid:18)\n\n2\n\nyt + ηy\n\n∇ ∥\n\nY\n\nP\n\nt+1)β gy\n\nt\n\n(vy\n\n(cid:19)\n\nxt+1 = xt\n\n−\n\n6: end for\n\nt ) and gy\n\nt =\n\nyF (xt, yt; ξy t )\n\n∇\n\nTheorem C.3 (stochastic). Under Assumptions 3.1, 3.2, 3.4 and 3.5, Algorithm 2 with stochastic gradient oracles satisfies that for any 0 < β < α < 1, after T iterations,\n\n(cid:34)\n\nE\n\n1 T\n\nT −1 (cid:88) t=0 ∥∇\n\n(cid:35)\n\nxf (xt, yt)\n\n2 ∥\n\n2∆ΦG2α ηxT 1−α +\n\n≤\n\n2lκηxG2−2α (1\n\nα)T α +\n\n−\n\n(cid:32)\n\n+\n\n(ηx)2 κ2 0 )β ηy 2 (vy\n\n+\n\n(ηx)2 κ2 μ(ηy)2\n\n(cid:32)\n\n0 )β G2 (vy 2μ2ηy + 4μ (cid:33) (cid:32) (cid:0)1 + log G2T\n\n(2βG) 1−β +3 (ηy) log vx −\n0 )2α−1 T 1−2β\n\n(vx\n\n0\n\n1\n\n(cid:1) G4β\n\n1\n\n1−β +2 G2 1−β +2 (vy\n\n1\n\n0 )2−2β\n\n(cid:33)\n\n1 T\n\n+\n\nηyG2−2β\n\n2(1\n\n−\n\nβ)T β\n\n1α≥0.5 +\n\n·\n\nG2−4α+4β\n\n2α)T 2α−2β ·\n\n(1\n\n−\n\n(cid:33)\n\n1α<0.5\n\n,\n\nand\n\nE\n\n(cid:32)\n\n≤\n\n(cid:34)\n\n1 T\n\nκ (vy\n\nT −1 (cid:88) t=0 ∥∇ 0 )β G2 μηy\n\nyf (xt, yt)\n\n(cid:35)\n\n2 ∥\n\n+\n\n4μ\n\n1\n\n2l (2βG) 1−β +3 (ηy) 2 (ηx)2 κ3 (ηy)2\n\n1\n\n1\n\n1−β +2 G2 1−β +2 (vy\n\n0 )2−2β (cid:33) (cid:32) (cid:0)1 + log G2T\n\nlog vx −\n0 )2α−1 T 1−2β\n\n0\n\n(vx\n\n(cid:33)\n\n1 T\n\n+\n\nlηyG2−2β β)T β (1 −\n(cid:1) G4β\n\n(cid:32)\n\n+\n\nl (ηx)2 κ2 0 )β ηy (vy\n\n+\n\n1α≥0.5 +\n\n·\n\nG2−4α+4β\n\n2α)T 2α−2β ·\n\n(1\n\n−\n\n(cid:33)\n\n1α<0.5\n\n.\n\nRemark C.1. The best rate achievable is (cid:101) O\n\n(cid:0)ε−6(cid:1) by choosing α = 1/2 and β = 1/3.\n\nProof. Lemmas C.1 and C.4 can be directly used here because they do not have or expand the effective stepsize of x, i.e., ηt. This is also the case for the beginning part of Appendix C.3, the proof of Theorem 3.2, up to Equation (12). However, we need to bound Terms (I), (J) and (K) in Equation (12) differently. According to our assumption on bounded stochastic gradients, we know that vx\n\nT are both upper bounded by T G2, which we will use throughout the proof.\n\nT and vy\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nTerm (I)\n\n(cid:34)T −1 (cid:88)\n\nΦ(xt)\n\n2E\n\nt=0\n\n(cid:35)\n\nΦ(xt+1)\n\n−\n\nηt\n\n(cid:34)\n\n(cid:34)\n\n2E\n\n2E\n\n≤\n\n≤\n\nΦ(x0)\n\nη0 −\n\nΦ(xT ) ηT −1\n\n+\n\nT −1 (cid:88)\n\nt=1\n\nΦ(xt)\n\n(cid:18) 1\n\nηt −\n\nΦmax\n\nη0 −\n\nΦ∗ ηT −1\n\n+\n\nT −1 (cid:88)\n\nt=1\n\nΦmax\n\n(cid:18) 1\n\nηt −\n\n1 ηt−1\n\n(cid:19)(cid:35)\n\n1 ηt−1 (cid:19)(cid:35)\n\n= 2E\n\n(cid:21)\n\n(cid:20) ∆Φ ηT −1\n\n= 2E\n\n(cid:20) ∆Φ\n\nηx (vx\n\nT )α\n\n(cid:21)\n\n2∆ΦG2αT α ηx\n\n.\n\n≤\n\nTerm (J)\n\nT −1 (cid:88)\n\nE\n\n(cid:20)\n\nηt\n\n2lκ\n\nt=0\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\n= 2lκηxE\n\n(cid:34)T −1 (cid:88)\n\nt=0\n\n1 (cid:0)vx\n\nt+1\n\n(cid:1)α\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:34)(cid:32)\n\n2lκηxE\n\n(cid:104)\n\nE\n\n2lκηx α\n1\n\n−\n\nvx 0\n0 )α + (vx T )1−α(cid:105)\n\n(vx\n\n≤\n\n≤\n\nT −1 (cid:88)\n\nt=0\n\n1 (cid:0)vx\n\nt+1\n\n(cid:1)α\n\n(cid:13) (cid:13) (cid:13)∇\n\nx (cid:101)f (xt, yt)\n\n2(cid:33)(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n2lκηxG2−2αT 1−α\n\nα\n\n1\n\n−\n\n.\n\n≤\n\nTerm (K) According to the smoothness and strong-concavity of f (xt,\n\n), we have\n\nE\n\n(cid:34)T −1 (cid:88) t=0 ∥∇\n\nxf (xt, yt)\n\nΦ(xt)\n\n− ∇\n\n(cid:35)\n\n2 ∥\n\nl2E\n\n≤\n\n(cid:34)T −1 (cid:88) t=0 ∥\n\nyt\n\n(cid:35)\n\n2\n\ny∗ t ∥\n\n−\n\n≤\n\n2lκE\n\nTo bound the RHS, we use Young’s inequality and have\n\n· (cid:34)T −1 (cid:88)\n\nt=0\n\n(cid:35)\n\n(f (xt, y∗ t )\n\n−\n\nf (xt, yt))\n\n.\n\n(cid:13) (cid:13)yt+1\n\n−\n\ny∗\n\nt+1\n\n(cid:13) 2\n(cid:13)\n\n(1 + λt)\n\n∥\n\n≤\n\nyt+1\n\n2 +\n\ny∗ t ∥\n\n−\n\n(cid:18)\n\n1 +\n\n(cid:19)\n\n1 λt\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n.\n\nThen applying Lemma C.1 with λt = μγt\n\n2 gives us\n\n(cid:34)T −1 (cid:88)\n\n(f (xt, y∗ t )\n\n(cid:35)\n\nf (xt, yt))\n\n−\n\nγtμ\n\n− 2γt\n\nyt\n\n∥\n\n−\n\n2\n\ny∗ t ∥\n\n−\n\nE\n\nE\n\n≤\n\nt=0 (cid:34)T −1 (cid:88)\n\n(cid:18) 1\n\nt=0\n\n(cid:34)T −1 (cid:88)\n\nt=0\n\nγt 2\n\n+ E\n\n(cid:124)\n\ny (cid:101)f (xt, yt)\n\n(cid:13) (cid:13) (cid:13)∇ (cid:123)(cid:122) (L)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:125)\n\n(cid:124)\n\n(cid:13) (cid:13)yt+1\n\n2(cid:19)(cid:35) (cid:13) (cid:13)\n\ny∗\n\nt+1\n\n1 γt(2 + μγt) (cid:16)\n\n\n\n− (cid:17)\n\n+ E\n\n\n\nT −1 (cid:88)\n\n1 + 2 μγt\n\nγt(2 + μγt)\n\nt=0\n\n\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n, \n\n(cid:123)(cid:122) (M)\n\n(cid:125)\n\nwhere the first term is follow.\n\nO\n\n(1) according to Lemma C.4. The other two terms can be bounded as\n\nTerm (L) (cid:32)\n\n(cid:34)\n\nE\n\n≤\n\nηy 2\n\n0\n\nvy 0 )β + (vy\n\nT −1 (cid:88)\n\nt=0\n\n1\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β\n\n(cid:13) (cid:13) (cid:13)∇\n\ny (cid:101)f (xt, yt)\n\n2(cid:33)(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:20)\n\nE\n\n≤\n\nηy\n\n2(1\n\nβ)\n\n−\n\n(vy\n\nT )1−β\n\n(cid:21)\n\n≤\n\nηyG2−2βT 1−β\n\n2(1\n\nβ)\n\n−\n\n.\n\nTerm (M) (cid:32) (cid:34)T −1 (cid:88)\n\n= E\n\nt=0\n\n1\n\n(cid:0)vy\n\nt+1\n\n(cid:1)β +\n\n2 μηy\n\n(cid:33) (cid:0)vy\n\n(cid:1)2β\n\nt+1 2ηy(1 + λt)\n\n(cid:35)\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n34\n\nPublished as a conference paper at ICLR 2023\n\n1 0 )β ηy 2 (vy\n\n+\n\n1 μ(ηy)2\n\n(cid:0)vy\n\nt+1\n\n(cid:1)2β (cid:13)\n\n(cid:13)y∗\n\nt+1 −\n\n(cid:35)\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n(cid:33)\n\n(cid:34)T −1 (cid:88)\n\nE\n\nt=0\n\n(cid:33)\n\n(cid:34)\n\n+\n\n1 μ(ηy)2\n\nE\n\n(vy\n\nT )2β\n\nT −1 (cid:88)\n\nt=0\n\n(cid:13) (cid:13)y∗\n\nt+1 −\n\n(cid:35)\n\ny∗\n\nt\n\n(cid:13) 2\n(cid:13)\n\n(cid:33)\n\n(cid:34)\n\nE\n\n(vy\n\nT )2β\n\nκ2 μ(ηy)2\n\nT −1 (cid:88) t=0 ∥\n\nxt+1\n\nxt\n\n∥\n\n−\n\n(cid:35)\n\n2\n\n(cid:32)\n\n(cid:32)\n\n(cid:32)\n\n(cid:32)\n\n(cid:32)\n\n(cid:32)\n\n≤\n\n≤\n\n≤\n\n=\n\n≤\n\n≤\n\n2 (vy\n\n1 0 )β ηy 2 (vy κ2 0 )β ηy (ηx)2 κ2 0 )β ηy 2 (vy (ηx)2 κ2 0 )β ηy 2 (vy (ηx)2 κ2 0 )β ηy 2 (vy\n\n+\n\n+\n\n+\n\n+\n\nx (cid:101)f (xt, yt)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:33)\n\n(cid:34)\n\nE\n\n(vy\n\nT )2β\n\n(cid:33)\n\n(cid:34)\n\nE\n\n(vy\n\nT )2β\n\nT −1 (cid:88)\n\n1\n\nt=0 (cid:32)\n\n(cid:0)vx\n\nt+1\n\n(cid:1)2α\n\n1 + log vx\n\n(cid:33) (cid:32) (cid:0)1 + log G2T\n\n(ηx)2 κ2 μ(ηy)2\n\n(ηx)2 κ2 μ(ηy)2\n\n(ηx)2 κ2 μ(ηy)2\n\nlog vx\n\n0\n\n(vx\n\nT − 0 )2α−1 log vx\n\n· (cid:1) G4βT 2β\n\n0\n\n1α≥0.5 +\n\n− 0 )2α−1\n\n(vx\n\n1α≥0.5 +\n\n·\n\nT )1−2α (vx 1\n\n2α ·\n\n(cid:33)(cid:35)\n\n1α<0.5\n\n− G2−4α+4βT 1−2α+2β\n\n2α\n\n1\n\n−\n\n(cid:33)\n\n1α<0.5\n\n,\n\n·\n\nwhere we used the the Lipschitzness of y∗( ·\n\nSummarizing all the terms, we finish the proof.\n\n) in the third inequality.\n\n35",
    "reference": "# Summary Of The Paper\n\nThe authors propose an algorithm named TiAda which is a time-scale adaptive algorithm for non-convex-strongly-convex (NC-SC) minimax problems. The algorithm is a single loop and problem-specific parameter agnostic one, which are improvements over the related prior work. The authors provide insight into the design of the algorithm, theoretically analyze the algorithm and empirically validate its usefulness of the algorithm.  The authors also provide some generalization of TiAda to other adaptive methods and empirically validate their usefulness against related baselines.\n\n# Strength And Weaknesses\n\n**Strengths:**\n\n1. The proposed method improves upon the main related prior work such as similar implementation in deterministic and stochastic cases (agnostic to the noise level in the gradient). It does not need complex subroutines in the inner loop update for termination.\n2. The work provides extensive validation of the proposed method against related baselines, in their experimental setting, and provides an ablation over some proposed algorithm-specific parameters.\n3. The intuition of the work is clear and the paper is easy to follow.\n\n**Weaknesses:**\n\n1. The rate obtained in Theorem 3.2 is worse compared to state-of-the-art Na-Ada. Could you explain the reason behind this worse rate?\n2. It is unclear how the list of values for r is chosen for results in Figure 2. What is the reason for choosing different orders of ratios for the two other experiments? It seems like NeAda would perform better for smaller choices of r. Furthermore, how was $\\eta_x$ (or $\\eta_y$) chosen for these experiments?\n3. It is not clear why Na-Ada is not included in GAN simulation in Figure 4. \n4. Some typos in the text (e.g. definition in Assumption 3.2).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written and easy to follow. The intuition behind the algorithm is clear. While the algorithm has some features of prior work, the improvements made by the proposed method are somewhat novel.\n\n# Summary Of The Review\n\nThe paper proposes a parameter-agnostic adaptive algorithm for solving NC-SC minimax problem. The authors have identified a gap in the literature in this regard and have provided a somewhat novel contribution. The authors provide the intuition as to why their solution addresses the prevailing issues, and theoretically and empirically justify their claims. The algorithm seems to be robust to the hyperparameter choices required by the algorithm, as per the results shown by the authors. The empirical results provided by the authors suggest the proposed methods outperform related baselines. There are some issues regarding the experimental setup in some of the empirical results, as mentioned above under “Weaknesses”. Furthermore, the theoretical results do not improve the prior work in the stochastic setting.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nINCREMENTAL LEARNING OF STRUCTURED MEMORY VIA CLOSED-LOOP TRANSCRIPTION\n\nShengbang Tong1, Xili Dai2, Ziyang Wu1, Mingyang Li3, Brent Yi1, Yi Ma1, 3 1 University of California, Berkeley, 2 The Hong Kong University of Science and Technology(Guangzhou) 3 Tsinghua-Berkeley Shenzhen Institute (TBSI), Tsinghua University\n\nABSTRACT\n\nThis work proposes a minimal computational model for learning structured memories of multiple object classes in an incremental setting. Our approach is based on establishing a closed-loop transcription between the classes and a corresponding set of subspaces, known as a linear discriminative representation, in a low-dimensional feature space. Our method is simpler than existing approaches for incremental learning, and more efficient in terms of model size, storage, and computation: it requires only a single, fixed-capacity autoencoding network with a feature space that is used for both discriminative and generative purposes. Network parameters are optimized simultaneously without architectural manipulations, by solving a constrained minimax game between the encoding and decoding maps over a single rate reduction-based objective. Experimental results show that our method can effectively alleviate catastrophic forgetting, achieving significantly better performance than prior work of generative replay on MNIST, CIFAR-10, and ImageNet-50, despite requiring fewer resources.\n\n1\n\nINTRODUCTION\n\nArtificial neural networks have demonstrated a great ability to learn representations for hundreds or even thousands of classes of objects, in both discriminative and generative contexts. However, networks typically must be trained offline, with uniformly sampled data from all classes simultaneously. When the same network is updated to learn new classes without data from the old ones, previously learned knowledge will fall victim to the problem of catastrophic forgetting (McCloskey & Cohen, 1989). This is known in neuroscience as the stability-plasticity dilemma: the challenge of ensuring that a neural system can learn from a new environment while retaining essential knowledge from previous ones (Grossberg, 1987).\n\nIn contrast, natural neural systems (e.g. animal brains) do not seem to suffer from such catastrophic forgetting at all. They are capable of developing new memory of new objects while retaining memory of previously learned objects. This ability, for either natural or artificial neural systems, is often referred to as incremental learning, continual learning, sequential learning, or life-long learning (Allred & Roy, 2020).\n\nWhile many recent works have highlighted how incremental learning might enable artificial neural systems that are trained in more flexible ways, the strongest existing efforts toward answering the stability-plasticity dilemma for artificial neural networks typically require raw exemplars (Rebuffi et al., 2017; Chaudhry et al., 2019b) or require external task information (Kirkpatrick et al., 2017). Raw exemplars, particularly in the case of high-dimensional inputs like images, are costly and difficult to scale, while external mechanisms — which, as surveyed in Section 2, include secondary networks and representation spaces for generative replay, incremental allocation of network resources, network duplication, or explicit isolation of used and unused parts of the network — require heuristics and incur hidden costs.\n\nIn this work, we are interested in an incremental learning setting that counters these trends with two key qualities. (1) The first is that it is memory-based. When learning new classes, no raw exemplars of old classes are available to train the network together with new data. This implies that one has to rely on a compact and thus structured “memory” of old classes, such as incrementally learned generative representations of the old classes, as well as the associated encoding and decoding mappings (Kemker\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Overall framework of our closed-loop transcription based incremental learning for a structured LDR memory. Only a single, entirely self-contained, encoding-decoding network is needed: for a new data class Xnew, a new LDR memory Znew is incrementally learned as a minimax game between the encoder and decoder subject to the constraint that old memory of past classes Zold is intact through the closed-loop transcription (or replay): Zold ≈ ˆZold = f (g(Zold)).\n\n& Kanan, 2018). (2) The second is that it is self-contained. Incremental learning takes place in a single neural system with a fixed capacity, and in a common representation space. The ability to minimize forgetting is implied by optimizing an overall learning objective, without external networks, architectural modifications, or resource allocation mechanisms.\n\nConcretely, the contributions of our work are as follows:\n\n(1) We demonstrate how the closed-loop transcription (CTRL) framework (Dai et al., 2022; 2023) can be adapted for memory-based, self-contained mitigation of catastrophic forgetting (Figure 1). To the best of our knowledge, these qualities have not yet been demonstrated by existing methods. Closedloop transcription aims to learn linear discriminative representations (LDRs) via a rate reductionbased (Yu et al., 2020; Ma et al., 2007; Ding et al., 2023) minimax game: our method, which we call incremental closed-loop transcription (i-CTRL), shows how these principled representations and objectives can uniquely facilitate incremental learning of stable and structured class memories. This requires only a fixed-sized neural system and a common learning objective, which transforms the standard CTRL minimax game into a constrained one, where the goal is to optimize a rate reduction objective for each new class while keeping the memory of old classes intact.\n\n(2) We quantitatively evaluate i-CTRL on class-incremental learning for a range of datasets: MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky et al., 2009), and ImageNet-50 (Deng et al., 2009). Despite requiring fewer resources (smaller network and nearly no extra memory buffer), i-CTRL outperforms comparable alternatives: it achieves a 5.8% improvement in average classification accuracy over the previous state of the art on CIFAR-10, and a 10.6% improvement in average accuracy on ImageNet-50.\n\n(3) We qualitatively verify the structure and generative abilities of learned representations. Notably, the self-contained i-CTRL system’s common representation is used for both classification and generation, which eliminates the redundancy of external generative replay representations used by prior work.\n\n(4) We demonstrate a “class-unsupervised” incremental reviewing process for i-CTRL. As an incremental neural system learns more classes, the memory of previously learned classes inevitably degrades: by seeing a class only once, we can only expect to form a temporary memory. Facilitated by the structure of our linear discriminative representations, the incremental reviewing process shows that the standard i-CTRL objective function can reverse forgetting in a trained i-CTRL system using samples from previously seen classes even if they are unlabeled. The resulting semi-supervised process improves generative quality and raises the accuracy of i-CTRL from 59.9% to 65.8% on CIFAR-10, achieving jointly-optimal performance despite only incrementally provided class labels.\n\n2 RELATED WORK\n\nA significant body of work has studied methods for addressing forms of the incremental learning problem. In this section, we discuss a selection of representative approaches, and highlight relationships to i-CTRL.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nIn terms of how new data classes are provided and tested, incremental learning methods in the literature can be roughly divided into two groups. The first group addresses task incremental learning (task-IL), where a model is sequentially trained on multiple tasks where each task may contain multiple classes to learn. At test time, the system is asked to classify data seen so far, provided with a task identifier indicating which task the test data is drawn from. The second group, which many recent methods fall under, tackles class-incremental learning (class-IL). Class-IL is similar to task-IL but does not require a task identitifier at inference. Class-IL is therefore more challenging, and is the setting considered by this work.\n\nIn terms of what information incremental learning relies on, existing methods mainly fall into the following categories.\n\nRegularization-based methods introduce penalty terms designed to mitigate forgetting of previously trained tasks. For instance, Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) and Synaptic Intelligence (SI) (Zenke et al., 2017) limit changes of model parameters deemed to be important for previous tasks by imposing a surrogate loss. Alternatively, Learning without Forgetting (LwF) (Li & Hoiem, 2017) utilizes a knowledge distillation loss to prevent large drifts of the model weights during training on the current task. Although these methods, which all apply regularization on network parameters, have demonstrated competitive performance on task-IL scenarios, our evaluations (Table 1) show that their performance does not transfer to the more challenging class-IL settings.\n\nArchitecture-based methods explicitly alter the network architecture to incorporate new classes of data. Methods such as Dynamically Expandable Networks (DEN) (Yoon et al., 2017), Progressive Neural Networks (PNN) (Rusu et al., 2016), Dynamically Expandable Representation (DER) (Yan et al., 2021) and ReduNet (Wu et al., 2021) add new neural modules to the existing network when required to learn a new task. Since these methods are not dealing with a self-contained network with a fixed capacity, one disadvantage of these methods is therefore their memory footprint: their model size often grows linearly with the number of tasks or classes. Most architecture-based methods target the less challenging task-IL problems and are not suited for class-IL settings. In contrast, our work addresses the class-IL setting with only a simple, off-the-shelf network (see Appendix B for details). Note that the method Redunet also uses rate-reduction inspired objective function to conduct class incremental learning. Our method is different from the method that (i) our method does not require dynamic expansion of the network (ii) Our method aims to learn a continuous encoder and decoder.(iii) Empirically, our method has shown better performance and scalability.\n\nExemplar-based methods combat forgetting by explicitly retaining data from previously learned tasks. Most early memory-based methods, such as iCaRL (Rebuffi et al., 2017) and ER (Chaudhry et al., 2019a), store a subset of raw data samples from each learned class, which is used along with the new classes to jointly update the model. A-GEM (Chaudhry et al., 2018) also relies on storing such an exemplar set: rather than directly training with new data, A-GEM calculates a reference gradient from the stored data and projects the gradient from the new task onto these reference directions in hope of maintaining performance on old tasks. While these methods have demonstrated promising results, storing raw data of learned classes is unnatural from a neuroscientific perspective(Robins, 1995) and resource-intensive, particularly for higher-dimensional inputs. A fair comparison is thus not possible: the structured memory used by i-CTRL is highly compact in comparison, but as demonstrated in Section 4 still outperforms several exemplar-based methods.\n\nGenerative memory-based methods use generative models such as GANs or autoencoders for replaying data for old tasks or classes, rather than storing raw samples and exemplars. Methods such as Deep Generative Replay (DGR) (Shin et al., 2017), Memory Replay Gans (MeRGAN) (Wu et al., 2018), and Dynamic Generative Memory (DGM) (Ostapenko et al., 2019) propose to train a GAN on previously seen classes and use synthesized data to alleviate forgetting when training on new tasks. Methods like DAE (Zhou et al., 2012) learn with add and merge feature strategy. To further improve memory efficiency, methods such as FearNet (Kemker & Kanan, 2018) and EEC (Ayub & Wagner, 2020) store intermediate features of old classes and use these more compact representations for generative replay. Existing generative memory-based approaches have performed competitively on class-IL without storing raw data samples, but require separate networks and feature representations for generative and discriminative purposes. Our comparisons are primarily focused on this line of work, as our approach also uses a generative memory for incremental learning. Uniquely, however, we do so with only a single closed-loop encoding-decoding network and store a minimum amount of\n\n3\n\nPublished as a conference paper at ICLR 2023\n\ninformation – a mean and covariance – for each class. This closed-loop generative model is more stable to train (Dai et al., 2022; Tong et al., 2022), and improves resource efficiency by obviating the need to train separate generative and discriminative representations.\n\n3 METHOD\n\n3.1 LINEAR DISCRIMINATIVE REPRESENTATION AS MEMORY\n\nConsider the task of learning to memorize k classes of objects from images. Without loss of generality, we may assume that images of each class belong to a low-dimensional submanifold in the space of images RD, denoted as Mj, for j = 1, . . . , k. Typically, we are given n samples X = [x1, . . . , xn] ⊂ RD×n that are partitioned into k subsets X = ∪k j=1Xj, with each subset Xj sampled from Mj, j = 1, . . . , k. The goal here is to learn a compact representation, or a “memory”, of these k classes from these samples, which can be used for both discriminative (e.g. classification) and generative purposes (e.g. sampling and replay).\n\nAutoencoding. We model such a memory with an autoencoding tuple {f, g, z} that consists of an encoder f (·, θ) parameterized by θ, that maps the data x ∈ RD continuously to a compact feature z in a much lower-dimensional space Rd, and a decoder g(·, η) parameterized by η, that maps a feature z back to the original data space RD:\n\nFor the set of samples X, we let Z = f (X, θ) the set of corresponding features. Similarly let ˆX The autoencoding tuple can be illustrated by the following diagram:\n\nf (·, θ) : x (cid:55)→ z ∈ Rd; (1) .\n= [z1, . . . , zn] ⊂ Rd×n with zi = f (xi, θ) ∈ Rd be .\n= g(Z, η) be the decoded data from the features.\n\ng(·, η) : z (cid:55)→ ˆx ∈ RD.\n\nX\n\nf (x,θ)\n\n−−−−−−→ Z\n\ng(z,η)\n\n−−−−−−→ ˆX.\n\n(2)\n\nWe refer to such a learned tuple: {f (·, θ), g(·, η), Z} as a compact “memory” for the given dataset X.\n\nStructured LDR autoencoding. For such a memory to be convenient to use for subsequent tasks, including incremental learning, we would like a representation Z that has well-understood structures and properties. Recently, Chan et al. (Chan et al., 2021) proposed that for both discriminative and generative purposes, Z should be a linear discriminative representation (LDR). More precisely, let Zj = f (Xj, θ), j = 1, . . . , k be the set of features associated with each of the k classes. Then each Zj should lie on a low-dimensional linear subspace Sj in Rd which is highly incoherent (ideally orthogonal) to others Si for i ̸= j. Notice that the linear subspace structure enables both interpolation and extrapolation, and incoherence between subspaces makes the features discriminative for different classes. As we will see, these structures are also easy to preserve when incrementally learning new classes.\n\n3.2 LEARNING LDR VIA CLOSED-LOOP TRANSCRIPTION\n\nAs shown in (Yu et al., 2020), the incoherence of learned LDR features Z = f (X, θ) can be promoted by maximizing a coding rate reduction objective, known as the MCR2 principle:\n\nmax θ\n\n∆R(Z) = ∆R(Z1, . . . , Zk)\n\n. =\n\n1 2\n(cid:124)\n\nlog det (cid:0)I + αZZ ∗(cid:1) (cid:125) (cid:123)(cid:122) R(Z)\n\n−\n\nk (cid:88)\n\nj=1\n\nγj\n\n1 2\n(cid:124)\n\nlog det (cid:0)I + αjZjZ ∗ (cid:123)(cid:122) R(Zj )\n\nj\n\n(cid:1) ,\n\n(cid:125)\n\nwhere, for a prescribed quantization error ε, α = d\n\nnε2 , αj = d\n\n|Zj |ε2 , γj = |Zj | n .\n\nAs noted in (Yu et al., 2020), maximizing the rate reduction promotes learned features that span the entire feature space. It is therefore not suitable to naively apply for the case of incremental learning, as the number of classes increases within a fixed feature space.1 The closed-loop transcription (CTRL) framework introduced by (Dai et al., 2022) suggests resolving this challenge by learning the encoder f (·, θ) and decoder g(·, η) together as a minimax game: while the encoder tries to maximize the rate reduction objective, the decoder should minimize it instead. That is, the decoder g minimizes\n\n1As the number of classes is initially small in the incremental setting, if the dimension of the feature space d\n\nis high, maximizing the rate reduction may over-estimate the dimension of each class.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nresources (measured by the coding rate) needed for the replayed data for each class ˆXj = g(Zj, η), decoded from the learned features Zj = f (Xj, θ), to emulate the original data Xj well enough. As it is typically difficult to directly measure the similarity between Xj and ˆXj, (Dai et al., 2022) proposes measuring this similarity with the rate reduction of their corresponding features Zj and ˆZj = f ( ˆXj(θ, η), θ)(∪ here represents concatenation): 1\n2\n\n(cid:0)R(cid:0)Zj) + R(cid:0) ˆZj)(cid:1).\n\n= R(cid:0)Zj ∪ ˆZj\n\n∆R(cid:0)Zj, ˆZj\n\n(cid:1) −\n\n(cid:1) .\n\n(3)\n\nThe resulting ∆R gives a principled “distance” between subspace-like Gaussian ensembles, with the property that ∆R(cid:0)Zj, ˆZj (cid:1) = 0 iff Cov(Zj) = Cov( ˆZj) (Ma et al., 2007).\n\nmin θ\n\nmax η\n\n∆R(cid:0)Z(cid:1) + ∆R(cid:0) ˆZ(cid:1) +\n\nk (cid:88)\n\nj=1\n\n∆R(cid:0)Zj, ˆZj\n\n(cid:1),\n\n(4)\n\none can learn a good LDR Z when optimized jointly for all k classes. The learned representation Z has clear incoherent linear subspace structures in the feature space which makes them very convenient to use for subsequent tasks (both discriminative and generative).\n\n3.3\n\nINCREMENTAL LEARNING WITH AN LDR MEMORY\n\nThe incoherent linear structures for features of different classes closely resemble how objects are encoded in different areas of the inferotemporal cortex of animal brains (Chang & Tsao, 2017; Bao et al., 2020). The closed-loop transcription X → Z → ˆX → ˆZ also resembles popularly hypothesized mechanisms for memory formation (Ven et al., 2020; Josselyn & Tonegawa, 2020). This leads to a question: since memory in the brains is formed in an incremental fashion, can the above closed-loop transcription framework also support incremental learning?\n\nLDR memory sampling and replay. The simple linear structures of LDR make it uniquely suited for incremental learning: the distribution of features Zj of each previously learned class can be explicitly and concisely represented by a principal subspace Sj in the feature space. To preserve the memory of an old class j, we only need to preserve the subspace while learning new classes. To this end, we simply sample m representative prototype features on the subspace along its top r principal components, and denote these features as Zj,old. Because of the simple linear structures of LDR, we can sample from Zj,old by calculating the mean and covariance of Zj,old after learning class j. The storage required is extremely small, since we only need to store means and covariances, which are sampled from as needed. Suppose a total of t old classes have been learned so far. If old], for all of these classes can be preserved when prototype features, denoted Zold learning new classes, the subspaces {Sj}t j=1 representing past memory will be preserved as well. Details about sampling and calculating mean and convariance can be found in the Appendix 1 and Appendix 2\n\nold, . . . , Zt\n\n. = [Z1\n\nIncremental learning LDR with an old-memory constraint. Notice that, with the learned autoencoding (2), one can replay and use the images, say ˆXold = g(Zold, η), associated with the memory features to avoid forgetting while learning new classes. This is typically how generative models have been used for prior incremental learning methods. However, with the closed-loop framework, explicitly replaying images from the features is not necessary. Past memory can be effectively preserved through optimization exclusively on the features themselves.\n\nConsider the task of incrementally learning a new class of objects.2 We denote a corresponding new sample set as Xnew. The features of Xnew are denoted as Znew(θ) = f (Xnew, θ). We concatenate them together with the prototype features of the old classes Zold and form Z = [Znew(θ), Zold]. We denote the replayed images from all features as ˆX = [ ˆXnew(θ, η), ˆXold(η)] although we do not actually need to compute or use them explicitly. We only need features of replayed images, denoted ˆZ = f ( ˆX, θ) = [ ˆZnew(θ, η), ˆZold(θ, η)].\n\nMirroring the motivation for the multi-class CTRL objective (4), we would like the features of the new class Znew to be incoherent to all of the old ones Zold. As Znew is the only new class whose features needs to be learned, the objective (4) reduces to the case where k = 1:\n\n2In Appendix A, we consider the more general setting where the task contains a small batch of new classes,\n\nand present algorthmic details in that general setting.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nmin η\n\nmax θ\n\n∆R(Z) + ∆R( ˆZ) + ∆R(Znew, ˆZnew).\n\n(5)\n\nHowever, when we update the network parameters (θ, η) to optimize the features for the new class, the updated mappings f and g will change features of the old classes too. Hence, to minimize the distortion of the old class representations, we can try to enforce Cov(Zj,old) = Cov( ˆZj,old). In other words, while learning new classes, we enforce the memory of old classes remain “self-consistent” through the transcription loop:\n\nZold (6) j=1 ∆R(Zj,old, ˆZj,old) = 0. Mathematically, this is equivalent to setting ∆R(Zold, ˆZold) Hence, the above minimax program (5) is revised as a constrained minimax game, which we refer to as incremental closed-loop transcription (i-CTRL). The objective of this game is identical to the standard multi-class CTRL objective (4), but includes just one additional constraint:\n\n. = (cid:80)t\n\nf (x,θ)\n\n−−−−−−→ ˆZold.\n\ng(z,η)\n\n−−−−−−→ ˆXold\n\nmin η\n\nmax θ\n\n∆R(Z) + ∆R( ˆZ) + ∆R(Znew, ˆZnew)\n\nsubject to ∆R(Zold, ˆZold) = 0.\n\n(7)\n\nIn practice, the constrained minimax program can be solved by alternating minimization and maximization between the encoder f (·, θ) and decoder g(·, η) as follows:\n\n∆R(Z) + ∆R( ˆZ) + λ · ∆R(Znew, ˆZnew) − γ · ∆R(Zold, ˆZold),\n\n∆R(Z) + ∆R( ˆZ) + λ · ∆R(Znew, ˆZnew) + γ · ∆R(Zold, ˆZold);\n\n(8)\n\n(9)\n\nmax θ\n\nmin η\n\nwhere the constraint ∆R(Zold, ˆZold) = 0 in (7) has been converted (and relaxed) to a Lagrangian term with a corresponding coefficient γ and sign. We additionally introduce another coefficient λ for weighting the rate reduction term associated with the new data. More algorithmic details are given in Appendix A.\n\nJointly optimal memory via incremental reviewing. As we will see, the above constrained minimax program can already achieve state of the art performance for incremental learning. Nevertheless, developing an optimal memory for all classes cannot rely on graceful forgetting alone. Even for humans, if an object class is learned only once, we should expect the learned memory to fade as we continue to learn new others, unless the memory can be consolidated by reviewing old object classes.\n\nTo emulate this phase of memory forming, after incrementally learning a whole dataset, we may go back to review all classes again, one class at a time. We refer to going through all classes once as one reviewing “cycle”.3 If needed, multiple reviewing cycles can be conducted. It is quite expected that reviewing can improve the learned (LDR) memory. But somewhat surprisingly, the closed-loop framework allows us to review even in a “class-unsupervised” manner: when reviewing data of an old class say Xj, the system does not need the class label and can simply treat Xj as a new class Xnew. That is, the system optimizes the same constrained mini-max program (7) without any modification; after the system is optimized, one can identify the newly learned subspace spanned by Znew, and use it to replace or merge with the old subspace Sj. As our experiments show, such an class-unsupervised incremental review process can gradually improve both discriminative and generative performance of the LDR memory, eventually converging to that of a jointly-learned memory.\n\n4 EXPERIMENTAL VERIFICATION\n\nWe now evaluate the performance of our method and compare with several representative incremental learning methods. Since different methods have very different requirements in data, networks, and computation, it is impossible to compare all in the same experimental conditions. For a fair comparison, we do not compare with methods that deviate significantly from the IL setting that our method is designed for: as examples, this excludes methods that rely on feature extracting networks pre-trained on additional datasets such as FearNet (Kemker & Kanan, 2018) or methods that expand the feature space such as DER (Yan et al., 2021). Instead, we demonstrate the effectiveness of our method by choosing baselines that can be trained using similar fixed network architectures without any pretraining. Nevertheless, most existing incremental learning methods that we can compare against still rely on a buffer that acts as a memory of past tasks. They require significantly more storage than i-CTRL, which only needs to track first and second moments of each seen class (see Appendix A for algorithm implementation details).\n\n3to distinguish from the term “epoch” used in the conventional joint learning setting.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nCategory\n\nMethod\n\nMNIST\n\nCIFAR-10\n\n10-splits Last Avg\n\n5-splits Last Avg\n\n10-splits Last Avg\n\n5-splits Last Avg\n\nRegularization\n\nLwF (Li & Hoiem, 2017) SI (Zenke et al., 2017)\n\nArchitecture\n\nReduNet (Wu et al., 2021)\n\n- -\n\n-\n\n- -\n\n-\n\n0.196 0.455 0.193 0.461\n\n0.961 0.982\n\n- -\n\n-\n\n- -\n\n-\n\n0.196 0.440 0.196 0.441\n\n0.539 0.645\n\nExemplar\n\nGenerative\n\niCaRL (Rebuffi et al., 2017) 0.322 0.588 0.725 0.803 0.212 0.431 0.487 0.632 A-GEM (Chaudhry et al., 2018) 0.382 0.574 0.597 0.764 0.115 0.293 0.204 0.473 0.662 CLR-ER (Arani et al., 2022)\n\n0.895\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\nDGMw (Ostapenko et al., 2019) EEC (Ayub & Wagner, 2020) EECS (Ayub & Wagner, 2020) i-CTRL (ours)\n\n- -\n-\n\n0.965 0.978 0.963\n\n- -\n-\n\n- -\n-\n\n- -\n-\n\n0.562 0.669 0.619\n\n- -\n-\n\n- -\n-\n\n0.975 0.989 0.978 0.990 0.599 0.727 0.627 0.723\n\nTable 1: Comparison on MNIST and CIFAR-10.\n\n4.1 DATASETS, NETWORKS, AND SETTINGS\n\nWe conduct experiments on the following datasets: MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky et al., 2014), and ImageNet-50 (Deng et al., 2009). All experiments are conducted for the more challenging class-IL setting. For both MNIST and CIFAR-10, the 10 classes are split into 5 tasks with 2 classes each or 10 tasks with 1 class each; for ImageNet-50, the 50 classes are split into 5 tasks of 10 classes each. For MNIST and CIFAR-10 experiments, for the encoder f and decoder g, we adopt a very simple network architecture modified from DCGAN (Radford et al., 2016), which is merely a four-layer convolutional network. For ImageNet-50, we use a deeper version of DCGAN which contains only 40% of the standard ResNet-18 structure.\n\n4.2 COMPARISON OF CLASSIFICATION PERFORMANCE\n\nWe first evaluate the memory learned (without review) for classification. Similar to (Yu et al., 2020), we adopt a simple nearest subspace algorithm for classification, with details given in Appendix B. Unlike other generative memory-based incremental learning approaches, note that we do not need to train a separate network for classification.\n\nMNIST and CIFAR-10. Table 1 compares i-CTRL against representative SOTA generative-replay incremental learning methods in different categories on the MNIST and CIFAR-10 datasets. We report results for both 10-splits and 5-splits, in terms of both last accuracy and average accuracy (following definition in iCaRL (Rebuffi et al., 2017)). Results on regularization-based and exemplar-based methods are obtained by adopting the same benchmark and training protocol as in (Buzzega et al., 2020). All other results are based on publicly available code released by the original authors. We reproduce all exemplar-based methods with a buffer size no larger than 2000 raw images or features for MNIST and CIFAR-10, which is a conventional buffer size used in other methods. Compared to these methods, i-CTRL uses a single smaller network and only needs to store means and covariances.\n\nFor a simple dataset like MNIST, we observe that i-CTRL outperforms all current SOTA on both settings. In the 10-task scenario, it is 1% higher on average accuracy, despite the SOTA is already as high as 97.8%. In general incremental learning methods achieve better performance for smaller number of steps. Here, the 10-step version even outperforms all other methods in the 5-step setting.\n\nFor CIFAR-10, we observe more significant improvement. For incremental learning with more tasks (i.e splits = 10), to our best knowledge, EEC/EECS (Ayub & Wagner, 2020) represents the current SOTA. Despite the fact that EEC uses multiple autoencoders and requires a significantly larger amount of memory (see Table 6 in the appendix), we see that i-CTRL outperforms EEC by more than 3%. For a more fair comparison, we have also included results of EECS from the same paper, which aggregate all autoencoders into one. i-CTRL outperforms EECS by nearly 10%. We also observe that i-CTRL with 10 steps is again better than all current methods that learn with 5 steps, in terms of both last and average accuracy.\n\niCaRL-S\n\nEEIL-S\n\nDGMw\n\nEEC\n\nEECS\n\ni-CTRL\n\n0.290\n\n0.118\n\n0.178\n\n0.352\n\n0.309\n\n0.458\n\nTable 2: Comparison on ImageNet-50. The results of other methods are as reported in the EEC paper.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Block diagonal structure of |Z ⊤Z| in the feature space for MNIST (left) and CIFAR-10 (right).\n\nImageNet-50. We also evaluate and compare our method on ImageNet-50, which has a larger number of classes and higher resolution inputs. Training details can be found in Appendix B. We adopt results from (Ayub & Wagner, 2020) and report average accuracy across five splits. From the table, we observe the same trend, a very significant improvement from the previous methods by almost 10%! Since ImageNet-50 is a more complicated dataset, we can even further improve the performance using augmentation. More discussion can be found in Appendix 11.\n\n4.3 GENERATIVE PROPERTIES OF THE LEARNED LDR MEMORY\n\nUnlike some of the incremental methods above, which learn models only for classification purposes (as those in Table 1), the i-CTRL model is both discriminative and generative. In this section, we show the generative abilities of our model and visualize the structure of the learned memory. We also include standard metrics for analysis in Appendix H.\n\nVisualizing auto-encoding properties. We begin by qualitatively visualizing some representative images X and the corresponding replayed ˆX on MNIST and CIFAR-10. The model is learned incrementally with the datasets split into 5 tasks. Results are shown in Figure 3, where we observe that the reconstructed ˆX preserves the main visual characteristics of X including shapes and textures. For a simpler dataset like MNIST, the replayed ˆX are almost identical to the input X! This is rather remarkable given: (1) our method does not explicitly enforce ˆx ≈ x for individual samples as most autoencoding methods do, and (2) after having incrementally learned all classes, the generator has not forgotten how to generate digits learned earlier, such as 0, 1, 2. For a more complex dataset like CIFAR-10, we also demonstrates good visual quality, faithfully capturing the essence of each image.\n\nPrincipal subspaces of the learned features. Most generative memory-based methods utilize autoencoders, VAEs, or GANs for replay purposes. The structure or distribution of the learned features Zj for each class is unclear in the feature space. The features Zj of the i-CTRL memory, on the other hand, have a clear linear structure. Figure 2 visualizes correlations among all learned features |Z⊤Z|, in which we observe clear block-diagonal patterns for both datasets.4 This indicates the features for different classes Zj indeed lie on subspaces that are incoherent from one another. Hence, features of each class can be well modeled as a principal subspace in the feature space. A more precise measure of affinity among those subspaces can be found in Appendix D.\n\nReplay images of samples from principal components. Since features of each class can be modeled as a principal subspace, we further visualize the individual principal components within each of those subspaces. Figure 4 shows the images replayed from sampled features along the top-4 principal components for different classes, on MNIST and CIFAR-10 respectively. Each row represents samples along one principal component and they clearly show similar visual characteristics but distinctively different from those in other rows. We see that the model remembers different poses of ‘4’ after having learned all remaining classes. For CIFAR-10, the incrementally learned memory remembers representative poses and shapes of horses and ships.\n\n4.4 EFFECTIVENESS OF INCREMENTAL REVIEWING\n\nWe verify how the incrementally learned LDR memory can be further consolidated with an unsupervised incremental reviewing phase described at the end of Section 3.3. Experiments are conducted on CIFAR-10, with 10 steps.\n\n4Notice that these patterns closely resemble the similarity matrix of response profiles of object categories\n\nfrom different areas of the inferotemporal cortex, as shown in Extended DataFig.3 of (Bao et al., 2020).\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a) MNIST X\n\n(d) CIFAR-10 ˆX Figure 3: Visualizing the auto-encoding property of the learned i-CTRL ( ˆX = g ◦ f (X)).\n\n(b) MNIST ˆX\n\n(c) CIFAR-10 X\n\n(a) sampled ˆxold of ‘4’ (b) sampled ˆxold of ‘7’ (c) sampled ˆxold of\n\n‘horse’\n\n(d) sampled ˆxold of ‘ship’\n\nFigure 4: Visualization of 5 reconstructed ˆx = g(z) from z’s with the closest distance to (top-4) principal components of learned features for MNIST (class ‘4’ and class ‘7’) and CIFAR-10 (class ‘horse’ and ‘ship’).\n\nImproving discriminativeness of the memory. In the reviewing process, all the parameters in the training are the same as incremental learning Table 3 shows how the overall accuracy improves steadily after each cycle of incrementally reviewing the entire dataset. After a few (here 8) cycles, the accuracy approaches the same as that from learning all classes together via Closed-Loop Transcription in a joint fashion (last column). This shows that the reviewing process indeed has the potential to learn a better representation for all classes of data, despite the review process is still trained incrementally.\n\n# Rev. cycles\n\n0\n\n2\n\n4\n\n6\n\n8\n\nJL\n\nAccuracy\n\n0.599\n\n0.626\n\n0.642\n\n0.650\n\n0.658\n\n0.655\n\nTable 3: The overall test accuracies after different numbers of review cycles on CIFAR-10.\n\nImproving generative quality of the memory. Figure 5 left shows replayed images of the first class ‘airplane’ at the end of incremental learning of all ten classes, sampled along the top-3 principal components – every two rows (16 images) are along one principal direction. Their visual quality remains very decent – observed almost no forgetting. The right figure shows replayed images after reviewing the first class once. We notice a significant improvement in visual quality after the reviewing, and principal components of the features in the subspace start to correspond to distinctively different visual attributes within the same class.\n\n5 CONCLUSION\n\nFigure 5: Visualization of replayed images ˆxold of class 1-‘airplane’ in CIFAR-10, before (left) and after (right) one reviewing cycle.\n\nThis work provides a simple and unifying framework that can incrementally learn a both discriminative and generative memory for multiple classes of objects. By combining the advantages of a closedloop transcription system and the simple linear structures of a learned LDR memory, our method outperforms prior work and proves, arguably for the first time, that both stability and plasticity can be achieved with only a fixed-sized neural system and a single unifying learning objective. The simplicity of this new framework suggests that its performance, efficiency and scalability can be significantly improved in future extensions. In particular, we believe that this framework can be extended to the fully unsupervised or self-supervised settings, and both its discriminative and generative properties can be further improved.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nAll authors agree and will adhere to the conference’s Code of Ethics. We do not anticipate any potential ethics issues regarding the research conducted in this work.\n\nREPRODUCIBILITY STATEMENT\n\nSettings and implementation details of network architectures, optimization methods, and some common hyper-parameters are described in the Appendix B. We will also make our source code available upon request by the reviewers or the area chairs.\n\nACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING\n\nYi Ma acknowledges support from ONR grants N00014-20-1-2002 and N00014-22-1-2102, the joint Simons Foundation-NSF DMS grant #2031899, as well as partial support from Berkeley FHL Vive Center for Enhanced Reality and Berkeley Center for Augmented Cognition, Tsinghua-Berkeley Shenzhen Institute (TBSI) Research Fund, and Berkeley AI Research (BAIR).\n\nREFERENCES\n\nJason M. Allred and Kaushik Roy. Controlled forgetting: Targeted stimulation and dopaminergic plasticity modulation for unsupervised lifelong learning in spiking neural networks. Frontiers in Neuroscience, 14, 2020. ISSN 1662-453X. doi: 10.3389/fnins.2020.00007.\n\nElahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learning fast, learning slow: A general continual learning method based on complementary learning system. arXiv preprint arXiv:2201.12604, 2022.\n\nAli Ayub and Alan Wagner. EEC: Learning to encode and regenerate images for continual learning.\n\nIn International Conference on Learning Representations, 2020.\n\nJihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. Rainbow memory: Continual learning with a memory of diverse samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8218–8227, 2021.\n\nPinglei Bao, Liang She, Mason McGill, and Doris Y. Tsao. A map of object space in primate\n\ninferotemporal cortex. Nature, 583:103–108, 2020.\n\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In 34th Conference on Neural Information Processing Systems (NeurIPS 2020), 2020.\n\nKwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. ReduNet: A white-box deep network from the principle of maximizing rate reduction. CoRR, abs/2105.10446, 2021. URL https://arxiv.org/abs/2105.10446.\n\nLe Chang and Doris Tsao. The code for facial identity in the primate brain. Cell, 169:1013–1028.e14,\n\n06 2017. doi: 10.1016/j.cell.2017.05.011.\n\nArslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient\n\nlifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018.\n\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019a.\n\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019b.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nXili Dai, Shengbang Tong, Mingyang Li, Ziyang Wu, Michael Psenka, Kwan Ho Ryan Chan, Pengyuan Zhai, Yaodong Yu, Xiaojun Yuan, Heung-Yeung Shum, et al. Ctrl: Closed-loop transcription to an ldr via minimaxing rate reduction. Entropy, 24(4):456, 2022.\n\nXili Dai, Ke Chen, Shengbang Tong, Jingyuan Zhang, Xingjian Gao, Mingyang Li, Druv Pai, Yuexiang Zhai, XIaojun Yuan, Heung-Yeung Shum, et al. Closed-loop transcription via convolutional sparse coding. arXiv preprint arXiv:2302.09347, 2023.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nTianjiao Ding, Shengbang Tong, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, and Benjamin D Haeffele.\n\nUnsupervised manifold linearizing and clustering. arXiv preprint arXiv:2301.01805, 2023.\n\nStephen Grossberg. Competitive learning: From interactive activation to adaptive resonance. Cogn.\n\nSci., 11:23–63, 1987.\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved\n\ntraining of Wasserstein GANs. arXiv preprint arXiv:1704.00028, 2017.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\nSheena A. Josselyn and Susumu Tonegawa. Memory engrams: Recalling the past and imagining the\n\nfuture. Science, 367, 2020.\n\nRonald Kemker and Christopher Kanan. FearNet: Brain-inspired model for incremental learning. In\n\nInternational Conference on Learning Representations, 2018.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\nCiteseer, 2009.\n\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset. online: http://www. cs.\n\ntoronto. edu/kriz/cifar. html, 55, 2014.\n\nYann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis\n\nand machine intelligence, 40(12):2935–2947, 2017.\n\nYi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data via\n\nlossy data coding and compression. PAMI, 2007.\n\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109–165. Elsevier, 1989.\n\nOleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. Learning to remember: A synaptic plasticity driven framework for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11321–11329, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAmeya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions our progress in continual learning. In European conference on computer vision, pp. 524–540. Springer, 2020.\n\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\n\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2016.\n\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. iCaRL: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 2001–2010, 2017.\n\nAnthony V. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connect. Sci., 7:123–146,\n\n1995.\n\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.\n\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative\n\nreplay. arXiv preprint arXiv:1705.08690, 2017.\n\nMahdi Soltanolkotabi, Ehsan Elhamifar, and Emmanuel J Candes. Robust subspace clustering. The\n\nAnnals of Statistics, 42(2):669–699, 2014.\n\nShengbang Tong, Xili Dai, Yubei Chen, Mingyang Li, Zengyi Li, Brent Yi, Yann LeCun, and Yi Ma. Unsupervised learning of structured representations via closed-loop transcription. arXiv preprint arXiv:2210.16782, 2022.\n\nGido M Ven, Hava T Siegelmann, Andreas S Tolias, et al. Brain-inspired replay for continual learning\n\nwith artificial neural networks. Nature Communications, 11(1):1–14, 2020.\n\nChenshen Wu, Luis Herranz, Xialei Liu, Joost van de Weijer, Bogdan Raducanu, et al. Memory replay GANs: Learning to generate new categories without forgetting. Advances in Neural Information Processing Systems, 31:5962–5972, 2018.\n\nZiyang Wu, Christina Baek, Chong You, and Yi Ma. Incremental learning via rate reduction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1125–1133, 2021.\n\nShipeng Yan, Jiangwei Xie, and Xuming He. DER: Dynamically expandable representation for class\n\nincremental learning. arXiv preprint arXiv:2103.16788, 2021.\n\nJaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically\n\nexpandable networks. arXiv preprint arXiv:1708.01547, 2017.\n\nYaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discriminative representations via the principle of maximal coding rate reduction. In Advances in neural information processing systems, 2020.\n\nFriedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.\n\nIn International Conference on Machine Learning, pp. 3987–3995. PMLR, 2017.\n\nGuanyu Zhou, Kihyuk Sohn, and Honglak Lee. Online incremental feature learning with denoising\n\nautoencoders. In Artificial intelligence and statistics, pp. 1453–1461. PMLR, 2012.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA ALGORITHM OUTLINE\n\nFor simplicity of presentation, the main body of this paper has described incremental learning with each incremental task containing one new class of data. In general, however, each incremental task may contain a finite C new classes. In this section, we detail the algorithms associated with i-CTRL in this more general setting.\n\nSuppose we divide the overall task of learning multiple classes of data D into a stream of smaller tasks D1, D2, . . . , Dt, . . . , DT , where each task consists of labeled data Dt = {X t, Y t} from C classes, i.e, X t = {X t\n\nC}. The overall i-CTRL process is summarized in Algorithm 3\n\n1, . . . , X t\n\nWe begin by training the model on the first task D1, optimized via the original objective function (4). We then use FORMING MEMORY MEAN AND COVARIANCE 1 to find M1, the means and covariances of the representations of classes in the first task. When learning a new task Dt, we first sample Zold using MEMORY SAMPLING 2. We then take (X t, Y t) from Dt, and calculate X t → Zt → ˆX t → ˆZt using f (·, θ), g(·, η) to obtain Zt and ˆZt. We next compute Zold → ˆXold → ˆZold using f (·, θ), g(·, η). So far, we get Z = [Zt, Zold] and ˆZ = [ ˆZt, ˆZold]. The encoder updates θ by optimizing the objective (8):\n\n∆R(Z) + ∆R( ˆZ) + λ∆R(Zt, ˆZt) − γ∆R(Zold, ˆZold).\n\nmax θ\n\nThe decoder updates η via optimizing the objective (9):\n\n∆R(Z) + ∆R( ˆZ) + λ∆R(Zt, ˆZt) + γ∆R(Zold, ˆZold).\n\nmin η\n\nWe optimize these objectives until the parameters converge. After the training session ends, we calculate M t of this learn task using FORMING MEMORY MEAN AND COVARIANCE 1 The process of training a new task is repeated until all tasks are learned.\n\nAlgorithm 1 FORMING MEMORY MEAN AND COVARIANCE(Zt, k, r)\n\nRequire: C classes of features Zt = [Zt\n\nC] of the entire t-th task, t ∈ [1, . . . , T ]. The parameters k and r, which correspond to top-k features on each of top-r eigenvectors which we will sample on each class;\n\n2, . . . , Zt\n\n1, Zt\n\n1: for j = 1, 2, . . . , C do 2:\n\nCalculate the top-r eigenvectors V j of Zt\n\nj. V j = [v1, . . . , vr] where vn means the n-th\n\neigenvector;\n\nfor i = 1, 2, . . . , r do\n\n3: 4:\n\ni Zt j;\n\nCalculate projection distance d = v⊤ Choose the top-k features from Zt Calculate mean μi and covariance Σi based on the set Si;\n\nend for Obtain memory mean and covariance of j-th class Bj\n\n5: 6: 7: 8: 9: end for 10: Memory of mean and covariance set for t-th task Mt . Ensure: Mt\n\n= [B1, . . . , BC].\n\nj based on the distance d to form the set Si;\n\n. = [(μ1, Σ1), . . . , (μr, Σr)];\n\nB IMPLEMENTATION DETAILS\n\nA simple network architecture. Tables 4 and 5 give details of the network architecture for the decoder and the encoder networks used for experiments reported in Section 4. All α values in Leaky-ReLU (i.e. lReLU) of the encoder are set to 0.2. We set (nz = 128 and nc = 1) for MNIST, (nz = 128 and nc = 3) for CIFAR-10 and CIFAR-100, (nz = 256 and nc = 3) for ImageNet-50. For ImagetNet-50, we added 2 down sample and up sample layer in f and g respectively to match the resolution of ImageNet-50. The details of architecture are given in Appendix B. The dimension d of the feature space is set accordingly for different datasets, d =128 for MNIST and CIFAR-10, d =256 for ImageNet-50. More details about the algorithmic settings and ablation studies are given in the Appendix.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2 MEMORY SAMPLING(M1, . . ., Mt, k, r, C)\n\nRequire: A set of Memory M1, . . . Mt, where Mi\n\n. = [B1, . . . , BC], k, r and C, which is the\n\nnumber of classes in each task; Initialize an empty Zold;\n\n2: for i = 1, . . . , t do\n\nfor j = 1, . . . , C do\n\nget Bj from Mi, Bj For each direction l ∈ r, sample k number of samples from distribution N (μl, Σl), add\n\n. = [(μ1, Σ1), . . . , (μr, Σr)];\n\nthem to Zold; end for\n\n4:\n\n6:\n\nend for Ensure: Zold\n\nAlgorithm 3 i-CTRL\n\nRequire: A stream of tasks D1, D2, . . . , DT , where Di = {X i, Y i}; A pre-trained encoder f (·, θ)\n\nand decoder g(·, η) on D1, k and r; Calculate Z1 via f (X 1, θ);\n\n2: Find M1 by FORMING MEMORY MEAN AND COVARIANCE(Z1, k, r);\n\nfor t = 2, . . . , T do\n\nSample Zold = MEMORY SAMPLING(M1, . . ., Mt−1, k, r, C); while not converged do\n\nDraw samples in (X t, Y t) from the t-th task Dt; Compute expressions: X t → Zt → ˆX t → ˆZt; Replay the old memory Zold → ˆXold → ˆZold; Z = [Zt, Zold]; ˆZ = [ ˆZt, ˆZold]; Update θ via the optimization objective (8); Update η via the optimization objective (9);\n\nend while Calculate Zt via f (X t, θ); Find Mt by FORMING MEMORY MEAN AND COVARIANCE(Zt, k, r);\n\n4:\n\n6:\n\n8:\n\n10:\n\n12:\n\n14:\n\nend for\n\nEnsure: f (·, θ) and g(·, η)\n\nOptimization settings. For all experiments, we use Adam (Kingma & Ba, 2014) as our optimizer, with hyperparameters β1 = 0.5, β2 = 0.999. Learning rate is set to be 0.0001. We choose ε2 = 1.0, γ = 1, and λ = 10 for both equation (8) and (9) in all experiments. For MNIST, CIFAR-10 and CIFAR-100, each task is trained for 120 epochs; For ImageNet-50, the first task D1 is trained for 500 epochs with constraint on augmentation used in (Chen et al., 2020) and 150 epochs for rest incremental 4 tasks using the normal i-CTRL objective 7. All experiments are conducted with 1 or 2 RTX 3090 GPUs.\n\nPrototype settings As we use prototype sampling in this method, so the storage becomes almost trivial. For MNIST, we choose r = 6, k = 10. For CIFAR-10, we choose r = 12, k = 20. For ImageNet-50, we us r = 10, k = 15. For CIFAR-100, we us r = 10, k = 20.\n\nz ∈ R1×1×nz 4 × 4, stride=1, pad=0 deconv. BN 256 ReLU 4 × 4, stride=2, pad=1 deconv. BN 128 ReLU 4 × 4, stride=2, pad=1 deconv. BN 64 ReLU 4 × 4, stride=2, pad=1 deconv. 1 Tanh\n\nTable 4: Network architecture of the decoder g(·, η).\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nImage x ∈ R32×32×nc 4 × 4, stride=2, pad=1 conv 64 lReLU 4 × 4, stride=2, pad=1 conv. BN 128 lReLU 4 × 4, stride=2, pad=1 conv. BN 256 lReLU 4 × 4, stride=1, pad=0 conv nz\n\nTable 5: Network architecture of the encoder f (·, θ).\n\nFigure 6: Affinity between memory subspaces within CIFAR-10.\n\nA simple nearest subspace classifier. Similar to (Dai et al., 2022) and (Yu et al., 2020), we adopt a very simple nearest subspace algorithm to evaluate how discriminative our learned features are for classification. Suppose Zj are the learned features of the j-th class. Let μj ∈ Rd be its mean and Uj ∈ Rd×rj be the first rj principal components for Zj, where rj is the estimated dimension of class j. For a test data x′, its feature z′ is given by f (x′, θ). Then, its class label can be predicted by j′ = arg minj∈{1,...,k} ∥(I − UjU ⊤ 2. It is especially noteworthy that our method does not need to train a separate deep neural network for classification whereas most other methods do.\n\nj )(z′ − μj)∥2\n\nC RESOURCE COMPARISON DETAILS\n\nIn Table 6 of the appendix, both i-CTRL and EEC methods are tested on CIFAR-10. Under joint learning, CTRL follows the setting in Appendix A.4 of (Dai et al., 2022), but we adopt the architectures of the encoder and decoder detailed in Table 5 and Table 4 respectively. The training batch size is 1600 over 1400 epochs because the generative CTRL model is more challenging to train than the simple classifier network that EEC uses in the joint learning setting; EEC uses the ResNet architecture from (Gulrajani et al., 2017) for the classifier, with training batch size and training epochs of 128 and 100 respectively.\n\nD AFFINITY BETWEEN LEARNED SUBSPACES\n\nAs we see in Figure 2, the learned features of different classes are highly incoherent and their correlations form a block-diagonal pattern. We here conduct more quantitative analysis of the affinity among subspaces learned for different classes. The analysis is done on features learned for CIFAR-10 using 10 splits with 2000 features. For two subspaces S and S ′ of dimension d and d’ , we follow the definition of normalized affinity in (Soltanolkotabi et al., 2014):\n\naff(S, S ′)\n\n. =\n\n(cid:115)\n\n(cid:80)d∗d′ i\n\ncos2 θi\n\nd ∗ d′\n\n.\n\n(10)\n\nWe calculate the aff(S, S ′) through ∥U ⊤U ′∥F where U /U ′ is the normalized column space of features Z/Z′ that can be obtained by SVD.\n\nThe affinity measures the angle between two subspaces. The larger the value, the smaller the angle. As shown in Figure 6, we see that similar classes have higher affinities. For example, 8-ship and 9-trucks have higher affinity in the figure, whereas 6-frogs has a much lower affinity than these two classes. This suggests that the affinity score of these subspaces captures similarity in visual attributes between different classes.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nResource\n\nJL\n\nIL\n\nDiff\n\ni-CTRL Model Size Train Time (ours) Model Size Train Time\n\nEEC\n\n2 M 15 hours 1 M\n\n2 M 1.5 hours 10 M\n\n0.6 hours ≥4 hours\n\nsame 10x faster 10x larger 7x slower\n\nTable 6: The resource comparison on the joint learning (JL) and incremental learning (IL) of different methods. Both methods are tested on CIFAR-10 and the details of the comparison setting can be found in Appendix C.\n\nE INCREMENTAL LEARNING VERSUS JOINT LEARNING.\n\nOne main benefit of incremental learning is to learn one class (or one small task) at a time. So it should result in less storage and computation than jointly learning. Table 6 shows this is indeed the case for our method: IL on CIFAR-10 is 10 times faster than JL.5 However, this is often not the case for many existing incremental methods such as EEC (Ayub & Wagner, 2020), the current SOTA in generative memory-based methods. Not only does its incremental mode require a much larger model size (than its joint mode and ours6), it also takes significantly (7 times) longer to train.\n\nF ABLATION STUDIES\n\nWe conduct all ablation studies under the setting of CIFAR-10 split into 5 tasks with feature size of 2000, and default values of k = 20, r = 12, λ = 10, and γ = 1. We use the average incremental accuracy as a measure for these studies.\n\nF.1\n\nIMPACT OF CHOICE OF OPTIMIZATION PARAMETERS\n\nParameter m and r for memory sampling. Here, we verify the impact of the memory size of Algorithm 1 on the performance of our method. The feature size is determined by two hyperparameters r, which is the number of the PCA directions and m, which is the number of sampled features around each principal direction. The value of r varies from 10 to 14, and the value of m varies from 20 to 40. Table 7 reports the results of the average incremental accuracy. From the table, we observe that as long as the selection of m and r are in a reasonable range, the overall performance is stable.\n\nm=20 m=30 m=40\n\nr=10 r=12 r=14\n\n0.713 0.719 0.718\n\n0.720 0.727 0.721\n\n0.728 0.725 0.725\n\nTable 7: Ablation study on varying m and r in PROTOTYPESAMPLING, in terms of the average incremental accuracy.\n\nHyperparameter λ and γ in the learning objective. λ and γ are two important hyperparameters in the objective functions for both (8) and (9). Here, we want to justify our selection of λ and γ and demonstrate the stability of our method to their choices. We analyze the sensitivity of the performance to the λ and γ respectively. In Table 8, we set γ = 1 and change the value of λ from 0.1 to 50. The results indicate the accuracy becomes low only when λ are chosen to be extreme (e.g 0.1, 1, 50). We then change the value of γ in a large range from 0.01 to 100 with λ fixed at 10. Results in Table 9 indicate that the accuracy starts to drop when γ is larger than 10. Hence, in all our experiments reported in Section 4, we set λ = 10 and γ = 1 for simplicity.\n\n5Note in our method, both JL and IL optimize on the same network. The JL mode is trained on all ten classes together, hence it normally takes more epochs to converge and longer time to train. But the IL mode converges much faster, as it should have.\n\n6For EEC, since its classifier and generators are separated, under the JL setting, it only needs a 8-layers convolutional network to train a classifier for all classes. In the incremental mode, it requires multiple generative models. Note that our JL model is also a generative model hence requires more time to train as well.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nλ\n\n0.1\n\n1\n\n10\n\n20\n\n50\n\nAccuracy\n\n0.592\n\n0.620\n\n0.712\n\n0.701\n\n0.691\n\nTable 8: Ablation study on varying λ in terms of the average incremental accuracy.\n\nγ\n\n0.01\n\n0.1\n\n1\n\n10\n\n100\n\nAccuracy\n\n0.713\n\n0.716\n\n0.712\n\n0.700\n\n0.655\n\nTable 9: Ablation study on varying γ in terms of the average incremental accuracy.\n\nF.2 SENSITIVITY TO CHOICE OF RANDOM SEED\n\nIt is known that some incremental learning methods such as (Kirkpatrick et al., 2017) can be sensitive to random seeds. We report in Table 10 the average incremental accuracy of i-CTRL with different random seeds (conducted on CIFAR-10 split into 10 tasks, with a feature size 2000). As we can see, the choice of random seed has very little effect on the performance of i-CTRL.\n\nRandom Seed\n\n1\n\n5\n\n10\n\n15\n\n100\n\nAverage Accuracy Last Accuracy\n\n0.720 0.594\n\n0.720 0.592\n\n0.720 0.593\n\n0.720 0.594\n\n0.721 0.594\n\nTable 10: Ablation study on varying random seeds.\n\nF.3 THE SIGNIFICANCE OF AUGMENTATION IN TRAINING IMAGENET-50\n\nIn this section, we study the the impact of using augmentation from Chen et al. (2020) to train the first task has on our method. From Tab 11, we conclude that augmentation did help the model the learn better representation. Even without it, it has shown that our method still outperform the current generative-replay based method by more than 10%. Through this experiment, we think that add augmentation may be the solution for generative-replay based methods to scale up to even larger datasets. We leave that to future study.\n\nF.4 THE SIGNIFICANCE OF CONSTRAINT IN MINMAX OPTIMIZATION\n\nHere, we want to justify the significance of this constraint in the context of incremental learning. We report in table 12 the performance of i-CTRL with and without constraint. Without the constraint, i-CTRL fall into the victim of catastrophic forgetting. We can conclude that constraint has played a significant role in the success of our method.\n\nG COMPARISON WITH MORE BASELINES\n\nDue the limitation of space in main paragraph, we present here a table with more comparison with other methods.\n\nFrom the table, we see that comparing to the previous methods especially exemplar-based methods, our method still leads them numerically. We have also conducted on experiments on CIFAR-100. On more complex data such as CIFAR-100(Krizhevsky et al., 2014), it is also observed in Tab 14 that i-CTRL has led the current exemplar-based methods. It is noteworthy that there is no generativereplayed based methods in the table. Since it hard for many of the current generative-replay based methods to scale up to more complex setting.\n\nH QUANTITATIVE EVALUATION OF LEARNED GENERATOR\n\nIn this section, we use FID score (Heusel et al., 2017) and Inception Scores (IS) (Salimans et al., 2016) to quantitatively measure the performance of our incrementally learned generator. As there\n\n17\n\nPublished as a conference paper at ICLR 2023\n\niCaRL-S\n\nEEIL-S\n\nDGMw\n\n0.290\n\n0.118\n\n0.178\n\nEEC\n\n0.352\n\nEECS\n\n0.309\n\ni-CTRL(without aug)\n\ni-CTRL(with aug)\n\n0.458\n\n0.523\n\nTable 11: Comparison on ImageNet-50. The results of other methods are as reported in the EEC paper.\n\nWith Constraint Without Constraint\n\nAverage Accuracy Last Accuracy\n\n0.723 0.627\n\n0.380 0.223\n\nTable 12: Ablation study on the important of constraint in (7)\n\nMethod\n\nRegularization LwF (Li & Hoiem, 2017) SI (Zenke et al., 2017)\n\nArchitecture ReduNet (Wu et al., 2021)\n\nExemplar iCaRL (Rebuffi et al., 2017) A-GEM (Chaudhry et al., 2018) CLR-ER (Arani et al., 2022) ER-Reservoir (Chaudhry et al., 2019b) GDumb (Prabhu et al., 2020) Rainbow Memory (Bang et al., 2021) DER++ (Buzzega et al., 2020)\n\nGenerative Memory DGMw (Ostapenko et al., 2019) EEC (Ayub & Wagner, 2020) EECS (Ayub & Wagner, 2020) i-CTRL (ours)\n\nMNIST\n\nCIFAR-10\n\n10-splits Last Avg\n\n5-splits Last Avg\n\n10-splits Last Avg\n\n5-splits Last Avg\n\n- -\n\n-\n\n- -\n\n-\n\n0.196 0.455 0.193 0.461\n\n0.961 0.982\n\n- -\n\n-\n\n- -\n\n-\n\n0.196 0.440 0.196 0.441\n\n0.539 0.645\n\n0.322 0.588 0.725 0.803 0.212 0.431 0.487 0.632 0.382 0.574 0.597 0.764 0.115 0.293 0.204 0.473 0.662 0.685 0.618 -\n0.648\n\n- -\n- 0.927 -\n\n0.895 -\n0.919 -\n-\n\n- -\n- -\n-\n\n- -\n- -\n-\n\n- -\n- -\n-\n\n- -\n- -\n-\n\n- -\n- -\n-\n\n- -\n-\n\n0.965 0.978 0.963\n\n- -\n-\n\n- -\n-\n\n- -\n-\n\n0.562 0.669 0.619\n\n- -\n-\n\n- -\n-\n\n0.975 0.989 0.978 0.990 0.599 0.727 0.627 0.723\n\nTable 13: Comparison on MNIST and CIFAR-10.\n\nMethod\n\nLast Accuracy 5-split\n\nLast Accuracy 10-split\n\nLast Accuracy 20-split\n\nLwF(Li & Hoiem, 2017) iCaRL(Rebuffi et al., 2017) GDUMB(Prabhu et al., 2020) Rainbow Memory(Bang et al., 2021) i-CTRL\n\n- -\n- 0.414 0.435\n\n0.252 0.346 -\n- 0.392\n\n0.141 -\n0.241 -\n0.378\n\nTable 14: Comparison on CIFAR-100.\n\nDCGAN (Radford et al., 2016) i-CTRL (ours)\n\nIS↑\n\n6.6 6.5\n\nFID↓\n\n37.4 36.7\n\nTable 15: Comparison IS and FID on CIFAR-10\n\nexist very few generative-based or replay-based incremental methods offer a quantitative result for us to compare. We here compare with DCGAN (Radford et al., 2016), which is the backbone of our method, trained jointly for all classes. Based on table15, it is seen that our method has competitive FID and IS score comparing to DCGAN. Hence, despite trained incrementally, our method still generates high-quality images.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nI\n\nI-CTRL IN EXTREME SETTING\n\nIn this section, we conduct some ablation study of i-CTRL implemented in extreme settings.\n\nI.1\n\nIMBALANCED DATASETS\n\nOften in real life, the data we encounter are not perfectly balanced. To testify our model’s performance in this situation, we conduct experiment on imbalance-CIFAR-10. In this subsection, CIFAR-10 is split into 5 tasks, with task 2 and task 4 having only half of the original data. We call this setting imbalance-CIFAR-10. i-CTRL is trained with parameters same as section B. From table 16, we observe that imbalance CIFAR-10 has very little impact on the performance of our method.\n\nCIFAR-10 (balance)\n\nCIFAR-10 (imbalance)\n\nLast Accuracy\n\nAverage Accuracy\n\nLast Accuracy\n\nAverage Accuracy\n\ni-CTRL\n\n0.627\n\n0.727\n\n0.623\n\n0.723\n\nTable 16: Comparison of i-CTRL performance on CIFAR-10 and imbalance-CIFAR-10\n\nI.2 SMALL SUBSET OF DATA\n\nAnother interesting extreme scenario to examine would be small subset of dataset. Again, we may not get large number of dataset for us to train every time. To testify i-CTRL’s performance in this scenario, we design small-CIFAR-10. For example, We denote CIFAR-10(50%), meaning we have deleted 50% of data from every class in CIFAR-10. We run i-CTRL on CIFAR-10(20%), CIFAR-10(40%), CIFAR-10(60%), CIFAR-10(80%), CIFAR-10(100%) without tuning any parameter. From Table 17, we observe that smaller daatset will have impact on the performance of our method. If the portion is larger than 20%, the impact is relatively small. When the size of data reduces to only 20%, the impact becomes larger. Nonetheless, since CIFAR-10(20%) is nearly a new dataset, we can reduce the impact by tuning parameters. In Table 18, we present results of i-CTRLon CIFAR-10(20%) after\n\nCIFAR-10 (balance)\n\nLast Accuracy\n\nAverage Accuracy\n\nCIFAR-10(20%) CIFAR-10(40%) CIFAR-10(60%) CIFAR-10(80%) CIFAR-10(100%)\n\n0.476 0.575 0.589 0.615 0.627\n\n0.625 0.691 0.709 0.721 0.727\n\nTable 17: Comparison of i-CTRL performance on different scales of CIFAR-10\n\ntuning the hyperparamter λ and epochs. Since CIFAR-10(20%) is a very small dataset, we reduce the number of λ and epochs to avoid forgetting previous learned classes. We observe that smaller λ and epochs can greatly improve the performance of i-CTRL on very small subset of data like CIFAR-10(20%).\n\nCIFAR-10 (balance)\n\nLast Accuracy\n\nAverage Accuracy\n\nCIFAR-10(20%), λ = 10, epochs=120 CIFAR-10(20%), λ = 5, epochs=60\n\n0.476 0.541\n\n0.625 0.671\n\nTable 18: Comparison of i-CTRL performance on CIFAR-10(20%) with different hyperparameters\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nJ USING AFFINITY TO MEASURE THE PERFORMANCE\n\nIn this section, we discuss if affinity between the memory subspaced learned can be used to evaluate the performance of our method. Following the setting of subset in CIFAR-10, we visualize the affinity in Fig 7. From the figure, we see that as the subset of CIFAR-10 becomes smaller, the affinity learned by i-CTRL becomes more distant. It can be used as a sign for unsatisdying performance because ideally, we would want the affinity between similar classes (truck and car) to be close. If the affinity graph shows that the model does not capture this kind of relationship, it is a sign that the overall performance could be worse.\n\n(a) Affinity between memory subspace within CIFAR(100%)\n\n(b) Affinity between memory subspace within CIFAR(80%)\n\n(c) Affinity between memory subspace within CIFAR(60%)\n\n(d) Affinity between memory subspace within CIFAR(40%)\n\nFigure 7: Visualization of the affinity between memory subspace under different subset of CIFAR-10\n\n(e) Affinity between memory subspace within CIFAR(20%)\n\n20",
    "reference": "# Summary Of The Paper\n\nThe paper provides a framework for incrementally learning a generative and discriminative model for multi-class problems using a Linear Discriminative Representation (LDR) and a coding rate reduction objective. The closed-loop transcription framework (CTRL) is formulated as a minimax game wherein the encoder tries to maximize the rate reduction objective and the decoder minimizes it instead. The \"distance\" between Gaussian ensemble subspaces is estimated.\n\n# Strength And Weaknesses\n\nStrength\n- Paper has extensive theoretical and empirical analysis\n- It is written coherently and studies related work carefully\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- Well written\n- Quality: High\n- Reproducibility -- Not discussed in the paper\n\n# Summary Of The Review\n\nThe paper provides a framework for incrementally learning a generative and discriminative model for multi-class problems using a Linear Discriminative Representation (LDR) and a coding rate reduction objective. \n\nThe closed-loop transcription framework (CTRL) is formulated as a minimax game wherein the encoder tries to maximize the rate reduction objective and the decoder minimizes it instead. The \"distance\" between Gaussian ensemble subspaces is estimated.\n\nThe authors state \"To this end, we simply sample m representative prototype features on the subspace along its top r principal components, and denote these features as Zj,old\" -> This step clearly depends on how well the principal components capture the variance of the data in the first place and affects the samples collected using the mean and variance of Zj,old.\na. How robust is the scheme with imbalanced classes?\nb. If incremental learning process learns from a small subset of data, this could affect the principal components (and hence their variances). How is this dealt with?\nc. Are there empirical results to justify how a change in percentage of variance of principal components affects the overall performance of the algorithm -- in terms of performance metrics, imbalanced multiple classes and relevant details? \nd. Can affinity based subspaces be used to measure the effect of percentage of variance captured by principal components?\n\nAppendix A contains a lot of crucial algorithmic details and the authors should consider incorporating those in the main paper.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nCONTINUOUS PSEUDO-LABELING FROM THE START\n\nDan Berrebbi∗\n\nCarnegie Mellon University dberrebb@andrew.cmu.edu\n\nRonan Collobert, Samy Bengio, Navdeep Jaitly, Tatiana Likhomanenko Apple {collobert,bengio,njaitly,antares}@apple.com\n\nABSTRACT\n\nSelf-training (ST), or pseudo-labeling has sparked significant interest in the automatic speech recognition (ASR) community recently because of its success in harnessing unlabeled data. Unlike prior semi-supervised learning approaches that relied on iteratively regenerating pseudo-labels (PLs) from a trained model and using them to train a new model, recent state-of-the-art methods perform ‘continuous training’ where PLs are generated using a very recent version of the model being trained. Nevertheless, these approaches still rely on bootstrapping the ST using an initial supervised learning phase where the model is trained on labeled data alone. We believe this has the potential for over-fitting to the labeled dataset in low resource settings and that ST from the start of training should reduce over-fitting. In this paper we show how we can do this by dynamically controlling the evolution of PLs during the training process in ASR. To the best of our knowledge, this is the first study that shows the feasibility of generating PLs from the very start of the training. We are able to achieve this using two techniques that avoid instabilities which lead to degenerate models that do not generalize. Firstly, we control the evolution of PLs through a curriculum that uses the online changes in PLs to control the membership of the cache of PLs and improve generalization. Secondly, we find that by sampling transcriptions from the predictive distribution, rather than only using the best transcription, we can stabilize training further. With these techniques, our ST models match prior works without an external language model.\n\n1\n\nINTRODUCTION\n\nThe past few years have witnessed a growth in methods that leverage large amount of unlabeled data in domains such as speech, vision and language to produce state-of-the-art results, e.g. Baevski et al. (2020; 2022); Chen et al. (2020a); Caron et al. (2021); He et al. (2022); Cai et al. (2022); Brown et al. (2020); Ramesh et al. (2021). Amongst the techniques that have made this possible are self-supervised learning (SSL) and self-training (ST) (Scudder, 1965; Lee, 2013). While SSL is typically used in unsupervised settings, ST is applied in supervised settings where labeled data can be extended with unlabeled data that is labeled using a prior model, a process known as pseudo-labeling (PL). These techniques can reduce the burden of expensive labeling processes while successfully train data hungry models such as transformers using large quantities of unlabeled data.\n\nCurrent state-of-the-art SSL methods in speech (Baevski et al., 2020; Hsu et al., 2021; Baevski et al., 2022; Chung et al., 2021) are typically trained in two phases. First, the models are pre-trained on thousands of hours of unlabeled speech, and then they are further adapted by fine-tuning on the actual task of automatic speech recognition (ASR) using a smaller supervised set. However, because the pre-training (PT) phase is task agnostic, self-supervision can under-perform on a specific downstream task (Talnikar et al., 2021; Dery et al., 2022). Further, SSL pre-training leads to a more complicated pipeline involving multiple phases. By contrast, ST algorithms also use unlabeled data but do not require phases of training with different objectives that makes the training pipeline simpler.\n\nIn this paper, we focus on recent ST algorithms that perform ‘continuous training’ of a single model. In contrast to earlier ST training methods that iterate between generating PLs over the entire unlabeled dataset and training a model (teacher-student) (Synnaeve et al., 2020; Kahn et al., 2020a; Zhang\n\n∗Work done during internship at Apple.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Continuous ST (using slimIPL) with different pre-training steps (M ) using a 10h dataset reveals that more pre-training can lead to worse results (we show word error rate, WER, on dev-clean).\n\nM\n\n10k WER 14.3\n\n20k 17.1\n\n40k 22.9\n\net al., 2020; Park et al., 2020), here pseudo-labels (PLs) are generated online with a very recent version of the model (Xu et al., 2020; Likhomanenko et al., 2021a; Manohar et al., 2021; Higuchi et al., 2021; 2022a;b) and training is faster and more resource-efficient. One of the main challenges for continuous ST is training stability (Likhomanenko et al., 2021a; Higuchi et al., 2021; 2022b; Cai et al., 2022). While these prior works use various techniques for stabilization, one common ingredient is that models are initially trained on labeled data for M steps. slimIPL (Likhomanenko et al., 2021a) showed robustness to M in some settings, but a well-established recipe does not seem to exist for the case of small labeled datasets (aka. the low resource setting). Indeed, we find that more pre-training steps, compared to what was shown previously in Likhomanenko et al. (2021a), can lead to worse results (see Table 1). We hypothesize that this is due to over-fitting to the labeled set early in training in low resource settings and in this paper we try to improve results by doing ST without any pre-training (i.e. M = 0). However, in our experiments, off-the-shelf slimIPL diverges early in training in low resource settings, so we developed methods to address this problem which we summarize here:\n\n• We show that sampling transcriptions from the output distribution instead of using the best tran-\n\nscription makes ST robust and stable, especially when no pre-training is performed.\n\n• We propose a new curriculum for controlling the PL distribution during training. The curriculum uses the Levenshtein distance between PLs at different time steps to control how PLs are updated, and how unsupervised examples are chosen for training.\n\nFor the first time, with these strategies we show that continuous PL can be done from the very start of the training matching prior works without an external language model.\n\n2 EXPERIMENTAL SETUP AND RELATED METHODS\n\nData All our experiments are performed using the LibriSpeech dataset (Panayotov et al., 2015). We use the train-clean-360 and train-other-500 regular subsets as unlabeled data, and consider either a subset of 10h randomly drawn from train-clean-100, or the full 100h set (train-clean-100) as labeled data. Comparisons with existing works are also provided using the 10h subset from Libri-Light (Kahn et al., 2020b)1. In addition, we evaluate the final configuration of our methods on the Common Voice dataset Ardila et al. (2020) for French language where we sample 10h and 100h from the train set to use as labeled data and the rest as unlabeled data (see Appendix A.3).\n\nAcoustic model Following Likhomanenko et al. (2021a), models are trained with English letters token set2, the Connectionist Temporal Classification Graves et al. (2006) (CTC) loss, identical SpecAugment (Park et al., 2019) parameters, and Adagrad optimizer (Duchi et al., 2011). The acoustic model is the same transformer architecture that was introduced in slimIPL, except that we encode positions with either absolute sinusoidal positional embedding (Vaswani et al., 2017) or the recently proposed CAPE (Likhomanenko et al., 2021b) instead of relative positional embedding (Shaw et al., 2018). This allows us to speed up training (by 2-3x) and decrease the memory footprint significantly. All models are trained on 8 GPUs for a maximum of 500k updates. We use either a static batch of 8 examples or a dynamic batch that packs ∼ 290s of audio per GPU.\n\nContinuous pseudo-labeling (PL) in ASR Let L = {xi, yi} and U = {xj} be the labeled and unlabeled datasets, respectively. We consider a semi-supervised PL approach where an acoustic model\n\n1Libri-Light 10h subset contains only 24 speakers drawn from the whole LibriSpeech (from both clean and noisy subsets). To keep our experiments consistent, and also to assess domain transfer to the unlabeled noisy subsets, we reconstructed the 10h set from the train-clean-100, sampling randomly from the speakers and retaining the original 250 speakers from this subset.\n\n226 letters augmented with the apostrophe and a word boundary token.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1: slimIPL algorithm and our proposed changes (red → deletion and green → addition) Inputs: labeled L = {xi, yi} and unlabeled U = {xj} data, ̃x = augmentation(x), initialization θ0, cache C = {}, learning rate ηk, losses LL and LU , parameters M , NL, NU , pout and C PL function P L(x; θ, τ ) = P L(x; θ) defined via Eq. (2) PL function P L(x; θ, τ ) defined via sampling with temperature τ (see Section 4.2)\n\nResult: Acoustic model A(x; θ)\n\n1 // Initial pre-training (PT) phase : 2 Train A on (x, y) ∈ L for M steps:\n\ntrain only on labeled samples\n\nθk+1 = θk − ηk∇LL(A( ̃x; θk), y), k = 1, M\n\n3 4 Decrease model’s A(x; θ) dropout 5 // Train on labeled data while filling the cache 6 for k = M + 1, M + C do\n\nFor random x ∈ U generate ˆy = P L(Ainf erence(x; θk), τ ) and C ← C (cid:83){(x, ˆy)} θk+1 = θk − ηk∇LL(A( ̃x; θk), y), (x, y) ∈ L τ = max(0, 1 − k/K)\n\n10 // Continuous pseudo-labeling training with the cache 11 repeat 12\n\nif rand(0, 1) < NL/(NL + NU ) then\n\nDraw (x, y) ∈ L and θk+1 = θk − ηk∇LL(A( ̃x; θk), y)\n\nelse\n\nDraw b = (x, y) ∈ C and θk+1 = θk − ηk∇LU (A( ̃x; θk), y) ˆy = P L(Ainf erence(x; θk, τ )) // Compute current model state PL pout = T ER(y, ˆy) if k < K else pout = 1 // Compute dynamic pout if rand(0, 1) < pout then\n\nFor random x′ ∈ U generate ˆy′ = P L(Ainf erence(x′; θk, τ )) and C ← C \\ b (cid:83){(x′, ˆy′)}\n\nelse\n\nC ← C \\ b (cid:83){(x, y)} // Same sample and PLs back into the cache C ← C \\ b (cid:83){(x, ˆy)} // Same sample but new PLs back into the cache\n\n7\n\n8\n\n9\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\nk ← k + 1 τ = max(0, 1 − k/K)\n\n24 25 until convergence or maximum iterations are reached\n\nA(x; θ) with model parameters θ is continuously trained on a combination of L and a pseudo-labelled set derived from U . The model is trained by minimizing a loss\n\nL(θ) = LL(θ) + λLU (θ) ,\n\n(1)\n\nwhere λ ∈ R+ is a tunable hyper-parameter controlling the importance of unlabeled data. The loss for labeled data is defined as LL(θ) = −E(x,y)∼L log pθ(y|x), where pθ(y|x) is the conditional distribution defined by A(x; θ). The loss for unlabeled data is defined as LU (θ) = −Ex∼U log pθ( ˆy|x), where ˆy is the PL transcription for a data point generated using the model being trained. Specifically,\n\nˆy = argmax\n\ny\n\nlog pθ(y|x).\n\n(2)\n\nContinuous PL keeps updating the pseudo-labels via Eq. (2), as the model trains. This procedure is prone to divergence, as without any constraint PLs can self-reinforce rapidly to a trivial distribution.\n\nMethods to stabilize training Several approaches have been proposed to stabilize continuous PL. A pre-training phase (PT) on the supervised data only (optimizing the loss LL(θ) for M updates) is always a key component. For e.g. in Chen et al. (2020b) PT is performed until full convergence. Another technique is the use of an exponential moving average (EMA) of the acoustic model to generate the pseudo-labels in Eq. (2) (Likhomanenko et al., 2021a; Manohar et al., 2021; Higuchi et al., 2021; 2022b; Zhang et al., 2022).\n\nslimIPL To avoid the significant memory footprint of EMA Likhomanenko et al. (2021a) introduced slimIPL, which uses a dynamic cache instead of the EMA to stabilize the training. The cache maintains a set of unlabeled samples U C (with fixed size |U C| = C) and their associated PLs, generated by previous model states. After the pre-training phase, slimIPL minimizes the loss in Eq. (1), using the\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Comparison between slimIPL (left) and how we control the cache by using PL evolution (right). The constant pout from slimIPL now is dynamic and computed based on the PL evolution.\n\nunlabeled subset U C, which is itself updated as training goes: at each iteration, slimIPL removes a sample from the cache with probability pout, replacing it with a new one x ∈ U along with its generated PL. More details about slimIPL can be found in Algorithm 1 and in Figure 1.\n\nPLs selection Pseudo-labels selection can help to achieve better convergence by filtering out noisy PLs that prevent model from faster training. There are also a lot of efforts on the curriculum pseudo-labeled data selection: e.g. confidence filtering (Zhang et al., 2021) or assigning weights to pseudo-labeled data based on the model uncertainty estimation (Huang et al., 2022). One of the recent works (Zhang et al., 2022) in ASR proposes to use PLs curriculum filtering based on the Levenshtein distance between PLs generated for original and weakly augmented inputs. Later we will see that our idea is based solely on the PL evolution rather than on input augmentation.\n\nRelation to consistency regularization Popular consistency regularization methods (Sajjadi et al., 2016; Laine & Aila, 2016; Sohn et al., 2020; Berthelot et al., 2019) leverage the idea that a model should output the same distribution for an unlabeled example even after it has been augmented. In this paper we take inspiration from these works but we focus on an orthogonal view: we consider distances between model outputs at different time steps. Also, contrary to consistency regularization, we do not use this distance as an objective function to train a model but as a data selection criterion.\n\nHyper-parameter selection All hyper-parameters and model selections are performed using devclean and dev-other sets. We report final token (TER) or word (WER) error rates on test-clean and test-other sets. In all experiments, we only tune (C, pout, M , λ) from the training procedure while everything else is kept as in the slimIPL paper. By default we use C = 1000, λ = 1, M = 0. In most experiments we try 3 different random seeds and report metric mean and standard deviation.\n\n3 MOTIVATION\n\nExisting continuous PL approaches rely on a two-step process: first pre-training (PT) on labeled data only, then continue the model training with both labeled and unlabeled data. While PT is known to be critical for the stability of continuous PL, we are interested in this work to find ways to remove the PT phase to simplify the whole procedure, and possibly improve the overall performance, both in terms of convergence speed and final WER.\n\nPT improves the final WER Initial experiments with slimIPL, Table 2, show that with even its simple cache strategy used to stabilize training, PT helps improving the final WER. It is not surprising, as without PT, PLs are of poor quality (> 90% WER) at the beginning of training as the model mostly produces random outputs. Careful tuning of the number of PT steps is however important, especially in low-resource supervised settings, as shown in Table 1.\n\nCaching as a replacement for PT Vanilla continuous PL is very similar to slimIPL with pout = 1 (see Section 2). With the caching strategy, slimIPL picks unlabeled samples (and their associated PLs) from a cache when needed, and immediately replaces these examples with new unlabeled samples (and their new PLs). This allows to always use PLs generated from a previous version of the trained model, while efficiently computing these PLs. While being simple, we observe in Table 2 that this approach is enough to stabilize continuous PL, assuming a large enough cache.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Continuous PL w/ and w/o pre-training (PT) phase for slimIPL. ‘DV’ states for divergence.\n\nData\n\npout\n\ndev-clean WER\n\ndev-other WER\n\nw/o PT w/ PT w/o PT w/ PT\n\n10h 10h\n\n100h 100h\n\n1 0.1\n\n1 0.1\n\n23.31.7 DV\n\n4.50.1 DV\n\n13.8 11.4\n\n3.1 3.6\n\n32.11.3 DV\n\n10.60.3 DV\n\n17.5 14.0\n\n8.1 7.5\n\nWhen to update the PLs from the cached samples is critical In slimIPL (Algorithm 1), each sample (x, ˆy) in the cache C at step k′ has a PL ˆy= P L(A(x; θk)) that was generated with the model θk at step k < k′ when it was added to the cache. After using the sample (x, ˆy) for training, slimIPL adds it back into the cache with probability 1 − pout, leaving its corresponding PLs unchanged. We found however that updating PLs with the current model state ˆy = P L(A(x; θk′ )) improves final WER performance. See Table 3, which compares the original slimIPL strategy (‘old’), with the one where the PLs are updated when a sample has been selected in the cache (‘new’). For that reason, in the following experiments, we will be using ˆy = P L(A(x; θk′ )) as a PL strategy, when keeping a sample back into the cache.\n\nControlling cache contents dynamically can improve WER When the cache is updated less often (pout < 1), we see in Table 2 that one may improve the WER, but then PT is essential to avoid any divergence. In Likhomanenko et al. (2021a), the authors of slimIPL have reported robustness (in terms of test WER) with respect to pout. However, our experiments reported in Table 3 and Figure 3b in Appendix C reveal different learning dynamics for different values of pout: our ablations with specific schedules on the probability pout suggest that models without a PT phase would benefit more from low pout at the beginning of training, which would make training easier initially by letting the model focus on the same examples. In addition, later in training, the training procedure might benefit from high pout, as seeing a wider range of examples may lead to more stability. While we observe significant changes in dynamics with 10h of supervision, with larger labeled set (100h) the different strategies do not make such a huge difference.\n\nThe above observations suggest that by dynamically controlling how the cache evolves we can improve results in limited data settings. One possible way of doing this is by using a strategy that depends on the rate of evolution of PLs in the cache. In the next section we present such a method.\n\nTable 3: Strategies of PLs and cache renewing (w/o PT phase). When pout < 1 and sample goes back into the cache, we compare models using the same PL as it was ˆy = P L(A(x; θk)) (old) or the newly re-generated PL ˆy = P L(A(x; θk′ )) (new). For cache renewing, we compare static pout and simple scheduling with pout being different before and after 130k steps.\n\npout\n\nPLs\n\n1 0.1 0.1 1 → 0.1 1 → 0.1 0.1 → 1 0.1 → 1\n\n- old new old new old new\n\n10h, WER\n\n100h, WER\n\ndev-clean\n\ndev-other\n\ndev-clean\n\ndev-other\n\n23.31.7 DV 15.30.6 23.01.1 24.81.4 DV 13.70.8\n\n32.11.3 DV 25.40.4 32.10.4 36.10.5 DV 20.70.8\n\n4.50.1 DV 4.50.1 4.50.1 4.40.0 DV 4.80.1\n\n10.60.3 DV 10.40.1 11.00.0 10.20.1 DV 11.30.1\n\n4 METHODS OF STABLE TRAINING\n\n4.1 CONTROLLING CACHE BY USING PL EVOLUTION\n\nLet’s consider an example x ∈ U to be put into the cache at training step k, see Figure 1. Its PL is defined as ˆy = P L(A(x; θk) = P L(x; k). At step k′ > k, this example (x, ˆy) is selected\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nfrom the cache and the model is updated to θk′+1 using the gradient of the loss. Unlike slimIPL, the probability of removing the example from the cache is not constant anymore. Instead, pout is dynamically computed at step k′ for sample x that is selected from the cache as follows:\n\npout(x; k) = f [ρ(P L(x; k), P L(x; k′))]\n\n(3)\n\nwhere ρ is the Levenshtein edit-distance, and f the function that encapsulates how evolution in PLs should determine the rate at which examples are removed from the cache. Using different choices of f we can consider different ways of actively controlling the cache (and hence the model training) using the evolution of the PLs. We consider simple functions f : x (cid:55)→ x and f : x (cid:55)→ 1 − x. The first function encourages the cache to maintain examples whose PLs are stable, which might lead to slower learning. The second function maintains examples whose PLs are changing fast which might lead to faster learning but less stable behavior.\n\nNote that while we explained the method using a single example x from the unlabelled set, in practice we operate the algorithm on a batch level, and the statistics are computed over a full batch of examples, which are all put back in the cache or removed together.\n\n4.2 ALIGNMENT SAMPLING\n\nAs discussed in Section 3 training instability shows up as the acoustic model distribution A(x; θk) collapses to a degenerate distribution, e.g. empty transcriptions. While a cache and/or an exponential moving average model can stabilize training, they do not resolve the issue entirely, especially in the low data regime, with no pre-training, and the model often collapses to a degenerate solution. Even our proposed method above (see Section 4.1) is susceptible to this collapse on the 10h dataset.\n\nIn order to overcome the collapse issue and still make use of unlabeled data as early as possible, we propose to sample targets from the token distribution for every frame (Likhomanenko et al., 2022). We believe that sampling PLs around the most probable hard labels is an effective stabilization technique which works by adding appropriate noise to the targets: it is a way to enforce a lower bound on the entropy of the label distribution which mitigates the collapse issue3. As the model is learnt with CTC, every per frame predicted distribution pt θ(w|x), w ∈ w for token set w and time frame t is considered to be independent. Thus, for every audio frame, we sample a token label wt ∼ pt θ(w|x). A temperature τ is introduced to smooth the distribution obtained from the model. After the frame level labels are sampled, they are transformed into the transcription by deduplicating consecutive repetitions of the same output token, and removing the left over auxiliary blank tokens4.\n\nSampling Temperature Schedule As τ → ∞ the distribution over tokens pt θ(w|x, τ ) approaches the uniform one, the PL sequence of tokens becomes purely random. On the other hand, as τ → 0 the distribution approaches the argmax function which is equivalent to the hard labels in slimIPL. We find that τ > 1 performs poorly. With τ = 1 a model avoids divergence at the beginning of training but end up with worse final performance than hard PLs (τ = 0): this happens mostly because of larger noise presence due to sampling (quality of PLs is observed being worse). Lower temperatures, e.g. τ = 0.1, give indistinguishable results from hard PLs (τ = 0). These observations suggest that decreasing temperature as training proceeds can stabilize training at the beginning and benefit from less noisy PLs in the end. We found that simple linear schedule for τ from 1 to 0.1 works well.\n\nThe summary of our proposed methods on top of slimIPL is given in Algorithm 1.\n\n5 RESULTS\n\n5.1 DYNAMIC SELECTION FOR PSEUDO-LABELED SAMPLES\n\nIn Table 4 we show results from using only the method introduced in Section 4.1. We experiment with token error rate (TER) distance computed between PLs on an entire batch and the two functions as discussed above. For both settings of 100h and 10h of supervised data the proposed dynamic\n\n3With no regularization (cache, and/or alignment sampling), the PL procedure often collapses to generating just blanks very quickly (Likhomanenko et al., 2021a) – it is biased, has 100% WER, but has no variance. Alignment sampling avoids this by generating noisy targets that have variance.\n\n4E.g. alignment ‘cc###aatttt#’ will be transformed into ‘cat’, where # is a CTC blank token.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: WER on dev-clean and dev-other for different cache selection methods (p). We use either pout = p or a strategy where pout = p for the first 130K steps, switching to pout = 1 afterwards, as shown in Section 3. Alignment sampling from Section 4.2 is not used.\n\n10h\n\n100h\n\np\n\n0.1 T ER[P L(k), P L(k′)] 1 − T ER[P L(k), P L(k′)]\n\npout = p\n\npout : p → 1\n\npout = p\n\npout : p → 1\n\nclean\n\n15.30.6 14.70.5 16.00.4\n\nother\n\n25.40.4 24.60.3 26.50.8\n\nclean\n\n13.70.8 13.21.6 17.81.2\n\nother\n\n20.70.8 19.11.6 30.42.3\n\nclean\n\n4.50.1 4.60.1 4.40.1\n\nother\n\n10.60.3 10.50.2 11.10.5\n\nclean\n\n4.80.1 4.40.1 4.50.0\n\nother\n\n11.30.1 10.10.2 10.50.5\n\n(a) pout=WER[P L(x; k),P L(x; k′)] per batch along the training.\n\n(b) Correlation between WER[P L(x; k),golden] and WER[P L(x; k),P L(x; k′)].\n\nFigure 2: Analysis of our curriculum PLs selection criteria. WER is given in scale of (0, 1).\n\nselection decreases WER over the baseline with constant pout. This behavior also holds when we switch from the dynamic strategy of Eq. (3) to a constant pout = 1 after 130K steps of training. For a 10h of labeled data setting the improvement over the baseline is larger and reaches around 1% absolute. The function f : x (cid:55)→ 1 − x performs worse than f : x (cid:55)→ x and hence we use this setting for subsequent experiments.\n\nOur analysis of dynamic probabilities pout from Table 4 shows: (i) T ER[P L(x; k), P L(x; k′)] is close to 100% at the beginning of training (the model changes very fast), and quickly decreases (less than 10% after 30k steps); (ii) over training different batches get different values of pout, see Figure 2a; (iii) proposed distance correlates with the oracle WER computed between PLs and ground truth labels for x ∈ U , see Figure 2b. The latter demonstrates that our choice of dynamic selection encapsulates knowledge about actual PLs quality.\n\n5.2 ALIGNMENT SAMPLING\n\nIn Table 5 we compare results for models trained with hard PLs (τ = 0), models trained with alignment sampling and constant τ > 0, and models trained with a linear schedule of τ from 1 to 0.1 (1 → 0.1), as described in Section 4.2. For this section we do not use dynamic control of the cache as introduced in Section 4.1. Here we highlight some observations. Firstly, alignment sampling with high τ reduces the number of diverged models (either τ = 1 or τ = 1 → 0.1). Secondly, constant temperature over the training does not provide best results: τ = 0.1 is similar to the baseline while τ = 1 is even worse; the difference is more pronounced for the 10h of supervision with pout = 0.1 → 1. Besides, WER we also report TER to highlight that sampling with τ = 1 leads to a notable CER degradation. However, scheduled τ = 1 → 0.1 provides both stable training (no divergence is observed in experiments) and similar or significantly better TER/WER (1.3%-2.7%) over the baseline. The best results are obtained with pout = 0.1 → 1 showing compatibility of sampling and dynamic probability.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: TER and WER on dev-other for sampling PLs with different temperature τ , including linear schedule of τ in case of constant pout (left parts) or alternated one (right parts), see Section 3. ‘DV’ denotes the number of diverged models over 3 runs with random seeds. PL evolution via dynamic cache probability from Section 4.1 is not used.\n\n10h\n\n100h\n\nτ\n\npout = 0.1\n\npout : 0.1 → 1\n\npout = 0.1\n\npout : 0.1 → 1\n\nTER\n\nWER\n\n#DV\n\nTER\n\nWER\n\n#DV TER WER\n\n#DV TER WER\n\n#DV\n\n0 (argmax) 0.1 1\n1 → 0.1\n\n10.10.2 10.91.0 11.41.9 9.71.2\n\n25.40.4 26.12.0 26.54.8 22.71.4\n\n0 0\n0 0\n\n7.80.7 8.40.1 12.10.6 7.50.6\n\n21.40.3 21.21.9 31.21.9 20.11.2\n\n1 0\n0 0\n\n3.90.1 3.90.1 4.20.2 3.80.1\n\n10.40.1 10.30.1 10.40.3 10.20.1\n\n1 1\n0 0\n\n3.70.1 3.60.1 3.70.1 3.70.1\n\n10.20.1 10.30.1 10.40.2 10.10.1\n\n1 2\n0 0\n\nTable 6: Combination of our methods (Sections 4.1 and 4.2) for hard labels (left part) and for sampling (right part) with a linear schedule on the temperature. ‘DV’ states for models divergence, ‘old’ denotes usage of P L(x; k), while ‘new’ denotes the use of P L(x; k′). We compare different pout (all with using ‘new’): scheduled pout = 0.1 → 1 (switching at 130K steps), ρ = T ER and scheduled ρ = T ER → 1 (switching at 130K steps). The WER on dev-other is reported. All results are reported across 3 runs with different seeds.\n\nData λ\n\nArgmax\n\nold\n\nnew\n\n0.1 → 1\n\nρ\n\n10h 10h\n\n1 DV 25.40.4 5 DV\n\nDV\n\n100h 100h\n\n1 DV 10.60.3 5 DV\n\nDV\n\n21.40.3 DV\n\n11.30.1 DV\n\nSampling\n\nnew\n\n0.1 → 1\n\nρ\n\nρ → 1\n\n19.11.6 DV\n\nold\n\nDV DV\n\n24.60.3 DV\n\n22.71.4 DV\n\n10.50.2 DV\n\n10.10.2 DV\n\n13.50.3 DV\n\n10.20.1 DV\n\n20.11.2 DV\n\n10.10.1 DV\n\n21.21.8 14.70.4\n\n10.50.2 10.70.3\n\nρ → 1\n\n20.71.9 13.30.2\n\n10.20.2 10.00.3\n\n5.3 COMBINING METHODS FOR BEST RESULTS\n\nIn this section we highlight the results that can be achieved by combining together all the methods reported above in Sections 4.1 and 4.2. In Table 6 we give a detailed comparison for both 10h and 100h of supervision. As we have now stable training pipeline from the start (no PT), we also play with a ratio λ (see Eq. (1)) searching it in range [1, 5]. This raises training instability risk while larger proportion of unlabeled data may improve the model according to Likhomanenko et al. (2021a).\n\nFor 10 hours of supervised data the models benefit a lot from the higher λ and become competitive with models trained with PT phase as well as with prior works (Baevski et al., 2020; Likhomanenko et al., 2021a)). Note that combining sampling with dynamic pout based on PLs evolution is necessary to have stable training for λ > 1.\n\nTo have a proper comparison with aforementioned prior works we increase the batch size and use dynamic batching for the best configuration. First, we confirm that both sampling and dynamically controlling the cache give stable training (see e.g. Appendix C Table 13). Second, in Table 75 for 10h/100h setup (λ = 5/λ = 3) our models achieve similar or better results with no PT compared to PT-based models (which are reproductions of slimIPL using the same settings that we use for our method) while matching the prior works.\n\nTo ensure our methods are general enough we probe the final configuration (found for LibriSpeech) on Common Voice, French language data. We use exactly the same models with sinusoidal positional embedding and the same hyper-parameters. The only thing we tune is slimIPL parameter M . Results in Table 8 show that our methods work out of the box: without PT we are able to match slimIPL baseline for 100h of supervision, while we improve results upon slimIPL for low supervision setting of 10h with an average relative WER reduction of 18%.\n\n5As we use different 10h split in this work we also report results for 10h set with 24 speakers from Libri-Light used in prior works. We found that training with no PT is more prone to unstable training for this set, while our method is able to stabilize it and get comparable performance with its baseline counterpart which lags behind the prior works.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 7: Comparison of our best models with prior works for 10h and 100h of supervision. Results are reported across 3 random seeds. For wav2vec 2.0 and slimIPL we report the prior work results and our reproduction following official open-sourced recipes. ‘Posemb’ denotes type of used positional embedding. The 10h set from Libri-Light is marked with ‘*’.\n\nModel\n\nData Posemb\n\nw2v 2.0, Large (Baevski et al., 2020) w2v 2.0, Large, reproduction slimIPL (Likhomanenko et al., 2021a)\n\nslimIPL Ours\n\nslimIPL Ours\n\nw2v 2.0, Large\n\nslimIPL Ours\n\nslimIPL Ours\n\nw2v 2.0, Large (Baevski et al., 2020) slimIPL (Likhomanenko et al., 2021a)\n\nslimIPL Ours\n\nslimIPL Ours\n\n10h∗\n\n10h\n\n100h\n\nconv conv relpos\n\nCAPE CAPE\n\nsinpos sinpos\n\nconv\n\nCAPE CAPE\n\nsinpos sinpos\n\nconv relpos\n\nCAPE CAPE\n\nsinpos sinpos\n\nLower bound, fully supervised\n\n960h CAPE\n\ndev WER\n\ntest WER\n\nclean\n\nother\n\nclean\n\nother\n\n8.1 8.10.3\n\n11.4\n\n14.40.3 15.81.8\n\n32.70.6 20.72.0\n\n12.0 12.90.2 14.0\n\n18.80.4 20.41.6\n\n36.80.3 24.42.0\n\n8.0 8.10.3\n\n11.4\n\n15.10.4 15.91.5\n\n33.70.7 21.42.1\n\n12.1 13.30.3 14.7\n\n19.30.3 20.41.3\n\n37.60.4 24.91.9\n\n7.40.3\n\n12.70.3\n\n7.70.3\n\n13.00.4\n\n10.00.4 8.20.2\n\n22.51.3 8.60.2\n\n15.10.5 13.11.4\n\n28.11.3 13.30.2\n\n9.90.4 8.50.2\n\n22.91.2 8.70.3\n\n15.70.5 13.62.1\n\n29.41.4 13.40.2\n\n4.6 3.7\n\n3.70.1 4.10.1\n\n3.70.1 4.00.1\n\n2.60.1\n\n9.3 7.3\n\n8.00.1 8.40.1\n\n7.80.1 8.10.2\n\n6.90.1\n\n4.7 3.8\n\n3.90.1 4.00.1\n\n3.80.1 4.10.1\n\n2.70.1\n\n9.0 7.5\n\n8.20.1 8.60.2\n\n8.00.1 8.40.2\n\n6.90.1\n\nTable 8: Comparison of fully supervised, slimIPL and our methods on Common Voice French. Results are reported across 6 random seeds. Sinusoidal positional embedding is used for all models.\n\nModel\n\nData\n\nFully supervised slimIPL Ours\n\nFully supervised slimIPL Ours\n\n10h\n\n100h\n\nWER\n\nvalid\n\ntest\n\n59.90.5 29.92.0 24.61.8\n\n17.30.1 12.80.2 13.00.2\n\n62.60.6 31.12.1 26.01.9\n\n19.30.1 14.10.2 14.30.2\n\nFully supervised\n\n540h\n\n10.90.4\n\n12.30.3\n\n6 CONCLUSION\n\nIn this paper we show that we can perform continuous pseudo-labeling from the very start of training and get improved results in low supervision settings. We were able to achieve these results by using alignment sampling and a dynamic cache selection strategy that is based on the evolution of the pseudo-labels during training. Being able to perform pseudo-labeling from the very start further simplifies training, avoiding complicated multi-step pipelines and allows us to focus on a simpler one. Our work also provides avenues for explorations into curriculum strategies for pseudo-labeling and we hope to build upon the ideas and results presented in this paper. In the future we wish to explore the effectiveness of these methods to other settings for ASR such as sequence-to-sequence/transducer models6, out-of-domain unsupervised data, and neural models not based on transformers.\n\n6The proposed dynamic control of the cache does not rely on anything specific to CTC. Alignment sampling should be transferable to Transducer directly, while for sequence-to-sequence we would sample transcription directly from the model.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n7 REPRODUCIBILITY STATEMENT\n\nWe report detailed settings of our experiments which are based on the previously open-sourced recipes for Likhomanenko et al. (2021a) through the paper and also in Appendix A.2 and B. We aim to open source the code of our method and experiments soon.\n\n8 ETHICS\n\nFor this paper we used publicly available datasets. Our goal is to build models that work for low supervision settings and hope this is a positive contribution towards under-represented data sources for ASR. While one can imagine ASR being used for negative purposes, it is our hope that the advantages generated by improving ASR for low-resource settings outweigh its possible negative uses.\n\nACKNOWLEDGMENTS\n\nWe would like to thank Richard He Bai, Jagrit Digani, David Grangier, Loren Lugosch, Yizhe Zhang, and machine learning research teammates for helpful discussions and support throughout the work.\n\nREFERENCES\n\nRosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: A massivelymultilingual speech corpus. In Proceedings of the 12th Language Resources and Evaluation Conference, pp. 4218–4222, 2020.\n\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems, 33, 2020.\n\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555, 2022.\n\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural information processing systems, 32, 2019.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nZhaowei Cai, Avinash Ravichandran, Paolo Favaro, Manchen Wang, Davide Modolo, Rahul Bhotika, Zhuowen Tu, and Stefano Soatto. Semi-supervised vision transformers at scale. arXiv preprint arXiv:2208.05688, 2022.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9650–9660, 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020a.\n\nYang Chen, Weiran Wang, and Chao Wang. Semi-supervised asr by end-to-end self-training. Proc.\n\nInterspeech 2020, pp. 2787–2791, 2020b.\n\nYu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 244–250. IEEE, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nLucio M. Dery, Paul Michel, Ameet Talwalkar, and Graham Neubig. Should we be pre-training? an argument for end-task aware training as an alternative. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=2bO2x8NAIMB.\n\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\n\nstochastic optimization. Journal of machine learning research, 12(Jul):2121–2159, 2011.\n\nAngela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with\n\nstructured dropout. In International Conference on Learning Representations, 2020.\n\nAlex Graves, Santiago Fern ́andez, Faustino Gomez, and J ̈urgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning, pp. 369–376, 2006.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked In Proceedings of the IEEE/CVF Conference on\n\nautoencoders are scalable vision learners. Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nYosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Momentum pseudo-labeling for\n\nsemi-supervised speech recognition. Proc. Interspeech, 2021.\n\nYosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Advancing momentum pseudolabeling with conformer and initialization strategy. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7672–7676. IEEE, 2022a.\n\nYosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Momentum pseudo-labeling: Semi-supervised asr with continuously improving pseudo-labels. IEEE Journal of Selected Topics in Signal Processing, pp. 1–14, 2022b. doi: 10.1109/JSTSP.2022.3195367.\n\nWei-Ning Hsu, Yao-Hung Hubert Tsai, Benjamin Bolte, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: How much can a bad teacher benefit asr pre-training? In ICASSP 20212021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6533–6537. IEEE, 2021.\n\nKexin Huang, Vishnu Sresht, Brajesh Rai, and Mykola Bordyuh. Adaptive pseudo-labeling for quantum calculations, 2022. URL https://openreview.net/forum?id=FFM_oJeqZx.\n\nJacob Kahn, Ann Lee, and Awni Hannun. Self-training for end-to-end speech recognition. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7084–7088. IEEE, 2020a.\n\nJacob Kahn, Morgane Rivi`ere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar ́e, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7669–7673. IEEE, 2020b.\n\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint\n\narXiv:1610.02242, 2016.\n\nDong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, 2013.\n\nTatiana Likhomanenko, Qiantong Xu, Jacob Kahn, Gabriel Synnaeve, and Ronan Collobert. slimipl:\n\nLanguage-model-free iterative pseudo-labeling. Proc. Interspeech, 2021a.\n\nTatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex Rogozhnikov. Cape: Encoding relative positions with continuous augmented positional embeddings. Advances in Neural Information Processing Systems, 34, 2021b.\n\nTatiana Likhomanenko, Ronan Collobert, Navdeep Jaitly, and Samy Bengio. Continuous soft pseudolabeling in asr. In I Can’t Believe It’s Not Better Workshop: Understanding Deep Learning Through Empirical Falsification, NeurIPS, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nVimal Manohar, Tatiana Likhomanenko, Qiantong Xu, Wei-Ning Hsu, Ronan Collobert, Yatharth Saraf, Geoffrey Zweig, and Abdelrahman Mohamed. Kaizen: Continuously improving teacher using exponential moving average for semi-supervised speech recognition. arXiv preprint arXiv:2106.07759, 2021.\n\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5206–5210. IEEE, 2015.\n\nDaniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition. Proc. Interspeech 2019, pp. 2613–2617, 2019.\n\nDaniel S Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, and Quoc V Le. Improved noisy student training for automatic speech recognition. Proc. Interspeech 2020, pp. 2817–2821, 2020.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821–8831. PMLR, 2021.\n\nMehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. Advances in neural information processing systems, 29, 2016.\n\nH Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transactions\n\non Information Theory, 11(3):363–371, 1965.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 464–468, 2018.\n\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in Neural Information Processing Systems, 33, 2020.\n\nGabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, and Ronan Collobert. End-to-end asr: from supervised to semi-supervised learning with modern architectures. In Workshop on Self-supervision in Audio and Speech, ICML, 2020.\n\nChaitanya Talnikar, Tatiana Likhomanenko, Ronan Collobert, and Gabriel Synnaeve. Joint masked cpc and ctc training for asr. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3045–3049. IEEE, 2021.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz In Advances in neural information\n\nKaiser, and Illia Polosukhin. Attention is all you need. processing systems, pp. 5998–6008, 2017.\n\nQiantong Xu, Tatiana Likhomanenko, Jacob Kahn, Awni Hannun, Gabriel Synnaeve, and Ronan Collobert. Iterative pseudo-labeling for speech recognition. Proc. Interspeech 2020, pp. 1006–1010, 2020.\n\nBowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. Advances in Neural Information Processing Systems, 34:18408–18419, 2021.\n\nBowen Zhang, Songjun Cao, Xiaoming Zhang, Yike Zhang, Long Ma, and Takahiro Shinozaki. Censer: Curriculum semi-supervised learning for speech recognition based on self-supervised pre-training. arXiv preprint arXiv:2206.08189, 2022.\n\nYu Zhang, James Qin, Daniel S Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V Le, and Yonghui Wu. Pushing the limits of semi-supervised learning for automatic speech recognition. arXiv preprint arXiv:2010.10504, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA DETAILS ON EXPERIMENTAL SETUP\n\nA.1 SPEAKERS IN LIBRISPEECH\n\nThere is no intersection between speakers in different LibriSpeech train sets as well as in validation / test sets – all speakers are unique and are present in only one of the LibriSpeech sets. To prepare the 10h set we randomly sampled audio per speaker to gather a total 10h of audio.\n\nA.2 ACOUSTIC MODEL TRAINING\n\nWe keep the original 16kHz sampling rate and compute log-mel filterbanks with 80 coefficients for a 25ms sliding window, strided by 10ms which are normalized to zero mean and unit variance per input sequence before feeding into a model.\n\nThroughout the paper we consider transformer-based models with a convolutional frontend to perform the proper striding. The encoder is composed of a 1-D convolution with kernel size 7 and stride 3 followed by 36 4-head Transformer blocks (Vaswani et al., 2017). The self-attention dimension is 768 and the feed-forward network (FFN) dimension is 3072 (with 4 heads) in each transformer block. The output of the encoder is followed by a linear layer to the output classes. We use dropout after the convolution, dropout on the self-attention and on the FFN for all transformer layers, and layer drop (Fan et al., 2020), dropping entire layers at the FFN level.\n\nWe get rid of relative positional embedding (Shaw et al., 2018) and use either sinusoidal one (Vaswani et al., 2017) or recently proposed CAPE embedding (Likhomanenko et al., 2021b) (only global shift of 30s is used): this speeds up training by 2-3x and decreases memory usage.\n\nFor SpecAugment Park et al. (2019) we follow parameters from Likhomanenko et al. (2021a): two frequency masks with frequency mask parameter F = 30, ten time masks with maximum time-mask ratio p = 0.1 and time mask parameter T = 50; time warping is not used.\n\nAll models are trained with CTC loss and Adagrad optimizer with linear warmup period of 64k steps, constant learning rate of 0.03 and step-wise (by 2) learning rate decay at the end of training. All models are trained on tf32 tensor cores of 8 Ampere A100 40GB GPUs for a maximum of 500k updates.\n\nFor slimIPL parameters we use always cache size of 1k. Throughout the paper we vary the proportion λ (by default we use λ = 1 if not stated otherwise) as well as pout. From experiments we observe that it is important to activate SpecAugment later in training (e.g. after 5k training steps) otherwise slimIPL baseline is even more prone to divergence.\n\nA.3 COMMON VOICE EXPERIMENTS\n\nWe use Common Voice data release from 21 July 20217 with French language. In total, there are 543 hours in train, 25.1h in validation and 25.8 in test sets. We randomly sample speakers from the train and take all audio belonging to the same speaker to form a 100h train subset. We end up with 982 speakers and 102h. We further sample speakers from this 100h subset to form a 10h subset: it contains 171 speakers with 11.5h. These 10h and 100h subsets are used as labeled data while the remaining 443h are used as unlabeled data. We normalize transcriptions by lower casing, removing any punctuation tokens except apostrophe, changing all diacritical marks to their corresponding English characters and removing any other non-English characters. Later, we use the same token set as for LibriSpeech.\n\nWe use the same acoustic model as for LibriSpeech experiments with sinusoidal positional embedding as all audios in Common Voice are very short (5.2s±1.5s). For fully supervised models we use dropout 0.5, 0.3 and 0.1 for 10h, 100h and 540h sets correspondingly. For slimIPL we change dropout and layer drop from 0.5 to 0.1 for 10h and from 0.3 to 0.1 for 100h, while for our methods we use dropout and layer drop of 0.1 from the beginning of training. For slimIPL we tune only parameter M for the 10h setting. The rest of parameters are the same as in original slimIPL work (Likhomanenko et al., 2021a): C is 1000 (100), cache probability pout is 0.1, data proportion λ is 10 (3), M is 40k\n\n7https://github.com/common-voice/cv-dataset/blob/main/datasets/\n\ncv-corpus-7.0-2021-07-21.json\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n(20k) for 10h (100h) setting. All models are trained with dynamic batch, same as for LibriSpeech. For our methods we use exactly the same parameters as for LibriSpeech experiments with dynamic batch.\n\nA.4 FULLY SUPERVISED MODELS\n\nTable 9: Fully supervised models for 10h and 100h of LibriSpeech. Results are reported across 3 random seeds. Sinusoidal, CAPE and relative positional embeddings are denoted as ‘sinpos’, ‘CAPE’ and ‘relpos’ correspondingly. The 10h set from Libri-Light is marked with ‘*’.\n\nModel\n\nSup. set\n\nWER\n\nrelpos (Likhomanenko et al., 2021a) CAPE sinpos\n\nrelpos CAPE sinpos\n\nrelpos (Likhomanenko et al., 2021a) CAPE sinpos\n\ndev-clean\n\ndev-other\n\ntest-clean\n\ntest-other\n\n10h∗\n\n10h\n\n100h\n\n31.9 37.10.1 76.00.8\n\n27.70.4 28.20.1 63.41.1\n\n6.2 5.90.1 6.50.3\n\n52.3 58.40.1 87.10.5\n\n48.40.4 48.50.3 78.50.9\n\n16.8 17.90.1 19.10.2\n\n32.6 37.70.3 77.10.7\n\n28.20.3 28.90.1 64.50.9\n\n6.2 6.20.1 7.10.3\n\n52.4 58.40.2 87.20.6\n\n48.80.3 48.90.2 78.91.1\n\n16.8 18.10.1 19.30.2\n\nA.5 SUMMARY OF HYPER-PARAMETERS\n\nHyper-parameter values for both experiments on LibriSpeech and Common Voice are summarized in Tables 11, 12 and 10.\n\nTable 10: Detailed hyper-parameters for the final experiments on Common Voice from Table 8.\n\nParameter\n\nslimIPL (10h)\n\nOur (10h)\n\nM C\npout λ\ndropout/layer drop embedding τ\ntotal batch\n\n40k 1000 0.1 10 0.5→0.1 sinpos 0\ndynamic 290s×8\n\n0k 1000 TER (1 after 130k) 5\n0.1 sinpos τk = max(0.1, 1 − 0.1 ∗ k/130, 000) dynamic 290s×8\n\nslimIPL (100h)\n\n20k 100 0.1 3\n0.3→0.1 sinpos 0\ndynamic 290s×8\n\nOur (100h)\n\n0k 1000 TER (1 after 130k) 3\n0.1 sinpos τk = max(0.1, 1 − 0.1 ∗ k/130, 000) dynamic 290s×8\n\nTable 11: Detailed hyper-parameters for the final experiments on LibriSpeech from Table 7.\n\nParameter\n\nslimIPL (10h)\n\nOur (10h)\n\nslimIPL (100h)\n\nOur (100h)\n\nC λ\ndropout/layerdrop τ\n\n1000 10 0.5→0.1 0\n\n1000 5\n0.1 τk = max(0.1, 1 − 0.1 ∗ k/130, 000)\n\n100 3\n0.3→0.1 0\n\n1000 3\n0.1 τk = max(0.1, 1 − 0.1 ∗ k/130, 000)\n\nembedding M\npout total batch\n\nembedding M\nCAPE is used after pout total batch\n\nsinpos 30k 0.1 dynamic 290s×8\n\nCAPE 50k 0k 0.1 dynamic 290s×8\n\nsinpos 0k TER (1 after 130k) 8x8\n\nCAPE 0k 25k TER (1 after 40k) dynamic 290s×8\n\nsinpos 20k 0.1 dynamic 290s×8\n\nCAPE 20k 0k 0.1 dynamic 290s×8\n\nsinpos 0k TER (1 after 130k) dynamic 290s×8\n\nCAPE 0k 5k TER (1 after 130k) dynamic 290s×8\n\nB WAV2VEC AND SLIMIPL REPRODUCTION\n\nTo reproduce baselines in Table 7 for slimIPL we follow Likhomanenko et al. (2021a) and its published recipe. The only change we do is positional embedding as discussed above and batch size.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTable 12: Detailed hyper-parameters for the final experiments on LibriSpeech from Table 7 for 10h∗ setting.\n\nParameter\n\nC τ\nSpecAugment\n\nembedding M\ndropout/layer drop λ\npout total batch\n\nembedding M\nCAPE is used after λ\ndropout/layerdrop pout total batch\n\nslimIPL (10h∗)\n\nOur (10h∗)\n\n1000 0\nT = 25, 20 time masks\n\n1000 τk = max(0.1, 1 − 0.1 ∗ k/130, 000) T = 50, 10 time masks\n\nsinpos 20k 0.5→0.1 10 0.1 dynamic 290s×8\n\nCAPE 40k 0k 10 0.5→0.1 0.1 dynamic 290s×8\n\nsinpos 0k 0.5 (0.1 after 35k) 1 (5 after 70k) TER (1 after 70k) 8×8\n\nCAPE 0k 70k 1 (5 after 130k) 0.5 (0.1 after 70k) TER (1 after 130k) 8×8\n\nThe rest of the training remains the same. To reproduce wav2vec 2.0 (Baevski et al., 2020) we take open-sourced Large model pre-trained on the full LibriSpeech8 and then perform fine-tuning on our 10h set and the 10h set from Libri-Light. For fine-tuning we use open-sourced configurations for 10h9. We fine-tune models on 24 GPUs as specified in Baevski et al. (2020) for 3 different seeds.\n\nC ABLATIONS: SAMPLING FOR LARGER BATCHES\n\nTable 13: Comparison (in WER) between different temperatures τ for sampling when large batch and longer training (600k) are used.\n\nτ\n\ndev-clean\n\ndev-other\n\ntest-clean\n\ntest-other\n\n0 (argmax) 1 → 0.1\n\n19.1 13.9\n\n26.7 17.5\n\n19.3 13.8\n\n27.8 18.0\n\n(a) Evolution of pout for the different curriculum selection strategies.\n\n(b) Comparison between models trained with different pout: constant 1 (blue) or 0.1 (orange), or scheduled 0.1 → 1 (green).\n\nFigure 3: Analysis of the probability pout.\n\n8Released at https://dl.fbaipublicfiles.com/fairseq/wav2vec/libri960_big.pt. 9They are availble at https://github.com/facebookresearch/fairseq/blob/main/\n\nexamples/wav2vec/config/finetuning/vox_10h.yaml.\n\n15",
    "reference": "# Summary Of The Paper\n\n**Summary**\nThis paper considers continuous self-training for ASR. This paper build on the observation that a previous method slimIPL performance degrades as the number of pretraining step M increase. The authors hypothesize that pretraining would cause overfitting when the supervised data is limited. To address this mentioned concerns, the authors propose self-training from the beginning of process. The authors introduces a series of tricks to improve the robustness and convergence of slimIPL.\n- regenerating PL when returning a sample to cache\n- dynamically compute the returning probability with Levenshtein edit-distance\n- sampling PL (with temperature scheduling) instead of using 1-best\n\nThe optimal recipe achieve similar result with the original paper on 100h dataset but much better on 10h dataset.\n\n# Strength And Weaknesses\n\n**Strength**\n- The experiment design is very well motivated and executed.\n- The resulting method is both simple in recipe (1 stage training), also achieve significant WER reduction on 10h dataset.\n- The paper writing is extremely clear.\n\n**Wakenesses**\n- Limited novelty.\n- Absence of hyperparameter table in the appendix.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**\nExtremely clear.\n\n**Quality**\nVery high.\n\n**Novelty**\nOK.\n\n**Reproducibility**\nShould be reproducible.\n\n# Summary Of The Review\n\naccept\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCURRENT ANOMALY DETECTORS ARE ANOMALOUS: ON SEMANTIC TREATMENT OF OOD INPUTS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nMachine learning models have achieved impressive performance across different modalities. It is well known that these models are prone to making mistakes on out-of-distribution inputs. OOD detection has, therefore, gained a lot of attention recently. We observe that most existing detectors use the distribution estimated by the training dataset for OOD detection. This can be a serious impediment since faulty OOD detectors can potentially restrict utility of the model. Such detectors, tied to the bias in data collection process, can be impermeable to inputs lying outside the training distribution but with the same semantic information (e.g., class labels) as the training data. We argue that in-distribution should not be tied to just the training distribution but to the distribution of the semantic information contained in the training data. To support our argument, we perform OOD detection on semantic information extracted from the training data of MNIST and COCO datasets, and show that it not only reduces false alarms but also significantly improves detection of OOD inputs with spurious features from training data.\n\n1\n\nINTRODUCTION\n\nMachine learning models have achieved remarkable success in accomplishing different tasks across modalities such as image classification (Gkioxari et al., 2015), speech recognition (Hannun et al., 2014), and natural language processing (Majumder et al., 2017). It is however known, that such models are unreliable on samples which are less likely to occur, according to the model’s in-distribution estimated from its training data (Guo et al., 2017; Hendrycks & Gimpel, 2016). Detection of these out-of-distribution (OOD) inputs is important for the deployment of machine learning models in safety-critical domains such as autonomous driving (Bojarski et al., 2016), and medical diagnosis (De Fauw et al., 2018). OOD detection has, therefore, gained a lot of attention recently (Liang et al., 2017; Lee et al., 2018; Hendrycks et al., 2019; Kaur et al., 2021b).\n\nEven though there is sufficient interest in OOD detection, to the best of our knowledge, its unclear what precisely entails an OOD input. Existing detectors estimate a distribution that is tied to the training dataset, and flagging inputs as OOD when the assigned probability according to the estimated distribution is low. The standard drill involves a set of in-distribution inputs drawn from a dataset such as CIFAR10, and detecting those inputs as OOD that are drawn from a different dataset such as SVHN (Hendrycks & Gimpel, 2016; Kaur et al., 2021a; Lee et al., 2018). Such external inputs (from SVHN) would have non-overlapping class labels (from CIFAR10). Ming et al. (2022) show that the existing detectors are unfortunately tied to the sampling bias of the training dataset. This results in low detection on OOD inputs with spurious features such as background, color, etc. from the training data. The authors report low detection performance of existing detectors on two datasets: 1) Birds (Sagawa et al., 2019) with class labels in {waterbirds, landbirds}, and 2) CelebA (Liu et al., 2015) with class labels in {grey hair, non-grey hair}. Table 1 shows these results for OOD images containing water (or land) as spurious feature for waterbirds (or landbirds), and OOD images of bald male with male as spurious feature for grey hair. This means that even though the classifier might be able to generalize better, OOD detectors itself can stifle its utility. On the other hand these detectors can be permeable to real OOD inputs which ought to be rejected. With this in mind, we propose to treat intended distribution of images as in-distribution. Images containing semantic information relevant to the training classes1. Inputs deficient of semantic information\n\n1We will be using the terms “in-distribution” and “intended distribution” exchangeably in the paper.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Low OOD detection by existing detectors on OOD inputs with spurious features from the training data (Ming et al., 2022). Our Algorithm 2 significantly improves detection on these OOD inputs.\n\nTest Set OOD for Birds OOD for CelebA\n\nBaseline ODIN Mahala 30.65 22.75 21.25 18.93\n\n25.32 16.30\n\nEnergy Gram 41.75 25.78 18.79 28.72\n\nOur 98.97 74.36\n\nFigure 1: The intended distribution has a much higher variability in terms of the samples it covers, when compared to the training distribution. The classifier trained to classify birds in {sitting birds, flying birds} is expected to generalize well for the intended distribution, which has birds sitting on trees, snow or water. OOD inputs are the ones which are unlikely to occur from the vantage point of the intended distribution DI .\n\nw.r.t any training class such as bird for the Birds dataset and hair for CelebA need to be detected as OOD.\n\nContributions: We propose two OOD detection algorithms based on distinct ways of estimating the intended distribution. The first algorithm uses expert guided semantic information. When expert guidance is absent, and there is sufficient labeled data, we propose using semantic segmentation network. Table 1 shows that we achieve significant improvement by 57.22% and 45.64% on OOD detection with semantic segmentation networks for Birds and CelebA, respectively. This highlights the drawback of the current approaches.\n\nOur experiments on COCO (Lin et al., 2014) and MNIST (LeCun et al., 1998) datasets show that the existing detectors overfit to the training data for estimating in-distribution, resulting in - 1. false OOD detection on inputs with same (training class) labels but from a separate dataset, and 2. low OOD detection on inputs with classes, absent from the set of training classes. This low detection is due to sensitivity of detectors to the spurious features from the training data. The proposed algorithms not only reduces false alarms significantly but also improves OOD detection (≥ 20%) on inputs with spurious features from training data.\n\nRelated Work. OOD detection has been extensively studied and detectors with OOD scores based on the difference in statistical, geometrical or topological properties of in-distribution and OOD inputs have been proposed. These detectors can be classified into three categories, supervised (Lee et al., 2018; Kaur et al., 2021a), self-supervised (Hendrycks et al., 2019; Kaur et al., 2022), and unsupervised (Hendrycks & Gimpel, 2016; Liang et al., 2017). Unsupervised approaches can function without an OOD dataset for training the detector, while supervised approaches do. Self-supervised approaches require a self-labeled dataset for training the detector. This dataset is created by applying transformations to the training data and labeling the transformed data with the applied transformation. The proposed OOD detection algorithms in this paper are unsupervised in nature. Ming et al. (2022) show that the existing detectors perform poorly on OOD inputs with spurious features from the training data. They, however, do not propose a solution for fixing the existing detectors.\n\nDomain generalization Zhou et al. (2022) is an active research area where efforts are made for generalizability of machine learning classifier to its classes beyond the training data. It tries to ask the question of whether a classifier trained on the images of birds on trees would work on images of birds on water? Domain-invariant representation learning (Li et al., 2018), training data augmentation with higher variability (Zhou et al., 2020) etc. have been proposed to solve this problem. With the intended distribution of images containing (training) class-specific information for a classifier, we propose inputs that do not contain this information as OOD.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nThere has been a great interest in making use of semantic segmentation networks in scene understanding problem (Mo et al., 2022), one of the core problems in computer vision with applications e.g. to autonomous driving, video surveillance, and robot perception (Garcia-Garcia et al., 2017). Recently, the use of segmentation networks was proposed to train classifiers with a handful of training examples Mojab et al. (2021). We make use of segmentation networks for OOD detection.\n\n2 PROBLEM FORMULATION AND METHODOLOGY\n\nLet (X , AX ) be the measurable space from which images are sampled. We assume X is an at most countable subset of a (possibly very high-dimensional) Euclidean space Rh×w×3 whose dimension depends on the size of the images. Here, h, w refer to the height and width of the image. Denote by ∆(X , AX ) the space of (countably additive) probability measures on (X , AX ), and consider a candidate distribution D ∈ ∆(X , AX ).2 Let then X1, . . . , Xn ∼ D iid, denote by x1, . . . , xn the realizations of random variables X1, . . . , Xn, and call S = {x1, . . . , xn} our training set (also referred to as input sample set). We assume that the support of D, written supp(D), is a proper subset of X , that is, supp(D) ⊊ X . Then, we introduce what we call the intended distribution, i.e. a probability measure DI ∈ ∆(X , AX ) whose support supp(DI ) is a proper superset of supp(D). We can write:\n\nsupp(D) ⊊ supp(DI ) ⊂ X .\n\nIntended distribution DI is needed because it assigns non-zero probability to the set of images which are likely to be seen by the classifier in the real world. For instance, in case of standard birds dataset, D captures images of birds on trees. But, the intended distribution DI can refer to birds on trees, water, or snow. We ask the following research question : Given the input sample set S, can we build an OOD detector that is able to estimate DI closely ?\n\n2.1 METHODOLOGY\n\nThis is a difficult problem, since it demands generalization from the OOD detectors which are strictly tied to the training set S. For an ε > 0, we define the intended set of inputs XI := {x ∈ X : DI (x) > ε} ⊂ supp(DI ); next we endow it with a sigma algebra AXI ⊂ AX . Let (Y, AY ) be the measurable space of (class) labels, and assume Y is at most countable. The oracle classifier is a AXI \\AY - measurable map C : XI → Y, x (cid:55)→ C(x) = y ∈ Y, which produces the ground truth labels. With X ′ := X \\ XI we define the following space\n\nF := {F : X × X ′ → XI , (s, z) (cid:55)→ F (s, z) = x, C(x) = C(s)} .\n\nThe elements of F are maps that combine semantically relevant and irrelevant images so as to preserve the original label of the image. The label is determined by the oracle classifier C.\n\nOur assumptions are the following three:\n\n1. F ̸= ∅. There exists at least one map F that combines semantically relevant and irrelevant\n\nimages so as to preserve the original label of the image.\n\n2. supx∈X |D(x) − DI (x)| ≤ δ, for some δ ≥ 0. The distance between the original and the\n\nintended distributions is bounded by a quantity δ.\n\n3. supx∈X |D(x) − ˆDI (x)| ≤ η, for some η ≥ 0. The distance between D, and an estimate of the intended distribution DI , that we are able to compute is bounded by a quantity η.\n\nThe first assumption is a realistic one, since it is almost always the case that we can decompose an image into the two aforementioned components. The second assumption is reasonable because it is usually the case, that the distance between the intended and the sampling distribution is not arbitrarily large. The third assumption states that when building the estimate of the intended distribution, we do not move too far away from the sampling distribution. Then, we have the following. Theorem 1. Let DI , ˆDI ∈ ∆(X , AX ) be defined as above. Then, there is ν ≥ 0 such that d(DI , ˆDI ) := supx∈X |DI (x) − ˆDI (x)| ≤ ν.\n\n2As no confusion arises, we do not distinguish between probability measure and probability distribution.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Detecting OOD Inputs as Out-of-Intended Distribution Inputs\n\nInput: Test datapoint o ∈ X Parameters: Training set S, detection threshold ε Output: “1” if o is detected as OOD; “0” otherwise ˆXI = estimated XI from S ˆDI = OOD Detection( ˆXI , x ∈ X ) Return 1 if ˆDI ( ˆXI , o) < ε, 0 otherwise\n\nThe proof of Theorem 1 is included in Appendix. The Theorem states that the distance between the estimated intended distribution ˆDI and the intended distribution DI is bounded by ν = δ + η ≥ 0. If the sampling process was perfect, then intended distribution DI would be arbitrarily close to D, hence δ = 0 and the distance between ˆDI and DI would be bounded by η only. That is the error due to the short-fall of the algorithm which estimates the intended distribution from the sampled data.\n\nOOD Detection: In image classification, the intended input set XI is the set of images with class labels in Y. An OOD input is an image whose (associated class) label does not belong to Y. In light of Theorem 1, that shows that – under three natural assumptions – the distance between DI and ˆDI is bounded, we propose to perform OOD detection using ˆDI . With ˆXI being the estimation of XI obtained from training set S, and ˆDI being estimated from ˆXI , Algorithm 1 proposes to detect those inputs as OOD whose probability according to ˆDI is low.\n\nWe can assign probabilities to the (class) labels associated to the elements of XI using a pushforward argument. That is, we define PY as:\n\nY ∋ y (cid:55)→ PY (y) ≡ C♯DI (y) := DI (C −1(y)),\n\nWe estimate DI for OOD detection. In the next section, we propose two ways of defining the OOD detection function DI in Algorithm 1.\n\n3 USING SEMANTICALLY RELEVANT INFORMATION FOR OOD DETECTION\n\nIn this section, we use two kinds of maps, Ns and Nr. The former – introduced in section 3.1 – represents a semantic segmentation network, while the latter – introduced in section 3.2 – represents a two-step process. We estimate the set XI of intended images for a classifier from the class-specific semantic information contained in S. Inputs without semantic information for any class in Y are detected as OOD. We define:\n\nˆXI := {x′ ∈ Rh×w×3 : x′ = N (x), x ∈ S}. where, N is a generic segmentation map.\n\nWe argue that ˆXI is a good estimator of XI because it preserves the semantically relevant information that is required for classification. This is in line with our intuition of using the elements of set F which combine semantically relevant and irrelevant information.\n\n3.1 OOD DETECTION WITH SEMANTIC SEGMENTATION NETWORK\n\nIn scenarios with large amount of labeled available data, we propose to use semantic segmentation models as Ns. The output of a segmentation network, the segmentation map, is the classification of each pixel in the image into either background (semantically irrelevant information) or one of the class labels in Y. We propose ˆXI as the set of segmentation maps on (the elements of) S, where class information is labeled by the segmentation network. We call that segment of a segmentation map as the foreground segment which is labeled with a class in Y.\n\nHere Ns is a segmentation algorithm that filters the input with the class-specific semantic information in Y. It can be seen as a AX \\B(Rh×w×(|Y|+1))-measurable function Ns : X → Rh×w×(|Y|+1), where B(R) denotes the Borel sigma-algebra of a generic Euclidean space R. In particular, Ns only keeps height and width of the image, losing the color information; the third dimension is given by a vector of dimension |Y| + 1, where |Y| is the number of (class) labels, and the extra dimension captures an “extra label” associated to the background and all the images that are not assigned any of\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nthe labels in Y. Its entries are reals between 0 and 1 that sum up to 1; they represent the probability of image x belonging to (class) label y ∈ Y or to the “extra label”.\n\nOOD Detection Scores: The classification-based detection scores by the existing detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017) can be used for OOD detection on the foreground segment of the input image. Similar to the baseline detector (Hendrycks & Gimpel, 2016), which uses softmax score of the predicted class by a classification network for detection, we propose to use softmax scores for the predicted class of the foreground segment for detection. Since the detection score has to be a single value, we take average of the softmax scores for the pixels in the foreground segment. Recall that X ⊂ Rh×w×3. Let C = {∅, X , c1, . . . , cN , cN +1}, N ∈ N, be a partition of X whose elements are N classes of images (e.g. birds, clothes, cars, etc.), computed as:\n\ncj := C −1(y) = {x ∈ XI : C(x) = y} for all y ∈ Y, j ∈ {1, . . . , N },\n\none class – the (N + 1)-th – that incorporates the background and everything that is not subsumed in the other N classes, that is, cN +1 = X \\ (∪N\n\nj=1cj), and the whole space X and the empty set ∅.\n\nPick any x ∈ X and let H = {1, . . . , h} and W = {1, . . . , w}. For a generic vector a, we denote by ai its i-th entry, while for a generic element t of Rh×w×(|Y|+1), we write ti,j to denote the (|Y| + 1)- dimensional vector that we obtain if we “slice” t at first coordinate i and second coordinate j. Let q = (q1, . . . , qN , qN +1)⊤ ∈ RN +1; define function\n\nq (cid:55)→ S(q) :=\n\n(cid:26)maxi qi when arg maxi qi ∈ {1, . . . , N, N + 1}\n\n0\n\notherwise\n\n.\n\nDefinition 1. Pick any x ∈ X . Define the following baseline score\n\nBS(x) :=\n\n(cid:80)\n\ni∈H\n\n(cid:80)\n\nj∈W S(cid:0)Ns(x)i,j h · w\n\n(cid:1)\n\n(1)\n\nWe can also use the classification-based score used by the ODIN detector (Liang et al., 2017). ODIN is an enhanced version of the baseline detector where the temperature-scaled softmax score of the preprocessed input is used for detection. The input is preprocessed by adding small perturbations:\n\n(cid:101)x := x − ζ sign(−∇xlogS′\n\n⋆(x, T )).\n\nHere ζ > 0 is the perturbation magnitude, sign denotes the sign function, T ∈ R>0 is the temperature scaling parameter, S′(x, T ) is an N -dimensional vector whose i-th entry\n\nS′\n\ni(x, T ) =\n\nexp(fi(x)/T ) j=1 exp(fj(x)/T )\n\n(cid:80)N\n\nis given by the temperature-scaled softmax score of the i-th class predicted by the classification network f = (f1, . . . , fN , ) that is trained to classify classes in {c1, . . . , cN }, and S′ ⋆(x, T ) = maxi S′\n\ni(x, T ).\n\nDefinition 2. Pick any x ∈ X . Define the ODIN score as\n\nOS(x) :=\n\n(cid:80)\n\ni∈H\n\n(cid:80)\n\nj∈W S(Ns((cid:101)x)i,j) h · w\n\n.\n\n(2)\n\nAlgorithm 2 is the proposed algorithm for OOD detection with ˆXI as the segmentation maps of S.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 ˆDI : OOD Detection with Semantic Segmentation Network\n\nAlgorithm 3 ˆDI : OOD Detection with Reference Set\n\nInput: Test input o ∈ X , Parameters: Semantic segmentation network Ns trained on S, detection score ds ∈ {BS, OS}, detection threshold ε Output: “1” if o is detected as OOD; “0” otherwise o′ = Ns(o) Create z s.t, zi,j = S(o′ Return 1 if ds(z) < ε, 0 otherwise\n\ni,j), ∀i ∈ h, ∀j ∈ w\n\nInput: Test input o ∈ X Parameters: Segmentation algorithm Nr, reference set R, detection threshold ε Output: “1” if o is detected as OOD; “0” otherwise v = ComputeN earestSample(R, Nr(o)) Return 1 if max(v) < ε, 0 otherwise\n\n3.2 OOD DETECTION WITH A REFERENCE SET\n\nDatasets such as MNIST, with a history of feature engineering techniques (Belongie et al., 2000), permit semantically relevant pixels to be derived easily. The generation of the segmentation AX \\B(Rh×w×3)-measurable map Nr : X → Rh×w×3 follows a two step process. First it uses a standard segmentation algorithm to define super-pixels of an image. Next, it removes the segments which are can be regarded as irrelevant information. We create an image out of the two components by setting different colors to these pieces. We leave the details with examples (Fig. 8) to the Appendix.\n\nOOD Detection Score: Given a pair of segmented images, we compare them using a metric known as the SSIM value. It is a well known metric to compare similarity between images. OOD detection is performed by measuring its similarity with the nearest neighbor in a reference set.\n\nSSIM : Structural similarity index metric (SSIM) (Wang et al., 2004) computes statistical similarity between two images. SSIM value is calculated as:\n\nSSIM (x1, x2) = S1(x1, x2)S2(x1, x2), where\n\nS1(x1, x2) = lum(x1, x2), S2(x1, x2) = con(x1, x2)corr(x1, x2)\n\n(3)\n\nThe functions lum, con and corr compare the luminosity, contrast and correlation between two image inputs x1 and x2. The details of the implementation can be found in (Wang et al., 2004) and (Brunet et al., 2012). SSIM permits fast GPU based implementation. Next, we define a reference set R ⊂ ˆXI as a set of size |Y| containing one representative of each class y ∈ Y; by representative, we mean an image whose associated (class) label is some y ∈ Y. It is defined as:\n\nR := {x ∈ ˆXI : x ∼ Unif(C −1(y)), ∀y ∈ Y} ⊂ XI . A more involved algorithm can be used to replace this simple choice, such as the ones proposed in Yang et al.; Dutta et al. (2022). But, we find this simple procedure to work well in this context.\n\nAlgorithm: Algorithm 3 combines these pieces together. We compute SSIM of an input image o ∈ X with all images in R and use the maximum value for detection. In other words if the similarity value of o with its nearest neighbor in R is below the threshold ε, we declare o as OOD.\n\n4 EXPERIMENTS\n\nWe perform experiments with the existing state-of-the-art (SOTA) detectors from all the three categories of supervised, unsupervised, and self-supervised OOD detection techniques.\n\nUnsupervised : Baseline detector (Hendrycks & Gimpel, 2016) is the SOTA unsupervised detector. It uses softmax score of a classifier for the predicted class. ODIN (Liang et al., 2017) is an enhanced version of the baseline detector that uses temperature-scaled softmax score but, for a perturbed input for detection. Details are in section 3.1.\n\nSupervised : Mahalanobis detector (Mahala) is the SOTA supervised detector which uses Mahalanobis distance (Mahalanobis, 1936) of the input in the training feature space of the classifier for detection.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Examples of images from the three test cases for COCO.\n\nSelf-supervised : Aux (Hendrycks et al., 2019) is the SOTA self-supervised detector which uses error in the prediction of the applied transformation on the input for detection. It trains a classifier with an auxiliary task of predicting the an applied rotation, vertical and horizontal translations. The sum of the error in the three predictions and classification error is used for detection.\n\nEvaluation Metrics: We call in-distribution inputs as positives and OOD inputs as negatives. We report the Receiver Operating Characteristic curve (ROC), Area under ROC (AUROC), and True Negative Rate (TNR) at 95% True Positive Rate (TPR) for evaluation. These are the standard metrics used in OOD detection (Hendrycks et al., 2019; Liang et al., 2017; Lee et al., 2018).\n\n4.1 CASE STUDY I: OOD DETECTION WITH SEMANTIC SEGMENTATION NETWORK\n\n4.1.1 DATASET AND MOTIVATION\n\nCommon Objects in Context-Stuff (COCO) dataset (Caesar et al., 2018b) is a large-scale vision dataset created for the purpose of training machine learning models for object detection, segmentation and captioning with 182 object classes. We use that subset (training and test) of COCO which can be classified with the class labels from the set Y = {cup, umbrella, orange, toaster, broccoli, banana, vase, zebra, kite}. These classes are common to another dataset Vizwiz (Chiu et al., 2020). Vizwiz is a real patient dataset captured by blind people. This dataset is collected with the purpose of assisting partially blind people. Where the quality of images captured can be an issue. So, images in the Vizwiz are labeled with either “no issues”, or with issues such as “blurry”, “too bright”, “too dark”, “camera obstructed” etc. We call the images with “no issues” label in the Vizwiz dataset as the clear Vizwiz.\n\nWe train the ResNet18 (He et al., 2016) model to classify the training set of COCO dataset. With 74.33% as model’s accuracy on test COCO, it achieves a comparable accuracy of 68.14% on the clear Vizwiz. Detecting inputs from clear Vizwiz as OOD by the existing detectors restricts the generalizability of classifiers from the training distribution D to the intended distribution DI .\n\n4.1.2 SEMANTIC SEGMENTATION NETWORK Ns FOR ALGORITHM 2 AND CLASSIFIER\n\nFOR DETECTION BY EXISTING DETECTORS\n\nAs recommended by the authors of the COCO dataset (Caesar et al., 2018a), we train the DeepLab (Chen et al., 2017) on the training set of COCO. DeepLab version 2 (v2) segmentation model v2 uses ResNet101 (He et al., 2016) model as the backbone model. For a fair comparison with the existing detectors, we train the ResNet101 classifier on the training set of COCO. We use the trained classifier for OOD detection by the existing SOTA unsupervised and supervised detectors. The accuracy of the classifier on the test COCO set is 68.64%. COCO dataset is commonly used for object detection and segmentation. The classification accuracy of 68.64% is comparable with the SOTA detection accuracy (in terms of mean average precision) of 64.2% on COCO (Wei et al., 2022). For the self-supervised detector AUX, we train the ResNet101 classifier with the auxiliary losses of rotations and translations. Its classification accuracy on the test COCO set is 74.11%.\n\n4.1.3 TEST CASES AND RESULTS\n\nWe conduct our experiments with the following three test cases: (a) In-Distribution from Clear Vizwiz: Inputs with the class labels in Y but from the clear Vizwiz. (b) OOD from Vizwiz: Inputs with blurry, too bright, too dark, and obstructed issues from Vizwiz. Due to the quality issues of these images, this dataset cannot be labeled with any labels in Y. (c) OOD From COCO: Inputs from the test COCO dataset with class labels not in Y. Here, we filter that subset of the test COCO that can be classified with the class labels from the set {traffic light, stop sign, parking meter, fire hydrant}.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: (a) AUROC less than 50% on in-distribution inputs from clear Vizwiz, and (c) highest AUROC on OOD inputs from COCO by Algorithm 2 shows that the proposed detection not only significantly reduces the false alarms but also improves on OOD detection on inputs with spurious features from training set.\n\nFigure 2 shows some examples of the images from the three test cases. Figure 3 compares the ROC and AUROC results of the existing detectors with Algorithm 2 on these test cases: (a) In-Distribution from Clear Vizwiz (Fig. 3(a)): AUROC less than 50% by our approach (with both the baseline and ODIN scores) implies that the proposed detector is not able to distinguish between the test COCO and Clear Vizwiz datasets. AUROC greater than 50% by the existing detectors implies that these detectors distinguish clear Vizwiz from the test COCO by assigning higher OOD detection scores to clear Vizwiz. (b) OOD from Vizwiz (Fig. 3(b)): With these images as OOD for COCO, we require the AUROC to be as close to one as possible. The existing supervised detector Mahala achieves the best AUROC of 92.90% and our results are 84.18% and 85.76% with baseline and ODIN scores respectively. (c) OOD from COCO (Fig. 3(c)): Significantly higher (≥ 20%) AUROC by Algorithm 2 (with both scores) than the existing ones indicates that the proposed detector performs OOD detection on these inputs with spurious features from the training data better than the existing ones.\n\nDiscussion The performance of the proposed Algorithm 2 depends on the semantic (or foreground) information segmented in XI by the segmentation network. We observe that the trained segmentation network labels all pixels in 22% images of the test COCO as background. This results in low AUROC (in comparison to 92.90% by Mahala) of 85.76% on low quality OOD inputs from Vizwiz.\n\nIf the segmentation network is trained well to segment inputs drawn from XI (ex. test COCO) then the detection performance of Algorithm 2 improves. Figure 4 shows the ROC and AUROC results on the 88% images of test COCO that are segmented with class labels in Y by the network. This test case is analogous to using a well trained segmentation network which segments 100% of the test COCO with class labels in Y as improving the training of segmentation network is beyond the scope of this paper. With the 100% test COCO segmented with class labels in Y by the segmentation network, the performance of Algorithm 2 improves from 85.76% to 98.52% (with ODIN score) on OOD inputs from Viziwz and 85.43% to 97.42% (with baseline score) on OOD inputs from COCO. Here we show results on test cases (b) and (c). Results on test case (a) are included in Appendix.\n\n4.2 CASE STUDY II: OOD DETECTION WITH A REFERENCE SET\n\n4.2.1 DATASET AND MOTIVATION\n\nWe use a mixture of MNIST-M (Ganin & Lempitsky, 2015) and Background-Colored-MNIST (BCMNIST) (Bui et al., 2021) datasets. Both MNIST-M and BC-MNIST are modified versions of\n\n8\n\n0.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRate(a)In-DistributionfromClearVizwizBaseline:67.57ODIN:72.70Mahala:60.53AUX:73.08Our(BS):31.57Our(OS):35.480.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRate(b)OODsfromVizwiz(blurry,toobrightetc.)Baseline:85.67ODIN:92.48Mahala:92.90AUX:92.72Our(BS):84.18Our(OS):85.760.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRate(c)OODsfromCOCOBaseline:54.22ODIN:64.75Mahala:63.16AUX:65.83Our(BS):85.43Our(OS):84.82Under review as a conference paper at ICLR 2023\n\nFigure 4: ROC and AUROC results of the existing detectors and Algorithm 2 on that subset of test COCO which is segmented with class labels in Y by the trained segmentation network Ns.\n\nFigure 5: AUROC less than 50% on in-distribution inputs from BC-MNIST by Algorithm 3 shows that the proposed algorithm significantly reduces false alarms (by the existing detectors) for training set of Mix-MNIST.\n\nMNIST (LeCun et al., 1998) dataset. MNIST-M is MNIST with its digits blended over patches of colored images. BC-MNIST is the colored version of MNIST where both digits and background are colored. We use a mixture dataset of 100% data from MNIST-M and 50% data from BC-MNIST. We call this dataset as Mix-MNIST.\n\nWith 60, 000 training images in MNIST-M and 4000 training images in BC-MNIST, 96.77% of the training data in Mix-MNIST comes from MNIST-M and the remaining 3.23% from BC-MNIST. We train the Lenet5 (LeCun et al., 1998) classifier on Mix-MNIST. The classifier achieves comparable accuracy of 90% and 91% on test MNIST-M and test BC-MNIST datasets respectively. Therefore, with the classifier’s ability to generalize on BC-MNIST with only 3.23% of BC-MNIST as the training data, detecting inputs from BC-MNIST as OOD by the exiting detectors (Fig. 5) limits the applicability of the classifier.\n\n4.2.2 EXPERIMENTAL DETAILS AND RESULTS\n\nFor the existing detectors, we use trained the LeNet5 model with its accuracy of 91.91% on the test set of Mix-MNIST. Figure 5 compares the ROC, AUROC, and TNR results of the existing detectors with Algorithm 3 on the test set of BC-MNIST. AUROC higher than 50% by the existing detectors implies that existing detectors distinguish the test data of Mix-MNIST from the test set of BC-MNIST with higher OOD detection scores assigned to BC-MNIST. AUROC less than 50% by the proposed Algorithm 3 shows that it does not distinguish between the test sets of Mix-MNIST and BC-MNIST. We achieve the lowest false alarm rate of 2.95% here.\n\nWe perform additional experiments for Mix-MNIST with OOD datasets from (low quality) Vizwiz and Fashion-MNIST. Details and results on these experiments are included in Appendix.\n\n5 CONCLUSION\n\nIn this paper we show that including more nuanced semantic information about the content of images can improve detection of out-of-distribution inputs. This to the best of our knowledge, is one of the first approaches which differentiates between sampling distribution and intended distribution.\n\n9\n\n0.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRate(b)OODsfromVizwiz(blurry,toobrightetc.)Baseline:87.13ODIN:92.87Mahala:93.65AUX:93.29Our(BS):96.75Our(OS):98.520.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRate(c)OODsfromCOCOBaseline:58.01ODIN:68.10Mahala:64.92AUX:68.56Our(BS):97.42Our(OS):96.60AUROC / TNRUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSerge Belongie, Jitendra Malik, and Jan Puzicha. Shape context: A new descriptor for shape match-\n\ning and object recognition. Advances in neural information processing systems, 13, 2000.\n\nMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.\n\nDominique Brunet, Edward R. Vrscay, and Zhou Wang. On the mathematical properties of the IEEE Transactions on Image Processing, 21(4):1488–1499, 2012.\n\nstructural similarity index. doi: 10.1109/TIP.2011.2173206.\n\nManh-Ha Bui, Toan Tran, Anh Tran, and Dinh Phung. Exploiting domain-specific features to enhance domain generalization. Advances in Neural Information Processing Systems, 34:21189– 21201, 2021.\n\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. The COCO-Stuff dataset. https://\n\ngithub.com/nightrome/cocostuff, 2018a.\n\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1209– 1218, 2018b.\n\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and IEEE transactions on pattern analysis and machine intelligence, 40(4): fully connected crfs. 834–848, 2017.\n\nTai-Yin Chiu, Yinan Zhao, and Danna Gurari. Assessing image quality issues for real-world problems. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3646–3656, 2020.\n\nJeffrey De Fauw, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, Xavier Glorot, Brendan O’Donoghue, Daniel Visentin, et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nature medicine, 24(9):1342–1350, 2018.\n\nSouradeep Dutta, Yahan Yang, Elena Bernardis, Edgar Dobriban, and Insup Lee. MemarXiv preprint\n\nory classifiers: Two-stage classification for robustness in machine learning. arXiv:2206.05323, 2022.\n\nPedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-based image segmentation. Inter-\n\nnational journal of computer vision, 59(2):167–181, 2004.\n\nYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In\n\nInternational conference on machine learning, pp. 1180–1189. PMLR, 2015.\n\nAlberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea, Victor Villena-Martinez, and Jose Garcia-Rodriguez. A review on deep learning techniques applied to semantic segmentation. arXiv preprint arXiv:1704.06857, 2017.\n\nGeorgia Gkioxari, Ross Girshick, and Jitendra Malik. Contextual action recognition with r* cnn. In Proceedings of the IEEE international conference on computer vision, pp. 1080–1088, 2015.\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural\n\nnetworks. arXiv preprint arXiv:1706.04599, 2017.\n\nAwni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution\n\nexamples in neural networks. arXiv preprint arXiv:1610.02136, 2016.\n\nDan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve model robustness and uncertainty. In Advances in Neural Information Processing Systems, pp. 15663–15674, 2019.\n\nPavel Iakubovskii.\n\nSegmentation models pytorch.\n\nhttps://github.com/qubvel/\n\nsegmentation_models.pytorch, 2019.\n\nRamneet Kaur, Susmit Jha, Anirban Roy, Sangdon Park, Oleg Sokolsky, and Insup Lee. Detecting\n\noods as datapoints with high uncertainty. arXiv preprint arXiv:2108.06380, 2021a.\n\nRamneet Kaur, Susmit Jha, Anirban Roy, Oleg Sokolsky, and Insup Lee. Are all outliers alike? on understanding the diversity of outliers for detecting oods. arXiv preprint arXiv:2103.12628, 2021b.\n\nRamneet Kaur, Susmit Jha, Anirban Roy, Sangdon Park, Edgar Dobriban, Oleg Sokolsky, and IniDECODe: In-distribution Equivariance for Conformal Out-of-distribution Detection,\n\nsup Lee. Association for the Advancement of Artificial Intelligence, 2022.\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n\nCheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive In IEEE Conference on Computer Vision and Pattern Recognition\n\nfacial image manipulation. (CVPR), 2020.\n\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.\n\nYa Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624–639, 2018.\n\nShiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution\n\nimage detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In European\n\nDoll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. conference on computer vision, pp. 740–755. Springer, 2014.\n\nTsung-Yi Lin, Piotr Doll ́ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125, 2017.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pp. 3730–3738, 2015.\n\nPrasanta Chandra Mahalanobis. On the generalized distance in statistics. National Institute of\n\nScience of India, 1936.\n\nNavonil Majumder, Soujanya Poria, Alexander Gelbukh, and Erik Cambria. Deep learning-based document modeling for personality detection from text. IEEE Intelligent Systems, 32(2):74–79, 2017.\n\nYifei Ming, Hang Yin, and Yixuan Li. On the impact of spurious correlation for out-of-distribution In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp.\n\ndetection. 10051–10059, 2022.\n\nYujian Mo, Yan Wu, Xinneng Yang, Feilin Liu, and Yujun Liao. Review the state-of-the-art technologies of semantic segmentation based on deep learning. Neurocomputing, 493:626–646, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nNooshin Mojab, Philip S Yu, Joelle A Hallak, and Darvin Yi. Cvs: Classification via segmentation\n\nfor small datasets. arXiv preprint arXiv:2111.00042, 2021.\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.\n\nZ. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image Quality Assessment: From Error Visibility to Structural Similarity. IEEE Transactions on Image Processing, 13(4):600–612, April 2004. doi: 10.1109/TIP.2003.819861.\n\nYixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation. arXiv preprint arXiv:2205.14141, 2022.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-\n\ning machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n\nYahan Yang, Ramneet Kaur, Souradeep Dutta, and Insup Lee. Interpretable detection of distribution\n\nshifts in learning enabled cyber-physical systems.\n\nKaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel domains for domain generalization. In European conference on computer vision, pp. 561–578. Springer, 2020.\n\nKaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A\n\nsurvey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 PROOF OF THEOREM 1\n\nProof. It is immediate to see that the proposed d is a proper metric. Then, the proof goes as follows\n\nd(DI , ˆDI ) := sup\n\nx∈X\n\n|DI (x) − ˆDI (x)| = sup\n\nx∈X\n\n|DI (x) − ˆDI (x) − D(x) + D(x)|\n\n(cid:110)\n\n|D(x) − DI (x)| + | ˆDI (x) − D(x)|\n\n(cid:111)\n\n≤ sup x∈X\n\n|D(x) − DI (x)| + sup x∈X\n\n≤ sup x∈X = δ + η.\n\n| ˆDI (x) − D(x)|\n\nSo putting ν = δ + η, we conclude the proof.\n\nA.2 EXAMPLE IMAGES FROM COCO SEGMENTED WITH CLASS-SPECIFIC RELEVANT\n\nINFORMATION\n\nFigure 6 shows some examples of images sampled from COCO and clear Vizwiz datasets on the left and corresponding output of the trained semantic segmentation network Ns from section 4.1.2 on the right.\n\nFigure 6: Example images from COCO and clear Vizwiz on left and output of the trained semantic segmentation network on these images on the right. Yellow color in the segmented images represent the class-specific semantically relevant information, and purple color represents the semantically irrelevant information.\n\nA.3 RESULTS ON IN-DISTRIBUTION INPUTS FROM CLEAR VIZWIZ WITH THAT SUBSET OF COCO WHICH IS SEGMENTED WITH CLASS LABELS IN Y BY THE SEGMENTATION NETWORK Ns\n\nFigure 7 shows these results. Again, AUROC less than 50% by our approach (with both the baseline and ODIN scores) implies that the proposed detector is not able to distinguish between the test COCO and Clear Vizwiz datasets. AUROC greater than 50% by the existing detectors implies that these detectors distinguish clear Vizwiz from the test COCO by assigning higher OOD detection scores to clear Vizwiz.\n\n13\n\nClear VizwizCOCOUnder review as a conference paper at ICLR 2023\n\nFigure 7: ROC and AUROC results of the existing detectors and Algorithm 2 on that subset of test COCO which is segmented with class labels in Y by the trained segmentation network Ns.\n\nA.4 DETAILS ABOUT THE TWO-STEP SEGMENTATION ALGORITHM Nr USED IN\n\nALGORITHM 3\n\nDetecting semantically relevant pixels is the first step in this algorithm. In order to separate the semantically relevant pixels, we first partition the image into meaningful segments using Felzenszwalb’s Algorithm Felzenszwalb & Huttenlocher (2004). Next we mark the segments placed away from the center as being semantically irrelevant. Whatever remains closely maps to semantically relevant information. We binarize the result in the previous step, to obtain a black and white version of the image. Figure 8 shows some examples of images sampled from Mix-MNIST on the left and corresponding output of the segmentation algorithm Nr from section 4.2.2 on the right.\n\nFigure 8: Example images from Mix-MNIST on left and the output of the segmentation algorithm on these images on the right. White color in the segmented images represent the class-specific semantically relevant information, and black color represents the semantically irrelevant information.\n\n14\n\n0.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRate(a)In-DistributionfromClearVizwizBaseline:70.42ODIN:74.82Mahala:60.09AUX:75.36Our(BS):39.52Our(OS):44.96Under review as a conference paper at ICLR 2023\n\nA.5 EXPERIMENTAL DETAILS AND ADDITIONAL EXPERIMENTS ON MIX-MNIST\n\nA.5.1 EXPERIMENTAL DETAILS\n\nGiven two binarized versions of an image pair by the segmentation algorithm Nr described in Appendix A.4, we compute the SSIM value between these images. We restrict ourselves to a nonnegative version of the SSIM metric in this paper. To estimate whether an image contains digit, we maintain a reference set for digits zero to nine. Figure 9 shows the reference set used in experiments. For a given test image, we compute the SSIM between the binary version of the image and each digit image in the reference set. If the test image does not resemble any digit in the reference set, we declare it to be OOD.\n\nFigure 9: Reference set of 0-9 digits extracted from images in Mix-MNIST by the segmentation algorithm Nr.\n\nA.5.2 ADDITIONAL EXPERIMENTS\n\nWe conduct additional experiments on Mix-MNIST with the following two test cases: (a) OOD from Vizwiz: Images with the blurry, too dark, and obstructed quality issues from Vizwiz. (b) OOD from Fashion-MNIST: Images from Fashion-MNIST (Xiao et al., 2017) dataset with class labels from fashion objects such as trousers, shoe etc.\n\nThe results are as follows: Figure 10 compares the ROC and AUROC results of the existing detectors with the proposed OOD detection Algorithm 3. Table 2 shows these results on TNR (at 95% TPR) on these test cases: (a) OOD from Vizwiz (Fig. 10(a)): With failure to assign any labels to this dataset due to quality issues, these images are OOD for the Mix-MNIST dataset and here we require the AUROC to be as close to one as possible. The existing detector ODIN achieves the best AUROC of 94.95% and our result is 90.93%. We achieve the best TNR (@95%TPR) detection of 67.15% here. (b) OOD from Fashion-MNIST (Fig. 10(b)): With the class labels of Fashion-MNIST disjoint from the classes in Mix-MNIST, images from Fashion-MNIST are OOD for Mix-MNIST. The existing supervised detector Mahala achieves the best AUROC of 86.03% and our (unsupervised) results are comparable at 84.27%. Mahala achieves the best TNR (@95%TPR) detection of 56.86% and ours is second best at 44.67%.\n\nFigure 10: AUROC results of the existing detectors and Algorithm 3 on Mix-MNIST.\n\nTable 2: TNR (@95% TPR) of existing detectors and Algorithm 3 on Mix-MNIST.\n\nTest Set OOD from Vizwiz OOD from Fashion-MNIST\n\nBaseline ODIN Mahala AUX 43.23 11.24\n\n64.73 23.13\n\n66.67 19.02\n\n54.16 56.86\n\nOur 67.15 44.67\n\n15\n\n0.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRate(a)OODfromVizwiz(blurry,toobrightetc.)Baseline:93.70ODIN:94.95Mahala:77.19AUX:80.28Our:90.930.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRate(b)OODfromFashion-MNISTBaseline:84.33ODIN:85.46Mahala:86.03AUX:68.59Our:84.27Under review as a conference paper at ICLR 2023\n\nA.6 DETAILS ON THE CELEBA AND BIRDS EXPERIMENTS\n\nA.6.1 CELEBA\n\nWe use the semantic segmentation network Ns by Lee et al. (2020) on CelebA dataset. The network segments faces into different parts including nose, hair, mouth, etc. In this experiment, we ran Algorithm 2 with the Baseline score.\n\nFigure 11: Images from CelebA dataset.\n\nA.6.2 BIRDS\n\nWe use the semantic segmentation network Ns by Iakubovskii (2019). Here, we select Feature Pyramid Network (FPN) (Lin et al., 2017) with ResNet50 (He et al., 2016) as its backbone architecture. It segments the images into two parts: bird and background. In this experiment, we ran Algorithm 2 with the Baseline score.\n\nFigure 12: Images from Birds dataset\n\n16",
    "reference": "# Summary Of The Paper\n\nThe paper presents a discussion on the definition of the in-distribution (ID) vs. out-of-distribution (OOD) for OOD detection problems, arguing that existing methods fail to semantically extrapolate the in-distribution (e.g. the ID of flying and sitting birds should include and generalize to various backgrounds, e.g. sky, woods, water, etc.). The paper calls such a \"semantically complete\" characterization of the ID the \"Intended Distribution\". Following this argument, the paper proposes to leverage semantic segmentation networks for OOD detection generalizing to the intended distribution. Here, classification OOD methods (e.g. maximum softmax or ODIN) are proposed to be used over the pixels in the foreground, whereas pixels in the background attain the max OOD score. An experimental evaluation on COCO and Vizwiz is presented showing that the proposed methods compares favorably over previous approaches in the intended distribution OOD detection task.\n\n# Strength And Weaknesses\n\n*Strengths*\n+ The paper contributes some helpful conceptual structure for thinking about OOD detection, essentially discussing the balance of OOD detection vs. generalization, arguing that OOD detection should generalize to semantically coherent classes (disregarding potentially spurious background correlations)\n+ The paper is well motivated and integrated into the existing literature.\n\n*Weaknesses*\n- The proposed method essentially defers OOD detection for classification to a segmentation problem, which requires more granular information (pixel-/object-level annotations). I think this aspect is not communicated clearly in the paper yet, and I think to some degree the reported improvements are expected, given the proposed model uses much more semantic information from a segmentation model.\n- On the other hand, the paper does not seem to put in much effort in testing the limits of well trained segmentation models (\"[...] as improving the training of segmentation network is beyond the scope of this paper.\"), which presents the foundation of the proposed approach. Here, I think the paper should go deeper in exploring the strengths and limits of this approach.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n+ The paper is overall well written and easy-to-follow.\n+ The paper seems to include all the details necessary to reproduce the presented results.\n\n* I find the novelty of the paper to be fair, though OOD generalization vs. detection has also been discussed in previous works.\n\n- The paper includes some technical inaccuracies, e.g. an \"AUROC less than 50% [...] implies that the proposed detector is not able to distinguish between the test COCO and Clear Vizwiz datasets.\" is false. A detector cannot distinguish and is random at an AUROC of 50%. Below 50%, the ranking becomes inverse.\n\n---\n\n*Additional Comments*\n* In the definition of the function $S$, the first case should only be on $\\{1, ..., N\\}$, right?\n* Strictly speaking, $C$ in the definition of $\\mathcal{F}$ is not defined on $\\mathcal{X}'$, but only $\\mathcal{X}$, right?\n\n# Summary Of The Review\n\nThe paper presents some interesting and valid discussion on OOD detection vs. generalization, proposing to leverage segmentation networks to improve OOD detection that generalizes to semantically coherent classes. However, cost aspects of requiring more granular information for segmentation are not yet sufficiently addressed and discussed in my opinion.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nIDEAL: QUERY-EFFICIENT DATA-FREE LEARNING FROM BLACK-BOX MODELS\n\nJie Zhang ∗† Zhejiang University {zj zhangjie}@zju.edu.cn\n\nChen Chen∗ & Lingjuan Lyu‡ Sony AI {ChenA.Chen,Lingjuan.Lv}@sony.com\n\nABSTRACT\n\nKnowledge Distillation (KD) is a typical method for training a lightweight student model with the help of a well-trained teacher model. However, most KD methods require access to either the teacher’s training data or model parameter, which is unrealistic. To tackle this problem, recent works study KD under data-free and black-box settings. Nevertheless, these works require a large number of queries to the teacher model, which incurs significant monetary and computational costs. To address these problems, we propose a novel method called query-effIcient Datafree lEarning from blAck-box modeLs (IDEAL), which aims to query-efficiently learn from black-box model APIs to train a good student without any real data. In detail, IDEAL trains the student model in two stages: data generation and model distillation. Note that IDEAL does not require any query in the data generation stage and queries the teacher only once for each sample in the distillation stage. Extensive experiments on various real-world datasets show the effectiveness of the proposed IDEAL. For instance, IDEAL can improve the performance of the best baseline method DFME by 5.83% on CIFAR10 dataset with only 0.02× the query budget of DFME.\n\n1\n\nINTRODUCTION\n\nKnowledge Distillation (KD) has emerged as a popular paradigm for model compression and knowledge transfer Gou et al. (2021). The goal of KD is to train a lightweight student model with the help of a well-trained teacher model. Then, the lightweight student model can be easily deployed to resource-limited edge devices such as mobile phones. In recent years, KD has attracted significant attention from various research communities, e.g., computer vision Wang (2021); Passalis et al. (2020); Hou et al. (2020); Li et al. (2020), natural language processing Hinton et al. (2015); Mun et al. (2018); Nakashole & Flauger (2017); Zhou et al. (2020b), and recommendation systems Kang et al. (2020); Wang et al. (2021a); Kweon et al. (2021); Shen et al. (2021).\n\nHowever, most KD methods are based on several unrealistic assumptions: (1) users can directly access teacher’s training data; (2) the teacher model is considered as a white-box model, i.e., model parameters and structure information can be fully utilized. For example, to facilitate the training process, FitNets Romero et al. (2015) uses not only the original training data, but also the output information from the teacher’s intermediate layers. However, in real-world applications, the teacher model is usually provided by a third party. Thus the teacher’s training data is usually not public and unable to access. In fact, the teacher model is mostly trained by big companies with extensive amounts of data and plenty of computation resources, which is the core competitiveness of companies. As a result, the specific parameters and structural information of the teacher model are never exposed in the real world. Consequently, accessing the teacher model or teacher’s training data render these KD methods impractical in reality.\n\nTo solve these problems, some recent studies Truong et al. (2021b); Fang et al. (2021a) attempt to learn from a black-box teacher model without any real data, i.e., data-free black-box KD. These\n\n∗Equal contribution. †Work done during internship at Sony AI. ‡Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: An empirical study of previous methods with a limited number of queries (we set the query budget Q = 25K for MNIST, Q = 250K for CIFAR10, and Q = 2M for CIFAR100.) in various scenarios. We also adopt CMI Fang et al. (2021b) for hard-label scenarios and name it “CMI∗”.\n\nMethod\n\naccess to training data white-box / black-box\n\nlogits / hard-label MNIST CIFAR10 CIFAR100\n\nNormal KD Hinton et al. (2015) CMI Fang et al. (2021b) CMI∗ DFME Truong et al. (2021a) ZSDB3 Wang (2021)\n\n✓ ×\n× ×\n×\n\nwhite-box white-box white-box black-box black-box\n\nlogits logits hard-label logits hard-label\n\n98.91% 94.34% 98.20% 92.22% 86.53% 76.17% 68.26% 51.28% 37.33% 32.18%\n\n76.87% 74.47% 63.45% 39.12% 14.28%\n\nmethods do not need to access the private data and can train the student model with the class probabilities returned by the teacher model. However, in real-world scenarios, the pre-trained model on the remote server may only provide APIs for inference purpose (e.g., commercial cloud services), these APIs usually return the top-1 class (i.e., hard label) of the given queries. For example, Google BigQuery1 provides APIs for several applications. Such APIs only return a category index for each sample instead of the class probabilities. Moreover, these APIs usually charge for each query to the teacher model, and thus budget should be considered in the process of query. Nevertheless, previous methods Truong et al. (2021a); Wang (2021); Zhou et al. (2020a) require a large number of queries to the teacher model, which is costly and impractical. Hence, training a high-performance student model with a small number of queries is still an unsolved problem.\n\nIn this paper, we consider a more practical and challenging setting: (1) the teacher’s training data is not accessible, i.e., data-free; (2) the parameter of the teacher model is not accessible, i.e., blackbox; (3) the teacher model only returns a category index for each sample, i.e., hard-label; and (4) the number of queries is limited, i.e., query-efficient. To better understand the difficulty of this setting, we report the top-1 test accuracy of student models under different scenarios with a limited query budget2 in Table 1.\n\nAs shown in Table 1, we have some valuable observations: (1) In white-box scenarios, data-free KD can achieve satisfied performance, but when the model API is restricted to only hard labels, CMI Fang et al. (2021b) suffers from serious performance degradation. It indicates that logits can provide more information for training, while hard labels are more difficult; (2) With the same number of queries, the performance of these methods dramatically decrease under the black-box scenarios. Furthermore, the performance of data-free black-box KD with hard labels is only 14.28% on CIFAR10 dataset, which is close to random guess (10%). Consequently, in this paper, we focus primarily on how to query-efficiently train a good student model from black-box models with hard labels, which is very practical but challenging.\n\nFor this purpose, we propose a novel method called query-effIcient Data-free lEarning from blAckbox modeLs (IDEAL), which trains the student model with two stages: a data generation stage and a model distillation stage. Instead of utilizing the teacher model (as in previous methods Truong et al. (2021b)), we propose to adopt the student model to train the generator in the first stage, which can solve the hard-label issue and largely reduce the number of queries to the teacher model. In the second stage, we train a student model that has similar predictions as the teacher model on the synthetic samples. As a result, IDEAL requires a much less query budget than previous methods, which saves a lot of money and becomes more practical in reality.\n\nIn summary, our main contributions include:\n\n• New Problem: We focus on how to query-efficiently train a good student model from blackbox models with only hard labels. To the best of our knowledge, our setting is the most practical and challenging to date.\n\n• More Efficient: We propose a novel method called IDEAL, which does not require any query in the data generation stage and queries the teacher only once for each sample in the distillation stage. Thus IDEAL can train a high-performance student with a small number of queries.\n\n1https://cloud.google.com/bigquery 2The detailed settings can be found in Section 4.1.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n• SOTA Results: Extensive experiments on various real-world datasets demonstrate the efficacy of our proposed IDEAL. For instance, IDEAL can improve the performance of the best baseline method (DFME) by 33.46% on MNIST dataset.\n\n2 RELATED WORKS\n\n2.1 WHITE-BOX DATA-FREE KNOWLEDGE DISTILLATION\n\nRecent advances in data-free knowledge distillation have enabled the compression of large neural networks into smaller networks without using any real data Micaelli & Storkey (2019); Fang et al. (2021b); Yin et al. (2020); Chen et al. (2019); Bhardwaj et al. (2019); Haroush et al. (2020); Yoo et al. (2019); Zhang et al.. Nevertheless, all of these approaches require access to the white-box teacher model and the logits (or probabilities) calculated by the teacher model, which is not always possible in realistic scenarios. For example, according to these approaches Fang et al. (2021b); Yin et al. (2020); Chen et al. (2019); Ye et al. (2020); Xu et al. (2020), the pre-trained teacher model is regarded as a discriminator, and then the generator is adversarially trained. As the teacher model is only accessible as a black-box model, it is not possible to propagate gradients in this manner. Furthermore, the widely used KL divergence is inapplicable, as the logits of the teacher model are not accessible. Additionally, some works utilize the specific structural information of the whitebox model, which also violate the black-box rules. For example, DeepIn Yin et al. (2020) used the running average statistics stored in the BatchNorm layers, while DAFL Chen et al. (2019) proposed to use features extracted by convolution filters. As a result of the above irrationality, these methods perform poorly in our settings. In more practical and challenging setting we considered in this work: data-free knowledge distillation from black-box models with only hard labels.\n\n2.2 DISTILLATION-BASED BLACK-BOX ATTACKS\n\nPrevious studies have explored several ways to attack black-box models without real-world training data, which can be roughly divided into transfer-based adversarial attack Zhou et al. (2020a); Yu & Sun (2022); Wang et al. (2021b) and data-free model extraction attack Truong et al. (2021a); Kariyappa et al. (2021). Essentially, these methods are based on data-free black-box model distillation. Actually, they are designed to train substitute models in black-box situations to attack the victim model. While these methods are suitable for black-box scenarios, they mainly rely on a score-based teacher which outputs class probabilities. By contrast, our study considers a much more challenging scenario, in which a black-box teacher only returns the top-1 class. Moreover, in real-world scenarios, these black-box models usually charge for each query. To achieve good performance, these methods require millions of queries, which consume a lot of computing resources and money in real-world scenarios.\n\n2.3 COMPARISON WITH RELATED WORKS\n\nThe most related work is ZSDB3 Wang (2021), which also studied data-free black-box distillation with hard labels. It proposes to generate pseudo samples distinguished by the teacher’s decision boundaries and then reconstruct the soft labels for distillation. More specifically, it calculates the minimal l2-norm distance between the current sample and those of other classes (measured by the teacher model) and uses the zeroth-order optimization method to estimate the gradient of the teacher model, which requires a large number of queries, making ZSBD3 not practical in real-world scenarios. By contrast, we consider a more challenging and practical setting where only a very small number of queries (to the teacher model) is allowed, i.e., query efficient. For example, ZSDB3 requires about 1000 queries to reconstruct the soft label (logits) of a single sample on MNIST dataset, while our method only requires one query, which hugely reduces the number of queries by 1000×.\n\n3 MAIN METHOD\n\n3.1 NOTATIONS\n\nWe use G, S, and T to denote the generator, the student model, and the teacher model, respectively. θG and θS denote the parameters of generator G and student model S, respectively. ˆx and ˆy denote\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Illustration of the training process of our proposed IDEAL. The left panel demonstrates the data generation stage. In this stage, we train a generator, that can generate desired synthetic samples, with the student model. The right panel shows the model distillation stage, which trains a student model that has similar predictions as the teacher model on the synthetic samples.\n\nthe synthetic sample (generated by G) and the corresponding prediction score. Subscript i denotes the i-th sample, e.g., ˆxi denotes the i-th synthetic sample. Superscript j denotes the j-th epoch, e.g., Dj denotes the set of synthetic samples generated in j-th epoch. C is the number of classes and B is the batch size. We use (ˆy)k to denote the k-th element of outputs ˆy, i.e., the prediction score of the k-th class.\n\n3.2 OVERVIEW\n\nIn data-free black-box KD, the teacher’s model and data are not accessible, and we are only given the prediction of a sample by the teacher model. In particular, we focus on a more practical and challenging setting where only a category index for each sample (i.e., hard-label) is given by the teacher model. Since each query to the teacher costs money, we consider the scenario with limited queries, i.e., query-efficient. Our goal is to query-efficiently learn from black-box models to train a good student without any real data.\n\nTo achieve this goal, we propose a novel method called IDEAL, which consists of two stages: a data generation stage and a model distillation stage. In the first stage, instead of utilizing the teacher model (as in previous methods Truong et al. (2021b); Kariyappa et al. (2021)), we propose to adopt the student model to train the generator, which can solve the hard-label issue and largely reduce the number of queries to the teacher model. In the second stage, we utilize the teacher model and synthetic samples to train the student model. The generator and student model are iteratively trained for E epochs. The training procedure is demonstrated in the Appendix (see Algorithm 1) and the illustration of the training process of IDEAL is shown in Fig. 1.\n\n3.3 DATA GENERATION\n\nIn data-free setting, we are unable to access the original training data for training the student model. Therefore, in the first stage, we aim to train a generator to generate the desired synthetic data (to train the student model). According to the finding in Zhang et al. (2022), we reinitialize the generator at each epoch. The data generation procedure is illustrated in Fig. 1(a).\n\nThe first step is generating the synthetic sample. Given a random noise z (sampled from a standard Gaussian distribution) and a corresponding random one-hot label y (sampled from a uniform distribution), the generator G aims to generate a desired synthetic sample ˆx corresponding to label y. Specifically, we feed z into the generator G and compute the synthetic sample as follows:\n\nˆx = G(z; θG),\n\n(1)\n\nwhere θG is the parameter of G. The synthetic samples are used to train G.\n\nIn the second step, we compute the prediction score of ˆx. A straightforward way is to use the teacher model to compute the prediction score. The prediction score is used to update θG, but the parameter\n\n4\n\nRandom NoiseSynthetic DataStudent (fixed)LogitsRandom LabelLLcccc+LLiiiiiiiiiiUpdateaaData GenerationTop-1 labelQuery the APIStudentDistillation LossUpdateBlack-box ModelbbModel DistillationSynthetic dataGeneratorLogitsPublished as a conference paper at ICLR 2023\n\nof the teacher model is not accessible in black-box setting, thus unable to conduct backpropagation. Previous black-box KD methods Truong et al. (2021b); Wang (2021); Kariyappa et al. (2021) used gradient estimation methods to obtain an approximate gradient. Nevertheless, they need to estimate the gradient from the black-box teacher model, which requires a large number of queries (to the teacher model), which is not practical. Moreover, in the hard-label setting, the prediction score is not accessible. To this end, we propose to use the student model (instead of the teacher model) to compute the prediction score of ˆx. The detail of the student model is discussed in Section 3.4. Note that in this stage, we do not train the student model and keep the parameter of the student model fixed. By utilizing the student model, we can directly conduct backpropagation and compute the gradient of the model without querying the teacher model. In this way, we can avoid the hard-label problem and the large number of queries at the same time. The prediction score is computed as follows:\n\nˆy = S(ˆx; θS ),\n\n(2)\n\nwhere S and θS are student model and model parameters.\n\nThe third step is optimizing the generator. We propose to train a generator that considers both confidence and balancing.\n\n3.3.1 CONFIDENCE\n\nFirst, we need to consider confidence, i.e., the synthetic sample is classified to the specified class with high confidence. To achieve this goal, we minimize the difference between the prediction score ˆy and the specified label y:\n\nLce = CE(ˆy, y),\n\n(3)\n\nwhere CE(·, ·) is the cross-entropy (CE) loss. Actually, in the training process, the generator G can quickly converge when using Lce. Since the generated data ˆx is fitted for the student S, and we intend to generate data according to the knowledge of the teacher’s model, we must avoid overfitting to S. Therefore, we need to control the number of iterations EG in data generation. Too few iterations may lead to poor data, while too many iterations may lead to overfitting. See the detailed experiments in the Section 4.1.\n\n3.3.2 BALANCING\n\nSecond, we need to consider balancing, i.e., the number of synthetic samples in each class should be balanced. Although we uniformly sample the specified label y, we observe that the prediction score ˆy is not balanced, i.e., the prediction score is high on some classes but low on the other classes. This leads to class imbalance of the generated synthetic samples. Motivated by Chen et al. (2019), we employ the information entropy loss to measure the class balance of the synthetic samples. In particular, given a batch of synthetic samples {ˆxi}B i=1, where B is the batch size, we first compute the average of the prediction scores as follows:\n\ni=1 and corresponding prediction scores {ˆyi}B\n\nˆyavg =\n\n1 B\n\nB (cid:88)\n\ni=1\n\nˆyi.\n\nThen, we compute the information entropy loss as follows:\n\nLinf o =\n\n1 C\n\nC (cid:88)\n\n(ˆyavg)k log((ˆyavg)k),\n\nk=1\n\n(4)\n\n(5)\n\nwhere (ˆyavg)k is the k-th element of ˆyavg, i.e., the average prediction score of the k-th class. When Linf o takes the minimum, each element in ˆyavg would equal to 1 C , which implies that G can generate synthetic samples of each class with an equal probability.\n\nBy combining the above losses, we can obtain the generator loss as follows:\n\nLgen = Lce + λLinf o,\n\n(6)\n\nwhere λ is the scaling factor. By minimizing Lgen, we train a generator that generates desired balanced synthetic samples.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n3.4 MODEL DISTILLATION\n\nIn the second stage, we train the student model S with teacher model T and the synthetic samples. The training process is illustrated in Fig. 1(b). Our goal is to obtain a student model S that has the same predictions as teacher model T on the synthetic samples (generated by generator G).\n\nIn particular, we first sample the random noise and generate synthetic sample ˆx with the generator. Second, we feed ˆx into the black-box teacher model and obtain its label as follows:\n\nyT = T (ˆx)\n\n(7)\n\nWe treat yT as the ground-truth label of ˆx. Since the teacher model only returns hard-label, yT is a ground-truth one-hot label. Afterwards, we feed ˆx into the student model and obtain the prediction score as follows:\n\nˆy = S(ˆx; θS ).\n\nLast, we optimize the student model by minimizing the CE loss as follows:\n\nLmd = CE(ˆy, yT )\n\n(8)\n\n(9)\n\nBy minimizing Lmd, the student model can have similar predictions as the teacher model on the synthetic samples, which leads to a desired student model.\n\nIn each epoch, our proposed IDEAL only queries the teacher model once for each sample, while previous black-box methods (e.g., ZSDB3 Wang (2021) and DFME Truong et al. (2021a)) may need more than 100 queries. Moreover, IDEAL requires similar epochs (compared with previous blackbox methods) to converge. As a result, IDEAL requires a much less query budget than previous methods, which saves a lot of money and becomes more practical in reality.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETUP\n\n4.1.1 DATASET AND MODEL ARCHITECTURE\n\nOur experiments are conducted on 7 real-world datasets: MNIST LeCun et al. (1998), FashionMNIST (FMNIST) Xiao et al. (2017), CIFAR10 and CIFAR100 Krizhevsky et al. (2009), SVHN Netzer et al. (2011), Tiny-ImageNet Le & Yang (2015), and ImageNet subset Deng et al. (2009). The ImageNet subset is generated by Li et al. (2021), which consists of 12 classes. We resize the original image with size 224*224*3 to 64*64*3 for fast training. Note that student models cannot access to any raw data during training. Only teacher models are trained on these datasets. In this work, we study the effectiveness of our method on several network architectures, including MLP Ruck et al. (1990), AlexNet Krizhevsky et al. (2012), LeNet Lecun et al. (1998), ResNet-18 He et al. (2016), VGG-16 Simonyan & Zisserman (2015), and ResNet-34 He et al. (2016). For each dataset, we train several different teacher models to evaluate the effectiveness of our method. We use the generator proposed in StyleGAN Karras et al. (2019) as the default generator.\n\n4.1.2 BASELINES\n\nAs discussed in the related work, we compare our approach with the following baselines: 1) SOTA data-free distillation methods that are originally designed for white-box scenarios (DAFL Chen et al. (2019), ZSKT Micaelli & Storkey (2019), DeepIn Yin et al. (2020), CMI Fang et al. (2021b)). Here we adapt them to the black-box scenarios in which only hard labels are provided. 2) Besides, we also compare with the SOTA methods in model extraction attack (DFME Truong et al. (2021a)) and transfer-based adversarial attack (DaST Zhou et al. (2020a)). In fact, these techniques are essential data-free distillation methods in black-box scenarios. 3) Furthermore, we compare our method with ZSDB3 Wang (2021), which also focuses on improving the performance of the black-box data-free distillation in label-only scenarios.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Accuracy (%) of student models trained with various teacher models on MNIST, FMNIST, SVHN, CIFAR10, and ImageNet subset. Best results are in bold. Best results of the baselines are underlined. “Improvement” denotes the improvements of IDEAL compared with the best baseline. Improvement\n\nCMI DaST ZSDB3 DFME Ours\n\nTeacher DAFL ZSKT DeepIn\n\nDataset\n\nModel\n\nMNIST\n\nFMNIST\n\nSVHN\n\nCIFAR10\n\nImageNet subset\n\nMLP LeNet AlexNet\n\nMLP LeNet AlexNet\n\nAlexNet VGG-16 ResNet-18\n\nAlexNet ResNet-34\n\nAlexNet VGG-16\n\n98.25 99.27 99.35\n\n84.54 90.23 92.66\n\n89.82 94.41 95.28\n\n84.76 93.85\n\n72.96 78.53\n\n16.97 18.92 20.43\n\n14.23 16.89 21.78\n\n16.89 19.24 21.25\n\n13.75 16.08\n\n17.15 19.36\n\n13.84 22.96 27.97\n\n16.86 18.52 20.22\n\n13.96 21.03 20.95\n\n12.56 14.31\n\n15.89 20.16\n\n16.68 24.43 29.54\n\n12.24 16.89 22.38\n\n16.71 24.65 24.75\n\n14.54 15.99\n\n17.75 19.66\n\n13.89 22.71 28.87\n\n10.30 14.44 21.24\n\n17.63 24.55 28.55\n\n13.98 15.95\n\n17.31 22.10\n\n15.62 22.49 23.86\n\n12.93 22.72 25.21\n\n24.47 25.17 24.33\n\n14.54 15.41\n\n16.96 22.03\n\n30.13 35.98 37.33\n\n24.52 32.46 34.47\n\n33.96 36.35 37.40\n\n29.38 32.18\n\n27.83 29.46\n\n56.32 62.86 66.45\n\n52.29 56.76 63.59\n\n58.92 62.53 64.82\n\n35.73 37.91\n\n32.89 34.65\n\n88.41 96.32 96.51\n\n76.95 83.92 86.14\n\n84.42 86.91 87.65\n\n65.61 68.82\n\n53.72 57.95\n\n32.09↑ 33.46↑ 30.06↑\n\n24.66↑ 27.16↑ 22.55↑\n\n25.50↑ 24.38↑ 22.83↑\n\n29.88↑ 30.91↑\n\n20.83↑ 23.30↑\n\n4.1.3 QUERY BUDGET AND TRAINING SETTINGS\n\nSince we consider the limited query budget scenario, we adopt the same query budget Q for all methods. In particular, we set the query budget Q = 25K for MNIST, Q = 100K for FMNIST and SVHN. Besides, the default query budget Q = 250K for CIFAR10 and ImageNet subset. For large datasets with a large number of classes (i.e., CIFAR100 and Tiny-ImageNet), we set the query budget Q = 2M . For our method, each sample only needs to query the teacher model once, so the total number of queries is Q = B × E, where B is the batch size and E denotes the training epochs. To update the generator, we use the Adam Optimizer with learning rate ηG = 1e − 3. To train the student model, we use the SGD optimizer with momentum=0.9 and learning rate ηS = 1e − 2. We set the batch size B = 250 for MNIST, FMNIST, SVHN, CIFAR10, and ImageNet subset, and B = 1000 for CIFAR100 and Tiny-ImageNet datasets. By default, we set the number of iterations in data generation EG = 5 and the scaling factor λ = 5. The number of epochs E is computed according to the query budget. For evaluation, We run experiments for 3 times, and report the average top-1 test accuracy.\n\n4.2 EXPERIMENTAL RESULTS\n\n4.2.1 PERFORMANCE COMPARISON ON SMALL DATASET\n\nFirst, we show the results of different KD methods on MNIST, FMNIST, SVHN, CIFAR10, and ImageNet subset using various teacher models in Table 2. From the table, we observe that:\n\n(1) Our proposed IDEAL outperforms all the baseline methods on all datasets. For instance, our method achieves 87.65% accuracy on SVHN dataset when the teacher model is ResNet-18, whereas the best baseline method DFME achieves only 64.82% accuracy under the same query budget. In general, IDEAL improves the performance of the best baseline by at least 20% under the same settings.\n\n(2) The black-box teacher models trained on MNIST are much easier for the student to learn. Even with very few queries, the student model of our proposed IDEAL achieves over 96% accuracy on MNIST. We argue that this is reasonable because this task is simple for neural networks to solve, and the underlying representations are easy to learn. However, even for such a simple task, other methods cannot derive a good student model with the same small query budget. For example, when learning from the black-box AlexNet trained on MNIST, the best baseline DFME only achieves 66.45% accuracy.\n\n(3) DAFL and ZSKT have the worst performance on all datasets. For example, the accuracy of ZSKT is only 12.56% when the teacher model is AlexNet on CIFAR10, which is close to random guess (10%). We conjecture this is because white-box KD methods are not suitable in black-box scenarios. These methods mainly depend on white-box information, such as model structure and probability or logits returned by the teacher model. Therefore, using these methods in black-box scenarios will significantly reduce their effectiveness.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Accuracy (%) of student models on datasets with hundreds of classes. We use ResNet-18 as the default student network, and all results are tested under the same query budget Q = 2M .\n\nDataset\n\nModel\n\nTeacher DAFL ZSKT DeepIn\n\nCMI DaST ZSDB3 DFME Ours\n\nImprovement.\n\nCIFAR100\n\nTiny-ImageNet\n\nAlexNet ResNet-34\n\nVGG-16 ResNet-34\n\n66.24 89.45\n\n52.96 64.53\n\n9.65 12.76\n\n6.51 9.78\n\n10.19 13.54\n\n7.24 10.35\n\n14.53 18.35\n\n11.95 15.79\n\n12.30 15.93\n\n9.79 12.82\n\n14.22 17.92\n\n8.42 11.21\n\n14.38 17.51\n\n10.98 12.16\n\n16.48 20.01\n\n15.06 17.64\n\n23.85 36.96\n\n21.72 27.95\n\n7.37↑ 16.95↑\n\n6.66↑ 10.31↑\n\n4.2.2 PERFORMANCE COMPARISON ON LARGE DATASETS\n\nIn addition to the performance on small datasets, the performance of the black-box distillation method on large datasets deserves further investigation. Data-free knowledge distillation has historically performed poorly Zhou et al. (2020a); Chen et al. (2019) for datasets with a large number of classes (e.g. Tiny-ImageNet and CIFAR100), since it is very difficult to generate synthetic data with particularly rich class diversity. Thus, we also conduct experiments on datasets with more classes (at least 100 classes). Table 3 demonstrates the results of all methods on CIFAR100 and Tiny-ImageNet. As shown in Table 3, it is also difficult for all these methods to produce a good student model in the black-box scenario. However, our proposed IDEAL consistently achieves the best performance on these large datasets. For example, IDEAL outperforms the best baseline DFME by 16.95% on CIFAR100 with ResNet-34. When compared with other baseline methods, our model achieves significant performance improvement by a large margin of over 18%.\n\n4.2.3 PERFORMANCE ON MICROSOFT AZURE\n\nIn particular, we adopt\n\nFollowing the settings in DaST Zhou et al. (2020a), we also conduct KD in a real-world the API scenario. provided by Microsoft Azure3 (trained on MNIST dataset) as the teacher model and utilize LeNet Lecun et al. (1998) as the student model. As illustrated in Fig. 2, our method converges quickly and is very stable compared to other methods. Actually, our method achieves over 98% test accuracy after 10,000 queries, which implies that our proposed method is also effective and efficient for real-world APIs.\n\nFigure 2: Transferring the knowledge of the online model on Microsoft Azure to the student.\n\n4.2.4 PERFORMANCE UNDER DIFFERENT QUERY BUDGET\n\nIn previous experiments, we consider training the student model with a limited query budget. As described in previous studies Zhou et al. (2020a); Truong et al. (2021a); Wang (2021), these methods require millions of queries to the black-box model. Therefore, we have increased the number of queries of other baseline methods to provide a more comprehensive comparison, but without increasing the number of queries in our method. More specifically, we increase the number of queries required by other baseline methods (ZSDB3 and DFME) on CIFAR10 dataset from 200K to 10M . Fig. 3 illustrates the training curves of these methods with Q = 200K and Q = 10M , respectively. Note that ZSDB3, DFME can achieve the highest accuracy of 56.39% and 57.94% respectively (right panel in Fig. 3), when a large number of queries are involved. By contrast, our approach achieves 63.77% with only 0.02× the query budget of both ZSDB3 and DFME. It validates the effectiveness of our method to perform query-efficient KD.\n\nFigure 3: Analyses of our method and other comparison methods (ZSDB3, DFME) with a small query budget (Q = 200K) and a large query budget (Q = 10M ).\n\n3https://azure.microsoft.com/en-us/services/machine-learning/\n\n8\n\n4XHU\\×$FFXUDF\\'D67=6'%')0(2XUV4XHU\\×.7HVW$FFXUDF\\4XHU\\EXGJHWQ=.=6'%')0(2XUV4XHU\\×.4XHU\\EXGJHWQ=0=6'%')0(Published as a conference paper at ICLR 2023\n\nTable 4: Ablation studies by cutting of different modules.\n\nMethod\n\nMNIST SVHN CIFAR10\n\nImageNet subset\n\nOurs w/o Linf or w/o Lce w/o generator re-initializing\n\n96.32 95.76 15.21 95.26\n\n86.91 83.21 12.48 80.53\n\n68.82 62.68 10.68 55.32\n\n57.95 54.31 9.84 51.25\n\n4.2.5 VISUALIZATION OF SYNTHETIC DATA\n\nIn this subsection, we present some synthesised examples of ZSDB3, DFME, and our method to evaluate the visual diversity. As can be seen in Fig. 4, images generated by ZSDB3 are all of very low quality, which cannot show any meaningful patterns. And the image samples generated by ZSDB3 and DFME both exhibit very similar patterns, which implies that the synthetic data has low sample diversity. By contrast, our proposed approach can synthesize more meaningful and diverse data. We observe that the images generated by our method have more different patterns, which indicates that our proposed IDEAL can synthesize more diverse data. It also proves that it is feasible and effective for our model to replace T with S in generator training without gradient estimation.\n\nFigure 4: Visualization of data generated by different methods on MNIST. Our approach can synthesize more diverse data, there is a clear visual distinction between samples in different classes.\n\n4.2.6 EFFECT INVESTIGATION OF DIFFERENT MODULES\n\nIn this section, we evaluate the contributions of different loss functions in Equation 6 used during data generation, and discuss the effect of re-initializing the generator. As shown in Table 4, removing both the generator and information loss Linf or can lead to significant performance degradation. Moreover, our model suffers from an obvious degradation when the generator re-initializing strategy is abandoned, especially on SVHN, CIFAR10, and ImageNet-subset. In fact, since the generator is reinitialized in each epoch during training, our method does not depend on the generator from the previous round. In other words, we do not need to train the generator and the student model adversarially, and therefore we do not require a large number of training iterations to guarantee convergence. Besides, we find a significant degradation when we remove Lce, which demonstrates its effectiveness in the data generation. The ablation experiments verify that all modules are essential in our method.\n\n4.2.7 EFFECT INVESTIGATION OF EG\n\nWe also conduct ablation study to investigate the effect of EG on the data generation stage. As show in Table 5 in Appendix, we modify the value of EG and report the top-1 test accuracy. We can observe that too small or too large EG is hard to obtain the optimal solution. To better understand the impact of EG, we show the t-SNE visualization of synthetic data in Fig. 5 in Appendix. More detailed results can be referred to the Appendix A.0.1.\n\n5 CONCLUSION\n\nIn this paper, we propose query-effIcient Data-free lEarning from blAck-box modeLs (IDEAL) in order to query-efficiently train a good student model from black-box teacher models under the datafree and hard-label setting. To the best of our knowledge, our setting is the most practical and challenging to date. Extensive experiments on various real-world datasets show the effectiveness of our proposed IDEAL. For instance, IDEAL can improve the performance of the best baseline method DFME by 5.83% on CIFAR10 dataset with only 0.02× the query budget of DFME. We envision this work as a milestone for query-efficient and data-free learning from black-box models.\n\n9\n\nZSDB3OursDFME0123456789Published as a conference paper at ICLR 2023\n\n6 ACKNOWLEDGEMENT\n\nThis work is funded by Sony AI.\n\nREFERENCES\n\nKartikeya Bhardwaj, Naveen Suda, and Radu Marculescu. Dream distillation: A data-independent model compression framework. CoRR, abs/1905.07072, 2019. URL http://arxiv.org/ abs/1905.07072.\n\nHanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, and Qi Tian. Data-free learning of student networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3514–3522, 2019.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255, 2009.\n\nGongfan Fang, Yifan Bao, Jie Song, Xinchao Wang, Donglin Xie, Chengchao Shen, and Mingli Song. Mosaicking to distill: Knowledge distillation from out-of-domain data. Advances in Neural Information Processing Systems, 34, 2021a.\n\nGongfan Fang, Jie Song, Xinchao Wang, Chengchao Shen, Xingen Wang, and Mingli Song. Contrastive model inversion for data-free knowledge distillation. CoRR, abs/2105.08584, 2021b.\n\nJianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A\n\nsurvey. International Journal of Computer Vision, 129(6):1789–1819, 2021.\n\nMatan Haroush, Itay Hubara, Elad Hoffer, and Daniel Soudry. The knowledge within: MethIn 2020 IEEE/CVF Conference on Computer Vision ods for data-free model compression. and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 8491–8499. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.00852. URL https://openaccess.thecvf.com/content_CVPR_2020/html/Haroush_ The_Knowledge_Within_Methods_for_Data-Free_Model_Compression_ CVPR_2020_paper.html.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR recognition. 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770–778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.\n\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\n\npreprint arXiv:1503.02531, 2015.\n\nYuenan Hou, Zheng Ma, Chunxiao Liu, Tak-Wai Hui, and Chen Change Loy.\n\nInterIn 2020 IEEE/CVF Conregion affinity distillation for road marking segmentation. ference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 12483–12492. Computer Vision Foundation / IEEE, 2020. doi: https://openaccess.thecvf.com/ 10.1109/CVPR42600.2020.01250. URL content_CVPR_2020/html/Hou_Inter-Region_Affinity_Distillation_ for_Road_Marking_Segmentation_CVPR_2020_paper.html.\n\nSeongKu Kang, Junyoung Hwang, Wonbin Kweon, and Hwanjo Yu. De-rrd: A knowledge disIn Proceedings of the 29th ACM International\n\ntillation framework for recommender system. Conference on Information & Knowledge Management, pp. 605–614, 2020.\n\nSanjay Kariyappa, Atul Prakash, and Moinuddin K. Qureshi. MAZE: data-free model stealing In IEEE Conference on Computer Vision and attack using zeroth-order gradient estimation. Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 13814–13823. Computer Vision Foundation / IEEE, 2021. URL https://openaccess.thecvf.com/content/ CVPR2021/html/Kariyappa_MAZE_Data-Free_Model_Stealing_Attack_ Using_Zeroth-Order_Gradient_Estimation_CVPR_2021_paper.html.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for genIn IEEE Conference on Computer Vision and Pattern erative adversarial networks. Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 4401–4410. Computer Vision Foundation / IEEE, 2019. URL http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_ Style-Based_Generator_Architecture_for_Generative_Adversarial_ Networks_CVPR_2019_paper.html.\n\n10.1109/CVPR.2019.00453.\n\ndoi:\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L ́eon Bottou, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pp. 1106–1114, 2012. URL https://proceedings.neurips.cc/paper/2012/hash/ c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\n\nWonbin Kweon, SeongKu Kang, and Hwanjo Yu. Bidirectional distillation for top-k recommender system. In Jure Leskovec, Marko Grobelnik, Marc Najork, Jie Tang, and Leila Zia (eds.), WWW ’21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021, pp. 3861– 3871. ACM / IW3C2, 2021. doi: 10.1145/3442381.3449878. URL https://doi.org/10. 1145/3442381.3449878.\n\nYa Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.\n\nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-\n\nnition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791.\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, pp. 2278–2324, 1998.\n\nBoren Li, Po-Yu Zhuang, Jian Gu, Mingyang Li, and Ping Tan.\n\nInterpretable foreground object search as knowledge distillation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and JanMichael Frahm (eds.), Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXVIII, volume 12373 of Lecture Notes in Computer Science, pp. 189–204. Springer, 2020. doi: 10.1007/978-3-030-58604-1\\ 12. URL https: //doi.org/10.1007/978-3-030-58604-1_12.\n\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learning: Training clean models on poisoned data. Advances in Neural Information Processing Systems, 34, 2021.\n\nPaul Micaelli and Amos J. Storkey. Zero-shot knowledge transfer via adversarial belief matching. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ́e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems, pp. 9547–9557, 2019.\n\nJonghwan Mun, Kimin Lee, Jinwoo Shin, and Bohyung Han.\n\nLearning to specialize with knowledge distillation for visual question answering. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr ́eal, Canada, pp. 8092–8102, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 0f2818101a7ac4b96ceeba38de4b934c-Abstract.html.\n\nNdapandula Nakashole and Raphael Flauger. Knowledge distillation for bilingual dictionary induction. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pp. 2497–2506. Association for Computational Linguistics, 2017. doi: 10.18653/v1/d17-1264. URL https://doi.org/10.18653/v1/d17-1264.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading\n\ndigits in natural images with unsupervised feature learning. 2011.\n\nNikolaos Passalis, Maria Tzelepi, and Anastasios Tefas. Heterogeneous knowledge distillation In 2020 IEEE/CVF Conference on Computer Vision and using information flow modeling. Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 2336–2345. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.00241. URL https://openaccess.thecvf.com/content_CVPR_2020/html/Passalis_ Heterogeneous_Knowledge_Distillation_Using_Information_Flow_ Modeling_CVPR_2020_paper.html.\n\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412. 6550.\n\nDennis W Ruck, Steven K Rogers, and Matthew Kabrisky. Feature selection using a multilayer\n\nperceptron. Journal of Neural Network Computing, 2(2):40–48, 1990.\n\nJiayi Shen, Haotao Wang, Shupeng Gui, Jianchao Tan, Zhangyang Wang, and Ji Liu. UMEC: unified model and embedding compression for efficient recommendation systems. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=BM---bH_RSh.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.1556.\n\nJean-Baptiste Truong, Pratyush Maini, Robert J. Walls, and Nicolas Papernot. Data-free model extraction. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 4771–4780, 2021a.\n\nJean-Baptiste Truong, Pratyush Maini, Robert J Walls, and Nicolas Papernot. Data-free model extraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4771–4780, 2021b.\n\nShuai Wang, Kun Zhang, Le Wu, Haiping Ma, Richang Hong, and Meng Wang. Privileged graph distillation for cold start recommendation. In Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (eds.), SIGIR ’21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, pp. 1187–1196. ACM, 2021a. doi: 10.1145/3404835.3462929. URL https: //doi.org/10.1145/3404835.3462929.\n\nWenxuan Wang, Bangjie Yin, Taiping Yao, Li Zhang, Yanwei Fu, Shouhong Ding, Jilin Li, Feiyue Huang, and Xiangyang Xue. Delving into data: Effectively substitute training for In IEEE Conference on Computer Vision and Pattern Recognition, CVPR black-box attack. 2021, virtual, June 19-25, 2021, pp. 4761–4770. Computer Vision Foundation / IEEE, 2021b. https://openaccess.thecvf.com/content/CVPR2021/html/Wang_ URL Delving_into_Data_Effectively_Substitute_Training_for_Black-box_ Attack_CVPR_2021_paper.html.\n\nZi Wang. Zero-shot knowledge distillation from a decision-based black-box model. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139, pp. 10675–10685, 2021.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-\n\ning machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n\nShoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, and Mingkui Tan. Generative low-bitwidth data free quantization. In Andrea Vedaldi, Horst Bischof, Thomas\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nBrox, and Jan-Michael Frahm (eds.), Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XII, volume 12357 of Lecture Notes in Computer Science, pp. 1–17. Springer, 2020. doi: 10.1007/978-3-030-58610-2\\ 1. URL https://doi.org/10.1007/978-3-030-58610-2_1.\n\nJingwen Ye, Yixin Ji, Xinchao Wang, Xin Gao, and Mingli Song. Data-free knowledge amalIn 2020 IEEE/CVF Conference on Computer Vision gamation via group-stack dual-gan. and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 12513– 12522. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01253. https://openaccess.thecvf.com/content_CVPR_2020/html/Ye_ URL Data-Free_Knowledge_Amalgamation_via_Group-Stack_Dual-GAN_CVPR_ 2020_paper.html.\n\nHongxu Yin, Pavlo Molchanov, Jose M. Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K. In Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8712–8721, 2020.\n\nJaemin Yoo, Minyong Cho, Taebum Kim, and U Kang.\n\nKnowledge extraction with no In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Floobservable data. rence d’Alch ́e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 2701–2710, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 596f713f9a7376fe90a62abaaedecc2d-Abstract.html.\n\nMengran Yu and Shiliang Sun. Fe-dast: Fast and effective data-free substitute training for black-box adversarial attacks. Comput. Secur., 113:102555, 2022. doi: 10.1016/j.cose.2021.102555. URL https://doi.org/10.1016/j.cose.2021.102555.\n\nJie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chunhua Shen, and Chao Wu. Dense: Data-free one-shot federated learning. In Advances in Neural Information Processing Systems.\n\nJie Zhang, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, Lei Zhang, and Chao Wu. Towards efficient data free black-box adversarial attack. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15115–15125, June 2022.\n\nMingyi Zhou, Jing Wu, Yipeng Liu, Shuaicheng Liu, and Ce Zhu. Dast: Data-free substitute training for adversarial attacks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 231–240, 2020a.\n\nYuanen Zhou, Meng Wang, Daqing Liu, Zhenzhen Hu, and Hanwang Zhang. More grounded image captioning by distilling image-text matching model. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 4776– 4785. Computer Vision Foundation / IEEE, 2020b. doi: 10.1109/CVPR42600.2020.00483. URL https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_More_ Grounded_Image_Captioning_by_Distilling_Image-Text_Matching_ Model_CVPR_2020_paper.html.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.0.1 EFFECT INVESTIGATION OF EG .\n\nIn Fig. 5, clearly, the student can easily identify the synthetic data when EG = 50 (the training accuracy on synthetic data is 100%, while the test accuracy on CIFAR10 is 58.69%), but when EG = 10, the student cannot distinguish the data accurately (the training accuracy is 63.78%, while the test accuracy is 68.82%). We guess that, a small value of EG leads to poor quality of the generated data (a large loss) while a large value of EG leads to a student model that overfits to the synthetic data. Thus, from the empirical experiments in Table 5, we set EG = 5 for MNIST, EG = 10 for CIFAR10 and ImageNet subset.\n\nIterations (EG)\n\nMNIST\n\nTeacher\n\nAlexNet\n\nStudent\n\n3\n\n5\n\n10\n\n30\n\n50\n\nLeNet\n\n65.36\n\n96.51\n\n95.24\n\n88.75\n\n80.15\n\nCIFAR10\n\nResNet-34 ResNet-18\n\n30.25\n\n57.56\n\n68.82\n\n62.45\n\n58.69\n\nImageNet subset\n\nVGG-16\n\nResNet-18\n\n26.38\n\n52.59\n\n57.95\n\n48.67\n\n41.73\n\nTable 5: The influence of different number of iterations EG on data generation. We report the top-1 test accuracy (%).\n\nFigure 5: T-SNE visualization of synthetic data on CIFAR10 and the corresponding training loss of the generator. When EG = 10, the features are not well separated, indicating that the student can still learn from synthetic data.\n\nA.0.2 EFFECT OF THE GENERATOR\n\nFor fair comparisons, we use the same generator StyleGan for all methods in our experiments. We also introduce the effects of different sizes of generators as shown in Table 6, where DCGAN, StyleGAN and Transformer-GAN have small, medium and large parameters. Different generative models have negligible effect on the performance of our method. Besides, our method still outperforms the best baseline when using generators with different sizes.\n\nA.0.3 CLASS IMBALANCE IN SYNTHETIC DATA\n\nTo avoid class imbalance, we generate the same number of samples per class. As illustrated in Table 7, even if we use some SOTA re-weighting methods to assign different weights to our model, the accuracy drop caused by the class imbalance can not be entirely eliminated. Hence, it is effective to consider class-balanced generation for each class, i.e., the number of synthetic samples per class is balanced.\n\nGenerator DCGAN StyleGAN Transformer-based GAN\n\nDFME\n\nOurs\n\n31.23\n\n55.81\n\n34.65\n\n57.95\n\n33.61\n\n57.62\n\nTable 6: The effect of the generator.\n\n14\n\n10 iterations 50 iterations Loss of generatorPublished as a conference paper at ICLR 2023\n\nMethod\n\nOurs+LDAM\n\nOurs+CB-Focal\n\nOurs(same number of samples in each class)\n\nCIFAR10\n\nSVHN\n\n62.14\n\n63.31\n\n68.82\n\n82.24\n\n81.58\n\n87.65\n\nTable 7: The influence of class imbalance in synthetic data.\n\nMethod\n\nModel training\n\nMethod A\n\nTrain from scratch with the synthetic data\n\nMethod B Using out-of-domain data to distill (CIFAR10)\n\nOurs\n\nUsing synthetic data to distill (our method)\n\nSVHN\n\n22.64\n\n39.68\n\n87.65\n\nTable 8: Domain gap between synthetic data and original data\n\nA.0.4 DOMAIN GAP BETWEEN SYNTHETIC DATA AND ORIGINAL DATA\n\nWe find that there is a significant difference between the data synthesized by our method and the test set. As shown in Table 8, we introduce more detailed experiments to investigate such distribution discrepancy. Specifically, (1) Method A denotes the performance when training from scratch with the synthetic data (i.e. no distillation) and directly evaluating on the test set. (2) Method B denotes the performance of using out-of-domain CIFAR10 (i.e. no synthetic data) to perform knowledge distillation. (3) and Ours denotes the performance of using synthetic data to perform distillation.\n\nObviously, Ours outperforms Method A by a large margin over 65%. It validates the significant distribution discrepancy between the synthetic data and the test set, and Ours can effectively address such domain gap via knowledge distillation. Besides, Ours performs better than domain adaptation strategy Method B, which also verifies the effectiveness of our model for black-box.\n\nA.0.5 DETAILED ALGORITHM\n\nAlgorithm 1 Training process of IDEAL Input: Generator G with parameter θG, student model S with parameter θS , teacher model T , number of training rounds EG for generator in each epoch, number of classes C, training epochs E, learning rate of generator ηG, learning rate of student model ηS , scaling factor λ, and batch size B.\n\nfor e = 1, · · · , E do\n\n// Stage 1: data generation for round = 1, · · · , EG do\n\nSample a batch of noises and labels {zi, yi}B Generate a batch of synthetic samples {ˆxi}B Compute Lgen by Equation 6 Update θG by minimizing Lgen\n\ni=1\n\ni=1 by Equation 1\n\nend for\n\n// Stage 2: model distillation Sample a batch of noises {zi}B Generate a batch of synthetic samples {ˆxi}B Compute Lmd by Equation 9 Update θS by minimizing Lmd\n\ni=1\n\ni=1 by Equation 1\n\nend for return θS\n\n15",
    "reference": "# Summary Of The Paper\n\nAuthors propose the “IDEAL” algorithm that can improve the distillation process when there is no data to be used for the distillation process, and also when the teacher provides only the hard-labels (no soft predictions over the labels). IDEAL employs a generator to generate examples on-the-fly in this data-free scenario. The generator aims to balance the class distribution and fit to one of the classes while the distillation progresses. In this particular setting (no-data and hard-labels), authors claim IDEAL can reduce the number of examples to be annotated significantly and still can achieve better performance on the given task.\n\n# Strength And Weaknesses\n\n**Strength**\n* Empirically strong in the particular distillation setup (no data, teacher can only provide hard-labels).\n* Utilizing the generator in the data-limited distillation setup has some novelty, particularly in the joint training of the generator during the distillation process.\n\n**Weakness**\n* The impact is limited on the very specific distillation setup (data-free & hard-label setting). Moreover, the algorithm can only be applicable for the classification set-up, and does not seem to scale up when there are a large set of labels (thousands or millions).\n* Authors seem to be over-claiming their impact. Particularly, I disagree with the author's claim that most KD assumes “users can directly access teacher’s training data.” Widely, KD focuses on a large set of unlabeled data because a teacher can provide pseudo-labels for them.\n* Writings can be improved to focus on their core motivation, core idea, and core techniques. Please see the “clarity/quality section.” Because of this reason, many of the design choices in Section 3 feel a bit arbitrary.\n* One of the baseline authors chosen, ZSDB3KD does seem to show 96.54% in their paper unlike the very low number in the current paper. Why is the number significantly different?\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity/Quality**\n* Abstract and introduction are unclear how exactly this work differs from prior sample-efficient distillation works. Particularly, the abstract focuses mostly on problem settings, not on motivation of this work, nor high-level sketch of what they are trying to do (they just simply refer to it works with two stages (“data generation stage and queries the teacher only once ...”).\n* Some terminologies are also misleading. For example, “white-box” and “black-box” refers to whether we have prior knowledge of model internals such as function classes or main assumptions. It does not mean the model class produces only the hard-labels (top1) or not (soft-labels). Hence, many classic distillation setup treats teacher models as black-box models (such as the original softmax-based distillation in Hinton’s paper [1] ).\n* Minor question: What are the teacher models used in Section 4?\n\n**Novelty**\nThere are mainly two lines of work that are closely related for this paper.\n* [2] is a very close work that is in the same problem formulation (zero-shot, black-box, and decision base) -- hence, the problem formulation is not novel unlike what authors claim in Section 1.\n* [3,4] proposes to use generators during the training and [4] particularly introduced during the distillation.\nThis paper is basically the combination of the two lines of work, and its novelty is in proposing the joint training of the generator that can maintain the class balance and the confidence, for the zero-shot setting. Hence, the paper does have some novelty.\n\n[1] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 2.7 (2015).\n[2] Wang, Zi. \"Zero-shot knowledge distillation from a decision-based black-box model.\" International Conference on Machine Learning. PMLR, 2021.\n[3] He, Xuanli, et al. \"Generate, annotate, and learn: Generative models advance self-training and knowledge distillation.\" (2021).\n[4] Zaheer, Manzil, et al. \"Teacher Guided Training: An Efficient Framework for Knowledge Transfer.\" arXiv preprint arXiv:2208.06825 (2022).\n\n# Summary Of The Review\n\nAs discussed above, the paper has some contributions to the community. However, the paper does seem to be limited in a very specific setting and lack the clarity/quality for the acceptance bar.\n\n---\n\nPost-rebuttal: After reading author's response as well as other reviewer's recommendation, I agree that the paper has some original contribution to the community (although it's limited). Hence, I updated the score accordingly.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nARE MORE LAYERS BENEFICIAL TO GRAPH TRANSFORMERS?\n\nHaiteng Zhao1∗, Shuming Ma2, Dongdong Zhang2, Zhi-Hong Deng1†, Furu Wei2 1 Peking University 2 Microsoft Research {zhaohaiteng,zhdeng}@pku.edu.cn {shumma,dozhang,fuwei}@microsoft.com\n\nABSTRACT\n\nDespite that going deep has proven successful in many neural architectures, the existing graph transformers are relatively shallow. In this work, we explore whether more layers are beneficial to graph transformers, and find that current graph transformers suffer from the bottleneck of improving performance by increasing depth. Our further analysis reveals the reason is that deep graph transformers are limited by the vanishing capacity of global attention, restricting the graph transformer from focusing on the critical substructure and obtaining expressive features. To this end, we propose a novel graph transformer model named DeepGraph that explicitly employs substructure tokens in the encoded representation, and applies local attention on related nodes to obtain substructure based attention encoding. Our model enhances the ability of the global attention to focus on substructures and promotes the expressiveness of the representations, addressing the limitation of self-attention as the graph transformer deepens. Experiments show that our method unblocks the depth limitation of graph transformers and results in stateof-the-art performance across various graph benchmarks with deeper models.\n\n1\n\nINTRODUCTION\n\nTransformers have recently gained rapid attention in modeling graph-structured data (Zhang et al., 2020; Dwivedi & Bresson, 2020; Maziarka et al., 2020; Ying et al., 2021; Chen et al., 2022). Compared to graph neural networks, graph transformer implies global attention mechanism to enable information passing between all nodes, which is advantageous to learn long-range dependency of the graph stuctures (Alon & Yahav, 2020). In transformer, the graph structure information can be encoded into node feature (Kreuzer et al., 2021) or attentions (Ying et al., 2021) by a variant of methods flexibly with strong expressiveness, avoiding the inherent limitations of encoding paradigms that pass the information along graph edges. Global attention (Bahdanau et al., 2015) also enables explicit focus on essential parts among the nodes to model crucial substructures in the graph.\n\nGraph transformer in current studies is usually shallow, i.e., less than 12 layers. Scaling depth is proven to increase the capacity of neural networks exponentially (Poole et al., 2016), and empirically improve transformer performance in natural language processing (Liu et al., 2020a; Bachlechner et al., 2021) and computer vision (Zhou et al., 2021). Graph neural networks also benefit from more depth when properly designed (Chen et al., 2020a; Liu et al., 2020b; Li et al., 2021). However, it is still not clear whether the capability of graph transformers in graph tasks can be strengthened by increasing model depth. So we conduct experiments and find that current graph transformers encounter the bottleneck of improving performance by increasing depth. The further deepening will hurt performance when model exceeds 12 layers, which seems to be the upper limit of the current graph transformer depth, as Figure 1 (left) shows.\n\nIn this work, we aim to answer why more self-attention layers become a disadvantage for graph transformers, and how to address these issues with the proper model design. Self-attention (Bahdanau et al., 2015) makes a leap in model capacity by dynamically concentrating on critical parts\n\n∗Work done during internship at Microsoft †Corresponding Author\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Left: Performance on ZINC dataset of different graph transformers by varying their depths. Our DeepGraph successfully scales up the depth, while the baselines can not. (Lower is better.) Right: Layer attention capacity to substructures with depth.\n\n(Chen et al., 2022), i.e., substructures of a graph, and obtaining particular features. Substructures are the basic intrinsic features of graph data widely used in data analysis (Yu et al., 2020) and machine learning (Shervashidze et al., 2009), as well as graph model interpretability (Miao et al., 2022). Although the self-attention module appears to be very beneficial for automatically learning important substructure features in the graph, our analysis indicates that this ability vanishes as depth grows, restricting the deeper graph transformer from learning useful structure features. Specifically, we focus on the influence of attention on different substructures, which we found to decrease after each self-attention layer. In consequence, it is difficult for deep models to autonomously learn effective attention patterns of substructures and obtain expressive graph substructure features.\n\nWe further propose a graph transformer model named DeepGraph with a simple but effective method to enhance substructure encoding ability of deeper graph transformer. The proposed model explicitly introduces local attention mechanism on substructures by employing additional substructure tokens in the model representation and applying local attention to nodes related to those substructures. Our method not only introduces the substructure based attention to encourage the model to focus on substructure feature, but also enlarges the attention capacity theoretically and empirically, which improves the expressiveness of representations learned on substructures.\n\nIn summary, our contributions are as follows:\n\n• We present the bottleneck of graph transformers’ performance when depth increases, illustrating the depth limitation of current graph transformers. We study the bottleneck from the perspective of attention capacity decay with layers theoretically and empirically, and demonstrate the difficulty for deep models to learn effective attention patterns of substructures and obtain informative graph substructure features.\n\n• According to the above finding, we propose a simple yet effective local attention mechanism based on substructure tokens, promoting focus on local substructure features of deeper graph transformer and improving the expressiveness of learned representations.\n\n• Experiments show that our method unblocks the depth limitation of graph transformer and\n\nachieves state-of-the-art results on standard graph benchmarks with deeper models.\n\n2 RELATED WORK\n\nGraph transformers Transformer with the self-attention has been the mainstream method in nature language processing (Vaswani et al., 2017; Devlin et al., 2019; Liu et al., 2019), and is also proven competitive for image in computer vision (Dosovitskiy et al., 2020). Pure transformers lack relation information between tokens and need position encoding for structure information. Recent works apply transformers in graph tasks by designing a variety of structure encoding techniques. Some works embed structure information into graph nodes by methods including Laplacian vector, random walk, or other feature (Zhang et al., 2020; Dwivedi & Bresson, 2020; Kreuzer et al., 2021; Kim et al., 2022; Wu et al., 2021). Some other works introduce structure information into attention by graph distance, path embedding or feature encoded by GNN (Park et al., 2022; Maziarka et al., 2020; Ying et al., 2021; Chen et al., 2022; Mialon et al., 2021; Choromanski et al., 2022). Other works use transformer as a module of the whole model (Bastos et al., 2022; Guo et al., 2022).\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nDeep neural networks There are many works focus on solving obstacles and release potential of deep neural networks in feed-forward network (FNN) (Telgarsky, 2016; Yarotsky, 2017) and convolutional neural network (CNN) (He et al., 2016a; Simonyan & Zisserman, 2014; Xiao et al., 2018; He et al., 2016b). For graph neural networks, early studies reveal severe performance degradation when stacking many layers (Li et al., 2019; Xu et al., 2018b; Li et al., 2018), caused by problems including over-smoothing and gradient vanishing (Oono & Suzuki, 2019; Zhao & Akoglu, 2019; Rong et al., 2019; Nt & Maehara, 2019). Recent works alleviate these problems by residual connection, dropout, and other methods (Chen et al., 2020a; Li et al., 2020; 2021), and deepen GNN into hundreds and even thousands of layers with better performance. Deep transformer is also proved powerful in nature language processing (Wang et al., 2022) and computational vision (Zhou et al., 2021), while better initialization (Zhang et al., 2019) or architecture (Wang et al., 2019; Liu et al., 2020a; Bachlechner et al., 2021) are proposed to solve optimization instability and over-smoothing (Shi et al., 2021; Wang et al., 2021; Dong et al., 2021).\n\nGraph substructure Substructure is one of the basic intrinsic features of graph data, which is widely used in both graph data analysis (Shervashidze et al., 2009; Yanardag & Vishwanathan, 2015; Rossi et al., 2020) and deep graph neural models (Chen et al., 2020b; Bouritsas et al., 2022; Bodnar et al., 2021; Zhang & Li, 2021; Zhao et al., 2021). In application, substructure related methods are widely used in various domain including computational chemistry (Murray & Rees, 2009; Jin et al., 2018; 2019; Duvenaud et al., 2015; Yu et al., 2020), computational biology (Koyut ̈urk et al., 2004) and social network (Jiang et al., 2010). Certain substructures can also be the pivotal feature for graph property prediction, which is a fundamental hypothesis in graph model interpretability studies (Ying et al., 2019; Miao et al., 2022), helping to understand how a graph model makes decisions.\n\n3 PRELIMINARY\n\n3.1 GRAPH AND SUBSTRUCTURE\n\nWe denote graph data as {Gi, yi}, where a graph G = (N, R, x, r) includes nodes N = {1 . . . |G|} and corresponding edges R ⊂ N × N , while x and r are node features and edge features. Label y can be graph-wise or node-wise, depending on the task definition. Given graph G, a substructure is defined as GS = {N S, RS, xS, rS}, where N S ⊂ N, RS = (N S × N S) ∩ R, i.e. nodes of GS form a subset of the graph G and edges are all the existing edges in G between nodes subset, which is also known as induced subgraph. Because attention is only applied to graph nodes, attention to arbitrary subgraphs is not well-defined. Therefore, we only consider induced subgraphs in this work.\n\n3.2 TRANSFORMER\n\nThe core module of the transformer is self-attention. Let Hl = [h1, · · · , hn]⊤ ∈ Rn×dh be the hidden representation in layer l, where n is the number of token, and dh is the dimension of hidden emˆAttn with parameter W V , W Q, W K ∈ Rdh×dk bedding of each token. The self-attention mapping and W O ∈ Rdk×dh is\n\nˆA =\n\n(HlW Q)(HlW K )⊤ dk\n\n√\n\n, A = softmax( ˆA),\n\n(1)\n\nˆAttn(Hl) = AHlW V W O = AHlW V O,\n\nwhere W V O ≜ W V W O. In practice, a complete transformer layer also contains a two-layer fullyconnected network FFN, and layer normalization with residual connection LN is also applied to both self-attention module and fully-connected network:\n\nHl+1 = FFN(Attn(Hl)),\n\n(2)\n\nwhere Attn(H) = LN(AHW V O), FFN(H ′) = LN(ReLU(H ′W F1 + 1bF1 LN(f (X)) = (X + f (X) − 1bN T normalizing coefficients.\n\n). )D is the layer normalization, where D is diagonal matrix with\n\n)W F2 + 1bF2\n\nT\n\nT\n\nFor graph transformer, structure information can be encoded into token representations or attentions. Our model adopts the distance and path-based relative position encoding methods as Ying et al. (2021), using graph distance DIS and shortest path information SP as relative position:\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nˆAij =\n\n(cid:0)hiW Q(cid:1) (cid:0)hjW K (cid:1)T √\ndk\n\n+ bD\n\nDIS(i,j) + Mean\n\nk∈SP(i,j)\n\nbR rk\n\n(3)\n\nWe also employ deepnorm (Wang et al., 2022) residual connection method in out model, which adjust the residual connection in layer normalization by constant η: LN(f (X)) = (ηX + f (X) − 1bN T\n\n)D to stabilize the optimization of deep transformers.\n\nFigure 2: Overview of the proposed graph encoding framework.\n\n4 THEORETICAL RESULTS\n\nIn this section, we study the capacity of global attention to explain the depth bottleneck in graph transformers. The global attention is intended to focus on important substructures and learn expressive substructure representations automatically. However, the fact that graph transformers perform poorly when more layers are stacked raises doubts about the effectiveness of these self-attention layers. The attention layers require sufficient capacity to represent various substructures, which is necessary for learning attention patterns. We define attention capacity, analyze its variation with depth, and propose that local attention to substructures is a potential solution.\n\n4.1 DEFINITION OF ATTENTION CAPACITY\n\nWe define the capacity of attention as the maximum difference between representations learned by different substructure attention patterns. Let supp(e) be all the non-zero dimensions of vector e. We define a fixed attention pattern e of substructure GS as an attention vector that only focuses on nodes of GS, i.e., supp(e) = N S, where e ∈ [0, 1]n, eT 1 = 1.\n\nGiven a graph G with several important substructures GS m and corresponding attention patterns e1 . . . em, denote E = (e1, e2 . . . , em) where columns of E are the base vectors of attention patterns on substructures. We consider the attention space spanned by these attention patterns. Attention from this space only focuses on the important substructures:\n\n1 . . . GS\n\n(4) Denote ∆n S as matrix space with n columns all in space ∆S. We then define attention capacity as the maximum difference between outputs computed by the self-attention with different attention ˆAttnA be the self-attention mapping where attention matrix equals A. matrices from space ∆n\n\n∆S ≜ {Ec|c ∈ [0, 1]m, cT 1 = 1},\n\nS. Let\n\nDefinition 1 The attention capacity is defined as:\n\nFH = max AT\n\n1 ,AT\n\n2 ∈∆n\n\nS\n\n| ˆAttnA1 (H) − ˆAttnA2 (H)|F\n\n=\n\nmax C1,C2∈{C|C∈[0,1]m×n,CT 1=1}\n\n|C T\n\n1 ET HW V O − C T\n\n2 ET HW V O|F ,\n\n(4.1)\n\nAttention capacity in graph transformers is crucial. Larger capacity enables varying features by focusing on different substructures, while smaller capacity limits attention patterns’ impact on learned representations. Smaller capacity modules have difficulty learning attention patterns as they are less sensitive to substructures.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n4.2 ATTENTION CAPACITY DECREASES WITH DEPTH\n\nAfter defining the measure of attention capacity for substructures, we investigate the attention capacity of graph transformers w.r.t depth both empirically and theoretically. We first empirically demonstrate how the attention capacity in the graph transformer varies with depth, then theoretically analyze the phenomenon and possible solution.\n\nThe attention capacity of deep graph transformers for various substructures is computed per Definition 4.1 and normalized by hidden embedding norm (see Appendix G). Figure 1 (right) shows the results. Graphormer (Ying et al., 2021)’s attention capacity decreases significantly after the 24th layer, while SAT (Chen et al., 2022) decreases across all layers, indicating their limitations.\n\nNext, we illustrate that the stacked self-attention layers can cause decreased attention capacity for deeper layers. We first demonstrate that the self-attention module decreases the capacity of attention, then discuss the case with fully-connected layers.\n\nT\n\n)Di. Assume HiW V O\n\nTheorem 1 (The stacked attention modules decrease the capacity of attention) We analyze capacity of attention FHl for stacked attention modules Hi+1 = Attn(Hi) = (Hi + AiHiW V O i − 1bN i Di is full row rank for each layer i. Due to the property of layer nori malization and properly designed initialization, assume the output of attention module equals to its input, i.e., for input X, | Attn(X)|F ≈ |X|F . Then the attention capacity of layer l is upper bounded by\n\n√\n\n2m\n\nFHl ≤\n\nl−1 (cid:89)\n\ni=1\n\n(αi)|PmET |2|W V O\n\nl\n\n|2|P H1|F\n\n(5)\n\n,where αi = m 11T ).\n\n(I − 1\n\n(|P Hi+P AiHiW V O\n\ni\n\n)Di|F\n\n|(P Hi+AiP HiW V O\n\ni −1bN\n\ni\n\nT )Di|F\n\n< 1 with probability 1, and P ≜ (I − 1\n\nn 11T ), Pm ≜\n\nProof is in Appendix 1. The analysis above reveals that the self-attention module decreases the upper bound of attention capacity exponentially. We next consider the case when fully-connected layers are also included in each layer, just like the architecture in practice.\n\nTheorem 2 (The upper bound of attention capacity after stacked transformer layers) For transformer layer with self-attention H ′ )Di, and fullyT )D′ i + ReLU (H ′ connected layer Hi+1 = FFN(H ′ i, with the same assumption as the previous, attention capacity of layer l is upper bounded as follows:\n\ni = Attn(Hi) = (Hi + AiHiW V O )W F2 iW F1\n\ni − 1bN i + 1bF2\n\ni + 1bF1\n\ni) = (H ′\n\n− 1bN2\n\nT\n\nT\n\nT\n\ni\n\ni\n\ni\n\ni\n\n√\n\n2m\n\nl−1 (cid:89)\n\nFHl ≤\n\n(αiγi)|PmET |2|W V O\n\nl\n\n|2|P H1|F\n\n(6)\n\n(|P Hi+P AiHiW V O\n\ni\n\n|(P Hi+AiP HiW V O\n\ni −1bN\n\ni\n\nT )Di|F\n\ni=1\n\n)Di|F\n\n,where αi = |W F1 |2|W F2\n\ni\n\ni\n\n|2).\n\n< 1 with probability 1, and γi = |D′\n\ni|2(1 +\n\nProof in Appendix 2. Fully-connected layer impact can be bounded by coefficients γi, which describe hidden embedding norm change. Previous work Shi et al. (2021) shows that γi is dominated by |D′ i|2 < 1, leading to attention capacity vanishing in these cases.\n\ni|2, and for a fraction of data |D′\n\n4.3 LOCAL ATTENTION FOR DEEP MODEL\n\nReducing attention capacity with depth creates two problems for graph transformers: difficulty attending to substructures and loss of feature expressiveness to substructures. To address the first problem, it’s natural to introduce substructure-based local attention as inductive bias into graph transformer to make up for the deficiency of attention on substructures, where each node can only attend to other nodes that belong to same substructures. Furthermore, We will next show that introducing substructures based local attention also addresses the second problem. We next prove that capacity decay can be alleviated if local attention to substructures is applied in each layer.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 3 Define substructure based local attention as where each node only attends nodes that belong to the same substructures. Assume that for each node, the number of nodes belonging to the same substructures is at most r, where r < n. Let αs be the decay coefficient of this model in Theorem 1, and α be the decay coefficient of the global attention model. Denoted minimum of αs and α for all possible representation Hi and model parameters by αs∗ and α∗ respectively. Then we have αs∗ > α∗.\n\nThe proof is in Appendix 3. This theory illustrates that substructure based local attention not only introduces the substructure based attention to encourage the model to focus on substructure features but also enlarges the attention capacity of deep layers, which promotes the representation capacity of the model when it grows deep.\n\n5 APPROACH\n\nReplacing global attention directly with local attention may lead to insufficient learning ability of the model for long-range dependencies. To better introduce local attention to substructure in selfattention models, we propose a novel substructure token based local attention method, DeepGraph, as Figure 2 shows. DeepGraph tokenizes the graph into node level and substructure level. Local attention is applied to the substructure token and corresponding nodes, while the global attention between nodes is still preserved. This provides a better solution for explicitly introducing attention to substructures in the model while combining global attention, to achieve a better global-local encoding balance.\n\nOur method first samples substructures of the graph, then encodes substructures into tokens, and finally enforces local attention on substructures.\n\n5.1 SUBSTRUCTURE SAMPLING\n\nAs we aim to stress critical substructures in graph, there are numerous types of substructures that are categorized into two groups: neighbors containing local features, such as k-hop neighbors Zhang & Li (2021) and random walk neighbors Zhao et al. (2021), and geometric substructures representing graph topological features, such as circles Bodnar et al. (2021), paths, stars, and other special subgraphs Bouritsas et al. (2022). We integrate these substructures into a unified substructure vocabulary in this work. In order to match geometric substructures, we use the Python package graph-tool that performs efficient subgraph matching. It is practical to pre-compute these substructures once, and then cache them for reuse. It takes about 3 minutes to compute all the substructures of ZINC, and about 2 hours for PCQM4M-LSC.\n\nSubstructures in a graph often exceed the number of nodes. In order to ensure the feasibility of the computation, we sample substructures in each computation. Formally, at each time of encoding, for a graph G with the corresponding substructure set {GS}, we sample a subset {GS m} as input to our model:\n\n2 . . . GS\n\n1 , GS\n\n{GS\n\n1 , GS\n\n2 . . . GS\n\nm} = SubstructureSampling({GS}), 2 . . . GS\n\nˆy = DeepGraph(G, {GS\n\n1 , GS\n\nm})\n\n(7)\n\nWe hope the sampled substructures cover every node of the graph as evenly as possible (Zhao et al., 2021) in order to reduce biases resulting from the uneven density of substructures. We also balance the types of substructures in our sampling, due to the divergent ratios of different substructure types in a graph. The details are in Appendix C and D.\n\n5.2 SUBSTRUCTURE TOKEN ENCODING\n\nThe input embedding contains node tokens embedding {h1, h2, . . . , hn} and substructure tokens embedding {hn+1, . . . , hn+m}, encoded by node feature encoder gn and substructure encoder gs individually. The token embedding of graph nodes is mapped from graph node feature x to feature vector h: hi = gn(xi), i ∈ {1, 2, . . . n}.\n\nFor the substructure tokens, we apply permutation-based structure encoding (Murphy et al., 2019; Chen et al., 2020b; Nikolentzos & Vazirgiannis, 2020) by encoding the substructure adjacency matrix AS i directly. The core idea of permutation-based encoding is pooling the encoding output of\n\n6\n\nPublished as a conference paper at ICLR 2023\n\npermuted graph information over all the possible permutations, which is permutation-invariant and has no information loss. We further apply depth-first search (DFS) with the order of node degree to reduce the possible order of substructure nodes. At the beginning and at each step of DFS, we use degrees to sort nodes. A random node from the ones with the least degree is selected as the starting point for DFS. The nodes are sorted according to their degree at each DFS step, while the same-degree nodes are randomly permuted. See Appendix E for more details. Using this method, the graph encoding permutations is invariant and the number of possible combinations is greatly reduced. The formal definition of substructure token encoder is\n\nhn+i = P ool\n\nπ∈DFS(AS\n\ni )\n\ngs(π(AS\n\ni )), i ∈ {1, 2, . . . m}\n\n(8)\n\nIn practice, sampling is applied instead of pooling. A single sample is sufficient during training to allow the model to learn the substructure stably.\n\n5.3 LOCAL ATTENTION ON SUBSTRUCTURES\n\nThe substructure and its corresponding nodes receive localized attention after substructure tokens have been added. Given input embedding H1 = (h1, h2, . . . , hn+m), mask M is added in selfattention module\n\nˆAttnm(H, M ) as\n\nˆAttnm(Hl, M ) = softmax( ˆA + M )HlW V O where Mij ∈ {0, −∞}, i.e., the elements of mask M can be 0 or −∞, leading to a sparse attention matrix A with zero in position of −∞ in M . In our model, the mask is defined as follows to induce local attention to substructure tokens and corresponding nodes: Mij = −∞ if i + n ∈ {n + 1, n + 2, . . . n + m}, j ̸∈ N S i or j + n ∈ {n + 1, n + 2, . . . n + m}, i ̸∈ N S j , and Mij = 0 otherwise. The substructure tokens only apply local attention to corresponding nodes.\n\n(9)\n\nThe attention in our model is a combination of local attention on substructures and global attention. Substructure tokens are the core of local attention, attending only to corresponding nodes, and integrating representation from the whole substructure. Nodes belonging to the same substructure share a substructure token message, increasing distinguishability of substructures and promoting substructure representation capacity.\n\n6 EXPERIMENTS\n\nIn this section, we aim to validate the performance of DeepGraph empirically. Specifically, we attempt to answer the following questions: (i) How does DeepGraph perform in comparison to existing transformers on popular benchmarks? (ii) Does DeepGraph’s performance improve with increasing depth? (iii) Is DeepGraph capable of alleviating the problem of shrinking attention capacity? (IV) What is the impact of each part of the model on overall performance? We first conduct experiments to evaluate our model on four popular graph datasets, comparing it with state-of-the-art graph transformer models, as well as their augmented deeper versions. Then we illustrate the attention capacity of DeepGraph by visualization. Finally, we validate the effect of different components through ablation studies. Codes are available at https://github.com/zhao-ht/DeepGraph.\n\n6.1 DATASETS\n\nOur method is validated on the tasks of the graph property prediction and node classification, specifically including PCQM4M-LSC (Hu et al., 2020), ZINC (Dwivedi et al., 2020), CLUSTER (Dwivedi et al., 2020) and PATTERN (Dwivedi et al., 2020), widely used in graph transformer studies. PCQM4M-LSC is a large-scale graph-level regression dataset with more than 3.8M molecules. ZINC consists of 12,000 graphs with a molecule property for regression. CLUSTER and PATTERN are challenging node classification tasks with graph sizes varying from dozens to hundreds, containing 14,000 and 12,000 graphs, respectively.\n\n6.2 BASELINES\n\nWe choose recent state-of-art graph transformer models: GT (Dwivedi & Bresson, 2020), SAN (Kreuzer et al., 2021), Graphormer (Ying et al., 2021) and SAT (Chen et al., 2022), covering various\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nGCN GIN GT-sparse GT-Full SAN-Sparse SAN-Full Graphormer SAT DeepGraph (12) DeepGraph (24) DeepGraph (48)\n\nGraph regression\n\nNode classification\n\nPCQM4M-LSC ↓ 0.1691 0.1537 -\n- -\n- 0.1234 -\n0.1220 0.1206 0.1193\n\nZINC ↓ 0.367 ± 0.011 0.526 ± 0.051 0.226 ± 0.014 0.598 ± 0.049 0.198 ± 0.004 0.139 ± 0.006 0.122 ± 0.006 0.094 ± 0.008 0.078 ± 0.006 0.076 ± 0.004 0.072 ± 0.004\n\nCLUSTER ↑ 68.498 ± 0.976 64.716 ± 1.553 73.169 ± 0.622 27.121 ± 8.471 75.738 ± 0.106 76.691 ± 0.247 -\n77.856 ± 0.104 77.526 ± 0.122 77.810 ± 0.167 77.912 ± 0.138\n\nPATTERN ↑ 71.892 ± 0.334 85.387 ± 0.136 84.808 ± 0.068 56.482 ± 3.549 81.329 ± 2.150 86.581 ± 0.037 -\n86.865 ± 0.043 90.015 ± 0.038 90.522 ± 0.059 90.657 ± 0.062\n\nTable 1: Comparison of our DeepGraph to SOTA methods on graph regression and node classification tasks.\n\ngraph transformer methods including absolute and relative position embedding. Graph neural networks baselines includes GCN (Kipf & Welling, 2016) and GIN (Xu et al., 2018a). We first compare our model with the standard state-of-art models. Then we compare our model with the deepened state-of-arts augmented by recent algorithms for deep transformers, including the fusion method (Shi et al., 2021), and reattention method (Zhou et al., 2021). As we use deepnorm to stabilize our training process, we compare DeepGraph to naive deepnorm in the ablation study.\n\n6.3 SETTINGS\n\nWe implement DeepGraph with 12, 24, and 48 layers. The hidden dimension is 80 for ZINC and PATTERN, 48 for CLUSTER, and 768 for PCQM4M-LSC. The training uses Adam optimizer, with warm-up and decaying learning rates. Reported results are the average of over 4 seeds. Both geometric substructure and neighbors are used as substructure patterns in our model. We imply both geometric substructures and k-hop neighbors on ZINC and PCQM4M-LSC. As for CLUSTER and PATTERN, only random walk neighbors are used due to the large scale and dense connection of these two datasets. The effect of different substructure patterns is shown in the ablation study.\n\n6.4 MAIN RESULTS\n\nTable 1 summarizes our experimental results on the graph regression and node classification tasks. The data for the baseline models are taken from their papers. The depth of our model varies from 12 to 48 layers. As the results illustrated, our 12-layer model outperforms baseline models on PCQM4M-LSC, ZINC, and PATTERN, especially on ZINC and PATTERN, surpassing the previous best result reported significantly. As model depth increases, our model achieves consistent improvement. Note that our model with 48 layers outperforms baseline models on all the datasets, proving the effectiveness of more stacked DeepGraph layers. The parameter number of each model for each dataset is provided in Appendix H.\n\n6.5 EFFECT OF DEEPENING\n\nWe next compare DeepGraph with other deepened baseline models. We choose Graphormer and SAT as baseline models, and we deepen them by 2 and 4 times compared to the original version. Along with deepening naively, we also enhance baseline models through the fusion method and reattention method.\n\nDeepGraph 12\n\nDeepGraph 48\n\nZINC CLUSTER ZINC CLUSTER - local attention 0.135 - substructure encoding 0.085 0.080 - deepnorm 0.078 DeepGraph\n\n76.167 76.531 77.202 77.526\n\n76.133 77.884 77.682 77.912\n\n0.141 0.124 0.074 0.072\n\nDeepGraph 12 DeepGraph 48\n\nGeometric only 0.078 Neighbors only 0.086 0.078 Both\n\n0.075 0.079 0.072\n\nTable 2: Ablation study of local attention, substructure encoding, and deepnorm on ZINC and CLUSTER.\n\nTable 3: Sensitivity study of different substructure types on ZINC.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Comparison of DeepGraph to SOTA methods deepened by different deep transformer methods.\n\nAs shown in Figure 3, DeepGraph outperforms baselines by a significant margin. Naive deepening decreases performance on all the datasets for both baseline models. Note that all the 4-time deep SAT fail to converge on CLUSTER due to the training difficulty, as well as Graphormer with reattention. Fusion improves deeper Graphormer on PCQM4M-LSC, and reattention boosts 2x deep SAT on CLUSTER slightly, but none of them are consistent. Our methods surpass fusion and reattention methods stably on all 4 datasets, supporting our theoretical claim that sensitivity to substructures is crucial in deep graph transformers.\n\n6.6 VISUALIZATION OF ATTENTION CAPACITY\n\nWe visualize attention capacity of our method and deep baselines on ZINC dataset, using the same substructures as our model. Additionally, to illustrate the capacity of token representation of substructures, we also directly compute the maximum difference between all the substructure tokens value vectors. All the results are normalized by the corresponding representation norm. See Appendix G for details. The results are plotted in Figure 1 (right).\n\nFirst, the attention capacity of our model remains high across all layers as compared to baselines, indicating that substructure-based local attention can be a useful method for preventing attention deficits. Second, substructure tokens have a much higher capacity than attention capacity computed on nodes, demonstrating the benefits of using substructure tokens for substructure encoding.\n\n6.7 ABLATION AND SENSITIVITY STUDY\n\nFinally, we verify our model’s effectiveness through an ablation study on ZINC and CLUSTER tasks, removing local attention, substructure encoding, and deepnorm to observe their impact. Note that without local attention, the model is only a relative position-based graph transformer with deepnorm residual connections. Without substructure encoding, randomly initialized embedding is applied to each substructure token. Results in Table 2 show that without local attention, performance decreases significantly on both datasets, especially for deeper models, proving the effectiveness of the proposed methods. Furthermore, substructure encoding also plays an instrumental role, emphasizing the importance of structural information. Finally, deepnorm also contributes to model performance by stabilizing the optimization, especially for 48-layer models.\n\nWe validate the sensitivity of our model to different substructure types by testing the performance of DeepGraph with only geometric substructures or neighbors. Table 3 indicates that both contribute to performance, but geometric substructures are more critical for ZINC. This result is expected because specific structures like rings are important fundamental features for the molecule data, and it can be further strengthened when combined with neighbor substructures. Our model can flexibly use different forms of substructure to fully utilize prior knowledge of data.\n\n7 CONCLUSIONS\n\nThis work presents the bottleneck of graph transformers’ performance when depth increases. By empirical and theoretical analysis, we find that deep graph transformers are limited by the capacity bottleneck of graph attention. Furthermore, we propose a novel graph transformer model based on substructure-based local attention with additional substructure tokens. Our model enhances the ability of the attention mechanism to focus on substructures and addresses the limitation of encoding expressive features as the model deepens. Experiments show that our model breaks the depth bottleneck and achieves state-of-the-art performance on popular graph benchmarks.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nUri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications.\n\nIn International Conference on Learning Representations, 2020.\n\nThomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. Rezero is all you need: Fast convergence at large depth. In Uncertainty in Artificial Intelligence, pp. 1352–1361. PMLR, 2021.\n\nDzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by jointly In 3rd International Conference on Learning Representations,\n\nlearning to align and translate. ICLR 2015, 2015.\n\nAnson Bastos, Abhishek Nadgeri, Kuldeep Singh, Hiroki Kanezashi, Toyotaro Suzumura, and Isaiah Onando Mulang. Investigating expressiveness of transformer in spectral domain for graphs. arXiv preprint arXiv:2201.09332, 2022.\n\nCristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael Bronstein. Weisfeiler and lehman go cellular: Cw networks. Advances in Neural Information Processing Systems, 34:2625–2640, 2021.\n\nGiorgos Bouritsas, Fabrizio Frasca, Stefanos P Zafeiriou, and Michael Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\nDexiong Chen, Leslie O’Bray, and Karsten Borgwardt. Structure-aware transformer for graph representation learning. In International Conference on Machine Learning, pp. 3469–3489. PMLR, 2022.\n\nMing Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International Conference on Machine Learning, pp. 1725–1735. PMLR, 2020a.\n\nZhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures? Advances in neural information processing systems, 33:10383–10395, 2020b.\n\nKrzysztof Choromanski, Han Lin, Haoxian Chen, Tianyi Zhang, Arijit Sehanobish, Valerii Likhosherstov, Jack Parker-Holder, Tamas Sarlos, Adrian Weller, and Thomas Weingarten. From blocktoeplitz matrices to differential equations on graphs: towards a general theory for scalable masked transformers. In International Conference on Machine Learning, pp. 3962–3983. PMLR, 2022.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.\n\nYihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning, pp. 2793–2803. PMLR, 2021.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.\n\nDavid K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al ́an Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. Advances in neural information processing systems, 28, 2015.\n\nVijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs.\n\narXiv preprint arXiv:2012.09699, 2020.\n\nVijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.\n\nBenchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nLingbing Guo, Qiang Zhang, and Huajun Chen. Unleashing the power of transformer for graphs.\n\narXiv preprint arXiv:2202.10581, 2022.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n\nIdentity mappings in deep residual\n\nnetworks. In European conference on computer vision, pp. 630–645. Springer, 2016b.\n\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3464–3473, 2019.\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020.\n\nChuntao Jiang, Frans Coenen, and Michele Zito. Finding frequent subgraphs in longitudinal social network data using a weighted graph mining approach. In International Conference on Advanced Data Mining and Applications, pp. 405–416. Springer, 2010.\n\nWengong Jin, Regina Barzilay, and Tommi Jaakkola.\n\nJunction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pp. 2323–2332. PMLR, 2018.\n\nWengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical graph-to-graph translation for\n\nmolecules. arXiv preprint arXiv:1907.11223, 2019.\n\nJinwoo Kim, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure transformers are powerful graph learners. arXiv preprint arXiv:2207.02505, 2022.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\n\nworks. arXiv preprint arXiv:1609.02907, 2016.\n\nMehmet Koyut ̈urk, Ananth Grama, and Wojciech Szpankowski. An efficient algorithm for detecting\n\nfrequent subgraphs in biological networks. Bioinformatics, 20(suppl 1):i200–i207, 2004.\n\nDevin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L ́etourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. Advances in Neural Information Processing Systems, 34:21618–21629, 2021.\n\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66–71, 2018.\n\nGuohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep In Proceedings of the IEEE/CVF international conference on computer vision, pp.\n\nas cnns? 9267–9276, 2019.\n\nGuohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train\n\ndeeper gcns. arXiv preprint arXiv:2006.07739, 2020.\n\nGuohao Li, Matthias M ̈uller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International conference on machine learning, pp. 6437–6449. PMLR, 2021.\n\nQimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI conference on artificial intelligence, 2018.\n\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difficulty of training transformers. In 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, pp. 5747–5763. Association for Computational Linguistics (ACL), 2020a.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nMeng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 338–348, 2020b.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nŁukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław\n\nJastrzebski. Molecule attention transformer. arXiv preprint arXiv:2002.08264, 2020.\n\nGr ́egoire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph\n\nstructure in transformers. arXiv preprint arXiv:2106.05667, 2021.\n\nSiqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. In International Conference on Machine Learning, pp. 15524–15543. PMLR, 2022.\n\nRyan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling In International Conference on Machine Learning, pp. 4663–4673.\n\nfor graph representations. PMLR, 2019.\n\nChristopher W Murray and David C Rees. The rise of fragment-based drug discovery. Nature\n\nchemistry, 1(3):187–192, 2009.\n\nGiannis Nikolentzos and Michalis Vazirgiannis. Random walk graph neural networks. Advances in\n\nNeural Information Processing Systems, 33:16211–16222, 2020.\n\nHoang Nt and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.\n\narXiv preprint arXiv:1905.09550, 2019.\n\nKenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node\n\nclassification. In International Conference on Learning Representations, 2019.\n\nWonpyo Park, Woong-Gi Chang, Donggeon Lee, Juntae Kim, et al. Grpe: Relative positional\n\nencoding for graph transformer. In ICLR2022 Machine Learning for Drug Discovery, 2022.\n\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pp. 4055– 4064. PMLR, 2018.\n\nBen Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. Advances in neural information processing systems, 29, 2016.\n\nYu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2019.\n\nRyan A Rossi, Nesreen K Ahmed, Eunyee Koh, Sungchul Kim, Anup Rao, and Yasin AbbasiYadkori. A structural graph representation learning framework. In Proceedings of the 13th international conference on web search and data mining, pp. 483–491, 2020.\n\nNino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt. Efficient graphlet kernels for large graph comparison. In Artificial intelligence and statistics, pp. 488–495. PMLR, 2009.\n\nHan Shi, JIAHUI GAO, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen MS Lee, and James Kwok. Revisiting over-smoothing in bert from the perspective of graph. In International Conference on Learning Representations, 2021.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMatus Telgarsky. Benefits of depth in neural networks.\n\nIn Conference on learning theory, pp.\n\n1517–1539. PMLR, 2016.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet:\n\nScaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555, 2022.\n\nPeihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2021.\n\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning deep transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1810–1822, 2019.\n\nZhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez, and Ion Stoica. Representing long-range context for graph neural networks with global attention. Advances in Neural Information Processing Systems, 34:13266–13279, 2021.\n\nLechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. In International Conference on Machine Learning, pp. 5393–5402. PMLR, 2018.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? In International Conference on Learning Representations, 2018a.\n\nKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International conference on machine learning, pp. 5453–5462. PMLR, 2018b.\n\nPinar Yanardag and SVN Vishwanathan. Deep graph kernels.\n\nIn Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1365–1374, 2015.\n\nDmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:\n\n103–114, 2017.\n\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? Advances in Neural Information Processing Systems, 34:28877–28888, 2021.\n\nRex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnn explainer: A tool for post-hoc explanation of graph neural networks. arXiv preprint arXiv:1903.03894, 2019.\n\nShuo Yu, Yufan Feng, Da Zhang, Hayat Dino Bedru, Bo Xu, and Feng Xia. Motif discovery in\n\nnetworks: a survey. Computer Science Review, 37:100267, 2020.\n\nBiao Zhang, Ivan Titov, and Rico Sennrich. Improving deep transformer with depth-scaled initialization and merged attention. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, pp. 898– 909. Association for Computational Linguistics (ACL), 2019.\n\nJiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. Graph-bert: Only attention is needed for\n\nlearning graph representations. arXiv preprint arXiv:2001.05140, 2020.\n\nMuhan Zhang and Pan Li. Nested graph neural networks. Advances in Neural Information Process-\n\ning Systems, 34:15734–15747, 2021.\n\nLingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International\n\nConference on Learning Representations, 2019.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nLingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any gnn with local structure awareness. In International Conference on Learning Representations, 2021.\n\nDaquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA PROOF OF THEOREMS\n\nProof 1 for brevity, define P ≜ (I − 1 n 11T ), and Pm ≜ (I − 1 m 11T ). We first introduce an important property of P : For any matrix B with left eigenvector B1 = β1, we have P BC=P BP C, ∀C. To prove it, we can deompose C = P C + 1\n\nn 11T C. Then\n\nP BC = P B(P C +\n\n1 n\n\n11T C) = P BP C + β\n\n1 n\n\nP 11T C = P BP C\n\nWe next prove the main theorem by recursion. By definition,\n\nFH = max AT\n\n1 ,AT\n\n2 ∈∆n\n\nS\n\n| ˆAttnA1(H) − ˆAttnA2(H)|F\n\n=\n\n=\n\nmax C1,C2∈{C|C∈[0,1]m×n,CT 1=1}\n\nmax C1,C2∈{C|C∈[0,1]m×n,CT 1=1}\n\n|C T\n\n1 ET HW V O − C T\n\n2 ET HW V O|F\n\n|(C1 − C2)T ET HW V O|F\n\nNote that (C1 − C2)T 1 = 1 − 1 = 0, so we can multiply Pm before it:\n\nFH =\n\nmax C1,C2∈{C|C∈[0,1]m×n,CT 1=1}\n\n|(C1 − C2)T ET HW V O|F\n\n=\n\n≤\n\n≤\n\nmax C1,C2∈{C|C∈[0,1]m×n,CT 1=1}\n\nmax C1,C2∈{C|C∈[0,1]m×n,CT 1=1} √\n\n2m|PmET HW V O|F\n\n|(C1 − C2)T PmET HW V O|F\n\n|(C1 − C2)T |2|PmET HW V O|F\n\n,where |(C1 − C2)T |2 ≤ (cid:112)|(C1 − C2)T |1|(C1 − C2)T |∞ =\n\nWe further decompose |PmET HW V O|F into\n\n√\n\n2m.\n\n(10)\n\n(11)\n\n|PmET HW V O|F = |PmET P HW V O|F ≤ |PmET |2|W V O|2|P H|F\n\n(12)\n\nFor layer l, we recursively compute |P Hi+1|F by |P Hi|F for layer i = l − 1, l − 2, . . . , 1:\n\n|P Hi+1|F = |P (Hi + AiHiW V O\n\ni − 1bN\n\ni\n\nT\n\n)Di|F T\n\n= |P (P Hi + AiP HiW V O\n\n=\n\n|P (P Hi + AiP HiW V O |(P Hi + AiP HiW V O\n\ni\n\ni − 1bN i − 1bN i − 1bN\n\ni\n\ni\n\n)Di|F\n\nT\n\n)Di|F T )Di|F\n\n|(P Hi + AiP HiW V O |P Hi|F\n\ni − 1bN\n\ni\n\nT\n\n)Di|F\n\n|P Hi|F\n\n= αiλi|P Hi|F\n\n,where αi = |P (P Hi+AiP HiW V O\n\n|(P Hi+AiP HiW V O\n\ni −1bN i −1bN\n\ni\n\ni\n\nT )Di|F T )Di|F\n\n, and λi = |(P Hi+AiP HiW V O\n\ni −1bN\n\ni\n\n|P Hi|F\n\n(13)\n\nT )Di|F\n\n.\n\nT\n\ni Di − 1bN\n\nNote that P = (I − 1 if AiP HiW V O because −1bN i\non 1-stretched space equals to 1bN i\nthat AiP HiW V ODi is full-rank with probability 1: As assumption, HiW V O 1-stretched space is the only subspace that P HiW V O i Di is orthogonal to, i.e.\n\nn 11T ) is a contraction mapping, so αi ≤ 1, and equal to 1 if and only Di is orthogonal to the 1-stretched space, which is usually impossible, i Di Di is 0 due to the property of random matrix. We further show i Di is full-rank, and\n\nDi is in the 1-stretched space, and the probability that projection of AiP HiW V O\n\nT\n\nT\n\ni\n\nxT P HiW V O\n\ni Di = 0 ⇔ x = β1\n\n15\n\nPublished as a conference paper at ICLR 2023\n\n. So if 1T AiP HiW V O random parameters in transformer.\n\ni Di = 0, 1T Ai = β1T , i.e. AT\n\ni 1 = β1, the probability of which is 0 for\n\nFor coefficient λi, due to the property of layer normalization and good parameter initialization that keeps norm of output equals to input, i.e. |(X + AiXW V O i )Di|F /|X|F ≈ 1, ∀X, we have λ ≈ 1.\n\ni − 1bN\n\nBased on the analysis above, we can bound FHl recursively:\n\n√\n\n2m\n\nFHl ≤\n\nl−1 (cid:89)\n\ni=1\n\n(αi)|PmET |2|W V O\n\nl\n\n|2|P H1|F\n\n(14)\n\n, where αi = |P (P Hi+AiP HiW V O ity 1.\n\n|(P Hi+AiP HiW V O\n\ni −1bN i −1bN\n\ni\n\ni\n\nT )Di|F T )Di|F\n\n=\n\n(|P Hi+P AiHiW V O\n\ni\n\n)Di|F\n\n|(P Hi+AiP HiW V O\n\ni −1bN\n\ni\n\nT )Di|F\n\n< 1 with probabil-\n\nProof 2 As the same method in the previous proof, we need to recursively bound |P Hi+1|F by |P Hi|F for layer i = l − 1, l − 2, . . . , 1. Deote H ′ i). Similar to the previous proof, we have\n\ni = Attn(Hi), and Hi+1 = FFN(H ′\n\n|P H ′\n\ni|F = |P (Hi + AiHiW V O\n\ni − 1bN\n\ni\n\nT\n\n)Di|F T\n\n= |P (P Hi + AiP HiW V O\n\n=\n\n|P (P Hi + AiP HiW V O |(P Hi + AiP HiW V O\n\ni\n\ni − 1bN i − 1bN i − 1bN\n\ni\n\ni\n\n)Di|F\n\nT\n\n)Di|F T )Di|F\n\n= αiλi|P Hi|F\n\nAs for P Hi+1, we can bound it as\n\n|(P Hi + AiP HiW V O |P Hi|F\n\ni − 1bN\n\ni\n\nT\n\n)Di|F\n\n|P Hi|F\n\n(15)\n\n|P Hi+1|F = |P (H ′\n\nT\n\ni + 1bF2 )W F2\n\nT\n\ni\n\ni\n\n)W F2\n\ni + 1bF1 iW F1\n\niW F1\n\ni|F + |P ReLU (H ′\n\ni + ReLU (H ′ iD′ i|F |D′ i|F |D′ i|F |D′\n\ni|2 + |P ReLU (H ′ i|2 + |P (H ′ i|2 + |P H ′\n\niW F1 i|F |W F1\n\ni + 1bF1 iW F1 i + 1bF1\n\n|2|W F2\n\nT\n\ni\n\ni\n\ni\n\ni\n\ni + 1bF1\n\ni\n\ni\n\n)|F |W F2 |2|D′\n\ni|2\n\ni D′ i|F )|F |W F2\n\ni\n\nT\n\n|2|D′\n\ni|2\n\n= |P H ′\n\n≤ |P H ′\n\n≤ |P H ′\n\n≤ |P H ′\n\nT\n\nT\n\n− 1bN2\n\ni\n\n)D′\n\ni|F\n\n|2|D′\n\ni|2\n\n(16)\n\n= |D′\n\ni|2(1 + |W F1\n\ni\n\n|2|W F2\n\ni\n\n|2)|P H ′\n\ni|F\n\nSo the final conclusion is\n\n√\n\n2m\n\nl−1 (cid:89)\n\nFHl ≤\n\n(αiγi)|PmET |2|W V O\n\nl\n\n|2|P H1|F\n\ni=1\n\n, where αi = |P (P Hi+AiP HiW V O ity 1, and γi = |D′\n\ni|2(1 + |W F1\n\n|(P Hi+AiP HiW V O\n\ni −1bN i −1bN\n\n|2|W F2\n\ni\n\ni\n\ni\n\ni\n\nT )Di|F T )Di|F |2).\n\n=\n\n(|P Hi+P AiHiW V O\n\ni\n\n)Di|F\n\n|(P Hi+AiP HiW V O\n\ni −1bN\n\ni\n\nT )Di|F\n\n(17)\n\n< 1 with probabil-\n\nProof 3 We denote global attention and local attention by A and As, respectively.\n\nBy definition, αi =\n\n(|P Hi+P AiHiW V O\n\ni\n\n)Di|F\n\n|(P Hi+AiP HiW V O\n\ni −1bN\n\ni\n\nT )Di|F\n\n. We analyse the worst case for αi:\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nαi =\n\n=\n\n≥\n\n(|P Hi + P AiHiW V O\n\ni\n\n)Di|F\n\n|(P Hi + AiP HiW V O\n\ni − 1bN\n\ni\n\nT )Di|F\n\n(|P Hi + P AiHiW V O\n\ni\n\n|(P Hi + (P + 1\n\nn 11T )AiP HiW V O\n\n)Di|F i − 1bN\n\ni\n\n(|P Hi + P AiHiW V O\n\ni\n\nT )Di|F )Di|F\n\n|(P Hi + P AiHiW V O\n\ni\n\n)Di|F + | 1\n\nn 11T AiP HiW V O\n\ni Di|F + |1bN\n\ni\n\nFurther analysis on | 1\n\nn 11T AiP HiW V O\n\ni Di|F shows that\n\n|\n\n1 n\n\n11T AiP HiW V O\n\ni Di|F = |1T AiP HiW V O\n\ni Di|2\n\nT Di|F\n\n(18)\n\n(19)\n\ni 1 = AT\n\nwhere P AT of which is 0. The maximum of |P AT are −1.\n\ni 1 − 1, the element of which is between [−1, n − 1], and the sum of all elements i 1|2 is achieved when one element is n − 1 and other elements\n\n≤ |HiW V O\n\ni Di|2|P AT\n\ni 1|2,\n\nHowever, for local attention As the elements of |P As i\n|P AT\n\ni 1|2.\n\nT 1|2 is only between [−1, r − 1], and thus the maximum |P As\n\ni that each node is only attended by at most r nodes, where r < n, T 1|2 is less than\n\ni\n\nWe further address that the two inequalities in 18 and 19 both can be achieved for proper Hi and parameters. So we can conclude that for the worst case of αi and αs\n\ni , denoted by α∗\n\ni and αs\n\n∗,\n\ni\n\nα∗\n\ni < αs\n\ni\n\n∗\n\nB DISCUSSION OF SUBSTRUCTURE BASED ATTENTION SPACE\n\nIn Definition 4.1, for a substructure GS, the fixed attention pattern e is an ideal attention pattern to help to concentrate on the information of this substructure, for example, a uniform attention vector on a benzene ring, or attention on a two-hop neighbor graph attenuating according to the distance to the central node. Traditional GNN is also an example of such a fixed attention pattern, where each node takes uniform attention to its’ neighbor nodes. Although we treat this pattern as fixed on the substructure, we denote that the definition is general because the value can be any learnable attention pattern.\n\nThe defined attention space construct attention patterns for all meaningful attention on substructures by containing the combination of all the elementary substructure pattern. As this substructure attention patterns are considered important indicative bias, the graph transformer is expected to learn attention patterns in this space to utilize the substructure feature. This definition is also general and expressive, as sufficient patterns can be added to include more general cases.\n\nC SUBSTRUCTURES\n\nThe sizes of the different substructures used in this study are neighbor substructures including 2-hop neighbor and 10-step random walk neighbor, and geometric substructures including circle with size from 3 to 8, star with size from 2 to 6, and path with size from 4 to 8. For geometric substructures, we use the python package graph-tool for substructure matching. It takes about 3 minutes to match all the geometric substructures of ZINC, and about 2 hours for PCQM4M-LSC. We cache all the substructures for reuse.\n\nD SUBSTRUCTURESAMPLING\n\nA greedy sampling algorithm is used for the substructure sampling process. In each step of the iteration, nodes with less coverage have a greater probability of being covered in the next iteration. At\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nthe beginning of sampling, we randomly select ninit substructures from all the substructures, where different types of substructures are balanced. In each iteration, we first mark nodes as set Nlef t that are covered less than threshold thre times by already sampled substructures. Then we calculate cnti for each substructure left, which is the number of nodes in Nlef t covered by substructure GS i . We select nsample from substructures with top k cnt randomly. The iteration ends when each node is covered at least thre times, or no substructure is left. In the algorithm, ninit, k, and nsample are determined by thre and nodes number to speed up the iteration and increase sampling variance, while retaining that nodes are covered as evenly as possible.\n\nE DEPTH-FIRST SEARCH (DFS)\n\nWe use degree to sort nodes at the beginning of DFS and each step. In the beginning, DFS start from node uniformly sampled from nodes with the least degree. At each step, nodes are sorted according to degree, while nodes with the same degree are permuted randomly. This method keeps the graph encoding permutation invariant, and reduces possible permutations a lot. The drawback of permutation-based encoding is that the number of possible permutations becomes intractable when the graph is large. Fortunately, the substructure size is usually not larger than 10.\n\nF RELATED WORK OF TOKEN UNIT\n\nThe most suitable basic unit for transformer as a token has been studied in many works in natural language processing and computational vision. For nature language, sentences are separated into sub-words containing a variant number of primary characters (Kudo & Richardson, 2018). In computational vision, while previous works apply transformer directly on image pixels (Parmar et al., 2018; Hu et al., 2019), recent works find it more beneficial to treat patches as tokens (Dosovitskiy et al., 2020), indicating the importance of proper segmentation of the input. For graph transformers, tokens in most current studies are nodes. We propose to encode substructures together with nodes in the graph as tokens in the transformer.\n\nG EMPIRICAL STUDY ON ATTENTION CAPACITY\n\nWe empirically explore the capacity of the model for the substructure, and validate our theoretical results. We aim to answer the following three questions: (1) Do attention capacity really decreases in deeper graph transformer? (2) Can our method, i.e., local attention with substructure tokens, help to increase model capacity of representing substructures? (3) Can our method, i.e., local attention with substructure tokens, help to alleviate attention capacity decrease?\n\nWe visualize the attention capacity of our method and deep baselines on the ZINC dataset. The set of substructures is the same as the substructures used in our model, while the base vectors E are defined as uniform distributions supported on corresponding substructures. Because attention capacity in Definition 4.1 depends on the norm of hidden representations, we normalize it by the norm of all the value vectors (cid:80) i=1...n |hiW V O|2 to eliminate the influence of the representation norm. Note that for our model, only node tokens are considered. Additionally, because we explicitly tokenize substructures as tokens, we also directly compute the maximum difference between all the substructure tokens value vectors, maxi,j∈{1...m} |hi+nW V O − hj+nW V O|2, and normalize it by i=1...m |hi+nW V O|2, to illustrate the capacity of token representation of substructures. The\n\n(cid:80)\n\n1 m\nresults are plotted in Figure 1 (right).\n\nFirst, the result of Graphormer and SAT reveals that attention capacity of substructures decreases in deep layers. Note that in the shallow layer the attention capacity of Graphormer increases, which is not contradicting our theory because we only claim that the upper bound of attention capacity decrease with depth. However, after the 24th layer, attention capacity decreases a lot, revealing the problem of deepening graph transformer. Note that because SAT uses GNN to encode substructures for query and key computation, it has a high attention capacity in the first layer. However, it decays fast due to the property of global attention.\n\nSecond, the result of our model indicates that local attention-based substructure tokens improve substructure representation capacity. The difference between substructure tokens is much larger\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nthan the representation learned by attention on substructures, indicating learning substructure token representation by local attention can significantly improve the representation capacity of model, which supports our first motivation that introducing substructure-based local attention help model to encode substructures better.\n\nFinally, compared to baselines, attention capacity of our model remains high across all the layers. Although it drops from 0.96 to 0.90 after the 24th layer, the dropping ratio is smaller than the baseline graph transformer. This supports our claim that substructure-based local attention helps alleviate attention capacity decrease. Note that the capacity of substructure tokens also decreases slower.\n\nH PARAMETER NUMBER COMPARISON\n\nWe list parameter numbers of our model and baselines. Note that parameter numbers of Graphormer and SAT are computed by running their official code directly, and other baselines are from papers. The parameter number of SAT (*1) is different from reported in their papers, which may be due to code updates. The parameter number of our base model is comparable to previous works, and even less on CLUSTER. The parameter number of our deeper model is also comparable to deeper baselines, while our performance surpasses them.\n\nGCN GIN GT-Sparse GT-Full SAN-Sparse SAN-Full Graphormer\n\nSAT\n\nOurs\n\nlayer -\n- -\n- -\n- x1 x2 x4 x1 x2 x4 x1 x2 x4\n\nPCQM4M-LSC ZINC 421k 2.0M 495k 3.8M 588,929 -\n588,929 -\n494,865 -\n508,577 -\n489,321 44,750,081 1,055,985 87,309,569 1,996,785 172,428,545 499,681 -\n991,873 -\n1,976,257 -\n612,705 45,563,393 1,078,225 88,122,881 2,019,025 173,241,857\n\nCLUSTER PATTERN 571k 684k 524,026 524,026 530,036 519,186 -\n- -\n741,990 1,480,806 2,958,438 382,865 686,225 989,585\n\n380k 455k 522,982 522,982 493,340 507,202 -\n- -\n825,986 1,646,978 3,288,962 612,705 1,083,105 2,023,905\n\nI TIME COMPLEXITY\n\nThe time complexity of our methods is mainly due to the larger input size, which increases the cost of transformer. Given graph size N and sampled substructure number M , the complexity of transformer is O((M + N )2). However, as we use sampling to reduce the substructure number, the substructure number is less than the graph size, i.e. M < N , so the complexity will not be more than four times of the standard transformer and remains O(N 2).\n\nJ ABLATION STUDY ON NEIGHBOR SIZE\n\nWe conduct ablation studies for random walk neighbors on CLUSTER and PATTERN datasets. While we use size 10 neighbors in the paper, we compare the performance with sizes 5 and 15. The result is as follows:\n\nCLUSTER 12 77.2 77.1 77.5\n\nsize 5 size 15 size 10\n\nPATTERN 12 90.1 89.6 90\n\n48 90.6 90.4 90.7\n\n48 77.7 78 77.9\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nThe result shows that the performance remains robust to different substructure sizes, especially the deeper model. Smaller substructures are easy for the model to learn its structure features, but they may not cover enough nodes and provide good graph structure information. Larger substructures are more informative about graph feature, but is more difficult for stable learning, especially for a shallow model.\n\n20",
    "reference": "# Summary Of The Paper\n\nThe paper studies the limitations to deepen graph transformers and argue that substructure learning gets increasingly harder in the canonical formulation of the graph transformers. To address this limitation, the paper proposes a variant of graph transformers that explicitly models substructure attention and encoding. Empirical results suggest that the proposed DeepGraph approach is effective and competitive.\n\n# Strength And Weaknesses\n\nStrengths:\n1) The paper tries to provide both theoretical and empirical insights on the importance of substructure sensitivity to the capacity of graph transformers.\n2) The writing is generally clear.\n3) I think the problem in itself is interesting (i.e., how can we train deeper graph transformers).\n4) Good empirical results and ablative studies.\n\nWeaknesses/questions:\n1) While it's nice that the paper provides theoretical analysis on the model capacity, it is unclear to me how meaningful they are. Specifically, for example:\n    - Inequality (6) provides an upper bound. In practice, $C_1$ and $C_2$ also depends on $H_\\ell$, so they are not independent. This makes the inequality (12) in the Appendix potentially vacuous. Moreover, while $\\alpha_i$ is accumulated in inequality (6) across layers indeed, it is unclear how $W_\\ell^{VO}$ (which is learnable) evolves across depth. Could it balance this decay process? As another example, Theorem 3 is merely comparing two upper bounds. Without any evidence on the looseness of these upper bounds, it's hard to evaluate the value of this theorem. It'd be great if the authors could elaborate on issues like these.\n    - The theoretical results applies mainly to the input level. $G_1^S, \\dots, G_m^S$ makes sense at the input layer where each node contains the information only about itself. But starting from layer 2, the hidden units already combines information from all lower-level nodes, making the substructure argument a bit vague.\n2) Given the assumptions, the theoretical results should also largely apply to the original (non-graph) transformers, such as those used in NLP. And yet we are able to train very deep transformers there, even though substructures do exist in language. Could the authors discuss more what their results imply in non-graph applications?\n3) Do techniques that allow very deep training of conventional transformers help deepen graph transformers (e.g., ReZero)? This would be an interesting ablative study that demonstrates why the substructure attention is a better alternative for graphs.\n4) For PCQM4M-LSC and ZINC, the # of nodes in these tasks is usually very small (e.g., <60). I'm a bit surprised that substructure is still very useful in these cases as global attention is not operating on a huge graph anyway.\n5) Could the authors provide the # of parameters for each model in Table 1?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is generally quite clear, and the empirical studies is relatively extensive. I did not check the math carefully but I did go through all of the proof. The paper provides a good insight to an important problem that exists in graph transformers, and the empirical results seem solid. However, structural encodings is not a completely new idea. The authors mentioned that the code will be provided.\n\n# Summary Of The Review\n\nOverall, I find the paper interesting and it provides a good insight into how substructure modeling plays a role in the attention and expressivity of modern graph transformers. There is still an obvious gap between the theory section and the empirical section (e.g., the paper did not empirically verify any of the theoretical conclusions), but the empirical results are quite good. I'm inclined to acceptance on the condition that my questions can be answered satisfactorily.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nIMPROVED FULLY QUANTIZED TRAINING VIA RECTIFYING BATCH NORMALIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nQuantization-aware Training (QAT) is able to reduce the training cost by quantizing neural network weights and activations in the forward pass and improve the speed at the inference stage. QAT can be extended to Fully-Quantized Training (FQT), which further accelerates the training by quantizing gradients in the backward pass as backpropagation typically occupies half of the training time. Unfortunately, gradient quantization is challenging as Stochastic Gradient Descent (SGD) based training is sensitive to the precision of the gradient signal. Particularly, the noise introduced by gradient quantization accumulates during backward pass, which causes the exploding gradient problem and results in unstable training and significant accuracy drop. Though Batch Normalization (BatchNorm) is a de-facto resort to stabilize training in regular full-precision scenario, we observe that it fails to prevent the gradient explosion when gradient quantizers are injected in the backward pass. Surprisingly, our theory shows that BatchNorm could amplify the noise accumulation, which in turn hastens the explosion of gradients. A BatchNorm rectification method is derived from our theory to suppress the amplification effect and bridge the performance gap between full-precision training and FQT. Adding this simple rectification loss to baselines generates better results than most prior FQT algorithms on various neural network architectures and datasets, regardless of the gradient bit-widths used (8,4, and 2 bits).\n\n1\n\nINTRODUCTION\n\nQuantization-aware Training (QAT) is a popular track of research that simulates the neural network quantization (weights and activations) during the course of training to curb the inference-time accuracy drop of low-bit models (e.g. INT8 quantization). On the other hand, theoretical calculations on the BitOps (Yang & Jin (2021); Guo et al. (2020)) computation costs can easily conclude that backpropagation accounts for half of the computations during training. Empirical data1 shows backward pass sometimes even costs more in practice. Decreasing the gradient bit-widths will apparently reduce computation overheads of backpropagation Horowitz (2014). If variables in backward pass are also quantized, adding up the forward quantization in QAT, all the network variables required in training would be fully quantized and the whole training process could be accelerated on dedicated hardware, i.e., Fully-Quantized Training (FQT), providing huge accessibility of large model training to users with limited computation capability. Recent work Zhu et al. (2020) has shown that INT8 FQT speeds up the forward pass and the backward pass by 1.63× and 1.94× respectively when training ResNet-50 on ImageNet with NVIDIA Pascal GPU.\n\nYet gradient quantization under the FQT scheme is vastly underexplored, as it is notoriously more challenging than forward quantization in QAT. It is observed that network training is sensitive to the precision of gradients, and low-bit gradient quantization leads to unstable training and significant accuracy drop (see Fig. 1). More importantly, the accumulation of gradient quantization noise in backward pass (see Fig. 2) causes the exploding gradient problem during backpropagation, even resulting in training failure. In contrast to weight/activation quantization, gradient quantization noise produced during backpropagation cannot be automatically corrected by optimizing objective loss.\n\nUnlike prior works on optimizing gradient quantizers for quantization noise reduction Zhou et al. (2016); Choi et al. (2018); Zhu et al. (2020), this paper reveals the negative effect of Batch Nor-\n\n1https://github.com/jcjohnson/cnn-benchmarks\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nmalization (BatchNorm) on amplifying the gradient quantization noise accumulation, when training deep Convolutional Neural Networks (CNNs) with low bit gradients. We show that the noise amplification effect further explodes the gradients during the backward pass. We thus propose a BatchNorm rectification method to suppress the noise amplification effect and alleviate the gradient explosion problem, which in turn enables stabilized training and better accuracy at low-bit gradients.\n\nOur contributions are summarized as follows:\n\n• We discover that BatchNorm fails to prevent the exploded low-bit gradients in fullquantized training through theoretic analysis, and may even amplify the accumulated gradient quantization noise, which further aggravates the gradient explosion.\n\n• According to our theory, we propose a simple yet effective BatchNorm variance rectification algorithm without introducing noticeable overhead, to suppress the noise amplification effect, resulting in alleviated gradient explosion.\n\n• Extensive experiments on MNIST, CIFAR-10, and ImageNet show that our method achieves improved training and higher accuracy over state-of-the-arts with vanilla gradient quantizers, regardless of the gradient bit-widths used (8,4,2 bits).\n\n2 RELATED WORK\n\nQuantization-Aware Training (QAT). DoReFa-Net Zhou et al. (2016) proposed to optimize the clipping value and the scaling factor of the uniform quantizers for weights and activations separately. It was validated on image classification task under multiple bit-widths, but only with the rather simple AlexNet architecture. PACT Choi et al. (2018) proposed to quantize the activations with a learnable layer-wise clipping value, which not surprisingly achieved better accuracy than DoReFa-Net at 5-bit down to 2-bit. Most QAT works quantize weights and activations simultaneously, by optimizing the uniform quantization parameters Zhang et al. (2018a); Esser et al. (2020); Bhalgat et al. (2020), layer-wise or channel-wise mixed-precision quantization Jin et al. (2020); Lou et al. (2020), or leveraging non-uniform quantization such as Logarithmic quantizer Miyashita et al. (2016) and piece-wise linear quantizer Fang et al. (2020). Most recent QAT works Zhou et al. (2016); Choi et al. (2018); Zhang et al. (2018a); Esser et al. (2020); Bhalgat et al. (2020) used “Straight-Through Estimator” (STE) Bengio et al. (2013) to estimate the gradient of the non-differentiable quantization function, while other work Gong et al. (2019) softened the linear quantization operation in order to match the true gradient with STE.\n\nFully-quantized Training (FQT). FQT aims to accelerate and quantize the backward pass of network training with low-bit error signals and gradients, agnostic to single machine or parallel training. Early attempt Zhou et al. (2016) adopted a primitive quantizer design based on uniform quantizer for gradients (without scaling and other optimization) and large performance drops are witnessed when training with low-bit gradients. SBM Banner et al. (2018) adopted fixed-point 8-bit gradient quantization, but only focused on improving the quantization schemes in the forward pass. WAGEUBN Yang et al. (2020) quantized gradients to 8-bit integers, but also showed a huge performance gap against its full-precision counterpart. NITI Wang et al. (2020) integrated gradient calculations with parameter update operations to reduce the gradient quantization noise with welldesigned quantizers. However, it can only support shallow CNN architectures and did not explore any deeper networks with BatchNorm. In Zhu et al. (2020), the authors considered the sharp and wide distribution of gradients, and proposed to clip the gradients according to the deviation of the gradient distribution before quantization, achieving on-par results with full-precision training. To compensate the quantization loss on gradient, AFP Zhang et al. (2020) and CPT Fu et al. (2021) used higher precision data to aid low-precision training. DAINT8 Zhao et al. (2021) adopted a bespoke 8-bit channel-wise gradient quantization to suppress the negative effect of quantization noise during training. Gradient quantization with less than INT8 representations remains largely unexplored. FP4 Sun et al. (2020) managed to train modern CNN architectures using 4-bit gradients without significant accuracy loss, but the gradients were represented as floating-point numbers.\n\nBatch Normalization in QAT. Most QAT approaches either left BatchNorm in between parameterized layer (Conv/FC) and activation layer (ReLU) without quantization Zhou et al. (2016); Choi et al. (2018) or with quantization Yang et al. (2020), or absorbed BatchNorm into Conv before weight quantization in the forward pass Jacob et al. (2018), or directly trained a BatchNorm-free shallow network architecture to achieve full 8-bit integer-only arithmetic Wang et al. (2020). To our\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\non\n\nFigure 1: Validation accuracy curve of AlexNet ImageNet with W2A2dx3dW 2 w/ or w/o the pro- (dx denotes error signal, posed Lσ. dW is gradients of weights, same below).\n\nFigure 2: Illustration of gradient explosion problem (measured by the variance of gradients across layers) with {2, 4, 8}-bit uniformly quantized gradients. Statistics are computed from ResNet-20 on CIFAR-10 with 8-bit W/A. Y-axis is in log scale.\n\nbest knowledge, this paper is the first to systematically study the effect of BatchNorm on the training stability of quantized networks with low-bit gradients.\n\nNorm-free Networks. There is another interesting line of works that gets rid of normalization layers from CNN architectures for good Zhang et al. (2018b) while still manages to train the full-precision networks stably. However, there is still a lack of attempts to adapt norm-free networks to QAT or even FQT settings at the first place, where potential problems need to be addressed when low-bit quantizers are introduced. Therefore, we decide currently it is not mature enough to discuss this track in this paper which targets at FQT.\n\nPrior efforts reducing variances in backpropagation. Rectifying gradient variances during backpropagation has been sporadically discussed for full-precision training, e.g. in Kaiming Initialization He et al. (2015) where it leverages weight distributions in Conv and FC layers and one can opt for backward variance rectification if the gradients are observed to be chaotic. However, optimal forward and backward rectification still cannot be satisfied simultaneously, especially when backward signals contains significant amount of quantization noises. In our attempts, using “fan-out” mode in kaiming initialization alone still cannot avoid training crashes in worse cases (e.g. MobileNet-V2 under W4A4G4). To our knowledge, there is no work studying the impact of normalization layers in CNNs for gradient rectification under the FQT setting.\n\n3 PRELIMINARIES\n\nKey Notations. We denote the variance of a probabilistic variable as D(·). We denote gradient w.r.t. weights as gw and error signal as gx. We use the plural term ”gradients” to generally refer to all backward pass variables including gw and gx. In such contexts, subscripts of g are omitted. Similar to the additive quantization noise for weight and activation quantization in Meller et al. (2019), the quantized gradients ̃g at each layer can be decomposed into three parts:\n\n ̃g = g + e(g) + δq(g),\n\n(1)\n\nwhere g and e(g) denote the original gradients and the gradient quantization noise at current layer respectively, and δq(g) represents the accumulated gradient quantization noise propagated from all its succeeding layers during backward. E.g., for the (l − 1)th layer, δq(gal−1 ) represents the quantization noise accumulated from layer l to the last layer (see Fig. 3).\n\nIn this manuscript, we format the bit-widths of weights (W), activations (A), backward errors (dx) and gradients of weights (dW ) used in experiments as W/A/dx/dW if error dx and gradients dW are assigned different bit-widths, or simplified as W/A/G if dx and dW have the same bit-width.\n\nBatch Normalization. Batch Normalization Ioffe & Szegedy (2015) is widely adopted technique to stabilize the training of deep full-precision networks. The forward pass in a BatchNorm layer consists of operations calculating the mean and variance of each channel over a mini-batch with N input samples {xn}N n=1. Each channel of the input xn is first normalized to ˆxc = (xc − μc)/σc. The normalized input ˆx is finally linearly scaled and shifted as y = γ⊺ ˆx + β. The relationship between backward error on BatchNorm’s input xn and that on ˆxn is: (we omit channel notations here for simplicity)\n\n3\n\nAccuracyEpochlayer#Var. of gradients (w.r.t.wl)Under review as a conference paper at ICLR 2023\n\nFigure 3: Illustration of weight/activation quantization in forward pass and gradient quantization in backward pass. MAC denotes the multiply–accumulate operations. Q(.) denotes the quantizer for weights (W), activations (A), or Gradients (G). In backward pass, the gradient quantization noise δq(gal−1 ) accumulated from the lth layer to the last layer is propagated to the (l − 1)th layer and added to the (l − 1)th layer’s gradient quantization noise e(gal−1 ) induced by itself.\n\ngxn =\n\n(cid:34)\n\n1 N σ\n\nN g ˆxn −\n\nN (cid:88)\n\nn=0\n\ng ˆxn − ˆxn\n\n(cid:35)\n\ng ˆxn ˆxn\n\n.\n\nN (cid:88)\n\nn=0\n\n(2)\n\n4 PROBLEM IDENTIFICATION: ACCUMULATION OF GRADIENT\n\nQUANTIZATION NOISE EXPLODES GRADIENTS\n\nGradients play a crucial role in backpropagation based optimization and make a huge impact on training stability and convergence speed. Intuitively, as we inject quantizers in between backward pass, the error signal become more and more noisy each time it passes through a quantizer. As the bit-width decreases, the quantization noise injected in error signal at each single layer increases exponentially. From a perspective of variance, since quantization introduces additive noise e(g) to the original signal g, the variance of quantization noise D(e(g)) is also added to the error signal D(g), which will be reflected in the propagated errors and will increase over the course of backpropagation. As shown in Fig. 2, under various bit-width of gradients, we observe the variance of quantized error signals expanded during backpropagation. As the bit-width decreases, the variance also inflated much more drastically on shallow layers than late layers, implying that quantization noise is the culprit of the variance accumulation and explosion. As a result, the quantization impact on error signal and the weight update are severely affected, especially in those early layers. Eventually in worse cases when the accumulated quantization noise is overwhelming, the training goes off nature course and crashes. (e.g. under extremely low bit-width Fig. 7)\n\nFig. 3 provides a glance at the accumulation mechanism of the gradient quantization noise in the backward pass. During backpropagation, the quantization of the error signal on the lth layer introduces the quantization noise denoted as e(gal ). e(gal ) is propagated to its predecessor - the (l−1)th layer, together with the gradient quantization noise δq(gal ) accumulated from the (l + 1)th layer to the last layer L. Similarly, e(gal−1 ) and δq(gal−1) are propagated to the (l − 2)th layer, and so on.\n\nAs shown in Fig. 3, both forward and backward pass have similar accumulation phenomenon, but why backward pass suffers more from the quantization during training? This is because quantization noise introduced in the forward pass are reflected in the computation graph w.r.t. the objective loss, therefore their impact can be partially offset by quantization-aware training, while quantization noise introduced during backward does not contribute to the objective loss.\n\nIn view of some cases under the setting of distributed gradient compression Alistarh et al. (2017), which only quantizes full-precision gradients after backpropagation is done, can stably train CNNs with as low as 4-bit gradients, we attribute the exploding gradient problem in FQT setting mainly to the accumulation of gradient quantization noise introduced by low-bit gradient quantizers.\n\n5 BATCHNORM AMPLIFIES THE ACCUMULATED NOISE\n\nIn this section, we develop a theoretical framework to understand the role of BatchNorm in gradient quantization, explaining why BatchNorm may worsen the gradient explosion problem in FQT. One can refer to Appendix A.2 for the proofs of the theorems.\n\n4\n\nl−1-thlayer (Conv/FC)෪Wl−1෥al−2MACal−1QAQWWl−1෪Wl෥alMACQWWll-thlayer (Conv/FC)Forward passBN-ReLUalQA෥alBN-ReLUBackward pass෥gWl−1MAC′෥gal−1QGl−1-thlayer (Conv/FC)gal−1,e(gal−1)BN-ReLUgal−2෥gWlMAC′෥galQGl-thlayer (Conv/FC)gal,δq(gal),δqgal,e(gal)BN-ReLUUnder review as a conference paper at ICLR 2023\n\nQuantifying the impact of BatchNorm on Noise Accumulation. Through theoretical studies, we find that BatchNorm may amplify the accumulation of gradient quantization noise. This finding might be counter-intuitive as BatchNorm has been expected to regularize the “variance” and prevent the gradient explosion problem, by scaling the activations in forward pass. However, not only vanilla BatchNorm mainly focuses on rectifying variances in forward pass, but also does not count in the situation where the gradient signals are noisy. When error signal passes through such scaling layer inside BatchNorm, the error signal is also scaled by the reciprocal of the corresponding scaling factor (σ) when calculating its derivative. When training process is in full-precision, such scaling on error signal is manageable. But when the error signal contains accumulated noise from previous layers, the noise is scaled at the same time, causing unpredictable behavior to the backpropagation. Hence our theoretical focus is fundamentally different from previous “variance rectification and reduction” studies.\n\nThe following theorem quantifies how specifically BatchNorm affects the accumulation effect. Assumption 1. δq(g ˆxi) and ˆxi are i.d.d. and are both zero-mean Zhao et al. (2021).\n\nTheorem 1. Given Assumption 1, for a BatchNorm layer in a quantized network, the relationship between the gradient quantization noise w.r.t. the BatchNorm’s input xi and that of the normalized input ˆxi depends on the σ of BatchNorm with batch size N , in the form of\n\nη =\n\nD(δq(gxi)) D(δq(g ˆxi))\n\n=\n\n1\n\nN 2σ2 (N 2 + 2N ).\n\n(3)\n\nRemark 1.1. In Eq. (3), we define the amplification factor of the accumulated noise as η, which is the ratio of statistical variances between the scaled error signal δq(gxi) and one before scaling δq(g ˆxi) (see Fig. 4). Corollary 1.1. To prevent BatchNorm from introducing more gradient quantization noise (i.e. the statistical variance D(δq(g ˆxi))) when propagating the error signal to preceding layers, a desirable η∗ should not be greater than 1. Thus, a desirable σ of the BatchNorm should be σ ≥ σ∗ = (cid:113)\n\n1 + 2\n\nN .\n\nIn short, Theorem 1 studies the amplification factor η over the accumulated gradient quantization noise when gradient passes through BatchNorm scaling, and Corollary 1.1 provides a close form solution of the ideal condition of the BatchNorm variance σ to satisify minimum noise accumulation.\n\nAs the accumulated quantization noise during backward cannot be automatically amortized by the training objective in FQT, ones are left with only options to either (1) minimize the primary source of noise by improving gradient quantizer design, or (2) minimize the accumulation of such noise. With the first choice being obvious, in this paper, we instead aim to raise people’s awareness about the second choice and the importance of properly scaling the noisy error signal to alleviate noise amplification problem in backward pass and the eventual training stability.\n\nFigure 4: propagation inside BatchNorm.\n\nIllustration of gradient\n\n6 OUR METHOD: RECTIFYING BATCHNORM FOR GRADIENT QUANTIZATION\n\nInspired by our theory in Sec. 5, we develop a solution to suppress the noise amplification effect in a principled way, which in turn reduces the gradient explosion for improved training of quantized networks with low bit gradients. Based on Theorem 1 and Corollary 1.1, we expect the σ to be larger than the ideal value σ∗, so that the noise amplification factor η = D(δq(gxi )) D(δq(g ˆxi )) between the output and the input of BatchNorm is minimized. Therefore, we propose a method to stabilize the training with low bit gradients, by rectifying the variance of BatchNorm computed in forward pass:\n\nmin.\n\nf (w)\n\ns.t. Lσ =\n\n1 L\n\nL (cid:88)\n\nl=1\n\nMSE\n\n(cid:16)\n\nmin\n\n(cid:16) σl\n\nσ∗ , 1\n\n(cid:17)\n\n(cid:17)\n\n, 1\n\n= 0,\n\n(4)\n\n5\n\nScalinggෝxiMAC′gyigxigσ2gμgγigβiUnder review as a conference paper at ICLR 2023\n\nwhere f : S → R is the regular objective loss (e.g., cross-entropy loss for classification) and w ∈ S denotes neural network weights. σL = {σl}L l=1 is the set of σ from all BatchNorm layers in the network with depth L. We use Mean Square Error (MSE) between σl σ∗ and 1 to enforce that σl at each BatchNorm layer approaches to σ∗. The Lagrangian dual approximation form of Eq. (4) is:\n\nf ′(w) = f (w) + λLσ,\n\n(5)\n\nwhere λ is an adjustable parameter to balance f (w) and the proposed rectification loss Lσ.\n\nGradient Computation and Computation Overhead. The overhead introduced by our proposed rectification term only has linear time complexity. During backpropagation, the error signal of rectification term Lσ when propagated to BatchNorm’s input x on channel i at layer l is:\n\n∂Lσ ∂ai l\n\n=\n\n(cid:40) 2\n\nσ∗ − 1\n\nσi l\n\nσ∗N ( 1 0,\n\n)(ai\n\nl − μi\n\nl), σi\n\nl < σ∗; otherwise\n\n,\n\n(6)\n\nwhere σ∗, N and channel-wise BatchNorm parameters μi l are all constant scalars during backpropagation, showing the added rectification is very cheap and negligible when training on devices capable of vectorized computation optimization including GPUs. More detailed analysis can be found in Appendix A.4.\n\nl, σi\n\n7 EXPERIMENTS\n\nTo evaluate our method, we conduct extensive experiments on various neural network architectures and popular datasets for image classification with low-bit gradients.\n\nExperimental Setup. To highlight the impact of BatchNorm, we only use two vanilla quantizers for gradients in all experiments without any optimizations: uniform quantizer and logarithmic quantizer (see Appendix A.1). Our method introduces only one hyper-parameter λ in Eq. (5), which is manually initialized then can be ramped down by the cosine rule during training or stay the same. For each parametrized layer in backpropagation, the gradient gw and error signal gx are quantized separately, and they can be quantized to different bit-widths. To evaluate our rectification method, we ensure all the used CNN architectures have BatchNorm layers, including ShallowNet and AlexNet-BN, the architecture details of which are listed in Appendix A.3. More details are listed in Appendix A.4\n\n7.1 MAIN RESULTS\n\nINT8 comparisons. We compare our method (training with Lσ) to state-of-the-art gradient quantization approaches reporting results with 8-bit gradients: UI8 Zhu et al. (2020), FP8 Wang et al. (2018), AFP Zhang et al. (2020), SBM Banner et al. (2018), DAINT8 Zhao et al. (2021). Using 8-bit Logarithmic quantizer for gradients and the proposed rectifier Lσ, Tab. 1 shows that our method outperforms the state-of-the-arts in almost all cases, despite that we simple use vanilla quantizers on the gradient, while the quantizer designs of the counterparts are heavily engineered, e.g. DAINT8 Zhao et al. (2021) adopts vector quantization to process error signal in channel-wise manner. We found that MobileNet-V2 on ImageNet is harder to train with 8-bit gradients with vanilla quantizer designs even training with our Lσ, ending up with around 1% accuracy drop than SOTA Zhao et al. (2021). As an ablation, for ResNet-20 on CIFAR10, our method boosts the accuracy by 1% from baseline training (w/o Lσ), also for ResNet-18 on ImageNet, Lσ achieves almost 2% improvement.\n\nComparisons on 4-bits gradients. Since there are very few works on quantized neural networks with less than 8-bit gradients, we compare our method to a 4-bit floating-point quantization method named FP4 Sun et al. (2020) which actually adopt 4 bit floating-point representations, and the baseline which is without the proposed Lσ. As shown in Tab. 2, we observe that in most cases our method with INT4 gradient quantization reports higher accuracy than FP4 Sun et al. (2020), despite FP4 Sun et al. (2020) adopts floating-point quantization with customized radix and scaling selections. We also observe that our method (w/ Lσ) performs better than the baseline, further verifies the effectiveness of the proposed Lσ. On the other hand, we observe MobileNet-V2 with INT4 gradients is still unstable and hard to converge even with our rectification method deployed (it explained why most FQT works did not report results for INT4 gradients on MobileNet-V2). Thus, we’d like to leave the FQT of MobileNet-V2 for future work.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nDataset\n\nArch\n\nCIFAR-10\n\nResNet-20\n\nMobile-V2\n\nResNet-18\n\nImageNet\n\nResNet-50\n\nMobile-V2\n\nMethod UI8 FP8 DAINT8 w/o Lσ Ours UI8 DAINT8 w/o Lσ Ours UI8 FP8 DAINT8 SBM w/o Lσ Ours UI8 SBM DAINT8 AFP w/o Lσ Ours UI8 DAINT8 AFP w/o Lσ Ours\n\nTop-1 92.0 92.2 92.8 91.9 92.9 93.4 94.4 94.5 94.6 69.7 67.3 70.2 69.6 69.1 70.9 76.3 76.3 76.6 76.2 76.4 76.8 71.2 71.9 70.5 70.74 70.9\n\nTable 1: Comparing state-of-the-art methods with bit-width W8A8G8. (Mobile-V2=MobileNet-V2)\n\n7.2 EXTREMELY LOW BIT-WIDTHS\n\nDataset\n\nArch\n\nCIFAR-10\n\nVGG-16\n\nResNet-18\n\nAlexNet-BN\n\nImageNet\n\nResNet-18\n\nResNet-50\n\nMethod FP4 w/o Lσ Ours FP4 w/o Lσ Ours FP4 w/o Lσ Ours FP4 w/o Lσ Ours FP4 w/o Lσ Ours\n\nTop-1 91.5 90.7 92.5 92.7 93.7 94.0 56.3 57.2 57.3 68.3 69.61 69.71 74.01 73.67 73.4\n\nTable 2: Comparison with FP4 gradient quantiation with bit-width W4A4G4. Log-INT4 quantizer is used for gradients.\n\nλ\n\nTop-1 (%)\n\n0.1 68.6\n\n0.25 68.4\n\n0.5 70.1\n\n1 67.0\n\nEffect of λ for\n\nTable 3: training quantized ResNet-18 on CIFAR-10 with bit-width W2A2G2.\n\nDespite scarce exploitation in the wild and the challenges, we further attempt to quantize gradients to even lower bit-widths, with different backbone network architectures, bit-widths combinations, and gradient quantizers, to further evaluate the theoretical capability of the proposed rectifier Lσ. When gradients are quantized to very low bit-widths, we expect the training becoming extremely unstable as the quantization noise and eventually the accumulation effect becoming much severer. Considering the increased training instability, we perform three independent trials for each experiments and report the comprehensive scores as (mean±std).\n\nSimple network architectures. As shown in Tab. 4, we first study the training of a two-layer quantized neural network ShallowNet on MNIST. We set the bit-width for weights, activations, and gradients as 2-bit (W2A2G2) and use Logarithmic quantizer for gradients. We observed that baseline training without Lσ crashed twice out of 3 repetitive runs, while the training with the proposed Lσ is stable throughout. In other words, our method outperforms the baseline by a large margin, improving the average accuracy by +54.4% (from 38.8% to 93.2%).\n\nOn larger dataset ImageNet, we also have the similar observation for AlexNet-Bn, where baseline method fails to train completely while our method with Lσ can train stably throughout three trials.\n\nMore complex network architectures. We further test the effectiveness of our method in FQT training of networks with more complex structures. We push the boundary of the lowest bit-widths settings we can achieve, as lowest as e.g. W2A2G2 on ResNet-18 in Tab. 5. We observed that the FQT on VGG-16 is slightly more sensitive to the quantization than ResNet-18, thus we have to set higher bit-widths for VGG-16. We also notice that on VGG-16, error (dx) requires more bit-width than gradients w.r.t. weights (dW ). In all, our method performs consistently better than the baseline (w/o Lσ) on all settings, in particular, the performance gain on VGG-16 is significant.\n\nAs an additional remark, we notice that compared to other models, ResNets are more robust against more quantization noise throughout our experiments as suggested in Tab. 2 and Tab. 5. We conjec-\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Training Loss lowNet@MNIST.\n\nfor\n\nShal-\n\n(b) Training Loss 16@CIFAR-10.\n\nfor VGG-\n\n(c) Training Loss 18@CIFAR-10.\n\nfor ResNet-\n\n(d) Test Acc. lowNet@MNIST.\n\nfor Shal-\n\n(e) Test Acc. 16@CIFAR-10.\n\nfor VGG-\n\n(f) Test Acc. 18@CIFAR-10.\n\nfor ResNet-\n\nFigure 5: Illustration of stabilized training of quantized networks with lower than 4-bit gradients w/ Lσ, compared to the baseline w/o Lσ. Our method (w/ Lσ) shows higher averaged accuracy and lower variance across repeated runs. X-axis in all sub-figures represents epochs.\n\nBit-widths (W/A/dx/dW )\n\n2/2/2/2\n\n2/2/3/2\n\nLσ\n\nw/o w/\n\nw/o w/\n\nRuns #2\n\n#1 ShallowNet on MNIST\n\n#3\n\nAvg Top-1 (%) Diff. (%)\n\n94.9 94.4\n\n10.0 92.0 AlexNet-BN on ImageNet\n\n11.4 93.3\n\n38.8 ± 48.7 93.2 ± 1.1\n\n- +54.4\n\n- 47.6\n\n- 49.46\n\n- 46.53\n\n-\n\n-\n\n47.86 ± 1.48 +47.86\n\nTable 4: Ablations of Lσ under extremely low bit-widths. Scores in red denotes failed train.\n\nture that it is due to the full-precision shortcuts within, making them naturally more robust against the accumulation effect during backward pass. Theoretical investigation towards this phenomenon would be an interesting future research topic.\n\n7.3 OTHER DISCUSSIONS\n\nCan Lσ stabilize training? Fig. 5 illustrates the improved training of quantized networks with less than 4-bit gradients, thanks to the rectification loss Lσ. Fig. 5a and Fig. 5d are ShallowNet trained on MNIST for bit-width configuration W2A2G2 with Logarithmic gradient quantizer. Fig. 5b and Fig. 5e are VGG-16 trained on CIFAR-10 for W4A4dx4dW 2 with Logarithmic gradient quantizer. Fig. 5c and Fig. 5f are ResNet-18 trained on CIFAR-10 for W4A4G4 with uniform gradient quantizer. We plot out the average loss/accuracy (AVG) and standard deviation of loss/accuracy (STD) of 3 trails separately. From Fig. 5a, Fig. 5b and Fig. 5c, we observe that the rectification loss Lσ dominates the training loss for the first few epochs, forcing the optimization adapted to the low-bit gradients. Afterwards, the training process becomes much more stable with training loss decreased and test accuracy increased gradually (see Fig. 5d, Fig. 5e and Fig. 5f). On the contrary, training without Lσ is not able to suppress the negative effect of gradient quantization noise, resulting in training instability or even crash.\n\nCan Lσ suppress the noise amplification effect? To verify such stabilizing effect indeed comes from the proposed rectification, we further studies its impacts on gradient distribution layer by layer. Fig. 6 illustrates the distribution of variances of gradients w.r.t. layer output activations when training quantized VGG-16 on CIFAR-10 with bit-width W4A4dx4dW 2. By injecting the rectification loss Lσ defined on BatchNorm in training objective function, one can see the noise amplification effect is largely suppressed, and thus the gradients are less exploded (measured by the variances of gradients).\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nbit-widths (W/A/dx/dW )\n\nGradient Quantizer\n\nVGG-16\n\n4/4/4/2\n\nUniform\n\n4/4/4/2\n\nLogarithm\n\nResNet-18\n\n2/2/2/2\n\nUniform\n\n2/2/2/2\n\nLogarithm\n\nLσ\n\nw/o w/ w/o w/\n\nw/o w/ w/o w/\n\nAvg Top-1 (%)\n\n50.0 ± 8.4 66.7 ± 8.7 66.8 ± 4.9 71.0 ± 5.4\n\n70.0 ± 1.5 71.0 ± 0.8 59.9 ± 2.1 61.6 ± 2.0\n\nTable 5: Ablations of Lσ and quantizers under extremely low bit-widths.\n\nFigure 6: Distribution of variances of gradients w.r.t. activations when training VGG-16 on CIFAR-10 with bit-width W4A4dx4dW 2. Logarithmic quantizer is used for gradients.\n\nFigure 7: Train loss curve of AlexNet on ImageNet with W4A4dx4dW 2 w/ or w/o BatchNorm. (dx denotes error signal, dW is gradients of weights, same below).\n\nAs a result, it help prevent crashing in training with low bit gradients (see NaN at the 2.4kth iteration for training without Lσ).\n\nEffect of λ. As shown in Tab. 3, we study the effect of the hyper-parameter λ when training quantized ResNet-18 on CIFAR-10 with bit-width W2A2G2. The optimal values of λ for different experiments could be different and we heuristically tune them separately.\n\nCan BatchNorm layer be discarded when training with low-bit gradients? Since BatchNorm amplifies the accumulated quantization noise during backpropagation, one may argue that a straightforward way to prevent the noise amplification effect is removing BatchNorm layer from the network architecture. To verify the point, we conducted experiments on training AlexNet with or without BatchNorm under W4A4dx4dW 2 using logarithmic quantizer. As shown in Fig. 7, training AlexNet without BatchNorm quickly collapsed in the early stage in training. This implies that at the moment BatchNorm is still an essential building block to deep CNNs training for its rectifying benefits mainly in forward pass, while our rectification method complementarily stabilizes the backward pass, at least in the case when quantization is applied especially in FQT.\n\n8 CONCLUSIONS\n\nIn this paper, we study an under-explored factor causing the gradient explosion problem when training deep CNNs with low-bit gradients, from a theoretical perspective. Our theory sheds light on the negative effect of BatchNorm in amplifying the accumulated gradient quantization noise during backpropagation, which leads to unstable training or even crash. The theory inspires a simple yet effective method to stabilize FQT with low-bit gradients, which consistently brings performance gain on a wide range of CNNs and datasets compared to state-of-the-art FQT algorithms.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDan Alistarh, Demjan Grubic,\n\nCommunication-efficient sgd via gradient quantization and encoding. Information Processing Systems, pp. 1709–1720, 2017.\n\nJerry Li, Ryota Tomioka, and Milan Vojnovic.\n\nQsgd: In Advances in Neural\n\nRon Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of\n\nneural networks. arXiv preprint arXiv:1805.11046, 2018.\n\nYoshua Bengio, Nicholas L ́eonard, and Aaron Courville. Estimating or propagating gradients\n\nthrough stochastic neurons for conditional computation. arXiv:1308.3432, 2013.\n\nYash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving In Proceedings of the low-bit quantization through learnable offsets and better initialization. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 696–697, 2020.\n\nJungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018.\n\nJungwook Choi, Swagath Venkataramani, Vijayalakshmi Srinivasan, Kailash Gopalakrishnan, Zhuo Wang, and Pierce Chuang. Accurate and efficient 2-bit quantized neural networks. In MLSys, 2019.\n\nSteven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen-\n\ndra S Modha. Learned step size quantization. In ICLR, 2020.\n\nJun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, and Joseph H In European\n\nHassoun. Post-training piecewise linear quantization for deep neural networks. Conference on Computer Vision, pp. 69–86. Springer, 2020.\n\nYonggan Fu, Han Guo, Meng Li, Xin Yang, Yining Ding, Vikas Chandra, and Yingyan Lin. Cpt: Efficient deep neural network training via cyclic precision. arXiv preprint arXiv:2101.09868, 2021.\n\nRuihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4852–4861, 2019.\n\nZichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. In European conference on computer vision, pp. 544–560. Springer, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026–1034, 2015.\n\nMark Horowitz. 1.1 computing’s energy problem (and what we can do about it). In 2014 IEEE\n\nInternational Solid-State Circuits Conference (ISSCC), 2014.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448–456. PMLR, 2015.\n\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2704–2713, 2018.\n\nQing Jin, Linjie Yang, and Zhenyu Liao. Adabits: Neural network quantization with adaptive bitwidths. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nIlya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n\nQian Lou, Feng Guo, Minje Kim, Lantao Liu, and Lei Jiang. Autoq: Automated kernel-wise neural\n\nnetwork quantizations. In ICLR, 2020.\n\nEldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman. Same, same but different: Recovering neural network quantization error through weight factorization. In International Conference on Machine Learning, pp. 4486–4495. PMLR, 2019.\n\nDaisuke Miyashita, Edward H Lee, and Boris Murmann. Convolutional neural networks using\n\nlogarithmic data representation. arXiv preprint arXiv:1603.01025, 2016.\n\nX Sun, N Wang, C Chen, J Ni, A Agrawal, X Cui, S Venka, K El, V Srini, and K Gopa. Ultra-low\n\nprecision 4-bit training of deep neural networks. In NeurIPS, 2020.\n\nMaolin Wang, Seyedramin Rasoulinezhad, Philip HW Leong, and Hayden KH So. Niti: Training integer neural networks using integer-only arithmetic. arXiv preprint arXiv:2009.13108, 2020.\n\nNaigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31, pp. 7675–7684. Curran Associates, Inc., 2018.\n\nLinjie Yang and Qing Jin. Fracbits: Mixed precision quantization via fractional bit-widths.\n\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 10612–10620, 2021.\n\nYukuan Yang, Lei Deng, Shuang Wu, Tianyi Yan, Yuan Xie, and Guoqi Li. Training highperformance and large-scale deep neural networks with full 8-bit integers. Neural Networks, 125:70–82, 2020.\n\nDongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pp. 365–382, 2018a.\n\nHongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without\n\nnormalization. In International Conference on Learning Representations, 2018b.\n\nXishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou, Jiaming Guo, Qi Guo, Zidong Du, Tian Zhi, et al. Fixed-point back-propagation training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2330–2338, 2020.\n\nKang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang, Zhenyu Gu, and Yinghui Xu. DisIn Proceedings of the Thirty-Fifth AAAI\n\ntribution adaptive int8 quantization for training cnns. Conference on Artificial Intelligence, 2021.\n\nShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.\n\nFeng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards unified int8 training for convolutional neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1969–1979, 2020.\n\nA APPENDIX\n\nA.1 GRADIENT QUANTIZER CHOICES\n\nFor gradient quantization, in each layer, the full-precision error dx and gradient dW are quantized, then de-quantized, and the quantized error propagated back to the next layer during backward pass\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nZhou et al. (2016); Zhu et al. (2020); Yang et al. (2020); Wang et al. (2018). Uniform quantizer is the default and common choice gradient quantization, where given the gradients g ∈ {dx, dW }, the asymmetric uniform quantizer with bit-width B and quantization levels ranging from [a, b] can be formulated as:\n\ngq = Quantu(g′, a, b, B) = round\n\n(cid:18)\n\nclip(g′, a, b) ·\n\n2B−1 − 1 b − a\n\n(cid:19)\n\n,\n\n(7)\n\nwhere g′ = g + δq(g) (see Sec. 3 for details) and clip(x, a, b) = min(max(x, a), b). The quantized gradients are de-quantized as ̃g = gq · 2B−1−1 . Similarly, the symmetric uniform quantizer can be defined as Quants = Quantu(g, −c, c, B) where the c is the clipping value.\n\nb−a\n\nWe also explored logarithmic quantizer Miyashita et al. (2016) in this paper for gradient quantization given by:\n\ngq = Quantlog(g′, c, B) =\n\n(cid:40)\n\nsign(g′) · 2Quantu(log2 |g′|,log2 (c)−2B−1,log2 (c),B), g′ ̸= 0; g′ = 0 0,\n\n(8)\n\nA.2 QUANTIFYING THE AMPLIFICATION EFFECT IN GQNA\n\nTheorem 1. For a BatchNorm layer within a quantized network, the relationship between gradient quantization error of BatchNorm’s input xi and that of the normalized input ˆxi only depends on the batch size N and the σ of BatchNorm, in the form of\n\nD(δq(gxi)) D(δq(g ˆxi))\n\n=\n\n1\n\nN 2σ2 (N 2 + 2N ).\n\nProof. First we extend the Eq.2 to its quantized counterpart:\n\n ̃gxi =\n\n(cid:34)\n\n1 N σ\n\nN ̃g ˆxi −\n\nN (cid:88)\n\ni=0\n\n ̃g ˆxi − ˆxi\n\n(cid:35)\n\n ̃g ˆxi ˆxi\n\nN (cid:88)\n\ni=0\n\nand\n\ne(gxi) =\n\n(cid:34)\n\n1 N σ\n\nN e(g ˆxi) −\n\nN (cid:88)\n\ni=0\n\ne(g ˆxi) − ˆxi\n\n(cid:35)\n\ne(g ˆxi ) ˆxi\n\n.\n\nN (cid:88)\n\ni=0\n\n(9)\n\n(10)\n\n(11)\n\nExpand Eq. (10) using Eq. (1):\n\ngxi + δq(gxi) + e(gxi ) =\n\n1 N σ\n\n− ˆxi\n\nN (cid:88)\n\ni=0\n\n(cid:34)\n\nN (g ˆxi + δq(g ˆxi) + e(g ˆxi )) −\n\n(g ˆxi + δq(g ˆxi) + e(g ˆxi))\n\nN (cid:88)\n\ni=0 (cid:35)\n\n(g ˆxi + δq(g ˆxi) + e(g ˆxi)) ˆxi\n\n.\n\nEliminating terms in Eq. (12) using Eq. (1) and Eq. (11), we have:\n\nδq(gxi) =\n\n(cid:34)\n\n1 N σ\n\nN δq(g ˆxi) −\n\nN (cid:88)\n\ni=0\n\nδq(g ˆxi) − ˆxi\n\n(cid:35)\n\nδq(g ˆxi) ˆxi\n\n.\n\nN (cid:88)\n\ni=0\n\n(12)\n\n(13)\n\nCalculate the variance D(·) of LHS and RHS of the above we have: (assume δq(g ˆxi) and ˆxi are i.d.d. and are both zero-mean)\n\nD(δq(gxi)) =\n\n(cid:32)\n\n1\n\nN 2σ2 D\n\nN δq(g ˆxi) −\n\nN (cid:88)\n\ni=0\n\nδq(g ˆxi) − ˆxi\n\n(cid:33)\n\nδq(g ˆxi) ˆxi\n\n(14)\n\nN (cid:88)\n\ni=0 (cid:33)\n\n=\n\n=\n\n1 N 2σ2\n\n1 N 2σ2\n\n(cid:34)\n\nN 2D(δq(g ˆxi)) + D\n\n(cid:32) N\n\n(cid:88)\n\ni=0\n\nδq(g ˆxi)\n\n+ D\n\nˆxi\n\n(cid:32)\n\n(cid:33)(cid:35)\n\nδq(g ˆxi) ˆxi\n\n(15)\n\nN (cid:88)\n\ni=0\n\n(cid:34)\n\nN 2D(δq(g ˆxi)) +\n\nN (cid:88)\n\ni=0\n\n(cid:32)\n\nD(δq(g ˆxi)) + D\n\nˆxi\n\nN (cid:88)\n\ni=0\n\n(cid:33)(cid:35)\n\nδq(g ˆxi) ˆxi\n\n(16)\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\n=\n\n=\n\n=\n\n=\n\n1 N 2σ2\n\n1 N 2σ2 1\nN 2σ2 (cid:20) 1 σ2 +\n\nTherefore,\n\n(cid:34)\n\nN 2D(δq(g ˆxi)) + N D(δq(g ˆxi)) + D( ˆxi)\n\n(cid:35)\n\nD(δq(g ˆxi))D( ˆxi)\n\n(17)\n\nN (cid:88)\n\ni=0\n\n(cid:2)N 2D(δq(g ˆxi)) + N D(δq(g ˆxi )) + N D(δq(g ˆxi))(cid:3)\n\n(cid:2)N 2D(δq(g ˆxi)) + 2N D(δq(g ˆxi))(cid:3)\n\n(cid:21)\n\n2 N σ2\n\nD(δq(g ˆxi)).\n\nD(δq(gxi)) D(δq(g ˆxi))\n\n=\n\n1\n\nσ2 +\n\n2\n\nN σ2 =\n\n1\n\nN 2σ2 (N 2 + 2N ).\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\nA.3 ARCHITECTURE DETAILS\n\nShallowNet.\n\nclass ShallowNet(nn.Module): def __init__(self):\n\nsuper().__init__() self.conv1 = nn.Conv2d(1, 20, 5, 1) self.bn1 = nn.BatchNorm2d(20) self.relu1 = nn.ReLU(inplace=False) self.pool1 = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(20, 50, 5, 1) self.bn2 = nn.BatchNorm2d(50) self.relu2 = nn.ReLU(inplace=False) self.pool2 = nn.MaxPool2d(2, 2) self.avgpool = nn.AvgPool2d(4, stride=1) self.fc = nn.Linear(50, 10)\n\ndef forward(self, x):\n\nx = self.pool1(self.relu1(self.bn1(self.conv1(x)))) x = self.pool2(self.relu2(self.bn2(self.conv2(x)))) x = self.avgpool(x) x = x.view(x.size(0), -1) x = self.fc(x) return x\n\nAlexNet-BN.\n\nclass AlexNetBN(nn.Module):\n\ndef __init__(self, num_classes=1000): super(AlexNetBN, self).__init__() self.features = nn.Sequential(\n\nnn.Conv2d(3, 96, kernel_size=12, stride=4), nn.ReLU(inplace=True),\n\nnn.Conv2d(96, 256, kernel_size=5, padding=2, groups=2,\n\nbias=False),\n\n(cid:44)→ nn.BatchNorm2d(256, eps=1e-4, momentum=0.9), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), nn.ReLU(inplace=True),\n\nnn.Conv2d(256, 384, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(384, eps=1e-4, momentum=0.9), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), nn.ReLU(inplace=True),\n\nnn.Conv2d(384, 384, kernel_size=3, padding=1, groups=2,\n\n(cid:44)→\n\nbias=False),\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nnn.BatchNorm2d(384, eps=1e-4, momentum=0.9), nn.ReLU(inplace=True),\n\nnn.Conv2d(384, 256, kernel_size=3, padding=1, groups=2,\n\nbias=False),\n\n(cid:44)→ nn.BatchNorm2d(256, eps=1e-4, momentum=0.9), nn.MaxPool2d(kernel_size=3, stride=2), nn.ReLU(inplace=True),\n\n)\n\nself.classifier = nn.Sequential(\n\nnn.Linear(256 * 6 * 6, 4096, bias=False), nn.BatchNorm1d(4096, eps=1e-4, momentum=0.9), nn.ReLU(inplace=True), nn.Linear(4096, 4096, bias=False), nn.BatchNorm1d(4096, eps=1e-4, momentum=0.9), nn.ReLU(inplace=True), nn.Linear(4096, num_classes),\n\n)\n\ndef forward(self, x):\n\nx = self.features(x) x = x.view(x.size(0), 256 * 6 * 6) x = self.classifier(x) return x\n\nA.4 TRAINING SETTINGS AND HYPER-PARAMETERS\n\nFollowing Sun et al. (2020), we adopt Parameterized Clipping Activation (PACT) Choi et al. (2018) for activation quantization and Statistics Aware Weight Binning (SAWB) Choi et al. (2019) for weight quantization. We choose the SGD optimizer and train all quantized network models from scratch. The learning rate is adjusted with a cosine scheduler (Loshchilov & Hutter (2017)). For MNIST, we set the learning rate as 0.1, weight decay as 0.0001, and train for 20 epochs. For CIFAR-10, we set the learning rate as 0.1 for all architectures, weight decay 0.0001 for ResNet-18 and VGG-16, and weight decay 0.0004 for MobileNet-V2. For ImageNet, we set the learning rate as 0.0512 and weight decay as 0.0001 for ResNet-18, learning rate 0.01 and weight decay 0.0005 for AlexNet, and learning rate 0.1 and weight decay 0.00004 for MobileNet-V2. All models on CIFAR-10 and ImageNet are trained for 120 epochs, except for MobileNet-V2 that is trained for 150 epochs. For ImageNet, training images are randomly cropped to 224×224 and then randomly flipped horizontally. For experiments on MNIST and CIFAR-10, we repeat the training of each model for 3 runs by varying the random seed and report the average accuracy with standard deviation. Same as other INT8 works Zhu et al. (2020), we left the first and the last weighted layers as well as activations in full-precision for all INT8 experiments. For INT4 experiments, we followed the setting in Choi et al. (2019) that only left shortcut layers in ResNets in full-precision and quantized all other layers.\n\nWe specify the choices of λ and its ramping down strategy as below.\n\nComparing SOTA with 8-bit gradients\n\nDataset\n\nCIFAR-10\n\nImageNet\n\nArch ResNet-20 MobileNet-V2 ResNet-18 ResNet-50 MobileNet-V2\n\nλ 0.5 0.5 0.5 0.5 0.5\n\nRamping down? N\nN N\nN N\n\nTable 6: Comparing state-of-the-art with 8-bit gradients.\n\nComparing SOTA with 4-bit gradients\n\nExperiments with lower-than-4-bit gradients\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nDataset\n\nCIFAR-10\n\nImageNet\n\nArch VGG-16 ResNet-18 AlexNet ResNet-18\n\nλ 0.5 0.5 0.5 0.5\n\nRamping down? Y\nY N\nY\n\nTable 7: Comparing our 4-bit fixed-point gradient quantization to the 4-bit floating-point gradient quantiation FP4 Sun et al. (2020).\n\nbit-widths (W/A/dx/dW )\n\nGradient Quantizer\n\nλ\n\nRamp down?\n\nShallowNet on MNIST\n\n2/2/2/2\n\nUniform\n\n0.1\n\n4/4/4/2 4/4/4/2\n\n2/2/2/2 2/2/2/2\n\nVGG-16 on CIFAR-10\n\n0.5 Uniform Logarithm 0.5 ResNet-18 on CIFAR-10\n\n0.5 Uniform Logarithm 0.1\n\nN\n\nY Y\n\nY Y\n\nTable 8: Less than 4-bit gradients.\n\nA.5 DETAILED ANALYSIS OF COMPUTATION OVERHEAD\n\nTo more directly derive in the linear complexity conclusion, one can simplify Eq. (6) into ∂Lσ =\n∂al C1 ⊙ al + C2, where ⊙ denotes the element-wise multiplication, C1, C2 are both constant tensors with the same shape as al. The partial derivative ∂Lσ is further aggregated with the error signal ∂al from objective function ∂f (w) no extra computation cost.\n\nto compute the gradient w.r.t. weights ∂f ′(w)\n\n. Here Lσ introduces\n\n∂wl\n\n∂al\n\nTab. 9 gives an empirical verification of the actual training overhead of our method compared to the baseline.\n\nDataset\n\nImageNet\n\nArch ResNet-18 ResNet-50 MobileNet-V2\n\nw/o Lσ 43.4 143.32 162.4\n\nOurs 44.6 152.58 171.2\n\nDiff. 0.67% 6.46% 5.41%\n\nTable 9: Comparison of training time (hours).\n\nA.6 DETAILED ABLATIONS ON HYPER-PARAMETER (ON HIGHER BIT-WIDTHS)\n\nTo comprehensively evaluate the influence of the choices of hyper-parameter λ, we further tested the model performance on higher bit-width.\n\nTab. 10 shows the results under different λ values of ResNet-18 on CIFAR-10. The quantizer choice for gradients is log quantizer.\n\nλ\n\nTop-1 (%)\n\n0 93.7\n\n0.1 93.88\n\n0.25 93.57\n\n0.5 94.0\n\n1 93.71\n\nTable 10: Ablation study of λ under bit-widths W4A4G4.\n\n15",
    "reference": "# Summary Of The Paper\n\nThe paper studies the problem of quantized training where the gradients are quantized as well in addition to quantization aware training methods. The paper builds up on an interesting observation related to the detrimental effect of batch normalisation in quantized training. It is shown that batch norm amplifies the accumulated gradient quantization noise during the backpropagation. Based on this observation, a rectification method is proposed to reduce the negative effect of accumulated gradient quantization noise.\n\n# Strength And Weaknesses\n\nStrengths:\n- The paper is easy to follow and well-written.\n- There has been a lot of research focused on quantization aware training methods in the literature but this paper aims to solve the challenging task of quantized training. It is an important problem to increase training efficiency and this paper makes a good attempt at it.\nThe proposed method achieves improvements consistently on multiple datasets.\n\nWeaknesses:\n- The computational overhead of the approach has been shown to be not significant but it would be useful if the authors put some empirical comparisons on training time of the proposed method vs the baselines.\n- In table 1, the results for Mobilenet-V2 are considerably worse than UI8 and DAINT8. Can the authors explain the reason why the proposed method is worse in that setup?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: The paper is easy to follow and clearly written.\nQuality and Novelty: Technically solid and proposed approach is novel to the best of my knowledge. The empirical results are ok but not too strong.\nReproducibility: I did not find the code with the paper. I would suggest the authors to release the code for the sake of reproducibility during the review process.\n\n# Summary Of The Review\n\nOverall the paper makes a good attempt at the problem of quantized training and the proposed idea seems novel as well as technically solid to me. Though the results are not particularly strong. Still I would recommend weak acceptance at this stage.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDO SUMMARIZATION MODELS SYNTHESIZE?\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nMulti-document summarization entails producing concise synopses of collections of inputs. For some applications, the synopsis should accurately synthesize inputs with respect to a key property or aspect. For example, a synopsis of film reviews all written about a particular movie should reflect the average critic consensus. As a more consequential example, consider narrative summaries that accompany biomedical systematic reviews of clinical trial results. These narratives should fairly summarize the potentially conflicting results from individual trials. In this paper we ask: To what extent do modern multi-document summarization models implicitly perform this type of synthesis? To assess this we perform a suite of experiments that probe the degree to which conditional generation models trained for summarization using standard methods yield outputs that appropriately synthesize inputs. We find that existing models do partially perform synthesis, but do so imperfectly. In particular, they are over-sensitive to changes in input ordering and under-sensitive to changes in input compositions (e.g., the ratio of positive to negative movie reviews). We propose a simple, general method for improving model synthesis capabilities by generating an explicitly diverse set of candidate outputs, and then selecting from these the string best aligned with the expected aggregate measure for the inputs, or abstaining when the model produces no good candidate. This approach improves model synthesis performance. Our hope is that by highlighting the need for synthesis (in some summarization settings), this work motivates further research into multi-document summarization methods and learning objectives that explicitly account for the need to synthesize.\n\n1\n\nINTRODUCTION\n\nMulti-document summarization (MDS) models aim to distill inputs into concise synopses that preserve key content. Examples of MDS include summarizing news articles (Dang, 2005; Fabbri et al., 2019; Ghalandari et al., 2020; Evans et al., 2004), answering questions from multiple sources (Dang, 2006), and producing overviews of scientific literature (Liu et al., 2018; Lu et al., 2020; Moll ́a & Santiago-Mart ́ınez, 2012; Wallace et al., 2020; DeYoung et al., 2021). We expect summarization models to produce outputs consistent with inputs (Kryscinski et al., 2020; Nan et al., 2021a), e.g., discussing the same types of entities (Nan et al., 2021b) and allowing one to answer questions similar in a way that is consistent with individual inputs (Wang et al., 2020a; Scialom et al., 2021).\n\nIn some applications models must synthesize inputs—i.e., aggregate potentially conflicting information—to yield an accurate synopsis (Figure 1). As a simple example, consider the metareviews of movies featured on Rotten Tomatoes,1 which provide a consensus view of individual critic opinions. These reviews should therefore reflect the mean and range of sentiment implicit in the input critiques: A summary of mostly negative reviews (e.g., Gigli) should communicate that the film was widely panned; a summary of mixed reviews (as in the case of The Fifth Element) ought to convey that critics disagreed and discuss the main positive and negative attributes.\n\nA more consequential example is the task of summarizing the evidence presented in clinical trials. Individual trials will frequently present conflicting evidence about whether or not a particular health intervention is effective. An ideal summary of the evidence would appropriately weigh the findings presented in the constituent inputs and reflect the evidence on balance.\n\n1A website that aggregates film reviews: https://www.rottentomatoes.com/.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Two multi-document summarization tasks where models must implicitly synthesize inputs to produce accurate summaries. Left: Summarizing film reviews with varying sentiment to yield a critics consensus. Right: Summarizing trials that have evaluated a particular medical invention.\n\nWhat are the desiderata of multi-document synthesis? First, summaries produced by models should be consistent with the input data, with respect to the latent property of interest. In the case of Rotten Tomatoes, the sentiment of the summary should be in line with the aggregate sentiment expressed in the individual critic reviews. A corollary to this is that models should be sensitive to changes in the composition of inputs, e.g., removing most of the negative reviews from a set of inputs should yield a summary with a corresponding increase in the expressed sentiment.\n\nIn this work we evaluate neural MDS models with respect to these criteria. To this end we use a meta-reviews dataset from Rotten Tomatoes (Leone, 2020) and a dataset of systematic reviews (meta-analyses) summarizing the evidence about medical interventions (Wallace et al., 2020). For the former we probe the degree to which generated meta-review sentiment agrees with the expected aggregate sentiment score; for the latter we evaluate whether the generated summary indicates that the input evidence suggests, on balance, that the intervention under consideration was effective.\n\nOur main contributions are summarized as follows. (1) To the best of our knowledge, this is the first work to investigate implicit synthesis in summarization, and the degree to which modern models are capable of this.2 (2) We show that “off-the-shelf” neural MDS models are somewhat inconsistent and insensitive with respect to performing synthesis in summarization. (3) We propose and evaluate a simple and general technique which involves generating a diverse set of output candidates (Vijayakumar et al., 2016) and then selecting from these on the basis of agreement with an expected aggregate measure (based on inputs), with promising results.\n\n2 SYNTHESIS AND SUMMARIZATION\n\n|\n\nXi|}\n\nxi1, ..., xi\n\nIn standard multi-document\n\nsummarization, we assume inputs\n\n(Xi, yi), where Xi = . We then typically train a summarization model with parameters θ, to consume {\nXi and yield summaries ˆyi as similar as possible to targets yi. More precisely, the standard objective entails finding estimates for θ which maximize target token log-probabilities. Assuming the input documents xij in Xi have been linearized (i.e., concatenated, usually with adjoining special tokens to demarcate individual inputs) into a string x⊕i of input tokens, this objective takes the form: (cid:80)| 1), x⊕i ), where pθ is a probability assigned to the token at position t in the (linearized) target x⊕i by a summarization model with parameters θ. By myopically focusing on encouraging the model to produce tokens that mimic the targets, this objective aligns with standard (but flawed) measures of automated summary quality like ROUGE (Lin, 2004), which quantify n-gram overlap between targets yi and outputs ˆyi.\n\nyi| t=1 log pθ(yit|\n\nyi1, ..., yi(t\n\n−\n\nWe are interested in settings in which there is an additional, latent property implicit in the constituent input texts xij, zij. For example, zij might reflect the sentiment in critique j of the film indexed by i. Summaries should synthesize this aspect, i.e., the generated summary ˆyi should implicitly convey an aggregated zi which reflects a synthesis or aggregation G over Zi = .\nXi|} That is, we assume zi = G(Zi) . In both cases considered here—summaries of film critiques and synopses of clinical trials evidence—G can reasonably be assumed to be a (weighted) mean, Xi| G(Zi) = 1 j=1 αijzij. That is, summaries should roughly reflect the average sentiment and reported treatment effect in the cases of movie reviews and clinical trial reports, respectively.\n\nzi1, ...zi\n\n(cid:80)|\n\nXi|\n\n{\n\n|\n\n|\n\n2See Appendix B for related content aggregation work, over structured relations Shah et al. (2021a).\n\n2\n\n...The Fifth Element is a bold, bright, loud, rowdy, lush, extravagant science fiction space opera ...Narratively challenged, visually monotonous and aurally overpowering, The Fifth Element is a staggering accretion of all the wrong elements ...... The Fifth Element is a fantastic piece of pop sci-fi that never takes itself too seriously}There was no significant difference in the risk of hospitalisation between hydroxychloroquine and placebo groupsThe effect size of hydroxychloroquine was higher than placebo for COVID-19 symptomatic infection ... although this was not statistically significant.Synthesizing movie reviewsSynthesizing reports of clinical trials}...The evidence does not support use of hydroxychloroquine for treating COVID-19.Under review as a conference paper at ICLR 2023\n\nNumber of metareviews Avg. metareview length Total number of inputs Avg. number of inputs Avg length of individual input Avg length of concatenated inputs Target Percent Positive\n\nTrain 7251 32.0 195033 26.9 30.6 822 59.5\n\nDev 932 32.6 24336 26.1 30.8 804 62.1\n\nTest 912 32.4 24474 26.8 30.6 822 61.2\n\nTrain 1675 101 11054 6.6 475 2641 31.9\n\nDev† 360 107 1238 3.4 379 1336 31.4\n\nTest 397 111 2669 6.7 449 2544 35.0\n\nTable 1: Dataset statistics for movie reviews (left) and systematic reviews (right). Number of metareviews, average meta-review length (tokens), number of input reviews per split, average number of inputs per instance, average total length of an input to an instance. For movie reviews, the target percent positive reports the fraction of metareviews with a positive sentiment; for systematic reviews this refers to the fraction of metareviews reporting a significant effect. We subset the original dev set to instances of\n\n4k tokens (to accommodate T5; the other models can consume up to 16k).\n\n†\n\n≤\n\nWe investigate the following questions. (1) Do model summaries ˆyi reflect the anticipated aggregate aspect of interest? That is, how well calibrated is the aspect communicated in the generated summary (ziˆy) compared to the expected zi? (2) Can we improve the ability of summarization models to synthesize by explicitly incorporating synthesis targets zi into the decoding process?\n\nWe propose a simple inference-time procedure to explicitly preference output candidates that align with the expected aggregate property of interest (e.g., average sentiment), and report promising results for the approach. This strategy also naturally lends itself to cautious summarization, i.e., approaches in which we allow the model to abstain from generating an output if it does not produce any candidates that reflect the anticipated aggregate measure.\n\n3 DATASETS AND MEASUREMENTS\n\n3.1 MOVIE REVIEWS\n\nWe first consider a dataset comprising movie reviews and associated meta-reviews summarizing these from Rotten Tomatoes. An in-house staffer summarizes audience reviews 3 into meta-reviews. These meta-reviews synthesize the constituent input reviews, and reflect the aggregate critic reception of a film. Each meta-review is associated with a numerical “Tomatometer” score, which is an overall measure of what percent reviews were positive for the corresponding film (G then is an average of the positive indicator per review). The Rotten Tomatoes dataset we use comprises 9095 movies with meta-reviews constructed from 244,000 individual reviews (Table 1).\n\nMeasuring sentiment in movie reviews. As our measure g we train a BERT model (Devlin et al., 2019) using the continuous (fine-grained) sentiment targets provided in the SST dataset (Socher et al., 2013).4 We trained this model for 3 epochs using a learning rate of 5e-5 using the Huggingface library5 with no hyperparameter tuning. While the raw text of the SST dataset is in-domain, the targets themselves are not. We find a reasonably strong correlation between our sentiment estimates and the “true” meta-review sentiment (“Tomatometer” score): The R2 (centered) is 0.696, mean squared error (MSE) of 0.022, and Pearson’s r of 0.836 (Figure 2, upper left).\n\n3.2 BIOMEDICAL SYSTEMATIC REVIEWS OF TREATMENTS\n\nOur second dataset is a collection of systematic reviews from the Cochrane Collaboration.6 This dataset comprises roughly 2600 systematic reviews summarizing a total of 16,500 clinical trials evaluating interventions in healthcare (Table 1). Each review includes both a natural language sum-\n\n3written by designated “top-critics”, audience members recognized for quality and quantity of reviews 4SST is itself based on a collection of Rotten Tomatoes critic reviews (Pang & Lee, 2005). We verified that the SST text fragments do not overlap with any of our target reviews by manually checking any (fragment, review) pair with substantial (>= 75%) overlap for approximately one quarter of all reviews.\n\n5https://github.com/huggingface/transformers/blob/main/examples/\n\npytorch/text-classification/run_glue.py\n\n6An international non-profit dedicated to helping healthcare providers make evidence-based decisions.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Movie Reviews: Actual vs. Predicted Sentiments on generated summaries. We replaced LED with human outputs (upper left) for comparison; see Figure 8 in Appendix D for all models.\n\nmary and accompanying statistical meta-analysis results. The latter provides an aggregate statistical summary of the individual (study-level) data extracted from the trials included in each review. The natural language summary should accurately convey and contextualize the findings of the metaanalysis. Therefore, the (lack of) treatment efficacy communicated in a given summary should generally agree with the direction of the corresponding meta-analytic point estimate.\n\nMeasuring effects in evidence syntheses For systematic reviews of clinical trials, we resort to a less granular classification model g(xij), g(yi) which attempts to infer whether a given piece of In particular we use RobotReviewer (Marshall et al., text reports a significant result or not. 2017; DeYoung et al., 2020). Given a narrative describing a clinical trial result (or a systematic review summary of such results), RobotReviewer predicts whether the reported result indicates a significant effect of the treatment being investigated, or not. We can compare this prediction to the “truth”, which here is derived from the meta-analytic result (specifically by checking whether p < 0.05). Applying this off-the-shelf model to the manually composed summaries accompanying the meta-analyses in our Cochrane set, we observe a macro-average F1 score of 0.577 (Table 10, Appendix D), providing a reasonable (if weak) measure for this task.\n\n4 MODELS\n\nWe evaluate a suite of transformer (Vaswani et al., 2017) summarization models: Longformer (Beltagy et al., 2020), Pegasus (Zhang et al., 2020), PRIMERA (Xiao et al., 2021), and T5 (Raffel et al., 2020). PRIMERA was designed and pre-trained specifically for multi-document summarization specifically. And while not explicitly designed as multi-document summarization models, both Pegasus Zhang et al. (2020) and T57 have been used on multi-document tasks, while Longformer has been used for a related multi-document summarization task (DeYoung et al., 2021). For all models we mostly use hyperparameters defaulted to in their respective huggingface implementations. We conduct a hyperparameter sweep over optimization steps and learning rate, selecting the best model by ROUGE1 performance on the dev set (Appendix C, Tables 8. 9).\n\n5 EXPERIMENTS\n\n5.1 HOW WELL DO SUMMARIZATION MODELS SYNTHESIZE?\n\nWe report sentiment performance for all models in Table 2. These are metrics quantifying the strength of the relationship between (a) the continuous sentiment inferred (via our text-regression\n\n7https://huggingface.co/osama7/t5-summarization-multinews\n\n4\n\n0.00.20.40.60.81.0HumanPRIMERA0.00.10.20.30.40.50.60.70.80.91.00.00.20.40.60.81.0T50.00.10.20.30.40.50.60.70.80.91.0PegasusPredicted Metareview SentimentActual SentimentUnder review as a conference paper at ICLR 2023\n\nR2 LED 0.551 PRIMERA 0.608 0.516 T5 0.530 Pegasus 0.697 Reference\n\nPearson’s r MSE 0.042 0.742 0.037 0.780 0.046 0.720 0.044 0.730 0.023 0.836\n\nROUGE1 0.242 0.254 0.253 0.245\n\nLED PRIMERA T5 Pegasus Reference\n\nF1-score ROUGE1 0.259 0.253 0.206 0.212\n\n0.490 0.526 0.521 0.568 0.577\n\nTable 2: Base synthesis results. Movie reviews (left): correlations between sentiment measured in model outputs and target sentiments. We report R2, Pearson’s r, and mean-squared errors. Systematic reviews (right): we report macro-averaged F1s. ROUGE1 included for reference.\n\nFigure 3: The spread of sentiment/treatment effect measured in outputs produced from permuted input orderings. Left: Movie review sentiment. Right: Systematic review significance prediction entropy (0 indicates order insensitivity) on the subset of reviews that report significant effects.\n\ng) over model generated or reference (human written) summaries and (b) the reference sentiment (Tomatometer) score. Across these metrics, correlations between the sentiment measured in model generated outputs and the Tomatometer score are considerably lower than that between the same measurement over human-composed summaries and said score. Based on these metrics, human authors do a better job of synthesis than the models when composing their summaries.\n\nFor systematic reviews (Section 3.2), we are able to measure g whether a text appears to report significant treatment effect or not, and we can compare this against the p-value from the corresponding statistical meta-analysis. This permits only a coarse assessment of synthesis, as we are unable to measure correlations. Instead we report classification metrics describing how often the effect significance inferred from a summary (generated or manually written) matches the ground truth derived from the meta-analysis (Table 2). The results are qualitatively similar to the sentiment case, in that the humans appear to do a better job of synthesis — as best we can measure, the significance reported in their summaries better aligns with the statistical results than in model generated summaries.\n\n5.2 SENSITIVITY TO INPUT ORDERING\n\n{\n\nxi1, ..., xi\n\nSynthesis of inputs should be invariant to ordering (e.g., the critics’ consensus on a film does not depend on the order in which one reads the reviews). Here we evaluate if models are sensitive to input orderings with respect to the synthesized aspect of interest (ziˆy) in the resultant outputs. Specifically, Xi = will constitute an arbitrary ordering of inputs reflected in the linearized version x⊕i . This ordering should not affect the aggregate aspect ziˆy in the summary. To evaluate if models realize this invariance, we permute the instance i inputs Xi (and, consequently, the linearized x⊕i ) one hundred times, randomizing input orderings. For each such permutation ̃Xi (and associated ̃x⊕i ), we generate a summary ˆyi and estimate of the resultant aspect ̃ziˆy, using the corresponding measurement model. By repeating this process for each instance i, we can construct an empirical distribution over ̃ziˆy’s under different random orderings.\n\nXi|}\n\n|\n\nMovie reviews. We zero-mean the ̃ziˆy’s inferred over each instance, and combine the distributions from all instances into a histogram (Figure 3 left). This shows the spread of sentiments inferred over outputs under random input orderings minus the corresponding instance mean sentiment. Were\n\n5\n\n0.00.10.20.30.4LEDPRIMERA0.40.20.00.20.40.00.10.20.30.4T50.40.20.00.20.4PegasusDensitySentiment Difference from Mean0.000.250.500.751.00LEDPegasus0.00.20.40.60.81.00.000.250.500.751.00PRIMERA0.00.20.40.60.81.0T5Average Predicted Effect Entropy / Significant EffectsDensityUnder review as a conference paper at ICLR 2023\n\nR2 LED 0.524 PRIMERA 0.572 0.481 T5 0.499 Pegasus\n\nPearson’s r MSE 0.057 0.724 0.052 0.756 0.063 0.694 0.060 0.706\n\nF1-score Accuracy LED 0.510 PRIMERA 0.533 0.469 T5 0.452 Pegasus\n\n0.684 0.680 0.675 0.658\n\nTable 3: Movie reviews (left): Correlation between subsampled inputs and generated meta-reviews. Systematic reviews (right): macro-averaged results (F1 and accuracy) for subsampled inputs.\n\nFigure 4: Model sentiment sensitivity to manipulated input sentiment. The intensity patterns indicate that models tend to oscillate between low and high sentiments in outputs, and are not responsive to subtler shifts in input sentiment compositions. For context we include a model regression (blue) and the reference sensitivity regression (black).\n\na model completely invariant to ordering, the empirical distribution over these differences would collapse to 0. Instead, we observe a relatively wide spread in the sentiment measured over outputs generated from different permutations, indicating a counter-intuitive sensitivity to orderings.8\n\nSystematic reviews. For each Xi we have 100 order permutations and associated summaries; we infer whether these report significant results or not, and record the fraction that do (pi). If models were invariant to ordering, this fraction would always be 0 or 1. Values in-between suggest the model flips the report conclusion as a result of different input orderings. We calculate the entropy of pi to quantify this. Figure 3 (right) shows a histogram of these entropies calculated over the subset of examples where the associated meta-analysis indicates a significant effect.9 Densities away from zero indicate sensitivity to ordering.\n\n5.3 SENSITIVITY TO INPUT COMPOSITION\n\nSynthesis models should be responsive to changes in the distribution of the attribute to be synthesized in the input composition: If we increase the ratio of positive to negative reviews in an input set, we would anticipate a concomitant change in the sentiment communicated in the meta-review ziˆy. To assess if models meet this synthesis desiderata, we manipulate model inputs Xi in such a way to induce an expected change in the target measure ziˆy; we then measure if the output yields a summary that aligns with this expected change.\n\nMovie reviews. We manipulate the ratio of positive to negative reviews and observe the resultant change in the property of interest latent in the corresponding output. We take movies with mixed reviews, and delete 10%, 20%, 30%, ..., 100% of the positive inputs, retaining the negative inputs; we then repeat the process but instead remove negative inputs. For each of these permutations, we measure the input sentiment, the meta-review sentiment, and how well they correlate (Table 3).\n\nFigure 4 plots the relationship between the fraction of positive reviews in the (manipulated) input sets and the granular sentiment score inferred over the resultant outputs. The models are generally undersensitive to changes in their input: rather than having a change in meta-review sentiment equivalent in size to changes in input sentiment (a slope of 1, as we observe when we fit a model to the human written summaries). Models tend to have trouble changing their sentiment, and require a large change in input distribution to substantially change the sentiment communicated in the output.\n\n8For a ROUGE1 comparison, see Appendix E, Figure 10. 9These are the more interesting cases; we provide results over the entire dataset in Appendix Figure 9.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Proposed strategy to improve synthesis. We generate an intentionally diverse set of output candidates (Vijayakumar et al., 2016) and then select from these the text that best agrees with the predicted aggregate property of interest (here, sentiment). We can also abstain when the model fails to yield an appropriate output.\n\nSystematic Reviews. To measure sensitivity to changes in input composition, we manipulate our inputs Xi such that the meta-analysis result (target ziˆy) flips from a significant effect to no effect, or from no effect to an effect. Operationally, we do this by first taking of a subset of the reviews that have conflicting evidence (yielding 139 unique reviews). We then order inputs in these by (weighted) effect sizes,10 and remove subsets which ought to flip the significance result.\n\n6\n\nIMPROVING SYNTHESIS IN SUMMARIZATION\n\nWe propose a simple post-hoc approach to improving the synthesis performed by multi-document summarization models. This involves the following steps: (1) Generate an explicitly diverse set of output candidates11; (2) Select from these as the final output the candidate that best agrees with the expected synthesis result (as predicted by an external model).12\n\nFor (1), we rely on a previously proposed technique for generating diverse outputs Ci from input x⊕i , namely Diverse Beam Search (DBS) (Vijayakumar et al., 2016). This method modifies standard beam search to maintain multiple groups of beams. During decoding, a term is added to the next-token log probabilities which effectively penalizes production of (partial) strings similar to candidates on beams in other groups.13\n\nIn (2) we would like to select the output that best synthesizes the property of interest; this requires a mechanism for specifying what we expect the synthesized property be, given the inputs. For example, if we know the sentiment scores associated with input movie reviews, we might enforce that the sentiment expressed in the output agrees with the average of these. To realize this intuition, we can select as final output from timent score or significance finding). Operationally, this requires an external model to measure—or estimate—the aspect of interest as latent in a given candidate output. This is a limitation of the approach, but in many settings it may be feasible to identify or construct a model; we were able to do so for both tasks considered in this paper.\n\nCi the string that best aligns with this anticipated aggregate property (sen-\n\nThere is no guarantee that any member of In such cases, we have no means of yielding an output consistent with respect to synthesis, and it may be desirable to abstain from outputting anything at all in such cases; that is, to be a cautious summarizer (Ferri et al., 2004; Hechtlinger et al., 2018). We consider this strategy in the case of generating narrative synopses of evidence, as this constitutes a case in which (a) one would very\n\nCi will align well with the anticipated aggregated property.\n\n10In fixed effects meta-analysis the weights are inverse variances associated with study-level effect estimates. 11See Appendix Tables 11, 12 for an ablation over diversity vs. standard beam search outputs 12For a related generate-and-select approach Oved & Levy (2021) see Appendix B. 13This penalty is associated with a hyperparameter λ that encodes the relative importance of realizing diverse; we use λ=0.5 here and did not extensively tune this. Other hyperparameters include number of groups and total number of beams; we used 5 for both of these, retaining 5 beams as used for analysis above.\n\n7\n\nˆzi=G(xi)<latexit sha1_base64=\"byrpb1BJ7VZRlYzVu9NoLTseSUg=\">AAACAnicbVDLSgMxFM34rPU16krcBItQN2WmCroRii50WcE+oDMOmTRtQzPJkGTEOhQ3/oobF4q49Svc+Tem7Sy09cCFwzn3cu89Ycyo0o7zbc3NLywuLedW8qtr6xub9tZ2XYlEYlLDggnZDJEijHJS01Qz0owlQVHISCPsX4z8xh2Rigp+owcx8SPU5bRDMdJGCuxdr4d0+jAMKDyDl8X7W0/ELFEBPQzsglNyxoCzxM1IAWSoBvaX1xY4iQjXmCGlWq4Taz9FUlPMyDDvJYrECPdRl7QM5Sgiyk/HLwzhgVHasCOkKa7hWP09kaJIqUEUms4I6Z6a9kbif14r0Z1TP6U8TjTheLKokzCoBRzlAdtUEqzZwBCEJTW3QtxDEmFtUsubENzpl2dJvVxyj0rl6+NC5TyLIwf2wD4oAhecgAq4AlVQAxg8gmfwCt6sJ+vFerc+Jq1zVjazA/7A+vwBVfqWvg==</latexit>Despite some issues with plot, the film showcases smart direction.xi<latexit sha1_base64=\"B19YrBl8GaCN1o4Dqs/0q31r7wc=\">AAAB8XicbVBNTwIxEJ3FL8Qv1KOXRmLiieyiiR6JXjxi4gIRVtItXWjotpu2ayQb/oUXDxrj1X/jzX9jgT0o+JJJXt6bycy8MOFMG9f9dgorq2vrG8XN0tb2zu5eef+gqWWqCPWJ5FK1Q6wpZ4L6hhlO24miOA45bYWj66nfeqRKMynuzDihQYwHgkWMYGOl+6eHrkx4qnusV664VXcGtEy8nFQgR6NX/ur2JUljKgzhWOuO5yYmyLAyjHA6KXVTTRNMRnhAO5YKHFMdZLOLJ+jEKn0USWVLGDRTf09kONZ6HIe2M8ZmqBe9qfif10lNdBlkTCSpoYLMF0UpR0ai6fuozxQlho8twUQxeysiQ6wwMTakkg3BW3x5mTRrVe+sWrs9r9Sv8jiKcATHcAoeXEAdbqABPhAQ8Ayv8OZo58V5dz7mrQUnnzmEP3A+fwDlWpEP</latexit>The movie was excellent ... [SEP] Meandering but well directed ... [SEP]The film is over long ... [SEP]{A mess; difficult to watch and too long.A bloated movie lacking direction.Candidate outputs from Diverse Beam Search and inferred sentiments over theseCi<latexit sha1_base64=\"m9C/8pwDpDOSMXHG1iHjJE/oypE=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWaqoMtiNy4r2Ae0Q8mkmTY0kxmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck+OHwuujeN8o8LG5tb2TnG3tLd/cHhUPj5p6yhRlLVoJCLV9YlmgkvWMtwI1o0VI6EvWMefNDK/M2VK80g+mlnMvJCMJA84JcZKXj8kZkyJSBvzAR+UK07VWQCvEzcnFcjRHJS/+sOIJiGThgqidc91YuOlRBlOBZuX+olmMaETMmI9SyUJmfbSReg5vrDKEAeRsk8avFB/b6Qk1HoW+nYyC6lXvUz8z+slJrj1Ui7jxDBJl4eCRGAT4awBPOSKUSNmlhCquM2K6ZgoQo3tqWRLcFe/vE7atap7Va09XFfqd3kdRTiDc7gEF26gDvfQhBZQeIJneIU3NEUv6B19LEcLKN85hT9Anz/3CJI5</latexit>Linearized inputSummarization model--/+-Select candidate that most agrees with predicted aggregate sentiment-/+Predict expected aggregate sentiment (e.g., “mixed” or )argmaxˆy2Ci(ˆzi,ˆziˆy)<latexit sha1_base64=\"zoqfCsy+mFJ2QEHxBGFK5rwCZi4=\">AAACa3icbVFNb9NAEF2brxKgBHoAAYcVUaS0qiK7IMGxohw4Fom0leJgjTfjZNX1rrU7bhssX/iJ3PgHXPgPrFNLlJaRVnrz5o1m5m1WKukoin4G4a3bd+7e27jfe/Dw0ebj/pOnR85UVuBEGGXsSQYOldQ4IUkKT0qLUGQKj7PTg7Z+fIbWSaO/0KrEWQELLXMpgDyV9r8npkQLZKyGAndqsItkt4CLJq2TJVC9angiNU8KoKUAVR80qfTUR1QEo7Xim5fKZpf/TbrGZrs3vKLgicKcwFpzzovRxVc/WFUuldtpfxCNo3XwmyDuwIB1cZj2fyRzI6oCNQkFzk3jqKSZX52kUNj0ksphCeIUFjj1sD3Mzeq1Vw0fembOc2P908TX7NWOGgrnVkXmle3N7nqtJf9Xm1aUv5/VUpcVoRaXg/JKcTK8NZ7PpUVBauUBCCv9rlwswYIg/z09b0J8/eSb4GhvHL8Z731+O9j/0NmxwV6y12zEYvaO7bNP7JBNmGC/gs3gWfA8+B1uhS/CV5fSMOh6ttg/EQ7/ADCevWQ=</latexit>g(ˆyil)<latexit sha1_base64=\"pg5n4T/yK1+WyNRY7BieIEKdRfQ=\">AAAB+HicbVBNS8NAEN3Ur1o/GvXoZbEI9VKSKuix6MVjBVsLbQib7aZdutmE3YkQQ3+JFw+KePWnePPfuG1z0NYHA4/3ZpiZFySCa3Ccb6u0tr6xuVXeruzs7u1X7YPDro5TRVmHxiJWvYBoJrhkHeAgWC9RjESBYA/B5GbmPzwypXks7yFLmBeRkeQhpwSM5NvVUX0wJpBnUz/nYnrm2zWn4cyBV4lbkBoq0Pbtr8EwpmnEJFBBtO67TgJeThRwKti0Mkg1SwidkBHrGypJxLSXzw+f4lOjDHEYK1MS8Fz9PZGTSOssCkxnRGCsl72Z+J/XTyG88nIukxSYpItFYSowxHiWAh5yxSiIzBBCFTe3YjomilAwWVVMCO7yy6uk22y4543m3UWtdV3EUUbH6ATVkYsuUQvdojbqIIpS9Ixe0Zv1ZL1Y79bHorVkFTNH6A+szx/Uo5Mz</latexit>Under review as a conference paper at ICLR 2023\n\nFigure 6: Differences relative to human summaries under vanilla decoding and the proposed generate-diverse then select strategy on the Rotten Tomatoes dataset and task. We report Pearson’s r and R2, both measures of synthesis “calibration”. Vanilla decoding yields synthesis performance worse than humans, but explicitly considering synthesis at inference time as proposed results in performance comparable to and sometimes better than the human summaries (as best we can measure).\n\nFigure 7: Distributions of outputs for the candiate summaries. Movie reviews (left) show a histogram for the range of differences between lowest and highest output sentiments. Systematic reviews (right) show histograms of the fractions of outputs reporting significant results.\n\nmuch prefer not to produce a misleading summary of clinical evidence (Kell et al., 2021), and, (b) we observe many cases where the diverse decoding strategy yields an output that seems to communicate (at a granular level) the aggregate findings expected.\n\n(Maas et al., 2011)14 to predict the sentiment of each input xij, using the proportion of xij ∈\n\nMovie Reviews For movie reviews we use a BERT (Devlin et al., 2019) model trained on IMDB Xi with a positive score as an approximation for the target sentiment ziˆy. For each diverse prediction Ci, we predict a sentiment ̃ziˆy using our sentiment regression model (Section 3.1), and select the prediction cloest to the estimated target sentiment . We find this improves model performance to ziˆy| |\nhuman-like levels in terms of synthesis, as best we can measure (Table 4, Figure 6).\n\n ̃ziˆy −\n\nSystematic Reviews. In the case of systematic reviews, we can have only a binary measure of significant effect (or not). As a proxy for ziˆy, we again use RobotReviewer to extract an effect Xi indicate for each of the model inputs xij, using the majority vote (i.e., do the plurality of xij ∈ Ci again using RobotReviewer to that there was an effect). We classify each output candidate in estimate ̃ziˆy. We then select for output the highest probability candidate in Ci which agrees with the majority vote of the inputs, and abstain where there are no viable candidates. For the models we do choose a summary for, we find performance similar to our measure (Table 5). Movie reviews show a wide range of sentiments; systematic reviews show some improvement but are biased towards no effect (qualitatively observed in Appendix G).\n\n7 RELATED WORK\n\nAutomatic (multi-document) summarization (Nenkova & McKeown, 2011; Maybury, 1999) has been an active subfield within NLP for decades. We have focused our analysis on modern, neural abstractive models for conditional text generation (Bahdanau et al., 2015). In light of their empirical\n\n14https://huggingface.co/lvwerra/bert-imdb\n\n8\n\n0.10.00.1Pearson's rVanilla decodingLEDPRIMERAT5Pegasus0.10.00.1Pearson's rGenerate-diverse then select0.20.10.00.10.2R2 Vanilla decodingLEDPRIMERAT5Pegasus0.20.10.00.10.2R2 Generate-diverse then select0.00.10.20.3LEDPRIMERA0.00.20.40.60.81.00.00.10.20.3T50.00.20.40.60.81.0PegasusDensityDiverse Sampling Sentiment Ranges0.000.250.500.751.00LEDPegasus0.00.20.40.60.81.00.000.250.500.751.00PRIMERA0.00.20.40.60.81.0T5Average Predicted EffectDensityUnder review as a conference paper at ICLR 2023\n\n0.656 LED Pegasus 0.694 PRIMERA 0.749 0.721 T5 0.697 Reference\n\nR2 MSE 0.032 0.029 0.024 0.026 0.023\n\nPearson’s r 0.821 0.835 0.880 0.856 0.836\n\nR1 0.229 0.229 0.240 0.231\n\n0.763 LED Pegasus 0.799 PRIMERA 0.890 0.876 T5 0.697 Reference\n\nR2 MSE 0.022 0.019 0.011 0.012 0.023\n\nPearson’s r 0.878 0.894 0.948 0.938 0.836\n\nR1 0.227 0.232 0.240 0.230\n\nTable 4: Movie Reviews: Generate diverse movie meta-reviews and then choose among them using an approximate target sentiment (left) or the oracle sentiment (right). R1 is ROUGE1 score.\n\nF1 LED 0.557 PRIMERA 0.581 0.568 T5 0.588 Pegasus 0.577 Reference\n\n%Abstention ROUGE1 Abstention-Oracle ROUGE1-Oracle 0.386 0.336 0.350 0.383\n\n0.259 0.248 0.210 0.225\n\n0.233 0.213 0.228 0.242\n\n0.252 0.251 0.202 0.211\n\nTable 5: Systematic Review results with modified-then-selected predictions. F1 is a macro-averaged F1 on the set of returned results. We abstain when no output matches the expected synthesis result.\n\nsuccess, we have specifically evaluated a set of Transformer-based (Vaswani et al., 2017) models which have recently been used for multi-document summarization (Beltagy et al., 2020; Zhang et al., 2020; Xiao et al., 2021; Raffel et al., 2020). There has been some work on highlighting conflicting evidence in health literature specifically (Shah et al., 2021b;a), though this was focused primarily on highlighting conflicting evidence, and explicitly aggregating extracted content.\n\nSentence fusion One view on synthesis might be that is a particular kind of sentence fusion (Barzilay & McKeown, 2005). However, past work on “fusing” sentences has assumed that the aim is to generate an output that contains the information common to similar sentences (Thadani & McKeown, 2013). This is intuitive in the context of, e.g., summarizing multiple news articles covering the same event. But here we are interested in the more challenging setting in which the output should reflect an aggregate measure of potentially conflicting evidence or opinions.\n\nInterpretation and analysis of neural models for NLP This work is also related to the emerging body of work on analyzing neural NLP models, their behaviors, “knowledge”, and “abilities” in general e.g., Linzen et al. (2016); Tenney et al. (2019); Petroni et al. (2019); Niven & Kao (2019); Meng et al. (2022). There has been some work specifically on analyzing neural summarization models. Xu et al. (2020a) investigated when a model is likely to extract (copy) rather than abstract (generate). Xu & Durrett (2021) furthered this analysis by assessing when models were relying on the local input to produce particular output tokens, and when they instead rely on mostly on a background language distribution acquired in pre-training.\n\nFactuality of neural summarizers Neural conditional generation models have proven adept at producing fluent outputs, but in the context of summarization they are prone to hallucinating content unsupported by input documents (Maynez et al., 2020; Kryscinski et al., 2019). Automated metrics such as ROUGE do not reliably capture such phenomena (Falke et al., 2019; Maynez et al., 2020). This has motivated several efforts to design automated factuality metrics (e.g., Wang et al. (2020b); Xu et al. (2020b); see Pagnoni et al. (2021) for an overview).\n\n8 CONCLUSIONS\n\nWe have outlined and investigated the problem of synthesis as related to some summarization tasks. We showed that existing models are partially able to synthesize implicitly, but do so imperfectly: For instance, the aggregation they perform is sensitive to input ordering, and they are not as sensitive to perturbations in the composition of inputs as one would hope. We proposed and validated a straightforward inference time method to improve model synthesis capabilities by preferentially outputting summary candidates that align with a predicted aggregate measure, and demonstrated empirically that this offers gains in performance. Our hope is that this work encourages additional research into summarization models that explicitly optimize to accurately synthesize potentially conflicting evidence and information.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (ICLR), 2015.\n\nRegina Barzilay and Kathleen R McKeown. Sentence fusion for multidocument news summariza-\n\ntion. Computational Linguistics, 31(3):297–328, 2005.\n\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\n\nArXiv, abs/2004.05150, 2020.\n\nHoa Trang Dang. Overview of duc 2005. In Document Understand Conference, 2005.\n\nHoa Trang Dang. Overview of DUC. In In Proceedings of HLT-NAACL, 2006.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423.\n\nJay DeYoung, Eric Lehman, Benjamin Nye, Iain Marshall, and Byron C. Wallace. Evidence inference 2.0: More data, better models. In Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing, pp. 123–132, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.bionlp-1.13. URL https://aclanthology.org/2020. bionlp-1.13.\n\nJay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Lu Wang. MSˆ2: In Proceedings of the 2021 Conference Multi-document summarization of medical studies. on Empirical Methods in Natural Language Processing, pp. 7494–7513, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.emnlp-main.594.\n\nDavid K Evans, Judith L Klavans, and Kathleen McKeown. Columbia newsblaster: Multilingual news summarization on the web. In Demonstration Papers at HLT-NAACL 2004, pp. 1–4, 2004.\n\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1074–1084, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1102. URL https://aclanthology.org/P19-1102.\n\nTobias Falke, Leonardo FR Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pp. 2214–2220, 2019.\n\nC ́esar Ferri, Peter Flach, and Jos ́e Hern ́andez-Orallo. Delegating classifiers. In Proceedings of the\n\ntwenty-first international conference on Machine learning, pp. 37, 2004.\n\nDemian Gholipour Ghalandari, Chris Hokamp, Nghia The Pham, John Glover, and Georgiana Ifrim. A large-scale multi-document summarization dataset from the wikipedia current events portal. arXiv preprint arXiv:2005.10070, 2020.\n\nYotam Hechtlinger, Barnab ́as P ́oczos, and Larry Wasserman. Cautious deep learning. arXiv preprint\n\narXiv:1805.09460, 2018.\n\nGregory Kell, Iain Marshall, Byron Wallace, and Andre Jaun. What would it take to get biomedIn Proceedings of the 3rd Workshop on Machine Reading ical QA systems into practice? for Question Answering, pp. 28–41, Punta Cana, Dominican Republic, November 2021. Asdoi: 10.18653/v1/2021.mrqa-1.3. URL https: sociation for Computational Linguistics. //aclanthology.org/2021.mrqa-1.3.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWojciech Kryscinski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. Neural text summarization: A critical evaluation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 540–551, Hong Kong, China, 2019.\n\nWojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the factual\n\nconsistency of abstractive text summarization. ArXiv, abs/1910.12840, 2020.\n\nStefano\n\nLeone.\n\nRotten\n\ntomatoes movies\n\nand\n\ncritic\n\nreviews\n\ndataset.\n\nhttps://www.kaggle.com/datasets/stefanoleone992/ rotten-tomatoes-movies-and-critic-reviews-dataset, 2020.\n\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\n\nbranches out, pp. 74–81, 2004.\n\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of lstms to learn syntaxsensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521– 535, 2016.\n\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, ArXiv,\n\nand Noam M. Shazeer. Generating wikipedia by summarizing long sequences. abs/1801.10198, 2018.\n\nYao Lu, Yue Dong, and Laurent Charlin. Multi-xscience: A large-scale dataset for extreme multi-\n\ndocument summarization of scientific articles. arXiv preprint arXiv:2010.14235, 2020.\n\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http: //www.aclweb.org/anthology/P11-1015.\n\nIain J Marshall, Jo ̈el Kuiper, Edward Banner, and Byron C Wallace. Automating biomedical evidence synthesis: Robotreviewer. In Proceedings of the Association for Computational Linguistics, volume 2017, pp. 7, 2017.\n\nMani Maybury. Advances in automatic text summarization. MIT press, 1999.\n\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1906–1919, 2020.\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\n\nknowledge in gpt. arXiv preprint arXiv:2202.05262, 2022.\n\nDiego Moll ́a and Mar ́ıa Elena Santiago-Mart ́ınez. Creation of a corpus for evidence based medicine\n\nsummarisation. The Australasian medical journal, 5(9):503, 2012.\n\nFeng Nan, C ́ıcero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, and Bing Xiang. Improving factual consistency of abstractive summarization via question answering. ArXiv, abs/2105.04623, 2021a.\n\nFeng Nan, Ramesh Nallapati, Zhiguo Wang, C ́ıcero Nogueira dos Santos, Henghui Zhu, Dejiao Zhang, Kathleen McKeown, and Bing Xiang. Entity-level factual consistency of abstractive text summarization. ArXiv, abs/2102.09130, 2021b.\n\nAni Nenkova and Kathleen McKeown. Automatic summarization. Now Publishers Inc, 2011.\n\nTimothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language\n\narguments. arXiv preprint arXiv:1907.07355, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nNadav Oved and Ran Levy. PASS: Perturb-and-select summarizer for product reviews. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 351–365, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.acl-long.30. URL https://aclanthology.org/2021.acl-long.30.\n\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. Understanding factuality in abstracIn Proceedings of the tive summarization with FRANK: A benchmark for factuality metrics. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pp. 4812–4829, 2021.\n\nBo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorizaIn Proceedings of the 43rd Annual Meeting of the Association with respect to rating scales. tion for Computational Linguistics (ACL’05), pp. 115–124, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. doi: 10.3115/1219840.1219855. URL https: //aclanthology.org/P05-1015.\n\nFabio Petroni, Tim Rockt ̈aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.\n\nColin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683, 2020.\n\nThomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, and Alex Wang. Questeval: Summarization asks for fact-based evaluation. In EMNLP, 2021.\n\nDarsh Shah, Lili Yu, Tao Lei, and Regina Barzilay. Nutri-bullets hybrid: Consensual multidocument summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5213– 5222, Online, June 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.411. URL https://aclanthology.org/2021.naacl-main.411.\n\nDarsh J Shah, Lili Yu, Tao Lei, and Regina Barzilay. Nutri-bullets: Summarizing health studies by composing segments. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 13780–13788, 2021b.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.\n\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=SJzSgnRcKX.\n\nKapil Thadani and Kathleen McKeown. Supervised sentence fusion with single-stage inference. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pp. 1410–1418, 2013.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nAshwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424, 2016.\n\nByron C. Wallace, Sayantani Saha, Frank Soboczenski, and Iain James Marshall. Generating (factual?) narrative summaries of rcts: Experiments with neural multi-document summarization. ArXiv, abs/2008.11293, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAlex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the\n\nfactual consistency of summaries. In ACL, 2020a.\n\nAlex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pp. 5008–5020, 2020b.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ́emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6.\n\nRuben Wolhandler, Arie Cattan, Ori Ernst, and Ido Dagan. How ”multi” is multi-document summa-\n\nrization?, 2022. URL https://arxiv.org/abs/2210.12688.\n\nWen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman Cohan. Primer: Pyramid-based masked\n\nsentence pre-training for multi-document summarization. ArXiv, abs/2110.08499, 2021.\n\nJiacheng Xu and Greg Durrett. Dissecting generation modes for abstractive summarization models\n\nvia ablation and attribution. arXiv preprint arXiv:2106.01518, 2021.\n\nJiacheng Xu, Shrey Desai, and Greg Durrett. Understanding neural abstractive summarization mod-\n\nels via uncertainty. arXiv preprint arXiv:2010.07882, 2020a.\n\nXinnuo Xu, Ondˇrej Duˇsek, Jingyi Li, Verena Rieser, and Ioannis Konstas. Fact-based content In Proceedings of the Annual Meeting of\n\nweighting for evaluating abstractive summarisation. the Association for Computational Linguistics (ACL), pp. 5071–5081, 2020b.\n\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. Pegasus: Pre-training with extracted\n\ngap-sentences for abstractive summarization. ArXiv, abs/1912.08777, 2020.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA NOTATION\n\nVariable Xi yi yi,t ˆyi xij x⊕i θ\npθ pθ(yi,t| zij zi ziˆy G\ng αij\n\nyi,1..t\n\n1, x⊕i )\n\n−\n\nDefinition The ith set of input documents, corresponding to instance i The target summary y of the ith instance The tth token of target yi A generated summary of the ith instance The jth input document for instance i A particular linearization of the input documents Xi Model & parameters Probabily under parameters θ Standard auto-regressive prediction of the next token given an input and partial summary Latent property (sentiment, significance finding) of xij Aggregated latent property (sentiment, significance finding) of Xi Latent property measured over summary ˆy Aggregation function over latent properties zij, yields zi Auxillary function to measure latent property zij of xij, or ziˆy of ˆy A weight for zij\n\nTable 6: Notation.\n\nB ADDITIONAL RELATED WORK\n\nShah et al. (2021a) created a ”nutri-bullets” system for generating consensus-based summaries of health and nutrition related content. They assume a low-supervision setting in which one has a set of tuples extracted from texts with which to train a content extractor, and where one can design heuristic rule-based aggregation strategies on top of extracted tuples mapping onto discrete categories like “consensus”. By contrast, we assume a more typical supervised summarization setting and are interested in continuous aggregation of a latent attribute of interest, and we do not assume (or have access to) relational tuples over inputs. Indeed, recent work Wolhandler et al. (2022) has shown that systematic reviews are categorically different than news summarization, and that relational tuple extractors do not perform well in the medical domain.\n\nFirst, Shah et al. (2021a)’s focus primarily on settings in which training data is (severely) limited, and motivate their pipeline approach on the basis of this limited supervision assumption. For this reason they define separate modules: The first performs content selection (tuple extraction; this does require manual annotations of tuples on a subset of texts to train such an extractor); The second applies (manually composed) deterministic aggregation rules over these extracted tuples to combine them; a final module then generates a “surface realization” conditioned on the aggregated result.\n\nWe have investigated more typical supervised settings (with thousands of input and summary pairs), and we are training modern end-to-end transformer-based summarization models. We have empirically assessed the extent to which model outputs in this typical training regime are consistent with the continuous synthesis result anticipated. We do not have annotated tuples on our inputs (as would be required to use the Shah et al. (2021a) approach, as it assumes a trained content extractor module). And while applying discrete (manually composed) aggregation operators over inputs makes sense in some settings, we are explicitly interested in the ability of models to aggregate variables of interest continuously, for example producing “very positive” summaries when movie reviews are overwhelmingly positive, and merely “positive” summaries when they are only mostly positive.\n\nIn sum, the approach proposed by Shah et al. (2021a) is appropriate in, and designed for, lowsupervision settings (which we do not consider here) where there are natural “tuples” to be extracted from inputs and supervision for this sub-task (which we do not have) and where discrete aggregation categories of inputs is natural (whereas we are interested in continuous aggregation, e.g., mean sentiment).\n\nWolhandler et al. (2022) attempts to measure how challenging multi-document summarization is, as a function of the unique knowledge (represented as relational tuples) required to produce a summary.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nModel LED T5 PRIMERA Pegasus\n\nHuggingface Checkpoint allenai/led-base-16384 t5-base allenai/PRIMERA sshleifer/distill-pegasus-cnn-16-4\n\nOptimizer Adam Adam Adam Adafactor\n\nSchedule P.nomial/decay 0.01 Linear Linear Linear\n\nWarmup 50 steps 50 steps 50 steps 500 steps\n\nSmoothing 0.1 0\n0 0.1\n\nTable 7: Model hyperparameters. We used optimizers, schedulers, weight decay, and label smoothing as best according to examples from source implementations (where available). Optimizer warmup was arbitrarily chosen. Non-specified parameters were the Huggingface defaults.\n\nThis work measures how many new tuples each input document might add in contrast to subsets of other inputs. By greedily building subsets of inputs as a function of new information added, they find that standard multiple document summarization datasets merely need to select two to four documents from inputs of up to ten, whereas their approach breaks down in the case of systematic reviews. They find that due to both technical constraints for relation extraction, as well as the inability to model contradiction, relational extraction and aggregation methods are insufficient for producing evidence syntheses.\n\nOved & Levy (2021) introduce the Perturb and Selection Summarizer (PASS) system for summarizing Amazon product reviews. It works by perturbing model inputs (i.e. keep random subsets of the input), generating a summary for each perturbation (via standard beam search), and then selecting amongst outputs (via a ranker) to produce a coherent, self-consistent, and fluent summary.\n\nPASS is similar to our work in that it generates multiple outputs and selects amongst them. However it differs in several key respects. The key conceptual difference between PASS and our work is that PASS’s target is a summary’s self-consistency (a product review might contradict itself on some aspect, e.g. simultaneously discussing a product fitting well in addition to the product running a size small), whereas our target is a continuous fact derived from the inputs as a whole (e.g. aggregate sentiment or effect sizes). PASS is designed to produce summaries that are plausible, as opposed (and complementary) to summaries that reflect inherent contradiction in the input data. As PASS produces summaries from subsets of each instance’s input, it cannot perform an explicit synthesis on its own, as opposed to our work, wherein each summary was produced with access to the whole of each instance’s input.\n\nC MODELS\n\nWe train all models using a modified Huggingface Transformers libraryWolf et al. (2020). For the Pegasus model, we use a distilled version provided by Huggingface (Table 7). All models were trained using their default hyperparameters, except for batch size, optimization steps, learning rates, and any parameters specified in Table 7. We fix our batch size to 16, using gradient accumulation over single instances at a time, with floating point 16 (fp16) precision (due to data size), and perform an approximate (subject to resource constraints) grid search over learning rates and training steps (Tables 8, 9), keeping the model highlighted in bold. Earlier experimentation was performed ad-hoc with Longformer and T5 models only; we found that while lower numbers of steps could perform well, they had high variance and were more sensitive to hyperparameter changes than longer runs. All training was performed on 48G NVIDIA RTX8000 GPUs, most models are unable to fit single instance gradient information into fewer than 40G, even at reduced precision.\n\nD DETAILED RESULTS\n\nMeasure Validation As our results rely on using proxy metrics, we measure the quality of these proxies. See Figure 8 for movie meta-review sentiment correlation with human results, and Table 10 for how well the automatic significance measures correlate with the underlying truth.\n\nDiversity Sampling. We include detailed results for the importance of diversity sampling; the diversity sampling procedure produces better metrics in every dimension (Table 11 top left vs. bottom left.). In the systematic reviews, most metrics drop slightly and abstention increases substantially (Table 12).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nmodel led led led led led led led led led led led pegasus pegasus pegasus pegasus pegasus pegasus pegasus pegasus pegasus pegasus pegasus pegasus pegasus pegasus pegasus pegasus primera primera primera primera primera primera primera primera primera t5 t5 t5 t5 t5 t5 t5 t5 t5 t5 t5 t5 t5 t5 t5 t5 t5 t5 t5 t5\n\nsteps 1000 1000 1000 1000 5000 5000 5000 5000 10000 10000 10000 1000 1000 1000 1000 2500 2500 2500 2500 5000 5000 5000 5000 10000 10000 10000 10000 2500 2500 2500 5000 5000 5000 10000 10000 10000 1000 1000 1000 1000 2500 2500 2500 2500 5000 5000 5000 5000 7500 7500 7500 7500 10000 10000 10000 10000\n\nlr 1e-5 1e-6 3e-5 5e-5 1e-5 1e-6 3e-5 5e-5 1e-5 1e-6 3e-5 1e-3 1e-4 1e-5 1e-6 1e-3 1e-4 1e-5 1e-6 1e-3 1e-4 1e-5 1e-6 1e-3 1e-4 1e-5 1e-6 1e-4 1e-5 1e-6 1e-4 1e-5 1e-6 1e-4 1e-5 1e-6 1e-4 1e-5 1e-6 5e-5 1e-4 1e-5 1e-6 5e-5 1e-4 1e-5 1e-6 5e-5 1e-4 1e-5 1e-6 5e-5 1e-4 1e-5 1e-6 5e-5\n\nrouge1 25.09 25.25 25.33 25.14 25.46 25.31 24.50 23.99 24.28 25.58 25.60 23.49 22.25 18.95 18.28 26.44 26.50 24.98 23.02 24.05 27.41 25.67 23.57 23.18 27.42 25.85 24.41 23.32 25.12 24.92 24.35 25.42 25.32 23.57 24.27 25.39 25.24 24.31 22.39 25.06 25.82 24.94 23.82 25.57 25.11 25.07 23.87 24.47 24.33 25.67 24.17 25.66 24.41 25.73 24.29 25.09\n\nrouge2 8.95 8.33 8.65 8.35 8.63 8.76 7.49 7.07 7.60 8.64 7.97 7.25 7.23 4.23 3.10 9.30 10.81 10.17 7.92 7.75 10.26 9.86 8.74 7.17 9.53 10.25 9.88 7.02 8.39 8.48 7.40 8.44 8.75 7.24 7.59 8.66 9.13 7.87 6.62 8.65 8.46 8.36 7.59 8.47 8.17 8.57 7.99 8.40 7.58 8.75 7.73 8.64 7.78 8.91 7.98 7.89\n\nrougeL 20.14 19.48 19.87 19.89 19.76 20.12 19.02 17.61 19.25 20.32 19.59 17.93 17.67 14.34 13.08 20.45 20.91 19.75 18.23 18.98 21.72 20.28 18.77 17.19 21.05 20.41 19.72 18.10 19.52 19.93 18.49 19.81 20.06 17.89 18.55 20.12 19.67 19.30 17.90 19.96 19.59 19.61 19.09 19.71 19.84 19.55 19.36 19.53 18.93 19.92 19.43 19.59 18.89 20.10 19.31 19.19\n\nTable 8: Movie Reviews dev training results, best models bolded.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nModel led led led led led pegasus pegasus pegasus pegasus pegasus primera primera primera primera primera primera primera primera primera primera t5 t5 t5 t5\n\nsteps 250 500 1000 2500 5000 250 500 1000 2500 5000 250 500 1000 2500 5000 250 500 1000 2500 5000 250 500 1000 2500\n\nlr 5e-5 5e-5 5e-5 5e-5 5e-5 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 5e-5 5e-5 5e-5 5e-5 5e-5 5e-5 5e-5 5e-5 5e-5\n\nROUGE1 ROUGE2 ROUGEL 6.70 6.91 7.31 6.52 8.65 5.64 6.71 6.67 6.71 5.69 7.68 5.72 6.70 6.99 6.96 6.90 6.11 6.71 6.89 6.59 7.09 6.99 0.00 0.00\n\n24.30 23.99 25.21 26.05 30.96 20.22 21.66 21.87 22.44 22.66 23.21 22.80 26.08 27.60 27.70 22.52 24.22 24.53 27.64 28.46 23.80 22.77 0.00 0.00\n\n18.68 17.09 18.54 17.65 20.33 15.86 16.92 16.79 17.24 16.87 17.96 16.39 16.77 18.16 18.02 17.98 17.53 17.01 18.79 18.08 18.58 18.30 0.00 0.00\n\nTable 9: Systematic Reviews dev training results, best models bolded. We experimented with other parameters (in particular learning rates), and found that total number of steps was more important.\n\nFigure 8: Actual sentiment vs. predicted sentiments on model outputs.\n\nE ROUGE RESULTS\n\nWe report mean differences in ROUGE outputs for both datasets in Figure 10. Ideally, these would have all mass at zero.\n\n17\n\n0.00.20.40.60.81.0LEDPRIMERA0.00.10.20.30.40.50.60.70.80.91.00.00.20.40.60.81.0T50.00.10.20.30.40.50.60.70.80.91.0PegasusPredicted Metareview SentimentActual SentimentUnder review as a conference paper at ICLR 2023\n\nNo significant difference Significant difference Accuracy Macro avg\n\nPrecision Recall 0.870 0.283\n\n0.726 0.500\n\n0.613\n\n0.577\n\nF1-score 0.792 0.362 0.686 0.577\n\nSupport 247 113 360 360\n\nTable 10: Systematic review significance validation results.\n\n0.656 LED Pegasus 0.694 PRIMERA 0.749 0.721 T5 0.697 Reference\n\n0.653 LED PEGASUS 0.649 PRIMERA 0.685 0.615 T5 0.697 Reference\n\nR2 MSE 0.032 0.029 0.024 0.026 0.023 R2 MSE 0.033 0.033 0.029 0.036 0.023\n\nPearson’s r 0.821 0.835 0.880 0.856 0.836 Pearson’s r 0.815 0.809 0.833 0.786 0.836\n\nR1 0.229 0.229 0.240 0.231\n\nR1 0.241 0.248 0.254 0.252\n\n0.711 LED Pegasus 0.705 PRIMERA 0.731 0.669 T5 0.697 Reference\n\n0.763 LED Pegasus 0.799 PRIMERA 0.890 0.876 T5 0.697 Reference\n\nR2 MSE 0.027 0.028 0.025 0.031 0.023 R2 MSE 0.022 0.019 0.011 0.012 0.023\n\nPearson’s r 0.847 0.840 0.857 0.819 0.836 Pearson’s r 0.878 0.894 0.948 0.938 0.836\n\nR1 0.240 0.247 0.255 0.253\n\nR1 0.227 0.232 0.240 0.230\n\nTable 11: Movie Reviews. Top left: Generate 5 diverse movie meta-reviews and then choose among them using an approximate target sentiment. Top right: Generate 25 diverse movie meta-reviews and then choose among them using an approximate target sentiment; this was accidentally referenced in an earlier version of this work. Bottom left: Generate 5 movie meta-reviews using standard beam search and choose among them using an approximate target sentiment. Bottom right: Generate 5 diverse movie meta-reviews and select amongst them using the oracle sentiment. In all cases R1 refers to ROUGE1.\n\nF1 LED 0.521 PRIMERA 0.551 0.546 T5 0.589 Pegasus 0.577 Reference\n\nAbstention ROUGE1 Abstention-Oracle ROUGE1-Oracle 0.503 0.464 0.422 0.469\n\n0.258 0.256 0.204 0.211\n\n0.358 0.342 0.328 0.281\n\n0.263 0.248 0.211 0.222\n\nTable 12: Systematic reviews results with multiple generate-then-select predictions, this time using the top-5 results from standard beam-search. F1 is a macro-averaged F1 on the set of returned results. We abstain when no output matches the expected synthesis result.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: Entropy of instance predictions. Broken out by whether or not the underlying truth is not significant (left); or the whole dataset (right)\n\nFigure 10: ROUGE1 deviations from instance means for movie reviews (left) and systematic reviews (right).\n\nF EXAMPLES OF DIVERSE MOVIE SUMMARIES\n\nSummary The Private Lives of Pippa Lee relies on a strong ensemble cast to deliver witty and poignant observations about life and relationships. The Private Lives of Pippa Lee relies on a strong ensemble cast to deliver witty and poignant observations about life and relationships. With a strong cast and Robin Wright Penn’s sharp performance, The Private Lives of Pippa Lee succeeds as both a witty tribute to lost characters and a showcase for Robin Wright Penn. With a strong cast and Robin Wright Penn’s empathetic direction, The Private Lives of Pippa Lee succeeds as both a humorous look at domestic issues and a poignant look at relationships. The Private Lives of Pippa Lee benefits from Robin Wright Penn’s superb performance, as well as a strong ensemble cast that includes Keanu Reeves, and Faye Dunaway. The Private Lives of Pippa Lee has an affecting ensemble cast and Robin Wright Penn delivers a noteworthy performance, although the film is a bit too episodic.\n\nSentiment 0.800731\n\n0.800731\n\n0.809596\n\n0.809081\n\n0.845693\n\n0.654905\n\nTable 13: Different meta-reviews of ”The Private Lives of Pippa Lee” and corresponding sentiments. The target sentiment for this meta-review is 70%, generating diverse candidates helps find a metareview closer to the target.\n\n19\n\n0.000.250.500.751.00LEDPegasus0.00.20.40.60.81.00.000.250.500.751.00PRIMERA0.00.20.40.60.81.0T5Average Predicted Effect Entropy / No Significant EffectsDensity0.000.250.500.751.00LEDPegasus0.00.20.40.60.81.00.000.250.500.751.00PRIMERA0.00.20.40.60.81.0T5Entropy of Average Predicted EffectDensity0.00.10.20.30.4LEDPRIMERA0.40.20.00.20.40.00.10.20.30.4T50.40.20.00.20.4PegasusDensityMean ROUGE1 Difference0.00.10.20.30.4LEDPegasus0.20.10.00.10.20.00.10.20.30.4PRIMERA0.20.10.00.10.2T5ROUGE1 Difference from MeanDensityUnder review as a conference paper at ICLR 2023\n\nSummary You Don’t Mess With the Zohan’s handful of laughs are almost enough to compensate for its inconsistent tone and stale, obvious jokes. You Don’t Mess with the Zohan has a handful of crotch thrusts, but not enough of them land. You Don’t Mess With the Zohan’s handful of laughs are almost enough to compensate for its aimless, crass script. You Don’t Mess with the Zohan has its moments, but not all of them – and the jokes are embarrassingly crass and often crude. You Don’t Mess with the Zohan has its moments, but not all of them – and the jokes are embarrassingly crass and often crude. The script\n\nSentiment 0.242698\n\n0.429654\n\n0.287896\n\n0.434442\n\n0.406172\n\nTable 14: Different meta-reviews for ”You Don’t Mess With The Zohan”; a relatively panned movie with a target meta-review sentiment of 37%.\n\nG EXAMPLES OF DIVERSE SYSTEMATIC REVIEWS\n\nGenerated Ketanserin versus placebo in the Raynaud’s phenomenon is neither effective nor safe. The Raynaud’s phenomenon is associated with significant adverse effects including dizziness and pain. The effectiveness of ketanserin for the Raynaud’s phenomenon is unknown. Ketanserin versus placebo in the Raynaud’s phenomenon is neither effective nor safe. The Raynaud’s phenomenon is associated with significant adverse effects including dizziness and pain. Ketanserin and serotonin receptor antagonists in the Raynaud’s phenomenon treatment of systemic scleroderma reduce the incidence of ischaemic ulcers and may reduce the frequency of adverse events. The Raynaud’s phenomenon is associated with a small number of adverse effects when administered orally to patients with Raynaud’s phenomenon. The frequency of Raynaud’s phenomenon is similar to that of other drugs. However, there is little evidence to aid the treatment of Raynaud’s phenomenon. The Raynaud’s phenomenon is associated with a small number of adverse effects when administered orally to patients with Raynaud’s phenomenon. The frequency of Raynaud’s phenomenon is similar to that of other drugs.\n\nEffect no significant difference\n\nno significant difference\n\nsignificant difference\n\nno significant difference\n\nno significant difference\n\nTable 15: An instance where generating multiple reviews allows our models to find a candidate summary reporting a significant difference (the target).\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nGenerated The overall evidence supports the use of topical antibiotics in surgical patients who have undergone minor surgery, compared to no treatment. The effect on other outcomes, other than infection rate, is consistent. The safety profile of topical antibiotics is also of concern. Further well-designed RCTs are needed to assess effectiveness of topical antibiotics in surgical patients. A single application of topical antibiotics in surgical site wounds reduces the risk of infection, and the risk of other complications, including wound dehiscence. The risk of infection recurrence is low. The use of topical antibiotics outside of surgery should be restricted to surgical site wounds. A single application of topical antibiotics in surgical site wounds reduces the risk of infection, and the risk of other complications, including wound dehiscence. The risk of infection recurrence is low. The overall evidence supports the use of topical antibiotics in surgical patients to reduce the risk of infection, and the risk of other complications, especially in high-risk patients. There is a lack of evidence in low-risk patients to support the use of topical antibiotics in this setting. A single application of topical antibiotics in surgical site infection prevention has been demonstrated to reduce the risk of infection in patients who have undergone surgery. The number of patients who have been treated with topical antibiotics has been small but this is due to risk of bias in the trials. Ointment use should be limited to patients whose primary wound is irradiated.\n\nEffect no significant difference\n\nno significant difference\n\nno significant difference\n\nsignificant difference\n\nsignificantly difference\n\nTable 16: An instance where generating multiple reviews allows our models to find a candidate summary reporting a significant difference (the target).\n\n21",
    "reference": "# Summary Of The Paper\n\nThis paper investigates the capacity of summarization models to synthesize (potentially conflicting) information from multiple documents. It defines a model-based metric for the aggregate latent aspect of interest, in this case, the sentiment of movie reviews (Rotten Tomatoes dataset) and the treatment efficacy of medical trials. This metric is applied to reference summaries as well as summaries generated by Longformer, PEGASUS, PRIMERA, and T5. Then, the results are compared to gold labels from the datasets so that a higher correlation would be an indicator for better synthesis capacity. The experimental results indicate inconsistent synthesis for off-the-shelf summarization models. The authors then propose a technique for improving model synthesis by ranking candidate summaries according to the expected aggregated aspect.\n\n# Strength And Weaknesses\n\nStrengths:\n1. The paper presents an interesting way to evaluate summarization that complements traditional ROUGE scores.\n2. The experiments are conducted in two quite different domains.\n\nWeaknesses:\n1. It is not clear why the authors decide not to fine-tune the measurement model on the datasets, especially for Rotten Tomatoes. The BERT and RobotViewer models were trained on human-written inputs and are now used on machine generated summaries, which might detrimental for its performance (and correlations to gold sentiments).\n2. The summarizers are the \"base\" models and the gap in quality with respect to the large versions could change some of the conclusions. At least one experiment with a large version (PEGASUS, for instance) could make the empirical evidence much stronger.\n3. Citation missing: Oved and Levy (2021) also used a strategy to generate several summaries and rank them according to a desired criteria. https://aclanthology.org/2021.acl-long.30/\n\nMinor comments:\n1. Figure 10 in the Appendix is cut.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is very clear and easy to follow. Experimental details and hyperparameters are well-defined for reproducibility. The proposed evaluation approach presents novelty but the idea of generating several candidate summaries is already explored in previous research.\n\n# Summary Of The Review\n\nThis work presents a novel way to evaluate multi-document summarization and its experimental methods are sound. I would recommend the authors to address the experimental improvements listed above to make the claims stronger.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nCHARACTERIZING THE INFLUENCE OF GRAPH ELEMENTS\n\nZizhang Chen, Peizhao Li, Hongfu Liu, Pengyu Hong Brandeis University {zizhang2,peizhaoli,hongfuliu,hongpeng}@brandeis.edu\n\nABSTRACT\n\nInfluence function, a method from robust statistics, measures the changes of model parameters or some functions about model parameters concerning the removal or modification of training instances. It is an efficient and useful post-hoc method for studying the interpretability of machine learning models without the need for expensive model re-training. Recently, graph convolution networks (GCNs), which operate on graph data, have attracted a great deal of attention. However, there is no preceding research on the influence functions of GCNs to shed light on the effects of removing training nodes/edges from an input graph. Since the nodes/edges in a graph are interdependent in GCNs, it is challenging to derive influence functions for GCNs. To fill this gap, we started with the simple graph convolution (SGC) model that operates on an attributed graph and formulated an influence function to approximate the changes of model parameters when a node or an edge is removed from an attributed graph. Moreover, we theoretically analyzed the error bound of the estimated influence of removing an edge. We experimentally validated the accuracy and effectiveness of our influence estimation function. In addition, we showed that the influence function of a SGC model could be used to estimate the impact of removing training nodes/edges on the test performance of the SGC without re-training the model. Finally, we demonstrated how to use influence functions to guide the adversarial attacks on GCNs effectively.\n\n1\n\nINTRODUCTION\n\nGraph data is pervasive in real-world applications, such as, online recommendations (Shalaby et al., 2017; Huang et al., 2021; Li et al., 2021), drug discovery (Takigawa & Mamitsuka, 2013; Li et al., 2017), and knowledge management (Rizun, 2019; Wang et al., 2018), to name a few. The growing need to analyze huge amounts of graph data has inspired work that combines Graph Neural Networks with deep learning (Gori et al., 2005; Scarselli et al., 2005; Li et al., 2016; Hamilton et al., 2017; Xu et al., 2019b; Jiang et al., 2019). Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017; Zhang & Chen, 2018; Fan et al., 2019), the most cited GNN architecture, adopts convolution and message-passing mechanisms.\n\nTo better understand GCNs from a data-centric perspective, we consider the following question:\n\nWithout model retraining, how can we estimate the changes of parameters in GCNs when the graph used for learning is perturbed by edge- or node-removals?\n\nThis question proposes to estimate counterfactual effects on the parameters of a well-trained model when there is a manipulation in the basic elements in a graph, where the ground truth of such an effect should be obtained from model retraining. With a computational tool as the answer, we can efficiently manipulate edges or nodes in a graph to control the change of model parameters of trained GCNs. The solution would provide further extensions like graph data rectification, improving model generalization, and graph data poison attacks through a pure data modeling way. Yet, current methods for training GCNs offer limited interpretability of the interactions between the training graph and the GCN model. More specifically, we fall short of understanding the influence of the input graph elements on both the changes in model parameters and the generalizability of a trained model (Ying et al., 2019; Huang et al., 2022; Yuan et al., 2021; Xu et al., 2019a; Zheng et al., 2021).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nIn the regime of robust statistics, an analyzing tool called influence functions (Hampel, 1974; Koh & Liang, 2017) is proposed to study the counterfactual effect between training data and model performance. For independent and identically distributed (i.i.d.) data, influence functions offer an approximate estimation of the model’s change when there is an infinitesimal perturbation added to the training distribution, e.g., a reweighing on some training instances. However, unlike i.i.d. data, manipulation on a graph would incur a knock-on effect through GCNs. For example, an edge removal will break down all message passing that is supposed to pass through this edge and consequentially change node representations and affect the final model optimization. Therefore, introducing influence functions to graph data and GCNs is non-trivial work and requires extra considerations.\n\nIn this work, we aim to derive influence functions for GCNs. As the first attempt in this direction, we focused on Simple Graph Convolution (Wu et al., 2019). Our contributions are three-fold:\n\n• We derived influence functions for Simple Graph Convolution. Based on influence functions, we developed computational approaches to estimate the changes in model parameters caused by two basic perturbations: edge removal and node removal.\n\n• We derived the theoretical error bounds to characterize the gap between the estimated changes\n\nand the actual changes in model parameters in terms of both edge and node removal.\n\n• We show that our influence analysis on the graph can be utilized to (1) rectify the training graph to improve model testing performance, and (2) guide adversarial attacks to SGC or conduct grey-box attacks on GCNs via a surrogate SGC.\n\nCode is publicly available at https://github.com/Cyrus9721/Characterizing_ graph_influence.\n\n2 PRELIMINARIES\n\nIn the following sections, we use a lowercase x for a scalar or an entity, an uppercase X for a constant or a set, a bolder lowercase x for a vector, and a bolder uppercase X for a matrix.\n\nInfluence Functions Influence functions (Hampel, 1974) estimate the change in model paramtraining samples is perturbed infinitesieters when the empirical weight distribution of i.i.d. mally. Such estimations are computationally efficient compared to learn-one-out retraining iterating every training sample. For N training instances x and label y, consider empirical risk minimization (ERM) ˆθ = arg minθ∈Θ 2 for some loss function l(·, ·) through a parameterized model θ and with a regularization term. When down weighing a training sample (xi, yi) by an infinitely small fraction ε, the substitutional ERM can be expressed as ˆθ(xi; −ε) = arg minθ∈Θ 2. Influence functions estimate the actual change I ∗(xi; −ε) = ˆθ(xi; −ε) − ˆθ for a strictly convex and twice differentiable l(·, ·):\n\nx,y l(x, y) − εl(xi, yi) + λ\n\nx,y l(x, y) + λ\n\n2 ∥θ∥2\n\n2 ∥θ∥2\n\n(cid:80)\n\n(cid:80)\n\n1 N\n\n1 N\n\nI(xi; −ε) = lim ε→0\n\nˆθ(xi; −ε) − ˆθ = −H−1\n\nˆθ\n\n∇ˆθl(xi, yi),\n\n(1)\n\n(cid:80)N\n\nN\n\n∇ˆθl(xi, yi). When N the size of the training data is large, by setting ε = 1\n\nl(xi, yi) + λI is the Hessian matrix with regularization at parameter ˆθ. where Hˆθ := 1 i=1 ∇2 ˆθ For some differentiable model evaluation function f : Θ → R like calculating total model loss over a test set, the change from down weighing ε → (xi, yi) to the evaluative results can be approximated by ∇ˆθf (ˆθ)H−1 N , we can approximate the change of ˆθ incurred by removing an entire training sample I(xi; − 1 N ) = I(−xi) via linear extrapolations 1 N → 0. Obviously, in terms of the estimated influence I, removing a training sample has the opposite value of adding the same training sample I(−xi) = −I(+xi). In our work, we shall assume an additivity of influence functions in computations when several samples are removed, e.g., when removing two samples: I(−xi, −xj) = I(−xi) + I(−xj).\n\nˆθ\n\nThough efficient, as a drawback, influence functions on non-convex models suffer from estimation errors due to the variant local minima and usually a computational approximation to H−1 for a noninvertible Hessian matrix. To introduce influence functions from i.i.d. data to graphs and precisely characterize the influence of graph elements to model parameters’ changes, we consider a convex model called Simple Graph Convolution from the GCNs family.\n\nˆθ\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nSimple Graph Convolution By removing non-linear activations between layers from typical Graph Convolutional Networks, Simple Graph Convolution (SGC) (Wu et al., 2019) formulates a linear simplification of GCNs with competitive performance on various tasks (He et al., 2020; Rakhimberdina & Murata, 2019). Let G = (V, E) denote an undirected attributed graph, where V = {v} contains vertices with corresponding feature X ∈ R|V |×D with D the feature dimension, and E = {eij}1≤i<j≤|V | is the set of edges. Let Γv denote the set of neighborhood nodes around v, and dv the node degrees of v. We use A denote the adjacency matrix where Aij = Aji = 1 if eij ∈ E, and 0 elsewhere. D = diag(dv) denotes the degree matrix. When the context is clear, we simplify the notation Γvi → Γi, and the same manner for other symbols. For multi-layer GNNs, let z(k) v = xv the initial node features. Simple Graph Convolution processes node representations as: z(k) v = W(k) (cid:16)(cid:80) + b(k), where W(k) and b(k) are trainable parameters in k-th layer. In transductive node classification, let Vtrain ⊂ V denote the set of N training nodes associated with labels y. ERM of SGC in this task is ˆθ = arg minθ∈Θ 2 ∥θ∥2 2. Due to the linearity of SGC, parameters W(k) and b(k) in each layer can be unified, and predictions after k layers can be simplified as y = arg max( ̃D− 1 2 )kXW + b with ̃A = A + I and ̃D the degree matrix of ̃A. Therefore, for node representations Z(k) = ( ̃D− 1 2 )kX with y and cross-entropy loss, l(·, ·) is convex. The parameters θ in l consist of matrix W ∈ RD×|Class| and vector b ∈ R|Class| with |Class| the number of class, and can be solved via logistic regression.\n\ndenote the hidden representation of node v in the k-th layer, and with z(0)\n\nu∈Γv∪{v} d−1/2\n\nv , yv) + λ\n\n2 ̃A ̃D− 1\n\n2 ̃A ̃D− 1\n\nz(k−1)\n\nl(z(k)\n\nd−1/2\n\nv∈Vtrain\n\n(cid:80)\n\n1 N\n\n(cid:17)\n\nu\n\nu\n\nv\n\nv\n\nAdditional Notations In what follows, we shall build our influence analysis upon SGC. For notational simplification, we omit (k) in Z(k) and use Z to denote the last-layer node representations from SGC. We use I ∗(−eij) = ˆθ(−eij) − ˆθ to denote the actual model parameters’ change where ˆθ(−eij) is obtained through ERM when eij is removed from E. Likewise, I ∗(−vi) denotes the change from vi’s removal from graph G. I(−eij) and I(−vi) are the corresponding estimated influence for I ∗(−eij) and I ∗(−vi) based on influence functions, respectively.\n\n3 MODELING THE INFLUENCE OF ELEMENTS IN GRAPHS\n\nWe mainly consider the use of influence functions of two fundamental operations over an attributed graph: removing an edge (in Section 3.1) and removing a complete node (in Section 3.2).\n\n3.1\n\nINFLUENCE OF EDGE REMOVAL\n\nWith message passing through edges in graph convolution, removing an edge will incur representational changes in Z. When eij is removed, the changes come from two aspects: (1) The message passing for node features via the removed edge will be blocked, and all the representations of k-hop neighboring nodes of the removed edge will be affected. (2) Due to the normalization operation over A, the degree of all adjacent edges ejk, ∀k ∈ Γi and eik, ∀k ∈ Γj will be changed, and these edges will have a larger value in ̃D− 1 2 . We have the following expression to describe the representational changes ∆(−eij) of node representations Z in SGC incurred by removing eij.\n\n2 ̃A ̃D− 1\n\n∆(−eij) = [( ̃D(−eij)− 1\n\n2 ̃A(−eij) ̃D(−eij)− 1 (2) A(−eij) is the modified adjacency matrix with A(−eij)ij/ji = 0 and A(−eij) = A elsewhere. ̃A(−eij) = A(−eij) + I and ̃D(−eij) the degree matrix of ̃A(−eij). By having ∆(−eij), we can access the change in every node. Let δk(−eij) denotes the k-th row in ∆(−eij). δk = 0 implies no change in k-th node from removing eij, and δk ̸= 0 indicates a change in zk.\n\n2 )k − ( ̃D− 1\n\n2 ̃A ̃D− 1\n\n2 )k]X.\n\nWe proceed to use influence functions to characterize the counterfactual effect of removing eij. Our high-level idea is, from an influence functions perspective, representational changes in nodes z → z + δ is equivalent to removing training instances with feature z, and adding new training instances with feature z + δ and with the same labels. The problem thus turns back to an instance reweighing problem developed by influence functions. In this case, we have the lemma below to prove the influence functions’ linearity. Lemma 3.1. Consider empirical risk minimization ˆθ = arg minθ∈Θ δ) = arg minθ∈Θ\n\ni l(xi, yi) and ˆθ(xj → xj + i̸=j l(xi, yi) + l(xj + δ, yj) with some twice-differentiable and strictly convex\n\n(cid:80)\n\n(cid:80)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nl, let I ∗(xj → xj + δ) = ˆθ(xj → xj + δ) − ˆθ, the estimated influence satisfies linearity:\n\nI(xj → xj + δ) = I(−xj) + I(+(xj + δ)).\n\n(3)\n\nBy having Lemma 3.1, we are ready to derive a proposition from characterizing edge removal.\n\nProposition 3.2. Let δk(−eij) denote the k-th row of ∆(−eij). The influence of removing an edge eij ∈ E from graph G can be estimated by:\n\nI(−eij) = I(z → z + δ(−eij)) =\n\n(cid:88)\n\nk\n\nI(+(zk + δk(−eij))) + I(−zk)\n\n= −H−1\n\nˆθ\n\n(cid:88)\n\nvk∈Vtrain\n\n(∇ˆθl(zk + δk(−eij), yk) − ∇ˆθl(zk, yk)).\n\n(4)\n\nProof. The second equality comes from Lemma 3.1, and the third equality comes from Equation (1). Realize that removing two representations I(−zi, −zj) = I(−zi) + I(−zj) completing the proof.\n\nProposition 3.2 offers an approach to calculate the estimated influence of removing eij. In practice, having the inverse hessian matrix, a removal only requires users to compute the updated gradients ∇ˆθl(zk + δk(−eij), yk) and its original gradients for all affected nodes in (k+1)-hop neighbors.\n\n3.2\n\nINFLUENCE OF NODE REMOVAL\n\nWe address the case of node removal. The impact from removing a node vi from graph G to parameters’ change are two-folds: (1) The loss term l(xi, yi) will no longer involved in ERM if vi ∈ Vtrain. (2) All edges link to this node {eij}, ∀j ∈ Γi will be removed either. The first aspect can be deemed as a regular training instance removal similar to an i.i.d. case, and the second aspect be can an incremental extension from edge removal in Proposition 3.2.\n\nThe representational changes from removing node vi can be expressed as:\n\n∆(−vi) = [( ̃D(−vi)− 1\n\n2 ̃A(−vi) ̃D(−vi)− 1\n\n2 )k − ( ̃D− 1\n\n2 ̃A ̃D− 1\n\n2 )k]X,\n\n(5)\n\nwith A(−vi)jk/kj = Ajk/kj, ∀j, k : j ̸= i ∧ k /∈ Γi, and A(−vi) = 0 elsewhere. Similarly, ̃A(−vi) = A(−vi) + I and ̃D(−vi) is the corresponding degree matrix of ̃A(−vi). Having ∆(−vi), Lemma 3.1 and Proposition 3.2, we state the estimated influence of removing vi. Proposition 3.3. Let δj(−vi) denote the j-th row of ∆(−vi). The influence of removing node vi from graph G can be estimated by:\n\nI(−vi) = I(−zi) + I(z → z + δ(−vi)) = I(−zi) +\n\n= −1vi∈Vtrain · H−1\n\nˆθ\n\n∇ˆθl (zi, yi) − H−1 ˆθ\n\n(cid:88)\n\nvj ∈Vtrain\n\n(cid:88)\n\nj\n\nI(+(zj + δj(−vi))) + I(−zj)\n\n(∇ˆθl(zj + δj(−vi), yj) − ∇ˆθl(zj, yj)),\n\nwhere 1 is an indicator function. Proof. Combining Lemma 3.1 and Equation (1) completes the proof.\n\n4 THEORETICAL ERROR BOUNDS\n\n(6)\n\nIn the above section, we show how to estimate the changes of model parameters due to edge removal: ˆθ → ˆθ(−eij) and node removals: ˆθ → ˆθ(−vi). In this section, we study the error between the estimated influence given by influence functions I and the actual influence I ∗ obtained by model retraining. We give upper error bounds on edge removal ∥I ∗(−eij) − I(−eij)∥2 (see Theorem 4.1) and node removal ∥I ∗(−vi) − I(−vi)∥2 (see Corollary A.1).\n\nIn what follows, we shall assume the second derivative of l(·, ·) is Lipschitz continuous at θ with constant C based on the convergence theory of Newton’s method. To simplify the notations, we use z′ i = zi + δi to denote the new representation of vi obtained after removing an edge or a node, where δi is the row vector of ∆(−eij) or ∆(−vi) depending on the context.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 4.1. Let σmin ≥ 0 denote the smallest eigenvalue of all eigenvalues of Hessian matrices l(zi, yi), ∀vi ∈ Vtrain of the original model ˆθ. Let σ′ ∇2 min ≥ 0 denote the smallest eigenvalue of all ˆθ l(zi, yi), ∀vi ∈ Vtrain of the retrained model ˆθ(−eij) with eigenvalues of Hessian matrices ∇2 eij removed from graph G. Use L denote the set {v : z′ ̸= z} containing affected nodes from the edge removal, and Err(−eij) = ∥I ∗(−eij) − I(−eij)∥2. Recall λ is the l2 regularization strength, we have an upper bound on the estimated error of model parameters’ change:\n\nˆθ(−eij )\n\nErr(−eij) ≤\n\n(N λ + (N − |L|)σmin + σ′\n\nN 3C\n\nmin|L|)3 · ∥\n\n(cid:88)\n\nvl∈L\n\n(∇ˆθl(z′\n\nl, yl) − ∇ˆθl(zl, yl))∥2\n\n2\n\n+\n\nN\n\nN λ + (N − |L|)σmin + min(σmin, σ′\n\nmin)|L|\n\n(cid:88)\n\n· ∥\n\n(∇ˆθl(z′\n\nl, yl) − ∇ˆθl(zl, yl))∥2.\n\nvl∈L\n\n(7)\n\nProof sketch. We use the one-step Newton approximation (Pregibon, 1981) as an intermediate step to derive the bound. The first term is the difference between the actual change I ∗(−eij) and its Newton approximation, and the second term is the difference between the Newton approximation and the estimated influence I(−eij). Combining these two parts result the bound.\n\nRemark 4.2. We have the following main observations from Theorem 4.1. (1) The estimation error of influence function is controlled by the l2 regularization strength within a factor of O(1/λ). A stronger regularization will likely produce a better approximation. (2) The error is controlled by the inherent property of a model. A smoother model in terms of its hessian matrix will help lower the upper bound. (3) The upper bound is controlled by the norm of the changed gradient from z → z′. Intuitively, if removing eij incurs smaller changes in node representations, the approximation of the actual influence would be more accurate. Also, a smaller Err(−vi) is expected if the model is less prone to changes in training samples. (4) There are no significant correlations between the bound and the number of training nodes N . As a special case, if σmin = σ′ min = 0, the bound is irrelevant to N . We attach empirical verification for our bound in Appendix D.\n\nSimilar to Theorem 4.1, we have Corollary A.1 to derive an upper bound on ∥I ∗(−vi) − I(−vi)∥2 for removing a node vi from graph presented in Appendix A.\n\n5 EXPERIMENTS\n\nWe conducted three major experiments: (1) Validate the estimation accuracy of our influence functions on graph in Section 5.2; (2) Utilize the estimated edge influence to carry out adversarial attacks and graph rectification for increasing model performance in Section 5.3; and (3) Utilize the estimated node influence to carry out adversarial attacks on GCN (Kipf & Welling, 2017) in Section 5.4.\n\n5.1 SETUP\n\nWe choose six real-world graph datasets:Cora, PubMed, CiteSeer (Sen et al., 2008), WiKiCS (Mernyei & Cangea, 2020), Amazon Computers, and Amazon Photos (Shchur et al., 2018) in our experiments. Statistics of these datasets are outlined in Appendix B Table 4. For the Cora, PubMed, and CiteSeer datasets, we used their public train/val/test splits. For the Wiki-CS datasets, we took a random single train/val/test split provided by Mernyei & Cangea (2020). For the Amazon datasets, we randomly selected 20 nodes from each class for training, 30 nodes from each class for validation and used the rest nodes in the test set. All the experiments are conducted under the transductive node classification settings. We only use the last three datasets for influence validation.\n\n5.2 VALIDATING INFLUENCE FUNCTIONS ON GRAPHS\n\nValidating Estimated Influence We compared the estimated influence of removing a node/edge with its corresponding ground truth effect. The actual influence is obtained by re-training the model after removing a node/edge and calculating the change in the total cross-entropy loss. We also validated the estimated influence of removing node embeddings, for example, removing l(zi, yi) of node vi from the ERM objective while keeping the embeddings of other nodes intact. Figure 2\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: The Cora experiment – the estimated influences of individual training nodes/edges on the validation loss. The largest connected component of the Cora dataset is visualized here. Left: The dataset. The node size indicates if a node is in the training subset (large) or not (small). Middle: Influence of the training edges. Each edge is colored accordingly to its estimated influence value (blue - negative influence, removing it is expected to decrease the loss on the validation set; red – positive influence, removing it is expected to increase the loss on the validation set; and grey – little influence. The deeper color indicates higher influence.). Right: Influence of the training nodes. The same color scheme in the middle plot is used here.\n\nshows that the estimated influence correlates highly with the actual influence (Spearman correlation coefficients range from 0.847 to 0.981). More results are included in Figure 4 in the appendix.\n\nVisualization Figure 1 visualizes the estimated influence of edge and node removals on the validation loss for the Cora dataset. This visualization hints at opportunities for improving the test performance of a model or attacking a model by removing nodes/edges with noticeable influences (see experiments in Sections 5.3 and 5.4).\n\n5.3 APPLICATIONS OF THE ESTIMATED EDGE INFLUENCE\n\nThe estimated influence of edge removals on the validation set can be utilized to improve the test performance of SGC or carry out adversarial attacks on SGC/GCN.\n\nCora\n\nPubmed\n\nCiteseer\n\nMethods\n\nTable 1: Our performance via eliminating edges with negative influence values.\n\nGraph Rectification via Edge Removals We begin by investigating the impact of edges with negative influences. Based on our influence analysis, removing negative influence edges from the original will decrease validation loss. Thus the classification accuracy on the test set is expected to increase correspondingly. We sort the edges by their estimated influences in descending order, then cumulatively remove edges starting from the one with the lowest negative influence. We train the SGC model, fine-tune it on the public split validation set and select the number of negative influence edges to be removed by validation accuracy. For a fair comparison, we fix the test set remaining unchanged regarding the removal of the edges. The results are derived based on Figure 8 and displayed in Table 1, where we also report the performance of several classical and state-of-the-art GNN models on the original whole set as references, including GCN (Kipf & Welling, 2017), GAT (Veliˇckovi ́c et al., 2018), FGCN (Chen et al., 2018), GIN (Xu et al., 2019b), DGI (Velickovic et al., 2019) with a nonlinear activation function and SGC (Wu et al., 2019).\n\n81.4 ± 0.4 83.3 ± 0.7 79.8 ± 0.3 77.6 ± 1.1 82.5 ± 0.7 81.0 ± 0.0 81.8 ± 0.0\n\n79.0 ± 0.4 78.5 ± 0.3 77.4 ± 0.3 77.0 ± 1.2 78.4 ± 0.7 78.9 ± 0.0 79.7 ± 0.0\n\n70.1 ± 0.5 72.6 ± 0.6 68.8 ± 0.6 66.1 ± 0.9 71.6 ± 0.7 71.9 ± 0.1 73.7 ± 0.0\n\nGCN GAT FGCN GIN DGI SGC Ours\n\nWe demonstrate that our proposed method can marginally improve the accuracy of SGC from the data perspective and without any change to the original model structure of SGC, which validates\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Estimated influence vs. actual influence. Three datasets are used in this illustration Cora (left column), Pubmed (middle column) and Citeseer (right column). In all plots, the horizontal axes indicate the actual influence on the validation set, the vertical axes indicate the predicted influence, and ρ indicates Spearman’s correlation coefficient between our predictions and the actual influences. Top row: Influence of node embedding removal. Each point represents a training node embedding Middle row: Influence of edge removals. Each point corresponds to a removed edge. Bottom row: Influence of node removal. Each point represents a removed training node.\n\nTable 2: Grey-box attacks to GCN via edge removals. A lower performance indicates a more successful attack. The best attacks are in bold font. The number following the dataset name is the preattack performance. ‘-’ denotes an out-of-memory issue encountered on GPU with 24GB VRAM.\n\nDataset\n\nCora - 81.10%\n\nCiteseer - 70.07%\n\nPubmed - 79.80%\n\nElimination Rate\n\n1%\n\n3%\n\n5%\n\n1%\n\n3%\n\n5%\n\n1%\n\n3%\n\n5%\n\nDICE GraphPoison MetaAttack Ours\n\n79.9% 80.1% 80.0% 71.1% 70.3% 69.8% 79.4% 79.7% 79.1% 80.0% 80.1% 79.6% 70.2% 70.1% 70.0% 79.4% 79.7% 79.1% 79.6% 77.1% 73.3% 70.4% 69.3% 65.4% 77.3% 74.2% 72.8% 69.3% 67.4% 64.7% 69.3% 65.2% 64.1%\n\n-\n\n-\n\n-\n\nthe impacts of edges with negative influences. In addition, the performance of the SGC model with eliminating the negative influence edges can outperform other GNN-based methods in most cases.\n\nAttacking SGC via Edge Removals We investigated how to use edge removals to deteriorate SGC performance. Based on the influence analysis, removing an edge with a positive estimated influence can increase the model loss and decrease the model performance on the validation set. Thus, our attack is carried out in the following way. We first calculated the estimated influence of all edges and cumulatively removed edges with the highest positive influence one at a time. Every time we remove a new edge, we retrain the model to obtain the current model performance. We remove 100 edges in total for each experiment.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Study of edges with positive influence on both validation (blue) and test (red) set. Columns correspond to Cora, Pubmed and Citeseer datasets. Top: scale of values of the edges with negative influence. Bottom: accuracy drop by cumulatively removing edges with positive influence.\n\nTable 3: Performance of node removing attack. Lower performance means better attacks. The number after the dataset name means the performance of GCN model without an attack. Victim model’s test accuracy averaged over 25 runs on the citation network. Citeseer - 70.07% Cora - 81.10%\n\nPubmed - 79.80%\n\nDataset\n\nRemoving Rate\n\n5%\n\n10%\n\n15%\n\n5%\n\n10%\n\n15%\n\n5%\n\n10%\n\n15%\n\nRandom Degree Ours\n\n80.4% 80.3% 80.2% 70.6% 69.0% 69.2% 78.9% 79.6% 77.3% 80.3% 78.7% 79.0% 69.4% 68.3% 68.4% 79.1% 79.6% 77.4% 74.7% 59.8% 57.9% 69.5% 65.5% 56.1% 79.0% 77.2% 75.2%\n\nWe present our results in Figure 3. Apparently, in general, the accuracy of SGC on node classification drops significantly. We notice the influence of edges is approximately power-law distributed, where only a small proportion of edges has a relatively significant influence. The performance worsens with increasingly cumulative edge removals on both validation and test sets. The empirical results verify our expectations of edges with a positive estimated influence.\n\nAttacking GCN via Surrogate SGC We further explored the impact of removing positive influences edges under adversarial grey-box attack settings. Here, we followed Z ̈ugner & G ̈unnemann (2019) to interpret SGC as a surrogate model for attacking the GCN (Kipf & Welling, 2017) as a victim model, where the assumption lays under that the increase of loss on SGC can implicitly drop the performance of GCN. We eliminated positive influence edges at different rates 1%, 3%, 5% among all edges. The drop in accuracy was compared against DICE (Z ̈ugner et al., 2018), GraphPoison (Bojchevski & G ̈unnemann, 2019), MetaAttack (Z ̈ugner et al., 2018). For a fair comparison, we restrict the compared method can only perturb graph structures via edge removals.\n\nOur results are presented in Table 2. Our attack strategy achieves the best performance in all the scenarios of edge eliminations, especially on Pubmed with 1% edge elimination rate. Our attack model outperforms others by over 10% in accuracy drop. Since we directly estimate the impact of edges on the model parameter change, our attack strategy is more effective in seeking the most vulnerable edges to the victim model. These indicate that our proposed influence on edges can guide the construction of grey-box adversarial attacks on graph structures.\n\n5.4\n\nINFLUENCE OF NODE REMOVAL\n\nAttacking GCN via Node Removals In this section, we study the impact of training nodes with a positive influence on transductive node classification tasks. Again, we assume that eliminating the positive influence nodes derived from SGC may implicitly harm the GCN model. We sort the nodes\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nby our estimated influence in descending order and cumulatively remove the nodes from the training set. We built two baseline methods, Random and Degree, to compare the accuracy drop in different node removal ratios: 5%, 10%, 15%. For the Random baseline, we randomly remove the nodes from the training sets. For Degree baseline, we remove nodes by their degree in descending order.\n\nAccording to Table Table 3, the model performance on GCN drops by a large margin in all three citation network datasets as the selected positive influence node is removed, especially on the Cora dataset. The model outperforms the baseline over 20% on 15% removing ratio. These results indicate that our estimation of node influence can be used to guide the adversarial attack on GCN in the settings of node removal.\n\n6 RELATED WORKS\n\nInfluence Functions Recently, more efforts have been dedicated to investigating influence functions (Koh et al., 2019; Giordano et al., 2019; Ting & Brochu, 2018) in various applications, such as,computer vision (Koh & Liang, 2017), natural language processing (Han et al., 2020), tabular data (Wang et al., 2020b), causal inference (Alaa & Van Der Schaar, 2019), data poisoning attack (Fang et al., 2020; Wang et al., 2020a), and algorithmic fairness (Li & Liu, 2022). In this work, we propose a major extension of influence functions to graph-structured data and systemically study how we can estimate the influence of nodes and edges in terms of different editing operations on graphs. We believe our work complements the big picture of influence functions in machine learning applications.\n\nUnderstanding Graph Data Besides influence functions, there are many other approaches to exploring the underlying patterns in graph data and its elements. Explanation models for graphs (Ying et al., 2019; Huang et al., 2022; Yuan et al., 2021; Bajaj et al., 2021; Abrate & Bonchi, 2021) provide an accessible relationship between the model’s predictions and corresponding elements in graphs or subgraphs. They show how the graph’s local structure or node features impact the decisions from GNNs. As a major difference, these approaches tackle model inference with fixed parameters, while we focus on a counterfactual effect and investigate the contributions from the presence of nodes and edges in training data to decisions of GNN models in the inference stage.\n\nAdversarial Attacks on Graph The adversarial attack on an attributed graph is usually conducted by adding perturbations on the graphic structure or node features (Z ̈ugner & G ̈unnemann, 2019; Zheng et al., 2021). In addition, Zhang et al. (2020) introduces an adversarial attack setting by flipping a small fraction of node labels in the training set that causes a significant drop in model performance. A majority of the attacker models (Z ̈ugner et al., 2018; Xu et al., 2019a) on graph structure are constructed based on the gradient information on both edges and node features and achieved costly but effective attacking results. These attacker models rely mainly on greedy-based methods to find the graph structure’s optimal perturbations. We only focus on the perturbations resulting from eliminating edges and directly estimate the change of loss in response to the removal effect guided by the proposed influence-based approach.\n\n7 CONCLUSIONS\n\nWe have developed a novel influence analysis to understand the effects of graph elements on the parameter changes of GCNs without needing to retrain the GCNs. We chose Simple Graph Convolution due to its convexity and its competitive performance to non-linear GNNs on a variety of tasks. Our influence functions can be used to approximate the changes in model parameters caused by edge or node removals from an attributed graph. Moreover, we provided theoretical bounds on the estimation error of the edge and node influence on model parameters. We experimentally validated the accuracy and effectiveness of our influence functions by comparing its estimation with the actual influence obtained by model retraining. We showed in our experiments that our influence functions could be used to reliably identify edge and node with negative and positive influences on model performance. Finally, we demonstrated that our influence function could be applied to graph rectification and model attacks.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENT\n\nWe would like to thank the three anonymous reviewers for their constructive questions and invaluable suggestions. This work is partially supported by NSF DMR 1933525 and NSF OAC 1920147. Any opinions or conclusions in this paper are those of the authors and do not reflect the views of the funding agencies.\n\nREFERENCES\n\nCarlo Abrate and Francesco Bonchi. Counterfactual graphs for explainable classification of brain In Proceedings of the 27th ACM SIGKDD International Conference on Knowledge\n\nnetworks. Discovery and Data Mining, 2021.\n\nAhmed Alaa and Mihaela Van Der Schaar. Validating causal inference models via influence func-\n\ntions. In International Conference on Machine Learning, 2019.\n\nMohit Bajaj, Lingyang Chu, Zi Yu Xue, Jian Pei, Lanjun Wang, Peter Cho-Ho Lam, and Yong In Advances in Neural\n\nZhang. Robust counterfactual explanations on graph neural networks. Information Processing Systems, 2021.\n\nAleksandar Bojchevski and Stephan G ̈unnemann. Adversarial attacks on node embeddings via graph\n\npoisoning. In International Conference on Machine Learning, 2019.\n\nStephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-\n\nversity press, 2004.\n\nJie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via\n\nimportance sampling. International Conference on Learning Representations, 2018.\n\nWenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural\n\nnetworks for social recommendation. In The World Wide Web Conference, 2019.\n\nMinghong Fang, Neil Zhenqiang Gong, and Jia Liu. Influence function based data poisoning attacks\n\nto top-n recommender systems. In Proceedings of The Web Conference, 2020.\n\nRyan Giordano, William Stephenson, Runjing Liu, Michael Jordan, and Tamara Broderick. A swiss army infinitesimal jackknife. In International Conference on Artificial Intelligence and Statistics, 2019.\n\nM. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In Proceed-\n\nings of the IEEE International Joint Conference on Neural Networks, 2005.\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n\nIn Advances in Neural Information Processing Systems, 2017.\n\nFrank R Hampel. The influence curve and its role in robust estimation. Journal of the American\n\nStatistical Association, 1974.\n\nXiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling data artifacts through influence functions. Annual Meeting of the Association for Computational Linguistics, 2020.\n\nXiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. Lightgcn: In Proceedings of Simplifying and powering graph convolution network for recommendation. the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2020.\n\nChao Huang, Jiahui Chen, Lianghao Xia, Yong Xu, Peng Dai, Yanqing Chen, Liefeng Bo, Jiashu Zhao, and Jimmy Xiangji Huang. Graph-enhanced multi-task learning of multi-level transition dynamics for session-based recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nQiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, and Yi Chang. Graphlime: Local interpretable model explanations for graph neural networks. IEEE Transactions on Knowledge and Data Engineering, 2022.\n\nXiaolong Jiang, Peizhao Li, Yanjing Li, and Xiantong Zhen. Graph neural based end-to-end data association framework for online multiple-object tracking. arXiv preprint arXiv:1907.05315, 2019.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\n\nworks. International Conference on Learning Representations, 2017.\n\nPang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In\n\nInternatioboonal Conference on Machine Learning, 2017.\n\nPang Wei W Koh, Kai-Siang Ang, Hubert Teo, and Percy S Liang. On the accuracy of influence functions for measuring group effects. In Advances in Neural Information Processing Systems, 2019.\n\nJunying Li, Deng Cai, and Xiaofei He. Learning graph-level representation for drug discovery. arXiv\n\npreprint arXiv:1709.03741, 2017.\n\nPeizhao Li and Hongfu Liu. Achieving fairness at no utility cost via data reweighing with influence.\n\nIn International Conference on Machine Learning, 2022.\n\nPeizhao Li, Yifei Wang, Han Zhao, Pengyu Hong, and Hongfu Liu. On dyadic fairness: Exploring and mitigating bias in graph connections. In International Conference on Learning Representations, 2021.\n\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\n\nnetworks. International Conference on Learning Representations, 2016.\n\nP ́eter Mernyei and C ̆at ̆alina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural net-\n\nworks. arXiv preprint arXiv:2007.02901, 2020.\n\nDaryl Pregibon. Logistic regression diagnostics. The annals of statistics, 9(4):705–724, 1981.\n\nZarina Rakhimberdina and Tsuyoshi Murata. Linear graph convolutional model for diagnosing brain\n\ndisorders. In International Conference on Complex Networks and Their Applications, 2019.\n\nMariia Rizun. Knowledge graph application in education: a literature review. Acta Universitatis\n\nLodziensis. Folia Oeconomica, 2019.\n\nF. Scarselli, S. L. Yong, M. Gori, M. Hagenbuchner, A. C. Tsoi, and M. Maggini. Graph neural networks for ranking web pages. In Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence, 2005.\n\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\n\nCollective classification in network data. AI magazine, 2008.\n\nWalid Shalaby, BahaaEddin AlAila, Mohammed Korayem, Layla Pournajaf, Khalifeh AlJadda, Shannon Quinn, and Wlodek Zadrozny. Help me find a job: A graph-based approach for job In 2017 IEEE international conference on big data (big data), pp. recommendation at scale. 1544–1553. IEEE, 2017.\n\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G ̈unnemann. Pitfalls of graph neural network evaluation. Relational Representation Learning Workshop, NeurIPS 2018, 2018.\n\nIchigaku Takigawa and Hiroshi Mamitsuka. Graph mining: procedure, application to drug discovery\n\nand recent advances. Drug Discovery Today, 2013.\n\nDaniel Ting and Eric Brochu. Optimal subsampling with influence functions. In Advances in Neural\n\nInformation Processing Systems, 2018.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua International Conference on Learning Representations,\n\nBengio. Graph Attention Networks. 2018.\n\nPetar Velickovic, William Fedus, William L Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon\n\nHjelm. Deep graph infomax. International Conference on Learning Representations, 2019.\n\nBinghui Wang, Tianxiang Zhou, Minhua Lin, Pan Zhou, Ang Li, Meng Pang, Cai Fu, Hai Li, and Yiran Chen. Evasion attacks to graph neural networks via influence function. arXiv preprint arXiv:2009.00203, 2020a.\n\nRuijie Wang, Yuchen Yan, Jialu Wang, Yuting Jia, Ye Zhang, Weinan Zhang, and Xinbing Wang. Acekg: A large-scale knowledge graph for academic data mining. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, 2018.\n\nZifeng Wang, Hong Zhu, Zhenhua Dong, Xiuqiang He, and Shao-Lun Huang. Less is better: Unweighted data subsampling via influence function. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020b.\n\nFelix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International Conference on Machine Learning, 2019.\n\nKaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, and Xue Lin. Topology attack and defense for graph neural networks: An optimization perspective. International Joint Conferences on Artificial Intelligence Organization, 2019a.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? International Conference on Learning Representations, 2019b.\n\nZhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks. In Advances in Neural Information Processing Systems, 2019.\n\nHao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks via subgraph explorations. In International Conference on Machine Learning, 2021.\n\nMengmei Zhang, Linmei Hu, Chuan Shi, and Xiao Wang. Adversarial label-flipping attack and defense for graph neural networks. In IEEE International Conference on Data Mining, 2020.\n\nMuhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances in\n\nNeural Information Processing Systems, 2018.\n\nQinkai Zheng, Xu Zou, Yuxiao Dong, Yukuo Cen, Da Yin, Jiarong Xu, Yang Yang, and Jie Tang. Graph robustness benchmark: Benchmarking the adversarial robustness of graph machine learning. Neural Information Processing Systems Datasets and Benchmarks Track, 2021.\n\nDaniel Z ̈ugner, Amir Akbarnejad, and Stephan G ̈unnemann. Adversarial attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018.\n\nDaniel Z ̈ugner and Stephan G ̈unnemann. Adversarial attacks on graph neural networks via meta\n\nlearning. In International Conference on Learning Representations, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA PROOFS\n\nLemma 3.1. Consider empirical risk minimization ˆθ = arg minθ∈Θ δ) = arg minθ∈Θ l, let I ∗(xj → xj + δ) = ˆθ(xj → xj + δ) − ˆθ, the estimated influence satisfies linearity:\n\ni l(xi, yi) and ˆθ(xj → xj + i̸=j l(xi, yi) + l(xj + δ, yj) with some twice-differentiable and strictly convex\n\n(cid:80)\n\n(cid:80)\n\nI(xj → xj + δ) = I(−xj) + I(+(xj + δ)).\n\n(3)\n\nProof. Notice the actual model parameters in response of the perturbations ∆ can be denoted as:\n\nˆθ(xj → xj + δ)def= arg min\n\nθ∈Θ\n\n1 N\n\nN (cid:88)\n\nk=1\n\nl (xk, yk) −\n\n1 N\n\nl (xj, yj) +\n\n1 N\n\nl (xj + δ, yj)\n\nIn this case, the actual change in model parameters in response of the perturbations can be represented as: I(xj → xj + δ)=ˆθ(xj → xj + δ)−ˆθ. For estimating ∆θ, we start by considering the parameter change from up weighting infinite small ε on {x′ l} and down weight infinite small ε on {xl} where ∀l ∈ L. By definition, the model parameter in response of perturbation ˆθε can be represented as:\n\nˆθε\n\ndef= arg min\n\nθ∈Θ\n\n1 N\n\nN (cid:88)\n\nk=1\n\nl (xk, yk) − εl (xj, yj) + εl (xj + δ, yj)\n\nThe change of model parameter due to the modification on group of data’s weight on loss be:\n\nSince ˆθε minimize the changed loss function under perturbation, take the derivative:\n\n∆θε = ˆθε − ˆθ\n\n0 =\n\n1 N\n\nN (cid:88)\n\nk=1\n\n∇ˆθε\n\nl (xk, yk) − ε∇ˆθε\n\nl (xj, yj)\n\n+ ε∇ˆθε\n\nl (xj + δ, yj)\n\nApply the first order Taylor expansion of ˆθε on ˆθ on the right side of the equation, we have:\n\n(cid:34)\n\n0 =\n\n(cid:34)\n\n1 N\n\n1 N\n\nN (cid:88)\n\nk=1\n\nN (cid:88)\n\nk=1\n\n∇θl (xk, yk) + ε∇θl (xj + δ, yj) − ε∇θl (xj, yj)\n\n+\n\n(cid:35)\n\n(cid:35)\n\n∇2\n\nθl (xk, yk) + ε∇2\n\nθl (xj + δ, yj) − ε∇2\n\nθl (xj, yj)\n\n· ∆θε + o(∆θ2 ε)\n\n(8)\n\n(9)\n\n(10)\n\n(11)\n\nSince ˆθ minimize the loss function without perturbation, 1 o(∆θ2\n\nε) term, We have:\n\nN\n\n(cid:80)N\n\nk=1 ∇ˆθε\n\nl (xk, yk)=0. Dropping\n\n∆θε ≈ −\n\n(cid:34)\n\n1 N\n\nN (cid:88)\n\nk=1\n\n∇2\n\nθl (xk, yk) + ε∇2\n\nθl (xj + δ, yj) − ε∇2\n\nθl (xj, yj)\n\n(cid:35)−1\n\n·\n\n[ε∇θl (xj + δ, yj) − ε∇θl (xj, yj)]\n\nTake the derivative of ∆θε over ε, by dropping O(ε) terms we have:\n\n∂∆θε ∂ε\n\n= −\n\n1 N\n\nN (cid:88)\n\nk=1\n\n∇2 ˆθ\n\nl (xk, yk)−1 (cid:2)∇ˆθl (xj + δ, yj) − ∇ˆθl (xj, yj)(cid:3)\n\n= −H −1\n\nˆθ\n\n(cid:88)\n\n(∇l (xj + δ, yl) − ∇l (xj, yj))\n\nl∈L\n\n13\n\n(12)\n\n(13)\n\nPublished as a conference paper at ICLR 2023\n\nFor sufficient large N , by setting ε to 1 N , the changed we can approximate the actual change in model parameters using: I(xj → xj + δ)=ˆθ(xj → xj + δ)−ˆθ≈ˆθε−ˆθ. Plugging in to Eq. (13) we finish the proof:\n\nI(xj → xj + δ) ≈ −H −1 = −H −1\n\nˆθ\n\nˆθ\n\n(∇l (xj + δ, yl) − ∇l (xj, yj)) ∇l (xj + δ, yl) + H −1\n\n∇l (xj, yl)\n\nˆθ\n\n(14)\n\n= I(+(xj + δ)) + I(−xj).\n\nProposition 3.3. Let δj(−vi) denote the j-th row of ∆(−vi). The influence of removing node vi from graph G can be estimated by:\n\nI(−vi) = I(−zi) + I(z → z + δ(−vi)) = I(−zi) +\n\n= −1vi∈Vtrain · H−1\n\nˆθ\n\n∇ˆθl (zi, yi) − H−1 ˆθ\n\n(cid:88)\n\nvj ∈Vtrain\n\n(cid:88)\n\nj\n\nI(+(zj + δj(−vi))) + I(−zj)\n\n(∇ˆθl(zj + δj(−vi), yj) − ∇ˆθl(zj, yj)),\n\nwhere 1 is an indicator function.\n\n(6)\n\nProof. Similarly to the edge removal, we first calculate the node representation change incurred from the removal of the node vi of a 2-layer SGC as follow:\n\n∆(−vi) =\n\n(cid:104) (D− 1\n\n2 −vi\n\nA−viD− 1\n\n2 −vi\n\n)2 − (D− 1\n\n2 AD− 1\n\n2 )2(cid:105)\n\nX.\n\n(15)\n\nThe above change will affect a set of nodes, including the node vi itself and the 2-hop neighbors of the node vi connected neighbors. A set of nodes S={s|s ∈ Ni ∪j∈Ni Nj} capture the changed node embeddings in the training set, i.e., δs ̸= 0, where ∆−vi = {δi}N i=1 in Eq. (15). The model parameter change of the removal of the node vi can be characterized by removing the representation of the node vi if the node vi is a training sample, and the node representation change from the set S. Thus, we have\n\nI(−vi) = −1vi∈Vtrain · I (zi, yi) +\n\n(cid:88)\n\ns∈{S\\vi}\n\n(I (z′\n\ns, ys) − I (zs, ys))\n\n= −1vi∈Vtrain · H−1\n\nˆθ\n\n∇LCE (zi, yi) − H−1\n\nˆθ\n\n(cid:88)\n\ns∈S\\vi\n\n(∇LCE (z′\n\ns, ys) − ∇LCE (zs, ys)).\n\n(16)\n\nWe finish the proof.\n\nTheorem 4.1. Let σmin ≥ 0 denote the smallest eigenvalue of all eigenvalues of Hessian matrices l(zi, yi), ∀vi ∈ Vtrain of the original model ˆθ. Let σ′ ∇2 min ≥ 0 denote the smallest eigenvalue of all ˆθ l(zi, yi), ∀vi ∈ Vtrain of the retrained model ˆθ(−eij) with eigenvalues of Hessian matrices ∇2 eij removed from graph G. Use L denote the set {v : z′ ̸= z} containing affected nodes from the edge removal, and Err(−eij) = ∥I ∗(−eij) − I(−eij)∥2. Recall λ is the l2 regularization strength, we have an upper bound on the estimated error of model parameters’ change:\n\nˆθ(−eij )\n\nErr(−eij) ≤\n\n(N λ + (N − |L|)σmin + σ′\n\nN 3C\n\nmin|L|)3 · ∥\n\n(cid:88)\n\nvl∈L\n\n(∇ˆθl(z′\n\nl, yl) − ∇ˆθl(zl, yl))∥2\n\n2\n\n+\n\nN\n\nN λ + (N − |L|)σmin + min(σmin, σ′\n\nmin)|L|\n\n(cid:88)\n\n· ∥\n\n(∇ˆθl(z′\n\nl, yl) − ∇ˆθl(zl, yl))∥2.\n\nvl∈L\n\n(7)\n\nProof. In this proof, we utilize one-step Newton approximation as an intermediary to estimate the error bound of the change in model parameters, i.e.,\n\nErr(−eij) = (cid:2)I ∗(−eij) − I N t(−eij)(cid:3) + (cid:2)I N t(−eij) − I(−eij)(cid:3) ,\n\n(17)\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nwhere I ∗(−eij)=∆ˆθε=ˆθε−ˆθ, I N t(−eij) is the one-step Newton approximation with the model parameter ˆθN t=ˆθ + ∆ˆθN t. According to Boyd et al. (2004) (Section 9.5.1), ∆ˆθN t can be calculated as follows:\n\n∆ˆθN t = − (cid:0)Hˆθ + λI(cid:1)−1\n\n·\n\n1 N\n\nN (cid:88) (\n\ni=1\n\n(cid:88)\n\n−\n\nvl∈L\n\n∇ˆθl(zi, yi) +\n\n(cid:88)\n\nvl∈L\n\n∇ˆθl(z′\n\nl, yl)\n\n(18)\n\n∇ˆθl(zl, yl) + λ∥ˆθ∥2).\n\nIn the following, we will calculate the bound of I ∗(−eij) − I N t(−eij) and I N t(−eij) − I(−eij) as two separate steps and combine them together. Here we define the before and after objective functions with the removal of edge eij as follows:\n\nLb(θ) =\n\nLa(θ) =\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\ni=1\n\nl(zi, yi) +\n\nλ 2\n\n∥θ∥2 2,\n\nl(zi, yi) +\n\n(cid:88)\n\nvl∈L\n\nl(z′\n\nl, yl) −\n\nl(zl, yl) +\n\nλ 2\n\n∥θ∥2 2.\n\n(cid:88)\n\nvl∈L\n\n(19)\n\nStep I: Bound of I ∗(−eij) − I N t(−eij).\n\nDue to that SGC model is convex on θ, we take the second derivative of La(θ) and have\n\nλI +\n\n1 N\n\n(cid:34) N\n\n(cid:88)\n\ni=1\n\n∇2L (zi, yi) +\n\n(cid:88)\n\nvl∈L\n\n∇2L (z′\n\nl, yl) −\n\n(cid:88)\n\nvl∈L\n\n(cid:35)\n\n∇2L (zl, yl)\n\n≻ 0.\n\n(20)\n\nTo simplify the above equation, we define σ′ of ∇2l (z′ we have\n\nmax are the smallest and largest eignenvalues l, yl) and σmin and σmax are the smallest and largest eignenvalues of ∇2L (zl, yl). Then\n\nmin and σ′\n\n(cid:18)\n\nI ·\n\nλ +\n\n(N − |L|) · σmin + |L| · σ′\n\nmin\n\n(cid:19)\n\n≻ 0.\n\n(21)\n\nN\n\nTherefore, the SGC loss function corresponds to the removal of edge is strictly convex with the\n\n(cid:16)\n\nλ + (N −|L|)·σmin+|L|·σ′\n\n(cid:17)\n\nparameter . By this convexity property and the implications of strong convexity (Boyd et al., 2004) (Section 9.1.2), we can bound I ∗(−eij) − I N t(−eij) with the first derivative of SGC loss function as follows:\n\nmin\n\nN\n\nI ∗(−eij) − I N t(−eij)\n\n=∥∆ˆθε − ∆ˆθN t∥2 = ∥(∆ˆθε + ˆθ) − (∆ˆθN t + ˆθ)∥2 = ∥ˆθε − ˆθN t∥2\n\n≤\n\n2N\n\nN λ + (N − |L|)σmin + |L|σ′\n\nmin\n\n·\n\n(cid:119) (cid:119) (cid:119)\n\n1 N\n\n−\n\nN (cid:88) (\n\n∇ˆθN t\n\nl(zi, yi) +\n\n(cid:88)\n\nvl∈L\n\n∇ˆθN t\n\nl(z′\n\nl, yl)\n\n(22)\n\ni=1 (cid:88)\n\n∇ˆθN t\n\nvl∈L\n\nl(zl, yl) + λ∥ˆθN t∥2)\n\n(cid:119) (cid:119) (cid:119)2\n\n.\n\nIf we take a close look at the second term in the above equation, we notice it is equal the first derivative of La(θ), i.e.,\n\n∇θla(ˆθN t) =\n\n1 N\n\nN (cid:88)\n\n(\n\nk=1\n\n∇ˆθN t\n\nl(zk, yk) +\n\n(cid:88)\n\nvl∈L\n\n∇ˆθN t\n\nl(z′\n\nl, yl) −\n\n(cid:88)\n\nvl∈L\n\nTherefore, we focus on bounding ∥∇θLa(ˆθN t)∥2 in the following.\n\n∇ˆθN t\n\nl(zl, yl) + λ∥ˆθN t∥2). (23)\n\n∥∇θLa(ˆθN t)∥2 = ∥∇θLa(ˆθ + ∆ˆθN t)∥2\n\n=∥∇θLa(ˆθ + ∆ˆθN t) − ∇θLa(ˆθ) + ∇θLa(ˆθ)∥2 =∥∇θLa(ˆθ + ∆ˆθN t) − ∇θLa(ˆθ) − ∇2\n\nθLa(ˆθ)∆ˆθN t∥2\n\n(24)\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nThe above last equation holds due to the definition of ∆ˆθN t in Eq. (18). For any continuous function f and any inputs a and b, there exists f (a + b) − f (a) − bf ′(a)=(cid:82) 1 (f ′(a + bt) − f ′(a))dt. Based on that, we can rewrite ∥∇θLa(ˆθN t)∥2 as follows:\n\n0 b ·\n\n∥∇θLa(ˆθN t)∥2 =∥∇θLa(ˆθ + ∆ˆθN t) − ∇θLa(ˆθ) − ∇2\n\n(cid:119) (cid:119) (cid:119)\n\n=\n\n(cid:90) 1\n\n0\n\n∆ˆθN t(∇2\n\nθLa(ˆθ + ∆ˆθN t · t) − ∇2\n\nθLa(ˆθ)∆ˆθN t∥2 (cid:119) (cid:119) (cid:119)2\n\nθLa(ˆθ))dt\n\n(25)\n\n.\n\nWe assume the loss function l on is twice differentiable and the second derivative of the loss function is Lipschitz continuous at θ, with parameter C. Here C is controlled by the third derivative (Curvature) of the loss function l. Thus, we have\n\nThen we take Eq. (26) into Eq. (25) and have\n\n∥∇2\n\nθl(θ1) − ∇2\n\nθl(θ2)∥2 ≤ C · ∥θ1 − θ2∥2.\n\ndelt∥∇θLa(ˆθN t)∥2 (cid:90) 1\n\n≤∥N C∆ˆθN t\n\n0\n\n≤\n\nN C 2\n\n·\n\ntdt∥2 =\n\nN C 2\nN 2\n\n∥∆ˆθN t∥2\n\n2 =\n\nN C 2\n\n∥∇2\n\nθLa(ˆθ)−1 · ∇θLa(ˆθ)∥2\n\n2\n\n(N λ + (N − |L|) · σmin + |L| · σ′\n\nmin)2 · ∥\n\n(∇ˆθl (z′\n\nl, yl) − ∇ˆθl (zl, yl))∥2 2.\n\n(cid:88)\n\nl∈L\n\nThe above last inequation holds according to the bound of ∇2\n\nθLa(ˆθ)−1 and Eq. (19).\n\nCombining Eq. (22), (23) and (27), we finish the bound of I ∗(−eij) − I N t(−eij) as follows:\n\n∥I ∗(−ei,j) − I N t(−ei,j)∥2\n\n≤\n\n(N λ + (N − |L|)σmin + σ′\n\nN 3C\n\nmin|L|)3 · ∥\n\n(cid:88)\n\n(∇ˆθl (z′\n\nl, yl) − ∇ˆθl (zl, yl))∥2 2.\n\n(28)\n\nvl∈L\n\nWe finish Step I.\n\nStep II: Bound of I N t(−eij) − I(−eij).\n\nBy the definition of I N t(−eij) and I(−eij)), we have:\n\nI N t(−eij) − I(−eij) (cid:40)(cid:32)\n\n(cid:34) n\n\n=\n\nλI +\n\n1 N\n\n(cid:88)\n\nk=1\n\n(cid:32)\n\n−\n\nλI +\n\n1 N\n\nn (cid:88)\n\nk=1\n\n∇2 ˆθ\n\nl (zk, yk) +\n\n(cid:88)\n\n∇2 l (z′ ˆθ\n\nl, yl) −\n\nvl∈L (cid:33)−1 (cid:41) ·\n\n(cid:40)\n\n(cid:88)\n\nvl∈L\n\n(cid:35)(cid:33)−1\n\n∇2 ˆθ\n\nl (zl, yl)\n\n(cid:88)\n\nvl∈L\n\n(cid:41)\n\n∇2 ˆθ\n\nl (zk, yk)\n\n(∇ˆθl (z′\n\nl, yl) − ∇l (zl, yl))\n\n.\n\nFor simplification, we use matrix A, B and C for the following substitutions:\n\nA = λI +\n\n1 N\n\n(cid:34) n\n\n(cid:88)\n\nk=1\n\n∇2 ˆθ\n\nl (zk, yk) −\n\n(cid:35)\n\n∇2 ˆθ\n\nl (zl, yl)\n\n,\n\n(cid:88)\n\nvl∈L\n\nB =\n\n1 N\n\n(cid:88)\n\nvl∈L\n\nl (z′ ∇2 ˆθ\n\nl, yl) , and C =\n\n1 N\n\n(cid:88)\n\nvl∈L\n\n∇2 ˆθ\n\nl (zl, yl) ,\n\nwhere A, B and C are positive definite matrix and have the following properties:\n\nλ +\n\n(N − |L|)σmax N\n|L|σ′ N\n\nmax\n\n≻A ≻ λ +\n\n(N − |L|)σmin N\n\n,\n\n≻B ≻\n\n|L|σ′ N\n\nmin\n\n, and\n\n|L|σmax N\n\n≻ C ≻\n\n|L|σmin N\n\n.\n\n16\n\n(26)\n\n(27)\n\n(29)\n\n(30)\n\n(31)\n\nPublished as a conference paper at ICLR 2023\n\nTherefore, we have\n\nI N t(−eij) − I(−eij) = ((A + B)−1 − (A + C)−1) ·\n\n(∇ˆθl (z′\n\nl, yl) − ∇ˆθl (zl, yl))\n\n, (32)\n\n(cid:41)\n\n(cid:40)\n\n(cid:88)\n\nvl∈L\n\nwhere (A + B)−1 − (A + C)−1 ≺\n\nN λ+(N −|L|)σmin+|L|min(σ′\n\nN\n\nmin,σmin) I.\n\nThe l2 norm of the error between our predicted influence and Newton approximation can be bounded as follows:\n\n∥I N t(−eij) − I(−eij)∥2\n\n≤\n\nN λ + (N − |L|)σmin + min(σ′\n\nmin, σmin)|L|\n\nN\n\n(cid:88)\n\n· ∥\n\n(∇lˆθ (z′\n\nl, yl) − ∇lˆθ (zl, yl))∥2.\n\n(33)\n\nvl∈L\n\nWe finish Step II.\n\nCombining the conclusion in Step I and II in Eq. (28) and (33), we have the error between the actual influence and our predicted influence as:\n\nErr(−eij)\n\n≤∥I ∗(−eij) − I N t(−eij)∥2 + ∥I N t(−eij) − I(−eij)∥2\n\n=\n\n(N λ + (N − |L|)σmin + |L|σ′\n\nN 3C\n\nmin)3 · ∥\n\n(cid:88)\n\n(∇ˆθl (z′\n\nl, yl) − ∇ˆθl (zl, yl))∥2\n\n2\n\n(34)\n\nvl∈L\n\n+\n\nN\n\nN λ + (N − |L|)σmin + min(σ′\n\nmin, σmin)\n\n(cid:88)\n\n· ∥\n\nvl∈L\n\n(∇ˆθl (z′\n\nl, yl) − ∇ˆθl (zl, yl))∥2.\n\nWe finish the whole proof.\n\nCorollary A.1. Let σmin ≥ 0 denote the smallest eigenvalue of all eigenvalues of Hessian matrices l(zi, yi), ∀vi ∈ Vtrain of the original model ˆθ. Let σ′ ∇2 min ≥ 0 denote the smallest eigenvalue of all ˆθ l(zi, yi), ∀vi ∈ Vtrain of the retrained model ˆθ(−vi) with eigenvalues of Hessian matrices ∇2 vi removed from graph G. Use S denote the set {v : z′ ̸= z} containing affected nodes from the node removal, and Err(−vi) = ∥I ∗(−vi) − I(−vi)∥2. We have the following upper bound on the estimated error of model parameters’ change:\n\nˆθ(−vi)\n\nErr(−vi) ≤\n\nN 3m2C\n\n((N − 1)λ + (N − |S|)σmin + σ′\n\nmin|S|)3 +\n\nN 3C\n\n+\n\n(N λ + (N − 1)σmin)3 · ∥l (z′\n\ni, yi) ∥2\n\n2 +\n\n(N − 1)m\n\nN λ + (N − |S|)σmin + min(σmin, σ′\n\nmin)|S|\n\nN N λ + N σmin\n\n· ∥l (z′\n\ni, yi) ∥2\n\n(35)\n\nwhere m = ∥ (cid:80)\n\nvs∈S[∇ˆθl(z′\n\ns, ys) − ∇ˆθl(zs, ys)] − ∇ˆθl(zi, yi)∥2.\n\nProof. We provide a simple proof for the error bound of removing a complete nodes. Notice that this error can be decomposed into two parts, 1, the error or removing a single node embedding zi and 2, the error of adding z′\n\ns and removing zs, where s∈S. where we have Err(zs → z′ Err(−vi) ≤\n\ns) + Err(−zi)\n\n(cid:88)\n\ns∈S\n\nNotice that Eq. Theorem 4.1 proofs the error bound of Err(−vi), in the proving process we decompose the problem in to deriving the error bound by adding z′ l and removing zl where l∈L, where L is the set of changed node embedding caused by removing a edge from the graph. Following the same proving setting of Eq. Theorem 4.1, Again, notice that S is the set of changed node embedding caused by removing a node from the graph. We simply substitute L by S, we have the error bounds for (cid:80)\n\ns∈S Err(zs → z′ s). (cid:88)\n\ns∈S\n\n+\n\nErr(zs → z′\n\ns) ≤\n\nN 3m2C\n\nN λ + (N − |S|)σmin + σ′\n\nmin|S|)3\n\n(N − 1)m\n\nN λ + (N − |S|)σmin + min(σmin, σ′\n\nmin)|S|\n\n,\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nWhere m = ∥ (cid:80) For Err(−zi), it can be derived following the same proving process as Eq. Theorem 4.1, but we only remove one data points. In this case we have:\n\ns, ys) − ∇ˆθl(zs, ys)]∥2.\n\nvs∈S[∇ˆθl(z′\n\nErr(−zi) ≤\n\nN 3C\n\n(N λ + (N − 1)σmin)3 · ∥l (z′\n\ni, yi) ∥2\n\n2 +\n\nN N λ + N σmin\n\n· ∥l (z′\n\ni, yi) ∥2.\n\nCombining the two error bounds we have:\n\nErr(−vi) ≤\n\nN 3m2C\n\n((N − 1)λ + (N − |S|)σmin + σ′\n\nN 3C\n\n+\n\n(N λ + (N − 1)σmin)3 · ∥l (z′\n\nmin|S|)3 + i, yi) ∥2\n\n2 +\n\n(N − 1)m\n\nN λ + (N − |S|)σmin + min(σmin, σ′\n\nmin)|S|\n\nN N λ + N σmin\n\n· ∥l (z′\n\ni, yi) ∥2\n\n(36)\n\nB DATASET STATISTICS\n\nWe present the data statistic on our experiments below. We choose only small and medium-sized data. Because, each time we validate the influence of the elements in a graph, we need to retrain the model.\n\nTable 4: Dataset Statistics\n\nDataset\n\n# Node\n\n# Edge\n\n# Class\n\n# Feature\n\n# Train/Val/Test\n\nCora Citeseer Pubmed WikiCS Amazon Computer Amazon Photo\n\n2,708 3,327 19,717 11,701 13,752 7,650\n\n5,429 4,732 44,338 216,123 245,861 119,081\n\n7 6\n3 10 10 8\n\n1,433 3,703 500 300 767 745\n\n140 / 500 / 1,000 120 / 500 / 1,000 60 / 500 / 1,000 250/ 1769 /5847 200 / 300 / Rest 160 / 240 / Rest\n\nC VALIDATING INFLUENCE OF ELEMENTS: EXTRA DATASETS\n\nFor the Wiki-CS dataset, we randomly select one of the train/val/test split as described in Mernyei & Cangea (2020) to explore the effect of training nodes/edges influence. For the Amazon Computers and Amazon Photo dataset, we follow the implementation of Shchur et al. (2018). To set random splits, On each dataset, we use 20 ∗ C nodes as training set, 30 ∗ C nodes as validating set and the rest nodes as testing set, where C is the number of classes.. Because for validating every edge’s influence, we need to retrain the model and compare the change on loss, the computation cost is exceptionally high. We randomly choose 10000 edges of each datasets and validate their influence. We observe that even for medium-size datasets, our estimated influence is of high correlation to the actual influence.\n\nD EMPIRICAL VERIFICATION OF THEOREM 4.1\n\nAs the value of l2 regularization term decreases, the accuracy of our estimation of the influence of edges drops, and the Spearman correlation coefficient decrease correspondingly. This trend is consistent with the interpretations of the error bound on Theorem 4.1 that the estimation error of an influence function is inversely related with the l2 regularization term. We also notice that the edges that connects high-degree nodes have overall less influence. Their estimation points lies relatively close to the y=x line and thus could have relative small estimation error.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Estimated influence vs. actual influence on medium-sized graphs. Three datasets are used in this illustration Wiki-CS (left column), Amazon Computers (middle column) and Amazon Photo (right column). In all plots, the horizontal axes indicate the actual influence on the test set, the vertical axes indicate the predicted influence, and ρ indicates Spearman’s correlation coefficient between our predictions and the actual influences. Top row: Influence of node embeddings. Middle row: Influence of edge removals. Each point corresponds a removed training edge. Bottom row: Influence of node removal. Each point represents a removed training node.\n\nFigure 5: Spearman correlation on Citeseer dataset with different l2 regularization term on validating influence of edges. The orange points denote the summations of the degrees of the two nodes that an edge connects is high. The blue points denote the edges, which are the summations of the degrees of the two nodes connecting the edge.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nE GROUP EFFECT OF REMOVING MULTIPLE EDGES\n\nWe study the group effect of influence estimation on removing multiple edges. On dataset Cora, we randomly sample k edges from the attributed graph, where k’s values were chosen increasingly as 2, 10, 50, 100, 200, 350. Every time, we remove k edges simultaneously and validate their estimated influence. We observe: though with high correlation, our influence plots tend to move downward as more edges are removed at the same time. In this case, our method tends to be less accurate and underestimates the influence of a simultaneously removed group of edges.\n\nFigure 6: Estimating group influence on Cora. The horizontal axes indicate the actual influence on the validation set, and the vertical axes indicate the predicted influence. On each set, we randomly sample k edges (k=2, 10, 50, 100, 200, 350) from the graph and repeat this process 5000 times. Each time, we remove k edges simultaneously and validate our influence estimation.\n\nF VALIDATING INFLUENCE FOR ARTIFICIALLY ADDED EDGES\n\nIn this section, we validate our influence estimation for artificially added edges on dataset Cora, Pubmed, and Citeseer. On each dataset, we randomly select 10000 unconnected node pairs, add an artificial edge between them and validate its influence estimation. Figure 7 shows that the estimated influence correlates highly with the actual influence. This demonstrates that our proposed method can successfully evaluate the influence of artificially added edges.\n\nFigure 7: Estimated influence vs. actual influence on artificially added edges. Three datasets are used in this illustration Cora, Pubmed, and Citeseer. Due to the high time complexity of evaluating the influence on every pair of nodes, we randomly sample 10000 node pairs and add artificial edge.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nG STUDY OF EDGES WITH NEGATIVE INFLUENCE\n\nHere we demonstrate the performance via cumulatively removing edges with negative influence in Figure 8. The detailed implementation has been discussed in Section 5.3. Due to that the inaccurate influence estimation with more edges removed, we consider a maximum of 50 edges to be removed for each dataset. We observe an overall increase in model performance as we cumulatively remove edges predicted as a negative influence. This again demonstrates the usefulness of our influence estimation on edges.\n\nFigure 8: Study of edges with negative influence, each column corresponds to Cora, Pubmed, and Citeseer dataset. Top: the scale of edges with negative influence. Bottom: accuracy by cumulatively removing edges with negative influence. Blue and red lines present the accuracy changes of validation and test in response to negative influence edge removal, respectively.\n\nH EXTEND INFLUENCE METHOD TO OTHER GNN MODELS\n\nTheoretically, our current pipeline can be extended to other nonlinear GNNs under some violation of assumption. (1) According to Propositions Proposition 3.2 and Proposition 3.3, we require the existence of the inverse of the Hessian matrix, which is based on the assumption that the loss function on model parameters is strictly convex. Under the context of some GNN models with nonlinear activation functions, we can use the pseudo-inverse of the hessian matrix instead. (2) For non-convex loss functions of most GNN, our proposed error bound in Theorem Theorem 4.1 does not hold unless a large regularization term is applied to make the hessian matrix positive definite. From the implementation purpose, (1) From the implementation perspective, the non-linear models usually have more parameters than the linear ones, which require more space to store the Hessian matrix. Accordingly, the calculation of the inverse of the Hessian matrix might be out of memory. It needs to reformulate the gradient calculation and apply optimization methods like Conjugate gradient for approximation. (2) Our current pipeline is constructed based on mathematical, hands-on derived gradients adopted from Koh et al. (2019). Existing packages like PyTorch use automatic differentiation to get the gradients on model parameters. It could be inaccurate for second-order gradients calculation. Extending the current pipeline to other GNNs may require extensive first and second-order gradient formulations. We will explore more GNN influence in the future.\n\nI RUNNING TIME COMPARISON\n\nWe present the running time comparison between calculating the edge influence via the influencebased method and retrieving the actual edge influence via retraining. We conduct our experiment on dataset Cora, Pubmed, and Citeseer. We demonstrate our method is 15-25 faster than the retrained method. Notably, for tasks like improving model performance or carrying out adversarial attacks via\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nedge removal, it could save a considerable amount of time in finding the edge to be removed with the lowest/largest influence.\n\nTable 5: Running time comparisons for edge removal by second. Self-loop edges are not recorded.\n\nDataset\n\nCora Pumbed Citeseer\n\nInfl. (single edge)\n\nInfl. (all edges)\n\nRetrain (single edge) Retrain (all edges)\n\n0.0049±0.0006 0.0008±0.0001 0.0097±0.0008\n\n24.86 34.58 45.90\n\n0.0683±0.0216 0.0203±0.0044 0.1578±0.0404\n\n370.80 899.62 746.47\n\n22",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a methodology to leverage influence functions toward estimating how GNN model parameters change under graph perturbation schemes. This is an important problem to efficiently understand the effect of graph elements on model parameters, without the need to retrain the model. Specifically, the paper considers edge and node removals as perturbation strategies, deriving theoretical bounds to characterize the changes on model parameters. Besides, it is empirically shown how to utilize such graph influence functions to (i) improve the prediction performance, and (ii) as tools to guide adversarial attacks to the GNN.\n\n# Strength And Weaknesses\n\n*Strengths:*\n* Leveraging influence functions to analyze the performance of a GNN constitutes an interesting theoretical framework. \n\n* The paper provides a good experimental pipeline in which the influence scores of nodes and edges are utilized in different ways (e.g., to define attacks).\n\n*Weaknesses:*\n* The formulation of the methodology is done on the Simplified Graph Convolution (SGC) model. Although the paper states that their analysis could be extended to other GNN models, this does not seem to be straightforward. \n\n* Missing possible connections to related work and theory. One of the practical applications of influence functions considered in the paper, examines how edge removals could improve classification performance. To do so, edges with negative influence are removed from the graph before training the SGC model. \n\n  First, what is the fraction of edges that are being removed, and how is the performance affected if we increase/decrease the number of deleted edges? \n\n  Second, how is this methodology related to other approaches that remove edges towards building more robust models? For instance, it has been shown that a simple random edge removal could improve performance by reducing over-smoothing (the DropEdge model). Is this something contradictory to the theoretical results presented in the paper? This point needs further clarification to understand the impact of edge influence scores.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nOverall, the paper is well-written, and the different concepts are clearly presented. At the same time, the paper keeps a good balance between theoretical contributions and empirical evaluation.  Despite building on existing ideas, the paper studies a novel application of influence functions in GNNs.\n\nTypos:\n* Page 4: Proposition 3.2 offer*s\n* Section 5.4: Table 3 is not mentioned in the paragraphs of the subsection.\n\n# Summary Of The Review\n\nOverall, the paper introduces an exciting approach to further understanding the impact of graph components on the performance of the model. Nevertheless, I still have some concerns about the practical application of the framework (restricted or not to SGC) and its relationship to previous developments (e.g., DropEdge).\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMODEL TRANSFERABILITY WITH RESPONSIVE DECISION SUBJECTS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nD\n\nThis paper studies model transferability when human decision subjects respond to a deployed machine learning model. In our setting, an agent or a user corresponds to a sample (X, Y ) drawn from a distribution and will face a model h and its classification result h(X). Agents can modify X to adapt to h, which will incur a distribution shift on (X, Y ). Therefore, when training h, the learner will need to consider the subsequently “induced” distribution when the output model is deployed. Our formulation is motivated by applications where the deployed machine learning models interact with human agents, and will ultimately face responsive and interactive data distributions. We formalize the discussions of the transferability of a model by studying how the model trained on the available source distribution (data) would translate to the performance on the induced domain. We provide both upper bounds for the performance gap due to the induced domain shift, as well as lower bounds for the trade-offs that a classifier has to suffer on either the source training distribution or the induced target distribution. We provide further instantiated analysis for two popular domain adaptation settings with covariate shift and target shift.\n\n1\n\nINTRODUCTION\n\nDecision makers are increasingly required to be transparent on their decision making to offer the “right to explanation” (Goodman & Flaxman, 2017; Selbst & Powles, 2018; Ustun et al., 2019) 1. Being transparent also invites potential adaptations from the population, leading to potential shifts. We are motivated by settings where the deployed machine learning models interact with human agents, which will ultimately face data distributions that reflect how human agents respond to the models. For instance, when a model is used to decide loan applications, candidates may adapt their features based on the model specification in order to maximize their chances of approval; thus the loan decision classifier observes a data distribution caused by its own deployment (e.g., see Figure 1 for a demonstration). Similar observations can be articulated for application in insurance sector (i.e. developing policy s.t. customers’ behaviors might adapt to lower premium (Haghtalab et al., 2020)), education sector (i.e. developing courses when students are less incentivized to cheat (Kleinberg & Raghavan, 2020)) and so on.\n\nFEATURE\n\nIncome\n\nEducation Level\n\nDebt\n\nSavings\n\nWEIGHT ORIGINAL VALUE\n\nADAPTED VALUE\n\n2\n\n3\n\n-10\n\n5\n\n$ 6,000\n\nCollege\n\n$40,000\n\n$20,000\n\n−→\n\n−→\n\n−→\n\n−→\n\n$ 6,000\n\nCollege\n\n$20,000\n\n$0\n\nFigure 1: An example of an agent who originally has both savings and debt, observes that the classifier penalizes debt (weight -10) more than it rewards savings (weight +5), and concludes that their most efficient adaptation is to use their savings to pay down their debt.\n\nThis paper investigates model transferability when the underlying distribution shift is induced by the deployed model. What we would like to have is some guarantee on the transferability of a classifier —\n\n1See Appendix A.1 for more detailed discussions.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthat is, how training on the available source distribution domain induced risk, defined as the error a model incurs on the distribution induced by itself:\n\nS translates to performance on the induced (h), which depends on the model h being deployed. A key concept in our setting is the\n\nD\n\nD\n\nInduced Risk : ErrD(h)(h) := PD(h)(h(X)\n\n= Y )\n\n(1)\n\nMost relevant to the above formulation is the strategic classification literature (Hardt et al., 2016a; Chen et al., 2020b). In this literature, agents are modeled as rational utility maximizers and game theoretical solutions were proposed to characterize the induced risk. However, our results are motivated by the following challenges in more general scenarios:\n\n•\n\n•\n\nModeling assumptions being restrictive In many practical situations, it is often hard to faithfully characterize agents’ utilities. Furthermore, agents might not be fully rational when they response. All the uncertainties can lead to a far more complicated distribution change in (X, Y ), as compared to often-made assumptions that agents only change X but not Y (Chen et al., 2020b).\n\nLack of access to response data Another relevant literature to our work is performative prediction (h) or (Perdomo et al., 2020). In performative prediction, one would often require knowing (h) through repeated experiments. We posit that machine learning having samples observed from practitioners may only have access to data from the source distribution during training, and although they anticipate changes in the population due to human agents’ responses, they cannot observe this new distribution until the model is actually deployed.\n\nD\n\nD\n\n•\n\nRetraining being costly Even when samples from the induced data distribution are available, retraining the model from scratch may be impractical due to computational constraints.\n\nThe above observations motivate us to understand the transferability of a model trained on the source data to the domain induced by the deployment of itself. We study several fundamental questions:\n\n•\n\n•\n\n•\n\n•\n\n⇒\n\nInduced risk For a given model h, how different is ErrD(h)(h), the error on the\n\nSource risk distribution induced by h, from ErrDS (h) := PDS (h(X) Induced risk induced distribution, than minh(cid:48) ErrD(h(cid:48))(h(cid:48)), the minimum achievable induced error? Induced risk of source optimal case of the above, how does ErrD(h∗ source distribution h∗\n\nMinimum induced risk Of particular interest, and as a special S )(h∗ S), the induced error of the optimal model trained on the\n\nMinimum induced risk How much higher is ErrD(h)(h), the error on the\n\nS := arg minh ErrDS (h), compare to h∗\n\nT := arg minh ErrD(h)(h)?\n\n= Y ), the error on the source?\n\n⇒\n\n⇒\n\nLower bound for learning tradeoffs What is the minimum error a model must incur on either the source distribution ErrDS (h) or its induced distribution ErrD(h)(h)?\n\nFor the first three questions, we prove upper bounds on the additional error incurred when a model trained on a source distribution is transferred over to its induced domain. We also provide lower bounds for the trade-offs a classifier has to suffer on either the source training distribution or the induced target distribution. We then show how to specialize our results to two popular domain adaptation settings: covariate shift (Shimodaira, 2000; Zadrozny, 2004; Sugiyama et al., 2007; 2008; Zhang et al., 2013b) and target shift (Lipton et al., 2018; Guo et al., 2020; Zhang et al., 2013b). All omitted proofs can be found in the Appendix.\n\n1.1 RELATED WORKS\n\nMost relevant to us are three topics: strategic classification (Hardt et al., 2016a; Chen et al., 2020b; Dekel et al., 2010; Dong et al., 2018; Chen et al., 2020a; Miller et al., 2020; Kleinberg & Raghavan, 2020), a recently proposed notion of performative prediction (Perdomo et al., 2020; Mendler-D ̈unner et al., 2020), and domain adaptation (Jiang, 2008; Ben-David et al., 2010; Sugiyama et al., 2008; Zhang et al., 2019; Kang et al., 2019; Zhang et al., 2020).\n\nHardt et al. (2016a) pioneered the formalization of strategic behavior in classification based on a sequential two-player game between agents and classifiers. Subsequently, Chen et al. (2020b) addressed the question of repeatedly learning linear classifiers against agents who are strategically trying to game the deployed classifiers. Most of the existing literature focuses on finding the optimal classifier by assuming fully rational agents (and by characterizing the equilibrium response). In contrast, we do not make these assumptions and primarily study the transferability when only having knowledge of source data.\n\n2\n\n(cid:54) (cid:54) Under review as a conference paper at ICLR 2023\n\nOur result was inspired by the transferability results in domain adaptations (Ben-David et al., 2010; Crammer et al., 2008; David et al., 2010). Later works examined specific domain adaptation models, such as covariate shift (Shimodaira, 2000; Zadrozny, 2004; Gretton et al., 2009; Sugiyama et al., 2008; Zhang et al., 2013b;a) and target/label shift (Lipton et al., 2018; Azizzadenesheli et al., 2019). A commonly established solution is to perform reweighted training on the source data, and robust and efficient solutions have been developed to estimate the weights accurately (Sugiyama et al., 2008; Zhang et al., 2013b;a; Lipton et al., 2018; Guo et al., 2020).\n\nOur work, at the first sight, looks similar to several other area of studies. For instance, the notion of observing an “induced distribution” resembles similarity to the adversarial machine learning literature (Lowd & Meek, 2005; Huang et al., 2011; Vorobeychik & Kantarcioglu, 2018). One of the major differences between us and adversarial machine learning is the true label Y stays the same for the attacked feature while in our paper, both X and Y might change in the adapted distribution (h). In Appendix A.2, we provide detailed comparisons with some areas in domain adaptations, including domain generalization, adversarial attack and test-time adaptation. In particular, similar to domain generalization, one of the biggest challenge for our setting is the lack of access to data from the target distribution during training.\n\nD\n\n2 FORMULATION\n\n{\n\n}\n\nxi, yi\n\ni=1 is drawn from a source distribution\n\nSuppose we are learning a parametric model h for a binary classification problem. Its training N\ndata set S := .\n} However, h will then be deployed in a setting where the samples come from a test or target distribution S. Therefore instead of minimizing the prediction error = Y ), the goal is to find h∗ that minimizes = Y ). This is often referred to as the domain adaptation problem, where T is assumed to be independent of the model h being deployed.\n\nD on the source distribution ErrDS (h) := PDS (h(X) ErrDT (h) := PDT (h(X) typically, the transition from\n\nT that can differ substantially from\n\nS, where xi\n\nRd and yi\n\n1, +1\n\n∈ {−\n\n∈ H\n\nS to\n\nD\n\nD\n\n∈\n\nD\n\nD\n\nWe consider a setting in which the distribution shift depends on h, or is thought of as being induced by h. We will use\n\n(h) to denote the induced domain by h:\n\nD\n\nS\n\nencounters model h\n\n(h)\n\n→ D\n\nD Strictly speaking, the induced distribution is a function of both by of we will further instantiate\n\nS(h). To ease the notation, we will stick with S. For now, we do not restrict the dependency of\n\nD D\n\n→\n\nD\n\nD\n\n(h) under specific domain adaptation settings.\n\nD\n\nS and h and should be better denoted (h), but we shall keep in mind of its dependency and h, but later in Section 4 and 5\n\n(h) of\n\nD\n\nThe challenge in the above setting is that when training h, the learner needs to carry the thoughts that (h) should be the distribution it will be evaluated on and that the training cares about. Formally, we\n\nD define the induced risk of a classifier h as the 0-1 error on the distribution h induces:\n\nD\n\nInduced risk :\n\nErrD(h)(h) := PD(h)(h(X)\n\n= Y )\n\n(2)\n\nDenote by h∗ when the loss may not be the 0-1 loss, we define the induced (cid:96)-risk as\n\nT := arg minh∈H ErrD(h)(h) the classifier with minimum induced risk. More generally,\n\nInduced (cid:96)-risk :\n\nErr(cid:96),D(h)(h) := Ez∼D(h)[(cid:96)(h; z)]\n\nThe induced risks will be the primary quantities that we are interested in minimizing. The following additional notation will also be helpful:\n\n:\n\nD\n\nD\n\nDY |S := PDS (Y = y).\n\nDistributions of Y on a distribution PD(h)(Y = y), Distribution of h on a distribution PD(h)(h(X) = y), Marginal distribution of X for a distribution PD(h)(X = x), Total variation distance (Ali & Silvey, 1966): dTV(\n\nDh|S := PDS (h(X) = y).\n\nDX|S := PDS (X = x)3.\n\nD\n\nD\n\nD\n\nD\n\n:\n\n:\n\n•\n\n•\n\n•\n\n•\n\nY := PD(Y = y)2, and in particular\n\nh := PD(h(X) = y), and in particular\n\nX := PD(X = x), and in particular\n\n,\n\nD\n\n(cid:48)) := supO|\n\nD\n\nPD(\n\n)\n\n−\n\nPD(cid:48)(\n\nO\n\n. )\n|\n\nO\n\nY (h) :=\n\nD\n\nh(h) :=\n\nD\n\nX (h) :=\n\nD\n\n2The “:=” defines the RHS as the probability measure function for the LHS. 3For continuous X, the probability measure shall be read as the density function.\n\n3\n\n(cid:54) (cid:54) (cid:54) Under review as a conference paper at ICLR 2023\n\n2.1 EXAMPLES OF DISTRIBUTION SHIFTS INDUCED BY MODEL DEPLOYMENT\n\nWe provide two example models to demonstrate the use cases for the distribution shift models described in our paper. We provide more details in Section 4.3 and Section 5.3.\n\nStrategic Classification An example of distribution shift is when decision subjects perform strategic response to a decision rule. It is well-known that when human agents are subject to a decision rule, they will adapt their features so as to get a favorable prediction outcome. In the literature of strategic classification, we say the human agents perform strategic adaptation (Hardt et al., 2016a).\n\nIt is natural to assume that the feature distribution before and after the human agents’ best response satisfies covariate shift: namely the feature distribution P(X) will change, but P(Y X), the mapping between Y and X, remain unchanged. Notice that this is different from the assumption made in the classic strategic classification setting Hardt et al. (2016a), which is to assume that the changes in the feature X does not change the underlying true qualification Y . In our paper, we assume that changes in feature X could potential lead to changes in the true qualification Y , and that the mapping between Y and X remains the same before and after the adaptation. This is a commonly assumption made in a recent line of work on incentivizing improvement behaviors from human agents(see, e.g. Chen et al. (2020a); Shavit et al. (2020)). We use Figure 2 (Left) as a demonstration of how distribution might shift for strategic response setting. In Section 4.3, we will use the strategic classification setup to verify our obtained results.\n\n|\n\nFigure 2: Example causal graph annotated to demonstrate covariate shift (Left) / target shift (Right) as a result of the deployment of h. Grey nodes indicate observable variables and transparent nodes are not observed at the training stage. Red arrow emphasises h induces changes of certain variables.\n\nReplicator Dynamics Replicator dynamics is a commonly used model to study the evolution of an adopted “strategy” in evolutionary game theory (Tuyls et al., 2006; Friedman & Sinervo, 2016; Taylor & Jonker, 1978; Raab & Liu, 2021). The core notion of it is the growth or decline of the population of each strategy depends on its “fitness”. Consider the label Y = as the strategy, and the following behavioral response model to capture the induced target shift:\n\n1, +1\n\n{−\n\n}\n\nPD(h)(Y = +1) PDS (Y = +1)\n\n=\n\nFitness(Y = +1) EDS [Fitness(Y )]\n\nIn short, the change of the Y = +1 population depends on how predicting Y = +1 “fits” a certain utility function. For instance, the “fitness” can take the form of the prediction accuracy of h for class +1: Fitness(Y = +1) := PDS (h(X) = +1 Y = +1) . Intuitively speaking, a higher “fitness” 1 or Y = +1). Therefore, describes more success of agents who adopted a certain strategy (Y = agents will imitate or replicate these successful peer agents by adopting the same strategy, resulting in an increase of the population (PD(h)(Y )). With assuming P(X Y ) stays unchanged, this instantiates one example of a specific induced target shift. We will specify the condition for target shift in Section 5. We use Figure 2 (Right) as a demonstrating of how distribution might shift for the replicator dynamic setting. In Section 5.3, we will use a detailed replicator dynamics model to further instantiate our results.\n\n−\n\n|\n\n|\n\n3 TRANSFERABILITY OF LEARNING TO INDUCED DOMAINS\n\nIn this section, we first provide upper bounds for the transfer error of a classifier h (that is, the difference between ErrD(h)(h) and ErrDS (h)), as well as between ErrD(h)(h) and ErrD(h∗ T ). We , that is, the minimum error a model h then provide lower bounds for max must incur on either the source distribution\n\n} S or the induced distribution\n\nErrDS (h), ErrD(h)(h)\n\nT )(h∗\n\n(h).\n\n{\n\nD\n\nD\n\n4\n\nX1X1X2X2X3X3YYh(X)h(X)X′ 1X′ 1X′ 2X′ 2X′ 3X′ 3Y′ Y′ YYX1X1 X3X3X2X2h(X)h(X)Y′ Y′ X′ 1X′ 1X′ 3X′ 3X′ 2X′ 2Under review as a conference paper at ICLR 2023\n\n3.1 UPPER BOUND We first investigate upper bounds for the transfer errors. We begin by showing generic bounds, and further instantiate the bound for specific domain adaptation settings in Section 4 and 5 . We begin with answering a central question in domain adaptation:\n\nHow does a model h trained on its training data set fare on the induced distribution\n\nTo that end, define the minimum and h-dependent combined error of two distributions\n\nD ErrD(cid:48)(h(cid:48)) + ErrD(h(cid:48)), ΛD→D(cid:48)(h) := ErrD(cid:48)(h) + ErrD(h)\n\nλD→D(cid:48) := min h(cid:48)∈H\n\n(h)?\n\nD and\n\n(cid:48) as:\n\nD\n\n-divergence as dH×H( .\nand The -divergence is celebrated measure proposed in the domain adaptation literature (Ben-David et al., 2010) which will be useful for bounding the difference in errors of two classifiers. Repeating classical arguments from Ben-David et al. (2010), we can easily prove the following:\n\n(cid:48)) = 2 suph,h(cid:48)∈H |\n\n= h(cid:48)(X))\n\n= h(cid:48)(X))\n\nPD(cid:48)(h(X)\n\nPD(h(X)\n\nH H\n\n−\n\nD\n\nD\n\n,\n\n|\n\nTheorem 3.1 (Source risk ⇒\nupper bounded by: ErrD(h)(h)\n\nInduced risk). The difference between ErrD(h)(h) and ErrDS (h) is 2 dH×H(\n\nErrDS (h) + λDS→D(h) + 1\n\n(h)).\n\nS,\n\n≤\n\nD\n\nD\n\nThe transferability of a model h between ErrD(h)(h) and ErrDS (h) looks precisely the same as in the classical domain adaptation setting (Ben-David et al., 2010). Nonetheless, an arguably more interesting quantity in our setting to understand is the difference between the induced error of a given model h and the error induced by the optimal model h∗ T ). We get the following bound, which differs from the one in Theorem 3.1:\n\nT : ErrD(h)(h)\n\nErrD(h∗\n\nT )(h∗\n\n−\n\nTheorem 3.2 (Induced risk\n\nMinimum induced risk). The difference between ErrD(h)(h) and\n\nErrD(h∗ 1\n\nT )(h∗\n\ndH×H(\n\n⇒ T ) is upper bounded by: ErrD(h)(h) (h∗\n\n(h)).\n\nT ),\n\nErrD(h∗\n\nT )(h∗\n\nT )\n\n−\n\nλD(h)→D(h∗ T\n\n)+ΛD(h)→D(h∗ T\n\n2\n\n)(h)\n\n+\n\n≤\n\nD\n\nD\n\n2 · The above theorem informs us that the induced transfer error is bounded by the “average” achievable error on both distributions divergence between the two distributions. Reflecting on the difference between the bounds of Theorem 3.1 and Theorem 3.2, we see that the primary change is replacing the minimum achievable error λ with the average of λ and Λ.\n\nT ), as well as the\n\n(h) and\n\nH × H\n\n(h∗\n\nD\n\nD\n\n3.2 LOWER BOUND Now we provide a lower bound on the induced transfer error. We particularly want to show that at least one of the two errors ErrDS (h), and ErrD(h)(h), must be lower-bounded by a certain quantity. Theorem 3.3 (Lower bound for learning tradeoffs ). Any model h must incur the following error on either the source or induced distribution: max dTV(DY |S ,DY (h))−dTV(Dh|S ,Dh(h)) 2\n\nErrDS (h), ErrD(h)(h)\n\n} ≥\n\n{\n\n.\n\nThe proof leverages the triangle inequality of dTV. This bound is dependent on h; however, by the data processing inequality of dTV (and f -divergence functions in general) (Liese & Vajda, 2006), we have dTV(\n\nX (h)). Applying this to Theorem 3.3 yields:\n\nh(h))\n\ndTV(\n\nDh|S,\n\nD\n\n≤\n\nDX|S,\n\nD ErrDS (h), ErrD(h)(h)\n\n{\n\n} ≥\n\ndTV(DY |S ,DY (h))−dTV(DX|S ,DX (h)) 2\n\n.\n\nCorollary 3.4. For any model h, max\n\n3.3 HOW TO USE OUR BOUNDS\n\nThe upper and lower bounds we derived in the previous sections (Theorem 3.2 and Theorem 3.3) (h) induced depend on the following two quantities either explicitly or implicitly: 1) the distribution by the deployment of the model h in question, and 2) the optimal target classifier h∗ T as well as the T ) it induces. The bounds may therefore seem to be of only theoretical interest, since distribution in reality we generally cannot compute T . Thus in general it is unclear how to compute the value of these bounds. Nevertheless, our bounds can still be useful and informative in the following ways:\n\n(h) without actual deployment, let alone compute h∗\n\n(h∗\n\nD\n\nD\n\nD\n\nGeneral modeling framework with flexible hypothetical shifting models The bounds can be evaluated if the decision maker has a particular shift model in mind, which specifies how the population would adapt to a model. A common special case is when the decision maker posits an\n\n5\n\n(cid:54) (cid:54) Under review as a conference paper at ICLR 2023\n\nindividual-level agent response model (e.g. the strategic agent (Hardt et al., 2016a) - we demonstrate how to evaluate in Section 4.3). In these cases, the H-divergence can be consistently estimated from finite samples of the population (Wang et al., 2005), allowing the decision maker to estimate the performance gap of a given h without deploying it. The general bounds provided can thus be viewed as a framework by which specialized, computationally tractable bounds can be derived.\n\nEstimate the optimal target classifier h∗ decision maker has access to a set of imperfect models ̃h1, ̃h2 range of possible shifted distribution distribution hT ∈ H this predicted set 4:\n\nT from a set of imperfect models Secondly, when the ̃ht H T that will predict a ( ̃h1), T and a range of possibly optimal target T can be further instantiated by calculating the worst case in\n\nD T , the bounds on h∗\n\n( ̃ht)\n\n· · · D\n\n∈ D\n\n· · ·\n\n∈\n\nErrD(h)(h)\n\n−\n\nErrD(h∗\n\nT )(h∗\n\nT ) (cid:46)\n\nmax D(cid:48)∈DT ,h(cid:48)∈HT\n\nUpperBound(\n\nmax\n\n{\n\nErrDS (h), ErrD(h∗\n\nT )(h∗\n\nT )\n\n(cid:38)\n\n}\n\nmin D(cid:48)∈DT ,h(cid:48)∈HT\n\nLowerBound(\n\n(cid:48), h(cid:48)),\n\n(cid:48), h(cid:48)).\n\nD\n\nD\n\nIn addition, the challenge we are facing in this paper also shed lights on the danger of directly applying existing standard domain adaptation techniques when the shifting is caused by the deployment of the classifier itself, since the bound will depend on the resulting distribution as well. We add discussions on the tightness of our theoretical bounds in Appendix G.\n\n4 COVARIATE SHIFT\n\nIn this section, we focus on a particular domain adaptation setting known as covariate shift, in which the distribution of features changes, but the distribution of labels conditioned on features does not:\n\nPD(h)(Y = y\n\n|\n\nX = x) = PDS (Y = y\n\n|\n\nX = x), PD(h)(X = x)\n\n= PDS (X = x)\n\n(3)\n\nThus with covariate shift, we have\n\nPD(h)(X = x, Y = y) =PD(h)(Y = y\n\nX = x) |\n\n·\n\nPD(h)(X = x) = PDS (Y = y\n\nX = x) |\n\n·\n\nPD(h)(X = x)\n\nLet ωx(h) := tion induced by h at instance x. Then for any loss function (cid:96) we have\n\nPD(h)(X=x) PDS (X=x) be the importance weight at x, which characterizes the amount of adapta-\n\nProposition 4.1 (Expected Loss on\n\n(h)). ED(h)[(cid:96)(h; X, Y )] = EDS [ωx(h)\n\nD\n\n(cid:96)(h; x, y)].\n\n·\n\nThe above derivation was not new and offered the basis for performing importance reweighting when learning under coviarate shift (Sugiyama et al., 2008). The particular form informs us that ωx(h) S and h, and is critical for (h) and encodes its dependency of both controls the generation of deriving our results below.\n\nD\n\nD\n\n4.1 UPPER BOUND\n\nWe now derive an upper bound for transferability under covariate shift. We will focus particularly on S := arg minh∈H ErrS(h). the optimal model trained on the source data D\nRecall that the classifier with minimum induced risk is denoted as h∗ T := arg minh∈H ErrD(h)(h). We can upper bound the difference between h∗ S and h∗ Theorem 4.2 (Suboptimality of h∗\n\nS, which we denote as h∗\n\nS). Let X be distributed according to\n\nT as follows:\n\nS. We have:\n\nD\n\nErrD(h∗\n\nS )(h∗ S)\n\n−\n\nErrD(h∗\n\nT )(h∗\n\nT )\n\n(cid:113)\n\nErrDS (h∗\n\nT )\n\n·\n\n≤\n\n(cid:18)(cid:113)\n\nVar(ωX (h∗\n\nS)) +\n\n(cid:113)\n\n(cid:19)\n\nVar(ωX (h∗\n\nT ))\n\n.\n\nThis result can be interpreted as follows: h∗ T incurs an irreducible amount of error on the source data set, represented by (cid:112)ErrDS (h∗ T is at its maximum when the two classifiers induce adaptations in “opposite” directions; this is represented by the sum of the standard deviations of their importance weights, (cid:112)Var(ωX (h∗ T )).\n\nT ). Moreover, the difference in error between h∗\n\nS)) + (cid:112)Var(ωX (h∗\n\nS and h∗\n\n4UpperBound and LowerBound are the RHS expressions in Theorem 3.3 and Theorem 3.2.\n\n6\n\n(cid:54) Under review as a conference paper at ICLR 2023\n\n4.2 LOWER BOUND\n\nRecall from Theorem 3.3, for the general setting, it is unclear whether the lower bound is strictly positive or not. In this section, we provide further understanding for when the lower bound dTV(DY |S ,DY (h))−dTV(Dh|S ,Dh(h)) 2\n\nis indeed positive under covariate shift. Under several assump-\n\ntions, our previously provided lower bound in Theorem 3.3 is strictly positive with covariate shift.\n\nAssumption 4.3.\n\nwhere X+(h) =\n\nEX∈X+(h),Y =+1[1 |\nx : ωx(h)\n\n1\n\n−\n\n{\n\n≥\n\n}\n\nand X−(h) =\n\nx : ωx(h) < 1\n\n{\n\n.\n\n}\n\nωX (h)]\n\nEX∈X−(h),Y =+1[1\n\n| ≥ |\n\nωX (h)] |\n\n.\n\n−\n\nThis assumption states that increased ωx(h) value points are more likely to have positive labels. ωX (h)] Assumption 4.4.\n\nEX∈X−(h),h(X)=+1[1\n\nEX∈X+(h),h(X)=+1[1 |\n\n−\n\nωX (h)] .\n|\n\n−\n\nThis assumption states that increased ωx(h) value points are more likely to be classified as positive. Assumption 4.5. Cov(cid:0)PDS (Y = +1 X = x) |\nThis assumption is stating that for a classifier h, within all h(X) = +1 or h(X) = PD(Y = +1 |\nTheorem 4.6. Assuming 4.3 - 4.5, the following lower bound is strictly positive for covariate shift:\n\nX = x), ωx(h)(cid:1) > 0. PDS (h(x) = +1 |\n\nX = x) associates with a higher ωx(h).\n\n1, a higher\n\n−\n\n−\n\n| ≥ |\n\nmax\n\nErrDS (h), ErrD(h)(h)\n\n{\n\ndTV(\n\nY |S,\n\nD\n\nY (h))\n\nD\n\n− 2\n\ndTV(\n\nh|S,\n\nD\n\nD\n\nh(h))\n\n> 0.\n\n} ≥\n\n4.3 EXAMPLE USING STRATEGIC CLASSIFICATION\n\n·\n\n∈\n\n−\n\nτh]\n\n1[x\n\nh(x)\n\n∈ {−\n\n1, +1\n\nRd and a binary true qualification y(x)\n\nAs introduced in Section 2.1, we consider a setting caused by strategic response in which agents are classified by and adapt to a binary threshold classifier. In particular, each agent is associated with a d dimensional continuous feature x , where y(x) is a function of the feature vector x. Consistent with the literature in strategic classification (Hardt et al., 2016a), a simple case where after seeing the threshold binary decision rule h(x) = 1, the agents will best response to it by maximizing the following utility function: 2\n≥ u(x, x(cid:48)) = h(x(cid:48)) c(x, x(cid:48)), where c(x, x(cid:48)) is the cost function for decision subjects to modify their feature from x to x(cid:48). We assume all agents are rational utility maximizers: they will only attempt to change their features when the benefit of manipulation is greater than the cost (i.e. when c(x, x(cid:48)) 2) and agent will not change their feature if they are already accepted (i.e. h(x) = +1). For a given threshold τh and manipulation budget B, the theoretical best response of an agent with original feature x is: ∆(x) = arg maxx(cid:48) u(x, x(cid:48)) s.t. c(x, x(cid:48)) B. To make the problem tractable and meaningful, we further specify the following setups: Setup 1. (Initial Feature) Agents’ initial features are uniformly distributed between [0, 1] Setup 2. (Agent’s Cost Function) The cost of changing from x to x(cid:48) is proportional to the distance between them: c(x, x(cid:48)) =\n\nR1.\n\nx(cid:48)\n\n≤\n\n≤\n\n−\n\n−\n\n∈\n\nx\n\n}\n\n(cid:107)\n\n−\n\n. (cid:107)\n\nB, τh) will attempt to change Setup 2 implies that only agents whose features are in between [τh their feature. We also assume that feature updates are probabilistic, such that agents with features closer to the decision boundary τh have a greater chance of updating their feature and each updated feature x(cid:48) is sampled from a uniform distribution depending on τh, B, and x (see Setup 3 & 4): Setup 3. (Agent’s Success Manipulation Probability) For agents who attempt to update their features, the probability of a successful feature update is P(X (cid:48) Setup 4 (Adapted Feature’s Distribution). An agent’s updated feature x(cid:48), given original x, manipulation budget B, and classification boundary τh, is sampled as X (cid:48)\n\nUnif(τh, τh +\n\n= X) = 1\n\n|x−τh| B .\n\n−\n\n−\n\nB\n\nx\n\n∼\n\n|\n\n−\n\n). |\n\nSetup 4 aims to capture the fact that even though agent targets to change their feature to the decision boundary τh (i.e. the least cost action to get a favorable prediction outcome), they might end up reaching to a feature that is beyond the decision boundary. With the above setups, we can specify the bound in Theorem 4.2 for the strategic response setting as follows:\n\nProposition 4.7 (Strategic Response Setting). ErrD(h∗\n\nS )(h∗ S)\n\n−\n\nErrD(h∗\n\nT )(h∗\n\nT )\n\n(cid:113) 2B\n\n3 ErrDS (h∗\n\nT ).\n\n≤\n\nWe can see that the upper bound for strategic response depends on the manipulation budget B, and the error the ideal classifier made on the source distribution ErrDS (h∗ T ). This aligns with our intuition\n\n7\n\n(cid:54) Under review as a conference paper at ICLR 2023\n\nthat the smaller manipulation budget is, the less agents will change their features, thus leading to a T ). This bound also allows us tighter upper bound on the difference between Errh∗ (h) and h, since we to bound this quantity even without the knowledge of the mapping between T ) from the source distribution and an estimated optimal classifier h∗ can directly compute ErrDS (h∗ T .\n\nS) and Errh∗\n\nT (h∗\n\nS (h∗\n\nD\n\n5 TARGET SHIFT\n\nWe consider another popular domain adaptation setting known as target shift, in which the distribution of labels changes, but the distribution of features conditioned on the label remains the same:\n\nPD(h)(X = x\n\nY = y) = PDS (X = x |\n\nY = y), PD(h)(Y = y) |\n\n= PDS (Y = y)\n\n(4)\n\nFor binary classification, let p(h) := PD(h)(Y = +1), and PD(h)(Y = p(h) encodes the induced adaptation from ED(h)[(cid:96)(h; X, Y )] =p(h) =p(h)\n\nD ED(h)[(cid:96)(h; X, Y ) ED(h)[(cid:96)(h; X, Y ) Y = +1] + (1 Y = |\n| ·\nEDS [(cid:96)(h; X, Y ) EDS [(cid:96)(h; X, Y ) Y = +1] + (1 Y = |\n| We will adopt the following shorthands: Err+(h) := EDS [(cid:96)(h; X, Y ) Y = +1], Err−(h) := |\nEDS [(cid:96)(h; X, Y ) 1]. Note that Err+(h), Err−(h) are both defined on the conditional source |\ndistribution, which is invariant under the target shift assumption.\n\nS and h. Then we have for any proper loss function (cid:96):\n\n− p(h))\n\np(h). Again,\n\n1) = 1\n\np(h))\n\nY =\n\n−\n\n−\n\n−\n\n−\n\n−\n\n−\n\n1]\n\n1]\n\n·\n\n·\n\n·\n\n5.1 UPPER BOUND\n\nWe again upper bound the transferability of h∗ label distribution on (PDS (X = x Theorem 5.1. For target shift, the difference between ErrD(h∗\n\nD 1)). Let p := PDS (Y = +1).\n\nS (PDS (X = x\n\nY = +1)) and\n\nY =\n\n−\n\nD\n\n|\n\n|\n\nS under target shift. Denote by\n\n− the negative label distribution on\n\n+ the positive S\n\nD\n\nD\n\nErrD(h∗\n\nS )(h∗ S)\n\n−\n\nErrD(h∗\n\nT )(h∗\n\nT )\n\np(h∗\n\nS)\n\np(h∗\n\nT )\n\n|\n\n−\n\n≤ |\n\n+ (1 + p)\n\n·\n\n(dTV(\n\nD\n\nS )(h∗\n\nS) and ErrD(h∗ +(h∗\n\n+(h∗\n\nT )(h∗ T )) + dTV(\n\nT ) bounds as: −(h∗\n\nS),\n\nS),\n\nD\n\nD\n\n−(h∗\n\nT )) .\n\nD\n\nThe above upper bound consists of two components. The first quantity captures the difference T ). The second quantity characterizes the between the two induced distributions difference between the two classifiers h∗\n\nS) and T on the source distribution.\n\n(h∗ D\nS, h∗\n\n(h∗\n\nD\n\n5.2 LOWER BOUND\n\nNow we discuss lower bounds. Denote by TPRS(h) and FPRS(h) the true positive and false positive rates of h on the source distribution Theorem 5.2. For target shift, any model h must incur the following error on either\n\nS. We prove the following:\n\nD\n\n(h):\n\nS or\n\nD\n\nD\n\nmax\n\nErrDS (h), ErrD(h)(h)\n\n{\n\np |\n\n−\n\np(h)\n\n| ·\n\n(1\n\n− |\n\nTPRS(h)\n\n2\n\nFPRS(h) |\n\n)\n\n.\n\n−\n\n} ≥\n\nD\n\nDh|S,\n\nh(h)) under the assumption of target shift. Since\n\nY (h)), The proof extends the bound of Theorem 3.3 by further explicating each of dTV( D\ndTV( < 0 unless we FPRS(h) |\nhave a trivial classifier that has either TPRS(h) = 1, FPRS(h) = 0 or TPRS(h) = 0, FPRS(h) = 1, the lower bound is strictly positive. Taking a closer look, the lower bound is determined linearly p(h). The difference is further determined by the by how much the label distribution shifts: p performance of h on the source distribution through 1 FPRS(h) . For instance, when |\nTPRS(h) > FPRS(h), the quality becomes FNRS(h) + FPRS(h), that is the more error h makes, the larger the lower bound will be.\n\nTPRS(h) |\n\nDY |S,\n\nTPRS(h)\n\n− |\n\n−\n\n−\n\n−\n\n5.3 EXAMPLE USING REPLICATOR DYNAMICS\n\nLet us instantiate the discussion using a specific fitness function for the replicator dynamics model (Section 2.1), which is the prediction accuracy of h for class y: Fitness(Y = y) := PDS (h(X) = y\n\nY = y)\n\n(5)\n\n|\n\nPrDS (h(X)=+1|Y =+1) 1−ErrDS (h)\n\n. Plugging\n\nThen we have E [Fitness(Y )] = 1 the result back to our Theorem 5.1 we have\n\n−\n\nErrDS (h), and\n\np(h)\n\nPDS (Y =+1) =\n\n8\n\n(cid:54) Under review as a conference paper at ICLR 2023\n\nFigure 3: Diff := ErrD(h∗ T )}, UB := upper bound specified in Theorem 4.2, and LB := lower bound specified in Theorem 4.6. For each time step K = k, we compute and deploy the source optimal classifier h∗ S and update the credit score for each individual according to the received decision as the new reality for time step K = k + 1. Details of the data generation is again deferred to Appendix C.\n\nT ), Max := max{ErrDS (h∗\n\nS ) − ErrD(h∗\n\nT ), ErrD(h∗\n\n)(h∗\n\n)(h∗\n\n)(h∗\n\nS\n\nT\n\nT\n\nProposition 5.3. Under the replicator dynamics model in Eqn. (21),\n\nω(h∗ |\n\nS)\n\n−\n\nω(h∗\n\nT )\n\n| ≤\n\nPDS (Y = +1)\n\n·\n\nErrDS (h∗ S) |\n(1\n\nErrDS (h∗ ErrDS (h∗\n\nT ) S))\n\n− −\n\n| · | (1 ·\n\n−\n\nbounds as:\n\nω(h∗\n\nω(h∗ S) |\nTPRS(h∗\n\n− S) −\nErrDS (h∗\n\nT ) |\nTPRS(h∗ T ))\n\nT )\n\n.\n\n|\n\nThat is, the difference between ErrD(h∗ between the two classifiers’ performances on the source data evaluate the possible error transferability using the source data only.\n\nS) and ErrD(h∗\n\nD\n\nT )(h∗\n\nS )(h∗\n\nT ) is further dependent on the difference S. This offers an opportunity to\n\n6 EXPERIMENTS\n\nWe perform synthetic experiments using real-world data to demonstrate our bounds. In particular, we use the FICO credit score data set (Board of Governors of the Federal Reserve System (US), 2007) which contains more than 300k records of TransUnion credit score of clients from different demographic groups. For our experiment on the preprocessed FICO data set (Hardt et al., 2016b), we convert the cumulative distribution function (CDF) of TransRisk score among different groups into group-wise credit score densities, from which we generate a balanced sample to represent a population where groups have equal representations. We demonstrate the application of our results in a series of resource allocations. We consider the hypothesis class of threshold classifiers and treat the classification outcome as the decision received by individuals.\n\nFor each time step K = k, we compute h∗ S, the statistical optimal classifier on the source distribution (i.e., the current reality for step K = k), and update the credit score for each individual according to the received decision as the new reality for time step K = k + 1. Details of the data generation is again deferred to Appendix C. We report our results in Figure 3. We do observe positive gaps ErrD(h∗ S. The gaps are well bounded by the theoretical upper bound (UB). Our lower bounds (LB) do return meaningful positive gaps, demonstrating the trade-offs that a classifier has to suffer on either the source distribution or the induced target distribution.\n\nT ), indicating the suboptimality of training on\n\nS )(h∗ S)\n\nErrD(h∗\n\nT )(h∗\n\n−\n\nD\n\nChallenges in Minimizing Induced Risk and Concluding Remarks We presented a sequence of model transferability results for settings where agents will respond to a deployed model. The response leads to an induced distribution that the learner would not know before deploying the model. Our results cover for both a general response setting and for specific ones (covariate shift and target shift). Looking forward to solving the induced risk minimization, the literature of domain adaptation has provided us solutions to minimize the risk on the target distribution via a nicely developed set of results (Sugiyama et al., 2008; 2007; Shimodaira, 2000). This allows us to extend the solutions to minimize the induced risk too. Nonetheless we will highlight additional computational challenges. Let’s use the covariate shift setting. The scenario for target shift is similar. For covariate shift, recall that earlier we derived the following fact:\n\n(Importance Reweighting) : ED(h)[(cid:96)(h; X, Y )] = ED[ωx(h)\n\n(cid:96)(h; x, y)] .\n\n·\n\n(6)\n\nThis formula informs us that a promising solution that uses ωx(h) to perform reweighted ERM. There are two primary challenges when carrying out optimization of the above objective. Of course, the primary challenge that stands in the way is how do we know ωx(h). When one could build models to (h) and then ωx(h) (e.g., using the replicator dynamics model as we introduced predict the response earlier), one could rework the above loss and apply standard gradient descent approaches. We provide a concrete example and discussion in Appendix E. Without making any assumptions on the mapping (h), one can only potentially rely on the bandit feedbacks from the decision subjects between h and D\nto estimate the influence of h on (h) - we also laid out a possibility in Appendix E too. It can also be inferred from Eqn. (6) that the second challenge is the induced risk minimization might not even be convex - due to the limit of space, we defer the detailed discussion again to the Appendix D.\n\nD\n\nD\n\n9\n\n012345K10−210−1ValueMaxLB012345K10−210−1ValueDiffUBUnder review as a conference paper at ICLR 2023\n\n7 ETHICAL STATEMENT\n\nThe primary goal of our study is to put human in the center when considering domain shift. The development of the paper is fully aware of any fairness concerns and we expect positive societal impact. Unawareness of the potential distribution shift might lead to unintended consequence when training a machine learning model. One goal of this paper is to raise awareness of this issue for a safe deployment of machine learning methods in high-stake societal applications.\n\nA subset of our results are developed under assumptions (e.g., Theorem 4.6). Therefore we want to caution readers of potential misinterpretation of applicability of the reported theoretical guarantees. Our contributions are mostly theoretical and our experiments use synthetic agent models to simulate distribution shift. A future direction is to collect real human experiment data to support the findings. Our paper ends with discussing the challenges in learning under the responding distribution and other objectives that might arise.\n\nWe believe this is a promising research direction for the machine learning community, both as a unaddressed technical problem and a stepstone for putting human in the center when training a machine learning model.\n\n8 REPRODUCIBILITY STATEMENT\n\nWe provide the following checklist for the purpose of reproducibility:\n\n1. Generals:\n\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s\n\ncontributions and scope? [Yes]\n\n(b) Did you describe the limitations of your work? [Yes] We have stated our assumptions and limitations of the results. We also discussed the limitations in the conclusion. (c) Did you discuss any potential negative societal impacts of your work? [Yes] One of our work’s goals is to raise awareness of this issue for a safe deployment of machine learning methods in high-stake societal applications. We discuss the potential misinterpretation of our results in conclusion.\n\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\n\nthem? [Yes]\n\n2. If you are including theoretical results...\n\n(a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes] We present the\n\ncomplete proofs in the appendix.\n\n3. If you ran experiments...\n\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We included experiment details in the appendix and submitted the implementation in the supplementary materials.\n\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\n\nwere chosen)? [Yes]\n\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] In our controlled experiment, we do not tune parameters and do not observe a significant variance in the results.\n\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\n\nof GPUs, internal cluster, or cloud provider)? [Yes]\n\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n\n(a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] (c) Did you include any new assets either in the supplemental material or as a URL? [No]\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\n(d) Did you discuss whether and how consent was obtained from people whose data you’re\n\nusing/curating? [N/A]\n\n(e) Did you discuss whether the data you are using/curating contains personally identifiable\n\ninformation or offensive content? [N/A]\n\n5. If you used crowdsourcing or conducted research with human subjects...\n\n(a) Did you include the full text of instructions given to participants and screenshots, if\n\napplicable? [N/A]\n\n(b) Did you describe any potential participant risks, with links to Institutional Review\n\nBoard (IRB) approvals, if applicable? [N/A]\n\n(c) Did you include the estimated hourly wage paid to participants and the total amount\n\nspent on participant compensation? [N/A]\n\nREFERENCES\n\nSyed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distribution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28 (1):131–142, 1966.\n\nKamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning\n\nfor domain adaptation under label shifts. arXiv preprint arXiv:1903.09734, 2019.\n\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Vaughan.\n\nA theory of learning from different domains. Machine Learning, 79:151–175, 2010.\n\nBoard of Governors of the Federal Reserve System (US). Report to the congress on credit scoring and its effects on the availability and affordability of credit. Board of Governors of the Federal Reserve System, 2007.\n\nAnirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopad-\n\nhyay. Adversarial attacks and defences: A survey, 2018.\n\nYatong Chen, Jialu Wang, and Yang Liu. Strategic recourse in linear classification. arXiv preprint\n\narXiv:2011.00355, 2020a.\n\nYiling Chen, Yang Liu, and Chara Podimata. Learning strategy-aware linear classifiers, 2020b.\n\nKoby Crammer, Michael Kearns, and Jennifer Wortman. Learning from multiple sources. Journal of\n\nMachine Learning Research, 9(8), 2008.\n\nShai Ben David, Tyler Lu, Teresa Luu, and D ́avid P ́al. Impossibility theorems for domain adaptation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 129–136. JMLR Workshop and Conference Proceedings, 2010.\n\nOfer Dekel, Felix Fischer, and Ariel D. Procaccia. Incentive compatible regression learning. J.\n\nComput. Syst. Sci., 76(8):759–777, December 2010.\n\nJinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classification from revealed preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation, EC ’18, pp. 55–70, New York, NY, USA, 2018. Association for Computing Machinery.\n\nAbraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization\n\nin the bandit setting: gradient descent without a gradient. arXiv preprint cs/0408007, 2004.\n\nDaniel Friedman and Barry Sinervo. Evolutionary games in natural, social, and virtual worlds.\n\nOxford University Press, 2016.\n\nMingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Sch ̈olkopf. Domain adaptation with conditional transferable components. In International conference on machine learning, pp. 2839–2848. PMLR, 2016.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nBryce Goodman and Seth Flaxman. European union regulations on algorithmic decision-making and\n\na “right to explanation”. AI Magazine, 38(3):50–57, Oct 2017.\n\nArthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernhard Sch ̈olkopf. Covariate shift by kernel mean matching. Dataset shift in machine learning, 3(4):5, 2009.\n\nJiaxian Guo, Mingming Gong, Tongliang Liu, Kun Zhang, and Dacheng Tao. Ltf: A label transformation framework for correcting label shift. In International Conference on Machine Learning, pp. 3843–3853. PMLR, 2020.\n\nNika Haghtalab, Nicole Immorlica, Brendan Lucier, and Jack Z. Wang. Maximizing welfare with incentive-aware evaluation mechanisms. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 160–166. International Joint Conferences on Artificial Intelligence Organization, 2020. doi: 10.24963/ijcai. 2020/23. URL https://doi.org/10.24963/ijcai.2020/23.\n\nMoritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, pp. 111–122, New York, NY, USA, 2016a. Association for Computing Machinery.\n\nMoritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Advances\n\nin Neural Information Processing Systems, pp. 3315–3323, 2016b.\n\nLing Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and J Doug Tygar. Adversarial machine learning. In ACM Workshop on Security and Artificial Intelligence, pp. 43–58, 2011.\n\nJing Jiang. A literature survey on domain adaptation of statistical classifiers. URL: http://sifaka. cs.\n\nuiuc. edu/jiang4/domainadaptation/survey, 3:1–12, 2008.\n\nGuoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4893–4902, 2019.\n\nJon Kleinberg and Manish Raghavan. How do classifiers induce agents to invest effort strategically?\n\nACM Transactions on Economics and Computation (TEAC), 8(4):1–23, 2020.\n\nDa Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-\n\nlearning for domain generalization, 2017.\n\nFriedrich Liese and Igor Vajda. On divergences and informations in statistics and information theory.\n\nIEEE Transactions on Information Theory, 52(10):4394–4412, 2006.\n\nZachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box predictors. In International conference on machine learning, pp. 3122–3130. PMLR, 2018.\n\nYang Liu and Mingyan Liu. An online learning approach to improving the quality of crowd-sourcing.\n\nACM SIGMETRICS Performance Evaluation Review, 43(1):217–230, 2015.\n\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation\n\nwith residual transfer networks. arXiv preprint arXiv:1602.04433, 2016.\n\nDaniel Lowd and Christopher Meek. Adversarial learning. In ACM SIGKDD International Conference\n\non Knowledge Discovery in Data Mining, pp. 641–647, 2005.\n\nCelestine Mendler-D ̈unner, Juan Perdomo, Tijana Zrnic, and Moritz Hardt. Stochastic optimization for performative prediction. In Advances in Neural Information Processing Systems, pp. 4929–4939. Curran Associates, Inc., 2020.\n\nJohn Miller, Smitha Milli, and Moritz Hardt. Strategic classification is causal modeling in disguise.\n\nIn International Conference on Machine Learning, pp. 6917–6926. PMLR, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nKrikamol Muandet, David Balduzzi, and Bernhard Sch ̈olkopf. Domain generalization via invariant\n\nfeature representation, 2013.\n\nZachary Nado, Shreyas Padhy, D. Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift, 2021.\n\nNicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from\n\nphenomena to black-box attacks using adversarial samples, 2016.\n\nJuan Perdomo, Tijana Zrnic, Celestine Mendler-D ̈unner, and Moritz Hardt. Performative prediction.\n\nIn International Conference on Machine Learning, pp. 7599–7609. PMLR, 2020.\n\nReilly Raab and Yang Liu. Unintended selection: Persistent qualification rate disparities and\n\ninterventions. Advances in Neural Information Processing Systems, 34, 2021.\n\nAndrew Selbst and Julia Powles. “meaningful information” and the right to explanation. In Sorelle A. Friedler and Christo Wilson (eds.), Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81 of Proceedings of Machine Learning Research, pp. 48–48. PMLR, 23–24 Feb 2018.\n\nYonadav Shavit, Benjamin Edelman, and Brian Axelrod. Causal strategic linear regression. In\n\nInternational Conference on Machine Learning, pp. 8676–8686. PMLR, 2020.\n\nHidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-\n\nlikelihood function. Journal of statistical planning and inference, 90(2):227–244, 2000.\n\nChuanbiao Song, Kun He, Liwei Wang, and John E. Hopcroft. Improving the generalization of\n\nadversarial training with domain adaptation, 2019.\n\nMasashi Sugiyama, Matthias Krauledat, and Klaus-Robert M ̈uller. Covariate shift adaptation by\n\nimportance weighted cross validation. Journal of Machine Learning Research, 8(5), 2007.\n\nMasashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul von B ̈unau, and Motoaki Kawanabe. Direct importance estimation for covariate shift adaptation. Annals of the Institute of Statistical Mathematics, 60(4):699–746, 2008.\n\nPeter D. Taylor and Leo B. Jonker. Evolutionary stable strategies and game dynamics. Mathematical\n\nBiosciences, 40(1):145–156, 1978. ISSN 0025-5564.\n\nKarl Tuyls, Pieter Jan’T Hoen, and Bram Vanschoenwinkel. An evolutionary dynamical analysis of multi-agent learning in iterated games. Autonomous Agents and Multi-Agent Systems, 12(1): 115–153, 2006.\n\nBerk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classification. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pp. 10–19, 2019.\n\nThomas Varsavsky, Mauricio Orbes-Arteaga, Carole H. Sudre, Mark S. Graham, Parashkev Nachev,\n\nand M. Jorge Cardoso. Test-time unsupervised domain adaptation, 2020.\n\nYevgeniy Vorobeychik and Murat Kantarcioglu. Adversarial Machine Learning. Morgan & Claypool\n\nPublishers, 2018.\n\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully\n\ntest-time adaptation by entropy minimization, 2021a.\n\nJindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip S. Yu. Generalizing to unseen domains: A survey on domain generalization, 2021b.\n\nQing Wang, S.R. Kulkarni, and S. Verdu. Divergence estimation of continuous distributions based on data-dependent partitions. IEEE Transactions on Information Theory, 51(9):3064–3074, 2005. doi: 10.1109/TIT.2005.853314.\n\nBianca Zadrozny. Learning and evaluating classifiers under sample selection bias. In Proceedings of\n\nthe twenty-first international conference on Machine learning, pp. 114, 2004.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nKai Zhang, Vincent Zheng, Qiaojun Wang, James Kwok, Qiang Yang, and Ivan Marsic. Covariate shift in hilbert space: A solution via surrogate kernels. In International Conference on Machine Learning, pp. 388–395. PMLR, 2013a.\n\nKun Zhang, Bernhard Sch ̈olkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under target and conditional shift. In International Conference on Machine Learning, pp. 819–827. PMLR, 2013b.\n\nKun Zhang, Mingming Gong, Petar Stojanov, Biwei Huang, QINGSONG LIU, and Clark Glymour. Domain adaptation as a problem of inference on graphical models. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 4965–4976. Curran Associates, Inc., 2020.\n\nYuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In International Conference on Machine Learning, pp. 7404–7413. PMLR, 2019.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nWe arrange the appendix as follows:\n\nAppendix A.1 provides some real life scenarios where transparent models are useful or required.\n\nAppendix A.2 provides comparisons of our setting and other sub-areas in domain adaptation.\n\nAppendix A.3 provides proof for Theorem 3.1.\n\nAppendix A.4 provides proof for Theorem 3.2.\n\nAppendix A.5 provides proof of Theorem 3.3.\n\nAppendix A.6 provides proof for Proposition 4.1.\n\nAppendix A.7 provides proof for Theorem 4.2.\n\nAppendix A.8 provides proof for Theorem 4.6.\n\nAppendix A.9 provides omitted assumptions and proof for Section 4.3.\n\nAppendix A.10 provides proof for Theorem 5.1.\n\nAppendix A.11 provides proof for Theorem B.1.\n\nAppendix A.12 provides proof for Proposition B.2.\n\nAppendix B provides additional lower bound and examples for the target shift setting.\n\nAppendix C provides missing experimental results , including new experimental results using synthetic datasets generated according to causal graphs defined in Figure 2. We also add additional experimental results on credit score data set.\n\nAppendix D discusses challenges in minimizing induced risk.\n\nAppendix E provides discussions on how to directly minimize the induced risk.\n\nAppendix F provides discussions on adding regularization to the objective function.\n\nAppendix G provides discussions on the tightness of our theoretical bounds.\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\n•\n\nA.1 EXAMPLE USAGES OF TRANSPARENT MODELS\n\nAs we mentioned in Section 1, there is an increasing requirement of making the decision rule to be transparent due to its potential consequences impacts to individual decision subject. Here we provide the following reasons for using transparent models:\n\n• Government regulation may require the model to be transparent, especially in public services;\n\n• In some cases, companies may want to disclose their models so users will have explanations\n\nand are incentivized to better use the provided services.\n\n• Regardless of whether models are published voluntarily, model parameters can often be\n\ninferred via well-known query “attacks”.\n\nIn addition, we name some concrete examples of some real-life applications:\n\n• Consider the Medicaid health insurance program in the United States, which serves lowincome people. There is an obligation to provide transparency/disclose the rules (model to automate the decisions) that decide whether individuals qualify for the program — in fact, most public services have ”terms” that are usually set in stone and explained in the documentation. Agents can observe the rules and will adapt their profiles to be qualified if needed. For instance, an agent can decide to provide additional documentation they need to guarantee approval. For more applications along these lines, please refer to this report5.\n\n• Credit score companies directly publish their criteria for assessing credit risk scores. In loan application settings, companies actually have the incentive to release criteria to incentivize agents to meet their qualifications and use their services.Furthermore, making decision models transparent will gain the trust of users.\n\n5https://datasociety.net/library/poverty-lawgorithms/\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n• It is also known that it is possible to steal model parameters, if agents have incentives to do so6. For instance, spammers frequently infer detection mechanisms by sending different email variants; they then adjust their spam content accordingly.\n\nA.2 COMPARISON OF OUR SETTING AND SOME AREAS IN DOMAIN ADAPTATION\n\nWe compare our setting (We address it as IDA, representing “induced domain adaptation”) with the following areas:\n\n• Adversarial attack Chakraborty et al. (2018); Papernot et al. (2016); Song et al. (2019): in adversarial attack, the true label Y stays the same for the attacked feature, while in IDA, we allow the true label to change as well. One can think of adversarial attack as a specific form of IDA where the induced distribution has a specific target, that is to maximize the classifier’s error by only perturbing/modifying. Our transferability bound does, however, provide insights for how standard training results transfer to the attack setting.\n\n• Domain generalization Wang et al. (2021b); Li et al. (2017); Muandet et al. (2013): the goal of domain generalization is to learn a model that can be generalized to any unseen distribution; Similar to our setting, one of the biggest challenges in domain generalization also the lack of target distribution during training. The major difference, however, is that our focus is to understand how the performance of a classifier trained on the source distribution degrades when evaluated on the induced distribution (which depends on how the population of decision subjects responds); this degradation depends on the classifier itself.\n\n• Test-time adaptation Varsavsky et al. (2020); Wang et al. (2021a); Nado et al. (2021): the issue of test-time adaptation falls into the classical domain adaptation setting where the adaptation is independent of the model being deployed. Applying this technique to solve S(h) our problem requires accessing data (either unsupervised or supervised) drawn from for each h being evaluated during different training epochs.\n\nD\n\nA.3 PROOF OF THEOREM 3.1\n\nProof. We first establish two lemmas that will be helpful for bounding the errors of a pair of classifiers. Both are standard results from the domain adaption literature Ben-David et al. (2010).\n\nLemma A.1. For any hypotheses h, h(cid:48)\n\n∈ H\n\nand distributions\n\n,\n\n(cid:48),\n\nErrD(h, h(cid:48))\n\n|\n\n−\n\nErrD(cid:48)(h, h(cid:48))\n\n| ≤\n\nD\n\nD\n\ndH×H( D\n2\n\n,\n\n(cid:48))\n\n.\n\nD\n\nProof. Define the-cross prediction disagreement between two classifiers h, h(cid:48) on a distribution ErrD(h, h(cid:48)) := PD(h(X)\n\n= h(cid:48)(X)). By the definition of the\n\ndivergence,\n\nas\n\nD\n\ndH×H(\n\n,\n\nD\n\nD\n\n(cid:48)) = 2 sup\n\nh,h(cid:48)∈H |\n\nPD(h(X)\n\n= h(cid:48)(X))\n\n−\n\nPD(cid:48)(h(X)\n\n= h(cid:48)(X)) |\n\nH−\n\n= 2 sup\n\nErrD(h, h(cid:48))\n\nErrD(cid:48)(h, h(cid:48))\n\nh,h(cid:48)∈H | ErrD(h, h(cid:48)) |\n\n−\n\n− ErrD(cid:48)(h, h(cid:48)) |\n\n.\n\n2\n\n≥\n\n|\n\nAnother helpful lemma for us is the well-known fact that the 0-1 error obeys the triangle inequality (see, e.g., Crammer et al. (2008)):\n\nLemma A.2. For any distribution have ErrD(f1, f2)\n\nD ErrD(f1, f3) + ErrD(f2, f3).\n\n≤\n\nover instances and any labeling functions f1, f2, and f3, we\n\nDenote by ̄h∗ the ideal joint hypothesis, which minimizes the combined error:\n\n ̄h∗ := arg min\n\nh(cid:48)∈H\n\nErrD(h)(h(cid:48)) + ErrDS (h(cid:48))\n\n6https://www.wired.com/2016/09/how-to-steal-an-ai/\n\n16\n\n(cid:54) (cid:54) (cid:54) Under review as a conference paper at ICLR 2023\n\nWe have:\n\nErrD(h)(h)\n\n≤\n\n≤\n\n≤\n\nErrD(h)( ̄h∗) + ErrD(h)(h, ̄h∗) ErrD(h)( ̄h∗) + ErrDS (h, ̄h∗) + (cid:12)\n\n(cid:12)ErrD(h)(h, ̄h∗)\n\nErrD(h)( ̄h∗) + ErrDS (h) + ErrDS ( ̄h∗) +\n\n1 2\n\n(Lemma A.2)\n\nErrDS (h, ̄h∗)(cid:12) (cid:12)\n\n−\n\ndH×H(\n\nD\n\n(h))\n\nS,\n\nD\n\n(Lemma A.1)\n\n= ErrDS (h) + λDS→D(h) +\n\n1 2\n\ndH×H(\n\nS,\n\nD\n\nD\n\n(h)).\n\n(Definition of ̄h∗)\n\nA.4 PROOF OF THEOREM 3.2\n\nProof. Invoking Theorem 3.1, and replacing h with h∗\n\nT and S with\n\nErrD(h)(h∗\n\nT )\n\n≤\n\nNow observe that\n\nErrD(h∗\n\nT )(h∗\n\nT ) + λD(h)→D(h∗\n\nT ) +\n\n1 2\n\n(h∗\n\nT ), we have\n\nD\n\ndH×H(\n\n(h∗\n\nT ),\n\n(h))\n\nD\n\nD\n\n(7)\n\nErrD(h)(h)\n\n≤\n\n≤\n\n≤\n\n≤\n\n≤\n\nErrD(h)(h∗\n\nErrD(h)(h∗\n\nT ) + ErrD(h)(h, h∗ T ) T )(h, h∗\n\nT ) + ErrD(h∗\n\nT ) +\n\nErrD(h)(h∗\n\nT ) + ErrD(h∗\n\nT )(h, h∗\n\nT ) +\n\n(cid:12) (cid:12)ErrD(h)(h, h∗ (cid:12) T ) 1\n2\n\ndH×H(\n\n(h∗\n\nD\n\nT ),\n\n−\n\nErrD(h∗\n\nT )(h, h∗\n\n(cid:12) (cid:12) T ) (cid:12)\n\n(h))\n\nD\n\n(by Lemma A.1)\n\nErrD(h)(h∗\n\nT ) + ErrD(h∗\n\nT )(h) + ErrD(h∗\n\nT )(h∗\n\nT ) +\n\n1 2\n\ndH×H(\n\nD\n\n(h∗\n\nT ),\n\n(h))\n\nD\n\n(by Lemma A.2)\n\nErrD(h∗\n\nT )(h∗\n\nT ) + λD(h)→D(h∗\n\nT ) +\n\n1 2\n\ndH×H(\n\nD\n\n(h∗\n\nT ),\n\n(h))\n\nD\n\n(by equation 7)\n\n+ ErrD(h∗\n\nT )(h) + ErrD(h∗\n\nT )(h∗\n\nT ) +\n\n1 2\n\ndH×H(\n\nD\n\n(h∗\n\nT ),\n\n(h))\n\nD\n\nAdding ErrD(h)(h) to both sides and rearranging terms yields\n\n2ErrD(h)(h)\n\n−\n\n2ErrD(h∗\n\nT )(h∗\n\nT )\n\nErrD(h)(h) + ErrD(h∗\n\nT )(h) + λD(h)→D(h∗\n\n≤ = ΛD(h)→D(h∗\n\nT )(h) + λD(h)→D(h∗\n\nT ) + dH×H(\n\nT ) + dH×H( (h∗\n\nT ),\n\n(h∗\n\nT ),\n\nD (h))\n\nD\n\nD\n\n(h))\n\nD\n\nDividing both sides by 2 completes the proof.\n\nA.5 PROOF OF THEOREM 3.3\n\nProof. Using the triangle inequality of dTV, we have\n\ndTV(\n\nDh|S) + dTV( ≤\nand by the definition of dTV, the divergence term dTV(\n\nDY |S,\n\nDY |S,\n\nY (h))\n\ndTV(\n\nD\n\nh(h),\n\nY (h))\n\nD\n\n(8)\n\ndTV(\n\nDY |S,\n\nDh|S) =\n\n=\n\n=\n\nDh|S, DY |S,\n\nh(h)) + dTV(\n\nD\n\nD Y (h)) becomes D\nPDS (h(x) = +1)\n\n−\n\n− EDS [h(X)] + 1 2\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nEDS [h(X)] 2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n|\n\nh(X)\n\n] |\n\nPDS (Y = +1) |\n(cid:12) EDS [Y ] + 1 (cid:12) (cid:12) 2\n(cid:12) (cid:12) EDS [Y ] (cid:12) (cid:12) 2\n(cid:12) 1\n2 ·\n\nEDS [\n\n−\n\n−\n\nY |\n≤ = ErrDS (h)\n\nSimilarly, we have\n\ndTV(\n\nh(h),\n\nD\n\nY (h))\n\nD\n\n≤\n\nErrD(h)(h)\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nAs a result, we have\n\nErrDS (h) + ErrD(h)(h)\n\ndTV( dTV(\n\nDY |S, DY |S,\n\n≥\n\n≥\n\nwhich implies\n\nDh|S) + dTV(\n\nY (h))\n\nD dTV(\n\nD\n\n−\n\nh(h), Dh|S,\n\nD\n\nD\n\nY (h)) h(h))\n\n(by equation 8)\n\nmax\n\n{\n\nErrDS (h), ErrD(h)(h)\n\n} ≥\n\ndTV(\n\nDY |S,\n\nY (h))\n\nD\n\n− 2\n\ndTV(\n\nDh|S,\n\nh(h))\n\nD\n\n.\n\nA.6 PROOF OF PROPOSITION 4.1\n\nProof.\n\nED(h)[(cid:96)(h; X, Y )] (cid:90)\n\nPD(h)(X = x, Y = y)(cid:96)(h; x, y) dxdy\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\nPDS (Y = y\n\nPDS (Y = y\n\nPDS (Y = y\n\nX = x) |\n\nX = x) |\n\nX = x) |\n\n·\n\n·\n\n·\n\nPD(h)(X = x)(cid:96)(h; x, y) dxdy\n\nPDS (X = x)\n\nPDS (X = x)\n\n·\n\n·\n\nPD(h)(X = x) PDS (X = x) ·\n\n(cid:96)(h; x, y) dxdy\n\nωx(h)\n\n·\n\n(cid:96)(h; x, y) dxdy\n\n=EDS [ωx(h)\n\n(cid:96)(h; x, y)]\n\n·\n\nA.7 PROOF OF THEOREM 4.2\n\nProof. We start from the error induced by h∗ ̄ω(h∗\n\nS)]; we add and subtract this from the error:\n\nS. Let the average importance weight induced by h∗\n\nS be\n\nS) = EDS [ωx(h∗ = EDS [ ̄ω(h∗ S)\n\nS)\n\n1(h∗ ·\n1(h∗\n\nS(x) S(x)\n\n= y)] = y)] + EDS [(ωx(h∗\n\nS)\n\n·\n\n ̄ω(h∗\n\nS))\n\n·\n\n−\n\n1(h∗\n\nS(x)\n\n= y)]\n\nS) = EDS [ωx(h∗ S )(h∗\n\nErrD(h∗\n\nIn fact, ̄ω(h∗\n\nS) = 1, since\n\n ̄ω(h∗\n\nS) =EDS [ωx(h∗\n\nS)] =\n\n(cid:90)\n\nωx(h∗\n\nS)PDS (X = x)dx\n\n=\n\n(cid:90) PD(h)(X = x) PDS (X = x)\n\nPDS (X = x)dx =\n\n(cid:90)\n\nPD(h)(X = x)dx = 1\n\nNow consider any other classifier h. We have\n\nS )(h∗ ErrD(h∗ S) = EDS [1(h∗ S(x)\n\nEDS [1(h(x)\n\n≤\n\n= EDS [ ̄ω(h)\n\n·\n\n= EDS [ωx(h)\n\n= y)] + EDS [(ωx(h∗ S)\n\n= y)] + EDS [(ωx(h∗\n\nS)\n\nS))\n\n ̄ω(h∗ S))\n\n− ̄ω(h∗\n\n1(h∗\n\nS(x)\n\n= y)]\n\nS(x)\n\n= y)]\n\n· 1(h∗\n\n·\n\n−\n\n1(h(x)\n\n= y)] + EDS [(ωx(h∗\n\nS)\n\n ̄ω(h∗\n\nS))\n\n·\n\n−\n\n1(h∗\n\nS(x)\n\n= y)]\n\n(by optimality of h∗\n\nS on\n\nS)\n\nD\n\n(multiply by ̄ω(h∗\n\nS) = 1)\n\n1(h(x)\n\n= y)] + EDS [( ̄ω(h)\n\nωx(h))\n\n·\n\n−\n\n1(h(x)\n\n·\n\n= y)]\n\n(add and subtract ̄ω(h∗\n\nS))\n\n+ EDS [(ωx(h∗ = ErrD(h)(h) + Cov(ωx(h∗\n\nS)\n\n−\n\nS)) S), 1(h∗\n\n1(h∗ S(x)\n\n ̄ω(h∗\n\n·\n\nS(x)\n\n= y)]\n\nCov(ωx(h), 1(h(x)\n\n= y))\n\n= y))\n\n−\n\n18\n\n(cid:54) (cid:54) (cid:54) (cid:54) (cid:54) (cid:54) (cid:54) (cid:54) (cid:54) (cid:54) (cid:54) (cid:54) (cid:54) (cid:54) Under review as a conference paper at ICLR 2023\n\nMoving the error terms to one side, we have\n\nS )(h∗ ErrD(h∗ S) −\nS), 1(h∗ Cov(ωx(h∗ (cid:113)\n\nVar(ωx(h∗\n\nS))\n\n·\n\nErrD(h)(h) S(x)\n\n= y))\n\nVar(1(h∗\n\n= y))\n\n− S(x)\n\n(cid:112)\n\n+\n\nVar(ωx(h))\n\n·\n\nVar(1(h(x)\n\n= y))\n\nCov(ωx(h), 1(h(x)\n\n= y))\n\n(\n\nCov(X, Y ) |\n\n| ≤\n\n(cid:112)Var(X)\n\nVar(Y ))\n\n·\n\nVar(ωx(h∗\n\nS))\n\nErrS(h∗\n\nS)(1\n\n·\n\nVar(ωx(h∗\n\nS))\n\nErrS(h∗\n\nS) +\n\n− (cid:112)\n\nErrS(h∗\n\nS)) +\n\nVar(ωx(h))\n\n·\n\n· (cid:18)(cid:113)\n\nVar(ωx(h∗\n\nS)) +\n\n(cid:112)\n\nVar(ωx(h))\n\n(cid:19)\n\nErrDS (h)\n\n·\n\n(cid:112)\n\nVar(ωx(h))\n\nErrDS (h)(1\n\n·\n\nErrDS (h)\n\n−\n\n(1\n\nErrDS (h))\n\nErrDS (h)\n\n1)\n\n≤\n\n−\n\n≤\n\n≤\n\n=\n\n≤\n\n≤\n\n(cid:113)\n\n(cid:113)\n\n(cid:112)\n\nSince this holds for any h, it certainly holds for h = h∗ T .\n\nA.8 OMITTED ASSUMPTIONS AND PROOF OF THEOREM 4.6\n\nDenote X+(h) =\n\nx : ωx(h)\n\n{\n\n1\n\n}\n\n≥\n\n(cid:90)\n\nand X−(h) =\n\nx : ωx(h) < 1\n\n. First we observe that\n\n{\n\n}\n\nPDS (X = x)(1\n\nPDS (X = x)(1\n\n−\n\n−\n\nωx(h))dx\n\nωx(h))dx = 0\n\nX+(h)\n\n(cid:90)\n\nX−(h)\n\n+\n\nThis is simply because of (cid:82)\n\nx\n\nPDS (X = x)\n\nωx(h)dx = (cid:82)\n\nx\n\n·\n\nPD(h)(X = x)dx = 1.\n\nProof. Notice that in the setting of binary classification, we can write the total variation distance Y (h) as the difference between the probability of Y = +1 and the probability between of Y =\n\nDY |S and\n\n1:\n\nD\n\n−\n\nY (h))\n\nD\n\nDY |S, dTV( = (cid:12) (cid:12)PDS (Y = +1) (cid:12) (cid:90) (cid:12) (cid:12) (cid:12) (cid:12) (cid:90) (cid:12) (cid:12) (cid:12)\n\n− PDS (Y = +1 |\n\nPDS (Y = +1 |\n\n=\n\n=\n\nPD(h)(Y = +1)(cid:12) (cid:12)\n\nX = x)PDS (X = x)dx\n\n−\n\n(cid:90)\n\nX = x)PDS (X = x)\n\n(1\n\n·\n\n−\n\n(cid:12) (cid:12) ωx(h))dx (cid:12) (cid:12)\n\nX = x)PDS (X = x)ωx(h)dx PDS (Y = +1 |\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(9)\n\nSimilarly we have\n\ndTV(\n\nDh|S,\n\nh(h)) =\n\nD\n\n(cid:90)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nPDS (h(x) = +1\n\nX = x)PDS (X = x) |\n\n·\n\n(1\n\n−\n\n(cid:12) (cid:12) ωx(h))dx (cid:12) (cid:12)\n\n(10)\n\n19\n\n(cid:54) (cid:54) (cid:54) (cid:54) =\n\n=\n\ndTV( (cid:12) (cid:90) (cid:12) (cid:12) (cid:12) (cid:90) (cid:12) (cid:12) (cid:12)\n\n(cid:124)\n\n+\n\n(cid:90)\n\nX−(h)\n\n(cid:124) (cid:90)\n\nX+(h)\n\n(cid:90)\n\nX−(h)\n\nX+(h) (cid:90)\n\nX−(h)\n\n=\n\n−\n\n− (cid:90)\n\n=\n\n+\n\n(cid:90)\n\n=\n\nUnder review as a conference paper at ICLR 2023\n\nWe can further expand the total variation distance between\n\nDY |S and\n\nY (h) as follows:\n\nD\n\nY (h))\n\nDY |S, D\nX = x)PDS (X = x) PDS (Y = +1 |\n\n(cid:12) (cid:12) ωx(h))dx (cid:12) (cid:12)\n\n(1\n\n·\n\n−\n\nX = x)PDS (X = x) PD(Y = +1 |\n\n·\n\n(1\n\n−\n\nωx(h))dx\n\nX+(h)\n\n(cid:123)(cid:122) ≤0\n\nPDS (Y = +1 |\n\nX = x)PDS (X = x)\n\n(cid:123)(cid:122) >0\n\nX = x)PDS (X = x) PDS (Y = +1 |\n\nPDS (Y = +1 |\n\nX = x)PDS (X = x)\n\n·\n\n·\n\n·\n\n(cid:125)\n\n(cid:12) (cid:12) ωx(h))dx (cid:12)\n\n(1\n\n−\n\n(cid:125)\n\nωx(h))dx\n\nωx(h))dx\n\n(by Assumption 4.3)\n\n(1\n\n(1\n\n−\n\n−\n\nPDS (Y = +1 X = x)PDS (X = x) |\n\n·\n\n(ωx(h)\n\n1)dx\n\n−\n\nPDS (Y = +1 X = x)PDS (X = x) |\n\n·\n\n(ωx(h)\n\n1)dx\n\n−\n\n(by equation 9)\n\nX = x)PDS (X = x) PDS (Y = +1 |\n\n·\n\n(ωx(h)\n\n1)dx\n\n−\n\nSimilarly, by assumption 4.4 and equation equation 10, we have\n\ndTV(\n\nDh|S,\n\nh(h)) =\n\nD\n\n(cid:90)\n\nPDS (h(x) = +1\n\nX = x)PDS (X = x) |\n\n·\n\n(ωx(h)\n\n1)dx\n\n−\n\nThus we can bound the difference between dTV(\n\nDY |S,\n\nY (h)) and dTV(\n\nD\n\nDh|S,\n\nh(h)) as follows:\n\nD\n\ndTV( (cid:90)\n\nD\n\ndTV(\n\nY (h))\n\nDY |S, Dh|S, X = x)PDS (X = x) PDS (Y = +1 |\n\n−\n\nD\n\nh(h))\n\n(ωx(h)\n\n1)dx\n\n−\n\n·\n\nX = x)PDS (X = x) PD(h(x) = +1 |\n\n·\n\n(ωx(h)\n\n1)dx\n\n−\n\n=\n\n(cid:90)\n\n=\n\n(cid:90)\n\n−\n\n[PDS (Y = +1 X = x) |\n\n= EDS [(PDS (Y = +1 |\n\n− X = x)\n\nPDS (h(x) = +1\n\nX = x)]PDS (X = x) |\n\nPDS (h(x) = +1\n\n|\n\n−\n\nX = x)) (ωx(h)\n\n−\n\n1)dx\n\n−\n\n(ωx(h)\n\n· 1)]\n\n(by Assumption 4.5)\n\n> EDS [PDS (Y = +1 X = x) |\n= 0\n\nX = x)]EDS [ωx(h) PDS (h(x) = +1 |\n\n−\n\n−\n\n1]\n\nCombining the above with Theorem 3.3, we have\n\nmax\n\nErrDS (h), ErrD(h)(h)\n\n{\n\ndTV(\n\nDY |S,\n\nY (h))\n\nD\n\n− 2\n\ndTV(\n\nDh|S,\n\nh(h))\n\nD\n\n> 0\n\n} ≥\n\nA.9 OMITTED DETAILS FOR SECTION 4.3\n\nWith Setup 2 - Setup 4, we can further specify the important weight wx(h) for the strategic response setting:\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nLemma A.3. Recall the definition for the covariate shift important weight coefficient ωx(h) := PD(h)(X=x) PDS (X=x) , for our strategic response setting, we have,\n\nwx(h) =\n\n \n\n\n\n1, τh−x\n\nB ,\n\n1\n\nB ( 1,\n\n−\n\nx x\nx + τh + 2B), x x\n\nB) [0, τh −\n[τh B, τh) [τh, τh + B) [τh + B, 1]\n\n−\n\n∈ ∈\n∈ ∈\n\n(11)\n\nProof for Lemma A.3:\n\nProof. We discuss the induced distribution\n\n(h) by cases:\n\nD\n\n• For the features distributed between [0, τh\n\nB]: since we assume the agents are rational, B] will not perform any under assumption 2, agents with feature that is smaller than [0, τh kinds of adaptations, and no other agents will adapt their features to this range of features either, so the distribution between [0, τh\n\nB] will remain the same as before.\n\n−\n\n−\n\n− B, τh] can be directly calculated from assumption\n\n• For the target distribution between [τh\n\n3.\n\n−\n\n• For distribution between [τh, τh + B], consider a particular feature x(cid:63)\n\nSetup 4, we know its new distribution becomes:\n\n[τh, τh + B], under\n\n∈\n\nPD(h)(x = x(cid:63)) = 1 +\n\n= 1 +\n\n(cid:90) τh\n\nx(cid:63)−B\n\n(cid:90) τh\n\nx(cid:63)−B\n\n1 B\n\n1 B\n\n−\n\n− dz\n\nτh−z B\nτh + z\n\ndz\n\n=\n\n1 B\n\n(\n\n−\n\nx(cid:63) + τh + 2B)\n\n• For the target distribution between [τh + B, 1]: under assumption 2 and 4, we know that no agents will change their feature to this feature region. So the distribution between [τh + B, 1] remains the same as the source distribution.\n\nRecall the definition for the covariate shift important weight coefficient ωx(h) := distribution of ωx(h) after agents’ strategic responding becomes:\n\nPD(h)(X=x) PDS (X=x) , the\n\nx x\nx + τh + 2B), x\n\nB) and x\n\n[0, τh −\nB, τh) [τh [τh, τh + B)\n\n−\n\n[τh + B, 1]\n\n∈\n\n(12)\n\n∈ ∈\n∈\n\notherwise\n\n1, τh−x\n\nB ,\n\n1\n\nB ( 0,\n\n−\n\nωx(h) =\n\n \n\n\n\nProof for Proposition 4.7:\n\nProof. According to Lemma A.3, we can compute the variance of wx(h) as Var(wx(h)) = E(wx(h)2) 3 B. Then by plugging it to the general bound for Theorem 4.2 gives us the desirable result.\n\nE(wx(h)2) = 2\n\n−\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nA.10 PROOF OF THEOREM 5.1\n\nProof. Defining p := PDS (Y = +1), p(h) = PD(h)(Y = +1), we have\n\nErrD(h∗\n\nS )(h∗\n\nS) = p(h∗\n\nS)\n\nErr+(h∗\n\nS) + (1\n\np(h∗\n\nS))\n\n·\n\n−\n\nErr−(h∗\n\nS)\n\n·\n\n= p (cid:124)\n\n·\n\nErr+(h∗\n\nS) + (1 (cid:123)(cid:122) (I)\n\np)\n\n·\n\n−\n\nErr−(h∗\n\nS) (cid:125)\n\n+(p(h∗\n\nS)\n\n−\n\np)[Err+(h∗\n\nS)\n\nErr−(h∗\n\nS)]\n\n−\n\nWe can expand (I) as follows:\n\n(by definitions of p(h∗\n\nS), Err+(h∗\n\nS), and Err−(h∗\n\nS)) (13)\n\nS) + (1 T ) + (1\n\nErr+(h∗ Err+(h∗ T )\n\np ·\np ≤\n· = p(h∗ Err+(h∗ ·\nT )(h∗ = ErrD(h∗\n\nT ) + (p\n\np) −\np) −\nT ) + (1\n\n· ·\n\nS) T )\n\nErr−(h∗ Err−(h∗ p(h∗ T )) T ))\n\n− p(h∗\n\n−\n\n·\n\nErr−(h∗ T )\n\n[Err+(h∗\n\n·\n\nT ) + (p\n\n− Err−(h∗\n\nT ))\n\np(h∗ T )] .\n\n−\n\n(by optimality of h∗ Err−(h∗ [Err+(h∗\n\nS on T )]\n\nT )\n\n−\n\n·\n\nS)\n\nD\n\nPlugging this back into equation 13, we have\n\nErrD(h∗\n\nS )(h∗ S)\n\n−\n\nNotice that\n\nErrD(h∗\n\nT )(h∗\n\nT )\n\n(p(h∗\n\nS)\n\n−\n\n≤\n\np)[Err+(h∗\n\nS)\n\n−\n\nErr−(h∗\n\nS)] + (p\n\np(h∗\n\nT ))\n\n·\n\n−\n\n[Err+(h∗\n\nT )\n\n−\n\nErr−(h∗\n\nT )]\n\n0.5(Err+(h)\n\n−\n\nErr−(h)) = 0.5 = 0.5\n\nP(h(X) = +1\n\n0.5\n\n1 −\nPDu(h(X) = +1)\n\n·\n\n·\n\n−\n\nY = +1) |\n\n−\n\n0.5\n\n·\n\nP(h(X) = +1\n\nY = |\n\n−\n\n1)\n\nwhere\n\nD\n\nu is a distribution with uniform prior. Then\n\n(p(h∗ (p\n\np)[Err+(h∗ S) S) −\nT ))[Err+(h∗ p(h∗ T )\n\n−\n\n−\n\n−\n\nErr−(h∗ Err−(h∗\n\nS)] = 2(p(h∗ T )] = 2(p\n\nS) −\np(h∗\n\np) T ))\n\n−\n\n(0.5 (0.5\n\n·\n\n·\n\n−\n\n−\n\nPDu(h(X) = +1)) PDu (h(X) = +1))\n\nAdding together these two equations yields\n\n−\n\nS)\n\n(p(h∗ S) = 2(p(h∗ = (p(h∗ S) + 2p S) + 2p\n\np(h∗\n\n≤ |\n\n−\n\nS)\n\n(0.5\n\n− PDu (h∗\n\np)[Err+(h∗ p) ·\np(h∗ T )) (PDu (h∗ p(h∗ T ) | · PDu (h∗ S(X) = +1)\n\n− 2 (p(h∗ −\nS(X) = +1)\n\n(1 + 2\n\n|\n\n−\n\n·\n\n− · |\n\nErr−(h∗\n\nS)] + (p\n\np(h∗ S(X) = +1)) + 2(p S(X) = +1)\n\nS)PDu (h∗\n\n−\n\n· −\n\n[Err+(h∗ T ) −\np(h∗ (0.5 T )) ·\nT )PDu (h∗ p(h∗\n\nErr−(h∗\n\nT )] PDu (h∗ T (X) = +1))\n\n−\n\nT ))\n\nT (X) = +1))\n\n− PDu (h∗\n\n−\n\nT (X) = +1))\n\nPDu (h∗ S(X) = +1) PDu (h∗\n\n− T (X) = +1) |\n\n−\n\nPDu (h∗\n\nT (X) = +1)\n\n) |\n\nMeanwhile,\n\n≤\n\nPDu (h∗ |\n0.5\n\n· | + 0.5\n\n= 0.5 (dTV(\n\n· |\n\nD\n\nS(X) = +1)\n\nPD|Y =+1(h∗\n\nPDu (h∗ S(X) = +1)\n\n−\n\nT (X) = +1) |\n\nPD|Y =+1(h∗\n\nPD|Y =−1(h∗ +(h∗ S),\n\nS(X) = +1) T )) + dTV(\n\n+(h∗\n\nPD|Y =−1(h∗ −(h∗ −(h∗\n\nS),\n\nT (X) = +1) |\nT ))\n\nD\n\nD\n\n−\n\n−\n\nD\n\nT (X) = +1) |\n\nCombining equation 14 and equation 15 gives\n\nPDu (h∗\n\nS(X) = +1)\n\nPDu (h∗\n\nT (X) = +1)\n\n|\n\n· |\n\np(h∗\n\n− · |\n\n(1 + 2\n\nS) + 2p p(h∗ S) + p ·\np(h∗ S) −\n\np(h∗ T ) | · PDu (h∗ S(X) = +1) p(h∗ (1 + dTV( T ) −\n(dTV( S), D\np(h∗ + (1 + p) T ) |\n\n| · +(h∗\n\nD\n\n·\n\n≤ |\n\n≤ |\n\n−\n\nPDu (h∗ S),\n\n− +(h∗\n\nT (X) = +1) |\n+(h∗\n\nT )) + dTV(\n\nD D\n+(h∗ T )) + dTV( (dTV(\n\nD +(h∗ S),\n\n−(h∗\n\nD\n\nD\n\n−(h∗ D\n−(h∗ T ))\n\nS), D\n+(h∗ T )) + dTV(\n\nS),\n\n−(h∗\n\nT ))\n\nD\n\n) |\n\n−(h∗\n\nS),\n\n−(h∗\n\nT )) .\n\nD\n\nD\n\n22\n\n(14)\n\n(15)\n\nUnder review as a conference paper at ICLR 2023\n\nA.11 PROOF OF THEOREM B.1\n\nWe will make use of the following fact:\n\nLemma A.4. Under label shift, TPRS(h) = TPRh(h) and FPRS(h) = FPRh(h).\n\nProof. We have\n\nTPRh(h) =PD(h)(h(X) = +1\n\nY = +1) |\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n=\n\n=\n\n=\n\n=\n\n=\n\nPD(h)(h(X) = +1, X = x\n\nY = +1)dx |\n\nX = x, Y = +1)PD(h)(X = x PD(h)(h(X) = +1 |\n\nY = +1)dx |\n\n1(h(x) = +1)PD(h)(X = x\n\nY = +1)dx\n\n|\n\n1(h(x) = +1)PDS (X = x\n\nY = +1)dx |\n\n(by definition of label shift)\n\nPDS (h(X) = +1\n\n|\n\nX = x, Y = +1)PDS (X = x\n\nY = +1)dx |\n\n=TPRS(h)\n\nThe argument for TPRh(h) = TPRS(h) is analogous.\n\nNow we proceed to prove the theorem.\n\nProof of Theorem B.1. In section 3.2 we showed a general lower bound on the maximum of ErrDS (h) and ErrD(h)(h):\n\nmax\n\n{\n\nErrDS (h), ErrD(h)(h)\n\n} ≥\n\ndTV(\n\nDY |S,\n\nY (h))\n\nD\n\ndTV(\n\nDh|S,\n\nh(h))\n\nD\n\n− 2\n\nIn the case of label shift, and by the definitions of p and p(h),\n\ndTV(\n\nDY |S,\n\nD\n\nY (h)) =\n\n|\n\nPDS (Y = +1)\n\nPD(h)(Y = +1) |\n\n=\n\np |\n\n−\n\n−\n\np(h)\n\n|\n\nIn addition, we have\n\nDh|S = PS(h(X) = +1) = p\n\n·\n\nTPRS(h) + (1\n\np)\n\n·\n\n−\n\nFPRS(h)\n\n(16)\n\n(17)\n\nSimilarly\n\nTherefore\n\nD\n\nh(h) = PD(h)(h(X) = +1) TPRh(h) + (1 TPRS(h) + (1\n\n= p(h) = p(h)\n\n·\n\n·\n\np(h)) p(h))\n\n·\n\n·\n\n−\n\n−\n\nFPRh(h) FPRS(h)\n\n(by Lemma A.4)\n\n(18)\n\ndTV(\n\nDh|S,\n\nD\n\nPDS (h(X) = +1) h(h)) = |\np(h)) (p |\n\n−\n\n=\n\n·\n\n−\n\nPD(h)(h(X) = +1) |\nFPRS(h)\n\np)\n\nTPRS(h) + (p(h)\n\n−\n\n·\n\n|\n\n(By equation 18 and equation 17)\n\n=\n\np |\n\n−\n\np(h)\n\n| · |\n\nTPRS(h)\n\n−\n\nFPRS(h)\n\n|\n\n(19)\n\nwhich yields:\n\ndTV(\n\nDY |S,\n\nD\n\nY (h))\n\ndTV(\n\nDh|S,\n\n−\n\nh(h)) =\n\nD\n\np\n\n|\n\n−\n\np(h)\n\n(1 |\n\ncompleting the proof.\n\n23\n\nTPRS(h)\n\n− | (By equation 16 and equation 19)\n\n−\n\nFPRS(h) |\n\n)\n\nUnder review as a conference paper at ICLR 2023\n\nA.12 PROOF OF PROPOSITION B.2\n\nProof.\n\np(h∗ |\n\nS)\n\n(1\n\n= |\n\n−\n\np(h∗\n\nT )\n\n− ErrDS (h∗ (1\n\n1 PDS (Y = +1) (1 (1\n\n| · S))TPRS(h∗ ErrDS (h∗\n\nS) S))\n\nErrDS (h∗ S) |\n(1\n\n≤\n\n− −\n\n− ErrDS (h∗ ErrDS (h∗\n\nT ) S))\n\n| · | ·\n\n−\n\n− −\n\nErrDS (h∗ ErrDS (h∗ S) −\nErrDS (h∗\n\nT ))TPRS(h∗ T )) TPRS(h∗ T ))\n\n− ·\nTPRS(h∗ (1\n\nT ) |\n\nT ) |\n\n(20)\n\nThe inequality above is due to Lemma 7 of Liu & Liu (2015).\n\nB LOWER BOUND AND EXAMPLE FOR TARGET SHIFT\n\nB.1 LOWER BOUND\n\nNow we discuss lower bounds. Denote by TPRS(h) and FPRS(h) the true positive and false positive rates of h on the source distribution\n\nS. We prove the following:\n\nTheorem B.1. Under target shift, any model h must incur the following error on either the\n\nD\n\nS or\n\nD\n\n(h):\n\nD\n\nmax\n\np |\n\n−\n\n≥\n\nErrDS (h), ErrD(h)(h) {\nTPRS(h) p(h)\n\n(1\n\n| ·\n\n− |\n\n}\n\n−\n\n2\n\nFPRS(h)\n\n) |\n\n.\n\nD\n\nDh|S, and\n\nh(h)) under the assumption of target shift. Since\n\nY (h)), The proof extends the bound of Theorem 3.3 by further explicating each of dTV( dTV( < 0 unless we have a trivial classifier that has either TPRS(h) = 1, FPRS(h) = 0 or TPRS(h) = 0, FPRS(h) = 1, the lower bound is strictly positive. Taking a closer look, the lower bound is determined linearly p(h). The difference is further determined by the by how much the label distribution shifts: p performance of h on the source distribution through 1 FPRS(h) . For instance, when |\nTPRS(h) > FPRS(h), the quality becomes FNRS(h) + FPRS(h), that is the more error h makes, the larger the lower bound will be.\n\nTPRS(h) |\n\nDY |S,\n\nTPRS(h)\n\nFPRS(h)\n\n− |\n\n−\n\n−\n\n−\n\nD\n\n|\n\nB.2 EXAMPLE USING REPLICATOR DYNAMICS\n\nLet us instantiate the discussion using a specific fitness function for the replicator dynamics model (Section 2.1), which is the prediction accuracy of h for class +1:\n\n[Fitness of Y = +1] := PDS (h(X) = +1\n\nThen we have E [Fitness of Y ] = ErrDS (h), and\n\nY = +1)\n\n|\n\n(21)\n\np(h) PDS (Y = +1)\n\n=\n\nPDS (h(X) = +1 Y = +1) |\nErrDS (h)\n\nPlugging the result back to our Theorem 5.1 we have\n\nProposition B.2. Under the replicator dynamics model in Eqn. (21), as:\n\np(h∗ |\n\nS)\n\np(h∗\n\nT ) |\n\n−\n\nfurther bounds\n\np(h∗ S) |\nErrDS (h∗\n\np(h∗ S)\n\n−\n\nT )\n\nPDS (Y = +1)\n\n| ≤ ErrDS (h∗ ErrDS (h∗\n\nT ) S)\n\n−\n\nTPRS(h∗ S) ErrDS (h∗ T )\n\n| · | ·\n\n|\n\n·\n\nTPRS(h∗\n\nT ) |\n\n.\n\n−\n\nThat is, the difference between ErrD(h∗ between the two classifiers’ performances on the source data evaluate the possible error transferability using the source data only.\n\nS) and ErrD(h∗\n\nD\n\nT )(h∗\n\nS )(h∗\n\nT ) is further dependent on the difference S. This offers an opportunity to\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nC MISSING EXPERIMENTAL DETAILS\n\nC.1 SYNTHETIC EXPERIMENTS USING DAG\n\nSynthetic experiments using simulated data We generate synthetic data sets from structural equation models described on simple causal DAG in Figure 2 for covariate shift and target shift. To Rd, generate the induced distribution , its induced features are precisely x(cid:48) = ∆(x, h). so that when an input x encounters classifier h We provide details of the data generation processes and adaptation functions in Appendix C.\n\n(h), we posit a specific adaptation function ∆ : Rd\n\n× H →\n\n∈ H\n\nD\n\n{\n\nx1, . . . , xn\n\n1. To compute h∗\n\nWe take our training data set ·\n} x)7. We then consider the hypothesis class x) > [0, 1] {\nτ ] S, the model that performs best on the source distribution, we simply vary τ and take the hτ with lowest prediction error. Then, we posit a specific adaptation function ∆(x, hτ ). Finally, to compute h∗ T , we vary τ from 0 to 1 and find the classifier hτ that minimizes the prediction error on its induced data set\n\nand learn a “base” logistic regression model h(x) = σ(w\n\n. We report our results in Figure 4.\n\n∆(x1, hτ ), . . . , ∆(xn, hτ )\n\n, where hτ (x) := 2\n\n1[σ(w\n\n:=\n\nhτ\n\nH\n\n−\n\n∈\n\n}\n\nτ\n\n·\n\n|\n\n·\n\n{\n\n}\n\nFigure 4: Results for synthetic experiments on simulated and real-world data. Diff := ErrD(h∗ ErrDS (h∗ T ), ErrD(h∗ ErrD(h∗ {\n} rem 4.2, and LB := lower bound specified in Theorem 4.6.\n\n− , UB := upper bound specified in Theo-\n\nT ), Max := max\n\nT )(h∗\n\nT )(h∗\n\nT )\n\nS )(h∗ S)\n\nCovariate Shift We specify the causal DAG for covariate shift setting in the following way:\n\nX1\n\nUnif(\n\n1, 1)\n\n∼\n\n−\n\nX2\n\nX3\n\n∼\n\n1.2X1 + X 2\n\n(0, σ2 2) (0, σ2 1 + 3) Y := 2sign(X2 > 0)\n\n∼ −\n\nN\n\nN\n\n1\n\n−\n\n2 and σ2\n\nwhere σ2 3 are parameters of our choices. Adaptation function We assume the new distribution of feature X (cid:48) way:\n\n1 will be generated in the following\n\nX (cid:48)\n\n1 = ∆(X) = X1 + c\n\n(h(X)\n\n1)\n\n∈\n\n· R1 > 0 is the parameter controlling how much the prediction h(X) affect the generating where c of X (cid:48) 1, namely the magnitude of distribution shift. Intuitively, this adaptation function means that if a feature x is predicted to be positive (h(x) = +1), then decision subjects are more likely to adapt to that feature in the induced distribution; Otherwise, decision subjects are more likely to be moving away from x since they know it will lead to a negative prediction.\n\n−\n\nTarget Shift We specify the causal DAG for target shift setting in the following way:\n\n(Y + 1)/2\n\nBernoulli(α)\n\nY = y\n\nX1\n\n|\n\n∼ N[0,1](μy, σ2)\n\n∼\n\nX2 =\n\n0.8X1 +\n\n− X3 = 0.2Y +\n\n(0, σ2 2)\n\nN (0, σ2 3)\n\nN\n\nR3 denotes the weights.\n\n7σ( ·\n\n) is the logistic function and w\n\n∈\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nN[0,1] represents a truncated Gaussian distribution taken value between 0 and 1. α, μy, σ2,σ2 where and σ2 3 are parameters of our choices. Adaptation function We assume the new distribution of the qualification Y (cid:48) will be updated in the following way:\n\n2\n\nP(Y (cid:48) = +1 h(X) = h, Y = y) = chy, where |\n\nh, y\n\n{\n\n} ∈ {−\n\n1, +1\n\n}\n\nwhere 0 and get predicted as h(X) = h to be qualified in the next step (Y (cid:48) = +1).\n\n1 represents the likelihood for a person with original qualification Y = y\n\nchy\n\n≤\n\n≤\n\n∈\n\nR1\n\nT )(h∗\n\nT ), indicating the suboptimality of training on\n\nDiscussion of the Results For all four datasets, we do observe positive gaps ErrD(h∗ −\nErrD(h∗ S. The gaps are well bounded by the theoretical results. For lower bound, the empirical observation and the theoretical bounds are roughly within the same magnitude except for one target shift dataset, indicating the effectiveness of our theoretical result. For upper bound, for target shift, the empirical observations are well within the same magnitude of the theoretical bounds while the results for the covariate shift are relatively loose.\n\nD\n\nS )(h∗ S)\n\nC.2 SYNTHETIC EXPERIMENTS USING REAL-WORLD DATA\n\nOn the preprocessed FICO credit score data set (Board of Governors of the Federal Reserve System (US), 2007; Hardt et al., 2016b), we convert the cumulative distribution function (CDF) of TransRisk score among demographic groups (denoted as A, including Black, Asian, Hispanic, and White) into group-dependent densities of the credit score. We then generate a balanced sample where each group has equal representation, with credit scores (denoted as Q) initialized by sampling from the corresponding group-dependent density. The value of attributes for each data point is then updated under a specified dynamics (detailed in Appendix C.2.1) to model the real-world scenario of repeated resource allocation (with decision denoted as D).\n\nC.2.1 PARAMETERS FOR DYNAMICS\n\nSince we are considering the dynamic setting, we further specify the data generating process in the following way (from time step T = t to T = t + 1):\n\nXt,1 Xt,2\n\n∼\n\n∼\n\n1.5Qt + U [ 0.8At + U [\n\n−\n\n(cid:15)1, (cid:15)1] (cid:15)2, (cid:15)2]\n\n− (0, σ2)\n\n∼\n\nAt + Bernoulli(qt) for a given value of Qt = qt\n\nXt,3 Yt Dt = ft(At, Xt,1, Xt,2, Xt,3)\n\nN\n\n∼\n\nQt\n\nQt+1 = At+1 = At (fixed population)\n\n[1 + αD(Dt) + αY (Yt)]\n\n{\n\n·\n\n}(0,1]\n\n{·}(0,1] represents truncated value between the interval (0, 1], ft(\n\nwhere ) represents the decision policy from input features, and (cid:15)1, (cid:15)2, σ are parameters of choices. In our experiments, we set (cid:15)1 = (cid:15)2 = σ = 0.1.\n\n·\n\nWithin the same time step, i.e., for variables that share the subscript t, Qt and At are root causes for all other variables (Xt,1, Xt,2, Xt,3, Dt, Yt). At each time step T = t, the institution first estimates the credit score Qt (which is not directly visible to the institution, but is reflected in the visible outcome label Yt) based on (At, Xt,1, Xt,2, Xt,3), then produces the binary decision Dt according to the optimal threshold (in terms of the accuracy).\n\nFor different time steps, e.g., from T = t to T = t + 1, the new distribution at T = t + 1 is induced by the deployment of the decision policy Dt. Such impact is modeled by a multiplicative update in Qt+1 from Qt with parameters (or functions) αD( ) that depend on Dt and Yt, respectively. In ) and αY ( ·\n· our experiments, we set αD = 0.01 and αY = 0.005 to capture the scenario where one-step influence of the decision on the credit score is stronger than that for ground truth label.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\n(a) L1 penalty, strong regularization strength.\n\n(b) L1 penalty, strong regularization strength.\n\n(c) L1 penalty, medium regularization strength.\n\n(d) L1 penalty, medium regularization strength.\n\n(e) L1 penalty, weak regularization strength.\n\n(f) L1 penalty, weak regularization strength.\n\nT ), ErrD(h∗\n\nErrDS (h∗ {\n\nFigure 5: Results of applying L1 penalty with different strength when constructing h∗ S. := and LB := lower bound specified in Theorem 4.6. The\n\nThe left column consisting of panels (a), max right column consisting of panels (b), (d), and (f) compares Diff := ErrD(h∗ −\nT ) and UB := upper bound specified in Theorem 4.2. For each time step ErrD(h∗ K = k, we compute and deploy the source optimal classifier h∗ S and update the credit score for each individual according to the received decision as the new reality for time step K = k + 1.\n\n(c), and (e) compares Max\n\nS )(h∗ S)\n\nT )(h∗\n\nT )(h∗\n\nT )\n\n}\n\nC.2.2 ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we present additional experimental results on the real-world FICO credit score data set. With the initialization of the distribution of credit score Q and the specified dynamics, we present results comparing the influence of vanilla regularization terms in decision-making (when estimating the credit score Q) on the calculation of bounds for induced risks.8 In particular, we consider L1 norm (Figure 5) and L2 norm (Figure 6) regularization terms when optimizing decision-making policies on the source domain. As we can see from the results, applying vanilla regularization terms (e.g., L1 norm and L2 norm) on source domain without specific considerations of the inducing-risk mechanism does not provide significant performance improvement in terms of smaller induced risk. For example, there is no significant decrease of the term Diff as the regularization strength increases, for both L1 norm (Figure 5) and L2 norm (Figure 6) regularization terms.\n\n8The regularization that involves induced risk considerations will be discussed in Appendix F.\n\n27\n\n012345K10−210−1ValueMaxLB012345K10−210−1ValueDiffUB012345K10−210−1ValueMaxLB012345K10−210−1ValueDiffUB012345K10−210−1ValueMaxLB012345K10−210−1ValueDiffUBUnder review as a conference paper at ICLR 2023\n\n(a) L2 penalty, strong regularization strength.\n\n(b) L2 penalty, strong regularization strength.\n\n(c) L2 penalty, medium regularization strength.\n\n(d) L2 penalty, medium regularization strength.\n\n(e) L2 penalty, weak regularization strength.\n\n(f) L2 penalty, weak regularization strength.\n\nT ), ErrD(h∗\n\nErrDS (h∗ {\n\nFigure 6: Results of applying L2 penalty with different strength when constructing h∗ S. := and LB := lower bound specified in Theorem 4.6. The\n\nThe left column consisting of panels (a), max right column consisting of panels (b), (d), and (f) compares Diff := ErrD(h∗ −\nT ) and UB := upper bound specified in Theorem 4.2. For each time step ErrD(h∗ K = k, we compute and deploy the source optimal classifier h∗ S and update the credit score for each individual according to the received decision as the new reality for time step K = k + 1.\n\n(c), and (e) compares Max\n\nS )(h∗ S)\n\nT )(h∗\n\nT )(h∗\n\nT )\n\n}\n\nD CHALLENGES IN MINIMIZING INDUCED RISK\n\nD.1 COMPUTATIONAL CHALLENGES\n\nThe literature of domain adaptation has provided us solutions to minimize the risk on the target distribution via a nicely developed set of results Sugiyama et al. (2008; 2007); Shimodaira (2000). This allows us to extend the solutions to minimize the induced risk too. Nonetheless we will highlight additional computational challenges.\n\nWe focus on the covariate shift setting. The scenario for target shift is similar. For covariate shift, recall that earlier we derived the following fact:\n\nED(h)[(cid:96)(h; X, Y )] = ED[ωx(h)\n\n(cid:96)(h; x, y)]\n\n·\n\nThis formula informs us that a promising solution that uses ωx(h) to perform reweighted ERM. Of course, the primary challenge that stands in the way is how do we know ωx(h). There are different (h) Zhang et al. methods proposed in the literature to estimate ωx(h) when one has access to (2013b); Long et al. (2016); Gong et al. (2016). How any of the specific techniques work in our induced domain adaptation setting will be left for a more thorough future study. In this section, we focus on explaining the computational challenges even when such knowledge of ωx(h) can be obtained for each model h being considered during training.\n\nD\n\n28\n\n012345K10−310−210−1ValueMaxLB012345K10−210−1ValueDiffUB012345K10−310−210−1ValueMaxLB012345K10−310−210−1ValueDiffUB012345K10−310−210−1ValueMaxLB012345K10−210−1ValueDiffUBUnder review as a conference paper at ICLR 2023\n\nThough ωx(h), (cid:96)(h; x, y) might both be convex with respect to (the output of) the classifier h, their product is not necessarily convex. Consider the following example: Example 1 (ωx(h) x\nmodel). Notice that (cid:96) is convex in h. Let\n\n= (0, 1]. Let the true label of each X\ny)2, and let h(x) = x (simple linear be the uniform distribution, whose density function is\n\n(cid:96)(h; x, y) is generally non-convex). Let (cid:1). Let (cid:96)(h; x, y) = 1\n\nbe y(x) = 1 (cid:0)x\n\n2 (h(x)\n\n∈ X\n\n−\n\n≥\n\n1 2\n\n·\n\nfD =\n\n(cid:26)1, 0 < x\n\n1 0, otherwise\n\n≤\n\n. Notice that if the training data is drawn from\n\n, then h is the linear classifier\n\nD\n\nthat minimizes the expected loss. Suppose that, since h rewards large values of x, it induces decision subjects to shift towards higher feature values. In particular, let\n\n(h) have density function\n\nfD(h) =\n\n(cid:26)2x, 0,\n\n0 < x 1\notherwise\n\n≤\n\nD\n\nThen for all x\n\n, ωx(h) =\n\n∈ X\n\nfD(h)(x)\n\nfD(x) = 2x. Notice that ωx(h) = 2x is convex in h(x) = x. Then\n\nD\n\nωx(h)\n\n·\n\n(cid:96)(h; x, y) = 2x\n\n1 2\n\n·\n\n(h(x)\n\ny)2\n\n− (cid:26)x3, x(x\n\ny)2 =\n\n0 < x < 1 2\nx 1\n\n1\n\n2 ≤\n\n≤\n\n1)2,\n\n−\n\nwhich is clearly non-convex.\n\n= x(x\n\n−\n\nNonetheless, we provide sufficient conditions under which ωx(h) Proposition D.1. Suppose ωx(h) and (cid:96)(h; x, y) are both convex in h, and ωx(h) and (cid:96)(h; x, y) satisfy (cid:96)(h; x, y) is convex.\n\n(cid:96)(h; x, y) is in fact convex:\n\nh, h(cid:48), x, y: (ωx(h)\n\n0. Then ωx(h)\n\n(cid:96)(h(cid:48); x, y))\n\n((cid:96)(h; x, y)\n\nωx(h(cid:48)))\n\n≥\n\n−\n\n−\n\n∀\n\n·\n\n·\n\n·\n\nProof. Let us use the shorthand ω(h) := ωx(h) and (cid:96)(h) := (cid:96)(h; x, y). To show that ω(h) convex, it suffices to show that for any α\n\n[0, 1] and any two hypotheses h, h(cid:48) we have\n\n(cid:96)(h) is\n\n·\n\n∈\n\n(cid:96)(α\n\n·\n\nh + (1\n\nα)\n\n·\n\n−\n\nh(cid:48))\n\nα\n\n·\n\n≤\n\nω(h)\n\n·\n\n(cid:96)(h) + (1\n\nα)\n\n·\n\n−\n\nω(h(cid:48))\n\n(cid:96)(h(cid:48))\n\n·\n\nω(α\n\n·\n\nh + (1\n\nα)\n\n·\n\n−\n\nh(cid:48))\n\nBy the convexity of ω,\n\nω(α\n\nand by the convexity of (cid:96),\n\n(cid:96)(α\n\n·\n\n·\n\n·\n\nh + (1\n\nα)\n\n−\n\nh + (1\n\nα)\n\n−\n\n·\n\n·\n\nh(cid:48))\n\nα\n\n·\n\n≤\n\nω(h) + (1\n\nα)\n\n−\n\nh(cid:48))\n\nα\n\n·\n\n≤\n\n(cid:96)(h) + (1\n\nα)\n\n−\n\nω(h(cid:48))\n\n(cid:96)(h(cid:48))\n\n·\n\n·\n\nTherefore it suffices to show that\n\nω(h) + (1\n\nα)\n\nω(h(cid:48))]\n\n[α\n\n[α\n\n⇔\n\n⇔\n\n· α(α\n\n−\n\nα(α\n\n− [ω(h)\n\n1)\n\n1)\n\n·\n\n·\n\n·\n\n− ω(h)(cid:96)(h)\n\n· α(α\n\n[ω(h)\n\nω(h(cid:48))]\n\n− ω(h(cid:48))]\n\n·\n\n− [(cid:96)(h)\n\n− [(cid:96)(h)\n\n· (cid:96)(h(cid:48))]\n\n− 0\n\n(cid:96)(h(cid:48))]\n\n0\n\n≤\n\n·\n\n(cid:96)(h) + (1\n\nα)\n\n(cid:96)(h(cid:48))] ·\n[ω(h)(cid:96)(h(cid:48)) + ω(h(cid:48))(cid:96)(h)] + α(α\n\nω(h)\n\n−\n\n−\n\nα\n\n·\n\n·\n\n1)\n\n(cid:96)(h) + (1\n\nα)\n\nω(h(cid:48))\n\n1)\n\n·\n\n−\n\n−\n\n·\n\nω(h(cid:48))(cid:96)(h(cid:48))\n\n0\n\n≤\n\n(cid:96)(h(cid:48))\n\n·\n\n0\n\n≤\n\n⇔ By the assumed condition, the left-hand side is indeed non-negative, which proves the claim.\n\n≥\n\n−\n\n−\n\n·\n\nThis condition is intuitive when each x belongs to a rational agent who responds to a classifier h to maximize her chance of being classified as +1: For y = +1, the higher loss point corresponds to the ones that are close to decision boundary, therefore, more 1 negative label points might shift to it, resulting to a larger ωx(h). For y = 1, the higher loss point corresponds to the ones that are likely mis-classified as +1, which “attracts” instances to deviate to.\n\n−\n\n−\n\nD.2 CHALLENGES DUE TO THE LACK OF ACCESS TO DATA\n\nWe discuss the challenges in performing induced domain adaptation. In the standard domain adaptation settings, one often assumes the access to a sample set of X, which already poses challenges\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nwhen there is no access to label Y after the adaptation. Nonetheless, the literature has observed a fruitful development of solutions Sugiyama et al. (2008); Zhang et al. (2013b); Gong et al. (2016).\n\nOne might think the above idea can be applied to our IDA setting rather straightforwardly by (h), the induced distribution under each model h during the assuming observing samples from training. However, we often do not know precisely how the distribution would shift under a model h until we deploy it. This is particularly true when the distribution shifts are caused by human responding to a model. Therefore, the ability to “predict” accurately how samples “react” to h plays a very important role Ustun et al. (2019). Indeed, the strategic classification literature enables this capability by assuming full rational human agents. For a more general setting, building robust domain adaptation tools that are resistant to the above “prediction error” is also going to be a crucial criterion.\n\nD\n\nE DISCUSSIONS ON PERFORMING DIRECT INDUCED RISK MINIMIZATION\n\nIn this section, we provide discussions on how to directly perform induced risk minimization for our induced domain adaptation setting. We first provide a gradient descent based method for a particular label shift setting where the underlying dynamic is replicator dynamic described in Section 5.3. Then we propose a solution for a more general induced domain adaptation setting where we do not make any particular assumptions on the undelying distribution shift model.\n\nE.1 GRADIENT DESCENT BASED METHOD\n\nHere we provide a toy example of performing direct induced risk minimization under the assumption of label shift with underlying dynamics as the replicator dynamics described in Section 5.3.\n\n∈\n\nR and a binary true qualification y\n\nSetting Consider a simple setting in which each decision subject is associated with a 1-dimensional continuous feature x . We assume label shift setting, and the underlying population dynamic evolves the replicator dynamic setting described in Section 5.3. We consider a simple threshold classifier, where ˆY = h(x) = 1[X θ], meaning that the classifier is completely characterized by the threshold parameter θ. Below we will use ˆY and h(X) interchangeably to represent the classification outcome. Recall that the replicator dynamics is specified as follows:\n\n∈ {−\n\n1, +1\n\n≥\n\n}\n\nPD(h)(Y = y) PDS (Y = y)\n\n=\n\nFitness(Y = y) EDS [Fitness(Y )]\n\n(22)\n\nPDS (Y = y)). where EDS [Fitness(Y )] = Fitness(Y = y)PDS (Y = y) + Fitness(Y = Fitness(Y = y) is the fitness of strategy Y = y, which is further defined in terms of the expected utility Uy,ˆy of each qualification-classification outcome pair (y, ˆy):\n\ny)(1\n\n−\n\n−\n\nFitness(Y = y) :=\n\n(cid:88)\n\nˆy\n\nP[ ˆY = ˆy Y = y] |\n\n·\n\nUy,ˆy\n\nthe utility (or each qualification-classification outcome Y = y) is sampled according to a Gaussian distribution, and will be |\n\nwhere Uy,ˆy is combination.P(X unchanged since we consider a label shift setting. We initialize the distributions we specify the initial qualification rate PDS (Y = +1). To test different settings, we vary the specification of the utility matrix Uy,ˆy and generate different dynamics.\n\nreward)\n\nfor\n\nFormulate the induced risk as a function of h To minimize the induced risk, we first formulate the induced risk as a function of the classifier h’s parameter θ taking into account of the underlying dynamic, and then perform gradient descent to solve for locally optimal classifier h∗ T .\n\nRecall from Section 5, under label shift, we can rewrite the induced risk as the following form:\n\n·\n\nEDS [(cid:96)(h; X, Y )\n\nED(h)[(cid:96)(h; X, Y )] =p(h) where p(h) = PD(h)(Y = +1). Since EDS [(cid:96)(h; X, Y ) |\nS, it suffices to show that the accuracy on S.\n\nD a function of θ and\n\nD\n\nY = +1] and EDS [(cid:96)(h; X, Y )\n\nD\n\nY = +1] + (1 |\n\n−\n\np(h))\n\n·\n\nEDS [(cid:96)(h; X, Y )\n\nY = |\n\n−\n\n1]\n\n1] are already functions of both h and (h), p(h) = PD(h)(Y = +1), can also be expressed as\n\nY = |\n\n−\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nTo see this, recall that for a threshold classifier ˆY = 1[X > θ], it means that the prediction accuracy can be written as a function of the threshold θ and target distribution\n\n(h):\n\nD\n\nPD(h)(Y = +1)\n\n= PD(h)( ˆY = +1, Y = +1) + PD(h)( ˆY = = PD(h)(X (cid:90) ∞\n\nθ, Y = +1) + PD(h)(X\n\n≤ PD(h)(Y = +1) P(X = x Y = 1) |\n(cid:125) (cid:123)(cid:122) unchanged because of label shift\n\n=\n\nθ\n\n−\n\n≥\n\n(cid:124)\n\nθ, Y =\n\ndx\n\n1, Y =\n\n1)\n\n−\n\n1)\n\n−\n\n(cid:90) θ\n\n+\n\n−∞\n\nPD(h)(Y =\n\n−\n\n1) P(X = x\n\nY = |\n(cid:123)(cid:122) unchanged because of label shift\n\n1) (cid:125)\n\n−\n\n(cid:124)\n\ndx\n\n(23)\n\nwhere P(X Y = y) remains unchanged over time, and PD(h)(Y = y) evolves over time according |\nto Equation (22), namely\n\nPD(h)(Y = y)\n\n=PDS (Y = y)\n\n=PDS (Y = y)\n\n×\n\n×\n\nFitnessg(Y = y) EDS [Fitnessg(Y )] (cid:80)\n\n(cid:80)\n\ny((cid:80)\n\nˆy\n\nPDS [ ˆY = ˆy |\n\nˆy\n\nPDS [ ˆY = ˆy |\n\nY = y, G = g]\n\nUˆy,y\n\nY = y, G = g]\n\nUˆy,y)PDS [Y = y]\n\n·\n\n·\n\n(24)\n\nNotice that ˆY is only a function of θ, and Uy,ˆy are fixed quantities, the above derivation indicates that we can express PD(h)(Y = y) as a function of θ and S. Plugging it back to Equation (23), we can see that the accuracy can also be expressed as a function of the classifier’s parameter θ, indicating that the induced risk can be expressed as a function of θ. Thus we can use gradient descent using automatic differentiation w.r.t θ to find a optimal classifier h∗\n\nT that minimize the induced risk.\n\nD\n\nFigure 7: Experimental results of directly optimizing for the induced risk under the assumption of replicator dynamic. The X-axis denotes the prediction accuracy of ErrD(h∗ S is the source optimal classifier under each settings. The Y-axis is the percent of performance improvement using the classifier that optimize for h∗ T = arg min ErrD(h)(h), which the decision maker considers the underlying response dynamics (according to replicator dynamics in Equation (22)) of the decision subjects. Different color represents different utility function, which is reflected by the specifications of values in Uy,ˆy; within each color, different dots represent different initial qualification rate.\n\nS), where h∗\n\nS )(h∗\n\nExperimental Results Figure 7 shows the experimental results for this toy example. We can see that for each setting, compared to the baseline classifier h∗ S, the proposed gradient based optimization\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nprocedure returns us a classifier that achieves a better prediction accuracy (thus lower induced risk) compared to the accuracy of the source optimal classifier.\n\nE.2 GENERAL SETTING: INDUCED RISK MINIMIZATION WITH BANDIT FEEDBACK\n\nD\n\nIn general, finding the optimal classifier that achieves the optimal induced risk h∗ T is a hard problem due to the interactive nature of the problem (see, e.g. the literature of performative prediction Perdomo et al. (2020) for more detailed discussions). Without making any assumptions on the mapping between (h), one can only potentially rely on the bandit feedbacks from the decision subjects to h and estimate the influence of h on (h): when the induced risk is a convex function of the classifier h’s parameter θ, one possible approach is to use the standard techniques from bandit optimization (Flaxman et al., 2004) to iteratively find induced optimal classifier h∗ T . The basic idea is: at each step t = 1, (ht) and their losses, and use them to construct an approximate gradient for the induced risk as a function of the model parameter θt. When the induced risk is a convex function in the model parameter θ, the above approach guarantees to converge to h∗ T , and have sublinear regret in the total number of steps T .\n\n, T , the decision maker deploy a classifier ht, then observe data points sampled from\n\n· · ·\n\nD\n\nD\n\nThe detailed description of the algorithm for finding h∗\n\nT is as follows:\n\nAlgorithm 1: One-point bandit gradient descent for performative prediction Result: return θT after T rounds θ1 foreach time step t\n\n0\n\n∼\n\n←\n\nUnif(S)\n\n1, . . . , T do\n\n← Sample a unit vector ut θ+ θt + δut t ← Observe data points z1, . . . , znt ∼ D (cid:101)IR(θ+ t ) ̃gt(θt) θt+1 (1\n\ni=1 (cid:96)(zi; θ+ t ) δ (cid:101)IR(θ+ ut t ) ·\nΠ(1−δ)Θ(θt\n\nη ̃gt(θt)) θ\nΘ\n\n← δ)Θ :=\n\n− δ)θ\n\n(cid:80)nt\n\n← d\n\n1 nt\n\n←\n\n(1\n\n{\n\n−\n\n|\n\n∈\n\n}\n\n−\n\nend\n\n(θ+ t )\n\n(cid:46) ̃gt(θt) is an approximation of\n\nθ (cid:98)IR(θt) (cid:46) Take gradient step; project onto\n\n∇\n\nF REGULARIZED TRAINING\n\nIn this section, we discuss the possibility that indeed minimizing regularized risk will lead to a tighter upper bound. Consider the target shift setting. Recall that p(h) := PD(h)(Y = +1) and we have for any proper loss function (cid:96):\n\nED(h)[(cid:96)(h; X, Y )] = p(h)\n\nEDS [(cid:96)(h; X, Y ) |\n\n·\n\nY = +1] + (1\n\np(h))\n\nEDS [(cid:96)(h; X, Y ) |\n\n·\n\nY =\n\n1]\n\n−\n\n−\n\nSuppose p < p(h∗ a smaller upper bound.\n\nT ), now we claim that minimizing the following regularized/penalized risk leads to\n\nEDS [(cid:96)(h; X, Y )] + α\n\nh(X) + 1 2\nuniform is a distribution with uniform prior for Y .\n\nEDuniform||\n\n·\n\n||\n\nwhere in above\n\nD\n\nWe impose the following assumption:\n\n• The number of predicted +1 for examples with Y = +1 and for examples with Y =\n\nare monotonic with respect to α.\n\n1\n\n−\n\nConsider the easier setting with (cid:96) = 0-1 loss. Then\n\nEDuniform||\n\nh(X)\n\n||\n\n= 0.5\n\n= 0.5\n\n·\n\n·\n\n(PX|Y =+1[h(X) = +1] + PX|Y =−1[h(X) = +1]) (EX|Y =+1[(cid:96)(h(X), +1)] 1])\n\nEX|Y =−1[(cid:96)(h(X),\n\n−\n\n0.5\n\n−\n\n−\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nThe above regularized risk minimization problem is equivalent to\n\n(p + 0.5\n\nα)\n\nEX|Y =+1[(cid:96)(h(X), +1)] + (p\n\n· Recall the upper bound in Theorem 5.1:\n\n·\n\n0.5\n\nα)\n\n·\n\n·\n\n−\n\nEX|Y =−1[(cid:96)(h(X),\n\n1]\n\n−\n\nErrD(h∗\n\nS )(h∗ S)\n\n−\n\nErrD(h∗\n\nT )(h∗\n\nT )\n\np(h∗ ≤ | (cid:124)\n\nS)\n\np(h∗\n\nT ) |\n(cid:125)\n\n− (cid:123)(cid:122) Term 1\n\n+ (1 + p)\n\n·\n\n+(h∗\n\nS),\n\n(dTV( (cid:124)\n\nD\n\nD\n\n+(h∗\n\nT )) + dTV( (cid:123)(cid:122) Term 2\n\n−(h∗\n\nS),\n\nD\n\nD\n\n−(h∗\n\n.\n\nT )) (cid:125)\n\nWith a properly specified α > 0, this leads to a distribution with a smaller gap of ,\nT ) |\nwhere ̃hS denotes the optimal classifier of the penalized risk minimization - this leads to a smaller Term 1 in the bound of Theorem 5.1. Furthermore, the induced risk minimization problem will correspond to an α s.t. α∗ = p(h∗ , and the original h∗ S corresponds to a distribution of α = 0. Using the monotonicity assumption, we will establish that the second term in Theorem 5.1 will also smaller when we tune a proper α.\n\nT )−p 0.5\n\n−\n\np(h∗\n\np( ̃hS) |\n\nG DISCUSSION ON THE TIGHTNESS OF OUR THEORETICAL BOUNDS\n\nGeneral Bounds in Section 3 For the general bounds reported in Section 3, it is not trivial to fully quantify the tightness without further quantifying the specific quantities of the terms, e.g. the H divergence of the source and the induced distribution, and the average error a classifier have to incur for both distribution. This part of our results adapted from the classical literature in learning from multiple domains Ben-David et al. (2010). The tightness of using -divergence and other terms seem to be partially validated therein.\n\nH\n\nBounds in Section 4 and Section 5 For more specific bounds provided in Section 4 (for covariate shift) and Section 5 (target shift), however, it is relatively easier to argue about the tightness: the proofs there are more transparent and are easier to back out the conditions where the inequalities are relaxed. For example, in Theorem 5.1, the inequalities of our bound are introduced primarily in the following two places: 1) one is using the optimiality of h∗ S on the source distribution. 2) the other is bounding the statistical difference in h∗ T ’s predictions on the positive and negative examples. Both are saying that if the differences in the two classifiers’ predictions are bounded in a range, then the result in Theorem 5.1 is relatively tight.\n\nS and h∗\n\n33",
    "reference": "# Summary Of The Paper\n\nThis paper formulates a very interesting and novel problem (IDA, induced domain adapatation) in transfer learning. Consider the supervised classification setting where one usually trains a classifier $h : X \\mapsto Y$ from samples $\\{(X, Y)\\}$ drawn from some distribution $\\sim \\mathcal D$. Oftentimes, when the data are generated from human input, the human could possibly modify $(X, Y)$ to adapt to $h$, resulting in a distribution shift over $\\mathcal D$. Therefore, when training $h$, it is important to take this distribution shift into consideration. However, this can become complicated and interactive if the human further adapt to $h$.\n\nThis paper conducts a rather detailed study on this problem. It proves upper and lower bounds for the transfer risks for several important fundamental questions in this setting, as outlined in page 2. Besides, the paper realizes their bounds by both showing how to compute them in practice and computing them on real datasets.\n\n# Strength And Weaknesses\n\n### Strength\n\n1. The setting studied in this paper is important, interesting, and novel.\n2. This paper studies most fundamental questions in this setting and presents satisfactory results that covers several fundamental lower and upper bounds in this setting.\n3. As a mainly theoretical paper, this paper is aware of the practical impact of their theoretical results. To this end, this paper shows how to compute their upper bounds in real-world problems and conducts experiments to demonstrate their results. Furthermore, the experimental results corroborates their theoretical results and suggests that the theoretical results could give meaningful upper and lower bounds for the transfer risks.\n\n### Weakness\n\n1. The proof techniques look simple and the techniques themselves might not inspire broader community.\n2. The authors could possibly conduct experiments on more datasets to strengthen their experimental results.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing is good. No apparaent typos are noticed. The authors supplies sufficient materials (codes) for reproducing the experimental results in this paper.\n\n# Summary Of The Review\n\nThis paper has good theoretical and empirical results, yet the techniques might not be exciting and the experiments might not be convincing enough due to paucity of datasets evaluated.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "REGRESSION WITH LABEL DIFFERENTIAL PRIVACY ∗\n\nBadih Ghazi\n\nPritish Kamath\n\nRavi Kumar\n\nEthan Leeman\n\nPasin Manurangsi\n\nAvinash Varadarajan\n\nChiyuan Zhang\n\nGoogle\n\nABSTRACT\n\nWe study the task of training regression models with the guarantee of label differential privacy (DP). Based on a global prior distribution on label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a “randomized response on bins”, and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm.\n\n1\n\nINTRODUCTION\n\nIn recent years, differential privacy (DP, Dwork et al., 2006a;b) has emerged as a popular notion of user privacy in machine learning (ML). On a high level, it guarantees that the output model weights remain statistically indistinguishable when any single training example is arbitrarily modified. Numerous DP training algorithms have been proposed, with open-source libraries tightly integrated in popular ML frameworks such as TensorFlow Privacy (Radebaugh & Erlingsson, 2019) and PyTorch Opacus (Yousefpour et al., 2021).\n\nIn the context of supervised ML, a training example consists of input features and a target label. While many existing research works focus on protecting both features and labels (e.g., Abadi et al. (2016)), there are also some important scenarios where the input features are already known to the adversary, and thus protecting the privacy of the features is not needed. A canonical example arises from computational advertising where the features are known to one website (a publisher), whereas the conversion events, i.e., the labels, are known to another website (the advertiser).1 Thus, from the first website’s perspective, only the labels can be treated as unknown and private. This motivates the study of label DP algorithms, where the statistical indistinguishability is required only when the label of a single example is modified.2 The study of this model goes back at least to the work of Chaudhuri & Hsu (2011). Recently, several works including (Ghazi et al., 2021a; Malek Esmaeili et al., 2021) studied label DP deep learning algorithms for classification objectives.\n\nIn this work, we study label DP for regression tasks. We provide a new Our Contributions. algorithm that, given a global prior distribution (which, if unknown, could be estimated privately), derives a label DP mechanism that is optimal under a given objective loss function. We provide an explicit characterization of the optimal mechanism for a broad family of objective functions including the most commonly used regression losses such as the Poisson log loss, the mean square error, and the mean absolute error.\n\nMore specifically, we show that the optimal mechanism belongs to a class of randomized response on bins (Algorithm 1). We show this by writing the optimization problem as a linear program (LP), and characterizing its optimum. With this characterization in mind, it suffices for us to compute the\n\n∗Authors in alphabetical order. Email: {badih.ghazi, ravi.k53}@gmail.com, {pritishk, ethanleeman,\n\npasin, avaradar, chiyuan}@google.com\n\n1A similar use case is in mobile advertising, where websites are replaced by apps. 2We note that this label DP setting is particularly timely and relevant for ad attribution and conversion measurement given the deprecation of third-party cookies by several browsers and platforms (Wilander, 2020; Wood, 2019; Schuh, 2020).\n\n1\n\nFigure 1: Learning with feature-oblivious label DP.\n\noptimal mechanism among the class of randomized response on bins. We then provide an efficient algorithm for this task, based on dynamic programming (Algorithm 2).\n\nIn practice, a prior distribution on the labels is not always available. This leads to our two-step algorithm (Algorithm 3) where we first use a portion of the privacy budget to build an approximate histogram of the labels, and then feed this approximate histogram as a prior into the optimization algorithm in the second step; this step would use the remaining privacy budget. We show that as the number of samples grows, this two-step algorithm yields an expected loss (between the privatized label and the raw label) which is arbitrarily close to the expected loss of the optimal local DP mechanism. (We give a quantitative bound on the convergence rate.)\n\nOur two-step algorithm can be naturally deployed in the two-party learning setting where each example is vertically partitioned with one party holding the features and the other party holding the (sensitive) labels. The algorithm is in fact one-way, requiring a single message to be communicated from the labels party to the features party, and we require that this one-way communication satisfies (label) DP. We refer to this setting, which is depicted in Figure 1, as feature-oblivious label DP.\n\nWe evaluate our algorithm on three datasets: the 1940 US Census IPUMS dataset, the Criteo Sponsored Search Conversion dataset, and a proprietary app install ads dataset from a commercial mobile app store. We compare our algorithm to several baselines, and demonstrate that it achieves higher utility across all test privacy budgets, with significantly lower test errors for the high privacy regime. For example, for privacy budget ε = 0.5, comparing to the best baseline methods, the test MSE for our algorithm is ∼ 1.5× smaller on the Criteo and US Census datasets, and the relative test error is ∼ 5× smaller on the app ads dataset.\n\nOrganization. In Section 2, we recall some basics of DP and learning theory, and define the feature-oblivious label DP setting in which our algorithm can be implemented. Our label DP algorithm for regression objectives is presented in Section 3. Our experimental evaluation and results are described in Section 4. A brief overview of related work appears in Section 5. We conclude with some interesting future directions in Section 6. Most proofs are deferred to the Appendix (along with additional experimental details and background material).\n\n2 PRELIMINARIES\n\nWe consider the standard setting of supervised learning, where we have a set of examples of the form (x, y) ∈ X × Y, drawn from some unknown distribution D and we wish to learn a predictor fθ (parameterized by θ) to minimize L(fθ) := E(x,y)∼D (cid:96)(fθ(x), y), for some loss function (cid:96) : R × Y → R≥0; we will consider the case where Y ⊆ R. Some common loss functions include the zeroone loss (cid:96)0-1( ̃y, y) := 1[ ̃y (cid:54)= y], the logistic loss (cid:96)log( ̃y, y) := log(1+e− ̃yy) for binary classification, and the squared loss (cid:96)sq( ̃y, y) := 1 2 ( ̃y−y)2, the absolute-value loss (cid:96)abs( ̃y, y) := | ̃y−y|, the Poisson log loss (cid:96)Poi( ̃y, y) := ̃y − y · log( ̃y) for regression. This paper focuses on the regression setting.\n\nHowever, we wish to perform this learning with differential privacy (DP). We start by recalling the definition of DP, which can be applied to any notion of adjacent pairs of datasets. For an overview of DP, we refer the reader to the book of Dwork & Roth (2014).\n\nDefinition 1 (DP; Dwork et al. (2006b)). Let ε be a positive real number. A randomized algorithm A taking as input a dataset is said to be ε-differentially private (denoted ε-DP) if for any two adjacent datasets X and X (cid:48), and any subset S of outputs of A, we have Pr[A(X) ∈ S] ≤ eε ·Pr[A(X (cid:48)) ∈ S].\n\nIn supervised learning applications, the input to a DP algorithm is the training dataset (i.e., a set of labeled examples) and the output is the description of a predictor (e.g., the weights of the trained\n\n2\n\nM(y1, ..., yn)ε-DPTrained ε-LabelDP modelLabels Partyy1, ..., ynStep 1Step 2Features Partyx1, ..., xnmodel). Typically, two datasets are considered adjacent if they differ on a single training example; this notion protects both the features and the label of any single example. As discussed in Section 1, in certain situations, protecting the features is either unnecessary or impossible, which motivates the study of label DP that we define next. Definition 2 (Label DP; Chaudhuri & Hsu (2011)). An algorithm A taking as input a training dataset is said to be ε-label differentially private (denoted as ε-LabelDP) if it is ε-DP when two input datasets are considered adjacent if they differ on the label of a single training example.\n\nWe next define an important special case of training with label DP, which commonly arises in practice. The setting could be viewed as a special case of vertical federated learning, where each training example is divided across different parties at the beginning of the training process; see, e.g., the survey of Kairouz et al. (2021) and the references therein. We consider the special case where we only allow a single round of communication from the first party who holds all the labels, to the second party, who holds all the features, and who then trains the ML model and outputs it. We refer to this setting (depicted in Figure 1) as feature-oblivious label DP since the label DP property is guaranteed without having to look at the features (but with the ability to look at the labels from all users); we next give the formal definition. Definition 3 (Feature-Oblivious Label DP). Consider the two-party setting where the “features party” holds as input a sequence (xi)n i=1 of all feature vectors (across all the n users), and the “labels party” holds as input a sequence (yi)n i=1 of the corresponding labels. The labels party sends a single message M (y1, . . . , yn) to the features party; this message can be randomized using the internal randomness of the labels party. Based on its input and on this incoming message, the features party then trains an ML model that it outputs. We say that this output is feature-oblivious ε-LabelDP if the message M (y1, . . . , yn) is ε-DP with respect to the adjacency relation where a single yi can differ across the two datasets.\n\nWe stress the practical applicability of the setting described in Definition 3. The labels party could be an advertiser who observes the purchase data of the users, and wants to enable a publisher (the features party) to train a model predicting the likelihood of a purchase (on the advertiser) being driven by the showing of a certain type of ad (on the publisher). The advertiser would also want to limit the leakage of sensitive information to the publisher. From a practical standpoint, the simplest option for the advertiser is to send a single privatized message to the publisher who can then train a model using the features at its disposal. This is exactly the feature-oblivious label DP setting.\n\n3 LABEL DP LEARNING ALGORITHM\n\nA common template for learning with label DP is to: (i) compute noisy labels using a local DP mechanism M, (ii) use a learning algorithm on the dataset with noisy labels. All of the baseline algorithms that we consider follow this template, through different ways of generating noisy labels, such as, (a) randomized response (for categorical labels), (b) (continuous/discrete) Laplace mechanism, (c) (continuous/discrete) staircase mechanism, etc. (for formal definitions of these mechanisms, see Appendix D). Intuitively, such a learning algorithm will be most effective when the noisy label mostly agrees with the true label.\n\nSuppose the loss function (cid:96) satisfies the triangle inequality, namely that (cid:96)( ̃y, y) ≤ (cid:96)(ˆy, y) + (cid:96)( ̃y, ˆy) for all y, ˆy, ̃y. Then we have that\n\nE (x,y)∼D\n\n(cid:96)(fθ(x), y) ≤\n\n(cid:96)(ˆy, y) +\n\nE y∼P ˆy∼M(y)\n\nE (x,y)∼D ˆy∼M(y)\n\n(cid:96)(fθ(x), ˆy) ,\n\n(1)\n\nwhere, for ease of notation, we use P to denote the marginal on y for (x, y) ∼ D. The learning algorithm, in part (ii) of the template above, aims to minimize the second term in the RHS of (1). Thus, it is natural to choose a mechanism M, in part (i) of the template, to minimize the first term in the RHS of (1).3 Ghazi et al. (2021a) studied this question for the case of 0-1 loss and showed that a randomized response on top k labels with the highest prior masses (for some k) is optimal. Their characterization was limited to this classification loss. In this work, we develop a characterization for a large class of regression loss functions.\n\n3Note that the first term in the RHS of (1) is a constant, independent of the number of training samples. Therefore, this upper bound is not vanishing in the number of samples, and hence this inequality can be quite loose.\n\n3\n\nAlgorithm 1 RR-on-BinsΦ ε .\n\nAlgorithm 2 Compute optimal Φ for RR-on-BinsΦ ε .\n\nParameters: Φ : Y → ˆY (for label set Y and output set ˆY), privacy parameter ε ≥ 0. Input: A label y ∈ Y. Output: ˆy ∈ ˆY.\n\nreturn a sample ˆy ∼ ˆY , where the random variable ˆY is distributed as (cid:40) eε\n\neε+| ˆY|−1 1\neε+| ˆY|−1\n\nif ˆy = Φ(y)\n\notherwise,\n\nPr[ ˆY = ˆy] =\n\nfor each ˆy ∈ ˆY\n\nInput: Distribution P over Y ⊆ R, privacy param. ε ≥ 0, loss function (cid:96) : R2 → R≥0. Output: Output set ˆY ⊆ R and Φ : Y → ˆY.\n\ny1, . . . , yk ← elements of Y in increasing order Initialize A[i][j] ← ∞ for all i, j ∈ {0, . . . , k} for r, i ∈ {1, . . . , k} do L[r][i] ← minˆy∈R (cid:80)\n\ny∈Y py ·e1[y∈[yr,yi]]·ε ·(cid:96)(ˆy, y)\n\nA[0][0] ← 0 for i ∈ {1, . . . , k} do\n\nfor j ∈ {1, . . . , i} do\n\nA[i][j] ← min0≤r<i A[r][j − 1] + L[r + 1][i]\n\nreturn Φ, ˆY correspond. to mind∈[k]\n\n1\n\nd−1+eε A[k][d]\n\n3.1 RANDOMIZED RESPONSE ON BINS: AN OPTIMAL MECHANISM\n\nWe propose a new mechanism for generating noisy labels that minimizes the first term in the RHS of (1). Namely, we define randomized response on bins (RR-on-BinsΦ ε ), which is a randomized algorithm parameterized by a scalar ε > 0 and a non-decreasing function Φ : Y → ˆY that maps the label set Y ⊆ R to an output set ˆY ⊆ R. This algorithm is simple: perform ε-randomized response on Φ(y), randomizing over ˆY; see Algorithm 1 for a formal definition. Any Φ we consider will be non-decreasing unless otherwise stated, and so we often omit mentioning it explicitly.\n\nAn important parameter in RR-on-Bins is the mapping Φ and the output set ˆY. We choose Φ with the goal of minimizing E (cid:96)(ˆy, y). First we show, under some basic assumptions about (cid:96), that for any given distribution P over labels Y, there exists a non-decreasing map Φ such that M = RR-on-BinsΦ ε minimizes Ey∼P,ˆy∼M(y) (cid:96)(ˆy, y). Since Φ is non-decreasing, it follows that Φ−1(ˆy) is an interval4 of Y, for all ˆy ∈ ˆY. Exploiting this property, we give an efficient algorithm for computing the optimal Φ. To state our results formally, we use L(M; P ) to denote Ey∼P,ˆy∼M(y) (cid:96)(ˆy, y), the first term in the RHS of (1). Our results hold under the following natural assumption; note that all the standard loss functions such as squared loss, absolute-value loss, and Poisson log loss satisfy Assumption 4. Assumption 4. Loss function (cid:96) : R × R → R≥0 is such that • For all y ∈ R, (cid:96)(·, y) is continuous. • For all y ∈ R, (cid:96)(ˆy, y) is decreasing in ˆy when ˆy ≤ y and increasing in ˆy when ˆy ≥ y. • For all ˆy ∈ R, (cid:96)(ˆy, y) is decreasing in y when y ≤ ˆy and increasing in y when y ≥ ˆy.\n\nWe can now state our main result: Theorem 5. For any loss function (cid:96) : R × R → R≥0 satisfying Assumption 4, all finitely supported distributions P over Y ⊆ R, there is an output set ˆY ⊆ R and a non-decreasing map Φ : Y → ˆY such that\n\nL(RR-on-BinsΦ\n\nε ; P ) = inf\n\nL(M; P ),\n\nM\n\nwhere the infimum is over all ε-DP mechanisms M.\n\nProof Sketch. This proof is done in two stages. First, we handle the case where ˆY is restricted to be a subset of O, where O is a finite subset of R. Under this restriction, the optimal mechanism M that minimizes L(M; P) can be computed as the solution to an LP. Since an optimal solution to an\n\n4An interval of Y is a subset of the form [a, b] ∩ Y, for some a, b ∈ R, consisting of consecutive elements\n\non Y in sorted order.\n\n4\n\nAlgorithm 3 Labels Party’s Randomizer LabelRandomizerε1,ε2.\n\nParameters: Privacy parameters ε1, ε2 ≥ 0. Input: Labels y1, . . . , yn ∈ Y. Output: ˆy1, . . . , ˆyn ∈ ˆY.\n\nε1\n\n(y1, . . . , yn)\n\nP (cid:48) ← MLap Φ(cid:48), ˆY (cid:48) ← Result of running Algorithm 2 with distribution P (cid:48) and privacy parameter ε2 for i ∈ [n] do\n\nˆyi ← RR-on-BinsΦ(cid:48)\n\n(yi)\n\nε2\n\nreturn (ˆy1, . . . , ˆyn)\n\nLP can always be found at the vertices of the constraint polytope, we study properties of the vertices of the said polytope and show that the minimizer amongst its vertices necessarily takes the form of ε . To handle the case of general ˆY, we first note that due to Assumption 4, it suffices to RR-on-BinsΦ consider ˆY ⊆ [ymin, ymax] (where ymin = miny∈Y y and ymax = maxy∈Y y). Next, we consider a sequence of increasingly finer discretizations of [ymin, ymax], and show that RR-on-BinsΦ ε can come arbitrarily close to the optimal L(M; P ) when restricting ˆY to be a subset of the discretized set. But observe that any Φ induces a partition of Y into intervals. Since there are only finitely many partitions of Y into intervals, it follows that in fact RR-on-BinsΦ ε can exactly achieve the optimal L(M; P ). The full proof is deferred to Appendix A.\n\nGiven Theorem 5, it suffices to focus on RR-on-BinsΦ ε mechanisms and optimize over the choice of Φ. We give an efficient dynamic programming-based algorithm for the latter in Algorithm 2. The main idea is as follows. We can breakdown the representation of Φ into two parts (i) a partition PΦ = {S1, S2, . . .} of Y into intervals5, such that Φ is constant over each Si, and (ii) the values yi that Φ(·) takes over interval Si. Let y1, . . . , yk be elements of Y in increasing order. Our dynamic program has a state A[i][j], which is (proportional to) the optimal mechanism if we restrict the input to only {y1, . . . , yi} and consider partitioning into j intervals S1, . . . , Sj. To compute A[i][j], we try all possibilities for Sj. Recall that Sj must be an interval, i.e., Sj = {yr+1, . . . , yi} for some r < i. This breaks the problem into two parts: computing optimal RR-on-Bins for {y1, . . . , yr} for j − 1 partitions and solving for the optimal output label ˆy for Sj. The answer to the first part is simply A[r][j − 1], whereas the second part can be written as a univariate optimization problem (denoted by L[r][i] in Algorithm 2). When (cid:96) is convex (in the first argument), this optimization problem is convex and can thus be solved efficiently. Furthermore, for squared loss, absolute-value loss, and Poisson log loss, this problem can be solved in amortized O(1) time, resulting in a total running time of O(k2) for the entire dynamic programming algorithm. The full description is presented in Algorithm 26; the complete analyses of its correctness and running time are deferred to Appendix B.\n\n3.2 ESTIMATING THE PRIOR PRIVATELY\n\nIn the previous subsection, we assumed that the distribution P of labels is known beforehand. This might not be the case in all practical scenarios. Below we present an algorithm that first privately approximates the distribution P and work with this approximation instead. We then analyze how using such an approximate prior affects the performance of the algorithm.\n\nTo formalize this, let us denote by MLap the ε-DP Laplace mechanism for approximating the prior. Given n samples drawn from P , MLap constructs a histogram over Y and adds Laplace noise with scale 2/ε to each entry, followed by clipping (to ensure that entries are non-negative) and normalization. A formal description of MLap\n\ncan be found in Algorithm 4 in the Appendix.\n\nε\n\nε\n\nε\n\n5For convenience, we assume that S1, S2, . . . are sorted in increasing order. the corresponding Φ, ˆY on the last 6We remark that\n\nline can be efficiently computed by recording the minimizer for each A[i][j] and L[r][i], and going backward starting from i = k and j = arg mind∈[k]\n\nd−1+eε A[k][d].\n\n1\n\n5\n\n(a) RR-on-Bins\n\n(b) Laplace Mechanism\n\nFigure 2: Illustration of the training label randomization mechanism under privacy budget ε = 3. In this case, RR-on-Bins maps the labels to 3 bins chosen for optimal MSE via Algorithm 2. The 2D density plot contours are generated in log scale. The legends show the MSE between the original labels and the ε-DP randomized labels.\n\nOur mechanism for the unknown prior case—described in Algorithm 3—is now simple: We split the privacy budget into ε1, ε2, run the ε1-DP Laplace mechanism to get an approximate prior distribution P (cid:48) and run the optimal RR-on-Bins for privacy parameter ε2 to randomize the labels. It is not hard to see that the algorithm is (ε1 + ε2)-DP. (Full proof deferred to the Appendix.) Theorem 6. LabelRandomizerε1,ε2 is (ε1 + ε2)-DP.\n\nLet us now discuss the performance of the above algorithm in comparison to the case where the prior P is known, under the assumption that y1, . . . , yn are sampled i.i.d. from P . We show in the following theorem that the difference between the expected population loss in the two cases converges to zero as the number of samples n → ∞, provided that Y is finite: Theorem 7. Let (cid:96) : R × R → R≥0 be any loss function satisfying Assumption 4. Furthermore, assume that (cid:96)(ˆy, y) ≤ B for some parameter B for all y, ˆy ∈ Y. For any distribution P on Y, ε > 0, and any sufficiently large n ∈ N, there is a choice of ε1, ε2 > 0 such that ε1 + ε2 = ε and\n\n[L(RR-on-BinsΦ(cid:48)\n\nε2\n\n; P )] − inf M\n\nL(M; P ) ≤ O\n\n(cid:16)\n\nB · (cid:112)|Y|/n\n\n(cid:17)\n\n,\n\nE y1,...,yn∼P P (cid:48),Φ(cid:48), ˆY (cid:48)\n\nwhere P (cid:48), Φ(cid:48), ˆY (cid:48) are as in LabelRandomizerε1,ε2 and the infimum is over all ε-DP mechanisms M.\n\nWe remark that the above bound is achieved by setting ε1 = (cid:112)|Y|/n; indeed, we need to assume that n is sufficiently large so that ε1 < ε. The high-level idea of the proof is to first use a known utility bound for Laplace mechanism Diakonikolas et al. (2015) to show that P, P (cid:48) are close. Then, we argue that the optimal RR-on-Bins is robust, in the sense that small changes in the prior (from P to P (cid:48)) and privacy parameter (from ε to ε2) do not affect the resulting population loss too much.\n\n4 EXPERIMENTAL EVALUATION\n\nWe evaluate the RR-on-Bins mechanism on three datasets, and compare with the Laplace mechanism (Dwork et al., 2006b), the staircase mechanism (Geng & Viswanath, 2014) and the exponential mechanism (McSherry & Talwar, 2007). Note the Laplace mechanism and the staircase mechanism both have a discrete and a continuous variant. For real-valued labels (the Criteo Sponsored Search Conversion dataset), we use the continuous variant, and for integer-valued labels (the US Census dataset and the App Ads Conversion Count dataset), we use the discrete variant. All of these algorithms can be implemented in the feature-oblivious label DP setting of Figure 1. Detailed model and training configurations can be found in Appendix E.\n\n6\n\n050100150200250300350400Original labels050100150200250300350400Noised labelsMSE = 4591.63560.0000.0050.0100.0000.025050100150200250300350400Original labels050100150200250300350400Noised labelsMSE = 14461.28930.0000.0050.0100.000.02Privacy Budget\n\n0.05 0.1 0.3 0.5 0.8 1.0 1.5 2.0 3.0 4.0 6.0 8.0 ∞\n\nMSE (Mechanism)\n\nMSE (Generalization)\n\nLaplace Mechanism\n\nRR-on-Bins\n\nLaplace Mechanism\n\nRR-on-Bins\n\n60 746.98 ± 46.31 59 038.06 ± 51.31 52 756.01 ± 56.64 47 253.12 ± 57.12 40 223.13 ± 48.66 36 226.54 ± 45.05 28 170.93 ± 39.45 22 219.20 ± 28.04 14 411.77 ± 20.26 9 851.53 ± 17.27 5 270.57 ± 10.30 3 239.22 ± 6.54 0.00 ± 0.00\n\n11 334.84 ± 9.07 11 325.53 ± 9.25 11 210.48 ± 9.06 10 977.09 ± 8.85 10 435.43 ± 9.77 9 976.86 ± 8.21 8 636.43 ± 7.04 7 260.05 ± 10.55 4 600.24 ± 11.15 2 631.36 ± 4.41 709.74 ± 6.18 176.47 ± 2.12 0.00 ± 0.00\n\n24 812.56 ± 139.35 11 339.71 ± 36.45 23 933.23 ± 172.43 11 328.04 ± 36.34 20 961.83 ± 149.47 11 185.20 ± 36.10 18 411.30 ± 111.82 10 901.33 ± 36.54 10 256.37 ± 37.39 15 428.75 ± 91.32 9 744.08 ± 37.59 13 788.51 ± 75.71 8 406.88 ± 36.57 10 808.53 ± 52.31 7 294.93 ± 34.03 8 892.80 ± 32.92 5 577.50 ± 31.75 6 770.33 ± 22.86 4 769.61 ± 25.01 5 764.32 ± 28.95 4 371.68 ± 25.31 4 955.21 ± 26.75 4 333.12 ± 31.94 4 668.40 ± 20.34 4 319.86 ± 29.27 4 322.91 ± 28.31\n\nTable 1: MSE on the Criteo dataset. The first column block (Mechanism) measures the error introduced by the DP randomization mechanisms on the training labels. The second column block (Generalization) measures the test error of the models trained on the corresponding private labels.\n\n4.1 CRITEO SPONSORED SEARCH CONVERSION\n\nThe Criteo Sponsored Search Conversion Log Dataset (Tallis & Yadav, 2018) contains logs obtained from Criteo Predictive Search (CPS). Each data point describes an action performed by a user (click on a product related advertisement), with additional information consisting of a conversion (product was bought) within a 30-day window and that could be attributed to the action. We formulate a (feature-oblivious) label DP problem to predict the revenue (in C) obtained when a conversion takes place (the SalesAmountInEuro attribute). This dataset represents a sample of 90 days of Criteo live traffic data, with a total of 15, 995, 634 examples. We remove examples where no conversion happened (SalesAmountInEuro is −1), resulting in a dataset of 1, 732, 721 examples. The conversion value goes up to 62, 458.773 C. We clip the conversion value at 400 C, which corresponds to the 95th percentile of the value distribution.\n\nIn Figure 2, we visualize an example of how RR-on-Bins randomizes those labels, and compare it with the Laplace mechanism, which is a canonical DP mechanism for scalars. In this case (and for ε = 3), RR-on-Bins chooses 3 bins at around 50, 100, and 250 and maps the sensitive labels to those bins with Algorithm 1. The joint distribution of the sensitive labels and randomized labels maintains an overall concentration along the diagonal. On the other hand, the joint distribution for the Laplace mechanism is generally spread out. Table 1 quantitatively compares the two mechanisms across different privacy budgets. The first block (Mechanism) shows the MSE between the sensitive training labels and the private labels generated by the two mechanisms, respectively. We observe that RR-on-Bins leads to significantly smaller MSE than the Laplace mechanism for the same label DP guarantee. Furthermore, as shown in the second block of Table 1 (Generalization), the reduced noise in the training labels leads to lower test errors.\n\nthe exponential mechanism, and the staircase Figure 3 compares with two additional baselines: mechanism. For both the “Mechanism” errors and “Generalization” errors, RR-on-Bins consistently outperforms other methods for both low- and high-ε regimes.\n\n4.2 US CENSUS\n\nThe 1940 US Census dataset has been made publicly available for research since 2012, and has 131, 903, 909 rows. This data is commonly used in the DP literature (e.g., Wang et al. (2019); Cao et al. (2021); Ghazi et al. (2021b)). In this paper, we set up a label DP problem by learning to predict the duration for which the respondent worked during the previous year (the WKSWORK1 field, measured in number of weeks). Figure 4a shows that RR-on-Bins outperforms the baseline mechanisms across a wide range of privacy budgets.\n\n7\n\n(a) Mechanism\n\n(b) Generalization\n\nFigure 3: MSE on the Criteo dataset: (a) error introduced by DP randomization mechanisms on the training labels and (b) test error of the models trained on the corresponding private labels.\n\n(a) US Census\n\n(b) App Ads\n\nFigure 4: Test performance across different privacy budgets. (a) MSE on the US Census dataset. (b) Relative performance on the App Ads Conversion Count dataset. The relative error is calculated with respect to the non-private baseline, i.e. (E − Ebaseline)/Ebaseline.\n\n4.3 APP ADS CONVERSION COUNT PREDICTION\n\nFinally, we evaluate our algorithm on an app install ads prediction dataset from a commercial mobile app store. The examples in this dataset are ad clicks and each label counts post-click events (aka conversions) occurring in the app after the user installs it. For example, if a user installs a ride share app after clicking the corresponding ad, the label could be the total number of rides that the user purchases in a given time window after the installation. We note that ad prediction tasks/datasets of a similar nature have been previously used for evaluation (Badanidiyuru et al., 2021).\n\nFigure 4b shows the performance on this dataset. Consistent with the results observed on the other datasets, RR-on-Bins outperforms other mechanisms across all the privacy budget values.\n\nIn summary, by first estimating a prior distribution (privately), RR-on-Bins significantly reduces label noise compared to other label DP mechanisms, especially in the high-privacy (i.e., intermediate to small ε) regime. This leads to significantly better test performance for the regression networks trained on these less noisy labels. For example, for ε = 0.5, comparing to the best baseline methods, the test MSE for RR-on-Bins is ∼ 1.5× smaller on the Criteo and US Census datasets, and the relative test error is ∼ 5× smaller on the App Ads dataset.\n\n8\n\n0.050.10.30.50.811.523468infPrivacy Budget0123456Mean Squared Error1e4LaplaceStaircaseExponentialRR-on-Bins0.050.10.30.50.811.523468infPrivacy Budget0.51.01.52.02.5Mean Squared Error1e4LaplaceStaircaseExponentialRR-on-Bins0.050.10.30.50.811.523468infPrivacy Budget150200250300350400450Mean Squared ErrorLaplaceStaircaseExponentialRR-on-Bins0.10.30.50.81.01.52.03.04.06.08.0infPrivacy Budget0102030405060Relative ErrorLaplaceStaircaseRR-on-Bins5 RELATED WORK\n\nThere has been a large body of work on learning with DP. Various theoretical and practical settings have been studied including empirical risk minimization (Chaudhuri et al., 2011), optimization (Song et al., 2013), regression analysis (Zhang et al., 2012), and deep learning (Abadi et al., 2016). The vast majority of the prior works studied the setting where both the features and the labels are deemed sensitive and hence ought to be protected.\n\nChaudhuri & Hsu (2011) and Beimel et al. (2016) studied the sample complexity of learning with label DP for classification tasks. Wang & Xu (2019) studied label DP for sparse linear regression in the local DP setting. Ghazi et al. (2021a) provided a procedure for training deep neural classifiers with label DP. For the classification loss, they formulate an LP and derive an explicit solution, which they refer to as RRTop-k. Our work could be viewed as an analog of theirs for regression tasks. Malek Esmaeili et al. (2021) used the PATE framework of Papernot et al. (2017; 2018) to propose a new label DP training method. We note that this, as well as related work on using unsupervised and semi-supervised learning to improve label DP algorithms (Esfandiari et al., 2022; Tang et al., 2022), do not apply to the feature-oblivious label DP setting.\n\nA variant of two-party learning (with a “features party” and a “labels party”) was recently studied by Li et al. (2021), who considered the interactive setting where the two parties can engage in a protocol with an arbitrary number of rounds (with the same application to computational advertising described in Section 1 as a motivation). By contrast, we considered in this paper the one-way communication setting (from Figure 1), which is arguably more practical and easier to deploy.\n\nKairouz et al. (2016) study optimal local DP algorithms under a given utility function and prior. For a number of utility functions, they show that optimal mechanisms belong to a class of staircase mechanisms. Staircase mechanisms are much more general than randomized response on bins. In that regard, our work shows that a particular subset of staircase mechanisms is optimal for regression tasks. In particular, while we give an efficient dynamic programming algorithm for optimizing over randomized response on bins, we are not aware of any efficient algorithm that can compute an optimal staircase mechanism. (In particular, a straightforward algorithm takes 2O(k) time.)\n\n6 CONCLUSIONS AND FUTURE DIRECTIONS\n\nIn this work we propose a new label DP mechanism for regression. The resulting training algorithm can be implemented in the feature-oblivious label DP setting. We provide theoretical results shedding light on the operation and guarantees of the algorithm. We also evaluate it on three datasets, demonstrating that it achieves higher accuracy compared to several non-trivial, natural approaches.\n\nOur work raises many questions for future exploration. First, in our evaluation, we set the objective function optimized using the LP to be the same as the loss function used during training; different ways of setting the LP objective in relation to the training loss are worth exploring. Second, our noising mechanism typically introduces a bias; mitigating it, say by adding unbiasedness constraints into the LP, is an interesting direction. Third, it might also be useful to investigate de-noising techniques that can be applied as a post-processing step on top of our label DP mechanism; e.g., the method proposed in Tang et al. (2022) for computer vision tasks, and the ALIBI method (Malek Esmaeili et al., 2021) for classification tasks, which is based on Bayesian inference.\n\nWe believe that due to its simplicity and practical appeal, the setting of learning with featureoblivious label DP merits further study, from both a theoretical and an empirical standpoint.\n\nFinally, we leave the question of obtaining better “feature-aware” label DP regression algorithms for a future investigation.\n\nACKNOWLEDGMENTS\n\nWe thank Peter Kairouz and Sewoong Oh for many useful discussions, and Eu-Jin Goh, Sam Ieong, Christina Ilvento, Andrew Tomkins, and the anonymous reviewers for their comments.\n\n9\n\nREFERENCES\n\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and\n\nLi Zhang. Deep learning with differential privacy. In CCS, pp. 308–318, 2016.\n\nAshwinkumar Badanidiyuru, Andrew Evdokimov, Vinodh Krishnan, Pan Li, Wynn Vonnegut, and Jayden Wang. Handling many conversions per click in modeling delayed feedback. In AdKDD, 2021.\n\nAmos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approxi-\n\nmate differential privacy. ToC, 12(1):1–61, 2016.\n\nA Colin Cameron and Pravin K Trivedi. Regression Analysis of Count Data. Cambridge University\n\nPress, 2013.\n\nXiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Data poisoning attacks to local differential\n\nprivacy protocols. In USENIX Security, pp. 947–964, 2021.\n\nKamalika Chaudhuri and Daniel Hsu. Sample complexity bounds for differentially private learning.\n\nIn COLT, pp. 155–186, 2011.\n\nKamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical\n\nrisk minimization. JMLR, 12(3), 2011.\n\nIlias Diakonikolas, Moritz Hardt, and Ludwig Schmidt. Differentially private learning of structured\n\ndiscrete distributions. In NIPS, pp. 2566–2574, 2015.\n\nCynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations\n\nand Trends® in Theoretical Computer Science, 9(3-4):211–407, 2014.\n\nCynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,\n\nourselves: Privacy via distributed noise generation. In EUROCRYPT, pp. 486–503, 2006a.\n\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitiv-\n\nity in private data analysis. In TCC, pp. 265–284, 2006b.\n\nHossein Esfandiari, Vahab Mirrokni, Umar Syed, and Sergei Vassilvitskii. Label differential privacy\n\nvia clustering. In AISTATS, pp. 7055–7075, 2022.\n\nQuan Geng and Pramod Viswanath. The optimal mechanism in differential privacy. In ISIT, pp.\n\n2371–2375, 2014.\n\nBadih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, and Chiyuan Zhang. Deep learning\n\nwith label differential privacy. NeurIPS, pp. 27131–27145, 2021a.\n\nBadih Ghazi, Ravi Kumar, Pasin Manurangsi, Rasmus Pagh, and Amer Sinha. Differentially private aggregation in the shuffle model: Almost central accuracy in almost a single message. In ICML, pp. 3692–3701, 2021b.\n\nArpita Ghosh, Tim Roughgarden, and Mukund Sundararajan. Universally utility-maximizing pri-\n\nvacy mechanisms. SICOMP, 41(6):1673–1693, 2012.\n\nPeter Kairouz, Sewoong Oh, and Pramod Viswanath. Extremal mechanisms for local differential\n\nprivacy. JMLR, 17:17:1–17:51, 2016.\n\nPeter Kairouz, H. Brendan McMahan, Brendan Avent, Aur ́elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri`a Gasc ́on, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Koneˇcn ́y, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancr`ede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer ̈Ozg ̈ur, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram`er, Praneeth Vepakomma, Jianyu\n\n10\n\nWang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nOscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Virginia Smith, and\n\nChong Wang. Label leakage and protection in two-party split learning. In ICLR, 2021.\n\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In ICLR,\n\n2017.\n\nMani Malek Esmaeili, Ilya Mironov, Karthik Prasad, Igor Shilov, and Florian Tramer. Antipodes of\n\nlabel differential privacy: PATE and ALIBI. NeurIPS, 34:6934–6945, 2021.\n\nFrank McSherry and Kunal Talwar. Mechanism design via differential privacy. In FOCS, pp. 94–\n\n103, 2007.\n\nNicolas Papernot, Mart ́ın Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-\n\nsupervised knowledge transfer for deep learning from private training data. In ICLR, 2017.\n\nNicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and ́Ulfar Er-\n\nlingsson. Scalable private learning with PATE. In ICLR, 2018.\n\nCarey Radebaugh and Ulfar Erlingsson. Introducing TensorFlow Privacy: Learning with Differential\n\nPrivacy for Training Data, March 2019. blog.tensorflow.org.\n\nJustin Schuh.\n\nparty building-more-private-web-path-towards.html.\n\nBuilding a more private web: obsolete,\n\nJanuary\n\ncookies\n\n2020.\n\nA path towards making third https://blog.chromium.org/2020/01/\n\nShuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differ-\n\nentially private updates. In GlobalSIP, pp. 245–248, 2013.\n\nMarcelo Tallis and Pranjul Yadav. Reacting to variations in product demand: An application for conversion rate (CR) prediction in sponsored search. In IEEE BigData, pp. 1856–1864, 2018.\n\nXinyu Tang, Milad Nasr, Saeed Mahloujifar, Virat Shejwalkar, Liwei Song, Amir Houmansadr, and Prateek Mittal. Machine learning with differentially private labels: Mechanisms and frameworks. PoPETS, 4:332–350, 2022.\n\nDi Wang and Jinhui Xu. On sparse linear regression in the local differential privacy model. In ICML,\n\npp. 6628–6637, 2019.\n\nTianhao Wang, Bolin Ding, Jingren Zhou, Cheng Hong, Zhicong Huang, Ninghui Li, and Somesh Jha. Answering multi-dimensional analytical queries under local differential privacy. In SIGMOD, pp. 159–176, 2019.\n\nStanley L Warner. Randomized response: A survey technique for eliminating evasive answer bias.\n\nJASA, 60(309):63–69, 1965.\n\nJohn Wilander. Full Third-Party Cookie Blocking and More, March 2020. https://webkit.org/\n\nblog/10218/full-third-party-cookie-blocking-and-more/.\n\nMarissa Wood.\n\ning by Default, September 2019. todays-firefox-blocks-third-party-tracking-cookies-and-cryptomining-by-default/.\n\nToday’s Firefox Blocks Third-Party Tracking Cookies and Cryptominhttps://blog.mozilla.org/en/products/firefox/\n\nAshkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya Mironov. Opacus: User-friendly differential privacy library in PyTorch. arXiv:2109.12298, 2021.\n\nJun Zhang, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, and Marianne Winslett. Functional mechanism:\n\nregression analysis under differential privacy. VLDB, 5(11):1364–1375, 2012.\n\n11\n\nA OPTIMALITY OF RR-on-BinsΦ\n\nε\n\nIn this section, we prove Theorem 5. We prove this in two stages. First we consider the case where P has finite support and ˆY is constrained to be a subset of a pre-specified finite set O ⊆ R. Next, we consider the case where ˆY is allowed to be an arbitrary subset of R, and thus, the output distribution of the mechanism on any input y can be an arbitrary probability measure over R.\n\nStage I: Finitely supported P , ˆY ⊆ O, (cid:96) strictly increasing/decreasing. Theorem 8. Let P be a distribution over R with finite support, O a finite subset of R, and (cid:96) : R × R → R≥0 be such that • For all y ∈ R, (cid:96)(ˆy, y) is strictly decreasing in ˆy when ˆy ≤ y and strictly increasing in ˆy when\n\nˆy ≥ y.\n\n• For all ˆy ∈ R, (cid:96)(ˆy, y) is strictly decreasing in y when y ≤ ˆy and strictly increasing in y when\n\ny ≥ ˆy.\n\nThen for all ε > 0, there exists Φ : Y → O such that\n\nL(RR-on-BinsΦ\n\nε ; P ) = inf\n\nL(M; P ).\n\nM\n\nwhere infM is over all ε-DP mechanisms M with inputs in Y and outputs in O.\n\nThe proof of Theorem 8 is inspired by the proof of Theorem 3.1 of Ghosh et al. (2012). Namely, we describe the problem of minimizing L(M; P ) as a linear program (LP). We arrange the variables of the LP into a matrix and associate any solution to the LP a signature matrix, which represents when constraints of the LP are met with equality. Then we make several observations of the signature matrix for any optimal solution, which eventually leads to the proof of the theorem.\n\nProof of Theorem 8. Without loss of generality, we may assume that Y = supp(P ) and therefore is finite. When ˆY is restricted to be a subset of O, the optimal mechanism M which minimizes L(M; P ) = Ey∼P,ˆy∼M(y) (cid:96)(ˆy, y), is encoded in the solution of the following LP, with |Y|·|O| variables My→ˆy = Pr[M(y) = ˆy] indexed by (y, ˆy) ∈ Y × O. Namely, the first two constraints enforce that (My→ˆy)y is a valid probability distribution and the third constraint enforces the ε-DP constraint.\n\n\n\n(cid:88)\n\ny∈Y\n\n(cid:88)\n\npy\n\n\n\nˆy∈O\n\nmin M\n\n\n\nMy→ˆy · (cid:96)(ˆy, y)\n\n ,\n\nsubject to ∀y ∈ Y, ˆy ∈ O :\n\n∀y ∈ Y :\n\nMy→ˆy ≥ 0, (cid:88)\n\nMy→ˆy = 1,\n\nˆy∈O\n\n(2)\n\n∀ˆy ∈ O, ∀y, y(cid:48) ∈ Y, y (cid:54)= y(cid:48) :\n\nMy(cid:48)→ˆy ≤ eε · My→ˆy.\n\nCorresponding to any feasible solution M := (My→ˆy)y,ˆy, we associate a |Y| × |O| signature matrix SM . First, let pmin ˆy = maxy My→ˆy. Note that, from the constraints it follows that pmax\n\nˆy = miny My→ˆy and let pmax\n\nˆy ≤ eε · pmin\n\nˆy\n\n.\n\nDefinition 9 (Signature matrix). For any feasible M for the LP in (2), the signature entry SM (y, ˆy) for all y ∈ Y and ˆy ∈ O is defined as\n\nSM (y, ˆy) =\n\n\n\n\n\n\n\n0 if My→ˆy = 0 U if My→ˆy = pmax if My→ˆy = pmin L\nS otherwise.\n\nˆy = eε · pmin ˆy = e−ε · pmax\n\nˆy\n\nˆy\n\n12\n\n1.5\n\n2.5\n\n3.5\n\n4.5\n\n1.5\n\n2.5\n\n3.5\n\n4.5\n\n1\n\n2\n\n3\n\n4\n\n5\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1/4\n\n1/4\n\n1/4\n\n1/3\n\n1/3\n\n1/4\n\n1/4\n\n1/2\n\n1/3\n\n1/3\n\n1/2\n\n1/2\n\n1/4\n\n1/3\n\n1/3\n\nM\n\n1\n\n2\n\n3\n\n4\n\n5\n\n0\n\n0\n\n0\n\n0\n\n0\n\nL\n\nL\n\nU\n\nS\n\nS\n\nS\n\nS\n\nS\n\nS\n\nS\n\nU\n\nU\n\nL\n\nS\n\nS\n\nSM\n\nFigure 5: Example of a signature matrix for eε = 1/2 for Y = {1, 2, 3, 4, 5} and O = {1.5, 2.5, 3.5, 4.5}\n\nWe visualize SM as a matrix with rows corresponding to y’s and columns corresponding to ˆy’s, both ordered in increasing order (see Figure 5).\n\nIf M is an RR-on-BinsΦ ε mechanism for some Φ : Y → O, then it is easy to see that the corresponding signature matrix satisfies some simple properties (given in Claim 10 below). Interestingly, we establish a converse, thereby characterizing the signature of matrices M that correspond to RR-on-BinsΦ\n\nε mechanism for some Φ : Y → O.\n\nClaim 10. M corresponds to an RR-on-BinsΦ (for ˆY ⊆ O) if and only if either\n\nε mechanism for some non-decreasing Φ : Y → ˆY\n\n(1) One column consists entirely of S, while all other columns are entirely 0; let Ψ(y) = ˆy, where\n\nˆy corresponds to the unique S column,\n\nor all of the following hold:\n\n(2a) Each column in SM is either entirely 0, or entirely consisting of U’s and L’s, with at least one\n\nU and one L.\n\n(2b) Each row contains a single U entry, with all other entries being either L or 0; for each y, we\n\ndenote this unique column index of U by Ψ(y).\n\n(2c) For all y < y(cid:48) it holds that Ψ(y) ≤ Ψ(y(cid:48)).\n\nIn each case, it holds that Φ = Ψ.\n\nProof of Claim 10. Suppose M corresponds to RR-on-BinsΦ ε for some non-decreasing Φ. If Φ is constant, then condition (1) holds, since all columns corresponding to ˆy /∈ range(Φ) in SM are all 0, whereas, the only remaining column consists of all S. If Φ is not constant, then My→ˆy = eε1[Φ(y)=ˆy]/(eε + | ˆY| − 1), and hence its signature SM is such that SM (y, ˆy) is U if Φ(y) = ˆy, and L if ˆy ∈ range(Φ) (cid:114) {Φ(y)}, and 0 otherwise. It is easy to verify that all three conditions hold: (2a) a column corresponding to ˆy is entirely 0 if and only if ˆy /∈ range(Φ) and entirely consisting of U’s and L’s otherwise with at least one U corresponding to y such that Φ(y) = ˆy and at least one L corresponding to y such that Φ(y) (cid:54)= ˆy (since Φ is non-constant), (2b) Each row corresponding to y has a unique U corresponding to ˆy = Ψ(y) = Φ(y), and (2c) For y < y(cid:48), it holds that Ψ(y) ≤ Ψ(y(cid:48)) since Φ is non-decreasing.\n\nTo establish the converse, suppose we have that SM satisfies condition (1). Then we immediately get that M corresponds to RR-on-Bins for the constant map Φ = Ψ. Next, suppose SM satisfies all three conditions (2a)–(2c). Immediately, we have that M is given as\n\nMy→ˆy =\n\n \n\n\n\nˆy\n\neε · pmin pmin ˆy 0\n\nif SM (y, ˆy) = U if SM (y, ˆy) = L if SM (y, ˆy) = 0.\n\nLet ˆY correspond to the set of non-zero columns of SM . Since each row of M corresponds to a probability distribution, we have for each ˆy ∈ ˆY that (cid:80) ˆy = 1 by considering\n\nˆy(cid:48) +(eε −1)pmin\n\nˆy(cid:48)∈ ˆY pmin\n\n13\n\nthe row corresponding to any y ∈ Ψ−1(ˆy). Thus, we get that pmin and in particular, pmin\n\nis the same for all values in ˆY, , and thus, we get that the corresponding M is uniquely given by\n\nˆy\n\nˆy =\n\n1 eε+| ˆY|−1\n\nMy→ˆy =\n\n \n\neε eε+| ˆY|−1 1\neε+| ˆY|−1  0\n\nif SM (y, ˆy) = U\n\nif SM (y, ˆy) = L\n\nif SM (y, ˆy) = 0,\n\nwhich clearly corresponds to RR-on-BinsΦ non-decreasing from condition (2c). This completes the proof of Claim 10.\n\nε for Φ(y) = Ψ(y). We have that Ψ, and hence Φ, is\n\nThus, to show optimality of RR-on-BinsΦ LP (2), such that SM satisfies the conditions in Claim 10.\n\nε , it suffices to show that there exists a minimizer M of the\n\nIt is well known that an optimal solution to any LP can be found at the vertices of the constraint polytope and that for a LP with n variables, vertices must meet n linearly independent constraints. We use this to find a submatrix of SM , which will determine M in its entirety.\n\nIf M is a feasible point, then each column of SM is either entirely 0, or entirely consisting of S, U, and L. This is necessary since if My→ˆy = 0, then 0 ≤ My(cid:48)→ˆy ≤ eεMy→ˆy = 0. Let k denote the number of non-zero columns of SM .\n\nLemma 11. Suppose M is a vertex of the LP (2) with k non-zero columns of SM . If k ≥ 2, then M has a k × k submatrix M (k) with all distinct rows, consisting of only U’s and L’s.\n\nProof. M is a vertex if and only if there are |Y| · |O| many linearly independent constraints that are tight. We count the number of tight constraints just from SM and note some instances of dependence to give a lower bound on the number of linearly independent constraints.\n\n1. |Y| constraints, given by (cid:80)\n\nˆy My→ˆy = 1 are always tight.\n\n2. For a zero column of SM corresponding to ˆy, each My→ˆy ≥ 0 is tight corresponding to\n\neach y ∈ Y. These correspond to |Y| · (|O| − k) constraints.\n\n3. For a non-zero column of SM corresponding to ˆy, let Cˆy(U) denote the number of U entries in column ˆy and similarly, define Cˆy(L) and Cˆy(S) analogously. For each pair y1, y2 such that SM (y1, ˆy) = L and SM (y2, ˆy) = U, we have a tight constraint My1→ˆy ≤ eε · My2→ˆy. However, these constraints are not linearly independent. In fact, there are only Cˆy(U) + Cˆy(L) − 1 = |Y| − Cˆy(S) − 1 many linearly independent constraints among these.\n\n4. Another instance where the constraints might be dependent is if two rows of SM , corresponding to say y1 and y2, are identical and do not contain any S’s. In this instance, the two equations of (cid:80) ˆy Myi→ˆy = 1 and the inequalities given by the 0s, U’s, and L’s are not independent. The DP inequality conditions imply that all the coordinates are equal between the two rows, which imply a dependence relation between those and the two conditions.\n\nCounting these all up, we have a lower bound on the number of linearly independent constraints. This must be at least |Y| · |O|. Let (# of duplicate rows not containing S) be the difference between the number of all rows not containing S and the number of distinct rows not containing S. Thus, we get\n\n|Y|+|Y|·(|O|−k)+(|Y|−1)·k −\n\n(cid:88)\n\nˆy\n\nCˆy(S)−(# of duplicate rows not containing S) ≥ |Y|·|O|.\n\nRearranging,\n\nand because\n\n|Y| −\n\n(cid:88)\n\nˆy\n\nCˆy(S) − (# of duplicate rows not containing S) ≥ k,\n\n(cid:88)\n\nˆy\n\nCˆy(S) ≥ (# rows containing S),\n\n14\n\nwe conclude\n\n|Y| − (# rows containing S) − (# of duplicate rows not containing S) ≥ k.\n\nHence, there are at least k rows, which are all distinct and contain only 0’s, U’s, and L’s. Narrowing our scope to just the k non-zero columns, we get a k × k sub-matrix M (k) that contains only U’s and L’s. This concludes the proof of Lemma 11.\n\nSo far, we did not use any information about the objective. Next we use the properties of the loss function (cid:96) to show that if M is an optimal solution to the LP, any submatrix provided by Lemma 11 can only be of one form, the matrix with U’s along the diagonal and L’s everywhere else:\n\nLemma 12. Suppose M is an optimal vertex solution to the LP 2 with k ≥ 2 non-zero columns. Then any k × k submatrix M (k) given by Lemma 11 has U’s along the diagonal and L’s otherwise.\n\nProof. Firstly, in each row of M (k), the U’s are consecutive. Suppose for any y and ˆy1 < ˆy2 < ˆy3, it holds that SM (y, ˆy1) = SM (y, ˆy3) = U, whereas SM (y, ˆy2) is either L or S. This implies that (cid:96)(ˆy1, y), (cid:96)(ˆy3, y) < (cid:96)(ˆy2, y), since otherwise, it is possible to reduce My→ˆy1 (resp. M (y → ˆy3)) and increase My→ˆy2 thereby further reducing the objective, without violating any constraints. But this is a contradiction to the assumption of (cid:96) in Theorem 8, since (cid:96)(·, y) cannot increase from ˆy1 to ˆy2 and then decrease from ˆy2 to ˆy3.\n\nSecondly, SM cannot contain the following 2 × 2 matrix, where y1 < y2 and ˆy1 < ˆy2:\n\nˆy1 L\nU\n\nˆy2 U\nL\n\ny1 y2\n\nSince M is optimal, we get that (cid:96)(ˆy1, y1) > (cid:96)(ˆy2, y1), and from the assumption on (cid:96) it follows that ˆy1 < y1 (since otherwise, y1 ≤ ˆy1 < ˆy2 which would imply (cid:96)(ˆy1, y1) < (cid:96)(ˆy2, y1)). Similarly, we have that (cid:96)(ˆy2, y2) > (cid:96)(ˆy1, y2) and y2 < ˆy2. However, since ˆy1 < y1 < y2 < ˆy2, we have that (cid:96)(ˆy2, y1) > (cid:96)(ˆy2, y2) and (cid:96)(ˆy1, y2) > (cid:96)(ˆy1, y1), which gives rise to a contradiction:\n\n(cid:96)(ˆy1, y1) > (cid:96)(ˆy2, y1) > (cid:96)(ˆy2, y2) > (cid:96)(ˆy1, y2) > (cid:96)(ˆy1, y1).\n\nIn particular, this claim of not containing the above 2 × 2 matrix applies to M (k).\n\nLastly, every row of M (k) has at least one U . Suppose for contradiction that the row y in SM does not contain a single U. Then it has all L’s and 0’s. Any column that contains an L must contain a U in SM . So necessarily there is a row y(cid:48) in SM containing a U. Now, y containing only L’s and 0’s implies that (cid:80)\n\nˆy My(cid:48)→ˆy, which is a contradiction of the constraints of the LP.\n\nˆy My→ˆy < (cid:80)\n\nSince the rows of this k × k sub-matrix M (k) are all distinct, and it does not contain any 2 × 2 submatrix as above, the only possible signature is where all the diagonal signature entries are U and the rest are L. Let si ∈ [k] be the index of the first index U for the ith row and similarly ei ∈ [k] be the last index of U. Because the U’s are continuous, si and ei determine the signature of the row. If si = sj for i (cid:54)= j, then ei = ej. Else if ei < ej, then (cid:80) ˆy My(cid:48)→ˆy where y corresponds to row i and y(cid:48) corresponds to j which is a contradiction. Similarly for ei > ej. The same argument can be made if ei = ej, then si = sj. The rows of M (k) are distinct, so this implies that si (cid:54)= sj, ei (cid:54)= ej for i (cid:54)= j. The observation about the 2 × 2 matrix shows that if i < j, si ≤ sj and ei ≤ ej. So the si and ei are both k distinct ordered sequences of [k]. The only such sequence is si = ei = i, implying M (k) is a diagonal matrix of this form. This completes the proof of Lemma 12.\n\nˆy My→ˆy < (cid:80)\n\nWe now have the necessary observations to prove Theorem 8. Let M be an optimal vertex solution to the LP 2. That is L(M ; P ) = infM L(M; P ). If M has one non-zero column, then SM has one column entirely of S and the rest 0. By Claim 10, M corresponds to a RR-on-BinsΦ ε mechanism. If\n\n15\n\nM has k ≥ 2 non-zero columns, then by Lemma 12, SM has a k × k submatrix M (k) with U along the diagonal and L otherwise. We show this completely determines SM in the form described by Claim 10(2a–2c).\n\nNote that SM for any vertex M has at most one S per row. If SM (y, ˆy1) and SM (y, ˆy2) are both equal to S for y1 (cid:54)= y2, then it is possible to write M as a convex combination of two other feasible points given as M + ηM (cid:48) and M − ηM (cid:48) for small enough η, where M (cid:48)(y → ˆy2) = −M (cid:48)(y → ˆy1) = 1 and M (cid:48)(y(cid:48) → ˆy(cid:48)) = 0 for all other y(cid:48), ˆy(cid:48). Intuitively this corresponds to moving mass My→ˆy1 to / from My→ˆy2, in a way that does not violate any of the constraints. But M is a vertex so it cannot be the convex combination of two feasible points.\n\nBut moreover, SM for an optimal vertex solution has exactly one U per row and the rest L’s and 0’s. Suppose the row corresponding to y has SM (y, ˆy) either L or S and the rest of the entries either L or 0. From the observations above of the submatrix M (k), there exists a row y(cid:48) such that SM (y(cid:48), ˆy) = U. Then, we have (cid:80) ˆy My(cid:48)→ˆy = 1, which contradicts feasibility. Similarly, if a row of SM contains a U and an S (or multiple U’s), then we would have (cid:80)\n\nˆy My→ˆy < (cid:80)\n\nˆy My→ˆy > (cid:80)\n\nˆy My(cid:48)→ˆy.\n\nThis allows us to define Ψ : Y → O to be the unique ˆy such that SM (y, ˆy) = U. Note that Ψ is non-decreasing from the observation earlier that SM cannot contain the 2 × 2 signature above. This completely characterizes SM as the form described by Claim 10(2a–2c), completing the proof of Theorem 8.\n\nWe use a continuity argument to show that Theorem 8 holds in the case where the loss function (cid:96)(ˆy, y) is decreasing / increasing instead of strictly decreasing / increasing. Corollary 13. Let P be a distribution over R with finite support, O a finite subset of R, and (cid:96) : R × R → R≥0 is such that • For all y ∈ R, (cid:96)(ˆy, y) is decreasing in ˆy when ˆy ≤ y and increasing in ˆy when ˆy ≥ y. • For all ˆy ∈ R, (cid:96)(ˆy, y) is decreasing in y when y ≤ ˆy and increasing in y when y ≥ ˆy.\n\nThen for all ε > 0, there exists Φ : Y → O such that\n\nL(RR-on-BinsΦ\n\nε ; P ) = inf\n\nL(M; P ).\n\nM\n\nwhere infM is over all ε-DP mechanisms M with inputs in Y and outputs in O.\n\nProof. Let V correspond to the set of solutions M that are vertices of the LP. Let W ⊆ V correspond to the set of vertex solutions that are RR-on-Bins. A rephrasing of Theorem 8 is that the minimum loss over mechanisms in V is equal to the minimum loss over mechanisms in W .\n\nFor η > 0, define (cid:96)η(ˆy, y) = (cid:96)(ˆy, y) + η · |y − ˆy| . Note that (cid:96)η satisfies the conditions of Theorem 8. For any fixed mechanism M , the loss is continuous at η = 0:\n\nlim η→0\n\n(cid:88)\n\ny∈Y\n\n\n\npy\n\n\n\n(cid:88)\n\nˆy∈O\n\n\n\n\n\n\n\nMy→ˆy · (cid:96)η(ˆy, y)\n\n =\n\n(cid:88)\n\ny∈Y\n\n(cid:88)\n\npy\n\n\n\nˆy∈O\n\nMy→ˆy · (cid:96)(ˆy, y)\n\n .\n\nThe minimum of finitely many continuous functions is continuous. In particular\n\nlim η→0\n\nmin M ∈V\n\nand similarly\n\nlim η→0\n\nmin M ∈W\n\n\n\npy\n\n\n\n(cid:88)\n\nˆy∈O\n\n(cid:88)\n\ny∈Y\n\n\n\npy\n\n\n\n(cid:88)\n\nˆy∈O\n\n(cid:88)\n\ny∈Y\n\n\n\nMy→ˆy · (cid:96)η(ˆy, y)\n\n = min\n\nM ∈V\n\n\n\nMy→ˆy · (cid:96)η(ˆy, y)\n\n = min\n\nM ∈W\n\n\n\npy\n\n\n\n(cid:88)\n\nˆy∈O\n\n(cid:88)\n\ny∈Y\n\n\n\nMy→ˆy · (cid:96)(ˆy, y)\n\n ,\n\n\n\n(cid:88)\n\ny∈Y\n\n(cid:88)\n\npy\n\n\n\nˆy∈O\n\n\n\nMy→ˆy · (cid:96)(ˆy, y)\n\n ,\n\nbut the LHS of both equations are equal by Theorem 8, so\n\n\n\nmin M ∈V\n\n(cid:88)\n\ny∈Y\n\n(cid:88)\n\npy\n\n\n\nˆy∈O\n\n\n\nMy→ˆy · (cid:96)(ˆy, y)\n\n = min\n\nM ∈W\n\n\n\n(cid:88)\n\ny∈Y\n\n(cid:88)\n\npy\n\n\n\nˆy∈O\n\n\n\nMy→ˆy · (cid:96)(ˆy, y)\n\n ,\n\ncompleting our proof.\n\n16\n\nStage II: Finitely supported P and ˆY ⊆ R. We now set out to prove Theorem 5. Let ymin and ymax denote the minimum and maximum values in Y, respectively. We will first show that\n\ninf ˆY⊆R\n\ninf Φ:Y→ ˆY\n\nL(RR-on-BinsΦ\n\nε ; P ) = inf\n\nL(M; P ),\n\nM\n\nwhere the infimum on the RHS is over all ε-DP mechanisms M. Since RR-on-BinsΦ\n\nε is an ε-DP mechanism, we have\n\ninf M\n\nL(M; P ) ≤ inf ˆY⊆R\n\ninf Φ:Y→ ˆY\n\nL(RR-on-BinsΦ\n\nε ; P ).\n\n(3)\n\n(4)\n\nTo show the converse, let γ > 0 be any parameter. There must exist an ε-DP mechanism M(cid:48) : Y → R such that\n\nL(M(cid:48); P ) ≤ inf\n\nM\n\nL(M; P ) + γ/2.\n\n(5)\n\nLet O ⊆ [ymin, ymax] be defined as follows:\n\n• For each y ∈ Y, let ay = minˆy∈[ymin,ymax] (cid:96)(ˆy, y) and by = maxˆy∈[ymin,ymax] (cid:96)(ˆy, y). Let\n\nT := (cid:100)4(by − ay)/γ(cid:101).\n\n• Let oy,t be the finite set containing the maximal and minimal element of {ˆy | (cid:96)(ˆy, y) =\n\nay + t\n\nT (by − ay)} if the set is non-empty. Otherwise, let it be the empty set.\n\n• Let Oy := (cid:83)T • Finally, let O = (cid:83)\n\nt=0 oy,t.\n\ny∈Y Oy.\n\nNaturally, O is finite. Finally, let M(cid:48)(cid:48) be the mechanism that first runs M(cid:48) to get ˆy and then outputs the element in O closest to ˆy. By post-processing of DP, M(cid:48)(cid:48) remains ε-DP. Furthermore, it is not hard to see that by the construction of O and by Assumption 4, we have\n\nL(M(cid:48)(cid:48); P ) ≤ L(M(cid:48); P ) + γ/2.\n\nFinally, since range(M(cid:48)(cid:48)) = O is finite, the proof in Stage I implies that\n\ninf ˆY⊆O\n\ninf Φ:Y→ ˆY\n\nL(RR-on-BinsΦ\n\nε ; P ) ≤ L(M(cid:48)(cid:48); P ).\n\nCombining (5), (6), and (7), we can conclude that\n\ninf ˆY⊆R\n\ninf Φ:Y→ ˆY\n\nL(RR-on-BinsΦ\n\nε ; P ) ≤ inf\n\nM\n\nL(M; P ) + γ.\n\nSince (8) holds for any γ > 0, combining with (4), we can conclude that (3) holds.\n\nNext, we will show that there exists ˆY ∗ and Φ∗ : Y → ˆY ∗ such that\n\nL(RR-on-BinsΦ∗\n\nε ; P ) = inf ˆY⊆R\n\ninf Φ:Y→ ˆY\n\nL(RR-on-BinsΦ\n\nε ; P ).\n\n(6)\n\n(7)\n\n(8)\n\n(9)\n\nCombining this with (3) completes the proof. For any Φ : Y → R, let PΦ denote the partition on Y induced by Φ−1. Note that the RHS of (9) can be written as\n\ninf ˆY⊆R\n\ninf Φ:Y→ ˆY\n\nL(RR-on-BinsΦ\n\nε ; P ) = min\n\nP\n\nL(RR-on-BinsΦ\n\nε ; P ),\n\n(10)\n\ninf Φ:Y→ ˆY ˆY⊆R PΦ=P\n\nwhere the minimum is over all partitions of P.\n\nFor a fixed partition P = PΦ of Y, L(RR-on-BinsΦ\n\nε ; P ) can simply be written as\n\n(cid:88)\n\n\n\n\n\n(cid:88)\n\npy\n\nSi∈P\n\ny∈Y\n\neε1[y∈Si] eε + | ˆY| − 1\n\n\n\n(cid:96)(ˆyi, y)\n\n ,\n\n17\n\nwhere ˆyi is the output corresponding to the part Si in the partition. (cid:16)(cid:80)\n\n(cid:17)\n\ny∈Y py\n\nNotice that the function ˆyi → vious that it increases once it becomes further from [ymin, ymax]. Thus, the minimum must be achieved at some point ˆy∗ i , this also achieves the minimum for inf Φ:Y→ ˆY ε ; P ). Therefore, plugging this back into ˆY⊆R PΦ=P\n\ni ∈ [ymin, ymax]. As a result, by defining Φ∗\n\nis continuous. Furthermore, it is ob-\n\nP such that Φ∗\n\nL(RR-on-BinsΦ\n\nP (Si) = ˆy∗\n\n(cid:96)(ˆyi, y)\n\neε1[y∈Si] eε+| ˆY|−1\n\n(10), we can conclude that the minimum of inf ˆY⊆R inf Φ:Y→ ˆY L(RR-on-BinsΦ by some ˆY ∗, Φ∗. This completes our proof of Theorem 5.\n\nε ; P ) must be achieved\n\nA.1 EXTENSION TO ARBITRARY (INFINITELY-SUPPORTED) P AND ˆY ⊆ R.\n\nWe show that Theorem 5 can be extended to hold even for infinitely-supported distributions P over a bounded interval, but under an additional assumption that the loss (cid:96) is Lipschitz over the said interval. Assumption 14. For a specified bounded interval [ymin, ymax], loss function (cid:96) : [ymin, ymax] × [ymin, ymax] → R≥0 is such that both (cid:96)(·, y) and (cid:96)(ˆy, ·) are L-Lipschitz. Theorem 15. For all ymin, ymax ∈ R, loss functions (cid:96) : R × R → R≥0 satisfying Assumptions 4 and 14, and all distributions P over Y ⊆ [ymin, ymax], there is a finite output set ˆY ⊆ R and a non-decreasing map Φ : Y → ˆY such that\n\nL(RR-on-BinsΦ\n\nε ; P ) = inf\n\nL(M; P ),\n\nM\n\nwhere the infimum is over all ε-DP mechanisms M.\n\nWe break down the proof into a series of lemmas. First we show that the infimum is reached by the infimum of RR-on-Bins mechanisms: Lemma 16. Under the assumptions of Theorem 15, it holds that\n\nL(RR-on-BinsΦ\n\nε ; P ) = inf\n\nL(M; P ).\n\nM\n\ninf ˆY⊆R, Φ:Y→ ˆY\n\nProof. To prove the lemma, it suffices to show that for any g ∈ N, there exists a choice of ˆY ⊆ R and Φ : Y → ˆY such that L(RR-on-BinsΦ\n\nε ; P ) ≤ infM L(M; P ) + γ where γ = L · ymax−ymin\n\n.\n\ng\n\nConsider a discretization of Y given as\n\n(cid:26)\n\n(cid:27)\n\n ̃Y :=\n\nymin +\n\niγ L\nLet ρ : Y → ̃Y be the map that rounds any element of Y to the closes element of ̃Y. Note that |y − ρ(y)| ≤ γ/(2L) for all y ∈ Y. Consider the distribution ̃P over ̃Y given by the “rounding process”, which samples y ∼ P and returns ρ(y). Note that there is a natural coupling between P and ̃P such that |y − ˆy| ≤ γ/(2L) holds with probability 1. From Theorem 5, we have that there exists a finite ˆY ⊆ R and non-decreasing map ̃Φ : ̃Y → ˆY such that\n\n: 0 ≤ i ≤ g\n\n.\n\nL(RR-on-Bins\n\n ̃Φ ε ; ̃P ) = inf ̃M\n\nL( ̃M; ̃P ),\n\n(11)\n\nwhere ̃M is an ε-DP mechanism mapping ̃Y to ˆY. We can extend ̃Φ to Φ : Y → ˆY given as Φ(y) := ̃Φ(ρ(y)). It is easy to see that since ̃Φ is non-decreasing, Φ is also non-decreasing. From Assumption 14, it follows that ε ; P ) = E\n\nL(RR-on-BinsΦ\n\n(cid:96)(RR-on-BinsΦ\n\nε (y), y)\n\ny∼P\n\n= E\n\ny∼P\n\n≤ E\n\ny∼P\n\n(cid:96)(RR-on-Bins\n\n ̃Φ ε (ρ(y)), y)\n\n(cid:96)(RR-on-Bins\n\n ̃Φ ε (ρ(y)), ρ(y)) + γ/2\n\n(from Assumption 14)\n\n= L(RR-on-Bins\n\n ̃Φ ε ; ̃P ) + γ/2.\n\n(12)\n\n18\n\nSimilarly, for any ε-DP mechanism M mapping Y to ˆY, we can construct an ε-DP mechanism ̃M mapping ̃Y to ˆY where ̃M( ̃y) is sampled as M(y) for y ∼ P |ρ(y)= ̃y. Note that sampling ̃y ∼ ̃P and returning y ∼ P |ρ(y)= ̃y is equivalent to sampling y ∼ P . Thus, we have\n\nL( ̃M; ̃P ) = E ̃y∼ ̃P = E\n\ny∼P\n\n(cid:96)( ̃M ( ̃y), ̃y)\n\n(cid:96)(M (y), ρ(y))\n\n≤ E\n\ny∼P\n\n(cid:96)(M (y), y) + γ/2\n\n(from Assumption 14)\n\n= L(M; P ) + γ/2.\n\n(13)\n\nThus, combining (11), (12), and (13), we get\n\nL(RR-on-BinsΦ\n\nε ; P ) ≤ L(RR-on-Bins\n\n ̃Φ ε ; ̃P ) + γ/2\n\n= inf ̃M ≤ inf M\n\nL( ̃M; ̃P ) + γ/2\n\nL(M; P ) + γ .\n\nNext we show that the infimum of RR-on-Bins is reached by considering the RR-on-Bins with finitely many bins. Towards this end, let RR-on-Binsn ε mechanisms where Φ : Y → ˆY with | ˆY| = n.\n\nε denote the set of all RR-on-BinsΦ\n\nLemma 17. Suppose (cid:96)(ˆy, y) is integrable over y for any ˆy. Then for all ε > 0, it holds that\n\nlim n→∞\n\ninf M∈RR-on-Binsn ε\n\nL(M; P ) ≥\n\ninf M∈RR-on-Bins1 ε\n\nL(M; P ).\n\nRemark: Interestingly, Lemma 17 does not require any assumption about the distribution P or (cid:96) aside from integrability. For example, P can be an unbounded distribution.\n\nProof. Let\n\nα :=\n\ninf M∈RR-on-Bins1 ε\n\nL(M; P ) = inf ˆy∈R\n\n(cid:90)\n\n(cid:96)(ˆy, y)dP (y) .\n\nThis is precisely infM∈RR-on-Bins1 Note that α does not depend on ε.\n\nε\n\nL(M; P ), where the ˆy is selecting the single bin to output to.\n\nFor any n > 1, consider RR-on-BinsΦ bound the loss from below:\n\nε , where Φ : Y → ˆY for ˆY = {ˆy1, . . . , ˆyn}. Then we can\n\nL(RR-on-BinsΦ\n\nε ; P ) =\n\n(cid:90) (cid:32)(cid:32) n (cid:88)\n\ni=1\n\n1 eε + n − 1\n\n(cid:33)\n\n(cid:96)(ˆyi, y)\n\n+\n\neε − 1 eε + n − 1\n\n(cid:33)\n\n(cid:96)(Φ(y), y)\n\ndP (y)\n\n(cid:90) (cid:32) n (cid:88)\n\ni=1\n\n1 eε + n − 1\n\n(cid:33)\n\n(cid:96)(ˆyi, y)\n\ndP (y)\n\n1 eε + n − 1\n\n(cid:96)(ˆyi, y)dP (y)\n\nn (cid:88)\n\n(cid:90)\n\ni=1 n\n(cid:88)\n\ni=1\n\n1 eε + n − 1\n\n· α\n\nn eε + n − 1\n\n· α.\n\n≥\n\n=\n\n≥\n\n=\n\nIn particular, infM∈RR-on-Binsn\n\nε\n\nL(M; P ) ≥\n\nn\n\neε+n−1 α which implies that\n\nlim n→∞\n\ninf M∈RR-on-Binsn ε\n\nL(M; P ) ≥ α .\n\n19\n\nCorollary 18. Suppose (cid:96)(ˆy, y) is integrable over y for any ˆy. Then for all ε > 0, there exists n ≥ 1 such that\n\ninf M∈RR-on-Binsn ε\n\nL(M; P ) = inf n\n\ninf M∈RR-on-Binsn ε\n\nL(M; P ).\n\nlet αn := infM∈RR-on-Binsn\n\nProof. For any n, L(M; P ). From Lemma 17, we have that limn→∞ αn ≥ α1. If inf n αn = α1, then the corollary is true for n = 1. Else, if inf n αn = α(cid:48) < α1, then there exists n0 > 0 such that for all n > n0, it holds that αn > (α1 + α(cid:48))/2. Thus, inf n αn = minn≤n0 αn which implies that the infimum is realized for some finite n.\n\nε\n\nFinally, we show that the infimum over RR-on-Binsn Theorem 15. Lemma 19. Suppose P is bounded within the interval Y = [ymin, ymax]. Suppose (cid:96) : R×R → R≥0 satisfies Assumptions 4 and 14, then for any n > 0, there is an output set ˆY ⊆ R with | ˆY| = n and a non-decreasing map Φ : Y → ˆY such that\n\nε is also achievable, completing the proof of\n\nL(RR-on-BinsΦ\n\nε ; P ) =\n\ninf M∈RR-on-Binsn ε\n\nL(M; P ).\n\nProof. Because of the increasing / decreasing nature of (cid:96), we can restrict the output of Φ to be in the range [ymin, ymax]. Given an output range {ˆy1, . . . , ˆyn}, the RR-on-BinsΦ ε mechanism that minimizes L(RR-on-BinsΦ\n\nε ; P ) satisfies\n\nΦ(y) = arg min\n\nˆyi\n\n(cid:96)(ˆyi, y) .\n\nSo we can consider L(RR-on-BinsΦ\n\nε ; P ) as a function of (ˆy1, . . . , ˆyn) ∈ [ymin, ymax]n.\n\nWe claim that L(RR-on-BinsΦ ε ; P ) is continuous with respect to these inputs {ˆy1, . . . , ˆyn}. Indeed by the Lipschitz continuity, a change in [ˆy1, . . . , ˆyn] to [ˆy(cid:48) i| < δ for some δ > 0 results in a change in L(RR-on-BinsΦ ε ; P ) is a continuous function of (ˆy1, . . . , ˆyn) over a compact set [ymin, ymax]n and hence it attains its infimum. This infimum defines Φ that satisfies the lemma.\n\nε ; P ) bounded by Lδ. Therefore L(RR-on-BinsΦ\n\nn] where |ˆyi − ˆy(cid:48)\n\n1, . . . , ˆy(cid:48)\n\nB ANALYSIS OF DYNAMIC PROGRAMMING ALGORITHM FOR FINDING\n\nOPTIMAL RR-on-Bins\n\nBelow we give correctness and running time analysis of Algorithm 2.\n\nCorrectness Proof. We will prove the following statement by strong induction on i:\n\nA[i][j] =\n\nmin Φ:{y1,...,yi}→R |PΦ|=j\n\n(cid:88)\n\n(cid:88)\n\nS∈PΦ\n\ny∈Y\n\npy · e1[y∈S]·ε · (cid:96)(Φ(S), y),\n\n(14)\n\nwhere the minimum is across all Φ : {y1, . . . , yi} → R such that, for each ˆy ∈ range(Φ), Φ−1(ˆy) is an interval and that |PΦ| = j where the j intervals are S1, . . . , Sj (in increasing order).\n\nBefore we prove the above statement, note that it implies that A[n][d] = (d − 1 + eε) · min ˆY,Φ:Y→ ˆY,|PΦ|=d L(RR-on-BinsΦ ε ; P ). Thus, the last line of the algorithm ensures that we output the optimal RR-on-Bins as desired.\n\nWe will now prove (14) via strong induction on i.\n\n• Base Case. For i = 0 (and thus j = 0), the statement is obviously true.\n\n20\n\n• Inductive Step. Now, suppose that the statement is true for all i = 0, . . . , t − 1 for some t ∈ N. We will show that it is also true for i = t. To see this, we may rewrite the RHS term (for i = t) as\n\nmin Φ:{y1,...,yt}→R |PΦ|=j\n\n(cid:88)\n\n(cid:88)\n\nS∈PΦ\n\ny∈Y\n\n= min 0≤r<t\n\nmin Φ:{y1,...,yt}→R |PΦ|=j {yr+1,...,yt}∈PΦ (cid:32)\n\n= min 0≤r<t\n\nmin Φ:{y1,...,yr}→R |PΦ|=j−1\n\npy · e1[y∈S]·ε · (cid:96)(Φ(S), y)\n\n(cid:88)\n\n(cid:88)\n\nS∈PΦ\n\ny∈Y\n\npy · e1[y∈S]·ε · (cid:96)(Φ(S), y)\n\n(cid:88)\n\n(cid:88)\n\nS∈PΦ\n\ny∈Y\n\npy · e1[y∈S]·ε · (cid:96)(Φ(S), y)\n\n+\n\nmin ˆy=Φ({yr+1,...,yt})\n\n(cid:88)\n\ny∈Y\n\npy · e1[y∈[yr+1,yt]]·ε · (cid:96)(ˆy, y)\n\n(cid:33)\n\n= min 0≤r<t\n\nA[r][j − 1] + L[r + 1][t].\n\nwhere the third inequality follows from the inductive hypothesis and the definition of L[r + 1][t]. The last expression is exactly how A[t][j] is computed in our algorithm. Thus, (14) holds for i = t.\n\nIt is clear that, apart from the computation of L[r + 1][i], the remainder Running Time Analysis. of the algorithm runs in time O(k2). Therefore, the total running time is O(k2 · T ) where T denotes the running time for solving the problem minˆy∈R (cid:80) y∈Y py · e1[y∈[yr,yi]]·ε · (cid:96)(ˆy, y). When the loss function (cid:96) is convex, this problem is a univariate convex optimization problem and can be solved in polynomial time. We can even speed this up further. In fact, for the three main losses we consider (squared loss, Poisson log loss, and absolute-value loss) this problem can be solved in amortized constant time, as detailed below. Therefore, for these three losses, the total running time of the dynamic programming algorithm is only O(k2).\n\n• Squared Loss.\n\nIn this case, the minimizer is simply\n\narg min ˆy∈R\n\n(cid:88)\n\ny∈Y\n\npy · e1[y∈[yr,yi]]·ε · (cid:96)sq(ˆy, y) =\n\n(cid:80)\n\n(cid:80)\n\ny∈Y py · e1[y∈[yr,yi]]·εy y∈Y py · e1[y∈[yr,yi]]·ε\n\n=: ˆy∗\n\nr,i.\n\nr,i, it suffices to keep the values (cid:80)\n\nTherefore, to compute ˆy∗ y∈Y py · e1[y∈[yr,yi]]·ε. We can start with r = i and as we increase i, these quantities can be updated in constant time. Note also that\n\ny∈Y py · e1[y∈[yr,yi]]·εy and (cid:80)\n\nmin ˆy∈R\n\n(cid:88)\n\ny∈Y\n\npy · e1[y∈[yr,yi]]·ε · (cid:96)sq(ˆy, y)\n\n=\n\n(cid:88)\n\ny∈Y\n\npy · e1[y∈[yr,yi]]·ε · y2 − 2\n\n\n\n\n\n(cid:88)\n\ny∈Y\n\npy · e1[y∈[yr,yi]]·ε · y\n\n  ˆy∗\n\nr,i + (ˆy∗\n\nr,i)2.\n\nTherefore, to compute the minimum value, we additionally keep (cid:80) which again can be updated in constant time for each i.\n\ny∈Y py · e1[y∈[yr,yi]]·εy2,\n\n• Poisson Log Loss.\n\nThe minimizer is exactly the same as in the squared loss, i.e.,\n\narg min ˆy∈R\n\n(cid:88)\n\ny∈Y\n\npy · e1[y∈[yr,yi]]·ε · (cid:96)Poi(ˆy, y) =\n\n21\n\n(cid:80)\n\n(cid:80)\n\ny∈Y py · e1[y∈[yr,yi]]·εy y∈Y py · e1[y∈[yr,yi]]·ε\n\n=: ˆy∗\n\nr,i.\n\nTherefore, using the same method as above, we can compute ˆy∗ in amortized constant time. The minimum is the simply\n\nmin ˆy∈R\n\n(cid:88)\n\ny∈Y\n\n\n\n=\n\n\n\n(cid:88)\n\ny∈Y\n\npy · e1[y∈[yr,yi]]·ε · (cid:96)Poi(ˆy, y)\n\npy · e1[y∈[yr,yi]]·ε\n\n  ˆy∗\n\nr,i −\n\n\n\n\n\n(cid:88)\n\ny∈Y\n\npy · e1[y∈[yr,yi]]·εy\n\n  log(ˆy∗\n\nr,i),\n\nso this can also be computed in constant time with the quantities that we have recorded.\n\n• Absolute-Value Loss.\n\nTo describe the minimizer, we define a weighted version of median. Let {(w1, a1), . . . , (wt, at)} be a set of t tuples such that w1, . . . , wt ∈ R≥0 and a1, . . . , at ∈ R with a1 ≤ · · · ≤ at. The weighted median of {(w1, a1), . . . , (wt, at)}, denoted by wmed({(w1, a1), . . . , (wt, at)}) is equal to the minimum value a∗ such that (cid:80) j∈[t] wj)/2. It is not hard to see\n\nwj ≥ ((cid:80)\n\nj∈[t] aj ≤a∗\n\nthat\n\narg min ˆy∈R\n\n(cid:88)\n\ny∈Y\n\npy · e1[y∈[yr,yi]]·ε · (cid:96)abs(ˆy, y) = wmed\n\n(cid:18)(cid:110)(cid:16)\n\npy · e1[y∈[yr,yi]]·ε, y\n\n(cid:17)(cid:111)\n\n(cid:19)\n\ny∈Y\n\n=: ˆy∗\n\nr,i.\n\nr,i ∈ Y. For a fixed r (and varying i), the algorithm is now as\n\nNotice also that we must have ˆy∗ follows: first compute ˆy∗\n\nr,r, and also compute (cid:88)\n\nwlo :=\n\nand\n\ny∈Y y≤ˆy∗\n\nr,r\n\n(cid:88)\n\ny∈Y y>ˆy∗\n\nr,r\n\nwhi :=\n\npy · e1[y∈[yr,yi]]·ε,\n\npy · e1[y∈[yr,yi]]·ε.\n\nr,i = ˆy∗\n\nr,i−1 and update wlo or whi (corresponding to the weight r,i That r,i∈[yr,yi]]·ε ≥ whi,\n\nFor i = r + 1, . . . , k, initialize ˆy∗ change from pyi to eεpyi of yi). We then perform updates to reach the correct value of ˆy∗ is, if wlo < whi, then move to the next larger value in Y; if wlo − pˆy∗ then move to the next smaller value in Y. Otherwise, stop and keep the current ˆy∗ To understand the running time of this subroutine, note that the initial running time for computing r,r and wlo, whi is O(k). Furthermore, each update for ˆy∗ ˆy∗ r,i takes O(1) time. Finally, observe that if we ever move ˆy∗ r,i will never decrease again. As a result, in total across all i = r + 1, . . . , k, the total number of updates can be at most 2k. Thus, the total running time for a fixed r is O(k). Summing up across r ∈ [k], we can conclude that the total running time of the entire dynamic programming algorithm is O(k2).\n\nr,i to a larger value, it must be that yi > ˆy∗\n\nr,i. After this i, ˆy∗\n\n· e1[ˆy∗\n\nr,i.\n\nr,i\n\nC RR-on-Bins WITH APPROXIMATE PRIOR\n\nRecall from Section 3.2 that, when the prior is unknown, we split the budget into ε1, ε2, use the ε1DP Laplace mechanism to approximate the prior and then run the RR-on-Bins with privacy parameter ε2. The remainder of this section gives omitted details and proofs from Section 3.2. Throughout this section, we use k to denote |Y| (the size of the input label set).\n\nC.1 LAPLACE MECHANISM AND ITS GUARANTEES\n\nWe start by recalling the Laplace mechanism for estimating distribution. Recall that the Laplace distribution with scale parameter b, denoted by Lap(b), is the distribution supported on R whose probability density function is 1 2b exp(−|x|/b). The Laplace mechanism is presented in Algorithm 4. It is well-known (e.g., Dwork et al. (2006b)) that this mechanism satisfies ε-DP. Its utility guarantee, which we will use in the analysis of LabelRandomizer, is also well-known:\n\n22\n\nAlgorithm 4 Laplace Mechanism for Estimating Probability Distribution MLap\n\nε\n\n.\n\nParameters: Privacy parameter ε ≥ 0. Input: Labels y1, . . . , yn ∈ Y. Output: A probability distribution P (cid:48) over Y.\n\nfor y ∈ Y do\n\nhy ← number of i such that yi = y h(cid:48) y ← max{hy + Lap(2/ε), 0}\n\nreturn Distribution P (cid:48) over Y such that p(cid:48)\n\ny =\n\n(cid:80)\n\ny\n\nh(cid:48) y∈Y h(cid:48)\n\ny\n\nTheorem 20 (e.g., Diakonikolas et al. (2015)). For any distribution P on Y, n ∈ N, and ε > 0, we have\n\nE y1,...,yn∼P\n\nP (cid:48)∼MLap\n\nε\n\n(y1,...,yn)\n\n[(cid:107)P (cid:48) − P (cid:107)1] ≤ O\n\n(cid:32)(cid:114)\n\n(cid:33)\n\n.\n\nk n\n\n+\n\nk εn\n\nC.2 PROOF OF THEOREM 6\n\nTheorem 6. LabelRandomizerε1,ε2 is (ε1 + ε2)-DP.\n\nProof of Theorem 6. The Laplace mechanism is ε1-DP; by post-processing property of DP, Φ(cid:48) is also ε1-DP. For fixed Φ(cid:48), since RR-on-BinsΦ(cid:48) ε2 is ε2-DP and it is applied only once on each label, the parallel composition theorem ensures that (ˆy1, . . . , ˆyn) is ε2-DP. Finally, applying the basic composition theorem, we can conclude that the entire algorithm is (ε1 + ε2)-DP.\n\nC.3 PROOF OF THEOREM 7\n\nTheorem 7. Let (cid:96) : R × R → R≥0 be any loss function satisfying Assumption 4. Furthermore, assume that (cid:96)(ˆy, y) ≤ B for some parameter B for all y, ˆy ∈ Y. For any distribution P on Y, ε > 0, and any sufficiently large n ∈ N, there is a choice of ε1, ε2 > 0 such that ε1 + ε2 = ε and\n\n[L(RR-on-BinsΦ(cid:48)\n\nε2\n\n; P )] − inf M\n\nL(M; P ) ≤ O\n\n(cid:16)\n\nB · (cid:112)|Y|/n\n\n(cid:17)\n\n,\n\nE y1,...,yn∼P P (cid:48),Φ(cid:48), ˆY (cid:48)\n\nwhere P (cid:48), Φ(cid:48), ˆY (cid:48) are as in LabelRandomizerε1,ε2 and the infimum is over all ε-DP mechanisms M.\n\nProof of Theorem 7. From Theorem 20, we have\n\nE D∼P n P (cid:48)∼MLap\n\nε1\n\n[(cid:107)P (cid:48) − P (cid:107)1] ≤ O\n\n(D)\n\n(cid:32)(cid:114)\n\n(cid:33)\n\n.\n\nk n\n\n+\n\nk ε1n\n\n23\n\nRecall from Theorem 5 that there exists Φ : Y → ˆY such that L(RR-on-BinsΦ infM L(M; P ). Consider instead the RR-on-BinsΦ\n\nε2 mechanism. We have\n\nε ; P ) =\n\n| inf M\n\nL(M; P ) − L(RR-on-BinsΦ ε2\n\n; P )|\n\n= |L(RR-on-BinsΦ\n\nε ; P ) − L(RR-on-BinsΦ \n(cid:12) (cid:12) (cid:12) \n(cid:12) (cid:12)\n\neε eε + | ˆY| − 1\n\neε2 eε2 + | ˆY| − 1\n\n; P )|\n\n−\n\nε2\n\npy · B ·\n\n2(| ˆY| − 1)(eε − eε2 ) (eε + | ˆY| − 1)(eε2 + | ˆY| − 1)\n\n≤\n\n(cid:88)\n\ny∈Y\n\n= B ·\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n+\n\n(cid:88)\n\nˆy∈ ˆY\\{Φ(y)}\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 eε + | ˆY| − 1\n\n−\n\n1 eε2 + | ˆY| − 1\n\n\n\n\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ 2B · (1 − eε2−ε) ≤ 2B(ε − ε2) = 2Bε1.\n\nRecall that RR-on-BinsΦ(cid:48)\n\nε2 is an ε2-DP optimal mechanism for prior P (cid:48). That is, we have\n\nL(RR-on-BinsΦ(cid:48)\n\nε2\n\n; P (cid:48)) ≤ L(RR-on-BinsΦ ε2\n\n; P (cid:48)).\n\nFinally, we also have\n\n|L(RR-on-BinsΦ ε2 |L(RR-on-BinsΦ(cid:48)\n\nε2\n\n; P ) − L(RR-on-BinsΦ ε2 ; P ) − L(RR-on-BinsΦ(cid:48)\n\nε2\n\n; P (cid:48))| ≤ B · (cid:107)P (cid:48) − P (cid:107)1.\n\n; P (cid:48))| ≤ B · (cid:107)P (cid:48) − P (cid:107)1.\n\nCombining the above five inequalities, we arrive at\n\nE y1,...,yn∼P P (cid:48),Φ(cid:48), ˆY (cid:48)\n\n[L(RR-on-BinsΦ(cid:48)\n\nε2\n\n; P )] − inf M\n\n(cid:32)\n\n(cid:32)\n\n(cid:114)\n\nL(M; P ) ≤ O\n\nB ·\n\nε1 +\n\n(cid:33)(cid:33)\n\n.\n\nk n\n\n+\n\nk ε1n\n\nSetting ε1 = (cid:112)k/n then yields the desired bound7.\n\nD DP MECHANISMS DEFINITIONS\n\nIn this section, we recall the definition of various DP notions that we use throughout the paper.\n\nDefinition 21 (Global Sensitivity). Let f be a function taking as input a dataset and returning as output a vector in Rd. Then, the global sensitivity ∆(f ) of f is defined as the maximum, over all pairs (X, X (cid:48)) of adjacent datasets, of ||f (X) − f (X (cid:48))||1.\n\nThe (discrete) Laplace distribution with scale parameter b > 0 is denoted by DLap(b). Its probability mass function is given by p(y) ∝ exp(−|y|/b) for any y ∈ Z.\n\nDefinition 22 (Discrete Laplace Mechanism). Let f be a function taking as input a dataset X and returning as output a vector in Zd. The discrete Laplace mechanism applied to f on input X returns f (X) + (Y1, . . . , Yd) where each Yi is sampled i.i.d. from DLap(∆(f )/ε). The output of the mechanism is ε-DP.\n\nNext, recall that the (continuous) Laplace distribution Lap(b) with scale parameter b > 0 has probability density function given by h(y) ∝ exp(−|y|/b) for any y ∈ R.\n\nDefinition 23 (Continuous Laplace Mechanism, Dwork et al. (2006b)). Let f be a function taking as input a dataset X and returning as output a vector in Rd. The continuous Laplace mechanism applied to f on input X returns f (X) + (Y1, . . . , Yd) where each Yi is sampled i.i.d. from Lap(∆(f )/ε). The output of the mechanism is ε-DP.\n\n7Note that this requires n > k/ε2 for the setting of ε2 = ε − ε1 to be valid.\n\n24\n\nWe next define the discrete and continuous versions of the staircase mechanism (Geng & Viswanath, 2014).\n\nDefinition 24 (Discrete Staircase Distribution). Fix ∆ ≥ 2. The discrete staircase distribution is parameterized by an integer 1 ≤ r ≤ ∆ and has probability mass function given by:\n\npr(i) =\n\nwhere\n\n \n\n\n\na(r) e−εa(r) e−kεpr(i − k∆) pr(−i) for i < 0,\n\nfor 0 ≤ i < r, for r ≤ i < ∆ for k∆ ≤ i < (k + 1)∆ and k ∈ N\n\n(15)\n\na(r) =:=\n\n1 − b 2r + 2b(∆ − r) − (1 − b)\n\n.\n\nLet f be a function taking as input a dataset X and returning as output a scalar in Z. The discrete staircase mechanism applied to f on input X returns f (X)+Y where Y is sampled from the discrete staircase distribution given in (15).\n\nDefinition 25 (Continuous Staircase Distribution). The continuous staircase distribution is parameterized by γ ∈ (0, 1) and has probability density function given by:\n\nhγ(x) =\n\n \n\n\n\na(γ) e−εa(γ) e−kεhγ(x − k∆) hγ(−x) for x < 0,\n\nfor x ∈ [0, γ∆) for x ∈ [γ∆, ∆) for x ∈ [k∆, (k + 1)∆) and k ∈ N\n\n(16)\n\nwhere\n\na(γ) =:=\n\n1 − e−ε 2∆(γ + e−ε(1 − γ)\n\n.\n\nLet f be a function taking as input a dataset X and returning as output a scalar in R. The continuous staircase mechanism applied to f on input X returns f (X) + Y where Y is sampled from the continuous staircase distribution given in (16).\n\nDefinition 26 (Exponential Mechanism, McSherry & Talwar (2007)). Let q(·, ·) be a scoring function such that q(X, r) is a real number equal to the score to be assigned to output r when the input dataset is X. The exponential mechanism returns a sample from the distribution that puts mass ∝ exp(εq(X, r)) on each possible output r. It is (2ε∆(q))-DP, where ∆(q) is defined as the maximum global sensitivity of q(·, r) over all possible values of r.\n\nDefinition 27 (Randomized Response, Warner (1965)). Let ε ≥ 0, and q be a positive integer. The randomized response mechanism with parameters ε and K (denoted by RRε,q) takes as input y ∈ {1, . . . , K} and returns a random sample ̃y drawn from the following probability distribution:\n\nPr[ ̃y = ˆy] =\n\n(cid:40) eε\n\neε+q−1 1\neε+q−1\n\nfor ˆy = y otherwise.\n\n(17)\n\nThe output of RRε,q is ε-DP.\n\nE ADDITIONAL EXPERIMENT DETAILS\n\nWe evaluate the proposed RR-on-Bins mechanism on three datasets, and compare with the Laplace mechanism (Dwork et al., 2006b), the staircase mechanism (Geng & Viswanath, 2014) and the exponential mechanism (McSherry & Talwar, 2007). For real valued labels (the Criteo Sponsored Search Conversion dataset), we use the continuous Laplace mechanism (Definition 23) and the continuous staircase mechanism (Definition 25), and for integer valued labels (the US Census dataset and the App Ads Conversion Count dataset), we use the discrete Laplace mechanism (Definition 22) and the discrete staircase mechanism (Definition 24).\n\n25\n\nE.1 CRITEO SPONSORED SEARCH CONVERSION\n\nThe Criteo Sponsored Search Conversion dataset is publicly available from https://ailab. criteo.com/criteo-sponsored-search-conversion-log-dataset/. To predict the conversion value (SalesAmountInEuro), we use the following attributes as inputs:\n\n• Numerical attributes: Time_delay_for_conversion, nb_clicks_1week, product_price.\n\n• Categorical attributes: product_age_group, device_type, audience_id, product_gender, product_category_1 ∼ product_category_7, product_country,\n\nproduct_brand, product_id, product_title, partner_id, user_id.\n\nAll categorical features in this dataset have been hashed. We build a vocabulary for each feature by counting all the unique values. All the values with less than 5 occurrences are mapped to a single out-of-vocabulary item.\n\nWe randomly partition the dataset into 80%–20% train–test splits. For each evaluation configuration, we report the mean and std over 10 random runs. In each run, the dataset is also partitioned with a different random seed.\n\nOur deep neural network consists of a feature extraction module and 3 fully connected layers. Specifically, each categorical attribute is mapped to a 8-dimensional feature vector using the prebuilt vocabulary for each attribute. The mapped feature vectors are concatenated together with the numerical attributes to form a feature vector. Then 3 fully connected layers with the output dimension 128, 64, and 1 are used to map the feature vector to the output prediction. The ReLU activation is applied after each fully connected layer, except for the output layer.\n\nWe train the model by minimizing the mean squared error (MSE) with L2 regularization 10−4, using the RMSProp optimizer. We use learning rate 0.001 with cosine decay (Loshchilov & Hutter, 2017), batch size 8192, and train for 50 epochs. Those hyperparameters are chosen based on a setup with minor label noise (generated via Laplace mechanism for ε = 6), and then fixed throughout all runs. For the RR-on-Bins mechanism, we use the recommended ε1 = (cid:112)|Y|/n in Theorem 7 to query a private prior, and with the remaining privacy budget, optimize the mean squared loss via dynamic programming. When running Algorithm 2, it would be quite expensive to use Y as the set of all unique (real valued) training labels. So we simply discretize the labels by rounding them down to integer values, and use Y = {0, 1, . . . , 400}. The integer labels are then mapped via Algorithm 1.\n\nE.2 US CENSUS\n\nThe 1940 US Census can be downloaded from https://www.archives.gov/research/census/ 1940. We predict the time that the respondent worked during the previous year (the WKSWORK1 field, measured in number of weeks), based on the following input fields: the gender (SEX), the age (AGE), the marital status (MARST), the number of children ever born to each woman (CHBORN), the school attendance (SCHOOL), the employment status (EMPSTAT), the primary occupation (OCC), and the type of industry in which the person performed an occupation (IND). We use only 50,582,693 examples with non-zero WKSWORK1 field, and randomly partition the dataset into 80%/20% train/test splits. For each evaluation configuration, we report the mean and std over 10 random runs. In each run, the dataset is also partitioned with a different random seed.\n\nOur deep neural network consists of a feature extraction module and 3 fully connected layers. The feature extraction module can be described via the following pseudocode, where the vocabulary size is chosen according to the value range of each field in the 1940 US Census documentation, and the embedding dimension for all categorical features are fixed at 8.\n\nFeatures = Concat([\n\nEmbedding{vocab_size=2}(SEX - 1), AGE / 30.0, Embedding{vocab_size=6}(MARST - 1), Embedding{vocab_size=100}(CHBORN), Embedding{vocab_size=2}(SCHOOL - 1), Embedding{vocab_size=4}(EMSTAT),\n\n26\n\nEmbedding{vocab_size=1000}(OCC), Embedding{vocab_size=1000}(IND),\n\n])\n\nThe features vectors are mapped to the output via 3 fully connected layers with the output dimension 128, 64, and 1. The ReLU activation is applied after each fully connected layer, except for the output layer.\n\nWe train the model by minimizing the MSE with L2 regularization 10−4, using the RMSProp optimizer. We use learning rate 0.001 with cosine decay (Loshchilov & Hutter, 2017), batch size 8192, and train for 200 epochs. For the RR-on-Bins mechanism, we use the recommended ε1 = (cid:112)|Y|/n in Theorem 7 to query a private prior, and with the remaining privacy budget, optimize the mean squared loss via dynamic programming.\n\nE.3 APP ADS CONVERSION COUNT PREDICTION\n\nThe app install ads prediction dataset is collected from a commercial mobile app store. The examples in this dataset are ad clicks, and each label counts post-click events (aka conversions) happening in the app after the user installs the app.\n\nThe neural network models used here is similar to the models used in other experiments: embedding layers are used to map categorical input attributes to dense feature vectors, and then the concatenated feature vectors are passed through several fully connected layers to generate the prediction. We use the Adam optimizer (Kingma & Ba, 2015) and the Poisson regression loss (Cameron & Trivedi, 2013) for training. For the RR-on-Bins mechanism, we use the recommended ε1 = (cid:112)|Y|/n in Theorem 7 to query a private prior, and with the remaining privacy budget, optimize the mean squared error via dynamic programming. Following the convention in event count prediction in ads prediction problems, we train the loss with the Poisson log loss. We report the relative Poisson log loss on the test set with respect to the non-private baseline.\n\nF LABEL CLIPPING\n\nThe Laplace mechanism and staircase mechanism we compared in this paper could both push the randomized labels out of the original label value ranges. This issue is especially severe for small ε’s, as the range of randomized labels could be orders of magnitude wider than the original label value range. Since the original label value range is known (required for deciding the noise scale of each DP mechanism), we could post process the randomized labels by clipping to this range.\n\nWe compare the effects of clipping for both Laplace mechanism and staircase mechanism on the Criteo dataset in Figure 6. In both case, this simple mitigation helps reduce the error significantly, especially for smaller epsilons. So in all the experiments of this paper, we always apply clipping to the randomized labels.\n\nFigure 6 also shows the exponential mechanism (McSherry & Talwar, 2007), which is equivalent to restricting the Laplace noises to the values that would not push the randomized labels out of the original value range. In our case, we implement it via rejection sampling on the standard Laplace mechanism. This algorithm avoid the boundary artifacts of clipping, but to guarantee the same level of privacy, the noise needs to scale with 2/ε, instead of 1/ε as in the vanilla Laplace mechanism. As a result, while it could be helpful in the low epsilon regime, in moderate to high ε regime, it is noticeably worse than all other methods, including the vanilla Laplace mechanism due to the 2× noise scaling with 1/ε.\n\nG COMPARISON WITH RRWithPrior\n\nIn this paper, we extended the formulation of Ghazi et al. (2021a) from classification to regression losses. Here we provide a brief comparison between our algorithm (RR-on-Bins) and the RRWithPrior algorithm from Ghazi et al. (2021a) on the US Census dataset. Instead of using multistage training, we fit RRWithPrior in our feature-oblivious label DP framework, and use the same privately queried global prior as the input to both RR-on-Bins and RRWithPrior. For RRWithPrior,\n\n27\n\n(a) Mechanism\n\n(b) Generalization\n\nFigure 6: MSE on the Criteo dataset, with or without postprocess clipping. (a) measures the error introduced by each DP randomization mechanism on the training labels. (b) measures the test error of the models trained on the corresponding private labels.\n\nPrivacy Budget\n\n0.5 1.0 2.0 4.0 8.0 ∞\n\nMSE (Mechanism)\n\nMSE (Generalization)\n\nRRWithPrior RR-on-Bins\n\nRRWithPrior RR-on-Bins\n\n274.11 274.11 274.11 138.00 11.58 0.00\n\n179.88 159.49 107.89 36.43 2.54 0.00\n\n273.97 273.97 273.97 145.89 134.26 134.27\n\n183.28 172.64 152.03 136.59 134.35 134.27\n\nTable 2: MSE on the US Census dataset, comparing RR-on-Bins with RRWithPrior. The first block (Mechanism) measures the error introduced by the DP randomization mechanisms on the training labels. The second block (Generalization) measures the test error of the models trained on the corresponding private labels. Note RRWithPrior was designed for classification loss (Ghazi et al., 2021a), here we just ignore the numerical similarity metric and treat each integer label as a separate category.\n\nwe simply treat each of the integer label values in US Census as an independent category during the randomization stage. Once the private labels are obtained, the training stage is identical for both algorithms. The results are shown in Table 2. Note the results are for reference purposes, as this is not a fair comparison. Because RRWithPrior was designed for classification problems (Ghazi et al., 2021a), which ignore the similarity metrics between different labels, while RR-on-Bins takes that into account.\n\n28\n\n1061080.050.10.30.50.811.523468infepsilon010K20K30K40K50Kmean squared error1051061070.050.10.30.50.811.523468infepsilon05K10K15K20K25K30KLaplace (w/o clip)LaplaceStaircase (w/o clip)StaircaseExponential",
    "reference": "# Summary Of The Paper\n\nPaper proposes regression with label differential privacy and introduces randomized response on bins as a new mechanism to induce differential privacy on labels.\n\n# Strength And Weaknesses\n\nProposed method is interesting and of practical use where one might want label differential privacy. Empirical evaluation shows that the proposed method performs better than other common DP methods such as Laplace, Exponential, and staircase. \n\nPaper is clearly written and is easy to follow. Assuming the proofs go through (I did not check them in detail), I think the paper will make positive contribution to the DP community. My only question is that there should be some discussion on why it is not compared against other label-DP methods such as [1].\n\n[1] Ghazi, B., Golowich, N., Kumar, R., Manurangsi, P., & Zhang, C. (2021). Deep learning with label differential privacy. Advances in Neural Information Processing Systems, 34, 27131-27145.\n\n# Clarity, Quality, Novelty And Reproducibility\n\npaper is clearly written and is easy to follow\n\n# Summary Of The Review\n\npaper is easy to follow and has the potential for a net positive impact.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nHYPER-PARAMETER TUNING FOR FAIR CLASSIFICATION WITHOUT SENSITIVE ATTRIBUTE ACCESS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFair machine learning methods seek to train models that balance model performance across demographic subgroups defined over sensitive attributes like race and gender. Although sensitive attributes are typically assumed to be known during training, they may not be available in practice due to privacy and other logistical concerns. Recent work has sought to train fair models without sensitive attributes on training data. However, these methods need extensive hyper-parameter tuning to achieve good results, and hence assume that sensitive attributes are known on validation data. However, this assumption too might not be practical. Here, we propose Antigone, a framework to train fair classifiers without access to sensitive attributes on either training or validation data. Instead, we generate pseudo sensitive attributes on the validation data by training a biased classifier and using the classifier’s incorrectly (correctly) labeled examples as proxies for minority (majority) groups. Since fairness metrics like demographic parity, equal opportunity and subgroup accuracy can be estimated to within a proportionality constant even with noisy sensitive attribute information, we show theoretically and empirically that these proxy labels can be used to maximize fairness under average accuracy constraints. Key to our results is a principled approach to select the hyper-parameters of the biased classifier in a completely unsupervised fashion (meaning without access to ground truth sensitive attributes) that minimizes the gap between fairness estimated using noisy versus ground-truth sensitive labels.\n\nINTRODUCTION\n\n1 Deep neural networks have achieved state-of-the-art accuracy on many tasks including face recognition (Buolamwini & Gebru, 2018; Grother et al., 2010; Ngan & Grother, 2015), autonomous driving (Zhang et al., 2021; Chitta et al., 2021), medical image diagnosis (Litjens et al., 2017; Cheplygina et al., 2019), etc. But, prior work (Hovy & Søgaard, 2015; Oren et al., 2019; Hashimoto et al., 2018a) has found that state-of-the-art networks exhibit unintended biases towards specific population groups, especially harming minority groups. Seminal work by Buolamwini & Gebru (2018) demonstrated, for instance, that commercial face recognition systems had lower accuracy on darker skinned women than other groups. A body of work has sought to design fair machine learning algorithms that account for a model’s performance on a per-group basis (Prost et al., 2019; Sagawa* et al., 2020; Liu et al., 2021; Sohoni et al., 2020).\n\nMuch of the prior work assume that demographic attributes like gender and race on which we seek to train a fair model, which we refer to as sensitive attributes, are available on training and validation data Sagawa* et al. (2020); Prost et al. (2019). However, there is a growing body of literature (Veale & Binns, 2017; Holstein et al., 2019) highlighting many real-world settings in which sensitive attributes may not be available. This is for multiple reasons. For example, the data subject may abstain from providing sensitive information to eschew potential discrimination in future (Markos et al., 2017). In other settings, the attributes on which the model discriminates might not even be known (Citron & Pasquale, 2014; Pasquale, 2015). For instance, in algorithmic hiring decisions, K ̈ochling & Wehner (2020) highlight that bias and discrimination are recognized only after making real world decisions on applicants due to unknown attributes on which the model discriminates during training. Consequently, a large American e-commerce company had to cease using algorithmic tools for hiring purposes as it was unintentionally discriminating female applicants (Dastin, 2018).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nRecent work seeks to train fair classifiers without access to sensitive attributes on the training set (Liu et al., 2021; Creager et al., 2021; Nam et al., 2020; Hashimoto et al., 2018a). The common theme across these methods is to up-weight misclassified examples either by splitting the training stage into two separate stages Liu et al. (2021) (identify mis-classified examples in stage 1 and upweight in stage 2) or by alternating between these stages across training epochs Nam et al. (2020) (identifying misclassified examples in one epoch and upweighting in the next). However, Liu et al. (2021) has shown that these methods are highly sensitive to choice of hyper-parameters; the up-weighting factor, for example, can have a large impact on the resulting model’s fairness. Some methods, therefore, tune hyper-parameters assuming access to sensitive information on the validation dataset. In fact, without this information, Liu et al. (2021) observed that these methods sometimes do worse than using standard ERM. But, sensitive information on the validation dataset may not be available for the same reasons they are hard to acquire on training data.\n\nIn this paper, we propose Antigone, a simple, principled approach that enables hyper-parameter tuning for fairness without access to sensitive attributes on validation data. Antigone can be used to tune hyper-parameters for any prior method, for instance, JTT (Liu et al., 2021), LfF (Nam et al., 2020), CVaR DRO (Hashimoto et al., 2018a), that trains fair models without sensitive attributes on training data, and for several fairness metrics including demographic parity, equal opportunity and worst sub-group accuracy. We note that these prior methods also address the problem of spurious correlations (Sagawa et al., 2020; Wang & Culotta, 2021) and their impact on accuracy. As such, Antigone can also be used to address spurious correlations, but we focus on fairness in this paper.\n\nAntigone builds on the same intuition as in prior work: mis-classified examples of a classifier trained with standard empirical risk serves as an effective proxy for minority groups. Accordingly, Antigone trains a biased classifier as a noisy sensitive attributes labeller on the validation data, labelling correctly and incorrectly classified examples as majority and minority groups, respectively. But this raises a key question: how do we select the hyper-parameters of the noisy labeler?\n\nIntuitively, to maximize utility of the noisily labelled validation set, we seek to maximize the fraction of minority (majority) samples in the incorrect (correct) sets. Since this cannot be measured directly, Antigone instead maximizes the distance between the data distributions of the two sets, which we measure using the Euclidean distance between the means (EDM) of the two distributions. We provide theoretical justification for our choice under the mutually contaminated (MC) noise model (Scott et al., 2013) that assumes that a fraction of majority (minority) group labels are contaminated with labels from minority (majority) group. Lamy et al. (2019) et al. show that common fairness metrics can be estimated up to a proportionality constant under the MC model. We show that Antigone’s EDM criteria maximizes this proportionality constant, thus providing the most reliable estimates of fairness.\n\nWe evaluate Antigone in conjunction with JTT Liu et al. (2021) on the CelebA, Waterbirds and Adult datasets which are commonly used in fairness literature. We compare Antigone with baselines that assume ground-truth knowledge of sensitive attributes and standard ERM on demographic parity, equal opportunity, and worst subgroup accuracy. Antigone significantly closes the fairness gap between standard ERM training and fairness with ground truth sensitive attributes. Compared with GEORGE that estimates majority/minority group labels by clustering the activations of an ERM model, Antigone produces more accurate labels and results in improved fairness. Ablation studies demonstrate the effectiveness of Antigone’s EDM based hyper-parameter tuning.\n\n2 PROPOSED METHODOLOGY We now describe Antigone, starting with the problem formulation (Section 2.1) followed by a description of the Antigone algorithm (Section 2.2).\n\n2.1 PROBLEM SETUP\n\nConsider a data distribution over set D = X × A × Y, the product of input data (X ), sensitive i }N tr attributes (A) and target labels (Y) triplets. We are given a training set Dtr = {xtr i , atr with N tr training samples, and a validation set Dval = {xval i=1 with N val validation samples. We will assume binary sensitive attributes (A ∈ {0, 1}) and target labels (Y ∈ {0, 1}). We note that for now Antigone is limited to binary sensitive attributes, but can be extended to multiple target labels.\n\ni }N val\n\ni , ytr\n\n, aval i\n\n, yval\n\ni=1\n\ni\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nWe seek to train a machine learning model, say a deep neural network (DNN), which can be represented as a parameterized function fθ : X → Y ∈ {0, 1}, where θ ∈ Θ are the trainable parameters, e.g., DNN weights and biases. Standard fairness unaware empirical risk minimization (ERM) optimizes over trainable parameters θ to minimize average loss LERM :\n\nLERM = −\n\n1 N tr\n\nN (cid:88)\n\ni=1\n\nl(xtr\n\ni , ytr\n\ni ),\n\n(1)\n\non Dtr, where l(xi, yi) is the binary cross-entropy loss.\n\nOptimized model parameters θ∗ are obtained by invoking a training algorithm, for instance stochastic gradient descent (SGD), on the training dataset and model, i.e., θ∗,γ = MERM (Dtr, fθ, γ), where γ ∈ Γ are hyper-parameters of the training algorithm including learning rate, training epochs etc. Hyper-parameters are tuned by evaluating models fθ∗,γ for all γ ∈ Γ on Dval and picking the best model. More sophisticated algorithms like Bayesian optimization can also be used.\n\nSince standard ERM models suffer from unintended biases in their predictions, fair ML algorithms seek instead to optimize metrics that explicitly account for the performance on demographic subgroups. We review three commonly used metrics below:\n\n• Demographic parity (DP): DP requires the model’s outcomes to be independent of sensi-\n\ntive attribute. In practice, we seek to minimize the demographic parity gap: θ = P[f”[fθ(X) = 1|A = 1] − P[fθ(X) = 1|A = 0]). Equal opportunity (EO): EO aims to equalize only the model’s true positive rates across sensitive attributes. In practice, we seek to minimize\n\n∆DP\n\n(2)\n\n∆EO\n\nθ = P[fθ(X) = 1|A = 1, Y = 1] − P[fθ(X) = 1|A = 0, Y = 1].\n\n(3)\n\n• Worst-group accuracy (WGA): WGA seeks to maximize the minimum accuracy over all\n\nsub-groups over sensitive attributes and target labels.That is, we seek to maximize:\n\nW GAθ =\n\nmin a∈{0,1},y∈{0,1}\n\nP[f (x) = y|A = a, Y = y].\n\n(4)\n\nIn all three settings, we seek to train models that optimize fairness under a constraint on average such that P[fθ(x) = accuracy. For example, for equal opportunity, we seek θ∗ = arg minθ∈Θ ∆EO Y ] ≥ Acc, where Acc is a user specified constraint. With access to sensitive attributes, the fairness metric (and average accuracy) can be evaluated on the validation set. The challenge here is that sensitive attributes are unavailable.\n\nθ\n\n2.2 ANTIGONE ALGORITHM\n\nWe now describe the Antigone algorithm which consists of three main steps. In step 1, we train multiple intentionally biased ERM models that each provide pseudo sensitive attribute labels on validation data. We view each model as a noisy sensitive attribute labeller on the validation set. In step 2, we use the proposed EDM metric to pick a noisy labeller from step 1 with the least noise. Finally, in step 3, we use the labelled validation set from step 2 to tune the hyper-parameters of methods like JTT that train fair classifiers without sensitive attributes on training data.\n\nStep 1: Generating sensitive attribute labels on validation set. In step 1, we use the training dataset and standard ERM training to obtain a set of classifiers, θ∗,γ = MERM (Dtr, fθ, γ), each corresponding to a different value of training hyper-parameters γ ∈ Γ. As we discuss in Section 2.1, these include learning rate, weight decay and number of training epochs. Each classifier is used to generate pseudo sensitive attribute labels on the validation set by assigning correctly (incorrectly) classified examples to the majority (minority) groups. That is, each classifier yields a validation set\n\nDval,γ = {xval\n\ni\n\n, aval,γ\n\ni\n\n, yval\n\ni }N val\n\ni=1\n\n∀γ ∈ Γ\n\n(5)\n\nwhere:\n\n(cid:26)1,\n\nif fθ∗,γ (xval\n\n) = yval\n\naval,γ\n\ni 0, otherwise. From these noisily labelled validation sets, we now seek to pick the one whose pseudo sensitive attribute labels match most closely with true (but unknown) sensitive attributes. That is, we seek to pick the hyper-parameters corresponding to the “best” noisy labeller.\n\n(6)\n\n=\n\ni\n\ni\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nStep 2: Picking the best noisy labeller. The noisy labellers in Step 1 partition inputs in the validation set into two sets containing correctly and incorrectly classified inputs. These serve as proxies for majority and minority groups, respectively. Specifically, let the correct set (or noisily labeled set of majority examples) be X val,γ A=1,noisy = {xval = 1} and the incorrect set (or noisily labeled set of minority examples) be X val,γ\n\n: aval,γ\n\n: aval,γ\n\n= 0}.\n\nA=0,noisy = {xval\n\ni\n\ni\n\ni\n\ni\n\nTo estimate fairness accurately, we would like our noisy labeler to be biased, i.e., to place all majority (minority) group inputs in the correct (incorrect) set. In the absence of true sensitive attribute labels, we can measure bias using the distance between the data distributions in the correct and incorrect sets. In Antigone, we pick the simplest distance metric between two distributions, i.e., the Euclidean distance between their means (EDM). Formally,\n\nEDM γ = ∥μ(X val,γ\n\nA=1,noisy) − μ(X val,γ\n\nA=0,noisy)∥2\n\n(7)\n\nwhere μ(.) represents the empirical mean of a dataset. In Section 2.3 we theoretically justify this choice. We pick γ∗ = arg maxγ∈Γ EDM γ. Note that in practice we pick two different noisy labellers corresponding to target labels Y = {0, 1}.\n\nStep 3: Training a fair model. Step 2 yields Dval,γ∗ , a validation dataset with (estimated) sensitive attribute labels. We can provide Dval,γ∗ as an input to any method that trains fair models without access to sensitive attributes on training data, but requires a validation set with sensitive attribute labels to tune its own hyper-parameters. In our experimental results, we use Dval,γ∗ to tune the hyper-parameters of JTT (Liu et al., 2021) and GEORGE (Sohoni et al., 2020). We note that GEORGE proposes its own method to obtain sensitive attributes labels on validation data, but replacing it with Antigone improves on GEORGE’s performance.\n\n2.3 ANALYZING ANTIGONE UNDER MC NOISE\n\nPrior work Lamy et al. (2019) has modeled noisy sensitive attributes using the mutually contaminated (MC) noise model Scott et al. (2013). Here, it is assumed that we have access to noisy datasets, DA=0,noisy and DA=1,noisy, corresponding to minority and majority groups, respectively, that are mixtures of their ground-truth datasets D0 and D1. Specifically,\n\nDA=1,noisy = (1 − α)DA=1 + αDA=0\n\nDA=0,noisy = βDA=1 + (1 − β)DA=0\n\n(8)\n\nwhere α and β are noise parameters. Note that strictly speaking Equation 8 should refer to the probability distributions of the respective datasets, but we will abuse this notation to refer to the datasets themselves. As such Equation 8 says that fraction α of the noisy majority group, DA=1,noisy, is contaminated with data from the minority group, and fraction β of the noisy minority group, DA=0,noisy, is contaminated with data from the majority group. An extension of this model assumes that the noise parameters are target label dependent, i,e., (α0,β0) for Y = 0 and (α1,β1) for Y = 1.\n\nNote that the MC model assumes that noisy datasets are constructed by sampling independently from the ground-truth distributions. While this is not strictly true in our case since the noise in our sensitive attribute labels might be instance dependent, the MC model can still shed light on the design of Antigone.\n\n(Lamy et al., 2019) Under the MC noise model in Equation 8, demographic parity Proposition 1. and equal opportunity gaps measured on the noisy datsets are proportional to the true DP and EO gaps. Mathematically:\n\n∆DP (DA=0,noisy ∪ DA=1,noisy) = (1 − α − β)∆DP (DA=0 ∪ DA=1),\n\nand\n\n∆EO(DA=0,noisy ∪ DA=1,noisy) = (1 − α1 − β1)∆EO(DA=0 ∪ DA=1).\n\n(9)\n\n(10)\n\nFrom Equation 9 and Equation 10, we can conclude that under the MC noise model, the DP and EO gaps can be equivalently minimized using noisy sensitive attribute labels, assuming independent\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Figure illustrates the schematic of Antigone generating pseudo sensitive attributes on CelebA validation data by training a biased classifier and using the classifier’s incorrectly (correctly) labeled examples as proxies for minority (majority) groups.\n\ncontamination and infinite validation data samples. In practice, these assumptions do not hold, however, and therefore we seek to maximize the proportionality constant 1 − α − β (or 1 − α1 − β1) to minimize the gap between the true and estimated fairness values.\n\nLemma 1. Assume XA=0,noisy and XA=1,noisy correspond to the input data of noisy datasets in the MC model. Then, maximizing the EDM between the XA=0,noisy and XA=1,noisy, i.e., ∥μ(XA=0,noisy) − μ(XA=1,noisy)∥2 maximizes 1 − α − β.\n\nProof. From Equation 8, we can see that ∥μ(XA=0,noisy) − μ(XA=1,noisy)∥2 = (1 − α − β)2∥μ(XA=0) − μ(XA=1)∥2. Here ∥μ(XA=0) − μ(XA=1)∥2 is the EDM between the ground truth majority and minority data and is therefore a constant. Hence, maximizing EDM between XA=0,noisy and XA=1,noisy maximizes 1 − α − β.\n\nIn practice, we separately maximize EDM for target labels Y = {0, 1} and hence maximize both 1 − α0 − β0 and 1 − α1 − β1. We note that our theoretical justification motivates the use of EDM for DP and EO fairness. While not exact, minimizing α + β using EDM as a proxy is still helpful for WGA because it reduces contamination and, empirically, provides more reliable estimates for sub-group accuracy.\n\n3 EXPERIMENTAL SETUP We empirically evaluate Antigone on the CelebA, Waterbirds and UCI Adult datasets which have been extensively studied in fairness literature. In this section, we present the details about Antigone’s implementation, evaluation and network architecture used for these three datasets. To begin, we note Antigone can be deployed in conjunction with any method that trains fair classifiers without sensitive attributes on training data. We evaluate Antigone with one such state-of-the-art method, JTT (Liu et al., 2021). We begin by briefly describing how Antigone is deployed in conjunction with JTT.\n\nJTT+Antigone: JTT operates in two stages. In the first stage, a biased model is trained using T epochs of standard ERM training to identify the incorrectly classified training examples. In the second stage, the misclassified examples are upsampled λ times and the model is trained again to completion with standard ERM. The hyperparameters of stage 1 and stage 2 classifiers, including early stopping epoch T and upsampling factor λ, are jointly tuned using a validation dataset with ground-truth sensitive attribute labels. We replace the ground-truth validation dataset with noisy sensitive attributes obtained from Antigone.\n\n3.1 CELEBA DATASET\n\nDataset details: CelebA (Liu et al., 2015) is an image dataset, consisting of 202,599 celebrity face images annotated with 40 attributes including gender, hair colour, age, smiling, etc. The task is to predict hair color, which is either blond Y = 1 or non-blond Y = 0 and the sensitive attribute is\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\ngender A = {Men, Women}. The dataset is split into training, validation and test sets with 162770, 19867 and 19962 images, respectively. Only 15% of individuals in the dataset are blond, and only 6% of blond individuals are men. Consequently, the baseline ERM model under-performs on the blond men.\n\nHyper-parameter settings: In all our experiments using CelebA dataset, we fine-tune a pretrained ResNet50 architecture for a total of 50 epochs using SGD optimizer and a batch size of 128. We tune Antigone and JTT over three pairs of learning rates and weight decays, (1e − 04, 1e − 04), (1e − 04, 1e − 02), (1e − 05, 1e − 01), which are also the values used in JTT. For Antigone, we also explore early stopping at any of the 50 training epochs. Antigone’s For JTT, we explore over T ∈ hyper-parameters are tuned using the EDM approach. {1, 2, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50} and λ ∈ {20, 50, 100} as reported in their paper. JTT’s hyper-parameters are tuned using the validation dataset produced by Antigone. We report results for DP, EO and WGA fairness metrics. In each case, we seek to optimize fairness while constraining average accuracy to ranges {[90, 91), [91, 92), [92, 93), [93, 94), [94, 95)}.\n\n3.2 WATERBIRDS DATASET\n\nDataset details: Waterbirds is a synthetically generated dataset, containing 11,788 images of water and land birds overlaid on top of either water or land backgrounds (Sagawa* et al., 2020). The task is to predict the bird type, which is either a waterbird Y = 1 or a landbird Y = 0 and the sensitive attribute is the background A = {Water background, Land background}. The dataset is split into training, validation and test sets with 4795, 1199 and 5794 images, respectively. While the validation and test sets are balanced within each target class, the training set contains a majority of waterbirds (landbirds) in water (land) backgrounds and a minority of waterbirds (landbirds) on land (water) backgrounds. Thus, the baseline ERM model under-performs on the minority group.\n\nHyper-parameter settings: In all our experiments using Waterbirds dataset, we train a pretrained ResNet50 architecture for a total of 300 epoch using the SGD optimizer and a batch size of 64. We tune Antigone and JTT over three pairs of learning rates and weight decays, (1e − 03, 1e − 04), (1e − 04, 1e − 01), (1e − 05, 1.0), which are also the values used in JTT. For JTT, we explore over T ∈ {25, 40, 50, 60, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300} and λ ∈ {20, 50, 100} as reported in their paper. In each case, we seek to optimize fairness while constraining average accuracy to ranges {[94, 94.5), [94.5, 95), [95, 95.5), [95.5, 96), [96, 96.5)}.\n\n3.3 BASELINES FOR COMPARISONS\n\nWe evaluate JTT+Antigone against several baselines.\n\nStandard ERM: A naive baseline is a standard ERM model that only seeks to maximize average accuracy and does not consider fairness. We train standard ERM models using the same network architectures and training hyper-parameters used for Antigone + JTT as reported above.\n\nJTT + Ground-truth sensitive attributes: An upper bound for JTT+Antigone is a JTT model trained with ground-truth sensitive attributes on validation data, i.e., the overall approach used in JTT. For this, we used the reference implementations provided by JTT.\n\nGEORGE Sohoni et al. (2020): GEORGE is a competing approach to Antigone that does not assume access to sensitive attributes on either training or validation data. GEORGE operates in two stages: In stage 1, an ERM model is trained until completion on the ground-truth target labels. The activation in the penultimate layer of the ERM model are clustered into k clusters to generate pseudo sensitive attributes on both the training and validation datasets. These pseudo sensitive attributes are used to train and tune the hyper-parameters of a Group DRO model Sagawa* et al. (2020).\n\nFor a fair comparison with GEORGE, we replace its stage 1 with Antigone, and use the resulting pseudo sensitive attribute labels to tune the hyper-parameters of a Group DRO model Sagawa* et al. (2020). We refer to this approach as GEORGE+Antigone.\n\n4 EXPERIMENTAL RESULTS We now discuss the results of our empirical evaluations. We begin by analyzing the quality of sensitive attribute labels produced by Antigone and evaluate JTT/GEORGE+Antigone.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Dataset Details\n\n(b) EDM and noise parameters on Waterbirds\n\nFigure 2: Figure (a) illustrates CelebA and Waterbirds datasets along with fraction of each sub-group examples in their respective training dataset. Figure (b) shows Euclidean Distance between Means (EDM) and noise parameters (α1, β1 and 1 − α1 − β1) for the positive target class of Waterbirds dataset. The noise parameters are unknown in practice. Blue dot indicates the model that we pick to generate pseudo sensitive attributes, while black star indicates the model that maximizes 1−α1 −β1. Quality of Antigone’s sensitive attribute labels: Antigone seeks to generate accurate sensitive attribute labels on validation data based on the EDM criterion (Lemma 1). In Figure 2(b), we empirically validate Lemma 1 by plotting EDM and noise parameters α1 (contamination in minority group labels), β1 (contamination in minority group labels) and 1 − α1 − β1 (proportionality constant between true and estimated fairness) on Waterbirds dataset (similar plot for CelebA dataset is in Appendix Figure 3). As per Lemma 1, EDM allows us to maximize 1 − α1 − β1 since it is ideally proportional to this value. From the figure, we observe that in both cases the EDM metric indeed captures the trend in 1 − α1 − β1, enabling early stopping at an epoch that minimizes contamination.\n\nTable 1: F1 Scores and Accuracy of the pseudo sensitive attribute labels generated by Antigone (w/o EDM), GEORGE, GEORGE with k = 2 clusters and Antigone (w/ EDM) on CelebA and Waterbirds datasets. Bold indicates higher F1 Scores/Accuracy across different methods. For CelebA, BM, BW, NBW and NBM refer to Blond Men, Blond Women, Nonblond Women and Non-blond Men, respectively. For Waterbirds, WL, WW, LW and LL refer to waterbirds landbkgd, waterbirds waterbkgd, landbirds waterbkgd and landbirds landbkgd, respectively.\n\nBM 0.28 ± 0.01 BW 0.95 ± 0.01 NBW 0.22 ± 0.02 NBM 0.67 ± 0.01\n\n0.12 ± 0.01 0.51 ± 0.02 0.6 ± 0.01 0.31 ± 0.01\n\n0.13 ± 0.02 0.43 ± 0.04 0.42 ± 0.01 0.4 ± 0.02\n\n0.35 ± 0.04 0.96 ± 0.00 0.22 ± 0.01 0.68 ± 0.01\n\nAntigone (w/o EDM)\n\nAntigone (w/ EDM)\n\nGEORGE (k = 2)\n\nCelebA (F1 Scores)\n\nGEORGE\n\nAcc.\n\n0.59 ± 0.01\n\n0.33 ± 0.01\n\n0.48 ± 0.00\n\n0.60 ± 0.00\n\nThe early stopping points based on EDM and oracular knowledge of 1 − α1 − β1 are shown in a blue dot and star, respectively. For Waterbirds these are very close.\n\nWaterbirds (F1 Scores)\n\n0.41 ± 0.02 WL WW 0.72 ± 0.00 LW 0.58 ± 0.02 0.76 ± 0.01 LL\n\n0.43 ± 0.02 0.36 ± 0.02 0.44 ± 0.03 0.34 ± 0.02\n\n0.52 ± 0.01 0.43 ± 0.02 0.55 ± 0.03 0.55 ± 0.03\n\n0.76 ± 0.03 0.83 ± 0.01 0.78 ± 0.04 0.84 ± 0.02\n\nAcc.\n\n0.68 ± 0.01\n\nNext, we evaluate the F1 scores of Antigone’s noisy sensitive attributes for all four subgroups in the CelebA and Waterbirds datasets. In Table 1 we compare Antigone’s F1 Score to GEORGE with the baseline number of clusters and GEORGE with k = 2 clusters. Across CelebA and Waterbirds datasets and all four subgroups, we find that Antigone outperforms GEORGE except for one sub-group in CelebA dataset. In the Appendix Table 4, we also include precision and recall values and reach similar conclusion. Additionally, we note that the pseudo label generated by Antigone are 1.25 and 1.5 times more accurate than GEORGE’s pseudo labels on CelebA and Waterbirds datasets, respectively.\n\n0.53 ± 0.03\n\n0.30 ± 0.02\n\n0.81 ± 0.02\n\nTo understand the benefits of the proposed EDM metric, we implement a version of Antigone but tune hyper-parameters using standard ERM. We refer to this as Antigone (w/o EDM) and find in Table 1 that Antigone with EDM outperforms the version without EDM. We later report on the fairness achieved by these different versions.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: (Avg. Accuracy, Fairness) on test data for different validation accuracy thresholds on the CelebA dataset. Lower DP and EO gaps are better. Higher WGA is better.\n\nVal. Thresh.\n\nMethod\n\nDP Gap\n\nEO Gap\n\nWorst-group Acc.\n\n[94, 95)\n\n[93, 94)\n\n[92, 93)\n\n[91, 92)\n\n[90, 91)\n\nAntigone + JTT\n\n(94.6, 15.0) ± (0.2, 0.7) Ground-Truth + JTT (94.7, 14.9) ± (0.2, 0.6)\n\n(94.7, 30.1) ± (0.2, 3.2) (94.5, 30.4) ± (0.2, 2.3)\n\n(94.4, 59) ± (0.2, 4.7) (94.3, 62.1) ± (0.3, 3.2)\n\nAntigone + JTT\n\n(93.7, 13.1) ± (0.2, 0.7) Ground-Truth + JTT (93.6, 13.1) ± (0.1, 0.6)\n\n(93.6, 26.4) ± (0.4, 5.0) (93.6, 22.7) ± (0.3, 2.7)\n\n(93.4, 62.6) ± (0.2, 7.0) (93.4, 67.9) ± (0.1, 1.9)\n\nAntigone + JTT\n\n(92.7, 11.1) ± (0.2, 0.5) Ground-Truth + JTT (92.7, 11.2) ± (0.3, 0.5)\n\n(92.3, 20.2) ± (0.2, 3.4) (92.7, 16.9) ± (0.4, 2.9)\n\n(92.7, 68.1) ± (0.4, 3.7) (92.7, 72.5) ± (0.2, 1.3)\n\nAntigone + JTT Ground-Truth + JTT\n\n(91.7, 9.6) ± (0.1, 0.5) (91.8, 9.7) ± (0.2, 0.5)\n\n(91.5, 16.3) ± (0.3, 3.4) (91.8, 10.1) ± (0.3, 4.1)\n\n(91.3, 63.2) ± (0.3, 2.6) (91.8, 77.3) ± (0.1, 2.4)\n\nAntigone + JTT Ground-Truth + JTT\n\n(91.0, 8.3) ± (0.2, 0.4) (91.0, 8.4) ± (0.2, 0.4)\n\n(90.9, 13.1) ± (0.1, 3.6) (90.7, 6.8) ± (0.4, 3.7)\n\n(90.9, 63.1) ± (0.5, 4.4) (91.4, 78.6) ± (0.2, 2.0)\n\nERM\n\n(95.8, 18.6) ± (0.0, 0.3)\n\n(95.8, 46.4) ± (0.0, 2.2)\n\n(95.8, 38.7) ± (0.0, 2.8)\n\nAntigone + JTT: Next, in Table 2, compare the test accuracy and fairness achieved by Antigone with JTT (Antigone + JTT) versus a baseline ERM model and with JTT using ground-truth sensitive attributes (Ground-Truth+JTT). As expected, baseline ERM yields unfair outcomes on all three fairness metrics: DP, EO and WGA. We observe that Antigone + JTT improves fairness over the baseline ERM model and closes the gap with Ground-Truth+JTT.\n\nOn DP, Antigone + JTT is very close to Ground-Truth + JTT results. Antigone + JTT substantially improve upon the EO Gap achieved by standard ERM. Antigone + JTT improves WGA from 38.7% using standard ERM to 68.1% at the expense of 3% accuracy drop. Ground-Truth + JTT improves WGA further up to 78.6% but with a larger average accuracy drop. Antigone + JTT achieves highest fairness for relatively high average accuracy values, although one would expect fairness to reduce with higher average accuracy. We believe this in part due to the noise in sensitive attribute labels that Antigone generates. Data for Adults (shown in the Appendix Table 8) and Waterbirds (shown in the Appendix Table 6) have the same trends.\n\nCelebA\n\nMethod\n\nAvg Acc\n\nTable 3: Performance of GEORGE using Antigone’s noisy validation data compared with GEORGE by itself. We observe that on CelebA and Waterbirds dataset, GEORGE + Antigone out-performs GEORGE, even if GEORGE assumes knowledge of number of clusters (k = 2) in its clustering step. We show errors for over five runs.\n\nComparison with GEORGE: Like Antigone, GEORGE also generates pseudo-sensitive attributes on validation data, but as we noted in Table 1, Antigone’s labels are more accurate and have higher for different F1 scores sub-groups. We now in compare the fairness terms of WGA achieved by GEORGE versus Antigone +GEORGE in which we use Antigone’s labeled validation data to tune GEORGE’s training algorithm. Antigone + GEORGE is more fair than GEORGE alone on CelebA and Waterbirds (Table 3). On CelebA, Antigone + GEORGE has a small drop in average accuracy compared to GEORGE, while on Waterbirds, both average accuracy and fairness are better. In interpreting the errors on Waterbirds dataset, we should note that Antigone + GEORGE’s WGA is atleast 8.4% higher than GEORGE’s in each one of our multiple runs, except in one run where GEORGE has 1% higher WGA compared to Antigone + GEORGE, whereas on CelebA, Antigone + GEORGE’s WGA was equal to or better than GEORGE’s in each one of our multiple runs. Similar trend holds for Antigone + GEORGE (k = 2) and GEORGE (k = 2).\n\nGEORGE (k=2) Antigone + GEORGE (k=2)\n\nGEORGE Antigone + GEORGE\n\n46.7 ± 11.7 54.4 ± 7.1\n\n50.0 ± 5.8 57.4 ± 6.6\n\n93.6 ± 0.3 93.3 ± 0.3\n\n94.6 ± 0.1 94.2 ± 0.3\n\n95.0 ± 0.8 95.8 ± 0.6\n\n62.6 ± 2.1 65.3 ± 2.9\n\n60.4 ± 2.3 62.1 ± 1.2\n\n95.5 ± 0.7 96.0 ± 0.2\n\n29.7 ± 1.6\n\n34.5 ± 3.1\n\n95.9 ± 0.2\n\n95.7 ± 0.1\n\nWaterbirds\n\nAvg Acc\n\nWGA\n\nWGA\n\nERM\n\nImpact of EDM metric on fairness: We already noted in Table 1 Antigone with the proposed EDM metric produces higher quality sensitive attribute labels compared to a version of Antigone that picks hyper-parameters using standard ERM. We evaluated these two approaches using JTT’s training algorithm and find that Antigone with EDM results in a 5.7% increase in WGA and a small 0.06% increase in average accuracy.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n5 RELATED WORKS Several works have observed that standard ERM training algorithms can achieve state-of-the-art accuracy on many tasks, but unintentionally make biased predictions for different sensitive attributes failing to meet the fairness objectives (Hovy & Søgaard, 2015; Oren et al., 2019; Hashimoto et al., 2018a; Buolamwini & Gebru, 2018).\n\nFairness objectives can be broadly categorized into two types: individual fairness and group fairness. Individual fairness (Dwork et al., 2012; Kusner et al., 2017) requires similar individual to be treated similarly. Whereas, group fairness Prost et al. (2019); Quadrianto et al. (2019); Hardt et al. (2016) requires the groups of individuals divided based on a sensitive attribute like race, gender, etc., be treated equally. In this paper, we focus on the popular group fairness notions that include Demographic Parity, Equal Opportunity and Worst-group performance.\n\nMethods that seek to achieve group fairness are three types: pre-processing, in-processing and postprocessing algorithms. Pre-processing (Quadrianto et al., 2019; Ryu et al., 2018) methods focus on curating the dataset that includes removal of sensitive information or balancing the datasets. In-processing methods (Hashimoto et al., 2018b; Agarwal et al., 2018; Zafar et al., 2019; Lahoti et al., 2020; Prost et al., 2019; Liu et al., 2021; Sohoni et al., 2020) alter the training mechanism by using adding fairness constrains to the loss function or by training an adversarial framework to make predictions independent of sensitive attributes Zhang et al. (2018). Post-processing methods (Hardt et al., 2016; Wang et al., 2020; Savani et al., 2020) alter the outputs, for e.g. use different threshold for different sensitive attributes. In this work, we focus on in-processing algorithms.\n\nPrior in-processing algorithms, including the ones referenced above, assume access to sensitive attributes on the training data and validation dataset. Recent work sought to train fair model without training data annotations Liu et al. (2021); Nam et al. (2020); Hashimoto et al. (2018a); Creager et al. (2021) but, except for GEORGE Sohoni et al. (2020), require sensitive attributes on validation dataset to tune the hyperparameters. Like GEORGE, we seek to train fair classification models without ground-truth sensitive information on either training or validation dataset.\n\nAntigone is different from GEORGE in three ways: (1) Unlike GEORGE, we account for the model prediction and the ground-truth target label to generate pseudo-sensitive attributes. (2) The hyperparameters of the clustering step in GEORGE are fixed from literature and not specifically tuned for each dataset. In this paper, we propose a more principled approach to tune the model’s hyperparameters in an unsupervised fashion to obtain noisy sensitive features. And finally, (3) GEORGE only focuses on worst-group accuracy, whereas Antigone can be adapted to different notions of fairness.\n\nA related body of work develops post-processing methods to improve fairness without access to sensitive attributes but assuming a small set of labelled data for auditing Kim et al. (2019). One could use Antigone to create this auditing dataset, albeit with noise. Evaluating Antigone with these post-processing methods is an important avenue for future work.\n\n6 CONCLUSION In this paper, we propose Antigone, a method to enable hyper-parameter tuning for fair ML models without access to sensitive attributes on training or validation sets. Antigone generates high-quality pseudo-sensitive attribute labels on validation data by training a family of biased classifiers using standard ERM and using correctly (incorrectly) classified examples as proxies for majority (minority) group membership. We propose a novel EDM metric based approach to pick the most biased model from this family and provide theoretical justification for this choice using the MC noise model. The resulting validation dataset with pseudo sensitive attribute labels can then be used to tune the hyper-parameters of a fair training algorithm like JTT or GEORGE. We show that Antigone produces the highest precision sensitive attributes compared to the state-of-art. Future work will also seek to address the variance in fairness metrics (Mozannar et al., 2020) introduced by finite sample size under the MC noise model, extend Antigone to multiple-sensitive attributes, inter-sectional fairness and active learning of sensitive attributes.\n\nAVAILABILITY\n\nCode with README.txt file is available at: https://anonymous.4open.science/r/ fairness_without_demographics-3BD0/README.md\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlekh Agarwal, Alina Beygelzimer, Miroslav Dud ́ık, John Langford, and Hanna Wallach. A reduc-\n\ntions approach to fair classification, 2018.\n\nJoy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in comIn Sorelle A. Friedler and Christo Wilson (eds.), Proceedings mercial gender classification. of the 1st Conference on Fairness, Accountability and Transparency, volume 81 of Proceedings of Machine Learning Research, pp. 77–91. PMLR, 23–24 Feb 2018. URL https: //proceedings.mlr.press/v81/buolamwini18a.html.\n\nVeronika Cheplygina, Marleen de Bruijne, and Josien PW Pluim. Not-so-supervised: a survey of semi-supervised, multi-instance, and transfer learning in medical image analysis. Medical image analysis, 54:280–296, 2019.\n\nKashyap Chitta, Aditya Prakash, and Andreas Geiger. Neat: Neural attention fields for end-to-end In Proceedings of the IEEE/CVF International Conference on Computer\n\nautonomous driving. Vision, pp. 15793–15803, 2021.\n\nDanielle Keats Citron and Frank Pasquale. The scored society: Due process for automated predic-\n\ntions. Wash. L. Rev., 89:1, 2014.\n\nElliot Creager, J ̈orn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant\n\nlearning. In International Conference on Machine Learning, 2021.\n\nJeffrey Dastin. Amazon scraps secret ai recruiting tool that showed bias against women. In Ethics\n\nof Data and Analytics, pp. 296–299. Auerbach Publications, 2018.\n\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.\n\nics.uci.edu/ml.\n\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS ’12, pp. 214–226, New York, NY, USA, 2012. Association for Computing Machinery. ISBN 9781450311151. doi: 10.1145/2090236.2090255. URL https://doi.org/10. 1145/2090236.2090255.\n\nPatrick Grother, George Quinn, and P Phillips. Report on the evaluation of 2d still-image face\n\nrecognition algorithms, 2010-06-17 2010.\n\nMoritz Hardt, Eric Price, Eric Price, and Nati Srebro.\n\nEquality of opportunity in supervised learning. I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/ 9d2682367c3935defcb1f9e247a97c0d-Paper.pdf.\n\nIn D. Lee, M. Sugiyama, U. Luxburg,\n\nTatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. In International Conference on Machine Learning, pp. 1929–1938. PMLR, 2018a.\n\nTatsunori B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness with-\n\nout demographics in repeated loss minimization. In ICML, 2018b.\n\nKenneth Holstein, Jennifer Wortman Vaughan, Hal Daum ́e, Miro Dudik, and Hanna Wallach. Improving fairness in machine learning systems: What do industry practitioners need? In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI ’19, pp. 1–16, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450359702. doi: 10.1145/3290605.3300830. URL https://doi.org/10.1145/3290605.3300830.\n\nDirk Hovy and Anders Søgaard. Tagging performance correlates with author age. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 483–488, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-2079. URL https://aclanthology.org/P15-2079.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMichael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 247–254, 2019.\n\nAlina K ̈ochling and Marius Claus Wehner. Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of hr recruitment and hr development. Business Research, 13(3):795–848, 2020.\n\nMatt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness.\n\nIn I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ a486cd07e4ac3d270571622f4f316ec5-Paper.pdf.\n\nPreethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and\n\nEd H. Chi. Fairness without demographics through adversarially reweighted learning, 2020.\n\nAlexandre Lamy, Ziyuan Zhong, Aditya Krishna Menon, and Nakul Verma. Noise-Tolerant Fair\n\nClassification. Curran Associates Inc., Red Hook, NY, USA, 2019.\n\nGeert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I S ́anchez. A survey on deep learning in medical image analysis. Medical image analysis, 42: 60–88, 2017.\n\nEvan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 6781–6792. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/ liu21f.html.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\n\nIn Proceedings of International Conference on Computer Vision (ICCV), December 2015.\n\nEreni Markos, George R. Milne, and James W. Peltier. Information sensitivity and willingness to provide continua: A comparative privacy study of the united states and brazil. Journal of Public Policy & Marketing, 36(1):79–96, 2017. doi: 10.1509/jppm.15.159. URL https://doi. org/10.1509/jppm.15.159.\n\nHussein Mozannar, Mesrob I. Ohannessian, and Nathan Srebro. Fair learning with private deIn Proceedings of the 37th International Conference on Machine Learning,\n\nmographic data. ICML’20. JMLR.org, 2020.\n\nJunhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: Training debiased classifier from biased classifier. In Advances in Neural Information Processing Systems, 2020.\n\nMei Ngan and Patrick Grother. Face recognition vendor test (frvt) - performance of automated\n\ngender classification algorithms, 2015-04-20 2015.\n\nYonatan Oren, Shiori Sagawa, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust language modeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4227–4237, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1432. URL https://aclanthology. org/D19-1432.\n\nFrank Pasquale. The black box society: The secret algorithms that control money and information.\n\nHarvard University Press, 2015.\n\nFlavien Prost, Hai Qian, Qiuwen Chen, Ed H. Chi, Jilin Chen, and Alex Beutel. Toward a better\n\ntrade-off between performance and fairness with kernel-based distribution matching, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nNovi Quadrianto, Viktoriia Sharmanska, and Oliver Thomas. Discovering fair representations in the data domain. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nHee Jung Ryu, Hartwig Adam, and Margaret Mitchell. Inclusivefacenet: Improving face attribute\n\ndetection with race and gender diversity, 2018.\n\nShiori Sagawa*, Pang Wei Koh*, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=ryxGuJrFvS.\n\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations. In ICML, pp. 8346–8356, 2020. URL http://proceedings.mlr.press/v119/sagawa20a.html.\n\nYash Savani, Colin White, and Naveen Sundar Govindarajulu. Intra-processing methods for debias-\n\ning neural networks, 2020.\n\nClayton Scott, Gilles Blanchard, and Gregory Handy. Classification with asymmetric label noise: In Shai Shalev-Shwartz and Ingo Steinwart (eds.), ProConsistency and maximal denoising. ceedings of the 26th Annual Conference on Learning Theory, volume 30 of Proceedings of Machine Learning Research, pp. 489–511, Princeton, NJ, USA, 12–14 Jun 2013. PMLR. URL https://proceedings.mlr.press/v30/Scott13.html.\n\nNimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R ́e. No subclass left behind: Fine-grained robustness in coarse-grained classification problems. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19339–19352. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ e0688d13958a19e087e123148555e4b4-Paper.pdf.\n\nMichael Veale and Reuben Binns. Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data & Society, 4(2):2053951717743530, 2017. doi: 10.1177/2053951717743530. URL https://doi.org/10.1177/2053951717743530.\n\nZeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation, 2020.\n\nZhao Wang and Aron Culotta. Robustness to spurious correlations in text classification via automatically generated counterfactuals. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14024–14031, 2021.\n\nMuhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness constraints: A flexible approach for fair classification. Journal of Machine Learning Research, 20(75):1–42, 2019. URL http://jmlr.org/papers/v20/18-262.html.\n\nRich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.\n\nIn International conference on machine learning, pp. 325–333. PMLR, 2013.\n\nBrian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-\n\nsarial learning, 2018.\n\nZhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. End-to-end urban driving by imitating a reinforcement learning coach. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15222–15232, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 UCI ADULT DATASET\n\nDataset details: Adult dataset (Dua & Graff, 2017) is used to predict if an individual’s annual income is ≤ 50K (Y = 0) or > 50K (Y = 1) based on several continuous and categorical attributes like the individual’s education level, age, gender, occupation, etc. The sensitive attribute is gender A = {Men, Women} Zemel et al. (2013). The dataset consists of 45,000 instances and is split into training, validation and test sets with 21112, 9049 and 15060 instances, respectively.\n\nHyper-parameter settings: In all our experiments using Adult dataset, we train a multi-layer neural network with one hidden layer consisting of 64 neurons. We train for a total of 100 epochs using the SGD optimizer and a batch size of 256. We tune Antigone and JTT by performing grid search over learning rates ∈ {1e − 03, 1e − 04, 1e − 05} and weight decays ∈ {1e−01, 1e−03}. For JTT, we explore over T ∈ {1, 2, 5, 10, 15, 20, 30, 35, 40, 45, 50, 65, 80, 95} and λ ∈ {5, 10, 20}. In each case, we seek to optimize fairness while constraining average accuracy to ranges {[82, 82.5), [81.5, 82), [81, 81.5), [80.5, 81), [80, 80.5)}.\n\nA.2 QUALITY OF ANTIGONE’S SENSITIVE ATTRIBUTE LABELS\n\nFigure 3: Euclidean Distance between Means (EDM) and noise parameters α1, β1 and and 1 − α1 − β1 for the positive target class of CelebA dataset. The noise parameters are unknown in practice. Blue dot indicates the model that we pick to generate pseudo sensitive attributes, while black star indicates the model that maximizes 1 − α1 − β1.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: We tabulate the precision, recall, F1-score of the noisy validation groups generated from ERM model, GEORGE, GEORGE with number of clusters = 2 and Antigone. We observe that Antigone has higher precision and F1 scores across different noisy groups on CelebA and Waterbirds, respectively.\n\nAntigone (w/o EDM)\n\nGEORGE\n\nGEORGE (k = 2)\n\nAntigone (w/ EDM)\n\nCelebA (Precision, Recall, F1 Scores)\n\nBlond Men\n\nBlond Women\n\nNon-blond Women\n\nNon-blond Men\n\nCelebA Accuracy\n\n0.26, 0.31, 0.28 (0.02, 0.03, 0.01) 0.95, 0.94, 0.95 (0.01, 0.01, 0.01) 0.82, 0.13, 0.22 (0.01, 0.01, 0.02) 0.52, 0.97, 0.67 (0.00, 0.00, 0.01) 0.59 ± 0.01\n\n0.09, 0.32, 0.13 (0.01, 0.09, 0.02) 0.94, 0.28, 0.43 (0.01, 0.04, 0.04) 0.51, 0.36, 0.42 (0.00, 0.01, 0.01) 0.53, 0.33, 0.40 (0.01, 0.02, 0.02) 0.33 ± 0.01\n\n0.06, 0.70, 0.12 (0.01, 0.05, 0.01) 0.95, 0.35, 0.51 (0.00, 0.02, 0.02) 0.5, 0.76, 0.6 (0.00, 0.01, 0.01) 0.47, 0.23, 0.31 (0.01, 0.01, 0.01) 0.48 ± 0.00\n\n0.36, 0.34, 0.35 (0.05, 0.04, 0.04) 0.96, 0.96, 0.96 (0.0, 0.0, 0.0) 0.86, 0.13, 0.22 (0.01, 0.01, 0.01) 0.52, 0.98, 0.68 (0.0, 0.0, 0.0) 0.60 ± 0.00\n\nWaterbirds (Precision, Recall, F1 Scores)\n\nWaterbirds Landbkgd\n\nWaterbirds Waterbkgd\n\nLandbirds Waterbkgd\n\nLandbirds Landbkgd\n\nWaterbirds Accuracy\n\n0.94, 0.26, 0.41 (0.01, 0.02, 0.02) 0.57, 0.98, 0.72 (0.01, 0.00, 0.00) 0.96, 0.42, 0.58 (0.00, 0.03, 0.02) 0.63, 0.98, 0.76 (0.01, 0.00, 0.01) 0.68 ± 0.01\n\n0.56, 0.34, 0.43 (0.03, 0.02, 0.02) 0.55, 0.27, 0.36 (0.07, 0.03, 0.02) 0.57, 0.36, 0.44 (0.04, 0.03, 0.03) 0.55, 0.24, 0.34 (0.04, 0.04, 0.02) 0.30 ± 0.02\n\n0.48, 0.57, 0.52 (0.01, 0.01, 0.01) 0.48, 0.39, 0.43 (0.02, 0.02, 0.02) 0.55, 0.55, 0.55 (0.03, 0.03, 0.03) 0.55, 0.56, 0.55 (0.03, 0.04, 0.03) 0.53 ± 0.03\n\n0.96, 0.63, 0.76 (0.01, 0.04, 0.03) 0.73, 0.97, 0.83 (0.02, 0.01, 0.01) 0.97, 0.65, 0.78 (0.00, 0.05, 0.04) 0.74, 0.98, 0.84 (0.03, 0.00, 0.02) 0.81 ± 0.02\n\nTable 5: We tabulate the precision, recall, F1-score, accuracy of the noisy validation groups generated by varying the fraction of minority group examples in each class of CelebA dataset. We observe that Antigone has higher precision, recall, F1 score and accuracy if the imbalance is more in the training dataset.\n\nFraction Minority\n\n5%\n\n20%\n\n35%\n\n50%\n\nPrecision, Recall, F1 Score\n\nBlond Men (Minority) Blond Women (Majority) 1 − α1 − β1 Blond Acc.\n\nNon-blond Women (Minority) Non-blond Men (Majority) 1 − α0 − β0 Non-blond Acc.\n\n0.57, 0.40, 0.47 0.97, 0.98, 0.98 0.54 0.95\n\n0.45, 0.26, 0.33 0.96, 0.98, 0.97 0.41 0.94\n\n0.81, 0.17, 0.29 0.83, 0.99, 0.90 0.64 0.83\n\n0.59, 0.17, 0.26 0.82, 0.97, 0.89 0.41 0.81\n\n0.67, 0.15, 0.24 0.68, 0.96, 0.79 0.35 0.68\n\n0.63, 0.18, 0.28 0.68, 0.94, 0.79 0.31 0.67\n\n0.75, 0.14, 0.24 0.53, 0.95, 0.68 0.28 0.55\n\n0.63, 0.16, 0.26 0.52, 0.91, 0.66 0.15 0.54\n\nOverall Acc.\n\n0.95\n\n0.81\n\n0.67\n\n0.54\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA.3 ANTIGONE + JTT\n\nTable 6: We report the (Test Average Accuracy, Test Fairness Metric) for different validation accuracy thresholds on Waterbirds dataset. We observe that Antigone + JTT (our noisy sensitive attributes) improves fairness over baseline ERM model and closes the gap with Ground-Truth + JTT (ground-truth sensitive attributes).\n\nVal. Thresh.\n\nMethod\n\nDP Gap\n\nEO Gap\n\nWorst-group\n\n[96, 96.5)\n\n[95.5, 96)\n\n[95, 95.5)\n\n[94.5, 95)\n\nAntigone + JTT Ground-Truth + JTT\n\n(95.8, 3.9) ± (0.4, 0.4) (95.8, 3.9) ± (0.4, 0.4)\n\n(96.2, 10.7) ± (0.3, 7.1) (96.0, 7.1) ± (0.3, 1.4)\n\n(96.3, 83.0) ± (0.4, 1.3) (96.3, 83.0) ± (0.4, 1.3)\n\nAntigone + JTT Ground-Truth + JTT\n\n(95.4, 2.8) ± (0.1, 0.1) (95.4, 2.9) ± (0.4, 1.1)\n\n(96.0, 7.5) ± (0.2, 1.5) (95.6, 6.0) ± (0.3, 2.1)\n\n(96.3, 83.2) ± (0.3, 0.6) (96.1, 83.5) ± (0.5, 0.8)\n\nAntigone + JTT Ground-Truth + JTT\n\n(94.5, 1.5) ± (0.6, 0.6) (94.4, 1.7) ± (0.7, 0.7)\n\n(94.7, 4.2) ± (0.9, 3.1) (94.3, 1.1) ± (0.5, 0.6)\n\n(94.7, 85.9) ± (0.9, 1.4) (95.1, 86.8) ± (0.6, 1.1)\n\nAntigone + JTT Ground-Truth + JTT\n\n(94.2, 0.4) ± (0.4, 0.4) (93.6, 0.6) ± (0.5, 0.5)\n\n(93.8, 2.0) ± (0.5, 1.4) (93.8, 2.0) ± (0.5, 1.4)\n\n(94.2, 86.7) ± (0.8, 1.8) (94.1, 88.2) ± (0.6, 0.7)\n\n[94.0, 94.5)\n\nAntigone + JTT Ground-Truth + JTT\n\n(93.0, 1.5) ± (0.6, 0.3) (93.1, 1.5) ± (0.3, 0.4)\n\n(93.6, 4.8) ± (1.2, 3.0) (93.2, 4.0) ± (1.0, 2.1)\n\n(93.7, 87.9) ± (0.5, 1.4) (93.8, 88.1) ± (0.7, 1.1)\n\nERM\n\n(97.3, 21.3) ± (0.2, 1.1)\n\n(97.3, 35.0) ± (0.2, 3.4)\n\n(97.3, 59.1) ± (0.2, 3.8)\n\nTable 7: Antigone + JTT vs Ideal MC Model + JTT (Avg. Accuracy, Fairness) comparison on test data for different validation accuracy thresholds on the CelebA dataset. Lower DP and EO gaps are better. Higher WGA is better.\n\nDP Gap\n\nEO Gap\n\nWGA\n\n>=94 and <95\n\nAntigone + JTT (94.9, 14.7) Ideal MC + JTT (94.9, 14.7)\n\n(94.6, 33.7) (94.4, 34.1)\n\n(94.5, 61.7) (94.4, 58.3)\n\n>=93 and <94\n\nAntigone + JTT (93.7, 12.2) Ideal MC + JTT (93.7, 12.2)\n\n(93.9, 30.3) (93.5, 26.3)\n\n(93.3, 60.0) (93.7, 65.0)\n\n>=92 and <93\n\nAntigone + JTT (93.1, 12.1) Ideal MC + JTT (93.1, 12.1)\n\n(92.4, 22.9) (93.0, 22.7)\n\n(92.9, 65.6) (93.2, 69.4)\n\n>=91 and <92\n\n>=90 and <91\n\nAntigone + JTT Ideal MC + JTT\n\nAntigone + JTT Ideal MC + JTT\n\n(91.9, 9.3) (91.9, 9.3)\n\n(91.1, 7.9) (90.9, 8)\n\n(91.1, 13.9) (92.2, 19.1)\n\n(91.1, 66.7) (91.8, 73.9)\n\n(91.1, 13.9) (90.4, 18.9)\n\n(91.1, 66.7) (91.4, 72.2)\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: (Avg. Accuracy, Fairness) on test data for different validation accuracy thresholds on the UCI Adult dataset. Lower DP and EO gaps are better. Higher WGA is better.\n\nVal. Thresh.\n\nMethod\n\nDP Gap\n\nEO Gap\n\nWorst-group Acc.\n\n[82, 82.5)\n\n[81.5, 82)\n\n[81, 81.5)\n\n[80.5, 81)\n\n[80, 80.5)\n\nAntigone + JTT\n\n(81.9, 11.7) Ground-Truth + JTT (81.8, 11.9)\n\nAntigone + JTT Ground-Truth + JTT\n\nAntigone + JTT Ground-Truth + JTT\n\nAntigone + JTT Ground-Truth + JTT\n\nAntigone + JTT Ground-Truth + JTT\n\n(81.5, 9.3) (81.5, 9.3)\n\n(80.8, 7.1) (80.8, 7.1)\n\n(80.4, 7.2) (79.8, 5.1)\n\n(79.7, 5.4) (79.5, 3.9)\n\n(81.9, 0) (81.4, 3.3)\n\n(81.5, 1.9) (81.1, 4.0)\n\n(80.9, 0.9) (81.0, 2.4)\n\n(80.1, 1.9) (80.5, 5.0)\n\n(79.7, 1.4) (80.4, 4.2)\n\n(81.7, 54.6) (81.7, 54.6)\n\n(81.0, 56.0) (81.0, 56.0)\n\n(81.0, 57.1) (81.0, 57.1)\n\n(80.4, 58.2) (80.5, 58.0)\n\n(80.4, 58.0) (80.4, 58.0)\n\nERM\n\n(84.8, 53.8)\n\n(84.8, 10.6)\n\n(84.8, 52.4)\n\n16",
    "reference": "# Summary Of The Paper\n\nThe paper focuses on the problem of accessing sensitive group attributes in the validation set for hyperparameter tuning. To solve this issue, the paper proposes Antigone, a hyper-parameter tuning framework for fairness techniques that do not access sensitive group attributes in training data. The proposed framework is inspired by a prior work (Lamy et al., 2019), which has been proposed for fair training under noisy sensitive attributes. This algorithm is evaluated on two datasets, including CelebA and Waterbirds, with some baselines.\n\n# Strength And Weaknesses\n\n[Strength]\n\nS1: The paper tackles the practical problem of accessing sensitive attributes in the validation set. \n\nS2: The paper utilizes an idea from fair training under noisy sensitive attributes, and such inspiration gives an interesting approach. \n\n[Weakness]\n\nW1: The significance of the proposed algorithm seems a bit limited, as it only targets fairness techniques that do not access sensitive attributes in training data. Since many more algorithms have been proposed for fair training with sensitive attributes, it would be helpful if the paper can clarify 1) the significance of this work and 2) how the proposed algorithm can be extended in other scenarios.\n\nW2: Several design choices in the proposed framework Antigone are not fully justified. For example, why using the previous idea of handling noisy group attributes is the most appropriate way to solve the target problem? Why the current approach is better than other approaches like training a weak-labeler on groups?\n\nW3: More importantly, the experimental results are insufficient. \n- The baselines are not enough. One of the key advantages the paper argues is that Antigone can be used for any algorithm of not accessing sensitive attributes on training data. However, the paper only shows its effectiveness on JTT. Thus, it is hard to believe that Antigone will effective in other algorithms like LfF and DRO.\n- Also, all experimental results are reported without error range, which makes the observations less reliable. For example, in several rows of Table 1, the improvements in Antigone compared to GEORGE are a bit marginal, so it is questionable how it changes after multiple runs. It would be much better if the paper shows the results with error ranges to clarify the performance gain.\n\nMinor Typo: First paragraph in Section 2.1) target labels (X) => target labels (Y)\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written, but several clarifications are needed as discussed in the previous section.\n\n# Summary Of The Review\n\nAlthough the paper tries to solve a practical problem, there seems some room for improvement in 1) clarification on the importance of the work, 2) justification of the design choices, and 3) enhancement of the experimental results. Overall, I think that this paper is on the borderline.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCONTEXTUAL REINFORCEMENT LEARNING\n\nSYMBOLIC\n\nPOLICY\n\nFOR META-\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nContext-based Meta-Reinforcement Learning (Meta-RL), which conditions the RL agent on the context variables, is a powerful method for learning a generalizable agent. Current context-based Meta-RL methods often construct their contextual policy with a neural network (NN) and directly take the context variables as a part of the input. However, the NN-based policy contains tremendous parameters which possibly result in overfitting, the difficulty of deployment and poor interpretability. To improve the generation ability, efficiency and interpretability, we propose a novel Contextual Symbolic Policy (CSP) framework, which generates contextual policy with a symbolic form based on the context variables for unseen tasks in meta-RL. Our key insight is that the symbolic expression is capable of capturing complex relationships by composing various operators and has a compact form that helps strip out irrelevant information. Thus, the CSP learns to produce symbolic policy for meta-RL tasks and extract the essential common knowledge to achieve higher generalization ability. Besides, the symbolic policies with a compact form are efficient to be deployed and easier to understand. In the implementation, we construct CSP as a gradient-based framework to learn the symbolic policy from scratch in an end-to-end and differentiable way. The symbolic policy is represented by a symbolic network composed of various symbolic operators. We also employ a path selector to decide the proper symbolic form of the policy and a parameter generator to produce the coefficients of the symbolic policy. Empirically, we evaluate the proposed CSP method on several Meta-RL tasks and demonstrate that the contextual symbolic policy achieves higher performance and efficiency and shows the potential to be interpretable.\n\n1\n\nINTRODUCTION\n\nMeta-Reinforcement Learning (Meta-RL) is a promising strategy to improve the generalization ability on unseen tasks of reinforcement learning. Meta-RL methods learn the shared internal structure of tasks from the experiences collected across a distribution of training tasks and then quickly adapt to a new task with a small amount of experiences. On this basis, context-based Meta-RL methods (Duan et al., 2016; Rakelly et al., 2019; Fakoor et al., 2019; Huang et al., 2021) are proposed with the motivation that only a part of the model parameters need to be updated in a new environment. They force their model to be conditional on a set of task-specific parameters named context variables which are formed by aggregating experiences. Context-based Meta-RL methods are attractive because of their empirically higher performance and higher efficiency compared with previous methods which update the whole model.\n\nHowever, how to incorporate the context variables into the policy is still an open problem. Most of the current methods construct their contextual policy with a neural network (NN) and directly take the context variables as a part of the input. This kind of NN-based policy usually involves thousands of parameters, which may bring training difficulties, possibly result in overfitting and hurt the generalization performance. In addition, deploying the complex NN-based policy is inefficient and even impossible on limited computational resources. What is worse, we have to treat the NNbased policy as a black box that is hard to comprehend and interpret, e.g., we cannot understand what the difference between the policies of different tasks is.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTo address the above issues, in this work, we propose a novel Contextual Symbolic Policy (CSP) framework to learn a contextual policy with a compact symbolic form for unseen tasks in meta-RL. We are inspired by the symbolic expression, which has a compact form but is capable of capturing complex relationships by composing variables, constants and various mathematical operators. In general, compact and effective representations can strip out irrelevant information and find the essential relationship of variables, which can benefit the generalization. Therefore, for meta-RL tasks with similar internal structures, CSP produces symbolic policies to model the relationship of the proper action and state and extract essential common knowledge across the tasks. With the common knowledge of a series of tasks, CSP is able to achieve higher generalization ability and quickly adapt to unseen tasks. Moreover, the compact symbolic policies learned by CSP are efficient to be deployed and easier to understand. In conclusion, contextual policies produced by CSP achieve higher generalization performance, efficiency, and show the potential to be interpretable when constrained in a compact symbolic form.\n\nHowever, finding the proper forms and constant values of the symbolic policies for a distribution of tasks is challenging. In this paper, we propose an efficient gradient-based learning method for the CSP framework to learn the contextual symbolic policy from scratch in an end-to-end differentiable way. To express the policy in a symbolic form, the proposed CSP consists of a symbolic network, a path selector and a parameter generator. The symbolic network can be considered as a full set of the candidate symbolic policies. In the symbolic network, the activation functions are composed of various symbolic operators and the parameters can be regarded as the coefficients in the symbolic expression. For a new task, the path selector chooses the proper compact symbolic form from the symbolic network by adaptively masking out most irrelevant connections. Meanwhile, the parameters of the chosen symbolic form are generated by the parameter generator. We design all these modules to be differentiable. Thus, we can update the whole framework with gradient efficiently.\n\nWe evaluate the proposed CSP on several Meta-RL tasks. The results show that our CSP achieves higher generalization performance than previous methods while reducing the floating point operations (FLOPs) by 2-45000×. Besides, the produced symbolic policies show the potential to be interpretable.\n\n2 RELATED WORKS\n\n2.1 META-REINFORCEMENT LEARNING\n\nMeta-RL extends the notion of meta-learning (Schmidhuber, 1987; Bengio et al., 1991; Thrun & Pratt, 1998) to the context of reinforcement learning. Some works (Li et al., 2017; Young et al., 2018; Kirsch et al., 2019; Zheng et al., 2018; Sung et al., 2017; Houthooft et al., 2018) aim to metalearn the update rule for reinforcement learning. We here consider another research line of works that meta-train a policy that can be adapted efficiently to a new task. Several works (Finn et al., 2017; Rothfuss et al., 2018; Stadie et al., 2018; Gupta et al., 2018; Liu et al., 2019) learn an initialization and adapt the parameters with policy gradient methods. However, these methods are inefficient because of the on-policy learning process and the gradient-based updating during adaptation. Recently, context-based Meta-RL achieve higher efficiency and performance. PEARL (Rakelly et al., 2019) proposes an off-policy Meta-RL method that infers probabilistic context variables with experiences from new environments. ADARL (Huang et al., 2021) characterizes a compact representation about changes of environments with a structural environment model, which enables efficient adaptation. Hyper (Sarafian et al., 2021) proposes a hypernetwork where the primary network determines the weights of a conditional network and achieves higher performance. Fu et al. (2020) introduce a contrastive learning method to train a compact context encoder. They also train an exploration policy to maximize the information gain. Most of the existing context-based Meta-RL methods(Fu et al., 2020; Zhang et al., 2021; Zintgraf et al., 2020) attempt to achieve higher performance by improving the context encoder or the exploration strategy. However, in this paper, we aim to improve the efficiency, interpretability and performance by replacing the pure neural network policy with a contextual symbolic policy.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2.2 SYMBOLIC REGRESSION\n\nSymbolic regression aims to find symbolic expressions to best fit the dataset from an unknown fixed function. Although this problem is likely NP-hard in principle, several works attempt to solve it with heuristic algorithms. Koza (1993) introduce genetic programming (GP) to evolve the symbolic expressions and a series of following works (Schmidt & Lipson, 2010; Cava et al., 2019; Virgolin et al., 2019; de Franc ̧a & Aldeia, 2021) expand the basic genetic programming method to improve the performance. Recently, some methods involve deep learning for symbolic regression. AI Feynman (Udrescu & Tegmark, 2019) utilizes neural networks to discover hidden simplicity in the dataset and break harder problems into simpler ones. DSR (Petersen et al., 2021) train a recurrent neural network (RNN) with reinforcement learning to produce the symbolic expression. Differentiable symbolic regression methods (Martius & Lampert, 2016; Sahoo et al., 2018) use a neural network whose activation functions are symbolic operators as a symbolic expression and decrease the length of symbolic expressions with L1 regularization. However, the plain structure and L1 regularization may fail with complex problems. We also employ a symbolic network for the differentiability. The difference is that we propose a densely connected symbolic network and probabilistic path selector, which enable symbolic meta-policy learning. Besides, these methods are designed for regression of a fixed function while we aim to learn a meta-policy which is conditional on the context variables.\n\nRecently, some works employ symbolic regression methods to obtain symbolic policies for efficiency and interpretability. Kubal ́ık et al. (2017) and Hein et al. (2018) aim to approximate a symbolic policy with genetic programming but require a given dynamics equations or a learned world model. DSP (Larma et al., 2021) employ a recurrent neural network to generate the symbolic policy. They use the average returns of the symbolic policies as the reward signal and train the neural network with risk-seeking policy gradients. However, for environments with multidimensional action spaces, they need a pre-trained neural network policy as the anchor model. Besides, in this framework, a single reward for reinforcement learning involves many environmental interactions, which is inefficient and makes it hard to combine the symbolic policy with Meta-RL. Recently, some works (Bastani et al., 2018; Verma et al., 2018) attempt to distill an interpretable policy from a pre-trained neural network policy but have a problem of objective mismatch (Larma et al., 2021). Different from the methods talked above, we propose an efficient gradient-based framework to obtain the symbolic policy without any pre-trained model. As far as we know, we are the first to learn a symbolic policy from scratch and use the symbolic policy for Meta-RL. There exist other works introduce the symbolic to reinforcement learning. Garnelo et al. (2016); d’Avila Garcez et al. (2018) learn the symbolic representation for better interpretability. Lyu et al. (2019) introduces symbolic planning for efficiency and interpretability. In this paper, we focus on learning the symbolic policy for Meta-RL.\n\n3 PRELIMINARIES\n\nIn the field of meta-reinforcement learning (Meta-RL), we consider a distribution of tasks p(κ) with each task κ ∼ p(κ) modeled as a Markov Decision Process(MDP). In common Meta-RL settings, tasks share similar structures but differ in the transition and/or reward function. Thus, we In this setting, S ⊆ Rn is a set of can describe a task κ with the 6-tuple (S, A, Pκ, ρ0, rκ, γ). n-dimensional states, A ⊆ Rm is a set of m-dimensional actions, Pκ : S × A × S → [0, 1] is the state transition probability distribution, ρ0 : S → [0, 1] is the distribution over initial states, rκ : S × A → R is the reward function, and γ ∈ (0, 1) is the per timestep discount factor. Following the setting of prior works (Rakelly et al., 2019; Fakoor et al., 2019), we assume there are M metatraining tasks {κm}m=1,··· ,M sampled from the training tasks distribution ptrain(κ). For metatesting, the tasks are sampled from the test tasks distribution ptest(κ). The two distributions are usually same in most settings but can be different in out-of-distribution(OOD) settings. We denote context cT = {(s1, a1, s′ T , rT )} as the collected experiences. For contextbased Meta-RL, agent encodes the context into a latent context variable z with a context encoder q(z|cT ) and the policy π is conditioned on the current state and the context variable z. During adaptation, agent first collect experiences for a few episodes and then update the context variables and maximize the return with the contextual policy. The Meta-RL objective can be formulated Eκ∼p(κ)[EcT ∼π[R(κ, π, q(z|cT ))]], where R(κ, π, q(z|cT )) denotes the expected episode as max π\nreturn.\n\n1, r1), · · · , (sT , at, s′\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustrations of our framework. The Contextual Symbolic Policy (CSP) produces different symbolic policies based on the context variables across tasks. CSP construct the symbolic policy with the symbolic network, selects the proper symbolic form with the Path Selector and generate the parameters of symbolic policy with the Parameter generator. The whole framework is differentiable and can be learned end-to-end.\n\n4 GRADIENT-BASED CONTEXTUAL SYMBOLIC POLICY\n\nThis section introduces the structure of our contextual symbolic policy, an end-to-end differentiable system that can be directly updated with gradient. As Figure 1 shows, the contextual symbolic policy consists of three main components: 1) the Symbolic Network, which expresses the policy in a symbolic form, 2) the Parameter Generator, which outputs the parameters for the symbolic network according to the context variables, and 3) the Path Selector, which select paths from the symbolic network to form compact symbolic expressions.\n\n4.1 DENSELY CONNECTED SYMBOLIC NETWORK\n\nTo construct a symbolic policy in an end-to-end differentiable form, we propose the densely connected symbolic network. Inspired by previous differentiable symbolic regression methods (Martius & Lampert, 2016; Sahoo et al., 2018), we employ a neural network with specifically designed units, which is named symbolic network. We now introduce the basic symbolic network named plain structure which is illustrated in Figure 2. The symbolic network is a feed-forward network with L layers. Different from traditional neural networks, the activation functions of the symbolic network is replaced by symbolic operators, e.g. trigonometric functions and exponential functions. For the lth layer of the symbolic network, we denote the input as xl−1 and the parameters as wl, bl. These parameters serve as the constant in a symbolic expression. We assume that the lth contains m unary m} and n binary functions {g2 functions {g1 n}. Firstly, the input of the lth layer will be linearly transformed by a fully-connected layer:\n\n1, · · · , g1\n\n1, · · · , g2\n\ny = Fl(x) = wlx + bl.\n\n(1)\n\nThe fully-connected layer realizes the addition and subtraction in symbolic expressions and produces m + 2n outputs. Then the outputs will go through the symbolic operators and be concatenated to form the layer output:\n\nGl(y) = [g1\n\n1(y1), · · · , g1\n\nm(ym), g2\n\n1(ym+1, ym+2), · · · , g2\n\nn(ym+2n−1, ym+2n)]\n\n(2)\n\nThen the lth layer of the symbolic network can be formulated as Sl : xl = Gl(Fl(xl−1)). Following the last layer, there will be a fully-connected layer to produce a single output. For multiple action dimensions, we construct a symbolic network for each dimension of action.\n\nsymbolic operators\n\nSymbolic operator. The are e.g. {sin, cos, exp, log, ×, ÷} for continuous control tasks. For the plain structure, we include an identical operator which retains the output of the previous layer to the next layer in the library. We also provide an optional conditional operator c(a, b, c) = sigmoid(a) ∗ b + (1 − sigmoid(a)) ∗ c to approximate if A then B else C. We find it is useful in some tasks. Note that we aim to find the symbolic policy with gradient. Thus, it is critical to ensure the numerical stability of the\n\nselected from a\n\nlibrary L,\n\n4\n\ncontext encoderMaskSymbolic NetworkzFYParameter GeneratorPath SelectorScoreParameterGumbel SigmoidQ(s,a,z) replaybuffercT= (si,ai,si′,r) i=1i=NStatetraininginferenceCritic LossContext Symbolic Policyforward flowgradient flowforward flowgradient flowq(z|c) q(z|c) sinsinidid××s1s1s2s2s3s3a1a1sinid×s1s2s3a1ActionActor LossActionActor Lossa1=sin s1+s3 ∗s2MaskSymbolic NetworkzFYParameter GeneratorPath SelectorScoreParameterGumbel SigmoidQ(s,a,z) replaybuffercT= (si,ai,si′,r) i=1i=NStatetraininginferenceCritic LossContext Symbolic Policyforward flowgradient flowq(z|c) sinid×s1s2s3a1ActionActor Lossa1=sin s1+s3 ∗s2Under review as a conference paper at ICLR 2023\n\nFigure 2: Example network structures for the symbolic network. Left: the plain structure. Middle: a symbolic work with dense connections. Right: a symbolic network with dense connections and arranged operators.\n\nsystem. However, this is not natural in a symbolic network. For example, the division operator and the logarithmic operator will create a pole when the input goes to zero and the exponential function may produce a large output. Thus, we regularize the operators and employ a penalty term to keep the input from ”forbidden” area. For example, the logarithmic operator y = log(x) returns log(x) for x > boundlog and log(boundlog) otherwise and the penalty term is defined as Llog = max(boundlog − x, 0). The division operator c = a/b returns a/b for b > bounddiv and 0 otherwise. The penalty term is defined as Ldiv = max(bounddiv − b, 0). The details of all regularized operators can be found in the Appendix.\n\nDense connectivity. We introduce dense connections (Huang et al., 2017) in the symbolic network, where inputs of each layer are connected to all subsequent layers. Consequently, the lth layer of the symbolic network will receive the environment state s and the output of all preceding layers x1, · · · , xl−1: xl = Gl(Fl([s, x1, · · · , xl−1])). On the one hand, the dense connections improve the information flow between layers and benefit the training procedure. On the other hand, the dense skip connections across layers enable us to control the complexity of the symbolic expressions with the parameters more flexibly. In practice, each layer in the symbolic network may contain different operators. With the dense connectivity, we can flexibly arrange the position of operators. For example, if we only arrange the sin operator in the last layer but the oracle expression contains terms like sin(s0), the input of the sin operator can still be from the original state because of the dense connections. We give an example of arranged operators in Figure 2 which we use for all tasks in the experiments. In this symbolic network, we assume that multiplication and division operations are more likely to occur at shallow layers, while more complex operations such as sines and cosines are more likely to occur at deep layers. In addition, we avoid the form sin(· · · exp(· · · ) · · · ) which rarely occurs in physics formulas with the arrangement.\n\n4.2\n\nINCORPORATING THE CONTEXT VARIABLES\n\nTo produce different symbolic policies with the symbolic network for different tasks κ sampled from the task distribution p(κ), we need to incorporate the context variables z ∼ q(z|cT ) to the symbolic network. To condition the parameters of the symbolic expression on the context variable, we propose a parameter generator: wg = Φ(z) which is a neural network to produce the parameters of symbolic networks for all action dimensions based on the context variables.\n\nHowever, the symbolic network serves as a full set of the search space of symbolic expressions. To select the proper paths from the symbolic network to produce a compact symbolic policy, we reduce the number of paths involved in the final symbolic policy then proper paths remain and redundant paths are removed. This can be naturally realized by minimizing the L0 norm of the symbolic network parameters. As the L0 norm is not differentiable, some methods (Martius & Lampert, 2016; Sahoo et al., 2018) employ L1 norm instead of L0 norm. However, L1 will penalize the magnitude of the parameters. In our framework, the parameters of the symbolic network is the output of a neural network rather than independent variables. The penalty of magnitude may severely affect the training of the parameter generator. Inspired by the probability-based sparsification method (Srinivas et al., 2017; Louizos et al., 2017; Zhou et al., 2021), we propose a probabilistic path selector which selects path from the network by multiplying a binary mask on the parameters of the symbolic network. The path selector first produce scores with the context variables: s = Ψ(z),\n\n5\n\nsincoslogidexpexpFully Connected LayermuldivsincoslogidexpexpFully Connected LayermuldivsincoslogidexpFully Connected LayermuldivsincoslogidexpexpFully Connected LayermuldivsincoslogidexpFully Connected LayermuldivFully Connected LayerFully Connected LayermulmulmulFully Connected LayerdivdivdivcosFully Connected LayersinsinsinsincosexpexpexpexploglogFully Connected LayerdivmulexpexplogFully Connected Layer. . ..........PlainDensely connectedArranged Densely connectedsincoslogexpexpFully Connected LayermuldivsincoslogexpexpFully Connected LayermuldivsincoslogexpFully Connected LayermuldivsincoslogexpexpFully Connected LayermuldivsincoslogexpFully Connected LayermuldivFully Connected Layer. . .sincoslogexpFully Connected LayermuldivsincoslogexpFully Connected LayermuldivsincoslogexpFully Connected LayermuldivFully Connected Layer. . .Under review as a conference paper at ICLR 2023\n\nwhere si ∈ (0, 1). The score si serves as the probability of the Bernoulli distribution and the binary mask mi is sampled from the distribution: mi ∼ Bern(si). Then the final parameters (cid:78) m, where (cid:78) is the element-wise multiply operation. of the symbolic network are w = wg Consequently, to get a compact symbolic expression, we only need to minimize the expectation of the L0 norm of the binary mask Em∼p(m|s) ∥m∥0 = (cid:80) si, without penalizing the magnitude of the parameters.\n\nDuring the process of collecting data or testing, we can directly sample the binary mask from the Bernoulli distribution. However, the sampling process does not have a well-defined gradient. Thus, for the training process we build up our sampling function with the gumbel-softmax trick (Jang et al., 2016). As the mask m is binary categorical variables, we replace the softmax with sigmoid and named the sampling function as gumbel sigmoid. The gumbel sigmoid function can be formulated as:\n\nmgs = sigmoid(\n\n),\n\n(3)\n\nlog( s\n\n1−s ) + g1 − g0\n\nτ\n\nwhere g1 and g0 are i.i.d samples drawn from Gumble(0, 1). τ is the temperature annealing parameter. Note that mgs is still not a binary mask. To obtain a binary mask but maintain the gradient, we employ the Straight-Through (ST) trick: m = 1≥0.5(mgs) + mgs − mgs, where 1≥0.5(x) ∈ {0, 1}n is the indicator function and the overline means stopping the gradient. With the path selector, the framework is able to produce very short symbolic policies for relatively simple tasks while produce complex but compact symbolic policies to handle hard tasks like Walker2d and Hopper in Mujoco Simulator(Todorov et al., 2012).\n\n5 META-LEARNING THE SYMBOLIC POLICY\n\nIn this section, we introduce the meta-learning process of our symbolic policy. Following PEARL (Rakelly et al., 2019), we build up our off-policy learning framework on top of the soft actor-critic algorithm (SAC) (Haarnoja et al., 2018). The main differences are the additional loss for the symbolic policy and the schedule for collecting data and simplifying symbolic expressions.\n\n5.1 LOSS FUNCTION\n\nWe now illustrate our additional loss function for the symbolic policy. As described in Section 4.1, to ensure the numerical stability, we regularize the symbolic operators and employ a penalty term for regularized operators. During training, we involve a penalty loss function Lpenalty which is the sum of the penalty terms of regularized operators in symbolic networks:\n\nLpenalty(θΦ, θΨ) =\n\ni=M (cid:88)\n\nj=L (cid:88)\n\nk=Nj (cid:88)\n\ni=1\n\nj=1\n\nk=1\n\nLgi,j,k (xi,j,k),\n\n(4)\n\nwhere θΦ, θΨ is the parameters of the parameter generator and the path selector, M is the dimension of action, L is the number of layers in a symbolic network, Nj is the number of regularized operators in layer j, xi,j,k is the input of operator gi,j,k and Lgi,j,k is the penalty term for this operator. We also involve a loss function Lselect to regularize the sum of score s which is the expectation of the L0 norm of the binary mask m as described in Section 4.2. To limit the minimum complexity of symbolic policies, we involve the target L0 norm defined as ltarget. Then the loss function can be defined as:\n\nLselect(θΨ) = max(\n\nsi − ltarget, 0)\n\n(5)\n\n(cid:88)\n\n5.2 TRAINING SCHEDULE\n\nIn practice, we train our symbolic policy in an off-policy manner. For meta-training epoch t, the agent first collects experiences into the corresponding buffer Bκi for several iterations. At the beginning of each iteration, we sample context cT from buffer Bκi and sample context variables z ∼ q(z|cT ) as PEARL does. The difference is that we also sample the symbolic policy with Φ(z) and Ψ(z) and use the sampled policy for the following steps of the iteration. Then we sample RL batch and context from the buffer and optimize the context encoder q(z|cT ) to recover the stateaction value function. For each training step, we sample a new symbolic policy. We employ the\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Comparison for different kinds of contextual policies on Meta-RL tasks. We show the mean and standard deviation of returns on test tasks averaged over five runs.\n\nsoft actor-critic to optimize the state-action value function. For the parameter generator and the path selector, we employ Lselect and Lpenalty in addition to the SAC loss. During training, we decrease the temperature parameter τ of gumbel sigmoid linearly and decrease the ltarget from the length of the origin parameters wg to a target value with a parabolic function. We illustrate the gradient flow in Figure 1. More details and the pseudo-code can be found in the Appendix.\n\n6 EXPERIMENT\n\nIn this section, we present the experimental results of our contextual symbolic policy (CSP). We first compare CSP to prior meta policies on several Meta-RL problems to evaluate the performance and the inference efficiency in Section 6.1. Then we analyze the symbolic policy generated for different tasks in Section 6.2. Finally, we carry out ablation experiments for CSP and show the results in 6.3.\n\n6.1 COMPARISON OF CONTEXTUAL POLICIES\n\nExperimental Settings. We first evaluate CSP on several continuous control environments. We modify the environments of OpenAI Gym (Brockman et al., 2016), including kinds of classic control, Box2D, and MuJoCo (Todorov et al., 2012) to be Meta-RL tasks similar to Rakelly et al. (2019); Huang et al. (2021); Fu et al. (2020). These environments require the agent to adapt across dynamics (random system parameters for Hopper-params, Walker2d-params, Lunarlander-params, InvDoublePend-params, different force magnitude and pole length for Cartpole-fl-ood) or reward functions (target velocity for Cheetah-vel-ood). For the symbolic network, we use the arranged densely connected structure in Figure 2. We run all environments based on the off-policy metalearning framework proposed by PEARL and use the same evaluation settings. We compare CSP with PEARL which concatenates the observation and context variables as the input of policy and Hyper (Sarafian et al., 2021) which generate the parameters of policy with a ResNet model based on the context variables. Note that the original Hyper also modifies the critic, but we build all the critics with the same network structure for consistency. More details of the environments and hyper-parameters can be found in the Appendix.\n\nPerformance comparisons. In Figure 3, we report the learning curves of undiscounted returns on the test tasks. We find that in all the environments, CSP achieves better or comparable performance than previous methods. In Hopper-params, Walker2d-params and InvDoublePend-params, CSP outperforms PEARL and Hyper during the whole training process. In Lunarlander-params, CSP achieves better final results. In Cartpole-fl-ood, CSP adapts to the optimal more quickly. In the out-of-distribution task Cheetah-vel-ood, we find the performance of PEARL and Hyper decrease during training because of over-fitting. But our CSP is less affected. In conclusion, expressing the policy in the symbolic form helps improve the generalization performance.\n\n7\n\n0.00M0.25M0.50M0.75M1.00M1.25M1.50M1.75M2.00Menv steps100150200250300350400450returnHopper-paramsPEARLHyperCSP0.00M0.50M1.00M1.50M2.00M2.50M3.00Menv steps100200300400500600700800returnWalker2d-params0.00M0.25M0.50M0.75M1.00M1.25M1.50M1.75M2.00Menv steps−600−500−400−300−200−1000returnCheetah-vel-ood0.00M0.25M0.50M0.75M1.00M1.25M1.50M1.75M2.00Menv steps−300−200−1000100200300returnLunarlander-params0.00M0.20M0.40M0.60M0.80M1.00Menv steps25050075010001250150017502000returnInvDoublePend-params0.00M0.20M0.40M0.60M0.80M1.00Menv steps50100150200returnCartpole-fl-oodUnder review as a conference paper at ICLR 2023\n\nTable 1: FLOPs and inference time of different contextual policies.\n\nEnvironment\n\nFLOPs/k\n\nTimes/ms\n\nCSP\n\nPEARL Hyper\n\nCSP\n\nPEARL Hyper\n\nWalker2d-params Hopper-params InvDoublePend-params Cartpole-fl-ood Lunarlander-g Cheetah-vel-ood\n\n3.11 0.51 0.039 0.004 0.015 0.53\n\n189.31 186.90 186.00 183.90 185.4 190.21\n\n5.64 4.10 3.59 1.79 3.08 7.18\n\n20.89 4.13 0.37 0.042 0.l4 4.90\n\n27.00 26.58 25.05 23.90 23.43 28.44\n\n22.64 17.23 12.32 9.08 12.34 24.16\n\nTable 2: Average count of all selected paths and paths selected by at least ninety percent policies.\n\nEnvironment\n\nSelected paths Mostly selected paths\n\nWalker2d-params Hopper-params InvDoublePend-params Cartpole-fl-ood Lunarlander-g Cheetah-vel-ood\n\n76.42 21.5 23.2 3.06 5.2 27.04\n\n70.3 20.33 21.0 3.0 5.0 18.5\n\nEfficiency comparisons. We also evaluate the deploying efficiency of contextual policies. We first calculate the flops of each kind of policy per inference step. Then we consider an application scenario that the algorithm control five thousand simulated robots with the Intel(R) Xeon(R) Gold 5218R @ 2.10GHz CPU and record the elapsed time per inference step 1. We report the results in Table 1. Compared to PEARL, CSP reduces the FLOPs by 60-45000x and reduces the inference time by up to 600x. Compared to Hyper, CSP reduces the flops by 2-450x and reduces the inference time by up to 200x. Thus, compared with pure neural network policies, the contextual symbolic policy has a significant advantage in computational efficiency.\n\n6.2 ANALYSIS OF SYMBOLIC POLICIES\n\nWe then analyze the symbolic policies for different tasks produced by CSP. For each environment, we sample 10 tasks from the environment task distribution and obtain the corresponding symbolic policies with CSP. Then we analyze the selected paths of these policies which determine the forms of the symbolic expressions. Table 2 shows the results. We calculate the average count of selected paths per action dimension among the policies2. We find that this number varies across different environments. The symbolic expression can be extremely short for simple environments like Cartpole or relatively long for complex environments like Walker2D. We also calculate the average count of paths which are selected by more than ninety percent of the symbolic policies. In almost all environments, the mostly selected paths account for a high percentage of the selected paths, which indicates that the expressions of symbolic policies for different tasks of the same environment share similar forms.\n\nThe proposed CSP can also improve interpretability. We take the Cartpole-fl-ood environment as an example and illustrate the Cartpole system in Figure 4. The form of the symbolic policies produced by CSP is action = c1 ∗ θ + c2 ∗ ̇θ + b, where θ is the the angle of the pole and ̇θ is the rotation rate of the pole. c1 and c2 are the positive coefficients and b is a small constant which can be ignored. The action is the force scale to push the cart. Then the policy can be interpreted as pushing the cart in the direction that the pole is deflected or will be deflected. To analyze the difference between policies for different tasks, we uniformly set the force magnitude and the length of the pole. Then we generate the symbolic policy with CSP and record the coefficients. As Figure 5 shows, c1 and c2 tend to increase when the force magnitude decrease and the length increase, which is in accord with our intuition. We will give examples of symbolic policy for other environments in the Appendix.\n\n1Note that we accelerate Hyper by only updating the parameters of policy when we update the context\n\nvariables. Thus, Hyper only uses the policy model without the ResNet model to infer an action.\n\n2We only consider paths that contribute to the final expression.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: The Cartpole system to be controlled.\n\nFigure 5: The coefficients of symbolic policies for Cartpole environments with different force magnitude and pole length.\n\nFigure 6: Ablation results of the symbolic network structure (left), the path selector(middle) and the symbolic operator(right). CSP P means the plain structure and CSP D means the densely connected structure. CSP L1 means replacing the path selector with the L1 norm minimization. CSP TANH means replace all the symbolic operators with tanh. CSP RELU means replace all the symbolic operators with relu.\n\n6.3 ABLATION\n\nFinally, we carry out experiments by ablating the features of CSP. We first examine our choice of the symbolic structure. We replace the symbolic network with a plain structure and a densely connected structure and compare the test task performance on Hopper-params environments. As Figure 6 shows, the dense connections effectively improve the performance and we can facilitate the search for the proper symbolic form by arranging the operators to further improve the performance. We also replace our path selector with the L1 norm minimization. For the stability of training, we linearly increase the scale of the L1 loss. To get a compact symbolic expression, we set the parameters with the absolute value less than 0.01 as zero near the end of training. We carry out experiments on the Hopper-params and show the learning curves of return in Figure 6. Besides, we calculate the average L0 norm of the mask for our path selector and the count of non-zero parameters for L1 norm minimization which is 30.38 and 34.59, respectively. Compared with the L1 norm minimization, our path selector achieves higher performance when producing slightly more compact symbolic policies. We also replace the symbolic operators with commonly used activation functions tanh and relu. We use the same framework to select the proper paths and set the same final L1 norm. The results in Figure 6 show that by combine different operators to form the symbolic policy, CSP are able to handle complex relationship between action and state and achieve higher performance compared with single kind of operators.\n\n7 CONCLUSION\n\nIn this paper, we propose to learn a contextual symbolic policy for Meta-RL. In our gradient-based learning framework, we train the contextual symbolic policy efficiently without any pre-trained model. The contextual symbolic policy achieves higher generalization performance than previous methods. Besides, it is more efficient when deployed and has better interpretability. Our approach may inspire future works of symbolic policy for reinforcement learning or meta-reinforcement learning.\n\n9\n\nlθ,ሶθF=f∗actionf89101112l0.30.40.50.60.7c120222426f89101112l0.30.40.50.60.7c21.41.51.61.71.80.00M0.25M0.50M0.75M1.00M1.25M1.50M1.75M2.00Menv steps100150200250300350400450returnCSP_PCSP_DCSP0.00M0.25M0.50M0.75M1.00M1.25M1.50M1.75M2.00Menv steps100150200250300350400450returnCSP_L1CSP0.00M0.25M0.50M0.75M1.00M1.25M1.50M1.75M2.00Menv steps100150200250300350400450returnCSP_RELUCSP_TANHCSPUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nOsbert Bastani, Yewen Pu, and Armando Solar-Lezama. Verifiable reinforcement learning via policy\n\nextraction. Advances in neural information processing systems, 31, 2018.\n\nY. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule.\n\nIn IJCNN-91-Seattle International Joint Conference on Neural Networks, volume ii, pp. 969 vol.2–, 1991. doi: 10. 1109/IJCNN.1991.155621.\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\n\nWojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n\nWilliam G. La Cava, Tilak Raj Singh, James Taggart, Srinivas Suri, and Jason H. Moore. LearnIn 7th International ing concise representations for regression by evolving networks of trees. Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Hke-JhA9Y7.\n\nArtur S. d’Avila Garcez, Aimore Resende Riquetti Dutra, and Eduardo Alonso. Towards symbolic reinforcement learning with common sense. CoRR, abs/1804.08597, 2018. URL http:// arxiv.org/abs/1804.08597.\n\nFabr ́ıcio Olivetti de Franc ̧a and Guilherme Seidyo Imai Aldeia.\n\nlutionary algorithm for symbolic regression. 10.1162/evco\\ a\\ 00285. URL https://doi.org/10.1162/evco_a_00285.\n\nEvol. Comput., 29(3):367–390, 2021.\n\nInteraction-transformation evodoi:\n\nYan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\n\nRasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J Smola. Meta-q-learning. arXiv\n\npreprint arXiv:1910.00125, 2019.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126–1135. PMLR, 2017.\n\nHaotian Fu, Hongyao Tang, Jianye Hao, Chen Chen, Xidong Feng, Dong Li, and Wulong Liu. Towards effective context for meta-reinforcement learning: an approach based on contrastive learning. arXiv preprint arXiv:2009.13891, 2020.\n\nMarta Garnelo, Kai Arulkumaran, and Murray Shanahan. Towards deep symbolic reinforcement learning. CoRR, abs/1609.05518, 2016. URL http://arxiv.org/abs/1609.05518.\n\nAbhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Metareinforcement learning of structured exploration strategies. Advances in neural information processing systems, 31, 2018.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861–1870. PMLR, 2018.\n\nDaniel Hein, Steffen Udluft, and Thomas A. Runkler. Interpretable policies for reinforcement learning by genetic programming. Eng. Appl. Artif. Intell., 76:158–169, 2018. doi: 10.1016/j.engappai. 2018.09.007. URL https://doi.org/10.1016/j.engappai.2018.09.007.\n\nRein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, and Pieter Abbeel. Evolved policy gradients. Advances in Neural Information Processing Systems, 31, 2018.\n\nBiwei Huang, Fan Feng, Chaochao Lu, Sara Magliacane, and Kun Zhang. Adarl: What, where, and\n\nhow to adapt in transfer reinforcement learning. arXiv preprint arXiv:2107.02729, 2021.\n\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv\n\npreprint arXiv:1611.01144, 2016.\n\nLouis Kirsch, Sjoerd van Steenkiste, and J ̈urgen Schmidhuber. Improving generalization in meta\n\nreinforcement learning using learned objectives. arXiv preprint arXiv:1910.04098, 2019.\n\nJohn R. Koza. Genetic programming - on the programming of computers by means of natural\n\nselection. Complex adaptive systems. MIT Press, 1993. ISBN 978-0-262-11170-6.\n\nJiˇr ́ı Kubal ́ık, Eduard Alibekov, and Robert Babuˇska. Optimal control via reinforcement learning with symbolic policy approximation. IFAC-PapersOnLine, 50(1):4162–4167, 2017. ISSN 2405-8963. doi: https://doi.org/10.1016/j.ifacol.2017.08.805. URL https://www.sciencedirect. com/science/article/pii/S2405896317312594. 20th IFAC World Congress.\n\nMikel Landajuela Larma, Brenden K. Petersen, Sookyung Kim, Cl ́audio P. Santiago, Ruben Glatt, T. Nathan Mundhenk, Jacob F. Pettit, and Daniel Faissol. Discovering symbolic policies with deep reinforcement learning. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 5979–5989. PMLR, 2021. URL http://proceedings.mlr.press/v139/landajuela21a.html.\n\nZhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-\n\nshot learning. arXiv preprint arXiv:1707.09835, 2017.\n\nHao Liu, Richard Socher, and Caiming Xiong.\n\nTaming maml: Efficient unbiased metareinforcement learning. In International conference on machine learning, pp. 4061–4071. PMLR, 2019.\n\nChristos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through\n\nl 0 regularization. arXiv preprint arXiv:1712.01312, 2017.\n\nDaoming Lyu, Fangkai Yang, Bo Liu, and Steven Gustafson. SDRL: interpretable and dataefficient deep reinforcement learning leveraging symbolic planning. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 2970–2977. AAAI Press, 2019. doi: 10.1609/aaai.v33i01.33012970. URL https://doi.org/10.1609/aaai.v33i01.33012970.\n\nGeorg Martius and Christoph H Lampert. Extrapolation and learning equations. arXiv preprint\n\narXiv:1610.02995, 2016.\n\nBrenden K. Petersen, Mikel Landajuela Larma, T. Nathan Mundhenk, Cl ́audio Prata Santiago, Sookyung Kim, and Joanne Taery Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=m5Qsh0kBQG.\n\nKate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International conference on machine learning, pp. 5331–5340. PMLR, 2019.\n\nJonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal\n\nmeta-policy search. arXiv preprint arXiv:1810.06784, 2018.\n\nSubham S. Sahoo, Christoph H. Lampert, and Georg Martius. Learning equations for extrapolation and control. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm ̈assan, Stockholm, Sweden, July 1015, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 4439–4447. PMLR, 2018. URL http://proceedings.mlr.press/v80/sahoo18a.html.\n\nElad Sarafian, Shai Keynan, and Sarit Kraus. Recomposing the reinforcement learning building blocks with hypernetworks. In International Conference on Machine Learning, pp. 9301–9312. PMLR, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJurgen Schmidhuber. Evolutionary principles in self-referential learning. on learning now to learn: The meta-meta-meta...-hook. Diploma thesis, Technische Universitat Munchen, Germany, 14 May 1987. URL http://www.idsia.ch/ ̃juergen/diploma.html.\n\nMichael D. Schmidt and Hod Lipson. Age-fitness pareto optimization. In Martin Pelikan and J ̈urgen Branke (eds.), Genetic and Evolutionary Computation Conference, GECCO 2010, Proceedings, Portland, Oregon, USA, July 7-11, 2010, pp. 543–544. ACM, 2010. doi: 10.1145/1830483. 1830584. URL https://doi.org/10.1145/1830483.1830584.\n\nSuraj Srinivas, Akshayvarun Subramanya, and R Venkatesh Babu. Training sparse neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 138–145, 2017.\n\nBradly Stadie, Ge Yang, Rein Houthooft, Peter Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever. The importance of sampling inmeta-reinforcement learning. Advances in Neural Information Processing Systems, 31, 2018.\n\nFlood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn:\n\nMeta-critic networks for sample efficient learning. arXiv preprint arXiv:1706.09529, 2017.\n\nSebastian Thrun and Lorien Pratt. Learning to learn. 1998.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026–5033. IEEE, 2012.\n\nSilviu-Marian Udrescu and Max Tegmark. AI feynman: a physics-inspired method for symbolic regression. CoRR, abs/1905.11481, 2019. URL http://arxiv.org/abs/1905.11481.\n\nAbhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri. Programmatically interpretable reinforcement learning. In International Conference on Machine Learning, pp. 5045–5054. PMLR, 2018.\n\nMarco Virgolin, Tanja Alderliesten, and Peter A. N. Bosman. Linear scaling with and within semantic backpropagation-based genetic programming for symbolic regression. In Anne Auger and Thomas St ̈utzle (eds.), Proceedings of the Genetic and Evolutionary Computation Conference, GECCO 2019, Prague, Czech Republic, July 13-17, 2019, pp. 1084–1092. ACM, 2019. doi: 10.1145/3321707.3321758. URL https://doi.org/10.1145/3321707.3321758.\n\nKenny Young, Baoxiang Wang, and Matthew E Taylor. Metatrace actor-critic: Online steparXiv preprint\n\nlearning control.\n\nsize tuning by meta-gradient descent for reinforcement arXiv:1805.04514, 2018.\n\nJin Zhang, Jianhao Wang, Hao Hu, Tong Chen, Yingfeng Chen, Changjie Fan, and Chongjie Zhang. Metacure: Meta reinforcement learning with empowerment-driven exploration. In International Conference on Machine Learning, pp. 12600–12610. PMLR, 2021.\n\nZeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient\n\nmethods. Advances in Neural Information Processing Systems, 31, 2018.\n\nXiao Zhou, Weizhong Zhang, Hang Xu, and Tong Zhang. Effective sparsification of neural networks with global sparsity constraint. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3599–3608, 2021.\n\nLuisa M. Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep RL via metalearning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/ forum?id=Hkl9JlBYvr.\n\n12",
    "reference": "# Summary Of The Paper\n\nThis paper focuses on the issues in the neural network-based policy in meta reinforcement learning (RL), such as overfitting \\& poor generalization ability, difficult/inefficient to deploy with limited computational resources, and poor interpretability. To address those issues, the framework of Contextual Symbolic Policy (CSP) is proposed by learning a contextual policy with a symbolic form based on the context variables for unseen tasks in meta-RL. Finally, experiments were conducted on several continuous control problems, with results demonstrating its effectiveness in terms of return, FLOPs, and interpretability.\n\n# Strength And Weaknesses\n\nStrengths:\n- Symbolic representation is attractive since it has shown some promising in mitigating the issues regarding generalization ability, sample efficiency, and interpretability. This work targets to applying the symbolic representation to the meta RL setting in order to improve the generalization ability, reduce the computational cost, and increase the interpretability. Specifically, a gradient-based learning method is proposed to learn the contextual symbolic policy from scratch in an end-to-end differentiable way, which consists of a symbolic network, a path selector and a parameter generator.\n\nWeaknesses:\n- The literature review could include some previous work of combining symbolic representation with reinforcement learning in different ways, such as [1,2,3], although this work focuses on achieving the symbolic representation in a differentiable manner.\n- The abuse notation about the context of $c_{T}$: such as $c$ and $c_{\\kappa}$.\n- In Figure 1: it's not clear where do actor loss and critic loss come from. Is the \"context symbolic policy\" network only for the policy network? Does $Q(s,a,z)$ represent the value network? If this is the case, what's the architecture for the value network?\n- Are the symbolic operators listed in the paper applicable to all different tasks? If not, there should indicate what kinds of tasks can work with this symbolic operator library.\n- What's the experimental setup? Are those tasks trained alternatively or separately?\n- What are the differences for the paths in Table 2? Currently, I don't understand what are those.\n- The empirical evaluation seems weak and the current form of results is somewhat far from what was said to close the gaps in the introduction section, such as overfitting \\& poor generalization issue, inefficient deployment issues with limited computational resources, and explainability issue. For example, the problem size looks small. Even for the PEARL, the FLOPs are only in the level of KBytes and this might not be a big issue w.r.t. the deployment. For interpretability, it works well for the classical control problems since the original policy itself has a direct relationship between the state features and the action features. Although the proposed method can learn a symbolic policy, it looks to me that this is only an explicit expression for the learned policy which has partial interpretability rather than only numerical numbers from neural networks. The learned expression could vary if there are more different symbolic operators included.\n\nReferences:\n- [1] Garnelo, M., Arulkumaran, K., & Shanahan, M. (2016). Towards deep symbolic reinforcement learning. arXiv preprint arXiv:1609.05518.\n- [2] Garcez, A. D. A., Dutra, A. R. R., & Alonso, E. (2018). Towards symbolic reinforcement learning with common sense. arXiv preprint arXiv:1804.08597.\n- [3] Lyu, D., Yang, F., Liu, B., & Gustafson, S. (2019). SDRL: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, No. 01, pp. 2970-2977).\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- There are some questions that could be clarified as I mentioned in the main Weaknesses.\n- The proposed method is somewhat novel, although it is built on the previous work of (Rakelly et al., 2019) and (Larma et al., 2021).\n\n# Summary Of The Review\n\nAccording to my comments in both the main Weaknesses and the section of Clarity, Quality, Novelty, I feel this is a paper where reasons to reject outweigh reasons to accept.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nAD-NEGF: AN END-TO-END DIFFERENTIABLE QUANTUM TRANSPORT SIMULATOR FOR SENSITIVITY ANALYSIS AND INVERSE PROBLEMS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nQuantum transport theory describes transport phenomena from first principles, which is essential for domains such as semiconductor fabrication. As a representative, the Non-Equilibrium Green Function (NEGF) method achieves superiority in numerical accuracy. However, its tremendous computational cost makes it unbearable for high-throughput simulation tasks such as sensitivity analysis, inverse design, etc. In this work, we propose AD-NEGF, to the best of our knowledge the first Automatic Differentiation (AD) based quantum transport simulator. AD-NEGF calculates gradient information efficiently by utilizing automatic differentiation and implicit layer techniques, while guaranteeing the correctness of the forward simulation. Such gradient information enables accurate and efficient calculation of differential physical quantities and solving inverse problems that are intractable by traditional optimization methods.\n\n1\n\nINTRODUCTION\n\nThe strong and lasting demand for higher computing power and lower energy consumption urges the downscale of semiconductor devices. Over the last 40 years, the microelectronics industry has successfully made the transistor feature size scale from 10μm to near 20nm, of which size the quantum mechanical effect starts to dominate (Anantram et al., 2008; Wang et al., 2008; Datta, 1997). Therefore, device simulators facing the future need to take a quantum theory oriented formulation, while NEGF, as a representative, is one of the most rigorous approaches among existing quantum transport methods (Jacoboni, 2010).\n\nAlthough NEGF shows superiority in simulation accuracy, it is also extremely time and computation consuming. Recently, many works successfully integrate machine learning techniques to resolve the accuracy-efficiency dilemma of scientific simulations. A typical paradigm is to build up learningbased surrogate models (e.g., a neural network) (Li et al., 2020; B ̈urkle et al., 2021; Pimachev & Neogi, 2021). By learning from data generated with highly accurate simulations beforehand, the surrogate model is expected to maintain first-principle accuracy while performing much faster in usage. A fatal problem of such methods is that there is no guarantee for prediction accuracy, especially for input out of the distribution of the training dataset. Such drawback limits the application of machine learning based surrogates in quantum transport scenarios.\n\nAn alternative is to utilize automatic differentiation to make the computation process differentiable. In quantum transport simulations, practically useful information is often related to calculating derivatives. For instance, the thermoelectric property measured by the Seebeck coefficient; the sub-threshold swing of MOSFET that is related to the derivative of the drain current ID with respect to the applied gate voltage Vg, etc. Compared to traditional numerical differentiation, automatic differentiation can overcome the trade-off between the round-off error and the truncation error when choosing the step-size (Gautschi, 1997, Chap. 3), and also can be numerically more efficient when the input dimension is high. Moreover, in theoretical inverse problems, an end-to-end differentiable solver is also extremely useful and in fact, critical. The availability of gradients makes it possible to conduct efficient gradient-based optimization, which can outperform black-box optimization methods such as Bayesian optimization, genetic algorithm, etc., and can conduct optimization on a scale that black-box methods cannot. Recent advances have also shown the value to apply differentiable\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nprogramming in scientific computation scenarios, such as fluid dynamics (Holl et al., 2019), quantum chemistry (Kasim & Vinko, 2021), molecular dynamics (Schoenholz & Cubuk, 2020), photonic crystal optimization (Minkov et al., 2020), etc.\n\nIn this work, we propose AD-NEGF, to the best of our knowledge the first end-to-end differentiable quantum transport simulator. The entire numerical process of NEGF and TB modeling is implemented in PyTorch, including the computation of the self-energy term, the Green function, the electrostatic potential, the transport properties, as well as an optional Slater-Koster Tight-Binding (SKTB) module to generate the block tri-diagonal Tight-Binding (TB) Hamiltonian (Klymenko et al., 2021), which we will introduce in detail in Section 3. The backward pass to compute the gradients is improved by utilizing the implicit gradient techniques and the adjoint sensitivity method for Partial Differential Equations (PDE). To efficiently backpropagate through Poisson’s equation in transport, we propose and implement the image charge gradient method, which can utilize the Fast Multi-pole Method (FMM) to reduce the backpropagation complexity of Poisson’s equation from O(N 3) to O(N 4/3). We demonstrate the capability of AD-NEGF to efficiently and accurately compute differential physical properties by comparing with numerical differentiation. Also, it is shown that by cooperating ADNEGF with the gradient-based optimizer, it can perform high-dimensional optimization at a scale that is not affordable with conventional optimization approaches. Furthermore, in a more practical scenario of material doping optimization where we optimize the empirical SK parameters of injected atoms, our method shows significant advances in convergence speed and optimization solution, compared with traditional black-box optimization methods.\n\nOur contributions can be summarized as follows:\n\n• We propose and implement AD-NEGF, as far as we know the first end-to-end differentiable quantum transport simulator, including the NEGF method, the Poisson’s equation module for self-consistent electrostatic potential computation, and the SKTB module to generate the tight-binding Hamiltonian from the coordinates and properties of the system atoms.\n\n• The efficiency of the backward gradient computation is improved by applying the implicit gradient method, the adjoint method for PDEs, as well as our newly proposed gradient computation for the image charge method.\n\n• We validate the advantages of AD-NEGF in calculating differential transport quantities, high-dimensional parameter fitting, and device optimization, where AD-NEGF outperforms numerical differentiation and black-box optimization methods.\n\n2 RELATED WORKS\n\nNEGF. Originating from Keldysh (1964); Kadanoff (2018), NEGF has been a well-received method in the quantum transport theory, which describes a system with a finite bias voltage and contact interactions under consideration. Recently, NEGF-based computation methods gain increasing popularity for the simplicity of the formulation, and the easy implementation in programming (Ferry & Goodnick, 1999; Taylor et al., 2001; Brandbyge et al., 2002; Fetter & Walecka, 2012), which makes NEGF one of the most widely applied methods in transport calculation. Several methods dedicated to improving its numerical stability and computational efficiency are proposed (Sancho et al., 1985; Krsti ́c et al., 2002; Rungger & Sanvito, 2008), some of which are widely implemented in modern quantum transport simulation software, including but not limited to Papior et al. (2017); Smidstrup et al. (2019); Steiger et al. (2011). On the other hand, despite its advantages, the NEGF method suffers from heavy computational burdens.\n\nAI for Quantum Transport. There have been prior works to apply machine learning techniques in quantum transport, mostly by training a neural network with data generated from first-principle simulations, so that the neural network can serve as an efficient surrogate model to predict transport properties, such as conductance (B ̈urkle et al., 2021; Pimachev & Neogi, 2021; Li et al., 2020), transport coefficients (Lopez-Bezanilla & von Lilienfeld, 2014), etc. Most existing methods use relatively simple deep learning models such as multi-layer perceptrons ( ˇZupanˇci ́c et al.) and convolutional networks (Han et al., 2021; Souma & Ogawa, 2021; 2020), while in some cases more advanced and specially designed models are utilized (B ̈urkle et al., 2021). However, as mentioned in Section 1, a dataset generated with ab-initio simulation is required, which is expensive to obtain. Moreover,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Workflow of AD-NEGF. Solid lines indicate the forward simulation flow, where loops denote self-consistent iterations. Dashed lines indicate the gradient backpropagation flow.\n\nlimited by the fundamental drawbacks of statistical learning, there is no guarantee for prediction accuracy, which will limit its application scenarios.\n\nDifferentiable Programming. Deep learning has been applied to more and more diverse scenarios, which requires the network structure to be more and more flexible. One emerging direction is to embed physical models or numerical computation processes into the model, in order to improve data efficiency, generalization capability, and interpretability. This is sometimes referred to as differentiable programming. It requires the automatic differentiation framework to support implicit numerical operations, such as fixed-point iterations (Bai et al., 2019), optimization (Amos & Kolter, 2017), initial value problems (Chen et al., 2018), etc. Differentiable programming has been widely applied to physical simulations (Hu et al., 2019; Innes et al., 2019), such as rigid body dynamics (de Avila Belbute-Peres et al., 2018; Freeman et al., 2021), computational fluid dynamics (Kochkov et al., 2021; Holl et al., 2019; Schenck & Fox, 2018), ray tracing (Li et al., 2018), etc. More specifically in ab-initio simulations, there have been works for density functional theory (Li et al., 2021; Kasim & Vinko, 2021), Hartree–Fock (Tamayo-Mendoza et al., 2018), coupled cluster methods (Pavoˇsevi ́c & Hammes-Schiffer, 2020), and molecular dynamics (Schoenholz & Cubuk, 2020). However, we have not found any previous works to apply differentiable programming techniques in the quantum transport domain.\n\n3 PRELIMINARIES FOR THE NON-EQUILIBRIUM GREEN FUNCTION METHOD\n\nIn this section, we give a brief introduction to the NEGF method, while more details can be referred to in Appendix A. Consider a transport system containing a device region and two semi-infinite contacts that attach to the left and right sides of the device, as shown in Figure 1. The contacts can also be referred to as leads or electrodes interchangeably. According to the theory of quantum mechanics, the whole system, including the device and the contacts, can be fully described by its Hamiltonian H. In this paper, we consider the Tight-Binding (TB) model (Slater & Koster (1954)), which makes H block tri-diagonal. We assume a set of basis has been selected so that the full NEGF process can be expressed in the matrix form. The stationary Schr ̈odinger equation of this open system is HΨ = EΨ, where Ψ stands for the wave function of electrons, and E is a scalar value corresponding to the system energy. The characteristics of the system are contained in its Green function\n\nG = [EI − H]−1,\n\n(1)\n\nwhere I is the identity matrix. However, the Hamiltonian H is infinitely large and hence intractable. This is resolved by computing the Green function only for the device part, and modeling the effect of two semi-infinite contacts in a term Σ referred to as self-energy. The device Green function GD will then be used to describe the non-equilibrium charge transport process by solving Poisson’s equation\n\n3\n\nCompute Green FunctionΣ=ΣL+ΣRGD(E)=EI−HD−Σ−1Compute Electrostatic Potential∇2V=ρ−ρ0Compute Electrode Self-Energygs−1=EI−HL/R−EI−HLD/RDgs(EI−HLD/RD∗)DeviceLeft Electrode Right ElectrodeΣL=HLDgsHLD∗,ΣR=HRDgsHRD∗gSρ=න−∞+∞ImGDEdEVCompute Transport PropertiesTE=TraceΓLGDΓRGD∗I=න−∞+∞T(E)(fS−fD)dEΣL,ΣRV,GGHD,HL,HR,HLD,HRDUnder review as a conference paper at ICLR 2023\n\nin a self-consistent iteration. The output self-consistent potential field V and GD can be used to compute transport properties, such as transmission, current, etc.\n\nDevice Green Function. The device Green function is expressed as a function of the device Hamiltonian HD and the self-energy Σ:\n\nGD = [EI − HD − Σ]−1.\n\n(2)\n\nDirectly computing the matrix inversion is with complexity O(N 3), which is unbearable as the matrix size is proportional to the vast amount of atoms. By utilizing the block tri-diagonal form of the Hamiltonian matrix, an efficient recursive algorithm (Anantram et al., 2008) can be implemented, which scales linearly with the system size.\n\nElectrode Self-Energy. Self-energy of electrodes is computed from the surface green function gs of the electrode layer coupled with devices. Under the half-infinite hypothesis, gs is approximated identically, expressed as a self-consistent equation:\n\ns = [Al − Al,l−1gsA† g−1\n\nl−1,l],\n\n(3)\n\nwhere Al,l−1 is blocks of EI − H of coupling between l and l − 1 layer. To speed up, we implement the Lopez-Sancho algorithm (Sancho et al., 1985), as illustrated in Algorithm 1, which converges exponentially faster than the conventional self-consistent iteration. Details of the algorithm are illustrated in Appendix. We also implement a modern method based on the generalized eigenvalue problem (Wang et al., 2008) as an alternative.\n\nElectrostatic Potential. In NEGF, charge transfer due to the applied bias voltage is modeled as an external potential, which is attained self-consistently by solving Poisson’s equation for the electrostatic field:\n\n(cid:26)∇ · (cid:15)(r)∇[∆V (r)] = −[ρ(r; ∆V ) − ρ0(r)],\n\n∆V (r)|{zL,zR}\n\n= {VL, VR}.\n\n(4)\n\nwhere VL and VR represent the voltage boundary conditions at electrodes zL and zR, ∆V = V − V0 is the difference between real potential energy with equilibrium one. This equation is solved selfconsistently with updated Hneq. Poisson’s equation can be solved using numerical PDE solvers with spherical charges. Meanwhile, a computationally more efficient image charge method using the Fast Multipole Method (FMM) is preferred (Svizhenko & Anantram, 2005; Zahn, 1976). After the procedure converges to a stable solution, transport properties can be computed accordingly. Once the convergence is achieved, the Green function computed with Hneq will be used to compute various transport properties. We refer to Appendix for details of equation solving, and expression of transport properties.\n\n4 METHOD OF DIFFERENTIATING THE NEGF PROCESS\n\nThe differentiable NEGF model is implemented with PyTorch (Paszke et al., 2019). We extend the autograd function with implicit gradient techniques for backpropagation through self-consistent iterations, with the adjoint sensitivity method for calculating gradients through Poisson’s equation (Pontryagin, 1987). Moreover, the efficient gradient formula for the image charge method (Svizhenko & Anantram, 2005), accelerated by the Fast Multipole Method (FMM) method, is proposed to speed up the gradient calculation of Poisson’s equation. The derived formula can be regarded as a summation of point charges produced by the gradients and thus can also be computed with FMM. Details of the customized backward propagation modules are explained as follows.\n\n4.1\n\nIMPLICIT GRADIENT\n\nThe implicit gradient method is implemented when the direct automatic differentiation through function y = f (x) is unavailable or expensive to compute. Instances often arise when one wants to calculate gradients through numerical solvers of equilibrium problems or complicated iterative algorithms. Based on the implicit function theorem (Krantz & Parks, 2002), if there exists such\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nconstrained function h(y, x) = 0 where y is taken as the converged output of function f , the gradient dx can be given as dy\n\n(cid:104) ∂h(y,x) ∂y\n\n∂h(y,x) ∂x\n\ndx = −\n\n(cid:105)−1\n\ndy\n\n.\n\nWe use the implicit gradient techniques to derive the gradient of the surface Green function (Sancho et al., 1985), where according to the ideal definition of the two semi-infinite leads, the converged surface Green function gs(θ) must satisfy the self-consistent Equation (3). Hence h(gs, θ) = [All − All−1gsA† l−1l] − gs−1 = 0, where All stands for [ESll − Hll], and θ denotes the input variables to compute gs. Thus we could write down the gradient of gs with respect to θ explicitly by dgs\n\n(cid:105)−1\n\ndθ = −\n\n(cid:104) ∂h(gs,θ) ∂gs\n\n∂h(gs,θ) ∂θ\n\n.\n\nAnother scenario that the implicit gradient method can be applied to is to compute gradients through the self-consistent Poisson’s equation loop, where the system electrostatic potential is updated until consistent with the bias voltage of contacts and other boundary conditions.\n\n4.2 ADJOINT METHOD FOR PDE\n\nIn order to perform backpropagation through the solver for Poisson’s equation, adjoint sensitivity method (Plessix, 2006; Pontryagin, 1987) for PDE-constrained optimization problems is adopted, which is widely applied in constrained optimization of inverse problems. Here, the forward process of the numerical PDE solver is unaltered, which is often denoted as the state equation that links the controlled parameter and the state of the constrained system. Meanwhile, an adjoint state equation that connects the perturbation of variables and states is solved by using the same numeral solver. Then gradients can be evaluated with the adjoint state, and join in the gradient chain of backward propagation. Since the adjoint state equation is independent of the number of controlled variables, the total complexity is only proportional to the forward process, which makes it suitable for control problems with scalar output and high-dimensional input. Recently, the adjoint sensitivity method has also been applied in designing neural network structures with physics intuitions, including the Neural ODE (Chen et al., 2018) and Deep Equilibrium Models (Bai et al., 2019), which can be considered as examples of cooperations of automatic differentiation and adjoint methods.\n\n4.3 GRADIENT OF FMM IMAGE CHARGE METHOD\n\nAn alternative approach to solve Poisson’s equation raised in Equation (4), is to apply the point charge approximation, where the charge density is considered as the linear combination of a series of point charges as ∆q(r) = (cid:80) i ∆qiδ(r − ri). Then by employing the linearity of Poisson’s equation, the original form can be further decomposed into Laplace’s equation with Dirichlet boundary conditions and Poisson’s equation with zero Dirichlet boundary conditions:\n\n(cid:26)−∇2(∆V1(r)) = 0,\n\n∆V1(r)|{zL,zR} = {VL, VR}.\n\n(cid:26)−∇2(∆V2(r)) = 1 ∆V2(r)|Σ = 0.\n\n(cid:15) ∆ρ(r),\n\n(5)\n\nThe first Laplace’s equation can be easily solved by a linear drop potential. The second equation can be solved by assuming the charge density as a combination of point charges of each atom site. The closed-form solution can be obtained using the image charge method (Svizhenko & Anantram, 2005; Harb, 2019), and the second potential can be written as:\n\n1\n\nt2 ij + (zi − zj)2\n\nV2(ri) =\n\n(cid:88)\n\nj∈N,j(cid:54)=i\n\nqj 4π(cid:15)\n\n+\n\n(cid:88)\n\nj∈N\n\nqj 4π(cid:15)\n\n∞ (cid:88)\n\nn=1\n\n(cid:113)\n\n\n\n\n\n(cid:113)\n\n1\n\nij + ∆2 t2\n\n1\n\n−\n\n1\n\n(cid:113)\n\nij + ∆2 t2\n\n2\n\n+\n\n(cid:113)\n\n1\n\nij + ∆2 t2\n\n3\n\n−\n\n(cid:113)\n\n1\n\nij + ∆2 t2\n\n4\n\n\n\n ,\n\n(6)\n\nij = (xi − xj)2 + (yi − yj)2, and ∆2 stands for the distance in the transport direction where t2 between central charges and charges from two electrodes. Therefore, the first term here describes the interactions inside the device, while all the remaining terms simulate the effect of its coupling to charges outside. The summation of the second term is computed until achieving certain accuracy, which is empirically hundreds of site numbers. Hence a direct summation is also too expensive to\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Structure of an AGNR with width 7 and length 5.\n\n(b) Structure of a 7-4 graphene nano-junction.\n\n(c) Structure of a 5-2 graphene nano-junction.\n\nFigure 2: Device structures used in the experiments.\n\n(a) Transmission and DOS calculated by AD-NEGF and confirmed with ASE.\n\n(b) Seebeck coefficient and differential conductance calculated by AD-NEGF.\n\nFigure 3: Transmission Quantity Computation with AD-NEGF.\n\ncompute. In this case, the Fast Multipole Method (Engheta et al., 1992) is employed to reduce the computational complexity from O(N 3) to O(N 4/3).\n\nTo perform backward propagation through the fast multipole layer, the gradient of the output potential to the charges is required. By taking the derivative of a target objective L : C d −→ R, the derivative of L with respect to charge qj can be expanded as the image summation form of accumulated gradients from the last layer, which is:\n\n∂L(V ) ∂qj\n\n(cid:88)\n\n=\n\ni\n\n∂L ∂Vi\n\n∂Vi ∂qj\n\n(cid:88)\n\n=\n\ni∈N,i(cid:54)=j\n\n∂L/∂Vi 4π(cid:15)\n\n1\n\n(cid:113)\n\nt2 ij + (zj − zi)2 \n\n+\n\n(cid:88)\n\ni∈N\n\n∂L/∂Vi 4π(cid:15)\n\n∞ (cid:88)\n\nn=1\n\n1\n\n\n\n(cid:113)\n\nt2 ij + ∆2\n\n1\n\n−\n\n(cid:113)\n\n(7)\n\n\n\n .\n\n(8)\n\n−\n\n(cid:113)\n\n1\n\nt2 ij + ∆2\n\n4\n\n1\n\nt2 ij + ∆2\n\n2\n\n+\n\n1\n\n(cid:113)\n\nt2 ij + ∆2\n\n3\n\nSimilarly, computing gradients of this form can be accelerated by the Fast Multipole Method, which is also with complexity O(N 4/3) and much faster than solving adjoint Poisson’s equation.\n\n5 APPLICATIONS\n\nIn this section, the advantages of AD-NEGF for sensitivity analysis and inverse problems are demonstrated with three applications. For all experiments, we take graphene as the transport system, including the Armchair Graphene NanoRibbon (AGNR) and the graphene nano-junction, the basic device structures of which are displayed in Figure 2. More details of the experimental setup can be found in Appendix B.\n\n6\n\n864202461017.55.02.50.02.55.07.52028642024620201234T(E)Transmission of AGNR(7)3210123E (ev)01020304050DOSDOS of AGNR(7)ASEAD-NEGF3210123E (ev)402002040S(E)Seebeck Coefficient by FDSeebeck Coefficient by AD0.00.51.01.52.02.53.0V (eV)0.00.10.20.3I (V)I (V)0.00.51.01.52.02.5T(E)T(E)0.00.51.01.5dI/dVdI/dVUnder review as a conference paper at ICLR 2023\n\n5.1 DIFFERENTIAL TRANSMISSION QUANTITY COMPUTATION\n\nA direct and major application to perform differentiation on physical models is to evaluate differential physical quantities. In most cases, the analytical form is difficult to obtain. For numerical differentiation, the trade-off will be encountered between the round-off error and the truncation error when choosing the step-size (Gautschi, 1997, Chap. 3), and the computation will be unacceptable when the input dimension is high. On the contrary, automatic differentiation can achieve machine precision while maintaining O(1) complexity when the output dimension is low and the input dimension is high (Baydin et al., 2018).\n\nIn this experiment, we first validate the correctness of the forward computation of AD-NEGF. As shown in Figure 3(a), the transmission coefficient and the density of states (DOS) of an AGNR system with width 7 are computed by AD-NEGF, which perfectly match the results of ASE (Larsen et al., 2017), an atomistic simulation package including electron transport modules. Based on it, we compute two differential transmission quantities, the Seebeck coefficient and the differential conductance, which are shown in Figure 3(b). The Seebeck coefficient is a measure of the magnitude of an induced thermoelectric voltage in response to a temperature difference across an atomic structure, mathematically expressed as the derivative of transmission T (E) concerning the chemical potential E (Reddy et al., 2007):\n\nSjunction = −\n\nBT\n\nπ2k2 3e\n\n∂ln(T (E)) ∂E\n\n,\n\n(9)\n\nwhere T stands for the temperature and kB is the Boltzmann constant. The differential conductance is the gradient of electronic current to voltage: ID = dI\n\ndV .\n\nThe singularity of the transmission function leads to peaks in the Seebeck coefficient curve, which is highly sensitive thus challenging for derivative calculation, as illustrated in Figure 4. To amplify the phenomenon for clearer demonstration, the output transmission coefficient T (E) of the forward computation is transformed into half-precision floating-point format for both automatic and numerical differentiation, before it is used to compute the Seebeck coefficient. It can be seen that, with ADNEGF, we can still generate high-quality results. However, for numerical differentiation, the trade-off between the truncation error and the round-off error is observed by selecting different step-sizes from 1e-2 to 1e-5. With a large step-size, peaks may be skipped or mistakenly generated due to truncation error. With a small step-size, lacking machine precision causes noises on the curve. Specifically for step-size 1e-5, the calculated curve becomes totally meaningless. Moreover, even though this is not a high-dimensional input situation, evaluating the Seebeck coefficient with AD-NEGF can still be faster than numerical differentiation, since in AD-NEGF the backward pass is improved. According to our experiments, for a smaller system with 70 carbon atoms, computing the Seebeck coefficient for 400 energy samples costs 71.1 seconds with AD-NEGF and 98.3 seconds with numerical differentiation. For a larger system with 240 carbon atoms, computing the Seebeck coefficient for 400 energy samples costs 363.1 seconds with AD-NEGF and 512.6 seconds with numerical differentiation.\n\nTo summarize, by conducting the above experiments, the correctness and effectiveness of AD-NEGF are validated. With AD-NEGF, differential transport quantities can be calculated simply by calling one backward step. Moreover, the process of computing derivatives is itself differentiable, permitting the computation of higher-order derivatives, which remains for further discovery.\n\n5.2 TRANSMISSION FITTING\n\nInverse problems, which require inferring input parameters reversely from the output objectives, are in general difficult in first-principle simulations. Black-box optimization methods require sampling a large number of input combinations, the cost of which grows exponentially with the number of parameters. Based on the efficient and accurate gradient computation capability of AD-NEGF, performing gradient-based optimization holds the potential to outperform black-box optimization methods for high dimensional inverse problems.\n\nWe conduct a 104 dimensional optimization experiment to fit the transmission curve of one graphene nano-junction to another. The target system is a 7-4 nano-junction, with 7 graphene rings on the left and 4 on the right. The fitting system is a 5-2 nano-junction, and the fitting variables are the elements of its Hamiltonian, including the device, leads, and the corresponding couplings. The dimension of the optimizing variables is at the level of 104. The transmission curve, as shown in Figure 5,\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Comparison of Automatic Differentiation and Numerical Differentiation with different step-sizes.\n\nFigure 5: The fitting loss and the fitted transmission curve of a 5-2 graphene nano-junction.\n\nconsists of 2000 energy points sampled from (-5eV, 5eV). Since directly computing the gradients of all 2000 points is inefficient for iterations, we apply the stochastic gradient descent algorithm to conduct mini-batch optimization, which has shown supremacy of efficiency and performance in high dimensional optimization problems. More specifically speaking, the fitting parameters are optimized with the Adam optimizer (Kingma & Ba, 2014) built in PyTorch, making the procedure highly similar to training a neural network.\n\nThe results are displayed in Figure 5, where the loss is reduced to a considerably low level, which means the converged parameters of the 5-2 nano-junction fit nicely to the larger 7-4 nano-junction. The fitted curve is akin to a smoothed version of the curve of the 7-4 junction, which agrees with the intuition since a graphene junction of 5-2 is of less freedom than that of a 7-4 nano-junction. On the other hand, we have also tried traditional black-box methods, such as Bayesian optimization, the genetic algorithm, and gradient-based optimization with numerical differentiation, but none of them can even work for this problem because of the curse of dimensionality.\n\n5.3 ON-SITE DOPING OPTIMIZATION\n\nModern material engineering is capable of manipulating at the atomic level. More specifically speaking, by performing processes such as deformation, doping, etc., microscopic physical quantities such as atomic spatial coordinates, bond lengths and doping positions can be changed, which further modify the macroscopic material properties. The doping process is one of the most common techniques in material development, which can dramatically change the properties of the original material, by injecting foreign atoms into specific positions. In this experiment, we further explore the possibility to solve practical inverse problems with AD-NEGF by performing an end-to-end doping optimization cooperated with established material models.\n\nIn this experiment, we try to reduce the average transmission of an AGNR system in a specified energy range of (-1eV, 1eV), by injecting other atoms into the center of the AGNR system along the transmission direction. A reduction of transmission coefficient near zero Energy point would indicate an increase of the truncation voltage, which changes the semi-conductive properties of the device (Wu et al., 2013). Doping can be modeled as an effective change in the site and the hopping terms in the tight-binding Hamiltonian, i.e., the diagonal and off-diagonal elements of the Hamiltonian matrix. This on-site approximation allows us to treat doping optimization as tuning local terms in the Hamiltonian influenced by the injected atoms. However, although the process above is applicable, the tuning terms in the TB Hamiltonian need to be distinguished carefully from those invariant ones. It will be more convenient to cooperate with an SKTB model, which constructs the TB Hamiltonian based on strict rules of local dependence of atom identities and their semi-empirical SK parameters. Besides convenience, it has more concrete physical interpretation than directly optimizing elements\n\n8\n\n20020AD20020FD (step-size=1e-2)20020FD (step-size=1e-3)20020FD (step-size=1e-4)3210123E (ev)20020FD (step-size=1e-5)02505007501000125015001750iter0.20.40.60.81.01.2loss42024E (eV)0123T(E)7-4 (target)5-2 (initial)5-2 (fitted)Under review as a conference paper at ICLR 2023\n\n(a) Loss against running time and iteration steps respectively.\n\n(b) Original and optimized transmission curves.\n\nFigure 6: Comparison between AD-NEGF and conventional black-box optimization methods in the doping optimization task.\n\nof the Hamiltonian, since it provides guidelines for practitioners to find the possible atom satisfying the SKTB parameters from the optimization result. In this way, doping optimization is modeled as an optimization of the SKTB parameters of the doped atoms, which include the orbital energy and parameters for two center integrals. The total number of optimization variables is 13.\n\nFor comparison, we also apply black-box optimization methods including the genetic algorithm and Bayesian optimization. The results are displayed in Figure 6. In the loss diagram, the gradient-based method converges significantly faster and better than the other approaches, especially at the beginning of the training. The loss curves of the genetic optimization and Bayesian optimization are also dropping, but much slower and less effective, in terms of either the running time or the iteration step. Moreover, the performances of the genetic / Bayesian optimization are sensitive to preset hyper-parameters. Corresponding to the loss curves, the results of optimized transmission curves demonstrate the advantages of AD-NEGF in a more straightforward way, where the gradient-based optimization gives a much cleaner band with low transmission in the target interval (-1eV, 1eV) compared to other methods. These results validate the effectiveness of the AD-NEGF method in conducting practical atomic-level inverse design to optimize transport properties by cooperating with material models.\n\n6 CONCLUSION\n\nIn this paper, we have proposed AD-NEGF, the first end-to-end differentiable quantum transport simulator to the best of our knowledge. It guarantees the correctness of the forward simulation without the need for data or training, while providing gradient information based on differentiable programming. Compared with numerical differentiation, gradients can be computed more efficiently and accurately. Moreover, it accelerates parameter fitting and parameter optimization with gradientbased optimization. The results are validated in applications such as differential physical quantity computation, transmission curve fitting, and device doping optimization.\n\nREFERENCES\n\nBrandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.\n\nIn International Conference on Machine Learning, pp. 136–145. PMLR, 2017.\n\nMP Anantram, Mark S Lundstrom, and Dmitri E Nikonov. Modeling of nanoscale devices. Proceed-\n\nings of the IEEE, 96(9):1511–1550, 2008.\n\n9\n\n020040060080010001200time (second)0.20.40.60.81.01.21.4loss020406080100step0.20.40.60.81.01.21.4lossBayesian OptimizationAD-NEGF basedGenetic Algorithm0.02.5Before Doping0.02.5Doping Based on AD-NEGF0.02.5Doping Based on Bayesian Optimization32101230.02.5Doping Based on Genetic Algorithm0.00.20.40.60.81.0E (eV)0.00.20.40.60.81.0T(E)Under review as a conference paper at ICLR 2023\n\nDenis A Areshkin and Branislav K Nikoli ́c. Electron density and transport in top-gated graphene nanoribbon devices: First-principles green function algorithms for systems containing a large number of atoms. Physical Review B, 81(15):155450, 2010.\n\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in Neural\n\nInformation Processing Systems, 32, 2019.\n\nAtilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of machine learning research, 18, 2018.\n\nMads Brandbyge, Jos ́e-Luis Mozos, Pablo Ordej ́on, Jeremy Taylor, and Kurt Stokbro. Densityfunctional method for nonequilibrium electron transport. Physical Review B, 65(16):165401, 2002.\n\nMarius B ̈urkle, Umesha Perera, Florian Gimbert, Hisao Nakamura, Masaaki Kawata, and Yoshihiro Asai. Deep-learning approach to first-principles transport simulations. Physical Review Letters, 126(17):177701, 2021.\n\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\n\ndifferential equations. Advances in neural information processing systems, 31, 2018.\n\nSupriyo Datta. Electronic transport in mesoscopic systems. Cambridge university press, 1997.\n\nFilipe de Avila Belbute-Peres, Kevin Smith, Kelsey Allen, Josh Tenenbaum, and J Zico Kolter. Endto-end differentiable physics for learning and control. Advances in neural information processing systems, 31:7178–7189, 2018.\n\nNader Engheta, William D Murphy, Vladimir Rokhlin, and Marius S Vassiliou. The fast multipole method (fmm) for electromagnetic scattering problems. IEEE Transactions on Antennas and Propagation, 40(6):634–641, 1992.\n\nDavid Ferry and Stephen Marshall Goodnick. Transport in nanostructures. Number 6. Cambridge\n\nuniversity press, 1999.\n\nAlexander L Fetter and John Dirk Walecka. Quantum theory of many-particle systems. Courier\n\nCorporation, 2012.\n\nC Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax–a differentiable physics engine for large scale rigid body simulation. arXiv preprint arXiv:2106.13281, 2021.\n\nWalter Gautschi. Numerical analysis. Springer Science & Business Media, 1997.\n\nSeung-Cheol Han, Jonghyun Choi, and Sung-Min Hong. Acceleration of three-dimensional device In 2021 International Conference on\n\nsimulation with the 3d convolutional neural network. Simulation of Semiconductor Processes and Devices (SISPAD), pp. 52–55. IEEE, 2021.\n\nMohammed Aziz Harb. Scattering Effects in Atomistic Quantum Transport Simulations. McGill\n\nUniversity (Canada), 2019.\n\nPhilipp Holl, Nils Thuerey, and Vladlen Koltun. Learning to control pdes with differentiable physics.\n\nIn International Conference on Learning Representations, 2019.\n\nYuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Fr ́edo Durand. Difftaichi: Differentiable programming for physical simulation. arXiv preprint arXiv:1910.00935, 2019.\n\nMike Innes, Alan Edelman, Keno Fischer, Chris Rackauckas, Elliot Saba, Viral B Shah, and Will Tebbutt. A differentiable programming system to bridge machine learning and scientific computing. arXiv preprint arXiv:1907.07587, 2019.\n\nCarlo Jacoboni. Theory of electron transport in semiconductors: a pathway from elementary physics\n\nto nonequilibrium Green functions, volume 165. Springer Science & Business Media, 2010.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nLeo P Kadanoff. Quantum statistical mechanics. CRC Press, 2018.\n\nMuhammad F Kasim and Sam M Vinko. Learning the exchange-correlation functional from nature with fully differentiable density functional theory. Physical Review Letters, 127(12):126403, 2021.\n\nL. V. Keldysh. Diagram technique for nonequilibrium processes. Zh. Eksp. Teor. Fiz., 47:1515–1527,\n\n1964.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nMV Klymenko, JA Vaitkus, JS Smith, and JH Cole. Nanonet: an extendable python framework for semi-empirical tight-binding models. Computer Physics Communications, 259:107676, 2021.\n\nDmitrii Kochkov, Jamie A Smith, Ayya Alieva, Qing Wang, Michael P Brenner, and Stephan Hoyer. Machine learning–accelerated computational fluid dynamics. Proceedings of the National Academy of Sciences, 118(21), 2021.\n\nSteven George Krantz and Harold R Parks. The implicit function theorem: history, theory, and\n\napplications. Springer Science & Business Media, 2002.\n\nPS Krsti ́c, X-G Zhang, and WH Butler. Generalized conductance formula for the multiband tight-\n\nbinding model. Physical Review B, 66(20):205319, 2002.\n\nAsk Hjorth Larsen, Jens Jørgen Mortensen, Jakob Blomqvist, Ivano E Castelli, Rune Christensen, Marcin Dułak, Jesper Friis, Michael N Groves, Bjørk Hammer, Cory Hargus, et al. The atomic simulation environment—a python library for working with atoms. Journal of Physics: Condensed Matter, 29(27):273002, 2017.\n\nKangyuan Li, Junqiang Lu, and Feng Zhai. Neural networks for modeling electron transport properties\n\nof mesoscopic systems. Physical Review B, 102(6):064205, 2020.\n\nLi Li, Stephan Hoyer, Ryan Pederson, Ruoxi Sun, Ekin D Cubuk, Patrick Riley, Kieron Burke, et al. Kohn-sham equations as regularizer: Building prior knowledge into machine-learned physics. Physical review letters, 126(3):036401, 2021.\n\nTzu-Mao Li, Miika Aittala, Fr ́edo Durand, and Jaakko Lehtinen. Differentiable monte carlo ray\n\ntracing through edge sampling. ACM Transactions on Graphics (TOG), 37(6):1–11, 2018.\n\nAlejandro Lopez-Bezanilla and O Anatole von Lilienfeld. Modeling electronic quantum transport\n\nwith machine learning. Physical Review B, 89(23):235411, 2014.\n\nMomchil Minkov, Ian AD Williamson, Lucio C Andreani, Dario Gerace, Beicheng Lou, Alex Y Song, Tyler W Hughes, and Shanhui Fan. Inverse design of photonic crystals through automatic differentiation. ACS Photonics, 7(7):1729–1741, 2020.\n\nFernando Nogueira. Bayesian Optimization: Open source constrained global optimization tool for\n\nPython, 2014–. URL https://github.com/fmfn/BayesianOptimization.\n\nTaisuke Ozaki. Continued fraction representation of the fermi-dirac function for large-scale electronic\n\nstructure calculations. Physical Review B, 75(3):035123, 2007.\n\nNick Papior, Nicol ́as Lorente, Thomas Frederiksen, Alberto Garc ́ıa, and Mads Brandbyge. Improvements on non-equilibrium and transport green function techniques: The next-generation transiesta. Computer Physics Communications, 212:8–24, 2017.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nFabijan Pavoˇsevi ́c and Sharon Hammes-Schiffer. Automatic differentiation for coupled cluster\n\nmethods. arXiv preprint arXiv:2011.11690, 2020.\n\nArtem K Pimachev and Sanghamitra Neogi. First-principles prediction of electronic transport in fabricated semiconductor heterostructures via physics-aware machine learning. npj Computational Materials, 7(1):1–12, 2021.\n\nR-E Plessix. A review of the adjoint-state method for computing the gradient of a functional with\n\ngeophysical applications. Geophysical Journal International, 167(2):495–503, 2006.\n\nLev Semenovich Pontryagin. Mathematical theory of optimal processes. CRC press, 1987.\n\nPramod Reddy, Sung-Yeon Jang, Rachel A Segalman, and Arun Majumdar. Thermoelectricity in\n\nmolecular junctions. Science, 315(5818):1568–1571, 2007.\n\nIvan Rungger and Stefano Sanvito. Algorithm for the construction of self-energies for electronic transport calculations based on singularity elimination and singular value decomposition. Physical Review B, 78(3):035407, 2008.\n\nMP Lopez Sancho, JM Lopez Sancho, JM Lopez Sancho, and J Rubio. Highly convergent schemes for the calculation of bulk and surface green functions. Journal of Physics F: Metal Physics, 15(4): 851, 1985.\n\nConnor Schenck and Dieter Fox. Spnets: Differentiable fluid dynamics for deep neural networks. In\n\nConference on Robot Learning, pp. 317–335. PMLR, 2018.\n\nSamuel S. Schoenholz and Ekin D. Cubuk. Jax m.d. a framework for differentiable physics. In Advances in Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2020.\n\nJohn C Slater and George F Koster. Simplified lcao method for the periodic potential problem.\n\nPhysical Review, 94(6):1498, 1954.\n\nSøren Smidstrup, Troels Markussen, Pieter Vancraeyveld, Jess Wellendorff, Julian Schneider, Tue Gunst, Brecht Verstichel, Daniele Stradi, Petr A Khomyakov, Ulrik G Vej-Hansen, et al. Quantumatk: an integrated platform of electronic and atomic-scale modelling tools. Journal of Physics: Condensed Matter, 32(1):015901, 2019.\n\nRyan Mohammad Solgi. Genetic algorithm package for python, 2020–. URL https://github.\n\ncom/rmsolgi/geneticalgorithm.\n\nSatofumi Souma and Matsuto Ogawa. Acceleration of nonequilibrium green’s function simulation for nanoscale fets by applying convolutional neural network model. IEICE Electronics Express, pp. 17–20190739, 2020.\n\nSatofumi Souma and Matsuto Ogawa. Neural network model for implementation of electron–phonon scattering in nanoscale device simulations based on negf method. In 2021 International Conference on Simulation of Semiconductor Processes and Devices (SISPAD), pp. 56–59. IEEE, 2021.\n\nSebastian Steiger, Michael Povolotskyi, Hong-Hyun Park, Tillmann Kubis, and Gerhard Klimeck. Nemo5: A parallel multiscale nanoelectronics modeling tool. IEEE Transactions on Nanotechnology, 10(6):1464–1474, 2011.\n\nA Svizhenko and MP Anantram. Effect of scattering and contacts on current and electrostatics in\n\ncarbon nanotubes. Physical Review B, 72(8):085430, 2005.\n\nTeresa Tamayo-Mendoza, Christoph Kreisbeck, Roland Lindh, and Al ́an Aspuru-Guzik. Automatic differentiation in quantum chemistry with applications to fully variational hartree–fock. ACS central science, 4(5):559–566, 2018.\n\nJeremy Taylor, Hong Guo, and Jian Wang. Ab initio modeling of quantum transport properties of\n\nmolecular electronic devices. Physical Review B, 63(24):245407, 2001.\n\nJ-S Wang, Jian Wang, and JT L ̈u. Quantum thermal transport in nanostructures. The European\n\nPhysical Journal B, 62(4):381–404, 2008.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nGuo-xun Wu, Zhen-qing Wang, Yu-hang Jing, and Chao-ying Wang.\n\nI–v curves of graphene nanoribbons under uniaxial compressive and tensile strain. Chemical Physics Letters, 559:82–87, 2013.\n\nMarkus Zahn. Point charge between two parallel grounded planes. American Journal of Physics, 44\n\n(11):1132–1134, 1976.\n\nT ˇZupanˇci ́c, I Stresec, and M Poljak. Predicting the transport properties of silicene nanoribbons using a neural network. In 2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO), pp. 44–48. IEEE.\n\nA ADDITIONAL DETAILS ON THE NEGF METHOD\n\nA.1 COMPUTATION OF THE SELF-CONSISTENT SURFACE GREEN FUNCTION\n\nSince the system is made up of a device and two semi-infinite contacts on the side, Equation (1) can be expanded in the following form:\n\n(cid:34) AL ALD\n\n0\n\nADL AD ADR ARD AR\n\n0\n\n(cid:35) (cid:34) GL GLD GLR GDL GD GDR GRL GRD GR\n\n(cid:35)\n\n= I,\n\n(10)\n\nwhere A = [EI − H], and the subscripts are used to distinguish the matrix elements corresponding to the left lead (L), the device (D), the right lead (R), and their interactions. Thanks to its block tri-diagonal form, the device Green function GD satisfies\n\n[AD − ADLA−1\n\nL ALD − ADRA−1\n\nR ARD]GD = I.\n\nSince AD = [EI − HD], compared with Equation (2), we have\n\nΣL = ADLA−1 ΣR = ADRA−1 Σ = ΣL + ΣR.\n\nL ALD, R ARD,\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\nHere we assume only the neighbouring layers have interactions with each other, and denote the left lead layer connected to the device by l. Then the left self-energy can be simplified as ΣL = ADlA−1 l AlD. The coupling matrix AlD is given as input of NEGF. What remains unclear is A−1 ,\nthe bottom-right block of A−1 L . This is known as the surface green function, denoted as gs. By utilizing the ideal lead assumption that removing one layer of the lead will not change gs, we obtain a self-consistent form as 3, where Lopez-Sancho algorithm (Sancho et al., 1985) can be applied to accelerate the convergence, here we display the detailed algorithm below:\n\nl\n\nAlgorithm 1 Lopez-Sancho algorithm for surface Green function\n\n0 = h0,0, (cid:15)0 = h0,0, α0 = h0,1 − ES0,1, β0 = h1,0 − ES1,0\n\nset (cid:15)s repeat i = (cid:15)s (cid:15)s (cid:15)i = (cid:15)i−1 + βi−1(ES − (cid:15)i−1)−1αi−1 + αi−1(ES − (cid:15)i−1)−1βi−1 αi = αi−1(ES − (cid:15)i−1)−1αi−1 βi = βi−1(ES − (cid:15)i−1)−1βi−1\n\ni−1 + αi−1(ES − (cid:15)i−1)−1βi−1,\n\nuntil converge g0,0 = (ES − (cid:15)s\n\nm)−1\n\nA.2 COMPUTATION OF THE SELF-CONSISTENT ELECTROSTATIC POTENTIAL\n\nDenote the charge densities in the equilibrium and non-equilibrium states as ρ0 and ρ, and the potential fields from the original neutral and redistributed charges as V0 and V . The equilibrium and non-equilibrium Hamiltonian can be expressed as H0 = T + V0, Hneq = T + V , where T is the kinetic energy. Poisson’s equation relates potentials to the corresponding charge densities:\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:26)∇ · (cid:15)(r)∇V (r) = −ρ(r),\n\n∇ · (cid:15)(r)∇V0(r) = −ρ0(r).\n\n(15)\n\nTherefore we have ∇ · (cid:15)(r)∇[∆V (r)] = −[ρ(r) − ρ0(r)], where ∆V = V − V0 is used to correct the Hamiltonian by Hneq = H0 + ∆V . The updated Hneq will again be used to update ∆V . Hence a self-consistent iteration is constructed:\n\n(cid:26)∇ · (cid:15)(r)∇[∆V (r)] = −[ρ(r; ∆V ) − ρ0(r)],\n\n∆V (r)|{zL,zR}\n\n= {VL, VR}.\n\n(16)\n\nCharge densities are necessary input for the above equation. Denote potentials in left and right (cid:82) +∞ electrodes as ul and ur (assume ul < ur), then the charge density ρ(r) = − i −∞ dEG(E), which can be decomposed into equilibrium and non-equilibrium terms:\n\n2π\n\nρ(r) = ρeq(r) + ρneq(r)\n\n=\n\n1 π\n\nIm\n\n(cid:20)(cid:90) ul\n\n−∞\n\n(cid:21)\n\ndEGD(E)\n\n+\n\n1 2π\n\n(cid:90) ur\n\nul\n\ndEGD(E).\n\n(17)\n\n(18)\n\nThe first integration up to infinity can be computed efficiently using contour integration with the residue theorem. It is achieved by expanding the Fermi-Dirac function (Ozaki, 2007; Areshkin & Nikoli ́c, 2010). On the other hand, the non-equilibrium charge density ρneq is computed directly by numerical integration. The density of neutral charges ρ0 can be computed by setting ul = ur = 0.\n\nA.3 EXPRESSIONS OF TRANSPORT PROPERTIES\n\nWith the NEGF theory, electronic transport properties can be derived, such as the transmission probability (T (E)), the density of states (DOS), the electronic current (I), the equilibrium and non-equilibrium electronic densities (ρeq and ρneq), etc. Here we list some of the expressions:\n\nT (E) = T race[ΓL(E)GD(E)ΓR(E)G†\n\nD(E)],\n\nDOS(E) = −\n\nT race[Im(GD(E))],\n\nI =\n\nρ(r) =\n\n(cid:90) +∞\n\n−∞\n\n(cid:20)(cid:90) ul\n\nIm\n\ndE 2π\n\nT (E)[f (E − ul) − f (E − ur)],\n\n(cid:21)\n\ndEGD(E)\n\n+\n\n1 2π\n\n(cid:90) ur\n\nul\n\ndEGD(E).\n\n−∞\n\n1 π\n2e ̄h 1\nπ\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\nFor Equation (21), the integral range of the current is decided by the subtraction of the Fermi-Dirac function, which is a little wider than (ul, ur).\n\nB ADDITIONAL DETAILS ON EXPERIMENTAL SETUP\n\nThe experiments are run on an Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz CPU, and an NVIDIA Tesla P40 GPU. We implemented our method in PyTorch 1.9.1. We validated the correctness of our simulation results by comparing with ASE of version 3.22.0.\n\nIn the experiments, we set the learning rate of the Adam optimizer as 0.001, and the batch size as 64. Bayesian optimization is implemented based on Nogueira (2014–), and the genetic algorithm is implemented based on Solgi (2020–). The bounds of the optimization variables for the black-box optimizers are (θ0 − 0.3, θ0 + 0.3), where θ0 is the initial value, namely the original 5-2 nano-junction TB Hamiltonian for the transmission curve fitting experiment, and undoped SKTB parameters for the device doping optimization experiment.\n\nThe hyper-parameters of the genetic algorithm are:\n\n{\n\n” m a x n u m i t e r a t i o n ” : None ,\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Forward simulation runtime cost against the system size.\n\n(b) Backward 1D derivative computation runtime cost against the system size.\n\n(c) Backward multi-dimensional derivative computation runtume cost against the system size.\n\nFigure 7: Runtime cost of forward and backward computation.\n\nr a t i o ” : 0 . 0 1 ,\n\n” p o p u l a t i o n s i z e ” : 2 0 , ” m u t a t i o n p r o b a b i l i t y ” : 0 . 1 , ” e l i t ” c r o s s o v e r p r o b a b i l i t y ” : 0 . 5 , ” p a r e n t s p o r t i o n ” : 0 . 3 , ” c r o s s o v e r ” m a x i t e r a t i o n w i t h o u t\n\n’ u n i f o r m ’ ,\n\nt y p e ” :\n\ni m p r o v ” : None\n\n}\n\nThe hyper-parameters of the Bayesian Optimization algorithm are:\n\n{\n\n}\n\n” r a n d o m s t a t e ” : 3 , ” v e r b o s e ” : 2 , ” k i n d ” : ” ucb ” , ” k a p p a ” : 2 . 5 ” x i ” : 0 . 0\n\nWe have uploaded our source code in the supplementary materials for cross-checking. The code will also be released and maintained as an open-source repository in the future.\n\nC SCALABILITY AND RUNTIME ANALYSIS OF AD AND FD\n\nIn this section, we present the test results and analysis of the scalability and runtime for both the forward simulation and the backward gradient calculation of AD-NEGF, compared against traditional numerical differentiation. The test system we use is AGNR. By controlling the length of AGNR, the number of atoms contained in the system changes accordingly, so that we can test the performances\n\n15\n\n20406080100120140160# atoms051015202530CPU timeRuntime of Forward Simulationw/ Op Trackingw/o Op Tracking20406080100120140160# atoms020406080100120140CPU timeRuntime of Backpropagation of 1D Seebeck CoefficientFDAD50100150200250300# atoms010203040CPU timeRuntime of Backpropagation for High-Dimentional HamiltoniansFDADUnder review as a conference paper at ICLR 2023\n\nwith respect to the system scale. The time cost is measured by the CPU time with functions provided by the python standard library.\n\nIn the forward simulation test, we compute the transmission function T (E) the same as in Section 5. Since a differentiable solver requires tracking each conducted operation in the automatic differentiation tool, which brings extra computational cost compared to conventional simulation programs, and turning off such operation tracking functionality will make the differentiable solver behave just like a traditional solver in forward simulation, we compare the CPU time cost for AGNR systems in different sizes, with both the operation tracking functionality turned on, and with the functionality turned off. The results are displayed in Figure 7(a), from which we can conclude that, the CPU time cost scales linearly with the system size, and the additional computational cost brought by operation tracking is almost negligible.\n\nIn the backpropagation test, we compare the runtime cost of calculating derivatives for both 1dimensional variables (the Seebeck coefficient) and multi-dimensional variables (device Hamiltonians). The results for both cases are shown in Figure 7(b) and 7(c) respectively. In the Seebeck coefficient calculation experiment, the derivative is computed as ∂T (E)/∂E, where we sample 100 energy points in each run. We can see the computational cost of both FD and AD scales linearly with the system size. Since the backpropagation computation is more complex than the forward computation, the slope of AD is a bit larger than FD. However, in such cases, efficiency will not be the bottleneck anyway, while as demonstrated in Section 5, AD can achieve higher precision than FD. For the high-dimensional variable gradient computation, as shown in Figure 7(c), we can see AD significantly outperforms FD in computing gradients for the device Hamiltonians of dimension N 2 A, where NA is the number of atom orbitals. This is because, FD can only compute the derivative for one dimension of the input variable each time, making its complexity grow in O(N 2 A). On the other hand, the complexity to compute gradients by backward-mode AD depends only on the complexity of the forward simulation, regardless of the increasing input variable dimensions.\n\nIn conclusion, by utilizing differentiable programming techniques, AD-NEGF, as an end-to-end differentiable quantum transport simulator, computes gradients for high-dimensional variables significantly faster than FD, while merely increasing the forward simulation cost.\n\n16",
    "reference": "# Summary Of The Paper\n\nThe paper proposes to use auto-diff to compute the gradients of the Non-Equilibrium Green Function (NEGF) to solve the quantum transport problem. Numerical differentiation methods are more expensive and can result in numerical errors that could lead to wrong estimations of the physical properties of the system. Moreover, while using Neural Networks as surrogate models is a possible path to achive faster solutions, but because they are trained from data, there's no guarantee that the derived results will be accurate. The paper moves on to briefly describe the NEFG method and specific techniques they employ to compute its derivatives. Finally, the AD-NEGF method is compared against numerical differentiation, showing the correctness of AD-NEGF on one side, while demonstrating the possible pitfalls of relying on numerical differentiation. In addition to establishing the basic benefits of the proposed method, additional experiments are carried out on other settings within this domain.\n\n# Strength And Weaknesses\n\nStrengths:\n* The first use of autodiff for solving the quantum transport problem that can alleviate many of the problems of earlier approaches.\n* Introducing a new topic for the ML community.\n\nWeaknesses:\n* The problem this paper aims to solve is too obscure for the ML audience. It would not be a problem if it were introduced more clearly in order to make it more accessible. As a reader with a bit of background in quantum physics but no familiarity with the quantum transport problem, I cannot say that I have a clear understanding of what exactly the authors are trying to solve.\n* The comparison to prior works is also a bit unclear. What exactly are black-box methods in this context? It would be useful to provide specific citations to what you are comparing against. Also, What is considered today to be the best performing method and how it compares? Finally, considering that one of the claimed benefits of your method is efficiency, comparing FLOPs or runtime would be crucial. (Beyond the time plot in Figure 6)\n* A more minor note is the overuse of initials. It's already hard to follow a new subject, but having so many unfamiliar intials makes reading the paper even harder.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: It might be that the paper would be clear to someone more familiar with the subject matter, but to the general ML audience or even those with limited familiarity with quantum physics that paper is difficult to read.\n\nQuality: The comparison to prior works seems lacking. It's not well communicated what are the main methods this work should be compared against and how well it performs.\n\nNovelty: To the extent of my limited understanding, it appears to be a novel application of auto-diff through a physical simulation to solve an intriguing problem. \n\nReproducibility: The code is provided, so it should probably be reproducible, but I didn't try running it.\n\n# Summary Of The Review\n\nMy main concern is that the paper might not be a good fit for the ICLR audience unless a major revision is provided to make it more accessible. While the proposed method belongs to the intersection of ML and Physics and so it could in principle find an audience in ICLR, more space should be spent on introducing the problem to those new to it.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSURGICAL FINE-TUNING IMPROVES ADAPTATION TO DISTRIBUTION SHIFTS\n\nYoonho Lee∗ Stanford University\n\nAnnie S. Chen∗ Stanford University\n\nFahim Tajwar Stanford University\n\nAnanya Kumar Stanford University\n\nHuaxiu Yao Stanford University\n\nPercy Liang Stanford University\n\nChelsea Finn Stanford University\n\nABSTRACT\n\nA common approach to transfer learning under distribution shift is to fine-tune the last few layers of a pre-trained model, preserving learned features while also adapting to the new task. This paper shows that in such settings, selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms commonly used fine-tuning approaches. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best. We validate our findings systematically across seven real-world data tasks spanning three types of distribution shifts. Theoretically, we prove that for two-layer neural networks in an idealized setting, first-layer tuning can outperform fine-tuning all layers. Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shift.\n\n1\n\nINTRODUCTION\n\nWhile deep neural networks have achieved impressive results in many domains, they are often brittle to even small distribution shifts between the source and target domains (Recht et al., 2019; Hendrycks & Dietterich, 2019; Koh et al., 2021). While many approaches to robustness attempt to directly generalize to the target distribution after training on source data (Peters et al., 2016; Arjovsky et al., 2019), an alternative approach is to fine-tune on a small amount of labeled target datapoints. Collecting such small labeled datasets can improve downstream performance in a cost-effective manner while substantially outperforming domain generalization and unsupervised adaptation methods (Rosenfeld et al., 2022; Kirichenko et al., 2022). We therefore focus on settings where we first train a model on a relatively large source dataset and then fine-tune the pre-trained model on a small target dataset, as a means of adapting to distribution shifts.\n\nThe motivation behind existing fine-tuning methods is to fit the new data while also preserving the information obtained during the pre-training phase. Such information preservation is critical for successful transfer learning, especially in scenarios where the source and target distributions share a lot of information despite the distribution shift. To reduce overfitting during fine-tuning, existing works have proposed using a smaller learning rate compared to initial pretraining (Kornblith et al., 2019; Li et al., 2020), freezing the early backbone layers and gradually unfreezing (Howard & Ruder, 2018; Mukherjee & Awadallah, 2019; Romero et al., 2020), or using a different learning rate for each layer (Ro & Choi, 2021; Shen et al., 2021).\n\nWe present a result in which preserving information in a non-standard way results in better performance. Contrary to conventional wisdom that one should fine-tune the last few layers to re-use the learned features, we observe that fine-tuning only the early layers of the network results in better performance on image corruption datasets such as CIFAR-10-C (Hendrycks & Dietterich, 2019). More specifically, as an initial finding, when transferring a model pretrained on CIFAR-10 to CIFAR-10-C by fine-tuning on a small amount of labeled corrupted images, fine-tuning only the first block of layers and freezing the others outperforms full fine-tuning on all parameters by almost 3% on average on unseen corrupted images.\n\n∗Equal contribution. Correspondence to yoonho@stanford.edu and asc8@stanford.edu.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Surgical fine-tuning, where we tune only one block of parameters and freeze the remaining parameters, outperforms full fine-tuning on a range of distribution shifts. Moreover, we find that tuning different blocks performs best for different types of distribution shifts. Fine-tuning the first block works best for input-level shifts such as CIFAR-C (image corruption), later blocks work best for feature-level shifts such as Entity-30 (shift in entity subgroup), and tuning the last layer works best for output-level shifts such as CelebA (spurious correlation between gender and hair color).\n\nTo better understand this counterintuitive result, we study a general class of fine-tuning algorithms which we call surgical fine-tuning, defined as fine-tuning only a small contiguous subset of all layers in the pre-trained neural network. Equivalently, we could define surgical fine-tuning as freezing all but a few layers during fine-tuning. Parameter freezing can be beneficial because, depending on the relationship between the source and target tasks, some layer parameters trained on the source task may be close to a minima for the target distribution. Therefore, freezing these layers can facilitate generalization to the target distribution. We evaluate the performance of surgical fine-tuning with various layer choices on 7 different distribution shift scenarios, which we categorize into input-level, feature-level, and output-level shifts. As shown in Figure 1, fine-tuning only the first block of layers, the middle block, or the last layer can perform best in different distribution shift conditions, with the best such subset consistently outperforming fine-tuning all parameters.\n\nTo support our empirical results, we theoretically analyze why different types of distribution shifts require fine-tuning different layers. For two-layer neural networks, we show why fine-tuning the first layer is better for input perturbations but fine-tuning the last layer is better for label perturbations. We then present a setting where surgical fine-tuning on the first layer provably outperforms fine-tuning all parameters. If the target distribution contains only a few new “directions” (inputs outside the span of the source distribution), we show that tuning only the first layer can learn these new directions with very few target examples, while preserving all the information learned from the source distribution. However, we show that full fine-tuning forgets information learned from the source distribution—the last layer changes to accommodate the new target directions, but now performs poorly on examples outside the span of the training data. Motivated by the theoretical insight that freezing some layers can help generalization, we empirically analyze two criteria for automatically selecting layers to tune based on loss gradients. Tuning the layers selected by such criteria can also outperform full fine-tuning, though this procedure does not outperform manually choosing the best layers to tune.\n\nOur main contribution is the empirical observation that fine-tuning only a small contiguous subset of layers can outperform full fine-tuning on a range of distribution shifts. Intriguingly, the best layers to tune differ for different distribution shift types (Figure 1). This finding is validated empirically across seven real-world datasets and three types of distribution shifts, and theoretically in an idealized two-layer neural network setup. We additionally empirically analyze two criteria for automatically selecting which layers to tune and find that fine-tuning only the layers with higher relative gradient norm outperforms full fine-tuning.\n\n2 SURGICAL FINE-TUNING: FREEZING PARAMETERS DURING ADAPTATION\n\nOur problem setting assumes two datasets from different distributions: a large dataset following the source distribution Psrc, and a relatively smaller dataset following the target distribution Ptgt. The objective is to achieve high accuracy on target data by leveraging the different but closely related source distribution, a common scenario in real-world applications that require adaptation. For example, the source dataset can be the 50, 000 training images in CIFAR-10 (Krizhevsky et al., 2009) while the target dataset is a smaller set of 1000 corrupted CIFAR datapoints with the same image corruption (Hendrycks & Dietterich, 2019); see Figure 1 for more examples of source-target\n\n2\n\nInput-Level Shift: CIFAR-CFirst blockMiddle/later blockLast layerSource dataTarget dataFeature-Level Shift: Entity-30Output-Level Shift: CelebAFull Fine-TuningSurgical Fine-Tuning82.8% 79.9%(+2.9)(+2.1)(+4.0)81.2%79.3%86.2%82.2%Subgroup shiftSpurious correlationImage corruptionPublished as a conference paper at ICLR 2023\n\ndataset pairs that we consider. To achieve high performance on the target distribution, a model should broadly fit the large source dataset and make minor adjustments based on the smaller target dataset.\n\nWe empirically evaluate transfer learning performance with a two-stage training procedure consisting of pre-training and fine-tuning. First, we pre-train a network to minimize the loss on the source dataset to obtain fsrc, which has high accuracy in the source distribution. The fine-tuning stage starts from pre-trained model parameters and minimizes the loss on the labeled target data, resulting in the model f tgt. We evaluate two fine-tuning settings in this section: supervised fine-tuning (Section 2.1) and unsupervised adaptation (Section 2.2). In all experiments, we perform early stopping on held-out target data according to the fine-tuning loss. Finally, we evaluate the performance of the fine-tuned model on held-out data from the target distribution, i.e. Ltgt(f tgt) = E(x,y)∼Ptgt[l(f tgt(x), y)]. Our main focus is analyzing surgical fine-tuning, in which we fine-tune only a subset of layers of the pre-trained model while keeping the others frozen. Denote the pre-trained model as f = fn ◦ . . . ◦ f1(x), where each layer fi has parameters θi, and the empirical target loss as (cid:98)Ltgt. Formally, surgical fine-tuning with respect to a subset S ⊆ {1, . . . , n} of layers is defined as solving the optimization problem\n\narg min θi ∀i∈S\n\n(cid:98)Ltgt(f (θ1, . . . , θn)),\n\n(1)\n\nwhere all non-surgery parameters (θi for i /∈ S) are fixed to their pre-trained values. Typical choices of parameters to optimize are fine-tuning all (S = {1, . . . , n}), last (S = {n}), or the last few layers (S = {n − k, · · · , n}). The main novelty of the surgical fine-tuning framework is that it additionally considers tuning earlier layers while keeping later layers frozen. For example, surgical fine-tuning on the first layer (S = {1}) updates only θ1, resulting in the fine-tuned model f tgt(x) = f src\n\nn ◦ . . . ◦ f src\n\n2 ◦ f tgt\n\n1 (x).\n\n1 such that changing only the first layer of fsrc to θ∗\n\nIntuitively, surgical fine-tuning can outperform full fine-tuning when some layers in f src are already near-optimal for the target distribution. As a hypothetical example, consider a scenario where there exist first-layer parameters θ∗ 1 achieves zero target loss. Here, first-layer fine-tuning (S = {1}) can find θ∗ 1 with a small amount of target data, while full fine-tuning (S = {1, . . . , n}) may needlessly update the other layers and thus underperform on held-out target data due to overfitting. We note that the efficacy of parameter freezing is a consequence of having limited target data, and choosing a bigger S will be beneficial in settings where target data is plentiful. Now that we have introduced the problem set-up, we will next empirically investigate how surgical fine-tuning with different choices of S performs on real datasets.\n\n2.1 SURGICAL FINE-TUNING: EXPERIMENTS ON REAL DATA\n\nIn this subsection, we aim to empirically answer the following question: how does surgical parameter fine-tuning compare to full fine-tuning in terms of sample efficiency and performance on real-world datasets?\n\nDatasets. We run experiments on nine real-world distribution shifts, categorized into input-level, feature-level, output-level, and natural shifts, with examples shown in Figure 1. For more details about these datasets, see Appendix B.3.\n\n• Input-level shift: (1) CIFAR-C (Hendrycks & Dietterich, 2019), (2) ImageNet-C (Kar et al., 2022). The source distributions correspond to the original CIFAR-10 and ImageNet datasets (Krizhevsky et al., 2009; Deng et al., 2009), respectively. The task is to classify images from the target datasets, which consist of corrupted images.\n\n• Feature-level shift: (3) Living-17 and (4) Entity-30 (Santurkar et al., 2020): While the source and target distributions consist of the same classes, they contain different subpopulations of those classes. For example, in Entity-30, for the class “vegetables”, Psrc and Ptgt will contain different subclasses of vegetables.\n\n• Output-level shift: (5) CIFAR-Flip, (6) Waterbirds, and (7) CelebA (Sagawa et al., 2019). CIFAR-Flip is a synthetic task where the Psrc consists of the original CIFAR-10 dataset and the target distribution is the same dataset where each label y has been flipped to be 9 − y, e.g. the label 0 is now label 9 and vice versa. For Waterbirds and CelebA, the task labels are spuriously correlated with an attribute. The source distribution Psrc is the training set while the target distribution Ptgt is a balanced subset with equal amounts of each of the four (label, spurious attribute) groups.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: We present plots of relative accuracy, i.e. (surgical fine-tuning accuracy) - (full fine-tuning accuracy), along with standard errors across three runs. Fine-tuning a single parameter block can outperform full fine-tuning, and more importantly, different blocks are best for different distribution shifts. The location of the best block reflects the nature of the shift: tuning earlier layers performs best for input-level shifts while tuning later layers is best for output-level shifts. • Natural shift: (8) Camelyon17 (Bandi et al., 2018) and (9) FMoW (Christie et al., 2018), part of the WILDS (Koh et al., 2021) benchmark. Camelyon17 contains medical images from different hospitals where variations in data collection and processing produces naturally occurring distribution shifts across data from different hospitals. FMoW contains satellite imagery of 62 types of buildings or land use, with both temporal and sub-population shifts resulting from geographical diversity.\n\nModel architectures and pre-training. For each task, before pre-training on Psrc, we initialize with a model with ResNet-50 architecture pre-trained on ImageNet, except for experiments with CIFAR-C and CIFAR-Flip, which both use a ResNet-26 trained from scratch, and experiments on Camelyon17 and FMoW, which use a pre-trained CLIP ViT-B/16. After initialization, we pre-train on the source domain Psrc and then fine-tune on a small amount of data from the target domain. We fine-tune with the Adam optimizer, sweeping over 3 learning rates. We choose the best hyperpameters and early stop based on accuracy on held-out target data. We report results across 3 seeds for all experiments. See Appendix B.4 for more fine-tuning details.\n\nSurgical fine-tuning. The models used consist of three (for ResNet-26) or four (for ResNet-50) convolutional blocks followed by a final fully connected layer. We denote these blocks as \"Block 1\", \"Block 2\", etc in the order that they process the input, and the fully connected layer as \"Last Layer\". CLIP ViT-B/16 has 1 embedding layer, 11 attention blocks and a linear classifier as the last layer. For each experimental setting, we report the relative target distribution accuracy and standard error across three runs after surgical fine-tuning on each block of the network, fine-tuning only that block while freezing all other parameters. We compare against full fine-tuning, i.e. tuning all parameters to minimize target loss.\n\n86.2\n\nFMoW\n\nParameters\n\nCamelyon17\n\nNo fine-tuning\n\nExperimental results. Results in Figure 2 show that on every domain, surgically fine-tuning one block of the network outperforms tuning all parameters on the target distribution. We note that even matching full fine-tuning performance with surgical fine-tuning would indicate that ignoring some gradients is harmless; these results show that ignoring some gradients has a positive effect. Furthermore, we find that the best block to fine-tune is different across settings, depending on the nature of the distribution shift between source and target data. Datasets with an input-level shift are best handled by tuning the first network block, and similarly for feature-level shifts with a middle block and output-level shifts with the last layer. We see the a similar phenomenon in the natural shifts: Camelyon17 is closer to an input-level shift due to the lighting difference in different hospitals, while the shift between different regions in FMoW can be seen as close to a feature-level shift because building shape and spacing is most salient in satellite imagery. Quantitative results in in Table 1 are in agreement with this intuition, where tuning the earliest embedding layer works best for Camelyon17 and tuning later attention blocks works best for FMoW. Following Kumar et al. (2022b), we also evaluate fine-tuning performance with the AdamW optimizer; results in Table 8 show a similar tendency but with smaller performance gaps. In Figure 4, we find that on CIFAR-C, fine-tuning the first block matches and even outperforms full fine-tuning as well as tuning with other individual blocks when given varying amounts of data for tuning, although the gap between Block 1 and All decreases as the number of training points increases.\n\nTable 1: OOD set accuracies after surgically fine-tuning different parameters in a CLIP ViT-B/16 model for two WILDS datasets. Bold numbers represent superior results for a dataset, and we also report the standard deviation from runs with 3 different seeds.\n\nAll Embedding First three Last three Last layer\n\n92.3 (1.7) 95.6 (0.4) 92.5 (0.5) 87.5 (4.1) 90.1 (1.5)\n\n38.9 (0.5) 36.0 (0.1) 39.8 (1.0) 44.9 (2.6) 36.9 (5.5)\n\n35.5\n\n4\n\nCIFAR-CImageNet-CLiving-17Entity-30CIFAR-FlipWaterbirdsCelebA7.55.02.50.02.55.07.5Relative Accuracy (%)Input-level shiftsFeature-level shiftsOutput-level shiftsSurgical Fine-Tuning on Different Distribution ShiftsBlock 1Block 2Block 3Block 4Last LayerPublished as a conference paper at ICLR 2023\n\nParameters\n\nEpisodic Online\n\nNo adaptation\n\nAll\n\nLayer 1 Layer 1-2 Block 1-2 Last\n\n67.8\n\n71.7\n\n69.0 69.0 69.0 67.9\n\n-\n\n69.7\n\n75.4 75.5 75.3 67.8\n\nTable 2: Unsupervised adaptation accuracy on CIFAR-10-C, averaged across 10 corruptions.\n\nParameters\n\nEpisodic Online\n\nNo adaptation\n\nAll\n\nLayer 1 Block 1 Last\n\n46.5\n\n47.8\n\n46.6 46.7 46.5\n\n-\n\n1.8\n\n46.4 49.0 46.5\n\nTable 3: Unsupervised adaptation accuracy on ImageNet-C, averaged across 14 corruptions.\n\nFigure 3: Surgical fine-tuning results on CIFAR-10-C with varying amounts of target data. Fine-tuning the early layers is beneficial even in the fewshot setting, given as few as 1 image per class for tuning. Error bars indicate the average standard error over 14 corruptions.\n\nFigure 4: Online unsupervised adaptation with surgical fine-tuning for the Gaussian corruption in the CIFAR-C dataset. Adding data in an online fashion to full or last-layer tuning results in worse performance, whereas more data helps for firstlayer adaptation. Error bars indicate the standard error across three runs.\n\nIntuitively, why might surgical fine-tuning match or even outperform full fine-tuning on distribution shifts? For each type of shift we consider (input-level, feature-level, and output-level), there is a sense in which one aspect of the distribution changes while everything else is kept the same, therefore requiring modification of only a small part of information learned during pre-training. For example, in image corruptions (categorized as an input-level shift), pixel-wise local features are shifted while the underlying structure of the data is the same in the source and target distributions. On the other hand, in a label shift (categorized as an output-level shift), the pixel-wise features remain the same in the source and target distributions while the mapping from final features to labels is shifted. This intuition is also in line with the independent causal mechanisms (ICM) principle (Schölkopf et al., 2012; Peters et al., 2017), which states that the causal generative process of a system’s variables is composed of autonomous modules that do not inform or influence one another. From this viewpoint, distribution shifts should correspond to local changes in the causal generative process. Because discriminative models learn to invert the generative process from label to datapoint, it suffices to fine-tune only the region of the network that corresponds to the change in the causal process. We formalize this intuition more concretely in our theoretical analysis in Section 3.\n\n2.2 UNSUPERVISED ADAPTATION WITH PARAMETER FREEZING\n\nIn this subsection, we aim to validate whether the findings from Section 2.1 hold in the unsupervised test-time adaptation setting, where we adapt a model trained on source data to the target distribution using only unlabeled target data. We experiment with variants of a representative state-of-the-art unsupervised adaptation method (Zhang et al., 2021a, MEMO), which minimizes marginal entropy of averaged predictions for a single image. We consider two settings: online, where the model retains updates from past test images, and episodic, where we reset the model back to the pre-trained weights after every test image.\n\nResults in Table 2 and Table 3 show that the highest accuracy is achieved by adapting the first two layers and first block in the online setting for CIFAR-10-C and ImageNet-C respectively, and doing so outperforms fine-tuning all parameters. With full fine-tuning, online MEMO performance deteriorates as the test set size increases due to distortion of pre-trained features, as shown graphically in Figure 4. In contrast, surgical fine-tuning mitigates this effect. These results are consistent with the supervised learning experiments in Section 2.1, where adapting the early parameters was best for image corruption datasets. We show detailed results in Appendix B.6.\n\n3 ANALYSIS OF SURGICAL FINE-TUNING\n\nWe now present a theoretical and empirical analysis on idealized examples of distribution shifts, to better understand the role of surgical parameter tuning in our previous experimental results. In Section 3.1, we present a setting with two-layer neural networks where tuning only the first layer can obtain zero loss on the target task while tuning only the last layer cannot and vice versa. Then, in Section 3.2, we study a setting in which tuning only the first layer provably achieves zero loss while full fine-tuning overfits and gets non-zero loss due to limited data. Finally, in Section 3.3, to support this theoretical analysis, we construct distribution shifts where localized subsets of parameters are substantially better suited for adaptation than tuning all or other parameters.\n\n5\n\n101102103104Target Set Size5560657075808590Target Accuracy (%)CIFAR-C SupervisedBlock 1Block 2Block 3Last LayerAll102103104Target Set Size50525456586062646668Target Accuracy (%)CIFAR-C UnsupervisedLayer 1Layer 1-2Last LayerAllPublished as a conference paper at ICLR 2023\n\nTheoretical setup. We focus on regression, where our goal is to map inputs x ∈ Rd to outputs y ∈ R, and l(y, ˆy) = (y − ˆy)2 is the squared loss. We consider two-layer networks fv,B(x) = v⊤φ(Bx) where v ∈ Rk, B ∈ Rk×d, and φ is an elementwise activation function such as ReLU. Let xsrc, ysrc ∼ Psrc and xtrg, ytrg ∼ Ptrg be the inputs and outputs in the source and target distributions. We assume ysrc = fvsrc,Bsrc(xsrc) for some vsrc, Bsrc. Note that xsrc, ysrc, xtrg, ytrg are all random variables, and expectations are taken over all random variables if not specified. We define the population losses for source and target as Lsrc(v, B) = E(cid:2)l(fv,B(xsrc), ysrc)(cid:3) and Ltrg(v, B) = E(cid:2)l(fv,B(xtrg), ytrg)(cid:3).\n\n3.1 LAYER CHOICE AND EXPRESSIVITY: WHY FINE-TUNING THE RIGHT LAYER MATTERS\n\nFirst note that for two-layer neural networks, we have two choices for surgical fine-tuning: the first layer and the last layer. We show by construction that if the distribution shift is closer to the input then first-layer tuning is better, but if the shift is closer to the output then last-layer tuning is better. In this section, we assume that φ is the elementwise ReLU function: φ(x)i = max(xi, 0). Recall that we first train on lots of source data—suppose this gives us pretrained parameters ˆvsrc, ˆBsrc which achieve minimum source loss: Lsrc(ˆvsrc, ˆBsrc) = 0.\n\nInput perturbation. Suppose that the target input is a “perturbed” or “corrupted” version of the source input: xtrg = Axsrc for some invertible matrix A ∈ Rn×n, where the corresponding label is unchanged: ytrg = ysrc. We note that this simplified perturbation class includes some common image corruptions as brightness shift and Gaussian blur as special cases, while others such as pixelation are similarly linear projections but non-invertible. Proposition 1 shows that for this distribution shift, tuning only the first layer can minimize the target loss but only changing the last layer may not.\n\nProposition 1. For all A, Psrc, Ptrg with xtrg = Axsrc for invertible A and ytrg = ysrc, there exists a first-layer B that can minimize the target loss: minB Ltrg(ˆvsrc, B) = 0. However, changing the last layer may not be sufficient: there exists such A, Psrc, Ptrg such that the target loss is non-zero for any choice of last layer v: for all i, minv Ltrg(v, ˆBsrc) > 0.\n\nIntuitively, the first-layer can learn to “undo” the perturbation by selecting B = A−1. However, if we freeze the first-layer then the representations φ( ˆBsrcxtrg) may miss important input directions in the target, so no last-layer v can produce the correct output. For a full statement and proof, see Appendix B.1.\n\nLabel perturbation. Now suppose that the source and target inputs are the same xtrg = xsrc, but the target output is perturbed from the source output: ytrg = tysrc for some t. Proposition 2 shows that tuning only the first layer may not achieve non-zero target loss for this distribution shift while tuning the last layer will do so.\n\nProposition 2. For all t, Psrc, Ptrg with xtrg = xsrc, and ytrg = tysrc, there exists a last-layer v that can minimize the target loss: minv Ltrg(v, ˆBsrc) = 0. However, changing the first layer may not be sufficient—there exists such t, Psrc, Ptrg such that the target loss is non-zero for any choice of first layer B: minB Ltrg(ˆvsrc, B) = 0\n\nSimilarly to Proposition 1, the last layer can adapt to the label shift by “reversing” the multiplication by t. In contrast, when the last layer is frozen and only the first layer is tuned, we may lack expressivity due to the information destroyed by the ReLU activation φ(·). For a full statement and proof, see Appendix B.1.\n\n3.2 CAN SURGICAL FINE-TUNING OUTPERFORM FULL FINE-TUNING?\n\nIn this section, we show that first-layer fine-tuning can provably outperform full fine-tuning when we have an insufficient amount of target data. We show that this can happen even in tuning two-layer linear networks (Kumar et al., 2022a), where φ is the identity map: for all i, φ(x)i = xi. Our analysis suggests perhaps a more general principle underlying the benefits of surgical fine-tuning over full fine-tuning: by fine-tuning more parameters than necessary, the model can overfit to the small target dataset while forgetting relevant information learned during pre-training.\n\nPretraining. We first start with v0, B0 which are initialized randomly: v0i ∼ N (0, σ2 B0ij ∼ N (0, σ2 we assume minimizes the source loss: Lsrc(ˆvsrc, ˆBsrc) = 0.\n\nv) and B) for all i, j. We then run gradient descent on Lsrc(v, B) to obtain ˆvsrc, ˆBsrc, which\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: In this setting, we construct distribution shifts such that a particular block of parameters is substantially more suited for adaptation. We find that tuning only the subset of parameters that is responsible for the shift performs better than tuning any other block of parameters or all parameters. Darker blue indicates higher accuracy while darker red indicates lower accuracy.\n\nFine-tuning data. Suppose we have n target datapoints sampled from Ptrg: (x(1) (x(n)\n\ntrg ). The empirical fine-tuning loss is given by: ˆLtrg(v, B) = (cid:80)n\n\ni=1 l(fv,B(x(i)\n\ntrg , y(n)\n\ntrg , y(1) trg).\n\ntrg), y(i)\n\ntrg ) . . .,\n\nFine-tuning algorithms. We study two different gradient flows each corresponding to fine-tuning methods: first-layer tuning (fl) and full fine-tuning (ft).\n\n∂tBfl(t) = −∇B ˆLtrg (vfl(t), Bfl(t)) ∂tBft(t) = −∇B ˆLtrg (vft(t), Bft(t)) ∂tvft(t) = −∇v ˆLtrg (vft(t), Bft(t)) , with initial conditions vfl(0) = vft(0) = ˆvsrc and Bfl(0) = Bft(0) = ˆBsrc. We denote the limit points of these gradient flows as v∞\n\n∂tvfl(t) = 0\n\nfl = limt→∞ vft(t), etc.\n\nThe following theorem shows that there exists a shift, where if we have a small target dataset, full fine-tuning does worse than first-layer tuning. Theorem 1. For any δ > 0, there exists d, k, Psrc, Ptrg, n such that with probability at least 1 − δ, first-layer tuning gets 0 loss at convergence, but full fine-tuning gets higher (non-zero) loss throughout the fine-tuning trajectory:\n\nLtrg(vft(t), Bft(t)) > Ltrg(v∞\n\nfl , B∞\n\nfl ) = 0 ∀t.\n\n(2)\n\nIntuitively, if Pood contains a few additional directions in the input that are not present in Pid, then first layer-tuning can quickly learn those new directions. Full fine-tuning changes both the head v and feature extractor B to fit these new directions—however, because the head v has changed it may be incompatible with B in some directions not seen in the finite training set, thus “forgetting” some knowledge present in the source data. The full proof is in Appendix B.2.\n\n3.3 SURGICAL FINE-TUNING ON SYNTHETIC DISTRIBUTION SHIFTS\n\nTo better illustrate how specific subsets of parameters are better suited depending on the distribution shift, we model distribution shifts by adding noise to individual blocks of layers. More specifically, we initialize with a ResNet-26 model pretrained on the CIFAR-10 (Krizhevsky et al., 2009) training dataset. We then add noise to each of the three blocks or the last layer, simulating distribution shifts localized to those parameters, and then tune each of the different blocks of the network while freezing all other parameters on the CIFAR-10 test dataset.\n\nIn Figure 5, we find that only tuning the subset of parameters that is responsible for the shift performs better than tuning any other subset of parameters and even outperforms tuning all layers, indicating that when tuning the parameters responsible for the shift, tuning other parameters may actually hurt performance.\n\n4 AUTOMATICALLY SELECTING WHICH LAYERS TO TUNE\n\nIn this section, we investigate three criteria for automatically finding an adequate subset of layers to perform surgical fine-tuning on. We evaluate their fine-tuning performance versus full fine-tuning and another prior method on the 7 real-data domains introduced in Section 2.1. We also analyze performance on the synthetic distribution shifts introduced in Section 3.3.\n\n4.1 CRITERIA FOR SELECTING LAYERS\n\nWe consider three metrics for automatically choosing which layers to freeze.\n\nCross-Val. After running surgical fine-tuning for all blocks, we select the best block based on a held-out validation set from the target distribution. While quite effective, this method requires as many fine-tuning runs as there are blocks inside the network.\n\n7\n\nAdded NoiseFirst blockSecond blockThird blockLast layerTuned LayersFirst block92.5 (0.3)27.7 (1.2)27.2 (1.7)37.4 (9.0)Second block68.8 (2.3)92.6 (0.3)46.6 (2.6)61.0 (4.4)Third block48.0 (1.3)59.7 (0.6)92.1 (0.2)92.0 (0.5)Last layer33.7 (0.4)34.8 (0.4)62.0 (1.1)91.8 (0.6)All87.8 (1.2)91.9 (0.2)92.0 (0.1)91.3 (0.3)Published as a conference paper at ICLR 2023\n\nMethod\n\nInput-level Shifts IN-C\n\nCIFAR-C\n\nFeature-level Shifts\n\nOutput-level Shifts\n\nLiving-17\n\nEntity-30\n\nCIFAR-F Waterbirds\n\nCelebA\n\nNo Adaptation\n\n52.6 (0)\n\n18.2 (0)\n\n80.7 (1.8)\n\n58.6 (1.1)\n\n0 (0)\n\n31.7 (0.3)\n\n27.8 (1.9)\n\nCross-Val\n\n82.8 (0.6)\n\n51.6 (0.1)\n\n93.2 (0.3)\n\n81.2 (0.6)\n\n93.8 (0.1)\n\n89.9 (1.2)\n\n86.2 (0.8)\n\nFull Fine-Tuning (All) Gradual Unfreeze (First → Last) Gradual Unfreeze (Last → First) L1 Regularize (Xuhong et al., 2018) Auto-SNR Auto-RGN\n\n79.9 (0.7) 80.5 (0.7) 78.8 (0.8) 81.7 (0.6) 80.9 (0.7) 81.4 (0.6)\n\n50.7 (0.1) 50.7 (0.1) 49.8 (0.2) 48.8 (0.3) 49.9 (0.2) 51.2 (0.2)\n\n92.8 (0.7) 90.1 (0.1) 90.6 (0.2) 93.4 (0.5) 93.5 (0.2) 93.5 (0.3)\n\n79.3 (0.6) 69.9 (3.0) 77.8 (0.5) 78.4 (0.1) 77.3 (0.3) 80.6 (1.2)\n\n85.9 (0.4) 81.9 (0.6) 84.3 (0.9) 84.2 (1.2) 17.3 (0.7) 87.7 (2.8)\n\n88.0 (1.2) 87.5 (0.2) 87.8 (0.1) 87.6 (1.9) 86.3 (0.7) 88.0 (0.7)\n\n82.2 (1.3) 71.9 (0.7) 78.5 (2.9) 82.6 (1.8) 78.5 (1.8) 82.2 (2.7)\n\nAvg Rank\n\n-\n\n-\n\n2.71 4.71 4.0 3.28 4.14 1.29\n\nTable 4: We report the average accuracy and standard error achieved on the target distribution on 7 real-data tasks. Cross-Val, which requires as a surgical fine-tuning run for each block, performs the best, but we find that Auto-RGN performs the best out of all methods that require only 1 fine-tuning run, outperforming Full Fine-tuning, Gradual Unfreezing, L1 Regularize, and Auto-SNR. The best overall method for each shift is underlined, and the best among methods that use 1 fine-tuning run is bolded.\n\nRelative Gradient Norm (Auto-RGN). Within each layer, we measure the ratio of gradient norm to parameter norm, and select layers that have relatively larger gradients. Intuitively, our hypothesis is that layers with large gradient magnitudes may carry more information about the target task than others and can therefore be more useful. Formally, denote gradients at layer i as gi. We define the relative gradient norm of this layer as RGN(θi) = (gi) ||θi|| . Then to alter fine-tuning with this criterion, at each epoch, we normalize the RGNs for each layer between 0 and 1 and then multiply the learning rate for each layer by its RGN. Using this criterion for fine-tuning requires no additional hyperparameters over tuning all layers and only one fine-tuning run.\n\nSignal-to-Noise Ratio (Auto-SNR). For each layer i, this criterion is defined as SNR(gi) = Avg(gi)2 Var(gi) , with average and variance computed across (target) datapoints. Intuitively, SNR measures how noisy the gradient of each layer is and thus how much it may contribute to distorting the function learned during pre-training. This gradient-based criterion has been shown to be useful for early stopping (Mahsereci et al., 2017). During fine-tuning, we normalize the SNR for each layer between 0 and 1 and then freeze all layers that have SNR under a threshold that is tuned as an additional hyperparameter.\n\nAs points of comparison, we compare the three criteria for layer selection above to existing methods for regularizing fine-tuning. We consider two variations of gradual unfreezing: Gradual Unfreeze (First → Last) and Gradual Unfreeze (Last → First) (Howard & Ruder, 2018; Romero et al., 2020; Kumar et al., 2022a), in addition to L1 Regularize (Xuhong et al., 2018). These methods are similar in spirit to surgical fine-tuning in that they aim to minimize changes to parameters. We additionally experimented with regularizing the L2 norm, but found that L1 consistently performs better.\n\n4.2 RESULTS ON REAL WORLD DATASETS\n\nIn Table 4, we compare Cross-Val with 6 methods (Full Fine-Tuning, Gradual Unfreeze (First → Last), Gradual Unfreeze (Last → First), L1 Regularize, Auto-SNR, and Auto-RGN) that require only 1 fine-tuning run. We find that auto-tuning with relative grad norm (Auto-RGN) matches or outperforms fine-tuning all parameters on all domains and is the most competitive method that requires only 1 fine-tuning run although it does not quite match the performance of Cross-Val. We find that Cross-Val corresponds in performance to the best surgical fine-tuning result for each dataset, which is expected, as the validation and test sets are both held-out subsets of the same target distribution. Auto-SNR struggles to extract the most effective layers for tuning and hence does worse on most shifts than All and Auto-RGN. While Gradual Unfreeze fails to consistently outperform full fine-tuning, the directionality of results is consistent with surgical fine-tuning: unfreezing first layers is best in input-level shifts and unfreezing last layers is best in output-level shifts. L1 Regularize performs slightly better than fine-tuning all, but performs worse than Auto-RGN on all datasets except CIFAR-C. All methods outperform no adaptation.\n\n4.3 AUTOMATIC SELECTIVE FINE-TUNING IN SYNTHETIC DISTRIBUTION SHIFTS\n\nAs Auto-RGN is the best performing method that requires only one fine-tuning run, and in particular outperforms fine-tuning all layers, we further analyze what layers Auto-RGN chooses to fine-tune and see to what extent they correlate with our experiments in Section 2. To do so, we evaluate Auto-RGN on the synthetic distribution shifts introduced in Section 3.3, where we model distribution shifts by adding noise to blocks of parameters, and plot the weights that Auto-RGN gives to the layers.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTuned Layers\n\nFull Fine-Tuning Auto-RGN\n\nBlock 1 Block 2 Block 3 Last Layer\n\n87.8 (1.2) 91.9 (0.2) 92.0 (0.1) 91.3 (0.3)\n\n92.0 (0.4) 92.7 (0.4) 92.5 (0.4) 92.8 (0.3)\n\nTable 5: Automatically choosing which layers to tune using the relative gradient norms (RGN) of each layer outperforms full fine-tuning on distribution shifts constructed by adding noise to different blocks of layers.\n\nFigure 6: Auto-RGN consistently gives higher weights for the layers in the block responsible for the distribution shift than for the other layers.\n\nWe find that Auto-RGN is able to ascertain which parameters may be responsible for the shift and weight the learning of those parameters to be higher than the others, resulting in an informative signal that matches the performance of tuning only the noisy subset of parameters and outperforms full fine-tuning, as seen in Table 5. Figure 6 shows the accumulated weights given by Auto-RGN over the course of training for each layer, colored by block. The weights for the layers responsible for the distribution shifts are higher than the weights for the other layers.\n\n5 RELATED WORK\n\nParameter freezing. Freezing parameters to preserve previously learned information has been shown to be an effective strategy in a diverse set of domains: domain adaptation (Sener et al., 2016; Long et al., 2016), early stopping (Mahsereci et al., 2017), generative models (Mo et al., 2020), and gradient-based meta-learning (Zintgraf et al., 2019; Raghu et al., 2019; Triantafillou et al., 2021), A highly effective approach to fast adaptation of large language models is prompt tuning (Li & Liang, 2021; Hambardzumyan et al., 2021; Lester et al., 2021; Wei et al., 2021), which can similarly be seen as an extreme special case of freezing where we only fine-tune the inputs to the neural network. Our surgical fine-tuning framework contains many such previous works as special cases, and our experiments highlight the value of carefully choosing the subset of parameters to freeze.\n\nTransfer learning. Prior works in transfer learning have studied how fine-tuning may be used to adapt pretrained features to a target distribution (Oquab et al., 2014; Yosinski et al., 2014; Sharif Razavian et al., 2014). To preserve information obtained during pre-training, many works propose methods of regularizing the fine-tuning process (Zhang et al., 2020; Xuhong et al., 2018; Lee et al., 2019a; Jiang et al., 2019; Li et al., 2020; Aghajanyan et al., 2020; Gouk et al., 2021; Shen et al., 2021; Karani et al., 2021). In particular, many works show that freezing some parameters in the pre-trained model can reduce overfitting during fine-tuning (Kirkpatrick et al., 2017; Lee et al., 2019b; Guo et al., 2019; Ramasesh et al., 2020; Liu et al., 2021b; Royer & Lampert, 2020; Eastwood et al., 2021; Evci et al., 2022; Eastwood et al., 2022; Cohen et al., 2022; Touvron et al., 2022), and we build on such observations. Module criticality (Zhang et al., 2019; Chatterji et al., 2019; Neyshabur et al., 2020), which independently examines each layers’ loss surface, is also closely related to our analysis. In contrast to existing works, we make the counterintuitive observation that freezing the later layers, or equivalently performing surgical fine-tuning on the early layers, can perform best in some settings. Furthermore, we study the relationship between the best subset of layers to tune and the nature of the distribution shift between the source and target distributions.\n\nDistribution shifts. Many existing works have studied adaptation and robustness to various distribution shifts (Tzeng et al., 2014; Byrd & Lipton, 2019; Hendrycks et al., 2019; Arjovsky et al., 2019; Salman et al., 2020; Liu et al., 2021a; Wiles et al., 2021; Andreassen et al., 2021; Miller et al., 2021; Creager et al., 2021; Lee et al., 2022; Kumar et al., 2022a). Such works typically frame robustness to distribution shift as a zero-shot generalization problem, where the model is trained on source and evaluated on target. We consider a different problem setting where the model is allowed to adapt to some labeled target data available. Some recent works have proposed methods for model adaptation at test time (Sun et al., 2020; Varsavsky et al., 2020; Iwasawa & Matsuo, 2021; Wang et al., 2020; Zhang et al., 2021a;b; Gandelsman et al., 2022). Recent works (Rosenfeld et al., 2022; Kirichenko et al., 2022) study a problem setting close to ours, showing that fine-tuning the last layer is sufficient for adapting to datasets with a spuriously correlated attribute. Our experiments in Section 2 confirm these results, and we further evaluate on a broader set of distribution shifts including image corruptions and shifts at the level of intermediate features. We find that fine-tuning different subsets of layers performs best for different types of distribution shifts, and also present theoretical analysis on the relationship between surgical fine-tuning and the type of distribution shift.\n\n9\n\n0481216202428323640Layer051015202530Auto-RGN WeightsBlock 1Block 2Block 3Block 1Block 2Block 3Published as a conference paper at ICLR 2023\n\nREFERENCES\n\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal Gupta. Better fine-tuning by reducing representational collapse. arXiv preprint arXiv:2008.03156, 2020.\n\nAnders Andreassen, Yasaman Bahri, Behnam Neyshabur, and Rebecca Roelofs. The evolution of out-of-distribution robustness throughout fine-tuning. arXiv preprint arXiv:2106.15831, 2021.\n\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\n\narXiv preprint arXiv:1907.02893, 2019.\n\nPeter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE Transactions on Medical Imaging, 2018.\n\nJonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning? In\n\nInternational Conference on Machine Learning, pp. 872–881. PMLR, 2019.\n\nNiladri S Chatterji, Behnam Neyshabur, and Hanie Sedghi. The intriguing role of module criticality\n\nin the generalization of deep networks. arXiv preprint arXiv:1912.00528, 2019.\n\nGordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In\n\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\nNiv Cohen, Rinon Gal, Eli A Meirom, Gal Chechik, and Yuval Atzmon. \" this is my unicorn, fluffy\": Personalizing frozen vision-language representations. arXiv preprint arXiv:2204.01694, 2022.\n\nElliot Creager, Jörn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant\n\nlearning. In International Conference on Machine Learning, pp. 2189–2200. PMLR, 2021.\n\nFrancesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670, 2020.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nSimon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. Advances in Neural Information Processing Systems, 31, 2018.\n\nCian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Schölkopf. Source-free adaptation to measurement shift via bottom-up feature restoration. arXiv preprint arXiv:2107.05446, 2021.\n\nCian Eastwood, Ian Mason, and Christopher KI Williams. Unit-level surprise in neural networks. In\n\nI (Still) Can’t Believe It’s Not Better! Workshop at NeurIPS 2021, pp. 33–40. PMLR, 2022.\n\nUtku Evci, Vincent Dumoulin, Hugo Larochelle, and Michael C Mozer. Head2toe: Utilizing intermediate representations for better transfer learning. In International Conference on Machine Learning, pp. 6009–6033. PMLR, 2022.\n\nYossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked\n\nautoencoders. arXiv preprint arXiv:2209.07522, 2022.\n\nHenry Gouk, Timothy Hospedales, and massimiliano pontil. Distance-based regularisation of deep\n\nnetworks for fine-tuning. In International Conference on Learning Representations, 2021.\n\nYunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and Rogerio Feris. Spottune: transfer learning through adaptive fine-tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4805–4814, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics, pp. 4921–4933, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.381.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n\nrecognition. CoRR, abs/1512.03385, 2015.\n\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019.\n\nDan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve model robustness and uncertainty. Advances in neural information processing systems, 32, 2019.\n\nJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification.\n\narXiv preprint arXiv:1801.06146, 2018.\n\nYusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021.\n\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. arXiv preprint arXiv:1911.03437, 2019.\n\nO ̆guzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir Zamir. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022.\n\nNeerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu. Test-time adaptable neural networks for robust medical image segmentation. Medical Image Analysis, 68:101907, 2021. ISSN 1361-8415.\n\nPolina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient\n\nfor robustness to spurious correlations. arXiv preprint arXiv:2204.02937, 2022.\n\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017.\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637–5664. PMLR, 2021.\n\nSimon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2661–2671, 2019.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. FineIn International\n\ntuning can distort pretrained features and underperform out-of-distribution. Conference on Learning Representations, 2022a.\n\nAnanya Kumar, Ruoqi Shen, Sébastien Bubeck, and Suriya Gunasekar. How to fine-tune vision\n\nmodels with sgd, 2022b.\n\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effective regularization to finetune\n\nlarge-scale pretrained language models. arXiv preprint arXiv:1909.11299, 2019a.\n\nJaejun Lee, Raphael Tang, and Jimmy Lin. What would elsa do? freezing layers during transformer\n\nfine-tuning. arXiv preprint arXiv:1911.03090, 2019b.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nYoonho Lee, Huaxiu Yao, and Chelsea Finn. Diversify and disambiguate: Learning from underspeci-\n\nfied data. arXiv preprint arXiv:2202.03418, 2022.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\n\ntuning. arXiv preprint arXiv:2104.08691, 2021.\n\nHao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Rethinking the hyperparameters for fine-tuning. In International Conference on Learning Representations, 2020.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\n\npreprint arXiv:2101.00190, 2021.\n\nEvan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In International Conference on Machine Learning, pp. 6781–6792. PMLR, 2021a.\n\nYuhan Liu, Saurabh Agarwal, and Shivaram Venkataraman. Autofreeze: Automatically freezing\n\nmodel blocks to accelerate fine-tuning. arXiv preprint arXiv:2102.01386, 2021b.\n\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, 2016.\n\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts.\n\nIn\n\nInternational Conference on Learning Representations, 2017.\n\nMaren Mahsereci, Lukas Balles, Christoph Lassner, and Philipp Hennig. Early stopping without a\n\nvalidation set. arXiv preprint arXiv:1703.09580, 2017.\n\nJohn P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In International Conference on Machine Learning, pp. 7721–7735. PMLR, 2021.\n\nSangwoo Mo, Minsu Cho, and Jinwoo Shin. Freeze the discriminator: a simple baseline for fine-\n\ntuning gans. arXiv preprint arXiv:2002.10964, 2020.\n\nSubhabrata Mukherjee and Ahmed Hassan Awadallah. Distilling transformers into simple neural\n\nnetworks with unlabeled transfer data. arXiv preprint arXiv:1910.01769, 1, 2019.\n\nBehnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning?\n\nAdvances in neural information processing systems, 33:512–523, 2020.\n\nMaxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1717–1724, 2014.\n\nJonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016.\n\nJonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations\n\nand learning algorithms. The MIT Press, 2017.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.\n\nAniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?\n\ntowards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157, 2019.\n\nVinay V Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hidden\n\nrepresentations and task semantics. arXiv preprint arXiv:2007.07400, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pp. 5389–5400. PMLR, 2019.\n\nYoungmin Ro and Jin Young Choi. Autolr: Layer-wise pruning and auto-tuning of learning rates in fine-tuning of deep networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 2486–2494, 2021.\n\nMiguel Romero, Yannet Interian, Timothy Solberg, and Gilmer Valdes. Targeted transfer learning to improve performance in small medical physics datasets. Medical Physics, 47(12):6246–6256, 2020.\n\nElan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. Domain-adjusted regression or: Erm may already learn features sufficient for out-of-distribution generalization. arXiv preprint arXiv:2202.06856, 2022.\n\nAmélie Royer and Christoph Lampert. A flexible selection scheme for minimum-effort transfer learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2191–2200, 2020.\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.\n\nHadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversarially robust imagenet models transfer better? Advances in Neural Information Processing Systems, 33:3533–3545, 2020.\n\nShibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation\n\nshift. arXiv preprint arXiv:2008.04859, 2020.\n\nBernhard Schölkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij.\n\nOn causal and anticausal learning. In International Conference on Machine Learning, 2012.\n\nOzan Sener, Hyun Oh Song, Ashutosh Saxena, and Silvio Savarese. Learning transferrable representations for unsupervised domain adaptation. Advances in neural information processing systems, 29, 2016.\n\nAli Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 806–813, 2014.\n\nZhiqiang Shen, Zechun Liu, Jie Qin, Marios Savvides, and Kwang-Ting Cheng. Partial is better than all: Revisiting fine-tuning strategy for few-shot learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 9594–9602, 2021.\n\nYu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020.\n\nHugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, and Hervé Jégou. Three things\n\neveryone should know about vision transformers. arXiv preprint arXiv:2203.09795, 2022.\n\nEleni Triantafillou, Hugo Larochelle, Richard Zemel, and Vincent Dumoulin. Learning a universal template for few-shot dataset generalization. In International Conference on Machine Learning, pp. 10424–10433. PMLR, 2021.\n\nEric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:\n\nMaximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.\n\nThomas Varsavsky, Mauricio Orbes-Arteaga, Carole H Sudre, Mark S Graham, Parashkev Nachev, and M Jorge Cardoso. Test-time unsupervised domain adaptation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 428–436. Springer, 2020.\n\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully\n\ntest-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n\nOlivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre Alvise-Rebuffi, Ira Ktena, Taylan Cemgil,\n\net al. A fine-grained analysis on distribution shift. arXiv preprint arXiv:2110.11328, 2021.\n\nSang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. InN-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations (ICLR), 2021.\n\nLI Xuhong, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with convolutional networks. In International Conference on Machine Learning, pp. 2825–2834. PMLR, 2018.\n\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep\n\nneural networks? Advances in neural information processing systems, 27, 2014.\n\nChiyuan Zhang, Samy Bengio, and Yoram Singer. Are all layers created equal? 2019.\n\nJeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning: a baseline for network adaptation via additive side networks. In European Conference on Computer Vision, pp. 698–714. Springer, 2020.\n\nMarvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and\n\naugmentation. arXiv preprint arXiv:2110.09506, 2021a.\n\nMarvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization: Learning to adapt to domain shift. Advances in Neural Information Processing Systems, 34:23664–23678, 2021b.\n\nLuisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adaptation via meta-learning. In International Conference on Machine Learning, pp. 7693–7702. PMLR, 2019.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA EXTENDED DISCUSSION\n\nIn this paper, we empirically find that when fine-tuning on a new target distribution, it is often best to perform surgical fine-tuning, i.e. to adapt only a small contiguous subset of parameters. More importantly, which subset is more effective to tune depends on the type of distribution shift. For example, on input-level shifts like image corruption, only tuning earlier layers can outperform fine-tuning all or only later layers. These results support our intuition from the independent causal mechanisms (ICM) principle: many distribution shifts can be explained by a shift in one module of the prediction mechanism and can thus be adapted to by tuning only a small subset of the network. Our empirical findings are supported by theoretical results, which show by construction that first-layer tuning may outperform full fine-tuning in an idealized two-layer neural network setting.\n\nAdditionally, manually choosing which layers to freeze with the framework of surgical fine-tuning requires more fine-tuning runs than fine-tuning all layers, so we analyze two criteria for automatically selecting which layers to tune. While Auto-RGN consistently improves over full fine-tuning, its performance does not match the best surgical fine-tuning approach. Future work may close this gap by investigating more effective criteria for automatic selection. More generally, a potentially fruitful direction for future work is in better understanding when a distribution shift would prefer a certain layer, potentially shedding light on the nature of different distribution shifts.\n\nB APPENDIX\n\nB.1 PROOFS FOR SECTION 3.1\n\nProposition 1. For all A, Psrc, Ptrg with xtrg = Axsrc for invertible A and ytrg = ysrc, there exists a first-layer B that can minimize the target loss: minB Ltrg(ˆvsrc, B) = 0. However, changing the last layer may not be sufficient: there exists such A, Psrc, Ptrg such that the target loss is non-zero for any choice of last layer v: for all i, minv Ltrg(v, ˆBsrc) > 0.\n\nProof. Let ˆBsrc, ˆvsrc be minimum loss solutions so that ysrc = ˆvsrcφ( ˆBsrcxsrc) for all xsrc, ysrc. Denoting B = ˆBsrcA−1, we have for all xtrg\n\nˆvsrcφ(Bxtrg) = ˆvsrcφ( ˆBsrcA−1Axsrc) = ˆvsrcφ( ˆBsrcxsrc) = ysrc = ytrg.\n\n(3)\n\nTherefore, this pair of parameters v, B achieves Ltrg(ˆvsrc, B) = 0.\n\nWe construct a counterexample showing the impossibility of last-layer tuning as follows. Recall that φ(·) is the elementwise ReLU function. Let A = −I, an invertible diagonal matrix with all entries −1. Let the source distribution be so that ˆBsrcxsrc has only positive entries for all xsrc in its support. Then for any v, we have vφ( ˆBsrcxtrg) = vφ(− ˆBsrcxsrc) = 0, so the expected loss is positive. Therefore, minv Ltrg(v, ˆBsrc) > 0.\n\nProposition 2. For all t, Psrc, Ptrg with xtrg = xsrc, and ytrg = tysrc, there exists a last-layer v that can minimize the target loss: minv Ltrg(v, ˆBsrc) = 0. However, changing the first layer may not be sufficient—there exists such t, Psrc, Ptrg such that the target loss is non-zero for any choice of first layer B: minB Ltrg(ˆvsrc, B) > 0\n\nProof. Let ˆBsrc, ˆvsrc be minimum loss solutions so that ysrc = ˆvsrcφ( ˆBsrcxsrc) for all xsrc, ysrc. Let v = tˆvsrc. Then for all xtrg,\n\nvφ( ˆBsrcxtrg) = tˆvsrcφ( ˆBsrcxsrc) = tysrc = ytrg.\n\n(4)\n\nTherefore, this pair of parameters v, ˆBsrc achieves Ltrg(v, ˆBsrc) = 0.\n\nWe next construct a counterexample showing that tuning only the first layer may not be sufficient. Recall that φ(·) is the elementwise ReLU function. Let t = −1. Let the source distribution be so that both Bxsrc and ˆvsrc consist only of positive entries for all xsrc in its support. For any B, both ˆvsrc and φ(Bxtrg) consist only of positive entries, so ˆvsrcφ(Bxtrg) cannot express ytrg = −ysrc < 0. so the expected loss is positive for any B. Therefore, minB Ltrg(ˆvsrc, B) > 0.\n\nB.2 PROOF OF THEOREM 1 IN SECTION 3.2\n\nWe introduce some additional setup, prove two key lemmas which bound the loss of first-layer tuning and full fine-tuning respectively, and then this immediately implies Theorem 1.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nDefining the label distribution. We assume that P (y | x) is the same in both the source and the target. That is, we assume there exists some v∗, B∗ such that y = v⊤ ∗ B∗x for both Psrc and Ptrg. Let w∗ = B⊤\n\n∗ v∗.\n\nDefining the covariate distributions. Now, we define the distribution over the inputs x. Let the source distribution Psrc have density on a dsrc-dimensional subspace, where dsrc < d (recall that the input dimension is d). Formally, this means that there exists some Ssrc ∈ Rd×dsrc with linearly independent columns, and some distribution P (z) src with density on Rdsrc, such that Psrc has the distribution Ssrcz where z ∼ P (z) src .\n\nWe assume a non-degeneracy condition—that the optimal model w∗ does not map (non-zero) source examples to 0: for all source examples x ∈ colspace(Ssrc), if x ̸= 0, then w⊤ ∗ x ̸= 0. If w∗ were random or had some noise then this would hold with probability 1.\n\nSuppose we have an orthogonal distribution Porth which has density on a dorth dimensional subspace. Formally, this means that there exists some Sorth ∈ Rd×dorth with linearly independent columns, and some distribution P (z) orth with density on Rdorth, such that Porth has the distribution Sorthz where z ∼ P (z) orth. We assume that the support of Porth and Psrc are orthogonal—that is, the columns of Sorth and the columns of Ssrc are all orthogonal.\n\nThe target distribution Ptrg is an equal mixture of the source distribution Psrc and the orthogonal distribution Porth:\n\nPtrg =\n\n(Psrc + Porth)\n\n(5)\n\n1 2\n\nThis means that to sample from Ptrg, with probability 0.5 we pick a sample from Psrc and with probability 0.5 we pick a sample from Porth.\n\nFirst layer tuning gets 0 target loss. We first show that first-layer tuning gets 0 loss on the target distribution: Lemma 1. For any δ > 0, suppose n > 10dorth log 2 tuning gets 0 loss at convergence:\n\nδ . Then with probability at least 1 − δ, first-layer\n\nLtrg(v∞\n\nfl , B∞\n\nfl ) = 0\n\nProof. We first note that first-layer tuning does not update the head v, so we have v∞\n\nfl = ˆvsrc.\n\nConvex so converges. Note that the loss ˆLtrg is convex in B. To see this, note that we can write:\n\nˆLtrg(v, B) =\n\nn (cid:88)\n\n(v⊤Bx(i)\n\ntrg − y(i)\n\ntrg)2 = (Tr(x(i)\n\ntrgv⊤B) − y(i)\n\ntrg)2.\n\n(7)\n\ntrgv⊤B) is a linear function of B, so this is simply a least squares regression problem and is\n\nTr(x(i) convex. This means that gradient flow converges to a minimizer of the train loss:\n\ni=1\n\n(8) However, since ˆvsrc ̸= 0, there exists B such that ˆLtrg(ˆvsrc, B) = 0, so since the loss is non-negative, this implies that:\n\nfl ) ≤ ˆLtrg(ˆvsrc, B) ∀B.\n\nˆLtrg(ˆvsrc, B∞\n\n(6)\n\n(9)\n\nˆLtrg(ˆvsrc, B∞\n\nfl ) = 0.\n\nDefine ID and orthogonal training examples. We note that every example x sampled from Ptrg comes from exactly one of Porth or Psrc.∗ We group examples based on which distribution they come from. Let Xsrc and Xorth denote the source and orthogonal examples respectively, where Xi denotes\n\nXsrc = {x(i)\n\ntrg : x(i)\n\ntrg ∈ colspace(Ssrc)} Xorth = {x(i)\n\ntrg : x(i)\n\ntrg ∈ colspace(Sorth)}\n\n(10)\n\nEnough to get a basis correct. Since we are working with linear models, it suffices to get all examples in a basis correct to get the entire subspace correct. Stated formally, if v⊤Bx = v⊤ ∗ B∗x ∗ B∗x′, then the equality holds for any linear combination as well: v⊤B(αx + and v⊤Bx′ = v⊤ βx′) = v⊤ fl ) = 0 is suffices to show that srcB∞ ˆv⊤\n\n∗ B∗(αx + βx′) for all α, β. So to show that Ltrg(ˆvsrc, B∞\n\n∗ B∗x for some set of x that spans the support of Psrc and Porth. ∗Since the distributions have density on some subspace, x is almost surely non-zero.\n\nfl x = v⊤\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nXorth spans orthogonal subspace. A standard application of Hoeffding gives us that with probability ≥ 1 − δ/2, we have at least dorth examples from the orthogonal distribution: |Xorth| ≥ dorth. Since Porth has density on a dorth dimensional subspace, from e.g., Lemma 3 in Xie et al. (2021) these examples will span the orthogonal subspace almost surely: span(Xorth) = colspace(Sorth). Intuitively, since Porth has density we will sample points in different directions.\n\nGet all examples in orthogonal subspace correct. Since we have 0 training loss, and Xorth spans the support of Porth, this means that we get all examples in the orthogonal subspace correct—that is, for all x in the support of Porth, we have: ˆv⊤\n\nsrcB∞\n\nfl x = v⊤\n\n∗ B∗x.\n\nGet all examples in source subspace correct. For the source subspace we will split into two cases. First, we define the region of the source subspace colspace(Ssrc) that is orthogonal to all source training examples Xsrc:\n\nX ⊥\n\nsrc = {x ∈ colspace(Ssrc) : ∀x′ ∈ Xsrc. x ⊥ x′}.\n\n(11)\n\nSince we have 0 training loss, we get all examples in the span of the source training examples correct: for all x ∈ span(Xsrc) we have: ˆv⊤ ∗ B∗x.\n\nfl x = v⊤\n\nsrcB∞\n\nFor all x ∈ X ⊥ But this means that ˆv⊤ source we have: ˆv⊤ src\n\nfl x = ˆv⊤\n\nsrcB∞ ˆBsrcx = v⊤\n\n∗ B∗x.\n\nsrc\n\nsrc, from Lemma A.3 in Kumar et al. (2022a) we have that B∞\n\nfl x = Bft(0)x = ˆBsrcx. ˆBsrcx. Since we assumed that we pretrained to get 0 loss on the\n\nCombining these two cases, we get all examples in the support of Psrc correct.\n\nPtrg is a mixture of Psrc and Porth. Since we get 0 loss on the support of Psrc and support of Porth, and Ptrg is a mixture of the two, we get 0 loss on Ptrg as well:\n\nLtrg(ˆvsrc, B∞\n\nfl ) = 0\n\n(12)\n\nLemma 2. Suppose the representation dimension is 1-dimensional (k = 1) and dsrc > n. Then with probability at least 1, full fine-tuning gets positive (non-zero) loss at all times t:\n\nLtrg(vft(t), Bft(t)) > 0\n\n(13)\n\nProof. First, we note that since n < dsrc, and we only have n training examples, our training examples do not span the source distribution. That is, there exists some source example xs ∈ colspace(Ssrc) which is in the support of Psrc, xs ̸= 0, which is orthogonal to all the training examples: xs ⊥ x(i) for all 1 ≤ i ≤ n. Choose such an xs. We note that since k = 1 (the representation dimension is 1), vft(t) ∈ R is a scalar, and Bft(t), B∗ ∈ R1×d are row vectors. For notational convenience, let b(t) = Bft(t)⊤, b∗ = B⊤ src, so we have for example y(i)\n\n∗ , and ˆbsrc = ˆB⊤\n\ntrg\n\ntrg = v∗b⊤\n\n∗ x(i) trg.\n\nvft(t) cannot change for zero loss. First, we show that if vft(t) ̸= ˆvsrc then Ltrg(vft(t), Bft(t)) > 0. Since xs is orthogonal to the training examples, from Lemma A.3 in Kumar et al. (2022a), we have b(t)⊤xs = ˆb⊤ srcxs. Since pretraining gave us 0 loss on the source distribution, we have ˆb⊤ srcxs = v∗b⊤ ˆvsrc srcxs ̸= 0 since pretraining gets all the source examples right, and the ground truth label for all non-zero source examples is non-zero. But then if vft(t) ̸= ˆvsrc, we have: vft(t)b(t)⊤xs ̸= ˆvsrc ∗ xs. Since Psrc has density, we can construct a small ball B of non-zero probability around xs such that for all x ∈ B, vft(t)b(t)⊤x ̸= v∗b⊤ ∗ x. This implies that Ltrg(vft(t), Bft(t)) > 0.\n\n∗ xs ̸= 0 if xs ̸= 0, which implies that ˆb⊤\n\n∗ xs. Recall that we assumed that w⊤\n\nˆb⊤ srcx = v∗b⊤\n\nvft(t) must change for zero loss. Next, suppose that Ltrg(vft(t), Bft(t)) = 0. We will show vft(t) ̸= ˆvsrc. Suppose for the sake of contradiction that vft(t) = ˆvsrc.\n\nFrom Lemma A.4 in Kumar et al. (2022a) (also see Theorem 2.2 in Du et al. (2018)), we have:\n\nsrc − ˆb⊤ ˆv2\n\nsrc\n\nˆbsrc = vft(t)2 − b(t)⊤b(t).\n\n(14)\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nSince ˆvsrc = vft(t), this gives us:\n\nˆb⊤\n\nsrc\n\nˆbsrc = b(t)⊤b(t).\n\n(15)\n\nLet R = colspace(Ssrc) be the source subspace. Since Psrc is a subset of Ptrg, we have that (vft(t), b(t)) also gets 0 loss on the source distribution, and we have:\n\nΠR(v∗b∗) = ΠR(ˆvsrc\n\nˆbsrc) = ΠR(vft(t)b(t)).\n\nSince v0 = vft(t), , we have:\n\nSince vft(t) ̸= 0 (otherwise we would get source examples wrong):\n\nΠR(vft(t)ˆbsrc) = ΠR(vft(t)b(t)).\n\nΠR(ˆbsrc) = ΠR(b(t)).\n\n(16)\n\n(17)\n\n(18)\n\nLet T = colspace(Sorth) be the orthogonal subspace. From Equation 15 and Equation 18, we have:\n\n∥ΠT (ˆbsrc)∥2\n\n2 = ∥ΠT (b(t))∥2\n\n2\n\nBut to get 0 loss on T , we have: ΠT (vft(t)b(t)) = ΠT (v∗b∗). Which implies:\n\n∥ΠT (b(t))∥2\n\n2 =\n\nv2 ∗\n\n(vft(t))2 ∥ΠT (b∗)∥2 2.\n\nFrom Equation 19 and since ˆvsrc = vft(t), we have:\n\n∥ΠT (ˆbsrc)∥2\n\n2 =\n\nv2 ∗\n\n(vft(t))2 ∥ΠT (b∗)∥2 2.\n\n(19)\n\n(20)\n\n(21)\n\nRecall that the way we got ˆbsrc was we initialized b0 = B⊤ BId). We then ran gradient descent on the source distribution However, from Lemma A.3 in Kumar et al. (2022a) this does not change the projection onto components orthogonal to the source distribution. In other words, ∥ΠT (ˆbsrc)∥2 2. However, this is a random variable with density, so the probability that this is exactly equal to the RHS of Equation 21 which is a fixed number, is 0. This is a contradiction.\n\n2 = ∥ΠT (b0)∥2\n\n0 ∼ N (0, σ2\n\nWrap up. Either ways, if vft(t) = ˆvsrc or vft(t) ̸= ˆvsrc we have: Ltrg(vft(t), Bft(t)) > 0.\n\nProof of Theorem 1. The proof follows directly because Lemma 2 shows that full fine-tuning gets positive (non-zero) loss but Lemma 1 gets zero loss.\n\nB.3 ADDITIONAL DATASET DETAILS\n\nBelow, we provide additional information on the datasets used in our experiments.\n\n• CIFAR-10 → CIFAR-10-C (Krizhevsky et al., 2009; Hendrycks & Dietterich, 2019): The task is to classify images into 10 classes, where the target distribution contains severely corrupted images. We run experiments over 14 of the corruptions (frost, gaussian blur, gaussian noise, glass blur, impulse noise, jpeg compression, motion blur, pixelate, saturate, shot noise, snow, spatter, speckle noise, and zoom blur). For the main experiments, we tune on 1000 images from CIFAR-10-C and evaluate on corrupted images from each of the corruptions. We use the data loading code from (Croce et al., 2020), which has 5 levels of severity, and we evaluate with the most severe level. In our main experiments, we report the accuracies averaged across all corruptions and the average std error for all corruptions.\n\n• ImageNet → ImageNet-C (Deng et al., 2009; Hendrycks & Dietterich, 2019): The task is to classify images into 1000 classes, where the target distribution contains severely corrupted images. We run experiments over 15 of the corruptions (brightness, contrast, defocus blur, elastic transform, fog, frost, Gaussian noise, glass blur, impulse noise, jpeg compression, motion blur, pixelate, shot noise, snow, zoom blur). For the main experiments, we tune on 5000 images from ImageNet-C, evenly split between classes, giving 5 corrupted images per class and evaluate on corrupted images from each of the corruptions. Similar to CIFAR-10-C, we evaluate with the most severe level. We also report the accuracies averaged across all corruptions and the average std error for all corruptions.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\n• Living-17 and Entity-30 (Santurkar et al., 2020): The task is to classify images into one of 17 animal categories or one of 30 entities. These datasets present subpopulation shifts, in that while the ID and OOD distributions have the same overall classes, they contain different subpopulations of those classes. For Living-17, we tune on 850 images from the target distribution, evenly split between the 17 classes, giving 50 images per class. For Entity-30, we tune on 1500 images from the target distribution, evenly split between the 30 classes, giving 50 images per class.\n\n• Waterbirds (Sagawa et al., 2019): The task is to classify images as being a “waterbird” or “landbird”. The label is spuriously correlated with the image background, which is either “land” or “water.” The source distribution is the training set while the target distribution is a balanced subset with equal amounts of each bird on each background. In the training data, 95 % of the waterbirds appear on water backgrounds, and 95% of the landbirds appear on land backgrounds, so the minority groups contain far fewer examples than the majority groups. We tune on 400 images from the target distribution, evenly split between the 4 groups of (bird, background) pairs, giving 100 images per group.\n\n• CelebA (Sagawa et al., 2019): The task is to classify the hair color in images as “blond” or “not blond”, and the label is spuriously correlated with the Male attribute. The source distribution is the training set while the target distribution is a balanced subset with equal amounts of each of the four (hair color, gender) groups. We tune on 400 images from the target distribution, evenly split between the 4 groups of (hair color, gender) pairs, giving 100 images per group.\n\n• Camelyon17 (Bandi et al., 2018): This dataset is part of the WILDS (Koh et al., 2021) datasets and contains roughly 450,000 images in the source distribution (Train) and 84,000 images in the target distribution (OOD test) of size 96 × 96. It comprises of medical images collected from 5 hospitals where difference in devices/data-processing between different hospitals produces a natural distribution shift. We pre-train on the 450,000 images of the source distribution and use 100 label-balanced images (50 per class) from the target distribution for fine-tuning. We use another 100 label-balanced target distribution images for tuning hyper-parameters, and report the performance of the fine-tuned model on the rest of the images of the target distribution.\n\n• FMoW (Christie et al., 2018): This is also part of the WILDS (Koh et al., 2021) datasets and its source distribution contains 520,000 satellite images of size 224x224 from 5 geographic regions. The task is to classify one of 62 building or land use types. For target distribution, we use Africa test, i.e., the subset of the OOD test data belonging to Africa region, which has roughly 2,500 images. We use 62 label-balanced images from target distribution for fine-tuning and report the accuracy on the rest of the target distribution images.\n\nB.4 ADDITIONAL DETAILS FOR SUPERVISED TRANSFER LEARNING EXPERIMENTS\n\nBelow, we provide additional details for our experiments on real data, including tuning details. For all datasets and experiments, we early stop according to the best accuracy on a held-out validation subset of the labeled target data.\n\n• CIFAR-10 → CIFAR-10-C (Krizhevsky et al., 2009; Hendrycks & Dietterich, 2019) and CIFARFlip: We use the Standard pre-trained model from (Croce et al., 2020), which is trained on the source CIFAR-10 distribution. We fine-tune on the labeled target data for 15 total epochs. We tune over the 3 learning rates {1e-3, 1e-4, 1e-5} for all methods except last-layer fine-tuning, where we tune over {1e-1, 1e-2, 1e-3}, and we use a weight decay of 0.0001 for all methods.\n\n• ImageNet → ImageNet-C (Deng et al., 2009; Hendrycks & Dietterich, 2019): We use the Standard pre-trained model from (Croce et al., 2020), which is trained on the source ImageNet distribution. We then fine-tune on the labeled target data for 10 total epochs. We tune over the 3 learning rates {1e-3, 1e-4, 1e-5} for all methods, and we use a weight decay of 0.0001 for all methods.\n\n• Living-17 and Entity-30 (Santurkar et al., 2020): We first train on the source data for 5 epochs, tuning only the head for 3 epochs and then fine-tuning all for 2 more epochs, following LP-FT (Kumar et al., 2022a) and using the Adam optimizer. We then fine-tune on the labeled target data for 15 epochs. We tune over the 3 learning rates {0.0005, 0.0001, 0.00001} for all methods and do not use any weight decay.\n\n• Waterbirds (Sagawa et al., 2019): We first start with a ResNet-50 pretrained on ImageNet and train on the source distribution for 300 epochs, taking the best checkpoint based on early stopping and using the Adam optimizer. We then fine-tune on the labeled target data for 100 total epochs. We tune over the 3 learning rates {0.005, 0.001, 0.0005} for all methods and use a weight decay of 0.0001.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nmCE (%)\n\nVanilla ResNet-50 (Hendrycks & Dietterich, 2019)\n\nCross-Val\n\nFull Fine-tuning Gradual (First → Last) Gradual (Last → First) L1 Regularize (Xuhong et al., 2018) Auto-SNR Auto-RGN\n\n76.7\n\n61.7\n\n62.8 62.4 63.6 64.7 62.9 61.9\n\nTable 6: For completeness, we report the mean corruption error (mCE) on ImageNet-C, which weights the target distribution error by the difficulty of the corruption. We report the average accuracies on the target distribution in Figure 2 and Table 4, and we see that the two metrics are correlated for our experiments, as the best performing methods according to average accuracy are also the best for mCE.\n\nFirst Layers\n\nBlock 1\n\nBlock 2\n\nBlock 3\n\nBlock 4\n\nLast\n\nAll\n\nNo Tuning\n\n74.6 (1.9)\n\n75.3 (1.9)\n\n81.6 (1.2)\n\n86.1 (0.7)\n\n82.9 (1.5)\n\n72.6 (1.3)\n\n77.3 (0.4)\n\n65.6 (0)\n\nTable 7: Surgical fine-tuning results on Living-17 (% accuracy) initialized with a CLIP ViT-B/16. We find that similar to the results using a ResNet-50 architecture, fine-tuning a single parameter block with this vision transformer architecture outperforms full fine-tuning, and in particular, a middle block still performs best for this feature-level shift.\n\n• CelebA (Sagawa et al., 2019): We first start with a ResNet-50 pretrained on ImageNet and train on the source distribution for 50 epochs, taking the best checkpoint based on early stopping and using the Adam optimizer. We then fine-tune on the labeled target data for 50 total epochs. We tune over the 3 learning rates {0.001, 0.0005, 0.0001} for all methods and use a weight decay of 0.0001.\n\n• Camelyon17 (Bandi et al., 2018; Koh et al., 2021): We start with a vision transformer, CLIP ViT-B/16 (Radford et al., 2021), pre-trained on the CLIP datasets. Next we fine-tune the model on Camelyon17 train dataset using an SGD optimizer with initial learning rate 0.0001 for 3 epochs. We use a cosine annealing learning rate scheduler (Loshchilov & Hutter, 2017) and batch size 32. Finally, we fine-tune on the labeled target data for 10 total epochs with the same setting as before, except we tune over learning rates {10−4, 10−5, 3 × 10−5, 7 × 10−5, 10−6, 3 × 10−6, 10−7, 10−8}\n\n• FMoW (Christie et al., 2018; Koh et al., 2021): Similar to Camelyon17, we start with a vision transformer, CLIP ViT-B/16 (Radford et al., 2021), pre-trained on the CLIP datasets. Next we finetune the model on FMoW train dataset using an SGD optimizer with initial learning rate 0.0003 for 5 epochs. We use a cosine annealing learning rate scheduler (Loshchilov & Hutter, 2017) and batch size 32. Finally, we fine-tune on the labeled target data for 10 total epochs with the same setting as before, except we tune over learning rates {10−6, 10−5, 0.0003, 0.0001, 0.001, 0.01, 0.1, 0.25}.\n\nIn Table 6, for completeness, we additionally report the mean corruption error (mCE), which weights the target distribution error by the difficulty of the corruption, on ImageNet-C since that is a common metric used for the dataset. We find that these results give similar conclusions as the average accuracies reported in Table 4, with Cross-Val and Auto-RGN performing the best.\n\nWe additionally include an ablation where we use a CLIP ViT-B/16 (Vision Transformer) as our initial model pretrained on the WebImageText dataset. This model consists of 2 first layers followed by 4 transformer blocks and 2 last layers. We analyze surgical fine-tuning with this model architecture on the Living-17 dataset, which has a feature-level shift. In Table 7, we find that our results in Section 2 hold similarly using this vision transformer, as tuning only a middle block outperforms full fine-tuning or tuning any other block of layers.\n\nB.5 MORE LARGE VISION TRANSFORMER EXPERIMENTS\n\nPrior work has shown that when fine-tuning vision transformers, task accuracy on held-out data is substantially higher when using the AdamW optimizer rather than SGD (Kumar et al., 2022b). We evaluate the performance of surgical fine-tuning on two large pre-trained vision transformer models (CLIP ViT-B/16 and ViT-L/14) while fine-tuning with AdamW. We follow the experimental setting of Kumar et al. (2022b) as closely as possible:\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nDataset Architecture Optimizer\n\nCamelyon17 Camelyon17\n\nViT-B/16 SGD\n\nViT-B/16 AdamW\n\nFMoW\n\nFMoW FMoW ViT-B/16 ViT-B/16 ViT-L/14 AdamW AdamW\n\nSGD\n\nNo fine-tuning\n\n86.2\n\n96.2\n\n35.5\n\n41.7\n\n50.3\n\nAll Embedding First three blocks Last three blocks Last layer\n\n92.3 (1.7) 95.6 (0.4) 92.5 (0.5) 87.5 (4.1) 90.1 (1.5)\n\n96.4 (0.4) 96.9 (0.1) 95.0 (0.2) 96.6 (0.1) 96.7 (0.1)\n\n38.9 (0.5) 36.0 (0.1) 39.8 (1.0) 44.9 (2.6) 36.9 (5.5)\n\n29.2 (2.6) 41.8 (0.1) 26.7 (1.3) 46.1 (2.3) 42.6 (0.1)\n\n52.9 (1.2) 50.1 (1.6) 51.6 (0.9) 52.4 (0.6) 52.3 (0.3)\n\nTable 8: OOD set accuracies after surgically fine-tuning different parameters in vision transformer models for two WILDS datasets. Bold numbers represent superior results for a dataset, and we also report the standard deviation from runs with 3 different seeds.\n\n• Camelyon17 (Bandi et al., 2018; Koh et al., 2021). We first train a pre-trained CLIP ViT-B/16 (Radford et al., 2021) model on the train split of the Camelyon17 dataset using an AdamW optimizer with initial learning rate 10−6 for 3 epochs. We use a cosine annealing learning rate scheduler (Loshchilov & Hutter, 2017) and batch size 32. Finally, we fine-tune on the labeled target data for 10 total epochs with the same setting as before, except we tune over learning rates {10−4, 10−5, 3 × 10−5, 7 × 10−5, 10−6, 3 × 10−6, 10−7, 10−8}.\n\n• FMoW (Christie et al., 2018; Koh et al., 2021). We similarly train a pre-trained CLIP ViT-B/16 or ViT-L/14 model on the train split of the FMoW dataset with initial learning rate 10−5 for 5 epochs. We use a cosine annealing learning rate scheduler (Loshchilov & Hutter, 2017) and batch size 32. For the ViT-L/14 setting, we use larger 336 × 336 images. Finally, we fine-tune on labeled target data for 10 total with the same setting as before, tuning over learning rates {10−4, 10−5, 3 × 10−5, 7 × 10−5, 10−6, 3 × 10−6, 10−7, 10−8}.\n\nWe show AdamW fine-tuning results in Table 8. While surgically fine-tuning the right layer continues to improve over no fine-tuning, the relative advantage compared to fine-tuning all layers is smaller than what we observed with SGD in Table 7. We observe instability in fine-tuning all layers of a ViT-B/16 network for FMoW target distributions. Fine-tuning later layers seems to consistently improve performance without running into such instability issues. We leave further investigation of such properties of fine-tuning ViT models to future work.\n\nB.6 COMPLETE UNSUPERVISED ADAPTATION RESULTS\n\nMethod. We experiment with MEMO (Zhang et al., 2021a) as our unsupervised adaptation method. Given a test image x, MEMO first takes an \"adapt\" stage, where it minimizes the marginal entropy over standard augmentations of x, then it takes a \"test\" step, where the network predicts a label for x. Note that MEMO tests a single image at a time, i.e., the test batch size is 1. We also consider the two following variations.\n\n• Episodic: This version is discussed in the origin work. Here after predicting the labels, we reset\n\nthe weights of the network to the pre-trained ones, i.e., we undo the \"adapt\" stage.\n\n• Online: We also consider the online variation of MEMO, where we do not reset the weights after\n\neach test image, i.e., we accumulate the \"adaptation\" changes over test images.\n\nWe have also experimented with TENT (Wang et al., 2020), but since TENT only updates the batchnorm modules (whereas MEMO updates all parameters), freezing parameters with TENT did not produce expected results and we did not pursue it further.\n\nDataset and Network. We use the CIFAR-10-C and ImageNet-C corruption datasets for our experiments. For CIFAR-10-C, we use the same ResNet-26 (He et al., 2015) pre-trained model used by MEMO (Zhang et al., 2021a), which is available in their GitHub repository. For ImageNet-C, we use RVT∗-small architecture and pre-trained weights used by Zhang et al. (2021a).\n\nHyper-parameters. For CIFAR-10-C, we use 1000 corrupted test image for hyper-parameter tuning and report the test accuracy on the held out 9000 examples. We consider the following hyper-parameter grid:\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nSettings\n\nTuned Layers\n\ngauss\n\nimpulse\n\nshot\n\nfog\n\nCorruption frost\n\nsnow\n\nNo Adaptation\n\nEpisodic\n\nOnline\n\n51.6\n\n49.7\n\n55.2\n\n72.0\n\n66.9\n\n75.9\n\nAll First layer First 2 layers First 2 blocks Last\n\nAll First layer First 2 layers First 2 blocks Last\n\n56.31 53.11 53.20 53.20 51.69\n\n51.48 68.14 68.00 67.84 51.61\n\n56.39 51.29 51.21 51.36 49.82\n\n49.53 59.21 60.92 59.30 49.71\n\n59.94 56.53 56.58 56.63 55.24\n\n55.85 71.10 73.07 70.96 55.21\n\n77.42 73.27 73.48 73.37 72.01\n\n75.69 79.61 79.22 79.11 71.99\n\n71.93 68.41 68.64 68.61 67.05\n\n72.64 76.20 76.16 75.91 66.92\n\n78.63 77.16 77.06 77.22 75.91\n\n77.36 79.29 79.34 79.44 75.87\n\nelast\n\n74.4\n\n78.99 76.46 76.41 76.45 74.56\n\n76.16 76.86 76.18 76.46 74.49\n\nbrit\n\ncontr\n\ndefoc\n\n85.9\n\n70.3\n\n75.9\n\n88.15 86.53 86.47 86.56 85.98\n\n86.83 86.72 86.65 86.91 86.00\n\n72.66 70.82 70.67 70.73 70.28\n\n70.88 74.26 73.97 74.67 70.27\n\n76.32 76.11 76.15 76.09 75.98\n\n79.89 82.18 81.97 82.12 75.91\n\nTable 9: MEMO (Zhang et al., 2021a) with parameter freezing results on CIFAR-10-C on 10 representative corruptions and severity level 5. Bold numbers represent superior results for a particular corruption.\n\n• Learning rate: 10−3, 10−4, 10−5 and 10−6, then 2.5x, 5x, 0.5x of the best learning rate from\n\nbefore.\n\n• Steps: 1, 2. • Weight decay: 0, 10−3, 10−2, 10−1.\n\n• Number of augmentations per image: 32\n\nFor ImageNet-C, we do not do any hyper-parameter tuning and simply use the best hyper-parameters described by Zhang et al. (2021a), which are:\n\n• Learning rate: 0.00001\n\n• Weight decay: 0.01\n\n• Steps: 1\n\n• Number of augmentations per image: 32 (This is 64 in Zhang et al. (2021a), but we use 32 for\n\ncomputational cost)\n\nMoreover, for ImageNet-C, the experiments are done over 2000 test images (the first 2 test image per class for each of the 1000 classes) instead of the entire test set of 50, 000 images, for computational reasons. This produces numbers that are slightly different from the original work, hence we include the no adaptation baselines as well for fair comparison.\n\nFinally, in practice, we saw that AdamW and SGD optimizers work better for the episodic and online setting, respectively.\n\nLayers. We use the following naming convention for the layers of ResNet-26:\n\n• First: Only the first conv layer.\n\n• First 2 layers: The first conv layer of the entire network, and the first conv layer within the first\n\nblock.\n\n• First 2 blocks: The first conv layer, and the first block.\n\n• Last: The last fully-connected (FC) layer.\n\nFor RVT∗-small:\n\n• First layer: First conv layer inside the first transformer block.\n\n• First block: First transformer block.\n\n• Last: Head or the final fully connected layer.\n\nResults. Table 9 and Table 10 show results for MEMO with parameter freezing for CIFAR-10-C and ImageNet-C respectively on severity level 5 and on a representative set of corruptions.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nSettings\n\nTuned Layers\n\ngauss\n\nimpul\n\nshot\n\nfog\n\nfrost\n\nsnow\n\nCorruption contr\n\nbrit\n\nelast\n\nglass motion\n\nzoom pixel\n\njpeg\n\nNo Adapt\n\nEpisodic\n\nOnline\n\n43.80\n\n45.90\n\n42.40\n\n55.20\n\n45.95\n\n50.05\n\n73.95\n\n53.25\n\n35.00\n\n19.35\n\n40.35\n\n32.05\n\n52.60\n\n60.60\n\nAll First layer First block Last\n\nAll First layer First block Last\n\n45.35 44.00 44.10 43.75\n\n1.05 44.70 46.50 43.80\n\n47.40 46.00 46.30 45.95\n\n0.90 46.30 49.65 45.90\n\n44.60 42.40 42.60 42.40\n\n1.40 43.40 46.75 42.40\n\n55.35 55.20 55.40 55.25\n\n0.35 46.50 55.80 55.15\n\n47.05 45.90 46.00 46.0\n\n3.40 47.0 47.40 45.95\n\n50.25 50.10 50.15 50.05\n\n1.60 50.9 52.55 50.00\n\n74.35 74.0 74.10 73.95\n\n0.80 74.15 74.80 73.90\n\n54.85 53.30 53.20 53.30\n\n0.80 48.85 54.15 53.20\n\n36.25 35.20 35.15 35.05\n\n2.75 36.40 40.50 35.05\n\n20.55 19.45 19.75 19.35\n\n1.05 20.20 24.75 19.35\n\n41.40 40.60 40.75 40.40\n\n2.50 41.65 41.45 40.35\n\n33.95 32.15 32.25 32.05\n\n2.25 32.50 34.15 32.05\n\n55.55 52.80 53.20 52.60\n\n2.70 55.70 56.85 52.60\n\n61.70 60.60 60.75 60.60\n\n3.40 61.05 61.05 60.60\n\nTable 10: MEMO (Zhang et al., 2021a) with parameter freezing results on ImageNet-C on 14 representative corruptions and severity level 5. Bold numbers represent superior results for a particular corruption.\n\n23",
    "reference": "# Summary Of The Paper\n\nThe paper performs an analysis of what the fine-tuning strategy should be when encountering various distribution shifts. They classify the different distribution shifts into three categories and show that for each category of distribution shift, different layers of the model should be surgically fine-tuned. This analysis results in interesting outcomes, for example, for shifts where the inputs are changing (like CIFAR-C), contrary to the conventional wisdom of fine-tuning the last few layers to re-use the learned features, it is better to fine-tune only the early layers of the network. The paper also discusses and compares some ways to automatically decide which layers to fine-tune.\n\n# Strength And Weaknesses\n\nStrength(s):\n1. The paper nicely and cleanly classifies the distribution shifts into three types and empirically shows how different fine-tuning strategies are optimal for different types of distribution shifts.\n2. The synthetic experiment in the paper is clever and successfully demonstrates that surgically fine-tuning the relevant layers/blocks can result in a significant performance gain.\n3. New strategies (Auto-SNR and Auto-RGN) for automatically selecting which layers to fine-tune were a nice outcome of the theoretical analysis performed by the authors.\n4. The empirical observations to fine-tune only a subset of layers for different distribution shifts are well supported by multiple datasets having different types of distribution shifts. \n\nWeakness(es)/Suggestion(s):\n1. For table 3, in addition to the L1 regularization method, it would be nice to include other methods like gradually unfreezing or different learning rates for different layers, etc. \n\n(some methods from the existing work: \nA. Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2661–2671, 2019. [page 1]\nB. Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Rethinking the hyperparameters for fine-tuning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum id=B1g8VkHFPH. [page 1, 9]\nC. Zhiqiang Shen, Zechun Liu, Jie Qin, Marios Savvides, and Kwang-Ting Cheng. Partial is better than all: Revisiting fine-tuning strategy for few-shot learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 9594–9602, 2021. [page 1, 9]\nD. Youngmin Ro and Jin Young Choi. Autolr: Layer-wise pruning and auto-tuning of learning rates in fine-tuning of deep networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 2486–2494, 2021. [page 1]\nE. Miguel Romero, Yannet Interian, Timothy Solberg, and Gilmer Valdes. Targeted transfer learning to improve performance in small medical physics datasets. Medical Physics, 47(12):6246–6256, 2020. [page 1])\n\n2.  Although the observation of fine-tuning different subsets of layers for different distribution shifts is an interesting one, it is not clear how the method of surgically fine-tuning a network would compare to existing methods for domain adaptation or methods that perform test-time adaptation (adjusting the covariate shift, etc.) [F,G,H]. It would strengthen the paper if authors can show that surgical fine-tuning is a better and simpler option to tackle distribution shifts as compared to training exotic models that enable robustness or allow for domain adaptation. \n\nF. Improving robustness against common corruption by covariate shift adaptation, NeurIPS 2020\nG. Adaptive Denoising via GainTuning, NeurIPS 2021\nH. Be Like Water: Robustness to Extraneous Variables Via Adaptive Feature Normalization, 2020\n\n3. Is the reported metric for CIFAR-C and IN-C (1- mean corruption error (mCE)), in Tables 1,2, and 3, and Figures 2, 3 and 4? Because mCE is a standard metric to track when dealing with corruption datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work is explained well and has novel aspects in it. In terms of reproducibility, I was not able to find the code in the supplementary material but it should be relatively straightforward to reproduce it\n\n# Summary Of The Review\n\nOverall, although the work has some interesting analyses and outcomes/results, it is still unclear how the method of surgically fine-tuning a network would compare to existing methods for domain adaptation or methods that perform test-time adaptation (adjusting the covariate shift, etc.). Therefore, my recommendation for this paper would be marginally below the acceptance threshold (5).\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDIFFUSION ADVERSARIAL REPRESENTATION LEARNING FOR SELF-SUPERVISED VESSEL SEGMENTATION\n\nBoah Kim∗, Yujin Oh∗, Jong Chul Ye Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea {boahkim,yujin.oh,jong.ye}@kaist.ac.kr\n\nABSTRACT\n\nVessel segmentation in medical images is one of the important tasks in the diagnosis of vascular diseases and therapy planning. Although learning-based segmentation approaches have been extensively studied, a large amount of groundtruth labels are required in supervised methods and confusing background structures make neural networks hard to segment vessels in an unsupervised manner. To address this, here we introduce a novel diffusion adversarial representation learning (DARL) model that leverages a denoising diffusion probabilistic model with adversarial learning, and apply it to vessel segmentation. In particular, for self-supervised vessel segmentation, DARL learns the background signal using a diffusion module, which lets a generation module effectively provide vessel representations. Also, by adversarial learning based on the proposed switchable spatially-adaptive denormalization, our model estimates synthetic fake vessel images as well as vessel segmentation masks, which further makes the model capture vessel-relevant semantic information. Once the proposed model is trained, the model generates segmentation masks in a single step and can be applied to general vascular structure segmentation of coronary angiography and retinal images. Experimental results on various datasets show that our method significantly outperforms existing unsupervised and self-supervised vessel segmentation methods.\n\n1\n\nINTRODUCTION\n\nIn the clinical diagnosis of vascular diseases, vessel segmentation is necessary to analyze the vessel structures and therapy planning. In particular, when diagnosing coronary artery disease, X-ray angiography is taken to enhance vessel visualization by injecting a contrast agent into the blood vessels (Cong et al., 2015). However, it is challenging to extract vessels accurately due to low contrast, motion artifacts, many tiny branches, structural interference in the backgrounds, etc (Xia et al., 2019; Chen et al., 2014).\n\nTo segment vascular structures, various segmentation methods have been explored. Traditional optimization models (Law & Chung, 2009; Taghizadeh Dehkordi et al., 2014) typically require complicated preprocessing steps and manual tuning. Furthermore, they are computationally expensive to process many images. On the other hand, learning-based methods (Nasr-Esfahani et al., 2016; Fan et al., 2018; Chen et al., 2019) generate segmentation maps in real-time once the models are trained. However, supervised methods require a huge amount of labeled data for training, which complicates their use in practical applications. Also, existing unsupervised methods designed on natural images are difficult to apply to medical vessel images due to low contrast subtle branches and confusing background structures. Although a recent self-supervised method (Ma et al., 2021) is presented to learn vessel representations, this requires two different adversarial networks to segment vessels, which leads to increasing training complexity.\n\nRecently, diffusion models such as denoising diffusion probabilistic model (DDPM) (Ho et al., 2020) has become one of the main research topics in modeling data distribution and sampling diverse images. By learning the Markov transformation of the reverse diffusion process from Gaussian\n\n∗ co-first authors\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Our proposed diffusion adversarial representation model for self-supervised vessel segIn path (A), given a real noisy angiography image xa ta , our model estimates vessel mentation. segmentation masks ˆsv. In path (B), given a real noisy background image xb tb and a vessel-like fractal mask sf , our model generates a synthetic angiography image ˆxa.\n\nnoise to data, DDPM is successfully applied to many low-level computer vision tasks such as superresolution (Chung et al., 2022), inpainting (Lugmayr et al., 2022), and colorization (Song et al., 2020). For high-level vision tasks, while a recent study (Baranchuk et al., 2021) shows that DDPM can capture semantic information and be used as image representations, methods applying DDPM in learning semantic segmentation without labeled data have so far not been developed. Also, the sampling process of the diffusion models often takes a relatively long time.\n\nIn this paper, we introduce a novel concept of diffusion adversarial representation learning (DARL), which is a non-iterative version of the diffusion-based generative model and can be successfully applied to self-supervised vessel segmentation without ground-truth labels. As illustrated in Figure 1, our model is composed of a diffusion module and a generation module, which learns semantic information of vessels via adversarial learning. Specifically, based on the observation that the diffusion model estimates the noise added to the perturbed input data, and the adversarial learning model generates images for given the noisy vectors, we can naturally connect the diffusion model with the adversarial model. This allows our model not only to generate images in real time but also to segment vessels with robustness to noises and various modalities. Here, inspired by the spatiallyadaptive denormalization (SPADE) layer (Park et al., 2019) that is effective in image synthesis given semantic masks, we present a switchable version of SPADE in the generation module to estimate vessel segmentation maps and mask-based fake angiograms simultaneously. This can yield a synergy effect in learning vessel representation by extracting proper features for angiogram synthesis.\n\nMore specifically, as shown in Figure 1, for given unpaired background images and angiography images that are taken before and after injection of the contrast agent, there are two paths for feeding the inputs into our proposed model: (A) when the real angiography images are given, our model without the SPADE estimates vessel segmentation maps; (B) when the background images are given, our model with the SPADE generates synthetic angiograms that composite vessel-like semantic masks with the input backgrounds. Also, as each vessel-like semantic mask in the (B) path can be regarded as the pseudo-label for the generated angiography image, by feeding the synthetic angiograms into the (A) path again, we apply the cycle consistency between the segmentation maps and the labels of fractal masks to capture semantic information of vessels. In addition, by designing the diffusion module to intensively learn the background signal, we let the module consider vessel structures of angiography images as outlier when estimating the latent feature. Thereby, vessel structures represented in the output of the diffusion module can guide the generation module to effectively segment the vessels.\n\nWe build our model on X-ray coronary angiography using XCAD dataset (Ma et al., 2021) and apply to several different blood vessel datasets, including retinal images. Experimental results show that our method outperforms several baseline methods by large margins for vessel segmentation tasks in the absence of labeled data. The main contributions are summarized as:\n\n1. We propose a diffusion adversarial representation model, a non-iterative version of diffusion model for image generation, and apply it for self-supervised vessel segmentation.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nSpecifically, the latent features of our diffusion module provide vessel information and thus improve the segmentation performance.\n\n2. Through the proposed generation module with switchable SPADE layers, our model not\n\nonly generates synthetic angiography images but also segments vessel structures.\n\n3. Experimental results verify that our model achieves superior segmentation performance by learning vessel representations. In particular, although the model is trained using X-ray coronary angiograms, it provides the state-of-the-art performance for un-/self-supervised retinal vessel segmentation as well, confirming the generalization capability of the model.\n\n2 BACKGROUNDS AND RELATED WORKS\n\nDenoising diffusion probabilistic model Diffusion model (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song & Ermon, 2019) is one of generative models that sample realistic data by learning the distribution of real images. In particular, the denoising diffusion probabilistic model (DDPM) (Ho et al., 2020) with a score matching has been shown superior performance in image generation. Specifically, DDPM learns the Markov chain to convert the Gaussian noise distribution xT ∼ N (0, I) into the target distribution x0. In the forward diffusion process, the noise is gradually added the noise to the data by:\n\nq(xt|xt−1) = N (xt; (cid:112)1 − βtxt−1, βtI), (1) where βt ∈ [0, 1] is a fixed variance. Accordingly, a noisy target xt distribution from the data x0 is represented as:\n\nq(xt|x0) = N (xt;\n\nαtx0, (1 − αt)I),\n\n√\n\nwhere αt = Πt\n\ns=1(1 − βs). Then, DDPM is trained to approximate reverse diffusion process:\n\npθ(xt−1|xt) = N (xt−1; μθ(xt, t), σ2\n\nt I),\n\nwhere σt is a fixed variance, and μθ is a parameterized mean with the noise predictor εθ:\n\nμθ(xt, t) =\n\n√\n\n1 1 − βt\n\nxt −\n\nβt√\n\n1 − αt\n\n(cid:18)\n\n(cid:19)\n\nεθ(xt, t)\n\n.\n\nThus, in the generative process, the sample can be obtained from the Gaussian noise by the iterative denoising steps: xt−1 = μθ(xt, t) + σtz, where z ∼ N (0, I).\n\nThrough this stochastic process, DDPM provides diverse realistic samples and has been exploited in many applications, including super-resolution (Chung et al., 2022; Saharia et al., 2021), inpainting (Lugmayr et al., 2022), and colorization (Song et al., 2020; Saharia et al., 2022). However, the application study of semantic segmentation is limited. Although several works (Baranchuk et al., 2021; Amit et al., 2021) are recently presented to solve high-level vision problems, they require annotated data to train the models.\n\nSelf-supervised vessel segmentation For the vessel segmentation task, it is difficult to obtain fine-grained labels for supervised learning, since the vessel has complex structures with numerous tiny branches. While this label scarcity issue can be alleviated by semi- or unsupervised learning, fully unsupervised methods to segment the tiny vessels with reasonable performance are relatively scarce. In fact, recent unsupervised learning methods trained with natural images have great generalization capability on unseen datasets (Ahn et al., 2021; Chen et al., 2019; Melas-Kyriazi et al., 2022), thus they can be easily adapted to medical image segmentation tasks. However, due to the unique characteristics of angiography, e.g. confusing background factors and sophisticated vessel structures, any unsupervised methods designed for natural image segmentation may get degraded performance when they are applied to vessel segmentation of noisy angiography images. As a type of unsupervised learning, self-supervised learning also has been introduced to utilize self-generated supervisory labels from data themselves to efficiently learn target representations in various medical image segmentation tasks and has demonstrated its potential (Mahmood et al., 2019; Ma et al., 2021; Oh & Ye, 2021). Specifically, Ma et al. (2021) introduces an end-to-end adversarial learning framework for vessel segmentation with the CycleGAN (Zhu et al., 2017) structure, which learns realistic angiogram generation that adds fractal-guided pseudo labels to the background images. However, the simple arithmetic operation for synthetic vessel generation often fails to yield realistic pseudovessel images, thus training the adversarial networks using unrealistic synthetic images is difficult to produce optimal segmentation performance.\n\n3\n\n(2)\n\n(3)\n\n(4)\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Training flow of our model. The generation module G with the switchable SPADE layers takes εθ and the noisy images, and generates desired outputs corresponding to the paths. xa ta and xb tb denote the noisy angiography and background images, where ta and tb are noise schedules. ˆsv is the generated vessel segmentation, and ˆxa is the synthetic angiography images. sf is the vessel-like fractal masks. Cat denotes the concatenation of images in channel dimension. Ldif f , Ladv, and Lcyc are the diffusion loss, the adversarial loss, and the cycle loss, respectively.\n\n3 DIFFUSION ADVERSARIAL REPRESENTATION LEARNING\n\nIn this section, we describe our novel diffusion adversarial representation learning (DARL) model, tailored for self-supervised vessel segmentation. We call the images before and after injecting the contrast agents into the blood vessels as background and angiography, respectively. Note that due to the different scanning times, these two images have different contrasts and are not aligned, caused by the movements of patients. Thus, as shown in Figure 1, our DARL model is trained on unpaired angiography images xa\n\n0 and background images xb 0.\n\nSpecifically, our model is comprised of a diffusion module εθ to estimate latent features, a generation module G to estimate both the vessel segmentation masks ˆsv and the synthetic angiograms ˆxa, and two discriminators (Ds, Da) to distinguish real and fake images of the vessel masks and the angiograms, respectively. Here, in generating angiography images, we provide the vessel-like fractal masks sf presented by Ma et al. (2021) to the generation module to perform image synthesis based on semantic layouts. Moreover, to estimate the segmentation maps and angiography images effectively, we design the generation module with novel switchable SPADE layers, where the SPADE Park et al. (2019) facilitates the semantic image synthesis.\n\nGeneration module with switchable SPADE layers As illustrated in Figure 1, the proposed generation module consists of N residual blocks (ResnetBlock) that have switchable SPADE (SSPADE) layers. Note that the (A) and (B) paths of our model are implemented simultaneously by sharing the learnable parameters except for the S-SPADE layers. Then, let v ∈ RM ×C×H×W be the feature map in the ResnetBlock, where M , C, H, and W are the size of batch, channel, height, and width, respectively. The switchable SPADE layer normalizes feature maps differently depending on the existence of an input mask s:\n\nv =\n\n(cid:26)SPADE(v, s),\n\nIN(v),\n\nif mask s is given, otherwise,\n\n(5)\n\nwhere IN is the instance normalization (Ulyanov et al., 2017). So, when our model is given the fake vessel mask sf , the SPADE is computed by:\n\nxm,c,h,w = γc,h,w(sf )\n\nxm,c,h,w − μc σc\n\n+ βc,h,w(sf ),\n\n(6)\n\nwhere xm,c,h,w denotes the (m, c, h, w)-th element of the feature tensor v, (μc, σc) are the mean and standard deviation of the feature map in channel c, and (γc,h,w, βc,h,w) are learned modulation parameters during training.\n\nThus, in the (A) path, given the noisy angiogram xa module G estimates the vessel segmentation masks ˆsv without the SPADE:\n\nt and the latent feature εθ(xa\n\nt , t), the generation\n\n(7) On the other hand, in the (B) path that provides the fractal mask sf , the generation module taking the noisy background xb\n\nt, t)) synthesizes the fake angiograms ˆxa:\n\nt and its latent feature G(εθ(xb\n\nt , t); 0).\n\nˆsv = G(εθ(xa\n\nˆxa = G(εθ(xb\n\nt, t); sf ).\n\n(8)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n3.1 NETWORK TRAINING\n\nIn contrast to the DDPM that pretrains the diffusion model, our method trains the diffusion module, generation module, and discriminators simultaneously using an adversarial learning framework. Figure 2 depicts the detailed training flow of our model. There are two distinct paths: (A) one feeds 0 into the model to provide vessel masks ˆsv, and (B) the other takes the real the real angiograms xa backgrounds xb 0 and the fractal masks sf for the model to generate fake angiograms ˆxa. Here, as shown in Figure 2(B), since the input fractal masks can be regarded as vessel segmentation labels of the fake angiograms, we forward the fake angiograms generated through the (B) path to the (A) path, and apply cycle consistency between the estimated segmentation masks and the fractal masks to capture the vessel information.\n\n3.1.1 LOSS FUNCTION\n\nTo train the model, we employ LSGAN (Mao et al., 2017) framework, which leads to the alternating application of the following two optimization problems:\n\nmin θ,G\n\nLG(εθ, G, Ds, Da), min Ds,Da\n\nLD(εθ, G, Ds, Da),\n\n(9)\n\nwhere LG, and LD denotes the losses for the diffusion/generator and discriminator, respectively, which are given by:\n\nLG(εθ, G, Ds, Da) = Ldif f (εθ) + αLG LD(εθ, G, Ds, Da) = LDs\n\nadv(εθ, G, Ds) + LDa\n\nadv(εθ, G, Da),\n\nadv(εθ, G, Ds, Da) + βLcyc(εθ, G),\n\n(10)\n\n(11)\n\nwhere α and β are hyperparameters, Ldif f is the diffusion loss, Ladv is adversarial loss, and Lcyc is cyclic reconstruction loss. The detailed description of each loss function is as follows.\n\nDiffusion loss Recall that the diffusion module learns the distribution of images to estimate meaningful latent features of the inputs. We follow the standard loss for DDPM training (Ho et al., 2020):\n\nLdif f (εθ) := Et,x0,ε\n\n∥ε − εθ(\n\n(cid:104)\n\n√\n\nαtx0 +\n\n√\n\n1 − αtε, t)∥2(cid:105)\n\n.\n\n(12)\n\nwhere ε ∼ N (0, I). In particular, to let the diffusion module represent the vessels of angiograms effectively, we define the diffusion loss on the background images, i.e. x0 = xb 0 in the (B) path and set the sampling schedule in t ∈ [0, T ]. Accordingly, the diffusion module is trained intensively to learn the background image signal, allowing the module in the (A) path to regard the vessel structures of the angiograms as outlier and represent vessels in the latent features.\n\nAdversarial loss To generate both vessel segmentation masks and synthetic angiograms without the ground-truth labels, the proposed model is trained by adversarial learning using the two discriminators Ds and Da. As shown in Figure 1, the discriminator Ds attempts to distinguish the estimated segmentation masks ˆsv from the real fractal mask sf (in the (A) path), while the discriminator Da tries to discriminate between the generated angiograms ˆxa and the real aniography images xa 0 (in the (B) path). As we employ LSGAN (Mao et al., 2017), the adversarial loss of generator LG adv can be formulated by:\n\nLG\n\nadv(εθ, G,Ds,Da) = Exa [(Ds(G(εθ(xa); 0)) − 1)2] + Exa,sf [(Da(G(εθ(xb); sf )) − 1)2].\n\n(13)\n\nOn the other hand, the discriminators are trained to compete against the generator with the adversarial loss functions, LDs\n\nadv and LDa adv, which are defined by: 1\n1 2\n2 1\n1 2\n2\n\nEsf [(Ds(sf ) − 1)2] +\n\n0) − 1)2] +\n\n[(Da(xa\n\nExa\n\n0\n\nLDs\n\nadv(εθ, G, Ds) =\n\nLDa\n\nadv(εθ, G, Da) =\n\nExa [(Ds(G(εθ(xa); 0)))2],\n\nExa,sf [(Da(G(εθ(xb); sf )))2].\n\n(14)\n\n(15)\n\nThis adversarial loss enables the single generator G to fool the discriminator Ds and Da, by generating realistic segmentation masks ˆsv = G(εθ(xa); 0) and angiograms ˆxa = G(εθ(xb); sf ). In contrast, the discriminators attempt to distinguish these generated images being fake and the real images of sf and xa\n\n0 being real.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Vessel segmentation according to the noise level ta. Our model estimates the segmentation masks ˆsv using the latent features εθ for the noisy angiograms xa\n\nta . sv is the ground-truth label.\n\nCyclic reconstruction loss For the generator G to capture the semantic information of the vessels, we also constrain our model with the cyclic reconstruction loss on the fractal masks. Specifically, as the vessel-like fractal masks sf can be labels for the synthetic angiograms ˆxa generated in the (B) path, we feed the ˆxa into our model and reconstruct the fractal masks by the (A) path. Therefore, the cyclic reconstruction loss is computed between the reconstructed segmentation masks and the real fractal masks, which can be written by:\n\nLcyc(εθ, G) = Exb,sf [||G(εθ(G(εθ(xb); sf )); 0) − sf ||1]. Here, we solve the segmentation problem as a vessel mask image generation, which is why we use L1 loss in the cyclic loss.\n\n(16)\n\n3.1.2\n\nIMAGE PERTURBATION FOR THE MODEL INPUT\n\n0 and xb Given real images of xa and noisy background images xb based on the forward diffusion process (2):\n\n0, our diffusion module takes noisy angiograms xa ta in the (A) path tb in the (B) path as the input, in which each noisy image is sampled\n\n√\n\nαtx0 +\n\n√\n\n1 − αtε,\n\n(17)\n\nxt =\n\nwhere ε ∼ N (0, I), and both of ta and tb are uniformly sampled time step in [0, T ]. Here, for the diffusion module not only to learn the background image signal in the (B) path but also to provide useful information for the generation module to segment the vessel structures under even certain noisy angiogram images in the (A) path, we sample ta in the range of [0, Ta] where Ta < T . Empirically, we found that this makes our model learn vessel representations robust to the noise.\n\n3.2\n\nINFERENCE OF VESSEL SEGMENTATION\n\nThe inference phase of DARL is different from the conventional diffusion model in that our model do not require iterative reverse process, similar to the recent diffusion-based unsupervised learning method called DiffuseMorph (Kim et al., 2021). Specifically, once the proposed DARL is trained, in the inference, we can obtain the vessel segmentation masks of angiograms from the (A) path by one step. For the noisy angiograms xa ta given by the forward diffusion process (17), our model provides the vessel segmentation masks using the latent features εθ(xa , ta) estimated from the ta diffusion module. As shown in Figure 3, our model can generate the segmentation masks for any noise level ta within a certain range (i.e. [0, Ta]). Nevertheless, since the angiography image xa 0\ncan be considered as one of the clean target images, the closer ta is to zero, the better the vessel segmentation performance. Therefore, we test our model by setting ta = 0.\n\n4 EXPERIMENTS\n\nIn this section, we thoroughly evaluate the vessel segmentation performance of our method. We firstly compare the proposed DARL to existing unsupervised and self-supervised baseline models on various angiography datasets, including X-ray coronary angiography and retinal images. Also, we study the noise robustness of our model. Then, we analyze the success of our model in vessel representation and conduct an ablation study.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nDatasets To realize the self-supervised learning framework, we train our model with the publicly available unlabeled X-ray coronary angiography disease (XCAD) dataset obtained during stent placement surgery and generated synthetic fractal masks (Ma et al., 2021). When training the network, each angiography and background data is independently sampled. Also, in testing, we utilize external 134 XCA (Cervantes-Sanchez et al., 2019) and 30 XCA (Hao et al., 2020) datasets. Furthermore, we evaluate cross-organ generalization capability on retinal imaging datasets; DRIVE (Staal et al., 2004) and STARE (Hoover & Goldbaum, 2003). Details of the datasets are in Appendix B.\n\nImplementation details Our model is implemented by employing the network architectures proposed in DDPM (Ho et al., 2020) and SPADE (Park et al., 2019) for the diffusion module and the generation module, respectively. Also, for the discriminators, we use the network of PatchGAN (Isola et al., 2017). To train the model, we set the number of time steps as T = 2000 with the linearly scheduled noise levels from 10−6 to 10−2. Within this range, we sample the noisy angiograms by setting Ta to 200. Also, we set the hyperparameters of loss function as α = 0.2 and β = 5. Our model is optimized by using the Adam algorithm (Kingma & Ba, 2014) with a learning rate of 5×10−6 on a single GPU card of Nvidia Quadro RTX 6000. We train the model for 150 epochs, and the model in the epoch with the best performance on the validation set is used for test data. All the implementations are done using the library of PyTorch (Paszke et al., 2019) in Python. The details of network structures and hyperparameter setting can be found in Appendix.\n\nBaseline methods and metrics We compare our model to several baseline methods of un-/selfsupervised learning, which do not require ground-truth vessel labels. For unsupervised learning methods, we utilize Spatial-Guided Clustering (SGC) (Ahn et al., 2021), Redrawing (Chen et al., 2019), and Deep Spectral (DS) (Melas-Kyriazi et al., 2022). For self-supervised learning methods, we employ Self-supervised Transformer with Energy-based Graph Optimization (STEGO) (Hamilton et al., 2022), Deep Adversarial (DA) (Mahmood et al., 2019), and Self-Supervised Vessel Segmentation (SSVS) (Ma et al., 2021). All these methods are implemented under identical training conditions to our model, unless the method needs no training procedure. For baseline methods that\n\nFigure 4: Visual comparison results on the vessel segmentation of various angiography images.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Quantitative evaluation results on the vessel segmentation of various angiography images.\n\nData\n\nMetric\n\nUnsupervised\n\nSelf-supervised\n\nSGC\n\nRedrawing\n\nDS\n\nSTEGO\n\nDA\n\nSSVS\n\nOurs\n\nXCAD\n\nIoU Dice Precision\n\n0.060±0.034 0.111±0.060 0.062±0.034\n\n0.059±0.032 0.109±0.056 0.139±0.081\n\n0.366±0.105 0.526±0.131 0.469±0.127\n\n0.146±0.070 0.249±0.103 0.152±0.077\n\n0.375±0.066 0.542±0.073 0.557±0.115\n\n0.410±0.087 0.575±0.091 0.590±0.119\n\n0.471±0.076 0.636±0.072 0.701±0.115\n\nExternal test: Coronary angiography\n\n134 XCA\n\n30 XCA\n\nIoU Dice Precision\n\nIoU Dice Precision\n\n0.045±0.035 0.085±0.063 0.047±0.036\n\n0.083±0.039 0.150±0.064 0.090±0.041\n\n0.056±0.018 0.105±0.033 0.058±0.019\n\n0.048±0.022 0.091±0.040 0.144±0.074\n\n0.256±0.110 0.394±0.159 0.280±0.123\n\n0.339±0.086 0.499±0.113 0.525±0.130\n\n0.134±0.081 0.228±0.109 0.136±0.088\n\n0.191±0.072 0.314±0.100 0.200±0.081\n\n0.190±0.155 0.291±0.217 0.506±0.201\n\n0.298±0.109 0.447±0.148 0.612±0.174\n\n0.318±0.128 0.468±0.156 0.592±0.125\n\n0.324±0.146 0.468±0.193 0.613±0.212\n\n0.426±0.059 0.595±0.058 0.781±0.118\n\n0.427±0.184 0.572±0.205 0.729±0.152\n\nCross-modality test: Retinal imaging\n\nDRIVE\n\nSTARE\n\nIoU Dice Precision\n\nIoU Dice Precision\n\n0.063±0.055 0.115±0.093 0.069±0.061\n\n0.055±0.045 0.101±0.077 0.058±0.047\n\n0.057±0.033 0.105±0.059 0.199±0.155\n\n0.074±0.048 0.134±0.080 0.227±0.157\n\n0.217±0.143 0.333±0.201 0.243±0.175\n\n0.180±0.141 0.281±0.201 0.205±0.172\n\n0.152±0.073 0.257±0.106 0.169±0.100\n\n0.125±0.076 0.216±0.109 0.135±0.092\n\n0.245±0.090 0.386±0.117 0.503±0.218\n\n0.237±0.122 0.367±0.167 0.427±0.233\n\n0.314±0.101 0.469±0.119 0.549±0.216\n\n0.311±0.148 0.454±0.185 0.490±0.230\n\n0.372±0.148 0.525±0.161 0.617±0.271\n\n0.368±0.191 0.508±0.216 0.537±0.280\n\nrequire heuristic thresholds, optimal performance is achieved by selecting data-specific thresholds within the range from 0.2 to 0.8 in increments of 0.1. To quantitatively evaluate the segmentation performance, we compute Intersection over Union (IoU), Dice similarity coefficient, and Precision.\n\n4.1 EXPERIMENTAL RESULTS\n\nFigure 4 shows the vessel segmentation masks from the baseline methods and our proposed method on three different coronary angiography datasets and two retinal imaging datasets. Quantitative evaluation results of the methods are presented in Table 1. The analysis of the results is as follows.\n\nComparison of ours to baselines When we compare the proposed method to the baselines, our model segments vessel structures including tiny branches more accurately. Also, as shown in Table 1, our model consistently achieves the SOTA performance by large margin compared to existing unsupervised and self-supervised methods. In specific, our network shows significantly improved precision scores, which demonstrates advantages of our DARL that effectively differentiates foreground vessel structure and eliminates false positive signals from the noisy backgrounds.\n\nGeneralization capability To verify that our trained DARL can be generally used for various vessel images taken from different machines or different anatomic region-of-interests (ROI), we evaluate the generalization capability by applying the models only trained on the XCAD dataset to the other datasets. First, for the external 134 XCA and 30 XCA datasets which have different resolutions and noise distributions to those of the XCAD dataset, as shown in Figure 4 and Table 1, our model achieves higher performance than the others. Also, with the DRIVE and STARE retinal datasets that have unseen data distributions due to the different modalities from the XCAD, our DARL shows the most promising cross-organ generalization performance. This may come from the proposed framework that reuses the generated angiography images for the segmentation process through the cycle path, diversifying the input data distribution. Also, the diffusion module learning the stochastic diffusion process enables our model to be used in general for vessel segmentation.\n\nRobustness to noises As X-ray images are often acquired under low-dose radiation exposure to reduce potential risks, we further evaluate the performance of our model on simulated noisy angiograms. Using the XCAD dataset, we add Gaussian noise to the angiogram with different levels of σ =10, 25, and 50. We show the segmentation results according to the noise levels in Figure 4. Also, we report the quantitative evaluation results in Table 2. It is noteworthy that our DARL is the only method to segment vessel structures with reasonable performance under noise corruption. Since the proposed segmentation method is trained through the diffusion module that perturbs the input images, the model is highly robust to segment vessel structure even from the noisy data.\n\nLatent representation To study the origin of the performance improvement, in Figure 5, we show the latent features εθ(xt, t) given x0 for (A) the angiography x0 = xa 0 and (B) the\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Results of noise robustness test according to the Gaussian noise with σ.\n\nσ\n\nMetric\n\nUnsupervised\n\nSelf-supervised\n\nSGC\n\nRedrawing\n\nDS\n\nSTEGO\n\nDA\n\nSSVS\n\nOurs\n\n10\n\n25\n\n50\n\nIoU Dice Precision\n\nIoU Dice Precision\n\nIoU Dice Precision\n\n0.066±0.033 0.122±0.059 0.069±0.035\n\n0.069±0.035 0.128±0.061 0.072±0.036\n\n0.070±0.025 0.130±0.045 0.072±0.026\n\n0.052±0.031 0.096±0.053 0.126±0.077\n\n0.036±0.021 0.069±0.039 0.095±0.058\n\n0.020±0.012 0.040±0.022 0.061±0.038\n\n0.331±0.104 0.487±0.133 0.480±0.135\n\n0.232±0.094 0.366±0.132 0.446±0.159\n\n0.077±0.065 0.136±0.109 0.221±0.168\n\n0.144±0.073 0.245±0.107 0.157±0.091\n\n0.118±0.064 0.206±0.095 0.144±0.115\n\n0.060±0.050 0.108±0.088 0.076±0.067\n\n0.353±0.065 0.519±0.073 0.481±0.104\n\n0.247±0.072 0.391±0.092 0.371±0.106\n\n0.102±0.056 0.180±0.091 0.169±0.094\n\n0.258±0.079 0.404±0.099 0.477±0.117\n\n0.059±0.033 0.109±0.058 0.149±0.082\n\n0.021±0.013 0.041±0.025 0.060±0.038\n\n0.451±0.080 0.617±0.076 0.710±0.115\n\n0.389±0.088 0.554±0.092 0.727±0.119\n\n0.269±0.081 0.417±0.101 0.716±0.147\n\nbackgrounds x0 = xb 0 with t = 100, respectively. In contrast to the (B) path, the latent representation in the (A) path emphasizes the vessel structures. This implies that although there are no ground-truth labels, our model learns the background image representation so that the vessel structure can be captured as outlier, leading to improved segmentation performance.\n\nFigure 5: Estimated latent features εθ in the (A) and (B) paths of our model.\n\nAblation study Table 3 shows the evaluation results of several ablation studies. Implementation details and visual results are in Appendix D.1. (a) Our model without the diffusion module and Ldif f shows lower performance by about 2% for all metrics compared to our model, which suggests that the diffusion module guides the generation module to extract vessel representation accurately. (b) The generation module without the proposed S-SPADE layers is degraded by more than 1% over (a) for all metrics, verifying that our SPADE-based unified generator effectively captures vessel semantic information through the synergy of learning both image segmentation and generation. (c) Through the implementation of our model without the proposed cyclic loss Lcyc, we verify that Lcyc allows our model to segment proper vessel regions. (d) When training our model by converting the L1 loss for Lcyc to the cross-entropy (CE) loss, the performance is much worse than ours in all metrics, which implies that our approach using L1 loss for the cycle path is proper to obtain the vessel masks.\n\nTable 3: Results of ablation study on the proposed model and loss function.\n\nMethod\n\nOurs\n\n(a) (b)\n\n(c) (d)\n\nModule\n\nLoss function\n\nDiffusion ✓\n\nGeneration ✓\n\n✓ w/o S-SPADE ✓\n✓\n\n✓ ✓\n\nLdif f Ladv\n\n✓\n\n✓ ✓\n\n✓\n\n✓ ✓\n\n✓ ✓\n\nLcyc ✓\n\n✓ ✓\n\nL1→CE\n\nIoU\n\nMetric\n\nDice\n\nPrecision\n\n0.471±0.076\n\n0.636±0.072\n\n0.701±0.115\n\n0.449±0.077 0.439±0.080\n\n0.322±0.055 0.346±0.084\n\n0.616±0.074 0.606±0.080\n\n0.485±0.064 0.508±0.094\n\n0.646±0.106 0.620±0.111\n\n0.580±0.112 0.672±0.147\n\n5 CONCLUSION\n\nWe present a non-iterative diffusion model called DARL for self-supervised vessel segmentation. Our model composed of the diffusion and generation modules learns vessel representation without labels via adversarial learning, in the guidance of latent features estimated from the diffusion module. Also, through the proposed switchable SPADE layer, we generate synthetic angiograms as well as vessel segmentation masks, leading to learning semantic information about vessels more effectively. Although the diffusion module training is combined with other loss functions, the inference is not iterative but only done in one step, which makes it faster and unique compared to the existing diffusion models. Using various medical vessel datasets, we verify that our model is much superior to existing un-/self-supervised learning methods. Moreover, thanks to the diffusion module, our model is robust to image diversity and noise, suggesting that our model can be an important platform for designing a general vessel segmentation model.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREPRODUCIBILITY\n\nSource code is available at https://github.com/bispl-kaist/DARL.\n\nACKNOWLEDGMENTS\n\nThis work was supported in part by the National Research Foundation of Korea under Grant NRF2020R1A2B5B03001980, in part by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT, Ministry of Science and ICT) (No. 2022-0-00984, Development of Artificial Intelligence Technology for Personalized Plugand-Play Explanation and Verification of Explanation), in part by the MSIT(Ministry of Science and ICT), under the ITRC(Information Technology Research Center) support program(IITP-2022-20200-01461) supervised by the IITP, and in part by the KAIST Key Research Institute (Interdisciplinary Research Group) Project.\n\nREFERENCES\n\nEuijoon Ahn, Dagan Feng, and Jinman Kim. A spatial guided self-supervised clustering network for medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 379–388. Springer, 2021.\n\nTomer Amit, Eliya Nachmani, Tal Shaharbany, and Lior Wolf. Segdiff: Image segmentation with\n\ndiffusion probabilistic models. arXiv preprint arXiv:2112.00390, 2021.\n\nDmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Labelefficient semantic segmentation with diffusion models. In International Conference on Learning Representations, 2021.\n\nFernando Cervantes-Sanchez,\n\nIvan Cruz-Aceves, Arturo Hernandez-Aguirre, Martha Alicia Hernandez-Gonzalez, and Sergio Eduardo Solorio-Meza. Automatic segmentation of coronary arteries in x-ray angiograms using multiscale analysis and artificial neural networks. ApISSN 2076-3417. doi: 10.3390/app9245507. URL https: plied Sciences, 9(24), 2019. //www.mdpi.com/2076-3417/9/24/5507.\n\nMicka ̈el Chen, Thierry Arti`eres, and Ludovic Denoyer. Unsupervised object segmentation by re-\n\ndrawing. Advances in neural information processing systems, 32, 2019.\n\nYang Chen, Luyao Shi, Qianjing Feng, Jian Yang, Huazhong Shu, Limin Luo, Jean-Louis Coatrieux, and Wufan Chen. Artifact suppressed dictionary learning for low-dose ct image processing. IEEE transactions on medical imaging, 33(12):2271–2292, 2014.\n\nHyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12413–12422, 2022.\n\nWeijian Cong, Jian Yang, Danni Ai, Yang Chen, Yue Liu, and Yongtian Wang. Quantitative analysis of deformable model-based 3-d reconstruction of coronary artery from multiple angiograms. IEEE Transactions on Biomedical Engineering, 62(8):2079–2090, 2015.\n\nJingfan Fan, Jian Yang, Yachen Wang, Siyuan Yang, Danni Ai, Yong Huang, Hong Song, Aimin Hao, and Yongtian Wang. Multichannel fully convolutional network for coronary artery segmentation in x-ray angiograms. Ieee Access, 6:44635–44643, 2018.\n\nMark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T Freeman. arXiv preprint\n\nUnsupervised semantic segmentation by distilling feature correspondences. arXiv:2203.08414, 2022.\n\nDongdong Hao, Song Ding, Linwei Qiu, Yisong Lv, Baowei Fei, Yueqi Zhu, and Binjie Qin. Sequential vessel segmentation via deep channel attention network. Neural Networks, 128:172– 187, 2020. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2020.05.005. URL https: //www.sciencedirect.com/science/article/pii/S0893608020301672.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\n\nNeural Information Processing Systems, 33:6840–6851, 2020.\n\nAdam Hoover and Michael Goldbaum. Locating the optic nerve in a retinal image using the fuzzy convergence of the blood vessels. IEEE transactions on medical imaging, 22(8):951–958, 2003.\n\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.\n\nImage-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1125–1134, 2017.\n\nBoah Kim, Inhwa Han, and Jong Chul Ye. Diffusemorph: Unsupervised deformable image registration along continuous trajectory using diffusion models. arXiv preprint arXiv:2112.05149, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nMax WK Law and Albert CS Chung. Efficient implementation for spherical flux computation and its application to vascular segmentation. IEEE transactions on image processing, 18(3):596–612, 2009.\n\nAndreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. In Proceedings of the\n\nRepaint: IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11461–11471, 2022.\n\nInpainting using denoising diffusion probabilistic models.\n\nYuxin Ma, Yang Hua, Hanming Deng, Tao Song, Hao Wang, Zhengui Xue, Heng Cao, Ruhui Ma, and Haibing Guan. Self-supervised vessel segmentation via adversarial learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7536–7545, 2021.\n\nFaisal Mahmood, Daniel Borders, Richard J Chen, Gregory N McKay, Kevan J Salimian, Alexander Baras, and Nicholas J Durr. Deep adversarial training for multi-organ nuclei segmentation in histopathology images. IEEE transactions on medical imaging, 39(11):3257–3267, 2019.\n\nXudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 2794–2802, 2017.\n\nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8364–8375, 2022.\n\nEbrahim Nasr-Esfahani, Shadrokh Samavi, Nader Karimi, SM Reza Soroushmehr, Kevin Ward, Mohammad H Jafari, Banafsheh Felfeliyan, B Nallamothu, and Kayvan Najarian. Vessel extraction in x-ray angiograms using deep learning. In 2016 38th Annual international conference of the IEEE engineering in medicine and biology society (EMBC), pp. 643–646. IEEE, 2016.\n\nYujin Oh and Jong Chul Ye. CXR Segmentation by AdaIN-based Domain Adaptation and Knowl-\n\nedge Distillation. arXiv preprint arXiv:2104.05892, 2021.\n\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2337–2346, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorchan-imperative-style-high-performance-deep-learning-library.pdf.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015.\n\nChitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. arXiv preprint arXiv:2104.07636, 2021.\n\nChitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pp. 1–10, 2022.\n\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256–2265. PMLR, 2015.\n\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\n\nAdvances in Neural Information Processing Systems, 32, 2019.\n\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben In Interna-\n\nPoole. Score-based generative modeling through stochastic differential equations. tional Conference on Learning Representations, 2020.\n\nJoes Staal, Michael D Abr`amoff, Meindert Niemeijer, Max A Viergever, and Bram Van Ginneken. IEEE transactions on medical\n\nRidge-based vessel segmentation in color images of the retina. imaging, 23(4):501–509, 2004.\n\nVadim Sushko, Edgar Sch ̈onfeld, Dan Zhang, Juergen Gall, Bernt Schiele, and Anna Khoreva. You only need adversarial supervision for semantic image synthesis. arXiv preprint arXiv:2012.04781, 2020.\n\nMaryam Taghizadeh Dehkordi, Ali Mohamad Doost Hoseini, Saeed Sadri, and Hamid Soltanianzadeh. Local feature fitting active contour for segmenting vessels in angiograms. IET Computer Vision, 8(3):161–170, 2014.\n\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6924–6932, 2017.\n\nShaoyan Xia, Haogang Zhu, Xiaoli Liu, Ming Gong, Xiaoyong Huang, Lei Xu, Hongjia Zhang, and Jialong Guo. Vessel segmentation of x-ray coronary angiographic image sequence. IEEE Transactions on Biomedical Engineering, 67(5):1338–1348, 2019.\n\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 2223–2232, 2017.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA DETAILS OF NETWORK ARCHITECTURE\n\nIn this section, we provide details of the generator G proposed in our diffusion adversarial representation learning (DARL) model, which is composed of the diffusion module and the generation module. For the diffusion module, we adapt the network architecture of DDPM (Ho et al., 2020) that has U-Net (Ronneberger et al., 2015) structure, as described in Table 4. The generation module is composed of four consecutive residual blocks (He et al., 2016) with switchable spatially-adaptive denormalization (SPADE) layers, as described in Table 5.\n\nTable 4: Detailed network architecture of the diffusion module. For each block (blk), Ci,j is the convolution layer with i × i kernel and stride length of j, RSi pairs are entry points for residual shortcut path within a block unit, RB is the residual block module, and SA is the self-attention module. GN is the group normalization, and Ch indicates the size of output channel dimension.\n\nBlk\n\n1 2\n3 4\n5 6\nMid\n\nDownstream\n\nUpstream\n\nDiffusion module\n\nRS3 C3,1 RS1 RS3 C3,2 RS1 RS3 C3,2 RS1 C3,2 RS1 RS3 C3,2 RS1 RB SA RS2 RB SA RS3 RS3 C3,2 RS1\n\nRS2 RS2 RS2 RS2\n\nRB RB RB RB\n\nRB RB RB RB\n\nRS2\n\nRB\n\nRB RB\n\nSA\n\nRB RB RB RB\n\nRS3 CB RB RS3 UP RB RS3 UP RB RS3 UP RB RS3 RB SA RS2 RB SA RS1 RB SA UP RS3 UP RB\n\nRS2 RS2 RS2 RS2\n\nRS1 RS1 RS1 RS1\n\nRB RB RB RB\n\nRS2\n\nRS1\n\nRB\n\nRB RB\n\nCh\n\n1 64 128 128 256 256\n\nNote: RB = [RSn - GN - Swish - C3,1 - GN - Swish - C3,1 - RSn], UP = [Upsample - C3,1], CB = [GN - Swich - C3]\n\nSA = [GN - C1 - C1 ],\n\nTable 5: Detailed network architecture of the generation module. UP is the nearest neighbor upsampling function, RSi pairs are entry points for residual shortcut path, Ci,j is the convolution layer with i × i kernel and stride length of j, and IN is the instance normalization layer. S-SPADE is the proposed switchable SPADE layer that turns on SPADE if the semantic layout is provided, otherwise turns off SPADE and applies IN. Ch indicates the size of output channel dimension.\n\nStream\n\nIn DownBlock1 DownBlock2 MidResBlock1 MidResBlock2 UpResBlock1 UpResBlock2 Out\n\nScale\n\nConv.\n\nAct.\n\nNorm.\n\nConv.\n\nAct.\n\nNorm.\n\nGeneration module\n\nRS1 RS2 RS3 RS4\n\nUP UP\n\nC7 C3,2 C3,2 C3,1 C3,1 C3,1 C3,1 C7\n\nIN IN IN S-SPADE S-SPADE S-SPADE S-SPADE\n\nReLU ReLU ReLU ReLU\n\nC3,1 C3,1 C3,1 C3,1\n\nReLU ReLU ReLU ReLU ReLU ReLU ReLU\n\nS-SPADE S-SPADE S-SPADE S-SPADE\n\nRS1 RS2 RS3 RS4\n\nCh\n\n64 128 256 256 256 128 64 1\n\nB DETAILS OF DATASET\n\nFor training the network, as described in Table 6, we utilize the XCAD dataset (Ma et al., 2021) which provides a total of 1,621 unlabeled X-ray coronary angiography frames. We use each first frame that is taken before the contrast agent injection as the real background image. We also generate 1,621 synthetic fractal masks by using the fractal synthetic module proposed by (Ma et al., 2021). The fractal masks are synthesized by drawing rectangles with randomly sampled thickness ranging from 15 to 25 pixels on a black background with a size of 512 x 512. Then, local distortions are taken to each rectangle, including affine transformation with a random scale and rotations with a random angle, resulting in generating masks with various shapes and thicknesses. This reduces the effort to match the real vessel thickness distribution, thus, one can simply synthesize such various fractal masks through the fractal synthetic module. Additional 126 angiography images, along with the ground-truth vessel masks annotated by experienced radiologists, are divided into validation and test sets by 10% and 90%, respectively. We subsample all data into 256×256.\n\nFor training the baseline methods, we utilize the same amount of angiography images from the XCAD dataset. In specific, our method and SSVS utilize angiography images, background images,\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nand synthetic fractal masks for training both segmentation and generation paths, but each data is randomly sampled independently. DA utilizes angiography images and fractal masks for adversarial training. For inferencing DS, we utilize pre-trained network parameters. Redrawing, STEGO and SGC are basically clustering-based methods, which need only angiography images. For all these methods, we use the same 256 × 256 images as ours, without any further image processing such as normalization, but augment data through random flipping and 90-degree rotation.\n\nFor external test dataset, we utilize two X-ray coronary angiography (XCA) datasets acquired from different machines. 134 XCA dataset is composed of 134 angiography images with the vessel masks labeled by an expert cardiologist (Cervantes-Sanchez et al., 2019). 30 XCA datset is composed of 30 sequences of angiography images (Hao et al., 2020). We utilize one angiography image from each sequence, along with its corresponding ground-truth vessel mask labeled by experts. All the test images are resized to 512×512. Furthermore, for evaluating cross-organ generalization capability, we utilize retinal imaging datasets. We use DRIVE (Staal et al., 2004) and STARE (Hoover & Goldbaum, 2003) datasets, each of which is composed of 20 retinal images and the corresponding expert-labeled vessel masks. Since retinal imaging is taken under high-resolution, we resize the image into 768×768 and split into 9 patches with 256×256.\n\nTable 6: Detailed dataset for training each (A) segmentation and (B) generation path.\n\nInput\n\nTrain\n\nValidation\n\nTest\n\nXCAD\n\nXCAD\n\nXCAD 134 XCA 30 XCA DRIVE STARE\n\n(A) Segmentation\n\n(B) Generation\n\nAngiogram; xa Ground-truth mask\n\nBackground; xb Fractal mask; sf\n\n1,621 0\n\n1,621 1,621\n\n12 12\n\n- -\n\n114 114\n\n- -\n\n134 134\n\n- -\n\n30 30\n\n- -\n\n20 20\n\n- -\n\n20 20\n\n- -\n\nC ADDITIONAL EXPERIMENTAL RESULTS\n\nC.1 STUDY ON SYNTHETIC ANGIOGRAM GENERATION\n\nAs described in the main paper, a single generation module with the switchable SPADE layers in our model provides both the synthetic angiograms and the vessel segmentation masks by onestep inference, compared to iterative inference steps of other diffusion models. To evaluate the angiogram synthesis performance, we compare our model with the methods of DA and SSVS. These baselines generate the segmentation masks and the synthetic angiograms, but unlike ours, they use two different networks employing the CycleGAN (Zhu et al., 2017) framework. Furthermore, in this study, we adopt an additional baseline method of OASIS (Sushko et al., 2020), one of the SOTA semantic image synthesis models. Under the same condition as ours with an unpaired dataset setting, we train the OASIS model. For all the baseline methods, a total of 1,621 synthetic angiography images are generated using the backgrounds and fractal masks as inputs.\n\nFigure 6 compares the visual results of synthetic angiograms. Compared to the others, our generation module yields the most realistic images that naturally reflect fractal masks on the background and also contain even tiny branches. This verifies that our model maintains consistency on the vessel-like fractal signals, capturing vessel semantic information effectively and leading to the improvement of segmentation performance. Also, we perform the quantitative evaluation on image generation of angiograms and vessel masks using Fr ́echet inception distance (FID) (Heusel et al., 2017). Table 7 shows that our model achieves much lower FID scores than the comparative methods even though the FID is originally designed for evaluating natural image synthesis, which suggests the superiority of ours in generating angiography images. Also, the proposed DARL model provides the most realistic segmentation masks over the other methods.\n\nC.2 STUDY ON HYPERPARAMETER SETTING\n\nIn the main paper, we report the vessel segmentation results from the model trained with α = 0.2 and β = 5 based on the study of hyperparameter setting, which yields optimal performance in our experiments. To study the effects of hyperparameters on the segmentation performance, using the\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: Visual comparison results of the generated fake angiograms. Yellow boxes in ×1 rows are magnified by two or three times in the corresponding bottom rows, respectively. Red triangles indicate remarkable points.\n\nTable 7: Quantitative comparison results using FID score on the image generation of vessel masks and angiograms. Lower FID means that the image generation is more realistic.\n\nImage generation\n\nVessel mask Angiogram\n\nDA\n\n123.20 261.64\n\nSSVS\n\n149.00 191.68\n\nOASIS\n\nN/A 307.50\n\nOurs\n\n93.49 177.59\n\nproposed loss function, we trained our model with the fixed β = 5.0 when adjusting the parameter α. Similarly, α is fixed with 0.2 when β is adjusted. Figure 7 shows graphs of the quantitative evaluation results of IoU, Dice, and Precision metrics according to the hyperparameters of α and β. We can see that our model can learn the semantic vessel segmentation when the parameter α that controls the adversarial loss Ladv is equal to or more than 0.2, though the performance gradually decreases as α increases. Also, the results show that the highest performance for all metrics is achieved when α = 0.2. On the other hand, while our model hardly captures the semantic information of vessels when there is no cycle path in network training (i.e. β = 0), the model can provide plausible vessel segmentation masks as long as the cycle path exists. Also, when we investigate the segmentation performance according to the β that weights the cyclic loss Lcyc, the optimal performance is obtained when β = 5.0.\n\nC.3 STUDY ON ANGIOGRAM PERTURBATION IN MODEL TRAINING\n\nWhen training our model, the background images are perturbed by the forward diffusion process with the uniformly sampled time step tb ∈ [0, T ], whereas the angiograms are perturbed with the time step ta ∈ [0, Ta] where Ta < T . To investigate the effect of time step size Ta for the angiogram perturbation on model performance, we train our model with different time step sizes by setting Ta as 100, 200, 500, and 1000. Figure 8 shows the quantitative evaluation results. When Ta is less than 200, the vessel segmentation performance on the clean angiograms gets better, but the performance is degraded on the simulated low-dose angiograms corrupted by Gaussian noise with σ levels of 25 and 50. Also, when Ta is set over 500, the model shows drastically low performance due to the lack of vascular information from the noisy angiograms. These results imply that the diffusion module can optimally provide latent features including vascular structures as long as the model is trained\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Vessel segmentation performance of our model on the XCAD dataset according to the hyper-parameters of the proposed loss function. Each column shows the average values of quantitative evaluation results with respect to (a) α for the adversarial loss Ladv and (b) β for the cyclic reconstruction loss Lcyc.\n\nin the setting of Ta ≤ 200. Also, the proposed implementation makes our DARL robust to noise, which suggests that our model can segment vascular structures even on low-dose medical images.\n\nFigure 8: Vessel segmentation performance of our model according to the settings of Ta for the angiogram perturbation. Each graph shows the quantitative evaluation results with respect to the Gaussian noise level σ.\n\nC.4 STUDY ON ADVERSARIAL LEARNING WITH TWO DISCRIMINATORS\n\nOur proposed model is trained via adversarial learning with two discriminators Ds and Da that distinguish real and fake segmentation masks and angiograms, respectively. To confirm that this discriminator setting is optimal, we additionally conduct experiments to train our model without either Ds or Da. As reported in Table 8(a), the model trained without Ds slightly increases the precision but degrades the scores for the IoU and Dice metrics. Also, the model trained without Da shows inferior segmentation performance as shown in Table 8(b), which may be due to the failure to generate realistic angiograms, making the model relatively hard to learn vessel semantic information. These results suggest that our model with both Ds and Da is optimal to learn vessel representations.\n\nTable 8: Vessel segmentation performance of our model without the discriminator Ds/Da.\n\nMethod\n\nDiscriminators\n\nDs ✓\n\n✓\n\nDa ✓\n\n✓\n\nOurs\n\n(a) (b)\n\nIoU\n\nMetric\n\nDice\n\nPrecision\n\n0.471±0.076\n\n0.636±0.072\n\n0.701±0.115\n\n0.456±0.081 0.348±0.061\n\n0.622±0.078 0.513±0.069\n\n0.728±0.117 0.496±0.106\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nC.5 STUDY ON DIFFUSION MODEL IN LATENT FEATURE ESTIMATION\n\nRecall that the output of the diffusion module is not a simple latent feature of networks but a score function that has spatial information of the data. To show the effect of the diffusion model in our framework, we additionally study our framework by replacing the diffusion module with the autoencoder model. For a fair comparison, We configure the autoencoder by adapting the same DDPM network architecture (Ho et al., 2020) as ours but removing the time embedding vectors and then train without the diffusion loss. Figure 9 shows the latent features and vessel masks of the angiography images. We can observe that while the autoencoder model estimates latent features that include vessels and other similar confusing structures, our proposed framework with the diffusion module represents vessels better in the latent features and provides more accurate segmentation masks. Table 9 also shows that the autoencoder model achieves inferior performance compared to ours. These results indicate that the latent features from the diffusion module allow the generation module to effectively learn vessel representation.\n\nFigure 9: Visual results of vessel segmentation according to the latent feature estimation models.\n\nTable 9: Quantitative evaluation results according to the latent feature estimation models.\n\nLatent feature estimation\n\nIoU\n\nDice\n\nAutoencoder Diffusion (Ours)\n\n0.399±0.076 0.471±0.076\n\n0.566±0.079 0.636±0.072\n\nPrecision\n\n0.621±0.109 0.701±0.115\n\nC.6 STUDY ON MODEL CONTRIBUTION\n\nThe contribution of proposed DARL model is further analyzed under supervised, semi-supervised, and self-supervised learning scenarios, and each result on various datasets are described in Table 10.\n\nSupervised Firstly, we apply the proposed framework as a supervised model. Here, since there are no labeled pairs in the training dataset, we conduct two supervised segmentation experiments utilizing the 12 pairs of labeled validation data: 1) training our model through only the (A) path, and 2) training our model by giving the labeled data to the (A) path while keeping the (B) path. To evaluate the performance of supervised approaches, we report the results of the best model among models saves at 10 epoch intervals. In Table 10, we can observe that in the few-label scenario, our DARL framework contributes to the segmentation path to achieve superior performance, leading to getting better than the supervised model only with the (A) path. Moreover, it is noteworthy that the performance of our self-supervised model surpasses the supervised approach only using the (A) path by large margins, even though ours is trained without any supervised data.\n\nSemi-supervised As the outputs in the (A) and (B) paths of our DARL model can be used as pseudo labels or inputs for vessel segmentation, we train the segmentation model in a semisupervised manner. In specific, we prepare the segmentation network that is identical network architecture to that of our generation module G for a fair comparison. Then, we train the network by utilizing the (A) path outputs ˆsv as paired pseudo-label for input xa. Similarly, we train the network by using the (B) path outputs ˆxa as paired pseudo-input for label sf . As reported in Table 10, when compared to the model trained using the data from the (A) path, the performance is slightly higher\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nthan ours. This suggests that the generated masks from our model can be used for pseudo-labels for unlabeled data. Also, it is remarkable that although our method estimates the vessel maps and synthetic angiography images simultaneously, our method achieves comparable performance with the semi-supervised method using the pseudo-labels. On the other hand, the model trained using the data from the (B) path shows mostly lower performance than ours, implying the cycle path in our model is more effective to extract vessels.\n\nSelf-supervised We further test the application of our model to an environment that has no background images. Our model is trained by replacing the background images xb in the (B) path with the real angiography images xa. Table 10 shows that the segmentation performance of our model without non-contrast background images is comparable or even superior to our model. As the generation module of our DARL takes latent features of input images, our model can synthesize images involving the information of input angiography images, based on semantic masks, and learn vessel features. This can be a unique characteristic compared to other semantic image synthesis models that typically take the images directly.\n\nTable 10: Study on model contribution to various learning frameworks.\n\nData\n\nMetric\n\nSupervised (few-label)\n\nSemi-supervised (pseudo-pair)\n\nSelf-supervised\n\n(A) path\n\n(A)+(B) path Data from (A) Data from (B)\n\nOurs\n\nOurs w/o BG\n\nXCAD\n\nIoU Dice Precision\n\n0.423±0.079 0.590±0.081 0.637±0.134\n\n0.548±0.072 0.705±0.062 0.746±0.088\n\n0.478±0.078 0.643±0.073 0.703±0.115\n\n0.445±0.072 0.613±0.071 0.645±0.117\n\n0.471±0.076 0.636±0.072 0.701±0.115\n\n0.479±0.080 0.644±0.076 0.700±0.125\n\nExternal test: Coronary angiography\n\n134 XCA\n\n30 XCA\n\nIoU Dice Precision\n\nIoU Dice Precision\n\n0.273±0.161 0.404±0.201 0.439±0.215\n\n0.365±0.042 0.534±0.045 0.717±0.121\n\n0.323±0.195 0.454±0.237 0.453±0.236\n\n0.484±0.066 0.649±0.061 0.829±0.083\n\nCross-modality test: Retinal imaging\n\nDRIVE\n\nSTARE\n\nIoU Dice Precision\n\nIoU Dice Precision\n\n0.344±0.134 0.497±0.153 0.617±0.249\n\n0.319±0.168 0.458±0.205 0.529±0.267\n\n0.388±0.141 0.544±0.153 0.754±0.213\n\n0.332±0.195 0.464±0.235 0.591±0.295\n\nBG; background images xb\n\n0.388±0.187 0.531±0.211 0.477±0.213\n\n0.429±0.064 0.598±0.064 0.772±0.124\n\n0.365±0.148 0.518±0.159 0.585±0.271\n\n0.375±0.187 0.517±0.210 0.528±0.269\n\n0.378±0.179 0.522±0.205 0.458±0.198\n\n0.425±0.080 0.592±0.082 0.737±0.133\n\n0.341±0.147 0.490±0.163 0.504±0.265\n\n0.354±0.183 0.495±0.208 0.481±0.254\n\n0.426±0.059 0.595±0.058 0.781±0.118\n\n0.427±0.184 0.572±0.205 0.729±0.152\n\n0.377±0.187 0.518±0.215 0.469±0.208\n\n0.429±0.070 0.597±0.070 0.762±0.124\n\n0.372±0.148 0.525±0.161 0.617±0.271\n\n0.368±0.191 0.508±0.216 0.537±0.280\n\n0.392±0.135 0.550±0.144 0.659±0.247\n\n0.393±0.183 0.538±0.205 0.566±0.257\n\nC.7 STUDY ON IMAGE PROCESSING FOR THE UNSUPERVISED METHODS\n\nFor the implementation of comparison methods, we did not perform any image processing except for resizing the images. However, as the performance of unsupervised methods would be affected by the processing such as normalization, we additionally tested the unsupervised methods by applying several normalization methods to the input images. As shown in Figure 10, we processed images using histogram equalization (HE) and contrast limited adaptive histogram equalization (CLAHE). Table 11 shows that the performance of unsupervised methods degrades when using the normalized data. This comes from the angiography images that are hard to visualize only vessel regions due to the confusing background structures even though the CLAHE and HE enhance the image contrast. On the other hand, our method outperforms the other comparative models, suggesting the superiority of our methods.\n\nFigure 10: Examples of image processing of HE and CLAHE normalization.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTable 11: The Dice scores for the segmentation performance of unsupervised methods applying HE and CLAHE to the input images.\n\nInput processing\n\nBaseline HE CLAHE\n\nSGC\n\n0.111 0.061 0.119\n\nRedrawing\n\n0.109 0.107 0.099\n\nDS\n\n0.526 0.492 0.495\n\nOurs\n\n0.636 -\n-\n\nC.8 STUDY ON TRAINING COMPLEXITY\n\nOur model has a unified generator module which can perform both the segmentation and the generation tasks simultaneously, by efficiently decreasing training complexity compared to CycleGAN structure of other adversarial learning framework such like DA or SSVS. We estimate floating point operation per second (FLOPS) of each method in Table 12, and prove the cost-effective characteristic of our model.\n\nTable 12: Training complexity (FLOPS).\n\nMethod\n\n(A) path\n\n(B) path\n\nCycle path\n\nDiscriminator\n\nDA SSVS Ours\n\n121.80 121.80 90.66\n\n121.80 121.80 173.51\n\n121.80 × 2 121.80 × 2 90.66 × 1\n\n6.24 × 2 6.24 × 2 6.24 × 2\n\nTotal\n\n499.68 499.68 367.31\n\nD ADDITIONAL VISUAL RESULTS OF VESSEL SEGMENTATION\n\nD.1 RESULTS OF ABLATION STUDY\n\nImplementation detail In Section 4.1 of the main paper, we implemented several ablated models of ours. For the methods without the diffusion module (i.e. (a) and (b) in Table 3), the real angiography images are given for the (A) path, and the real background and synthetic fractal masks are given for the (B) path. Here, as there is no diffusion module, the input images are not perturbed. In particular, for the methods without S-SPADE layers in the generation module (i.e. (b) in Table 3), there are two independent generation modules for the (A) and (B) paths, with instance normalization and SPADE as normalization layer for each, so that the input image for each path is given to the corresponding module. On the other hand, for the ablation studies on the loss functions, the data flows are the same as ours.\n\nVisual results Figure 11 shows the qualitative comparison results for the ablation studies in the main paper. (a) and (b) show that our model trained without the diffusion module generates the vessel masks including many false positive regions. When compared ours with (c), we can observe that the cycle consistency on the fake vessels allows our model to segment tiny vessels more accurately. Moreover, the comparison of (d) and ours verify that the proposed model achieves better performance, although we solve the segmentation problem with the image generation in that we use L1 loss for the segmentation masks.\n\nD.2 RESULTS OF OUR DARL MODEL\n\nIn this section, we provide additional vessel segmentation results that show the success of our DARL in self-supervised segmentation. Figure 12 shows that our model consistently provides the best performance on XCAD dataset. Also, our model segments vascular structures better than the other baselines even on the external angiography datasets (134 XCA and 30 XCA) and the retinal image datasets (DRIVE and STARE). These results suggest that the proposed model can be used as a general vessel segmentation model for various vascular images. Also, Figure 13 shows the vessel segmentation results on the XCAD data that are corrupted by Gaussian noise with different levels of σ. The visual results demonstrate that our DARL is the only method which endures harsh corruption and outperforms the baseline methods.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 11: Visual comparison results of the ablation study. (a-d) correspond to each case study (a-d) in Table 3 of the main paper. Yellow boxes denote remarkable parts.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nFigure 12: Additional visual comparison results on different angiogram and retinal datasets.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nFigure 13: Additional visual comparison results on XCAD datasets with different levels (σ) of Gaussian noise.\n\n22",
    "reference": "# Summary Of The Paper\n\nThe paper presents a DDPM based method for vessel image synthesis and segmentation. The idea is to train one base DDPM for both tasks, using a switchable SPADE as a means for incorporating the dfferences between these two tasks. Further, the DDPM is trained in a self-supervised manner. Experiments on the benchmark datasets, including both vessel and non-vessel datasets, demonstrate that the proposed method is effective and achieve better performances that competing SOTA methods.\n\n# Strength And Weaknesses\n\nStrength:\nGood writing and easy to follow. Decent novelty. Solid experiments（but with the below weaknesses)\n\nWeaknesses:\n\nThe paper claims a fully self-supervised manner for learning the model. In fact, for the segmentation path A that takes an image as input and outputs the segmentation mask, the adversarial loss is used on the mask output. Practically speaking, this is a bit uncessary as the real segmentation masks are used in the loss function. For each real segmentation mask, it comes with a real input image. Therefore, it is possible to conduct supervised training at least for this path.  I am afraid that the good performances in fact arises from this part.\n\nThere are two experiments that can can be done to verify the effectiveness of the proposed method.\n1) Change the path A to a fully supervised one while keeping the path B unchanges and then  compare with fully supervised segmentation approach\n2) Change the output masks of path A to synthetic masks.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: The paper is written clearly.\n\nNovelty: The key idea appears novel to me. While the idea of buiding one base model with switchable componets to deal with multiple tasks is not unseen, doing so in the context of DDPM is new.\n\nReproducibility: The paper is easy to understand but may be not so easy to reproduce without the codes. \n\nQuality: The paper is of high quality with good motivation, decent novelty, and solid experiments.\n\n# Summary Of The Review\n\nOverall, the paper is of high quality with good motivation, decent novelty, and solid experiments. The experiments should be augmented with new results to make the paper even stronger.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nEXPLICITLY MAINTAINING DIVERSE PLAYING STYLES IN SELF-PLAY\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nSelf-play has proven to be an effective training schema to obtain a high-level agent in complex games through iteratively playing against an opponent from its historical versions. However, its training process may prevent it from generating a well-generalised policy since the trained agent rarely encounters diversely-behaving opponents along its own historical path. In this paper, we aim to improve the generalisation of the policy by maintaining a population of agents with diverse playing styles and high skill levels throughout the training process. Specifically, we propose a bi-objective optimisation model to simultaneously optimise the agents’ skill level and playing style. A feature of this model is that we do not regard the skill level and playing style as two objectives to maximise directly since they are not equally important (i.e., agents with diverse playing styles but low skill levels are meaningless). Instead, we create a meta bi-objective model to enable high-level agents with diverse playing styles more likely to be incomparable (i.e. Pareto non-dominated), thereby playing against each other through the training process. We then present an evolutionary algorithm working with the proposed model. Experiments in a classic table tennis game Pong and a commercial roleplaying game Justice Online show that our algorithm can learn a well generalised policy and at the same time is able to provide a set of high-level policies with various playing styles.\n\n1\n\nINTRODUCTION\n\nRecent years have witnessed impressive results of self-play for Deep Reinforcement Learning (DRL) in sophisticated game environments such as various board (Silver et al., 2016; 2017a; Jiang et al., 2019) and video games (Jaderberg et al., 2019; Vinyals et al., 2019; Berner et al., 2019). The idea behind self-play is using a randomly initialised DRL agent to bootstrap itself to high-level intelligence by iteratively playing against an opponent from its historical versions (Silver et al., 2016). However, the training process of self-play may prevent it from obtaining a well-generalised policy since the trained agent rarely encounters diversely-behaving opponents along its own historical path. This can easily be taken advantage of by human players. Taking OpenAI Five (Berner et al., 2019) as an example, it has a win rate of 99.4% in more than 7000 Dota 2 open matches1, but the replay shows that 8 of the top 9 teams that defeated the OpenAI Five are the same team and the policies they use in each game are very similar. This indicates that despite the remarkable high performance, the OpenAI Five agent is still not fully comfortable in some circumstances, which can be found and further exploited by human players (e.g., through meta-policies).\n\nGenerally, an agent can be identified from two aspects, skill levels and playing styles (Mouret & Clune, 2015). These two aspects are crucial for learning a high-level agent in the self-play training process because only playing against opponents with diverse playing styles and appropriate skill levels (i.e., not too low) can maximise the gains of the learning. If one only considers opponents’ skill levels, there can be a catastrophic forgetting problem in which the agent “forgets” how to play against a wide variety of opponents (Hernandez et al., 2019). On the other hand, if one only considers playing styles, there will be a lot of meaningless games that the agent learns very little from its far inferior opponents (Laterre et al., 2018).\n\n1https://arena.openai.com/#/results\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nUnfortunately, it is intrinsically challenging to strike a good balance between skill levels and playing styles in self-play. During the training process, the network weights of the DRL agent are usually optimised by a gradient-based method, which progresses along a single path that relies on the random seed of the network and the environment. At each iteration, the incumbent agent is the only response-policy for its historical versions. Such single-path optimisation is very unlikely to experience sufficiently diverse opponents, especially within a sophisticated environment. This may make common self-play algorithms, which use a probability function to decide which opponents to consider (e.g., the latest opponent or the past versions (Berner et al., 2019; Oh et al., 2019)), unable to generalise their policies, i.e., struggle to cope with opponents that are very different from which they have encountered before.\n\nA viable way to introduce diverse playing styles in self-play is to consider the population-based approach, where a population of agents/opponents are maintained during the training process, with each potentially representing one play style. The population-based approach has already been frequently used in DRL (Jung et al., 2020; Carroll et al., 2019; Parker-Holder et al., 2020; Zhao et al., 2021). For example, Population-Based Training (PBT) (Jaderberg et al., 2017; 2019; Li et al., 2019; Liu et al., 2019) optimises a population of networks at the same time, allowing for the optimal hyperparameters and model to be quickly found. Neuroevolution (Heidrich-Meisner & Igel, 2009; Such et al., 2017; Salimans et al., 2017; Stanley et al., 2019) uses population-based evolutionary search (e.g., genetic algorithm and evolution strategy) to generate the agents’ network parameters and topology.\n\nIn these population-based methods, an interesting idea to promote diversity of the agents’ behaviours is to proactively search for “novel” behaviours. This can be very useful since maintaining a population of behaviours does not necessarily mean diversifying them over the search space (Jaderberg et al., 2019). This is particularly true in sparse/deceptive reward problems (Salimans et al., 2017; Conti et al., 2018) where the reward function may provide useless/misleading feedback leading to the agent to get stuck and fail to learn properly (Lehman & Stanley, 2011; Ecoffet et al., 2019). Such proactive-noveltysearch techniques include novelty search (Conti et al., 2018; Lehman & Stanley, 2011), intrinsic motivation (Bellemare et al., 2016), count-based exploration (Ostrovski et al., 2017; Tang et al., 2017), variational information maximisation (Houthooft et al., 2016), curiosity-driven learning (Baranes & Oudeyer, 2013; Forestier et al., 2017), multi-behaviour search (Mouret & Doncieux, 2009; Shen et al., 2020) and quality-diversity (Cully & Demiris, 2018). They, based on the history information in the environment, motivate the agent to visit unexplored states in order to accumulate higher rewards (Conti et al., 2018; Ecoffet et al., 2019; Guo & Brunskill, 2019). For example, the qualitydiversity algorithms use domain dependent behaviour characterisations to abstractly describe the agent’s behaviour trajectory and encourage the agent to uncover as many diverse behaviour niches as possible, with each niche being represented by its highest-level agent (Mouret & Clune, 2015; Pugh et al., 2016). However, such proactive novelty search may not always be promising since novel behaviours that we search for do not always come with high skill levels. When it comes to population-based self-play, a game could be meaningless when the difference of agents’ skill levels is too big, albeit their playing styles being very different. Indeed, what we need effectively is a population of high-level and diverse-style agents which play against each other through the training process.\n\nTo this end, this paper proposes a novel Bi-Objective (BiO) optimisation model to optimise skill levels and playing styles. One feature of this model is that we do not regard these two aspects as objectives to maximise directly, but rather we create a meta bi-objective model to enable highlevel agents with diverse playing styles more likely incomparable (i.e. Pareto nondominated to each other), thus being always kept in the training process. Specifically, in BiO each objective is composed of two components. The first component is related to skill level of the agent, same for the two objectives, while the second component is related to playing style of the agent, we making it completely conflicting for the two objectives. As such, the Pareto optimal solutions in BiO are typically those far away from each other in playing styles but all with reasonably good skill levels (this will be explained in details in Section 3).\n\nWe propose an evolutionary algorithm to work with the proposed model. We follow the basic framework of multi-objective evolutionary algorithms, but with customized components for self-play.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nWe evaluate our algorithm in a classic table tennis game Pong and a commercial online role-playing game Justice Online 2.\n\nIt is worth mentioning that the problem here we are dealing with is different from multiobjective optimisation-related RL (Yang et al., 2019; Liu & Qian, 2021; Xue et al., 2022), such as those in the Multi-Objective Reinforcement Learning (MORL) (Moffaert & Nowé, 2014; Mossalam et al., 2016). MORL is a generalisation of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, whereas our problem is to simultaneously optimise the skill levels and playing styles of the players.\n\n2 PRELIMINARIES AND RELATED WORK\n\nEach iteration of self-play can be considered as a Multi-agent Markov Decision Process defined by a tuple ⟨N , S, {Ai}i∈N , P, {ri}i∈N , γ⟩, where N = {1, · · · , N } is the set of N > 1 agents, S is the state space observed by all the agents, and Ai is the action space of agent i. Let A := A1 × · · · × AN , then P : S × A × S → [0, 1] is the transition probability from any state s ∈ S to any state s′ ∈ S for any joint action a ∈ A. ri : S × A × S → R is the reward function that determines the immediate reward received by agent i for a transition from (s, a) to s′. γ ∈ (0, 1] is the discount factor. At step t, each agent i ∈ N executes an action ai t, according to the system state st. The system then transitions to state st+1, and rewards each agent i by ri(st, at, st+1). The goal for agent i is to maximise its own long-term reward Ji(π) = Eπ\n\nt=0 γtri(st, at, st+1)(cid:3), by finding the policy ai\n\nt ∼ πi(·|st).\n\n(cid:2)(cid:80)\n\nPROXIMAL POLICY OPTIMISATION (PPO). PPO (Schulman et al., 2017) is a popular deep policy gradient method where policy updates are computed by a surrogate objective regularised by the clipped probability ratios. Inspired by a trust region method, the algorithm updates the policy within a close neighbourhood around the previous-iteration policy each time to guarantee monotonic performance improvement. As shown in a clipped surrogate objective LCLIP\n\nLCLIP\n\nπold (π) = E\n\n(cid:20)\n\nmin\n\n(cid:18) π(a | s) πold(a | s)\n\nAπold (s, a), clip\n\n(cid:18) π(a | s) πold(a | s)\n\nπold (π): (cid:19)\n\n, 1 − ε, 1 + ε\n\nAπold (s, a)\n\n(cid:19)(cid:21) ,\n\n(1)\n\nwhere clip(·) removes the incentive of the probability ratio π(a|s) πold(a|s) outside the interval [1 − ε, 1 + ε] (ε = 0.2). PPO can sample data from the stable previous-iteration policy πold, and incrementally refines the policy using multiple steps of stochastic gradient ascent before sampling new data.\n\nMULTI-OBJECTIVE OPTIMIZATION. Multi-objective optimisation (Matthias, 2006) is an optimisation scenario that considers multiple objectives/criteria simultaneously. Without loss of generality, let us consider a maximisation scenario. Formally, a multi-objective optimisation problem can be expressed as:\n\nmaximise F (x) = (f1(x), · · · , fm(x))T ,\n\n(2)\n\nwhere m is the number of objectives. In the context of multi-objective optimisation, a solution x1 is said better than x2, if and only if x1 is not worse than x2 for all the objectives and better for at least one objective. We call this “better” relation as Pareto dominance, i.e., x1 (Pareto) dominates x2. Two solutions being incomparable means that they are non-dominated to each other. For a solution x ∈ X, if there is no solution in the solution set X dominating x, then x is called a Pareto optimal solution in X.\n\nRELATED WORK. Diversifying playing styles of opponents that the agent encounters in the training process is an important topic in self-play. Interesting attempts include using diverse expert data as opponents to enrich the agent’s experiences (Silver et al., 2016; Vinyals et al., 2019; Lowe et al., 2020), learning diverse sub-policies and combining them into an ensemble model (Xu et al., 2018), using reward shaping to create a series of different playing styles (Oh et al., 2019), and exploring unseen playing styles by domain randomisation (Jaderberg et al., 2019; Berner et al., 2019). However, striking a good balance between playing styles and skill levels is a challenging task; some of these methods need extra resources (e.g. expert data (Silver et al., 2017b)) or human hand engineering\n\n2https://mmos.com/review/justice.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(e.g. manual tuning of the reward weights (Oh et al., 2019)), and some may degrade in sophisticated environments (Xu et al., 2018).\n\nAnother way to maintain diverse opponents in self-play is to consider the population-based training (PBT) (Jaderberg et al., 2019; Vinyals et al., 2019; Hernandez et al., 2019), where the playing style of each agent in the population is controlled by its own hyperparameters and reward weights. In such methods, PBT is to meta-optimise the internal rewards and hyperparameters of the agents. Since each agent in the population learns from the experience generated by playing against opponents sampled from the population, more generalised policies can be learned (Bansal et al., 2018; Zhao et al., 2021). However, maintaining an agent population does not necessarily mean maintaining diverse playing styles; thus it would be beneficial for this model to proactively explore novel playing styles and then properly maintain them during the training process. We will compare our model with PBT in the experimental studies.\n\nIt is necessary to note that multi-objective optimisation models, despite not designed for self-play, have been used to diversify agents’ behaviours in games (Agapitos et al., 2008; Mouret & Doncieux, 2009; Shen et al., 2020; Pierrot et al., 2022b). They created one or several behaviour-related objectives for the algorithm to optimise directly. For example, the novelty of the agents’ behaviour is considered as an auxiliary objective in Mouret & Doncieux (2009), and opposite behaviours (i.e., aggressive and defensive) of the agents are considered as two objectives in Shen et al. (2020). One main issue of such models is that an agent with a different behaviour from the rest will always be considered Pareto optimal no matter how poor its skill level is (since no other agent performs better in the corresponding behaviour objective). This may easily lead to the population to have diverse playing-style agents, but their skill levels can be highly variable. We will compare our model with one recent representative of such models in the experimental studies.\n\nLastly, it is worth mentioning that some recent studies formalised skills and styles with differentiable functions and leveraged gradient information to efficiently improve the skill level of agents from different stylistic directions (Fontaine & Nikolaidis, 2021; Pierrot et al., 2022a; Tjanaka et al., 2022), although they are not implemented in the self-play paradigm.\n\n3 PROPOSED MODEL\n\nOur bi-objective (BiO) optimisation model is designed to optimise skill levels of the agents and at the same time to diversify their playing styles. To do so, each objective is constructed to consist of two components. The first is the agent’s skill level, which is shared by the two objectives. The second component is concerned with the agent’s playing style, which we design to be opposite in the two objectives in order to make diverse style agents likely incomparable (thus as the Pareto optimal agents kept during the search). Formally, the BiO model for an agent π with the skill level g(π) and the playing style h(π) is expressed as follows:\n\nmaximise\n\n(cid:26)f1(π) = g(π) + κh(π) f2(π) = g(π) − κh(π)\n\n(3)\n\nwhere κ is a coefficient to make g(π) and h(π) commensurable. g(π) is an indicator to reflect the agent’s skill level, and it can be represented by win rate (Chen et al., 2018), Elo score (Jaderberg et al., 2019), etc. h(π) is an indicator to reflect the agent’s playing style, and its calculation will be explained later. The difference between two agents in h(π) indicates how differently they behave, e.g., aggressively versus defensively.\n\nFigure 1 gives an example to illustrate the proposed model, where Figure 1(a) presents six agents in the original skill-style space and Figure 1(b) presents them in the proposed bi-objective space. To help understand the characteristics of the BiO, we provide the following remarks, which can be derived from the figure as well as Equation 3.\n\n• A higher skill-level agent will not be dominated by a lower level one in the proposed model. This can be derived from Equation 3 — if g is higher for the agent π1 than π2, then whatever their playing styles h(π1) and h(π2) are, π2 will not be better than π1 on both objectives f1 and f2; in the best case for π2, they are non-dominated to each other (e.g., the agents A and B in Figure 1).\n\n• Similar playing style agents tend to be comparable (i.e., dominating or being dominated) even if their skill levels are close. For example, in Figure 1 the agent E’s playing style\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(a) The original skill-style space\n\n(b) The proposed BiO space\n\nFigure 1: An illustration that the proposed bi-objective model makes diverse style agents with fairly good skill level more likely to be Pareto optimal (i.e., A, B, E). As implemented in this paper, the skill level and playing style are normalized and the coefficient κ = 1. (a) The six agents in the skill-style space. (b) The six agents in the proposed BiO space where the two meta-objectives are to be maximised. As can be seen in Figure 1(a), the agents B and C have the same playing style, while B has a higher skill level than C. The agents E and F have similar styles, while E has a higher skill level than F. A’s style is dissimilar from the others’ and the same for D, while A’s skill level is significantly better than D’s. Therefore, if one would like to choose three diverse style agents with good skill levels as an opponent population for an agent to play against, the agents A, B and E can be the best choices. This, as shown in Figure 1(b), is in line with the proposed model, where these three agents are Pareto optimal in the bi-objective space.\n\nis similar to F’s and their skill levels are fairly close, but E dominates F in the proposed model. In the special case that two agents have the same style, the higher skill level agent always dominates the lower one (e.g. the agents B and C in the figure).\n\n• Dissimilar playing style agents tend to be incomparable (i.e., non-dominated to each other) even if one is fairly inferior to the other in skill level. For example, the agent A in Figure 1, which has relatively low skill level, is not dominated by any agent since its style is rather different from the others’. In fact, it can be derived from Equation 3 that the probability of two agents being incomparable increases linearly with the distance of their playing styles.\n\n• The coefficient κ in the model is a critical parameter which weighs up between agents’ skill level and playing style. In our implementation, we simply set κ = 1 after normalising them (i.e. after making g(π) ∈ [0, 1], h(π) ∈ [0, 1]). It is worth mentioning that a different κ setting may potentially be more suitable for a specific problem.\n\nTo sum up, the proposed BiO model enables different style agents with good skill level to be ranked high. As can be seen in Figure 1, if one would like to choose three out of the six agents as the population of opponents for an agent to play against, then the agents A, B and E will be chosen (since they are Pareto optimal). B and E are chosen because they have higher skill levels than their similar agents (i.e., C and F, respectively). A is chosen because A is far away from the others in playing style, whereas D is not chosen because despite being far away from the others, but its skill level is significantly worse than them too. In fact, the model can be seen to identify representative good agents, but without a need of setting a niche. As such, one can maintain a population of high-level agents with diverse playing styles who keep playing against each other throughout the training process.\n\nIn BiO, the playing style h(π) of an agent π can be estimated by π playing against all the opponents in the pool. Formally, it can be calculated as\n\nh(π) =\n\n1 K\n\nK (cid:88)\n\nk=1\n\n1 Tk − 1\n\nTk−1 (cid:88)\n\nt=1\n\nvarπk (st, st+1)\n\n(4)\n\nwhere K is the size of the opponent pool, Tk is the length of the trajectory produced by π playing against the opponent πk, and varπk (st, st+1) denotes the relevant state change (e.g. the position change of the agent π) between time steps t and t + 1.\n\nBased on the gameplay of games, the calculation of the state change function can generally be classified into two categories, opponent-independent mode and opponent-dependent mode. In the\n\n5\n\n0.40.20Skill levelsDefensive0.40.60.8100.80.20.61.0AggressivePlaying stylesABCDEF1.20‐0.2Skill levels – Play styles0.20.40.60.80.81.61.01.41.8Skill levels + Playing stylesABCDEF0.6Under review as a conference paper at ICLR 2023\n\nopponent-independent mode, the function can directly be estimated by the change of the agent π’s own states. This mode is for games where the players have their own venues, for example the games Pong, Tennis and Blobby Volley. In such games, the movement frequency of the agent π is the main way to reflect its playing style — aggressive players frequently change their positions whereas conservative players tend to move slowly and steadily.\n\nIn the opponent-dependent mode, the state change function is estimated by not only the agent π’s states but the opponent πk’s. That is, the opponent’s states are used as a reference to estimate the agent’s playing style. This mode is for games where the agent and its opponent share the same venue, for example the battle games Justice Online, B&S Arena Battle (Oh et al., 2019) and Toribash (Kanervisto & Hautamäki, 2019). In such games, the agent’s playing style needs to be measured by the difference between the agent’s movement trajectory and its opponent’s movement trajectory — aggressive players like close combat, thus the difference typically small, whereas defensive players prefer to move around and tend to stay in a certain distance from their opponents, thus the difference typically large.\n\n4 OPTIMISATION\n\nIn this section, we present an evolutionary algorithm working with the Proximal Policy Optimisation (PPO) (Schulman et al., 2017) to optimise the proposed bi-objective model. Specifically, we use the framework of the classic multi-objective evolutionary algorithm NSGA-II (Deb et al., 2002), but with customized components for self-play DRL, e.g., an evaluation population “frozen” for a while allowing the agents to play against.\n\n5\n\n6\n\nInput: Evolutionary population\n\nP ′ ← V ariation(P ); foreach ⟨θ′, ω′⟩ ∈ P ′ do\n\nP ← ⟨θ1, ω1⟩, ..., ⟨θK , ωK ⟩; Evaluation population E ← P ; Generation gen ← 0\n\nAlgorithm 1: Algorithm to solve the biobjective model\n\n1 P ← Evaluation(P, E); 2 P ← F itnessAssignment(P ); 3 while termination condition not met do 4\n\nAlgorithm 1 gives the main procedure. The procedure starts by initialising the evolutionary population with a set of agents defined by random neural networks (θ) with reward weights (ω). Other hyperparameters of the agents (e.g. neural architecture, discount factor and learning rate) are set manually and can be found in Appendix C. Step 1 in the algorithm is to estimate the skill level and playing style of the evolutionary population. Each of its agents plays against all the agents of the evaluation population and obtains the average skill level and playing style. Then the agents’ fitness (i.e. objective functions) of the proposed BiO model is calculated in Step 2. After that, the following steps repeat for each generation of the evolutionary algorithm. Step 4 is to update the population by varying the agents’ reward weights. Here, we adopt two basic real-valued variation operators, simulated binary crossover and polynomial mutation (Deb et al., 2002) (details can be seen in Appendix B). After new agents generated, we use PPO to train their network parameters (Steps 5 and 6), where each new agent randomly picks one agent in the parental population to play against and this repeats a few times. Then, the new agent population are evaluated against the evaluation population (Step 7) and their objective functions of the BiO model are calculated (Step 8). Next, based on the objective functions, the environmental selection is performed by using the nondominated sorting and crowding distance in NSGA-II (Deb et al., 2002) (details are given in Appendix B) to select the K best solutions from the parental and new populations as the next-generation population (Step 9).\n\nP ′ ← Evaluation(P ′, E); P ′ ← F itnessAssignment(P ′); P ← EnvironmentalSelection(P, P ′); gen ← gen + 1; if gen%f req = 0 then\n\nE ← P ; P ← Evaluation(P, E); P ← F itnessAssignment(P );\n\n15 return ⟨θ, ω⟩ which has the highest skill level in\n\nθ′ ← PPO(θ′, ω′, P );\n\nP\n\n11\n\n12\n\n13\n\n10\n\n14\n\n9\n\n7\n\n8\n\nSteps 11 to 14 are to update the evaluation population after a while (i.e. every f req generations). Introducing an evaluation population that lets the current population play against for estimating the skill level and playing style plays an important role. Fixing the opponents of the agents from different generations enables their fitness comparable; playing against different opponents can easily produce different skill level and playing style even for the same agent. Note that the update frequency\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nparameter f req cannot be set very large/small. A large value may make the skill level of the evaluation population far lower than the current agents’, while a very small value may make the fitness of the agents unstable during the evolutionary process.\n\nFinally, after the procedure terminates, the returned agent is the one that has the highest skill in the evolutionary population (Step 15).\n\n5 EXPERIMENTS\n\nWe evaluate our model on the Atari game Pong and the commercial game Justice Online. To implement the bi-objective model, the skill level in Pong is estimated by the difference of the final score between the agent and its opponent, and the playing style is done by the move of the agent’s paddle. In Justice Online, the skill level is estimated by the difference in the final health point between the agent and its opponent, and the playing style by their average distance in the game. Details about the two environments and their BiO implementations are given in Appendix A.\n\nWe consider four competitors to compare with our proposed model; they are PPO (Schulman et al., 2017), a DRL strategy based on Population-Based Training (Jaderberg et al., 2017) (denoted by PBT for simplicity), a DRL strategy considering multiple rewards (Oh et al., 2019) (denoted by multi-reward), and a DRL strategy based on evolutionary multi-objective optimisation (called EMOGI) (Shen et al., 2020). Note that all of these competitors except multi-reward were not designed specifically for self-play; here you used their self-play versions, where the agent is trained by playing against an opponent from its historical versions. PPO is a baseline DRL method, upon which the other algorithms (including ours) are based. To alleviate the catastrophic forgetting problem, in PPO we follow the practice in (Berner et al., 2019) to let the agent to play against the latest version with a probability of 80% and against past versions with a probability of 20%. The multi-reward competitor is PPO working with a multi-reward strategy (Oh et al., 2019) where multiple rewards are used in training to shape different playing styles (aggressive, balanced and defensive) for generalisation. The PBT competitor is PPO working under the population-based learning environment (Jaderberg et al., 2017), where each PPO style agent of the population can exploit information from the rest of the population. Like the proposed BiO, EMOGI (Shen et al., 2020) combines PPO with the evolutionary multi-objective optimisation approach, but the objectives are defined based on a hierarchical comparison of win rate and two handcrafted playing styles (i.e., aggressive and defensive). More details about these four competitors can be found in Appendix C.\n\nFor a fair comparison, all the four algorithms are run within the same computational budgets (1G frames) and the average win rate of 500 games for each environment is reported. In our algorithm, the frequency of updating the evaluation population f req is set to 5. The hyperparameters of PPO for all the algorithms are set according to (Schulman et al., 2017). Appendix C gives all the parameters and their settings in the experiments.\n\nWe aim to evaluate the proposed algorithm through answering the following four questions.\n\n• Research Question 1 (RQ1): How does BiO perform, in comparison with the other algorithms, in the same training and evaluation environments when playing against a built-in AI?\n\n• Research Question 2 (RQ2): How does BiO perform, in comparison with the other algo-\n\nrithms, in different training and evaluation environments when against a built-in AI?\n\n• Research Question 3 (RQ3): What happens when the agents obtained by BiO and the other\n\nalgorithms play directly against each other?\n\n• Research Question 4 (RQ4): What kind of agents can BiO provide? Do they have diverse\n\nplaying styles?\n\nThe experiment for the question RQ1 is to see the ability of the obtained policy in playing against a rule-based built-in AI player when the training and evaluation environments are the same. The experiment for RQ2 is to see the generalisation of the policy to a different environment. Here, we change the start position of the opponent from the central point of the site in the training to a random boundary point in the evaluation. The above two experiments are all about playing against a build-in AI; then one would be interested to know what if the agents obtained by all the algorithms play directly against each other. RQ3 is designed to answer this question. The experiment for RQ4 is to\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nsee if the proposed BiO model can produce diverse playing styles. Since Algorithm 1 returns only the highest skill-level policy, one may be curious about what the whole population (i.e., P in Step 15 of the algorithm) looks like. RQ4 is for this question.\n\nRQ1: How does BiO perform in the same environment when against the built-in AI? Table 1 gives the win rate of the policies obtained by the five algorithms. As can be seen in the table, PBT has the best overall performance. This may be attributed to the fact that PBT configures optimal hyperparameters for the training to avoid premature convergence. Our algorithm BiO performs best in Pong and takes the second place in Justice Online, achieving a win rate of over 90% for both games. In contrast, the agents trained by the other three algorithms perform differently in the two environments, especially for Multi-reward which only has close to 20% win rate in Justice Online, but more than 97% win rate in Pong. One explanation is that the built-in AI in Justice Online is rather different from the opponents that the three algorithms’ agent (PPO, Multi-reward and EMOGI) encounters in the training, but in Pong they are very similar.\n\nTable 1: Win rate of the policies against the built-in AI in the same training and evaluation environments.\n\nPPO\n\nPBT Multi-reward\n\nEMOGI\n\nBiO (ours)\n\nPong Justice Online\n\n92.8% 99.4% 74.0% 99.2%\n\n97.4% 20.4%\n\n99.4% 80.8%\n\n99.7% 93.0%\n\nRQ2: How does BiO perform in different environments when against the built-in AI? Table 2 gives the win rate of the policies obtained by the five algorithms. It can be seen that the environmental changes have a greater impact on all the algorithms except BiO. In Pong, the performance of the four peer algorithms (PPO, PBT, Multi-reward and EMOGT) have a significant drop, especially the win rate of PPO, PBT and EMOGT being less than 50%. In Justice Online, these four algorithms still cannot obtain a good winning rate, despite a slight increase of the Multi-reward’s performance compared with the result in the same training and evaluation environments (Table 1). In contrast, our BiO maintains a win rate above 80% when the environment changes, even reaching over 90% win rate in Justice Online. One explanation for this is that the built-in AI is affected very differently by changes in different environments.\n\nTable 2: Win rate of the policies against the built-in AI in different training and evaluation environments.\n\nPPO\n\nPBT Multi-reward\n\nEMOGI\n\nBiO (ours)\n\nPong Justice Online\n\n39.8% 19.8% 2.4% 35.6%\n\n64.5% 39.0%\n\n8.4% 75.0%\n\n84.1% 91.0%\n\nRQ3: What happens when the agents obtained by BiO and the other algorithms play directly against each other? In this experiment, we construct an opponent pool of the agents ever generated, i.e., one agent from PPO, one from PPT, three from multi-reward (as it generates three agents with aggressive, balanced and defensive playing styles respectively), 30 from EMOGI and 30 from BiO (the population size is 30). We then randomly pick one from the opponent pool to be played against our agent. This repeats 60 times and the average win rate is reported in Table 3. As shown, BiO achieves a significantly higher win rate than the other four algorithms (except in Justice Online with the same environment where PBT’s win rate is slightly higher than BiO’s), indicating better generalisation of the proposed model to different opponents. It is worth noting that the advantage of BiO over its competitors is more evident when training and testing in different environments. This implies the importance of maintaining a set of high skill-level agents with diverse styles during the training, which provides the agent with opportunities to play against very different opponents.\n\nRQ4: Can BiO provide diverse playing styles? Since our algorithm (Algorithm 1 in Section 4) returns only the highest skill-level agent in the final population, one may ask how the remaining agents look like. Figure 2(b) plots the final population obtained by the algorithm under the Justice Online environment in the space of skill level and playing style. As can be seen, these agents have similar skill level and diverse playing styles. This indicates that our algorithm can provide agents with very different behaviours. These agents should all have good generalisation as they encountered the same set of various opponents in the training.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Win rate of the policies against a pool of the opponents obtained by all the algorithms in the same and different training-evaluation environments.\n\nPPO\n\nPBT Multi-reward\n\nEMOGI\n\nBiO (ours)\n\nPong (same environment) Justice Online (same environment)\n\n54.6% 59.0% 71.0% 79.6%\n\nPong (diff. environment) Justice Online (diff. environment)\n\n60.7% 39.3% 45.9% 45.9%\n\n52.4% 6.0%\n\n70.9% 45.7%\n\n26.9% 55.0%\n\n42.4% 63.7%\n\n83.5% 78.0%\n\n88.8% 81.8%\n\nTo get a sense of how the results look like in the game environments, Figures 2(c)-(e) plot the behaviour trajectories of three representative agents (aggressive, neural and defensive) tagged in Figure 2(b) on nine randomly selected games. As shown, although each agent has very similar behavioural trajectories (orange line) across the nine games, different agents behave rather differently. The aggressive agent runs towards the opponent, the defensive agent tends to avoid confrontation with the opponent and runs around the arena, and the neutral agent stays or walks around some of the arena’s boundaries.\n\nFigure 2: An illustration that BiO can obtain a set of agents with diverse play styles under the Justice Online environment. (a) A scene of the game environment. (b) The final agent population obtained by BiO in the space of skill level and playing style. (c)-(e) The behaviour trajectories of three representative agents tagged in Figure 2(b) on nine randomly selected games, where the orange and blue lines represent the footprints of the agent and opponent, respectively.\n\n6 CONCLUSION\n\nThis paper proposes a bi-objective optimisation model (BiO), working with an evolutionary algorithm, to improve the generalisation of the policy in self-play. BiO maintains an agent population and enables high-level, diverse-style agents more likely to be Pareto optimal, thus playing against each other throughout the training process. The experimental studies have shown its effectiveness and robustness in different environments. In addition to the improvement of the policy’s generalisation, a by-product from this model is that it can provide a set of high skill-level policies with diverse behaviours.\n\nThis work is the first step towards a new attempt of balancing performance and behaviour in games, and it could be potentially improved by different ways, for example, tuning the coefficient κ of the model for specific problems and developing other population-based algorithms for the model. In addition, despite being used for self-play here, it is extendable to other RL settings where the agent’s behaviour can be properly measured.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlexandros Agapitos, Julian Togelius, Simon M. Lucas, Jurgen Schmidhuber, and Andreas Konstantinidis. Generating diverse opponents with multiobjective evolution. In 2008 IEEE Symposium On Computational Intelligence and Games, pp. 135–142, 2008. doi: 10.1109/CIG.2008.5035632.\n\nTrapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent In 6th International Conference on Learning Repcomplexity via multi-agent competition. resentations, ICLR, Vancouver, BC, Canada, April 2018. OpenReview.net. URL https: //openreview.net/forum?id=Sy0GnUxCb.\n\nAdrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49–73, January 2013.\n\nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Rémi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, NIPS, pp. 1471–1479, Barcelona, Spain, December 2016.\n\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pondé de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019.\n\nMicah Carroll, Rohin Shah, Mark K. Ho, Tom Griffiths, Sanjit A. Seshia, Pieter Abbeel, and Anca D. Dragan. On the utility of learning about humans for human-AI coordination. In Advances in Neural Information Processing Systems, NIPS, pp. 5175–5186, Vancouver, BC, Canada, December 2019.\n\nYutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and\n\nNando de Freitas. Bayesian optimization in AlphaGo. CoRR, abs/1812.06855, 2018.\n\nEdoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In Advances in Neural Information Processing Systems, NIPS, pp. 5032–5043, Montréal, Canada, December 2018.\n\nAntoine Cully and Yiannis Demiris. Quality and diversity optimization: A unifying modular\n\nframework. IEEE Transactions on Evolutionary Computation, 22(2):245–259, April 2018.\n\nKalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE transactions on evolutionary computation, 6(2):182–197, April 2002.\n\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Go-explore: a new approach for hard-exploration problems. CoRR, abs/1901.10995, 2019. URL http: //arxiv.org/abs/1901.10995.\n\nMatthew Fontaine and Stefanos Nikolaidis. Differentiable quality diversity. Advances in Neural\n\nInformation Processing Systems, NIPS, 34:10040–10052, 2021.\n\nSébastien Forestier, Yoan Mollard, and Pierre-Yves Oudeyer. Intrinsically motivated goal exploration processes with automatic curriculum learning. CoRR, abs/1708.02190, 2017. URL http: //arxiv.org/abs/1708.02190.\n\nZhaohan Daniel Guo and Emma Brunskill. Directed exploration for reinforcement learning. CoRR,\n\nabs/1906.07805, 2019. URL http://arxiv.org/abs/1906.07805.\n\nVerena Heidrich-Meisner and Christian Igel. Neuroevolution strategies for episodic reinforcement\n\nlearning. Journal of Algorithms, 64(4):152–168, October 2009.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDaniel Hernandez, Kevin Denamganaï, Yuan Gao, Peter York, Sam Devlin, Spyridon Samothrakis, and James Alfred Walker. A generalized framework for self-play training. In 2019 IEEE Conference on Games (CoG), pp. 1–8, London, United Kingdom, August 2019. doi: 10.1109/CIG.2019. 8848006.\n\nRein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME: variational information maximizing exploration. In Advances in Neural Information Processing Systems, NIPS, pp. 1109–1117, Barcelona, Spain, December 2016.\n\nMax Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training of neural networks. CoRR, abs/1711.09846, 2017. URL http://arxiv.org/abs/1711.09846.\n\nMax Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Humanlevel performance in 3d multiplayer games with population-based reinforcement learning. Science, 364(6443):859–865, 2019.\n\nQiqi Jiang, Kuangzheng Li, Boyao Du, Hao Chen, and Hai Fang. Deltadou: Expert-level doudizhu AI through self-play. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pp. 1265–1271, Macao, China, July 2019. International Joint Conferences on Artificial Intelligence Organization. doi: 10.24963/ijcai.2019/176. URL https: //doi.org/10.24963/ijcai.2019/176.\n\nWhiyoung Jung, Giseung Park, and Youngchul Sung. Population-guided parallel policy search In 8th International Conference on Learning Representations, for reinforcement learning. ICLR, Addis Ababa, Ethiopia, April 2020. URL https://openreview.net/forum?id= rJeINp4KwH.\n\nAnssi Kanervisto and Ville Hautamäki. Torille: Learning environment for hand-to-hand combat. In IEEE Conference on Games, CoG 2019, pp. 1–8, London, United Kingdom, August 2019. IEEE.\n\nAlexandre Laterre, Yunguan Fu, Mohamed Khalil Jabri, Alain-Sam Cohen, David Kas, Karl Hajjar, Torbjorn S. Dahl, Amine Kerkeni, and Karim Beguir. Ranked reward: Enabling self-play reinforcement learning for combinatorial optimization. CoRR, abs/1807.01672, 2018. URL http://arxiv.org/abs/1807.01672.\n\nJoel Lehman and Kenneth O. Stanley. Evolving a diversity of virtual creatures through novelty search and local competition. In 13th Annual Genetic and Evolutionary Computation Conference, GECCO, pp. 211–218, Dublin, Ireland, July 2011. ACM.\n\nAng Li, Ola Spyra, Sagi Perel, Valentin Dalibard, Max Jaderberg, Chenjie Gu, David Budden, Tim Harley, and Pramod Gupta. A generalized framework for population based training. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD, pp. 1791–1799, Anchorage, AK, USA, August 2019. ACM.\n\nF. Liu and C. Qian. Prediction guided meta-learning for multi-objective reinforcement learning. IEEE\n\nCongress on Evolutionary Computation, CEC, pp. 2171–2178, 2021.\n\nSiqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, and Thore Graepel. Emergent coordination through competition. In 7th International Conference on Learning Representations, ICLR, New Orleans, LA, USA, May 2019. URL https://openreview.net/ forum?id=BkG8sjR5Km.\n\nRyan Lowe, Abhinav Gupta, Jakob Foerster, Douwe Kiela, and Joelle Pineau. On the interaction between supervision and self-play in emergent communication. arXiv preprint arXiv:2002.01093, 2020.\n\nEhrgott Matthias. Multicriteria optimization. Springer Science & Business Media, 2006.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nVolodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML, pp. 1928–1937, New York City, NY, USA, June 2016.\n\nKristof Van Moffaert and Ann Nowé. Multi-objective reinforcement learning using sets of pareto dominating policies. The Journal of Machine Learning Research, 15(1):3483–3512, January 2014.\n\nHossam Mossalam, Yannis M. Assael, Diederik M. Roijers, and Shimon Whiteson. Multi-objective deep reinforcement learning. CoRR, abs/1610.02707, 2016. URL http://arxiv.org/abs/ 1610.02707.\n\nJean-Baptiste Mouret and Jeff Clune.\n\nIlluminating search spaces by mapping elites. CoRR,\n\nabs/1504.04909, 2015. URL http://arxiv.org/abs/1504.04909.\n\nJean-Baptiste Mouret and Stéphane Doncieux. Using behavioral exploration objectives to solve deceptive problems in neuro-evolution. In Genetic and Evolutionary Computation Conference, GECCO, pp. 627–634, Montreal, Québec, Canada, July 2009. ACM.\n\nInseok Oh, Seungeun Rho, Sangbin Moon, Seongho Son, Hyoil Lee, and Jinyun Chung. Creating pro-level AI for real-time fighting game with deep reinforcement learning. arXiv preprint arXiv:1904.03821, 2019.\n\nGeorg Ostrovski, Marc G. Bellemare, Aäron van den Oord, and Rémi Munos. Count-based exploration with neural density models. In Proceedings of the 34th International Conference on Machine Learning, ICML, volume 70, pp. 2721–2730, Sydney, NSW, Australia, August 2017.\n\nJack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, and Stephen Roberts. Effective\n\ndiversity in population-based reinforcement learning. CoRR, abs/2002.00632, 2020.\n\nKrzysztof Pawelczyk, Michal Kawulok, and Jakub Nalepa. Genetically-trained deep neural networks. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, GECCO, pp. 63–64, Kyoto, Japan, July 2018. ACM.\n\nThomas Pierrot, Valentin Macé, Felix Chalumeau, Arthur Flajolet, Geoffrey Cideron, Karim Beguir, Antoine Cully, Olivier Sigaud, and Nicolas Perrin-Gilbert. Diversity policy gradient for sample efficient quality-diversity optimization. In ICLR Workshop on Agent Learning in Open-Endedness, 2022a.\n\nThomas Pierrot, Guillaume Richard, Karim Beguir, and Antoine Cully. Multi-objective quality\n\ndiversity optimization. arXiv preprint arXiv:2202.03057, 2022b.\n\nJustin K. Pugh, Lisa B. Soros, and Kenneth O. Stanley. Quality diversity: A new frontier for\n\nevolutionary computation. Front. Robotics and AI, 3:40, July 2016.\n\nTim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative\n\nto reinforcement learning. CoRR, abs/1703.03864, 2017.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347.\n\nRuimin Shen, Yan Zheng, Jianye Hao, Zhaopeng Meng, Yingfeng Chen, Changjie Fan, and Yang Liu. Generating behavior-diverse game ais with evolutionary multi-objective deep reinforcement learning. In International Joint Conferences on Artificial Intelligence, IJCAI, 2020. URL https: //sites.google.com/view/ijcai20-emogi.\n\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484–489, January 2016. doi: 10.1038/nature16961.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017a. URL http://arxiv.org/ abs/1712.01815.\n\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550:354–359, October 2017b.\n\nKenneth O Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. Designing neural networks\n\nthrough neuroevolution. Nature Machine Intelligence, 1(1):24–35, January 2019.\n\nFelipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. CoRR, abs/1712.06567, 2017.\n\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, NIPS, pp. 2753– 2762, Long Beach, CA, USA, December 2017.\n\nBryon Tjanaka, Matthew C Fontaine, Julian Togelius, and Stefanos Nikolaidis. Approximating gradients for differentiable quality diversity in reinforcement learning. arXiv preprint arXiv:2202.03666, 2022.\n\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nature, 575(7782):350–354, October 2019.\n\nHuazhe Xu, Keiran Paster, Qibin Chen, Haoran Tang, Pieter Abbeel, Trevor Darrell, and Sergey Levine. Hierarchical deep reinforcement learning agent with counter self-play on competitive games. https://openreview.net/forum?id=HJz6QhR9YQ, September 2018.\n\nK. Xue, J. Xu, L. Yuan, M. Li, C. Qian, Z. Zhang, and Y. Yu. Multi-agent dynamic algorithm\n\nconfiguration. Advances in Neural Information Processing Systems, NIPS, 2022.\n\nRunzhe Yang, Xingyuan Sun, and Karthik Narasimhan. A generalized algorithm for multi-objective reinforcement learning and policy adaptation. In Advances in Neural Information Processing Systems, NIPS, pp. 14610–14621, Vancouver, BC, Canada, December 2019.\n\nRui Zhao, Jinming Song, Hu Haifeng, Yang Gao, Yi Wu, Zhongqian Sun, and Yang Wei. Maximum entropy population based training for zero-shot human-ai coordination. arXiv preprint arXiv:2112.11701, 2021.\n\nA ENVIRONMENTS\n\nThis section describes the two environments Pong and Justice Online and the implementation of the bi-objective model BiO in them, i.e., how to determine the skill level function g(π) and the playing style function h(π) in the two environments.\n\nPONG. Pong is a two-player game that simulates table tennis, in which each player controls an in-game paddle by moving it vertically across the left or right side of the screen. There are three actions that the players can take: move up the paddle, move down the paddle, or stay still. Points are earned when one fails to return the ball to the other. The goal of the game is for each player to reach 20 points before the opponent. The self-play version of Pong we used in the experiments is accessible at https://github.com/xinghai-sun/deep-rl/blob/master/docs/ selfplay_pong.md.\n\nTo estimate the skill level of an agent in Pong, we consider the average difference of the final scores between the agent and its opponent in the opponent pool. Formally, the skill level function g(π) of\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nthe agent π is calculated as:\n\ng(π) =\n\n1 K\n\nK (cid:88)\n\nk=1\n\nscoreπ − scoreπk + 20 40\n\n(5)\n\nwhere K is the size of the opponent pool (i.e. the population size in our algorithm) and scoreπ is the score of the agent π at the end of the game.\n\nThe playing style in Pong is in the opponent-independent mode. It can be reflected by the move frequency of the agent’s paddle. As such, the state change function varπk (when π plays against πk) in the playing style function (i.e. Equation (3) in the text)\n\nh(π) =\n\n1 K\n\nK (cid:88)\n\nk=1\n\n1 Tk − 1\n\nTk−1 (cid:88)\n\nt=1\n\nvarπk (st, st+1)\n\ncan be defined as\n\nvarπk (st, st+1) =\n\n(cid:26)0,\n\nif the position of π’s paddle in state st+1 is the same as in st\n\n1, otherwise\n\n(6)\n\n(7)\n\nwhere Tk denotes the length of the trajectory produced by π playing against the opponent πk.\n\nJustice Online 3 supplies duels between two players called “Arena Battles”. The JUSTICE ONLINE. arena battle is a two-player zero-sum game, where two players fight against each other to decrease the opponent’s Health Point (HP) to zero as the goal within a fixed amount of time. This game is a non-trivial task. First, it is a real-time game which requires two players to make decisions simultaneously under imperfect information. At any time of the game, neither player knows what the opponent’s current skills and skill levels are likely to be. Second, the game has a massive state space. An agent must make various skills, moves and targeted decisions simultaneously, leading to a huge number of possible states.\n\nTo estimate the skill level of an agent, we consider the average difference of the final HP values between the agent and its opponent in the opponent pool, expressed as:\n\ng(π) =\n\n1 K\n\nK (cid:88)\n\nk=1\n\nHPπ − HPπk\n\n(8)\n\nwhere K is the size of the opponent pool and HPπ is the HP value of the agent π at the end of the game.\n\nThe playing style in Justice Online is in the opponent-dependent mode. It needs to be measured by the relative position (i.e. distance) between the agent and its opponent. Aggressive agents like close combat and their distance to the opponent is small, whereas defensive agents like to stay in a certain distance from their opponents. We thus define the varπk function in Equation 6 as\n\nvarπk (st, st+1) = distt+1(π, πk) (9) where distt+1(π, πk) denotes the Euclidean distance between the agents π and πk at step t + 1. Note that both the skill level and playing style values are normalized into the range of [0, 1] in our implementation.\n\nB ALGORITHM DETAILS\n\nThis section first describes how to generate new agents in our algorithm and then explains how to constitute the new population based on the old population and newly-generated agents.\n\nB.1 AGENT GENERATION\n\nIn the proposed algorithm, variable parameters of the agents are the network weight θ and the reward weight ω; other hyperparameters (e.g. neural architecture, discount factor and learning rate) are fixed and set manually (their settings can be found in Appendix C). Here, we consider basic variation operators from evolutionary algorithm for searching for promising θ and ω. For θ, we use the single-point crossover (Pawelczyk et al., 2018) to recombine two networks. It randomly selects a hidden layer of two networks and swaps part of them to produced two new networks. Figure 3 illustrates this operator.\n\n3https://github.com/NeteaseFuxiRL/nsh\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: An illustration of the single-point crossover for generating new neural networks.\n\nB.2 ENVIRONMENTAL SELECTION\n\nWe use the classic NSGA-II environmental selection procedure (Deb et al., 2002) to select the K best solutions as the next-generation population from the union of the old population P and the new generated solutions P ′, as shown in Algorithm 2. First, we divide the union into different fronts (F1, F2, ..., Fi, ...) where the solutions in the same front are nondominated to each other. Then, the critical front Fi is found such that |F1 ∪ F2 ∪ ... ∪ Fi−1| ≤ K and |F1 ∪ F2 ∪ ... ∪ Fi−1 ∪ Fi| > K; correspondingly the first i − 1 fronts (F1, F2, ..., Fi−1) make up the new population P . Now if the size of P is less than the population capacity K, we calculate the crowding distance of the solutions in Fi and sort them in a descending order. Next, we choose the first K − |P | solutions of Fi and put them into P . Now, P is the population to be returned.\n\nAlgorithm 2: EnvironmentalSelection(P, P ′) (Deb et al., 2002)\n\nInput: Population capacity K;\n\n1 F1, F2, ..., Fi, ... ← N ondominatedSort(P ∪ P ′) ;\n\n// Partition P into\n\ndifferent nondominated fronts F1, F2, ..., Fi, ... and find the critical front Fi where 0 ≤ K − |F1 ∪ F2 ∪ ... ∪ Fi−1| < Fi\n\n2 P ← F1 ∪ F2 ∪ ... ∪ Fi−1 ; 3 if |P | < K then\n\n// Calculate crowding distance of\n\nFi ← CrowdingDistance(Fi) ;\n\neach solution in Fi\n\nFi ← Sort(Fi) ;\n\ncrowding distance\n\n4\n\n5\n\n6\n\n// Sort in the desending order accroding to\n\nP ← P ∪ Fi[1 : K − |P |] ; // Choose the first K − |P | solutions of Fi\n\n7 return P\n\nC EXPERIMENT DETAILS\n\nThis section details all parameters/hyperparameters used in the experiments. We first list the PPOassociated hyperparameters (Mnih et al., 2016) which are shared by all the four algorithms. This is followed by the parameters/hyperparameters specific to each algorithm. Finally we give the reward settings of the four algorithms for the two environments Pong and Justice Online.\n\nPARAMETERS FOR PPO USED IN ALL THE ALGORITHMS.\n\n• Discount Rate (γ = 0.99). This parameter controls how much the RL agent cares about\n\nrewards in the immediate future relative to those in the distant future.\n\n• Weights for Loss Function ([1, 0.5, 0.01]). The loss function of PPO consists of three terms, the policy loss (actor), the value loss (critic), and an entropy loss. Their weights were assigned 1, 0.5 and 0.01, respectively.\n\n15\n\nParent1Parent2Children1Children2Under review as a conference paper at ICLR 2023\n\n• Learning Rate for Adam (0.00025). This parameter controls the learning rate of the loss\n\nfunction.\n\n• Neural Architecture [256, 128, 128]. The network consists of three hidden layers, of which the first one has 256 nodes and the other two have 128 nodes. For the image input (such as Pong game), we stack 4 layers of 3 × 3 convolutions before the fully connected layers.\n\nPARAMETERS FOR PPO-SP.\n\n• Opponent Pool Update Frequency (1, 000 episodes). This parameter controls the update frequency of the opponent pool in self-play, i.e., how often the algorithm sends its agent into the opponent pool.\n\n• Opponent Change Frequency (3 episodes). This parameter controls how long the agent\n\nplays against one opponent.\n\nPARAMETERS FOR THE MULTI-REWARD ALGORITHM.\n\n• Opponent Pool Update Frequency (1, 000 episodes). This parameter controls the update\n\nfrequency of the opponent pool in self-play.\n\n• Opponent Change Frequency (3 episodes). This parameter controls how long the agent\n\nplays against one opponent.\n\n• Opponent Selection Probability. In the multi-reward algorithm, the most recent 10 models of each playing style are selected to play against the agent with a probability p, while the other past versions are selected uniformly with probability 1 − p. As practiced in (Oh et al., 2019), p was set gradually decreased from 0.8 to 0.1 with the progress of the training.\n\nPARAMETERS FOR THE PROPOSED ALGORITHM.\n\n• Population Size (K = 30). This parameter controls the size the population in the evolution-\n\nary process.\n\n• Single-Point Crossover Probability (0.5). This parameter controls the probability of\n\nperforming the recombination of two networks.\n\n• Simulated Binary Crossover Probability (1.0). This parameter controls the probability of performing the recombination of two agents’ reward weights. The distribution index ηc = 20 was used (Deb et al., 2002).\n\n• Polynomial Mutation Probability (pm = 0.05). This parameter controls the probability of performing the disturbance of an agent’s reward weights. The distribution index ηm = 20 was used (Deb et al., 2002).\n\n• PPO Training Budget (P ong = 600, 000, Justice Online = 1, 000, 000). This parameter controls the training overheads (frames) of PPO in one generation of the evolutionary algorithm.\n\nIn Pong, the internal reward at step t for the algorithms PPO and REWARD SETTINGS IN PONG. PPO-SP was set to rt = 1 if the agent wins at that step; otherwise rt = 0. As for the algorithms multi-reward and BiO, different playing styles of agents are considered explicitly or implicitly. So a factor to reflect the agent’s playing style was also included in the reward function. Formally, the internal reward r at step t is expressed as\n\nwhere\n\nand\n\nrstyle\n\nt\n\n=\n\nrt = w1rskill\n\nt\n\n+ w2rstyle\n\nt\n\nrskill\n\nt\n\n=\n\n(cid:26)1,\n\nif the agent wins at step t\n\n0, otherwise\n\n(cid:26)1,\n\nif the position of the agent’s paddle in state st is different from that in st−1\n\n0, otherwise\n\n16\n\n(10)\n\n(11)\n\n(12)\n\nUnder review as a conference paper at ICLR 2023\n\nw1 and w2 are two weight parameters. Among them, w2 is to control agents’ playing styles. In the multi-reward algorithm, w1 was set to 1 across the three styles, while w2 was set to 0.01, 0 and −0.01 for the busy, neutral and lazy styles, respectively. In our algorithm, w1 and w2 are generated by the evolutionary search (i.e. simulated binary crossover and polynomial mutation) within the range of w1 ∈ [0, 1] and w2 ∈ [−0.01, 0.01].\n\nREWARD SETTINGS IN JUSTICE ONLINE. In Justice Online, the goal of the agent is to decrease the opponent’s Health Point (HP) while trying to keep its own HP non-decreasing. Therefore, the HP margin can be used as the internal reward. Specifically, in PPO and PPO-SP we considered the following internal reward at step t when the agent π plays against its opponent πk. t − HP π t denotes the HP value of the agent π at step t. The term (HP π t−1 − HP πk\n\nwhere HP π the agent to defend the opponent’s attack, and the term (HP πk to attack the opponent.\n\nt−1) is to encourage ) is to encourage the agent\n\nt−1) + (HP πk\n\nt−1 − HP πk\n\nrt = (HP π\n\nt − HP π\n\n(13)\n\n)\n\nt\n\nt\n\nAs for the algorithms multi-reward and BiO, since different playing styles of agents are considered, we added a term to reflect the agent’s playing style. Specifically, we used the following internal reward at step t when the agent π plays against its opponent πk.\n\nrt = w1(HP π\n\nt − HP π\n\nt−1) + w2(HP πk\n\nt−1 − HP πk\n\nt\n\n) + w3(distt(π, πk) − distt−1(π, πk))\n\n(14)\n\nwhere distt(π, πk) denotes the Euclidean distance between the agents π and πk at step t. Among the three weight parameters, w1 and w2 are concerned with the skill level while w3 controls the playing style. In the multi-reward algorithm, w1 and w2 were set to 1 across the three styles, while w3 was set to 1, 0 and −1 for the aggressive, neutral and defensive styles, respectively. In our algorithm, all the weights are generated by the evolutionary search within the range of w1 ∈ [0, 1], w2 ∈ [0, 1] and w3 ∈ [−1, 1]. Table 4 and Table 5 summarize the reward weight settings of the multi-reward algorithm and the proposed algorithm in the two environments, respectively.\n\nTable 4: Reward weights of each style in the multi-reward algorithm.\n\nEnvironments\n\nPlaying styles\n\nReward weights\n\nPong\n\nbusy\n\nneutral\n\nlazy\n\nw1 = 1, w2 = 0.01\n\nw1 = 1, w2 = 0\n\nw1 = 1, w2 = −0.01\n\naggressive\n\nw1 = 1, w2 = 1, w3 = 1\n\nJustice Online\n\nneutral\n\nw1 = 1, w2 = 1, w3 = 0\n\ndefensive\n\nw1 = 1, w2 = 1, w3 = −1\n\nTable 5: The range of reward weights for the search in the proposed algorithm.\n\nEnvironments\n\nWeight range\n\nPong\n\nw1 ∈ [0, 1], w2 ∈ [−0.01, 0.01]\n\nJustice Online w1 ∈ [0, 1], w2 ∈ [0, 1], w3 ∈ [−1, 1]\n\n17",
    "reference": "# Summary Of The Paper\n\nThe paper introduces a bi-objective optimization method for learning strategies for playing games from a pool of diverse agents. The method is population-based in the sense that it maintains a population of agents that attempts to optimize for a bi-objective function. Such a function accounts for the diversity of play, which is game-dependent, and winning rate. \n\nThe system was evaluated in the games of Pong and Justice Online, where the proposed method was competitive in some settings and stronger in others with respect to other population-based agents and RL baselines.\n\n# Strength And Weaknesses\n\nStrength\n\nThis paper deals with a challenging topic, which is the one of learning strategies for playing complex games. The paper works with the underlying hypothesis that one is able to learn stronger strategies while keeping a population of diverse agents. While other population-based works dealt with similar research question, the paper does a good job reviewing some of these works. The experiments in Pong and Justice Online seem to support this underlying assumption.\n\nWeaknesses\n\nA key weakness of the paper is to not discuss the paper \"A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning\" by Lanctot et al (2017). The paper talks vaguely about \"self play\" without explaining exactly which self-play algorithm they are referring to. Lanctot et al. describe a general framework for multiagent RL that allows one to instantiate algorithms such as Iterated-Best Response, Fictitious Play, and Double Oracle. As far as I know these are all self-play algorithms. In particular, Fictitious Play can be seem as a population-based algorithm and should probably be used as a baseline in the experiments. Why not consider these algorithms as baselines? \n\nThe paper is also constructed under the assumption that \"maintaining an agent population does not necessarily mean maintaining diverse playing styles.\" (see Related Work section). However, population-based agents such as AlphaStar were designed to learn a diverse set of playing styles. It isn't clear from the experiments whether the proposed method is able to produce \"more diverse agents\" than algorithms such as AlphaStar. \n\nAnother weakness is related to how opponents are sampled to generate the results shown in Table 3. The experiments starts with a pool of agents formed by 1 PPO agent, 1 PPT (this is probably a typo and authors meant PBT) agent, 3 Multi-Reward Agents, 30 EMOGI agents, and 30 BiO agents. Then, an opponent is randomly sampled from this pool of agents for each of the evaluated methods. The agents play a game and the result is stored. The process is repeated 60 times and the average win rate is reported in Table 3. \n\nThis experimental design is problematic because it gives an advantage to EMOGI and BiO as they represent a much bigger share of the pool of agents. That way, both EMOGI and BiO are likely to face, during test, agents they were trained with. Despite this significant disadvantage, PBT is still competitive with BiO. This makes me wonder whether BiO is bringing something new in comparison to other PBT agents.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clear and mostly well written. I would argue that adding PPO's equation doesn't  help readers who aren't familiar with the method and it doesn't teach anything new to readers already familiar with the method. The paper would be better off without Equation 1. \n\nThe appendix provides enough information to possibly reproduce the results from the paper.\n\n# Summary Of The Review\n\nPaper deals with a challenging and important problem, but misses an important part of the literature that could be used as baseline in the experiments. The empirical design seems to be problematic as it might be giving advantage to the method introduced in this paper.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCONCEPTUAL SCAN: LEARNING WITH AND ABOUT RULES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe ability to learn from a mix of rules and examples and to reflect on the learned abstractions is an important aspect of human intelligence. At the same time, there is a lack of benchmarks that systematically test for this ability, which makes it hard to evaluate the degree to which it is present in state-of-the-art ML architectures. We introduce a novel task format for such benchmarks by using an example structure that allows us to explicitly provide and ask about rules that are relevant for the given task. We present a simple dataset illustrating this format, and we use it to analyze the performance of a variety of T5-based ML models. We identify three challenge areas in this setup: maintaining consistency between learned rules and their application, scaling to larger rule sets, and compositional generalization.\n\n1\n\nINTRODUCTION\n\nMachine learning algorithms are typically designed to be able to learn functions from examples. This is a very general paradigm, but it does not explicitly capture some aspects of human learning. Humans, in contrast, are able to learn both by being shown examples of the task to accomplish and by being told rules or instructions about this task. They can even provide relevant rules to others once they have learned the task from examples.\n\nAs a realistic illustration of this ability, consider the task of a personal assistant who, among other things, is expected to make movie recommendations based on the age and interests of a user. Even for a task such as this that would currently be considered a standard use case for an example-based recommender system, as humans, we do not learn how to perform this task exclusively by observing examples of movie recommendations. Instead, we can accomplish this task much more efficiently by also taking into account relevant knowledge in the form of rules that have been communicated to us explicitly, i.e., by “learning with rules”. For recommending a movie to a girl called Anna, we may, among others, use the rules (and facts, which we consider a special case of rules) illustrated on the left side of Figure 1.\n\nIn addition to the ultimate goal of providing movie recommendations (e.g., “What movie could Anna watch?”), we would also expect a human to be able to answer the intermediate questions shown on\n\nFigure 1: Personal assistants answer questions using knowledge consisting of rules and facts. Note that the last knowledge bullet point above can also be construed as an example of the underlying “movie recommendation” task, while the other bullet points represent other relevant knowledge. The first bullet point is a traditional “rule” that states conditional knowledge that can apply to many different movies. The second is a concept definition, which can be equivalently construed as a rule relating two different pieces of information about a person. The other bullet points are facts stated at varying levels of granularity.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthe right side of Figure 1 – i.e., to “learn about rules” that are relevant to the ultimate task – and we would expect these questions to be answered consistently w.r.t. the provided knowledge and ultimate recommendation. These questions allow us to introspect the understanding of the assistant, e.g., to debug why a movie recommendation was not as expected.\n\nSimilar interactions between learning from examples and learning with and about rules can also be observed in simpler synthetic tasks. Consider, for instance, the SCAN task of Lake & Baroni (2017), which our work builds on. This task requires the learner to translate natural language commands (such as “jump left twice”) to corresponding actions (such as “LTURN JUMP LTURN JUMP”). The learner is presented with a certain subset of some thousands of (command, action sequence) pairs during training and is then expected to translate unseen commands.\n\nThis focus on learning purely from examples, while typical of most traditional ML tasks, differs from the way one would “teach” a human the task, and indeed how the authors of the SCAN paper “teach” the task to their readers. On the one hand, while humans are also adept at guessing rules from examples, rather than depending on thousands of examples, they can often grasp the relevant rule from just a handful of examples (Lake et al., 2015), as we as readers may find ourselves doing when seeing the handful of illustrative examples of SCAN provided by the authors in the paper figures. More fundamentally, however, rather than expecting readers to learn the translation function purely from examples, the authors provide this function in a much more direct and efficient fashion using a set of interpretation rules like those in Figure 2. The explicit nature of the provided rules has the additional advantage that it allows us to deduce the translation of a given command by applying the translation rules rather than having to always speculatively induce the translation by generalizing from a set of examples.\n\nFigure 2: Examples of SCAN interpretation rules from Lake & Baroni (2017).\n\n=WALK =LTURN (cid:74) x1 x1 (cid:75)\n\nwalk (cid:74) (cid:75) x1 left (cid:75) (cid:74) x1 twice (cid:74) (cid:75) ...\n\nx1\n\n(cid:75)(cid:74)\n\n=\n\n(cid:75)\n\n(cid:74)\n\nIn this paper, we introduce conceptual learning tasks (CLTs), which are a type of learning task that is specifically designed to evaluate the combination of such inductive and deductive learning, and we make the following contributions:\n\n• We define the notion of a CLT (Section 2).\n\n• We present a first simple instance of a CLT called Conceptual SCAN (cSCAN), which is a\n\nsynthetically-constructed conceptual learning variation of SCAN (Section 3).\n\n• We formalize metrics to measure a learner’s performance on cSCAN, including a novel measure-\n\nment of consistency between learned rules and their application (Section 4).\n\n• We analyze the performance of baseline ML architectures on cSCAN and identify three challenge\n\nareas: consistency, rule set size, and compositional generalization (Section 6).\n\n• We provide the code used in generating cSCAN, constructing compositional generalization splits,\n\nand calculating consistency from experiment results.1\n\n2 CONCEPTUAL LEARNING TASKS (CLTS)\n\n2.1 DESIRED PROPERTIES\n\nMotivated by the use case from the introduction and our goal of evaluating learning with and about rules, we interest ourselves in tasks with the following properties, which we formalize in the following section.\n\n1. Context. The learner answers requests based on some explicit knowledge (context), which consists of “examples” that directly determine the replies of certain requests and “rules” that provide indirect information to do so. Part of this knowledge varies across contexts (e.g., transient knowledge about Anna’s preferences or country-specific rules about movie ratings).\n\n2. Request. In addition to requests corresponding to the ultimate goal, (e.g., “What movie could Anna watch?”), we can ask the learner whether an intermediate rule holds given the context (e.g., “Is Jerry Maguire appropriate for 14-year-olds?”). This allows us to test whether the learner “understands” the rules by checking for consistency between rules that the learner claims to be true (or false) and their application to answering the ultimate requests.\n\n1To be released on GitHub upon paper acceptance. For an overview, see Appendices F and G.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3. Output. The learner needs to indicate in the output whether a reply follows deductively – and thus monotonically – from the context (e.g., “How old is Anna? ⇒ 14”) or whether it requires generalizing the context inductively (i.e, “Will Anna like Mission Impossible 6? ⇒ Probably”), which would imply that the reply could change if more information were to become available (i.e., that the reply is defeasible). Also, the learner needs to identify if a request cannot be answered based on the given context (e.g., “Who is Anna’s best friend? ⇒ I don’t know”).\n\n4. Compositionality of rules. The rules have a compositional structure, which means that it is possible to determine the meaning of an unknown rule from its syntax if we are given the meaning of a sufficient subset of rules. (E.g., we as humans understand the meaning of the rules shown in Figure 1 because of the compositional syntax and semantics of natural language. Even if we have never seen these exact sentences before, we know what they mean based on our exposure to other sentences built from the same natural language building blocks.)\n\n2.2 STRUCTURAL DEFINITION\n\nAs a way of providing a concrete but generic task format that satisfies these properties, we define a conceptual learning task (CLT) as a supervised learning task T with the following structure. (See Appendix A for a discussion of the design choices.)\n\n• The task T = {e1,...,eN} is a finite set of examples ek ∈ E, where E = 2Q×R × Q × R × U\n\ndenotes the set of possible examples.\n\n• Each example ek = (cid:104)Ck,qk,rk,uk(cid:105) is a quadruple consisting of a context Ck ∈ 2Q×R, a request qk ∈Q, a reply rk ∈R, and a qualifier uk ∈U. Of these, the context and request together form the input of the ML system, while the reply and qualifier together form the output. (See Appendix O for details on the exact format in which these are input and output by our T5 baseline systems.)\n\n• The set of possible requests Q can be partitioned into rule requests QR ⊆Q (i.e., requests that\n\nask whether a certain rule holds) and non-rule requests QN ⊆Q.\n\n• The set of possible replies R must contain dedicated elements representing true (1), false (0),\n\nand unknown (?), which are the only valid replies for rule requests q∈QR.\n\n• The set of qualifiers U = {M,D} indicates whether the reply follows monotonically from the\n\ncontext (M) or whether it is defeasible (D).\n\n• Each context C ∈2Q×R consists of a set of context examples ei =(cid:104)qi,ri(cid:105), which represent either examples of an underlying task or other relevant knowledge expressed in “example” form. Note that unlike the top-level examples, these context examples do not themselves contain a context or qualifier, as for the purposes of this paper, we take all context examples to be unconditional and monotonic. (See Appendix C for a possible generalization of this definition.)\n\nNote that even though the context is strictly a set of examples, it may still contain any rule q ∈ QR by means of including the example (cid:104)q,1(cid:105), which asserts the rule q to be true.\n\nAs a more complete illustration, Figure 3 shows a few examples from the cSCAN task, which is a CLT that we introduce in Section 3, while Appendix B shows the motivating example represented as a CLT.\n\n2.3 CONSISTENCY REQUIREMENTS\n\nIn addition to satisfying the structural definition, for the purposes of this paper, we require CLTs to be logically consistent, which means that each example and the example set as a whole must be noncontradictory. For instance, if a CLT contains a monotonic example (cid:104)C,q,r,M(cid:105), it may not contain an example (cid:104)C(cid:48),q,r(cid:48),M(cid:105) where C ⊆C(cid:48) and r (cid:54)=r(cid:48) as this would violate monotonicity. (Or using the example from the introduction, this would mean, for instance, that if for a given context the task contains “How old is Anna? ⇒ 14” and “Is Jerry Maguire appropriate for 14-year-olds? ⇒ No”, it should not contain “Is Jerry Maguire appropriate for Anna ⇒ Yes”.) While this requirement could be relaxed in practice, consistency of the task is helpful, as it enables us to precisely measure the consistency of the learner.\n\nFor the task to be logically consistent, it requires at minimum that the monotonic examples adhere to the axioms of propositional logic. (See Appendix D for a formalization of the consistency requirements.)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nNote that even while requiring a CLT to be logically consistent, we still allow for the common phenomenon in which rules are stated in a form that is only “usually” true. Exceptions to a rule are allowed so long as the rule is assigned the qualifier “defeasible” (D).\n\n2.4 CLASSIFICATION OF EXAMPLES\n\nThe structure of CLTs allows us to classify examples according to the following dimensions.\n\n• Request q∈Q: Rule request (q∈QR) vs. non-rule request (q∈QN)\n\n• Reply r ∈R: Known vs. unknown (?), and, for rule requests, true (1) vs. false (0)\n\n• Qualifier u∈U: Monotonic (M) vs. defeasible (D)\n\nEach class of example should be reasonably well represented in a conceptual learning dataset.\n\n3 CONCEPTUAL SCAN (CSCAN)\n\nOne benefit of abstracting out key properties of interest from our motivating use case into the definition of a CLT is that we can now seek to study the capabilities of current ML methods on this family of tasks by starting with a very simple CLT instance, which illustrates the key dynamics of a CLT in as pure as possible a form. In particular, it gives us a basis by which we can construct a simple synthetic task that enables exploration of ML system performance on CLTs, while carefully controlling the task complexity. In this section, we present one such “simplest possible” CLT, named Conceptual SCAN (cSCAN). cSCAN is a conceptual learning adaptation of the SCAN task (Lake & Baroni, 2017), which was itself originally presented as a kind of “simplest possible” compositional generalization task. We construct cSCAN according to a recipe illustrating one possible way of deriving a CLT from a base task.\n\n3.1 BASE TASK: SCAN\n\nSCAN is a task where natural language commands (such as “jump left twice”) are translated into sequences of actions (such as “LTURN JUMP LTURN JUMP”). SCAN was designed to evaluate compositional generalization using non-standard train-test splits, which is one of the themes we explore in this paper. In addition, SCAN is an attractive choice of base task for a first conceptual learning benchmark because it can be generated automatically from a simple set of rules: a phrase-structure grammar for generating the valid input commands and a set of interpretation rules that specifies how to compute the action sequence for each command (see Figure 2 and Appendix E.1).\n\n3.2 CONSTRUCTING CSCAN\n\nIn contrast to SCAN, which tests whether a learner can learn one specific translation from commands to actions, the goal of cSCAN is to test whether the learner can learn to perform a family of SCAN-like tasks using knowledge that consists of an arbitrary mix of rules and examples.\n\nNote that moving from a single task to a family of related tasks is essential for cSCAN because it forces the learner to take into account the knowledge provided by the context rather than just memorize behavior that is constant across all examples. In our original motivating example, this corresponds to the fact that we do not want a learner to make movie recommendations in a single, fixed scenario, but rather based on knowledge that may differ from person to person and evolve over time.\n\ncSCAN is constructed using the following recipe.\n\nStep 1: Accommodate the base task. A CLT subsumes the base task. This means that any input of SCAN is a valid request q ∈ Q, and any output of SCAN is a valid reply r ∈ R.\n\nStep 2: Make some of the relevant knowledge available as explicit rules. We choose which part of the relevant knowledge we want to be able to talk about explicitly and vary across examples. For cSCAN, we choose to talk only about interpretation rules like those shown in Figure 2. This means that any interpretation rule is a valid request r ∈R, which allows us to assert such a rule in the context (by adding the example (cid:104)r,1(cid:105) to the context) and teach / ask the learner whether any such rule holds. CLTs require that explicit rules have a\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Hypothetical cSCAN examples based on two different contexts that contradict each other.\n\ncompositional structure (Section 2.1). This is the case for cSCAN because the meaning of the interpretation rules can be determined using a compositional grounding function that we provide in Appendix E.\n\nStep 3: Generate or curate examples. For cSCAN, we generate examples automatically using a Python program, which we summarize here and describe in detail in Appendix F. As a first step of generating each example, we create a context by first picking a coherent set of interpretation rules like those shown in Figure 2 (or in Figure 4 from the appendix) and then choosing which of those rules to (a) provide explicitly in the context, (b) illustrate implicitly through context examples, (c) illustrate insufficiently or not at all, or (d) contradict in one or more cases. Once we have fixed a context, we choose a request, reply and qualifier that, together with the context, satisfy the consistency criteria stated in Section 2.3 and agree with the task’s assumed inductive bias for distinguishing between a rule being “defeasibly true” vs. “unknown” (see Appendix E.4).\n\nWe make sure that the different example classes are evenly represented (see the statistics in Section 3.3) and that, for simplicity, contexts contain only unconditional, monotonic examples that do not conflict with one another. We provide a detailed specification of cSCAN in Appendix E.\n\n3.3 THE CSCAN DATASET\n\nExamples. Figure 3 shows some hypothetical cSCAN examples in human-friendly notation. (We chose these examples for simplicity and conciseness. For a sample of actual cSCAN examples, see Appendix I. For examples in the exact format in which they are presented to the baseline systems, see Appendix O.)\n\nThese examples are based on two contexts (C1 and C2) that are incompatible (e.g., they define the meaning walk differently) and correspond to different SCAN variations. Each context contains some explicit of (cid:75) (cid:74) in C1) as well as rules that are illustrated implicitly via examples (e.g., =\nrules (e.g., (cid:75) (cid:75)(cid:74) in C1). The qualifier of the examples indicates whether any implicit rules are needed x1 twice (cid:75) (cid:74) to determine the reply (in which case the qualifier is D).\n\nx1 and x2 (cid:74) =\n\nx1 (cid:74)\n\nx1 (cid:74)\n\nx2\n\nx1\n\n(cid:75)(cid:74)\n\n(cid:75)\n\n(cid:75)\n\n=\n\n(cid:75)(cid:74)\n\n(cid:75)(cid:74)\n\nx1\n\nx1\n\nx1 thrice In context C2, we do not provide explicit rules for . While the rule (cid:75) (cid:74) x1 x1 twice is expected to be induced from the provided examples, there is no obvious (cid:74) (cid:74) (cid:75) x1 thrice . As a consequence, we expect the learner to reply “unknown” rule that can be induced for (cid:75) (cid:74) (?) for the request “jump thrice”. At the same time, we expect the learner to reply “false” for the rule x1 thrice (cid:74) (cid:75) Rule space variants. In order to cover a range of rule set complexities, we construct two versions of cSCAN (cSCAN-B and cSCAN-X), using different sizes of rule space (Table 1).\n\nbecause there is an example in the context that contradicts it.\n\nx1 twice (cid:75) (cid:74)\n\nas well as\n\nx1 (cid:74)\n\nx1\n\n(cid:75)(cid:74)\n\n=\n\n(cid:75)\n\n(cid:75)\n\ncSCAN-B (short for “cSCAN-Base”) uses a fixed phrase-structure grammar equivalent to that of the original SCAN task, as reformulated in Nye et al. (2020) using 14 interpretation rules. Action sequences are kept short by allowing a token or variable to be repeated at most twice in an interpretation rule’s output sequence. This ensures that cSCAN-B examples do not exceed input size of 2048 tokens or output size of 256 tokens in our baseline models.\n\ncSCAN-X (short for “cSCAN-Extended”) is based on a richer grammar space, which extends the original SCAN phrase-structure grammar with additional terminals, variations of existing rules, and an additional level that enables adverbs. Output sequences for individual interpretation rules are allowed to contain up to 4 repetitions of any given token or variable, which is the same as in original SCAN, but longer than in cSCAN-B. To keep context sizes manageable, we apply rule sampling for each context, so that the number of interpretation rules actually used in any given context is the same as in cSCAN-B, and apply a limit to the\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: cSCAN rule space variants. cSCAN-B uses a fixed phrase-structure grammar equivalent to that of the original SCAN task, while keeping action sequences short. cSCAN-X is based on a richer grammar space, while using rule sampling to keep the number of rules used in any given context equivalent to the number of rules in cSCAN-B and the reformulation of original SCAN.\n\nVariant\n\nCommand grammar Action sequences Context Depth Terminals Max length\n\nUnderlying rules\n\nMax # tokens Input Output\n\ncSCAN-B cSCAN-X Reformulated SCAN\n\n6 7\n6\n\n13 36 13\n\n4 8\n8\n\n2048 4096\n\n256 512\n\n14 14 14\n\nTable 2: Key statistics of the main cSCAN datasets. For full details, see Appendix H.\n\ncSCAN-B Random cSCAN-X Random cSCAN-B MCD\n\ntrain\n\ntest\n\ntrain\n\ntest\n\ntrain\n\ntest\n\ncSCAN-X MCD test\n\ntrain\n\nNumber of examples Number of contexts\n\n100,000 1,000\n\n100,000 100\n\n100,000 1,000\n\n100,000 100\n\n100,000 11,921\n\n10,000 6,509\n\n99,999 7,965\n\n10,000 4,599\n\nRequest: Rule (%) Request: Non-rule (%)\n\nQualifier: Monotonic (%) Qualifier: Defeasible (%)\n\nReply: Unknown (%) Reply: Yes (%) Reply: No (%)\n\n50.1 49.9\n\n46.3 53.7\n\n13.3 21.8 21.7\n\n50.0 50.0\n\n46.0 54.0\n\n13.1 21.8 21.9\n\n50.1 49.9\n\n50.0 50.0\n\n13.2 21.9 21.7\n\n50.0 50.0\n\n48.0 52.0\n\n13.6 21.5 22.1\n\n72.5 27.5\n\n26.9 73.1\n\n52.2 12.0 12.1\n\n64.0 36.0\n\n36.6 63.4\n\n35.2 18.1 18.2\n\n59.8 40.2\n\n37.0 63.0\n\n39.7 12.7 12.9\n\n67.4 32.6\n\n38.9 61.1\n\n36.3 18.5 19.2\n\ncumulative output sequence size across all interpretation rules used in the given context. These measures ensure that cSCAN-X examples do not exceed input size of 4096 tokens or output size of 512 tokens.\n\nSplitting methods. For each of the two sizes of rule space, we prepare datasets based on two splitting methods: random and maximum compound divergence (MCD) (Keysers et al., 2020).\n\nFor the cSCAN Random variants, we generate 1200 contexts and split these contexts randomly into a train set of 1000 contexts and validation and test sets of 100 contexts each. For the train set, we generate 100 top-level examples per context, while for the validation and test contexts, we generate 1000 top-level examples per context so as to provide a denser signal of potential implications among the examples of each context for use in calculating the consistency metric. This leads to a total of 100K top-level examples in each of the train, validation and test sets.\n\nFor the cSCAN MCD variants, we apply a variation of the MCD splitting algorithm of Keysers et al. (2020) in order to evaluate the ability of the system to compositionally generalize to new rule combinations, which we consider a stronger test of the system’s ability to “understand” the meaning of the rules and to apply them correctly in new situations. Specifically, we start by generating 12K contexts with 100 top-level examples each, yielding a pool of 1.2M top-level examples. We then annotate each top-level example with a set of atoms and compounds based on the phrase-structure grammar rules that were composed to form the toplevel example request, and we split the set of top-level examples in such a way as to maximize the divergence in the distribution of compounds between train, validation, and test, while keeping the distribution of atoms nearly the same. Similarly to Keysers et al. (2020), we down-sample during the splitting process for more effective control of the distribution divergences, leading to a total of 100K top-level examples in train (comparable to cSCAN Random) and 10K top-level examples in each of validation and test.\n\nStatistics. Table 2 gives an overview of the key statistics of the cSCAN Random datasets and representative MCD datasets. (See Appendix H for details of other cSCAN dataset variants.)\n\nWe construct the examples such that all the classes discussed in Section 2.4 are well represented. In particular, the Random datasets contain roughly equal numbers of rule vs. non-rule, as well as examples with replies of “true” (1) vs. “false” (0). There are slightly more defeasible examples than there are monotonic examples because “unknown” (?) examples are always qualified as defeasible.\n\nNote that compared to the random split, the splitting method used in the cSCAN MCD variants leads to a somewhat less balanced dataset in terms of example classes, although each of the classes is still well\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Accuracy vs. consistency. Learner A is more accurate, but learner B is more consistent.\n\nGolden (cid:104)C1, x1 twice (cid:74) (cid:75) (cid:104)C1, walk (cid:75) (cid:74) (cid:104)C1,walk twice,WALK WALK(cid:105)\n\nx1 x1 (cid:74) =WALK,1(cid:105)\n\n,1(cid:105) (cid:75)\n\n(cid:75)(cid:74)\n\n=\n\nLearner A (cid:104)C1, x1 twice (cid:74) (cid:75) (cid:104)C1, walk (cid:75) (cid:74) (cid:104)C1,walk twice,JUMP JUMP(cid:105)\n\nx1 x1 (cid:74) =WALK,1(cid:105)\n\n,1(cid:105) (cid:75)\n\n(cid:75)(cid:74)\n\n=\n\nLearner B (cid:104)C1, x1 twice x1 (cid:74) (cid:75) (cid:75)(cid:74) (cid:74) (cid:104)C1, =JUMP,1(cid:105) walk (cid:75) (cid:74) (cid:104)C1,walk twice,JUMP JUMP JUMP(cid:105)\n\n,1(cid:105) (cid:75)\n\nx1\n\nx1\n\n(cid:75)(cid:74)\n\n=\n\ncovered. Also, while it leads to a challenging split in terms of generalization to new top-level request patterns, it is potentially easier than the random split in terms of the contexts shown, as we do not prevent the same context from appearing in train and test, and due to the effect of down-sampling from a larger context pool, the total number of contexts that are shown in the MCD train set is an order of magnitude greater than those shown in the random train set.\n\n4 METRICS\n\nwalk twice (cid:75) (cid:74)\n\nAccuracy Our primary accuracy metric is example-level accuracy, where credit is given only when the reply + qualifier exactly matches the ground truth. For more nuanced error analysis, we secondarily track several accuracy variants that give credit for partially correct outputs (see Appendix N). Consistency A key aspect of learning with rules is that a learner does not just learn how to memorize and recite rules, but is actually able to combine and apply them consistently. For instance in x1 twice = JUMP hold, a cSCAN, if the learner believes that the rules (cid:75) (cid:74) consistent learner should also believe that all combinations and applications of these rules hold, such as\n\n= JUMP JUMP JUMP.\n\nwalk (cid:75) (cid:74)\n\nx1 (cid:74)\n\nand\n\nx1\n\nx1\n\n(cid:75)(cid:74)\n\n(cid:75)(cid:74)\n\n=\n\n(cid:75)\n\nNote that unlike accuracy, this notion of consistency is not judging the correctness of individual predictions. Instead, it judges to what degree a whole set of predictions is consistent within itself. While a perfectly accurate learner would also be perfectly consistent, when accuracy is low to moderate, consistency can potentially be quite orthogonal to accuracy.\n\nAs an illustration, consider Table 3, which shows a golden example set as well as the predictions of two learners A and B. Learner A is more accurate because it gets 2 examples correct, whereas learner B gets none of the examples correct. At the same time, learner A is not consistent, because it is not able to correctly apply the two rules that it believes in to derive the example (cid:104)walk twice,WALK WALK(cid:105). In contrast, learner B is perfectly consistent, as it correctly combines the rules it believes in to derive the example (cid:104)walk twice, JUMP JUMP JUMP(cid:105).\n\nTo capture this notion of consistency, we introduce for any set of predictions E ⊆E the consistency metric C(E), which is the percentage of subsets of E that contain a logical implication, in comparison to the number of subsets of E that contain an implication or a contradiction. This means that C(E) is a value in [0,100]: C(E)=100 says that the set E is perfectly consistent, while C(E)=0 says that E is completely inconsistent. (See Appendix D for a formalization of this metric and Appendix G for practicalities of calculation.)\n\nFor learner A, the example (cid:104)C1,walk twice,JUMP JUMP(cid:105) contradicts the rules (cid:104)C1, x twice (cid:74) (cid:75) and (cid:104)C1, walk (cid:74) (cid:75) B, the example (cid:104)C1,walk twice,JUMP JUMP JUMP(cid:105) is implied by the rules (cid:104)C1, =\nx twice (cid:75) (cid:74) and (cid:104)C1, walk (cid:75) (cid:74)\n\n,1(cid:105) x\n(cid:75) = WALK,1(cid:105), which means that the consistency of learner A is 100·0/1 = 0. For learner ,1(cid:105) x\n(cid:75)\n\nx (cid:74) = JUMP,1(cid:105), which means that the consistency of learner B is 100·1/1 = 100.\n\nx (cid:74) x\n\n(cid:75)(cid:74)\n\n(cid:75)(cid:74)\n\n(cid:75)(cid:74)\n\n=\n\n5 BASELINES\n\nAs baselines, we evaluate variations of T5 (Raffel et al., 2019), a Transformer encoder-decoder model (Vaswani et al., 2017), which when pre-trained on natural language served as a strong baseline on the original SCAN task (Furrer et al., 2020; Csord ́as et al., 2021; Onta ̃n ́on et al., 2021).\n\nThe most computationally and memory intensive of the T5 architectures that we evaluate is the standard T5 architecture, which applies full self-attention in the encoder, in addition to self-attention and cross-attention in the decoder. We refer to these models as simply T5.\n\nMotivated by the potentially large context size associated with CLTs, we further evaluate two variants of T5 that were designed to more efficiently scale to longer input sequences. LongT5 (Guo et al., 2022) reduces the computational load of attention in the encoder by applying local attention within a sliding window (Ainslie et al., 2020). LongT5-TGlobal (Guo et al., 2022) extends LongT5 with a local-global\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Test results on cSCAN random splits by model size: S (Small), B (Base), and L (Large).\n\ncSCAN-B Random\n\ncSCAN-X Random\n\nAccuracy\n\nConsistency\n\nAccuracy\n\nConsistency\n\nModel\n\nPretrain S\n\nB\n\nL\n\nS\n\nB\n\nL\n\nS\n\nB\n\nL\n\nS\n\nB\n\nL\n\nT5\n\nTrue False LongT5-TGlobal False False LongT5 True T5 w/o Context\n\n92.5 92.6 96.5 19.3 17.9 20.0 17.0 19.5 21.4 26.8\n\n71.8 71.3 87.5 0.0 0.0 0.1 0.1\n\n0.0 0.2 0.0\n\n68.3 71.0 81.6 16.8 17.2 18.0 18.8 18.1 16.7 23.8\n\n30.1 32.4 43.5 0.0 0.1 0.1 0.4\n\n0.1 0.2 0.1\n\nattention sparsity pattern (Ainslie et al., 2020; Zaheer et al., 2020). The global attention is designed to facilitate propagation of attention across the full input sequence within two hops, in comparison with pure local attention, in which propagation of attention is limited by the size of the attention window.\n\nFor each architecture, we evaluate at minimum two sizes: Small (60M parameters) and Base (220M parameters). For the best-performing T5 architecture, we further evaluate on size Large (770M parameters). We report results for both pre-trained and non-pretrained versions of standard T5, but only non-pretrained versions of the long variants, as we failed to find a converging setup for the pre-trained versions (see Appendix L for details). For comparison, we also evaluate a naive variant of T5-Base (T5 w/o Context), which considers only the request, while ignoring the context. Additional details of the baseline configurations are provided in Appendix L and of the input-output format in Appendix O.\n\n6 EXPERIMENTAL RESULTS AND DISCUSSION\n\n6.1 CONSISTENCY AND SCALING TO LARGER RULE SETS\n\nAs a first set of experiments, we compare baseline performance on the random splits of the cSCAN-B and cSCAN-X datasets. The results are shown in Table 4.\n\nOn the smaller cSCAN-B rule space, it can be seen that the provided 100K examples are sufficient for a pre-trained full-attention Transformer to achieve accuracies in excess of 90%, with accuracies increasing steadily with model size. Even in this relatively simple setting, however, several challenges can be observed. First, it appears that appropriate pre-training of the model is critical, as all the Transformer variants when trained from scratch managed to learn only superficial statistical correlations, as evidenced by them failing to outperform a naive baseline. Second, regarding LongT5 and LongT5-TGlobal, while it is possible that performance could be improved through more thorough hyperparameter tuning, our initial results show these to struggle on the conceptual learning task. Specifically, non-pretrained versions suffer from the same poor performance as non-pretrained T5, while when fine-tuning from an existing checkpoint, we were not able to find a converging setup. One possible explanation is that unlike document summarization tasks, for which LongT5 produced strong results (Guo et al., 2022), CLTs may depend heavily on full attention over the context. If so, this could pose challenges in scaling to real-world conceptual learning tasks with even larger contexts. Third, while consistency scores for the evaluated models correlate roughly with accuracy, significantly more headroom is seen in consistency, with even the best-performing T5-Large scoring under 0.9, while the naive baseline and all non-pretrained models score close to 0.\n\nOn the cSCAN-X rule space, accuracy drops significantly for all sizes of T5, suggesting that scaling to larger rule sets will be a challenge. Consistency continues to correlate with accuracy for these models, but drops rapidly as performance degrades. Non-pretrained models continue to fail to outperform the naive baseline.\n\n6.2 COMPOSITIONAL GENERALIZATION\n\nAs a second set of experiments, we evaluate the ability for baseline solutions to compositionally generalize on CLTs using the cSCAN-B MCD and cSCAN-X MCD datasets (Table 5).\n\nPrior research on semantic parsing tasks has shown that while pre-trained Transformers exhibit strong performance on specialized cases of compositional generalization, they tend to struggle with more complex forms of compositional generalization, as reflected in low performance on MCD splits when an appropriate notion of “atom” and “compound” is identified (Furrer et al., 2020). Here we show that in the context of a conceptual learning task, one form of compositional generalization that is challenging for T5-based\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nmodels is generalization to new syntactic patterns in the request, even in the relatively easy setting where the same contexts could appear in train as in test.\n\nTable 5: Test accuracy on different cSCAN MCD splits by model size: S (Small), B (Base), and L (Large).\n\nSpecifically, as can be seen in the cSCAN-B MCD results, when switching from random to MCD split, accuracy drops from over 90% to less than 70%, even for the largest of the pre-trained T5 models, illustrating that compositional generalization is a challenge for these models, independently of the size of the rule space. Accuracy on cSCAN-X MCD is roughly similar to both cSCAN-B MCD and cSCAN-X Random, suggesting that the challenges of compositional generalization and rule space size do not necessary compound.\n\nTrue False LongT5-TGlobal False False LongT5 True T5 w/o Context\n\n69.2 39.9 40.9 41.2 42.7\n\n54.7 38.4 39.3 38.0 46.2\n\n53.8 40.1 40.6 40.7\n\n70.7 41.6 41.0 40.9\n\ncSCAN-X MCD\n\ncSCAN-B MCD\n\nPretrain\n\nModel\n\n76.4\n\n67.6\n\nT5\n\nB\n\nB\n\nL\n\nL\n\nS\n\nS\n\nNote also that while accuracies for non-pretrained models are somewhat higher on the MCD splits than on the random splits, this is not actually a sign of stronger performance, but rather simply an artifact of the mix of example classes that occur in the different splits, due to the down-sampling that is performed when constructing the MCD split. As shown in Table 8, a side effect of this splitting algorithm was a relative increase in the proportion of defeasible rule examples vs. monotonic or non-rule examples and in the proportion of examples with “unknown” as the reply. This leads to an increase in the accuracy achievable by the naive “T5 w/o Context” baseline. On the MCD splits, like the random splits, none of the non-pretrained models manage to outperform the naive baseline.\n\nFor the MCD splits, we do not report consistency metrics, as due to technicalities of the MCD splitting algorithm, there ended up being insufficient signal for logical implications among the test examples, leading the consistency metric to be undefined in most cases. (See Appendix F for details.)\n\n7 RELATED WORK\n\nHere we discuss the most closely related lines of research. See Appendix K for more related work.\n\nTasks providing knowledge as context. In representing the input of a CLT as a request paired with a context, we build on a long tradition of QA and reasoning task formulations that provide knowledge relevant to a task via various forms of context, such as a text passage (Kwiatkowski et al., 2019; Weston et al., 2015), set of natural language statements (Talmor et al., 2020), knowledge graph fragment (Sinha et al., 2020), or grid world (Ruis et al., 2020). Our formulation of a CLT is novel in adopting a set-like context mixing rules and examples, which varies materially across examples.\n\nMeta-learning. The presence of examples within the context of an example gives CLTs a nested structure that allows us to view CLTs through the lens of meta-learning or “learning to learn” (Thrun & Pratt, 1998). In this view, top-level examples that share the same context correspond to to an episode where the context examples are the training examples and the top-level examples (w/o the context) are the test examples. Closely related to cSCAN are two pieces of work that apply meta-learning to SCAN (Lake, 2019; Nye et al., 2020). Our approach differs in that we include in the context a mixture of rules and examples, and we use the synthetically-generated contexts to define a new task, rather than as a means to improve accuracy on the original SCAN task.\n\n8 CONCLUSIONS AND FUTURE WORK\n\nIn this paper, we presented the cSCAN benchmark as a first instance of a “conceptual learning task” (CLT), following a task format motivated by a personal assistant use case. Through experiments on baseline solutions, we identified several challenge areas with headroom for improvement. As next steps, we are interested in exploring solutions to CLTs, including prompting of large language models, neuro-symbolic solutions, and improved ways of handling large set-like contexts. In parallel, we are interested in exploring CLTs based on a wider range of base tasks and rule formats, including non-synthetic tasks and tasks that draw on a full KB as context.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n9 REPRODUCIBILITY STATEMENT\n\nExperiments Appendix L describes the details of the baseline configurations that we used, together with other details of the environment in which we ran the experiments reported in this paper, while Appendix O provides details of the input-output format. Upon paper acceptance, we plan to release on GitHub both the cSCAN datasets and the code needed to reproduce the experiments. Dataset generation The cSCAN datasets themselves were synthetically generated using a configurationdriven Python program described in Appendix F, which we also plan to open-source, together with the specific configurations used for each of the cSCAN datasets. While regeneration of the datasets is not necessary for reproducing our experiment results, researchers can use this code to generate new conceptual learning datasets based either on the existing cSCAN grammar spaces or on modified grammars. When the code is run with the provided configurations, it can reproduce the generation of datasets with statistically comparable content to the official cSCAN datasets.\n\nREFERENCES\n\nJoshua Ainslie, Santiago Onta ̃n ́on, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured data in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020), 2020.\n\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In International Conference on Computer Vision (ICCV), 2015.\n\nDzmitry Bahdanau, Harm de Vries, Timothy J O’Donnell, Shikhar Murty, Philippe Beaudoin, Yoshua Bengio, and Aaron Courville. Closure: Assessing systematic generalization of clevr models. arXiv preprint arXiv:1912.05783, 2019a.\n\nDzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville. Systematic generalization: What is required and can it be learned? In International Conference on Learning Representations, 2019b. URL https://openreview.net/forum? id=HkezXnA9YX.\n\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\n\nSteven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text\n\nwith the natural language toolkit. ” O’Reilly Media, Inc.”, 2009.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2015.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/ google/jax.\n\nPawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I ̃nigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gasi ́c. MultiWOZ - a large-scale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 5016–5026, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1547. URL https://aclanthology.org/ D18-1547.\n\nGiovanni Casini, Thomas Meyer, and Ivan Varzinczak. Contextual conditional reasoning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(7):6254–6261, May 2021. URL https://ojs. aaai.org/index.php/AAAI/article/view/16777.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nXinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. Compositional generalization via neural-symbolic stack machines. Advances in Neural Information Processing Systems, 33:1690–1701, 2020.\n\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.\n\nWilliam W Cohen. Grammatically biased learning: Learning logic programs using an explicit antecedent\n\ndescription language. Artificial Intelligence, 68(2):303–366, 1994.\n\nWilliam W Cohen. Fast effective rule induction. In Machine learning proceedings 1995, pp. 115–123.\n\nElsevier, 1995.\n\nR ́obert Csord ́as, Kazuki Irie, and J ̈urgen Schmidhuber. The devil is in the detail: Simple tricks improve systematic generalization of transformers. CoRR, abs/2108.12284, 2021. URL https://arxiv. org/abs/2108.12284.\n\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment, 2005. URL http://www.cs.biu.ac.il/ ̃glikmao/rte05/.\n\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\n\ntransformers. arXiv preprint arXiv:1807.03819, 2018.\n\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\n\nnetworks. In International conference on machine learning, pp. 1126–1135. PMLR, 2017.\n\nDaniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Sch ̈arli. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. arXiv preprint arXiv:2007.08970, 2020.\n\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017.\n\nDan Goldwasser and Dan Roth. Learning from natural instructions. Machine learning, 94(2):205–232,\n\n2014.\n\nJerzy W. Grzymala-Busse. Data Mining and Knowledge Discovery Handbook, chapter Rule Induction, pp. 249–265. Springer, Boston, MA, 2010. ISBN 978-0-387-09822-7. URL https://doi.org/10. 1007/978-0-387-09823-4_13.\n\nAlbert Gu, Karan Goel, and Christopher R ́e. Efficiently modeling long sequences with structured state\n\nspaces. arXiv preprint arXiv:2111.00396, 2021.\n\nMandy Guo, Joshua Ainslie, David Uthus, Santiago Onta ̃n ́on, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 724–736, 2022. URL https://aclanthology. org/2022.findings-naacl.55.\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International Conference on Machine Learning, pp. 3929–3938. PMLR, 2020.\n\nJonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL http: //github.com/google/flax.\n\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas M ̈uller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nTimothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):5149–5169, 2021.\n\nDrew A. Hudson and Christopher D. Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. URL https://arxiv.org/pdf/1902. 09506.pdf.\n\nJustin Johnson, Bharath Hariharan, Laurens van der Maaten, Fei-Fei Li, Lawrence C. Zitnick, and Ross Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017. URL https://arxiv.org/pdf/1612.06890.pdf.\n\nDaniel Keysers, Nathanael Sch ̈arli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: A comprehensive method on realistic data. In ICLR, 2020. URL https://arxiv.org/abs/1912.09713.pdf.\n\nKevin C. Klement. Propositional logic. In Internet Encyclopedia of Philosophy. 2004.\n\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword toIn Eduardo Blanco and Wei Lu (eds.), Prokenizer and detokenizer for neural text processing. ceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018, pp. 66–71. doi: 10.18653/v1/d18-2012. URL https: Association for Computational Linguistics, 2018. //doi.org/10.18653/v1/d18-2012.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.\n\nBrenden M Lake. Compositional generalization through meta sequence-to-sequence learning. In Advances\n\nin Neural Information Processing Systems, pp. 9788–9798, 2019.\n\nBrenden M. Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of\n\nsequence-to-sequence recurrent networks. 2017.\n\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through\n\nprobabilistic program induction. Science, 350(6266):1332–1338, 2015.\n\nBrenden M. Lake, Tal Linzen, and Marco Baroni. Human few-shot learning of compositional instructions. In Ashok K. Goel, Colleen M. Seifert, and Christian Freksa (eds.), Proceedings of the 41th Annual Meeting of the Cognitive Science Society, CogSci 2019: Creativity + Cognition + Computation, Montreal, Canada, July 24-27, 2019, pp. 611–617. cognitivesciencesociety.org, 2019. URL https://mindmodeling.org/cogsci2019/papers/0123/index.html.\n\nHector J. Levesque, Ernest Davis, and Leora Morgenstern. The Winograd Schema Challenge.\n\nIn Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR’12, pp. 552–561. AAAI Press, 2012. ISBN 978-1-57735-560-1. URL https: //cs.nyu.edu/faculty/davise/papers/WSKR2012.pdf.\n\nTao Li, Vivek Gupta, Maitrey Mehta, and Vivek Srikumar. A logic-driven framework for consistency of neural models. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), EMNLP/IJCNLP (1), pp. 3922–3933. Association for Computational Linguistics, 2019. ISBN 978-1-950737-90-1. URL http://dblp.uni-trier.de/db/conf/emnlp/emnlp2019-1.html#LiGMS19.\n\nQian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and Dongmei Zhang. Compositional generalization by learning analytical expressions. Advances in Neural Information Processing Systems, 33:11416–11427, 2020.\n\nMaxwell Nye, Michael Tessler, Josh Tenenbaum, and Brenden M Lake.\n\nImproving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Advances in Neural Information Processing Systems, 34:25192–25204, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nMaxwell I. Nye, Armando Solar-Lezama, Joshua B. Tenenbaum, and Brenden M. Lake. Learning\n\ncompositional rules via neural program synthesis, 2020.\n\nSantiago Onta ̃n ́on, Joshua Ainslie, Vaclav Cvicek, and Zachary Fisher. Making transformers solve compositional tasks. CoRR, abs/2108.04378, 2021. URL https://arxiv.org/abs/2108. 04378.\n\nLong Ouyang, Jeff Wutrain, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https: //arxiv.org/abs/2203.02155.\n\nPanupong Pasupat, Yuan Zhang, and Kelvin Guu. Controllable semantic parsing via retrieval augmentation.\n\narXiv preprint arXiv:2110.08458, 2021.\n\nLinlu Qiu, Peter Shaw, Panupong Pasupat, Pawel Krzysztof Nowak, Tal Linzen, Fei Sha, and Kristina Toutanova. Improving compositional generalization with latent structure and data augmentation. In Marine Carpuat, Marie-Catherine de Marneffe, and Iv ́an Vladimir Meza Ru ́ız (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pp. 4341– 4362. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.naacl-main.323. URL https://doi.org/10.18653/v1/2022.naacl-main.323.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for\n\nmachine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n\nChandra Reddy and Prasad Tadepalli. Learning first-order acyclic horn programs from entailment. In\n\nInternational Conference on Inductive Logic Programming, pp. 23–37. Springer, 1998.\n\nAdam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022. URL https://arxiv.org/abs/2203.17189.\n\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of Plausible AlterIn AAAI Spring Symposium on natives: An Evaluation of Commonsense Causal Reasoning. Logical Formalizations of Commonsense Reasoning, Stanford University, March 2011. URL http://ict.usc.edu/pubs/Choice%20of%20Plausible%20Alternatives-% 20An%20Evaluation%20of%20Commonsense%20Causal%20Reasoning.pdf.\n\nLaura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden M. Lake. A benchmark for\n\nsystematic generalization in grounded language understanding, 2020.\n\nDaniel Lehmann Sarit Kraus and Menachem Magidor. Nonmonotonic reasoning, preferential models and cumulative logics. Journal of Artificial Intelligence, Vol. 44 Nos. 1-2:167–207, 1990. URL https://arxiv.org/abs/cs/0202021.\n\nSemantic Machines, Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth Short, Div Slomin,\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nBen Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson, Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander Zotov. Task-oriented dialogue as dataflow synthesis. Transactions of the Association for Computational Linguistics, 8:556–571, September 2020. URL https://doi.org/10.1162/tacl_a_00333.\n\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? arXiv preprint arXiv:2010.12725, 2020.\n\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. Clutrr: A diagnostic benchmark for inductive reasoning from text. Empirical Methods of Natural Language Processing (EMNLP), 2019.\n\nKoustuv Sinha, Shagun Sodhani, Joelle Pineau, and William L. Hamilton. Evaluating logical generalization\n\nin graph neural networks, 2020.\n\nMadhumita Sushil, Simon uster, and Walter Daelemans. Rule induction for global explanation of trained\n\nmodels, 2018.\n\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. Leap-of-thought: Teaching\n\npre-trained models to systematically reason over implicit knowledge, 2020.\n\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020a.\n\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM\n\nComputing Surveys (CSUR), 2020b.\n\nSebastian Thrun and Lorien Pratt. Learning to Learn, chapter Learning to Learn: Introduction and Overview, pp. 3–17. Springer, Boston, MA, 1998. ISBN 978-1-4613-7527-2. URL https://doi. org/10.1007/978-1-4615-5529-2_1.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. URL https://arxiv.org/ pdf/1706.03762.pdf.\n\nPat Verga, Haitian Sun, Livio Baldini Soares, and William W Cohen. Facts as experts: Adaptable and\n\ninterpretable neural memory over symbolic knowledge. arXiv preprint arXiv:2007.00849, 2020.\n\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear\n\ncomplexity. arXiv preprint arXiv:2006.04768, 2020.\n\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705, 2022.\n\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= gEZrGCozdqR.\n\nJason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merri ̈enboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks, 2015.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 2369–2380. Association for Computational Linguistics, 2018. doi: 10.18653/v1/ d18-1259. URL https://doi.org/10.18653/v1/d18-1259.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In EMNLP, 2018. URL https://arxiv.org/pdf/1809. 08887.pdf.\n\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020.\n\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. Pegasus: Pre-training with extracted\n\ngap-sentences for abstractive summarization, 2019.\n\nDenny Zhou, Nathanael Sch ̈arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA CLT DESIGN CHOICES\n\nHere we discuss the rationale behind the design choices made in our definition of a Conceptual Learning Task (CLT).\n\nSplitting the input into a set-like context and a request. The main goal of CLTs is to test whether a learner is capable of learning from explicitly provided knowledge consisting of rules and examples. Assuming a supervised learning setup as a basis, the applicable knowledge must somehow be provided in the input of each example. It is therefore quite natural to split the input into two parts: context and request. Since the examples and rules that form the background knowledge do not usually have a specific order (in the movie recommendation example for instance, it does not matter whether we are first told that Top Gun is rated PG-13 or that Jerry Maguire is rated R), representing the context as an unordered set is a natural choice.\n\nRepresenting rule assertions as examples. Another important property of CLTs is the ability to ask the learner explicitly whether a certain rule holds (in a given context). One straightforward way to achieve this is to include examples where the request asks for the validity of a certain rule and the output provides the corresponding truth value. These kind of examples also provide us with a natural way to assert (or refute) rules in the context, which allows us to represent the context simply as a set of examples (rather than a heterogeneous set containing both examples and a dedicated representation of rule assertions and refutations).\n\nDistinguishing monotonic and defeasible replies. Once we include the context as part of the input, we can distinguish between two different methods of how the learner may determine the reply for a given request: deduction and induction. For deduction, the learner infers the reply for a given request from the context using deductive reasoning alone. As an illustration, consider an example where we assert in the context that Top Gun is a PG-13 movie and then ask for the rating of Top Gun in the request (see Appendix D for a formalization of the logic axioms underlying deductive reasoning).\n\nFor induction, the information provided by the context is not sufficient to unambiguously determine the reply for a given request. As an illustration, suppose that we ask for the rating of Jerry Maguire in an example whose context asserts that both Mission Impossible 1 and Jerry Maguire are movies starring Tom Cruise and that Mission Impossible 1 is rated PG-13. This information is not sufficient for us to deduce the answer. Instead, the learner needs to rely on inductive bias to determine whether it should speculatively generalize the PG-13 rating from Mission Impossible 1 to Jerry Maguire or whether it should play it safe and say that it doesn’t know.\n\nDeductive reasoning is always monotonic w.r.t. the context (i.e., new knowledge cannot lead to a different reply), so we use the qualifier M to indicate deductive replies. Inductive reasoning is always defeasible the context (i.e., new knowledge may lead to a different reply), so we use the qualifier D to w.r.t. indicate inductive replies.\n\nB MOTIVATING EXAMPLE AS CLT\n\nHere we show what the motivating example from the introduction (Figure 1) could look like when formulated in the syntax of a CLT. Here we take Q to be the space of natural language statements and questions.\n\nContext containing assertions of relevant knowledge:\n\nC1={(cid:104)“R rated movies are not appropriate for kids who are less than 17 years old.”,1(cid:105) (cid:104)“The age of a person is the time that has passed since the person was born.”,1(cid:105) (cid:104)“The current date is June 3, 2021.”,1(cid:105) (cid:104)“Mission Impossible 1 – 6 are PG-13 rated action movies starring Tom Cruise.”,1(cid:105) (cid:104)“Jerry Maguire is an R rated comedy starring Tom Cruise.”,1(cid:105) (cid:104)“Anna was born in January of 2008 and Tom Cruise is her favorite actor.”,1(cid:105) (cid:104)“Anna saw Mission Impossible 1 – 5 and liked all of them.”,1(cid:105) ...\n\n}\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTop-level example set containing a mixture of non-rule examples (natural language QA for movie recommendations) and rule examples (which probe the model’s understanding of intermediate steps in the recommendation process):\n\nE ={(cid:104)C1,“What movie could Anna watch?”,“Mission Impossible 6”,D(cid:105)\n\n(cid:104)C1,“Anna is 14 years old.”,1,M(cid:105) (cid:104)C1,“Anna is at least 17 years old.”,0,M(cid:105) (cid:104)C1,“Jerry Maguire is appropriate for 14-year-olds.”,0,M(cid:105) (cid:104)C1,“Anna will like Mission Impossible 6.”,1,D(cid:105) (cid:104)C1,“Tom Cruise is Anna’s best friend.”,?,D(cid:105) ...\n\n}\n\nThe above is a relatively straightforward translation of the motivating example into CLT syntax, while using the format of rule examples for all of the context examples and for all of the intermediate questions. If we assume that the request space Q and reply space R include natural language questions and answers for querying about background knowledge, as well as for the end goal of providing movie recommendations, the above example could be alternatively expanded to represent some of the background knowledge in non-rule format (e.g., (cid:104)“What is the rating of Jerry Maguire?”,“R”(cid:105)) and/or to represent some of the intermediate questions as non-rule top-level examples (e.g., (cid:104)C1,“How old is Anna?”,“14 years”,M(cid:105)).\n\nC GENERALIZATION OF CLTS TO SUPPORT NESTED CONTEXTS\n\nAs discussed in Section 2, since the context of each top-level example in a CLT is itself represented as a set of “examples”, we have similar but slightly different notions of “example” at two different levels in a CLT:\n\n• Top-level examples, which we represent as quadruples of (cid:104)context,request,reply,qualifier(cid:105).\n\n• Context examples, which in Section 2 we require to be unconditional and monotonic and which\n\nwe thus represent as simply (cid:104)request,reply(cid:105) pairs.\n\nWhile for the simple cSCAN task, it was sufficient to provide only unconditional monotonic examples in the context, in the general case, one could imagine extensions of the notion of a CLT to allow inclusion of conditional and/or defeasible examples in the context as well. In this more general view of a CLT, we can drop the distinction between top-level examples and context examples, and instead adopt a recursive structure in which each example is a quadruple of (cid:104)context,request,reply,qualifier(cid:105), while each context is a set of examples.\n\nOne motivation for this more general view of CLTs is if we were to think of each top-level CLT example as representing one observation of the behavior of a personal assistant with very strong decision-making capabilities (which we might call the “teacher”), whose behavior at all times is conditioned on the knowledge available to it, and whose knowledge is stored in a large and growing set-like knowledge base. In this view, the context of the top-level example can be thought of as a snapshot of the relevant contents of the assistant’s knowledge base at the time that the assistant received the given request and output the given reply and qualifier.\n\nNow let us further suppose that we have another personal assistant (which we might call the “student”), which itself has some kind of growing set-like knowledge base, whose contents may or may not agree with the contents of the teacher’s knowledge base. One interesting question is how the student can go about selectively “learning” from the teacher, so as to emulate its decision-making capabilities, without necessarily accepting wholesale the full contents of its knowledge base, which may also include information that is transient, situation-specific, or debatable, and which may thus not be relevant or appropriate for the student to adopt. One natural approach could be to simply select the top-level examples of interest (i.e., to select the specific instances of the teacher’s behavior that the student wishes to emulate) and assert those examples in the student’s knowledge base. This would be the equivalent of storing the knowledge\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nthat “if such and such things (i.e., the contents of the example’s context) were true, then when faced with the given request, this would be the appropriate reply and the appropriate qualifier”.\n\nUnder this approach, the student’s set-like knowledge base would now come to contain full CLT examples (including context and qualifier). Continuing with the view of a “context” as being a snapshot of some or all of an assistant’s knowledge base, our contexts could now contain examples that themselves contain non-empty contexts. The maximum depth to which we allow such recursive structures to continue could be considered an arbitrary choice in the design of a conceptual learning system or task.\n\nC.1 GENERALIZED CLT\n\nBelow is a formalization of this more general definition of a CLT, along with some shorthand notations, by which the simpler CLT definition from Section 2 can be seen as just a special case of the general definition.\n\n• The task T = {e1,...,eN} is a finite set of examples ek ∈ E, where E = I ×O denotes the set of\n\npossible examples, with I being the set of possible inputs and O the possible outputs.\n\n• Each example ek =(cid:104)ik,ok(cid:105) is a pair consisting of an input ik ∈I and an output ok ∈O. • Each input ik = (cid:104)C,q(cid:105) is a pair consisting of a context C ∈ 2E and a request q ∈ Q. The set of possible requests Q can be partitioned into rule requests QR ⊆Q (i.e., requests that ask whether a certain rule holds) and non-rule requests QN ⊆Q.\n\n• Each output ok = (cid:104)r,u(cid:105) is a pair consisting of the reply r ∈ R and a qualifier u ∈ U. The set of possible replies R must contain dedicated elements representing true (1), false (0), and unknown (?), which are the only valid replies for rule requests q ∈ QR. The set of qualifiers U = {M,D} indicates whether the reply follows monotonically from the context (M) or whether it is defeasible (D).\n\nFor conciseness, we use the flat notation (cid:104)C,q,r,u(cid:105) to mean the nested pairs (cid:104)(cid:104)C,q(cid:105),(cid:104)r,u(cid:105)(cid:105), we use (cid:104)C,q,r(cid:105) as a shorthand for the monotonic example (cid:104)C,q,r,M(cid:105), and we use (cid:104)q,r(cid:105) as a shorthand for the unconditional monotonic example (cid:104)∅,q,r,M(cid:105). Hence, the example (cid:104)C,q,r(cid:105) means that given the context C or any superset of C, the request q is translated to the reply r, while (cid:104)q,r(cid:105) means that under all circumstances, the request q is translated to the reply r.\n\nNote that even though the context is strictly a set of examples, it may still contain any rule q∈QR by means of including the example (cid:104)∅,q,1,M(cid:105), which asserts the rule q to be true unconditionally. Similarly, we can express that the rule q does not hold in a context by including the example (cid:104)∅,q,0,M(cid:105).\n\nWe refer to an example as unconditional if it has an empty context.\n\nNote also that since examples within a context are of the same form as top-level examples, they may in principle themselves contain contexts up to arbitrary levels of nesting. We can, however, for any given CLT, choose a maximum level to which we allow such nesting to occur. In cSCAN, for example, contexts contain only unconditional examples, so that there is no nesting of contexts.\n\nC.2 ADDITIONAL SHORTHAND FOR RULE ASSERTIONS IN A CONTEXT\n\nTo express the assertion of a rule q∈QR in contexts more concisely, we use q to mean its unconditional monotonic assertion (cid:104)∅,q,1,M(cid:105). For instance, (cid:104){q},q(cid:48),r(cid:105) stands for (cid:104){(cid:104)∅,q,1,M(cid:105)},q(cid:48),r(cid:105).\n\nIn the example from Figure 1, if we take Q to be the space of natural language statements and questions, then with the above shorthand the following would be equivalent:\n\n• (cid:104){(cid:104)∅,“Anna was born in January of 2008.”,1,M(cid:105)},“Who is Anna’s best friend?”,?,D(cid:105) • (cid:104){“Anna was born in January of 2008.”},“Who is Anna’s best friend?”,?,D(cid:105)\n\nD FORMALIZATION OF CONSISTENCY REQUIREMENTS AND METRICS\n\nIn this appendix, we formalize the consistency requirements and consistency metrics of CLTs by mapping example sets into classical propositional logic. We start by summarizing the definitions and axioms of propositional logic, and we specify functions to capture the meaning of rules and the inductive bias. This\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nallows us to map example sets to logical formulas, which in turn allows us to formalize the consistency requirements as well as the metrics for measuring consistency of a learner.\n\nFor the purposes of this formalization, we assume the more general form of a CLT described in Appendix C, for which the simpler CLT form used for cSCAN follows as a special case.\n\nNote that while we provide this formalization for reference purposes and to facilitate future investigation into the formal properties of CLTs, it is possible in practice to use CLTs and the related consistency metric without detailed consideration of this formalization.\n\nD.1 TERMINOLOGY\n\nIf not otherwise indicated, we assume that C,C(cid:48) ∈2E are arbitrary contexts, q,q(cid:48) ∈Q are arbitrary requests, r,r(cid:48) ∈ R are arbitrary replies, u,u(cid:48) ∈ U are arbitrary qualifiers, E,E(cid:48),F ∈ E are arbitrary example sets, and e, e(cid:48), f are arbitrary examples.\n\nFurthermore, we assume that M(e) and D(e) denote the monotonic and defeasible variants of the example e, respectively. This means that for any u∈U, M((cid:104)C,q,r,u(cid:105))=(cid:104)C,q,r,M(cid:105) and D((cid:104)C,q,r,u(cid:105))=(cid:104)C,q,r,D(cid:105).\n\nD.2 CLASSICAL PROPOSITIONAL LOGIC\n\nFor reference in later proofs, we summarize the basic definitions and axioms of Lukasiewicz classical propositional logic (Klement, 2004), which we adopt as-is, with only a change of symbols for the logical connectives to avoid ambiguity with the ∧, ∨, and ¬ symbols that we use elsewhere in this document in first order logic statements.\n\nWe assume a set of propositional variables V which represent atomic formulas. General logical formulas L are recursively constructed from these atomic formulas using the logical connectives → (implication), ¬ (negation), ∧ (and), ∨ (or), and ≡ (equivalence). In our formalization, we assume the primitive connectives → and the constant ⊥ (falsum), and we define the other connectives as follows (assuming that x,y,z∈L).\n\n¬x:=x→⊥ x ∨ y:=¬x→y x ∧ y:=¬(x→¬y) x≡y:=(x→y) ∧ (y→x)\n\n(cid:62):=¬⊥\n\n(1)\n\n(2)\n\n(3)\n\n(4)\n\n(5)\n\nTo formulate the propositional logic axioms and inference rules, we use the notation x(cid:96)y to express that we can infer y from x and we use (cid:96) x to express that x is a tautology.\n\n(cid:96)x→(y→x) (cid:96)(x→(y→z))→((x→y)→(x→z)) (cid:96)(¬x→¬y)→(y→x)\n\nx,(x→y)(cid:96)y\n\n(6) (7)\n\n(8)\n\n(9)\n\nThe axioms (6), (7), (8) form the Lukasiewicz system, while the inference rule (9) is modus ponens.\n\nD.3 SEMANTICS OF RULES AND INDUCTIVE BIAS\n\nTo map example sets to logical formulas, we need to be able to refer to the semantics of rules and the inductive bias.\n\nFor the semantics of rules, we assume a grounding function G : QR → 2E, which maps each rule to an equivalent set of examples. This means that in a consistent task, a rule q∈QR holds (i.e., (cid:104)∅,q,1(cid:105) is a valid example) if and only if each example in G(q) is valid. It also means that if the context of an example e∈E\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\ncontains the assertion (cid:104)∅,q,1(cid:105) of a rule q∈QR, we can obtain an equivalent example e(cid:48) by replacing the assertion of q with the examples G(q). We provide the grounding function of cSCAN in Appendix E.3.\n\nSince a CLT requires a learner to induce as well as deduce rules, every CLT inherently assumes some form of inductive bias, which determines the criteria based upon which a learner is expected to induce a rule to be “true” vs. considering it to be “unknown”. While the inductive bias of the task is in principle arbitrary (being dependant on the choice of the task designer or the needs of the real-world use case), the learner will need to be able to emulate this bias in order to perform well on the task. We formalize this domain-specific inductive bias through a bias function B : 2E → 2ED, which maps each set of examples to the set of examples that are expected to be induced. We use EM,ED ⊆ E to denote the subsets of all monotonic and defeasible examples, respectively. We provide the bias function of cSCAN in Appendix E.4.\n\nWhile the grounding and bias functions are needed to precisely formalize the consistency requirements and metrics, they are not a requirement for a conceptual learning task. This is important because providing complete grounding and bias functions may not be feasible or practical for more realistic CLTs. Instead, we may only provide partial functions, which means that the formalization in this section will only approximate the true consistency requirements and consistency metrics.\n\nD.4 MAPPING EXAMPLE SETS TO LOGICAL FORMULAS\n\nWe treat each example as a propositional variable (i.e, (cid:104)C,q,r,u(cid:105) ∈ V). This allows us to define the embedding function M : 2E → L as follows.\n\nM(∅):=(cid:62) M({(cid:104)C,q,r,u(cid:105)}):=(cid:104)C,q,r,u(cid:105)\n\nM(E∪E(cid:48)):=M(E) ∧ M(E(cid:48))\n\n(10)\n\n(11)\n\n(12)\n\nThe empty set maps to true (10), a set of size one maps to its only element (11), and union maps to logical conjunction (12). In addition, our embedding adheres to the following axioms.\n\n(cid:96)(cid:104)C,q,r,M(cid:105)≡(M(C)→(cid:104)∅,q,r,M(cid:105)) (cid:96)(cid:104)C,q,r,M(cid:105)→(cid:104)C,q,r,D(cid:105) M(C)≡M(C(cid:48)): (cid:96)(cid:104)C,q,r,D(cid:105)≡(cid:104)C(cid:48),q,r,D(cid:105)\n\nr (cid:54)=r(cid:48) : (cid:96)(cid:104)C,q,r,D(cid:105) ∧ (cid:104)C,q,r(cid:48),D(cid:105)→(C →⊥)\n\n(cid:96)(cid:104)∅,q,1,M(cid:105)≡M(G(q)) (cid:96)M(E)→M(B(E))\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\n(17) (18)\n\nAxiom (13) says that the context of a monotonic example becomes the antecedent of a logical implication, and axiom (14) specifies that a monotonic examples implies the corresponding defeasible example. Axiom (15) says that defeasible examples that differ only in equivalent contexts are equivalent. Axiom (16) specifies that defeasible examples are functional or the context must be contradictory, which means that each request with a non-contradictory context must have a unique reply. Finally, axiom (17) specifies that the assertion of a rule is equivalent to its grounding, and axiom (18) says that each set of examples implies the set of examples that can be induced using the inductive bias.\n\nTogether with the axioms of propositional logic, we obtain the following theorems.\n\n(cid:96)(cid:104)C,q,r,M(cid:105)→(cid:104)C∪C(cid:48),q,r,M(cid:105) (cid:96)M({(cid:104)C,q,1,M(cid:105)})≡M({(cid:104)C∪C(cid:48),q(cid:48),r(cid:48),u(cid:48)(cid:105): (cid:104)C(cid:48),q(cid:48),r(cid:48),u(cid:48)(cid:105)∈G(q)})\n\nr (cid:54)=r(cid:48) : (cid:96)(cid:104)C,q,r,u(cid:105) ∧ (cid:104)C,q,r(cid:48),u(cid:105)→(C →⊥)\n\n(19)\n\n(20)\n\n(21)\n\nTheorem (19) is obtained from axioms (6), (13) and (12), and it says that monotonic examples behave monotonically w.r.t. their context. This allows us to rewrite the grounding axiom (17) to obtain theorem (20). The last theorem (21) says that functionality applies independently of the qualifier.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nD.5 CONSISTENCY METRIC\n\nAs discussed in Section 4, the consistency metric C(E) differs from the standard accuracy metric in that it does not judge the correctness of a learner’s predictions on individual examples, but rather measures the degree to which a complete set of examples E is consistent w.r.t. to the axioms of classical propositional logic.\n\nIn essence, we define C(E) to be the percentage of subsets of E that contain a logical implication, in comparison to the number of subsets of E that contain an implication or a contradiction.\n\nNote that the definition of the consistency metric provided in equation (26) could essentially be applied as-is to any arbitrary CLT, provided there is some way to identify the “implications” (impl(E)) and “contradictions” (cont(E)) among the learner’s predictions. In practice, task designers may be free to apply any reasonable heuristic to identify such implications and contradictions. The formal definition of the consistency metric below can be considered an ideal, which we seek to emulate closely in the consistency metric implementation provided for cSCAN, as described in Appendix G.\n\nE (cid:44)→f :⇔ (cid:48)¬M(E)∧ (cid:96)M(E)→M({f })∧((cid:64)E(cid:48) (cid:40)E : (cid:96)M(E(cid:48))→M({f }))∧\n\n¬(f ∈ED ∧ (cid:96)M(E)→M({M(f )}))\n\nE (cid:54)(cid:44)→f :⇔((cid:48)¬M(E)∧ (cid:96)M(E)→¬M({f })∧((cid:64)E(cid:48) (cid:40)E : M(E(cid:48))→¬M({f })))∨\n\n(f ∈ED ∧E (cid:44)→M(f )) impl(E):= {F ⊆E : (∃f ∈F : F\\{f }(cid:44)→{f })} cont(E):= {F ⊆E : (∃f ∈F : F\\{f }(cid:54)(cid:44)→{f })}\n\nC(E):=\n\n(cid:40)\n\n100·\n\n|impl(E)|\n\n|impl(E)|+|cont(E)|,\n\n|impl(E)|+|cont(E)|>0\n\nNaN,\n\notherwise\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\n(26)\n\nMinimal implication E (cid:44)→ f means that the non-contradictory example set E implies the example f and that there is no strict subset of E that has the same property. Furthermore, we require matching qualifiers, which means that E must not imply the monotonic variant of f if the latter is marked as defeasible (22).\n\nThe minimal contradiction E (cid:54)(cid:44)→f can be met by either of the following two conditions (23). First, E (cid:54)(cid:44)→f holds if the non-contradictory example set E implies the negation of f and there is no strict subset of E that has the same property. Secondly, E (cid:54)(cid:44)→ f holds if there is a qualifier mismatch, i.e., if f is marked defeasible but E minimally implies the monotonic variant of f . Note that we do not consider it to be a qualifier mismatch if f is marked as monotonic and E implies only the defeasible variant because there may be other evidence outside of E that may justify the monotonic qualifier.\n\nThis allows us to define impl(E) to be the set of subsets F ⊆E that contain a minimal implication (24) and cont(E) to be the set of subsets F ⊆E that contain a minimal contradiction (25). Finally, the consistency metric C(E) is the percentage of subsets of E that contain implications, in comparison with the number of subsets of E that contain implications or contradictions (26). If the set E does not contain any implications or contradictions, then the consistency metric is not defined.\n\nIllustration. As an illustration, consider the following example set E, which consists of 4 rule assertions and 8 examples. (We assume the syntax and semantics of cSCAN, which is formally specified in Appendix E.)\n\n=\n\n(cid:75)(cid:74)\n\nu (cid:74) =\n\n,1(cid:105), u\n(cid:75) (cid:75)(cid:74) ,1(cid:105), x\ny (cid:74) (cid:75) =WALK,1(cid:105), =JUMP,1(cid:105),\n\nE :={(cid:104)C1, u twice (cid:74) (cid:75) (cid:104)C1, x after y (cid:74) (cid:75) (cid:104)C1, walk (cid:75) (cid:74) (cid:104)C1, jump (cid:75) (cid:74) (cid:104)C1,walk,WALK(cid:105), (cid:104)C1,eat twice,EAT EAT(cid:105), (cid:104)C1,walk twice,WALK WALK WALK WALK(cid:105), (cid:104)C1,jump twice,JUMP JUMP,D(cid:105), (cid:104)C1,walk after walk,WALK WALK(cid:105),\n\n21\n\n(a)\n\n(b)\n\n(c) (d)\n\n(e)\n\n(f) (g)\n\n(h)\n\n(i)\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:104)C1,jump after walk,WALK JUMP,D(cid:105), (cid:104)C1,walk after jump,WALK JUMP(cid:105), (cid:104)C1,walk twice after jump,JUMP WALK WALK(cid:105)}\n\n(j)\n\n(k)\n\n(l)\n\nIn this example set there are a total of 5 minimal implication sets (with the implied example indicated in green above):\n\n• {c,e}\n\n• {b,c,i}\n\n• {b,e,i}\n\n• {a,b,c,d,l}\n\n• {a,b,e,d,l}\n\nAnd there are a total of 8 minimal contradiction sets (with the contradictory example indicated in red above), of which the following are because of an inconsistent reply:\n\n• {a,c,g}\n\n• {a,e,g}\n\n• {b,c,d,k}\n\n• {b,e,d,k}\n\n• {b,d,g,l}\n\nwhile the following are because of an inconsistent qualifier:\n\n• {a,d,h}\n\n• {b,c,d,j}\n\n• {b,e,d,j}\n\nBased on this, the consistency of example set E would be C(E) = 100· 5/13 ≈ 38.5.\n\nNote that example d is both implied and contradicted. Also, since examples c and e are semantically equivalent according to cSCAN’s grounding function (Appendix E.3), each minimal implication or contradiction set that involves example c can be written alternatively using example e, causing such sets to appear in pairs in the lists above.\n\nD.6 CLT: DEFINITION AND CONSISTENCY REQUIREMENTS\n\nTo obtain a precise measure for the consistency of the predictions produced by a given learner, it is important that the task T itself be consistent. Specifically, this means that each example e ∈ T must be consistent on its own, and the example set T as a whole must be consistent. For the purposes of this paper, we further require that contexts are non-contradictory.\n\nWe capture these requirements using a predicate consistentCLT(T ), which is recursively defined as follows.\n\nconsistentCLT({(cid:104)C,q,r,u(cid:105)}):⇔consistentCLT(C) ∧ (cid:48)¬(cid:104)C,q,r,u(cid:105) ∧\n\n(u=M⇔ (cid:96)M({(cid:104)C,q,r,u(cid:105)}))\n\nconsistentCLT(E):⇔(∀e∈E : consistentCLT({e})) ∧ (C(E)=100 ∨ C(E)=NaN)\n\n(27) (28)\n\n(29)\n\nWe first define the consistency of an individual example (cid:104)C,q,r,u(cid:105) ∈ E, which requires that its context C is consistent (and therefore non-contradictory), that the example itself is not a contradiction, and that it is qualified as monotonic if and only if it maps to a tautology (28). Then, we define that an example set E ⊆ E is consistent if and only if each individual example e ∈ E is consistent and the example set is consistent as a whole (29) (i.e., C(E) must be 100 or undefined).\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\n=WALK\n\n=LTURN\n\nwalk (cid:74) (cid:75) =RUN run (cid:74) (cid:75) =JUMP jump (cid:74) (cid:75) =LOOK look (cid:74) (cid:75) turn left (cid:75) (cid:74) =RTURN turn right (cid:74) (cid:75) x1 x1 left =LTURN (cid:74) (cid:74) (cid:75) (cid:75) x1 x1 right =RTURN (cid:74) (cid:74) (cid:75) x1 x1 x1 twice =\n(cid:75) (cid:74) (cid:75) (cid:74) x1 x1 x1 thrice =\n(cid:75)(cid:74) (cid:74) (cid:75) (cid:74)\n\n(cid:75)(cid:74) (cid:75)(cid:74)\n\n(cid:75) x1\n\n(cid:75)\n\n=RTURN RTURN\n\nx2 (cid:75) x1 (cid:75) =LTURN LTURN\n\nx1 x1 andx2 =\n(cid:74) (cid:75)(cid:74) (cid:74) (cid:75) x2 x1 afterx2 =\n(cid:74) (cid:75)(cid:74) (cid:74) (cid:75) turn opposite left (cid:75) (cid:74) turn opposite right (cid:75) (cid:74) =LTURN LTURN LTURN LTURN turn around left (cid:75) (cid:74) =RTURN RTURN RTURN RTURN turn around right (cid:74) (cid:75) x1 opposite left =\nturn opposite left (cid:75) (cid:75)(cid:74) (cid:75) (cid:74) (cid:74) x1 opposite right x1 =\nturn opposite right (cid:75) (cid:74) (cid:75)(cid:74) (cid:74) x1 x1 x1 around left =LTURN LTURN (cid:75) (cid:75) (cid:74) (cid:74) (cid:74) (cid:75) x1 x1 x1 around right =RTURN RTURN (cid:74) (cid:74) (cid:75) (cid:74)\n\n(cid:75) x1 LTURN (cid:75) (cid:74) x1 RTURN (cid:74)\n\nx1\n\n(cid:75)\n\n(cid:75)\n\nx1 LTURN (cid:75) (cid:74) x1 RTURN (cid:74)\n\n(cid:75)\n\n(cid:75)\n\nFigure 4: SCAN interpretation rules as provided by Lake & Baroni (2017). Double brackets denote the interpretation function translating SCAN’s linguistic commands into sequences of actions. Symbols x1 and x2 denote variables.\n\nNote that while the above formulation of CLT consistency requirements is sufficient for cSCAN, it could be desirable to adjust this for more complex tasks, which we leave for future work. In particular, while this definition of consistency provides only basic control over the behavior of defeasible examples, we could imagine defining stricter consistency requirements for defeasible examples, e.g., by requiring them to adhere to the KLM properties (Sarit Kraus & Magidor, 1990; Casini et al., 2021). Also, while in this paper, for simplicity, we require contexts to be non-contradictory, this requirement is not strictly necessary for the task to be consistent or for the consistency metrics to be meaningful, as long as the top-level examples in the dataset do not contain a contradiction or contradict each other. Taken a step farther, for dealing with real-world datasets which may contain noise, it may be desirable to relax the requirement that the task be strictly consistent. In such an approach, the “requirements” of a CLT can be considered more as an aspiration rather than as strict requirements. The consistency metric could in that case still be calculated, but would be less reliable an indicator of learner consistency compared to the case where the task is consistent.\n\nE SPECIFICATION OF CSCAN\n\nThis appendix contains a specification of the cSCAN task. It consists of the phrase-structure grammars to generate valid requests and replies, the compositional grounding function (which defines the meaning of the explicit rules), and the bias function (which defines the inductive bias). Together with the formalization of simple CLTs provided in Appendix D, this specifies the complete behavior of cSCAN.\n\nNote that in this section, as in Appendix D, we assume the more general CLT formalism described in Appendix C, in which contexts can contain examples of the same form as the top-level examples. While the cSCAN specification could be expressed equivalently in terms of the simplified CLT formalism of Section 2, the more general formalism allows us to express some aspects of the specification more concisely, as we can thus describe the semantics of both top-level examples and context examples using the same grounding function.\n\nFor readability, we make use of the shorthand notation described in Section C.2 to allow expressing some of the more verbose examples more concisely.\n\nNote also that while we provide the complete formal specification of cSCAN here as a reference, it is not necessary in general to provide a specification at this level of detail when defining future CLTs.\n\nE.1 RULE SPACES\n\nMost of the specification of cSCAN is identical between cSCAN-B and cSCAN-X. In this section, we summarize the points that differ between the two, together with some notes on the original SCAN task for comparison.\n\nOriginal SCAN\n\nIn the original SCAN task, natural language commands are generated by a fixed phrase-structure grammar as described in Lake & Baroni (2017), which is equivalent to the phrase-structure grammar shown for cSCAN-B in the top left in Figure 5. (Note that for readability, we renamed the non-terminals in this\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: The green area on the top left is the phrase-structure grammar to generate the cSCAN-B commands. The red area on the lower right is the extension to generate the left-hand side of cSCAN-B rules.\n\npresentation to proceed alphabetically, beginning with “S” as the traditional start symbol.) The mapping from command to action sequence follows the fixed set of interpretation rules shown in Figure 4.\n\nAs can be seen in the figure, action sequences in the original SCAN task are constructed from a set of 6 possible actions.\n\nA:={WALK,RUN,JUMP,LOOK,LTURN,RTURN}\n\ncSCAN-B\n\nIn cSCAN-B, the set of non-rule requests QN consists of all natural language commands that are generated by the phrase-structure grammar shown on the top left in Figure 5, which is equivalent to the original SCAN phrase structure grammar, as reformulated in Nye et al. (2020). The mapping from command to action sequence varies from context to context.\n\ncSCAN-B constructs action sequences from the same 6 actions used in the original SCAN task, plus several additional ones provided for diversity. (In a nod to earlier research on the SCAN task, we follow the lead of Nye et al. (2020) in using as these additional actions the ones that appear in the “MiniSCAN” task of Lake et al. (2019).)\n\nA:={WALK,RUN,JUMP,LOOK,LTURN,RTURN,\n\nRED,YELLOW,GREEN,BLUE,PURPLE,PINK,BLACK,WHITE}\n\ncSCAN-X\n\nIn cSCAN-X, the set of non-rule requests QN consists of all natural language commands that are generated by the phrase-structure grammar shown on the top left in Figure 6. The mapping from command to action sequence varies from context to context, similarly to cSCAN-B.\n\ncSCAN-X constructs action sequences from a set of 13 possible actions.\n\nA:={WALK,RUN,JUMP,LOOK,LTURN,RTURN,\n\nDRIVE,RIDE,FLY,LEAP,PEEK,UTURN,DTURN}\n\nE.2 REQUESTS AND REPLIES\n\nFor simplicity, this section focuses on a formal description of cSCAN-B. The specification of cSCAN-X would follow the same form, with the exception of the differences described in Appendix E.1 above.\n\nNon-rule requests. In cSCAN, the set of non-rule requests QN consists of all natural language commands that are generated by the phrase-structure grammar shown on the top left in Figure 5. Note that for convenience of generation, the grammar adopted here is based on the alternative formulation of the SCAN grammar from Nye et al. (2020). This generates a slightly larger set of commands than the original SCAN grammar from Lake & Baroni (2017), as it includes commands such as “turn” and “turn and turn”.\n\nReplies. Since cSCAN is a CLT, the set of replies R includes the dedicated replies 1 (true), 0 (false), and ? (unknown). In addition, R contains all sequences consisting of actions A (described in Appendix E.2 above) separated by space with a maximum sequence length N:\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: The green area on the top left is the phrase-structure grammar to generate the cSCAN-X commands. The red area on the lower right is the extension to generate the left-hand side of cSCAN-X rules. The items in blue boldface are those which do not appear in cSCAN-B.\n\nA∗ :={a1 a2 ... ak : 1≤k <N,ai ∈A} R:={1,0,?}∪A∗\n\n(30)\n\n(31)\n\nRule requests. All cSCAN rule requests are of the form q=r, where q is an element of the left-hand side (LHS) expressions Q∗\n\nN and r is an element of the right-hand side (RHS) expressions R∗.\n\nThe set of LHS expressions Q∗ N is generated by the extended phrase-structure grammar shown in Figure 5, with the resulting phrases wrapped in brackets . Note that this is almost the same grammar that is used to generate the cSCAN commands QN with the only difference being that it allows generating expressions with variables from the set X := {x1,x2,x3,x4}. Examples of LHS expressions are x1 twice (cid:74) (cid:75) The set of RHS expressions R∗ consists of sequences of maximum length N whose elements are separated by space and consist of actions A and LHS expressions Q∗ N. Examples of RHS expressions are “WALK x twice ”. Note that RHS expressions are a superset look thrice (cid:74) (cid:75) (cid:75) of LHS expressions, i.e., Q∗\n\nLTURN” and “WALK JUMP N ⊂ R∗.\n\n. walk opposite left (cid:75) (cid:74)\n\nrun left (cid:74)\n\nand\n\n(cid:75) (cid:74)\n\n(cid:74)(cid:75)\n\nR∗ :={a1 a2 ... aK : 1≤K <N,ai ∈A∪Q∗\n\nN}\n\n(32)\n\nWe assume a function var : R∗ → 2X , which returns the set of variables used by a given RHS expression. This allows us to define the set of rule requests QR as an LHS and an RHS expression with matching variables:\n\nQR :={q=r : q∈Q∗\n\nN,r ∈R∗,var(q)=var(r)}\n\n(33)\n\nE.3 GROUNDING OF RULES\n\nAs discussed in Appendix E.3, we specify the semantics of cSCAN rules via a grounding function G :QR →2E, which maps each rule to an equivalent set of examples. For rules where the LHS consists of a command without any variables, we can define the grounding as an example that provides the interpretation of this command. For example, the rule\n\n= WALK is grounded as follows:\n\nwalk (cid:75) (cid:74)\n\nG( walk (cid:75) (cid:74)\n\n=WALK):={(cid:104)∅,walk,WALK(cid:105)}\n\nNote that because we determine consistency based on propositional logic equivalence (see Appendix D), we could specify equivalent groundings that include additional examples that are logically implied by {(cid:104)∅, walk, WALK(cid:105)}. For instance, we could add variants with non-empty contexts (e.g., (cid:104){ =\nrun (cid:75) (cid:74) RUN},walk,WALK(cid:105)), which follow from monotonicity.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nFor rules containing variables, the grounding may consist of hundreds or thousands of examples, even if we do not include any examples that are logically implied. This is because the variable can be replaced with any can be grounded as follows: command that leads to a valid LHS. For example, the rule\n\n=\n\nx1 twice (cid:75) (cid:74)\n\nx1\n\nx1 (cid:74)\n\n(cid:75)(cid:74)\n\n(cid:75)\n\nx1 twice G( (cid:75) (cid:74)\n\n=\n\nx1 (cid:74)\n\nx1\n\n(cid:75)(cid:74)\n\n=RUN},run twice,RUN RUN(cid:105), =JUMP},run twice,JUMP JUMP(cid:105),\n\n):={(cid:104){ run (cid:74) (cid:75) (cid:75) (cid:104){ run (cid:74) (cid:75) (cid:104){ run left (cid:75) (cid:74) run left twice,LTURN RUN LTURN RUN(cid:105), ...}\n\n=LTURN RUN},\n\nThe first example in this grounding can be read as: if is translated to “RUN RUN”.\n\nrun (cid:75) (cid:74)\n\nis translated to “RUN” then “run twice”\n\nCompositional grounding function of cSCAN. Since the semantics of cSCAN rules is compositional (which is a requirement for all CLTs), we are able to specify the grounding function in a complete yet concise fashion using the following helper constructs. For simplicity, we focus here on describing the compositional grounding function for cSCAN-B. The grounding function for cSCAN-X would follow the same general form.\n\nsions Q∗\n\nN. For example, lhs(WALK JUMP\n\n• For any RHS sequence r ∈R∗, we use lhs(r) to denote the set of elements of r that are LHS expres- }. ,\n)={ look thrice run left look thrice run left (cid:74) (cid:75) (cid:74) (cid:75) (cid:74) (cid:75) • For any subset of LHS expressions Q ⊆ Q∗ N, we use c2a(Q) to denote the set of all possible functions f : Q→A∗ that map each expression in Q to an action sequence in A∗. (Note that this also applies to the empty set, i.e., there is exactly one function in c2a(∅).)\n\n(cid:75)(cid:74)\n\n• We define subsets of the commands QN generated by the phrase-structure grammar in Figure 5. N ⊂ QN denotes the commands that are generated when\n\nFor each z ∈ {T,U,V,W,X} the set Qz starting from the symbol z (rather than S).\n\n• For each z ∈ {T,U,V,W,X}, we assume a function varz : Q∗\n\nN to a subset of its variables var(q). For any expression q∈Q∗\n\nN → 2X , which map each LHS N, the following\n\nexpression q∈Q∗ holds:\n\n– varT(q) consists of the variables that are generated using the rule path T → U, U → V, V\n\n→ W, W → Y.\n\n– varU(q) consists of the variables that are generated using the rule path U → V, V → W, W\n\n→ Y.\n\n– varV(q) consists of the variables that are generated using the rule path V → W, W → Y. – varW(q) consists of the variables that are generated using the rule W → Y. – varX(q) consists of the variables that are generated using the rule X → Y.\n\n• For all LHS expressions q ∈ Q∗\n\nN we use v2c(q) to denote the set of all possible functions f : var(q) → QN that map each variable in var(q) to a command in QN such that ∀z ∈ {T,U,V,W,X} : f (varz(q)) ∈ Qz N. Note that this restriction of the mapping ensures that variable substitution does not break the implicit precedence rules that apply to the interpretation of commands (see the discussion below for more detail).\n\n• For any RHS expression r ∈ R∗ and any partial function f : R∗ ∪X (cid:55)→ R∗, we use subst(r,f ) to denote the RHS expression that we obtain by lexically substituting in r all occurrences of r(cid:48) ∈dom(f ) with f (r(cid:48)).\n\nThese constructs allow us to define the grounding of all rules (q = r) ∈ QR as follows:\n\nG(q=r):=\n\n(cid:40)\n\n∪f ∈c2a(lhs(r)){(cid:104)∪q(cid:48)∈dom(f ){(cid:104)∅,q(cid:48) =f (q(cid:48)),1(cid:105)},q=subst(r,f ),1(cid:105)}, ∪f ∈v2c(q){(cid:104)∅,subst(q,f ),subst(r,f )(cid:105)},\n\nvar(q)=∅ otherwise\n\n(34)\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nDiscussion and illustration. The first case of definition 34 applies when the rule q=r does not contain any variables. It says that whenever a LHS expression q(cid:48) occurs in the RHS expression r, we can substitute q(cid:48) with an arbitrary action sequence f (q(cid:48)) as long as we make sure that the context contains a rule asserting that q(cid:48) is interpreted as f (q(cid:48)).\n\nAs an illustration, consider the grounding of the rule LTURN (cid:75)(cid:74) that is shown below. The set of LHS expressions on the right-hand side of this rule is{run, jump}. The first example of the grounding can be read as: if maps to “JUMP” then the RHS of the rule becomes “RUN RUN JUMP LTURN”.\n\nrun twice and jump (cid:74) (cid:75) maps to “RUN” and\n\nrun (cid:74) jump (cid:75) (cid:74)\n\njump (cid:75)\n\nrun (cid:75) (cid:74)\n\nrun\n\n(cid:75)(cid:74)\n\n=\n\nG( run twice and jump (cid:74) (cid:75) {(cid:104){(cid:104)∅, run (cid:75) (cid:74) run twice and jump (cid:74) (cid:75) (cid:104){(cid:104)∅, run (cid:75) (cid:74) run twice and jump (cid:74) (cid:75) (cid:104){(cid:104)∅, run (cid:75) (cid:74) run twice and jump (cid:74) (cid:75) ...}\n\n=\n\nrun (cid:75)(cid:74) (cid:74) =RUN,1(cid:105),(cid:104)∅, jump (cid:75) (cid:74)\n\nrun\n\nLTURN)=\n\njump (cid:75)(cid:74) (cid:75) =JUMP,1(cid:105)},\n\n=RUN RUN JUMP LTURN,1(cid:105),\n\n=LOOK,1(cid:105),(cid:104)∅, jump (cid:75) (cid:74)\n\n=WALK,1(cid:105)},\n\n=LOOK LOOK WALK LTURN,1(cid:105),\n\n=RUN,1(cid:105),(cid:104)∅, jump (cid:75) (cid:74)\n\n=RUN RUN,1(cid:105)},\n\n=RUN RUN RUN RUN LTURN,1(cid:105),\n\nThe second case of definition 34 applies when the rule q=r contains a set of variables. It says that we can substitute any variable x with a command f (x) that corresponds to the same non-terminal in the parse tree.\n\nx twice As an illustration, consider the grounding of the rule LTURN shown below. In the first (cid:75) (cid:74) example of the grounding, we replace the variable x with the command “run”, in the second example we replace it with “jump around left”, and in the last example we replace it with “look thrice”.\n\nx (cid:75)\n\nx (cid:74)\n\n(cid:75)(cid:74)\n\n=\n\nLTURN)=\n\n=\n\n(cid:75)(cid:74) =\n\nx1 (cid:75) run (cid:74)\n\nx1 x1 twice G( (cid:75) (cid:74) (cid:74) {(cid:104)∅, run run twice (cid:75) (cid:75) (cid:74) (cid:104)∅, jump around left twice (cid:74) (cid:75) (cid:104)∅, look thrice twice (cid:75) (cid:74) ...}\n\n(cid:75)(cid:74)\n\n=\n\nLTURN,1(cid:105), =\n\njump around left (cid:74) look thrice (cid:74)\n\nlook thrice (cid:75)\n\n(cid:75)(cid:74)\n\n(cid:75)(cid:74)\n\njump around left (cid:75) LTURN,1(cid:105),\n\nLTURN,1(cid:105),\n\nNote that because x1 ∈varU( ) and x1 (cid:54)∈varT( ), the variable x1 cannot be substituted x1 twice x1 twice (cid:75) (cid:74) (cid:75) (cid:74) with commands such as “walk and jump” that are in QT N but not in QU N. This is important because the grounding would otherwise contain examples that violate the higher precedence of “twice” when compared to “and”, which is not what we intended. I.e., our formalization makes sure that:\n\n(cid:104)∅, walk and jump twice (cid:75) (cid:74)\n\n=\n\nE.4\n\nINDUCTIVE BIAS\n\nwalk and jump (cid:74)\n\nwalk and jump (cid:75) x1 =\n\nLTURN,1(cid:105)(cid:54)∈ LTURN)\n\nx1 (cid:74)\n\n(cid:75)(cid:74)\n\n(cid:75)\n\n(cid:75)(cid:74) x1 twice G( (cid:75) (cid:74)\n\nAs discussed in Appendix E.4, the inductive bias of a CLT is the criteria based upon which a learner is expected to induce a rule to be “true” as opposed to considering it to be “unknown”. For cSCAN, we adopt a simple set of criteria, based loosely on our own intuition, in which we consider there to be sufficient evidence to support induction of a rule if there are examples in the context that could be explained by the given rule (in combination with other rules that are explicitly or implicitly provided in the context) and which in total illustrate at least 4 different substitutions of each of the rule’s variables. The number 4 is arbitrary, but based on our intuition that we would be comfortable in generalizing rules from a relatively small number of examples, but that 2 examples is not quite enough to justify inducing a general pattern. We chose the threshold of 4 rather than 3 because we use the inductive bias criteria internally for determining the minimum number of illustrating examples to include in the context for each rule that we intend to be induced to be “true”, and we wanted to avoid penalizing a learner that is slightly cautious in its inductions.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nAt the same time, we took care to avoid illustrating any rules’ variable substitutions exactly three times, so as to avoid penalizing a learner that is just slightly on the aggressive side in its inductions. In this way, while we describe the task’s inductive bias formally as requiring a minimum of 4 illustrated variable substitutions, a learner could succeed on the cSCAN task by adopting a minimum threshold of either 3 or 4 substitutions.\n\nWe formalize this domain-specific inductive bias of cSCAN with the bias function B : 2E → 2ED. To avoid circularity, we assume a variation (cid:96)∗ of the logic embedding defined in Appendix D.4 with the only difference being that (cid:96)∗ does not use axiom (18), which depends on the inductive bias. Similarly, we use (cid:44)→∗ to denote the variant of the minimal implication (cid:44)→ that is based on →∗ (rather than →).\n\nAs we mentioned in Section 3.2, we expect the learner to induce a rule if it has been illustrated with a sufficient number of variable substitutions. To formalize this, we define the function numsubst : X ×QR× 2Q → N such that numsubst(x,q,Q) is the number of different expressions that can be substituted for the variable x in the rule q to obtain a rule in Q (35). We also define the function minsubst : QR×2Q →N such that minsubst(q,Q) is the minimum number of such substitutions for any variable used in q (36).\n\nnumsubst(x,q,Q):=|{f (x)∈QN :(∃f ∈X (cid:55)→R∗ : subst(q,f )∈Q)}|\n\nminsubst(q,Q):=\n\n(cid:40)\n\nminx∈var(q)(numsubst(x,q,Q)), 0,\n\nvar(q)>0\n\notherwise\n\nFor any E ⊆ E, the bias function B(E) is then defined as follows:\n\nQ1(E,C):=∪{Q⊆QR : (∃e∈E : (E\\{e})∪{(cid:104)C,q,1(cid:105): q∈Q}(cid:44)→∗ e)} B1(E):={(cid:104)C,q,1,D(cid:105)∈E : q∈Q1(E,C) ∧ minsubst(q,Q1(E,C))≥4} B?(E):={(cid:104)C,q,?,D(cid:105)∈E : (∀r ∈{1,0}: (cid:48)∗ M(E∪B1(E))→M((cid:104)C,q,r,D))} B(E):=B1(E)∪B?(E)\n\n(35)\n\n(36)\n\n(37)\n\n(38) (39)\n\n(40)\n\nThe set Q1(E,C) is the set of candidate rules with context C that we may want to induce from the example set E. Specifically, it is the union of all sets of rules Q that, together with E \\ {e}, allow us to (minimally) explain some example e ∈ E (37).\n\nThe set B1(E) contains the assertions of all rules that are expected to be induced to be true based on the examples provided by E. Specifically, it consists of the assertions (cid:104)C,q,1,D(cid:105) of all the candidate rules q for which Q1(E,C) contains instances with at least 4 different substitutions for each variable (38).\n\nB?(E) contains the assertions of all rules for which E does not provide any evidence about whether or not they hold. Specifically, it contains an example (cid:104)C,q,?,D(cid:105) for all contexts C and rules q for which E does not imply a clear answer, i.e., 1 or 0 (39). Finally, B(E) is the union of all the rules that should be induced to be true and all the rules that should be induced to be unknown (40).\n\nDiscussion. Because all cSCAN contexts exclusively contain unconditional, monotonic examples, it is sufficient to induce rules with the empty context, i.e., we only need to consider Q1(E,∅).\n\nThe consistency criteria in B1 is relatively simplistic, which makes the task easier for humans. Indeed, we only need to make sure that each induced rule q is consistent with E but do not need to check whether the induced rules are consistent with each other. This allows us to greedily induce rules one at a time without worrying about potential conflicts among them.\n\nF DATASET GENERATION\n\nThe dataset generation process is config-driven, with a different dataset spec config being defined for each of the cSCAN datasets.\n\nAll cSCAN examples are automatically generated by a Python program built on the NLTK library (Bird et al., 2009).2 Generation is performed via the following steps.\n\n2To be released on GitHub upon paper acceptance.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nContext generation For efficiency, and to aid in generating clusters of related examples for calculation of the consistency metric, we generate examples in batches, in which we first generate a context and then generate multiple top-level examples that share that same context. Each context is created by first randomly generating a coherent set of interpretation rules of similar form to those shown in Figure 4 for the original SCAN task. While many of these rules may never be shown directly to the learner, this initial rule set serves as a kind of “basis rule set” from which the truth value of all other possible rules can be derived. We then randomly choose which of those basis rules to (a) provide explicitly in the context via an example that asserts that rule to be “true”, (b) illustrate implicitly through some set of context examples sufficient to satisfy the task’s inductive bias (see Appendix E.4 for details), (c) illustrate insufficiently or not at all, or (d) contradict in one or more cases, so that it should be inferrable to be “false”.\n\nBy generating contexts via the above procedure, we ensure that the basis rules cover each of the different possible replies and qualifiers for rule examples: monotonically “true” (case a), defeasibly “true” (case b), “unknown” (case c), and either monotonically or defeasibly “false” (case d). By extension, this also ensures that we achieve a diverse mixture of possible replies and qualifiers across the much larger set of rules that could be derived from different combinations of those basis rules.\n\nWe ensure that the exact ratio between the different above cases varies randomly from context to context, while achieving across the dataset as a whole the desired ratio that is configured in the dataset spec. Top-level example generation Once we have fixed a context, we then randomly generate a set of (request, reply, qualifier) triples corresponding to the top-level examples that we wish to generate using the given context. To aid in sampling such examples, we first construct a pair of inference engines in which we exhaustively generate all possible examples that would be considered “true” either monotonically or defeasibly (respectively) based on the given context. To generate a non-rule example or positive rule example, we then randomly sample a “true” example from the full set of examples that were inferred by one of those inference engines. To construct a negative rule example, we first sample a positive example and then apply one of a number of different heuristics to construct an example that is similar to that positive example, but which is not among the examples inferred by the inference engine.\n\nThese two inference engines encapsulate the logic needed to ensure that each top-level example satisfies the consistency criteria stated in Section 2.3 and agrees with the domain-specific inductive bias. Sub-sampling In order to ensure that the different example classes are evenly represented in the dataset, we perform the top-level example generation process described above in separate streams, each of which is dedicated to generating one specific example class (e.g., “positive monotonic rule examples” or “non-rule examples with reply of unknown”, etc.). We then sub-sample examples from each of the different streams in order to achieve the desired ratio of examples from each of the different classes. We ensure that the exact ratio between the different classes varies randomly from context to context, while matching the desired ratio in the dataset overall. Splitting Due to interdependencies between the splitting algorithm and the example generation process (see, e.g., the notes on “additional top-level example generation” below), we perform the train-validationtest split as one step of the dataset generation process, rather than generating a single dataset and then splitting it in multiple ways.\n\nFor performing MCD splits, we build on the open-sourced Python implementation of the MCD algorithm from Shaw et al. (2020). While their version of the MCD algorithm consists of initially performing a random split and then iteratively swapping examples to increase compound divergence, however, we found that we were able to achieve higher compound divergence on the cSCAN dataset by implementing an algorithm closer to that of the original one described in Keysers et al. (2020). In this approach, we begin with empty train and test sets and then iteratively select examples from a large example pool to add to one of the two sets, while once in every three steps selecting an example from one of the two sets to remove and put back in the pool. At each addition or removal step, we select from among a random sample of 200 examples the one whose addition or removal would maximize divergence at that stage, while keeping atom divergence low. One of the advantages of the insertion/deletion approach is that, in cases where it is acceptable to use only a portion of the available examples, the process can be stopped early, which can result in train and test splits with significantly higher compound divergence than would be possible if the algorithm were constrained to use all of the examples from the example pool. As described in Section 3.3, we do perform such sub-sampling when constructing our MCD splits, beginning with a pool of 1.2M top-level examples and ending with 100K top-level examples in train and 10K top-level examples in each of validation and test.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nWhile our above algorithm largely emulates the one described in Keysers et al. (2020), we do introduce one additional enhancement to enable generating a 3-way compound divergence split between train, validation and test. This is in contrast to Keysers et al. (2020), which performs only a single stage of MCD splitting for maximizing compound divergence between train and test. In our approach, we perform two stages of MCD splitting, with the goal of constructing a train set, validation set, and test set, with high pairwise compound divergence between any two of the three. In the first stage, we split the example pool into a train+validation pool and a test set, with the objective of maximizing the compound divergence between the two. In the second stage, we keep the test set fixed, while further splitting the train+validation pool into a train set and a validation set, with the joint objective of maximizing compound divergence between train and validation and maximizing compound divergence between train and test. We perform down-sampling in each of the two stages to increase the compound divergences that we are able to achieve. Appendix H contain statistics on the MCD splits, along with other details of the datasets. Additional top-level example generation For the cSCAN Random variants, after performing a random split, we augment the validation and test sets by generating additional top-level examples for each validation and test context, using the same example generation logic described above. This allows us to achieve a higher density of logically related examples in the validation and test sets, so as to yield a larger number of potential implications and contradictions for use in calculating the consistency metric. We do not perform this step for the cSCAN MCD variants, however, to avoid impacting the compound divergences between the train, validation and test sets. For this reason, we focus our investigation of consistency in this paper on the cSCAN random splits, as it is only in the random splits that we are able to identify a significant number of implications and contradictions. Improving the sampling and splitting algorithms to enable investigation of consistency in MCD splits could be a topic for future work.\n\nG CONSISTENCY METRIC CALCULATION\n\nWhile the consistency metric as described in Appendix D in its most general form can be prohibitively expensive to calculate if approached naively, a close approximation of it can be calculated efficiently through the application of several task-specific assumptions.\n\nFirst of all, in the consistency metric calculation we use with cSCAN, we consider for simplicity only implications and contradictions among predictions for top-level examples that share the same context. While in general implications and contradictions can occur even among examples with different contexts (particularly if the examples are monotonic and if one context is a superset of the other), due to the way in which we construct the cSCAN dataset, such situations are extremely unlikely to occur. By focusing only on identifying implications and contradictions among examples sharing the same context, we are able to cleanly partition the dataset into independent clusters of examples, such that we can analyze each cluster efficiently in parallel. Dealing with clusters of examples that share the same context also simplifies analysis in that we now only need to consider the implications and contradictions among the request-reply pairs, while effectively ignoring the context.\n\nAs the cSCAN validation and test sets contain up to 1000 top-level examples per context, however, it would still be prohibitively expensive to enumerate each of the possible subsets of these examples to identify the sets that involve a “minimal” implication or contradiction. Instead, we find that we are able to identify the implications and contradictions much more efficiently by seeding an inference engine with the rule assertions that would correspond to the up to 1000 top-level request-reply pairs (ignoring negative rule replies and unknown replies for simplicity) and then performing exhaustive forward inference to determine all possible rules that could be inferred from combinations of these asserted rules. This is essentially the same inference process that is used in top-level example generation (as described in Appendix F), except that we take the additional step of tracking the provenance of each inferred rule, and we continue the inference process so as to generate all possible provenances of each rule (rather than omitting reprocessing of rules that have already been inferred via a different route). For the purposes of this consistency calculation, it is sufficient to consider as provenance the set of asserted rules that led to the given inference. Once the exhaustive inference process is complete, we then check each of the asserted rules (i.e., each of the top-level predictions) against the contents of the inference engine. If the asserted rule was also inferred from some other rules, then we take the full set of inference provenances for that rule, filter out any inference provenances that are supersets of some other provenance, and then treat each of those remaining provenances together with the asserted rule as one “minimal implication” (i.e., as one minimal implication set F ∈ impl(E) as defined in Equation 24). Similarly, if a rule was inferred that shares the same left-hand side as the asserted but contains a different right-hand side, then we look at\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Atom and compound divergence of the cSCAN MCD datasets between pairs of splits.\n\ntrain vs test Atom Compound Atom Compound Atom Compound\n\ntrain vs validation\n\nvalidation vs test\n\ncSCAN-B MCD 0.066 cSCAN-X MCD 0.050\n\n0.685 0.722\n\n0.039 0.046\n\n0.634 0.726\n\n0.030 0.067\n\n0.562 0.724\n\nTable 7: Atom and compound counts of the cSCAN MCD datasets. The ”held out” column records the number of compounds that appear in the test split but not in the train split.\n\nMin. occurrence of atom # Compounds train\n\nvalid\n\nvalid\n\ntrain\n\ntest\n\ncSCAN-B MCD 4180 cSCAN-X MCD 755\n\n1110 92\n\n1440 455\n\n88 729\n\n91 758\n\ntest\n\n118 1125\n\nall\n\nheld out\n\n138 1754\n\n47 752\n\nthe provenances of each such inferred rule, filter out any that are supersets of some other provenance of the same rule, and then treat each of those remaining provenances together with the asserted rule as one “minimal contradiction” (i.e., as one minimal implication set F ∈cont(E) as defined in Equation 25).\n\nDespite the relatively large number of rules with which we seed each inference engine and the extra expense of tracking multiple rule provenances, we find that we are able to complete exhaustive inference quickly in practice, due to the fact that the majority of the rules that are asserted in top-level examples tend to be more specific than the rules that are typically asserted inside of a context, and thus lead to only a limited number of interactions, which ends up being comparable to or less than the computational cost of the inference involved in constructing the contexts in the first place. In practice, when calculating the consistency metric in parallel using a different work unit for each context, we find that we are able to calculate the consistency metric for a full cSCAN experiment within a few minutes.\n\nH DATASET DETAILS\n\nTable 6 shows atom divergence and compound divergence between pairs of splits.\n\nTable 7 shows the minimal number of times an atom occurs in each split, the number of compounds in each split, and the number of compounds in the test split that are held out from the train split.\n\nTable 8 shows details of all splits of all datasets used in the cSCAN experiments.\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Key statistics of the cSCAN datasets.\n\ncSCAN-B Random\n\ncSCAN-X Random\n\ntrain\n\nvalid\n\ntest\n\ntrain\n\nvalid\n\ntest\n\nNumber of examples Number of contexts\n\nRequest: Rule (%) Request: Non-rule (%)\n\nQualifier: Monotonic (%) Qualifier: Defeasible (%)\n\nReply: Unknown (%) Reply: Yes (%) Reply: No (%)\n\nNumber of examples Number of contexts\n\nRequest: Rule (%) Request: Non-rule (%)\n\nQualifier: Monotonic (%) Qualifier: Defeasible (%)\n\nReply: Unknown (%) Reply: Yes (%) Reply: No (%)\n\nNumber of examples Number of contexts\n\nRequest: Rule (%) Request: Non-rule (%)\n\nQualifier: Monotonic (%) Qualifier: Defeasible (%)\n\nReply: Unknown (%) Reply: Yes (%) Reply: No (%)\n\n100,000 100,000 100,000 100,000 99,756 100,000 100\n\n1,000\n\n1,000\n\n100\n\n100\n\n100\n\n50.1 49.9\n\n46.3 53.7\n\n13.3 21.8 21.7\n\n50.0 50.0\n\n45.5 54.5\n\n13.3 21.3 22.1\n\n50.0 50.0\n\n46.0 54.0\n\n13.1 21.8 21.9\n\n50.1 49.9\n\n50.0 50.0\n\n13.2 21.9 21.7\n\n49.9 50.1\n\n50.0 50.0\n\n12.9 22.2 21.4\n\ncSCAN-B MCD valid\n\ntrain\n\ncSCAN-X MCD valid\n\ntrain\n\ntest\n\n50.0 50.0\n\n48.0 52.0\n\n13.6 21.5 22.1\n\ntest\n\n100,000 10,000 10,000 99,999 10,000 10,000 4,599\n\n7,965 4,627\n\n11,921\n\n6,509\n\n6,115\n\n72.5 27.5\n\n26.9 73.1\n\n52.2 12.0 12.1\n\n76.5 23.5\n\n19.0 81.0\n\n67.2 7.2 7.8\n\n64.0 36.0\n\n36.6 63.4\n\n35.2 18.1 18.2\n\n59.8 40.2\n\n37.0 63.0\n\n39.7 12.7 12.9\n\n73.8 26.2\n\n29.9 70.1\n\n50.6 14.3 13.7\n\n67.4 32.6\n\n38.9 61.1\n\n36.3 18.5 19.2\n\ncSCAN-B 100 Contexts\n\ntrain\n\nvalid\n\ntest\n\ncSCAN-B 8000 Contexts test valid\n\ntrain\n\n10,000 100,000 100,000 800,000 99,713 100,000 100\n\n8,000\n\n100\n\n100\n\n100\n\n100\n\n50.1 49.9\n\n44.6 55.4\n\n12.8 22.1 21.8\n\n50.1 49.9\n\n45.6 54.4\n\n13.0 21.9 21.7\n\n50.1 49.9\n\n45.5 54.5\n\n13.6 21.1 22.3\n\n50.1 49.9\n\n46.1 53.9\n\n13.4 21.8 21.6\n\n49.9 50.1\n\n45.5 54.5\n\n14.2 21.4 21.6\n\n50.1 49.9\n\n44.9 55.1\n\n13.6 21.1 22.2\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nI ANALYSIS OF REALISTIC CSCAN EXAMPLES\n\nIn this appendix, we provide realistic examples from a slightly earlier version of the cSCAN dataset and outline a systematic strategy for solving them. We selected a total of 22 examples (Figure 8) that are based on a single context of size 23 (Figure 7). We made sure that the different classes (see Section 2.4) are well represented among the selected examples. This means that the examples cover all valid combinations of request types, replies, and qualifiers.\n\nTo illustrate how cSCAN can be solved by humans, we outline one possible strategy below. This strategy is based on the assumption that we are aware of the grammar (Appendix E.2 and Figure 5), the rule semantics (Appendix E.3), and the inductive bias (Appendix E.4) of cSCAN.\n\nI.1 MONOTONIC EXAMPLES\n\nIn a first step, we check whether a given example can be deduced directly from the context. This is the case for all requests that exclusively contain syntactic constructs for which the behavior is completely determined by explicit rules in the context. In our context C, the rules (C1) through (C11) completely determine the behavior of all constructs except “look”, “thrice”, and “and”. This means that we can deduce the reply for the examples (E1), (E2), (E3), (E4), (E5), and (E7), which we consequently mark as monotonic.\n\n=JUMP (C3),\n\nAs an illustration, consider the request “jump around left twice” from example (E1). Once we know that x1 jump (cid:74) (cid:75) (cid:74) (cid:75) (C10), we can immediately deduce that “jump around left twice” is translated to “JUMP PURPLE JUMP”. Note that some of the examples are closely related to one another. For example, the example (E1) is an instance of the rule\n\nx1 around x2 (cid:75) (cid:74)\n\nx1 twice (cid:75) (cid:74)\n\nfrom example (E5).\n\n=PURPLE (C1),\n\nleft (cid:75) (cid:74)\n\n(C8), and\n\nx1 (cid:74)\n\nx1 (cid:75)\n\nx2\n\nx2\n\n(cid:75)(cid:74)\n\n(cid:75)(cid:74)\n\n=\n\n=\n\n=\n\nx1 around x2 twice (cid:75) (cid:74)\n\nx1 (cid:74)\n\n(cid:75)(cid:74)\n\nx1 (cid:75)\n\n(cid:75)(cid:74)\n\nThe examples (E6), (E8), and (E9) contain some of the syntactic constructs that are not completely determined (i.e., “look”, “thrice”, and “and”), but they can still be deduced from the context and are thus marked as monotonic. The example (cid:104)C, =WHITE WHITE PURPLE,1,M(cid:105) (E6) follows look left (cid:75) (cid:74) look x1 directly from the rules (cid:75) (cid:74)\n\nleft (cid:75) (cid:74) = WHITE YELLOW,0,M(cid:105) (E8) can be deduced from the examples look (cid:75) (cid:74) =WHITE YELLOW were true, (C7) would allow us to deduce the rule look (cid:75) (cid:74) = WHITE YELLOW WHITE YELLOW\n\nSimilarly, the example (cid:104)C, (C7), and (C12). Indeed, if\n\n= PURPLE (C1) and\n\n= WHITE WHITE\n\nx1 (cid:75) (cid:74)\n\n(C12).\n\nlook x1 (cid:74) (cid:75) Finally, (E9) can be deduced from (C2), (C8), and (C14) because x1 around right thrice (cid:75) (cid:74)\n\nx1 (cid:75) (cid:74)\n\nGREEN\n\nGREEN\n\nx1 (cid:75)\n\nx1 (cid:74)\n\n(cid:75)(cid:74)\n\n=\n\nx1 , which contradicts (C12). (cid:75) (cid:74)\n\nx1 thrice (cid:75) (cid:74)\n\n= x1 , which contradicts (C14). (cid:75) (cid:74)\n\nx1 (cid:75)\n\nx1 (cid:74)\n\n(cid:75)(cid:74)\n\nwould imply that\n\nI.2 DEFEASIBLE EXAMPLES\n\nTo determine the reply of examples that cannot be deduced from the context, we check whether the context allows us to induce some rules for the constructs “look”, “thrice”, and “and”, which are not fully determined.\n\nlook (cid:75) (cid:74)\n\nInducing rules for “look”, “thrice”, and “and”. We start with the primitive command “look” and identify all context examples that contain “look” but none of the other partially-determined constructs “thrice” and “and”. This yields the rules (C12) and (C13). Together with (C7), the rule (C12) tells us that “look” must either be “WHITE” or undefined (i.e., ?), and the same holds for (C13). While these two examples alone are not sufficient under the task’s inductive bias (Appendix E.4) to justify inducing the general form of the “look” rule, we will proceed with the assumption that\n\n= WHITE for the time being.\n\nNext, we apply the same process to the syntactic construct “thrice”. This means that we identify all context examples that contain “thrice” but not “look” and “and”, which are (C14), (C15), and (C16). The first x1 x1 x1 x1 thrice ” or be undefined. However, ” must either translate to “ two of them tell us that “ (cid:75) (cid:74) (cid:75) (cid:74) (cid:75)(cid:74) x1 x1 x1 x1 thrice example (C16) is incompatible with the rule . In accordance with the inductive (cid:75) (cid:75)(cid:74) (cid:74) (cid:75) (cid:74) bias, we therefore cannot induce a general rule for “thrice”.\n\n(cid:75)(cid:74) (cid:75)(cid:74)\n\n=\n\nNow, we apply the process to “and”, which means that we look at the rules (C17) and (C18). Together x1 x2 x1 and x2 ” or be ” must either translate to “ with the example (C10), they both tell us that “ (cid:75) (cid:74) (cid:75) (cid:74) x2 undefined. So, we proceed with the assumption that (cid:74)\n\nAs a next step, we look at the examples (C19) and (C20) whose requests contain both “and” and “look” =\nbut not “thrice”. Both of them follow from our current assumptions x1 x2 . This means that we have now found four examples in the context that agree with our (cid:75) (cid:74)\n\nx1 and x2 (cid:75) (cid:74)\n\nx1 and x2 (cid:75) (cid:74)\n\n=WHITE and\n\nlook (cid:75) (cid:74)\n\nx1 .\n(cid:75)\n\nx2\n\nx2\n\nx2\n\n(cid:75)(cid:74)\n\n(cid:75)(cid:74)\n\n(cid:75)(cid:74)\n\n(cid:75)(cid:74)\n\n(cid:75)(cid:74)\n\n(cid:75)(cid:74)\n\n=\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: A context C from the cSCAN dataset. The examples that make up the context are sorted, starting with 11 explicit rules that specify the complete behavior of all syntactic constructs except “look”, “thrice”, and “and”. The remaining examples are sorted such that we first have the examples that do not contain “thrice” and “and”, then the examples that do not contain “and”, and finally the remaining examples.\n\nFigure 8: Examples from the cSCAN dataset. The examples are based on context C shown in Figure 7 above, and they are grouped by request type and qualifier.\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\nassumed rules for “look” and “and”, and there are no examples that contradict them. Based on our inductive bias, this means that we induce these rules.\n\nAll the remaining examples (i.e., (C21), (C22), and (C23)) contain the construct “thrice”, for which we cannot induce a general rule.\n\nApplying the induced rules for “look” and “and”. Using the induced rules for “look” and “and”, we can determine a concrete reply for the examples (E10) through (E12) as well as (E16) through (E19). These examples are marked as defeasible because they are based (among others) on induced rules, which means that the reply might change if the context is expanded.\n\nUsing reply “unknown” for examples containing “thrice”. Because we are not able to induce a generic rule for “thrice”, we are not able to determine a concrete reply for the examples (E13) through (E14) and (E20) through (E22). Instead we use the reply “unknown” (?) and mark them as defeasible because the reply might again change if the context is expanded.\n\nJ EFFECT OF DATA SIZE\n\nTable 9: Accuracy as a function of training data size. Results from experiments with pre-trained T5-Base (220M parameters) on datasets of similar form to cSCAN-B, but of varying size.\n\nAs an additional set of experiments, we investigate the impact of training data size on the performance of the relatively strong pre-trained T5-Base (220M parameters) baseline. In each experiment, we use a dataset generated with a similar mix of examples as in cSCAN-B, but with a varying number of contexts and examples in the train set. As can be seen in the results in Table 9, while the model is able to achieve high accuracy and relatively high consistency as the train size approaches 800K examples, performance drops dramatically as the train decreases from 100K examples down to 10K examples. This suggests that finding ways to reduce models’ dependency on large amounts of task-specific training data will be an additional important theme for future work in conceptual learning.\n\nContexts Examples Accuracy Consistency\n\n10,000 100,000 800,000\n\n100 1,000 8,000\n\n6.9 71.3 91.9\n\n38.6 92.6 97.7\n\nK EXTENDED RELATED WORK\n\nK.1 COMPARISON WITH EXISTING DATASETS\n\nIn representing the input of a CLT as a request paired with a context, we build on a long tradition of QA and reasoning task formulations that provide knowledge relevant to a task via various forms of context, such as a text passage (Kwiatkowski et al., 2019; Weston et al., 2015; Dua et al., 2019; Sinha et al., 2019; Yang et al., 2018; Rajpurkar et al., 2016; Levesque et al., 2012), logical premise (Roemmele et al., 2011; Dagan et al., 2005; Bowman et al., 2015), set of natural language statements (Talmor et al., 2020), knowledge graph fragment (Sinha et al., 2020), antecedent description grammar (Cohen, 1994), dialog (Semantic Machines et al., 2020; Budzianowski et al., 2018), image (Antol et al., 2015; Johnson et al., 2017; Hudson & Manning, 2019; Bahdanau et al., 2019a;b), grid world (Ruis et al., 2020), or DB schema (Yu et al., 2018).\n\nHere, to give a better sense of how a CLT is similar to and different from these existing task formulations, we make a closer comparison of cSCAN with several representative NLU tasks that provide explicit knowledge as part of the input and satisfy some of the desired properties for a CLT formulated in Section 2.1. This is illustrated in Figure 9.\n\nNote that we focus our comparison here specifically on the defining features of a CLT. This should not be construed as a commentary on the overall usefulness or quality of these benchmarks. Indeed, many of the benchmarks described here have complementary strengths which cSCAN lacks, such as being based on true natural language, supporting multi-modal input, illustrating specific domains of reasoning, or covering more complex reasoning or syntax.\n\nTalmor et al. present a series of “Leap-of-Thought” tasks (Talmor et al., 2020) where the learner needs to judge yes/no hypotheses by performing inference over knowledge that is obtained implicitly from language model pretraining while part of the knowledge is also provided explicitly using a context containing natural\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: Comparison of cSCAN and other NLU task against the key features of conceptual learning. For the context (explicit knowledge), we distinguish whether it is organized as a set of independent units rather than a monolithic block (column 1) and whether different examples use different and sometimes contradictory knowledge (column 2). We also distinguish whether the context contains both rules (column 3) as well as examples (column 4). For the request, we indicate whether the task contains examples that explicitly test the truth value of rules (column 5). For the output, we indicate whether the task requires identifying whether a request cannot be answered based on the given context (column 6) and whether it distinguishes between inductive and deductive reasoning.\n\nlanguage statements. This setup satisfies various CLT properties. In particular, arbitrary hypotheses can be asserted in the context, and the requests can ask for the truth value of any context statement (examples and rules coincide in this task because all examples are yes/no hypotheses). However, the knowledge is constant across all examples (albeit different parts of this knowledge are provided explicitly for different examples). Therefore, Leap-of-Thought tasks do not investigate whether a learner is able to adapt to transient knowledge, e.g., explicit knowledge that may contradict the implicit knowledge obtained during language model pretraining (e.g., an actor was married and is now divorced). Similarly, these tasks exclusively focus on monotonic inference: they do not test whether the learner is able to induce defeasible hypotheses from the explicit knowledge nor do they test the ability to identify certain hypotheses as “unknown”.\n\nGraphLog (Sinha et al., 2020) is a benchmark suite based on tasks that are quite similar to CLTs. The learner is presented with part of a graph consisting of labeled edges (context) and then needs to predict a the label for an edge that is not part of the context. For example, the context may contain two “father-child” relations and the learner needs to predict the “grandfather-grandchild” relation. As for cSCAN, the graph is constructed automatically based on a set of first-order logic rules, which make sure that each task is consistent. However, in contrast to cSCAN, the underlying rules cannot be part of the context, nor are they expressible as requests. In the CLT terminology, this means that GraphLog tests only the case where the context consists of examples and the learner has to induce new examples. It does not support the case where the context contains rules and the learner has to consistently combine and apply these rules deductively.\n\nThe bAbI tasks (Weston et al., 2015) also require the learner to answer a question using variable context consisting of natural language statements. Each context consists of a sequence of relatively simple factual statements that are, at least for some of the tasks, order-dependent (Dehghani et al., 2018). This means that a bAbI context does not directly correspond to a set-based context that we are using with CLTs. One way to bridge this gap is to consider a sequence of bAbI statements as a single “macro rule”, but the truth value of these rules cannot be requested. bAbI contains some tasks that require deductive reasoning and some tasks that require inductive reasoning.\n\nThe gSCAN task (Ruis et al., 2020) is an extension of SCAN where the learner is provided with a context that describes a spacial configuration of objects in order to translate commands into a sequences of actions. However, as with bAbI, the context consists of a single dedicated structure describing the spacial configuration as a whole rather than a set of rules that describe the different objects that are part of the spacial configuration one by one. This makes the language to specify the context disjoint from the request language.\n\n36\n\nUnder review as a conference paper at ICLR 2023\n\nAs an example of a reading comprehension task, Natural Questions (Kwiatkowski et al., 2019) is a benchmark where the learner is given a Wikipedia page as context and then needs to answer a natural language question by outputting a long answer (e.g., the paragraph containing the answer) as well as a short answer. As with bAbI, the context is not a set of independent rules but instead a sequence of inter-dependent statements, which makes this benchmark quite different from a CLT.\n\nK.2 OTHER RELATED WORK\n\nCompositional generalization. Our evaluation of cSCAN MCD splits builds on existing research in measuring the ability of machine learning models to compositionally generalize (Keysers et al., 2020; Lake & Baroni, 2017). In response to compositional generalization benchmarks such as SCAN, a range of techniques have been proposed which have not yet been evaluated on cSCAN. Some of these solutions involve specialized architectures for enforcing a compositional bias (Qiu et al., 2022; Chen et al., 2020; Liu et al., 2020; Nye et al., 2021). Such architectures are appealing due to their potential for achieving a principled solution to compositional generalization, but would require some effort to adapt to the context and heterogenous output of a CLT. In the past, some specialized architectures have shown limited success when transferring to new tasks, compared to more general techniques such as language model pre-training (Furrer et al., 2020). In the latter category, there is some promise shown by recent developments in decompositional prompting techniques, which have led to strong results on the SCAN MCD splits using off-the-shelf large language models (Zhou et al., 2022).\n\nInstruction following. In evaluating the ability for a system to apply rules to a task, our work relates to research in building systems that learn to follow instructions (Goldwasser & Roth, 2014), including recent research in the instruction-following capabilities of large language models (Wei et al., 2022; Ouyang et al., 2022; Wang et al., 2022). Our approach differs in that we provide in the context a set of rules, each of which may be applicable to some part of the task, and we evaluate the ability to infer new rules in addition to applying the rules to an underlying task.\n\nMeta-learning. Meta-learning or “learning to learn” generally refers to a setup where the learner is provided with a family of tasks, which are also called episodes, each of which comes with its own set of training examples and test examples. A learner is able to “learn to learn” if its performance for each task increases both with increasing training data and with an increasing number of tasks (Thrun & Pratt, 1998; Hospedales et al., 2021; Finn et al., 2017).\n\nThe presence of examples within the context of an example gives CLTs a nested structure that allows us to view CLTs through the lens of the meta-learning setup. In this view, top-level examples that share the same context correspond to an episode where the context examples are the training examples and the top-level examples (w/o the context) are the test examples.\n\nClosely related to cSCAN are two pieces of work that apply meta-learning to SCAN, both of which generate large numbers of SCAN-like grammars, from which they construct meta-learning episodes, similarly to how we generate cSCAN contexts based on SCAN-like grammars. Lake (2019) uses a memory-augmented network to attend to the train examples of each episode, while Nye et al. (2020) trains for each episode a program synthesis model that outputs the underlying rules of the task. Our approach differs in that we include in the context a mixture of rules and examples, rather than just examples of the underlying task, and we use the synthetically-generated contexts to define a new task for evaluating the ability of the system to generalize to many different rule sets, rather than using meta-learning techniques as a means to improve accuracy on the original SCAN task.\n\nRule induction and logic deduction tasks. CLTs require the learner to judge whether a certain rule can be induced from a given set of observations (i.e., examples provided as part of the context). This is similar to a rule induction task (Cohen, 1995; Reddy & Tadepalli, 1998; Grzymala-Busse, 2010), with the main caveat that the learner only needs to verify rules rather than generate them. CLTs also require the learner to apply rules and judge whether a rule may be deductively obtained from a set of other rules.\n\nInterpretable ML models. CLTs make part of the rules that govern the underlying tasks explicit, which allows us to “introspect” the behavior of the learner by asking, as part of the task, whether or not a certain rule holds (inductively or deductively). This means that the question of whether and why a certain model behaves correctly or incorrectly can be broken down into two parts: (a) did the model learn the right rules and (b) is it able to apply these rules consistently. This is related to, yet different from, other efforts to make ML models more interpretable. For example, Sushil et al. (Sushil et al., 2018) propose a method to\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\ninduce if-then-else rules to explain the behavior of ML models. However, unlike for CLTs, this method is external to the actual task: it does not reflect whether the model claims a certain rule to be true but instead identifies the if-then-else rules between different input features and class labels that are the most important for classification according to the model.\n\nConsistency. Our consistency metric is related to research into evaluating and improving the consistency of neural networks. One closely related work is Li et al. (2019), which evaluates consistency by generating clusters of examples that by construction are related via logical symmetry, transitivity, or some other logical constraint. Based on these examples, they calculate a “conditional violation” metric, which similarly to our consistency metric is a ratio of violated (in our case, satisfied) constraints vs. the total number of logical constraints. Our approach differs in that we gather the logical constraints automatically using symbolic inference over the generated examples, rather than depending on a specific algorithm for generating related examples.\n\nHandling of large contexts. Due to the potentially large context size in CLTs, another relevant area of research is how to deal with very large contexts, including large set-like contexts. One line of research in this area involves modifying the Transformer architecture to be able to handle longer inputs more efficiently (Tay et al., 2020a;b). We evaluated two such architecture variants in our LongT5 and LongT5Global baselines (Guo et al., 2022), but many other such variants have been proposed (Gu et al., 2021; Zaheer et al., 2020; Choromanski et al., 2020; Wang et al., 2020). In cases where the context takes the form of a set or a graph, some approaches seek to explicitly take into account this structure by encoding the input structure in positional embeddings (Herzig et al., 2020), guiding the Transformer’s attention via the structural relations within the input (Ainslie et al., 2020), or message-passing in graph neural networks (Gilmer et al., 2017; Battaglia et al., 2018). Other lines of research seek to more efficiently deal with large pools of potentially relevant knowledge by either performing cross-attention from portions of the input to knowledge stored in neural memory (Verga et al., 2020) or by retrieving only the most relevant material from a knowledge base or text corpus for concatenation to the input (Guu et al., 2020; Pasupat et al., 2021).\n\nL REPRODUCIBILITY\n\nHardware and training period Table-10 shows the different hardware used for each experiment and the training period (in steps). T5 versions For our T5 baselines, we use T5X (Roberts et al., 2022), which is a re-implementation of T5 in JAX (Bradbury et al., 2018) using Flax (Heek et al., 2020). Table 11 presents the configurations of different T5 variants that we used in our experiments. For the full-attention version of T5, we experiment with both fine-tuning from a standard pre-trained checkpoint and training from scratch.\n\nFor LongT5 and LongT5-TGlobal, while we initially evaluated with both fine-tuning from a standard pre-trained checkpoint and training from scratch, when fine-tuning, we failed to find a setup in which the models converge on the train set, possibly due to poor compatibility between the cSCAN task and the summarization-oriented PEGASUS Principle Sentences Generation pre-training objective (Zhang et al., 2019) used in LongT5. For this reason, we only report results on LongT5 and LongT5-TGlobal models trained from scratch.\n\nFor each of the architectures, we evaluate at minimum two sizes: Small (60M parameters) and Base (220M parameters). For the best-performing T5 architecture, we further evaluate on size Large (770M parameters). We omitted experiments on Large variants of the other architectures for reasons of computational cost, as the poor performance on the Small and Base sizes suggest that it is unlikely for the performance of LongT5, LongT5-TGlobal or the non-pretrained version of T5 to improve significantly with model size alone. Hyperparameters reasons for choosing those hyperparameters are:\n\nTable 12 summarizes the hyperparameters used for each of the baselines. The\n\n• Config: We chose the config version based on experiments on an earlier version of the dataset. In that version, we found that T5 achieved higher performance using the T5.1.0 config, while LongT5 and LongT5-TGlobal performed better on the T5.1.1 config.\n\n• Learning Rate: A constant learning rate is the standard way to fine-tune pre-trained T5 models. For non-pretrained models we found that a constant learning rate performed as well as the inverse square root decay with linear warmup scheduler (the standard learning rate scheduler for pretraining T5).\n\n38\n\nUnder review as a conference paper at ICLR 2023\n\nTable 10: Hardware and training period (in steps).\n\nModel\n\nSize\n\nPretrain Dataset\n\nTPU Type\n\nTPU Slices\n\nTraining Steps\n\nTraining Epochs\n\ncSCAN-B\n\ncSCAN-B\n\ncSCAN-B\n\ncSCAN-B\n\ncSCAN-B\n\ncSCAN-B\n\ncSCAN-B\n\ncSCAN-B\n\ncSCAN-B\n\ncSCAN-X\n\ncSCAN-X\n\ncSCAN-X\n\ncSCAN-X\n\ncSCAN-X\n\ncSCAN-X\n\ncSCAN-X\n\ncSCAN-X\n\ncSCAN-X\n\ncSCAN-B MCD\n\ncSCAN-B MCD\n\ncSCAN-B MCD\n\ncSCAN-B MCD\n\ncSCAN-B MCD\n\ncSCAN-B MCD\n\ncSCAN-B MCD\n\ncSCAN-B MCD\n\ncSCAN-B MCD\n\ncSCAN-X MCD\n\ncSCAN-X MCD\n\ncSCAN-X MCD\n\ncSCAN-X MCD\n\ncSCAN-X MCD\n\ncSCAN-X MCD\n\ncSCAN-X MCD\n\ncSCAN-X MCD\n\ncSCAN-X MCD\n\nT5\n\nT5\n\nT5\n\nT5\n\nT5\n\nLongT5\n\nLongT5\n\nLongT5-TGlobal\n\nLongT5-TGlobal\n\nT5\n\nT5\n\nT5\n\nT5\n\nT5\n\nLongT5\n\nLongT5\n\nLongT5-TGlobal\n\nLongT5-TGlobal\n\nT5\n\nT5\n\nT5\n\nT5\n\nT5\n\nLongT5\n\nLongT5\n\nLongT5-TGlobal\n\nLongT5-TGlobal\n\nT5\n\nT5\n\nT5\n\nT5\n\nT5\n\nLongT5\n\nLongT5\n\nLongT5-TGlobal\n\nLongT5-TGlobal\n\nT5\n\nT5\n\nT5\n\nS\n\nB\n\nL\n\nS\n\nB\n\nS\n\nB\n\nS\n\nB\n\nS\n\nB\n\nL\n\nS\n\nB\n\nS\n\nB\n\nS\n\nB\n\nS\n\nB\n\nL\n\nS\n\nB\n\nS\n\nB\n\nS\n\nB\n\nS\n\nB\n\nL\n\nS\n\nB\n\nS\n\nB\n\nS\n\nB\n\nB\n\nB\n\nB\n\nTrue\n\nTrue\n\nTrue\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nTrue\n\nTrue\n\nTrue\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nTrue\n\nTrue\n\nTrue\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nTrue\n\nTrue\n\nTrue\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nFalse\n\nTrue\n\nTrue\n\nTrue\n\nTPU V2\n\nTPU V3\n\nTPU V4\n\nTPU V2\n\nTPU V3\n\nTPU V2\n\nTPU V3\n\nTPU V2\n\nTPU V3\n\nTPU V2\n\nTPU V3\n\n64\n\n16\n\n64\n\n64\n\n16\n\n64\n\n16\n\n64\n\n16\n\n64\n\n16\n\nTPU V4\n\n128\n\nTPU V2\n\nTPU V3\n\nTPU V2\n\nTPU V3\n\nTPU V2\n\nTPU V3\n\nTPU V2\n\nTPU V3\n\nTPU V4\n\nTPU V2\n\nTPU V3\n\nTPU V2\n\nTPU V3\n\nTPU V2\n\nTPU V3\n\nTPU V2\n\nTPU V3\n\nTPU V4\n\nTPU V2\n\nTPU V3\n\nTPU V2\n\nTPU V3\n\nTPU V2\n\nTPU V3\n\n64\n\n16\n\n64\n\n16\n\n64\n\n16\n\n64\n\n16\n\n64\n\n64\n\n16\n\n64\n\n16\n\n64\n\n16\n\n64\n\n16\n\n64\n\n64\n\n16\n\n64\n\n16\n\n64\n\n16\n\n16\n\n16\n\n16\n\n150,000\n\n50,000\n\n50,000\n\n300,000\n\n150,000\n\n300,000\n\n150,000\n\n300,000\n\n150,000\n\n150,000\n\n50,000\n\n50,000\n\n300,000\n\n150,000\n\n300,000\n\n150,000\n\n300,000\n\n150,000\n\n150,000\n\n50,000\n\n50,000\n\n300,000\n\n150,000\n\n300,000\n\n150,000\n\n300,000\n\n150,000\n\n150,000\n\n50,000\n\n50,000\n\n300,000\n\n150,000\n\n300,000\n\n150,000\n\n300,000\n\n150,000\n\n100,000\n\n100,000\n\n100,000\n\n192\n\n64\n\n64\n\n384\n\n192\n\n384\n\n192\n\n384\n\n192\n\n192\n\n64\n\n64\n\n384\n\n192\n\n384\n\n192\n\n384\n\n192\n\n192\n\n64\n\n64\n\n384\n\n192\n\n384\n\n192\n\n384\n\n192\n\n192\n\n64\n\n64\n\n384\n\n192\n\n384\n\n192\n\n384\n\n192\n\n1280\n\n25.6\n\n16\n\ncSCAN-B 100 Contexts\n\nTPU V3\n\ncSCAN-B 5000 Contexts\n\nTPU V3\n\ncSCAN-B 8000 Contexts\n\nTPU V3\n\nTable 11: Configurations of different T5 variants.\n\nName\n\n#Parameters\n\n#Layers\n\nSmall Base Large\n\n60M 220M 770M\n\n6 12 24\n\n39\n\ndmodel\n\n512 768 1024\n\ndff\n\n#heads\n\n2048 3072 4096\n\n8 12 16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 12: Hyperparameters used in the cSCAN baselines.\n\nT5 (Pretrained)\n\nSmall\n\nBase\n\nLarge\n\nT5 (Non-Pretrained) Base\n\nSmall\n\nLongT5 Small\n\nLongT5-TGlobal\n\nBase\n\nSmall\n\nBase\n\nConfig\n\nT5.1.0\n\nT5.1.0\n\nT5.1.0\n\nT5.1.0\n\nT5.1.0\n\nT5.1.1\n\nT5.1.1\n\nT5.1.1\n\nT5.1.1\n\nLearning Rate Scheduler Learning Rate\n\nConstant Constant Constant Constant Constant Constant Constant Constant Constant 0.001\n\n0.001\n\n0.001\n\n0.001\n\n0.001\n\n0.001\n\n0.001\n\n0.001\n\n0.001\n\nBatch Size\n\n128\n\n128\n\n128\n\n128\n\n128\n\n128\n\n128\n\n128\n\n128\n\nLoss Normalizing Factor\n\n233472\n\n233472\n\n233472\n\n233472\n\n233472\n\n233472\n\n233472\n\n233472\n\n233472\n\nInitial Checkpoint\n\n1000000\n\n999900\n\n1000700\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n• Loss Normalizing Factor: By instruction of T5 creators, we used a fixed loss normalizing factor\n\nof\n\nPretrainingBatchSize(2048)×TargetTokensLength(114)=233472\n\nfor fine-tuning and we used the same value for non-pretrained models to maintain consistency.\n\nNote that for the small models using T5.1.1 config, we set the number of layers and heads to 6 and 8 respectively to match those set in T5.1.0 config. Tokenization All models use the pretrained SentencePiece tokenizer (Kudo & Richardson, 2018) provided by T5, which is pretrained to cover the English, French, German and Romanian languages with 32,000 tokens. We also tried using a simple whitespace tokenizer, which resulted in similar performance when compared to the pretrained T5 tokenizer.\n\n40\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nT5\n\nModel\n\nT5\n\nTable 13: Breakdown of accuracy by example characteristics on cSCAN-B Random.\n\nKnownness\n\nRequest/Reply Type\n\nPretrain Size Defeasible Monotonic Unknown Neg. Rule Pos. Rule Non-Rule\n\nS B\nL\n\nS B\n\nS B\n\nS B\n\nB\n\n90.2 90.5 95.3\n\n13.9 15.0\n\n16.3 13.9\n\n17.1 16.1\n\n19.3\n\n93.9 93.9 97.7\n\n16.4 12.3\n\n15.9 13.1\n\n14.0 17.8\n\n23.5\n\n97.4 97.7 97.7\n\n75.0 74.4\n\n74.1 68.7\n\n73.8 80.6\n\n83.0\n\n79.8 78.8 91.9\n\n32.0 33.4\n\n28.8 29.5\n\n31.2 33.1\n\n40.6\n\n96.5 96.9 98.1\n\n27.7 20.5\n\n34.2 24.0\n\n29.5 32.6\n\n44.9\n\n95.8 96.2 97.6\n\n2.9 2.7\n\n3.0 2.2\n\n3.0 3.9\n\n5.8\n\nLongT5-TGlobal False\n\nLongT5\n\nFalse\n\nT5 w/o Context\n\nTrue\n\nTable 14: Breakdown of accuracy by example characteristics on cSCAN-X Random.\n\nKnownness\n\nRequest/Reply Type\n\nPretrain Size Defeasible Monotonic Unknown Neg. Rule Pos. Rule Non-Rule\n\nS B\nL\n\nS B\n\nS B\n\nS B\n\nB\n\n59.2 62.6 74.9\n\n10.3 11.2\n\n13.0 9.4\n\n11.7 12.3\n\n17.0\n\n68.7 71.6 83.7\n\n16.4 14.8\n\n14.6 18.2\n\n16.3 15.2\n\n20.6\n\n97.8 98.4 97.7\n\n63.0 72.7\n\n72.4 78.6\n\n70.8 57.8\n\n83.3\n\n43.3 46.5 69.5\n\n29.0 29.5\n\n28.1 28.1\n\n32.8 29.5\n\n35.5\n\n69.2 79.4 91.4\n\n25.1 22.6\n\n26.8 28.5\n\n23.5 25.4\n\n39.9\n\n75.2 74.8 80.6\n\n1.9 2.2\n\n2.7 2.7\n\n2.5 2.0\n\n4.0\n\nLongT5-TGlobal False\n\nLongT5\n\nFalse\n\nT5 w/o Context\n\nTrue\n\nTrue\n\nFalse\n\nTrue\n\nFalse\n\nM BREAKDOWN METRICS\n\nM.1 CSCAN-B RANDOM\n\nTable 13 shows breakdown of accuracy by example characteristics on cSCAN-B Random. From the breakdowns for T5-Large (770M parameters), we can see that for a sufficiently large model with full attention, it is feasible to achieve high accuracy across example classes. Across the pre-trained T5 models, however, we can notice several trends:\n\n• Examples with reply of “unknown” tend to be easier than other examples.\n\n• Negative rule examples, i.e., those with reply of “false”, are the most challenging class of example.\n\n• Accuracy on non-rule examples is roughly similar to accuracy on rule examples, despite the fact that rule examples involve classification into a much smaller space of possible replies (which would make them more amenable to solving through guessing).\n\nIn contrast, when looking at the results of the models that failed to outperform the naive baseline, we can see that for these, accuracy on rule examples is significantly higher than that on non-rule examples, with the highest accuracy on examples with the reply of “unknown”, consistent with the view that these models are relying on guessing based on superficial example characteristics.\n\n41\n\nUnder review as a conference paper at ICLR 2023\n\nTable 15: Breakdown of accuracy by example characteristics on cSCAN-B MCD.\n\nKnownness\n\nRequest/Reply Type\n\nPretrain Size Defeasible Monotonic Unknown Neg. Rule Pos. Rule Non-Rule\n\nS B\nL\n\nS B\n\nS B\n\nS B\n\nB\n\n38.2 38.8 55.3\n\n17.9 13.8\n\n18.4 13.1\n\n16.7 13.4\n\n25.2\n\n24.0 25.3 46.8\n\n14.3 14.7\n\n14.0 13.2\n\n15.0 14.0\n\n19.5\n\n99.8 99.9 99.8\n\n96.6 97.4\n\n98.5 97.4\n\n97.2 97.6\n\n96.6\n\n60.0 59.8 75.6\n\n25.8 26.7\n\n30.3 29.8\n\n36.8 42.1\n\n38.0\n\n39.7 35.3 60.4\n\n30.3 24.2\n\n25.7 16.7\n\n18.7 6.5\n\n40.2\n\n22.5 27.2 42.4\n\n8.9 6.2\n\n8.9 10.8\n\n10.3 6.7\n\n14.6\n\nModel\n\nT5\n\nTrue\n\nFalse\n\nLongT5-TGlobal False\n\nLongT5\n\nFalse\n\nT5 w/o Context\n\nTrue\n\nM.2 CSCAN-X RANDOM\n\nTable 14 shows breakdown of accuracy by example characteristics on cSCAN-X Random. From the pre-trained T5 models, we can see the following trends:\n\n• Examples with reply of “unknown” continue to be much easier than other examples. In fact, the models are able to answer these examples with comparably high accuracy on cSCAN-X as on cSCAN-B.\n\n• Negative rule examples continue to be the most challenging class of example.\n\n• While the models struggle with both rule examples and non-rule examples, accuracies are even lower on rule examples than on non-rule examples, again despite the fact that rule examples would be more amenable to random guessing.\n\nFor the models that failed to outperform the naive baseline, the pattern is similar on cSCAN-X as on cSCAN-B.\n\nM.3 CSCAN-B MCD\n\nTable 15 shows breakdown of accuracy by example characteristics on cSCAN-B MCD. From these results, we can see the following trends:\n\n• All models achieve particularly high accuracy on examples with reply “unknown”, as would be expected from the dataset stats shown in Table 2, where we can see that this class of examples makes up over 50% of the examples in the cSCAN-B MCD train set. This makes the answer of “unknown” a natural guess in any situation where the model is unsure.\n\n• Given that T5 w/o Context is able to achieve significantly higher than zero accuracy on rule examples with replies other than “unknown”, however, it is clear that the model is doing more than simply predicting “unknown” every time. Rather, it appears that a moderate amount of statistical clues must be available in the request itself to allow some degree of “educated guessing” of the reply, particularly in the case of rule examples.\n\n• For all models, accuracy on non-rule examples lags significantly behind accuracy on rule examples. This is in contrast to the cSCAN Random datasets, where the stronger-performing pre-trained models frequently performed better on non-rule examples than on rule examples. One reason for this difference is likely the fact that the train set for cSCAN-B MCD is skewed toward rule examples, which make up somewhat over 70% of the dataset. Taken in light of the observation above about the naive T5 w/o Context baseline, however, the poor performance on non-rule examples also suggests that the T5 baselines may be achieving even less proper “understanding” of the examples than one would have thought from looking at the overall accuracy numbers alone, and is likely relying to a large degree on “educated guessing”, based on statistical clues from the\n\n42\n\nUnder review as a conference paper at ICLR 2023\n\nTable 16: Breakdown of accuracy by example characteristics on cSCAN-X MCD.\n\nKnownness\n\nRequest/Reply Type\n\nPretrain Size Defeasible Monotonic Unknown Neg. Rule Pos. Rule Non-Rule\n\nModel\n\nT5\n\nTrue\n\nFalse\n\nLongT5-TGlobal False\n\nLongT5\n\nFalse\n\nT5 w/o Context\n\nTrue\n\nS B\nL\n\nS B\n\nS B\n\nS B\n\nB\n\n51.6 50.3 60.0\n\n13.3 13.5\n\n14.6 14.4\n\n13.2 14.7\n\n12.6\n\n57.1 54.2 67.4\n\n19.4 17.3\n\n17.3 17.8\n\n19.0 18.4\n\n21.2\n\n99.5 99.7 99.7\n\n96.5 94.8\n\n96.4 95.5\n\n94.9 95.1\n\n96.2\n\n65.0 14.3 36.2\n\n29.2 28.7\n\n28.1 30.6\n\n32.8 32.2\n\n27.1\n\n37.8 83.1 88.6\n\n28.0 24.4\n\n26.5 24.8\n\n23.5 24.8\n\n33.3\n\n66.6 66.0 73.4\n\n6.8 5.6\n\n6.4 6.5\n\n6.3 6.8\n\n8.7\n\nrequest and context, which is much easier to do on rule examples than on non-rule examples, due to the smaller space of possible replies for rule examples.\n\nM.4 CSCAN-X MCD\n\nTable 16 shows breakdown of accuracy by example characteristics on cSCAN-X MCD. From these results, we can see the following trends:\n\n• Similarly to cSCAN-B MCD, all models achieve high accuracy on examples with reply “unknown”, which is again the most commonly occurring class of examples in this dataset (around 40% of examples in the train set).\n\n• For pre-trained T5, however, accuracy on non-rule examples is significantly higher than on cSCAN-B MCD, suggesting that these models are likely benefiting from the more balanced distribution of examples in the cSCAN-X MCD dataset, where around 40% of the train examples are non-rule examples, compared with less than 30% in cSCAN-B MCD.\n\n• The large gap in accuracy between negative and possible rule examples on pre-trained T5 suggests that while they are able to use information from the context to do a better job than the naive T5 w/o Context at distinguishing between “unknown” and “not unknown” rules, they are still relying largely on guessing for determining the rules’ actual truth value.\n\nM.5 EFFECT OF EXAMPLE AND CONTEXT CHARACTERISTICS\n\nFigures 10, 11, 12, and 13 show T5’s performance on the test splits of cSCAN-B, cSCAN-X, cSCAN-B MCD, and cSCAN-X MCD datasets with respect to various features of the examples and the contexts, broken down into rule and non-rule examples.\n\nThe features being considered are:\n\n• num rules: The number of distinct rules used to create an example. For example, for the request\n\n”walk and walk”, num rules is 2: it is created with the rules:\n\nx1 and x2 (cid:74)\n\n(cid:75)\n\n=... and\n\n=...\n\nwalk (cid:75) (cid:74)\n\n• num variables: The number of variables in the rule. For example, for the request ”walk and x1”,\n\nnum variables is 1.\n\n• derivation level: The number of compositions used to build an example. For example, for the =...\n\nrequest ”walk and walk”, derivation level is 2: it is created by first composing with\n\nx1 and x2 (cid:74) =..., followed by another composition with walk (cid:75) (cid:74)\n\n• frac explicit rules bucket: The fraction of explicit rules among all distinct rules used to create an example. The fractions are bucketed for legibility: frac explicit rules bucket=0.5 includes all examples with a fraction of explicit rules at least 0.5 and less than 0.6.\n\nwalk and x2 (cid:74)\n\nwalk (cid:75) (cid:74)\n\n=... to get\n\n(cid:75) =....\n\n(cid:75)\n\n43\n\nUnder review as a conference paper at ICLR 2023\n\n• input length bucket: The length of the input (context + request) in tokens. The lengths are bucketed for legibility: input length bucket=500 includes all examples with length at least 500 and less than 600.\n\n• context num explicit rules: The number of rules explicitly asserted in the context. Every context is based on 14 grammar rules, so for example, context num explicit rules=5 means that the context contains explicit assertions of 5 of these rules, while the other 9 rules are either illustrated indirectly via examples (such that the learner is expected to induce the rule to be true) or are not illustrated sufficiently (such that the learner is expected to consider the rule to be either false or unknown).\n\nIn all cases the accuracy appears negatively correlated num rules and derivation level. See text below each figure for additional observations.\n\nN FINE-GRAINED EVALUATION METRICS\n\nIn addition to exact match accuracy, for more nuanced error analysis, we track several additional finergrained metrics, including partial accuracy metrics, edit distance, and counts of implications and contradictions related to the consistency metric.\n\nN.1 PARTIAL ACCURACY METRICS\n\nSequence level accuracy is a hard metric where a single wrong token leads the entire prediction to be labeled as wrong. For this reason we define additional accuracy measures that take into account partial success in solving the actual task. These measures are:\n\n• Reply Accuracy: A prediction is considered correct if the reply portion is correct.\n\n• Qualifier Accuracy: A prediction is considered correct if the qualifier portion is correct.\n\n• Pattern Accuracy: Pattern accuracy assigns each token an incremental ID based on the order it appears in the sequence, thus ignoring the specific predicted tokens and focusing on token variation pattern. E.g. the sequence JUMP JUMP RUN JUMP and WALK WALK EAT WALK both have the same pattern of A A B A where A replaces JUMP and WALK in the first and second sequences respectively, while B replaces RUN and EAT in the first and second sequence respectively.\n\n• Naive Accuracy: A prediction is considered correct if it produces the same set of unique tokens as the target regardless of the order or the count. E.g. the sequences JUMP JUMP RUN JUMP and RUN JUMP both have the same unique tokens set (JUMP and RUN).\n\n• Token Accuracy: Token-wise accuracy between the prediction and the target. The two sequences are aligned at the start token, and the shorter sequence is padded to have the same length as the longer sequence such that the padded tokens are considered wrong predictions.\n\nTables 17, 18, 20, and 20 show the performance of each baseline on all of these metrics.\n\n44\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: T5 accuracy on cSCAN-B examples. For non-rule examples, the accuracy appears positively correlated with the fraction of explicit rules (bottom-left).\n\nFigure 11: T5 accuracy on cSCAN-X examples. For non-rule examples, the accuracy appears positively correlated with the number of explicit examples in the context (bottom-right), and negatively correlated with the input length (bottom-center).\n\n45\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: T5 accuracy on cSCAN-B MCD examples.\n\nFigure 13: T5 accuracy on cSCAN-X MCD examples. For both rule and non-rule examples, the accuracy appears negatively correlated with the input length (bottom-center).\n\n46\n\nUnder review as a conference paper at ICLR 2023\n\nTable 17: Partial accuracy measures on cSCAN-B Random\n\nPartial Accuracy\n\nBaseline\n\nPretrain Size Accuracy Reply Qualifier Pattern Naive Token\n\nTrue T5 True T5 True T5 False T5 False T5 False LongT5 False LongT5 LongT5-TGlobal False LongT5-TGlobal False True T5 w/o Context\n\nS B\nL S\nB S\nB S\nB B\n\n92.5 92.6 96.5 19.3 17.9 19.5 21.4 20.0 17.2 26.8\n\n93.2 93.3 97.1 28.4 27.9 28.4 29.9 28.7 27.2 43.8\n\n99.2 99.1 99.2 61.1 55.0 62.2 65.6 62.9 54.5 54.8\n\n98.2 98.4 99.1 51.8 51.7 51.9 52.7 51.8 51.5 53.3\n\n93.2 93.3 96.8 38.9 18.5 33.2 39.2 43.2 18.0 26.9\n\n96.9 97.2 98.5 27.5 17.7 26.7 30.7 29.5 16.7 15.5\n\nTable 18: Partial accuracy measures on cSCAN-X Random\n\nPartial Accuracy\n\nBaseline\n\nPretrain Size Accuracy Reply Qualifier Pattern Naive Token\n\nTrue T5 True T5 True T5 False T5 False T5 False LongT5 LongT5 False LongT5-TGlobal False LongT5-TGlobal False True T5 w/o Context\n\nS B\nL S\nB S\nB S\nB B\n\n68.3 71.1 81.6 16.8 17.2 18.1 16.7 18.0 18.8 23.8\n\n70.1 72.7 83.6 26.8 27.1 27.7 26.6 27.6 28.2 38.7\n\n96.4 96.4 97.1 53.0 52.9 55.8 53.8 55.6 54.8 52.0\n\n88.5 88.1 91.2 51.0 51.1 51.3 51.1 51.4 51.3 52.1\n\n77.2 79.9 88.5 20.1 19.8 22.9 20.0 21.9 22.0 23.9\n\n85.7 85.0 90.2 10.5 8.6 11.4 9.4 11.1 8.8 4.4\n\nTable 19: Partial accuracy measures on cSCAN-B MCD\n\nPartial Accuracy\n\nBaseline\n\nPretrain Size Accuracy Reply Qualifier Pattern Naive Token\n\nTrue T5 True T5 True T5 False T5 False T5 False LongT5 LongT5 False LongT5-TGlobal False LongT5-TGlobal False True T5 w/o Context\n\nS B\nL S\nB S\nB S\nB B\n\n53.8 54.7 67.6 40.1 38.4 40.7 38.2 40.6 39.3 46.2\n\n62.0 62.9 76.7 47.7 45.7 47.6 45.2 48.0 45.9 60.0\n\n77.4 78.4 83.2 68.4 64.7 68.5 67.5 68.0 66.9 65.2\n\n73.8 76.0 82.7 67.4 66.0 68.2 66.8 67.6 68.0 69.4\n\n67.4 67.2 76.9 50.4 39.2 48.5 45.2 52.1 45.6 46.2\n\n47.0 51.0 65.4 31.8 24.7 31.0 29.6 33.6 30.2 22.9\n\n47\n\nUnder review as a conference paper at ICLR 2023\n\nTable 20: Partial accuracy measures on cSCAN-X MCD\n\nPartial Accuracy\n\nBaseline\n\nPretrain Size Accuracy Reply Qualifier Pattern Naive Token\n\nTrue T5 True T5 True T5 False T5 False T5 False LongT5 LongT5 False LongT5-TGlobal False LongT5-TGlobal False True T5 w/o Context\n\nS B\nL S\nB S\nB S\nB B\n\n70.7 69.2 76.8 41.6 40.0 40.9 41.2 41.0 40.9 42.7\n\n72.8 71.0 78.4 48.7 47.6 48.2 48.5 48.4 48.1 52.3\n\n95.5 95.8 97.0 69.5 66.5 68.1 68.0 67.7 67.8 64.4\n\n90.1 89.8 92.2 69.7 69.0 69.5 69.7 69.5 69.5 70.3\n\n78.3 76.9 82.9 51.4 42.4 49.9 46.1 49.2 44.8 42.8\n\n83.2 81.4 87.2 18.7 12.3 17.9 14.3 18.0 12.5 6.2\n\n48\n\nUnder review as a conference paper at ICLR 2023\n\nTable 21: Consistency breakdown for cSCAN-B Random\n\nBaseline\n\nPretrain Size Consistency Implications Contradictions\n\nTrue T5 True T5 True T5 False T5 False T5 False LongT5 LongT5 False LongT5-TGlobal False LongT5-TGlobal False True T5 w/o Context\n\nS B\nL S\nB S\nB S\nB B\n\n71.8 71.3 87.5 0.0 0.0 0.0 0.1 0.2 0.0 0.1\n\n431 437 446 0\n0 0\n1 3\n1 1\n\n169 176 64 881 2107 1419 689 1269 3128 774\n\nTable 22: Consistency breakdown for cSCAN-X Random\n\nBaseline\n\nPretrain Size Consistency Implications Contradictions\n\nTrue T5 True T5 True T5 False T5 False T5 False LongT5 LongT5 False LongT5-TGlobal False LongT5-TGlobal False True T5 w/o Context\n\nS B\nL S\nB S\nB S\nB B\n\n30.1 32.3 43.5 0.1 0.0 0.1 0.1 0.2 0.1 0.4\n\n237 268 335 4\n2 2\n4 3\n3 3\n\n550 561 435 3967 5375 1791 3678 1651 3002 822\n\nN.2 CONSISTENCY BREAKDOWN BY IMPLICATIONS AND CONTRADICTIONS\n\nIn this section we show the number of implications and contradictions used to calculate the consistency metric. Tables 21 and 22 show this breakdown.\n\nTo further illustrate the type of inconsistencies the model is making, we show the consistency sets (implications and contradictions) size distribution in Figures 14 and 15. This shows that sets of size 2 where one prediction implies or contradicts another are the most common. Appendix P.2 expands on this by providing examples of the contradictions.\n\nAs can be seen in these tables, the consistency metric for each cSCAN Random experiment is calculated based on a minimum of 500 implications and contradictions.\n\nAs discussed in Section 6.2 and Appendix F, we do not report consistency metrics for the MCD datasets, as we are only able to achieve a high enough density of potential implications in the Random datasets, where after splitting, we augment each context with additional top-level examples from the same distribution. We do not perform this additional example generation step for the MCD datasets to avoid impacting the compound divergences between the train and test sets.\n\n49\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 14: Consistency sets size distribution for cSCAN-B Random\n\n50\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 15: Consistency sets size distribution for cSCAN-X Random\n\n51\n\nUnder review as a conference paper at ICLR 2023\n\nTable 23: Edit distance measures for cSCAN-B Random\n\nBaseline\n\nPretrain Size Distance Substitutions\n\nInsertions Deletions\n\nTrue T5 True T5 True T5 False T5 False T5 False LongT5 LongT5 False LongT5-TGlobal False LongT5-TGlobal False True T5 w/o Context\n\nS B\nL S\nB S\nB S\nB B\n\n0.19 0.17 0.09 4.62 6.00 4.75 4.44 4.30 6.39 6.19\n\n0.07 0.07 0.03 1.96 3.22 2.18 1.93 1.86 3.23 2.89\n\n0.07 0.03 0.01 1.30 1.30 1.23 1.09 1.18 1.29 2.22\n\n0.05 0.07 0.04 1.35 1.48 1.34 1.42 1.26 1.87 1.08\n\nTable 24: Edit distance measures for cSCAN-X Random\n\nBaseline\n\nPretrain Size Distance Substitutions\n\nInsertions Deletions\n\nTrue T5 True T5 True T5 False T5 False T5 False LongT5 LongT5 False LongT5-TGlobal False LongT5-TGlobal False True T5 w/o Context\n\nS B\nL S\nB S\nB S\nB B\n\n3.01 3.21 2.04 37.87 48.64 38.81 46.49 36.74 46.59 44.81\n\n0.27 0.30 0.15 12.41 12.73 11.65 13.13 11.59 12.54 14.17\n\n1.40 0.65 0.57 5.77 5.18 5.30 4.21 6.11 5.23 8.37\n\n1.33 2.26 1.32 19.69 30.73 21.87 29.15 19.04 28.82 22.27\n\nTable 25: Edit distance measures for cSCAN-B MCD\n\nBaseline\n\nPretrain Size Distance Substitutions\n\nInsertions Deletions\n\nTrue T5 True T5 True T5 False T5 False T5 False LongT5 False LongT5 LongT5-TGlobal False LongT5-TGlobal False True T5 w/o Context\n\nS B\nL S\nB S\nB S\nB B\n\n1.96 1.72 1.23 3.41 4.29 3.39 3.64 3.18 3.50 4.46\n\n0.69 0.60 0.42 1.26 2.12 1.32 1.54 1.29 1.28 1.90\n\n0.82 0.90 0.63 1.28 1.07 1.47 1.23 1.15 1.47 1.77\n\n0.44 0.22 0.18 0.86 1.10 0.60 0.87 0.74 0.75 0.79\n\nN.3 EDIT DISTANCE\n\nEdit distance is the number of edits that would need to be applied to the predicted sequence to transform it to the target sequence. There are three types of edits: Substitutions (S), Insertions (I) and Deletions (D).\n\nTables 23, 24, 26, and 26 show the edit distance and constituent metrics of each baseline on all of these metrics.\n\n52\n\nUnder review as a conference paper at ICLR 2023\n\nTable 26: Edit distance measures for cSCAN-X MCD\n\nBaseline\n\nPretrain Size Distance Substitutions\n\nInsertions Deletions\n\nTrue T5 True T5 True T5 False T5 False T5 False LongT5 LongT5 False LongT5-TGlobal False LongT5-TGlobal False True T5 w/o Context\n\nS B\nL S\nB S\nB S\nB B\n\n2.60 2.90 1.96 24.02 26.84 24.31 29.05 24.21 29.79 30.25\n\n0.26 0.33 0.22 5.06 7.22 5.58 7.02 5.37 7.90 9.72\n\n1.63 0.79 0.65 4.30 5.50 3.91 3.76 4.37 3.86 6.49\n\n0.70 1.78 1.09 14.66 14.13 14.83 18.26 14.47 18.03 14.04\n\n53\n\nUnder review as a conference paper at ICLR 2023\n\nO INPUT-OUTPUT FORMAT\n\nWe evaluate T5 variants, which are all encoder-decoder architectures. Here, we show how we prepare the input (that is fed to the encoder) and output (that the decoder generates) of our models, given an example from cSCAN. The example below is selected from the cSCAN-B dataset. Note that the request and the context examples are all concatenated into a single newline-separated string to form the input, and the bullet-points here (and in Appendix P.1) are just added to improve the readability of the examples.\n\nInput: <Request, Context>\n\nturn opposite left and jump around left twice\n\n•[left] = PURPLE •[x1 around x2] = [x2] [x2] [x1] [x1] •[x1 opposite x2] = [x1] [x2] [x2] •[turn] = YELLOW •[x1 after x2] = [x2] [x1] •[walk] = LTURN •[jump] = RUN •run around left and turn opposite right twice PURPLE PURPLE JUMP JUMP YELLOW RTURN RTURN YELLOW RTURN RTURN YELLOW RTURN RTURN •walk opposite left twice after run thrice JUMP RUN LTURN PURPLE PURPLE LTURN PURPLE PURPLE •[run opposite left] = JUMP PURPLE PURPLE •[run opposite x1 twice] = JUMP [x1] [x1] •turn opposite right twice and turn around left thrice YELLOW RTURN RTURN YELLOW RTURN RTURN YELLOW RTURN RTURN PURPLE PURPLE YELLOW YELLOW •run right thrice JUMP RTURN RTURN •[x1 around x2 thrice] = [x2] [x2] [x1] [x1] •[look around x1 thrice] = [x1] [x1] PINK PINK •look around right thrice and run opposite left RTURN RTURN PINK PINK JUMP PURPLE PURPLE •look around left thrice PURPLE PURPLE PINK PINK •[look left thrice] = PINK PURPLE PURPLE •[look opposite x1] = PINK [x1] [x1] •run opposite right twice and turn around left twice JUMP RTURN RTURN PURPLE PURPLE YELLOW YELLOW •turn around left and walk twice PURPLE PURPLE YELLOW YELLOW LTURN LTURN JUMP •[x1 and x2 thrice] = [x1] [x2] •[x1 thrice and x2] = [x1] [x2] •look thrice and run around right thrice PINK RTURN RTURN JUMP JUMP •turn opposite right YELLOW RTURN RTURN •[jump right] = RUN RTURN RTURN •[x1 right] = [x1] RTURN RTURN •turn left after look opposite right PINK RTURN RTURN YELLOW PURPLE PURPLE •look right twice YELLOW PINK RTURN RTURN PINK RTURN RTURN •[look x1] = PINK [x1] [x1] •[x1 left] = [x1] PURPLE PURPLE •walk around left twice afterz walk around left thrice PURPLE PURPLE LTURN LTURN PURPLE PURPLE LTURN LTURN PURPLE PURPLE LTURN LTURN YELLOW •jump right twice RUN RTURN RTURN RUN RTURN RTURN RTURN •[look around x1 twice] = RUN [x1] [x1] PINK PINK [x1] [x1] PINK PINK •[jump around left twice] = PINK PURPLE PURPLE RUN RUN PURPLE PURPLE RUN RUN\n\nOutput: <Reply, Qualifier>\n\nYELLOW PURPLE PURPLE PINK PURPLE PURPLE RUN RUN PURPLE PURPLE RUN RUN (Reason-\n\ning: Defeasible)\n\nNote that we concatenate request + context, rather than context + request, so as to make the system more robust to truncation of the example string, if any example were to exceed the maximum length of T5’s input buffer (although in our experiments we made sure that the example lengths did not exceed this buffer size).\n\nIn representing the context for T5, it can be noted that we omitted special syntactic tokens such as braces, angle brackets, and commas wherever possible, so as to reduce the token count and keep the format closer to natural language, to the extent that this could be done without introducing ambiguity. For context examples\n\n54\n\nUnder review as a conference paper at ICLR 2023\n\nthat represent rule assertions, we also adopted a simplified syntax, similar to the shorthand described in Appendix C.2, consisting of a single line containing the rule request alone, while omitting the reply, since in cSCAN we only include positive rule assertions in the context (i.e., never rule examples with reply of 0 or ?).\n\nNote also that we do not perform any clustering of top-level examples by their context, but rather represent each top-level example in flattened form as shown here (with its context included). We then shuffle the full set of top-level examples before batching them for input into T5. This means that even when there may be 100 or more top-level examples with the same context, T5 will in general not see them all in sequence or in the same batch, but rather intermixed with top-level examples with different contexts.\n\nP QUALITATIVE ERROR ANALYSIS\n\nP.1 ACCURACY ERROR ANALYSIS\n\nHere, we showcase examples where our best model (T5-Large with pretraining) fails at producing the accurate results when evaluated on an example from the cSCAN-B. We show different cases where the target and the prediction from the model are different.\n\nInput\n\n[run left after turn right] = LOOK WALK WHITE WHITE PINK LOOK WALK\n\nExample of T5-Large failing at rule assertion (when the request is False).\n\n•[left] = PINK •[turn] = LOOK •[x1 twice] = [x1] [x1] •[x1 thrice] = [x1] •[x1 and x2] = [x1] [x2] [x1] •[look] = BLUE •[x1 opposite x2] = [x2] [x1] [x1] [x2] •[x1 x2] = [x1] [x2] •[walk] = RTURN •[x1 after x2] = [x2] [x1] [x1] [x2] •[jump] = GREEN •[right] = WALK •[run] = WHITE •walk around right and turn around left thrice RTURN WALK RTURN LOOK PINK LOOK RTURN WALK RTURN •jump opposite right after look around left PINK BLUE PINK WALK GREEN GREEN WALK WALK GREEN GREEN WALK PINK BLUE PINK •[jump around left] = GREEN GREEN •[x1 around right twice] = WALK [x1] WALK WALK [x1] WALK\n\ntarget\n\n0 (Reasoning: Monotonic)\n\nprediction\n\n1 (Reasoning: Monotonic)\n\nCommentary\n\nSince [x1 x2] = [x1] [x2], [run left] and [turn right] would map to WHITE PINK and LOOK WALK respectively, and with [x1 after x2] = [x2] [x1] [x1] [x2], the correct output should be LOOK WALK WHITE PINK WHITE PINK LOOK WALK.\n\n55\n\nUnder review as a conference paper at ICLR 2023\n\nInput\n\n[run and run around right thrice] = PURPLE\n\nExample of T5-Large failing at rule assertion (when the request is True).\n\n•[jump] = RED •[x1 and x2] = [x2] [x1] [x2] •[left] = LOOK •[run] = PURPLE •[x1 x2] = [x2] [x2] [x1] •[x1 around x2] = [x1] [x2] [x1] •[look] = RUN •[x1 after x2] = [x2] [x2] [x1] •[turn] = JUMP •[walk] = LTURN •[x1 twice] = [x1] [x1] •walk opposite right GREEN LTURN •look opposite right twice and jump twice RED RED GREEN RUN GREEN RUN RED RED •[turn opposite right] = GREEN JUMP •[look opposite x1] = [x1] RUN •look right and jump around right thrice RED GREEN RED RED GREEN RED GREEN GREEN RUN RED GREEN RED RED GREEN RED •turn opposite left twice and look thrice LOOK JUMP LOOK JUMP •[x1 opposite right thrice] = GREEN [x1] GREEN [x1] •[run around x1 thrice] = EMPTY STRING •run around right after run opposite right twice GREEN PURPLE GREEN PURPLE GREEN PURPLE GREEN PURPLE PURPLE GREEN PURPLE •jump opposite left after turn opposite right twice GREEN JUMP GREEN JUMP GREEN JUMP GREEN JUMP LOOK RED •[run around right] = PURPLE GREEN PURPLE •[x1 opposite right] = GREEN [x1]\n\ntarget\n\n1 (Reasoning: Monotonic)\n\nprediction\n\n0 (Reasoning: Monotonic)\n\nCommentary\n\nSince run around x1 thrice maps to an empty string and [x1 and x2] = [x2] [x1] [x2] with [run] = PURPLE, the provided rule is True.\n\n56\n\nUnder review as a conference paper at ICLR 2023\n\nInput\n\nturn around right thrice and jump opposite left thrice\n\nExample of T5-Large failing at acknowledging lack of information to reply.\n\n•[x1 and x2] = [x1] [x2] [x1] •[x1 around x2] = [x1] [x2] [x2] •[x1 opposite x2] = [x1] [x2] [x2] •[x1 twice] = [x1] •[x1 after x2] = [x2] [x1] [x2] •[left] = BLUE •[x1 thrice] = [x1] [x1] •[look] = PINK •[run] = RED •walk opposite right after run opposite left twice RED BLUE BLUE WHITE YELLOW YELLOW RED BLUE BLUE •walk opposite left twice and run WHITE BLUE BLUE RED WHITE BLUE BLUE •[walk opposite x1] = WHITE [x1] [x1] •[walk x1] = [x1] WHITE •jump opposite left thrice and turn left thrice RTURN BLUE BLUE RTURN BLUE BLUE BLUE RUN BLUE RUN RTURN BLUE BLUE RTURN BLUE BLUE •turn opposite right twice after look opposite right PINK YELLOW YELLOW RUN YELLOW YELLOW PINK YELLOW YELLOW •[turn left] = BLUE RUN •[turn around x1 twice] = RUN [x1] [x1] •jump left twice after turn around left RUN BLUE BLUE BLUE LTURN RUN BLUE BLUE •jump around right thrice WALK WHITE YELLOW YELLOW WALK WHITE YELLOW YELLOW •[jump right] = YELLOW WALK WALK •[jump opposite left] = YELLOW BLUE BLUE •walk right twice and walk around left thrice YELLOW WHITE WHITE BLUE BLUE WHITE BLUE BLUE YELLOW WHITE •turn around right twice after jump around left WALK WALK BLUE BLUE RUN YELLOW YELLOW WALK WALK BLUE BLUE •[jump opposite right] = RED WALK YELLOW YELLOW •[turn opposite right] = RUN YELLOW YELLOW •jump right twice and jump thrice YELLOW BLACK BLACK BLACK YELLOW BLACK •run right thrice after look around left twice PINK BLUE BLUE YELLOW RED YELLOW RED PINK BLUE BLUE •[x1 left] = BLUE [x1] •[run x1] = [x1] RED\n\ntarget\n\n? (Reasoning: Defeasible)\n\nprediction\n\nRUN YELLOW YELLOW RUN YELLOW YELLOW YELLOW BLUE BLUE YELLOW BLUE BLUE RUN YELLOW YELLOW RUN YELLOW YELLOW (Reasoning: Defeasible)\n\nCommentary\n\n[jump] = therefore the mapping of [jump] is unknown.\n\nis not well illustrated by at least 2 unique substitutions,\n\n57\n\nUnder review as a conference paper at ICLR 2023\n\nExample of T5-Large failing at drawing correct information to compose the reply.\n\nInput\n\nturn opposite left after turn around left thrice\n\n•[x1 and x2] = [x1] [x2] •[jump] = RTURN •[x1 opposite x2] = [x1] [x2] [x2] [x1] •[left] = YELLOW •[x1 twice] = [x1] •[turn] = RED •[right] = WHITE •[x1 thrice] = [x1] [x1] •[walk] = GREEN •[x1 after x2] = [x1] [x1] [x2] [x2] •[run] = JUMP •[x1 around x2] = [x2] [x1] [x2] [x1] •look around left and walk twice YELLOW LOOK YELLOW LOOK GREEN •look opposite left thrice WALK WALK YELLOW YELLOW WALK WALK WALK WALK YELLOW YELLOW WALK WALK •[look opposite left thrice] = WALK WALK YELLOW YELLOW WALK WALK WALK WALK YELLOW YELLOW WALK WALK •[look right] = WHITE WHITE RTURN RTURN •walk right twice WHITE WHITE GREEN GREEN •run left thrice and run YELLOW YELLOW JUMP JUMP YELLOW YELLOW JUMP JUMP JUMP •[x1 left] = YELLOW YELLOW [x1] [x1] •[turn x1] = [x1] [x1] RED RED\n\ntarget\n\nRED YELLOW YELLOW RED RED YELLOW YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED (Reasoning: Monotonic)\n\nprediction\n\nRED YELLOW YELLOW RED RED YELLOW YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED (Reasoning: Monotonic)\n\nCommentary\n\n•Composing [x1 around x2] = [x2] [x1] [x2] [x1] into [x1 thrice] = [x1] [x1] gives [x1 around x2 thrice] = [x2] [x1] [x2] [x1] [x2] [x1] [x2] [x1]. •Composing the above rule and [x1 opposite x2] = x1 x2 x2 x1 into x1 and x2 of [x1 after x2] = [x1] [x1] [x2] [x2] respectively results in the rule [x1 opposite x2 after x3 around x4 thrice] = [x1] [x2] [x2] [x1] [x1] [x2] [x2] [x1] [x4] [x3] [x4] [x3] [x4] [x3] [x4] [x3] [x4] [x3] [x4] [x3] [x4] [x3] [x4] [x3]. •Substituting x1 and x3 by [turn] = RED and x2 and x4 by [left] = YELLOW results in the rule [turn opposite left after turn around left thrice] = RED YELLOW YELLOW RED RED YELLOW YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED YELLOW RED. Which varies from the predicted reply by the underlined tokens.\n\n58\n\nUnder review as a conference paper at ICLR 2023\n\nExample of T5-Large failing at inferring the type of reasoning, while the reply is correct.\n\nInput\n\n[x1 around left and x2 opposite right twice] = LOOK LOOK [x2] [x1] [x1]\n\nGREEN [x1] [x1] BLUE LOOK LOOK [x2] •[x1 after x2] = [x1] [x2] [x1] •[x1 thrice] = [x1] [x1] •[x1 around x2] = [x1] [x1] [x2] •[walk] = LTURN •[right] = LOOK •[left] = GREEN •[x1 opposite x2] = [x2] [x2] [x1] •[look] = WALK •[x1 twice] = [x1] •look twice and jump right WALK BLUE LOOK WALK WALK WALK BLUE LOOK •turn twice and walk left GREEN LTURN RUN RUN GREEN LTURN •[jump left] = BLUE BLUE GREEN •[run left] = GREEN BLACK •run around left twice BLACK BLACK GREEN •turn around left and run opposite right LOOK LOOK BLACK RUN RUN GREEN RUN RUN GREEN LOOK LOOK BLACK •[run around x1 thrice] = BLACK BLACK [x1] BLACK BLACK [x1] •[run opposite x1] = [x1] [x1] BLACK •walk right thrice after turn opposite left thrice LTURN LTURN LOOK LTURN LTURN LOOK GREEN GREEN RUN GREEN GREEN RUN LTURN LTURN LOOK LTURN LTURN LOOK •turn around right twice RUN RUN LOOK •[turn around x1] = RUN RUN [x1] •[turn opposite left] = GREEN GREEN RUN •run opposite left thrice and look WALK GREEN GREEN BLACK GREEN GREEN BLACK GREEN GREEN BLACK GREEN GREEN BLACK WALK •run around right and look around right twice WALK WALK LOOK BLACK BLACK LOOK BLACK BLACK LOOK WALK WALK LOOK •[x1 and x2 thrice] = [x2] [x2] [x1] [x1] [x2] [x2] •[x1 and x2 opposite x3] = [x3] [x3] [x2] [x1] [x1] [x3] [x3] [x2] •jump opposite right thrice after walk around left LOOK LOOK BLUE LOOK LOOK BLUE LTURN LTURN GREEN LOOK LOOK BLUE LOOK LOOK BLUE •jump around right thrice and turn thrice RUN RUN BLUE BLUE LOOK BLUE BLUE LOOK BLUE BLUE LOOK BLUE BLUE LOOK RUN RUN •[jump opposite right] = LOOK LOOK BLUE •[jump opposite x1 twice] = [x1] [x1] BLUE\n\ntarget\n\n0 (Reasoning: Defeasible)\n\nprediction\n\n0 (Reasoning: Monotonic)\n\nCommentary\n\n•The rule [x1 and x2] = [x2] [x1] [x1] [x2] is hidden and can be induced from the context examples highlighted by an underline. •Composing [x1 opposite x2] = [x2] [x2] [x1] into [x1 twice] = [x1] results into the rule [x1 opposite x2 twice] = [x2] [x2] [x1] •Composing [x1 around x2] = [x1] [x1] [x2] and [x1 opposite x2 twice] = [x2] [x2] [x1] into x1 and x2 of [x1 and x2] = [x2] [x1] [x1] [x2] respectively results in the rule [x1 around x2 and x3 opposite x4 twice] = [x4] [x4] [x3] [x1] [x1] [x2] [x1] [x1] [x2] [x4] [x4] [x3] •Substituting x2 and x4 by [left] = GREEN and [right] = LOOK respectively results in the rule [x1 around left and x2 opposite right twice] = LOOK LOOK [x3] [x1] [x1] GREEN [x1] [x1] GREEN LOOK LOOK [x3].\n\n59\n\nUnder review as a conference paper at ICLR 2023\n\nP.2 CONSISTENCY ERROR ANALYSIS\n\nAll the above examples show inaccurate output, where there is a mismatch between the expected target and model prediction. However, as discussed in Section 4, regardless of being accurate, a model may fail to stay consistent in replying to different requests. In this section we sample contradictions made by T5-Large on cSCAN-X Random and categorize the type of contradictions the model makes. Note that consistency is independent of the context and therefore it’s omitted when presenting the examples.\n\nAs seen in Appendix N.2, the models produce contradictory sets at different sizes. We find it easy to analyse each size independently as the type of mistakes vary between them.\n\nContradictions of size 2 At this level, the inconsistencies are 1:1 relationships between two contradictory predictions P1 and P2. There are two types at this level\n\n• Type 1: Both P1 and P2 are asserted rules with matching right-hand sides and and different\n\nright-hand sides.\n\n• Type 2: One prediction is an asserted rule with no variables and the other is a rule application prediction with a request matching the right-hand side of the rule request while the reply does not match the rule request’s right-hand side.\n\nFrom a sample of 20 contradictory sets of size 2, 11 sets were of type 1 while 9 were found to be of type 2. Here are examples of both types\n\nContradictory set of size 2: Type 1\n\nRequest 1\n\n[drive left 5x framing x1 x2] = [x1] [x2] [x1] [x1] [x2] [x2] [x1] [x2] [x1] [x1] [x2] [x2] DTURN LOOK DTURN DTURN LOOK LOOK DTURN LOOK DTURN DTURN LOOK LOOK [x1] [x2] [x1] [x1] [x2] [x2] [x1] [x2] [x1] [x1] [x2] [x2] [x1] [x2] [x1] [x1] [x2] [x2] [x1] [x2] [x1] [x1] [x2] [x2]\n\nReply 1\n\n1 (Reasoning: Monotonic)\n\nRequest 2\n\n[drive left 5x framing x1 x2] = [x1] [x2] [x1] [x1] [x2] [x2] [x1] [x1] [x1] [x1] [x2] [x2] DTURN LOOK DTURN DTURN LOOK LOOK DTURN LOOK DTURN DTURN LOOK LOOK [x1] [x2] [x2] [x1] [x2] [x2] [x1] [x2] [x1] [x1] [x2] [x2] [x1] [x2] [x1] [x1] [x2] [x2] [x1] [x2] [x1] [x1] [x2] [x2]\n\nReply 2\n\n1 (Reasoning: Monotonic)\n\nCommentary\n\nThe two right-hand sides are different in the underlined tokens.\n\nContradictory set of size 2: Type 2\n\nRequest 1\n\n[run zigzag left after run fast around left] = PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK RUN PEEK PEEK PEEK PEEK PEEK PEEK PEEK RUN RUN RUN PEEK RUN RUN RUN PEEK RUN RUN RUN PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK RUN PEEK PEEK PEEK PEEK PEEK PEEK\n\nReply 1\n\n1 (Reasoning: Defeasible)\n\nRequest 2\n\nrun zigzag left after run fast around left\n\nReply 2\n\nPEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK RUN PEEK PEEK PEEK PEEK PEEK PEEK PEEK RUN RUN RUN PEEK RUN RUN RUN PEEK RUN RUN RUN PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK PEEK RUN PEEK PEEK PEEK PEEK PEEK PEEK (Reasoning: Defeasible)\n\nCommentary\n\nReply 2 is different from the right-hand side of Request 1 by one additional token (highlighted with an underline).\n\nContradictions of size 3 At this level, the inconsistencies are 2:1 relationships with at least two rule assertions.\n\n• Type 1: The composition of two rules directly contradicts a third rule.\n\n60\n\nUnder review as a conference paper at ICLR 2023\n\n• Type 2: The composition of two rules forms a rule with no variables where it’s right-hand side matches the request of a rule application prediction and it’s right-hand side contradicts the reply of that prediction.\n\nFrom a sample of 20 contradictory sets of size 3, both Type 1 and Type 2 had 10 sets each. Here are examples of both types\n\nRequest 1\n\n[x1 cautiously x2] = [x1] [x1] [x1] [x2] [x2] [x2] [x1] [x1] [x1] [x2] [x2]\n\nContradictory set of size 3: Type 1\n\nReply 1\n\n1 (Reasoning: Monotonic)\n\nRequest 2\n\n[walk] = RIDE RIDE\n\nReply 2\n\n1 (Reasoning: Monotonic)\n\nRequest 3\n\n[walk cautiously x1] = RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE [x1] [x1]\n\nReply 3\n\n1 (Reasoning: Monotonic)\n\nCommentary\n\nComposing Rule 2 into Rule 1 would result in the rule •[walk cautiously x1] = RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] Which is different from Rule 3 in places highlighted by an underline.\n\nRequest 1\n\n[run opposite up thrice following x1] = LOOK LOOK RUN LOOK LOOK LOOK RUN LOOK LOOK LOOK RUN LOOK [x1] LOOK LOOK RUN LOOK [x1] LOOK LOOK RUN LOOK [x1]\n\nContradictory set of size 3: Type 2\n\nReply 1\n\n1 (Reasoning: Monotonic)\n\nRequest 2\n\nrun opposite up thrice following run zigzag\n\nReply 2\n\nLOOK LOOK RUN LOOK LOOK LOOK RUN LOOK LOOK LOOK RUN LOOK LOOK LOOK RUN LOOK LOOK LOOK LOOK RUN LOOK LOOK LOOK RUN LOOK LOOK LOOK RUN LOOK LOOK LOOK LOOK RUN LOOK LOOK LOOK RUN LOOK LOOK LOOK RUN LOOK LOOK (Reasoning: Monotonic)\n\nRequest 3\n\n[run zigzag] = LOOK\n\nReply 3\n\n1 (Reasoning: Monotonic)\n\nCommentary\n\nComposing Rule 2 into Rule 3 would result in the rule •[run opposite up thrice following run zigzag] = LOOK LOOK RUN LOOK LOOK LOOK RUN LOOK LOOK LOOK RUN LOOK LOOK LOOK LOOK RUN LOOK LOOK LOOK LOOK RUN LOOK LOOK Which matches Request 2 in its right-hand side but is different from Reply 2 by its right-hand side in places highlighted by an underline.\n\nContradictions of size 4 At this level there are more complex contradictory relations as the space of possible compositions grows. we categorise 3 types of contradictions\n\n• Type 1: Composing 3 rules to form a rule with no variables that contradicts a rule application\n\nprediction.\n\n• Type 2: The composition of a subset set of rules contradicts the composition of another set of\n\nrules.\n\n• Type 3: The composition of 3 rules directly contradicts another rule.\n\nFor this size, the model only made 10 contradictions: 5 were of type 2, 3 were of type 3 and 2 were of type 1. Here are examples for each type\n\n61\n\nUnder review as a conference paper at ICLR 2023\n\nContradictory set of size 4: Type 1\n\nRequest 1\n\n[run drunkenly down] = RTURN PEEK PEEK PEEK PEEK PEEK\n\nReply 1\n\n1 (Reasoning: Monotonic)\n\nRequest 2\n\njump drunkenly twice after run drunkenly down\n\nReply 2\n\nRIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RTURN PEEK PEEK PEEK PEEK PEEK (Reasoning: Monotonic)\n\nRequest 3\n\n[x1 twice after x2] = [x1] [x1] [x1] [x1] [x1] [x1] [x1] [x1] [x1] [x1] [x1] [x1] [x2]\n\nReply 3\n\n1 (Reasoning: Monotonic)\n\nRequest 4\n\n[jump drunkenly] = RIDE RIDE RIDE RIDE RIDE RIDE\n\nReply 4\n\n1 (Reasoning: Monotonic)\n\nCommentary\n\nComposing rule 1 into x2 of rule 3 and rule 4 into x1 of rule 3 would result in the rule •[jump drunkenly twice after run drunkenly down] = RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RTURN PEEK PEEK PEEK PEEK PEEK Which matches Request 2 in its right-hand side but is different from Reply 2 by its right-hand side in places highlighted by an underline.\n\n62\n\nUnder review as a conference paper at ICLR 2023\n\nContradictory set of size 4: Type 2\n\nRequest 1\n\n[x1 cautiously x2 5x] = [x1] [x1] [x1] [x2] [x2] [x2] [x1] [x1] [x1] [x2] [x2] [x1] [x1] [x1] [x2] [x2] [x2] [x1] [x1] [x1] [x2] [x2] [x1] [x1] [x1] [x2] [x2] [x2] [x1] [x1] [x1] [x2] [x2] [x1] [x1] [x1] [x2] [x2] [x2] [x1] [x1] [x1] [x2] [x2] [x1] [x1] [x1] [x2] [x2] [x2] [x1] [x1] [x1] [x2] [x2] [x1] [x1] [x1] [x2] [x2] [x2] [x1] [x1] [x1] [x2] [x2]\n\nReply 1\n\n1 (Reasoning: Monotonic)\n\nRequest 2\n\n[walk cautiously x1] = RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE [x1] [x1]\n\nReply 2\n\n1 (Reasoning: Monotonic)\n\nRequest 3\n\n[x1 5x] = [x1] [x1] [x1] [x1] [x1] [x1]\n\nReply 3\n\n1 (Reasoning: Defeasible)\n\nRequest 4\n\n[walk] = RIDE RIDE\n\nReply 4\n\n1 (Reasoning: Monotonic)\n\nCommentary\n\nComposing rule 4 into x1 of rule 1 results in the rule •Rule 5 = [walk cautiously x1 5x] = RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] [x1] And composing rule 2 into rule 3 results in the rule •Rule 6 = [walk cautiously x1 5x] = RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE [x1] [x1] RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE [x1] [x1] RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE [x1] [x1] RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE [x1] [x1] RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE [x1] [x1] RIDE RIDE RIDE [x1] [x1] [x1] RIDE RIDE RIDE [x1] [x1] Which directly contradicts rule 5 in the places highlighted by an underline.\n\n63\n\nUnder review as a conference paper at ICLR 2023\n\nRequest 1\n\n[x1 between fly cautiously 90 x2] = [x1] [x1] WALK WALK WALK WALK WALK WALK [x2] WALK WALK WALK WALK WALK WALK WALK WALK [x2] [x1]\n\nContradictory set of size 4: Type 3\n\nReply 1\n\n1 (Reasoning: Monotonic)\n\nRequest 2\n\n[walk] = RIDE RIDE\n\nReply 2\n\n1 (Reasoning: Monotonic)\n\nRequest 3\n\n[x1 cautiously 90 x2 3x] = [x1] [x1] [x1] [x1] [x1] [x1] [x2] [x1] [x1] [x1] [x1] [x1] [x1] [x1] [x1] [x1] [x2]\n\nReply 3\n\n1 (Reasoning: Defeasible)\n\nRequest 4\n\n[walk cautiously 90 x1 3x between fly cautiously 90 x2] = RIDE RIDE RIDE RIDE RIDE RIDE [x1] RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE [x1] RIDE RIDE RIDE RIDE RIDE RIDE [x1] RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE [x1] WALK WALK WALK WALK WALK WALK [x2] WALK WALK WALK WALK WALK WALK WALK WALK WALK [x2] RIDE RIDE RIDE RIDE RIDE RIDE [x1] RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE [x1]\n\nReply 4\n\n1 (Reasoning: Monotonic)\n\nCommentary\n\nComposing rule 2 into x1 of rule 3 results in the rule •Rule 5 = [walk cautiously 90 x1 3x] = RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE [x1] RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE [x1] And composing rule 5 into x1 of rule 1 results in the rule •Rule 6 = [walk cautiously 90 x1 3x between fly cautiously 90 x2] = RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE [x1] RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE [x1] RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE [x1] RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE [x1] WALK WALK WALK WALK WALK WALK [x2] WALK WALK WALK WALK WALK WALK WALK WALK [x2] RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE [x1] RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE RIDE [x1] Which directly contradicts rule 4 in the places highlighted by an underline.\n\n64",
    "reference": "# Summary Of The Paper\n\nA new approach is introduced for constructing benchmarks for the application of consistent rules in language models. This approach is used to create a new dataset that is then used to evaluate T5 models on 3 dimensions: applying learned rules, scaling to larger sets of rules and compositional generalization.\n\n# Strength And Weaknesses\n\n**Strengths**: The idea is novel and it explores an important direction. See more details in the question below. The paper presents an idea, concrete usage of it, and empirical analysis on metrics of accuracy and consistency. The dataset section presents the data in sufficient detail, and the baselines are also described with a good level of detail. Exploring more baselines beyond T5 though would strengthen the paper. \n\n**Weaknesses**:\n* **Presentation of the motivation, real-world case**: The motivating example about the movie recommendations used to open the paper is not convincing in my opinion.  First, in general movie recommendation is more of a typical case of example-based learning, where people too give recommendations not by strict rules but by an approximate sense of what prior movies a person like, and what are his general preferences. More specifically, all except one of the rules presented in the example to illustrate rule-based inference are actually not rules but either facts or evidence. This is problematic as an introduction since evaluating rule application in language models is the main theme of the paper, where the example doesn’t show actual rules. Other more natural examples could be used to illustrate where people use rules, beyond the most obvious examples of math or physics problems etc, one could use examples of day-to-day such as admission criteria to a school or university, rules about payments or finance, examples of rules from the law system, etc etc. \n\n* **Presentation of the motivation, SCAN case**: The second example about SCAN is closer to exemplify about rules but the claim about people presented with rules rather than examples is not compelling either. E.g. if I showed a person example that f(A B and then C B)=aacc , f(D earlier than C earlier than A)=acd, we would quickly look for identifying rules in the example, learning that “and then” means putting after, “earlier than” means before and B means to do the action twice, even without being presented with the rules. People have a tendency to look for simple rules in examples, and they identify general rules (sometimes to general or strong) very commonly. People also commonly do induction and not only deduction, e.g. if f(abc)=d, and f(klm)=n then they will figure out that f(wxy) is likely to be z. Overall, I totally agree that rule-based learning is important in real-world and that AI models aren’t strong in this silk, but there could be the presentation of the motivation for that could be improved a lot.\n\n* **Conceptual learning task definition**: The concept seeks to achieve  5 properties that the learner should possess. No justification is given about why these 5, how they were selected, how they complement each other into a common goal etc. I believe the definition should be reworked to be more cohesive. The definitions of some of the properties also weren't clear to me.\n\n* **Experiments**: I would suggest to extend the experimental section, e.g. see the impact of the complexity of the rules and number of rules needed on the performance along different metrics, see generalization across multiple dimensions beyond only compositional generalization, do error analysis of what mistakes and trends are common in terms of accuracy and consistency and if any general behaviors and trends of models (vs. even human study) could be identified, etc.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**: The writing quality is good but I think there could be significant improvements to the presentations of the ideas in the paper, as discussed above. Also a small comment beyond those above: would be good that figure 2 would be much closer to where it’s mentioned (page 1) compared to where it’s at right now (page 4).\n\n**Novelty**: The explicit discussion and exploration of evaluation of systematic learning and the approach introduced is both novel and pursuing an important and underexplored direction in the field.\n\n**Results**: A theoretical idea is introduced (conceptual learning, a method for creating benchmarks) and then a concrete application of it is explored (a dataset) and analyzed (experiments over language models). So the paper has the necessary components to convey a full story. I do recommend having more experiments and more baselines though (see comments above).\n\n**Reproducibility**: The paper and especially appendixes provide a lot of detail that could allow reproducing the paper.\n\n# Summary Of The Review\n\nI think overall the paper explores and important direction but could be improved in the presentation and extended with more experiments to make it much better and therefore at this time I recommend rejection but also wish to emphasize that I encourage the authors to keep working on the paper to make it better and then resubmit to a conference!\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDROPIT: DROPPING INTERMEDIATE TENSORS FOR MEMORY-EFFICIENT DNN TRAINING\n\nJoya Chen1∗, Kai Xu1∗, Yuhui Wang1, Yifei Cheng2, Angela Yao1 1National University of Singapore 2University of Science and Technology of China joyachen@u.nus.edu\n\n{kxu,yuhuiw,ayao}@comp.nus.edu.sg\n\nchengyif@mail.ustc.edu.cn\n\nABSTRACT\n\nA standard hardware bottleneck when training deep neural networks is GPU memory. The bulk of memory is occupied by caching intermediate tensors for gradient computation in the backward pass. We propose a novel method to reduce this footprint - Dropping Intermediate Tensors (DropIT). DropIT drops min-k elements of the intermediate tensors and approximates gradients from the sparsified tensors in the backward pass. Theoretically, DropIT reduces noise on estimated gradients and therefore has a higher rate of convergence than vanilla-SGD. Experiments show that we can drop up to 90% of the intermediate tensor elements in fullyconnected and convolutional layers while achieving higher testing accuracy for Visual Transformers and Convolutional Neural Networks on various tasks (e.g. , classification, object detection, instance segmentation). Our code and models are available at https://github.com/chenjoya/dropit.\n\n1\n\nINTRODUCTION\n\nThe training of state-of-the-art deep neural networks (DNNs) (Krizhevsky et al., 2017; Simonyan & Zisserman, 2015; He et al., 2016; Vaswani et al., 2017; Dosovitskiy et al., 2021) for computer vision often requires a large GPU memory. For example, training a simple visual transformer detection model ViTDet-B (Li et al., 2022), with its required input image size of 1024×1024 and batch size of 64, requires ∼700 GB GPU memory. Such a high memory requirement makes the training of DNNs out of reach for the average academic or practitioner without access to high-end GPU resources.\n\nWhen training DNNs, the GPU memory has six primary uses (Rajbhandari et al., 2020): network parameters, parameter gradients, optimizer states (Kingma & Ba, 2015), intermediate tensors (also called activations), temporary buffers, and memory fragmentation. Vision tasks often require training with large batches of high-resolution images or videos, which can lead to a significant memory cost for intermediate tensors. In the instance of ViTDet-B, approximately 70% GPU memory cost (∼470 GB) is assigned to the intermediate tensor cache. Similarly, for NLP, approximately 50% of GPU memory is consumed by caching intermediate tensors for training the language model GPT2 (Radford et al., 2019; Rajbhandari et al., 2020). As such, previous studies (Gruslys et al., 2016; Chen et al., 2016; Rajbhandari et al., 2020; Feng & Huang, 2021) treat the intermediate tensor cache as the largest consumer of GPU memory.\n\nFor differentiable layers, standard implementations store the intermediate tensors for computing the gradients during back-propagation. One option to reduce storage is to cache tensors from only some layers. Uncached tensors are recomputed on the fly during the backward pass – this is the strategy of gradient checkpointing (Gruslys et al., 2016; Chen et al., 2016; Bulo et al., 2018; Feng & Huang, 2021). Another option is to quantize the tensors after the forward computation and use the quantized values for gradient computation during the backward pass (Jain et al., 2018; Chakrabarti & Moseley, 2019; Fu et al., 2020; Evans & Aamodt, 2021; Liu et al., 2022) – this is known as activation compression training (ACT). Quantization can reduce memory considerably, but also brings inevitable performance drops. Accuracy drops can be mitigated by bounding the error at each layer through adaptive quantization (Evans & Aamodt, 2021; Liu et al., 2022), i.e. adaptive ACT. However, training time consequently suffers as extensive tensor profiling is necessary during training.\n\n∗Equal contribution.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nIn this paper, we propose to reduce the memory usage of intermediate tensors by simply dropping elements from the tensor. We call our method Dropping Intermediate Tensors (DropIT). In the most basic setting, dropping indices can be selected randomly, though dropping based on a min-k ranking on the element magnitude is more effective. Both strategies are much simpler than the sensitivity checking and other profiling strategies, making DropIT much faster than adaptive ACT.\n\nDuring training, the intermediate tensor is transformed over to a sparse format after the forward computation is complete. The sparse tensor is then recovered to a general tensor during backward gradient computation with dropped indices filled with zero. Curiously, with the right dropping strategy and ratio, DropIT has improved convergence properties compared to SGD. We attribute this to the fact that DropIT can, theoretically, reduce noise on the gradients. In general, reducing noise will result in more precise and stable gradients. Experimentally, this strategy exhibits consistent performance improvements on various network architectures and different tasks.\n\nTo the best of our knowledge, we are the first to propose activation sparsification. The closest related line of existing work is ACT, but unlike ACT, DropIT leaves key elements untouched, which is crucial for ensuring accuracy. Nevertheless, DropIT is orthogonal to activation quantization, and the two can be combined for additional memory reduction with higher final accuracy. The key contributions of our work are summarized as follows:\n\n• We propose DropIT, a novel strategy to reduce the activation memory by dropping the\n\nelements of the intermediate tensor.\n\n• We theoretically and experimentally show that DropIT can be seen as a noise reduction on\n\nstochastic gradients, which leads to better convergence.\n\n• DropIT can work for various settings: training from scratch, fine-tuning on classification, object detection, etc. Our experiments demonstrate that DropIT can drop up to 90% of the intermediate tensor elements in fully-connected and convolutional layers with a testing accuracy higher than the baseline for CNNs and ViTs. We also show that DropIT is much better regarding accuracy and speed compared to SOTA activation quantization methods, and it can be combined with them to pursue higher memory efficiency.\n\n2 RELATED WORK\n\nMemory-efficient training. Current DNNs usually incur considerable memory costs due to huge model parameters (e.g. GPTs (Radford et al., 2019; Brown et al., 2020)) or intermediate tensors (e.g. , high-resolution feature map (Sun et al., 2019; Gu et al., 2022)). The model parameters and corresponding optimizer states can be reduced with lightweight operations (Howard et al., 2017; Xie et al., 2017; Zhang et al., 2022), distributed optimization scheduling (Rajbhandari et al., 2020), and mixed precision training (Micikevicius et al., 2018). Nevertheless, intermediate tensors, which are essential for gradient computation during the backward pass, consume the majority of GPU memory (Gruslys et al., 2016; Chen et al., 2016; Rajbhandari et al., 2020; Feng & Huang, 2021), and reducing their size can be challenging.\n\nGradient checkpointing. To reduce the tensor cache, gradient checkpointing (Chen et al., 2016; Gruslys et al., 2016; Feng & Huang, 2021) stores tensors from only a few layers and recomputes any uncached tensors when performing the backward pass; in the worst-case scenario, this is equivalent to duplicating the forward pass, so any memory savings come as an extra computational expense. InPlace-ABN (Bulo et al., 2018) halves the tensor cache by merging batch normalization and activation into a single in-place operation. The tensor cache is compressed in the forward pass and recovered in the backward pass. Our method is distinct in that it does not require additional recomputation; instead, the cached tensors are sparsified heuristically.\n\nActivation compression. (Jain et al., 2018; Chakrabarti & Moseley, 2019; Fu et al., 2020; Evans & Aamodt, 2021; Chen et al., 2021; Liu et al., 2022) explored lossy compression on the activation cache via low-precision quantization. (Wang et al., 2022) compressed high-frequency components while (Evans et al., 2020) adopted JPEG-style compression. In contrast to all of these methods, DropIT reduces activation storage via sparsification, which has been previously unexplored. In addition, DropIT is more lightweight than adaptive low-precision quantization methods (Evans & Aamodt, 2021; Liu et al., 2022).\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n(a) Baseline\n\n(b) DropIT\n\nFigure 1: For a regular baseline network (a), the intermediate tensor is fully cached in the forward pass to be used for gradient computation during the backward pass. For DropIT, elements of the intermediate tensors are dropped during caching; only the retained elements with zero padding are used for gradient computation during the backward pass. DropIT can save GPU memory for two reasons. First, cached tensors are accumulated layer by layer during the forward pass, and DropIT sparsifies them, thereby reducing maximum memory allocation. Second, backward tensors are released after use, making the memory cost associated with padding negligible. Best viewed in color.\n\nGradient approximation. Approximating gradients has been explored in large-scale distributed training to limit communication bandwidth for gradient exchange. (Strom, 2015; Dryden et al., 2016; Aji & Heafield, 2017; Lin et al., 2018) propose dropping gradients based on some fixed thresholds and sending only the most significant entries of the stochastic gradients with the guaranteed convergence (Stich et al., 2018; Cheng et al., 2022; Chen et al., 2020). Instead of dropping gradient components, DropIT directly drops elements within intermediate tensors as our objective is to reduce the training memory.\n\n3 METHODOLOGY\n\n3.1 PRELIMINARIES\n\nWe denote the forward function and learnable parameters of the i-th layer as l and θ, respectively. In the forward pass, l operates on the layer’s input a to compute the output z: 1\n\nz = l(a, θ).\n\n(1)\n\nFor example, if layer i is a convolution layer, l would indicate a convolution operation with θ representing the kernel weights and bias parameter.\n\nGiven a loss function F (Θ), where Θ represents the parameters of the entire network, the gradient, with respect to θ at layer i, can be estimated according to the chain rule as\n\n∇θ ≜ ∂F (Θ)\n\n∂θ\n\n= ∇z\n\n∂z ∂θ\n\n= ∇z\n\n∂l(a, θ) ∂θ\n\n,\n\n(2)\n\nis the gradient passed back from layer i + 1. Note that the computation of\n\nwhere ∇z ≜ ∂F (Θ)\n\n∂z\n\n∂l(a,θ) ∂θ\n\nrequires a if the forward function l involves tensor multiplication between a and θ. This is the case for common learnable layers, such as convolutions in CNNs and fully-connected layers in transformers. As such, a is necessary for estimating the gradient and is cached after it is computed\n\n1Note that the output from the previous layer i−1, i.e. ai = zi−1. However, we assign different symbols to denote the input and output of a given layer explicitly; this redundant notation conveniently allows us, for clarity purposes, to drop the explicit reference of the layer index i as a superscript.\n\n3\n\nCache aaLayeriiin forward passParameter θθInput aaForwardfunctionLayeriiin backward passLayerii−1Gradient ∇θθOutput zzGradient ∇zzGradient ∇aaLayer ii+1Layer ii+1Layer ii−1Cached aaParameter θθBackward functionLayeriin backward passPart Gradient ▽θGradient ∇zzGradient ▽aLayer ii+1Layer i -1Padded RR(�aa)Parameter θθBackward functionCached �aaLayeriiin forward passParameter θθInput aaForwardfunctionLayer ii−1Output zLayer ii+1DropIT Cache aaLayeriiin forward passParameter θθInput aaForwardfunctionLayeriiin backward passLayerii−1Gradient ∇θθOutput zzGradient ∇zzGradient ∇aaLayer ii+1Layer ii+1Layer ii−1Cached aaParameter θθBackward functionLayeriin backward passPart Gradient ▽θGradient ∇zzGradient ▽aLayer ii+1Layer i -1Padded RR(�aa)Parameter θθBackward functionCached �aaLayeriiin forward passParameter θθInput aaForwardfunctionLayer ii−1Output zLayer ii+1DropIT Published as a conference paper at ICLR 2023\n\nLayer Type\n\nParameter θ\n\nTensor a\n\nConvolution Fully Connected\n\nO(CaCzK 2) O(BCaLa) O(BLaCa)\n\nO(CaCz)\n\nTable 1: Space complexity for parameters and intermediate tensors in a single layer. B: batch size, La: input sequence length (e.g. , width×height), Ca, Cz: the number of input, output channels, K: convolutional kernel size. Typically, Ca, Cz, K would be fixed once the model has been built, so the complexity for intermediate tensors would be considerable with large B, La.\n\nduring the forward pass, as illustrated in Figure 1(a). A common way to reduce storage for a is to store a quantized version (Jain et al., 2018; Chakrabarti & Moseley, 2019; Fu et al., 2020; Evans & Aamodt, 2021; Liu et al., 2022). Subsequent gradients in the backward pass are then computed using the quantized a. The gradient ∇a can be estimated similarly via chain rule as\n\n∇a ≜ ∂F (Θ)\n\n∂a\n\n= ∇z\n\n∂z ∂a\n\n= ∇z\n\n∂l(a, θ) ∂a\n\n.\n\n(3)\n\nAnalogous to Eq. 2, the partial ∂l(a,θ) ∂a may depend on the parameter θ and θ is similarly stored in the model memory. However, the stored θ always shares memory with the model residing in the GPU, so it does not incur additional memory consumption. Furthermore, θ typically occupies much less memory. In Table 1, the intermediate tensor’s space complexity becomes significant when B or La is large, which is common in CV and NLP tasks.\n\n3.2 DROPPING INTERMEDIATE TENSORS\n\nLet X denote the set of all indices for an intermediate tensor a. Suppose that X is partitioned into two disjoint sets Xd and Xr, i.e. Xr ∩ Xd = ∅ and Xr ∪ Xd = X . In DropIT, we introduce a dropping operation D(·) to sparsify a into ˆa, where ˆa consists of the elements aXr and the indices Xr, i.e. ˆa = D(a) = {aXr , Xr}. The sparse ˆa can be used as a substitute for a in Eq. 2. While sparsification can theoretically reduce both storage and computation time, we benefit only from storage savings in practice. We retain general matrix multiplication because the sparsity rate is insufficient for sparse matrix multiplication to provide meaningful computational gains. As such, the full intermediate tensors are recovered for gradient computation, i.e. ∇θ ≈ ∇z ∂l(R(ˆa),θ) , where R(·) represents the process that inflates ˆa back to a general matrix with dropped indices filled with zero. The overall procedure is demonstrated in Figure 1(b).\n\n∂θ\n\nConsider for a convolutional layer with Cz kernels of size K × K. For the jth kernel, where j ∈ [1, Cz], the gradient at location (u, v) for the kth channel is given by convolving incoming gradient ∇z and input a:\n\n∇θj,k(u, v) =\n\n(cid:88)\n\n∇zn\n\nj (x, y)an\n\nk (x′, y′),\n\n(4)\n\n(n,x,y)∈X\n\nwhere x′ = x + u and y′ = y + v. The set X in this case would denote the set of all sample indices n ∈ [1, B] and all location indices (x, y) ∈ [1, W ] × [1, H] in the feature map. Without any loss in generality, we can partition X into two disjoint sets Xr and Xd to split Eq. 4 as\n\n∇θj,k(u, v) =\n\n(cid:104) (cid:88)\n\n(n,x,y)∈Xd\n\n∇zn\n\nj (x, y)an\n\nk (x′, y′) +\n\n(cid:88)\n\n(n,x,y)∈Xr (cid:124)\n\n∇zn\n\nj (x, y) an\n\nk (x′, y′)\n\n(cid:105) .\n\n(5)\n\n(cid:123)(cid:122) ⊤θj,k(u,v)\n\n(cid:125)\n\nAssume now, that some element an k (x′, y′) is small or near-zero; in CNNs and Transformers, such an assumption is reasonable due to preceding batch/layer normalization and ReLU or GeLU activations (see Figure 3). Accordingly, this element’s contribution to the gradient will also be correspondingly small. If we assign the spatial indices (x, y) in sample n of all small or near-zero elements to Xd, then we can approximate the gradient ∇θj,k(u, v) with simply the second term of Eq. 5. We denote the approximated gradient as gdropit = ⊤θj,k(u, v).\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n(a) Forward\n\n(b) Backward\n\nFigure 2: Forward and backward of DropIT on the fully-connected layer (without bias). In the forward pass, we sparsify the cache tensor and drop γ percentage storage. In the backward pass, only saved elements participate in the gradient computation.\n\nFigure 3: Distribution of element values in intermediate tensors’ on DeiT-Ti. Dropped elements are shaded in grey. DropIT with min-k only discards elements that are close to zero. Here we only show the final block while observing that the distributions of other blocks are similar.\n\nFor a fully connected layer, the approximated gradient can be defined similarly as\n\ngdropit = ⊤θj,k =\n\n(cid:88)\n\n∇zn\n\nj an k .\n\nn∈Xr\n\n(6)\n\nA visualization of the gradient approximation is shown in Figure 2. With the approximated gradient, we can use any standard deep learning optimization scheme to update the parameters.\n\n3.3 DROPPING FUNCTION D(·)\n\nWe define the overall dropping rate as γ = |Xr| BCaHW for BCa a convolutional layer. γ can be varied and will be used later to define the dropping function D(·). As we aim to drop elements with minimal contribution to the gradient, it is logical to perform a min-k based selection on the elements’ magnitudes before dropping the elements. As a baseline comparison, we also select Xd based on uniform random sampling. We investigate the following options for D(·):\n\nfor a fully connected layer and γ = |Xr|\n\nRandom Elements: γ fraction of elements are dropped randomly within a mini-batch.\n\nMin-K Elements: Within a mini-batch, we drop the smallest γ fraction of elements according to their absolute magnitudes.\n\n5\n\nInput aaOutputzzWeight θθ×→Cache �aaGradient ∇zzWeight θθTT←×Gradient ∇aaRR�aaTT←×Gradient ⊤θθ(approximated)Gradient ∇zzDropping Function DD(⋅)(e.g.,Random, γ= 0.9)Input aaOutputzzWeight θθ×→Cache �aaGradient ∇zzWeight θθTT←×Gradient ∇aaRR�aaTT←×Gradient ⊤θθ(approximated)Gradient ∇zzDropping Function DD(⋅)(e.g.,Random, γ= 0.9)42024Value024Count (106)blocks.11.attn.qkv=70%0.80.60.40.20.00.20.40.60.8Value024blocks.11.attn.proj=70%42024Value024blocks.11.mlp.fc1=70%0.20.00.20.40.60.81.01.21.4Value024blocks.11.mlp.fc2=70%Published as a conference paper at ICLR 2023\n\n3.4 THEORETICAL ANALYSIS\n\nBelow, we analyze convergence for dropping min-k elements. The gradient of Stochastic Gradient Descent (SGD) is commonly viewed as Gradient Descent (GD) with noise:\n\ngsgd = ggd + n(0, ξ2),\n\n(7)\n\nwhere n represents some zero-mean noise distribution with a variance of ξ2 introduced by variation in the input data batches.\n\nWith min-k dropping, the gradient becomes biased; we assume it can be modeled as:\n\ngmin-k = αggd + βn(0, ξ2).\n\n(8)\n\nThat is, min-k dropping results in a bias factor α while affecting noise by a factor of β. α and β vary each iteration, i.e., α = {α1, α2, ..., αt} and β = {β1, β2, ..., βt}. Additionally in Appendix A.2, we provide a nonlinear approximation to gmin−k that achieves same convergence. By scaling the learning rate with a factor of 1 can also be expressed as:\n\nα , the gradient after min-k dropping as given in Eq. 8\n\ngmin-k = ggd +\n\nβ α\n\nn(0, ξ2).\n\n(9)\n\nWe can formally show (see Appendix A.3) that E[α] ≥ E[β] ≥ 1 − γ and therefore E[ β α ] ≤ 1. This suggests that min-k dropping reduces the noise of the gradient. With less noise, better theoretical convergence is expected.\n\nSimilar to convergence proofs in most optimizers, we will assume that the loss function F is Lsmooth. Under the L-smooth assumption, for SGD with a learning rate η and min-k dropping with a learning rate η αt\n\n, we can reach the following convergence after T iterations:\n\nSGD:\n\nDropIT with min-k:\n\n1 T\n\n1 T\n\nT (cid:88)\n\nE\n\nt=1\n\nT (cid:88)\n\nt=1\n\nE\n\n∥∇F (xt)∥2 ≤\n\n2(F (x1) − F (x∗)) T η\n\n+ ηLξ2\n\n∥∇F (xt)∥2 ≤\n\n2(F (x1) − F (x∗)) T η\n\n+ ηLξ2 1\n\nT\n\nT (cid:88)\n\nt=1\n\nβ2 t\nα2 t\n\n,\n\n(10)\n\n(11)\n\nwhere x∗ indicates an optimal solution. Full proof can be found in Appendix A.1. Note that the two inequalities differ only by the second term in the right-hand side. αt represents the bias caused by dropping at the t-th iteration and βt measures the noise reduction effect after dropping. We further investigate α and β in the supplementary and show that under certain conditions, E[α] ≥ E[β], thereby reducing the noise and improving the convergence of DropIT from standard SGD.\n\n3.5 DROPIT FOR NETWORKS\n\nFor some layers, e.g. normalization and activations, ∂l(a,θ) ∂a may also depend on a. In these cases, we do not drop the cache of intermediate tensors as this will affect subsequent back-propagation. For DropIT, dropping happens only when the gradient flows to the parameters, which prevents the aggregation of errors from approximating the gradient.\n\nNow, we have discussed dropping tensor elements from the cache of a single layer. DropIT is theoretically applicable for all convolutional and fully-connected layers in a network since it does not affect the forward pass. For Visual Transformers (Dosovitskiy et al., 2021), we apply DropIT for most learnable layers, though we ignore the normalization and activations like LayerNorm (Ba et al., 2016) and GELU (Hendrycks & Gimpel, 2016)). The applicable layers include fully-connected layers in multi-head attention and MLPs in each block, the beginning convolutional layer (for patches projection), and the final fully-connected classification layer. For CNNs the applicable layers include all convolutional layers and the final fully-connected classification layer. We leave networks unchanged during inference.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nStrategy\n\n72.0 72.2 ∗ From Touvron et al. (2021)’s official implementation, we obtain 72.13 with public weights and our training.\n\n10% 20% 30% 40% 50% 60% 70% 80% 90% 72.4 60.8 70.8 72.5 72.1 66.4\n\n0%(Baseline) 72.1∗ 72.1∗\n\nRandom Min-K\n\n65.8 70.8\n\n72.4 72.1\n\n68.1 72.1\n\n71.7 72.4\n\n69.6 72.4\n\nDropping Rate γ\n\nTable 2: Ablation study on dropping strategy and dropping rate. Reported results are top-1 accuracy on the ImageNet-1k validation set, achieved by DeiT-Ti training from scratch on the ImageNet-1k training set. We highlight that the accuracy is higher than baseline ( ≥72.1).\n\nFigure 4: Training loss curves of Min-K DropIT. Baseline (γ = 0%) is bolded. γ = 80%, 90% are hidden as their losses are obviously higher than the baseline. γ = 10%, 30% are also hidden for easier viewing. γ = 40%∼70% achieve lower loss than baseline at the end. Best viewed in color.\n\n4 EXPERIMENTS\n\nIn this section, we present a comprehensive evaluation of DropIT’s effectiveness, leveraging experiments on training from scratch on ImageNet-1k (Russakovsky et al., 2015). Our results demonstrate that DropIT outperforms existing methods by achieving lower training loss, higher testing accuracy, and reduced GPU memory consumption. We showcase the versatility of DropIT in various finetuning scenarios, such as ImageNet-1k to CIFAR-100 (Krizhevsky et al., 2009), object detection, and instance segmentation on MS-COCO (Lin et al., 2014). Furthermore, we compare DropIT with recent state-of-the-art ACT methods (Pan et al., 2022; Liu et al., 2022) and establish its superiority in terms of accuracy, speed, and memory cost.\n\n4.1 EXPERIMENTAL DETAILS\n\nModels. For image classification, we employed DeiT (Touvron et al., 2021) instead of vanilla ViT (Dosovitskiy et al., 2021) since it doesn’t require fine-tuning from ImageNet-21k. DeiT and ViT share the same architecture, differing only in their training hyper-parameters. Additionally, for transfer learning, we utilized Faster/Mask R-CNN models (Ren et al., 2017; He et al., 2017) to evaluate our approach in object detection and instance segmentation.\n\nImplementation Details. We use the official implementations of DeiT (without distillation) and Faster/Mask R-CNN, and keep all hyper-parameters consistent. The only difference is that we compute gradients using DropIT. Our implementation is based on PyTorch 1.12 (Paszke et al., 2019), and we utilize torch.autograd package. During the forward pass, we use DropIT to convert the dense tensor to coordinate format, and recover it during the backward pass. The min-k strategy of DropIT is implemented by torch.topk, which retains elements with the largest absolute value, based on a proportion of 1−γ. The corresponding indices of these elements are also maintained. For all experiments, we follow DeiT (Touvron et al., 2021) and set a fixed random seed of 0. We measure training speed and memory on NVIDIA RTX A5000 GPUs. Additional details can be found in Appendix A.8.\n\n7\n\n(SRFK/RVV      Published as a conference paper at ICLR 2023\n\n(a) CIFAR-100 fine-tuning results. DeiT networks are initialized from their publicly available pre-trained ImageNet-1k weights.\n\n(b) Detection & instance segmentation on COCO. R50-FPN denotes ResNet-50 with FPN (Lin et al., 2017), initialized from public ImageNet-1k weights.\n\nNetwork\n\nDeiT-S\n\nDeiT-B\n\nDropIT -\nγ = 90% -\nγ = 90%\n\nTop-1 Accuracy 89.7 90.1 90.8 91.3\n\nNetwork Faster R-CNN (R50-FPN) Mask R-CNN (R50-FPN)\n\nDropIT -\nγ = 90% -\nγ = 80%\n\nAPbox APmask 37.0 37.2 37.9 38.5\n\nn/a n/a 34.5 34.5\n\nTable 3: Fine-tuning with DropIT on image classification, object detection & instance segmentation.\n\nCached\n\nTensor\n\nγ = 0% 11.26 G 4.50 G (−60%)\n\nγ = 60%\n\nDropping Rate γ γ = 70% 3.38 G (−70%)\n\nγ = 80% 2.25 G (−80%)\n\nγ = 90% 1.13 G (−90%)\n\nTable 4: Memory cost of DropIT cached tensors (without indices) for different γ. DropIT can precisely reduce the memory by γ. The measured model is DeiT-S with a batch size of 1024.\n\n4.2\n\nIMPACT ON ACCURACY\n\nTraining from scratch on ImageNet-1k. Table 2 shows that training DeiT-Ti from scratch without DropIT (baseline) has a top-1 accuracy of 72.1 on ImageNet-1k. Random dropping matches or improves the accuracy (72.4) when γ ≤ 20%, but with higher γ (γ ≥ 30%), accuracy progressively decreases from the baseline. The phenomenon can be explained by the following: (1) Small amounts of random dropping (γ ≤ 20%) can be regarded as adding random noise to the gradient. The noise has a regularization effect on the network optimization to improve the accuracy, similar to what was observed in previous studies (Neelakantan et al., 2015; Evans & Aamodt, 2021). (2) Too much random dropping (γ ≥ 30%) results in deviations that can no longer be seen as small gradient noise, hence reducing performance.\n\nWith min-k dropping, DropIT can match or exceed the baseline accuracy over a wide range of γ (≤ 70%). Intuitively, training from scratch should be difficult with DropIT, especially under large dropping rates, as the computed gradients are approximations. However, our experiments demonstrate that DropIT achieves 0.4% and 0.3% higher accuracy in γ = 50% and 60%, respectively. In fact, DropIT can match the baseline accuracy even after discarding 70% of the elements.\n\nFig. 4 compares the loss curves when training from scratch on the baseline DeiT-Ti model without and with DropIT using a min-k strategy. The loss curves of DropIT with various γ values follow the same trend as the baseline; up to some value of γ, the curves are also but are consistently lower than the baseline, with γ = 50%, 60% achieving the lowest losses and highest accuracies. As such, we conclude that DropIT accurately approximates the gradient while reducing noise, as per our theoretical analysis.\n\nFine-tuning on CIFAR-100. Table 3(a) shows that DeiT networks can be fine-tuned with DropIT to achieve higher than baseline accuracies even while dropping up to 90% intermediate elements. Compared to training from scratch from Table 2, DropIT can work with a more extreme dropping rate (90% vs. 70%). We interpret that this is because the network already has a good initialization before fine-tuning, thereby simplifying the optimization and allowing a higher γ to be used.\n\nBackbone fine-tuning, head network training from scratch, on COCO. We investigated DropIT in two settings: training from scratch and fine-tuning from a pre-trained network. We also studied a backbone network initialized with ImageNet pre-training, while leaving others, such as RPN and R-CNN head, uninitialized, which is common practice in object detection. Table 3(b) shows that DropIT can steadily improve detection accuracy (APbox). When γ = 80%, we observed an impressive 0.6 AP gain in Mask R-CNN, although this gain was not observed in APmask. We believe that the segmentation performance may be highly related to the deconvolutional layers in the mask head, which are not currently supported by DropIT. We plan to investigate this further in future work. These experiments demonstrate the effectiveness of DropIT on CNNs, and in Appendix A.4, we demonstrate the effectiveness of DropIT for ResNet training on ImageNet-1k.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nBenchmark\n\nDeiT-S on CIFAR-100\n\nFC Cache none DropIT (γ = 90%) MESA (8-bit)\n\nOthers Cache none none MESA (8-bit) DropIT(γ = 90%) MESA (8-bit) GACT (4-bit) GACT (4-bit)\n\nGACT (4-bit) DropIT(γ = 90%)\n\nAcc MaxM MaxM (-Index) 89.7 90.1 89.7 89.9 89.7 90.0\n\n6.66 G 5.29 G 3.52 G 2.97 G 2.16 G 1.97 G\n\n6.66 G 5.59 G 3.52 G 3.27 G 2.16 G 2.27 G\n\nSpeed (ms) 172 212 416 375 290+49∗ 286+25∗\n\n∗Note: GACT has a time-consuming sensitivity profiling computation every 1000 iterations. It costs 49.81 and 25.43 (+DropIT) seconds in our benchmark. So we add an average of this time over 1000 iterations).\n\nTable 5: Compare and combine with state-of-the-art ACT methods. FC: fully-connected. MaxM: maximum memory. MaxM (- Index): maximum memory without index (moved to CPU). We follow MESA to use batch size 128 to measure memory and speed on a single GPU.\n\n4.3\n\nIMPACT ON MEMORY & SPEED, SOTA COMPARISON\n\nIntermediate Tensor Cache Reduction. Table 4 shows the intermediate tensor cache reduction achieved by DropIT. In DropIT applied layers (FC layers of DeiT-S), the total reserved activation (batch size = 1024) is 11.26 G. When we use DropIT to discard activations, the memory reduction is precisely controlled by γ, i.e. γ = 90% means the reduction is 11.26 × 0.9. DropIT does incur some memory cost for indexing, but as we show next, the maximum GPU memory can still be reduced.\n\nComparison and Combination with SOTA. In Table 5, we compare and combine DropIT with state-of-the-art activation quantization methods. Measuring performance individually, with γ = 90%, DropIT improves accuracy by 0.4 and reduces maximum memory by 1.07 G (1.37G activations - 0.3G indexing), and slightly increases the time (40 ms) per iteration. The max memory reduction is less than that shown Table 4 because activations from non-applicable layers still occupy considerable memory. Therefore, a natural idea to supplement DropIT is to perform activation quantization for layers without DropIT. We next present the combination results of DropIT with recent methods MESA (Pan et al., 2022) and GACT (Liu et al., 2022).\n\nAs shown in Table 5, MESA can reduce memory with 8-bit quantization and it has no impact on the baseline accuracy (89.7). However, the time cost of the MESA algorithm is also considerable, and is 416/172 ≈ 2.4× slower than baseline and 416/212 ≈ 2× more than DropIT, with no accuracy improvement in CIFAR-100 finetuning. MESA achieves 71.9 accuracy of DropIT-Ti on ImageNet1k, but DropIT can go up to 72.5 (Table 2, γ = 50%). We can combine MESA with DropIT by applying DropIT in the conv/fc layers and applying MESA in the other layers. Together, the accuracy, memory, and speed are all improved over MESA alone, conclusively demonstrating the effectiveness of DropIT.\n\nWe compare similarly to GACT; Table 5 shows that at 4 bits, it can reduce max-memory even further. Combining GACT with DropIT marginally increases the max-memory due to DropIT’s indexing consumption; however, there are both accuracy and speed gains. Furthermore, GACT reports 0.2∼0.4 APbox loss on COCO (Liu et al., 2022), though our DropIT can produce 0.6 APbox improvement on COCO (Table 3(b)). To sum up, DropIT has its unique advantages in terms of accuracy and speed compared to existing activation quantization methods. Although it saves less memory than the latter, we can combine the two to achieve higher memory efficiency.\n\n5 CONCLUSION\n\nIn this paper, we propose the Dropping Intermediate Tensors (DropIT) method to reduce the GPU memory cost during the training of DNNs. Specifically, DropIT drops elements in intermediate tensors to achieve a memory-efficient tensor cache, and it recovers sparsified tensors from the remaining elements in the backward pass to compute the gradient. Our experiments show that DropIT can improve the accuracies of DNNs and save GPU memory on different backbones and datasets. DropIT provides a new perspective to reduce GPU memory costs during DNN training. For future work, DropIT can be explored in training large (vision-)language models.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n6 ACKNOWLEDGEMENTS\n\nThis research is supported by the National Research Foundation, Singapore under its NRF Fellowship for AI (NRF-NRFFAI1-2019-0001). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.\n\nREFERENCES\n\nAlham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. In\n\nEMNLP, pp. 440–445, 2017.\n\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv: 1607.06450,\n\n2016.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, volume 33, pp. 1877– 1901, 2020.\n\nSamuel Rota Bulo, Lorenzo Porzi, and Peter Kontschieder.\n\nIn-place activated batchnorm for\n\nmemory-optimized training of dnns. In CVPR, pp. 5639–5647, 2018.\n\nAyan Chakrabarti and Benjamin Moseley. Backprop with approximate activations for memory-\n\nefficient network training. In NeurIPS, pp. 2426–2435, 2019.\n\nJianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael W. Mahoney, and Joseph Gonzalez. Actnn: Reducing training memory footprint via 2-bit activation compressed training. In ICML, pp. 1803–1813, 2021.\n\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\n\nmemory cost. arXiv, 1604.06174, 2016.\n\nZhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Just pick a sign: Optimizing deep multitask models with gradient sign\n\nDragomir Anguelov. dropout. NeurIPS, pp. 2039–2050, 2020.\n\nFeng Cheng, Mingze Xu, Yuanjun Xiong, Hao Chen, Xinyu Li, Wei Li, and Wei Xia. Stochastic backpropagation: A memory efficient strategy for training video models. In CVPR, pp. 8301– 8310, 2022.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n\nNikoli Dryden, Tim Moon, Sam Ade Jacobs, and Brian Van Essen. Communication quantization\n\nfor data-parallel training of deep neural networks. In MLHPC workshop, pp. 1–8, 2016.\n\nR. David Evans and Tor M. Aamodt. AC-GC: lossy activation compression with guaranteed conver-\n\ngence. In NeurIPS, pp. 27434–27448, 2021.\n\nR David Evans, Lufei Liu, and Tor M Aamodt. Jpeg-act: accelerating deep learning via transform-\n\nbased lossy compression. In ISCA, pp. 860–873, 2020.\n\nJianwei Feng and Dong Huang. Optimal gradient checkpoint search for arbitrary computation\n\ngraphs. In CVPR, pp. 11433–11442, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nFangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, and Bin Cui. Don’t waste your bits! squeeze activations and gradients for deep neural networks via tinyscript. In ICML, pp. 3304–3314, 2020.\n\nAudrunas Gruslys, R ́emi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-efficient\n\nbackpropagation through time. In NeurIPS, pp. 4125–4133, 2016.\n\nKerui Gu, Linlin Yang, and Angela Yao. Dive deeper into integral pose regression. In ICLR, 2022.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In CVPR, pp. 770–778, 2016.\n\nKaiming He, Georgia Gkioxari, Piotr Doll ́ar, and Ross B. Girshick. Mask R-CNN. In ICCV, pp.\n\n2980–2988, 2017.\n\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arxiv:1606.08415, 2016.\n\nAndrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv:1704.04861, 2017.\n\nAnimesh Jain, Amar Phanishayee, Jason Mars, Lingjia Tang, and Gennady Pekhimenko. Gist:\n\nEfficient data encoding for deep neural network training. In ISCA, pp. 776–789, 2018.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-\n\nlutional neural networks. Communications of the ACM, 60(6):84–90, 2017.\n\nYanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer back-\n\nbones for object detection. In ECCV, pp. 280–296, 2022.\n\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ́ar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, pp. 740–755, 2014.\n\nTsung-Yi Lin, Piotr Doll ́ar, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J. Be-\n\nlongie. Feature pyramid networks for object detection. In CVPR, pp. 936–944, 2017.\n\nYujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing\n\nthe communication bandwidth for distributed training. In ICLR, 2018.\n\nXiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael Mahoney, and Alvin Cheung. GACT: Activation compressed training for generic network architectures. In ICML, pp. 14139–14152, 2022.\n\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David Garc ́ıa, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018.\n\nArvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv:1511.06807, 2015.\n\nZizheng Pan, Peng Chen, Haoyu He, Jing Liu, Jianfei Cai, and Bohan Zhuang. Mesa: A memory-\n\nsaving training framework for transformers. arXiv:2111.11124, 2022.\n\nAdam Paszke, Sam Gross, and Francisco et al Massa. Pytorch: An imperative style, high-\n\nperformance deep learning library. In NeurIPS, pp. 8026–8037, 2019.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n\nmodels are unsupervised multitask learners. 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations\n\ntoward training trillion parameter models. In SC, pp. 20, 2020.\n\nShaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell., 39(6): 1137–1149, 2017.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis., 115(3):211–252, 2015.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. In ICLR, 2015.\n\nSebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with memory. In\n\nNeurIPS, pp. 4452–4463, 2018.\n\nNikko Strom. Scalable distributed DNN training using commodity GPU cloud computing. In Inter-\n\nspeech, pp. 1488–1492, 2015.\n\nKe Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for\n\nhuman pose estimation. In CVPR, pp. 5693–5703, 2019.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers & distillation through attention. In ICML, pp. 10347–10357, 2021.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, In NeurIPS, pp. 6000–6010,\n\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017.\n\nGuanchu Wang, Zirui Liu, Zhimeng Jiang, Ninghao Liu, Na Zou, and Xia Hu. A concise framework\n\nof memory efficient training via dual activation precision. arXiv:2208.04187, 2022.\n\nSaining Xie, Ross B. Girshick, Piotr Doll ́ar, Zhuowen Tu, and Kaiming He. Aggregated residual\n\ntransformations for deep neural networks. In CVPR, pp. 5987–5995, 2017.\n\nDavid Junhao Zhang, Kunchang Li, Yali Wang, Yunpeng Chen, Shashwat Chandra, Yu Qiao, Luoqi Liu, and Mike Zheng Shou. Morphmlp: An efficient mlp-like backbone for spatial-temporal representation learning. In ECCV, pp. 230–248, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 COMPLETE CONVERGENCE ANALYSIS\n\nHere we prove convergence of DropIT with min-k dropping strategy. By scaling the learning rate with a factor of 1\n\nα , the gradient of min-k dropping is modeled as:\n\ngmin-k = ggd +\n\nβ α\n\nn(0, ξ2).\n\n(12)\n\nwhere n is zero-mean noise with a variance of ξ2, and α, β are varied each iteration.\n\nWe assume that the loss function F is L-smooth, i.e., F is differentiable and there exists a constant L > 0 such that\n\nF (y) ≤ F (x) + ⟨∇F (x), y − x⟩ +\n\nL 2\n\n∥y − x∥2,\n\n∀x, y ∈ Rd.\n\n(13)\n\nPerforming Taylor expansion we have:\n\nE[F (xt+1)] ≤ F (xt) − ⟨∇F (xt), xt+1 − xt⟩ +\n\nη2L 2\n\nE[∥∇F (xt)∥2]\n\n≤ F (xt) − η∥∇F (xt)∥2 +\n\nη2Lξ2β2 t\n2α2 t\n\nRearranging the terms of the above inequality and dividing by η\n\n2 , we obtain:\n\n∥∇F (xt)∥2 ≤\n\n2(F (xt) − E[F (xt+1)]) η\n\n+\n\nηLξ2β2 t\nα2 t\n\nSumming up from t = 1 to T and divided by T , we get:\n\n1 T\n\nT (cid:88)\n\nE\n\nt=1\n\n∥∇F (xt)∥2 ≤\n\n2(F (x1) − F (x∗)) T η\n\n+ ηLξ2 1\n\nT\n\nT (cid:88)\n\nt=1\n\nβ2 t\nα2 t\n\nwhere x∗ indicates an optimal solution.\n\n(14)\n\n(15)\n\n(16)\n\nA.2 MODELING MIN-k DROPPING GRADIENT WITH NONLINEAR FUNCTION\n\nWe can replace Eq. 8 (gradient model of min-k dropping) with nonlinear function and still achieve the same convergence as in Eq. 10.\n\nThe gradient is biased with min-k dropping, we assume it can be modeled as:\n\ngmin-k = ggd + βn(0, ξ2) + b,\n\n(17)\n\nwhere b is a bias and ||b||2 ≤ (1−α)||ggd||2. α and β varies each iteration, i.e., α = {α1, α2, ..., αt} and β = {β1, β2, ..., βt}.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAssuming the loss function F is L-smooth, we obtain:\n\nEF (xt+1) ≤ F (xt) − ⟨∇F (xt), xt+1 − xt⟩ +\n\nη2L 2\n\nE||∇F (xt) + b||2\n\n= f (xt) − η⟨∇F (xt), ∇F (xt) + b⟩ +\n\nη2Lξ2β2 t\n2\n\n≤ f (xt) − η⟨∇F (xt), ∇F (xt) + b⟩ +\n\n= f (xt) −\n\n= f (xt) −\n\n(cid:18)\n\n(cid:18)\n\nη 2\n\nη 2\n\n2⟨∇F (xt), ∇F (xt) + b⟩ − ||∇F (xt) + b||2\n\n||∇F (xt)||2 − ||b||2\n\n(cid:19)\n\n+\n\nη2Lξ2β2 t\n2\n\nη 2\n\n||∇F (xt) + b||2 + (cid:19)\n\nη2Lξ2β2 t\n2 η2Lξ2β2 t\n2\n\n+\n\n≤ f (xt) −\n\nηαt 2\n\n||∇F (xt)||2 +\n\nη2Lξ2β2 t\n2\n\nRearranging the terms of the above inequality and dividing by ηαt\n\n∥∇F (xt)∥2 ≤\n\n2(F (xt) − E[F (xt+1)]) ηαt\n\n2 , we obtain: ηLξ2β2 t\nαt\n\n+\n\nUsing a learning rate of η αt\n\ninstead, we have:\n\n∥∇F (xt)∥2 ≤\n\n2(F (xt) − E[F (xt+1)]) η\n\n+\n\nηLξ2β2 t\nα2 t\n\nSumming up from t = 1 to T and divided by T , we get:\n\n1 T\n\nT (cid:88)\n\nE\n\nt=1\n\n∥∇F (xt)∥2 ≤\n\n2(F (x1) − F (x∗)) T η\n\n+ ηLξ2 1\n\nT\n\nT (cid:88)\n\nt=1\n\nβ2 t\nα2 t\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\nwhere x∗ indicates an optimal solution. The convergence is exactly the same as in Appendix A.1.\n\ntest\n\nOptimizer\n\nSGD SGD\n\nDropIT\n\nDropIT (modeling nonlinearity as in A.2)\n\nLearning Rate\n\nη η\nα η\nα η\nα\n\nConvergence ∆F T η + ηLξ2 α ∆F T η + 1 T η + ηLξ2 1 T η + ηLξ2 1\n\nα ηLξ2 (cid:80)T\n\n(cid:80)T\n\nT\n\nT\n\n∆F\n\n∆F\n\nt=1\n\nt=1\n\nβ2 t\nα2 t\nβ2 t\nα2 t\n\nTable 6: Theoretical convergences of SGD and DropIT under L-smooth condition\n\nIn Table 6, we compare convergence of SGD and DropIT under various learning rates. Under a fixed learning rate, SGD and DropIT differ no both convergence speed (the 1st term in convergence) and error (the 2nd term in convergence formula). For a fair setting, we compare SGD with learning rate η and DropIT with learning rate η α . With a fixed convergence speed, DropIT theoretically achieves lower error.\n\nA.3 THEORETICAL ANALYSIS ON α AND β\n\nIn this section we compare the gradients of SGD and DropIT with min-k dropping. Note we slightly change the notation of the gradients from gsgd and gmink in the main paper to improve clarity for\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nelement-wise analysis. We denote the gradient of SGD as G and DropIT as G′. Both gradients are computed by an input tensor A and intermediate tensor Z. In DropIT we drop γ percent of the elements in A. Thus we have:\n\nG = A × Z G′ = (A ⊙ D) × Z,\n\n(22)\n\n(23)\n\nwhere ⊙ is element-wise multiplication and D is a dropping mask where each element is either 1 or 0.\n\nFrom an element-wise viewpoint, we rewrite the computation of gradients:\n\ngij =\n\ng′\n\nij =\n\n(cid:88)\n\nk (cid:88)\n\nk\n\naikzkj\n\naikdikzkj,\n\n(24)\n\n(25)\n\nwhere dik is a mask, i.e., dik is either 0 or 1 depending on aik.\n\nFor the simplicity of analysis, we assume A and Z are independent. Let μ be the mean value of A and c be the mean of dropped elements, after dropping, A ⊙ D has a mean of μ − c. Taking expectation over all possible inputs, we have:\n\nE[g′\n\nij] = E[αgij]\n\naikzkj\n\n=\n\n=\n\nμ − c μ\n\nμ − c μ\n\n(cid:88)\n\nk\n\ngij.\n\n(26)\n\nTherefore the bias caused by dropping is expected to be μ−c μ . Assuming the mean value of A is μ and the mean of dropped value is c, after dropping, D(A) has a mean of μ − c. Thus the bias caused by dropping is E[α] = μ−c μ . Recall that we drop elements with small absolute values. In the extreme case where every element in A has the same value as μ, c will reach the upper bound γμ. Therefore, E[α] ≥ 1 − γ.\n\nNow we analyze on noise and compute β. Due to the variation on input samples, we have noise in A and Z, which results in noise in G and G′. To highlight the noise, we rewrite a noisy element x as ̄x + nx, where ̄x is the mean value of x and nx is a zero-mean noise. Applying it to Eq.24 and Eq.25 we arrive at:\n\n ̄gij + ng =\n\n ̄g′\n\nij + ng′ =\n\n(cid:88)\n\nk (cid:88)\n\nk\n\n( ̄aik + na)( ̄zkj + nz)\n\ndik( ̄aik + na)( ̄zkj + nz).\n\nFocusing on the noise of gradients, we obtain:\n\nng =\n\nng′ =\n\n(cid:88)\n\nk (cid:88)\n\nk\n\n( ̄aiknz + ̄zkjna + nanz)\n\n(dik ̄aiknz + dik ̄zkjna + diknanz).\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\nRecall that dik is a mask depending on aik and therefore depending on ̄aik, thus from Eq.26 we have\n\nE[dik ̄aik] =\n\nμ − c μ\n\n ̄aik ≈ ̄aik > (1 − γ) ̄aik.\n\n(31)\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nDataset\n\nCIFAR-100\n\nImageNet\n\nMethod ResNet-18 (32 × 32) ResNet-18 (32 × 32) + DropIT (γ = 0.8) ViT-B/16 (224 × 224) ViT-B/16 (224 × 224) + DropIT (γ = 0.9) ResNet-18 (224 × 224) ResNet-18 (224 × 224) + DropIT (γ = 0.8) ViT-B/16 (224 × 224) ViT-B/16 (224 × 224) + DropIT (γ = 0.9)\n\nTop-1 Top-5 Memory (GB) 77.96 78.17 90.32 90.90 69.76 69.85 83.40 83.61\n\n648 598 20290×4 16052×4 2826 2600 20290×4 16056×4\n\n94.05 94.19 98.88 99.02 89.08 89.39 96.96 97.01\n\nTable 7: More results of different network architecture achieved by DropIT. ResNet-18 results are training from scratch, and ViT-B/16 are fine-tuning from public ImageNet-21k weights.\n\nBecause γ percent of D is 0 and the other 1 − γ percent of D is 1, we obtain E[dik] = 1 − γ. Plugging them in Eq.30 we have:\n\nE[ng′] = E[βng]\n\n ̄aiknz + (1 − γ) ̄zkjna + (1 − γ)nanz)\n\n(cid:88)\n\n(\n\n=\n\nμ − c μ\n\nk μ − c μ\n\n≤\n\n(cid:88)\n\n( ̄aiknz + ̄zkjna + nanz)\n\nk\n\n= E[αng],\n\n(32)\n\nwhere the inequality is satisfied due to c ≤ γμ. This result tells us E[β] ≤ E[α], which suggests DropIT with min-k dropping has a noise reduction effect and should converge better than SGD.\n\nA.4 RESNET-50 TRAINING FROM SCRATCH ON IMAGENET-1K\n\nWe follow torchvision training script to train ResNet with and without DropIT. No hyperparameters are changed. When γ = 70%, ResNet-50 with DropIT achieves 76.3 top-1 accuracy, slightly higher than baseline’s 76.1 accuracy, demonstrating the effectiveness of DropIT.\n\nA.5 MORE NETWORK RESULTS\n\nWe present more results of different network architectures as shown in Table 7. ResNet-18 are trained from scratch by 90 epochs on ImageNet, totally following torchvision reference code. ViT-B/16 is fine-tuned in 3 epochs from its ImageNet-21k pretrained weights. Our proposed DropIT can improve the accuracy for these setting with lower GPU memory cost.\n\nA.6 WHY DROPIT IS NOT USED FOR THE NETWORK FIRST & FINAL LAYERS\n\nWe do not apply DropIT to conv/fc layer if it is the first/final layer of the network. The reason is that this does not save memory:\n\n(1) The first layer: The logic of our DropIT on saving memory can be concluded as: creating a smaller tensor xdropped (i.e. by torch.topk) from input tensor x, then the input tensor x will be automatically released by python garbage collection. However, popular code style is like:\n\n1 dataloader = ... 2 loss_func = ...\n\n3 4 def model(x):\n\n5\n\n6\n\n7\n\n8\n\n9\n\nx = layer1(x) x = layer2(x) # x can be released with DropIT ... x = layeri1(x) # x can be released with DropIT x = layeri(x) # x can be released with DropIT, but cannot save maximum memory\n\n16\n\nPublished as a conference paper at ICLR 2023\n\n10\n\nreturn x\n\n11 12 for x, y in dataloader:\n\n13\n\n14\n\n15\n\nx = model(x) # x will not be released with DropIT loss_func(x, y).backward() ...\n\nAs we can see, in the dataloader loop, the input x to model can only be recycled when model running is finished. So, using DropIT in layer1 will not reduce maximum memory — instead, it will increase the maximum memory as DropIT created a new xdropped.\n\n(2) The final layer: it is easy to understand that DropIT using in the final layer has no effect on memory. See the code block, when running to layeri, the maximum memory should be layer1 ∼ layeri1 cached tensors plus x input to layeri. If we use DropIT at layeri, then there would be an extra xdropped produced, making the maximum memory even higher.\n\nA.7 HOW TO SELECT γ OF DROPIT\n\nFrom our experiments, we recommend γ = 70% for training from scratch and γ = 80, 90% for finetuning. As DropIT incurs memory cost for indexing, γ should be larger than 50% to be meaningful (assuming index data type is int32 with the same number of bits of float32 for activation). Empirically, we observe that γ is reflected consistently in both training loss and testing accuracy. A too-high γ which will bias the gradient will have training losses higher than the baseline. As such, an alternative way to select γ is to observe the training loss after a some iterations (e.g. 100); if it is lower than the baseline, then the testing accuracy is likely to improve as well.\n\nA.8 MORE EXPERIMENTAL DETAILS\n\nWe list the detailed key training hyper-parameters, though they are totally the same with the offical implementations:\n\n• DeiT-Ti, training from scratch, ImageNet-1k, w/wo DropIT 2: batch size 1024, AdamW optimizer, learning rate 10−3, weight decay 0.05, cosine LR schedule, 300 epochs, with auto mixed precision (AMP) training;\n\n• DeiT-S, finetuning from official DeiT-S ImageNet-1k weights, CIFAR-100, w/wo DropIT 3: batch size 768, SGD optimizer (momentum 0.9), learning rate 10−2, weight decay 10−4, cosine LR schedule, 1000 epochs, with AMP training;\n\n• DeiT-B, finetuning from official DeiT-B ImageNet-1k weights, CIFAR-100, w/wo DropIT 4: batch size 768, SGD optimizer (momentum 0.9), learning rate 10−2, weight decay 10−4, cosine LR schedule, 1000 epochs, with AMP training;\n\n• Faster R-CNN, finetuning from torchvision ResNet-50 ImageNet-1k weights (V1), COCO, w/wo DropIT 5: batch size 16, SGD optimizer (momentum 0.9), learning rate 0.02, weight decay 10−4, Multistep LR schedule (16,22 epochs), 26 epochs, without AMP training;\n\n• Mask R-CNN, finetuning from torchvision ResNet-50 ImageNet-1k weights (V1), COCO, w/wo DropIT 6: batch size 16, SGD optimizer (momentum 0.9), learning rate 0.02, weight decay 10−4, Multistep LR schedule (16,22 epochs), 26 epochs, without AMP training.\n\n2https://www.github.com/facebookresearch/deit/blob/main/README_deit.md 3https://www.github.com/facebookresearch/deit/issues/45 4https://www.github.com/facebookresearch/deit/issues/45 5https://www.github.com/pytorch/vision/tree/main/references/detection#\n\nfaster-r-cnn-resnet-50-fpn\n\n6https://www.github.com/pytorch/vision/tree/main/references/detection#\n\nmask-r-cnn\n\n17",
    "reference": "# Summary Of The Paper\n\nThe authors propose DropIT, where stashed intermediate tensors are pruned via a Top-K function and converted to a compressed sparse format. This sparsification reduces the memory consumption of stashing, enabling larger batch sizes and faster training. During the backward pass, the stashed tensors are first decompressed to dense representation since the sparsity is usually not high enough for sparse matmuls to be effective.\n\nThe authors show that by scaling the learning rate, DropIT can actually decrease the gradient noise.\n\nExperiments show that a variety of vision benchmarks such as DeiT and R-CNN can be trained on datasets like ImageNet and COCO with 70%-90% sparsity without reducing the final accuracy. This is a great result. However, wall-clock time improvements seem less impressive, without only about 10% run time improvement in Table 5.\n\nEDIT: raised score from 5 to 6 after rebuttal\n\n# Strength And Weaknesses\n\nStrenghts:\n - Straightforward and easy to implement method that seems fairly robust, at least on vision benchmarks\n\nWeaknesses:\n - The idea is not novel: it's already part of the GIST paper. GIST combines Min-K stashed activation compression with binarized stashed ReLU masks. The GIST paper is systems-focused and contains multiple techniques. This paper gives a more focused study of the Min-K sparsity technique and is still a useful result.\n - The memory savings is great but the actual run time savings seem unimpressive. Perhaps it would be better to focus on benchmarks which are much more memory-hungry such as Transformers. These models are typically trained with microbatching and increasing the microbatch size should grant a big performance boost.\n\nQuestion:\n - The proof that DropIT can decrease the gradient noise depends on Equation 8, where the noise introduced by the Min-K sparsification is modeled as $g_{min-k} = \\alpha g_{gd} + \\beta n(0, \\xi^2)$. It's not clear to me why this noise is zero-mean. In CNNs, we have non-symmetric activation functions like ReLU. Min-K sparsity would change the means of the stashed activations tensors, which in turn would change the means of the gradient update tensors. I am not an expert here so I would like to see the authors' response.\n\n[1]https://ieeexplore.ieee.org/abstract/document/8416872?casa_token=0XtaIex9huQAAAAA:RwuzO_eOgBsEms6Y3XESj66oOFSaihBdAT0vVXlq_YETd7xlO_ifa9feFTDT0wrGlU1KKM7D\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe authors give some details on how their pytoch implementation works, and combined with the simplicity of the technique it should not be hard to reproduce the results. The paper is also clear and well-written.\n\nThe theoretical proof seems to be based on shaky assumptions. I'm not an expert in this area so I would like to see the authors' response.\n\nThe novelty of the idea does not seem very high. I already mentioned the GIST paper and there may be other papers which study pruning on stashed activations.\n\n# Summary Of The Review\n\nDropIT proposes Min-K pruning of stashed activations to save memory. This isn't a novel idea and has been investigated in at least the GIST paper[1]. However, this paper is still useful as it presents an isolated study on this technique and presents impressive results on memory savings for vision benchmarks.\n\nI would raise the score if I see additional discussion regarding the assumptions for the theoretical proof and results on more memory-intensive benchmarks.\n\n[1] https://ieeexplore.ieee.org/abstract/document/8416872?casa_token=0XtaIex9huQAAAAA:RwuzO_eOgBsEms6Y3XESj66oOFSaihBdAT0vVXlq_YETd7xlO_ifa9feFTDT0wrGlU1KKM7D\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nQUARK: A GRADIENT-FREE QUANTUM LEARNING FRAMEWORK FOR CLASSIFICATION TASKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nAs more practical and scalable quantum computers emerge, much attention has been focused on realizing quantum supremacy in machine learning. Existing quantum ML methods either (1) embed a classical model into a target Hamiltonian to enable quantum optimization or (2) represent a quantum model using variational quantum circuits and apply classical gradient-based optimization. The former method leverages the power of quantum optimization but only supports simple ML models, while the latter provides flexibility in model design but relies on gradient calculation, resulting in barren plateau (i.e., gradient vanishing) and frequent classical-quantum interactions. To address the limitations of existing quantum ML methods, we introduce Quark, a gradient-free quantum learning framework that optimizes quantum ML models using quantum optimization. Quark does not rely on gradient computation and therefore avoids barren plateau and frequent classical-quantum interactions. In addition, Quark can support more general ML models than prior quantum ML methods and achieves a dataset-sizeindependent optimization complexity. Theoretically, we prove that Quark can outperform classical gradient-based methods by reducing model query complexity for highly non-convex problems; empirically, evaluations on the Edge Detection and Tiny-MNIST tasks show that Quark can support complex ML models and significantly reduce the number of measurements needed for discovering near-optimal weights for these tasks.\n\n1\n\nINTRODUCTION\n\nQuantum computing provides a new computational paradigm to achieve exponential speedups over classical counterparts for various tasks, such as cryptography (Shor, 1994), scientific simulation (Tazhigulov et al., 2022), and data analytics (Arute et al., 2019). A key advantage of quantum computing is its ability to entangle multiple quantum bits, called qubits, allowing n qubits to encode a 2n-dimensional vector, while encoding this vector in classical computing requires 2n bits.\n\nInspired by this potential, recent work (Jaderberg et al., 2022; Macaluso et al., 2020b; Torta et al., 2021; Kapoor et al., 2016; Bauer et al., 2020; Farhi & Neven, 2018a; Schuld et al., 2014; Cong et al., 2019b) has focused on realizing quantum speedups over classical algorithms in the field of supervised learning. Existing quantum ML work can be divided into two categories: classical model with quantum optimization (CMQO) and quantum model with classical optimization (QMCO). First, CMQO methods embed a classical ML model jointly with the optimization problem into a target Hamiltonian and optimize the model using quantum adiabatic evolution (QAE) (Finnila et al., 1994) or quantum approximate optimization algorithm (QAOA) (Farhi et al., 2014; Torta et al., 2021). As the transition between a classical model and the target Hamiltonian only applies to loworder polynomial activations (see Figure 2), CMQO methods do not support ML models with nonlinear activations that cannot be represented in low-order polynomial (e.g., ReLU). Second, QMCO methods optimize variational quantum models 1 by iteratively performing gradient descent using classical optimizers. QMCO methods are fundamentally limited by barren plateau (i.e., gradient vanishing (McClean et al., 2018)) and the high cost of frequent quantum-classical interactions.\n\nTo address the limitations of existing quantum ML methods, we introduce Quark, a gradient-free quantum learning framework for classification tasks that optimizes quantum models with quantum\n\n1They are also known as variational quantum circuits (VQC)-based models in the quantum literature.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: An overview of the Quark optimization framework. Each horizontal line indicates a qubit, and each box on these lines represents one or multiple quantum gates applied on these qubits. Quark’s optimization pipeline includes three stages: (1) Ψ0 preparation, which initializes RW , RD, RO and performs model’s forward processing UM , (2) amplitude amplification using a Grover-based algorithm, and (3) weights measurement. Quark uses K-parallel datasets (KPD) to maximize the probability of observing highly accurate weights in each measurement. Darker bars in the probability plots denote weights with higher accuracies.\n\noptimization (QMQO). Figure 1 shows an overview of Quark. A key idea behind Quark is entangling the weight 2 of an ML model (i.e., the RW register in Figure 1) and the encoded dataset (i.e., the RD register in Figure 1) in a quantum state, where model weights that achieve optimal classification accuracy on the training dataset can be observed with the highest probabilities in a measurement. Therefore, users can obtain highly accurate model weights by directly measuring the updated RW weight register. To maximize the probability of observing optimal weights, we introduce two key techniques.\n\nAmplitude amplification. Quark uses a Grover-based mechanism to iteratively update the probability distribution of weights based on their training accuracies. As a result, the probability of observing weights with higher accuracy increases after each Grover iteration, as shown in Figure 1.\n\nK-parallel datasets (KPD). Applying amplitude amplification on one dataset results in a linear amplification scenario where the measuring probability of each weight is proportional to its training accuracy J(wi). We further introduce K-parallel datasets, a technique to enable exponential amplification. Specifically, by entangling k identical training datasets with model weights in parallel (using k × RD, as shown in Figure 1), the probability of observing weight wi in a measurement is proportional to J(wi)k. Therefore, as k increases, the optimized probability distribution of weights gradually converges to the optimal weights.\n\nCompared with CMQO methods, Quark provides more flexibility in model design by composing models directly on quantum circuits and therefore supports a broader range of ML models. Compared with QMCO methods, Quark does not require gradient calculation and therefore does not suffer from barren plateau. Quark avoids frequent classical-quantum interactions by realizing both model design and optimization fully on quantum. Besides, by using basis encoding for the training dataset, Quark supports non-linear operations (e.g., ReLU) in its model architecture, and the optimization complexity is independent of the training dataset size.\n\nTheoretically, we compare model query complexity3 between Quark and gradient-based methods on a balanced C-way classification task, and prove that Quark can outperform gradient-based methods by reducing model query complexity for highly non-convex problems. In addition, we prove that using K-parallel datasets can further reduce model query complexity under certain circumstances.\n\nSimulations on two tasks (i.e., Edge Detection and Tiny-MNIST) show that Quark supports complex ML models, which can include quantum convolution, pooling, fully connected, and ReLU layers.\n\n2Throughout the paper, we use the term weight to refer to the set of all trainable parameters of a model. 3Number of model forward being called\n\n2\n\nRWROHUDUMUs⊤UΨ0Us⊤UΨ0⋯UcombUcombM|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩UDRDk× preparationΨ0Amplitude amplificationMeasureAmplitude amplificationK Parallel DatasetsWeightsProbHHHHMMMMWeightsProbWeightsProbUnder review as a conference paper at ICLR 2023\n\nFigure 2: Comparison between CMQO, QMCO, and Quark (QMQO).\n\nIn addition, Quark can significantly reduce the number of measurements needed for discovering a near-optimal weight by applying amplitude amplification and KPD.\n\nContributions. This paper makes the following contributions:\n\n• We propose Quark, a gradient-free quantum learning framework for classification tasks that optimizes quantum ML models with quantum optimization. Quark avoids barren plateau and frequent classical-quantum interactions, supports more general models than prior quantum ML frameworks, and achieves a dataset-size-independent optimization complexity.\n\n• Theoretically, we prove that Quark can outperform gradient-based methods by reducing model query complexity for highly non-convex problems and that using KPD can further reduce model query complexity.\n\n• Empirically, we show that Quark can support complex ML models and significantly reduce the number of measurements needed for discovering a near-optimal weight for the Edge Detection and Tiny-MNIST tasks.\n\n2 RELATED WORK\n\nFigure 2 compares Quark with existing quantum ML approaches.\n\n2.1 CLASSICAL MODEL WITH QUANTUM OPTIMIZATION\n\nExisting CMQO methods aim at solving classical ML problems with quantum optimization techniques by leveraging the advantage of quantum parallelism (Nielsen & Chuang, 2002). Based on the well-established algorithmic foundation in quantum annealing (Finnila et al., 1994; Kadowaki & Nishimori, 1998; Brooke et al., 1999; Santoro et al., 2002; Santoro & Tosatti, 2006) and adiabatic quantum computing (Farhi et al., 2001; Albash & Lidar, 2018), prior work (Denil & De Freitas, 2011; Dumoulin et al., 2014; Adachi & Henderson, 2015) attempts for the quantum restricted Boltzmann machine (RBM) by formulating RBM as an Ising model (Cipra, 1987). Inspired by the quantum approximate optimization algorithm (QAOA) (Farhi et al., 2014), Torta et al. (2021) embeds a single binary perceptron layer into a target Hamiltonian to search for optimal weights. However, CMQO methods are limited by the locality restriction of the target Hamiltonian and can only embed models with low-order polynomial activations (e.g., square). This limitation prevents CMQO methods from supporting practical deep learning architectures, which generally contain non-polynomial activations such as ReLU and sigmoid.\n\nSimilar to Quark, Kapoor et al. (2016) also uses Grover’s algorithm to find a hyperplane that can perfectly separate the training dataset. However, this method only applies to an idealistic setup where a hyperplane with perfect classification exists in its search space. Besides, the method cannot adapt to generic model architectures other than single-layer perceptrons.\n\n3\n\nQuantum ComputingClassical ComputingImmune to Barren-PlateauSupport Diverse ML modelsQuantum Model Classical Optimizer DataVQC ModelAmplitude EncodingW=W−η∇J(W)∇J(W)Gradient-based OptimizerLimited to linear operationsQuark (Quantum Model Quantum Optimizer)DataBasis EncodingQuark ModelGrover-based OptimizerDataClassical ModelHamiltonian EmbeddingQuantum Annealing based optimizerLimited to low-order polynomial activationsClassical Model Quantum Optimizer Under review as a conference paper at ICLR 2023\n\n2.2 QUANTUM MODEL WITH CLASSICAL OPTIMIZATION\n\nMotivated by the recent advances in variational quantum algorithms (VQAs) (Cerezo et al., 2021), QMCO methods use variational quantum circuits (VQC) (Benedetti et al., 2019) to represent the trainable parameters of an ML model. Havl ́ıˇcek et al. (2019); Schuld & Killoran (2019) use VQC as a variational feature map to reproduce linear support vector machines (SVM) and kernel methods on quantum circuits, which can outperform classical counterparts under certain circumstances (Liu et al., 2021).\n\nBesides conventional ML methods, recent work has also explored the feasibility of classical neural networks on quantum circuits (Massoli et al., 2022). Farhi & Neven (2018b); Macaluso et al. (2020a); Killoran et al. (2019) use VQC as building blocks for their quantum perceptron models with a classical gradient-based optimizer. Quantum dissipative neural network (Beer et al., 2020) (QDNN) and quantum convolutional neural network (Cong et al., 2019a) (QCNN), on the other hand, move a step forward towards more complicated neural architectures. QDNN enlarges its model space by applying unitary operators on both the input and output qubits, while QCNN uses a measurement-controlled operation to enable non-linear operations. However, McClean et al. (2018) shows that the barren plateau phenomenon commonly exists in VQC-based methods, where gradients vanish exponentially with the model size. Though Beer et al. (2020) claims to design a VQC model immune to barren plateau, Sharma et al. (2022) contradicts such claim with analytical proof. Though Du et al. (2021) uses a Grover algorithm as part of their method, they still require VQC as their model building blocks that require gradients update.\n\nBesides, due to amplitude-based data encoding, VQC-based methods in general suffer from a linear dependency with respect to dataset size in terms of model query complexity during training. Another drawback for amplitude encoding is that due to the unitary constraint of quantum transformations, non-linear operations are hard to implement for VQC-based methods. In contrast, our method uses basis encoding that concerns only qubits state transformation rather than amplitude transformation, which enables more efficient model query complexity and more general non-linear transformations.\n\n3\n\nPRELIMINARIES\n\n3.1 NOTATIONS\n\nLet D = {(xi, yi)}i∈N denote a training dataset, where xi ∈ {0, 1}dx is the binarized feature vector associated with the i-th sample, and yi ∈ {0, 1}dy is its label. Let ˆy = f (w, ˆx) : {0, 1}dw × {0, 1}dx → {0, 1}dy denote our model parameterized by w ∈ {0, 1}dw . Given an objective l(ˆy, y), our goal is to find a near-optimal w∗ that minimizes/maximizes the overall objective J(w) = 1 (xi,yi)∈D l(f (w, xi), yi). We focus on classification tasks and use the objective l(ˆy, y) = 1(ˆy = y). We use | · | to denote the cardinality of a set and absolute value of a scalar, and use ∥ · ∥2 and ∥ · ∥∞ to denote the L2-norm and infinity norm of a vector. Finally, we use ⊗ to denote tensor product, and use ¬, ⊕, ∧, and ∨ to denote NEGATE, XOR, AND, and OR in logical expressions, respectively.\n\n(cid:80)\n\nN\n\n3.2 QUANTUM BASICS\n\nA bit in the quantum regime, called a qubit, is represented by a super-position of |0⟩ and |1⟩, which is formally defined as |z⟩ = α|0⟩ + eiφβ|1⟩, where α and β are the amplitudes, and eiφ is the relative phase. Furthermore, the rule also enforces ⟨z|z⟩ = 1 where ⟨z| is the conjugate transpose of |z⟩. In an n-qubit system, a quantum state is represented as a superposition of 2n basis states. For computational simplicity, we will be using the basis states that are spanned by {|0⟩, |1⟩}n and denote the superposition of a n-qubits state as |Ψ⟩ = (cid:80)2n−1 2n |i⟩ where |i⟩ is the corresponding computational basis. We thus use |wi⟩ ∈ {|0⟩, |1⟩}dw to denote weight basis state, |xj, yj⟩ for the entangled data basis state where |xj⟩ ∈ {|0⟩, |1⟩}dx is the feature and |yj⟩ ∈ {|0⟩, |1⟩}dy the label.\n\n1√\n\ni=0\n\nAs a comparison, in the classical computing regime, each operation can only be applied to one state.\n\n4 METHODOLOGY\n\nTo realize quantum supremacy, we circumvent the overhead induced by frequent classical-quantum interactions and the gradient calculation step in the VQC-based methods, which may result in barren\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Toy illustration of Quark’s insight, where the dataset {(x0 = 0, y0 = 0), (x1 = 1, y1 = 1)} is constructed by oracle function g(x) = 0 ⊕ x. We define our learner model as o = f (x, w) = w ⊕ x. The colored qubit strings stands for the states activated by model forward (cyan for |w⟩ = 0 and yellow for |w⟩ = 1). violet bars stand for solution states measuring probability, gray stands for non-solution states measuring probability.\n\nplateau as the circuit depth increases. In addition, as VQC-based methods use the amplitude encoding scheme for data encoding, non-linear operations are hard to implement, while their training time can linearly depend on the training dataset size.\n\nTo this end, we designed Quark in a QMQO fashion, which (1) achieves gradients-free optimization over the entire weight distribution, (2) uses basis encoding to achieve potential speedup as sample size scales up, and (3) enables a flexible quantum model design that can easily incorporate nonlinearities. Figure 1 shows an overview of the quantum circuit design in Quark.\n\nAs we are using basis encoding over the entire data distribution, we initialize the data register RD as |D⟩ = (cid:80) |xi, yi⟩. To initialize a model’s weights register RW , we construct a\n\n1√\n\n(xi,yi)∈D\n\n|D|\n\nuniform state by applying the Hadamard gate on each weight qubit: |W ⟩ = (cid:80)2dw −1 |wi⟩. In addition, Quark includes an auxiliary register RO for storing intermediate results and model’s output, which is initialized as |O⟩ = |0⟩do. Therefore, the initial state is represented as |W ⟩ ⊗ |D⟩ ⊗ |O⟩. By encoding the quantum model architecture as a unitary matrix UM , the model’s forward processing is defined as:\n\n1√\n\n2dw\n\ni=0\n\nUM (|W ⟩ ⊗ |D⟩ ⊗ |O⟩) =\n\n(cid:88)\n\n1\n\ndw 2\n\n2\n\ni\n\n(cid:88)\n\n|wi⟩\n\nj\n\n1 (cid:112)|D|\n\n|xj, yj⟩|f (wi, xj)⟩\n\n(1)\n\n4. Given the above expression where f (wi, xj) is the model’s output for weight wi and sample xj for the model forward processing, the key insight for Quark is to update the probability of observing weight w in a measurement based on its training objective J(w). This is achieved through Grover’s algorithm by defining the solution state space S = {|wi⟩|xj, yj⟩|f (wi, xj)⟩ | yj = f (wi, xj)}. Appendix A.1 includes an introduction to the original Grover’s algorithm. After amplitude amplification, we observe a model weight by measuring the weight register RW .\n\nTo further illustrate our insight, Figure 3 shows a toy example, where the underline oracle function that generates the training data is g(x) = 0 ⊕ x. Using one qubit for x, the training dataset is constructed as {(x0 = 0, y0 = 0), (x1 = 1, y1 = 1)}. By defining the learner model as o = f (x, w) = w ⊕ x, where w is the trainable parameter, Quark includes four qubits (i.e., |w⟩, |x⟩, |y⟩, and |o⟩). For initial state preparation, we simply go through the model forward circuit with a uniform weight initialization that result in an uniform amplitude state, as shown on the left of Figure 3. As no optimization happens at this stage, the probabilities for observing |w⟩ = |0⟩ and |w⟩ = |1⟩ in a measurement are equivalent. After amplitude amplification, Quark reaches an optimized amplitude state (shown on the right of Figure 3), where the probability of observing the optimal weight |w⟩ = |0⟩ in a measurement is much higher.\n\nFor the rest of this section, Section 4.1 introduces Quark’s on-circuit model design philosophy, Section 4.2 describes the Grover-based method for amplitude amplification, and Section 4.3 introduce\n\n4We omit the model’s intermediate results |O⟩ for simplicity.\n\n5\n\n|w⟩|x⟩|y⟩|o⟩0 0 0 00 0 0 1...0 1 1 00 1 1 11 0 0 01 0 0 1...1 1 1 01 1 1 1Prob0 0 0 00 0 0 1...0 1 1 00 1 1 11 0 0 01 0 0 1...1 1 1 01 1 1 1Amplitude Amplification|w⟩=|0⟩|w⟩|x⟩|y⟩|o⟩|w⟩=|0⟩|w⟩=|1⟩|w⟩=|1⟩.25.25.25.25P()=0.5P()=0.5.49.49.01.01P()=0.98P()=0.02Under review as a conference paper at ICLR 2023\n\n(a) Convolution\n\n(b) MaxPool\n\nFigure 4: Logical gates illustration of quantum module design. (a) Convolution: for a 1 × 3 Conv kernel with weights W = [W1, W2, W3] and input Xi = [Xi1, Xi2, Xi3], the output is given by Oi = (¬(W1 ⊕ Xi1)) ∧ (¬(W2 ⊕ Xi2)) ∧ (¬(W3 ⊕ Xi3)). (b) MaxPooling: for a input Xi = [Xi1, Xi2, Xi3] the max pooling output is given by Oi = Xi1 ∨ Xi2 ∨ Xi3.\n\nK-Parallel Dataset (KPD), a technique that enables exponential amplification to further improve the optimization algorithm.\n\n4.1 MODEL DESIGN\n\nThe model circuit UM can be designed through arbitrary combinations of base gates (CNOT, Rotational gates, Hadamard gate, etc.), which can then entangle |W ⟩, |D⟩ to get a trainable model.\n\nLeveraging the flexibility of basis encoding, Quark is capable of realizing modules used in classical deep models. Figure 4 illustrates two Quark modules (Conv, MaxPool) used in our experiments. Besides Convolution and MaxPooling, Quark can support more diverse operations given enough qubits. We include the demonstration of the Fully Connective and ReLU modules in Appendix A.4.\n\nDuring training, we encode the entire data distribution through basis encoding so we can apply model forward simultaneously for all data samples. During inference, we encode each sample as a single state |ˆx⟩. Prediction can be obtained by measurements over the output register after the model forward step UM |w∗⟩|ˆx, 0⟩|0⟩ = |w∗⟩|ˆx, 0⟩|f (w∗, ˆx)⟩, where w∗ is the measured optimal weight.\n\n4.2 AMPLITUDE AMPLIFICATION\n\nTo increase the probability of measuring the weights with the highest classification accuracy, we use Grover’s algorithm to amplify the amplitude for any state |wi⟩|xj, yj⟩|f (wi, xj)⟩ ∈ S, thus the measuring probability of optimal weights would be amplified the most. In this section, we will formally define the unitary operators we need for Grover’s update, followed by the complexity analysis of the optimization algorithm.\n\nFor conventional Grover’s algorithm, we need two reflection unitaries, namely Us⊤ , UΨ0 , where Us⊤ is the reflection against the non-solution sub-space and UΨ0 is the reflection against the initial state. In our case, by defining solution state\n\n|s⟩ =\n\n(cid:88)\n\n|wi⟩|xj ,yj ⟩|f (wi,xj )⟩∈S\n\n1 (cid:112)|S|\n\n|wi⟩|xj, yj⟩|f (wi, xj)⟩\n\nand initial state\n\nΨ0 =\n\n(cid:88)\n\ni,j\n\n1 (cid:112)2dw |D|\n\n|wi⟩|xj, yj⟩|f (wi, xj)⟩\n\n, the Grover operator can be easily built from Ucomb = UΨ0Us⊤ where Us⊤ = I − 2|s⟩⟨s|, UΨ0 = 2|Ψ0⟩⟨Ψ0| − I.\n\nNow that we have the Grover operator well-defined, we can formally do analysis on the algorithm in terms of model query complexity.\n\n(cid:82)\n\nJ(wi)dδ\n\nTheorem 1 By defining α =\n\nwi∈W J(wi)dδ as the probability to measure an ε-optimal solution after the Grover’s update, the model query complexity for getting an ε-optimal solution wi ∈ Wε on a balanced C-way classification task is O(\n\n√\n\nC\n\nwi∈Wε (cid:82)\n\nα ).\n\n6\n\nX11O1W1X31X21X12X32X22W2W3O2∧¬¬¬¬¬¬∧⊕⊕⊕⊕⊕⊕X11X12X13X21X22X23W1W2W3*=O1O2X1OX2X3∨X1X2X3max()=OUnder review as a conference paper at ICLR 2023\n\nJ(wi) is the objective value (accuracy) for weights wi, W is the complete weight space, and Wε is the ε-optimal weight subspace. The proof of Theorem 1 is included in Appendix A.3.1, it follows\n\nfrom the analysis that we need O(\n\nC) Grover iterations per measurement and O(\n\n(cid:82)\n\nmeasurements in expectation for sampling an ε-optimal weight.\n\nwi∈Wε\n\n(cid:82) wi∈W J(wi)dδ\n\nJ(wi)dδ )\n\n√\n\n(cid:82) wi∈W dδ\n\nNotice that the complexity of our method does not depend on sample size N . As we are sampling solutions from an optimized distribution for a general non-convex problem, our method does depend\n\n(cid:82)\n\nwi∈W J(wi)dδ\n\non an O(\n\nJ(wi)dδ ) term. However, to achieve an ε-optimal solution on a general non-convex problem, the worst case scenario for VQC-based methods with gradient-based optimizers needs\n\nwi∈Wε\n\n(cid:82)\n\n(cid:82)\n\nO(\n\ndδ ) iterations to sample initial points lie within convex regions that contain ε-optimal solutions. With per iteration gradient evaluation cost that is in the order of O(N ), this gives an overall\n\nwi∈Wε\n\ncomplexity of O(\n\ndδ ) C ≪ N in practice. Our method provides a speed-up for finding an ε-optimal solution in the\n\ndδ N ) for VQC-based methods. Since O(\n\nJ(wi)dδ ) < O(\n\nwi∈Wε\n\nwi∈Wε\n\nwi∈Wε\n\n(cid:82)\n\n(cid:82)\n\n(cid:82)\n\n√\n\n(cid:82) wi∈W J(wi)dδ\n\n(cid:82)\n\nwi∈W dδ\n\n(cid:82)\n\nwi∈W dδ\n\nand general non-convex setup.\n\nIn addition, as Quark does not require gradients calculation, it will not suffer from the notorious problem of barren plateau. For more idealistic case of convex problems, Garg et al. (2020) has given a proof of no quantum speed-ups can be obtained in this case with no exception of our method.\n\n4.3 K-PARALLEL DATASETS (KPD)\n\nA naive implementation can only achieve p(wi) ∝ J(wi), which requires more measurements for sampling ε-optimal solution. To this end, we further extend our method to achieve p(wi) ∝ J(wi)k through K-Parallel Dataset (KPD), as shown in Figure 1 when k > 1. The intuition is similar to simulated annealing, by updating weights distribution proportional to J(wi)k, the probability mass would gradually converge to the global optimal solutions.\n\nWe do so by concatenating the same dataset K times for increasing solution states in the order of K as:\n\n(cid:32)\n\nUM\n\n|W ⟩\n\nK−1 (cid:79)\n\nk=0\n\n(cid:33)\n\n(|Dk⟩ ⊗ |Ok⟩)\n\n=\n\n(cid:88)\n\ni\n\n1 dw 2\n\n2\n\n|wi⟩\n\nK−1 (cid:89)\n\n\n\n(cid:88)\n\n\n\nk=0\n\nj\n\n1 (cid:112)|Dk|\n\n\n\n|xj, yj⟩|f (wi, xj)⟩\n\n\n\nthe solution set is now defined as:\n\nSK : {|wi⟩\n\nK−1 (cid:79)\n\nk=0\n\n|xjk , yjk ⟩|f (wi, xjk )⟩ |\n\nK−1 (cid:94)\n\nk=0\n\n(yjk = f (wi, xjk ))}\n\nWe then update |s⟩, |Ψ0⟩, Ucomb accordingly as in Appendix A.2. Now the ratio of solution states between different weights can grow exponentially in terms of K.\n\n(cid:82)\n\ndδ\n\nwi∈Wε\n\nTheorem 2 By defining β =\n\ndδ as the volume ratio between ε-optimal weight subspace |Wε| and non-ε-optimal weight subspace |W − Wε|, the model query complexity for getting an ε-optimal solution wi ∈ Wε on a balanced C-way classification task with k-parallel dataset is O((1 + βk−1( 1\n\nwi∈W/Wε\n\n2 ).\n\n(cid:82)\n\nα − 1)k)kC k\n\nThe proof of Theorem 2 is included in Appendix A.3.2. Here, the trade-off is that the number\n\nof Grover iterations needed per measurement grows to O\n\n. As we also need to apply the\n\nmodel forward for each dataset, the KPD Grover complexity in terms of model query complexity is now O(kC k 2 ). On the other hand, as we are converging to the global solution, the number of measurements needed can be reduced from O( 1 α − 1)k) as β < α. Thus the optimal value for k depends on the specification of α, β, C.\n\nα ) to O(1 + βk−1( 1\n\n(cid:17)\n\n(cid:16)\n\nC k\n\n2\n\nTheorem 3 Given α, β, C, the optimal value of k is k∗ = ⌊log α\n\n1\n\nα ⌋ + 1 if α\n\nβ ≥ m\n\n√\n\n1 m−1\n\nC where\n\nβ\n\nm = ⌊log α\n\nβ\n\n1\n\nα ⌋ + 1 else k∗ = 1.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Initial distribution\n\n(b) Amplitude ampl. w/ 1-PD (c) Amplitude ampl. w/ 4-PD Figure 5: Amplitude ampl. + KPD on Edge Detection. normalized accuracy: ˆJ(wi) = J(wi)\n\n(cid:80)\n\ni J(wi) .\n\nThus for cases where m ness of KPD under certain circumstances. We include the proof of Theorem 3 in Appendix A.3.3.\n\nα , we have our optimal k > 1 which proves the effective-\n\nC ≤ α\n\nβ ≪ 1\n\n√\n\n1 m−1\n\n5\n\nEXPERIMENTS\n\nIn this section, we empirically verify the effectiveness of Quark on two tasks, namely Edge Detection and Tiny-MNIST. Algorithm 1 formally states the pipeline of Quark5. Instead of leveraging the approximated iteration number from theoretical derivations for Grover’s update, we use a more precise but efficient estimation in practice to achieve a more accurate amplitude amplification effect. We include the detail of Grover iteration number estimation in Appendix A.5.1. Notice that most of the simulation results are obtained numerically since we have no access to large scale quantum devices that fit our setup. However, we do use Qiskit Aer (Anis & et al., 2021) to verify the reproducibility of our numerical simulation results on models that are applicable, results are included in Appendix A.6.\n\nAlgorithm 1: Quark’s optimization pipeline. Input: Data Oracle: UD; Model Oracle: UM ; Objective Oracle: UL; Number of parallel\n\ndataset: k; Measurement budget: m; Weights buffer: B\n\nOutput: Optimized Model Weight: w∗\n\ng, Ucomb, |Ψ0⟩ = Preprocessing(UD, UM , UL, k) ;\n\nGrover operator Ucomb and initial state |Ψ0⟩\n\n// get Grover iteration g,\n\nfor i = 0; i < m; ++i do\n\nfor j = 0; j < g; ++j do\n\n|Ψj+1⟩ = Ucomb|Ψj⟩ ;\n\nwi = Measure(|Ψg⟩) ; B.add(wi);\n\nreturn arg maxw∈B(Evaluate(w))\n\n5.1 EDGE DETECTION\n\n// Grover update\n\n// Weights measurement on RW\n\nFor Edge Detection, our goal is to identify if a 3 × 3 binary matrix has 1) both vertical and horizontal lines 2) vertical lines only 3) horizontal lines only 4) no lines, where a line is defined by three consecutive ’1’s in a row or column. Thus the task is a 4-way classification task with 512 instances. We split the 512 samples into a training set with 400 randomly selected instances and a test set with the rest. We use a Quantum Convolutional Model that consists of several 1 × 3 convolution kernels with Maxpooling modules as described in Figure 4 for this task. Results are demonstrated in Figure 5 (normalized accuracy: ˆJ(wi) = J(wi) i J(wi) ), from which we can clearly see a linear relationship between model accuracy and optimized weights distribution using 1-PD. For 4-PD, we can observe that weights distribution further concentrates on the global optimal solutions.\n\n(cid:80)\n\nIndeed, as we concatenate more training datasets, the probability mass will gradually converge to the optimal solutions, thus reducing the number of measurements we need significantly. We include the results for 2-PD and 3-PD in Appendix A.5.3.\n\n5Preprocessing() and Evaluate() are included in Appendix A.5.1\n\n8\n\n:HLJKW0HDVXULQJ3UREDELOLW\\QRUPDOL]HGDFFXUDF\\:HLJKW0HDVXULQJ3UREDELOLW\\QRUPDOL]HGDFFXUDF\\:HLJKW0HDVXULQJ3UREDELOLW\\QRUPDOL]HGDFFXUDF\\Under review as a conference paper at ICLR 2023\n\n(a) Initial Distribution\n\n(b) Amplitude ampl. w/ 1-PD (c) Amplitude ampl. w/ 4-PD Figure 6: Amplitude ampl. + KPD on Tiny-MNIST. normalized accuracy: ˆJ(wi) = J(wi)\n\n(cid:80)\n\ni J(wi) .\n\n5.2 TINY-MNIST\n\nFor Tiny-MNIST, we down-sample original 28 × 28 images from MNIST to 3 × 3 and remove duplicated samples. Due to device limitation, we only consider classes 1, 2, 7, which makes this task a 3-way classification task. We use Weighted Mask modules (details are included in Appendix A.4) with MaxPooling modules to compose our model, which can achieve 86.08% training accuracy and 82.61% testing accuracy by the best model in the search space. Whereas a well-optimized classical single layer perceptron model can achieve 85% in training accuracy and 80% test accuracy.\n\nSimilarly, as shown in Figure 6 (normalized accuracy: ˆJ(wi) = J(wi) i J(wi) ), a linear relationship between model accuracy and optimized weights distribution can be observed using 1-PD, while 4PD can further increase the probability for sampling optimal solutions. We also include the results of 2-PD and 3-PD on Tiny-MNIST in Appendix A.5.4.\n\n(cid:80)\n\nBesides distribution evolution, we also statistically demonstrate the relationship between measured top-1 model’s test accuracy and measurements budget using uniform random sampling, 1-PD, and 4-PD respectively in Figure 7. In order to achieve equally well-performed model, 4-PD only requires ∼ 30 shots on Edge Detection task for a mean test accuracy > 98% while uniform random sampling needs ∼ 900 (∼ 30× more) shots with much higher variance. Similar trend can be observed on Tiny-MNIST where 4-PD only requires ∼ 1000 shots for a mean test accuracy > 76.5% while uniform random sampling needs ∼ 20000 (∼ 20× more) shots with higher variance. We include the training ones in Appendix A.5.5.\n\n(a) Edge Detection\n\n(b) Tiny-MNIST\n\nFigure 7: The mean ± std of the test accuracy of the best discovered model with different measurement budget (in shots). URS shows the result where the weights are uniformly random sampled.\n\n6 CONCLUSION\n\nIn this paper, we propose a new quantum learning framework Quark that does not involve gradients calculation and operates in a fully-quantum fashion. Acknowledging the notorious problem of barren plateaus from VQC based methods, Quark shed some lights on circumventing this phenomenon through a gradient-free optimization pipeline. Quark also enables a more general set of module design due to basis encoding, so that non-linear operations can be easily implemented. Theoretically, we present some evidences in terms of model query complexity for Quark to demonstrate trade-offs between VQC based methods and Quark. Empirically, we have verified the effectiveness of Quark through numerical simulations on Edge Detection and Tiny-MNIST.\n\n9\n\n:HLJKW0HDVXULQJ3UREDELOLW\\HQRUPDOL]HGDFFXUDF\\:HLJKW0HDVXULQJ3UREDELOLW\\HQRUPDOL]HGDFFXUDF\\:HLJKW0HDVXULQJ3UREDELOLW\\HQRUPDOL]HGDFFXUDF\\PHDVXUHPHQWEXGJHWVKRWVWHVWDFFXUDF\\8563'3'PHDVXUHPHQWEXGJHWVKRWVWHVWDFFXUDF\\8563'3'Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSteven H Adachi and Maxwell P Henderson. Application of quantum annealing to training of deep\n\nneural networks. arXiv preprint arXiv:1510.06356, 2015.\n\nTameem Albash and Daniel A Lidar. Adiabatic quantum computation. Reviews of Modern Physics,\n\n90(1):015002, 2018.\n\nMd Sajid Anis and et al. Qiskit: An open-source framework for quantum computing, 2021.\n\nFrank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C Bardin, Rami Barends, Rupak Biswas, Sergio Boixo, Fernando GSL Brandao, David A Buell, et al. Quantum supremacy using a programmable superconducting processor. Nature, 574(7779):505–510, 2019.\n\nBela Bauer, Sergey Bravyi, Mario Motta, and Garnet Kin-Lic Chan. Quantum algorithms for quantum chemistry and quantum materials science. Chemical Reviews, 120(22):12685–12717, 2020.\n\nKerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J. Osborne, Robert Salzmann, Daniel Scheiermann, and Ramona Wolf. Training deep quantum neural networks. Nature Communications, 11, 2020.\n\nMarcello Benedetti, Erika Lloyd, Stefan Sack, and Mattia Fiorentini. Parameterized quantum cir-\n\ncuits as machine learning models. Quantum Science and Technology, 4(4):043001, 2019.\n\nJ Brooke, David Bitko, Rosenbaum, and Gabriel Aeppli. Quantum annealing of a disordered magnet.\n\nScience, 284(5415):779–781, 1999.\n\nMarco Cerezo, Andrew Arrasmith, Ryan Babbush, Simon Benjamin, Suguro Endo, Keisuke Fujii, Jarrod Ryan McClean, Kosuke Mitarai, Xiao Yuan, Lukasz Cincio, and Patrick Coles. Variational quantum algorithms. Nature Reviews Physics, 2021. URL https://www.nature.com/ articles/s42254-021-00348-9.\n\nBarry A Cipra. An introduction to the ising model. The American Mathematical Monthly, 94(10):\n\n937–959, 1987.\n\nIris Cong, Soonwon Choi, and Mikhail D. Lukin. Quantum convolutional neural networks. Nature Physics, 15(12):1273–1278, aug 2019a. doi: 10.1038/s41567-019-0648-8. URL https:// doi.org/10.1038%2Fs41567-019-0648-8.\n\nIris Cong, Soonwon Choi, and Mikhail D Lukin. Quantum convolutional neural networks. Nature\n\nPhysics, 15(12):1273–1278, 2019b.\n\nMisha Denil and Nando De Freitas. Toward the implementation of a quantum rbm. 2011.\n\nYuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng Tao. A grover-search based quantum learning scheme for classification. New Journal of Physics, 23(2):023020, feb 2021. doi: 10. 1088/1367-2630/abdefa. URL https://doi.org/10.1088/1367-2630/abdefa.\n\nVincent Dumoulin, Ian Goodfellow, Aaron Courville, and Yoshua Bengio. On the challenges of physical implementations of rbms. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 28, 2014.\n\nEdward Farhi and Hartmut Neven. Classification with quantum neural networks on near term pro-\n\ncessors. arXiv preprint arXiv:1802.06002, 2018a.\n\nEdward Farhi and Hartmut Neven. Classification with quantum neural networks on near term pro-\n\ncessors, 2018b. URL https://arxiv.org/abs/1802.06002.\n\nEdward Farhi, Jeffrey Goldstone, Sam Gutmann, Joshua Lapan, Andrew Lundgren, and Daniel Preda. A quantum adiabatic evolution algorithm applied to random instances of an np-complete problem. Science, 292(5516):472–475, 2001.\n\nEdward Farhi, Jeffrey Goldstone, and Sam Gutmann. A quantum approximate optimization algo-\n\nrithm. arXiv preprint arXiv:1411.4028, 2014.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAleta Berk Finnila, MA Gomez, C Sebenik, Catherine Stenson, and Jimmie D Doll. Quantum annealing: A new method for minimizing multidimensional functions. Chemical physics letters, 219(5-6):343–348, 1994.\n\nAnkit Garg, Robin Kothari, Praneeth Netrapalli, and Suhail Sherif. No quantum speedup over gradient descent for non-smooth convex optimization. arXiv preprint arXiv:2010.01801, 2020.\n\nVojtˇech Havl ́ıˇcek, Antonio D. C ́orcoles, Kristan Temme, Aram W. Harrow, Abhinav Kandala, Supervised learning with quantum-enhanced feadoi: 10.1038/\n\nJerry M. Chow, and Jay M. Gambetta. ture spaces. Nature, 567(7747):209–212, Mar 2019. s41586-019-0980-2. URL https://doi.org/10.1038/s41586-019-0980-2.\n\nISSN 1476-4687.\n\nBen Jaderberg, Lewis W Anderson, Weidi Xie, Samuel Albanie, Martin Kiffner, and Dieter Jaksch.\n\nQuantum self-supervised learning. Quantum Science and Technology, 7(3):035005, 2022.\n\nTadashi Kadowaki and Hidetoshi Nishimori. Quantum annealing in the transverse ising model.\n\nPhysical Review E, 58(5):5355, 1998.\n\nAshish Kapoor, Nathan Wiebe, and Krysta Svore. Quantum perceptron models. Advances in neural\n\ninformation processing systems, 29, 2016.\n\nNathan Killoran, Thomas R Bromley, Juan Miguel Arrazola, Maria Schuld, Nicol ́as Quesada, and Seth Lloyd. Continuous-variable quantum neural networks. Physical Review Research, 1(3): 033063, 2019.\n\nYunchao Liu, Srinivasan Arunachalam, and Kristan Temme. A rigorous and robust quantum speed-up in supervised machine learning. Nature Physics, 17(9):1013–1017, Sep 2021. ISSN doi: 10.1038/s41567-021-01287-z. URL https://doi.org/10.1038/ 1745-2481. s41567-021-01287-z.\n\nAntonio Macaluso, Luca Clissa, Stefano Lodi, and Claudio Sartori. A variational algorithm for\n\nquantum neural networks. Computational Science – ICCS 2020, 12142:591 – 604, 2020a.\n\nAntonio Macaluso, Luca Clissa, Stefano Lodi, and Claudio Sartori. A variational algorithm for quantum neural networks. In International Conference on Computational Science, pp. 591–604. Springer, 2020b.\n\nFabio Valerio Massoli, Lucia Vadicamo, Giuseppe Amato, and Fabrizio Falchi. A leap among quantum computing and quantum neural networks: A survey. ACM Comput. Surv., mar 2022. ISSN 0360-0300. doi: 10.1145/3529756. URL https://doi.org/10.1145/3529756. Just Accepted.\n\nJarrod R McClean, Sergio Boixo, Vadim N Smelyanskiy, Ryan Babbush, and Hartmut Neven. Barren plateaus in quantum neural network training landscapes. Nature communications, 9(1):1–6, 2018.\n\nMichael A Nielsen and Isaac Chuang. Quantum computation and quantum information, 2002.\n\nGiuseppe E Santoro and Erio Tosatti. Optimization using quantum mechanics: quantum annealing through adiabatic evolution. Journal of Physics A: Mathematical and General, 39(36):R393, 2006.\n\nGiuseppe E Santoro, Roman Marton ́ak, Erio Tosatti, and Roberto Car. Theory of quantum annealing\n\nof an ising spin glass. Science, 295(5564):2427–2430, 2002.\n\nMaria Schuld and Nathan Killoran. Quantum machine learning in feature hilbert spaces. Phys. Rev. Lett., 122:040504, Feb 2019. doi: 10.1103/PhysRevLett.122.040504. URL https://link. aps.org/doi/10.1103/PhysRevLett.122.040504.\n\nMaria Schuld, Ilya Sinayskiy, and Francesco Petruccione. The quest for a quantum neural network.\n\nQuantum Information Processing, 13(11):2567–2586, 2014.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nKunal Sharma, M. Cerezo, Lukasz Cincio, and Patrick J. Coles. Trainability of dissipative perceptron-based quantum neural networks. Physical Review Letters, 128(18), may 2022. doi: 10.1103/physrevlett.128.180505. URL https://doi.org/10.1103%2Fphysrevlett. 128.180505.\n\nPeter W Shor. Algorithms for quantum computation: discrete logarithms and factoring. In Proceed-\n\nings 35th annual symposium on foundations of computer science, pp. 124–134. Ieee, 1994.\n\nRuslan Tazhigulov, Shi-Ning Sun, Reza Haghshenas, Huanchen Zhai, Adrian Tan, Nicholas Rubin, Ryan Babbush, Austin Minnich, and Garnet Kin-Lic Chan. Simulating challenging correlated molecules and materials on the sycamore quantum processor. arXiv:2203.15291, 2022. URL https://arxiv.org/abs/2203.15291.\n\nPietro Torta, Glen B Mbeng, Carlo Baldassi, Riccardo Zecchina, and Giuseppe E Santoro. QuanarXiv preprint\n\ntum approximate optimization algorithm applied to the binary perceptron. arXiv:2112.10219, 2021.\n\nA APPENDIX\n\nA.1 GROVER’S ALGORITHM\n\nA well-known algorithm for amplitude amplification is Grover’s algorithm. The original Grover’s algorithm is trying to search specific states that satisfy some properties which are called solution states. Instead of enumerating over all possible states to find the solution states that lie in the solution set S, Grover’s algorithm tries to amplify the solution states’ amplitudes using two reflection unitary matrices Us⊤, UΨ0 . Let |Ψ0⟩ denote the initial state of all qubits and |s⟩ = (cid:80) |si⟩ |S| represent the basis state spanned by all solution states. Us⊤ and UΨ0 are constructed as follows: Us⊤ = I − 2|s⟩⟨s| UΨ0 = 2|Ψ0⟩⟨Ψ0| − I\n\nsi∈S\n\n1√\n\n(3)\n\n(2)\n\nGeometrically, Us⊤ is the reflection operator over |s⊤⟩ = (cid:80) state orthogonal to the solution space. Similarly, UΨ0 is the reflection operator over |Ψ0⟩. Given θ = arcsin (⟨Ψ0|s⊤⟩), |Ψ0⟩ can be expressed as:\n\n|si⟩, which is an\n\n|{|0⟩,|1⟩}n\\S|\n\nsi /∈S\n\n√\n\n1\n\n|Ψ0⟩ = cos θ|s⊤⟩ + sin θ|s⟩\n\nThe combination of the two reflection unitary matrices Ucomb = UΨ0 Us⊤ is equivalent to a rotation of 2θ on the plane spanned by |s⊤⟩ and |Ψ0⟩. Therefore, applying the combination of the two reflection matrices k times gives:\n\n|Ψk⟩ =\n\n(cid:33)\n\nUcomb\n\n|Ψ0⟩\n\n(cid:32) k\n\n(cid:89)\n\ni=1\n\n= cos ((2k + 1)θ)|s⊤⟩ + sin ((2k + 1)θ)|s⟩\n\n(4)\n\n(5)\n\nAs for most practical setups the solutions are always sparse and existed, we have 0 < θ ≪ π maximize the amplitude for |s⟩, k should be in the order of O( 1\n\n3 . To\n\nθ ).\n\nA.2 DEFINITION\n\nThe |s⟩, |Ψ0⟩ in KPD are updated as:\n\n|s⟩ =\n\n(cid:88)\n\n|wi⟩ (cid:78)K−1\n\nk=0 |xjk ,yjk ⟩|f (wi,xjk )⟩∈SK\n\n1 (cid:112)|SK|\n\n|wi⟩\n\nK−1 (cid:79)\n\nk=0\n\n|xjk , yjk ⟩|f (wi, xjk )⟩\n\nand\n\nΨ0 =\n\n(cid:88)\n\ni,j0,··· ,jK−1\n\n1 (cid:112)2dw |D|K\n\n|wi⟩\n\nK−1 (cid:79)\n\nk=0\n\n|xjk , yjk ⟩|f (wi, xjk )⟩\n\n,again the Grover operator can be easily built from Ucomb = Us⊤ UΨ0 where Us⊤ = I − 2|s⟩⟨s|, UΨ0 = 2|Ψ0⟩⟨Ψ0| − I.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA.3 PROOF\n\nA.3.1 THEOREM 1\n\nGiven the probability for sampling a solution state is p, then the Grover iterations to achieve the p . As in our case, the probability for sampling a\n\nmaximum amplitude amplification effect is\n\n(cid:113) 1\n\nsolution state is given by\n\n(cid:82) wi∈W J(wi)dδ (cid:82) wi∈W dδ task the Grover iterations we need is:\n\n, then suppose we are doing a balanced C-way classification\n\n(cid:115) (cid:82)\n\nwi∈W dδ (cid:82) wi∈W J(wi)dδ\n\n=\n\n=\n\n=\n\n1 Ewi∼W [J(wi)]\n\n1 1\nC\n\n(cid:115)\n\n(cid:115)\n\n√\n\nC\n\n(6)\n\n(7)\n\n(8)\n\n(9) → (10) is due to for a balanced C-way classification, we should expect the accuracy for a random model to be the same as a random guess 1\n\nC\n\nAs we have defined α =\n\nin order to sample a ε-optimal solution by measurements, it takes O( 1\n\nwi∈Wε (cid:82)\n\nwi∈W J(wi)dδ as the probability to measure a ε-optimal solution, thus α ) trials. Which gives us an\n\n(cid:82)\n\nJ(wi)dδ\n\n√\n\nC\n\nα )\n\noverall complexity of O(\n\nA.3.2 THEOREM 2\n\nGiven we are doing a balanced C-way classification task for k parallel dataset, the probability for\n\nsampling a solution state is now\n\nwi∈W 1kdδ = Ewi∼W [J(wi)k]. Since the objective function we defined is non-negative J(wi) ≥ 0, thus we have J(wi)k to be a convex function in J(wi). As expectation operator preserve convexity we have E[J(wi)k] ≥ E[J(wi)]k, we have:\n\n(cid:82)\n\n(cid:82)\n\nwi∈W J(wi)kdδ\n\n(cid:115) (cid:82)\n\nwi∈W 1kdδ wi∈W J(wi)kdδ\n\n(cid:82)\n\n=\n\n≤\n\n=\n\n(cid:115)\n\n(cid:115)\n\n1 Ewi∼W [J(wi)k]\n\n1 Ewi∼W [J(wi)]k\n\n√\n\nC k\n\n(9)\n\n(10)\n\n(11)\n\nThus the Grover iteration needed is upper bounded by C k\n\n2\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFor measurement, the expected iterations we need is as:\n\n(cid:82)\n\nwi∈Wε\n\n(cid:82)\n\nwi∈W J(wi)kdδ\n\nJ(wi)kdδ which can be approximated\n\n(cid:82)\n\n(cid:82)\n\nwi∈W J(wi)kdδ J(wi)kdδ\n\nwi∈Wε\n\n(cid:82)\n\nwi∈Wε\n\n=\n\nJ(wi)kdδ + (cid:82)\n\n(cid:82)\n\nwi∈Wε\n\nwi∈W/Wε\n\nJ(wi)kdδ\n\nJ(wi)kdδ\n\n(cid:82)\n\n(cid:82)\n\n= 1 +\n\n= 1 +\n\nwi∈W/Wε (cid:82)\n\nwi∈Wε\n\nJ(wi)kdδ\n\nJ(wi)kdδ\n\nwi∈W/Wε (cid:82)\n\nwi∈Wε\n\nJ(wi)kdδ\n\nJ(wi)kdδ\n\n(cid:82)\n\n(cid:82)\n\nwi∈Wε\n\n1kdδ\n\n(cid:82)\n\nwi∈W/Wε\n\n1kdδ\n\n1kdδ\n\nwi∈W/Wε (cid:82)\n\nwi∈Wε\n\n1kdδ\n\n= 1 +\n\n(cid:82)\n\n(cid:82)\n\nwi∈W/Wε\n\n(cid:82)\n\nJ(wi)kdδ 1kdδ\n\nwi∈W/Wε\n\nJ(wi)kdδ 1kdδ\n\nwi∈Wε\n\n(cid:82)\n\nwi∈Wε\n\n1 β\n\n= 1 +\n\n≈ 1 +\n\n= 1 + (\n\n= 1 + (\n\n(cid:82)\n\nEwi∈W/Wε [J(wi)k] Ewi∈Wε [J(wi)k] Ewi∈W/Wε [J(wi)]k Ewi∈Wε [J(wi)]k Ewi∈W/Wε[J(wi)] Ewi∈Wε[J(wi)]\n\n1 β\n\n1 β\n\n)k 1\n\nβ\n\n(cid:82)\n\nwi∈W/Wε\n\n(cid:82)\n\nJ(wi)dδ\n\n1dδ\n\nwi∈W/Wε\n\nJ(wi)dδ\n\nwi∈Wε\n\n(cid:82)\n\nwi∈Wε\n\n1dδ\n\n(cid:82)\n\nwi∈Wε\n\n(cid:82)\n\n1dδ\n\n)k 1\n\nβ\n\n= 1 + (\n\n(cid:82)\n\n= 1 + (β\n\nwi∈W/Wε (cid:82)\n\nJ(wi)dδ\n\nJ(wi)dδ\n\n)k 1\n\nβ\n\n1dδ\n\nwi∈W/Wε (cid:82)\n\nwi∈W J(wi)dδ − (cid:82)\n\nwi∈Wε\n\n(cid:82)\n\nwi∈Wε\n\nwi∈Wε J(wi)dδ\n\nJ(wi)dδ\n\n)k 1\n\nβ\n\n= 1 + (β(\n\n1 α\n\n− 1))k 1\n\nβ\n\n= 1 + βk−1(\n\n1 α\n\n− 1)k\n\n(12)\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n(18) → (19) is assuming Ewi∈W/Wε[J(wi)] > Varwi∈W/Wε[J(wi)] 1 2 which is usually the case in practice since J(wi) itself is upper bounded by [0, 1 − O(ε)] within the non-ε optimal region and J(wi) can be assumed to be near-uniformly distributed within W/Wε across range [0, 1 − O(ε)], Ewi∈W/Wε [J(wi)]k Ewi∈Wε [J(wi)k] ≤\n\nwhich is a general assumption. Thus\n\nEwi∈W/Wε [J(wi)k] Ewi∈Wε [J(wi)k]\n\ncan be dominated by\n\nEwi∈W/Wε [J(wi)]k Ewi∈Wε [J(wi)]k .\n\nA.3.3 THEOREM 3\n\nAs the trade-off is between measurements needed versus Grover iterations per measurement, we can list their rate of change to make the comparison. For k-parallel dataset, Grover iterations is increased α −1)k , however we α −1)k > 1 which is equivalent\n\n. For measurements needed, the exact ratio should be\n\ncan further approximate this ratio as\n\n2 = kC C\n\n1+βk−1( 1\n\nby kC\n\n= ( α\n\nk−1\n\n1 α\n\nk 2\n\n1 2\n\n1 α\nk−1 1 α\n\nβ α\n\n1\n\nα ⌋ + 1. Thus the optimal k should satisfy α\n\nβ )k−1 given βk−1( 1 k−1 C 1\n\nβ ≥ k\n\n1\n\n2 , k ≥ 2 is monotonically decreasing with respect to k thus as long as α\n\nα ⌋ + 1 we could have our optimal k = m = ⌊log α\n\nα ⌋ + 1, otherwise k = 1.\n\n1\n\nβ\n\n2 as well as k ≤ ⌊log α\n\nβ\n\n1\n\nα ⌋ + 1. m−1 C 1\n\n1\n\n2\n\nβ ≥ m\n\nβ\n\nto k ≤ ⌊log α k−1 C 1 As k for m = ⌊log α\n\n1\n\n1\n\nβ\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA.4 MODULE DESIGN DETAILS\n\n(a) Fully connected layer\n\n(b) ReLU\n\nFigure 8: Representing a fully connected layer and ReLU activation as quantum circuits in the Quark optimization framework. (a) Fully connective layer: for a 2 × 3 FC layer with weights Wij and input X = [X1, X2, X3], the output is given by Oi = (Wi1 ∧ X1) + (Wi2 ∧ X2) + (Wi3 ∧ X3). (b) ReLU: for an input Xi = [S, X1, X2] where S is the sign qubit, the ReLU output is given by X ′ = (X ⊕ X) ∨ (¬S ∧ X).\n\nIn addition to the convolution and maxpooling layers shown in Figure 4a and Figure 4b, Quark can also incorporate other commonly used tensor algebra operators. Figure 8 demonstrates how to represent a fully connected and a ReLU layer as quantum circuits in Quark.\n\nDue to the limitation of the number of qubits, our parameterization can be viewed as a binary model over a bounded weight space W. In our experiments, we make a little modification to Quantum Learning modules introduced before. For Edge Detection task,\n\n2 (cid:95)\n\n(\n\n2 (cid:94)\n\nO0 = (\n\ni=0\n\nj=0\n\n2 (cid:95)\n\n(\n\n2 (cid:94)\n\nO1 = (\n\ni=0\n\nj=0\n\nW0,j ⊕ Xi,j)) ⊕ W0,3\n\nW1,j ⊕ Xj,i)) ⊕ W1,3\n\nWe use |O0O1⟩ = |00⟩, |01⟩, |10⟩, |11⟩ to express the 4 different predictions respectively. For Tiny MNIST task,\n\n2 (cid:95)\n\n(\n\n2 (cid:95)\n\nOk = (\n\ni=0\n\nj=0\n\nWk,3i+jXi,j)) ⊕ Wk,9\n\nk = 0, 1\n\n(24)\n\n(25)\n\n(26)\n\nWe use |O0O1⟩ = |10⟩, |11⟩ to express the prediction of Number 1 and use |O0O1⟩ = |01⟩, |00⟩ to express the prediction of Number 2 and Number 7 respectively.\n\nA.5 EXPERIMENTS\n\nA.5.1 QUARK PROCESS\n\n(a) QCA\n\n(b) QCB\n\nFigure 9: QCA is the quantum circuit for calculating the average accuracy for different parameters, which is used in computing the number of Grover iterations as shown in Algorithm 2. QCB shows the quantum circuit for computing the accuracy of a specific weight, which is used in Algorithm 3.\n\n15\n\nX1O11X3X2O21+∧∧∧W21W23W22O12O22W11W13W12∧∧∧+W11W12W13W21W22W23X1X2X3=O1O2S1X1X2X′ 1X′ 2⊕⊕S1X1X2=X1X2S1=000S1=1RWRDROUMM|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩k×ULHUDUDHHHHRWRDRO|W⟩UMM|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩k×ULUDUDUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2: Get Grover Iterations Number Input: Data Encoder Oracle: UD; Model Forward Oracle: UM ; Objective Function Oracle:\n\nUL; number of parallel dataset: k; Shots to estimate accuracy: s\n\nOutput: Grover Iterations Number: g\n\nInitialize Obj = 0; QC = QCA(UD, UM , UL, k) ;\n\nFigure 9(a)\n\nfor i = 0; i < s; ++i do\n\n// Construct the quantum circuit shown in\n\nObj = Obj + QC.measure(RO[−1])\n\nObj = Obj/S; θ = arcsin(Obj); g = [ mπ+ π return g;\n\n2 −θ\n\n2θ\n\n], m ∈ N ;\n\nAlgorithm 3: Evaluate Input: Data Encoder Oracle: UD; Model Forward Oracle: UM ; Loss Function Oracle: UL;\n\nnumber of parallel dataset: k; Shots to estimate accuracy: s; Weight: w\n\nOutput: J(w)\n\nInitialize J = 0; QC = QCB(UD, UM , UL, k, w) ; // Construct the quantum circuit shown in\n\nFigure 9(b)\n\nfor i = 0; i < s; ++i do\n\nJ = J + QC.measure(RO[−1])\n\nJ = J/s; return J;\n\nWe use Preprocessing() to:\n\n• Calculate number of Grover iterations we need (The procedure is illustrated in Figure 9(a)\n\nand Algorithm 2).\n\n• Construct Grover Operator Ucomb.\n\n• Uniformly initialize RW with Hadamard gates.\n\n• Initialize k × RD with UDs to encode k identical training dataset in parallel.\n\nIn Algorithm 2, we may find that θ is close to π 4 , making G extremely large or non-existent. However, we can introduce extra samples to dataset to resolve the issue. These auxiliary samples are identified by an additional qubit, that automatically being classified into non-solution space.\n\nIn function Evaluate(), we evaluate the objective value of a specific weight w. The procedure is illustrated in Figure 9(b) and Algorithm 3.\n\nA.5.2 TINY-MNIST EXPERIMENTAL SETUP\n\nTo construct Tiny-MNIST dataset, we select images with label 1, 2, 7 form the original MNIST training set and downsample them to 3x3 images with binarization applied to form D1. As samples with same representations in D1 cannot share different labels, we use majority voting to decide labels for duplicate samples within D1 to form our final dataset. We apply same procedure for both training and testing datasets. This gives us a Tiny-MNIST dataset with 79 instances for training and 46 instances for testing. We use Tiny-MNIST for evaluating both classical methods and Quark. The settings of classical methods are shown in Table 1.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nArchitecture Loss Function Optimizer learning rate batch size epochs seed\n\n9 × 3 FC + Softmax CrossEntropyLosss Adadelta 1.0 8\n100 0,1,2,3,4,5,6,7,8,9\n\nTable 1: Basic Settings of Classical Method\n\nA.5.3 EDGE DETECTION RESULTS\n\n(a) 2-PD\n\n(b) 3-PD\n\nFigure 10: The effect of amplitude amplification for Edge Detection.\n\nA.5.4 TINY-MNIST RESULTS\n\n(a) 2-PD\n\n(b) 3-PD\n\nFigure 11: The effect of amplitude amplification for Tiny-MNIST.\n\n17\n\n:HLJKW0HDVXULQJ3UREDELOLW\\QRUPDOL]HGDFFXUDF\\:HLJKW0HDVXULQJ3UREDELOLW\\QRUPDOL]HGDFFXUDF\\:HLJKW0HDVXULQJ3UREDELOLW\\HQRUPDOL]HGDFFXUDF\\:HLJKW0HDVXULQJ3UREDELOLW\\HQRUPDOL]HGDFFXUDF\\Under review as a conference paper at ICLR 2023\n\nA.5.5 SHOTS & TRAINING ACCURACY\n\n(a) Edge Detection\n\n(b) Tiny-MNIST\n\nFigure 12: mean ± std of the best model’s training accuracy measured within a given measurement budget.\n\nA.6 QUANTUM SIMULATION\n\nTo further verify our framework, we use Qiskit Aer (Anis & et al., 2021) to simulate the process of solving a simplified Edge Detection task with Quark. The goal is to identify whether a 3×3 binary matrix has horizontal lines. The task is a binary classification task with 512 instances. We randomly select 400 of them as the training set and the rest as the test set. The result of simulation is shown in Table 2. Each data point is acquired with 20 runs. The corresponding result of numerical simulation is shown in Table 3.\n\nshots train std test std\n\n1\n\n2\n\n4\n\n8\n\n16\n\n32\n\n64\n\n128\n\n59.14% 67.9% 68.36% 75.33% 85.04% 85.79% 91.93% 94.14% 12.21% 10.34% 9.05% 11.26% 9.27% 4.30% 58.17% 65.04% 65.54% 75.63% 84.69% 84.55% 90.18% 92.41% 5.77% 13.54% 10.35% 9.60% 11.32% 10.09% 10.59% 7.33%\n\n9.01%\n\n6.18%\n\nTable 2: Relationships between training & test accuracy and shots. (Qiskit Simulation)\n\nshots train std test std\n\n1\n\n2\n\n4\n\n8\n\n16\n\n32\n\n64\n\n128\n\n60.74% 66.20% 71.52% 77.49% 83.82% 89.59% 93.93% 96.88% 10.72% 10.26% 10.96% 11.12% 10.09% 7.87% 3.86% 60.03% 64.99% 70.11% 76.18% 82.83% 88.90% 93.22% 95.96% 4.95% 11.17% 11.07% 11.97% 12.28% 11.23% 8.72% Table 3: Relationships between training & test accuracy and shots. (Numerical Simulation)\n\n6.11%\n\n5.53%\n\n18\n\nPHDVXUHPHQWEXGJHWVKRWVWUDLQLQJDFFXUDF\\8563'3'PHDVXUHPHQWEXGJHWVKRWVWUDLQLQJDFFXUDF\\8563'3'Under review as a conference paper at ICLR 2023\n\nFigure 13: The quantum circuit for simplified edge detection.\n\n19",
    "reference": "# Summary Of The Paper\n\nThe authors present a gradient-free framework for quantum deep learning using known techniques for the optimization, in particular Grover's algorithm. The techniques are not really new and the scheme is not at all near-term as is shown from the lack of experiments.\n\n# Strength And Weaknesses\n\n+ new framework for quantum neural networks that avoids barren plateau\n- using amplitude estimation within a quantum neural network makes it not really near-term\n- there are many different ways of avoiding barren plateau that also keep the possibility of implementing the neural networks\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clear, the techniques are quite straightforward.\n\n# Summary Of The Review\n\nInteresting idea but with many drawbacks and not enough novelty in the techniques\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nEFFICIENT BLOCK CONTRASTIVE LEARNING VIA PARAMETER-FREE META-NODE APPROXIMATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nContrastive learning has recently achieved remarkable success in many domains including graphs. However contrastive loss, especially for graphs, requires a large number of negative samples which is unscalable and computationally prohibitive with a quadratic time complexity. Sub-sampling is not optimal. Incorrect negative sampling leads to sampling bias. In this work, we propose a meta-node based approximation technique that can (a) proxy all negative combinations (b) in quadratic cluster size time complexity, (c) at graph level, not node level, and (d) exploit graph sparsity. By replacing node-pairs with additive cluster-pairs, we compute the negatives in cluster-time at graph level. The resulting Proxy approximated meta-node Contrastive (PamC) loss, based on simple optimized GPU operations, captures the full set of negatives, yet is efficient with a linear time complexity. By avoiding sampling, we effectively eliminate sample bias. We meet the criterion for larger number of samples, thus achieving block-contrastiveness, which is proven to outperform pair-wise losses. We use learnt soft cluster assignments for the meta-node construction, and avoid possible heterophily and noise added during edge creation. Theoretically, we show that real world graphs easily satisfy conditions necessary for our approximation. Empirically, we show promising accuracy gains over state-of-the-art graph clustering on 6 benchmarks. Importantly, we gain substantially in efficiency; up to 2x in training time and over 5x in GPU memory reduction. The code is publicly available.\n\n1\n\nINTRODUCTION\n\nDiscriminative approaches based on contrastive learning has been outstandingly successful in practice (Guo et al., 2017; Wang & Isola, 2020), achieving state-of-the-art results (Chen et al., 2020a) or at times outperforming even supervised learning (Logeswaran & Lee, 2018; Chen et al., 2020b). Specifically in graph clustering, contrastive learning can outperform traditional convolution and attention-based Graph Neural Networks (GNN) on speed and accuracy (Kulatilleke et al., 2022).\n\nWhile traditional objective functions encourage similar nodes to be closer in embedding space, their penalties do not guarantee separation of unrelated graph nodes (Zhu et al., 2021a). Differently, many modern graph embedding models (Hamilton et al., 2017; Kulatilleke et al., 2022), use contrastive objectives. These encourage representation of positive pairs to be similar, while making features of the negatives apart in embedding space (Wang & Isola, 2020). A typical deep model consists of a trainable encoder that generates positive and negative node embedding for the contrastive loss (Zhu et al., 2021a). It has been shown that convolution is computationally expensive and may not be necessary for representation learning (Chen et al., 2020a). As the requirement for contrastive loss is simply an encoder, recently researchers have been able to produce state-of-the-art results using simpler and more efficient MLP based contrastive loss implementations (Hu et al., 2021; Kulatilleke et al., 2022). Thus, there is a rapidly expanding interest and scope for contrastive loss based models.\n\nWe consider the following specific but popular (Hu et al., 2021; Kulatilleke et al., 2022) form of contrastive loss where τ is the temperature parameter, γij is the relationship between nodes i, j and the loss for the ith node is:\n\nli = − log\n\n(cid:80)B\n\nj=1 1[j̸=i]γij · exp (sim (zi, zj) · τ ) (cid:80)B k=1 1[k̸=i] exp (sim (zi, zk) · τ )\n\n,\n\n(1)\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nWhen no labels are present, sampling of positive and negative nodes plays a crucial role (Kipf & Welling, 2016) and is a key implementation detail in contrastive methods (Velickovic et al., 2019). Positive samples in graphs are typically connected by edges (Kulatilleke et al., 2021), similar to words in a sentence in language modelling (Logeswaran & Lee, 2018). Often data augmentation is used to generate positive samples; Chen et al. (2020b) used crop, coloring, blurring. However, it is harder to obtain negative samples. With no access to labels, negative counterparts are typically obtained via uniform sampling (Park et al., 2022), via synthesizing/augmenting (Chen et al., 2020b) or adding noise. Also, in graphs, adjacency information can be exploited to derive negatives (Hu et al., 2021; Kulatilleke et al., 2022) for feature contrastion. However, while graphs particularly suited for contrastive learning, to be effective, a large number of negative samples must be used (Wang & Isola, 2020) (e.g., 65536 in He et al. (2020)), along with larger batch sizes and longer training compared to its supervised counterparts (Chen et al., 2020b). Prior work has used data augmentation-based contrastive methods Zhu et al. (2020; 2021b), negative samples using asymmetric structures Thakoor et al. (2021) or avoided negative samples altogether via feature-level decorrelation Zhang et al. (2021b). While Thakoor et al. (2021); Zhang et al. (2021b) address complexity and scalability, as seen in Appendix Table 4, their performance can be further improved.\n\nUnlike other domains, such as vision, negative sample generation brings only limited benefits to graphs (Chuang et al., 2020; Zhu et al., 2021a). To understand this phenomenon, observe the raw embedding of USPS image dataset, in the top row of Figure 7 which looks already clustered. A direct consequence of this is that graphs are more susceptible to sampling bias (Chuang et al., 2020; Zhu et al., 2021a). Thus, graph contrastive learning approaches suffer from insufficient negatives and the complex task of sample generation in addition to O(N 2) time complexity required to contrast every negative node.\n\nHowever, what contrastive loss exactly does remain largely a mystery (Wang & Isola, 2020). For example, Arora et al. (2019)’s analysis based on the assumption of latent classes provides good theoretical insights, yet their explanation on representation quality dropping with large number of negatives is inconsistent with experimental findings (Chen et al., 2020b). Contrastive loss is seen as maximizing mutual information (MI) between two views. Yet, contradictorily, tighter bound on MI can lead to poor representations (Wang & Isola, 2020).\n\nMotivation: Prior work has approximated the task in order to approximate the loss. SwAV (Caron et al., 2020) learns to predict a node prototype code of an augmented view from the other view. GRCCA (Zhang et al., 2021a) maps augmented graphs to prototypes using k-means for alignment. PCL (Li et al., 2020) assigns several prototypes of different granularity to an image enforcing its representation to be more similar to its corresponding prototype. However, all these works use some form of data augmentation which assumes that the task-relevant information is not significantly altered and require computationally expensive operations.\n\nWang & Isola (2020) identifies alignment and uniformity as key properties of contrastive loss: alignment encourages encoders to assign similar features to similar samples; uniformity encourages a feature distribution that preserves maximal information. It is fair to assume that latent clusters are dissimilar. Even with the rare possibility of two identical cluster centers initially, one will usually change or drift apart. It is intuitive that cluster centers should be uniformly distributed in the hyperspace, similar to nodes, in order to preserve as much information of the data as possible. Uniformly distributing points on a hyperspace is defined as minimizing the total pairwise potential w.r.t. a certain kernel function and is well-studied (Wang & Isola, 2020).\n\nThus, we are naturally motivated to use the cluster centers as meta-nodes for negative contrastion. By aggregation, all its constituent nodes cab be affected. Thus, we avoid sampling, effectively eliminate sample bias, and also meet the criterion of larger number of samples. Learned soft cluster assignments can avoid possible heterophily and add robustness to noise in edge construction.\n\nIn this work, we propose a novel contrastive loss, PamC, which uses paramaterless proxy metanods to approximate negative samples. Our approach indirectly uses the full set of negative samples and yet is efficient with a time complexity of O(N ). Not only does PamCGC, based on PamC, outperform or match previous work, but it is also simpler than any prior negative sample generation approach, faster and uses relatively less GPU memory. It can be incorporated to any contrastive learning-based clustering model with minimal modifications, and works with diverse data, as we demonstrate using benchmark datasets from image, text and graph modalities.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: PamCGC jointly learns structure and clustering via probabilistic soft assignment which is used to derive the real cluster centers ˆμ, used as proxy for negative samples. Grey dotted section outlines the training components. Cluster centroids μ are obtained by pre-training an AE for reconstruction. Red dotted section is our core contribution: we use ˆμ as an efficient approximation, computing centroid-pairs instead of node-pairs, achieve block-contrastivness and do so at graph level, not instance level.\n\nTo summarize, our main contributions are:\n\n• We introduce an efficient novel parameter-free proxy, PamC, for negative sample approximation that is scalable, computationally efficient and able to include all samples. It works with diverse data, including graphs. We claim PamC is the first to implicitly use the whole graph with O(N ) time complexity, in addition to further 3-fold gains.\n\n• We provide theoretical proof and show that real world graphs always satisfies the necessary conditions, and that PamCGC is block-contrastive, known to outperform pair-wise losses.\n\n• Extensive experiments on 6 benchmark datasets show PamCGC, using proposed PamC, is on par with or better than state-of-the-art graph clustering methods in accuracy while achieving 2x training time and 5x GPU memory efficiency.\n\n2\n\nIMPLEMENTATION\n\nFirst we describe PamC, which is our parameter-free proxy to efficiently approximate the negative samples, as shown in Figure 1. Next, we introduce PamCGC, a self-supervised model based on PamC to simultaneously learn discriminative embeddings and clusters.\n\n2.1 NEGATIVE SAMPLE APPROXIMATION BY META-NODE PROXIES\n\nContrastive loss makes positive or connected nodes closer and negative or unconnected nodes further away in the feature space (Kulatilleke et al., 2022). However, in order to be effective, all negative nodes need to be contrasted with xi which is computationally expensive. A cluster center is formed by combining all member nodes, and can be seen as an aggregated representation, or a proxy, of its compositional elements. Motivated by this, we use the cluster centers to enforce negative contrastion. Specifically, we contrast every cluster center ˆμi with every cluster center ˆμj where i ̸= j.\n\n3\n\nfce0fce0fce0fczfcd0fcd0fcd0nodefeaturesxzClusterCentersμ( -)pivotDistribution (Q2)yQPҧxpre trainingMSEx,ҧxACC,NMI,FI,ARIzClustering−SelfsupervisionRealclustercenters−NegativesKLP||QGraphedges−PositivesReal Cluster Centres ҧμ00001111111xixj∝ij−αij∙log(exp(simxi,xj)+log(exp(simҧμa,ҧμb)Inter-clusterclustercenters−repulsiveIntra-clustersamecluster+attractsMembers are linked toCluster centresrepulseattractUnder review as a conference paper at ICLR 2023\n\nFollowing Arora et al. (2019); Chuang et al. (2020), we assume an underlying set of discrete latent classes C which represents semantic content, i.e., similar nodes xi, xj are in the same latent class ˆμ. Thus, we derive our proxy for negative samples as:\n\nlproxy = log\n\nC (cid:88)\n\nC (cid:88)\n\na=1\n\nb=1\n\n1[a̸=b] exp (sim (ˆμa, ˆμb) · τ ),\n\n(2)\n\nNote that, lproxy contains no i or j terms! resulting in three fold gains. Firstly, we replace (cid:80)N i=1, with a more efficient (cid:80)C a=1 where N ≫ C, typically many magnitudes, in almost all datasets, as evident from Table 1. Secondly, the lproxy is at graph level with time complexity of O(N ) rather than an instance level O(N 2). Finally, given real world graphs (especially larger graphs,) are sparse, a sparse implantation for the positives, using edge-lists, will result in a third efficiency gain, which is only possible by not having to operate on the negatives explicitly.\n\nNote that a prerequisite of the proxy approximation is the availability of labels to construct the learned cluster centers ˆμ, which we explain in the next section. Thus, the complete graph level contrastive loss can be expressed as:\n\nlP contrast = −\n\n1 N\n\nN (cid:88)\n\nlog\n\nN (cid:88)\n\ni=1\n\nj=1\n\n1[j̸=i]γij exp (sim (zi, zj) · τ ) + lproxy,\n\n(3)\n\nTheoretical explanation. The standard contrastive loss uses Jensen-Shannon divergence, which yields log 2 constant and vanishing gradients for disjoint distributions of positive and negative sampled pairs (Zhu et al., 2021a). However, in the proposed method, positive pairs are necessarily edge-linked (either explicitly or via influence (Kulatilleke et al., 2022)), and unlikely to be disjoint. Using meta-nodes for negatives, which are compositions of multiple nodes, lowers the possibility of disjointness. An algorithm using the average of the positive and negative samples in blocks as a proxy instead of just one point has a strictly better bound due to Jensen’s inequality getting tighter and is superior compared to their equivalent of element-wise contrastive counterparts (Arora et al., 2019). The computational and time cost is a direct consequence of node level contrastion. Given, N ≫ clusters, we circumvent the problem of large N by proposing a proxy-ed negative contrastive objective that operates directly at the cluster level.\n\nEstablishing mathematical guarantee: Assume node embeddings Z = {z1, z2, z3 . . . zN }, clusters μ = {μ1, μ2 . . . μC}, a label assignment operator label(zi) such that μa = (cid:80)N i=1 1[i∈label(zi)=a] ·zi, a temperature hyperparameter τ and,\n\nsimilarity(i, j, zi, zj) = sim(zi, zj)\n\n(cid:40)\n\n0, ∥zi∥∥zj ∥ ,\n\nzi·zj\n\ni = j i ̸= j\n\n(4)\n\nUsing sim(zi, zj) as the shorthand notation for similarity(i, j, zi, zj), the classic contrastive loss is:\n\nlossN N =\n\n1 N\n\nN (cid:88)\n\ni=1\n\nlog\n\n\n\n\n\nN (cid:88)\n\n\n\nexp(sim(i, j, zi, zj)τ )\n\n ,\n\nj=1\n\nSimilarly, we can express the cluster based contrastive loss as:\n\nlossCC =\n\n1 C\n\nC (cid:88)\n\na=1\n\nlog\n\n(cid:34) M (cid:88)\n\nb=1\n\nexp(sim(a, b, μa, μb)τ )\n\n(cid:35)\n\nAs 0 ≤ sim ≤ 1.0, we establish the condition for our inequality as;\n\nlossN N lossCC\n\n>\n\nlog(N ) log [1 + (C − 1)eτ ]\n\nWe provide the full derivation in Appendix A.1.\n\n4\n\n(5)\n\n(6)\n\n(7)\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Nodes N vs Clusters C with different τ temperature values. Grey surface shows the ratio = 1.0 inequality boundary. Generally, real world graphs satisfy the condition ratio > 1.0 easily. Best viewed in color.\n\nAs C > 1 (minimum 2 are needed for a cluster), and log(x) : x > 0 is strictly increasing, N > 1 + (C − 1)eτ is the necessary condition, which is easily satisfied for nearly all real world datasets and as seen in Figure 2 for multiple τ temperatures.\n\nThus, as lossN N > lossCC, lossN N upper bounds lossCC, the more efficient variant. Additionally lossCC benefits from block-contrastiveness (Arora et al., 2019), achieves a lower minima and uses the fullest possible negative information. We also show, experimentally, that minimizing lossCC results in effective, and sometimes better, representations for downstream tasks.\n\n2.2 CONSTRUCTING THE META-NODE CLUSTER CENTERS ( ˆμ)\n\nIn order to derive the real cluster centers ˆμ, which is distinct from the learnt cluster centers μ, we simply aggregate all the node embedding z of a cluster using its label. Even with unlabeled data, label() can be accomplished using predicted soft labels. The intuition here is that, during backpropagation, the optimization process will update the constituent node embeddings, z, to incorporate negative distancing. Thus,\n\nˆμc =\n\n1 N\n\nN (cid:88)\n\ni=1\n\n1[i∈label(c)]zi,\n\n(8)\n\nwhere label(c) is either the ground-truth or learnt soft labels. Accordingly, our proxy can equally be used in supervised and unsupervised scenarios and has a wider general applicability as an improvement of the contrastive loss at large. Finlay, Equation 8 can be realized with sof tmax() and mean() operations, which are well optimized GPU primitives in any machine learning framework. We provide a reference pytorch implementation.\n\n2.3 OBTAINING THE SOFT LABELS\n\nGraph clustering is essentially unsupervised. To this end, following Xie et al. (2016); Guo et al. (2017); Wang et al. (2019); Kulatilleke et al. (2022), we use probability distribution derived softlabels and a self-supervision mechanism for cluster enhancement. Specifically, we obtain soft cluster assignments probabilities qiu for embedding zi and cluster center μu. In order to handle differently scaled clusters and be computationally convenient (Wang et al., 2019), we use the student’s t-distribution (Maaten & Hinton, 2008) as a kernel for the similarity measurement between the embedding and centroid:\n\nqiu =\n\n(1 + ∥zi − μu∥2 /η)− η+1 u′(1 + ∥zi − μu′∥2 /η)− η+1\n\n2\n\n2\n\n(cid:80)\n\n,\n\n(9)\n\nwhere, η is the Student’s t-distribution’s degree of freedom. Cluster centers μ are initialized by K-means on embeddings from the pre-trained AE. We use Q = [qiu] as the distribution of the cluster assignments of all samples, and η=1 for all experiments following Bo et al. (2020); Peng et al. (2021); Kulatilleke et al. (2022)\n\n5\n\nΤ= 0.1Τ= 5.0Τ= 1.0ratio = 1.0Under review as a conference paper at ICLR 2023\n\nTable 1: Statistics of the datasets (left) and PamCGC hyperparameters (right).\n\nDataset\n\nType\n\nNodes Classes\n\ndimension\n\nUSPS HHAR REUT ACM CITE DBLP\n\nImage Record Text Graph Graph Graph\n\n9298 10299 10000 3025 3327 4057\n\n10 6\n4 3\n6 4\n\n256 561 2000 1870 3703 334\n\nα\n\n2 0.5 1\n0.5 2\n2\n\nβ K\n\nτ\n\n2 12.5 0.2 0.5 2\n2.5\n\n4 2\n1 1\n1 3\n\n0.5 1.5 0.25 0.5 1\n0.5\n\nLR\n\n10−3 10−3 10−4 10−3 10−3 10−3\n\nNodes closer in embedding space to a cluster center has higher soft assignment probabilities in Q. A target distribution P that emphasizes the confident assignments is obtained by squaring and normalizing Q, given by :\n\nik/ (cid:80) i qiu is the soft cluster frequency of centroid u.\n\n(cid:80)\n\npiu =\n\nwhere (cid:80)\n\niu/ (cid:80) q2 k (q2\n\ni qiu\n\ni qik)\n\n,\n\n(10)\n\nFollowing Kulatilleke et al. (2022), we minimize the KL divergence between Q and P distributions, which forces the current distribution Q to approach the more confident target distribution P . KL divergence updates models more gently and lessens severe disturbances on the embeddings (Bo et al., 2020). Further, it can accommodate both the structural and feature optimization targets of PamCGC. We self-supervise cluster assignments 1 by using distribution Q to target distribution P , which then supervises the distribution Q in turn by minimizing the KL divergence as:\n\nlosscluster = KL(P ||Q) =\n\n(cid:88)\n\n(cid:88)\n\npiulog\n\ni\n\nu\n\npiu qiu\n\n,\n\n(11)\n\nThe final proposed model, after incorporating PamC contrastive objective with self-supervised clustering, where α > 0 controls structure incorporation and β > 0 controls cluster optimization is:\n\nPamCGC : Lfinal = αlPcontrast(K, τ ) + βlosscluster,\n\n(12)\n\n2.4 COMPLEXITY ANALYSIS\n\nGiven input data dimension d and AE layer dimensions of d1, d2, · · · , dL, following Kulatilleke et al. (2022), OAE = O(N d2d2 L/2) for PamCGC-AE. Assuming K clusters, from Equation 9, the time complexity is Ocluster = O(N K + N log N ) following Xie et al. (2016).\n\n1...d2\n\nFor PamC, we only compute ∥z∥2 2 and zi · zj for the actual positive edges E using sparse matrix resulting in a time complexity O+ = O(N Edz), linear with the number of edges E, with dz embedding dimension. For the negatives, we use the meta-node based negatives O− = O(CC) where C is the meta-node. Note that, for real graphs, N ≫ C in many magnitudes. Thus, the overall time complexity is linearly related to the number of samples and edges.\n\n3 EXPERIMENTS\n\nWe evaluate PamCGC on transductive node clustering comparing to state-of-the-art self-supervised, contrastive and (semi-)supervised methods.\n\nDatasets. Following Bo et al. (2020); Peng et al. (2021); Kulatilleke et al. (2022), experiments are conducted on six common clustering benchmarks, which includes one image dataset (USPS (Le Cun et al., 1990)), one sensor data dataset (HHAR (Stisen et al., 2015)), one text dataset (REUT (Lewis et al., 2004)) and three citation graphs (ACM2, CITE4, and DBLP3). For the non-graph data, we use\n\n1We follow Bo et al. (2020) use of the term ’self-supervised’ to be consistent with the GCN training method.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nundirected k-nearest neighbour (KNN (Altman, 1992)) to generate adjacency matrix A following Bo et al. (2020); Peng et al. (2021). Table 1 summarizes the datasets.\n\nBaseline Methods. We compare with multiple models. K-means (Hartigan & Wong, 1979) is a classical clustering method using raw data. AE (Hinton & Salakhutdinov, 2006) applies K-means to deep representations learned by an auto-encoder. DEC (Xie et al., 2016) clusters data in a jointly optimized feature space. IDEC (Guo et al., 2017) enhances DEC by adding KL divergence-based reconstruction loss. Following models exploit graph structures during clustering: SVD (Golub & Reinsch, 1971) applies singular value decomposition to the adjacency matrix. DGI (Velickovic et al., 2019) learns embeddings by maximizing node MI with the graph. GAE (Kipf & Welling, 2016) combines convolution with the AE. ARGA (Pan et al., 2018) uses an adversarial regularizer to guide the embeddings learning. Following deep graph clustering jointly optimize embeddings and graph clustering: DAEGC (Wang et al., 2019), uses an attentional neighbor-wise strategy and clustering loss. SDCN (Bo et al., 2020), couples DEC and GCN via a fixed delivery operator and uses feature reconstruction. AGCN (Peng et al., 2021), extends SDCN by adding an attention-based delivery operator and uses multi scale information for cluster prediction. CGC (Park et al., 2022) uses a multi-level, hierarchy based contrastive loss. SCGC and SCGC* (Kulatilleke et al., 2022) uses block contrastive loss with an AE and MLP respectively. The only difference between SCGC* and PamCGC is the novel PamC loss, Also as SCGC* is the current state-of-the-art. Thus, it is used as the benchmark.\n\nEvaluation Metrics. Following Bo et al. (2020); Peng et al. (2021), we use Accuracy (ACC), Normalized Mutual Information (NMI), Average Rand Index (ARI), and macro F1-score (F1) for evaluation. For each, larger values imply better clustering.\n\n3.1\n\nIMPLEMENTATION\n\nThe positive component of our loss only requires the actual connections and can be efficiently represented by sparse matrices. Further, the negative component of the loss is graph-based, and not instance based, thus needs to be computed only once per epoch. Thus, by decoupling the negatives, our loss is inherently capable of batching and is trivially parallelizable. Computation of the negative proxy, which is only C · C does not even require a GPU!\n\nFor fair comparison, we use the same 500 − 500 − 2000 − 10 AE dimensions as in Guo et al. (2017); Bo et al. (2020); Peng et al. (2021); Kulatilleke et al. (2022) and the same pre-training procedure, i.e. 30 epochs; learning rate of 10−3 for USPS, HHAR, ACM, DBLP and 10−4 for REUT and CITE; batch size of 256. We made use of the publicly available pre-trained AE from Bo et al. (2020). We use a once computed edge-list for training, which is not needed during inference. For training, for each dataset, we initialize the cluster centers from K-means and repeat the experiments 10 times with 200 epochs to prevent extreme cases. We cite published accuracy results from Bo et al. (2020); Peng et al. (2021); Kulatilleke et al. (2022) for other models.\n\nFor all timing and memory experiments, we replicate the exact same training loops, including internal evaluation metric calls, when measuring performance for fair comparison. Our code will be made publicly available.\n\n3.2 RESULTS\n\nWe show our hyperparameters in Table 1. Comparison of results with state-of-the-art graph and non-graph datasets are in Table 2 and Table 3, respectively. For the graph data, PamCGC is stateof-the-art for DBLP. A paired-t test shows ACM and CITE results to be best for both SCGC* and PamCGC. In non-graph results, PamCGC comes second best in USPS image data. While results for HHAR are somewhat lagging, PamCGC is the best for REUT. Generally we achieve better results on the natural graph datasets; ACM, DBLP and CITE, while being competitive on other modalities. We present the qualitative results in Appendix A.4.\n\n2http://dl.acm.org/ 3https://dblp.uni-trier.de 4http://citeseerx.ist.psu.edu/index\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Clustering performance the three graph datasets (mean±std). Best results are bold. Results reproduced from Bo et al. (2020); Peng et al. (2021); Kulatilleke et al. (2022); Park et al. (2022). SCGC (Kulatilleke et al., 2022) uses neighbor based contrastive loss with AE while SCGC* variant uses r-hop cumulative Influence contrastive loss with MLP, same as our PamCGC\n\nMethod\n\nDBLP\n\n.\n\nACM\n\nCITE\n\nACC\n\nNMI\n\nARI\n\nF1\n\nACC\n\nNMI\n\nARI\n\nF1\n\nACC\n\nNMI\n\nARI\n\nF1\n\n0.1±0.0 3.7±1.8\n\nK-means 38.7±0.7 11.5±0.4 7.0±0.4 31.9±0.3 67.3±0.7 32.4±0.5 30.6±0.7 67.6±0.7 39.3±3.2 16.9±3.2 13.4±3.0 36.1±3.5 51.4±0.4 25.4±0.2 12.2±0.4 52.5±0.4 81.8±0.1 49.3±0.2 54.6±0.2 82.0±0.1 57.1±0.1 27.6±0.1 29.3±0.1 53.8±0.1 AE 58.2±0.6 29.5±0.3 23.9±0.4 59.4±0.5 84.3±0.8 54.5±1.5 60.6±1.9 84.5±0.7 55.9±0.2 28.3±0.3 28.1±0.4 52.6±0.2 DEC 60.3±0.6 31.2±0.5 25.4±0.6 61.3±0.6 85.1±0.5 56.6±1.2 62.2±1.5 85.1±0.5 60.5±1.4 27.2±2.4 25.7±2.7 61.6±1.4 IDEC 0.0±0.1 13.3±2.2 39.9±5.8 29.3±0.4 0.1±0.3 11.4±1.7 SVD 32.5±2.4 1.7±0.9 29.3±3.3 88.0±1.1 63.0±1.9 67.7±2.5 88.0±1.0 64.1±1.3 38.8±1.2 38.1±1.9 60.4±0.9 DGI 61.2±1.2 30.8±0.9 22.0±1.4 61.4±2.2 84.5±1.4 55.4±1.9 59.5±3.1 84.7±1.3 61.4±0.8 34.6±0.7 33.6±1.2 57.4±0.8 GAE 58.6±0.1 26.9±0.1 17.9±0.1 58.7±0.1 84.1±0.2 53.2±0.5 57.7±0.7 84.2±0.2 61.0±0.4 32.7±0.3 33.1±0.5 57.7±0.5 VGAE 61.6±1.0 26.8±1.0 22.7±0.3 61.8±0.9 86.1±1.2 55.7±1.4 62.9±2.1 86.1±1.2 56.9±0.7 34.5±0.8 33.4±1.5 54.8±0.8 ARGA DAEGC 62.1±0.5 32.5±0.5 21.0±0.5 61.8±0.7 86.9±2.8 56.2±4.2 59.4±3.9 87.1±2.8 64.5±1.4 36.4±0.9 37.8±1.2 62.2±1.3 77.6±0.5 46.1±0.6 49.7±1.1 77.2±0.4 92.3±0.3 72.9±0.7 78.4±0.6 92.3±0.3 69.6±0.6 44.6±0.6 46.0±0.6 65.5±0.7 CGC 68.1±1.8 39.5±1.3 39.2±2.0 67.7±1.5 90.5±0.2 68.3±0.3 73.9±0.4 90.4±0.2 66.0±0.3 38.7±0.3 40.2±0.4 63.6±0.2 SDCN 73.3±0.4 39.7±0.4 42.5±0.3 72.8±0.6 90.6±0.2 68.4±0.5 74.2±0.4 90.6±0.2 68.8±0.2 41.5±0.3 43.8±0.3 62.4±0.2 AGCN 77.7±0.1 47.1±0.2 51.2±0.2 77.3±0.1 92.6±0.0 73.3±0.0 79.2±0.0 92.5±0.0 73.2±0.1 46.8±0.1 50.0±0.1 63.3±0.0 SCGC 77.7±0.1 47.1±0.1 50.2±0.1 77.5±0.1 92.6±0.0 73.7±0.1 79.4±0.1 92.6±0.0 73.3±0.0 46.9±0.0 50.2±0.0 63.4±0.0 SCGC* PamCGC 79.6±0.0 49.2±0.1 54.7±0.1 79.0±0.1 92.5±0.0 73.7±0.1 79.2±0.1 92.5±0.0 73.3±0.2 47.3±0.3 50.1±0.4 63.4±0.2\n\n3.1±4.2 30.1±8.2 24.1±1.2\n\n3.8±4.3\n\n5.7±1.5\n\nTable 3: Clustering performance the three non-graph datasets (mean±std). Best results are bold; second best is underlined. Results reproduced from Bo et al. (2020); Peng et al. (2021); Kulatilleke et al. (2022). SCGC (Kulatilleke et al., 2022) uses neighbour based contrastive loss with AE while SCGC* variant uses r-hop cumulative Influence contrastive loss with MLP, same as our PamCGC .\nDAEGC\n\nDataset Metric K-means\n\nAGCN\n\nVGAE\n\nSCGC\n\nGAE\n\nUSPS\n\nHHAR\n\nREUT\n\nACC NMI ARI F1 ACC NMI ARI F1 ACC NMI ARI F1\n\nSCGC*\n\nSDCN 66.82±0.04 63.10±0.33 56.19±0.72 73.55±0.40 78.08±0.19 80.98±0.28 82.90±0.08 84.91±0.06 84.20±0.24 62.63±0.05 60.69±0.58 51.08±0.37 71.12±0.24 79.51±0.27 79.64±0.32 82.51±0.07 84.16±0.10 80.32±0.38 54.55±0.06 50.30±0.55 40.96±0.59 63.33±0.34 71.84±0.24 73.61±0.43 76.48±0.11 79.50±0.06 77.75±0.56 64.78±0.03 61.84±0.43 53.63±1.05 72.45±0.49 76.98±0.18 77.61±0.38 80.06±0.05 81.54±0.06 78.82±0.17 59.98±0.02 62.33±1.01 71.30±0.36 76.51±2.19 84.26±0.17 88.11±0.43 89.49±0.22 89.36±0.16 84.94±1.09 58.86±0.01 55.06±1.39 62.95±0.36 69.10±2.28 79.90±0.09 82.44±0.62 84.24±0.29 84.50±0.41 79.54±0.65 46.09±0.02 42.63±1.63 51.47±0.73 60.38±2.15 72.84±0.09 77.07±0.66 79.28±0.28 79.11±0.18 72.57±1.20 58.33±0.03 62.64±0.97 71.55±0.29 76.89±2.18 82.58±0.08 88.00±0.53 89.59±0.23 89.48±0.17 84.13±1.30 54.04±0.01 54.40±0.27 60.85±0.23 65.50±0.13 79.30±0.11 79.30±1.07 80.32±0.04 79.35±0.00 81.78±0.01 41.54±0.51 25.92±0.41 25.51±0.22 30.55±0.29 56.89±0.27 57.83±1.01 55.63±0.05 55.16±0.01 59.13±0.00 27.95±0.38 19.61±0.22 26.18±0.36 31.12±0.18 59.58±0.32 60.55±1.78 59.67±0.11 57.80±0.01 63.51±0.03 41.28±2.43 43.53±0.42 57.14±0.17 61.82±0.13 66.15±0.15 66.16±0.64 63.66±0.03 66.54±0.01 69.48±0.03\n\nPamCGC\n\n3.3 PERFORMANCE\n\nIn Figure 3 we compare the GPU based training time and GPU memory. Our model times also include the time taken for the cumulative influence computation. For all the datasets, PamCGC is superior by 2.2x training time and 5.3x GPU memory savings. Especially, for larger datasets USPS, HHAR and REUT, PamCGC uses 5.2,7.7,8.7x less GPU memory.\n\nAdditionally, we used CITE dataset (3327 nodes) to create synthetics nodes. For a scale factor n, as contact nodes n times, along with edge-lists. Figure 3(right) shows the scaled edges and nodes for scale factors 5, 10, 15 · · · 45 and the GPU memory and training time for 1 epoch on Google colab T4 GPU with 16GB memory. Without PamC, scales over 5 is not possible due to running out of memory. With PamC over x45 (150,000 nodes) is possible. GPU and memory increase is liner confirming the theoretical time complexity. We used CITE as it is a very common dataset. We used synthetic node creation to capture variation over node size. Appendix A.8 shows GPU time breakup. Appendix A.6 shows the CITE dataset results with PamC when scaled from 1 . . . 20 in steps of 1.\n\n3.4 ABLATION STUDY\n\nTo investigate PamCs ability to generalize to other models, we incorporate it to SDCN and AGCN models, modified for contrastive loss. Figure 4 shows the GPU training time and accuracy. As PamC is a loss function, there is no difference in the inference times. As expected, training times\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: GPU performance from the pytorch profiler on Google Colab with T4 16Gb GPU. left:training time for 200 epochs. center:memory utilization per epoch. right:graph size vs time and memory on synthetic CITE data per epoch; W/o PamC, model runs out of memory after 17,000 nodes. With PamC, 150,000 nodes and over 18 million edges can be handled on the T4’s 16GB. Note that SCGC* only differs from PamCGC by its use of the novel proxy-ed PamC to which we solely attribute the time and memory savings.\n\nFigure 4: Left:GPU training times with PamCGC on SDCN and AGCN is consistently lower, and significant in large datasets (usps,hhar,reut). Right:Accuracy loss of the approximation is very low. For dblp, usps, reut accuracy is actually better.\n\nare significantly shorter, with (often better) training accuracy due to block contrastiveness. Note that PamC only improves loss computation efficiency. Majority of the SDCN and AGCN computation time is spent in their GNNs convolution operations.\n\nWe also carry out extensive experimentation to assess the behavior of hyperparameters. PamC is robust to changes in hyperparameter values and performs best with a learning rate of 0.001, as shown in Appendix A.2. Further, PamC accuracy against all hyperparameter combinations is generally equal or better than the less efficient non proxy-ed contrastive loss variant, as seen in Appendix A.3.\n\n3.5 FUTURE WORK\n\nOur parameter-free proxy-ed contrastive loss uses the full positive edge information which, as some of our experiments has shown, is redundant. For example, USPS gives similar results with 40% positive edges removed. An algorithm to drop un-informative edges may result in further efficiency improvements, which we leave for future work. While theoretically possible, it would be interesting to see how our proxy-ed contrastive loss works with semi or fully supervised data. Further study is needed to explore how hard cluster centers effect the optimization process.\n\n4 CONCLUSION\n\nIn this work, we present an efficient parameter-free proxy approximation to incorporate negative samples in contrastive loss for joint clustering and representation learning. We eliminate sample bias, achieve block contrastiveness and 0(N ). Our work is supported by theoretical proof and empirical results. We improve considerably over previous methods accuracy, speed and memory usage. Our approach differs from prior self-supervised clustering by the proxy mechanism we use to incorporate all negative samples efficiently. The strength of this simple approach indicates that, despite the increased interest in graphs, effective contrastive learning remains relatively unexplored.\n\n9\n\n0306090120uspshharreutacmdblpciteTraining time (s)SDCNAGCNSCGCSCGC*MODEL0.00.51.01.52.02.53.0uspshharreutacmdblpciteGPU memory usage (Gb/epoch)00.511.522.5024681012141612244944209837085773829311269146991858531733506783100116133150time per traning epoch (s)GPU memory per epcoh (GB) Number of edges (top) and nodes (below) x1000GPU memeory vs number of nodes (CITE)w/o PAMC - memoryPAMC - memoryw/o PAMC - timePAMC - timemax 16GB GPU memory (T4)90120Training time (s)SDCNAGCNSCGCSCGC*MODEL0306090120uspshharreutacmdblpciteTraining time (s)SDCNAGCNSCGCSCGC*0.60.81.0DBLPACMCITEUSPSHHARREUTAccuracy AGCN contrastive AGCN contrastive (ours) SDCN contrastive SDCN contrastive (ours)020406080100DBLPACMCITEUSPSHHARREUTGPU training timesUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nNaomi S Altman. An introduction to kernel and nearest-neighbor nonparametric regression. The\n\nAmerican Statistician, 46(3):175–185, 1992.\n\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. A theoretical analysis of contrastive unsupervised representation learning. In 36th International Conference on Machine Learning, ICML 2019, pp. 9904–9923. International Machine Learning Society (IMLS), 2019.\n\nDeyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui. Structural deep clustering\n\nnetwork. In Proceedings of The Web Conference 2020, pp. 1400–1410, 2020.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912–9924, 2020.\n\nMing Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International Conference on Machine Learning, pp. 1725–1735. PMLR, 2020a.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020b.\n\nChing-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. Advances in neural information processing systems, 33:8765–8775, 2020.\n\nGene H Golub and Christian Reinsch. Singular value decomposition and least squares solutions. In\n\nLinear algebra, pp. 134–151. Springer, 1971.\n\nXifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. Improved deep embedded clustering with\n\nlocal structure preservation. In IJCAI, pp. 1753–1759, 2017.\n\nWilliam L Hamilton, Rex Ying, and Jure Leskovec.\n\nInductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025–1035, 2017.\n\nJohn A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm. Journal\n\nof the Royal Statistical Society. Series C (Applied Statistics), 28(1):100–108, 1979.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on\n\nunsupervised visual representation learning. computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nGeoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural\n\nnetworks. Science, 313(5786):504–507, 2006.\n\nYang Hu, Haoxuan You, Zhecan Wang, Zhicheng Wang, Erjin Zhou, and Yue Gao. Graph-mlp: node classification without message passing in graph. arXiv preprint arXiv:2106.04051, 2021.\n\nThomas N Kipf and Max Welling.\n\nVariational graph auto-encoders.\n\narXiv preprint\n\narXiv:1611.07308, 2016.\n\nGayan K Kulatilleke, Marius Portmann, Ryan Ko, and Shekhar S Chandra. Fdgatii: Fast dynamic graph attention with initial residual and identity mapping. arXiv preprint arXiv:2110.11464, 2021.\n\nGayan K Kulatilleke, Marius Portmann, and Shekhar S Chandra. Scgc: Self-supervised contrastive\n\ngraph clustering. arXiv preprint arXiv:2204.12656, 2022.\n\nYann Le Cun, Ofer Matan, Bernhard Boser, John S Denker, Don Henderson, Richard E Howard, Wayne Hubbard, LD Jacket, and Henry S Baird. Handwritten zip code recognition with multilayer networks. In ICPR, volume 2, pp. 35–40. IEEE, 1990.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDavid D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark collection for\n\ntext categorization research. Journal of machine learning research, 5(Apr):361–397, 2004.\n\nJunnan Li, Pan Zhou, Caiming Xiong, and Steven Hoi. Prototypical contrastive learning of unsuper-\n\nvised representations. In International Conference on Learning Representations, 2020.\n\nLajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representa-\n\ntions. In International Conference on Learning Representations, 2018.\n\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\n\nlearning research, 9(Nov):2579–2605, 2008.\n\nLeland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and\n\nprojection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.\n\nShirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially regularized graph autoencoder for graph embedding. arXiv preprint arXiv:1802.04407, 2018.\n\nNamyong Park, Ryan Rossi, Eunyee Koh, Iftikhar Ahamath Burhanuddin, Sungchul Kim, Fan Du, Nesreen Ahmed, and Christos Faloutsos. Cgc: Contrastive graph clustering forcommunity detection and tracking. In Proceedings of the ACM Web Conference 2022, pp. 1115–1126, 2022.\n\nZhihao Peng, Hui Liu, Yuheng Jia, and Junhui Hou. Attention-driven graph clustering network. In\n\nProceedings of the 29th ACM International Conference on Multimedia, pp. 935–943, 2021.\n\nAllan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen. Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition. In SenSys, pp. 127–140, 2015.\n\nShantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Rémi Munos, Petar Veliˇckovi ́c, and Michal Valko. Bootstrapped representation learning on graphs. In ICLR 2021 Workshop on Geometrical and Topological Representation Learning, 2021.\n\nPetar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon\n\nHjelm. Deep graph infomax. ICLR (Poster), 2(3):4, 2019.\n\nChun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and Chengqi Zhang. Attributed graph clustering: A deep attentional embedding approach. arXiv preprint arXiv:1906.06532, 2019.\n\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pp. 9929–9939. PMLR, 2020.\n\nJunyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.\n\nIn ICML, pp. 478–487, 2016.\n\nChunyang Zhang, Hongyu Yao, CL Chen, and Yuena Lin. Graph representation learning via con-\n\ntrasting cluster assignments. arXiv preprint arXiv:2112.07934, 2021a.\n\nHengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonical correlation analysis to self-supervised graph neural networks. Advances in Neural Information Processing Systems, 34:76–89, 2021b.\n\nYizhen Zheng, Yu Zheng, Xiaofei Zhou, Chen Gong, Vincent Lee, and Shirui Pan. Unifying graph contrastive learning with flexible contextual scopes. arXiv preprint arXiv:2210.08792, 2022.\n\nHao Zhu, Ke Sun, and Peter Koniusz. Contrastive laplacian eigenmaps. Advances in Neural Infor-\n\nmation Processing Systems, 34, 2021a.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\n\nrepresentation learning. arXiv preprint arXiv:2006.04131, 2020.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, pp. 2069–2080, 2021b.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 PROOFS OF THEORETICAL RESULTS - DERIVATION OF EQUATION 7\n\nAssume node embeddings Z = {z1, z2, z3 . . . zN }, clusters μ = {μ1, μ2 . . . μC}, a label assignment operator label(zi) such that μa = (cid:80)N i=1 1[i∈label(zi)=a] · zi, a hyperparameter τ related to the temperature in contrastive loss and\n\nsimilarity(i, j, zi, zj) = sim(zi, zj)\n\n(cid:40)\n\n0, ∥zi∥∥zj ∥ ,\n\nzi·zj\n\ni = j i ̸= j\n\n(13)\n\nWe use sim(zi, zj) as the shorthand notation for similarity(i, j, zi, zj) interchangeably for brevity.\n\nWe begin with Equation 1, which is the popular form of contrastive loss (Hu et al., 2021; Kulatilleke et al., 2022). With τ as the temperature parameter, γij the relationship between nodes i, j, the loss for the ith can be expanded as:\n\nli = + log\n\nB (cid:88)\n\nj=1\n\n1[j̸=i] exp (sim (zi, zj) τ ) − log\n\nB (cid:88)\n\nj=1\n\n1[j̸=i]γij exp (sim (zi, zj) τ ),\n\n(14)\n\nwhere, the first part on the right corresponds to the negative node contrasting portion and the second portion contrasts the positives for node i. From Equation 14, for all nodes N , we take to negative node contrasting portion, by averaging over N nodes to obtain:\n\nlossN N =\n\n1 N\n\nN (cid:88)\n\ni=1\n\nlog\n\n\n\n\n\nN (cid:88)\n\nesim(i,j,zi,zj )τ\n\n\n\n ,\n\n(15)\n\nj=1\n\nNote our use of the more concise sim() and the compact e notation over exp() interchangeably for compactness reasons.\n\nWe expand Equation 15, together with e0 = 1 in cases where i = j, as:\n\nlossN N =\n\n1 N\n\n(cid:34)\n\nlog\n\nlog\n\nlog\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\nlog\n\n1\n\n+ esim(z1,z2)τ + esim(z1,z3)τ + esim(z1,z4)τ . . . + esim(z1,zN )τ (cid:17)\n\n+\n\nesim(z2,z1)τ +\n\n1\n\nesim(z3,z1)τ + esim(z3,z2)τ +\n\n+ esim(z2,z3)τ + esim(z2,z4)τ . . . + esim(z2,zN )τ (cid:17) + esim(z3,z4)τ . . . + esim(z3,zN )τ (cid:17)\n\n1\n\n+\n\n+\n\n· · ·\n\nesim(zN ,z1)τ + esim(zN ,z2)τ + esim(zN ,z3)τ + esim(zN ,z4)τ . . . + 1\n\nSimilarly, we can express the cluster based contrastive loss as:\n\nlossCC =\n\n1 C\n\nC (cid:88)\n\na=1\n\nlog\n\n(cid:34) M (cid:88)\n\nb=1\n\n(cid:35)\n\nesim(a,b,μa,μb)τ\n\n12\n\n(cid:35)\n\n(cid:17)\n\n(16)\n\n(17)\n\nUnder review as a conference paper at ICLR 2023\n\nwith the following expansion:\n\nlossCC =\n\n1 C\n\n(cid:34)\n\nlog\n\nlog\n\nlog\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\nlog\n\n1\n\n+ esim(μ1,μ2)τ + esim(μ1,μ3)τ + esim(μ1,μ4)τ . . . + esim(μ1,μC )τ (cid:17)\n\n+\n\nesim(μ2,μ1)τ +\n\n1\n\nesim(μ3,μ1)τ + esim(μ3,μ2)τ +\n\n+ esim(μ2,μ3)τ + esim(μ2,μ4)τ . . . + esim(μ2,μC )τ (cid:17) + esim(μ3,μ4)τ . . . + esim(μ3,μC )τ (cid:17)\n\n1\n\n+\n\n+\n\n· · ·\n\nesim(μC ,μ1)τ + esim(μC ,μ2)τ + esim(μC ,μ3)τ + esim(μC ,μ4)τ . . . + 1\n\n(cid:35)\n\n(cid:17)\n\n(18)\n\nN N > lossmax\n\nIf, lossmin ing this inequality.\n\nCC , we have lossN N\n\nlossCC\n\n> 1. Next we show the conditions necessary for establish-\n\nAs 0 ≤ sim ≤ 1.0, we obtain the min using simmin = 0:\n\nlossmin\n\nN N =\n\n(cid:34)\n\n1 N\n\nlog (cid:0)1 + e0 + e0 + . . . + e0(cid:1) + · · · + log (cid:0)1 + e0 + e0 + . . . + e0(cid:1)\n\n= log (cid:2)1 + (N − 1)e0(cid:3) = log(N )\n\n(cid:35)\n\n(19)\n\nSimilarly, we can obtain the max, using simmax = 1.0:\n\nlossmax\n\nCC =\n\n1 C\n\n(cid:34)\n\nlog (cid:0)1 + e1.τ + e1.τ + . . . + e1.τ (cid:1) + · · · + log (cid:0)1 + e1.τ + e1.τ + . . . + e1.τ (cid:1)\n\n(cid:35)\n\n= log [1 + (C − 1)eτ ]\n\n(20)\n\nCombining Equation 19 and Equation 20, we establish the necessary condition for our inequality, Equation 7 as;\n\nlossN N lossCC\n\n>\n\nlog(N ) log [1 + (C − 1)eτ ]\n\nThis derivation is used in Section 2.1, where we show how the condition is almost always satisfied in real graphs. As a result, lossN N upper bounds lossCC. Note that a lower loss is better.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA.2 HYPERPARAMETERS VS ACCURACY\n\nFigure 5: Ablation study on the hyperparameters. TAU=τ , ALPHA=α, ORDER=R and LR denotes learning rate. A hyperparameter with higher and more condensed distribution represents its superiority over its counterpart. PamCGC is robust to τ, α, R and best with a learning rate 0f 0.001. Best viewed in color.\n\nA.3 HYPERPARAMETER BEHAVIOUR WITH AND WITHOUT PAMC\n\nFigure 6: Comparison of hyperparameters with and without PamC. TAU=τ , ALPHA=α, ORDER=R and LR denotes learning rate. A hyperparameter with higher and more condensed distribution represents its superiority over its counterpart. PamC is generally better in accuracy for majority of the hyperparameter combinations. Best viewed in color.\n\n14\n\n0.250.51.01.52.0TAU on usps0.700.750.800.85ACC_AVGLR0.00010.0010.250.51.01.52.0TAU on hhar0.650.700.750.800.850.90ACC_AVG0.250.51.01.52.0TAU on reut0.780.790.800.810.82ACC_AVG0.250.51.01.52.0TAU on acm0.870.880.890.900.910.920.93ACC_AVG0.250.51.01.52.0TAU on dblp0.500.550.600.650.700.750.800.85ACC_AVG0.250.51.01.52.0TAU on cite0.620.640.660.680.700.720.74ACC_AVG0.51.02.03.04.05.010.0ALPHA on usps0.700.750.800.85ACC_AVG0.51.02.03.04.05.010.0ALPHA on hhar0.650.700.750.800.850.90ACC_AVG0.51.02.03.04.05.010.0ALPHA on reut0.780.790.800.810.82ACC_AVG0.51.02.03.04.05.010.0ALPHA on acm0.870.880.890.900.910.920.93ACC_AVG0.51.02.03.04.05.010.0ALPHA on dblp0.500.550.600.650.700.750.800.85ACC_AVG0.51.02.03.04.05.010.0ALPHA on cite0.620.640.660.680.700.720.74ACC_AVG1234ORDER on usps0.700.750.800.85ACC_AVG1234ORDER on hhar0.650.700.750.800.850.90ACC_AVG1234ORDER on reut0.780.790.800.810.82ACC_AVG1234ORDER on acm0.870.880.890.900.910.920.93ACC_AVG1234ORDER on dblp0.500.550.600.650.700.750.800.85ACC_AVG1234ORDER on cite0.620.640.660.680.700.720.74ACC_AVG0.10.20.30.40.51.01.52.02.5BETA on usps0.700.750.800.85ACC_AVG5.07.510.012.515.0BETA on hhar0.650.700.750.800.850.90ACC_AVG0.10.20.30.40.51.01.52.02.5BETA on reut0.780.790.800.810.82ACC_AVG0.10.20.30.40.51.01.52.02.5BETA on acm0.870.880.890.900.910.920.93ACC_AVG0.10.20.30.40.51.01.52.02.5BETA on dblp0.500.550.600.650.700.750.800.85ACC_AVG0.10.20.30.40.51.01.52.02.5BETA on cite0.620.640.660.680.700.720.74ACC_AVG0.250.51.01.52.0TAU on usps0.6750.7000.7250.7500.7750.8000.8250.8500.875ACC_AVGMethodNNCC0.250.51.01.52.0TAU on hhar0.650.700.750.800.850.90ACC_AVG0.250.51.01.52.0TAU on reut0.760.770.780.790.800.810.82ACC_AVG0.250.51.01.52.0TAU on acm0.840.860.880.900.92ACC_AVG0.250.51.01.52.0TAU on dblp0.500.550.600.650.700.750.800.85ACC_AVG0.250.51.01.52.0TAU on cite0.640.660.680.700.720.74ACC_AVG0.51.02.03.04.05.010.0ALPHA on usps0.6750.7000.7250.7500.7750.8000.8250.8500.875ACC_AVG0.51.02.03.04.05.010.0ALPHA on hhar0.650.700.750.800.850.90ACC_AVG0.51.02.03.04.05.010.0ALPHA on reut0.760.770.780.790.800.810.82ACC_AVG0.51.02.03.04.05.010.0ALPHA on acm0.840.860.880.900.920.94ACC_AVG0.51.02.03.04.05.010.0ALPHA on dblp0.500.550.600.650.700.750.800.85ACC_AVG0.51.02.03.04.05.010.0ALPHA on cite0.640.660.680.700.720.74ACC_AVG1234ORDER on usps0.6750.7000.7250.7500.7750.8000.8250.8500.875ACC_AVG1234ORDER on hhar0.650.700.750.800.85ACC_AVG1234ORDER on reut0.760.770.780.790.800.810.82ACC_AVG1234ORDER on acm0.840.860.880.900.920.94ACC_AVG1234ORDER on dblp0.500.550.600.650.700.750.800.85ACC_AVG1234ORDER on cite0.640.660.680.700.720.74ACC_AVG0.10.20.30.51.02.0BETA on usps0.6750.7000.7250.7500.7750.8000.8250.8500.875ACC_AVG5.07.510.012.515.0BETA on hhar0.650.700.750.800.850.90ACC_AVG0.10.20.30.51.02.0BETA on reut0.760.770.780.790.800.810.82ACC_AVG0.10.20.30.51.02.0BETA on acm0.840.860.880.900.920.94ACC_AVG0.10.20.30.51.02.0BETA on dblp0.500.550.600.650.700.750.800.85ACC_AVG0.10.20.30.51.02.0BETA on cite0.640.660.680.700.720.74ACC_AVG0.00010.001LR on usps0.7000.7250.7500.7750.8000.8250.8500.875ACC_AVG0.00010.001LR on hhar0.650.700.750.800.85ACC_AVG0.00010.001LR on reut0.760.770.780.790.800.810.82ACC_AVG0.00010.001LR on acm0.840.860.880.900.920.94ACC_AVG0.00010.001LR on dblp0.500.550.600.650.700.750.800.85ACC_AVG0.00010.001LR on cite0.640.660.680.700.720.74ACC_AVGUnder review as a conference paper at ICLR 2023\n\nA.4 QUALITATIVE RESULTS\n\nFigure 7: Visual comparison of embeddings; top: raw data, second row: after AE pre-training, third-row: from SCGC*, and last-row: from PamCGC*. Colors represent ground truth groups. Black squares, ˆμ, are the approximated meta-nodes. Red dots, μ, are the cluster centroids.\n\nWe use UMAP (McInnes et al., 2018), in Figure 7, to get a visual understanding of the raw and learnt embedding spaces. Except for USPS, which is a distinct set of 0 · · · 9 handwritten digits (raw 1), we see that all other datasets produce quite indistinguishable clusters. Clustering is nearly non-existent in the (last 3) graph datasets. This clearly shows a characteristic difference in graph data, which can lead to high samplings bias. Note that ˆμ ̸= μ for any meta-node.\n\nA.5 DATASET SIZE VS GPU MEMORY AND TIME, WITH AND WITHOUT PAMC\n\nFigure 8: Graph size vs GPU memory and training time with and without PamC. Using PamC is generally better and is more effective with larger graph sizes. Best viewed in color.\n\n15\n\nuspshharreutacmdblpcite010203040506070020004000600080001000012000time (s)Number of nodes (or items)traing timew/o PAMCPAMC0.0000.5001.0001.5002.0002.500020004000600080001000012000GPU memory (Gb)Number of nodes (or items)GPU memory w/o PAMCPAMCUnder review as a conference paper at ICLR 2023\n\nA.6\n\nIMPROVED GPU MEMORY AND TRAINING TIME ON SYNTHETIC CITE DATASET\n\nFigure 9: Graph size vs GPU memory and training time with and w/o PamC for synthetic CITE dataset scaled from 1 to 20. Scaled edge and node sizes are indicated in the x-axis. PamC achieves linear time and memory and is more effective with larger graph sizes. W/o PamC, model runs out of memory on Google Colab T4 (16GB GPU memory). Best viewed in color.\n\nA.7 RESULTS FROM SELECTED WORKS ON THE CITE DATASET\n\nTable 4: Results for CITE dataset shows PamC is competitiveness in terms of accuracy. ‡Results reproduced from Zheng et al. (2022)\n\nModel\n\nAccuracy – CITE dataset\n\nGRACE Zhu et al. (2020) GCA Zhu et al. (2021b) BGRL Thakoor et al. (2021) CCA-SSG Zhang et al. (2021b) PamC(Ours)\n\n71.7 ± 0.6 ‡ 71.2 ± 0.2 ‡ 71.6 ± 0.4 ‡ 73.1 ± 0.3 73.3 ± 0.2\n\nA.8 GPU WORKLOAD BREAKDOWN WITH AND WITHOUT PAMC.\n\nTable 5: The GPU time breakdown for USPS dataset for 200 epochs on Colab T4 (16GB). The model forward figures (1.272 and 2.257) are different because the GPU is caching the results in the case of no PamC. During inference, these figures are identical.\n\nDescription of task\n\nw/o PamC With PamC\n\nForward pass Model forward (computation of z) KL (self-supervision loss) Pseudo label and negative computation Contrastive loss (once the pairs are computed)\n\n24.378s 1.272s 7.200ms 18.947s 2.548s\n\n6.670s 2.257s 13.448ms 28.970ms = 0.02897s 1.914s\n\n16\n\n00.10.20.30.40.50.60.70.80.910246810121416124392159244348469609767944113813511582183120982384268830103350370837101317202327303337404347505357606367Time per training epoch (s)GPU memory per epcoh (GB) Number of edges (top) and nodes (below) x1000GPU memeory vs number of nodes (CITE)w/o PAMC - MemoryPAMC -memoryw/o PAMC - timePAMC - timemax 16GB GPU memory (Google ColabT4)",
    "reference": "# Summary Of The Paper\n\nThis paper proposes Proxy approximated meta-node Contrastive (PamC) for contrastive representation learning on graphs. PamC is motivated by the computational burden of vanilla contrastive loss (i.e., InfoNCE), and to deal with this problem, it proposes a meta-node based approximation technique which proxies all negative combinations in quadratic cluster size time complexity. Empirical reuslts sho that the proposed method demonstrates promising accuracy gains over sota graph clustering methods on 6 benchmarks with better efficiency.\n\n# Strength And Weaknesses\n\nStrenghts:\n1. The motivation of this paper is good and solid. The computation complexity of InfoNCE loss is quadratic with respect to the number of nodes, which severly prevent full-graph training.\n2. The proposed method does show better efficacy than traditional contrastive methods.\n\nWeaknesses:\n  1. The authors claim that contrastive learning on graphs require a large number fo negative samples while subsampling is suboptimal. Is there any empirical support for this claim as according to my experience, subsampling will not severly degrade the model’s performance.\n\n2. Compared with data augmentation based contrastive learning  (InfoNCE loss) methods which ony requires two shared GNN encoder to generate node embeddings, the proposed method looks much more complicated (e.g., pretraining to get soft clusters). Although the overall complexity is linear, I doubt whether it can lead to efficacy when really applied to these datasets (especially when the graph is not that large, e.g., the datasets used in experiments)\n3. The paper focus on contrastive learning on graphs, however, a lot of important related works are missing in both related works and experiments. For example, [1] and [2] are two data augmentation-based contrastive methods using InfoNCE loss. [3] avoids negative samples using asymmetric structures. [4] avoids negative samples through feature-level decorrelation. The complexity of [3] and [4] are both linear to the graph size and thus they are scalable. However, this paper never consider these important baselines.\n4.  I am also confused about the tasks and datasets used in experiments. According to my knowledge, most self-supervised learning methods (including contrastive ones) foucs on node classifcation tasks (e.g., [1-4]). Why you consider graph clustering tasks instead of more commonly used node classication tasks.\n5. Although the most imporant claimed advantage is scalability, the datasets used for evaluation are really small. The authors should consider use larger graphs.\n\nReferences:\n[1] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive representation learning. arXiv preprint arXiv:2006.04131, 2020b.\n[2] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In WWW, 2021.\n[3] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Rémi Munos, Petar Velickovic, and Michal Valko. Bootstrapped representation learning on graphs. arXiv preprint arXiv:2102.06514, 2021.\n[4] Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonical correlation analysis to self-supervised graph neural networks. In NeurIPS, 2021.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nOverall, the propsoed method is novel and is clearly presented.\n\n# Summary Of The Review\n\nGenerally, I think the motivation of this paper is good. However, I think the propsoed method is over-complicted while does not show prominently better performance than simple methods. Besides, I believe the proposed method is not properly evaluated, in terms of tasks, datasets and baselines. I am leaning on rejection.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDISENTANGLED CONDITIONAL VARIATIONAL AUTOENCODER FOR UNSUPERVISED ANOMALY DETECTION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRecently, generative models have shown promising performance in anomaly detection tasks. Specifically, autoencoders learn representations of high-dimensional data, and their reconstruction ability can be used to assess whether a new instance is likely to be anomalous. However, the primary challenge of unsupervised anomaly detection (UAD) is in learning appropriate disentangled features and avoiding information loss, while incorporating known sources of variation to improve the reconstruction. In this paper, we propose a novel architecture of generative autoencoder by combining the frameworks of β-VAE, conditional variational autoencoder (CVAE), and the principle of total correlation (TC). We show that our architecture improves the disentanglement of latent features, optimizes TC loss more efficiently, and improves the ability to detect anomalies in an unsupervised manner with respect to high-dimensional instances, such as in imaging datasets. Through both qualitative and quantitative experiments on several benchmark datasets, we demonstrate that our proposed method excels in terms of both anomaly detection and capturing disentangled features. Our analysis underlines the importance of learning disentangled features for UAD tasks.\n\n1\n\nINTRODUCTION\n\nUnsupervised anomaly detection (UAD) has been a fertile ground for methodological research for several decades. Recently, generative models, such as Variational Autoencoders (VAEs) (Kingma & Welling, 2014) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2020; Arjovsky et al., 2017), have shown exceptional performance at UAD tasks. By learning the distribution of normal data, generative models can naturally score new data as anomalous based on how well they can be reconstructed. For a recent review of deep learning for anomaly detection, see Pang et al. (2021).\n\nIn a complex task like UAD, disentanglement as a meta-prior encourages latent factors to be captured by different independent variables in the low-dimensional representation. This phenomenon has been on disply in recent work that has used representation learning as a backbone for developing new VAE architectures. Some of the methods proposed new objective functions (Higgins et al., 2017; Mathieu et al., 2019), efficient decomposition of the evidence lower bound (ELBO) (Chen et al., 2018), partitioning of the latent space by adding a regularization term to the mutual information function (Zhao et al., 2017), introducing disentanglement metrics (Kim & Mnih, 2018), and penalizing total correlation (TC) loss (Gao et al., 2019). Penalized TC efficiently learns disentangled features and minimizes the dependence across the dimension of the latent space. However, it often leads to a loss of information, which leads to lower reconstruction quality. For example, methods such as β-VAE, Disentangling by Factorising (FactorVAE) (Kim & Mnih, 2018), and Relevance FactorVAE (RFVAE) (Kim et al., 2019) encourage more factorized representations with the cost of either losing reconstruction quality or losing a considerable among of information about the data and drop in disentanglement performance. To draw clear boundaries between an anomalous sample and a normal sample, we must minimize information loss.\n\nTo address these limitations, we present Disentangled Conditional Variational Autoencoder (dCVAE). Our approach is based on multivariate mutual information theory. Our main contribution is\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\na generative modeling architecture which learns disentangled representations of the data while minimizing the loss of information and thus maintaining good reconstruction capabilities. We achieve this by modeling known sources of variation, in a similar fashion as Conditional VAE (Pol et al., 2019).\n\nOur paper is structured as follows. We first briefly discuss related methods (Section 2), draw connection between them, and present our proposed method dCVAE (Section 3). In Section 4, we discuss our experimental design including competing methods, datasets, and model configuration. Finally, experimental results are presented in Section 5, and Section 6 concludes this paper.\n\n2 RELATED WORK\n\nIn this section, we discuss related work on autoencoders. We focus on two types of architecture: extensions of VAE enforcing disentanglement, and architectures based on mutual information theory.\n\n2.1 β-VAE\n\nβ-VAE and its extensions proposed by (Higgins et al., 2017; Mathieu et al., 2019; Chen et al., 2018) is an augmentation of the original VAE with learning constraints of β applied to the objective function of the VAE. The idea of including such a hyper-parameter is to balance the latent channel capacity and improve the reconstruction accuracy. As a result, β-VAE is capable of discovering the disentangled latent factors and generating more realistic samples while retaining the small distance between the actual and estimated distributions.\n\nRecall the objective function of VAE proposed by Kingma & Welling (2014):\n\nLVAE(θ, φ) = −Ez∼qφ(z|x) log pθ(x | z) + DKL (qφ(z | x)∥pθ(z)) .\n\n(1)\n\nHere, pθ(x | z) is the probabilistic decoder, qφ(z | x) is the recognition model, KLD is denoted by DKL(qφ(z | x)∥pθ(z | x)) parameterized by the weights (θ) and bias (φ) of inference and generative models. As the incentive of β-VAE is to introduce the disentangling property, maximizing the probability of generating original data, and minimizing the distance between them, a constant δ is introduced in the objective VAE to formulate the approximate posterior distributions as below:\n\nEx∼X\n\n(cid:2)Eqφ(z|x) [log pθ(x | z)](cid:3)\n\nmax φ,θ\n\nsuch that DKL (qφ(z | x)||p(z)) < δ.\n\n(2)\n\nRewriting the Equation in Lagrangian form and using the KKT conditions, Higgins et al. (2017) derive the following objective function:\n\nLβV AE(θ, φ) = Eqφ(z|x) [log pθ(x | z)] − βDKL (qφ(z | x)∥p(z)) ,\n\n(3)\n\nHere, β is the regularization coefficient that enforces the constraints to limit the capacity of the latent information z. When β = 1, we recover the original VAE. Increasing the value of β > 1 enforces the constraints to capture disentanglement. However, Hoffman et al. (2017) argue that with an implicit prior, optimizing the regularized ELBO is equivalent to performing variational expectation maximization (EM).\n\n2.2 FACTORVAE\n\nDisentangling by Factorising or FactorVAE is another modification of β-VAE proposed by Kim & Mnih (2018). FactorVAE emphasizes the trade-off between disentanglement and reconstruction quality. The authors primarily focused on the objective function of the VAE and β-VAE. The authors propose a new loss function to mitigate the loss of information that arise while penalizing both the mutual information and the KLD to enforce disentangled latent factors.\n\nAccording to Hoffman & Johnson (2016) and Makhzani & Frey (2017), the objective function of β-VAE can be further extended into:\n\nEpdata (x)[KL(q(z | x)∥p(z))] = I(x; z) + KL(q(z)∥p(z)),\n\n(4)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nHere, I(x; z) is the mutual information between x and z under the joint distribution pdata (x)q(z | x). FactorVAE learns the second term of KL(q(z)∥p(z)) and resolved the aforementioned issues by introducing total correlation penalty and density-ratio trick to approximate the distribution ̄q(z) generated by d samples from q(z). The loss function of the FactorVAE is as follows:\n\nE\n\nq(z|x(i))\n\n(cid:104)\n\nlog p\n\n(cid:16)\n\nx(i) | z\n\n(cid:17)(cid:105)\n\n− KL\n\n(cid:16)\n\n(cid:16)\n\nz | x(i)(cid:17)\n\nq\n\n(cid:17)\n\n∥p(z)\n\n− γKL(q(z)∥q(z))\n\n(5)\n\n2.3 THE PRINCIPLE OF TOTAL CORRELATION EXPLANATION (COREX)\n\nGao et al. (2019) introduced CorEx to mitigate the problem of learning disentangled and interIn general, for VAE, we assume pretable representations in a purely information-theoretic way. a generative model where x is a function of a latent variable z, and afterward maximize the log likelihood of x. On the other hand, CorEx follows the reverse process where z is a stochastic function of x parameterized by θ, i.e., pθ(z | x), and seek to estimate the joint distribution pθ(x, z) = pθ(z | x)p(x). The underlying true data distribution maximizes the following objective:\n\nL(θ; x) = T Cθ(x; z) (cid:124) (cid:125) (cid:123)(cid:122) informativeness\n\n− T Cθ(z) (cid:124) (cid:123)(cid:122) (cid:125) (dis)entanglement\n\n= T C(x) − T Cθ(x | z) − T Cθ(z).\n\n(6)\n\nRecall the definition of the total correlation (TC) in terms of entropy H (x) (Studen`y & Vejnarov ́a, 1998):\n\nT C(x) =\n\nd (cid:88)\n\ni=1\n\nH (xi) − H(x) = DKL\n\np(x)∥\n\n(cid:32)\n\n(cid:33)\n\np (xi)\n\n.\n\nd (cid:89)\n\ni=1\n\n(7)\n\nBy non-negativity of TC, Equation 6 naturally forms variational lower bound T C(x) to the CorEx objective, i.e., T C(x) ≥ L(θ; x) for any θ. Equation 6 can be rewritten in terms of mutual information I(x : z) = H(x) − H(x | z) = H(z) − H(z | x). Further constraining the search space pθ(z | x) to have the factorized form pθ(z | x) = (cid:81)m i=1 pθ(zi | x) and the mutual information terms can be bounded by approximating the conditional distributions pθ(xj | z) and pθ(zj | x). Finally, we can further rewrite and derive the lower bound for the objective function:\n\nL(θ; x) =\n\nd (cid:88)\n\ni=1\n\nIθ (xi : z) −\n\nm (cid:88)\n\ni=1\n\nIθ (zi : x)\n\n≥\n\n(cid:32) d\n\n(cid:88)\n\ni=1\n\n(cid:33)\n\nH (xi)\n\n+ Epθ(x,z)\n\n\n\n\n\n log qφ(x | z) (cid:124) (cid:123)(cid:122) (cid:125) decoder\n\n \n\n− DKL(pθ(z | x) (cid:124) (cid:123)(cid:122) (cid:125) encoder\n\n∥rα(z)).\n\n(8)\n\n2.4 TOTAL CORRELATION VARIATIONAL AUTOENCODER (β-TCVAE)\n\nChen et al. (2018) proposed disentanglement in their learned representations by adjusting the functional structure of the ELBO objective. The authors argued that each dimension of a disentangled representation should be able to represent a different factor of variation in the data and be changed independently of the other dimensions. β-TCVAE modifies the originally proposed ELBO objective by Higgins et al. (2017) forcing the algorithm to learn representations without explicitly making restrictions or reduction to the latent space. Recall the ELBO objective function (Equation 3) of β-VAE:\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nLβV AE(θ, φ) = Eqφ(z|x) [log pθ(x | z)] − βDKL (qφ(z | x)∥p(z))\n\n(9)\n\nTo introduce TC and disentanglement into the original β-VAE, Chen et al. decomposed the original KLD into Index-Code MI, Total Correlation and Dimension-wise KL terms. Furthermore, in the ELBO TC-Decomposition, each training samples are identified with a unique index n and a uniform random variable that refers to the aggregated posterior as q(z) = (cid:80)N n=1 q(z | n)p(n) and can be denoted as:\n\nEp(n)[KL(q(z | n)∥p(z))] = KL(q(z, n)∥q(z)p(n)) + KL\n\nq(z)∥\n\n\n\n\n\nq (zj)\n\n\n\n(cid:89)\n\nj\n\n(10)\n\n(cid:88)\n\n+\n\nj\n\nKL (q (zj) ∥p (zj))\n\nFinally, with a set of latent variables zj, with known factors vk, the authors introduced a disentanglement measuring metric called mutual information gap (MIG) and defined in terms of empirical mutual information In (zj; vk):\n\n(cid:18)\n\n1 K\n\nK (cid:88)\n\nk=1\n\n1 H (vk)\n\nIn\n\n(cid:0)zj(k); vk\n\n(cid:1) − max\n\nj̸=j(k)\n\n(cid:19)\n\nIn (zj; vk)\n\n(11)\n\nHere, j(k) = argmaxj In (zj; vk) and K is the number of known factors under vk.\n\n3 DISENTANGLED CONDITIONAL VARIATIONAL AUTOENCODER (DCVAE)\n\nOur approach builds on CorEx and models known sources of variation in the data, in a manner similar to Conditional Variational Autoencoder (CVAE) Pol et al. (2019). In what follows, we will represent this known source of variation using the variable C. In the experiment below, C is discrete and represents the class of each image. Modifying Equation 6 to incorporate C, we get\n\nL(θ; x, c) = T Cθ(x | c) − T Cθ(x | z, c) − T Cθ(z | c).\n\n(12)\n\nRecall that the first two terms measure the amount of correlation explained by z, and by maximizing it, we maximize the informativeness of the latent representation. The third term measures the correlation between the components of z, and by minimizing it, we maximize the disentanglement between the latent dimensions.\n\nUsing Mutual Information Theory (Studen`y & Vejnarov ́a, 1998), we can define the conditional differential entropy of H(x) given c and interpret mutual information as a reduction in uncertainty after conditioning:\n\nI(x : z | c) = H(x | c) + H(z | c) − H(x, z | c)\n\nI(x : z | c) = H(x | c) − H(x | z, c) = H(z | c) − H(z | x, c).\n\nWe can now rewrite Equation 12 using derived mutual information theory from Equation 13:\n\nL(θ; x, c) =\n\np (cid:88)\n\nj=1\n\nI (xj : z | c) −\n\nd (cid:88)\n\nj=1\n\nI (zj : x | c) .\n\n(13)\n\n(14)\n\nNow, consider the KLD between pθ(x | z, c) and an approximating distribution qφ(x | z, c). In terms of expectations with respect to the joint distribution pθ(x, z | c), we can write:\n\n− H(x | z, c) = E(log pθ(x | z, c)) ≥ E(log qφ(x | z, c)).\n\n(15)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nCombing Equation 14 and 15 and assuming an approximating distribution rα(zj | c) for pθ(zj | c), we obtain two inequalities:\n\nI (xj : z | c) = H (xj | c) − H (xj | z, c) ≥ H (xj | c) + E(log qφ(x | z, c)), I (zj : x | c) = DKL (pθ (zj | x, c) ∥rα (zj | c)) .\n\n(16)\n\n(17)\n\nCombining these bounds, we finally derive a lower bound for the objective function for dCVAE:\n\nL(θ; x, c) ≥\n\np (cid:88)\n\nj=1\n\nH (xj | c) + E (log qφ(x | z, c)) −\n\nd (cid:88)\n\nj=1\n\nDKL\n\n(cid:0)p(zj |x,c)∥r(zj |c)\n\n(cid:1) .\n\n(18)\n\nEquation 18 illustrates the lower bound objective function of dCVAE where qφ(x | z, c) is the generative model or decoder and pθ (zj | x, c) is the recognition model or encoder.\n\n4 EXPERIMENTS\n\nIn the experiments below, we compare our dCVAE method to five baseline methods: VAE, CVAE, β-VAE, Factor-VAE, and RFVAE. The first two methods were selected as well-known baselines that do not explicitly enforce disentanglement; on the other hand, the latter three methods seek to achieve a disentangled representation of the data.\n\n4.1 DATASETS\n\nWe evaluate dCVAE and other baseline models on the following four datasets. Three datasets (MNIST (Deng, 2012), Fashion-MNIST (Xiao et al., 2017), KMNIST (Clanuwat et al., 2018)) are trained for UAD. The fourth dataset (EMNIST (Cohen et al., 2017)) is used for testing on a realworld dataset to assess overall performance. A more detailed description of these datasets follows:\n\n• MNIST and Fashion-MNIST (FMNIST) Firstly, we apply all models to two benchmark datasets, MNIST and Fashion-MNIST, for a fair comparison with other baseline methods. We used 10 classes with 60000 and 10000 training and testing samples for both datasets with 28 × 28 × 1 pixels channel.\n\n• KMNIST Secondly, we applied the same training process to another complex dataset, Kuzushiji-MNIST or KMNIST. KMNIST is a drop-in replacement for the MNIST dataset, a Japanese cursive writing style. KMNIST contains similar 10 classes with 60000 and 10000 training and testing samples with 28 × 28 × 1 pixels channel.\n\n• EMNIST Finally, all models are tested on the Extending MNIST or EMNIST Dataset. Using all 62 classes (digit 0-9, letters uppercase A-Z and lowercase a-z) with 700000 and 80000 training and testing samples with 28 × 28 × 1 pixels channels. The dataset was processed from NIST Special Database 19 Grother (1995) and contained handwritten digits and characters collected from over 500 writers.\n\n4.2 RECONSTRUCTION ERROR AND ANOMALY SCORE\n\nLeveraging methods for the discriminator as the anomaly score and drawing separation between normal and anomalous data is challenging for the divergent architectures of autoencoders. Depending on the task the architecture is trained for, the discriminator varies greatly. In general, the UAD methods utilize reconstruction error (Baur et al., 2018), distribution-based error (Goldstein & Uchida, 2016), and density-based error (Kiran et al., 2018) scores to distinguish normal and anomalous data. Formally, for each input x, a test input (cid:98)xl is considered to be anomalous if reconstruction error or Anomaly score(A) is greater than the minimum threshold value and denoted as follows:\n\nA(ˆx) = ∥x − D(G(ˆx))∥2.\n\n(19)\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4.3 PERFORMANCE METRICS\n\nOne of the challenges of measuring the performance of disentanglement is to apply appropriate metrics based on the nature of the dataset, not of latent factors or dimensions in the latent space. Therefore, considering the different model architectures and datasets, we first measure the performance using Numerical AUC Score, reconstruction error (A), and negative ELBO score (E). These metrics provide a quantifiable method of accuracy, while also measuring the disentanglement among the latent factors.\n\nWe also measure performance qualitatively by visualizing the latent space and the 2D-manifold. Both allow us to visualize the orthogonality between latent features and demonstrate the accuracy of the models to handle reduced latent variables and the ability to reconstruct samples.\n\n4.4 MODEL CONFIGURATION\n\nA fixed set of hyper-parameters are chosen to formulate a similar platform for all models and identify the computational cost and reproducibility of the models. Although baseline models that we chose, β-VAE, FactorVAE, RFVAE are highly sensitive to hyper-parameters tuning, the hyper-parameters throughout the experiment are kept consistent to observe how the models perform under similar values. A minimal 50 epochs are used to train the datasets. For MNIST, FMNIST, and KMNIST the batch size is kept to 64, with primary and secondary learning rates as α = 10−5 and α = 10−3 respectively. However, for the EMNIST dataset, the batch size increased to 128, and learning rates as α = 10−6 and α = 10−5.\n\n5 RESULTS AND DISCUSSION\n\nIn this section, we evaluate the results of dCVAE and other baseline methods on the downstream task of anomaly detection. A considerable volume of results was produced from our exhaustive evaluation. However, accounting for limitations of space here, we elected to focus on the results from EMNIST and KMNIST datasets in the main text. The remaining results (MNIST and FMNIST) are presented as Supplementary Material.\n\nWe show the results of our evaluation in three stages: firstly, using sample reconstruction and the negative ELBO score (E) with reconstruction error A, we evaluate and compare the disentanglement ability of dCVAE with baseline architectures. Secondly, we use the UMAP algorithm (Sainburg et al., 2021) to reduce dimensions and visualize both latent representation, as well as interpolation of the 2D-manifold to distinguish the TC by comparing information loss and effects of modeling known sources of variation. Finally, we present AUC scores and training time to summarize the overall accuracy of the experimented methods.\n\nWe evaluate the quality of disentanglement by considering explicit separation of A between normal and anomalous data and minimization of E. A better disentanglement is achieved when:\n\n(a) A higher reconstruction error A for anomalous sample and lower reconstruction error A for\n\nnormal sample is obtained and\n\n(b) E is minimized by enforcing regularization that either minimizes the negative ELBO decompo-\n\nsition DKL\n\n(cid:0)p(zj |x,c)∥r(zj |c)\n\n(cid:1) or regularizes the approximate posterior qφ(z | x).\n\nA clear boundary in terms of learning efficient disentanglement between dCVAE and baseline methods can be observed from both EMNIST (Figure 1) and KMNIST (Figure 2) reconstruction. The first row corresponds to anomalous reconstruction and the second row shows normal sample reconstruction. Both E and A score suggests that dCVAE captures more independent factors and identifies anomalous and normal samples efficiently. This observation strongly justifies one of our primary claims, namely that dCVAE incorporates the disentanglement learning through enforcing TC and restrict independent latent variables to prioritize the minimization of the divergence. The other disentanglement methods presented here either only emphasize TC (indicated by the dependence between random variables) or introduce β (weighing the prior enforcement term), which limits the ability to learn randomness in a case when the hyperparameters are not tuned for certain dimensions.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a) E = -235 A = 0.97\n\n(b) E = -248 A = 0.90\n\n(c) E = -245 A = 0.91\n\n(d) E = -246 A = 0.91\n\n(e) E = -244 A = 0.92\n\n(f) E = -246 A = 0.92\n\n(g) E = -171 A = 0.33\n\n(h) E = -199 A = 0.48\n\n(i) E = -180 A = 0.53\n\n(j) E = -185 A = 0.52\n\n(k) E = -195 A = 0.57\n\n(l) E = -190 A = 0.55\n\nFigure 1: Reconstruction for digit zero (0) and the capital letter O. Here, E refers to Negative ELBO score and A is the reconstruction error or anomaly score. Only dCVAE and FactorVAE show steady improvement for both types of reconstruction. All the other methods misclassify the samples. Moreover, we can observe higher reconstruction error and ELBO scores compared to MNIST (Figure A1) and FMNIST (Figure A2).\n\n(a) E = -251 A = 0.98\n\n(b) E = -281 A = 0.90\n\n(c) E = -279 A = 0.98\n\n(d) E =-266 A = 0.97\n\n(e) E = -277 A = 0.97\n\n(f) E = -270 A = 0.96\n\n(g) E = -188 A = 0.28\n\n(h) E = -211 A = 0.47\n\n(i) E = -185 A = 0.41\n\n(j) E = -201 A = 0.57\n\n(k) E = -183 A = 0.41\n\n(l) E = -170 A = 0.38\n\nFigure 2: In KMNIST dataset, without dCVAE, all other methods fail to classify both anomalous and normal samples. Reconstruction scores suggest FactorVAE, VAE almost fail to distinguish normal and anomalous observations. Since the stroke of the samples are similar in this dataset, methods that only emphasize disentanglement or empirical approximation lose more information in latent variable resulting in false anomaly detection.\n\nThe second observation is drawn using latent representation (Figure 3) and 2D-manifold embeddings (Figure 4 and 5). Through this experiment, we observe the effect of modeling using a known source of variation (i.e. introducing conditional variable C into the objective function) and minimizing information loss through multivariate mutual information theory (i.e. decomposition of TC). We can observe clear similarities between KLD loss and modeling with known score of variance in a reduced latent space. Due to enforced divergence loss, the plot of VAE and β-VAE are noticeably different from other architectures. Feature space is more compact for VAE, β-VAE, and we can see the cluster of the different classes are not well separated. However, conditioning the generative function (encoder) of CVAE and dCVAE provides the leverage to construct higher feature space and retain more accurate information in 2D-manifold (EMNIST, Figure 4; and KMNIST, Figure 5). Furthermore, TC reduces the correlation among disentanglement degrees when a specific feature is\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nlearned (shape, strokes, color, boundaries). Such classes can be observed to cluster together and the other gets scattered with higher feature space (Figure 3). Compared to other methods, it is evident that dCVAE maintain consistent latent space and create separate clusters more accurately. This indicates that more disentangled variables are captured, and they retain more information through conditioning the generative model by minimizing the ELBO DKL\n\n(cid:0)p(zj |x,c)∥r(zj |c)\n\n(cid:1).\n\n(a) EMNIST\n\n(b) KMNIST\n\nFigure 3: Latent Representation of EMNIST and KMNIST\n\n(a) dCVAE\n\n(b) VAE\n\n(c) CVAE\n\n(d) FactorVAE\n\n(e) β-VAE\n\n(f) RFVAE\n\nFigure 4: Manifold Embeddings (EMNIST)\n\n(a) dCVAE\n\n(b) VAE\n\n(c) CVAE\n\n(d) FactorVAE\n\n(e) β-VAE\n\n(f) RFVAE\n\nFigure 5: Manifold Embeddings (KMNIST)\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFinally, Table 1 illustrates the results of model evaluation through AUC score and training time. dCVAE outperforms other methods in terms of AUC score. However, for larger divergent datasets like KMNIST and EMNIST, VAE shows lower training time compared to dCVAE. Since VAE only optimizes the negative log-likelihood, reconstruction loss and prior enforcement term, the training takes fewer latent variables to regularize, resulting in less training time. Nevertheless, compared to methods that incorporate TC (e.g. FactorVAE and RFVAE) or a constraint on the posterior (β-VAE), our proposed dCVAE scales to all larger datasets with higher classification accuracy.\n\nTable 1: Evaluation metrics score\n\nModel\n\nMNIST\n\nTraining\n\nFMNIST\n\nTraining\n\nEMNIST\n\nTraining\n\nKMNIST\n\nTraining\n\nAUC\n\n88.31 88.21 87.57 87.11 85.31 85.31\n\nTime (min) 37 37 43 53 51 55\n\nAUC\n\n88.63 84.12 83.31 82.78 82.31 81.11\n\nTime (min) 44 39 48 50 53 57\n\nAUC\n\n78.98 67.23 66.01 62.91 65.12 55.03\n\nTime (min) 102 92 117 138 123 130\n\nAUC\n\n61.02 51.13 42.35 49.23 50.01 49.51\n\nTime (min) 95 78 104 117 119 132\n\ndCVAE VAE CVAE FactorVAE β-VAE RFVAE\n\nThe only trade-offs in our proposed method seem to occur when minimizing the negative ELBO In certain conditions, dCVAE reaches a lower reconstruction loss (anomalous sample) yet loss. minimizes the negative ELBO score (Figure 3, 4). In general, negative ELBO loss should illustrate symmetrical change with reconstruction error. Such inconsistency could lead to a significant drop in the classification accuracy, thus leading to a false anomaly detection result.\n\n6 CONCLUSION\n\nIn this research, we present a novel generative variational model dCVAE, to improve the unsupervised anomaly detection task through disentanglement learning, TC loss, and minimizing trade-offs between reconstruction loss and reconstruction quality. Introducing a conditional variable to mitigate the loss of information effectively captures more disentangled features and produces more accurate reconstructions. Such architecture could be used in a wider range of applications, including generating controlled image synthesis, efficient molecular design and generation, source separation for bio-signals and images, and conditional text generation. Future research direction includes investigating in the gap between the posterior and the prior distribution, resolving the trade-offs between loss function and reconstruction, and inspect dCVAE using different disentanglement metrics.\n\nREPRODUCIBILITY STATEMENT\n\nIn this research, we carefully considered reproducibility in designing and conducting all experiments. In our supplemental texts, we have attached our source code. The experiments are designed independently to make the results reproducible. Image reconstruction and generation, 2D-Manifold embeddings, training time, and ELBO score calculation are performed separately from other downstream tasks like classification accuracy, reconstruction error, and latent representation. Furthermore, we used both TensorFlow and PyTorch frameworks to remove package dependencies. To remove the library dependencies and installation issues, virtual environment and package requirement files are also added. Finally, to make the results more accessible, we also provided randomly generated images with supplementary texts.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein generative adversarial networks.\n\nIn International conference on machine learning, pp. 214–223. PMLR, 2017.\n\nChristoph Baur, Benedikt Wiestler, Shadi Albarqouni, and Nassir Navab. Deep autoencoding models for unsupervised anomaly segmentation in brain mr images. In International MICCAI brainlesion workshop, pp. 161–169. Springer, 2018.\n\nRicky TQ Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement in VAEs. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 2615–2625, 2018.\n\nTarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David\n\nHa. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718, 2018.\n\nGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr ́e van Schaik. Emnist: an extension of\n\nmnist to handwritten letters (2017). arXiv preprint arXiv:1702.05373, 2017.\n\nLi Deng. The MNIST database of handwritten digit images for machine learning research [best of\n\nthe web]. IEEE Signal Processing Magazine, 29(6):141–142, 2012.\n\nShuyang Gao, Rob Brekelmans, Greg Ver Steeg, and Aram Galstyan. Auto-encoding total correlation explanation. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1157–1166. PMLR, 2019.\n\nMarkus Goldstein and Seiichi Uchida. A comparative evaluation of unsupervised anomaly detection\n\nalgorithms for multivariate data. PloS one, 11(4):e0152173, 2016.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020.\n\nPatrick J Grother. Nist special database 19. Handprinted forms and characters database, National\n\nInstitute of Standards and Technology, 10, 1995.\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017.\n\nMatthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS, volume 1, 2016.\n\nMatthew D Hoffman, Carlos Riquelme, and Matthew J Johnson. The β-vae’s implicit prior.\n\nIn\n\nWorkshop on Bayesian Deep Learning, NIPS, pp. 1–5, 2017.\n\nHyunjik Kim and Andriy Mnih. Disentangling by factorising.\n\nIn International Conference on\n\nMachine Learning, pp. 2649–2658. PMLR, 2018.\n\nMinyoung Kim, Yuting Wang, Pritish Sahu, and Vladimir Pavlovic. Relevance factor vae: Learning\n\nand identifying disentangled factors. arXiv preprint arXiv:1902.01568, 2019.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational Bayes.\n\nIn 2nd International\n\nConference on Learning Representations, ICLR, 2014.\n\nB Ravi Kiran, Dilip Mathew Thomas, and Ranjith Parakkal. An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos. Journal of Imaging, 4(2):36, 2018.\n\nAlireza Makhzani and Brendan J Frey. Pixelgan autoencoders. Advances in Neural Information\n\nProcessing Systems, 30, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nEmile Mathieu, Tom Rainforth, Nana Siddharth, and Yee Whye Teh. Disentangling disentanglement in variational autoencoders. In International Conference on Machine Learning, pp. 4402–4412. PMLR, 2019.\n\nGuansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for\n\nanomaly detection: A review. ACM Computing Surveys (CSUR), 54(2):1–38, 2021.\n\nAdrian Alan Pol, Victor Berger, Cecile Germain, Gianluca Cerminara, and Maurizio Pierini. Anomaly detection with conditional variational autoencoders. In 2019 18th IEEE international conference on machine learning and applications (ICMLA), pp. 1651–1657. IEEE, 2019.\n\nTim Sainburg, Leland McInnes, and Timothy Q. Gentner. Parametric UMAP Embeddings for Representation and Semisupervised Learning. Neural Computation, 33(11):2881–2907, 10 2021. ISSN 0899-7667. doi: 10.1162/neco a 01434. URL https://doi.org/10.1162/neco_ a_01434.\n\nMilan Studen`y and Jirina Vejnarov ́a. The multiinformation function as a tool for measuring stochas-\n\ntic dependence. In Learning in graphical models, pp. 261–297. Springer, 1998.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A novel image dataset for bench-\n\nmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n\nShengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational\n\nautoencoders. arXiv preprint arXiv:1706.02262, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nSince we couldn’t accommodate all results in our main paper, in this section we present results produced from MNIST and FMNIST datasets. The results are categorized into three sections: Reconstructions (A.1), Latent Representation (A.2), and 2D-Manifold embeddings (A.3).\n\nA.1 RECONSTRUCTION\n\n(a) E = -152, A = 0.88\n\n(b) E = -181, A = 0.71\n\n(c) E = -170, A = 0.75\n\n(d) E = -166, A = 0.78\n\n(e) E = -176, A = 0.76\n\n(f) E = -178, A = 0.81\n\n(g) E = -106, A = 0.23\n\n(h) E = -140, A = 0.48\n\n(i) E = -136, A = 0.32\n\n(j) E = -132, A = 0.32\n\n(k) E = -126, A = 0.30\n\n(l) E -131, A = 0.31\n\nFigure A1: The reconstruction from the MNIST dataset shows similar negative ELBO and reconstruction error (A) values for CVAE, β-VAE, and RFVAE. our proposed model dCVAE performs best in terms of both reconstructing anomalous observation (first row) and normal observation (second row). We can observe a trade-off in FactorVAE with respect to β-VAE and RFVAE. FactorVAE performs better in reconstructing the anomalous observation whether as the β-VAE shows good performance in normal observations.\n\n(a) E = -187 A = 0.94\n\n(b) E = -198 A = 0.79\n\n(c) E = -192 A = 0.82\n\n(d) E = -221 A = 0.90\n\n(e) E = -231 A = 0.85\n\n(f) E = -229 A = 0.92\n\n(g) E = -151 A = 0.12\n\n(h) E = -170 A = 0.37\n\n(i) E = -162 A = 0.40\n\n(j) E = -165 A = 0.41\n\n(k) E = -155 A = 0.38\n\n(l) E = -162 A = 0.42\n\nFigure A2: Similar to the MNIST dataset, the FMNIST illustrates similar trade-offs among FactorVAE, RFVAE, and β-VAE. However, for some samples, β-VAE mis-classifies the closely matched classes. dCVAE constrains the blurry reconstruction by enforcing conditions in the prior.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA.2 LATENT SPACE VISUALIZATION\n\nFigure A3: Latent Space Representation (MNIST)\n\nFigure A4: Latent Space Representation (FMNIST)\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA.3 LATENT MANIFOLD\n\n(a) dCVAE\n\n(b) VAE\n\n(c) CVAE\n\n(d) FactorVAE\n\n(e) β-VAE\n\n(f) RFVAE\n\nFigure A5: Latent Embeddings (MNIST)\n\n(a) dCVAE\n\n(b) VAE\n\n(c) CVAE\n\n(d) FactorVAE\n\n(e) β-VAE\n\n(f) RFVAE\n\nFigure A6: Latent Embeddings (FMNIST)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA.4 RANDOM GENERATION\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\n19",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors proposed a framework for unsupervised anomaly detection which builds on top of existing prior art on disentangled variational autoencoders (VAE's). Authors do a deep dive on existing prior art techniques such as $\\beta$ - VAE, Factor VAE and others by introducing a conditional variable to avoid loss of information and produce more accurate reconstructions. Authors present empirical results on several image datasets by demonstrating reconstruction score, accuracy and other important metrics for anomaly detection.\n\n# Strength And Weaknesses\n\n**Strengths**\n\n- Paper is straightforward and relatively easy to follow.\n\n**Weakness**\n\n- The paper has this major flaw in its structure\na) Excessive emphasis on related work and prior art sections which is not necessary\n\n- For reasons not known, the authors have conveniently ignored most prominent unsupervised deep anomaly baselines such as \n  ### Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. 2018. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International Conference on Learning Representations.\n\n ### Unsupervised Anomaly Detection with Adversarial Mirrored AutoEncoders. Somepelli et al. UAI 2021\n\n- I also believe that anomaly detection frameworks need to benchmarked with results on metrics such as AUPR and not just indicate AUC improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work is highly incremental in nature and the empirical analysis is insufficient in its current form.\n\n# Summary Of The Review\n\nUnsupervised anomaly detection is an important problem, but new papers in this space need to be more novel as there is exists a significant body of work in this space (within both VAE's and GAN's). In addition, empirical evaluation should make sure the coverage is exhaustive to conclude if the results are significant. The current paper lacks most of these insights.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nHYPERDEEPONET: LEARNING OPERATOR WITH COMPLEX TARGET FUNCTION SPACE USING THE LIMITED\n\nRESOURCES VIA HYPERNETWORK\n\nJae Yong Lee∗ Center for Artificial Intelligence and Natural Sciences Korea Institute for Advanced Study Seoul, 02455, Republic of Korea {jaeyong}@kias.re.kr\n\nSung Woong Cho∗& Hyung Ju Hwang † Department of Mathematics Pohang University of Science and Technology Pohang, 37673, Republic of Korea {swcho95kr,hjhwang}@postech.ac.kr\n\nABSTRACT\n\nFast and accurate predictions for complex physical dynamics are a significant challenge across various applications. Real-time prediction on resource-constrained hardware is even more crucial in real-world problems. The deep operator network (DeepONet) has recently been proposed as a framework for learning nonlinear mappings between function spaces. However, the DeepONet requires many parameters and has a high computational cost when learning operators, particularly those with complex (discontinuous or non-smooth) target functions. This study proposes HyperDeepONet, which uses the expressive power of the hypernetwork to enable the learning of a complex operator with a smaller set of parameters. The DeepONet and its variant models can be thought of as a method of injecting the input function information into the target function. From this perspective, these models can be viewed as a particular case of HyperDeepONet. We analyze the complexity of DeepONet and conclude that HyperDeepONet needs relatively lower complexity to obtain the desired accuracy for operator learning. HyperDeepONet successfully learned various operators with fewer computational resources compared to other benchmarks.\n\n1\n\nINTRODUCTION\n\nOperator learning for mapping between infinite-dimensional function spaces is a challenging problem. It has been used in many applications, such as climate prediction (Kurth et al., 2022) and fluid dynamics (Guo et al., 2016). The computational efficiency of learning the mapping is still important in real-world problems. The target function of the operator can be discontinuous or sharp for complicated dynamic systems. In this case, balancing model complexity and cost for computational time is a core problem for the real-time prediction on resource-constrained hardware (Choudhary et al., 2020; Murshed et al., 2021).\n\nMany machine learning methods and deep learning-based architectures have been successfully developed to learn a nonlinear mapping from an infinite-dimensional Banach space to another. They focus on learning the solution operator of some partial differential equations (PDEs), e.g., the initial or boundary condition of PDE to the corresponding solution. Anandkumar et al. (2019) proposed an iterative neural operator scheme to learn the solution operator of PDEs.\n\nSimultaneously, Lu et al. (2019; 2021) proposed a deep operator network (DeepONet) architecture based on the universal operator approximation theorem of Chen & Chen (1995). The DeepONet consists of two networks: branch net taking an input function at fixed finite locations, and trunk net taking a query location of the output function domain. Each network provides the p outputs. The two p-outputs are combined as a linear combination (inner-product) to approximate the underlying operator, where the branch net produces the coefficients (p-coefficients) and the trunk net produces the basis functions (p-basis) of the target function.\n\n∗These authors contributed equally. †corresponding author\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nWhile variant models of DeepONet have been developed to improve the vanilla DeepONet, they still have difficulty approximating the operator for a complicated target function with limited computational resources. Lanthaler et al. (2022) and Kovachki et al. (2021b) pointed out the limitation of linear approximation in DeepONet. Some operators have a slow spectral decay rate of the Kolmogorov n-width, which defines the error of the best possible linear approximation using an n−dimensional space. A large n is required to learn such operators accurately, which implies that the DeepONet requires a large number of basis p and network parameters for them.\n\nHadorn (2022) investigated the behavior of DeepONet, to find what makes it challenging to detect the sharp features in the target function when the number of basis p is small. They proposed a Shift-DeepONet by adding two neural networks to shift and scale the input function. Venturi & Casey (2023) also analyzed the limitation of DeepONet via singular value decomposition (SVD) and proposed a flexible DeepONet (flexDeepONet), adding a pre-net and an additional output in the branch net. Recently, to overcome the limitation of the linear approximation, Seidman et al. (2022) proposed a nonlinear manifold decoder (NOMAD) framework by using a neural network that takes the output of the branch net as the input along with the query location. Even though these methods reduce the number of basis functions, the total number of parameters in the model cannot be decreased. The trunk net still requires many parameters to learn the complex operators, especially with the complicated (discontinuous or non-smooth) target functions.\n\nIn this study, we propose a new architecture, HyperDeepONet, which enables operator learning, and involves a complex target function space even with limited resources. The HyperDeepONet uses a hypernetwork, as proposed by Ha et al. (2017), which produces parameters for the target network. Wang et al. (2022) pointed out that the final inner product in DeepONet may be inefficient if the information of the input function fails to propagate through a branch net. The hypernetwork in HyperDeepONet transmits the information of the input function to each target network’s parameters. Furthermore, the expressivity of the hypernetwork reduces the neural network complexity by sharing the parameters (Galanti & Wolf, 2020). Our main contributions are as follows.\n\n• We propose a novel HyperDeepONet using a hypernetwork to overcome the limitations of DeepONet and learn the operators with a complicated target function space. The DeepONet and its variant models are analyzed primarily in terms of expressing the target function as a neural network (Figure 4). These models can be simplified versions of our general HyperDeepONet model (Figure 5).\n\n• We analyze the complexity of DeepONet (Theorem 2) and prove that the complexity of the HyperDeepONet is lower than that of the DeepONet. We have identified that the DeepONet should employ a large number of basis to obtain the desired accuracy, so it requires numerous parameters. For variants of DeepONet combined with nonlinear reconstructors, we also present a lower bound for the number of parameters in the target network.\n\n• The experiments show that the HyperDeepONet facilitates learning an operator with a small number of parameters in the target network even when the target function space is complicated with discontinuity and sharpness, which the DeepONet and its variants suffer from. The HyperDeepONet learns the operator more accurately even when the total number of parameters in the overall model is the same.\n\n2 RELATED WORK\n\nMany machine learning methods and deep learning-based architectures have been successfully developed to solve PDEs with several advantages. One research direction is to use the neural network directly to represent the solution of PDE (E & Yu, 2018; Sirignano & Spiliopoulos, 2018). The physics-informed neural network (PINN), introduced by Raissi et al. (2019), minimized the residual of PDEs by using automatic differentiation instead of numerical approximations.\n\nThere is another approach to solve PDEs, called operator learning. Operator learning aims to learn a nonlinear mapping from an infinite-dimensional Banach space to another. Many studies utilize the convolutional neural network to parameterize the solution operator of PDEs in various applications (Guo et al., 2016; Bhatnagar et al., 2019; Khoo et al., 2021; Zhu et al., 2019; Hwang et al., 2021). The neural operator (Kovachki et al., 2021b) is proposed to approximate the nonlinear operator inspired by Green’s function. Li et al. (2021) extend the neural operator structure to the Fourier Neural\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nOperator (FNO) to approximate the integral operator effectively using the fast Fourier transform (FFT). Kovachki et al. (2021a) proved the universality of FNO and identified the size of the network.\n\nThe DeepONet (Lu et al., 2019; 2021) has also been proposed as another framework for operator learning. The DeepONet has significantly been applied to various problems, such as bubble growth dynamics (Lin et al., 2021), hypersonic flows (Mao et al., 2021), and fluid flow (Cai et al., 2021). Lanthaler et al. (2022) provided the universal approximation property of DeepONet. Wang et al. (2021) proposed physics-informed DeepONet by adding a residual of PDE as a loss function, and Ryck & Mishra (2022) demonstrated the generic bounds on the approximation error for it. Prasthofer et al. (2022) considered the case where the discretization grid of the input function in DeepONet changes by employing the coordinate encoder. Lu et al. (2022) compared the FNO with DeepONet in different benchmarks to demonstrate the relative performance. FNO can only infer the output function of an operator as the input function in the same grid as it needs to discretize the output function to use Fast Fourier Transform(FFT). In contrast, the DeepONet can predict from any location.\n\nFigure 1: Example of operator learning: the input function and the output function for the solution operator of shallow water equation\n\nHa et al. (2017) first proposed hypernetwork, a network that creates a weight of the primary network. Because the hypernetwork can achieve weight sharing and model compression, it requires a relatively small number of parameters even as the dataset grows. Galanti & Wolf (2020) proved that a hypernetwork provides higher expressivity with low-complexity target networks. Sitzmann et al. (2020) and Klocek et al. (2019) employed this approach to restoring images with insufficient pixel observations or resolutions. de Avila Belbute-Peres et al. (2021) investigated the relationship between the coefficients of PDEs and the corresponding solutions. They combined the hypernetwork with the PINN’s residual loss. For time-dependent PDE, Pan et al. (2022) designated the time t as the input of the hypernetwork so that the target network indicates the solution at t. von Oswald et al. (2020) devised a chunk embedding method that partitions the parameters of the target network; this is because the output dimension of the hypernetwork can be large.\n\n3 OPERATOR LEARNING\n\n3.1 PROBLEM SETTING\n\nThe goal of operator learning is to learn a mapping from infinite-dimensional function space to the others using a finite pair of functions. Let G : U → S be a nonlinear operator, where U and S are compact subsets of infinite-dimensional function spaces U ⊂ C(X ; Rdu ) and S ⊂ C(Y; Rds) with compact domains X ⊂ Rdx and Y ⊂ Rdy . For simplicity, we focus on the case du = ds = 1, and all the results could be extended to a more general case for arbitrary du and ds. Suppose we have observations {ui, G(ui)}N i=1 where ui ∈ U and G(ui) ∈ S. We aim to find an approximation Gθ : U → S with parameter θ using the N observations so that Gθ ≈ G. For example, in a dam break scenario, it is an important to predict the fluid flow over time according to a random initial height of the fluid. To this end, we want to find an operator Gθ, which takes an initial fluid height as an input function and produces the fluid height over time at any location as the output function (Figure 1).\n\nAs explained in Lanthaler et al. (2022), the approximator Gθ can be decomposed into the three components (Figure 2) as\n\nGθ := R ◦ A ◦ E.\n\n(1)\n\nFirst, the encoder E takes an input function u from U to generate the finite-dimensional encoded data in Rm. Then, the approximator A is an operator approximator from the encoded data in finite dimension space Rm to the other finite-dimensional space Rp. Finally, the reconstructor R reconstructs the output function s(y) = G(u)(y) with y ∈ Y using the approximated data in Rp.\n\nFigure 2: Diagram for the three components for operator learning.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3.2 DEEPONET AND ITS LIMITATION\n\nDeepONet can be analyzed using the above three decompositions. Assume that all the input functions u are evaluated at fixed locations {xj}m j=1 ⊂ X ; they are called ”sensor points.” DeepONet uses an encoder as the pointwise projection E(u) = (u(x1), u(x2), ..., u(xm)) of the continuous function u, the so-called ”sensor values” of the input function u. An intuitive idea is to employ a neural network that simply concatenates these m sensor values and a query point y as an input to approximate the target function G(u)(y). DeepONet, in contrast, handles m sensor values and a query point y separately into two subnetworks based on the universal approximation theorem for the operator (Lu et al., 2021). See Appendix B for more details. Lu et al. (2021) use the fully connected neural network for the approximator A : Rm → Rp. They referred to the composition of these two maps as branch net\n\nβ : U → Rp, β(u) := A ◦ E(u) (2) for any u ∈ U. The role of branch net can be interpreted as learning the coefficient of the target function G(u)(y). They use one additional neural network, called trunk net τ as shown below.\n\nτ : Y → Rp+1, τ (y) := {τk(y)}p\n\nk=0\n\n(3)\n\nfor any y ∈ Y. The role of trunk net can be interpreted as learning an affine space V that can efficiently approximate output function space C(Y; Rds ). The functions τ1(y), ...,τp(y) become the p-basis of vector space associated with V and τ0(y) becomes a point of V . By using the trunk net τ , the τ -induced reconstructor R is defined as\n\nRτ : Rp → C(Y; Rds), Rτ (β)(y) := τ0(y) +\n\np (cid:88)\n\nk=1\n\nβkτk(y)\n\n(4)\n\nwhere β = (β1, β2, ..., βp) ∈ Rp. In DeepONet, τ0(y) is restricted to be a constant τ0 ∈ R that is contained in a reconstructor R. The architecture of DeepONet is described in Figure 4 (b).\n\nHere, the τ -induced reconstructor Rτ is the linear approximation of the output function space. Because the linear approximation R cannot consider the elements in its orthogonal complement, a priori limitation on the best error of DeepONet is explained in Lanthaler et al. (2022) as\n\n(cid:18)(cid:90)\n\n(cid:90)\n\nU\n\nY\n\n|G(u)(y) − Rτ ◦ A ◦ E(u)(y)|2dydμ(u)\n\n(cid:19) 1\n\n2\n\n(cid:115)(cid:88)\n\n≥\n\nλk,\n\nk>p\n\n(5)\n\nwhere λ1 ≥ λ2 ≥ ... are the eigenvalues of the covariance operator ΓG#μ of the push-forward measure G#μ. Several studies point out that the slow decay rate of the lower bound leads to inaccurate approximation operator learning using DeepONet (Kovachki et al., 2021b; Hadorn, 2022; Lanthaler et al., 2022). For example, the solution operator of the advection PDEs (Seidman et al., 2022; Venturi & Casey, 2023) and of the Burgers’ equation (Hadorn, 2022) are difficult to approximate when we are using the DeepONet with the small number of basis p.\n\n3.3 VARIANT MODELS OF DEEPONET\n\nSeveral variants of DeepONet have been developed to overcome its limitation. All these models can be viewed from the perspective of parametrizing the target function as a neural network. When we think of the target network that receives y as an input and generates an output Gθ(u)(y), the DeepONet and its variant model can be distinguished by how information from the input function u is injected into this target network Gθ(u), as described in Figure 3. From this perspective, the trunk net in the DeepONet can be considered as the target network except for the final output, as shown in Figure 4 (a). The output of the branch net gives the weight between the last hidden layer and the final output.\n\nFigure 3: The perspective target network parametrization for operator learning.\n\nHadorn (2022) proposed Shift-DeepONet. The main idea is that a scale net and a shift net are used to shift and scale the input query position y. Therefore, it can be considered that the information of\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: DeepONet and its variant models for operator learning.\n\ninput function u generates the weights and bias between the input layer and the first hidden layer, as explained in Figure 4 (b).\n\nVenturi & Casey (2023) proposed FlexDeepONet, explained in Figure 4 (c). They used the additional network, pre-net, to give the bias between the input layer and the first hidden layer. Additionally, the output of the branch net also admits the additional output τ0 to provide more information on input function u at the last inner product layer.\n\nNOMAD is recently developed by Seidman et al. (2022) to overcome the limitation of DeepONet. They devise a nonlinear output manifold using a neural network that takes the output of branch net {βi}p i=1 and the query location y. As explained in Figure 4 (d), the target network receives information about the function u as an additional input, similar to other conventional neural embedding methods (Park et al., 2019; Chen & Zhang, 2019; Mescheder et al., 2019).\n\nThese methods provide information on the input function u to only a part of the target network. It is a natural idea to use a hypernetwork to share the information of input function u to all parameters of the target network. We propose a general model HyperDeepONet (Figure 5), which contains the vanilla DeepONet, FlexDeepONet, and Shift-DeepONet, as a special case of the HyperDeepONet.\n\n4 PROPOSED MODEL: HYPERDEEPONET\n\n4.1 ARCHITECTURE OF HYPERDEEPONET\n\nThe HyperDeepONet structure is described in Figure 5. The encoder E and the approximator A are used, similar to the vanilla DeepONet. The proposed structure replaces the branch net with the hypernetwork. The hypernetwork generate all parameters of the target network. More precisely, we define the hypernetwork h as\n\nhθ : U → Rp, hθ(u) := A ◦ E(u) for any u ∈ U. Then, h(u) = Θ ∈ Rp is a network parameter of the target network, which is used in reconstructor for the HyperDeepONet. We define the reconstructor R as\n\n(6)\n\nR : Rp → C(Y; Rds), R(Θ)(y) := NN(y; Θ)\n\n(7)\n\nFigure 5: The proposed HyperDeepONet structure\n\nwhere Θ = [W, b] ∈ Rp, and NN denotes the target network. Two fully connected neural networks are employed for the hypernetwork and target network.\n\nTherefore, the main idea is to use the hypernetwork, which takes an input function u and produces the weights of the target network. It can be thought of as a weight generator for the target network. The hypernetwork determines the all parameters of the target network containing the weights between the final hidden layer and the output layer. It implies that the structure of HyperDeepONet contains the entire structure of DeepONet. As shown in Figure 4 (b) and (c), Shift-DeepONet and FlexDeepONet can also be viewed as special cases of the HyperDeepONet, where the output of the hypernetwork determines the weights or biases of some layers of the target network. The outputs of\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nthe hypernetwork determine the biases for the first hidden layer in the target network for NOMAD in Figure 4 (d).\n\n4.2 COMPARISON ON COMPLEXITY OF DEEPONET AND HYPERDEEPONET\n\nIn this section, we would like to clarify the complexity of the DeepONet required for the approximation A and reconstruction R based on the theory in Galanti & Wolf (2020). Furthermore, we will show that the HyperDeepONet entails a relatively lower complexity than the DeepONet using the results on the upper bound for the complexity of hypernetwork (Galanti & Wolf, 2020).\n\n4.2.1 NOTATIONS AND DEFINITIONS\n\nSuppose that the pointwise projection values (sensor values) of the input function u is given as E(u) = (u(x1), u(x2), ..., u(xm)). For simplicity, we consider the case Y = [−1, 1]dy and E(u) ∈ [−1, 1]m. For the composition R ◦ A : Rm → C(Y; R), we focus on approximating the mapping O : Rm+dy → R, which is defined as follows:\n\nO(E(u), y) := (R ◦ A(E(u)))(y),\n\nfor y ∈ [−1, 1]dy , E(u) ∈ [−1, 1]m.\n\n(8)\n\nThe supremum norm ∥h∥∞ is defined as maxy∈Y ∥h(y)∥. Now, we introduce the Sobolev space Wr,n, which is a subset of C r([−1, 1]n; R). For r, n ∈ N,\n\n(cid:26)\n\nWr,n :=\n\nh : [−1, 1]n → R\n\n(cid:12) (cid:12)∥h∥s (cid:12)\n\nr := ∥h∥∞ +\n\n(cid:88)\n\n1≤|k|≤r\n\n(cid:27)\n\n∥Dkh∥∞ ≤ 1\n\n,\n\nwhere Dkh denotes the partial derivative of h with respect to multi-index k ∈ {N ∪ {0}}dy . We assume that the mapping O lies in the Sobolev space Wr,m+dy .\n\nFor the nonlinear activation σ, the class of neural network F represents the fully connected neural network with depth k and corresponding width (h1 = n, h2, · · · , hk+1), where W i ∈ Rhi × Rhi+1 and bi ∈ Rhi+1 denote the weights and bias of the i-th layer respectively.\n\nF := (cid:8)f : [−1, 1]n → R|f (y; [W, b]) = W k · σ(W k−1 · · · σ(W 1 · y + b1) + bk−1) + bk(cid:9)\n\nSome activation functions facilitate an approximation for the Sobolev space and curtail the complexity. We will refer to these functions as universal activation functions. The formal definition can be found below, where the distance between the class of neural network F and the Sobolev space Wr,n is defined by d(F; Wr,n) := supψ∈Wr,n inf f ∈F ∥f − ψ∥∞. Most well-known activation functions are universal activations that are infinitely differentiable and non-polynomial in any interval (Mhaskar, 1996). Furthermore, Hanin & Sellke (2017) state that the ReLU activation is also universal. Definition 1. (Galanti & Wolf, 2020) (Universal activation). The activation function σ is called universal if there exists a class of neural network F with activation function σ such that the number of parameters of F is O(ε−n/r) with d(F; Wr,n) ≤ ε for all r, n ∈ N.\n\nWe now introduce the theorem, which offers a guideline on the neural network architecture for operator learning. It suggests that if the entire architecture can be replaced with a fully connected neural network, large complexity should be required for approximating the target function. It also verifies that the lower bound for a universal activation function is a sharp bound on the number of parameters. First, we give an assumption to obtain the theorem. Assumption 1. Suppose that F and Wr,n represent the class of neural network and the target function space to approximate, respectively. Let F ′ be a neural network class representing a structure in which one neuron is added rather than F. Then, the followings holds for all ψ ∈ Wr,n not contained in F.\n\ninf f ∈F\n\n∥f − ψ∥∞ > inf f ∈F ′\n\n∥f − ψ∥∞.\n\nFor r = 0, Galanti & Wolf (2020) remark that the assumption is valid for 2-layered neural networks with respect to the L2 norm when an activation function σ is either a hyperbolic tangent or sigmoid function. With Assumption 1, the following theorem holds, which is a fundamental approach to identifying the complexity of DeepONet and its variants. Note that a real-valued function g ∈ L1(R) is called a bounded variation if its total variation supφ∈C1\n\n(cid:82) R g(x)φ′(x)dx is finite.\n\nc (R),∥φ∥∞≤1\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 1. (Galanti & Wolf, 2020). Suppose that F is a class of neural networks with a piecewise C 1(R) activation function σ : R → R of which derivative σ′ is bounded variation. If any nonconstant ψ ∈ Wr,n does not belong to F, then d(F; Wr,n) ≤ ε implies the number of parameters in F should be Ω(ε−n/r).\n\n4.2.2 LOWER BOUND FOR THE COMPLEXITY OF THE DEEPONET\n\nNow, we provide the minimum number of parameters in DeepONet. The following theorem presents It states that the number of a criterion on the DeepONet’s complexity to get the desired error. required parameters increases when the target functions are irregular, corresponding to a small r. FDeepONet(B, T ) denotes the class of function in DeepONet, induced by the class of branch net B and the class of trunk net T . Theorem 2. (Complexity of DeepONet) Let σ : R → R be a universal activation function in C r(R) such that σ and σ′ are bounded. Suppose that the class of branch net B has a bounded Sobolev norm (i.e., ∥β∥s r ≤ l1, ∀β ∈ B). Suppose any non-constant ψ ∈ Wr,n does not belong to any class of neural network. In that case, the number of parameters in the class of trunk net T is Ω(ε−dy/r) when d(FDeepONet(B, T ); Wr,dy+m) ≤ ε.\n\nThe core of this proof is showing that the inner product between the branch net and the trunk net could be replaced with a neural network that has a low complexity (Lemma 1). Therefore, the entire structure of DeepONet could be replaced with a neural network that receives [E(u), y] ∈ Rdy+m as input. It gives the lower bound for the number of parameters in DeepONet based on Theorem 1. The proof can be found in Appendix C.1.\n\nThe analogous results holds for variant models of DeepONet. Models such as Shift-DeepONet and flexDeepONet could achieve the desired accuracy with a small number of basis. Still, there was a trade-off in which the first hidden layer of the target network required numerous units. There was no restriction on the dimension of the last hidden layer in the target network for NOMAD, which uses a fully nonlinear reconstruction. However, the first hidden layer of the target network also had to be wide enough, increasing the number of parameters. Details can be found in Appendix C.2.\n\nFor the proposed HyperDeepONet, the sensor values E(u) determine the weight and bias of all other layers as well as the weight of the last layer of the target network. Due to the nonlinear activation functions between linear matrix multiplication, it is difficult to replace HyperDeepONet with a single neural network that receives [E(u), y] ∈ Rdy+m as input. Galanti & Wolf (2020) state that there exists a hypernetwork structure (HyperDeepONet) such that the number of parameters in the target network is O(ε−dy/r). It implies that the HyperDeepONet reduces the complexity compared to all the variants of DeepONet.\n\n5 EXPERIMENTS\n\nIn this section, we verify the effectiveness of the proposed model HyperDeepONet to learn the operators with a complicated target function space. To be more specific, we focus on operator learning problems in which the space of output function space is complicated. Each input function ui generates multiple triplet data points (ui, y, G(u)(y)) for different values of y. Except for the shallow water problem, which uses 100 training function pairs and 20 test pairs, we use 1,000 training input-output function pairs and 200 test pairs for all experiments.\n\nFor the toy example, we first consider the identity operator G : ui (cid:55)→ ui. The Chebyshev polynomial is used as the input (=output) for the identity operator problem. The Chebyshev polynomials of the first kind Tl of degree 20 can be written as ui ∈ {(cid:80)19 l=0 clTl(x)|cl ∈ [−1/4, 1/4]} with random sampling cl from uniform distribution U [−1/4, 1/4]. The differentiation operator G : ui (cid:55)→ d dx ui is considered for the second problem. Previous works handled the anti-derivative operator, which makes the output function smoother by averaging (Lu et al., 2019; 2022). Here, we choose the differentiation operator instead of the anti-derivative operator to focus on operator learning when the operator’s output function space is complicated. We first sample the output function G(u) from the above Chebyshev polynomial of degree 20. The input function is generated using the numerical method that integrates the output function.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nModel Identity Differentiation\n\nDeepONet 0.578±0.003 0.559±0.001\n\nShift 0.777±0.018 0.624±0.015\n\nFlex 0.678±0.062 0.562±0.016\n\nNOMAD 0.578±0.020 0.558±0.003\n\nHyper(ours) 0.036±0.005 0.127±0.043\n\nTable 1: The mean relative L2 test error with standard deviation for the identity operator and the differentiation operator. The DeepONet, its variants, and the HyperDeepONet use the target network dy-20-20-10-1 with tanh activation function. Five training trials are performed independently.\n\nFigure 6: One test data example of differentiation operator problem.\n\nFinally, the solution operators of PDEs are considered. We deal with two problems with the complex target function in previous works (Lu et al., 2022; Hadorn, 2022). The solution operator of the advection equation is considered a mapping from the rectangle shape initial input function to the solution w(t, x) at t = 0.5, i.e., G : w(0, x) (cid:55)→ w(0.5, x). We also consider the solution operator of Burgers’ equation which maps the random initial condition to the solution w(t, x) at t = 1, i.e., G : w(0, x) (cid:55)→ w(1, x). The solution of the Burgers’ equation has a discontinuity in a short time, although the initial input function is smooth. For a challenging benchmark, we consider the solution operator of the shallow water equation that aims to predict the fluid height h(t, x1, x2) from the initial condition h(0, x1, x2), i.e., G : h(0, x1, x2) (cid:55)→ h(t, x1, x2) (Figure 1). In this case, the input of the target network is three dimension (t, x1, x2), which makes the solution operator complex. Detail explanation is provided in Appendix E.\n\nExpressivity of target network. We have compared the expressivity of the small target network using different models. We focus on the identity and differentiation operators in this experiment. All models employ the small target network dy-20-20-10-1 with the hyperbolic tangent activation function. The branch net and the additional networks (scale net, shift net, pre-net, and hypernetwork) also use the same network size as the target network for all five models.\n\nTable 1 shows that the DeepONet and its variant models have high errors in learning complex operators when the small target network is used. In contrast, the HyperDeepONet has lower errors than the other models. This is consistent with the theorem in the previous section that HyperDeepONet can achieve improved approximations than the DeepONet when the complexity of the target network is the same. Figure 6 shows a prediction on the differentiation operator, which has a highly complex target function. The same trends are observed when the activation function or the number of sensor points changes (Table 5) and the number of layers in the branch net and the hypernetwork vary (Figure 11).\n\nSame number of learnable parameters. The previous experiments compare the models using the same target network structure. In this section, the comparison between the DeepONet and the HyperDeepONet is considered when using the same number of learnable parameters. We focus on the solution operators of the PDEs.\n\nAdvection\n\nBurgers\n\nShallow\n\nShallow w/ small param\n\nBranch net (Hypernetwork) m-256-256 m-70-70-70-70-70-Nθ\n\nDeepONet Hyper(ours) c-Hyper(ours) m-128-128-128-128-128-1024 DeepONet Hyper(ours) c-Hyper(ours) DeepONet Hyper(ours) DeepONet Hyper(ours)\n\nm-128-128-128-128 m-66-66-66-66-66-Nθ m-66-66-66-66-66-512 m-100-100-100-100 m-30-30-30-30-Nθ m-20-20-10 m -10-10-10-Nθ\n\nTarget dy-256-256-256-256-1 dy-33-33-33-33-1 dy-256-256-256-256-1 dy-128-128-128-128-1 dy-20-20-20-20-1 dy-128-128-128-128-1 dy-100-100-100-100-1 dy-30-30-30-30-1 dy-20-20-10-1 dy-10-10-10-1\n\n#Param 274K 268K 208K 115K 114K 115K 107K 101K 6.5K 5.7K\n\nRel error 0.0046±0.0017 0.0048±0.0009 0.0043±0.0004 0.0391±0.0040 0.0196±0.0044 0.0066±0.0009 0.0279 ± 0.0042 0.0148 ± 0.0002 0.0391 ± 0.0066 0.0209 ± 0.0013\n\nTable 2: The mean relative L2 test error with standard deviation for solution operator learning problems. Nθ and #Param denote the number of parameters in the target network and the number of learnable parameters, respectively. Five training trials are performed independently.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: One test data example of prediction on the advection equation (First row) and Burgers’ equation (Second row) using the DeepONet and the HyperDeepONet.\n\nFor the three solution operator learning problems, we use the same hyperparameters proposed in Lu et al. (2022) and Seidman et al. (2022) for DeepONet. First, we use the smaller target network with the larger hypernetwork for the HyperDeepONet to compare the DeepONet. Note that the vanilla DeepONet is used without the output normalization or the boundary condition enforcing techniques explained in Lu et al. (2022) to focus on the primary limitation of the DeepONet. More Details are in Appendix E. Table 2 shows that the HyperDeepONet achieves a similar or better performance than the DeepONet when the two models use the same number of learnable parameters. The HyperDeepONet has a slightly higher error for advection equation problem, but this error is close to perfect operator prediction. It shows that the complexity of target network and the number of learnable parameters can be reduced to obtain the desired accuracy using the HyperDeepONet. The fourth row of Table 2 shows that HyperDeepONet is much more effective than DeepONet in approximating the solution operator of the shallow water equation when the number of parameters is limited. Figure 7 and Figure 12 show that the HyperDeepONet learns the complex target functions in fewer epochs for the desired accuracy than the DeepONet although the HyperDeepONet requires more time to train for one epoch (Table 8).\n\nScalability. When the size of the target network for the HyperDeepONet is large, the output of the hypernetwork would be high-dimensional (Ha et al., 2017; Pawlowski et al., 2017) so that its complexity increases. In this case, the chunked HyperDeepONet (c-HyperDeepONet) can be used with a trade-off between accuracy and memory based on the chunk embedding method developed by von Oswald et al. (2020). It generates the subset of target network parameters multiple times iteratively reusing the smaller chunked hypernetwork. The c-HyperDeepONet shows a better accuracy than the DeepONet and the HyperDeepONet using an almost similar number of parameters, as shown in Table 2. However, it takes almost 2x training time and 2∼30x memory usage than the HyperDeepOnet. More details on the chunked hypernetwork are in Appendix D.\n\n6 CONCLUSION AND DISCUSSION\n\nIn this work, the HyperDeepONet is developed to overcome the expressivity limitation of DeepONet. The method of incorporating an additional network and a nonlinear reconstructor could not thoroughly solve this limitation. The hypernetwork, which involves multiple weights simultaneously, had a desired complexity-reducing structure based on theory and experiments.\n\nWe only focused on when the hypernetwork and the target network is fully connected neural networks. In the future, the structure of the two networks can be replaced with a CNN or ResNet, as the structure of the branch net and trunk net of DeepONet can be changed to another network (Lu et al., 2022). Additionally, it seems interesting to research a simplified modulation network proposed by Mehta et al. (2021), which still has the same expressivity as HyperDeepONet.\n\nSome techniques from implicit neural representation can improve the expressivity of the target network (Sitzmann et al., 2020). Using a sine function as an activation function with preprocessing will promote the expressivity of the target network. We also leave the research on the class of activation functions satisfying the assumption except for hyperbolic tangent or sigmoid functions as a future work.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nJ. Y. Lee was supported by a KIAS Individual Grant (AP086901) via the Center for AI and Natural Sciences at Korea Institute for Advanced Study and by the Center for Advanced Computation at Korea Institute for Advanced Study. H. J. Hwang and S. W. Cho were supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (RS-202200165268) and by Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government(MSIP) (No.2019-0-01906, Artificial Intelligence Graduate School Program (POSTECH)).\n\nREFERENCES\n\nAnima Anandkumar, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Nikola Kovachki, Zongyi Li, Burigede Liu, and Andrew Stuart. Neural operator: Graph kernel network for partial differential equations. In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations, 2019. URL https://openreview.net/forum?id=fg2ZFmXFO3.\n\nSaakaar Bhatnagar, Yaser Afshar, Shaowu Pan, Karthik Duraisamy, and Shailendra Kaushik. Prediction of aerodynamic flow fields using convolutional neural networks. Comput. Mech., 64 ISSN 0178-7675. doi: 10.1007/s00466-019-01740-0. URL https: (2):525–545, 2019. //doi.org/10.1007/s00466-019-01740-0.\n\nShengze Cai, Zhicheng Wang, Lu Lu, Tamer A Zaki, and George Em Karniadakis. Deepm&mnet: Inferring the electroconvection multiphysics fields based on operator approximation by neural networks. Journal of Computational Physics, 436:110296, 2021.\n\nTianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. IEEE Transactions on Neural Networks, 6(4):911–917, 1995.\n\nZhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5939–5948, 2019.\n\nTejalal Choudhary, Vipul Mishra, Anurag Goswami, and Jagannathan Sarangapani. A comprehensive survey on model compression and acceleration. Artificial Intelligence Review, 53(7): 5113–5155, 2020.\n\nFilipe de Avila Belbute-Peres, Yi-fan Chen, and Fei Sha. Hyperpinn: Learning parameterized differential equations with physics-informed hypernetworks. In The Symbiosis of Deep Learning and Differential Equations, 2021.\n\nWeinan E and Bing Yu. The deep Ritz method: a deep learning-based numerical algorithm for solving variational problems. Commun. Math. Stat., 6(1):1–12, 2018. ISSN 2194-6701. doi: 10. 1007/s40304-018-0127-z. URL https://doi.org/10.1007/s40304-018-0127-z.\n\nTomer Galanti and Lior Wolf. On the modularity of hypernetworks. Advances in Neural Information\n\nProcessing Systems, 33:10409–10419, 2020.\n\nXiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady flow approximation. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 481–490, 2016.\n\nDavid Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=rkpACe1lx.\n\nPatrik Simon Hadorn. Shift-deeponet: Extending deep operator networks for discontinuous output\n\nfunctions. ETH Zurich, Seminar for Applied Mathematics, 2022.\n\nBoris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width.\n\narXiv preprint arXiv:1710.11278, 2017.\n\nRakhoon Hwang, Jae Yong Lee, Jin Young Shin, and Hyung Ju Hwang. Solving pde-constrained\n\ncontrol problems using operator learning. arXiv preprint arXiv:2111.04941, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nYuehaw Khoo, Jianfeng Lu, and Lexing Ying. Solving parametric PDE problems with artificial neural networks. European J. Appl. Math., 32(3):421–435, 2021. ISSN 0956-7925. doi: 10.1017/ S0956792520000182. URL https://doi.org/10.1017/S0956792520000182.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n\nIn Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.\n\nSylwester Klocek, Łukasz Maziarka, Maciej Wołczyk, Jacek Tabor, Jakub Nowak, and Marek In International Conference on Arti-\n\n ́Smieja. Hypernetwork functional image representation. ficial Neural Networks, pp. 496–510. Springer, 2019.\n\nNikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation and error bounds for fourier neural operators. Journal of Machine Learning Research, 22:Art–No, 2021a.\n\nNikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. arXiv preprint arXiv:2108.08481, 2021b.\n\nThorsten Kurth, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, and Animashree Anandkumar. Fourcastnet: Accelerating global high-resolution weather forecasting using adaptive fourier neural operators. arXiv preprint arXiv:2208.05419, 2022.\n\nSamuel Lanthaler, Siddhartha Mishra, and George E Karniadakis. Error estimates for deeponets: A deep learning framework in infinite dimensions. Transactions of Mathematics and Its Applications, 6(1):tnac001, 2022.\n\nZongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial In International Conference on Learning Representations, 2021. URL differential equations. https://openreview.net/forum?id=c8P9NQVtmnO.\n\nChensen Lin, Zhen Li, Lu Lu, Shengze Cai, Martin Maxey, and George Em Karniadakis. Operator learning for predicting multiscale bubble growth dynamics. The Journal of Chemical Physics, 154(10):104118, 2021.\n\nLu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. arXiv preprint arXiv:1910.03193, 2019.\n\nLu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nature Machine Intelligence, 3(3):218–229, 2021.\n\nLu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data. Computer Methods in Applied Mechanics and Engineering, 393:114778, 2022.\n\nZhiping Mao, Lu Lu, Olaf Marxen, Tamer A. Zaki, and George Em Karniadakis. DeepM&Mnet for hypersonics: predicting the coupled flow and finite-rate chemistry behind a normal shock using neural-network approximation of operators. J. Comput. Phys., 447:Paper No. 110698, 24, 2021. ISSN 0021-9991. doi: 10.1016/j.jcp.2021.110698. URL https://doi.org/10.1016/j. jcp.2021.110698.\n\nIshit Mehta, Micha ̈el Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and Manmohan Chandraker. Modulated periodic activations for generalizable local functional representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14214–14223, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4460–4470, 2019.\n\nHrushikesh N Mhaskar. Neural networks for optimal approximation of smooth and analytic func-\n\ntions. Neural computation, 8(1):164–177, 1996.\n\nMG Sarwar Murshed, Christopher Murphy, Daqing Hou, Nazar Khan, Ganesh Ananthanarayanan, and Faraz Hussain. Machine learning at the network edge: A survey. ACM Computing Surveys (CSUR), 54(8):1–37, 2021.\n\nShaowu Pan, Steven L Brunton, and J Nathan Kutz. Neural implicit flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data. arXiv preprint arXiv:2204.03216, 2022.\n\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 165–174, 2019.\n\nNick Pawlowski, Andrew Brock, Matthew CH Lee, Martin Rajchl, and Ben Glocker. Implicit weight\n\nuncertainty in neural networks. arXiv preprint arXiv:1711.01297, 2017.\n\nMichael Prasthofer, Tim De Ryck, and Siddhartha Mishra. Variable-input deep operator networks.\n\narXiv preprint arXiv:2205.11404, 2022.\n\nM. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys., 378:686–707, 2019. ISSN 0021-9991. doi: 10.1016/j.jcp.2018.10.045. URL https://doi.org/10.1016/j.jcp.2018.10.045.\n\nTim De Ryck and Siddhartha Mishra. Generic bounds on the approximation error for physicsinformed (and) operator learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=bF4eYy3LTR9.\n\nJacob H Seidman, Georgios Kissas, Paris Perdikaris, and George J Pappas. Nomad: Nonlinear\n\nmanifold decoders for operator learning. arXiv preprint arXiv:2206.03551, 2022.\n\nJustin Sirignano and Konstantinos Spiliopoulos. DGM: a deep learning algorithm for solving partial differential equations. J. Comput. Phys., 375:1339–1364, 2018. ISSN 0021-9991. doi: 10.1016/ j.jcp.2018.08.029. URL https://doi.org/10.1016/j.jcp.2018.08.029.\n\nVincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in Neural Information Processing Systems, 33:7462–7473, 2020.\n\nMakoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk Pfl ̈uger, and Mathias Niepert. PDEBench: An extensive benchmark for scientific machine learning. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=dh_MkX0QfrK.\n\nSimone Venturi and Tiernan Casey. Svd perspectives for augmenting deeponet flexibility and inter-\n\npretability. Computer Methods in Applied Mechanics and Engineering, 403:115718, 2023.\n\nJohannes von Oswald, Christian Henning, Benjamin F. Grewe, and Jo ̃ao Sacramento. Continual learning with hypernetworks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJgwNerKvB.\n\nSifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential equations with physics-informed deeponets. Science advances, 7(40):eabi8605, 2021.\n\nSifan Wang, Hanwen Wang, and Paris Perdikaris. Improved architectures and training algorithms\n\nfor deep operator networks. Journal of Scientific Computing, 92(2):1–42, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nYinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physicsconstrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. J. Comput. Phys., 394:56–81, 2019. ISSN 0021-9991. doi: 10.1016/j.jcp. 2019.05.024. URL https://doi.org/10.1016/j.jcp.2019.05.024.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA NOTATIONS\n\nWe list the main notations in Table 3 which is not concretely described in this paper.\n\nU Y\ndu dy dx ds {x1, · · · , xm} m\nRd C r(R)\n\nC(Y; Rds ) C r([−1, 1]n; R)\n\nn = O(ε)\n\nn = Ω(ε)\n\nn = o(ε)\n\nDomain of input function Domain of output function Dimension of U Dimension of Y Dimension of the codomain of input function Dimension of the codomain of output function Sensor points Number of sensor points Euclidean space of dimension d Set of functions that has continuous r-th derivative. Set of continuous function from Y to Rds Set of functions from [−1, 1]n to R whose r-th partial derivatives are continuous. There exists a constant C such that n ≤ C · ε, ∀ε > 0 There exists a constant C such that n ≥ C · ε, ∀ε > 0 n/ε converges to 0 as ε approaches to 0.\n\nTable 3: Notation\n\nB ORIGINAL STRUCTURE OF DEEPONET\n\nFigure 8: The structure of DeepONet\n\nFor a clear understanding of previous works, we briefly leave a description of DeepONet. In particular, we explain the structure of the unstacked DeepONet in Lu et al. (2019) which is being widely used in various experiments of the papers. Note that Figure 4(a) represents the corresponding model which is called simply DeepONet throughout this paper. The overall architecture of the model is formulated as\n\nRτ ◦ Aβ(u(x1), · · · , u(xm))(y) := ⟨β(u(x1), · · · , u(xm); θβ), τ (y; θτ )⟩, where τ and β are referred to as the trunk net and the branch net respectively. Note that Rτ and Aβ denote the reconstructor and the operator in Section 3.1. For m fixed observation points (x1, · · · , xm) ∈ X m, the unstacked DeepONet consists of an inner product of branch Net and trunk Net, which are fully connected neural networks. For a function u, the branch net receives pointwise projection values (u(x1), · · · , u(xm)) as inputs to detect which function needs to be transformed. The trunk net queries a location y ∈ Y of interest where Y denotes the domain of output functions.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nIt was revealed that the stacked DeepONet, the simplified version of the unstacked DeepONet, is a universal approximator in the set of continuous functions. Therefore, the general structure also becomes a universal approximator which enables close approximation by using a sufficient number of parameters. Motivated by the property, we focus on how large complexity should be required for DeepONet and its variants to achieve the desired error.\n\nC ON COMPLEXITY OF DEEPONET AND ITS VARIANTS\n\nC.1 PROOF OF THEOREM 2 ON DEEPONET COMPLEXITY\n\nThe following lemma implies that the class of neural networks is sufficiently efficient to approximate the inner product. Lemma 1. For the number of basis p ∈ N, consider the inner product function πp : [−1, 1]2p → R defined by\n\nπp(a1, · · · , ap, b1, · · · , bp) :=\n\np (cid:88)\n\ni=1\n\naibi = ⟨(a1, · · · , ap), (b1, · · · , bp)⟩.\n\nFor an arbitrary positive t, there exists a class of neural network F with universal activation σ : R → R such that the number of parameters of F is O(p1+1/tε−1/t) with inf f ∈F ∥f − πp∥∞ ≤ ε.\n\nProof. Suppose that t is a positive integer and the Sobolev space W2t,2 is well defined. First, we would like to approximate the product function π1 : [−1, 1]2 → R which is defined as\n\nπ1(a, b) = ab.\n\nNote that partial derivatives Dkπ1 = 0 for all multi-index k ∈ {N (cid:83) {0}}2 such that |k| ≥ 2. For a multi-index k with |k| = 1, Dkπ1 contains only one term which is either a or b. In this case, we can simply observe that (cid:80) |k|=1 ∥Dkπ1∥∞ ≤ 2 · 1 = 2 by the construction of the domain [−1, 1]2 for π1. And finally,\n\n∥π1∥∞ ≤ ∥ab∥∞ ≤ ∥a∥∞∥b∥∞ ≤ 1 · 1 = 1,\n\nso that a function π1/3 should be contained in Wr,2 for any r ∈ N. In particular, π1/3 lies in W2t,2 so that there exists a neural network approximation fnn in some class of neural network F ∗ with an universal activation function σ such that the number of parameters of F ∗ is O((ε/3p)−2/2t) = O(p1/tε−1/t), and\n\n∥π1/3 − fnn∥∞ ≤ ε/3p,\n\nby Definition 1. Then the neural network 3fnn approximates the function π1 by an error ε/p which can be constructed by adjusting the last weight values directly involved in the output layer of neural network fnn.\n\nFinally, we construct a neural network approximation for the inner product function πp. Decompose the 2p−dimensional inner product function πp into p product functions {Proji(πp)}p i=1 which are defined as\n\nProji(πp) : R2p → R,\n\nProji(πp)(a1, · · · , ap, b1, · · · , bp) := π1(ai, bi) = aibi,\n\nfor ∀i ∈ {1, · · · , p}. Then each function Proji(πp) could be approximated within an error ε/p by neural network N Ni which has O(p1/tε−1/t) parameters by the above discussion. Finally, by adding the last weight [1, 1, · · · , 1] ∈ R1×p which has input as the outputs of p neural networks {N Ni}p i=1 Proji(πp) such that the number of parameters is O(1+p+p·p1/tε−1/t) = O(p1+1/tε−1/t). Class of neural network F, which represents the structure of N N , satisfies the desired property.\n\ni=1, we can construct the neural network approximation N N of πp = (cid:80)p\n\nObviously, the statement holds for an arbitrary real t which is not an integer.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nNow we assume that O (defined in Eq. (8)) lies in the Sobolev space Wr,dy+m. Then, we can obtain the following lemma which presents the lower bound on the number of basis p in DeepONet structure. Note that we apply L∞-norm for the outputs of branch net and trunk net which are multidimensional vectors. Lemma 2. Let σ : R → R be a universal activation function in C r(R) such that σ′ is a bounded variation. Suppose that the class of branch net B has a bounded Sobolev norm (i.e., ∥β∥s r ≤ l1, ∀β ∈ B). Assume that supremum norm ∥·∥∞ of the class of trunk net T is bounded by l2 and the number of parameters in T is o(ε−(dy+m)/r). If any non-constant ψ ∈ Wr,dy+m does not belong to any class of neural network, then the number of basis p in T is Ω(ε−dy/r) when d(FDeepONet(B, T ); Wr,dy+m) ≤ ε.\n\nProof. To prove the above lemma by contradiction, we assume the opposite of the conclusion. Suppose that there is no constant C that satisfies the inequality p ≥ C(ε−dy/r) for ∀ε > 0. In other words, there exists a sequence of DeepONet which has {pn}∞ n=1 as the number of basis with a sequence of error {εn}∞ 1\nn\n\nn=1 in R such that εn → 0, and satisfies\n\ni.e., pn = o(ε−dy/r\n\n) with respect to n\n\nε−dy/r\n\npn ≤\n\n(9)\n\n(cid:17)\n\n(cid:16)\n\nn\n\nn\n\n,\n\nand\n\nd(FDeepONet(Bn, Tn); Wr,dy+m) ≤ εn,\n\nwhere Bn and Tn denote the corresponding class sequence of branch net and trunk net respectively. Then, we can choose the sequence of branch net {βn : Rm → Rpn }∞ n=1 and trunk net (cid:8)τn : Rdy → Rpn (cid:9)∞\n\nn=1 satisfying\n\n∥O(E(u), y) − πpn (βn(E(u)), τn(y))∥∞ ≤ 2εn, ∀[E(u), y] ∈ [−1, 1]dy+m, O ∈ Wr,dy+m\n\nfor the above sequence of DeepONet by the definition of d(FDeepONet(Bn, Tn); Wr,dy+m).\n\nNow, we would like to construct neural network approximations fn for the branch net βn. By the assumption on the boundedness of B, the i-th component [βn]i of βn has a Sobolev norm bounded by l1. In other words, ∥[βn]i/l1∥s r ≤ 1 and therefore, [βn]i/l1 is contained in W1,m. Since σ is a universal activation function, we can choose a neural network approximation [fn]i of [βn]i such that the number of parameters is O((εn/l1)−m/r) = O(ε−m/r\n\n), and\n\nn\n\n∥[fn]i − [βn]i∥∞ ≤ εn/l1.\n\nThen, fn = (l1[fn]1, l1[fn]2, · · · , l1[fn]pn) becomes neural network approximation of βn which has O(pnε−m/r Recall the target function corresponding to m observation E(u) ∈ [−1, 1]m by O(E(u), ·) : Rdy → R which is defined in Eq. (8). Then, for ∀E(u), we can observe the following inequalities:\n\n) parameters within an error εn.\n\nn\n\n∥O(E(u), y) − πpn (fn(E(u)), τn(y))∥∞\n\n≤ ∥O(E(u), y) − πpn (βn (E(u)) , τn(y)) ∥∞ + ∥πpn (βn (E (u)) − fn(E(u)) , τn(y))∥∞\n\n≤ 2εn + εn∥τn(y)∥∞ ≤ εn(2 + l2),\n\nby the assumption on the boundedness of T . Now we would like to consider the sequence of neural network which is an approximation of inner product between pn−dimensional vector in [−1, 1]pn . Note the following inequality\n\n∥fn(E(u))∥∞ ≤ ∥fn(E(u)) − βn(E(u))∥∞ + ∥βn(E(u))∥∞\n\n≤ εn + ∥βn(E(u))∥s ≤ εn + l1 ≤ 2l1, It implies that fn(E(u))/2l1 and τn(x)/l2 lie in [−1, 1]pn . By with ∥τn∥∞ ≤ l2 for large n. Lemma 1, there exists a class of neural network Hn such that the number of parameters is O(p1+1/2dyr\n\nε−1/2dyr\n\n) and,\n\nr\n\nn\n\nn\n\ninf h∈Hn\n\n∥h − πpn ∥∞ ≤ εn\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nwhere πpn is the inner product corresponding to pn−dimensional vector. Choose a neural network hn : [−1, 1]2pn → R such that ∥hn − πpn ∥∞ ≤ 2εn. Then, by the triangular inequality,\n\n∥O(E(u), y) − 2l1l2hn(fn(E(u))/2l1, τn(y)/l2)∥∞\n\n≤ ∥O(E(u), y) − πpn(fn(E(u)), τn(y))∥∞ + 2l1l2∥πpn (fn(E(u))/2l1, τn(y)/l2) − hn(fn(E(u))/2l1, τn(y)/l2)∥∞\n\n≤ εn(2 + l2) + 2l1l2(2εn) = εn(2 + l2 + 4l1l2).\n\n(10)\n\nFinally, we compute the number of parameters which is required to implement the function 2l1l2hn(fn(E(u))/2l1, τn(y)/l2). The only part that needs further consideration is scalar multiplication. Since we need one weight to multiply a constant with one real value, three scalar multiplications\n\nhn(fn(E(u))/2l1, τn(y)/l2) (cid:55)→ 2l1l2hn(fn(E(u))/2l1, τn(y)/l2), fn(E(u)) (cid:55)→ fn(E(u))/2l1, and\n\nτn(x) (cid:55)→ τn(y)/l2,\n\nrequire 1, pn, pn-parameters respectively. Combining all the previous results with the size of trunk net, the total number of parameters is obtained in the form of\n\nO(1 + 2pn + p1+1/2dyr\n\nn\n\nε−1/2dyr\n\nn\n\n+ pnε−m/r\n\nn\n\n) + o(ε−(dy+m)/r\n\nn\n\n) = o(ε−(dy+m)/r\n\nn\n\n),\n\nsince the initial assumption (9) on the number of basis gives the following inequality.\n\np1+1/2dyr\n\nn\n\nε−1/2dyr\n\nn\n\n+ pnε−m/r\n\nn\n\n≤ pn(p1/2dyr\n\nn\n\nε−1/2dyr\n\nn\n\n+ ε−m/r\n\nn\n\n)\n\n≤\n\n≤\n\n1 n\n1 n\n\nε−dy/r\n\nn\n\n(ε−1/2r2−1/2dyr\n\nn\n\n+ ε−m/r\n\nn\n\n)\n\nε−dy/r\n\nn\n\n2ε−m/r\n\nn\n\n=\n\n2 n\n\nε−(dy+m)/r\n\nn\n\n.\n\nOn the one hand, the sequence of function {2l1l2hn(fn(E(u))/2l1, τn(y)/l2)}∞ n=1 is an sequence of approximation for O(E(u), y) within a corresponding sequence of error {εn(2 + l2 + 4l1l2)}∞ n=1. Denote the sequence of the class of neural networks corresponding to the sequence of the function {2l1l2hn(fn(E(u))/2l1, τn(y)/l2)}∞ n=1. By the assumption, Theorem 1 implies the number of parameters in {Fn}∞ ). Therefore, the initial assumption (9) would result in a contradiction so the desired property is valid.\n\nn=1 is Ω((εn(2 + l2 + 4l1l2))−(dy+m)/r) = Ω(ε−(dy+m)/r\n\nn=1 by {Fn}∞\n\nn\n\nNote that the assumption on the boundedness of the trunk net could be valid if we use the bounded universal activation function σ. Using the above results, we can prove our main theorem, Theorem 2.\n\nProof of Theorem 2. Denote the number of parameters in T by NT . Suppose that there is no constant C satisfies the inequality NT ≥ Cε−dy/r, ∀ε > 0. That is, there exists a sequence of DeepONet with the corresponding sequence of trunk net class {Tn}∞ n=1 such that εn → 0, and it satisfies\n\nn=1 and sequence of error {εn}∞\n\nNTn <\n\n(cid:16)\n\nε−dy/r\n\nn\n\n1 n\n\ni.e.,NTn = o(ε−dy/r\n\nn\n\n) with respect to n\n\n(cid:17)\n\n.\n\nNote that the above implies NTn = o(ε−(dy+m)/r where Bn denotes the corresponding class sequence of branch net.\n\nn\n\n). On the one hand, the following inequality holds\n\nd(FDeepONet(Bn, Tn); Wr,dy+m) ≤ εn.\n\nSince σ is bounded, Tn consists of bounded functions with respect to the supremum norm. Therefore, if we apply the Lemma 2 with respect to n, the number of basis pn should be Ω(ε−dy/r ). Since pn is also the number of output dimensions for the class of trunk net Tn, the number of parameters in Tn should be larger than pn = Ω(ε−dy/r\n\n). This leads to a contradiction.\n\nn\n\nn\n\nFinally, we present a lower bound on the total number of parameters of DeepONet, considering the size of the branch net. Keep in mind that the proof of this theorem can be applied to other variants of DeepONet.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 3. (Total Complexity of DeepONet) Let σ : R → R be a universal activation function in C r(R) such that σ and σ′ are bounded. Suppose that the class of branch net B has a bounded Sobolev norm (i.e., ∥β∥s r ≤ l1, ∀β ∈ B). If any non-constant ψ ∈ Wr,n does not belong to any class of neural network, then the number of parameters in DeepONet is Ω(ε−(dy+m)/R) for any R > r when d(FDeepONet(B, T ); Wr,dy+m) ≤ ε.\n\nProof. For a positive ε < l1, suppose that there exists a class of branch net B and trunk net T such that d(FDeepONet(B, T ); Wr,dy+m) ≤ ε. By the boundedness of σ, there exists a constant l2 which is the upper bound on the supremum norm ∥ · ∥∞ of the trunk net class T . Let us denote the number of parameters in FDeepONet by NFDeepONet. Using the Lemma 1 to replace DeepONet’s inner products with neural networks as in the inequality (10), we can construct a class of neural network F such that the number of parameters F is O(NFDeepONet + p1+1/tε−1/t) and,\n\nd(F; Wr,dy+m) ≤ (1 + l1l2)ε.\n\nSuppose that NFDeepONet = o(ε−(dy+m)/r). Then, by Theorem 1, p1+1/tε−1/t should be Ω(ε−(dy+m)/r). Since t can be arbitrary large, the number of basis p should be Ω(ε−(dy+m)/R) for any R > r.\n\nC.2 COMPLEXITY ANALYSIS ON VARIANTS OF DEEPONET.\n\nWe would like to verify that variant models of DeepONet require numerous units in the first hidden layer of the target network. Now we denote the class of Pre-net in Shift-DeepONet and flexDeepONet by P. The class of Shift-DeepONet and flexDeepONet will be written as Fshift-DeepONet(P, B, T ) and FflexDeepONet(P, B, T ) respectively. The structure of Shift-DeepONet can be summarized as follows. Denote the width of the first hidden layer of the target network by w. We define the pre-net as ρ = [ρ1, ρ2] : Rm → Rw×(dy+1) where ρ1 : Rm → Rw×dy and ρ2 : Rm → Rw, the branch net as β : Rm → Rp, and the trunk net as τ : Rw → Rp. The Shift-DeepONet fShift-DeepONet(ρ, β, τ ) is defined as\n\nfShift-DeepONet(ρ, β, τ )(E(u), y) := πp(β(E(u)), τ (Φ(ρ1(E(u)), y) + ρ2(E(u))))\n\nwhere Φ is defined in Eq. (11).\n\nWe claim that it does not improve performance for the branch net to additionally output the weights on the first layer of a target network. The following lemma shows that the procedure can be replaced by a small neural network structure. Lemma 3. Consider a function Φ : Rdy(w+1) → Rw which is defined below.\n\nΦ(x1, · · · , xdy , · · · , x(dy−1)w+1 · · · , xdyw, y1, · · · , ydy )\n\n\n\n\n\ndy (cid:88)\n\ni=1\n\nxiyi, · · · ,\n\ndy (cid:88)\n\ni=1\n\n\n\nx(dy−1)w+iyi\n\n .\n\n(11)\n\n:=\n\nFor any arbitrary positive t, there exists a class of neural network F with universal activation σ : R → R such that the number of parameters of F is O(wd1+1/t ε−1/t) with inf f ∈F ∥f − Φ∥∞ ≤ ε.\n\ny\n\nProof. Using the Lemma 1, we can construct a sequence of neural network {fi}w ε-approximation of the inner product with O(d1+1/t approximations, we get the desired neural network.\n\ni=1 which is an ε−1/t) parameters. If we combine all of the w\n\ny\n\nNow we present the lower bound on the number of parameters for Shift-DeepONet. We derive the following theorem with an additional assumption that the class of trunk net is Lipschitz continuous. The function τ : Rdy → Rp is called Lipschitz continuous if there exists a constant C such that\n\n∥τ (y1) − τ (y2)∥1 ≤ C∥y1 − y2∥1.\n\nFor the neural network f , the upper bound of the Lipschitz constant for f could be obtained as i=1∥W i∥1, where L is the Lipschitz constant of σ and the norm ∥ · ∥1 denotes the matrix Lk−1Πk\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nnorm induced by vector 1-norms. We can impose constraints on the upper bound of the weights, which consequently enforce affine transformation W i to be bounded with respect to the L1 norm. Therefore, we can guarantee the Lipschitz continuity of the entire neural network in this way.\n\nWe would like to remark on the validity of the weight assumptions in the theorem since the bounded assumptions of the weight may be a possible reason for increasing the number of parameters. However, the definition of Sobolev space forces all elements to have the supremum norm ∥ · ∥∞ less than 1. It may be somewhat inefficient to insist on large weights for approximating functions with a limited range. Theorem 4. Let σ : R → R be a universal activation function in C r(R) such that σ and σ′ are bounded. Suppose that the class of branch net B and pre-net P has a bounded Sobolev norm (i.e., ∥β∥s r ≤ l3, ∀ρ ∈ P) and any neural network in the class of trunk net T is Lipschitz continuous with constant l2. If any non-constant ψ ∈ Wr,n does not belong to any class of neural network, then the number of parameters in T is Ω(ε−dy/r) when d(Fshift-DeepONet(P, B, T ); Wr,dy+m) ≤ ε.\n\nr ≤ l1, ∀β ∈ B, and ∥ρ∥s\n\nProof. Denote the number of parameters in T by NT . Suppose that there exists a sequence of prenet {ρn}∞ n=1 with the corresponding sequence of error {εn}∞\n\nn=1 and trunk net {τn}∞\n\nn=1, branch net {βn}∞ n=1 such that εn → 0 and, NT = o(ε−dy/r\n\n),\n\nn\n\nand\n\nsup ψ∈Wr,dy +m\n\n∥fshift-DeepONet(ρn, βn, τn) − ψ∥∞ ≤ εn.\n\nof ρn of which size is O(w · ε−m/r\n\nThe proof can be divided into three parts. Firstly, we come up with a neural network approximation ρN N ) within an error εn. Next, construct a neural network n\napproximation of Φ using the Lemma 3. Finally, the inner product πpn (βn, τn) is replaced with a neural network as in (10) of Lemma 2.\n\nn\n\nSince all techniques such as triangular inequality are consistent with the previous discussion, we will briefly explain why additional Lipschitz continuity is required for the trunk network, and omit the details. Approximating the Pre-Net of Shift DeepOnet, which is not in DeepOnet, inevitably results in an error in the input of the trunk net. We are reluctant to allow this error to change the output of the trunk net significantly. In this situation, the Lipschitz continuity provides the desired result.\n\nFor dy = 1, the additional rotation is only multiplying by 1 or −1. Since the weight and bias of the first layer alone can cover the scalar multiplication, flexDeepONet has the same properties as Shift-DeepONet in the above theorem. Theorem 5. Consider the case dy = 1. Let σ : R → R be a universal activation function in C r(R) such that σ and σ′ are bounded. Suppose that the class of branch net B and pre-net P has a bounded Sobolev norm(i.e., ∥β∥s r ≤ l3, ∀ρ ∈ P), and any neural network in the class of trunk net T is Lipschitz continuous with constant l2. If any non-constant ψ ∈ Wr,n does not belong to any class of neural network, then the number of parameters in T is Ω(ε−dy/r) when d(FflexDeepONet(B, T ); Wr,dy+m) ≤ ε.\n\nr ≤ l1, ∀β ∈ B, and ∥ρ∥s\n\nProof. The main difference between flexDeepONet and Shift-DeepONet, which is not mentioned earlier, is that the branch net affects the bias of the output layer. However, adding the values of the two neurons can be implemented in a neural network by adding only one weight of value 1 for each neuron, so all of the previous discussion are valid.\n\nIn fact, NOMAD can be substituted with the embedding method handled by Galanti & Wolf (2020). Suppose that the branch net of NOMAD is continuously differentiable. Let’s also assume that the Lipschitz constant of branch net and trunk net is bounded. We would like to briefly cite the relevant theorem here. Theorem 6. (Galanti & Wolf (2020)) Suppose that σ is a universal activation in C 1(R) such that σ′ is a bounded variation on R. Additionally, suppose that there is no class of neural network that can represent any function in W1,dy+m other than a constant function. If the weight on the first layer of target network in NOMAD is bounded with respect to L1-norm, then d(N ; W1,dy+m) ≤ ε implies the number of parameters in N is Ω(ε−min(dy+m,2·my)) where N denotes the class of function contained as a target network of NOMAD.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nD CHUNKED EMBEDDING METHOD\n\nThe HyperDeepONet may suffer from the large complexity of the hypernetwork when the size of the target network increases. Although even a small target network can learn various operators with proper performance, a larger target network will be required for more accurate training. To take into account this case, we employ a chunk embedding method which is developed by von Oswald et al. (2020). The original hypernetwork was designed to generate all of the target network’s weights so that the complexity of hypernetwork could be larger than the complexity of the target network. Such a problem can be overcome by using a hypernetwork with smaller outputs.\n\nFigure 9: Chunk Embedding Method\n\nMore specifically, Figure 9 describes how the chunk embedding method reduces the number of learnable parameters. First, they partition the weights and biases of the target network. The hypernetwork then creates the weights of each parameter group by using the sensor values {u(xi)}m i=1 with a latent vector zj. All groups share the hypernetwork so that the complexity decreases by a factor of the number of groups. Since the latent vectors {zj}Nc j=1 learn the characteristics of each group during the training period, the chunked embedding method preserves the expressivity of the hypernetwork. The chunked architecture is a universal approximator for the set of continuous functions with the existence of proper partitions (Proposition 1 in von Oswald et al. (2020)). We remark that the method can also generate the additional weights and discard the unnecessary ones when the number of the target network’s parameters is not multiple of NC, which is the number of group.\n\nE EXPERIMENTAL DETAILS\n\nIdentity Differentiation Advection Burgers Shallow\n\nDeepONet\n\n0.0005 0.0005 0.0005 0.0001 0.0001\n\nShift\n\n0.0002 0.0002 0.0001 0.0005 0.0005\n\nFlex\n\n0.0005 0.0005 0.0001 0.0002 -\n\nNOMAD\n\nHyper(ours)\n\n0.0001 0.0001 0.0002 0.0001 0.0001\n\n0.0001 0.0001 0.0005 0.0001 0.0005\n\nTable 4: Setting of the decay rate for each operator problem\n\nIn most experiments, we follow the hyperparameter setting in Lu et al. (2019; 2021; 2022). We use ADAM in Kingma & Ba (2015) as an optimizer with a learning rate of 1e−3 and zero weight decay. In all experiments, an InverseTimeDecay scheduler was used, and the step size was fixed to 1. In the experiments of identity and differentiation operators, grid search was performed using the sets 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005 for decay rates. The selected values of the decay rate for each model can be found in Table 4.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: One test data example of differentiation operator problem.\n\nE.1\n\nIDENTITY\n\nAs in the text, we developed an experiment to learn the identity operator for the 20th-order Chebyshev polynomials (Figure 10). Note that the absolute value of the coefficients of all orders is less than or equal to 1/4. We discretize the domain [−1, 1] with a spatial resolution of 50. For the experiments described in the text, we construct all of the neural networks with tanh. We use 1,000 training pairs and 200 pairs to validate our experiments. The batch size during the training is determined to be 5,000, which is one-tenth the size of the entire dataset.\n\nE.2 DIFFERENTIATION\n\nIn this experiment, we set functions whose derivatives are 20th-order Chebyshev polynomials as input functions. As mentioned above, all coefficients of the Chebyshev polynomial are between −1/4 and 1/4. We use the 100 uniform grid on the domain [-1, 1]. The number of training and test samples is 1,000 and 200, respectively. We use a batch size of 10,000, which is one-tenth the size(100, 000 = 100 · 1, 000) of the entire dataset.\n\nE.3 ADVECTION EQUATION\n\nWe consider the linear advection equation on the torus T := R/Z as follows: (cid:26) ∂w\n\n∂t + c · ∂w ∂x = 0. w(x, 0) = w0(x), x ∈ T,\n\n(12)\n\nwhere c is a constant which denotes the propagation speed of w. By constructing of the domain as T, we implicitly assume the periodic boundary condition. In this paper, we consider the case when c = 1. Our goal is to learn the operator which maps w0(x)(= w(0, x)) to w(0.5, x). We use the same data as in Lu et al. (2022). We discretize the domain [0, 1] with a spatial resolution of 40. The number of training samples and test samples is 1,000 and 200, respectively. We use the full batch for training so that the batch size is 40 · 1, 000 = 40, 000.\n\nE.4 BURGERS’ EQUATION\n\nWe consider the 1D Burgers’ equation which describes the movement of the viscous fluid\n\n(cid:40) ∂w\n\n∂t = −u · ∂w w(x, 0) = w0(x),\n\n∂x + ν ∂2w ∂x2 ,\n\n(x, t) ∈ (0, 1) × (0, 1], x ∈ (0, 1),\n\n(13)\n\nwhere w0 is the initial state and ν is the viscosity. Our goal is to learn the nonlinear solution operator of the Burgers’ equation, which is a mapping from the initial state w0(x)(= w(x, 0)) to the solution w(x, 1) at t = 1. We use the same data of Burgers’ equation provided in Li et al. (2021). The initial state w0(x) is generated from the Gaussian random field N (0, 54(−∆ + 25I)−2) with the periodic boundary conditions. The split step and the fine forward Euler methods were employed to generate a solution at t = 1. We set viscosity ν and a spatial resolution to 0.1 and 27 = 128, respectively. The size of the training sample and test sample we used are 1,000 and 200, respectively.\n\nWe take the ReLU activation function and the InverseTimeDecay scheduler to experiment with the same setting as in Lu et al. (2022). For a fair comparison, all experiments on DeepONet retained\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nthe hyperparameter values used in Lu et al. (2022). We use the full batch so that the batch size is 1,280,000.\n\nE.5 SHALLOW WATER EQUATION\n\nThe shallow water equations are hyperbolic PDEs which describe the free-surface fluid flow problems. They are derived from the compressible Navier-Stokes equations. The physical conservation laws of the mass and the momentum holds in the shock of the solution. The specific form of the equation can be written as\n\n∂h\n\n∂(hu)\n\n∂t + ∂ ∂t + ∂ ∂t + ∂\n\n∂(hv)\n\n\n\n\n\n\n\n∂x (hu) + ∂\n\n∂y (hv) = 0,\n\n∂x (u2h + 1 ∂y (v2h + 1\n\n2 gh2) + ∂ 2 gh2) + ∂\n\n∂y (huv) = 0, ∂x (huv) = 0,\n\nh(0, x1, x2) = h0(x1, x2),\n\n(14)\n\nfor t ∈ [0, 1] and x1, x2 ∈ [−2.5, 2.5] where h(t, x1, x2) denotes the height of water with horizontal and vertical velocity (u, v). g denotes the gravitational acceleration. In this paper, we aim to learn the operator h0(x1, x2) (cid:55)→ {h(t, x1, x2)}t∈[1/4,1] without the information of (u, v). For the sampling of initial conditions and the corresponding solutions, we directly followed the setting of Takamoto et al. (2022). The 2D radial dam break scenario is considered so that the initialization of the water height is generated as a circular bump in the center of the domain. The initial condition is generated by\n\nh(t = 0, x1, x2) =\n\n(cid:40)\n\n2.0, 1.0,\n\nfor r < (cid:112)x2 for r ≥ (cid:112)x2\n\n1 + x2 1 + x2\n\n2\n\n2\n\n(15)\n\nwith the radius r randomly sampled from U [0.3, 0.7]. The spatial domain is determined to be a 2dimensional rectangle [−2.5, 2.5] × [−2.5, 2.5]. We use 256 = 162 grids for the spatial domain. We train the models with three snapshots at t = 0.25, 0.5, 0.75, and predict the solution h(t, x1, x2) for four snapshots t = 0.25, 0.5, 0.75, 1 on the same grid. We use 100 training samples and the batch size is determined to be 25,600.\n\nF ADDITIONAL EXPERIMENTS\n\nF.1 COMPARISON UNDER VARIOUS CONDITIONS\n\nThe experimental results under various conditions are included in Table 5 by modifying the network structure, activation function, and number of sensor points. Although the DeepONet shows good performance in certain settings, the proposed HyperDeepONet shows good performance without dependency on the various conditions.\n\nActivation: ReLU, M = 30\n\nTarget network\n\ndy-30-30-30-1 dy-50-50-1 Activation: ReLU, M = 100\n\nTarget network\n\ndy-30-30-30-1 dy-50-50-1 Activation: PReLU, M = 30\n\nTarget network\n\ndy-30-30-30-1 dy-50-50-1 Activation: PReLU, M = 100\n\nTarget network\n\ndy-30-30-30-1 dy-50-50-1\n\nDeepONet 0.16797 0.04822 DeepONet 0.02234 0.07255 DeepONet 0.11354 0.00873 DeepONet 0.01035 0.07255\n\nShift 1.30852 1.08760 Shift 1.08310 1.47373 Shift 1.09395 1.14073 Shift 1.05080 1.47373\n\nFlex 1.04292 1.11957 Flex 1.03741 1.13217 Flex 1.03502 1.06947 Flex 1.07791 1.13217\n\n0.01743 0.04645\n\n0.02059 0.05562\n\nNOMAD Hyper(Ours) 0.27209 0.21391 NOMAD Hyper(Ours) 0.19089 0.14020 NOMAD Hyper(Ours) 0.25651 0.04054 NOMAD Hyper(Ours) 0.16592 0.14020\n\n0.02844 0.04302\n\n0.01083 0.04645\n\nTable 5: The relative L2 test errors for experiments on training the identity operator under various conditions\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nFigure 11: Varying the number of layers of branch net and hypernetwork in DeepONet and HyperDeepONet for identity operator problem (left) and differentiation operator problem (right).\n\nF.2 VARYING THE NUMBER OF LAYERS IN BRANCH NET AND HYPERNETWORK\n\nFigure 11 compares the relative L2 error of the training data and test data for the DeepONet and the HyperDeepONet by varying the number of layers in the branch net and the hypernetwork while maintaining the same small target network. Note that the bottom 150 training and test data with lower errors are selected to observe trends cleary. The training and test error for the DeepONet is not reduced despite the depth of the branch net becoming larger. This is a limitation of DeepONet’s linear approximation. DeepONet approximates the operator with the dot product of the trunk net’s output that approximates the basis of the target function and the branch net’s output that approximates the target function’s coefficient. Even if a more accurate coefficient is predicted by increasing the depth of the branch net, the error does not decrease because there is a limit to approximating the operator with a linear approximation using the already fixed trunk net.\n\nThe HyperDeepONet approximates the operator with a low test error in all cases with a different number of layers. Figure 11 shows that the training error of the HyperDeepONet remains small as the depth of the hypernetwork increases, while the test error increases. The increasing gap between the training and test errors is because of overfitting. HyperDeepONet overfits the training data because the learnable parameters of the model are more than necessary to approximate the target operator.\n\nF.3 COMPARISON OF HYPERDEEPONET WITH FOURIER NEURAL OPERATOR\n\nThe Fourier Neural Operator (FNO) (Li et al., 2021) is a well-known method for operator learning. Lu et al. (2022) consider 16 different tasks to explain the relative performance of the DeepONet and the FNO. They show that each method has its advantages and limitations. In particular, DeepONet has a great advantage over FNO when the input function domain is complicated, or the position of the sensor points is not uniform. Moreover, the DeepONet and the HyperDeepONet enable the inference of the solution of time-dependent PDE even in a finer time grid than a time grid used for training, e.g.the continuous-in-time solution operator of the shallow water equation in our experiment. Since the FNO is image-to-image based operator learning model, it cannot obtain a continuous solution operator over time t and position x1, x2. In this paper, while retaining these advantages of DeepONets, we focused on overcoming the difficulties of DeepONets learning complex target functions because of linear approximation. Therefore, we mainly compared the vanilla DeepONet and its variants models to learn the complex target function without the result of the FNO.\n\nTable 6 shows the simple comparison of the HyperDeepONet with the FNO for the identity operator and differentiation operator problems. Although the FNO structure has four Fourier layers, we use only one Fourier layer with 2,4,8, and 16 modes for fair comparison using similar number of parameters. The FNO shows a better performance than the HyperDeepONet for the identity operator problem. Because the FNO has a linear transform structure with a Fourier layer, the identity\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nModel\n\nHyperDeepONet (ours)\n\nIdentity Differentiation #Param\n\n0.0358 0.1268 15741(or 16741)\n\nFourier Neural Operator Mode 2 Mode 4 Mode 8 Mode 16 0.0005 0.8256 20993\n\n0.0004 0.6084 29185\n\n0.0004 0.0118 78337\n\n0.0003 0.3437 45569\n\nTable 6: The relative L2 test errors and the number of parameters for the identity and differentiation operator problems using HyperDeepONet and FNO with different number of modes. #Param denote the number of learnable parameters.\n\noperator is easily approximated even with the 2 modes. In contrast, the differentiation operator is hard to approximate using the FNO with 2, 4, and 8 modes. Although the FNO with mode 16 can approximate the differentiation operator with better performance than the HyperDeepONet, it requires approximately 4.7 times as many parameters as the HyperDeepONet.\n\nModel\n\nAdvection\n\nBurgers\n\nShallow\n\nShallow w/ small param\n\n#Param Rel error #Param Rel error #Param Rel error #Param Rel error\n\nDeepONet 274K 0.0046 115K 0.0391 107K 0.0279 6.5K 0.0391\n\nShift 281K 0.0095 122K 0.1570 111K 0.0299 8.5K 0.0380\n\nFlex 282K 0.0391 122K 0.1277 -\n- -\n-\n\nNOMAD Hyper(Ours)\n\n270K 0.0083 117K 0.0160 117K 0.0167 6.4K 0.0216\n\n268K 0.0048 114K 0.0196 101K 0.0148 5.6K 0.0209\n\nTable 7: The relative L2 test errors and the number of parameters for the solution operators of PDEs experiments. #Param denote the number of learnable parameters. Note that the all five models use the similar number of parameters for each problem.\n\nF.4 PERFORMANCE OF OTHER BASELINES WITH THE SAME NUMBER OF LEARNABLE\n\nPARAMETERS\n\nFor three different PDEs with complicated target functions, we compare all the baseline methods in Table 7 to evaluate the performances. We analyze the model’s computation efficiency based on the number of parameters and fix the model’s complexity for each equation. All five models demonstrated their prediction abilities for the advection equation. DeepONet shows the greatest performance in this case, and other variants can no longer improve the performance. For the Burgers’ equation, NOMAD and HyperDeepONet are the two outstanding algorithms from the perspective of relative test error. NOMAD seems slightly dominant to our architectures, but the two models compete within the margin of error. Furthermore, HyperDeepONet improves its accuracy using the chunk embedding method, which enlarge the target network’s size while maintaining the complexity. Finally, HyperDeepONet and NOMAD outperform the other models for 2-dimensional shallow water equations. The HyperDeepONet still succeeds in accurate prediction even with a few parameters. It can be observed from Table 7 that NOMAD is slightly more sensitive to an extreme case using a low-complexity model. Because of the limitation in computing 3-dimensional rotation, FlexDeepONet cannot be applied to this problem.\n\nFigure 13 shows the results on prediction of shallow water equations’ solution operator using the DeepONet and the HyperDeepONet. The overall performance of the DeepONet is inferior to that of the HyperDeepONet, which is consistent with the result in Figure 12. In particular, the DeepONet has difficulty matching the overall circular shape of the solution when the number of parameters is small. This demonstrates the advantages of the HyperDeepONet when the computational resource is limited.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nFigure 12: The test L2 relative errors of four methods during training for the solution operator of shallow water equations.\n\nSame target (Differentiation)\n\nSame #param (Advection)\n\nDeepONet HyperDeepONet DeepONet HyperDeepONet\n\nTraining time (s) (per 1 epoch) 1.018 1.097 0.466 0.500\n\nInference time (ms)\n\n0.883 1.389 0.921 1.912\n\nTable 8: The training time and inference time for the differentiation operator problem and the solution operator of advection equation problem using DeepONet and HyperDeepONet.\n\nF.5 COMPARISON OF TRAINING TIME AND INFERENCE TIME\n\nTable 8 shows the training time and the inference time for the DeepONet and the HyperDeepONet for two different operator problems. When the same small target network is employed for the DeepONet and the HyperDeepONet, the training time and inference time for the HyperDeepONet are larger than for the DeepONet. However, in this case, the time is meaningless because DeepONet does not learn the operator with the desired accuracy at all (Table 1 and Figure 6).\n\nEven when both models use the same number of training parameters, HyperDeepONet takes slightly longer to train for one epoch than the DeepONet. However, the training complex operator using the HyperDeepONet takes fewer epochs to get the desired accuracy than DeepONet, as seen in Figure 7. This phenomenon can also be observed for the shallow water problem in Figure 12. It shows that the HyperDeepONet converges to the desired accuracy faster than any other variants of DeepONet.\n\nThe HyperDeepONet also requires a larger inference time because it can infer the target network after the hypernetwork is used to generate the target network’s parameters. However, when the input function’s sensor values are already fixed, the inference time to predict the output of the target function for various query points is faster than that of the DeepONet. This is because the size of the target network for HyperDeepONet is smaller than that of the DeepONet, although the total number of parameters is the same.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nFigure 13: The examples of predictions on the solution operator of shallow water equations using the DeepONet and the HyperDeepONet. The first column represents the exact solution generated in Takamoto et al. (2022), and the other four columns denote the predicted solutions using the corresponding methods. The four rows shows the predictions h(t, x1, x2) at four snapshots t = [0.25, 0.5, 0.75, 1].\n\n26",
    "reference": "# Summary Of The Paper\n\nThis paper targets a cutting-edge research problem that tries to use deep neural networks to solve partial differential equations (PDEs) through operator learning, with the potential to achieve faster and/or more accurate predictions for complex physical dynamics than traditional numerical methods. Although previously proposed methods (e.g., DeepONet and its variants) achieve some success, due to the limitation of linear approximation investigated by recent work, they typically need a large number of parameters and computational costs to learn the complex operators. In order to solve the problem, the authors propose HyperDeepONet, which learns complex operators with a target network whose parameters are generated by a hypernetwork (conditioned on the input function $u(\\cdot)$). Both theoretical and empirical evidence is provided to demonstrate the lower complexity and effectiveness of the proposed method.\n\n# Strength And Weaknesses\n\n**Pros:**\n\n* The motivation is clear, and the idea of leveraging a hypernetwork to generate parameters of the target network is interesting.\n* The authors analyze DeepONet with its variants from the perspective of: \"how information from the input function $u$ is injected into the target network $\\mathcal{G}_{\\theta}(u)$\" as illustrated in Figure 2. It is quite clear and makes connections among these methods, including HyperDeepONet.\n* Thorough theoretical analysis of the complexity is provided (but I cannot follow this part).\n\n**Cons:**\n\n* The writing of this paper could be further polished. Many typos also exist.\n\n* About the experiment section: I do not see the results of FNO [1], which also belongs to operator learning. Could the authors explain the reason? Moreover, A PDE benchmark [2] has been proposed recently. It would be better to provide some experimental results on it (the second half does not affect my evaluation since it was proposed recently).\n* The scalability of the proposed method seems to be an issue. This paper mainly considers the complexity of the target network. It would be better to consider the complexity of the branch network (also the hypernet) as well in the proposed method (i.e., the complexity of the Hyper will increase as that of the target network increases). \n\n**Question:**\n\n* About the experiment section, \n  * in Figure 7 and 8, could the authors explain why increasing the number of layers in the branch net does not reduce the $L^2$ error of DeepONet. For the HyperDeepONet, why it may even increase the error? What is the intuition behind this phenomenon?\n  * when considering the whole complexity (i.e., the same total Params in table 2), the performance of Hyper seems to be inferior to that of DeepONet. How about the performance of other baselines? \n* The authors mention that real-time prediction on resource-constrained hardware is crucial and challenging, so for each method, how about the training time, and inference time to obtain a desired prediction?\n\n**Minor:**\n\n* Please explicitly define the abbreviation when they first appear, e.g., PDEs on Page 1.\n\n* \"shows\" -> \"show\"; \"small\" -> \"a small\" in point 3 of main contributions.\n* \"solving\" -> \"solve\"; \"nonlinear\" -> \"a nonlinear\" in the second paragraph of related work.\n* \"considered\" -> \"considered as\" in section 3.3.\n\n[1] Fourier neural operator for parametric partial differential equations.\n\n[2] PDEBench: An Extensive Benchmark for Scientific Machine Learning.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: Fair. The writing of this paper could definitely be further improved.\n\nQuality: Good. The idea is interesting, along with theoretical and empirical evidence to support it.\n\nNovelty: I'm not an expert in this area, so I cannot fairly evaluate the novelty of this paper.\n\nReproducibility: Good, the authors provide the code of this paper.\n\n# Summary Of The Review\n\nOverall, this paper focuses on a significant research problem and proposes an interesting method to make learning complex operators with lower complexity (e.g., fewer parameters) possible. Well-rounded theory and empirical results are provided. I believe this work could be further improved with better writing and further evaluation. Based on the current version, I recommend a borderline acceptance.\n\n**Update on 04 Nov after reading two other reviews:** I agree with the other two reviewers, and my main concerns about this work are: a) the writing is not good (*eaJx*); b) the evaluation is not strong (*8Dyr*). If the authors could resolve these two concerns and update the draft accordingly before 18 Nov, I would be happy to recommend this work.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPRETRAINING THE VISION TRANSFORMER USING SELFSUPERVISED METHODS FOR VISION BASED DEEP REINFORCEMENT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe Vision Transformer architecture has shown to be competitive in the computer vision (CV) space where it has dethroned convolution-based networks in several benchmarks. Nevertheless, Convolutional Neural Networks (CNN) remain the preferential architecture for the representation module in Reinforcement Learning. In this work, we study pretraining a Vision Transformer using several state-of-the-art self-supervised methods and assess data-efficiency gains from this training framework. We propose a new self-supervised learning method called TOV-VICReg that extends VICReg to better capture temporal relations between observations by adding a temporal order verification task. Furthermore, we evaluate the resultant encoders with Atari games in a sample-efficiency regime. Our results show that the vision transformer, when pretrained with TOV-VICReg, outperforms the other self-supervised methods but still struggles to overcome a CNN. Nevertheless, we were able to outperform a CNN in two of the ten games where we perform a 100k steps evaluation. Ultimately, we believe that such approaches in Deep Reinforcement Learning (DRL) might be the key to achieving new levels of performance as seen in natural language processing and computer vision.\n\n1\n\nINTRODUCTION\n\nDespite the successes of deep reinforcement learning agents in the last decade, these still require a large amount of data or interactions to learn good policies. This data inefficiency makes current methods difficult to apply to environments where interactions are more expensive or data is scarce, which is the case in many real-world applications. In environments where the agent doesn’t have full access to the current state (partially observable environments), this problem becomes even more prominent, since the agent not only needs to learn the state-to-action mapping but also a state representation function that tries to be informative about a state given an observation. In contrast, humans, when learning a new task, already have a well-developed visual system and a good model of the world which are components that allow us to easily learn new tasks. Previous works have tried to tackle the sample inefficiency problem by using auxiliary learning tasks (Schwarzer et al., 2021b; Stooke et al., 2021; Guo et al., 2020), that try to help the network’s encoder to learn good representations of the observations given by the environments. These tasks can be supervised or unsupervised and can happen during a pretraining phase or a reinforcement learning (RL) phase in a joint-learning or decoupled-learning scheme.\n\nIn recent years, self-supervised learning has shown to be very useful in computer vision, the increasing interest in this area has resulted in the appearance of new and improved methods that train a network to learn important features from the data using only the data itself as supervision. A common approach to evaluating such methods is to train a network composed of the pretrained encoder, with the parameters frozen, paired with a linear layer in popular datasets, like ImageNet. These evaluations have shown that these methods can achieve high scores in different benchmarks, which shows how well the current state-of-the-art methods are able to encode useful information from the given images without being task-specific. Additionally, it has been shown that pretraining a network using self-supervised learning (or unsupervised) adds robustness to the network and gives better generalization capabilities (Erhan et al., 2010).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nAlso recently, a new architecture for vision-based tasks called the Vision Transformer (ViT) (Dosovitskiy et al., 2020) has shown impressive results in several benchmarks without using any convolutions. This architecture presents much weaker inductive biases when compared to a CNN, which can result in lower data efficiency. But the Vision Transformer, unlike the CNNs, can capture relations between parts of an image (patches) that are far apart from each other, thus deriving global information that can help the model perform better in certain tasks. Furthermore, when the model is pretrained, using supervised or self-supervised learning, it manages to surpass the best convolution-based models in terms of task performance. Nonetheless, and despite these successes in computer vision these results are yet to be seen in reinforcement learning.\n\nMotivated by the potential of the Vision Transformer, in particular when paired with a pretraining phase, and the increasing interest in self-supervised tasks for DRL, we study pretraining ViT using state-of-the-art (SOTA) self-supervised learning methods and use it as the representation module in a Deep RL algorithm. Consequently, we propose extending VICReg (Variance Invariance Covariance Regularization) (Bardes et al., 2022) with a temporal order verification task (Misra et al., 2016) to help the model better capture the temporal relations between consecutive observations. We named this approach Temporal Order Verification-VICReg or in short TOV-VICReg. While we could have adapted any of the other methods, we opted for VICReg due to its computational performance, simplicity, and good results in early experiments and metrics such as the ones presented in Section 7. After our empirical results in the Atari games, we present a small study of the pretrained encoders using several metrics to understand if they suffer from any representational collapse and also analyse the learned representations using similarity matrices and attention maps.\n\nOur main contributions are:\n\n• We propose a new self-supervised learning method which extends VICReg to capture the temporal relations between consecutive frames through a temporal order verification task, in Section 4.\n\n• We pretrain a Vision Transformer using several SOTA self-supervised methods and our proposed method, and study them through metrics (Section 7), visualizations (Section 8) and fine-tuning in reinforcement learning ( Section 6), where we show that temporal relations learned by the model pretrained with our method contribute to a relevant increase in data efficiency.\n\n2 RELATED WORK\n\nPretraining representations Previous work, similarly to our approach, has explored pretraining representations using self-supervised methods which led to great data-efficiency improvements in the fine-tuning phase (Schwarzer et al., 2021b; Zhan et al., 2020) or superior results in evaluation tasks, like AtariARI (Anand et al., 2020). Others have pretrained representations using RL algorithms, like DQN, and transfer those learned representations to a new learning task (Wang et al., 2022).\n\nTemporal Relations Other works have explored learning representations that have temporal information encoded. ATC (Augmented Temporal Contrast) (Stooke et al., 2021) trains an encoder to compute temporally consistent representations using contrastive learning, and the ST-DIM (SpatioTemporal DeepInfoMax) (Anand et al., 2020) captures spatial-temporal information by maximizing the mutual information between features of two consecutive observations.\n\nJoint learning In recent years, adding an auxiliary loss to the RL loss, usually called joint learning, has become a common approach by many proposed methods. Curl (Srinivas et al., 2020) adds a contrastive loss using a siamese network with a momentum encoder. Another work studies different joint-learning frameworks using different self-supervised methods (Li et al., 2022). SPR (Schwarzer et al., 2021a) uses an auxiliary task that consists of training the encoder followed by an RNN to predict the encoder representation k steps into the future. PSEs (Agarwal et al., 2021a) combines a policy similarity metric (PSM), that measures the similarity of states in terms of the behaviour of the policy in those states, and a contrastive task for the embeddings (CME) that helps to learn more robust representations. PBL (Guo et al., 2020) learns representations through an interdependence between an encoder, that is trained to be informative about the history that led to that observation, and an RNN that is trained to predict the representations of future observations. Proto-RL (Yarats\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\net al., 2021) uses an auxiliary self-supervised objective to learn representations and prototypes (Caron et al., 2020), and uses the learned prototypes to compute intrinsic rewards which will push the agent to explore the environment.\n\nAugmentations While we only use augmentations in the pre-training phase, their use during reinforcement learning has also been studied. Methods like DrQ (Kostrikov et al., 2021) and RAD (Laskin et al., 2020) pair an RL algorithm, like SAC, with image augmentations to improve data efficiency and generalization of the algorithms.\n\nVision Transformer for vision-based Deep RL Recent works, also compare the Vision Transformer to convolution-based architectures with a similar number of parameters and show that ViT is very data inefficient even when paired with an auxiliary task (Tao et al., 2022).\n\nSelf-Supervised learning for image sequences Multiple works propose simple pretext tasks to train encoders to capture information from image sequences. These pretexts tasks can be playback speed classification (Yao et al., 2020), a temporal order classification (Misra et al., 2016; Lee et al., 2017; Xu et al., 2019), a jigsaw game (Ahsan et al., 2019) or a masked modelling task (Sun et al., 2019). A different approach consists of using contrastive learning. In this category, we can find methods that maximise the similarity between image sequences (Feichtenhofer et al., 2021), use autoregressive models to predict frames multiple steps in the future Lorre et al. (2020), and maximize the similarity between temporally adjacent frames (Knights et al., 2021).\n\n3 BACKGROUND\n\n3.1 VISION TRANSFORMER\n\nViT (Dosovitskiy et al., 2020) is a model, for image classification tasks, that doesn’t rely on CNNs using only attention. The model wraps the encoder of a Transformer, uses patches of the input image as tokens and adds a classification token which after the computation will serve as the image representation. When compared to CNNs, ViT presents weaker image-specific inductive biases which can impact the sample-efficiency of the model during learning (d’Ascoli et al., 2021), however, it has been shown that with enough data the image-specific inductive biases become less important (Dosovitskiy et al., 2020). Moreover, ViT can capture relations between patches that are far apart from each other, thus deriving global information that can help the model perform better in certain tasks\n\n3.2 REINFORCEMENT LEARNING\n\nThe problem of an agent learning to solve a task in a certain environment can be defined as a Markov Decision Process (MDP). A MDP M is defined by the tuple ⟨S, A, R, T ⟩, where S is the set of states, A the set of actions, R the reward function, and T the transition function. At each timestep the agent is in a state s ∈ S and takes an action a ∈ A. Upon performing the action the agent receives from the environment a reward r ∈ R and a new state s′ ∈ S which is determined by the transition function T (s′, s, a). The MDP assumes that the Markov property holds in the environment, i.e. the state transitions are independent and the agent only needs to know the current state to perform an action P (at|x0, x1...xt) = P (at|xt). For the agent to decide what action to take it uses a policy function π, which gives a distribution over actions given a state, π(at|st). This policy is evaluated using the function V π(s), which estimates the expected total discounted reward of an agent in a state s and which follows a policy π.\n\n3.2.1 DQN AND RAINBOW\n\nDQN (Mnih et al., 2013) is a value-based method and uses a network with parameters φ that given a state s outputs a prediction of the distribution of Q values over actions, Qφ(s, a). The network learns the Q function by minimizing the mean squared error: (y − Qφ(s, a))2, where y = r + γ maxa′Qφ(s′, a′), as shown in Algorithm 1 at the Appendix.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nSeveral works followed the DQN algorithm which introduced changes to improve performance. Rainbow (Hessel et al., 2017) combines six improvements, Double Q-Learning (van Hasselt et al., 2016), Prioritized Replay (Schaul et al., 2016), Dueling Networks (Wang et al., 2016), Multi-step Learning (Sutton & Barto, 2018), Distributional RL (Bellemare et al., 2017), and Noisy Nets (Fortunato et al., 2018) resulting in a more stable and sample efficient algorithm.\n\n3.3 SELF-SUPERVISED METHODS\n\nRecent self-supervised methods for vision tasks can be put in two main categories: contrastive and non-contrastive.\n\nIn contrastive learning, methods like MoCo (He et al., 2020) or SimCLR (Chen et al., 2020a) learn using a loss function that pulls the positive samples together and pushes the negative samples apart. These methods usually require very large batch sizes or auxiliary structures that allow for more negative samples. MoCo, in particular, has three iterations v1 (He et al., 2020), v2 (Chen et al., 2020b), and v3(Chen et al., 2021). In this work, we consider the more recent version (v3). This version uses a siamese network, where in one path the augmented samples (queries) are computed by an encoder fθ ′ and (backbone) and a projector gφ, and in the other the samples (keys) by a momentum-encoder fθ a projector gφ. The loss function is the InfoNCE loss, with temperature, of the dot product of the queries with the keys.\n\nOn the other hand, non-contrastive methods don’t rely on the notion of positive and negative samples which results in a vast number of different approaches. DINO (Caron et al., 2021) consists of a siamese network where each path is fed with a random augmentation of the input and where the encoders learn to minimize the cross-entropy between their normalized output probability distributions, computed using a softmax with temperature scaling. The teacher encoder is updated using an exponential moving average of the student encoder parameters and in its computation path is used an additional centring operation that contributes to an asymmetry that helps the method avoid collapse. Unlike, most methods, DINO creates more than 2 augmentations of the same source. More precisely it creates a set of views composed of two global views and several local views. All views are computed by the student network while only the global views are computed by the teacher network, which pushes the student to create a local-to-global correspondence.\n\nVICReg, on the other hand, tries to learn representations invariant to augmentations by minimizing the L2 distance while maintaining some variance in the representation features and decorrelating features. A more detailed explanation of the method will be presented in Section 4.\n\nFor this study we selected DINO, MoCo, and VICReg since they are currently considered state-ofthe-art, their official implementations are available in PyTorch, and each represents a different type of approach.\n\n4 TOV-VICREG\n\nVICReg is a non-contrastive method that trains a network to be invariant to augmentations applied to the inputs while avoiding a trivial solution with the help of two additional losses, called variance and covariance, that act as regularizers over the embeddings. While VICReg is agnostic concerning the architectures used and even the weight sharing, in this work we consider the version where paths are symmetric, the weights are shared, and each path is composed of an encoder (also called backbone) and an expander. The expander increases the dimension of the representation vector in a non-linear way allowing the covariance loss to reduce dependencies and not only correlations of the representation vector. In addition, the expander also removes information that is not common to both representations.\n\nVICReg uses three loss functions: invariance is the mean of square distance between each pair of embeddings from the same original image, as shown in Equation 1, where Z, and Z ′ are two sets of embeddings, of size N , that result from computing two different augmentations of N sources, and zj denotes the j-th embedding in the set; variance is a hinge loss that computes, over the batch, the standard deviation of the variables in the embedding vector and pushes that value to be above a certain threshold, as shown in Equation 2, where d denotes the number of dimensions of the embedding vector, and Z j is the set of the j-th variables in the set of embedding Z; covariance is a function\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nthat computes the sum of the squared off-diagonal coefficients of a covariance matrix computed over a batch of embeddings, as shown in Equation 3, to decorrelate the variables from the embedding. While the invariance loss function tries to make the model invariant to augmentations, i.e. output the same representation vector, the other two functions regularize the method by pushing the variables of the embedding vector to vary above a certain threshold and decorrelating the variables in each embedding vector.\n\ni(Z, Z ′) =\n\n1 N\n\nN (cid:88)\n\nj\n\n(cid:13) (cid:13)zj − z′\n\nj\n\n(cid:13) 2\n(cid:13) 2\n\nv(Z) =\n\n1 d\n\nd (cid:88)\n\nj\n\nmax(0, γ − (cid:112)V ar(Z j))\n\nc(Z) =\n\n1 d\n\n(cid:88)\n\ni̸=j\n\n[Cov(Z)]2\n\ni,j\n\n(1)\n\n(2)\n\n(3)\n\nTOV-VICReg or Temporal-Order-Verification-VICReg extends VICReg to better capture the temporal relations between consecutive observations and consequently encode extra information that can be useful in the deep reinforcement learning phase. To achieve that we add a new temporal order verification task, as seen in Shuffle-and-Learn (Misra et al., 2016), that consists of a binary classification task where a linear layer learns to predict if three given representation vectors are in the correct order or not. Like the other losses, we also employ a coefficient for the temporal loss and in most of our experiments, the value is 0.1. Figure 1 visually illustrates TOV-VICReg.\n\nFigure 1: TOV-VICReg architecture\n\nAt each step we sample 3 consecutive observations, {xt−1, xt, xt+1}, xt is processed by two different augmentations, and like VICReg these are the augmentations used in BYOL (Grill et al., 2020), while xt−1 and xt+1 are processed by two simple augmentations composed of a color jitter and a random grayscale. The xt augmentations are computed by the VICReg computation path and the resultant embeddings are used for the loss functions, i.e. variance, invariance, and covariance. In the temporal order verification task we encode the augmentation of xt−1 and xt+1, and concatenate those two representations with one of the representations of xt, in our case we used the one that was augmented without solarize, obtaining the vector {yt−1, yt, yt+1}. At last, we randomly permute the order of the representations in the vector and feed the resultant concatenated vector to a linear layer with a single output node that predicts if the given concatenated vector has the representations in the right order or not. The temporal loss used to optimize the model for this task is a Binary Cross Entropy loss. TOV-VICReg’s pseudocode can be found in Appendix D.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n5 PRE-TRAINING METHODOLOGY\n\nWe pretrained four encoders, one using our proposed method TOV-VICReg and three using state-ofthe-art self-supervised methods: MoCov3 (Chen et al., 2021), DINO (Caron et al., 2021) and VICReg (Bardes et al., 2022). For this study, the encoder used is a Vision Transformer, more precisely the ViT tiny with a patch size of 8. We chose this patch size based on experiments that show that this value performed well in terms of data-efficiency when compared to 6, 10, and 12 without being too computationally intensive (Appendix B). Moreover, the implementation we use is an adaptation of the timm library (Wightman, 2019) implementation, which can be found in the source code of the DINO method. The dataset used is a set of observations from 10 of the 26 games in the Atari 100k benchmark, all available in the DQN Replay Dataset (Agarwal et al., 2020). For each game, we use three checkpoints with a size of one hundred thousand data points (observations), which makes up a total of three million data points (~55 hours). The pretraining phase is 10 epochs with two warmup epochs. We used the official code bases of all the self-supervised methods and tried to change the least amount of hyperparameters. Appendix H contains the tables with the hyperparameters used for each method.\n\n6 DATA-EFFICIENCY\n\nTo test the pretrained Vision Transformers in reinforcement learning and compare data-efficiency gains, we trained in the 10 games used for pre-training for 100k steps using the Rainbow algorithm (Hessel et al., 2017), with the DER (van Hasselt et al., 2019) hyperparameters. The only difference between the agents at the start is the representation module. We chose two networks to compare against, the Nature CNN (Mnih et al., 2015), and a ResNet with an amount of parameters similar to ViT tiny (Appendix C) (Schwarzer et al., 2021b) that has a size roughly similar to the ViT tiny. Moreover, we use a learning rate two orders of magnitude smaller for the encoder (1 × 10−6), which previous works and experiments performed by us show to be beneficial (Schwarzer et al., 2021b).\n\nIn this section, to report our results we follow the rliable (Agarwal et al., 2021b) evaluation framework, where the scores of all games are normalized and treated as one single task.\n\n6.1 RESULTS\n\nFigure 2 shows the aggregate metrics of seven different encoders on 10 Atari games with training runs of 100k steps. The first four (ViT+<method>) are ViT tiny models pretrained with four different self-supervised methods, while the last three (ViT, ResNet, and Nature CNN) are randomly initialized models. Starting with the randomly initialized models we can assess that the Nature CNN and the ResNet are the most sample efficient models, with ViT far behind. Regarding the pretrained models, ViT, when pretrained with our method, performs better than the other models and the non-pretrained ViT in all metrics. It is worth noting that we report a higher variance in the results of our proposed method when compared to the remaining methods and non-pretrained models. ViT+TOV-VICReg when compared to Nature CNN, which has far fewer parameters, and ResNet, with a similar number of parameters seems to closely match their sample-efficiency performance (Appendix Table 7). Furthermore, the difference between the non-pretrained ViT and ViT pretrained with TOV-VICReg shows that a good self-supervised method that explores temporal relations and 3 million data points can help close the sample-efficiency gap while remaining a more complex and capable model. Regarding the remaining self-supervised methods, MoCo seems to perform considerably well obtaining even a median very similar to TOV-VICReg and is then followed by DINO and VICReg, respectively. All pretrained ViTs show an improvement in comparison to the non-pretrained ViT.\n\n7 METRICS\n\nA significant phenomenon when doing self-supervised training is the collapse of the representations, which can be seen in three forms: representational collapse, dimensional collapse, and informational collapse. Representational collapse refers to the features of the representation vector collapsing to a single value for every input, meaning the variance of the features is zero, or close to zero. In\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The eval runs across the different games are normalized and treated as a single task. The IQM corresponds to the Inter-Quartile Mean among all the runs, where the top and bottom 25% are discarded and the mean is calculated over the remaining 50%. The Optimality Gap refers to the number of runs that fail to surpass the human average score, i.e. 1.0.\n\ndimensional collapse, the representations don’t use the full representation space, which can be measured by calculating the singular values of the covariance matrix calculated over the representations. Informational collapse defines the case where the features of the representation vector are correlated and therefore are representing the same information.\n\nDimensional Collapse All methods seem to avoid dimensional collapse, i.e. most dimensions have a singular value larger than zero, as observed in Figure 3. However, we notice that some methods make better use of the space available since they present higher singular values. TOV-VICReg, in particular, seems to excel in this metric, even improving the results obtained by VICReg. It is worth noting that both VICReg and TOV-VICReg employ a covariance loss that helps decorrelate the embedding variables which may be contributing positively to these results. Furthermore, we used a covariance coefficient of 10 for TOV-VICReg and 1 for VICReg a change that according to our experiments culminates in the increase here observed.\n\nFigure 3: Logarithm of the singular values of the representation vector’s covariance matrix sorted by value.\n\nRepresentational Collapse Results in the first row of Table 1 show the computed standard deviation of the representation vector over a batch of thousands of data points. DINO, VICReg and TOVVICReg show a value well above zero, meaning that none of the methods suffered from representation collapse during training. On the other hand, MoCo shows a much smaller value of 0.178, which is still, is far from a complete collapse. Both VICReg and TOV-VICReg use a hinge loss that pushes the representation vector to have a standard deviation of 1 or above. While VICReg slowly converges to this value our method converges to roughly 1.65, which might be the result of adding a temporal order verification task.\n\nInformational Collapse We report in the second row of Table 1, the comparison of the average correlation coefficients of the representation vectors. TOV-VICReg performs better than the other methods, including VICReg, which present very similar coefficients. Like in the dimensional collapse, this result is in part due to the higher covariance coefficient used in TOV-VICReg which by design\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nMetric\n\nDINO MoCo VICReg TOV-VICReg\n\nStd Corr. Coef.\n\n0.979 0.1764\n\n0.178 0.1538\n\n1.003 0.1531\n\n1.648 0.0780\n\nTable 1: Average standard deviation and correlation coefficient of the representation vector\n\nhelps the model to decorrelate the representation’s features. Increasing the coefficient in VICReg results in a lower correlation coefficient as well, but is still higher than TOV-VICReg.\n\n8 REPRESENTATIONS\n\nIn this section, we present different visualizations to better understand the representations learned by each of the methods. Our goal with the following visualizations is to help us better understand the learned representations and give some intuitions about their properties.\n\nCosine similarity Figure 4 presents a similarity matrix of the representations where we can observe that TOV-VICReg can better distinguish between observations of different games but also observations from the same game, as shown in Figure 5. MoCo, on the other hand, seems to make a good distinction between observations from the same game. However, we can observe in the colour bar that all the representations are very similar to each other, which corroborates the results obtained in Section 7. Oppositely, VICReg and DINO manage to spread representations more, as we can see in the colour bars, but, the yellow squares in the diagonal show that the representations from the same game are more similar to each other which is corroborated by Figure 5. Given the empirical results, we believe that this capacity to distinguish observations from the same game might be a good indicator.\n\nFigure 4: Similarity matrices of the representations computed by MoCo, DINO, VICReg, and TOVVICReg respectively. There are a total of 64 data points, from 4 different games: Alien, Breakout, MsPacman, and Pong, where from 0-15 are from Alien, 16-31 are from Breakout and so forth.\n\nFigure 5: Similarity matrices of the representations computed by MoCo, DINO, VICReg, and TOVVICReg respectively, of observations from MsPacman.\n\nAttention visualisation The research work that proposes DINO shows that the Vision Transformer is able to attend to important parts of the input after training using DINO. Inspired by these results, we try to make the same evaluation for the several self-supervised methods we are studying, including TOV-VICReg, and try to understand if any of the encoders can attend to interesting parts of the input.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nIn Figure 6, we can see the results of all methods for an observation from the game of Pong, where each method produces three attention maps, one for each self-attention head of the last block of the Vision Transformer. All pretrained ViT seem to attend at some level to important game features like the ball and the paddles. However, TOV-VICReg is the only method that doesn’t spread the attention to other parts of the frame that we don’t consider important to describe the current state of the game. When comparing to VICReg’s attention maps we believe that the temporal order verification task greatly helped the attention of the model. In more visually complex games, e.g. Freeway or MsPacman, these attention maps start to be more difficult to analyse but it is still possible to discern some important features.\n\nFigure 6: Attention maps produced by the pretrained ViTs. We fed a pretrained ViT with an observation from the game Pong and obtained the attention maps from the three heads in the last block.\n\n9 DISCUSSION & CONCLUSION\n\nIn this work, we presented a study of ViT for vision-based deep reinforcement learning using self-supervised pretraining, and proposed a self-supervised method that extends VICReg to better capture temporal relations between consecutive observations. Our results showed that the agent using a Vision Transformer that was pretrained with our method manages to surpass all other Vision Transformers, pretrained and non-pretrained, in sample efficiency and also achieves results very close to convolution-based models with far fewer parameters. These results reinforce the importance of encoding temporal relations between observations in the representation model, as shown by previous works, and also show that even vision models with weaker inductive biases and more parameters, when well pretrained, can achieve similar results in sample efficiency. Furthermore, we show several metrics, evaluation tasks and visualizations which can be of great value for future work.\n\nThe ability to use larger models, with millions of parameters, that are as sample efficient as some of the most popular CNN-based models (like Nature CNN or Impala ResNet), with thousands of parameters, is very important since it opens the door to using Deep RL in even more complex problems where smaller models tend to struggle to perform, without losing sample-efficiency. In this work, we try to advance the knowledge by studying the pretrain of a vision transformer using self-supervised methods. This approach has seen successes in natural language processing (Devlin et al., 2019; Brown et al., 2020), and computer vision (Radford et al., 2021) and we believe that similar approaches in RL have the potential to unlock new levels of performance never achieved before (Baker et al., 2022).\n\nREFERENCES\n\nRishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An Optimistic Perspective on Offline\n\nReinforcement Learning. arXiv:1907.04543, June 2020.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nRishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G. Bellemare. Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning. arXiv:2101.05265, March 2021a.\n\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep Reinforcement Learning at the Edge of the Statistical Precipice. In Advances in neural information processing systems, 2021b.\n\nUnaiza Ahsan, Rishi Madhok, and Irfan Essa. Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 179–189. IEEE, 2019.\n\nAnkesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre Côté, and R. Devon Hjelm. Unsupervised State Representation Learning in Atari. Technical Report arXiv:1906.08226, arXiv, November 2020.\n\nBowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos. arXiv:2206.11795, June 2022.\n\nAdrien Bardes, Jean Ponce, and Yann Lecun. Vicreg: Variance-invariance-covariance regularizaIn ICLR 2022-10th International Conference on Learning\n\ntion for self-supervised learning. Representations, 2022.\n\nMarc G. Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International learning. Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 449–458, 06–11 Aug 2017.\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\n\nWojciech Zaremba. OpenAI Gym. arXiv:1606.01540, June 2016.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901, 2020.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. In Advances in Neural Information Processing Systems, volume 33, pp. 9912–9924, 2020.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging Properties in Self-Supervised Vision Transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9650–9660, 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for Contrastive Learning of Visual Representations. In Proceedings of the 37th International Conference on Machine Learning, pp. 1597–1607, November 2020a.\n\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved Baselines with Momentum\n\nContrastive Learning. arXiv:2003.04297, March 2020b.\n\nXinlei Chen, Saining Xie, and Kaiming He. An Empirical Study of Training Self-Supervised Vision\n\nTransformers. arXiv:2104.02057, August 2021.\n\nStéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases. arXiv:2103.10697, June 2021.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\n\nBidirectional Transformers for Language Understanding. arXiv:1810.04805, May 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations, September 2020.\n\nDumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why Does Unsupervised Pre-training Help Deep Learning? In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 201–208, March 2010.\n\nChristoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3299–3309, 2021.\n\nMeire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. Noisy Networks For Exploration. February 2018.\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised Learning. arXiv:2006.07733, September 2020.\n\nDaniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-bastien Grill, Florent Altché, Rémi Munos, and Mohammad Gheshlaghi Azar. Bootstrap Latent-Predictive Representations for Multitask Reinforcement Learning. arXiv:2004.14646, April 2020.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised Visual Representation Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729–9738, 2020.\n\nMatteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining Improvements in Deep Reinforcement Learning. arXiv:1710.02298, October 2017.\n\nJoshua Knights, Ben Harwood, Daniel Ward, Anthony Vanderkop, Olivia Mackenzie-Ross, and Peyman Moghadam. Temporally coherent embeddings for self-supervised video representation learning. In 2020 25th International Conference on Pattern Recognition (ICPR), pp. 8914–8921. IEEE, 2021.\n\nIlya Kostrikov, Denis Yarats, and Rob Fergus. Image Augmentation Is All You Need: Regularizing\n\nDeep Reinforcement Learning from Pixels. arXiv:2004.13649, March 2021.\n\nMisha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement Learning with Augmented Data. In Advances in Neural Information Processing Systems, volume 33, pp. 19884–19895, 2020.\n\nHsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Unsupervised representation learning by sorting sequences. In Proceedings of the IEEE international conference on computer vision, pp. 667–676, 2017.\n\nXiang Li, Jinghuan Shang, Srijan Das, and Michael S. Ryoo. Does Self-supervised Learning Really\n\nImprove Reinforcement Learning from Pixels? arXiv:2206.05266, June 2022.\n\nGuillaume Lorre, Jaonary Rabarisoa, Astrid Orcesi, Samia Ainouz, and Stephane Canu. Temporal contrastive pretraining for video action recognition. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 662–670, 2020.\n\nIshan Misra, C. Lawrence Zitnick, and Martial Hebert. Shuffle and Learn: Unsupervised Learning Using Temporal Order Verification. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision – ECCV 2016, Lecture Notes in Computer Science, pp. 527–544, Cham, 2016.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning. arXiv:1312.5602, December 2013.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, February 2015.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning, pp. 8748–8763, July 2021.\n\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized Experience Replay. In\n\nICLR (Poster), January 2016.\n\nMax Schwarzer, Ankesh Anand, Rishab Goel, R. Devon Hjelm, Aaron Courville, and Philip Bachman. Data-Efficient Reinforcement Learning with Self-Predictive Representations. arXiv:2007.05929, May 2021a.\n\nMax Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, R Devon Hjelm, Philip Bachman, and Aaron C Courville. Pretraining Representations for Data-Efficient Reinforcement Learning. In Advances in Neural Information Processing Systems, volume 34, pp. 12686–12699, 2021b.\n\nAravind Srinivas, Michael Laskin, and Pieter Abbeel. CURL: Contrastive Unsupervised Representa-\n\ntions for Reinforcement Learning. arXiv:2004.04136, September 2020.\n\nAdam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling Representation Learning from Reinforcement Learning. In Proceedings of the 38th International Conference on Machine Learning, pp. 9870–9879, July 2021.\n\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7464–7473, 2019.\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning, second edition: An Introduction.\n\nNovember 2018.\n\nTianxin Tao, Daniele Reda, and Michiel van de Panne. Evaluating Vision Transformer Methods for\n\nDeep Reinforcement Learning from Pixels. arXiv:2204.04905, May 2022.\n\nHado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with Double\n\nQ-Learning. In Thirtieth AAAI Conference on Artificial Intelligence, March 2016.\n\nHado P van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforcement learning? In Advances in Neural Information Processing Systems, volume 32, 2019.\n\nHan Wang, Erfan Miahi, Martha White, Marlos C. Machado, Zaheer Abbas, Raksha Kumaraswamy, Vincent Liu, and Adam White. Investigating the Properties of Neural Network Representations in Reinforcement Learning. arXiv:2203.15955, March 2022.\n\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling Network Architectures for Deep Reinforcement Learning. In Proceedings of The 33rd International Conference on Machine Learning, pp. 1995–2003, June 2016.\n\nRoss Wightman.\n\nPytorch image models.\n\nhttps://github.com/rwightman/\n\npytorch-image-models, 2019.\n\nDejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised spatiotemporal learning via video clip order prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10334–10343, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nYuan Yao, Chang Liu, Dezhao Luo, Yu Zhou, and Qixiang Ye. Video playback rate perception for self-supervised spatio-temporal representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6548–6557, 2020.\n\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement Learning with Prototypical Representations. In Proceedings of the 38th International Conference on Machine Learning, pp. 11920–11931, July 2021.\n\nAlbert Zhan, Philip Zhao, Lerrel Pinto, Pieter Abbeel, and Michael Laskin. A Framework for Efficient\n\nRobotic Manipulation. arXiv:2012.07975, December 2020.\n\n13",
    "reference": "# Summary Of The Paper\n\nThe paper studies the effect of self-supervised pre-training for Vision Transformer based RL agents. It shows the effect of temporal order verification and VICReg on 10 Atari games. The proposed methods improves final return on Atari games.\n\n# Strength And Weaknesses\n\n[Strengths]\n- The paper is easy to follow.\n- The proposed method makes intuitive sense to me.\n\n[Weaknesses]\n- Most importantly, the main contribution of the submission is not very clear to me.\n- Also, the technical novelty and empirical novelty is quite limited. TOV and VICReg are existing self-supervised learning methods. Also, it is well-known that the use of pre-training is helpful for vision-based reinforcement learning.\n- The analysis in Section 7 (representation collapse, dimension collapse, etc) are very generic and do not provide much insight on using self-supervised learning for reinforcement learning.\n- It seems that the CNN result is without self-supervised training and ViT result is with self-supervised training (although it is unclear to me). Can authors add more on this?\n- I don't see the learning curve of return vs environment steps. This is pretty important for demonstrating sample efficiency in RL.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- The core contribution is not very clear to me.\n- Regarding the use of self-supervised learning for RL, \"CURL: Contrastive Unsupervised Representations for Reinforcement Learning\" is one of the work that popularized this. The authors should cite and compare against this work.\n- In light of the existing work, more work should be done to advance the field. Just trying self-supervised learning on reinforcement learning is not sufficient for publication at ICLR.\n- The contribution may be the use of self-supervised learning on ViT based image RL. The proposed method does not contain modifications for ViTs: TOV and VICReg are very generic methods, and they can be applied to any deep nets that take images (regardless of CNN vs ViT, RL vs supervised learning problems). Can authors provide why the proposed method is significantly novel or overcomes challenges in using self-supervised learning for RL?\n- Also, could authors provide the results of applying the proposed methods on CNNs? If the CNN performance can't be improved, why so?\n\n# Summary Of The Review\n\nThe submission is limited in terms of both theoretical novelty and empirical novelty. Especially, the empirical validity is pretty limited due to small set of experiments, missing experiment details, etc. More work should be done to warrant the publication of this work.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nWHAT MATTERS IN THE STRUCTURED PRUNING OF GENERATIVE LANGUAGE MODELS?\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nAuto-regressive large language models such as GPT-3 require enormous computational resources to use, leading to huge financial cost and environmental impact. Structured pruning methods traditionally reduce resource usage, however, their application to and efficacy for generative language models is heavily under-explored. We analyze the effects of magnitude, random, and movement (Lagunas et al., 2021) pruning on MLP layers in GPT-like models. We find that movement can underperform for these models while random pruning nearly matches the best methods. By examining neuron-level redundancy measures, we discover that movement does not select neurons based on how unique they are compared to other neurons, leaving behind excess redundancy. In view of this, we introduce Globally Unique Movement (GUM) to select neurons based on both uniqueness and sensitivity. We then discuss the roles of our techniques on different redundancy metrics through careful comparisons and ablations.\n\n1\n\nINTRODUCTION\n\nLarge language models (LLMs), such as the state-of-the-art GPT-3 model (Brown et al., 2020) with up to 175 billion parameters, have achieved remarkable performance in natural language processing (NLP) tasks. However, training and deploying such massive models also poses significant challenges in terms of computational cost, energy consumption, and environmental impact. Therefore, it is crucial to develop effective methods to reduce the size of LLMs without compromising their quality.\n\nNeural network pruning is a long-standing model compression method (Janowsky, 1989; Mozer & Smolensky, 1988; Frankle & Carbin, 2018; Karnin, 1990; Blalock et al., 2020). It can be broadly classified into two types: unstructured and structured. Unstructured pruning removes individual weights from the network based on some criteria, such as magnitude or movement, resulting in sparse weight matrices that can be stored and processed more efficiently. Structured pruning, on the other hand, eliminates whole components, such as neurons, channels, or blocks, leading to smaller architectures to reduce end-to-end inference latency. While unstructured pruning has been extensively studied and applied to LLMs (Wang et al., 2020b; Xu et al., 2021; Zafrir et al., 2021; Li et al., 2022), structured pruning is more challenging and less explored. However, structured pruning is also more desirable in many practical scenarios, such as deploying these models on resource-constrained devices or providing fast and reliable services based on LLMs.\n\nExisting work on structured pruning for LLMs focuses on BERT-like networks (Devlin et al., 2018) that consist of an encoder-decoder or an encoder-only architecture (Li et al., 2020; Xia et al., 2022; Zhang et al., 2022; Yao et al., 2021). These models are mainly used for natural language understanding (NLU) tasks, such as question answering, sentiment analysis, or natural language inference. Among the various methods, Block Movement Pruning (Lagunas et al., 2021) is a recent and popular technique that removes weight blocks based on movement. However, there is a lack of systematic research on structured pruning for decoder-only architectures such as GPT-2 Radford et al. (2019), GPT-3 Brown et al. (2020), or GPT-Neo Black et al. (2021), which are mainly used for natural language generation (NLG) tasks, such as text summarization, machine translation, or text completion. While there are some works that apply unstructured pruning (Li et al., 2022) or many kinds of orthogonal compression techniques to decoder-only LLMs (Wang et al., 2020a; Li et al., 2021; Edalati et al., 2022; Tao et al., 2022; Xu & Hu, 2022; Chen et al., 2021), there is no comprehensive evaluation of traditional structured pruning for these models on NLG tasks.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nIn this work, we compress decoder-only auto-regressive language models. Due to the lack of prior literature towards the same goal, we evaluate the performance of several general-domain pruning methods on NLG tasks, including magnitude and movement pruning. However, we find these methods can struggle or under-perform compared to naïve baselines, leading to the following question:\n\nWhat determines the performance of structured pruning on generative language models?\n\nWe aim to fill this gap by conducting a systematic study of structured fine-pruning (pruning while finetuning) methods for decoder-only LLMs on NLG tasks1, and further proposing a novel method that combines the strengths of different existing methods. Our main contributions are:\n\n• To our knowledge, we perform the first systematic evaluation of several structured pruning methods to decoder-only LLMs on NLG tasks. We find that they only achieve marginal improvements over randomly selecting neurons in finetuning. We explain their limitations under our proposed analysis framework, and characterize their advantages and disadvantages via the metrics we evaluated.\n\n• We propose an empirical analysis framework for structured pruning that relies on two fundamental measures of redundancy: sensitivity and uniqueness. Sensitivity reflects how much the removal of a network component affects the output of the model, while uniqueness reflects how much the information provided by a network component differs from others. Our framework allows us to understand and compare the behavior and performance of different pruning methods.\n\n• To show the impact made possible by our analysis, we propose a proof-of-concept method, Globally Unique Movement (GUM), that aims to maximize both sensitivity and uniqueness by pruning network components based on their global movement and local uniqueness scores. GUM outperforms the existing methods on several NLG tasks and achieves competitive compression rates, proving that future methods should preserve both sensitivity and uniqueness. We also conduct ablation studies to validate the effectiveness of our method components and design choices.\n\n2 BACKGROUND & METHODOLOGY\n\nThere are many general-domain pruning methods. We focus on fine-pruning, a relevant technique for language models which performs automated gradual pruning (Zhu & Gupta, 2017) while fine-tuning. We focus on pruning the MLPs of generative models. At inferencing time, generative models can cache attention vector states. Therefore, especially in large models, MLPs account for more time than attention for new tokens. MLPs also seem to store factual knowledge (Petroni et al., 2019; Meng et al., 2022), making their reduction possibly challenging.\n\nNotation and Background We shall define some notations for the MLP layers. Let σ(·) : Rm (cid:55)→ Rm be an element-wise activation function (e.g. GeLU), and let W1 ∈ Rm×d, W2 ∈ Rd×m be two weight matrices and b ∈ Rm be the bias vector. For an input token x ∈ Rd, the MLP layer output of x is expressed as MLP(x) = x + W2h(x) with intermediate output h(x) = σ(W1LN(x)), where LN represents layer normalization. We use ⊙ to denote element-wise multiplications. Lastly, we use L to denote the loss of the task. We study methods of reducing m, the intermediate dimension, which is usually set at m = 4d.\n\nMovement Pruning Movement Pruning (Sanh et al., 2020) is a popular fine-pruning method. In this paper, we focus on the block version of movement pruning (Lagunas et al., 2021), and we first introduce the original unstructured method. Let L(W ) be the task loss with weight parameters W . For each weight parameter Wi,j, we compute a accumulated score Si,j at iteration T , by the following expression2:\n\nS(T )\n\ni,j = −ηS\n\nW (t)\n\ni,j ·\n\n(cid:88)\n\nt≤T\n\n∂L(W (t)) ∂Wi,j\n\n(1)\n\n1All code publicly available at (removed for peer-review). 2Gradients are calculated straight-through to the mask scores, otherwise it is undefined (Bengio et al., 2013).\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nScore S Selection Structure Regularization Criteria\n\nMagnitude Gradual Random Hard Mvmt. L2-norms Topv(S) Local None Magnitude\n\nRandom (frozen) Topv(S) Local None Random\n\nEq. 1 Topv(S) Local R(S) Sensitivity\n\nSoft Mvmt.\n\nEq. 1 Threshold Global R(S) Sensitivity\n\nGUM\n\nEq. 1 Topv(S) Global R(S) + Rsim(S) (Eq.4) Sensitivity&Uniqueness\n\nTable 1: Comparison of pruning methods used. R(S) is defined in Section 2 and Rsim(S) is described in Eq 4.\n\nAfterwards, the scores are used to compute a mask M with entries Mi,j ∈ {0, 1}. And we apply the mask by W ′ = M ⊙ W , b′ = M ⊙ b, and U ′ = U ⊙ M to remove the masked weights.\n\nThere are two ways to compute the mask M : M = Topv(S) for hard movement and M = 1sigmoid(S)>τ for soft movement. τ and v are both hyperparameters, and Topv(S) is defined as: (cid:26)1,\n\nSi in top v%,\n\n(2)\n\nTopv(S)i =\n\n0, otherwise.\n\n(cid:80)\n\nAdditionally, mask scores are regularized via a regularization term with multiplier λmvp of the form R(S) = λmvp i,j sigmoid(Si,j). Hard movement prunes all layers by the same amount. Soft movement, however, allows for adaptive sparsity for different layers, which is known to be crucial for high-sparsity regimes (He et al., 2018b; Mallya et al., 2018), and is seen to be the superior method for NLU tasks in Sanh et al. (2020). Block pruning expands on this method to operate on groups of weights by combining mask scores per block, allowing for structured pruning (Lagunas et al., 2021).\n\nMagnitude Pruning We use a mask version of block magnitude pruning (a block extension of group-lasso, like Shen et al. (2021)) as the baseline. For each set G of parameters, we assign SG = ((cid:80) (i,j)∈G |Wi,j|2)1/2 as the mask score and gradually prune groups with the smallest scores.\n\nGradual Random pruning Random pruning approaches have been explored previously (Yu et al., 2017; Blalock et al., 2020), and in particular gradual, random pruning (Gale et al., 2019) has been found to perform relatively well. We further explore random pruning in conjunction with distillation. Our gradual random method freezes S at random initialization for the duration of finetuning and prunes using Topv(S).\n\nKnowledge Distillation In practice, pruning is often paired with knowledge distillation (Hinton et al., 2015) to boost performance. Distillation loss adds KL divergence between a teacher model and smaller student model. When used, we distill from a finetuned version of the model being pruned.\n\n3 FINE-PRUNING FOR GENERATIVE LANGUAGE MODELS\n\nWe present our framework for understanding the redundancy of pruning methods. In this work, we focus on improving the seminal work of movement pruning proposed in Sanh et al. (2020). However, naïvely applying movement often results in incremental or worse performance compared to random. We dissect our results using a systematic framework and analyze their behaviors and properties.\n\n3.1 OBSERVATIONS OF PREVIOUS PRUNING METHODS\n\nSoft Movement (BERT’s Best Method) Struggles for GPT-like Models It is shown in Lagunas et al. (2021) that soft movement enjoys better performance over hard movement when block pruning encoder-decoder models. However, we find the method severely struggles when using the original implementation3 on GPT-like models due to highly sensitive hyperparameters. For instance, the mask regularization parameter λmvp can either be too large and prune too aggressively, or too little,\n\n3Soft movement is the best to our knowledge at time of writing. Code is available at https://github. com/huggingface/nn_pruning. In this code, mask scores are added directly to the optimizer and are affected by optimizer algorithm or other hyperparameters. We use this code for a fair comparison between architectures, but manually updating the mask according to the definition might help (Zhang et al., 2022).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nGPT-Neo-125m Finetuned: 65.92\n\nMethod Soft L2-Magnitude\n\n50% / + Distil 63.81 / 65.25 62.71 / 65.32 Gradual Random 64.29 / 65.74 65.33 / 65.94 63.88 / 66.23\n\nHard/Topv GUM\n\n25% / + Distil 63.23 / 65.04 61.35 / 64.58 63.06 / 65.10 64.88 / 65.79 64.36 / 66.18\n\n10% / + Distil 62.996∗ / 64.95∗ 61.10 / 63.90 62.27 / 64.63 64.23 / 65.17 63.81 / 65.65\n\nTable 2: GPT-Neo-125m: Performance in Acclf on the validation set for decreasing amount leftover on WikiSQL. GUM outperforms compared to other methods, soft movement struggles to match other methods, and gradual random nearly performs as well as Topv. * indicates having 1-3% excess leftover neurons unpruned.\n\nModel\n\nMethod\n\n50% / + Distil\n\n25% / + Distil\n\nGPT-2-sm Finetuned: 70.32\n\nSoft L2-Magnitude\n\n68.27 / 69.319 68.33 / 69.62 Gradual Random 69.07 / 69.61 69.62 / 69.93 68.62 / 70.38\n\nHard/Topv GUM\n\n67.74 / 69.314 66.92 / 68.96 67.78 / 69.35 69.10 / 69.33 68.82 / 69.63\n\n10% / + Distil 67.25∗ / 69.11∗ 66.02 / 68.43 66.77 / 69.00 68.30 / 69.26 68.07 / 69.46\n\nTable 3: GPT-2-sm: Performance in Acclf on the validation set for decreasing amount leftover on WikiSQL. * indicates having 1-3% excess leftover neurons unpruned.\n\nresulting in under-pruning as shown below. Even after grid searching λmvp we still find subpar performance, and given the extremely high runtimes for this method as listed in Appendix A, we find this method impractical to use.\n\nRandom Pruning Works Surprisingly Well One might expect movement to easily beat random pruning, however, we find their performances to only slightly differ or sometimes match, especially under distillation. Other works have noted random pruning’s effectiveness (Gale et al., 2019), but we find the difference in generative tasks to be particularly slim. As shown in Tables 2 and 4, random pruning performs very close to both hard and soft movement pruning over WikiSQL and Wikitext datasets. Moreover, when combined with distillation, the gaps are largely closed between random pruning and other methods, which is also another intriguing observation itself, as we discuss below.\n\nDistillation Closes the Gaps Between Different Methods As shown in Table 2 and Table 3, methods with very different performances would perform rather similar if distilled from a non-pruned, finetuned model. Indeed, both WikiSQL and Wikitext experiments in Table 4 and Table 5 showed that when the network has fewer left-over neurons (e.g., 10% or 25%), the difference of accuracies or perplexities often fall below half of the difference without distillation. This observation remains consistent across models of different sizes, architectures, and tasks. Results for GPT-neo with 1.3billion parameters in Table 6 shows that pruning a larger model can still benefit from distillation. Knowledge distillation often boosts the performance of weaker methods even more, which might suggest the differences between methods are largely due to the inability to learn more diverse features set in fine-pruning, as suggested by the work of Allen-Zhu & Li (2020).\n\n3.2 TWO TYPES OF REDUNDANCY MEASURES: SENSITIVITY AND UNIQUENESS\n\nIn order to understand why these pruning methods display such behaviors, we devise a framework to characterize the leftover neurons of pruned network based on two criteria: sensitivity and uniqueness4. Sensitivity captures how much a neuron contributes to the task objective L, while uniqueness captures how much information it provides that is not already captured by other neurons. We formalize these notions of redundancy as follows:\n\n4These are two known concepts in literature, but have not been both combined into one pruning method.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition 3.1 (Redundancy Criteria) Given a set of neurons {hi(·)}i∈[m] and input X, we call one neuron hi redundant if it meets at least one of the following two conditions:\n\n1. Sensitivity/Saliency: the neuron is not salient if its outputs are either negligible or has small\n\ngradient when optimizing for the downstream task, mathematically described as\n\n(cid:104)\n\nE\n\n|hi(X) ·\n\n∂L ∂hi(X)\n\n(cid:105) |\n\n≈ 0\n\n2. Uniqueness: the neuron is not unique if its outputs could be reconstructed entirely with a\n\nlinear combination of the outputs from other neurons, mathematically described as\n\nhi(X) ∈ span({hj(X)}j̸=i),\n\nover all inputs X\n\nIntuitively, a sensitive neuron has outputs that greatly contribute to the final output, while a unique neuron has outputs which are different from that of others. These metrics are independent from one another, so a neuron could be highly salient but replaceable by other neurons, or it could be highly unique but ultimately contribute little to the network. Consider a toy example where two neurons hi and hj have the same non-zero weights and large gradient. Neuron hi could easily be removed by doubling the outputs of hj, so they are not unique, but both are highly salient.\n\nThe General Trade-off Between Saliency and Uniqueness Equipped with Definition 3.1, we find the following important trends. Figure 1 and Figure 2 show that without distillation, different methods often have a preference of focus on one of the redundancy measures. We now comment on trends across all experimental result tables (Table 2, 3, 4, 5, 6, 7, 8). The best performing methods strike a balance between both measures, establishing a strong correlative link between these metrics and final pruning performance. Under distillation, however, sensitivity seemingly concentrates across methods. Regardless of method, as more pruning occurs, sensitivity decreases and uniqueness increases in general. For individual methods, we can further dive deeper as below:\n\n• Magnitude pruning universally scores worst on both metrics, explaining its poorer performance in all experiments. However, with distillation, gaps of sensitivity between methods noticeably decreases, which partially describes why distillation improves it significantly.\n\n• Random pruning obtains similar distillation sensitivity and uniqueness, though slightly lower, to hard movement, lending credence to its overall high performance. However, sensitivity is markedly lower without distillation as is reflected in all figures. This is proof that hard movement does not target uniqueness, given random pruning does not target uniqueness.\n\nFigure 1: Sensitivity and Uniqueness measured on the training set for GPT-Neo-125m. The vertical axis is defined as the ratio of the corresponding metric between the pruned model and a baseline model (which is non-pruned and fully fine-tuned) with a maximum of 1x. We are able to use these graphs to analyze and compare the performance of different pruning methods. Details of measurements are given in Appendix E.\n\n5\n\n[[[[[6HQVLWLYLW\\[[[[[8QLTXHQHVV/HIWRYHU:LNL64/[[[[[6HQVLWLYLW\\[[[8QLTXHQHVV/HIWRYHU:LNLWH[W0DJQLWXGH*805DQGRP6RIW7RSvZR'LVWLOZ'LVWLOUnder review as a conference paper at ICLR 2023\n\nFigure 2: Sensitivity and Uniqueness measured on the training set for GPT-2-sm. The vertical axis is defined as the ratio of the corresponding metric between the pruned model and a baseline model (which is non-pruned and fully fine-tuned) with a maximum of 1x. Details of measurements are given in Appendix E.\n\n• Soft movement pruning also usually scores poorly on both metrics, and sometimes abysmally\n\nas in its sensitivity in Figure 1, helping describe its overall poor performance.\n\n• Hard movement pruning consistently obtains the highest sensitivity with not-far-behind uniqueness across different datasets and architectures. This correlates with the high performance when distillation is not used. However, when combined with distillation, the gaps of sensitivity between methods converge, and the advantage of hard movement fades.\n\n• GUM, our proof-of-concept method, nearly always obtains best uniqueness while maintaining decent sensitivity, further improved using distillation, explaining its superiority across various tasks. However, GUM has a larger performance increase for GPT-Neo-125m than for GPT-2-sm on WikiSQL; this is explained in Figure 2 as pruned GPT-2 already has high baseline uniqueness for WikiSQL (∼2x) so further increase incurs diminishing returns.\n\nGiven the training/validation split and general noise in the datasets, there are some outlier points, for instance, GUM’s surprisingly poor distilled uniqueness for Wikitext in Figure 1. We observe higher absolute uniqueness on Wikitext in general (around 95% of neurons are unique per cosine similarity), meaning uniqueness varies over datasets and improving uniqueness is difficult.\n\n4 GLOBALLY UNIQUE MOVEMENT\n\nAfter observing the lack of uniqueness amongst leftover neurons, we set out to improve the performance of hard movement. We introduce two techniques which together comprise Globally Unique Movement (GUM). In essence, we encourage a score-weighted uniqueness term by multiplying the score regularizer and the cosine similarity together, to obtain a balance of uniqueness and sensitivity.\n\nTackling Non-Unique Redundancy Regularizing or pruning via similarity is a well-explored topic (Ayinde et al., 2019; Zhu et al., 2018; Srinivas & Babu, 2015; Santacroce et al., 2020) and existing techniques would increase uniqueness. However, we integrate more cleanly with movement to insulate weights from regularization, with a small increase in training time as listed in Appendix A.\n\nOur approach regularizes mask scores based on cosine similarity 5. Cosine similarity between the outputs of any two neurons given input X (for example, a batch of tokens) is defined simply as\n\nsim(hi(X), hj(X)) =\n\nhi(X)⊤hj(X) ∥hi(X)∥2 ∗ ∥hj(X)∥2\n\n(3)\n\n5Solving for linear combinations of neurons during training is prohibitively expensive, so we consider cosine\n\nsimilarity as a \"first-order\" proxy.\n\n6\n\n[[6HQVLWLYLW\\[[[8QLTXHQHVV/HIWRYHU:LNL64/[[[6HQVLWLYLW\\[[[[8QLTXHQHVV/HIWRYHU:LNLWH[W0DJQLWXGH*805DQGRP6RIW7RSvZR'LVWLOZ'LVWLOUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Running Cosine Similarity Update Require: a set of neurons hi(·) for i ∈ [m], inputs from a set Z (usually intermediate outputs of\n\nattention layers), an update multiplier λsim;\n\n1: Initialize running similarity sim(0)(hi, hj) = 0 for i, j ∈ [m], running inner products C (0)\n\ni,j = 0,\n\nand running output vector norms Q(0)\n\ni = 0 for i ∈ [m];\n\n2: while still training do 3:\n\nSample a input X ∈ Z, compute the output vector neuron hj(X) for j ∈ [m]7. Update C (t+1) Update Q(t+1) Update similarity by:\n\ni,j + λsim · hi(X)⊤hj(X), i + λsim · ∥hi(X)∥2 2,\n\ni,j ← (1 − λsim)C (t) i ← (1 − λsim)Q(t)\n\n∀i, j ∈ [m]\n\n∀i ∈ [m]\n\n4:\n\n5: 6:\n\nsim(t+1)(hi, hj) ← C (t+1)\n\ni,j\n\n(cid:113)\n\n/\n\nQ(t+1)\n\ni\n\nQ(t+1)\n\nj\n\n,\n\n∀i, j ∈ [m]\n\n7: end while\n\nHowever, calculating similarity with only intra-batch estimates is noisy and unreliable, so we introduce a running version of its estimates in Algorithm 1 to obtain cosine similarity sim(t)(hi, hj) between neurons hi(·) and hj(·). Now, we build on the regularization term Rsim(S) of movement pruning to define a new regularization: Let Nleft be the number of leftover neurons, for each group j ∈ [m] and its corresponding score Sj, we define a term Uj = 1 i∈[m],i̸=j sim(hj, hi), and then we multiply Uj to the original terms in R(S) to obtain (cid:88) j\n\nUj · sigmoid(Sj)\n\nRsim(S) = λmvp\n\nNleft\n\n(cid:80)\n\n(4)\n\nGlobal Topv for Soft-Like Movement Hard movement removes the same amount of weights per layer independently. Global Topv instead uses Topv function on the set of all mask scores in the network jointly. Global Topv was originally explored for movement (Sanh et al., 2020) and was found to perform similarly. We find when used in conjunction with uniqueness regularization, Global outperforms Local. Global Topv intuitively allows for more flexibility when pruning. When pruning locally, it is necessary to choose the least common pruning percent - if one layer requires 50% neurons before severe degradation, all layers must keep 50%. Global comparison removes this loophole in a similar manner to soft movement 6.\n\n5 RESULTS\n\nIn this section, we present results on three different kinds of generative language modeling tasks: language modeling with Wikitext-103 Merity et al. (2016), text-to-text generation and natural language understanding with SAMsum Gliwa et al. (2019), and exact match measurable text-to-code generation with WikiSQL Zhong et al. (2017). Details and hyperparameters are listed in Appendix F. When distilling, the teacher model used is the finetuned version of the model. To ensure trends hold when scaling up, we present one experiment with GPT-Neo-1.3b in section 5. For all pruning amounts, we will present in terms of final percentage leftover - i.e., 75% of neurons remain after pruning. For soft movement, final prune percentage is shown in parentheses when it differs from desired by a large amount.\n\nIn general, GUM is found to outperform Topv by a margin similar to the difference between Topv and gradual random pruning, with some exceptions. While small, we argue this gap shows the effectiveness of preserving neuron uniqueness alongside saliency.\n\nWikitext-103 Results on the Wikitext-103 dataset Merity et al. (2016), one of the most popular datasets for causal language modeling, are shown in Tables 4 and 5. Because performance on Wikitext-103 is in perplexity (PPL), it is a highly consistent and discriminatory dataset to prune on. We are unable to ever fully recover original model performance after pruning, suggesting that any compression increases uncertainty. Distillation generally hurts performance across all methods.\n\n6Appendix D shows an example pruning distribution for one pruned network.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nGPT-Neo-125m Finetuned: 16.138\n\nMethod Soft L2-Magnitude\n\n75% / + Distil 17.814 / 17.651 18.524 / 18.048 Gradual Random 17.307 / 17.144 16.974 / 17.142 16.822 / 17.158\n\nHard/Topv GUM\n\n50% / + Distil 19.470 / 19.053 20.834 / 20.041 18.900 / 18.410 18.253 / 18.369 17.881 / 18.314\n\n25% / + Distil 21.169 / 20.4678∗ 23.692 / 22.604 21.458 / 20.546 20.495 / 20.194 20.059 / 19.833\n\nTable 4: GPT-Neo-125m: Performance in perplexity (PPL) on the validation set for decreasing amount leftover on Wikitext-103. * indicates having 1-3% excess leftover neurons unpruned.\n\nModel\n\nGPT-2-sm Finetuned: 15.571\n\nMethod Soft L2-Magnitude\n\n75% / + Distil 16.754 / 16.950 17.399 / 17.414 Gradual Random 16.574 / 16.823 16.363 / 16.730 16.242 / 16.680\n\nHard/Topv GUM\n\n50% / + Distil 18.261 / 18.051 19.595 / 19.178 17.974 / 17.862 17.611 / 17.742 17.444 / 17.692\n\n25% / + Distil 19.948 / 19.473∗ 22.593 / 21.667 20.444 / 19.798 20.016 / 19.663 19.877 / 19.681\n\nTable 5: GPT-2-sm: Performance in perplexity (PPL) on the validation set for decreasing amount leftover on Wikitext-103. * indicates having 1-3% excess leftover neurons unpruned.\n\nWikiSQL As opposed to the other datasets, WikiSQL Zhong et al. (2017) contains hard groundtruth labels for comparison via Exact Match (EM). Due to this, our best performance is achieved in WikiSQL, where GUM is able to remove up to 75% of neurons while maintaining performance on GPT-Neo. Results are shown in Tables 2 and 3. We also present results for GPT-Neo-1.3b only on WikiSQL in Table 6. Results for this experiment follow a similar trend to smaller models.\n\nModel\n\nMethod\n\n50% / + Distil\n\n25% / + Distil\n\n10% / + Distil\n\nGPT-Neo-1.3B Finetuned: 74.88\n\nGradual Random 72.18 / 74.76 73.33 / 74.75 72.88 / 74.70\n\nHard/Topv GUM\n\n70.38 / 73.83 72.18 / 74.54 71.80 / 74.62\n\n68.56 / 72.77 71.14 / 73.77 71.35 / 74.157\n\nTable 6: GPT-Neo-1.3B: Performance in Acclf for decreasing amount leftover on WikiSQL.\n\nSAMsum Results on SAMsum Gliwa et al. (2019) are presented in Tables 7 and 8. Popular for encoder-decoder models, this dataset entails summarizing short instant message conversations. Larger generative models have been explored for this task (Feng et al., 2021; Zhang et al., 2019), achieving competitive results. We use this dataset to test the natural language understanding and summarization skills of small models under pruning. We note poor relative baseline results relative to encoder-decoder models as expected, however, pruning trends follow that of other datasets and GUM generally outperforms Topv.\n\n6 ADDITIONAL RELATED WORKS\n\nGeneral Domain Pruning Neural net pruning has been proposed years before the explosion of deep learning research (Janowsky, 1989; Mozer & Smolensky, 1988; Karnin, 1990), and are summarized in an outdated survey Reed (1993). Previous works have explored many approaches of pruning neural nets (Wen et al., 2016; Han et al., 2015b;a; Li et al., 2016). Recently, the lottery ticket hypothesis Frankle & Carbin (2018) proposed a new direction to prune at initialization instead. However, there is also a massive divergence of methods or claims that it is not worthwhile Liu et al. (2018); Blalock et al. (2020). Regardless, many strong techniques exist in modern incarnations across all kinds of architectures (Yang et al., 2016; Luo et al., 2017; He et al., 2018a).\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nMethod No Prune\n\nGradual Random\n\nHard/Topv\n\nGUM\n\nLeftover %\n\n50% / + Distil 25% / + Distil 10% / + Distil 50% / + Distil 25% / + Distil 10% / + Distil 50% / + Distil 25% / + Distil 10% / + Distil\n\nRouge_1 38.68 35.54 / 36.82 33.11 / 35.71 31.83 /34.60 37.68 / 36.94 36.38 / 37.34 33.07 / 36.12 37.22 / 38.45 36.18 / 37.57 34.72 / 36.52\n\nRouge_2 14.74 12.71 / 13.40 11.01 / 13.13 10.02 / 11.72 14.17 / 13.72 13.00 / 14.24 10.95 / 12.62 13.79/ 14.27 13.18 / 13.71 11.88 / 13.40\n\nRouge_L 31.73 29.11 / 30.28 27.39 / 29.50 26.49 / 28.40 31.12 / 30.64 29.96 / 31.17 27.70 / 29.70 30.72 / 31.35 29.99 / 30.91 28.82 / 29.97\n\nRouge_LSUM 31.76 29.04 / 30.27 27.37 / 29.48 26.46 / 28.34 31.09 / 30.62 29.95 / 31.15 27.68 / 29.67 30.72 / 31.36 30.00/ 30.93 28.79 / 29.96\n\nTable 7: GPT-Neo-125m: Validation results on SAMsum. Higher is better for all metrics. In general, more pruning hurts performance, and GUM outperforms Topv.\n\nMethod No Prune\n\nGradual Random\n\nHard/Topv\n\nGUM\n\nLeftover %\n\n50% / + Distil 25% / + Distil 10% / + Distil 50% / + Distil 25% / + Distil 10% / + Distil 50% / + Distil 25% / + Distil 10% / + Distil\n\nRouge_1 40.83 39.29 / 39.69 38.09 / 39.73 36.91 / 38.88 39.93 / 40.23 38.84 / 40.49 38.28 / 39.49 38.80 / 40.74 37.56 / 39.74 39.12 / 39.80\n\nRouge_2 16.85 15.25 / 15.74 14.43 / 15.35 13.02 / 14.71 16.43 / 16.07 14.88 / 16.12 14.58 / 15.32 15.52 / 16.22 14.24 / 15.72 15.01 / 15.79\n\nRouge_L 33.72 32.16 /32.72 31.16 / 32.72 30.13 / 31.85 33.40 / 33.39 31.83 / 33.46 31.36 / 32.57 32.27 / 33.99 30.94 / 32.93 32.20 / 32.96\n\nRouge_LSUM 33.70 32.15 / 32.72 31.15 / 32.66 30.13 / 31.83 33.38 / 33.39 31.82 / 33.45 31.37 / 32.57 32.23 / 33.95 30.91 / 32.93 32.21 / 32.95\n\nTable 8: GPT-2-sm: Validation results on SAMsum. Higher is better for all metrics. In general, more pruning hurts performance, and GUM outperforms Topv.\n\nCompressing Language Models Compressing LLMs in particular has spawned a unique kind of pruning. Given that LLMs first undergo pre-training on massive amounts of data, works such as Xu et al. (2021); Zafrir et al. (2021); Li et al. (2022) find ways to prune and finetune these models on downstream data. Building on automated gradual pruning (Zhu & Gupta, 2017) and learned threshold pruning (Azarian et al., 2020), movement pruning (Sanh et al., 2020) and further block movement (Lagunas et al., 2021) have become highly popular methods for fine-pruning, combining finetuning with pruning for overall best performance. Since then, many works have attempted to improve on movement (Yao et al., 2021; Zhang et al., 2022; Xia et al., 2022; Kwon et al., 2022). As previously mentioned, however, we are unable to find any comparable works systematically exploring structured pruning for decoder-only models.\n\n7 CONCLUSION & FUTURE WORK\n\nIn this paper, we have performed an evaluation of structured pruning on generative language models, finding existing methods to improve over random pruning less than expected. In addition, we have proposed a framework for analyzing pruning methods in terms of uniqueness and saliency, two important criteria for preserving model quality and diversity. We have presented a novel method based on these metrics, GUM, for structured pruning of generative models, based on uniqueness regularization and global Topv pruning. Our method can be applied to the MLP layers of various generative models, but there are still many open questions and challenges for pruning other components, such as attention heads or MoE modules. We also acknowledge the limitations of our method, which can reduce saliency and performance, suggesting possible directions for improving uniqueness pruning. Our work is an initial step towards understanding and improving structured pruning of generative models.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nZeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and\n\nself-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.\n\nBabajide O. Ayinde, Tamer Inanc, and Jacek M. Zurada. Regularizing deep neural networks by enhancing diversity in feature extraction. IEEE Transactions on Neural Networks and Learning Systems, 30(9):2650–2661, 2019. doi: 10.1109/TNNLS.2018.2885972.\n\nKambiz Azarian, Yash Bhalgat, Jinwon Lee, and Tijmen Blankevoort. Learned threshold pruning,\n\n2020. URL https://arxiv.org/abs/2003.00075.\n\nYoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. URL https://arxiv.org/abs/ 1308.3432.\n\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/ 10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.\n\nDavis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of\n\nneural network pruning?, 2020. URL https://arxiv.org/abs/2003.03033.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https: //arxiv.org/abs/2005.14165.\n\nXuxi Chen, Tianlong Chen, Yu Cheng, Weizhu Chen, Zhangyang Wang, and Ahmed Hassan Awadallah. Dsee: Dually sparsity-embedded efficient tuning of pre-trained language models, 2021. URL https://arxiv.org/abs/2111.00160.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2018. URL https://arxiv.org/ abs/1810.04805.\n\nOndˇrej Dušek, Jekaterina Novikova, and Verena Rieser. Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge. Computer Speech & Language, 59: 123–156, January 2020. doi: 10.1016/j.csl.2019.06.009.\n\nAli Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Nia, James Clark, and Mehdi Rezagholizadeh. Kronecker decomposition for GPT compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 219–226, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.24. URL https://aclanthology.org/2022.acl-short.24.\n\nXiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, and Ting Liu. Language model as an annotator: Exploring dialogpt for dialogue summarization, 2021. URL https://arxiv.org/ abs/2105.12544.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. 2018. doi: 10.48550/ARXIV.1803.03635. URL https://arxiv.org/abs/1803. 03635.\n\nTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks, 2019. URL\n\nhttps://arxiv.org/abs/1902.09574.\n\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization. Association for Computational Linguistics, 2019. doi: 10.18653/v1/d19-5409. URL https://doi.org/10.18653%2Fv1%2Fd19-5409.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.\n\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\n\nefficient neural network. Advances in neural information processing systems, 28, 2015b.\n\nYihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018a.\n\nYihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices, 2018b. URL https://arxiv.org/abs/ 1802.03494.\n\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.\n\nURL https://arxiv.org/abs/1503.02531.\n\nSteven A. Janowsky. Pruning versus clipping in neural networks. Phys. Rev. A, 39:6600–6603, Jun 1989. doi: 10.1103/PhysRevA.39.6600. URL https://link.aps.org/doi/10.1103/ PhysRevA.39.6600.\n\nE.D. Karnin. A simple procedure for pruning back-propagation trained neural networks. IEEE\n\nTransactions on Neural Networks, 1(2):239–242, 1990. doi: 10.1109/72.80236.\n\nWoosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers, 2022. URL https:// arxiv.org/abs/2204.09656.\n\nFrançois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M. Rush. Block pruning for faster\n\ntransformers, 2021. URL https://arxiv.org/abs/2109.04838.\n\nBingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li, Zhengang Li, Hang Liu, and Caiwen Ding. Efficient transformer-based large scale language representations using hardware-friendly block structured pruning, 2020. URL https://arxiv.org/abs/2009.08065.\n\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for\n\nefficient convnets. arXiv preprint arXiv:1608.08710, 2016.\n\nTianda Li, Yassir El Mesbahi, Ivan Kobyzev, Ahmad Rashid, Atif Mahmud, Nithin Anchuri, Habib Hajimolahoseini, Yang Liu, and Mehdi Rezagholizadeh. A short study on compressing decoderbased language models, 2021. URL https://arxiv.org/abs/2110.08460.\n\nYuchao Li, Fuli Luo, Chuanqi Tan, Mengdi Wang, Songfang Huang, Shen Li, and Junjie Bai. Parameter-efficient sparsity for large language models fine-tuning, 2022. URL https://arxiv. org/abs/2205.11005.\n\nZhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of\n\nnetwork pruning, 2018. URL https://arxiv.org/abs/1810.05270.\n\nJian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.\n\nArun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights, 2018. URL https://arxiv.org/abs/1801.06519.\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\n\nassociations in gpt, 2022. URL https://arxiv.org/abs/2202.05262.\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\n\nmodels, 2016.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nMichael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a network via relevance assessment. In D. Touretzky (ed.), Advances in Neural Information Processing Systems, volume 1. Morgan-Kaufmann, 1988. URL https://proceedings.neurips. cc/paper/1988/file/07e1cd7dca89a1678042477183b7ac3f-Paper.pdf.\n\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. Language models as knowledge bases?, 2019. URL https://arxiv. org/abs/1909.01066.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n\nmodels are unsupervised multitask learners. 2019.\n\nRussell Reed. Pruning algorithms-a survey. IEEE transactions on Neural Networks, 4(5):740–747,\n\n1993.\n\nVictor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by\n\nfine-tuning, 2020. URL https://arxiv.org/abs/2005.07683.\n\nMichael L. Santacroce, Daniel Koranek, and Rashmi Jha. Detecting malware code as video with IEEE Access, 8:132748–132760, 2020. doi:\n\ncompressed, time-distributed neural networks. 10.1109/ACCESS.2020.3010706.\n\nMaying Shen, Pavlo Molchanov, Hongxu Yin, and Jose M Alvarez. When to prune? a policy towards\n\nearly structural pruning. arXiv preprint arXiv:2110.12007, 2021.\n\nSuraj Srinivas and R. Venkatesh Babu. Data-free parameter pruning for deep neural networks, 2015.\n\nURL https://arxiv.org/abs/1507.06149.\n\nChaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. Compression of generative pre-trained language models via quantization. arXiv preprint arXiv:2203.10705, 2022.\n\nHanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning, 2020a. URL https://arxiv.org/abs/2012.09852.\n\nZiheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2020b. doi: 10.18653/v1/2020.emnlp-main. 496. URL https://doi.org/10.18653%2Fv1%2F2020.emnlp-main.496.\n\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in\n\ndeep neural networks, 2016. URL https://arxiv.org/abs/1608.03665.\n\nMengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate\n\nmodels, 2022. URL https://arxiv.org/abs/2204.00408.\n\nDongkuan Xu, Ian E. H. Yen, Jinxi Zhao, and Zhibin Xiao. Rethinking network pruning – under the pre-train and fine-tune paradigm, 2021. URL https://arxiv.org/abs/2104.08682.\n\nGuangxuan Xu and Qingyuan Hu. Can model compression improve nlp fairness, 2022. URL\n\nhttps://arxiv.org/abs/2201.08542.\n\nTien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks using energy-aware pruning, 2016. URL https://arxiv.org/abs/1611.05128.\n\nZhewei Yao, Xiaoxia Wu, Linjian Ma, Sheng Shen, Kurt Keutzer, Michael W. Mahoney, and Yuxiong He. Leap: Learnable pruning for transformer-based models, 2021. URL https: //arxiv.org/abs/2105.14636.\n\nRuichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I. Morariu, Xintong Han, Mingfei Gao, Ching-Yung Lin, and Larry S. Davis. Nisp: Pruning networks using neuron importance score propagation, 2017. URL https://arxiv.org/abs/1711.05908.\n\nOfir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, and Moshe Wasserblat. Prune once for all: Sparse pre-trained language models, 2021. URL https://arxiv.org/abs/2111.05754.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nQingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, and Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight importance, 2022. URL https://arxiv.org/abs/2206.12562.\n\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational response generation, 2019. URL https://arxiv.org/abs/1911.00536.\n\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from\n\nnatural language using reinforcement learning. CoRR, abs/1709.00103, 2017.\n\nMichael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model\n\ncompression, 2017. URL https://arxiv.org/abs/1710.01878.\n\nXiaotian Zhu, Wengang Zhou, and Houqiang Li. Improving deep neural network sparsity through decorrelation regularization. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pp. 3264–3270. International Joint Conferences on Artificial Intelligence Organization, 7 2018. doi: 10.24963/ijcai.2018/453. URL https://doi.org/ 10.24963/ijcai.2018/453.\n\nA COMPUTATIONAL RUNTIME COMPARISON\n\nThe training runtime of all pruning methods are compared in Tables 9, 10, and 11. For all experiments, Soft movement has a greatly increased runtime compared to other pruning methods. This is due to the over-pruning problem previously described: if soft movement prunes too many neurons, it must default to hard movement as a backup. To reach a specific pruning percentage, this must occur, resulting in significantly more computation.\n\nGUM is certainly slower than hard movement, however, we find the difference to be minimal. Compared to no pruning, all pruning methods add a significant amount of time to training, especially when also combined with distillation. Therefore, the additional runtime incurred by GUM is small in comparison.\n\nModel\n\nNo Prune\n\nGPT-Neo-125m GPT-Neo-125m + Distil GPT-2-sm GPT-2-sm + Distil GPT-Neo-1.3b GPT-Neo-1.3b + Distil\n\n4.75 -\n6.83 -\n3.92 -\n\nL2 10.5 13.83 11.62 9.28 -\n-\n\nRandom Hard GUM Soft\n\n9.93 13.83 10.06 14.22 11.83 14.5\n\n10.26 13.93 10.07 14.55 12.25 14.8\n\n11.46 15.4 12.83 17.08 12.83 15.56\n\n25.33 31.5 27.5 32.67 -\n-\n\nTable 9: WikiSQL Training Runtime in Hours. GPT-Neo-125m and GPT-2-sm were run on 8xV100 GPUs, while GPT-Neo-1.3b was run on 8xA100 GPUs. All results are averaged over all pruning runs, which are comparable given neuron removal occurs after training.\n\nModel\n\nNo Prune\n\nGPT-Neo-125m GPT-Neo-125m + Distil GPT-2-sm GPT-2-sm + Distil\n\n5.42 -\n4.72 -\n\nL2 8.55 11.9 6.45 11.33\n\nRandom Hard GUM Soft\n\n6.82 11.9 6.17 10.88\n\n7.1 11.8 6.12 11.12\n\n8.87 13.45 9.1 13.48\n\n18.08 23.33 17.13 22.17\n\nTable 10: Wikitext Training Runtime in Hours. GPT-Neo-125m and GPT-2-sm were run on 8xV100 GPUs. All results are averaged over all pruning runs, which are comparable given neuron removal occurs after training.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nNo Prune Random Hard GUM\n\nGPT-Neo-125m GPT-Neo-125m + Distil GPT-2-sm GPT-2-sm + Distil\n\n1.72 -\n1.55 -\n\n2.22 4.45 2.41 3.96\n\n2.37 5.13 2.27 3.93\n\n2.71 5.33 2.79 4.17\n\nTable 11: SAMsum Training Runtime in Hours. GPT-Neo-125m and GPT-2-sm were run on 8xV100 GPUs. All results are averaged over all pruning runs, which are comparable given neuron removal occurs after training.\n\nB E2E NLG CHALLENGE RESULTS\n\nWe also tested pruning on the E2E NLG Challenge Dušek et al. (2020) in Tables 12 and 13. A highly popular dataset, performance in this domain is measured by a variety of metrics that all measure the quality of the output indirectly as opposed to the direct measurement versus ground truth in other experiments.\n\nAfter many rounds of hyperparameter optimization, results for GPT-2-sm loosely follow previously seen trends, however, GPT-Neo-125m results are highly inconsistent. For this model removing even 90% of neurons can result in best performance, and increasing pruning does not monotonically affect performance, two highly concerning phenomenon unseen in other datasets.\n\nWe speculate these inconsistencies could be due to many reasons; such as the open-endedness of the problem domain, or too little training data. Measuring the performance of generative models is a non-trivial task, especially when model outputs are highly similar as is the case when pruning.\n\nFor these reasons, we do not further experiment on this dataset and leave it out of our main analysis. However, we include this experiment to show that in some cases, pruning results can be inconsistent for language models. Further exploration is required into this area.\n\nMethod & Leftover % BLEU 68.22 68.26 68.80 68.15 68.07 68.22 68.26\n\nNo Prune Topv 75% Topv 25% Topv 10% GUM 75% GUM 25% GUM 10%\n\nNIST METEOR ROUGE_L CIDEr 2.3141 8.6315 2.3120 8.6660 2.3238 8.6780 2.2955 8.6377 2.2835 8.5698 2.3230 8.6597 2.3012 8.6684\n\n0.7103 0.7108 0.7126 0.7144 0.7090 0.7093 0.7109\n\n0.4479 0.4486 0.4487 0.4472 0.4458 0.4495 0.4490\n\nTable 12: GPT-Neo-125m: Testing results on the E2E NLG Challenge. Higher is better for all metrics. Even at 10% leftover, performance is similar to the baseline, for both Topv and GUM.\n\nMethod & Leftover % BLEU 68.05 66.79 66.85 66.63 68.02 66.58 67.06\n\nNo Prune Topv 75% Topv 25% Topv 10% GUM 75% GUM 25% GUM 10%\n\nNIST METEOR ROUGE_L CIDEr 2.4500 8.6547 2.4308 8.5064 2.4322 8.5115 2.4368 8.4783 2.4502 8.6552 2.4286 8.4853 2.4417 8.5237\n\n0.7143 0.7119 0.7098 0.7091 0.7134 0.7098 0.7122\n\n0.4623 0.4590 0.4587 0.4588 0.4619 0.4588 0.4601\n\nTable 13: GPT-2-sm: Testing results on the E2E NLG Challenge. Higher is better for all metrics. In general, more pruning results in worse performance, and GUM outperforms Topv.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nC UNIQUENESS REGULARIZATION PER LAYER\n\nThe goal of uniqueness regularization is to punish similarity between neurons, especially those with high similarity. Figure 3 shows that GUM is generally successful in removing neurons with high similarity across all layers.\n\nFigure 3: For each layer, this graph shows the percentage of neurons with at least one similarity per range. Similarity is defined as the absolute value of cosine similarity over the entire validation dataset, increasing from 0 to 1. Topv and GUM are compared, training on WikiSQL with GPT-Neo-125m. Total leftover neurons is exactly 25% of all neurons.\n\nD GLOBAL TOPv REMOVAL PER LAYER\n\nA natural question arises with Global Topv pruning: what is the final prune percentage per-layer? Figure 4 shows the prune percentages per layer for one sample training run. From this test, later layers are clearly prioritized, with a large emphasis on the last layer. While the exact layers pruned more or less will vary with noise, model, and dataset, we observe this trend to generally hold true.\n\nE UNIQUENESS AND SENSITIVITY GRAPHING\n\nTo measure sensitivity for a model, we measure the global sum of sensitivity for all neurons {hi(·)}i∈[m] in feedforward layers {l ∈ L} on the training dataset via (taking the absolute value to account for sign):\n\nΣlΣi|hi(X) ·\n\n∂L ∂hi(X)\n\n|\n\n15\n\n/D\\HU/D\\HU/D\\HU/D\\HU/D\\HU/D\\HU/D\\HU/D\\HU/D\\HU/D\\HU/D\\HU/D\\HU*807RS.Under review as a conference paper at ICLR 2023\n\nFigure 4: The percentage leftover for layers 1-12 after Global Topv pruning, using GUM training on WikiSQL with GPT-Neo-125m. Total leftover neurons is exactly 25% of all neurons.\n\nTo measure uniqueness for a model, we measure the cosine similarity of each neuron with each other neuron as in equation 3 over the entire dataset, measuring each pair only once. This can be measured using the running cosine similarity without a decay multiplier. Then, we measure the percentage of neurons with at least 1 similarity above 0.8 to another neuron (i.e. these neurons nearly match each other) across the entire network.\n\nBoth sensitivity and uniqueness are not useful on their own, but are useful relative to other models. We therefore divide both metrics by the value obtained for the fully finetuned version of the model.\n\nOverall, uniqueness values for Wikitext-103 and for WikiSQL were quite different. Neurons on Wikitext-103 seem to already be highly unique, with some specific layers containing a large amount of redundancy, while WikiSQL has redundancy throughout the entire network.\n\nFigures 5 and 6 show non-re-scaled results for the previous graphs. These graphs show uniqueness as a percent of all neurons (i.e. 50% of neurons are unique) and sensitivity as a raw value.\n\nFigure 5: Sensitivity and Uniqueness measured without re-scaling, for GPT-2-sm.\n\nF TRAINING HYPERPARAMETERS\n\nAll training hyperparameters are provided. Only one random seed was attempted per training run. We note that it is possible different pruning amounts require different hyperparameters and therefore performance could be better, however, searching for each possible combination would be far too\n\n16\n\n6HQVLWLYLW\\8QLTXHQHVV/HIWRYHU:LNL64/6HQVLWLYLW\\8QLTXHQHVV/HIWRYHU:LNLWH[W0DJQLWXGH*805DQGRP6RIW7RSvZR'LVWLOZ'LVWLOUnder review as a conference paper at ICLR 2023\n\nFigure 6: Sensitivity and Uniqueness measured without re-scaling, for GPT-Neo-125m.\n\nexpensive. Therefore, the same hyperparameters are used for different pruning percentages. Only the best combination for each model is listed, but each model had its hyperparameters tuned manually. We explored hyperparameters by starting with known vaules from literature, then performing grid searches on relevant pruning vaules (i.e. distillation temperature, or λgum).\n\nL1 regularization was used on the mask scores for all models. Learning weight decayed linearly to 0 for all models. LR means Learning Rate, WD means Weight Decay, and LS means Label Smoothing. When distillation was used, the teacher was trained using the same hyperparameters (sans pruning arguments).\n\nAdam β1 Adam β2 Adam ε Batch LR Warmup Percent GUM λc 8\n\n0.999\n\n10%\n\n1e-4\n\n0.99\n\n0.9\n\nTable 14: Shared hyperparameters for all runs.\n\nWikiSQL A max token length of 512 was used. Strings were not converted to lowercase. Special tokens were added by the tokenizer.\n\nGPT-Neo-125m used a mask LR of 1e-2 for Hard Movement/GUM and 1e1 for Soft Movement. GPT-2-sm used a mask LR of 1e-2 for Hard Movement/GUM and 1e1 for Soft Movement. Soft movement required a large mask learning rate as it would otherwise not converge - other combinations of more regularization were attempted.\n\nModel\n\nLR WD Epochs\n\nGPT-Neo-125m 5e-4 3e-4 2e-4\n\nGPT-2-sm GPT-Neo-1.3b\n\n0.05 0.1 0.05\n\n10 11 6\n\nλmvp 2\n2 2\n\nλgum 1e2 1e1 1e1\n\nLS\n\n0.05 0.05 0\n\nDistil α Distil Temp.\n\n0.5 0.9 0.5\n\n2 1\n1\n\nTable 15: WikiSQL hyperparameters.\n\nWikitext-103 A max token length of 1024 was used. Strings were not converted to lowercase. Special tokens were added by the tokenizer.\n\nGPT-Neo-125m used a mask LR of 1e-2 for Hard Movement/GUM and 1e1 for Soft Movement. GPT-2-sm used a mask LR of 1e-2 for Hard Movement/GUM and 1e1 for Soft Movement. Soft\n\n17\n\n6HQVLWLYLW\\8QLTXHQHVV/HIWRYHU:LNL64/6HQVLWLYLW\\8QLTXHQHVV/HIWRYHU:LNLWH[W0DJQLWXGH*805DQGRP6RIW7RSvZR'LVWLOZ'LVWLOUnder review as a conference paper at ICLR 2023\n\nmovement required a large mask learning rate as it would otherwise not converge - other combinations of more regularization were attempted.\n\nFor Wikitext-103 specifically, Soft Movement also required an extremely large λmvp. This causes a large increase in pruning at the beginning of training, which could explain the overall poor performance. Without this large value, however, Soft Movement would not converge to the desired pruning amount.\n\nModel\n\nLR WD Epochs\n\nGPT-Neo-125m 5e-4 3e-4\n\nGPT-2-sm\n\n0.05 0.1\n\n3 3\n\nλmvp 2, 1e2 (Soft) 2, 1e2 (Soft)\n\nλgum 1e1 1e1\n\nLS\n\n0.05 0.05\n\nDistil α Distil Temp.\n\n0.9 0.9\n\n1 1\n\nTable 16: Wikitext-103 hyperparameters.\n\nSAMsum A max token length of 1024 was used. Strings were not converted to lowercase, and whitespace was not stripped. More than 3 or 4 epochs starts to result in overtraining for both models, so both were limited to not overtrain.\n\nFor GPT-Neo-125m, both methods used a mask LR of 1e-2. For GPT-2-sm, GUM required a mask LR of 1e-3, while TopK used a mask LR of 1e-2. However, for both models λgum must be 1e1 for no distillation, and 1e2 for distillation, as too high a λgum results in poor non-distil performance. Soft movement was not attempted on this dataset.\n\nModel\n\nLR WD Epochs\n\nGPT-Neo-125m 5e-4 3e-4\n\nGPT-2-sm\n\n0.01 0.05\n\n4 6\n\nλmvp 2\n2\n\nλgum 1e1/1e2 1e1\n\nLS\n\n0.01 0.01\n\nDistil α Distil Temp.\n\n0.5 0.9\n\n1 1\n\nTable 17: SAMsum hyperparameters.\n\nE2E A max token length of 512 was used. When testing, BEAM search was used with 10 beams, a length penalty of .9, and no ngram repeat size of 4.\n\nGPT-Neo-125m used a mask LR of 1e-2 for Hard Movement/GUM and 1e1 for Soft Movement. GPT-2-sm used a mask LR of 1e-2 for Hard Movement/GUM and 1e1 for Soft Movement. Soft movement required a large mask learning rate as it would otherwise not converge - other combinations of more regularization were attempted.\n\nModel\n\nLR WD Epochs\n\nGPT-Neo-125m 5e-4 3e-4\n\nGPT-2-sm\n\n0.01 0.1\n\n6 6\n\nλmvp 2, 1e1 (Soft) 2, 1e2 (Soft)\n\nλgum 1e1 1e1\n\nLS\n\n0.05 0.05\n\nDistil α Distil Temp.\n\n0.5 0.9\n\n2 1\n\nTable 18: E2E NLG Challenge hyperparameters.\n\n18",
    "reference": "# Summary Of The Paper\n\nThe authors are interested in structured pruning of generative language models. In particular, this work builds on top of [Movement Pruning](https://arxiv.org/abs/2005.07683) and [Block movement pruning](https://arxiv.org/abs/2109.04838) to prune entire structures (and not individual weights) in models similar to GPT2 and the fine-tuning stage.\n\nThe authors first notice that current structure pruning applied to decoder only language models on NLG tasks perform relatively similarly at pruning rates between 10% and 50% (percentage of remaining weights), and more surprisingly, similarly to random pruning.\n\nThe authors then introduce two fundamental measures of redundancy called “sensitivity” and “uniqueness” which respectively measure how much a group of parameters impact the training objective and how unique a group of parameters is compared to other groups of parameters. This motivates the introduction of Globally Unique Movement, a method that essentially encourages remaining neurons to be dissimilar (as measured by cosine similarity), by modifying the regularization in movement pruning.\n\n# Strength And Weaknesses\n\nStrengths:\n- The problem is relatively well-motivated and the paper makes a noticeable effort to be didactic.\n- The contributions are somewhat novel, and experiments are well conducted on reasonably large setups.\n- The result about gradual pruning performing similarly to previous state-of-the-art structured pruning is surprising and insightful.\n\nWeaknesses:\n- The numbers showing the superiority of the method are weak or show only weak improved performance or trends.\n- The connection between sensitivity/uniqueness and fine-pruning performance is not well articulated (see questions).\n- I have doubts about whether the choice of benchmarks is the most appropriate. For instance, the authors note that on E2E, pruned models perform better than the non-pruned baseline.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- Presentation suggestions:\n  - Does Top_v throughout the presentation refer to Hard movement pruning in Table 2? If so, it would help the reader to uniformize these short names.\n  - I have found not intuitive to present results with GUM (Figure 1 and 2) before introducing GUM\n- Why is the sensitivity decreasing as the model is pruned? I would have expect that the remaining weights have a greater impact on the final output and thus as you prune, the average sensitivity increases.\n- Section 3.2: you mention a few times that poor sensitivity and uniqueness explains poor benchmark performance. Is it a causality link? A correlation? Could you articulate the intuition behind?\nExamples:\n  - *“Magnitude pruning universally scores worst on both metrics, explaining its poorer performance in all experiments”*\n  - *“which partially explains why distillation improves it significantly”*\n  - *“explaining its superiority across various tasks”*\n- What is the maximum number of epochs you tried for block soft movement pruning? In my experience, this method requires a very slow pruning (5x to 10x more steps than hard weight movement pruning).\n- What are the trends for extreme pruning (i.e. less than 10% leftover) for Table 2, 3 and 4, 5? Does it lead to more unequivocal results?\n- *“Random pruning obtains similar distillation sensitivity and uniqueness, though slightly lower, to hard movement, lending credence to its overall high performance. However, sensitivity is markedly lower without distillation as is reflected in all tables. We point to this as proof that hard movement does not target uniqueness.”* Could you expand on this insight? I have found it to be a very generous conclusion from Figure 1 and 2. Handholding the reader for that conclusion might bring some clarity.\n\n# Summary Of The Review\n\nThis paper extends movement pruning and block movement pruning by introducing a \"uniqueness\" metric in the training objective which encourages the model to prune similar neurons.\nWhile the method and insights are relatively novel, the numbers and comparisons of previous pruning methods are somewhat weak.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nL E A R N I N G T O P O L O G I E S\n\nG R A P H\n\nN E U R A L\n\nN E T W O R K\n\nAnonymous authors Paper under double-blind review\n\nA B S T R A C T\n\nGraph convolutional networks (GCNs) enable end-to-end learning on graph structured data. However, many works begin by assuming a given graph structure. As the ideal graph structure is often unknown, this limits applicability. To address this, we present a novel end-to-end differentiable graph-generator which builds the graph topology on the fly. Our module can be readily integrated into existing pipelines involving graph convolution operations, replacing the predetermined or existing adjacency matrix with one that is learned, and optimised, as part of the general objective. As such it is applicable to any GCN. We show that integrating our module into both node classification and trajectory prediction pipelines improves accuracy across a range of datasets and backbones.\n\n1\n\nI N T R O D U C T I O N\n\nThe success of Graph Neural Networks (GNNs) (Duvenaud et al., 2015; Bronstein et al., 2017; Monti et al., 2017), has led to a surge in the use of graph-based representation learning. GNNs provide an efficient framework to learn from graph-structured data, making them widely applicable in any domain where data can be represented as a relation or interaction system. They have been successfully applied in a wide range of tasks including particle physics (Choma et al., 2018), protein science (Gainza et al., 2020) and many others (Monti et al., 2019), (Stokes et al., 2020).\n\nIn a GNN, each node iteratively updates its state by interacting with its neighbors, typically through message passing. However, a fundamental limitation of such architectures is the assumption that the underlying graph is provided. While node or edge features may be updated during message passing, the graph topology remains fixed, and its choice may be suboptimal for various reasons. For instance, when classifying nodes on a citation network, an edge connecting nodes of different classes can diminish classification accuracy. These edges can degrade performance by causing irrelevant information to be propagated across the graph. When no graph is explicitly provided, one common practice is to generate a k-nearest neighbor (k-NN) graph. In such cases, k is a hyperparameter and tuned to find the model with the best performance. For many applications, fixing k is overly restrictive as the optimal choice of k may vary for each node in the graph. While there has been an emergence of approaches which learn the graph structure for use in downstream GNNs (Zheng et al., 2020; Kazi et al., 2020; Kipf et al., 2018), all of them treat the node degree k as a fixed hyperparameter.\n\nWe propose a general differentiable graph-generator (DGG) module for learning graph topology with or without an initial edge structure. This module can be placed within any graph convolutional network, and jointly optimized with the rest of the network’s parameters, learning topologies which favor the downstream task without hyperparameter selection or indeed any additional training signal. The primary contributions of this paper are as follows:\n\n1. We propose a novel, differentiable graph-generator (DGG) module which jointly optimizes both the neighbourhood size, and the edges that should belong to each neighbourhood. Note that existing approaches (Zheng et al., 2020; Kipf et al., 2018; Kazi et al., 2020) do not allow for learnable neighbourhood sizes.\n\n2. Our DGG module is directly integrable into any pipeline involving graph convolutions, where either the given adjacency matrix is noisy, or is not explicitly provided and must be determined heuristically. In both cases, our DGG generates the adjacency matrix as part of the GNN training and can be trained end-to-end to optimize performance on the downstream task. Should a good graph structure be known, the generated adjacency matrix can be learned to remain close to it while optimizing performance.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n3. To demonstrate the power of the approach, we integrate our DGG within a range of SOTA pipelines — without modification — across different datasets in trajectory prediction and node classification and demonstrate improvements in model accuracy.\n\n2 R E L AT E D W O R K Graph Representation Learning: GNNs (Bronstein et al., 2017) provide a powerful class of neural architectures for modelling data which can be represented as a set of nodes and relations (edges). Most use message-passing to build node representations by aggregating neighborhood information. A common formulation is the Graph Convolution Network (GCNs) which generalizes the convolution operation to graphs (Kipf & Welling, 2017; Defferrard et al., 2016; Wu et al., 2018; Hamilton et al., 2017). More recently, the Graph Attention Network (GAT) (Veliˇckovi ́c et al., 2018) utilizes a self-attention mechanism to aggregate neighborhood information. However, these works assumed that the underlying graph structure is predetermined, with the graph convolutions learning features that describe preexisting nodes and edges. In contrast, we simultaneously learn the graph structure while using our generated adjacency matrix in downstream graph convolutions. The generated graph topology of our module is jointly optimized alongside other network parameters with feedback signals from the downstream task.\n\nGraph Structure Learning: In many applications, the optimal graph is unknown, and a graph is constructed before training a GNN. One question to ask is: “Why isn’t a fully-connected graph suitable?” Constructing adjacency matrices weighted by distance or even an attention mechanism (Veliˇckovi ́c et al., 2018) over a fully-connected graph incorporates many task-irrelevant edges, even if their weights are small. While an attention mechanism can zero these out — i.e., discover a subgraph within the complete graph — discovering this subgraph is challenging given the combinatorial complexity of graphs. A common remedy is to sparsify a complete graph by selecting the k-nearest neighbors (k-NN). Although this can prevent the propagation of irrelevant information between nodes, the topology of the constructed graph may have no relation to the downstream task. Not only can irrelevant edges still exist, but pairs of relevant nodes may remain unconnected and can lead GCNs to learn representations with poor generalization (Zheng et al., 2020).\n\nThis limitation has led to works which learn a graph’s structure within a deep learning framework. Some methods (Shi et al., 2019; Liu et al., 2020) take a fixed adjacency matrix as input and then learn a residual mask over it. Since these methods directly optimize the residual adjacency by treating each element as a learnable parameter, the learned adjacency matrix is not linked to the representation space and only works in tasks where the training nodes are the same as that at test time. To overcome this, recent approaches (Zheng et al., 2020; Kipf et al., 2018; Luo et al., 2021; Kazi et al., 2020) generate a graph structure by sampling from discrete distributions. As discrete sampling is not directly optimizable using gradient descent, these methods use the Gumbel-Softmax reparameterization trick (Jang et al., 2016) to generate differentiable graph samples. The Gumbel-Softmax approximates an argmax over the edges for each node, and sampling in these approaches is typically performed k times to obtain the top-k edges. Here, k is a specified hyperparameter that controls the node degree for the entire graph/dataset. Unlike these works, we generate edge samples by selecting the top-k in a differentiable manner, where we learn a distribution over the edges and over the node degree k. This allows the neighborhood and its size to be individually selected for each node. Additionally, a known ‘ideal’ graph structure can be used as intermediate supervision to further constrain the latent space.\n\n3 M E T H O D In this section, we provide details of our differentiable graph generation (DGG) module. We begin with notation and the statistical learning framework guiding its design, before describing the module, and how it is combined with graph convolutional backbone architectures.\n\nNotation We represent a graph of N nodes as G = (V, E): where V is the set of nodes or vertices, and E the edge set. A graph’s structure can be described by its adjacency matrix A, with aij = 1 if an edge connects nodes i and j and aij = 0 otherwise. This binary adjacency matrix A is directed, and potentially asymmetrical.\n\nProblem definition. We reformulate the baseline prediction task based on a fixed graph with an adaptive variant where the graph is learned. Typically, such baseline tasks make learned predictions Y given a set of input features X and a graph structure A of node degree k:\n\nY = Qφ(X, A),\n\n(1)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: (Left) A typical prediction task using graphs Y = Qφ(X, A) where A and k are predetermined. (Right) Our reformulation P (Y |X) ≈ (cid:80) k Qφ(X, A)Qθ(A|X, k)Qρ(k|X) which learns a distribution over A and k alongside the downstream task.\n\n(cid:80)\n\nA\n\nwhere Qφ is an end-to-end neural network parameterized by learnable weights φ. These formulations require a predetermined graph-structure A, typically based on choice of node degree k, and take A as additional input to the model. In contrast, we learn both A and k in an end-to-end manner, and use them to make predictions Y . As graphs are inherently binary, with edges either present or absent, they are not directly optimizable using gradient descent. Instead, we consider a distribution of graphs, G, which then induce a distribution of labels, Y, in the downstream task. This distribution takes the factorized form:\n\nP (Y |X) =\n\n(cid:88)\n\n(cid:88)\n\nQφ(X, A)P (A|X, k)P (k|X),\n\n(2)\n\nA∈G\n\nk∈N|V |\n\nwhere P (k|X) is the distribution of node degree k given X (i.e., the choice of k in k−NN), P (A|X, k) the distribution of graph structures A conditioned on the learned k and input X, and P (Y |X) is the downstream distribution of labels conditioned on data X. For clarity, the adjacency A represents a subgraph of a complete graph over X, and k is a multidimensional variable controlling the number of top-k neighbors for each node individually. To avoid learning individual probabilities for each possible graph A in an exponential state space, we further assume that P (A|X, k) has a factorized distribution where each neighborhood is sampled independently, i.e. P (A|X, k) = (cid:81)\n\ni∈V P (ai|X, k).\n\nWe approximate the distributions over adjacencies A and k with tractable functions:\n\nP (Y |X) ≈\n\n(cid:88)\n\n(cid:88)\n\nA\n\nk\n\nQφ(X, A)Qθ(A|X, k)Qρ(k|X),\n\n(3)\n\nwhere Qθ and Qρ are functions parameterized by θ and ρ to approximate P (A|X, k) and P (k|X), respectively. In Fig. 1, we illustrate the functions of our method compared to the typical prediction task in Eq. 1.\n\nUsing this formulation, we train the entire system end-to-end to minimize the expected loss when sampling Y . This can be efficiently performed using stochastic gradient descent. In the forward pass, we first sample a subgraph/set of nodes X from the space of datapoints, and conditioning on X we sample A and compute the associated label Y . When computing the gradient step, we update Qφ(X, A) as normal and update the distributions using two standard reparametrization tricks: one for discrete variables (Jang et al., 2016) such that Qθ(A|X, k) can generate differentiable graph samples A′, and another for continuous variables (Kingma & Welling, 2013) of k′ drawn from Qρ(k|X):\n\nP (Y |X) ≈\n\n(cid:88)\n\n(cid:88)\n\nA′\n\nk′\n\nQφ(X, A′), where A′ ∼ Qθ(A|X, k′) and k′ ∼ Qρ(k|X).\n\n(4)\n\nAs both the graph structure A′ and variable k′ samplers are differentiable, our DGG module can be readily integrated into pipelines involving graph convolutions and jointly trained end-to-end.\n\n3 . 1 D I F F E R E N T I A B L E G R A P H G E N E R AT I O N Our differentiable graph-generator (DGG) takes a set of nodes V = {v1, ..., vN } with d-dimensional features X ∈ RN ×d and generates an asymmetric adjacency matrix A ∈ RN ×N . This adjacency matrix can be used directly in any downstream graph convolution operation (see Module Instatiation below). As illustrated by Fig. 2, the DGG module consists of four components:\n\n1. Encoder: this component projects the input node features X ∈ RN ×d to a latent representation\n\nˆX ∈ RN ×d′\n\n, and forms the primary representation space for the model.\n\n3\n\nTYPICAL PREDICTION TASKOUR REFORMULATIONDifferentiable pathwayNon-differentiable pathwayDifferentiable func.Non-differentiable func.KEYUnder review as a conference paper at ICLR 2023\n\nFigure 2: Our differentiable graph-generator (DGG) takes input nodes X and generates an adjacency matrix A. It consists of: (1) Degree-estimator: generates samples of ki for each node, (2) Edgeranker: generates edge samples ei for each node and (3) Top-k selector: takes ki and edge samples ei and selects top-k elements in a differentiable manner to output a final adjacency A. 2. Edge ranking: this takes the latent node features ˆX ∈ RN ×d′\n\nand generates a matrix representing a stochastic ordering of edges E ∈ RN ×N drawn from a learned distribution over the edgeprobabilities (A′ ∼ Qθ(A|X, k′) from Eq. 4).\n\n3. Degree estimation: this component estimates the number of neighbors each individual node is and generates random samples\n\nconnected to. It takes as input the latent node features ˆX ∈ RN ×d′ k ∈ RN drawn from a learned distribution over the node degree (k′ ∼ Qρ(k|X) from Eq. 4). 4. Differentiable top-k edge selector: takes k and the edge-samples e and performs a soft thresholding that probabilistically selects the most important elements, based on the output of the Edge-ranking to output an adjacency matrix A ∈ RN ×N .\n\nWe describe each component below. Encoder. We construct a single latent space from the input node features, and use it for edge ranking and degree estimation. We first map input node features X ∈ RN ×d into latent features ˆX ∈ RN ×d′ using a multi-layer perceptron (MLP) fφ with weights φ: ˆX = fφ(X). These latent features form the input for the rest of the module. Furthermore, they are output by the DGG and passed to the GCN downstream to prevent vanishing gradients.\n\nEdge ranking. The edge ranking returns an implicit distribution of edge orderings, from which we sample the neighborhood for each node. For each node vi, it draws a set of scores ei = {eij}N j\nquantifying its relevance to all nodes vj ∈ V , including itself. To generate differentiable edge samples ei, we use the Gumbel-Softmax (Jang et al., 2016). Without loss of generality, we focus on a single node vi ∈ V , with latent features ˆxi ∈ Rd. We implement the approximation function Qθ(A|X, k) of the Edge-ranker as follows:\n\n1. Using latent node features ˆxi ∈ ˆX, calculate pairwise edge probabilities pi ∈ RN between pairs\n\nof auxiliary node features (ˆxi, ˆxj):\n\npi = {exp(−||ˆcij||1)|∀j ∈ N }, where ˆcij = ˆxi − ˆxj is the difference between node features. Each element pij ∈ pi represents a similarity measure between the latent features of node vi and vj. In practice, any distance measure can be used here, including learnable approaches.\n\n(5)\n\n2. Using Gumbel-Softmax over the edge probabilities pi ∈ RN , we generate differentiable samples\n\nei ∈ RN with Gumbel noise g:\n\n(cid:40)\n\nei =\n\nexp((log(pij) + gi) + τ ) j exp((log(pij) + gi) + τ )\n\n(cid:80)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:41)\n\n∀j ∈ N\n\n, g ∼ Gumbel(0, 1)\n\n(6)\n\nwhere τ is a temperature hyperparameter controlling the interpolation between a discrete one-hot categorical distribution and a continuous categorical density. When τ → 0, the edge energies eij ∈ ei approach a degenerate distribution. The temperature τ is important for inducing sparsity, but given the exponential function, this results in a single element in ei given much more weighting than the rest, i.e., it approaches a one-hot argmax over ei. Instead, we want a variable number of edges to be given higher importance, and others to be close to zero. Hence, we select a higher temperature and use the top-k selection procedure (detailed below) to induce sparsity. This has the added benefit of avoiding the high-variance gradients induced by lower temperatures.\n\n4\n\nDEGREE ESTIMATOREDGE RANKERDIFF. TOP-KSELECTORSAMPLESAMPLEINPUT NODESADJACENCY MATRIXUnder review as a conference paper at ICLR 2023\n\nDegree estimation. A key limitation of existing graph generation methods (Kazi et al., 2020; Kipf et al., 2018; Zheng et al., 2020) is their use of a fixed node degree k across the entire graph. This can be suboptimal as mentioned previously. In our approach, rather than fixing k for the entire graph, we sample it per node from a learned distribution. Focusing on a single node as before, the approximation function Qρ(k|X) of the Degree-estimator works as follows:\n\n1. We approximate the distribution of latent node features ˆxi ∈ Rd following the a VAE-like formulation (Kingma & Welling, 2013). We encode its mean μi ∈ Rd and variance σi ∈ Rd using two MLPs Mρ and Sρ, and then reparametrize with noise ε to obtain latent variable zi ∈ Rd:\n\nμi, σi = Mρ( ˆxi), Sρ( ˆxi),\n\nzi = μi + εiσi, εi ∼ N (0, 1).\n\n(7)\n\n2. Finally, we concatenate each latent variable zi ∈ Rd with the L1-norm of the edge samples hi = ||eij||1 and decode it into a scalar ki ∈ R with another MLP Dρ, representing a continuous relaxation of the neighborhood size for node vi:\n\nki = Dρ(zi + hi).\n\n(8)\n\nSince hi is a summation of a node’s edge probabilities, it can be understood as representing an initial estimate of the node degree which is then improved by combining with a second node representation zi based entirely on the node’s features. Using the edge samples to estimate the node degree links these representation spaces back to the primary latent space of node features ˆX.\n\nTop-k Edge-Selector. Having sampled edge weights, and node degrees k, this function selects the top-k edges for each node. The top-k operation, i.e. finding the indices corresponding to the k largest elements in a set of values, is a piecewise constant function and cannot be directly used in gradient-based optimization. Previous work (Xie et al., 2020) frames the top-k operation as an optimal transport problem, providing a smoothed top-k approximator. However, as their function is only defined for discrete values of k it cannot be optimized with gradient descent. As an alternative that is differentiable with respect to k, we relax the discrete constraint on k, and instead use it to control the x-axis value of the inflection point on a smoothed-Heaviside function (Fig. 3). For a node vi ∈ V , of smoothed degree ki ∈ R and edges ei ∈ RN , our Top-k Edge Selector outputs an adjacency vector ai ∈ RN where the k largest elements from ei are close to 1, and the rest close to 0. Focusing on a single node vi as before, the implementation is as follows:\n\n1. Draw 1D input points di = {1, ..., N } where N is the number of nodes in V .\n\n2. Pass di through a hyperbolic tangent (tanh) which serves as a smooth approximation of the\n\nHeaviside function:\n\nhi = 1 − 0.5 ∗ (cid:8)1 + tanh(λ−1di − λ−1ki)(cid:9) , here λ > 0 is a temperature parameter controlling the gradient of the function’s inflection point. As λ → 0, the smooth function approaches the Heaviside step function. The first-k values in hi = {hij}N\n\nj will now be closer to 1, while the rest closer to 0.\n\n(9)\n\n3. Finally, for each node i we sort its edge-energies ei = {eij}N\n\nin descending order, multiply by j and then restore the original order to obtain the final adjacency vector ai = {aij}N j .\n\nhi = {hij}N Stacking ai over all nodes vi ∈ V then creates the final adjacency matrix A ∈ RN ×N .\n\nj\n\nStraight through Top-k Edge Selector. To make our final adjacency matrix A ∈ RN ×N discrete, we follow the trick used in the Straight-Through Gumbel Softmax (Jang et al., 2016): we output the discretized version of A in the forward pass and the continuous version in the backwards pass. For the discretized version in the forward pass, we simply replace the smooth-Heaviside function in Eq. 9 with a step function.\n\nModule Instantiation: The DGG module can be easily combined with any graph convolution operation. A typical graph convolution (Kipf & Welling, 2017) is defined as follows: X′ = ˆD−1/2 ˆA ˆD−1/2XΘ. Here, ˆA = A + I denotes the adjacency matrix with inserted self-loops, ˆD its diagonal degree matrix and Θ its weights. To use this graph convolution with the DGG, we simply use our module to generate the adjacency matrix ˆA.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: The differentiable Top-k Edge Selector. This component uses the node degree ki output by the Degree Estimator to control the inflection point on a smooth-Heaviside function and uses it to select the top edges from ei output by the Edge Ranker. This produces an adjacency vector ai for each node, and stacking ai across all nodes produces the final adjacency matrix A.\n\n4 E X P E R I M E N T S\n\nWe evaluate our DGG on node classification and trajectory prediction tasks. We chose these tasks as the former has an input graph structure, while the latter does not, demonstrating the flexibility of our module. Furthermore, we integrate our module into a variety of GCN baselines, not only for their state-of-the-art (SOTA) performance, but as they all use different graph convolution operations.\n\n4 . 1 N O D E C L A S S I F I C AT I O N Beginning with node classification, we conduct ablations examining the behaviour of different parts of the DGG, followed by comparisons to other state-of-the-art graph-topology learning approaches. In the appendix we include experiments investigating the effect of the DGG on downstream models under the addition of noisy edges to input graphs. We perform these experiments under both transductive and inductive scenarios, as well as semi-supervised and fully-supervised settings.\n\nTransductive datasets. We evaluate on the three citation benchmark datasets Cora, Citeseer and Pubmed (NLM 2022) introduced by Yang et al. (2016). Each citation graph contains nodes corresponding to documents and edges indicating citations. Node features are bag-of-words features and node labels are categorized by topic. We follow the semi-supervised setting in Yang et al. (2016) and Kipf & Welling (2017) along with their train/test splits. Inductive datasets. In an inductive setting, we evaluate our approach on three datasets: 1. Flickr (Zeng et al., 2020) — categorizing images based on their descriptions; 2. Reddit (Zeng et al., 2020) — predicting the communities of online posts from user comments. 3. PPI (Hamilton et al., 2017) — classifying protein-protein interactions. Further dataset details can be found in the appendix.\n\nBaselines. We integrate our DGG into four representative GNN backbones: GCN (Kipf & Welling, 2017), GraphSage (Hamilton et al., 2017), GAT (Veliˇckovi ́c et al., 2018) and GCNII (Chen et al., 2020). On these backbones, we compare the effectiveness of DGG against other state-of-the-art graph sampling and sparsification methods: DropEdge (Rong et al., 2020), NeuralSparse (Zheng et al., 2020) and PTDNet (Luo et al., 2021).\n\nImplementation details. We integrate our DGG into the official publicly available code of all baselines, without architectural modification. Models are trained and evaluated as their original implementation. See appendix for further details.\n\nTraining details. A node classification model partitions the latent space of node embeddings into separate classes. However, when message-passing, there is one phenomenon of the input graph that can limit classification accuracy: two nodes with different classes but similar features and an edge connecting them. Classifying these nodes is challenging as their feature similarity can be compounded by passing messages between them. The goal of the DGG is to move such nodes apart in the latent space such that there is no edge and communication between them. However, traversing the loss landscape from the initial random initialization of the network to one where the model is able to discriminate between these nodes can take several iterations using only the downstream classification loss. To speed up training, we add an intermediate loss to further partition the latent space. We do this by supervising the adjacency matrix generated by the DGG to remove all edges between classes and only maintain those within a class. We then anneal this loss over the training cycle, eventually leaving only the downstream classification loss. We provide more details in the appendix.\n\n6\n\nSMOOTH HEAVISIDESORTMULT.RESTOREUnder review as a conference paper at ICLR 2023\n\n4 . 1 . 1 A B L AT I O N S In Table 1, we explore the effect of disabling different components of our DGG module when integrated into a GCN (Kipf & Welling, 2017) for node classification: 1. DGG without Degree Estimation and Differentiable Top-k Edge Selection — we remove the Degree Estimator and instead fixe k to select the top-k stochastically ordered edges. 2. DGG with deterministic Edge Ranking — we remove the noise in Eq. 6 of the Edge Ranker. 3. DGG with deterministic Degree Estimation — we remove the noise in Eq. 7 of the Degree Estimator. We perform this under the transductive, semi-supervised setting on Cora and omit the annealed intermediate loss during training.\n\nTable 1: Ablation study. DGG integrated into a GCN for node classification on Cora.\n\nModel Fixed node degree, k = {1, 5, 10, 100} With deterministic Edge Ranking and Degree Estimation With deterministic Edge Ranking With deterministic Degree Estimation DGG\n\nAccuracy {49.7, 78.9, 55.0, 37.0} 82.4 82.7 82.8 83.2\n\nTable 1 shows the benefit of learning a distribution over the node degree. When learning it deterministically, the accuracy decreases by 0.5%. This becomes significantly worse when the node degree is fixed for the entire graph rather than learned per node. Note also, the sensitivity with respect to choice of k. A fixed node degree of k = 10 or k = 1 reduces accuracy by almost 30% vs a graph of 5. This is due to the graph convolution operation: as it has no adaptive weighting mechanism for a node’s neighborhood, each of the neighbors is given the same weight. Naturally, this leads to information sharing between unrelated nodes, reducing the quality of node representation after message-passing. In contrast, by learning a distribution over the node degree we are able to select only the most relevant neighbors, even though these are then weighted equally in the graph convolution. Finally, the inclusion of noise in any of the DGG components does increase accuracy, but only by about 0.5% — demonstrating both its benefit and the robustness of the DGG without it.\n\n4 . 1 . 2 C O M PA R I S O N T O S TAT E - O F - T H E - A R T In Table 2 we compare our method to DropEdge (Rong et al., 2020)), which randomly sparsifies the input graph, and those which learn better graph structures (NeuralSparse (Zheng et al., 2020) and PTDNet Luo et al. (2021)). For fair comparison with the literature, we present two versions of our method: DGG-wl trained with the downstream loss only and DGG* trained with both the downstream and intermediate loss.\n\nDGG improves performance over the original model across all baselines and datasets. Against other approaches, DGG-wl generally outperforms the state-of-the-art NeuralSparse and PTDNet-wl (both trained with only the downstream loss). The accuracy difference can be attributed to our method for modelling sparsity, which explicitly lets each node to select the size of its neighborhood based on the downstream training signal. This training signal helps partition the node representation space, while the estimated node-degree additionally prevents communication between distant nodes. Although PTDNet-wl does this implicitly through its attention mechanism, discovering this sparse subgraph of the input graph is challenging given its complexity. NeuralSparse on the other hand selects k for its entire generated subgraph, which is both suboptimal and requires additional hyperparameter tuning.\n\nComparing methods which enforce additional constraints on the adjacency matrix, DGG* demonstrates larger accuracy gains than PTDNet*. PTDNet* regularizes its adjacency matrix to be of low-rank, as previous work (Savas & Dhillon, 2011) has shown that the rank of an adjacency matrix can reflect the number of clusters. This regularizer reasons about the graph’s topology globally. While this may aid generalization, the accuracy difference may then be attributed to our intermediate loss providing stronger signals to discriminate between nodes with similar features but different classes (and therefore remove the edges between them). Furthermore, their regularizer uses the sum of the top-k singular values during training, where k again is a hyperparameter tuned to each dataset individually. Our method requires no additional parameters to be chosen.\n\nWe observe that there may be cases where tuning the node degree k can assist accuracy, as seen by NeuralSparse’s performance on Reddit. However this requires a hyperparameter search over Reddit’s large graph, and is ultimately outperformed when intermediate supervision is applied to the DGG’s adjacency matrix (shown by the DGG* method).\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Semi-supervised node classification compared to other SOTA graph-topology learning methods. We compare against prior methods reported in (Luo et al., 2021; Zheng et al., 2020; Chen et al., 2020), using the official results where available.\n\nGCN\n\nBackbone Method Original DropEdge NeuralSparse PTDNet-wl DGG-wl PTDNet-wl + low rank DGG*\n\nGraphSage Original\n\nDropEdge NeuralSparse PTDNet-wl DGG-wl PTDNet-wl + low rank DGG* Original DropEdge NeuralSparse PTDNet-wl DGG-wl PTDNet-wl + low rank DGG* Original DropEdge DGG-wl DGG*\n\nGAT\n\nGCNII\n\nCora Citeseer Pubmed Reddit 81.1 80.9 82.1 82.4 82.7 82.8 83.9 79.2 78.7 79.3 79.4 79.4 80.3 80.4 83.0 83.2 83.4 83.7 84.2 84.4 85.1 85.3 84.9 86.8 87.7\n\n79.0 78.5 78.8 79.1 80.1 79.8 83.9 76.7 74.8 75.1 77.0 77.6 77.1 80.1 79.0 77.9 78.0 79.2 79.5 79.3 81.8 80.2 79.4 81.2 81.9\n\n70.3 72.2 71.5 71.7 72.4 72.7 74.9 67.6 67.0 67.4 67.8 68.2 67.9 70.6 72.1 70.9 72.4 72.3 73.0 73.7 76.3 73.2 73.4 74.4 75.8\n\n92.2 96.1 96.6 -\n96.5 -\n97.2 93.8 96.3 96.7 -\n96.7 -\n96.9 -\n- -\n- -\n- -\n- -\n- -\n\nPPI 53.2 54.8 65.1 75.2 76.7 80.3 81.5 61.8 61.0 62.6 64.5 65.1 64.8 67.3 97.3 85.1 92.1 97.8 97.3 98.0 97.5 99.5 99.0 99.5 99.7\n\nTable 3: Adjacency matrix constraints: our intermediate annealed loss vs. PTDNet’s low rank regularizer Luo et al. (2021) for semi-supervised node classification with a GCN backbone.\n\nMethod Ours-wl Ours-wl + low rank Ours-wl + int. loss Ours-wl + int. loss + low rank\n\nCora Citeseer Pubmed Reddit PPI 76.7 80.1 82.7 80.6 81.0 83.3 81.5 83.9 83.9 81.8 84.2 84.0\n\n96.1 96.1 97.2 97.2\n\n72.4 73.1 74.9 75.1\n\nFinally in Table 3 we compare the low-rank constraint of PTDNet with our intermediate annealed loss. Our intermediate loss (‘Ours-wl + int. loss’) outperforms the low-rank constraint (‘Ours-wl + low rank’). However, using both constraints (‘Ours-wl + int. loss + low rank’) increases classification accuracy further, suggesting the edges removed by both methods are complementary.\n\n4 . 2 T R A J E C T O R Y P R E D I C T I O N We consider four datasets covering a range of scenarios from baseketball to crowded urban environments. On each, we integrate our DGG into a state-of-the-art trajectory prediction method and compare results to another state-of-the-art graph-topology learning approach DGM (Kazi et al., 2020).\n\nDatasets. We evaluate on four trajectory prediction benchmarks. 1. ETH (Pellegrini et al., 2009) and UCY (Lerner et al., 2007) — 5 subsets of widely used real-world pedestrian trajectories . 2. STATS SportVU (SportVU) — multiple NBA seasons tracking trajectories of basketball players over a game. Stanford Drone Dataset (SDD) (Robicquet et al., 2016) — top-down scenes across multiple areas at Stanford University, consisting of different agents from pedestrians to cars. Further details on these datasets can be found in the appendix.\n\nBaselines. We integrate our DGG module into two state-of-the-art trajectory prediction pipelines: Social-STGCNN (Mohamed et al., 2020) built upon a spatio-temporal convolutional network using graphs to represent pedestrian trajectories and DAGNet (Monti et al., 2021) built upon a VAE\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: ADE/FDE metrics on the ETH & UCY datasets using Social-STGCNN. For DGM, k = 2.\n\nDataset\n\nOriginal\n\nDGM\n\nDGM Gain\n\nOurs\n\nOur Gain\n\nADE↓\n\nFDE ↓ ADE ↓\n\nFDE ↓ ADE ↑\n\nFDE↑ ADE ↓\n\nFDE ↓ ADE ↑\n\nFDE ↑\n\nETH Hotel Univ Zara1 Zara2 Mean\n\n0.64 0.49 0.44 0.34 0.30 0.44\n\n1.11 0.85 0.79 0.53 0.48 0.75\n\n0.62 0.42 0.41 0.33 0.29 0.41\n\n1.04 0.69 0.76 0.46 0.43 0.68\n\n6.4% 2.4% 14.2% 18.9% 6.2% 3.5% 3.8% 13.7% 5.0% 5.0% 6.3% 10.6%\n\n0.60 0.38 0.40 0.32 0.27 0.39\n\n0.89 0.54 0.70 0.42 0.40 0.59\n\n6.3% 20% 22.4% 36.5% 9.1% 11.3% 5.9% 20.8% 10.0% 16.7% 11.4% 21.3%\n\nTable 5: ADE/FDE metrics on the SportVU Basketball dataset using DAGNet. For DGM, k = 3.\n\nSplit\n\nTeam ADE FDE ADE FDE ADE\n\nOriginal\n\nDGM\n\nDGM Gain FDE\n\nOurs\n\nOur Gain\n\nADE FDE ADE\n\nFDE\n\n10-40 ATK DEF 20-30 ATK DEF 40-10 ATK DEF\n\nMean\n\n2.74 2.09 2.03 1.53 0.81 0.72 1.65\n\n4.29 2.97 3.98 3.07 1.71 1.49 2.92\n\n2.75 2.10 2.03 1.53 0.80 0.71 1.7\n\n4.30 2.97 3.98 3.06 1.69 1.48 2.9\n\n-0.4% -0.2% 2.62 -0.5% -0.1% 1.95 0.1% 0.1% 1.92 0.2% 0.3% 1.24 1.3% 0.9% 0.70 0.8% 0.8% 0.66 0.3% 0.3% 1.51\n\n4.11 2.87 3.75 2.56 1.48 1.29 2.68\n\n4.4% 4.2% 6.8% 3.3% 5.6% 5.8% 18.7% 16.6% 13.6% 13.5% 8.3% 13.4% 9.6% 9.5%\n\nbackbone (Kingma & Welling, 2013) with a graph attention network modelling agent interactions across a fully-connected graph. Our DGG is placed within both networks to generate the adjacency matrix on the fly and forms part of its forward and backward pass. To integrate the DGG with DAGNet’s attention mechanism, the adjacency generated is multiplied by the attention weights.\n\nImplementation details. We integrate DGG into the publicly available code of each method, without any architectural modification. We use the same DGG hyperparameters as for node classification except the intermediate loss is disabled and the training signal is entirely from the downstream task. Evaluation metrics. Model performance is measured with Average Displacement Error (ADE) and Final Displacement Error (FDE). ADE measures the average Euclidean distance along the entire predicted trajectory, while the FDE is that of the last timestep only.\n\n4 . 2 . 1 R E S U LT S In Table 4, the integration of our DGG into Social-STGCNN reduces ADE/FDE compared to both the baseline and the integration of DGM. In Table 5 and 6 our DGG displays similar gains over DGM when integrated into DAGNet. First, this shows the benefit of inducing sparsity when message-passing over a distance weighted adjacency matrix like Social-STGCNN or even an attention-mechanism like DAGNet. The larger error reduction of our DGG compared to DGM may be attributed to DGM’s use of a fixed node-degree k across its learned graph. While this can prevent the propagation of irrelevant information across the graph in some cases, in others it might limit the context available to certain nodes. On the other hand, trying to discover the subgraph entirely through attention makes optimization a challenge. Instead, constraining the model by allowing each node to select its neighborhood and size eases optimization and can still be done entirely from the downstream training signal. We provide further qualitative analysis for these results in the appendix.\n\n5 C O N C L U S I O N\n\nWe have presented a novel approach for learning graph topologies, and shown how it obtains stateof-the-art performance across multiple baselines and datasets for node classification and trajectory prediction. The principal advantage of our approach is that it can be combined with any existing approach using graph convolutions on an automatically generated graph, such as a k−nearest neighbor graph.\n\nTable 6: ADE/FDE metrics on the Stanford Drone Dataset using DAGNet. For DGM, k = 2.\n\nSplit\n\nOriginal\n\nDGM\n\nDGM Gain\n\nOurs\n\nOur Gain\n\nADE FDE ADE FDE ADE\n\nFDE ADE FDE\n\nADE\n\nFDE\n\n8-12\n\n0.53\n\n1.04\n\n0.52\n\n1.01\n\n1.9% 3.0% 0.48\n\n0.96\n\n10.4% 8.3%\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nR E F E R E N C E S\n\nAlexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. Social lstm: Human trajectory prediction in crowded spaces. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 961–971, 2016.\n\nMichael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017.\n\nMing Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International Conference on Machine Learning, pp. 1725–1735. PMLR, 2020.\n\nNicholas Choma, Federico Monti, Lisa Gerhardt, Tomasz Palczewski, Zahra Ronaghi, Prabhat Prabhat, Wahid Bhimji, Michael M Bronstein, Spencer R Klein, and Joan Bruna. Graph neural networks for icecube signal classification. In 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), pp. 386–391. IEEE, 2018.\n\nMicha ̈el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems, 29: 3844–3852, 2016.\n\nDavid K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al ́an Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. Advances in neural information processing systems, 28, 2015.\n\nPablo Gainza, Freyr Sverrisson, Frederico Monti, Emanuele Rodola, D Boscaini, MM Bronstein, and BE Correia. Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. Nature Methods, 17(2):184–192, 2020.\n\nAgrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2255–2264, 2018.\n\nWilliam L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025–1035, 2017.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv\n\npreprint arXiv:1611.01144, 2016.\n\nAnees Kazi, Luca Cosmo, Nassir Navab, and Michael Bronstein. Differentiable graph module (dgm)\n\nfor graph convolutional networks. arXiv preprint arXiv:2002.04999, 2020.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nThomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In International Conference on Machine Learning, pp. 2688–2697. PMLR, 2018.\n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 2426, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview. net/forum?id=SJU4ayYgl.\n\nAlon Lerner, Yiorgos Chrysanthou, and Dani Lischinski. Crowds by example. In Computer graphics\n\nforum, volume 26, pp. 655–664. Wiley Online Library, 2007.\n\nZiyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang, and Wanli Ouyang. Disentangling and unifying graph convolutions for skeleton-based action recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 143–152, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen, and Xiang Zhang. Learning to drop: Robust graph neural network via topological denoising. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining, pp. 779–787, 2021.\n\nAbduallah Mohamed, Kun Qian, Mohamed Elhoseiny, and Christian Claudel. Social-stgcnn: A social spatio-temporal graph convolutional neural network for human trajectory prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14424–14432, 2020.\n\nAlessio Monti, Alessia Bertugli, Simone Calderara, and Rita Cucchiara. Dag-net: Double attentive graph neural network for trajectory forecasting. In 2020 25th International Conference on Pattern Recognition (ICPR), pp. 2551–2558. IEEE, 2021.\n\nFederico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5115–5124, 2017.\n\nFederico Monti, Fabrizio Frasca, Davide Eynard, Damon Mannion, and Michael M Bronstein. Fake news detection on social media using geometric deep learning. arXiv preprint arXiv:1902.06673, 2019.\n\nNLM 2022. Pubmed. courtesy of the us national library of medicine.\n\nStefano Pellegrini, Andreas Ess, Konrad Schindler, and Luc Van Gool. You’ll never walk alone: Modeling social behavior for multi-target tracking. In 2009 IEEE 12th international conference on computer vision, pp. 261–268. IEEE, 2009.\n\nAlexandre Robicquet, Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Learning social etiquette: Human trajectory understanding in crowded scenes. In European conference on computer vision, pp. 549–565. Springer, 2016.\n\nYu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Hkx1qkrKPr.\n\nTim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamicallyfeasible trajectory forecasting with heterogeneous data. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVIII 16, pp. 683–700. Springer, 2020.\n\nBerkant Savas and Inderjit S Dhillon. Clustered low rank approximation of graphs in information science applications. In Proceedings of the 2011 SIAM International Conference on Data Mining, pp. 164–175. SIAM, 2011.\n\nLei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Two-stream adaptive graph convolutional networks for skeleton-based action recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12026–12035, 2019.\n\nSportVU.\n\nSportvu basketball, Sep 2019. URL https://www.statsperform.com/\n\nteam-performance/basketball/optical-tracking/.\n\nJonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al. A deep learning approach to antibiotic discovery. Cell, 180(4):688–702, 2020.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.\n\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513–530, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, and Tomas Pfister. Differentiable top-k with optimal transport. Advances in Neural Information Processing Systems, 33:20520–20531, 2020.\n\nZhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pp. 40–48. PMLR, 2016.\n\nHanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. GraphSAINT: Graph sampling based inductive learning method. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=BJe8pkHFwS.\n\nCheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. Robust graph representation learning via neural sparsification. In International Conference on Machine Learning, pp. 11458–11468. PMLR, 2020.\n\nA N O D E C L A S S I F I C AT I O N E X P E R I M E N T S\n\nA . 1 D ATA S E T D E TA I L S\n\nIn Table 7 we provide dataset statistics on the node classification datasets used in this paper.\n\nTable 7: Dataset statistics. ‘s’ stands for single class classification and ‘m’ for multi-class.\n\nDataset Nodes 2,708 Cora 3,327 Citeseer 19,717 PubMed 56,944 PPI 89,250 Flickr 232,965 Reddit\n\nEdges 5,429 4,732 44,338 81,8716 899,756 11,606,919\n\nFeatures Classes Train / Val / Test 140 / 500 / 1,000 1,433 120 / 500 / 1,000 3,703 100 / 500 / 1,000 500 44,906 / 6,514 / 5,524 50 44,760 / 22,312 / 22,312 500 153,756 / 23,295 / 55,911 602\n\n7 (s) 6 (s) 3 (s) 121 (m) 7 (s) 41 (s)\n\nA . 2\n\nI M P L E M E N TAT I O N D E TA I L S\n\nIn our DGG, all MLPs use a signle fully-connected layer of dimension 64. In our DGG, all MLPs use a single fully-connected layer of dimension 64. We use a Gumbel-Softmax temperature of 2.5. When adding Gumbel noise to the edge log-probabilities, we do not add any to the self-loops (i.e. the diagonal of the edge probability matrix). During training, we keep the Gaussian and Gumbel noise on, but turn it off during inference. While in practice it can be left on, we found it does not significantly impact the results.\n\nA . 3 T R A I N I N G D E TA I L S\n\nWe train the entire network end-to-end using the classification loss from the downstream model and an annealed MSE loss on the adjacency matrix generated by the DGG:\n\nLtotal = Lclass +\n\nα M\n\nM (cid:88)\n\ni=1\n\n(yi − ˆyi)2,\n\n(10)\n\nwhere the first term is the classification loss from the downstream GCN, the second term is the MSE ˆA ∈ RN ×N for which we have a node label, loss applied to every element ˆyi of the adjacency matrix and α is loss weight. The model can be trained by annealing α smoothly or in a step-wise manner. In practice we keep α constant for the first 100 epochs and then set it to zero for the rest of the training schedule (where the total number of epochs is determined by schedule of the downstream GCN).\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA . 4 R O B U S T N E S S T O N O I S E\n\nWe test the effect of the DGG when the input graph has random edges added across it. We do this by adding edges between previously unconnected nodes. Broadly, the results in Fig. 4 demonstrate the detrimental effects of noisy edges on classification accuracy, but the inclusion of the DGG can mitigate this. Interestingly, the state-of-the-art GCNII Chen et al. (2020) demonstrates the largest drops in accuracy as the noise increases, which may be attributed to the depth of their graph convolution layers. In such deeper message-passing models, the edge structure is even more significant, highlighting the importance of learning a structure that prevents the propagation of irrelevant information.\n\nFigure 4: Node classification accuracy with noisy edges added to the input graph of different datasets.\n\nB T R A J E C T O R Y P R E D I C T I O N E X P E R I M E N T S\n\nB . 1 D ATA S E T D E TA I L S\n\nETH and UCY. ETH (Pellegrini et al., 2009) and UCY (Lerner et al., 2007) are two common benchmarks for pedestrian trajectory prediction. These datasets consist of 5 subsets of widely used real-world pedestrian trajectories (Alahi et al., 2016; Gupta et al., 2018; Mohamed et al., 2020; Salzmann et al., 2020). The primary challenge in these datasets are the frequent interactions of agents in very crowded scenes. Furthermore, the number of pedestrians varies considerably. Some frames contain only 2 pedestrians, while many have over 50.\n\nSportVU. The STATS SportVu (SportVU) is a tracking dataset composed of multiple NBA seasons. Each scene consists of two teams of 5 players, with each team categorized as either making an offensive or defensive play in a particular game. Each play contains 50 timesteps sampled at 5Hz, with the player trajectories expressed in (x, y, z) coordinates.\n\nStanford Drone. The Stanford Drone Dataset (SDD) (Robicquet et al., 2016) is a large dataset with 20 different top-down scenes across multiple areas at Stanford University. Each scene consists of agents of different types, from pedestrians and skaters to cars and buses. Trajectories are recorded at 2.5Hz and expressed in (x, y) world coordinates. Despite the heterogeneity of agents, the maximum number of agents in any one scene is 21.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB . 2 Q U A L I TAT I V E R E S U LT S\n\nIn Fig. 5 we plot the node-degree distribution learned by our DGG across multiple datasets. While on average, a pedestrian may only look at their 2 nearest neighbors in crowded scenes such as Zara1 and Univ, this can increase to almost 5 nearest neighbors in some cases. This suggests that both a fully-connected graph, or one with a fixed node-degree like DGM (Kazi et al., 2020) are both suboptimal.\n\nFigure 5: Distribution of the learned node degree k over the test split for different trajectory prediction datasets.\n\nFigure 6 compares our predicted trajectories to DGM (Kazi et al., 2020), on the SportVU dataset. As shown, our trajectories are closer to the ground truth. We illustrate this further in Fig. 7, which shows the graph generated by our DGG for 3 different basketball players in a game. The figure demonstrates how our DGG lets each player look at a different number of neighbours, while DAGNet Monti et al. (2021) forces each player to look at all others in the game.\n\nFigure 6: Qualitative results for trajectory prediction on the SportVU dataset. Orange: ground truth; Blue: DGM prediction; Red: our prediction.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Graph topology visualisation in basketball players on the SportVU dataset: ours vs. DAGNet (Monti et al., 2021). We display the selected neighborhood for 3 different players and a histogram of the node-degree k accumulated over the dataset/scene. First row: DAGNet’s fully-connected graph, second row: our DGG.\n\n15",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a differentiable graph-generator that builds the graph topology on the fly.\n\n# Strength And Weaknesses\n\nStrength:\n- The paper is clearly written.\n- The approach is technically sound.\n\nWeakness:\n- The idea of learning the graph structure for GNN is not novel. I have read several papers with similar ideas, for example, http://proceedings.mlr.press/v97/franceschi19a/franceschi19a.pdf\n- The proposed method is only evaluated with basic GNN models - the reported number is far below the state-of-the-art performance, on the classic benchmark datasets being used.\n- The approach is clearly not going to scale to large graphs. And I did not find discussions regarding the scalability of the method.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written. The font of the submission is not standard, though.\n\n# Summary Of The Review\n\nOverall, this paper proposes yet another method for applying Graph Neural Networks to datasets without explicitly relational structure through inferring latent graphs. While the proposed method is technically sound, it lacks technical novelty. Also, the empirical performance is weak, and the method is not scalable.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  }
]