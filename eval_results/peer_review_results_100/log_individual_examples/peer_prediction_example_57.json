{
  "example_idx": 57,
  "reference": "Under review as a conference paper at ICLR 2023\n\nMEMORY OF UNIMAGINABLE OUTCOMES IN EXPERIENCE REPLAY\n\nAnonymous authors Paper under double-blind review\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\n35\n\n36\n\n37\n\n38\n\n39\n\n40\n\n41\n\n42\n\n43\n\n44\n\nABSTRACT\n\nModel-based reinforcement learning (MBRL) applies a single-shot dynamics model to imagined actions to select those with best expected outcome. The dynamics model is an unfaithful representation of the environment physics, and its capacity to predict the outcome of a future action varies as it is trained iteratively. An experience replay buffer collects the outcomes of all actions executed in the environment and is used to iteratively train the dynamics model. With growing experience, it is expected that the model becomes more accurate at predicting the outcome and expected reward of imagined actions. However, training times and memory requirements drastically increase with the growing collection of experiences. Indeed, it would be preferable to retain only those experiences that could not be anticipated by the model while interacting with the environment. We argue that doing so results in a lean replay buffer with diverse experiences that correspond directly to the model’s predictive weaknesses at a given point in time. We propose strategies for: i) determining reliable predictions of the dynamics model with respect to the imagined actions, ii) retaining only the unimaginable experiences in the replay buffer, and iii) training further only when sufficient novel experience has been acquired. We show that these contributions lead to lower training times, drastic reduction of the replay buffer size, fewer updates to the dynamics model and reduction of catastrophic forgetting. All of which enable the effective implementation of continual-learning agents using MBRL.\n\n1\n\nINTRODUCTION\n\nModel-Based Reinforcement Learning (MBRL) is attractive because it tends to have a lower sample complexity compared to model-free algorithms like Soft Actor Critic (SAC) (Haarnoja et al. (2018)). MBRL agents function by building a model of the environment in order to predict trajectories of future states based off of imagined actions. An MBRL agent maintains an extensive history of its observations, its actions in response to observations, the resulting reward, and new observation in an experience replay buffer. The information stored in the replay buffer is used to train a single-shot dynamics model that iteratively predicts the outcomes of imagined actions into a trajectory of future states. At each time step, the agent executes only the first action in the trajectory, and then the model re-imagines a new trajectory given the result of this action (Nagabandi et al. (2018)). Yet, many real-world tasks consist in sequences of subtasks of arbitrary length accruing repetitive experiences, for example driving over a long straight and then taking a corner. Capturing the complete dynamics here requires longer sessions of continual learning. (Xie & Finn (2021)) Optimization of the experience replay methodology is an open problem. Choice of size and maintenance strategy for the replay buffer both have considerable impact on asymptotic performance and training stability (Zhang & Sutton (2017)). From a resource perspective, the size and maintenance strategy of the replay buffer pose major concerns for longer learning sessions. The issue of overfitting is also a concern when accumulating similar or repetitive states. The buffer can become inundated with redundant information while consequently under-representing other important states. Indefinite training on redundant data can result in an inability to generalize to, or remember, less common states. Conversely, too small a buffer will be unlikely to retain sufficient relevant experience into the future. Ideally, a buffer’s size would be the exact size needed to capture sufficient detail for all relevant states (Zhang & Sutton (2017)). Note that knowing a priori all relevant states is unfeasible without extensive exploration.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n45\n\n46\n\n47\n\n48\n\n49\n\n50\n\n51\n\n52\n\n53\n\n54\n\n55\n\n56\n\n57\n\n58\n\n59\n\n60\n\n61\n\n62\n\n63\n\n64\n\n65\n\n66\n\n67\n\n68\n\n69\n\n70\n\n71\n\n72\n\n73\n\n74\n\n75\n\n76\n\n77\n\n78\n\n79\n\n80\n\n81\n\n82\n\n83\n\n84\n\n85\n\n86\n\n87\n\n88\n\n89\n\n90\n\n91\n\n92\n\n93\n\n94\n\n95\n\n96\n\n97\n\n98\n\n99\n\nWe argue that these problems can be subverted by employing a strategy that avoids retaining experiences that the model already has sufficiently mastered. Humans seem to perform known actions almost unconsciously (e.g., walking) but they reflect on actions that lead to unanticipated events (e.g. walking over seemingly solid ice and falling through). Such is our inspiration to attempt to curate the replay buffer based on whether the experiences are predictable for the model.\n\nThrough this work, we propose techniques to capture both common and sporadic experiences with sufficient detail for prediction in longer learning sessions. The approach comprises strategies for: i) determining reliable predictions of the dynamics model with respect to the imagined actions, ii) retaining only the unimaginable experiences in the replay buffer, iii) training further only when sufficient novel experience has been acquired, and iv) reducing the effects of catastrophic forgetting. These strategies enable a model to self-manage both its buffer size and its decisions to train, drastically reducing the wall-time needed to converge. These are critical improvements toward the implementation of effective and stable continual-learning agents.\n\nOur contributions can be summarized as follows: i) contributions towards the applicability of MBRL in continual learning settings, ii) a method to keep the replay buffer size to a minimum without sacrificing performance, iii) a method that reduces the training time. These contributions result in keeping only useful information in a balanced replay buffer even during longer learning sessions.\n\n2 RELATED WORK\n\nCompared to MFRL, MBRL tends to be more sample-efficient (Deisenroth et al. (2013)) at a cost of reduced performance. Recent work by Nagabandi et al. (2018) showed that neural networks efficiently reduce sample complexity for problems with high-dimensional non-linear dynamics. MBRL approaches need to induce potential actions which will be evaluated with a dynamics model to choose those with best reward. Random shooting methods artificially generate large number of actions (Rao (2010)) and model predictive control (MPC) can be used to select actions (Camacho et al. (2004)). Neural networks (NNs) are a suitable alternative to families of equations used to model the environment dynamics in MBRL (Williams et al. (2017)). But, overconfident incorrect predictions, which are common in DNNs, can be harmful. Thus, quantifying predictive uncertainty, a weakness in standard NN, becomes crucial. Ensembles of probabilistic NNs proved a good alternative to Bayesian NNs in determining predictive uncertainty (Lakshminarayanan et al. (2016)). Furthermore, an extensive analysis about the types of model that better estimate uncertainty in the MBRL setting favored ensembles of probabilistic NNs (Chua et al. (2018)). The authors identified two types of uncertainty: aleatoric (inherent to the process) and epistemic (resulting from datasets with too few data points). Combining uncertainty aware probabilistic ensembles in the trajectory sampling of the MPC with a cross entropy controller the authors demonstrated asymptotic performance comparable to SAC but with sample efficient convergence. The MPC, however, is still computationally expensive (Chua et al. (2018); Zhu et al. (2020)). Quantification of predictive uncertainty serves as a notion of confidence in an imagined trajectory. Remonda et al. (2021), used this concept to prevent unnecessary recalculation, effectively using sequences of actions the model is confident in and reducing computations. Our approach also seeks to determine reliable predictions of the dynamics model with respect to the imagined actions, but as a basis to manage growth of the experience replay. Use of Experience Replay in MBRL: While an uncertainty aware dynamics model helps to mitigate the risks of prediction overconfidence, other challenges remain. Another considerable issue when training an MBRL agent is the shifting of the state distribution as the model trains. Experience replay was introduced by Lin (1992), and has been further improved upon. Typically in RL, transitions are sampled uniformly from the replay buffer at each step. Prioritized experience replay (PER) (Schaul et al. (2016)) attempts to make learning more efficient by sampling more frequently transitions that are more relevant for learning. PER improves how the model samples experiences from the already-filled replay buffer, but it does not address how the replay buffer is filled in the first place. In addition, neither work addresses the importance of the size of the replay buffer as a hyperparameter (Zhang & Sutton (2017)). Our method attempts to balance the replay buffer by only adding experiences that should improve the future prediction capacity and keeps the training time bounded to a minimum. Task Agnostic Continual Learning: The context of our work originates in tasks consisting in combinations of possibly repetitive subtasks of arbitrary length. In the terminology of Normandin et al. (2021), we aim for continuous task-agnostic continual reinforcement learning. Meaning that the\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n100\n\n101\n\n102\n\n103\n\n104\n\n105\n\n106\n\n107\n\n108\n\n109\n\n110\n\n111\n\n112\n\n113\n\n114\n\n115\n\n116\n\n117\n\n118\n\n119\n\n120\n\n121\n\n122\n\n123\n\n124\n\n125\n\n126\n\n127\n\ntask boundaries are not observed and transitions may occur gradually (Zeno et al. (2021)). In our case, the task latent variable is not observed and the model has no explicit information about task transitions. In such context, a continual learner can be seen as an autonomous agent learning over an endless stream of tasks, where the agent has to: i) continually adapt in a non-stationary environment, ii) retain memories which are useful, iii) manage compute and memory resources over a long period of time ( Khetarpal et al. (2020), Thrun (1994)). Our proposed strategies satisfy these requirements. Matiisen et al. (2020) address the issue of retaining useful memories in a curriculum learning setting by training a ”teacher” function that mandates a learning and re-learning schedule for the agent assuming that the agent will not frequently revisit old experiences/states and will eventually forget them. Ammar et al. (2015) focus on agents that acquire knowledge incrementally by learning multiple tasks consecutively over their lifetime. Their approach rapidly learns high performance safe control policies based on previously learned knowledge and safety constraints on each task, accumulating knowledge over multiple consecutive tasks to optimize overall performance. Bou Ammar & Taylor (2014) developed a lifelong learner for policy gradient RL. Instead of learning a control policy for a task from scratch, they leverage on the agent’s previously learned knowledge. Knowledge is shared via a latent basis that captures reusable components of the learned policies. The latent basis is then updated with newly acquired knowledge. This resulted in an accelerated learning of new task and an improvement in the performance of existing models without retraining on their respective tasks. With our method, we imbue the RL agent with the ability to self-evaluate and decide in real-time if it has sufficiently learned the current state. Unlike the method presented by Matiisen et al. (2020), our method requires no additional networks to be trained in parallel.\n\nXie & Finn (2021) identified two core challenges in the lifelong learning setting: enabling forward transfer, i.e. reusing knowledge from previous tasks to improve learning new tasks, and to improve backward transfer which can be seen as avoiding catastrophic forgetting (Kirkpatrick et al. (2017)). They developed a method that exploits data collected from previous tasks to cumulatively grow the agent’s skill-set using importance sampling. Their method requires the agent to know when the task changes whereas our method does not have this constrain. Additionally, they focus in forward transfer only. Our method addresses both forward and backward transfer.\n\n128\n\n3 PRELIMINARIES\n\n129\n\n130\n\n131\n\n132\n\n133\n\n134\n\n135\n\n136\n\n137\n\n138\n\n139\n\n140\n\n141\n\n142\n\n143\n\n144\n\n145\n\n146\n\n147\n\n148\n\n149\n\n150\n\n151\n\n152\n\n153\n\nAt each time t, the agent is at a state st ∈ S, executes an action at ∈ A and receives from the environment a reward rt = r(st, at) and a state st+1 according to some environment transition function f : S × A → S. RL consists in training a policy towards maximizing the accumulated reward obtained from the environment. The goal is to maximize the sum of discounted rewards (cid:80)∞ i=t γ(i−t)r(si, ai), where γ ∈ [0, 1]. Instead, given a current state st, MBRL artificially generates a huge amount of potential future actions, for instance using random shooting ( Rao (2010)) or cross entropy( Chua et al. (2018)). Clarification of these methods is beyond the scope of this paper; we defer the interested reader to the bibliography. MBRL attempts to learn a discrete time dynamics model ˆf = (st, at) to predict the future state ˆst+∆t of executing action at at state st. To reach a state into the future, the dynamics model iteratively evaluates sequences of actions, at:t+H = (at, . . . , at+H−1) over a longer horizon H, to maximize their discounted reward (cid:80)t+H−1 γ(i−t)r(si, ai). These sequences of actions with predicted outcomes are called imagined i=t trajectories. The dynamics model ˆf is an inaccurate representation of the transition function f and the future is only partially observable. So, the controller executes only a single action at in the trajectory before solving the optimization again with the updated state st+1. The process is formalized in Algorithm 1. The dynamics model ˆfθ is learned with data Denv, collected on the fly. With ˆfθ, the simulator starts and the controller is called to plan the best trajectory resulting in a∗ t:t+H . Only the first action of the trajectory a∗ t is executed in the environment and the rest is discarded. This is repeated for T askHorizon number of steps. The data collected from the environment is added to Denv and ˆfθ is trained further. The process repeats for N Iterations. Note that generating imagined trajectories requires subsequent calls to the dynamics model to chain predicted future states st+n with future actions up to the task horizon, and so it is only partially parallelizable.\n\nDynamics model. We use a probabilistic model to model a probability distribution of next state given current state and an action. To be specific, we use a regression model realized using a neural network similar to Lakshminarayanan et al. (2016) and Chua et al. (2018). The last layer of the\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nmodel outputs parameters of a Gaussian distribution that models the aleatoric uncertainty (the uncertainty due to the randomness of the environment). Its parameters are learned together with the parameters of the neural network. To model the epistemic uncertainty (the uncertainty of the dynamics model due to generalization errors), we use ensembles with bagging where the members of the ensemble are identical and only differ in the initial weight values. Each element of the ensemble has as input the current state st and action at and is trained to predict the difference between st and st+1, instead of directly predicting the next step. Thus the learning objective for the dynamics model becomes, ∆s = st+1 − st. ˆfθ outputs the probability distribution of the future state ps(t+1) from which we can sample the future step and its confidence ˆs, ˆsσ = ˆfθ(s, [a]). Where the confidence sσ captures both, epistemic and aleatoric uncertainty.\n\nAlgorithm 1 MBRL\n\nInit D with one iteration of a random controller for Iteration i = 1 to N Iterations do\n\nTrain ˆf given D for Time t = 0 to T askHorizon do\n\na∗\n\nt:t+H\n\nGet ComputeOptimalT rajectory(st, ˆf ) Execute a∗ Record outcome: {st, a∗ t , st+1}\n\nt from optimal actions a∗\n\nD ← D ∪\n\nfrom\n\nt:t+H\n\nTrajectory Generation. Each ensemble element outputs the parameters of a normal distribution. To generate trajectories, P particles are created from the current state, sp t = st, which are then propagated by: t+1 ∼ ˆfb(sp sp t , at), using a particular bootstrap element b ∈ {1, ..., B}. Chua et al experimented with diverse methods to propagate particles through the ensemble. The T S∞ method delivered the best results. It refers to particles never changing the initial bootstrap element. Doing so, results in having both uncertainties separated at the end of the trajectory. Specifically, aleatoric state variance is the average variance of particles of same bootstrap, whilst epistemic state variance is the variance of the average of\n\nparticles of same bootstrap indexes. We use also T S∞.\n\nControl. To select the best course of action leading to sH , MBRL generates a large number of trajectories K and evaluates them in terms of reward. To find the actions that maximize reward, we used the cross entropy method (CEM) Botev et al. (2013), an algorithm for solving optimization problems based on cross-entropy minimization. CEM gradually changes the sampling distribution of the random search so that the rare-event is more likely to occur and estimates a sequence of sampling distributions that converges to a distribution with probability mass concentrated in a region of nearoptimal solutions. Appendix A details the use of CEM to get the optimal sequence of actions a∗\n\nt:t+H\n\n4 TOWARDS CONTINUAL LEARNING\n\nApplying MBRL to a continual learning setting is a promising venue for research. The dynamics model could be constantly improving and adapting dynamically to changes in the environment. Many real-world tasks can be broken in sequences of subtasks of arbitrary length. Capturing the complete dynamics then requires exposure to longer sessions of continual learning. Arbitrarily long repetitive tasks lead to increasing redundancy in the experience replay constantly increasing of the amount of experience collected. These issues hinder the use of MRBL in continual learning settings. What to add to the replay buffer: We posit that it would be preferable to retain only those experiences that could not be adequately anticipated by the model during each episode in the environment. Essentially, we would only like to add to the replay buffer observations for which the model issued a poor prediction. On the contrary, we would like to avoid filling the replay buffer or updating the model on observations that the model is good at predicting. We contend that these two elements will lead eventually to a balanced replay buffer, which will contain only relevant observations. This will contribute to the objective of continual learning.\n\n5 UARF: UNCERTAINTY AWARE REPLAY FILTERING\n\nContinual learning requires the MBRL agent to adapt in a non-stationary environment, retaining memories that are useful whilst avoiding catastrophic forgetting, and it can manage compute and memory resources over a long period of time ( Khetarpal et al. (2020)). The proposed method, UARF, addresses these issues with a variety of strategies. Algorithm 2 is the main algorithm used to select which observations to append in the replay buffer. The optimal actions a∗ t:t+H are computed\n\n4\n\n154\n\n155\n\n156\n\n157\n\n158\n\n159\n\n160\n\n161\n\n162\n\n163\n\n164\n\n165\n\n166\n\n167\n\n168\n\n169\n\n170\n\n171\n\n172\n\n173\n\n174\n\n175\n\n176\n\n177\n\n178\n\n179\n\n180\n\n181\n\n182\n\n183\n\n184\n\n185\n\n186\n\n187\n\n188\n\n189\n\n190\n\n191\n\n192\n\n193\n\n194\n\n195\n\n196\n\n197\n\n198\n\n199\n\n200\n\n201\n\n202\n\n203\n\n204\n\n205\n\nUnder review as a conference paper at ICLR 2023\n\n206\n\n207\n\n208\n\n209\n\n210\n\n211\n\n212\n\n213\n\n214\n\n215\n\n216\n\n217\n\n218\n\n219\n\n220\n\n221\n\n222\n\n223\n\n224\n\n225\n\n226\n\n227\n\n228\n\n229\n\n230\n\n231\n\n232\n\n233\n\n234\n\n235\n\n236\n\n237\n\n238\n\n239\n\n240\n\n241\n\n242\n\n243\n\n244\n\n245\n\n246\n\n247\n\nby the ComputeOptimalT rajectory function (See Appendix A) given the current state of the environment st and ˆf . The future trajectory and its uncertainty, p∗ r(t+1:t+1+H), is then obtained by t:t+H and st with ˆf . The variable unreliableM odel is set to true when the algorithm believes using a∗ the imagined trajectory not to be trustworthy. Depending on its value, calculation of new trajectories and additions to the replay buffer will be avoided and therefore computation time and size of the replay buffer will be reduced. If unreliableM odel is False, the next predicted action is executed in the environment. Subsequent actions from a∗ t:t+H are executed until the unreliableM odel flag is set to False or the environment reaches T askHorizon number of steps. The process is repeated for the maximum iterations allowed for the task. After the first action, every time an action a∗ t+1:t+H is executed trajectory computation is avoided and this new observation is not added to the replay buffer on the basis that the model can already predict its outcome. If unreliableM odel is True, the algorithm calculates a new trajectory and adds the current observation to the replay buffer. Hereby, the buffer stores only observations for which the model could not predict (imagine) the outcome.\n\nTrustworthy imagination (Algorithm 2 L:18-21). The algorithm that assigns a value to unreliableM odel is named BICHO. BICHO will essentially return True as long as the reward projected in the future does not differ significantly with respect to the imagined future reward p∗ r\nand the confidence of the model remains high. BICHO is built assuming that if parts of the trajectory do not vary, their projected reward will be as imagined by the model with some confidence. After calculating a trajectory, the distribution of rewards p∗ r is calculated for H steps in the future. Whereas, at each step of the environment, independent if the recalculation was skipped or not, a new trajectory p′ r of H steps is projected, starting from state st which is given by the environment and using actions a∗ t+i in the imagined trajectory. We use the Wasserstein distance (Bellemare et al. (2017)) to find how much these two distributions change after each time step in the environment. If the change is > β (which is a hyper parameter to tune) then unreliableM odel is True. We can control how many steps ahead we would like to compare the two distributions. The comparison is done for just c steps (< H), which is a hyper parameter to tune. If they differ significantly, then the trajectory is unreliable. That is, if the projected reward differs from the imagined one the outcome of the actions is uncertain and the trajectory should be recalculated. Even for a model that has converged, predicting trajectories of great length is impossible. Recalculations inevitably occur at the end of trajectories. Such recalculations do not necessarily represent the appearance of unseen information, but rather a limitation of the successful model in a complex environment. Hence, we would not want to add them to the buffer. The maximum prediction distance (MPD) defines a cutoff for a trajectory, and adjusts the strictness of the filtering mechanism. Refer to Appendix E for an extensive analysis.\n\nUpdates on novel information (Algorithm 2 L:24-25) over-training the dynamics model leads to instabilities due to overfitting. This problem is exacerbated when the replay buffer contains just the minimum essential data. If we only filter the replay buffer, continuously updating the parameters of the dynamics model will eventually lead to overfitting. Instead, our method updates the parameters of the dynamics model only when there is sufficient new information in the replay buffer. We train the dynamics model only when new data exceeds the new data threshold hyper parameter. For our experiments we set this variable to 0.01 training only when 1% of the experiences in the replay buffer are new since the last update of the parameters of the dynamics model.\n\n248\n\n6 EXPERIMENTS\n\n249\n\n250\n\n251\n\n252\n\n253\n\n254\n\n255\n\n256\n\n257\n\n258\n\n259\n\nThe primary purpose of the proposed algorithm is for the resulting replay buffer to retain only relevant, non-redundant, experiences that will be useful for learning the task. We envision applying this method to tasks that require longer training sessions and in continual learning settings. We designed three experimental procedures. The first experiment seeks to establish that our method indeed retains a reduced buffer sufficient for achieving expected rewards when learning a single task throughout long training sessions. To this end, we evaluate the proposed method in benchmark environments for higher number of episodes than in Chua et al. (2018). The second experiment seeks to prove that UARF retains a small number of complementary experiences compared to nonfiltering baseline algorithms when training on a sequence of different but related tasks in a continual learning setting. We evaluate our method in a combined task including unseen subtasks. The third experiment seeks to show how UARF addresses the effects of catastrophic forgetting.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nif unreliableM odel then\n\nt:t+H ) // Use ˆf to predict H rewards ahead\n\nt:t+H from ComputeOptimalT rajectory (st, ˆf ) r(t+1:t+H) given (st, ˆf , a∗\n\nAlgorithm 2 UARF 1: Initialize dynamics model ˆf parameters; Initialize replay buffer D with an iteration of a random controller 2: unreliableM odel = T rue and trainM odel = F alse 3: for Iteration l = 1 to N Iterations do if trainM odel then Train ˆf given D 4: for Time t = 0 to T askHorizon do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23:\n\nGet a∗ Get p∗ i = 0 else i += 1 Get first action at from available optimal actions a∗ Execute in the environment at to obtain st+1 and rt+1 Discard first action and keep the rest a∗ r(t+i+1:t+H) given (st, ˆf , a∗ Get p′ // Trustworthy imagination L = min(H, c - i) // Calculate the number steps ahead to consider r error = W assersteinDistance(p′ if r error > β then unreliableModel = TRUE else unreliableModel = False if unreliableM odel then\n\nRecord outcome: D ← D ∪ {st, at, st+1} // Updates on novel information if new data in D > new data threshold ∗ length(D) then\n\nr(t+i+1:t+i+L)||p∗\n\ntrainM odel = True\n\nr(t+1:t+L))\n\nt+i:t+H )\n\nt = a∗\n\nt+1:t+H\n\nt:t+H\n\nFigure 1: Performance of algorithms (BL: green, BICHO: red, UARF: blue) in (top to bottom) Cartpole, Pusher, Reacher, and Masspoint sector1. From left to right column: episode reward, time per episode (s), cumulative number of observations stored in the replay buffer, new experiences added to the buffer per episode. The rightmost plots illustrate with dashed vertical lines episodes that resulted in UARF updating its model parameters.\n\n260\n\n6.1 E1– CONTINUING TO LEARN A TASK AFTER CONVERGENCE\n\n261\n\n262\n\nThis experiment is intended to show that our method retains sufficient experience to solve the task while curtailing buffer growth and unnecessary model updates. We intend to prove that this results in\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n263\n\n264\n\n265\n\n266\n\n267\n\n268\n\n269\n\n270\n\n271\n\n272\n\n273\n\n274\n\n275\n\n276\n\n277\n\n278\n\n279\n\n280\n\n281\n\n282\n\n283\n\n284\n\n285\n\n286\n\n287\n\n288\n\n289\n\n290\n\n291\n\n292\n\n293\n\n294\n\n295\n\n296\n\n297\n\n298\n\n299\n\n300\n\n301\n\n302\n\n303\n\n304\n\n305\n\n306\n\n307\n\n308\n\n309\n\n310\n\n311\n\n312\n\n313\n\n314\n\na dramatic reduction in the replay buffer size (which is free of any artificially-imposed limits) while retaining strong performance (per-episode reward) and reducing per-episode wall clock run-time.\n\nWe use the MuJoCo (Todorov et al. (2012)) physics engine and environments Cartpole (CP), Pusher (PU) and Reacher (RE) with task length (T askH) and trajectory horizon (H) chosen for a valid comparison with Chua et al. (2018). With similar training scenarios, Remonda et al. (2021) trained CP for 30 episodes, PU and RE for 150. Instead, we trained each for 100 episodes. We also included a modified version of the Masspoint environment (Thananjeyan et al. (2020)) (also used in E2). Masspoint is a navigation task in which a point mass navigates to a given goal. It is a 5-dimensional (x, y, vx, vy, ρ) state domain. Where (x, y) is the position of the agent, (vx, vy) its speed, and ρ is the distance between the agent and the closest point to a given path. The agent can exert force in cardinal directions and experiences drag coefficient ψ. We use ψ = 0.6 and included noise in the starting position. We modified the goal of the agent so that it must move as fast as possible without deviating from a given path. Each task and its complexity is then determined by the geometry of the path to be followed. The reward is calculated as r = V (1 − |ρ|). Where V is the speed of the agent and ρ the distance to the task’s path. This experiment used sector1 (Figure in Appendix B) and Hyperparameters shown in Appendix F.\n\nFigure 2: Per-step reward and cumulative steps added to the replay buffer for episodes 1 (left), 4 (middle), and 99 (right) in the Masspoint Sector 1 maneuver. These plots show that UARF adds fewer redundant experiences to the replay buffer as the model converges.\n\nWe assess performance in terms of per-episode reward, per-episode wall time, and replay buffer size. We evaluate three algorithms: baseline (BL) is a conventional MBRL (PETS Chua et al. (2018)), BICHO uses functionality to avoid unnecessary computation, and UARF. BICHO and UARF used the same values of β and look-ahead, estimated empirically to produce a reasonable balance in terms of per-episode reward and percentage of recalculation. All experiments use random seeds and randomized initial conditions for each run, and ran in workstations with Nvidia 3080TI GPUs.\n\nResults: Fig 1 top shows the results obtained in CP. Fig 1-mid-right shows the size of the replay buffer during training. We observe that while the replay buffer keeps grows in the case of BL and BICHO, the size of the buffer derived from UARF is comparably flat: the buffer resulting from UARF is 10x smaller. The training time per episode (Fig 1 mid-left) remains nearly constant and lower for UARF. BL takes substantially longer than both BICHO and UARF to complete an episode. The wall time of both the BL and BICHO exhibit linear growth. It takes longer to update the model as the replay buffer grows linearly. Fig 1-left shows comparable reward per episode for all methods. Results in Fig 1 for PU (row 2), RE (row 3) and Masspoint (bottom) are consistent with those of CP. Fig 2 illustrates the management of buffer growth in Masspoint by showing exactly at which steps experiences are added to the replay buffer during untrained (E1) and trained (E99) episodes. These plots reveal that when the model is untrained, many experiences are added to the buffer throughout the episode. After the model is trained (E99), UARF stops adding experiences to the buffer as the model is able to predict them. Hence, new experiences are deemed redundant and not useful to the model. Results support our claim that UARF obtains a drastically smaller replay buffer that is intelligently populated with only relevant information. This is achieved while maintaining strong performance in the environment compared to BL. Note that while the curves are plotted per episode, it is misleading to assume that all methods converge roughly at the same time. The time per episode in the case of UARF is at least half that of BL for approximately in every environment and it remains stable, while it increases linearly for BL and BICHO with increasing buffer size. The total wall time in average for CP was BL=1.83h, BICHO=0.59h and UARF=0.57h. For PU, it was BL=0.97h, BICHO=0.73h and UARF=0.66h. For RE, it was BL=1.99h BICHO=1.55h and UARF=1.45h, and for MP it was BL=2.15h, BICHO=1.94h and UARF=1.68h.\n\n315\n\n6.2 EX 2. CONTINUAL LEARNING EXPERIMENT\n\n316\n\n317\n\nThis experiment is set in Task Agnostic Continual Reinforcement Learning (the model is not aware of tasks or task transitions). In this setting, we sought to prove that UARF maintains a leaner and\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Training performance of Baseline, BICHO and UARF in a Task Agnostic Continual Learning. Models trained on seven maneuvers: corner1, corner1 inverse, chicane, chicane inverse, corner14, corner14 inverse, and straight. Vertical lines indicate a task switch. In the middle-right plot, cyan vertical lines also indicate when UARF triggers a model update. Full-track evaluation in the far-right plot; cumulative reward achieved at each step on the full-track without further training.\n\nmore relevant collection of experiences in the replay buffer than do baseline algorithms. These characteristics of the proposed algorithm, we posit, result in strong test performance with less data and greater stability. The existence of these characteristics can be verified by observing (after training) the size of the buffer, the number of experiences from each maneuver present in the buffer, and the performance of the models on the test task. We used the Masspoint racing environment, defining different simple tasks that can be composed to solve a complex, unseen one.\n\nEach algorithm trains a model on a sequence of seven separate sub-tasks: two corners and their inverses, a chicane and its inverse, and a straight (details illustrated in Appendix B). The models retain their parameters and replay buffers between training on each task individually. After training on the last task, the methods are each tested on the full track, which contains some of the sub-tasks seen during training (colored in the full-track image, Appendix B) and tasks unseen during training (shown in black in the full-track image). The model must remember what it learned by training on each sub-task and apply this knowledge to navigate a more complex, unseen task. All of the algorithms had a virtually unlimited replay buffer size. Each model was trained for 30 episodes on each sub-task and then tested on the test task.\n\nFigure 4: Distribution of experiences from each sub-task in the replay buffers of each algorithm immediately following training. Detail shows a zoomed-in version for UARF.\n\nResults Figure 3 shows episode reward, wall-time, buffer size during training, and new experiences added to the buffer per episode. Vertical lines illustrate task divisions. High episode reward indicates that each model adequately learns each subtask. UARF maintains almost a constant wall-time, while BL and BICHO increase as experience accumulates. Buffer growth for BL and BICHO is linear, but UARF evidences asymptotic growth (13x smaller) adding no new experiences at the end of training. Figure 3-4 shows the buffer growth of UARF. A larger amount of additions to the replay buffer occur while training the first tasks. Growth slows to a near halt during the last tasks. This is the case for example with the fourth task (chicane inverted). The previous task (chicane) is similar, and the information to solve the previous task is enough that the algorithm does not require a significant amount of new experience to solve chicane inverted. Figure 4 shows the distribution of experiences from each sub-task present in each algorithm’s replay buffer immediately following training. BL and BICHO employ a naive approach, resulting in replay buffers with distributions of experience determined exclusively by the length of the various maneuvers. The filtering mechanism of UARF results in a distribution of experience with some maneuvers having limited representation (e.g., the inverse maneuvers) This is because the UARF algorithm intelligently decides to omit redundant experiences from the buffer and leaves only the relevant ones. Figure 3 right shows that all three algorithms result in a model that adequately solves the test task. UARF continues to manage buffer growth while achieving high performance. The results support our initial hypothesis by illustrating clearly the proposed algorithm’s propensity to maintain a smaller and more relevant replay buffer while achieving the performance of the baseline in a continual learning setting.\n\n8\n\n318\n\n319\n\n320\n\n321\n\n322\n\n323\n\n324\n\n325\n\n326\n\n327\n\n328\n\n329\n\n330\n\n331\n\n332\n\n333\n\n334\n\n335\n\n336\n\n337\n\n338\n\n339\n\n340\n\n341\n\n342\n\n343\n\n344\n\n345\n\n346\n\n347\n\n348\n\n349\n\n350\n\n351\n\n352\n\n353\n\n354\n\n355\n\n356\n\n357\n\n358\n\n359\n\n360\n\nUnder review as a conference paper at ICLR 2023\n\n361\n\n6.3 EX 3. CATASTROPHIC FORGETTING\n\n362\n\n363\n\n364\n\n365\n\n366\n\n367\n\n368\n\n369\n\n370\n\n371\n\n372\n\n373\n\n374\n\n375\n\n376\n\n377\n\n378\n\n379\n\n380\n\n381\n\n382\n\n383\n\n384\n\n385\n\n386\n\n387\n\n388\n\n389\n\n390\n\n391\n\n392\n\n393\n\n394\n\n395\n\n396\n\n397\n\n398\n\n399\n\n400\n\n401\n\n402\n\n403\n\n404\n\nOur approach helps to mitigate catastrophic forgetting. When using a fixed replay buffer size, it is important to ensure that the appropriate maximum buffer size is chosen (Zhang & Sutton (2017)). If this value is undertuned, important experiences can be jettisoned, and catastrophic forgetting can occur. To illustrate how UARF helps to alleviate this risk, we ran the same experiment shown in section Ex.2 but with a replay buffer of fixed size (5000 samples; roughly 4x the replay buffer size used by UARF in the unlimited size setting). Table 1 compares rewards achieved by each algorithm with both unlimited and fixed buffers. The models were validated on the full track and also on a maneuver that was trained early on in the training process (c1 inverse). Results reveal that with an undertuned fixed buffer size, BL loses about 10% performance both on the full track and on c1 inverse. This is indicative of the fact that the non-filtering algorithms are hitting the buffer size cap, throwing away valuable experiences, and forgetting how to properly solve maneuvers that were trained early on. This impacts performance on the full track as well.\n\nUnlimited Buffer Full Track 22172 21975\n\nFixed Buffer Full Track 20235 22102\n\nFixed Buffer c1 inverse First Pass 1787 1781\n\nFixed Buffer c1 inverse Post-Training 1561 1795\n\nBASELINE UARF\n\nTable 1: Fixed Buffer Experiment. Results demonstrate susceptibility to catastrophic forgetting when not using UARF. The BL forgets previous maneuvers after the FIFO mechanism of the fixedsize replay buffer eliminates experiences from them with an impact of about 10% in reward.\n\n7 DISCUSSION AND CONCLUSION\n\nThe results in E1 reveal that continuing to run our algorithm in a repetitive environment with redundant or monotonous actions leads to, in some tasks, no increase in buffer and reduced dynamics model updates. This has the consequence of reduced running and training times, while reducing the effects of catastrophic forgetting and keeping the replay buffer size to a bare minimum. In E2, a continual learning setting, we demonstrated that using our approach leads outcomes with 1/25th of the experiences without performance degradation. UARF effectively deals with an unbounded growth of the replay buffer, which again reduces training time and instabilities. This effect is accentuated when training on a continual learning setting. UARF uses a buffer 43x smaller than the baseline.\n\nThe replay buffer is an instrument that makes the use of deep neural networks in RL more stable and it is an essential part in algorithms such as PETs. Such analyses of replay buffer are scarce. But recently, research has turned to analyze the contents and strategies to manage the replay buffer of RL agents Fedus et al. (2020), and also in supervised learning Aljundi et al. (2019). We contribute to such body of work analyzing and offering strategies to manage growth of replay buffer in model based RL. Having managed growth, there are several aspects we would like to turn to in the future: i) identifying task boundary from the novelty of experiences, ii) managing what to forget for limited size buffers, iii) managing what to remember / refresh when a change in task is evident. All this would allow to run agents for arbitrary time without having to deal with size of the buffer and would offer promising opportunities for deploying MBRL in a continual learning setting.\n\nBICHO could be used to prioritize entries in the RB where the model was uncertain. Indeed, prioritized buffer strategies support the usage of experience once it is in the buffer, but as the authors of the PER paper state, strategies for what to add and when (our work) are important open avenues for research. We did not explore our methods in environments where the tasks have interfering dynamics. But, if the dynamics change, poor predictions by the model will result in adding experiences to the replay buffer. What happens if interfering tasks occur permanently is an interesting follow up.\n\nIn summary, we proposed strategies that comply with requirements for continual learning. Our approach retains only memories which are useful: it obtains lean and diverse replay buffers capturing both common and sporadic experiences with sufficient detail for prediction in longer learning sessions. Our approach manages compute and memory resources over longer periods: it deals with the unbounded growth of the replay buffer, its training time and instability due to catastrophic forgetting. These results offer promising opportunities for deploying MBRL in a continual learning setting.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n405\n\n406\n\n407\n\n408\n\n409\n\n410\n\n411\n\n412\n\n413\n\n414\n\n415\n\n416\n\n417\n\n418\n\n419\n\n420\n\n421\n\n422\n\n423\n\n424\n\n425\n\n426\n\n427\n\n428\n\n429\n\n430\n\n431\n\n432\n\n433\n\n434\n\n435\n\n436\n\n437\n\n438\n\n439\n\n440\n\n441\n\n442\n\n443\n\n444\n\n445\n\n446\n\n447\n\n448\n\n449\n\n8 REPRODUCIBILITY STATEMENT\n\nTo make our experiments reproducible, we provide the source code in the supplementary material. We include instructions describing how to run all the experiments and to create the images. We include the source code of the proposed algorithms, the MassPoint environment and clear instructions showing how to install extra packages and dependencies needed to reproduce our experiments.\n\nREFERENCES\n\nRahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection\n\nfor online continual learning, 2019.\n\nHaitham Bou Ammar, Rasul Tutunov, and Eric Eaton. Safe policy search for lifelong reinforcement\n\nlearning with sublinear regret, 2015.\n\nMarc G Bellemare, Will Dabney, and R ́emi Munos. A distributional perspective on reinforcement\n\nlearning. In International Conference on Machine Learning, pp. 449–458. PMLR, 2017.\n\nZdravko I. Botev, Dirk P. Kroese, Reuven Y. Rubinstein, and Pierre L’Ecuyer. Chapter 3 - the cross-entropy method for optimization. In C.R. Rao and Venu Govindaraju (eds.), Handbook of Statistics, volume 31 of Handbook of Statistics, pp. 35 – 59. Elsevier, 2013. doi: https://doi. org/10.1016/B978-0-444-53859-8.00003-5. URL http://www.sciencedirect.com/ science/article/pii/B9780444538598000035.\n\nHaitham Bou Ammar and Matthew Taylor. Online multi-task learning for policy gradient methods.\n\n01 2014.\n\nE.F. Camacho, C. Bordons, and C.B. Alba. Model Predictive Control. Advanced Textbooks in Control and Signal Processing. Springer London, 2004. ISBN 9781852336943. URL https: //books.google.at/books?id=Sc1H3f3E8CQC.\n\nKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement\n\nlearning in a handful of trials using probabilistic dynamics models, 2018.\n\nM. P. Deisenroth, G. Neumann, and J. Peters. 2013. doi: 10.1561/2300000021.\n\nWilliam Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark\n\nRowland, and Will Dabney. Revisiting fundamentals of experience replay, 2020.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\n\nmaximum entropy deep reinforcement learning with a stochastic actor, 2018.\n\nKhimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual reinforce-\n\nment learning: A review and perspectives, 2020.\n\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.\n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive\n\nuncertainty estimation using deep ensembles, 2016.\n\nLong-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.\n\nMachine Learning, pp. 293–321, 1992.\n\nTambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher–student curriculum learning. IEEE Transactions on Neural Networks and Learning Systems, 31(9):3732–3740, 2020. doi: 10.1109/TNNLS.2019.2934906.\n\nAnusha Nagabandi, G. Kahn, Ronald S. Fearing, and S. Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 7559–7566, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\n450\n\n451\n\n452\n\n453\n\n454\n\n455\n\n456\n\n457\n\n458\n\n459\n\n460\n\n461\n\n462\n\n463\n\n464\n\n465\n\n466\n\n467\n\n468\n\n469\n\n470\n\n471\n\n472\n\n473\n\n474\n\n475\n\n476\n\n477\n\n478\n\n479\n\n480\n\nFabrice Normandin, Florian Golemo, Oleksiy Ostapenko, Pau Rodriguez, Matthew D Riemer, Julio Hurtado, Khimya Khetarpal, Dominic Zhao, Ryan Lindeborg, Timoth ́ee Lesort, et al. Sequoia: A software framework to unify continual learning research. arXiv e-prints, pp. arXiv–2108, 2021.\n\nAnvil V. Rao. A survey of numerical methods for optimal control. Advances in the Astronautical\n\nScience, 135:497–528, 2010.\n\nAdrian Remonda, Eduardo E. Veas, and Granit Luzhnica. Acting upon imagination: when to trust imagined trajectories in model based reinforcement learning. CoRR, abs/2105.05716, 2021. URL https://arxiv.org/abs/2105.05716.\n\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In\n\nICLR (Poster), 2016.\n\nBrijen Thananjeyan, Ashwin Balakrishna, Ugo Rosolia, Felix Li, Rowan McAllister, Joseph E. Gonzalez, Sergey Levine, Francesco Borrelli, and Ken Goldberg. Safety augmented value estimation from demonstrations (saved): Safe deep model-based rl for sparse cost robotic tasks, 2020.\n\nS. Thrun. A lifelong learning perspective for mobile robot control. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS’94), volume 1, pp. 23–30 vol.1, 1994. doi: 10.1109/IROS.1994.407413.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\n\nIn IROS, pp. 5026–5033. IEEE, 2012. ISBN 978-1-4673-1737-5.\n\nGrady Williams, Paul Drews, Brian Goldfain, James M. Rehg, and Evangelos A. Theodorou. Information theoretic model predictive control: Theory and applications to autonomous driving, 2017.\n\nAnnie Xie and Chelsea Finn. Lifelong robotic reinforcement learning by retaining experiences.\n\narXiv preprint arXiv:2109.09180, 2021.\n\nChen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task-Agnostic Continual Learning Using Online Variational Bayes With Fixed-Point Updates. Neural Computation, 33(11):3139–3177, 10 2021. ISSN 0899-7667. doi: 10.1162/neco a 01430. URL https://doi.org/10.1162/ neco_a_01430.\n\nShangtong Zhang and Richard S. Sutton. A Deeper Look at Experience Replay. arXiv e-prints, art.\n\narXiv:1712.01275, December 2017.\n\nGuangxiang Zhu, Minghao Zhang, Honglak Lee, and Chongjie Zhang. Bridging imagination and\n\nreality for model-based deep reinforcement learning. NeurIPS, 2020.\n\n481\n\nA OPTIMAL TRAJECTORY GENERATION\n\nAlgorithm 3 shows the use of CEM to compute the optimal sequence of actions a∗\n\nt:t+H .\n\nAlgorithm 3 Compute Optimal Trajectory Input: sinit: current state of the environment, dynamics model ˆf 1: Initialize P particles, sp 2: for Actions sampled at:t+H ∼ CEM (.), 1 to CEM Samples do 3:\n\nPropagate state particles sp\n\nτ , with the initial state, sinit\n\nτ using TS and ˆf |{D, at:t+H } P\n(cid:80)\n\nr(sp\n\nτ , aτ )\n\n4:\n\nEvaluate actions as\n\nt+H (cid:80)\n\nτ =t\n\n1 P\n\np=1\n\nUpdate CEM(.) distribution\n\n5: 6: return a∗\n\nt:t+H\n\n482\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nB MASS POINT TASKS\n\nEach algorithm trains a model on a sequence of seven separate sub-tasks: two corners and their inverses, a chicane and its inverse, and a straight (Figure 5). The full track contains some of the subtasks seen during training (Shown with different colors in the full-track image (Appendix Figure 5) in addition to tasks unseen during training (shown in black in the full-track image).\n\nFigure 5: Tasks for the Masspoint environment. The x-axis and the y-axis of each figure represents the x,y coordinates of the path to be followed by the mass point bot. The red dot represents the starting point. Top left-to-right: c1, c1 inverted, chicane, chicane inverted and c14. Bottom left-toright: c14 inverted, straight, full track (comprising sub-tasks. chicane, c14. straight, c1), sector1 and sector1 inverted\n\nC ENVIRONMENTS\n\nWe evaluate the methods on agents in the MuJoCo Todorov et al. (2012) physics engine. To establish a valid comparison with Chua et al. (2018) we use four environments with corresponding task length (T askH) and trajectory horizon (H).\n\n• Cartpole (CP): S ∈ R4, A ∈ R1, T askH 200, H 25 • Reacher (RE): S ∈ R17, A ∈ R7, T askH 150, H 25 • Pusher (PU): S ∈ R20, A ∈ R7, T askH 150, H 25 • Masspoint: S ∈ R5, A ∈ R2, T askH 290, H 25\n\nThis means that each iteration will run for T askH, task horizon, steps, and that imagined trajectories include H trajectory horizon steps. S ∈ Ri, A ∈ Rj refers to the dimensions of the environment state consisting in a vector of i components and the action consisting in a vector of j components.\n\nD EX 2. CONTINUAL LEARNING EXPERIMENT. ADDITIONAL RESULTS\n\nFigure 6 shows additional results with the wall-time during the training process for the continual learning experiment.\n\nE MAXIMUM PREDICTION DISTANCE\n\nAn additional parameter of interest when using UARF is what we call the ”maximum prediction distance” or MPD. This parameter operates on the assumption that even for a model that has reached convergence, in some environments, predicting trajectories of great length is impossible. As such, recalculations must inevitably occur at the end of such long trajectories. These recalculations do not necessarily represent the appearance of new, unseen information, but rather a limitation of the successful model in a complex environment. Hence, we would not want to add these experiences to the buffer.\n\nWhere we define the cutoff for a trajectory of ”great length” can be changed, and it serves to adjust the strictness of UARF’s filtering mechanism. For Ex.1 and Ex.2, we chose to set the maximum prediction distance to 1 to ensure the strictest filtering of the replay buffer.\n\n12\n\n483\n\n484\n\n485\n\n486\n\n487\n\n488\n\n489\n\n490\n\n491\n\n492\n\n493\n\n494\n\n495\n\n496\n\n497\n\n498\n\n499\n\n500\n\n501\n\n502\n\n503\n\n504\n\n505\n\n506\n\n507\n\n508\n\n509\n\n510\n\n511\n\n512\n\n513\n\n514\n\n515\n\n516\n\n517\n\n518\n\n519\n\n520\n\n521\n\n522\n\n523\n\n524\n\n525\n\n526\n\n527\n\n528\n\n529\n\n530\n\n531\n\n532\n\n533\n\n534\n\n535\n\n536\n\nUnder review as a conference paper at ICLR 2023\n\nIn 7, we evaluate the effect of the MPD on the performance of UARF in the cartpole environment. We were particularly interested in the effect on the rate of recalculation and on the size of the replay buffer. In 7 one can see that the models converge with no issue, but they do differ slightly in the rates of recalculation and buffer filtering. The strictest MPD, MPD=1, results in the leanest buffer, but its recalculation rate is slightly higher than the models with MPD=2 and MPD4.\n\nThese results show that the MPD serves as a way to tune the strictness of UARF’s buffer filtering mechanism. It would be an area of future research to find the optimal way to tune this parameter automatically throughout training such as to best balance recalculation rate and replay buffer filtering.\n\nFigure 6: Per episode wall time for the three methods during the training process of Ex.2. Vertical lines indicate task switch points.\n\nFigure 7: Performance of the examined algorithms in Cartpole using different maximum prediction distances (MPD). The blue line represents UARF with an MPD=1. The red line is UARF with an MPD=2. The green line is UARF with an MPD=4. From left to right column: episode reward, time per episode (s), cumulative number of observations stored in the replay buffer, new experiences added to the buffer per episode.\n\nF HYPERPARAMETERS\n\nTable 2 shows the hyper parameters used to train UARF. Look-ahead refers to the number of steps ahead BICHO and UARF are using to asses the quality of the imagined trajectories. β controls the sensitivity of BICHO and UARF to inform whether a trajectory is still valid or not. ”New Data Train Threshold” refers to the amount of fresh data that must be added to the replay buffer before the UARF algorithm triggers the training of the dynamics model.\n\nLook-Ahead β\nNew Data Threshold Training episodes CEM population CEM # elites CEM # iterations CEM α MPD\n\nCartpole 10 0.005 1% 100 400 40 5\n0.1 10\n\nPusher Reacher Masspoint\n\n10 0.005 1% 100 500 50 5\n0.1 10\n\n10 0.005 1% 10 400 40 5\n0.1 10\n\n10 0.5 1% 30/task 400 40 5\n0.1 1\n\nTable 2: Hyperparameters used for UARF implementation.\n\n13",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes approaches for determining when to add samples to replay buffer (in context of MBRL). It considers experiments on simple environments to show that the proposed method can perform at par with the baselines while keeping the size of the replay buffer small.\n\n# Strength And Weaknesses\n\n## Strength\n\n1. The paper studies an important problem - choosing which examples to save to the replay buffer (in the context of MBRL).\n\n## Areas for improvement\n\n1. Writing issues:\n\n    1.1 The paper needs to flow better. It starts with MBRL (which is an important problem to study), then jumps to Replay Buffer (RB) and Continual Learning (CL). The transition from MBRL to RB and CL needs to be clarified. Are RB-related issues the only challenges in MBRL, or why are those issues more important? Similarly, why should one care about CL in the context of MBRL? The authors need to motivate these questions better. \n\n    1.2 Related work needs references to approaches that select what examples are retained/sampled from the replay buffer in the context of CL methods. Given that the paper's primary focus is on reducing the size of the replay buffer and determining what examples/transitions to store, referencing such works is helpful.\n\n    1.3  Typos (like MRBL in line 192)\n\n    1.4 Need to include details like the number of seeds/trials for experiments.\n\n    1.5 Several phrases are not explained, e.g., \"complementary experiences\" in line 256, \n\n2. Experiments\n\n2.1 Several details are missing or glossed over. E.g., in line 254, \"we evaluate the proposed method in benchmarks environments for higher number of episodes than in Chua et al. (2018)\" - why more episodes? Or line 270, \"With similar training scenarios, Remonda et al. (2021) trained CP for 30 episodes, PU and RE for 150. Instead, we trained each for 100 episodes.\" - > why change the number of episodes?\n\n2.2 Several hyperparameters are introduced, but the corresponding ablations/analysis are missing.\n\n2.3 Reporting wall-clock time can be misleading as the wall-clock time depends on the load on the system when the time was recorded. A better metric to report is the number of floating-point operations\n\n2.4 It is not obvious to me what the difference is between BICHO and UARF\n\n2.5 In line 298, the paper notes, \"It takes longer to update the model as the replay buffer grows linearly.\" Is this because the system is trained on the entire replay buffer at every update step? Did the authors consider the variant where a fixed number of data points (respective of the size of the replay buffer) is used for an update? \n\n2.6 In line 263, the paper says, \"replay buffer size (which is free of any artificially-imposed limits)\" - it is unclear if the baseline approaches need the unlimited buffer size. Maybe a small buffer size (equal to the buffer size used by the proposed method) is sufficient for solving the task, and the extra examples are unnecessary. My suspicion grows stronger when I look at the first column of figure 1, where the models often reach quite close to the convergence performance in a few episodes. Still, the training seems to have continued to inflate the size of the replay buffer for the baseline methods. I would also like to see an ablation where the size of the replay buffer (for the baselines) is fixed, and older entries are thrown away in FIFO order as new entries become available.\n\n2.7 Regarding \"time per episode,\" I understand that the baselines would take longer because the replay buffers are larger (read 2.5 for a query on that), but this effect should not kick in when the number of episodes is very small. e.g., in Figure 1, for 0 episodes, I expect all the methods to take the same amount of time (maybe the proposed method takes a bit longer due to the computation of some variables). Still, the PETS baseline is taking much longer, even with 0 episodes. Why is that the case? Isn't the proposed method using the PETS model as the underlying MBRL method?\n\n2.8 In Section 6.2, the tasks considered are basically the same line-following task. Given that the agent has access to the distance from the closest point on the path, there is not much distinction between the tasks. It would not be surprising that an agent that (relatively) easily generalize across different task instances (presented as different tasks).\n\n2.9 The previous question (2.6) about the replay buffer size also applies here. It must be clarified if the baseline needs large buffers to solve the task.\n\n2.10 Generally, the paper needs to consider more complex tasks (with longer episode lengths) if the claims are about improving performance in the lifelong learning setup.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nCovered above.\n\n# Summary Of The Review\n\nAt this point, I think the paper needs several updates to both the writing and the experiments. I encourage the authors to point out mistakes / oversights in my review and I am open to changing my views and scores.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nNA",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing the experience replay buffer in Model-Based Reinforcement Learning (MBRL) through a method called Uncertainty Aware Replay Filtering (UARF). The key contributions include strategies for assessing the reliability of predictions made by the dynamics model, retaining only unpredictable experiences, and training with sufficient novel experiences. Empirical results demonstrate that UARF significantly reduces replay buffer size, training times, and mitigates the issue of catastrophic forgetting while maintaining competitive performance on benchmark tasks.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to tackle the redundancy problem in traditional experience replay methods and its applicability to continual learning scenarios, which is a growing area of interest in reinforcement learning. The empirical results are compelling, showcasing the benefits of UARF in terms of performance and efficiency. However, a potential weakness lies in the limited exploration of the theoretical underpinnings of the proposed method, which could enhance its understanding and applicability across various tasks. Furthermore, the experiments could benefit from a broader range of environments to fully validate the robustness of UARF.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, findings, and implications. The quality of the writing is high, making complex concepts accessible. The novelty of the UARF approach is significant, particularly in the context of continual learning in MBRL. Additionally, the authors provide source code and detailed instructions for reproducing the experiments, ensuring a high level of transparency and reproducibility in their research.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of Model-Based Reinforcement Learning by introducing a method that enhances the efficiency of experience replay in continual learning settings. While the empirical results are strong, further exploration of the theoretical aspects and a broader range of experiments could strengthen its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to Model-Based Reinforcement Learning (MBRL) aimed at optimizing experience replay buffers, particularly in the context of continual learning. The authors introduce the Uncertainty Aware Replay Filtering (UARF) algorithm, which selectively retains experiences that the model cannot predict reliably, thereby reducing both training times and memory usage. Key findings indicate that UARF achieves a significantly smaller replay buffer size while maintaining competitive performance across various tasks and effectively mitigating catastrophic forgetting in continual learning scenarios.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to enhancing memory efficiency, which is crucial for practical applications of reinforcement learning, particularly in environments that require continual learning. The robust empirical validation across multiple benchmarks demonstrates the effectiveness of the proposed method compared to traditional approaches. However, the paper presents some weaknesses, such as the sensitivity of UARF to hyperparameter tuning, which could complicate its application in diverse environments. Additionally, the method's performance may depend on the specific dynamics of the tasks, potentially limiting its generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions and findings. The methodology is detailed, allowing for potential reproducibility, although the complex parameter tuning might pose challenges in practice. The novelty of the approach is evident in the way it integrates uncertainty estimation into experience replay, marking a significant advancement in the MBRL landscape. Overall, the quality of the writing and the presentation of the results contribute positively to the paper's clarity and impact.\n\n# Summary Of The Review\nThis paper offers a significant contribution to the field of Model-Based Reinforcement Learning by introducing a method that enhances experience replay efficiency in continual learning contexts. While the approach is innovative and well-validated, it may require careful tuning and consideration of environmental dynamics to achieve optimal performance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to improving Model-Based Reinforcement Learning (MBRL) by addressing the challenges associated with experience replay buffers in continual learning settings. The authors propose an Uncertainty Aware Replay Filtering (UARF) mechanism that selectively retains experiences with poor model predictions, thereby curating the replay buffer to include only those experiences that are deemed \"unimaginable.\" The methodology involves leveraging probabilistic neural networks to model uncertainty in predictions and employing strategies that reduce the size of the replay buffer while maintaining or enhancing performance across various tasks. Experimental results demonstrate that UARF significantly reduces training times and the replay buffer size while mitigating issues like catastrophic forgetting.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to filtering experiences based on prediction uncertainty, which addresses a critical limitation in traditional experience replay methods. The experimental validation across different environments illustrates the robustness of the proposed method, showing notable improvements in efficiency and learning stability. However, the paper could benefit from a more extensive discussion on the limitations of the UARF approach, particularly regarding its applicability to more complex environments or tasks with higher dimensionality. Additionally, while the theoretical foundations are well-articulated, further clarification on the implementation details could enhance reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-organized, with logical progression from problem statement to methodology and results. The quality of writing is high, making the concepts accessible to a broad audience. The novelty of the approach is significant, as it introduces a new perspective on experience replay in MBRL by focusing on uncertainty management. However, the reproducibility of the results might be hindered by insufficient details regarding the experimental setup, hyperparameters, and specific implementations of the dynamics model and UARF algorithm.\n\n# Summary Of The Review\nOverall, the paper contributes meaningfully to the field of reinforcement learning by introducing a novel strategy for managing experience replay in MBRL. While it demonstrates clear empirical benefits, further elaboration on implementation details and limitations would enhance the paper's impact and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Memory of Unimaginable Outcomes in Experience Replay\" presents a novel strategy for experience replay in model-based reinforcement learning (MBRL). The main contributions include a new algorithm called Unimaginable Outcome Adaptation Replay Filter (UARF), which filters the replay buffer to retain experiences that the model struggles to predict. The authors demonstrate that this approach not only reduces training time and memory usage but also mitigates catastrophic forgetting. Empirical results indicate effective performance across multiple controlled environments, showcasing the practical applicability of the proposed method.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative approach to experience replay, which enhances learning efficiency by focusing on challenging experiences and addresses the traditional problem of catastrophic forgetting in continual learning. However, several limitations are noted: the reliance on the model's initial prediction accuracy may lead to discarding valuable experiences, and the method's effectiveness in dynamic, real-world scenarios remains uncertain due to the controlled nature of the experiments. Furthermore, while the algorithm is well-defined, its complexity may hinder practical implementation, suggesting a need for simplification or heuristic guidelines.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology, making it accessible to readers. The algorithmic framework is presented in a detailed manner, contributing to the work's reproducibility. The inclusion of a reproducibility statement, along with source code and instructions, is commendable. However, the discussion on the generalizability of results across different hardware setups could be improved to bolster the reproducibility claims further.\n\n# Summary Of The Review\nThis paper makes significant contributions to the field of experience replay in model-based reinforcement learning by introducing an innovative filtering approach that enhances learning efficiency and addresses catastrophic forgetting. While the proposed method shows promise, its reliance on predictive accuracy and the narrow scope of continual learning addressed are areas for improvement.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Memory of Unimaginable Outcomes in Experience Replay\" presents a novel approach to Model-Based Reinforcement Learning (MBRL) that prioritizes the retention of \"unimaginable experiences\" — those poorly predicted by the dynamics model. The methodology focuses on the selective retention of experiences to create a more efficient replay buffer aimed at reducing memory usage, training time, and the risk of catastrophic forgetting. Key contributions include a mechanism for identifying and retaining these experiences, dynamic model evaluation based on significant data, and a reliability assessment of predicted actions, leading to improved performance across various environments.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative approach to experience selection, which addresses common pitfalls in traditional experience replay methods. The empirical results demonstrate significant improvements in performance metrics and reductions in replay buffer size, indicating the methodology's practical utility. However, a potential weakness lies in the limited exploration of the filtering mechanism's implications for different task types, as well as the need for further validation across a broader range of environments beyond those tested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, findings, and implications, making it accessible to readers. The quality of the experiments is high, and the results are convincingly presented. The novelty of the approach is significant, as it introduces a new paradigm for experience replay in MBRL. Reproducibility is facilitated by detailed descriptions of the methodology and experimental setup, which should allow other researchers to replicate the results.\n\n# Summary Of The Review\nOverall, the paper offers a compelling and innovative contribution to the field of Model-Based Reinforcement Learning by redefining experience replay. The selective retention of unpredictable experiences presents a promising avenue for enhancing learning efficiency and mitigating catastrophic forgetting, although further exploration in diverse task settings is warranted.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Memory of Unimaginable Outcomes in Experience Replay\" introduces a novel approach to adversarial training in deep learning models by enhancing the experience replay mechanism. The main contributions include the development of a dynamic experience replay buffer that retains adversarial examples the model struggles to predict, thereby promoting efficient learning and adaptation to new adversarial strategies. The empirical results demonstrate significant improvements in model robustness against adversarial attacks while also reducing training time and memory usage.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to adversarial training, effectively addressing the limitations of traditional methods that rely on a fixed set of adversarial examples. The empirical validation across various benchmarks showcases strong improvements in robustness and efficiency. However, the paper lacks a thorough analysis of potential trade-offs regarding the exclusion of useful adversarial examples. Additionally, the exploration of the method's performance in complex or rapidly changing adversarial environments is limited, which could affect its practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and contributions. The quality of the empirical results is high, providing a solid foundation for the claims made. The novelty of the dynamic experience replay concept is apparent, and it is grounded in theoretical reasoning that connects to human learning behavior. However, further details on the reproducibility of the results and the criteria for evaluating retained adversarial examples would enhance the clarity and completeness of the study.\n\n# Summary Of The Review\nOverall, the paper presents a promising and innovative approach to adversarial training through the use of a dynamic experience replay mechanism, yielding significant improvements in model robustness. While the findings are compelling, a deeper exploration of the method's limitations and broader applicability would strengthen the paper's contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Memory of Unimaginable Outcomes in Experience Replay\" introduces a novel approach to Model-Based Reinforcement Learning (MBRL) with a focus on an innovative experience replay mechanism. The key contributions include a method for optimizing the replay buffer, which significantly reduces its size while enhancing learning efficiency, the ability to train agents in a fraction of the time compared to traditional algorithms, and a mechanism that purportedly eliminates catastrophic forgetting in continual learning scenarios. The authors present empirical results demonstrating superior performance over state-of-the-art methods, advocating for a fundamental shift in reinforcement learning practices.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its ambitious claims and the innovative design of the replay buffer optimization, which could address longstanding issues in reinforcement learning. The reduction in training time is another notable advantage, potentially allowing for faster deployment in real-world applications. However, the claim of completely eliminating catastrophic forgetting may be overstated, as such a feat has not been achieved consistently in prior literature. Additionally, while the experimental results are promising, the breadth of validation and robustness across diverse conditions remains a concern, which could limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that outlines the methodology and findings. However, some claims, particularly regarding the elimination of catastrophic forgetting, require more rigorous substantiation to ensure clarity and quality. The novelty of the replay buffer optimization and the filtering mechanism is commendable, yet the reproducibility of results is uncertain without extensive details on experimental setups and conditions. Overall, while the paper presents exciting concepts, further elaboration on the implementation and validation processes would enhance its clarity and reproducibility.\n\n# Summary Of The Review\nThis paper presents an innovative approach to experience replay in MBRL, with claims of significant improvements in training efficiency and memory management. While the contributions are noteworthy, the bold claims regarding catastrophic forgetting and the need for more comprehensive experimental validation temper the overall impact of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to model-based reinforcement learning (MBRL) through the introduction of the Unimaginable Experience Replay Framework (UARF). This framework aims to optimize experience replay by selectively retaining only unpredictable experiences, which the authors argue leads to reduced training times, smaller replay buffer sizes, and mitigated catastrophic forgetting. The methodology includes enhancements to predictive accuracy in the dynamics model and strategic thresholds for model updates. Experimental results across various environments demonstrate that while UARF effectively reduces buffer size and maintains performance, the improvements are less dramatic than initially claimed, particularly in complex tasks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to experience replay, which addresses key limitations of existing MBRL frameworks by focusing on the retention of relevant experiences. The experimental validation across multiple environments provides a solid foundation for the proposed methodology. However, the weaknesses include the modest performance improvements observed, which may not justify the complexity introduced by the new framework. Additionally, the paper acknowledges that the effectiveness of UARF can vary significantly based on task dynamics, suggesting a need for further refinement.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodologies and experimental setups. However, some results, particularly concerning performance gains, could benefit from deeper analysis and clearer presentation. The novelty of the proposed approach is significant, as it brings fresh insights into experience replay in MBRL. The reproducibility of the results appears feasible given the detail provided in the experiments, although more explicit guidelines for implementation would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the field of model-based reinforcement learning through its innovative approach to experience replay. While the proposed framework shows promise in reducing buffer sizes and mitigating forgetting, the empirical benefits may not be as pronounced as claimed, warranting further investigation and refinement.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper proposes a novel approach to experience replay in reinforcement learning, emphasizing the categorization of experiences into \"predictable\" and \"unimaginable.\" The authors introduce a method that filters experiences based on the confidence of a dynamics model, with the aim of optimizing the replay buffer size to enhance training efficiency. The findings indicate that this approach can reduce training time while addressing issues related to catastrophic forgetting in continual learning scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to experience selection, which could potentially lead to improved sample efficiency and training times. However, there are notable weaknesses, including the oversimplified binary classification of experiences, reliance on the accuracy of the dynamics model's confidence estimates, and insufficient exploration of the implications of fixed versus dynamic buffer sizes. Additionally, the lack of a comprehensive evaluation of the model's performance across diverse tasks raises concerns about the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly. The methodology is described in sufficient detail, allowing for reproducibility. However, the novelty is somewhat diminished by the lack of strong theoretical justifications for the assumptions made regarding experience filtering and the dynamics model's predictive capabilities. The empirical results, while promising, would benefit from a more extensive evaluation across varied environments to establish robustness.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to experience replay that could lead to improvements in training efficiency. While the methodology shows promise, several concerns regarding its assumptions, generalizability, and reliance on specific hyperparameters need to be addressed for the findings to be widely applicable in diverse scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper introduces a novel method called Uncertainty Aware Replay Filtering (UARF) aimed at improving experience replay efficiency in Model-Based Reinforcement Learning (MBRL). The authors propose retaining only unpredictable experiences in the replay buffer to reduce buffer size, training time, and mitigate issues such as catastrophic forgetting. The methodology is grounded in a probabilistic model that accounts for uncertainties in state predictions, and the experiments demonstrate that UARF effectively maintains a more compact replay buffer without compromising performance compared to existing methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to managing experience replay in MBRL by focusing on uncertainty, which is a critical aspect often overlooked in prior work. The experimental results provide strong evidence of the efficacy of UARF, showcasing tangible improvements in buffer size and training efficiency. However, one notable weakness is the potential overfitting of UARF to specific tasks, which may limit its generalizability across diverse environments. Additionally, the paper could benefit from a more extensive comparison with a broader range of baseline methods to better contextualize the performance gains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its objectives, methodology, and results. The quality of writing is high, with a logical flow that guides the reader through the technical details effectively. The novelty of the approach is significant, particularly in its focus on uncertainty quantification in experience replay. The authors provide source code and detailed instructions for reproduction, which enhances the paper's transparency and reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a compelling contribution to the field of Model-Based Reinforcement Learning by addressing key challenges in experience replay management. The proposed UARF method demonstrates clear advantages in efficiency and effectiveness, though there are some concerns regarding its generalizability across different tasks.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel framework for enhancing the robustness of neural networks against adversarial attacks, specifically focusing on a new regularization technique. The authors propose a method called Adversarial Regularization with Gradient Masking (ARGM), which integrates adversarial training and gradient masking into the training process. Empirical results demonstrate that ARGM significantly improves model performance on various benchmark datasets while maintaining robustness against known adversarial attacks.\n\n# Strength And Weaknesses\n**Strengths:**\n- **Innovative Approach**: The introduction of ARGM is a significant advancement in adversarial training methodologies, combining two effective strategies to bolster model robustness.\n- **Comprehensive Evaluation**: The paper includes thorough empirical evaluations across multiple datasets and attack scenarios, showcasing the effectiveness of the proposed method compared to existing techniques.\n- **Clear Presentation**: The structure and clarity of the paper make complex ideas accessible, enhancing reader comprehension.\n\n**Weaknesses:**\n- **Lack of Theoretical Justification**: While the empirical results are strong, the theoretical foundations of ARGM could be better articulated to explain why the combination of adversarial training and gradient masking leads to enhanced performance.\n- **Limited Scope of Experiments**: Although the paper evaluates various datasets, the range of adversarial attacks tested is somewhat narrow, which may limit the generalizability of the findings.\n- **No Ablation Study**: The absence of an ablation study makes it difficult to assess the individual contributions of the components within ARGM.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, making it easy for readers to follow the proposed methodology and results. The quality of the experiments is high, though the reproducibility could be enhanced by providing additional implementation details. The novelty of the approach is evident, as it combines established techniques in a new way, but the lack of theoretical backing may affect its perceived significance in the community.\n\n# Summary Of The Review\nThe paper presents a promising new methodology for enhancing the robustness of neural networks through adversarial regularization. While the empirical results are compelling, the theoretical justification and broader experimental scope require further attention to strengthen the overall contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to experience replay in model-based reinforcement learning (MBRL) by proposing the method of Unimaginable Action Replay Filtering (UARF). The authors argue that by retaining only experiences that the model predicts inaccurately, the efficiency of the replay buffer can be significantly improved. This strategy aims to reduce the memory requirements and training times associated with traditional experience replay methods while avoiding overfitting and catastrophic forgetting in continual learning scenarios. Empirical results demonstrate that UARF can substantially decrease the size of the replay buffer without compromising performance, particularly in environments requiring continual learning.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear identification of a critical challenge in MBRL—managing replay buffer size—and its innovative approach to address this issue through the selective retention of experiences. The methodology is well-structured, providing a clear rationale for focusing on unpredictable experiences, which aligns with the needs of continual learning. However, a potential weakness lies in the generalizability of the proposed method across diverse environments and tasks, as the experiments may be limited to specific scenarios. Additionally, while the concept of 'unimaginable outcomes' is intriguing, the operationalization of this idea could benefit from further elaboration and clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, with a coherent flow that makes it accessible to readers familiar with reinforcement learning concepts. The quality of the methodology appears robust, yet the novelty, while present, could be bolstered by a more comprehensive exploration of related works. The reproducibility of the findings would be enhanced by providing detailed experimental setups and hyperparameter settings, which are crucial for other researchers looking to replicate or build upon this work.\n\n# Summary Of The Review\nOverall, this paper makes a meaningful contribution to the field of model-based reinforcement learning by introducing a method that efficiently curates experiences in the replay buffer. While the approach shows promise in reducing memory and training overheads, further validation across a broader range of tasks is necessary to establish its full applicability and robustness.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Memory of Unimaginable Outcomes in Experience Replay\" presents a novel approach to enhancing experience replay in model-based reinforcement learning (MBRL). The authors introduce a method for curating replay buffers by retaining only unpredictable experiences, which is informed by uncertainty quantification. The proposed algorithm, UARF (Uncertainty-Aware Replay Filtering), is shown to effectively minimize buffer size, reduce training time, and mitigate catastrophic forgetting during continual learning. Empirical results demonstrate that UARF can maintain performance comparable to traditional methods while operating with significantly smaller replay buffers.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to experience replay, which addresses the critical issues of redundancy and overfitting in continual learning. The experimental validation across benchmark environments effectively showcases the benefits of UARF in terms of performance, buffer size, and training efficiency. However, a notable weakness is the lack of extensive exploration of the selection criteria's optimization and its implications on varying task dynamics, which could enhance the paper's depth and applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, methodologies, and findings. The technical details are presented in a manner that is accessible, although some sections could benefit from further elaboration on the underlying assumptions and limitations of the proposed methods. The novelty of the approach is significant, as it provides a fresh perspective on experience management in MBRL. However, reproducibility could be improved by providing more detailed descriptions of the experimental setup and hyperparameter choices.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of reinforcement learning by proposing a novel method for experience replay that enhances efficiency and learning performance. While the results are promising, further exploration of the selection criteria's optimization could provide deeper insights into the approach's applicability in diverse environments.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach to experience replay in Model-Based Reinforcement Learning (MBRL) through the Uncertainty Aware Replay Filtering (UARF) method. It addresses the limitations of traditional experience replay buffers by proposing a strategy that retains only diverse and unpredictable experiences, thereby reducing buffer size and training time while mitigating issues like catastrophic forgetting. The authors validate their approach across three experimental setups, demonstrating that UARF outperforms baseline methods in terms of performance metrics, including episode rewards and buffer efficiency.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to managing experience replay, which is a significant challenge in MBRL. The UARF method offers a clear advancement in maintaining a lean replay buffer, promoting efficiency in training while addressing issues of overfitting and redundancy. However, the paper could benefit from a more comprehensive exploration of the limitations of UARF, particularly in scenarios with highly dynamic environments or when faced with entirely novel tasks. Additionally, while the empirical results are promising, further validation across a wider range of tasks would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The writing is precise, and the logical flow of ideas facilitates understanding. The novelty of the UARF method is evident, addressing a gap in the current literature on experience replay in MBRL. The authors have also provided source code and instructions for reproduction, which enhances the reproducibility of the work and supports its empirical claims.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of Model-Based Reinforcement Learning by introducing a novel method for experience replay that effectively balances memory retention and training efficiency. The empirical validation and clarity of the presentation further bolster its impact, although a deeper exploration of potential limitations could enhance the overall robustness of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to experience replay in Model-Based Reinforcement Learning (MBRL) by proposing a method called Uncertainty Aware Replay Filtering (UARF). This method aims to enhance the efficiency of replay buffers by retaining only those experiences that the model has not accurately predicted. The authors demonstrate that this approach significantly reduces the size of the replay buffer while maintaining performance levels comparable to baseline methods. Additionally, UARF mitigates the problem of catastrophic forgetting, thereby improving the continual learning capabilities of agents within MBRL frameworks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to filtering replay experiences, which directly addresses the challenges of overfitting and inefficiencies associated with traditional experience replay methods. The empirical results substantiate the effectiveness of UARF, showing reduced training times and memory usage without sacrificing performance. However, a potential weakness is that while the methods are theoretically grounded and backed by empirical validation, the paper could provide more extensive comparisons with existing techniques and more diverse experimental settings to strengthen its claims further.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The clarity of the explanations, particularly regarding the UARF algorithm, is commendable. The quality of the empirical experiments is satisfactory, although additional experiments in varied environments could enhance robustness. The novelty of the approach is significant, offering a fresh perspective on experience replay in MBRL. The reproducibility is supported by a clear description of the experimental framework and metrics used, but inclusion of code or datasets would enhance this aspect further.\n\n# Summary Of The Review\nThis paper introduces an innovative filtering method for experience replay in MBRL, demonstrating significant improvements in efficiency and performance. While the contributions are substantial and the methodology is well-articulated, further validation across diverse tasks and additional resources for reproducibility would strengthen the overall impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel approach to reinforcement learning that emphasizes the use of experience replay buffers in conjunction with a dynamics model to enhance sample efficiency. The authors introduce various strategies aimed at filtering and selecting experiences deemed 'trustworthy' for training, which they argue can lead to improved learning outcomes. However, the paper also reveals limitations, such as the dynamics model's inability to accurately represent complex environments and the potential for increased computational costs.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to leveraging experience replay, which could theoretically enhance learning efficiency. However, the heavy reliance on experience replay introduces several weaknesses, including inefficiencies and potential overfitting on limited experiences. Furthermore, the complexity added by various proposed strategies could hinder practical applications. The vague definitions of key terms and insufficient experimental rigor in demonstrating the method's robustness across diverse tasks further detracts from the paper’s contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper suffers from a lack of clarity, particularly with vague terminologies like \"trustworthy imagination\" and \"unreliable model,\" which may confuse readers. The quality of the experiments conducted is questionable, as they do not adequately represent real-world applications or complex scenarios. While the paper attempts to present novel strategies, the overall novelty is diminished by the lack of rigorous theoretical foundations and empirical validation. Additionally, the reliance on hyperparameter tuning raises concerns about reproducibility and practical implementation.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to reinforcement learning through experience replay and dynamics modeling. However, it is plagued by significant limitations in clarity, rigor, and practical applicability, which undermine its contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Memory of Unimaginable Outcomes in Experience Replay\" introduces innovative strategies aimed at enhancing Model-Based Reinforcement Learning (MBRL) within continual learning environments. Its main contributions include a significant reduction in replay buffer size, which optimizes memory usage and accelerates the learning process. The authors demonstrate that their approach enables agents to focus on novel experiences while mitigating the risk of catastrophic forgetting, thereby allowing for effective continual learning. Experimental results indicate that the proposed method achieves robust performance across various tasks using fewer experiences, highlighting its practical implications in real-world applications.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its substantial contributions to both efficiency and capability in MBRL. The reduction of replay buffer size and the promotion of diverse experience capture are particularly noteworthy, as they directly address critical issues in reinforcement learning. Moreover, the self-management feature of the learning process enhances the adaptability of agents in dynamic environments. However, potential weaknesses include a lack of detailed exploration of certain implementation challenges and the need for further empirical validation across a broader range of tasks to fully establish generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the methodology accessible to readers. The high quality of the writing enhances understanding of complex concepts. The novelty of the approach is significant, particularly in its application to continual learning and its self-managing capabilities. However, while the experiments are promising, the reproducibility of the results could be strengthened by providing more comprehensive details of the experimental setup.\n\n# Summary Of The Review\nThis paper presents a compelling advancement in Model-Based Reinforcement Learning, addressing key challenges in continual learning through innovative techniques that enhance efficiency and adaptability. While the contributions are substantial, further empirical validation and clarity in implementation details would bolster the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a theoretical framework for enhancing experience replay in model-based reinforcement learning (MBRL) by focusing on the selective retention of experiences that the dynamics model struggles to predict. The authors propose methodologies for evaluating the reliability of model predictions, curating experiences based on novelty, and managing memory resources to improve learning efficiency. Key findings suggest that a more informed approach to experience selection can lead to accelerated convergence and better generalization in non-stationary environments, while also addressing challenges related to catastrophic forgetting in continual learning scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to experience replay, which aligns theoretical insights from cognitive science with practical strategies in MBRL. By focusing on the predictive weaknesses of the model, the authors provide a compelling argument for why selective retention could improve learning outcomes. However, the paper may lack empirical validation, as it primarily emphasizes theoretical contributions without presenting experimental results to support the proposed methodologies. Additionally, the reliance on theoretical constructs may limit the immediate applicability of the findings in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates complex concepts, making it accessible to a broad audience. The quality of writing is high, with a logical flow that guides the reader through the theoretical constructs. In terms of novelty, the integration of cognitive parallels and memory management strategies is noteworthy, although the lack of empirical evidence may hinder reproducibility. Future research directions are clearly outlined, suggesting pathways for validating the theoretical claims presented.\n\n# Summary Of The Review\nOverall, the paper makes significant theoretical contributions to the understanding of experience replay in MBRL, emphasizing the importance of selective retention of experiences based on predictive failures. However, without empirical validation, the practical implications of the proposed methodologies remain to be explored.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Memory of Unimaginable Outcomes in Experience Replay\" presents a novel approach to experience replay in reinforcement learning (RL) by introducing the Unreliable Action Replay Buffer (UARF). The methodology includes a probabilistic dynamics model that predicts the next state based on current state and action, incorporating both aleatoric and epistemic uncertainties. Key findings highlight UARF's effectiveness in efficiently managing replay buffer resources, reducing catastrophic forgetting, and improving continual learning through a selective retention of experiences that the model cannot accurately predict.\n\n# Strength And Weaknesses\nThe contributions of the paper are significant in advancing the management of experience replay in RL systems. The implementation of a probabilistic dynamics model and the selective retention strategy through UARF demonstrate a thoughtful approach to mitigating issues associated with fixed buffer sizes. However, the paper lacks a deeper discussion on the broader implications of the findings in continual learning contexts and does not sufficiently address the conceptual advancements over existing methodologies. The emphasis on technical details may detract from the perceived novelty of the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear descriptions of the algorithms and methodologies employed. The inclusion of specific hyperparameters and experimental setups enhances reproducibility, as the code is made available in the supplementary material. Nonetheless, while the technical execution is commendable, the novelty of the contributions could be better articulated, particularly in terms of their significance in the broader context of continual learning.\n\n# Summary Of The Review\nOverall, this paper offers a solid technical contribution to the field of reinforcement learning through an efficient experience replay strategy. However, it would benefit from a more detailed discussion of the implications of its findings and the novelty of its contributions in relation to existing approaches.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to model-based reinforcement learning (MBRL) named UARF, which claims to enhance efficiency by reducing sample complexity compared to model-free methods like Soft Actor Critic (SAC). The authors propose a new algorithm, BICHO, aimed at uncertainty estimation within reinforcement learning contexts. They assert that their method achieves faster training times and a more compact replay buffer while addressing issues such as catastrophic forgetting and experience replay optimization. However, the paper lacks comprehensive comparisons with state-of-the-art methods, raising questions about the robustness of the reported improvements.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its attempt to innovate within the reinforcement learning domain by proposing a new algorithm and addressing key challenges such as sample complexity and task transitions. However, the weaknesses are pronounced, particularly the lack of thorough benchmarking against existing methodologies. The authors fail to provide adequate empirical comparisons that could substantiate their claims regarding training efficiency, buffer size, and management of catastrophic forgetting. Furthermore, the discussion does not critically evaluate how their contributions relate to established techniques, leaving gaps in the analysis of their method's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by an insufficient engagement with existing literature, which could mislead readers regarding the novelty of the proposed methods. While the ideas presented are relevant, the execution lacks rigor, particularly in the reproducibility of results. The absence of detailed performance comparisons with previous studies diminishes the quality and reliability of the findings, making it challenging for other researchers to replicate the results or fully appreciate the significance of the contributions.\n\n# Summary Of The Review\nOverall, the paper presents interesting concepts in the realm of model-based reinforcement learning but falls short in providing rigorous empirical validation and critical analysis of its contributions against the background of existing work. The claims made regarding efficiency and effectiveness require more substantial backing through comparative studies to establish their true significance.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"MEMORY OF UNIMAGINABLE OUTCOMES IN EXPERIENCE REPLAY\" presents a novel approach to experience replay in reinforcement learning, specifically focusing on memory management within the context of model-based reinforcement learning (MBRL). The authors propose an innovative algorithm, referred to as UARF, which aims to significantly reduce the size of the experience replay buffer while maintaining effective learning performance. The methodology involves defining a maximum prediction distance (MPD) and employing a single-shot dynamics model to enhance the replay process. The findings demonstrate that the proposed method leads to a drastic reduction in replay buffer size while improving the agent's ability to handle less common states.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its novel approach to managing experience replay and the potential implications for efficiency in reinforcement learning models. The introduction of the MPD as a cutoff point for trajectory processing is a valuable contribution. However, the paper suffers from several weaknesses, including inconsistent formatting, vague terminology, and a need for clearer definitions of key concepts and acronyms. Additionally, the results are presented somewhat verbosely, which detracts from the overall clarity and impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by inconsistent terminology and formatting issues, such as the inconsistent use of acronyms and the need for clearer explanations of technical terms. While the novelty of the proposed UARF algorithm is notable, the paper could benefit from improved organization and presentation of results to enhance reader comprehension. Reproducibility is also a concern, as the algorithm is not described in sufficient detail for others to replicate the experiments effectively. Overall, the quality of writing and presentation needs significant improvement to meet the standards expected for ICLR submissions.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to experience replay in reinforcement learning with promising implications for model efficiency. However, it suffers from clarity issues, inconsistent formatting, and vague terminology that hinder its impact. Addressing these concerns would greatly enhance the paper's contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents new methodologies for managing replay buffers in reinforcement learning (RL), aiming to improve sample efficiency and learning stability. The authors propose various strategies for buffer management, including adaptive techniques that adjust to the dynamics of the learning environment. The findings suggest that these new approaches outperform conventional methods in specific scenarios, although the paper does not extensively investigate their behavior under diverse environmental conditions or with varying complexities.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to replay buffer management, which has the potential to enhance the performance of RL algorithms. However, the paper has several weaknesses. It lacks a thorough exploration of the impact of environmental noise on the proposed methods, which is critical for real-world applications. Additionally, there is insufficient analysis of how these techniques could be integrated with other state-of-the-art RL algorithms, potentially missing opportunities for greater improvement. The paper also falls short in addressing the scalability of the proposed methods and their adaptability to non-stationary dynamics, which are prevalent in many practical scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a structured presentation of methodologies and results; however, the lack of detailed discussions on practical implications and limitations may hinder the overall understanding of the proposed approaches. The quality of the experiments appears sound, but a more comprehensive analysis comparing computational overhead against traditional methods would enhance the quality of the findings. The novelty is evident in the proposed techniques, although their applicability across different RL paradigms remains unexplored. Reproducibility could be improved through clearer guidelines on the implementation of the methodologies and more extensive empirical validation.\n\n# Summary Of The Review\nThe paper provides a promising contribution to the field of reinforcement learning through innovative replay buffer management techniques. However, it lacks depth in exploring various practical aspects and environmental challenges that could limit the applicability of its findings. Overall, while the proposed methods are interesting, further exploration and validation in diverse contexts are essential.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel framework for Model-Based Reinforcement Learning (MBRL) termed UARF (Uncertainty-Aware Replay Framework). The authors emphasize the importance of statistical evaluation in assessing performance metrics such as accumulated rewards, particularly in the context of experience replay buffers. The methodology includes a series of experiments that investigate the effects of buffer size, continuous learning, and catastrophic forgetting, all supported by rigorous statistical validation. The findings indicate that UARF outperforms existing baseline methods, demonstrating improved performance metrics and reduced instances of catastrophic forgetting.\n\n# Strength And Weaknesses\nStrengths of the paper include its thorough exploration of significant aspects of MBRL, particularly the role of experience replay buffers and the statistical evaluation of performance improvements. The authors provide a clear experimental design that effectively addresses critical challenges in the field, such as catastrophic forgetting and prediction reliability. However, the paper could strengthen its claims by providing more explicit statistical analyses to support its findings, particularly in terms of significance testing for various performance metrics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulates its contributions and methodology. However, certain sections could benefit from enhanced clarity, particularly in the presentation of results, where the inclusion of error bars and confidence intervals would improve the understanding of variability and reliability. The novel aspects of the UARF framework present a significant contribution to the field, yet reproducibility could be enhanced by a more detailed documentation of statistical methods and software used in the analysis.\n\n# Summary Of The Review\nOverall, the paper provides a valuable contribution to the field of MBRL through the introduction of the UARF framework and its rigorous statistical approach. While it effectively addresses key challenges, improved clarity in results presentation and a stronger emphasis on statistical validation would enhance the paper's impact and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach to memory management in reinforcement learning (RL) for continual learning scenarios, focusing on the retention of useful experiences in a replay buffer. The proposed methodology aims to enhance the model's ability to learn from past experiences while adapting to new tasks, using techniques to predict useful memory retention. The findings indicate improvements in short-term performance metrics in specific environments, particularly within the MuJoCo framework.\n\n# Strength And Weaknesses\nWhile the paper makes significant contributions to memory retention strategies in RL, it has notable weaknesses. The limited exploration of how the proposed methods perform in environments with conflicting dynamics raises concerns about the model's robustness in more complex scenarios. Additionally, the absence of task boundary detection may hinder the effectiveness of memory management in non-stationary environments. The lack of comprehensive memory management strategies and automated hyperparameter tuning further limits the applicability of the approach. Furthermore, the paper does not sufficiently compare the proposed method with a broader range of existing methodologies, which could provide a clearer context for its effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written clearly, with a logical structure and adequate explanations of the methodology and findings. However, the novelty is somewhat constrained by the lack of comprehensive comparisons and discussions on generalizability. While the experiments demonstrate the method's effectiveness in specific environments, the reproducibility may be challenged by the absence of automated hyperparameter tuning and detailed evaluations on long-term performance and overfitting risks.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the field of continual learning in reinforcement learning but is limited by its narrow focus on specific environments and the lack of comprehensive strategies for memory management. Future work should address these limitations to enhance the applicability and robustness of the proposed methods.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper \"Memory of Unimaginable Outcomes in Experience Replay\" proposes an approach to enhance model-based reinforcement learning (MBRL) by optimizing the experience replay mechanism. The authors introduce the concept of a \"lean replay buffer,\" which retains only those experiences deemed \"unimaginable,\" thereby purportedly improving sample efficiency and reducing overfitting. They present a new algorithm, BICHO, designed to filter experiences based on predictive uncertainty. Experimental results indicate that their method achieves reduced buffer sizes and training times compared to baseline methods, although these findings are not particularly surprising given the established goals of MBRL.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its clear identification of problems in traditional experience replay methods and its straightforward experimental design. However, the contributions are undermined by a lack of novel insights, as many concepts are rehashed from existing literature. The complexity of the proposed algorithm could also be seen as a barrier to understanding and implementation, detracting from the paper's overall utility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but the clarity suffers from the use of jargon and unnecessarily complicated explanations. The novelty of the approach is questionable, as many of the ideas presented are already well-known in the field of reinforcement learning. Reproducibility is likely achievable due to the straightforward nature of the experiments, but the lack of true innovation diminishes motivation for others to replicate the work.\n\n# Summary Of The Review\nOverall, this paper does not make a significant contribution to the field of model-based reinforcement learning. While it addresses relevant issues, the solutions proposed are neither novel nor compelling, and the complexity of the methodology could hinder understanding and application.\n\n# Correctness\n4/5 - The methodology appears sound, but the novelty of the findings is minimal.\n\n# Technical Novelty And Significance\n2/5 - The technical contributions are largely derivative and do not advance the field in a meaningful way.\n\n# Empirical Novelty And Significance\n2/5 - While the empirical results are valid, they do not provide new insights or significantly improve upon established methods in reinforcement learning.",
    "# Summary Of The Paper\nThe paper presents a novel approach to experience replay in model-based reinforcement learning (MBRL), introducing the concept of \"unimaginable experiences\" to optimize the replay buffer's efficiency. The authors propose the UARF method, which retains only high-value experiences to improve the predictive capacity of the dynamics model. Experimental results demonstrate that UARF not only reduces training time and buffer size but also effectively addresses the issue of catastrophic forgetting in continual learning settings.\n\n# Strength And Weaknesses\nThe main strength of this paper is its innovative approach to experience replay, particularly the focus on maintaining a lean replay buffer through the retention of \"unimaginable experiences.\" This is aligned with ongoing research in memory management and enhances learning efficiency. However, the paper could benefit from a more comprehensive evaluation of experience novelty and the integration of advanced techniques such as probabilistic uncertainty quantification. Additionally, although the experiments show promising results, there is a need for comparative studies against state-of-the-art methods to validate the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its methodology and findings. The novelty of the proposed UARF method is evident, although the paper could further elaborate on its theoretical foundations. The reproducibility of the results may be improved by providing more detailed experimental setups and parameter settings. Overall, the quality of the work is commendable, but the inclusion of additional details could enhance clarity and reproducibility.\n\n# Summary Of The Review\nThis paper introduces a compelling approach to experience replay in MBRL, with promising experimental results supporting the proposed method. While the contributions are significant, further exploration of advanced techniques and comparative analyses would strengthen the findings and provide a more comprehensive understanding of the methodology's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Uncertainty Aware Replay Filtering (UARF) method aimed at enhancing continual learning in reinforcement learning (RL) environments. Through a series of experiments, the authors demonstrate that UARF significantly reduces the size of the replay buffer while maintaining competitive performance metrics, such as per-episode rewards and training time. The findings indicate that UARF effectively mitigates issues related to catastrophic forgetting and experience retention, showcasing its potential for improving efficiency in continual learning scenarios.\n\n# Strength And Weaknesses\nThe primary strength of this work lies in its comprehensive evaluation of UARF against established baseline algorithms, such as BL and BICHO. The results highlight UARF's ability to maintain a drastically smaller replay buffer size—up to 10 times smaller—while achieving similar or improved performance in terms of rewards and computational efficiency. Additionally, the experimental design is robust, effectively illustrating the advantages of UARF in managing experience retention and reducing training time. However, the paper could benefit from a deeper exploration of the underlying mechanisms that contribute to UARF’s performance improvements and its applicability to a broader range of RL environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings, making it accessible to readers. The quality of the experiments is high, and the results are presented with sufficient detail to allow for reproducibility. However, while the novelty of the UARF approach is noted, the paper could enhance its contribution by providing more theoretical insights into why UARF outperforms the baselines at a fundamental level.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of continual learning in reinforcement learning through the introduction of the UARF method. The empirical results strongly support its claims, demonstrating significant improvements in efficiency and performance. The paper’s clarity and methodological rigor make it a valuable addition to the literature, although further theoretical elaboration could strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to [insert specific problem or area of research], introducing [briefly describe the methodology or techniques employed]. The main contributions include [list key contributions], which were evaluated through [describe experimental setup or analysis]. The findings reveal that [summarize the main results and their implications], highlighting the potential of this approach in [discuss broader impacts or applications].\n\n# Strength And Weaknesses\nThe paper makes significant contributions by [describe strengths, such as addressing gaps in the literature, proposing innovative solutions, etc.]. However, it also has notable weaknesses, including [mention weaknesses like clarity issues, structural problems, or overly complex jargon]. The repetitive phrasing and excessive technical terminology may hinder engagement and accessibility for a broader audience. Additionally, the clarity in stating the contributions could be improved by presenting them in a more structured format, such as bullet points or numbered lists.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel approach, clarity suffers due to structural issues and lengthy sentences that complicate comprehension. The use of passive voice and technical jargon further detracts from readability. The paper would benefit from clearer transitions between sections and concise reiteration of essential points. Regarding reproducibility, while the methodology is described, the clarity of figures, tables, and references may impede attempts by others to replicate the findings. Overall, the quality of presentation needs significant improvement to enhance accessibility.\n\n# Summary Of The Review\nIn summary, the paper presents valuable contributions to [specific domain], but suffers from clarity and structural issues that detract from its overall impact. Addressing these concerns could significantly enhance the paper's readability and engagement with a wider audience.\n\n# Correctness\n4/5 - The methodology appears sound, and the findings are generally well-supported, although some clarity issues may introduce minor ambiguities.\n\n# Technical Novelty And Significance\n4/5 - The proposed approach is innovative and addresses a relevant problem within the field, contributing meaningful advancements to existing methodologies.\n\n# Empirical Novelty And Significance\n3/5 - While the empirical results demonstrate the approach's effectiveness, the presentation of these results lacks clarity and may not fully convey their significance to the broader research community."
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.334650000168666,
    -1.6664526032069558,
    -1.659552222822967,
    -1.644894085654054,
    -1.9492668823389483,
    -1.7708821994515043,
    -1.822136605842743,
    -1.8670126519553905,
    -1.771198156770063,
    -1.772916403842195,
    -1.563376852274253,
    -1.359956611144144,
    -1.635292467708274,
    -1.7636160490262218,
    -1.6562674903843126,
    -1.5971595556060383,
    -1.9401686307305985,
    -1.8730077960849718,
    -1.7440622326880606,
    -2.047180445810201,
    -1.9773723267922398,
    -1.8699763533385123,
    -1.838732572375316,
    -1.8315798136085952,
    -1.8569707667181163,
    -1.8125664240889885,
    -1.8618076309892984,
    -1.6246871025153655,
    -1.771794482524497
  ],
  "logp_cond": [
    [
      0.0,
      -2.2330757667484633,
      -2.2481100992956793,
      -2.243156931262186,
      -2.2428801047396236,
      -2.2602694582890788,
      -2.2643336351387418,
      -2.259995508896514,
      -2.243091117357049,
      -2.255413145797962,
      -2.24654495135606,
      -2.257968123806251,
      -2.2533299438184673,
      -2.2466254691762604,
      -2.243887106174059,
      -2.240094618424056,
      -2.2619679160275097,
      -2.2539272836511355,
      -2.27377869157342,
      -2.2495965605348403,
      -2.225061366024229,
      -2.2463901705697062,
      -2.2680558168457186,
      -2.254152239719108,
      -2.262625505357965,
      -2.249071271419105,
      -2.2427890954850116,
      -2.222951532685568,
      -2.253218570768735
    ],
    [
      -1.263266680825647,
      0.0,
      -1.1377875954941017,
      -1.1203907146911758,
      -1.208433943247888,
      -1.2277942167664062,
      -1.2903185481211286,
      -1.2531911662932123,
      -1.2267070273196288,
      -1.2489963638751007,
      -1.1384047682476084,
      -1.3378374920554628,
      -1.1559163912058668,
      -1.1210601578489923,
      -1.115462584495443,
      -1.1221917369196162,
      -1.2994336294974718,
      -1.2040132216713066,
      -1.2458655190390273,
      -1.1914880602564741,
      -1.2455146983645602,
      -1.2605496841413402,
      -1.324577859316688,
      -1.2556250862982468,
      -1.2783664468861173,
      -1.3283564331068733,
      -1.166482659989303,
      -1.1910887062852893,
      -1.4238316913022036
    ],
    [
      -1.2868689684588437,
      -1.1457769284119133,
      0.0,
      -1.1744710642093499,
      -1.2693160140303363,
      -1.2977686002290065,
      -1.3265812839785582,
      -1.2733739297587556,
      -1.2478163583074797,
      -1.292541980679817,
      -1.2321623923472877,
      -1.3795061472346928,
      -1.221822913660868,
      -1.213865846791071,
      -1.2330270940900918,
      -1.2253387235758135,
      -1.320544785608967,
      -1.2449827241283553,
      -1.3578331363111005,
      -1.2447168641307729,
      -1.256872229165131,
      -1.2429832787198065,
      -1.3679776549088773,
      -1.3252330399634495,
      -1.2920671441102216,
      -1.348458308912379,
      -1.214218558903141,
      -1.2402374285363404,
      -1.4061609252696896
    ],
    [
      -1.289472093695632,
      -1.1282129354693735,
      -1.206086934858093,
      0.0,
      -1.255446602844734,
      -1.2028783925905155,
      -1.297477599602634,
      -1.2495792085684287,
      -1.2046117952461401,
      -1.2314258768409307,
      -1.2257465334604396,
      -1.3803650603439,
      -1.180214786397854,
      -1.1739616503021886,
      -1.2205913904476149,
      -1.1925709159802838,
      -1.2723447681826763,
      -1.226596172370872,
      -1.3187072351494977,
      -1.1810795808242749,
      -1.2409051357394711,
      -1.1811550580623587,
      -1.3593186941823086,
      -1.2960581387085488,
      -1.3091146137192107,
      -1.306552585964436,
      -1.1769188661588879,
      -1.2506674783101244,
      -1.3896005941799594
    ],
    [
      -1.6271082459835102,
      -1.5144303080010455,
      -1.540387218669778,
      -1.5407052336719418,
      0.0,
      -1.4408315369769362,
      -1.4839722486477693,
      -1.3616947635455894,
      -1.4756127008738866,
      -1.4927263991567836,
      -1.512533756937661,
      -1.6309879167738872,
      -1.4331405751907977,
      -1.4344108644166251,
      -1.5252064039728463,
      -1.4966663576786463,
      -1.5892259818205319,
      -1.3893606179978593,
      -1.532075404629895,
      -1.4436223451001144,
      -1.5665973706658565,
      -1.4610781427492952,
      -1.6331481877876992,
      -1.6027832316938917,
      -1.575753537465941,
      -1.5106422118102592,
      -1.4201349358025939,
      -1.6076375424600482,
      -1.6178280092358612
    ],
    [
      -1.4947252919420126,
      -1.322871053942768,
      -1.3762169833120343,
      -1.3489199162065206,
      -1.2537515676912976,
      0.0,
      -1.3525067650008589,
      -1.2298336276412694,
      -1.2590386386490633,
      -1.3527806657995927,
      -1.3516184891979883,
      -1.4681127533150427,
      -1.291243371857148,
      -1.2616265502400632,
      -1.3772520892339424,
      -1.3721217440264235,
      -1.4315510400680307,
      -1.2240480667486497,
      -1.3312407307977443,
      -1.2835742928769818,
      -1.4312829354969872,
      -1.3454072485557802,
      -1.480740945452771,
      -1.4031458684087792,
      -1.4276055792250062,
      -1.3266004020405486,
      -1.324884462961583,
      -1.4133041073011872,
      -1.4928981724927863
    ],
    [
      -1.5346225554309934,
      -1.451446877162841,
      -1.4410818702264279,
      -1.4185762021457535,
      -1.3381853841761535,
      -1.341473448029492,
      0.0,
      -1.3188264864294705,
      -1.490491977823542,
      -1.4512947481090301,
      -1.5031953026784297,
      -1.4944875782347582,
      -1.4323348938425564,
      -1.3301167192531413,
      -1.4479301823211939,
      -1.4677298707464748,
      -1.4553785096243543,
      -1.344599415448276,
      -1.4654701548135394,
      -1.350355243842211,
      -1.466187526992173,
      -1.3975709221343253,
      -1.4574532630075725,
      -1.4793397785796512,
      -1.46145442869623,
      -1.4485818793352734,
      -1.4099885133564667,
      -1.4808052792568658,
      -1.5678514459397639
    ],
    [
      -1.6367696944926606,
      -1.4898241214764756,
      -1.5094700705200836,
      -1.5212774820744988,
      -1.3999249362770867,
      -1.3887121185099738,
      -1.4432864042818248,
      0.0,
      -1.4451293930034803,
      -1.5219183476638043,
      -1.4872062869995042,
      -1.5971547390019596,
      -1.4721546523491027,
      -1.4234210805698146,
      -1.4887139014729074,
      -1.5424620445610402,
      -1.5436442636926135,
      -1.2823930220481004,
      -1.4672308399787624,
      -1.4620093508473697,
      -1.5019430285242839,
      -1.4655541708453268,
      -1.5617970434658637,
      -1.5741446833288288,
      -1.5630726972296496,
      -1.4592904731512173,
      -1.4556958186858644,
      -1.5819188462012268,
      -1.6012164477657629
    ],
    [
      -1.4317514314135638,
      -1.327522127786209,
      -1.331603908284007,
      -1.3291190327436035,
      -1.369380644150063,
      -1.278795767358407,
      -1.4466676254500157,
      -1.3280922737068674,
      0.0,
      -1.403547795619156,
      -1.291464482504859,
      -1.5010715536271642,
      -1.301102462374512,
      -1.304299079794646,
      -1.351792764291775,
      -1.3612965662565368,
      -1.4642254292486072,
      -1.3158451717340112,
      -1.3699886441776432,
      -1.357572702171779,
      -1.3830295154682644,
      -1.3075302616590125,
      -1.4862503479715823,
      -1.4239345545477529,
      -1.4518937524181843,
      -1.4044570089802004,
      -1.2972262879518475,
      -1.380073492647078,
      -1.4762767777643102
    ],
    [
      -1.4028591301997804,
      -1.2977812165604738,
      -1.3454996232024528,
      -1.309722654341612,
      -1.3207735419396822,
      -1.3293897650776598,
      -1.3694530246919687,
      -1.3018003120886878,
      -1.3784246851690012,
      0.0,
      -1.395223148449376,
      -1.4499624691445203,
      -1.354961031846194,
      -1.3237904966677698,
      -1.3760244697860902,
      -1.3723670722538441,
      -1.2954911899567538,
      -1.3715300395336023,
      -1.310655879917984,
      -1.3160254943732959,
      -1.4103958443517703,
      -1.3238278791511706,
      -1.4408614812198441,
      -1.4134437244555096,
      -1.3007892238697112,
      -1.3818266727071915,
      -1.2403538921379218,
      -1.3987227752502485,
      -1.4932597341223326
    ],
    [
      -1.2039572214170806,
      -1.0506501629103586,
      -1.1244716932310141,
      -1.1396485051186038,
      -1.123834888727766,
      -1.182162060743276,
      -1.2407810694217702,
      -1.1794675771451573,
      -1.1006694208622374,
      -1.2250785251124656,
      0.0,
      -1.3127534475498286,
      -1.1320800515960903,
      -1.0896223823277993,
      -1.048104403919602,
      -1.0979168087151703,
      -1.2528267241798434,
      -1.1823174092520967,
      -1.1916125714724393,
      -1.1507649337277894,
      -1.2020314017577771,
      -1.1768918384908433,
      -1.2904453874192021,
      -1.2048129415220696,
      -1.231580702158958,
      -1.1924915866370478,
      -1.1335460968543516,
      -1.2115895059640625,
      -1.2484526373477012
    ],
    [
      -1.0871053462826963,
      -1.0565211499214646,
      -1.0917789693268227,
      -1.0736500279973877,
      -1.068447782145166,
      -1.0845145526899949,
      -1.0672280603825217,
      -1.0716273022211495,
      -1.0935267615020472,
      -1.068735746222041,
      -1.0921236558733805,
      0.0,
      -1.0799097692660269,
      -1.0634369999849145,
      -1.0663297705521841,
      -1.0606542665451242,
      -1.0925288463047005,
      -1.0722260371874024,
      -1.077156810245516,
      -1.0206746085653484,
      -1.0626111221722514,
      -1.0450041452966405,
      -1.0583411108453578,
      -1.0688480128002178,
      -1.0599942758641727,
      -1.1305803884796408,
      -1.0618643449905214,
      -1.069718618647783,
      -1.0720217514391939
    ],
    [
      -1.3371396485577187,
      -1.2183659923741497,
      -1.2700832225184515,
      -1.2467347679562415,
      -1.1973795104270533,
      -1.2373853211839134,
      -1.3088336317284255,
      -1.1835284012971208,
      -1.2222764759848135,
      -1.2930687929563263,
      -1.2292655895127123,
      -1.3773572566793157,
      0.0,
      -1.1753300000489992,
      -1.283735903751381,
      -1.2168561425790991,
      -1.3808945147922624,
      -1.1845985990367134,
      -1.290775263622745,
      -1.2064583555339732,
      -1.3301691384707257,
      -1.2619661970117468,
      -1.4012002591920882,
      -1.3385317055946044,
      -1.3475030594919841,
      -1.2719569975046776,
      -1.150721855968264,
      -1.328670793950994,
      -1.3813281966738453
    ],
    [
      -1.3791967259454243,
      -1.226690178075385,
      -1.249310757797014,
      -1.241659309794474,
      -1.2655362823785576,
      -1.2498967663660314,
      -1.2984407667647753,
      -1.263389289647243,
      -1.250538881030463,
      -1.3096002819762111,
      -1.2739236569336791,
      -1.439827761166239,
      -1.2408083820343376,
      0.0,
      -1.2689905803282222,
      -1.2632523525600903,
      -1.4097257350482488,
      -1.2388329236497468,
      -1.3850290575424957,
      -1.241008455909773,
      -1.3297078054359996,
      -1.201692561573928,
      -1.4254944802031004,
      -1.376917346987569,
      -1.3763954367617026,
      -1.313087995017133,
      -1.2668615294837664,
      -1.3058800043053316,
      -1.4743265496471802
    ],
    [
      -1.29243554731403,
      -1.132646502664163,
      -1.2009356454773383,
      -1.1991584224592866,
      -1.2142111872462447,
      -1.2691912152063076,
      -1.3106651096560968,
      -1.2801690633040514,
      -1.239052997559437,
      -1.2988874068324885,
      -1.1064671963025217,
      -1.3897139632475213,
      -1.201892234394871,
      -1.1748199124409588,
      0.0,
      -1.1963537287188564,
      -1.3438868583272827,
      -1.2618777950059192,
      -1.3611329879112188,
      -1.2131206886884258,
      -1.252070211107814,
      -1.234957064132339,
      -1.3570742162085678,
      -1.2892831678813823,
      -1.2848277138270618,
      -1.3301854855482205,
      -1.1970841502987326,
      -1.2305273174068114,
      -1.41111706100386
    ],
    [
      -1.2466417593007928,
      -1.128891323193281,
      -1.157738273141831,
      -1.1475063595960224,
      -1.1684286297185267,
      -1.2201073701083978,
      -1.2571925800802222,
      -1.176023686599136,
      -1.189169321631275,
      -1.267279111585642,
      -1.1674397250995567,
      -1.315915569937098,
      -1.1082115563538892,
      -1.1495788903922781,
      -1.1684298390986352,
      0.0,
      -1.2652930895602055,
      -1.1767723018052443,
      -1.2388321500065242,
      -1.2101409874797673,
      -1.2473295582969568,
      -1.2150069273893866,
      -1.3218448432627083,
      -1.218043700603521,
      -1.2809361474636314,
      -1.270970707174705,
      -1.1577975418099933,
      -1.209990327517919,
      -1.33995384720306
    ],
    [
      -1.6422705949671588,
      -1.5858126946334559,
      -1.6229650805578237,
      -1.5769751473394658,
      -1.6526704868892224,
      -1.6037058430534128,
      -1.602903835590506,
      -1.644260249917151,
      -1.6240408342726171,
      -1.5314189904246407,
      -1.6551648004007278,
      -1.6569907645628128,
      -1.6688463635202515,
      -1.6180919882670644,
      -1.6066525059523515,
      -1.6207539232294996,
      0.0,
      -1.6523804052691298,
      -1.6546499851447254,
      -1.5922496013548746,
      -1.6104336073546985,
      -1.5796182779500554,
      -1.623166591326165,
      -1.605551568739478,
      -1.5993868870278374,
      -1.63409931201623,
      -1.5788818549793069,
      -1.6251363206578917,
      -1.6800893504594556
    ],
    [
      -1.5784638423389385,
      -1.469428710455225,
      -1.521160168812039,
      -1.4725224714191152,
      -1.4104988797128326,
      -1.4246998085606508,
      -1.4656110199813674,
      -1.3086449038068497,
      -1.4697141678062182,
      -1.5028767026378789,
      -1.4844001223272116,
      -1.5620745095247073,
      -1.4315234721739702,
      -1.406060872317605,
      -1.5255954975740105,
      -1.4858836943597604,
      -1.524327300946261,
      0.0,
      -1.4951829601112663,
      -1.4333103354255015,
      -1.5192557310231298,
      -1.4447937365187091,
      -1.5724481583343304,
      -1.5356853540854694,
      -1.5459074711853977,
      -1.4433572787810673,
      -1.4926996633570846,
      -1.5590212530101235,
      -1.5633674566762117
    ],
    [
      -1.4500298218550354,
      -1.35882508394727,
      -1.3941954110927959,
      -1.3542742395621588,
      -1.3565064751727802,
      -1.324621402489935,
      -1.4319551474316063,
      -1.3266539305677811,
      -1.3589131223848079,
      -1.368526002180445,
      -1.3812230677465984,
      -1.4585869194317291,
      -1.3372298790159798,
      -1.401277232981889,
      -1.4516497975197469,
      -1.4289127440786473,
      -1.4000409686716644,
      -1.3358198355275155,
      0.0,
      -1.339846689728667,
      -1.4340104822080861,
      -1.375008765126452,
      -1.466233569951719,
      -1.4250757435739874,
      -1.387982804808535,
      -1.4329019072796008,
      -1.3549842189597026,
      -1.4379844122140144,
      -1.4437865519647282
    ],
    [
      -1.655531146091549,
      -1.5903507372768637,
      -1.5784087004557548,
      -1.6055181610234277,
      -1.4921831652447606,
      -1.5512146918210554,
      -1.5948514854017772,
      -1.5624578959394675,
      -1.5970034358483804,
      -1.5975620852964925,
      -1.626799609519077,
      -1.7099582200321881,
      -1.5490650235681576,
      -1.4988483003550486,
      -1.6150245821402704,
      -1.6140914380377087,
      -1.6765292746112808,
      -1.5538478969539347,
      -1.6516345888267376,
      0.0,
      -1.6326212171540144,
      -1.5259915273237108,
      -1.7075772583225433,
      -1.6345495707437945,
      -1.6537999820836233,
      -1.663965088146558,
      -1.5498713157108084,
      -1.6553408167055375,
      -1.7202598401848976
    ],
    [
      -1.620724590353065,
      -1.5901824028223495,
      -1.5384854194552324,
      -1.561696584540815,
      -1.6359172445720462,
      -1.6580966647601618,
      -1.6283262502824731,
      -1.5833053632060252,
      -1.5997739185929216,
      -1.611662173162248,
      -1.6415038644801003,
      -1.6748813955930093,
      -1.632065930412664,
      -1.6017850718836177,
      -1.594171811853433,
      -1.6130742775184714,
      -1.6309024351438373,
      -1.625183059468079,
      -1.6484649996195975,
      -1.565663115513852,
      0.0,
      -1.5830455493853213,
      -1.6374257473746008,
      -1.5620747868532485,
      -1.6192741640419557,
      -1.6459764557928207,
      -1.5552071212701688,
      -1.578348811919189,
      -1.6794703223491985
    ],
    [
      -1.511729882590424,
      -1.4672338531796336,
      -1.4929857489027647,
      -1.4618157398417713,
      -1.3916183089503222,
      -1.435174553431329,
      -1.4441419984783679,
      -1.39747487604375,
      -1.4570962064972577,
      -1.476679278856879,
      -1.486338587653202,
      -1.5577364008015095,
      -1.4572846377341158,
      -1.3721302751338622,
      -1.475748056467488,
      -1.4779240738592436,
      -1.4910304482084167,
      -1.4243193176632338,
      -1.5173044639127622,
      -1.3638806926131357,
      -1.4774783274419894,
      0.0,
      -1.5351650121186635,
      -1.4777092435365773,
      -1.5022581397940742,
      -1.4917753019358428,
      -1.4425476041097332,
      -1.5029421784000594,
      -1.543662401939972
    ],
    [
      -1.5391509171482387,
      -1.5300466777307913,
      -1.5083760077735158,
      -1.5345291603000428,
      -1.4993661393362454,
      -1.5184380355328768,
      -1.4893120415626375,
      -1.4791727883366792,
      -1.5485946685284702,
      -1.4784753826401356,
      -1.5475122320916646,
      -1.5358173423308117,
      -1.5440863695081317,
      -1.5082577137877784,
      -1.5216187636257867,
      -1.531239952537705,
      -1.4731221496261822,
      -1.4916613697437688,
      -1.5044665711482519,
      -1.4961772742562884,
      -1.4998903940373378,
      -1.5195205829396294,
      0.0,
      -1.4910407799539986,
      -1.429690101690194,
      -1.5774592083960892,
      -1.5063163073911139,
      -1.5279482809272202,
      -1.5680824346609146
    ],
    [
      -1.4511745372917784,
      -1.407072121321125,
      -1.4102175408795266,
      -1.431696800529742,
      -1.4590870089702082,
      -1.4613250652411895,
      -1.4908924259182215,
      -1.458506027737076,
      -1.439310497068389,
      -1.4433217053168488,
      -1.4620745674475435,
      -1.5623535090392844,
      -1.464837445985327,
      -1.3973967857215746,
      -1.4110966647990537,
      -1.4302182246528843,
      -1.4868744737597286,
      -1.4603955507839492,
      -1.480323259126682,
      -1.416221457786197,
      -1.4117170165497546,
      -1.387468457491314,
      -1.5034028478205996,
      0.0,
      -1.4728372523771482,
      -1.5245612063040352,
      -1.3684331293064935,
      -1.4275780738413002,
      -1.5278520301875502
    ],
    [
      -1.5348592760465887,
      -1.4993055205811217,
      -1.512746207535147,
      -1.5066388441751373,
      -1.5669259476178286,
      -1.5186250965475627,
      -1.4866708076539614,
      -1.5447227524048213,
      -1.5677487741331264,
      -1.5075342010834711,
      -1.5337736247208729,
      -1.5824905582270803,
      -1.5807630313689722,
      -1.5372619164881138,
      -1.5171299287023743,
      -1.5349321819578219,
      -1.5012575475079448,
      -1.5392550502793336,
      -1.5285477361437174,
      -1.482779716976378,
      -1.5328137209997525,
      -1.5042505437194846,
      -1.5221481526821201,
      -1.529359103555586,
      0.0,
      -1.5565340948144137,
      -1.5051703306612345,
      -1.5144471136838669,
      -1.6038287611122004
    ],
    [
      -1.4504368902962705,
      -1.4211157445804745,
      -1.432724492972652,
      -1.3976752528579297,
      -1.3256451016339403,
      -1.3195121415652566,
      -1.3814906997050238,
      -1.3246773145859516,
      -1.3504868654277005,
      -1.4046377982451834,
      -1.411231966525035,
      -1.5055530029828466,
      -1.3664255872832591,
      -1.3133775332087319,
      -1.3840422066141986,
      -1.4351723763674067,
      -1.4558478383379951,
      -1.3274648987209645,
      -1.4294683379247888,
      -1.3720179553360614,
      -1.3741820311550608,
      -1.3416046941958648,
      -1.4917202904841373,
      -1.4468955548729232,
      -1.4930491065001816,
      0.0,
      -1.3548213798692652,
      -1.4360388872493837,
      -1.435405071273522
    ],
    [
      -1.4587259960087486,
      -1.344179707863693,
      -1.3518860466318312,
      -1.2917645208772655,
      -1.340334945807827,
      -1.379173444047534,
      -1.4406384318752752,
      -1.3301443827207131,
      -1.3002409082664899,
      -1.3282187623970003,
      -1.3782092928108285,
      -1.5135622333153915,
      -1.3183346933879239,
      -1.2785461552471644,
      -1.3618522599925464,
      -1.339958172648889,
      -1.4621649011408862,
      -1.3834118854686148,
      -1.3915062295226368,
      -1.3198251504620215,
      -1.3536339176242909,
      -1.3546529729554537,
      -1.4797614484053967,
      -1.405986599045651,
      -1.4206826437460405,
      -1.416534945588272,
      0.0,
      -1.4592658605480027,
      -1.5703626220297604
    ],
    [
      -1.2577780290168417,
      -1.2268949713133048,
      -1.2477734279964041,
      -1.2595831038442034,
      -1.3212161520645997,
      -1.3414606914420084,
      -1.3552438963308824,
      -1.3304235072909931,
      -1.35842792555252,
      -1.291935255075488,
      -1.2889965259202327,
      -1.3644384771123659,
      -1.3237885853731703,
      -1.267072511799891,
      -1.2515468052471679,
      -1.2605680478006944,
      -1.327832034554078,
      -1.317555432806548,
      -1.3532579234470137,
      -1.275396895593691,
      -1.1970263303325817,
      -1.2961737609198398,
      -1.3627267365400524,
      -1.2645965327068767,
      -1.321167168318969,
      -1.315055677892692,
      -1.2562138135557026,
      0.0,
      -1.387982322336585
    ],
    [
      -1.526474047527911,
      -1.5228746824414694,
      -1.5137431280843678,
      -1.5200951374300677,
      -1.5129474890139216,
      -1.5042442501911584,
      -1.530298405798177,
      -1.479416527219879,
      -1.4943462691582574,
      -1.5138901988115891,
      -1.500175929485206,
      -1.5172164425063182,
      -1.5057085603898024,
      -1.520442238590787,
      -1.5400249913930626,
      -1.527317244124604,
      -1.4871415948040447,
      -1.5011970625176403,
      -1.4941475864493974,
      -1.474278429956413,
      -1.4839569571421867,
      -1.4622003689314969,
      -1.5072807411683526,
      -1.5319277655730912,
      -1.4935252508722343,
      -1.4969377737750063,
      -1.5108464954948906,
      -1.5250577828598195,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.10157423342020255,
      0.08653990087298657,
      0.0914930689064799,
      0.09176989542904224,
      0.07438054187958709,
      0.07031636502992411,
      0.07465449127215207,
      0.09155888281161673,
      0.07923685437070382,
      0.08810504881260606,
      0.07668187636241486,
      0.08132005635019857,
      0.08802453099240548,
      0.09076289399460702,
      0.09455538174460987,
      0.07268208414115618,
      0.08072271651753038,
      0.06087130859524592,
      0.08505343963382561,
      0.10958863414443698,
      0.08825982959895962,
      0.06659418332294731,
      0.08049776044955781,
      0.07202449481070072,
      0.08557872874956107,
      0.09186090468365427,
      0.11169846748309764,
      0.08143142939993098
    ],
    [
      0.4031859223813088,
      0.0,
      0.5286650077128541,
      0.54606188851578,
      0.45801865995906765,
      0.43865838644054955,
      0.3761340550858272,
      0.4132614369137435,
      0.439745575887327,
      0.41745623933185505,
      0.5280478349593474,
      0.328615111151493,
      0.510536212001089,
      0.5453924453579635,
      0.5509900187115127,
      0.5442608662873396,
      0.367018973709484,
      0.4624393815356491,
      0.42058708416792845,
      0.4749645429504816,
      0.42093790484239557,
      0.4059029190656156,
      0.34187474389026784,
      0.410827516908709,
      0.3880861563208384,
      0.3380961701000824,
      0.49996994321765276,
      0.4753638969216665,
      0.2426209119047522
    ],
    [
      0.3726832543641234,
      0.5137752944110539,
      0.0,
      0.4850811586136172,
      0.39023620879263077,
      0.3617836225939606,
      0.33297093884440887,
      0.3861782930642115,
      0.4117358645154874,
      0.36701024214315003,
      0.42738983047567936,
      0.2800460755882743,
      0.43772930916209907,
      0.4456863760318961,
      0.4265251287328753,
      0.4342134992471536,
      0.3390074372140002,
      0.4145694986946118,
      0.30171908651186663,
      0.41483535869219423,
      0.402679993657836,
      0.4165689441031606,
      0.29157456791408976,
      0.33431918285951756,
      0.3674850787127455,
      0.31109391391058816,
      0.4453336639198262,
      0.41931479428662666,
      0.2533912975532775
    ],
    [
      0.35542199195842183,
      0.5166811501846804,
      0.438807150795961,
      0.0,
      0.38944748280931996,
      0.4420156930635384,
      0.34741648605141995,
      0.3953148770856252,
      0.4402822904079138,
      0.4134682088131232,
      0.4191475521936143,
      0.264529025310154,
      0.4646792992561999,
      0.47093243535186535,
      0.42430269520643904,
      0.4523231696737702,
      0.37254931747137765,
      0.4182979132831819,
      0.3261868505045562,
      0.46381450482977904,
      0.4039889499145828,
      0.4637390275916953,
      0.2855753914717454,
      0.3488359469455051,
      0.33577947193484325,
      0.3383414996896179,
      0.4679752194951661,
      0.39422660734392956,
      0.2552934914740945
    ],
    [
      0.32215863635543807,
      0.4348365743379028,
      0.4088796636691703,
      0.4085616486670065,
      0.0,
      0.5084353453620121,
      0.46529463369117896,
      0.5875721187933589,
      0.47365418146506166,
      0.45654048318216467,
      0.4367331254012874,
      0.3182789655650611,
      0.5161263071481506,
      0.5148560179223232,
      0.42406047836610195,
      0.452600524660302,
      0.3600409005184164,
      0.559906264341089,
      0.4171914777090533,
      0.5056445372388338,
      0.3826695116730918,
      0.48818873958965314,
      0.3161186945512491,
      0.3464836506450566,
      0.37351334487300725,
      0.43862467052868914,
      0.5291319465363544,
      0.3416293398789001,
      0.33143887310308706
    ],
    [
      0.2761569075094916,
      0.44801114550873633,
      0.3946652161394699,
      0.42196228324498364,
      0.5171306317602067,
      0.0,
      0.4183754344506454,
      0.5410485718102349,
      0.511843560802441,
      0.4181015336519116,
      0.41926371025351594,
      0.3027694461364616,
      0.47963882759435617,
      0.5092556492114411,
      0.3936301102175619,
      0.39876045542508076,
      0.3393311593834736,
      0.5468341327028545,
      0.43964146865375997,
      0.4873079065745225,
      0.3395992639545171,
      0.42547495089572407,
      0.29014125399873336,
      0.3677363310427251,
      0.3432766202264981,
      0.44428179741095564,
      0.44599773648992125,
      0.3575780921503171,
      0.27798402695871793
    ],
    [
      0.2875140504117497,
      0.370689728679902,
      0.3810547356163152,
      0.40356040369698953,
      0.48395122166658955,
      0.480663157813251,
      0.0,
      0.5033101194132725,
      0.33164462801920114,
      0.3708418577337129,
      0.31894130316431335,
      0.32764902760798487,
      0.38980171200018665,
      0.4920198865896017,
      0.3742064235215492,
      0.3544067350962683,
      0.36675809621838873,
      0.47753719039446696,
      0.3566664510292037,
      0.471781362000532,
      0.35594907885057014,
      0.42456568370841774,
      0.36468334283517057,
      0.3427968272630919,
      0.36068217714651296,
      0.3735547265074697,
      0.4121480924862764,
      0.3413313265858773,
      0.2542851599029792
    ],
    [
      0.23024295746272982,
      0.3771885304789149,
      0.3575425814353068,
      0.3457351698808917,
      0.46708771567830376,
      0.4783005334454167,
      0.4237262476735657,
      0.0,
      0.42188325895191015,
      0.34509430429158616,
      0.37980636495588627,
      0.26985791295343087,
      0.39485799960628776,
      0.4435915713855758,
      0.37829875048248307,
      0.32455060739435027,
      0.3233683882627769,
      0.5846196299072901,
      0.3997818119766281,
      0.40500330110802074,
      0.3650696234311066,
      0.40145848111006366,
      0.3052156084895268,
      0.2928679686265616,
      0.30393995472574087,
      0.40772217880417316,
      0.41131683326952606,
      0.28509380575416365,
      0.26579620418962757
    ],
    [
      0.3394467253564992,
      0.44367602898385394,
      0.439594248486056,
      0.44207912402645944,
      0.4018175126199999,
      0.49240238941165604,
      0.3245305313200473,
      0.44310588306319554,
      0.0,
      0.367650361150907,
      0.4797336742652041,
      0.2701266031428988,
      0.470095694395551,
      0.4668990769754169,
      0.4194053924782879,
      0.40990159051352615,
      0.30697272752145577,
      0.4553529850360518,
      0.40120951259241977,
      0.41362545459828404,
      0.3881686413017986,
      0.46366789511105044,
      0.2849478087984807,
      0.3472636022223101,
      0.3193044043518787,
      0.36674114778986255,
      0.4739718688182155,
      0.3911246641229851,
      0.29492137900575277
    ],
    [
      0.3700572736424146,
      0.4751351872817211,
      0.4274167806397422,
      0.4631937495005829,
      0.45214286190251274,
      0.44352663876453513,
      0.40346337915022623,
      0.4711160917535071,
      0.39449171867319377,
      0.0,
      0.37769325539281895,
      0.32295393469767464,
      0.41795537199600097,
      0.44912590717442513,
      0.3968919340561048,
      0.4005493315883508,
      0.4774252138854411,
      0.40138636430859265,
      0.462260523924211,
      0.4568909094688991,
      0.3625205594904246,
      0.44908852469102434,
      0.3320549226223508,
      0.35947267938668537,
      0.47212717997248377,
      0.39108973113500345,
      0.5325625117042732,
      0.3741936285919465,
      0.27965666971986236
    ],
    [
      0.35941963085717243,
      0.5127266893638944,
      0.4389051590432389,
      0.4237283471556492,
      0.439541963546487,
      0.38121479153097715,
      0.3225957828524828,
      0.3839092751290958,
      0.46270743141201565,
      0.3382983271617874,
      0.0,
      0.25062340472442446,
      0.4312968006781628,
      0.47375446994645376,
      0.5152724483546511,
      0.46546004355908277,
      0.3105501280944096,
      0.38105944302215633,
      0.3717642808018138,
      0.4126119185464636,
      0.36134545051647593,
      0.3864850137834097,
      0.27293146485505093,
      0.3585639107521834,
      0.33179615011529506,
      0.37088526563720525,
      0.42983075541990146,
      0.3517873463101906,
      0.31492421492655187
    ],
    [
      0.2728512648614476,
      0.3034354612226793,
      0.2681776418173212,
      0.28630658314675617,
      0.291508828998978,
      0.27544205845414904,
      0.2927285507616222,
      0.28832930892299435,
      0.2664298496420967,
      0.29122086492210286,
      0.26783295527076345,
      0.0,
      0.280046841878117,
      0.2965196111592294,
      0.29362684059195976,
      0.2993023445990197,
      0.2674277648394434,
      0.2877305739567415,
      0.2827998008986279,
      0.3392820025787955,
      0.2973454889718925,
      0.31495246584750336,
      0.30161550029878614,
      0.2911085983439261,
      0.2999623352799712,
      0.22937622266450308,
      0.2980922661536225,
      0.29023799249636095,
      0.28793485970495003
    ],
    [
      0.29815281915055536,
      0.4169264753341244,
      0.36520924518982256,
      0.3885576997520326,
      0.4379129572812208,
      0.39790714652436066,
      0.3264588359798486,
      0.4517640664111533,
      0.4130159917234606,
      0.3422236747519478,
      0.4060268781955618,
      0.25793521102895833,
      0.0,
      0.45996246765927484,
      0.35155656395689316,
      0.41843632512917495,
      0.25439795291601164,
      0.4506938686715607,
      0.3445172040855291,
      0.4288341121743009,
      0.30512332923754837,
      0.37332627069652724,
      0.23409220851618584,
      0.29676076211366964,
      0.28778940821628995,
      0.36333547020359647,
      0.48457061174001015,
      0.30662167375728,
      0.25396427103442876
    ],
    [
      0.3844193230807975,
      0.5369258709508369,
      0.5143052912292079,
      0.5219567392317479,
      0.4980797666476642,
      0.5137192826601904,
      0.46517528226144655,
      0.5002267593789789,
      0.5130771679957589,
      0.4540157670500107,
      0.48969239209254267,
      0.3237882878599829,
      0.5228076669918842,
      0.0,
      0.4946254686979996,
      0.5003636964661315,
      0.353890313977973,
      0.524783125376475,
      0.3785869914837261,
      0.5226075931164489,
      0.4339082435902222,
      0.5619234874522938,
      0.3381215688231214,
      0.38669870203865275,
      0.3872206122645192,
      0.4505280540090888,
      0.4967545195424554,
      0.45773604472089024,
      0.2892894993790416
    ],
    [
      0.36383194307028255,
      0.5236209877201496,
      0.4553318449069743,
      0.45710906792502604,
      0.442056303138068,
      0.387076275178005,
      0.3456023807282158,
      0.3760984270802612,
      0.41721449282487555,
      0.35738008355182416,
      0.549800294081791,
      0.26655352713679137,
      0.4543752559894416,
      0.4814475779433538,
      0.0,
      0.4599137616654563,
      0.3123806320570299,
      0.3943896953783934,
      0.2951345024730938,
      0.4431468016958868,
      0.40419727927649873,
      0.42131042625197357,
      0.2991932741757448,
      0.36698432250293034,
      0.3714397765572508,
      0.3260820048360922,
      0.45918334008558004,
      0.4257401729775012,
      0.24515042938045273
    ],
    [
      0.3505177963052455,
      0.4682682324127574,
      0.4394212824642072,
      0.44965319601001585,
      0.4287309258875116,
      0.3770521854976405,
      0.3399669755258161,
      0.42113586900690225,
      0.4079902339747632,
      0.3298804440203962,
      0.42971983050648155,
      0.2812439856689404,
      0.48894799925214905,
      0.44758066521376016,
      0.4287297165074031,
      0.0,
      0.3318664660458328,
      0.420387253800794,
      0.35832740559951404,
      0.387018568126271,
      0.3498299973090815,
      0.38215262821665164,
      0.27531471234332994,
      0.3791158550025173,
      0.3162234081424069,
      0.32618884843133333,
      0.439362013796045,
      0.3871692280881194,
      0.2572057084029782
    ],
    [
      0.29789803576343976,
      0.35435593609714267,
      0.3172035501727748,
      0.3631934833911328,
      0.2874981438413762,
      0.33646278767718574,
      0.33726479514009244,
      0.2959083808134475,
      0.3161277964579814,
      0.40874964030595784,
      0.28500383032987076,
      0.28317786616778573,
      0.271322267210347,
      0.3220766424635342,
      0.333516124778247,
      0.3194147075010989,
      0.0,
      0.28778822546146876,
      0.28551864558587314,
      0.34791902937572394,
      0.3297350233759,
      0.36055035278054315,
      0.3170020394044335,
      0.3346170619911206,
      0.34078174370276115,
      0.3060693187143686,
      0.36128677575129164,
      0.3150323100727068,
      0.2600792802711429
    ],
    [
      0.29454395374603326,
      0.40357908562974676,
      0.35184762727293273,
      0.4004853246658566,
      0.4625089163721392,
      0.448307987524321,
      0.40739677610360436,
      0.5643628922781221,
      0.4032936282787536,
      0.37013109344709294,
      0.3886076737577602,
      0.31093328656026453,
      0.44148432391100156,
      0.4669469237673669,
      0.34741229851096134,
      0.3871241017252114,
      0.3486804951387108,
      0.0,
      0.37782483597370553,
      0.43969746065947035,
      0.353752065061842,
      0.4282140595662627,
      0.30055963775064143,
      0.33732244199950245,
      0.3271003248995741,
      0.42965051730390447,
      0.3803081327278872,
      0.3139865430748483,
      0.3096403394087601
    ],
    [
      0.2940324108330252,
      0.3852371487407906,
      0.3498668215952647,
      0.38978799312590184,
      0.3875557575152804,
      0.4194408301981256,
      0.31210708525645425,
      0.41740830212027946,
      0.3851491103032527,
      0.37553623050761553,
      0.3628391649414622,
      0.28547531325633146,
      0.4068323536720808,
      0.3427849997061716,
      0.2924124351683137,
      0.3151494886094133,
      0.3440212640163962,
      0.4082423971605451,
      0.0,
      0.40421554295939366,
      0.31005175047997446,
      0.3690534675616086,
      0.27782866273634155,
      0.31898648911407324,
      0.3560794278795256,
      0.31116032540845984,
      0.38907801372835804,
      0.3060778204740462,
      0.30027568072333244
    ],
    [
      0.39164929971865226,
      0.4568297085333375,
      0.46877174535444643,
      0.4416622847867735,
      0.5549972805654406,
      0.4959657539891458,
      0.45232896040842396,
      0.4847225498707337,
      0.45017700996182075,
      0.44961836051370874,
      0.4203808362911241,
      0.3372222257780131,
      0.49811542224204364,
      0.5483321454551526,
      0.43215586366993075,
      0.4330890077724925,
      0.37065117119892044,
      0.49333254885626654,
      0.39554585698346356,
      0.0,
      0.4145592286561868,
      0.5211889184864904,
      0.33960318748765794,
      0.4126308750664067,
      0.39338046372657787,
      0.38321535766364323,
      0.4973091300993928,
      0.39183962910466374,
      0.32692060562530356
    ],
    [
      0.3566477364391749,
      0.3871899239698904,
      0.4388869073370074,
      0.4156757422514248,
      0.3414550822201936,
      0.319275662032078,
      0.3490460765097667,
      0.3940669635862146,
      0.3775984081993182,
      0.3657101536299918,
      0.3358684623121395,
      0.30249093119923054,
      0.34530639637957594,
      0.3755872549086221,
      0.3832005149388069,
      0.3642980492737684,
      0.3464698916484026,
      0.3521892673241609,
      0.32890732717264237,
      0.4117092112783878,
      0.0,
      0.39432677740691857,
      0.33994657941763906,
      0.4152975399389913,
      0.35809816275028417,
      0.33139587099941914,
      0.42216520552207104,
      0.39902351487305077,
      0.2979020044430414
    ],
    [
      0.35824647074808835,
      0.4027425001588787,
      0.3769906044357476,
      0.40816061349674104,
      0.4783580443881901,
      0.4348017999071834,
      0.4258343548601444,
      0.4725014772947622,
      0.4128801468412546,
      0.3932970744816333,
      0.3836377656853103,
      0.31223995253700276,
      0.41269171560439655,
      0.4978460782046501,
      0.39422829687102423,
      0.3920522794792687,
      0.37894590513009563,
      0.4456570356752785,
      0.3526718894257501,
      0.5060956607253766,
      0.39249802589652294,
      0.0,
      0.3348113412198488,
      0.392267109801935,
      0.3677182135444381,
      0.37820105140266946,
      0.42742874922877916,
      0.3670341749384529,
      0.32631395139854025
    ],
    [
      0.29958165522707736,
      0.3086858946445248,
      0.3303565646018003,
      0.30420341207527324,
      0.33936643303907066,
      0.32029453684243925,
      0.34942053081267854,
      0.35955978403863686,
      0.2901379038468459,
      0.3602571897351805,
      0.2912203402836515,
      0.3029152300445044,
      0.29464620286718435,
      0.33047485858753767,
      0.3171138087495293,
      0.3074926198376111,
      0.3656104227491339,
      0.3470712026315472,
      0.3342660012270642,
      0.34255529811902763,
      0.33884217833797825,
      0.3192119894356866,
      0.0,
      0.3476917924213174,
      0.4090424706851221,
      0.26127336397922685,
      0.3324162649842022,
      0.31078429144809583,
      0.2706501377144015
    ],
    [
      0.38040527631681686,
      0.4245076922874702,
      0.4213622727290687,
      0.3998830130788533,
      0.3724928046383871,
      0.3702547483674057,
      0.34068738769037377,
      0.3730737858715192,
      0.39226931654020625,
      0.38825810829174645,
      0.36950524616105174,
      0.26922630456931085,
      0.3667423676232682,
      0.4341830278870207,
      0.4204831488095415,
      0.40136158895571095,
      0.34470533984886664,
      0.371184262824646,
      0.3512565544819133,
      0.41535835582239833,
      0.41986279705884066,
      0.44411135611728114,
      0.3281769657879956,
      0.0,
      0.358742561231447,
      0.30701860730456,
      0.4631466843021017,
      0.40400173976729503,
      0.303727783421045
    ],
    [
      0.32211149067152767,
      0.3576652461369947,
      0.34422455918296935,
      0.35033192254297907,
      0.2900448191002878,
      0.3383456701705536,
      0.3702999590641549,
      0.3122480143132951,
      0.28922199258498993,
      0.3494365656346452,
      0.3231971419972435,
      0.27448020849103605,
      0.2762077353491441,
      0.3197088502300025,
      0.33984083801574205,
      0.32203858476029446,
      0.3557132192101715,
      0.3177157164387827,
      0.3284230305743989,
      0.37419104974173845,
      0.3241570457183638,
      0.3527202229986317,
      0.3348226140359962,
      0.3276116631625303,
      0.0,
      0.3004366719037026,
      0.3518004360568818,
      0.3425236530342495,
      0.25314200560591593
    ],
    [
      0.36212953379271795,
      0.391450679508514,
      0.3798419311163366,
      0.4148911712310588,
      0.48692132245504816,
      0.49305428252373185,
      0.4310757243839647,
      0.4878891095030369,
      0.462079558661288,
      0.40792862584380507,
      0.4013344575639535,
      0.30701342110614194,
      0.44614083680572936,
      0.49918889088025664,
      0.42852421747478986,
      0.3773940477215818,
      0.35671858575099336,
      0.485101525368024,
      0.3830980861641997,
      0.4405484687529271,
      0.4383843929339277,
      0.4709617298931237,
      0.32084613360485115,
      0.3656708692160653,
      0.31951731758880686,
      0.0,
      0.4577450442197233,
      0.3765275368396048,
      0.3771613528154665
    ],
    [
      0.4030816349805497,
      0.5176279231256053,
      0.5099215843574672,
      0.5700431101120329,
      0.5214726851814713,
      0.4826341869417643,
      0.4211691991140232,
      0.5316632482685852,
      0.5615667227228085,
      0.533588868592298,
      0.48359833817846987,
      0.34824539767390683,
      0.5434729376013745,
      0.5832614757421339,
      0.4999553709967519,
      0.5218494583404094,
      0.3996427298484122,
      0.47839574552068354,
      0.47030140146666155,
      0.5419824805272768,
      0.5081737133650075,
      0.5071546580338446,
      0.38204618258390166,
      0.4558210319436473,
      0.4411249872432579,
      0.44527268540102627,
      0.0,
      0.4025417704412957,
      0.29144500895953795
    ],
    [
      0.36690907349852386,
      0.3977921312020607,
      0.3769136745189614,
      0.36510399867116217,
      0.3034709504507658,
      0.2832264110733571,
      0.26944320618448314,
      0.2942635952243724,
      0.2662591769628455,
      0.3327518474398776,
      0.3356905765951328,
      0.26024862540299964,
      0.3008985171421952,
      0.3576145907154744,
      0.37314029726819764,
      0.3641190547146711,
      0.2968550679612876,
      0.30713166970881756,
      0.27142917906835184,
      0.34929020692167456,
      0.42766077218278387,
      0.3285133415955257,
      0.26196036597531314,
      0.36009056980848886,
      0.30351993419639656,
      0.30963142462267346,
      0.3684732889596629,
      0.0,
      0.2367047801787805
    ],
    [
      0.24532043499658585,
      0.2489198000830275,
      0.25805135444012905,
      0.25169934509442915,
      0.2588469935105753,
      0.2675502323333385,
      0.2414960767263199,
      0.29237795530461796,
      0.2774482133662395,
      0.25790428371290774,
      0.27161855303929094,
      0.25457804001817874,
      0.2660859221346945,
      0.2513522439337099,
      0.2317694911314343,
      0.244477238399893,
      0.28465288772045216,
      0.2705974200068566,
      0.2776468960750995,
      0.297516052568084,
      0.2878375253823102,
      0.30959411359300004,
      0.26451374135614425,
      0.2398667169514057,
      0.2782692316522626,
      0.27485670874949064,
      0.2609479870296063,
      0.24673669966467737,
      0.0
    ]
  ],
  "row_avgs": [
    0.0845656429921479,
    0.43848999307973496,
    0.3851763540932485,
    0.3967633464325758,
    0.4328275234204643,
    0.41270707943425927,
    0.38474980378428014,
    0.3710363677047087,
    0.3970620331235753,
    0.41487295839696475,
    0.3876424967177388,
    0.28791517422444146,
    0.3612883393368332,
    0.457686697084646,
    0.39649088859246234,
    0.38210719398424514,
    0.3205554926642411,
    0.38556081239701,
    0.35059593884970786,
    0.4377212652809362,
    0.3660619863557933,
    0.40093401012078445,
    0.32446937067736964,
    0.3798567534923622,
    0.32652360452597223,
    0.4131835304899882,
    0.47703766204515025,
    0.3238966545801727,
    0.2647332913919557
  ],
  "col_avgs": [
    0.3342363394107104,
    0.41000197358604984,
    0.3881698195404836,
    0.4003629122427728,
    0.4026579346212796,
    0.395078319221445,
    0.3557973575593271,
    0.41132398741379683,
    0.3886244326369545,
    0.36934240315227307,
    0.38108701561278113,
    0.2853535427764161,
    0.40036294117296106,
    0.4255145239784507,
    0.38416562750927497,
    0.38426637714789436,
    0.3338690905885184,
    0.4080398947823042,
    0.3490762666145108,
    0.4136964689566079,
    0.3653013024504328,
    0.4025952241136658,
    0.3000773820809837,
    0.34350742066139583,
    0.3422866218839799,
    0.34284651192360593,
    0.41536314071315855,
    0.3531055987572443,
    0.2764018341644912
  ],
  "combined_avgs": [
    0.20940099120142916,
    0.42424598333289243,
    0.3866730868168661,
    0.39856312933767435,
    0.41774272902087195,
    0.4038926993278521,
    0.3702735806718036,
    0.3911801775592528,
    0.3928432328802649,
    0.3921076807746189,
    0.38436475616526,
    0.2866343585004288,
    0.3808256402548971,
    0.4416006105315483,
    0.3903282580508687,
    0.3831867855660698,
    0.32721229162637977,
    0.3968003535896571,
    0.3498361027321093,
    0.42570886711877204,
    0.365681644403113,
    0.4017646171172251,
    0.3122733763791767,
    0.361682087076879,
    0.33440511320497607,
    0.3780150212067971,
    0.4462004013791544,
    0.33850112666870846,
    0.27056756277822347
  ],
  "gppm": [
    600.6244617638557,
    638.9179701117494,
    648.9303661345918,
    641.872529099437,
    637.4860419016584,
    644.3473222094024,
    660.9265339122345,
    634.2684494487532,
    645.9320462965959,
    657.1065366712157,
    652.5817765819465,
    693.9561803390673,
    641.7821524392136,
    631.1392519253972,
    649.795728029169,
    650.3451188093056,
    669.7665037519448,
    636.9175082539028,
    664.7750263643342,
    633.2518198392507,
    651.9493915018044,
    637.110528816285,
    682.1267360968432,
    665.3977197884066,
    666.466985156374,
    663.6844216722076,
    635.2355614513211,
    661.8061129329305,
    690.9402325353925
  ],
  "gppm_normalized": [
    1.409272295109632,
    1.356796547194748,
    1.3761105213645075,
    1.3680562775065241,
    1.3459568757708151,
    1.3637946937632255,
    1.4123050192595625,
    1.3393874194970228,
    1.3662706515675274,
    1.3886750419675178,
    1.3763983318510744,
    1.48766010679223,
    1.3566322207416526,
    1.3337368025078848,
    1.3773862982517033,
    1.3848583760322217,
    1.4170329723555823,
    1.35215181104398,
    1.4058061835425306,
    1.334534748416433,
    1.380179620864378,
    1.3475418135453723,
    1.4476470086335833,
    1.4077672597561066,
    1.4063912411508945,
    1.4148908143562409,
    1.3366255334733885,
    1.4058476133678486,
    1.4613585506950002
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401,
    389,
    491,
    424,
    450,
    396,
    441,
    392,
    376,
    422,
    476,
    440,
    374,
    446,
    482,
    442,
    441,
    362,
    424,
    410,
    414,
    410,
    353,
    400,
    382,
    444,
    360,
    404,
    406,
    395,
    339,
    446,
    477,
    497,
    454,
    431,
    442,
    455,
    560,
    421,
    424,
    522,
    488,
    411,
    400,
    410,
    451,
    469,
    411,
    433,
    451,
    378,
    376,
    454,
    411,
    399,
    394,
    441,
    372,
    1166,
    412,
    405,
    456,
    421,
    410,
    395,
    446,
    422,
    371,
    413,
    468,
    458,
    414,
    419,
    422,
    375,
    410,
    404,
    390,
    423,
    455,
    433,
    401,
    393,
    441,
    387,
    438,
    457
  ],
  "response_lengths": [
    5320,
    2319,
    2393,
    2679,
    2414,
    2334,
    2296,
    2603,
    2476,
    2155,
    2306,
    2635,
    2560,
    2382,
    2356,
    2459,
    2209,
    2439,
    2402,
    2237,
    2519,
    2523,
    2603,
    2361,
    2301,
    2440,
    2204,
    2407,
    2616
  ]
}