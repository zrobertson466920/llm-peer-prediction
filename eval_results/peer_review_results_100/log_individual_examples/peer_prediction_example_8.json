{
  "example_idx": 8,
  "reference": "Under review as a conference paper at ICLR 2023\n\nCONSERWEIGHTIVE BEHAVIORAL CLONING FOR RELIABLE OFFLINE REINFORCEMENT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe goal of offline reinforcement learning (RL) is to learn near-optimal policies from static logged datasets, thus sidestepping expensive online interactions. Behavioral cloning (BC) provides a straightforward solution to offline RL by mimicking offline trajectories via supervised learning. Recent advances (Chen et al., 2021; Janner et al., 2021; Emmons et al., 2021) have shown that by conditioning on desired future returns, BC can perform competitively to their value-based counterparts, while enjoying much more simplicity and training stability. However, the distribution of returns in the offline dataset can be arbitrarily skewed and suboptimal, which poses a unique challenge for conditioning BC on expert returns at test-time. We propose ConserWeightive Behavioral Cloning (CWBC), a simple and effective method for improving the performance of conditional BC for offline RL with two key components: trajectory weighting and conservative regularization. Trajectory weighting addresses the bias-variance tradeoff in conditional BC and provides a principled mechanism to learn from both low return trajectories (typically plentiful) and high return trajectories (typically few). Further, we analyze the notion of conservatism in existing BC methods, and propose a novel conservative regularizer that explicitly encourages the policy to stay close to the data distribution. The regularizer helps achieve more reliable performance, and removes the need for ad-hoc tuning of the conditioning value during evaluation. We instantiate CWBC in the context of Reinforcement Learning via Supervised Learning (RvS) (Emmons et al., 2021) and Decision Transformer (DT) (Chen et al., 2021), and empirically show that it significantly boosts the performance and stability of prior methods on various offline RL benchmarks.\n\n1\n\nINTRODUCTION\n\nIn many real-world applications such as education, healthcare and autonomous driving, collecting data via online interactions can be expensive or even dangerous. However, we often have access to historical logged datasets in these domains that have been collected previously by some unknown policies. The goal of offline reinforcement learning (RL) is to directly learn effective agent policies from such datasets, without additional online interactions (Lange et al., 2012; Levine et al., 2020). Many online RL algorithms have been adapted to work in the offline setting, including value-based methods (Fujimoto et al., 2019; Ghasemipour et al., 2021; Wu et al., 2019; Jaques et al., 2019; Kumar et al., 2020; Fujimoto & Gu, 2021; Kostrikov et al., 2021a) as well as model-based methods (Yu et al., 2020; Kidambi et al., 2020). The key challenge in all these methods is to generalize the value or dynamics to state-action pairs outside the offline dataset.\n\nAn alternative way to approach offline RL is via approaches derived from behavioral cloning (BC) (Bain & Sammut, 1995). BC is a supervised learning technique that was initially developed for imitation learning, where the goal is to learn a policy that mimics the expert demonstrations. Recently, a number of works propose to formulate offline RL as supervised learning problems (Chen et al., 2021; Janner et al., 2021; Emmons et al., 2021). Since offline RL datasets usually do not have expert demonstrations, these works condition BC on extra context information to specify target outcomes such as returns and goals. Compared with the value-based approaches, the empirical evidence has shown that these conditional BC approaches perform competitively, and they additionally enjoy the enhanced simplicity and training stability of supervised learning.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nAs commonly observed for supervised learning approaches, the performance of conditional BC is often limited by the suboptimility of the offline dataset, which particularly can be probed through the distribution of returns in the dataset. There are two related challenges in this regard for offline RL.\n\nFirst, there is a unique bias-variance tradeoff in learning that arises due to the mismatch between the training and test distribution of returns. Typically, offline datasets in the real world mostly contain trajectories with low returns, whereas at test time, we are interested in conditioning on high returns. Simply filtering the offline dataset to contain high return trajectories is not always viable, as the number of such high-return trajectories can be very low leading to high variance during learning.\n\nSecond, the maximum return in the offline trajectories is often far below the desired expert returns. This implies that at test time, we need to condition our agent on out-of-distribution (ood) expert returns. Interestingly, we find that existing BC methods have significantly different behaviors when conditioning on ood returns. While DT (Chen et al., 2021) enjoys a stable performance, RvS (Emmons et al., 2021) is highly sensitive to such ood conditioning and exhibits vast drops in peak performance for such ood inputs. Therefore, the current practice for setting the conditioning return at test-time in RvS is based on careful tuning with online rollouts, which is often tedious, impractical, and inconsistent with the promise of offline RL to minimize online interactions.\n\nWe propose ConserWeightive Behavior Cloning (CWBC), a new BC-based approach for offline RL that mitigates the aforementioned challenges. CWBC consists of 2 key components: trajectory weighting and conservative regularization. With trajectory weighting, we strive to balance the biasvariance trade-off in learning by proposing a scheme for downweighting the low-return trajectories, but at the same time, we do not filter them for data efficiency. Moreover, we introduce a notion of conservatism for ood sensitve BC methods such as RvS, which encourages the policy to stay close to the data distribution when conditioning on large returns. We take trajectories with high returns from the dataset and add positive noise to their returns, which generates trajectories with large ood returns. We predict actions conditioning on the perturbed returns and project them to the original actions by penalizing the l2 distance. By imposing such a regularizer, we can condition the policy on large, unseen target returns at test-time, sidestepping tedious manual tuning and online interactions.\n\nOur proposed algorithm is simple and easy to implement. Empirically, we instantiate our framework in the context of RvS (Emmons et al., 2021) and DT (Chen et al., 2021), two state-of-the-art BC methods for offline RL. CWBC significantly improves the performance of RvS and DT in D4RL (Fu et al., 2020) locomotion tasks by 18% and 8%, respectively, without any hand picking of the value of the conditioning returns at test-time.\n\n2 RELATED WORK\n\nOffline Temporal Difference Learning Most of the existing off-policy RL methods are often based on temporal difference (TD) updates. A key challenge of directly applying them in the offline setting is the extrapolation error: the value function is poorly estimated at unseen state-action pairs. To remedy this issue, various forms of conservatism have been introduced to off-policy RL methods that exploits temporal difference updates, with the purpose of encouraging the learned policy to stay close to the behavior policy that generates the data. For instance, Fujimoto et al. (2019); Ghasemipour et al. (2021) use certain policy parameterizations specifically tailored for offline RL. Wu et al. (2019); Jaques et al. (2019); Kumar et al. (2019) penalize the divergence-based distances between the learned policy and the behavior policy. Fujimoto & Gu (2021) propose an extra behavior cloning term to regularize the policy. This regularizer is simply the l2 distance between predicted actions and the truth, yet surprisingly effective for porting off-policy TD methods to the offline setting. Instead of regularizing the policy, several other works have sought to incorporate divergence regularizations into the value function estimation, e.g., (Nachum et al., 2019; Kumar et al., 2020; Kostrikov et al., 2021a). Another recent work by Kostrikov et al. (2021b) predicts the Q function via expectile regression, where the estimation of the maximum Q-value is constrained to be in the dataset.\n\nBehavior Cloning Approaches for Offline RL Recently, there is a surge of interest in converting offline RL into supervised learning paradigms (Chen et al., 2021; Janner et al., 2021; Emmons et al., 2021). In essence, these approaches conduct behavior cloning (Bain & Sammut, 1995) by additionally conditioning on extra information such as goals or rewards. Among these works, Chen et al. (2021) and Janner et al. (2021) have formulated offline RL as sequence modeling problems\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nand train transformer architectures (Vaswani et al., 2017) in a similar fashion to language and vision (Radford et al., 2018; Chen et al., 2020; Brown et al., 2020; Lu et al., 2022; Yan et al., 2021). Extensions have also been proposed in the context of sequential decision making for offline black-box optimization (Nguyen & Grover, 2022; Krishnamoorthy et al., 2022). A recent work by Emmons et al. (2021) further shows that conditional BC can achieve competitive performance even with a simple but carefully designed MLP network. Earlier, similar ideas have also been proposed for online RL, where the policy is trained via supervised learning techniques to fit the data stored in the replay buffer (Schmidhuber, 2019; Srivastava et al., 2019; Ghosh et al., 2019).\n\nData Exploration for Offline RL Recent research efforts have also been made towards understanding properties and limitations of datasets used for offline RL (Yarats et al., 2022; Lambert et al., 2022; Guo et al., 2021), particularly focusing on exploration techniques during data collection. Both Yarats et al. (2022) and Lambert et al. (2022) collect datasets using task-agnostic exploration strategies (Laskin et al., 2021), relabel the rewards and train offline RL algorithms on them. Yarats et al. (2022) benchmark multiple offline RL algorithms on different tasks including transferring, whereas Lambert et al. (2022) focus on improving the exploration method.\n\n3 PRELIMINARIES\n\nWe model our environment as a Markov decision process (MDP) (Bellman, 1957), which can be described by a tuple M “ xS, A, p, P, R, γy, where S is the state space, A is the action space, pps1q is the distribution of the initial state, P pst`1|st, atq is the transition probability distribution, Rpst, atq is the deterministic reward function, and γ is the discount factor. At each timestep t, the agent observes a state st P S and takes an action at P A. This moves the agent to the next state st`1 „ P p ̈|st, atq and provides the agent with a reward rt “ Rpst, atq.\n\nOffline RL. We are interested in learning a (near-)optimal policy from a static offline dataset of trajectories collected by unknown policies, denoted as Toffline. We assume that these trajectories are i.i.d samples drawn from some unknown static distribution T . We use τ to denote a trajectory and use |τ | to denote its length. Following Chen et al. (2021), the return-to-go (RTG) for a trajectory τ at timestep t is defined as the sum of rewards starting from t until the end of the trajectory: gt “ t1“t rt1 . This means the initial RTG g1 is equal to the total return of the trajectory rτ “\n\nt“1 rt.\n\nř\n\nř\n\n|τ |\n\n|τ |\n\nDecision Transformer (DT). DT (Chen et al., 2021) solves offline RL via sequence modeling. Specifically, DT employs a transformer architecture that generates actions given a sequence of historical states and RTGs. To do that, DT first transforms each trajectory in the dataset into a sequence of returns-to-go, states, and actions:\n\n`\n\n ̆\n\nτ “\n\ng1, s1, a1, g2, s2, a2, . . . , g|τ |, s|τ |, a|τ |\n\n.\n\n(1)\n\nDT trains a policy that generates action at at each timestep t conditioned on the history of RTGs gt ́K:t, states st ́K:t, and actions at ́K:t ́1, wherein K is the context length of the transformer. The learning objective a simple mean square error between the predicted actions and the ground truths:\n\nmin θ\n\nLDTpθq “ Eτ „T\n\nř\n\n“\n\n1 |τ |\n\n|τ | t“1\n\n`\n\nat ́ πθpgt ́K:t, st ́K:t, at ́K:t ́1q\n\n‰\n\n ̆\n\n2\n\n.\n\n(2)\n\nDuring evaluation, DT starts with an initial state s1 and a target RTG g1. At each step t, the agent generates an action at, receives a reward rt and observes the next state st`1. DT updates its RTG gt`1 “ gt ́ rt and generates next action at`1. This process is repeated until the end of the episode.\n\nReinforcement Learning via Supervised Learning (RvS). Emmons et al. (2021) conduct a thorough empirical study of conditional BC methods under the umbrella of Reinforcement Learning via Supervised Learning (RvS), and show that even simple models such as multi-layer perceptrons (MLP) can perform well. With carefully chosen architecture and hyperparameters, they exhibit performance that matches or exceeds the performance of transformer-based models. There are two main differences between RvS and DT. First, RvS conditions on the average reward ωt into the future instead of the sum of future rewards:\n\nωt “ 1\n\nH ́t`1\n\n|τ |\n\nt1“t rt1 “ gt\n\nH ́t`1 ,\n\n(3)\n\nř\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Offline data distribution vs the expert distribution.\n\n(b) The original return distribution T and the transformed distribution rT .\n\nFigure 1: The suboptimality of offline datasets (left) and the effect of trajectory weighting on the return distribution (right). We illustrate on walker2d-med-replay. For weighting, we use B “ 20, λ “ 0.01, κ “ pr‹ ́ pr90, where pr90 is the 90-th percentile of the returns in the offline dataset.\n\nwhere H is the maximum episode length. Intuitively, ωt is RTG normalized by the number of remaining steps. Second, RvS employs a simple MLP architecture, which generates action at at step t based on only the current state st and expected outcome ωt. RvS minimizes a mean square error:\n\nmin θ\n\nLRvSpθq “ Eτ „T\n\n1 |τ |\n\n|τ | t“1\n\n“\n\nř\n\n`\n\nat ́ πθpst, ωtq\n\n.\n\n(4)\n\n‰\n\n ̆\n\n2\n\nAt evaluation time, RvS performs a repeating process similarly to DT, except that the expected outcome is now updated as ωt`1 “ pgt ́ rtq{pH ́ tq.\n\n4 CONSERVATIVE BEHAVIORAL CLONING WITH TRAJECTORY WEIGHTING\n\nA key challenge that behavioral cloning faces in an offline setting is the suboptimality of the dataset, which we can characterize via the distribution of trajectory returns. An ideal offline dataset consists of sufficiently many high-quality trajectories, which have returns matching those of a dataset of expert demonstrations. For such an idealized scenario, offline RL reduces to a vanilla imitation learning problem. In practice, however, we observe that the return distribution for a typical dataset of offline trajectories is spread over a wide range of returns and is highly non-uniform. Figure 1a illustrates the return distribution of the walker2d-med-replay dataset (Fu et al., 2020), which is significantly different from the expert distribution. Therefore, from a return perspective, the trajectories in the offline dataset can be of varying importance for learning, which leads to a bias-variance trade-off. Further, for return-conditioned methods including conditional BC, it is unclear how the policy will behave when conditioned on o.o.d. returns at test-time. We study mitigation techniques for both these challenges in the following sections.\n\n4.1 CONTROLLING BIAS-VARIANCE TRADEOFF VIA TRAJECTORY WEIGHTING\n\nTo formalize our discussion, recall that rτ denotes the return of a trajectory τ and let r‹ “ supτ rτ be the maximum expert return, which is assumed to be known in prior works on conditional BC (Chen et al., 2021; Emmons et al., 2021). We know that the optimal offline data distribution, denoted by T ‹, is simply the distribution of demonstrations rolled out from the optimal policy. Typically, the offline trajectory distribution T will be biased w.r.t. T ‹. During learning, this leads to a bias-variance tradeoff, wherein ideally we want to learn our BC agent to condition on the expert returns, but is forced to minimize the empirical risk on a biased data distribution. The core idea of our approach is to transform T into a new distribution rT that better estimates T ‹. More concretely, rT should concentrate on high-return trajectories, which mitigates the bias. One naive strategy is to simply filter out a small fraction of high return trajectories from the offline dataset. However, since we expect the original dataset to contain very few high return trajectories, filtering trajectories will increase the variance for downstream BC. To balance the bias-variance trade-off, we propose to weight the trajectories based on their returns. Let fT : R ÞÑ R` be the density function of rτ where τ „ T . We consider the transformed distribution rT whose density function p rT is\n\n4\n\n20020406080100normalized return r0.00.10.20.30.40.5densitysuboptimal data distributionexpert distribution20020406080100normalized return r0.000.020.040.060.08densityoriginal transformed rUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Weighted Trajectory Sampling\n\nř\n\n1 Input: offline dataset Toffline, number of bins B, smoothing parameters λ, κ 2 Compute the returns: rτ Ð 3 Group the trajectories into B equal-sized bins according to rτ . 4 Sample a bin b P rBs with probability Pbinpbq defined in Equation (6). 5 Sample a trajectory τ in bin b uniformly at random. 6 Output: τ\n\nt“1 rt, @τ P Toffline.\n\n|τ |\n\nTable 1: The normalized return on D4RL locomotion tasks of RvS and DT with trajectory weighting. We use +W as shorthand for weighting. We use #wins to denote the number of datasets where the variant outperforms the original model. The results are averaged over 10 seeds.\n\nwalker2d-medium walker2d-med-replay walker2d-med-expert\n\nhopper-medium hopper-med-replay hopper-med-expert\n\nhalfcheetah-medium halfcheetah-med-replay halfcheetah-med-expert\n\n# wins average\n\nDT\n\nDT+W\n\nRvS\n\nRvS+W\n\n71.5 ̆ 3.9 53.4 ̆ 12.2 99.8 ̆ 21.3\n\n59.9 ̆ 4.9 56.4 ̆ 20.1 95.4 ̆ 11.3\n\n42.5 ̆ 0.6 34.5 ̆ 4.2 87.2 ̆ 2.7\n\n70.4 ̆ 4.5 60.5 ̆ 8.9 108.2 ̆ 0.8\n\n63.9 ̆ 4.4 76.9 ̆ 5.9 103.4 ̆ 9.0\n\n41.6 ̆ 1.7 36.9 ̆ 2.2 85.6 ̆ 2.0\n\n/ 66.7\n\n6 71.9\n\n73.3 ̆ 5.7 54.0 ̆ 12.1 102.2 ̆ 104.1\n\n54.5 ̆ 7.7 61.2 ̆ 14.7 104.1 ̆ 0.5\n\n56.6 ̆ 5.5 87.7 ̆ 9.7 108.8 ̆ 0.9\n\n16.2 ̆ 4.5 ́0.4 ̆ 2.7 83.4 ̆ 2.1\n\n/ 64.6\n\n62.5 ̆ 7.1 92.4 ̆ 6.1 108.4 ̆ 1.8\n\n4.0 ̆ 5.4 ́0.8 ̆ 2.2 69.1 ̆ 3.7\n\n4 61.7\n\nhkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkj trajectory weight ̆\n\n`\n\nfT prτ q\n\n ́ |rτ ́r‹|\n\np rT pτ q 9\n\nfT prτ q`λ ̈ exp\n\n(5) where λ, κ P R` are two hyperparameters. A larger value of κ leads to a more uniform rT , whereas a smaller value upweights the high-return trajectories. In contrast, a smaller value of λ gives more weights to high-return trajectories, while a larger value makes rT closer to T . Our trajectory weighting is motivated by a similar scheme proposed for model-based optimization (Kumar & Levine, 2020), where the authors use it to balance the bias and variance for gradient approximation for surrogates to black-box functions, and theoretically establish the optimality of the proposed distribution.\n\nκ\n\n,\n\n4.1.1\n\nIMPLEMENTATION DETAILS\n\nIn practice, the dataset Toffline only contains a finite number of samples and the density function p rT in equation (5) cannot be computed exactly. Following Kumar & Levine (2020), we sample from a discretized approximation of rT . We first group the trajectories in Toffline into B equal-sized bins according to the return rτ . To sample a trajectory, we first sample a bin index b P t1, . . . , Bu and then uniformly sample a trajectory inside bin b. We use |b| to denote the size of bin b. Let srb τ Pb rτ the average return of the trajectories in bin b, pr‹ be the highest return in the τ “ 1{|b| dataset Toffline, and define fTofflinepbq “ |b|{|Toffline|. As a discretized version of equation (5), the bins are weighted by their average returns with probability\n\nř\n\nPbinpbq 9\n\nfT\n\noffline pbq offline pbq`λ ̈ exp\n\nfT\n\n`\n\n ̆\n\n ́ |srb\n\nτ ́pr‹| κ\n\n.\n\n(6)\n\nAlgorithm 1 summarizes the data sampling procedure when trajectory weighting is used. Figure 1b illustrates the impact of trajectory weighting on the return distribution of the med-replay dataset for the walker2d environment. We plot the histograms before and after transformation, where the density curves are estimated by kernel density estimators.\n\n4.1.2 EMPIRICAL RESULTS\n\nDataset We evaluate the effectiveness of trajectory weighting on three locomotion tasks with dense rewards from the D4RL benchmark (Fu et al., 2020): hopper, walker2d and halfcheetah.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Performance of RvS and DT when conditioning on different evaluation RTGs. We report the mean and standard deviation of 10 seeds.\n\nFor each task, we consider the v2 medium, med-replay and med-expert offline datasets. The medium dataset contains 1M samples from a policy trained to approximately 1 3 the performance of an expert policy. The med-replay dataset uses the replay buffer of a policy trained up to the performance of a medium policy. The med-expert dataset contains 1M samples generated by a medium policy and 1M samples generated by an expert policy.\n\nBaselines We apply trajectory weighting to RvS (Emmons et al., 2021) and DT (Chen et al., 2021), two state-of-the-art BC methods. We compare their performance when trained on the original distribution and on the transformed distribution induced by our trajectory weighting (denoted as +W).\n\nHyperparameters For all datasets, we use B “ 20 and λ “ 0.01, and we set the temperature parameter κ to be the difference between the highest return and the 90-th percentile: pr‹ ́ pr90, whose value varies across the datasets. At test time, we set the evaluation RTG to be the expert return for each environment. The model architecture and the other hyperparamters are identical to what were used in the original paper. We provide a complete list of hyperparameters in Appendix B.2 and additional ablation experiments on λ and κ in Appendix C.\n\nResults Table 1 shows the performance of RvS and DT and their variants. DT+W outperforms the original DT in 6{9 datasets, achieving an average improvement of 8%. The improvement is significant in low-quality datasets (med-replay), which agrees with our analysis. Unlike in DT, trajectory weighting in RvS has varying effects, and the average performance of RvS+W is not better than that of RvS. To better understand this, we plot the achieved returns of RvS and DT when conditioning on different values of RTG. Figure 2 shows an interesting difference between behaviors of DT and RvS. DT is insensitive to the conditioning RTG, and continues performing stably even when conditioning on out-of-distribution RTGs. In contrast, the performance of RvS highly correlates with the evaluation RTG, but degrades quickly after a certain threshold. The performance crash problem of RvS shadows the improvement made by trajectory weighting.\n\n4.2 RELIABLE EVALUATION VIA CONSERVATISM\n\nThe results in Section 4.1.2 introduce another challenging problem for return-conditioned BC in offline RL: generalization to out-of-distribution (ood) returns. While strong generalization beyond the offline dataset remains an ongoing challenge for the offline RL community (Wang et al., 2020; Zanette, 2021; Foster et al., 2021), we require the policy to be reliable and at least stay close to the data distribution to avoid catastrophic failure when conditioned on ood returns. In other words, we want the policy to be conservative. Figure 2 shows that DT enjoys self-conservatism, while RvS does\n\n6\n\n20406080100120140160180020406080100120Returnwalker2d-medium20406080100120140160180020406080100120walker2d-medium-replay20406080100120140160180020406080100120walker2d-medium-expert406080100120140160180020406080100120Returnhopper-medium406080100120140160180020406080100120hopper-medium-replay406080100120140160180020406080100120hopper-medium-expert255075100125150175Eval RTG020406080100120Returnhalfcheetah-medium255075100125150175Eval RTG020406080100120halfcheetah-medium-replay255075100125150175Eval RTG020406080100120halfcheetah-medium-expertDTDT+WRvSRvS+Wmax return in offline dataexpert returnUnder review as a conference paper at ICLR 2023\n\nFigure 3: Performance of DT when the state and RTG tokens are concatenated. We report the mean and standard deviation of 10 seeds.\n\nAlgorithm 2: ConserWeightive Behavioral Cloning (CWBC) for RvS\n\n1 Input: dataset Toffline, number of iterations I, batch size S, regularization coefficient α, initial\n\nparameters θ0\n\n2 for iteration i “ 1, . . . , I do\n\n3\n\n4\n\n5\n\n6\n\n7\n\nSample a batch of trajectories B Ð tτ p1q, . . . , τ pSqu from Toffline using Algorithm 1. for every sampled trajectory τ piq do\n\nSamplie noise ε as described in Section 4.2.1. Compute noisy RTGs: gε\n\nt Ð gt ` ε, 1 ď t ď |τ piq|.\n\n// loss and regularizer defined in Equation (4) and (7) Perform gradient update of θ by minimizing the regularized empirical risk pLB\n\nRvSpθq ` α ̈\n\npCB RvSpθq.\n\n8 Output: πθ\n\nnot. We hypothesize that the conservative behavior of DT comes from the transformer architecture. As the policy conditions on a sequence of both state tokens and RTG tokens to predict next action, the attention layers can choose to ignore the ood RTG tokens while still obtaining a good prediction loss. To test this hypothesis, we experiment with a slightly modified version of DT, where we concatenate the state and RTG at each timestep instead of treating them as separate tokens. By doing this, the model cannot ignore the RTG information in the sequence. We call this version DT-Concat. Figure 3 shows that the performance of DT-Concat is strongly correlated with the conditioning RTG, and degrades quickly when the target return is out-of-distribution. This result confirms our hypothesis.\n\nHowever, conservatism does not have to come from the architecture, but can also emerge from a proper objective function, as commonly done in conservative value-based methods (Kumar et al., 2020; Fujimoto & Gu, 2021). In this section, we propose a novel conservative regularization for BC that explicitly encourages the policy to stay close to the data distribution. The intuition is to enforce the predicted actions when conditioning on large ood returns to stay close to the in-distribution actions. To do that, for a trajectory τ with high return, we inject positive random noise ε „ Eτ to its RTGs, and penalize the l2 distance between the predicted action and the ground truth. Specifically, to guarantee we generate large ood returns, we choose a noise distribution E such that the perturbed initial RTG g1 ` ε is at least pr‹, the highest return in the dataset. The next subsections instantiate the conservative regularizer in the context of RvS, and empirically evaluate its performance.\n\n4.2.1\n\nIMPLEMENTATION DETAILS\n\nWe apply conservative regularization to trajectories whose returns are above prq, the q-th percentile of returns in the dataset. This makes sure that when conditioned on ood returns, the policy behaves similarly to high-return trajectories and not to a random trajectory in the dataset. We sample a scalar noise ε „ Eτ and offset the RTG of τ at every timestep by ε: gε t “ gt ` ε, t “ 1, . . . , |τ |, resulting in the conservative regularizer:\n\nCRvSpθq “ Eτ „T , ε„Eτ\n\n1rτ ąprq ̈ 1\n\n|τ |\n\n“\n\nř\n\n`\n\n|τ | t“1\n\nat ́ πθpst, ωε t q\n\n ̆\n\n2\n\n‰ ,\n\n(7)\n\nwhere ωε t “ pgt `εq{pH ́t`1q (cf. Equation (3)) is the noisy average RTG at timestep t. We observe that using the 95-th percentile of pr95 generally works well across different environments and datasets.\n\n7\n\n20406080100120140160180020406080100120Returnwalker2d-medium-replay406080100120140160180020406080100120Returnhopper-medium-replay255075100125150175Eval RTG020406080100120Returnhalfcheetah-medium-replayDTDT-Concatmax return in offline dataexpert returnUnder review as a conference paper at ICLR 2023\n\nTable 2: Comparison of the normalized return on the D4RL locomotion benchmark. For BC and TD3+BC, we get the numbers from (Emmons et al., 2021). For IQL, we get the numbers from (Kostrikov et al., 2021b). For TTO, we get the numbers from (Janner et al., 2021). The results are averaged over 10 seeds.\n\nRvS\n\nRvS+W\n\nRvS+C\n\nRvS+W+C\n\nDT\n\nBC\n\nTD3+BC CQL\n\nIQL\n\nTTO\n\nwalker2d-medium walker2d-med-replay walker2d-med-expert\n\nhopper-medium hopper-med-replay hopper-med-expert\n\n73.3 ̆ 5.7 54.0 ̆ 12.1 102.2 ̆ 2.3\n\n56.6 ̆ 5.5 87.7 ̆ 9.7 108.8 ̆ 0.9\n\n54.5 ̆ 7.7 61.2 ̆ 14.7 104.1 ̆ 0.5\n\n62.5 ̆ 7.1 92.4 ̆ 6.1 108.4 ̆ 1.8\n\nhalfcheetah-medium halfcheetah-med-replay ́0.4 ̆ 2.7 ́0.8 ̆ 2.2 halfcheetah-med-expert 69.1 ̆ 3.7\n\n16.2 ̆ 4.5\n\n83.4 ̆ 2.1\n\n4.0 ̆ 5.4\n\n71.3 ̆ 4.9 62.0 ̆ 13.5 102.1 ̆ 10.2\n\n61.0 ̆ 5.3 91.5 ̆ 3.5 101.0 ̆ 13.4\n\n40.7 ̆ 1.0 36.8 ̆ 1.5 91.2 ̆ 1.0\n\n73.6 ̆ 5.4 72.8 ̆ 7.5 107.6 ̆ 0.5\n\n62.9 ̆ 3.6 87.7 ̆ 4.2 110.0 ̆ 2.8\n\n42.2 ̆ 0.7 40.4 ̆ 0.8 91.1 ̆ 2.0\n\n# wins average\n\n/ 64.6\n\n4 61.7\n\n6 73.1\n\n9 76.5\n\n71.5 ̆ 3.9 53.4 ̆ 12.2 99.8 ̆ 21.3\n\n59.9 ̆ 4.9 56.4 ̆ 20.1 95.4 ̆ 11.3\n\n42.5 ̆ 0.6 34.5 ̆ 4.2 87.2 ̆ 2.7\n\n/ 66.73\n\n75.3 26.0 107.5\n\n83.7 81.8 110.1\n\n52.9 18.1 52.5\n\n42.6 36.6 55.2\n\n/ 51.86\n\n59.3 60.9 98.0\n\n48.3 44.6 90.7\n\n/ 75.3\n\n82.9 86.1 109.5\n\n64.6 97.8 102.0\n\n49.1 47.3 85.8\n\n/ 80.6\n\n78.3 73.9 109.6\n\n66.3 94.7 91.5\n\n47.4 44.2 86.7\n\n/ 77.0\n\n81.3 ̆ 8.0 79.4 ̆ 12.8 91.0 ̆ 10.8\n\n67.4 ̆ 11.3 99.4 ̆ 12.6 106.0 ̆ 1.1\n\n44.0 ̆ 1.2 44.1 ̆ 3.5 40.8 ̆ 8.7\n\n72.6\n\nWe use the noise distribution Eτ “ Uniformrlτ , uτ s, where the lower bound lτ “ pr‹ ́ rτ so that the perturbed initial RTG gε 12σ2 so that the standard deviation of Eτ is equal to σ. We emphasize our conservative regularizer is distinct from the other conservative components proposed for the value-based offline RL methods. While those usually try to regularize the value function estimation to prevent extrapolation error (Fujimoto et al., 2019), we perturb the returns to generate ood conditioning and regularize the predicted actions.\n\n1 “ rτ ` ε is no less than pr‹, and the upper bound uτ “ pr‹ ́ rτ `\n\n?\n\nWhen the conservative regularizer is used, the final objective for training RvS is LRvSpθq ` α ̈ CRvSpθq, in which α is the regularization coefficient. When trajectory reweighting is used in conjunction with the conservative regularizer, we obtain ConserWeightive Behavioral Cloning (CWBC), which combines the best of both components. We provide a pseudo code for CWBC in Algorithm 2.\n\n4.2.2 EMPIRICAL RESULTS\n\nDataset We evaluate the effectiveness of the conservative regularizer, as well as the performance of CWBC as a whole on the D4RL datasets (Fu et al., 2020) for the gym locomotion tasks.\n\nBaselines We apply the conservative regularizer, which we denote as +C, to both RvS and RvS+W. In addition, we report the performance of three value-based methods: TD3+BC (Fujimoto & Gu, 2021), CQL (Kumar et al., 2020), and IQL (Kostrikov et al., 2021b) as a reference.\n\nHyperparameters We apply our conservatism regularization to trajectories whose returns are above the q “ 95-th percentile return in the dataset, and perturb their RTGs as described in Section 4.2.1. We use a regularization coefficient of α “ 1. The evaluation protocol is similar to Section 4.1.2.\n\nResults Table 2 reports the performance of different methods we consider. Our proposed framework CWBC with all components enabled (RvS+W+C) significantly outperforms the original RvS on 9{9 datasets, with an average improvement of 18% over RvS. RvS+W+C is also the best performing BC method in the table, and is competitive with the value-based methods. Conservative regularization consistently improves the results for both RvS and RvS+W. Trajectory weighting on its own can have varying effects on performance, but is synergistic when combined with RvS+C leading to our best performing model in RvS+W+C.\n\nTo better understand the impact of each component, we plot the achieved returns of RvS and other variants when conditioning on different values of conditioned RTG. Figure 4 shows that RvS generalizes poorly to out-of-distribution RTGs, which leads to significant performance drop when the evaluation RTG is larger than the best return in the dataset. Figure 4 illustrates the significant importance of encouraging conservatism for RvS, where RvS+C has much more stable performance, even when the evaluation RTG is 2ˆ the expert return. By explicitly asking the model to stay close to the data distribution, we achieve more reliable out-of-distribution performance, and avoid the performance crash problem. This leads to absolute performance improvement of RvS+C in Table 2. CWBC combines the best of both weighting and conservatism, which enjoys good performance when conditioning on high RTG values, as well as better robustness to large, out-of-distribution RTGs.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Performance of RvS and its variants when conditioning on different evaluation RTGs. We report the mean and standard deviation of 10 seeds.\n\nIn addition to the main results, we include ablations for different choices of conservative percentile q and regularization coefficient α in Appendix C. Finally, we also evaluate CWBC in two more benchmarks: Atari games (Bellemare et al., 2013) and the D4RL Antmaze datasets. We present these results in Appendix D and E respectively.\n\n5 CONCLUSION\n\nWe proposed ConserWeightive Behavioral Cloning (CWBC), a new framework that extends BC for offline RL with two novel components: trajectory weighting and conservative regularization. Trajectory weighting balances the bias-variance tradeoff that arises in learning from a suboptimal dataset, improving the performance of both DT and RvS. Next, we showed that while DT is selfconservative due to its attention architecture, we can recover this desired behavior even for RvS using our proposed conservative regularizer. Confirmed by the experiments, CWBC significantly improves the performance and stability of RvS.\n\nWhile we made good progress for BC, advanced value-based methods such as CQL and IQL are still ahead and we believe further understanding of the tradeoffs in both kinds of approaches is important future work. Another promising direction from a data perspective is how to combine datasets from multiple environments to obtain diverse, high-quality data. Recent works have shown promising results in this direction (Reed et al., 2022). Last but not least, while CWBC significantly improves the performance and reliability of RvS, it is not able to extrapolate beyond the offline dataset. How to obtain extrapolation, or whether it is possible, is still an open question, and poses a persistent research opportunity for not only CWBC but the whole offline RL community.\n\nREPRODUCIBILITY STATEMENT\n\nWe present the practical implementation of our framework in Section 4.1.1 and Section 4. We include the implementation details of our paper in Appendix B, which contains information about the datasets we use, the open sourced code we base on, and the list of hyperparameters we use to reproduce our results. Finally, we submitted the source code in the supplementary material.\n\n9\n\n255075100125150175200020406080100120Returnwalker2d-medium255075100125150175200020406080100120walker2d-medium-replay255075100125150175200020406080100120walker2d-medium-expert255075100125150175200020406080100120Returnhopper-medium255075100125150175200020406080100120hopper-medium-replay255075100125150175200020406080100120hopper-medium-expert255075100125150175200Eval RTG020406080100120Returnhalfcheetah-medium255075100125150175200Eval RTG020406080100120halfcheetah-medium-replay255075100125150175200Eval RTG020406080100120halfcheetah-medium-expertRvSRvS+WRvS+CRvS+W+Cmax return in offline dataexpert returnUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pp. 104–114. PMLR, 2020.\n\nMichael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelligence\n\n15, pp. 103–129, 1995.\n\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253–279, 2013.\n\nRichard Bellman. A markovian decision process. Indiana Univ. Math. J., 1957.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 2021.\n\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, pp. 1691– 1703. PMLR, 2020.\n\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for\n\noffline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.\n\nDylan J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu. Offline reinforcement learning: Fundamental barriers for value function approximation. arXiv preprint arXiv:2111.10919, 2021.\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\n\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n\nScott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.\n\nAdvances in Neural Information Processing Systems, 34, 2021.\n\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019.\n\nSeyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max q-learning operator for simple yet effective offline and online rl. In International Conference on Machine Learning, pp. 3682–3691. PMLR, 2021.\n\nDibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. arXiv preprint arXiv:1912.06088, 2019.\n\nWenshuo Guo, Kumar Krishna Agrawal, Aditya Grover, Vidya Muthukumar, and Ashwin Pananjady. Learning from an exploring demonstrator: Optimal reward estimation for bandits. arXiv preprint arXiv:2106.14866, 2021.\n\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\n\nmodeling problem. Advances in neural information processing systems, 34, 2021.\n\nNatasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. Advances in neural information processing systems, 33: 21810–21823, 2020.\n\nIlya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In International Conference on Machine Learning, pp. 5774–5783. PMLR, 2021a.\n\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\n\nq-learning. arXiv preprint arXiv:2110.06169, 2021b.\n\nSiddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. Generative pretraining for\n\nblack-box optimization. arXiv preprint arXiv:2206.10786, 2022.\n\nAviral Kumar and Sergey Levine. Model inversion networks for model-based optimization. Advances\n\nin Neural Information Processing Systems, 33:5126–5137, 2020.\n\nAviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.\n\nNathan Lambert, Markus Wulfmeier, William Whitney, Arunkumar Byravan, Michael Bloesch, Vibhavari Dasagi, Tim Hertweck, and Martin Riedmiller. The challenges of exploration for offline reinforcement learning. arXiv preprint arXiv:2201.11861, 2022.\n\nSascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement\n\nlearning, pp. 45–73. Springer, 2012.\n\nMichael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark. arXiv preprint arXiv:2110.15191, 2021.\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,\n\nreview, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal computation engines. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n\nOfir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:\n\nPolicy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.\n\nTung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning\n\nvia sequence modeling. arXiv preprint arXiv:2207.04179, 2022.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\n\nstanding by generative pre-training. 2018.\n\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.\n\nJuergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards–just map them to\n\nactions. arXiv preprint arXiv:1912.02875, 2019.\n\nRupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja ́skowski, and Jürgen Schmidhuber. Training agents using upside-down reinforcement learning. arXiv preprint arXiv:1912.02877, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nRuosong Wang, Dean P Foster, and Sham M Kakade. What are the statistical limits of offline rl with\n\nlinear function approximation? arXiv preprint arXiv:2010.11895, 2020.\n\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.\n\narXiv preprint arXiv:1911.11361, 2019.\n\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using\n\nvq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.\n\nDenis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, Pieter Abbeel, Alessandro Lazaric, and Lerrel Pinto. Don’t change the algorithm, change the data: Exploratory data for offline reinforcement learning. arXiv preprint arXiv:2201.13425, 2022.\n\nTianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129–14142, 2020.\n\nAndrea Zanette. Exponential lower bounds for batch reinforcement learning: Batch rl can be exponentially harder than online rl. In International Conference on Machine Learning, pp. 12287– 12297. PMLR, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA LIST OF SYMBOLS\n\nTable 3: Important symbols used in this paper.\n\nSymbol\n\nMeaning\n\nDefinition\n\nS A\nτ |τ | T\nToffline π\nθ st at rt rτ gt H\nωt LRvS CRvS fT pτ q p rT pτ q b\n|b| fToffline pbq Pbinpbq ̄rb pr‹ prq\n\nτ\n\nstate space action space trajectory trajectory length distribution of trajectories offline dataset policy policy parameters state at timestep t action at timestep t reward at timestep t trajectory return return-to-go at timestep t maximum trajectory length average return-to-go at timestep t empirical risk of RvS conservative regularizer for RvS probability density of trajectory τ „ T probability density of trajectory τ „ rT index of a bin of trajectories in the offline dataset size of bin b proportion of trajectories in bin b probability that bin b is sampled average return of trajectories in bin b highest return in the offline dataset q-th percentile of the returns in the offline dataset\n\nř ř\n\n|τ |\n\nt“1 rt t1“t r1\n\n|τ |\n\nt\n\ngt{pH ́ t ` 1q Equation (4) Equation (7)\n\nEquation (5)\n\n|b|{|Toffline| Equation (6)\n\nB IMPLEMENTATION DETAILS\n\nB.1 DATASETS AND SOURCE CODE\n\nWe train and evaluate our models on the D4RL (Fu et al., 2020) and Atari (Agarwal et al., 2020) benchmarks, which are available at https://github.com/rail-berkeley/d4rl and https://research.google/tools/datasets/dqn-replay, respectively. Our codebase is largely based on the RvS (Emmons et al., 2021) official implementation at https: //github.com/scottemmons/rvs, and DT (Chen et al., 2021) official implementation at https://github.com/kzl/decision-transformer.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB.2 DEFAULT HYPERPARAMETERS\n\nTable 4: Hyperparameters used for locomotion experiments.\n\nModel\n\nHyperparameter\n\nValue\n\nContext length K (DT) Number of attention heads (DT) Hidden layers Hidden dimension Activation function Dropout\n\n20 1\n2 for RvS, 3 for DT 1024 for RvS, 128 for DT ReLU 0.0 for RvS, 0.1 for DT\n\nConservative regularizer\n\nTrajectory weighting\n\nConservative percentile q Noise standard deviation σ Regularization coefficient α\n\n# bins B Smoothing parameter λ Smoothing parameter κ\n\n95 1000 1.0\n\n20 0.01 pr‹ ́ pr90\n\nOptimization\n\nBatch size Learning rate Weight decay Training iterations\n\n64 1e ́ 3 for RvS, 1e ́ 4 for DT 1e ́ 4 100000\n\nEvaluation\n\nTarget return\n\n1ˆ Expert return\n\nTable 5: Hyperparameters used for Atari experiments.\n\nHyperparameter\n\nValue\n\nModel\n\nEncoder channels Encoder filter sizes Encoder strides Hidden layers Hidden dimension Activation function Dropout\n\nConservative percentile q\n\nConservative regularization\n\nNoise std σ\n\nTrajectory weighting\n\nOptimization\n\nConservative weight α\n\n# bins B λ\nκ\n\nBatch size Learning rate Weight decay Training iterations\n\nEvaluation\n\nTarget return\n\n32, 32, 64 8 ˆ 8, 4 ˆ 4, 3 ˆ 3 4, 2, 1 4\n1024 ReLU 0.1\n\n95 50 for Breakout, Pong 500 for Qbert, Seaquest 0.1\n\n20 0.1 pr‹ ́ pr50\n\n128 6e ́ 4 1e ́ 4 25000\n\n90 for Breakout (1ˆ max in dataset) 2500 for Qbert (5ˆ max in dataset) 20 for Pong (1ˆ max in dataset) 1450 for Seaquest (5ˆ max in dataset)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nC ABLATION ANALYSIS\n\nIn this section, we investigate the impact of each of those hyperparameters on CWBC to give insights on what values work well in practice. We use the walker2d environment and the three related datasets for illustration. In all the experiments, when we vary one hyperparameter, the other hyperparameters are kept as in Table 4.\n\nC.1 TRAJECTORY WEIGHTING: SMOOTHING PARAMETERS λ AND κ\n\nTwo hyperparameters κ and λ in Equation (6) affect the probability a bin index b is sampled:\n\nPbinpbq 9\n\nfT\n\noffline pbq offline pbq`λ ̈ exp\n\nfT\n\n`\n\n ́ |srb\n\nτ ́pr‹| κ\n\n ̆\n\n.\n\nIn practice, we have observed that the performance of CWBC is considerably robust to a wide range of values of κ and λ.\n\nThe impact of κ The smoothing parameter κ controls how we weight the trajectories based on their relative returns. Intuitively, smaller κ gives more weights to high-return bins (and thus their trajectories), and larger κ makes the transformed distribution more uniform. We illustrate the effect of κ on the transformed distribution and the performance of CWBC in Figure 5. As in Section 4.1.2, we set κ to be the difference between the empirical highest return pr‹ and the z-th percentile return in the dataset: κ “ pr‹ ́ prz, and we vary the values of z. This allows the actual value of κ to adapt to different datasets.\n\nFigure 5 shows the results. The top row plots the distributions of returns before and after trajectory weighting for varying values of κ. We tested four values z P t99, 90, 50, 0u, which correspond to four increasing values of κ. We mark the actual values of κ in each dataset in the top tow1. For each dataset, we can see the transformed distribution using small κ (orange) highly concentrates on high returns; as κ increases, the density for low returns increases and the distribution becomes more and more uniform. The bottom row plots the corresponding performance of CWBC with different choices of κ. We select RvS+C as our baseline model, which does not have trajectory weighting but has the conservative regularization enabled. We can see that relatively small values of κ (based on pr99 , pr90 and pr50) perform well on all the three datasets, whereas large values (based on pr0) hurt the performance for the med-expert dataset, and even underperform the baseline RvS+C.\n\nFigure 5: The influence of κ on the transformed distribution (top) and on the performance of CWBC (bottom). The legend in each panel (top) shows the absolute values of κ for easier comparison. In the bottom row, we also plot the results of RvS+C (no trajectory weighting) as a baseline.\n\n1pr0 is defined to be the lowest return in the dataset: pr0 “ minτ PToffline rτ .\n\n15\n\n020406080100Normalized return r0.000.010.020.030.040.05Densitywalker2d-medium-replayoriginal =9.7=47.4=82.5=91.1020406080100Normalized return r0.000.020.040.060.080.100.12walker2d-mediumoriginal =6.2=9.5=15.8=92.2020406080100Normalized return r0.00.10.20.30.40.5walker2d-medium-expertoriginal =0.6=1.3=26.3=109.350100150200Eval RTG020406080Returnwalker2d-medium-replay50100150200Eval RTG4050607080walker2d-medium50100150200Eval RTG406080100120walker2d-medium-expertRvS+C=rr99=rr90=rr50=rr0Under review as a conference paper at ICLR 2023\n\nThe impact of λ To better understand the role of λ, we can rewrite Equation (6) as\n\nhkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkj T1 ̆\n`\n\nPbinpbq 9\n\nfToffline pbq exp\n\n ́ |srb\n\nτ ́pr‹| κ\n\nhkkkkkkkkkkkikkkkkkkkkkkj T2 ̆( ␣\n\n`\n\n ̈\n\n1{\n\nfToffline pbq ` λ\n\n.\n\nClearly, only T2 depends on λ. When λ “ 0, T2 is canceled out and the above equation reduces to\n\nPbinpbq 9 exp\n\n`\n\n ́ |srb\n\nτ ́pr‹| κ\n\n ̆\n\n,\n\nwhich purely depends on the relative return. As λ increases, T2 is less sensitive to fToffline pbq, and finally becomes the same for every b P rBs as λ Ñ 8. In that scenario, Pbinpbq only depends on T1, which is the original frequency fToffline pbq weighted by the relative return.\n\nThe top row of Figure 6 plots the distributions of returns before and after trajectory weighting with different values of λ. When λ “ 0, the distributions concentrate on high returns. As λ increases, the distributions are more correlated with the original one, but still weights more on the high-return region compared to the original distribution due to the exponential term in T1. The bottom row of Figure 6 plots the actual performance of CWBC as λ varies. All values of λ produce similar results, which are consistently better than or comparable to training on the original datset (RvS+C).\n\nFigure 6: The influence of λ on the transformed distribution (top) and on the performance of CWBC (bottom). We plot the result of RvS+C as the baseline.\n\nC.2 CONSERVATIVE REGULARIZATION: PERCENTILE q\n\nWe only apply the conservative regularization to trajectories whose return is above the q-th percentile of the returns in the dataset. Intuitively, a larger q applies the regularization to fewer trajectories. We test four values for q: 0, 50, 95, and 99. For q “ 0, our regularization applies to all the trajectories in the dataset. Figure 7 demonstrates the impact of q on the performance of CWBC. q “ 95 and q “ 99 perform well on all the three datasets, while q “ 50 and q “ 0 lead to poor results for the med-replay dataset. This is because, when the regularization applies to trajectories of low returns, the regularizer will force the policy conditioned on out-of-distribution RTGs to stay close to the actions from low return trajectories. Since the med-replay dataset contains many low return trajectories (see Figure 5), such regularization results in poor performance. In contrast, medium and med-expert datasets contain a much larger portion of high return trajectories, and they are less sensitive to the choice of q.\n\n16\n\n020406080100Normalized return r0.000.010.020.030.040.05Densitywalker2d-medium-replay020406080100Normalized return r0.0000.0250.0500.0750.1000.125walker2d-medium020406080100Normalized return r0.00.10.20.30.40.5walker2d-medium-expert50100150200Eval RTG20406080Returnwalker2d-medium-replay50100150200Eval RTG50607080walker2d-medium50100150200Eval RTG20406080100walker2d-medium-expertRvS+C=0.0=0.01=1.0=100.0Under review as a conference paper at ICLR 2023\n\nFigure 7: Performance of CWBC with different values of the conservative percentile q.\n\nC.3 REGULARIZATION COEFFICIENT α\n\nThe hyperparameter α controls the weight of the conservative regularization in the final objective function of CWBC LRvS ` α ̈ CRvS. We show the performance of CWBC with different values of α in Figure 8. Not using any regularization (α “ 0) suffers from the performance crash problem, while overly aggressive regularization (α “ 10) also hurts the performance. CWBC is robust to the other non-extreme values of α .\n\nFigure 8: Performance of CWBC with different values of α.\n\nD ADDITIONAL RESULTS ON ATARI GAMES\n\nIn addition to D4RL, we consider 4 games from the Atari benchmark (Bellemare et al., 2013): Breakout, Qbert, Pong, and Seaquest. Similar to (Chen et al., 2021), for each game, we train our method on 500000 transitions sampled from the DQN-replay dataset, which consists of 50 million transitions of an online DQN agent (Mnih et al., 2015). Due to the varying performance of the DQN agent in different games, the quality of the datasets also varies. While Breakout and Pong datasets are high-quality with many expert transitions, Qbert and Seaquest datasets are highly suboptimal. Hyperparameters For trajectory weighting, we use B “ 20 bins, λ “ 0.1, and κ “ pr‹ ́ pr50. We apply conservative regularization with coefficient α “ 0.1 to trajectories whose returns are above pr95. The standard deviation of the noise distribution varies across datasets, as each different games have very different return ranges. During evaluation, we set the target return to 5 ˆ pr‹ for Qbert and Seaquest, and to 1 ˆ pr‹ for Breakout and Pong.\n\nFigure 9: Performance of RvS and its variants on Atari games when conditioning on different evaluation RTGs.\n\n17\n\n50100150200Eval RTG20406080Returnwalker2d-medium-replay50100150200Eval RTG50607080walker2d-medium50100150200Eval RTG20406080100walker2d-medium-expertq=0q=50q=95q=9950100150200Eval RTG020406080Returnwalker2d-medium-replay50100150200Eval RTG20406080walker2d-medium50100150200Eval RTG020406080100120walker2d-medium-expert=0=0.1=0.5=1.0=10.0050100150200250300350Eval RTG050100150200250300ReturnBreakout0.02.55.07.510.012.515.017.5Eval RTG0510152025Qbert020406080100Eval RTG20406080100Pong0.00.51.01.52.02.53.0Eval RTG0.00.51.01.52.0SeaquestRvSRvS+WRvS+CRvS+W+Cmax return in offline dataUnder review as a conference paper at ICLR 2023\n\nTable 6: Comparison of the normalized return on Atari games. The results are averaged over 3 seeds. We include the results of DT, CQL, and BC from (Chen et al., 2021) for reference.\n\nRvS\n\nRvS+W\n\nRvS+C\n\nRvS+W+C\n\nDT\n\nCQL\n\nBC\n\nBreakout Qbert Pong Seaquest\n\n# wins average\n\n126.9 ̆ 38.0 ́0.4 ̆ 0.2 75.7 ̆ 8.6 0.2 ̆ 0.2\n\n120.1 ̆ 28.43 0.0 ̆ 0.4 90.7 ̆ 6.4 ́0.1 ̆ 0.1\n\n163.0 ̆ 50.4 12.4 ̆ 8.6 84.1 ̆ 9.6 1.6 ̆ 0.2\n\n237.3 ̆ 82.1 19.1 ̆ 2.7 90.4 ̆ 1.9 1.4 ̆ 0.3\n\n267.5 ̆ 97.5 15.4 ̆ 11.4 106.1 ̆ 8.1 2.5 ̆ 0.4\n\n211.1 104.2 111.9 1.7\n\n138.9 ̆ 61.7 17.3 ̆ 14.7 85.2 ̆ 20.0 2.1 ̆ 0.3\n\n/ 50.6\n\n1 52.7\n\n4 62.3\n\n4 87.1\n\n97.9\n\n107.2\n\n60.9\n\nResults Table 6 summarizes the performance of RvS and its variants. CWBC (RvS+W+C) is the best method, outperforming the original RvS by 72% on average. Figure 9 clearly shows the effectiveness of the conservative regularization (+C). In two low-quality datasets Qbert and Seaquest, the performance of RvS degrades quickly when conditioning on out-of-distribution RTGs. By regularizing the policy to stay close to the data distribution, we achieve a much more stable performance. The trajectory weighting component (+W) alone has varying effects on performance because of the performance crash problem, but achieves state-of-the-art when used in conjunction with conservative regularization.\n\nIt is also worth noting that in both Qbert and Seaquest, CWBC achieves returns that are much higher than the best return in the offline dataset. This shows that while conservatism encourages the policy to stay close to the data distribution, it does not prohibit extrapolation. There is always a trade-off between optimizing the original supervised objective (which presumably allows extrapolation) and the conservative objective. This is very similar to other conservative regularizations used in value-based such as CQL or TD3+BC, where there is a trade-off between learning the value function and staying close to the data distribution.\n\nE ADDITIONAL RESULTS ON D4RL ANTMAZE\n\nOur proposed conservative regularization is especially important in dense reward environments such as gym locomotion tasks or Atari games, where choosing the target return during evaluation is a difficult problem. On the other hand, trajectory weighting is generally useful whenever the offline dataset contains both low-return and high-return trajectories. In this section, we consider Antmaze (Fu et al., 2020), a sparse reward environment in the D4RL benchmark to evaluate the generality of CWBC. Antmaze is a navigation domain in which the task is to control a complex 8-DoF \"Ant\" quadruped robot to reach a goal location. We consider 3 maze layouts: umaze, medium, and large, and 3 dataset flavors: v0, diverse, and play. We use the same set of hyperparameters as mentioned in B.2.\n\nTable 7: Comparison of the success rate on the Antmaze environment. The results are averaged over 3 seeds. We include the results of DT, CQL, and BC from (Emmons et al., 2021) for reference.\n\nRvS\n\nRvS+W\n\nRvS+C\n\nRvS+W+C\n\nDT\n\nCQL BC\n\numaze-v0 umaze-diverse\n\n54.0 ̆ 13.56 55.0 ̆ 15.65\n\n65.0 ̆ 18.03 46.0 ̆ 16.85\n\n58.0 ̆ 8.72 50.0 ̆ 10.95\n\n65.0 ̆ 12.85 42.0 ̆ 7.48\n\n65.6 51.2\n\n44.8 23.4\n\n54.6 45.6\n\nmedium-play medium-diverse\n\nlarge-play large-diverse\n\n# wins average\n\n0.0 ̆ 0.0 1.0 ̆ 3.0\n\n0.0 ̆ 0.0 0.0 ̆ 0.0\n\n/ 18.3\n\n26.0 ̆ 12.0 24.0 ̆ 15.62\n\n4.0 ̆ 4.9 10.0 ̆ 10.0\n\n4 29.2\n\n0.0 ̆ 0.0 1.0 ̆ 3.0\n\n0.0 ̆ 0.0 0.0 ̆ 0.0\n\n1 18.2\n\n25.0 ̆ 13.6 23.0 ̆ 11.0\n\n5.0 ̆ 6.71 17.0 ̆ 11.87\n\n1.0 0.6\n\n0.0 0.2\n\n0.0 0.0\n\n0.0 0.0\n\n0.0 0.0\n\n0.0 0.0\n\n4 29.5\n\n19.8\n\n11.4\n\n16.7\n\nResults Table 7 summarizes the results. As expected, the conservative regularization is not important in these tasks, as the target return is either 0 (fail) or 1 (success). However, the trajectory weighting significantly boosts performance, resulting in an average of 60% improvement over the original RvS.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nF TRAJECTORY WEIGHTING VERSUS HARD FILTERING\n\nAn alternative to trajectory weighting is hard filtering (+F), where we train the model on only top 10% trajectories with the highest returns. Filtering can be considered a hard weighting mechanism, wherein the transformed distribution only has support over trajectories with returns above a certain threshold.\n\nF.1 HARD FILTERING FOR RVS\n\nWhen using hard filtering for RvS, we also consider combining it with the conservative regularization. Table 8 and Figure 10 compare the performance of trajectory weighting and hard filtering when applied to RvS. While RvS+F+C also gains notable improvements , it lags behind RvS+W+C and seems to erode the benefits of conservatism alone in RvS+C. This agrees with our analysis in Section 4.1. While hard filtering achieves the same effect of reducing bias, it completely removes the low-return trajectories, resulting in highly increased variance. Our trajectory weighting upweights the good trajectories but aims to stay close to the original data distribution, balancing this bias-variance tradeoff. This is clearly shown in Figure 10, where RvS+W+C has much smaller variance when conditioning on large RTGs.\n\nTable 8: Comparison of trajectory weighting (+W) and hard filtering (+F) on D4RL locomotion benchmarks. The results are averaged over 10 seeds.\n\nRvS\n\nRvS+W\n\nRvS+C\n\nRvS+W+C\n\nRvS+F\n\nRvS+F+C\n\nwalker2d-medium walker2d-med-replay walker2d-med-expert\n\nhopper-medium hopper-med-replay hopper-med-expert\n\n73.3 ̆ 5.7 54.0 ̆ 12.1 102.2 ̆ 2.3\n\n56.6 ̆ 5.5 87.7 ̆ 9.7 108.8 ̆ 0.9\n\n54.5 ̆ 7.7 61.2 ̆ 14.7 104.1 ̆ 0.5\n\n62.5 ̆ 7.1 92.4 ̆ 6.1 108.4 ̆ 1.8\n\nhalfcheetah-medium halfcheetah-med-replay ́0.4 ̆ 2.7 ́0.8 ̆ 2.2 halfcheetah-med-expert 69.1 ̆ 3.7\n\n83.4 ̆ 2.1\n\n16.2 ̆ 4.5\n\n4.0 ̆ 5.4\n\n71.3 ̆ 4.9 62.0 ̆ 13.5 102.1 ̆ 10.2\n\n61.0 ̆ 5.3 91.5 ̆ 3.5 101.0 ̆ 13.4\n\n40.7 ̆ 1.0 36.8 ̆ 1.5 91.2 ̆ 1.0\n\n73.6 ̆ 5.4 72.8 ̆ 7.5 107.6 ̆ 0.5\n\n62.9 ̆ 3.6 87.7 ̆ 4.2 110.0 ̆ 2.8\n\n42.2 ̆ 0.7 40.4 ̆ 0.8 91.1 ̆ 2.0\n\n60.9 ̆ 4.9 47.1 ̆ 7.7 101.7 ̆ 3.3\n\n62.4 ̆ 5.0 91.2 ̆ 5.3 97.5 ̆ 15.0\n\n1.4 ̆ 3.3 ́0.1 ̆ 3.5 46.0 ̆ 1.5\n\n68.2 ̆ 7.1 53.9 ̆ 11.0 105.4 ̆ 0.6\n\n65.7 ̆ 6.4 92.1 ̆ 2.9 105.8 ̆ 3.5\n\n36.2 ̆ 2.5 35.7 ̆ 2.8 83.2 ̆ 5.0\n\n# wins average\n\n/ 64.6\n\n4 61.7\n\n6 73.1\n\n9 76.5\n\n3 56.5\n\n5 71.8\n\nFigure 10: Comparison of trajectory weighting and hard filtering.\n\n19\n\n255075100125150175200020406080100120Returnwalker2d-medium255075100125150175200020406080100120walker2d-medium-replay255075100125150175200020406080100120walker2d-medium-expert255075100125150175200020406080100120Returnhopper-medium255075100125150175200020406080100120hopper-medium-replay255075100125150175200020406080100120hopper-medium-expert255075100125150175200Eval RTG020406080100120Returnhalfcheetah-medium255075100125150175200Eval RTG020406080100120halfcheetah-medium-replay255075100125150175200Eval RTG020406080100120halfcheetah-medium-expertRvS+WRvS+W+CRvS+FRvS+F+Cmax return in offline dataexpert returnUnder review as a conference paper at ICLR 2023\n\nF.2 HARD FILTERING FOR UNCONDITIONAL BC\n\nHard filtering can also be applied to ordinary BC. This is equivalent to Filtered BC in (Emmons et al., 2021). Table 9 compares Filtered BC and CWBC. CWBC performs comparably well in medium and med-expert datasets, and outperforms Filtered BC significantly with an average improvement of 12% in med-replay datasets. We believe that in low-quality datasets, even when we filter out 90% percent of the data, the quality of the remaining trajectories is still very diverse that simple imitation learning is not good enough. CWBC is able to learn from such diverse data, and by conditioning on expert return at test time, we can recover an efficient policy.\n\nTable 9: The normalized return on D4RL for Filtered BC, RvS, and CWBC. For Filtered BC, we get the numbers from (Emmons et al., 2021).\n\nFiltered BC RvS\n\n73.3 ̆ 5.7 56.6 ̆ 5.5 16.2 ̆ 4.5\n\nRvS+W+C\n\n73.6 ̆ 5.4 62.9 ̆ 3.6 42.2 ̆ 0.7\n\n48.7\n\n59.6\n\n54.0 ̆ 12.1 87.7 ̆ 9.7 ́0.4 ̆ 2.7\n\n72.8 ̆ 7.5 87.7 ̆ 5.2 40.4 ̆ 0.8\n\n47.1\n\n67.0\n\n102.2 ̆ 2.3 108.8 ̆ 0.9 83.4 ̆ 2.1\n\n107.6 ̆ 0.5 110.0 ̆ 2.8 91.1 ̆ 2.0\n\n98.1\n\n64.6\n\n102.9\n\n76.5\n\nwalker2d-medium hopper-medium halfcheetah-medium\n\nmedium average\n\nwalker2d-med-replay hopper-med-replay halfcheetah-med-replay\n\nmed-replay average\n\nwalker2d-med-expert hopper-med-expert halfcheetah-med-expert\n\nmed-expert average\n\naverage\n\n75.0 56.9 42.5\n\n58.1\n\n62.5 75.9 40.6\n\n59.7\n\n109.0 110.9 92.9 ̆\n\n104.3\n\n74.0\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nG BIAS-VARIANCE TRADEOFF ANALYSIS\n\nWe formalize our discussion on the bias-variance tradeoff when learning from a suboptimal distribution mentioned in Section 4.1. The objective functions for training DT (2) and RvS (4) can be rewritten as:\n\nLpD pθq “ Eτ „T rDpτ, πθqs\n\nmin θ\n\n“ Er„pDprq,τ „Tr rDpτ, πθqs .\n\n(8)\n\n(9)\n\nIn which, pDprq is the data distribution over trajectory returns, Tr is a uniform distribution over the set of trajectories whose return is r, and Dpτ, πθq is the supervised loss function with respect to the |τ | sampled trajectory τ . For DT, Dpτ, πθq “ 1 , and for t“1 |τ | ̆\nř 2\nRvS, Dpτ, πθq “ 1 . Equation (9) is equivalent to first sampling a return r, then sampling a trajectory τ whose return is r, and calculating the loss on τ . Ideally, we want to train the model from an optimal return distribution p‹prq, which is centered around the expert return r‹:\n\nat ́ πθpgt ́K:t, st ́K:t, at ́K:t ́1q\n\nat ́ πθpst, ωtq\n\n|τ | t“1\n\nř\n\n|τ |\n\n`\n\n`\n\n ̆\n\n2\n\nmin θ\n\nLp‹ pθq “ Er„p‹prq,τ „Tr rDpτ, πθqs .\n\n(10)\n\nIn practice, we only have access to the suboptimal return distribution pDprq, which leads to a biased training objective with respect to p‹prq. While the dataset is fixed, we can transform the data distribution pDprq to qprq that better estimates the ideal distribution p‹prq. The objective function with respect to q is:\n\nmin θ\n\nLqpθq “ Er„qprq,τ „Tr rDpτ, πθqs\n\n„\n\n“ Er„pDprq,τ „Tr\n\nqprq pDprq\n\n ̈ Dpτ, πθq\n\nȷ\n\n(11)\n\n(12)\n\nIn the extreme case, qprq “ 1rr “ r‹s, which means we only train the policy on trajectories whose return matches the expert return r‹. However, since offline datasets often contain very few expert trajectories, this q leads to a very high-variance training objective. An optimal distribution q should lead to a training objective that balances the bias-variance tradeoff. We quantify this by measuring the l2 of the difference between the gradient of Lqpθq and the gradient of the optimal objective function Lp‹ pθq. Analogous to Kumar & Levine (2020), we can prove that for some constants C1, C2, C3, with high confidence:\n\n“ ||∇θLqpθq ́ ∇θLp‹pθq||2\n\n2\n\n‰\n\nE\n\nď C1 ̈ Er„qprq\n\n„\n\nȷ\n\n1 Nr\n\n` C2 ̈\n\nd2pq||pDq |D|\n\n` C3 ̈ DTVpp‹, qq2. (13)\n\nIn which, Nr is the number of trajectories in dataset D whose return is r, d2 is the exponentiated Renyi divergence, and DTV is the total variation divergence. The right hand side of inequality (13) shows that an optimal distribution q should be close to the data distribution pD to reduce variance, while approximating well p‹ to reduce bias. As shown in Kumar & Levine (2020), qprq9 Nr\n\nq minimizes this bound, which inspires our trajectory weighting.\n\nNr`K ̈ expp ́ |r ́r‹|\n\nκ\n\n21",
  "translations": [
    "# Summary Of The Paper\n\nThe paper identifies the skewed distribution of returns in the offline dataset as a challenge to return-conditioned methods, and proposes to weight the trajectories in the dataset based on their returns.\n\n# Strength And Weaknesses\n\nStrengths\n\n1. The paper is relatively well-written and easy to follow.\n\n2. The proposed idea is intuitive, whose benefits are demonstrated on two different classes of return-conditioned methods (DT and RvS).\n\nWeaknesses\n\n1. The paper makes repeated reference to a \"bias-variance trade-off\" to motivate their methods. AFAIK, what this trade-off means exactly, and a theoretical or rigorous analyses as to why their method leads to a more favorable trade-off are missing completely.\n\n2. Skewed distribution of returns is a well-known problem in RL, and common technique to tackle this problem is to ensure each batch consists of equal amount of high return and low return episodes (so equivalently, use two bins only). I would have liked to see a comparison to such a simple baseline.\n\n3. My biggest concern is the motivation behind studying return-condition method? Because AFAIK, they under-performs value-based methods, especially on tasks that require stitching (such as antmaze navigation and notshown in the paper). Even in table 2, the performance of return-conditioned methods are under-whelming. I am not saying that the paper should not be accepted because it does not out-perform state-of-the-art, but there should be a reason as to why the class of return-conditioned method is interesting relative to value-based methods.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is very well written and easy to follow. \n\nThe proposed method is novel in the context of return-conditioned method, but I would have like to see a comparison to the simple baseline commonly used in value-based methods (mentioned previously).\n\nThe pseudo-code is included so I believe the algorithm is reproducible.\n\n# Summary Of The Review\n\nI recommend a weak reject, because it is unclear why we should be interested in return-conditioned methods, especially when they face their own sets of unique challenges, and when enhanced with proposals such as the ones in this paper, still under-perform value-based methods. What are their unique selling points?\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents ConserWeightive Behavioral Cloning (CWBC), a novel approach aimed at improving offline reinforcement learning (RL) by effectively learning near-optimal policies from static logged datasets. The methodology integrates two key components: trajectory weighting, which enhances the estimation of expert return distributions by leveraging both low-return and high-return trajectories, and conservative regularization, which stabilizes policy performance by penalizing deviations from the training data distribution. The findings demonstrate that CWBC significantly outperforms existing methods, such as RvS and DT, on various benchmarks, showing substantial improvements in performance without necessitating manual tuning of conditioning returns.\n\n# Strength And Weaknesses\nThe strengths of the paper include its clear articulation of the challenges in offline RL and the innovative combination of trajectory weighting and conservative regularization, which addresses key issues related to bias-variance trade-offs and stability in policy performance. The empirical results are robust, showcasing the effectiveness of CWBC across multiple datasets and tasks. However, the paper could be strengthened by providing a more detailed discussion of the limitations of the proposed method and potential scenarios where it might underperform. Additionally, a comparison of CWBC with a wider array of baseline methods could enhance the evaluation of its significance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The writing is accessible, making it easy for readers to follow the technical details. The novelty of the proposed method is evident in its integration of established concepts in a new way to tackle specific challenges in offline RL. Reproducibility is supported by detailed descriptions of the experimental setup and results; however, additional information on hyperparameter tuning and implementation specifics would be beneficial for practitioners looking to replicate the results.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in offline reinforcement learning through the CWBC framework, which effectively addresses critical challenges in the field. The combination of trajectory weighting and conservative regularization offers a promising direction for future research, although further exploration of its limitations and broader comparisons could provide a more comprehensive understanding of its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces ConserWeightive Behavioral Cloning (CWBC), a novel approach aimed at enhancing offline reinforcement learning (RL) by addressing the limitations of behavioral cloning (BC) when working with suboptimal datasets. The main contributions include trajectory weighting, which balances the bias-variance tradeoff by downweighting low-return trajectories for improved data efficiency, and conservative regularization, which ensures that the policy remains close to the data distribution to mitigate performance drops when confronted with out-of-distribution returns. The methodology is validated through empirical evaluations on D4RL locomotion benchmarks, where CWBC demonstrates significant performance enhancements over baseline methods, achieving an average improvement of 18% across diverse datasets.\n\n# Strength And Weaknesses\nThe strengths of CWBC lie in its innovative approach and empirical validation, showcasing robust performance improvements and stability across various benchmarks. The methodology is straightforward, which enhances its accessibility for future research in offline RL. However, the paper also presents limitations, including challenges with extrapolation beyond the offline dataset and sensitivity to hyperparameter tuning. Additionally, while the method excels in low-quality datasets, its efficacy may diminish in scenarios where high-quality trajectories are present, indicating a potential area for further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The quality of the empirical results is high, demonstrating rigorous evaluation against established benchmarks. The novelty of CWBC is notable, as it presents a fresh perspective on improving behavioral cloning in offline RL. The reproducibility of the results is facilitated by the clear description of methodologies and hyperparameters, although the sensitivity to hyperparameter choices may require careful consideration by future researchers.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in offline reinforcement learning through the introduction of CWBC, which effectively enhances behavioral cloning methods. While the approach shows significant promise, particularly in low-quality datasets, challenges regarding extrapolation and hyperparameter sensitivity remain pertinent issues for further investigation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents **ConserWeightive Behavioral Cloning (CWBC)**, a novel approach to offline reinforcement learning (RL) that addresses challenges associated with skewed and suboptimal return distributions in logged datasets. CWBC utilizes trajectory weighting to mitigate the bias-variance tradeoff and incorporates conservative regularization to ensure policies remain close to the data distribution, thus improving reliability when conditioning on out-of-distribution returns. The methodology is empirically validated on various tasks (D4RL locomotion and Atari games), demonstrating significant performance enhancements over existing methods like Reinforcement Learning via Supervised Learning (RvS) and Decision Transformer (DT).\n\n# Strength And Weaknesses\nStrengths of the paper include the clear identification of challenges in offline RL and the introduction of a well-structured solution through CWBC, which combines innovative components such as trajectory weighting and conservative regularization. The empirical results are robust, showing consistent improvements across tasks, indicating that the proposed method effectively addresses the identified issues. However, the paper could benefit from a deeper exploration of the theoretical underpinnings of the proposed methods and additional comparisons with a broader range of baselines. Moreover, while the empirical results are promising, the generalizability of the findings across diverse environments needs to be cautiously interpreted.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly written, making complex concepts accessible. The methodology is detailed enough for reproducibility, with explicit equations and algorithm descriptions provided. The novelty of the approach lies in its unique combination of trajectory weighting and conservative regularization, which has not been previously explored to this extent in the offline RL context. However, the clarity could be enhanced by providing more intuitive explanations of the proposed components and their interactions.\n\n# Summary Of The Review\nOverall, the paper successfully addresses critical issues in offline reinforcement learning through the CWBC approach, demonstrating significant empirical improvements. While the contributions are impactful, additional theoretical insights and comparisons would strengthen the paper's claims.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents ConserWeightive Behavioral Cloning (CWBC), a novel method designed to enhance offline reinforcement learning (RL) by combining trajectory weighting and conservative regularization techniques. The authors conduct extensive empirical evaluations across various benchmarks, including D4RL locomotion tasks and Atari games, demonstrating that CWBC significantly improves performance and stability over existing approaches. Key findings highlight that CWBC effectively addresses the bias-variance tradeoff, though it still faces challenges related to learning from suboptimal data and hyperparameter sensitivity.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach through CWBC, which integrates trajectory weighting with conservative regularization, and its clear performance improvements across multiple RL benchmarks. The method's ability to balance the bias-variance tradeoff is particularly noteworthy, as it enhances learning from imperfect datasets. However, the novelty of CWBC necessitates further validation against a broader range of existing methods to establish its robustness. Additionally, the reliance on specific hyperparameters may limit the generalizability of results, and the conservative nature of the policy learning could restrict exploration, potentially inhibiting policy optimization in certain scenarios.\n\n**Areas for Improvement:**\n1. Broader benchmarking in diverse, complex environments to validate robustness.\n2. A systematic exploration of hyperparameter sensitivity and its influence on performance.\n3. Incorporation of exploration strategies within the conservative framework for improved policy discovery.\n4. Investigations into long-term generalization capabilities in sustained performance tasks.\n5. Comprehensive comparative analysis with a wider array of existing methods to clarify CWBC's advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas with clarity, offering comprehensive implementation details, including pseudocode and hyperparameters, which significantly enhance reproducibility. The novelty of combining trajectory weighting with conservative regularization offers a fresh perspective in the field of offline RL. However, the evaluation of CWBC against a limited set of benchmarks may hinder the perceived novelty and significance of the contributions. Future work could address these limitations to improve the overall quality and applicability of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to offline reinforcement learning through the introduction of CWBC, which significantly improves performance and stability. While the contributions are substantial, further empirical validation and exploration of hyperparameter sensitivity are necessary to fully establish the method's robustness and generalizability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach called **ConserWeightive Behavioral Cloning (CWBC)**, designed to enhance the efficiency and reliability of offline reinforcement learning (RL). The proposed methodology consists of two main components: **trajectory balancing**, which adjusts the influence of low-return trajectories to improve data utilization, and **conservative action projection**, which mitigates the risks associated with out-of-distribution returns. Empirical results demonstrate that CWBC outperforms traditional behavioral cloning methods, achieving an 18% improvement in performance across various benchmarks and showing increased stability in challenging environments.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to tackling the bias-variance tradeoff through trajectory balancing and its emphasis on conservative action projection, which enhances the reliability of the learned policy. The empirical validation across multiple tasks provides strong evidence of the method's effectiveness. However, a notable weakness is the lack of detailed implementation specifics, which may hinder reproducibility. Additionally, while the paper references related works, a deeper exploration of how CWBC fits into the existing literature could strengthen its position.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly written, making complex concepts accessible. The quality of the research is high, with rigorous empirical validation supporting the claims made. The novelty of the approach is significant, as it combines elements from behavioral cloning and conservative value-based methods in a unique way. However, the reproducibility of the results could be improved with more comprehensive implementation details and clearer guidelines for replicating the experiments.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of offline reinforcement learning through the introduction of the CWBC framework, offering both theoretical and empirical advancements. While the methodology is innovative and the results promising, improvements in the clarity of implementation details could enhance reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces ConserWeightive Behavioral Cloning (CWBC), a novel approach designed to enhance adversarial training in machine learning models. The authors present two primary innovations: trajectory weighting, which prioritizes adversarial examples based on their impact on model performance, and conservative regularization, which stabilizes the model by encouraging it to remain close to the distribution of benign inputs. Empirical evaluations on benchmark datasets demonstrate that CWBC significantly improves the robustness and performance of existing adversarial training techniques.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to addressing the bias-variance tradeoff in adversarial training and the introduction of practical techniques that show promise in improving model robustness. The empirical validation across multiple datasets provides strong evidence for the effectiveness of the proposed methods. However, the paper could benefit from a deeper theoretical exploration of the mechanisms behind trajectory weighting and conservative regularization. Additionally, more real-world application examples could reinforce the practical relevance of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The quality of the writing is high, with appropriate technical detail provided to support the claims made. The novelty of the approach is significant, as it combines two innovative concepts to tackle challenges in adversarial training. While the results are reproducible based on the provided methodologies, additional details on implementation and hyperparameter selection could enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in adversarial training through the introduction of CWBC. Its innovative techniques and strong empirical results contribute valuable insights into improving model robustness. However, addressing theoretical foundations and real-world applications could further strengthen the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces ConserWeightive Behavioral Cloning (CWBC), a novel approach to offline reinforcement learning that claims to address key limitations of traditional behavioral cloning methods. The authors propose a trajectory weighting mechanism and a conservative regularization technique that they assert completely eliminate the bias-variance tradeoff, allowing for optimal performance without expert tuning or prior knowledge of dataset distributions. Empirical results reportedly demonstrate significant performance improvements over state-of-the-art methods, suggesting CWBC's potential to set new standards in offline RL research and practice.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambitious claims regarding the trajectory weighting and conservative regularization, which, if validated, could significantly enhance the utility of offline RL methods. The reported empirical results, showing improvements over existing methods, further bolster the paper's contributions. However, the paper's claims may be overly optimistic, as it lacks rigorous analysis and validation of the proposed methods across diverse tasks and settings. The sweeping assertions about the obsolescence of prior methodologies and the ease of implementation may also undermine the credibility of the work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written in an engaging manner, clearly articulating its contributions and findings. However, certain claims appear exaggerated, which could lead to skepticism within the research community. Although the methodology is presented as straightforward, the lack of comprehensive experimental validation raises concerns about reproducibility. The novelty of the theoretical insights on bias-variance tradeoffs is notable, though it would benefit from a more thorough discussion of how these insights relate to existing literature.\n\n# Summary Of The Review\nOverall, while the paper presents a potentially impactful approach to offline reinforcement learning, its claims may be overstated, and the empirical results require further validation. The ease of implementation and the proposed theoretical framework are promising, but the paper lacks sufficient rigor to fully substantiate its assertions.\n\n# Correctness\n3/5 - The proposed methods have potential but require additional empirical validation to confirm their correctness.\n\n# Technical Novelty And Significance\n4/5 - The introduction of trajectory weighting and conservative regularization is technically novel and could significantly influence future research in offline RL.\n\n# Empirical Novelty And Significance\n3/5 - While the reported performance improvements are noteworthy, the empirical results need further substantiation across a wider range of tasks to establish their significance.",
    "# Summary Of The Paper\nThe paper presents ConserWeightive Behavioral Cloning (CWBC), an innovative approach aimed at enhancing conditional Behavioral Cloning (BC) in offline reinforcement learning (RL). The methodology integrates trajectory weighting and conservative regularization to effectively address the bias-variance tradeoff, particularly when dealing with limited expert trajectories. The findings demonstrate that CWBC achieves significant performance improvements of 12% in RvS and 6% in DT across various offline RL benchmarks, enhancing both performance and stability while minimizing the need for extensive value tuning during evaluation.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its novel approach to managing the bias-variance tradeoff in offline RL, which is a critical issue when expert data is limited. The introduction of trajectory weighting and conservative regularization effectively enhances the reliability of BC, particularly in out-of-distribution settings. However, one weakness is that while the empirical results are convincing, the paper could provide further insight into the potential limitations or scenarios where CWBC might underperform compared to other state-of-the-art methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The empirical results are presented with sufficient detail, allowing for a clear understanding of the improvements achieved by CWBC. However, the reproducibility of the results could be strengthened by providing more details on the experimental setup and hyperparameter tuning. The novelty of the approach is significant, as it combines established techniques in a new way to tackle a persistent problem in offline RL.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in offline reinforcement learning through the CWBC framework, demonstrating notable improvements in performance and reliability. While the empirical results are promising, further exploration of the method’s limitations and broader applicability would enhance the contribution.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel approach to offline reinforcement learning that emphasizes the importance of conservative policy learning from expert returns. The methodology relies on trajectory weighting to balance the bias-variance tradeoff and employs a regularization technique to maintain proximity to the behavior policy. The findings indicate that the proposed method improves performance on specific benchmarks, notably D4RL and Atari, although the paper raises concerns about the generalizability of these results to broader scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to conservative policy learning, which could potentially enhance stability in offline reinforcement learning settings. However, several weaknesses are evident. The reliance on expert returns may limit the applicability of the method in real-world scenarios where true expert policies are unknown. Furthermore, the assumption that simply downweighting low-return trajectories improves performance overlooks the complexities of the bias-variance tradeoff, risking overfitting to noise. Additionally, the focus on specific benchmarks may not adequately represent the range of challenges present in offline RL, raising questions about the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, although some sections could benefit from more detailed explanations regarding the assumptions made. The quality of the methodology is sound but relies heavily on specific neural network architectures, which could hinder reproducibility across different settings. The novelty is present in the conservative regularization approach, but the degree to which it represents a significant advancement in the field is tempered by the concerns about generalization and empirical evaluation.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to conservative policy learning in offline reinforcement learning but is hampered by several limitations regarding its assumptions and empirical validation. The reliance on expert returns and specific benchmarks raises questions about the method's generalizability and practical applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces ConserWeightive Behavioral Cloning (CWBC), a novel approach to enhance offline reinforcement learning (RL) by mitigating biases in logged datasets through trajectory weighting and conservative regularization. The authors argue that traditional behavioral cloning struggles with suboptimal trajectories and skewed return distributions, which CWBC aims to address. The methodology involves upweighting high-return trajectories to improve data efficiency while incorporating a conservative regularization strategy. Empirical evaluations on D4RL locomotion tasks demonstrate significant performance improvements over baseline methods, validating the effectiveness of the proposed approach.\n\n# Strength And Weaknesses\nThe primary strength of the paper is its innovative approach to tackling bias in offline RL, which is a critical challenge in the field. The introduction of trajectory weighting combined with conservative regularization offers a promising solution to improve the stability and performance of policies derived from historical datasets. However, a potential weakness lies in the generalizability of the findings beyond the specific tasks evaluated. The paper could benefit from additional experiments on a wider range of datasets and environments to fully establish the robustness of CWBC across different scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and methodologies. The quality of writing is high, making it accessible to both experts and those less familiar with the nuances of offline RL. The novelty of the approach is significant, particularly in the context of conservative regularization within behavioral cloning. However, reproducibility may be a concern, as the paper provides limited details on the implementation specifics and hyperparameter choices that could influence the results. Including more extensive implementation guidelines and code availability would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful contribution to the offline reinforcement learning domain by introducing CWBC, which effectively addresses biases in logged datasets. While the methodology shows substantial promise and yields impressive empirical results, the generalizability of the approach and reproducibility could be further improved.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to addressing the challenge of unsupervised domain adaptation (UDA) in the context of image classification. The authors propose a new framework that integrates a self-supervised learning mechanism with a domain-invariant feature extractor, enabling the model to effectively learn from unlabelled data in the target domain. Experimental results demonstrate that the proposed method significantly outperforms existing UDA techniques on several benchmark datasets, achieving higher accuracy and robustness against domain shifts.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Novelty**: The integration of self-supervised learning with domain-invariant feature extraction presents a fresh and innovative approach to UDA, contributing new insights into the field.\n2. **Empirical Validation**: The paper includes thorough experiments across multiple datasets, providing strong evidence of the method's effectiveness and robustness in real-world scenarios.\n3. **Clarity**: The writing is clear and well-organized, making it easy for readers to follow the rationale behind the proposed method and its implementation.\n\n**Weaknesses:**\n1. **Baseline Comparisons**: While the results are promising, the paper lacks comparisons with a wider variety of state-of-the-art methods, which could enhance the robustness of the findings.\n2. **Limited Discussion on Generalizability**: There is insufficient exploration of how the proposed method may perform across different types of datasets or in scenarios with varying levels of domain shift.\n3. **Reproducibility Concerns**: The experimental setup lacks detailed descriptions of hyperparameter settings and data preprocessing steps, which could hinder reproducibility by other researchers.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, providing a coherent narrative that effectively communicates the contributions. The novelty of combining self-supervised learning with a domain-invariant architecture stands out, offering a significant advancement in UDA. However, the lack of detailed experimental setup information raises concerns about reproducibility, which is a critical aspect for validating the results presented.\n\n# Summary Of The Review\nOverall, the paper makes a noteworthy contribution to the field of unsupervised domain adaptation by proposing a novel framework that demonstrates strong empirical performance. While the clarity and novelty of the work are commendable, addressing the weaknesses related to baseline comparisons and reproducibility would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces ConserWeightive Behavioral Cloning (CWBC), a novel framework designed to improve the performance of behavioral cloning (BC) in offline reinforcement learning (RL) scenarios. It addresses challenges associated with skewed return distributions in offline datasets by incorporating two key components: trajectory weighting, which balances the bias-variance tradeoff by leveraging both low and high return trajectories, and conservative regularization, which ensures that the learned policies remain close to the original data distribution. The proposed methodology is validated through experiments on two existing BC methods (RvS and Decision Transformer), demonstrating significant improvements in both performance and stability across various offline RL benchmarks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to mitigating the challenges of offline RL through CWBC, specifically by enhancing the reliability of behavioral cloning methods. The introduction of trajectory weighting and conservative regularization provides a structured way to tackle issues related to bias and variance in the context of offline datasets. However, a notable weakness is that while CWBC shows improvements over baseline BC methods, it still falls short compared to value-based methods like CQL and IQL, indicating that there is still considerable room for improvement. Additionally, the paper could benefit from a more extensive discussion on the limitations of the proposed approach and potential pitfalls in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, making it accessible to readers familiar with reinforcement learning concepts. The methodology is described in sufficient detail, allowing for reproducibility of the experiments conducted. The novelty of the CWBC framework is significant, introducing new techniques to enhance the reliability of BC in offline settings. However, the paper could have benefited from more extensive empirical comparisons with a broader range of methods to better contextualize the contributions made by CWBC.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the offline reinforcement learning literature through the introduction of the ConserWeightive Behavioral Cloning framework, which effectively addresses critical challenges in learning from static datasets. While the results are promising, the performance of CWBC still lags behind that of existing value-based methods, indicating that further research is needed to enhance its applicability and effectiveness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to offline reinforcement learning (RL) through a method termed Conservative Behavioral Cloning (CWBC). CWBC addresses the challenges of conditioning behavioral cloning on expert returns from skewed datasets by incorporating trajectory weighting and conservative regularization. The methodology demonstrates significant improvements in performance and stability across various offline RL benchmarks, including D4RL locomotion tasks and Atari games. Empirical results indicate that CWBC effectively balances the learning from low and high-return trajectories while maintaining reliability in generalizing to out-of-distribution returns.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to offline RL, which combines trajectory weighting and conservative regularization to mitigate issues associated with skewed datasets. The empirical results robustly support the effectiveness of CWBC, showcasing improved performance over baseline methods. However, a potential weakness is the limited exploration of the underlying assumptions made in trajectory weighting and regularization, which may affect the method's applicability in diverse environments. Additionally, while the paper references prior work, a more detailed comparison with other state-of-the-art methods could enhance the discussion on CWBC's advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The quality of writing is high, making complex concepts accessible. The technical novelty of CWBC is significant, as it builds upon existing behavioral cloning techniques while addressing critical challenges in offline RL. The authors provide sufficient implementation details and hyperparameters, ensuring reproducibility of results, a commendable practice in the field.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to offline reinforcement learning by proposing a method that combines trajectory weighting and conservative regularization. The empirical results validate the approach, demonstrating both performance and stability improvements. However, further exploration of the method's assumptions and a more comprehensive comparison with existing techniques could strengthen the paper.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"ConserWeightive Behavioral Cloning for Reliable Offline Reinforcement Learning\" addresses the challenges of offline reinforcement learning (RL) by proposing a novel method called Conservative Weighting Behavioral Cloning (CWBC). The methodology combines two key components: trajectory weighting and conservative regularization, aiming to improve the reliability and performance of offline RL algorithms. Through comprehensive experiments on D4RL locomotion tasks, the authors demonstrate that CWBC outperforms existing state-of-the-art methods, showcasing significant improvements in stability and effectiveness in offline settings.\n\n# Strength And Weaknesses\nThe strengths of the paper include its clear identification of the problem in offline RL and the introduction of a well-defined and innovative methodology that integrates trajectory weighting and conservative regularization. The experimental results are robust, demonstrating the advantages of the proposed method with quantitative evidence. However, a notable weakness is the limited scope of evaluated tasks, which may restrict the generalizability of the findings. The discussion could also benefit from a deeper exploration of the implications of the proposed method in various practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents information clearly, with appropriate use of figures and tables that enhance comprehension. The mathematical formulations are precise and consistent, promoting understanding of the proposed methodology. The novelty of CWBC is significant, given its dual approach to addressing the limitations of offline RL. Regarding reproducibility, the authors provide sufficient details on datasets and algorithms, along with indications about the availability of code, which supports the claims made in the paper.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of offline reinforcement learning through its innovative approach and strong empirical results. While the methodology is promising, the scope of evaluation should be broadened to enhance its applicability across various tasks.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces ConserWeightive Behavioral Cloning (CWBC), a novel framework for offline reinforcement learning that aims to enhance the performance and stability of behavioral cloning (BC) methods when conditioned on expert returns. CWBC addresses the bias-variance tradeoff by integrating trajectory weighting and conservative regularization, ensuring that policy predictions remain close to the distribution of the training data while effectively handling out-of-distribution returns. The empirical evaluations demonstrate significant improvements in performance across various locomotion tasks, achieving an 18% enhancement for RvS and 8% for DT in medium-replay tasks from the D4RL suite.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its clear articulation of the challenges in offline RL, particularly the bias-variance tradeoff, and the innovative approach of combining trajectory weighting with conservative regularization. This dual strategy is shown to improve the robustness of BC methodologies in offline contexts. However, the paper could be strengthened by providing more extensive comparisons with existing methods beyond the immediate benchmarks used, as well as a more thorough discussion of the limitations and potential pitfalls of the proposed method in diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its contributions clearly, with a logical flow from the introduction of the problem to the proposed method and empirical results. The methodology is described in sufficient detail to allow for reproducibility, with defined mathematical formulations and explanations of hyperparameters. In terms of novelty, CWBC presents a fresh approach to a recognized challenge in offline RL, although the concept of combining weighting and regularization is not entirely new in the broader context of supervised learning.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of offline reinforcement learning through the introduction of CWBC, which effectively tackles the bias-variance tradeoff and enhances the performance of behavioral cloning methods. While the results are promising, further exploration of the method's limits and broader comparisons with other state-of-the-art techniques could provide additional insights.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a method called ConserWeightive Behavioral Cloning (CWBC), which aims to enhance performance in behavioral cloning and offline reinforcement learning through trajectory weighting and conservative regularization. The authors claim to achieve performance improvements of 18% and 8% on certain benchmarks. However, the proposed methodology appears to lack substantial novelty, primarily revisiting existing concepts without introducing significant new insights. The findings are presented with an emphasis on marginal improvements, raising questions about the effectiveness and complexity of the proposed approach.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to address performance issues in offline reinforcement learning and behavioral cloning. However, the weaknesses are pronounced: the method lacks clear novelty and depth, relying on convoluted mechanisms that do not effectively address the core challenges of the field. The performance improvements reported are marginal, and the empirical results lack sufficient statistical rigor. Additionally, the paper fails to discuss the limitations of the proposed method adequately, particularly concerning generalization to unseen scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by an overemphasis on technical details, which may confuse readers who are not specialists. The quality of the empirical results is questionable due to the lack of rigorous statistical analysis and a superficial treatment of biases in the training data. The novelty of the contributions is limited, as the paper primarily reiterates established concepts without offering substantial improvements. Reproducibility is also a concern, given the ambiguous presentation of results and insufficient discussion of potential pitfalls.\n\n# Summary Of The Review\nOverall, the paper presents a method that lacks significant novelty and introduces unnecessary complexity without adequately addressing key challenges in offline reinforcement learning. The performance claims are not sufficiently supported by rigorous empirical evidence, and the paper does not effectively acknowledge the limitations of the proposed approach.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents ConserWeightive Behavioral Cloning (CWBC), a novel approach in the domain of offline reinforcement learning (RL) that enhances behavioral cloning (BC) through the integration of trajectory weighting and conservative regularization. The methodology involves leveraging supervised learning techniques, which simplifies the training process and improves data efficiency. The findings indicate that CWBC significantly boosts performance, achieving an 18% improvement in benchmark tasks compared to existing methods, while demonstrating robust generalization capabilities across various environments.\n\n# Strength And Weaknesses\nCWBC's strengths lie in its innovative approach to combining trajectory weighting and conservative regularization, which allows agents to learn effectively from diverse datasets without discarding valuable information. The empirical results are compelling, showcasing CWBC's superiority over state-of-the-art methods in offline RL. However, the paper may benefit from a more comprehensive explanation of the theoretical underpinnings of the trajectory weighting mechanism and its implications for learning dynamics. Additionally, while the empirical evaluations are extensive, further exploration of the limitations in specific environments would strengthen the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the methodology and findings. The authors provide sufficient detail for reproducibility, including implementation insights that enhance accessibility for the research community. The novelty of the approach is evident, and the simplicity of CWBC compared to traditional value-based methods is a significant advantage. However, some sections could be elaborated upon to enhance clarity, particularly regarding the theoretical motivations behind certain design choices.\n\n# Summary Of The Review\nOverall, CWBC introduces a significant advancement in offline reinforcement learning, demonstrating exceptional performance improvements and robust generalization. Its straightforward implementation and empirical success position it as a valuable tool for various applications, while also laying the groundwork for future research in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework called Conservative Behavioral Cloning (CWBC) aimed at addressing the challenges inherent in offline reinforcement learning (RL). It builds on the theoretical foundations of Behavioral Cloning (BC) while incorporating trajectory weighting and conservative regularization. The proposed methodology seeks to balance the bias-variance tradeoff by emphasizing high-return trajectories, thus improving policy learning from static datasets. The findings suggest that CWBC enhances the reliability of learned policies and mitigates the risks of overfitting to high, unseen return values.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its theoretical contributions, particularly the in-depth analysis of the bias-variance tradeoff in offline RL and the introduction of conservative regularization. The approach of trajectory weighting adds a practical dimension to BC, enhancing learning efficiency without discarding less optimal data. However, the paper could benefit from empirical validation to substantiate its theoretical claims, as the current focus is predominantly on theoretical insights without extensive experimental results. Additionally, the discussion on future research directions is somewhat vague and could be more explicit.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, providing a coherent explanation of the theoretical concepts. The quality of writing is high, with complex ideas presented in an accessible manner. The novelty of the CWBC framework is significant, as it provides a new perspective on addressing challenges in offline RL. However, the reproducibility of the proposed methods remains uncertain due to the lack of detailed experimental setups or validation results, which would be essential for future researchers aiming to build upon this work.\n\n# Summary Of The Review\nOverall, the paper makes a substantial theoretical contribution to the field of offline reinforcement learning through the introduction of the CWBC framework, which addresses key challenges related to bias and variance. While the theoretical insights are valuable, the lack of empirical validation limits the ability to fully assess the practical impact of the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"ConserWeightive Behavioral Cloning for Reliable Offline Reinforcement Learning\" presents a novel approach to address challenges in offline reinforcement learning (RL) through a method called Conservative Weighting Behavioral Cloning (CWBC). The key contributions include the introduction of trajectory weighting to mitigate the bias-variance tradeoff and conservative regularization to encourage policies that remain close to the data distribution. The methodology is instantiated in the context of two algorithms: one for weighted trajectory sampling and the other specifically tailored for reinforcement learning benchmarks such as D4RL and Atari. Empirical results demonstrate that the proposed CWBC method yields performance improvements across various settings, with ablation studies highlighting the importance of hyperparameters on the overall effectiveness of the approach.\n\n# Strength And Weaknesses\nStrengths of the paper include the clear articulation of the CWBC methodology and its empirical validation on well-established benchmarks, which enhances its applicability in the offline RL domain. The detailed implementation steps, including the algorithms and hyperparameter settings, contribute to the reproducibility of the work. However, the paper lacks a thorough discussion of the broader implications of the proposed methods and does not explore potential limitations or alternatives in depth. This could leave the reader wanting more context regarding the significance of the findings relative to existing approaches.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and methodologies, making it easy to follow. The quality of the writing is high, with sufficient detail provided for the algorithms and their respective implementations. The novelty of the approach lies in the combination of trajectory weighting and conservative regularization, although the foundational ideas are drawn from established concepts in behavioral cloning and reinforcement learning. The reproducibility is adequately supported by the availability of a codebase based on official implementations, although additional details regarding the datasets and experimental setup could enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper presents a solid contribution to the field of offline reinforcement learning through the innovative CWBC approach, demonstrating empirical improvements over traditional methods. While the methodology and results are compelling, a deeper exploration of the implications and limitations would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces ConserWeightive Behavioral Cloning (CWBC), a framework designed to enhance conditional behavioral cloning for offline reinforcement learning (RL). The authors claim that CWBC improves performance metrics compared to existing methods, specifically addressing the bias-variance tradeoff in offline datasets. Key components of the methodology include trajectory weighting and a conservative regularization approach, which the authors argue contribute to the reliability of performance in various benchmarks.\n\n# Strength And Weaknesses\nWhile CWBC demonstrates improved empirical results over prior methods such as RvS and DT, the contributions are overshadowed by similarities to existing works. Several aspects, including the handling of the bias-variance tradeoff and the trajectory weighting technique, have been previously explored in the literature. Additionally, the authors do not adequately contextualize their findings relative to other prominent models like CQL and IQL, which could mislead the reader regarding CWBC's actual performance advantages. The paper also lacks a critical evaluation of hyperparameter tuning methods, which could provide a more balanced view of its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, but its quality is tempered by the insufficient contextualization of findings within the existing literature. While the methodology is reproducible, the novelty of the contributions is questionable, as many proposed techniques appear to echo prior work. The empirical results are presented convincingly, but the lack of comprehensive comparisons to baseline models detracts from the overall clarity regarding the significance of the findings.\n\n# Summary Of The Review\nThe CWBC framework presents promising results in specific benchmarks; however, the paper falls short in establishing its novelty and significance relative to existing literature. A more thorough comparison with prior works would strengthen the authors' claims and provide clearer insights into the contributions of CWBC to offline RL.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"CONSERVATIVE BEHAVIORAL CLONING FOR RELIABLE OFFLINE REINFORCEMENT LEARNING\" introduces a novel approach to offline reinforcement learning (RL) by employing conservative behavioral cloning (BC) techniques. The authors propose a methodology that conditions BC on expert returns during testing, aiming to enhance the reliability and performance of learned policies from historical datasets. The findings suggest that their approach outperforms traditional methods, achieving competitive results while maintaining training stability.\n\n# Strength And Weaknesses\nThe paper contributes significantly to the field of offline reinforcement learning by addressing the challenges of policy learning from historical datasets. The methodology is well-defined, and experimental results demonstrate the efficacy of the proposed approach. However, the paper suffers from several issues related to clarity and presentation, such as inconsistent notation and formatting. These weaknesses may hinder the reader's understanding and engagement with the work.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the core methodology is innovative, the clarity of the paper could be improved with better organization and consistent terminology. Issues such as unclear figure references, inconsistent notation, and typographical errors detract from the overall quality. The novel aspects of the approach are clear, but the paper would benefit from a more rigorous presentation to enhance reproducibility. Additionally, the empirical results, while promising, could be more robust with additional experiments or comparisons to other state-of-the-art methods.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to offline reinforcement learning, showcasing a novel approach with promising results. However, the clarity and presentation issues need to be addressed to improve reader comprehension and engagement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach called Conservative Weighted Behavior Cloning (CWBC) aimed at improving offline reinforcement learning (RL). The authors propose a methodology that integrates trajectory weighting and conservative regularization techniques to enhance learning from historical data while mitigating the risks of overfitting and distributional shift. Empirical results demonstrate that CWBC outperforms existing methods on benchmark datasets, including D4RL and Atari games, highlighting its effectiveness in leveraging offline data for policy learning.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the challenges associated with offline RL, particularly through the incorporation of conservative regularization techniques. The empirical results are robust and demonstrate significant improvements over baseline methods. However, several weaknesses were identified, including a limited exploration of the implications of combining CWBC with other offline RL methods, insufficient analysis of hyperparameter sensitivity, and a lack of discussion on the performance of the method in environments with sparse rewards or imbalanced datasets. Additionally, the potential for CWBC to adapt to non-stationary environments remains unexplored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. However, the clarity could be enhanced by including a more comprehensive discussion on the limitations of the proposed techniques and their applicability in diverse scenarios. The novelty of the approach is commendable, yet the reproducibility of the findings could be improved by providing additional empirical validations on a broader range of datasets and conditions. The authors could also benefit from a deeper theoretical analysis of the method's trade-offs.\n\n# Summary Of The Review\nOverall, the paper introduces a promising method for offline RL that demonstrates notable improvements over existing approaches. While the contributions are significant, there are several avenues for future work that could enhance the robustness and applicability of CWBC in real-world scenarios.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach called ConserWeightive Behavioral Cloning (CWBC) designed to enhance offline reinforcement learning (RL) through the implementation of trajectory weighting and conservative regularization. The authors address the inherent bias-variance tradeoff in offline RL, proposing a framework that optimally adjusts the distribution of returns to improve learning outcomes. Empirical evaluations across D4RL locomotion tasks demonstrate that CWBC consistently outperforms baseline methods, achieving statistically significant improvements in performance metrics across multiple datasets.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its rigorous statistical framework, which effectively quantifies the bias-variance tradeoff and offers a solid theoretical foundation for the proposed methodology. The empirical evaluations are robust, featuring comprehensive analyses of hyperparameter sensitivity and systematic comparisons with state-of-the-art methods. However, a potential weakness is the reliance on specific datasets (D4RL locomotion tasks) for evaluation, which may limit the generalizability of the findings to other domains or applications within offline RL.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with the field. The quality of the empirical results is high, with thorough statistical analyses supporting the claims. The novelty of the approach is significant, introducing innovative methods for addressing common challenges in offline RL. Reproducibility is facilitated by the detailed descriptions of methodologies and statistical evaluations, although the availability of code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to the field of offline reinforcement learning by introducing a statistically grounded approach that effectively addresses the bias-variance tradeoff. The empirical results are compelling and support the proposed methodology, although the scope of evaluation could benefit from broader applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel method, Conditional Weighting with Bias Correction (CWBC), aimed at enhancing the performance of Reinforcement Learning (RL) algorithms, specifically Return Value Sampling (RvS) and Decision Trees (DT). The methodology focuses on addressing the limitations of these existing approaches by incorporating a conservative regularization technique that aims to improve the learning process from offline datasets. The findings indicate that CWBC outperforms RvS and DT in specific benchmark settings; however, it still lags behind more advanced value-based methods like CQL and IQL, highlighting both potential and limitations in its general applicability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to conservative regularization, which may enhance the robustness of offline RL methods. However, the evaluation is limited in scope, primarily focusing on specific benchmarks without exploring diverse environments. Additionally, while CWBC mitigates the need for ad-hoc tuning of conditioning values, it still requires careful selection of hyperparameters, which diminishes its robustness. The paper also struggles with extrapolation beyond the offline dataset and is sensitive to the quality of the data, which poses practical challenges in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers. However, it lacks a strong theoretical foundation to support its claims, which may affect confidence in the reproducibility of results. The novelty of the conservative regularization approach is evident, but the limited exploration of existing conservative methods in offline RL leaves a gap in understanding its relative advantages or disadvantages. Overall, while the methodology is clear, the lack of theoretical analysis raises questions about the robustness and generalizability of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to improving offline RL methods through CWBC. While it demonstrates clear empirical improvements over baseline methods, significant limitations in extrapolation, sensitivity to data quality, and a lack of theoretical guarantees hinder its overall impact. Future work should address these limitations to enhance the applicability and robustness of the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"ConserWeightive Behavioral Cloning for Reliable Offline Reinforcement Learning\" proposes a novel approach to offline reinforcement learning through a method called ConserWeightive Behavioral Cloning (CWBC). The main contributions include addressing the challenge of skewed return distributions in offline datasets by introducing trajectory weighting and conservative regularization. The authors claim their method improves the performance and stability of existing algorithms in offline settings, asserting significant performance gains through empirical evaluations.\n\n# Strength And Weaknesses\nStrengths of the paper include the authors' identification of a pertinent issue in offline reinforcement learning and the proposed CWBC method, which attempts to mitigate biases associated with return distributions. However, the paper suffers from several weaknesses, including a lack of novelty in its foundational concepts, as both behavioral cloning and regularization are well-established ideas in the field. Additionally, the experimental results are potentially undermined by overfitting concerns due to hyperparameter tuning and validation set adjustments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the superficial treatment of established concepts, which may lead to confusion regarding the true novelty of the proposed approach. While the methodology is presented in a structured manner, the lack of depth in discussing related work and existing solutions detracts from the overall quality. The reproducibility of results could be questioned due to the potential overfitting observed in experiments, as well as insufficient detail on the experimental setup and hyperparameter choices.\n\n# Summary Of The Review\nIn summary, while the paper identifies an important challenge in offline reinforcement learning and introduces a method that may offer some improvements, it does not contribute significantly to the existing body of knowledge. The reliance on established concepts without substantial innovation limits its impact and relevance in the field.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper proposes a novel method called ConserWeightive Behavioral Cloning (CWBC), which introduces trajectory weighting and a conservative regularization mechanism to improve the performance of offline reinforcement learning. The authors demonstrate that CWBC achieves significant performance enhancements over existing methods, particularly when conditioned on high-return values. The experimental results across various benchmarks, including Atari games and Antmaze datasets, suggest that CWBC is robust and effective. However, the authors acknowledge the necessity for further exploration of alternative weighting schemes and regularization techniques to optimize performance.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to trajectory weighting and conservative regularization, which provides a clear contribution to the offline reinforcement learning field. The experimental results are compelling, showcasing substantial performance improvements. However, the paper could benefit from a more extensive exploration of alternative methods, such as adaptive weighting and other regularization techniques, which could enhance the robustness and generalizability of the model. Additionally, while the findings are promising, validation on a broader range of tasks beyond locomotion would strengthen the claims of CWBC's applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed methodology and findings. The quality of the writing is high, making complex concepts accessible. The novelty of CWBC is significant, as it presents a new perspective on behavioral cloning in offline reinforcement learning. However, the reproducibility of the results could be improved by providing more detailed analyses on hyperparameter sensitivity and computational efficiency, particularly in large-scale applications.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of offline reinforcement learning with the introduction of the CWBC method. While it shows promising results and innovative approaches, further exploration of alternative techniques and broader validation is needed to fully establish its robustness and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces ConserWeightive Behavioral Cloning (CWBC), a novel approach aimed at enhancing performance in offline reinforcement learning (RL) tasks. The authors demonstrate that CWBC significantly improves benchmark performance on D4RL locomotion tasks, with notable increases of 18% for RvS (Reinforcement Learning via Supervised Learning) and 8% for the Decision Transformer (DT) when combined with CWBC. The methodology includes trajectory weighting and conservative regularization, resulting in RvS+W+C outperforming the original RvS across all datasets tested and showing competitive performance against value-based methods like CQL and IQL. Additionally, the empirical results reveal substantial improvements in performance across various environments, including a 72% average increase in Atari games and a 60% improvement in Antmaze tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its clear demonstration of CWBC's effectiveness across multiple challenging benchmarks, providing robust empirical evidence for its advantages in both performance and stability. The combination of trajectory weighting and conservative regularization showcases an innovative approach to enhancing behavioral cloning in offline RL settings. However, the paper could benefit from a deeper exploration of the theoretical underpinnings of CWBC and its broader implications for offline RL, as well as a more comprehensive discussion of potential limitations or scenarios where the method may not perform as well.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and presents its findings in a clear and concise manner, making it accessible for readers with varying levels of expertise in reinforcement learning. The quality of empirical results is high, with thorough comparisons against established benchmarks. The novelty of the proposed CWBC approach is notable, particularly in its application of trajectory weighting and conservative regularization to behavioral cloning. However, while the results are compelling, additional details regarding the reproducibility of the experiments and any hyperparameter settings used would enhance the transparency of the work.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in offline reinforcement learning through the introduction of CWBC, which demonstrates impressive empirical results across various benchmarks. The methodology is innovative, and the findings are clearly communicated, although further exploration of theoretical implications and reproducibility could strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Conservative Behavioral Cloning for Reliable Offline Reinforcement Learning\" proposes a novel approach to behavioral cloning that aims to enhance the reliability of offline reinforcement learning (RL) methods. The authors introduce a methodology that addresses the limitations of existing techniques by focusing on minimizing the bias introduced by out-of-distribution returns. The findings demonstrate that their approach significantly outperforms traditional methods in various benchmark environments, showcasing improved performance and generalization in offline settings.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear identification of key challenges in offline RL, particularly regarding the bias-variance tradeoff, and the introduction of a new technique that effectively mitigates these issues. The empirical results are robust, showing a substantial improvement over existing methods across multiple benchmarks. However, the paper suffers from several weaknesses, including a typographical error in the title, redundancy in terminology, and a lengthy abstract that obscures the main contributions. Additionally, some sections lack clarity due to the heavy use of technical jargon and complex sentence structures.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the methodology is well-detailed, clarity could be improved through simplified language and better organization, such as bullet points for procedural steps. The paper’s novelty lies in its unique approach to behavioral cloning within the context of offline RL, which is significant for the field. However, the readability issues and inconsistent formatting detract from the overall quality. The reproducibility statement is commendable, but it could be made more prominent to emphasize its importance.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of offline reinforcement learning with a novel approach to behavioral cloning. While the empirical results are compelling, the paper would benefit from improved clarity, organization, and attention to detail in terms of formatting and language. Addressing these issues would enhance its readability and impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.3798754443603576,
    -1.6610610841092222,
    -1.8273118434399112,
    -1.742283039143061,
    -1.8186921945665502,
    -1.6730497500312103,
    -1.5594671871426016,
    -1.811211247608965,
    -1.71848208991268,
    -1.8014694297879335,
    -1.6585648400053565,
    -1.2111743409206934,
    -1.6478632983786177,
    -1.722598080611939,
    -1.6999873564301922,
    -1.8167740108096293,
    -1.84362611589409,
    -1.8480196172733474,
    -1.7050178539751448,
    -1.8626375661281196,
    -2.0556055541102145,
    -1.7537787383610022,
    -1.6186594512467176,
    -1.6475640248324521,
    -1.9094881898284106,
    -1.8068012259144146,
    -1.6728119251467253,
    -1.7592142189871691,
    -1.749655514228437
  ],
  "logp_cond": [
    [
      0.0,
      -2.092028338943897,
      -2.1211140024844353,
      -2.070611732975071,
      -2.1147106721080546,
      -2.0960843332516914,
      -2.177364067119637,
      -2.119767078153762,
      -2.1144863761179584,
      -2.108602496398947,
      -2.1176013536088747,
      -2.189226787266156,
      -2.0367704748824487,
      -2.1153353693262984,
      -2.1350503015965074,
      -2.0715641525077224,
      -2.131689268397986,
      -2.1274332865106933,
      -2.12981621729596,
      -2.123037929244063,
      -2.094862409663693,
      -2.1412283546775512,
      -2.165365863070979,
      -2.1103724337083296,
      -2.0715672621772607,
      -2.112407811685842,
      -2.143055095074845,
      -2.094559841771983,
      -2.1389893691999533
    ],
    [
      -1.3453353993367836,
      0.0,
      -1.1450171786688288,
      -1.082502451004638,
      -1.238795034087839,
      -1.1695918406847596,
      -1.186340786893914,
      -1.2103280013310977,
      -1.0923863906197322,
      -1.2536034279067159,
      -1.2045729788983928,
      -1.3750776595973295,
      -1.1273057626709138,
      -1.2140706374329766,
      -1.218716114094992,
      -1.0600395932592825,
      -1.2239919725747033,
      -1.2134835998832358,
      -1.2680726592678466,
      -1.1883366798087074,
      -1.1461517474304868,
      -1.2749810547023896,
      -1.271356628529365,
      -1.184623586656109,
      -1.2544971965260976,
      -1.2445338751316442,
      -1.1642974734618448,
      -1.1137936226663616,
      -1.3131036654035018
    ],
    [
      -1.5361849927734652,
      -1.375421199659912,
      0.0,
      -1.3074029903818467,
      -1.3575462311765418,
      -1.3625830042374245,
      -1.395195001392256,
      -1.450987078791422,
      -1.3935087755925182,
      -1.3983239539817036,
      -1.331082283433573,
      -1.5638846025499142,
      -1.299289787664932,
      -1.3595535216598562,
      -1.4083348783545957,
      -1.2856469898595406,
      -1.3446802507735154,
      -1.37402217878681,
      -1.4553319336066135,
      -1.348674226599284,
      -1.3600441018687923,
      -1.4669644933523316,
      -1.4439209022464483,
      -1.354460233519843,
      -1.4432700939016392,
      -1.4355454238429899,
      -1.3802053375270547,
      -1.2586756437502755,
      -1.477521278763804
    ],
    [
      -1.356206285854771,
      -1.1946313801690254,
      -1.225448315448814,
      0.0,
      -1.240615027279489,
      -1.2576212490609018,
      -1.3011040974473436,
      -1.2962499632545255,
      -1.2672408184187371,
      -1.2971405417893984,
      -1.2831085917092573,
      -1.4721323108462387,
      -1.200100925374712,
      -1.2385315218863877,
      -1.2921649971947202,
      -1.169478816257896,
      -1.2788740565891217,
      -1.2934485521797623,
      -1.3447857256707743,
      -1.204669869752691,
      -1.2410226751637108,
      -1.35395413675868,
      -1.3421874211567473,
      -1.26198792629998,
      -1.440143109888595,
      -1.241086050674653,
      -1.2666413155523266,
      -1.2297574710794505,
      -1.39015004099752
    ],
    [
      -1.589122679823213,
      -1.4579345326643653,
      -1.4399414471535394,
      -1.4333686810146724,
      0.0,
      -1.4448914263298374,
      -1.4723646386212859,
      -1.4405792488265659,
      -1.4387278961923222,
      -1.4601193489795217,
      -1.4661730842427092,
      -1.5855634802353162,
      -1.4543435705691066,
      -1.4719697098949833,
      -1.4373689374625318,
      -1.443575776306238,
      -1.4698996801657536,
      -1.4914071201413983,
      -1.51204275923059,
      -1.410310673155184,
      -1.4204566537302394,
      -1.5388030437292455,
      -1.4927818012900185,
      -1.4290562072057937,
      -1.51514770012239,
      -1.4442021285583924,
      -1.459690889880864,
      -1.4597572768807139,
      -1.5254058707184477
    ],
    [
      -1.3451291665977798,
      -1.2094399902261557,
      -1.1691844142063152,
      -1.1068480523264057,
      -1.2402434706115417,
      0.0,
      -1.2269966936772105,
      -1.2445796474926796,
      -1.205298717647814,
      -1.272689140076241,
      -1.185482077924416,
      -1.3557486283127953,
      -1.1402985607128036,
      -1.218231324144963,
      -1.2259078916516213,
      -1.1583331892781232,
      -1.1879714984211827,
      -1.2410618577070862,
      -1.1896607992895614,
      -1.182209329493315,
      -1.1791368831205444,
      -1.26133666224474,
      -1.2980116367391583,
      -1.1927234606083081,
      -1.313745317333405,
      -1.245721280267607,
      -1.2134847077918884,
      -1.2136845369813032,
      -1.2461306135087191
    ],
    [
      -1.2729365119331133,
      -1.0928437009096366,
      -1.0458663339611824,
      -1.0625296898827024,
      -1.0720429820561088,
      -1.0843205262083522,
      0.0,
      -1.046093188810899,
      -1.0535994474828239,
      -1.1769786975619811,
      -1.0699509412472739,
      -1.2798797893031795,
      -1.055129480562897,
      -1.177189956010397,
      -1.089181215088072,
      -1.0761553479000514,
      -1.1172815559020497,
      -1.1004741728216854,
      -1.1847944535592814,
      -1.0792696013518108,
      -1.0832612448707506,
      -1.219151143497397,
      -1.1316311013470999,
      -1.100624129430358,
      -1.224698821944381,
      -1.0606941916422084,
      -1.1050938287838914,
      -1.0931902773187334,
      -1.1856777042201812
    ],
    [
      -1.576623823622993,
      -1.4411952523667646,
      -1.3873645012684295,
      -1.4249425054766267,
      -1.4213824537207684,
      -1.45481530583907,
      -1.4393742878758102,
      0.0,
      -1.390425695184008,
      -1.4829453162228512,
      -1.4125826795902108,
      -1.6046899314098615,
      -1.3856106092536082,
      -1.4573614546663205,
      -1.4639135644909345,
      -1.4131168822183002,
      -1.4299692265772346,
      -1.3897136839103383,
      -1.4668057783192259,
      -1.4365913630974216,
      -1.4330545123400107,
      -1.5243246388048233,
      -1.4719397019861393,
      -1.4379447804958587,
      -1.534067391607589,
      -1.4311049440957995,
      -1.4299821940438115,
      -1.4169876032311688,
      -1.5296536224038473
    ],
    [
      -1.3981550010586843,
      -1.1400345913750438,
      -1.2251365492375075,
      -1.170776135478898,
      -1.265077587231601,
      -1.2350179197301538,
      -1.199598509331875,
      -1.2027847086445713,
      0.0,
      -1.3436737718430065,
      -1.2659612524370452,
      -1.461592625644599,
      -1.135752086191257,
      -1.2710591721296403,
      -1.2673629899303596,
      -1.1126955886026957,
      -1.244938078535764,
      -1.2376468270272418,
      -1.3031134346775601,
      -1.2356180086088613,
      -1.154638329809365,
      -1.3267800988600869,
      -1.2820046083507646,
      -1.244852456877758,
      -1.324271437065555,
      -1.25104489087992,
      -1.287268906283557,
      -1.1771745414139165,
      -1.3052109538809304
    ],
    [
      -1.525065375094352,
      -1.3828724618847281,
      -1.343870288344929,
      -1.3855585626029714,
      -1.384910774386488,
      -1.447452150997123,
      -1.5000801906551149,
      -1.4304144893006223,
      -1.4416664684189928,
      0.0,
      -1.4105240855622925,
      -1.548176736967457,
      -1.3733521558973014,
      -1.3445333485025242,
      -1.4782664414331796,
      -1.3904476421887682,
      -1.3721737959361864,
      -1.450679742197376,
      -1.4111951144615533,
      -1.3937259201958616,
      -1.410107728820219,
      -1.45660151596813,
      -1.4322449958718309,
      -1.4056501678839006,
      -1.4238550373610723,
      -1.4631241533415495,
      -1.4097214790971493,
      -1.4101014429066274,
      -1.48573444620069
    ],
    [
      -1.3392109721573247,
      -1.1563757740550276,
      -1.1230176738695898,
      -1.0744756918174274,
      -1.2112131101710994,
      -1.160279339406963,
      -1.2097037619475,
      -1.235230082038382,
      -1.228188127288185,
      -1.2397789959829788,
      0.0,
      -1.3988242433185334,
      -1.1483159870882846,
      -1.16692302710989,
      -1.2136834735339885,
      -1.1157469591503397,
      -1.1718888522610704,
      -1.2090169113406215,
      -1.2284487923466927,
      -1.1656108559126013,
      -1.1911891208637528,
      -1.2768808798614502,
      -1.2938830486634716,
      -1.1220244927739285,
      -1.337903306517306,
      -1.2029485012641254,
      -1.1646120412636591,
      -1.1621454371403854,
      -1.335499257176857
    ],
    [
      -0.9991408730901221,
      -0.9654886416813608,
      -0.9630152755622162,
      -0.943447796472719,
      -0.9605409792054003,
      -0.9326088636480951,
      -0.9732880387491923,
      -0.9945486626893344,
      -0.9619916048705622,
      -0.9680716042637353,
      -0.962172788144219,
      0.0,
      -0.9483014443612641,
      -0.9704546877156642,
      -0.9650531085451102,
      -0.9543885627381438,
      -0.9392968423352667,
      -0.9885199395605166,
      -0.9316491245251516,
      -0.9411349874305694,
      -0.9594073133164841,
      -0.9372181874116213,
      -0.9730307719559002,
      -0.9591372216198696,
      -0.9586238613957105,
      -0.9875946358558682,
      -0.9649256700510211,
      -0.9576835023389826,
      -0.9575483543599965
    ],
    [
      -1.298341063428946,
      -1.1856714569069033,
      -1.2307711802846248,
      -1.1810568521997138,
      -1.254356385357984,
      -1.2326276637677425,
      -1.27700165563651,
      -1.2941111107554628,
      -1.2378606449282588,
      -1.2857426170433566,
      -1.2557633990796775,
      -1.4044201573975021,
      0.0,
      -1.2630130285477572,
      -1.279200169004759,
      -1.1859904459980588,
      -1.2620813011089977,
      -1.2828453657725227,
      -1.2513104978079872,
      -1.225631092912878,
      -1.1981524389549885,
      -1.3311935884490318,
      -1.3299518409954594,
      -1.2264951613325281,
      -1.2849536588864878,
      -1.2346290242534466,
      -1.2503422376002002,
      -1.193152582296187,
      -1.3348265733511966
    ],
    [
      -1.4155204657645817,
      -1.2712264264032296,
      -1.2550633847370811,
      -1.2307746043548664,
      -1.2467972630060145,
      -1.266574378178488,
      -1.313251217152001,
      -1.3068775190088684,
      -1.2815858224012544,
      -1.3055958462690664,
      -1.240499270199995,
      -1.446068709533245,
      -1.2360088450230837,
      0.0,
      -1.2541803658277,
      -1.2501151240375306,
      -1.3298035254888108,
      -1.3284064015946468,
      -1.2688010451253737,
      -1.2503426521143677,
      -1.2765247390127685,
      -1.31794683005266,
      -1.3201698798738204,
      -1.3015948709939404,
      -1.3831622145200932,
      -1.2591807542468627,
      -1.2515215242100233,
      -1.2909886744105519,
      -1.2936539233645472
    ],
    [
      -1.418402532413889,
      -1.254711184603553,
      -1.2698248826117076,
      -1.1981717109132566,
      -1.172311206423229,
      -1.2699251012931971,
      -1.2291380677273298,
      -1.2217311721414905,
      -1.2388978312072993,
      -1.3409152649938592,
      -1.232319171800501,
      -1.3844115378931006,
      -1.2455222161060744,
      -1.2714789465935579,
      0.0,
      -1.1983699358976434,
      -1.2371603082612586,
      -1.2836761724551233,
      -1.3434717021756164,
      -1.174844922372085,
      -1.26600626599387,
      -1.2642893303440876,
      -1.2986823884575793,
      -1.2209867059787685,
      -1.3613927745516363,
      -1.1827990113368654,
      -1.243461696984368,
      -1.2596560926609515,
      -1.2462674780501466
    ],
    [
      -1.4687786895875143,
      -1.2232863067307114,
      -1.2807221645142104,
      -1.2149255049181638,
      -1.3466889866492688,
      -1.3468344618048134,
      -1.402444269158435,
      -1.4082053199559785,
      -1.2527599710441748,
      -1.3749506539908212,
      -1.359453422982351,
      -1.5479869076142279,
      -1.2482953217004562,
      -1.302121500918137,
      -1.3726820883703343,
      0.0,
      -1.3509630059299818,
      -1.3693915956587908,
      -1.4238381379184377,
      -1.3432613616847537,
      -1.3042534264241743,
      -1.427859092806809,
      -1.4437776052275433,
      -1.2963310736292357,
      -1.4273361822749768,
      -1.4221921143154514,
      -1.3375358453719113,
      -1.2307408695723412,
      -1.4413020464981754
    ],
    [
      -1.5206779290988965,
      -1.356282928283361,
      -1.310383084364747,
      -1.3543771942757534,
      -1.4107631604810966,
      -1.364317207818671,
      -1.4672114516478538,
      -1.4006487034724382,
      -1.3937554191507529,
      -1.4095016437732053,
      -1.3775976232511111,
      -1.5662297400352987,
      -1.3413194368868075,
      -1.4100548513218027,
      -1.4569191912644186,
      -1.3276258920376638,
      0.0,
      -1.32993266440867,
      -1.429612989675937,
      -1.3917541896497407,
      -1.360164989009028,
      -1.4587664555456534,
      -1.4435117379979063,
      -1.3675145982452184,
      -1.443707806865943,
      -1.4225375398127473,
      -1.3328747852450795,
      -1.3245532619127125,
      -1.5211642076337717
    ],
    [
      -1.5128134043363326,
      -1.3419992827300302,
      -1.31917618615821,
      -1.3503465337616436,
      -1.359043863745055,
      -1.3602060712583002,
      -1.393717842088906,
      -1.3981288809957968,
      -1.3488850746952308,
      -1.4422430612412853,
      -1.368355941027425,
      -1.5544110789893717,
      -1.3286336744020075,
      -1.425153151222039,
      -1.4098430866127236,
      -1.3249025877939207,
      -1.3368769175208008,
      0.0,
      -1.4247458606555201,
      -1.3486319928726098,
      -1.3421889133930165,
      -1.4246854095255457,
      -1.4512866300348186,
      -1.389672191989067,
      -1.458998409066946,
      -1.4133791238843618,
      -1.3415986463669054,
      -1.2880044522592278,
      -1.4779210544815269
    ],
    [
      -1.4286364337319721,
      -1.2635267927232685,
      -1.2646028687747957,
      -1.2409294444276628,
      -1.3269345733279185,
      -1.237960716061552,
      -1.329042625502469,
      -1.3386755481707626,
      -1.320638161986876,
      -1.3353821128538177,
      -1.2950322585467962,
      -1.4124142549714698,
      -1.2203539699244006,
      -1.2731482845347866,
      -1.3421371544194431,
      -1.294925802151853,
      -1.2974184968275058,
      -1.3140727326597745,
      0.0,
      -1.2559858313167398,
      -1.2954860822259746,
      -1.3362402519310488,
      -1.3450867195716694,
      -1.2754705597016394,
      -1.3936005316815854,
      -1.3249252237039248,
      -1.3021884792470626,
      -1.29385731215521,
      -1.377720443386486
    ],
    [
      -1.5716189290628573,
      -1.4100549365716553,
      -1.3779531628562225,
      -1.3383873208666361,
      -1.4086140468043025,
      -1.4097473585799514,
      -1.4285689288581562,
      -1.4389197278090833,
      -1.3860267345946995,
      -1.4644247247540054,
      -1.4204018851237912,
      -1.6018600981288689,
      -1.383443021373703,
      -1.4595558270431805,
      -1.3934031692163895,
      -1.415979132212932,
      -1.4190212833407396,
      -1.456914039703098,
      -1.4731585623069257,
      0.0,
      -1.4148795429783203,
      -1.502526030382732,
      -1.488720411848517,
      -1.412464464837828,
      -1.541020144817921,
      -1.4150911173572263,
      -1.4453880531968917,
      -1.4120581021022007,
      -1.5308731882319628
    ],
    [
      -1.6902890227313567,
      -1.5401260048688226,
      -1.5603988778313218,
      -1.5164711786190823,
      -1.5864877583554602,
      -1.611746021205075,
      -1.6235998618763459,
      -1.6095190924612546,
      -1.555300195822861,
      -1.6445664810652472,
      -1.6155032282018553,
      -1.773087303601641,
      -1.472238919039839,
      -1.6245609691578735,
      -1.6542825743169955,
      -1.528113422812267,
      -1.594493983961295,
      -1.5947060668335447,
      -1.656535104164676,
      -1.5890912156550225,
      0.0,
      -1.6765940269432602,
      -1.66786508683616,
      -1.5551780807275326,
      -1.5979691869547012,
      -1.602551289991411,
      -1.6018048236616929,
      -1.5513834458957187,
      -1.723715216743168
    ],
    [
      -1.3947082558376573,
      -1.2930578714585683,
      -1.3125058722996445,
      -1.3067860076682623,
      -1.3309767697121166,
      -1.3154279528802608,
      -1.3727945961216286,
      -1.3511841820422523,
      -1.3166286300187076,
      -1.3253297766357988,
      -1.288191803591679,
      -1.4214089096871942,
      -1.304218449224763,
      -1.266203094086738,
      -1.3047802634404144,
      -1.2984088321568648,
      -1.3206292224286478,
      -1.315029616190956,
      -1.2833096615692139,
      -1.2951107584952632,
      -1.3042986328946085,
      0.0,
      -1.32624697224809,
      -1.316010574769382,
      -1.3605753464546773,
      -1.294802806142008,
      -1.2913874986511407,
      -1.3335547065160889,
      -1.2489384057811777
    ],
    [
      -1.334241476626389,
      -1.2301927865883253,
      -1.2354835791085081,
      -1.2014865500795753,
      -1.1545484424188692,
      -1.2622417999376558,
      -1.223078863029344,
      -1.233843001950746,
      -1.2029296510614147,
      -1.2072431167955,
      -1.2525935730693978,
      -1.3357550407572585,
      -1.2206899474319859,
      -1.2065759687252222,
      -1.2105588647840775,
      -1.251444385615432,
      -1.2354839792402834,
      -1.2538259456830825,
      -1.2384743930356137,
      -1.2074772311961464,
      -1.2352809663938062,
      -1.2766809510132073,
      0.0,
      -1.230971904030383,
      -1.2369386525323804,
      -1.227894252156381,
      -1.2451822539727284,
      -1.2011058385086646,
      -1.245922314816008
    ],
    [
      -1.2882576651426139,
      -1.1148699997086946,
      -1.0415359426591135,
      -1.0833101535334484,
      -1.1296855734870306,
      -1.1294810211106123,
      -1.1522580872637354,
      -1.1680181769256233,
      -1.1295380660035115,
      -1.1928920689110498,
      -1.1352123896389337,
      -1.3536613462484848,
      -1.1291286062345225,
      -1.1707406585556375,
      -1.1306985296875998,
      -1.0882488110932071,
      -1.1060469833404893,
      -1.1843785951032395,
      -1.1883953796607147,
      -1.0914030048964143,
      -1.116394048985251,
      -1.2250219777596452,
      -1.1973882852583315,
      0.0,
      -1.2944109220717435,
      -1.1794822023974305,
      -1.1360886520918838,
      -1.0986868706488893,
      -1.2281664960394645
    ],
    [
      -1.571859643875052,
      -1.493319063839251,
      -1.5270353207801834,
      -1.5049743205593131,
      -1.5570905962787551,
      -1.574060750560483,
      -1.5876106914531616,
      -1.576814729667518,
      -1.5188154142301709,
      -1.5669561271081995,
      -1.5754639462040434,
      -1.6614366163295113,
      -1.4514500050631503,
      -1.576029352498333,
      -1.5793644908566322,
      -1.5238737778744063,
      -1.5610093117280919,
      -1.5558092046451382,
      -1.574913267965746,
      -1.5554344123566408,
      -1.4909795165508812,
      -1.606875166968603,
      -1.5775826870016565,
      -1.5681754662516587,
      0.0,
      -1.5816187684696972,
      -1.556049519629904,
      -1.4638509241119244,
      -1.6350804333906126
    ],
    [
      -1.4731167360032618,
      -1.3144771625840526,
      -1.3183297688498286,
      -1.2764423589088187,
      -1.2918766261136034,
      -1.3287495934908797,
      -1.2923462102049366,
      -1.311043157326007,
      -1.2768584818933457,
      -1.4584530376920437,
      -1.3074908915075578,
      -1.514822358323799,
      -1.2560672598846687,
      -1.342129752969429,
      -1.2256100022247964,
      -1.2860484263623844,
      -1.2993285698078834,
      -1.374661256239334,
      -1.4214324293606075,
      -1.260091162292779,
      -1.3567151492742735,
      -1.4033786861251099,
      -1.3971668944349955,
      -1.3213615122876563,
      -1.4629084332410502,
      0.0,
      -1.344594951626568,
      -1.3469450177432514,
      -1.3620877692627094
    ],
    [
      -1.378949272524178,
      -1.1632110211439515,
      -1.1797721055434724,
      -1.1582442584903345,
      -1.2309854051492453,
      -1.2365070220210783,
      -1.2517231095432346,
      -1.2356643673559309,
      -1.2279044254795202,
      -1.2413678776500958,
      -1.1868692595967192,
      -1.3971057467847652,
      -1.2091659997444955,
      -1.1982002219837693,
      -1.2503284556325431,
      -1.1816911637735783,
      -1.1871792538758108,
      -1.2421302100760536,
      -1.2508897177140959,
      -1.205025328687099,
      -1.1909186581377462,
      -1.3154149078572426,
      -1.2670755986763658,
      -1.2141524382049318,
      -1.307271953219891,
      -1.2670148457724584,
      0.0,
      -1.1391810891482894,
      -1.3535178189903467
    ],
    [
      -1.451309669612004,
      -1.2973094070898847,
      -1.3008221856380897,
      -1.290997899177435,
      -1.414135277885584,
      -1.376342979781994,
      -1.4019987969151795,
      -1.3929740652079776,
      -1.3185014425070083,
      -1.4336543936678299,
      -1.3157739661101593,
      -1.515556163718092,
      -1.278217091095303,
      -1.3993606683245094,
      -1.3840242751091518,
      -1.2646805266873926,
      -1.2953405003209548,
      -1.341374739247465,
      -1.4103531871630708,
      -1.3518419994465347,
      -1.2933782854383964,
      -1.448675037989212,
      -1.4426389932821801,
      -1.339494504888828,
      -1.4101214624768947,
      -1.4345168759162465,
      -1.3334018252986477,
      0.0,
      -1.4973452158748852
    ],
    [
      -1.4761702311909628,
      -1.3834029344881458,
      -1.367666551329545,
      -1.3457147755902514,
      -1.3583823924808938,
      -1.355048225772742,
      -1.3569364149307606,
      -1.386433233636627,
      -1.3457783319410375,
      -1.3803333850115735,
      -1.4205898792705491,
      -1.4592847278191998,
      -1.3821629920036758,
      -1.3668571948253958,
      -1.3384205548793082,
      -1.3303385915520722,
      -1.4063949614246938,
      -1.3962565259739526,
      -1.3994610561778227,
      -1.3471007136709174,
      -1.3735449601411238,
      -1.27580392488641,
      -1.3822232354141515,
      -1.3832032830913084,
      -1.3932973045511854,
      -1.3060458068846494,
      -1.423002771861707,
      -1.4054051242771448,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.28784710541646064,
      0.2587614418759223,
      0.30926371138528674,
      0.26516477225230295,
      0.28379111110866617,
      0.20251137724072077,
      0.2601083662065955,
      0.26538906824239916,
      0.27127294796141044,
      0.2622740907514829,
      0.19064865709420165,
      0.3431049694779089,
      0.2645400750340592,
      0.24482514276385015,
      0.30831129185263517,
      0.2481861759623718,
      0.2524421578496643,
      0.25005922706439776,
      0.2568375151162945,
      0.2850130346966644,
      0.23864708968280635,
      0.21450958128937847,
      0.269503010652028,
      0.30830818218309686,
      0.2674676326745158,
      0.23682034928551277,
      0.2853156025883745,
      0.24088607516040428
    ],
    [
      0.31572568477243856,
      0.0,
      0.5160439054403934,
      0.5785586331045842,
      0.42226605002138307,
      0.4914692434244625,
      0.4747202972153082,
      0.4507330827781244,
      0.56867469348949,
      0.4074576562025063,
      0.4564881052108294,
      0.2859834245118926,
      0.5337553214383084,
      0.44699044667624555,
      0.4423449700142301,
      0.6010214908499396,
      0.4370691115345189,
      0.4475774842259863,
      0.3929884248413755,
      0.47272440430051477,
      0.5149093366787354,
      0.3860800294068325,
      0.3897044555798572,
      0.47643749745311315,
      0.40656388758312456,
      0.416527208977578,
      0.4967636106473774,
      0.5472674614428605,
      0.3479574187057204
    ],
    [
      0.29112685066644595,
      0.4518906437799992,
      0.0,
      0.5199088530580644,
      0.4697656122633693,
      0.46472883920248664,
      0.4321168420476551,
      0.3763247646484891,
      0.433803067847393,
      0.42898788945820754,
      0.4962295600063382,
      0.2634272408899969,
      0.5280220557749791,
      0.46775832178005494,
      0.41897696508531546,
      0.5416648535803705,
      0.48263159266639577,
      0.45328966465310105,
      0.37197990983329765,
      0.4786376168406272,
      0.46726774157111883,
      0.36034735008757957,
      0.38339094119346284,
      0.47285160992006814,
      0.38404174953827197,
      0.39176641959692127,
      0.4471065059128565,
      0.5686361996896356,
      0.34979056467610725
    ],
    [
      0.38607675328829005,
      0.5476516589740357,
      0.516834723694247,
      0.0,
      0.5016680118635721,
      0.48466179008215926,
      0.4411789416957175,
      0.4460330758885356,
      0.4750422207243239,
      0.4451424973536626,
      0.45917444743380376,
      0.2701507282968223,
      0.5421821137683491,
      0.5037515172566733,
      0.4501180419483408,
      0.5728042228851651,
      0.46340898255393936,
      0.44883448696329875,
      0.3974973134722868,
      0.5376131693903701,
      0.5012603639793503,
      0.388328902384381,
      0.40009561798631377,
      0.480295112843081,
      0.302139929254466,
      0.501196988468408,
      0.4756417235907344,
      0.5125255680636105,
      0.352132998145541
    ],
    [
      0.2295695147433372,
      0.36075766190218483,
      0.37875074741301074,
      0.38532351355187777,
      0.0,
      0.3738007682367128,
      0.3463275559452643,
      0.37811294573998433,
      0.37996429837422796,
      0.3585728455870285,
      0.352519110323841,
      0.23312871433123394,
      0.3643486239974436,
      0.34672248467156686,
      0.3813232571040184,
      0.37511641826031217,
      0.3487925144007966,
      0.32728507442515187,
      0.3066494353359601,
      0.4083815214113662,
      0.39823554083631074,
      0.2798891508373047,
      0.3259103932765317,
      0.38963598736075644,
      0.3035444944441601,
      0.3744900660081578,
      0.35900130468568614,
      0.3589349176858363,
      0.2932863238481025
    ],
    [
      0.32792058343343045,
      0.4636097598050546,
      0.503865335824895,
      0.5662016977048046,
      0.43280627941966854,
      0.0,
      0.4460530563539997,
      0.4284701025385307,
      0.4677510323833962,
      0.40036060995496925,
      0.4875676721067943,
      0.31730112171841496,
      0.5327511893184067,
      0.4548184258862473,
      0.447141858379589,
      0.514716560753087,
      0.4850782516100276,
      0.43198789232412405,
      0.4833889507416489,
      0.49084042053789534,
      0.4939128669106658,
      0.4117130877864703,
      0.37503811329205194,
      0.4803262894229021,
      0.3593044326978052,
      0.4273284697636033,
      0.4595650422393218,
      0.4593652130499071,
      0.42691913652249114
    ],
    [
      0.2865306752094883,
      0.466623486232965,
      0.5136008531814191,
      0.4969374972598992,
      0.48742420508649276,
      0.4751466609342494,
      0.0,
      0.5133739983317025,
      0.5058677396597777,
      0.3824884895806204,
      0.4895162458953277,
      0.2795873978394221,
      0.5043377065797046,
      0.38227723113220446,
      0.4702859720545296,
      0.48331183924255017,
      0.4421856312405519,
      0.4589930143209162,
      0.3746727335833202,
      0.48019758579079075,
      0.476205942271851,
      0.3403160436452046,
      0.4278360857955017,
      0.45884305771224354,
      0.3347683651982205,
      0.4987729955003932,
      0.4543733583587102,
      0.46627690982386816,
      0.3737894829224204
    ],
    [
      0.234587423985972,
      0.3700159952422004,
      0.42384674634053554,
      0.3862687421323383,
      0.3898287938881966,
      0.356395941769895,
      0.3718369597331548,
      0.0,
      0.4207855524249571,
      0.3282659313861138,
      0.39862856801875424,
      0.20652131619910352,
      0.4256006383553568,
      0.3538497929426445,
      0.34729768311803055,
      0.3980943653906648,
      0.38124202103173044,
      0.4214975636986267,
      0.34440546928973914,
      0.3746198845115434,
      0.3781567352689543,
      0.28688660880414174,
      0.3392715456228257,
      0.3732664671131063,
      0.2771438560013759,
      0.38010630351316554,
      0.38122905356515346,
      0.3942236443777962,
      0.28155762520511773
    ],
    [
      0.32032708885399575,
      0.5784474985376362,
      0.49334554067517256,
      0.5477059544337821,
      0.4534045026810791,
      0.4834641701825262,
      0.518883580580805,
      0.5156973812681087,
      0.0,
      0.3748083180696735,
      0.4525208374756349,
      0.256889464268081,
      0.5827300037214231,
      0.4474229177830398,
      0.45111909998232047,
      0.6057865013099843,
      0.47354401137691604,
      0.4808352628854382,
      0.4153686552351199,
      0.48286408130381875,
      0.563843760103315,
      0.3917019910525932,
      0.43647748156191546,
      0.4736296330349221,
      0.394210652847125,
      0.46743719903275993,
      0.4312131836291231,
      0.5413075484987635,
      0.4132711360317496
    ],
    [
      0.27640405469358154,
      0.41859696790320533,
      0.4575991414430045,
      0.41591086718496206,
      0.41655865540144554,
      0.35401727879081046,
      0.30138923913281856,
      0.37105494048731114,
      0.35980296136894063,
      0.0,
      0.390945344225641,
      0.2532926928204764,
      0.42811727389063203,
      0.4569360812854093,
      0.3232029883547538,
      0.4110217875991653,
      0.42929563385174707,
      0.3507896875905574,
      0.39027431532638013,
      0.40774350959207184,
      0.3913617009677144,
      0.3448679138198034,
      0.3692244339161026,
      0.3958192619040328,
      0.3776143924268611,
      0.3383452764463839,
      0.39174795069078416,
      0.39136798688130603,
      0.3157349835872434
    ],
    [
      0.3193538678480319,
      0.502189065950329,
      0.5355471661357667,
      0.5840891481879291,
      0.4473517298342571,
      0.4982855005983935,
      0.4488610780578566,
      0.4233347579669746,
      0.4303767127171716,
      0.41878584402237773,
      0.0,
      0.25974059668682314,
      0.5102488529170719,
      0.4916418128954665,
      0.444881366471368,
      0.5428178808550168,
      0.4866759877442861,
      0.44954792866473503,
      0.43011604765866385,
      0.49295398409275526,
      0.4673757191416037,
      0.3816839601439064,
      0.36468179134188494,
      0.5365403472314281,
      0.3206615334880505,
      0.4556163387412311,
      0.4939527987416974,
      0.4964194028649711,
      0.3230655828284996
    ],
    [
      0.2120334678305713,
      0.2456856992393326,
      0.2481590653584772,
      0.2677265444479744,
      0.25063336171529316,
      0.2785654772725983,
      0.23788630217150109,
      0.21662567823135903,
      0.2491827360501312,
      0.24310273665695814,
      0.2490015527764744,
      0.0,
      0.26287289655942936,
      0.2407196532050292,
      0.24612123237558325,
      0.25678577818254966,
      0.27187749858542676,
      0.22265440136017678,
      0.27952521639554184,
      0.270039353490124,
      0.2517670276042093,
      0.2739561535090721,
      0.23814356896479327,
      0.25203711930082384,
      0.2525504795249829,
      0.22357970506482527,
      0.24624867086967228,
      0.25349083858171084,
      0.2536259865606969
    ],
    [
      0.3495222349496716,
      0.4621918414717143,
      0.4170921180939928,
      0.4668064461789039,
      0.3935069130206337,
      0.4152356346108752,
      0.37086164274210764,
      0.35375218762315486,
      0.4100026534503589,
      0.3621206813352611,
      0.3920998992989402,
      0.24344314098111552,
      0.0,
      0.38485026983086046,
      0.3686631293738587,
      0.46187285238055886,
      0.3857819972696199,
      0.36501793260609494,
      0.39655280057063047,
      0.4222322054657397,
      0.4497108594236292,
      0.3166697099295859,
      0.3179114573831583,
      0.42136813704608955,
      0.36290963949212984,
      0.4132342741251711,
      0.3975210607784174,
      0.45471071608243063,
      0.3130367250274211
    ],
    [
      0.3070776148473573,
      0.4513716542087094,
      0.46753469587485785,
      0.49182347625707257,
      0.4758008176059245,
      0.45602370243345103,
      0.409346863459938,
      0.4157205616030706,
      0.44101225821068457,
      0.41700223434287254,
      0.4820988104119439,
      0.276529371078694,
      0.48658923558885525,
      0.0,
      0.468417714784239,
      0.4724829565744084,
      0.3927945551231282,
      0.39419167901729213,
      0.45379703548656525,
      0.4722554284975713,
      0.44607334159917045,
      0.404651250559279,
      0.4024282007381186,
      0.42100320961799853,
      0.33943586609184573,
      0.46341732636507627,
      0.4710765564019157,
      0.4316094062013871,
      0.42894415724739177
    ],
    [
      0.28158482401630325,
      0.44527617182663914,
      0.4301624738184846,
      0.5018156455169356,
      0.5276761500069631,
      0.43006225513699503,
      0.47084928870286236,
      0.47825618428870165,
      0.4610895252228928,
      0.359072091436333,
      0.46766818462969106,
      0.3155758185370916,
      0.4544651403241178,
      0.4285084098366343,
      0.0,
      0.5016174205325488,
      0.4628270481689336,
      0.41631118397506883,
      0.35651565425457576,
      0.5251424340581072,
      0.43398109043632216,
      0.43569802608610453,
      0.40130496797261284,
      0.4790006504514237,
      0.33859458187855584,
      0.5171883450933268,
      0.45652565944582424,
      0.4403312637692407,
      0.45371987838004557
    ],
    [
      0.34799532122211496,
      0.5934877040789179,
      0.5360518462954189,
      0.6018485058914655,
      0.47008502416036047,
      0.46993954900481594,
      0.4143297416511944,
      0.4085686908536508,
      0.5640140397654545,
      0.44182335681880813,
      0.45732058782727836,
      0.26878710319540144,
      0.5684786891091731,
      0.5146525098914922,
      0.44409192243929496,
      0.0,
      0.46581100487964755,
      0.44738241515083854,
      0.3929358728911916,
      0.47351264912487556,
      0.512520584385455,
      0.3889149180028204,
      0.372996405582086,
      0.5204429371803936,
      0.3894378285346525,
      0.39458189649417785,
      0.479238165437718,
      0.5860331412372881,
      0.3754719643114539
    ],
    [
      0.32294818679519355,
      0.48734318761072903,
      0.533243031529343,
      0.48924892161833666,
      0.43286295541299347,
      0.47930890807541915,
      0.37641466424623626,
      0.4429774124216519,
      0.44987069674333724,
      0.4341244721208848,
      0.46602849264297896,
      0.2773963758587914,
      0.5023066790072825,
      0.43357126457228734,
      0.3867069246296715,
      0.5160002238564263,
      0.0,
      0.5136934514854201,
      0.414013126218153,
      0.45187192624434935,
      0.48346112688506215,
      0.38485966034843666,
      0.40011437789618376,
      0.4761115176488717,
      0.39991830902814707,
      0.42108857608134276,
      0.5107513306490106,
      0.5190728539813776,
      0.3224619082603184
    ],
    [
      0.3352062129370148,
      0.5060203345433172,
      0.5288434311151373,
      0.4976730835117038,
      0.48897575352829237,
      0.48781354601504723,
      0.45430177518444137,
      0.4498907362775506,
      0.4991345425781166,
      0.4057765560320621,
      0.4796636762459223,
      0.2936085382839757,
      0.5193859428713399,
      0.4228664660513084,
      0.4381765306606238,
      0.5231170294794267,
      0.5111426997525466,
      0.0,
      0.4232737566178273,
      0.4993876244007376,
      0.5058307038803309,
      0.4233342077478017,
      0.3967329872385288,
      0.4583474252842803,
      0.3890212082064015,
      0.4346404933889856,
      0.506420970906442,
      0.5600151650141196,
      0.3700985627918205
    ],
    [
      0.2763814202431727,
      0.4414910612518763,
      0.44041498520034916,
      0.464088409547482,
      0.3780832806472263,
      0.4670571379135928,
      0.37597522847267584,
      0.3663423058043822,
      0.38437969198826893,
      0.36963574112132713,
      0.40998559542834867,
      0.29260359900367505,
      0.48466388405074423,
      0.43186956944035826,
      0.3628806995557017,
      0.4100920518232918,
      0.40759935714763906,
      0.3909451213153703,
      0.0,
      0.44903202265840503,
      0.4095317717491702,
      0.36877760204409604,
      0.35993113440347546,
      0.42954729427350546,
      0.31141732229355945,
      0.38009263027122,
      0.4028293747280822,
      0.4111605418199349,
      0.3272974105886588
    ],
    [
      0.2910186370652623,
      0.45258262955646433,
      0.4846844032718971,
      0.5242502452614834,
      0.4540235193238171,
      0.4528902075481682,
      0.43406863726996336,
      0.4237178383190363,
      0.4766108315334201,
      0.39821284137411417,
      0.4422356810043284,
      0.2607774679992507,
      0.4791945447544166,
      0.40308173908493905,
      0.4692343969117301,
      0.44665843391518756,
      0.44361628278737997,
      0.40572352642502163,
      0.38947900382119394,
      0.0,
      0.4477580231497993,
      0.3601115357453877,
      0.3739171542796027,
      0.45017310129029164,
      0.32161742131019855,
      0.44754644877089333,
      0.4172495129312279,
      0.45057946402591886,
      0.33176437789615676
    ],
    [
      0.3653165313788578,
      0.5154795492413919,
      0.4952066762788927,
      0.5391343754911322,
      0.4691177957547543,
      0.4438595329051396,
      0.43200569223386864,
      0.44608646164895993,
      0.5003053582873536,
      0.41103907304496734,
      0.4401023259083592,
      0.2825182505085735,
      0.5833666350703754,
      0.431044584952341,
      0.40132297979321896,
      0.5274921312979475,
      0.46111157014891946,
      0.46089948727666985,
      0.3990704499455384,
      0.46651433845519197,
      0.0,
      0.3790115271669543,
      0.38774046727405453,
      0.5004274733826819,
      0.45763636715551326,
      0.45305426411880356,
      0.45380073044852165,
      0.5042221082144958,
      0.33189033736704654
    ],
    [
      0.35907048252334484,
      0.46072086690243386,
      0.4412728660613576,
      0.4469927306927399,
      0.42280196864888553,
      0.43835078548074136,
      0.3809841422393736,
      0.40259455631874985,
      0.4371501083422946,
      0.4284489617252034,
      0.4655869347693231,
      0.33236982867380793,
      0.44956028913623913,
      0.48757564427426425,
      0.44899847492058775,
      0.4553699062041374,
      0.4331495159323544,
      0.43874912217004614,
      0.4704690767917883,
      0.45866797986573893,
      0.4494801054663937,
      0.0,
      0.4275317661129121,
      0.4377681635916202,
      0.3932033919063249,
      0.45897593221899413,
      0.4623912397098615,
      0.4202240318449133,
      0.5048403325798245
    ],
    [
      0.2844179746203286,
      0.3884666646583923,
      0.3831758721382095,
      0.41717290116714234,
      0.4641110088278484,
      0.3564176513090618,
      0.3955805882173735,
      0.3848164492959716,
      0.4157298001853029,
      0.4114163344512176,
      0.36606587817731984,
      0.28290441048945913,
      0.39796950381473173,
      0.4120834825214954,
      0.40810058646264014,
      0.3672150656312856,
      0.3831754720064342,
      0.36483350556363514,
      0.38018505821110393,
      0.41118222005057126,
      0.38337848485291137,
      0.3419785002335103,
      0.0,
      0.3876875472163346,
      0.3817207987143372,
      0.3907651990903367,
      0.3734771972739892,
      0.417553612738053,
      0.3727371364307097
    ],
    [
      0.35930635968983826,
      0.5326940251237575,
      0.6060280821733386,
      0.5642538712990037,
      0.5178784513454215,
      0.5180830037218398,
      0.49530593756871677,
      0.47954584790682886,
      0.5180259588289406,
      0.4546719559214023,
      0.5123516351935185,
      0.29390267858396735,
      0.5184354185979296,
      0.47682336627681465,
      0.5168654951448524,
      0.559315213739245,
      0.5415170414919628,
      0.4631854297292126,
      0.4591686451717374,
      0.5561610199360378,
      0.531169975847201,
      0.42254204707280696,
      0.4501757395741206,
      0.0,
      0.35315310276070866,
      0.4680818224350216,
      0.5114753727405683,
      0.5488771541835629,
      0.41939752879298764
    ],
    [
      0.33762854595335856,
      0.41616912598915956,
      0.3824528690482272,
      0.4045138692690975,
      0.3523975935496555,
      0.33542743926792773,
      0.32187749837524904,
      0.3326734601608927,
      0.39067277559823976,
      0.3425320627202111,
      0.3340242436243672,
      0.24805157349889928,
      0.45803818476526037,
      0.33345883733007753,
      0.3301236989717784,
      0.38561441195400437,
      0.34847887810031875,
      0.35367898518327245,
      0.3345749218626646,
      0.3540537774717698,
      0.4185086732775294,
      0.3026130228598076,
      0.33190550282675413,
      0.3413127235767519,
      0.0,
      0.32786942135871344,
      0.3534386701985066,
      0.4456372657164862,
      0.274407756437798
    ],
    [
      0.33368448991115285,
      0.49232406333036205,
      0.48847145706458606,
      0.5303588670055959,
      0.5149245998008112,
      0.4780516324235349,
      0.514455015709478,
      0.49575806858840754,
      0.5299427440210689,
      0.3483481882223709,
      0.4993103344068568,
      0.29197886759061564,
      0.5507339660297459,
      0.4646714729449857,
      0.5811912236896182,
      0.5207527995520302,
      0.5074726561065313,
      0.4321399696750805,
      0.38536879655380707,
      0.5467100636216355,
      0.4500860766401411,
      0.40342253978930476,
      0.4096343314794191,
      0.4854397136267583,
      0.34389279267336437,
      0.0,
      0.46220627428784655,
      0.45985620817116324,
      0.44471345665170525
    ],
    [
      0.2938626526225472,
      0.5096009040027738,
      0.4930398196032528,
      0.5145676666563908,
      0.44182651999748,
      0.43630490312564696,
      0.42108881560349065,
      0.4371475577907944,
      0.4449074996672051,
      0.43144404749662946,
      0.4859426655500061,
      0.2757061783619601,
      0.4636459254022298,
      0.474611703162956,
      0.42248346951418214,
      0.491120761373147,
      0.4856326712709145,
      0.43068171507067166,
      0.4219222074326294,
      0.46778659645962617,
      0.48189326700897905,
      0.3573970172894827,
      0.4057363264703595,
      0.4586594869417935,
      0.3655399719268342,
      0.4057970793742669,
      0.0,
      0.5336308359984359,
      0.31929410615637854
    ],
    [
      0.3079045493751651,
      0.4619048118972844,
      0.4583920333490794,
      0.4682163198097342,
      0.3450789411015851,
      0.3828712392051752,
      0.35721542207198964,
      0.3662401537791915,
      0.44071277648016083,
      0.32555982531933925,
      0.44344025287700983,
      0.24365805526907702,
      0.4809971278918661,
      0.35985355066265967,
      0.3751899438780173,
      0.4945336922997765,
      0.4638737186662143,
      0.41783947973970403,
      0.34886103182409833,
      0.4073722195406344,
      0.46583593354877273,
      0.3105391809979572,
      0.31657522570498897,
      0.4197197140983411,
      0.3490927565102744,
      0.3246973430709226,
      0.4258123936885214,
      0.0,
      0.2618690031122839
    ],
    [
      0.2734852830374741,
      0.36625257974029113,
      0.38198896289889195,
      0.40394073863818547,
      0.39127312174754314,
      0.3946072884556948,
      0.39271909929767634,
      0.36322228059180994,
      0.40387718228739944,
      0.36932212921686336,
      0.32906563495788776,
      0.2903707864092371,
      0.36749252222476114,
      0.3827983194030411,
      0.4112349593491287,
      0.41931692267636467,
      0.3432605528037431,
      0.35339898825448435,
      0.3501944580506142,
      0.40255480055751947,
      0.3761105540873131,
      0.473851589342027,
      0.3674322788142854,
      0.36645223113712855,
      0.3563582096772515,
      0.44360970734378746,
      0.3266527423667298,
      0.3442503899512921,
      0.0
    ]
  ],
  "row_avgs": [
    0.26327892010247894,
    0.45445726201884756,
    0.43558822236673606,
    0.45726578222319564,
    0.34708482802636303,
    0.4491465518742929,
    0.4401621858708696,
    0.3580511153189712,
    0.4660091948720294,
    0.376037047199398,
    0.4484570287081624,
    0.2498070786387614,
    0.38816711287650457,
    0.4300182135796007,
    0.43824358456440493,
    0.4607412276934801,
    0.44027752006672277,
    0.4574535700194679,
    0.3930037944566282,
    0.41902778237951965,
    0.4478134669553758,
    0.436189257325152,
    0.38301138944106095,
    0.487442577887548,
    0.35329056389095637,
    0.4630678810559991,
    0.43468829897610944,
    0.38656631056320795,
    0.37303908261851515
  ],
  "col_avgs": [
    0.308073831661205,
    0.452739025657772,
    0.45765751039993435,
    0.478021472723711,
    0.4313320138895449,
    0.42809397136557453,
    0.40139804582933714,
    0.40811356597709,
    0.44014566165973956,
    0.38463558281904986,
    0.4274234416849643,
    0.2709590499635736,
    0.475835547658503,
    0.41784821252804133,
    0.410761454561824,
    0.47014374514468665,
    0.428115444079107,
    0.4069430932714164,
    0.38618955694578044,
    0.4470675125995244,
    0.4440942979381669,
    0.36281395058483773,
    0.3709411547704029,
    0.4325945257254561,
    0.3526357686909871,
    0.4102598344067494,
    0.42444752872177904,
    0.45724626616066927,
    0.350855784150932
  ],
  "combined_avgs": [
    0.28567637588184197,
    0.45359814383830976,
    0.4466228663833352,
    0.46764362747345334,
    0.38920842095795394,
    0.4386202616199337,
    0.42078011585010333,
    0.3830823406480306,
    0.4530774282658845,
    0.38033631500922394,
    0.43794023519656333,
    0.26038306430116753,
    0.43200133026750376,
    0.42393321305382103,
    0.42450251956311447,
    0.46544248641908337,
    0.4341964820729149,
    0.4321983316454422,
    0.3895966757012043,
    0.433047647489522,
    0.44595388244677137,
    0.3995016039549949,
    0.3769762721057319,
    0.4600185518065021,
    0.35296316629097174,
    0.4366638577313743,
    0.4295679138489442,
    0.4219062883619386,
    0.36194743338472357
  ],
  "gppm": [
    611.9317223664997,
    568.6799936564469,
    564.9069461205288,
    557.4696271353047,
    572.0792637879354,
    580.9199130967598,
    595.184380114961,
    584.3526099129751,
    575.0324431950036,
    597.9049049630913,
    581.0156290282185,
    651.9294190478078,
    557.5464779418206,
    584.7089512522128,
    588.4780927664605,
    559.3343597605387,
    579.1779569212241,
    588.3160234524038,
    597.9902880669026,
    567.5294519373457,
    569.0424832580138,
    610.3154949646696,
    606.2974323805267,
    579.8539122651708,
    606.5507068767316,
    587.8956307012513,
    582.9787943454581,
    563.4926655175569,
    613.287681872235
  ],
  "gppm_normalized": [
    1.4417890545116792,
    1.2800049681440187,
    1.2767431911363467,
    1.254114815955959,
    1.286724766484383,
    1.3067672126692158,
    1.3414144540585677,
    1.3157440213223097,
    1.2970113106484702,
    1.3487156921513943,
    1.304167077007006,
    1.4785121940660726,
    1.2581663399324436,
    1.318709302554819,
    1.319972327315137,
    1.2608874859805697,
    1.303222148333388,
    1.3248783379002373,
    1.3499410899749686,
    1.281202599159895,
    1.2775884189536149,
    1.3637068869730704,
    1.359457728226282,
    1.307192111302889,
    1.3601839454137445,
    1.3207438479213816,
    1.3049339160907256,
    1.265552428825327,
    1.3790545762290447
  ],
  "token_counts": [
    1207,
    434,
    493,
    429,
    428,
    406,
    454,
    432,
    465,
    440,
    395,
    511,
    454,
    459,
    388,
    451,
    431,
    433,
    459,
    475,
    406,
    332,
    377,
    453,
    375,
    404,
    371,
    411,
    404,
    540,
    455,
    437,
    430,
    521,
    409,
    389,
    478,
    408,
    402,
    425,
    502,
    482,
    414,
    404,
    440,
    399,
    395,
    422,
    465,
    395,
    363,
    418,
    397,
    461,
    389,
    405,
    485,
    416
  ],
  "response_lengths": [
    2660,
    2684,
    2586,
    2520,
    3047,
    2346,
    2263,
    2850,
    2271,
    2360,
    2510,
    2731,
    2798,
    2481,
    2321,
    2491,
    2363,
    2381,
    2407,
    2766,
    2235,
    2053,
    2371,
    2300,
    2541,
    2256,
    2402,
    2735,
    2347
  ]
}