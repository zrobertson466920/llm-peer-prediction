{
  "example_idx": 59,
  "reference": "Published as a conference paper at ICLR 2023\n\nHYPERBOLIC DEEP REINFORCEMENT LEARNING\n\nEdoardo Cetin∗† King’s College London\n\nBenjamin P Chamberlain† Charm Therapeutics\n\nMichael M Bronstein† University of Oxford\n\nJonathan J Hunt Twitter Inc.\n\nABSTRACT\n\nIn deep reinforcement learning (RL), useful information about the state is inherently tied to its possible future successors. Consequently, encoding features that capture the hierarchical relationships between states into the model’s latent representations is often conducive to recovering effective policies. In this work, we study a new class of deep RL algorithms that promote encoding such relationships by using hyperbolic space to model latent representations. However, we find that a naive application of existing methodology from the hyperbolic deep learning literature leads to fatal instabilities due to the non-stationarity and variance characterizing common gradient estimators in RL. Hence, we design a new general method that directly addresses such optimization challenges and enables stable end-to-end learning with deep hyperbolic representations. We empirically validate our framework by applying it to popular on-policy and off-policy RL algorithms on the Procgen and Atari 100K benchmarks, attaining near universal performance and generalization benefits. Given its natural fit, we hope this work will inspire future RL research to consider hyperbolic representations as a standard tool.\n\n1\n\nINTRODUCTION\n\nReinforcement Learning (RL) achieved notable milestones in several game-playing and robotics applications (Mnih et al., 2013; Vinyals et al., 2019; Kalashnikov et al., 2018; OpenAI et al., 2019; Lee et al., 2021). However, all these recent advances relied on large amounts of data and domain-specific practices, restricting their applicability in many important real-world contexts (Dulac-Arnold et al., 2019). We argue that these challenges are symptomatic of current deep RL models lacking a proper Figure 1: Hierarchical relationship between prior to efficiently learn generalizable features for states in breakout, visualized in hyperbolic space. control (Kirk et al., 2021). We propose to tackle this issue by introducing hyperbolic geometry to RL, as a new inductive bias for representation learning.\n\nThe evolution of the state in a Markov decision process can be conceptualized as a tree, with the policy and dynamics determining the possible branches. Analogously, the same hierarchical evolution often applies to the most significant features required for decision-making (e.g., presence of bricks, location of paddle/ball in Fig. 1). These relationships tend to hold beyond individual trajectories, making hierarchy a natural basis to encode information for RL (Flet-Berliac, 2019). Consequently, we hypothesize that deep RL models should prioritize encoding precisely hierarchically-structured features to facilitate learning effective and generalizable policies. In contrast, we note that nonevolving features, such as the aesthetic properties of elements in the environment, are often linked with spurious correlations, hindering generalization to new states (Song et al., 2019). Similarly, human cognition also appears to learn representations of actions and elements of the environment by focusing on their underlying hierarchical relationship (Barker & Wright, 1955; Zhou et al., 2018).\n\nHyperbolic geometry (Beltrami, 1868; Cannon et al., 1997) provides a natural choice to efficiently encode hierarchically-structured features. A defining property of hyperbolic space is exponential volume growth, which enables the embedding of tree-like hierarchical data with low distortion using only a few dimensions (Sarkar, 2011). In contrast, the volume of Euclidean spaces only grows\n\n∗E-mail: edoardo.cetin@kcl.ac.uk; † work done while at Twitter Inc.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\npolynomially, requiring high dimensionality to precisely embed tree structures (Matouˇsek, 1990), potentially leading to higher complexity, more parameters, and overfitting. We analyze the properties of learned RL representations using a measure based on the δ-hyperbolicity (Gromov, 1987), quantifying how close an arbitrary metric space is to a hyperbolic one. In line with our intuition, we show that performance improvements of RL algorithms correlate with the increasing hyperbolicity of the discrete space spanned by their latent representations. This result validates the importance of appropriately encoding hierarchical information, suggesting that the inductive bias provided by employing hyperbolic representations would facilitate recovering effective solutions.\n\nHyperbolic geometry has recently been exploited in other areas of machine learning showing substantial performance and efficiency benefits for learning representations of hierarchical and graph data (Nickel & Kiela, 2017; Chamberlain et al., 2017). Recent contributions further extended tools from modern deep learning to work in hyperbolic space (Ganea et al., 2018; Shimizu et al., 2020), validating their effectiveness in both supervised and unsupervised learning tasks (Khrulkov et al., 2020; Nagano et al., 2019; Mathieu et al., 2019). However, most of these approaches showed clear improvements on smaller-scale problems that failed to hold when scaling to higher-dimensional data and representations. Many of these shortcomings are tied to the practical challenges of optimizing hyperbolic and Euclidean parameters end-to-end (Guo et al., 2022). In RL, We show the nonstationarity and high-variance characterizing common gradient estimators exacerbates these issues, making a naive incorporation of existing hyperbolic layers yield underwhelming results.\n\nIn this work, we overcome the aforementioned challenges and effectively train deep RL algorithms with latent hyperbolic representations end-to-end. In particular, we design spectrally-regularized hyperbolic mappings (S-RYM), a simple recipe combining scaling and spectral normalization (Miyato et al., 2018) that stabilizes the learned hyperbolic representations and enables their seamless integration with deep RL. We use S-RYM to build hyperbolic versions of both on-policy (Schulman et al., 2017) and off-policy algorithms (Hessel et al., 2018), and evaluate on both Procgen (Cobbe et al., 2020) and Atari 100K benchmarks (Bellemare et al., 2013). We show that our framework attains near universal performance and generalization improvements over established Euclidean baselines, making even general algorithms competitive with highly-tuned SotA baselines. We hope our work will set a new standard and be the first of many incorporating hyperbolic representations with RL. To this end, we share our implementation at sites.google.com/view/hyperbolic-rl.\n\n2 PRELIMINARIES\n\nIn this section, we introduce the main definitions required for the remainder of the paper. We refer to App. A and (Cannon et al., 1997) for further details about RL and hyperbolic space, respectively.\n\n2.1 REINFORCEMENT LEARNING\n\nThe RL problem setting is traditionally described as a Markov Decision Process (MDP), defined by the tuple (S, A, P, p0, r, γ). At each timestep t, an agent interacts with the environment, observing some state from the state space s ∈ S, executing some action from its action space a ∈ A, and receiving some reward according to its reward function r : S × A (cid:55)→ R. The transition dynamics P : S×A×S (cid:55)→ R and initial state distribution p0 : S (cid:55)→ R determine the evolution of the environment’s state while the discount factor γ ∈ [0, 1) quantifies the agent’s preference for earlier rewards. Agent behavior in RL can be represented by a parameterized distribution function πθ, whose sequential interaction with the environment yields some trajectory τ = (s0, a0, s1, a1, ..., sT , aT ). The agent’s objective is to learn a policy maximizing its expected discounted sum of rewards over trajectories:\n\narg max θ\n\nEτ ∼πθ ,P\n\n(cid:35)\n\nγtr(st, at)\n\n.\n\n(cid:34) ∞ (cid:88)\n\nt=0\n\n(1)\n\nWe differentiate two main classes of RL algorithms with very different optimization procedures based on their different usage of the collected data. On-policy algorithms collect a new set of trajectories with the latest policy for each training iteration, discarding old data. In contrast, off-policy algorithms maintain a large replay buffer of past experiences and use it for learning useful quantities about the environment, such as world models and value functions. Two notable instances from each class are Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Rainbow DQN (Hessel et al., 2018), upon which many recent advances have been built.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2.2 MACHINE LEARNING IN HYPERBOLIC SPACES\n\nA hyperbolic space Hn is an n-dimensional Riemannian manifold with constant negative sectional curvature −c. Beltrami (1868) showed the equiconsistency of hyperbolic and Euclidean geometry using a model named after its re-discoverer, the Poincar ́e ball model. This model equips an ndimensional open ball Bn = {x ∈ Rn : c∥x∥ < 1} of radius 1/ c with a conformal metric of the form Gx = λ2 1−c∥x∥2 is the conformal factor (we will omit the dependence on the curvature −c in our definitions for notation brevity). The geodesic (shortest path) between two points in this metric is a circular arc perpendicular to the boundary with the length given by:\n\nxI, where λx =\n\n√\n\n2\n\nd(x, y) =\n\n1 √\nc\n\ncosh−1\n\n(cid:18)\n\n1 + 2c\n\n∥x − y∥2 (1 − c∥x∥2) (1 − c∥y∥2)\n\n(cid:19)\n\n.\n\n(2)\n\nFrom these characteristics, hyperbolic spaces can be viewed as a continuous analog of trees. In particular, the volume of a ball on Hn grows exponentially w.r.t. its radius. This property mirrors the exponential node growth in trees with constant branching factors. Visually, this makes geodesics between distinct points pass through some midpoint with lower magnitude, analogously to how tree geodesics between nodes (defined as the shortest path in their graph) must cross their closest shared parent (Fig. 2).\n\nKey operations for learning. On a Riemannian manifold, the exponential map expx(v) outputs a unit step along a geodesic starting from point x in the direction of an input velocity v. It thus allows locally treating Hn as Euclidean space. We use the exponential map from the origin of the Poincar ́e ball to map Euclidean input vectors v into Hn, c∥v∥(cid:1)\n\nexp0(v) = tanh (cid:0)√\n\n√\n\nFigure 2: Geodesics on H2 and shortest paths connecting nodes of a tree.\n\n.\n\n(3)\n\nv c∥v∥\n\nFollowing Ganea et al. (2018), we consider the framework of gyrovector spaces (Ungar, 2008) to extend common vector operations to non-Euclidean geometries, and in particular Hn. The most basic such generalized operation is the Mobius addition ⊕ of two vectors, (1 + 2x⟨x, y⟩ + c∥y∥2)x + (1 + c∥x∥2)y 1 + 2c⟨x, y⟩ + c2∥x∥2∥y∥2\n\nx ⊕ y =\n\n(4)\n\n.\n\nNext, consider a Euclidean affine transformation f (x) = ⟨x, w⟩ + b used in typical neural network layers. We can rewrite this transformation as f (x) = ⟨x − p, w⟩ and interpret w, p ∈ Rd as the normal and shift parameters of a hyperplane H = {y ∈ Rd : ⟨y −p, w⟩ = 0} (Lebanon & Lafferty, 2004). This allows us to further rewrite f (x) in terms of the signed distance to the hyperplane H, effectively acting as a weighted ‘decision boundary’:\n\nf (x) = sign (⟨x − p, w⟩) ∥w∥d(x, H). (5) This formulation allows to extend affine transformations to the Poincar ́e ball by considering the signed distance from a gyroplane in Bd (generalized hyperplane) H = {y ∈ Bd : ⟨y⊕−p, w⟩ = 0},\n\nf (x) = sign(⟨x ⊕ −pw⟩)\n\n2∥w∥ (cid:112)1 − c∥p∥2\n\nd(x, H); d(x, H) =\n\n1 √\nc\n\nsinh−1\n\n√\n\n(cid:18) 2\n\nc|⟨x ⊕ −p, w⟩|\n\n(1 − c∥x ⊕ −p∥2)∥w∥\n\n(cid:19)\n\n(6)\n\nSimilarly to recent hyperbolic deep learning work (Mathieu et al., 2019; Guo et al., 2022), we use these operations to parameterize hybrid neural nets: we first process the input data x with standard layers to produce Euclidean vectors xE = fE(x). Then, we obtain hyperbolic representations by applying the exponential map treating xE as a velocity, xH = exp0(xE). Finally, we use linear operations of the form in Eq. 6 to output the set of policy and value scalars, fH (xH ) = {fi(xH )}. We extend this model with a new stabilization (Sec 3.3) with observed benefits beyond RL (App. B).\n\n3 HYPERBOLIC REPRESENTATIONS FOR REINFORCEMENT LEARNING\n\nIn this section, we base our empirical RL analysis on Procgen (Cobbe et al., 2020). This benchmark consists of 16 visual environments, with procedurally-generated random levels. Following common practice, we train agents using exclusively the first 200 levels of each environment and evaluate on the full distribution of levels to assess agent performance and generalization.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: A geodesic space is δ-hyperbolic if every triangle is δ-slim, i.e., each of its sides is entirely contained within a δ-sized region from the other two. We illustrate the necessary δ to satisfy this property for △ABC in a tree triangle (Left), a hyperbolic triangle (Center) and an Euclidean triangle (Right); sharing vertex coordinates. In tree triangles, δtree = 0 since AC always intersects both AB and BC.\n\nFigure 4: Performance and relative δ-hyperbolicity of the final latent representations of a PPO agent.\n\n3.1 THE INHERENT HYPERBOLICITY OF DEEP RL\n\nKey quantities for each state, such as the value and the policy, are naturally related to its possible successors. In contrast, other fixed, non-hierarchical information about the environment such as its general appearance, can often be safely ignored. This divide becomes particularly relevant when considering the problem of RL generalization. For instance, Raileanu & Fergus (2021) found that agents’ can overfit to spurious correlations between the value and non-hierarchical features (e.g., background color) in the observed states. Hence, we hypothesize that effective representations should encode features directly related to the hierarchical state relationships of MDPs.\n\nδ-hyperbolicity. We analyze the representation spaces learned by RL agents, testing whether they preserve and reflect this hierarchical structure. We use the δ-hyperbolicity of a metric space (X, d) (Gromov, 1987; Bonk & Schramm, 2011), which we formally describe in App. A.2. For our usecase, X is δ-hyperbolic if every possible geodesic triangle △xyz ∈ X is δ-slim. This means that for every point on any side of △xyz there exists some point on one of the other sides whose distance is at most δ. In trees, every point belongs to at least two of its sides yielding δ = 0 (Figure 3). Thus, we can interpret δ-hyperbolicity as measuring the deviation of a given metric from an exact tree metric.\n\nThe representations learned by an RL agent from encoding the collected states span some finite subset of Euclidean space xE ∈ XE ⊂ Rn, yielding a discrete metric space XE. To test our hypothesis, we compute the δ-hyperbolicity of XE and analyze how it relates to agent performance. Similarly to (Khrulkov et al., 2020), we compute δ using the efficient algorithm proposed by Fournier et al. (2015). To account for the scale of the representations, we normalize δ by diam(XE), yielding a relative hyperbolicity measure δrel = 2δ/diam(XE) (Borassi et al., 2015), which can span values between 0 (hyperbolic hierarchical tree-like structure) and 1 (perfectly non-hyperbolic spaces).\n\nResults. We train an agent with PPO (Schulman et al., 2017) on four Procgen environments, encoding states from the latest rollouts using the representations before the final linear policy and value heads, xE = fE(s). Hence, we estimate δrel from the space spanned by these latent encodings as training progresses. As shown in Figure 4, δrel quickly drops to low values (0.22 − 0.28) in the first training iterations, reflecting the largest relative improvements in agent performance. Subsequently, in the fruitbot and starpilot environments, δrel further decreases throughout training as the agent recovers high performance with a low generalization gap between the training and test distribution of levels. Instead, in bigfish and dodgeball, δrel begins to increase again after the initial drop, suggesting that the latent representation space starts losing its hierarchical structure. Correspondingly, the agent starts overfitting as test levels performance stagnates while the generalization gap with the training levels performance keeps increasing. We believe these results support our hypothesis, empirically showing the importance of encoding hierarchical features for recovering effective solu-\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Analysis of key statistics for our naive implementations of hyperbolic PPO agents using existing practices to stabilize optimization in hyperbolic space. On the left, we display performance (A.1) and negative entropy (A.2). On the right, we display magnitudes (B.1) and variances (B.2) of the backpropagated gradients.\n\ntions. Furthermore, they suggest that PPO’s poor generalization in some environments is due to the observed tendency of Euclidean latent spaces to encode spurious features that hinder hyperbolicity.\n\nMotivated by our findings, we propose employing hyperbolic geometry to model the latent representations of deep RL models. Representing tree-metrics in Euclidean spaces incurs non-trivial worse-case distortions, growing with the number of nodes at a rate dependent on the dimensionality (Matouˇsek, 1990). This property suggests that it is not possible to encode complex hierarchies in Euclidean space both efficiently and accurately, explaining why some solutions learned by PPO could not maintain their hyperbolicity throughout training. In contrast, mapping the latent representations to hyperbolic spaces of any dimensionality enables encoding features exhibiting a tree-structured relation over the data with arbitrarily low distortion (Sarkar, 2011). Hence, hyperbolic latent representations introduce a different inductive bias for modeling the policy and value function, stemming from this inherent efficiency of specifically encoding hierarchical information (Tifrea et al., 2018).\n\n3.2 OPTIMIZATION CHALLENGES\n\nNaive integration. We test a simple extension to PPO, mapping the latent representations of states s ∈ S before the final linear policy and value heads xE = fE(s) to the Poincar ́e ball with unitary curvature. As described in Section 2, we perform this with an exponential map to produce xH = exp1 0(xE), replacing the final ReLU. To output the value and policy logits, we then finally perform a set of affine transformations in hyperbolic space, π(s), V (s) = fH (xH ) = {f 1 i=0. We also consider a clipped version of this integration, following the recent stabilization practice from Guo et al. (2022), which entails clipping the magnitude of the latent representations to not exceed unit norm. We initialize the weights of the last two linear layers in both implementations to 100× smaller values to start training with low magnitude latent representations, which facilitates the network first learning appropriate angular layouts (Nickel & Kiela, 2017; Ganea et al., 2018).\n\ni (xH )}|A|\n\nFigure 6: PPO model with an hyperbolic latent space, extending the architecture from Espeholt et al. (2018).\n\nResults. We analyze this naive hyperbolic PPO implementation in Figure 5. As shown in part (A.1), performance is generally underwhelming, lagging considerably behind the performance of standard PPO. While applying the clipping strategy yields some improvements, its results are still considerably inferior on the tasks where Euclidean embeddings appear to already recover effective representations (e.g. starpilot). In part (A.2) we visualize the negated entropy of the different PPO agents. PPO’s policy optimization objective includes both a reward maximization term, which requires an auxiliary estimator, and an entropy bonus term that can instead be differentiated exactly and optimized end-to-end. Its purpose is to push PPO agents to explore if they struggle to optimize performance with the current data.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Analysis of hyperbolic PPO with the proposed S-RYM stabilization. We visualize performance (A) and gradient magnitudes (B) as compared to the original Euclidean and the naive hyperbolic baselines.\n\nWe note that the Hyperbolic PPO agents take significantly longer to reach higher levels of entropy in the initial training phases and are also much slower to reduce their entropy as their performance improves. These results appear to indicate the presence of optimization challenges stemming from end-to-end RL training with hyperbolic representations. Therefore, we turn our attention to analyzing the gradients in our hyperbolic models. In part (B.1), we visualize the magnitude of the gradients both as backpropagated from the final representations and to the convolutional encoder. In part (B.2), we also visualize the variance of the same gradients with respect to the different input states in a minibatch. We find that hyperbolic PPO suffers from a severe exploding gradients problem, with both magnitudes and variances being several orders of magnitude larger than the Euclidean baseline. Similar instabilities have been documented by much recent literature, as described in App. B. Yet, in the RL case, common stabilization techniques such as careful initialization and clipping are visibly insufficient, resulting in ineffective learning and inferior agent performance.\n\n3.3 STABILIZING HYPERBOLIC REPRESENTATIONS\n\nWe hypothesize that the high variance and non-stationarity characterizing RL are the main cause of the observed optimization challenges of this naive hyperbolic PPO. implementation. Initialization and clipping have been designed for stationary ML applications with fixed dataset and targets. In these settings, regularizing the initial learning iterations enables the model to find appropriate angular layouts of the representations for the underlying fixed loss landscape. Without appropriate angular layouts, useful representations become hard to recover due to the highly non-convex spectrum of hyperbolic neural networks, resulting in failure modes with low performance (Ganea et al., 2018; L ́opez & Strube, 2020). We can intuitively see how this reliance is likely incompatible with the RL setting, where the trajectory data and loss landscape can change significantly throughout training, making early angular layouts inevitably suboptimal. We believe this is further exacerbated by the high variance gradients already characterizing policy gradient optimization (Sutton & Barto, 2018) which facilitate entering unstable learning regimes that can lead to our observed failure modes.\n\nSpectral norm. Another sub-field of ML dealing with non-stationarity and brittle optimization is generative modeling with adversarial networks (GANs) (Goodfellow et al., 2014). In GAN training, the generated data and discriminator’s parameters constantly evolve, making the loss landscape highly non-stationary as in the RL setting. Furthermore, the adversarial nature of the optimization makes it very brittle to exploding and vanishing gradients instabilities which lead to common failure modes (Arjovsky & Bottou, 2017; Brock et al., 2018). In this parallel literature, spectral normalization (SN) (Miyato et al., 2018) is a popular stabilization practice whose success made it ubiquitous in modern GAN implementations. Recent work (Lin et al., 2021) showed that a reason for its surprising effectiveness comes from regulating both the magnitude of the activations and their respective gradients very similarly to LeCun initialization (LeCun et al., 2012). Furthermore, when applied to the discriminator model, SN’s effects appear to persist throughout training, while initialization strategies tend to only affect the initial iterations. In fact, they also show that ablating SN from GAN training empirically results in exploding gradients and degraded performance, closely resembling our same observed instabilities. We provide details about GANs and SN in App. A.3.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Performance comparison for the considered versions of PPO full Procgen benchmark\n\nTask\\Algorithm\n\nPPO\n\nLevels distribution\n\ntrain/test\n\nPPO + data aug.\n\ntrain/test\n\nPPO + S-RYM\n\ntrain/test\n\nPPO + S-RYM, 32 dim.\n\ntrain/test\n\nbigfish bossfight caveflyer chaser climber coinrun dodgeball fruitbot heist jumper leaper maze miner ninja plunder starpilot\n\n3.71±1 1.46±1 12.43±4 (+235%) 13.07±2 (+797%) 13.27±2 (+258%) 12.20±2 (+737%) 20.58±5 (+455%) 16.57±2 (+1037%) 8.18±1 7.04±2 7.01±1 5.86±1 6.58±2 5.89±1 8.66±2 5.11±1 9.50±0 8.25±0 5.07±1 1.87±1 30.10±2 26.33±2 7.42±1 2.92±1 8.86±1 6.14±1 4.86±2 4.36±2 9.25±0 6.50±0 12.95±0 9.28±1 7.62±1 6.50±1 6.92±2 6.06±3\n\n3.38±1 (-59%) 2.96±1 (-58%) 6.08±1 (-13%) 4.89±1 (-16%) 2.14±0 (-67%) 2.18±0 (-63%) 7.61±1 (-12%) 5.74±2 (+12%) 8.40±1 (-12%) 9.00±1 (+9%) 3.94±1 (-22%) 3.20±1 (+71%) 27.56±3 (-8%) 27.98±1 (+6%) 4.20±1 (-43%) 3.60±0 (+23%) 7.70±1 (-13%) 5.70±0 (-7%) 6.80±1 (+40%) 7.00±1 (+61%) 8.50±1 (-8%) 7.10±1 (+9%) 9.81±0 (-24%) 9.36±2 (+1%) 6.90±1 (-10%) 4.50±1 (-31%) 5.13±0 (-26%) 4.96±1 (-18%)\n\n9.26±1 (+13%) 9.02±1 (+28%) 6.38±1 (-9%) 5.20±1 (-11%) 9.04±1 (+37%) 7.32±1 (+24%) 8.32±1 (-4%) 7.28±1 (+43%) 9.70±0 (+2%) 9.20±0 (+12%) 7.74±2 (+53%) 7.14±1 (+281%) 29.15±1 (-3%) 29.51±1 (+12%) 6.40±1 (-14%) 3.60±1 (+23%) 8.50±0 (-4%) 6.10±1 (-1%) 7.70±1 (+59%) 7.00±1 (+61%) 9.20±0 (-1%) 7.10±1 (+9%) 12.94±0 (+0%) 9.86±1 (+6%) 7.50±1 (-2%) 5.60±1 (-14%) 7.35±1 (+6%) 6.68±0 (+10%) 41.48±4 (+36%) 38.27±5 (+44%)\n\n8.61±1 (+5%) 8.14±1 (+16%) 6.15±1 (-12%) 5.15±1 (-12%) 6.60±2 (+0%) 7.82±1 (+33%) 8.91±1 (+3%) 6.64±1 (+30%) 9.30±1 (-2%) 8.40±0 (+2%) 7.10±1 (+40%) 6.52±1 (+248%) 30.43±1 (+1%) 27.97±3 (+6%) 5.40±1 (-27%) 2.70±1 (-7%) 9.00±1 (+2%) 6.70±1 (+9%) 8.00±1 (+65%) 7.30±1 (+68%) 9.50±0 (+3%) 6.10±1 (-6%) 12.09±1 (-7%) 10.08±1 (+9%) 6.50±1 (-15%) 6.10±1 (-6%) 7.26±1 (+5%) 6.87±1 (+13%) 37.08±3 (+22%) 41.22±3 (+55%)\n\n30.50±5 26.57±5 43.43±7 (+42%) 32.41±3 (+22%)\n\nAverage norm. score Median norm. score # Env. improvements\n\n0.5614 0.3476 0.6085 0.3457 0/16 0/16\n\n0.4451 (-21%) 0.3536 (+2%) 0.5262 (-14%) 0.3312 (-4%)\n\n3/16 10/16\n\n0.5846 (+4%) 0.4490 (+29%) 0.6055 (+0%) 0.4832 (+40%) 11/16 12/16\n\n0.6326 (+13%) 0.4730 (+36%) 0.6527 (+7%) 0.4705 (+36%) 8/16 13/16\n\nFigure 8: Performance comparison for the considered versions of PPO agents with Euclidean and hyperbolic latent representations, increasingly lowering the number of training levels. S-RYM. Inspired by these connections, we propose to counteract the optimization challenges in RL and hyperbolic representations with SN. Our implementation differs from its usual application for GANs in two main ways. First, we apply SN to all layers in the Euclidean sub-network (fE), as the observed instabilities already occur in the gradients from the hyperbolic representations, but leave the final linear hyperbolic layer (fH ) unregularized to avoid further limiting expressivity. Second, we propose to scale the latent representations to account for their dimensionality. In particular, modeling xE ∈ Rn by an independent Gaussian, the magnitude of the representations follows some scaled Chi distribution ∥xE∥ ∼ χn, which we can reasonably approximate with E[∥xE∥] = E[χn] ≈ n. Therefore, we propose to rescale the output of fE by 1/ n, such that modifying the dimensionality of the representations should not significantly affect their magnitude before mapping them Hn. We call this general stabilization recipe spectrally-regularized hyperbolic mappings (S-RYM).\n\n√\n\n√\n\nResults. As shown in Figure 7, integrating S-RYM with our hyperbolic RL agents appears to resolve their optimization challenges and considerably improve the Euclidean baseline’s performance (A). To validate that the performance benefits are due to the hyperbolic geometry of the latent space, we also evaluate a baseline using SN and rescaling in Euclidean space, which fails to attain consistent improvements. Furthermore, S-RYM maintains low gradient magnitudes (B), confirming its effectiveness to stabilize training. In App. E.1, we also show that SN and rescaling are both crucial for S-RYM. Thus, in the next section, we evaluate hyperbolic deep RL on a large-scale, analyzing its efficacy and behavior across different benchmarks, RL algorithms, and training conditions.\n\n4 EXTENSIONS AND EVALUATION\n\nTo test the generality of our hyperbolic deep RL framework, in addition to the on-policy PPO we also integrate it with the off-policy Rainbow DQN algorithm (Hessel et al., 2018). Our implementations use the same parameters and models specified in prior traditional RL literature, without any additional tuning. Furthermore, in addition to the full Procgen benchmark (16 envs.) we also evaluate on\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 9: Absolute difference in normalized performance (Y-axis) and relative improvements (Above bars) from integrating hyperbolic representations with S-RYM onto our Rainbow implementation.\n\nthe popular Atari 100K benchmark (Bellemare et al., 2013; Kaiser et al., 2020) (26 envs.), repeating for 5 random seeds. We provide all details about benchmarks and implementations in App. C.\n\nGeneralization on Procgen. Given the documented representation efficiency of hyperbolic space, we evaluate our hyperbolic PPO implementation also reducing the dimensionality of the final representation to 32 (see App. E.2), with relative compute and parameter efficiency benefits. We compare our regularized hyperbolic PPO with using data augmentations, a more traditional way of encoding inductive biases from inducing invariances. We consider random crop augmentations from their popularity and success in modern RL. As shown in Table 1, our hyperbolic PPO implementation with S-RYM appears to yield conspicuous performance gains on most of the environments. At the same time, reducing the size of the representations provides even further benefits with significant improvements in 13/16 tasks. In contrast, applying data augmentations yields much lower and inconsistent gains, even hurting on some tasks where hyperbolic RL provides considerable improvements (e.g. bossfight). We also find that test performance gains do not always correlate with gains on the specific 200 training levels, yielding a significantly reduced generalization gap for the hyperbolic agents. We perform the same experiment but apply our hyperbolic deep RL framework to Rainbow DQN with similar results, also obtaining significant gains in 13/16 tasks, as reported in App. D.1.\n\nWe also evaluate the robustness of our PPO agents to encoding spurious features, only relevant for the training levels. In particular, we examine tasks where PPO tends to perform well and consider lowering the training levels from 200 to 100, 50, and 25. As shown in Figure 8, the performance of PPO visibly drops at each step halving the number of training levels, suggesting that the Euclidean representations overfit and lose their original efficacy. In contrast, hyperbolic PPO appears much more robust, still surpassing the original PPO results with only 100 training levels in fruitbot and 50 in starpilot. While also applying data augmentation attenuates the performance drops, its effects appear more limited and inconsistent, providing almost null improvements for starpilot.\n\nTable 2: Aggregate results on Atari 100K\n\nSample-efficiency on Atari 100K. We focus on the performance of our hyperbolic Rainbow DQN implementation, as the severe data limitations of this benchmark make PPO and other on-policy algorithms impractical. We show the absolute and relative per-environment performance changes from our hyperbolic RL framework in Figure 9, and provide aggregate statistics in Table 2. Also on this benchmark, the exact same hyperbolic deep RL framework provides consistent and significant benefits. In particular, we record improvements on 22/26 Atari environments over the Euclidean baseline, almost doubling the final human normalized score.\n\nHuman norm. mean Human norm. median Super human scores\n\nMetric\\Algorithm Rainbow Rainbow + S-RYM\n\n0.686 (+93%) 0.366 (+41%) 5\n\n0.353 0.259 2\n\nConsiderations and comparisons. Our results empirically validate that introducing hyperbolic representations to shape the prior of deep RL models is both remarkably general and effective. We record almost universal improvements on two fundamentally different RL algorithms, considering both generalizations to new levels from millions of frames (Procgen) and to new experiences from only 2hrs of total play time (Atari 100K). Furthermore, our hyperbolic RL agents outperform the scores reported in most other recent advances, coming very close to the current SotA algorithms which incorporate different expensive and domain-specialized auxiliary practices (see App. D.2D.3). Our approach is also orthogonal to many of these advances and appears to provide compatible and complementary benefits (see App. E.3). Taken together, we believe these factors show the great potential of our hyperbolic framework to become a standard way of parameterizing deep RL models.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: Visualization of 2-dimensional hyperbolic embeddings in the bigfish environment as we progress through a trajectory, encoding states from either policy transitions or random transitions (details in App. D.4).\n\nRepresentations interpretation. We train our hyperbolic PPO agent with only 2-dimensional representations, which still remarkably provide concrete generalization benefits over Euclidean PPO (App. D.4). Then, we analyze how these representations evolve within trajectories, mapping them on the Poincar ́e disk and visualizing the corresponding states. We observe a recurring cyclical behavior, where the magnitude of the representations monotonically increases within subsets of the trajectory as more obstacles/enemies appear. We show this in Fig. 10 and Fig. 12, comparing the representations of on-policy states sampled at constant intervals with trajectory deviations from executing random behavior. We observe the representations form tree-like structures, with the magnitudes in the on-policy states growing in the direction of the Value function’s gyroplane’s normal. This intuitively reflects that as new elements appear the agent recognizes a larger opportunity for rewards, yet, requiring a finer level of control as distances to the policy gyroplanes will also grow exponentially, reducing entropy. Instead, following random deviations, magnitudes grow in directions orthogonal to the Value gyroplane’s normal. This still reflects the higher precision required for optimal decision-making, but also the higher uncertainty to obtain future rewards from worse states.\n\n5 RELATED WORK\n\nGeneralization is a key open problem in RL (Kirk et al., 2021). End-to-end training of deep models with RL objectives has been shown prone to overfitting from spurious features only relevant in the observed transitions (Song et al., 2019; Bertran et al., 2020). To address this, prior work considered different data augmentation strategies (Laskin et al., 2020b; Yarats et al., 2021a; Cobbe et al., 2019), and online adaption methods on top to alleviate engineering burdens (Zhang & Guo, 2021; Raileanu et al., 2020). Alternative approaches have been considering problem-specific properties of the environment (Zhang et al., 2020; Raileanu & Fergus, 2021), auxiliary losses (Laskin et al., 2020a; Schwarzer et al., 2020), and frozen pre-trained layers (Yarats et al., 2021b; Stooke et al., 2021). Instead, we propose to encode a new inductive bias making use of the geometric properties of hyperbolic space, something orthogonal and likely compatible with most such prior methods.\n\nWhile hyperbolic representations found recent popularity in machine learning, there have not been notable extensions for deep RL (Peng et al., 2021). Most relatedly, Tiwari & Prannoy (2018) proposed to produce hyperbolic embeddings of the state space of tabular MDPs to recover options (Sutton et al., 1999). Yet, they did not use RL for learning, but fixed data and a supervised loss based on the co-occurrence of states, similarly to the original method by Nickel & Kiela (2017).\n\n6 DISCUSSION AND FUTURE WORK\n\nIn this work, we introduce hyperbolic geometry to deep RL. We analyze training agents using latent hyperbolic representations and propose spectrally-regularized hyperbolic mappings, a new stabilization strategy that overcomes the observed optimization instabilities. Hence, we apply our framework to obtain hyperbolic versions of established on-policy and off-policy RL algorithms, which we show substantially outperform their Euclidean counterparts in two popular benchmarks. We provide numerous results validating that hyperbolic representations provide deep models with a more suitable prior for control, with considerable benefits for generalization and sample-efficiency. We share our implementation to facilitate future RL advances considering hyperbolic space as a new, general tool.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nWe proposed to provide deep RL models with a more suitable prior for learning policies, using hyperbolic geometry. In terms of carbon footprint, our implementation does not introduce additional compute costs for training, and even appears to perform best with more compact representation sizes. Consequently, given the nature of our contribution, its ethical implications are bound to the implications of advancing the RL field. In this regard, as autonomous agents become more applicable, poor regulation and misuse may cause harm. Yet, we believe these concerns are currently out-weighted by the field’s significant positive potential to advance human flourishing.\n\nREPRODUCIBILITY STATEMENT\n\nWe provide detailed descriptions of our integration of hyperbolic space, experimental setups, and network architectures in Section 3 and also Appendix B. We provide all details, including a full list of hyper-parameters, in Appendix C. We currently shared an anonymized version of our code to reproduce the main experiments in the supplementary material. We shared our open-source implementation to facilitate future extensions.\n\nREFERENCES\n\nMartin Arjovsky and L ́eon Bottou. Towards principled methods for training generative adversarial\n\nnetworks. In International Conference on Learning Representations, 2017.\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein gans.\n\narXiv preprint\n\narXiv:1701.07875, 2017.\n\nRoger G Barker and Herbert F Wright. Midwest and its children: The psychological ecology of an\n\namerican town. 1955.\n\nGary B ́ecigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. In Inter-\n\nnational Conference on Learning Representations, 2019.\n\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253–279, 2013.\n\nMarc G Bellemare, Will Dabney, and R ́emi Munos. A distributional perspective on reinforcement\n\nlearning. In International Conference on Machine Learning, pp. 449–458. PMLR, 2017.\n\nRichard Bellman. A markovian decision process. Journal of mathematics and mechanics, pp. 679–\n\n684, 1957.\n\nEugenio Beltrami. Teoria fondamentale degli spazii di curvatura costante. Annali di Matematica\n\nPura ed Applicata (1867-1897), 2(1):232–255, 1868.\n\nMartin Bertran, Natalia Martinez, Mariano Phielipp, and Guillermo Sapiro.\n\nInstance-based generalization in reinforcement learning. Advances in Neural Information Processing Systems, 33: 11333–11344, 2020.\n\nJohan Bjorck, Carla P Gomes, and Kilian Q Weinberger. Towards deeper deep reinforcement learn-\n\ning. In Advances in neural information processing systems, 2021.\n\nMario Bonk and Oded Schramm. Embeddings of gromov hyperbolic spaces. In Selected Works of\n\nOded Schramm, pp. 243–284. Springer, 2011.\n\nSilvere Bonnabel. Stochastic gradient descent on riemannian manifolds.\n\nIEEE Transactions on\n\nAutomatic Control, 58(9):2217–2229, 2013.\n\nMichele Borassi, Alessandro Chessa, and Guido Caldarelli. Hyperbolicity measures democracy in\n\nreal-world networks. Physical Review E, 92(3):032812, 2015.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\n\nimage synthesis. In International Conference on Learning Representations, 2018.\n\nJames W Cannon, William J Floyd, Richard Kenyon, Walter R Parry, et al. Hyperbolic geometry.\n\nFlavors of geometry, 31(59-115):2, 1997.\n\nPablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare. Dopamine: A Research Framework for Deep Reinforcement Learning. 2018. URL http: //arxiv.org/abs/1812.06110.\n\nEdoardo Cetin, Philip J Ball, Stephen Roberts, and Oya Celiktutan. Stabilizing off-policy deep reinforcement learning from pixels. In International Conference on Machine Learning, pp. 2784– 2810. PMLR, 2022.\n\nBenjamin P Chamberlain, James Clough, and Marc P Deisenroth. Neural embeddings of graphs in\n\nhyperbolic space. arXiv preprint arXiv:1705.10359, 2017.\n\nKarl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. In International Conference on Machine Learning, pp. 1282–1289. PMLR, 2019.\n\nKarl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pp. 2048– 2056. PMLR, 2020.\n\nKarl W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic policy gradient. In Interna-\n\ntional Conference on Machine Learning, pp. 2020–2027. PMLR, 2021.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nGabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement\n\nlearning. Machine Learning, 2019.\n\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pp. 1407–1416. PMLR, 2018.\n\nYannis Flet-Berliac. The promise of hierarchical reinforcement learning. The Gradient, 9, 2019.\n\nMeire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. Noisy networks for exploration. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rywHCPkAW.\n\nHerv ́e Fournier, Anas Ismail, and Antoine Vigneron. Computing the gromov hyperbolicity of a\n\ndiscrete metric space. Information Processing Letters, 115(6-8):576–579, 2015.\n\nOctavian Ganea, Gary B ́ecigneul, and Thomas Hofmann. Hyperbolic neural networks. Advances in\n\nneural information processing systems, 31, 2018.\n\nFlorin Gogianu, Tudor Berariu, Mihaela C Rosca, Claudia Clopath, Lucian Busoniu, and Razvan Pascanu. Spectral normalisation for deep reinforcement learning: an optimisation perspective. In International Conference on Machine Learning, pp. 3734–3744. PMLR, 2021.\n\nGene H Golub and Henk A Van der Vorst. Eigenvalue computation in the 20th century. Journal of\n\nComputational and Applied Mathematics, 123(1-2):35–65, 2000.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672–2680, 2014.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nMikhael Gromov. Hyperbolic groups. In Essays in group theory, pp. 75–263. Springer, 1987.\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in neural information processing systems, pp. 5767–5777, 2017a.\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. Advances in neural information processing systems, 30, 2017b.\n\nYunhui Guo, Xudong Wang, Yubei Chen, and Stella X Yu. Clipped hyperbolic classifiers are superIn Proceedings of the IEEE/CVF Conference on Computer Vision and\n\nhyperbolic classifiers. Pattern Recognition, pp. 11–20, 2022.\n\nAnupam Gupta. Embedding tree metrics into low dimensional euclidean spaces. In Proceedings of\n\nthe thirty-first annual ACM symposium on Theory of computing, pp. 694–700, 1999.\n\nHado Hasselt. Double q-learning. Advances in neural information processing systems, 23:2613–\n\n2621, 2010.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\nRiashat Islam, Peter Henderson, Maziar Gomrokchi, and Doina Precup. Reproducibility of arXiv preprint\n\nlearning tasks for continuous control.\n\nbenchmarked deep reinforcement arXiv:1708.04133, 2017.\n\nMinqi Jiang, Edward Grefenstette, and Tim Rockt ̈aschel. Prioritized level replay. In International\n\nConference on Machine Learning, pp. 4940–4950. PMLR, 2021.\n\nLukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osi ́nski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, and Henryk Michalewski. Model based reinforcement learning for Atari. In International Conference on Learning Representations, 2020.\n\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018.\n\nValentin Khrulkov, Leyla Mirvakhabova, Evgeniya Ustinova, Ivan Oseledets, and Victor Lempitsky. Hyperbolic image embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6418–6428, 2020.\n\nKacper Piotr Kielak. Do recent advancements in model-based deep reinforcement learning really\n\nimprove data efficiency? 2019.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\n\nConference on Learning Representations, 2015.\n\nRobert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rockt ̈aschel. A survey of generalisation in\n\ndeep reinforcement learning. arXiv preprint arXiv:2111.09794, 2021.\n\nMax Kochurov, Rasul Karimov, and Serge Kozlukov. Geoopt: Riemannian optimization in pytorch,\n\n2020.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representa-\n\ntions for reinforcement learning. pp. 5639–5650, 2020a.\n\nMisha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. Advances in neural information processing systems, 33: 19884–19895, 2020b.\n\nGuy Lebanon and John Lafferty. Hyperplane margin classifiers on the multinomial manifold. In\n\nProceedings of the twenty-first international conference on Machine learning, pp. 66, 2004.\n\nYann A LeCun, L ́eon Bottou, Genevieve B Orr, and Klaus-Robert M ̈uller. Efficient backprop. In\n\nNeural networks: Tricks of the trade, pp. 9–48. Springer, 2012.\n\nAlex X Lee, Coline Manon Devin, Yuxiang Zhou, Thomas Lampe, Konstantinos Bousmalis, Jost Tobias Springenberg, Arunkumar Byravan, Abbas Abdolmaleki, Nimrod Gileadi, David Khosid, et al. Beyond pick-and-place: Tackling robotic stacking of diverse shapes. In 5th Annual Conference on Robot Learning, 2021.\n\nZinan Lin, Vyas Sekar, and Giulia Fanti. Why spectral normalization stabilizes gans: Analysis and\n\nimprovements. Advances in neural information processing systems, 34:9625–9638, 2021.\n\nFederico L ́opez and Michael Strube. A fully hyperbolic neural model for hierarchical multi-class classification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 460–475, 2020.\n\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=Skq89Scxx.\n\nMarlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523–562, 2018.\n\nShie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance approximation\n\nin value function estimates. Management Science, 53(2):308–322, 2007.\n\nEmile Mathieu, Charline Le Lan, Chris J Maddison, Ryota Tomioka, and Yee Whye Teh. Continuous hierarchical representations with poincar ́e variational auto-encoders. In Advances in neural information processing systems, volume 32, 2019.\n\nJiˇr ́ı Matouˇsek. Bi-lipschitz embeddings into low-dimensional euclidean spaces. Commentationes\n\nMathematicae Universitatis Carolinae, 31(3):589–600, 1990.\n\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization\n\nfor generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\nSharada Mohanty, Jyotish Poonganam, Adrien Gaidon, Andrey Kolobov, Blake Wulfe, Dipam Chakraborty, Grazvydas Semetulskis, Joao Schapke, Jonas Kubilius, Jurgis Pasukonis, et al. Measuring sample efficiency and generalization in reinforcement learning benchmarks: Neurips 2020 procgen benchmark. arXiv preprint arXiv:2103.15332, 2021.\n\nYoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fujita, and Masanori Koyama. A wrapped normal distribution on hyperbolic space for gradient-based learning. In International Conference on Machine Learning, pp. 4693–4702. PMLR, 2019.\n\nMaximillian Nickel and Douwe Kiela. Poincar ́e embeddings for learning hierarchical representa-\n\ntions. Advances in neural information processing systems, 30, 2017.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nOpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik’s cube with a robot hand. CoRR, abs/1910.07113, 2019.\n\nMark D Pendrith, Malcolm RK Ryan, et al. Estimator variance in reinforcement learning: Theoretical problems and practical solutions. University of New South Wales, School of Computer Science and Engineering, 1997.\n\nWei Peng, Tuomas Varanka, Abdelrahman Mostafa, Henglin Shi, and Guoying Zhao. Hyperbolic deep neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\n\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\nRoberta Raileanu and Rob Fergus. Decoupling value and policy for generalization in reinforcement\n\nlearning. In International Conference on Machine Learning, pp. 8787–8798. PMLR, 2021.\n\nRoberta Raileanu, Max Goldstein, Denis Yarats,\n\nmatic data augmentation for generalization in deep reinforcement learning. arXiv:2006.12862, 2020.\n\nIlya Kostrikov, and Rob Fergus. AutoarXiv preprint\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.\n\nRik Sarkar. Low distortion delaunay embedding of trees in hyperbolic plane.\n\nIn International\n\nSymposium on Graph Drawing, pp. 355–366. Springer, 2011.\n\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In\n\nInternational Conference on Learning Representations, 2016.\n\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. HigharXiv preprint\n\ndimensional continuous control using generalized advantage estimation. arXiv:1506.02438, 2015.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347.\n\nMax Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bach-\n\nman. Data-efficient reinforcement learning with self-predictive representations. 2020.\n\nRyohei Shimizu, Yusuke Mukuta, and Tatsuya Harada. Hyperbolic neural networks++. arXiv\n\npreprint arXiv:2006.08210, 2020.\n\nOndrej Skopek, Octavian-Eugen Ganea, and Gary B ́ecigneul. Mixed-curvature variational autoen-\n\ncoders. In International Conference on Learning Representations, 2019.\n\nXingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, and Behnam Neyshabur. Observational overfitting in reinforcement learning. In International Conference on Learning Representations, 2019.\n\nAdam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from reinforcement learning. In International Conference on Machine Learning, pp. 9870–9879. PMLR, 2021.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nRichard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181– 211, 1999.\n\nAlexandru Tifrea, Gary Becigneul, and Octavian-Eugen Ganea. Poincare glove: Hyperbolic word\n\nembeddings. In International Conference on Learning Representations, 2018.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nSaket Tiwari and M Prannoy. Hyperbolic embeddings for learning options in hierarchical reinforce-\n\nment learning. arXiv preprint arXiv:1812.01487, 2018.\n\nAbraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis Lectures\n\non Mathematics and Statistics, 1(1):1–194, 2008.\n\nHado P Van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in rein-\n\nforcement learning? Advances in Neural Information Processing Systems, 32, 2019.\n\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha ̈el Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n\nKaixin Wang, Bingyi Kang, Jie Shao, and Jiashi Feng. Improving generalization in reinforcement learning with mixture regularization. Advances in Neural Information Processing Systems, 33: 7968–7978, 2020.\n\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International conference on machine learning, pp. 1995–2003. PMLR, 2016.\n\nCathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M Bayen, Sham Kakade, Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent factorized baselines. In International Conference on Learning Representations, 2018.\n\nOmry Yadan. Hydra - a framework for elegantly configuring complex applications. Github, 2019.\n\nURL https://github.com/facebookresearch/hydra.\n\nDenis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=GY6-6sTvGaf.\n\nDenis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 10674–10681, 2021b.\n\nAmy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup. Invariant causal prediction for block mdps. In International Conference on Machine Learning, pp. 11214–11224. PMLR, 2020.\n\nHanping Zhang and Yuhong Guo. Generalization of reinforcement learning with policy-aware ad-\n\nversarial data augmentation. arXiv preprint arXiv:2106.15587, 2021.\n\nYuansheng Zhou, Brian H Smith, and Tatyana O Sharpee. Hyperbolic geometry of the olfactory\n\nspace. Science advances, 4(8), 2018.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA EXTENDED BACKGROUND\n\nA.1 RL ALGORITHMS DESCRIPTIONS\n\nContinuing from section 2.1, we provide an overview of standard RL definitions and the deep RL algorithms we use in this work.\n\nTwo important functions in RL are the value function and the action-value function (also called the Q function). These quantify, for policy π, the expected sum of discounted future rewards given any initial fixed state or state-action pair, respectively:\n\nV π(st) = Eat,st+1,at+1,···∼πθ,P\n\n(cid:34) ∞ (cid:88)\n\nt′=0\n\nγt′\n\n(cid:35)\n\nr(st+t′, at+t′)\n\n,\n\nQπ(st, at) = r(st, at) + γEst+1∼P [V π(st+1)] .\n\n(7)\n\nRelatedly, the advantage function Aπ(st, at) = Qπ(st, at) − V π(st) quantifies the expected improvement from executing any given action at from st rather than following the policy. These functions summarize the future evolution of an MDP and are often parameterized and learned auxiliary to or even in-place of the policy model.\n\nOn-policy methods. Modern on-policy RL algorithms collect a new set of trajectories at each iteration with the current policy, discarding old data. They use these trajectories to learn the current policy’s value function and recover a corresponding advantage function from the observed MonteCarlo returns, using techniques such as the popular Generalized Advantage Estimator (GAE) (Schulman et al., 2015). The estimated advantages AGAE are then used to compute the policy gradient and update the policy, maximizing the probability of performing the best-observed actions (Sutton & Barto, 2018). Since the values of AGAE are based on a limited set of trajectories, on-policy methods generally suffer from high-variance targets and gradients (Pendrith et al., 1997; Mannor et al., 2007; Wu et al., 2018). Proximal Policy Optimization (PPO) (Schulman et al., 2017) is one of the most established on-policy algorithms that attenuates these issues by taking conservative updates, restricting the policy update from making larger than ε changes to the probability of executing any individual action. PPO considers the ratio between the updated and old policy probabilities Rπ(at|st) = πθ(at|st)\n\nπold(at|st) to optimize a pessimistic clipped objective of the form:\n\nmin{Rπ(at|st)AGAE(st, at), clip(Rπ(at|st), 1 − ε, 1 + ε)AGAE(st, at)}.\n\n(8)\n\nAs mentioned in the main text, PPO also includes a small entropy bonus to incentivize exploration and improve data diversity. This term can be differentiated and optimized without any estimator since we have full access to the policy model and its output logits, independently of the collected data.\n\nOff-policy methods. In contrast, off-policy algorithms generally follow a significantly different optimization approach. They store many different trajectories collected with a mixture of old policies in a large replay buffer, B. They use this data to directly learn the Q function for the optimal greedy policy with a squared loss based on the Bellman backup (Bellman, 1957):\n\nE(st,at,st+1,rt)∈B\n\n(cid:104)\n\nQ(st, at) −\n\n(cid:16)\n\nrt + max\n\na′\n\nQ(st+1, a′)\n\n(cid:17)(cid:105)2\n\n(9)\n\n(Bellman, 1957). Agent behavior is then implicitly defined by the epsilon-greedy policy based on the actions with the highest estimated Q values. We refer to the deep Q-networks paper (Mnih et al., 2013) for a detailed description of the seminal DQN algorithm. Rainbow DQN (Hessel et al., 2018) is a modern popular extension that introduces several auxiliary practices from proposed orthogonal improvements, which they show provide compatible benefits. In particular, they use n-step returns (Sutton & Barto, 2018), prioritized experience replay (Schaul et al., 2016), double Q-learning (Hasselt, 2010), distributional RL (Bellemare et al., 2017), noisy layers (Fortunato et al., 2018), and a dueling network architecture (Wang et al., 2016).\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nA.2\n\nδ-HYPERBOLICITY\n\nδ-hyperbolicity was introduced by Gromov (1987) as a criterion to quantify how hyperbolic a metric space (X, d) is. We can express δ-hyperbolicity in terms of the Gromov product, defined for x, y ∈ X at some base point r ∈ X as measuring the defect from the triangle inequality:\n\n(x|y)r =\n\n1 2\n\n(d(x, r) + d(r, y) − d(x, y)).\n\n(10)\n\nThen, X is δ-hyperbolic if for all base points r ∈ X and for any three points x, y, z ∈ X the Gromov product between x and y is lower than the minimum Gromov product of the other pairs by at most some slack variable δ:\n\n(x|y)r ≥ min((x|y)r, (x|y)r) − δ. In our case (a complete finite-dimensional path-connected Riemannian manifold, which is a geodesic metric space), δ-hyperbolicity means that for every point on one of the sides of a geodesic triangle △xyz, there exists some other point on one of the other sides whose distance is at most δ, or in other words, geodesic triangles are δ-slim. In trees, the three sides of a triangle must all intersect at some midpoint (Figure 3). Thus, every point belongs to at least two of its sides yielding δ = 0. Thus the δ-hyperbolicity can be interpreted as measuring the deviation of a given metric from an exact tree metric.\n\n(11)\n\nA.3 GENERATIVE ADVERSARIAL NETWORKS AND SPECTRAL NORMALIZATION\n\nGANs. In GAN training, the goal is to obtain a generator network to output samples resembling some ‘true’ target distribution. To achieve this, Goodfellow et al. (2014) proposed to alternate training of the generator with training an additional discriminator network, tasked to distinguish between the generated and true samples. The generator’s objective is then to maximize the probability of its own samples according to the current discriminator, backpropagating directly through the discriminator’s network. Since both the generated data and discriminator’s network parameters constantly change from this alternating optimization, the loss landscape of GANs is also highly non-stationary, resembling, to some degree, the RL setting. As analyzed by several works, the adversarial nature of the optimization makes it very brittle to exploding and vanishing gradients instabilities (Arjovsky & Bottou, 2017; Brock et al., 2018) which often result in common failure modes from severe divergence or stalled learning (Lin et al., 2021). Consequently, numerous practices in the GAN literature have been proposed to stabilize training (Radford et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017b). Inspired by recent work, in this work, we focus specifically on spectral normalization (Miyato et al., 2018), one such practice whose recent success made in ubiquitous in modern GAN implementations.\n\nj\n\nj\n\n∥sn = σ(W SN\n\nj=1∥lj∥Lip ≤ (cid:81)L\n\nSpectral normalization. In the adversarial interplay characterizing GAN training, instabilities commonly derive from the gradients of the discriminator network, fD (Salimans et al., 2016). Hence, Miyato et al. (2018) proposed to regularize the spectral norm of discriminator’s layers, lj ∈ fD, i.e., the largest singular values of the weight matrices ∥W W N ), to be approximately one. Consequently, spectral normalization effectively bounds the Lipschitz constant of the whole discriminator network since, ∥fD∥Lip ≤ (cid:81)L ∥sn = 1. In practice, the proposed implementation approximates the largest singular value of some original unconstrained weight matrices by running power iteration (Golub & Van der Vorst, 2000). Thus, it recovers the spectrallynormalized weights with a simple re-parameterization, dividing the unconstrained weights by their relative singular values W SN σ(Wj ) . As mentioned in the main text, recent work (Lin et al., 2021) showed that one of the main reasons for the surprising effectiveness of spectral normalization in GAN training comes from effectively regulating both the magnitude of the activations and their respective gradients, very similarly to LeCun initialization (LeCun et al., 2012). Furthermore, when applied to the discriminator, spectral normalization’s effects appear to persist throughout training, while initialization strategies tend to only affect the initial iterations. In fact, in Figure 2 of their paper, they also show that ablating spectral normalization empirically results in exploding gradients and degraded performance, closely resembling our same observed instabilities in Figure 5 (B).\n\nj = Wj\n\nj=1∥W SN\n\nj\n\nS-RYM entails applying spectral normalization to all the layers in the Euclidean encoder subnetwork, as already after backpropagation through the final latent hyperbolic representation we observe exploding high-variance gradients. We leave the final linear transformation in hyperbolic space\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nunregularized as we do not want unnecessarily restrict the expressivity of the model. Furthermore, there is also no direct way of performing power iteration with our final layer parameterization (see Section 2.2). We note that by applying spectral normalization to all layers, we would enforce our models of the value and policy to be 1-Lipschitz, a property that is likely not reflective of the true optimal policy and value functions. We validate this hypothesis in Appendix E.4 by enforcing Lipschitz continuity with gradient penalties (Gulrajani et al., 2017b) on top of our regularized Hyperbolic PPO implementation. For Euclidean PPO, preliminary experiments confirmed that enforcing the model to be 1-Lipschitz by applying either spectral normalization to all layers or gradient penalties leads to worse results than applying S-RYM. This intuition is also consistent with other recent works that studied the application of spectral normalization for reinforcement learning (Bjorck et al., 2021; Gogianu et al., 2021). Yet, these works also observed performance benefits when applying spectral normalization exclusively to particular layers of the model. These empirical insights could inform future improvements for S-RYM to retain the stability benefits of spectral normalization with even less restrictive regularization.\n\nB STABILIZATION OF HYPERBOLIC REPRESENTATIONS\n\nOne of the main challenges of incorporating hyperbolic geometry with neural networks comes from end-to-end optimization of latent representations and parameters located in hyperbolic space. For instance, numerical issues and vanishing gradients occur as representations get too close to either the origin or the boundary of the Poincar ́e ball (Ganea et al., 2018). Moreover, training dynamics can tend to push representations towards the boundary, slowing down learning and make optimization problems of earlier layers ineffective (Guo et al., 2022). A number of methods have been used to help stabilize learning of hyperbolic representations including constraining the representations to have a low magnitude early in training, applying clipping and perturbations (Ganea et al., 2018; Khrulkov et al., 2020), actively masking invalid gradients (Mathieu et al., 2019), and designing initial ‘burn-in’ periods of training with lower learning rates (Nickel & Kiela, 2017; B ́ecigneul & Ganea, 2019). More recently Guo et al. (2022) also showed that very significant magnitude clipping of the latent representations can effectively attenuate these numerical and learning instabilities when training hyperbolic classifiers for popular image classification benchmarks.\n\nB.1 MAGNITUDE CLIPPING\n\nGuo et al. (2022) recently proposed to apply significant clipping of the magnitude of the latent representations when using hyperbolic representations within deep neural networks. As in our work, they also consider a hybrid architecture where they apply an exponential map before the final layer to obtain latent representations in hyperbolic space. They apply the proposed clipping to constrain the input vector of the exponential map to not exceed unit norm, producing hyperbolic representations via:\n\nxH = exp1\n\n0\n\n(cid:18)\n\n(cid:26)\n\nmin\n\n1,\n\n(cid:27)\n\n1 ||xE||\n\n(cid:19)\n\n× xE\n\n.\n\n(12)\n\nThe main motivation for this practice is to constrain representation magnitudes, which the authors linked to a vanishing gradient phenomenon when training on standard image classification datasets (Krizhevsky et al., 2009; Deng et al., 2009). However, a side effect of this formulation is that the learning signal from the representations exceeding a magnitude of 1 will solely convey information about the representation’s direction and not its magnitude. Since the authors do not share their implementation, we tested applying their technique as described in the paper. We found some benefits in additionally initializing the parameters of the last two linear layers (in Euclidean and hyperbolic space) to 100× smaller values to facilitate learning initial angular layouts.\n\nB.2\n\nIMAGE CLASSIFICATION EXPERIMENTS\n\nTo empirically validate and analyze our clipping implementation we consider evaluating deep hyperbolic representations on image classification tasks, following the same training practices and datasets from Guo et al. (2022). In particular, we utilize a standard ResNet18 architecture (He et al., 2016) and test our network on CIFAR10 and CIFAR100 (Krizhevsky et al., 2009). We optimize the Euclidean parameters of the classifier using stochastic gradient descent with momentum and the hyperbolic parameters using its Riemmanian analogue (Bonnabel, 2013). We train for 100 epochs with\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nFigure 11: Visualization of test images from CIFAR10, with the corresponding final latent representations magnitudes from our hyperbolic ResNet18 classifier implemented with S-RYM. We sample datapoints with the 5% highest magnitudes (Top) and the 5% lowest magnitudes (Bottom).\n\nan initial learning rate of 0.1 and a cosine schedule (Loshchilov & Hutter, 2017), using a standard batch size of 128. We repeat each experiment 3 times, recording the final top-1 classification accuracy together with the latent representations in Euclidean space right before applying the exponential map at the final layer.\n\nTable 3: Performance results on standard image classification benchmarks\n\nCIFAR10 with ResNet18\n\nMetric\\Architecture\n\nEuclidean\n\nHyperbolic + Clipping Hyperbolic + S-RYM\n\nTop-1 accuracy L2 representations magnitudes Magnitudes standard deviation\n\n94.92 ± 0.19 5.846 0.747\n\n94.81 ± 0.17 1.00 0.00\n\n95.12 ± 0.09 0.481 0.039\n\nCIFAR100 with ResNet18\n\nMetric\\Architecture\n\nEuclidean ResNet18 Hyperbolic + Clipping Hyperbolic + S-RYM\n\nTop-1 accuracy L2 representations magnitudes Magnitudes standard deviation\n\n76.86 ± 0.23 11.30 1.571\n\n76.75 ± 0.23 1.00 0.00\n\n77.49 ± 0.35 0.852 0.076\n\nIn Table 3, we report the different classifiers’ performance together with the mean and standard deviation of the representations’ magnitudes from the images in the test set. The performance of the clipped hyperbolic classifier is very close to the performance of the Euclidean classifier, matching Guo et al. (2022)’s results and validating our implementation. However, the learned representations’ magnitudes soon overshoot the clipping threshold and get mapped to constant-magnitude vectors throughout training. Therefore, the model will effectively stop optimizing for the representations’ magnitudes and only focus on their unit direction. As volume and distances on the Poincar ́e ball grow expoentially with radius, the magnitude component of the hyperbolic representations is pre-\n\n19\n\nPublished as a conference paper at ICLR 2023\n\ncisely what facilitates encoding hierarchical information, providing its intuitive connection with tree structures. Hence, the resulting clipped ‘hyperbolic’ space spanned by the clipped latent representations will lose its defining degree of freedom and approximately resemble an n − 1-dimensional Euclidean space with a rescaled metric, potentially explaining its performance similarity with standard Euclidean classifiers. Even though the focus of our work is not image classification, we find S-RYM’s performance remarkably recovers and even marginally exceeds the performance of both the Euclidean and the clipped hyperbolic classifiers on these saturated benchmarks. Furthermore, its representations do not explode and maintain magnitude diversity, enabling to more efficiently capture the relative hierarchical nature of image-classification benchmarks (Khrulkov et al., 2020). Overall, these results suggest that clipping simply treats the symptoms of the instabilities caused by end-to-end large scale training by essentially resorting back to Euclidean representations for image classification.\n\nAnalyzing the magnitude component of the latent representations for our hyperbolic classifier with S-RYM, we find it correlates with classification performance. For instance, on CIFAR10 the test performance on the images with representations’s with the top 5% magnitudes is 97.17%, while for the bottom 5% is 79.64%. Furthermore, we display some samples from these two distinct groups in Figure 11. From these results and visualizations, it appears that the hyperbolic hierarchical structure serves to encode the degree of uncertainty to disambiguate between multiple image labels due to the blurriness and varying difficulty of the CIFAR10 datapoints. Hence, we believe the observed accuracy improvements of our hyperbolic classifier might be specifically due to more efficiently capturing this specific hierarchical property of the considered datasets.\n\nC IMPLEMENTATION AND EXPERIMENT DETAILS\n\nWe provide details of the experimental settings and implementations with the corresponding hyperparameters for both our Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Rainbow DQN experiments (Hessel et al., 2018). We consider these two main baselines since they are two of the most studied algorithms in the recent RL literature onto which many other recent advances also build upon (e.g., (Cobbe et al., 2021; Laskin et al., 2020b; Mohanty et al., 2021; Raileanu et al., 2020; Raileanu & Fergus, 2021; Yarats et al., 2021a; Van Hasselt et al., 2019; Laskin et al., 2020a; Schwarzer et al., 2020)). Furthermore, PPO and Rainbow DQN are based on the main families of model-free RL algorithms, with very distinct properties as described in Appendix A.1. Hence, unlike most prior advances, we do not constrain our analysis to a single class of methods, empirically showing the generality of hyperbolic deep RL. Our implementations closely follow the reported details from recent research, and were not tuned to facilitate our integration of hyperbolic representations. The main reason for this choice is that we wanted to avoid introducing additional confounding factors from our evaluation of hyperbolic representations, as ad-hoc tuning frequently plays a significant role in RL performance (Islam et al., 2017).\n\nWe implemented the proposed dimensionality-based rescaling, followed by the exponential map and the linear layer in hyperbolic space as a single Pytorch module (the PoincarePlaneDistance class in the shared code). Our implementation allows us to easily integrate our framework with existing neural network models by simply swapping the final layer with our new module. We would like to acknowledge the Geoopt optimization library (Kochurov et al., 2020), which we used to efficiently train and store the network parameters located in hyperbolic space. We also make use of the Hydra library (Yadan, 2019) to facilitate storing hyper-parameters and quickly specifying the different algorithm variations for the experiments in our work.\n\nC.1 BENCHMARKS\n\nProcgen. The Procgen generalization benchmark (Cobbe et al., 2020) consists of 16 game environments with procedurally-generated random levels. The state spaces of these environments consist of the RGB values from the 64x64 rescaled visual renderings. Following common practice and the recommended settings, we consider training agents using exclusively the first 200 levels of each environment and evaluate on the full distribution of levels to assess agent performance and generalization. Furthermore, we train for 25M total environment steps and record final training/test performance collected across the last 100K steps averaged over 100 evaluation rollouts.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nAtari 100K. The Atari 100K benchmark (Kaiser et al., 2020) is based on the seminal problems from the Atari Learning Environment (Bellemare et al., 2013). In particular, this benchmark consists of 26 different environments and only 100K total environment steps for learning each, corresponding roughly to 2hrs of play time. The environments are modified with the specifications from (Machado et al., 2018), making the state spaces of these environments 84x84 rescaled visual renderings and introducing randomness through sticky actions. We note that this is a significantly different setting than Procgen, testing the bounds for the sample efficiency of RL agents.\n\nC.2 PPO IMPLEMENTATION\n\nOur PPO implementation follows the original Procgen paper (Cobbe et al., 2020), which entails a residual convolutional network (Espeholt et al., 2018) and produces a final 256-dimensional latent representation with a shared backbone for both the policy and value function. Many prior improvements over PPO for on-policy learning have been characterized by either introducing auxiliary domain-specific practices, increasing the total number of parameters, or performing additional optimization phases - leading to significant computational overheads (Cobbe et al., 2021; Raileanu & Fergus, 2021; Mohanty et al., 2021). Instead, our approach strives for an orthogonal direction by simply utilizing hyperbolic geometry to facilitate encoding hierarchically-structured features into the final latent representations. Thus, it can be interpreted as a new way to modify the inductive bias of deep learning models for reinforcement learning.\n\nTable 4: PPO hyper-parameters used for the Procgen generalization benchmark\n\nPPO hyperparameters\n\nParallel environments Stacked input frames Steps per rollout Training epochs per rollout Batch size Normalize rewards Discount γ GAE λ (Schulman et al., 2015) PPO clipping Entropy coefficient Value coefficient Shared network Impala stack filter sizes Default latent representation size Optimizer Optimizer learning rate Optimizer stabilization constant (ε) Maximum gradient norm.\n\n64 1\n16384 3\n2048 True 0.999 0.95 0.2 0.01 0.5 True 16, 32, 32 256 Adam (Kingma & Ba, 2015) 5×10−4 1×10−5 0.5\n\nIn Table 4 we provide further details of our PPO hyper-parameters, as also described by the original Procgen paper (Cobbe et al., 2020). When using hyperbolic latent representations, we optimize the hyperbolic weights of the final Gyroplane linear layer with the Riemannian Adam optimizer (B ́ecigneul & Ganea, 2019), keeping the same learning rate and other default parameters. As per common practice in on-policy methods, we initialize the parameters of the final layer with 100× times lower magnitude. We implemented the naive hyperbolic reinforcement learning implementations introduced in Subsection 3.2 by initializing also the weights of the preceding layer with 100× lower magnitudes to facilitate learning appropriate angular layouts in the early training iterations. We found our S-RYM stabilization procedure also enable to safely remove this practice with no effects on performance.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nC.3 RAINBOW IMPLEMENTATIONS\n\nOur implementation of Rainbow DQN uses the same residual network architecture as our PPO implementation (Espeholt et al., 2018) but employs a final latent dimensionality of 512, as again specified by Cobbe et al. (2020). Since Cobbe et al. (2020) do not open-source their Rainbow implementation and do not provide many of the relative details, we strive for a simple implementation removing unnecessary complexity and boosting overall efficiency. Following Castro et al. (2018), we only consider Rainbow DQN’s three most significant advances over vanilla DQN (Mnih et al., 2013): distributional critics (Bellemare et al., 2017), prioritized experience replay (Schaul et al., 2016), and n-step returns (Sutton & Barto, 2018). While the methodology underlying off-policy algorithms is fundamentally different from their on-policy counterparts, we apply the same exact recipe of integrating hyperbolic representations in the final layer, and compare against the same variations and baselines.\n\nTable 5: Rainbow DQN hyper-parameters used for the Procgen generalization benchmark\n\nRainbow DQN Procgen hyperparameters\n\n64 Parallel environments 1\nStacked input frames 1.28M Replay buffer size 512 Batch size 32K steps Minimum data before training 256 env. steps Update network every 12800 env. steps Update target network every 1→0.01 in 512K steps ε-greedy exploration schedule Discount γ 0.99 3\nN-step False Use dueling (Wang et al., 2016) False Use noisy layers (Fortunato et al., 2018) Use prioritized replay (Schaul et al., 2016) True Use distributional value (Bellemare et al., 2017) True Distributional bins Maximum distributional value Minimum distributional value Impala stack filter sizes Default latent representation size Optimizer Optimizer learning rate Optimizer stabilization constant (ε) Maximum gradient norm.\n\n51 10 -10 16, 32, 32 512 Adam (Kingma & Ba, 2015) 5×10−4 1×10−5 0.5\n\nIn Table 5 we provide details of our Rainbow DQN hyper-parameters. We note that sampling of off-policy transitions with n-step returns requires retrieving the future n rewards and observations. To perform this efficiently while gathering multiple transitions from the parallelized environment, we implemented a parellalized version of a segment tree. In particular, this extends the original implementation proposed by Schaul et al. (2016), through updating a set of segment trees implemented as a unique data-structure with a single parallelized operation, allowing for computational efficiency without requiring any storage redundancy. We refer to the shared code for further details. As with our hyperbolic PPO extensions, we also optimize the final layer’s hyperbolic weights with Riemannian Adam, keeping the same parameters as for the Adam optimizer used in the other Euclidean layers.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Rainbow DQN hyper-parameters changes for the Atari 100K benchmark\n\nRainbow DQN Atari 100K training hyper-parameters\n\nStacked input frames Batch size Minimum data before training Network updates per step Update target network every ε-greedy exploration schedule\n\n4 32 1600 steps 2\n1 env. steps 1→0.01 in 20K steps\n\nThe characteristics of the Atari 100K benchmark are severely different from Procgen, given by the lack of parallelized environments and the 250× reduction in total training data. Hence, we make a minimal set of changes to the training loop hyper-parameters of our Rainbow DQN implementation to ensure effective learning, as detailed in Table 6. These are based on standard practices employed by off-policy algorithm evaluating on the Atari 100K benchmark (Van Hasselt et al., 2019; Laskin et al., 2020a; Yarats et al., 2021a) and were not tuned for our specific implementation.\n\nC.4 EXPERIMENTAL RECORDINGS\n\nBy default, we repeat each experiment with five random seeds. To visualize the evolution of relevant values using performance curves, we collect measurements from each algorithm in five evaluation rollouts every 64×1024 frames (64 is the number of parallel environments used in Procgen). Hence, the reported values correspond to the mean results over all the collected seeds and runs, while the shaded regions correspond to the standard deviation between the mean results from different random seeds. For the tabular data illustrating final performance, we perform ten additional evaluation rollouts at the end of the specified training allowance. The reported uncertainty again represents the standard deviation between the mean results from different seeds.\n\nC.5 CURRENT LIMITATIONS\n\nWe identify three main current limitations of the proposed implementation. First, stabilizing hyperbolic representations with S-RYM inherently constrains the expressivity of the Euclidean subnetwork encoder model (fE) to be 1-Lipschitz. This loss of expressivity might hinder the network’s ability to learn complex representations, preventing our hyperbolic framework to achieve its full potential. Second, the training and evaluation time of our hyperbolic agents are consistently higher than their Euclidean counterparts. Our hyperbolic PPO implementation takes on average 4.27 seconds to collect rollouts and train for three epochs, and takes 0.961 seconds to collect a full episode of experience at the end of training. In contrast, our Euclidean baseline takes 3.69 seconds for training (16% speedup) and 0.854 seconds for evaluation (13% speedup). We find this slowdown is mainly due to the power iteration procedure performed to apply spectral normalization in S-RYM and, to a lesser extent, an overhead when computing and backpropagating through hyperbolic operations. Third, our algorithm utilizes a model of hyperbolic space with fixed negative curvature to build representations of the whole state space. However, as different RL problems might have considerably different structures, we believe that any fixed curvature might not always yield the most appropriate inductive bias. To this end, recent work showed potential benefits in using mixed curvature latent spaces and even learning the curvature parameter for unsupervised tasks (Skopek et al., 2019). We hope these limitations will be addressed in future work, further studying how differential geometry can be used to empower RL.\n\nD EXTENDED RESULTS AND COMPARISONS\n\nIn this Section, we provide detailed per-environment Rainbow DQN Procgen results that were omitted from the main text due to space constraints. For both Rainbow DQN and PPO. We also compare the performance improvements from the integration of our deep hyperbolic representations with the reported improvements from recent state-of-the-art (SotA) algorithms, employing one or several orthogonal domain-specific practices. In Appendix E.3, we provide examples empirically validating\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nTable 7: Detailed performance comparison for the Rainbow DQN algorithm on the full Procgen benchmark. We train for a total of 25M steps on 200 training levels and test on the full distribution of levels. We report the mean returns, the standard deviation, and relative improvements from the original Rainbow DQN baseline over 5 random seeds.\n\nTask\\Algorithm\n\nRainbow DQN\n\nRainbow DQN + data aug.\n\nRainbow DQN + S-RYM\n\nRainbow DQN + S-RYM, 32 dim.\n\nLevels distribution\n\ntrain/test\n\ntrain/test\n\ntrain/test\n\ntrain/test\n\nbigfish bossfight caveflyer chaser climber coinrun dodgeball fruitbot heist jumper leaper miner maze ninja plunder starpilot\n\n7.17±1 7.29±1 7.00±1 3.92±1 3.09±1 2.31±0 3.68±1 1.73±1 5.56±1 4.33±1 7.42±1 4.67±1\n\n6.22±1 (-13%) 6.97±1 (-4%) 7.59±0 (+8%) 5.36±1 (+37%) 2.89±0 (-6%) 2.61±0 (+13%) 2.57±1 (-30%) 2.36±1 (+36%) 3.22±1 (-42%) 3.00±1 (-31%) 8.91±1 (+20%) 6.96±1 (+49%) 21.51±3 16.94±2 22.29±2 (+4%) 20.53±3 (+21%)\n\n23.17±4 15.47±2 19.61±4 (-15%) 17.39±4 (+12%) 27.61±0 (+19%) 23.03±2 (+49%) 30.85±2 (+33%) 22.37±2 (+45%) 8.21±1 (+15%) 8.71±1 (+20%) 6.45±1 (-8%) 5.46±1 (+39%) 3.78±0 (+23%) 3.29±0 (+43%) 4.80±2 (+31%) 3.00±0 (+73%) 6.00±1 (+8%) 6.33±1 (+46%) 6.89±1 (-7%) 5.31±1 (+14%)\n\n9.41±1 (+31%) 7.75±1 (+6%) 6.39±1 (-9%) 3.11±1 (-21%) 4.03±1 (+30%) 3.65±1 (+58%) 3.91±0 (+6%) 2.39±0 (+38%) 5.20±0 (-6%) 5.07±1 (+17%) 6.07±1 (-18%) 3.60±1 (-23%) 20.31±1 (-6%) 20.30±1 (+20%) 22.81±1 (+6%) 21.87±2 (+29%) 0.67±0 0.11±0 1.67±0 (+150%) 0.67±0 (+500%) 1.27±0 (+90%) 0.40±0 (+260%) 0.93±1 (+40%) 0.47±0 (+320%) 5.53±1 (+4%) 3.47±1 (+11%) 5.33±1 3.11±0 4.78±1 (-10%) 2.44±1 (-21%) 1.78±1 2.56±1 6.11±1 (+244%) 5.11±1 (+100%) 2.00±1 (+13%) 1.00±0 (-61%) 0.80±0 (-55%) 0.53±0 (-79%) 2.22±1 2.33±0 2.73±1 (+23%) 2.00±0 (-14%) 2.40±0 (+8%) 1.40±0 (-40%) 1.97±0 (-2%) 0.92±0 (+38%) 1.91±0 (-5%) 0.93±0 (+40%) 2.01±0 0.67±0 3.73±1 (+12%) 3.33±1 (+43%) 3.33±1 (+0%) 2.11±0 (-10%) 3.33±0 2.33±1 8.69±0 6.28±1 7.11±1 (-18%) 5.71±1 (-9%) 7.33±1 (-16%) 5.93±1 (-5%)\n\n1.89±0 (-15%) 1.33±1 (-43%) 2.07±0 (+3%) 1.58±1 (+137%) 3.44±1 (+3%) 2.56±1 (+10%) 6.06±1 (-30%) 5.30±1 (-16%)\n\n4.22±0 (-21%) 2.78±1 (-11%)\n\n47.83±6 42.42±1 51.79±3 (+8%) 46.23±5 (+9%) 57.64±2 (+21%) 55.86±3 (+32%) 59.94±1 (+25%) 54.77±3 (+29%)\n\nAverage norm. score Median norm. score # Env. improvements\n\n0.2679 0.1605 0.1856 0.0328 0/16 0/16\n\n0.2698 (+1%) 0.2106 (+31%) 0.1830 (-1%) 0.1010 (+208%) 8/16 11/16\n\n0.2774 (+4%) 0.1959 (+22%) 0.2171 (+17%) 0.0250 (-24%) 8/16 9/16\n\n0.3097 (+16%) 0.2432 (+51%) 0.2634 (+42%) 0.1559 (+376%) 11/16 13/16\n\nthat hyperbolic representations provide mostly complementary benefits and are compatible with different domain-specific practices, potentially yielding even further performance gains. Finally, we provide additional qualitative 2-dimensional visualizations of learned trajectory representation and δ-hyperbolicity recordings also for our regularized hyperbolic PPO algorithm.\n\nD.1 RAINBOW DQN PROCGEN RESULTS\n\nAs shown in Table 7, our hyperbolic Rainbow DQN with S-RYM appears to yield conspicuous performance gains on the majority of the environments. Once again, we find that reducing the dimensionality of the representations to 32 provides even further benefits, outperforming the Euclidean baseline in 13 out of 16 environments. This result not only highlights the efficiency of hyperbolic geometry to encode hierarchical features, but also appears to validate our intuition about the usefulness of regularizing the encoding of non-hierarchical and potentially spurious information. While still inferior to our best hyperbolic implementation, data augmentations seem to have a greater overall beneficial effect when applied to Rainbow DQN rather than PPO. We believe this result is linked with recent literature (Cetin et al., 2022) showing that data-augmentation also provides off-policy RL with an auxiliary regularization effect that stabilizes temporal-difference learning.\n\nD.2 SOTA COMPARISON ON PROCGEN\n\nIn Table 8 we compare our best hyperbolic PPO agent with the reported results for the current SotA Procgen algorithms from Raileanu & Fergus (2021). All these works propose domain-specific practices on top of PPO (Schulman et al., 2017), designed and tuned for the Procgen benchmark: Mixture Regularization (MixReg) (Wang et al., 2020), Prioritized Level Replay (PLR) (Jiang et al., 2021), Data-regularized Actor-Critic (DraC) (Raileanu et al., 2020), Phasic Policy Gradient (PPG) (Cobbe et al., 2021), and Invariant Decoupled Advantage Actor Critic (Raileanu & Fergus, 2021).Validating our implementation, we see that our Euclidean PPO results closely match the previously reported ones, lagging severely behind all other methods. In contrast, we see that introducing our deep hyperbolic representations framework makes PPO outperform all considered baselines but IDAAC, attaining overall similar scores to this algorithm employing several domain-specific practices. In particular, IDAAC not only makes use of a very specialized architecture, but also introduces an auxiliary objective to minimize the correlation between the policy representations and the number of steps until task-completion. Raileanu & Fergus (2021) found this measure to be an effective heuristic correlating with the occurrence of overfitting in many Procgen environments. Moreover, we see that our hyperbolic PPO attains the best performance on 7 different environments, more than any\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nTable 8: Performance comparison on the test distribution of levels for our Euclidean and Hyperbolic PPO agents with the reported results of recent RL algorithms designed specifically for the Procgen benchmark.\n\nTask\\Algorithm\n\nPPO (Reported) Mixreg PLR UCB-DrAC PPG IDAAC PPO (Ours) Hyperbolic PPO + S-RYM (Ours)\n\nbigfish bossfight caveflyer chaser climber coinrun dodgeball fruitbot heist jumper leaper maze miner ninja plunder starpilot\n\n3.7 7.4 5.1 3.5 5.6 8.6 1.6 26.2 2.5 5.9 4.9 5.5 8.4 5.9 5.2 24.9\n\n7.1 8.2 6.1 5.8 6.9 8.6 1.7 27.3 2.6 6\n5.3 5.2 9.4 6.8 5.9 32.4\n\n10.9 8.9 6.3 6.9 6.3 8.8 1.8 28 2.9 5.8 6.8 5.5 9.6 7.2 8.7 27.9\n\n9.2 7.8 5\n6.3 6.3 8.6 4.2 27.6 3.5 6.2 4.8 6.3 9.2 6.6 8.3 30\n\n11.2 10.3 7\n9.8 2.8 8.9 2.3 27.8 2.8 5.9 8.5 5.1 7.4 6.6 14.3 47.2\n\n18.5 9.8 5\n6.8 8.3 9.4 3.2 27.9 3.5 6.3 7.7 5.6 9.5 6.8 23.3 37\n\n1.46±1 7.04±2 5.86±1 5.89±1 5.11±1 8.25±0 1.87±1 26.33±2 2.92±1 6.14±1 4.36±2 6.50±0 9.28±1 6.50±1 6.06±3 26.57±5\n\nAverage norm. score Median norm. score\n\n0.3078 0.3055\n\n0.3712 0.4139 0.4263 0.4093\n\n0.3931 0.4264\n\n0.4488 0.5048 0.4456 0.5343\n\n0.3476 0.3457\n\n16.57±2 (+1037%) 9.02±1 (+28%) 5.20±1 (-11%) 7.32±1 (+24%) 7.28±1 (+43%) 9.20±0 (+12%) 7.14±1 (+281%) 29.51±1 (+12%) 3.60±1 (+23%) 6.10±1 (-1%) 7.00±1 (+61%) 7.10±1 (+9%) 9.86±1 (+6%) 5.60±1 (-14%) 6.68±0 (+10%) 38.27±5 (+44%)\n\n0.4730 (+36%) 0.4705 (+36%)\n\nTable 9: Performance comparison for our Euclidean and Hyperbolic Rainbow DQN agents with the reported results of recent RL algorithms designed specifically for the Atari 100K benchmark.\n\nTask\\Algorithm\n\nRandom Human DER OTRainbow CURL DrQ\n\nSPR Rainbow DQN (Ours) Rainbow DQN + S-RYM (Ours)\n\nAlien Amidar Assault Asterix Bank Heist Battle Zone Boxing Breakout Chopper Command Crazy Climber Demon Attack Freeway Frostbite Gopher Hero Jamesbond Kangaroo Krull Kung Fu Master Ms Pacman Pong Private Eye Qbert Road Runner Seaquest Up N Down\n\n0.2 1.9 861.8\n\n0.10 1.70 811.00\n\n739.9 188.6 431.2 470.8 51\n\n227.80 5.80 222.40 210.00 14.20\n\n7127.70 1719.50 742.00 8503.30 753.10 2360.00 37187.50 10124.6 12.10 30.50 7387.80 10780.50 35829.40 16185.3 1971.00 29.60 4334.70 2412.50\n\n508 152.10 27.9 0.00 866.8 65.20 257.60 349.5 1027.00 30826.40 6857 301.6 302.80 779.3 3035.00 1598.00 2665.50 2851.5 258.50 22736.30 14346.1 6951.60 1204.1 307.30 -19.3 14.60 -20.70 24.90 97.8 69571.30 163.90 13455.00 1152.9 11.50 9600 7845.00 42054.70 354.1 68.40 533.40 11693.20 2877.4\n\n29.00 52.00\n\nHuman Norm. Mean 0.000 Human Norm. Median 0.000 # Super N/A\n\n1.000 1.000 N/A\n\n0.285 0.161 2\n\n824.7 82.8 351.9 628.5 182.1 4060.6 2.5 9.8 1033.3 21327.8 711.8 25 231.6 778 6458.8 112.3 605.4 3277.9 5722.2 941.9 1.3 100 509.3 2696.7 286.9 2847.6\n\n0.264 0.204 1\n\n801.5 176.3 571 977.8 380.9 16651 35.8 17.1 974.8\n\n771.2 558.2 102.8 142.1 452.4 600.6 603.5 734.5 168.9 131.6 12954 14870 6\n1.2 16.1 4.9 1058.5 780.3 12146.5 20516.5 42923.6 545.2 1113.4 817.6 24.4 9.8 26.7 1821.5 331.1 1181.3 669.3 715.2 636.3 6279.3 3736.3 7019.2 365.4 236 471 3276.4 872.5 940.6 4229.6 4018.1 3688.9 14307.8 9111 13192.7 1465.5 1313.2 960.5 -5.9 -8.5 -16.5 218.4 124 -13.6 854.4 1042.4 669.1 5661 8895.1 14220.5 301.2 384.5 2955.2 3180.8 28138.5\n\n583.1\n\n0.381 0.175 2\n\n0.357 0.268 2\n\n0.704 0.415 7\n\n548.33 132.55 539.87 448.33 187.5 12466.7 2.92 13.72 791.67 20496.7 1204.75 30.5 318.17 343.67 9453.25 190.83 1200 3445.02 7145 1044.17 3.85 72.28 860.83 6090 259.33 2935.67\n\n0.353 0.259 2\n\n679.20 (+41%) 118.62 (-11%) 706.26 (+52%) 535.00 (+36%) 255.00 (+39%) 25800.00 (+132%) 9.28 (+226%) 58.18 (+370%) 888.00 (+498%) 22226.00 (+18%) 4031.60 (+269%) 29.50 (-3%) 1112.20 (+314%) 1132.80 (+917%) 7654.40 (-21%) 380.00 (+117%) 1020.00 (-16%) 3885.02 (+24%) 10604.00 (+50%) 1135.60 (+12%) 11.98 (+33%) 106.06 (+71%) 2702.00 (+264%) 22256.00 (+266%) 476.80 (+114%) 3255.00 (+13%)\n\n0.686 (+94%) 0.366 (+41%) 5\n\nother method. Furthermore, in these environment the other Euclidean algorithms specifically struggle, again indicating the orthogonal effects of our approach as compared to traditional RL advances.\n\nD.3 SOTA COMPARISON ON ATARI 100K\n\nIn Table 9 we provide detailed raw results for our hyperbolic Rainbow DQN agent, comparing with the results for recent off-policy algorithms for the Atari 100K benchmark, as reported by Schwarzer et al. (2020). All the considered algorithms build on top of the original Rainbow algorithm (Hessel et al., 2018). We consider Data Efficient Rainbow (DER) (Van Hasselt et al., 2019) and Overtrained Rainbow (OTRainbow) (Kielak, 2019) which simply improve the model architectures and other training-loop hyper-parameters, for instance, increasing the number of update steps per collected environment step. We also compare with other more recent baselines that incorpo-\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nrate several additional auxiliary practices and data-augmentation such as Data-regularized Q (DrQ) (Yarats et al., 2021a), Contrastive Unsupervised Representations (CURL) (Laskin et al., 2020a), and Self-Predictive Representations (SPR) (Schwarzer et al., 2020). While our Euclidean Rainbow implementation attains only mediocre scores, once again we see that introducing our deep hyperbolic representations makes our approach competitive with the state-of-the-art and highly-tuned SPR algorithm. In particular, SPR makes use of several architectural advancements, data-augmentation strategies from prior work, and a model-based contrastive auxiliary learning phase. Also on this benchmark, our hyperbolic agent attains the best performance on 8 different environments, more than any other considered algorithm.\n\nD.4\n\n2-DIMENSIONAL REPRESENTATIONS PERFORMANCE AND INTERPRETATION\n\ntrain/test\n\nTask\\Algorithm\n\nLevels distribution\n\nPPO + S-RYM, 2 dim.\n\n30.27±3 (-1%) 29.72±6 (+12%)\n\nbigfish dodgeball fruitbot starpilot\n\n5.65±4 (+52%) 2.34±3 (+60%) 2.62±0 (-48%) 2.36±1 (+26%) 27.18±4 (-10%) 25.75±1 (-2%)\n\nTable 10: Performance of 2-dimensional hyperbolic PPO as compared to the original PPO algorithm.\n\nTo visualize and allow us to interpret the structure of the learned representations, we analyze our hyperbolic PPO agents using only two dimensions to model the final latent representations. As mentioned in Section 4 and shown in Table 10, we find even this extreme implementation to provide performance benefits on the test levels over Euclidean PPO. Furthermore, the generalization gap with the training performance is almost null in three out of the four considered environments. As the 2-dimensional representation size greatly constrains the amount of encoded information, these results provide further validation for the affinity of hyperbolic geometry to effectively prioritize features useful for RL. We then observe how these 2-dimensional hyperbolic latent representations evolve within trajectories, mapping them on the Poincar ́e disk and visualizing the corresponding input states. As summarized in Section 4, we observe a recurring cyclical behavior, where the magnitude of the representations monotonically increases within subsets of the trajectory as more obstacles and/or enemies appear. Together with Figure 10 (on the bigfish environment), we provide additional qualitative visualizations of this phenomenon in Figure 12 (on the starpilot (A), dodgeball (B), and fruitbot (C) environments). These plots compare the representations of on-policy states sampled at constant intervals within a trajectory, every 15 timesteps, and deviations from executing 15 timesteps of random behavior after resetting the environment to the previous on-policy state. We observe the state representations form tree-like branching structures, somewhat reflecting the tree-like nature of MDPs. Within the subtrajectories in starpilot and fruitbot, we find that the magnitudes in the on-policy trajectory tend to grow in the direction of the Value function’s gyroplane’s normal. Intuitively, this indicates that as new elements appear (e.g., new enemies in starpilot), the agent recognizes a larger opportunity for rewards (e.g., from defeating them) and also that it now requires a much finer level of control for optimality. This is because as magnitudes increase, the signed distances with the policy gyroplanes will also grow exponentially, and so will the value of the different action logits, decreasing the policy’s entropy. In contrast, the magnitudes of the state representations following the random deviations grow in directions with considerably larger orthogonal components to the Value gyroplane’s normal. This still reflects the higher precision required for optimal decision-making, as magnitudes still increase, but also the higher uncertainty to obtain future rewards from these less optimal states. As opposed to the rest of the environments, in dodgeball all enemies and other elements are already present from the first time-step of a trajectory. Furthermore, our 2-dimensional hyperbolic agent appears to lack the representation power to recover good performance. These two properties make the observed branching phenomenon less accentuated, with the magnitude of both random and on-policy transitions changing in mostly orthogonal directions to the gyroplane’s normal.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nFigure 12: Visualization of 2-dimensional hyperbolic embeddings in the starpilot (A), dodgeball (B), and fruitbot (C) Procgen environment. We sub-sample states from recorded agent trajectories every 15 timesteps. We show the evolution of the hyperbolic latent representations following the recorded policy transitions as compared to random transitions collected by resetting the environments from each state and executing a random policy for the same 15 timesteps.\n\nD.5\n\nδ HYPERBOLICITY\n\nWe repeat the experiment in Figure 4, collecting the relative δ-hyperbolicity of the latent representations space produced by our regularized Hyperbolic PPO agent throughout training. Our analysis assumes that as different RL tasks likely require encoding different amounts of hierarchicallystructured information, relative changes in δ-hyperbolicity should be more informative than its overall scale. Furthermore, we would like to emphasize that we tractably estimate δrel using the efficient but approximate algorithm from Fournier et al. (2015) and that, in practice, there are many sources of noise that affect RL optimization. Both these factors inevitably add uninformative noise to the latent representations and our hyperbolicity recordings, which likely affects many of the small local observed changes in δrel.\n\nWe visualize the evolution of the δ-hyperbolicity of our hyperbolic PPO agent regularized with SRYM in Figure 13. As we would expect, using a hyperbolic latent space yields latent representations with significantly lower values of δrel, as compared to standard PPO, implying they possess an increased hierarchical tree-like structure. We observe this consistently for all considered tasks and during all stages of the RL training process. This difference is particularly evident at initialization where δrel ≈ 0.2 for our hyperbolic PPO while δrel ≈ 0.4 for a standard PPO agent, reflecting\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nFigure 13: Performance and relative δ-hyperbolicity of the final latent representations of our hyperbolic PPO agent regularized with S-RYM.\n\nhow the properties of hyperbolic space facilitate capturing hierarchical relationships even with no training. Interestingly, in the dodgeball environment, the recorded value of δrel for our Hyperbolic PPO experiences some considerable fluctuations in some of the very initial and later training stages. Analogously to our results in Figure 4, it appears that during the iterations where δrel increases or attains its higher values, test performance grows significantly slower. Overall, dodgeball is also the environment with the largest generalization gap relative to the hyperbolic agent’s training performance.\n\nE FURTHER EXPERIMENTS AND ABLATION STUDIES\n\nIn this section, we further analyze the properties of our hyperbolic RL framework and its implementation, through additional experiments and ablations. We focus on our hyperbolic PPO algorithm and four representative tasks from the Procgen benchmark.\n\nE.1 S-RYM’S COMPONENTS CONTRIBUTION\n\nFigure 14: Performance ablating either spectral normalization or rescaling from our Hyperbolic PPO agent stabilized with S-RYM.\n\nOur proposed spectrally-regularized hyperbolic mappings (S-RYM) relies on two main distinct components: spectral normalization and rescaling. As described in Section 3, we design our deep RL models to produce a representation by applying traditional neural network layers in Euclidean space xE = fE(s). Before the final linear layer fH , we then use an exponential map from the origin of the Poincar ́e to yield a final representation in hyperbolic space xH = exp1 0(xE). As shown by Lin et al. (2021), applying spectral normalization to the layers of fE regulates both the values and gradients similarly to LeCun initialization (LeCun et al., 2012). Hence, we make the regularization approximately dimensionality-invariant by rescaling xE ∈ Rn, simply dividing its value by n. In Figure 14, we show the results from ablating either component from S-RYM. From our results, both components seem crucial for performance. As removing spectral normalization simply recovers the unregularized hyperbolic PPO implementation with some extra rescaling in the activations, its performance is expectedly close to the underwhelming performance of our naive implementations in Figure 5. Removing our dimensionality-based rescaling appears to have an even larger effect, with almost no agent improvements in 3 out of 4 environments. The necessity of appropriate scaling comes from the influence the representations magnitudes have on optimization. When applying spectral normalization, the dimensionality of the representations directly affects its expected magnitude. Thus, high-dimensional latents will result in high-magnitude representations, making it challenging to optimize for appropriate angular layouts in hyperbolic space (Nickel & Kiela, 2017;\n\n√\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nGanea et al., 2018) and making the gradients of the Euclidean network parameters stagnate (Guo et al., 2022). These issues cannot even be alleviate with appropriate network initialization, since the magnitudes of all weights will be rescaled by the intruduced spectral normalization.\n\nE.2 REPRESENTATION SIZE\n\nFigure 15: Final performance comparison between PPO agents with Euclidean and hyperbolic representations with different dimensionalities.\n\nIn Figure 15, we show the final train and test performance attained by our Euclidean and hyperbolic PPO agents with different dimensionalities for their final latent representations. We collect results on a log scale 2n with n ∈ {3, 4, 5, 6, 7, 8}, i.e., ranging from 23 = 8 to 28 = 256 latent dimensions. Integrating our hyperbolic representations framework with PPO boosts performance across all dimensionalities. Moreover, in 3/4 environments we see both train and test performance of the Euclidean PPO agent considerably dropping as we decrease the latent dimensions. In contrast, the performance of hyperbolic PPO is much more robust, even attaining some test performance gains from more compact representations. As described in Section 2, Euclidean representations require high dimensionalities to encode hierarchical features with low distortion (Matouˇsek, 1990; Gupta, 1999), which might explain their diminishing performance. Instead, as hyperbolic representations do not have such limitation, lowering the dimensionality should mostly affect their ability of encoding non-hierarchical information, which we believe to counteract the agent’s tendency of overfitting to the limited distribution of training levels and observed states.\n\nE.3 COMPATIBILITY WITH ORTHOGONAL PRACTICES\n\nIntroducing hyperbolic geometry to model the representations of RL agents is fundamentally orthogonal to most recent prior advances. Thus, we validate the compatibility of our approach with different methods also aimed at improving the performance and generalization of PPO.\n\nFigure 16: Performance comparison from integrating the advances from the PPG algorithm our hyperbolic reinforcement learning framework.\n\nPhasic Policy Gradient (PPG). We re-implement this recent PPO extension designed by Cobbe et al. (2021) specifically for the Procgen benchmark. PPG adds non-trivial algorithmic and computational complexity, by performing two separate optimization phases. In the first phase, it optimizes the same policy and value optimization objective as in PPO, utilizing the latest on-policy data. In the second phase, it utilizes a much larger buffer of past experience to learn better representations in its policy model via an auxiliary objective, while avoiding forgetting with an additional behavior cloning weighted term. The two phases are alternated infrequently after several training epochs. Once again, we incorporate our hyperbolic representation framework on top of PPG without any additional tuning. In Figure 16, we show the results from adding our deep hyperbolic representation\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nframework to PPG. Even though PPG’s performance already far exceeds PPO, hyperbolic representations appear to have similar effects on the two algorithms, with performance on the 200 training levels largely invaried, and especially notable test performance gains on the bigfish and dodgeball environments. Hence, in both PPO and PPG, the new prior induced by the hyperbolic representations appears to largely reduce overfitting to the observed data and achieve better generalization to unseen conditions. Our approach affects RL in an orthogonal direction to most other algorithmic advances, and our results appear to confirm the general compatibility of its benefits.\n\nFigure 17: Performance comparison from integrating data augmentation with the Euclidean and hyperbolic PPO agents.\n\nData augmentation. Finally, we also test introducing data augmentation to our Hyperbolic PPO implementation. We consider the same popular random shifts from Yarats et al. (2021a), evaluated in Section 4. We note that the problem diversity characterizing procgen makes it challenging for individual hand-designed augmentations to have a generally beneficial effect, with different strategies working best in different environments (Raileanu et al., 2020). In fact, applying random shifts to PPO appears to even hurt performance on a considerable subset of environments (see Table 1), likely due to the agents losing information about the exact position and presence of key objects at the borders of the environment scene. This inconsistency is reflected onto the hyperbolic PPO agent. In particular, while the addition of random shifts further provides benefits on the bigfish environment, it appears to hurt performance on dodgeball. Overall, integrating our hyperbolic framework still appears considerably beneficial even for the test performance of the data-augmented agent, further showing the generality of our method.\n\nE.4 ENFORCING LIPSCHITZ CONTINUITY\n\nFigure 18: Performance of a standard PPO agent and the hyperbolic PPO agent stabilized with SRYM after integrating gradient penalties (GP) (Gulrajani et al., 2017a).\n\nS-RYM leaves the final layer of the network unregularized since there is no direct way of performing power iteration with its parameterization and we also do not want to constrain our models of the value and policy to be 1-Lipschitz. We validate that this property is not reflective of the true optimal policy and value functions, by enforcing Lipschitz continuity with gradient penalties (GP) (Gulrajani et al., 2017a). We apply GP on top of both our Hyperbolic PPO with S-RYM and standard PPO. Our results in Figure E.4 appear to validate our hypothesis by showing that enforcing either the hyperbolic or Euclidean PPO models to be 1-Lipschitz makes performance collapse across all environments.\n\n30",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes spectrally-regularized hyperbolic mappings (S-RYM) to learn a hyperbolic representation for deep RL, by applying a spectral normalization for learning the hyperbolic representation.\n\n# Strength And Weaknesses\n\nStrength:\n1. As an empirical paper, it demonstrates superior performance on many benchmarks (Atari and Procgen).\n2. It illustrates the connection between the hierarchical structure in hyperbolic representation and reinforcement learning\n\nWeakness/Questions:\n1. To the best of the reviewer’s understanding, the implementation details of this paper are not clearly stated:\n(a) If the reviewer understands the implementation clearly, it seems that the implementation adopts the spectral normalization to the hydra package (https://github.com/facebookresearch/hydra)? The reviewer would appreciate it if the author can clarify the novelty in the implementation for better readability! (b) Regardless of whether the novelty in implementation is purely “spectral normalize+hydra”, the author should at least cite the Hydra Package if they are using it as an implementation backbone. (c) The reviewer guesses the last paragraph of section 2.2 “In line with recent use of hyperbolic geometry in supervised …” characterizes the main contents of the main implementation details of the hyperbolic embedding, but perhaps the authors can elaborate more on this part so that the main contribution of this work is much better than  “spectral normalization + some hyperbolic representation learning method other people proposed”.\n2. The motivation from $\\delta$-hyperbolicity to the pursuit of hyperbolic representation is very inspiring. The reviewer is wondering whether the authors can reproduce Figure 4 using PPO + S-RYM. If the authors can demonstrate the $\\delta$-hyperbolicity decreases with PPO + S-RYM, it would also improve the results of this work.\n3. How does the implementation of the hyperbolic embeddings different from other online packages (e.g., https://github.com/nalexai/hyperlib)?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is nicely written for motivation and stating the results. However, the reviewer cannot grasp the implementation details of this paper which hinders the reviewer’s evaluation of the novelties of this paper. The reviewer believes the empirical results of this work are reproducible (although the reviewer did not run the code).\n\n# Summary Of The Review\n\nIn summary, since the empirical results of this work beat the SOTA, the reviewer believes it definitely reaches the bar for acceptance. However, the reviewer believes there is a large room for the writing part (for clarifying the methods and addressing how the proposed method is different from SN + Hydra) so that other readers to better appreciate the merits of this work.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper \"Hyperbolic Deep Reinforcement Learning\" by Edoardo Cetin et al. introduces a novel framework for deep reinforcement learning (RL) that integrates hyperbolic geometry to improve the modeling of hierarchical structures in latent representations. The authors identify challenges associated with applying hyperbolic methods naively, which can lead to instability in training due to the non-stationarity of common RL gradient estimators. They propose a new methodology termed spectrally-regularized hyperbolic mappings (S-RYM) to stabilize the learning process in hyperbolic space. Empirical evaluations demonstrate that S-RYM significantly enhances performance and generalization capabilities on benchmark tasks, specifically Procgen and Atari 100K, compared to traditional Euclidean methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to addressing the limitations of traditional RL models by leveraging hyperbolic geometry, which is particularly effective in capturing hierarchical relationships within data. The proposed S-RYM method effectively mitigates optimization challenges, leading to improved stability and performance across various tasks. However, a potential weakness is the increased training time associated with using S-RYM compared to Euclidean methods, which may limit its practical applicability in time-sensitive contexts. Additionally, while the results are promising, the paper could benefit from a more extensive exploration of the limitations and trade-offs involved in implementing hyperbolic representations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The descriptions of hyperbolic geometry and its integration into RL are articulated effectively, making the concepts accessible to readers. The quality of the empirical evaluation is commendable, with a thorough analysis of performance across multiple benchmarks. The authors demonstrate a commitment to reproducibility by providing clear implementation details and sharing their code, which enhances the credibility of their findings. In terms of novelty, the integration of hyperbolic geometry into RL represents a significant advancement, though the exploration of its practical implications could be further elaborated.\n\n# Summary Of The Review\nThis paper presents a compelling and innovative approach to deep reinforcement learning by leveraging hyperbolic geometry, resulting in improved model performance and generalization. The proposed S-RYM method effectively addresses the instability issues associated with naive hyperbolic implementations. While the contributions are significant, the paper could enhance its practical relevance by addressing the increased training time and exploring potential limitations.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper addresses the limitations of existing deep reinforcement learning (RL) algorithms, particularly their lack of hierarchical feature encoding, by introducing hyperbolic geometry as an inductive bias for representation learning. The proposed method, Spectrally-Regularized Hyperbolic Mappings (S-RYM), stabilizes the training of hyperbolic representations, which are tested on prominent RL benchmarks such as Procgen and Atari 100K using both on-policy (PPO) and off-policy (Rainbow DQN) algorithms. The findings demonstrate that hyperbolic representations outperform Euclidean baselines, exhibit improved generalization capabilities, and enhance sample efficiency, particularly in complex environments.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to leveraging hyperbolic geometry, providing a fresh perspective on feature representation in RL. The comprehensive evaluation across multiple benchmarks solidifies the claims of improved performance and generalization. However, the paper also presents notable weaknesses, including increased complexity in model training and optimization, slower training times compared to Euclidean counterparts, and potential limitations in expressivity due to reliance on fixed curvature. Additionally, the dependency on careful initialization strategies may affect the reproducibility of results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings, enhancing its overall clarity. The quality of the experiments is high, with rigorous evaluation metrics and comprehensive benchmarking. The novelty of applying hyperbolic geometry in RL is significant, contributing to the field's understanding of hierarchical relationships in state representations. However, the reproducibility of results may be challenged by the computational overhead and the need for specific initialization strategies, which could vary across different implementations.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in reinforcement learning by introducing hyperbolic geometry for representation learning. While the contributions are innovative and empirically validated, the increased complexity and computational demands may pose challenges for practical applications and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Hyperbolic Deep Reinforcement Learning\" introduces a novel class of deep reinforcement learning (RL) algorithms that leverage hyperbolic geometry to enhance the modeling of latent representations. The authors identify significant stability issues when integrating naive hyperbolic layers due to non-stationarity and high variance in RL settings, proposing a stabilization technique called Spectrally-Regularized Hyperbolic Mappings (S-RYM) to facilitate stable end-to-end learning. The methodology is empirically validated through experiments on Procgen and Atari 100K benchmarks, demonstrating superior performance and generalization capabilities compared to traditional Euclidean representations.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative application of hyperbolic geometry to address challenges in RL, notably the effective encoding of hierarchical relationships in state representations. The empirical results provide strong evidence of improved performance and generalization across diverse environments. However, the paper could benefit from a more detailed discussion of the limitations of hyperbolic representations and potential computational overheads associated with the proposed methods. Additionally, while the empirical validation is robust, further comparisons with a wider range of state-of-the-art algorithms could enhance the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with deep RL and geometry. The quality of the writing is high, and the methodology is described in sufficient detail to allow for reproducibility. The novelty of using hyperbolic geometry in RL is significant, as it offers a fresh approach to overcoming existing limitations in feature learning and generalization.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in deep reinforcement learning by effectively integrating hyperbolic geometry to improve model performance and generalization. The proposed S-RYM stabilization technique addresses critical challenges associated with hyperbolic representations, making the findings both novel and practical for future research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Hyperbolic Deep Reinforcement Learning\" introduces a novel approach to reinforcement learning (RL) by integrating hyperbolic geometry into existing frameworks. The authors propose the Spectrally-Regularized Hyperbolic Mappings (S-RYM) technique to address optimization challenges, enhancing the stability of training. Empirical validation is conducted on benchmark environments such as Procgen and Atari 100K, demonstrating significant performance improvements compared to traditional Euclidean methods. The paper also explores the implications of hierarchical feature encoding and provides an open-source implementation for reproducibility.\n\n# Strength And Weaknesses\nThe paper presents several notable strengths, including the innovative application of hyperbolic geometry, which offers a fresh perspective on representation learning in RL. The empirical validation on well-known benchmarks highlights the effectiveness of the proposed method, although the limited range of environments raises concerns about generalizability. The introduction of S-RYM effectively tackles optimization challenges, yet it may impose constraints on model expressivity. While the authors provide valuable insights into gradient behavior, the lack of a robust theoretical framework regarding the superiority of hyperbolic representations undermines the strength of their arguments. Additionally, the open-source implementation is commendable, but documentation may need to be improved for broader accessibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it easy to follow the proposed methodologies and findings. The novelty of integrating hyperbolic geometry into RL is significant, though it would benefit from a stronger theoretical underpinning. The empirical results are compelling, yet the limited scope of environments tested raises questions about the reproducibility of the results across other RL tasks. The open-source implementation enhances reproducibility, but users may require additional guidance to effectively navigate the complexities of the methods described.\n\n# Summary Of The Review\nOverall, the paper presents an exciting and novel approach to reinforcement learning by incorporating hyperbolic geometry, supported by empirical validation. However, the limitations in theoretical justification and the restricted range of environments tested may hinder the broader applicability and understanding of the proposed methods. Future work could address these gaps to solidify the contributions made.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a novel approach to deep reinforcement learning (RL) by leveraging hyperbolic geometry for the representation of hierarchical state relationships. The authors propose a new family of deep RL algorithms that utilize hyperbolic representations, addressing the limitations of traditional Euclidean methods. Key contributions include a unique stabilization technique known as \"spectrally-regularized hyperbolic mappings\" (S-RYM) and empirical validation showing significant performance improvements in various benchmarks, such as Procgen and Atari 100K, compared to standard RL algorithms.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative integration of hyperbolic geometry into RL, which is well-justified by theoretical insights and empirical evidence. The proposed S-RYM technique effectively addresses critical optimization challenges, such as exploding gradients and variance, enhancing the stability of learning in hyperbolic representations. However, the paper could benefit from broader comparisons with recent advancements in RL, which would better contextualize the advantages of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the presentation is commendable, with a well-structured narrative that facilitates understanding of complex concepts. The quality of empirical validation is robust, showcasing significant improvements over traditional methods, which enhances the paper's credibility. The novelty is substantial, as it pioneers the application of hyperbolic geometry in RL, though further exploration of reproducibility across diverse tasks would strengthen the findings.\n\n# Summary Of The Review\nThis paper presents a significant advancement in deep reinforcement learning by applying hyperbolic geometry, leading to improved performance and stability in learning. The innovative methodologies and thorough empirical validation make it a valuable contribution to the field, though it could further benefit from comparative studies with contemporary RL approaches.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel adversarial training methodology that leverages hyperbolic geometry to enhance the robustness of deep learning models against adversarial attacks. It proposes a new class of adversarial training algorithms that utilize hyperbolic space to effectively capture complex relationships within data distributions. The methodology includes a stabilization technique known as Spectrally-Regularized Hyperbolic Mappings (S-RYM), which addresses the instabilities associated with gradient estimators in adversarial contexts. The empirical findings demonstrate significant improvements in model robustness, with hyperbolic representations leading to enhanced adversarial accuracy across various benchmarks compared to traditional Euclidean approaches.\n\n# Strength And Weaknesses\n**Strengths:**\n- The introduction of hyperbolic geometry into adversarial training is a novel approach that offers theoretical and practical implications for improving model robustness.\n- The paper provides a strong theoretical foundation that explains the advantages of hyperbolic representations in capturing hierarchical relationships in data.\n- The extensive empirical validation across multiple datasets shows clear improvements in adversarial robustness, contributing valuable insights to the field.\n\n**Weaknesses:**\n- The complexity of hyperbolic geometry and the associated computational overhead may limit the accessibility of the proposed methods to practitioners.\n- Scalability concerns are raised, as the paper does not sufficiently address how the approach performs with larger datasets or more complex adversarial scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the proposed methodology and its theoretical underpinnings. The quality of writing is high, making the complex concepts of hyperbolic geometry more approachable. The novelty of the approach is significant, particularly in the context of adversarial training. However, the reproducibility of the results may be a concern due to the intricate nature of the methods and potential computational challenges highlighted in the paper.\n\n# Summary Of The Review\nThis paper presents a compelling and innovative approach to adversarial training through hyperbolic geometry, demonstrating significant improvements in model robustness. While the theoretical contributions and empirical results are strong, concerns regarding complexity and scalability may hinder practical adoption. Overall, the work opens up new avenues for research in adversarial machine learning.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel class of deep reinforcement learning (RL) algorithms that utilize hyperbolic geometry to enhance the encoding of hierarchical relationships in latent representations. Key contributions include the introduction of Hyperbolic Geometry as a foundational framework for RL, the development of Spectrally-Regularized Hyperbolic Mappings (S-RYM) for optimization, and empirical results demonstrating significant performance improvements on benchmarks such as Procgen and Atari 100K. The authors claim that their approach not only surpasses existing methods but also has the potential to redefine the future of RL research.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to integrating hyperbolic geometry into RL, which adds a fresh perspective and could provide new insights into hierarchical representations. The empirical results presented are impressive, with claims of performance improvements exceeding 1000% across various benchmarks, suggesting robust applicability. However, the paper also displays weaknesses, particularly in the overstatements regarding the implications of their findings. The claims about the complete obsolescence of previous techniques and the naive integration issues may require more nuanced discussion and acknowledgment of potential challenges in practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and communicates its ideas clearly. However, the claims made about the transformative nature of the research may detract from the overall quality, as they could lead to skepticism regarding the reproducibility of results. While the introduction of hyperbolic geometry is novel, the lack of detailed exploration of potential limitations and challenges in broader contexts diminishes the clarity of the practical implications.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for the use of hyperbolic geometry in deep reinforcement learning, with impressive empirical results that could inspire further research. However, the exaggerated claims regarding the implications and the potential for universal adoption may lead to critical scrutiny, highlighting the need for a more balanced discussion of the limitations and challenges involved.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Hyperbolic Deep Reinforcement Learning\" introduces a novel approach that integrates hyperbolic geometry into deep reinforcement learning (RL) to effectively capture hierarchical relationships among states. The authors present a method called spectrally-regularized hyperbolic mappings (S-RYM) to improve training stability in hyperbolic space. Empirical evaluations on Procgen and Atari benchmarks indicate that this hyperbolic approach enhances both the performance and generalization of RL agents, with significant improvements observed across multiple environments.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative application of hyperbolic geometry, which offers a theoretical framework for better representation of hierarchical structures in RL. The empirical results are robust, demonstrating clear performance enhancements and reduced overfitting in various benchmarks. However, a potential weakness is the reliance on specific algorithms (PPO and Rainbow DQN) for empirical validation, which may limit the generalizability of the findings to other RL algorithms. Additionally, while the S-RYM method addresses optimization stability, the paper could benefit from discussing the limitations and potential challenges of implementing hyperbolic representations in broader RL contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents its methodology and results, making it accessible to readers with a background in deep learning and reinforcement learning. The novelty of introducing hyperbolic geometry into RL is significant, as it provides a fresh perspective on how to tackle challenges related to data efficiency and generalization. The authors have made their implementation available, enhancing reproducibility. This commitment to reproducibility is commendable and supports further exploration of their proposed method.\n\n# Summary Of The Review\nOverall, the paper provides a valuable contribution to the field of deep reinforcement learning through its innovative use of hyperbolic geometry. The empirical results convincingly demonstrate the advantages of this approach, though further exploration of its applicability across different algorithms would strengthen the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel framework for representing state transitions in reinforcement learning (RL) using hyperbolic geometry, positing that these structures can capture hierarchical relationships more effectively than traditional Euclidean representations. The authors provide an empirical evaluation using benchmark environments like Procgen and Atari 100K, suggesting that hyperbolic representations improve generalization in RL tasks. However, the paper also discusses various challenges, such as optimization issues and the complexities of integrating hyperbolic layers into existing architectures.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to leveraging hyperbolic geometry for representing hierarchical relationships in RL, which could potentially enhance performance in certain scenarios. However, several weaknesses undermine its contributions: the assumption that state transitions can be strictly hierarchical may oversimplify complex relationships, and the empirical results are based on specific benchmarks that may not generalize to all RL environments. Additionally, the paper raises valid concerns about optimization challenges and the integration of hyperbolic methods, which may hinder their practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas clearly, but the exposition could benefit from a deeper exploration of the implications of hyperbolic representations in diverse environments. While the novelty of applying hyperbolic geometry in RL is notable, the reproducibility of the results is questionable due to insufficient detail regarding experimental setups and parameter choices. The authors provide code but fail to document methodologies comprehensively, which is crucial for ensuring transparency and reproducibility.\n\n# Summary Of The Review\nOverall, the paper introduces an intriguing concept of using hyperbolic geometry for RL state representation, with potential benefits in generalization. However, the assumptions made regarding hierarchical structures and the challenges of optimization and reproducibility raise significant concerns that need to be addressed for the proposed methods to gain wider acceptance in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to deep reinforcement learning (RL) that integrates hyperbolic geometry for latent representations, aiming to enhance the encoding of hierarchical state relationships. The authors introduce a new optimization method, S-RYM, which stabilizes the learning process when using hyperbolic representations. Empirical results demonstrate that this approach leads to improved performance on standard RL benchmarks such as Procgen and Atari, showcasing the efficacy of hyperbolic space in enhancing policy recovery and generalization capabilities.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative application of hyperbolic geometry to reinforcement learning, which is a relatively unexplored area that promises to address critical issues in feature representation. The empirical results provide compelling evidence of the method's effectiveness across various environments. However, a notable weakness is the complexity of the proposed optimization technique, which may present challenges for practitioners aiming to implement these methods. Additionally, while the results are promising, further exploration of the limitations and potential scalability issues is necessary.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates the rationale behind the use of hyperbolic representations in RL, making it accessible to readers familiar with the field. The quality of the writing is high, with sufficient technical detail provided to understand the methodology and its implications. The novelty of the approach is significant, as it presents a new perspective on feature representation in RL. The authors have made efforts to ensure reproducibility by providing detailed experimental setups and code, adhering to best practices in transparency.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in deep reinforcement learning by leveraging hyperbolic geometry for better state representation and optimization. The proposed methods show promising empirical results, although the complexity may hinder practical implementation. Further investigation into the limitations of the approach is warranted to maximize its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework aimed at addressing a significant problem in the machine learning domain. The authors leverage advanced mathematical techniques to propose a method that reportedly enhances performance in a specified task. Experimental results indicate that this approach outperforms existing methods, suggesting its potential for broader applications and future research inquiries.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Innovative Contribution**: The paper introduces a new framework that could significantly advance existing methodologies, promoting exploration in related areas.\n2. **Relevance**: The addressed problem is pertinent to the machine learning community, with implications for both practical implementation and theoretical understanding.\n3. **Empirical Validation**: The authors provide robust experimental results that demonstrate the efficacy of their approach against established baselines, suggesting a comprehensive evaluation process.\n4. **Clear Motivation**: The authors effectively communicate the motivations behind their work, outlining the shortcomings of previous methods and the necessity for their proposed solution.\n5. **Future Work Directions**: The discussion on potential future applications and research directions enhances the paper's value to the community.\n\n**Weaknesses:**\n1. **Lack of Detail**: Certain methodological aspects are insufficiently detailed, which may hinder understanding and replication of the work.\n2. **Generality of Findings**: The generalizability of the results across various contexts and datasets is uncertain, limiting broader applicability.\n3. **Comparative Analysis**: A more extensive comparison with additional state-of-the-art methods would bolster the validation of the claimed improvements.\n4. **Evaluation Metrics**: The rationale behind the choice of performance metrics is not thoroughly justified, which might affect the interpretation of results.\n5. **Theoretical Insights**: A more in-depth theoretical analysis of the proposed framework would strengthen the paper's contributions and lend credence to its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulates the main ideas. However, some sections could benefit from more elaboration to enhance understanding, particularly regarding the methodology and its implications. The novelty of the proposed framework is noteworthy, but the reproducibility may be hindered by the lack of detailed methodological descriptions and the need for clearer experimental setups. \n\n# Summary Of The Review\nThis paper makes a valuable contribution to the field of machine learning with its innovative framework and relevant problem-solving approach. While it presents promising results, improvements in methodological detail and comparative analysis would enhance its impact and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to deep reinforcement learning (RL) by leveraging hyperbolic geometry for latent representations. The authors propose a method called spectrally-regularized hyperbolic mappings (S-RYM) to stabilize learning in RL, addressing the issues of non-stationarity and variance in gradient estimators that arise from naive applications of hyperbolic representations. Through experiments on Procgen and Atari benchmarks, the study demonstrates significant improvements in performance and generalization capabilities of RL algorithms utilizing hyperbolic representations, thereby advocating for their broader adoption in the field.\n\n# Strength And Weaknesses\nOne of the key strengths of the paper is its innovative application of hyperbolic geometry, which aligns well with the hierarchical nature of many RL problems, potentially leading to improved generalization. The introduction of S-RYM is a noteworthy contribution that provides a practical solution to the challenges posed by hyperbolic representations in dynamic RL environments. However, a potential weakness is that the paper could benefit from a more detailed discussion on the limitations of the proposed method and its applicability to various types of RL problems beyond the tested benchmarks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its motivations, methodology, and findings. The writing is generally clear, making it accessible to readers familiar with deep reinforcement learning and geometric representations. The novelty of the approach is significant, as it introduces a new perspective on representation learning in RL. The reproducibility of the results appears to be supported by the experimental validation, although additional details on hyperparameter tuning and implementation specifics would enhance transparency.\n\n# Summary Of The Review\nOverall, the paper provides a compelling contribution to the field of deep reinforcement learning by introducing hyperbolic geometry as a framework for enhancing representation learning. The proposed S-RYM method effectively addresses key challenges in the application of hyperbolic representations, leading to improved performance in standard benchmarks. Future work could expand on the applicability of these methods to a broader range of RL tasks.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Hyperbolic Deep Reinforcement Learning\" introduces novel deep reinforcement learning (RL) algorithms that leverage hyperbolic geometry for latent state representations. The authors propose a method called Spectrally-Regularized Hyperbolic Mappings (S-RYM) to address the challenges of non-stationarity and variance in RL training. Empirical results demonstrate that using hyperbolic representations leads to significant performance improvements on benchmark environments, such as Procgen and Atari, particularly in scenarios where conventional methods struggle.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to incorporating hyperbolic geometry into deep RL, which effectively encodes hierarchical relationships in state representations and enhances generalization. The empirical validation across multiple environments provides compelling evidence of the method's effectiveness, suggesting its potential as a standard tool in RL frameworks. However, a noted weakness is the limitation regarding the expressivity of the Euclidean subnetwork, which may restrict the applicability of the proposed approach in certain contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is high, with well-structured sections that guide the reader through the methodology and results. The quality of the writing is professional and accessible, making complex concepts understandable. The novelty of using hyperbolic geometry in RL is significant, filling a gap in existing literature on hierarchical feature learning. The authors have also provided a reproducibility statement, indicating that they have shared detailed methods and implementations, which enhances confidence in the replicability of their results.\n\n# Summary Of The Review\nOverall, the paper presents a novel and significant advancement in deep reinforcement learning by effectively utilizing hyperbolic geometry to improve state representation and generalization. While the empirical results are promising, further exploration of the method's limitations will be crucial for broader applications in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Hyperbolic Deep Reinforcement Learning\" by Edoardo Cetin et al. introduces a novel approach to reinforcement learning (RL) that incorporates hyperbolic geometry to enhance the learning process. The authors develop spectrally-regularized hyperbolic mappings (S-RYM) to encode hierarchical relationships in latent space, addressing optimization challenges inherent in traditional RL methods. The empirical results demonstrate significant performance improvements on benchmark tasks such as Procgen and Atari 100K, indicating that hyperbolic representations can enhance both generalization and efficiency in RL.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach of leveraging hyperbolic geometry to tackle known limitations in reinforcement learning, particularly in terms of generalization and optimization. The methodology is well-articulated, with a clear presentation of the S-RYM framework and its implementation. However, the paper does acknowledge certain weaknesses, particularly regarding the expressivity of hyperbolic representations and potential computational overhead, which could limit practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers. The writing quality is high, with a coherent narrative that effectively communicates complex ideas. The novelty of incorporating hyperbolic representations into RL is significant, providing a fresh perspective on representation learning. The authors ensure reproducibility by providing detailed implementation guidelines and hyper-parameter settings, which enhances the reliability of their findings.\n\n# Summary Of The Review\nOverall, this paper offers a significant advancement in the field of reinforcement learning by introducing hyperbolic representations, supported by strong empirical results. Despite some limitations concerning expressivity and computational efficiency, the contributions are well-founded and relevant, making it a strong candidate for publication.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper introduces a novel class of deep reinforcement learning (RL) algorithms that leverage hyperbolic space for latent representation modeling, which aims to enhance the encoding of hierarchical state relationships. The authors identify significant instabilities in naive implementations of hyperbolic representations, primarily due to the non-stationarity inherent in RL and the high variance in gradient estimators. To address these issues, they propose a stabilization technique called Spectrally-Regularized Hyperbolic Mappings (S-RYM), which allows for stable end-to-end learning with hyperbolic representations. Empirical evaluations on Procgen and Atari 100K benchmarks demonstrate that the proposed methods outperform traditional approaches, suggesting that hyperbolic geometry may serve as a beneficial inductive bias in RL architectures.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to addressing the challenges of representation learning in RL by utilizing hyperbolic geometry, which is a departure from conventional Euclidean methods. The proposed S-RYM effectively stabilizes training, which is a crucial contribution given the optimization challenges faced in RL. However, the paper could be criticized for not thoroughly addressing the limitations of hyperbolic representations, such as expressivity constraints and computational overheads. Furthermore, while the empirical results are promising, a more extensive comparison with a broader range of state-of-the-art methods would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that guides the reader through the theoretical background, methodology, and empirical results. The novelty of applying hyperbolic geometry in deep RL is significant, as it opens new avenues for improving representation learning. The reproducibility of the results is supported by comprehensive details in the appendix regarding experimental setups, hyperparameter configurations, and benchmarking protocols. However, the paper may benefit from a more detailed discussion on implementation intricacies that could assist in reproducing the results.\n\n# Summary Of The Review\nOverall, this paper presents a novel and impactful approach to deep reinforcement learning by employing hyperbolic geometry, which addresses key challenges in representation learning. The proposed stabilization methodology is a valuable contribution, while the empirical results affirm its effectiveness. Some limitations regarding expressivity and the need for a broader comparative analysis should be addressed to further substantiate the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the application of hyperbolic geometry in deep reinforcement learning (RL), proposing a novel framework that aims to leverage hyperbolic representations for improved inductive bias. The authors introduce a stabilization technique, S-RYM, to address optimization challenges associated with hyperbolic layers. The findings indicate that while the proposed method shows some performance improvements, these enhancements are marginal compared to existing state-of-the-art approaches.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its exploration of hyperbolic geometry within the context of deep RL, which is an area of growing interest. However, the contributions are undermined by several weaknesses, including a lack of robust empirical evidence to support the claims regarding the superiority of hyperbolic representations. The reported improvements in performance are not convincing, and the paper fails to address the significant training time and computational costs associated with the proposed approach. Additionally, the authors do not adequately explore the broader implications of their method across diverse environments, limiting the generalizability of their findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but it lacks clarity in certain areas, particularly in justifying the claimed advantages of hyperbolic representations. The novelty of the approach is questionable, as previous research has already addressed similar concepts without achieving significant advancements. The reproducibility of the results is also a concern due to the absence of comprehensive ablation studies that would isolate the effects of hyperbolic geometry from other influences.\n\n# Summary Of The Review\nOverall, the paper presents an interesting concept by applying hyperbolic geometry to deep reinforcement learning; however, it falls short in providing substantial evidence for its claims and the practical applicability of its methods. The marginal performance improvements and significant computational costs raise concerns about the efficacy and efficiency of the proposed approach.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThis paper introduces an innovative approach to deep reinforcement learning (RL) by employing hyperbolic geometry to encode hierarchical relationships in data. The methodology includes the development of Spectrally-Regularized Hyperbolic Mappings (S-RYM), which stabilizes training while enhancing the performance of RL agents across various tasks. The empirical findings demonstrate significant improvements in generalization capabilities and efficiency when tested on established benchmarks such as Procgen and Atari 100K, highlighting the versatility and robustness of the proposed framework.\n\n# Strength And Weaknesses\nThe paper presents several notable strengths, including the innovative application of hyperbolic geometry, which fundamentally transforms how hierarchical structures are represented in RL. The introduction of S-RYM is a significant contribution, effectively addressing stability issues while maintaining high performance. The empirical validation across diverse environments showcases the method's adaptability and effectiveness, providing compelling evidence for its advantages over traditional Euclidean approaches. However, a potential weakness lies in the reliance on specific benchmarks for empirical validation, which may not fully capture the framework's performance across all types of tasks. Additionally, the paper could benefit from a more detailed discussion on the theoretical implications of hyperbolic geometry within the context of RL.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its contributions, making it accessible to both experts and newcomers in the field. The quality of the writing is high, with a logical flow that facilitates understanding of complex concepts. The novelty of introducing hyperbolic geometry to RL is significant, potentially inspiring further research in geometric approaches to learning. The authors have made their implementation publicly available, which enhances reproducibility and encourages collaboration within the research community.\n\n# Summary Of The Review\nOverall, this paper represents a substantial advancement in deep reinforcement learning by leveraging hyperbolic geometry. Its contributions not only enhance the performance and adaptability of RL agents but also pave the way for future exploration of geometric methodologies in AI. The work is commendable for its empirical validation and accessibility, marking it as a noteworthy addition to the literature.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a theoretical framework for integrating hyperbolic geometry into deep reinforcement learning (RL) to enhance the representation of hierarchical structures within Markov Decision Processes (MDPs). The authors argue that hyperbolic representations provide a superior inductive bias for RL agents by facilitating better encoding of tree-like relationships, which can improve policy optimization. The methodology includes the introduction of Spectrally-Regularized Hyperbolic Mappings (S-RYM) to stabilize training in hyperbolic spaces, addressing challenges such as non-stationarity and high variance in gradient estimators. The findings suggest that hyperbolic geometry can lead to improved generalization and robustness in RL models, particularly in complex environments.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its novel approach to representing hierarchical relationships in RL using hyperbolic geometry, which is theoretically supported by the authors' arguments. The introduction of S-RYM provides a practical solution to the optimization challenges inherent in hyperbolic spaces, showing potential for real-world applications. However, the paper could be strengthened by including empirical validation of the proposed methods, as the current focus is predominantly theoretical. Moreover, the limitations concerning expressivity and the fixed curvature of hyperbolic models may constrain the applicability of the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is generally good, with a coherent structure that guides the reader through the theoretical constructs and methodologies. The quality of the theoretical analysis is high, although some sections could benefit from more detailed explanations or illustrative examples to enhance understanding, particularly for readers less familiar with hyperbolic geometry. The novelty of the approach is significant, as it introduces a new perspective on representation learning in RL. However, the reproducibility of the results remains uncertain due to the lack of empirical validation and detailed experimental procedures.\n\n# Summary Of The Review\nThis paper offers a compelling theoretical framework that integrates hyperbolic geometry into deep reinforcement learning, presenting novel insights into hierarchical representation learning. While theoretically sound, the absence of empirical validation limits the assessment of its practical effectiveness. Overall, the paper is a meaningful contribution to the field, but further experimental work is needed to substantiate its claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"Hyperbolic Deep Reinforcement Learning\" presents a novel approach to deep reinforcement learning (RL) by leveraging hyperbolic geometry for latent state representation. It introduces the Spectrally-Regularized Hyperbolic Mappings (S-RYM) to address optimization challenges associated with hyperbolic representations, such as high variance and non-stationarity. The authors implement this methodology within two RL algorithms: a hyperbolic version of Proximal Policy Optimization (PPO) and Rainbow DQN. Experimental results demonstrate that the hyperbolic PPO outperforms standard PPO and competes favorably with state-of-the-art methods, while also showing improved generalization, particularly in environments with fewer levels.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its innovative application of hyperbolic geometry to improve RL performance, along with a well-defined stabilization technique (S-RYM) that effectively mitigates optimization issues. The experimental evaluation on multiple environments (Procgen and Atari) lends credibility to the findings and showcases the practical efficacy of the proposed approach. However, a potential weakness is the paper's reliance on specific hyper-parameter settings for its experiments, which may limit the generalizability of the results across other tasks or environments. Furthermore, while the methodology is sound, the complexity of the proposed framework may pose challenges for practitioners seeking to adopt hyperbolic representations in RL.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly explains the motivation behind using hyperbolic geometry in RL, as well as the technical details of the S-RYM stabilization method. The writing is generally clear, although some sections may benefit from further elaboration to enhance understanding, especially for readers less familiar with hyperbolic geometry. The reproducibility of the results is supported by the availability of code and detailed hyper-parameter settings, which facilitate implementation. The novelty of the approach is evident, as it introduces a new perspective on RL representation learning.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to deep reinforcement learning through the integration of hyperbolic geometry, demonstrating significant improvements in performance and generalization. While the methodology is robust and well-supported by experimental results, some concerns regarding the complexity and specific hyper-parameter dependencies remain.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a framework that integrates hyperbolic geometry into deep reinforcement learning (RL), claiming enhancements in performance and generalization. The authors propose spectrally-regularized hyperbolic mappings (S-RYM) to address optimization challenges within the framework. They provide experimental results demonstrating improved performance on benchmarks such as Procgen and Atari compared to standard Proximal Policy Optimization (PPO) and various state-of-the-art (SotA) algorithms.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its exploration of hyperbolic geometry in RL, which could potentially offer a new perspective in the field. However, the authors overstate the novelty of their contributions, as prior works have already addressed similar concepts without the claimed universal benefits. Notably, their methodology lacks robustness, with naive implementations resulting in poor performance when compared to existing algorithms. While they present results from specific environments, there is insufficient contextualization within the broader literature, raising concerns about the generalizability and significance of their findings. Furthermore, the proposed S-RYM stabilization technique appears to repackage existing strategies rather than introduce truly innovative approaches.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written with reasonable clarity, but it fails to adequately engage with existing literature, which weakens its contributions. The novelty of the proposed methods is questionable, and the reliance on anecdotal evidence for performance claims diminishes the overall quality of the findings. Reproducibility is not thoroughly addressed, as the paper does not provide sufficient details on experimental setups that would allow for independent verification of results.\n\n# Summary Of The Review\nWhile the paper presents an interesting approach by applying hyperbolic geometry to deep reinforcement learning, it suffers from overstatements regarding its novelty and contributions. The methodology lacks robustness, and the contextualization of results within the literature is insufficient, limiting the overall impact of the work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"HYPERBOLIC DEEP REINFORCEMENT LEARNING\" introduces a novel framework for deep reinforcement learning (RL) that utilizes hyperbolic geometry to address performance issues associated with traditional approaches. The authors propose a hyperbolic version of both on-policy and off-policy algorithms, leveraging the S-RYM method to create representations that can better capture complex hierarchical relationships in data. The findings suggest that this approach yields significant performance and efficiency benefits, although the term \"near universal performance\" necessitates further clarification.\n\n# Strength And Weaknesses\nThe main strengths of this paper lie in its innovative approach to integrating hyperbolic geometry into RL, which could offer new avenues for research and applications. The methodology is theoretically grounded, and the results demonstrate promising empirical performance across various tasks. However, the paper suffers from several clarity issues, such as vague terminology and inconsistent notation, which may hinder comprehension for readers. Additionally, the lack of detailed performance metrics makes it difficult to assess the full impact of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel contribution to the field, the clarity of the writing suffers from repetitive language and vague phrases that could be more precisely defined. The quality of mathematical notation and terminology needs improvement to enhance readability. The reproducibility aspect is somewhat addressed, yet details on specific algorithms used and performance metrics are insufficiently detailed, which can impede replication efforts by other researchers.\n\n# Summary Of The Review\nOverall, this paper presents a compelling approach to enhancing deep reinforcement learning through hyperbolic representations. However, it requires significant revisions to improve clarity and ensure that the contributions are communicated effectively. Addressing these issues would strengthen the paper's impact and accessibility to the research community.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper investigates the application of hyperbolic representations in the context of reinforcement learning (RL). The authors propose a framework that leverages the properties of hyperbolic geometry to enhance learning efficiency and representation quality. They validate their approach empirically using established benchmarks such as Procgen and Atari 100K, demonstrating improvements in performance metrics. However, the exploration of hyperbolic representations is primarily limited to RL, neglecting potential implications in other domains like supervised and unsupervised learning.\n\n# Strength And Weaknesses\nThe paper presents significant contributions in exploring hyperbolic geometry's potential within RL, particularly in terms of representation learning and performance enhancement. However, it exhibits several weaknesses, including a limited empirical evaluation scope, focusing only on Procgen and Atari 100K, which may not generalize well to other environments. The authors also fail to sufficiently address optimization challenges, scalability, and the adaptability of hyperbolic representations, limiting the practical applicability of their findings. Moreover, the lack of exploration into dynamic curvature adjustment and alternative regularization methods diminishes the robustness and versatility of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly; however, the depth of discussion concerning the implications and applications of hyperbolic representations could have been improved. The novelty in applying hyperbolic geometry to RL is noteworthy, but the paper does not sufficiently engage with more recent advances in this area or provide a comprehensive review of existing literature. Reproducibility is somewhat supported by the implementation details provided, yet the lack of discussion on future research directions and ethical implications represents a missed opportunity for enhancing the paper's overall impact.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the intersection of hyperbolic geometry and reinforcement learning but falls short in exploring broader applications and addressing significant limitations. While the proposed framework shows promise, its impact could be enhanced through a more comprehensive examination of optimization techniques, generalization strategies, and ethical considerations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel class of deep reinforcement learning (RL) algorithms that leverage hyperbolic geometry to enhance the modeling of latent representations. The proposed methodology, termed Spectrally-Regularized Hyperbolic Mappings (S-RYM), aims to address optimization issues caused by non-stationarity and variance typical in RL environments. Empirical evaluations on Procgen and Atari 100K benchmarks demonstrate that the hyperbolic representations lead to significant performance improvements and increased stability in RL tasks, validated through various statistical analyses.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative application of hyperbolic geometry to deep RL, which presents a promising avenue for addressing inherent challenges in the field. The comprehensive empirical validation across well-known benchmarks lends credibility to the findings, while the detailed statistical methodology, including δ-hyperbolicity analysis and ablation studies, provides robust support for the proposed framework. However, a potential weakness is the high variance observed in the gradients of hyperbolic PPO, which may indicate instability, even with the proposed stabilizing techniques. Furthermore, while the paper covers a range of tasks, a deeper exploration of the limitations or potential drawbacks of hyperbolic representations could enhance the discussion.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the manuscript is commendable, with a well-structured presentation of the methodology, results, and analyses. The quality of the empirical results is high, supported by thorough statistical validation. The novelty of applying hyperbolic geometry to deep RL is significant, as it opens new avenues for research in representation learning. The reproducibility of the results appears strong, as the paper provides sufficient details on experimental setups, performance metrics, and statistical methods employed.\n\n# Summary Of The Review\nOverall, the paper makes a noteworthy contribution to the field of deep reinforcement learning by introducing and validating a novel approach that utilizes hyperbolic geometry. The empirical results are compelling and well-supported by statistical analyses, although some concerns regarding gradient stability warrant further examination.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper investigates the application of hyperbolic representations in reinforcement learning (RL), addressing the challenges associated with their naive integration into existing frameworks. The authors introduce a new stabilization technique, S-RYM, to mitigate issues such as exploding gradients and non-stationarity that arise when training deep RL algorithms in a hyperbolic space. While the results indicate improved performance in certain environments, the findings reveal limitations in generalization, optimization, and expressivity, with significant computational overhead and fixed curvature constraints posing further challenges.\n\n# Strength And Weaknesses\nThe paper makes a noteworthy contribution by highlighting the specific integration challenges of hyperbolic representations in RL, which is often overlooked in existing literature. The introduction of S-RYM as a stabilization technique is a meaningful advancement, addressing some of the identified optimization challenges. However, the limitations in generalization across environments, particularly in complex tasks like \"dodgeball,\" raise concerns about the robustness of the proposed methods. Additionally, the high computational overhead and fixed curvature limitations restrict the practical applicability of the approach, while the lack of comprehensive hyperparameter tuning guidance and limited experimental scope diminish the overall impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the challenges and contributions of using hyperbolic representations in RL. However, the novelty is somewhat tempered by the limited exploration of potential solutions to avoid spurious correlations and the absence of comprehensive comparisons with other hyperbolic approaches. Reproducibility may be hindered by the lack of detailed hyperparameter tuning guidelines, which could lead to variability in results across different implementations.\n\n# Summary Of The Review\nOverall, this paper presents an important exploration of hyperbolic representations in reinforcement learning, introducing valuable techniques for addressing integration challenges. However, the findings are tempered by limitations in generalization and practical applicability, suggesting that further research is necessary to fully realize the potential of this approach in diverse RL settings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Hyperbolic Deep Reinforcement Learning\" proposes a novel approach using hyperbolic geometry to encode hierarchical relationships in deep reinforcement learning (RL). The authors introduce a method they call Spectrally-Regularized Hyperbolic Mappings (S-RYM) to address optimization challenges such as non-stationarity and high variance in RL. They claim that their approach improves generalization and stability in performance, supporting their findings with empirical validation across various tasks.\n\n# Strength And Weaknesses\nWhile the paper attempts to contribute to the understanding of hierarchical relationships in RL by leveraging hyperbolic geometry, it largely rehashes existing concepts without providing substantial new insights. The proposed methodology feels like a superficial adaptation of known techniques, and the claims of improved performance could be seen as incremental rather than groundbreaking. On the positive side, the paper is well-cited and situates its work within the broader context of RL challenges, but the lack of truly novel ideas diminishes its impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, though it occasionally relies on jargon that may obfuscate its core contributions. The clarity of the methodology is somewhat undermined by the complex terminology used, particularly with the S-RYM approach. While the empirical results are presented clearly, they do not convincingly demonstrate novelty or significant advancement over existing methods. Reproducibility is supported by the empirical validation, but the paper does not provide enough detail about its experimental setup to fully assess this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a rehashed approach to deep reinforcement learning using hyperbolic geometry, lacking significant novelty and depth. While it addresses some known challenges in the field, the contributions feel trivial and do not substantially advance the current state of knowledge in RL.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces hyperbolic geometry as an inductive bias for representation learning in deep reinforcement learning (RL). The authors propose the Spectrally-Regularized Hyperbolic Mappings (S-RYM) to address optimization challenges associated with hyperbolic representations, particularly in the context of non-stationarity and variance in RL. Through empirical experiments, the authors demonstrate that hyperbolic representations enhance generalization and sample efficiency, particularly in environments characterized by hierarchical structures. Additionally, the paper discusses the potential for dimensionality reduction in hyperbolic representations and the integration of these representations with existing RL techniques.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative application of hyperbolic geometry to RL, which offers a fresh perspective on representation learning. The S-RYM method is a noteworthy contribution that addresses significant optimization challenges. The empirical results supporting improved generalization and sample efficiency provide a solid foundation for the claims made. However, the paper could benefit from a more extensive comparison of S-RYM with other stabilization techniques, as well as a deeper exploration of the impact of hyperbolic representations across diverse environments. Additionally, while the visualizations of latent representations are valuable, further advanced visualization techniques could enhance interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers familiar with reinforcement learning and representation learning concepts. The methodology is described in sufficient detail to allow for reproducibility, although additional information on experimental settings and baseline comparisons would enhance this aspect. The novelty of the approach is significant, particularly in its application of hyperbolic geometry, which has been underexplored in the context of RL. The findings contribute meaningfully to the field, paving the way for future research.\n\n# Summary Of The Review\nOverall, this paper presents a compelling case for the use of hyperbolic geometry in reinforcement learning, offering novel insights and practical methodologies. While the contributions are significant, there are opportunities for further exploration and validation that could strengthen the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces hyperbolic representations in deep reinforcement learning (RL), specifically assessing their performance on the Procgen and Atari 100K benchmarks. The authors propose two hyperbolic algorithms: Hyperbolic PPO with S-RYM and Hyperbolic Rainbow DQN. The findings reveal that Hyperbolic PPO outperforms standard PPO in 13 out of 16 Procgen tasks, achieving remarkable improvements in specific environments. Similarly, Hyperbolic Rainbow DQN shows superior performance across 22 out of 26 Atari environments, demonstrating enhanced sample efficiency and generalization capabilities. The results suggest that hyperbolic geometry can significantly enhance RL algorithms, providing a compelling alternative to existing methods.\n\n# Strength And Weaknesses\nThe paper presents substantial contributions to the field of RL by demonstrating the efficacy of hyperbolic representations. The strengths include comprehensive experimental evaluations that highlight significant performance improvements over standard algorithms, as well as reduced generalization gaps with less training data. However, the paper could benefit from a more detailed exploration of the theoretical underpinnings of hyperbolic geometry in RL and clearer explanations of the implications of hyperbolic representations on algorithm behavior. A potential weakness is the lack of extensive comparisons with a wider variety of state-of-the-art algorithms, which could provide a more robust context for the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical flow of ideas. The methodology is described in sufficient detail, enabling reproducibility of the experiments. The novelty of applying hyperbolic geometry to RL is noteworthy, as it opens new avenues for research in representation learning. However, some technical aspects could be elaborated further to enhance understanding, particularly for readers less familiar with hyperbolic geometry.\n\n# Summary Of The Review\nOverall, the paper provides a significant advancement in the application of hyperbolic representations in deep reinforcement learning, demonstrating improved performance across several benchmarks. While the empirical results are compelling, further theoretical insights and broader comparisons would strengthen the contribution.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to reinforcement learning (RL) by introducing hyperbolic representations to improve the efficiency and performance of RL agents in complex environments. The methodology involves a theoretical framework that integrates hyperbolic geometry into the design of RL algorithms, allowing for more effective exploration and exploitation strategies. Findings indicate that agents utilizing this approach outperform traditional methods, particularly in environments with high dimensionality and sparse rewards.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative incorporation of hyperbolic geometry into RL, which addresses significant challenges in exploration and representation learning. The empirical results are compelling, showcasing marked improvements over baseline methods. However, the paper has weaknesses, including dense language and complex sentence structures that may hinder reader comprehension. Additionally, while the theoretical contributions are promising, the practical implications could be better articulated, particularly regarding the scalability of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper suffers due to lengthy paragraphs and the use of specialized jargon that may not be readily accessible to all readers. The quality of writing could be enhanced through improved structure, particularly in the introduction and conclusion. The novelty of integrating hyperbolic representations into RL is significant, though the paper would benefit from clearer definitions and explanations of key concepts, especially for non-expert readers. Reproducibility is somewhat limited by the lack of detail regarding experimental setups and the need for clearer explanations of results.\n\n# Summary Of The Review\nOverall, this paper contributes valuable insights into the integration of hyperbolic geometry in reinforcement learning, presenting promising empirical results. However, the writing quality and clarity require significant improvement to make the findings more accessible and impactful.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.403471637754214,
    -1.620406069977153,
    -1.7644629615498697,
    -1.6275592088346515,
    -1.7939150476959962,
    -1.7349813307181996,
    -1.6063568524753247,
    -1.8859458216316278,
    -1.6183853893071536,
    -1.747144916411755,
    -1.6732705976122588,
    -1.5792931914866393,
    -1.4911220268333523,
    -1.8388609904085322,
    -1.8079251079715193,
    -1.7407054490940685,
    -1.6170634314663226,
    -1.680424540518389,
    -1.628639175123297,
    -1.5434499408132747,
    -1.878967998650931,
    -1.9319236841110767,
    -1.763605602617815,
    -1.8127613695896732,
    -1.8364846534425152,
    -1.7992959095020793,
    -1.5845938300287097,
    -1.5260104426612775,
    -1.65022408647826
  ],
  "logp_cond": [
    [
      0.0,
      -2.139592494825148,
      -2.162482515403895,
      -2.162449781316528,
      -2.1490192828292094,
      -2.170083707493117,
      -2.184461603614248,
      -2.157485962249693,
      -2.1442262115103,
      -2.252005566759442,
      -2.2035256989227454,
      -2.267740304739471,
      -2.154204687803005,
      -2.1548553520029983,
      -2.148216888833442,
      -2.15768851210217,
      -2.2238395060616805,
      -2.1483654955359817,
      -2.181588362453498,
      -2.161697398284182,
      -2.159463301628139,
      -2.214424993453297,
      -2.241706404916903,
      -2.1379016613598956,
      -2.231135203990788,
      -2.162983700294795,
      -2.1667999550013177,
      -2.213502515715465,
      -2.2508568743826163
    ],
    [
      -1.270998477857236,
      0.0,
      -1.185503852670603,
      -1.1545585201086466,
      -1.164437627557168,
      -1.1580455862852679,
      -1.263575224371712,
      -1.1747973436006667,
      -1.1402509300414698,
      -1.2804486893252989,
      -1.224805468619759,
      -1.4048831819508172,
      -1.1863609744508135,
      -1.154252802148316,
      -1.1118989919218323,
      -1.129307973125331,
      -1.2731365322080934,
      -1.202715555982005,
      -1.2165565232979259,
      -1.1704530495799474,
      -1.1847254483670697,
      -1.2827523794352886,
      -1.3048585010665072,
      -1.1648544678256312,
      -1.2606532181191006,
      -1.2005828452176184,
      -1.2236654044793167,
      -1.2393978958597063,
      -1.3375492067145032
    ],
    [
      -1.3013020065018448,
      -1.2333289024818577,
      0.0,
      -1.2035804505845051,
      -1.2113366849480665,
      -1.2621309587944876,
      -1.3108152525412338,
      -1.2189156018042806,
      -1.2353386388734098,
      -1.3306221835515784,
      -1.280847855467009,
      -1.4773781663792471,
      -1.2877546982220451,
      -1.2077209464406584,
      -1.2019119684820374,
      -1.1812594761509008,
      -1.3895562952497467,
      -1.2219526457752206,
      -1.2277033762366385,
      -1.2477584918203952,
      -1.2864898788783694,
      -1.3440530893767617,
      -1.4054390732767257,
      -1.2553061358078126,
      -1.2546953499755031,
      -1.2494934818565437,
      -1.2226180237783375,
      -1.29494623291626,
      -1.408402196149703
    ],
    [
      -1.237022987627678,
      -1.0244078212690388,
      -1.0968830503505862,
      0.0,
      -1.0896385578422128,
      -1.094506254156408,
      -1.1734323009688858,
      -1.064193774284993,
      -1.0933525187513404,
      -1.2238520532647417,
      -1.1220278832650976,
      -1.346786094869398,
      -1.1009033158632269,
      -1.0699171352113115,
      -1.0722366954674003,
      -0.9147805876582802,
      -1.2036041443754746,
      -1.0939837432371773,
      -1.1811647683446587,
      -1.083598741305051,
      -1.104402490593667,
      -1.2251501176921051,
      -1.2506715975616225,
      -1.05576440665677,
      -1.198075873037659,
      -1.0902670368349545,
      -1.1161776717725453,
      -1.1354031243759322,
      -1.2869831146192767
    ],
    [
      -1.4434019142634342,
      -1.3005748726428263,
      -1.3352851841362399,
      -1.3464497333870453,
      0.0,
      -1.2572994064106007,
      -1.3863143369151638,
      -1.321916489786504,
      -1.3601619482072662,
      -1.4412977679380552,
      -1.4402094961289678,
      -1.5759364385279726,
      -1.3887134962329082,
      -1.3695712945656235,
      -1.3266066134431553,
      -1.3030257419501876,
      -1.3844302739530785,
      -1.2966452698606337,
      -1.3281622740563785,
      -1.3372264977220394,
      -1.3480424268964413,
      -1.4671861739839105,
      -1.4599630930667198,
      -1.295792874574647,
      -1.3689923728166453,
      -1.3571715863894247,
      -1.3614298186070901,
      -1.4367173755423581,
      -1.509532772639727
    ],
    [
      -1.3026250499624585,
      -1.1215490197280154,
      -1.2300169711887554,
      -1.1090263319858062,
      -1.0839798505436935,
      0.0,
      -1.197171722546979,
      -1.1316319576932141,
      -1.2062951864480604,
      -1.3346766535552552,
      -1.2744330678808997,
      -1.4621651914585387,
      -1.2120031320635964,
      -1.2083635398823824,
      -1.1514770857491397,
      -1.100848743375685,
      -1.240087742987073,
      -1.173438968308461,
      -1.2086390077162361,
      -1.1362791500770262,
      -1.18214132443485,
      -1.2937352399911723,
      -1.3355680763059514,
      -1.1327364610173984,
      -1.2623443254877986,
      -1.2055025181122274,
      -1.1670598983243226,
      -1.2936143327812706,
      -1.4013450002673116
    ],
    [
      -1.2848146840582764,
      -1.1869068416728052,
      -1.2066676930158666,
      -1.1829808524754097,
      -1.1744930849067325,
      -1.163807876892873,
      0.0,
      -1.188300315988736,
      -1.2331593684985525,
      -1.3227457258894757,
      -1.2462425371533619,
      -1.3503958796322435,
      -1.2034359612338938,
      -1.2059516357347098,
      -1.2040185653057487,
      -1.1533776097238162,
      -1.236169078289413,
      -1.2039252074738398,
      -1.1606662125894829,
      -1.1738585944365159,
      -1.2280605099240551,
      -1.2922976768639478,
      -1.3551305897190924,
      -1.1632008848094544,
      -1.2655385655657676,
      -1.2054485656654519,
      -1.1888408281088407,
      -1.2959483846365372,
      -1.3198679087705578
    ],
    [
      -1.4736632944187933,
      -1.3830426378402092,
      -1.4080197480314827,
      -1.367805272195613,
      -1.3986161427539168,
      -1.3857526914039429,
      -1.484325572684692,
      0.0,
      -1.4259195940714333,
      -1.530305802567131,
      -1.4411124449285486,
      -1.6586501120547568,
      -1.4408762187834272,
      -1.4038423328383376,
      -1.3711010379564161,
      -1.3648317625452315,
      -1.5446328741751483,
      -1.383623640813428,
      -1.4562779302939992,
      -1.4515475260933073,
      -1.4067618003900504,
      -1.5160259422171363,
      -1.5549695011390738,
      -1.3908666670147618,
      -1.473243725686037,
      -1.4618996165212261,
      -1.4609729568479677,
      -1.47959308703624,
      -1.6105694772780488
    ],
    [
      -1.1738070472659883,
      -1.0280111229667495,
      -1.1008899378081414,
      -1.0691005924119756,
      -1.07387362356386,
      -1.0802650807118332,
      -1.1654122403527498,
      -1.1021828992190994,
      0.0,
      -1.2236651757617256,
      -1.117957356738382,
      -1.33960768458794,
      -1.0874201858699155,
      -1.0632650194013622,
      -1.0678732158645012,
      -1.0782480493419981,
      -1.18175411986035,
      -1.0666203916224808,
      -1.1044599718245132,
      -1.055538496565485,
      -1.0900333912698708,
      -1.1796680995158522,
      -1.2414717373767394,
      -1.087684833778211,
      -1.1954181305886158,
      -1.07834912536576,
      -1.120909488589546,
      -1.1213786908925336,
      -1.2827493320245258
    ],
    [
      -1.4737050450928828,
      -1.3194621559762791,
      -1.3504044426255923,
      -1.3280646499910391,
      -1.337901607564475,
      -1.3840619667217071,
      -1.4659625109547845,
      -1.3040502507207428,
      -1.363959226376812,
      0.0,
      -1.3580614129115354,
      -1.4807724083851865,
      -1.3901402196355266,
      -1.3628918199233433,
      -1.3339470359146888,
      -1.2928446515105911,
      -1.3656375918946178,
      -1.3465530546777134,
      -1.3890834330910473,
      -1.4121265636823237,
      -1.3434308102852814,
      -1.4045916312174564,
      -1.3368890049164281,
      -1.4042533872965726,
      -1.3449624295583797,
      -1.3842654095151476,
      -1.4279770257896915,
      -1.3616312694563977,
      -1.4212191232876776
    ],
    [
      -1.307672437686304,
      -1.2274031259318172,
      -1.2330416547140668,
      -1.1931403041299158,
      -1.224449143037694,
      -1.1952843858785256,
      -1.2778337528032921,
      -1.2100354501446564,
      -1.1999195555850148,
      -1.2348028164875666,
      0.0,
      -1.3634893707370659,
      -1.2228912389064714,
      -1.174809459896091,
      -1.1963954241408465,
      -1.1328737296936846,
      -1.2506345027077368,
      -1.2266551822506802,
      -1.220186065481863,
      -1.1743302132826607,
      -1.1855482091683336,
      -1.2115646484604672,
      -1.306422430703126,
      -1.2380447062256077,
      -1.226324567954955,
      -1.2296118784283727,
      -1.2933544373767663,
      -1.2300005122959017,
      -1.2785540356678624
    ],
    [
      -1.3876786123961153,
      -1.3935447112978245,
      -1.3904025875270603,
      -1.393224529420872,
      -1.3583272272499232,
      -1.3670776858260516,
      -1.3731641027671093,
      -1.3686421805609432,
      -1.3933464178416526,
      -1.3769495810840886,
      -1.3891836266122148,
      0.0,
      -1.3758073395243922,
      -1.3871295936354047,
      -1.3783261393465975,
      -1.3653009982000812,
      -1.3921127003137501,
      -1.3746218701403514,
      -1.3697754579745274,
      -1.389433433251959,
      -1.3737327306933003,
      -1.373022712286153,
      -1.3896155015622123,
      -1.3970165870686273,
      -1.3582924201624837,
      -1.3540069988305412,
      -1.3942519070965953,
      -1.3910305851773044,
      -1.361285022243532
    ],
    [
      -1.0600677013401856,
      -0.982309720925049,
      -1.045271093002056,
      -0.9836885892735591,
      -1.0234611641419369,
      -0.9807837889958445,
      -1.0869571004191416,
      -1.0087174473128737,
      -0.9889275749911502,
      -1.1365424109033455,
      -1.0255222323422455,
      -1.2392772872947015,
      0.0,
      -0.9843560364041326,
      -1.0118763277441052,
      -0.9114749985848142,
      -1.142528033392366,
      -1.0118641753422764,
      -0.9965890948705579,
      -0.9607113872387063,
      -0.9789791547849044,
      -1.0995985032480224,
      -1.176791863929717,
      -1.034564602829212,
      -1.0929528001420448,
      -1.0139837276289208,
      -1.0313792882144588,
      -1.086553482915734,
      -1.1730819146498246
    ],
    [
      -1.3795717056672974,
      -1.2267947184116144,
      -1.2481237899579978,
      -1.2303686811468728,
      -1.2469051630878765,
      -1.3007088978522074,
      -1.3295308099487648,
      -1.2776095022155334,
      -1.2406806819092884,
      -1.4401449476278512,
      -1.2771705529615576,
      -1.5422943947958672,
      -1.2757202647104555,
      0.0,
      -1.2383184720507598,
      -1.2548310736291697,
      -1.4034440765772929,
      -1.2714249598168044,
      -1.2708977253003233,
      -1.2764329856665333,
      -1.3221119799362988,
      -1.3868810497291935,
      -1.4746664135439576,
      -1.2631452556726976,
      -1.3685413592828302,
      -1.2548745000570982,
      -1.324174609661969,
      -1.4013670355254122,
      -1.47422083324999
    ],
    [
      -1.2987385709373975,
      -1.1018952285498054,
      -1.2489976552042195,
      -1.2491298020504669,
      -1.1709881829600572,
      -1.2414849675481103,
      -1.3567233303090631,
      -1.1946627865970731,
      -1.2390409399981706,
      -1.3695122087605707,
      -1.3123146858706818,
      -1.5096546974755445,
      -1.2784261642160026,
      -1.2029769366932268,
      0.0,
      -1.1834717535617783,
      -1.380597977100302,
      -1.2196230829370478,
      -1.258125844287284,
      -1.1870764904649866,
      -1.2246453382613502,
      -1.323979246272471,
      -1.3368806444678787,
      -1.2275709770787164,
      -1.3153423489067162,
      -1.2204331702697298,
      -1.2652672661229116,
      -1.3171231934171708,
      -1.3743788397077938
    ],
    [
      -1.384160736730818,
      -1.2132084573561075,
      -1.2394737608183708,
      -1.1345496550938698,
      -1.273652661763463,
      -1.2167476033789566,
      -1.3165013885648291,
      -1.2424018853329282,
      -1.3034445561840724,
      -1.3851740395759482,
      -1.2802453398724798,
      -1.5057779901693764,
      -1.233704738438174,
      -1.2196959971470602,
      -1.2251615971946286,
      0.0,
      -1.401252165472518,
      -1.2621745947778575,
      -1.2366925714451893,
      -1.2614115819274117,
      -1.300519387289061,
      -1.3834576439927269,
      -1.3981521021823993,
      -1.207062391046588,
      -1.3430862230733192,
      -1.290613251565133,
      -1.2837851672792249,
      -1.3436147646703713,
      -1.4374342260658188
    ],
    [
      -1.2775243481090492,
      -1.2147536365266007,
      -1.266449390232391,
      -1.2389648865174863,
      -1.1461054733020537,
      -1.1721324037827923,
      -1.2374472037644808,
      -1.19435341492537,
      -1.2305962040141707,
      -1.268692446356881,
      -1.2176428460960547,
      -1.3778254373499361,
      -1.2572740968083767,
      -1.253360010242088,
      -1.2124792080000184,
      -1.2364177862384578,
      0.0,
      -1.2388117499982012,
      -1.235115776086046,
      -1.182174175736489,
      -1.148780923016059,
      -1.2606683130030079,
      -1.2590222626548808,
      -1.181942648291632,
      -1.1896092257652426,
      -1.2643035144214236,
      -1.1821265919655797,
      -1.2074934520179232,
      -1.3089040288344316
    ],
    [
      -1.2368495343093358,
      -1.133942194952209,
      -1.188366037106028,
      -1.154834966126873,
      -1.1102235544621324,
      -1.169904042560832,
      -1.2160661085131816,
      -1.1189292800770037,
      -1.1294900495253544,
      -1.2842664229701597,
      -1.2322330266568,
      -1.3628349690445631,
      -1.2077054408778782,
      -1.1555189781470945,
      -1.1559452280439377,
      -1.1554481534372951,
      -1.3052637108494711,
      0.0,
      -1.1867876830120587,
      -1.2139597925831354,
      -1.1607888113808857,
      -1.2710658554395844,
      -1.2606814578160106,
      -1.1822858159014051,
      -1.2428054212052173,
      -1.167356867660675,
      -1.1891759962656854,
      -1.2525222278271482,
      -1.338346232941964
    ],
    [
      -1.2447145208681376,
      -1.156124552676039,
      -1.1445265832819334,
      -1.1853377855421514,
      -1.1543006762017158,
      -1.1848098470031758,
      -1.2024248865726306,
      -1.1656937102134246,
      -1.2230876144388931,
      -1.3120108106692967,
      -1.2307414283973528,
      -1.3826642132190199,
      -1.1868241759195606,
      -1.1428616343215268,
      -1.1548446141766806,
      -1.1085773808650041,
      -1.222956838490687,
      -1.1881321315448399,
      0.0,
      -1.1548110650434615,
      -1.1552444734850436,
      -1.2297821260337405,
      -1.3083852740477422,
      -1.1933701389665587,
      -1.194852636697553,
      -1.1503886134416628,
      -1.171474508438498,
      -1.2416326801616275,
      -1.2896575615277845
    ],
    [
      -1.1801880626903543,
      -1.0708357856653696,
      -1.1027158268036183,
      -1.073457858382669,
      -1.057598930457349,
      -1.068586999013011,
      -1.1253894893620915,
      -1.1102654004363137,
      -1.0692095910937083,
      -1.242191447212057,
      -1.144179840343341,
      -1.3031821244663089,
      -1.085615835367824,
      -1.1314965266605956,
      -1.0959933022574122,
      -1.050989352657843,
      -1.1308534937884176,
      -1.1215183674380649,
      -1.0957206539783328,
      0.0,
      -1.0778921677455886,
      -1.1816373004462424,
      -1.2236910125028235,
      -1.0569664691638196,
      -1.166733527381158,
      -1.130729339414669,
      -1.070108297141387,
      -1.1334453671250566,
      -1.2545970254404426
    ],
    [
      -1.4328825778690606,
      -1.3394992078816244,
      -1.397239080627295,
      -1.4006716387621672,
      -1.4046076532971743,
      -1.3755772629458927,
      -1.4643715599833205,
      -1.3903909073081786,
      -1.3533934326725405,
      -1.5409368037402478,
      -1.4574543333934324,
      -1.602743133137778,
      -1.402888048708712,
      -1.4095306697599301,
      -1.3921292899935531,
      -1.3815156331138545,
      -1.4646881582083695,
      -1.3936403477938986,
      -1.389985483118321,
      -1.3651703754613358,
      0.0,
      -1.4705850313835902,
      -1.5202870816785756,
      -1.4384238582858209,
      -1.4214768583623667,
      -1.4327419153326215,
      -1.4495079256151133,
      -1.4185207780424085,
      -1.5615023030120723
    ],
    [
      -1.558653787664274,
      -1.44234715726151,
      -1.5358091260674982,
      -1.4897054832590952,
      -1.509359020070139,
      -1.5074073583729297,
      -1.5542750005253085,
      -1.5284908929541716,
      -1.4743892952344115,
      -1.5958792221644125,
      -1.5052031020259855,
      -1.6737057602225827,
      -1.5166478001450827,
      -1.4764118877051406,
      -1.4486503659001466,
      -1.519654531567421,
      -1.5541473114163509,
      -1.4861579227602637,
      -1.545256024680022,
      -1.5116591696240256,
      -1.5396787141098531,
      0.0,
      -1.5848713324162442,
      -1.5435976663022424,
      -1.539308655769272,
      -1.4962357174697887,
      -1.5263616279649272,
      -1.5213044920517431,
      -1.5790302601142558
    ],
    [
      -1.4818096788746156,
      -1.3799550344141165,
      -1.402160068930704,
      -1.3985767915667848,
      -1.3805682160646,
      -1.4547011052819996,
      -1.4843808963349188,
      -1.3975060122378529,
      -1.4177638111828679,
      -1.4124581574101347,
      -1.4724733012443887,
      -1.554627566041014,
      -1.4614914563668608,
      -1.4418403934003758,
      -1.3714909422528998,
      -1.3770879623954522,
      -1.4315206183286588,
      -1.4032810417526131,
      -1.4362347680243752,
      -1.4463198305941731,
      -1.4413325478952153,
      -1.4741162702923218,
      0.0,
      -1.4232231501596573,
      -1.3602926327834928,
      -1.4439822634468333,
      -1.468935318993843,
      -1.4098536202786853,
      -1.4626630048839333
    ],
    [
      -1.3829368839682858,
      -1.3119216850757407,
      -1.3409335930957684,
      -1.219952726266231,
      -1.2437442344645542,
      -1.2625899980855626,
      -1.35811069907399,
      -1.2750944625453184,
      -1.3467058087222055,
      -1.4790260010458263,
      -1.4108173097026682,
      -1.571616960094716,
      -1.334642551142641,
      -1.3438528899635025,
      -1.3225772167761458,
      -1.2307756262367942,
      -1.4008479646052954,
      -1.3249717106725287,
      -1.3390231809927988,
      -1.239144877116716,
      -1.3160815649248652,
      -1.4279801635394203,
      -1.4922273821987884,
      0.0,
      -1.4190640647489996,
      -1.3212774579526965,
      -1.2929863350909656,
      -1.3545308056631806,
      -1.5074715406934482
    ],
    [
      -1.4726111723261712,
      -1.4229789190698936,
      -1.438593559016015,
      -1.4242733290971399,
      -1.4201377401921849,
      -1.4503757026416468,
      -1.5005793725488545,
      -1.4302147860319123,
      -1.4695313408061592,
      -1.4805395905740235,
      -1.4642857368917304,
      -1.6027245280823705,
      -1.4620028643411298,
      -1.4633403377666854,
      -1.4117143683469993,
      -1.4147216213897098,
      -1.4623467102429253,
      -1.4793531758110838,
      -1.4413425424817718,
      -1.4637891378315049,
      -1.454260789903693,
      -1.4713563875509243,
      -1.4640636942167073,
      -1.4694755442755743,
      0.0,
      -1.4981112562343564,
      -1.4754625479016221,
      -1.4498650227531302,
      -1.5333824516181467
    ],
    [
      -1.3840922338814152,
      -1.3101112957669645,
      -1.3513286347614122,
      -1.2993885817859567,
      -1.3129830824780473,
      -1.3104973811086076,
      -1.39490159852737,
      -1.3317523802298097,
      -1.3046209460062188,
      -1.4786135366093673,
      -1.3942582503402083,
      -1.5537582885012486,
      -1.3159864346089674,
      -1.279933349824364,
      -1.2922030579657153,
      -1.2955150773417503,
      -1.425541490022195,
      -1.2973561215000375,
      -1.3125092744079763,
      -1.3194953332245918,
      -1.3293797179375937,
      -1.4144192423131274,
      -1.4710631212537384,
      -1.317758482028174,
      -1.385124872223553,
      0.0,
      -1.3238050630486065,
      -1.4364324846953493,
      -1.5003219646492547
    ],
    [
      -1.1619309721291275,
      -1.129131588201101,
      -1.1198907417742832,
      -1.108112154886051,
      -1.107697070931826,
      -1.1081678201541323,
      -1.1483751226423793,
      -1.1237892424092095,
      -1.1637556331662207,
      -1.2708020889835208,
      -1.180479972808719,
      -1.3721174006021821,
      -1.1114256241649016,
      -1.1292428985552259,
      -1.13538006961203,
      -1.0547099836077933,
      -1.1179891877557815,
      -1.128869603395327,
      -1.0724107555668556,
      -1.072419527739006,
      -1.1275900019919212,
      -1.2267475951270452,
      -1.2782912019039867,
      -1.119048801383942,
      -1.1456799239100157,
      -1.113622468654918,
      0.0,
      -1.1941085127883913,
      -1.2902166453776134
    ],
    [
      -1.229090187930933,
      -1.1546260861408584,
      -1.129899658487925,
      -1.1093588715767455,
      -1.1713490659165895,
      -1.1591018616060917,
      -1.233346652991146,
      -1.1388809586860165,
      -1.1395362011193353,
      -1.2270582648340278,
      -1.1474986368989277,
      -1.2928085738094446,
      -1.1622841397285666,
      -1.1720457483496962,
      -1.14181566215466,
      -1.1228582961668276,
      -1.180584886430775,
      -1.1563911726867275,
      -1.203238307692631,
      -1.1440791060688464,
      -1.1484972892542482,
      -1.1533387148388188,
      -1.183726660480252,
      -1.1453035241341376,
      -1.1736667127279767,
      -1.1978335815186345,
      -1.1597772602839462,
      0.0,
      -1.2134656677279845
    ],
    [
      -1.2791507577708743,
      -1.2716937959197436,
      -1.325836430676969,
      -1.2863685068618822,
      -1.257597030581999,
      -1.2827511269185252,
      -1.2876674174143072,
      -1.2325246017558331,
      -1.2925571454223737,
      -1.2712562270106849,
      -1.268922071859271,
      -1.289671992205477,
      -1.2773826555025936,
      -1.2807336637701632,
      -1.2473577421900854,
      -1.2615819313595198,
      -1.2951392117782436,
      -1.2773164603715783,
      -1.3066035683142196,
      -1.2758227320266637,
      -1.2629056950257576,
      -1.1769080609689524,
      -1.2398551450872737,
      -1.2916645618605835,
      -1.2462591373917145,
      -1.2622419758003818,
      -1.304942177992493,
      -1.2611526291648718,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2638791429290661,
      0.2409891223503191,
      0.24102185643768603,
      0.2544523549250046,
      0.23338793026109705,
      0.2190100341399659,
      0.24598567550452088,
      0.2592454262439139,
      0.15146607099477194,
      0.1999459388314686,
      0.13573133301474316,
      0.24926694995120924,
      0.24861628575121575,
      0.25525474892077193,
      0.2457831256520442,
      0.17963213169253356,
      0.25510614221823236,
      0.22188327530071605,
      0.24177423947003218,
      0.24400833612607498,
      0.1890466443009169,
      0.16176523283731115,
      0.26556997639431845,
      0.1723364337634261,
      0.24048793745941888,
      0.2366716827528963,
      0.18996912203874894,
      0.15261476337159774
    ],
    [
      0.3494075921199169,
      0.0,
      0.43490221730655,
      0.4658475498685064,
      0.455968442419985,
      0.4623604836918851,
      0.356830845605441,
      0.4456087263764863,
      0.48015513993568315,
      0.3399573806518541,
      0.39560060135739405,
      0.2155228880263358,
      0.4340450955263395,
      0.4661532678288369,
      0.5085070780553207,
      0.49109809685182193,
      0.34726953776905956,
      0.4176905139951479,
      0.4038495466792271,
      0.4499530203972055,
      0.43568062161008325,
      0.33765369054186434,
      0.3155475689106457,
      0.4555516021515218,
      0.3597528518580524,
      0.4198232247595346,
      0.39674066549783626,
      0.3810081741174467,
      0.2828568632626498
    ],
    [
      0.4631609550480249,
      0.531134059068012,
      0.0,
      0.5608825109653646,
      0.5531262766018032,
      0.5023320027553821,
      0.45364770900863594,
      0.5455473597455891,
      0.5291243226764599,
      0.43384077799829135,
      0.4836151060828606,
      0.2870847951706226,
      0.47670826332782457,
      0.5567420151092113,
      0.5625509930678323,
      0.5832034853989689,
      0.37490666630012304,
      0.5425103157746491,
      0.5367595853132312,
      0.5167044697294745,
      0.4779730826715003,
      0.420409872173108,
      0.359023888273144,
      0.5091568257420571,
      0.5097676115743666,
      0.514969479693326,
      0.5418449377715322,
      0.4695167286336097,
      0.3560607654001666
    ],
    [
      0.3905362212069736,
      0.6031513875656127,
      0.5306761584840654,
      0.0,
      0.5379206509924388,
      0.5330529546782437,
      0.4541269078657657,
      0.5633654345496586,
      0.5342066900833111,
      0.4037071555699099,
      0.5055313255695539,
      0.2807731139652536,
      0.5266558929714247,
      0.5576420736233401,
      0.5553225133672512,
      0.7127786211763714,
      0.42395506445917697,
      0.5335754655974743,
      0.4463944404899929,
      0.5439604675296006,
      0.5231567182409846,
      0.4024090911425464,
      0.37688761127302906,
      0.5717948021778816,
      0.4294833357969925,
      0.537292171999697,
      0.5113815370621062,
      0.4921560844587194,
      0.3405760942153748
    ],
    [
      0.35051313343256196,
      0.49334017505316985,
      0.4586298635597563,
      0.44746531430895087,
      0.0,
      0.5366156412853955,
      0.4076007107808324,
      0.4719985579094923,
      0.43375309948873,
      0.352617279757941,
      0.35370555156702843,
      0.21797860916802358,
      0.405201551463088,
      0.42434375313037265,
      0.46730843425284085,
      0.4908893057458086,
      0.4094847737429177,
      0.4972697778353625,
      0.46575277363961765,
      0.4566885499739568,
      0.4458726207995549,
      0.3267288737120857,
      0.3339519546292764,
      0.4981221731213492,
      0.42492267487935087,
      0.4367434613065715,
      0.4324852290889061,
      0.35719767215363807,
      0.28438227505626923
    ],
    [
      0.43235628075574106,
      0.6134323109901842,
      0.5049643595294442,
      0.6259549987323934,
      0.6510014801745061,
      0.0,
      0.5378096081712207,
      0.6033493730249855,
      0.5286861442701392,
      0.4003046771629444,
      0.4605482628372999,
      0.27281613925966086,
      0.5229781986546032,
      0.5266177908358172,
      0.58350424496906,
      0.6341325873425145,
      0.4948935877311267,
      0.5615423624097386,
      0.5263423230019635,
      0.5987021806411734,
      0.5528400062833496,
      0.44124609072702725,
      0.3994132544122482,
      0.6022448697008012,
      0.472637005230401,
      0.5294788126059722,
      0.567921432393877,
      0.441366997936929,
      0.33363633045088803
    ],
    [
      0.3215421684170483,
      0.4194500108025194,
      0.39968915945945804,
      0.42337599999991493,
      0.4318637675685921,
      0.44254897558245165,
      0.0,
      0.4180565364865887,
      0.3731974839767722,
      0.28361112658584897,
      0.3601143153219628,
      0.25596097284308117,
      0.4029208912414308,
      0.4004052167406149,
      0.402338287169576,
      0.45297924275150847,
      0.37018777418591164,
      0.4024316450014849,
      0.4456906398858418,
      0.4324982580388088,
      0.37829634255126954,
      0.31405917561137686,
      0.2512262627562323,
      0.4431559676658703,
      0.34081828690955707,
      0.4009082868098728,
      0.41751602436648394,
      0.3104084678387875,
      0.28648894370476685
    ],
    [
      0.41228252721283454,
      0.5029031837914186,
      0.47792607360014516,
      0.5181405494360147,
      0.48732967887771106,
      0.500193130227685,
      0.4016202489469358,
      0.0,
      0.4600262275601945,
      0.3556400190644968,
      0.4448333767030792,
      0.22729570957687106,
      0.44506960284820063,
      0.48210348879329024,
      0.5148447836752117,
      0.5211140590863963,
      0.34131294745647955,
      0.5023221808181999,
      0.4296678913376286,
      0.43439829553832054,
      0.47918402124157744,
      0.36991987941449156,
      0.330976320492554,
      0.495079154616866,
      0.41270209594559093,
      0.4240462051104017,
      0.42497286478366014,
      0.4063527345953879,
      0.27537634435357905
    ],
    [
      0.4445783420411653,
      0.5903742663404041,
      0.5174954514990122,
      0.549284796895178,
      0.5445117657432936,
      0.5381203085953203,
      0.45297314895440377,
      0.5162024900880542,
      0.0,
      0.39472021354542797,
      0.5004280325687716,
      0.2787777047192135,
      0.5309652034372381,
      0.5551203699057914,
      0.5505121734426524,
      0.5401373399651554,
      0.4366312694468035,
      0.5517649976846728,
      0.5139254174826404,
      0.5628468927416685,
      0.5283519980372828,
      0.4387172897913014,
      0.37691365193041415,
      0.5307005555289426,
      0.4229672587185378,
      0.5400362639413936,
      0.4974759007176075,
      0.49700669841462,
      0.3356360572826278
    ],
    [
      0.27343987131887215,
      0.4276827604354758,
      0.3967404737861626,
      0.41908026642071583,
      0.40924330884728,
      0.36308294969004784,
      0.2811824054569705,
      0.4430946656910122,
      0.38318569003494285,
      0.0,
      0.3890835035002196,
      0.2663725080265684,
      0.35700469677622837,
      0.38425309648841166,
      0.4131978804970662,
      0.4543002649011638,
      0.3815073245171372,
      0.4005918617340416,
      0.35806148332070764,
      0.33501835272943126,
      0.4037141061264735,
      0.3425532851942985,
      0.4102559114953268,
      0.3428915291151824,
      0.4021824868533752,
      0.3628795068966073,
      0.3191678906220634,
      0.3855136469553573,
      0.3259257931240773
    ],
    [
      0.3655981599259548,
      0.4458674716804416,
      0.44022894289819203,
      0.48013029348234304,
      0.4488214545745648,
      0.47798621173373324,
      0.3954368448089667,
      0.4632351474676024,
      0.473351042027244,
      0.4384677811246922,
      0.0,
      0.30978122687519294,
      0.4503793587057874,
      0.49846113771616785,
      0.4768751734714123,
      0.5403968679185742,
      0.42263609490452203,
      0.44661541536157867,
      0.4530845321303958,
      0.4989403843295981,
      0.48772238844392524,
      0.46170594915179164,
      0.3668481669091328,
      0.43522589138665113,
      0.4469460296573038,
      0.4436587191838861,
      0.3799161602354926,
      0.44327008531635714,
      0.3947165619443964
    ],
    [
      0.19161457909052393,
      0.18574848018881474,
      0.18889060395957902,
      0.18606866206576722,
      0.2209659642367161,
      0.21221550566058767,
      0.20612908871952995,
      0.21065101092569605,
      0.18594677364498668,
      0.20234361040255067,
      0.19010956487442443,
      0.0,
      0.2034858519622471,
      0.1921635978512346,
      0.20096705214004174,
      0.21399219328655805,
      0.18718049117288915,
      0.20467132134628785,
      0.2095177335121119,
      0.18985975823468038,
      0.205560460793339,
      0.20627047920048636,
      0.18967768992442702,
      0.18227660441801197,
      0.22100077132415552,
      0.22528619265609806,
      0.18504128439004397,
      0.1882626063093349,
      0.21800816924310729
    ],
    [
      0.4310543254931667,
      0.5088123059083033,
      0.44585093383129637,
      0.5074334375597932,
      0.4676608626914154,
      0.5103382378375078,
      0.40416492641421065,
      0.48240457952047855,
      0.502194451842202,
      0.35457961593000675,
      0.4655997944911068,
      0.2518447395386507,
      0.0,
      0.5067659904292197,
      0.4792456990892471,
      0.5796470282485381,
      0.3485939934409863,
      0.4792578514910759,
      0.49453293196279435,
      0.530410639594646,
      0.5121428720484479,
      0.3915235235853298,
      0.31433016290363525,
      0.45655742400414034,
      0.3981692266913075,
      0.4771382992044315,
      0.4597427386188935,
      0.4045685439176183,
      0.3180401121835277
    ],
    [
      0.45928928474123487,
      0.6120662719969179,
      0.5907372004505345,
      0.6084923092616594,
      0.5919558273206558,
      0.5381520925563248,
      0.5093301804597674,
      0.5612514881929989,
      0.5981803084992439,
      0.39871604278068107,
      0.5616904374469747,
      0.2965665956126651,
      0.5631407256980767,
      0.0,
      0.6005425183577724,
      0.5840299167793626,
      0.43541691383123937,
      0.5674360305917279,
      0.5679632651082089,
      0.5624280047419989,
      0.5167490104722334,
      0.4519799406793388,
      0.3641945768645747,
      0.5757157347358346,
      0.4703196311257021,
      0.583986490351434,
      0.5146863807465631,
      0.43749395488312004,
      0.36464015715854226
    ],
    [
      0.5091865370341218,
      0.7060298794217139,
      0.5589274527672998,
      0.5587953059210524,
      0.636936925011462,
      0.566440140423409,
      0.45120177766245617,
      0.6132623213744461,
      0.5688841679733487,
      0.4384128992109486,
      0.4956104221008375,
      0.2982704104959748,
      0.5294989437555166,
      0.6049481712782925,
      0.0,
      0.624453354409741,
      0.4273271308712172,
      0.5883020250344715,
      0.5497992636842353,
      0.6208486175065326,
      0.583279769710169,
      0.4839458616990482,
      0.4710444635036406,
      0.5803541308928029,
      0.4925827590648031,
      0.5874919377017895,
      0.5426578418486077,
      0.4908019145543485,
      0.43354626826372544
    ],
    [
      0.3565447123632506,
      0.527496991737961,
      0.5012316882756978,
      0.6061557940001987,
      0.4670527873306056,
      0.523957845715112,
      0.4242040605292394,
      0.4983035637611404,
      0.43726089290999615,
      0.35553140951812034,
      0.46046010922158875,
      0.23492745892469213,
      0.5070007106558945,
      0.5210094519470083,
      0.51554385189944,
      0.0,
      0.3394532836215505,
      0.478530854316211,
      0.5040128776488793,
      0.4792938671666569,
      0.4401860618050075,
      0.3572478051013417,
      0.3425533469116693,
      0.5336430580474805,
      0.3976192260207494,
      0.4500921975289356,
      0.4569202818148437,
      0.3970906844236972,
      0.3032712230282497
    ],
    [
      0.33953908335727334,
      0.40230979493972185,
      0.3506140412339316,
      0.3780985449488363,
      0.4709579581642689,
      0.4449310276835303,
      0.3796162277018418,
      0.4227100165409525,
      0.3864672274521519,
      0.34837098510944164,
      0.3994205853702679,
      0.23923799411638647,
      0.35978933465794594,
      0.3637034212242345,
      0.4045842234663042,
      0.3806456452278648,
      0.0,
      0.3782516814681214,
      0.38194765538027653,
      0.4348892557298336,
      0.46828250845026353,
      0.35639511846331473,
      0.35804116881144177,
      0.4351207831746906,
      0.42745420570107995,
      0.35275991704489895,
      0.4349368395007429,
      0.40956997944839935,
      0.308159402631891
    ],
    [
      0.4435750062090533,
      0.54648234556618,
      0.492058503412361,
      0.525589574391516,
      0.5702009860562567,
      0.510520497957557,
      0.46435843200520743,
      0.5614952604413854,
      0.5509344909930347,
      0.3961581175482294,
      0.44819151386158906,
      0.31758957147382594,
      0.47271909964051084,
      0.5249055623712946,
      0.5244793124744513,
      0.524976387081094,
      0.37516082966891795,
      0.0,
      0.49363685750633035,
      0.4664647479352537,
      0.5196357291375033,
      0.4093586850788047,
      0.4197430827023785,
      0.49813872461698394,
      0.43761911931317177,
      0.5130676728577142,
      0.4912485442527037,
      0.4279023126912409,
      0.3420783075764251
    ],
    [
      0.3839246542551593,
      0.47251462244725784,
      0.48411259184136357,
      0.4433013895811455,
      0.4743384989215811,
      0.4438293281201211,
      0.42621428855066634,
      0.4629454649098723,
      0.4055515606844038,
      0.3166283644540002,
      0.39789774672594413,
      0.24597496190427703,
      0.44181499920373635,
      0.48577754080177016,
      0.4737945609466163,
      0.5200617942582928,
      0.4056823366326099,
      0.44050704357845705,
      0.0,
      0.4738281100798354,
      0.47339470163825337,
      0.39885704908955644,
      0.32025390107555474,
      0.4352690361567382,
      0.433786538425744,
      0.47825056168163416,
      0.4571646666847988,
      0.3870064949616694,
      0.3389816135955124
    ],
    [
      0.36326187812292043,
      0.47261415514790506,
      0.44073411400965634,
      0.46999208243060564,
      0.4858510103559257,
      0.47486294180026367,
      0.4180604514511832,
      0.43318454037696097,
      0.4742403497195664,
      0.3012584936012177,
      0.3992701004699337,
      0.2402678163469658,
      0.45783410544545067,
      0.41195341415267905,
      0.4474566385558625,
      0.4924605881554316,
      0.4125964470248571,
      0.4219315733752098,
      0.44772928683494184,
      0.0,
      0.4655577730676861,
      0.3618126403670323,
      0.31975892831045116,
      0.48648347164945505,
      0.3767164134321166,
      0.4127206013986058,
      0.4733416436718876,
      0.4100045736882181,
      0.2888529153728321
    ],
    [
      0.4460854207818703,
      0.5394687907693065,
      0.48172891802363593,
      0.47829635988876373,
      0.4743603453537566,
      0.5033907357050382,
      0.4145964386676104,
      0.4885770913427523,
      0.5255745659783904,
      0.3380311949106831,
      0.42151366525749845,
      0.27622486551315295,
      0.4760799499422188,
      0.46943732889100076,
      0.48683870865737777,
      0.4974523655370764,
      0.4142798404425614,
      0.48532765085703233,
      0.48898251553260996,
      0.5137976231895951,
      0.0,
      0.4083829672673407,
      0.35868091697235527,
      0.44054414036511,
      0.4574911402885642,
      0.4462260833183094,
      0.42946007303581757,
      0.4604472206085224,
      0.3174656956388586
    ],
    [
      0.37326989644680264,
      0.48957652684956665,
      0.39611455804357854,
      0.4422182008519815,
      0.42256466404093773,
      0.424516325738147,
      0.3776486835857682,
      0.4034327911569051,
      0.4575343888766652,
      0.33604446194666426,
      0.4267205820850912,
      0.25821792388849407,
      0.415275883965994,
      0.4555117964059361,
      0.4832733182109301,
      0.4122691525436557,
      0.37777637269472586,
      0.44576576135081303,
      0.38666765943105474,
      0.42026451448705116,
      0.3922449700012236,
      0.0,
      0.3470523516948325,
      0.38832601780883436,
      0.39261502834180484,
      0.43568796664128806,
      0.4055620561461495,
      0.4106191920593336,
      0.3528934239968209
    ],
    [
      0.2817959237431995,
      0.3836505682036986,
      0.36144553368711096,
      0.36502881105103024,
      0.38303738655321506,
      0.3089044973358155,
      0.2792247062828963,
      0.3660995903799622,
      0.3458417914349472,
      0.3511474452076804,
      0.2911323013734264,
      0.20897803657680103,
      0.30211414625095423,
      0.32176520921743923,
      0.39211466036491527,
      0.3865176402223629,
      0.3320849842891562,
      0.3603245608652019,
      0.32737083459343985,
      0.31728577202364194,
      0.3222730547225998,
      0.28948933232549323,
      0.0,
      0.3403824524581578,
      0.4033129698343223,
      0.31962333917098174,
      0.2946702836239721,
      0.35375198233912974,
      0.30094259773388177
    ],
    [
      0.4298244856213873,
      0.5008396845139325,
      0.4718277764939047,
      0.5928086433234421,
      0.569017135125119,
      0.5501713715041106,
      0.4546506705156832,
      0.5376669070443547,
      0.4660555608674677,
      0.3337353685438469,
      0.401944059887005,
      0.24114440949495708,
      0.4781188184470322,
      0.46890847962617066,
      0.49018415281352734,
      0.581985743352879,
      0.4119134049843778,
      0.48778965891714443,
      0.47373818859687433,
      0.5736164924729572,
      0.496679804664808,
      0.3847812060502529,
      0.3205339873908848,
      0.0,
      0.39369730484067356,
      0.4914839116369767,
      0.5197750344987075,
      0.45823056392649253,
      0.3052898288962249
    ],
    [
      0.363873481116344,
      0.4135057343726216,
      0.39789109442650017,
      0.41221132434537533,
      0.41634691325033035,
      0.3861089508008684,
      0.3359052808936607,
      0.4062698674106029,
      0.366953312636356,
      0.3559450628684917,
      0.37219891655078485,
      0.23376012536014468,
      0.3744817891013854,
      0.37314431567582984,
      0.42477028509551595,
      0.4217630320528054,
      0.37413794319958993,
      0.3571314776314314,
      0.39514211096074336,
      0.37269551561101033,
      0.3822238635388222,
      0.36512826589159086,
      0.37242095922580787,
      0.36700910916694096,
      0.0,
      0.33837339720815884,
      0.3610221055408931,
      0.386619630689385,
      0.30310220182436853
    ],
    [
      0.41520367562066407,
      0.4891846137351148,
      0.4479672747406671,
      0.49990732771612256,
      0.486312827024032,
      0.4887985283934717,
      0.40439431097470924,
      0.4675435292722696,
      0.4946749634958605,
      0.32068237289271195,
      0.40503765916187096,
      0.24553762100083065,
      0.48330947489311193,
      0.5193625596777154,
      0.507092851536364,
      0.503780832160329,
      0.3737544194798843,
      0.5019397880020418,
      0.486786635094103,
      0.4798005762774875,
      0.4699161915644856,
      0.38487666718895186,
      0.3282327882483409,
      0.48153742747390527,
      0.41417103727852633,
      0.0,
      0.47549084645347284,
      0.36286342480672995,
      0.2989739448528246
    ],
    [
      0.42266285789958213,
      0.45546224182760864,
      0.4647030882544265,
      0.4764816751426586,
      0.4768967590968838,
      0.4764260098745774,
      0.4362187073863304,
      0.46080458761950016,
      0.420838196862489,
      0.3137917410451889,
      0.4041138572199907,
      0.21247642942652756,
      0.47316820586380803,
      0.4553509314734838,
      0.4492137604166797,
      0.5298838464209163,
      0.4666046422729282,
      0.45572422663338275,
      0.5121830744618541,
      0.5121743022897036,
      0.4570038280367885,
      0.35784623490166445,
      0.306302628124723,
      0.46554502864476777,
      0.43891390611869396,
      0.4709713613737916,
      0.0,
      0.39048531724031843,
      0.2943771846510963
    ],
    [
      0.29692025473034445,
      0.3713843565204191,
      0.39611078417335244,
      0.416651571084532,
      0.354661376744688,
      0.36690858105518576,
      0.2926637896701314,
      0.38712948397526104,
      0.3864742415419422,
      0.2989521778272497,
      0.3785118057623498,
      0.23320186885183292,
      0.3637263029327109,
      0.3539646943115813,
      0.3841947805066175,
      0.40315214649444986,
      0.3454255562305024,
      0.36961926997454997,
      0.32277213496864654,
      0.38193133659243106,
      0.3775131534070293,
      0.3726717278224587,
      0.3422837821810254,
      0.3807069185271399,
      0.35234372993330076,
      0.328176861142643,
      0.36623318237733127,
      0.0,
      0.312544774933293
    ],
    [
      0.37107332870738574,
      0.3785302905585164,
      0.32438765580129103,
      0.3638555796163778,
      0.392627055896261,
      0.3674729595597348,
      0.3625566690639528,
      0.4176994847224269,
      0.35766694105588637,
      0.37896785946757516,
      0.38130201461898894,
      0.3605520942727831,
      0.37284143097566647,
      0.36949042270809684,
      0.4028663442881746,
      0.38864215511874023,
      0.3550848747000164,
      0.3729076261066817,
      0.3436205181640404,
      0.3744013544515963,
      0.38731839145250246,
      0.4733160255093076,
      0.41036894139098634,
      0.3585595246176765,
      0.40396494908654557,
      0.38798211067787824,
      0.345281908485767,
      0.3890714573133882,
      0.0
    ]
  ],
  "row_avgs": [
    0.2212464969155008,
    0.4037622602561654,
    0.4875823164669703,
    0.49365964236116966,
    0.417198706815816,
    0.5150257753655718,
    0.37791929393834406,
    0.4313440569680437,
    0.4906134235521283,
    0.37254312573411497,
    0.44236798190592497,
    0.20013950362622257,
    0.44580733030257064,
    0.5173982604087638,
    0.5361728818991432,
    0.443449860579472,
    0.3863144509642824,
    0.47386747410074914,
    0.4257737293288061,
    0.416457462440565,
    0.44781223616917176,
    0.40463087390325175,
    0.3317968004237654,
    0.46022902335897115,
    0.37250485951594137,
    0.4370405060363071,
    0.43059373680644164,
    0.35488680872403566,
    0.3783003560138659
  ],
  "col_avgs": [
    0.38150409417190634,
    0.4766747294057809,
    0.43562770128211753,
    0.46652038320456873,
    0.46914230228211745,
    0.45312950593652157,
    0.39290632693835653,
    0.460638412350427,
    0.44236448045594035,
    0.3462010609186238,
    0.406933255386404,
    0.2551024258373046,
    0.43127126708198704,
    0.44637933514164496,
    0.46276353677531545,
    0.4897331006478724,
    0.3773175227415644,
    0.44667282428465793,
    0.4352791232865395,
    0.45626698569657786,
    0.44381294238011565,
    0.37494047757079,
    0.3417244107198587,
    0.44841653337000403,
    0.4037962152860078,
    0.4339158204057947,
    0.4264046441779878,
    0.3978055809400196,
    0.3131942362052706
  ],
  "combined_avgs": [
    0.30137529554370357,
    0.44021849483097314,
    0.4616050088745439,
    0.4800900127828692,
    0.4431705045489667,
    0.4840776406510467,
    0.3854128104383503,
    0.44599123465923535,
    0.4664889520040343,
    0.3593720933263694,
    0.4246506186461645,
    0.22762096473176358,
    0.4385392986922788,
    0.4818887977752044,
    0.49946820933722935,
    0.46659148061367217,
    0.3818159868529234,
    0.46027014919270354,
    0.4305264263076728,
    0.4363622240685714,
    0.44581258927464373,
    0.3897856757370209,
    0.3367606055718121,
    0.4543227783644876,
    0.3881505374009746,
    0.43547816322105093,
    0.42849919049221474,
    0.37634619483202764,
    0.3457472961095682
  ],
  "gppm": [
    579.903642866219,
    564.020354129541,
    582.7659716146316,
    572.2320406601566,
    566.2429027004058,
    577.5007514835357,
    602.2534545097689,
    570.9099833174755,
    582.776191606497,
    624.94952740167,
    598.7677641997436,
    661.1468392156565,
    588.8471112660176,
    578.8099321638673,
    572.5188783670526,
    556.9745590068649,
    612.6588572138562,
    579.171717807734,
    583.3520440351394,
    575.0160489722358,
    578.8172507517833,
    610.6100527619816,
    625.0386273265461,
    575.7064709779553,
    597.1127638621689,
    585.1963052138528,
    589.1649994842164,
    603.6131294990257,
    641.9590598497688
  ],
  "gppm_normalized": [
    1.3432673874126615,
    1.2522927401505306,
    1.2926139237110486,
    1.2671575481301045,
    1.2532272996155833,
    1.276608126889317,
    1.3362797474696602,
    1.2644059829930936,
    1.289826292827446,
    1.386634551577065,
    1.32641978263117,
    1.4685002643205445,
    1.3079399367095497,
    1.2855605421242529,
    1.2664934106161745,
    1.2359370980956415,
    1.3574015482816928,
    1.2820356659261738,
    1.2922513434590874,
    1.2663100562346838,
    1.2766876388785844,
    1.3468123121012983,
    1.38552896079126,
    1.2669288797570188,
    1.3179801687031905,
    1.2991783406808064,
    1.3031830447142967,
    1.3420555330647852,
    1.4195548208302184
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355,
    957,
    556,
    414,
    421,
    711,
    446,
    549,
    400,
    474,
    425,
    417,
    545,
    467,
    418,
    439,
    416,
    407,
    483,
    419,
    485,
    407,
    447,
    409,
    389,
    441,
    349,
    389,
    441,
    400,
    582,
    504,
    431,
    471,
    477,
    481,
    474,
    472,
    456,
    402,
    422,
    361,
    467,
    465,
    480,
    475,
    423,
    417,
    438,
    472,
    382,
    345,
    403,
    454,
    464,
    449,
    395,
    435,
    487,
    735,
    467,
    433,
    388,
    384,
    430,
    381,
    444,
    427,
    420,
    429,
    395,
    434,
    422,
    442,
    435,
    437,
    405,
    391,
    423,
    370,
    402,
    416,
    425,
    405,
    359,
    400,
    416,
    394,
    657,
    523,
    436,
    419,
    474,
    390,
    480,
    426,
    428,
    404,
    410,
    529,
    440,
    402,
    391,
    498,
    406,
    448,
    479,
    502,
    416,
    388,
    440,
    448,
    427,
    397,
    451,
    445,
    376
  ],
  "response_lengths": [
    3267,
    2975,
    2488,
    2384,
    2722,
    2187,
    2722,
    2430,
    2411,
    2380,
    2380,
    3016,
    2473,
    2270,
    2224,
    2821,
    2296,
    2621,
    2747,
    2720,
    2340,
    2239,
    2596,
    2477,
    2527,
    2155,
    2594,
    2480,
    2239
  ]
}