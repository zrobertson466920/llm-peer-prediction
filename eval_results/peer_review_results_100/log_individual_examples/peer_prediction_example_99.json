{
  "example_idx": 99,
  "reference": "Published as a conference paper at ICLR 2023\n\nLANGUAGE MODELLING WITH PIXELS\n\nPhillip Rust1 Jonas F. Lotz1,2 Emanuele Bugliarello1 Elizabeth Salesky3 Miryam de Lhoneux5 Desmond Elliott1,6 1University of Copenhagen 3Johns Hopkins University 5KU Leuven p.rust@di.ku.dk\n\n6Pioneer Centre for AI\n\n2ROCKWOOL Foundation Research Unit\n\nABSTRACT\n\nLanguage models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens.1 We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.\n\n1\n\nINTRODUCTION\n\nNatural language processing has rapidly progressed in recent years due to a combination of selfsupervised representation learning, i.e. pretrained language models (PLMs) like BERT (Devlin et al., 2019), GPT-3 (Brown et al., 2020), and XLM-R (Conneau et al., 2020); large unlabelled datasets; such as C4 (Raffel et al., 2020), The Pile (Gao et al., 2020); and large-scale computing power (Hirschberg & Manning, 2015). Despite this progress, these models only cover a fraction of the world’s languages, with large inequalities in performance (Pires et al., 2019; Lauscher et al., 2020), and the majority of languages are falling behind English (Joshi et al., 2020b; Bugliarello et al., 2022). Even within English, these models struggle when tasked with processing noisy inputs (Sun et al., 2020; Eger & Benz, 2020). In this paper, we show how to effectively support thousands of written languages in a single model while being robust to variations caused by character-level noise.\n\nLanguage models typically support a finite vocabulary of categorical inputs, e.g. characters, subwords or even words, and much effort has been devoted to vocabulary construction (Wan, 2022). On one end of the spectrum, a vocabulary over words has three problems: (i) it is not possible to encode out-of-vocabulary words because they lack an entry in a closed vocabulary, e.g. “doxing”, (ii) there are too many parameters in the word embedding layer, and relatedly, (iii) the normalising constant for the softmax activation in the output layer is too expensive to compute. On the other end of the spectrum, vocabularies over bytes or characters are much smaller, which leads to increased sequence lengths (Keren et al., 2022). In practice, most current models operate over inputs smaller than words but larger than characters: subword units (Sennrich et al., 2016; Kudo, 2018). Subwords prevent the problem of extremely large embedding and output layers, and support open vocabulary processing. While this is a practical solution in a monolingual context and for some languages like English, dealing with many languages with a variety of scripts will either result in a very large vocabulary or a trade-off over what is represented within a fixed number of subwords (see §5). Taken\n\n1See Appendix A for reconstructions of this abstract.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) PIXEL pretraining\n\n(b) PIXEL finetuning\n\nFigure 1: Overview of PIXEL’s architecture. Following He et al. (2022), we use a masked autoencoder with a ViT architecture and a lightweight decoder for pretraining (left). At finetuning time (right), the decoder is replaced by a task-specific classification head that sits on top of the encoder.\n\ntogether, given a language model with a finite vocabulary, there is a bottleneck in two locations: at the level of the encoding of the inputs and at the level of estimating the probability distribution over the vocabulary. We call this the vocabulary bottleneck. A language model that can handle thousands of languages needs to deal with this problem.\n\nWe propose to rethink language modelling as a visual recognition task, removing the need for a finite vocabulary. Our proposal is inspired by Salesky et al. (2021), who showed how to train a machine translation model with “visual text representations” in the encoder instead of subwords. Our Pixelbased Encoder of Language (PIXEL) is built on the Masked Autoencoding Visual Transformer (ViTMAE; He et al., 2022). ViT-MAE is a Transformer-based encoder-decoder trained to reconstruct the pixels in masked image patches. PIXEL does not have a vocabulary embedding layer; instead, text is rendered as a sequence of fixed-sized patches, which are processed using a Vision Transformer encoder (Dosovitskiy et al., 2021). PIXEL also does not have an expensive output layer when it reconstructs the pixels of the masked patches. In effect, PIXEL provides a solution to the vocabulary bottleneck without needing the prohibitively long sequences of character-based models.\n\nPIXEL is pretrained on the same data as BERT, given our computational resources. This means that it has encountered only ∼0.05% non-English text (Blevins & Zettlemoyer, 2022).2 We evaluate PIXEL on a range of syntactic and semantic tasks in 32 typologically diverse languages across 14 scripts, showing that it can rapidly adapt to new languages and unseen scripts. PIXEL is also evaluated on its ability to handle noisy text caused by orthographic attacks, where pixel-based encoding is a clear improvement over subword-based vocabularies. In lexical code-switching experiments, PIXEL performs on-par with BERT and sometimes outperforms the multilingually pretrained MBERT.\n\nPIXEL is a new type of language model that can theoretically support any language that can be typeset by a modern computer. We make the implementation, the pretrained model including intermediate training checkpoints, and the fine-tuned models freely available for the community.3\n\n2 APPROACH\n\nThe Pixel-based Encoder of Language, PIXEL, consists of three major components: a text renderer, which draws text as an image; an encoder, which encodes the unmasked regions of the image; and a decoder, which reconstructs the masked regions at the pixel level. Figure 1 provides an illustration.\n\n2.1 TEXT RENDERER\n\nThe key component of PIXEL is a text renderer that takes one or more pieces of text and renders them onto a blank RGB image x ∈ RH×W ×C. We set height H = 16 and width W = 8464 and choose\n\n2We do not claim that a language model designed to support thousands of languages should be pretrained only on English text. We expect that pretraining on an appropriate choice of another language or multilingually may provide more remarkable results. PIXEL represents an initial effort at smaller scale.\n\n3https://github.com/xplip/pixel\n\n2\n\nsoftmaxRender Text as Image1Projection + Position Embedding2⊕EncoderMy cool cat ᓚᘏᗢ sits in a beautiful box full of black beans.CLS Embedding3CLS CLS CLS MLPTrue: 0.999 / False: 0.001My cool cat ᓚᘏᗢ sits in a beautiful box full of black beans.Render Text as Image1My cat ᓚᘏᗢ enjoys eating warm oatmeal for lunch and dinner.My cat ᓚᘏᗢ enjoys eating warm oatmeal for lunch and dinner.Projection + Position Embedding2⊕CLS Embedding & Span Mask m patches 3CLS EncoderMy cat ᓚᘏᗢ enjoys eating warm oatmeal for lunch and dinner.CLS CLS DecodersoftmaxRender Text as Image1Projection + Position Embedding2⊕EncoderMy cool cat ᓚᘏᗢ sits in a beautiful box full of black beans.CLS Embedding3CLS CLS CLS MLPTrue: 0.999 / False: 0.001My cool cat ᓚᘏᗢ sits in a beautiful box full of black beans.Published as a conference paper at ICLR 2023\n\nFigure 2: Illustrative examples of our rendered text. PIXEL natively supports most writing systems, colour emoji (a), and complex text layouts such as right-to-left writing and ligatures (b). Black patches serve as separators and end-of-sequence markers. Blank patches to the right of the endof-sequence marker are treated as sequence padding. For word-level tasks, horizontal spacing can be added between words (c) so that every patch can be assigned to exactly one word (dotted lines indicate patch boundaries for demonstration).\n\nC = 3 RGB input channels, which is equivalent to a square colour image with a 368 × 368 resolution and corresponds to a sequence of 529 image patches of size 16 × 16 pixels.4 Figure 2 shows examples of text inputs rendered by the text renderer. The renderer supports (a) colour emoji and hieroglyphs scripts, (b) left-to-right and right-to-left writing systems, and (c) text that requires ligatures. Analogous to BERT, a sequence can either contain a single paragraph of text or a text pair; we use black 16 × 16 patches to serve as separators and end-of-sequence (EOS) markers. Blank (white) patches after the end-of-sequence marker are treated as padding by PIXEL, where no attention scores or losses are computed. Sequences longer than the maximum length are either truncated or split into multiple sequences. Further technical details about the renderer are provided in Appendix D.\n\n2.2 ARCHITECTURE\n\nPIXEL-base is a 112M parameter ViT-MAE architecture (He et al., 2022) with a 12-layer ViT encoder (Dosovitskiy et al., 2021) and an 8-layer Transformer decoder (Vaswani et al., 2017). The encoder has 86M parameters and the decoder has 26M parameters, respectively. The 8-layer decoder is not used for downstream tasks. We give an overview of the architecture below, with more details in Appendix E. We did not train larger PIXEL variants for lack of computational resources.\n\nPatch Embeddings The images produced by the text renderer (§2.1) are patch-wise linearly projected to obtain a sequence of patch embeddings with a 16 × 16 pixel resolution, to which fixed sinusoidal position embeddings are added.5\n\nAlgorithm 1 PIXEL Span Masking\n\nInput: #Image patches N , masking ratio R, maximum masked span length S, span length cumulative weights W = {w1, . . . , wS } Output: Masked patches M M ← ∅ repeat\n\nPatch Span Masking Instead of the random masking procedure used in ViT-MAE or block-wise masking in BEiT (Bao et al., 2022), PIXEL uses span masking with a 25% masking ratio as outlined in Algorithm 1, which masks spans of up to S = 6 consecutive image patches with a dynamic number of unmasked patches left between them. The idea behind the span masking approach, inspired by T5 (Raffel et al., 2020) and SpanBERT (Joshi et al., 2020a), is that it masks more meaningful units of text (full words or phrases) than random masking where the model more often has to fill in (parts of) individual characters, thereby encouraging PIXEL to model a higher level of abstraction. In practice, span masking was slightly more effective than random masking in early prototypes of PIXEL. This effect may be less noticeable at higher masking ratios (such as the 75% used in ViT-MAE), when random masking would more often masks consecutive patches. We found 25% masking ratio to work well for PIXEL-base, which is in line with recent findings for BERT-type models of similar size (Wettig et al., 2022). We mask spans of s ∈ {1, 2, 3, 4} patches in length, each with 20% probability, and spans of s ∈ {5, 6} patches with 10% probability each, so E(s) = 3.1.\n\ns ← randchoice({1, . . . , S}, W ) l ← randint(0, max(0, N − s)) r ← l + s if M ∩ {l − s, . . . , l − 1} = ∅ and\n\nM ∩ {r + 1, . . . , r + s} = ∅ then M ← M ∪ {l, . . . , r}\n\nuntil |M| > R · N return M\n\nend if\n\n4We chose a sequence length of 529 so that the memory requirements at maximum length are approx. equal to those of BERT. Forward and backward passes of the transformer layers at equal length are also equally fast. 5This is a fast operation that does not require the large text embedding layer found in subword-based models, saving parameters which could in theory be re-allocated to the self-attention stack. We refer to Xue et al. (2022) for a discussion regarding benefits and drawbacks of re-allocation of embedding layer weights.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nEncoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only processes unmasked patches (i.e., ≈ 396 “visible” patches at 25% masking) rather than on a sequence including mask tokens, which not only reduces memory requirements and increases training speed, but also has the advantage of not creating a mismatch between pretraining and finetuning. This mismatch would occur when training the encoder with inserted mask tokens because they are not inserted during finetuning (He et al., 2022). We also prepend the special CLS embedding to the unmasked patches.6 The resulting CLS and unmasked patches are processed by a 12-layer Transformer encoder to produce a sequence of encoder output representations.\n\nDecoder The PIXEL decoder first projects the encoder outputs into the same space as the decoder model hidden size. It then inserts learnable mask embeddings at the masked positions; these are what PIXEL tries to reconstruct at the pixel level. Fixed sinusoidal position embeddings (Vaswani et al., 2017) are added to inject order information. After processing this sequence via 8 Transformer layers, a linear projection yields patch logits. Note that the decoder does not have to compute an expensive softmax over a subword vocabulary and circumvents the question of whether to tie the subword embedding weights. PIXEL is trained with a normalised mean squared error (MSE) pixel reconstruction loss measuring the discrepancy between normalised target image patches and reconstructed patches. This loss is only computed for masked, non-blank (text) patches.\n\n2.3 PRETRAINING\n\nPIXEL-base is pretrained on a rendered version of the English Wikipedia and the Bookcorpus (Zhu et al., 2015), which is roughly equivalent to the BERT pretraining data.7 For better compute efficiency, we concatenate paragraphs until the maximum sequence length is reached, albeit not across document and book boundaries. Wikipedia has 2B words rendered into 11.4M examples and the Bookcorpus has 1.1B words rendered into 5.4M examples; in total ∼3.1B words (BERT used 3.3B) rendered into 16.8M examples.8 PIXEL is pretrained for 1M steps with batch size 256 (i.e. ∼16 epochs) using the AdamW optimizer (Kingma & Ba, 2015; Loshchilov & Hutter, 2019) with a linear warmup over the first 50k steps to a peak learning rate of 1.5e−4 and a cosine decay to a minimum learning rate of 1e−5. Pretraining took 8 days on 8×40GB Nvidia A100 GPUs. We show the loss curve and additional pretraining details in Appendix E. We stored PIXEL checkpoints every 10k steps and make them available alongside the fully trained model on the HuggingFace Hub (Wolf et al., 2020),which we hope will be useful to analyze training dynamics of PIXEL models (Sellam et al., 2022). Figure 5 in Appendix B shows, for three unseen examples, how PIXEL learns to model language over the course of pretraining.\n\n2.4 FINETUNING\n\nPIXEL can be finetuned for downstream NLP tasks in a similar fashion to BERT-like encoders by simply replacing the PIXEL decoder with a suitable classification head. By truncating or interpolating the sinusoidal position embeddings, we can finetune with sequences shorter or longer than 529 patches, respectively. The latter, in particular, is common in computer vision applications to finetune on higher resolution images (Touvron et al., 2019; Kolesnikov et al., 2020; Dosovitskiy et al., 2021; He et al., 2022). For most common NLP tasks, we can typically finetune with sequences shorter than 529 to accelerate training while retaining performance. To demonstrate that PIXEL supports a variety of downstream tasks, we conduct finetuning experiments in four settings as follows:\n\nWord Classification For word-level tasks like part-of-speech (POS) tagging and named entity recognition (NER), we render each word at the start of a new image patch so that we can create a bijective mapping between words and patches (see Figure 2 for an example).9 To finetune PIXEL on these images, we add a linear classifier with dropout. We assign the label of a word only to its first corresponding image patch and compute a cross-entropy loss with softmax.\n\nDependency Parsing For dependency parsing, we render text as above but obtain word-level representations by mean pooling over all corresponding image patches of a word and employ a biaffine parsing head (Dozat & Manning, 2017), following the implementation from Glavaš & Vuli ́c (2021).\n\n6In pretraining, no loss is computed for the CLS embedding but it can be used for finetuning. 7We use a similar Wikipedia dump Devlin et al. (2019) used for BERT (February 1, 2018) and a slightly\n\nnewer version of the Bookcorpus available at https://huggingface.co/datasets/bookcorpusopen.\n\n8This rendering is quite compact; see Appendix D. 9This particular formulation assumes that word boundaries are available. We note that subword-based and character-based models also make this assumption. For further discussion on the implications, see Appendix F.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nSequence Classification For sequence-level tasks, e.g. in GLUE (Wang et al., 2018), we render text as in pretraining. For sentence-pair tasks like natural language inference (NLI) we separate the sentences with a black patch. We finetune with different strategies, including training a classifier on top of (1) the CLS embedding, (2) the mean-pooled or max-pooled representations of all patches, (3) a multi-head attention block. Although we did not notice significant performance differences between them in our experiments, we mainly used option (1), which is exactly the same as in BERT, and (2), which has been shown to work well for image classification (Liang et al., 2022).\n\nExtractive Question Answering (QA) For extractive QA datasets like SQuAD (Rajpurkar et al., 2016), we render the question and context like in sequence-pair tasks above and, same as Devlin et al. (2019), use a sliding window approach to extract answers for examples exceeding the maximum sequence length. We use a linear classifier to predict the start and end patches of the span containing the answer. Appendix D explains how we obtain the mapping between characters and rendered text.\n\n3 EXPERIMENTS\n\nWe finetune PIXEL on common NLP tasks and evaluate its syntactic and semantic processing capabilities in English, as well as its adaptability to unseen languages. Table 8 (Appendix F) describes the languages used in these experiments, and our language and data selection is also motivated below.\n\n3.1 TASKS AND LANGUAGES\n\nSyntactic Tasks We evaluate PIXEL on part-of-speech (POS) tagging and dependency parsing using data from Universal Dependencies v2.10 treebanks (Nivre et al., 2020; Zeman et al., 2022) for a set of typologically diverse languages that captures a large variety of unseen scripts10: Arabic (ARA), Coptic (COP), English (ENG), Hindi (HIN), Japanese (JPN), Korean (KOR), Tamil (TAM), Vietnamese (VIE), Chinese (ZHO).11 We compare how well PIXEL transfers to these languages compared to BERT. Note that BERT does not support all of these writing systems. However, both models have been trained on the same data. This comparison allows us to gauge the extent to which PIXEL can overcome the script barrier and vocabulary bottleneck of subword-based models.\n\nSemantic Tasks We evaluate both monolingual (ENG) and cross-lingual word-level understanding on MasakhaNER (Adelani et al., 2021), a named entity recognition (NER) benchmark for 10 African languages (AMH, HAU, IBO, KIN, LUG, LUO, PCM, SWA, WOL, YOR), which also includes a copy of the ConLL-2003 dataset (ENG; Tjong Kim Sang & De Meulder, 2003). For monolingual ENG sentence-level understanding we rely on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016). Finally, we evaluate cross-lingual sentence-level understanding on TyDiQA-GoldP (Clark et al., 2020) in the in-language multitask setting where we train on the combined gold data in all 9 target languages (ARA, BEN, ENG, FIN, IND, KOR, RUS, SWA, TEL) at once, and on two additional larger monolingual extractive question answering (QA) corpora: KorQuAD 1.0 (KOR; Lim et al., 2019) and JaQuAD (JPN; So et al., 2022).\n\n3.2 BASELINES AND FINETUNING PROTOCOLS We compare results to BERT-base which is trained on the same data.12 We do not compare to newer monolingual English models like ROBERTA (Liu et al., 2019), T5 (Raffel et al., 2020) or DEBERTA (He et al., 2021b;a) because these models have been pretrained longer on much larger corpora.13 Likewise, we do not compare against models trained on massively multilingual corpora. However, to contextualise the performance of PIXEL in cross-lingual settings, we report results for MBERT and, if results are available, for CANINE (Clark et al., 2022). For BERT, we use the standard finetuning protocols used by Devlin et al. (2019) and the same biaffine classifier for parsing as for PIXEL. We list finetuning details for all tasks in Appendix F.\n\n3.3 RESULTS\n\nSyntactic Tasks We present results for POS tagging and dependency parsing in Table 1. While BERT is slightly better than PIXEL in the monolingual setting (ENG), PIXEL clearly outperforms\n\n10By unseen, we mean not present in the pretraining data. 11Table 10 in Appendix F gives an overview of the treebanks we use. 12We use BERT weights from https://huggingface.co/bert-base-cased. 13We do not intend to claim state-of-the-art performance, but to demonstrate that PIXEL can overcome the vocabulary bottleneck and to provide a starting point for further research on pixel-based encoding of language.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n|θ|\n\nENG\n\nARA\n\nCOP\n\nHIN\n\nJPN\n\nKOR\n\nTAM VIE\n\nZHO\n\n[UNK]% Fertility\n\nPOS Tagging (Accuracy)\n\nBERT PIXEL\n\n110M 97.2 86M 96.7\n\n95.4 95.7\n\n26.5 96.0\n\n86.4 96.3\n\n87.9 97.2\n\n60.0 94.2\n\n45.4 81.0\n\n84.5 85.7\n\n58.6 92.8\n\nDependency Parsing (LAS)\n\nBERT PIXEL\n\n110M 90.6 86M 88.7\n\n77.7 77.3\n\n13.0 83.5\n\n75.9 89.2\n\n73.8 90.7\n\n30.2 78.5\n\n15.2 52.6\n\n49.4 50.5\n\n28.8 73.7\n\nENG ARA COP HIN JPN KOR TAM VIE ZHO\n\n0 1.8 93.6 32.6 45.5 84.7 82.3 4.5 73.2\n\n1.2 3.7 1.0 2.7 1.5 1.0 1.3 2.5 1.5\n\nTable 1: Results for PIXEL and BERT finetuned for POS tagging and dependency parsing on various Universal Dependencies treebanks. We report test set results averaged over 5 runs each. |θ| denotes the number of model parameters. The table on the right shows BERT’s proportion of [UNK]s as a measure of (inverse) vocabulary coverage and fertility (i.e., number of subwords per tokenized word; Ács, 2019; Rust et al., 2021) as a measure of over-segmentation in respective UD treebanks.\n\n|θ|\n\nENG\n\nAMH\n\nHAU\n\nIBO\n\nKIN\n\nLUG\n\nLUO\n\nPCM SWA WOL\n\nYOR\n\nMBERT* CANINE-C + n-gram* CANINE-C*\n\n#L\n\n104 104 104\n\n179M 92.2 167M 89.8 127M 79.8\n\nBERT PIXEL\n\n1 1\n\n110M 92.9 86M 89.5\n\n0 50.0 44.6\n\n0 47.7\n\n87.3 88.0 76.1\n\n86.6 82.4\n\n85.3 85.0 75.6\n\n83.5 79.9\n\n72.6 72.8 58.3\n\n72.0 64.2\n\n79.3 79.6 69.4\n\n78.4 76.5\n\n73.5 74.2 63.4\n\n73.2 66.6\n\n86.4 88.7 66.6\n\n87.0 78.7\n\n87.5 83.7 72.7\n\n83.3 79.8\n\n62.2 66.5 60.7\n\n62.2 59.7\n\n80.0 79.1 67.9\n\n73.8 70.7\n\nTable 2: Results for PIXEL and BERT finetuned for NER on MasakhaNER. We report test set F1 scores averaged over 5 runs each. BERT outperforms PIXEL in all of the languages that use Latin script, whereas PIXEL does better on AMH, whose script is not covered by BERT’s vocabulary. The performance gap is smaller for languages heavier in diacritics, e.g. YOR. It is larger for languages closer to English such as Naija Pidgin (PCM), an English-based creole. #L denotes the number of pretraining languages and * indicates results taken from Clark et al. (2022) for additional context.\n\nBERT in the remaining languages. On the lower end, the accuracy gap in favor of PIXEL in ARA and VIE, both languages covered by BERT’s vocabulary, is relatively small (∼1%). On the higher end, in COP, where BERT has an out-of-vocabulary ([UNK]) token ratio of 93%, the gap is ∼70% for both tasks. There is a strong correlation14 between the proportion of [UNK]s (shown in Table 1 on the right) and the performance gap, which shows that PIXEL overcomes BERT’s vocabulary bottleneck. These results are further analysed in Appendix I.\n\nSemantic Tasks We present results for NER in Table 2, for GLUE in Table 3, for QA in Table 4. We also conduct experiments on XNLI in the translate-train-all setting which we present in Table 16 in Appendix I, for brevity. We find that BERT consistently achieves higher performance than PIXEL in its pretraining language ENG. Likewise, it often outperforms on languages using the Latin writing system; for instance in NER where all languages besides AMH use Latin script, in QA for FIN, IND and SWA. Although BERT has more trainable parameters, this finding indicates that a PIXEL model pretrained for the same number of steps as BERT is slightly worse at semantic tasks, and it may require longer pretraining or an additional inductive bias to close the performance gap. Similarly, character-based models also tend to underperform subword-based models on NER (Keren et al., 2022), here seen by the CANINE-C results. Since the addition of n-gram embeddings improves the performance of CANINE-C, likely due to boosting entity memorisation capabilities (Clark et al., 2022), we hypothesize that PIXEL may benefit from equivalent enhancements.\n\nFor languages where BERT only partially covers the script, such as KOR, JPN and TEL in QA, PIXEL consistently outperforms BERT, sometimes by large amounts (e.g. , +63 F1 points better on KorQuAD). In the extreme case where BERT has no coverage of the script whatsoever, seen in NER for AMH, BERT fails completely (0 F1) while PIXEL outperforms the larger, multilingually trained CANINE and performs competitively with its n-gram variant. In other words, PIXEL also overcomes the vocabulary bottleneck of subword-based PLMs in semantics-driven tasks. Note that although BERT was trained on English, its vocabulary has a high coverage of the Arabic script, explaining its good performance in ARA and URD.15\n\n14Pearson correlation r = 0.9, p < 0.001 for POS tagging, r = 0.95, p < 0.0001 for dependency parsing.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n|θ|\n\nMNLI-M/MM 393k\n\nBERT PIXEL\n\n110M 86M\n\n84.0 / 84.2 78.1 / 78.9\n\nQQP 364k\n\n87.6 84.5\n\nQNLI 105k\n\nSST-2 67k\n\nCOLA 8.6k\n\nSTS-B 5.8k\n\nMRPC 3.7k\n\n91.0 87.8\n\n92.6 89.6\n\n60.3 38.4\n\n88.8 81.1\n\n90.2 88.2\n\nRTE 2.5k\n\n69.5 60.5\n\nWNLI 635\n\n51.8 53.8\n\nAVG\n\n80.0 74.1\n\nTable 3: Results for PIXEL and BERT finetuned on GLUE. We report validation set performance averaged over 5 runs. The metrics are F1 score for QQP and MRPC, Matthew’s correlation for COLA, Spearman’s ρ for STS-B, and accuracy for the remaining datasets. PIXEL achieves non-trivial performance scores on GLUE, indicating pixel-based encoders can learn higher-level semantic tasks, but performs worse overall than BERT, so it may require (a) more pretraining steps than subwordtokenized PLMs or (b) additional inductive bias to acquire the same level of monolingual abstraction.\n\n#L\n\n|θ|\n\nENG\n\nARA\n\nBEN\n\nFIN\n\nIND\n\nKOR\n\nRUS\n\nSWA\n\nTEL AVG\n\nTyDiQA-GoldP\n\nSQuAD KorQuAD JaQuAD KOR\n\nENG\n\nJPN\n\nMBERT\n\n104\n\n179M 75.6\n\nBERT PIXEL\n\n1 1\n\n110M 68.5 86M 59.6\n\n78.1\n\n58.0 57.3\n\n74.7\n\n43.2 36.3\n\n75.5\n\n58.3 57.1\n\n84.3\n\n67.1 63.6\n\n64.8\n\n12.4 26.1\n\n74.9\n\n53.2 50.5\n\n83.1\n\n71.3 65.9\n\n81.6\n\n48.2 61.7\n\n77.1\n\n51.5 52.3\n\n88.6\n\n88.2 81.4\n\n90.0\n\n14.9 78.0\n\n76.4\n\n28.8 34.1\n\nTable 4: Results for PIXEL and BERT finetuned on extractive QA datasets. We report validation set F1 scores averaged over 5 runs each. Average (AVG) scores for TyDiQA-GoldP exclude ENG as customary (Clark et al., 2020). While BERT clearly outperforms PIXEL in ENG, PIXEL is much better in KOR, TEL, and JPN—a consequence of the vocabulary bottleneck in BERT—thereby gaining an edge on average. In some languages, answer span extraction adversely affects results (see §3.3).\n\nWhile the same may apply to languages like BEN and RUS in QA, where one may otherwise expect PIXEL to outperform BERT, there is an external factor at play; in the standard QA task formulation used by BERT, answer spans are extracted by predicting start and end tokens. We adopt this procedure in PIXEL for simplicity. However, an image patch will often overlap two words at variable positions, so the answer may actually start or end mid-patch. By only predicting on a full-patch level, and extracting the entire content of the patch, PIXEL will sometimes extract leading and trailing characters that should not be part of the answer, which degrades the F1 score—even though the model may have correctly identified the span. Languages not using whitespace to delimit words are particularly affected, which also explains why PIXEL is only slightly better than BERT in JPN.\n\nGenerally, and in particular when transferring to unseen scripts, we find that PIXEL performs best when finetuning on larger corpora. An example of this behaviour can be seen in QA, where PIXEL performs significantly better on KorQuAD (60k examples) than the KOR subset of TyDi (1.6k examples). While large corpora may often not be available when dealing with unseen scripts, we hypothesize that multilingual pretraining will alleviate the need for long finetuning, while potentially being even more conducive to positive transfer (Conneau et al., 2020; Chau et al., 2020; Pfeiffer et al., 2021) by not being vocabulary-bottlenecked.\n\n4 ROBUSTNESS TO ORTHOGRAPHIC ATTACKS AND CODE-SWITCHING\n\nInformal text, commonly found on social media, often contains orthographic noise such as typos and other variations (Baldwin et al., 2015; van Esch et al., 2019; Caswell et al., 2020). Previous work has demonstrated the vulnerability of pretrained language models to character-level adversarial attacks and noise (Sun et al., 2020; Eger & Benz, 2020), with text normalization typically required to maintain performance (Pruthi et al., 2019; Keller et al., 2021). To evaluate PIXEL’s robustness to textual noise and variation and inspired by the robustness tests of Salesky et al. (2021), we experiment with the Zeroé benchmark (Eger & Benz, 2020; Keller et al., 2021) which covers a variety of low-level orthographic attacks as illustrated in Table 13. We replace their version of visual attacks with the Unicode Technical Standard #39 set of visually-confusable characters.16 We apply Zeroé attacks during finetuning and evaluation of two English downstream tasks, POS tagging and NLI (Bowman et al., 2015), where we expect models to rely on different levels of abstraction.\n\n15Arabic is lexically sparse (Antoun et al., 2020; Al-Sallab et al., 2017), so the characters can be covered in the vocabulary. However, it is morphologically complex, which leads to over-segmentation, as the fertility of 3.7 in Table 1 shows. This over-segmentation is not necessarily problematic in our selection of tasks (Keren et al., 2022), e.g. due to the sliding window in QA, but can be a disadvantage in others (Rust et al., 2021).\n\n16https://util.unicode.org/UnicodeJsps/confusables.jsp\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n(a) 0%, contradiction\n\n(b) 80%, contradiction\n\n(c) 80%, entailment\n\nFigure 3: Visual explanations of correct PIXEL predictions (for classes contradiction and entailment) for NLI examples with 0% and 80% CONFUSABLE substitutions using method by Chefer et al. (2021), providing qualitative evidence for PIXEL’s robustness to character-level noise and the interpretability of its predictions. Red heatmap regions represent high relevancy.\n\nFigures 8 and 9 in Appendix G compare PIXEL and BERT across three levels of token-level noise for POS tagging and NLI. There is little impact on POS tagging performance with either model from most low-level attacks, with the exception of visually-confusable character substitutions (CONFUSABLE); here PIXEL expectedly maintains performance above 92% as it generalizes across orthographic similarities but BERT drops to 38%. For NLI, both models are negatively affected, but PIXEL exhibits less degradation than BERT with higher proportions of noise, with the impact varying across the types of attacks which each affect subword tokenization differently. Figure 3 shows relevancy heatmaps (Chefer et al., 2021) for SNLI predictions made with and without CONFUSABLE substitutions. The heatmaps are similarly clear with and without noise, providing qualitative evidence that PIXEL is indeed robust to the noise. The illustrated robustness may be dependent upon finetuning, however; we find that PIXEL can struggle in zero-shot applications when text is rendered differently from observed during pretraining (see Appendix D on using different fonts). Future work could explore the impact of data augmentation during pretraining on PIXEL’s robustness and ability to transfer across scripts. Furthermore, it would be interesting to investigate how the choice of font influences the search space during reconstruction of masked patches (Bland et al., 2022).\n\n64.0\n\n86.3\n\n97.1\n\nMBERT\n\nPOS Tagging\n\nNamed Entity Recognition SPA-ENG HIN-ENG SPA-ENG HIN-ENG MSA-EA\n\nIn addition to robustness to orthographic noise, dealing with character-level substitutions is important for effectively modelling different morphological forms. There are also many types of higher-level token, phrase or sequence-level variations such as codeswitching—when a speaker alternates between two or more languages in the same utterance, while being grammatically consistent in each language (Joshi, 1982)—or the lexical substitutions in social media text. We evaluate PIXEL on the LinCE benchmark (Aguilar et al., 2020), which includes core tasks and downstream applications for linguistic code-switching. PIXEL is fine-tuned on POS Tagging and NER in Spanish-English, Hindi-English and Modern Standard Arabic-Egyptian Arabic. Table 5 shows that PIXEL and BERT perform similarly on SPA-ENG tasks, with BERT outperforming PIXEL on NER for (romanised) HIN-ENG. On the other tasks, PIXEL performs better than BERT and even outperforms MBERT on HIN-ENG POS tagging. The gap between MBERT and PIXEL is larger on Arabic scripts, which were extensively seen by MBERT during pretraining.\n\nTable 5: Code-switching results on LINCE.\n\nBERT PIXEL\n\n96.9 96.8\n\n74.5 73.0\n\n61.1 61.0\n\n87.0 88.2\n\n59.4 63.7\n\n65.4\n\n72.6\n\n5 RELATED WORK\n\nThe question of vocabulary construction is an open problem in NLP, especially in a multilingual context.17 The most widely used language models, e.g. BERT, RoBERTa, T5, GPT-2 inter alia, rely on different tokenizers, such as WordPiece (Devlin et al., 2019), Byte-Pair Encoding (BPE; Sennrich et al., 2016) and Unigram LM (Kudo, 2018). There is an established ecosystem around subword tokenizers, such as the SentencePiece (Kudo & Richardson, 2018) and HuggingFace Tokenizers.\n\nIn a monolingual context and for some languages like English, vocabularies of subwords are a good tradeoff between vocabularies of characters and vocabularies of words. When representing a large number of languages in multilingual PLMs like mBERT and XLM-R, adequately representing the vocabulary of each individual language would be computationally prohibitive. The tokenization then becomes a bottleneck when trying to scale up to a large number of languages (Conneau et al., 2020; Rust et al., 2021), which manifests itself in degraded cross-lingual performance to languages and\n\n17See Mielke et al. (2021) for a recent, comprehensive survey on open-vocabulary modeling and tokenization.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nlanguage families that are underrepresented in the data used for training multilingual PLMs. There are large inequalities in the performance of these models across typologically diverse languages (Wu & Dredze, 2020; Lauscher et al., 2020). This issue is further exacerbated by tokenizations out-ofthe-box not being compatible across languages (Maronikolakis et al., 2021). Language imbalance and poor character coverage in the vocabulary can also decrease downstream performance (Zhang et al., 2022). To some extent, these problems can be attenuated through techniques such as subword mapping (Vernikos & Popescu-Belis, 2021), transliteration (Moosa et al., 2022), leveraging lexical overlap (Patil et al., 2022), vocabulary clustering and reallocation (Chung et al., 2020), continued or language-adaptive pretraining (Ebrahimi & Kann, 2021), adaptation via bilingual lexica (Wang et al., 2022), and embedding matrix adaptation (Artetxe et al., 2020). However, these are post-hoc workarounds to expand model vocabularies after training. They do not provide a direct solution to the vocabulary bottleneck problem.\n\nSome subword-based algorithms can also produce undesirable segmentations for morphologically rich languages (Klein & Tsarfaty, 2020; Amrhein & Sennrich, 2021), so dedicated morphologicallyaware tokenizers have been developed (e.g. Smit et al. (2014)), but this process often requires expertlevel knowledge and may only work for individual languages.\n\nDue to the limitations of subword vocabularies in multilingual language modelling, some works have used vocabularies over characters (Lee et al., 2017; Ma et al., 2020, inter alia) or bytes (Wang et al., 2020; Wei et al., 2021). These provide benefits over purely subword-based models in terms of robustness and most of them are readily applicable in a multilingual context,18 but they typically come at the cost of increased sequence lengths or latency. Also, such models cannot exploit orthographic similarities between characters across and within scripts and do not account for the fact that meaning of language may be carried visually such as in writing systems that are (partially) logographic like Chinese, in ancient hieroglyphs, or when using emoji.\n\nFinally, some works have developed pixel-based approaches. Broscheit (2018) embedded images of Chinese glyphs but still relied on a fixed vocabulary. Wu et al. (2019) combined character-level images and embeddings for a variety of Chinese tasks. Radford et al. (2021) trained a linear probe for CLIP, which also incorporates a tokenizer, on a rendered version of SST-2 (Socher et al., 2013). Other works have trained pixel-based models that removed the need for a fixed vocabulary: Sun et al. (2019) trained a convolutional sentiment classifier on pixels. Mansimov et al. (2020) used images of text for in-image MT. Salesky et al. (2021) employed a convolutional embedder for a Transformerbased MT system with a subword-based decoder. Our method differs from these in that it provides a general-purpose language encoder that completely removes the need for a vocabulary.\n\n6 CONCLUSION\n\nThis paper introduced PIXEL, a pretrained language model that renders text as images, which allows it to represent any written language that can be typeset using its text renderer. PIXEL was pretrained on the predominantly English Wikipedia and Bookcorpus datasets, and evaluated on part-of-speech tagging, dependency parsing, question answering, and language understanding tasks. The results demonstrate that PIXEL readily transfers to unseen scripts, as shown by its performance on 14 scripts across 32 languages. PIXEL currently lags behind BERT when processing languages with a Latin script, including English; however, PIXEL is more robust than BERT against low-level orthographic attacks and performs competitively to BERT and MBERT on linguistic code-switching tasks. Overall, these results show that pixel-based representations are a strong backbone for cross-lingual and crossscript transfer learning. The limitations of this work are discussed in Appendix J.\n\nIn future work, we will investigate inductive biases and additional objectives that can better capture long-range dependencies in PIXEL models. We hope that this will help overcome the limits of PIXEL in semantic processing. We also plan to pretrain PIXEL on multilingual text with a view to further improving its cross-script and cross-lingual abilities. This will also allow us to more fairly compare pixel-based models against larger subword-based and tokenization-free multilingual models. Finally, we will also develop new rendering and finetuning formulations that are better tailored to pixel-based models, e.g. for improving downstream question answering.\n\n18Character-aware models are not directly applicable to languages that do not use whitespace to delimit\n\nsentences (Tay et al., 2021), for example.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nWe thank Ákos Kádár, Barbara Plank, and Kris Cao for their comments on an earlier draft. We also thank Davide Rigoni, Rita Ramos, Stella Frank, and members of the CoAStaL and LAMP groups for discussions. Miryam de Lhoneux is funded by the Swedish Research Council (grant 2020-00437). Phillip Rust is funded by the Novo Nordisk Foundation (grant NNF 20SA0066568). Jonas F. Lotz is funded by the ROCKWOOL Foundation (grant 1242). ⋆ Emanuele Bugliarello is supported by funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 801199. Elizabeth Salesky is supported by the Apple Scholars in AI/ML fellowship. Desmond Elliott is partially supported by the Innovation Foundation (grant 0176-00013B) and the Novo Nordisk Foundation (grant NNF 20SA0066568). This work was supported by a research grant (VIL53122) from VILLUM FONDEN. The computing power was generously supported by EuroHPC grants 2010PA5869, 2021D02-068, and 2021D05-141, and with Cloud TPUs from Google’s TPU Research Cloud (TRC).\n\n⋆⋆⋆\n\n⋆ ⋆ ⋆\n\n⋆\n\n⋆\n\n⋆\n\n⋆\n\n⋆\n\nREFERENCES\n\nJudit Ács. Exploring BERT’s Vocabulary. Blog Post, 2019. URL http://juditacs.github.io/\n\n2019/02/19/bert-tokenization-stats.html.\n\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D’souza, Julia Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime, Shamsuddeen H. Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yimam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani, Rubungo Andre Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom, Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi, Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde, Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya, Mouhamadane MBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi Nwaike, Degaga Wolde, Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou, Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo, Adewale Akinfaderin, Tendai Marengereke, and Salomey Osei. MasakhaNER: Named Entity Recognition for African Languages. Transactions of the Association for Computational LinguisISSN 2307-387X. doi: 10.1162/tacl_a_00416. URL https: tics, 9:1116–1131, 10 2021. //doi.org/10.1162/tacl_a_00416.\n\nGustavo Aguilar, Sudipta Kar, and Thamar Solorio. LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation. In Proceedings of The 12th Language Resources and Evaluation Conference, pp. 1803–1813, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://www.aclweb.org/anthology/2020.lrec1.223.\n\nAhmad Al-Sallab, Ramy Baly, Hazem Hajj, Khaled Bashir Shaban, Wassim El-Hajj, and Gilbert Badaro. Aroma: A recursive deep learning model for opinion mining in arabic as a low resource language. ACM Trans. Asian Low-Resour. Lang. Inf. Process., 16(4), jul 2017. ISSN 2375-4699. doi: 10.1145/3086575. URL https://doi.org/10.1145/3086575.\n\nChantal Amrhein and Rico Sennrich. How suitable are subword segmentation strategies for In Findings of the Association for Computational translating non-concatenative morphology? Linguistics: EMNLP 2021, pp. 689–705, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.60. URL https://aclanthology.org/2021.findings-emnlp.60.\n\nWissam Antoun, Fady Baly, and Hazem Hajj. AraBERT: Transformer-based model for Arabic language understanding. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pp. 9–15, Marseille, France, May 2020. European Language Resource Association. ISBN 979-10-95546-51-1. URL https://aclanthology.org/2020.osact-1.2.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4623–4637, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.421. URL https://aclanthology.org/2020.aclmain.421.\n\nMasayuki Asahara, Hiroshi Kanayama, Takaaki Tanaka, Yusuke Miyao, Sumire Uematsu, Shinsuke Mori, Yuji Matsumoto, Mai Omura, and Yugo Murawaki. Universal Dependencies version 2 for Japanese. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL https://aclanthology.org/L18-1287.\n\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint,\n\n2016. URL http://arxiv.org/abs/1607.06450.\n\nTimothy Baldwin, Marie Catherine de Marneffe, Bo Han, Young-Bum Kim, Alan Ritter, and Wei Xu. Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition. In Proceedings of the Workshop on Noisy User-generated Text, pp. 126–135, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-4319. URL https://aclanthology.org/W15-4319.\n\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image transIn International Conference on Learning Representations, 2022. URL https://\n\nformers. openreview.net/forum?id=p-BhZSz59o4.\n\nMaxwell Troy Bland, Anushya Iyer, and Kirill Levchenko. Story beyond the eye: Glyph positions break PDF text redaction. arXiv preprint, 2022. URL https://arxiv.org/abs/2206.02285.\n\nTerra Blevins and Luke Zettlemoyer. Language contamination explains the cross-lingual capabilities of english pretrained models. arXiv preprint, 2022. URL https://doi.org/10.48550/arXiv. 2204.08110.\n\nTerra Blevins, Hila Gonen, and Luke Zettlemoyer. Analyzing the mono-and cross-lingual pretraining dynamics of multilingual language models. arXiv preprint, 2022. URL https://arxiv.org/ abs/2205.11758.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large anIn Proceedings of the 2015 Confernotated corpus for learning natural language inference. ence on Empirical Methods in Natural Language Processing, pp. 632–642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https://aclanthology.org/D15-1075.\n\nSamuel Broscheit. Learning distributional token representations from visual features. In Proceedings of The Third Workshop on Representation Learning for NLP, pp. 187–194, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-3025. URL https://aclanthology.org/W18-3025.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nEmanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo Maria Ponti, and Ivan Vuli ́c. IGLUE: A benchmark for transfer learning across modalities, tasks, and languages. In Proceedings of the 39th International Conference on Machine Learning, Balitmore, MA, July 2022. PMLR. URL https://arxiv.org/abs/2201.11732.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nIsaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. Language id in the wild: Unexpected challenges on the path to a thousand-language web text corpus. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 6588–6608, Barcelona, Spain (Online), 2020. Association for Computational Linguistics. URL https://aclanthology.org/ 2020.coling-main.579.pdf.\n\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. Parsing with multilingual BERT, a small In Findings of the Association for Computational Linguistics: corpus, and a small treebank. EMNLP 2020, pp. 1324–1334, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.118. URL https://aclanthology.org/2020. findings-emnlp.118.\n\nHila Chefer, Shir Gur, and Lior Wolf.\n\nGeneric attention-model explainability for interthe IEEE/CVF In Proceedings of preting bi-modal and encoder-decoder transformers. International Conference on Computer Vision (ICCV), pp. 397–406, October 2021. https://openaccess.thecvf.com/content/ICCV2021/papers/Chefer_Generic_ URL Attention-Model_Explainability_for_Interpreting_Bi-Modal_and_Encoder-Decoder_ Transformers_ICCV_2021_paper.pdf.\n\nJayeol Chun, Na-Rae Han, Jena D. Hwang, and Jinho D. Choi. Building Universal Dependency In Proceedings of the Eleventh International Conference on Language treebanks in Korean. Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL https://aclanthology.org/L18-1347.\n\nHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and Jason Riesa.\n\nImproving multilingual In Proceedings of the 2020 Conference on Emmodels with language-clustered vocabularies. pirical Methods in Natural Language Processing (EMNLP), pp. 4536–4546, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.367. URL https://aclanthology.org/2020.emnlp-main.367.\n\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454–470, 2020. doi: 10.1162/tacl_a_00317. URL https://aclanthology.org/2020.tacl1.30.\n\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an efficient tokenization-free encoder for language representation. Trans. Assoc. Comput. Linguistics, 10: 73–91, 2022. doi: 10.1162/tacl\\_a\\_00448. URL https://doi.org/10.1162/tacl_a_00448.\n\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2475–2485, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1269. URL https://aclanthology.org/D18-1269.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. UnIn Proceedings of the 58th Annual supervised cross-lingual representation learning at scale. Meeting of the Association for Computational Linguistics, pp. 8440–8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https: //aclanthology.org/2020.acl-main.747.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogniIn International Conference on Learning Representations, 2021. URL https: tion at scale. //openreview.net/forum?id=YicbFdNTTy.\n\nTimothy Dozat and Christopher D. Manning. Deep biaffine attention for neural dependency In 5th International Conference on Learning Representations, ICLR 2017, Toulon, parsing. France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=Hk95PK9le.\n\nAbteen Ebrahimi and Katharina Kann. How to adapt your pretrained multilingual model to 1600 languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4555–4567, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.351. URL https://aclanthology.org/2021.acl-long.351.\n\nSteffen Eger and Yannik Benz. From hero to zéroe: A benchmark of low-level adversarial attacks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pp. 786–803, Suzhou, China, December 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.aacl-main.79.\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint, 2020. URL https://arxiv.org/abs/2101.00027.\n\nGoran Glavaš and Ivan Vuli ́c. Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 3090–3104, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.270. URL https://aclanthology.org/2021.eacl-main.270.\n\nJan Hajiˇc, Otakar Smrž, Petr Zemánek, Petr Pajas, Jan Šnaidauf, Emanuel Beška, Jakub Kracmar, and Kamila Hassanová. Prague arabic dependency treebank 1.0, 2009. URL https://ufal. mff.cuni.cz/padt/PADT_1.0/docs/index.html.\n\nMasked autoencoders are scalable vision learners.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár,\n\nand Ross Girthe shick. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000–16009, URL https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_ 2022. Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf.\n\nIn Proceedings of\n\nPengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electrastyle pre-training with gradient-disentangled embedding sharing. arXiv preprint, 2021a. URL https://arxiv.org/abs/2111.09543.\n\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced BERT with disentangled attention. In International Conference on Learning Representations, 2021b. URL https://openreview.net/forum?id=XPZIaotutsD.\n\nDan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian\n\nerror linear units. arXiv preprint, 2016. URL http://arxiv.org/abs/1606.08415.\n\nJulia Hirschberg and Christopher D Manning. Advances in natural language processing. Science, 349(6245):261–266, 2015. URL https://cs224d.stanford.edu/papers/advances.pdf.\n\nAravind K. Joshi. Processing of sentences with intra-sentential code-switching. In Coling 1982: Proceedings of the Ninth International Conference on Computational Linguistics, 1982. URL https://aclanthology.org/C82-1023.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64–77, 2020a. doi: 10.1162/tacl_a_00300. URL https://aclanthology.org/2020.tacl-1.5.\n\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6282–6293, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.560. URL https: //aclanthology.org/2020.acl-main.560.\n\nYannik Keller, Jan Mackensen, and Steffen Eger. BERT-defense: A probabilistic model based In Findings of the on BERT to combat cognitively inspired orthographic adversarial attacks. Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 1616–1629, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.141. URL https://aclanthology.org/2021.findings-acl.141.\n\nOmri Keren, Tal Avinari, Reut Tsarfaty, and Omer Levy. Breaking character: Are subwords good enough for mrls after all? arXiv preprint, 2022. URL https://arxiv.org/abs/2204.04748.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n\nIn Yoshua Bengio and Yann LeCun (eds.), Proceedings of the 3rd International Conference on Learning Representations (ICLR), San Diego, CA, USA, 2015. URL http://arxiv.org/abs/1412.6980.\n\nStav Klein and Reut Tsarfaty. Getting the ##life out of living: How adequate are word-pieces for modelling complex morphology? In Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pp. 204–209, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.sigmorphon-1.24. URL https://aclanthology.org/2020.sigmorphon-1.24.\n\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 491–507, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58558-7. URL https://link.springer.com/chapter/10.1007/978-3-030-58558-7_29.\n\nTaku Kudo. Subword regularization: Improving neural network translation models with multiIn Proceedings of the 56th Annual Meeting of the Association for ple subword candidates. Computational Linguistics (Volume 1: Long Papers), pp. 66–75, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1007. URL https: //aclanthology.org/P18-1007.\n\nTaku Kudo and John Richardson.\n\nSentencePiece: A simple and language independent subIn Proceedings of the 2018 Conword tokenizer and detokenizer for neural text processing. ference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012.\n\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ́c, and Goran Glavaš. From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4483– 4499, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.emnlp-main.363. URL https://aclanthology.org/2020.emnlp-main.363.\n\nJason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine translation without explicit segmentation. Transactions of the Association for Computational Linguistics, 5: 365–378, 2017. doi: 10.1162/tacl_a_00067. URL https://aclanthology.org/Q17-1026.\n\nFeng Liang, Yangguang Li, and Diana Marculescu. Supmae: Supervised masked autoencoders are efficient vision learners. arXiv preprint, 2022. URL https://arxiv.org/abs/2205.14540.\n\nSeungyoung Lim, Myungji Kim, and Jooyoul Lee. Korquad1.0: Korean QA dataset for machine\n\nreading comprehension. arXiv preprint, 2019. URL http://arxiv.org/abs/1909.07005.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. arXiv preprint, 2019. URL http://arxiv.org/abs/1907.11692.\n\nIlya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/ forum?id=Skq89Scxx.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the 7th International Conference on Learning Representations (ICLR), New Orleans, LA, USA, 2019. OpenReview.net. URL https://openreview.net/forum?id=Bkg6RiCqY7.\n\nWentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin Wang, and Guoping Hu. CharBERT: In Proceedings of the 28th International ConCharacter-aware pre-trained language model. ference on Computational Linguistics, pp. 39–50, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.4. URL https://aclanthology.org/2020.coling-main.4.\n\nElman Mansimov, Mitchell Stern, Mia Chen, Orhan Firat, Jakob Uszkoreit, and Puneet Jain. ToIn Proceedings of the First Internawards end-to-end in-image neural machine translation. tional Workshop on Natural Language Processing Beyond Text, pp. 70–74, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.nlpbt-1.8. URL https://aclanthology.org/2020.nlpbt-1.8.\n\nAntonis Maronikolakis, Philipp Dufter, and Hinrich Schütze. Wine is not v i n. on the compatibilIn Findings of the Association for Computational Linity of tokenizations across languages. guistics: EMNLP 2021, pp. 2382–2399, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.205. URL https://aclanthology.org/2021.findings-emnlp.205.\n\nSabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja, Chenglei Si, Wilson Y. Lee, Benoît Sagot, and Samson Tan. Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP. arXiv preprint, 2021. URL https://arxiv.org/abs/2112.10508.\n\nIbraheem Muhammad Moosa, Mahmud Elahi Akhter, and Ashfia Binte Habib. Does transliteration help multilingual language modeling? arXiv preprint, 2022. URL https://arxiv.org/abs/ 2201.12501.\n\nPhuong-Thai Nguyen, Xuan-Luong Vu, Thi-Minh-Huyen Nguyen, Van-Hiep Nguyen, and HongPhuong Le. Building a large syntactically-annotated corpus of Vietnamese. In Proceedings of the Third Linguistic Annotation Workshop (LAW III), pp. 182–185, Suntec, Singapore, August 2009. Association for Computational Linguistics. URL https://aclanthology.org/W09-3035.\n\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajiˇc, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the 12th Language Resources and Evaluation Conference, pp. 4034–4043, Marseille, France, May 2020. European ISBN 979-10-95546-34-4. URL https://aclanthology. Language Resources Association. org/2020.lrec-1.497.\n\nMartha Palmer, Owen Rambow, Rajesh Bhatt, Dipti Misra Sharma, Bhuvana Narasimhan, and F. Xia. Hindi syntax: Annotating dependency, lexical predicate-argument structure, and phrase structure. In Proceedings of ICON-2009: 7th International Conference on Natural Language Processing, India, 2009. Macmillan Publishers. URL http://cdn.iiit.ac.in/cdn/ltrc.iiit. ac.in/hutb_release/related_publications/ICON09.pdf.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperaIn Hanna M. Wallach, Hugo Larochelle, tive style, high-performance deep learning library.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nAlina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract.html.\n\nVaidehi Patil, Partha Talukdar, and Sunita Sarawagi. Overlap-based vocabulary generation improves cross-lingual transfer among related languages. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 219–233, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long. 18. URL https://aclanthology.org/2022.acl-long.18.\n\nJonas Pfeiffer, Ivan Vuli ́c, Iryna Gurevych, and Sebastian Ruder. UNKs everywhere: Adapting multilingual language models to new scripts. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10186–10203, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.800. URL https://aclanthology.org/2021.emnlp-main.800.\n\nTelmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual BERT?\n\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4996–5001, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1493. URL https://aclanthology.org/P19-1493.\n\nDanish Pruthi, Bhuwan Dhingra, and Zachary C. Lipton. Combating adversarial misspellings with robust word recognition. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5582–5591, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1561. URL https://aclanthology.org/P19-1561.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 8748–8763. PMLR, 2021. URL http://proceedings.mlr.press/ v139/radford21a.html.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified textto-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/ D16-1264.\n\nLoganathan Ramasamy and Zdenˇek Žabokrtský. Prague dependency style treebank for Tamil. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pp. 1888–1894, Istanbul, Turkey, May 2012. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2012/pdf/456_Paper. pdf.\n\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli ́c, Sebastian Ruder, and Iryna Gurevych. How good is In Proyour tokenizer? on the monolingual performance of multilingual language models. ceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3118–3135, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.243. URL https://aclanthology.org/2021.acl-long.243.\n\nElizabeth Salesky, David Etter, and Matt Post. Robust open-vocabulary translation from visual text representations. In Proceedings of the 2021 Conference on Empirical Methods in Natural\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nLanguage Processing, pp. 7235–7252, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.576. URL https://aclanthology.org/2021.emnlp-main.576.\n\nThibault Sellam, Steve Yadlowsky, Ian Tenney, Jason Wei, Naomi Saphra, Alexander D’Amour, Tal Linzen, Jasmijn Bastings, Iulia Raluca Turc, Jacob Eisenstein, Dipanjan Das, and Ellie Pavlick. The multiBERTs: BERT reproductions for robustness analysis. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=K0E_F0gFDgA.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715–1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/ P16-1162.\n\nMo Shen, Ryan McDonald, Daniel Zeman, and Peng Qi. Ud_chinese-gsd. GitHub repository, 2016.\n\nURL https://github.com/UniversalDependencies/UD_Chinese-GSD.\n\nNatalia Silveira, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning. A gold standard dependency corpus for English. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pp. 2897–2904, Reykjavik, Iceland, May 2014. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf.\n\nPeter Smit, Sami Virpioja, Stig-Arne Grönroos, and Mikko Kurimo. Morfessor 2.0: Toolkit In Proceedings of the Demonstrations at the 14th for statistical morphological segmentation. Conference of the European Chapter of the Association for Computational Linguistics, pp. doi: 21–24, Gothenburg, Sweden, April 2014. Association for Computational Linguistics. 10.3115/v1/E14-2006. URL https://aclanthology.org/E14-2006.\n\nByungHoon So, Kyuhong Byun, Kyungwon Kang, and Seongjin Cho. JaQuAD: Japanese question answering dataset for machine reading comprehension. ArXiv preprint, 2022. URL https: //arxiv.org/abs/2202.01764.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170.\n\nBaohua Sun, Lin Yang, Catherine Chi, Wenhan Zhang, and Michael Lin. Squared english word: A method of generating glyph to use super characters for sentiment analysis. In AffCon@AAAI, 2019. URL https://arxiv.org/abs/1902.02160.\n\nLichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip S. Yu, and Caiming Xiong. Adv-bert: BERT is not robust on misspellings! generating nature adversarial samples on BERT. arXiv preprint, 2020. URL https://arxiv.org/abs/2003.04985.\n\nYi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. Charformer: Fast character transformers via gradient-based subword tokenization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=JtBRnrlOEFN.\n\nOwen Taylor. Pango, an open-source unicode text layout engine. In Proceedings of the 25th Internationalization and Unicode Conference, Washington, D.C., USA, 2004. The Unicode Consortium. URL https://people.redhat.com/otaylor/iuc25/pango-unicode-paper.pdf.\n\nErik F. Tjong Kim Sang and Fien De Meulder.\n\nIntroduction to the CoNLL-2003 shared task: In Proceedings of the Seventh Conference Language-independent named entity recognition. on Natural Language Learning at HLT-NAACL 2003, pp. 142–147, 2003. URL https:// aclanthology.org/W03-0419.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution discrepancy. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ d03a857a23b5285736c4d55e0bb067c8-Paper.pdf.\n\nIulia Turc, Kenton Lee, Jacob Eisenstein, Ming-Wei Chang, and Kristina Toutanova. Revisiting the primacy of english in zero-shot cross-lingual transfer. arXiv preprint, 2021. URL https: //arxiv.org/abs/2106.16171.\n\nDaan van Esch, Elnaz Sarbar, Tamar Lucassen, Jeremy O’Brien, Theresa Breiner, Manasa Prasad, Evan Crew, Chieu Nguyen, and Françoise Beaufays. Writing across the world’s languages: Deep internationalization for gboard, the google keyboard. Technical report, 2019. URL http:// arxiv.org/abs/1912.01218.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\nGiorgos Vernikos and Andrei Popescu-Belis. Subword mapping and anchoring across languages. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2633–2647, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.224. URL https://aclanthology.org/2021. findings-emnlp.224.\n\nAda Wan. Fairness in representation for multilingual NLP: Insights from controlled experiments on conditional language modeling. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=-llS6TiOew.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446.\n\nChanghan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-level subwords. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The ThirtySecond Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 9154–9160. AAAI Press, 2020. URL https://ojs.aaai.org/index. php/AAAI/article/view/6451.\n\nXinyi Wang, Sebastian Ruder, and Graham Neubig. Expanding pretrained models to thousands In Proceedings of the 60th Annual Meeting of more languages via lexicon-based adaptation. the Association for Computational Linguistics (Volume 1: Long Papers), pp. 863–877, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long. 61. URL https://aclanthology.org/2022.acl-long.61.\n\nJunqiu Wei, Qun Liu, Yinpeng Guo, and Xin Jiang. Training multilingual pre-trained language model with byte-level subwords. arXiv preprint, 2021. URL https://arxiv.org/abs/2101. 09469.\n\nAlexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. Should you mask 15% in masked\n\nlanguage modeling? arXiv preprint, 2022. URL https://arxiv.org/abs/2202.08005.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nMariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural In Proceedings of the 2020 Conference on Empirical Methods in Natlanguage processing. ural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6.\n\nShijie Wu and Mark Dredze. Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pp. 120–130, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.repl4nlp-1.16. URL https://aclanthology.org/2020.repl4nlp-1.16.\n\nWei Wu, Yuxian Meng, Fei Wang, Qinghong Han, Muyu Li, Xiaoya Li, Jie Mei, Ping Nie, Xiaofei Sun, and Jiwei Li. Glyce: Glyph-vectors for chinese character representations. In Neural Information Processing Systems, 2019.\n\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models. Transactions of the Association for Computational Linguistics, 10:291–306, 03 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00461. URL https://doi.org/10.1162/tacl_a_ 00461.\n\nAmir Zeldes and Mitchell Abrams. The Coptic Universal Dependency treebank. In Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pp. 192–201, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6022. URL https://aclanthology.org/W18-6022.\n\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, Elia Ackermann, Noëmi Aepli, Hamid Aghaei, Željko Agi ́c, Amir Ahmadi, Lars Ahrenberg, Chika Kennedy Ajede, Gabriel ̇e Aleksandraviˇci ̄ut ̇e, Ika Alfina, Avner Algom, Erik Andersen, Lene Antonsen, Katya Aplonova, Angelina Aquino, Carolina Aragon, Glyd Aranes, Maria Jesus Aranzabe, Bilge Nas Arıcan, (cid:127)Hórunn Arnardóttir, Gashaw Arutie, Jessica Naraiswari Arwidarasti, Masayuki Asahara, Deniz Baran Aslan, Cengiz Asmazo ̆glu, Luma Ateyah, Furkan Atmaca, Mohammed Attia, Aitziber Atutxa, Liesbeth Augustinus, Elena Badmaeva, Keerthana Balasubramani, Miguel Ballesteros, Esha Banerjee, Sebastian Bank, Verginica Barbu Mititelu, Starkaður Barkarson, Rodolfo Basile, Victoria Basmov, Colin Batchelor, John Bauer, Seyyit Talha Bedir, Kepa Bengoetxea, Yifat Ben Moshe, Gözde Berk, Yevgeni Berzak, Irshad Ahmad Bhat, Riyaz Ahmad Bhat, Erica Biagetti, Eckhard Bick, Agn ̇e Bielinskien ̇e, Kristín Bjarnadóttir, Rogier Blokland, Victoria Bobicev, Loïc Boizou, Emanuel Borges Völker, Carl Börstell, Cristina Bosco, Gosse Bouma, Sam Bowman, Adriane Boyd, Anouck Braggaar, Kristina Brokait ̇e, Aljoscha Burchardt, Marie Candito, Bernard Caron, Gauthier Caron, Lauren Cassidy, Tatiana Cavalcanti, Gül ̧sen Cebiro ̆glu Eryi ̆git, Flavio Massimiliano Cecchini, Giuseppe G. A. Celano, Slavomír ˇCéplö, Neslihan Cesur, Savas Cetin, Özlem Çetino ̆glu, Fabricio Chalub, Shweta Chauhan, Ethan Chi, Taishi Chika, Yongseok Cho, Jinho Choi, Jayeol Chun, Juyeon Chung, Alessandra T. Cignarella, Silvie Cinková, Aurélie Collomb, Ça ̆grı Çöltekin, Miriam Connor, Daniela Corbetta, Marine Courtin, Mihaela Cristescu, Philemon Daniel, Elizabeth Davidson, Mathieu Dehouck, Martina de Laurentiis, Marie-Catherine de Marneffe, Valeria de Paiva, Mehmet Oguz Derin, Elvis de Souza, Arantza Diaz de Ilarraza, Carly Dickerson, Arawinda Dinakaramani, Elisa Di Nuovo, Bamba Dione, Peter Dirix, Kaja Dobrovoljc, Timothy Dozat, Kira Droganova, Puneet Dwivedi, Hanne Eckhoff, Sandra Eiche, Marhaba Eli, Ali Elkahky, Binyam Ephrem, Olga Erina, Tomaž Erjavec, Aline Etienne, Wograine Evelyn, Sidney Facundes, Richárd Farkas, Federica Favero, Jannatul Ferdaousi, Marília Fernanda, Hector Fernandez Alcalde, Jennifer Foster, Cláudia Freitas, Kazunori Fujita, Katarína Gajdošová, Daniel Galbraith, Federica Gamba, Marcos Garcia, Moa Gärdenfors, Sebastian Garza, Fabrício Ferraz Gerardi, Kim Gerdes, Filip Ginter, Gustavo Godoy, Iakes Goenaga, Koldo Gojenola, Memduh Gökırmak, Yoav Goldberg, Xavier Gómez Guinovart, Berta González Saavedra, Bernadeta Grici ̄ut ̇e, Matias Grioni, Loïc Grobol, Normunds Gr ̄uz ̄ıtis, Bruno Guillaume, Céline GuillotBarbance, Tunga Güngör, Nizar Habash, Hinrik Hafsteinsson, Jan Hajiˇc, Jan Hajiˇc jr., Mika Hämäläinen, Linh Hà M ̃y, Na-Rae Han, Muhammad Yudistira Hanifmuti, Takahiro Harada, Sam Hardwick, Kim Harris, Dag Haug, Johannes Heinecke, Oliver Hellwig, Felix Hennig, Barbora Hladká, Jaroslava Hlaváˇcová, Florinel Hociung, Petter Hohle, Jena Hwang, Takumi Ikeda, Anton Karl Ingason, Radu Ion, Elena Irimia, O. lájídé Ishola, Kaoru Ito, Siratun Jannat, Tomáš\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nJelínek, Apoorva Jha, Anders Johannsen, Hildur Jónsdóttir, Fredrik Jørgensen, Markus Juutinen, Sarveswaran K, Hüner Ka ̧sıkara, Andre Kaasen, Nadezhda Kabaeva, Sylvain Kahane, Hiroshi Kanayama, Jenna Kanerva, Neslihan Kara, Ritván Karahóˇga, Boris Katz, Tolga Kayadelen, Jessica Kenney, Václava Kettnerová, Jesse Kirchner, Elena Klementieva, Elena Klyachko, Arne Köhn, Abdullatif Köksal, Kamil Kopacewicz, Timo Korkiakangas, Mehmet Köse, Natalia Kotsyba, Jolanta Kovalevskait ̇e, Simon Krek, Parameswari Krishnamurthy, Sandra Kübler, O ̆guzhan Kuyrukçu, Aslı Kuzgun, Sookyoung Kwak, Veronika Laippala, Lucia Lam, Lorenzo Lambertino, Tatiana Lando, Septina Dian Larasati, Alexei Lavrentiev, John Lee, Phương Lê H`ông, Alessandro Lenci, Saran Lertpradit, Herman Leung, Maria Levina, Cheuk Ying Li, Josie Li, Keying Li, Yuan Li, KyungTae Lim, Bruna Lima Padovani, Krister Lindén, Nikola Ljubeˇsi ́c, Olga Loginova, Stefano Lusito, Andry Luthfi, Mikko Luukko, Olga Lyashevskaya, Teresa Lynn, Vivien Macketanz, Menel Mahamdi, Jean Maillard, Aibek Makazhanov, Michael Mandl, Christopher Manning, Ruli Manurung, B ̈us ̧ra Mars ̧an, Cătălina Mărănduc, David Mareˇcek, Katrin Marheinecke, Stella Markantonatou, Héctor Martínez Alonso, Lorena Martín Rodríguez, André Martins, Jan Maˇsek, Hiroshi Matsuda, Yuji Matsumoto, Alessandro Mazzei, Ryan McDonald, Sarah McGuinness, Gustavo Mendonc ̧a, Tatiana Merzhevich, Niko Miekka, Karina Mischenkova, Margarita Misirpashayeva, Anna Missil ̈a, Cătălin Mititelu, Maria Mitrofan, Yusuke Miyao, AmirHossein Mojiri Foroushani, Judit Molnár, Amirsaeid Moloodi, Simonetta Montemagni, Amir More, Laura Moreno Romero, Giovanni Moretti, Keiko Sophie Mori, Shinsuke Mori, Tomohiko Morioka, Shigeki Moro, Bjartur Mortensen, Bohdan Moskalevskyi, Kadri Muischnek, Robert Munro, Yugo Murawaki, Kaili M ̈u ̈urisep, Pinkey Nainwani, Mariam Nakhlé, Juan Ignacio Navarro Hor ̃niacek, Anna Nedoluzhko, Gunta Neˇspore-B ̄erzkalne, Manuela Nevaci, Lương Nguy ̃ên Thị, Huy`ên Nguy ̃ên Thị Minh, Yoshihiro Nikaido, Vitaly Nikolaev, Rattima Nitisaroj, Alireza Nourian, Hanna Nurmi, Stina Ojala, Atul Kr. Ojha, Adédayò Olúòkun, Mai Omura, Emeka Onwuegbuzia, Noam Ordan, Petya Osenova, Robert ̈Ostling, Lilja Øvrelid, S ̧ aziye Bet ̈ul ̈Ozates ̧, Merve ̈Ozc ̧elik, Arzucan ̈Ozg ̈ur, Balkız ̈Ozt ̈urk Bas ̧aran, Teresa Paccosi, Alessio Palmero Aprosio, Hyunji Hayley Park, Niko Partanen, Elena Pascual, Marco Passarotti, Agnieszka Patejuk, Guilherme PaulinoPassos, Giulia Pedonese, Angelika Peljak-Łapi ́nska, Siyao Peng, Cenel-Augusto Perez, Natalia Perkova, Guy Perrier, Slav Petrov, Daria Petrova, Andrea Peverelli, Jason Phelan, Jussi Piitulainen, Tommi A Pirinen, Emily Pitler, Barbara Plank, Thierry Poibeau, Larisa Ponomareva, Martin Popel, Lauma Pretkalnin ̧a, Sophie Prévost, Prokopis Prokopidis, Adam Przepiórkowski, Tiina Puolakainen, Sampo Pyysalo, Peng Qi, Andriela R ̈a ̈abis, Alexandre Rademaker, Mizanur Rahoman, Taraka Rama, Loganathan Ramasamy, Carlos Ramisch, Fam Rashel, Mohammad Sadegh Rasooli, Vinit Ravishankar, Livy Real, Petru Rebeja, Siva Reddy, Mathilde Regnault, Georg Rehm, Ivan Riabov, Michael Rießler, Erika Rimkut ̇e, Larissa Rinaldi, Laura Rituma, Putri Rizqiyah, Luisa Rocha, Eiríkur R ̈ognvaldsson, Mykhailo Romanenko, Rudolf Rosa, Valentin Ros, ca, Davide Rovati, Ben Rozonoyer, Olga Rudina, Jack Rueter, Kristján Rúnarsson, Shoval Sadde, Pegah Safari, Benoˆıt Sagot, Aleksi Sahala, Shadi Saleh, Alessio Salomoni, Tanja Samardˇzi ́c, Stephanie Samson, Manuela Sanguinetti, Ezgi Sanıyar, Dage S ̈arg, Baiba Saul ̄ıte, Yanin Sawanakunanon, Shefali Saxena, Kevin Scannell, Salvatore Scarlata, Nathan Schneider, Sebastian Schuster, Lane Schwartz, Djamé Seddah, Wolfgang Seeker, Mojgan Seraji, Syeda Shahzadi, Mo Shen, Atsuko Shimada, Hiroyuki Shirasu, Yana Shishkina, Muh Shohibussirri, Dmitry Sichinava, Janine Siewert, Einar Freyr Sigurðsson, Aline Silveira, Natalia Silveira, Maria Simi, Radu Simionescu, Katalin Simkó, Mária Šimková, Kiril Simov, Maria Skachedubova, Aaron Smith, Isabela Soares-Bastos, Shafi Sourov, Carolyn Spadine, Rachele Sprugnoli, Vivian Stamou, Stein(cid:127)hór Steingrímsson, Antonio Stella, Milan Straka, Emmett Strickland, Jana Strnadová, Alane Suhr, Yogi Lesmana Sulestio, Umut Sulubacak, Shingo Suzuki, Daniel Swanson, Zsolt Szántó, Chihiro Taguchi, Dima Taji, Yuta Takahashi, Fabio Tamburini, Mary Ann C. Tan, Takaaki Tanaka, Dipta Tanaya, Mirko Tavoni, Samson Tella, Isabelle Tellier, Marinella Testori, Guillaume Thomas, Sara Tonelli, Liisi Torga, Marsida Toska, Trond Trosterud, Anna Trukhina, Reut Tsarfaty, Utku Türk, Francis Tyers, Sumire Uematsu, Roman Untilov, Zdeˇnka Urešová, Larraitz Uria, Hans Uszkoreit, Andrius Utka, Elena Vagnoni, Sowmya Vajjala, Rob van der Goot, Martine Vanhove, Daniel van Niekerk, Gertjan van Noord, Viktor Varga, Uliana Vedenina, Eric Villemonte de la Clergerie, Veronika Vincze, Natalia Vlasova, Aya Wakasa, Joel C. Wallenberg, Lars Wallin, Abigail Walsh, Jing Xian Wang, Jonathan North Washington, Maximilan Wendt, Paul Widmer, Shira Wigderson, Sri Hartati Wijono, Seyi Williams, Mats Wirén, Christian Wittern, Tsegay Woldemariam, Tak-sum Wong, Alina Wróblewska, Mary Yako, Kayo Yamashita, Naoki Yamazaki, Chunxiao Yan, Koichi Yasuoka, Marat M. Yavrumyan, Arife Betül Yenice,\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nOlcay Taner Yıldız, Zhuoran Yu, Arlisa Yuliawati, Zdenˇek Žabokrtský, Shorouq Zahra, Amir Zeldes, He Zhou, Hanzhi Zhu, Anna Zhuravleva, and Rayan Ziane. Universal dependencies 2.10, 2022. URL http://hdl.handle.net/11234/1-4758. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University.\n\nShiyue Zhang, Vishrav Chaudhary, Naman Goyal, James Cross, Guillaume Wenzek, Mohit Bansal, and Francisco Guzmán. How robust is neural machine translation to language imbalance in multilingual tokenizer training? arXiv preprint, 2022. URL https://arxiv.org/abs/2204.14268.\n\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015. URL https://www.cv-foundation.org/openaccess/content_iccv_2015/ papers/Zhu_Aligning_Books_and_ICCV_2015_paper.pdf.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nA ABSTRACT RECONSTRUCTIONS\n\nFigure 4: PIXEL image reconstructions of the abstract with different span masks.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nB WEB TEXT RECONSTRUCTIONS\n\n(a) 100k steps\n\n(b) 500k steps\n\n(c) 1M steps\n\nFigure 5: PIXEL image reconstructions after 100k, 500k, and 1M steps of pretraining. We overlay the masked original image with the model’s predictions. Images are wrapped into squares and resized for visualization purposes only. The texts were not part of the training data. We see that the fully trained PIXEL (1M) predicts masked spans more clearly and accurately. For longer spans with a larger possible prediction space, multiple predictions may appear together creating blurred text.\n\nReconstructions of three sources of text19 20 21 after 100K, 500K and 1M pretraining steps. The figure also shows how PIXEL (visually) expresses uncertainty, e.g. for reconstructions of long spans where the space of possible outputs is much larger than for short spans, and how it captures longrange dependencies. In the third row, we can for instance see that PIXEL uses context from the beginning of a sequence (Barack Obama) to correctly fill in a gap later in the sequence, and viceversa (Brienomyrus).\n\n19https://www.nationalpeanutboard.org/peanut-info/our-message.htm 20https://www.penguinsinternational.org/2019/07/10/do-penguins-have-knees-and-other-\n\nfrequently-asked-questions/\n\n21https://www.theatlantic.com/science/archive/2021/05/electric-fish-pause/618993/\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nC CODE\n\nPIXEL is implemented in PyTorch (Paszke et al., 2019) and built on HuggingFace transformers (Wolf et al., 2020). We make our code available at https://github.com/xplip/pixel. Our pretrained PIXEL model, including a large number of intermediate checkpoints, is available at https://huggingface.co/Team-PIXEL/pixel-base and our finetuned models, including multiple seeds each, are available through the model hub.\n\nD TEXT RENDERER DETAILS\n\nRendering backend We experimented with different text rendering backends. Following Salesky et al. (2021), our first implementation was based on PyGame,22 which PIXEL was also pretrained with. Later on, we switched to a backend based on Pango (Taylor, 2004) and Cairographics,23 which has native support for complex text layouts, making it possible to specify fallback fonts, and has faster rendering speed. Without fallback fonts, we would be limited to a maximum number of 216−1 glyphs that can fit into a single OpenType or TrueType font file due to a technical limitation.24 By leveraging fallback fonts, we can theoretically cover all Unicode codepoints, including emojis.\n\nFonts We rely on the Google Noto Sans fonts collection,25 which covers the majority of Unicode codepoints and is actively growing.26. Note, however, that PIXEL is compatible with any font and can therefore encode anything that can be typeset on a computer screen. We used a font size of 8 at 120 DPI for pretraining with PyGame, which was selected manually to fit most scripts into a rendered height of 16px. It can, however, also be adjusted at finetuning time. For finetuning with PangoCairo, we use a font size of 8 · (120/72) ≈ 13.33 which yields roughly the same outputs as the PyGame renderer. Due to how glyphs are shaped by the two backends, the outputs of the two renderers do not exactly match. Because we did not employ data augmentation to make PIXEL robust to such changes in font size, we recommend using the PyGame renderer it was pretrained with for zero-shot applications with PIXEL. When finetuning, this minor mismatch in rendering outputs is easily overcome by PIXEL, so we generally recommend using the PangoCairo renderer.\n\nCharacters versus glyphs For extractive QA, it is necessary to obtain a mapping between the characters in the context paragraph and where they appear on the rendered image. Obtaining this mapping is not straightforward due to how text is rendered. The shaping step in the rendering pipeline converts characters into glyphs.27 In ligatures, as common for instance in Arabic, a glyph is composed of multiple characters. Likewise, an emoji often consists of a base codepoint and a modifier codepoint (e.g. to change the emoji skin colour) which are represented by a single glyph. For accents, on the other hand, one character might yield multiple glyphs.28 In practice, the renderer therefore uses grapheme clusters, whose logical boundaries in the rendered image we can map to the input characters.29 For simplicity, we assign each codepoint of a grapheme cluster to the logical horizontal offset at which the cluster starts on the rendered image. Future work may investigate alternative mapping strategies.\n\nRGB rendering PIXEL supports RGB rendering which may be useful to accurately represent colour emoji and for multimodal applications in the future. However, 24-bit RGB rendering is slightly slower than 8-bit grayscale rendering (see Table 6 below) for text written in Latin script, which is why we made RGB rendering an optional setting. In our pretraining and finetuning experiments we rendered text in grayscale, and we generally recommend doing so when not working with coloured inputs.\n\n22https://www.pygame.org/ 23https://www.cairographics.org/ 24See https://en.wikipedia.org/wiki/Unicode_font for an explanation. 25https://fonts.google.com/noto 26See https://notofonts.github.io/overview/ for an overview of Noto’s Unicode coverage. 27See https://docs.gtk.org/Pango/pango_rendering.html for an overview of the rendering pipeline. 28https://docs.gtk.org/Pango/pango_fonts.html#glyphs 29https://unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nRight-to-left scripts PIXEL’s renderer natively supports right-to-left (RTL) writing. In the default setting, the base text direction (which for instance determines on which side of a sentence punctuation marks are placed) is inferred automatically by the rendering backend based on the first “strong directional” character in a given paragraph.30 The mirroring of RTL characters is also handled automatically according to their Unicode bidi attributes. Optionally, the base text direction can be set manually, which is useful when working on monolingual data, e.g. in Arabic or Hebrew, as the renderer does not have to go through the direction check. In §J, we describe limitations of how we currently handle RTL writing.\n\nFigure 6: Distributions of sentence lengths from monolingual UD corpora after tokenizing by BERT and MBERT and rendering by PIXEL, compared to the reference by UD treebank annotators.\n\nProcessor\n\nBatched Throughput [ex / s] ZHO ENG\n\nRenderer (Grayscale) Renderer (RGB)\n\nTokenizer (Rust)\n\nTokenizer (Python)\n\n✗ ✗\n\n✓ ✗\n\n✓ ✗\n\n3944.1 3615.1\n\n19128.9 4782.9\n\n1286.6 1286.8\n\n6309.0 6849.5\n\n18550.5 5684.4\n\n2637.1 2580.9\n\nTable 6: Throughput comparison between PIXEL’s PangoCairo renderer and the fast and slow BERT tokenizers, implemented in Rust and Python respectively, from the HuggingFace tokenizers library. We estimate throughput, measured in examples per second, by how long it takes to process 1M lines of English (ENG) and Chinese (ZHO) Wikipedia text on the same desktop workstation (AMD Ryzen 9 3900X 12-core CPU). We distinguish between tokenizing all lines individually (Batched = ✗) and as one single batch (✓).\n\nEfficiency analysis We briefly analyze the text processing (rendering versus tokenizing) efficiency in terms of a) length of the processed sequence, which has a direct effect on GPU memory consumption and the time it takes to compute forward and backward passes, and b) processing throughput.\n\nFor a), we follow Rust et al. (2021) and process the training and validation splits of all available UD v2.10 treebanks in various languages with the PIXEL renderer and the tokenizers of BERT and MBERT. We plot the resulting sentence length distributions in Figure 6, including a comparison\n\n30See https://unicode.org/reports/tr9/ for an overview of the Unicode bidi algorithm.\n\n25\n\n0.000.010.020.03ProportionArabic0.000.020.04Chinese0.000.020.04English0.000.030.050.08ProportionFinnish0.000.020.04Indonesian0.000.030.050.08Japanese050100150Length [Tokens/Patches]0.000.020.040.06ProportionKorean050100150Length [Tokens/Patches]0.000.020.04Russian050100150Length [Tokens/Patches]0.000.050.10TurkishReferenceBERTmBERTPIXELPublished as a conference paper at ICLR 2023\n\nwith the reference segmentations from the UD annotators. For English text, the PIXEL renderer is slightly less efficient, i.e., it produces slightly longer sequences on average than the tokenizers. For other languages with Latin script, e.g. Finnish and Turkish, the renderer is more efficient than the BERT tokenizer, albeit slightly less efficient than the MBERT tokenizer. For non-Latin scripts such as Arabic and Japanese, we see that the renderer can be a lot more efficient than both tokenizers. The English BERT tokenizer is technically fairly space-efficient for non-Latin scripts but this is misleading because it largely produces [UNK]s (recall right side of Table 1) and each [UNK] is a single token; the functionality of the BERT model on a sequence of [UNK] is strongly compromised.\n\nFor b), we compare the processing throughput of HuggingFace’s BERT tokenizers and our PIXEL renderer in Table 6. We find that the Rust-based BERT tokenizer with batch processing achieves the highest throughput by leveraging parallelization. When not using batch processing, it is comparable in throughput with PIXEL’s renderer, i.e. depending on the language or script, rendering can be slightly slower (ENG) or faster (ZHO) than tokenizing. Since the rendering backend (PangoCairo) is implemented in C, we expect to achieve similar gains in rendering throughput by also leveraging parallelization for batch processing (in contrast to the Python-based tokenizer which is limited by Python’s global interpreter lock (GIL)). We plan to implement batch rendering functionality in the future.\n\nE ARCHITECTURE & PRETRAINING DETAILS\n\nPARAMETER\n\nVALUE\n\n(16, 8464, 3) 16 768 3072 12 12 512 2048 16 8\n1e−12 0.25 6\n\nImage size Patch size P Encoder hidden size Denc Encoder intermediate size Encoder num attention heads Encoder num layers L Decoder hidden size Ddec Decoder intermediate size Decoder num attention heads Decoder num layers K Layer norm ε (Ba et al., 2016) Span masking ratio R Span masking max length S Span masking cumulative weights W {0.2, 0.4, 0.6, 0.8, 0.9, 1} Span masking spacing Dropout probability Hidden activation Optimizer Adam β Adam ε Weight decay Peak learning rate Learning rate schedule Minimum learning rate Learning rate warmup ratio Training steps Batch size\n\nDynamic 0.1 GeLU (Hendrycks & Gimpel, 2016) AdamW (Loshchilov & Hutter, 2019; Kingma & Ba, 2015) (0.9, 0.999) 1e−8 0.05 1.5e−4 Cosine Decay (Loshchilov & Hutter, 2017) 1e−5 0.05 1M 256\n\nTable 7: PIXEL pretraining settings\n\nPIXEL reshapes each image x into a sequence of N = W/P non-overlapping Patch Embeddings flattened 2D patches xf ∈ RN ×(P 2C), where P = 16 is the patch size, and linearly projects them via E ∈ R(P 2C)×Denc to obtain patch embeddings xp = (xf E) ∈ RN ×Denc with encoder hidden size Denc = P 2C = 768.31 Afterwards, fixed sinusoidal position embeddings Epos ∈ R(N +1)×Denc are added, leaving out the position vector in position 0 for a classification (CLS) embedding later: ̃xp = xp + [E1\n\npos, . . . , E(N +1)\n\npos\n\n].\n\n31This is equivalent to projecting each rendered image x ∈ RH×W ×C via a 2D-convolutional layer with C input channels and Denc output channels and kernel size and stride both equal to the patch size P , which we do in practice.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: PIXEL pretraining loss curve\n\nPIXEL then masks out R = 25% of the N = 529 embedded patches via span Span Masking masking with max span length S = 6 and cumulative span weights W = {0.2, 0.4, 0.6, 0.8, 0.9, 1}, i.e. E(s) = 3.1, as outlined in Algorithm 1. Applying the mask M, we obtain the unmasked patches ̃xvis = { ̃xi\n\np : i /∈ M}N\n\ni=0.\n\nEncoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only operates on unmasked patches (i.e., ≈ 396 patches at 25% masking) and a special CLS embedding with its positional encodpos ∈ R1×Denc is prepended to the sequence: h0 = [c, ̃xvis] ∈ R(1+⌊R·N ⌋)×Denc .32 ing c = x[cls] + E0 Let {hi}L i=1 be the encoder hidden states after each of the L = 12 encoder transformer layers, and h0 denotes the input sequence. The outputs of each transformer layer are computed as detailed in (Vaswani et al., 2017), 33 and the last layer’s output hL ∈ R(1+⌊R·N ⌋)×Denc is passed to the decoder.\n\nDecoder The PIXEL decoder first projects the encoder outputs via Edec ∈ RDenc×Ddec to obtain decoder embeddings xd = hLEdec ∈ R(1+⌊R·N ⌋)×Ddec, where Ddec = 512. Next, mask embeddings x[mask] ∈ R1×Ddec are inserted at the masked-out positions and fixed sinusoidal position embeddings are added to obtain d0 = [(xd ∪ {x[mask] : i ∈ M}N i=1 are the decoder hidden states after each of the K = 8 decoder transformer layers, computed in the same way as the encoder hidden states, and d0 denotes the input sequence. There is no encoderdecoder cross-attention. The decoder output dK ∈ R(N +1)×Ddec is projected via O ∈ RDdec×(P 2C) to obtain patch-wise logits o = (dKO) ∈ R(N +1)×(P 2C). Finally, the CLS logits are removed and a normalized mean squared error (MSE) pixel reconstruction loss is computed: Lnormpix = f ) − oi|2 with i denoting the indices in the set of masked, non-blank (text) i=0 and normalize(·) dividing the difference between the target\n\n1 |Q| patches Q = {i : i ∈ (M ∩ T )}N patch and its mean by its standard deviation.\n\ni=0) + Epos] ∈ R(N +1)×Ddec . {di}K\n\ni∈Q |normalize(xi\n\n(cid:80)\n\nF FINETUNING DETAILS\n\nTable 8 gives an overview of all languages used in our finetuning experiments, Table 9 links to our finetuning datasets, and Table 10 lists the UD treebanks we used.\n\nWe list our finetuning recipes in Table 11 for POS tagging, dependency parsing, NER, QA, and XNLI and in Table 12 for the GLUE tasks. Due to compute limitations we did not run comprehensive hyperparameter sweeps. Instead, we relied on sensible priors from finetuning BERT and made slight modifications as needed. In most cases, hyperparameters that work well for BERT also work well for PIXEL. For some of the semantic tasks, in particular NLI and SST-2, we found that some random initializations did not converge. In those cases, minor tweaks to the learning rate or increasing the batch size usually helped. For GLUE, we found that PIXEL performed slightly better on some tasks with the PangoCairo renderer, whereas for others, using the PyGame renderer (which PIXEL was\n\n32In pretraining, no loss is computed for the CLS embedding but it can optionally be used when finetuning\n\nPIXEL for sequence-level downstream tasks.\n\n33Note that encoder and decoder do not attend to the blank (padding) patches that appear after the EOS patch.\n\n27\n\n02004006008001000Training steps [K]0.00.20.40.60.81.0LossPublished as a conference paper at ICLR 2023\n\npretrained with) was more stable. We plan to further optimize the training recipes and study PIXEL’s convergence behaviour in the future.\n\nFor word-level tasks, we add padding in order to render each word at the start of a new image patch and so create a bijective mapping between words and patches. Doing so assumes that word boundaries are available. We note that subword-based and character-based models also make this assumption. In BERT, for instance, word-level tasks are formulated such that a word’s label is assigned to its first subword token, requiring word boundaries. During training, continuation tokens are then masked out when computing the loss. Consequently, predictions for continuation tokens also need to be masked out at inference time, which again requires word boundaries or aggregation strategies that may introduce errors. The same applies to character-based models. For PIXEL, should this assumption be violated, it is still possible to render the text without adding spacing, although the mapping is then no longer bijective as multiple words can overlap on one image patch. In such cases, assigning the prediction for a patch to either word can cause loss of information. Although in practice this approach does not necessarily affect performance negatively, future work will investigate alternative approaches.\n\nLanguage\n\nISO 639-3 Language Family\n\nScript\n\nAmharic Arabic Bengali Bulgarian Chinese Coptic English Finnish French German Greek Hausa Hindi Igbo Indonesian Japanese Kinyarwanda Korean Luganda Luo Naija Pidgin Russian Spanish Swahili Tamil Telugu Thai Turkish Urdu Vietnamese Wolof Yorùbá\n\nAMH ARA BEN BUL ZHO COP ENG FIN FRA DEU ELL HAU HIN IBO IND JPN KIN KOR LUG LUO PCM RUS SPA SWA TAM TEL THA TUR URD VIE WOL YOR\n\nAfro-Asiatic Afro-Asiatic Indo-European Indo-European Sino-Tibetan Afro-Asiatic Indo-European Uralic Indo-European Indo-European Indo-European Afro-Asiatic Indo-European Niger-Congo Austronesian Japonic Niger-Congo Koreanic Niger-Congo Nilo-Saharan English Creole Indo-European Indo-European Niger-Congo Dravidian Dravidian Kra-Dai Turkic Indo-European Austro-Asiatic Niger-Congo Niger-Congo\n\nGe\\ez Arabic Bengali Cyrillic Chinese Coptic Latin Latin Latin Latin Greek Latin Devanagari Latin Latin Japanese Latin Korean Latin Latin Latin Cyrillic Latin Latin Tamil Telugu Thai Latin Perso-Arabic Latin Latin Latin\n\nTable 8: Overview of languages used in our experiments.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nDataset\n\nDownload Link\n\nReference\n\nUniversal Dependencies 2.10 MasakhaNER GLUE TyDiQA-GoldP SQuADv1.1 KorQuAD 1.0 JaQuAD XNLI\n\nhttps://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4758 https://github.com/masakhane-io/masakhane-ner/tree/main/data https://huggingface.co/datasets/glue https://huggingface.co/datasets/tydiqa https://huggingface.co/datasets/squad https://huggingface.co/datasets/squad_kor_v1 https://huggingface.co/datasets/SkelterLabsInc/JaQuAD https://huggingface.co/datasets/xnli\n\n(Zeman et al., 2022; Nivre et al., 2020) (Adelani et al., 2021) (Wang et al., 2018) (Clark et al., 2020) (Rajpurkar et al., 2016) (Lim et al., 2019) (So et al., 2022) (Conneau et al., 2018)\n\nTable 9: Links and references to the datasets we used in our finetuning experiments.\n\nLanguage Treebank\n\n#Sentences Reference\n\nENG ARA COP HIN JPN KOR\n\nTAM VIE ZHO\n\nEnglish-EWT Arabic-PADT Coptic-Scriptorium Hindi-HDTB Japanese-GSD Korean-GSD Tamil-TTB Vietnamese-VTB Chinese-GSD\n\n16621\n\nSilveira et al. (2014)\n\n7664 Hajiˇc et al. (2009) 2011 Zeldes & Abrams (2018)\n\n16647\n\nPalmer et al. (2009)\n\n8100 Asahara et al. (2018) 6339 Chun et al. (2018)\n\n600 Ramasamy & Žabokrtský (2012)\n\n3000 Nguyen et al. (2009) 4997\n\nShen et al. (2016)\n\nTable 10: Overview of the Universal Dependencies v2.10 (Zeman et al., 2022; Nivre et al., 2020) treebanks used in our POS tagging and dependency parsing experiments with the number of sentences in their respective training splits. As mentioned in §3.1, these treebanks were chosen with typological and script diversity in mind.\n\nPARAMETER\n\nRendering backend Classification head pooling Optimizer Adam β Adam ε Weight decay Learning rate Learning rate warmup steps Learning rate schedule Max sequence length Stride Batch size Max steps Early stopping Eval steps Dropout probability\n\nPOS\n\n—\n\nDP\n\n—\n\n5e−5 100\n\n{5e−5, 8e−5} 100\n\n256 —\n64 15000\n\n256 —\n64 15000\n\n500\n\n500\n\nQA\n\n—\n\nNER\n\nPangoCairo\n\n— AdamW (0.9, 0.999) 1e−8 0\n5e−5 100\n\n{3e−5, 5e−5, 7e−5} 100\n\nLinear decay\n\n196 —\n64 15000 ✓\n\n500\n\n0.1\n\n400 160 32 20000\n\n500\n\nXNLI\n\nCLS\n\n2e−5 1000\n\n196 —\n256 50000\n\n1000\n\nTable 11: Finetuning settings for POS tagging, dependency parsing (DP), NER, QA, and XNLI. We did not run a comprehensive hyperparameter search due to compute limitations; these settings were manually selected based on a small number of preliminary runs. Maximum performance was often reached well before the specified number of max steps.\n\nPARAMETER\n\nMNLI\n\nQQP\n\nQNLI\n\nSST-2\n\nCOLA\n\nSTS-B\n\nMRPC\n\nRTE\n\nWNLI\n\nRendering backend Classification head pooling Optimizer Adam β Adam ε Weight decay Learning rate Learning rate warmup steps Learning rate schedule Max sequence length Batch size Max steps Early stopping Eval interval Dropout probability\n\nPangoCairo\n\nPyGame\n\nPangoCairo\n\nPyGame\n\nPyGame\n\nPyGame\n\nPyGame\n\nPyGame\n\nPyGame\n\n3e−5 100\n\n3e−5 100\n\n3e−5 100\n\nMean AdamW (0.9, 0.999) 1e−8 0\n\n3e−5 100\n\n2e−5 200\n\nLinear decay 256\n\n2e−5 100\n\n3e−5 100\n\n3e−5 200\n\n1e−5 100\n\n64 15000\n\n256 15000\n\n64 15000\n\n256 15000\n\n500 steps\n\n500 steps\n\n500 steps\n\n500 steps\n\n256 15000\n\n✓ 100 steps 0.1\n\n64 15000\n\n64 15000\n\n64 15000\n\n256 400\n\n100 steps\n\n100 steps\n\n250 steps\n\n1 epoch\n\nTable 12: Finetuning settings for GLUE tasks. We did not run a comprehensive hyperparameter search due to compute limitations; these settings were manually selected based on a small number of preliminary runs. Increasing the batch size to 256 and switching to the PyGame renderer helped achieve more consistent convergence behaviour for some tasks. For the smaller datasets (to the right of QQP), maximum performance was reached well before the specified number of max steps.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nG EXAMPLES OF Zeroé ORTHOGRAPHIC ATTACKS\n\nAttack\n\nNONE\n\nCONFUSABLE SHUFFLE (INNER) SHUFFLE (FULL) DISEMVOWEL INTRUDE KEYBOARD TYPO NATURAL NOISE TRUNCATE SEGMENTATION PHONETIC\n\nSentence\n\nPenguins are designed to be streamlined\n\nPegnuins are dnesiged to be sieatrnmled ngePnius rae dsgednei to be etimaslernd Pngns r dsgnd to be strmlnd Pe‘nguins a{re d)esigned t;o b*e stre<amlined Penguinz xre dwsigned ro ne streamllned Penguijs ard design4d ti bd streamlinfd Penguin are designe to be streamline Penguinsaredesignedtobestreamlined Pengwains’s ar dhiseind te be storimlignd\n\nTable 13: Examples of low-level orthographic attacks based on the Zeroé benchmark.\n\nFigure 8: Test set accuracy for a single run of PIXEL and BERT across different levels of noise introduced through various orthographic attacks in SNLI. The results show that PIXEL is more robust than BERT to most of these attacks.\n\nFigure 9: Test set accuracy for a single run of PIXEL and BERT across different levels of noise introduced through various orthographic attacks in POS tagging. The results show that PIXEL is more robust than BERT to most of these attacks, especially when dealing with visually-confusable character substitutions. SEGMENTATION is not applied to the task of POS tagging, since the joined words would not have a proper tag.\n\n30\n\n405060708090100Accuracy [%]ConfusableDisemvowelShuffle (full)Shuffle (inner)Intrude0205080405060708090100Accuracy [%]Keyboard typo0205080Natural noise0205080Phonetic0205080Segmentation0205080TruncateModelBERTBASEPIXEL5075100Accuracy [%]ConfusableDisemvowelShuffle (full)5075100Accuracy [%]Shuffle (inner)IntrudeKeyboard typo02050805075100Accuracy [%]Natural noise0205080Phonetic0205080TruncateModelBERTBASEPIXELPublished as a conference paper at ICLR 2023\n\nH FONT TRANSFER ANALYSIS\n\nIn this section, we analyse the adaptation capabilities of PIXEL to new fonts at finetuning time. Specifically, we finetune PIXEL models for POS tagging and dependency parsing on the UD_English-EWT treebank and sentiment analysis on SST-2, once with a font similar to our GoNotoCurrent / NotoSans-Regular pretraining font, NotoSerif-Regular, and once with a font strikingly different from it, JournalDingbats1. We compare the three fonts in Table 14 below:\n\nFont\n\nRendered Example Sentence\n\nGoNotoCurrent NotoSerif-Regular JournalDingbats1\n\nTable 14: An example sentence rendered in three different fonts.\n\nGoNotoCurrent\n\nNotoSerif-Regular\n\nJournalDingbats1\n\nPOS DP SST-2\n\n96.7 90.6 89.6\n\n95.9 88.1 84.2\n\n93.9 81.3 72.9\n\nTable 15: Results for fine-tuning PIXEL for POS tagging, dependency parsing (DP), and sentiment analysis on SST-2 with three different fonts: the font used in pretraining (GoNotoCurrent), a visually similar font (NotoSerif-Regular), and a highly dissimilar font (JournalDingbats1). We report test accuracy for POS, test LAS for DP, and validation accuracy for SST-2, each averaged over 5 runs.\n\nThe font transfer results are shown in Table 15. We find that PIXEL exhibits fairly high font transfer ability out-of-the-box, i.e. without any font or image augmentation strategies employed during pretraining.34 In line with our expectations, transfer to a visually similar font (NotoSerif-Regular) is easier than to a dissimilar font (JournalDingbats1). Nevertheless, PIXEL is able to transfer surprisingly well to the JournalDingbats1 font, in which every letter is simply mapped to the icon of an object or animal.\n\nI FURTHER ANALYSIS\n\nTo investigate where PIXEL currently lags behind BERT, we analyse the impact that dependency length has on both models in dependency parsing in ENG. We can see in Figure 10 that the LAS gap between BERT and PIXEL increases with longer dependencies, indicating that PIXEL struggles slightly more with long syntactic dependencies.\n\n#L\n\nθ\n\nENG\n\nARA\n\nBUL\n\nDEU\n\nELL\n\nFRA\n\nHIN\n\nRUS\n\nSPA\n\nSWA\n\nTHA\n\nTUR\n\nURD\n\nVIE\n\nZHO\n\nMBERT\n\n104\n\n179M 83.3\n\nBERT PIXEL\n\n1 1\n\n110M 83.7 86M 77.2\n\n73.2\n\n64.8 58.9\n\n77.9\n\n69.1 66.5\n\n78.1\n\n70.4 68.0\n\n75.8\n\n67.7 64.9\n\n78.5\n\n72.4 69.4\n\n70.1\n\n59.2 57.8\n\n76.5\n\n66.4 63.4\n\n79.7\n\n72.4 70.3\n\n67.2\n\n62.2 60.8\n\n67.7\n\n35.7 50.2\n\n73.3\n\n66.3 64.0\n\n66.1\n\n54.5 54.1\n\n77.2\n\n67.6 64.8\n\n77.7\n\n46.2 52.0\n\nTable 16: Results for PIXEL and BERT finetuned on XNLI in the translate-train-all setting where we train on the joint training data in all 15 languages, originally translated from ENG by Conneau et al. (2018). We report test set accuracy averaged over 5 runs each. Despite the relatively large performance gap in favor of BERT in ENG (which is in line with the GLUE results in Table 3), the gap is much smaller for other languages, particularly those not using the Latin writing system. PIXEL is overall more consistent across scripts, outperforming BERT in THA and ZHO.\n\n34We believe such augmentation strategies would further improve robustness to font variations and leave this experiment to future work. Considering that we have full control over the font when working with NLP text datasets, robustness to font variations was not a primary goal in this work.\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: LAS scores (ENG) across different dependency lengths averaged over 5 random intitializations of BERT and PIXEL. In ENG, long syntactic dependencies are more challenging for PIXEL.\n\nJ LIMITATIONS\n\nThis paper introduces a new approach to processing written language as images, which removes the need for a finite vocabulary, providing a solution to the vocabulary bottleneck. While our results show that PIXEL is a promising approach in this direction, this is only the first step. Here, we highlight current limitations and avenues for future work for pixel-based models:\n\n• PIXEL is pretrained on predominantly English text written in the Latin script. The choice of English is driven by the scientific goal of comparing against a widely used model (English BERT) but English may not be the best source language for cross-lingual transfer (Turc et al., 2021; Blevins et al., 2022). We expect that PIXEL trained on typologically diverse languages in multiple scripts would considerably surpass the cross-script and cross-lingual transferability of English-only PIXEL but this remains to be verified, and training a model on large amounts of data will require large computational resources.\n\n• PIXEL currently seems to be less sample-efficient than subword-based PLMs. PIXEL excels at syntactic tasks after being pretrained for the same number of steps/datapoints as BERT (a challenging setup within an academic budget), but still lags behind in semantic processing. As a consequence, it also requires more training steps than BERT to converge during finetuning. Closing this gap might involve longer pretraining with additional (longdependency) objectives.\n\n• There are challenges to be addressed when working with languages written right-to-left. PIXEL currently processes sentences in such languages from the end to the beginning which may lead to learning inadequate features for sentence separation and position embeddings. • PIXEL cannot be used for language generation tasks because it is not possible to produce\n\ndiscrete words from the pretrained decoder.\n\n• Rendering text as images requires more disk space than reading text from a file. This can be alleviated by caching the dataset in a compressed format, or rendering the images on-thefly. Rendering images on-the-fly will create additional overhead when training for multiple epochs.\n\n32\n\n12[3, 6]>6Distance to Head [# words]707580859095100Labeled Attachment Score [%]ModelBERTBASEPIXELModelBERTBASEPIXEL",
  "translations": [
    "# Summary Of The Paper\n\nIn this work, the authors developed a clever twist to the traditional neural language modeling paradigm by building a model that operates on image patches of the text. It learns to output “masked” patches using an MSE loss. Through this, the model alleviates issues with fixed or non-transferable vocabulary. When fine-tuned on a suite of downstream syntactic and semantic tasks, PIXEL observes commendable performance often on par with BERT etc. or better for languages whose writing scripts are unfamiliar to BERT. The authors also show that the model can handle code-switching and works well in the face of character-level noise.\n\n# Strength And Weaknesses\n\nStrength:\nI commend the authors for the innovation here. Although a simple idea, it is highly effective and clever, and presents a very exciting alternative to BERT & friends. I look forward to future research on what these models encode and share across languages/writing systems.\n\nQuestions:\n- It would be useful to have a more detailed discussion on the fertility of the pixel maps, ie., how many patches correspond to a word on average and the relationship to some of the tasks. For example, I am curious if there is a relationship between this and the question answering performance of PIXEL as the authors allude to as well.\n- It would help to order the languages in the results tables based on [UNK]%. and insert a row showing the difference between BERT and pixel for each result table.\n- Any hypotheses for why its performs much worse than BERT on COLA specifically?\n- I understand the space and time constraints but I would be excited to see that perhaps training PIXEL on two very different script systems leads to big performance gains than traditional models and PIXEL trained on english only. Especially if the other language has a rich morphology for example.\n- How hard was hyper-parameter tuning for this model? what do the loss curves look like?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: Extremely clear and well written+motivated! I enjoyed reading the paper.\nQuality: The authors evaluated many different aspects of this model and sufficiently demonstrated its capabilities.\nNovelty: To the best of my knowledge, this is the first paper to use pixel-based language systems as language models.\nReproducibility: The authors will provide a public link with access to many aspects of the model it seems.\n\n# Summary Of The Review\n\nGiven the issues with vocabulary and tokenization in NLP, I think this paper offers a very innovative solution and I am curious to see how this research direction pans out.\n\nNB:I would give this paper a 9/10 if the option existed.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.",
    "# Summary Of The Paper\nThe paper \"Language Modelling with Pixels\" introduces PIXEL, a novel pretrained language model designed to address the vocabulary bottleneck prevalent in traditional language models like BERT and GPT-3. PIXEL represents text as images, converting textual data into RGB images through a text renderer, before utilizing a Vision Transformer (ViT) architecture to process and reconstruct masked pixel regions. The authors demonstrate that PIXEL outperforms BERT on various syntactic and semantic tasks across multiple languages, particularly those that are underrepresented in BERT's training data. The model exhibits robustness against orthographic attacks and shows potential for handling code-switching scenarios, although it faces challenges with Latin scripts and right-to-left languages.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to language modeling by leveraging image representations to circumvent the limitations of fixed vocabularies. The evaluation across diverse syntactic and semantic tasks highlights PIXEL's effectiveness, particularly for languages not adequately represented in previous models. However, the reliance on English pretraining may limit its applicability, and the performance drop in Latin scripts suggests areas for improvement. Additionally, while the robustness against orthographic attacks is noteworthy, the paper could benefit from more detailed analyses of the model’s limitations in specific scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates both the problem being addressed and the proposed solution. The methodology is described in sufficient detail, enabling reproducibility of the experiments. The novelty of the approach is substantial, as it combines concepts from visual recognition with language modeling, an area not widely explored prior to this work. However, the paper could expand on the implications of findings regarding the model's limitations, particularly concerning right-to-left languages and the specific challenges faced during fine-tuning.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of language modeling by proposing a novel approach that bypasses the vocabulary limitations of traditional models. While the results are promising, particularly for underrepresented languages, there are notable limitations that need to be addressed in future work. The combination of innovative methodology and comprehensive evaluation makes this paper a valuable contribution to the NLP community.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces PIXEL (Pixel-based Encoder of Language), a novel pretrained language model designed to overcome the vocabulary limitations of traditional models by transforming text into images. The authors employ a masked autoencoder architecture based on Vision Transformers (ViT-MAE) for pretraining on English datasets, utilizing a unique span masking technique to enhance contextual learning. PIXEL is evaluated on various syntactic and semantic tasks across 32 diverse languages, demonstrating its capabilities in generalization and cross-lingual transfer, particularly outperforming BERT in languages not included in its pretraining dataset.\n\n# Strength And Weaknesses\nThe key strengths of the paper include its innovative approach to eliminating the vocabulary bottleneck, enabling support for a diverse range of languages, and its robustness to orthographic variations, making it suitable for practical applications. Additionally, PIXEL's superior cross-lingual transfer performance highlights its adaptability to unseen languages. However, the model exhibits limitations, including a predominant focus on English during pretraining, which may restrict its effectiveness across other languages, and a lower sample efficiency compared to subword-based models like BERT. Moreover, challenges in processing right-to-left languages and the inability to perform generation tasks are notable drawbacks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology, experimental design, and results. The novelty of the approach is significant, as it presents an alternative to traditional text processing methods, yet there are concerns regarding reproducibility, particularly due to the model's reliance on specific training data and architecture choices. The authors provide sufficient detail on the methodology and results, but additional information on potential challenges in reproducing the results with diverse datasets would enhance clarity.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to language modeling through the use of pixel-based representations, showcasing promising results across a range of languages. While the contributions are noteworthy, certain limitations regarding pretraining data diversity and sample efficiency warrant further exploration and improvement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces PIXEL (Pixel-based Encoder of Language), a novel pretrained language model that represents text as images, thereby addressing the vocabulary bottleneck prevalent in traditional models. This approach allows for multilingual representation, supporting 32 languages and 14 scripts. The methodology involves converting text into images, employing a Vision Transformer (ViT) architecture for encoding, and reconstructing masked pixels during training. Evaluation results demonstrate that PIXEL outperforms BERT in tasks involving non-Latin scripts while showing robustness to orthographic attacks and code-switching, though it performs slightly weaker on Latin script tasks.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to language modeling, which allows for flexibility across multiple languages without the need for a fixed vocabulary. The robustness of PIXEL against orthographic noise and its performance on lower-representation languages are notable contributions. However, the paper also presents weaknesses; specifically, the performance gap with BERT on Latin script tasks raises questions about its applicability in English-centric contexts. The reliance on image representation might also introduce challenges in efficiency and computational resource requirements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its ideas clearly, with sufficient detail provided on the methodology and experimental setup. The novelty of rethinking language modeling as a visual recognition task is significant, potentially paving the way for future research directions in multilingual NLP. However, reproducibility may be a concern due to the complexity of the model and the specificity of the datasets used; detailed methodologies and hyperparameters are provided, yet practical implementation remains a challenge.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel approach to language modeling through the representation of text as images. While it shows promising results in multilingual contexts, particularly for non-Latin scripts, further improvements are needed for its applicability to Latin script tasks. The contributions challenge traditional tokenization methods but may encounter practical hurdles in implementation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces PIXEL, an innovative language modeling framework that utilizes pixel-based representations of text to overcome the traditional limitations associated with finite vocabularies. Key contributions include its ability to support a wide array of languages, particularly non-Latin scripts, and its demonstrated robustness against noisy inputs and orthographic variations. Methodologically, PIXEL employs an image rendering approach for text representation, which allows it to capture orthographic similarities across diverse linguistic contexts. The findings indicate that PIXEL outperforms BERT in several syntactic and semantic tasks, especially for languages not present in the pretraining data, although it faces challenges in zero-shot tasks and performance consistency across different scripts.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to addressing the vocabulary bottleneck of traditional language models and its adaptability to diverse languages, especially non-Latin scripts. The robustness of PIXEL against noisy inputs and its flexibility for various NLP tasks further enhance its utility. However, the pretraining bias towards English text limits its cross-lingual capabilities and may hinder performance in Latin script tasks. Additionally, while the model demonstrates promise in processing complex scripts, it struggles with long syntactic dependencies and is less sample-efficient during fine-tuning compared to subword-based models. The reliance on community contributions for further development raises concerns about consistency and ongoing support.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical flow that effectively communicates its methodologies and findings. The novelty of using pixel-based representations for language modeling is significant, pushing the boundaries of traditional NLP approaches. The authors provide open-source resources, including implementations and pretrained models, which enhance reproducibility. However, the challenges noted in the limitations highlight areas where further refinement is needed before the approach can be fully realized in practical applications.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to language modeling through pixel-based representations, showcasing significant potential for multilingual NLP applications. Nevertheless, the identified limitations regarding training diversity and performance consistency warrant attention for future research and development.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to language modeling called PIXEL (Pixel-based Encoder of Language), which innovatively represents text as images to address the limitations of traditional vocabulary constraints in multilingual scenarios. The methodology involves transforming textual data into visual representations, utilizing a Masked Autoencoding Visual Transformer (ViT-MAE) architecture, and employing span masking to enhance semantic understanding. The findings indicate that PIXEL achieves superior performance compared to traditional models like BERT, particularly in cross-lingual tasks involving non-Latin scripts, while also demonstrating resilience to orthographic variations and code-switching scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to language modeling by utilizing visual representations, which effectively mitigates the vocabulary bottlenecks encountered in conventional text-based models. The model exhibits impressive performance in diverse languages and tasks, showcasing its versatility and robustness. However, the reliance on English for pretraining may limit its effectiveness in structurally divergent languages, and the performance in semantic tasks may require further enhancement. Additionally, the computational overhead associated with image processing could pose challenges for practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, facilitating understanding for readers. The novel approach of using pixel representations adds significant value to the field of language modeling, and the empirical results are presented with clarity. However, the reproducibility of the results may be impacted by the computational demands of the model, which should be addressed in future work to ensure broader accessibility.\n\n# Summary Of The Review\nOverall, the PIXEL framework represents a significant advancement in language modeling by leveraging visual representations of text, which effectively overcomes traditional vocabulary limitations and enhances performance across multilingual tasks. Despite some limitations regarding pretraining and computational overhead, the contributions of this work are substantial and warrant further exploration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper introduces PIXEL, a novel approach to adversarial training that transforms textual data into pixel-based representations. By leveraging visual characteristics instead of traditional tokenization methods, PIXEL aims to address vulnerabilities in text-based models against adversarial attacks. The authors detail the architecture of PIXEL, which utilizes a Vision Transformer (ViT) for reconstructing masked image patches. They demonstrate that PIXEL exhibits improved robustness across various adversarial scenarios, outperforming existing models like BERT, particularly when faced with noise and orthographic manipulation in multiple languages.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative representation of textual data, which moves beyond conventional tokenization and provides a fresh perspective on adversarial robustness. The empirical results are compelling, showing significant performance gains in challenging adversarial settings. However, a notable weakness is the high computational resource requirements for training PIXEL, which may limit its practical applicability. Additionally, while the paper discusses future work, it does not thoroughly address scalability issues or how PIXEL could be adapted for broader datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly structured, making it accessible to readers with varying levels of expertise in the field. The methodology is described in sufficient detail, allowing for reproducibility of the results. The novelty of the pixel-based representation is significant, as it challenges existing paradigms in adversarial training. However, the authors could enhance clarity by providing more contextual background on existing models and their limitations.\n\n# Summary Of The Review\nOverall, the paper presents a novel and promising approach to adversarial training through pixel-based representations, demonstrating significant improvements in robustness against various adversarial attacks. While the methodology and results are strong, the practical limitations regarding computational efficiency warrant further exploration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces PIXEL, a novel language model that reframes language modeling as a visual recognition task, thereby eliminating traditional vocabulary structures. It demonstrates remarkable performance across a variety of syntactic and semantic tasks in over 32 languages, including non-Latin scripts, suggesting its capacity as a universal language model. PIXEL outperforms BERT in handling orthographic attacks and code-switching, highlighting its robustness in real-world applications. The authors provide open access to PIXEL's implementation and pretrained models, fostering community engagement and future research.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to language modeling, which could significantly advance multilingual processing by removing the need for vocabulary constraints. The performance of PIXEL across diverse languages is a major contribution, indicating its broad applicability. However, the paper downplays weaknesses, such as its slightly inferior performance in Latin scripts compared to BERT, which could mislead readers about its overall efficacy. Additionally, while the model's training efficiency is noted, the suggestion that more training steps are necessary may deter some practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings with clarity, making the contributions easily understandable. The novelty of the PIXEL model is striking, as it challenges existing paradigms in language modeling. The authors' decision to provide open access to the model enhances reproducibility, allowing researchers to build upon their work without access barriers. However, some limitations regarding language reliance and training efficiency could have been elaborated more thoroughly.\n\n# Summary Of The Review\nOverall, the paper presents a transformative model in PIXEL that has the potential to redefine language modeling, particularly in multilingual contexts. While it showcases significant advancements, certain weaknesses should be addressed for a more balanced view of its performance across different language scripts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the PIXEL model, a novel pretrained language model that utilizes images to represent text, addressing the vocabulary limitations of traditional language models. With 86 million parameters, PIXEL is trained to process text as images through a combination of a text renderer, encoder, and decoder, leveraging Vision Transformer architecture for encoding. The results demonstrate that PIXEL outperforms BERT on syntactic and semantic tasks, particularly in languages with non-Latin scripts, while showing competitive performance in Latin scripts and enhanced robustness against orthographic attacks and code-switching challenges.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to language modeling by integrating visual representations, which effectively mitigates the vocabulary bottleneck. The empirical results highlight significant improvements in performance across a diverse set of languages, especially those with less common scripts, making a strong case for the model's applicability in multilingual contexts. However, the paper could benefit from a more detailed analysis of the model's limitations, particularly regarding its performance in Latin-script languages, where the advantages over BERT are less pronounced. Additionally, more insights into the model's training dynamics and computational requirements would enhance the reader's understanding.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The writing quality is high, with appropriate technical depth for the target audience. The novelty of using visual representations for language modeling is significant, and the experiments are designed to be reproducible, with sufficient details regarding dataset sources and training processes. However, it would be beneficial to include more comprehensive details on the hyperparameter choices and training conditions to further facilitate reproducibility.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling and innovative approach to language modeling with the PIXEL model, demonstrating outstanding performance improvements in a multilingual context, particularly for languages with non-Latin scripts. While the findings are impressive, further exploration of the model's limitations and enhanced details on reproducibility could strengthen the contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents PIXEL, a novel approach to language modeling that reframes the task as one of visual recognition, thereby bypassing traditional vocabulary constraints. The authors argue that this method eliminates the vocabulary bottleneck associated with conventional language models, allowing for potentially improved performance on underrepresented languages and scripts. The methodology involves pretraining the model primarily on English data and evaluating its capabilities in cross-lingual tasks, robustness to orthographic noise, and generalization across different scripts.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its ambitious approach to redefining language modeling and its potential insights into handling underrepresented languages. The emphasis on visual representation may open new avenues for model development. However, several weaknesses are apparent. The assumptions regarding the sufficiency of English-centric pretraining raise concerns about generalization to other linguistic contexts. Additionally, the paper's claims about robustness and computational efficiency require thorough empirical validation, as the reliance on specific rendering technologies may limit broader applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its key concepts and contributions. However, some assumptions and claims lack sufficient empirical backing, which may affect reproducibility. The novelty of modeling language as visual input is significant, but the practical implications and limitations of this approach should be more thoroughly examined. The comparisons with existing models, particularly BERT, need to be more comprehensive to establish a fair assessment of PIXEL's advantages.\n\n# Summary Of The Review\nOverall, the paper introduces an innovative perspective on language modeling with the PIXEL framework, but it suffers from certain methodological weaknesses and a lack of comprehensive empirical validation. While the approach is promising, it requires further exploration of its limitations and the practical implications of its assumptions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents PIXEL, a pretrained language model designed to overcome the vocabulary bottleneck in multilingual NLP by converting text into images. The methodology involves a text renderer that transforms text into visual representations, an encoder that processes these images, and a decoder that reconstructs masked portions at the pixel level. The findings indicate that PIXEL outperforms BERT in various syntactic and semantic tasks, especially for languages with non-Latin scripts, while demonstrating improved robustness against orthographic noise and code-switching compared to traditional models.\n\n# Strength And Weaknesses\nThe main strengths of this paper include its innovative approach to circumvent the vocabulary limitations of traditional language models, which is particularly relevant in multilingual contexts. The empirical results highlight PIXEL's superior performance on non-Latin scripts and its resilience to orthographic attacks, making it a significant contribution to the field. However, a notable weakness is its performance lag behind BERT on tasks involving Latin-script languages, which raises questions about its overall versatility across all language types. Additionally, the paper could benefit from a more detailed analysis of the limitations of the proposed method and the potential computational costs associated with image rendering.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making it accessible to readers familiar with NLP and language modeling. The methodology is innovative, particularly in its approach to text representation. However, the reproducibility of the results could be improved by providing more details on the dataset used for training and evaluation, as well as the specific implementation of the model. Overall, while the quality of the writing is high, further transparency in methodology would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper introduces a novel and compelling approach to multilingual NLP that addresses significant limitations of existing models. While it demonstrates impressive results, particularly for non-Latin scripts, the performance on Latin-script languages and the model's computational efficiency warrant further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework for enhancing the performance of neural networks in multi-task learning scenarios. It addresses the challenge of balancing shared and task-specific representations by proposing a dual-branch architecture that facilitates more effective feature extraction across diverse tasks. The authors conduct comprehensive experiments on several benchmark datasets, demonstrating significant improvements in accuracy and generalization compared to existing methods.\n\n# Strengths And Weaknesses\nStrengths of the paper include a robust theoretical foundation that is well-supported by empirical results, showcasing the effectiveness of the proposed architecture. The clarity of the methodology and the logical progression of ideas contribute to a strong narrative throughout the paper. However, the experimental setup could be improved by including additional datasets and tasks to validate the generalizability of the findings. Furthermore, while the contributions are well-articulated, some sections lack in-depth discussions regarding the theoretical implications of the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and organized, with clear explanations of the proposed methodology and results. Visualizations and tables are effectively utilized, although additional context in figure captions would enhance understanding. The novelty of the approach is commendable, but certain methodological details are insufficiently elaborated, which may hinder reproducibility. The authors should consider providing clearer guidelines or pseudocode to facilitate replication of the experiments.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in multi-task learning with a novel dual-branch architecture. While the contributions are significant, the paper would benefit from additional experimental validations and improved clarity in methodological details to enhance reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces PIXEL (Pixel-based Encoder of Language), a novel approach to language modeling that addresses the limitations of traditional language models, which are constrained by fixed vocabularies. PIXEL represents text as images, allowing for transfer learning across languages based on orthographic similarities through pixel co-activation. The findings indicate that PIXEL outperforms BERT on various syntactic and semantic tasks in languages not included in its pretraining, though it shows slightly lower performance on Latin scripts. Additionally, PIXEL demonstrates enhanced robustness against orthographic attacks and is effective in handling linguistic code-switching.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to language modeling, which could significantly expand the accessibility and applicability of language models to underrepresented languages. The methodology of using pixel representations is a creative solution to the vocabulary bottleneck, and the empirical results support the effectiveness of this approach. However, the paper notes that PIXEL currently underperforms on Latin scripts compared to BERT, indicating a potential area for improvement. Additionally, while the robustness against orthographic noise is commendable, the paper could benefit from more extensive evaluation across diverse datasets and languages to fully establish its efficacy.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, making it accessible to readers with varying levels of expertise in the field. The novelty of representing language as images is significant, pushing the boundaries of traditional language models. However, further details on the training process and specific experimental setups would enhance reproducibility and allow other researchers to build upon this work effectively. Overall, the quality of the research appears solid, but more comprehensive empirical validation would strengthen the claims made.\n\n# Summary Of The Review\nOverall, the paper presents a novel and promising approach to language modeling that addresses existing limitations in traditional models. While the findings are encouraging, there is room for improvement in the evaluation of the model's performance across different languages, particularly Latin scripts, and further clarification on reproducibility would enhance the impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces PIXEL, a novel pretrained language model that represents text as images, addressing the vocabulary bottlenecks observed in traditional language models. The methodology includes a unique architecture consisting of a text renderer, encoder, and decoder, utilizing a masked autoencoder approach for training. The findings indicate that PIXEL outperforms BERT on non-Latin scripts and demonstrates robustness against orthographic attacks and code-switching, although it shows weaker performance on Latin scripts and exhibits limitations related to multilinguality and sample efficiency.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to tackling the vocabulary bottleneck issue in NLP, providing significant improvements for non-Latin language processing. The robustness analysis against orthographic attacks is a notable contribution, highlighting PIXEL's practical applicability in real-world scenarios. However, weaknesses include its limited pretraining on English text, which restricts its cross-lingual performance, and the increased resource requirements due to the image-based representation of text. Additionally, the model's inefficiency in processing right-to-left languages and its challenges in generating discrete words are significant limitations that could hinder its broader applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and results of the research. The quality of the experiments is commendable, providing a thorough evaluation of PIXEL against established baselines. The novelty of representing text as images is striking, and the potential implications for multilingual NLP are significant. However, reproducibility may be an issue due to the resource-intensive nature of the training process and the specific requirements for rendering text as images, which may not be easily accessible to other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel approach to language modeling with its PIXEL model. While it offers significant advantages for non-Latin scripts, the limitations regarding Latin script performance and the challenges related to multilingual training and resource efficiency must be addressed in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces PIXEL, a novel pretrained language model that innovatively represents text as images to address the vocabulary bottlenecks faced by traditional language models, particularly in diverse languages. The PIXEL model employs a Vision Transformer architecture and masked autoencoding, pretraining primarily on English datasets akin to BERT. The evaluation encompasses various syntactic and semantic tasks across 32 languages and 14 scripts, demonstrating that PIXEL outperforms BERT on non-Latin scripts while showing slightly lower performance on Latin scripts.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach of using visual representations for language modeling, which expands the potential for multilingual understanding and processing. The comprehensive evaluation across multiple languages and tasks provides strong empirical support for the claims. However, a notable weakness is the reliance on English for pretraining, which raises questions about the model's effectiveness in semantic processing and its performance in right-to-left languages. Moreover, the inability to generate text poses limitations for practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear structure that effectively communicates the methodology and findings. The quality of the presentation is high, featuring organized tables for results and a coherent discussion of implications and limitations. The novelty of representing text as images is significant, although the reproducibility is somewhat constrained by the model's training focus and the challenges noted in handling certain language structures.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and innovative approach to language modeling through the use of visual representations, with promising results in multilingual applications. While there are limitations regarding training data and certain language features, the contributions to the field are noteworthy, warranting a strong recommendation for publication.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents PIXEL, a novel pretrained language model designed to address the vocabulary bottleneck inherent in traditional language models (LMs) by encoding text as images. By utilizing masked pixel reconstruction instead of token prediction, PIXEL demonstrates superior performance across a variety of syntactic and semantic tasks, particularly in underrepresented languages and scripts. The architecture consists of a text renderer, an encoder, and a decoder, with extensive evaluation across 32 languages showing robustness against orthographic variations and improved handling of out-of-vocabulary words.\n\n# Strength And Weaknesses\nThe primary strength of PIXEL lies in its innovative approach to overcoming the limitations of conventional LMs by eliminating the need for a finite vocabulary, thereby enhancing multilingual capabilities and resilience to noise. The model's architecture is well-defined, employing a Transformer-based design that is consistent with current methodologies in the field. However, a notable weakness is its reliance on pretraining primarily on English text, which may limit its effectiveness in truly diverse linguistic settings. Additionally, the model's performance on tasks involving Latin scripts was not as robust, and challenges remain regarding processing right-to-left languages and discrete word generation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, articulating complex concepts in an accessible manner. The quality of the writing and the presentation of results are commendable, providing thorough explanations of the methodology and experiments conducted. The novelty of PIXEL is significant, particularly in its approach to encoding text as images, which diverges from traditional token-based methods. Reproducibility is facilitated through detailed descriptions of the architecture, training processes, and evaluation metrics; however, the reliance on specific datasets for pretraining may impact generalizability.\n\n# Summary Of The Review\nOverall, PIXEL represents a substantial advancement in language modeling by effectively addressing the vocabulary bottleneck issue, allowing for enhanced performance across diverse languages. While the model shows promise, particularly in non-Latin scripts, its reliance on English-centric pretraining and certain limitations in handling specific tasks warrant further exploration and optimization.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents PIXEL, a novel language model designed to process text as images, aiming to enhance performance in multilingual contexts. The methodology involves transforming text inputs into image representations, leveraging visual characteristics for language understanding. However, the findings reveal significant limitations, including poor performance on Latin scripts, sample inefficiency, and vulnerabilities to orthographic attacks, which raise concerns about the model's practicality and robustness across various languages and applications.\n\n# Strength And Weaknesses\nThe main strength of PIXEL lies in its innovative approach to text representation, which could theoretically allow for unique insights into language processing. However, the weaknesses are pronounced: the model's reliance on a predominantly English pretraining dataset limits its generalizability and efficacy in multilingual settings. Additionally, its inefficiency in sample usage, vulnerability to orthographic attacks, and challenges in processing right-to-left languages further diminish its competitiveness against established models like BERT. The inability to generate text and increased resource requirements are additional drawbacks that hinder its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its concepts clearly, outlining the model's framework and intended applications. Nonetheless, the lack of comprehensive hyperparameter tuning and the acknowledgment of numerous areas for future work suggest that the model's current state may not be reproducible or optimized for various tasks. While the novelty of processing text as images is commendable, the empirical results indicate that the model may not yet be ready for practical deployment.\n\n# Summary Of The Review\nOverall, while PIXEL introduces an intriguing methodology for language processing, it faces significant limitations that compromise its effectiveness and applicability. The weak performance on Latin scripts, inefficiency, and vulnerability to attacks detract from its potential as a robust multilingual model.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces PIXEL (Pixel-based Encoder of Language), a novel approach to language modeling that represents text as images, effectively addressing the limitations posed by traditional vocabulary constraints. The methodology employs masked autoencoders and dynamic span masking to enhance learning efficiency and abstraction. The findings demonstrate PIXEL's remarkable performance across 32 languages and 14 scripts, showcasing superior robustness against orthographic attacks and linguistic code-switching, as well as effective transfer learning capabilities based on orthographic similarities. The authors provide open-source access to the model and its implementations, promoting collaborative advancement in multilingual natural language processing (NLP).\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to language modeling, which significantly enhances multilingual capabilities and breaks down barriers posed by vocabulary limitations. The empirical results are impressive, with PIXEL outperforming existing models like BERT in robustness and transfer learning. However, a potential weakness lies in the complexity of the proposed methodology, which may require further clarification for practitioners new to the domain. Additionally, while the paper highlights diverse applications, specific case studies demonstrating practical implementations could strengthen its impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, making it accessible to a broad audience. The novel approach of rendering text as images is a significant contribution to the field, enhancing both technical and empirical aspects of language modeling. The open-source availability of the model and its components supports reproducibility and encourages further research. Additional elaboration on certain technical aspects could improve clarity, particularly regarding the training methodology and its implications for real-world applications.\n\n# Summary Of The Review\nOverall, the paper presents a groundbreaking approach to language modeling with PIXEL, demonstrating significant advancements in multilingual NLP through innovative methodologies and impressive empirical results. Its open-access framework fosters community collaboration, positioning PIXEL as a transformative force in addressing language barriers.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents PIXEL, a novel approach to language modeling that addresses the limitations of traditional tokenization by treating language as a visual recognition task. Key contributions include the introduction of a Masked Autoencoding Visual Transformer (ViT-MAE) architecture, which reconstructs masked pixel patches instead of predicting discrete tokens, thereby enhancing the model's ability to capture higher-order linguistic features. The findings suggest that PIXEL improves robustness to orthographic variability and facilitates better cross-lingual transfer learning, potentially allowing for greater inclusivity in processing diverse languages and scripts.\n\n# Strength And Weaknesses\nThe primary strength of PIXEL lies in its innovative conceptual framework that redefines language modeling through visual representation, potentially overcoming the vocabulary bottleneck inherent in traditional models. The span masking mechanism enhances the model's focus on meaningful text units, aligning with cognitive theories of language processing. However, the paper does not adequately address challenges such as sample efficiency and the effective learning of long-range dependencies, which may limit the practicality of the proposed approach in real-world applications. Additionally, while the theoretical implications are promising, empirical validation and concrete experimental results are necessary to fully establish the model's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its theoretical framework clearly, making the novel contributions comprehensible. The quality of the writing is high, with a focus on theoretical implications and innovative methodologies. However, the lack of empirical validation raises concerns about reproducibility, as the paper primarily emphasizes theoretical aspects without providing sufficient experimental evidence to support its claims.\n\n# Summary Of The Review\nOverall, PIXEL offers a compelling and innovative shift in language modeling by leveraging visual representations, which may address several limitations of traditional NLP approaches. While the theoretical contributions are significant, the lack of empirical validation and attention to practical challenges diminishes its immediate applicability and impact in the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents PIXEL, a novel pixel-based encoder of language that addresses the vocabulary bottleneck in traditional language models by converting text into images. The methodology involves rendering text as RGB images and employing a Vision Transformer (ViT) architecture to process the resulting image patches. The model, which consists of 86 million parameters, is pretrained on a large corpus of English data and evaluated on a variety of language tasks, demonstrating competitive performance, especially in handling unseen scripts.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach of leveraging pixel representations for language processing, which allows for a more flexible handling of vocabulary and potentially enhances cross-lingual capabilities. The implementation details are thorough, with clear descriptions of the architecture, pretraining, and fine-tuning processes, along with the availability of code and pretrained models for reproducibility. However, the paper acknowledges limitations, such as the model's current focus on English, challenges with right-to-left languages, and lower sample efficiency compared to subword-based models. Additionally, while the technical implementation is well-documented, the broader implications and potential impacts of the PIXEL model are not extensively discussed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, providing clear explanations of the methodology and architecture. The quality of the experiments is solid, with a variety of tasks and languages assessed, enhancing the credibility of the findings. The novelty lies in the approach of treating text as images, which is less common in the field of language modeling. The authors have ensured reproducibility by making their code and models publicly available, which is a significant positive aspect for the research community.\n\n# Summary Of The Review\nOverall, the PIXEL paper presents an interesting and innovative approach to language modeling by using pixel representations of text. While the implementation is robust and the findings are compelling, the paper could benefit from a stronger emphasis on the broader impacts and implications of the proposed model.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces PIXEL, a pixel-based language model designed to handle multiple scripts, with claims of outperforming BERT on non-Latin scripts. The methodology involves a unique architecture aimed at overcoming vocabulary bottlenecks. The findings suggest that PIXEL demonstrates robustness against orthographic attacks and performs well in code-switching scenarios. However, the authors do not provide a comprehensive analysis of how PIXEL's performance compares to BERT in various contexts, particularly in terms of data diversity and fine-tuning capabilities.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its presentation of PIXEL as a novel approach to language modeling, particularly for non-Latin scripts and scenarios involving code-switching. However, the paper exhibits several weaknesses, including a biased framing of its contributions against established models like BERT. The authors downplay BERT's established performance on Latin scripts and its advantages stemming from extensive training data. Furthermore, the discussion on limitations is insufficient, as it fails to adequately address BERT's efficiency and the systematic advantages it has over PIXEL in various contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by its selective comparisons and lack of a balanced view regarding the strengths of BERT. While the novelty of PIXEL is evident, it appears to build on the existing landscape of language models without fully acknowledging previous advancements. Reproducibility is supported by the authors' mention of PIXEL being publicly available; however, the lack of a comprehensive evaluation against BERT's ecosystem may hinder its practical adoption.\n\n# Summary Of The Review\nOverall, while the paper presents PIXEL as a promising model for specific applications, it suffers from biased comparisons with BERT and fails to recognize the established advantages of existing models. A more balanced discussion would strengthen the contributions of PIXEL within the broader context of language modeling.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"LANGUAGE MODELLING WITH PIXELS\" presents a novel approach to language modeling by treating it as a visual recognition task. The authors introduce PIXEL, a pretrained language model capable of rendering text as images, which supports thousands of written languages within a single model. The findings demonstrate that PIXEL significantly outperforms BERT in various benchmarks, showcasing its effectiveness in cross-lingual transferability and the ability to handle non-Latin scripts.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by proposing a fresh perspective on language modeling, leveraging visual recognition techniques. The methodology is innovative, integrating text rendering with established language modeling frameworks. However, the paper suffers from several clarity and consistency issues, such as inconsistent formatting of figures, section headings, and mathematical notations. Additionally, some claims lack objective phrasing, which could lead to misinterpretation of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the novelty of the approach is commendable, the clarity of the presentation is compromised by inconsistent formatting and grammatical errors. The paper would benefit from revisions to enhance readability, such as uniform citation styles and clearer definitions of introduced terms. Despite these issues, the core methodology is reproducible, as the authors provide sufficient detail regarding their experimental setup and results.\n\n# Summary Of The Review\nOverall, the paper introduces an innovative approach to language modeling with PIXEL, demonstrating significant empirical results. However, clarity and consistency issues detract from its presentation, necessitating revisions for improved readability and professionalism.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents PIXEL, a novel model designed for processing English text rendered as images. It primarily focuses on syntactic tasks, offering a unique approach to feature extraction from visual text representations. The authors report that while PIXEL performs well on syntactic tasks, its performance on semantic tasks remains subpar. Limitations include its applicability to other languages and scripts, inadequate handling of right-to-left languages, and a lack of generative capabilities. The paper also discusses the challenges related to data augmentation, disk space requirements, and the need for additional methodologies to enhance the model's performance.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to utilizing visual representations of text and the initial results demonstrating competency in syntactic tasks. However, it also has notable weaknesses. The limited focus on English and Latin scripts restricts its applicability, and the inadequate performance on semantic tasks highlights a significant gap that needs addressing. Additionally, the lack of exploration into generative capabilities and the handling of diverse languages diminishes its robustness and versatility. The absence of a comprehensive discussion on data quality, computational resource requirements, and ethical implications further weakens the paper’s contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is relatively clear and well-structured, presenting its methodology and findings in an accessible manner. However, the novelty of the approach is somewhat undermined by the lack of detailed comparisons with other state-of-the-art models and inadequate exploration of alternative rendering techniques. The reproducibility of the results may be hindered by the limited discussion on computational resources and the data's quality. Overall, while the paper introduces a novel concept, it falls short in providing sufficient detail for effective replication and understanding of the broader implications.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting approach to text processing through visual representations but has significant limitations regarding its applicability, performance on semantic tasks, and handling of diverse languages. Future work should address these gaps and thoroughly explore the implications of the model's use in practical applications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents PIXEL, a pretrained language model that innovatively encodes text as images, thereby addressing the vocabulary bottleneck commonly encountered in language modeling. It evaluates PIXEL's performance on a range of syntactic and semantic tasks across 32 languages and 14 scripts, highlighting its competitive edge, especially in low-resource languages. The findings indicate that while PIXEL slightly underperforms compared to BERT in English, it excels in languages with a high incidence of unknown tokens, suggesting potential benefits for multilingual applications.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to language modeling by transforming text into images, which could significantly enhance performance in multilingual contexts. The experimental design is robust, covering a diverse array of languages and scripts, and the evaluation metrics are well-defined. However, the paper lacks comprehensive statistical analyses to substantiate its claims, such as p-values for performance comparisons and confidence intervals around results. Additionally, the reliance on predominantly English pretraining data raises questions regarding the model's cross-lingual transferability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, presenting its methodology and findings in an accessible manner. However, the novelty of the approach could be better highlighted with explicit comparisons to existing models beyond BERT. The reproducibility of results may be hindered by insufficient detail regarding the statistical methods employed in the analyses. Future iterations of the work should focus on providing thorough statistical validation to allow for independent verification of findings.\n\n# Summary Of The Review\nOverall, the paper introduces a novel language modeling approach that shows promise for multilingual applications. However, the lack of thorough statistical validation and reliance on English data limits the robustness of its claims and the generalizability of its findings. The authors are encouraged to strengthen their statistical methodology in future work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces PIXEL, a novel approach that utilizes image representations of text for various natural language processing tasks. The methodology involves pretraining on predominantly English text and adapting it for syntactic and semantic tasks. The authors find that while PIXEL performs commendably on syntactic benchmarks, it exhibits limitations in semantic processing, particularly in comparison to established models like BERT. Additionally, challenges related to processing right-to-left languages, sample efficiency, and font adaptation are highlighted, suggesting areas for improvement in future work.\n\n# Strength And Weaknesses\nThe main strengths of PIXEL lie in its innovative use of image-based text representation and its solid performance on syntactic tasks. However, the weaknesses are significant; the model shows poor performance in semantic tasks, struggles with cross-lingual transfer, and is less sample-efficient compared to subword-based models. Furthermore, the architecture's handling of right-to-left languages and the incompatibility with language generation tasks limit its applicability. The reliance on English for pretraining also raises questions about the model's generalizability across different languages and scripts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation of methodology and findings, although some sections could benefit from deeper explanations of the underlying mechanisms of the model. The novelty of using images for text representation is notable, but the limitations in performance and efficiency reduce its overall impact. Reproducibility might pose a challenge due to the specific conditions under which the model was trained and the potential issues with font variations and disk space requirements.\n\n# Summary Of The Review\nOverall, while PIXEL presents an interesting approach to text representation through images, its limitations in semantic processing, efficiency, and language adaptability significantly hinder its effectiveness compared to existing models like BERT. Future work addressing these issues could enhance the model’s relevance and applicability in the NLP field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Language Modelling with Pixels\" proposes a novel approach to address the vocabulary bottleneck in natural language processing (NLP) by rendering text as images, thus leveraging Vision Transformers for text representation. The authors argue that their PIXEL architecture can improve performance across multiple languages, particularly non-Latin scripts, by pretraining on a diverse corpus of 32 languages. Their experimental results indicate that while PIXEL performs well in certain scenarios, it faces limitations such as sample efficiency and challenges with right-to-left languages.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its attempt to tackle the well-known issue of vocabulary limitations in NLP models and its evaluation across a diverse set of languages, which is commendable. However, the methodology of using image representations for text seems to lack novelty, as similar concepts have been previously explored. The authors' findings regarding performance improvements appear to confirm existing knowledge rather than provide groundbreaking insights. Furthermore, the limitations discussed could have been more prominently addressed in the introduction to set clearer expectations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, though some sections could benefit from more detailed explanations, particularly regarding the architectural choices and their implications. The quality of the writing is generally good, but the novelty of the proposed approach is questionable, as it does not significantly diverge from established methods. Reproducibility is a potential concern, as the lack of detailed experimental protocols may hinder other researchers from replicating the results.\n\n# Summary Of The Review\nThe paper presents a commendable effort to address vocabulary limitations in NLP through a novel architectural approach. However, the contributions lack significant novelty and the findings largely reaffirm existing beliefs in the field. Improvements in clarity and a more robust discussion of limitations would enhance the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces PIXEL, a novel pixel-based language model that represents a significant departure from traditional tokenization methods in NLP. Utilizing a Masked Autoencoding Visual Transformer (ViT-MAE), PIXEL is designed to address challenges such as orthographic variations and code-switching, making it particularly relevant in multilingual contexts. The authors demonstrate that PIXEL outperforms BERT in several languages, especially where BERT struggles due to vocabulary constraints. They also emphasize the importance of longer pretraining for improved semantic processing and propose future work to pretrain PIXEL on diverse multilingual data.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to representation learning through a pixel-based model, which could potentially enhance performance in multilingual NLP tasks. The empirical findings are compelling, especially the model's robustness against orthographic attacks and its adaptability to unseen scripts, supporting the idea of universal models. However, the paper's limitations, particularly its reliance on a predominantly English training corpus, raise concerns about the model's generalizability and effectiveness across diverse languages. Additionally, the requirement for longer pretraining to achieve optimal performance suggests a trade-off in practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly, making it accessible to readers familiar with NLP concepts. The quality of the experiments and evaluation metrics appears rigorous, although further details on the training process and hyperparameter optimization would enhance reproducibility. The novel approach of leveraging pixel-based representations is a significant contribution, though additional exploration of alternative masking strategies could have further emphasized the model's potential.\n\n# Summary Of The Review\nOverall, the paper presents a promising advancement in the field of multilingual NLP through the introduction of PIXEL, showcasing its ability to overcome limitations of traditional models like BERT. While the findings are significant, the model's dependency on English training data and the need for extensive pretraining warrant further examination to ensure broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces PIXEL, a transformer-based model aimed at improving performance on various natural language processing (NLP) tasks, particularly for languages with limited representation in pretraining datasets. The authors evaluate PIXEL on syntactic tasks such as part-of-speech tagging and dependency parsing, as well as semantic tasks like named entity recognition and overall performance on the GLUE benchmark. Findings indicate that PIXEL excels in non-Latin scripts, significantly outperforming BERT in specific languages, while exhibiting robustness against orthographic attacks. The paper also notes that PIXEL requires more training steps to converge and suggests that enhanced multilingual pretraining could improve its performance across all tasks.\n\n# Strength And Weaknesses\nStrengths of the paper include its focus on cross-lingual scenarios and the rigorous evaluation across a range of languages, particularly non-Latin scripts where PIXEL shows substantial improvements over BERT. The model also displays robustness in various noisy conditions, which is a key consideration for real-world applications. However, the weaknesses are notable: PIXEL struggles with Latin-script tasks and consistently underperforms BERT in semantic tasks. Additionally, its slower convergence rate raises concerns about its sample efficiency compared to established models like BERT.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, findings, and implications. The quality of the experiments is commendable, with a comprehensive analysis of performance across different languages. However, the novelty of the approach could be debated, as the architecture and principles underlying PIXEL are derived from existing transformer models. Reproducibility is supported by the detailed descriptions of the training processes and evaluation metrics, although additional information on the datasets used would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for PIXEL's strengths in handling cross-lingual tasks, particularly for non-Latin scripts, while also highlighting its limitations in Latin-script languages and semantic tasks. The results are promising but suggest the need for further development and training to enhance performance relative to state-of-the-art models like BERT.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"LANGUAGE MODELLING WITH PIXELS\" introduces a novel approach to language modeling utilizing pixel-based representations. The authors propose a methodology that leverages a masked autoencoder and the Vision Transformer (ViT) architecture to overcome challenges such as the vocabulary bottleneck and orthographic similarity that plague traditional natural language processing (NLP) models. The findings indicate that the proposed PIXEL model demonstrates superior performance in various benchmarks compared to established models like BERT, suggesting the efficacy of pixel representation in language tasks.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by addressing key limitations of existing NLP models, particularly through its innovative use of pixel representations. The methodology is robust and integrates well-established techniques in a new context. However, the paper has weaknesses, including a lack of clarity in the presentation of its results and insufficient contextualization of technical terms, which may hinder understanding for readers less familiar with the field. Additionally, while the findings are promising, the implications of the results could be better articulated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe writing quality varies throughout the paper, with some sections being dense and complex. The introduction lacks clear structure, making it difficult for readers to follow the logical flow of ideas. While the novelty of using pixel representations in language modeling is commendable, the paper could improve its reproducibility by providing clearer explanations of methodologies and data presentation. The use of visuals and tables is helpful, but they require better captions and contextualization to enhance reader comprehension.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to language modeling that shows promise in addressing existing challenges. However, the clarity of presentation and contextualization of complex ideas need improvement to make the findings more accessible and impactful for a wider audience.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.7395520404773315,
    -1.7289808396055009,
    -1.9614489080147002,
    -1.8820189005944419,
    -2.0686938525889427,
    -1.814837736772387,
    -1.7353106331380699,
    -2.0480816491952107,
    -1.7102669802558197,
    -1.994010166457444,
    -1.6740542519420911,
    -1.6772388142940686,
    -1.8026535940625121,
    -1.7825953244685215,
    -2.017574537002039,
    -1.9103525902137395,
    -1.98143703513459,
    -1.9854807489519706,
    -1.9356260036925497,
    -1.797301454592452,
    -2.040487181067542,
    -1.940854032184768,
    -1.8080782276443563,
    -1.7983903787356657,
    -1.8473244570315208,
    -1.8323222174042078,
    -1.879488244476831,
    -1.7927213126333998,
    -1.812503793871571
  ],
  "logp_cond": [
    [
      0.0,
      -2.485936658981231,
      -2.4952145025626065,
      -2.5114226880513364,
      -2.4921250671189203,
      -2.5076419977100697,
      -2.5363863340909556,
      -2.4822111707754497,
      -2.4872875591102686,
      -2.518442101509231,
      -2.498117390160238,
      -2.6182155248496004,
      -2.4833811719833925,
      -2.4956114952863,
      -2.5128749520205425,
      -2.5028907276612755,
      -2.5465518699001466,
      -2.5044685833504956,
      -2.5312727235097685,
      -2.51014386050293,
      -2.492883299913581,
      -2.544001226288308,
      -2.5639432593426394,
      -2.506138680205335,
      -2.529686168830126,
      -2.536436170601547,
      -2.4888314810456182,
      -2.51428294507997,
      -2.524731878052555
    ],
    [
      -1.3378605444661047,
      0.0,
      -1.2639332391205629,
      -1.2869156726775224,
      -1.2892342055450512,
      -1.3450201448562769,
      -1.382496946753386,
      -1.3287241483203258,
      -1.2559789825585073,
      -1.368672444917995,
      -1.2774553426069624,
      -1.502903888682687,
      -1.2808393063013357,
      -1.2262567588824416,
      -1.288390825454323,
      -1.222647197005302,
      -1.3709781172656053,
      -1.3757074412745245,
      -1.3749231389978795,
      -1.3372698966381218,
      -1.3364721510413133,
      -1.3849174739740862,
      -1.4184407689879222,
      -1.3601881991043385,
      -1.3534439999740997,
      -1.3410282003077234,
      -1.3125126849111128,
      -1.3036646441827118,
      -1.3709588865067734
    ],
    [
      -1.5436270998149861,
      -1.389932275373794,
      0.0,
      -1.435839074250312,
      -1.3554852877512449,
      -1.3998563356013751,
      -1.5136022153992656,
      -1.4511759397130342,
      -1.4725941062312504,
      -1.4895298766582106,
      -1.4449100155976242,
      -1.6658569711054512,
      -1.4137100082080385,
      -1.3338725926141386,
      -1.3518437131140857,
      -1.3615056048870273,
      -1.4182575582922294,
      -1.4438774520823396,
      -1.4799625447052938,
      -1.406976213512508,
      -1.4909799970748745,
      -1.5382597710194397,
      -1.543455922398381,
      -1.497783472538606,
      -1.402376591328491,
      -1.441298246231351,
      -1.4166201122637319,
      -1.4835396639320895,
      -1.508740508819576
    ],
    [
      -1.4859211821433909,
      -1.3879869647646632,
      -1.3773706652025486,
      0.0,
      -1.4346200382833676,
      -1.3756381746746031,
      -1.4675537381124204,
      -1.3977612365764809,
      -1.4313498217242389,
      -1.4372251953835358,
      -1.3657157579956676,
      -1.6200881071737245,
      -1.3951952651023372,
      -1.3581222490395324,
      -1.3715288562350207,
      -1.4058550783853607,
      -1.4942764600897434,
      -1.397005190725548,
      -1.4116171362963483,
      -1.5024567729332294,
      -1.4240115129044972,
      -1.5084519543196608,
      -1.521393828561777,
      -1.4514687903741759,
      -1.5323489359926241,
      -1.4924208594880364,
      -1.428796135307234,
      -1.4527342508303822,
      -1.4947623796539264
    ],
    [
      -1.6703171319988355,
      -1.5917884388482015,
      -1.5567669266455462,
      -1.6367501036946062,
      0.0,
      -1.6359762976122758,
      -1.7638474049159896,
      -1.6141431719267676,
      -1.6349442088974262,
      -1.6631692920565724,
      -1.607884112081532,
      -1.8112206382637581,
      -1.5828013121521736,
      -1.5774458389257413,
      -1.6120688180473466,
      -1.5788870202195788,
      -1.6435735229260264,
      -1.6457816680870618,
      -1.7197099041054302,
      -1.5934069104305513,
      -1.6465910498334535,
      -1.6663929831648627,
      -1.7228222992202369,
      -1.66952360826415,
      -1.6426004697769538,
      -1.6502640037586023,
      -1.6538176407962522,
      -1.6844045601403497,
      -1.6964815763033052
    ],
    [
      -1.381833371222805,
      -1.2622854162204873,
      -1.2016452268240376,
      -1.2535180567707207,
      -1.295804457901857,
      0.0,
      -1.3300377521784483,
      -1.3125772602257568,
      -1.3331197333060596,
      -1.2930063564552485,
      -1.3207519179954188,
      -1.504297730994273,
      -1.2874444413989645,
      -1.2792655526226644,
      -1.1996531085893185,
      -1.3139595720194857,
      -1.3759606755600957,
      -1.2803620242029863,
      -1.2725943993626743,
      -1.3277128165564351,
      -1.3442799811700967,
      -1.3948418254729396,
      -1.4220117566207777,
      -1.2937246000999059,
      -1.3926132660786268,
      -1.3831952095327187,
      -1.2484445859174624,
      -1.4038200306240056,
      -1.3759558863411459
    ],
    [
      -1.3339827113694882,
      -1.2750883371355006,
      -1.2501987322165806,
      -1.2544773503252171,
      -1.2937689304993143,
      -1.2524190228397676,
      0.0,
      -1.291192038323321,
      -1.3004635092419123,
      -1.285965192610215,
      -1.3413430913474549,
      -1.3946386788712264,
      -1.28426288719122,
      -1.3331105221280082,
      -1.2504122487190528,
      -1.3051168240915567,
      -1.33015962633081,
      -1.3097574173007827,
      -1.2768414655570473,
      -1.316113209280327,
      -1.2878556939278059,
      -1.3351472970695895,
      -1.3285167461784217,
      -1.2723606538842442,
      -1.3607565710673415,
      -1.3312550875991849,
      -1.2654568209483537,
      -1.3001021350231614,
      -1.2903948912169378
    ],
    [
      -1.6243437622851442,
      -1.5641163180246684,
      -1.5743093893173645,
      -1.5292382586820847,
      -1.512131909162331,
      -1.6664905375171672,
      -1.6756494227908234,
      0.0,
      -1.5120649222521731,
      -1.614212790738183,
      -1.5455010761665862,
      -1.7371636803371993,
      -1.5099313730038262,
      -1.5280527453921209,
      -1.5654785462706626,
      -1.5945174157841555,
      -1.6184761554829896,
      -1.5646728621958812,
      -1.6620151525317164,
      -1.5974760368157037,
      -1.5111004402064276,
      -1.5994152603261353,
      -1.6982477058942067,
      -1.58675034183025,
      -1.6550558215113542,
      -1.6199223163494425,
      -1.5578315528229203,
      -1.5229776025314619,
      -1.6408221396510756
    ],
    [
      -1.3370917884329443,
      -1.1619023180575814,
      -1.2058718679158813,
      -1.1991964980639733,
      -1.2559085095754634,
      -1.2762051901719216,
      -1.3330925359206516,
      -1.227469005561005,
      0.0,
      -1.3159058148610439,
      -1.1171251350147429,
      -1.449658649543197,
      -1.1751659498585338,
      -1.1636643133754498,
      -1.2052427455456673,
      -1.1565937211655004,
      -1.3052093928622528,
      -1.2491614719498463,
      -1.3485043288166343,
      -1.2997094457667382,
      -1.2804265747905577,
      -1.292687284338897,
      -1.3945457915365225,
      -1.2620499752879764,
      -1.3401656541500626,
      -1.267367929481189,
      -1.2528159972607178,
      -1.243991607010867,
      -1.28676222340138
    ],
    [
      -1.5719840764769544,
      -1.4823457118806291,
      -1.4897062976878048,
      -1.4791059832241233,
      -1.450759730456487,
      -1.4572118121973767,
      -1.5396459843609278,
      -1.470462024872955,
      -1.5269934936397536,
      0.0,
      -1.5302323377191474,
      -1.6237375004530097,
      -1.5113915819080943,
      -1.4704096630138073,
      -1.4689065498034897,
      -1.4895321475325713,
      -1.5363140336663594,
      -1.5170729483680856,
      -1.4988224471517817,
      -1.5638677307753326,
      -1.5071229137245938,
      -1.5510832484424062,
      -1.5567012175353732,
      -1.484785177978194,
      -1.547666794403745,
      -1.5807494566600435,
      -1.5214191566967041,
      -1.5448813642333667,
      -1.5498050240265004
    ],
    [
      -1.2479102426658497,
      -1.208087949181218,
      -1.192996283179269,
      -1.170727820982676,
      -1.2204781998795529,
      -1.229629624613011,
      -1.310173297773465,
      -1.1782586312114458,
      -1.1459618110859222,
      -1.2655634706057983,
      0.0,
      -1.4396535282948522,
      -1.1660467158918544,
      -1.114899076085422,
      -1.2370918149842427,
      -1.182355048506724,
      -1.2327903176044657,
      -1.2448304227065508,
      -1.2778391145351635,
      -1.3031637440773702,
      -1.219299557559183,
      -1.2958049893486705,
      -1.3304833886572431,
      -1.2479680978730237,
      -1.2791631356140607,
      -1.29623473840493,
      -1.2241761563922355,
      -1.226683650907223,
      -1.2813508729017702
    ],
    [
      -1.369179319586586,
      -1.3378657640563025,
      -1.3111245218672543,
      -1.3250446793764956,
      -1.3486260959721303,
      -1.3085011896135743,
      -1.311430808621063,
      -1.3277611825854596,
      -1.3440203480972601,
      -1.2923761501010314,
      -1.364113220695949,
      0.0,
      -1.3623676556604927,
      -1.3569112622643604,
      -1.2828006046767517,
      -1.3537776299300375,
      -1.381564992591246,
      -1.3619030363175757,
      -1.317194794651098,
      -1.3423839892106375,
      -1.3571312940143954,
      -1.306737602445185,
      -1.3216532662394842,
      -1.2446407316763641,
      -1.380181060823845,
      -1.3717559286300642,
      -1.3519617751058528,
      -1.3395381649944962,
      -1.2435826566767065
    ],
    [
      -1.3758744570381802,
      -1.2621078731389297,
      -1.252743855769888,
      -1.2735666518766837,
      -1.251812334636607,
      -1.3095572950563634,
      -1.4025232228466318,
      -1.2941571794362827,
      -1.2621309397383356,
      -1.3394887213059086,
      -1.2606374169959451,
      -1.5271464069767202,
      0.0,
      -1.2765041541595266,
      -1.3123664388361282,
      -1.2948955761385028,
      -1.3841276167893803,
      -1.2911971142203569,
      -1.3799017548488144,
      -1.3403700046533948,
      -1.2959770619408586,
      -1.3789550370015022,
      -1.4445619426704812,
      -1.3537957682708868,
      -1.41831088943214,
      -1.3382206037884767,
      -1.370878718114253,
      -1.3478289684091804,
      -1.360663702766214
    ],
    [
      -1.4306767426584375,
      -1.3095591022231214,
      -1.2712309509292017,
      -1.2862613485928407,
      -1.3168348877210156,
      -1.3746421631813106,
      -1.445599788180078,
      -1.3365812602976404,
      -1.3180404058596327,
      -1.3814764807516828,
      -1.294065229202922,
      -1.5232229213633313,
      -1.310903409760857,
      0.0,
      -1.328820174367524,
      -1.237825698949188,
      -1.3257492397216577,
      -1.335950708543255,
      -1.4102953094411341,
      -1.3439616753203947,
      -1.2932217745514332,
      -1.4028101724158335,
      -1.4387430310960878,
      -1.382151176189579,
      -1.365352720300772,
      -1.3823788422402177,
      -1.3596009125681776,
      -1.3707708349019636,
      -1.3926964170431073
    ],
    [
      -1.5550617077620255,
      -1.4736963186019463,
      -1.3942202902755245,
      -1.5004506760853957,
      -1.5058382700734547,
      -1.4354122329611583,
      -1.5712162776745278,
      -1.528914605070213,
      -1.49211983897014,
      -1.554794628560872,
      -1.5416391233229942,
      -1.6937390944028854,
      -1.5015702389932795,
      -1.4870683671637026,
      0.0,
      -1.4290764751256646,
      -1.5152070553794512,
      -1.5310394415492201,
      -1.5728938856967105,
      -1.5101779508432556,
      -1.5749484288572801,
      -1.5900185415514434,
      -1.5875314526702033,
      -1.3835181935458547,
      -1.562713016846648,
      -1.5023158704911856,
      -1.518213265442655,
      -1.5712298521269057,
      -1.5671957387481397
    ],
    [
      -1.5018632812969843,
      -1.3383569757147362,
      -1.2896996787450217,
      -1.4161853570687446,
      -1.3699046202483085,
      -1.4528773215105686,
      -1.5268924974588067,
      -1.4449262315853213,
      -1.415575756167772,
      -1.4360313405645266,
      -1.3980643430613746,
      -1.6234755442383209,
      -1.4156162388799172,
      -1.3019084233081775,
      -1.3558511010841765,
      0.0,
      -1.4120220873996632,
      -1.5007250299032309,
      -1.472628624584283,
      -1.4383340299841447,
      -1.459606193804433,
      -1.5209294128656354,
      -1.522565530673389,
      -1.4197949948961062,
      -1.4527750302833717,
      -1.4122033402257232,
      -1.4423309966543172,
      -1.452059997246501,
      -1.4856550015894503
    ],
    [
      -1.598770810776745,
      -1.486366611580515,
      -1.4003650188280663,
      -1.5386633654704902,
      -1.447032246896761,
      -1.524856885378407,
      -1.5829612453370125,
      -1.4771385887407185,
      -1.519044807659578,
      -1.501037958417019,
      -1.4793793926792422,
      -1.7096201940648825,
      -1.477552406477752,
      -1.395396082906281,
      -1.4611120670956237,
      -1.441977531937629,
      0.0,
      -1.4920543451574249,
      -1.5635447405279643,
      -1.4918164822991367,
      -1.4904377718093962,
      -1.5221355271875696,
      -1.5438108847841274,
      -1.490788622986276,
      -1.4103922856652882,
      -1.549575985026896,
      -1.4202531687685147,
      -1.5066026918467652,
      -1.5636887319096369
    ],
    [
      -1.542355279113483,
      -1.4856551331501036,
      -1.417643939775341,
      -1.455705236694294,
      -1.4282150400756786,
      -1.4176940208699984,
      -1.5243789689880127,
      -1.3474436661867166,
      -1.47725834515419,
      -1.4643563186503574,
      -1.4864309582258086,
      -1.6634188748236656,
      -1.3463844128984872,
      -1.4365244914088902,
      -1.4874646929359043,
      -1.5197083307183201,
      -1.4848559274129516,
      0.0,
      -1.5240450044104912,
      -1.5104893142913511,
      -1.466041726162138,
      -1.4833975323715662,
      -1.5829882442227639,
      -1.4938340111077375,
      -1.5485641821235612,
      -1.5222509881423758,
      -1.4273922896441882,
      -1.5051980040358295,
      -1.4813479193092325
    ],
    [
      -1.5695012881185901,
      -1.5160225056128787,
      -1.450688112687758,
      -1.4594441625562655,
      -1.4876999865769787,
      -1.383798938733795,
      -1.5052073539969322,
      -1.5347877099794291,
      -1.5930115826501314,
      -1.4617634143605358,
      -1.5311160932621197,
      -1.621020563060127,
      -1.5505200966311135,
      -1.4966258782424469,
      -1.4902734782347398,
      -1.4921766349740986,
      -1.5338691939701337,
      -1.4912280117119656,
      0.0,
      -1.5226688231818766,
      -1.589910423902886,
      -1.5342652171747428,
      -1.5502402346496056,
      -1.5145107101402342,
      -1.531700460649394,
      -1.5174337905421742,
      -1.4344377130575854,
      -1.5970415837989336,
      -1.504911192947378
    ],
    [
      -1.3962951982684966,
      -1.3215424220880454,
      -1.2590119653068823,
      -1.3485330505147883,
      -1.3033486189416745,
      -1.3535093944054994,
      -1.464378002473138,
      -1.3910970216979865,
      -1.37095766163124,
      -1.3890002564639037,
      -1.4022777912626432,
      -1.5577256741033434,
      -1.3834280388409752,
      -1.3177885653117027,
      -1.3638542076187934,
      -1.356685966107884,
      -1.3626037830286377,
      -1.3923068010813657,
      -1.412074283716327,
      0.0,
      -1.3779068802334773,
      -1.4086036672453623,
      -1.4425610323620055,
      -1.376693039085503,
      -1.3655785692716933,
      -1.3117717224580865,
      -1.3987833015530573,
      -1.4277671130370853,
      -1.3930776929799709
    ],
    [
      -1.643000872022083,
      -1.5287116260533449,
      -1.5765982599434152,
      -1.587777280733379,
      -1.5641975665645846,
      -1.605994103033509,
      -1.6597798729719033,
      -1.4897141803765088,
      -1.556310591751308,
      -1.621572535830367,
      -1.5764837312292235,
      -1.7844320322519749,
      -1.5364028528399392,
      -1.4674955871415267,
      -1.6417530534842268,
      -1.5922024643360049,
      -1.6005660054173967,
      -1.5442033011889915,
      -1.6707084786054578,
      -1.5972193932233087,
      0.0,
      -1.581488922022823,
      -1.6878767856249233,
      -1.6473681294043752,
      -1.6075979055907694,
      -1.6314180531683666,
      -1.562761574868276,
      -1.5938074645266815,
      -1.5942943945642902
    ],
    [
      -1.5423798557109807,
      -1.4321581996881607,
      -1.4805078657178539,
      -1.487488236164517,
      -1.4523343814923428,
      -1.5059762530297438,
      -1.5247481924842081,
      -1.4421188462768146,
      -1.457049911145844,
      -1.4698035479017633,
      -1.447554812290467,
      -1.5804920882257747,
      -1.4916666843280406,
      -1.4262024366843191,
      -1.5076242926394932,
      -1.4674900300923857,
      -1.4828284359889952,
      -1.4669880911288693,
      -1.4827927783857275,
      -1.5114280085647134,
      -1.4610538656431884,
      0.0,
      -1.553378855272603,
      -1.4651409552524493,
      -1.5192071584460574,
      -1.461504254016911,
      -1.498568617246486,
      -1.4988182614539545,
      -1.377383623311363
    ],
    [
      -1.4733791545183235,
      -1.4138795589404298,
      -1.4149061012593118,
      -1.4308388097044153,
      -1.4103239764630906,
      -1.4020358188854163,
      -1.430769163138023,
      -1.432109115927794,
      -1.433235754972037,
      -1.3869072207078137,
      -1.4139681115582257,
      -1.5297649935148439,
      -1.427321963296875,
      -1.4028981850434974,
      -1.375737458116017,
      -1.4215010757577526,
      -1.4045692545828883,
      -1.431142181532029,
      -1.439999386850974,
      -1.4375345666231212,
      -1.4458613591487004,
      -1.4420882413513951,
      0.0,
      -1.3792399204661006,
      -1.3938038773391994,
      -1.4423823060933334,
      -1.4466564469383871,
      -1.4327260321849093,
      -1.4666927298384886
    ],
    [
      -1.4158792935195736,
      -1.3698503856099031,
      -1.3500445515365522,
      -1.3416059029109941,
      -1.3759007800436607,
      -1.338209618895977,
      -1.4111121475388173,
      -1.363769255864864,
      -1.3901946975100268,
      -1.3523540668318073,
      -1.3789085200165256,
      -1.490635019823951,
      -1.4038130360498549,
      -1.3780530972741534,
      -1.2518667072429008,
      -1.3570465748510114,
      -1.3891190765967607,
      -1.3881222752762588,
      -1.3921091012713938,
      -1.4082932861379938,
      -1.4133149974661767,
      -1.4041065391973508,
      -1.3700848827879069,
      0.0,
      -1.4021889411885993,
      -1.4220800154822733,
      -1.3887964216144784,
      -1.4305408647354891,
      -1.412212984438391
    ],
    [
      -1.4520777502034963,
      -1.3711393304435213,
      -1.296122156516236,
      -1.440630132570865,
      -1.3462583766712541,
      -1.4374447422942207,
      -1.4683012161309603,
      -1.4230321762133655,
      -1.4146407306277113,
      -1.4532170042896442,
      -1.4067808813841824,
      -1.5878997927116414,
      -1.4076179419567283,
      -1.3158851274680934,
      -1.3681240854725043,
      -1.3799038157327776,
      -1.3178787730941564,
      -1.4185068628119328,
      -1.4679576029494117,
      -1.376777887260057,
      -1.3960681643451036,
      -1.4341078894978614,
      -1.3718210370389414,
      -1.4401870859133805,
      0.0,
      -1.3868966432818668,
      -1.3701496190486662,
      -1.3925162207371438,
      -1.429985128633185
    ],
    [
      -1.4803351941817355,
      -1.336318471165834,
      -1.3207661877825296,
      -1.4133539452939565,
      -1.375938773936997,
      -1.4070919173799181,
      -1.4870094529312998,
      -1.4070713910296078,
      -1.35348356202006,
      -1.4562700866395788,
      -1.3894959003148537,
      -1.5470423224134626,
      -1.3978077068770611,
      -1.3404649027136089,
      -1.354053583975306,
      -1.338573815196311,
      -1.4396285530903998,
      -1.367904491463239,
      -1.432816177880447,
      -1.355762393937209,
      -1.3961082989244533,
      -1.3721010941059186,
      -1.4867109179130573,
      -1.4111115427027143,
      -1.4218575064355785,
      0.0,
      -1.4314190191290108,
      -1.425771436859741,
      -1.393993184652278
    ],
    [
      -1.5477888314831756,
      -1.4567865905754935,
      -1.423628740748358,
      -1.510239878477526,
      -1.489467993066833,
      -1.3935954594214917,
      -1.5198117612117135,
      -1.4447045803344483,
      -1.4685565189519698,
      -1.5374951255878524,
      -1.4888325245913028,
      -1.6159436825049227,
      -1.4827547612204246,
      -1.460736662655718,
      -1.4639138953744235,
      -1.4899122356199426,
      -1.5026419169492604,
      -1.4386936790973686,
      -1.4891755097857917,
      -1.519198025286825,
      -1.4425347569675262,
      -1.5339133109694738,
      -1.5761835630224514,
      -1.4878527961200607,
      -1.5174727731190538,
      -1.5291144658078417,
      0.0,
      -1.4699153609415185,
      -1.49567043970014
    ],
    [
      -1.4650767112986398,
      -1.403006431806792,
      -1.3864351677456461,
      -1.463576155095275,
      -1.375737640946542,
      -1.4789762373206852,
      -1.4817190418236088,
      -1.3967480953771245,
      -1.3806632857722938,
      -1.4433427088909279,
      -1.3898629303245928,
      -1.5726300979821395,
      -1.3958256884060907,
      -1.412114507495202,
      -1.4501856965691338,
      -1.4172297656378277,
      -1.4098026948389035,
      -1.447051351370196,
      -1.4556678844905058,
      -1.4156611430413704,
      -1.370425177773216,
      -1.4518849536833578,
      -1.4055878230335577,
      -1.424256907828544,
      -1.3551173729362271,
      -1.4213989964773672,
      -1.4067871893032642,
      0.0,
      -1.4721274373086128
    ],
    [
      -1.4687872683764,
      -1.4208221582833518,
      -1.4169235465204912,
      -1.454057946180476,
      -1.430115374028279,
      -1.400069463772359,
      -1.451409549403006,
      -1.4604977296591153,
      -1.4349734542056032,
      -1.4669471482141014,
      -1.4751000013360969,
      -1.5035376747407314,
      -1.4376661070498284,
      -1.4026317234802272,
      -1.4295157373626914,
      -1.4579572814457094,
      -1.468159526886153,
      -1.447609996119734,
      -1.4631521649619375,
      -1.4429071659367272,
      -1.4138075702881765,
      -1.3583287264741346,
      -1.4991072919795019,
      -1.4603098774723131,
      -1.4535137410843704,
      -1.416592380059001,
      -1.3995968878979295,
      -1.448439230864861,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2536153814961004,
      0.244337537914725,
      0.22812935242599508,
      0.24742697335841113,
      0.2319100427672618,
      0.20316570638637588,
      0.25734086970188175,
      0.2522644813670629,
      0.22110993896810038,
      0.24143465031709344,
      0.12133651562773107,
      0.256170868493939,
      0.24394054519103126,
      0.22667708845678902,
      0.23666131281605596,
      0.19300017057718488,
      0.23508345712683587,
      0.20827931696756297,
      0.22940817997440144,
      0.24666874056375043,
      0.19555081418902365,
      0.17560878113469203,
      0.23341336027199633,
      0.20986587164720527,
      0.20311586987578467,
      0.25072055943171323,
      0.2252690953973615,
      0.2148201624247763
    ],
    [
      0.39112029513939617,
      0.0,
      0.465047600484938,
      0.4420651669279785,
      0.43974663406044967,
      0.383960694749224,
      0.3464838928521148,
      0.40025669128517505,
      0.4730018570469936,
      0.36030839468750586,
      0.4515254969985385,
      0.22607695092281377,
      0.4481415333041652,
      0.5027240807230593,
      0.4405900141511778,
      0.5063336426001988,
      0.3580027223398956,
      0.3532733983309764,
      0.3540577006076213,
      0.39171094296737907,
      0.3925086885641875,
      0.3440633656314147,
      0.3105400706175787,
      0.36879264050116234,
      0.37553683963140116,
      0.38795263929777746,
      0.416468154694388,
      0.4253161954227891,
      0.3580219530987274
    ],
    [
      0.41782180819971404,
      0.5715166326409062,
      0.0,
      0.5256098337643882,
      0.6059636202634553,
      0.561592572413325,
      0.4478466926154345,
      0.510272968301666,
      0.48885480178344975,
      0.4719190313564896,
      0.516538892417076,
      0.295591936909249,
      0.5477388998066617,
      0.6275763154005616,
      0.6096051949006145,
      0.5999433031276729,
      0.5431913497224707,
      0.5175714559323605,
      0.4814863633094064,
      0.5544726945021923,
      0.4704689109398257,
      0.4231891369952605,
      0.41799298561631915,
      0.46366543547609407,
      0.5590723166862093,
      0.5201506617833491,
      0.5448287957509683,
      0.4779092440826107,
      0.4527083991951242
    ],
    [
      0.396097718451051,
      0.49403193582977867,
      0.5046482353918933,
      0.0,
      0.4473988623110743,
      0.5063807259198387,
      0.4144651624820215,
      0.484257664017961,
      0.450669078870203,
      0.4447937052109061,
      0.5163031425987743,
      0.26193079342071734,
      0.4868236354921047,
      0.5238966515549095,
      0.5104900443594211,
      0.47616382220908116,
      0.3877424405046985,
      0.4850137098688938,
      0.47040176429809355,
      0.37956212766121245,
      0.45800738768994465,
      0.37356694627478104,
      0.3606250720326649,
      0.430550110220266,
      0.34966996460181776,
      0.3895980411064055,
      0.45322276528720784,
      0.42928464976405967,
      0.38725652094051544
    ],
    [
      0.3983767205901072,
      0.47690541374074114,
      0.5119269259433965,
      0.4319437488943365,
      0.0,
      0.43271755497666686,
      0.3048464476729531,
      0.4545506806621751,
      0.4337496436915165,
      0.4055245605323703,
      0.46080974050741075,
      0.25747321432518455,
      0.4858925404367691,
      0.49124801366320137,
      0.4566250345415961,
      0.48980683236936384,
      0.42512032966291624,
      0.42291218450188084,
      0.34898394848351244,
      0.4752869421583914,
      0.42210280275548917,
      0.40230086942407994,
      0.3458715533687058,
      0.39917024432479264,
      0.42609338281198883,
      0.4184298488303404,
      0.4148762117926905,
      0.384289292448593,
      0.37221227628563747
    ],
    [
      0.4330043655495819,
      0.5525523205518996,
      0.6131925099483493,
      0.5613196800016662,
      0.5190332788705299,
      0.0,
      0.4847999845939386,
      0.5022604765466301,
      0.4817180034663273,
      0.5218313803171384,
      0.4940858187769681,
      0.3105400057781138,
      0.5273932953734224,
      0.5355721841497225,
      0.6151846281830684,
      0.5008781647529013,
      0.4388770612122912,
      0.5344757125694006,
      0.5422433374097126,
      0.4871249202159518,
      0.4705577556022902,
      0.41999591129944736,
      0.3928259801516092,
      0.521113136672481,
      0.4222244706937601,
      0.43164252723966823,
      0.5663931508549245,
      0.4110177061483813,
      0.43888185043124106
    ],
    [
      0.4013279217685817,
      0.46022229600256925,
      0.48511190092148926,
      0.48083328281285276,
      0.4415417026387556,
      0.4828916102983023,
      0.0,
      0.4441185948147488,
      0.43484712389615754,
      0.44934544052785497,
      0.393967541790615,
      0.3406719542668435,
      0.4510477459468498,
      0.4022001110100617,
      0.4848983844190171,
      0.43019380904651316,
      0.4051510068072599,
      0.4255532158372872,
      0.45846916758102263,
      0.41919742385774295,
      0.447454939210264,
      0.4001633360684804,
      0.40679388695964813,
      0.46294997925382564,
      0.3745540620707284,
      0.404055545538885,
      0.4698538121897162,
      0.43520849811490847,
      0.44491574192113204
    ],
    [
      0.42373788691006653,
      0.4839653311705423,
      0.4737722598778462,
      0.518843390513126,
      0.5359497400328797,
      0.3815911116780435,
      0.37243222640438733,
      0.0,
      0.5360167269430376,
      0.43386885845702783,
      0.5025805730286246,
      0.3109179688580115,
      0.5381502761913846,
      0.5200289038030899,
      0.4826031029245481,
      0.45356423341105523,
      0.4296054937122211,
      0.48340878699932954,
      0.38606649666349435,
      0.450605612379507,
      0.5369812089887831,
      0.4486663888690754,
      0.34983394330100404,
      0.46133130736496075,
      0.3930258276838565,
      0.42815933284576824,
      0.49025009637229044,
      0.5251040466637489,
      0.40725950954413515
    ],
    [
      0.3731751918228754,
      0.5483646621982383,
      0.5043951123399384,
      0.5110704821918464,
      0.4543584706803563,
      0.4340617900838981,
      0.3771744443351681,
      0.4827979746948148,
      0.0,
      0.39436116539477584,
      0.5931418452410768,
      0.2606083307126228,
      0.5351010303972858,
      0.5466026668803698,
      0.5050242347101523,
      0.5536732590903193,
      0.4050575873935669,
      0.4611055083059734,
      0.3617626514391854,
      0.4105575344890815,
      0.42984040546526203,
      0.4175796959169227,
      0.3157211887192972,
      0.44821700496784334,
      0.3701013261057571,
      0.44289905077463065,
      0.4574509829951019,
      0.4662753732449527,
      0.42350475685443967
    ],
    [
      0.42202608998048974,
      0.511664454576815,
      0.5043038687696393,
      0.5149041832333208,
      0.5432504360009571,
      0.5367983542600674,
      0.4543641820965163,
      0.5235481415844891,
      0.46701667281769055,
      0.0,
      0.46377782873829676,
      0.3702726660044344,
      0.4826185845493498,
      0.5236005034436368,
      0.5251036166539544,
      0.5044780189248728,
      0.4576961327910847,
      0.4769372180893585,
      0.4951877193056624,
      0.4301424356821115,
      0.4868872527328503,
      0.4429269180150379,
      0.43730894892207095,
      0.5092249884792501,
      0.4463433720536991,
      0.4132607097974006,
      0.47259100976074,
      0.44912880222407736,
      0.44420514243094367
    ],
    [
      0.4261440092762414,
      0.46596630276087314,
      0.48105796876282203,
      0.5033264309594152,
      0.45357605206253826,
      0.44442462732908017,
      0.3638809541686261,
      0.4957956207306453,
      0.5280924408561689,
      0.4084907813362928,
      0.0,
      0.23440072364723896,
      0.5080075360502367,
      0.5591551758566691,
      0.4369624369578484,
      0.4916992034353671,
      0.44126393433762545,
      0.42922382923554037,
      0.3962151374069276,
      0.3708905078647209,
      0.45475469438290816,
      0.3782492625934206,
      0.343570863284848,
      0.42608615406906747,
      0.3948911163280304,
      0.37781951353716114,
      0.4498780955498556,
      0.44737060103486814,
      0.39270337904032093
    ],
    [
      0.3080594947074826,
      0.3393730502377661,
      0.36611429242681437,
      0.352194134917573,
      0.3286127183219383,
      0.3687376246804943,
      0.36580800567300553,
      0.349477631708609,
      0.3332184661968085,
      0.3848626641930373,
      0.31312559359811964,
      0.0,
      0.31487115863357595,
      0.3203275520297082,
      0.3944382096173169,
      0.32346118436403115,
      0.2956738217028225,
      0.3153357779764929,
      0.3600440196429706,
      0.3348548250834311,
      0.32010752027967326,
      0.3705012118488835,
      0.3555855480545844,
      0.4325980826177045,
      0.2970577534702237,
      0.3054828856640044,
      0.3252770391882158,
      0.33770064929957244,
      0.4336561576173621
    ],
    [
      0.4267791370243319,
      0.5405457209235824,
      0.5499097382926241,
      0.5290869421858284,
      0.5508412594259051,
      0.4930962990061487,
      0.40013037121588035,
      0.5084964146262294,
      0.5405226543241766,
      0.46316487275660356,
      0.542016177066567,
      0.2755071870857919,
      0.0,
      0.5261494399029856,
      0.4902871552263839,
      0.5077580179240093,
      0.4185259772731318,
      0.5114564798421553,
      0.42275183921369774,
      0.4622835894091173,
      0.5066765321216535,
      0.4236985570610099,
      0.3580916513920309,
      0.44885782579162536,
      0.38434270463037223,
      0.4644329902740354,
      0.43177487594825914,
      0.4548246256533317,
      0.44198989129629807
    ],
    [
      0.35191858181008406,
      0.4730362222454001,
      0.5113643735393198,
      0.49633397587568084,
      0.4657604367475059,
      0.40795316128721093,
      0.3369955362884436,
      0.4460140641708812,
      0.46455491860888887,
      0.4011188437168387,
      0.48853009526559954,
      0.2593724031051903,
      0.4716919147076646,
      0.0,
      0.45377515010099745,
      0.5447696255193335,
      0.45684608474686383,
      0.4466446159252666,
      0.3723000150273874,
      0.43863364914812686,
      0.48937354991708837,
      0.37978515205268804,
      0.3438522933724337,
      0.4004441482789425,
      0.4172426041677495,
      0.40021648222830386,
      0.4229944119003439,
      0.4118244895665579,
      0.3898989074254142
    ],
    [
      0.4625128292400136,
      0.5438782184000928,
      0.6233542467265145,
      0.5171238609166433,
      0.5117362669285843,
      0.5821623040408808,
      0.4463582593275113,
      0.4886599319318261,
      0.525454698031899,
      0.462779908441167,
      0.4759354136790448,
      0.32383544259915364,
      0.5160042980087596,
      0.5305061698383364,
      0.0,
      0.5884980618763744,
      0.5023674816225878,
      0.4865350954528189,
      0.4446806513053285,
      0.5073965861587835,
      0.4426261081447589,
      0.42755599545059564,
      0.4300430843318357,
      0.6340563434561843,
      0.45486152015539094,
      0.5152586665108534,
      0.49936127155938403,
      0.44634468487513335,
      0.45037879825389937
    ],
    [
      0.40848930891675517,
      0.5719956144990033,
      0.6206529114687178,
      0.4941672331449949,
      0.540447969965431,
      0.4574752687031709,
      0.38346009275493276,
      0.4654263586284182,
      0.4947768340459675,
      0.47432124964921285,
      0.5122882471523649,
      0.28687704597541863,
      0.4947363513338223,
      0.608444166905562,
      0.554501489129563,
      0.0,
      0.4983305028140763,
      0.4096275603105086,
      0.4377239656294565,
      0.47201856022959476,
      0.4507463964093066,
      0.38942317734810405,
      0.38778705954035053,
      0.4905575953176333,
      0.45757755993036775,
      0.49814924998801624,
      0.4680215935594223,
      0.45829259296723857,
      0.4246975886242892
    ],
    [
      0.3826662243578449,
      0.49507042355407505,
      0.5810720163065237,
      0.4427736696640998,
      0.5344047882378291,
      0.456580149756183,
      0.3984757897975775,
      0.5042984463938716,
      0.4623922274750121,
      0.480399076717571,
      0.5020576424553478,
      0.2718168410697075,
      0.5038846286568379,
      0.586040952228309,
      0.5203249680389663,
      0.5394595031969611,
      0.0,
      0.48938268997716516,
      0.4178922946066257,
      0.48962055283545336,
      0.49099926332519384,
      0.4593015079470204,
      0.4376261503504626,
      0.4906484121483141,
      0.5710447494693018,
      0.43186105010769404,
      0.5611838663660753,
      0.4748343432878248,
      0.41774830322495315
    ],
    [
      0.44312546983848766,
      0.499825615801867,
      0.5678368091766295,
      0.5297755122576766,
      0.557265708876292,
      0.5677867280819722,
      0.46110177996395785,
      0.638037082765254,
      0.5082224037977805,
      0.5211244303016132,
      0.49904979072616196,
      0.322061874128305,
      0.6390963360534834,
      0.5489562575430804,
      0.49801605601606624,
      0.46577241823365045,
      0.500624821539019,
      0.0,
      0.4614357445414794,
      0.47499143466061944,
      0.5194390227898327,
      0.5020832165804043,
      0.4024925047292067,
      0.4916467378442331,
      0.4369165668284094,
      0.46322976080959477,
      0.5580884593077824,
      0.4802827449161411,
      0.5041328296427381
    ],
    [
      0.36612471557395954,
      0.419603498079671,
      0.4849378910047917,
      0.47618184113628415,
      0.447926017115571,
      0.5518270649587547,
      0.4304186496956175,
      0.4008382937131205,
      0.34261442104241824,
      0.47386258933201386,
      0.40450991043042994,
      0.3146054406324226,
      0.3851059070614362,
      0.4390001254501028,
      0.4453525254578099,
      0.4434493687184511,
      0.401756809722416,
      0.44439799198058405,
      0.0,
      0.4129571805106731,
      0.3457155797896636,
      0.4013607865178068,
      0.3853857690429441,
      0.4211152935523155,
      0.40392554304315564,
      0.41819221315037547,
      0.5011882906349643,
      0.3385844198936161,
      0.4307148107451717
    ],
    [
      0.40100625632395537,
      0.47575903250440654,
      0.5382894892855696,
      0.4487684040776636,
      0.4939528356507774,
      0.4437920601869525,
      0.33292345211931385,
      0.4062044328944654,
      0.42634379296121194,
      0.40830119812854826,
      0.39502366332980876,
      0.23957578048910855,
      0.41387341575147674,
      0.4795128892807492,
      0.43344724697365855,
      0.4406154884845679,
      0.4346976715638142,
      0.4049946535110862,
      0.385227170876125,
      0.0,
      0.41939457435897465,
      0.3886977873470896,
      0.35474042223044644,
      0.42060841550694894,
      0.43172288532075864,
      0.48552973213436545,
      0.39851815303939464,
      0.3695343415553667,
      0.4042237616124811
    ],
    [
      0.397486309045459,
      0.5117755550141971,
      0.46388892112412683,
      0.45270990033416303,
      0.47628961450295737,
      0.43449307803403303,
      0.3807073080956387,
      0.5507730006910332,
      0.484176589316234,
      0.41891464523717503,
      0.46400344983831854,
      0.2560551488155671,
      0.5040843282276029,
      0.5729915939260153,
      0.39873412758331517,
      0.44828471673153714,
      0.4399211756501453,
      0.4962838798785505,
      0.3697787024620842,
      0.44326778784423326,
      0.0,
      0.458998259044719,
      0.35261039544261874,
      0.39311905166316685,
      0.4328892754767726,
      0.4090691278991754,
      0.4777256061992661,
      0.44667971654086047,
      0.4461927865032518
    ],
    [
      0.39847417647378736,
      0.5086958324966073,
      0.4603461664669142,
      0.45336579602025107,
      0.4885196506924252,
      0.43487777915502424,
      0.4161058397005599,
      0.4987351859079534,
      0.4838041210389241,
      0.4710504842830048,
      0.49329921989430114,
      0.36036194395899335,
      0.4491873478567274,
      0.5146515955004489,
      0.4332297395452749,
      0.4733640020923824,
      0.4580255961957729,
      0.47386594105589874,
      0.4580612537990405,
      0.42942602362005466,
      0.47980016654157964,
      0.0,
      0.38747517691216493,
      0.4757130769323188,
      0.4216468737387107,
      0.47934977816785707,
      0.442285414938282,
      0.44203577073081357,
      0.563470408873405
    ],
    [
      0.33469907312603286,
      0.3941986687039265,
      0.3931721263850445,
      0.377239417939941,
      0.39775425118126573,
      0.40604240875894004,
      0.3773090645063333,
      0.3759691117165622,
      0.3748424726723194,
      0.42117100693654264,
      0.39411011608613067,
      0.2783132341295125,
      0.38075626434748133,
      0.40518004260085894,
      0.4323407695283392,
      0.3865771518866037,
      0.40350897306146805,
      0.3769360461123272,
      0.3680788407933824,
      0.37054366102123515,
      0.36221686849565593,
      0.3659899862929612,
      0.0,
      0.4288383071782558,
      0.4142743503051569,
      0.36569592155102293,
      0.3614217807059692,
      0.375352195459447,
      0.3413854978058677
    ],
    [
      0.382511085216092,
      0.4285399931257625,
      0.4483458271991134,
      0.4567844758246715,
      0.422489598692005,
      0.46018075983968876,
      0.38727823119684834,
      0.4346211228708017,
      0.4081956812256389,
      0.4460363119038584,
      0.41948185871914,
      0.30775535891171457,
      0.3945773426858108,
      0.4203372814615123,
      0.5465236714927648,
      0.4413438038846542,
      0.40927130213890495,
      0.4102681034594069,
      0.4062812774642719,
      0.3900970925976719,
      0.3850753812694889,
      0.3942838395383148,
      0.4283054959477588,
      0.0,
      0.3962014375470664,
      0.3763103632533924,
      0.4095939571211873,
      0.3678495140001765,
      0.3861773942972746
    ],
    [
      0.39524670682802454,
      0.4761851265879995,
      0.5512023005152848,
      0.4066943244606558,
      0.5010660803602667,
      0.4098797147373001,
      0.3790232409005605,
      0.4242922808181553,
      0.4326837264038095,
      0.39410745274187664,
      0.44054357564733837,
      0.25942466431987943,
      0.4397065150747925,
      0.5314393295634274,
      0.4792003715590165,
      0.46742064129874317,
      0.5294456839373645,
      0.428817594219588,
      0.37936685408210913,
      0.4705465697714639,
      0.4512562926864172,
      0.41321656753365943,
      0.4755034199925794,
      0.4071373711181403,
      0.0,
      0.460427813749654,
      0.47717483798285465,
      0.45480823629437706,
      0.41733932839833576
    ],
    [
      0.3519870232224722,
      0.49600374623837373,
      0.5115560296216781,
      0.4189682721102512,
      0.4563834434672107,
      0.42523030002428963,
      0.34531276447290793,
      0.4252508263746,
      0.4788386553841477,
      0.37605213076462896,
      0.44282631708935405,
      0.28527989499074513,
      0.4345145105271466,
      0.4918573146905989,
      0.4782686334289017,
      0.49374840220789684,
      0.3926936643138079,
      0.46441772594096875,
      0.39950603952376085,
      0.47655982346699877,
      0.43621391847975444,
      0.4602211232982891,
      0.34561129949115044,
      0.4212106747014934,
      0.41046471096862924,
      0.0,
      0.40090319827519694,
      0.4065507805444668,
      0.4383290327519298
    ],
    [
      0.33169941299365546,
      0.4227016539013375,
      0.45585950372847295,
      0.3692483659993051,
      0.39002025140999796,
      0.4858927850553394,
      0.3596764832651176,
      0.4347836641423828,
      0.41093172552486124,
      0.34199311888897865,
      0.3906557198855283,
      0.26354456197190834,
      0.39673348325640645,
      0.41875158182111294,
      0.41557434910240754,
      0.3895760088568885,
      0.37684632752757063,
      0.4407945653794625,
      0.39031273469103933,
      0.36029021919000614,
      0.43695348750930485,
      0.34557493350735724,
      0.3033046814543796,
      0.39163544835677033,
      0.36201547135777723,
      0.35037377866898933,
      0.0,
      0.4095728835353125,
      0.38381780477669114
    ],
    [
      0.32764460133476003,
      0.38971488082660777,
      0.4062861448877537,
      0.3291451575381248,
      0.4169836716868578,
      0.31374507531271467,
      0.31100227080979104,
      0.3959732172562753,
      0.41205802686110604,
      0.349378603742472,
      0.402858382308807,
      0.22009121465126036,
      0.3968956242273092,
      0.3806068051381979,
      0.3425356160642661,
      0.3754915469955722,
      0.3829186177944963,
      0.34566996126320393,
      0.3370534281428941,
      0.3770601695920295,
      0.4222961348601839,
      0.34083635895004205,
      0.3871334895998422,
      0.36846440480485576,
      0.4376039396971727,
      0.3713223161560326,
      0.38593412333013566,
      0.0,
      0.320593875324787
    ],
    [
      0.34371652549517084,
      0.3916816355882191,
      0.39558024735107966,
      0.358445847691095,
      0.38238841984329186,
      0.41243433009921193,
      0.361094244468565,
      0.3520060642124556,
      0.3775303396659677,
      0.34555664565746946,
      0.33740379253547403,
      0.3089661191308395,
      0.37483768682174246,
      0.40987207039134366,
      0.38298805650887946,
      0.35454651242586155,
      0.344344266985418,
      0.36489379775183695,
      0.34935162890963345,
      0.3695966279348437,
      0.3986962235833944,
      0.4541750673974363,
      0.313396501892069,
      0.3521939163992578,
      0.35899005278720053,
      0.3959114138125699,
      0.41290690597364144,
      0.36406456300670986,
      0.0
    ]
  ],
  "row_avgs": [
    0.22451163017395864,
    0.3969152949156795,
    0.5080392947818877,
    0.43831616708465354,
    0.41964453426417175,
    0.49038355776290776,
    0.4334835726990041,
    0.4556543086282802,
    0.4458565616944912,
    0.4753417232828149,
    0.4322820483162628,
    0.34451989549115075,
    0.4669285331033488,
    0.4265444895266502,
    0.4942952213308698,
    0.47182191231934645,
    0.4783521618411714,
    0.5022292184911336,
    0.41898760528523354,
    0.41697425026782453,
    0.4422108589686506,
    0.45890087009248137,
    0.38085420033173517,
    0.4130256272458926,
    0.44118416505655983,
    0.4273128662989875,
    0.38675482163422725,
    0.36597491639848406,
    0.3702703394400242
  ],
  "col_avgs": [
    0.38917781925773137,
    0.47133532927490573,
    0.48991296270935747,
    0.4509661672792796,
    0.4661103126209114,
    0.44673271343550064,
    0.38009432420928924,
    0.4519663147558243,
    0.4466925923327065,
    0.41949101750646695,
    0.4482815891472254,
    0.27940232915851715,
    0.45634439497408014,
    0.48789893999816647,
    0.4640464969868612,
    0.4616975734457472,
    0.41894667884467485,
    0.43338860560125564,
    0.40225000229226754,
    0.4242537026723939,
    0.43477927883776724,
    0.40078414903547593,
    0.3680585077816177,
    0.4354771238310679,
    0.40750559104316675,
    0.4145677601697538,
    0.44717455073964185,
    0.418403930595475,
    0.41182988819075517
  ],
  "combined_avgs": [
    0.306844724715845,
    0.4341253120952926,
    0.4989761287456226,
    0.44464116718196656,
    0.44287742344254155,
    0.46855813559920423,
    0.4067889484541467,
    0.45381031169205227,
    0.44627457701359885,
    0.4474163703946409,
    0.4402818187317441,
    0.3119611123248339,
    0.4616364640387145,
    0.45722171476240836,
    0.4791708591588655,
    0.46675974288254685,
    0.4486494203429231,
    0.46780891204619457,
    0.41061880378875054,
    0.42061397647010923,
    0.4384950689032089,
    0.4298425095639786,
    0.37445635405667643,
    0.42425137553848025,
    0.4243448780498633,
    0.42094031323437064,
    0.41696468618693455,
    0.3921894234969795,
    0.3910501138153897
  ],
  "gppm": [
    614.9022284919532,
    609.7485803274686,
    600.3112995368967,
    617.8478399678854,
    607.722127436285,
    621.8946905072819,
    652.1131062032792,
    616.9498884227306,
    622.0617495675924,
    632.0881412244429,
    622.1874804772253,
    697.8025946209115,
    616.0415137559897,
    603.770237750171,
    613.0342652243108,
    612.4707506611642,
    632.9829331512462,
    625.0496671295397,
    638.9434215045013,
    631.0570776214847,
    624.0766314537477,
    643.9452065391575,
    653.7860900943772,
    627.5651364450108,
    638.2089159833898,
    636.0633177969888,
    619.5674786093831,
    632.0760150140139,
    638.7097456964875
  ],
  "gppm_normalized": [
    1.4413948450466914,
    1.3502806074256035,
    1.3243727897523834,
    1.3732171685654475,
    1.3353692854612373,
    1.371757336214364,
    1.4520350863939786,
    1.3602753384619477,
    1.3720817924603115,
    1.3917050964889346,
    1.3655592143911264,
    1.55443606863021,
    1.3572884357935464,
    1.3286123124139426,
    1.3523928762278203,
    1.3577675974120342,
    1.3907676725183058,
    1.3838971338883466,
    1.408668777066289,
    1.389724022521979,
    1.3743351039873024,
    1.4219019615314175,
    1.4414076432722145,
    1.3816800732547811,
    1.4012262388636505,
    1.4098696084451272,
    1.3592529321952018,
    1.3943745460310517,
    1.4030689120026596
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401,
    389,
    491,
    424,
    450,
    396,
    441,
    392,
    376,
    422,
    476,
    440,
    374,
    446,
    482,
    442,
    441,
    362,
    424,
    410,
    414,
    410,
    353,
    400,
    382,
    444,
    360,
    404,
    406,
    395,
    339,
    446,
    477,
    497,
    454,
    431,
    442,
    455,
    560,
    421,
    424,
    522,
    488,
    411,
    400,
    410,
    451,
    469,
    411,
    433,
    451,
    378,
    376,
    454,
    411,
    399,
    394,
    441,
    372,
    1166,
    412,
    405,
    456,
    421,
    410,
    395,
    446,
    422,
    371,
    413,
    468,
    458,
    414,
    419,
    422,
    375,
    410,
    404,
    390,
    423,
    455,
    433,
    401,
    393,
    441,
    387,
    438,
    457,
    475,
    438,
    496,
    454,
    447,
    436,
    483,
    459,
    425,
    390,
    418,
    596,
    408,
    457,
    477,
    438,
    347,
    421,
    393,
    423,
    422,
    403,
    347,
    442,
    425,
    397,
    418,
    466,
    401,
    668,
    451,
    488,
    442,
    477,
    399,
    415,
    391,
    477,
    467,
    410,
    552,
    421,
    402,
    423,
    411,
    434,
    392,
    381,
    432,
    411,
    388,
    434,
    406,
    404,
    403,
    408,
    482,
    364,
    515,
    394,
    445,
    456,
    590,
    399,
    441,
    475,
    430,
    427,
    416,
    502,
    397,
    443,
    381,
    435,
    391,
    431,
    382,
    457,
    431,
    379,
    342,
    407,
    378,
    371,
    372,
    478,
    412,
    676,
    441,
    444,
    451,
    462,
    493,
    380,
    407,
    445,
    468,
    464,
    604,
    461,
    488,
    449,
    498,
    445,
    422,
    409,
    421,
    392,
    428,
    402,
    455,
    441,
    429,
    408,
    494,
    372,
    307,
    405,
    426,
    444,
    444,
    447,
    430,
    442,
    415,
    412,
    410,
    547,
    406,
    431,
    431,
    401,
    422,
    414,
    409,
    427,
    431,
    407,
    386,
    438,
    364,
    436,
    366,
    452,
    371,
    499,
    431,
    463,
    475,
    444,
    451,
    385,
    460,
    462,
    388,
    408,
    370,
    480,
    462,
    364,
    435,
    420,
    473,
    381,
    480,
    406,
    350,
    421,
    441,
    416,
    367,
    438,
    468,
    386,
    745,
    461,
    466,
    403,
    398,
    400,
    445,
    399,
    439,
    369,
    381,
    515,
    427,
    400,
    396,
    475,
    427,
    413,
    414,
    424,
    362,
    378,
    383,
    411,
    394,
    387,
    398,
    416,
    403,
    607,
    476,
    436,
    437,
    451,
    416,
    399,
    400,
    438,
    393,
    428,
    359,
    447,
    437,
    387,
    441,
    382,
    414,
    417,
    424,
    403,
    340,
    448,
    410,
    407,
    398,
    437,
    453,
    389
  ],
  "response_lengths": [
    3016,
    2712,
    2514,
    2488,
    2706,
    2465,
    2314,
    2287,
    2559,
    2278,
    2434,
    2109,
    2590,
    2455,
    2220,
    2559,
    2229,
    2518,
    2474,
    2382,
    2216,
    1942,
    2565,
    2310,
    2313,
    2265,
    2501,
    2519,
    2248
  ]
}