{
  "example_idx": 46,
  "reference": "Under review as a conference paper at ICLR 2023\n\nSHARP CONVERGENCE ANALYSIS OF GRADIENT DESCENT FOR OVERPARAMETERIZED DEEP LINEAR NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThis paper presents sharp rates of convergence of the gradient descent (GD) method for overparameterized deep linear neural networks with different random initializations. This study touches upon one major open theoretical problem in machine learning–why deep neural networks trained with GD methods are efficient in many practical applications? While the solution of this problem is still beyond the reach of general nonlinear deep neural networks, extensive efforts have been invested in studying relevant questions for deep linear neural networks, and many interesting results have been reported to date. For example, recent results on loss landscape show that even though the loss function of deep linear neural networks is non-convex, every local minimizer is also a global minimizer. When the GD method is applied to train deep linear networks, it’s convergence behavior depends on the initialization. In this study, we obtained sharp rate of convergence of GD for deep linear networks and demonstrated that this rate does not depend on the types of random initialization. Furthermore, here, we show that the depth of the network does not affect the optimal rate of convergence, if the width of each hidden layer is appropriately large. Finally, we explain why the GD for an overparameterized deep linear network automatically avoids bad saddles.\n\n1\n\nINTRODUCTION\n\nDeep linear neural networks, as a class of toy models, are frequently used to understand loss surfaces and gradient-based optimization methods related to non-convex problems. Dauphin et al. (2014) and Choromanska et al. (2015a) explored the loss function of deep nonlinear networks based on random matrix theory (such as a spherical spin-glass model). This theory essentially converts the loss surface of deep nonlinear neural networks into that of deep linear neural networks under certain assumptions, some of which are unrealistic. Choromanska et al. (2015b) suggested an open problem to establish a connection between the loss function of neural networks and the Hamiltonian of spherical spin-glass models under milder assumptions. Later, Kawaguchi (2016) successfully discarded most of these assumptions by analyzing the loss surface of the deep linear neural networks.\n\nThe landscape for deep linear neural network (Kawaguchi, 2016; Kawaguchi & Lu, 2017; Laurent & Brecht, 2018) focuses on several properties of the critical points: (i) every local minimum is a global minimum; (ii) every critical point that is not a local minimum is a saddle point; and (iii) there exists a saddle such that all eigenvalues of its Hessian are zeros if the network is deeper than three layers. Thus, for deep linear neural networks, convergence to a global minimum is impeded by the existence of poor saddles.\n\nLee et al. (2016) showed that the gradient method almost surely never converges to a strict saddle point, although the time cost can depend exponentially on the dimension (Du et al., 2017). Gradient descent (GD) with perturbations (Ge et al., 2015; Jin et al., 2017) can find a local minimizer in polynomial time. Thus, the trajectory approach combined with random initialization or random algorithm circumvents the obstacle of existence of poor saddles. According to studies on continuous time dynamics of a gradient flow (Du et al., 2018; Arora et al., 2018b), the balance property of deep linear network is preserved if the initialization is balanced. Arora et al. (2018a;b), Du & Hu (2019), and Hu et al. (2020) successfully proved that GD with its corresponding initialization schemes con-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nverges to a global minimizer of deep linear neural networks with high probability. Furthermore, the rate of convergence is linear, and behaves like GD for a convex problem.\n\nHu et al. (2020) established that the convergence for Gaussian initialization can be very slow for deep linear neural networks with large depths, unless the width is almost linear. They also showed that orthogonal initialization in deep linear neural networks accelerate the convergence. Thus, the convergence behavior of the GD method, for training deep linear neural networks, crucially networks depends on the initialization.\n\nRecent studies have demonstrated the connection between deep learning and kernel methods (Daniely, 2017; Arora et al., 2019a;b; Chizat et al., 2019; Lee et al., 2019; Du et al., 2019; Cao & Gu, 2019; Woodworth et al., 2020), especially the neural tangent kernel (NTK), introduced by Jacot et al. (2018). For most common neural networks, the NTK becomes constant (Jacot et al., 2018; Liu et al., 2020) and remains so throughout the training in the limit of a large layer width. Throughout the training, the neural networks are well described by their first-order Taylor expansion around their parameters at the initialization (Lee et al., 2019).\n\nIn this paper, we first evaluate the convergence region, i.e. the set of initialization parameters that lead to the linear convergence of GD for deep linear neural networks (see Lemma 4.1 or Lemma D.1). Next, we demonstrate that if the minimum width among all the hidden layers is sufficiently large, then the random initialization will fall into the convergence region with high probability (see Theorem 3.1, Theorem B.1, Theorem B.2 and Theorem B.3). Furthermore, the worst-case convergence rate of GD for deep linear neural networks is almost the same as the original convex problem with a corresponding learning rate. We also demonstrate that the GD trajectories for deep linear neural networks are arbitrarily close to those for the convex problem. The precise statement is related to remark 3, Theorem 3.2, Corollary 1 and Lemma 4.4 (also see Lemma D.5).\n\nThe present study was inspired by a recent reported work Du & Hu (2019); Hu et al. (2020), in which the authors carefully constructed the upper and lower bounds of the eigenvalues of the Gram matrix along the GD and established a linear convergence. In this paper, we generalize their results to strongly convex loss functions with layer varying widths and obtain sharper results. We also show that our rate of convergence for GD in deep linear neural networks is sharp in the sense that it matches the worst-case convergence rate for the original convex problem. The trajectories between the GD for deep linear neural networks and the original convex problem (1) can be arbitrary close. Furthermore, we show that if the width of each hidden layer is appropriately large, then the optimal rate does not depend on the random initialization types and network depth. Lastly, we elucidate the mechanism underlying the observed automatic avoidance of bad saddles by the GD for overparameterized deep linear networks.\n\n2 PRELIMINARIES\n\n2.1 PROBLEM SETUP\n\nLet x ∈ Rnx and y ∈ Rny be an input vector and a target vector, respectively. Define {(xi, yi)}m i=1 as a training dataset of size m, and let X = [x1, x2, · · · , xm] ̸= 0 and Y = [y1, y2, · · · , ym]. Denote the weight parameters by W ∈ Rny×nx .\n\nConsider the well-studied convex optimization problem:\n\nminimize W\n\nL(W ) :=\n\n1 m\n\nm (cid:88)\n\ni=1\n\nl(W xi, yi).\n\nThe GD for convex problem (1) with a learning rate of η∗ is given by:\n\nW (t + 1) = W (t) − η∗∇L(W (t)), t = 0, 1, 2, · · · .\n\n(1)\n\n(2)\n\nFor any matrix A, let σmax(A) and σmin(A) be the largest and smallest singular values of A respectively. Here, we consider two types of matrix norms and one type of semi-norm for A, ∥A∥ := σmax(A), ∥A∥2 F := tr(AAT ), and ∥A∥X := ∥APX ∥F , where PX = X(X T X)†X T is the orthogonal projection matrix onto the column space of X, and (X T X)† is the Moore–Penrose inverse.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFor two real matrices A, B with the same sizes, we consider their Frobenius inner product as well as their semi-inner product, ⟨A, B⟩ = ⟨A, B⟩F := tr(AT B), ⟨A, B⟩X := ⟨APX , BPX ⟩. Here, we list some basic properties for the semi-norm and semi-inner product. Lemma 2.1. The loss function L(W ) defined in (1) satisfies the following properties: for any W, V ∈ Rny×nx ,\n\n1.L(W ) = L(W PX ), 2.∇L(W ) = ∇L(W PX )PX , 3.⟨∇L(W ), V ⟩F = ⟨∇L(W ), V ⟩X , 4. ∥∇L(W )∥F = ∥∇L(W )∥X , 5. ∥W ∥X ≡ ∥W ∥F if and only if X is full row rank.\n\nThe next lemma demonstrates the importance of the semi-norm ∥·∥X in our analysis. Lemma 2.2. Assume that l(·, y) is α(l)−strongly convex. Then, the following statements hold.\n\n1. If X is not a full row rank matrix, then L(W ) is neither strictly convex nor strongly convex\n\nwith respect to ∥·∥F .\n\n2. L(W ) is α(l)λmin(XX T )\n\nm\n\n−strongly convex with respect to ∥·∥X , where λmin(XX T ) is the\n\nsmallest non-zero eigenvalue of XX T .\n\nThe proofs of the two aforementioned lemmas are provided in appendix A. Hereafter, if inner product is not specified, then we will consider the semi-inner product.\n\nAssume that L is α−strongly convex (α > 0), and ∇L is β−Lipschitz (with respect to the seminorm ∥·∥X ); that means, for any W, V ∈ Rny×nx ,\n\nL(W ) ≥ L(V ) + ⟨∇L(V ), W − V ⟩X +\n\nα 2\n\n∥W − V ∥2\n\nX ,\n\n∥∇L(W ) − ∇L(V )∥X = ∥∇L(W ) − ∇L(V )∥F ≤ β ∥W − V ∥X . Without loss of generality, we assume that α and β are the best constants. Then, Lemma 2.2 implies that α ≥ α(l)λmin(XX T ) β(l)−Lipschitz and λmax(XX T ) is the largest eigenvalue of XX T .\n\n. Similarly, we can also show that β ≤ β(l)λmax(XX T )\n\n, where ∇l(·, y) is\n\nm\n\nm\n\nDefine the effective condition number of the convex function L by κ = κ(L) = β α < ∞. κ appears naturally in the rate of convergence of the GD. Let W∗ be a global minimizer of L(W ), that is L(W∗) = minW L(W ). Notice that W∗ might not be unique, but W∗PX is unique.\n\nThe well-known results for the rate of convergence of GD (2) state are:\n\nη∗ =\n\n1 β\n\n(cid:18)\n\n=⇒ E(t) ≤\n\n1 −\n\n(cid:19)t\n\n1 κ\n\nE(0), t = 1, 2, · · · , as well as,\n\nη∗ =\n\n2 α + β\n\n=⇒ E(t) ≤\n\n(cid:18)\n\n1 −\n\nβ 2\n\n4κ (1 + κ)2\n\n(cid:19)t\n\n∥W (0) − W∗∥2\n\nX , t = 1, 2, · · · ,\n\n(3)\n\n(4)\n\nwhere E(t) = L(W (t)) − L(W∗).\n\n2.2 DEEP LINEAR NETWORK SETUP\n\nLet N − 1 be the number of hidden layers. Assume rank(X) = r. Denote the weight parameters by Wk ∈ Rnk×nk−1 , k = 1, 2 · · · , N , with nN = ny, n0 = nx, where the nk is the width of the k-th layer. Set nmin = min{n1, n2, · · · , nN −1}, and nmax = max{n1, n2, · · · , nN −1}. For notational convenience, we denote nj:i = (cid:81) i≤k≤j nk and denote Wj:i = WjWj−1 · · · Wi for each 1 ≤ i ≤ j ≤ N . Define ni−1:i = 1 and Wi−1:i = I (of appropriate dimension) for completeness.\n\nConsidering the implicit regularization W = WN :1 for the convex problem (1). We obtain the following non-convex optimization problem of deep linear neural networks:\n\nminimize W1,··· ,WN\n\nL(WN :1) =\n\n1 m\n\nm (cid:88)\n\ni=1\n\nl(WN :1xi, yi).\n\n(5)\n\nExample 2.1. Specifically, if we set the loss to be l(W xi, yi) = ∥W xi − yi∥2 m ∥W X − Y ∥2\n\n−strongly convex, and ∇L is 2λmax(XX T )\n\nF is 2λmin(XX T )\n\nm\n\nm\n\n1\n\n−Lipschitz.\n\n2, then L(W ) =\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nExample 2.2. Deep linear neural networks with regularization λ ∥WN · · · W1PX ∥2 verted into a new optimization problem\n\nF can be con-\n\nminimize W1,··· ,WN\n\nL(WN :1) + λ ∥WN :1∥2\n\nX .\n\nLet Lλ(W ) = L(W ) + λ ∥W ∥2 Lipschitz.\n\nX . Then, Lλ(·) is α + 2λ-strongly convex, and ∇Lλ(·) is β + 2λ-\n\nMore generally, if we consider regularization with a form R(W ) = λ · g(W PX ), and g(·) is α′- strongly convex, and β′-Lipschitz, then for the optimization problem\n\nminimize W1,··· ,WN\n\nL(WN :1) + R(WN · · · W1) =: LR(WN · · · W1),\n\nwe know that LR(·) is α + λα′-strongly convex, and ∇LR(·) is β + λβ′-Lipschitz.\n\n2.3\n\nINITIALIZATION SCHEMES\n\nIn previous studies, the following form of deep linear networks was considered, instead of (5):\n\nminimize W1,··· ,WN\n\nL(aN WN :1) =\n\n1 m\n\nm (cid:88)\n\ni=1\n\nn1n2 · · · nN is a normalization constant.\n\nwhere aN = 1/\n\n√\n\nl(aN WN :1xi, yi),\n\n(6)\n\nBy applying GD on (6), where we update Wj simultaneously for j, we obtain\n\nWj(t + 1) = Wj(t) − η · aN (WN :j+1(t))T ∇L (aN WN :1(t)) (Wj−1:1(t))T , j = 1, · · · , N. (7)\n\nIn a recent study, the authors considered GD (7) and adopted a Gaussian initialization (Du & Hu, 2019) or scaled orthogonal initialization (Hu et al., 2020) for initializing Wj(0).\n\nIn this paper, we consider the following three kinds of random initializations, which generalize their idea.\n\nGaussian initialization: Let W1(0), · · · , WN (0) be the weight matrices at initialization. We assume that all the entries of Wj, 1 ≤ j ≤ N are independent Gaussian random variables with a zero mean and unit variance. Then, aN is a normalization constant in the sense that for any x ∈ Rn0 , we have\n\nE\n\n(cid:104) ∥aN WN :1(0)x∥2\n\n2\n\n(cid:105)\n\n= ∥x∥2 2 .\n\n(8)\n\nIn fact, all the initializations discussed in this paper satisfy (8). Remark 1. Let Vi(t) = 1√ ni initialization is equivalent to\n\nWi(t), for 1 ≤ i ≤ N . Then, GD (7) with a unit variance Gaussian\n\nVj(t + 1) = Vj(t) −\n\nη nj\n\n(VN :j+1(t))T ∇L (VN :1(t)) (Vj−1:1(t))T ,\n\n(9)\n\nwith a zero mean and variance 1 ni\n\nGaussian initialization for Vi, i = 1, · · · , N .\n\nGD (9) for loss (5) is equivalent to GD (7) for loss (6). Hereafter, we will only consider GD (7) for deep linear neural network (6).\n\nOrthogonal initialization: We consider the so-called one peak random orthogonal projection and embedding initialization, which generalize the idea of orthogonal initialization (Hu et al., 2020). Definition 2.1. An initialization WN :1(0) = WN (0)WN −1(0) · · · W1(0) is said to be a one peak random orthogonal projection and embedding initialization if there exists 1 ≤ p < N , such that n0 ≤ n1 ≤ n2 ≤ · · · ≤ np, np ≥ np+1 ≥ np+2 ≥ · · · nN −1 ≥ nN , and W1(0), W2(0), · · · , Wp(0), Wp+1(0), Wp+2(0), · · · , WnN (0) are independent and uniformly distributed over rectangular matrices, which satisfy\n\n(cid:26)W T\n\ni (0)Wi(0) = niIni−1, 1 ≤ i ≤ p,\n\nWj(0)W T\n\nj (0) = nj−1Inj , p + 1 ≤ j ≤ N.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nRemark 2. In this definition,\n\nWj(0), p + 1 ≤ j ≤ N are random orthogonal projections. Notably, A is a random orthogonal projection if and only if AT is a random embedding.\n\nWi(0), 1 ≤ i ≤ p are random embeddings and\n\nnj−1\n\n(cid:113) 1\n\n(cid:113) 1 ni\n\nArora et al. (2018a) studied the rate of convergence of GD to a global optimum for training a deep linear neural network for a balanced initialization. Here, we will consider a special case of balanced initialization, which is described as follows:\n\n√\n\nnUN [Iny , 0ny×(n−ny)]V T\n\nSpecial balanced initialization: Assume n1 = · · · = nN −1 = n. Consider the initialization WN (0) = 1 and Wi(0) = √\nnUiInV T , 2 ≤ i ≤ N − 1, where UN −1, UN , V1, Vi = Ui−1, 2 ≤ i ≤ N − 1 are orthogonal matrices (random or deterministic), and VN has a uniform distribution over the orthogonal matrices. Notice that only VN is required to be random.\n\nnU1[Inx , 0nx×(n−nx)]T V T\n\nN , W1(0) =\n\n√\n\ni\n\nA simple estimation of the loss at the initialization is given by the following lemma. Lemma 2.3. If the initialization satisfies (8) for all x, then with probability at least 1 − δ\n\n2 , we have\n\nL(aN WN :1(0)) − L(W∗) ≤ βBδ, where Bδ =\n\n(cid:18) 2 · rank(X) δ\n\n+ ∥W∗∥2\n\nX\n\n(cid:19)\n\n.\n\nNote that the bound Bδ can be improved by using a sharp concentration inequality.\n\n3 MAIN THEOREMS\n\nAssume the thinnest layer is either the input layer or the output layer; that is nmin ≥ max{n0, nN }, and the ratio between the width of any hidden layer is bounded from above, precisely we have nmax ≤ C0 < ∞. The quantities C2, C5 and C6 are defined in appendix D and are dependent on nmin hyperparameters nN , κ, δ, rank(X), C0, and N .\n\nFor notational convenience, we denote\n\nE(t) = L(W (t)) − L(W∗), and EDLN (t) = L(aN WN :1(t)) − L(W∗).\n\nOur assumptions and notation are now in place. We next state our main theorems in this section.\n\n3.1 LINEAR CONVERGENCE OF DEEP LINEAR NEURAL NETWORKS\n\nIn appendix B we present a sharp estimate of the linear convergence of GD for deep linear neural networks in Theorem B.1 for Gaussian initialization, Theorem B.2 for orthogonal initialization, and Theorem B.3 for a special balanced initialization. In particular, with a specific learning rate η = nN βN , Theorem B.1 and Theorem B.2 yield the following optimum rate of convergence: Theorem 3.1. Given any δ, ε ∈ (0, 1 following two overparameterization condition holds:\n\n2 ), there exists a constant C := C(ε), such that if one of the\n\n1. nmin ≥ C · C2 · N with the Gaussian initialization,\n\n2. nmin ≥ C · C5 with the one peak random orthogonal projection and embedding initialization\n\nand with probability at least 1 − δ, then we have\n\n(cid:18)\n\nEDLN (t) ≤\n\n1 −\n\n(cid:19)t\n\n1 − ε κ\n\nEDLN (0), t = 1, 2, · · · .\n\nRemark 3. Consider GD (2) with a learning rate of η∗ = 1 β and initialization W (0) = aN WN :1(0). The well-known result of rate of convergence (3) for GD (2) of convex problem (1) matches the rates obtained from Theorem B.1 and Theorem B.2. Remark 4. Du & Hu (2019), and Hu et al. (2020) showed that the number of iterations required (cid:1) for l2 loss. We only improved the rate of convergence and to reach a precision ε is O (cid:0)κ log 1 generalized their results to any strongly convex loss.\n\nε\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n3.2 RESULTS OF TRAJECTORIES\n\nTheorem 3.1 and remark 3 establish that the rate of convergence to a global optimum for GD to train a deep linear neural network is almost the same as the trajectories for the GD to train the corresponding convex problem with high probability, if the width is sufficiently large. Moreover, the GD for the fully-connected deep linear neural network (7) and that for GD (2) have almost the same trajectories.\n\nLet η1 = 2nN βN be an upper bound of the learning rate η. We can show that the trajectories of GD (7) for deep linear neural network (6) with a learning rate of η < η1 are close to those of GD (2) with a learning rate of η∗ = N η for the corresponding convex problem (1) with high probability, if the nN width of each hidden layer is sufficiently large. The precise statement is as follows: Theorem 3.2. Consider the GD for deep linear neural network (7) with a learning rate of η < η1 for aN WN :1(t), t = 0, 1, · · · , and GD (2) with a learning rate of η∗ = N η for W (t), t = 0, 1, · · · . nN Given τ, δ ∈ (0, 1), there exists a constant C := C(τ, η/η1) such that if one of the following three overparameterization conditions holds:\n\n1. nmin ≥ C · C2 · N with the Gaussian initialization,\n\n2. nmin ≥ C · C5 with the one peak random orthogonal projection and embedding initialization,\n\n3. nmin ≥ C · C6 with the special balanced initialization,\n\nthen with probability at least 1 − δ, we obtain\n\nX ≤ D(τ, q, t) ∥aN WN :1(0) − W∗∥2 ∥aN WN :1(t) − W (t)∥2 (cid:18)\n\n(cid:19)\n\nX ,\n\n|EDLN (t) − E(t)| ≤ β\n\nqt/2(cid:112)D(τ, q, t) +\n\n1 2\nEDLN (t) ≤ 3β(q + τ )t ∥aN WN :1(0) − W∗∥2 1−q , 2(q + τ )t(cid:111) (cid:110) τ\n\nX ,\n\nwhere D(τ, q, t) = min\n\n, with 0 < q < 1 defined in (15).\n\nD(τ, q, t)\n\n∥aN WN :1(0) − W∗∥2\n\nX , (10b)\n\n(10a)\n\n(10c)\n\nRemark 5. To the best of knowledge, this is the first paper that reveals that the trajectory of the overparameterized deep linear neural networks is close to the original convex problem with an appropriately rescaled learning rate. Corollary 1. According to Theorem 3.2, if we set η = 2nN high probability,\n\n(α+β)N , the following inequality holds with\n\n(cid:18)\n\nEDLN (t) ≤ 3β\n\n1 −\n\n(cid:19)t\n\n4κ\n\n(1 + κ)2 + τ\n\n∥aN WN :1(0) − W∗∥2\n\nX .\n\n(11)\n\nNotably, the rate of convergence in (11) is better than that in Theorem 3.1, because if κ > 1, then we can choose a sufficiently small τ such that the following inequality holds:\n\n1 −\n\n4κ\n\n(1 + κ)2 + τ < 1 −\n\n1 κ\n\n.\n\nTheorem 3.1, Theorem 3.2, Theorem B.1, and Theorem B.2 indicate that the implicit regularization induced by the GD for a convex problem recovers the convex problem itself in terms of optimization, at the cost of linear convergence only with high probability for random initialization. Remark 6. Recall the constants C2, C5, and C6 defined in appendix B. The term rank(X) is not optimal, since our concentration inequality depends only on the second moment. By using stronger concentration inequalities for our Lemma 2.3, similar to the proof of proposition 6.5 (Du & Hu, 2019) and Lemma 4.2 (Hu et al., 2020), the rank(X) ). C2 is δ\nproportional to κ2, which is slightly better than the constant in Du & Hu (2019), which is proportional to κ3. C5 is also slightly better than the constant reported by Hu et al. (2020), since we do not have the extra term ∥X∥2 ∥X∥2 . The improvement of the constant is mainly due to the introduction of the semi-norm ∥·∥X .\n\ncan be improved to 1 + log( rank(X)\n\nδ\n\nδ\n\nF\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4\n\nINSIGHTS FOR THEOREM 3.2\n\nInitialization and convergence region: Arora et al. (2018a) showed that if the initialization is approximately balanced, and the product matrix WN :1(0) is very close to a global minimizer, then the GD linearly converges to the global minimum for the deep linear network without any width requirement. However, the convergence region in (Arora et al., 2018a) is very small, because WN :1(0) needs to be very close to W∗. Later, Du & Hu (2019), and Hu et al. (2020) successfully proved that the GD with a Gaussian, or orthogonal initialization linearly converges to a global minimizer of the overparameterized deep linear neural network with high probability. They introduced a technique to analyze the trajectories of GD with large widths for any deterministic initialization.\n\nWe introduce the following lemma, which describes the linear convergence result for a deep linear network with a deterministic initialization.\n\nLemma 4.1. Under the setting of Lemma D.1, the GD for a deep linear network satisfies\n\nEDLN (t) ≤ (1 − ηγ)t EDLN (0), t = 1, 2, · · · .\n\nOur convergence region (see (31) in Lemma D.1 and Definition D.1) originates from the analysis of Du & Hu (2019), and Hu et al. (2020) and can be view as a neighborhood of the special balanced initialization, if n1 = n2 = · · · = nN −1. Both Gaussian and orthogonal initialization are approximately balanced.\n\nFor l2 loss, without loss of generality, we can assume X to be a full rank matrix and L(W∗) = 0 because of the decomposition method in claim B.1 from Du & Hu (2019). However, when considering a general strongly convex loss, we have to confront the low rank X directly in our analysis. Thus, ∥·∥X appears naturally and aids in achieving the sharp rate of convergence in our main theorems. In addition to the technique reported in Du & Hu (2019), and Hu et al. (2020), we also used classical convex optimization techniques (such as inequalities in Lemma C.1, and Polyak-Łojasiewicz inequality in (26)) as well as the classical concentration inequalities for beta distribution (such as the Chernoff type bound in Lemma F.3).\n\nWhy GD trajectories for overparameterized deep linear neural networks with approximate balanced initialization are close to those for convex problems? The underlying mechanism can be understood as follows: Even though recent results of (Ziyin et al., 2022) can describe the exact global minimizer for a deep linear network (with a regularization term such as l2), the evolution of each Wj is still difficult to track. Instead, we consider the discrete dynamics for product matrices (see (41) and (42)):\n\naN WN :1(t + 1) = aN WN :1(t) − η · P (t)[∇L(aN WN :1(t)PX )] + aN E(t).\n\nFor their own linear operator Pt, Du & Hu (2019) showed that λmax(Pt) ≤ O( N nN and λmin(Pt) ≥ Ω( N nN to proved that for our operator P (t)[·] ≈ N nN is negligible, which leads to the following result on discrete dynamics (see Lemma D.3).\n\n) · λmax(X T X) ) · λmin(X T X). To the best of our knowledge, the present paper is the first I (also see (44)), where I is the identity operator. E(t)\n\nLemma 4.2. Under the setting of Lemma D.3, we have\n\naN WN :1(t + 1) = aN WN :1(t) −\n\nN nN\n\nη∇L(aN WN :1(t)) + R(t),\n\nwith ∥R(t)∥X ≤ τ ∥aN WN :1(t) − W∗∥X .\n\nWithout the R(t) term, the discrete dynamics is exactly the GD for a convex function. To control the distance between the two trajectories, we introduce the following lemma (also see Lemma D.4).\n\nLemma 4.3. Assume τ ∈ [0, 1), and consider a discrete dynamical system V (t) such that,\n\nV (t + 1) = V (t) − η∗∇L(V (t)) + R(t), where ∥R(t)∥X ≤ τ ∥V (t) − W∗∥X .\n\nIf η∗ ≤ 2/β, then we have ∥V (t) − W∗∥2\n\nX ≤ (q + 7τ )t ∥V (0) − W∗∥2\n\nX , where q is defined in (15).\n\nWith the help of this lemma, we further obtain the following trajectories comparison lemma (also see Lemma D.5), which leads to the main conclusions in Theorem 3.2.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 4.4. Under the setting of Lemma D.5, we have\n\nX ≤ D(τ, q, t) ∥aN WN :1(0) − W∗∥2 ∥aN WN :1(t) − W (t)∥2 (cid:18)\n\n(cid:19)\n\nX ,\n\n|EDLN (t) − E(t)| ≤ β\n\nqt/2(cid:112)D(τ, q, t) +\n\n1 2\nEDLN (t) ≤ 3β(q + τ )t ∥aN WN :1(0) − W∗∥2 1−q , 2(q + τ )t(cid:111) (cid:110) τ\n\nX ,\n\nwhere D(τ, q, t) = min\n\n, with 0 < q < 1 defined in (15).\n\nD(τ, q, t)\n\n∥aN WN :1(0) − W∗∥2\n\nX , (12b)\n\n(12a)\n\n(12c)\n\nWhy do bad saddles not affect GD for overparameterized deep linear neural networks? A critical point x∗ of f is a bad saddle if λmin(∇2f (x∗)) = 0. Kawaguchi (2016) showed that deep linear networks have bad saddles, and thus, in general, a vanishing Hessian can hinder the optimization. Theorem 2.3 in Kawaguchi (2016) explains that for all bad saddles satisfy that WN −1:2 is a non-full rank matrix. Thus, to show that the trajectories of GD are away from bad saddle points, it is sufficient to demonstrate that inf t σmin(WN −1:2(t)) > 0. According to previous studies, there are two main ways to avoid bad saddles for GD to train deep linear networks.\n\nOn the one hand, following Arora et al. (2018b), it can be showed that if the approximate balanced initialization satisfies ∥WN :1(0) − W∗∥F ≤ σmin(W∗) − c, for some 0 < c < σmin(W∗), then σmin(WN :1(t)) ≥ c through the training as well as ∥W1(t)∥ ≤ (4 ∥W∗∥F )1/N , and ∥WN (t)∥ ≤ (4 ∥W∗∥F )1/N , then σmin(WN −1:2(t)) ≥ σmin(WN :1(t))\n\n∥W1(t)∥∥WN (t)∥ ≥\n\n(4∥W∗∥)2/N .\n\nc\n\nOn the other hand, if we assume that our rescaled and overparameterized weight initialization falls into convergence region (31), then we can show that (see B(t) in the proof of Lemma D.1)\n\nσmin(WN −1:2(t)) ≥ max\n\n(cid:26) σmin(WN :2(t)) σmax(WN (t))\n\n,\n\nσmin(WN −1:1(t)) σmax(W1(t))\n\n(cid:27)\n\n.\n\nThus, σmin( WN −1:2\n\n(nN −1:2)1/2 ) ≥ e−c1−c2 max{ n1\n\nnN −1\n\n, nN −1 n1\n\n} ≥ e−c1−c2 > 0.\n\nIn conclusion, we first made a conjecture that according to Arora et al. (2018b), for a nonoverparameterized deep linear network, there are no bad saddles satisfying ∥WN :1(0) − W∗∥F < σmin(W∗). Thus, ∥WN :1(0) − W∗∥F < σmin(W∗) is indeed a convergence region. However, this region in general is very small, and can even be empty if σmin(W∗) = 0. For an overparameterized deep linear network, the GD initialized in the convergence region will force the trajectories away from all the bad saddles.\n\nWhy does the width have to be large? We will discuss overparameterization phenomena in deep linear networks. For simplicity, we consider a special balanced initialization. First, we know that ∥Wi(t) − Wi(0)∥F = O( 1 βN and γ = O( αN nN\n\nN ) (see C(t) through the proof of Lemma D.1), provided η = h nN\n\n), where h ∈ (0, 2).\n\nAn overparameterized deep linear network around the special balanced initialization is full of global 2 , · · · , W ∗ minimizers, i.e., the trajectory limit (W ∗ N ) neighborhood of the special balanced initialization (W1(0), W2(0), · · · , WN (0)). Notably,\n\nN ) is in the O( 1\n\n1 , W ∗\n\nσmin(WN −1:2(0)) =\n\nN −1 (cid:89)\n\nj=2\n\nσmin(Wj(0)) = σmax(WN −1:2(0)) =\n\nN −1 (cid:89)\n\nj=2\n\nσmax(Wj(0)) = n(N −2)/2,\n\nas well as for any (W1, W2, · · · , WN ) in the O( 1 N ) neighborhood of any given initialization (W1(0), W2(0), · · · , WN (0)), we have (detailed argument can be found in the proof of B(t) in Lemma D.1):\n\n∥WN −1:2 − WN −1:2(0)∥F ≤\n\nN −2 (cid:88)\n\ns=1\n\n(cid:19)\n\n(cid:18)N − 1 s\n\nO(\n\n1 N\n\n)s(n(N −2−s)/2) ≤ n(N −2)/2O\n\n(cid:18) 1 √\nn\n\n(cid:19)\n\n.\n\nThus in terms of landscape, we have\n\nσmin(WN −1:2) σmin(WN −1:2(0))\n\n≥\n\nσmin(WN −1:2(0)) − ∥WN −1:2 − WN −1:2(0)∥F σmin(WN −1:2(0))\n\n≥ 1 − O(\n\n1 √\nn\n\n),\n\n(13)\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nwhich implies that no bad saddle is present in the O( 1 initialization, if the width n is sufficiently large.\n\nN ) neighborhood of the special balanced\n\nIn terms of training, we have (see the proof of Lemma D.1),\n\n∥Wi(t) − Wi(0)∥F = O(\n\n1 N\n\n), ∥Wi(0)∥F = n, 2 ≤ i ≤ N − 1,\n\n∥Wi(0)∥F\n\nas well as ∥Wi(t)−Wi(0)∥F N n ). Thus for an overparameterized deep linear network, the GD with an approximate balanced initialization only trains W1 and WN , and the other weight matrices remain almost constant. Here, we provide empirical evidence in appendix G to support the aforementioned argument.\n\n= O( 1\n\nOn the other hand, the sharp rate of convergence depends on the trajectory limit, and when the minimum width is sufficiently large, the trajectory limit and the initialization are not far away from each other. For deep linear network with small widths, the result (Ziyin et al., 2022) might shed light on convergence analysis, because the exact global minimizer can be described for a deep linear network with L2 regularization.\n\nNumerical Experiments: In appendix H, we will discuss some empirical evidence to support the main results shown in Section 3. Further, Figure 1 and 2 in appendix H show plots of the logarithm of loss as a function of number of iterations. When n is small, the trajectories of loss for deep linear neural networks do not decrease in some iterations. However, when n is large, the loss trajectories are close to those for the corresponding convex problem.\n\n5 OVERVIEW OF THE PROOFS OF MAIN THEOREMS AND LEMMAS\n\nIn this section, we provide an overview of the proofs for all the theorems obtained in the main results. Since Theorem 3.1 in the main results is a special cases of general theorems with nonoptimal learning rates (see Theorem B.1 and Theorem B.2), we only need to focus on the proofs of the general theorems (see Theorem B.1, Theorem B.2, Theorem B.3, and Theorem 3.2).\n\nWe begin with the convergence region of deep linear neural networks, which is basically the set of initializations that lead to the convergence of the GD for deep linear neural networks. The precise definition can be found in appendix D. Lemma 4.1 and Lemma 4.4 (also see Lemma D.1 and Lemma D.5) prove that this convergence region satisfies the following properties: if the initialization falls into the convergence region, then\n\n(i) the GD is guaranteed to converge to a global minimizer of the deep linear neural networks,\n\n(ii) the worst-case GD rate of convergence for the deep linear neural networks, which is a non-convex problem, is almost the same as the corresponding convex problem with a corresponding learning rate, and,\n\n(iii) the trajectories of the GD for the deep linear neural networks are arbitrarily close to those\n\nfor the corresponding convex problem.\n\nMore precisely, Lemma 4.1 (also see Lemma D.1) establishes the convergence region for a deterministic initialization, and it demonstrates the first two properties, (i) and (ii). Additionally, in appendix E and appendix F we also prove that the spectral properties of the products of random matrices partially reveal that the overparameterization realized by adding the width of each hidden layer guarantees that the random initialization falls into the convergence region with high probability. These results provide a foundation to establish the main linear convergence theorem for random initialization (see Theorem B.1, B.2, and B.3).\n\nBy contrast, Lemma 4.2 (also see Lemma D.3) shows that if the initialization falls into the convergence region, then the update rule for the product of weight matrices in the GD for deep linear neural networks is more or less given by (2). This result can be used to establish both Lemma 4.4 (also see Lemma D.5), and Theorem 3.2, which is precisely property (iii) of the convergence region for deterministic and non-deterministic initializations, respectively.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient\n\ndescent for deep linear neural networks. CoRR, abs/1810.02281, 2018a.\n\nSanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit In International Conference on Machine Learning, pp.\n\nacceleration by overparameterization. 244–253. PMLR, 2018b.\n\nSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pp. 322–332. PMLR, 2019a.\n\nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. Advances in Neural Information Processing Systems, 32, 2019b.\n\nYuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and\n\ndeep neural networks. Advances in neural information processing systems, 32, 2019.\n\nDjalil Chafaı, Djalil Chaf ̈a, Olivier Gu ́edon, Guillaume Lecue, and Alain Pajor. Singular values of\n\nrandom matrices. Lecture Notes, 2009.\n\nL ́ena ̈ıc Chizat, Edouard Oyallon, and Francis R. Bach. On lazy training in differentiable program-\n\nming. In NeurIPS, 2019.\n\nAnna Choromanska, Mikael Henaff, Michael Mathieu, G ́erard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial intelligence and statistics, pp. 192–204. PMLR, 2015a.\n\nAnna Choromanska, Yann LeCun, and G ́erard Ben Arous. Open problem: The landscape of the loss surfaces of multilayer networks. In Conference on Learning Theory, pp. 1756–1760. PMLR, 2015b.\n\nAmit Daniely. Sgd learns the conjugate kernel class of the network. Advances in Neural Information\n\nProcessing Systems, 30, 2017.\n\nYann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. Advances in neural information processing systems, 27, 2014.\n\nSimon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In\n\nInternational Conference on Machine Learning, pp. 1655–1664. PMLR, 2019.\n\nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pp. 1675– 1685. PMLR, 2019.\n\nSimon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. Advances in neural information processing systems, 30, 2017.\n\nSimon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. Advances in Neural Information Processing Systems, 31, 2018.\n\nMorris L. Eaton. Group invariance applications in statistics. Regional Conference Series in Proba-\n\nbility and Statistics, 1:i–133, 1989. ISSN 19355912.\n\nSam Elder. Bayesian adaptive data analysis guarantees from subgaussianity. CoRR, abs/1611.00065,\n\n2016.\n\nRong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient for tensor decomposition. In Conference on learning theory, pp. 797–842. PMLR, 2015.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Learning Representations, 2020.\n\nArthur Jacot, Franck Gabriel, and Cl ́ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\nGJO Jameson. Inequalities for gamma function ratios. The American Mathematical Monthly, 120\n\n(10):936–940, 2013.\n\nChi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape In International Conference on Machine Learning, pp. 1724–1732.\n\nsaddle points efficiently. PMLR, 2017.\n\nKenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information\n\nProcessing Systems, pp. 586–594, 2016.\n\nKenji Kawaguchi and Haihao Lu. Deep creates no bad local minima. arXiv: 1702.08580, 2017.\n\nThomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are\n\nglobal. International Conference on Machine Learning, pp. 2908–2913, 2018.\n\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. Advances in neural information processing systems, 32, 2019.\n\nJason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only\n\nconverges to minimizers. In Conference on learning theory, pp. 1246–1257. PMLR, 2016.\n\nChaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33: 15954–15964, 2020.\n\nOlivier Marchal and Julyan Arbel. On the sub-gaussianity of the beta and dirichlet distributions.\n\nElectronic Communications in Probability, 22(none):1 – 14, 2017.\n\nFeng Qi, Bai-Ni Guo, and C. Chen. The best bounds in gautschi-kershaw inequalities. Mathematical\n\nInequalities Applications, pp. 427–436, 2006.\n\nBlake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, In Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. Conference on Learning Theory, pp. 3635–3673. PMLR, 2020.\n\nLiu Ziyin, Botao Li, and Xiangming Meng. Exact solutions of a deep linear network. arXiv preprint\n\narXiv:2202.04777, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS OF BASIC PROPERTIES OF THE SEMI-NORM\n\nProof of Lemma 2.1. The first property is a direct consequence of the definition of the projection matrix PX .\n\nNotice that\n\n1 ε\n\n(L(W + ε∆W ) − L(W )) =\n\n1 ε\n\n(L(W PX + ε∆W PX ) − L(W PX )).\n\nConsidering ε → 0, the definition of the directional derivative implies\n\n⟨∇L(W ), ∆W ⟩F = ⟨∇L(W PX ), ∆W PX ⟩F = ⟨∇L(W PX )PX , ∆W ⟩F , ∀∆W ∈ Rny×nx,\n\nsince PX = P T\n\nX . This completes the proof of the second property.\n\nThe third property is derived from the condition: orthogonal projection matrix satisfies PX = P T P 2\n\nX = P 3\n\nX , since\n\nX =\n\n⟨∇L(W ), V ⟩F = ⟨∇L(W PX )PX , V ⟩F\n\n=⟨∇L(W PX )P 2\n\nX , V PX ⟩F = ⟨∇L(W PX )PX , V ⟩X = ⟨∇L(W ), V ⟩X .\n\nIf we set V = ∇L(W ), then the fourth property is implied by the third property.\n\nFor the last property, first recall that ∥W ∥X = ∥W PX ∥F and PX = X(X T X)†X T . X is of a full row rank matrix if and only if PX is an identity matrix, which completes the proof.\n\nProof of Lemma 2.2. Because X is not full row rank, we know that I − PX ̸= 0. There exists W such that W (I − PX ) ̸= 0. Applying the first property in Lemma 2.1, we obtain\n\nL(\n\n1 2\n\nW +\n\n1 2\n\nW PX ) = L((\n\n1 2\n\nW +\n\n1 2\n\nW PX )PX ) = L(W PX ) =\n\n1 2\n\nL(W ) +\n\n1 2\n\nL(W PX ),\n\nprovided W ̸= W PX .\n\nHence, L is not strictly convex, which implies that L is not strongly convex.\n\nTo prove the second property, it is sufficient to show that g(W ) = L(W ) − α(l)λmin(XX T ) is convex. Obviously,\n\nm\n\n∥W ∥2\n\nX\n\ng(W ) = L(W ) −\n\nα(l) m\n\nm (cid:88)\n\ni=1\n\n∥W xi − yi∥2\n\n2 +\n\nα(l) m\n\n(∥W X − Y ∥2\n\nF − λmin(X T X) ∥W ∥2\n\nX ).\n\n(14)\n\n(cid:80)m\n\ni=1 ∥W xi, yi∥2\n\nL(W ) − α(l) ∥W X − Y ∥2 F − λmin(W T W ) ∥W PX ∥2 is also convex. This completes the proof.\n\nm\n\nF is convex, because l(·, yi) is strongly convex. The Hessian of F has no negative eigenvalue; thus the second term in (14)\n\nB EXACT STATEMENTS OF THE MAIN THEOREMS\n\nDefinitions of some quantities:\n\n(cid:40)\n\nq =\n\n1 − αη∗(2 − η∗α), 1 − βη∗(2 − η∗β),\n\n12\n\n0 < η∗ ≤ 2 (α+β) < η∗ < 2 β ,\n\nα+β\n\n2\n\n(15)\n\nUnder review as a conference paper at ICLR 2023\n\nBδ =\n\n(cid:18) 2 · rank(X) δ\n\n+ ∥W∗∥2\n\nX\n\n(cid:19)\n\n,\n\nC1 = nN κ2Bδ\n\nC0 (η0 − η)2/η2\n\n0\n\n+ ln N,\n\nC2 = nN κ2BδC0 + ln N,\n\nC3 = nN κ2Bδ\n\nC4 = nN κ2Bδ\n\nC0 (η0 − η)2/η2 1\n(η0 − η)2/η2\n\n0\n\n0\n\n+ C0 ln(N ),\n\n,\n\nC5 = nN κ2BδC0 + C0 ln(N ), C6 = nN κ2Bδ,\n\nwhere N denotes the number of distinct elements in the set {n1, · · · , nN −1}, η1 = 2nN η0 = 2nN\n\ne2cN β with c > 0.\n\nN β , and\n\nTheorem B.1. Given any c > 0, and 0 < δ < 1/2, define η0 = 2nN rate of η < η0. There exists a constant C := C(c), such that if\n\ne2cN β , and consider the learning\n\nnmin ≥ C · C1 · N,\n\n(16)\n\nthen with probability at least 1 − δ over the random Gaussian initialization, we have\n\n(cid:32)\n\nEDLN (t) ≤\n\n1 − 4e−c\n\nη η0\n\n(1 − η η0\n\n)\n\n(cid:33)t\n\nκ\n\nEDLN (0).\n\nTheorem B.2. Given any c > 0, and 0 < δ < 1/2, define η0 = 2nN rate to be η < η0. There exists a constant C := C(c), such that if\n\ne2cβN , and consider the learning\n\nthen with probability at least 1−δ over the random one peak projection and embedding initialization, we have\n\nnmin ≥ C · C3,\n\n(17)\n\n(cid:32)\n\nEDLN (t) ≤\n\n1 − 4e−c\n\nη η0\n\n(1 − η η0\n\n)\n\n(cid:33)t\n\nκ\n\nEDLN (0).\n\nSpecially, if n1 = n2 = · · · = nN −1 = n ≥ min{nN , n0}, then requirement (17) can be replaced by\n\nRemark 7. Assume L(aN WN · · · W1) = 1 F , and n1 = · · · = nN −1 = n. Then, for Gaussian initialization, our Theorem B.1 leads to Theorem 4.1 in Du & Hu (2019). Similarly, for orthogonal initialization, our Theorem B.2 leads to Theorem 4.1 of Hu et al. (2020).\n\nn ≥ C · C4. 2 ∥aN WN · · · W1X − Y ∥2\n\n(18)\n\nNext, we present a version of the theorem related to balanced initialization.\n\nTheorem B.3. Assume n1 = · · · = nN −1 = n. Given any c > 0, and 0 < δ < 1/2, define η0 = 2nN e2cβN , and consider the learning rate as η < η0. There exists a constant C := C(c), such that as long as\n\nn ≥ C · C4.\n\n(19)\n\nthen with probability at least 1 − δ over the special balanced initialization, we have\n\n(cid:32)\n\nEDLN (t) ≤\n\n1 − 4e−c\n\nη η0\n\n(1 − η η0\n\n)\n\n(cid:33)t\n\nκ\n\nEDLN (0).\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nC INEQUALITIES IN CONVEX OPTIMIZATION\n\nConvex optimization has been studied for about a century. Recall the definitions and basic inequalities for α−strongly convex and β−Lipschitz functions.\n\nDefinition C.1. A continues differentiable function f is said to be β− Lipschitz if the gradient ∇f is β− Lipschitz, that is if for all x, y,\n\nf is said to be α−strongly convex if for all x, y, we have\n\n∥∇f (y) − ∇f (x)∥ ≤ β ∥y − x∥ ,\n\nf (y) ≥ f (x) + ⟨∇f (x), y − x⟩ +\n\nα 2\n\n∥y − x∥2 .\n\n(20)\n\n(21)\n\nProposition C.1. If f is α−strongly convex and ∇f is β−Lipschitz with respect to a (semi-)norm, then α ≤ β and\n\n⟨∇f (x), y − x⟩ +\n\nα 2\n\n∥y − x∥2 ≤ f (y) − f (x) ≤ ⟨∇f (x), y − x⟩ +\n\nβ 2\n\n∥y − x∥2 ,\n\n⟨∇f (x) − ∇f (y), x − y⟩ ≥\n\nαβ α + β ∥∇f (x) − ∇f (y)∥ ≥ α ∥x − y∥ ,\n\n∥x − y∥2 +\n\n1 α + β\n\n∥∇f (x) − ∇f (y)∥2 ,\n\nf (x) − f (y) ≤ ⟨∇f (x), x − y⟩ −\n\n1 2β\n\n∥∇f (x) − ∇f (y)∥2 .\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\nProof of Proposition C.1. We only prove the last inequality. Let z = y − 1\n\nβ (∇f (y) − ∇f (x)). Since f is convex β−Lipschitz, we have\n\nand\n\nThus,\n\nf (z) − f (x) ≥ ⟨∇f (x), z − x⟩\n\nf (z) − f (y) ≤ ⟨∇f (y), z − y⟩ +\n\nβ 2\n\n∥z − y∥2 .\n\nf (x) − f (y) =f (x) − f (z) + f (z) − f (y)\n\n≤⟨∇f (x), x − z⟩ + ⟨∇f (y), z − y⟩ +\n\nβ 2\n\n∥z − y∥2\n\n=⟨∇f (x), x − y⟩ −\n\n1 2β\n\n∥∇f (x) − ∇f (y)∥2.\n\nBefore we prove Lemma D.1, let us first include and prove the following result.\n\nLemma C.2. 1. Assume L is α−strongly convex, α > 0. Denote a global minimizer of L by W∗. Then, for any W ,\n\nL(W∗) − L(W ) ≥ −\n\n2. Assume ∇L is β−Lipschitz, then\n\n1 2α\n\n∥∇L(W )∥2\n\nX .\n\nL(W∗) − L(W ) ≤ −\n\n1 2β\n\n∥∇L(W )∥2\n\nX .\n\n(26)\n\n(27)\n\nProof of Lemma C.2. 1. First, we know that ∇L(W∗) = 0. L is α−strongly convex, which implies the inequality (22) holds. Thus\n\nL(V ) − L(W ) ≥ ⟨∇L(W ), V − W ⟩X +\n\nα 2\n\n∥V − W ∥2\n\nX =: g(V ).\n\nMinimizing both sides in terms of V gives (26).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nNow we focus on minimizing g(V ). Since g(V ) ∈ C 1 and the global minimizer exits, we have\n\n∇g(V ∗) = ∇L(W )PX + α(V ∗ − W )PX = 0,\n\nwhere V ∗ is a global minimizer for g(V ). Thus, 1\n2α\n\ng(V ∗) = −\n\n∥∇L(W )∥2\n\nX .\n\n2. Applying proposition C.1 to a β−Lipschitz function ∇L, we obtain\n\n(28)\n\nL(W∗) − L(W )\n\n≤⟨∇L(W∗), W∗ − W ⟩X −\n\n= −\n\n1 2β\n\n∥∇L(W )∥2\n\nX .\n\n1 2β\n\n∥∇L(W ) − ∇L(W∗)∥2\n\nX\n\nD CONVERGENCE REGION\n\nIn this section, we evaluate a class of the convergence region for deep linear neural networks with a deterministic initialization. Define A|R(X) = AX T (XX T )−X = APX , and view A|R(X) as a linear operator on R(X).\n\nRecall the optimization problem\n\nminimize W1,··· ,WN\n\nLN (W1, · · · WN ) :=\n\n1 m\n\nm (cid:88)\n\ni=1\n\nl(aN WN :1xi, yi) = L(aN WN :1),\n\n(29)\n\nand GD\n\n(cid:40)\n\nWj(t + 1) = Wj(t) − η ∂LN where ∂LN\n\n∂Wj\n\n∂Wj\n\n(W1(t), · · · , WN (t)), j = 1, · · · , N,\n\n(W1, · · · , WN ) = aN (WN :j+1)T ∇L(aN WN :1)(Wj−1:1)T ,\n\n(30)\n\nwhere the normalization factor aN =\n\n√\n\n1 n1n2···nN −1nN\n\n.\n\nThe following theorem generalizes the idea from a recent work (Du & Hu, 2019; Hu et al., 2020).\n\nFor notational convenience, we denote Wj:i(t) = Wj(t) · · · Wi(t), Lt = L(aN WN :1(t)), ∇Lt = ∇L(aN WN :1(t)) etc. Lemma D.1. Assume the initialization simultaneously satisfies the following conditions:\n\n\n\n \n\nσmax(WN :i+1(0)) ≤ ec1/2(nN −1:i)1/2, 1 ≤ i ≤ N − 1, σmin(WN :i+1(0)) ≥ e−c2/2(nN −1:i)1/2, 1 ≤ i ≤ N − 1, σmax(Wi−1:1(0)|R(X)) ≤ ec1/2(ni−1:1)1/2, 2 ≤ i ≤ N, σmin(Wi−1:1(0)|R(X)) ≥ e−c2/2(ni−1:1)1/2, 2 ≤ i ≤ N, ∥Wj:i(0)∥ ≤ M/2 · N θ((cid:81) L0 − L(W∗) ≤ βB0 =: B,\n\ni≤k≤j−1 nk · max{ni−1, nj})1/2, 1 < i ≤ j < N,\n\nwhere c1, c2, M are positive constant and θ ≥ 0. Notice that B0 is a proper upper bound for ∥aN WN :1(0)∥2\n\nX + ∥W∗∥2\n\nX .\n\nSet the learning rate as η = (1−ε)2nN\n\ne6c1+3c2 βN , where 0 < ε < 1. Define γ = 2e6c1 εαN\n\nnN\n\n.\n\nAssume that\n\nnmin ≥\n\nC(c1, c2)M 2κ2B0 ε2\n\nN 2θnN .\n\nThen, GD (30) satisfies\n\nLt − L(W∗) ≤ (1 − ηγ)t (L0 − L(W∗)), t = 1, 2, · · · .\n\n15\n\n(31)\n\n(32)\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition D.1. For given c1, c2, M, B0 > 0, and θ ≥ 0, we define the convergence region R(c1, c2, θ, M, B0) by the set of initialization that satisfies the inequality system (31).\n\nRemark 8. The condition (31) describes the convergence region for initialization and the condition (32) describes the overparameterization for deep linear neural networks. At this time, it is not clear how large this convergence region is. Later, we will show that the properly scaled random initialization with some extra mild overparameterization conditions will fall into this convergence region with high probability.\n\nProof of Lemme D.1. To prove Lemma D.1, it suffices to show that the following three properties hold A(t), B(t), and C(t) for all t = 0, 1, · · · .\n\n1. A(t):\n\n2. B(t):\n\n3. C(t):\n\n\n\n \n\nLt − L(W∗) ≤ (1 − ηγ)t (L0 − L(W∗)).\n\nσmax(WN :i+1(t)) ≤ ec1(nN −1:i)1/2, 1 ≤ i ≤ N − 1, σmin(WN :i+1(t)) ≥ e−c2(nN −1:i)1/2, 1 ≤ i ≤ N − 1, σmax(Wi−1:1(t)|R(X)) ≤ ec1(ni−1:1)1/2, 2 ≤ i ≤ N, σmin(Wi−1:1(t)|R(X)) ≥ e−c2(ni−1:1)1/2, 2 ≤ i ≤ N, ∥Wj:i(t)∥ ≤ M · N θ(\n\n(cid:81)\n\n1 nmin\n\ni−1≤k≤j nk)1/2, 1 < i ≤ j < N.\n\n∥Wi(t) − Wi(0)∥F ≤\n\n2e2c1 √\n\n√\n\n2βB\n\nnN γ\n\n=: R, 1 ≤ i ≤ N.\n\nUsing simultaneous induction, the proof of Lemma D.1 is divided into the following three claims.\n\nClaim 1. A(0), · · · , A(t), B(0), · · · , B(t) =⇒ C(t + 1).\n\nClaim 2. C(t) =⇒ B(t), if nmin ≥ C(c1,c2)M 2κ2B0 only depend on c1, c2.\n\nε2\n\nN 2θnN , where C(c1, c2) is a positive constant\n\nClaim 3. A(t), B(t) =⇒ A(t + 1), if nmin ≥ C(c1, c2)M 2B0N 2θnN , where C(c1, c2) is a positive constant only depend on c1, c2.\n\nProof of Claim 1. As a consequence of Lemma C.2 and Lemma 2.1, and A(s), s ≤ t, we have\n\n∥∇L(aN WN :1(s))∥2\n\nF = ∥∇Ls − ∇L(W∗PX )∥2\n\nX\n\n≤2β[Ls − L(W∗)] ≤2β (1 − ηγ)s B.\n\nFrom A(0), · · · , A(t), B(0), · · · , B(t), we have for any 0 ≤ s ≤ t,\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n∂L ∂Wi\n\n(s)\n\n(cid:13) (cid:13) (cid:13) (cid:13)F\n\n≤ aN ∥WN :i+1(s)∥ ∥∇L(aN WN :1(s))∥F\n\n(cid:13) (cid:13)Wi−1:1(s)|R(X)\n\n(cid:13) (cid:13)\n\n≤\n\n≤\n\ne2c1 √\nnN e2c1 √\nnN\n\n∥∇L(aN WN :1(s))∥F\n\n(cid:113)\n\n2β (1 − ηγ)s B.\n\n16\n\n(33)\n\n(34)\n\nUnder review as a conference paper at ICLR 2023\n\nThen,\n\n∥Wi(t + 1) − Wi(0)∥F ≤\n\n=\n\nt (cid:88)\n\ns=0\n\nt (cid:88)\n\ns=0\n\n∥Wi(s + 1) − Wi(s)∥F\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nη\n\n∂L ∂Wi\n\n(s)\n\n(cid:13) (cid:13) (cid:13) (cid:13)F\n\nt (cid:88)\n\n(1 − ηγ)s/2\n\ns=0\n\nt (cid:88)\n\n(1 − ηγ/2)s\n\n≤ η\n\ne2c1 √\nnN\n\n(cid:112)2βB\n\n≤ η\n\ne2c1 √\nnN √\n\n2e2c1 √\n\n(cid:112)2βB\n\ns=0\n\n= R.\n\n2βB\n\nnN γ\n\nThis proves C(t + 1).\n\n≤\n\nProof of Claim 2. Let δi = Wi(t) − Wi(0), 1 ≤ i ≤ N . Using C(t), we have ∥δi∥F ≤ R, 1 ≤ i ≤ N . Set ε1 = e−c1/2 min{ec1 − ec1/2, e−c2/2 − e−c2 , 1/2}. It is suffices to show that\n\n∥WN :i(t) − WN :i(0)∥ ≤ ec1/2ε1(nN −1nN −1 · · · ni−1)1/2, 1 < i ≤ N, (cid:13) (cid:13) (cid:13) ≤ ec1/2ε1(n1n2 · · · ni−1)1/2, 1 ≤ i < N, (cid:13)(Wi:1(t) − Wi:1(0))|R(X)\n\n(35)\n\n(36)\n\nand\n\n∥Wj:i(t) − Wj:i(0)∥ ≤ M/2 · N θ\n\n\n\n\n\n1 nmin\n\n(cid:89)\n\ni−1≤k≤j\n\n\n\n1/2\n\nnk\n\n\n\n, 1 < i ≤ j < N,\n\n(37)\n\nbecause σmin(A + B) ≥ σmin(A) − σmax(B) = σmin(A) − ∥B∥ and σmax(A + B) ≤ σmax(A) + σmax(B) = ∥A∥ + ∥B∥ (e.g. see Theorem 1.3 in Chafaı et al. (2009)). Case 1. We first prove (37). For 1 ≤ i < j ≤ N , we can write Wj:i(t) = (Wj(0) + δj) · · · (Wi(0) + δi). Expanding the above product, each term has the form:\n\nWj:(ks+1)(0) · δks · W(ks−1):(ks−1+1)(0) · δks−1 · · · δk1 · W(k1−1):i(0),\n\n(38)\n\nwhere i ≤ k1 < · · · < ks ≤ j are positions at which perturbation terms δkl are taken out. Notice that the convergence region assumption (31) implies that for any 1 < i ≤ j < N ,\n\n∥Wj:i(0)∥ ≤ M/2 · N θ\n\n\n\n\n\n(cid:89)\n\ni≤k≤j−1\n\n\n\n1/2\n\nnk · max{ni−1, nj}\n\n\n\n≤ M · N θ\n\n(cid:18) (cid:81)\n\ni−1≤k≤j nk nmin\n\n(cid:19)1/2\n\n.\n\n(39)\n\nWLOG, assume M ≥ 1. If i = j + 1, then\n\nAssuming i > 1, j < N , and applying inequality (39) as well as the following inequality\n\n∥Wj:i(0)∥ = ∥I∥ ≤ M · N θ(nj/nmin)1/2.\n\nj−i+1 (cid:88)\n\ns=1\n\n(cid:19)\n\n(cid:18)j − i + 1 s\n\nxs = (1 + x)j−i+1 − 1 ≤ (1 + x)N − 1, ∀x ≥ 0,\n\nwe obtain\n\n≤\n\n∥Wj:i(t) − Wj:i(0)∥ (cid:18)j − i + 1 s\n\nj−i+1 (cid:88)\n\n(cid:19)\n\ns=1\n\nRs(M · N θ)s+1n−s/2\n\nmin (ni−1 · · · nj/nmin)1/2\n\n≤M · N θ(ni−1 · · · nj/nmin)1/2[(1 + R · M · N θ/ ≤ε1M · N θ(ni−1 · · · nj/nmin)1/2.\n\n√\n\nnmin)N − 1]\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nThe last line holds due to the following reasons: there exists absolute constant A1, A2 > 0 such that\n\n(1 + x)N − 1 ≤ A2xN,\n\nif x ≥ 0, N ≥ 1, and xN ≤ A1. Since there exists positive constant C(c1, c2), which only depends on c1, c2, such that when\n\nnmin ≥\n\nC(c1, c2)M 2κ2B0 ε2\n\nN 2θnN\n\n(40)\n\nwe can have\n\nas well as\n\nR · M · N θ+1/\n\n√\n\nnmin ≤ A1,\n\n[(1 + R · M · N θ/\n\n√\n\nnmin)N − 1] ≤ A2 · M · R · N θ+1/\n\n√\n\nnmin ≤ ε1 = ε1(c1, c2).\n\nCase 2. The proof of (35) is similar. Set j = N , we can save the factor M · N θ from previous calculation, which means\n\n∥WN :i(t) − WN :i(0)∥\n\n≤ec1/2\n\nN −i+1 (cid:88)\n\ns=1\n\n(cid:18)N − i + 1 s\n\n(cid:19)\n\nRs(M · N θ)sn−s/2\n\nmin (ni−1 · · · nN −1)1/2\n\n√\n\nnmin)N − 1]\n\n≤ec1/2(ni−1 · · · nN −1)1/2[(1 + R · M · N θ/ ≤ec1/2ε1(ni−1 · · · nN −1)1/2, i ≥ 2,\n\nwhere the last line is implied by equation (40). Case 3. Similarly, we have\n\n(cid:13) (cid:13)Wj:1(t)|R(X) − Wj:1(0)|R(X)\n\n(cid:13) (cid:13)\n\n≤ec1/2\n\n(cid:19)\n\n(cid:18)j s\n\nj (cid:88)\n\ns=1\n\nRs(M · N θ)sn−s/2\n\nmin (n1 · · · nj)1/2\n\n≤ec1/2(n1 · · · nj)1/2[(1 + R · M · N θ/ ≤ec1/2ε1(n1 · · · nj)1/2, j ≤ N − 1\n\n√\n\nnmin)N − 1]\n\nThis proves B(t).\n\nProof of Claim 3. GD (7) implies\n\nWN :1(t + 1) (cid:18)\n\n=\n\nWN (t) − η\n\n∂LN ∂WN\n\n(cid:19) (cid:18)\n\n(t)\n\nWN −1(t) − η\n\n∂LN ∂WN −1\n\n(cid:19)\n\n(cid:18)\n\n(t)\n\n· · ·\n\nW1(t) − η\n\n(cid:19)\n\n(t)\n\n∂LN ∂W1\n\n=WN :1(t) − η · aN\n\nN (cid:88)\n\ni=1\n\nWN :i+1(t)W T\n\nN :i+1(t)∇L(aN WN :1(t))(Wi−1:1(t))T (Wi−1:1(t)) + E(t),\n\nwhere E(t) contains all the high-order terms (those with η2 or higher). We define a linear operator\n\nWN :i+1(t)W T\n\nN :i+1(t)(APX )(Wi−1:1(t)|R(X))T Wi−1:1(t)|R(X),\n\n(41)\n\nP (t)[A] = a2\n\nN\n\nN (cid:88)\n\ni=1\n\nfor any A ∈ RnN ×n0 .\n\nNow we have\n\naN WN :1(t + 1) = aN WN :1(t) − η · P (t)[∇L(aN WN :1(t)PX )] + aN E(t).\n\n(42)\n\nEasy to check that P (t)[·] is a sum of positive semidefinite linear operator.\n\nThe following proposition describes the eigenvalues of the linear operator P (t)[·].\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nProposition D.2. Let S1, S2 be symmetric matrices. Suppose S1 = U Λ1U T , S2 = V Λ2V T , where U = [u1, u2, · · · , um], and V = [v1, v2, · · · , vn] are orthogonal matrices, and Λ1 = diag(λ1, λ2, · · · , λm) and Λ2 = diag(μ1, μ2, · · · , μn) are diagonal matrices. Then, the linear operator L(A) := S1AS2 is orthogonally diagonalizable, and L(Aij) = λiμjAij, where λiμj represent all the eigenvalues corresponding to their eigenvectors Aij = uivT j .\n\nApplying this proposition and the assumption B(t), we obtain the upper bound and lower bound for the maximum and minimum eigenvalues of the positive definite operator P (t), respectively,\n\nλmax(P (t)) ≤ a2\n\nN\n\nN (cid:88)\n\ni=1\n\nσ2\n\nmax(Wi−1:1(t)|R(X)) · σ2\n\nmax(WN :i+1(t)) ≤\n\nN nN\n\ne2c1,\n\nand\n\nλmin(P (t)) ≥ a2\n\nN\n\nN (cid:88)\n\ni=1\n\nIn conclusion, we have\n\nσ2\n\nmin(Wi−1:1(t)|R(X)) · σ2\n\nmin(WN :i+1(t)) ≥\n\nN nN\n\ne−2c2.\n\n(43)\n\nλmax(P (t)) ≤\n\nN nN\n\ne2c1 , and λmin(P (t)) ≥\n\nN nN\n\ne−2c2.\n\nWith a learning rate of η = ηε = (1−ε)2nN\n\ne6c1+3c2 βN , 0 < ε < 1, we have\n\nLt+1 − Lt\n\n≤ ⟨∇Lt, −ηP (t)[∇Lt]⟩X + ⟨∇Lt, aN E(t)⟩X +\n\nβ 2\n\n∥ηP (t)[∇Lt] − aN E(t)∥2\n\nX\n\n= ⟨∇Lt, −ηP (t)[∇Lt]⟩ +\n\nη2 ∥P (t)[∇Lt]∥2\n\nX + F (t)\n\n(cid:18)\n\n≤ −\n\nηλmin(P (t)) −\n\nβ 2\n\n(cid:19)\n\nη2λ2\n\nmax(P (t))\n\n∥∇Lt∥2\n\nX + F (t)\n\nβ 2\n\n≤ −e−2c2\n\n(cid:18)\n\nN nN\n\nη\n\n1 − e4c1+2c2\n\n(cid:19)\n\nβ 2\n\nη\n\nN nN\n\n∥∇Lt∥2\n\nX + F (t),\n\nwhere\n\nF (t) = ⟨∇Lt, aN E(t)⟩X +\n\nβ 2\n\n∥ηP (t)[∇Lt] − aN E(t)∥2\n\nX −\n\nβ 2\n\nη2 ∥P (t)[∇Lt]∥2\n\nX .\n\nWe claim that F (t) is sufficiently small, such that\n\nη\n\n(cid:18)\n\n≤ −e−2c2\n\nLt+1 − Lt N\nnN N\nnN = −e−6(c1+c2) 2ε(1 − ε)\n\n≤ −e−3c2\n\n(cid:18)\n\nη\n\n1 − e4c1+2c2\n\n1 − e6c1+3c2\n\n∥∇Lt∥2\n\nX .\n\nβ\n\n(cid:19)\n\n(cid:19)\n\nβ 2\n\nβ 2\n\nη\n\nη\n\nN nN N\nnN\n\n∥∇Lt∥2\n\nX + F (t)\n\n∥∇Lt∥2\n\nX\n\n(44)\n\n(45)\n\n(46)\n\nAssuming this claim for the moment, we complete the proof. Combining (26) and (46), we have\n\n(cid:40)\n\nwhich implies\n\nLt+1 − Lt ≤ −e−6(c1+c2) 2ε(1−ε) L(W∗) − Lt ≥ − 1\n\n2α ∥∇Lt∥2\n\nX ,\n\nβ\n\n∥∇Lt∥2\n\nX ,\n\nLt+1 − L(W∗) ≤\n\n(cid:18)\n\n1 − e−6(c1+c2) 4ε(1 − ε)\n\nκ\n\n(cid:19)\n\n(Lt − L(W∗)),\n\n(47)\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nthat is\n\nLt − L(W∗) ≤\n\n(cid:18)\n\n1 − e−6(c1+c2) 4ε(1 − ε)\n\nκ\n\n(cid:19)t\n\nWhile estimating F (t), we observe that\n\n(L0 − L(W∗)) = (1 − ηγ)t (L0 − L(W∗)).\n\n(48)\n\n|F (t)|\n\n≤ ∥∇Lt∥X ∥aN E(t)∥X +\n\nβ 2\n\n= : I1 + I2.\n\n(2ηλmax(P (t)) ∥∇Lt∥X ∥aN E(t)∥X + ∥aN E(t)∥2\n\nX )\n\nFrom (34), we have (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13)\n\n∂L ∂Wi\n\n(t)\n\n≤\n\ne2c1 √\nnN\n\n∥∇L(aN WN :1(t))∥F =\n\ne2c1 √\nnN\n\n∥∇L(aN WN :1(t))∥X =: K.\n\nExpanding the product\n\nWN :1(t + 1) =\n\nWN (t) − η\n\n(cid:18)\n\n∂LN ∂WN\n\n(cid:19) (cid:18)\n\n(t)\n\nWN −1(t) − η\n\n∂LN ∂WN −1\n\n(cid:19)\n\n(cid:18)\n\n(t)\n\n· · ·\n\nW1(t) − η\n\n(cid:19)\n\n(t)\n\n,\n\n∂LN ∂W1\n\neach term has the form:\n\n∆ = WN :(ks+1)(t) · η\n\n∂L ∂Wks\n\n(t) · W(ks−1):(ks−1+1)(t) · η\n\n∂L ∂Wks−1\n\n(t) · · · η\n\n∂L Wk1\n\n(t) · W(k1−1):1(t),\n\nwhere 1 ≤ k1 < k2 < · · · < ks ≤ N .\n\nAs a direct consequence of inequality B(t) and inequality (39), we obtain\n\n∥∆∥X = ∥∆PX ∥F ≤\n\n1 √\n\nnN\n\naN\n\ne2c1 (ηK)s\n\n(cid:18) M · N θ nmin\n\n√\n\n(cid:19)s−1\n\n,\n\nRecall that E(t) contains all high-order terms (those with η2 or higher) in the expansion of the product. Thus, E(t) can be expressed as follows:\n\nN (cid:88)\n\n(cid:88)\n\ns=2\n\n1≤k1<k2<···<ks≤N\n\nWN :(ks+1)(t)·η\n\n∂L ∂Wks\n\n(t)·W(ks−1):(ks−1+1)(t)·η\n\n∂L ∂Wks−1\n\n(t) · · · η\n\n∂L Wk1\n\n(t)·W(k1−1):1(t).\n\nSet ξ = min{(e−2c2 − e−3c2 )/e4c1+1, 1\n\n4 (e6c1 − e4c1)/e6c1+1, 1\n\n2 (e6c1 − e4c1 )1/2/e4c1+1, 1}.\n\nRecall the inequality\n\n≤ (eN )s. Thus, we have\n\n(cid:19)\n\n(cid:18)N s\n\naN ∥E(t)∥X\n\n≤\n\n√\n\n≤\n\n√\n\n1 nN\n\n1 nN\n\n1 nN N\nnN\n\ne2c1\n\n(cid:19)\n\n(cid:18)N s\n\nN (cid:88)\n\ns=2\n\n(ηK)s\n\n(cid:19)s−1\n\n(cid:18) M · N θ nmin\n\n√\n\n(cid:18) M · N θ nmin\n\n√\n\n(cid:19)−1\n\ne2c1\n\nN (cid:88)\n\ns=2\n\n(eN )s(ηK)s\n\n(cid:19)s\n\n(cid:18) M · N θ nmin\n\n√\n\n≤\n\n√\n\ne2c1(ηeKN )\n\n√\n\nηeKM · N θ+1/ 1 − ηeKM · N θ+1/\n\nnmin √\n\nnmin\n\n≤ξ\n\nη · e4c1+1 ∥∇L(aN WN :1(t))∥X ( if ηeKM · N θ+1/\n\n=ξ · e4c1+1\n\n(cid:19)\n\n(cid:18)\n\nη\n\nN nN\n\n∥∇L(aN WN :1(t))∥X .\n\n(49)\n\nnmin < ξ/(1 + ξ))\n\n√\n\nUsing (33) and the upper bound of η, we know that there exists constant C(c1, c2), such that\n\nnmin ≥ C(c1, c2)M 2 · B0N 2θnN ,\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nand\n\nηeKM · N θ+1/\n\n√\n\nnmin ≤\n\n√\n\n2\n\nUsing (49), we have\n\n2M · e1+2c1\n\n√\n\nB0N θ√\n\nnN\n\n√\n\nnmin\n\n=\n\n1 C ′(c1, c2)\n\n≤\n\nξ 2\n\n≤\n\nξ 1 + ξ\n\n.\n\nI1 ≤ ξ · e4c1+1\n\n(cid:19)\n\n(cid:18)\n\nη\n\nN nN\n\n∥∇Lt∥2\n\nX ≤ (e−2c2 − e−3c2 )\n\n(cid:19)\n\n(cid:18)\n\nη\n\nN nN\n\n∥∇Lt∥2\n\nX ,\n\n(50)\n\nand\n\nI2\n\n≤\n\nβ 2\n\n(cid:18)\n\n2ξ · e6c1+1\n\n≤(e6c1 − e4c1)\n\nβ 2\n\nThus, (46) valid. This proves A(t).\n\n(cid:19)\n\n∥∇Lt∥2\n\nX + ξ2 · e8c1+2\n\n(cid:19)\n\n(cid:18)\n\nη2 N 2 n2 N\n\n(cid:19)\n\n∥∇Lt∥2\n\nX\n\n(cid:18)\n\nη2 N 2 n2 N\nη2 N 2 n2 N\n\n∥∇Lt∥2\n\nX .\n\nAs a direct consequence of the proof of Lemma D.1, we can obtain the following lemma.\n\nLemma D.3. Assume all assumptions in Lemma D.1 hold. For any τ > 0, we can choose new constants c1, c2 as well as C := C(c1, c2) such that the overparameterization assumption (32) in Lemma D.1 hold and\n\n∥R(t)∥X ≤ τ ∥aN WN :1(t) − W∗∥X ,\n\n(51)\n\nwhere\n\naN WN :1(t + 1) = aN WN :1(t) −\n\nN nN\n\nη∇L(aN WN :1(t)) + R(t).\n\nProof of Lemma D.3. Due to (33), (42), (44), (49), and lemma C.2, we have\n\n∥R(t)∥X =\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\naN E(t) + η\n\n(cid:18) N nN\n\n∇Lt − P (t)[∇Lt]\n\n≤ ∥aN E(t)∥X + η max\n\n(cid:26)\n\nλmax(P (t)) −\n\n(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)X N\nnN N\nnN\n\n(cid:27)\n\n− λmin(P (t))\n\n∥∇Lt∥X\n\n,\n\nN nN\n\n· ∥∇Lt∥X\n\n≤(C ′ · ξ + max{e2c1 − 1, 1 − e−2c2 }) · η\n\n2(cid:112)2β(Lt − L(W∗)) e6c1+3c2 · β\n\n≤\n\n· (C ′ · ξ + max{e2c1 − 1, 1 − e−2c2}).\n\nBecause Lt − L(W∗) is non-increasing in t, and C ′ is a constant that depends only on c1, c2, we can choose a sufficiently small positive c1, c2 and ξ, which depends on τ , such that\n\n∥R(t)∥X ≤ τ\n\n(cid:112)2β(Lt − L(W∗)) β\n\n≤ τ ∥aN WN :1(t) − W∗∥X .\n\nLemma D.4. Assume τ ∈ [0, 1). Consider a discrete dynamical system V (t) such that,\n\nwhere ∥R(t)∥X ≤ τ ∥V (t) − W∗∥X . If η∗ ≤ 2/β, we have\n\nV (t + 1) = V (t) − η∗∇L(V (t)) + R(t),\n\n∥V (t) − W∗∥2\n\nX ≤ (q + 7τ )t ∥V (0) − W∗∥2\n\nX ,\n\nwhere q is defined in (15).\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nProof of Lemma D.4. Set ∆(t) = V (t) − W∗ and τ ′ = τ ∥∆(t)∥X . Notice that\n\n∆(t + 1) = ∆(t) − η∗(∇L(V (t)) − ∇L(W∗)) + R(t),\n\nand\n\n≤η2\n\nX\n\n∥∆(t + 1)∥2 ∗ ∥∇L(V (t)) − ∇L(W∗)∥2 + ∥∆(t)∥2\n\nX − 2η∗⟨∆(t), ∇L(V (t)) − ∇L(W∗)⟩X X + (2 ∥∆(t)∥X + 2η∗ ∥∇L(V (t)) − ∇L(W∗)∥X + τ ′)τ ′.\n\nBy inequality (23),\n\n∥∆(t + 1)∥2\n\nX\n\n≤ ∥∆(t)∥2\n\nX − 2η∗⟨∆(t), ∇L(V (t)) − ∇L(W∗)⟩X X + 7τ ∥∆(t)∥2\n\n∗ ∥∇L(V (t)) − ∇L(W∗)∥2\n\nX\n\n+ η2\n\n=(1 + 7τ ) ∥∆(t)∥2\n\nX − 2η∗⟨∆(t), ∇L(V (t)) − ∇L(W∗)⟩X\n\n+ η2\n\n∗ ∥∇L(V (t)) − ∇L(W∗)∥2\n\nX\n\n≤(1 + 7τ ) ∥∆(t)∥2\n\nαβ α + β\n\n∥∆(t)∥2\n\nX\n\nX − 2η∗ (cid:19)\n\n∥∇L(V (t)) − ∇L(W∗)∥2\n\nX .\n\n2η∗ α + β\n\n(cid:18)\n\n+\n\nη2\n\n∗ −\n\n2\n\nCase 1: In this case, we have\n\nα+β < η∗ < 2 β .\n\n∥∆(t + 1)∥2\n\nX\n\n≤(1 + 7τ ) ∥∆(t)∥2\n\nX − 2η∗\n\n≤(1 + 7τ ) ∥∆(t)∥2\n\nX − 2η∗\n\nαβ α + β\n\nαβ α + β\n\n∥∆(t)∥2\n\nX +\n\n∥∆(t)∥2\n\nX +\n\n(cid:18)\n\nη2\n\n∗ −\n\n(cid:18)\n\nη2\n\n∗ −\n\n(cid:19)\n\n(cid:19)\n\n2η∗ α + β\n\n2η∗ α + β\n\n∥∇L(V (t)) − ∇L(W∗)∥2\n\nX\n\nβ2 ∥∆(t)∥2\n\nX\n\n≤ (1 + 7τ − βη∗(2 − η∗β)) ∥∆(t)∥2 =(q + 7τ ) ∥∆(t)∥2\n\nX\n\nX .\n\nCase 2: 0 < η∗ ≤ 2 Similarly, we have\n\nα+β .\n\n∥∆(t + 1)∥2\n\nX ≤ (1 + 7τ − αη∗(2 − η∗α)) ∥∆(t)∥2\n\nX = (q + 7τ ) ∥∆(t)∥2\n\nX .\n\nIn both cases, we have ∥∆(t + 1)∥2\n\nX ≤ (q + 7τ ) ∥∆(t)∥2\n\nX .\n\nThus, ∥∆(t)∥2\n\nX ≤ (q + 7τ )t ∥∆(0)∥2\n\nX .\n\nNext, we will show that the trajectories of the GD (30) for deep linear neural networks (29) are close to those of GD (2) for the corresponding convex problem (1).\n\nLemma D.5. Consider the GD for the deep linear neural networks (30) with learning rate η < η1 for aN WN :1(t), t = 0, 1, · · · , and the GD (2) with learning rate η∗ = N η for W (t), t = 0, 1, · · · . nN\n\nAssume C(c1, c2) exists in Lemma D.1 for any c1, c2 > 0. For any τ ∈ (0, 1), η < η1 (η1 defined in B), we can choose c1, c2 > 0 and the constant C = C(c1, c2) = C ′(τ, η/η1), such that inequality (51) holds, given initialization condition (31), and overparameterization condition\n\nnmin ≥ CM 2κ2B0N 2θnN .\n\n(52)\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFurthermore, we have\n\nX ≤ D(τ, q, t) ∥aN WN :1(0) − W∗∥2 ∥aN WN :1(t) − W (t)∥2 (cid:18)\n\n(cid:19)\n\nX ,\n\n|EDLN (t) − E(t)| ≤ β\n\nqt/2(cid:112)D(τ, q, t) +\n\n1 2\nEDLN (t) ≤ 3β(q + τ )t ∥aN WN :1(0) − W∗∥2 1−q , 2(q + τ )t(cid:111) (cid:110) τ\n\nX ,\n\nwhere D(τ, q, t) = min\n\n, with q defined in (15).\n\nD(τ, q, t)\n\n∥aN WN :1(0) − W∗∥2\n\nX , (53b)\n\n(53a)\n\n(53c)\n\nProof of Lemma D.5. Using Lemma D.3, we obtain that for any τ ∈ (0, 1) and η < η1, we can find sufficiently small positive constants c1, c2, which only depend on τ, η/η1, and constant C = C(c1, c2) = C ′′(τ, η/η1) mentioned in Lemma D.3, such that\n\nη =\n\n(1 − ε)2nN e6c1+3c2 βN\n\n,\n\nwhere 0 < ε < 1, as well as\n\nV (t + 1) = V (t) − η∗∇L(V (t)) + R(t),\n\nwhere V (t) = aN WN :1(t), η∗ = N nN\n\nη, and ∥R(t)∥X ≤ τ ′ = τ ∥V (t) − W∗∥X .\n\nNotice that θ0 := η/η1 = 1−ε\n\ne6c1+3c2 and η/η0 = 1 − ε, where η0 =\n\n2nN\n\ne6c1+3c2 βN .\n\nFor the right hand side of inequality (32), we have\n\nC(c1, c2)M 2κ2B0 ε2\n\nN 2θnN =\n\nC ′′(τ, η/η1)M 2κ2B0 ε2\n\nN 2θnN .\n\nTo show that inequality (32) is equivalent to inequality (52), it suffices to show that ε only depend on τ, η/η1. Notice that\n\nε = 1 − η/η0 = 1 − θ0e6c1+3c2 ,\n\nand c1, c2 only depend on τ and η/η1, which implies ε only depend on τ, η/η1.\n\nNow, we will prove the three inequalities in (53).\n\nRecall GD (2) for W (t). Define ∆(t) = V (t) − W (t) = aN WN :1(t) − W (t). Notice that\n\n∆(t + 1) = ∆(t) − η∗(∇L(V (t)) − ∇L(W (t))) + R(t),\n\nand\n\n≤η2\n\nX\n\n∥∆(t + 1)∥2 ∗ ∥∇L(V (t)) − ∇L(W (t))∥2 + ∥∆(t)∥2\n\nX − 2η∗⟨∆(t), ∇L(V (t)) − ∇L(W (t))⟩X X + (2 ∥∆(t)∥X + 2η∗ ∥∇L(V (t)) − ∇L(W (t))∥X + τ ′)τ ′.\n\nLet lt = 2 ∥∆(t)∥X + 2η∗ ∥∇L(V (t)) − ∇L(W (t))∥X + τ ′. Now, we aim to find an upper bound for lt. Applying lemma C.2 with the assumption 0 < η∗ = N nN\n\nη < 2\n\nβ , we know that\n\nlt ≤ (6 ∥∆(t)∥X + τ ′) ≤ 7(∥W (t) − W∗∥X + ∥V (t) − W∗∥X ).\n\n(54)\n\nThus\n\nltτ ′ ≤ 7τ ∥V (t) − W∗∥X (∥V (t) − W∗∥X + ∥W (t) − W∗∥X ) =: Utτ.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nBy inequality (23),\n\n∥∆(t + 1)∥2\n\nX\n\n≤ ∥∆(t)∥2\n\nX − 2η∗⟨∆(t), ∇L(V (t)) − ∇L(W (t))⟩X\n\n∗ ∥∇L(V (t)) − ∇L(W (t))∥2\n\nX + Utτ\n\n+ η2 = ∥∆(t)∥2\n\nX − 2η∗⟨V (t) − W (t), ∇L(V (t)) − ∇L(W (t))⟩X\n\n+ η2\n\n∗ ∥∇L(V (t)) − ∇L(W (t))∥2 αβ α + β (cid:19)\n\n∥∆(t)∥2\n\nX − 2η∗\n\nX\n\n(cid:18)\n\n≤ ∥∆(t)∥2\n\nX + Utτ\n\nη2\n\n∗ −\n\n2η∗ α + β\n\n∥∇L(V (t)) − ∇L(W (t))∥2\n\nX + Utτ.\n\n+\n\n2\n\nCase 1: In this case, we have\n\nα+β < η∗ < 2 β .\n\n∥∆(t + 1)∥2\n\nX\n\n≤ ∥∆(t)∥2\n\nX − 2η∗\n\n≤ ∥∆(t)∥2\n\nX − 2η∗\n\nαβ α + β\n\nαβ α + β\n\n∥∆(t)∥2\n\nX +\n\n∥∆(t)∥2\n\nX +\n\n(cid:18)\n\nη2\n\n∗ −\n\n(cid:18)\n\nη2\n\n∗ −\n\n(cid:19)\n\n(cid:19)\n\n2η∗ α + β\n\n2η∗ α + β\n\n∥∇L(V (t)) − ∇L(W (t))∥2\n\nX + Utτ\n\nβ2 ∥∆(t)∥2\n\nX + Utτ\n\n≤ (1 − βη∗(2 − η∗β)) ∥∆(t)∥2 = : q ∥∆(t)∥2\n\nX + Utτ.\n\nX + Utτ\n\nCase 2: 0 < η∗ ≤ 2 Similarly, we have\n\nα+β .\n\n∥∆(t + 1)∥2 In both cases, we have 0 < q < 1.\n\nX ≤ (1 − αη∗(2 − η∗α)) ∥∆(t)∥2\n\nX + Utτ =: q ∥∆(t)∥2\n\nX + Utτ.\n\n(55)\n\nFirst of all, since Ut ≤ U0 and ∥∆(0)∥X = 0, we obtain that\n\n∥∆(t)∥2\n\nX ≤\n\nU0τ 1 − q\n\n+ qt\n\n(cid:18)\n\n∥∆(0)∥2\n\nX −\n\n(cid:19)\n\nU0τ 1 − q\n\n≤\n\nU0τ 1 − q\n\n≤\n\nApplying Lemma D.4 for V (t) and W (t), we obtain ∥V (t) − W∗∥2 and ∥W (t) − W∗∥2\n\nX ≤ qt ∥W (0) − W∗∥2\n\nX , respectively. Thus,\n\n∥V (0) − W∗∥2\n\n14τ 1 − q X ≤ (1 + ε)tqt ∥V (0) − W∗∥2\n\nX .\n\nX\n\n|L(W (t)) − L(aN WN :1(t))|\n\n≤|⟨∇L(W (t)), ∆(t)⟩X | +\n\nβ 2\n\n∥∆(t)∥2\n\nX\n\n≤β ∥W (t) − W∗∥X · ∥∆(t)∥X +\n\n(cid:18)\n\nqt/2\n\n≤β\n\n(cid:114) 14τ 1 − q\n\n+\n\n7τ 1 − q\n\n(cid:19)\n\nGenerally speaking, (55) implies\n\nβ 2\n\n∥∆(t)∥2\n\nX\n\n∥V (0) − W∗∥2\n\nX .\n\nWe have\n\n∥∆(t)∥2\n\nX ≤ τ\n\nt−1 (cid:88)\n\nj=0\n\nqt−1−jUj.\n\n∥∆(t)∥2\n\nX ≤ 14τ\n\nt−1 (cid:88)\n\nj=0\n\n(q + 7τ )jqt−1−j ∥V (0) − W∗∥2\n\nX\n\n≤ 2(q + 7τ )t\n\n(cid:18)\n\n1 − (cid:0)\n\n(cid:1)t(cid:19)\n\nq q + 7τ\n\n∥V (0) − W∗∥2\n\nX\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nThus, we have\n\n∥aN WN :1(t) − W (t)∥2\n\nX ≤ min\n\n(cid:26) 14τ 1 − q\n\n(cid:27)\n\n, 2(q + 7τ )t\n\n∥V (0) − W∗∥2\n\nX ,\n\nas well as\n\n|L(W (t)) − L(aN WN :1(t))|\n\n≤β ∥W (t) − W∗∥X · ∥∆(t)∥X +\n\nβ 2\n\n∥∆(t)∥2\n\nX\n\n(cid:32)(cid:115)\n\n≤β\n\nmin\n\n(cid:26) 14τ 1 − q\n\n, 2(q + 7τ )t\n\n(cid:27)\n\n· qt/2 +\n\n1 2\n\nmin\n\n(cid:26) 14τ 1 − q\n\n(cid:27)(cid:33)\n\n, 2(q + 7τ )t\n\n∥V (0) − W∗∥2\n\nX .\n\nBy triangle inequality as well as L(W (t)) − L(W∗) ≤ β\n\n2 qt ∥V (0) − W∗∥2\n\nX , we have\n\n|L(aN WN :1(t)) − L(W∗)| ≤ 3β(q + 7τ )t ∥V (0) − W∗∥2\n\nX .\n\nWithout loss of generality, we replace all 14τ and 7τ by τ , which completes the proof.\n\nE GAUSSIAN INITIALIZATION FALL INTO THE CONVERGENCE REGION\n\nIn this section, we first establish some spectral properties of the products of random Gaussian matrices. The spectral properties lead to the conclusion: overparameterization guarantees that the random initialization will fall into the convergence region with high probability. Gaussian initialization: Denote by N (0, 1) the standard Gaussian distribution, and χ2 degrees of freedom. Let Sd−1 = {x ∈ Rd; ∥x∥2 = 1} be the unit sphere in Rd. The scaling factor aN = of every input in expectation. Lemma E.1. For any x ∈ Rn0, the Gaussian initialization satisfies\n\nensures that the networks at initialization preserves the norm\n\nk the chi square distribution with k\n\nn1n2···nN\n\n1√\n\n(cid:104)\n\nE\n\n∥aN WN :1(0)x∥2\n\n2\n\n(cid:105)\n\n= ∥x∥2 2 .\n\nProof of Lemma E.1. For random matrix A ∈ Rni×ni−1 with i.i.d N (0, 1) entries and any vector 0 ̸= v ∈ Rni−1 , the distribution of ∥Av∥2\n\n. We rewrite\n\n2 ∥v∥2 2\n\nis χn2\n\ni\n\n∥WN :1(0)x∥2 where Zi = ∥Wi:1(0)x∥2 / ∥Wi−1:1(0)x∥2. Then we know that the distribution of random variable Z1 ∼ χ2 random variables Zi|(Z1, · · · , Zi−1) ∼ χ2 ni the law of iterated expectations, we have\n\n2 / ∥x∥2\n\n2 = ZN ZN −1 · · · Z1,\n\nn1, and conditional distribution of (1 < i ≤ N ). Thus, Z1, · · · , Zni are independent. By\n\nE[∥WN :1(0)x∥2\n\n2/ ∥x∥2\n\n2] =\n\nN (cid:89)\n\nj=1\n\nnj.\n\nDefine ∆1 = (cid:80)N −1\n\nj=1 1/nj. Now, we introduce a new notation Ω\n\nexists k > 0, such that Ω\n\n(cid:17)\n\n(cid:16) 1 ∆1\n\n≥ k ∆1\n\n.\n\n(cid:17)\n\n(cid:16) 1 ∆1\n\n, which means that there\n\nLemma E.2. Consider real random matrix Aj ∈ Rnj ×nj−1, 1 ≤ j ≤ q with i.i.d N (0, 1) entries and any vector 0 ̸= x ∈ Rn1 . Define ∆1(q) = (cid:80)q\n\nand nmin = min1≤j≤q nj. Then\n\n1 nj\n\nj=1\n\nP(∥AqAq−1 · · · A1x∥2\n\n2 / ∥x∥2\n\n2 > ecn1 · · · nq) ≤ exp\n\n(cid:27)\n\n(cid:26)\n\n−\n\nc2 8∆1(q)\n\n=: f1(c), ∀c > 0.\n\n(56)\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nWhen 0 < c ≤ 3 ln 2, ∆1(q) ≤ c/(12 ln 2), we have\n\nP(∥AqAq−1 · · · A1x∥2\n\n2 / ∥x∥2\n\n2 < e−cn1 · · · nq) ≤ exp\n\n(cid:26)\n\n−\n\nc2 36 ln(2)∆1(q)\n\n(cid:27)\n\n=: f2(c).\n\n(57)\n\nHence, for any x ∈ Sn0−1 with probability at least 1 − e−Ω(\n\n1\n\n∆1(q) ), we have\n\ne−c2/2(n1 · · · nq)1/2 ≤ ∥Aq · · · A1x∥2 ≤ ec1/2(n1 · · · nq)1/2,\n\nwhen 0 < c2 ≤ 3 ln 2, ∆1(q) ≤ c2/(12 ln 2).\n\nProof of Lemma E.2. For random matrix Ai ∈ Rni×ni−1 with i.i.d N (0, 1) entries and any vector 0 ̸= v ∈ Rni−1 , the random variable ∥Aiv∥2\n\nis distributed as χ2\n\n2\n\nni. We rewrite\n\n∥v∥2 2\n\n∥Aq · · · A1x∥2\n\n2 / ∥x∥2\n\n2 = ZqZq−1 · · · Z1,\n\nwhere Zi = ∥Ai:1x∥2 / ∥Ai−1:1x∥2. We have Z1 ∼ χ2 Recall the moments of Z ∼ χ2\n\nm:\n\nn1, Zi|(Z1, · · · , Zi−1) ∼ χ2\n\nni\n\n(1 < i ≤ q).\n\nE[Z λ] =\n\n2λΓ( m 2 + λ) Γ( m 2 )\n\n, ∀λ > −\n\nm 2\n\n.\n\nNow, we aim to find the Chernoff type bound.\n\nCase 1: We define ratio of Gamma function\n\nR(x, λ) =\n\nΓ(x + λ) Γ(x)\n\n, λ > 0, x > 0.\n\nIn Jameson (2013), we have\n\nR(x, λ) ≤ x(x + λ)λ−1 ≤ (x + λ)λ, λ > 0, x > 0.\n\n(58)\n\nFixed c > 0, for any λ > 0 we have\n\nP(Zq · · · Z1 > ecn1 · · · nq) ≤ P((Zq · · · Z1)λ > eλc(n1 · · · nq)λ) ≤ e−λc(n1 · · · nq)−λE[(Zq · · · Z1)λ]\n\n(Markov inequality)\n\n= exp{−λ(c + ln(n1 · · · nq))}\n\nq (cid:89)\n\nj=1\n\n2λR(nj/2, λ) (Law of total expectation)\n\n≤ exp{−λ(c + ln(n1 · · · nq)) + qλ ln 2 +\n\nq (cid:88)\n\nj=1\n\nλ ln(\n\nnj 2\n\n+ λ)}(Inequality (58))\n\n= exp{−λc + λ\n\nq (cid:88)\n\nj=1\n\nln(1 +\n\n2λ nj\n\n)}\n\n≤ exp{−λc + 2λ2\n\nq (cid:88)\n\nj=1\n\n1 nj\n\n}.\n\nDefine constant ∆1(q) = (cid:80)q\n\nj=1\n\n1 nj\n\n. Set λ = c\n\n4∆1(q) , we obtain (56).\n\nCase 2: Let nmin = min1≤j≤q nj.\n\nP(Zq · · · Z1 < e−cn1 · · · nq) ≤ P((Zq · · · Z1)λ > e−λc(n1 · · · nq)λ)\n\n≤ exp{λ(c − ln(n1 · · · nq)) + qλ ln 2 +\n\nq (cid:88)\n\nj=1\n\nln R(\n\nnj 2\n\n, λ)}.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nDefine\n\nf (λ) = λ(c − ln(n1 · · · nq)) + qλ ln 2 +\n\nq (cid:88)\n\nj=1\n\nln R(\n\nnj 2\n\n, λ), −\n\nnmin 2\n\n< λ ≤ 0.\n\nNotice that f (0) = 0. Define digamma function,\n\nψ(x) =\n\nd dx\n\nln(Γ(x)) =\n\nΓ′(x) Γ(x)\n\n.\n\nQi et al. (2006) proved the following sharp inequality of digamma function,\n\nln(x +\n\n1 2\n\n) −\n\n1 x\n\n< ψ(x) < ln(x + e−γ) −\n\n1 x\n\n, x > 0,\n\nwhere γ is the Euler-Mascheroni constant, and e−γ ≈ 0.561459. Thus,\n\nf ′(λ) = c +\n\nq (cid:88)\n\n(cid:104)\n\nj=1\n\n− ln(\n\nnj 2\n\n) + ψ(\n\nnj 2\n\n(cid:105)\n\n+ λ)\n\n≥ c +\n\nq (cid:88)\n\nj=1\n\nln(1 +\n\nλ + 1/2 nj/2\n\n) −\n\nq (cid:88)\n\nj=1\n\n1 nj/2 + λ\n\n.\n\nSince ln(1 + x) is concave, we have\n\nln(1 + x) ≥ 2 ln(2)x, x ∈ [−1/2, 0].\n\nIf − nmin\n\n4 ≤ λ ≤ 0, then\n\nf (λ) = f (0) −\n\n(cid:90) 0\n\nf ′(x)dx\n\nx + 1/2 nj/2\n\n) −\n\nq (cid:88)\n\nj=1\n\n1 nj/2 + x\n\n\n\n dx\n\nλ + 1/2 nj/2\n\n) + (nj/2 + 1/2) ln(1 +\n\nλ nj/2 + 1/2\n\n) − λ − ln(1 +\n\n(cid:21)\n\nλ nj/2\n\n)\n\nλ \n\n\n\nq (cid:88)\n\nln(1 +\n\nj=1\n\n(cid:20)\n\nλ ln(1 +\n\n≤ cλ +\n\n= cλ +\n\n≤ cλ +\n\n(cid:90) λ\n\n0\n\nq (cid:88)\n\nj=1\n\nq (cid:88)\n\nj=1\n\n(λ − 1) ln(1 +\n\nλ nj/2\n\n)\n\n≤ cλ + 4 ln(2)λ(λ − 1)∆1(q).\n\nAssume 0 < c ≤ 3 ln 2. Let A = 12 ln 2, and λ∗ = − c λ∗ ≥ −nmin/4. Assume ∆1(q) ≤ c/(12 ln 2). Thus\n\nA∆1(q) . Since nmin∆1(q) ≥ 1, we have\n\nc2 A∆1(q)\n\n+ 4 ln 2\n\nc2 A∆1(q)\n\n(cid:18) ∆1(q) c\n\n+\n\n(cid:19)\n\n1 A\n\n≤ −\n\nc2 36 ln(2)∆1(q)\n\n.\n\n(59)\n\nf (λ∗) ≤ −\n\nThus, we obtain (57).\n\nLemma E.3. There exists a positive constant C(c1, c2) which only depends on c1, c2, such that if (cid:17)(cid:111) nN ∆1 ≤ C(c1, c2), then for any fixed 1 < i ≤ N , with probability at least 1 − exp we have\n\n(cid:16) 1 ∆1\n\n−Ω\n\n(cid:110)\n\nσmax(WN :i(0)) ≤ ec1(ni−1ni · · · nN −1)1/2,\n\n(60)\n\n(61)\n\nand\n\nσmin(WN :i(0)) ≥ e−c2 (ni−1ni · · · nN −1)1/2.\n\nProof of Lemma E.3. Let A = W T\n\nN :i(0). We know that\n\nσmax(A) = ∥A∥ = sup\n\nv∈SnN −1\n\n∥Av∥2\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nand\n\nσmin(A) = inf\n\nv∈SnN −1\n\n∥Av∥2 .\n\nApplying lemma E.2, we know that with probability at least 1 − exp\n\n(cid:110)\n\n−Ω\n\n(cid:16) 1 ∆1\n\n(cid:17)(cid:111) ,\n\n∥Av∥2 / ∥v∥2 ∈ [e−c2/2P, ec1/2P ],\n\nwhere P = (ni−1 · · · nN −1)1/2. Set φ = min{1 − e−c1/2, (e−c2/2 − e−c2)/(e−c2/2 + ec1)}. Take a φ-net Nφ for SnN −1 with size |Nφ| ≤ (3/φ)nN . Notice that with this size we can actually cover the unit ball, not only the unit sphere.\n\nThus, with probability at least 1 − |Nφ| exp\n\n−Ω\n\n, for all u ∈ Nφ simultaneously we have\n\n(cid:110)\n\n(cid:17)(cid:111)\n\n(cid:16) 1 ∆1\n\n∥Au∥2 / ∥u∥2 ∈ [e−c2/2P, ec1/2P ].\n\nFixed v ∈ SnN −1, there exists u ∈ Nφ such that ∥u − v∥2 ≤ φ. WLOG, we assume 1 − φ ≤ ∥u∥2 ≤ 1. We obtain\n\n∥Av∥2 ≤ ∥Au∥2 + ∥A(u − v)∥2 ≤ ec1/2P + φ ∥A∥ .\n\nTaking supereme over ∥v∥2 = 1, we obtain\n\nσmax(A) = ∥A∥ ≤\n\nec1/2 1 − φ\n\nP ≤ ec1 P.\n\nFor the lower bound, we have\n\n∥Av∥2 ≥ ∥Au∥2 − ∥A(u − v)∥2 ≥ e−c2/2P ∥u∥ − φ ∥A∥ ≥\n\n(cid:104)\n\n(1 − φ)e−c2/2 − φec1\n\n(cid:105)\n\nP ≥ e−c2 P.\n\nTaking the infimum over ∥v∥2 = 1, we get\n\nσmin(A) ≥ e−c2P.\n\nThe conclusions hold with probability at least\n\n(cid:26)\n\n1 − |Nφ| exp\n\n−Ω\n\n(cid:19)(cid:27)\n\n(cid:18) 1 ∆1\n\n(cid:26)\n\n≥1 − exp{nN ln(3/φ)} exp\n\n−Ω\n\n(cid:26)\n\n≥1 − exp\n\n−Ω\n\n(cid:19)(cid:27)\n\n,\n\n(cid:18) 1 ∆1\n\n(cid:19)(cid:27)\n\n(cid:18) 1 ∆1\n\nsince nN ∆1 ≤ C(c1, c2).\n\nLemma E.4. There exists a positive constant C(c1, c2) which only depends on c1, c2, such that if rank(X)∆1 ≤ C(c1, c2), then for any fixed 1 ≤ j < N , with probability at least 1 − exp{−Ω\n\n} we have\n\n(cid:17)\n\n(cid:16) 1 ∆1\n\nand\n\nσmax(Wj:1(0)|R(X)) ≤ ec1(n1n2 · · · nj)1/2,\n\nσmin(Wj:1(0)|R(X)) ≥ e−c2(n1n2 · · · nj)1/2.\n\n(62)\n\n(63)\n\nProof of Lemma E.4. The proof is similar to that of previous lemma. The only difference is that now we consider the φ−net to cover the unit sphere in R(X) ∩ Rn0 , with dim R(X) ∩ Rn0 = rank(X), where R(X) represents the column space of X.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nLemma E.5. Set C = nmax/nmin < ∞, θ = 1/2. Assume Ω(1/∆1) ≥ k ∆1 constant and ∆1 satisfies\n\n, where 0 < k < 1 is a\n\n \n\n\n\n(cid:110) k\n\n5 ln(6) , (cid:110)\n\nk 5 ln(5 ln(6)e/k)\n\n(cid:111)\n\n5 ln(5 ln(6)e/k) , k\n\nk\n\n5\n\n(cid:111)\n\n∆1 ≤ min\n\n∆1 ln(C) ≤ min ∆1 ln(N 2θ) ≤ k/5.\n\nGiven 1 < i ≤ j < N , with probability at least 1 − 2e−k/(5∆1) = 1 − e−Ω(1/∆1) we have\n\n∥Wj:i(0)∥ ≤ Mk\n\n√\n\nCN θ(ni · · · nj−1 · max{ni−1, nj})1/2,\n\nwhere Mk is a positive constant that only depends on k.\n\nProof of Lemma E.5. WLOG, assume ni−1 ≤ nj. Let A = Wj:i(0). From lemma E.2, we know that fixed v ∈ Sni−1−1, with probability at least 1−e−Ω(1/∆1) we have ∥Av∥2 ≤ 4/3(ni · · · nj)1/2. .\nTake a small constant c = 5 ln(6)C . Let v1, · · · , vni−1 be an orthonormal basis for Rni−1 . Partition the index set {1, 2, · · · , vni−1} = S1 ∪ S2 ∪ · · · ∪ S⌈N 2θ/c⌉, where |Sl| ≤ ⌈cni−1/N 2θ⌉ for each 1 ≤ l ≤ ⌈N 2θ/c⌉. The following discussion is similar to the proof of lemma E.3, hence we omit some details. For each l, taking a 1/2− net Nl for the set VSl = {v ∈ Sni−1−1; v ∈ span{vi; i ∈ Sl}}, we can get\n\nkN 2θ 5 ln(6)∆1ni−1\n\n≥\n\nk\n\nwith probability at least\n\n∥Au∥2 ≤ 4(ni · · · nj)1/2, u ∈ VSl ,\n\n1 − |Nl|e−k/∆1 ≥ 1 − exp{−k/∆1 + (cni−1/N + 1) ln 6} ≥ 1 − e−3k/(5∆1),\n\n5 ln(6) .\n\nsince ∆1 ≤ k Therefore, for any v ∈ Rni−1, we can write it as the sum v = (cid:80) for each l. We also know that ∥v∥2 Then we have\n\nl≥1 |αl|2.\n\n2 = (cid:80)\n\nl alvl, where αl ∈ R and vl ∈ VSl\n\n∥Av∥2 ≤\n\n(cid:88)\n\nl\n\nThus,\n\n|αl| ∥Avl∥2 ≤ 4(ni · · · nj)1/2\n\n(cid:115)\n\n⌈N 2θ/c⌉\n\n(cid:88)\n\nl\n\n√\n\n|al|2 ≤ Mk\n\nCN θ(ni · · · nj)1/2 ∥v∥2 .\n\n√\n\n∥A∥ ≤ Mk\n\nCN θ(ni · · · nj)1/2.\n\nNotice that when C ≤ e, ∆1 ≤\n\nk\n\n5 ln(5 ln(6)e/k) ≤\n\nk\n\n5 ln(5 ln(6)·C/k) , and when C > e, we have\n\n∆1 ln(C) ≤ min\n\n(cid:26)\n\nk 5 ln(5 ln(6)e/k)\n\n(cid:27)\n\n, k/5\n\n≤\n\nk ln(C) 5 ln(5 ln(6) · C/k)\n\n.\n\nThe success probability is at least\n\n1 − ⌈N 2θ/c⌉ · e−3k/(5∆1) (cid:18) 5 ln(6) · C k\n\n≥1 − exp\n\nln\n\n(cid:26)\n\n(cid:19)\n\n+ ln(N 2θ) − 3k/(5∆1)\n\n(cid:27)\n\n− e−3k/(5∆1)\n\n≥1 − 2e−k/(5∆1),\n\nsince\n\n∆1 ≤\n\nk 5 ln (5 ln(6) · C/k)\n\nand ∆1 ln(N 2θ) ≤ k/5.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nProof of Lemma 2.3. Set r = rank(X), and u1, · · · , ur be an orthonormal basis of the column space of X. Then, PX = (cid:80)r\n\ni=1 uiuT i .\n\nNotice that\n\n∥anWN :1(0)∥2\n\nX = ∥anWN :1(0)PX ∥2\n\nF =\n\nr (cid:88)\n\ni=1\n\n∥anWN :1(0)ui∥2 2 .\n\nBy assumption, we have\n\nE ∥anWN :1(0)∥2\n\nX = E\n\nr (cid:88)\n\ni=1\n\n∥anWN :1(0)ui∥2\n\n2 = r.\n\nThe Markov inequality implies\n\nP(∥anWN :1(0)∥2\n\nX ≥\n\n2r δ\n\n) ≤\n\nδ 2\n\n.\n\nTherefore, we can bound the initial loss value as\n\nL0 − L(W∗) ≤ ⟨∇L(W∗), aN WN :1(0)X − W∗⟩ +\n\nβ 2\n\n∥aN WN :1(0) − W∗∥2\n\nX\n\n=\n\nβ 2\n\n∥aN WN :1(0) − W∗∥2\n\nX\n\n≤ β(∥aN WN :1(0)∥2\n\nX + ∥W∗∥2\n\nX )\n\n≤ β(\n\n2r δ\n\n+ ∥W∗∥2\n\nX ),\n\nwith probability at least 1 − δ/2.\n\nProof of Theorem B.1. The requirement on size {n1, n2, · · · , nN −1, N } in (16) ensures that lemma E.3, E.4, E.5, 2.3, and D.1 hold. WLOG, we set c1 = c/6, c2 = c/3, M = 2Mk probability at least\n\nC0, B0 = Bδ, and η =: (1−ε)2nN\n\ne2cβN , then with\n\n√\n\n1 − N 2e−Ω(1/∆1) − δ/2 ≥ 1 − δ, since ∆1 ≤\n\n1 C(c)\n\nmin\n\n(cid:26) 1\n\nln N\n\n,\n\n1 ln(1/δ)\n\n(cid:27)\n\n,\n\nthe random initialization satisfies the initialization assumption (31) and the overparameterization assumption (32). By applying Lemma D.1, we complete the proof.\n\nF ORTHOGONAL INITIALIZATION FALL INTO THE CONVERGENCE REGION\n\nThe following are some basic facts for random projection and embedding. Most of the following properties can be found in Eaton (1989).\n\nProposition F.1.\n\n1. A is a random embedding if and only if AT is a random projection.\n\n2. If A is a square matrix, then random projection, random embedding and random orthogo-\n\nnal matrix are equivalent.\n\n3. The uniform distribution on the group is a left and right invariant probability measure: that is, if A is a random orthogonal matrix, then A, U A, AU are all random orthogonal matrices, where U is a non-random orthogonal matrix.\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\n4. Assume X is a n × q(q ≤ n) random matrix whose entries are i.i.d. N (0, 1) random variables. Then A := X(X T X)−1/2 is a random embedding, since AT A = Iq and the distribution of A is left invariant, which means that A and U A have the same distribution, where U is a non-random orthogonal matrix.\n\n5. If A is a uniform distribution over an orthogonal group of order n and A is partitioned as 2 are both random\n\nA = (A1, A2), where A1 is n × q and A2 is n × (n − q), then AT orthogonal matrix.\n\n1 and AT\n\n6. The columns of uniform distribution over the orthogonal group of order n, and\n\n(ξ1, · · · , ξn) 1 + ξ2 have the same distribution, where ξ1, · · · , ξn are i.i.d. N (0, 1) random variables.\n\n2 + · · · + ξ2\n\n(cid:112)ξ2\n\nn\n\n7. Assume A = An×p, n ≤ p is a random orthogonal projection. For any v ∈ Sp−1, ∥Av∥2 j ) are both following beta distribution with α = n/2, β = (p −\n\nand ((cid:80)n n)/2, where ξ1, · · · , ξn are i.i.d. N (0, 1) random variables.\n\ni )/((cid:80)p\n\nj=1 ξ2\n\ni=1 ξ2\n\n2\n\nRemark 9. There are several ways to construct the random matrix A = (aij)q×n, q ≤ n, which is uniformly distributed over rectangular matrices with AAT = c2Iq, c > 0. Let On be uniformly distributed over a real orthogonal group of order n, and On is partitioned as On = (AT 2 )T , where A1 is q × n. Assume X = (xij)q×n, and xij are independent standard normal random variables. Then, A, cA1, and c(XX T )−1/2X have the same distribution. Lemma F.2. For any x ∈ Rn0, the one peak random projection and embedding initiation satisfies\n\n1 , AT\n\n(cid:104)\n\nE\n\n∥aN WN :1(0)x∥2\n\n2\n\n(cid:105)\n\n= ∥x∥2 2 .\n\nProof. Let D = Wp:1(0)/ Let Ai = Wi:p+1(0)/\n\n√\n\n√\n\nn1n2 · · · np. Then D is an embedding matrix. Thus, ∥Dx∥2\n\n2 = ∥x∥2 2.\n\nnpnp+1 · · · ni−1, where i ≥ p + 1, and Ap = I.\n\nSet Bi = ∥AiDx∥2 2, i ≥ p + 1. Then, Bi follows beta distribution B(ni/2, (ni−1 − ni)/2) given Bi−1, Bi−2, · · · , Bp+1, i ≥ p + 1. If ni = ni−1, then Bi|(Bi−1, Bi−2, · · · , Bp+1) = 1, a.s.\n\n2 / ∥Ai−1Dx∥2\n\nIf B ∼ B(a, b), then the expectation is given by the following equation,\n\nThus, by the law of total expectation, we have\n\nEB =\n\na a + b\n\n.\n\nnN np\n\nE ∥aN WN :1(0)x∥2\n\n2 = E ∥AN Dx∥2\n\n2 = EBN BN −1 · · · Bp+1 ∥Dx∥2\n\n2 =\n\nnN np\n\n∥x∥2 2 .\n\nThis completes the proof.\n\nNext, we introduce sub-Gaussian random variables, associated with bounds on how a random variables deviate their expected value.\n\nDefinition F.1. A random variable X with finite mean μ = EX is sub-Gaussian if there is a positive number σ such that:\n\nE[exp(λ(X − μ))] ≤ exp\n\n(cid:19)\n\n(cid:18) λ2σ2 2\n\nfor all λ ∈ R\n\n(64)\n\nSuch a constant σ2 is called a proxy variance, and we say that X is σ2-sub-Gaussian, and we write X ∼ SG(σ2). Example F.1. Normal distribution N (μ, σ2) of course is σ2 sub-Gaussian. For beta distribution, Elder (2016) showed that B(a, b) is\n\n4(a+b)+2 -sub-Gaussian and later, Marchal\n\n1\n\n& Arbel (2017) concluded\n\n1\n\n4(a+b+1) -sub-Gaussian.\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nThe Hoeffding bound for random variable X with a mean μ and sub-Gaussian parameter σ is given by,\n\nP [|X − μ| ≥ t] ≤ 2 exp\n\n−\n\n, ∀t ≥ 0.\n\n(65)\n\n(cid:26)\n\n(cid:27)\n\nt2 2σ2\n\nSimply applying the Chernoff bound for B(a, b), we obtain the following lemma. Lemma F.3. Assume a random variable B distributed as a beta distribution B(a, b) with two positive shape parameters a and b. Then, (cid:12) (cid:12) (cid:12) (cid:12)\n\n≥ y) ≤ 2 exp (cid:8)−2(a + b)y2(cid:9) , y ≥ 0.\n\na a + b\n\n(cid:12) (cid:12) P( (cid:12) (cid:12)\n\nB −\n\nHence,\n\nP\n\n(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)\n\nB −\n\na a + b\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ ε\n\n(cid:19)\n\na a + b\n\n≥ 1 − exp{−Ω(a2/(a + b))},\n\nwhere Ω(·) only depend on ε. For the upper tail, we can obtain a better bound, (cid:18)\n\n(cid:19)\n\nP\n\nB ≥ (1 + ε)\n\na a + b\n\n≤ exp {−(ε − ln(ε + 1))a} .\n\n(66)\n\nProof of Lemma F.3. We only need to prove the third inequality. Assume random variable B ∼ B(a, b). Set v = a + b, (1 + t) a v ≤ y < 1, t > 0, and r > 0. We are going to estimate the Chernoff bound for B, which is\n\nP(B ≥ y) ≤ e−(ry−ln EerB ) =: e−Ir(y).\n\nThe moment generating function of B is given by\n\nEerB = 1 +\n\n∞ (cid:88)\n\nk=1\n\na(a + 1) · · · (a + k − 1) v(v + 1) · · · (v + k − 1)\n\nrk k!\n\n≤ 1 +\n\n∞ (cid:88)\n\nk=1\n\na(a + 1) · · · (a + k − 1) vk\n\nrk k!\n\n, r > 0.\n\nRecall that the Maclaurin series of (1 − r/v)−a over (−v, v), is given by equation\n\nThus,\n\n(1 − r/v)−a = 1 +\n\n∞ (cid:88)\n\nk=1\n\na(a + 1) · · · (a + k − 1) vk\n\nrk k!\n\n.\n\nIr(y) = ry − ln EerB ≥ ry + a ln(1 − r/v).\n\nSet r = v − a/y ∈ (0, v). We obtain\n\nP(B ≥ y) ≤ exp{−(vy − a + a ln(a/(vy)))} =: exp{−vy · g(a/(vy))}, (1 + t)\n\na v\n\n≤ y < 1\n\nwhere g(x) = 1 − x + x ln(x), x = a/(vy) ∈ (0, 1/(1 + t)]. Notice that g(1) = 0 and g′(x) = ln(x) < 0 over x ∈ (0, 1). We know that\n\ng(x) ≥ g(1/(1 + t)) =\n\nThus,\n\nt − ln(1 + t) t + 1\n\n, t > 0.\n\nP(B ≥ y) ≤ exp\n\n(cid:26)\n\n−vy ·\n\n(cid:27)\n\nt − ln(1 + t) t + 1\n\n= exp {−(t − ln(1 + t))a} , y = (1 + t)\n\na v\n\n< 1.\n\nSet y = (1 + ε) a\n\na+b . We obtain the inequality (66).\n\nRemark 10. It is trivial to check ∥Wj:i(0)∥ = (nini+1 · · · nj)1/2, 1 ≤ i ≤ j ≤ p, ∥Wj:i(0)∥ = (ni−1ni · · · nj−1)1/2, p + 1 ≤ i ≤ j ≤ N, ∥Wj:i(0)∥ ≤ (nini+1 · · · nj−1)1/2(np)1/2\n\n(cid:19)1/2\n\n≤\n\n(cid:18) nmax nmin\n\n(nini+1 · · · nj−1 · max{ni−1, nj})1/2, 1 ≤ i < p < j ≤ N, (i, j) ̸= (1, N ).\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nRemark 11. As a special case, if n1 = n2 = · · · = nN −1 = n, we know that ∥Wj:i(0)∥ = (ni−1ni · · · nN −1)1/2 = n(N −i+1)/2. Lemma F.4. Assume np/ min{n1, nN −1} ≤ C0 < ∞. Set ε > 0. Let C(ε) represent the constant depend only on ε. If n1/C0 ≥ C(ε)nN , then with probability at least 1 − e−Ω(n1/C0)\n\nσmax(WN :i(0)) ≤ (1 + ε)(ni−1ni · · · nN −1)1/2, 2 ≤ i ≤ p σmin(WN :i(0)) ≥ (1 − ε)(ni−1ni · · · nN −1)1/2, 2 ≤ i ≤ p.\n\nSimilarly, if nN −1/C0 ≥ C(ε)rank(X), then with probability at least 1 − e−Ω(nN −1/C0)\n\nσmax(Wj:1(0)|R(X)) ≤ (1 + ε)(n1n2 · · · nj)1/2, p + 1 ≤ j ≤ N σmin(Wj:1(0)|R(X)) ≥ (1 − ε)(n1n2 · · · nj)1/2, p + 1 ≤ j ≤ N.\n\nN :p+1(0) and p:i(0). Assume v ∈ SnN −1. Easy to see that Ai is a product of random\n\nProof of Lemma F.4. Let D = (nN −1nN −2 · · · np)−1/2W T Ai = (npnp−1 · · · ni)−1/2W T orthogonal projection and D is a random embedding. Let e1 = (1, 0, 0, · · · , 0)T ∈ Rnp . There exists orthogonal matrix T such that T Dv = e1, ∥e1∥2 = ∥T Dv∥2 = ∥v∥2 = 1. Since random orthogonal projection are right invariant, we have\n\nP(∥AiDv∥2 ≥ y) = E\n\n(cid:16)\n\n(cid:104)\n\nE\n\nI{∥AiT T e1∥2≥y}\n\n(cid:17)(cid:105)\n\n(cid:12) (cid:12) (cid:12) D\n\n= E (cid:2)E (cid:0) I{∥Aie1∥2≥y}\n\n(cid:12) (cid:12) D(cid:1)(cid:3) = P(∥Aie1∥2 ≥ y).\n\nThis proves that ∥AiDv∥2\n\n2 and ∥Aie1∥2 2 / ∥v∥2\n\n2 have the same distribution. 2 = (cid:13) Claim: If v ̸= 0, then ∥AiDv∥2 beta distribution B(ni−1/2, (np − ni−1)/2). Define Bp = ∥Ape1∥2 Then Bp ∼ B(np−1/2, (np − np−1)/2), Bp−1|Bp ∼ B(np−2/2, (np−1 − np−2)/2), Bi|(Bp, · · · , Bi+1) ∼ B(ni−1/2, (ni − ni−1)/2). If ni+1 = ni, we know that Bi|(Bp, · · · , Bi+1) = 1, a.s. If B ∼ B(a, b), then the moments are given by the following equations,\n\n2, i = p − 1, p − 2, · · · , 1.\n\np · · · nN −1)−1/2W T\n\n2, Bi = ∥Aie1∥2\n\n2 / ∥Ai+1e1∥2\n\n(cid:13)(nini+1 · · · n2\n\nN :iv(cid:13) 2\n(cid:13) 2\n\n/ ∥v∥2\n\n2 follows\n\n· · · ,\n\n(67)\n\nEB =\n\na a + b\n\n, and EBk =\n\na a + b\n\na + 1 a + b + 1\n\n· · ·\n\na + k − 1 a + b + k − 1\n\n.\n\nBy the law of total expectation, we have\n\nEBiBi+1 · · · Bp =\n\nni−1 ni\n\nni ni+1\n\n· · ·\n\nnp−1 np\n\n=\n\nni−1 np\n\n,\n\nas well as\n\nE(BiBi+1 · · · Bp)k =\n\nni−1/2 np/2\n\nni−1/2 + 1 np/2 + 1\n\n· · ·\n\nni−1/2 + k − 1 np/2 + k − 1\n\n.\n\nNotice that all the integer moments of BiBi+1 · · · Bp match those of B(ni−1/2, (np −ni−1)/2). We can verify that beta distribution satisfies Carleman’s condition, which implies that BiBi+1 · · · Bp ∼ B(ni−1/2, (np − ni−1)/2). Thus, ∥AiDv∥2\n\n2 ∼ B(ni−1/2, (np − ni−1)/2), which proves the claim.\n\n2 / ∥v∥2\n\nWith probability at least 1 − exp{−Ω(n1/C0)}, we have\n\n(1 − ε)2 ni−1 np\n\n≤ ∥ADv∥2\n\n2 ≤ (1 + ε)2 ni−1\n\nnp\n\n, ∥v∥2 = 1.\n\nUsing the φ−net technique, which has been already used to prove lemma E.3, we know that\n\nand\n\nσmin(AD) ≥ (1 − ε)\n\nσmax(AD) ≤ (1 + ε)\n\n(cid:19)1/2\n\n(cid:18) ni−1 np\n\n(cid:19)1/2\n\n(cid:18) ni−1 np\n\n,\n\n,\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nwith probability at least 1 − exp{nN ln(3/φ(ε))} exp{−Ω(n1/C0)} ≥ 1 − exp{−Ω(n1/C0), since n1/C0 ≥ C(ε)nN , for 2 ≤ i ≤ p. Hence, with probability at least 1 − e−Ω(n1/C0), we have\n\nσmin(WN :i(0)) ≥ (1 − ε) (ni−1 · · · nN −1)1/2 ,\n\nand\n\nσmax(WN :i(0)) ≤ (1 + ε) (ni−1 · · · nN −1)1/2 . The other part of the proof is similar to that of lemma E.4, so we omit it.\n\nProof of Theorem B.2 . Set c > 0, c1 = c/6, c2 = c/3. In lemma F.4, we can pick a ε > 0, such that 1 + ε ≤ ec1/2 and 1 − ε ≥ e−c2/2. Set M = 2 The requirement on size {n1, n2, · · · , nN −1, N } in (17) make sure that the remark 10, lemma F.4, lemma 2.3, and lemma D.1 all hold. Notice that even though we need the conclusions in lemma F.4 to hold simultaneously for 2 ≤ i ≤ p, p + 1 ≤ j ≤ N , it suffices to apply lemma F.4 over i ∈ I and j ∈ J, such that {ni; i ∈ I} and {nj; j ∈ I} both have distinct values. Since |I| ≤ N and |J| ≤ N , with probability at least\n\nC0, θ = 0, B0 = Bδ, and η = (1−ε)2nN e2cβN .\n\n√\n\n1 − 2N e−Ω(nmin/C0) − δ/2 ≥ 1 − δ,\n\nthe one peak random orthogonal projection and embedding initialization satisfies the initialization assumption (31) and the overparameterization assumption (32).\n\nUnder the assumption n1 = n2 = · · · = nN −1, we can use remark 11 to replace lemma F.4. Thus, with probability at least 1 − δ/2 ≥ 1 − δ, (31) holds. By applying lemma 2.3 and D.1, we complete the proof.\n\nProof of Theorem B.3. Let WN (0) = √\n1, and W1(0) =\n\nnU1[Inx , 0]T V T\n\nnUN [Iny , 0]V T\n\n, 2 ≤ i ≤ N − 1 . Now, we want to verify (31). By a simple calculation, we have\n\nN , · · · , Wi(0) =\n\nnUiInV T\n\ni\n\n√\n\n√\n\n \n\n\n\nσmax(WN :i+1(0)) = σmin(WN :i+1(0)) = n(N −i)/2, 1 ≤ i ≤ N − 1, σmax(Wi−1:1(0)|R(X)) = σmax(Wi−1:1(0)|R(X)) = n(i−1)/2, 2 ≤ i ≤ N, ∥Wj:i(0)∥ = n(j−i+1)/2, 1 < i ≤ j < N.\n\n(68)\n\nNotice that for any 1 ≤ p ≤ m\n\n∥aN WN :1(0)x∥2\n\n2 =\n\nn nN\n\n(cid:13) (cid:13)UN [Iny , 0]V T\n\nN UN [Inx , 0]T V T\n\n1 x(cid:13) 2\n(cid:13)\n\n2 =\n\nn nN\n\n(cid:13) (cid:13)UN [Iny , 0]V T\n\nN x′(cid:13) 2\n(cid:13)\n\n2 ,\n\nwhere x′ = UN [Inx , 0]T V T Since the distribution of UN [Iny , 0]V T have\n\n1 x, ∥x∥2 = ∥x′∥2.\n\nN is right invariant under multiplying orthogonal matrices, we\n\n(cid:13) (cid:13)UN [Iny , 0]V T\n\nN x′(cid:13) 2\n2 / ∥x∥2 (cid:13)\n\n2 ∼ B(\n\nny 2\n\n,\n\nn − ny 2\n\n).\n\nThus,\n\nApplying lemma 2.3, we have\n\n(cid:104)\n\nE\n\n∥aN WN :1(0)x∥2\n\n2\n\n(cid:105)\n\n= ∥x∥2 2 .\n\nL0 − L(W∗) ≤ β\n\n(cid:18) 2 · rank(X) δ\n\n+ ∥W∗∥2\n\nX\n\n(cid:19)\n\n,\n\nwith probability at least 1 − δ/2.\n\nBy applying Lemma D.1 with c > 0, c1 = c/6, c2 = c/3, θ = 0, we complete the proof.\n\nProof of Theorem 3.1. Theorem 3.1 is a special case of Theorem B.1 and Theorem B.2. Hence, we omit the proof.\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\nProof of Theorem 3.2. In Theorem B.1, B.2, and B.3, we proved that for given constant c1, c2 > 0 and 0 < ε, δ/2 < 1/2 as well as a learning rate η, there exists a constant C = C(c1, c2) such that all three kinds of random initializations will fall into the convergence region with probability at least 1 − δ. By applying Lemma 2.3, we complete the proof.\n\nG TABLES\n\nIn this section, we provide some empirical evidences to support the argument expressed in Section 4:Why do bad saddles not affect GD for overparameterized deep linear neural networks? Consider the following procedures for tables of ∥Wi(t)−Wi(0)∥F\n\n:\n\n∥Wi(0)∥F\n\na) We consider X ∈ R128×1000, and W∗ ∈ R10×128 and set Y = W∗X + ε, where the entries\n\nin X and ε are drawn i.i.d. from N (0, 1).\n\nb) We consider the loss function 1\n\n2 ∥aN WN :1X − Y ∥2 F .\n\nc) For the given deep linear networks, we apply the orthogonal initializations, which are de-\n\nnoted as Wj(0), 1 ≤ j ≤ N .\n\nd) We set the learning rate as η = nN\n\nN ·∥X∥2 for the deep linear neural networks.\n\ne) We prepare the tables for ∥Wi(t)−Wi(0)∥F\n\n∥Wi(0)∥F\n\n.\n\nLet n1 = n2 = n3 = 2000, N = 4. Assume W∗ are drawn i.i.d. from N (0, 25). We obtain the following table:\n\ni = 1 0.05161 t = 1 0.08779 t = 2 0.11335 t = 3 0.12109 t = 4 0.12527 t = 5 0.12611 t = 6 0.12755 t = 7 0.12745 t = 8 t = 9 0.12819 t = 10 0.12793\n\ni = 2 0.00826 0.01389 0.01781 0.01894 0.01956 0.01967 0.01988 0.01986 0.01997 0.01992\n\ni = 3 0.00826 0.01389 0.01779 0.01889 0.01948 0.01958 0.01978 0.01975 0.01987 0.01982\n\ni = 4 0.18464 0.31396 0.40435 0.42920 0.44282 0.44476 0.44955 0.44876 0.45136 0.45018\n\nLet n1 = n2 = 10000, N = 2. Assume W∗ are drawn i.i.d. from N (0, 4). We obtain the following table:\n\ni = 1 0.02708 t = 1 0.04319 t = 2 0.05296 t = 3 0.05888 t = 4 0.06248 t = 5 0.06468 t = 6 0.06603 t = 7 0.06688 t = 8 t = 9 0.06741 t = 10 0.06775\n\ni = 2 0.00153 0.00244 0.00299 0.00333 0.00353 0.00365 0.00373 0.00377 0.00380 0.00382\n\ni = 3 0.04844 0.07727 0.09474 0.10533 0.11176 0.11569 0.11811 0.11962 0.12057 0.12117\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nLet n1 = n2 = 4000, N = 2. Assume W∗ are drawn i.i.d. from N (0, 1). We obtain the following table:\n\nLet n1 = n2 = 8000, N = 2. Assume W∗ are drawn i.i.d. from N (0, 1). We obtain the following table:\n\nLet n1 = n2 = 12000, N = 2. Assume W∗ are drawn i.i.d. from N (0, 1). We obtain the following table:\n\nLet n1 = n2 = 20000, N = 2. Assume W∗ are drawn i.i.d. from N (0, 1). We obtain the following table:\n\ni = 1 0.01622 t = 1 0.02684 t = 2 t = 3 0.03411 0.03919 t = 4 t = 5 0.04280 0.04539 t = 6 t = 7 0.04729 0.04869 t = 8 t = 9 0.04974 t = 10 0.05054\n\ni = 1 0.01173 t = 1 0.01944 t = 2 0.02470 t = 3 0.02838 t = 4 0.03098 t = 5 0.03287 t = 6 0.03426 t = 7 0.03530 t = 8 t = 9 0.03608 t = 10 0.03668\n\ni = 1 0.00965 t = 1 0.01597 t = 2 0.02025 t = 3 0.02323 t = 4 0.02535 t = 5 0.02690 t = 6 0.02804 t = 7 0.02890 t = 8 t = 9 0.02955 t = 10 0.03006\n\ni = 1 0.00713 t = 1 0.01181 t = 2 0.01499 t = 3 0.01720 t = 4 0.01878 t = 5 0.01994 t = 6 0.02079 t = 7 0.02144 t = 8 t = 9 0.02193 t = 10 0.02231\n\ni = 2 0.00290 0.00480 0.00609 0.00700 0.00764 0.00810 0.00844 0.00868 0.00887 0.00901\n\ni = 2 0.00148 0.00246 0.00312 0.00358 0.00391 0.00415 0.00432 0.00445 0.00455 0.00463\n\ni = 2 0.00099 0.00164 0.00208 0.00239 0.00261 0.00277 0.00289 0.00297 0.00304 0.00309\n\ni = 2 0.00057 0.00095 0.00121 0.00138 0.00151 0.00161 0.00168 0.00173 0.00177 0.00179\n\ni = 3 0.05802 0.09601 0.12202 0.14018 0.15306 0.16232 0.16908 0.17408 0.17782 0.18066\n\ni = 3 0.04195 0.06955 0.08838 0.10151 0.11083 0.11758 0.12253 0.12624 0.12904 0.13118\n\ni = 3 0.03453 0.05712 0.07244 0.08310 0.09069 0.09621 0.10029 0.10336 0.10570 0.10750\n\ni = 3 0.02551 0.04225 0.05362 0.06154 0.06720 0.07132 0.07438 0.07668 0.07844 0.07981\n\nH FIGURES\n\nIn this section, we provide some empirical evidences to support the results in Section 4: Numerical Experiments. We will show how the trajectories of the non-convex deep linear neural networks are\n\n36\n\nUnder review as a conference paper at ICLR 2023\n\nrelated to a convex optimization problem for GD under different initialization schemes. Consider the following procedures for plots of the logarithm of loss as a function of number of iterations:\n\na) We choose X ∈ R128×1000 and W∗ ∈ R10×128 and set Y = W∗X + ε, where the entries\n\nin X, W∗ and ε are drawn i.i.d. from N (0, 1).\n\nb) We consider the loss function 1\n\n2 ∥aN WN :1X − Y ∥2 F .\n\nc) For the given linear networks, we apply the Gaussian initialization and one peak random orthogonal projection and embedding initialization, which are denoted as Wj(0), 1 ≤ j ≤ N .\n\nd) For the convex optimization problem (1), we set the initialization to be W (0) =\n\naN WN (0) · · · W1(0).\n\ne) We set the learning rates as η = nN and convex problem, respectively.\n\nN ·∥X∥2 and η∗ = N\n\nnN\n\nη for the deep linear neural networks\n\nf) We draw the loss function through 25 iterations.\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Plot of Loss as a function of number of iterations with n1 = n2 = n3 = 128 (First), 200 (Second), 2000 (Third) for Gaussian initialization, respectively.\n\n38\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Plot of Loss as a function of number of iterations with n1 = n2 = n3 = 128 (First), 200 (Second), 5000 (Third) for Orthogonal initialization, respectively.\n\n39",
  "translations": [
    "# Summary Of The Paper\n\nThis paper studies the convergence rates of gradient descent (GD) on minimizing the training loss $L(W_N \\cdots W_1)$ of deep linear networks, where $L(W)$ is the convex training loss function of the corresponding linear model (i.e., the loss when the product $W_N \\cdots W_1$ is collapsed into a single matrix $W$). We assume that $L(W)$ is strongly convex and smooth in the subspace spanned by the data points. The paper considers the following three initialization schemes\n\n- Gaussian initialization;\n- One peak random orthogonal projections and embeddings initialization (which generalizes the orthogonal initialization proposed by Hu et al. (2020));\n- Special balanced initialization (which is a special case of balanced initialization considered in Arora et al. (2018a)).\n\nFor these schemes, the paper proves that, for sufficiently wide networks,\n\n1. GD converges linearly, and the convergence rate is of the same order as minimizing $L(W)$ as a function of $W$ (Theorems B.1, B.2, and B.3; Theorems 3.1 and 3.2 in the main text are special cases of these theorems).\n2. The trajectory of the product $W_N(t) \\cdots W_1(t)$ as we minimize $L(W_N \\cdots W_1)$ in fact stays close to the trajectory of the $W(t)$ as we minimize $L(W)$ with an appropriately rescaled learning rate (Theorem 3.3).\n\n# Strength And Weaknesses\n\nConvergence of optimization methods on training linear neural networks is an important area as it can provide valuable intuitions for understanding nonlinear networks. The paper generalizes existing results and presents the main results clearly.\n\nThe $O(\\kappa \\log 1/\\varepsilon)$ iteration complexity to achieve $\\varepsilon$-suboptimality looks indeed sharp because it matches the convergence rate of GD on the convex counterpart. Nevertheless, I should mention that the $O(\\kappa \\log 1/\\varepsilon)$ complexity was also achieved by some previous results such as Du and Hu (2019) and Hu et al. (2020). \n\nThe next main result that the trajectory of the product $W_N(t) \\cdots W_1(t)$ closely follows the trajectory of $W(t)$ as we minimize the corresponding convex function $L(W)$ is something that I haven't seen in the literature, unless I missed some existing results. This part is quite intriguing as it establishes that optimizing linear NN follows a similar trajectory as the corresponding convex problem, while the problem itself is nonconvex. This observation can deliver useful insights to the community.\n\nWhile I like the observation made in Theorem 3.3, for the rest of the main results (Theorems B.1, B.2, and B.3), I got the impression that the contributions made by this paper may be somewhat limited. As pointed out above, for Gaussian and orthogonal initializations, the sharp rate $O(\\kappa \\log 1/\\varepsilon)$ was already achieved by Du and Hu (2019) and Hu et al. (2020). \n\nA quick perusal of the proof reveals that the paper also builds on the techniques developed by these two existing papers. Remark 5 states that for the case where all hidden layers have the same width, Theorems B.1 and B.2 recover the main results of these papers. The \"convergence region\" established in Lemma D.1 almost exactly follows the conditions developed in the two papers. Appendix E also seems to follow the flow of the proof in Du and Hu (2019).\n\nThese observations make me question if Theorems B.1 and B.2 are merely a technical extension of the existing results on \"hidden layers having identical widths\" to just \"hidden layers with general widths.\" I know this could be a false impression as I didn't go through the proof carefully; I would be happy if the authors prove me wrong.\n\nAnyway, this concern of novelty makes me hesitate to recommend acceptance at the moment. Importantly, the main text of this paper does not make any precise comparisons against the existing results. Can you elaborate/highlight the technical challenges that had to be overcome in extending the existing results to get Theorems B.1 and B.2?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written and delivers the main results clearly. I found the paper quite enjoyable to read. \n\nIn terms of novelty, the observation made in Theorem 3.3 (Lemmas D.3–D.5) looks novel to me, but as noted above, the remaining elements of the proof seem to rely upon existing results. \n\nSome minor issues:\n- In Eq (4), the RHS has to have a leading $\\frac{\\beta}{2}$ factor?\n- In Definition 2.1, $n_i I_{n_i}$ -> $n_i I_{n_{i-1}}$ and $n_{j-1} I_{n_{j-1}}$ -> $n_{j-1} I_{n_j}$?\n- In the definition of special balanced initialization, there is no mention on how $V_1$ is defined? Also I thought $V_N = U_{N-1}$ should also hold here?\n- In the statements of Theorem 3.1 and 3.2, it looks weird that $\\delta$ does not show up anywhere in the stated bound.\n- In Theorem 3.3, it would be useful to mention that $q \\in (0,1)$ for the step size of interest?\n- The plots in Figure 1 should better be drawn in \"semilogy\" style?\n\n# Summary Of The Review\n\nI enjoyed reading the paper and I think the results presented in the paper deliver valuable insights. However, at the same time, it may be the case that some of the main theorems rely too heavily on some existing results and hence are of limited novelty. In the rebuttal, it would be very helpful if the authors could clarify the technical barriers that had to be overcome in carrying out the extensions. I would be happy to raise my score if my concerns get resolved.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable",
    "# Summary Of The Paper\nThe paper presents a detailed convergence analysis of the Gradient Descent (GD) algorithm applied to overparameterized deep linear neural networks. It establishes that the convergence rates of GD are sharp and independent of the type of random initialization used, provided that the hidden layers have sufficient width. The authors demonstrate that GD effectively avoids bad saddle points, leading to efficient training even in the non-convex landscape of deep learning. Key findings include the equivalence of the convergence rates for GD in deep linear networks to those of convex problems, supported by both theoretical theorems and empirical experiments.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive theoretical analysis, which builds upon previous research to provide clearer insights into the behavior of GD in the context of deep linear networks. The establishment of sharp convergence rates across different initialization schemes is particularly noteworthy, as it generalizes existing results and provides practical implications for neural network training. However, the paper's focus on deep linear networks may limit its applicability to real-world scenarios involving non-linear architectures. Additionally, while the empirical experiments validate the theoretical claims, the scope of experiments could be expanded to include more diverse network configurations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, making it accessible to readers with a background in optimization and neural networks. The methodology is clearly articulated, and the theoretical proofs are presented in a logical manner. The novelty of the work is significant, particularly in how it links GD convergence to network initialization and architecture. The reproducibility of the results is supported by the inclusion of numerical experiments, though further details on the experimental setup could enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper provides a valuable contribution to the understanding of convergence behavior in deep linear neural networks, with rigorous theoretical backing and empirical validation. While the findings are significant, their applicability may be somewhat limited to linear models. The clarity and quality of the writing further bolster the paper's strengths.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the convergence rates of the Gradient Descent (GD) algorithm for overparameterized deep linear neural networks (DLNNs). It establishes that the convergence behavior of GD is affected by random initialization methods, with uniform convergence rates achievable under sufficiently large hidden layer widths. The findings suggest that the depth of the network does not influence the optimal convergence rate, and that GD can effectively navigate bad saddle points in the loss landscape of overparameterized DLNNs. The methodology includes a rigorous mathematical framework to define the loss function as a convex optimization problem and the conditions for linear convergence based on various initialization strategies.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough theoretical contributions, providing a significant advancement in understanding the optimization dynamics of neural networks. The integration of empirical validation with theoretical results strengthens its claims and practical relevance. However, the focus on deep linear networks limits the generalizability of the findings to more complex, nonlinear architectures. Additionally, the emphasis on sufficient hidden layer width raises concerns about practical implementation, where such conditions may not be easily met. The paper also lacks a deeper exploration of the real-world implications of the initialization strategies discussed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its findings clearly, with a logical structure that aids comprehension. The quality of the theoretical analysis is high, and the empirical results are convincingly presented, demonstrating the effectiveness of the proposed initialization methods. The novelty of the contributions is significant, addressing a critical gap in the literature regarding GD convergence in DLNNs. However, the reproducibility may be challenged due to the specific conditions and assumptions made regarding initialization and model architecture, which may not hold in all practical scenarios.\n\n# Summary Of The Review\nThis paper makes a valuable contribution to the understanding of GD convergence in overparameterized deep linear neural networks by rigorously analyzing the impact of initialization strategies and network width. While the theoretical insights are compelling, the practical implications may be limited to specific settings, necessitating further exploration in more complex architectures.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Sharp Convergence Analysis of Gradient Descent for Overparameterized Deep Linear Neural Networks\" investigates the convergence properties of gradient descent (GD) in training deep linear networks. It emphasizes how GD can effectively navigate the non-convex loss landscape of these networks, particularly highlighting the role of initialization in achieving optimal convergence rates. The authors derive several theoretical results, including conditions for linear convergence and the behavior of GD trajectories that align closely with those of corresponding convex problems. Empirical results corroborate the theoretical findings, illustrating that larger widths in the networks lead to more favorable convergence trajectories.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by providing a detailed theoretical framework for understanding the convergence behavior of GD in overparameterized settings. The clear delineation of different initialization schemes, along with their effects on convergence rates, adds depth to the analysis. However, the focus on deep linear networks, while insightful, may limit the applicability of the findings to more complex, non-linear architectures commonly used in contemporary deep learning. Additionally, while the empirical validation supports the theoretical claims, the experiments could benefit from a broader range of datasets and more comprehensive ablation studies.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its theoretical concepts alongside empirical results. The quality of writing is high, making complex ideas accessible to the reader. The novelty lies in the rigorous exploration of convergence dynamics in overparameterized deep linear networks, a topic that has not been exhaustively addressed in prior literature. The reproducibility of the results is facilitated by the detailed description of methodologies and initialization schemes, although sharing code and datasets would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a thorough investigation into the convergence behavior of gradient descent in overparameterized deep linear networks, with strong theoretical and empirical support. While it offers valuable insights, the limited scope regarding network architectures and the need for broader empirical validation are notable considerations.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper investigates the convergence properties of gradient descent in overparameterized deep linear neural networks. The authors derive sharp rates of convergence, demonstrating that these rates are independent of the random initialization types used. They also provide a theoretical framework explaining how gradient descent can avoid bad saddle points, although the findings are primarily applicable to linear networks. The paper includes empirical evidence to support its theoretical claims but acknowledges limitations in terms of generalizability to nonlinear architectures and practical scenarios.\n\n# Strength And Weaknesses\nThe paper has several strengths, including the derivation of sharp convergence rates and the elucidation of gradient descent dynamics in deep linear networks. The independence of convergence rates from initialization types is a significant theoretical advancement. However, the focus on linear networks limits the generalizability of the findings. Additionally, while the paper provides a comprehensive literature review, it lacks depth in discussing the implications for nonlinear networks. The theoretical complexity may also pose a barrier to understanding for some readers, suggesting a need for clearer explanations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents a clear theoretical framework, making it accessible for readers familiar with the subject matter. However, some theoretical derivations are complex, which may hinder understanding. The novelty lies in the insights into initialization schemes and convergence dynamics, but the empirical validation is limited in scope, potentially affecting reproducibility and generalizability of the results.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to understanding the convergence of gradient descent in deep linear networks, offering sharp theoretical insights and some empirical validation. However, the limitations regarding generalizability to nonlinear architectures and the complexity of the theoretical content suggest that further research is needed.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Sharp Convergence Analysis of Gradient Descent for Overparameterized Deep Linear Neural Networks\" presents a comprehensive study on the convergence rates of gradient descent (GD) applied to overparameterized deep linear neural networks. The authors redefine the convergence region, elucidate the impact of random initializations, and demonstrate that the convergence rate is independent of network depth, provided the hidden layers are sufficiently wide. Utilizing advanced mathematical techniques, the paper provides a rigorous framework for understanding how GD avoids poor saddle points, and it extends the analysis to cover strongly convex loss functions. Empirical validation through numerical experiments supports the theoretical claims, reinforcing the relevance of the findings to practical applications in deep learning.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to redefining convergence regions, which challenges existing paradigms in the literature. The independence of convergence rates from initialization types and network depth represents a significant contribution to the understanding of gradient descent dynamics. The theoretical framework is robust, supported by rigorous proofs and a thorough empirical validation. However, a potential weakness lies in the focus on deep linear networks, which may not fully generalize to non-linear architectures. Additionally, while the paper offers insightful theoretical advancements, the implications for more complex network structures and their real-world applicability could be further elucidated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its key contributions. The methodology is described in sufficient detail, allowing for reproducibility, particularly in the empirical validation section. The novelty of the theoretical insights is evident, as it challenges previously held beliefs and provides a more generalized understanding of GD behavior in overparameterized settings. Overall, the paper maintains high quality in its presentation and rigor.\n\n# Summary Of The Review\nThis paper makes substantial contributions to the theoretical understanding of gradient descent in overparameterized deep linear neural networks, particularly regarding convergence rates and initialization impacts. Its innovative approach and thorough validation make it a strong candidate for acceptance.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper investigates the convergence properties of gradient descent (GD) in the context of adversarial training for deep linear neural networks. The authors propose a novel adversarial training framework specifically for overparameterized linear models and derive sharp convergence rates for GD that are independent of initialization strategies. Additionally, they explore the impact of network architecture, demonstrating that wider layers improve convergence rates while also showing that adversarial training helps avoid \"bad saddle points\" commonly encountered in standard training.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative focus on deep linear networks, which are often overlooked in adversarial training discussions, and the solid theoretical groundwork that provides new insights into the training dynamics under adversarial conditions. The empirical results support the theoretical claims, showing notable improvements in robustness against adversarial attacks. However, a significant weakness is the limited applicability of findings to more complex nonlinear models, which are prevalent in practical applications. Additionally, the discussion on real-world implications and deployment of the proposed methods is somewhat superficial.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and logically structured, making complex theoretical concepts accessible. The quality of the exposition is high, with appropriate definitions and explanations. The novelty lies primarily in the unique focus on linear networks and the specific convergence results derived. While the methodology is reproducible, further elaboration on the empirical setup and results might enhance clarity for practitioners looking to apply the findings.\n\n# Summary Of The Review\nOverall, this paper presents a significant contribution to the understanding of adversarial training in deep linear networks, offering sharp convergence results and demonstrating the potential to avoid problematic saddle points. However, the implications for more complex models and practical applications could be better addressed.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a comprehensive analysis of the convergence behavior of gradient descent (GD) in overparameterized deep linear neural networks. Its main contributions include establishing sharp convergence rates that are independent of initialization schemes, demonstrating that network depth does not affect convergence rates under certain conditions, and proposing that GD can automatically avoid poor saddle points. The authors employ a novel methodology combining random matrix theory and optimization techniques, and substantiate their theoretical findings with extensive empirical validation across various experimental setups.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its groundbreaking insights and the potential implications for simplifying the training of neural networks. The claim that convergence rates are independent of initialization is particularly compelling, as it challenges long-standing beliefs in the field and could lead to more straightforward implementations. However, the weaknesses include the need for rigorous validation of universal applicability and the potential skepticism regarding the empirical evidence, as the claims may require further scrutiny to ensure they hold across all network configurations and tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to both experts and non-experts in the field. The quality of the methodology is high, with a blend of theory and empirical validation that enhances the credibility of the findings. The novelty is significant, as it challenges existing paradigms in neural network training. However, reproducibility may be a concern if the empirical results are not readily replicable across different settings or if the theoretical claims are not sufficiently supported by the data.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for a paradigm shift in our understanding of gradient descent in deep learning, with significant theoretical and empirical contributions. While the findings are promising and could simplify neural network training, they warrant careful validation to ensure their broad applicability and robustness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the convergence rates of Gradient Descent (GD) for deep linear neural networks, focusing on the impact of initialization and network width. The authors claim to present sharp convergence rates that are independent of the type of initialization when the network layers are sufficiently wide. Key findings include the identification of faster convergence rates with orthogonal initialization compared to Gaussian initialization, and empirical results demonstrating that wider networks converge significantly faster, reducing loss more efficiently than their narrower counterparts.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous analysis of convergence rates and the thorough empirical validation of theoretical claims. The enhancement of results regarding the convergence behavior of wider networks provides practical insights that could influence initialization strategies in real-world applications. However, the paper could benefit from a more detailed discussion on the limitations of the findings, particularly regarding the generalizability of the results to non-linear networks or different activation functions. Additionally, while the empirical results are compelling, further experimental validation across diverse datasets would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its findings clearly. The methodology is sound, and the results are backed by adequate empirical evidence. However, while the novelty in convergence analysis for deep linear networks is noteworthy, the overall contribution to the broader field of deep learning may be limited due to the specific focus on linear models. The reproducibility of the experiments appears feasible, given the detailed descriptions of the experimental setup and data used.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the understanding of convergence behavior in deep linear neural networks, particularly highlighting the advantages of network width over initialization. While the results are promising and well-supported, further exploration of their implications and validations across different contexts would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the convergence behavior of gradient descent (GD) in the context of overparameterized deep linear networks. It primarily claims that the rate of convergence is independent of the random initialization method, and that deeper networks do not adversely affect convergence rates as long as the width is sufficiently large. The authors assert that all local minima in deep linear networks are global minima, and GD effectively avoids bad saddle points. The paper employs random matrix theory to analyze the loss landscape and provides empirical results to support its theoretical claims.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its ambitious exploration of the convergence properties of gradient descent in overparameterized settings and its use of random matrix theory to derive insights about the loss landscape. However, several assumptions warrant caution. The claim regarding initialization independence may overlook nuances in optimization dynamics across different initialization strategies. Additionally, the simplification of depth's impact on convergence may not hold in more complex architectures. The assumption that local minima are global minima needs further justification, particularly in non-linear networks, and the paper should offer a more robust discussion on the practical implications of its findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is generally good, but the heavy reliance on specific assumptions and mathematical constructs may hinder broader comprehension. The quality of the empirical results is commendable; however, they require more detailed exploration regarding experimental setups and variability. While the novelty of the theoretical contributions is notable, the paper does not sufficiently address how the findings might generalize to non-linear architectures. The reproducibility of results appears to be a concern, as the reliance on specific initialization methods and random matrix theory poses questions about the robustness of the findings.\n\n# Summary Of The Review\nOverall, the paper presents interesting theoretical contributions regarding the convergence of gradient descent in overparameterized deep linear networks. However, it is hindered by several strong assumptions that may limit the applicability of its findings to more complex, non-linear architectures. A more comprehensive examination of these assumptions and their implications would strengthen the paper's theoretical foundation.\n\n# Correctness\nRating: 3/5  \nWhile the theoretical framework is intriguing, the validity of several key assumptions requires further scrutiny to ensure correctness, especially in non-convex scenarios.\n\n# Technical Novelty And Significance\nRating: 4/5  \nThe paper offers a significant contribution to the understanding of convergence in overparameterized networks, although its assumptions may dilute the impact of its findings.\n\n# Empirical Novelty And Significance\nRating: 3/5  \nThe empirical results provide some support for the theoretical claims but suffer from a lack of depth in the analysis of variability and generalization to different architectures.",
    "# Summary Of The Paper\nThis paper investigates the convergence rates of gradient descent in overparameterized deep linear neural networks, presenting sharp theoretical results that highlight the efficiency of gradient descent despite the challenges posed by non-convex loss landscapes. The authors derive theorems that establish conditions for linear convergence, demonstrating that these rates are independent of initialization methods and network depth, provided the hidden layers are sufficiently wide. Empirical experiments are conducted to corroborate these theoretical findings, illustrating the convergence behavior across different network configurations and initialization schemes.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear theoretical contributions, which build upon previous literature and provide a more generalized understanding of optimization in deep linear networks. The methodology is rigorous, with a well-structured presentation of theorems and proofs that enhance the comprehension of the results. However, the paper could benefit from a deeper exploration of the implications of its findings in practical applications beyond the theoretical framework. The numerical experiments, while supportive, lack extensive discussion on the limitations and potential real-world scenarios where these results may or may not hold true.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, making complex concepts accessible to readers familiar with the field. The quality of the theoretical analysis is high, and the proofs are presented in a coherent manner. In terms of novelty, the paper offers significant contributions to the understanding of gradient descent in overparameterized settings, although the exploration of practical applicability could enhance its relevance. The reproducibility of the results appears promising, given the detailed presentation of methodologies, although a more extensive description of the experimental setup would be beneficial.\n\n# Summary Of The Review\nOverall, this paper provides valuable insights into the convergence properties of gradient descent in deep linear networks, supported by robust theoretical findings and empirical validation. While the contributions are significant, further exploration of practical implications and a more thorough discussion of the experiments could strengthen the paper's impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel framework for enhancing the robustness of deep learning models against adversarial attacks. It introduces a new training methodology that incorporates adversarially generated examples into the training process while maintaining model accuracy on clean data. The authors conduct extensive empirical evaluations across multiple datasets, demonstrating that their approach outperforms existing state-of-the-art methods in terms of both robustness and generalization.\n\n# Strength And Weaknesses\n**Strengths:**\n- The proposed methodology addresses a critical issue in the field of adversarial machine learning, which is the trade-off between robustness and accuracy.\n- The thorough empirical evaluation using various datasets strengthens the claims made by the authors, demonstrating the versatility of the approach.\n- The paper is well-organized, allowing for a clear understanding of the motivations, methods, and findings.\n\n**Weaknesses:**\n- The theoretical underpinning of the proposed method could be elaborated further; specific assumptions made in the model could limit its generalizability.\n- Some experimental comparisons are conducted only against a limited number of existing methods, which may not provide a comprehensive view of the landscape.\n- The paper lacks a detailed discussion on the computational costs associated with the proposed approach, which could be a critical factor for practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe writing is generally clear, but certain technical terms may not be well-defined for readers not familiar with adversarial training. The quality of the presentation is high, with detailed figures and tables that support the findings. The novelty of the approach is significant, as it provides a fresh perspective on integrating adversarial training without sacrificing performance on clean data. However, the reproducibility of the results could be further enhanced by providing clearer guidelines for implementation and more detailed descriptions of the experimental setups.\n\n# Summary Of The Review\nThis paper introduces a promising new framework for improving the robustness of deep learning models against adversarial attacks. While the contributions are substantial and the empirical results are compelling, the clarity of the theoretical aspects and the depth of the experimental comparisons could be improved. Overall, the work is a valuable addition to the literature with the potential to influence future research directions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Sharp Convergence Analysis of Gradient Descent for Overparameterized Deep Linear Neural Networks\" investigates the convergence rates of gradient descent (GD) for deep linear neural networks characterized by overparameterization. The authors demonstrate that all local minima in the loss landscape are, in fact, global minima, and establish that the convergence behavior of GD is largely independent of the initialization strategy, provided that the network is sufficiently deep and wide. Key findings indicate that GD efficiently avoids poor saddle points in overparameterized settings, leading to a convergence rate comparable to that of convex optimization problems.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its theoretical contributions, particularly the rigorous analysis of convergence rates and the clarification of the role of initialization strategies in deep linear networks. By leveraging insights from random matrix theory, the authors provide a solid foundation for understanding GD's performance in practice. However, a potential weakness is the focus on deep linear networks, which, while theoretically significant, may not fully capture the complexities of training nonlinear deep networks. Additionally, while the findings are compelling, the implications for practical applications in nonlinear settings remain somewhat speculative.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions, methodology, and findings. The use of mathematical rigor enhances the quality of the analysis, making the arguments easy to follow. The novelty of the work lies in its focus on overparameterized deep linear networks and the clear delineation of convergence behavior based on network architecture. As for reproducibility, the theoretical nature of the results suggests that they can be tested and verified through simulation, although empirical validation in real-world scenarios remains necessary to strengthen the claims.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the theoretical understanding of gradient descent in deep linear neural networks, effectively linking convergence properties to network architecture. While the findings are promising, further exploration of their implications for nonlinear networks would enhance the paper's impact.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 4\n\n# Empirical Novelty And Significance\nRating: 3",
    "# Summary Of The Paper\nThe paper presents a thorough analysis of the convergence rates of gradient descent (GD) in overparameterized deep linear neural networks, addressing the theoretical aspects of GD's efficiency in training these networks despite the non-convex nature of their loss surfaces. Key contributions include demonstrating that local minima are globally optimal in deep linear networks, showing that convergence rates are invariant to random initialization types, and establishing that increasing the depth of the network does not impede convergence rates when layer widths are sufficiently large. The methodology involves defining convergence regions based on initialization parameters and providing theoretical results supported by empirical evidence, showcasing the close trajectories of GD for deep linear networks to those of convex optimization problems.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear articulation of the theoretical foundations of GD in deep linear networks, which is a crucial aspect given the broader implications for training deep learning models. The results are well-supported by empirical data, reinforcing the theoretical claims made. However, a notable weakness is the paper's limited exploration of how these findings may extend to more complex, non-linear architectures, which are prevalent in modern deep learning applications. This lack of generalization could limit the paper's applicability in a broader context.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections that guide the reader through the theoretical framework and empirical results. The quality of the writing is high, and the logical flow aids in understanding the complex concepts discussed. In terms of novelty, the paper offers fresh insights into the dynamics of GD in a specific context, although the exploration of overparameterization is an established area of research. The reproducibility of the results appears strong, given the clear definitions and the empirical validation provided.\n\n# Summary Of The Review\nOverall, the paper delivers significant insights into the convergence properties of gradient descent in deep linear networks, with a solid theoretical foundation bolstered by empirical evidence. While the findings are valuable, the limited exploration of their applicability to non-linear networks presents a potential area for further investigation.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Sharp Convergence Analysis of Gradient Descent for Overparameterized Deep Linear Neural Networks\" provides a thorough investigation into the convergence behavior of gradient descent (GD) applied to deep linear neural networks. The authors establish sharp convergence rates that are independent of initialization types and network depth under specific conditions. The theoretical framework is well-structured, presenting key definitions, lemmas, and theorems that collectively demonstrate how GD can effectively avoid poor saddle points in overparameterized regimes. Empirical results are provided to support these theoretical claims, showcasing loss trajectories that align with the predicted convergence behavior.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its clear contributions to understanding the convergence dynamics of GD in deep linear networks, an area that has significant implications for the broader context of deep learning. The theoretical results are well-founded and enhance the existing body of knowledge about loss landscapes in deep networks. However, a potential weakness is that while the theoretical aspects are robust, some sections may benefit from additional clarity, particularly for readers who may not have a strong mathematical background. Additionally, while the empirical validation supports the theoretical findings, further exploration of different architectures and real-world datasets could strengthen the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and maintains a logical flow throughout, making it relatively easy to follow the complex mathematical arguments presented. The quality of writing is generally high, though some sections could be more accessible to a broader audience. The novelty of the work is significant, as it addresses an important open problem in the field of machine learning, particularly in relation to the efficiency of GD methods in training deep networks. The reproducibility of the results is supported by clear descriptions of the methodologies and empirical experiments, although providing additional details on experimental setups could enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the theoretical understanding of gradient descent in deep linear neural networks, establishing sharp convergence rates that are independent of initialization. While the theoretical and empirical results are compelling, some sections could benefit from enhanced clarity to ensure broader accessibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper investigates the sharp convergence rates of Gradient Descent (GD) for overparameterized deep linear neural networks, providing significant theoretical insights into the behavior of GD in non-convex optimization landscapes. The authors establish that local minima in these networks correspond to global minima and that the convergence rate is invariant to random initialization methods, dependent instead on the width of hidden layers. They also demonstrate that careful initialization strategies can prevent GD from converging to detrimental saddle points, ultimately establishing a framework that parallels convex optimization dynamics in high-dimensional settings.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its comprehensive theoretical analysis and clear establishment of convergence properties for GD in overparameterized deep linear networks. The use of rigorous mathematical formalism to define convergence regions and analyze convergence rates is commendable. However, a potential weakness is the reliance on specific initialization strategies, which may not generalize across all neural network architectures or optimization scenarios. Additionally, while the paper provides numerical experiments to support theoretical claims, further exploration of real-world applicability and robustness under various conditions could enhance its contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making complex theoretical concepts accessible. The quality of the mathematical derivations is high, and the logical flow from theoretical exposition to empirical validation is coherent. The novelty of the work is significant as it addresses a prominent gap in understanding GD's behavior in overparameterized settings. Reproducibility is supported through explicit definitions of initialization schemes and the mathematical framework, although the paper could benefit from more detailed descriptions of experimental setups and datasets used.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the understanding of gradient descent in the context of overparameterized deep linear neural networks. The findings on convergence properties and saddle point avoidance are both novel and significant, though further empirical validation in diverse scenarios would strengthen the claims made.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the convergence properties of deep linear neural networks, emphasizing that every local minimum is a global minimum under certain conditions. The authors argue that the depth of the network does not affect the optimal rate of convergence if the network width is sufficiently large. They provide a theoretical framework that relies on specific initialization schemes to analyze convergence regions and the avoidance of bad saddle points. However, the findings are limited by the restrictive nature of deep linear networks, which may not reflect the complexities of real-world nonlinear architectures.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical rigor, particularly in its analysis of convergence in deep linear networks. The claim that every local minimum is a global minimum offers an interesting perspective on the behavior of these models. However, significant weaknesses include the limited practical relevance of deep linear networks as \"toy models,\" the heavy reliance on specific initialization methods that may not generalize, and an oversimplification of the interaction between depth and width. Furthermore, the theoretical constructs regarding convergence may mislead readers about the applicability of these results to more complex, real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe presentation of the theoretical results is convoluted, making it challenging for readers to understand the implications without considerable effort. While the authors provide empirical experiments that support their theoretical claims, these experiments lack depth and diversity, raising questions about their reproducibility and generalizability. The novelty of the findings is questionable, given the narrow focus on deep linear networks and the lack of engagement with existing literature on optimization challenges in deep learning.\n\n# Summary Of The Review\nOverall, the paper contributes to the understanding of convergence in deep linear neural networks but suffers from significant limitations in practical applicability and relevance. The findings are constrained by oversimplifications and a lack of engagement with the complexities of real-world deep learning scenarios, which diminishes their impact.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Sharp Convergence Analysis of Gradient Descent for Overparameterized Deep Linear Neural Networks\" introduces significant advancements in the understanding of optimization in machine learning. The authors present sharp rates of convergence for gradient descent applied to deep linear networks. Key findings include the assurance that local minima are global minima, convergence rates independent of initialization, and the observation that network depth does not impede convergence when hidden layer widths are sufficiently large. The paper further establishes robust theoretical foundations by generalizing results to strongly convex loss functions and drawing connections to kernel methods.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative convergence rates and its resolution of open theoretical questions in the training of deep networks. The findings provide practical implications for efficient training strategies, enhancing the robustness and stability of gradient descent methods. However, the focus on deep linear networks may limit the direct applicability of the results to more complex non-linear networks. Additionally, while the empirical evidence supports the theoretical claims, further exploration into diverse scenarios and real-world applications would strengthen the impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, making complex concepts accessible. The quality of the writing and the presentation of numerical experiments reinforce the findings effectively. The novelty of the research is significant, offering fresh insights into the optimization landscape of deep learning. Reproducibility is addressed through thorough documentation of methodologies, though additional details regarding the experimental setup could enhance clarity for replication purposes.\n\n# Summary Of The Review\nThis paper represents a substantial contribution to the theoretical understanding of optimization in deep linear neural networks, with implications for practical training methods. Its findings are well-supported by empirical evidence and offer a foundation for future research in both linear and non-linear architectures. Overall, the paper is a commendable advancement in the field of machine learning.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper offers a comprehensive theoretical analysis of the convergence rates of gradient descent (GD) in the context of overparameterized deep linear neural networks. The authors establish that despite the non-convex nature of the loss function, all local minima correspond to global minima, thus presenting a novel perspective on the influence of network depth and width on convergence rates. Key findings include the characterization of convergence regions based on initialization strategies and network parameters, as well as the demonstration of linear convergence rates akin to those found in convex optimization. The paper also provides insights into how GD trajectories align with those in convex optimization and the role of initialization in avoiding bad saddle points.\n\n# Strength And Weaknesses\nStrengths of the paper include its rigorous theoretical framework that bridges concepts from convex optimization to the behavior of gradient descent in deep networks. The clear delineation of the role of initialization and the mathematical characterization of convergence regions represent significant contributions to the field. However, the paper lacks empirical validation of its theoretical claims, which may limit the practical applicability of its findings. Additionally, the focus on deep linear networks, while insightful, may not fully capture the complexities of real-world deep learning scenarios involving non-linear architectures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical contributions, making it accessible to readers with a solid understanding of optimization and neural networks. The quality of the mathematical exposition is high, with thorough derivations and explanations. The novelty lies in the theoretical insights regarding overparameterization and convergence dynamics, though the lack of empirical experiments limits the reproducibility of the findings in practical settings. Overall, the clarity and quality of the presentation are commendable.\n\n# Summary Of The Review\nThis paper provides a rigorous theoretical framework for understanding gradient descent in deep linear neural networks, revealing important insights into convergence behavior and the role of initialization. While the theoretical contributions are significant and well-articulated, the absence of empirical validation may restrict the broader applicability of the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Sharp Convergence Analysis of Gradient Descent for Overparameterized Deep Linear Neural Networks\" focuses on analyzing the convergence rates of Gradient Descent (GD) applied to deep linear neural networks, particularly under various initialization schemes. The authors provide sharp convergence rates for different types of random initializations, specifically Gaussian, orthogonal, and a special balanced initialization. Key theoretical results include theorems that characterize the convergence behavior of GD for each initialization type, with the learning rate being a critical factor for ensuring convergence. Although the paper includes empirical experiments that support its theoretical claims, it lacks detailed information on code or datasets, which limits reproducibility.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its rigorous theoretical analysis, which provides clear and sharp convergence rates for GD under various initialization types. This contributes valuable insights into the optimization landscape of deep linear networks. However, the paper has notable weaknesses, including a lack of practical implementation details that would allow readers to reproduce the results. Additionally, the paper does not adequately contextualize the implications of its findings for broader applications, particularly in the realm of nonlinear networks, which may limit its impact on the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with well-structured sections and clearly stated theorems and lemmas. The quality of the theoretical contributions is strong; however, the novelty is somewhat limited as it primarily focuses on a specific class of models (deep linear networks) without exploring their implications for more complex architectures. The reproducibility of the findings is a significant concern, as the paper does not provide access to code or datasets, making it challenging for other researchers to validate the empirical results.\n\n# Summary Of The Review\nOverall, the paper presents a solid theoretical contribution to the understanding of convergence rates for Gradient Descent in deep linear neural networks. However, its practical implications are weakened by the lack of reproducibility and contextualization within broader machine learning frameworks.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper investigates the convergence rates of gradient descent (GD) in deep linear neural networks, claiming to provide sharper rates than those previously established in the literature. The authors generalize findings from earlier works, particularly focusing on the effects of initialization schemes and the automatic avoidance of bad saddle points. However, the methodology does not present substantial new insights, as it builds significantly on prior studies without offering a clear differentiation from them.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its attempt to clarify convergence behavior in deep linear networks and its reference to notable prior works, providing a broad context for its claims. However, it suffers from a lack of novelty, as it heavily relies on existing literature for its foundational claims. The comparisons made to earlier research, such as the work of Kawaguchi (2016) and Hu et al. (2020), appear selective and fail to recognize the complexities and nuances that those studies introduced. Additionally, the empirical results are undermined by insufficient comparative analysis against established benchmarks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in its presentation; however, it lacks a critical evaluation of how its findings extend beyond previous work, which diminishes its perceived novelty. The quality of the experimental methodology raises concerns due to insufficient rigor in comparisons with existing results. The reproducibility of the results is not explicitly addressed, which is a crucial aspect for empirical studies in this domain.\n\n# Summary Of The Review\nWhile the paper presents results on the convergence of GD for deep linear networks, it struggles to establish its novelty and significance in light of previous works. The reliance on earlier studies without adequate differentiation leads to an overstated impression of its contributions.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Sharp Convergence Analysis of Gradient Descent for Overparameterized Deep Linear Neural Networks\" presents a thorough investigation into the convergence properties of gradient descent (GD) applied to overparameterized deep linear neural networks. The authors develop new theoretical insights into the conditions under which GD exhibits sharp convergence behavior. Methodologically, they employ advanced mathematical analyses, including proofs that delineate the relationship between network architecture and convergence rates. The findings suggest that overparameterization significantly enhances the convergence speed of GD, offering a clearer understanding of the dynamics at play in deep learning models.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its rigorous theoretical framework, which provides valuable insights into the convergence properties of GD in the context of overparameterized networks. The authors present a clear and logical progression of arguments, supported by well-structured proofs. However, the paper also has notable weaknesses. There are several instances of inconsistent notation, typographical errors, and formatting issues that detract from the overall clarity and professionalism. Additionally, while the theoretical contributions are significant, empirical validation of the findings appears limited, raising questions about the practical implications of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by various formatting inconsistencies and typographical errors, such as misuses of punctuation and inconsistent notation for mathematical expressions. While the theoretical contributions are novel and significant, the overall quality is affected by these issues. The reproducibility of the results may be challenged by the lack of clear numerical examples and the inconsistent presentation of mathematical proofs. To enhance clarity and reproducibility, the authors should address the minor issues mentioned in the review notes, ensuring a more polished presentation of their findings.\n\n# Summary Of The Review\nOverall, the paper offers a substantial contribution to the understanding of GD in overparameterized deep linear neural networks through its rigorous theoretical insights. However, its impact is somewhat diminished by formatting and clarity issues, which the authors should address to improve the professionalism of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel sharp convergence analysis for gradient descent applied to deep linear networks. The authors establish theoretical results demonstrating improved convergence rates compared to existing literature, specifically focusing on a single loss function. The findings suggest that the convergence behavior is highly dependent on network architecture, leading to implications for the design of deep learning systems. However, the authors do not explore the applicability of their results to non-linear networks or practical scenarios, which limits the generalizability of their conclusions.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its rigorous theoretical framework that provides insights into convergence rates for deep linear networks, which is a significant contribution to the understanding of optimization in deep learning. However, the paper has notable weaknesses, including a lack of exploration into the implications of its findings on non-linear networks, which remain an open question. Additionally, the focus on a specific loss function restricts the applicability of the results, and there is minimal discussion on how these findings translate to practical applications in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and findings. The theoretical results are presented in a structured manner, facilitating comprehension. However, the novelty of the work is somewhat limited, as it primarily addresses linear networks without sufficient discussion on extensions to other architectures. Reproducibility could be enhanced by including practical experiments with real datasets to validate the theoretical insights provided.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the understanding of convergence behavior in deep linear networks, but it falls short in addressing broader implications and applications of its findings. Future research should consider extending the analysis to non-linear networks and varying conditions to enhance the relevance of the results.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a thorough analysis of the convergence behavior of gradient descent (GD) in the context of overparameterized deep linear neural networks, emphasizing the impact of different random initialization techniques on convergence rates. The authors establish that convergence rates are independent of the specific initialization type, leading to the identification of a convergence region based on initialization parameters. Utilizing probabilistic methods, particularly concentration inequalities, the paper provides statistical guarantees for convergence behavior and supports its theoretical claims with empirical results that demonstrate the predicted convergence trends.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous approach to convergence analysis, the establishment of probabilistic guarantees, and the comprehensive presentation of theoretical results through lemmas and theorems. The use of various initialization schemes and their impact on convergence is well-articulated, enhancing the practical relevance of the findings. However, the methodology may be seen as limited due to its focus on linear networks, which might restrict the generalizability of the results to more complex architectures. Furthermore, while the empirical results are supportive, additional experiments on real-world datasets could strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents its findings, making the theoretical concepts accessible. The quality of the writing is high, with rigorous definitions and thorough explanations of lemmas and theorems. Novelty is present in the probabilistic approach taken to analyze convergence in deep linear networks; however, the exploration of overparameterization in neural networks is a growing area of research. The reproducibility of the results is facilitated by the clear presentation of methodologies and the inclusion of empirical data, although code or detailed experimental setups are not provided.\n\n# Summary Of The Review\nOverall, the paper provides significant insights into the convergence behavior of gradient descent in overparameterized deep linear neural networks, backed by strong theoretical and empirical evidence. While the findings are relevant and contribute to the understanding of neural network training dynamics, the scope could benefit from broader application beyond linear models.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a theoretical analysis of convergence properties for gradient descent in deep linear networks. It focuses on specific initialization schemes, such as Gaussian and orthogonal initializations, to establish convergence regions where bad saddles can be avoided. However, the analysis is limited to deep linear architectures, lacking a comprehensive approach to more complex nonlinear networks. The paper suggests that the defined convergence region may not be optimal and calls for further exploration of network configurations and initialization methods.\n\n# Strength And Weaknesses\nThe main strength of the paper is its focused theoretical contribution to understanding convergence in deep linear networks, particularly in relation to specific initialization schemes. However, it has several weaknesses: it does not extend its analysis to nonlinear networks, which are more prevalent in practice; it lacks empirical validation of its theoretical claims; it does not consider practical training scenarios, such as batch training or data noise; and it fails to address the complexity and overparameterization issues that could impact generalization. The study's scope is narrow, limiting its applicability in real-world settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology clearly. However, the novelty is somewhat constrained by the focus on linear architectures, limiting its broader impact. The reproducibility of results is hampered by the lack of empirical validation and a detailed exploration of the proposed methods in varied practical contexts. The absence of a comprehensive convergence analysis for nonlinear networks further detracts from the paper's clarity regarding its applicability.\n\n# Summary Of The Review\nWhile the paper makes a notable theoretical contribution to understanding convergence in deep linear networks, its limitations in scope, lack of empirical validation, and failure to address practical training considerations significantly detract from its overall impact. Future research should aim to explore these aspects to enhance the relevance of the findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Sharp Convergence Analysis of Gradient Descent for Overparameterized Deep Linear Neural Networks\" presents an analysis of the convergence rates of gradient descent applied to deep linear networks. The authors argue that they provide \"sharp rates of convergence\" and emphasize the significance of initialization strategies, such as Gaussian and orthogonal initializations, on the convergence behavior. Their findings suggest that the worst-case convergence rates for deep linear networks are comparable to those observed in convex optimization problems, although the practical implications for nonlinear networks remain unexplored.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its systematic approach to analyzing convergence rates, which is framed around well-established principles of convex optimization. However, it suffers from significant weaknesses, including a lack of originality and insufficient exploration of more complex, nonlinear network architectures. The insights offered regarding initialization and convergence feel redundant, as these concepts are already well-documented in existing literature. Consequently, the paper does not make a substantial contribution to the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is articulated in a clear and structured manner, the excessive use of technical jargon may obfuscate its findings for some readers. The quality of the mathematical exposition is generally sound, but the novelty is quite limited, as the results largely reiterate established knowledge. Reproducibility is achievable given the straightforward nature of the experiments and methodologies discussed, although the lack of novel insights may deter interest in replication.\n\n# Summary Of The Review\nOverall, this paper rehashes well-known concepts in optimization and deep learning without offering significant new insights or advancements. While the clarity and presentation are commendable, the lack of originality and depth in exploring more complex models detracts from its overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper investigates the efficiency of gradient descent in training deep linear neural networks, providing a detailed analysis of convergence behavior based on various initialization strategies. The authors demonstrate that the depth of the network does not significantly affect the optimal rate of convergence when the width is sufficiently large, and they highlight the automatic avoidance of bad saddle points in overparameterized settings. The findings suggest that these insights may be extended to nonlinear neural networks, proposing potential avenues for improving initialization methods and training algorithms in deep learning.\n\n# Strength And Weaknesses\nThe paper presents several valuable contributions, particularly in its thorough examination of convergence rates and the role of initialization in deep linear networks. The exploration of the loss landscape and the implications of random initialization provide a solid foundation for further research. However, the paper could benefit from a more extensive empirical validation of its theoretical claims, particularly regarding the applicability of its results to nonlinear networks. Additionally, the proposed experiments on different initialization methods could enhance the robustness of its conclusions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making complex concepts accessible. The quality of the writing is high, with a logical flow that guides the reader through the theoretical discussions. While the novelty of the work lies in its focus on linear networks, the potential applications to nonlinear networks remain largely speculative without empirical backing. The reproducibility of the findings could be strengthened by providing more detailed methodologies for the proposed experiments.\n\n# Summary Of The Review\nOverall, this paper offers significant insights into the convergence dynamics of deep linear neural networks and emphasizes the importance of initialization strategies. While it lays a strong foundation for future research, it could be improved with additional empirical validation and exploration of its implications for nonlinear architectures.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper investigates the convergence behavior of gradient descent (GD) in deep linear neural networks, focusing on how different initialization strategies—namely Gaussian, orthogonal, and special balanced initialization—affect performance metrics. The authors present empirical results showing that with sufficiently wide hidden layers, GD converges to a global minimum at rates comparable to those observed in convex optimization problems, regardless of the chosen initialization. The experiments demonstrate that as network depth increases, wider networks exhibit loss trajectories closely aligned with those of convex problems, suggesting increased stability and efficiency during training due to overparameterization.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough empirical analysis, which supports its theoretical claims about the effects of initialization and network architecture on convergence rates. The use of various initialization strategies provides a comprehensive view of their impact on GD performance, and the presented tables effectively highlight the variations in convergence based on network size and initialization type. However, the paper could benefit from a more in-depth exploration of the underlying mechanisms that lead to the observed convergence behavior, as well as a discussion on the implications of these findings for practical applications in deep learning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, making it accessible to readers. The quality of the empirical results is high, as they are backed by rigorous experimentation. In terms of novelty, while the exploration of convergence in deep linear networks is a valuable contribution, the concepts of initialization and overparameterization have been studied previously. Nevertheless, the specific focus on how these factors influence convergence rates presents a unique angle. The reproducibility of the results is supported by the clear presentation of experimental setups and results.\n\n# Summary Of The Review\nOverall, the paper provides a valuable empirical contribution to the understanding of convergence in deep linear neural networks under various initialization strategies. While the findings are significant and well-supported, further exploration of the underlying mechanisms and practical implications would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to analyzing neural networks through the lens of the Neural Tangent Kernel (NTK). The authors propose a methodology that integrates gradient descent (GD) dynamics with NTK to provide insights into the convergence behavior of deep learning models. The findings suggest that the proposed framework not only improves understanding of optimization landscapes but also enhances predictive performance on benchmark tasks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative integration of NTK with gradient descent to elucidate training dynamics, which is a significant contribution to the field of deep learning theory. The empirical results demonstrate that the proposed method yields improved generalization performance, supporting its practical relevance. However, the paper is hindered by clarity issues, including dense technical language and inconsistent referencing, which may limit accessibility to a broader audience. Additionally, the empirical evidence lacks sufficient context, making it challenging to fully grasp the implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper's contributions are technically significant, the clarity and quality of the writing detract from its impact. The use of technical jargon and lengthy sentences complicates comprehension, and the inconsistent referencing style undermines the paper's professionalism. The novelty of integrating NTK with GD is commendable, yet the lack of detailed methodological explanations and the disjointed presentation of numerical experiments raise concerns about reproducibility. Improved organization and clearer exposition would enhance the paper's overall quality.\n\n# Summary Of The Review\nIn summary, the paper offers a noteworthy contribution to understanding neural network training dynamics through the integration of NTK and GD, demonstrating improved performance on benchmark tasks. However, clarity issues, inconsistent referencing, and a lack of contextualization in empirical evidence limit its accessibility and reproducibility. Addressing these concerns could significantly strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -1.8343911954025074,
    -1.6370545423967242,
    -1.7221645344587828,
    -1.57835060848199,
    -1.7353181041535115,
    -1.8146616427332662,
    -1.8513875987499515,
    -1.6628406811872436,
    -1.7077858168866398,
    -1.7100096699718716,
    -1.636212862671191,
    -1.4466479473933123,
    -1.552485899357722,
    -1.642228228290978,
    -1.6084981439221817,
    -1.7223689061884657,
    -1.7374410912020062,
    -1.8013030153844958,
    -1.5533734746165386,
    -1.5556157388137086,
    -2.1196033932805425,
    -1.629806794574606,
    -1.6003689708696134,
    -1.6281286952923022,
    -1.7717580056150235,
    -1.794124288730667,
    -1.6860834166060443,
    -1.6143977219573185,
    -1.6342052251424304
  ],
  "logp_cond": [
    [
      0.0,
      -1.7444378489863286,
      -1.7590022441899393,
      -1.7435190953539652,
      -1.7608812135241936,
      -1.7600661720621005,
      -1.7697949426789008,
      -1.7618814500759477,
      -1.753425009902507,
      -1.7533133202908953,
      -1.753639551075874,
      -1.7785876847381428,
      -1.7666832608495384,
      -1.748240326301864,
      -1.7563494570076301,
      -1.7570082193657655,
      -1.7571362794097378,
      -1.7632559564443426,
      -1.7559219191463669,
      -1.7452993202787122,
      -1.749432290558301,
      -1.7632811326393394,
      -1.7581429422289716,
      -1.7572617504403587,
      -1.755958096417684,
      -1.744513405271678,
      -1.7605891470375048,
      -1.736237741022436,
      -1.7794009688598014
    ],
    [
      -1.2465651718512025,
      0.0,
      -1.2498987182451042,
      -1.2290921836771798,
      -1.1852033280202219,
      -1.1760573226900992,
      -1.2849647550727146,
      -1.2586888552293776,
      -1.2623983123428921,
      -1.2941990302440107,
      -1.2088126316641659,
      -1.3229708640014761,
      -1.2782637373511068,
      -1.1923718799139393,
      -1.2108186464483237,
      -1.2042528471477807,
      -1.2585461637134479,
      -1.2514538533051511,
      -1.2272809439911507,
      -1.252290356748041,
      -1.221014962621065,
      -1.2718409028187108,
      -1.2748951153636336,
      -1.2078970536441604,
      -1.269802384089189,
      -1.2393945193623197,
      -1.2619930530825985,
      -1.251143379973664,
      -1.3028934651029083
    ],
    [
      -1.4316735232679783,
      -1.3478661740353381,
      0.0,
      -1.364413420010511,
      -1.3075653708664465,
      -1.3319059943190055,
      -1.4236697436627657,
      -1.3592033278348903,
      -1.4019321083933924,
      -1.2918387688460662,
      -1.3832888539922932,
      -1.4552627808498326,
      -1.3471405225001114,
      -1.3823641218766383,
      -1.34419880239978,
      -1.3332632813920149,
      -1.3335477164911145,
      -1.4185793671433105,
      -1.3369015157873818,
      -1.3445723963368457,
      -1.3564562677335703,
      -1.40415185060203,
      -1.4685571958258017,
      -1.3800071488063166,
      -1.3977408086081127,
      -1.3640357446237823,
      -1.291655312226875,
      -1.3944594002559212,
      -1.4479837514675413
    ],
    [
      -1.254775598233589,
      -1.2314734776998428,
      -1.2232058089455107,
      0.0,
      -1.2608235896252313,
      -1.2008395371539982,
      -1.2323006547234336,
      -1.2660207721970513,
      -1.252845549732492,
      -1.2565289508575486,
      -1.2126678859571638,
      -1.2738168314680025,
      -1.2640778335976244,
      -1.180658649796966,
      -1.1915107013576485,
      -1.195556306507554,
      -1.247850800739323,
      -1.2415667813976459,
      -1.2423888529064193,
      -1.2096350849429984,
      -1.235387496617972,
      -1.201721215709714,
      -1.2379276503084802,
      -1.1942586608553234,
      -1.2631933281253611,
      -1.19633143952799,
      -1.2473083917514407,
      -1.2282480305562875,
      -1.294891520198323
    ],
    [
      -1.3991047502956582,
      -1.207863251875761,
      -1.2525507606719088,
      -1.2981135311442096,
      0.0,
      -1.2358442712375146,
      -1.3167653108492732,
      -1.2705826644281937,
      -1.346870727832714,
      -1.2577153204114098,
      -1.336339335347812,
      -1.400825818037396,
      -1.3214339500114622,
      -1.3066526043508613,
      -1.2397872679972457,
      -1.2905671154924203,
      -1.2899967926839817,
      -1.3459184458542528,
      -1.3222723107138943,
      -1.2891785846405879,
      -1.2868389288336495,
      -1.3533493870868705,
      -1.3440728047132622,
      -1.2849790951409399,
      -1.3473697790918233,
      -1.3449218006089618,
      -1.28190741264049,
      -1.3840733595177384,
      -1.4289610812982931
    ],
    [
      -1.4678172902076285,
      -1.2921587117362598,
      -1.3948420273285427,
      -1.3811580673059958,
      -1.3238939943268937,
      0.0,
      -1.4188343889647217,
      -1.3257333617893876,
      -1.3738286414575407,
      -1.3675017372280973,
      -1.3222426013036126,
      -1.496892992204341,
      -1.3175336696862028,
      -1.3452750225758319,
      -1.2785456847385135,
      -1.3431169012412523,
      -1.4352265051610562,
      -1.2886282526265334,
      -1.4079887765734798,
      -1.4130766052678576,
      -1.3969225192369794,
      -1.3739336959021329,
      -1.4355551146694439,
      -1.353191298944998,
      -1.4509414291701592,
      -1.3886997652661035,
      -1.420542223956068,
      -1.4134815164909,
      -1.5152850714214607
    ],
    [
      -1.5393077758761409,
      -1.3990343545070536,
      -1.4924284811449904,
      -1.4670467169843227,
      -1.4177344654536046,
      -1.4136536995051772,
      0.0,
      -1.4810123464581986,
      -1.436107541842377,
      -1.4730082844285948,
      -1.4411298214819377,
      -1.458024393661836,
      -1.491345936327807,
      -1.4619050475191233,
      -1.442448840440358,
      -1.4468836794021975,
      -1.4725337485696142,
      -1.480408516828589,
      -1.4740185928467402,
      -1.4921068677179201,
      -1.4586662949614189,
      -1.4524783487292745,
      -1.4701139505473055,
      -1.4285519963883506,
      -1.4656528129956194,
      -1.4564503881487822,
      -1.459700097574173,
      -1.469071061020172,
      -1.477231534056441
    ],
    [
      -1.356634844160942,
      -1.274289894009071,
      -1.3404791524448953,
      -1.3269155226157934,
      -1.2998631097694415,
      -1.280296657999433,
      -1.3397620132992933,
      0.0,
      -1.3104979160615107,
      -1.3045074161495265,
      -1.3076066183814365,
      -1.403976286285826,
      -1.3302665925501558,
      -1.3138330264645872,
      -1.3150282540743023,
      -1.3066820375995298,
      -1.3639425067638637,
      -1.3270383722805055,
      -1.3096464684211147,
      -1.345517797263848,
      -1.27340028867116,
      -1.319061063123106,
      -1.3467661922558398,
      -1.2873073518056368,
      -1.3169272742815217,
      -1.3113184325047085,
      -1.3409533646957892,
      -1.317954120452559,
      -1.3888941203545
    ],
    [
      -1.3293878076595782,
      -1.2945605455176061,
      -1.399257544571692,
      -1.332621824514069,
      -1.3442416154560501,
      -1.3261859113812005,
      -1.3321702397956805,
      -1.3250521179744976,
      0.0,
      -1.4162127393805148,
      -1.3256316023350396,
      -1.4050986164071169,
      -1.3627285338324535,
      -1.3157644026727715,
      -1.3662936448008431,
      -1.3582012901073064,
      -1.3642466353318414,
      -1.3456830260704722,
      -1.3696720909523092,
      -1.3416998012603263,
      -1.275581580160144,
      -1.3598511652053409,
      -1.3528299244805377,
      -1.3061309439433675,
      -1.3645845223604387,
      -1.317761855605222,
      -1.3746344735273346,
      -1.2386917092542562,
      -1.386922130589013
    ],
    [
      -1.5042454866916843,
      -1.4416034370105137,
      -1.474637330228438,
      -1.5000319925854202,
      -1.4589267888650834,
      -1.497912229812781,
      -1.5057861539016704,
      -1.4401824220651296,
      -1.5276678904358711,
      0.0,
      -1.4679678456727674,
      -1.5240477985484853,
      -1.479098140350485,
      -1.4438047614291443,
      -1.480019293272524,
      -1.470072735482206,
      -1.4270289730340853,
      -1.4903826407516978,
      -1.4684394608281415,
      -1.5314417617824583,
      -1.4866424315483835,
      -1.5208665408395297,
      -1.5186568041849977,
      -1.4961960974050128,
      -1.4883624580156263,
      -1.5104856985929587,
      -1.465219240485162,
      -1.4646397604210728,
      -1.5057173567585596
    ],
    [
      -1.2885784610862172,
      -1.2401926447653198,
      -1.2922805578860077,
      -1.262690657307672,
      -1.3008188886082026,
      -1.2654383323648564,
      -1.292881477196347,
      -1.2761085563472994,
      -1.3107171854020334,
      -1.268172091219858,
      0.0,
      -1.351247643648005,
      -1.3292196148347404,
      -1.2361504826664325,
      -1.2537144890712322,
      -1.23548123577775,
      -1.3098367192508622,
      -1.2762940557759823,
      -1.3064214694940648,
      -1.277313240683336,
      -1.2632420058961567,
      -1.2827206130372772,
      -1.3209863589228368,
      -1.2573749041956281,
      -1.3287368319226491,
      -1.3044760158036,
      -1.3370027914361557,
      -1.2667630696196162,
      -1.3315674219957303
    ],
    [
      -1.1787290654752332,
      -1.169555374976452,
      -1.1868008810900588,
      -1.181564049613109,
      -1.182964106936053,
      -1.1712873836059081,
      -1.1747647550536109,
      -1.1644754748227946,
      -1.1671947568709666,
      -1.202374738974336,
      -1.170111492952933,
      0.0,
      -1.1944050614406745,
      -1.1808529463146444,
      -1.1843686381885061,
      -1.1859643540572051,
      -1.147538075360846,
      -1.1998475904923196,
      -1.1781550528931242,
      -1.2087176720547506,
      -1.1494294352741898,
      -1.174156685198344,
      -1.1732607931644512,
      -1.168409768913087,
      -1.1825806981389102,
      -1.1726734693384635,
      -1.1890302097639796,
      -1.151257442199852,
      -1.1709280452024962
    ],
    [
      -1.2504977122341039,
      -1.1726969929678719,
      -1.2159445104945166,
      -1.2142066487662195,
      -1.1893637783858368,
      -1.1510776416433999,
      -1.2430590381962896,
      -1.168154431067095,
      -1.2405743952248383,
      -1.188209380329641,
      -1.2081922359480508,
      -1.2902313887260086,
      0.0,
      -1.1826979950471632,
      -1.1585079574661994,
      -1.1652858766089011,
      -1.2531218426107202,
      -1.194305080337829,
      -1.2503472576065213,
      -1.2049628529073735,
      -1.1901492887487384,
      -1.2031817183988796,
      -1.2356184424264154,
      -1.2022940867425005,
      -1.2404108509739626,
      -1.137831513733466,
      -1.221920444920198,
      -1.2078629197494468,
      -1.2954010169122945
    ],
    [
      -1.3101005135752293,
      -1.2365979199972337,
      -1.30232212618563,
      -1.2539765405585814,
      -1.282976342334826,
      -1.238729596564801,
      -1.310389093256134,
      -1.3090526220323064,
      -1.3190297283819468,
      -1.2749526184545452,
      -1.2601850529532632,
      -1.3521259197065238,
      -1.3275445033668873,
      0.0,
      -1.300641137103937,
      -1.196008018814839,
      -1.294538599906913,
      -1.3025276214624542,
      -1.2227137415960658,
      -1.319953014566767,
      -1.2920589708608674,
      -1.2982325837185797,
      -1.3111889998066788,
      -1.2302597034737304,
      -1.3142751365294134,
      -1.3091517824708763,
      -1.29475317951081,
      -1.2002767329231552,
      -1.371907773274045
    ],
    [
      -1.325327928990004,
      -1.2061797556552483,
      -1.2852302128428696,
      -1.2447373062769356,
      -1.2323322762145794,
      -1.1710986906814045,
      -1.2565054182211512,
      -1.23274687142264,
      -1.2819033733139034,
      -1.272351835535973,
      -1.2689676738442803,
      -1.3188096177478035,
      -1.2180313693829634,
      -1.2541185883982393,
      0.0,
      -1.240761880816247,
      -1.3239758211141461,
      -1.190602590588328,
      -1.2866723483568265,
      -1.2324350945581604,
      -1.2825668517818132,
      -1.2381624630083754,
      -1.3001414237187154,
      -1.2411985275041835,
      -1.3027684177203351,
      -1.2503127873615685,
      -1.2323661753834754,
      -1.2650578000217834,
      -1.3486220239694824
    ],
    [
      -1.4018966728266218,
      -1.3002039992994345,
      -1.3610841801794509,
      -1.3452831192689414,
      -1.3770963586428684,
      -1.3220654101565361,
      -1.376378574188451,
      -1.361566771869448,
      -1.4209927221074947,
      -1.3538806453582346,
      -1.3254441186294381,
      -1.4171095142033454,
      -1.3353113394606506,
      -1.2576561549473722,
      -1.355909379915642,
      0.0,
      -1.3253802178249094,
      -1.4023818007357902,
      -1.2864376649679146,
      -1.3943570384083448,
      -1.3386695087832021,
      -1.3763798552654443,
      -1.4112128195524343,
      -1.3557785429875642,
      -1.3827769105170835,
      -1.386110797801162,
      -1.3933638092208336,
      -1.3798619346165897,
      -1.4498424950472129
    ],
    [
      -1.4142561309669919,
      -1.3119572121405927,
      -1.3742896833659974,
      -1.3424208786037533,
      -1.364540665252257,
      -1.32736535973226,
      -1.3874222455984353,
      -1.3697763544047055,
      -1.379983158308412,
      -1.316528708778422,
      -1.3319264255581829,
      -1.4308063949652046,
      -1.3795411510017435,
      -1.3157039311738843,
      -1.3859965807865888,
      -1.3190154432585026,
      0.0,
      -1.3469088426462597,
      -1.3169759073779086,
      -1.3986708433843333,
      -1.3628538559305627,
      -1.3866973863891503,
      -1.3528002461784459,
      -1.3941165452648558,
      -1.3222086669852189,
      -1.352015154546148,
      -1.3560199913539204,
      -1.3789747667766077,
      -1.4158503340043895
    ],
    [
      -1.4783315133270443,
      -1.3636991909136333,
      -1.4183546892996135,
      -1.3795632762722794,
      -1.418602664086201,
      -1.3179927845501536,
      -1.422763407784648,
      -1.3874771878661887,
      -1.44447838458756,
      -1.3969109955930656,
      -1.4042695190737895,
      -1.4636309362811597,
      -1.4091177574371045,
      -1.3482427923285607,
      -1.349722891335383,
      -1.4051196728484399,
      -1.425632927681182,
      0.0,
      -1.4428425311330517,
      -1.3985581090595416,
      -1.4321541099332817,
      -1.4223581776222194,
      -1.412811848319831,
      -1.4176293570653054,
      -1.448627278416229,
      -1.3783020301741933,
      -1.411241932571502,
      -1.4315605027707528,
      -1.4885332707167713
    ],
    [
      -1.223839098309668,
      -1.1471831528578038,
      -1.1613584161808617,
      -1.167990481056747,
      -1.1726941498223082,
      -1.1101303034323906,
      -1.2020592126649032,
      -1.202942878579181,
      -1.1977176115925243,
      -1.2186663369546196,
      -1.1786951702555497,
      -1.243988764125745,
      -1.1543893681201391,
      -1.0886553079085814,
      -1.224730146648103,
      -1.0792907385825428,
      -1.1515244678064256,
      -1.19061166434024,
      0.0,
      -1.2501485431232642,
      -1.166870505527223,
      -1.2141222397157223,
      -1.2172134323593382,
      -1.1516491289147242,
      -1.164147885521393,
      -1.1490146297724773,
      -1.140585965607925,
      -1.1663072924117104,
      -1.244634921232904
    ],
    [
      -1.214929556353081,
      -1.1849695768745363,
      -1.2175913134254774,
      -1.1844055691993371,
      -1.1805483692213639,
      -1.223182308196246,
      -1.2523722110910103,
      -1.267527673084495,
      -1.2085823315093156,
      -1.2344754931201611,
      -1.2428575086172329,
      -1.312837086986268,
      -1.2259658345837996,
      -1.2517568186188424,
      -1.1478033517994604,
      -1.249593598440574,
      -1.2483052497340195,
      -1.221496361666576,
      -1.2831149482011028,
      0.0,
      -1.2019786436895947,
      -1.225058856895438,
      -1.2095011625382817,
      -1.2071262660737891,
      -1.2328557676656702,
      -1.1615347677446515,
      -1.223781360353587,
      -1.2169442243053545,
      -1.3081506969612766
    ],
    [
      -1.697222176119751,
      -1.6888210904905063,
      -1.757916133589289,
      -1.7414656932334243,
      -1.7297937847973526,
      -1.6874407562508569,
      -1.7068301122679879,
      -1.6690594565731096,
      -1.7222485783853207,
      -1.7488129188212886,
      -1.7268904596332528,
      -1.7606513672058948,
      -1.751051973183393,
      -1.753404016423824,
      -1.7644241525020565,
      -1.7135668432781799,
      -1.6816844836041207,
      -1.7152171562770675,
      -1.7290107564752322,
      -1.7707079281326181,
      0.0,
      -1.7243283942751062,
      -1.745721829535015,
      -1.7309550333822947,
      -1.715420698045719,
      -1.7399048120004097,
      -1.7324469446687507,
      -1.7406037384584252,
      -1.783043431153371
    ],
    [
      -1.3158264295378297,
      -1.3027290662724023,
      -1.3266219223680447,
      -1.2595925551648965,
      -1.3133329147822486,
      -1.2396015338213382,
      -1.3277879390388607,
      -1.327360771450142,
      -1.3320306801955673,
      -1.3502571916831323,
      -1.2926484442567268,
      -1.351704563608105,
      -1.2725650279927696,
      -1.3055284421971294,
      -1.2402231782312427,
      -1.2954828135772154,
      -1.2966709186961907,
      -1.3081134881421248,
      -1.33037487295383,
      -1.2909214053369802,
      -1.3184573657246526,
      0.0,
      -1.284111016543577,
      -1.3212420032581431,
      -1.3300244916192052,
      -1.250239902169605,
      -1.3310399627913025,
      -1.3212786845840776,
      -1.3149909610276813
    ],
    [
      -1.318299653406546,
      -1.2842298153500544,
      -1.335948504583293,
      -1.3053141005837363,
      -1.3030789773721905,
      -1.275090285007837,
      -1.3227499586738438,
      -1.292985353565992,
      -1.305033039140526,
      -1.3164678002573689,
      -1.3086590027469114,
      -1.3233171150218672,
      -1.303854396874078,
      -1.3065077346820744,
      -1.3190471849970975,
      -1.3071822889373719,
      -1.2841946872410026,
      -1.2769716146687367,
      -1.337230294451955,
      -1.2474739615775083,
      -1.2896303264991165,
      -1.2744782153861622,
      0.0,
      -1.2983294323004209,
      -1.2506564060632033,
      -1.25629186054888,
      -1.3256122429855224,
      -1.3012536025397639,
      -1.2847174499315237
    ],
    [
      -1.3024010920377158,
      -1.2713150291549085,
      -1.3633275147867885,
      -1.265022582107598,
      -1.2685751207594422,
      -1.24916191135309,
      -1.3229706021799323,
      -1.304907857964439,
      -1.3199599024967053,
      -1.3223684985314388,
      -1.2960398837648939,
      -1.3626374375724062,
      -1.3295475536392192,
      -1.2021422462996791,
      -1.2854353747836873,
      -1.3167805067570229,
      -1.3019946450614976,
      -1.3240205397171725,
      -1.2923180803578804,
      -1.2916894592865795,
      -1.292088543382219,
      -1.2732714959500786,
      -1.319787662188909,
      0.0,
      -1.3303692131444045,
      -1.316848091880599,
      -1.358542218830662,
      -1.2929019120542204,
      -1.3399231546986712
    ],
    [
      -1.4346902126657939,
      -1.3705862929442032,
      -1.4434212084768394,
      -1.4337453273134573,
      -1.3952387234021286,
      -1.3995743398504876,
      -1.4054902406358345,
      -1.3951758768294629,
      -1.4317682090667485,
      -1.41266654294918,
      -1.4061284105356384,
      -1.4701485417731357,
      -1.4546549321180213,
      -1.4249914781559896,
      -1.466979022838102,
      -1.4084827698548732,
      -1.3710546576449265,
      -1.4633596601175352,
      -1.3997122006270974,
      -1.4425106639974283,
      -1.3878162498490088,
      -1.441253247653491,
      -1.3782677956349518,
      -1.4096331462547675,
      0.0,
      -1.4173707353724903,
      -1.43704590088935,
      -1.397051300499465,
      -1.4538134410629113
    ],
    [
      -1.4150716787584323,
      -1.338505404917869,
      -1.3902496857557116,
      -1.3114904539681764,
      -1.3797276818234605,
      -1.2993318898656145,
      -1.370610001999722,
      -1.4009935552707682,
      -1.4009150707663112,
      -1.4278831630457536,
      -1.3847649408643607,
      -1.455671126947318,
      -1.3451086255266602,
      -1.3854408498720658,
      -1.337886245081985,
      -1.3543225047901473,
      -1.4190002915008326,
      -1.3002115056821895,
      -1.3571287450233211,
      -1.331748862704449,
      -1.386006606129751,
      -1.3309157712571043,
      -1.3497206746213568,
      -1.374565888888708,
      -1.3881546877773634,
      0.0,
      -1.4010904200824874,
      -1.3586736362741751,
      -1.4427838808585602
    ],
    [
      -1.4218893739356466,
      -1.3414869803395697,
      -1.244322577973536,
      -1.3480191508981794,
      -1.258933023283453,
      -1.3520553707038194,
      -1.4110274080590386,
      -1.3710290476166749,
      -1.4219406187487458,
      -1.3042792690403162,
      -1.4095416991983916,
      -1.4494689721860194,
      -1.3218997703896105,
      -1.3607786779612863,
      -1.323354685259211,
      -1.3801251067279654,
      -1.3016938483004405,
      -1.3841413624779662,
      -1.3134544863674547,
      -1.3600838351565447,
      -1.3289059259436269,
      -1.3975520233300058,
      -1.3539326144450958,
      -1.367898726258985,
      -1.3474375449453593,
      -1.3614868972572525,
      0.0,
      -1.3988470301652909,
      -1.4244240162131179
    ],
    [
      -1.2061555491812797,
      -1.2240027654618217,
      -1.3069664750572687,
      -1.2615301721917378,
      -1.2958915036360652,
      -1.2780286689586224,
      -1.3238498761232202,
      -1.274395155862184,
      -1.2559980824713315,
      -1.304403507249652,
      -1.242603155821867,
      -1.3511287134707648,
      -1.2722728832888828,
      -1.2006137156917223,
      -1.2921271749884826,
      -1.2866169322542484,
      -1.2910394443464859,
      -1.2989130472048007,
      -1.2597477531317813,
      -1.2670508875163706,
      -1.2777935574496968,
      -1.2889349262914118,
      -1.2957452157744243,
      -1.2445238996167771,
      -1.2719316493697579,
      -1.2361209301568303,
      -1.2920320722156948,
      0.0,
      -1.329119901717246
    ],
    [
      -1.3029840714809227,
      -1.302394955210259,
      -1.3523712488385329,
      -1.359459941326341,
      -1.336091395579847,
      -1.3135517755702344,
      -1.3266033660727912,
      -1.3088966058071896,
      -1.321726839066852,
      -1.3466592937513886,
      -1.3106773996160443,
      -1.335781671768168,
      -1.3586836268823015,
      -1.326443407228679,
      -1.3845407208079634,
      -1.3350552011825096,
      -1.2769531991913317,
      -1.3160339801665297,
      -1.334730602538631,
      -1.3353159430120363,
      -1.3068872119549328,
      -1.2524981025409077,
      -1.2915311045283242,
      -1.3503649956819892,
      -1.3104592079733965,
      -1.3233042908878154,
      -1.3569291062452475,
      -1.2865560084562189,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.0899533464161788,
      0.07538895121256806,
      0.09087210004854218,
      0.07350998187831381,
      0.07432502334040691,
      0.06459625272360658,
      0.07250974532655974,
      0.08096618550000034,
      0.08107787511161213,
      0.0807516443266334,
      0.05580351066436462,
      0.06770793455296897,
      0.0861508691006434,
      0.07804173839487727,
      0.07738297603674194,
      0.07725491599276957,
      0.07113523895816476,
      0.07846927625614053,
      0.08909187512379524,
      0.08495890484420632,
      0.071110062763168,
      0.07624825317353578,
      0.07712944496214869,
      0.07843309898482342,
      0.08987779013082942,
      0.07380204836500259,
      0.09815345438007128,
      0.05499022654270602
    ],
    [
      0.3904893705455217,
      0.0,
      0.38715582415162,
      0.40796235871954445,
      0.45185121437650233,
      0.460997219706625,
      0.35208978732400964,
      0.37836568716734664,
      0.3746562300538321,
      0.3428555121527135,
      0.42824191073255835,
      0.3140836783952481,
      0.35879080504561744,
      0.4446826624827849,
      0.4262358959484005,
      0.4328016952489435,
      0.37850837868327636,
      0.3856006890915731,
      0.4097735984055735,
      0.3847641856486832,
      0.4160395797756593,
      0.36521363957801345,
      0.36215942703309056,
      0.42915748875256376,
      0.3672521583075352,
      0.3976600230344045,
      0.37506148931412575,
      0.38591116242306023,
      0.33416107729381594
    ],
    [
      0.29049101119080456,
      0.37429836042344466,
      0.0,
      0.3577511144482719,
      0.4145991635923363,
      0.39025854013977734,
      0.29849479079601715,
      0.36296120662389253,
      0.3202324260653904,
      0.4303257656127166,
      0.33887568046648964,
      0.2669017536089502,
      0.3750240119586714,
      0.33980041258214455,
      0.37796573205900286,
      0.3889012530667679,
      0.38861681796766834,
      0.30358516731547236,
      0.385263018671401,
      0.37759213812193715,
      0.3657082667252125,
      0.31801268385675274,
      0.2536073386329811,
      0.34215738565246623,
      0.32442372585067014,
      0.3581287898350005,
      0.4305092222319078,
      0.3277051342028616,
      0.2741807829912415
    ],
    [
      0.32357501024840096,
      0.3468771307821472,
      0.35514479953647937,
      0.0,
      0.3175270188567587,
      0.3775110713279919,
      0.3460499537585564,
      0.3123298362849387,
      0.3255050587494981,
      0.3218216576244415,
      0.36568272252482625,
      0.30453377701398754,
      0.31427277488436567,
      0.39769195868502405,
      0.3868399071243416,
      0.382794301974436,
      0.3304998077426671,
      0.3367838270843442,
      0.33596175557557073,
      0.3687155235389916,
      0.34296311186401796,
      0.37662939277227614,
      0.34042295817350987,
      0.38409194762666665,
      0.31515728035662893,
      0.382019168954,
      0.33104221673054934,
      0.35010257792570254,
      0.28345908828366695
    ],
    [
      0.33621335385785334,
      0.5274548522777505,
      0.48276734348160266,
      0.43720457300930193,
      0.0,
      0.4994738329159969,
      0.41855279330423834,
      0.46473543972531783,
      0.3884473763207974,
      0.47760278374210174,
      0.3989787688056996,
      0.3344922861161155,
      0.41388415414204927,
      0.42866549980265023,
      0.4955308361562658,
      0.4447509886610912,
      0.4453213114695298,
      0.3893996582992587,
      0.4130457934396172,
      0.44613951951292363,
      0.44847917531986203,
      0.38196871706664104,
      0.39124529944024933,
      0.4503390090125716,
      0.3879483250616882,
      0.3903963035445497,
      0.4534106915130216,
      0.35124474463577315,
      0.3063570228552184
    ],
    [
      0.3468443525256377,
      0.5225029309970064,
      0.41981961540472357,
      0.43350357542727047,
      0.49076764840637255,
      0.0,
      0.39582725376854455,
      0.48892828094387863,
      0.44083300127572556,
      0.44715990550516893,
      0.49241904142965365,
      0.3177686505289252,
      0.49712797304706347,
      0.4693866201574344,
      0.5361159579947528,
      0.471544741492014,
      0.37943513757221004,
      0.5260333901067329,
      0.4066728661597865,
      0.40158503746540863,
      0.41773912349628683,
      0.44072794683113337,
      0.3791065280638224,
      0.46147034378826834,
      0.363720213563107,
      0.42596187746716274,
      0.39411941877719814,
      0.4011801262423662,
      0.2993765713118055
    ],
    [
      0.31207982287381064,
      0.4523532442428979,
      0.35895911760496113,
      0.38434088176562886,
      0.4336531332963469,
      0.43773389924477435,
      0.0,
      0.37037525229175294,
      0.4152800569075745,
      0.37837931432135674,
      0.4102577772680138,
      0.3933632050881155,
      0.3600416624221445,
      0.3894825512308282,
      0.4089387583095936,
      0.404503919347754,
      0.37885385018033735,
      0.37097908192136253,
      0.3773690059032113,
      0.3592807310320314,
      0.39272130378853265,
      0.398909250020677,
      0.381273648202646,
      0.4228356023616009,
      0.38573478575433207,
      0.39493721060116926,
      0.39168750117577855,
      0.38231653772977947,
      0.3741560646935105
    ],
    [
      0.30620583702630166,
      0.3885507871781726,
      0.32236152874234825,
      0.3359251585714502,
      0.36297757141780207,
      0.38254402318781056,
      0.32307866788795025,
      0.0,
      0.3523427651257329,
      0.3583332650377171,
      0.35523406280580705,
      0.25886439490141755,
      0.3325740886370878,
      0.3490076547226564,
      0.34781242711294125,
      0.35615864358771376,
      0.29889817442337985,
      0.3358023089067381,
      0.35319421276612895,
      0.3173228839233957,
      0.38944039251608364,
      0.3437796180641377,
      0.3160744889314038,
      0.3755333293816068,
      0.3459134069057219,
      0.3515222486825351,
      0.3218873164914544,
      0.3448865607346845,
      0.2739465608327436
    ],
    [
      0.37839800922706157,
      0.41322527136903364,
      0.30852827231494784,
      0.3751639923725707,
      0.36354420143058963,
      0.3815999055054393,
      0.3756155770909593,
      0.38273369891214215,
      0.0,
      0.29157307750612493,
      0.38215421455160015,
      0.3026872004795229,
      0.34505728305418626,
      0.39202141421386827,
      0.34149217208579663,
      0.3495845267793334,
      0.34353918155479835,
      0.3621027908161676,
      0.33811372593433053,
      0.3660860156263135,
      0.43220423672649577,
      0.3479346516812989,
      0.35495589240610204,
      0.4016548729432723,
      0.3432012945262011,
      0.3900239612814178,
      0.3331513433593052,
      0.4690941076323836,
      0.32086368629762685
    ],
    [
      0.2057641832801873,
      0.2684062329613579,
      0.23537233974343374,
      0.20997767738645146,
      0.2510828811067882,
      0.2120974401590907,
      0.20422351607020128,
      0.26982724790674206,
      0.1823417795360005,
      0.0,
      0.24204182429910426,
      0.18596187142338638,
      0.2309115296213866,
      0.2662049085427274,
      0.22999037669934763,
      0.23993693448966558,
      0.2829806969377864,
      0.21962702922017385,
      0.24157020914373017,
      0.17856790818941337,
      0.22336723842348816,
      0.189143129132342,
      0.19135286578687394,
      0.21381357256685884,
      0.22164721195624537,
      0.19952397137891298,
      0.2447904294867096,
      0.24536990955079885,
      0.2042923132133121
    ],
    [
      0.3476344015849737,
      0.39602021790587116,
      0.3439323047851832,
      0.373522205363519,
      0.3353939740629883,
      0.37077453030633456,
      0.3433313854748439,
      0.36010430632389157,
      0.3254956772691575,
      0.36804077145133296,
      0.0,
      0.28496521902318594,
      0.30699324783645054,
      0.40006238000475847,
      0.38249837359995875,
      0.4007316268934409,
      0.3263761434203287,
      0.3599188068952086,
      0.32979139317712614,
      0.35889962198785486,
      0.3729708567750343,
      0.3534922496339137,
      0.31522650374835415,
      0.3788379584755628,
      0.3074760307485418,
      0.331736846867591,
      0.2992100712350352,
      0.36944979305157477,
      0.3046454406754606
    ],
    [
      0.2679188819180791,
      0.27709257241686025,
      0.2598470663032535,
      0.26508389778020325,
      0.26368384045725923,
      0.27536056378740414,
      0.2718831923397014,
      0.2821724725705177,
      0.27945319052234563,
      0.24427320841897626,
      0.2765364544403792,
      0.0,
      0.2522428859526378,
      0.2657950010786678,
      0.26227930920480613,
      0.26068359333610713,
      0.2991098720324663,
      0.24680035690099267,
      0.2684928945001881,
      0.2379302753385617,
      0.2972185121191224,
      0.2724912621949682,
      0.27338715422886106,
      0.27823817848022525,
      0.26406724925440206,
      0.27397447805484876,
      0.2576177376293327,
      0.2953905051934602,
      0.2757199021908161
    ],
    [
      0.3019881871236181,
      0.3797889063898501,
      0.3365413888632054,
      0.33827925059150243,
      0.3631221209718851,
      0.4014082577143221,
      0.30942686116143236,
      0.3843314682906269,
      0.3119115041328837,
      0.364276519028081,
      0.34429366340967116,
      0.26225451063171334,
      0.0,
      0.36978790431055875,
      0.3939779418915226,
      0.3872000227488208,
      0.2993640567470017,
      0.35818081901989296,
      0.30213864175120064,
      0.3475230464503485,
      0.3623366106089836,
      0.3493041809588424,
      0.3168674569313066,
      0.3501918126152215,
      0.3120750483837593,
      0.41465438562425594,
      0.33056545443752405,
      0.34462297960827515,
      0.2570848824454275
    ],
    [
      0.33212771471574865,
      0.40563030829374425,
      0.339906102105348,
      0.3882516877323965,
      0.359251885956152,
      0.40349863172617684,
      0.33183913503484397,
      0.3331756062586715,
      0.32319849990903116,
      0.3672756098364327,
      0.3820431753377147,
      0.29010230858445407,
      0.3146837249240906,
      0.0,
      0.3415870911870409,
      0.44622020947613894,
      0.3476896283840649,
      0.33970060682852377,
      0.4195144866949121,
      0.32227521372421086,
      0.3501692574301105,
      0.3439956445723982,
      0.3310392284842991,
      0.41196852481724755,
      0.3279530917615645,
      0.33307644582010165,
      0.3474750487801679,
      0.44195149536782274,
      0.2703204550169329
    ],
    [
      0.28317021493217776,
      0.40231838826693345,
      0.32326793107931207,
      0.36376083764524614,
      0.3761658677076023,
      0.4373994532407772,
      0.35199272570103046,
      0.37575127249954177,
      0.32659477060827835,
      0.3361463083862086,
      0.3395304700779014,
      0.28968852617437824,
      0.3904667745392183,
      0.3543795555239424,
      0.0,
      0.36773626310593466,
      0.2845223228080356,
      0.41789555333385375,
      0.3218257955653552,
      0.37606304936402135,
      0.32593129214036853,
      0.3703356809138063,
      0.3083567202034663,
      0.36729961641799824,
      0.3057297262018466,
      0.35818535656061323,
      0.37613196853870634,
      0.34344034390039835,
      0.2598761199526993
    ],
    [
      0.32047223336184394,
      0.4221649068890312,
      0.36128472600901484,
      0.3770857869195243,
      0.34527254754559733,
      0.4003034960319296,
      0.3459903320000146,
      0.36080213431901775,
      0.301376184080971,
      0.3684882608302311,
      0.3969247875590276,
      0.3052593919851203,
      0.3870575667278151,
      0.46471275124109357,
      0.36645952627282363,
      0.0,
      0.3969886883635563,
      0.3199871054526755,
      0.43593124122055116,
      0.3280118677801209,
      0.3836993974052636,
      0.34598905092302146,
      0.31115608663603145,
      0.36659036320090155,
      0.3395919956713822,
      0.33625810838730374,
      0.32900509696763214,
      0.34250697157187604,
      0.27252641114125287
    ],
    [
      0.3231849602350143,
      0.4254838790614135,
      0.36315140783600874,
      0.39502021259825293,
      0.37290042594974926,
      0.4100757314697461,
      0.3500188456035709,
      0.3676647367973007,
      0.3574579328935943,
      0.4209123824235841,
      0.4055146656438233,
      0.3066346962368016,
      0.3578999402002627,
      0.4217371600281219,
      0.35144451041541735,
      0.41842564794350356,
      0.0,
      0.39053224855574653,
      0.4204651838240976,
      0.33877024781767284,
      0.37458723527144344,
      0.3507437048128559,
      0.38464084502356033,
      0.34332454593715034,
      0.4152324242167873,
      0.3854259366558581,
      0.3814210998480858,
      0.3584663244253985,
      0.32159075719761665
    ],
    [
      0.3229715020574515,
      0.43760382447086243,
      0.3829483260848823,
      0.4217397391122164,
      0.3827003512982947,
      0.4833102308343422,
      0.3785396075998477,
      0.4138258275183071,
      0.3568246307969358,
      0.40439201979143014,
      0.3970334963107063,
      0.3376720791033361,
      0.39218525794739123,
      0.4530602230559351,
      0.45158012404911285,
      0.3961833425360559,
      0.37567008770331367,
      0.0,
      0.35846048425144406,
      0.4027449063249542,
      0.36914890545121404,
      0.3789448377622764,
      0.3884911670646647,
      0.3836736583191904,
      0.3526757369682667,
      0.4230009852103025,
      0.3900610828129938,
      0.369742512613743,
      0.31276974466772445
    ],
    [
      0.32953437630687055,
      0.4061903217587348,
      0.39201505843567697,
      0.38538299355979166,
      0.3806793247942304,
      0.443243171184148,
      0.3513142619516354,
      0.3504305960373577,
      0.3556558630240143,
      0.33470713766191906,
      0.3746783043609889,
      0.30938471049079364,
      0.3989841064963995,
      0.4647181667079572,
      0.32864332796843554,
      0.47408273603399587,
      0.40184900681011304,
      0.3627618102762986,
      0.0,
      0.30322493149327445,
      0.3865029690893156,
      0.3392512349008163,
      0.3361600422572004,
      0.4017243457018145,
      0.3892255890951457,
      0.40435884484406137,
      0.4127875090086137,
      0.38706618220482825,
      0.30873855338363465
    ],
    [
      0.34068618246062754,
      0.3706461619391723,
      0.3380244253882312,
      0.3712101696143715,
      0.37506736959234477,
      0.33243343061746256,
      0.30324352772269836,
      0.2880880657292135,
      0.347033407304393,
      0.3211402456935475,
      0.31275823019647575,
      0.24277865182744063,
      0.32964990422990903,
      0.30385892019486627,
      0.40781238701424827,
      0.3060221403731347,
      0.30731048907968916,
      0.3341193771471327,
      0.2725007906126058,
      0.0,
      0.3536370951241139,
      0.3305568819182707,
      0.34611457627542697,
      0.3484894727399195,
      0.32275997114803845,
      0.39408097106905715,
      0.33183437846012165,
      0.3386715145083541,
      0.247465041852432
    ],
    [
      0.42238121716079147,
      0.43078230279003615,
      0.3616872596912535,
      0.3781377000471182,
      0.3898096084831899,
      0.4321626370296856,
      0.4127732810125546,
      0.45054393670743287,
      0.39735481489522173,
      0.37079047445925384,
      0.39271293364728965,
      0.3589520260746477,
      0.3685514200971496,
      0.36619937685671844,
      0.35517924077848595,
      0.4060365500023626,
      0.4379189096764218,
      0.404386237003475,
      0.3905926368053103,
      0.34889546514792436,
      0.0,
      0.3952749990054363,
      0.3738815637455275,
      0.38864835989824775,
      0.40418269523482353,
      0.3796985812801328,
      0.38715644861179177,
      0.3789996548221173,
      0.33655996212717154
    ],
    [
      0.3139803650367763,
      0.3270777283022037,
      0.3031848722065613,
      0.37021423940970943,
      0.3164738797923574,
      0.3902052607532678,
      0.30201885553574526,
      0.3024460231244639,
      0.2977761143790387,
      0.2795496028914737,
      0.3371583503178792,
      0.2781022309665009,
      0.3572417665818364,
      0.3242783523774766,
      0.38958361634336325,
      0.3343239809973906,
      0.3331358758784153,
      0.3216933064324812,
      0.2994319216207759,
      0.3388853892376258,
      0.3113494288499534,
      0.0,
      0.345695778031029,
      0.30856479131646286,
      0.29978230295540076,
      0.3795668924050011,
      0.29876683178330343,
      0.3085281099905284,
      0.31481583354692466
    ],
    [
      0.2820693174630673,
      0.31613915551955896,
      0.2644204662863203,
      0.29505487028587707,
      0.2972899934974229,
      0.32527868586177644,
      0.27761901219576957,
      0.3073836173036213,
      0.29533593172908734,
      0.2839011706122445,
      0.291709968122702,
      0.27705185584774616,
      0.2965145739955355,
      0.293861236187539,
      0.2813217858725159,
      0.2931866819322415,
      0.3161742836286108,
      0.32339735620087673,
      0.2631386764176584,
      0.35289500929210504,
      0.3107386443704969,
      0.32589075548345114,
      0.0,
      0.3020395385691925,
      0.3497125648064101,
      0.3440771103207334,
      0.274756727884091,
      0.29911536832984953,
      0.3156515209380897
    ],
    [
      0.3257276032545864,
      0.35681366613739374,
      0.2648011805055137,
      0.3631061131847042,
      0.35955357453285997,
      0.3789667839392121,
      0.30515809311236985,
      0.3232208373278631,
      0.30816879279559695,
      0.3057601967608634,
      0.3320888115274083,
      0.265491257719896,
      0.298581141653083,
      0.42598644899262306,
      0.3426933205086149,
      0.31134818853527935,
      0.32613405023080455,
      0.3041081555751297,
      0.33581061493442177,
      0.3364392360057227,
      0.3360401519100833,
      0.3548571993422236,
      0.3083410331033931,
      0.0,
      0.2977594821478977,
      0.31128060341170327,
      0.26958647646164025,
      0.33522678323808175,
      0.288205540593631
    ],
    [
      0.3370677929492296,
      0.4011717126708203,
      0.3283367971381841,
      0.3380126783015662,
      0.3765192822128949,
      0.3721836657645359,
      0.366267764979189,
      0.37658212878556063,
      0.339989796548275,
      0.35909146266584346,
      0.3656295950793851,
      0.3016094638418878,
      0.3171030734970022,
      0.3467665274590339,
      0.3047789827769214,
      0.3632752357601503,
      0.400703347970097,
      0.30839834549748835,
      0.37204580498792605,
      0.32924734161759517,
      0.38394175576601475,
      0.33050475796153256,
      0.39349020998007167,
      0.36212485936025596,
      0.0,
      0.35438727024253325,
      0.3347121047256736,
      0.3747067051155586,
      0.3179445645521122
    ],
    [
      0.37905260997223467,
      0.4556188838127979,
      0.40387460297495537,
      0.48263383476249055,
      0.4143966069072065,
      0.49479239886505244,
      0.42351428673094493,
      0.3931307334598988,
      0.3932092179643558,
      0.3662411256849134,
      0.40935934786630623,
      0.33845316178334905,
      0.44901566320400677,
      0.40868343885860114,
      0.4562380436486819,
      0.4398017839405197,
      0.37512399722983436,
      0.4939127830484775,
      0.43699554370734583,
      0.46237542602621806,
      0.4081176826009161,
      0.46320851747356273,
      0.4444036141093102,
      0.419558399841959,
      0.4059696009533036,
      0.0,
      0.3930338686481796,
      0.43545065245649184,
      0.35134040787210674
    ],
    [
      0.2641940426703977,
      0.3445964362664746,
      0.44176083863250826,
      0.33806426570786496,
      0.4271503933225913,
      0.3340280459022249,
      0.27505600854700574,
      0.31505436898936945,
      0.26414279785729855,
      0.3818041475657281,
      0.27654171740765277,
      0.23661444442002488,
      0.36418364621643384,
      0.32530473864475806,
      0.36272873134683326,
      0.3059583098780789,
      0.3843895683056038,
      0.3019420541280782,
      0.3726289302385897,
      0.3259995814494996,
      0.35717749066241744,
      0.2885313932760385,
      0.3321508021609485,
      0.3181846903470593,
      0.33864587166068505,
      0.32459651934879186,
      0.0,
      0.28723638644075344,
      0.26165940039292646
    ],
    [
      0.4082421727760388,
      0.3903949564954967,
      0.30743124690004975,
      0.35286754976558066,
      0.31850621832125325,
      0.336369052998696,
      0.2905478458340982,
      0.34000256609513446,
      0.35839963948598696,
      0.3099942147076664,
      0.37179456613545137,
      0.26326900848655366,
      0.3421248386684357,
      0.4137840062655962,
      0.32227054696883584,
      0.32778078970307,
      0.3233582776108326,
      0.31548467475251774,
      0.3546499688255371,
      0.3473468344409478,
      0.33660416450762165,
      0.32546279566590663,
      0.3186525061828942,
      0.36987382234054134,
      0.34246607258756057,
      0.3782767918004881,
      0.3223656497416236,
      0.0,
      0.28527782024007253
    ],
    [
      0.33122115366150773,
      0.33181026993217144,
      0.2818339763038975,
      0.2747452838160893,
      0.29811382956258337,
      0.320653449572196,
      0.3076018590696392,
      0.3253086193352408,
      0.31247838607557843,
      0.2875459313910418,
      0.32352782552638604,
      0.2984235533742623,
      0.2755215982601289,
      0.30776181791375135,
      0.24966450433446696,
      0.29915002395992074,
      0.35725202595109873,
      0.31817124497590066,
      0.2994746226037994,
      0.2988892821303941,
      0.32731801318749754,
      0.38170712260152273,
      0.3426741206141062,
      0.2838402294604412,
      0.32374601716903384,
      0.31090093425461496,
      0.27727611889718284,
      0.3476492166862115,
      0.0
    ]
  ],
  "row_avgs": [
    0.077489025896835,
    0.39080438390688016,
    0.34915613195322326,
    0.34485734414302815,
    0.4215732304817763,
    0.4309885046339093,
    0.3900284703421616,
    0.33929187066081873,
    0.36379659205996034,
    0.22464954386473274,
    0.3481261549491956,
    0.2694551610230515,
    0.3426249243872048,
    0.3552114578128693,
    0.34764153233534467,
    0.35828203630337946,
    0.37545456031865854,
    0.38992695327561416,
    0.37547483842278806,
    0.32921406356547495,
    0.39000893903898476,
    0.3244225607522838,
    0.301990209962664,
    0.32397340490866433,
    0.35202117957883344,
    0.42133950837157214,
    0.326797343635237,
    0.3383428070823031,
    0.31050932252216656
  ],
  "col_avgs": [
    0.3258459243005934,
    0.38074881271312205,
    0.333705277490073,
    0.3537455267123949,
    0.3539844253332241,
    0.37710672975454296,
    0.3241666240832507,
    0.3483137754519857,
    0.32615935827880704,
    0.34298071238838307,
    0.3508740717920548,
    0.2850416937425759,
    0.33908547672833317,
    0.3669225913293844,
    0.3564180416452644,
    0.3602323967085931,
    0.34274924658409683,
    0.34008714370516924,
    0.34261868192836914,
    0.33734151942185525,
    0.3521825284553506,
    0.34049897718454586,
    0.3291848610223095,
    0.35504843446026485,
    0.3297315347225644,
    0.35062815739528475,
    0.33440090542559825,
    0.34693520767795727,
    0.28671341975366715
  ],
  "combined_avgs": [
    0.2016674750987142,
    0.3857765983100011,
    0.34143070472164816,
    0.3493014354277115,
    0.3877788279075002,
    0.4040476171942261,
    0.35709754721270615,
    0.3438028230564022,
    0.34497797516938367,
    0.2838151281265579,
    0.3495001133706252,
    0.2772484273828137,
    0.34085520055776897,
    0.3610670245711268,
    0.3520297869903045,
    0.3592572165059863,
    0.3591019034513777,
    0.3650070484903917,
    0.35904676017557857,
    0.3332777914936651,
    0.37109573374716764,
    0.3324607689684148,
    0.31558753549248675,
    0.33951091968446456,
    0.3408763571506989,
    0.3859838328834284,
    0.3305991245304176,
    0.3426390073801302,
    0.29861137113791686
  ],
  "gppm": [
    575.004562641315,
    613.3077050666772,
    631.2952446864618,
    625.4916175328863,
    625.7617370911095,
    612.5449234378463,
    637.6344681160298,
    628.3852183570042,
    637.5607114955095,
    621.7823180833473,
    626.5118014322842,
    656.2149072565005,
    632.2454191818488,
    617.9096032113122,
    622.6133793049539,
    621.3058789926523,
    628.9416249805821,
    629.5704870136052,
    630.6529818254857,
    632.598593515688,
    621.4885170757809,
    630.733003272604,
    638.3950190586019,
    624.3771950605704,
    635.0539372713508,
    625.7115986586434,
    634.2768712001271,
    627.3817218988786,
    655.7353909841115
  ],
  "gppm_normalized": [
    1.2791564981361832,
    1.4888104509240823,
    1.5377892611872679,
    1.5128127485772775,
    1.5160915437153777,
    1.484610708535056,
    1.5451719283382024,
    1.5131853449372332,
    1.537909304036042,
    1.5039776290889069,
    1.5084172770422042,
    1.5827814776935332,
    1.5307926673377796,
    1.4926528420261078,
    1.5058415581860782,
    1.4987069789852774,
    1.518035178421533,
    1.5223035180534754,
    1.5206101389549977,
    1.5307628412785976,
    1.49725640783835,
    1.516893471481069,
    1.5345249686205413,
    1.506431997064448,
    1.531588228006379,
    1.5069351693184916,
    1.5311717591706604,
    1.5134553549244387,
    1.572797095783995
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391,
    303,
    497,
    433,
    459,
    752,
    459,
    495,
    438,
    433,
    410,
    431,
    387,
    410,
    444,
    431,
    438,
    393,
    372,
    470,
    373,
    439,
    420,
    389,
    490,
    384,
    410,
    374,
    372,
    394,
    324,
    414,
    458,
    484,
    436,
    428,
    501,
    399,
    399,
    381,
    466,
    408,
    398,
    376,
    425,
    390,
    398,
    422,
    389,
    407,
    452,
    424,
    401,
    424,
    397,
    379,
    413,
    401,
    344,
    781,
    392,
    394,
    392,
    352,
    525,
    360,
    385,
    392,
    397,
    413,
    380,
    455,
    358,
    381,
    399,
    424,
    396,
    397,
    404,
    410,
    394,
    346,
    390,
    420,
    367,
    398,
    386,
    349,
    693,
    449,
    447,
    450,
    437,
    450,
    518,
    423,
    431,
    416,
    460,
    437,
    476,
    460,
    444,
    463,
    451,
    458,
    421,
    440,
    470,
    414,
    457,
    429,
    431,
    416,
    443,
    439,
    406,
    1366,
    431,
    451,
    431,
    382,
    435,
    379,
    402,
    392,
    554,
    420,
    469,
    441,
    450,
    466,
    420,
    414,
    410,
    438,
    442,
    378,
    440,
    388,
    437,
    397,
    387,
    391,
    431,
    399
  ],
  "response_lengths": [
    5736,
    2489,
    2642,
    2555,
    2254,
    2579,
    2268,
    2364,
    2376,
    3205,
    2521,
    2653,
    2529,
    2569,
    2727,
    2520,
    2411,
    2468,
    2574,
    2497,
    2095,
    2586,
    2276,
    2561,
    2294,
    2207,
    2345,
    2569,
    2312
  ]
}