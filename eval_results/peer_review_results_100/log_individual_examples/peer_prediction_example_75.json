{
  "example_idx": 75,
  "reference": "Published as a conference paper at ICLR 2023\n\nHOW TO PREPARE YOUR TASK HEAD FOR FINETUNING\n\nYi Ren University of British Columbia renyi.joshua@gmail.com\n\nShangmin Guo University of Edinburgh s.guo@ed.ac.uk\n\nWonho Bae University of British Columbia whbae@cs.ubc.ca\n\nDanica J. Sutherland University of British Columbia & Amii dsuth@cs.ubc.ca\n\nABSTRACT\n\nIn deep learning, transferring information from a pretrained network to a downstream task by finetuning has many benefits. The choice of task head plays an important role in fine-tuning, as the pretrained and downstream tasks are usually different. Although there exist many different designs for finetuning, a full understanding of when and why these algorithms work has been elusive. We analyze how the choice of task head controls feature adaptation and hence influences the downstream performance. By decomposing the learning dynamics of adaptation, we find that the key aspect is the training accuracy and loss at the beginning of finetuning, which determines the “energy” available for the feature’s adaptation. We identify a significant trend in the effect of changes in this initial energy on the resulting features after finetuning. Specifically, as the energy increases, the Euclidean and cosine distances between the resulting and original features increase, while their dot products (and the resulting features’ norm) first increase then decrease. Inspired by this, we give several practical principles that lead to better downstream performance. We analytically prove this trend in an overparamterized linear setting, and verify its applicability to different experimental settings.\n\n1\n\nINTRODUCTION\n\nIn the era of deep learning, pretraining a model on a large dataset and adapting it to downstream tasks is a popular workflow. With the help of large amount of data and huge computing resources, the pretrained model can usually provide beneficial features for the downstream tasks. Such a framework is proven to be efficient and effective in many domains and tasks, e.g. natural language processing (Kenton & Toutanova, 2019), computer vision (Chen et al., 2020b), graph based learning (Liu et al., 2022), and so on. Although different variants of pretraining and finetuning (FT) methods are widely applied – including direct finetuning, finetuning after linear probing (Kumar et al., 2022), side-tuning (Zhang et al., 2020a), using different learning rates for different layers (Zhang et al., 2021), and more – a detailed understanding of how features are adapted during finetuning under different settings remains elusive.\n\nOur work builds significantly off the analysis of Kumar et al. (2022), who study the interactions between the “task head” (the final layer of the network, usually randomly initialized) and the “backbone” (usually copied from the pretrained model). Kumar et al. claim that the standard finetuning method, randomly initializing a task head then updating all parameters of the whole network, can distort the pretrained features and hence can deteriorate the generalization ability if (as they assume) the previous backbone features were optimal for the downstream task. By analyzing an overparameterized linear model, they prove that linear probing (i.e., only updating the parameters of the task head) first, followed by finetuning the whole network, leads to better performance in their setting.\n\nIn this work, we consider less stringent assumptions than they made, and study more practical settings from a different perspective. First, we consider scenarios where the pretrained features are not optimal for the downstream tasks, thus feature adaptation is indeed beneficial. Unlike the two extreme cases studied by Kumar et al. (2022), i.e. finetuning with fully random initialization and\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Left: a general example of pretraining (PT), head probing (HP) and finetuning (FT) procedure (DS is short for downstream). Right: an example showing that neither probing the head to converge nor no probing is the optimum (pretrained on ImageNet-1K and finetuned on STL10). fully-pretrained parameters, we consider intermediate cases where features are mildly adapted by stopping earlier (before convergence) in the linear probing procedure. To better understand the feature’s behavior, we decompose the learning dynamics of the feature vector during finetuning based on “energy” and “direction” of the learning. We discover a non-trivial trend in how this “energy” affects the way that features change from their initialization, which can inspires us to design an appropriate finetuning procedure. Under this framework, we demonstrate that the “unchanged feature” assumption of Kumar et al. (2022) is hard to achieve.\n\nSecond, our task heads are not necessarily linear. Inspired by the illustrations of Olah et al. (2020), it is reasonable to only preserve the lower layers of the pretrained model and reinitialize the top layers, assuming that the low-level features are common across task. That is, the probed task head is non-linear, and we refer to this more general process as “head probing” (HP) rather than linear probing. Our analysis can also help to explain feature behavior in this setting.\n\nFinally, following our analysis, we provide a user guide to conclude when and why specific methods should be considered. Specifically, we have one basic method: stop head probing earlier, before convergence; and three advanced tricks: 1.) use label smoothing during head probing; 2.) use more complex task head design; 3.) merge and reinitialize some later layers of the backbone and attach them to the task head. In summary, in this work:\n\n• we formalize and explain feature adaptation by decomposing the learning dynamics; • we find a non-trivial trend in feature adaptation and verify it in many cases; • and we show how controlling feature adaptation can improve downstream performance.\n\n2 MOTIVATION\n\nPretrain-then-finetune is a popular workflow for many tasks in deep learning. One common practice is to 1) randomly initialize a task head, 2) attach it to a pretrained backbone, then 3) finetune the whole network together (Li et al., 2020). However, the untrained task head may distort the pretrained features during finetuning. To solve this problem, Kumar et al. (2022) propose to train the head to fully converge before the finetuning. However, suppose we train the head long enough and its training accuracy (HP-train-acc) converges to 100%, then the features won’t change during the finetuning stage. To sum up from the above, we can see that neither probing the head to converge nor no probing is optimal, since the pretraining and downstream tasks (or datasets) are usually distinct. To verify this argument, we HP various number of epochs before finetuning, and record the corresponding validation accuracy after finetuning (FT-valid-acc for short), and the results are shown in Figure 1. It is surprising to see that stopping the head training earlier (before the convergence of HP-train-acc) brings more improvement. As the only variable among these experiments is the parameters of the head before finetuning, the following two questions emerge:\n\nHow does the task head influence the pretrained features during finetuning? How does the feature adaptation influence the generalization performance after finetuning?\n\n3 BACKGROUND\n\nWe first clarify the scope of our analysis. We don’t consider the details of the pretraining procedure, instead just assuming that there are some well-trained checkpoints for a particular dataset or task.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nMeanwhile, our formulation is not restricted to classification tasks; our use of the term “label” or “target” can be any form of supervisory signals.\n\n3.1 PROBLEM SETTING AND TWO STAGE TRAINING\n\nWhen training a model for a downstream task, our goal is to find a predictor f ◦ g : Rd → Rk that maps a high-dimensional input signal x ∈ Rd to a task-related prediction q ∈ Rk. As depicted by the left-bottom panel of Figure 1, we split the predictor into two parts: the backbone f (x; B) : Rd → Rh which maps the input signal to intermediate representations z ∈ Rh, and the task head g(z; v) : Rh → Rk which gives the prediction vector q ∈ Rk (e.g. logits in classification tasks using cross-entropy loss). Usually, the backbone f is parameterized by B and initialized by copying from a pretrained model. The task head g parameterized by v, on the other hand, is usually randomly initialized and might be a complex non-linear function. The training has two distinct stages: 1) head probing (HP) where we fix f and only update the parameters v of g for τ epochs; 2) finetuning (FT) where the parameters {v, B} of f ◦ g are updated together until convergence. In this work, we analyze how the FT stage is influenced by the architecture and v at the beginning of finetuning.\n\nFollowing the general formulation above, we show a simple overparameterized linear regression (or equivalently, a binary classification) below as a case study to illustrate more insights. Suppose the N input signals are stacked, X = [x(1), ..., x(N )]⊤ ∈ RN ×d, the loss function can be written as\n\nLB,v =\n\n1 2\n\n∥XB⊤v − Y ∥2 2,\n\n(1)\n\nwhere B ∈ Rh×d, v ∈ Rh, and Y ∈ RN . That is, k = 1, z = Bx, q = z⊤v, and L(q, y) = 1\n\n2 (q−y)2.\n\n3.2 WHAT TO EXPECT DURING ADAPTATION\n\nCompared with training a randomly initialized model, adaptation on downstream tasks needs more care. One reason is that the pretrained parameters from f (x; B) inherits all the information from the pretraining task, even any bias and noise. Furthermore, as mentioned by Kumar et al. (2022) and Du et al. (2018), f (x; B) is tied to g(z; v) at each time step during FT, thus the bias and noise in f (x; B) also influence the learning of g(z; v). Hence, before conducting downstream adaptation, we might consider: to what extent do we want to change the feature extractor f (x; B)? Here, we list three possible cases of how much we should update f (x; B):\n\n• Strong: the pretrained features are far from the optimal ones for downstream task, so we\n\nneed substantial feature adaptation.\n\n• Mild: f (x; B) is reasonably good, but adaptation to the downstream domain is helpful.\n\n• Tiny: the pretrained f (x; B) is near optimal and only need to be slightly adapted.\n\nIn the rest of this paper, we first analyze how f (x; B) and g(z; v) interact with each other, under both the general case and the simplified case. Based on our observations, we propose several practical principles in Section 5.1 for better downstream performance.\n\n4\n\nINTERACTION BETWEEN THE BACKBONE AND HEAD\n\nRather than linking the choice of v0 (the task head parameters at the beginning of FT) to the loss function, we sketch how f (x; B) and g(z; v) interact during FT, which depicts how z changes accordingly. Although our analysis cannot provide any theoretical guarantee, knowing how z changes under different v0 will lead us to better HP-FT design in practice.\n\n4.1 AVERAGE INITIAL ENERGY\n\nWe start from analyzing the behavior of z(j) t = f (x(j); Bt), i.e. the feature extractor at time t, when the network parameters are updated with samples x(1), ..., x(N ) using gradient descent. When using\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n(a) Influence of τ on energy.\n\n(b) Influence of τ on direction.\n\nFigure 2: Left: histogram of p0 and ey under different τ . Right: approximated change of the ‘direction’ term in Equation (2) under different τ . The titles represent the settings. HP τ means HP for τ epochs. Toy+MLP means pretrain a 4-layer MLP on full MNIST then transfer to a distorted subset of MNIST. Res50-IN1K(Sup)-STL means pretrain a ResNet50 on ImageNet-1k using supervised classification, then transfer to a downstream task on STL10. See Appendix B for more details.\n\ncross-entropy loss, we can have the following result: 1\n\nt+1 − z(j) z(j)\n\nt =\n\n\n\n \n\n\nγ N\n\nN (cid:88)\n\nn=1\n\nt\n\nκ(j,n) (cid:124) (cid:123)(cid:122) (cid:125) slow-change\n\n·\n\n(cid:16)\n\n(cid:124)\n\n∇zq(n) t\n(cid:123)(cid:122) direction\n\n(cid:17)⊤\n\n(cid:16)\n\n·\n\n(cid:125)\n\n(cid:124)\n\n\n\n \n\n\n(cid:17)\n\n(cid:125)\n\neyn − p(n) (cid:123)(cid:122) energy\n\nt\n\n+ O(γ2),\n\n(2)\n\n(cid:16)\n\n(cid:17) (cid:16)\n\n(cid:17)⊤\n\nwhere γ is the learning rate, κ(j,n) =\nkernel (NTK) of the backbone between x(j) and x(n) at time t,2 ∇zq(n) the task head prediction w.r.t the representation vector z at time t, 3 p(n) predicted probability vector for input x(n), and eyn is the one-hot vector of the label yn.\n\nt ∈ Rk×h is the gradient of ) is the\n\nt = Softmax(q(n)\n\n∈ Rh×h is the empirical neural tangent\n\n∇Bz(n)\n\n∇Bz(j)\n\nt\n\nt\n\nt\n\nt\n\nIn this decomposition, the first term is often understood to change only slowly during FT, i.e. the lazy-parameters setting used by Chizat et al. (2019); Yang & Hu (2020). The second term determines the direction, and the last term provides “energy” for the update of z. Formally, we can define the Average Initial Energy (AIE) and use it to bound the norm of z(j) Proposition 1. E 0 ∥2 ≤ c · Eaie, where Eaie ≜ E Initial Energy (AIE). Here c is a constant, T is the FT epochs, and p(n) sample x(n) at the beginning of FT (or at the end of HP).\n\n0 ∥2 is the Average is the model’s prediction of\n\nx(n)∥eyn − p(n)\n\nT − z(j) 0 :\n\nT − z(j)\n\nx(j)∥z(j)\n\n0\n\nProof. See Appendix A.\n\nAlthough this bound is loose and requires approximations based on assumptions, the proposition supports our intuition: if we prob the head for too long (i.e. a large τ ), the HP-train-acc can be very high, hence on average p0 is close to ey and the features adapt less, and vice versa. Our intuition is further verified in the right panel of Figure 2 where we plot the predicted probability of the correct class, i.e. [p(n) 0 ]yn, for each sample. Furthermore, we find that it is unlikely to obtain zero energy (not as assumed by Kumar et al. (2022)) in many practical applications: even with high accuracy there will always be some gap between p0 and ey by definition, and moreover the training accuracy after HP is sometimes far from perfect (see Section 5).\n\n4.2 NON-TRIVIAL TREND OF FEATURE ADAPTATION\n\nFollowing Proposition 1, we can link the adaptation of features to τ via AIE. In the proof of Proposition 1, we disentangle the dependence of the direction and energy terms using Cauchy-Schwarz\n\n1The case for MSE loss, as well as the derivation of this equation, can be found in Appendix A. 2In a linear model z = Bx, κ(j,n) 3Note that ∇zq(n)\n\n= (x(j))⊤(x(n)) · Ih×h is invariant during FT (i.e., independent of t). depends on the parameters of the current task head vt in the generally, whereas ∇zq(n)\n\nt\n\nt\n\nt =\n\nv⊤ is independent of n if the model is linear.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: How zt changes from z0. The ellipses in the third panel represent the scattered features. The scatter plots are the PCA projection of the resulting zt after finetuning. For tiny, mild, and strong adaption, we use τ = 1024, 64, 0, respectively. The heat-maps of the last column are the “head-exchange” experiments (all under the Toy+MLP settings).\n\ninequality, i.e., (∇zq)⊤(ey − p0) ≤ ∥∇zq∥2 · ∥ey − p0∥2. However, recall that the direction term Equation (2) in also plays an important role, especially at the beginning of finetuning.\n\nTo verify our hypothesis about the direction term, we depict the change of this term during finetuning in the right panel of Figure 2. Suppose a linear task-head (non-linear heads have similar behavior in the NTK regimes), this quantity can be approximated by the norm of the gradients to vt, since ∥∇zqt+1 − ∇zqt∥2 2. As we find ∥vt∥2 changes little during the finetuning stage, the large ∥∇vL∥2 is more likely from a big direction change. As illustrated by Figure 2, when τ = 0, ∇zqt changes a lot at the beginning of FT, which can make z(n) change in inconsistent directions. When τ = 1024, the direction term changes only a little through finetuning.\n\n2 = γ2∥∇vL∥2\n\nF = ∥v⊤\n\nt+1 − v⊤\n\nt ∥2\n\nThis finding inspires us to look deeper at what is the difference between a strong adaptation (e.g., τ = 0) and a mild adaptation (e.g., τ = 4). To get an overall picture of z’s change, only ∥zT − z0∥2 is not enough. Hence we analyze the following four quantities related to the similarity between the features before finetuning z0 and afterwards zT : ∥zT − z0∥2, ∥zT ∥2, z⊤ T z0, and cos(zT , z0). With an overparameterized model, we can analytically calculate the expressions of them and make the following conclusion: Proposition 2 (Informal). In an overparameterized two-layer linear model, when τ increases (the AIE decreases), ∥zT − z0∥2 T z0 exhibit a quadratic trend. The trend of cos(zT , z0) is hard to predict, but there is a phase that this value increases fast.\n\n2 monotonically decreases while ∥zT ∥2\n\n2 and z⊤\n\nSuch a trend is illustrated in Figure 3, supported by various experiments in Appendix C, and strictly proved in Appendix D. With this information, we can infer the behavior of zT and sketch it in the second and the third panels. In the “tiny” case, when vτ 0 is fully converged (τ → ∞), the features are almost stable during finetuning; this only works well when the pretrained features are perfect for the downstream task. In the “mild” case, when τ is reasonably large, the resulting zT will be stretched (∥zT ∥2 increases) in a similar direction (cosine changes little). This kind of z can make the original overlapped features become more separable without changing the manifold of the features too much, which is desirable in most transfer learning scenarios. For the “strong” case, where we only HP for a few updates or simply use a random head, zT will change in an unpredictable way, especially for the early updates. Although the fine-tuned model may generalize well, zT might be quite different from z0. Thus, if we believe the pretrained features are too specific to the pretraining dataset, the “strong” case is a reasonable choice.\n\nNote that even though the scatter plots of the mild and strong cases look similar, the corresponding feature manifold might be quite different. To verify this, we first run HPτ -FT (i.e., load the pretrained model, prob the head for τ epochs, then finetune to converge) for 7 different τ . Then we save the converged backbone and task-head separately for each τ , and pair-wisely exchange the head and backbone to build 49 new models (without further tuning). The training and validation accuracy of the 49 models are reported in Figure 3. The results match well with our analysis: 1) the\n\n5\n\nPublished as a conference paper at ICLR 2023\n\noff-diagonal values are lower, which means the representations learned by experiments with different τ are different; 2) for the strong adapted backbones (the first two columns), the heads from other cases are not compatible, which means the features’ manifold are significantly changed; 3) for the mild and tiny cases (latter columns), the aforementioned incompatibility almost disappears, which means the features’ manifold of these cases are quite similar.\n\n4.3 BACKBONE DEPTH AND HEAD CAPACITY\n\nBeyond how long we train the task head, the structure and capacity of the task head also influence the training prediction after this stage, thus the adaptation of zt. We briefly discuss the trend here and verify them in Section 5. When using a low-capacity head (e.g. a linear head parameterized by a h×k matrix), it might be hard to preserve the pretrained z0 even a very large τ is chosen, as the head cannot achieve a high enough training accuracy to decrease AIE. On the other hand, if the capacity of the task head is much bigger than the backbone, the information from the pretrained network might be easily distorted. For example, consider using only the first block of a pretrained ResNet18 as the backbone and concatenating the other 3 blocks with a 10-layer wide MLP. If this huge task head is randomly initialized, the information contained in the backbone could be completely rewritten, as the random changing phase of zt can be very long (remember that in Equation (2), zt will change in random directions before ∇vqt becomes less noisy).\n\n5 EXAMPLES ON REAL TASKS\n\nIn this section, we first provide a “user guide” of all the aforementioned methods. Note that given the pretrained and downstream tasks, determining the optimum adaptation energy is rather heuristic: the user guide only provides some high-level suggestions for the practitioners. We then provide abundant real examples of how to apply these principles. Generally, we can first try sweeping the optimal τ using linear head and consider other advanced tricks if necessary.\n\n5.1 USER GUIDE\n\nThe goal of this paper is to provide a toolbox for preparing the task-head before finetuning. Here, we suggest a “phenomenon → hypothesis → solution” workflow and use the validation performance for verification. Recall how we mitigate the overfitting problem using dropout: validation accuracy decrease after convergence → model overfitted → add dropout. Similarly, we can have: HP trainacc converge to 100% → no enough energy → use smaller τ .\n\nHowever, in practice, it is unknown that how much energy is beneficial, as the neural network might not encode the input samples as we humans expected. Hence, we suggest starting from the basic setting, i.e. using a linear head and sweeping τ to get a high-level understanding of how the energy influence the downstream generalization. Usually, selecting the optimal τ ∗ using validation accuracy can ensure a reasonably good test performance, as verified in the next subsection.\n\nThe advanced tricks are only applicable to specific scenarios and need more consideration. For example, if we really want tiny energy but using the linear head only achieves less than 50% training accuracy after head probing, we can consider an MLP head to increase the converged training accuracy (hence reduce the energy). If we want a mild adaptation, but the training accuracy during HP goes to 100% too fast, using label smoothing during HP can be considered. If we believe the downstream task only needs low-level features of the backbone, partial-backbone is a good choice. However, these advanced tricks also have side effects that are deleterious to the downstream performance. We will analyze their advantage and limitations in Section 5.3 with concrete examples.\n\n5.2 BASIC METHOD: EARLIER STOPPING HP\n\nFollowing our theory, when the learning rate for HP is fixed, the value of τ is positively correlated with the training accuracy, thus negatively correlated with energy. Hence, given a pretrained backbone and a downstream task, we can always start from sweeping τ (see Figure 4).\n\nAs stated in the motivation part, during finetuning, we usually expect some feature adaptation while keeping some pretrained information. Hence, neither τ = 0 nor τ = ∞ is optimal. We find such a\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Sweeping τ from 0 to 200 (in 2n fashion). The valid-accuracy of FT-only setting is the left most point in each panel. The first 4 columns are on image classification task, the fifth one is on graph task, and the last one is on image segmentation task.\n\ndownstream data:\n\nFlowers\n\nSTL10\n\nCIFAR100\n\npretraining task-data: MoCo-IN Sup-IN Sup-C10 MoCo-IN Sup-IN Sup-C10 MoCo-IN Sup-IN Sup-C10\n\nHP-train-acc HP0-FT HP200-FT HPτ ∗-FT lsHP200-FT\n\n59.711 76.953 84.882 86.831 86.946\n\n85.826 91.295 91.653 92.299 92.746\n\n7.031 60.714 43.973 63.711 42.299\n\n91.889 96.136 96.363 96.753 96.639\n\n96.291 97.452 97.374 97.697 97.959\n\n75.181 86.811 84.778 87.739 85.912\n\n56.397 80.432 83.744 83.814 83.412\n\n62.709 84.736 84.347 84.971 85.357\n\n11.735 65.608 63.992 65.966 65.244\n\nTox21 Sup-pcba 79.121 81.274 79.666 83.263 83.853\n\nTable 1: Downstream test accuracy across different settings. τ ∗ is selected based on validation accuracy in sweeping. The HPτ ∗ always bring some improvement. The lsHP can further improve when the HP-only method achieves a high accuracy (no enough energy for adaptation), but fails when the pretraining features are not suitable (in Sup-C10 case). See Section 5.3 for more discussions.\n\ntrend is consistent across various settings: from image input to graph input, from classification task to segmentation task, and from supervisory pre-training to unsupervised pre-training.\n\nOther than this general trend, the nuance of experiments under different settings also supports our analysis well. Specifically, in the image classification experiments shown in Figure 4, adapting a ResNet50 to STL10 behaves differently on different pretraining task. In the first column, we see a large τ (small energy) hurts the downstream generalization performance, because the features pretrained on CIFAR10 might be far from optimal for the downstream task. In contrast, the features pretrained on ImageNet-1K (IN1K for short) all provide good results. Among those IN1K pretrained models, the model from a supervised classification task leads the best overall downstream performance, but large τ is still harmful. In other words, the features pretrained this way might be somewhat too specific to the pretraining dataset, and hence mild adaptation is beneficial. Regarding the unsupervised pretraining cases, BYOL (Grill et al., 2020) is less sensitive to the choice of τ , while MoCo (He et al., 2020) behaves more similarly to the supervised case.\n\nAnother interesting finding is that the HP training accuracy at the optimal τ ∗ is usually smaller than the converged value: we cannot select τ based on the standard early stopping criterion on HP train accuracy. As the task head under τ ∗ usually has not converged on the pretraining dataset, we call this method “earlier stopping HP” (HPτ ∗-FT for short). As shown in Table 1, HPτ ∗-FT can always bring improvements.\n\n5.3 ADVANCED TRICKS: SUPERIORITY AND LIMITATION\n\nMLP Head: Instead of using a linear head, authors of Chen et al. (2020b) claim that using an MLP head can sometimes bring improvement. Following the analysis of this paper, we can consider this trick when we want small energy while the linear head cannot reach a high HP training accuracy even after a long HP. For example, in the first two columns in Table 2, the HP-train-acc in the linear head cases plateaued after reaching 92% or 78%, while a 2-layer MLP can reach 99% and 95%. As the energy decreases, features adapt less during finetuning (see the decreased distance metrics) and the final models generalize better.\n\nHowever, we should be careful when applying this trick if we want to use a small τ (i.e., large energy). Recall our analysis that the inconsistent direction term (i.e., ∇zqt defined in Equation (2)) makes the feature adaptation more unpredictable at the beginning of finetuning. Increasing the head capacity in this case would make the head converge slower and hence prolong such a chaos phase.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nHP-train-acc 1-cos(zT , z0) ∥zT − z0∥2 2\nFT-val-acc HP-train-acc 1-cos(zT , z0) ∥zT − z0∥2 2\nFT-val-acc\n\nBaseline Linear-head\n\n2MLP-head\n\nSim-Real 92.676 0.1269 5.247 78.075 99.121 0.1098 4.942 78.453\n\nSup-Sketch 78.213 0.1422 4.752 61.545 94.883 0.1104 4.206 63.773\n\nSim-STL Sup-STL\n\nSim-STL Sup-STL\n\nBaseline Small energy ηF T = 1 ηHP = 1\n\nMore energy ηF T = 1 ηHP = 0.9\n\n96.875 0.0044 1.792 93.914 96.875 0.014 1.841 94.304\n\n100 0.017 6.716 97.581 100 0.0549 9.278 97.92\n\nSmall energy ηF T = 0.9 ηHP = 0.9\n\nOpposite Energy ηF T = 0.9 ηHP = 1\n\n97.607 0.0049 1.731 94.015 97.982 0.0109 3.041 93.208\n\n100 0.0256 7.391 97.694 100 0.0849 13.881 97.039\n\nTable 2: How ls-HP and larger head influence the feature adaptation. The models are pretrained using SimCLR (Sim) or supervised (Sup) classification on IN1K. Number in blue (red) represent an decrease (increase) compared with its counterpart in the baseline. See more results in Appendix C.\n\nLabel Smoothing HP: Recall that the energy is upper bounded by ∥ey − p0∥2 2, where p0 is the model’s prediction after HP for τ epochs. p0 converges to the labels used in HP when τ → ∞. Hence, instead of changing τ , we can also manipulate the labels in HP to achieve a similar goal. One simple yet effective way is label smoothing (M ̈uller et al., 2019, e.g.). By setting the labels during HP as ηHP ey + (1 − ηHP ) ∗ u, where u is a uniform K-class categorical distribution, the HP stage can always reserve at least (1 − ηHP ) ∗ ∥ey − u∥2 energy for the following feature adaptation, even τ → ∞. Such a trick (lsHP for short) is quite helpful when the HP-train-acc converges to 90%+ very fast, yet we still want a mild adaptation, like the example shown in the second two columns in Table 2. With lsHP, we see that the features adapt more even the HP-train-accs are unchanged.\n\nTo verify that the aforementioned improvement comes from the reserved energy during lsHP, we further try using smoothed labels during finetuning (e.g., ηF T = 0.9). The results match our analysis: when ηHP = ηF T = 0.9, the reserved energy disappears, as the labels of the two phases are the same again. Hence, all the numbers under this condition are quite similar to the baseline case (ηHP = ηF T = 1). For the “opposite energy” case, we observe a larger adaptation but a worse generalization performance. That is because the reserved energy make the features adapt in opposite directions. These results remind us that if we decide to use smooth label in both finetuning and head probing (e.g., we assume most of the samples in the downstream dataset are ambiguous), we need to set ηHP ≤ ηF T to ensure a correct direction.\n\nIn summary, the lsHP trick is suitable for scenarios where the pretrained features are pretty well and the standard HP converges to 90%+ very fast. When the HP-train-acc is too low, the assumption used in HP, i.e. p0 converges to the labels, no longer holds. Hence, lsHP does not always bring enhancement, an example is given in the last row in Table 1.\n\nPartial Backbone:\n\nThis is a more intricate trick that requires a stronger and more heuristic hypothesis. For example, there is a common belief in the deep-learning community that the lower layers extract fundamental features (e.g. edges, curves, and texture in vision tasks) while the higher layers learn more semantic features (e.g. dog heads or wheels) (Baldock et al., 2021; Olah et al., 2020). Hence, if we believe the downstream task treats the low-level features as beneficial while the high-level features are harmful, reinitializing the higher layers (like the fortuitous forgetting mentioned by Zhou et al. (2022)) and incorporating them as part of the task head can be beneficial.\n\nHP-train-acc -L4.3 -L4.2, -L4.1 FT-val-acc -L4.3 -L4.2 -L4.1\n\nReal 92.676 100 100 78.075 78.528 78.427 76.689\n\nSketch Quick 64.307 78.213 97.666 100 100 100 59.703 61.545 65.683 67.011 67.087 65.336 68.07 65.017\n\nTable 3: ResNet50 pretrained on IN1K using SimCLR. -L4.x means reinitializing the blocks in the resnet (merge them to task head) until layer 4.x.\n\nSee the results in Table 3, where the Quick4 dataset is likely to rely more on the low-level features learned during pretraining. So the optimal setting for the Quick column is removing information from the last three layers (i.e., L4.3, L4.2, and L4.1) in ResNet50, while the optimal setting for the other two cases is to reinitialize only the last layer (i.e., L4.3). Using this trick, the task-head capacity might increase significantly (see the HP-train-acc increase to 100% after reinitializing). Hence, the principles discussed in the MLP-head trick also hold here. However, as it is hard to figure out what are the beneficial features for downstream tasks, such a trick has the lowest priority in our toolbox.\n\n4Refer to Figure 8 to get an intuition about what the samples in theses dataset look like.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n6 RELATED WORK AND DISCUSSIONS\n\nHP, FT and HP-FT. Head probing and fine tuning are two fundamental algorithms in transfer learning, which have also attracted much discussion and debate. Intuitively, by freezing the pretrained backbone, HP will re-combine the features without distorting them, and hence yields better performance than FT when features are perfect (Peters et al., 2019). However, when pretrain and downstream tasks are very different, adapting the features is important and FT outperforms HP (Chen et al., 2020b; Zhai et al., 2019; He et al., 2022). Combining the strengths of HP and FT, authors of Kumar et al. (2022) demonstrate that HP-FT (i.e., first HP, then FT) yields the best performance on both in-distribution and out-of-distribution cases. Although they provide some theoretical guarantees for the superiority of HP-FT, some of their assumptions are dubious. To shed more lights on the pros and cons of these two methods, this paper analyze in detail how the features change under different task-head settings. Specifically, by controlling the HP epochs τ or otherwise influencing the energy term, we can design different types of feature adaptations for different downstream tasks. The relationship between the choice of task head and resulting z’s adaptation is explained using Equation (2), verified by various experiments, and proved in an overparameterized linear model. Although the paper only analyze these fundamental settings, we believe the analysis provided here can also be combined with (or explain the benefits of) other more complex finetuning mechanisms, like those of Guo et al. (2019); Zhang et al. (2020a); Aghajanyan et al. (2021); Howard & Ruder (2018). For example, a common practice in transfer learning is to use a small learning rate for the backbone and a large learning rate for task head. From our perspective, this method can weaken the influence of the “noisy direction” term and make the features adapt in a low energy condition.\n\nBackbone and head depth. Besides the influence of the initial value of the head, this paper also discusses the influence of the relative capacity between backbone and head. Chen et al. (2020b) show that a simple MLP head sometimes also brings enhancement. When the backbone only copies the early layers of the pretrained network, as the downstream task might need different levels of features (Olah et al., 2020; Baldock et al., 2021; Zhang et al., 2020b), the later layers and the original head can be combined as the new complex task-head, like the example in Figure 1. Furthermore, in some encoder-decoder style models, like some the models for some language tasks (Peters et al., 2019; Zhu et al., 2019), the head (decoder) might have comparable capacity with the backbone (encoder). In another extreme case, if we use pretrained word2vec features (Mikolov et al., 2013) and plan to FT them in a downstream task, the task head is the whole network, which is much bigger than the embedding layer. We believe the discussion in this paper might also help inspire the design of HP-FT strategy for these practical scenarios.\n\n7 CONCLUSION\n\nThis paper studies how the choice of task heads influence the pretrained features z’s adaptation and hence influence the downstream performance. By decomposing the learning dynamics of z, we find the keys are the energy and direction terms, which highly correlate with the accuracy at the beginning of the FT process (i.e., the accuracy after HP). Hence under a common HP-FT framework, we carefully analyze the relationship between the HP epochs (τ ) and the features’ adaptation (discrepancy between zT and z0). Different from most existing works, which mainly focus on Euclidean or cosine distance, we further analyze z⊤ 2 to depict a more comprehensive relationship between τ and zT , like the examples in Figure 3. This non-trivial trend is strictly proved in an overparameterized linear model and experimentally verified under different practical settings.\n\nT z0 and ∥z0∥2\n\nBased on these observations, we speculate that a suitable adaptation of zT is beneficial when the pretrained features are not perfect for the downstream tasks. Under different experimental settings, we illustrate how to achieve better downstream performance by controlling the adaptations of zT (using early-stopping HP, label smoothing during HP, or manipulating the head’s capacity).\n\nFinally, there are still many open questions and phenomena not address by this study. For example, methods to quantitatively or even adaptively analyze the discrepancy between the pretrain and downstream tasks would be very useful in knowing what kind of head probing to perform in advance. However, we believe the methods proposed in this paper can help provide a new perspective on understanding feature adaptation, which we hope will aid future work in this area.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal Gupta. Better fine-tuning by reducing representational collapse. In International Conference on Learning Representations, 2021.\n\nRobert Baldock, Hartmut Maennel, and Behnam Neyshabur. Deep learning through the lens of\n\nexample difficulty. Advances in Neural Information Processing Systems, 2021.\n\nL.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethinking Atrous Convolution for Semantic In Proceedings of the IEEE/CVF Conference on Computer Vision and\n\nImage Segmentation. Pattern Recognition, 2017a.\n\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, pp. 834– 848, 2017b.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020a.\n\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\n\nImproved baselines with momentum\n\ncontrastive learning. arXiv preprint, 2020b.\n\nLenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.\n\nAdvances in Neural Information Processing Systems, 32, 2019.\n\nAdam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nSimon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. Advances in Neural Information Processing Systems, 2018.\n\nM. Everingham, A. Eslami, L. Gool, C. Williams, J. Winn, and A. Zisserman. Pascal Vis. Obj. Class\n\nChallenge: A Retrospective. IJCV, 2015.\n\nMario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training in deep neural networks. Journal of Statistical Mechanics: Theory and Experiment, 2020 (11):113301, 2020.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020.\n\nYunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and Rogerio Feris. Spottune: transfer learning through adaptive fine-tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4805–4814, 2019.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on\n\nunsupervised visual representation learning. computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nTong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks In Proceedings of the IEEE/CVF\n\nfor image classification with convolutional neural networks. Conference on Computer Vision and Pattern Recognition, pp. 558–567, 2019.\n\nJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification.\n\nACL, 2018.\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.\n\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171– 4186, 2019.\n\nThomas N. Kipf and Max Welling.\n\nSemi-supervised classification with graph convolutional In International Conference on Learning Representations, 2017. URL https:\n\nnetworks. //openreview.net/forum?id=SJU4ayYgl.\n\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained In 4th International IEEE Workshop on 3D Representation and Recognition\n\ncategorization. (3dRR-13), Sydney, Australia, 2013.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nAnanya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can International Conference on\n\ndistort pretrained features and underperform out-of-distribution. Learning Representations, 2022.\n\nYann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.\n\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. Advances in neural information processing systems, 2019.\n\nHao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Rethinking the hyperparameters for fine-tuning. In International Conference on Learning Representations, 2020.\n\nShengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. In International ConferPre-training molecular graph representation with 3d geometry. ence on Learning Representations, 2022. URL https://openreview.net/forum?id= xQUe1pOKPam.\n\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-\n\ntations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\nRafael M ̈uller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Ad-\n\nvances in neural information processing systems, 2019.\n\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722–729. IEEE, 2008.\n\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\n\nZoom in: An introduction to circuits. Distill, 2020.\n\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1406–1415, 2019.\n\nMatthew E Peters, Sebastian Ruder, and Noah A Smith. To tune or not to tune? adapting pretrained\n\nrepresentations to diverse tasks. ACL, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513–530, 2018.\n\nGreg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint\n\narXiv:2011.14522, 2020.\n\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint, 2019.\n\nJeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning: a baseline for network adaptation via additive side networks. In European Conference on Computer Vision, pp. 698–714. Springer, 2020a.\n\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. Revisiting few-\n\nsample bert fine-tuning. In International Conference on Learning Representations, 2020b.\n\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. Revisiting fewsample {bert} fine-tuning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=cO1IH43yUF.\n\nHattie Zhou, Ankit Vani, Hugo Larochelle, and Aaron Courville. Fortuitous forgetting in connec-\n\ntionist networks. International Conference on Learning Representations, 2022.\n\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tieyan Liu. Incorporating bert into neural machine translation. In International Conference on Learning Representations, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: The flow of this paper. Code is available at https://github.com/Joshua-Ren/ how_to_prepare_taskhead.\n\nA DECOMPOSITION OF CHANGE OF Z\n\nProof of Equation (2): Recall z = fB(x), q = gz(z) and p = Softmax(q). Use b ∈ Rhd×1 to represent the vector form of B ∈ Rh×d. The MSE loss on one sample is Lmse 2. The cross-entropy loss on one sample is Lce k pk log qk is the cross entropy for categorical distribution. Using 1st Taylor expansion, we have:\n\n(cid:1) = H (cid:0)eyn , q(n)(cid:1), where H(p, q) = − (cid:80)\n\n2 ∥q(n) − eyn ∥2\n\n(cid:0)q(n), eyn\n\n(cid:0)q(n), eyn\n\n(cid:1) = 1\n\nz(0) t+1 − z(0) (cid:125) (cid:123)(cid:122) (cid:124) h×1\n\nt\n\n= ∇bz(0) (cid:124) (cid:123)(cid:122) (cid:125) h×hd\n\nt\n\n· (bt+1 − bt) (cid:125) (cid:123)(cid:122) hd×1\n\n(cid:124)\n\n+O (cid:0)∥bt+1 − bt∥2(cid:1) .\n\nWe then calculate bt+1 − bt assuming the parameters are updated in batch-SGD:\n\n= −\n\nbt+1 − bt (cid:125) (cid:123)(cid:122) (cid:124) hd×1\n\n\n\n \n\n\nγ N\n\na\n\nN (cid:88)\n\nn=1\n\n∇bL (cid:124)\n\n(cid:16)\n\nq(n) t\n(cid:123)(cid:122) 1×hd\n\n\n\n⊤\n\n(cid:17)\n\n, eyn\n\n \n\n\n(cid:125)\n\n= −\n\n= −\n\nγ N\n\nγ N\n\n\n\n \n\n\nN (cid:88)\n\nn=1\n\n∇qL (cid:124)\n\n(cid:16)\n\nq(n) t\n(cid:123)(cid:122) 1×k\n\n, eyn\n\n(cid:17)\n\n(cid:125)\n\n· ∇zq(n) (cid:124) (cid:123)(cid:122) (cid:125) k×h\n\nt\n\n\n\n⊤\n\n∇bz(n) (cid:124) (cid:123)(cid:122) (cid:125) h×hd\n\nt\n\n \n\n\nN (cid:88)\n\n(cid:16)\n\nn=1\n\n(cid:124)\n\n∇bz(n) t\n(cid:123)(cid:122) hd×h\n\n(cid:17)⊤\n\n(cid:16)\n\n(cid:125)\n\n(cid:124)\n\n∇zq(n) t\n(cid:123)(cid:122) h×k\n\n(cid:17)⊤\n\n(cid:16)\n\n(cid:16)\n\n∇qL\n\n(cid:125)\n\n(cid:124)\n\n, eyn\n\nq(n) t\n(cid:123)(cid:122) k×1\n\n(cid:17)(cid:17)⊤\n\n(cid:125)\n\n(3)\n\n(4)\n\n(5)\n\n(6)\n\nFor different loss functions, we have different expressions for the last term:\n\n∇qLmse\n\n(cid:16)\n\nq(n)\n\nt\n\n, eyn\n\n(cid:17)\n\n(cid:16)\n\n=\n\nq(n)\n\nt − eyn\n\n(cid:17)\n\n; ∇qLce\n\n(cid:16)\n\nq(n)\n\nt\n\n, eyn\n\n(cid:17)\n\n(cid:16)\n\n=\n\np(n)\n\nt − eyn\n\n(cid:17)\n\n,\n\n(7)\n\nwhere q is the logits and p = Softmax(q) is the predicting probability. We see these two kinds of loss have similar expression on this term, so we only explain the cross-entropy version (as it is more common in practices) in the main content.\n\nUsing the above expressions, we can first bound the high-order term (cross-entropy version):\n\nO(∥bt+1 − bt∥2) = O(γ2∥∇zq(n)\n\nt ∥2\n\nop · ∥∇bz(n)\n\nt ∥2\n\nop · ∥q(n)\n\nt − eyn ∥2\n\nop) = O(γ2),\n\n(8)\n\nas long as the hyperparameters are appropriately chosen and the loss doesn’t blow up (or gradient clipping is applied) in FT stage.\n\n13\n\n1. Theory in overparameterizedlinear example2. Decomposition in Eq.23. Correlation between HP-train-acc(gap) and dist(zt, z0)4. Further decompose dist(zt, z0)5. Trends holds for different settings (MLP, CNN, GNN)1. Usually mild adaptation is beneficial2. Better features need less adaptation3. Other designs like label-smoothing HP,MLP-head, latter layers forgettingStructure and value of v0Resulting ztID valid accuracyEnergy, direction(ey−p0)∇zq0Depends on PT/DS tasksPublished as a conference paper at ICLR 2023\n\nFinally, combining all the expressions, Equation (10) can be rewritten as:\n\nt+1 − z(0) z(0) (cid:124) (cid:125) (cid:123)(cid:122) h×1\n\nt\n\n= −\n\nγ N\n\nN (cid:88)\n\n(cid:16)\n\nn=1\n\n(cid:124)\n\n∇bz(0) t\n(cid:123)(cid:122) h×hd\n\n(cid:17)\n\n(cid:16)\n\n(cid:125)\n\n(cid:124)\n\n∇bz(0) t\n(cid:123)(cid:122) hd×h\n\n(cid:17)⊤\n\n(cid:16)\n\n(cid:125)\n\n(cid:124)\n\n∇zq(n) t\n(cid:123)(cid:122) h×k\n\n(cid:17)⊤\n\n(cid:16)\n\n(cid:125)\n\n(cid:124)\n\n∇qLce\n\n(cid:16)\n\nq(n) t\n(cid:123)(cid:122) k×1\n\n(cid:17)(cid:17)⊤\n\n, eyn\n\n+O(γ2)\n\n(9)\n\n(cid:125)\n\n=\n\nγ N\n\nN (cid:88)\n\nn=1\n\n·\n\nt\n\nκ(0,n) (cid:124) (cid:123)(cid:122) (cid:125) h×h\n\n(cid:16)\n\n(cid:124)\n\n∇zq(n) t\n(cid:123)(cid:122) h×k\n\n(cid:17)⊤\n\n(cid:16)\n\n(cid:125)\n\n(cid:124)\n\neyn − p(n) (cid:123)(cid:122) k×1\n\nt\n\n(cid:17)\n\n(cid:125)\n\n+O(γ2),\n\n(10)\n\nwhich is the same with Equation (2) in the main context.\n\nProof of Proposition 1:\n\nIt is easy to get z(j)\n\nT − z(j)\n\n0 by stacking the LHS of Equation (2) (or Equation (10)) under different t:\n\nT − z(j) z(j)\n\n0 =\n\nT −1 (cid:88)\n\nt=0\n\nγ N\n\nN (cid:88)\n\nn=1\n\n= γ\n\nT −1 (cid:88)\n\nt=0\n\nE\n\nx(n)\n\n(cid:18)\n\nκ(j,n)\n\nt\n\n(cid:20)\n\nκ(j,n)\n\nt\n\n(cid:16)\n\n·\n\n∇zq(n)\n\nt\n\n(cid:17)⊤\n\n(cid:16)\n\n·\n\neyn − p(n)\n\nt\n\n(cid:17)(cid:19)\n\n+ O(γ2)\n\n(cid:16)\n\n·\n\n∇zq(n)\n\nt\n\n(cid:17)⊤\n\n(cid:16)\n\n·\n\neyn − p(n)\n\nt\n\n(cid:17)(cid:21)\n\n+ O(γ2)\n\n(11)\n\n(12)\n\n= γ · E\n\nx(n)\n\n(cid:34)\n\n(cid:34)\n\n= γ · E\n\nx(n)\n\nκ(j,n) ·\n\nT −1 (cid:88)\n\n(cid:16)\n\nκ(j,n) ·\n\n∇zq(n)\n\nt\n\n(cid:17)⊤\n\n(cid:16)\n\n·\n\neyn − p(n)\n\nt\n\n(cid:35)\n\n(cid:17)\n\n+ O(γ2)\n\n(13)\n\nt=0\n\nT −1 (cid:88)\n\nt=0\n\n(cid:16)\n\nv⊤\n\nt ·\n\neyn − p(n)\n\nt\n\n(cid:35)\n\n(cid:17)\n\n+ O(γ2),\n\n(14)\n\nwhere the first equation follows definition. The second equation is assuming a uniform training sample distribution, i.e., p(x) = 1 N . The third equation follows the slow-change NTK assumption, hence κ(j,n) no longer depends on t. The last equation follows the linear-head assumption, i.e., ∇zq(n)\n\nt = vt.\n\nWe cannot go further as there are three matrices (or vectors) in the expectation. Hence we instead analyze the F-norm (i.e., L2-norm for the vector z) of the features’ change.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\n∥z(j)\n\nT − z(j)\n\n0 ∥2 = γ ·\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nE\n\nx(n)\n\n(cid:34)\n\nκ(j,n) ·\n\nT −1 (cid:88)\n\nt=0\n\n(cid:16)\n\nv⊤\n\nt ·\n\neyn − p(n)\n\nt\n\n(cid:17)\n\n(cid:35)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2\n\n+ O(γ2)\n\n+ O(γ2)\n\n(cid:17)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2\n\n+ O(γ2)\n\n(cid:17)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2\n\n(cid:17)(cid:13) (cid:13) (cid:13)2\n\n+ O(γ2)\n\n+ O(γ2)\n\nT −1 (cid:88)\n\n(cid:16)\n\nv⊤\n\nt ·\n\neyn − p(n)\n\nt\n\n(cid:17)\n\n≤ γ · E\n\nx(n)\n\n≤ γ · E\n\nx(n)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nκ(j,n) ·\n\n(cid:13) (cid:13)\n\n(cid:13)κ(j,n)(cid:13)\n\n(cid:13) (cid:13)2\n\nt=0 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n·\n\nT −1 (cid:88)\n\nt=0\n\n(cid:16)\n\nv⊤\n\nt ·\n\neyn − p(n)\n\nt\n\n≤ γ · C1 · E\n\nx(n)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nT −1 (cid:88)\n\nt=0\n\n(cid:16)\n\nv⊤\n\nt ·\n\neyn − p(n)\n\nt\n\nE\n\nx(n)\n\n(cid:16)\n\n(cid:13) (cid:13)v⊤ (cid:13) t ·\n\neyn − p(n)\n\nt\n\n= γ · C1 ·\n\n≤ γ · C1 ·\n\nT −1 (cid:88)\n\nt=0\n\nT −1 (cid:88)\n\nt=0\n\n≤ γ · C1 · C2 ·\n\nE\n\nx(n)\n\n(cid:13) (cid:13)v⊤\n\nt\n\n(cid:13) (cid:13)2 ·\n\n(cid:13) (cid:13)\n\n(cid:13)eyn − p(n)\n\nt\n\n(cid:13) (cid:13) (cid:13)2\n\n+ O(γ2)\n\nT −1 (cid:88)\n\nt=0\n\n(cid:13) (cid:13)\n\nE\n\nx(n)\n\n(cid:13)eyn − p(n)\n\nt\n\n+ O(γ2)\n\n(cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13)2\n\n≤ γ · C1 · C2 · T · E\n\nx(n)\n\n(cid:13) (cid:13)\n\n(cid:13)eyn − p(n)\n\n0\n\n+ O(γ2).\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\nHere the first equation is by definition. The second inequality follows triangle inequality. The third inequality follows Cauchy-Schwarz inequality. The forth inequality is assuming the F-norm of NTK is bounded by C1. The fifth equation is sweeping the summation order. The sixth inequality also follows Cauchy-Schwarz inequality. The seventh inequality assumes the norm of the last layer is bounded by C2. The eighth inequality is assuming a stable learning process where the gap between eyn and p(n) Finally, by taking expectation of all the input samples, we can have:\n\nkeeps decreasing during training.\n\n0\n\nE\n\nx(j) ∥z(j)\n\nT − z(j)\n\n0 ∥2 ≤ c · E\n\nx(n) ∥eyn − p(n)\n\n0 ∥,\n\n(23)\n\nwhere c = γ · C1 · C2 · T is a constant and E Proposition 1.\n\nx(n) ∥eyn − p(n)\n\n0 ∥ is the AIE term (Eaie) defined in\n\nVerification of Slow Kernel Change Assumption:\n\nFigure 6: In finetuning, the NTK of the backbone adapts quite slow if the model is pretrained.\n\nWe decompose the one-step dynamics of z(j) in Equation (2) into three parts, which represent kernel, direction, and energy respectively. In our analysis, we have a mild assumption that the kernel of the backbone, i.e., κ(j,n) ∈ Rh×h, changes slow during finetuning when the learning rate is small and the training is stable (the loss converges). Although there are several\n\n∇Bz(n)\n\n∇Bz(j)\n\n(cid:17) (cid:16)\n\n(cid:17)⊤\n\n=\n\n(cid:16)\n\nt\n\nt\n\nt\n\nt\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nworks supporting this assumption (Yang & Hu, 2020; Geiger et al., 2020), to make the paper more self-contained, we directly observe how this term adapts during pretraining in Figure 6.\n\nt\n\nWe measure the change of this term by calculating knorm = ∥κ(j,n) t+1 − κ(j,n) ∥F for each t during finetuning. As computing the empirical NTK on the whole dataset requires huge memory, we randomly sample 50 x as our “probing samples”, computing knorm and kgap on 2,500 different (x(j), x(n)) pairs, and then report their mean values at each t. To verify our assumption, we compare four different settings:\n\n∥F and kgap = ∥κ(j,n)\n\nt\n\n• In Rnd Backbone, we randomly initialize the whole network and train it on the downstream\n\ndataset;\n\n• In PT Backbone HPτ ep, we copy the pretrained backbone, attach a randomly initialized\n\nhead, and then HP for τ epochs.\n\nIn the first panel in Figure 6, we see the training accuracy of all these settings converge to 100% (they also have similar validation accuracy). In the second and third panel, we plot the change of knorm and kgap respectively. It is obvious that compared with using random backbone, the NTK of a pretrained backbone indeed changes slow during the finetuning stage.\n\nB EXPERIMENTAL SETTINGS\n\nB.1 TOY CASE\n\nThe experiments of the toy setting appear in Figure 2 and Figure 3. In the toy setting, we first pretrain a 4+1 (4 layers of backbone and 1 layer of task head) layers MLP on the full MNIST dataset (LeCun, 1998), with learning rate 10−3, with cosine scheduler until convergence. The hidden width is 128 and relu activation is applied for each layer in the backbone. In the downstream task, we only consider the first three classes and randomly select 1000 samples for each class (i.e., 3*1000 samples in total). We also apply random rotation and center crop augmentations to simulate the distribution shift in downstream task, like the right panel in Figure 7. The downstream linear task head is then a 128*3 matrix (weights) and a 128*1 vector (bias). To ensure a fine-grained observation of feature’s adaptation, we use a small constant learning rate (10−4) in all the experiments. The FT epoch is 50, and the HP epoch ranges from 0 to 50,000 (50,000 is simulating the v∞ hp case, usually train 0 to 1,024 epochs).\n\n(a) Pretrain dataset.\n\n(b) Downstream (HP/FT) dataset.\n\nFigure 7: Dataset for toy experiments. We use full MNIST dataset to pretrain the network. The downstream dataset is a subset of MNIST (only 0, 1 and 2) applying random rotation and zoom-in.\n\nB.2 PRACTICAL SETTINGS – CLASSIFICATION\n\nBesides the simple MNIST and MLP, we also verify our analysis in many real settings. To demonstrate the generality of our findings, we conduct experiments on image classification, image seg-\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: DomainNet dataset. Only three domains are used: ‘real’ is similar to IN1K, ‘sketch’ is less sensitive to color and background, ‘quick’ only contains some lines, which is quite different from IN1K.\n\nFigure 9: Introduction of models used in this paper.\n\nmentation, and molecular graph classification tasks. The qualitative trend is quite consistent across different tasks and networks.\n\nModel Structure:\n\nSpecifically, we consider ResNet18/34/50 for the image classification task. They all have 4 blocks, each containing 3 layers, as illustrated in Figure 9. For the linear task head (or an MLP head), we directly concatenate a linear layer (or an MLP layer) to the backbone. For the partial backbone method, several layers in the last block (e.g., L4.3, L4.2, L4.1) might be reinitialized and merged to the task head. We also provide what is the backbone (all the blue squares) when self-supervisedlearning is considered, e.g., SimCLR, BYOL, and MoCo. In short, after pre-training, the backbones under different PT tasks might have the same structure.\n\nPretraining tasks:\n\nWe pretrain the ResNet on CIFAR10 using common recipe (10−2 learning rate with a cosine scheduler (He et al., 2019), 5 ∗ 10−4 weight decay, simple augmentations like random flipping and cropping, etc.). For the model pretrained on ImageNet, we directly download the checkpoints from open-source implementations. Due to time and computing resource limitations, for vision tasks, we only consider MoCo (He et al., 2020), Byol (Grill et al., 2020), SimCLR (Chen et al., 2020a), and\n\n17\n\nsketchairplanealarm-clockangleapplerealquickbananabasketballbicycleBasic setting:SimCLR:XT1T2X1X2ResNet-BackboneResNet-Backbonez1z2Projection HeadProjection HeadLossBYOL:XT1T2X1X2BackboneEMABackbonesg(z2#)Projection HeadLossPredictionHeadProjection Headz1XB3L4.3L4.2L4.1B2B1ResNet-BackbonezTask headLossMoCo:XBackboneMemory bankSampleLossqkImage Segmentation:XB3L4.3L4.2L4.1B2B1ResNet-BackbonezTask headLossPartial Backbone:XB3B2B1ResNet-BackbonezLossB4ASPPCOnv1Conv2UpsamplePublished as a conference paper at ICLR 2023\n\nsupervised classification tasks for pretraining. For the graph dataset case, we consider a fundamental 5-layers graph convolutional network (GCN, (Kipf & Welling, 2017)), the hyper-parameters and other detailed settings can be found in our github repo. The datasets applied in this paper are listed in Table 4. Note that the backbone of image segmentation is the same as that in image classification. The segmentation head is more complex, which will be discussed later.\n\nHyper-parameters for HP and FT:\n\nIn this paper, experiments with the same setting in different figures or tables share the same set of hyper-parameters. Generally, for all the experiments, the batch size is 128, hidden layer width is 256 (in the MLP head case). The input image size for CIFAR pretrained model is 32 × 32 while that for the IN1K pretrained model is 224 × 224. Hence in the corresponding experiments, we will first resize the input samples and then apply random cropping and random flipping augmentations. For the HP phase, the τ ∗ is selected based on the validation performance, but the detailed learning rate and maximum τ might be different under different settings (as the dataset size are different). For the FT phase, we use a standard SGD with momentum (β = 0.9). The default learning rate is 10−3 and a cosine scheduler is applied (the maximum FT epoch also varies for different downstream datasets). Note that we will early stop the FT process, hence the maximum FT epoch doesn’t influence the reported performance a lot. For those dataset-dependent hyper-parameters, we summarize them as:\n\n• For the insufficient pretrained backbone cases, i.e., Res50-C10(Sup), Res18-C10(Sup), in Figure 4, Table 1 and other related experiments in the appendix, we set τ ∈ [0, 200], HP learning rate is 3 ∗ 10−2, maximum FT epochs is 200 (usually converge less than 100 epochs), with usually a larger initial FT learning rate (e.g., 3 ∗ 10−3);\n\n• For the IN1K pretrained backbone (no matter by supervisory training, MoCo, Byol or SimCLR), which can be found in Figure 1, 2, 4, Table 1 and many figures in the appendix, we usually set τ ∈ [0, 200], HP learning rate as 10−2, maximum FT epochs as 200, and a relative small FT learning rate (e.g., 3 ∗ 10−4);\n\n• For any experiments whose downstream tasks are Flowers, Cars (the dataset is small while number of classes is large), we consider to increase the HP learning rate to 5 ∗ 10−2, and the maximum FT epochs to 1000 (usually converge less than 200 epochs);\n\n• For any experiments whose downstream task is CIFAR (the dataset is bigger than STL), we consider to increase the HP learning rate to 5 ∗ 10−3, and the maximum FT epochs to 100 (usually converge less than 50 epochs);\n\nIn summary, a general principle for these hyper-parameter selections is that the maximum τ should make the HP training accuracy converge (increase slowly for several consecutive epochs), while the finetuning epochs should make the training accuracy converge when τ = 0. For example, if the downstream task is CIFAR10, which has 50,000 training samples and only 10 classes, finetune 20 epochs is enough. But CIFAR100, which also has 50,000 training samples but with more classes, 50 finetuning epochs are required (we set the maximum FT epochs as 100 for both CIFAR10 and CIFAR100). For Flowers102, which only has 6,149 training samples but 102 classes, we set the maximum FT epochs as 1,000 epochs. Anyway, we find the proposed trend is quite robust to these hyper-parameters. The detailed settings can be found in our code base.\n\nB.3 PRACTICAL SETTINGS – IMAGE SEGMENTATION\n\nIn addition to the image classification tasks, we also conduct experiments on image segmentation tasks to verify the robustness of our analysis on various tasks. An image segmentation task is a task where a model is trained to make class predictions at each pixel of an image given an input image. We train and test segmentation models on PASCAL VOC (Everingham et al., 2015) dataset, one of the most popular image segmentation datasets. An example of input image and segmentation label are provided in Figure 10.\n\nModel Structure:\n\nFor image segmentation tasks, we employ DeeplabV3 (Chen et al., 2017a) with ResNet50 as a backbone. As with the classification tasks, ResNet50 consists of 4 blocks, each of which contains 3 layers. But unlike the classification tasks, kernel strides for convolutional layers are adjust to have\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nMNIST STL10 CIFAR10 CIFAR100 Flowers102 StanfordCars Dom-real Dom-sketch Dom-quick\n\n# train 60,000 5,000 50,000 50,000 6,149 8,144\n\n# test 10,000 8,000 10,000 10,000 2,040 8,041 <20,000 <4,000 <20,000 <4,000 <20,000 <4,000\n\nImageNet-1K 1,281,167 ogbg-moltox21 ogbg-molhiv ogbg-molpcba\n\n6,272 32,901 437,929\n\n– 1,742 8,226 –\n\n# class 10 10 10 100 102 196\n\nComments Toy downstream use a subset, as in Figure 7. Coates et al. (2011) Krizhevsky et al. (2009) Krizhevsky et al. (2009) Nilsback & Zisserman (2008) Krause et al. (2013)\n\n200 (345) A subset, original 345 classes and more samples. 200 (345) 200 (345) 1000 12 regression 128\n\nLess sensitive to color and texture. Only contains simple curves. (Peng et al., 2019) Deng et al. (2009) Wu et al. (2018) Hu et al. (2020) Hu et al. (2020)\n\nTable 4: Datasets (vision and molecular graph) used in experiments.\n\n(a) An input image.\n\n(b) A segmentation label.\n\nFigure 10: An example of an image and label pair in PASCAL VOC dataset.\n\nthe higher spatial resolution for the output features of the backbone; the spatial resolution of the features is 7 × 7 for the classification tasks whereas it is 17 × 17 for the segmentation tasks.\n\nThe features extracted from the ResNet50 backbone are passed to a segmentation head, which consists of Atrous Spatial Pyramid Pooling (ASPP) (Chen et al., 2017b) layers followed by a couple of convolutional layers. ASPP is a stack of five regular and dilated convolutional layers which provide the features with various size of receptive fields. The spatial resolution of the outputs of the segmentation head stays in 17 × 17, which are then upsampled to the original input image size through bilinear interpolation. Here, we use the whole segmentation head as the task head.\n\nHyper-parameters for HP and FT: The ResNet50 backbone is pretrained on ImageNet dataset. We set τ ∈ [0, 200], HP learning rate as 0.3, maximum FT epochs as 200 (usually converge less than 50 epochs). Also, we use batch size of 16, and a SGD optimizer with momention (β = 0.9) but without weight decay nor learning rate scheduler.\n\nC MORE EXPERIMENTS\n\nC.1 VERIFICATION ON THE CHANGE OF Z\n\nIn Equation (2), we decompose the learning dynamics of zt into three parts: relatively stable kernel, direction, and energy. Based on this, we can expect a larger adaptation when the initial energy is large. Furthermore, to get a more precise description of zt’s adaptation, we analyze a simple overparameterized linear model in Appendix D. We show that as the initial prediction gap (i.e., energy) increases, the resulting zt will first be stretched with a small direction change (compared with the original z0). If the initial energy keeps increasing, the resulting zt might be more and more dissimilar to the original one. We use four distance related quantity to describe this trend, and verify it under many different settings. In short, the trends are: 1.) more energy leads to larger ∥zt − z0∥2 and smaller cos(zt, z0); 2.) z⊤ 2 has a quadratic shape when energy decrease. Figure 11 provides an example of how zt changes when different τ is chosen.\n\nt z0 and ∥zt∥2\n\n2\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 11: Change of z during FT under a toy setting. Using transparent square markers, we first draw the 2-D projections (the first two components in PCA) of 100 randomly selected z(n) 0 . Then keeping the eigen-directions of this PCA, we project z(n) of different t and encode t by transparency. The converged z is represented by a cross marker with full transparency. It is clear that FT using vrnd makes z change a lot, while v50000\n\ndoesn’t change z too much.\n\nt\n\nhp\n\nFigure 12: Explanation of why small change of z⊤\n\nt z0, cos(zt, z0) can influence a lot.\n\nBefore diving deep into these examples, we provide some general commands. First, we do not expect the experimental results to align perfectly with the examples in Figure 3, because too many designs (like dropout, data augmentation, SGD noise, locking in strange local minima, learning rate schedule, etc.) can influence the shape of the curve, hence we believe observing specific trends in many cases is already a strong support for our analysis. Second, one might be curious about why in some cases, the ranges between z⊤ t z0 and cos(zt, z0) are so small. Hence in Figure 12, we provide an illustration of why this happens: as the dot product measures two long vectors (remember they have the same origin), a small change of z⊤ t z0 and cos(zt, z0) is a sign that zt already moves to another basin, which is the strong-case in Figure 3. Third, in Appendix D, the change of z is linked to q0 − ey under an MSE loss, while in these experiments, we use τ as our x-axis and consider a cross-entropy loss. Combining with the NTK approximation applied in this ideal model, the experimental trends might not be exactly the same with the theoretical ones.\n\nt z0 can make a big difference. A large range of z⊤\n\n2, increasing cos(zt, z0), quadratic z⊤\n\nThen, in Figure 13, Figure 14 and Figure 15, we demonstrate the trend when the model is pretrained on different datasets, on different tasks, and when the downstream tasks are different, respectively. Besides the general trends, i.e., decreasing ∥zt − z0∥2 t z0 and ∥zt∥2 2, we can observe another interesting phenomenon when the model is pretrained using different tasks. See Figure 14, in Byol and SimCLR, sometimes we might get a very big z⊤ t z0 (together with a very small cos(zt, z0)), which makes it hard to observe any quadratic trend in some metrics. But in the supervised pretraining case (no matter what dataset we use in pretrain and downstream tasks), we never observe such a phenomenon – the ∥zt − z0∥2 2 always changes in a relatively small range. We speculate that in the first several updates during head probing, the features first adapt to the downstream task (i.e., classification), then gradually adapt to the downstream dataset distribution (e.g., STL, Flowers, etc.). So a randomly initialized head might influence more when the pretrain task is different from the downstream one. The detailed mechanism of adaptation of task and data distribution might be more complex than we expect, so we left this for the future work.\n\nC.2\n\nINFLUENCE OF THE BACKBONE AND THE TASK-HEAD CAPACITY\n\nIn Section 4.3, we mention a case where the task head is more complex. For example, the head can be a two-layers MLP rather than one linear layer, or we can only copy part of the pretrained model as our backbone. However, these methods and tricks are quite complex and might not necessarily enhance the performance, hence we put some results and discussions here. Remember that our\n\n20\n\nz0ztzt−z022cos(zt,z0)Feature Manifold0Published as a conference paper at ICLR 2023\n\nFigure 13: Adaptation of zt when model is pretrained on different datasets.\n\nanalysis of how energy influences the features adaptation is still valid in these cases, as illustrated by the distance related metrics in the tables.\n\nFrom Table 5, 6, 7, and 8, we can draw the following three conclusions. First, the two-layers MLP design will surely increase the training accuracy after head probing, and hence make the features adapt less (but this might not enhance the downstream performance). Second, we compare the downstream performance when some layers of the backbone are reinitialized as part of the task head. The title “+L4.3” means we take the last layer (i.e., the 3-rd layer) of the last block (i.e., the 4-th block) in a ResNet50 out, and treat it as part of the task head. The title “+L4.2” and “+L4.1” means we continue treating the 2-nd or the 1-st layer as the backbone. Under such a setting, the “+L4.1” case will inherit the least amount of information from the pretrained model, and at the same time, have the biggest task head. For the results in these tables, we train the head until convergence in head probing phase, then fine tune the whole network together. Hence we see the “HP-train-acc” value all increase to roughly 100%. However, the best setting differs when different downstream datasets are considered. When the pretrained features are good (i.e., trained using IN1K), we see the Domain-real dataset needs less adaptation and usually performs the best in the baseline case (i.e., copy all parameters from the pre-trained model). But Domain-quick dataset, which only contains some black-and-white lines, prefers the “+L4.1” setting. In other words, preserving the features captured by the earlier layers of the pretrained model is beneficial. When the pretrained features are bad, like the CIFAR-pretraining case in Table 7, all these datasets prefer throwing away the later layers.\n\nIn summary, although the correlation between features adaptation and downstream task performance under different settings is quite complicated, our analysis of energy can still explain some phenomena well.\n\nLinear-head Baseline\n\n+L4.3 +L4.2 +L4.1\n\nHP-train-acc 1 − cos(zt, z0) ∥zt − z0∥2 val-acc val-acc val-acc val-acc\n\nReal 98.994 0.0315 17.868 54.839 53.629 53.226 53.856\n\nSketch Quick 48.555 63.369 0.3506 0.2275 27.954 22.246 37.789 61.416 61.568 HP-train-acc 35.966 61.391 HP-train-acc 36.69 59.199 HP-train-acc 35.446\n\n2-MLP Head\n\nReal 100 0.0149 17.694 53.856 99.492 99.033 99.863\n\nSketch Quick 63.75 93.447 0.3375 0.2032 27.634 21.171 59.199 35.446 93.975 99.072 98.193 99.336 63.75 93.447\n\nTable 5: Results on a ResNet50 pertrained on Domain-Real. The pretrained validation accuracy is only 54.612 while the training accuracy is 100. Maybe because the DomainNet-Real dataset only contains less than 20k samples.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nFigure 14: Adaptation of zt when model is pretrained using different tasks.\n\nLinear-head Baseline\n\n+L4.3 +L4.2 +L4.1\n\nHP-train-acc 1 − cos(zt, z0) ∥zt − z0∥2 val-acc val-acc val-acc val-acc\n\nReal 60.039 0.3492 9.963 58.846 59.929 60.837 61.164\n\nSketch Quick 51.855 39.092 0.5034 0.4181 9.287 9.72 61.593 45.023 59.602 HP-train-acc 42.882 61.542 HP-train-acc 44.734 62.626 HP-train-acc 45.775\n\n2-MLP Head\n\nReal 88.965 0.3296 9.104 58.77 100 100 100\n\nSketch Quick 76.445 79.58 0.4923 0.3672 9.874 9.209 60.307 43.171 98.252 100 100 100 100 100\n\nTable 6: Results on a ResNet34 pertrained on CIFAR-100 in a classification task.\n\nD ANALYZE LINEAR OVERPARAMETERIZATION PROBLEM\n\nD.1 FORMALIZE THE CHANGE OF REPRESENTATIONS\n\nSection 4 provides some intuitive explanations of how feature extractor f (x; B) changes given different g(z; v), which are well supported by the experimental results. To provide more insights, we\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nFigure 15: Adaptation of zt when downstream tasks are different.\n\nLinear-head Baseline\n\n+L4.3 +L4.2 +L4.1\n\nHP-train-acc 1 − cos(zt, z0) ∥zt − z0∥2 val-acc val-acc val-acc val-acc\n\nReal 96.875 0.1384 16.569 85.685 84.929 83.342 81.074\n\nSketch Quick 64.063 85.938 0.4432 0.2807 21.869 18.005 69.531 72.51 68.422 HP-train-acc 69.734 HP-train-acc 68.547 68.246 HP-train-acc 69.155\n\n2-MLP Head\n\n68.7\n\nReal 96.671 0.1544 16.843 85.031 100 100 100\n\nSketch Quick 84.375 98.438 0.4147 0.2661 21.389 17.672 66.683 69.651 95.232 100 98.087 100 98.842 100\n\nTable 7: Results on a ResNet50 pertrained on ImageNet-1K in a classification task.\n\nanalyze the change of features (i.e., z) in a simplified overparameterization problem, i.e., the one provided in Section 3 and in Kumar et al. (2022). Under some mild assumptions and approximations, we provide an overview of how the norm and direction of z changes under different choice of v0. We show that the v0 satisfying q0 = Y or q0 = 1 2 Y are two critical points in feature adaptation.\n\nWe first rewrite Equation (1) in a non-matrix form:\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nLinear-head Baseline\n\n+L4.3 +L4.2 +L4.1\n\nHP-train-acc 1 − cos(zt, z0) ∥zt − z0∥2 val-acc val-acc val-acc val-acc\n\nReal 92.676 0.1269 5.247 78.075 78.528 78.427 76.689\n\nSketch Quick 64.307 78.213 0.1934 0.1422 4.361 4.752 59.703 61.545 65.683 67.011 HP-train-acc 67.087 HP-train-acc 65.336 68.07 HP-train-acc 65.017\n\n2-MLP Head\n\nReal 99.121 0.1098 4.942 78.453 100 100 100\n\nSketch Quick 82.354 94.883 0.1522 0.1104 3.868 4.206 63.609 63.773 97.666 100 100 100 100 100\n\nTable 8: Results on a ResNet50 pertrained on ImageNet-1K in a SimCLR task.\n\nLB,v =\n\n1 N\n\nN (cid:88)\n\nn=1\n\n1 2\n\n∥v⊤Bx(n) − y∥2 2,\n\n(24)\n\nwhere y ∈ R as we are considering a regression problem (or a classification problem using MSE loss). We use the subscript to represent the time step, e.g., q0, z0, v0 and B0 are output, feature, head parameters and backbone parameters before finetuning. Similarly, qt, zt, vt and Bt are the corresponding values after finetuned t steps. Note that q(X) and z(X) are functions of N input samples X ∈ RN ×d, but we omit it for simplicity. We use lowercase letters to represent the nth element of the vector. Before discussing a specific element, we will clarify whether we are discussing the initialized case or the finetuned case. For example, specifying qt, we use qn = f (xn) = (f (X))n to represent the prediction of xn after finetuning.\n\nRemember the goal of this paper is finding a suitable way to select task head, i.e., v0, given the pretrained feature extractor, i.e., B0. Depending on the discrepancy between the pretraining task and the downstream task, we might expect zt change differently after finetuning. In other words, we care about the expected change of zt compared to z0, i.e., Ex∼D [d(zt, z0)], where D is the data distribution of the downstream task. Depending on what distance measurement (i.e., d(·, ·)) we choose, there are three different metrics:\n\n• Euclidean: ̄deuc ≜ Ex∼D • Dot product: ̄ddot ≜ Ex∼D\n\n• Cosine: ̄dcos ≜ Ex∼D\n\n(cid:104)\n\nz⊤ t z0 ∥zt∥2·∥z0∥2\n\n(cid:2)∥zt − z0∥2\n\n(cid:3),\n\n2\n\n(cid:2)z⊤\n\nt z0\n\n(cid:3), (cid:105) .\n\nWe start from the Euclidean case:\n\n ̄deuc = Ex∼D = Ex∼D = Ex∼D\n\n= Ex∼D\n\n= Ex∼D\n\n= Ex∼D\n\n= Ex∼D\n\n≤ Ex∼D (cid:104)\n\n(cid:104)\n\n(cid:104)\n\n(cid:104)\n\n(cid:104)\n\n(cid:2)(zt − z0)⊤(zt − z0)(cid:3) (cid:2)z⊤ t zt − z⊤ (cid:2)z⊤ t zt − 2z⊤ (cid:104) x⊤B⊤\n\nt z0 − z⊤ 0 zt + z⊤ (cid:3) 0 zt + z⊤ 0 z0 0 Btx + x⊤B⊤ t Btx − 2x⊤B⊤\n\n0 z0\n\n(cid:3)\n\n(cid:105) 0 B0x (cid:105) 0 B0)x) (cid:105) 0 B0)xx⊤) (cid:105) 0 B0)xx⊤) (cid:105)\n\ntr(x⊤(B⊤\n\nt Bt − 2B⊤\n\n0 Bt + B⊤\n\ntr((B⊤\n\nt Bt − 2B⊤\n\n0 Bt + B⊤\n\ntr((B⊤\n\nt Bt − 2B⊤\n\n0 Bt + B⊤\n\ntr(B⊤\n\nt Bt − 2B⊤\n\n0 Bt + B⊤\n\n0 B0) · M\n\n=\n\ntr(B⊤\n\nt Bt) − 2tr(B⊤\n\n0 Bt) + tr(B⊤\n\n0 B0)\n\n(cid:105)\n\n· Ex∼D[M ]\n\n(25)\n\nwhere M ≜ tr(xx⊤) or M ≜ ∥xx⊤∥op). Similarly, we can get the expressions of ̄ddot and ∥zt∥2\n\n2, which are building blocks of ̄dcos:\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nEx∼D\n\n(cid:2)z⊤\n\nt z0\n\n(cid:3) = M · tr(B⊤\n\n0 Bt)\n\nEx∼D\n\n(cid:2)∥zt∥2\n\n2\n\n(cid:3) = M · tr(B⊤\n\nt Bt)\n\n(26)\n\n(27)\n\nD.2 CRITICAL POINTS\n\nOur goal is to find a good initialized task head, i.e., v0, which can lead to better downstream performance. Instead of directly linking v0 to the expected risk, which is a common practice for generalization analysis 5, we consider this problem from another indirect way. Specifically, we assume the SGD algorithm with appropriate regularization in FT stage can find a good optimum for the learning task. What we care more about is whether the features learned from the pretraining stage adapts well to the downstream task. We believe that if the features are properly adapted to the new environment, the model has more potential to generalize better. Hence in this part, considering the aforementioned three distance metrics as the target functions, we formalize how v0 influence them.\n\nFrom Equation (25), (26) and (27), we find that the behavior of tr(B⊤ t Bt) are the keys. In these two terms, B0 is given and cannot change, while Bt is determined by the choice of v0. One way to link these two quantities is using Lemma A.4 in Kumar et al. (2022) or theorem 2.2 in Du et al. (2018):\n\n0 Bt) and tr(B⊤\n\nv0v⊤\n\n0 − B0B⊤\n\n0 = vtv⊤\n\n(28) however, the expression of vt is still hard to obtain (but collecting and visualizing vt is much more cheaper than Bt). We left this direction for our future work. In this paper, we analyze the problem in the NTK regime (this is the main assumption of our analysis, which can cause discrepancies between the theory and experiments).\n\nt − BtB⊤ t ,\n\n∀t,\n\nClosed-form of parameters under NTK approximation:\n\nTo get more insights, we approximate the behavior of this model in the NTK regime, in which the converged parameters can be analytically calculated. Specifically, be applying Equation (8) in Lee et al. (2019) and assuming t → ∞, we can have:\n\nθt = θ0 − (∇θq0)⊤K−1\n\n(29) where q0 ∈ RN ×1 is the model’s prediction on N training samples, i.e., X, and θt, θ0 ∈ R(d+1)∗h×1 are the stacked parameters. Without loss of generality, the first d ∗ h parameters in θ come from B and the last h parameters come from v. The K0 = ∇θq0 · (∇θq0)⊤ ∈ RN ×N here is the empirical NTK on X. Specifically, by stacking the paramters, we can calculate each elements in this kernel as:\n\n0 (q0 − Y ),\n\nκ(x, x′) = x⊤B⊤Bx′ +\n\nh (cid:88)\n\nd (cid:88)\n\n(vixj)(vix′\n\nj)\n\n= x⊤B⊤Bx′ +\n\ni=1\n\nj=1\n\nh (cid:88)\n\nv2\n\ni\n\nd (cid:88)\n\ni=1\n\nj=1\n\nxjx′\n\nj\n\n(30) where vi and xi is the i-th element in v and x respectively. Then, the matrix form of emperical NTK is K0 = X(B⊤\n\n0 B0 + ∥v0∥2\n\n2 · Id×d)X ⊤.\n\n= x⊤(B⊤B + ∥v∥2\n\n2 · Id×d)x′,\n\nWith the help of Equation (29), we can get the closed-form expression of bt ≜ vec(Bt) (i.e., the vectorization of matrix Bt):\n\nbt = b0 − (∇bq0)⊤K−1\n\n0 (q0 − Y ),\n\n(31)\n\nIn Equation (31), we know q0 = XB⊤ 0 v0. The term ∇bq0 also depends on v0. As controlling q0, i.e., the model’s prediction on training samples, is easier in practice (we can directly observe the training loss or training accuracy), we will consider q0 as the optimizing variable in the rest of the paper. The following lemma can link ∇bq0 to q0:\n\n5But it is hard to get tight and informative bounds in deep learning, especially in such a practical scenario.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nLemma 1. For b0 ∈ Rh∗d×1 and ∇bq0 ∈ RN ×h∗d, we have ∇bq0 · b0 = q0.\n\nProof. We check that equation elementwise. For the n-th element in the RHS, we have:\n\nqn = x⊤\n\nn B⊤\n\n0 v0\n\n=\n\n=\n\nd (cid:88)\n\nh (cid:88)\n\ni=1\n\nj=1\n\nd (cid:88)\n\nh (cid:88)\n\ni=1\n\nj=1\n\nxivj(B⊤\n\n0 )i,j\n\nxivj(B0)j,i,\n\n(32)\n\nwhere qn is the n-th element of q0, xi is the i-th element of xn, and vj is the j-th element of v0.\n\nFor the LHS, the n-th value is e⊤ of ∇bq0. From the definition, we know e⊤ is a long row vector. Then, the n-th element in the LHS should be:\n\nn ∇bq0 · b0, where e⊤\n\nn ∇bq0 = vec(v0x⊤\n\nn is a one-hot row vector selecting the n-th row 0 ], which\n\nn )⊤ = [x1v⊤\n\n0 , ..., xdv⊤\n\n0 , x2v⊤\n\nn ∇bq0 · b0 = vec(v0x⊤ e⊤\n\nn )⊤ · b0\n\n=\n\n=\n\nh∗d (cid:88)\n\n(vec(v0x⊤\n\nn ))l(b0)l\n\nl\n\nd (cid:88)\n\nh (cid:88)\n\ni=1\n\nj=1\n\nxivj(B0)j,i\n\nwhere the last equation holds the rule of stacking elements in a matrix. As each elements of the two sides are equal, the lemma holds.\n\n= qn,\n\n(33)\n\nCritical points of tr(B⊤\n\nLemma 2. q∗\n\n0 = 1\n\n0 Bt) and tr(B⊤ t Bt): 2 Y is a maximum of tr(B⊤\n\n0 Bt).\n\nProof. By definition, we have:\n\ntr(B⊤\n\n0 bt\n\n0 Bt) = b⊤ = b⊤ = b⊤ = const. − q⊤\n\n0\n\n(cid:0)b0 − (∇bq0)⊤K−1 0 b0 − (∇bq0 · b0)⊤K−1\n\n0 (q0 − Y )(cid:1)\n\n0 (q0 − Y )\n\n0 C1(q0 − Y ),\n\n(34)\n\nwhere we call C1 = K−1 know q∗ critical point is a maximum.\n\n0 = 1\n\n0\n\nfor convenience. By taking first derivative to q0 and let is equal zero, we 2 Y is a critical point. As the second derivative is −2C1 and C1 is positive definite, this\n\nLemma 3. q∗ α = (cid:0)IN ×N + (C2 − 2C1)−1C1 (∇vq0)(∇vq0)⊤ is the NTK when fixing the parameters of the backbone.\n\n0 = αY, α ∈ (0, 0.5) is a critical point (usually maximum) of tr(B⊤ and ̃K0 = XB⊤\n\n(cid:1) Y , where C2 = K−1\n\n ̃K0K−1\n\n0\n\n0\n\nt Bt). Here 0 B0X T =\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nFigure 16: Illustrations of different metrics if they are 1-D quadratic functions. Here a1 = αY ⊤C1Y and a2 = Y ⊤C2Y . The x-axis represents the choice of q0, where αY is an example of the critical point of ∥zt∥2\n\n2. The black dots are the critical points.\n\nProof. Similar to Lemma 2, we can write tr(B⊤\n\nt Bt) as b⊤\n\nt bt and substitute Equation (31):\n\ntr(B⊤\n\nt Bt) = b⊤\n\nt bt\n\n0 (q0 − Y )(cid:1)\n\n0 (q0 − Y ) + (q0 − Y )⊤K−1\n\n0 (q0 − Y )(cid:1)⊤ (cid:0)b0 − (∇bq0)⊤K−1\n\n= (cid:0)b0 − (∇bq0)⊤K−1 = b⊤ = b⊤ = b⊤ = (b⊤ = const. + q⊤\n\n0 b0 − 2(∇bq0 · b0)⊤K−1 0 K−1 0 (q0 − Y ) + (q0 − Y )⊤K−1 0 b0 − 2q⊤ 0 b0 − 2q⊤ 0 C1(q0 − Y ) + (q0 − Y )⊤C2(q0 − Y ) 0 b0 + Y ⊤C2Y ) + q⊤\n\n0 (2C1 − 2C2)Y + q⊤ 0 (C2 − 2C1)q0\n\n0 (2C1 − 2C2)Y + q⊤\n\n ̃K0K−1\n\n0\n\n0 (C2 − 2C1)q0\n\n0 (q0 − Y )\n\n0 (∇bq0)(∇bq0)⊤K−1\n\n0 (q0 − Y )\n\n(35)\n\nBy taking first derivative and letting it equal zero, assuming (C2 − 2C1) is invertible, we know (cid:1) Y is a critical point. The second derivative is 2C2 − 4C1, which 0 = (cid:0)IN ×N + (C2 − 2C1)−1C1 q∗ is usually positive definite in our settings (explain later).\n\nTo get more insights, we can look deeper into the critical point mentioned in Lemma 3. Following the definition of C1 and C2, we can have:\n\nC2 − 2C1 = K−1 = (K−1\n\n0\n\n ̃K0K−1 0 − 2K−1 ̃K0 − 2IN ×N )K−1 0 ,\n\n0\n\n0\n\n(36)\n\nwhere K−1\n\n0\n\n ̃K0 is the key of understanding this term.\n\nThe exact form of this expression is hard to obtain, but as all the NTK or covariance matrices mentioned here are sysmetric, we can compare the trace of them to get some insights. By definition, tr( ̃K0) = tr(XB⊤ If each dimensions of the data samples are independent, then X ⊤X ≈ Id×d, and tr( ̃K0) ≈ tr(b⊤ 2. Then similarly, tr(K0) ≈ ∥b0∥2 ·IN ×N :\n\n2. Thus the behavior of term K−1\n\n ̃K0 can be described by\n\n0 B0X ⊤) = tr(B⊤\n\n0 b0) = ∥b0∥2\n\n0 B0X ⊤X).\n\n2+∥v0∥2\n\n2\n\n0\n\n∥b0∥2 2+∥v0∥2\n\n2\n\n∥b0∥2\n\n• if ∥b0∥2\n\n2 ≫ ∥v0∥2\n\n2, which might happen as the backbone contains more parameters than\n\nthe head, K−1\n\n0\n\n ̃K0 ≈ IN ×N ,\n\n• if ∥b0∥2 K−1\n\n2 ≪ ∥v0∥2 ̃K0 ≈ 0 · IN ×N .\n\n0\n\n2, which might happen if we split the network in the earlier layer,\n\nBut in either case, the negative of the second derivative of tr(B⊤ t Bt), i.e., Equation (36), is likely to be positive definite, which means the critical point in Lemma 3 is usually a maximum. Another interesting fact of Lemma 3 is q∗ 0 under the aforementioned two extreme conditions. After some 0 → 0 · Y if ∥b0∥2 calculation, we can verify that q∗\n\n2 ≪ ∥v0∥2 2.\n\n2 ≫ ∥v0∥2\n\n2 and q∗\n\n0 → 1\n\n2 Y if ∥b0∥2\n\nCritical points of ̄deuc, ̄ddot and ̄dcos:\n\n27\n\nzt−z022YY/2αY0ztTz0YY/2αY0b022+a1b022zt22YY/2αY0b022+a2b022Published as a conference paper at ICLR 2023\n\nRecall the definitions of different distance metrics we care about. First, ̄ddot is proportional to z⊤ t z0, hence its shape would like the second panel in Figure 16. For ̄deuc, we know it is proportional to tr(B⊤ 0 B0). Substituting results in Equation (34) and Equation (35), we have:\n\nt Bt) − 2tr(B⊤\n\n0 Bt) + tr(B⊤\n\n ̄deuc ∝ tr(B⊤ = q⊤ = −2q⊤\n\nt Bt) − 2tr(B⊤ 0 (2C1 − 2C2)Y + q⊤\n\n0 Bt) + const.\n\n0 C2Y + q⊤\n\n0 C2q0 + const.\n\n0 (C2 − 2C1)q0 + 2q⊤\n\n0 C1(q0 − Y ) + const.\n\n(37)\n\nObviously, q∗ ̄dcos is hard to obtain. However in the last panel of Figure 16, we demonstrate ∥zt∥2 to assist our further analysis.\n\n0 = Y is the minimum of ̄deuc, as depicted in the first panel in Figure 16. The shape of 2, its denominator,\n\nD.3 LEARNING DYNAMICS OF THE REPRESENTATION\n\nq0 = Y → 1 2 Y 1\n2 Y → αY αY → 0\n\nEaie ↑\n↑ ↑\n\n ̄deuc ↑\n↑ ↑\n\n ̄ddot ↑\n↓ ↓\n\n∥zt∥2 ↑\n↑ ↓\n\n ̄dcos ? (↓) ↓↓ ? (↓)\n\nTable 9: Question marks in the last column means we cannot accurately predict its change. But it usually decreases in experiments.\n\nIn this subsection, we will put everything together to provide an overview of how zt changes compared with z0. Remember when q0 = Y , the gradient of B0 is zero, hence zt = z0 (Kumar et al., 2022). When q0 moves linearly from Y to 0, we have the following three phases (see Table 9):\n\n2 Y : zt lengthen its norm with a slightly change in the direction, hence ̄ddot also\n\n• Y → 1 increase; 2 Y → αY : the norm of zt keeps increasing, but its direction drastically changes in this phase, which makes ̄ddot decrease;\n\n• 1\n\n• αY → 0: the norm of zt begins to decrease and the angle between zt and z0 keeps increas-\n\ning, which makes zt changes a lot.\n\n28",
  "translations": [
    "# Summary Of The Paper\n\nThis paper studies the two-stage transfer learning approach of head tuning first (a generalization of linear probing where the head can be nonlinear), followed by finetuning both the backbone and continuing to finetune the head. Their work builds upon the setup of Kumar et al, but they claim to relax the assumption made in that work that the pretrained model is optimal for the downstream task. \n\nThey decompose the training update to the features during finetuning and show theoretically that when the adaptation ‘energy’ increases (meaning that the predictions on the downstream task differ from the correct labels), the final representations (obtained after finetuning) change more. So, intuitively, controlling that energy is one way of controlling how much adaptation is performed on the features. One way to increase the energy is by doing less head probing before the fine-tuning phase (this way the head will be less suited to the downstream task at the start of the finetuning phase, so more adaptation occurs). Since the number of head probing epochs is difficult to tune, they propose label smoothing as an additional knob that can affect energy increase and also explore the effect of the capacity of the head. They show in several empirical scenarios results consistent with their intuitions and theoretical results.\n\n# Strength And Weaknesses\n\nStrengths\n========\n- To the best of my knowledge, this analysis of the influence of ‘energy’ in the head on the changes made to the features during FT is novel and it is interesting.\n\n- The finding that adding label smoothing can make the HP-FT procedure more robust to the number of epochs for HP is an interesting one.\n\nWeaknesses\n==========\n- I found that the paper has clarity issues (see below)\n\n- Does label smoothing always help? Are there cases where this addition can degrade performance? (I see at least one such entry in Table 1, for Flowers dataset, Sup-C10). The authors don’t discuss the potential dangers of using it / weaknesses of this approach.\n\n- I’m not sure what the practical take-aways are (see below)\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity\n=====\n\nFig 1, the authors mention that one of the ways the setup differs between their work and Kumar et al is that ‘information exchanges between the two parts of the network’. I don’t really understand this point. Doesn’t this information exchange always happen during finetuning? What is special here?\n\nIn Section 2.3, for the different degrees to which we want to change the feature extractor, in ‘strong’, what is meant by ‘the pretrained model overfits’? Not clear if this refers to overfitting to the upstream task or the downstream task, and how exactly this is measured.\n\nFig 1 caption: define X_{PT} and X_{DS} – I assume it’s ‘pretraining’ and ‘downstream’ respectively? But this should be stated.\n\nUnder Equation 1, in the equation z = Bx, I think this x should be X.\n\nIn Section 3.1, it’s not clear to me what the superscript 0 in z_t^{(0)} and x^{(0)} means. Is this the first dimension of the embedding or the first example in the batch or something else?\n\nFigure 2: should explain the caption. I’m assuming this HP x means that the head probing was run for x epochs? Should clarify.\n\nThe title of Section 3.2 is “Initial value: two extreme cases”. I did not understand what this is referring to. Initial value of what? And what are the two extreme cases? Are they ‘strong’ and ‘tiny’? It’s confusing as the results shown and discussed in this section (e.g. in Figures 2 and 3) show FT results for various phases of HT tuning which if I understand correctly are the ‘intermediate’ cases?\n\nFigure 4 caption says it’s clear that mild adaptation makes the features more separable. More separable than what? Looks like strong adaptation makes them even more separable. Also, this figure should clarify what is the task in this case. I assume we don’t always (i.e. for all tasks) expect greater separability with more adaptation. What does this depend on? Could we have the opposite phenomenon for other tasks?\n\nI don’t understand Proposition 1. My understanding was that when the energy increases, the euclidean distance between original and final features also increases. But Proposition 1 states the opposite. Is this a mistake?\n\nFigure 5: the names of the columns are hard to understand. I also don’t know what “1-gap” means. Why is the last column LP (linear probe?) instead of HP (head probe)? Not clear what exactly is the experimental setup here and what is the intended take-away.\n\nThe authors mention their analysis is also relevant for finding features that work well in a new, OOD domain. It would be useful to elaborate on this, as I feel if this is true it would add a lot of value to the paper. But I’m not currently able to see how it’s true.\n\nFigure 6: the caption says HP but the x-axis of subplots says LP. which is it?\n\nThe tasks used in all tables should be described. All notations used in figures should be described. There were several other instances aside from the above where I wondered what exactly a symbol means.\n\nQuality\n======\nIn practice, it is not easy to judge to what degree we want to update the pretrained model for a new downstream task (‘strong’, ‘mild’ or ‘tiny’). This is due to often not having enough data or resources downstream to assess aspects like ‘the model overfits’, and in addition it is difficult to estimate task relatedness (e.g. how related or different is the downstream task to the upstream one) - this in and of itself is an open problem. So it’s not easy to tell whether the pretrained model is ‘reasonably good’ (for mild for instance) on the downstream task. Given this, it is not clear to me what is the recommendation given by the authors for how to modify the way in which we finetune models, since the recommendation seems to be conditional on somehow making this difficult choice.\n\nWhile the paper proposes a set of practical tricks to alleviate this, like label smoothing, I’m not necessarily convinced that it always helps and never hurts (for all downstream tasks). For example, I see at least one entry in Table 1 where it can hurt performance, for the Flowers dataset, with Sup-C10. It feels like more analysis on the effect of label smoothing needs to be done before this recommendation can be safely made.\n\nIn Figure 3, it seems that the ‘end’ histogram always matches the e_y pretty well, regardless of the head used (random or after HT). This doesn’t really make a strong case for having to be careful about how we do HT / FT. Are there other cases where we can see bigger differences? Perhaps transferring to a downstream task that is “very different” from the upstream one?\n\nWhile the analysis here is interesting, it’s not clear to me how exactly it relates to the ultimate goal: analyzing the effect of design choices of HP/FT on the actual downstream *performance*. As the authors also pointed out, analyzing the change on the features is an indirection and it’s less clear why it’s relevant / how to inform practical decision making.\n\nSection 3.3 “backbone depth and head capacity” makes some intuitive observations which however aren’t too surprising, and also aren’t very actionable. Again, how can we assess whether low-level features (of the pretrained model) will be beneficial for a new downstream task while high-level features harmful? \n\nNovelty\n======\nThe proposed particular analysis is novel to the best of my knowledge. \n\nReproducibility\n============\nThe procedure proposed is simple enough that I feel confident I could implement it. But I encourage the authors to provide all details about hyperparameters, task details etc, to aid in reproducibility, and make their code available.\n\n# Summary Of The Review\n\nThis paper analyzes the effect of different head designs on the adaptation that occurs in the learned features after the finetuning phase. Their analysis is novel to the best of my knowledge and some findings are interesting. However, I found that the writing and presentation can be improved (see clarity section above) and I’m also unsure about the usefulness of practical takeaways from the paper (see above detailed comments). For this reason, I’m doubtful if the size of the contribution meets the bar for acceptance.\n\n===============================\n\nAfter rebuttal: The new version of the paper is clearer and better structured. I particularly liked the User Guide section. I also like the discussion around difficulty in making these design choices in practice, the risks associated with some decisions (e.g. label smoothing, etc) and the inclusion of recommendations for making them based on increasingly-risky interventions. Based on this, I increased my score from a 5 to a 6.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper \"How to Prepare Your Task Head for Finetuning\" by Yi Ren et al. investigates the critical role of task head design in the fine-tuning of pretrained neural networks. The authors introduce the concept of \"Average Initial Energy\" (AIE) to analyze how task head configurations influence feature adaptation and downstream task performance. Through empirical evaluations, the paper demonstrates that stopping head training before full convergence often results in better performance across various tasks, including image classification and segmentation. The authors provide practical guidelines for practitioners on optimizing task head design, emphasizing a balance between adaptation energy and feature retention.\n\n# Strength And Weaknesses\nThe paper’s strengths lie in its clear articulation of the interplay between task head design and feature adaptation, which is a relatively underexplored area in the literature. The introduction of AIE as a metric provides a valuable and actionable insight for practitioners. Furthermore, the empirical results substantiate the theoretical claims, showing that different training configurations can significantly influence performance outcomes. However, a potential weakness is that while the findings are compelling, they may still be context-dependent and require extensive validation across diverse datasets and architectures to be universally applicable.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making complex concepts accessible. The quality of the empirical results is high, supported by detailed figures and tables that effectively communicate the findings. The novelty of the approach, particularly the introduction of AIE and the insights regarding early stopping during head training, adds significant value to the field. Additionally, the code availability enhances reproducibility, allowing other researchers to build upon the work.\n\n# Summary Of The Review\nOverall, this paper provides insightful contributions to understanding task head design in fine-tuning processes, with practical implications for improving downstream performance. The methodological rigor and empirical validation support its claims, although further exploration across different contexts would strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper investigates how the design of the task head influences feature adaptation during the fine-tuning of pretrained neural networks across various deep learning applications. The authors introduce the concepts of \"energy\" and \"direction,\" which relate to the learning dynamics of model adaptation. They provide a theoretical framework supported by empirical experiments on different tasks and architectures, demonstrating that early stopping of head probing, the use of label smoothing, and designing more complex task heads can significantly enhance downstream performance. The findings suggest that the choice and complexity of the task head are crucial for effective feature adaptation.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive theoretical analysis, which is substantiated by extensive empirical experiments across a variety of datasets, including MNIST and CIFAR. The practical guidelines proposed are actionable and can aid practitioners in improving model performance through optimized task head design. However, the theoretical framework rests on certain assumptions, such as overparameterization, which may not be universally applicable. Furthermore, the complexity of real-world scenarios is not fully addressed, as various factors influencing feature adaptation are not captured in the study. The heuristic nature of the guidelines may also require task-specific tuning, potentially limiting their generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The theoretical concepts are articulated effectively, and the empirical results are documented in a way that facilitates understanding. However, some assumptions may compromise the reproducibility of results in different contexts. The novelty of the paper lies in its integrated approach to theoretical and empirical analyses, although the reliance on specific conditions may limit the broader applicability of its findings.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to understanding the role of task head design in feature adaptation during fine-tuning in deep learning. While the theoretical insights and practical guidelines are valuable, the assumptions and complexity of real scenarios may limit the generalizability of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"How to Prepare Your Task Head for Finetuning\" investigates the critical role of task head selection in the finetuning process of pretrained models. The authors explore the dynamics of feature adaptation through a two-stage training framework consisting of head probing and finetuning. They establish that the initial training accuracy and loss impact how well the features adapt during finetuning. Their empirical findings suggest that early stopping during head training can enhance downstream performance, with analytical proofs supporting these trends in an overparameterized linear setting.\n\n# Strength And Weaknesses\nThe paper offers significant contributions by providing a thorough analysis of the interaction between task heads and backbone networks during finetuning. It critiques conventional approaches, such as full convergence of task heads, and presents a novel paradigm that emphasizes the importance of early stopping. However, the paper could benefit from further empirical validation across a broader range of tasks, as the experiments primarily focus on classic image and graph tasks. Additionally, while the theoretical insights are valuable, they might be challenging for practitioners to implement without detailed guidance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its methodology, findings, and implications. The use of equations and propositions enhances the clarity of the theoretical aspects. However, the reproducibility of the results could be improved with a more detailed description of experimental setups and hyperparameter choices. The novelty of the proposed framework and insights into the dynamics of feature adaptation stands out, as it challenges established finetuning practices in deep learning.\n\n# Summary Of The Review\nOverall, the paper presents a compelling argument for the importance of task head selection in finetuning pretrained models, supported by both theoretical and empirical evidence. While the findings are insightful and contribute to the understanding of feature adaptation, the practical implementation could be further clarified to enhance reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the impact of task head selection on feature adaptation during the finetuning process in transfer learning. The authors propose a novel decomposition of learning dynamics into energy and direction components, providing insights into how different task heads can influence model performance. Extensive empirical validation across various settings is presented, supporting their theoretical claims and offering practical guidelines for practitioners regarding task head selection and finetuning procedures.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Novel Insights:** The exploration of task head influence on feature adaptation is a significant and relatively unexplored area in transfer learning, contributing unique perspectives to the field.\n2. **Practical Guidelines:** The paper provides actionable principles for selecting and tuning task heads, which can enhance model performance in practical applications.\n3. **Empirical Validation:** Comprehensive experiments validate the proposed methods, showcasing their robustness and applicability in different scenarios.\n4. **Decomposition of Learning Dynamics:** The clear separation into energy and direction components offers a nuanced understanding of the adaptation process during finetuning.\n5. **Addressing Prior Work:** The authors effectively critique and build upon previous research, enriching the discussion surrounding finetuning strategies.\n\n**Weaknesses:**\n1. **Theoretical Limitations:** The reliance on an overparameterized linear model may oversimplify the complexities involved in deep neural networks, potentially limiting the theoretical insights' applicability.\n2. **Context-Sensitivity of Guidelines:** The practical recommendations may not be universally applicable across diverse real-world tasks, as they are grounded in empirical observations that could vary significantly by context.\n3. **Generalizability of Experiments:** While the experiments span various domains, there is a risk that they do not cover all possible scenarios or datasets, which could restrict the generalizability of the findings.\n4. **Simplification of Learning Dynamics:** The decomposition of learning dynamics may overlook other critical factors, such as noise or architectural choices, which can influence the adaptation process.\n5. **Limited Comparison to Existing Methods:** The critique of prior work could benefit from a more comprehensive comparison with a broader range of existing methods to better contextualize the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, clearly outlining its objectives, methodology, and findings. The clarity of presentation allows readers to easily follow the proposed concepts and empirical results. The novelty of the insights and the practical implications of the findings are evident. However, reproducibility may be hindered by the lack of detailed descriptions regarding the specific experimental setups and datasets used, which would benefit future researchers seeking to replicate or build upon this work.\n\n# Summary Of The Review\nThis paper makes valuable contributions to the understanding of task head selection in the context of finetuning in deep learning, offering both theoretical insights and practical guidelines. However, its reliance on simplified models and a potentially narrow empirical scope may limit its broader applicability. Future work should aim to explore more complex architectures and diverse datasets to enhance the generalizability of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper investigates the influence of task head design on the fine-tuning of pretrained models in deep learning, introducing the concept of \"adaptive probing.\" The authors assert that the interaction between the task head and backbone is crucial for effective feature adaptation and performance in downstream tasks. They propose a methodology that focuses on intermediate probing stages rather than complete convergence, and they present a theoretical framework analyzing the dynamics of feature adaptation through \"energy\" and \"direction.\" Empirical results demonstrate improvements in model performance with non-linear task heads and adaptive probing across various tasks.\n\n# Strength And Weaknesses\nStrengths of the paper include its addressing of a significant gap in the literature regarding task head design and its contribution to the understanding of feature adaptation in deep learning. The introduction of adaptive probing is particularly noteworthy, providing a practical approach that can be easily implemented by researchers and practitioners. The experimental validation across diverse domains enhances the credibility of the proposed methods. However, the paper could improve by offering a deeper exploration of architectural choices for non-linear task heads, and the theoretical content could be made more accessible for a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the proposed methodologies and findings. The quality of the research is high, supported by extensive experiments and theoretical analysis. The novelty of the adaptive probing concept is significant, as it offers a fresh perspective on task head fine-tuning. While the reproducibility of the experiments is likely high given the thorough documentation, further details on specific implementations could enhance this aspect.\n\n# Summary Of The Review\nThis paper offers valuable insights into task head design and fine-tuning in deep learning, introducing adaptive probing and analyzing feature adaptation dynamics. It provides strong empirical support and practical guidance, making it a meaningful contribution to the field. Some aspects, such as architectural exploration and theoretical accessibility, could be improved.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a comprehensive study on the dynamics of adversarial training and its effects on feature adaptation in neural networks. The authors introduce a novel analytical framework that explores how the choice of adversarial examples influences model robustness and performance in downstream tasks. Key findings include the identification of a correlation between the initial \"energy\" of adversarial examples and the divergence of feature representations post-training, alongside actionable guidelines for practitioners on optimizing adversarial training strategies. Empirical validation across various architectures and datasets supports the proposed framework.\n\n# Strength And Weaknesses\n**Strengths:**\n- The paper provides an innovative perspective on adversarial training, emphasizing the dynamics of feature adaptation rather than solely focusing on the adversarial examples themselves.\n- A solid theoretical foundation is supplemented by robust empirical evidence, enhancing the overall credibility of the findings.\n- The guidelines offered for practitioners are practical, actionable, and relevant for improving model performance.\n\n**Weaknesses:**\n- The investigation could have delved deeper into the implications of specific neural network architectures, particularly those that deviate from standard configurations.\n- There is a lack of discussion regarding potential challenges or computational constraints that may arise when implementing the proposed methods in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its contributions, making complex concepts accessible to readers. The quality of writing is high, with a logical flow that aids in comprehension. The novelty of the approach is significant, as it shifts the focus of adversarial training analysis toward feature adaptation dynamics. However, reproducibility may be a concern if details regarding the experimental setup and computational resources are not explicitly documented, which could affect the application of the proposed guidelines.\n\n# Summary Of The Review\nOverall, this paper significantly advances the understanding of adversarial training dynamics, offering valuable insights into feature adaptation in neural networks. The practical guidelines provided for optimizing adversarial training strategies are particularly noteworthy, positioning the work as a meaningful contribution to the field of deep learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"How to Prepare Your Task Head for Finetuning\" presents a purportedly novel approach to finetuning neural networks, emphasizing the importance of task head selection in controlling feature adaptation. The authors introduce concepts related to \"energy\" and \"direction\" in learning dynamics, claiming these principles can universally enhance model training efficiency. Their experimental results suggest improvements over previous techniques, though these claims may be overstated, as they appear to be context-specific rather than broadly applicable.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its ambitious attempt to address the challenges of feature transfer during finetuning, and its introduction of new concepts that could spark further research. However, the weaknesses are significant; the claims made about the paper's contributions are exaggerated. The authors present their findings as universal solutions to a complex problem, which misrepresents the nuances of feature adaptation. Additionally, the practical toolkit they propose might not be as comprehensive or effective as suggested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written with a clear structure, but the clarity is undermined by the grandiose claims made throughout. While the methodology is innovative, the novelty may not be as impactful as presented. The reproducibility of the results is questionable since the experimental setup and conditions are not adequately detailed, raising concerns about whether others can replicate the findings reliably.\n\n# Summary Of The Review\nOverall, the paper attempts to tackle an important issue in finetuning but suffers from overinflated claims and a lack of nuanced understanding of its contributions. While it introduces some interesting concepts, the evidence presented does not convincingly support the transformative impact implied by the authors.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper \"HOW TO PREPARE YOUR TASK HEAD FOR FINETUNING\" investigates the critical role of task head selection in the fine-tuning of pretrained models. It challenges existing assumptions by exploring how variations in task head design impact feature adaptation and downstream performance. Key findings reveal that increased \"energy\" during fine-tuning can lead to suboptimal feature representations and that non-linear task heads can preserve low-level features better than linear ones. Practical recommendations are offered, particularly regarding early stopping during head probing and the use of label smoothing.\n\n# Strength And Weaknesses\nThe paper makes several significant contributions, including a nuanced understanding of how energy levels during fine-tuning influence feature adaptation. It provides empirical evidence that contradicts previous expectations about early stopping and offers a comparative analysis of task head designs. However, the findings also reveal inconsistencies across different datasets, suggesting that the proposed methods may not be universally applicable. The complexity of the results, particularly regarding the relationship between energy and performance, could be a point of confusion for readers.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its findings clearly. The methodology is sound, and the empirical results are well-documented, although some details could be elaborated for better clarity. The novelty lies in the exploration of task head designs and their effects on feature representation, which adds a valuable perspective to the existing literature. However, the reproducibility may be affected by the observed inconsistencies across different datasets, requiring careful consideration when interpreting the results.\n\n# Summary Of The Review\nOverall, this paper provides insightful contributions concerning task head selection in fine-tuning pretrained models, highlighting the complexity of feature adaptation. While it offers practical recommendations, the inconsistencies observed across datasets raise questions about the generalizability of the findings. Further research is needed to clarify these complexities and improve task head selection strategies.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper challenges the conventional wisdom surrounding the utility of pretrained features in downstream tasks, positing that these features may not always be optimal. It introduces a novel framework centered on the concept of \"energy,\" derived from initial training accuracy, to analyze the relationship between pretrained models and their downstream adaptation. The authors advocate for reinitializing task heads to enhance performance and explore the use of non-linear task heads, ultimately providing a user guide based on observed trends. However, the empirical validation of these principles remains limited, as does the generalizability of findings across diverse datasets and architectures.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to reevaluating pretrained models and offering actionable guidelines for task head initialization. However, it has notable weaknesses, including a reliance on potentially oversimplified assumptions regarding training dynamics and feature adaptation. The proposed framework may not adequately account for the complexity of various architectures and datasets, and it lacks extensive empirical validation to support its claims. Additionally, the potential for its principles to fail under certain conditions is not thoroughly explored, raising concerns about their practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a structured presentation of ideas. However, some aspects, such as the assumptions related to training accuracy and feature adaptation, could benefit from clearer justification and empirical backing. The novelty is significant as it challenges established norms in the field, but the reproducibility of the results is questionable due to the limited scope of experiments and the specificity of the datasets used. The guidelines provided may not hold under varying conditions, which could hinder reproducibility in practice.\n\n# Summary Of The Review\nOverall, the paper presents a thought-provoking examination of pretrained models, proposing new concepts and methodologies that could influence future research. However, its reliance on untested assumptions and limited empirical evidence raises concerns about the robustness and applicability of its conclusions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper investigates the effects of task head design on feature adaptation during the fine-tuning process of pretrained models. The authors establish that the initial training accuracy and loss of task heads significantly influence downstream performance. They propose a framework that analyzes how variations in initial \"energy\" can affect feature transformation, offering both basic and advanced techniques for optimizing task head preparation. Empirical validation across various tasks supports their findings, and a user guide is provided for practitioners to select appropriate task head configurations.\n\n# Strength And Weaknesses\nThe paper makes notable contributions by addressing an underexplored area regarding the interaction between task heads and pretrained features during fine-tuning. The findings highlight practical implications for enhancing downstream performance, particularly through early stopping and the careful design of task heads. However, while the proposed framework is insightful, it could benefit from a more rigorous theoretical foundation that explicitly connects the observed phenomena to established principles in transfer learning. Additionally, some advanced techniques discussed may require further empirical support to validate their effectiveness comprehensively.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible for readers with varying levels of expertise. The methodology is described in a straightforward manner, and the empirical results are presented clearly, allowing for easy interpretation. The novelty lies in the focus on task head dynamics, which is less explored in existing literature. Although reproducibility is addressed through provided guidelines and empirical results, the paper would benefit from sharing code or detailed experimental setups to facilitate independent validation of results.\n\n# Summary Of The Review\nOverall, the paper presents a valuable exploration of task head design and its impact on feature adaptation during fine-tuning, with practical implications for improving model performance. While it offers novel insights and practical guidelines, there are opportunities for strengthening the theoretical underpinnings and empirical support for some of the proposed techniques.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper explores the intricacies of feature adaptation within the realm of transfer learning, presenting a robust theoretical framework that analyzes the interactions among model components. The authors provide a comprehensive methodology that includes the derivation of key theoretical insights and their implications for optimizing model performance across various tasks. The findings indicate that specific feature adaptation strategies can significantly enhance transfer learning outcomes, suggesting a pathway for practical applications in diverse domains.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Novel Insights**: The paper delivers fresh perspectives on feature adaptation dynamics, enriching the existing literature in transfer learning.\n2. **Theoretical Rigor**: The theoretical framework is well-structured, with clear connections made between theory and practice, enhancing the paper's credibility.\n3. **Broad Applicability**: The implications of the findings extend to multiple tasks, indicating potential for widespread impact across various fields.\n4. **Practical Guidelines**: The inclusion of actionable recommendations for practitioners enhances the utility of the research.\n\n**Weaknesses:**\n1. **Clarity Issues**: Certain sections lack clarity and could benefit from more illustrative examples to elucidate complex theoretical concepts for a wider audience.\n2. **Limited Experimental Validation**: The experimental section is somewhat underdeveloped; additional validations against state-of-the-art methods would strengthen the claims made.\n3. **Focus Limitations**: The narrow focus on specific aspects of feature adaptation may restrict the generalizability of the conclusions drawn.\n4. **Reproducibility Concerns**: The documentation of the experimental setup, including datasets and hyperparameters, is insufficient, which raises concerns about the reproducibility of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is inconsistent; while the theoretical contributions are compelling, some concepts require more detailed explanations for better accessibility. The quality of the research is high, supported by a strong theoretical foundation. However, the novelty is somewhat tempered by the lack of extensive empirical validation. Reproducibility is a significant concern due to inadequate details regarding the experimental setup.\n\n# Summary Of The Review\nOverall, the paper provides a substantial contribution to the field of transfer learning through its innovative theoretical insights and practical implications. However, to maximize its impact, the authors should address clarity issues and enhance experimental validation and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper \"How to Prepare Your Task Head for Finetuning\" investigates the impact of task head selection on the adaptation of pretrained models to downstream tasks. It provides a comprehensive analysis of how initial training accuracy and loss at the beginning of finetuning influence the \"energy\" available for feature adaptation. The authors present a broader definition of \"head probing,\" accommodating non-linear task heads, and propose practical guidelines to enhance downstream performance, such as early stopping during head probing, label smoothing, and adjusting task head capacity. The findings highlight the relationship between initial conditions and feature adaptation dynamics, offering insights into improving model performance.\n\n# Strength And Weaknesses\nThe paper makes several valuable contributions to the understanding of finetuning practices, particularly how the choice of task head affects feature adaptation and downstream task performance. The introduction of a broader concept of \"head probing\" is particularly noteworthy as it allows for more flexible applications beyond traditional linear heads. However, the paper could benefit from more empirical validation of the proposed methods, as the current findings are primarily theoretical. Additionally, while the practical guidelines are useful, their implementation in real-world scenarios remains to be thoroughly explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its main contributions, methodologies, and findings. The structure is logical, allowing readers to follow the arguments and insights easily. However, while the theoretical underpinnings are solid, the reproducibility of the results is somewhat unclear, as specific datasets and experimental settings for the proposed methods are not detailed. The novelty of the approach, particularly the broader definition of \"head probing,\" is commendable and adds significant value to the existing literature.\n\n# Summary Of The Review\nOverall, the paper presents a thoughtful exploration of task head selection in finetuning, offering both theoretical insights and practical recommendations. While the contributions are meaningful, further empirical validation is necessary to solidify the proposed methods' effectiveness in real-world applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"How to Prepare Your Task Head for Finetuning\" by Yi Ren et al. investigates the critical role of task head design in the finetuning of pretrained networks. The authors introduce the concept of \"initial energy,\" which significantly influences the feature adaptation process during finetuning. They propose that stopping head probing early may yield better downstream performance and provide practical principles for optimizing task head training. The paper includes experimental validation of these theoretical insights across various tasks, demonstrating that optimal probing strategies depend on the quality of pretrained features and the specific nature of downstream tasks.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its nuanced exploration of the interaction between task heads and backbone networks, contributing valuable insights to the field of transfer learning. The introduction of \"initial energy\" as a metric for feature adaptation is a novel perspective that adds depth to the understanding of finetuning processes. However, a potential weakness is that while the empirical results support the theoretical claims, the paper could benefit from a more extensive discussion on the limitations of the proposed techniques or scenarios where they might not apply.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clear and well-structured, making it accessible for readers familiar with transfer learning concepts. The quality of writing is high, with a logical flow from motivation to conclusion. The novelty of the approach is significant, particularly in how it frames task head training in relation to initial energy. Reproducibility is supported by detailed experimental setups, although additional details on specific datasets and hyperparameters would enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a compelling analysis of task head design in finetuning, offering both theoretical insights and practical guidelines. The findings are well-supported by experimental evidence, although further discussion on limitations would strengthen the contribution. The work is a valuable addition to the literature on transfer learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"How to Prepare Your Task Head for Finetuning\" by Yi Ren et al. addresses the critical role of task head selection in the finetuning of pretrained neural networks. The authors analyze the impact of task head initialization on feature adaptation and subsequent performance in various tasks. Methodologically, they decompose learning dynamics into components such as energy and direction, employing overparameterized linear models for theoretical analysis. The findings suggest that optimizing task head adjustments can significantly enhance downstream performance, leading to practical principles for effective finetuning practices.\n\n# Strength And Weaknesses\nThe paper makes several noteworthy contributions, including a robust theoretical framework that elucidates the dynamics of finetuning and an empirical validation of its claims across diverse tasks such as image classification, segmentation, and graph-based learning. The practical guidance offered for selecting task heads and tuning parameters is especially valuable for practitioners. However, the paper could improve by providing a more extensive discussion on the limitations of the proposed methods and potential scenarios where they might not apply effectively. Additionally, further exploration into the applicability of these principles across different architectures could strengthen the argument.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper exhibits a clear structure, with logical progression from the introduction to the conclusion. Definitions and explanations of key concepts are well-articulated, contributing to overall clarity. The theoretical analysis is solid, supported by empirical results that enhance its credibility. The novelty of the insights into finetuning processes is significant, offering new principles for the community. The reproducibility of the experiments is facilitated by detailed descriptions of methodologies and results; however, more information on the experimental setup would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents valuable contributions to the field of transfer learning and finetuning of neural networks, offering both theoretical and practical insights. The research findings are well-supported by empirical evidence, making a compelling case for the proposed principles in optimizing finetuning practices.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a comprehensive examination of the fine-tuning process of pretrained neural networks, emphasizing the critical role of the task head in adapting to downstream tasks. It introduces an analytical framework to study the learning dynamics of feature adaptation, specifically focusing on the concept of Average Initial Energy (AIE) and its relationship to performance metrics. The findings reveal that the initial \"energy\" available for feature adaptation significantly influences the success of fine-tuning, and the paper provides empirical evidence supporting recommendations for optimizing task head configurations and training strategies.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous analytical approach and the establishment of a clear relationship between initial energy and feature adaptation dynamics. The two-stage training methodology and the detailed exploration of the interactions between backbone and task head offer valuable insights for practitioners. However, the paper could benefit from a more extensive empirical validation across a wider range of tasks and datasets, as the current findings may not be universally applicable. Additionally, the theoretical constructs, while insightful, may be challenging for practitioners unfamiliar with the underlying mathematical framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodologies. The quality of writing is high, making complex concepts accessible, although some sections may require more background information for full comprehension. The novelty of the approach, particularly the introduction of AIE as a metric, adds significant value to the existing literature. However, reproducibility could be a concern as the empirical results rely on specific configurations that may not be straightforward for others to replicate without additional context regarding the implementation details.\n\n# Summary Of The Review\nThe paper offers a significant contribution to understanding the fine-tuning of pretrained neural networks, particularly concerning the role of the task head and the concept of Average Initial Energy. While it presents a solid theoretical foundation and practical recommendations, further empirical validation across diverse contexts would enhance its applicability and robustness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a theoretical framework aimed at improving downstream performance of machine learning models through specific finetuning principles. The authors utilize an overparameterized linear model to derive insights into feature adaptation and pretrained feature optimality. However, the experimental validation lacks depth, with limited datasets and tasks, leading to inconclusive findings regarding the applicability of the proposed principles in real-world scenarios.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its theoretical contributions, which attempt to address finetuning challenges in machine learning. However, several weaknesses undermine its impact: the over-reliance on theoretical analysis limits practical implications, while the simplistic guidelines offered do not adequately address the complexities of real-world applications. Furthermore, the experimental scope is narrow, and the authors fail to engage with existing literature, leaving the reader with an incomplete understanding of the topic.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper exhibits a lack of clarity, with convoluted explanations that may alienate readers not well-versed in the subject. The terminology used is inconsistent, leading to confusion regarding key concepts. The novelty of the theoretical approach is overshadowed by the limited empirical support, and reproducibility is hampered due to insufficient detail in the experimental setup and a lack of comprehensive results.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing theoretical exploration of finetuning principles but suffers from significant shortcomings in clarity, empirical validation, and practical relevance. The contributions are overshadowed by a lack of engagement with existing literature and an unclear target audience.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"How to Prepare Your Task Head for Finetuning\" presents a novel approach to fine-tuning pretrained models, emphasizing the critical role of the task head in optimizing performance on downstream tasks. The authors investigate feature adaptation and introduce the concept of \"initial energy,\" which reflects the connection between early training accuracy/loss and effective feature adaptation. Additionally, the paper provides actionable strategies for practitioners, such as early stopping of head probing, label smoothing, and advanced task head designs. Empirical validation shows consistent performance improvements across various applications, reinforcing the methods' effectiveness.\n\n# Strength And Weaknesses\nThe paper’s strengths lie in its innovative insights into feature adaptation and the practical strategies it offers for enhancing model performance. The introduction of \"initial energy\" as a metric for guiding fine-tuning decisions is particularly valuable, providing a data-driven approach for practitioners. However, a potential weakness is that while the paper covers a range of applications, it may benefit from deeper exploration of specific case studies or comparative analyses with existing methods to further substantiate its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections and a logical flow of ideas. The quality of the writing and illustrations aids in understanding complex concepts. The novelty is significant, as it brings fresh perspectives to transfer learning and fine-tuning strategies. The reproducibility of the results is enhanced by the user guide provided, which outlines implementation steps for real-world applications, although more detailed descriptions of experimental setups would further bolster this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of transfer learning by elucidating the importance of the task head in fine-tuning pretrained models. Its practical insights and empirical validations present a significant advancement that can influence both research and application in various domains.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents a theoretical analysis of task head design in deep learning, emphasizing its critical role in feature adaptation for downstream tasks. The authors propose a framework that decomposes learning dynamics into energy and direction, providing insights into how the initial energy gap between predicted and true labels influences feature adaptation. Key findings include the significance of task head initialization and capacity, as well as the complex interplay between the task head and the backbone in enhancing model performance.\n\n# Strength And Weaknesses\nThe paper offers valuable theoretical insights into the underexplored area of task head design, making a compelling case for the importance of initialization and energy management in fine-tuning. The propositions presented are well-justified and provide a foundation for future research. However, the analysis may benefit from empirical validation to support its theoretical claims, as well as a more detailed exploration of practical implications and potential applications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its arguments clearly. The theoretical framework is robust and thoughtfully constructed, contributing to the clarity of the paper's main points. However, the novelty may be perceived as somewhat limited if not substantiated by empirical experiments. Reproducibility might be challenging, as the theoretical insights would require specific experimental setups that are not clearly detailed in the paper.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the understanding of task head design in deep learning through a solid theoretical framework. While the insights are valuable, the lack of empirical validation and practical implications may limit its impact on the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"How to Prepare Your Task Head for Finetuning\" by Yi Ren et al. addresses the critical role of the task head in the finetuning process of pretrained models. The authors introduce the concept of Average Initial Energy (AIE) to analyze its impact on training accuracy and feature adaptation. The methodology includes a two-stage training process—Head Probing (HP) and Finetuning (FT)—with practical recommendations for improving performance, such as using label smoothing and reinitializing backbone layers. Extensive experiments on several datasets demonstrate the efficacy of the proposed techniques, with the authors providing code for reproducibility.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear articulation of the significance of the task head in model finetuning and the introduction of AIE as a novel metric for assessing adaptation. The empirical validation across multiple datasets adds robustness to the findings. However, a potential weakness lies in the reliance on an overparameterized linear model for analysis, which may limit the generalizability of the insights to more complex architectures. Additionally, while the recommendations are practical, they could benefit from more thorough discussions on their specific impacts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, making it accessible to readers with varying levels of expertise. The quality of writing is high, with appropriate technical detail. The concept of AIE is a novel addition that contributes to the understanding of task head dynamics. The authors also provide a solid foundation for reproducibility by making their code available on GitHub, enhancing the paper's credibility.\n\n# Summary Of The Review\nOverall, this paper presents meaningful contributions to the field of model finetuning, particularly focusing on the task head's role. The introduction of AIE and the practical recommendations are noteworthy, although some aspects could benefit from deeper exploration. The empirical results support the claims made, making this work valuable for researchers and practitioners alike.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents an analysis of feature adaptation in transfer learning, comparing its findings to those of Kumar et al. (2022). It introduces the concept of \"head probing\" and explores the use of non-linear task heads, while claiming to offer practical guidelines for hyperparameter tuning. However, the methodology largely replicates existing findings without substantial empirical or theoretical advancements. The authors argue for the benefits of adapting pretrained features, yet their contributions lack novelty, as many of their claims are already well-established in the literature.\n\n# Strength And Weaknesses\nThe paper attempts to provide a broader perspective on feature adaptation than prior works, particularly Kumar et al. (2022), but largely reiterates existing themes without significant new insights. While the introduction of non-linear task heads and the discussion of energy dynamics are interesting, they do not differentiate from similar approaches in the literature. The paper's practical guidelines, although practical, are not innovative and echo suggestions from previous studies. The experimental validation appears less rigorous compared to prior work, diminishing the reliability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, but the lack of rigorous theoretical backing and empirical validation diminishes its quality. The novelty of the contributions is questionable, as many ideas presented are not new to the field. The authors' claims about the effectiveness of their methods lack strong empirical support, raising concerns about reproducibility. Overall, while the paper is articulated clearly, it does not contribute significantly to the advancement of knowledge in the area of feature adaptation.\n\n# Summary Of The Review\nOverall, this paper does not provide substantial new insights into feature adaptation and largely reiterates concepts already explored in prior literature. The contributions, while framed as novel, lack the empirical rigor and theoretical depth needed to substantiate their claims. Consequently, the paper fails to significantly advance the understanding of feature adaptation in transfer learning.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a systematic approach to optimizing task heads for fine-tuning in natural language processing (NLP). The authors propose various methodologies, including direct fine-tuning and linear probing, to assess their impact on downstream performance. Key findings indicate that neither converging the head nor opting out of probing yields optimal results, suggesting the need for nuanced strategies in fine-tuning.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its comprehensive examination of fine-tuning methodologies, which contributes valuable insights to the field of NLP. However, the paper suffers from multiple clarity issues, including inconsistent terminology and lack of definitions for key concepts, which could hinder reader comprehension. The findings, while promising, are presented without sufficient detail in terms of their implications, limiting their potential impact on future research.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper contains novel insights into fine-tuning practices, the clarity is hampered by inconsistent use of terminology (e.g., \"finetuning\" vs. \"fine-tuning\") and inadequate explanations of introduced concepts (e.g., \"head probing\"). The overall quality of presentation is affected by typographical errors and formatting inconsistencies, which detracts from the reproducibility of the findings. A more structured approach to the presentation of methods and results would enhance understanding.\n\n# Summary Of The Review\nOverall, the paper offers valuable contributions to fine-tuning methodologies in NLP, but significant clarity and presentation issues detract from its effectiveness. The findings are intriguing but require clearer exposition and definition of terms to maximize their impact on the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the impact of task head design on feature adaptation during the fine-tuning of neural networks. It presents a series of empirical analyses and theoretical proofs that elucidate how different configurations of task heads can influence downstream task performance. The findings suggest that task head complexity has a significant relationship with feature adaptation dynamics, though the authors acknowledge limitations in their approach, particularly concerning the use of a linear model for analysis.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its thorough examination of task head adaptations and their effects on feature adaptation trends, providing valuable insights into the dynamics of model fine-tuning. However, it has several weaknesses, including a lack of exploration into the effects of various pretraining strategies and an incomplete analysis of how different types of downstream tasks (beyond classification) might benefit from these adaptations. Additionally, the paper could enhance its practical relevance by addressing computational overhead and the implications of feature interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that aids in understanding the proposed methodologies and findings. However, the novelty of the contributions is somewhat diminished by the limitations noted, such as the lack of discussion on advanced learning rate techniques and data augmentation effects. Reproducibility is supported by a user guide, but the absence of concrete examples of edge cases limits practical applicability.\n\n# Summary Of The Review\nOverall, the paper presents an interesting exploration of task head design and its effects on feature adaptation, with notable contributions to the understanding of fine-tuning dynamics. Nevertheless, it lacks depth in several critical areas, including pretraining strategies and the variability of findings across different tasks and datasets.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"How to Prepare Your Task Head for Finetuning\" presents a comprehensive analysis of the statistical implications of task head choices in the context of finetuning pretrained deep learning models. It introduces a methodological framework that decomposes feature adaptation into three key components: energy, direction, and statistical trends. The authors establish significant statistical relationships, particularly through two key propositions: the correlation between Average Initial Energy (AIE) and feature adaptation, and the quadratic trend of distance metrics in relation to energy during finetuning. Empirical validation through statistical experiments across different pretrained models and datasets further supports the findings, showcasing the generalizability of the proposed framework.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its rigorous statistical analysis and the introduction of novel propositions that clarify the relationship between energy and feature adaptation. The empirical validation across multiple contexts enhances the credibility of the findings, indicating a strong foundation for future work in this area. However, the paper could benefit from a more detailed exploration of the implications of varying task head capacities on performance outcomes, as well as clearer discussions on how these findings can be practically implemented in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology and findings with clarity. The statistical propositions are articulated in a manner that is accessible to readers with a basic understanding of statistical principles in machine learning. The novelty of the approach lies in the emphasis on statistical rigor in analyzing finetuning processes; however, some technical details may require more elaboration for full reproducibility. The experiments conducted are robust, but additional information on implementation specifics would enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the understanding of feature adaptation in deep learning finetuning processes through a rigorous statistical framework. While it provides valuable insights and empirical validation, further exploration of practical implications and detailed methodology would strengthen its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to feature adaptation in neural networks, focusing on the adjustment of task heads following pretraining on various datasets. The methodology encompasses linear and multi-layer perceptron (MLP) heads, assessing their effectiveness in transferring learned features to downstream tasks. The findings indicate that while certain head architectures can improve performance, the paper falls short of providing a comprehensive framework for all feature adaptation scenarios.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its attempt to address the adaptation of pretrained features for downstream tasks; however, it has significant weaknesses. It does not adequately explore the impact of complex architectures beyond linear and MLP heads, potentially limiting its applicability. Additionally, assumptions made in the analysis restrict generalizability across neural network architectures and tasks. The paper also lacks a detailed examination of how different pretraining techniques affect performance and fails to quantify trade-offs associated with energy levels during head probing. This absence of rigorous analysis and exploration of adaptive hyperparameter tuning diminishes the practical implications of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by its limited exploration of complex architectures and the implications of various pretraining techniques, which affects the overall quality of the contributions. The novelty of the proposed methods is somewhat overshadowed by the lack of thorough analysis and the need for further investigation into advanced learning dynamics. Reproducibility is compromised due to insufficient details regarding hyperparameter tuning and empirical validation of findings across different domains.\n\n# Summary Of The Review\nOverall, the paper presents a valuable exploration of task head adjustments for feature adaptation but suffers from significant limitations that hinder its general applicability and practical relevance. The lack of comprehensive analysis and exploration of advanced architectures suggests that further work is needed to validate the proposed methods in diverse contexts.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"How to Prepare Your Task Head for Finetuning\" by Yi Ren et al. discusses the nuances of preparing a task-specific head following pretraining in deep learning models. The authors aim to clarify the often-overlooked aspects of this process, emphasizing the importance of controlling feature adaptation during finetuning. They introduce practical principles derived from their analysis, suggesting that stopping the training of the task head before convergence can help maintain feature integrity. Empirical results are presented to support their claims, although these findings largely align with established understanding in the field.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its attempt to consolidate existing knowledge about finetuning and feature adaptation into a user-friendly guide. However, the paper falls short by presenting well-known concepts as novel insights, which may detract from its perceived value. The analysis and empirical results, while coherent, do not provide substantial new evidence or methodologies that advance the field. The lack of originality in the contributions may limit its impact on practitioners and researchers alike.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, making it accessible to readers familiar with transfer learning and finetuning. However, the novelty is questionable, as many of the insights presented are not new to the community and have been discussed in prior literature. The quality of the empirical analysis is acceptable, but it does not introduce any innovative experimental designs or findings that would allow for a deeper understanding of the topic. Reproducibility is supported by standard practices, but since the techniques are not groundbreaking, this aspect is less critical.\n\n# Summary Of The Review\nOverall, the paper provides a basic overview of task head preparation techniques for finetuning in deep learning, but it lacks significant novelty or depth. While the clarity and structure are commendable, the contributions do not extend beyond established practices, making the paper feel somewhat redundant. The authors would benefit from presenting more innovative concepts or experimental results that could enhance the relevance of their work.\n\n# Correctness\n4/5 - The claims made in the paper are largely correct and align with established understanding, although they do not provide substantial new insights.\n\n# Technical Novelty And Significance\n2/5 - The technical contributions are minimal, largely reiterating concepts that are already well-understood in the field without offering novel methodologies or frameworks.\n\n# Empirical Novelty And Significance\n2/5 - The empirical results, while valid, do not introduce new findings or significant advancements in the understanding of finetuning processes, reflecting expected trends rather than innovative discoveries.",
    "# Summary Of The Paper\nThe paper presents a novel framework for task head adaptation in transfer learning, focusing on the preparation of task-specific heads prior to fine-tuning. It introduces various strategies for optimizing the complexity and structure of these heads, aiming to enhance performance across downstream tasks. The methodology is primarily validated using specific architectures, notably ResNet, and highlights the importance of task head complexity and adaptation dynamics. However, it lacks integration with modern optimization techniques and broader validation across diverse model architectures.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear articulation of the task head adaptation framework and its emphasis on complexity management, which is crucial for effective transfer learning. However, it has notable weaknesses, including a limited exploration of modern optimization techniques, such as adaptive learning rates, which could enhance convergence. Additionally, the empirical validation is restricted to a narrow range of architectures, limiting the generalizability of the findings. The paper also does not consider uncertainty estimation or multi-task learning, both of which could provide valuable insights and improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and logically structured, making it accessible to readers. However, while its contributions are relevant, the novelty is somewhat constrained by the lack of integration with recent advancements in transfer learning and regularization techniques. Reproducibility is hindered by the absence of comprehensive benchmarking against state-of-the-art methods and the limited range of tested architectures, which could make it challenging for practitioners to replicate the results in different contexts.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of transfer learning, particularly in the context of task head adaptation. However, its impact is diminished by the narrow scope of empirical validation and the omission of modern techniques that could enhance its findings. Broader exploration and integration of contemporary methods would significantly strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a comprehensive study on task head preparation techniques for enhancing the performance of pre-trained models when fine-tuning on downstream tasks. The authors demonstrate the effectiveness of their proposed method, denoted as HPτ∗, through benchmark performance results across a variety of datasets, including image classification and graph-based tasks. Key findings indicate that optimal selection of parameters based on validation accuracy significantly improves performance, while techniques such as MLP heads and label smoothing contribute to further gains. The study emphasizes the importance of controlling the head probing duration and shows consistent improvements across diverse datasets like STL10, CIFAR100, and Tox21.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its systematic approach to evaluating the impact of task head preparation on model performance. The use of multiple datasets and tasks enhances the robustness of the findings, providing strong empirical support for the proposed methods. Additionally, the insights regarding the early stopping of head probing before convergence are valuable for practitioners. However, a notable weakness is the performance of the HP-only method, which underperformed in some scenarios, suggesting that the pretrained features may not always align well with specific downstream tasks. This indicates a potential limitation in the generalizability of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions, methodologies, and findings, making it accessible to a broad audience. The quality of the experiments is high, with detailed results presented in tables that facilitate comparison across different settings. In terms of novelty, while the concept of task head preparation is not entirely new, the specific methodologies and empirical results presented in this work add valuable insights to the field. Reproducibility is supported through the thorough description of the experimental setup, though further details on the implementation of the proposed methods could enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the understanding of task head preparation for fine-tuning pre-trained models, demonstrating consistent performance improvements across various datasets. While it offers valuable insights and practical applications, the variability in performance across different methods suggests areas for further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to pretraining and fine-tuning methods in machine learning, aiming to improve model performance on various tasks. The authors propose a new architecture that combines elements from existing frameworks, enhancing efficiency and effectiveness. Empirical results demonstrate significant improvements over baseline models on benchmark datasets, along with a thorough analysis of the model's strengths.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive evaluation of the proposed method across multiple datasets and comparison with state-of-the-art techniques. The methodology is well-defined and replicable, allowing for potential adoption in practical applications. However, the paper suffers from a lack of clarity in certain sections, particularly in the introduction and figures, which may overwhelm readers with excessive detail. Additionally, the conclusion could be more impactful by outlining future research directions.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-structured, it contains several areas where clarity could be improved, particularly in the transitions between sections and the presentation of figures. The novelty of the proposed method is noteworthy, as it introduces a unique combination of techniques that may offer significant advantages. The reproducibility of the results is supported by detailed descriptions of the methodology and datasets, though the complexity of the language may hinder understanding for some readers.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of machine learning by proposing a novel pretraining and fine-tuning approach. However, it requires improvements in clarity and structure to enhance readability and accessibility for a broader audience.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.4129106016565607,
    -1.8598637475639836,
    -1.8878485614977,
    -1.8966622581003465,
    -1.5886672247484288,
    -1.8361275354101143,
    -1.709488469367497,
    -2.0371636483978413,
    -1.9182857522055614,
    -1.8963615007090984,
    -1.943119900279532,
    -1.4991098279882913,
    -1.7997896993596312,
    -1.8002879815921993,
    -1.9238477247824448,
    -1.6912655890873571,
    -1.8348318691072014,
    -1.913951741833968,
    -1.8462023819662121,
    -1.805677409407405,
    -1.9042405667074562,
    -1.8776619747647334,
    -1.9821805797797685,
    -1.9276890468303738,
    -2.050561358996726,
    -1.7313437235868971,
    -1.715050137106261,
    -1.8635043048701803,
    -1.5526136948247156
  ],
  "logp_cond": [
    [
      0.0,
      -2.3439575663935783,
      -2.329674829580546,
      -2.3384033312960306,
      -2.3312311909756733,
      -2.343640797700145,
      -2.345067950080139,
      -2.334658396401519,
      -2.3272881183574694,
      -2.348734648648681,
      -2.3400345603563317,
      -2.3556895473864032,
      -2.3277280029688687,
      -2.3402784354817,
      -2.3483773777690056,
      -2.3502452861691236,
      -2.356010790970981,
      -2.3317141184470267,
      -2.350789127630349,
      -2.333436530667489,
      -2.3337937738273427,
      -2.357057842183598,
      -2.3370948427960703,
      -2.3477856069279053,
      -2.3385101779819113,
      -2.344692798466419,
      -2.347356302724042,
      -2.345774014573551,
      -2.3587746864937333
    ],
    [
      -1.4723329458645322,
      0.0,
      -1.4172397498997003,
      -1.3716159092125295,
      -1.4583894111167248,
      -1.4330580529600534,
      -1.5231411839383935,
      -1.446706887314331,
      -1.3767931500227513,
      -1.4789312980149594,
      -1.395582327397914,
      -1.5843608831640386,
      -1.3674261685066305,
      -1.296089372188018,
      -1.3266856945002954,
      -1.370582637153722,
      -1.5541019540180232,
      -1.342263749703545,
      -1.4546958985536462,
      -1.323682127532049,
      -1.474129426595308,
      -1.540748443644232,
      -1.4847324818888532,
      -1.3689875717235163,
      -1.5082258459263476,
      -1.466170602187273,
      -1.5335027859569481,
      -1.4924889123705265,
      -1.5897452929198386
    ],
    [
      -1.4651478083258966,
      -1.485302015843213,
      0.0,
      -1.462686035218568,
      -1.3613887747023325,
      -1.4903653305685005,
      -1.530435616381874,
      -1.464030248644566,
      -1.398778687870931,
      -1.4801194504580917,
      -1.4747117809053634,
      -1.561851252078118,
      -1.4220222899412065,
      -1.466655928906128,
      -1.4557001940126753,
      -1.5076863811646974,
      -1.4686701656944174,
      -1.4518334727600255,
      -1.4538726511602589,
      -1.4482922688825106,
      -1.4798771053317412,
      -1.5392992686253255,
      -1.4892455278823917,
      -1.490944704413293,
      -1.4281442892761644,
      -1.5381656798767716,
      -1.5719535836644267,
      -1.5027729424133323,
      -1.597141652041403
    ],
    [
      -1.5058486804091848,
      -1.406240153661947,
      -1.3975742589944387,
      0.0,
      -1.4378134470966255,
      -1.4246982463289428,
      -1.5476664453171538,
      -1.4627889935457732,
      -1.3793650220663707,
      -1.4540046588851097,
      -1.3796348818711366,
      -1.605913986837099,
      -1.3043661961140431,
      -1.3662841363975313,
      -1.3550167930831547,
      -1.4059880625571826,
      -1.5285566379932403,
      -1.3903877613170228,
      -1.4526439208151227,
      -1.416717918413779,
      -1.4828740410330883,
      -1.496981216292056,
      -1.481188638252821,
      -1.397063500058538,
      -1.5071574663555574,
      -1.5231085051152473,
      -1.5387898065209114,
      -1.4606373095232035,
      -1.5800874212975695
    ],
    [
      -1.3686944033073922,
      -1.3614409255121942,
      -1.291598027720716,
      -1.3672738434998253,
      0.0,
      -1.3568293757183776,
      -1.368412248186548,
      -1.30917232277574,
      -1.3307688903115293,
      -1.3695262711261567,
      -1.34344917210436,
      -1.3976734852962487,
      -1.3395615787458424,
      -1.3617256448456094,
      -1.303963840024322,
      -1.3617933428405657,
      -1.3429970272417064,
      -1.3722630962920048,
      -1.3393138184258089,
      -1.31403272797292,
      -1.3624703456483844,
      -1.3892884721787853,
      -1.3680450870471352,
      -1.3219745825496292,
      -1.3495745724084445,
      -1.3780960971477376,
      -1.4044041502986269,
      -1.4123233885319832,
      -1.4222331591912718
    ],
    [
      -1.4671405710538556,
      -1.3490888281374491,
      -1.302940130857714,
      -1.305497435825602,
      -1.370317889086206,
      0.0,
      -1.44709330525388,
      -1.4225491943637147,
      -1.340044597997752,
      -1.3907336577764329,
      -1.3548230962975127,
      -1.5569642034719153,
      -1.3474751105281175,
      -1.3419699598988932,
      -1.3515798029068677,
      -1.3877712918369123,
      -1.5195289311662794,
      -1.3679234979275647,
      -1.3638209012176308,
      -1.3931957611070576,
      -1.4118832783106672,
      -1.487685599179909,
      -1.427323540840136,
      -1.3680889821080697,
      -1.4393225696399907,
      -1.4876650053175717,
      -1.5128090776527592,
      -1.411677269283347,
      -1.5537539254968438
    ],
    [
      -1.3741611355710162,
      -1.3366892573561049,
      -1.2858969383142127,
      -1.3616052560746539,
      -1.3142013055037616,
      -1.348658305726889,
      0.0,
      -1.3694488074052988,
      -1.3514262123704062,
      -1.3030987940377037,
      -1.2984366941907644,
      -1.4038376297877406,
      -1.300590102145,
      -1.32991275763547,
      -1.3317461610167909,
      -1.2848393797733368,
      -1.3738150420869621,
      -1.3208340860499599,
      -1.325303794800031,
      -1.3529622134379713,
      -1.3378813098994178,
      -1.41843564574273,
      -1.3657589177329195,
      -1.303009495081838,
      -1.3721600564231429,
      -1.4227931879228137,
      -1.3607500058556834,
      -1.3912384233246653,
      -1.421846621616148
    ],
    [
      -1.7040876846463702,
      -1.573051641018529,
      -1.505987711366243,
      -1.5546074945528603,
      -1.5339408297203538,
      -1.590568007358422,
      -1.6848916589953586,
      0.0,
      -1.4920779852257493,
      -1.6425491788744297,
      -1.5787625754211636,
      -1.7019878366458239,
      -1.5261147241895574,
      -1.566395047436136,
      -1.506225592606289,
      -1.6148603031947217,
      -1.6452957266467148,
      -1.5652179975457963,
      -1.5652235896183269,
      -1.5345606977337547,
      -1.65088276164096,
      -1.7155332048045053,
      -1.627731407376257,
      -1.5281596056003148,
      -1.6175364032215238,
      -1.5627227108510566,
      -1.6786239905619655,
      -1.6731690278893288,
      -1.6592483106233986
    ],
    [
      -1.4981813485914286,
      -1.4679179039108994,
      -1.4049835287540222,
      -1.4766148291522365,
      -1.4985212737180784,
      -1.5127823931660016,
      -1.6044492043174996,
      -1.4517420295428347,
      0.0,
      -1.5740326688857078,
      -1.4843977032563949,
      -1.6482532899100413,
      -1.3867058235129717,
      -1.4820386602904276,
      -1.505338607257447,
      -1.554694848297051,
      -1.5962321317542663,
      -1.4861081943219103,
      -1.5500799926212978,
      -1.4497741865345999,
      -1.5131736691209356,
      -1.6039111012563305,
      -1.571106032333679,
      -1.533637769377866,
      -1.480702163382951,
      -1.5148862277525619,
      -1.589993005457386,
      -1.5361536360322086,
      -1.600115217543841
    ],
    [
      -1.5452748868748212,
      -1.543842328805589,
      -1.463752127811692,
      -1.5096571994457717,
      -1.5223943167080383,
      -1.502433363910039,
      -1.5528331420000216,
      -1.5138859906996809,
      -1.4671109859247926,
      0.0,
      -1.4558630611593484,
      -1.6262351024431787,
      -1.421417271129175,
      -1.5148343872406722,
      -1.516165948352966,
      -1.4953085009343123,
      -1.508836159912246,
      -1.4694095164156469,
      -1.4794926935188077,
      -1.5248360865406283,
      -1.5057223795028065,
      -1.5593699791655844,
      -1.5000413089180824,
      -1.5058500920203173,
      -1.461727665762898,
      -1.6116744618794117,
      -1.5133571515161934,
      -1.5225902715489372,
      -1.5655734302121582
    ],
    [
      -1.5244959432202672,
      -1.4743630534438532,
      -1.467869012659899,
      -1.4325069915270705,
      -1.4906327404070958,
      -1.5033825105196341,
      -1.5367866745003518,
      -1.523882143986084,
      -1.448109087741842,
      -1.4400315741824705,
      0.0,
      -1.6304427057076512,
      -1.3796657185138888,
      -1.451923457061771,
      -1.4992885487766023,
      -1.4560735231784443,
      -1.5860011294487344,
      -1.423636123985995,
      -1.499208810945799,
      -1.4848519154290851,
      -1.4679675356166462,
      -1.5741386881451362,
      -1.5326102642804291,
      -1.4935369571564592,
      -1.5505293997424492,
      -1.549528310201718,
      -1.5599932580921199,
      -1.5571092488732672,
      -1.6566452091497361
    ],
    [
      -1.3593800033815933,
      -1.306641709101053,
      -1.2966804595135308,
      -1.3222045793703503,
      -1.224595766710713,
      -1.3255452934803837,
      -1.2136800352384827,
      -1.2577666885208338,
      -1.2907283894385657,
      -1.2642854170267246,
      -1.2905592704617714,
      0.0,
      -1.2806160087597152,
      -1.3020724592319455,
      -1.314291306360265,
      -1.3154216229353484,
      -1.2507329084753145,
      -1.3317216914620085,
      -1.2931567776399866,
      -1.2916578768526923,
      -1.238250275134578,
      -1.2617040816514755,
      -1.2887245317972722,
      -1.2909376513614474,
      -1.235727753525468,
      -1.3048153834055531,
      -1.2755257808336828,
      -1.316347985894705,
      -1.2667320154834576
    ],
    [
      -1.393567336722384,
      -1.3794014484742736,
      -1.3431685565383302,
      -1.2881767797181225,
      -1.3991223373627637,
      -1.4030695541199774,
      -1.4693821316650884,
      -1.3907330729158784,
      -1.2678472324545995,
      -1.4105975397893915,
      -1.3012172680487797,
      -1.5239956149381992,
      0.0,
      -1.3431028312058202,
      -1.3707531110601119,
      -1.3878152761010736,
      -1.493884175298175,
      -1.3047104384981494,
      -1.3479657989223968,
      -1.33384606545259,
      -1.386725301355628,
      -1.4499506523615484,
      -1.4197528518529026,
      -1.3543924294065797,
      -1.4303402283806594,
      -1.4630839631014747,
      -1.4858246740848344,
      -1.408756396539384,
      -1.5202153964113996
    ],
    [
      -1.3769360972577651,
      -1.2282604511041508,
      -1.3335514272338171,
      -1.2345676346879517,
      -1.3420997576491707,
      -1.3287111252470891,
      -1.4255969213272959,
      -1.3440774447421693,
      -1.2832398328321966,
      -1.3778644879366675,
      -1.322256722766812,
      -1.5015410940761802,
      -1.2387941238608373,
      0.0,
      -1.2475041788928376,
      -1.317991290081575,
      -1.4587738442879634,
      -1.2491754705215217,
      -1.3434854846397903,
      -1.2698797321086286,
      -1.3505382590735897,
      -1.4114151932830126,
      -1.4201658793226173,
      -1.2807457192499843,
      -1.4234300835654654,
      -1.3819246773160165,
      -1.4445572071353514,
      -1.379044381409749,
      -1.458107833300141
    ],
    [
      -1.5205305984315016,
      -1.388200220201086,
      -1.4571670889890973,
      -1.3825233513573694,
      -1.4079924201851,
      -1.4878517382550942,
      -1.5648802714460799,
      -1.4406071294755094,
      -1.4564282072787542,
      -1.506733357459058,
      -1.4752400203874476,
      -1.606884557935559,
      -1.4215347714047208,
      -1.3840090114958745,
      0.0,
      -1.5002994269665415,
      -1.5281717064768616,
      -1.4354486544662062,
      -1.4620517133089508,
      -1.4540736291085419,
      -1.5296220051623912,
      -1.5766145595818553,
      -1.5291847234141447,
      -1.4268990683321006,
      -1.5275829052316627,
      -1.5061329698395864,
      -1.6025609102886043,
      -1.524822690624746,
      -1.6106038725052005
    ],
    [
      -1.419942413497526,
      -1.2734657499011455,
      -1.3558106522591034,
      -1.3336799442043172,
      -1.369329630905649,
      -1.3255718644074055,
      -1.3563290696027328,
      -1.3835549845721953,
      -1.312773972674719,
      -1.32679156505274,
      -1.3214235794033167,
      -1.4353442876618558,
      -1.2469917231651562,
      -1.333384082572155,
      -1.367935140948753,
      0.0,
      -1.4410336455605126,
      -1.333239771545313,
      -1.3036368463799566,
      -1.314721623918724,
      -1.356544691043779,
      -1.411567350886217,
      -1.38019884748707,
      -1.350317391305875,
      -1.3971740639859356,
      -1.4392965162227196,
      -1.3985381113543196,
      -1.3826135065355925,
      -1.4420164356100786
    ],
    [
      -1.4797089768155558,
      -1.5101135741190013,
      -1.4279640277411725,
      -1.4357786611329322,
      -1.4256071092692937,
      -1.5207429280204356,
      -1.5289520403320926,
      -1.4471235379120329,
      -1.4592318452916884,
      -1.4470304958909759,
      -1.480419138392534,
      -1.4945734605071705,
      -1.4909489014495527,
      -1.4711314546141312,
      -1.4377928371863586,
      -1.4999529206148141,
      0.0,
      -1.4775728116098537,
      -1.488759925219896,
      -1.448807784840305,
      -1.4760191169728885,
      -1.4623356374860264,
      -1.4802589500681655,
      -1.5036560088572357,
      -1.4309946280672874,
      -1.4472206723348047,
      -1.5245411785885665,
      -1.5194848160561198,
      -1.4474559827605447
    ],
    [
      -1.4686778778021716,
      -1.4060804780403422,
      -1.4311971663791405,
      -1.3922602608505672,
      -1.5028424876656745,
      -1.4813336267187869,
      -1.5136126897567397,
      -1.4266356949762362,
      -1.3971644684669102,
      -1.410478781319454,
      -1.3549417236174923,
      -1.5643947298447225,
      -1.3039448791426478,
      -1.349680057567369,
      -1.4336263583727167,
      -1.4774674569461097,
      -1.5521883046426133,
      0.0,
      -1.4662108679496453,
      -1.3970146585308718,
      -1.4569164027412198,
      -1.5682782458190823,
      -1.5084784598226413,
      -1.4285953501958168,
      -1.4845129222434577,
      -1.5223399202436023,
      -1.5310729854341962,
      -1.4854289424149485,
      -1.5728080080246136
    ],
    [
      -1.3996196359542659,
      -1.3710242812022657,
      -1.3777149288664252,
      -1.370465262535224,
      -1.354193358072207,
      -1.3246937524713047,
      -1.4429428597638538,
      -1.3897046277970864,
      -1.3864672368742106,
      -1.4211500670929063,
      -1.3404645775711401,
      -1.5471151210626324,
      -1.3397634147130857,
      -1.4051209281348536,
      -1.3636384121107525,
      -1.3040678570796143,
      -1.4618189865745999,
      -1.3859744951967756,
      0.0,
      -1.4320048249036141,
      -1.4249502485761385,
      -1.4376640596609185,
      -1.4068789514837137,
      -1.2998018168297538,
      -1.465508575789544,
      -1.4992433739323336,
      -1.4646751151999629,
      -1.4568980050604785,
      -1.5049214259549606
    ],
    [
      -1.4472417651904133,
      -1.2720424614365942,
      -1.3284863423855298,
      -1.278239453937842,
      -1.3667597008263739,
      -1.4095063133607268,
      -1.4574264211673824,
      -1.3418517955846327,
      -1.2718028302456486,
      -1.4263478306079374,
      -1.352988899360524,
      -1.5083984831601502,
      -1.2686186547700289,
      -1.283875996531293,
      -1.2874423099752659,
      -1.3326380159275208,
      -1.4023809120653374,
      -1.2773925772474068,
      -1.3785240679921118,
      0.0,
      -1.3962024801195152,
      -1.4698939453772113,
      -1.4289379720582405,
      -1.3113176910233661,
      -1.3955995314818166,
      -1.3574980123608582,
      -1.4678802622672913,
      -1.3759012704890095,
      -1.4738420115111182
    ],
    [
      -1.5061641425320698,
      -1.5193457179136445,
      -1.457941134899206,
      -1.4559734235691255,
      -1.5248023425435138,
      -1.4850151521484314,
      -1.5324714701814002,
      -1.5352944649772335,
      -1.4963826083408656,
      -1.4334625081821326,
      -1.457027783127801,
      -1.5837648697766253,
      -1.4192931492201328,
      -1.5088280629853739,
      -1.506827411715736,
      -1.512487254028899,
      -1.5659435030134126,
      -1.477360067005323,
      -1.4906200922613309,
      -1.5287025034800203,
      0.0,
      -1.5432225090420664,
      -1.5280808122433724,
      -1.4919818477411386,
      -1.4727821975806943,
      -1.6351557305140167,
      -1.5625420512609747,
      -1.517509474170666,
      -1.6394904771816907
    ],
    [
      -1.5123874977548888,
      -1.4234043458991967,
      -1.4427507284789294,
      -1.4036375900857903,
      -1.482892780258168,
      -1.456743987574636,
      -1.5283534073006046,
      -1.4585513064230384,
      -1.395718412070281,
      -1.433535480702706,
      -1.4360499002906657,
      -1.5941416183178807,
      -1.4087822746749963,
      -1.3953090897187383,
      -1.4371749466085972,
      -1.4673305823387899,
      -1.4862990098090267,
      -1.3934929916005547,
      -1.4523095025297752,
      -1.433937592110732,
      -1.44651880311379,
      0.0,
      -1.4872607824289545,
      -1.4614959187707872,
      -1.4613820276078926,
      -1.5032020038127292,
      -1.4613569055864637,
      -1.4108219645843116,
      -1.5109858025858307
    ],
    [
      -1.5676719108002246,
      -1.5022979380494073,
      -1.5102587363379258,
      -1.525856348448036,
      -1.5281085286544673,
      -1.5364762765273945,
      -1.5931594679110384,
      -1.5483736837265654,
      -1.505055478663742,
      -1.4847010614573795,
      -1.4715245371737742,
      -1.631787034756905,
      -1.5262688467155108,
      -1.5495765100693102,
      -1.5451654952235403,
      -1.5155494828678697,
      -1.5511833945826585,
      -1.5023774234904093,
      -1.5585984207474424,
      -1.5666998567532655,
      -1.5101408435022725,
      -1.5334814196549333,
      0.0,
      -1.5261077657804152,
      -1.4110679716528687,
      -1.5908438710149264,
      -1.5216078400862947,
      -1.5573621758155065,
      -1.6179036292469913
    ],
    [
      -1.5443499199651496,
      -1.421554400331118,
      -1.4632929771350158,
      -1.414377373345112,
      -1.4798470543036684,
      -1.478163328029114,
      -1.5447476843849828,
      -1.4572638751872002,
      -1.400560146821067,
      -1.5256507726287456,
      -1.4479051542407733,
      -1.6193502856126878,
      -1.4045913365840095,
      -1.4472993487882444,
      -1.426686711081854,
      -1.455062591711964,
      -1.5837740041894846,
      -1.4625122964821355,
      -1.4460702056952757,
      -1.4312984394670085,
      -1.5221967279183177,
      -1.6059485107572364,
      -1.5642813158065605,
      0.0,
      -1.5392105779564516,
      -1.53957446909158,
      -1.557492886578152,
      -1.533534371615122,
      -1.6152156787569667
    ],
    [
      -1.7113501468705286,
      -1.6937681419704922,
      -1.5871533793962356,
      -1.6892489357942455,
      -1.6597782891516037,
      -1.6673488081582843,
      -1.7200319415207315,
      -1.6194211701599495,
      -1.6033553600501487,
      -1.6675896598479842,
      -1.691932495027512,
      -1.7492234985949986,
      -1.657881030340055,
      -1.6863569725732654,
      -1.6790771043902213,
      -1.6959443191589916,
      -1.655356326697015,
      -1.662932865459064,
      -1.7145470424152458,
      -1.6118210164692808,
      -1.6260089407134652,
      -1.7209460008109645,
      -1.6536046832291196,
      -1.7090724965519757,
      0.0,
      -1.7224609479070345,
      -1.708268553799539,
      -1.6656278575793428,
      -1.7220495394587163
    ],
    [
      -1.4213629235542034,
      -1.3279551438543933,
      -1.392990129533209,
      -1.3589815266249252,
      -1.3811474326452355,
      -1.41352725418692,
      -1.4675660575842908,
      -1.345945805389404,
      -1.363972979805049,
      -1.4056401621187304,
      -1.4015828386401579,
      -1.459878255346916,
      -1.3682908947411652,
      -1.3061002572418607,
      -1.3280377121856721,
      -1.436993167419813,
      -1.4042586812874087,
      -1.3767402106235023,
      -1.4169029750316067,
      -1.3639661487425094,
      -1.396333239565216,
      -1.4596222247125985,
      -1.415522802040027,
      -1.3630776335850021,
      -1.3981991211950062,
      0.0,
      -1.433737919361912,
      -1.4277098794074672,
      -1.4404580961142504
    ],
    [
      -1.4112541387755029,
      -1.3707913566237202,
      -1.3628769628485797,
      -1.3602903501238324,
      -1.3772914471163629,
      -1.3763556803369115,
      -1.371337063155577,
      -1.3766799955921272,
      -1.378711421761769,
      -1.3006216045245038,
      -1.343733659507614,
      -1.5089516147887834,
      -1.3102006092041008,
      -1.3756415972701133,
      -1.3828246263562032,
      -1.337563400603789,
      -1.4332449374005243,
      -1.3622468071798468,
      -1.3557701775094457,
      -1.3883372010999466,
      -1.364306262363883,
      -1.3670689010815553,
      -1.3235789684111827,
      -1.3804146613410082,
      -1.3094333639227325,
      -1.4213609563530263,
      0.0,
      -1.3811770019539078,
      -1.454171522382068
    ],
    [
      -1.5842630542415694,
      -1.4816648097532972,
      -1.5028101742516704,
      -1.4544528238258456,
      -1.5578313687485594,
      -1.5059428646794806,
      -1.5848490577922518,
      -1.5555055282388537,
      -1.4721262699631334,
      -1.5034547293255909,
      -1.4817631724746443,
      -1.613636508763027,
      -1.4494089716828582,
      -1.4759804782374475,
      -1.5211377560196846,
      -1.5236069898469369,
      -1.5825972310415823,
      -1.464861076166999,
      -1.5416699515503478,
      -1.487958061813316,
      -1.5124585271902364,
      -1.5395495045656775,
      -1.5221357345012123,
      -1.5000217223245444,
      -1.5094439595357583,
      -1.5560488378854178,
      -1.5359656331461589,
      0.0,
      -1.5947575376349248
    ],
    [
      -1.2836452797984803,
      -1.1967598726857958,
      -1.1637587032394516,
      -1.202456250387483,
      -1.1606218918296745,
      -1.1790295183749324,
      -1.1864316431638637,
      -1.147914624931901,
      -1.1417925568194276,
      -1.197739711873105,
      -1.19754778943688,
      -1.1903557470649253,
      -1.1984213713559035,
      -1.1932588451114454,
      -1.1879780509305764,
      -1.2176831938013104,
      -1.152007460558604,
      -1.1951524566974827,
      -1.221081634389996,
      -1.159631127077523,
      -1.2083311627499416,
      -1.1842230184408766,
      -1.1795071506451846,
      -1.2010878846442667,
      -1.1546076510853611,
      -1.1817868049332267,
      -1.1727014366685962,
      -1.1831255319572083,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.06895303526298235,
      0.08323577207601485,
      0.07450727036053006,
      0.08167941068088735,
      0.06926980395641547,
      0.06784265157642189,
      0.07825220525504184,
      0.08562248329909128,
      0.06417595300787982,
      0.07287604130022896,
      0.057221054270157445,
      0.08518259868769196,
      0.07263216617486057,
      0.06453322388755511,
      0.06266531548743703,
      0.05689981068557959,
      0.08119648320953399,
      0.06212147402621149,
      0.07947407098907178,
      0.07911682782921803,
      0.05585275947296253,
      0.07581575886049041,
      0.0651249947286554,
      0.07440042367464939,
      0.0682178031901417,
      0.06555429893251885,
      0.06713658708300985,
      0.05413591516282734
    ],
    [
      0.3875308016994514,
      0.0,
      0.4426239976642834,
      0.48824783835145413,
      0.40147433644725883,
      0.42680569460393025,
      0.33672256362559017,
      0.4131568602496527,
      0.4830705975412324,
      0.3809324495490243,
      0.46428142016606966,
      0.2755028643999451,
      0.4924375790573532,
      0.5637743753759656,
      0.5331780530636883,
      0.4892811104102617,
      0.30576179354596045,
      0.5175999978604386,
      0.4051678490103374,
      0.5361816200319347,
      0.38573432096867566,
      0.3191153039197516,
      0.37513126567513044,
      0.4908761758404674,
      0.3516379016376361,
      0.39369314537671074,
      0.32636096160703554,
      0.36737483519345715,
      0.2701184546441451
    ],
    [
      0.42270075317180344,
      0.402546545654487,
      0.0,
      0.4251625262791321,
      0.5264597867953675,
      0.39748323092919957,
      0.3574129451158261,
      0.4238183128531341,
      0.4890698736267691,
      0.4077291110396084,
      0.4131367805923367,
      0.32599730941958205,
      0.4658262715564936,
      0.421192632591572,
      0.43214836748502483,
      0.38016218033300264,
      0.41917839580328264,
      0.4360150887376746,
      0.4339759103374412,
      0.43955629261518947,
      0.40797145616595887,
      0.34854929287237457,
      0.39860303361530836,
      0.396903857084407,
      0.45970427222153565,
      0.34968288162092853,
      0.31589497783327336,
      0.3850756190843678,
      0.29070690945629707
    ],
    [
      0.39081357769116165,
      0.4904221044383994,
      0.49908799910590784,
      0.0,
      0.458848811003721,
      0.4719640117714037,
      0.3489958127831927,
      0.4338732645545733,
      0.5172972360339758,
      0.44265759921523684,
      0.5170273762292099,
      0.2907482712632474,
      0.5922960619863034,
      0.5303781217028152,
      0.5416454650171918,
      0.49067419554316394,
      0.36810562010710624,
      0.5062744967833237,
      0.4440183372852238,
      0.47994433968656747,
      0.4137882170672582,
      0.39968104180829056,
      0.4154736198475255,
      0.4995987580418084,
      0.38950479174478914,
      0.3735537529850992,
      0.35787245157943515,
      0.436024948577143,
      0.316574836802777
    ],
    [
      0.21997282144103658,
      0.2272262992362346,
      0.29706919702771284,
      0.22139338124860353,
      0.0,
      0.23183784903005122,
      0.22025497656188087,
      0.2794949019726889,
      0.25789833443689947,
      0.21914095362227215,
      0.24521805264406882,
      0.19099373945218012,
      0.24910564600258644,
      0.22694157990281938,
      0.2847033847241067,
      0.22687388190786306,
      0.24567019750672237,
      0.21640412845642398,
      0.24935340632261993,
      0.2746344967755088,
      0.22619687910004438,
      0.19937875256964355,
      0.22062213770129357,
      0.2666926421987996,
      0.23909265233998434,
      0.2105711276006912,
      0.18426307444980194,
      0.17634383621644556,
      0.166434065557157
    ],
    [
      0.36898696435625866,
      0.4870387072726652,
      0.5331874045524003,
      0.5306300995845123,
      0.4658096463239083,
      0.0,
      0.3890342301562344,
      0.41357834104639957,
      0.4960829374123623,
      0.44539387763368143,
      0.48130443911260157,
      0.279163331938199,
      0.4886524248819968,
      0.4941575755112211,
      0.48454773250324656,
      0.44835624357320203,
      0.3165986042438349,
      0.4682040374825496,
      0.47230663419248353,
      0.44293177430305675,
      0.42424425709944713,
      0.34844193623020536,
      0.4088039945699784,
      0.4680385533020446,
      0.3968049657701236,
      0.3484625300925426,
      0.3233184577573551,
      0.4244502661267673,
      0.28237360991327054
    ],
    [
      0.33532733379648083,
      0.3727992120113921,
      0.4235915310532843,
      0.3478832132928431,
      0.3952871638637354,
      0.36083016364060794,
      0.0,
      0.34003966196219815,
      0.3580622569970908,
      0.4063896753297933,
      0.41105177517673264,
      0.30565083957975636,
      0.40889836722249706,
      0.3795757117320271,
      0.3777423083507061,
      0.42464908959416015,
      0.33567342728053484,
      0.3886543833175371,
      0.38418467456746597,
      0.3565262559295257,
      0.3716071594680792,
      0.2910528236247669,
      0.3437295516345775,
      0.40647897428565893,
      0.3373284129443541,
      0.28669528144468326,
      0.3487384635118136,
      0.3182500460428317,
      0.28764184775134893
    ],
    [
      0.3330759637514711,
      0.46411200737931235,
      0.5311759370315983,
      0.48255615384498096,
      0.5032228186774874,
      0.4465956410394192,
      0.3522719894024826,
      0.0,
      0.545085663172092,
      0.3946144695234115,
      0.45840107297667765,
      0.3351758117520174,
      0.5110489242082839,
      0.47076860096170536,
      0.5309380557915522,
      0.4223033452031195,
      0.3918679217511265,
      0.471945650852045,
      0.4719400587795144,
      0.5026029506640866,
      0.3862808867568812,
      0.3216304435933359,
      0.40943224102158426,
      0.5090040427975264,
      0.41962724517631744,
      0.4744409375467846,
      0.3585396578358757,
      0.3639946205085125,
      0.37791533777444264
    ],
    [
      0.4201044036141328,
      0.45036784829466203,
      0.5133022234515392,
      0.4416709230533249,
      0.419764478487483,
      0.4055033590395598,
      0.3138365478880618,
      0.4665437226627267,
      0.0,
      0.3442530833198536,
      0.43388804894916655,
      0.27003246229552014,
      0.5315799286925897,
      0.4362470919151338,
      0.41294714494811435,
      0.36359090390851034,
      0.32205362045129515,
      0.4321775578836511,
      0.3682057595842636,
      0.4685115656709615,
      0.40511208308462576,
      0.3143746509492309,
      0.34717971987188245,
      0.3846479828276954,
      0.4375835888226105,
      0.40339952445299954,
      0.3282927467481753,
      0.38213211617335285,
      0.3181705346617205
    ],
    [
      0.3510866138342772,
      0.35251917190350945,
      0.4326093728974063,
      0.38670430126332667,
      0.3739671840010601,
      0.39392813679905947,
      0.3435283587090767,
      0.3824755100094175,
      0.42925051478430576,
      0.0,
      0.44049843954975,
      0.27012639826591966,
      0.4749442295799233,
      0.38152711346842616,
      0.38019555235613245,
      0.40105299977478603,
      0.3875253407968524,
      0.4269519842934515,
      0.4168688071902906,
      0.37152541416847007,
      0.39063912120629185,
      0.33699152154351397,
      0.39632019179101596,
      0.3905114086887811,
      0.43463383494620045,
      0.2846870388296867,
      0.38300434919290494,
      0.37377122916016114,
      0.33078807049694015
    ],
    [
      0.4186239570592649,
      0.46875684683567886,
      0.4752508876196331,
      0.5106129087524616,
      0.45248715987243626,
      0.43973738975989796,
      0.4063332257791803,
      0.41923775629344817,
      0.49501081253768997,
      0.5030883260970616,
      0.0,
      0.3126771945718809,
      0.5634541817656433,
      0.4911964432177611,
      0.44383135150292974,
      0.4870463771010878,
      0.3571187708307977,
      0.5194837762935371,
      0.443911089333733,
      0.45826798485044695,
      0.4751523646628859,
      0.36898121213439583,
      0.41050963599910295,
      0.4495829431230729,
      0.3925905005370829,
      0.39359159007781397,
      0.3831266421874122,
      0.3860106514062649,
      0.28647469112979596
    ],
    [
      0.13972982460669803,
      0.19246811888723836,
      0.20242936847476045,
      0.176905248617941,
      0.27451406127757827,
      0.17356453450790754,
      0.2854297927498086,
      0.24134313946745745,
      0.20838143854972557,
      0.2348244109615667,
      0.20855055752651985,
      0.0,
      0.21849381922857614,
      0.1970373687563458,
      0.18481852162802626,
      0.18368820505294292,
      0.2483769195129768,
      0.16738813652628282,
      0.2059530503483047,
      0.207451951135599,
      0.2608595528537132,
      0.23740574633681577,
      0.2103852961910191,
      0.20817217662684384,
      0.26338207446282325,
      0.19429444458273815,
      0.22358404715460845,
      0.18276184209358637,
      0.2323778125048337
    ],
    [
      0.4062223626372472,
      0.4203882508853576,
      0.45662114282130095,
      0.5116129196415087,
      0.4006673619968675,
      0.3967201452396538,
      0.3304075676945428,
      0.40905662644375274,
      0.5319424669050317,
      0.38919215957023967,
      0.4985724313108515,
      0.27579408442143194,
      0.0,
      0.456686868153811,
      0.4290365882995193,
      0.4119744232585576,
      0.3059055240614561,
      0.4950792608614818,
      0.4518239004372344,
      0.4659436339070411,
      0.4130643980040032,
      0.34983904699808277,
      0.3800368475067286,
      0.4453972699530515,
      0.36944947097897174,
      0.3367057362581565,
      0.31396502527479675,
      0.3910333028202473,
      0.2795743029482316
    ],
    [
      0.42335188433443416,
      0.5720275304880484,
      0.4667365543583821,
      0.5657203469042476,
      0.4581882239430286,
      0.47157685634511015,
      0.3746910602649034,
      0.45621053685003,
      0.5170481487600027,
      0.4224234936555318,
      0.4780312588253872,
      0.2987468875160191,
      0.561493857731362,
      0.0,
      0.5527838026993617,
      0.4822966915106244,
      0.34151413730423585,
      0.5511125110706776,
      0.45680249695240893,
      0.5304082494835707,
      0.44974972251860956,
      0.3888727883091867,
      0.380122102269582,
      0.519542262342215,
      0.37685789802673386,
      0.41836330427618273,
      0.35573077445684786,
      0.4212436001824502,
      0.34218014829205834
    ],
    [
      0.4033171263509432,
      0.5356475045813589,
      0.4666806357933475,
      0.5413243734250754,
      0.5158553045973449,
      0.4359959865273506,
      0.35896745333636493,
      0.4832405953069354,
      0.4674195175036906,
      0.4171143673233868,
      0.4486077043949972,
      0.31696316684688575,
      0.502312953377724,
      0.5398387132865703,
      0.0,
      0.4235482978159033,
      0.3956760183055832,
      0.48839907031623864,
      0.46179601147349403,
      0.46977409567390294,
      0.3942257196200536,
      0.34723316520058956,
      0.3946630013683001,
      0.49694865645034425,
      0.3962648195507821,
      0.41771475494285837,
      0.32128681449384056,
      0.39902503415769885,
      0.3132438522772443
    ],
    [
      0.2713231755898311,
      0.41779983918621166,
      0.3354549368282538,
      0.3575856448830399,
      0.32193595818170806,
      0.36569372467995165,
      0.33493651948462433,
      0.30771060451516186,
      0.3784916164126382,
      0.36447402403461715,
      0.36984200968404046,
      0.2559213014255013,
      0.444273865922201,
      0.3578815065152021,
      0.3233304481386041,
      0.0,
      0.2502319435268445,
      0.3580258175420441,
      0.38762874270740055,
      0.3765439651686331,
      0.3347208980435781,
      0.27969823820114015,
      0.31106674160028724,
      0.3409481977814821,
      0.29409152510142156,
      0.25196907286463754,
      0.29272747773303753,
      0.3086520825517647,
      0.2492491534772785
    ],
    [
      0.3551228922916456,
      0.32471829498820015,
      0.40686784136602894,
      0.39905320797426924,
      0.40922475983790774,
      0.31408894108676577,
      0.3058798287751088,
      0.38770833119516857,
      0.37560002381551305,
      0.38780137321622554,
      0.3544127307146674,
      0.34025840860003087,
      0.3438829676576487,
      0.3637004144930702,
      0.3970390319208428,
      0.3348789484923873,
      0.0,
      0.35725905749734777,
      0.34607194388730544,
      0.3860240842668965,
      0.35881275213431296,
      0.37249623162117507,
      0.35457291903903587,
      0.3311758602499657,
      0.40383724103991403,
      0.38761119677239675,
      0.3102906905186349,
      0.31534705305108157,
      0.38737588634665676
    ],
    [
      0.44527386403179636,
      0.5078712637936258,
      0.4827545754548275,
      0.5216914809834008,
      0.41110925416829347,
      0.4326181151151811,
      0.4003390520772283,
      0.4873160468577318,
      0.5167872733670578,
      0.5034729605145141,
      0.5590100182164757,
      0.3495570119892455,
      0.6100068626913202,
      0.5642716842665989,
      0.4803253834612513,
      0.4364842848878583,
      0.36176343719135473,
      0.0,
      0.4477408738843227,
      0.5169370833030962,
      0.4570353390927482,
      0.34567349601488573,
      0.4054732820113267,
      0.48535639163815114,
      0.42943881959051033,
      0.39161182159036567,
      0.38287875639977176,
      0.4285227994190195,
      0.34114373380935437
    ],
    [
      0.44658274601194625,
      0.4751781007639464,
      0.4684874530997869,
      0.47573711943098806,
      0.49200902389400514,
      0.5215086294949074,
      0.4032595222023583,
      0.45649775416912575,
      0.45973514509200153,
      0.42505231487330586,
      0.505737804395072,
      0.29908726090357973,
      0.5064389672531264,
      0.44108145383135855,
      0.4825639698554596,
      0.5421345248865979,
      0.38438339539161226,
      0.4602278867694365,
      0.0,
      0.414197557062598,
      0.4212521333900736,
      0.40853832230529363,
      0.43932343048249844,
      0.5464005651364583,
      0.38069380617666804,
      0.34695900803387847,
      0.38152726676624926,
      0.3893043769057336,
      0.3412809560112515
    ],
    [
      0.3584356442169918,
      0.5336349479708109,
      0.4771910670218753,
      0.527437955469563,
      0.4389177085810312,
      0.3961710960466782,
      0.34825098824002265,
      0.4638256138227723,
      0.5338745791617565,
      0.3793295787994677,
      0.45268851004688115,
      0.2972789262472548,
      0.5370587546373762,
      0.5218014128761121,
      0.5182350994321392,
      0.4730393934798842,
      0.40329649734206763,
      0.5282848321599982,
      0.42715334141529326,
      0.0,
      0.40947492928788987,
      0.33578346403019377,
      0.3767394373491646,
      0.4943597183840389,
      0.4100778779255885,
      0.44817939704654686,
      0.3377971471401138,
      0.4297761389183956,
      0.3318353978962869
    ],
    [
      0.39807642417538647,
      0.38489484879381175,
      0.44629943180825027,
      0.4482671431383307,
      0.37943822416394246,
      0.41922541455902484,
      0.37176909652605605,
      0.3689461017302227,
      0.40785795836659067,
      0.4707780585253236,
      0.4472127835796553,
      0.3204756969308309,
      0.4849474174873234,
      0.39541250372208236,
      0.39741315499172014,
      0.3917533126785573,
      0.33829706369404366,
      0.42688049970213315,
      0.4136204744461254,
      0.3755380632274359,
      0.0,
      0.36101805766538986,
      0.3761597544640838,
      0.4122587189663176,
      0.43145836912676194,
      0.2690848361934395,
      0.34169851544648155,
      0.38673109253679017,
      0.2647500895257655
    ],
    [
      0.3652744770098446,
      0.45425762886553667,
      0.434911246285804,
      0.47402438467894314,
      0.39476919450656545,
      0.42091798719009743,
      0.34930856746412875,
      0.41911066834169497,
      0.4819435626944524,
      0.4441264940620273,
      0.4416120744740677,
      0.2835203564468527,
      0.4688797000897371,
      0.48235288504599505,
      0.44048702815613616,
      0.4103313924259435,
      0.39136296495570666,
      0.48416898316417867,
      0.4253524722349582,
      0.4437243826540014,
      0.4311431716509433,
      0.0,
      0.39040119233577886,
      0.4161660559939462,
      0.4162799471568408,
      0.37445997095200423,
      0.4163050691782697,
      0.46684001018042176,
      0.3666761721789027
    ],
    [
      0.41450866897954386,
      0.4798826417303612,
      0.4719218434418426,
      0.4563242313317324,
      0.45407205112530113,
      0.445704303252374,
      0.3890211118687301,
      0.4338068960532031,
      0.47712510111602646,
      0.497479518322389,
      0.5106560426059943,
      0.35039354502286346,
      0.4559117330642577,
      0.4326040697104583,
      0.4370150845562282,
      0.46663109691189875,
      0.43099718519710994,
      0.4798031562893592,
      0.4235821590323261,
      0.41548072302650296,
      0.47203973627749596,
      0.44869916012483513,
      0.0,
      0.4560728139993533,
      0.5711126081268998,
      0.39133670876484206,
      0.4605727396934738,
      0.42481840396426196,
      0.3642769505327772
    ],
    [
      0.3833391268652242,
      0.5061346464992558,
      0.46439606969535796,
      0.5133116734852619,
      0.4478419925267054,
      0.4495257188012598,
      0.382941362445391,
      0.47042517164317355,
      0.5271289000093067,
      0.4020382742016282,
      0.4797838925896005,
      0.308338761217686,
      0.5230977102463643,
      0.4803896980421294,
      0.5010023357485198,
      0.47262645511840984,
      0.34391504264088923,
      0.46517675034823824,
      0.4816188411350981,
      0.49639060736336527,
      0.40549231891205606,
      0.32174053607313735,
      0.3634077310238133,
      0.0,
      0.38847846887392223,
      0.38811457773879376,
      0.3701961602522217,
      0.3941546752152518,
      0.3124733680734071
    ],
    [
      0.33921121212619765,
      0.356793217026234,
      0.4634079796004906,
      0.3613124232024807,
      0.39078306984512245,
      0.38321255083844186,
      0.33052941747599474,
      0.43114018883677674,
      0.44720599894657753,
      0.38297169914874196,
      0.3586288639692141,
      0.3013378604017276,
      0.39268032865667113,
      0.36420438642346076,
      0.3714842546065049,
      0.3546170398377346,
      0.39520503229971116,
      0.38762849353766216,
      0.3360143165814804,
      0.4387403425274454,
      0.42455241828326096,
      0.3296153581857617,
      0.3969566757676066,
      0.3414888624447505,
      0.0,
      0.3281004110896917,
      0.3422928051971872,
      0.38493350141738336,
      0.3285118195380099
    ],
    [
      0.3099808000326938,
      0.40338857973250386,
      0.33835359405368814,
      0.3723621969619719,
      0.3501962909416616,
      0.3178164693999772,
      0.2637776660026063,
      0.38539791819749314,
      0.3673707437818481,
      0.3257035614681667,
      0.3297608849467393,
      0.2714654682399811,
      0.363052828845732,
      0.4252434663450364,
      0.403306011401225,
      0.29435055616708405,
      0.3270850422994884,
      0.3546035129633949,
      0.3144407485552905,
      0.3673775748443877,
      0.3350104840216812,
      0.27172149887429864,
      0.31582092154687014,
      0.36826609000189503,
      0.3331446023918909,
      0.0,
      0.29760580422498517,
      0.30363384417942996,
      0.2908856274726468
    ],
    [
      0.3037959983307581,
      0.34425878048254077,
      0.35217317425768124,
      0.3547597869824286,
      0.3377586899898981,
      0.3386944567693495,
      0.34371307395068396,
      0.3383701415141338,
      0.3363387153444919,
      0.41442853258175716,
      0.37131647759864705,
      0.2060985223174776,
      0.4048495279021602,
      0.3394085398361477,
      0.3322255107500578,
      0.3774867365024719,
      0.2818051997057367,
      0.35280332992641417,
      0.35927995959681525,
      0.32671293600631435,
      0.3507438747423779,
      0.34798123602470565,
      0.39147116869507825,
      0.33463547576525277,
      0.4056167731835285,
      0.2936891807532347,
      0.0,
      0.3338731351523532,
      0.26087861472419305
    ],
    [
      0.2792412506286108,
      0.3818394951168831,
      0.36069413061850986,
      0.40905148104433464,
      0.30567293612162083,
      0.3575614401906997,
      0.27865524707792844,
      0.3079987766313266,
      0.3913780349070468,
      0.3600495755445894,
      0.381741132395536,
      0.24986779610715315,
      0.41409533318732206,
      0.38752382663273277,
      0.34236654885049567,
      0.3398973150232434,
      0.2809070738285979,
      0.39864322870318136,
      0.32183435331983246,
      0.37554624305686435,
      0.3510457776799438,
      0.32395480030450274,
      0.34136857036896795,
      0.3634825825456358,
      0.354060345334422,
      0.30745546698476245,
      0.3275386717240214,
      0.0,
      0.2687467672352555
    ],
    [
      0.26896841502623525,
      0.3558538221389198,
      0.38885499158526393,
      0.3501574444372326,
      0.3919918029950411,
      0.37358417644978315,
      0.3661820516608518,
      0.40469906989281457,
      0.41082113800528797,
      0.35487398295161054,
      0.35506590538783556,
      0.36225794775979026,
      0.354192323468812,
      0.35935484971327014,
      0.36463564389413916,
      0.33493050102340516,
      0.40060623426611164,
      0.35746123812723285,
      0.3315320604347196,
      0.39298256774719253,
      0.344282532074774,
      0.36839067638383893,
      0.37310654417953093,
      0.3515258101804488,
      0.39800604373935444,
      0.3708268898914888,
      0.3799122581561194,
      0.3694881628675073,
      0.0
    ]
  ],
  "row_avgs": [
    0.0704891497545738,
    0.41513479169703,
    0.4061665933889778,
    0.44346946859485187,
    0.23120651414307644,
    0.4261036991765196,
    0.3608692716216601,
    0.4371631589204873,
    0.39769548648938724,
    0.38280829319646204,
    0.43614809540472843,
    0.2130204093079728,
    0.4043826114031842,
    0.4512081117739726,
    0.434038739796386,
    0.33007889399218343,
    0.36111117545893584,
    0.4536594630650469,
    0.4398278748067614,
    0.4353546233909355,
    0.3903665395061385,
    0.4210252871562064,
    0.44685179586151563,
    0.43012431667090956,
    0.3736985902790115,
    0.3357543852819524,
    0.3405416981923818,
    0.341507792898715,
    0.36551946730137896
  ],
  "col_avgs": [
    0.3557135387021916,
    0.4082778317935775,
    0.41840608425161546,
    0.41878598852228294,
    0.40192666803024885,
    0.3806477081094307,
    0.3340818814605458,
    0.38940266851172334,
    0.42831789544930554,
    0.38516106707474684,
    0.4103183760485366,
    0.2857375814143827,
    0.4446105416103242,
    0.4134708950073104,
    0.4087313949293029,
    0.3902616006396465,
    0.3352779362330936,
    0.41349476252412387,
    0.38372499096685697,
    0.40749752950498813,
    0.3778339047141744,
    0.3265254129061538,
    0.3572391702424129,
    0.4014163514778239,
    0.3769699743072255,
    0.33940972828407506,
    0.32967414662308075,
    0.35752513611377473,
    0.29488553307838844
  ],
  "combined_avgs": [
    0.21310134422838267,
    0.41170631174530375,
    0.41228633882029664,
    0.4311277285585674,
    0.31656659108666263,
    0.40337570364297515,
    0.34747557654110295,
    0.4132829137161053,
    0.4130066909693464,
    0.38398468013560444,
    0.4232332357266325,
    0.24937899536117775,
    0.4244965765067542,
    0.4323395033906415,
    0.4213850673628444,
    0.36017024731591496,
    0.3481945558460147,
    0.4335771127945854,
    0.4117764328868092,
    0.4214260764479618,
    0.38410022211015643,
    0.3737753500311801,
    0.4020454830519643,
    0.4157703340743667,
    0.3753342822931185,
    0.33758205678301373,
    0.33510792240773124,
    0.34951646450624485,
    0.33020250018988373
  ],
  "gppm": [
    622.3187861105156,
    736.2526697846826,
    730.0203350735027,
    731.8516600079735,
    731.1315754245201,
    748.868669619651,
    768.565418371034,
    742.9782017536796,
    726.3392901872533,
    745.7171121018109,
    734.7052641722149,
    789.4070800334376,
    720.6427107946764,
    734.8010161833851,
    734.5131166795411,
    746.9803619424382,
    770.9601835462933,
    733.6830551081565,
    750.2871710669178,
    735.5311359502675,
    747.7176399458286,
    774.5603839653003,
    758.1127576941805,
    737.7530828754016,
    747.2611288278538,
    761.2432870630082,
    772.476947317521,
    755.6852464488231,
    792.3073949344904
  ],
  "gppm_normalized": [
    1.3828700898596493,
    1.8664463533107316,
    1.8593721574100035,
    1.8533890845023178,
    1.8540323522996636,
    1.8902227296421195,
    1.9397798122336414,
    1.8695984532420458,
    1.8377304652815751,
    1.8820011236095258,
    1.8521306340897576,
    1.9825054532148518,
    1.829126006956881,
    1.8572458358941744,
    1.8578873692270783,
    1.8823190035101613,
    1.9387034102281897,
    1.8566974987151907,
    1.8913749082022207,
    1.8636788329254599,
    1.880983348276007,
    1.94161878037139,
    1.9036751055219903,
    1.8620748751916236,
    1.8843649832615703,
    1.9101178223082669,
    1.9391034383202685,
    1.904122327142805,
    1.9810162425499507
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391,
    303,
    497,
    433,
    459,
    752,
    459,
    495,
    438,
    433,
    410,
    431,
    387,
    410,
    444,
    431,
    438,
    393,
    372,
    470,
    373,
    439,
    420,
    389,
    490,
    384,
    410,
    374,
    372,
    394,
    324,
    414,
    458,
    484,
    436,
    428,
    501,
    399,
    399,
    381,
    466,
    408,
    398,
    376,
    425,
    390,
    398,
    422,
    389,
    407,
    452,
    424,
    401,
    424,
    397,
    379,
    413,
    401,
    344,
    781,
    392,
    394,
    392,
    352,
    525,
    360,
    385,
    392,
    397,
    413,
    380,
    455,
    358,
    381,
    399,
    424,
    396,
    397,
    404,
    410,
    394,
    346,
    390,
    420,
    367,
    398,
    386,
    349,
    693,
    449,
    447,
    450,
    437,
    450,
    518,
    423,
    431,
    416,
    460,
    437,
    476,
    460,
    444,
    463,
    451,
    458,
    421,
    440,
    470,
    414,
    457,
    429,
    431,
    416,
    443,
    439,
    406,
    1366,
    431,
    451,
    431,
    382,
    435,
    379,
    402,
    392,
    554,
    420,
    469,
    441,
    450,
    466,
    420,
    414,
    410,
    438,
    442,
    378,
    440,
    388,
    437,
    397,
    387,
    391,
    431,
    399,
    1428,
    431,
    434,
    454,
    409,
    434,
    388,
    406,
    484,
    448,
    357,
    618,
    424,
    397,
    365,
    453,
    437,
    439,
    452,
    382,
    412,
    347,
    390,
    420,
    459,
    322,
    408,
    451,
    409,
    557,
    395,
    451,
    431,
    438,
    437,
    417,
    383,
    398,
    374,
    428,
    387,
    416,
    435,
    406,
    409,
    457,
    395,
    425,
    379,
    415,
    441,
    340,
    408,
    417,
    426,
    414,
    438,
    335,
    468,
    442,
    473,
    474,
    383,
    401,
    465,
    446,
    413,
    388,
    399,
    481,
    480,
    453,
    411,
    415,
    389,
    446,
    406,
    459,
    411,
    398,
    367,
    435,
    415,
    350,
    346,
    429,
    364,
    533,
    424,
    479,
    491,
    712,
    431,
    444,
    521,
    415,
    476,
    408,
    564,
    485,
    428,
    449,
    470,
    420,
    441,
    430,
    446,
    447,
    421,
    459,
    433,
    414,
    473,
    353,
    452,
    397,
    650,
    429,
    489,
    455,
    450,
    445,
    432,
    435,
    464,
    421,
    395,
    489,
    392,
    476,
    454,
    450,
    376,
    391,
    389,
    441,
    381,
    413,
    421,
    418,
    441,
    428,
    410,
    396,
    417,
    1943,
    416,
    433,
    407,
    650,
    411,
    434,
    376,
    416,
    422,
    408,
    502,
    428,
    414,
    429,
    420,
    352,
    404,
    352,
    439,
    417,
    355,
    372,
    425,
    401,
    534,
    404,
    461,
    349
  ],
  "response_lengths": [
    9209,
    2429,
    2494,
    2315,
    3682,
    2406,
    2601,
    2039,
    2398,
    2446,
    2468,
    2842,
    2456,
    2340,
    2508,
    2514,
    1974,
    2313,
    1989,
    2308,
    2345,
    1921,
    2142,
    2525,
    2373,
    2967,
    2359,
    2675,
    1966
  ]
}