{
  "example_idx": 0,
  "reference": "Under review as a conference paper at ICLR 2023\n\nRECURSION OF THOUGHT: DIVIDE AND CONQUER REASONING WITH LANGUAGE MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWith the recent advances in language models, attempts are being made to apply them to solving multi-step reasoning problems. A major breakthrough in this line of research is to let language models generate intermediate steps, often called Chain of Thought (CoT), before producing a final answer. However, language models have an upper bound on the context size, i.e., the number of input tokens, such as 2048 for the recent GPT-3 and PaLM. Although several thousand tokens are enough to handle various tasks, solving more complex reasoning tasks can require orders of magnitude more tokens. Therefore, the context limit imposes a fundamental limit on the model’s reasoning capability. Inspired by human’s incredible reasoning ability based on abstraction and recursion, we propose Recursion of Thought (RoT) as a model-agnostic framework with the novel paradigm of teaching a language model to divide and conquer complex problems by recursively creating multiple contexts. Since RoT casts the context-related operations as tokens, a language model can trigger the recursion operations by simply producing the corresponding tokens. On multiple arithmetic and algorithmic reasoning tasks, we demonstrate that RoT dramatically improves the recent large-scale language model GPT-3 to solve extremely complex problems. Moreover, RoT can make tiny, randomly initialized Transformers or LSTMs to solve problems that even humans find daunting.\n\n1\n\nINTRODUCTION\n\nRecently, language models (LMs) have become a prominant direction to solve reasoning. Given a question sequence, the models are tasked to predict the following answer sequence. One recent line of research for reasoning with LMs is chain of thought (CoT) generation (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022; Lewkowycz et al., 2022). In CoT generation, complex reasoning problems are solved by generating intermediate reasoning steps, or a chain of thought, before producing the final answer. Directly answering a question would require a model to fully solve the problem in a single forward pass, meaning the range of solvable problems is severely limited by the model’s capacity. On the other hand, generating CoT before the answer allows the problem’s complexity to be spread across the CoT, making each token generation more straightforward given the previous tokens. This is closer to how humans solve complex problems, as we think step by step, instead of producing an answer reflexively.\n\nAlthough CoT seems promising, there is a critical issue that significantly limits its utility: the effective context size of sequence models cannot grow unbounded. In this work, context refers to the set of input tokens that a model is conditioned on when generating output. Practically, all sequence models have a limit on the maximum context length due to various reasons. For instance, Transformers (Vaswani et al., 2017) suffer from a quadratic computational cost on the context length, and RNNs (Hochreiter & Schmidhuber, 1997) struggle with long-term dependency modeling. Therefore, even the state-of-the-art language models, such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022), limit the maximum context length by up to 2048 tokens. However, the length of intermediate steps can grow rapidly with the problem’s complexity and exceeds the context limit. Since CoT can handle a problem only if the process of solving it fits into a single context, the range of problems that CoT can handle is severely constrained by the context limit. This issue must be\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\novercome to solve more challenging and useful reasoning problems, whose solutions may require millions of tokens.\n\nHumans can handle this issue by using abstraction and recursion. We divide a large problem into smaller subproblems and focus on each subproblem while solving it, instead of considering the entire problem at every step. We can further subdivide a subproblem into even smaller subproblems. With this intuition, we propose Recursion of Thought (RoT) as a model-agnostic framework for recursively solving multi-step reasoning problems. The key feature of RoT is to grant the model the ability to recursively create and utilize multiple contexts for subproblems. We achieve this feat by introducing several special tokens that a model can output to control its context. During inference, the model recursively solves the problems by producing appropriate tokens at the right time. Moreover, RoT supports tail recursion, which enables general computation with an indefinitely long chain of recursion.\n\nWe demonstrate RoT on four basic arithmetic operations (addition, subtraction, multiplication, and division) and four algorithmic tasks (longest common subsequence, longest palindromic subsequence, 0-1 knapsack, and matrix chain multiplication) to show its generality. Without any taskspecific component, such as a calculator, all tasks are formulated as autoregressive sequence modeling problems. These tasks require a model to generalize by just seeing a tiny fraction of the problem space since the space is combinatorially large. For example, even in simple arithmetic operations, two 6-digit operands result in one trillion possible combinations. Hence, we evaluate whether a model understands the underlying rules, instead of brute force memorization. In our experiments, the range of problems that CoT can handle is seriously constrained by the context limit. On the other hand, RoT leads language models to achieve near perfect accuracy, even if the problem size increases to the extreme, where solving one problem requires producing hundreds of thousands of tokens. Moreover, the dramatic improvement is not limited to large pre-trained language models like GPT-3. RoT can make tiny, randomly initialized Transformers or LSTMs perform extremely complex reasoning.\n\nThe key messages of this work are summarized as follows:\n\n• The reasoning capability of current language models is seriously constrained by the maxi-\n\nmum length of a single context.\n\n• Our Recursion of Thought (RoT) unleashes the reasoning capability of language models by letting them recursively create and utilize multiple contexts of subproblems, following the principle of divide and conquer.\n\nIn the supplementary file, we provide the source code to fully reproduce our experiments.\n\n2 RELATED WORK\n\nChain of Thought. Among several prior works on applying language models to reasoning, Scratchpad (Nye et al., 2021) may be the most closely related to our work. It is the first approach to fine-tune language models to produce CoT before generating an answer. It demonstrates its effectiveness on 8-digit addition, polynomial evaluation, and Python program execution. It also mentions the confined context size as a major limitation to be overcome. In order to unlock the full potential of Scratchpad, the authors argue that Transformers should be improved to allow greater context sizes. We solve this exact problem from a completely different perspective, i.e., using multiple contexts to divide-and-conquer. Our approach is more practical and scalable, compared to increasing the context limit. More recently, it has been found that sufficiently large pre-trained language models can be induced to produce CoT, by simply tuning the prompt. For instance, CoT prompting (Wei et al., 2022) adds several QA exemplars with CoT before the main question, encouraging the model to generate final answers in the similar manner. Kojima et al. (2022)’s prompting is even simpler; after a question, they start the answer with “Let’s think step by step,” and then let the model finish the rest. Even without fine-tuning, these methods significantly improve the reasoning accuracy of language models. Minerva (Lewkowycz et al., 2022) utilizes these prompting techniques with a specially curated scientific pre-training dataset to achieve remarkable results on various reasoning benchmarks. However, all of these works are still limited by the maximum context size.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nNeural Programmer-Interpreter (NPI). Unlike language models, NPI (Reed & de Freitas, 2016) interacts with its environment through a series of program execution. It consists of an LSTM core, an encoder for each domain, and a memory of program embeddings. At every time step, the LSTM core takes a program embedding, arguments, and an observation of its environment to produce the next program embedding and corresponding arguments. Cai et al. (2017) combine NPI with recursion and show that recursion plays a critical role in generalization. Since NPI requires full execution traces for training, there are multiple works to relax this requirement using reinforcement learning (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019).\n\nSystem 1 Approaches for Reasoning. Kahneman (2013) classifies cognitive tasks into two categories: System 1 and System 2. System 1 refers to fast and reflexive thinking, while System 2 refers to sequential reasoning. It is hard to define strict criteria to distinguish between System 1 and System 2 approaches. We classify a model as System 1 if it directly outputs an answer, while System 2 generates its process, as well. In that sense, RoT, CoT and NPI are System 2 approaches, while there have been various System 1 approaches to solve symbolic reasoning. Zaremba & Sutskever (2014) train LSTMs with curriculum learning to solve integer addition up to nine digits. Kaiser & Sutskever (2016) propose a convolutional architecture called Neural GPU that performs binary addition and multiplication. Trained on 20-bit problems, this model operates like a digital circuit for the arithmetic operations, which can generalize up to 2,000 bits. Similarly, Yan et al. (2020) solve 8-bit binary addition and 12-bit binary multiplication with Transformers. If Neural GPUs are like digital circuits, Neural Arithmetic Logic Units (Trask et al., 2018) are like analog circuits for arithmetic. They represent numerical quantities with the activation values of neural networks and design a clever architecture for arithmetic operations. Although these System 1 approaches fall behind System 2 methods in terms of generality, they can be highly efficient in specific domains. We believe future systems will often be a hybrid of System 1 and System 2, harnessing the advantages of both types.\n\n3 RECURSION OF THOUGHT\n\nThe main idea of Recursion of Thought (RoT) is to let the model recursively solve small subproblems in separate contexts, keeping each reasoning step simple and learnable. Our RoT is model-agnostic and general enough to be combined with any kind of sequence model that supports autoregressive generation. The only requirement is that the model should be able to infer p(xi+1∣X1∶i), the probability of the next token xi+1 given a sequence X1∶i = [x1; ...; xi]. Therefore, sequence models such as Transformers, RNNs, or more advanced ones can all be used in the RoT framework. RoT teaches a sequence model to solve a problem using the divide and conquer paradigm in a supervised manner. That is, we assume that ground truths for the intermediate steps of how to recurse are readily available, as in Scratchpad (Nye et al., 2021) or NPI (Reed & de Freitas, 2016).\n\nFor better understanding, we discuss RoT in the reverse order of the pipeline. In §3.1, we first describe how to perform RoT inference with a fully trained model. In §3.2, we introduce the training process. Finally, in §3.3, we discuss how to recursively divide the problems and build the training data for intermediate steps automatically.\n\n3.1\n\nINFERENCE\n\nRecursion of Thought (RoT) grants a language model the ability to control the recursion process. For basic recursion control, we first introduce the following special tokens: GO , STOP , and THINK . GO and STOP respectively mark the start and end of a problem sequence. They can be nested inside another GO - STOP pair to indicate a subproblem. THINK initiates a recursion procedure. RoT teaches a model how to use these tokens so that it can perform divide-and-conquer problem solving. We formulate each inference context of a QA problem, denoted X, as the following concatenation:\n\nX = [Q; Qsub,1; Asub,1; . . . ; Qsub,N ; Asub,N ; A]\n\n(1)\n\nwhere Q and A are the main question and answer sequence, and Qsub,∗ are the questions and answers of the top-level subproblems. During inference, a model is given Q and tasked to generate the rest. Questions (both Q and Qsub,∗ ) start with a GO token, and answers (both A and Asub,∗ ) end with a STOP token. For trivial cases, i.e., the base cases of recursion, the context\n\nand Asub,∗\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: An example of the Recursion of Thought inference in Algorithm 1. Each table represents an inference context X k in order of creation, which has the structure of Eq.1. For each context, the model is given Q and tasked to generate the rest, one token at a time. The model outputs the THINK token when it needs to generate Asub,∗ , the answer of a subproblem. The THINK token triggers a recursive process that solves the subproblem in a new context and returns the answer.\n\nFigure 2: The target sequence Y 1 is produced from X 1 in Figure 1 by Algorithm 2. Given X 1 model is trained to output Y 1 make sure ∥X∥ = ∥Y ∥.\n\n1∶i, the i+1, except for PAD s that are ignored. Note PAD is a dummy token to\n\ncontains no (Qsub,∗ ) pair. A subproblem can have smaller, lower-level subproblems recursively, but only the top-level subproblems remain in a context. As a result, we can abstract away the details for solving the subproblems and keep only the high-level results in the current context.\n\n, Asub,∗\n\nFor tail recursion, where the last subquestion’s answer becomes the final answer, we additionally introduce the TAIL token. If TAIL is used in the place of a GO token in the last subquestion Qsub,N , its answer Asub,N is treated as the final answer A, and the context X does not have duplicate A. Algorithm 1 summarizes the inference process. Figure 1 presents a example of solving 408 + 351 for better understanding. More detailed illustrations of inference can be found in Appendix A. The RoT function (L1) takes a fully trained model with a question Q as input and returns the answer A as output. The procedure starts by initializing the context X with the original question Q (e.g., GO 4 0 8 + 3 5 1 = in Figure 1). ians is the starting index of the answer, which is initialized to ∣X∣ + 1, where ∣ ⋅ ∣ is the length of a sequence. Then, in the main loop, the model iteratively generates a next token x from X, which is appended to the end of X. After the initialization, the model is expected to (i) generate answer A directly or (ii) output GO . If the model immediately generates A and finishes it with a STOP , the answer is returned (L9), which is the base case of the recursion. Up to this point, the algorithm is identical to common language models.\n\nOn the other hand, if the model decides to output GO , which signals the start of a subproblem, its index is stored in igo (L11). Since a subproblem has started, the next several tokens from the model should constitute the question of the subproblem. In Figure 1, the first subproblem of the main context X 1 is adding the last digits, i.e., 8 + 1. Once the subquestion is generated, the next step is to find an answer to it. This is the pivotal moment: instead of producing the answer, the model outputs the THINK token, which initiates the recursion with a new context (L16-23). First, we separate the subquestion Qsub, starting from igo (L16). Second, using it as an input, we trigger the recursive call (L17) to obtain the answer Asub. As shown as red arrows in Figure 1, this call creates another context X 2 and initializes its question part with the subquestion. Then, the same inference process is executed inside the new context, sometimes running more recursions. In the case of 8 + 1, the answer 9 STOP is immediately returned since it is a base case. Finally, the THINK token is replaced\n\n4\n\nQ408+35GO1=Qsub,1GO8+1=Asub,1THINKTHINKAsub,2GO40+35=Qsub,275STOP9A9STOP75STOPX1GO8+1=QAX2GO40+35=THINKGO0+5=Asub,1Qsub,1Q5STOPTHINKGO4+3=Asub,2Qsub,2A75STOPX3GO0+5=QA5STOPX4GO4+3=QAX57STOP7STOP9STOPQ408+35GO1=Qsub,1GO8+1=Asub,19STOPX1GO8+1=THINKY1PAD75STOPAsub,2GO40+35=Qsub,2THINKGO40+35=75STOP9A75STOP9PADPADPADPAD···Under review as a conference paper at ICLR 2023\n\nwith Asub (L21) and the starting position of the answer is updated (L23). Hence, when predicting the next token, the model sees the returned answer as input, instead of the THINK token.\n\nOnce a subproblem is finished, the model can solve another subproblem in the same way, or output the final answer. In Figure 1, the second subproblem is to add all the remaining digits, i.e., 40 + 35. To solve it, the model recursively solves two other subproblems, i.e., adding each digit, before generating the answer. If the model outputs the final answer followed by a STOP token, the answer is returned (L9). If the model starts a subproblem with TAIL instead of GO , it becomes a tail recursion (L14), and its answer is returned directly as the final answer (L19).\n\n3.2 TRAINING\n\nWe teach RoT in a supervised manner; the model is trained with the ground truth (GT) intermediate steps, which also include when to output the special tokens. Each training example is constructed as a pair of a ground truth context sequence X and the corresponding target sequence Y . The GT context X is structured as Equation 1 and automatically built by the algorithms that will be introduced in §3.3 and Appendix D. In this section, we discuss how to construct the target sequence Y for X, and define the training objective.\n\nAlgorithm 2 summarizes the process of converting X to Y , where Y has the same length with X. Refer to Figure 2 for an example. Overall, Y is a copy of X except for the parts corresponding to Q and Asub,∗ . Since the question Q is always given in a context, Q is replaced by special PAD tokens (L1), which mean “nothing to predict for this part.” Each subproblem’s answer Asub,n is replaced by a THINK token followed by several PAD s that fill in the rest to make sure ∣X∣ = ∣Y ∣ (L4). This way, the model is trained to output THINK instead of the first token of Asub,n. Since the whole Asub,n will be returned from the recursive process and replace the THINK during inference (L17,21 of Algorithm 1), we do not need a training signal for the rest of Asub,n.\n\nGiven a pair (X, Y ), the training objective is defined as follows:\n\nL = − ∑\n\nI[yi+1 ≠ PAD ] log p(yi+1∣X1∶i) (2)\n\ni\n\nAlgorithm 1 Recursion of Thought Inference\n\nRequire: A sequence model M trained for Recursion of Thought, a question sequence Q\n\n▷ Initialize context with Q ▷ Start of answer ▷ Tail recursion\n\nx ← M(X) ▷ Generate next token X ← [X; x] if x = STOP then\n\nX ← Q ians ← ∣X∣ + 1 t ← f alse while True do\n\n1: function ROT(M, Q) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15:\n\nreturn Xians∶∣X∣ else if x = GO then igo ← ∣X∣\n\nelse if x = TAIL then\n\n▷ Mark last GO\n\nigo ← ∣X∣ t ← true ▷ Mark tail recursion\n\nelse if x = THINK then Qsub ← Xigo∶∣X∣−1 Asub ← ROT(M, Qsub if t then\n\n)\n\nreturn Asub\n\nend if X ← [X1∶∣X∣−1; Asub\n\n]\n\n▷ Replace THINK with Asub\n\nians ← ∣X∣ + 1\n\n16:\n\n17: 18: 19: 20: 21:\n\n22: 23: end if 24: end while 25: 26: end function\n\nAlgorithm 2 Creating the target sequence\n\nRequire: Context X = [Q; Qsub,1; Asub,1;\n\n. . . ; Qsub,N ; Asub,N ; A]\n\n1: Y ← PAD ... PAD (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) ∣Q∣\n\n2: for n in 1...N do 3: 4:\n\nY ← [Y ; Qsub,n Y ← [Y ; THINK PAD ... PAD (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) ∣Asub,n∣−1\n\n]\n\n]\n\n5: end for 6: Y ← [Y ; A] 7: return Y\n\nwhere I is the indicator function that excludes PAD s from training. Simply, it means that the sequence model is trained to output yi+1 as the next token for a given X1∶i. Its form is almost identical to the standard language modeling objective: LLM = − ∑ i log p(xi+1∣X1∶i), which is to predict the next token given previous tokens. With this objective, any sequence model is trained in the standard way, i.e., end-to-end via stochastic gradient descent. For decoder-only transformers with causal masks, the training can be efficiently done in parallel for all tokens.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n3.3 THE RECURSIVE REASONING PROCEDURES\n\nAs explained in §3.2, we teach the recursive reasoning procedure for a problem type by providing ground truth contexts. To produce these contexts, we design an exemplary reasoning procedure for each problem type and implement it as a teacher program that automatically generates the contexts given a problem. The major desiderata for the exemplary reasoning procedures are two fold: (i) each context should be short, and (ii) the next tokens to generate should be obvious from the previous sequence. Although the definition of “being obvious” may vary depending on the model’s architecture (e.g., Transformers vs. LSTMs), the reasoning procedures developed for human generally meet these desiderata. Therefore, our procedures for the arithmetic problems are borrowed from the elementary school math. For example, the addition procedure shown in Figure 1 is a recursive version of adding digits one by one starting from the last digit. In another example of multiplication, an N -digit × M -digit multiplication is divided into an N -digit × 1-digit multiplication and an N -digit × (M − 1)-digit multiplication. The N -digit × 1-digit multiplication is further divided into a 1-digit × 1-digit multiplication and an (N − 1)-digit × 1-digit multiplication. For the algorithmic problems, we also borrow standard, well-known algorithms. In Appendix D, we provide the full details of the procedures for each problem type, with Python code snippets of the key parts. Note that our proposals for the reasoning procedures in Appendix D are one of many possible solutions, which are not necessarily optimal.\n\nTraining Data Distribution. We use the same problem distribution for both training and evaluation, since out-of-distribution generalization is not within the scope of this paper. That is, when teaching 6-digit multiplication to the model, both training and test sets are all examples of 6-digit multiplication. The problem distributions are elaborated in Appendix C. Another important detail regarding the training of RoT is that each training example in a batch is a context, not a whole problem. Since RoT generates multiple contexts per problem, often a large portion of contexts can be duplicate (mostly the base cases). Therefore, to build a training batch for RoT, we first sample a top level problem and find the set of unique RoT contexts from the problem. Out of the unique contexts, we randomly sample one context as a training example. We find this simple technique works well, and we do not need more sophisticated method, such as the adaptive curriculum learning in Reed & de Freitas (2016).\n\n4 EXPERIMENTS\n\nSince Recursion of Thought is the first approach of this kind, we mainly compare with two baselines. The first one is to output an answer directly from a question, which we call Without Thought (WT). The other one is to generate all the intermediate steps before the answer without recursion (Nye et al., 2021), which we refer to as Chain of Thought (CoT; not to be confused with the CoT prompting (Wei et al., 2022)) for consistency. We construct the ground truths for CoTs by unraveling the same recursive process which we design for RoT, into a single context sequence (see Appendix B for examples). Therefore, the number of tokens to generate while solving a problem is the same for both CoT and RoT (if we do not count the THINK tokens). However, the sizes of the individual contexts of CoT are far longer than those of RoT due to the recursively nested subproblems. Refer to Appendix I for more detailed analysis of the context sizes. Note that we train these baselines and do not use any prompting technique. When evaluating, we consider a problem to be correctly solved only if all the intermediate steps and the answer are correct. In other words, we impose stricter rules on both RoT and CoT by not counting “lucky guesses” as correct.\n\n4.1 THE REASONING PROBLEMS\n\nTo evaluate the reasoning capabilities, we test various reasoning tasks that are grouped into two categories: arithmetic reasoning and algorithmic reasoning. We below provide a rough description of the tasks, whose details can be found in Appendix C. All the reasoning tasks share one characteristic in common: we can easily adjust the problem’s difficulty. Therefore, we can gradually increase the degree of difficulty and see which method fails first. Since the goal of our experiments is to test the reasoning capability of language models, all problems are formulated in pure sequence modeling, with no external program (e.g., calculator) called by the models.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nArithmetic Reasoning. We test four basic arithmetic operations, i.e., addition, subtraction, multiplication, and division, with two non-negative integers. The difficulty of arithmetic problems is represented by the maximum number of digits in an operand. For instance, in 6-digit multiplication, each operand can range from 0 to 999,999. When we sample a problem, we sample each operand from the log-uniform distribution. Compared to the uniform distribution where the samples are highly biased towards extremely large numbers, we get roughly the same ratio of samples for each number of digits. The sampling schemes for each operation are elaborated in Appendix C.1.\n\nAlgorithmic Reasoning. We test four algorithmic tasks with distinct characteristics. These problems are generally solved via dynamic programming (DP), and the length of intermediate steps can increase rapidly since the time complexity of DP algorithms ranges from O(N 2 ) (LCS, LPS, and 0-1 Knapsack) to O(N 3 ) (MCM). (1) Longest Common Subsequence (LCS): Given two random sequences of length N , the model finds the longest common subsequence and its length. The sequences consist of 10 characters from 0 to 9, and the problem difficulty is defined to be the sequence length N . (2) Longest Palindromic Subsequence (LPS): Similar to LCS, given a random sequence of length N , the model finds the longest palindromic subsequence and its length. (3) 0-1 Knapsack: Given a list of N items with specific value and weight, the model finds the best combination of items that maximizes the total value under the weight limit of a knapsack. The problem difficulty is represented by the number of items. (4) Matrix Chain Multiplication (MCM): The computational cost of multiplying N (> 2) matrices varies greatly depending on the order of multiplication. MCM is the task of finding the best multiplication order that yields minimal computation cost. The difficulty is controlled by the number of matrices N .\n\n4.2 UNLEASHING GPT-3’S REASONING CAPABILITY THROUGH RECURSION OF THOUGHT\n\nDespite their remarkable language modeling capabilities, the state-of-the-art large language models, including GPT-3 (Brown et al., 2020), struggle to solve even the basic arithmetic tasks (Nye et al., 2021). For instance, it cannot correctly handle multiplication with more than one or two digits. Using the OpenAI API, we fine-tune GPT-3 on the reasoning tasks in §4.1 for 10K steps with a batch size of 256. Each training batch is randomly sampled from the training data distribution explained in §3.3. The results are presented in Figure 3a, and the technical details are described in Appendix E. Each point in the graphs represents one experiment at a certain problem difficulty. We report the accuracy on a test set of 1K unique problems randomly sampled as explained in Appendix C. To the best of our knowledge, the problems at this scale (e.g., 48-digit addition/subtraction and 16-digit multiplication/division) have never been solved by any language model without the help of external programs. For reference, Minerva (Lewkowycz et al., 2022) achieves around 80% accuracy on 10-digit addition and 20% on 18-digit addition.\n\nEven WT fine-tuning cannot make GPT-3 deal with such a level of complexity, while CoT is not applicable due to the context limit of 2048. The green dotted lines mark the maximum difficulty that can be handled by CoT under the context limit. On the other hand, RoT finetunes the GPT-3 to achieve near perfect scores in every experiment. As presented in Appendix I, solving each problem requires up to tens of thousands of tokens. Without any architectural change, RoT makes GPT-3 handle these extremely complex problems.\n\n4.3 RECURSION OF THOUGHT WITH TINY LANGUAGE MODELS\n\nRecent research on reasoning has been mostly focused on extremely large pre-trained language models. In this section, we show an interesting result that RoT can make even tiny models, without any pre-training, perform convoluted reasoning procedures. Since RoT is model-agnostic, we test the two basic sequence model architectures: Transformer Vaswani et al. (2017) and LSTM Hochreiter & Schmidhuber (1997). For Transformer, we use a decoder-only model with 4 layers, 2 attention heads, 128 embedding dimensions, and 256 feed-forward dimensions, a total of only 536K parameters. It is a million times smaller than the largest PaLM (Chowdhery et al., 2022) with 540B parameters. The context limit is set to 2048 following GPT-3 and PaLM. For LSTM, we use 4 layers, 64 input dimensions, and 256 hidden dimensions, which result in 272K parameters. We set the context limit of the LSTM to 512 since (i) it takes a lot of time for LSTMs to process the tokens sequentially, and (ii) they are not good at handling long-term dependency.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) GPT-3\n\n(b) Tiny Transformer\n\n(c) Tiny LSTM\n\nFigure 3: Comparison of the thought processes. In each graph, the x-axis is the problem difficulty, while the y-axis is the reasoning accuracy. Each point represents an independent experiment. The green vertical lines indicate the maximum problem difficulty that CoT can handle without exceeding the maximum context size.\n\nBy virtue of their small sizes, we conduct far more extensive experiments than GPT-3, which are presented in Figure 3b and Figure 3c. We test both arithmetic and algorithmic reasoning problems with Transformer, and the arithmetic problems with LSTM. For each experiment, we train a randomly initialized model and evaluate it on a test set of 30K unique problems. With a batch size of 256, Transformers and LSTMs are trained for 500K steps and 800K steps, respectively. We repeat each experiment eight times and report the average and standard deviation of the accuracies. Appendix K enumerates the exact values of Figure 3. With the tiny Transformer, we experiment to the extent where even humans would find daunting. For example, we test addition/subtraction up to 64 digits and multiplication/division up to 32 digits. Note that a 32-digit number cannot even fit into the 64-bit integer datatype.\n\nThroughout the experiments, we observe consistent patterns:\n\n• WT’s accuracy drops most quickly as the problem difficulty increases.\n\n8\n\n0D['LJLWV$FFXUDF\\$GGLWLRQ:LWKRXW7KRXJKW&KDLQRI7KRXJKW5HFXUVLRQRI7KRXJKW0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQ6HTXHQFH/HQJWK$FFXUDF\\/&66HTXHQFH/HQJWK$FFXUDF\\/36RI,WHPV$FFXUDF\\.QDSVDFNRI0DWULFHV$FFXUDF\\0&00D['LJLWV$FFXUDF\\$GGLWLRQ0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQ6HTXHQFH/HQJWK$FFXUDF\\/&66HTXHQFH/HQJWK$FFXUDF\\/36RI,WHPV$FFXUDF\\.QDSVDFNRI0DWULFHV$FFXUDF\\0&00D['LJLWV$FFXUDF\\$GGLWLRQ0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQUnder review as a conference paper at ICLR 2023\n\n• CoT achieves near perfect accuracy, but it can only be applied to simple problems due to\n\nthe context limit.\n\n• RoT achieves near perfect accuracy and can be scaled up to extremely complex problems.\n\nDespite the small sizes, RoT makes the Transformers master all types of extremely complex problems. We do not test more difficult problems mainly because the evaluation becomes too costly, not because RoT is incapable of learning them.\n\n5 DISCUSSION\n\nThe results of the tiny Transformer suggest that we might have to rethink the capability of large language models. If RoT enables the tiny Transformer to easily master 32-digit multiplication or division, what would a million times bigger model, like PaLM, be capable of? In contrast to the currently ongoing arms race in language models, the number of parameters might not be the main bottleneck anymore to increase models’ reasoning capability. We believe that our new paradigm of utilizing multiple contexts has the potential to make a huge leap in this line of research.\n\nThe current limitation of RoT is the need for supervision to learn divide and conquer for each task. In order to apply RoT to a wider range of tasks, it may be crucial to reduce the expensive supervision. As one possible approach, we may borrow the RL-based methodologies that are developed for reducing supervision of NPI (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019).\n\nInterestingly, RoT cannot facilitate length generalization, e.g., training on 8-digit multiplication with RoT cannot make a model generalize to 16-digit multiplication. We believe this problem is rooted in more fundamental limitation of the Transformer architecture (Hahn, 2020), orthogonal to RoT. Fortunately, since RoT is a model-agnostic framework, we would be able to apply RoT to more advanced architectures to come in the future, which might be capable of length generalization.\n\n6 CONCLUSION\n\nDespite the remarkable advances in language models, their reasoning capability has always been constrained by the maximum size of a single context. In this work, we introduce Recursion of Thought to solve this problem by utilizing multiple contexts. We prove its potential through extensive experiments, showing that it is possible to make language models solve problems that require hundreds of thousands of tokens. We believe the core idea of utilizing multiple contexts will play an essential role in future language models.\n\nREFERENCES\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nJonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via recursion. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=BkbY4psgg.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. MeierHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.\n\nRoy Fox, Richard Shin, Sanjay Krishnan, Ken Goldberg, Dawn Song, and Ion Stoica. Parametrized In 6th International Conference on Learnhierarchical procedures for neural programming. ing Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id= rJl63fZRb.\n\nMichael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156–171, 2020. doi: 10.1162/tacl_a_00306. URL https://aclanthology.org/2020.tacl-1.11.\n\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735–\n\n1780, 1997.\n\nDaniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2013. ISBN 9780374533557.\n\nLukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/ 1511.08228.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n\nIn Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n\nlanguage models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022.\n\nAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. ArXiv, abs/2206.14858, 2022.\n\nChengtao Li, Daniel Tarlow, Alexander L. Gaunt, Marc Brockschmidt, and Nate Kushman. Neural program lattices. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=HJjiFK5gx.\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114, 2021.\n\nThomas Pierrot, Guillaume Ligner, Scott E. Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, and Nando de Freitas. Learning compositional neural programs with recursive tree search and planning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 14646–14656, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/95b431e51fc53692913da5263c214162-Abstract.html.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nScott E. Reed and Nando de Freitas. Neural programmer-interpreters. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv. org/abs/1511.06279.\n\nAndrew Trask, Felix Hill, Scott E. Reed,\n\nNeural arithmetic logic units.\n\nJack W. Rae, Chris Dyer, and Phil Blunsom. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 8046–8055, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 0e64a7b00c83e3d22ce6b3acf2c582b6-Abstract.html.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.\n\nYujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. Neural\n\nexecution engines: Learning to execute subroutines. ArXiv, abs/2006.08084, 2020.\n\nWojciech Zaremba and Ilya Sutskever. Learning to execute. ArXiv, abs/1410.4615, 2014.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA A STEP BY STEP ILLUSTRATION OF ROT INFERENCE\n\nIn this section, we provide a step by step illustration of the example in Figure 1. Here we assume an ideal model fully trained for RoT.\n\nStep 1\n\nThe context is initialized with the question Q.\n\nQ\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nStep 2\n\nThe model generates the first subquestion 8 + 1.\n\nQ\n\nQsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nX 1\n\nStep 3\n\nInstead of immediately producing the answer, the model outputs the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nX 1\n\nStep 4\n\nThe THINK token triggers the creation of a new context. The new context is initialized with the subproblem starting from the last GO of X 1, i.e., 8 + 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nQ\n\nX 2\n\nGO 8 + 1 =\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nStep 5\n\nSince the subproblem is a base case, the model outputs the answer 9 immediately.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nQ\n\nA\n\nGO 8 + 1 =\n\n9 STOP\n\nX 1\n\nX 2\n\nStep 6\n\nThe answer is returned and replaces the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nX 1\n\nStep 7\n\nThe model generates the next subproblem, which is to add the remaining digits. Then, it produces THINK to find its answer.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nStep 8\n\nThe THINK token creates a new context X 3 for solving 40 + 35.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nX 3\n\nGO 4 0 + 3 5 =\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nStep 9\n\nSince 40 + 35 is not a base case, the model recursively produces more subproblems. In this case, the first subproblem is to add the last digits, i.e., 0 and 5. Then it outputs the THINK token to solve the subproblem.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\nTHINK\n\nStep 10\n\nThe new context X 4 is created to solve 0 + 5.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\nTHINK\n\nQ\n\nA\n\nGO 0 + 5 =\n\n5 STOP\n\nX 3\n\nX 4\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nStep 11\n\nThe answer is returned to X 3 and replaces the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nStep 12\n\nThe model generates the next subproblem.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\nTHINK\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nStep 13\n\nX 5 created to solve the subproblem 4 + 3. Since this is a base case, the model produces the answer directly.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\nTHINK\n\nQ\n\nA\n\nGO 4 + 3 =\n\n7 STOP\n\nX 3\n\nX 5\n\nStep 14\n\nThe answer from X 5 replaces the THINK token in X 3.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\n7 STOP\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nStep 15\n\nSince all subproblems are solved in X 3, the answer 75 is generated and returned to X 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\n7 STOP\n\nA\n\n7 5 STOP\n\nStep 16\n\nThe answer of X 3 replaces the THINK token in X 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\n7 5 STOP\n\nStep 17\n\nSince the subproblems in X 1 are all solved, the model produces the final answer.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nA\n\nGO 4 0 + 3 5 =\n\n7 5 STOP\n\n7 5 9 STOP\n\nB EXAMPLES OF COT TRAINING DATA\n\nIf we solve the example of 408+351 in figure 1 with RoT, the following five contexts are produced.\n\n• X 1: GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP GO 4 0 + 3 5 = 7 5 STOP 7 5\n\n9 STOP\n\n• X 2: GO 8 + 1 = 9 STOP • X 3: GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP GO 4 + 3 = 7 STOP 7 5 STOP • X 4: GO 0 + 5 = 5 STOP\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n• X 5: GO 4 + 3 = 7 STOP\n\nThe CoT context of the same problem is:\n\n• X CoT: GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP GO 4 0 + 3 5 = GO 0 + 5\n\nSTOP GO 4 + 3 STOP 7 5 STOP 7 5 9 STOP\n\nIn a slightly more complicated example of 34 × 5, the RoT contexts are as follows:\n\n• X 1: GO 3 4 * 5 = GO 4 * 5 = 2 0 STOP GO 3 * 5 = 1 5 STOP TAIL 1 5 0\n\n+ 2 0 = THINK\n\n• X 2: GO 4 * 5 = 2 0 STOP • X 3: GO 3 * 5 = 1 5 STOP • X 4: GO 1 5 0 + 2 0 = GO 0 + 0 = 0 STOP GO 1 5 + 2 = 1 7 STOP 1 7 0\n\nSTOP\n\n• X 5: GO 0 + 0 = 0 STOP • X 6: GO 1 5 + 2 = GO 5 + 2 = 7 STOP 1 7 STOP • X 7: GO 5 + 2 = 7 STOP\n\nThe corresponding CoT context is:\n\n• X CoT: GO 3 4 * 5 = GO 4 * 5 = 2 0 STOP GO 3 * 5 = 1 5 STOP TAIL 1 5 0 + 2 0 = GO 0 + 0 = 0 STOP GO 1 5 + 2 = GO 5 + 2 = 7 STOP 1 7 STOP 1 7 0 STOP\n\nNotice that the CoT context consists of all the corresponding RoT contexts as its subsequences. The number of tokens to generate is identical to that of RoT, if we do not count the THINK tokens. Even in these simple examples, however, the context size of CoT is far longer than that of RoT. For much more complex problems, such as 8-digit multiplication or 0-1 Knapsack, the CoT context size can be orders of magnitude larger than RoT. See Appendix I for more details on the distribution of context sizes.\n\nC PROBLEM SPECIFICATIONS\n\nC.1 THE ARITHMETIC PROBLEMS\n\nFor arithmetic tasks, we test addition, subtraction, multiplication, and division on non-negative integers. For subtraction, we add a constraint that the first operand is not less than the second one, in order to enforce non-negative answers. For division, we let the output include both a quotient and a remainder, separated by a special token R , e.g., GO 7 ÷ 3 = 2 R 1 STOP .\n\nAs briefly mentioned in §4.1, naively sampling the operands from a uniform distribution makes the operands extremely biased towards large numbers. For example, the probability of sampling a 2-digit number from the 6-digit space is less than 0.01%. Thus, we define a variation of the loguniform distribution (often called the reciprocal distribution) to sample the operands. As a result, we obtain roughly the same proportion of operands for each number of digits.\n\nThe probability density of a log-uniform distribution is proportional to the reciprocal of the value. By definition, zero is not the support of a log-uniform distribution, and samples are overly concentrated to the first few values in the sampling range. Therefore, we slightly extend the log-uniform distribution by introducing an offset parameter δ. To sample an integer in range [α, β) with offset δ, we first uniformly sample a real number r in range [log(α + δ), log(β + δ)]. Then, r is transformed to ⌊exp(r) − δ⌋. We denote the extended log-uniform distribution Ulog(α, β, δ). As δ gets larger, the samples are more dispersed to larger numbers. In the experiments, we set δ = 3.\n\nAdditionally, we introduce several other sampling details for division problems. Assume that we independently sample two numbers a and b for the dividend and the divisor. In about half of the\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nAddition 1330 + 121163 114780 + 4356 638 + 2 35 + 77 114261 + 354 3 + 13792 10151 + 7 22 + 1399 363356 + 450475 73 + 11 179895 + 4128 3 + 10 1 + 141972 57612 + 18403 9 + 1621 3370 + 381 678 + 8854 422 + 10348 118 + 582 1343 + 408534 24 + 9251 315 + 652424 355 + 4434 22 + 834928 3028 + 357 777 + 1355 154874 + 81059 64936 + 216852 3 + 340939 3 + 984775 50581 + 1183 415 + 943 110 + 49 15 + 17058 36278 + 100 6 + 23516 1462 + 848 1002 + 2773 135 + 178346 22672 + 162038\n\nSubtraction 376776 − 35241 10638 − 100 109033 − 52649 85137 − 3098 22355 − 2824 7 − 1 652781 − 78853 64914 − 3114 13041 − 1422 28293 − 4540 11553 − 3576 656291 − 2795 93 − 42 55972 − 1782 84587 − 51 273269 − 5867 274405 − 14 51926 − 9 4272 − 229 223267 − 377 14857 − 1994 914771 − 836 3035 − 2963 30 − 12 149 − 4 89057 − 6 296410 − 9 45 − 3 78906 − 3 56560 − 29960 98 − 6 16551 − 920 25606 − 194 45 − 37 129443 − 70196 221 − 54 11010 − 818 47759 − 67 10 − 8 1439 − 153\n\nMultiplication 9466 × 176175 179 × 516 5509 × 133 6783 × 2 6 × 80285 37275 × 19258 168484 × 154 3331 × 40 349 × 158 17988 × 262130 8140 × 1670 51 × 5 16497 × 158 74 × 10 216 × 13414 621 × 2 2 × 5951 189486 × 13080 552792 × 763 77 × 3 179090 × 469029 1037 × 258 8 × 769974 47765 × 7254 5608 × 18164 21437 × 12 15007 × 15 539860 × 427 3583 × 9754 13 × 66 266394 × 185 3988 × 12 5514 × 57 5 × 1712 17 × 430178 227 × 127 20888 × 54 96 × 232801 175 × 1050 146 × 166\n\nDivision 620261 ÷ 155034 111730 ÷ 1176 28268 ÷ 1 588137 ÷ 25571 180330 ÷ 739 879975 ÷ 97772 111461 ÷ 905026 42338 ÷ 14003 108 ÷ 384103 60002 ÷ 7479 131467 ÷ 131290 890679 ÷ 62 228 ÷ 131108 892 ÷ 124 15 ÷ 964156 369044 ÷ 28364 457 ÷ 46 14687 ÷ 730 200361 ÷ 1049 19715 ÷ 965179 98 ÷ 7 406 ÷ 9 47345 ÷ 122 391613 ÷ 1631 892642 ÷ 3898 241554 ÷ 1901 116475 ÷ 12908 488317 ÷ 197443 7519 ÷ 325 3560 ÷ 847611 9711 ÷ 1385 44540 ÷ 103 19721 ÷ 58 59544 ÷ 24 333057 ÷ 333057 25719 ÷ 5142 7544 ÷ 46 45 ÷ 410 195659 ÷ 2047 412572 ÷ 16\n\nTable 1: 40 randomly selected samples of each type of 6-digit arithmetic problems.\n\ncases, the dividend a would be less than the divisor b, so the quotients will be zero for those cases. To ensure a diverse range of quotients, we sample the divisor b from Ulog(1, 10N , 3), the quotient c from Ulog(0, 10N /b, 3), and the remainder r from Ulog(0, b, 3). The dividend is calculated from these values: a = b × c + r. This way, we can sample division problems with a diverse range of quotients and remainders.\n\nTable 1 presents 40 problem samples for each 6-digit problem type. Several properties of our sampling scheme can be observed from the table. First, each number ranges over diverse numbers of digits. Second, the division problems are mostly non-trivial, i.e., the quotients are not concentrated at zero.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nC.2 THE ALGORITHMIC PROBLEMS\n\nC.2.1 LONGEST COMMON SUBSEQUENCE (LCS)\n\nThe question of an LCS problem is two number sequences joined by the LCS token, and the answer is the corresponding LCS and its length separated by ; . Here is an example of length-4 LCS problem:\n\n• Q: GO 1 2 3 4 LCS 2 4 6 8 = • A: 2 4 ; 2 STOP\n\nFor a length-N LCS problem, we sample two sequences of length N . Each character of the sequences are randomly sampled from 0-9 with equal probability.\n\nC.2.2 LONGEST PALINDROMIC SUBSEQUENCE (LPS)\n\nThe question of a length-N LPS problem starts with the LPS , followed by a sequence of length N . Similar to LCS, the answer contains the corresponding LPS and its length separated by ; . The following is an example of length-8 LPS problem:\n\n• Q: GO LPS 4 1 2 5 3 2 6 1 = • A: 1 2 3 2 1 ; 5 STOP\n\nThe sequence of an LPS problem is sampled in the same way as done for the LCS problem.\n\nC.2.3\n\n0-1 KNAPSACK\n\nEach item in a 0-1 Knapsack problem is represented by its value and weight. For instance, 1 2 & 3 4 represents an item with a value of 12 and a weight of 34. The question part of a 0-1 Knapsack problem is a sequence consisting of the KNAPSACK token, a list of items separated by , , the token @ , and the capacity of the knapsack. The answer part starts with a list of items to include, then $ , and finally the total value. The following is an example of a 3-item knapsack problem.\n\n• Q: GO KNAPSACK 5 & 1 2 , 2 5 & 1 5 , 1 9 & 1 8 @ 4 0 =\n\n• A: 2 5 & 1 5 , 1 9 & 1 8 $ 4 4 STOP\n\nIn this example, given a knapsack of capacity 40, the last two are selected with the total value of 44.\n\nFor a fixed number of items, we uniformly sample each item’s value and weight from the integers of range [1, 99].\n\nC.2.4 MATRIX CHAIN MULTIPLICATION (MCM)\n\nThe cost of multiplying many matrices is very sensitive to the order of multiplication. Matrix chain multiplication is the task of finding the best order with the minimum cost. Here, the cost is defined to be the total number of element multiplications. In the example of three matrices A, B, and C, whose shapes are 4×2, 2×8, and 8×3 respectively, the cost of computing (AB)C is 4×2×8+4×8×3 = 160, while another order A(BC) costs only 2 × 8 × 3 + 4 × 2 × 3 = 72. In the question of an MCM problem, the sizes of the matrices are enumerated, and the answer contains the order and the total cost separated by ; . The example above is represented as the following sequences.\n\n• Q: GO MCM 4 × 2 , 2 × 8 , 8 × 3 =\n\n• A: 4 × 2 , ( 2 × 8 , 8 × 3 ) ; 7 2 STOP\n\nGiven a fixed number of matrices, we sample the sizes of matrices from the range [1, 99].\n\nC.2.5 SORTING\n\nAlthough not included in the main text, we test the problem of sorting multi-digit numbers. The results are presented in Appendix J. The problem difficulty is defined by the maximum number of\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nterms. For a sorting problem of at most N terms, we first uniformly sample the number of terms from [2, N ]. Then we sample each term from Ulog(0, 1000, 5). The following is an example of the sorting problem.\n\n• Q: GO SORT 1 3 9 , 1 6 0 , 4 3 4 , 7 9 6 , 4 1 =\n\n• A: 4 1 , 1 3 9 , 1 6 0 , 4 3 4 , 7 9 6 STOP\n\nD DETAILS OF THE RECURSIVE REASONING PROCEDURES\n\nIn this section, we elaborate the procedures to recursively solve the arithmetic problems. Specifically, we present the algorithms to produce the subproblems of a problem. Therefore, for a set of randomly sampled questions, we can generate ground truth contexts using these algorithms. For better understanding, we present the key parts of our Python code, the thought methods. For each problem, we create a child class the Problem class and implement thought static method. The method takes a set of arguments for a problem and returns the list of direct subproblems. Each subproblem is represented by a problem class, problem arguments, and recursion type (whether it is a tail recursion or not). We use named tuple T to group these information:\n\n1 2\n\nfrom collections import namedtuple T = namedtuple('Thought', ['prob_cls', 'args', 'type'], defaults=[''])\n\nFor instance, T(Mul, (3, 4)) represents a regular subproblem of 3 × 4, and T(Add, (12, 340), ’tail’) represents a subproblem of 12 + 340 which should be performed as a tail recursion. Once the thought method returns a list of Ts, we can recursively find more subproblems for each subproblem.\n\nD.1 ADDITION\n\nThe core idea of our recursive procedure for addition is to first add the last digits, and then add the rest. If the sum of the last digits is greater than or equal to 10, we insert another subproblem for adding the carry right after adding the last digits.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21\n\nclass Add(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left < 10 and right < 10:\n\nreturn []\n\nl_last, r_last = left % 10, right % 10 thoughts = [T(Add, (l_last, r_last))]\n\nl_rest, r_rest = left // 10, right // 10 if l_last + r_last >= 10:\n\nthoughts.append(T(Add, (l_rest, 1))) l_rest += 1\n\nif l_rest > 0 and r_rest > 0:\n\nthoughts.append(T(Add, (l_rest, r_rest)))\n\nreturn thoughts\n\nFigure 1 in the main draft is an example with no carry, and the following is another example of 27+65 with a carry.\n\n• X 1: GO 3 1 7 + 6 5 = GO 7 + 5 = 1 2 STOP GO 3 1 + 1 = 3 2 STOP GO 3\n\n2 + 6 = 3 8 STOP 3 8 2 STOP\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\n• X 2: GO 7 + 5 = 1 2 STOP • X 3: GO 3 1 + 1 = GO 1 + 1 = 2 STOP 3 2 STOP • X 4: GO 1 + 1 = 2 STOP • X 5: GO 3 2 + 6 = GO 2 + 6 = 8 STOP 3 8 STOP • X 6: GO 2 + 6 = 8 STOP\n\nD.2 SUBTRACTION\n\nSimilar to addition, we first subtract the last digits and solve the rest recursively. When subtracting the last digits x and y, we always borrow 10 for x to prevent a negative result. The borrowing of 10 is easy for a sequence model: just put 1 before x. Therefore, the base cases of subtraction are when a ≤ 19 and b ≤ 9. If the subtraction result of the last digits is smaller than 10, i.e., the borrow is actually needed, we subtract 1 from the rest of the first operand m.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20\n\nclass Sub(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left <= 19 and right <= 9:\n\nreturn []\n\nl_last = left % 10 + 10 r_last = right % 10 thoughts = [T(Sub, (l_last, r_last))] l_rest, r_rest = left // 10, right // 10 if l_last - r_last < 10:\n\nthoughts.append(T(Sub, (l_rest, 1))) l_rest -= 1\n\nif r_rest > 0:\n\nthoughts.append(T(Sub, (l_rest, r_rest)))\n\nreturn thoughts\n\nHere is an example of 432-216:\n\n• X 1: GO 4 3 2 - 2 1 6 = GO 1 2 - 6 = 6 STOP GO 4 3 - 1 = 4 2 STOP GO\n\n4 2 - 2 1 = 2 1 STOP 2 1 6 STOP\n\n• X 2: GO 1 2 - 6 = 6 STOP • X 3: GO 4 3 - 1 = GO 1 3 - 1 = 1 2 STOP 4 2 STOP • X 4: GO 1 3 - 1 = 1 2 STOP • X 5: GO 4 2 - 2 1 = GO 1 2 - 1 = 1 1 STOP GO 4 - 2 = 2 STOP 2 1 STOP • X 6: GO 1 2 - 1 = 1 1 STOP • X 7: GO 4 - 2 = 2 STOP\n\nNotice that the final answer and the questions of each subproblem can be easily constructed from previous sequence.\n\nD.3 MULTIPLICATION\n\nThe base cases of multiplication are (i) when either operands are 0 or 1, or (ii) when both operands are less than 10. If one of the operands is 0, then the answer is zero; when one of them is 1, then the answer is just a copy of the other operand. For the cases where both operands are less than 10, we just let the model memorize them, which is similar to an elementary school math curriculum.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nThere are two types of non-base cases. For the simpler case, where the second operand is less than 10, we first split the first operand into the last digit and the rest. We then multiply each of them with the second operand and combine the results. Otherwise, we split the second operand into the last digit and the rest. The first operand is multiplied to each of them, and the results are summed.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n\nclass Mul(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left <= 1 or right <= 1:\n\nreturn []\n\nif left <= 9 and right <= 9:\n\nreturn []\n\nthoughts = [] if right < 10:\n\nthoughts.append(T(Mul, (left % 10, right))) thoughts.append(T(Mul, (left // 10, right)))\n\na1 = (left % 10) * right a2 = (left // 10) * right thoughts.append(T(Add, (a2 * 10, a1), 'tail'))\n\nelse:\n\na1 = left * (right % 10) thoughts.append(T(Mul, (left, right % 10)))\n\na2 = left * (right // 10) thoughts.append(T(Mul, (left, right // 10)))\n\nthoughts.append(T(Add, (a2 * 10, a1), 'tail'))\n\nreturn thoughts\n\nHere are some example contexts of multiplication:\n\n• X 1: GO 4 3 * 2 1 = GO 4 3 * 1 = 4 3 STOP GO 4 3 * 2 = 8 6 STOP TAIL\n\n8 6 0 + 4 3 = THINK\n\n• X 2: GO 4 3 * 1 = 4 3 STOP\n\n• X 3: GO 4 3 * 2 = GO 3 * 2 = 6 STOP GO 4 * 2 = 8 STOP TAIL 8 0 + 6 =\n\nTHINK\n\n• X 4: GO 3 * 2 = 6 STOP\n\n• X 5: GO 4 * 2 = 8 STOP\n\n• X 6: GO 8 0 + 6 = GO 0 + 6 = 6 STOP 8 6 STOP\n\n• X 7: GO 0 + 6 = 6 STOP\n\n• X 8: GO 8 6 0 + 4 3 = GO 0 + 3 = 3 STOP GO 8 6 + 4 = 9 0 STOP 9 0 3\n\nSTOP\n\n• X 9: GO 0 + 3 = 3 STOP\n\n• X 10: GO 8 6 + 4 = GO 6 + 4 = 1 0 STOP GO 8 + 1 = 9 STOP 9 0 STOP\n\n• X 11: GO 6 + 4 = 1 0 STOP\n\n• X 12: GO 8 + 1 = 9 STOP\n\nNotice that we use tail recursion in X 1 and X 3.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nD.4 COMPARISON\n\nComparison is used as a subroutine during division. The procedure for comparison consists of three steps:\n\n1. Compare the numbers of digits.\n\n2. If the numbers of digits are the same, compare the most significant digits.\n\n3. If the most significant digits are identical, compare the remaining digits recursively.\n\nWe find that the sequence models can perform the first step without an explicit subproblem. Therefore, we only add intermediate steps for the second and the third steps.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22\n\nclass Compare(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left < 10 and right < 10:\n\nreturn []\n\nthoughts = [] digit_l, digit_r = len(str(left)), len(str(right)) if digit_l == digit_r:\n\n# Compare first digit l_first, r_first = int(str(left)[0]), int(str(right)[0]) thoughts.append(T(Compare, (l_first, r_first))) if l_first == r_first: # Compare the rest l_rest = int(str(left)[1:]) r_rest = int(str(right)[1:]) thoughts.append(T(Compare, (l_rest, r_rest)))\n\nreturn thoughts\n\nThe following is an example of comparing 153 and 159.\n\n• X 1: GO 1 5 3 VS 1 5 9 = GO 1 VS 1 = EQ STOP GO 5 3 VS 5 9 = LT STOP\n\nLT STOP\n\n• X 2: GO 1 VS 1 = EQ STOP • X 3: GO 5 3 VS 5 9 = GO 5 VS 5 = EQ STOP GO 3 VS 9 = LT STOP LT STOP • X 4: GO 5 VS 5 = EQ STOP • X 5: GO 3 VS 9 = LT STOP\n\nD.5 DIVISION\n\nSolving division is the most challenging among the four basic arithmetic operations since the procedure is basically trial and error, searching for the correct quotient. Nonetheless, the following process is a recursive version of the elementary school division.\n\nThe base case is when the dividend is less than or equal to the divisor. If the dividend is smaller than the divisor, the quotient is 0, and the remainder is the dividend. If the dividend is equal to the divisor, than the quotient is 1, and the remainder is 0. Both cases can be handled relatively easily by neural sequence models. To determine whether it is one of these cases, we always perform the comparison as the first subproblem.\n\nIf it is not a base case, we check whether the dividend is smaller than 10 times the divisor. If the dividend is smaller, we subtract the divisor from the dividend and recursively divide the result with the divisor. The final answer is attained by simply adding 1 to the quotient of the smaller division.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nTo explain the other case, where the dividend is greater than 10 times the divisor, let us call the dividend a and the divisor b. First, we split the a into the last digit x and the remaining digits m. Then, we divide m with the divisor b, i.e., we are solving one-digit-smaller subproblem first. Since we define the division operation to return both a quotient and a remainder, the quotient q1 = m/b and the remainder r1 = m mod b from the subproblem is added to the context. Next, we concatenate the remainder and x, which is numerically computing r × 10 + x, and divide it again with b. Let the quotient and the remainder of this operation q2 and r2. Then, the quotient of the final answer is q1 × 10 + q2, while the remainder is simply r2.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20\n\nclass Div(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args thoughts = [T(Compare, (left, right))]\n\n# Base cases if left <= right:\n\nreturn thoughts\n\nthoughts.append(T(Compare, (left, right * 10))) if left <= right * 10:\n\ndiff = left - right thoughts.append(T(Sub, (left, right))) thoughts.append(T(Div, (diff, right)))\n\nelse:\n\nthoughts.append(T(Div, (left // 10, right))) left_remainder = (left // 10) % right * 10 + left % 10 thoughts.append(T(Div, (left_remainder, right)))\n\nreturn thoughts\n\nThe following is an example of 76 ÷ 29.\n\n• X 1: GO 7 6 ÷ 2 9 = GO 7 6 VS 2 9 = GT STOP GO 7 6 VS 2 9 0 = LT STOP GO 7 6 - 2 9 = 4 7 STOP GO 4 7 ÷ 2 9 = 1 R 1 8 STOP 2 R 1 8 STOP\n\n• X 2: GO 7 6 VS 2 9 = GO 7 VS 2 = GT STOP GT STOP\n\n• X 3: GO 7 VS 2 = GT STOP\n\n• X 4: GO 7 6 VS 2 9 0 = LT STOP\n\n• X 5: GO 7 6 - 2 9 = GO 1 6 - 9 = 7 STOP GO 7 - 1 = 6 STOP GO 6 - 2 =\n\n4 STOP 4 7 STOP\n\n• ... • X 9: GO 4 7 ÷ 2 9 = GO 4 7 VS 2 9 = GT STOP GO 4 7 VS 2 9 0 = LT STOP GO 4 7 - 2 9 = 1 8 STOP GO 1 8 ÷ 2 9 = 0 R 1 8 STOP 1 R 1 8 STOP\n\n• X 10: GO 4 7 VS 2 9 = GO 4 VS 2 = GT STOP GT STOP\n\n• X 11: GO 4 VS 2 = GT STOP\n\n• X 12: GO 4 7 VS 2 9 0 = LT STOP\n\n• X 13: GO 4 7 - 2 9 = GO 1 7 - 9 = 8 STOP GO 4 - 1 = 3 STOP GO 3 - 2 =\n\n1 STOP 1 8 STOP\n\n• ... • X 17: GO 1 8 ÷ 2 9 = GO 1 8 VS 2 9 = LT STOP 0 R 1 8 STOP\n\n• X 18: GO 1 8 VS 2 9 = GO 1 VS 2 = LT STOP LT STOP\n\n• ...\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nD.6 LONGEST COMMON SUBSEQUENCE (LCS)\n\nGiven sequences A and B, the algorithm starts by comparing the last characters of the two sequences. If the last two characters are the same, we find LCS of the subsequences without the last characters, i.e., LCS of A∶−1 and B∶−1. Otherwise, we compute the LCSs of the cases where the last character of either side is removed, and return the better one. In the following code, LCS._answer is the subroutine that finds the LCS of two sequences. Equal returns TRUE if the two arguments are the same, or FALSE otherwise.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22\n\nclass LCS(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nl, r = args if len(l) == 0 or len(r) == 0:\n\nreturn []\n\nthoughts = [T(Equal, (l[-1], r[-1]))] if l[-1] == r[-1]:\n\nthoughts.append(T(LCS, (l[:-1], r[:-1]))) return thoughts\n\nlcs1_args = (l[:-1], r) lcs2_args = (l, r[:-1]) lcs1 = LCS._answer(lcs1_args) lcs2 = LCS._answer(lcs2_args) thoughts.extend([\n\nT(LCS, lcs1_args), T(LCS, lcs2_args), T(Compare, (len(lcs1), len(lcs2)))\n\n]) return thoughts\n\nThe following is an example of finding the LCS of 123 and 234.\n\n• X 1: GO 1 2 3 LCS 2 3 4 = GO EQUAL 3 , 4 = FALSE STOP GO 1 2 LCS 2 3 4 = 2 ; 1 STOP GO 1 2 3 LCS 2 3 = 2 3 ; 2 STOP GO 1 VS 2 = LT STOP 2 3 ; 2 STOP\n\n• X 2: GO EQUAL 3 , 4 = FALSE STOP • X 3: GO 1 2 LCS 2 3 4 = GO EQUAL 2 , 4 = FALSE STOP GO 1 LCS 2 3 4 = ; 0 STOP GO 1 2 LCS 2 3 = 2 ; 1 STOP GO 0 VS 1 = LT STOP 2 ; 1 STOP\n\n• ... • X 21: GO 1 2 3 LCS 2 3 = GO EQUAL 3 , 3 = TRUE STOP GO 1 2 LCS 2 = 2\n\n; 1 STOP 2 3 ; 2 STOP\n\n• ... • X 23: GO 1 VS 2 = LT STOP\n\nD.7 LONGEST PALINDROMIC SUBSEQUENCE (LPS)\n\nThe overall algorithm for LPS is similar to LCS. The base cases are when the sequence length is less then 3. If it is not a base case, we first check if the characters at both ends of the sequence are the same. If they are the same, we find the LPS of the subsequence excluding them. Otherwise, we compare the cases where one of the end characters are excluded.\n\n1 2\n3 4\n5 6\n\nclass LPS(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\n# Base cases if len(args) == 1: return []\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n\nelif len(args) == 2:\n\nreturn [T(Equal, args)]\n\nthoughts = [T(Equal, (args[0], args[1]))] if args[0] == args[-1]:\n\nsub_lps = LPS._answer(args[1:-1]) thoughts.extend([\n\nT(LPS, args[1:-1]), T(Add, (len(sub_lps), 2))\n\n])\n\nelse:\n\nlps1_args = args[:-1] lps2_args = args[1:] lps1 = LPS._answer(lps1_args) lps2 = LPS._answer(lps2_args) thoughts.extend([\n\nT(LPS, lps1_args), T(LPS, lps2_args), T(Compare, (len(lps1), len(lps2)))\n\n])\n\nreturn thoughts\n\nThe following is an example of LPS.\n\n• X 1: GO LPS 1 2 3 2 = GO EQUAL 1 , 2 = FALSE STOP GO LPS 1 2 3 = 1 ; 1 STOP GO LPS 2 3 2 = 2 3 2 ; 3 STOP GO 1 VS 3 = LT STOP 2 3 2 ; 3 STOP\n\n• X 2: GO EQUAL 1 , 2 = FALSE STOP • X 3: GO LPS 1 2 3 = GO EQUAL 1 , 3 = FALSE STOP GO LPS 1 2 = 1 ; 1\n\nSTOP GO LPS 2 3 = 2 ; 1 STOP GO 1 VS 1 = EQ STOP 1 ; 1 STOP\n\n• ... • X 10: GO LPS 2 3 2 = GO EQUAL 2 , 2 = TRUE STOP GO LPS 3 = 3 ; 1 STOP\n\nGO 1 + 2 = 3 STOP 2 3 2 ; 3 STOP\n\n• ... • X 14: GO 1 VS 3 = LT STOP\n\nD.8\n\n0-1 KNAPSACK\n\nThe base cases are when there is only one item. In this case, we simply compare the item’s weight and the knapsack’s capacity, to determine whether the item should be included. If it is a non-base case, we compare two possibilities: (i) include the first item, or (ii) exclude the first item. We recursively compute the subproblems and find the case with the best value.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17\n\nclass LPS(Problem):\n\n@staticmethod def thought(args) -> list[T]: items, capacity = args value, weight = items[0]\n\n# Base case if len(items) == 1:\n\nreturn [T(Compare, (weight, capacity))]\n\n# When excluding the current item items_max, value_max = Knapsack._answer((items[1:], capacity)) thoughts = [\n\nT(Knapsack, (items[1:], capacity)), T(Compare, (weight, capacity)),\n\n]\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\n18 19 20 21 22 23 24 25 26 27 28 29 30\n\n# When including the current item if weight <= capacity:\n\nitems_sub, value_sub = Knapsack._answer( (items[1:], capacity - weight))\n\nvalue_incl = value_sub + value thoughts.extend([\n\nT(Sub, (capacity, weight)), T(Knapsack, (items[1:], capacity - weight)), T(Add, (value_sub, value)), T(Compare, (value_incl, value_max)),\n\n])\n\nreturn thoughts\n\nThe following is an example of 0-1 knapsack problem with three items and a knapsack capacity of 10.\n\n• X 1: GO KNAPSACK 3 & 9 , 4 & 2 , 9 & 5 @ 1 0 = GO KNAPSACK 4 & 2 , 9 & 5 @ 1 0 = 4 & 2 , 9 & 5 $ 1 3 STOP GO 9 VS 1 0 = LT STOP GO 1 0 - 9 = 1 STOP GO KNAPSACK 4 & 2 , 9 & 5 @ 1 = $ 0 STOP GO 0 + 3 = 3 STOP GO 3 VS 1 3 = LT STOP 4 & 2 , 9 & 5 $ 1 3 STOP\n\n• X 2: GO KNAPSACK 4 & 2 , 9 & 5 @ 1 0 = GO KNAPSACK 9 & 5 @ 1 0 = 9 & 5 $ 9 STOP GO 2 VS 1 0 = LT STOP GO 1 0 - 2 = 8 STOP GO KNAPSACK 9 & 5 @ 8 = 9 & 5 $ 9 STOP GO 9 + 4 = 1 3 STOP GO 1 3 VS 9 = GT STOP 4 & 2 , 9 & 5 $ 1 3 STOP\n\n• ... • X 11: GO 9 VS 1 0 = LT STOP • X 12: GO 1 0 - 9 = 1 STOP • X 13: GO KNAPSACK 4 & 2 , 9 & 5 @ 1 = GO KNAPSACK 9 & 5 @ 1 = $ 0 STOP\n\nGO 2 VS 1 = GT STOP $ 0 STOP\n\n• ... • X 17: GO 0 + 3 = 3 STOP • X 18: GO 3 VS 1 3 = LT STOP\n\nD.9 TERNARY ADDITION AND MULTIPLICATION\n\nTernary addition and multiplication arises as a subproblem while solving MCM, which will be explained in the next section. They are simple extensions of addition and multiplication to three integers.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18\n\nclass TernaryAdd(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\na1, a2, a3 = args return [\n\nT(Add, (a1, a2)), T(Add, (a1 + a2, a3), 'tail')\n\n]\n\nclass TernaryMul(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\na1, a2, a3 = args return [\n\nT(Mul, (a1, a2)), T(Mul, (a1 * a2, a3), 'tail')\n\n]\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nD.10 MATRIX CHAIN MULTIPLICATION (MCM)\n\nGiven N matrices, the N − 1 subproblems are defined for each possible binary split. For the multiplication of four matrices ABCD, there are three possible binary splits: A(BCD), (AB)(CD), and (ABC)D. For each binary split, the total cost is the sum of (i) the minimum cost of computing the first group, (ii) the minimum cost of computing the second group, and (iii) the cost of multiplying the two matrices resulting from each group. Once we get the total costs of each binary split, we return choose the best split with the minimum cost. The following code implements this procedure.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\n\nclass MCM(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nmats, min_order, min_cost = args\n\n# Base cases if len(mats) == 1: return []\n\nif min_order is None:\n\n# Top-level problem l_mats, r_mats = mats[:1], mats[1:]\n\nelse:\n\n# Middle of recursion l_mats, r_mats = mats\n\nl_args = (l_mats, None, None) r_args = (r_mats, None, None) l_order, l_cost = MCM._answer(l_args) r_order, r_cost = MCM._answer(r_args) agg_cost = l_mats[0][0] * r_mats[0][0] * r_mats[-1][1] thoughts = [\n\nT(MCM, l_args), T(MCM, r_args), T(TernaryMul, (l_mats[0][0], r_mats[0][0], r_mats[-1][1])), T(TernaryAdd, (l_cost, r_cost, agg_cost)),\n\n]\n\ncost = l_cost + r_cost + agg_cost if min_cost is not None:\n\nthoughts.append(T(Compare, (cost, min_cost)))\n\nif min_cost is None or cost < min_cost:\n\nmin_cost = cost min_order = l_order, r_order\n\nif len(r_mats) > 1:\n\nnew_l_mats = l_mats + (r_mats[0],) new_r_mats = r_mats[1:] thoughts.append(\n\nT(MCM, ((new_l_mats, new_r_mats), min_order, min_cost), 'tail'))\n\nreturn thoughts\n\nThe following is an example of three-matrix MCM.\n\n• X 1: GO MCM 3 × 9 , 9 × 4 , 4 × 5 = GO MCM 3 × 9 = 3 × 9 ; 0 STOP GO MCM 9 × 4 , 4 × 5 = 9 × 4 , 4 × 5 ; 1 8 0 STOP GO 3 * 9 * 5 = 1 3 5 STOP GO 0 + 1 8 0 + 1 3 5 = 3 1 5 STOP TAIL MCM 3 × 9 , 9 × 4 | 4 ×\n\n5 ACC 3 × 9 , ( 9 × 4 , 4 × 5 ) ; 3 1 5 = THINK\n\n• ... • X 32: GO MCM 3 × 9 , 9 × 4 | 4 × 5 ACC 3 × 9 , ( 9 × 4 , 4 × 5 ) ; 3 1 5 = GO MCM 3 × 9 , 9 × 4 = 3 × 9 , 9 × 4 ; 1 0 8 STOP GO MCM 4 × 5 = 4 × 5 ; 0 STOP GO 3 * 4 * 5 = 6 0 STOP GO 1 0 8 + 0 + 6 0 = 1 6 8\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nSTOP GO 1 6 8 VS 3 1 5 = LT STOP ( 3 × 9 , 9 × 4 ) , 4 × 5 ; 1 6 8 STOP\n\n• ...\n\nD.11 SORTING\n\nAmong several sorting algorithms, we choose merge sort for our experiments with CoT and RoT. Note that WT is not relevant to the sorting algorithm since it produces the answer directly. The merge sort algorithm is simple: (i) split the given sequence to two equally sized subsequences, (ii) sort each subsequence, and (iii) merge the two sorted sequences. Since the final merge operation is quite complicated, we define the merge as a problem type.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n\nclass Merge(Problem): @staticmethod def thought(args) -> list[T]:\n\nl, r = args if len(l) == 0 or len(r) == 0:\n\nreturn []\n\nthoughts = [T(Compare, (l[0], r[0]))] if l[0] < r[0] and len(l) > 1:\n\nthoughts.append(T(Merge, (l[1:], r)))\n\nelif l[0] >= r[0] and len(r) > 1:\n\nthoughts.append(T(Merge, (l, r[1:])))\n\nreturn thoughts\n\nclass MergeSort(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nif len(args) < 2: return []\n\nl_len = (len(args) + 1) // 2 l = args[:l_len] r = args[l_len:] return [\n\nT(MergeSort, l), T(MergeSort, r), T(Merge, (tuple(sorted(l)), tuple(sorted(r))), 'tail')\n\n]\n\nE FINE-TUNING GPT-3 FOR RECURSION OF THOUGHT\n\nUsing the OpenAI API, we fine-tune GPT-3 for Recursion of Thought. The goal is to learn 16-digit addition, 16-digit subtraction, 8-digit multiplication, and 8-digit division simultaneously. GPT-3’s fine-tuning API takes a dataset where each example is a prompt-completion pair in plain text. It is converted to tokens by a special tokenizer for GPT, which we cannot control. This API is not directly compatible with RoT due to several reasons.\n\n• There is no special tokens such as GO , THINK , and STOP .\n\n• The input and target sequences have to be the same. However, they are different in RoT due to the THINK token. Once THINK is produced, the RoT framework triggers the recursion process to find the subproblem’s answer and replace the THINK token with it. Therefore, the THINK token appears in the target sequences, but never in the input sequences.\n\nMoreover, the way that GPT-3 tokenizes numbers hinders the learning of arithmetic reasoning rules. GPT-3 tokenizes a multi-digit number into a set of two-digit or three-digit numbers. For example, the text 1234567 is converted to the sequence of tokens 123 45 67 . Under this tokenization scheme, the relationship between the numbers become obscured. As an example, the tokens 7 , 17 ,\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\n27 , ..., 997 all have 7 as their last digit. Since there is no direct way for a model to know that they share the same digit, it is crucial to use each digit as a token. We believe that OpenAI needs to correct this tokenization of GPT-3 for numbers.\n\nLuckily, we can mimic the RoT procedures with the API by using several tricks. First, we replace the special tokens with plain lower-case words, e.g., GO → go and STOP → stop, which are included in the vocabulary of GPT-3. Second, we add a space before each token to make sure that the GPT tokenizer separates each token. We also add space before each digit to prevent the tokenizer grouping a number into 2-to-3-digit tokens. Finally, to simulate the behavior of the THINK and STOP tokens, we derive multiple examples from each context, one for each THINK or STOP output.\n\nAs an example, context X 3 in Figure 1 is converted to the following JSON lines for GPT-3 as follows:\n\nX 3 GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP\n\nGO 4 + 3 = 7 STOP\n\n7 5 STOP\n\nY 3 PAD ×7\n\nGO 0 + 5 = THINK PAD GO 4 + 3 = THINK PAD 7 5 STOP\n\n⇓\n\n1 2\n3\n\n{\"prompt\": \" go 4 0 + 3 5 =\", \"completion\": \" go 0 + 5 = think\"} {\"prompt\": \" go 4 0 + 3 5 = go 0 + 5 = 5 stop\", \"completion\": \" go 4 + 3 = think\"} {\"prompt\": \" go 4 0 + 3 5 = go 0 + 5 = 5 stop go 4 + 3 = 7 stop\", \"completion\": \"\n\n7 5 stop\"}\n\nIn the case of Without Thought (WT), each problem is simply converted into a single example:\n\nX GO 4 0 + 3 5 = 7 5 STOP Y PAD ×7\n\n7 5 STOP\n\n⇓\n\n1\n\n{\"prompt\": \" go 4 0 + 3 5 =\", \"completion\": \" 7 5 stop\"}\n\nIn both cases of RoT and WT, we fine-tune GPT-3 for 10K steps with a batch size of 256. Among the several variants of GPT-3, we use Ada which is offered at the lowest cost. Note that RoT produces multiple contexts for each problem, and each RoT context is converted to multiple training examples. For this reason, the GPT-3 fine-tuned for RoT encounters much fewer problems during training, although the number of training steps are the same.\n\nF TRAINING DETAILS OF THE TINY MODELS\n\nIn all experiments, we use a batch size of 256 and Adam optimizer Kingma & Ba (2015) with a learning rate of 0.001, i.e., the default learning rate in PyTorch. We train the Transformers for 500K steps and and decay the learning rate by half every 50K steps. Since the LSTMs converge slower than the Transformers, we train them for 800K steps and decay the learning rate by half every 100K steps. At every 20K steps, we evaluate the model on a test set of 30K problems, and if a model reaches a perfect accuracy of 1.0, we do not train the model further. The models can be trained on a single GPU with 12GB memory.\n\nG EFFICIENT EVALUATION OF ROT\n\nAt the problem scales that RoT is tested, solving a single problem can require hundreds of thousands of tokens. Therefore, we have to develop an efficient evaluation algorithm. Since the models are\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\ndeterministic, we do not need to evaluate the same context multiple times. Therefore, given a test set, we first find all unique contexts that should appear while solving the test set. Note that there would be a lot of duplicate contexts, especially for the base cases. These unique contexts can be evaluated in parallel. We evaluate each context in a similar manner to the training. For each context, we construct the target sequence, as we described in §3.2, and check if the model outputs the target sequence correctly, given the GT context. In case of GPT-3, we convert the contexts and target sequences as described in Appendix E to feed into the API. Once every context is individually evaluated, the next step is to aggregate the results. In this aggregation stage, a (sub)problem is marked correct only if its subproblems are all marked correct. This way, we efficiently evaluate RoT with minimal cost.\n\nH FAILURE CASE ANALYSIS OF GPT-3 WITH ROT\n\nIn this section we provide some failure cases of RoT in the GPT-3 experiments for further insights. Although the model achieves an accuracy of over 99% in almost every case, there are several problems where it makes mistakes. Nonetheless, only a few characters are wrong in each context.\n\nThe most common type of error is repeating the same character for a wrong number of times. The following is the examples of this error type.\n\nPrompt\n\nCompletion\n\nModel Output\n\nPrompt\n\nCompletion\n\nModel Output\n\ngo 1 6 4 3 4 vs 1 0 1 1 6 = go 1 vs 1 = eq stop\n\ngo 6 4 3 4 vs 1 1 6 = think\n\ngo 6 4 3 4 vs 1 1 1 6 = think\n\ngo 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 4 4 3 0 2 5 3 6 0 1 7\n\n= go 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 7 = 3 3 6 6 6 1 4 6 6 2 7 7 9 1 3 7 stop go 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 4 4 3 0 2 5 3 6 0 1 = 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 stop\n\ntail 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 0\n\n+ 3 3 6 6 6 1 4 6 6 2 7 7 9 1 3 7 = think\n\ntail 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 0\n\n+ 3 3 6 6 6 6 1 4 6 6 2 7 7 9 1 3 7 = think\n\nPrompt\n\ngo 1 0 5 4 6 2 0 * 1 5 = go 1 0 5 4 6 2 0 * 5 = 5 2 7 3\n\n1 0 0 stop go 1 0 5 4 6 2 0 * 1 = 1 0 5 4 6 2 0 stop\n\nCompletion\n\nModel Output\n\ntail 1 0 5 4 6 2 0 0 + 5 2 7 3 1 0 0 = think\n\ntail 1 0 5 4 6 2 0 0 0 + 5 2 7 3 1 0 0 = think\n\nAnother common case is incorrectly counting the number of digits while comparing two numbers. When comparing numbers following our recursive procedure, the model has to immediately return the answer if the number of digits are different. In the following example, the first number has seven digits, and the second one has six digits.\n\nPrompt\n\ngo 1 0 6 9 0 3 4 vs 8 9 1 5 0 6 =\n\nCompletion\n\ngt stop\n\nModel Output\n\ngo 1 vs 8\n\nAlthough the model should directly output the answer, i.e., gt stop, it decides that the number of digits are the same and moves on to the next step, which is to compare the most significant digits. This error is caused by the specific recursive procedure of our choice. Since we assume that the model would easily figure out the difference in the numbers of digits, we did not add explicit steps to count the digits. Although the model detects the difference in most cases, it turns out to be a\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nrelatively challenging operation. A possible solution is to teach the model to explicitly count the number of digits and compare them.\n\nI CONTEXT LENGTH DISTRIBUTION\n\nIn Figure 4, we present the distributions of context lengths for each problem type. We compare the context lengths of RoT and CoT. For each configuration, we randomly sample 10K contexts from the training distribution and plot the histogram of their lengths. The graphs show that the context sizes of CoT are many orders of magnitude larger than RoT. In theory, the total number of tokens to generate for each problem is identical in both RoT and CoT (if we do not count the THINK tokens). However, RoT’s context sizes are much smaller since it utilizes multiple contexts.\n\nAnother advantage of RoT is the utilization of dynamic programming. Since we can easily cache the duplicate computations of RoT as explained in Appendix G, we can drastically reduce the amount of token generation if there is a redundant structure in the problem. The amount of tokens to generate for each problem is plotted in Figure 5. The benefit is especially prominent in the algorithmic problems. For example, finding the LCS of two 32-digit sequences results in more than 1018 tokens if we naively use CoT or RoT. If we use dynamic programming with RoT, we can efficiently solve the same problem with much less cost.\n\nJ TRANSFORMERS ARE POWERFUL SORTING MACHINES\n\nIn fact, the first algorithmic task that we tested is sorting since it has been widely used as a benchmark for algorithmic reasoning (Reed & de Freitas, 2016; Cai et al., 2017; Pierrot et al., 2019). However, we find that Transformers are incredibly good at sorting, even in the WT setting. Figure 6 shows the sorting experiment. For CoT and RoT, we train the merge sort algorithm. Interestingly, WT easily achieves a perfect score in sorting 64 three-digit numbers. Also the training converges much faster than RoT. The Transformer architecture, more specifically the attention mechanism, seems to be perfectly suited for the sorting operation.\n\nK THE EXACT VALUES OF FIGURE 3\n\nTable 2-5 show the exact values of the graphs in Figure 3. Except for the GPT-3 experiments in Table 2, we report the average and the standard deviation of eight runs. Each GPT-3 experiment is done only once.\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: The distributions of context lengths.\n\n34\n\n$GGGLJLW5R7$GGGLJLW&R7$GGGLJLW5R7$GGGLJLW&R76XEGLJLW5R76XEGLJLW&R76XEGLJLW5R76XEGLJLW&R70XOGLJLW5R70XOGLJLW&R70XOGLJLW5R70XOGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7/&6OHQJWK5R7H/&6OHQJWK&R7/&6OHQJWK5R7H/&6OHQJWK&R7/36OHQJWK5R7/36OHQJWK&R7/36OHQJWK5R7H/36OHQJWK&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R70&0PDWULFHV5R70&0PDWULFHV&R70&0PDWULFHV5R7H0&0PDWULFHV&R7Under review as a conference paper at ICLR 2023\n\nFigure 5: The distribution of the total number of tokens to produce in order to solve each problem. RoT can utilize dynamic programming to reduce redundant computations.\n\n35\n\n$GGGLJLW5R7$GGGLJLW&R7$GGGLJLW5R7$GGGLJLW&R76XEGLJLW5R76XEGLJLW&R76XEGLJLW5R76XEGLJLW&R70XOGLJLW5R70XOGLJLW&R70XOGLJLW5R70XOGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7/&6OHQJWK5R7H/&6OHQJWK&R7/&6OHQJWK5R7H/&6OHQJWK&R7/36OHQJWK5R7/36OHQJWK&R7/36OHQJWK5R7H/36OHQJWK&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R70&0PDWULFHV5R70&0PDWULFHV&R70&0PDWULFHV5R7H0&0PDWULFHV&R7Under review as a conference paper at ICLR 2023\n\nFigure 6: Sorting experiment with the tiny Transformer.\n\nProblem\n\nDifficulty\n\nWT CoT\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\nLCS\n\nLPS\n\n0-1 Knapsack\n\nMCM\n\n32-digit 48-digit\n\n32-digit 48-digit\n\n8-digit 16-digit\n\n8-digit 16-digit\n\nlength 16 length 24\n\nlength 24 length 40\n\n4 items 6 items\n\n3 matrices 4 matrices\n\n0.991 0.853\n\n0.991 0.886\n\n0.337 0.098\n\n0.363 0.123\n\n0.980 0.832\n\n0.995 0.800\n\n0.945 0.634\n\n0.481 0.110\n\nRoT − 0.998 − 0.995 − 0.998 − 0.998 − 0.999 − 0.994 − 1.000 − 0.989 − 0.995 − 0.998 − 1.000 − 0.974 − 0.999 − 1.000 − 0.997 − 0.992\n\nTable 2: The exact values of the GPT-3 experiments in Figure 3a.\n\n36\n\nRI,WHPV$FFXUDF\\6RUWLQJ:LWKRXW7KRXJKW&KDLQRI7KRXJKW5HFXUVLRQRI7KRXJKWUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\n8-digit 16-digit 24-digit 32-digit 40-digit 48-digit 56-digit 64-digit\n\n8-digit 16-digit 24-digit 32-digit 40-digit 48-digit 56-digit 64-digit\n\n2-digit 4-digit 8-digit 12-digit 16-digit 20-digit 24-digit 28-digit 32-digit\n\n2-digit 4-digit 8-digit 12-digit 16-digit 20-digit 24-digit 28-digit 32-digit\n\nWT 0.863 ± 0.265 0.370 ± 0.475 0.336 ± 0.430 0.455 ± 0.458 0.119 ± 0.316 0.082 ± 0.216 0.105 ± 0.277 0.000 ± 0.000 0.982 ± 0.006 0.705 ± 0.411 0.238 ± 0.412 0.221 ± 0.385 0.426 ± 0.433 0.114 ± 0.303 0.116 ± 0.307 0.161 ± 0.282 1.000 ± 0.000 0.817 ± 0.023 0.340 ± 0.032 0.169 ± 0.015 0.104 ± 0.016 0.048 ± 0.020 0.033 ± 0.017 0.014 ± 0.006 0.012 ± 0.001 1.000 ± 0.000 0.978 ± 0.008 0.354 ± 0.029 0.186 ± 0.009 0.128 ± 0.011 0.087 ± 0.012 0.075 ± 0.005 0.059 ± 0.007 0.048 ± 0.008\n\nCoT 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000\n\n1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000\n\n1.000 ± 0.000 1.000 ± 0.000\n\n1.000 ± 0.000 1.000 ± 0.000\n\nRoT 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.001 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.999 ± 0.001 − 0.999 ± 0.001 − 0.999 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.999 ± 0.000 − 0.999 ± 0.000\n\nTable 3: The exact values of the Transformer experiments in Figure 3b (arithmetic problems).\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nLCS\n\nLPS\n\n0-1 Knapsack\n\nMCM\n\nlength 3 length 4 length 8 length 12 length 16 length 20 length 24 length 28 length 32\n\nlength 4 length 7 length 8 length 16 length 24 length 32 length 40 length 48 length 56\n\n2 items 4 items 6 items 8 items 10 items 12 items\n\n2 matrices 4 matrices 6 matrices 8 matrices 10 matrices 12 matrices\n\nWT 1.000 ± 0.000 0.997 ± 0.008 0.999 ± 0.002 0.965 ± 0.025 0.880 ± 0.035 0.759 ± 0.043 0.622 ± 0.038 0.484 ± 0.043 0.375 ± 0.030 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 0.999 ± 0.001 0.950 ± 0.019 0.788 ± 0.019 0.608 ± 0.023 0.477 ± 0.030 0.365 ± 0.029 1.000 ± 0.000 0.966 ± 0.006 0.849 ± 0.007 0.640 ± 0.242 0.481 ± 0.279 0.435 ± 0.252 0.973 ± 0.009 0.177 ± 0.069 0.088 ± 0.029 0.033 ± 0.025 0.051 ± 0.032 0.026 ± 0.011\n\nCoT 1.000 ± 0.000\n\n1.000 ± 0.000 1.000 ± 0.000\n\n1.000 ± 0.000 1.000 ± 0.000\n\n1.000 ± 0.000\n\nRoT\n\n− − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.999 ± 0.000 − 0.999 ± 0.000 −\n− − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.999 ± 0.001 − 0.998 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.988 ± 0.029 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.998 ± 0.001 − 0.996 ± 0.002\n\nTable 4: The exact values of the Transformer experiments in Figure 3b (algorithmic problems).\n\n38\n\nUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\n2-digit 4-digit 6-digit 8-digit 10-digit 12-digit 14-digit 16-digit\n\n2-digit 4-digit 6-digit 8-digit 10-digit 12-digit 14-digit 16-digit\n\n2-digit 3-digit 4-digit 5-digit 6-digit 7-digit 8-digit\n\n1-digit 2-digit 3-digit 4-digit 5-digit 6-digit 7-digit 8-digit\n\nWT 1.000 ± 0.000 0.642 ± 0.305 0.005 ± 0.008 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 1.000 ± 0.000 0.776 ± 0.179 0.006 ± 0.001 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 1.000 ± 0.000 0.855 ± 0.044 0.636 ± 0.061 0.338 ± 0.063 0.270 ± 0.030 0.162 ± 0.025 0.138 ± 0.025 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.001 0.891 ± 0.072 0.516 ± 0.077 0.308 ± 0.069 0.192 ± 0.028 0.115 ± 0.015\n\nCoT 1.000 ± 0.000 1.000 ± 0.001 0.997 ± 0.005 0.905 ± 0.155 0.795 ± 0.341\n\n1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 0.896 ± 0.252 0.443 ± 0.377\n\n1.000 ± 0.000\n\n1.000 ± 0.000\n\nRoT 1.000 ± 0.000 1.000 ± 0.000 0.999 ± 0.000 0.999 ± 0.001 0.986 ± 0.024 − 0.871 ± 0.275 − 0.358 ± 0.430 − 0.120 ± 0.202 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 0.994 ± 0.016 0.908 ± 0.236 − 0.507 ± 0.398 − 0.295 ± 0.406 − 0.101 ± 0.137 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.987 ± 0.008 − 0.896 ± 0.105 − 0.670 ± 0.208 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.998 ± 0.004 − 0.996 ± 0.007 − 0.958 ± 0.036 − 0.914 ± 0.090\n\nTable 5: The exact values of the LSTM experiments in Figure 3c.\n\n39",
  "translations": [
    "# Summary Of The Paper\n\nThis paper demonstrates that for simple algorithmic problems (arithmetic), language models can be taught to split problems into multiple subproblems, which can then be fed back to the LM to be solved independently. With very small models this can achieve great performance on these tasks, and it circumvents the limit in context-length of existing transformer models.\n\n# Strength And Weaknesses\n\nMy main concern with this approach is that the paper does not discuss how to apply this approach to more general problems. The problems considered here are very simple, and an algorithm to solve them needs to be known in order to generate the training data. So, in its current form this does not enable any new abilities as we could use the base algorithm instead of using expensive LMs.\n\nIn terms of the insight from a learning point-of-view, I also do not see anything surprising in this work. After decomposing the problem manually (i.e. by writing an algorithm to generate training data), the learning task is pretty simple (for addition, for example, all the model has to do is to extract the last digit from two numbers). So the high accuracy of the resulting method is not that surprising.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing style is a bit \"flashy\", which gives room for miscommunications. For example, the paper opens with the following statement:\n\n\"Although neural networks have achieved amazing results on various domains, e.g., images, texts,\naudios, videos, games, etc., nearly all of them are classified as System 1 tasks (Kahneman, 2013), ...\"\n\nAfaik, this statement is not supported by the literature, and in particular not by the given reference. Following Kahneman's theory, tasks like playing the board game go would certainly require system 2 thinking. IMO, dropping the first paragraph would help the paper.\n\nThe paper also makes claims such as \"the length of CoT can grow rapidly with the problem’s complexity\", without further explanation and I am not even sure what exactly this means.\n\n# Summary Of The Review\n\nThis paper demonstrates that decomposing simple algorithmic problems can circumvent the limit in context length language models.\n\nHowever, the technique only seems to work for problems for which we already have algorithms, and it is unclear to me if this could be extended to more general problems. The writing style should be improved.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" presents a novel framework called Recursion of Thought (RoT) aimed at enhancing the reasoning capabilities of language models (LMs) by overcoming limitations imposed by context size. RoT allows LMs to recursively divide complex problems into smaller subproblems, enabling the handling of multiple contexts and ultimately improving performance on reasoning tasks. The authors demonstrate that RoT significantly enhances the reasoning abilities of models like GPT-3 and even smaller models, achieving near-perfect accuracy on challenging tasks that require extensive token processing.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to enhancing reasoning in LMs through recursion, which is a significant advancement over existing Chain of Thought (CoT) methods limited by context size. The experimental results are compelling, showing clear benefits of RoT across a variety of reasoning tasks, and the framework is versatile enough to be applied to both large and small models. However, a notable weakness is the reliance on supervised training for RoT, which may limit its applicability in scenarios where labeled data is scarce. Additionally, while the paper outlines the architecture and training process well, it could benefit from a more detailed exploration of the implications and potential limitations of using RoT in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulate, presenting its ideas clearly and logically. The methodology is detailed, allowing for reproducibility of the experiments, which is crucial in the field of machine learning. The novelty of RoT is evident, as it proposes a recursive approach that contrasts with traditional methods. However, the paper could improve in discussing the potential challenges in applying RoT to broader contexts beyond the experimental setups presented.\n\n# Summary Of The Review\nOverall, the paper provides a significant contribution to the field of language model reasoning by introducing the Recursion of Thought framework, which effectively addresses the limitations of existing methods. The experimental validation of RoT's effectiveness is robust, although the reliance on supervised training presents some challenges for broader applicability. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" proposes a novel framework called Recursion of Thought (RoT) to enhance the reasoning capabilities of language models (LMs) in solving complex tasks. The methodology incorporates special tokens to manage context during inference, allowing LMs to recursively handle subproblems and generate intermediate steps. The findings demonstrate that RoT significantly improves performance on arithmetic and algorithmic reasoning tasks, even enabling smaller models to achieve high accuracy levels while outperforming traditional approaches like Chain of Thought (CoT) and direct answer generation (Without Thought).\n\n# Strength And Weaknesses\nThe key strengths of the paper include its innovative approach that addresses the limitations of context size in LMs, making it a versatile and effective tool for a range of reasoning tasks. The ability to apply RoT to various autoregressive models enhances its generalizability. Furthermore, the empirical results are compelling, showcasing substantial improvements over existing methods. However, the paper also has notable weaknesses, such as its reliance on supervised learning for effective problem decomposition, which might not be feasible for all scenarios. Additionally, RoT's limitation in length generalization restricts its effectiveness for unseen problem sizes, and its performance may vary depending on the underlying model architecture.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a structured presentation of the methodology and results that facilitates understanding. The quality of the experimental design is high, with appropriate comparisons and robust evaluations. The novelty of RoT is significant, as it introduces a new paradigm for reasoning in LMs. However, the reproducibility of the results may be hindered by the requirement for supervised learning and the potential variability in performance across different model architectures.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the use of LMs for complex reasoning tasks through the introduction of the Recursion of Thought framework. While it demonstrates significant improvements over existing methods, the reliance on supervised learning and architecture-specific performance may limit its broader applicability. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces **Recursion of Thought (RoT)**, a model-agnostic framework designed to enhance the reasoning capabilities of language models (LMs) by overcoming the inherent limitations of context size in existing models like GPT-3 and PaLM. The main contribution is a novel inference procedure that utilizes recursive contexts to break down complex reasoning tasks into smaller, manageable subproblems. RoT employs special tokens to structure the input and output sequence, allowing models to generate intermediate reasoning steps effectively. Experimental results demonstrate significant improvements in reasoning accuracy for both large and small models across various arithmetic and algorithmic tasks, highlighting RoT's potential to enable even smaller models to tackle previously complex problems.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to extending the context capabilities of LMs through recursion, which effectively addresses the limitations of traditional Chain of Thought (CoT) methods. The experiments conducted provide compelling evidence of RoT's effectiveness, showcasing significant performance gains across multiple reasoning tasks. However, a notable weakness is the reliance on supervised learning for training recursive strategies, which may limit the scalability of RoT in real-world applications where labeled data is scarce. Additionally, the paper does not fully explore the implications of length generalization, which could impact the practical deployment of RoT in diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations behind RoT, alongside a thorough explanation of the methodology. The experimental design is robust, providing sufficient details to allow for reproducibility, particularly in the training configurations and evaluation metrics used. The novelty of RoT is significant, as it introduces a paradigm shift in how LMs can process reasoning tasks. However, the paper could benefit from a more detailed discussion on potential limitations and failure cases observed during evaluations to enhance its overall comprehensiveness.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the field of language models through the introduction of the RoT framework, which successfully addresses the context size limitations and enhances reasoning capabilities. While the methodology and results are robust, the reliance on supervised learning for recursive strategies presents a challenge for broader applications. The paper is a valuable contribution to the ongoing exploration of recursive reasoning in LMs.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Recursion of Thought (RoT) framework, designed to enhance the reasoning capabilities of language models by enabling them to decompose complex problems into smaller, manageable subproblems. The methodology involves a model-agnostic approach that can be integrated into various architectures, leading to significant improvements in performance on complex tasks such as arithmetic and algorithmic reasoning. Findings indicate that RoT achieves near-perfect accuracy on challenging problems, although it requires task-specific supervision and careful implementation.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative approach to improving reasoning capabilities, demonstrated effective performance enhancements in specific tasks, and its model-agnostic nature, which allows broader applicability. However, limitations include the complexity of implementing the RoT framework, the necessity for supervision that may hinder generalizability, and the dependence on the underlying model architecture, which can affect performance. Additionally, while the paper provides comprehensive experimental validation, the focus on specific tasks may restrict the evaluation of RoT's applicability to more diverse real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear in its presentation, making the concepts of RoT understandable. The experimental results are robust, providing a thorough comparison with established baselines. The novelty of the proposed framework is significant, as it introduces a new method for reasoning enhancement. The inclusion of open-source code promotes reproducibility, although potential resource constraints for running experiments may limit accessibility for some researchers.\n\n# Summary Of The Review\nOverall, the paper offers a promising and innovative framework for enhancing reasoning in language models through the RoT approach. While it demonstrates substantial improvements in specific tasks, the complexities involved in implementation and the need for further exploration into generalizability present important considerations for future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework called \"Recursion of Thought\" (RoT), designed to address the limitations of current language models (LMs) in handling complex reasoning tasks. RoT employs a divide-and-conquer approach, allowing LMs to manage intricate multi-step problems by recursively generating and managing multiple contexts. Key contributions include a model-agnostic design that utilizes special tokens for contextual management, a recursive learning mechanism that enhances problem-solving capabilities, and empirical evidence demonstrating significant performance improvements on arithmetic and algorithmic reasoning tasks.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to enhancing the reasoning capabilities of LMs through recursion and context management. The framework's model-agnostic nature suggests broad applicability, which is a notable contribution to the field. However, the paper also acknowledges limitations, such as the reliance on supervised training, which may restrict its applicability in unsupervised settings. Additionally, the potential challenges in generalizing the framework across varying problem sizes and types indicate areas for future research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulately presents the RoT framework, making it accessible to readers. The quality of the methodology is high, with detailed explanations of token utilization and training processes. The novelty of RoT lies in its recursive approach and the use of special tokens, which distinguishes it from existing methods like Chain of Thought (CoT). Reproducibility is bolstered by the clear description of the training and inference processes, although practical implementation details could further support this aspect.\n\n# Summary Of The Review\nOverall, the paper offers a significant advancement in the reasoning capabilities of language models through the introduction of the RoT framework. Its innovative use of recursion and contextual management provides a promising direction for future research, although limitations in training methodologies and generalizability warrant further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" introduces a novel adversarial training framework called \"Recursion of Thought\" (RoT). The proposed method enhances language models' robustness against adversarial attacks by enabling them to break complex problems into simpler subproblems through a recursive reasoning mechanism. The authors empirically validate their approach, demonstrating significant performance improvements in both large pre-trained models (like GPT-3) and smaller architectures (like LSTMs) across various adversarial tasks. The findings suggest that RoT not only bolsters resilience against adversarial examples but also promotes better generalization in different adversarial contexts.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to adversarial training, incorporating recursive reasoning that mirrors human problem-solving strategies. This methodological advancement addresses a critical challenge in the field of language models, specifically their vulnerability to adversarial perturbations. The empirical results are robust and compelling, showcasing notable performance enhancements. However, a potential weakness is related to the dependency on supervision for implementing the RoT framework, which may limit its applicability in unsupervised settings. Additionally, the challenges concerning length generalization in adversarial contexts are acknowledged but could benefit from a more extensive discussion.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, methodologies, and findings. The clarity of the writing aids in understanding the complex concepts introduced. The novelty of the RoT framework is significant, presenting a fresh perspective on adversarial training. The empirical validation is thorough, although the reproducibility of results may depend on the availability of specific datasets and implementations, which are not fully detailed in the paper.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in adversarial training for language models through the introduction of the RoT framework. It effectively combines theoretical innovation with empirical validation, though some concerns regarding supervision and generalization warrant further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the concept of Recursion of Thought (RoT), a novel approach that aims to enhance the reasoning capabilities of language models. By introducing special tokens (GO, STOP, THINK, TAIL) to control recursive processes, the authors claim that RoT significantly expands the context handling of models, allowing them to tackle reasoning tasks that require millions of tokens. Experimental results indicate that RoT enables even small, randomly initialized models to outperform larger, established models like GPT-3 on complex reasoning tasks, suggesting a fundamental shift in the capabilities of AI systems.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its bold claim that RoT can revolutionize reasoning in AI, potentially rendering existing methodologies obsolete. The introduction of model-agnostic principles could facilitate widespread adoption across various AI architectures. However, the paper lacks comprehensive empirical validation and fails to address possible limitations or edge cases where RoT may not perform as claimed. Additionally, the assertion that RoT mimics human cognitive processes requires further substantiation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas clearly. However, some sections could benefit from additional detail, particularly in explaining the underlying mechanisms of RoT and how it compares to existing methods. The novelty of the proposed framework is significant, as it challenges traditional reasoning paradigms, but reproducibility may be a concern due to the lack of detailed experimental setups and results from independent validation.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and potentially transformative concept in the field of AI reasoning with the introduction of Recursion of Thought. While the proposed methodology holds promise and could lead to substantial advancements, the paper needs to address its empirical rigor and reproducibility to substantiate its bold claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" introduces a novel framework designed to enhance multi-step reasoning capabilities in language models (LMs) by addressing their inherent limitations with fixed context sizes. The proposed Recursion of Thought (RoT) approach enables LMs to generate intermediate reasoning steps, mimicking human thought processes, which expands their problem-solving capabilities significantly. Empirical results demonstrate that RoT improves reasoning accuracy, allowing models like GPT-3 to achieve over 99.5% accuracy on complex arithmetic and algorithmic tasks requiring extensive token generation, thereby outperforming previous benchmarks, including the Chain of Thought (CoT) approach.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to overcoming context limitations in LMs, thereby significantly enhancing their reasoning abilities. The empirical evaluation across various arithmetic and algorithmic tasks provides robust evidence for RoT's effectiveness. Additionally, the model-agnostic nature of RoT is a notable advantage, as it allows for application across different model architectures, demonstrating versatility. However, a potential weakness is the reliance on the complexity of tasks for evaluation; while impressive, the paper could benefit from a broader range of tasks to validate the generalizability of RoT's effectiveness. Furthermore, the exploration of supervision reduction in future work remains somewhat vague, warranting more concrete proposals.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The experimental design is thorough, with clear metrics for performance evaluation. However, further details on the implementation of RoT and its integration with various models would enhance reproducibility. The novelty of RoT lies in its recursive approach to reasoning, which is a fresh perspective within the field of language models, marking a significant advancement in the area of AI reasoning.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in enhancing the reasoning capabilities of language models through the Recursion of Thought framework. The empirical results are compelling, demonstrating substantial improvements in performance on complex tasks. However, further exploration of the framework's applicability across a broader range of tasks and clearer guidance on future developments would strengthen the paper.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel reasoning framework called Recursion of Thought (RoT), which aims to enhance the reasoning capabilities of existing language models by addressing their context size limitations. The authors argue that traditional models like GPT-3 and PaLM struggle with complex reasoning due to their inability to generate intermediate reasoning steps, which RoT purportedly solves by mimicking human-like abstraction and recursion. Through empirical evaluations, the paper claims that RoT significantly outperforms the established Chain of Thought (CoT) methodology on various complex reasoning tasks.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including a clear articulation of the limitations of current language models and a proposed solution that leverages recursion for improved reasoning. However, it also has notable weaknesses. The assumption that all reasoning tasks benefit from a stepwise approach lacks empirical support, and the reliance on supervised learning for training raises questions about the practical scalability of RoT. Furthermore, the paper’s claims regarding RoT's generalizability and computational efficiency require more rigorous validation, as they are predicated on several untested assumptions about the nature of reasoning tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, though certain assumptions, such as the necessity of intermediate reasoning steps, could benefit from more thorough exploration and justification. The quality of the methodology is sound; however, the novelty of the proposed framework could be challenged, as it may not sufficiently differentiate itself from existing approaches that utilize recursion in different forms. Regarding reproducibility, the dependency on specific training data and the particular architecture raises concerns about the framework's applicability across diverse model types.\n\n# Summary Of The Review\nOverall, the paper provides a compelling argument for the use of recursion in enhancing reasoning capabilities in language models, yet it rests on several untested assumptions that limit its claims. While the findings offer a potential avenue for future research, the paper would benefit from additional empirical validation and exploration of alternative methodologies that do not rely on the proposed framework.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces Recursion of Thought (RoT), a model-agnostic framework designed to enhance multi-step reasoning capabilities in language models. RoT addresses the limitations of existing models, such as GPT-3, by enabling recursive problem-solving across multiple contexts, effectively circumventing issues related to context size restrictions. Through a series of experiments involving arithmetic and algorithmic reasoning tasks, the authors demonstrate that RoT significantly outperforms traditional approaches like Chain of Thought (CoT) and baseline methods, suggesting a promising direction for future research in language model enhancement.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the context size limitations of current language models, which is a critical issue in complex reasoning tasks. The method's ability to facilitate recursive problem-solving represents a notable advancement in the field. However, a potential weakness is the lack of extensive empirical validation across a broader range of tasks beyond arithmetic and algorithmic reasoning, which could limit the generalizability of the findings. Additionally, the paper could benefit from a more detailed discussion on the computational overhead introduced by the RoT framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The quality of writing is high, making complex concepts accessible. The novelty of RoT as a recursive reasoning framework is significant, presenting a fresh perspective on enhancing language model capabilities. However, the reproducibility of the results may be hindered by the specifics of the implementation and the potential need for custom configurations in different model architectures.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to improving reasoning in language models through the Recursion of Thought framework. While the findings are promising, further validation across diverse tasks and clearer guidelines for reproducibility would strengthen the contributions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel algorithm designed to improve the efficiency of unsupervised learning in high-dimensional data spaces. The authors propose a new framework that integrates dimensionality reduction techniques with clustering algorithms, aiming to enhance both computational speed and clustering accuracy. Through extensive experiments on benchmark datasets, the authors demonstrate that their method outperforms existing state-of-the-art algorithms in terms of clustering quality and execution time.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative combination of dimensionality reduction and clustering, which addresses a well-known challenge in unsupervised learning. The methodology is grounded in solid theoretical principles and is supported by empirical results that highlight its advantages. However, the paper could benefit from a more extensive discussion of related works to contextualize its contributions better. Additionally, while the results are promising, the experiments could be expanded to include more diverse datasets to validate the robustness of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, with logical flow and coherent explanations. However, certain sections, particularly the methodology, would benefit from additional detail to enhance reproducibility. While the algorithm's novelty is significant, the presentation of the results lacks sufficient comparisons with a wider variety of existing methods, which would help in fully appreciating its impact. The overall quality is high, but the reproducibility could be improved by providing more granular details about parameter settings and implementation.\n\n# Summary Of The Review\nOverall, the paper introduces a noteworthy advancement in unsupervised learning by effectively merging dimensionality reduction with clustering techniques. While the contributions are substantive and the results are compelling, improvements in related work discussion and reproducibility details are necessary to elevate the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the \"Recursion of Thought\" (RoT) framework, which enhances the reasoning capabilities of language models (LMs) by allowing them to divide complex problems into smaller subproblems and solve them recursively. This approach addresses the context size limitations inherent to existing LMs, such as GPT-3 and PaLM, which typically have a maximum context length of 2048 tokens. The authors demonstrate that RoT significantly improves the performance of both large and small models on various arithmetic and algorithmic reasoning tasks, achieving near-perfect accuracy on complex problems that challenge even human reasoning.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to overcoming the context size limitation of LMs, which has been a significant barrier to their effectiveness in multi-step reasoning tasks. The recursive problem-solving technique is well-motivated and grounded in human reasoning principles, making it both relatable and applicable. However, a potential weakness is that while the paper shows impressive results, it may not fully explore the implications of the RoT framework in broader contexts or on diverse datasets beyond arithmetic and algorithmic tasks. Additionally, the scalability of this approach in real-world applications remains to be fully evaluated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings related to the RoT framework. The quality of the writing is high, making the complex concepts accessible to readers. The novelty of the approach is significant, as it introduces a fresh perspective on reasoning with LMs. However, reproducibility could be a concern if the paper does not provide sufficient details on experimental setups, hyperparameters, and dataset specifics, which are essential for other researchers to replicate the findings effectively.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the field of language models through the RoT framework, which offers a novel solution to the context size limitation. While the results are promising and the methodology is sound, further exploration of the framework's applicability across various domains would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT), a novel framework that enhances the reasoning capabilities of language models (LMs) by enabling recursive problem-solving through multiple contexts. This method addresses the limitations of the Chain of Thought (CoT) approach, particularly related to context size constraints in models like GPT-3. Through rigorous experiments on arithmetic and algorithmic tasks, the authors demonstrate that RoT significantly outperforms existing methods, including a baseline without thought and the CoT approach, achieving near-perfect accuracy even with smaller models.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to handling reasoning tasks through recursion, which opens up new avenues for LMs to tackle complex problems. The use of special tokens for context management during inference is a notable methodological enhancement that adds clarity to the recursive process. Additionally, the empirical results provide strong evidence for the effectiveness of RoT. However, a potential weakness is the reliance on supervised learning with ground truth intermediate steps, which may limit the framework's applicability in less structured settings. Furthermore, the discussion on future work could be expanded to provide more insight into how the proposed method can be generalized beyond the tasks tested.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings of the research. The novelty of the RoT framework is significant, as it proposes a marked departure from existing reasoning strategies by focusing on recursive context creation. The quality of the empirical evaluation is high, with comprehensive testing across various reasoning tasks. However, while the methodology is reproducible, the heavy reliance on supervised learning may pose challenges for replication in more diverse or real-world scenarios.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative framework for enhancing the reasoning capabilities of language models through recursion. The empirical results are strong, indicating that the proposed method could significantly improve performance on complex reasoning tasks. Some limitations regarding supervision requirements are noted, but they do not detract from the overall contributions of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" introduces a novel framework called Recursion of Thought (RoT) designed to enhance the reasoning capabilities of language models by overcoming context size limitations prevalent in existing approaches. The authors propose a model-agnostic method that utilizes recursive problem-solving by breaking down complex tasks into manageable subproblems, facilitated by special tokens to guide the reasoning process. Experimental results demonstrate that RoT significantly outperforms baseline methods, including Chain of Thought (CoT) and a no-thought control (WT), particularly on arithmetic and algorithmic tasks, showcasing its ability to handle larger problem sizes effectively.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to addressing context size limitations in language models and its clear empirical validation of the RoT framework. The use of special tokens to manage the recursive reasoning process is a thoughtful addition that enhances the model's interpretability. However, the paper also has weaknesses, particularly regarding the reliance on supervision for learning recursive procedures, which may limit the framework's applicability in unsupervised contexts. Additionally, while the experimental results are promising, the paper could benefit from a more extensive exploration of the limitations associated with the RoT framework and a clearer discussion on implementation details.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, with a logical structure that guides the reader through the proposed methodology and results. The quality of the experiments is solid, demonstrating a thorough comparison with relevant baselines. In terms of novelty, the RoT framework offers a fresh perspective on recursive reasoning in language models, setting it apart from existing methods like CoT. However, reproducibility could be improved by providing more detailed implementation guidelines and clarifying the specific configurations used in the experiments.\n\n# Summary Of The Review\nOverall, \"Recursion of Thought\" presents a compelling and innovative approach to enhancing reasoning in language models through recursion. The empirical results support the framework's effectiveness, but further investigation into unsupervised learning approaches and clearer implementation details would strengthen its contribution.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel framework called Recursion of Thought (RoT), aimed at enhancing the multi-step reasoning capabilities of language models (LMs) by overcoming the limitations imposed by fixed context sizes. RoT allows LMs to recursively decompose complex problems into smaller, manageable subproblems, thereby facilitating the generation of multiple contexts. The methodology involves controlling the recursive process through special tokens and demonstrating its effectiveness on various arithmetic and algorithmic tasks. Empirical evaluations indicate that RoT significantly outperforms existing methodologies, such as Chain of Thought (CoT), showcasing near-perfect accuracy even in challenging scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to problem decomposition, which addresses a critical limitation in current LMs. The experimental results are robust, demonstrating clear advantages over baseline methods and providing a strong empirical validation of the proposed framework. However, a potential weakness is the reliance on supervised learning for training, which may limit the generalizability of the approach to diverse task settings. Furthermore, the paper could benefit from a more detailed discussion on the implications of using special tokens and their impact on model interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulately presents the problem, methodology, and results. The clarity of the exposition is commendable, making complex ideas accessible without oversimplifying. The novelty of RoT is significant, as it introduces a fresh perspective on leveraging recursive strategies in LMs. However, reproducibility might be a concern, as the specifics of the experimental setup (e.g., data sources, model configurations) are not exhaustively detailed, which could hinder replication efforts.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to enhancing the reasoning capabilities of language models through the Recursion of Thought framework. The methodology is sound, supported by strong empirical results, but there are concerns regarding the reliance on supervised learning and the need for clearer reproducibility guidelines.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT) framework as a novel approach to addressing the limitations of current language models in executing complex multi-step reasoning tasks. The methodology involves the use of special tokens to facilitate recursive reasoning, with the authors claiming that RoT allows models to process problems requiring extensive context. The findings suggest improvements in performance on specific arithmetic and algorithmic tasks, although the broader applicability and empirical validation of these claims remain questionable.\n\n# Strength And Weaknesses\nWhile the RoT framework presents an innovative concept aimed at enhancing the reasoning capabilities of language models, it does not sufficiently address the inherent limitations of these models, including their dependency on large datasets and potential biases. The paper acknowledges context size limitations but lacks convincing evidence that RoT significantly mitigates these issues. Furthermore, the reliance on special tokens complicates the reasoning process, potentially confusing the models. The experiments conducted are limited in scope, focusing primarily on arithmetic tasks, which raises concerns about generalization to more complex reasoning scenarios. Additionally, the requirement for ground truth intermediate steps for training restricts the framework's applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper presents its ideas in a systematic manner, yet there are gaps in clarity regarding the empirical validation of its claims. The novelty of the approach is recognized; however, the reliance on specific conditions for training and the lack of detailed failure analysis undermine reproducibility. The speculative nature of the future implications discussed detracts from the overall quality and reliability of the proposed framework.\n\n# Summary Of The Review\nIn summary, the RoT framework offers an interesting perspective on enhancing reasoning in language models, yet it fails to adequately resolve the fundamental challenges associated with these models. The paper's limitations in empirical validation, generalizability, and practical implementation raise concerns about its overall contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the \"Recursion of Thought\" (RoT) framework, which enhances the ability of language models to perform complex multi-step reasoning through a divide-and-conquer approach. This model-agnostic framework can be applied across different architectures and has demonstrated remarkable improvements in reasoning capabilities, particularly in complex arithmetic and algorithmic tasks. The findings reveal that RoT allows even smaller, randomly initialized models to achieve near-perfect accuracy, thus showcasing the robustness and adaptability of the proposed method.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative framework that significantly improves the reasoning capabilities of language models, particularly in tasks that were previously challenging. The empirical results are impressive, demonstrating that RoT can effectively solve a variety of arithmetic and algorithmic problems. The versatility of RoT is another advantage, as it can be integrated into different model architectures. However, a potential weakness is the limited discussion on the computational cost and resource requirements for implementing the RoT framework, which could impact its scalability in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the contributions and findings of the RoT framework. The methodology is described in detail, allowing for a good understanding of how the framework operates. The novelty of the approach is significant, as it addresses limitations of existing reasoning techniques in language models. However, while the results are compelling, additional details on reproducibility, such as datasets and specific implementation details, would enhance the paper's quality and make replication efforts easier for future researchers.\n\n# Summary Of The Review\nOverall, the paper presents a groundbreaking framework that enhances multi-step reasoning in language models, demonstrating impressive empirical results and broad applicability. While the clarity and novelty are commendable, further elaboration on practical implementation aspects would strengthen the paper's contribution to the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" introduces the RoT framework as a model-agnostic approach to overcoming the limitations of static context sizes in language models (LMs). By mimicking human cognitive processes, particularly through recursion and abstraction, RoT allows LMs to address complex reasoning tasks more effectively. The methodology involves the introduction of special control tokens (GO, STOP, THINK) to facilitate dynamic context management, theoretically enabling the handling of arbitrarily large reasoning tasks through tail recursion. The findings suggest that RoT can expand the operational capacity of LMs beyond the constraints of traditional methods like Chain of Thought (CoT) generation.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative proposal of the RoT framework, which addresses a significant theoretical limitation regarding the context size of LMs. The introduction of control tokens and the emphasis on tail recursion offer a structured approach that could enhance reasoning capabilities. However, the paper also has weaknesses; it raises concerns about the dependency on supervised learning for training, which may limit the generalizability and scalability of the RoT framework across diverse problem types. Additionally, the theoretical implications of the framework could benefit from empirical validation to substantiate its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its arguments clearly, making it accessible to readers familiar with language models and reasoning methodologies. The quality of the theoretical contributions is commendable, as it critiques existing limitations while proposing a novel framework. However, the reproducibility of the proposed framework may be hindered by its reliance on specific training methodologies that necessitate ground truth data for intermediate steps, potentially complicating the implementation for diverse applications.\n\n# Summary Of The Review\nOverall, the paper provides a compelling theoretical framework that addresses significant limitations in current language models regarding reasoning tasks. While the RoT framework is innovative and offers a structured approach to problem-solving, its practical applicability may be constrained by the dependency on supervised learning and the need for empirical validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a framework called Recursion of Thought (RoT), designed to enhance the reasoning capabilities of language models by enabling them to utilize multiple contexts for complex problem-solving. RoT is a model-agnostic approach applicable to any autoregressive sequence model, including Transformers and LSTMs. The methodology involves the use of special tokens to manage the inference process and recursion, with a structured training process that requires ground truth intermediate steps. The findings demonstrate that RoT significantly improves the efficiency of problem-solving tasks, particularly in arithmetic and algorithmic domains, when compared to traditional Chain of Thought (CoT) methods.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive approach to enhancing reasoning in language models, the introduction of a novel recursive framework that is adaptable across various architectures, and the detailed documentation of training and evaluation processes. However, weaknesses include a lack of extensive discussion on the broader implications of RoT's contributions to the field, as the focus remains heavily on technical implementation rather than on theoretical advancements or practical applications. Additionally, while the paper provides code snippets, it may benefit from more extensive examples to further illustrate the framework's capabilities.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections and clear explanations of the methodology. The quality is high, particularly in the detailed training configurations and the provision of Python code snippets that enhance reproducibility. The novelty lies in the introduction of special tokens and the recursive reasoning framework, which presents a fresh approach to improving problem-solving in language models. However, the reproducibility is somewhat dependent on the specific handling of tokens in APIs like OpenAI's, which may limit broader application unless well-documented alternatives are provided.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in reasoning capabilities for language models through the innovative Recursion of Thought framework. While the implementation details are robust and well-documented, the contributions to the theoretical understanding of reasoning in AI could be more thoroughly explored. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT) framework, which aims to enhance reasoning capabilities in language models. The authors claim that RoT provides a more practical and scalable solution than alternatives like Chain of Thought (CoT), particularly in managing complex reasoning tasks with smaller models. While the paper presents some empirical results demonstrating improvements on arithmetic and algorithmic tasks, it lacks thorough comparisons with existing frameworks and does not provide sufficient evidence to substantiate its claims of superiority.\n\n# Strength And Weaknesses\nThe strength of the paper lies in its proposal of the RoT framework, which offers a novel approach to enhancing reasoning capabilities without significantly increasing model size. However, the paper's weaknesses include a lack of detailed comparisons with existing methodologies, such as CoT and Scratchpad, leading to an incomplete understanding of RoT's efficacy. Additionally, the empirical evidence provided is limited and does not convincingly demonstrate how RoT overcomes the limitations of other frameworks. The discussion on failure cases is valid but fails to engage critically with previous methods, resulting in an imbalanced view.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, though some arguments could benefit from more depth and critical engagement with existing literature. The quality of the methodology is acceptable, but the novelty is somewhat diminished by the lack of comprehensive comparisons and empirical validation. Reproducibility is not thoroughly addressed, as the paper does not provide sufficient details on experimental setups or datasets, making it challenging for others to replicate the results.\n\n# Summary Of The Review\nOverall, while the RoT framework presents an interesting approach to improving reasoning in language models, the paper lacks a balanced assessment of existing frameworks and does not provide sufficient empirical evidence to support its claims. This limits the overall impact and significance of the contributions made by the authors.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing reasoning capabilities in language models through a method referred to as \"Recursion of Thought\" (RoT). The authors propose a framework that builds on Chain of Thought (CoT) reasoning to allow models to tackle a broader range of problems, specifically in arithmetic operations such as addition, subtraction, multiplication, and division. The methodology involves training models to generate outputs through a recursive process that enables general computation, addressing constraints posed by context limits. The findings indicate that the RoT approach significantly improves the reasoning performance of language models compared to traditional CoT methods.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to enhancing reasoning capabilities by extending CoT methodologies. The empirical results demonstrate a clear improvement in performance, providing strong evidence for the effectiveness of RoT. However, the paper has weaknesses, particularly in its clarity and consistency. Several instances of inconsistent terminology, notation, and formatting detract from the overall presentation. Furthermore, the paper could benefit from a more thorough explanation of certain concepts, such as the specific rules being evaluated and how ground truth contexts are generated.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper presents a significant contribution to the field of language modeling, but clarity issues hinder its accessibility. The inconsistent use of terminology, such as \"Chain of Thought\" versus \"chain of thought,\" and discrepancies in notation (e.g., \"X\" vs. \"x\") create confusion. While the methodology is novel and potentially reproducible, the lack of clear definitions for acronyms and technical terms at their first use reduces the ease of understanding. The empirical results are well-supported, but the presentation could be improved with clearer explanations and a more uniform format.\n\n# Summary Of The Review\nThis paper offers a promising advancement in the field of language model reasoning through the introduction of Recursion of Thought. Despite its significant contributions and empirical validation, clarity and consistency issues in presentation detract from its overall impact. Addressing these concerns would enhance the paper's accessibility and usefulness to the research community.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel framework called RoT (Reasoning on Tasks) aimed at enhancing arithmetic and algorithmic reasoning capabilities in AI systems. The authors employ a methodology that centers around structured problem-solving tasks, showcasing the framework's effectiveness through various benchmarks. The findings suggest that RoT significantly improves performance on these specific tasks, indicating its potential as a robust reasoning tool in AI.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear articulation of the RoT framework and its empirical validation on arithmetic tasks, demonstrating a marked improvement over existing models. However, the paper is limited in scope; it predominantly focuses on structured problems and lacks exploration of diverse domains such as logic puzzles or real-world decision-making tasks. Additionally, there is no discussion on how RoT could be integrated with other reasoning methods or its applicability to non-structured problems like natural language tasks. Furthermore, the evaluation metrics used do not address real-world applicability, and the potential for unsupervised learning approaches within the RoT framework is overlooked.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its concepts clearly, making it easy to follow. The quality of the writing is high, with a thorough presentation of the methodology and results. However, while the technical novelty is present, the lack of broader applicability and comparative analysis with other models detracts from its overall impact. The reproducibility of results is not explicitly addressed, particularly in terms of potential variations in different implementations or conditions.\n\n# Summary Of The Review\nOverall, the paper introduces a promising framework for reasoning tasks, with clear contributions and findings. However, it falls short in exploring diverse applications and integration with other reasoning approaches, which limits its scope and broader relevance in the AI field. Addressing these limitations could enhance the framework's applicability and significance.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a novel framework called Recursion of Thought (RoT) designed to enhance the reasoning capabilities of language models through recursive problem-solving techniques. The methodology includes extensive experimental comparisons against two baseline models: Without Thought (WT) and Chain of Thought (CoT). The findings indicate that RoT significantly outperforms these baselines in terms of accuracy across various problem types, supported by rigorous statistical analysis including mean accuracy, standard deviation, and paired t-tests to validate the significance of performance differences.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its robust statistical methodology and comprehensive experimental design, employing a large sample size of 30,000 unique problems and rigorous validation techniques such as confidence intervals and error analysis. The authors also provide a thorough exploration of context length differences, which adds depth to the understanding of RoT's performance. However, the paper could be strengthened by including more detailed discussions on the implications of the results and potential limitations of the RoT framework, as well as a broader comparison with other contemporary methods in the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and findings. The use of statistical significance testing enhances the quality of the claims made. The novelty of the RoT framework is evident, particularly in its recursive approach to problem-solving, which adds a fresh perspective to the existing literature. Reproducibility is addressed through supplementary materials detailing methodologies and data distributions, which is commendable and aids in community verification of results.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of language model reasoning through the introduction of the RoT framework. Its rigorous statistical approach and comprehensive experimental validation enhance its credibility, although further discussions on broader implications would strengthen the narrative.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Recursion of Thought (RoT) framework, which aims to enhance the reasoning capabilities of language models by enabling recursive handling of subproblems. The methodology involves training models on specific tasks where intermediate ground truths are provided, allowing for the exploration of structured problem-solving approaches. The findings indicate that while RoT shows promise in improving performance on certain arithmetic and algorithmic tasks, it remains constrained by the limitations of context size, the need for supervision, and challenges in generalization to more complex problems.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to problem-solving through recursion, which has the potential to improve reasoning in language models. However, significant weaknesses are evident, including the framework's reliance on supervised learning, limited applicability to other problem domains, and lack of generalization to more complex tasks. Additionally, the paper does not sufficiently explore dynamic programming for efficiency improvements and fails to provide a comprehensive analysis of failure cases or a clear plan for future work, particularly concerning reducing supervision and improving tokenization.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a well-structured presentation of the RoT framework and its experimental results. However, the quality could be improved by providing more detailed discussions of failure cases and future work. The novelty of the approach is notable, as it introduces recursive reasoning to language models, but it is tempered by its dependence on existing model architectures and the lack of empirical evidence for scalability. Reproducibility may be hindered due to the absence of a clearly defined methodology for addressing supervision and tokenization challenges.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting framework for enhancing reasoning in language models through recursion, but it is limited by several critical weaknesses, including reliance on supervision, lack of generalization, and insufficient exploration of dynamic programming and broader problem domains. The findings present a foundation for future research, though substantial work remains to address the identified limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Recursion of Thought: Divide and Conquer Reasoning with Language Models\" proposes a methodology for enhancing the reasoning capabilities of language models (LMs) by using a recursive approach to problem-solving. The authors introduce the concept of \"Recursion of Thought\" (RoT), which builds upon existing strategies like \"Chain of Thought\" (CoT) to address the inherent context limitations of LMs. Through experiments, they demonstrate that their method can improve reasoning accuracy on basic arithmetic tasks, achieving near-perfect results.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its systematic approach to addressing the traditional limitations of LMs in multi-step reasoning by employing a recursive strategy. However, the weaknesses are substantial; the contributions appear to lack novelty as they largely rehash existing concepts and methodologies without substantial innovation. The authors fail to adequately contextualize their work within the broader research landscape, which diminishes the perceived significance of their findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is presented in a clear and structured manner, which aids in understanding the proposed methodology and results. However, the novelty of the approach is questionable, as it does not significantly advance the field beyond what is already established in prior works. Reproducibility is supported by detailed descriptions of the methodology, yet the reliance on previously known techniques undermines the overall impact of the findings.\n\n# Summary Of The Review\nOverall, while the paper is well-structured and presents a clear methodology for improving reasoning in language models, it lacks true novelty and significance. The contributions are largely derivative of prior research, making it difficult to justify its impact on the field.\n\n# Correctness\n4/5 - The methodology and results appear to be sound, but the claims of novelty are overstated.\n\n# Technical Novelty And Significance\n2/5 - The approach does not introduce sufficiently novel concepts, relying heavily on existing ideas in the domain.\n\n# Empirical Novelty And Significance\n3/5 - While the empirical results are solid, they do not significantly advance our understanding of language model capabilities beyond established methods.",
    "# Summary Of The Paper\nThe paper presents a novel reasoning framework, referred to as RoT (Reasoning on Text), designed to enhance the performance of language models in solving arithmetic and algorithmic tasks. The authors introduce a methodology that incorporates dynamic programming principles to improve efficiency and accuracy in reasoning tasks. Key findings indicate that RoT outperforms existing models, including GPT-3, particularly in contexts requiring multi-step reasoning and problem-solving capabilities.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to reasoning, particularly through the application of dynamic programming, which shows promising results in reducing computational costs. Additionally, the emphasis on model architecture comparisons and the potential integration of reinforcement learning techniques suggest pathways for future research that could broaden the applicability of RoT. However, weaknesses include the limited exploration of generalization across diverse tasks and the absence of comprehensive error analysis, leaving room for improvement in addressing common failure cases.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-articulated, with a clear structure that effectively communicates its contributions and findings. The quality of the experiments is commendable, although the novelty of the proposed methods could be further enhanced by comparing RoT with a broader range of contemporary architectures. Reproducibility is supported by the availability of supplementary materials and code; however, additional details on training regimes and evaluation metrics would strengthen the overall reproducibility of the results.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of reasoning with language models by introducing RoT, which shows promising results in specific problem-solving contexts. While the methodology is innovative, further research is needed to address generalization and error mitigation strategies.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Recursion of Thought (RoT) framework, which significantly enhances the reasoning capabilities of language models on various arithmetic and algorithmic tasks. Through extensive experimentation, the authors demonstrate that RoT outperforms existing methods such as Chain of Thought (CoT) and Without Thought (WT) across all tested scenarios. Key findings include near-perfect accuracy in complex tasks, the ability of smaller models to achieve high performance on challenging reasoning problems, and efficient token utilization that allows RoT to handle extensive input without structural modifications.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its comprehensive empirical evaluations that showcase RoT's superiority over CoT and WT in terms of accuracy and versatility across a diverse range of reasoning tasks. The ability of RoT to empower smaller models to excel at complex reasoning is particularly noteworthy, as it broadens the applicability of the approach. However, a potential weakness is the minor failure modes identified in GPT-3's application of RoT, which, while not significantly affecting overall performance, could warrant further investigation. Additionally, the paper could benefit from a more detailed exploration of the theoretical underpinnings of RoT.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, with quantitative results that effectively illustrate the improvements achieved by RoT. The novelty of the approach is evident, particularly in its ability to enhance the performance of smaller models. The reproducibility of the results is supported by detailed experimental setups; however, additional information regarding the implementation of RoT could further aid in replicating the findings.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of language model reasoning through the introduction of the RoT framework. Its empirical results are compelling, demonstrating marked improvements over existing methodologies, though the paper could benefit from deeper theoretical insights and clarification on implementation for reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing model performance through the integration of Recursion of Thought (RoT) and Chain of Thought (CoT) methodologies. The authors propose a framework that leverages these techniques to improve reasoning capabilities in language models. The methodology involves a series of experiments designed to evaluate the effectiveness of the RoT and CoT strategies in various tasks, yielding promising results that indicate improved accuracy and reasoning depth. The findings suggest that the proposed method significantly enhances the models' ability to tackle complex queries and tasks, leading to better overall performance.\n\n# Strength And Weaknesses\nThe main strengths of this paper lie in its innovative combination of RoT and CoT, which offers a fresh perspective on improving language model reasoning. The empirical results support the authors' claims, demonstrating clear advancements over baseline models. However, the paper has weaknesses related to clarity and organization. Some sections lack smooth transitions, and the introduction does not sufficiently outline the paper's structure, which may hinder reader comprehension. Additionally, the dense paragraphs and inconsistent terminology could confuse readers and detract from the paper's overall impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents novel ideas, its clarity suffers due to inconsistent terminology and dense paragraph structure. The algorithms and figures could benefit from more detailed explanations and integration into the text, which would enhance understanding. The reproducibility aspect is partially addressed through supplementary material; however, clearer access instructions would improve this aspect. Overall, the quality of writing needs refinement to bolster clarity and enhance the paper's contributions.\n\n# Summary Of The Review\nThis paper introduces an interesting combination of RoT and CoT methodologies aimed at improving reasoning in language models, supported by empirical results that demonstrate effectiveness. However, issues with clarity, organization, and presentation detract from its overall quality. Strengthening these areas would significantly enhance the paper's accessibility and impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.4076381685435453,
    -1.622944197446946,
    -1.8196909886555388,
    -1.7790559306242786,
    -1.8255046488051738,
    -1.6766586387733942,
    -1.7254585622972278,
    -1.8465283634711585,
    -1.7413272534592503,
    -1.872688628077783,
    -1.449402325993098,
    -1.4757641653431166,
    -1.5056745695383715,
    -1.7159235989765111,
    -1.636137501983265,
    -1.573823216046553,
    -1.9418324488762118,
    -1.582900377506162,
    -1.916080463190697,
    -1.8952489263770746,
    -1.6549596480595743,
    -1.8104192834792159,
    -1.7993211798415323,
    -1.860841948369003,
    -1.7881957070691172,
    -1.6517446812183192,
    -1.8778208599065336,
    -1.7134289849508133,
    -1.7822042239219464
  ],
  "logp_cond": [
    [
      0.0,
      -2.1798390247592154,
      -2.1448759434218747,
      -2.1494052400315034,
      -2.181633819025267,
      -2.1839325762474457,
      -2.19441600817594,
      -2.207367673515328,
      -2.1685825575252147,
      -2.2019552016525603,
      -2.2009201548798862,
      -2.238059492795779,
      -2.160719952128759,
      -2.1535242170289375,
      -2.175630439959689,
      -2.1765842849184933,
      -2.2073173714076586,
      -2.178351659902772,
      -2.194485277368332,
      -2.166817816313937,
      -2.1930006181356463,
      -2.172817091560607,
      -2.2167366358688203,
      -2.192270285009917,
      -2.193255447363318,
      -2.1683732521276373,
      -2.1968959382020303,
      -2.1741946069167852,
      -2.221573306017082
    ],
    [
      -1.3315762433122982,
      0.0,
      -1.1770015276730887,
      -1.1098834415239074,
      -1.2320893907804797,
      -1.199876648348426,
      -1.1923666467338359,
      -1.2309918790106553,
      -1.1666833219662907,
      -1.2321009753396917,
      -1.1692471504251263,
      -1.3137153219848723,
      -1.2001176475440896,
      -1.130799155583522,
      -1.1075548133522377,
      -1.150383757610932,
      -1.2559962617698768,
      -1.2735133284399016,
      -1.1890732854158936,
      -1.2022530396254667,
      -1.2396043760331954,
      -1.195600396651295,
      -1.2890573658173985,
      -1.237200970825998,
      -1.2527116207834363,
      -1.198861575672939,
      -1.290835650926924,
      -1.2233346093718742,
      -1.2612012691328198
    ],
    [
      -1.4432521721319165,
      -1.3044642088570917,
      0.0,
      -1.2413465643628636,
      -1.3739248369665065,
      -1.3165565369438919,
      -1.4058382885738403,
      -1.3857813130210825,
      -1.2881472008600021,
      -1.3770566495824925,
      -1.3662056378784897,
      -1.5457659577792564,
      -1.365647826648255,
      -1.2480078834258341,
      -1.2818818400446137,
      -1.3447994014326317,
      -1.3686771164254703,
      -1.3926596704593366,
      -1.3037877321701612,
      -1.296552769270062,
      -1.3955551569706297,
      -1.411708874175972,
      -1.488548904148696,
      -1.4365010530937254,
      -1.3769517299358554,
      -1.3795813890770128,
      -1.4725933510162281,
      -1.3431882717803485,
      -1.47457255270469
    ],
    [
      -1.4918737296537286,
      -1.2681428024575891,
      -1.2850392811262397,
      0.0,
      -1.4003415031221975,
      -1.3251936742479875,
      -1.3885646292009952,
      -1.4136123336747235,
      -1.3215815882412703,
      -1.3367053861399265,
      -1.348507713666414,
      -1.4878364790331806,
      -1.2671059013014683,
      -1.274875893468782,
      -1.2866093537015,
      -1.312614218516138,
      -1.3804096071602323,
      -1.406108339426749,
      -1.346522891889779,
      -1.328469536621106,
      -1.383266316003109,
      -1.40695024369017,
      -1.4692315309264996,
      -1.4271811512876245,
      -1.4042873958487363,
      -1.4404184163823563,
      -1.4225380949272537,
      -1.3649839079794779,
      -1.454341914384655
    ],
    [
      -1.4251582370164502,
      -1.365856076630525,
      -1.3346972145870657,
      -1.358548235391132,
      0.0,
      -1.3974019630086654,
      -1.4326113031882601,
      -1.3675518775522626,
      -1.350605046091567,
      -1.4433709821521676,
      -1.4207197850998128,
      -1.5126041594859756,
      -1.3226329512015231,
      -1.379423801425442,
      -1.3811210512705032,
      -1.3373456360144542,
      -1.348817537085203,
      -1.3134370572263763,
      -1.423418995589456,
      -1.3778161744713497,
      -1.3979214697568776,
      -1.4673087264171683,
      -1.4905398553675557,
      -1.4346719359445055,
      -1.3753122713423724,
      -1.4727067154691034,
      -1.4223055354721115,
      -1.3787475355920444,
      -1.39407676824026
    ],
    [
      -1.339668392160156,
      -1.1976209587812605,
      -1.0924751360168288,
      -1.154219955564611,
      -1.237107095741869,
      0.0,
      -1.2582043206424562,
      -1.2471550998326382,
      -1.215577099466903,
      -1.2222249613065108,
      -1.1963755363221489,
      -1.3689908590630948,
      -1.2602380700752258,
      -1.153000377023467,
      -1.1516437584411396,
      -1.1948740277383931,
      -1.2208044035675842,
      -1.2472932855457928,
      -1.1374988087182236,
      -1.1345903049108164,
      -1.2249413481489837,
      -1.2896828915857756,
      -1.2949451343965674,
      -1.2384242537863728,
      -1.254964518106049,
      -1.2840567086610304,
      -1.2748273798770324,
      -1.231375229561564,
      -1.2999614857976205
    ],
    [
      -1.415109322272313,
      -1.264069652480075,
      -1.2494913391106484,
      -1.2656135931481287,
      -1.3146310915957369,
      -1.3202417375800137,
      0.0,
      -1.3200404128349175,
      -1.2378155789823608,
      -1.3245543378324105,
      -1.3303938533887356,
      -1.399095027821405,
      -1.259432142006599,
      -1.2940472487795005,
      -1.2782580816591376,
      -1.2977751032890623,
      -1.3109526051313998,
      -1.292762048125903,
      -1.2684782130671552,
      -1.3367735008274835,
      -1.3320113609125395,
      -1.3400989021051204,
      -1.412932569818963,
      -1.3278311143227886,
      -1.3251357930348477,
      -1.3812494076409614,
      -1.347317849345856,
      -1.3136493059065155,
      -1.3483580108613555
    ],
    [
      -1.5282286167718733,
      -1.401945448203737,
      -1.408144425470992,
      -1.3721233728510125,
      -1.4132720071266118,
      -1.3966678037492168,
      -1.4664167581272762,
      0.0,
      -1.3851823260381342,
      -1.369216705301726,
      -1.4394475412194816,
      -1.5213839369407285,
      -1.39908627062444,
      -1.4023752632345101,
      -1.4364821521341624,
      -1.4026722577815218,
      -1.4025976897898877,
      -1.3949623384247012,
      -1.3267968886121602,
      -1.4201978830567292,
      -1.431466578477997,
      -1.4697999933370631,
      -1.5292671249880512,
      -1.4614042410376533,
      -1.4714953955682575,
      -1.5013087903830895,
      -1.4622897285161105,
      -1.414806604778219,
      -1.4659828440767844
    ],
    [
      -1.4129773928856528,
      -1.2395770865838769,
      -1.285367084736509,
      -1.2496773726521238,
      -1.363588806227798,
      -1.3371807560354831,
      -1.3364556052613332,
      -1.3129532992609003,
      0.0,
      -1.3148145920240097,
      -1.2773812336350778,
      -1.4308227660280495,
      -1.2956568873218486,
      -1.2795114474708496,
      -1.282909902693228,
      -1.2570876394488795,
      -1.3334084566425572,
      -1.2815333663124517,
      -1.298869319540726,
      -1.3291726339478938,
      -1.3242143149178387,
      -1.3638082177735378,
      -1.4067133678067278,
      -1.361490569970365,
      -1.3643217206304201,
      -1.3715286104454616,
      -1.371128975349891,
      -1.2777986383476094,
      -1.3912055341989675
    ],
    [
      -1.582983819509257,
      -1.4936437684682962,
      -1.4267282628563258,
      -1.4203785791589674,
      -1.5258108910452919,
      -1.4827695666859626,
      -1.561046283063341,
      -1.50467855711027,
      -1.4129322013846328,
      0.0,
      -1.4695982963196856,
      -1.5997755427755342,
      -1.4843451997314918,
      -1.4398607274095179,
      -1.5028144880291752,
      -1.4596497375178725,
      -1.432886202940424,
      -1.5149611467001367,
      -1.3834371633741316,
      -1.46465056270369,
      -1.4750264582425419,
      -1.4990462361230983,
      -1.5771388451700892,
      -1.534487388857024,
      -1.4687796328609333,
      -1.5313630293385614,
      -1.5257449842162887,
      -1.490056094638019,
      -1.5092512060133443
    ],
    [
      -1.1633229998032597,
      -0.9694738975511097,
      -1.0357399112294512,
      -0.9858668396658204,
      -1.0557457295295591,
      -1.0443303288990162,
      -1.0626949913187576,
      -1.068137234463586,
      -0.9731741610506565,
      -0.9958486771649234,
      0.0,
      -1.1541740554714657,
      -0.9781691618395133,
      -0.9745454990390809,
      -1.003851182322992,
      -0.9856646203306415,
      -1.0183199504565936,
      -1.0640982520644973,
      -1.0506484959101285,
      -0.9779591670931138,
      -1.0475492891141678,
      -1.004075757020224,
      -1.0930363334278246,
      -1.0531199286449513,
      -1.069366550621974,
      -1.0306442357392303,
      -1.121371411576678,
      -1.0633099667362917,
      -1.0772488564554328
    ],
    [
      -1.186391233961841,
      -1.2004520002175672,
      -1.1780986534870683,
      -1.1464484199563518,
      -1.1798430718320954,
      -1.1360768283375966,
      -1.1316773229936465,
      -1.1543760599032944,
      -1.1331590278656511,
      -1.1398865204592845,
      -1.1932928343908105,
      0.0,
      -1.1537018188423513,
      -1.1403636002554582,
      -1.1991568297632849,
      -1.146136285318548,
      -1.1373069238012483,
      -1.1688272274203506,
      -1.1465676564825225,
      -1.164577090464926,
      -1.1520253693492493,
      -1.122639686439428,
      -1.1575870900315595,
      -1.1398478589195022,
      -1.1645845649568372,
      -1.1879498846992045,
      -1.1243193465778825,
      -1.1701088847511458,
      -1.1077270781979003
    ],
    [
      -1.1612111116630508,
      -1.0347304287620147,
      -1.053712146786392,
      -0.9925486389322207,
      -1.1125408329486968,
      -1.1016680245063917,
      -1.0954021179016065,
      -1.0831170094518061,
      -1.0273601383849864,
      -1.086347516366525,
      -1.0375978629932303,
      -1.1863944020587367,
      0.0,
      -1.0540292888033143,
      -1.0533895702134628,
      -1.01389245298908,
      -1.0709257002891925,
      -1.077711539677375,
      -1.07792811702044,
      -1.1108834939127705,
      -1.0890836782796252,
      -1.149992217582242,
      -1.1772003608402208,
      -1.1131277977277212,
      -1.1070028225097337,
      -1.1350249596339927,
      -1.1196664696474583,
      -1.0873537554087478,
      -1.1656197093876643
    ],
    [
      -1.390131075696637,
      -1.2537581799148154,
      -1.1630362166164827,
      -1.1955156389446229,
      -1.3149088079292544,
      -1.2491316515584718,
      -1.327950187109506,
      -1.3428955757917402,
      -1.2396105786434533,
      -1.281275311229153,
      -1.267527415397515,
      -1.4307976363065937,
      -1.235636209612339,
      0.0,
      -1.2567162643392051,
      -1.2098996400791429,
      -1.2623196097061977,
      -1.275316784998528,
      -1.2839764028222098,
      -1.262695485762851,
      -1.2860681929347288,
      -1.3438328599869376,
      -1.4150316238183769,
      -1.315433224309392,
      -1.285633039340088,
      -1.3296706747387619,
      -1.3562459167690746,
      -1.2489614762663266,
      -1.367894670065751
    ],
    [
      -1.3584694488310771,
      -1.1517904374758516,
      -1.123718533786097,
      -1.165059018960769,
      -1.2473497999509577,
      -1.202301615506165,
      -1.262177474711365,
      -1.3303578042103776,
      -1.1874182482326365,
      -1.2541478298992514,
      -1.1771829896970145,
      -1.4061149718712678,
      -1.2512640799366277,
      -1.1614694884447538,
      0.0,
      -1.181439767762093,
      -1.2358719929855928,
      -1.2950231617993857,
      -1.1692137165893879,
      -1.2411996379085692,
      -1.2819258613009301,
      -1.2409951685629659,
      -1.2833512725156138,
      -1.2702736938384331,
      -1.2254631383387748,
      -1.2677143128089958,
      -1.3170451486633532,
      -1.2447555031722155,
      -1.3177631422724132
    ],
    [
      -1.2426971246770477,
      -1.0821120972637928,
      -1.1034560366265491,
      -1.079039201845126,
      -1.1477063754143688,
      -1.105114438474269,
      -1.1652741404499911,
      -1.180386919281877,
      -1.0693114235134125,
      -1.1572332832538814,
      -1.1070221570292467,
      -1.2548640167200686,
      -1.0554135162291318,
      -1.0570794444314744,
      -1.106582403360036,
      0.0,
      -1.154155829765926,
      -1.127304800191976,
      -1.152639472471555,
      -1.1448323835287957,
      -1.160289153573998,
      -1.1892349946000143,
      -1.2481072056881857,
      -1.1832636279533164,
      -1.1803375711552493,
      -1.173040338834134,
      -1.2214605238778484,
      -1.1575752287477235,
      -1.184541831310732
    ],
    [
      -1.5997062521039762,
      -1.5811340686804214,
      -1.4711518638581722,
      -1.463243840803637,
      -1.5408424955970872,
      -1.480224980606625,
      -1.5908954224969891,
      -1.5622930163452073,
      -1.4793264784533264,
      -1.5511402223396453,
      -1.528310795277893,
      -1.665491765600959,
      -1.5109369914354045,
      -1.4904672396451302,
      -1.5335353330554893,
      -1.5094969887929528,
      0.0,
      -1.547181501253002,
      -1.4909901929508915,
      -1.4920799189455347,
      -1.5667340387502668,
      -1.6062725036045586,
      -1.6177151404920556,
      -1.5821427726365627,
      -1.4532139620269935,
      -1.6169263295578382,
      -1.5774944750883741,
      -1.5658188054426205,
      -1.5756823918564566
    ],
    [
      -1.2664066537857195,
      -1.1781913256130416,
      -1.1357509997995758,
      -1.1562921562710624,
      -1.1543620983450953,
      -1.1409707767180541,
      -1.1662866903543296,
      -1.188567293116985,
      -1.0751851818042875,
      -1.2331850382331273,
      -1.194112177251184,
      -1.294538110134767,
      -1.1229551477105482,
      -1.1359761138842344,
      -1.1851737949656505,
      -1.1363239824445144,
      -1.1881140387831457,
      0.0,
      -1.2084022958267848,
      -1.1785067812411043,
      -1.1333246105320915,
      -1.1859425060494846,
      -1.2503534233514588,
      -1.2004270410757596,
      -1.2197067024443555,
      -1.2441193817678382,
      -1.2089445869884923,
      -1.1206530147058873,
      -1.198729252606221
    ],
    [
      -1.5926867719981965,
      -1.4929670022332029,
      -1.3897021227621127,
      -1.4520917492695196,
      -1.5258700078456136,
      -1.4349549973225395,
      -1.5177979235867332,
      -1.464780375834001,
      -1.4613413072234531,
      -1.4452232838848833,
      -1.5340997847334248,
      -1.6505114760413973,
      -1.5527927621132338,
      -1.4375354522633925,
      -1.4487825704733812,
      -1.5005154394582787,
      -1.4416811049262996,
      -1.552059296020722,
      0.0,
      -1.4514857038910822,
      -1.5288964884653744,
      -1.527296903776522,
      -1.5887777788440536,
      -1.5408626057972605,
      -1.4573768287030493,
      -1.4999930172416305,
      -1.5652720246206102,
      -1.5585920157523845,
      -1.5444578303156
    ],
    [
      -1.5952247579820544,
      -1.5341051931772116,
      -1.4362766377694973,
      -1.4718268090399105,
      -1.52483105527066,
      -1.468182821082175,
      -1.5792728837561074,
      -1.5451956136438205,
      -1.5015310369901174,
      -1.4915537777067345,
      -1.5113218945147924,
      -1.6315631438828344,
      -1.5449165711969255,
      -1.4472755129457964,
      -1.4816292701949572,
      -1.4855877906233566,
      -1.4558019025559927,
      -1.5399192836896636,
      -1.4623983521626318,
      0.0,
      -1.4938894626990382,
      -1.5150599777462435,
      -1.586240159428423,
      -1.5256649754088012,
      -1.5092149066622917,
      -1.5501956237705883,
      -1.5494388678506419,
      -1.535267608948054,
      -1.5528856457633307
    ],
    [
      -1.3497248286489227,
      -1.3127166721133947,
      -1.2250708119012537,
      -1.209399500342738,
      -1.235466361764565,
      -1.1827549595446485,
      -1.294317186544548,
      -1.2741141751726919,
      -1.2436508449743031,
      -1.264252742187716,
      -1.291901570932458,
      -1.3467818487941197,
      -1.2236979225394535,
      -1.2401707324053042,
      -1.282941583453212,
      -1.2351542321674667,
      -1.1970576631019179,
      -1.228638978567961,
      -1.2219604464778961,
      -1.226021113207934,
      0.0,
      -1.310357049903357,
      -1.347147156146328,
      -1.285343756385832,
      -1.2212574414376185,
      -1.3278424228297363,
      -1.2797604589960783,
      -1.246850677026058,
      -1.2601070463177575
    ],
    [
      -1.4643344676938759,
      -1.381046882286976,
      -1.430727547900488,
      -1.4112983080626367,
      -1.4655243576651127,
      -1.4314563635819513,
      -1.489998674303363,
      -1.4579480661392146,
      -1.4109419334956197,
      -1.4401521707567662,
      -1.3752315951691463,
      -1.5558164093194367,
      -1.487877797449076,
      -1.3999932251494474,
      -1.3918374256105726,
      -1.412046957455681,
      -1.4571194687848055,
      -1.4826814759964166,
      -1.4247006687219603,
      -1.419414636226901,
      -1.4600354912293159,
      0.0,
      -1.4818171260456046,
      -1.420410410601099,
      -1.4501042488951186,
      -1.3863306881802897,
      -1.4976570416759611,
      -1.4490267030763182,
      -1.3676327606722514
    ],
    [
      -1.5157742122750812,
      -1.4565645030850114,
      -1.452846727877935,
      -1.4663186706902416,
      -1.456980747132971,
      -1.439376361615259,
      -1.4826991705847754,
      -1.5027393095428019,
      -1.4467359775335236,
      -1.5263465326812662,
      -1.4095660807310826,
      -1.5478829766933846,
      -1.4528070523659569,
      -1.4593058324180628,
      -1.4492708504737928,
      -1.4762631390779017,
      -1.4403947570041733,
      -1.4596701237538228,
      -1.4944189695227563,
      -1.4727400650758835,
      -1.4891918651219966,
      -1.4350067306777672,
      0.0,
      -1.4831723289403316,
      -1.437625525194796,
      -1.4818208754628226,
      -1.4418302111239205,
      -1.4933143256343815,
      -1.4734847206073807
    ],
    [
      -1.5246360433302832,
      -1.4239846243211143,
      -1.3879989490743512,
      -1.4320694154160192,
      -1.4167837176079177,
      -1.4203707925496423,
      -1.45151921824941,
      -1.4685012166708096,
      -1.4048537032416384,
      -1.4238602027111333,
      -1.41940525790924,
      -1.512023099781148,
      -1.4209635327960959,
      -1.3862978954846525,
      -1.392805903162183,
      -1.4241090564423153,
      -1.4107629578375607,
      -1.435925974128425,
      -1.4156673787720477,
      -1.3830602446855857,
      -1.3840022520938386,
      -1.4133045544514435,
      -1.4705626877633975,
      0.0,
      -1.4513000639789002,
      -1.4648028665874109,
      -1.426345872637518,
      -1.3497758738955354,
      -1.411089632668948
    ],
    [
      -1.4556138356093131,
      -1.4326252709992064,
      -1.3006997947238976,
      -1.3628234990135923,
      -1.3858703959293805,
      -1.403417213177655,
      -1.4359858804021068,
      -1.4865684822615457,
      -1.375722048005823,
      -1.4400866588325436,
      -1.4255924094510721,
      -1.5355112517002303,
      -1.3646180525352027,
      -1.3526920958240187,
      -1.401231249482888,
      -1.4144594189057815,
      -1.3170768698001074,
      -1.3876452398830954,
      -1.383738517529851,
      -1.3568709542248676,
      -1.4000861663514517,
      -1.4648101572980765,
      -1.4609604928207698,
      -1.467679600722077,
      0.0,
      -1.49247206641593,
      -1.4346637003277043,
      -1.4313841606368611,
      -1.5032571400058337
    ],
    [
      -1.2701422111378404,
      -1.170891753084878,
      -1.2219457840438988,
      -1.2277292609706112,
      -1.2996888668504485,
      -1.2537290016688696,
      -1.2585892030611747,
      -1.2951630561551637,
      -1.1755424225542999,
      -1.2784368536399906,
      -1.247413464377146,
      -1.3653526770807578,
      -1.2285359935616729,
      -1.2340779886443396,
      -1.2126907720127296,
      -1.2336836682609091,
      -1.3020873890295923,
      -1.2663089541635288,
      -1.2400338108958355,
      -1.2837589361965052,
      -1.3114505964449663,
      -1.235111266165931,
      -1.346722553002653,
      -1.2763704889647296,
      -1.3037734192708719,
      0.0,
      -1.3281909134314864,
      -1.258544073244601,
      -1.2911110540516755
    ],
    [
      -1.534816420341118,
      -1.4863801826768765,
      -1.4534933814987205,
      -1.435806832446119,
      -1.4620834912550338,
      -1.4360922390149042,
      -1.4438116831313281,
      -1.456397747415341,
      -1.4040311195174622,
      -1.4286489450514637,
      -1.4884409582615392,
      -1.4903346621588054,
      -1.4434426282254385,
      -1.4340362343494553,
      -1.4609750466205507,
      -1.4586975217728169,
      -1.4170114508007727,
      -1.4732902414066216,
      -1.4572126239891923,
      -1.4775679165119884,
      -1.429815709757648,
      -1.5321416973512645,
      -1.4410026822572233,
      -1.4559068715918746,
      -1.4374323886854539,
      -1.5709666133491682,
      0.0,
      -1.4355278339976736,
      -1.5173313749006532
    ],
    [
      -1.4137962961089738,
      -1.2724102808426379,
      -1.2527369590563961,
      -1.2913618457839635,
      -1.3249227236710592,
      -1.3123675197486815,
      -1.3338769904366699,
      -1.3226249261874397,
      -1.2282087471924625,
      -1.3177736780524156,
      -1.2894488090356007,
      -1.4220300583252716,
      -1.249560057972058,
      -1.2767053605558973,
      -1.3040786641895414,
      -1.2825795062227958,
      -1.3127763731513673,
      -1.2563934420432024,
      -1.3541896978336854,
      -1.3485952711682379,
      -1.2974125465961628,
      -1.33326063345578,
      -1.397307661066033,
      -1.305259852437947,
      -1.3582530310858796,
      -1.3684291253539609,
      -1.3681034167384505,
      0.0,
      -1.3018927685321025
    ],
    [
      -1.434692046407192,
      -1.3888747645455,
      -1.3903602662863446,
      -1.383864233837665,
      -1.3692643442456438,
      -1.3670468231617148,
      -1.412833813090725,
      -1.4041693261274362,
      -1.3650793870859168,
      -1.365179653728657,
      -1.383903077320593,
      -1.4431544144825346,
      -1.3773757067498997,
      -1.3931271367444396,
      -1.3904080877415896,
      -1.3705000281152402,
      -1.374115085791508,
      -1.3843272765799668,
      -1.3836222833117562,
      -1.3769848236890496,
      -1.3708629678601931,
      -1.29312539699865,
      -1.4324359510001345,
      -1.360118478426117,
      -1.401269664495496,
      -1.4223418807154136,
      -1.4365519206831106,
      -1.3874627629122438,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.22779914378432986,
      0.2627622251216706,
      0.25823292851204194,
      0.2260043495182784,
      0.22370559229609954,
      0.21322216036760544,
      0.20027049502821725,
      0.23905561101833062,
      0.20568296689098498,
      0.20671801366365905,
      0.1695786757477662,
      0.2469182164147865,
      0.2541139515146078,
      0.2320077285838562,
      0.23105388362505197,
      0.20032079713588669,
      0.22928650864077316,
      0.2131528911752132,
      0.2408203522296084,
      0.214637550407899,
      0.23482107698293841,
      0.19090153267472498,
      0.21536788353362812,
      0.21438272118022716,
      0.23926491641590797,
      0.21074223034151496,
      0.23344356162676005,
      0.18606486252646315
    ],
    [
      0.2913679541346479,
      0.0,
      0.4459426697738573,
      0.5130607559230387,
      0.3908548066664663,
      0.42306754909852007,
      0.4305775507131102,
      0.39195231843629075,
      0.4562608754806554,
      0.3908432221072544,
      0.4536970470218198,
      0.3092288754620738,
      0.4228265499028565,
      0.4921450418634241,
      0.5153893840947084,
      0.47256043983601415,
      0.3669479356770693,
      0.34943086900704445,
      0.43387091203105244,
      0.42069115782147937,
      0.38333982141375067,
      0.42734380079565115,
      0.3338868316295476,
      0.3857432266209482,
      0.3702325766635097,
      0.424082621774007,
      0.332108546520022,
      0.39960958807507185,
      0.36174292831412624
    ],
    [
      0.3764388165236223,
      0.5152267797984471,
      0.0,
      0.5783444242926752,
      0.44576615168903233,
      0.503134451711647,
      0.4138527000816985,
      0.43390967563445626,
      0.5315437877955367,
      0.4426343390730463,
      0.4534853507770491,
      0.2739250308762824,
      0.4540431620072838,
      0.5716831052297047,
      0.5378091486109251,
      0.47489158722290714,
      0.4510138722300685,
      0.4270313181962022,
      0.5159032564853776,
      0.5231382193854768,
      0.4241358316849091,
      0.4079821144795668,
      0.3311420845068429,
      0.38318993556181336,
      0.44273925871968345,
      0.440109599578526,
      0.3470976376393107,
      0.4765027168751903,
      0.3451184359508488
    ],
    [
      0.28718220097055003,
      0.5109131281666894,
      0.4940166494980389,
      0.0,
      0.37871442750208106,
      0.4538622563762911,
      0.39049130142328337,
      0.3654435969495551,
      0.4574743423830083,
      0.44235054448435207,
      0.4305482169578645,
      0.29121945159109797,
      0.5119500293228103,
      0.5041800371554965,
      0.4924465769227786,
      0.46644171210814056,
      0.39864632346404627,
      0.3729475911975295,
      0.43253303873449966,
      0.4505863940031727,
      0.3957896146211697,
      0.3721056869341086,
      0.309824399697779,
      0.3518747793366541,
      0.3747685347755423,
      0.33863751424192223,
      0.3565178356970249,
      0.4140720226448007,
      0.3247140162396236
    ],
    [
      0.40034641178872366,
      0.45964857217464883,
      0.49080743421810813,
      0.4669564134140418,
      0.0,
      0.42810268579650845,
      0.3928933456169137,
      0.45795277125291123,
      0.47489960271360676,
      0.3821336666530062,
      0.40478486370536104,
      0.3129004893191982,
      0.5028716976036507,
      0.44608084737973175,
      0.4443835975346706,
      0.4881590127907196,
      0.4766871117199709,
      0.5120675915787976,
      0.40208565321571776,
      0.4476884743338241,
      0.42758317904829624,
      0.35819592238800557,
      0.3349647934376181,
      0.39083271286066834,
      0.4501923774628014,
      0.35279793333607046,
      0.40319911333306235,
      0.4467571132131294,
      0.4314278805649139
    ],
    [
      0.3369902466132382,
      0.47903767999213365,
      0.5841835027565654,
      0.5224386832087831,
      0.4395515430315251,
      0.0,
      0.4184543181309379,
      0.429503538940756,
      0.4610815393064911,
      0.45443367746688335,
      0.4802831024512453,
      0.30766777971029935,
      0.41642056869816835,
      0.5236582617499272,
      0.5250148803322545,
      0.48178461103500103,
      0.4558542352058099,
      0.4293653532276014,
      0.5391598300551705,
      0.5420683338625778,
      0.45171729062441046,
      0.3869757471876185,
      0.3817135043768267,
      0.4382343849870214,
      0.42169412066734524,
      0.3926019301123638,
      0.40183125889636173,
      0.4452834092118301,
      0.37669715297577366
    ],
    [
      0.3103492400249148,
      0.46138890981715286,
      0.47596722318657947,
      0.4598449691490991,
      0.41082747070149095,
      0.4052168247172141,
      0.0,
      0.40541814946231036,
      0.487642983314867,
      0.4009042244648173,
      0.3950647089084922,
      0.3263635344758229,
      0.4660264202906288,
      0.4314113135177273,
      0.44720048063809026,
      0.4276834590081655,
      0.41450595716582805,
      0.4326965141713248,
      0.4569803492300726,
      0.38868506146974435,
      0.39344720138468836,
      0.38535966019210743,
      0.31252599247826485,
      0.3976274479744393,
      0.4003227692623801,
      0.34420915465626645,
      0.37814071295137186,
      0.4118092563907123,
      0.3771005514358723
    ],
    [
      0.3182997466992852,
      0.44458291526742144,
      0.4383839380001664,
      0.47440499062014596,
      0.4332563563445466,
      0.4498605597219416,
      0.3801116053438822,
      0.0,
      0.46134603743302427,
      0.47731165816943255,
      0.40708082225167685,
      0.32514442653042996,
      0.44744209284671843,
      0.44415310023664833,
      0.41004621133699604,
      0.4438561056896366,
      0.4439306736812707,
      0.45156602504645726,
      0.5197314748589983,
      0.4263304804144292,
      0.4150617849931615,
      0.3767283701340953,
      0.3172612384831073,
      0.38512412243350513,
      0.37503296790290097,
      0.34521957308806894,
      0.38423863495504795,
      0.4317217586929394,
      0.3805455193943741
    ],
    [
      0.3283498605735975,
      0.5017501668753734,
      0.45596016872274125,
      0.49164988080712657,
      0.3777384472314522,
      0.4041464974237672,
      0.4048716481979171,
      0.42837395419835,
      0.0,
      0.4265126614352406,
      0.4639460198241725,
      0.31050448743120085,
      0.44567036613740174,
      0.4618158059884008,
      0.4584173507660223,
      0.48423961401037086,
      0.4079187968166931,
      0.4597938871467986,
      0.4424579339185244,
      0.41215461951135657,
      0.4171129385414116,
      0.37751903568571255,
      0.3346138856525225,
      0.37983668348888533,
      0.3770055328288302,
      0.36979864301378873,
      0.37019827810935935,
      0.4635286151116409,
      0.3501217192602828
    ],
    [
      0.2897048085685261,
      0.37904485960948686,
      0.4459603652214572,
      0.45231004891881565,
      0.34687773703249114,
      0.3899190613918204,
      0.3116423450144421,
      0.3680100709675129,
      0.4597564266931502,
      0.0,
      0.4030903317580974,
      0.2729130853022488,
      0.3883434283462912,
      0.43282790066826515,
      0.36987414004860786,
      0.4130388905599105,
      0.4398024251373591,
      0.3577274813776463,
      0.48925146470365144,
      0.40803806537409293,
      0.39766216983524116,
      0.37364239195468474,
      0.29554978290769385,
      0.33820123922075895,
      0.40390899521684975,
      0.34132559873922164,
      0.34694364386149434,
      0.382632533439764,
      0.36343742206443874
    ],
    [
      0.2860793261898382,
      0.47992842844198824,
      0.41366241476364674,
      0.4635354863272775,
      0.3936565964635388,
      0.4050719970940817,
      0.3867073346743404,
      0.38126509152951193,
      0.47622816494244147,
      0.4535536488281745,
      0.0,
      0.29522827052163225,
      0.4712331641535846,
      0.474856826954017,
      0.44555114367010584,
      0.4637377056624564,
      0.4310823755365043,
      0.38530407392860067,
      0.3987538300829694,
      0.4714431588999841,
      0.40185303687893015,
      0.44532656897287404,
      0.35636599256527335,
      0.39628239734814663,
      0.3800357753711239,
      0.4187580902538677,
      0.32803091441641996,
      0.3860923592568062,
      0.37215346953766515
    ],
    [
      0.2893729313812756,
      0.27531216512554946,
      0.2976655118560483,
      0.3293157453867648,
      0.29592109351102125,
      0.33968733700552,
      0.34408684234947007,
      0.3213881054398222,
      0.34260513747746546,
      0.3358776448838321,
      0.28247133095230614,
      0.0,
      0.32206234650076526,
      0.33540056508765836,
      0.27660733557983175,
      0.32962788002456866,
      0.3384572415418683,
      0.306936937922766,
      0.32919650886059415,
      0.31118707487819064,
      0.32373879599386735,
      0.3531244789036887,
      0.31817707531155714,
      0.33591630642361436,
      0.31117960038627945,
      0.2878142806439121,
      0.35144481876523415,
      0.3056552805919708,
      0.3680370871452163
    ],
    [
      0.34446345787532073,
      0.4709441407763568,
      0.4519624227519796,
      0.5131259306061509,
      0.39313373658967476,
      0.40400654503197986,
      0.41027245163676507,
      0.4225575600865654,
      0.4783144311533851,
      0.4193270531718465,
      0.4680767065451412,
      0.31928016747963484,
      0.0,
      0.4516452807350573,
      0.45228499932490873,
      0.4917821165492915,
      0.434748869249179,
      0.42796302986099644,
      0.42774645251793153,
      0.394791075625601,
      0.4165908912587464,
      0.3556823519561296,
      0.3284742086981507,
      0.39254677181065034,
      0.39867174702863784,
      0.3706496099043788,
      0.3860080998909132,
      0.41832081412962374,
      0.3400548601507072
    ],
    [
      0.3257925232798742,
      0.4621654190616957,
      0.5528873823600284,
      0.5204079600318883,
      0.4010147910472568,
      0.4667919474180393,
      0.38797341186700507,
      0.3730280231847709,
      0.4763130203330579,
      0.4346482877473581,
      0.44839618357899624,
      0.2851259626699174,
      0.48028738936417215,
      0.0,
      0.459207334637306,
      0.5060239588973683,
      0.4536039892703134,
      0.44060681397798307,
      0.4319471961543013,
      0.4532281132136602,
      0.4298554060417823,
      0.37209073898957357,
      0.30089197515813426,
      0.4004903746671191,
      0.43029055963642304,
      0.38625292423774926,
      0.35967768220743657,
      0.46696212271018456,
      0.3480289289107601
    ],
    [
      0.2776680531521878,
      0.48434706450741327,
      0.5124189681971678,
      0.4710784830224959,
      0.38878770203230717,
      0.4338358864770999,
      0.3739600272718999,
      0.30577969777288727,
      0.4487192537506284,
      0.38198967208401347,
      0.4589545122862504,
      0.23002253011199714,
      0.3848734220466372,
      0.47466801353851107,
      0.0,
      0.454697734221172,
      0.4002655089976721,
      0.3411143401838792,
      0.466923785393877,
      0.39493786407469567,
      0.3542116406823348,
      0.39514233342029903,
      0.35278622946765115,
      0.36586380814483177,
      0.41067436364449006,
      0.3684231891742691,
      0.3190923533199117,
      0.3913819988110494,
      0.3183743597108517
    ],
    [
      0.3311260913695053,
      0.4917111187827603,
      0.47036717942000394,
      0.49478401420142704,
      0.42611684063218425,
      0.468708777572284,
      0.40854907559656195,
      0.39343629676467606,
      0.5045117925331406,
      0.41658993279267165,
      0.4668010590173064,
      0.3189591993264844,
      0.5184096998174212,
      0.5167437716150787,
      0.46724081268651707,
      0.0,
      0.41966738628062705,
      0.44651841585457697,
      0.4211837435749981,
      0.4289908325177574,
      0.413534062472555,
      0.3845882214465388,
      0.3257160103583674,
      0.39055958809323665,
      0.3934856448913038,
      0.4007828772124191,
      0.35236269216870464,
      0.4162479872988296,
      0.38928138473582097
    ],
    [
      0.34212619677223555,
      0.36069838019579037,
      0.47068058501803955,
      0.47858860807257475,
      0.40098995327912457,
      0.46160746826958676,
      0.35093702637922264,
      0.3795394325310044,
      0.4625059704228853,
      0.39069222653656643,
      0.41352165359831883,
      0.27634068327525285,
      0.43089545744080726,
      0.4513652092310816,
      0.4082971158207225,
      0.43233546008325896,
      0.0,
      0.39465094762320985,
      0.45084225592532023,
      0.44975252993067705,
      0.3750984101259449,
      0.3355599452716531,
      0.3241173083841562,
      0.3596896762396491,
      0.48861848684921827,
      0.32490611931837354,
      0.3643379737878376,
      0.3760136434335912,
      0.3661500570197551
    ],
    [
      0.3164937237204426,
      0.40470905189312045,
      0.44714937770658625,
      0.42660822123509967,
      0.4285382791610668,
      0.44192960078810795,
      0.41661368715183245,
      0.394333084389177,
      0.5077151957018746,
      0.3497153392730348,
      0.38878820025497807,
      0.28836226737139503,
      0.4599452297956139,
      0.4469242636219277,
      0.39772658254051163,
      0.4465763950616477,
      0.3947863387230164,
      0.0,
      0.37449808167937726,
      0.4043935962650578,
      0.44957576697407053,
      0.3969578714566775,
      0.33254695415470326,
      0.3824733364304025,
      0.36319367506180655,
      0.3387809957383239,
      0.3739557905176698,
      0.4622473628002748,
      0.3841711248999411
    ],
    [
      0.3233936911925004,
      0.423113460957494,
      0.5263783404285842,
      0.46398871392117735,
      0.39021045534508336,
      0.4811254658681574,
      0.39828253960396376,
      0.4513000873566959,
      0.4547391559672438,
      0.4708571793058136,
      0.3819806784572721,
      0.26556898714929966,
      0.3632877010774631,
      0.4785450109273044,
      0.4672978927173157,
      0.4155650237324182,
      0.4743993582643973,
      0.3640211671699749,
      0.0,
      0.4645947592996147,
      0.3871839747253225,
      0.38878355941417486,
      0.32730268434664334,
      0.3752178573934364,
      0.4587036344876476,
      0.41608744594906644,
      0.3508084385700867,
      0.35748844743831243,
      0.3716226328750969
    ],
    [
      0.3000241683950202,
      0.36114373319986304,
      0.45897228860757733,
      0.42342211733716417,
      0.37041787110641455,
      0.42706610529489963,
      0.31597604262096723,
      0.35005331273325413,
      0.3937178893869573,
      0.4036951486703402,
      0.3839270318622823,
      0.26368578249424024,
      0.35033235518014916,
      0.4479734134312783,
      0.4136196561821175,
      0.4096611357537181,
      0.4394470238210819,
      0.3553296426874111,
      0.43285057421444284,
      0.0,
      0.40135946367803643,
      0.3801889486308312,
      0.30900876694865165,
      0.36958395096827346,
      0.3860340197147829,
      0.34505330260648637,
      0.34581005852643276,
      0.35998131742902073,
      0.342363280613744
    ],
    [
      0.30523481941065156,
      0.3422429759461796,
      0.42988883615832063,
      0.4455601477168363,
      0.41949328629500937,
      0.4722046885149258,
      0.3606424615150263,
      0.3808454728868824,
      0.4113088030852712,
      0.3907069058718582,
      0.36305807712711635,
      0.30817779926545463,
      0.43126172552012076,
      0.4147889156542701,
      0.3720180646063622,
      0.4198054158921076,
      0.45790198495765644,
      0.4263206694916133,
      0.43299920158167815,
      0.42893853485164035,
      0.0,
      0.3446025981562173,
      0.30781249191324633,
      0.3696158916737422,
      0.4337022066219558,
      0.327117225229838,
      0.37519918906349603,
      0.4081089710335164,
      0.3948526017418168
    ],
    [
      0.34608481578534,
      0.42937240119223996,
      0.37969173557872793,
      0.3991209754165792,
      0.3448949258141032,
      0.3789629198972646,
      0.3204206091758528,
      0.3524712173400013,
      0.3994773499835962,
      0.3702671127224497,
      0.4351876883100696,
      0.2546028741597792,
      0.3225414860301399,
      0.41042605832976853,
      0.4185818578686433,
      0.3983723260235348,
      0.3532998146944104,
      0.32773780748279924,
      0.38571861475725555,
      0.391004647252315,
      0.3503837922499,
      0.0,
      0.3286021574336113,
      0.3900088728781168,
      0.3603150345840973,
      0.4240885952989262,
      0.31276224180325474,
      0.3613925804028977,
      0.4427865228069645
    ],
    [
      0.28354696756645104,
      0.3427566767565209,
      0.3464744519635974,
      0.3330025091512907,
      0.3423404327085613,
      0.3599448182262732,
      0.3166220092567569,
      0.2965818702987304,
      0.3525852023080087,
      0.2729746471602661,
      0.3897550991104497,
      0.2514382031481477,
      0.3465141274755754,
      0.3400153474234695,
      0.3500503293677395,
      0.32305804076363054,
      0.358926422837359,
      0.3396510560877095,
      0.30490221031877596,
      0.3265811147656488,
      0.31012931471953564,
      0.3643144491637651,
      0.0,
      0.3161488509012007,
      0.36169565464673625,
      0.3175003043787097,
      0.35749096871761177,
      0.3060068542071508,
      0.3258364592341516
    ],
    [
      0.33620590503871983,
      0.4368573240478888,
      0.4728429992946519,
      0.4287725329529839,
      0.4440582307610854,
      0.4404711558193608,
      0.40932273011959297,
      0.3923407316981935,
      0.4559882451273647,
      0.43698174565786974,
      0.4414366904597631,
      0.34881884858785517,
      0.4398784155729072,
      0.47454405288435053,
      0.4680360452068202,
      0.43673289192668774,
      0.4500789905314424,
      0.42491597424057814,
      0.44517456959695534,
      0.4777817036834173,
      0.4768396962751644,
      0.44753739391755953,
      0.3902792606056056,
      0.0,
      0.40954188439010286,
      0.3960390817815922,
      0.4344960757314851,
      0.5110660744734676,
      0.44975231570005514
    ],
    [
      0.3325818714598041,
      0.3555704360699108,
      0.4874959123452196,
      0.4253722080555249,
      0.4023253111397367,
      0.38477849389146224,
      0.3522098266670104,
      0.30162722480757154,
      0.41247365906329425,
      0.3481090482365736,
      0.3626032976180451,
      0.25268445536888695,
      0.4235776545339145,
      0.4355036112450985,
      0.3869644575862292,
      0.3737362881633357,
      0.4711188372690098,
      0.40055046718602183,
      0.4044571895392661,
      0.43132475284424965,
      0.38810954071766557,
      0.3233855497710407,
      0.32723521424834745,
      0.3205161063470403,
      0.0,
      0.2957236406531871,
      0.35353200674141294,
      0.3568115464322561,
      0.2849385670632836
    ],
    [
      0.38160247008047876,
      0.48085292813344127,
      0.42979889717442044,
      0.424015420247708,
      0.3520558143678707,
      0.3980156795494496,
      0.39315547815714447,
      0.3565816250631555,
      0.47620225866401933,
      0.3733078275783286,
      0.40433121684117324,
      0.2863920041375614,
      0.4232086876566463,
      0.4176666925739796,
      0.4390539092055896,
      0.41806101295741005,
      0.3496572921887269,
      0.38543572705479034,
      0.41171087032248366,
      0.367985745021814,
      0.3402940847733529,
      0.41663341505238827,
      0.3050221282156662,
      0.3753741922535896,
      0.3479712619474473,
      0.0,
      0.32355376778683276,
      0.39320060797371825,
      0.3606336271666437
    ],
    [
      0.3430044395654155,
      0.39144067722965703,
      0.42432747840781304,
      0.4420140274604145,
      0.4157373686514998,
      0.44172862089162934,
      0.43400917677520545,
      0.4214231124911927,
      0.4737897403890714,
      0.4491719148550699,
      0.38937990164499436,
      0.3874861977477282,
      0.43437823168109513,
      0.4437846255570783,
      0.4168458132859829,
      0.4191233381337167,
      0.46080940910576085,
      0.404530618499912,
      0.4206082359173413,
      0.4002529433945452,
      0.44800515014888553,
      0.3456791625552691,
      0.4368181776493103,
      0.421913988314659,
      0.4403884712210797,
      0.3068542465573654,
      0.0,
      0.44229302590886,
      0.3604894850058804
    ],
    [
      0.29963268884183947,
      0.44101870410817545,
      0.46069202589441716,
      0.4220671391668498,
      0.3885062612797541,
      0.4010614652021318,
      0.37955199451414345,
      0.39080405876337365,
      0.48522023775835077,
      0.3956553068983977,
      0.42398017591521264,
      0.2913989266255417,
      0.46386892697875526,
      0.43672362439491597,
      0.40935032076127187,
      0.4308494787280175,
      0.400652611799446,
      0.45703554290761095,
      0.35923928711712794,
      0.36483371378257545,
      0.4160164383546505,
      0.38016835149503336,
      0.3161213238847802,
      0.40816913251286624,
      0.3551759538649337,
      0.3449998595968524,
      0.3453255682123628,
      0.0,
      0.41153621641871085
    ],
    [
      0.3475121775147545,
      0.3933294593764465,
      0.39184395763560187,
      0.39833999008428145,
      0.41293987967630263,
      0.41515740076023167,
      0.36937041083122146,
      0.37803489779451027,
      0.41712483683602963,
      0.41702457019328953,
      0.3983011466013535,
      0.3390498094394119,
      0.4048285171720467,
      0.3890770871775069,
      0.3917961361803568,
      0.41170419580670625,
      0.40808913813043834,
      0.39787694734197965,
      0.39858194061019026,
      0.4052194002328968,
      0.4113412560617533,
      0.4890788269232964,
      0.3497682729218119,
      0.42208574549582933,
      0.3809345594264504,
      0.3598623432065329,
      0.3456523032388359,
      0.39474146100970264,
      0.0
    ]
  ],
  "row_avgs": [
    0.22215474381995826,
    0.4067430663163577,
    0.44720688545064746,
    0.4060804365499968,
    0.4281214738733814,
    0.4472750173148186,
    0.407311447872873,
    0.4145633282346539,
    0.4145002678110336,
    0.380765596926215,
    0.4093492015452072,
    0.3199381235689521,
    0.4136937779427036,
    0.4233568007376483,
    0.3914640284106601,
    0.42417766103691995,
    0.39717352788699495,
    0.4007039069417049,
    0.4104231551407701,
    0.37645351436055136,
    0.3919432129208147,
    0.3710206084025942,
    0.32988729973799374,
    0.4348854130744114,
    0.37126132768087144,
    0.38684909436235104,
    0.4148674135373726,
    0.3957019762777892,
    0.3942380952742775
  ],
  "col_avgs": [
    0.3232491287317415,
    0.4216752393317206,
    0.44711374793077197,
    0.4471558330442949,
    0.3878832185563915,
    0.41797042328951045,
    0.3744564325376348,
    0.3758651954929406,
    0.44495005539366994,
    0.39764114904331327,
    0.40713032598215954,
    0.2915024573330944,
    0.4204963239131575,
    0.4429543477030817,
    0.4240399752177587,
    0.42732713265239164,
    0.4125935971940326,
    0.39458618996759254,
    0.41937361973475584,
    0.41526581139071794,
    0.3933788608816942,
    0.3792685204368393,
    0.327836795646807,
    0.37351747371124017,
    0.39074615673052104,
    0.3577764863106426,
    0.35394852984894676,
    0.40069189395082405,
    0.3617154810522779
  ],
  "combined_avgs": [
    0.2727019362758499,
    0.41420915282403914,
    0.4471603166907097,
    0.42661813479714583,
    0.40800234621488646,
    0.43262272030216453,
    0.39088394020525385,
    0.39521426186379727,
    0.42972516160235175,
    0.38920337298476415,
    0.40823976376368337,
    0.3057202904510232,
    0.41709505092793053,
    0.433155574220365,
    0.4077520018142094,
    0.4257523968446558,
    0.4048835625405138,
    0.3976450484546487,
    0.41489838743776297,
    0.3958596628756347,
    0.3926610369012544,
    0.3751445644197168,
    0.32886204769240035,
    0.4042014433928258,
    0.3810037422056962,
    0.37231279033649683,
    0.38440797169315966,
    0.39819693511430665,
    0.37797678816327773
  ],
  "gppm": [
    605.3346616126316,
    581.6884273792847,
    568.6336142784619,
    567.1891948203374,
    597.7277158216968,
    585.6604812207406,
    602.5131355641025,
    602.3042447231549,
    569.8263803877214,
    590.1317169606243,
    593.2490225761461,
    644.7356369828674,
    585.3144746789058,
    571.9539564154945,
    580.7195687239567,
    581.7120631049652,
    584.9758073074674,
    597.5256289299176,
    579.5794665489339,
    581.8295246996735,
    597.3740707363932,
    597.9908738710891,
    623.1383874830719,
    603.1887647336196,
    594.89818473563,
    610.3785193589448,
    612.8474100724934,
    592.0829801911686,
    608.9021974394756
  ],
  "gppm_normalized": [
    1.410378775634611,
    1.3399774497166532,
    1.3075122700030093,
    1.308921436962031,
    1.36653516865776,
    1.3417638928543474,
    1.3835350291946578,
    1.377802344768854,
    1.31247466736855,
    1.3550920960539592,
    1.360720287046709,
    1.4754066614332735,
    1.3453445661319512,
    1.3153776484335218,
    1.3371415243828895,
    1.336079877160282,
    1.3376702257318676,
    1.368496635507238,
    1.331809916965627,
    1.3366617087672938,
    1.3671304192162286,
    1.3756886298246676,
    1.427516715458903,
    1.3796388670899173,
    1.3650046926884891,
    1.4028476140316108,
    1.3986014212544418,
    1.3577291955438402,
    1.3942493499077027
  ],
  "token_counts": [
    548,
    467,
    449,
    492,
    391,
    412,
    438,
    397,
    480,
    439,
    411,
    388,
    456,
    452,
    464,
    438,
    403,
    409,
    446,
    444,
    401,
    457,
    413,
    395,
    434,
    435,
    372,
    423,
    413
  ],
  "response_lengths": [
    2708,
    2533,
    2515,
    2827,
    2288,
    2325,
    2517,
    2179,
    2694,
    2509,
    2304,
    2254,
    2447,
    2546,
    2630,
    2432,
    2369,
    2343,
    2554,
    2556,
    2256,
    2543,
    2275,
    2295,
    2506,
    2372,
    2152,
    2347,
    2387
  ]
}