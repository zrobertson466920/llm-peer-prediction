[
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nRECURSION OF THOUGHT: DIVIDE AND CONQUER REASONING WITH LANGUAGE MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWith the recent advances in language models, attempts are being made to apply them to solving multi-step reasoning problems. A major breakthrough in this line of research is to let language models generate intermediate steps, often called Chain of Thought (CoT), before producing a final answer. However, language models have an upper bound on the context size, i.e., the number of input tokens, such as 2048 for the recent GPT-3 and PaLM. Although several thousand tokens are enough to handle various tasks, solving more complex reasoning tasks can require orders of magnitude more tokens. Therefore, the context limit imposes a fundamental limit on the model’s reasoning capability. Inspired by human’s incredible reasoning ability based on abstraction and recursion, we propose Recursion of Thought (RoT) as a model-agnostic framework with the novel paradigm of teaching a language model to divide and conquer complex problems by recursively creating multiple contexts. Since RoT casts the context-related operations as tokens, a language model can trigger the recursion operations by simply producing the corresponding tokens. On multiple arithmetic and algorithmic reasoning tasks, we demonstrate that RoT dramatically improves the recent large-scale language model GPT-3 to solve extremely complex problems. Moreover, RoT can make tiny, randomly initialized Transformers or LSTMs to solve problems that even humans find daunting.\n\n1\n\nINTRODUCTION\n\nRecently, language models (LMs) have become a prominant direction to solve reasoning. Given a question sequence, the models are tasked to predict the following answer sequence. One recent line of research for reasoning with LMs is chain of thought (CoT) generation (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022; Lewkowycz et al., 2022). In CoT generation, complex reasoning problems are solved by generating intermediate reasoning steps, or a chain of thought, before producing the final answer. Directly answering a question would require a model to fully solve the problem in a single forward pass, meaning the range of solvable problems is severely limited by the model’s capacity. On the other hand, generating CoT before the answer allows the problem’s complexity to be spread across the CoT, making each token generation more straightforward given the previous tokens. This is closer to how humans solve complex problems, as we think step by step, instead of producing an answer reflexively.\n\nAlthough CoT seems promising, there is a critical issue that significantly limits its utility: the effective context size of sequence models cannot grow unbounded. In this work, context refers to the set of input tokens that a model is conditioned on when generating output. Practically, all sequence models have a limit on the maximum context length due to various reasons. For instance, Transformers (Vaswani et al., 2017) suffer from a quadratic computational cost on the context length, and RNNs (Hochreiter & Schmidhuber, 1997) struggle with long-term dependency modeling. Therefore, even the state-of-the-art language models, such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022), limit the maximum context length by up to 2048 tokens. However, the length of intermediate steps can grow rapidly with the problem’s complexity and exceeds the context limit. Since CoT can handle a problem only if the process of solving it fits into a single context, the range of problems that CoT can handle is severely constrained by the context limit. This issue must be\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\novercome to solve more challenging and useful reasoning problems, whose solutions may require millions of tokens.\n\nHumans can handle this issue by using abstraction and recursion. We divide a large problem into smaller subproblems and focus on each subproblem while solving it, instead of considering the entire problem at every step. We can further subdivide a subproblem into even smaller subproblems. With this intuition, we propose Recursion of Thought (RoT) as a model-agnostic framework for recursively solving multi-step reasoning problems. The key feature of RoT is to grant the model the ability to recursively create and utilize multiple contexts for subproblems. We achieve this feat by introducing several special tokens that a model can output to control its context. During inference, the model recursively solves the problems by producing appropriate tokens at the right time. Moreover, RoT supports tail recursion, which enables general computation with an indefinitely long chain of recursion.\n\nWe demonstrate RoT on four basic arithmetic operations (addition, subtraction, multiplication, and division) and four algorithmic tasks (longest common subsequence, longest palindromic subsequence, 0-1 knapsack, and matrix chain multiplication) to show its generality. Without any taskspecific component, such as a calculator, all tasks are formulated as autoregressive sequence modeling problems. These tasks require a model to generalize by just seeing a tiny fraction of the problem space since the space is combinatorially large. For example, even in simple arithmetic operations, two 6-digit operands result in one trillion possible combinations. Hence, we evaluate whether a model understands the underlying rules, instead of brute force memorization. In our experiments, the range of problems that CoT can handle is seriously constrained by the context limit. On the other hand, RoT leads language models to achieve near perfect accuracy, even if the problem size increases to the extreme, where solving one problem requires producing hundreds of thousands of tokens. Moreover, the dramatic improvement is not limited to large pre-trained language models like GPT-3. RoT can make tiny, randomly initialized Transformers or LSTMs perform extremely complex reasoning.\n\nThe key messages of this work are summarized as follows:\n\n• The reasoning capability of current language models is seriously constrained by the maxi-\n\nmum length of a single context.\n\n• Our Recursion of Thought (RoT) unleashes the reasoning capability of language models by letting them recursively create and utilize multiple contexts of subproblems, following the principle of divide and conquer.\n\nIn the supplementary file, we provide the source code to fully reproduce our experiments.\n\n2 RELATED WORK\n\nChain of Thought. Among several prior works on applying language models to reasoning, Scratchpad (Nye et al., 2021) may be the most closely related to our work. It is the first approach to fine-tune language models to produce CoT before generating an answer. It demonstrates its effectiveness on 8-digit addition, polynomial evaluation, and Python program execution. It also mentions the confined context size as a major limitation to be overcome. In order to unlock the full potential of Scratchpad, the authors argue that Transformers should be improved to allow greater context sizes. We solve this exact problem from a completely different perspective, i.e., using multiple contexts to divide-and-conquer. Our approach is more practical and scalable, compared to increasing the context limit. More recently, it has been found that sufficiently large pre-trained language models can be induced to produce CoT, by simply tuning the prompt. For instance, CoT prompting (Wei et al., 2022) adds several QA exemplars with CoT before the main question, encouraging the model to generate final answers in the similar manner. Kojima et al. (2022)’s prompting is even simpler; after a question, they start the answer with “Let’s think step by step,” and then let the model finish the rest. Even without fine-tuning, these methods significantly improve the reasoning accuracy of language models. Minerva (Lewkowycz et al., 2022) utilizes these prompting techniques with a specially curated scientific pre-training dataset to achieve remarkable results on various reasoning benchmarks. However, all of these works are still limited by the maximum context size.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nNeural Programmer-Interpreter (NPI). Unlike language models, NPI (Reed & de Freitas, 2016) interacts with its environment through a series of program execution. It consists of an LSTM core, an encoder for each domain, and a memory of program embeddings. At every time step, the LSTM core takes a program embedding, arguments, and an observation of its environment to produce the next program embedding and corresponding arguments. Cai et al. (2017) combine NPI with recursion and show that recursion plays a critical role in generalization. Since NPI requires full execution traces for training, there are multiple works to relax this requirement using reinforcement learning (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019).\n\nSystem 1 Approaches for Reasoning. Kahneman (2013) classifies cognitive tasks into two categories: System 1 and System 2. System 1 refers to fast and reflexive thinking, while System 2 refers to sequential reasoning. It is hard to define strict criteria to distinguish between System 1 and System 2 approaches. We classify a model as System 1 if it directly outputs an answer, while System 2 generates its process, as well. In that sense, RoT, CoT and NPI are System 2 approaches, while there have been various System 1 approaches to solve symbolic reasoning. Zaremba & Sutskever (2014) train LSTMs with curriculum learning to solve integer addition up to nine digits. Kaiser & Sutskever (2016) propose a convolutional architecture called Neural GPU that performs binary addition and multiplication. Trained on 20-bit problems, this model operates like a digital circuit for the arithmetic operations, which can generalize up to 2,000 bits. Similarly, Yan et al. (2020) solve 8-bit binary addition and 12-bit binary multiplication with Transformers. If Neural GPUs are like digital circuits, Neural Arithmetic Logic Units (Trask et al., 2018) are like analog circuits for arithmetic. They represent numerical quantities with the activation values of neural networks and design a clever architecture for arithmetic operations. Although these System 1 approaches fall behind System 2 methods in terms of generality, they can be highly efficient in specific domains. We believe future systems will often be a hybrid of System 1 and System 2, harnessing the advantages of both types.\n\n3 RECURSION OF THOUGHT\n\nThe main idea of Recursion of Thought (RoT) is to let the model recursively solve small subproblems in separate contexts, keeping each reasoning step simple and learnable. Our RoT is model-agnostic and general enough to be combined with any kind of sequence model that supports autoregressive generation. The only requirement is that the model should be able to infer p(xi+1∣X1∶i), the probability of the next token xi+1 given a sequence X1∶i = [x1; ...; xi]. Therefore, sequence models such as Transformers, RNNs, or more advanced ones can all be used in the RoT framework. RoT teaches a sequence model to solve a problem using the divide and conquer paradigm in a supervised manner. That is, we assume that ground truths for the intermediate steps of how to recurse are readily available, as in Scratchpad (Nye et al., 2021) or NPI (Reed & de Freitas, 2016).\n\nFor better understanding, we discuss RoT in the reverse order of the pipeline. In §3.1, we first describe how to perform RoT inference with a fully trained model. In §3.2, we introduce the training process. Finally, in §3.3, we discuss how to recursively divide the problems and build the training data for intermediate steps automatically.\n\n3.1\n\nINFERENCE\n\nRecursion of Thought (RoT) grants a language model the ability to control the recursion process. For basic recursion control, we first introduce the following special tokens: GO , STOP , and THINK . GO and STOP respectively mark the start and end of a problem sequence. They can be nested inside another GO - STOP pair to indicate a subproblem. THINK initiates a recursion procedure. RoT teaches a model how to use these tokens so that it can perform divide-and-conquer problem solving. We formulate each inference context of a QA problem, denoted X, as the following concatenation:\n\nX = [Q; Qsub,1; Asub,1; . . . ; Qsub,N ; Asub,N ; A]\n\n(1)\n\nwhere Q and A are the main question and answer sequence, and Qsub,∗ are the questions and answers of the top-level subproblems. During inference, a model is given Q and tasked to generate the rest. Questions (both Q and Qsub,∗ ) start with a GO token, and answers (both A and Asub,∗ ) end with a STOP token. For trivial cases, i.e., the base cases of recursion, the context\n\nand Asub,∗\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: An example of the Recursion of Thought inference in Algorithm 1. Each table represents an inference context X k in order of creation, which has the structure of Eq.1. For each context, the model is given Q and tasked to generate the rest, one token at a time. The model outputs the THINK token when it needs to generate Asub,∗ , the answer of a subproblem. The THINK token triggers a recursive process that solves the subproblem in a new context and returns the answer.\n\nFigure 2: The target sequence Y 1 is produced from X 1 in Figure 1 by Algorithm 2. Given X 1 model is trained to output Y 1 make sure ∥X∥ = ∥Y ∥.\n\n1∶i, the i+1, except for PAD s that are ignored. Note PAD is a dummy token to\n\ncontains no (Qsub,∗ ) pair. A subproblem can have smaller, lower-level subproblems recursively, but only the top-level subproblems remain in a context. As a result, we can abstract away the details for solving the subproblems and keep only the high-level results in the current context.\n\n, Asub,∗\n\nFor tail recursion, where the last subquestion’s answer becomes the final answer, we additionally introduce the TAIL token. If TAIL is used in the place of a GO token in the last subquestion Qsub,N , its answer Asub,N is treated as the final answer A, and the context X does not have duplicate A. Algorithm 1 summarizes the inference process. Figure 1 presents a example of solving 408 + 351 for better understanding. More detailed illustrations of inference can be found in Appendix A. The RoT function (L1) takes a fully trained model with a question Q as input and returns the answer A as output. The procedure starts by initializing the context X with the original question Q (e.g., GO 4 0 8 + 3 5 1 = in Figure 1). ians is the starting index of the answer, which is initialized to ∣X∣ + 1, where ∣ ⋅ ∣ is the length of a sequence. Then, in the main loop, the model iteratively generates a next token x from X, which is appended to the end of X. After the initialization, the model is expected to (i) generate answer A directly or (ii) output GO . If the model immediately generates A and finishes it with a STOP , the answer is returned (L9), which is the base case of the recursion. Up to this point, the algorithm is identical to common language models.\n\nOn the other hand, if the model decides to output GO , which signals the start of a subproblem, its index is stored in igo (L11). Since a subproblem has started, the next several tokens from the model should constitute the question of the subproblem. In Figure 1, the first subproblem of the main context X 1 is adding the last digits, i.e., 8 + 1. Once the subquestion is generated, the next step is to find an answer to it. This is the pivotal moment: instead of producing the answer, the model outputs the THINK token, which initiates the recursion with a new context (L16-23). First, we separate the subquestion Qsub, starting from igo (L16). Second, using it as an input, we trigger the recursive call (L17) to obtain the answer Asub. As shown as red arrows in Figure 1, this call creates another context X 2 and initializes its question part with the subquestion. Then, the same inference process is executed inside the new context, sometimes running more recursions. In the case of 8 + 1, the answer 9 STOP is immediately returned since it is a base case. Finally, the THINK token is replaced\n\n4\n\nQ408+35GO1=Qsub,1GO8+1=Asub,1THINKTHINKAsub,2GO40+35=Qsub,275STOP9A9STOP75STOPX1GO8+1=QAX2GO40+35=THINKGO0+5=Asub,1Qsub,1Q5STOPTHINKGO4+3=Asub,2Qsub,2A75STOPX3GO0+5=QA5STOPX4GO4+3=QAX57STOP7STOP9STOPQ408+35GO1=Qsub,1GO8+1=Asub,19STOPX1GO8+1=THINKY1PAD75STOPAsub,2GO40+35=Qsub,2THINKGO40+35=75STOP9A75STOP9PADPADPADPAD···Under review as a conference paper at ICLR 2023\n\nwith Asub (L21) and the starting position of the answer is updated (L23). Hence, when predicting the next token, the model sees the returned answer as input, instead of the THINK token.\n\nOnce a subproblem is finished, the model can solve another subproblem in the same way, or output the final answer. In Figure 1, the second subproblem is to add all the remaining digits, i.e., 40 + 35. To solve it, the model recursively solves two other subproblems, i.e., adding each digit, before generating the answer. If the model outputs the final answer followed by a STOP token, the answer is returned (L9). If the model starts a subproblem with TAIL instead of GO , it becomes a tail recursion (L14), and its answer is returned directly as the final answer (L19).\n\n3.2 TRAINING\n\nWe teach RoT in a supervised manner; the model is trained with the ground truth (GT) intermediate steps, which also include when to output the special tokens. Each training example is constructed as a pair of a ground truth context sequence X and the corresponding target sequence Y . The GT context X is structured as Equation 1 and automatically built by the algorithms that will be introduced in §3.3 and Appendix D. In this section, we discuss how to construct the target sequence Y for X, and define the training objective.\n\nAlgorithm 2 summarizes the process of converting X to Y , where Y has the same length with X. Refer to Figure 2 for an example. Overall, Y is a copy of X except for the parts corresponding to Q and Asub,∗ . Since the question Q is always given in a context, Q is replaced by special PAD tokens (L1), which mean “nothing to predict for this part.” Each subproblem’s answer Asub,n is replaced by a THINK token followed by several PAD s that fill in the rest to make sure ∣X∣ = ∣Y ∣ (L4). This way, the model is trained to output THINK instead of the first token of Asub,n. Since the whole Asub,n will be returned from the recursive process and replace the THINK during inference (L17,21 of Algorithm 1), we do not need a training signal for the rest of Asub,n.\n\nGiven a pair (X, Y ), the training objective is defined as follows:\n\nL = − ∑\n\nI[yi+1 ≠ PAD ] log p(yi+1∣X1∶i) (2)\n\ni\n\nAlgorithm 1 Recursion of Thought Inference\n\nRequire: A sequence model M trained for Recursion of Thought, a question sequence Q\n\n▷ Initialize context with Q ▷ Start of answer ▷ Tail recursion\n\nx ← M(X) ▷ Generate next token X ← [X; x] if x = STOP then\n\nX ← Q ians ← ∣X∣ + 1 t ← f alse while True do\n\n1: function ROT(M, Q) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15:\n\nreturn Xians∶∣X∣ else if x = GO then igo ← ∣X∣\n\nelse if x = TAIL then\n\n▷ Mark last GO\n\nigo ← ∣X∣ t ← true ▷ Mark tail recursion\n\nelse if x = THINK then Qsub ← Xigo∶∣X∣−1 Asub ← ROT(M, Qsub if t then\n\n)\n\nreturn Asub\n\nend if X ← [X1∶∣X∣−1; Asub\n\n]\n\n▷ Replace THINK with Asub\n\nians ← ∣X∣ + 1\n\n16:\n\n17: 18: 19: 20: 21:\n\n22: 23: end if 24: end while 25: 26: end function\n\nAlgorithm 2 Creating the target sequence\n\nRequire: Context X = [Q; Qsub,1; Asub,1;\n\n. . . ; Qsub,N ; Asub,N ; A]\n\n1: Y ← PAD ... PAD (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) ∣Q∣\n\n2: for n in 1...N do 3: 4:\n\nY ← [Y ; Qsub,n Y ← [Y ; THINK PAD ... PAD (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) ∣Asub,n∣−1\n\n]\n\n]\n\n5: end for 6: Y ← [Y ; A] 7: return Y\n\nwhere I is the indicator function that excludes PAD s from training. Simply, it means that the sequence model is trained to output yi+1 as the next token for a given X1∶i. Its form is almost identical to the standard language modeling objective: LLM = − ∑ i log p(xi+1∣X1∶i), which is to predict the next token given previous tokens. With this objective, any sequence model is trained in the standard way, i.e., end-to-end via stochastic gradient descent. For decoder-only transformers with causal masks, the training can be efficiently done in parallel for all tokens.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n3.3 THE RECURSIVE REASONING PROCEDURES\n\nAs explained in §3.2, we teach the recursive reasoning procedure for a problem type by providing ground truth contexts. To produce these contexts, we design an exemplary reasoning procedure for each problem type and implement it as a teacher program that automatically generates the contexts given a problem. The major desiderata for the exemplary reasoning procedures are two fold: (i) each context should be short, and (ii) the next tokens to generate should be obvious from the previous sequence. Although the definition of “being obvious” may vary depending on the model’s architecture (e.g., Transformers vs. LSTMs), the reasoning procedures developed for human generally meet these desiderata. Therefore, our procedures for the arithmetic problems are borrowed from the elementary school math. For example, the addition procedure shown in Figure 1 is a recursive version of adding digits one by one starting from the last digit. In another example of multiplication, an N -digit × M -digit multiplication is divided into an N -digit × 1-digit multiplication and an N -digit × (M − 1)-digit multiplication. The N -digit × 1-digit multiplication is further divided into a 1-digit × 1-digit multiplication and an (N − 1)-digit × 1-digit multiplication. For the algorithmic problems, we also borrow standard, well-known algorithms. In Appendix D, we provide the full details of the procedures for each problem type, with Python code snippets of the key parts. Note that our proposals for the reasoning procedures in Appendix D are one of many possible solutions, which are not necessarily optimal.\n\nTraining Data Distribution. We use the same problem distribution for both training and evaluation, since out-of-distribution generalization is not within the scope of this paper. That is, when teaching 6-digit multiplication to the model, both training and test sets are all examples of 6-digit multiplication. The problem distributions are elaborated in Appendix C. Another important detail regarding the training of RoT is that each training example in a batch is a context, not a whole problem. Since RoT generates multiple contexts per problem, often a large portion of contexts can be duplicate (mostly the base cases). Therefore, to build a training batch for RoT, we first sample a top level problem and find the set of unique RoT contexts from the problem. Out of the unique contexts, we randomly sample one context as a training example. We find this simple technique works well, and we do not need more sophisticated method, such as the adaptive curriculum learning in Reed & de Freitas (2016).\n\n4 EXPERIMENTS\n\nSince Recursion of Thought is the first approach of this kind, we mainly compare with two baselines. The first one is to output an answer directly from a question, which we call Without Thought (WT). The other one is to generate all the intermediate steps before the answer without recursion (Nye et al., 2021), which we refer to as Chain of Thought (CoT; not to be confused with the CoT prompting (Wei et al., 2022)) for consistency. We construct the ground truths for CoTs by unraveling the same recursive process which we design for RoT, into a single context sequence (see Appendix B for examples). Therefore, the number of tokens to generate while solving a problem is the same for both CoT and RoT (if we do not count the THINK tokens). However, the sizes of the individual contexts of CoT are far longer than those of RoT due to the recursively nested subproblems. Refer to Appendix I for more detailed analysis of the context sizes. Note that we train these baselines and do not use any prompting technique. When evaluating, we consider a problem to be correctly solved only if all the intermediate steps and the answer are correct. In other words, we impose stricter rules on both RoT and CoT by not counting “lucky guesses” as correct.\n\n4.1 THE REASONING PROBLEMS\n\nTo evaluate the reasoning capabilities, we test various reasoning tasks that are grouped into two categories: arithmetic reasoning and algorithmic reasoning. We below provide a rough description of the tasks, whose details can be found in Appendix C. All the reasoning tasks share one characteristic in common: we can easily adjust the problem’s difficulty. Therefore, we can gradually increase the degree of difficulty and see which method fails first. Since the goal of our experiments is to test the reasoning capability of language models, all problems are formulated in pure sequence modeling, with no external program (e.g., calculator) called by the models.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nArithmetic Reasoning. We test four basic arithmetic operations, i.e., addition, subtraction, multiplication, and division, with two non-negative integers. The difficulty of arithmetic problems is represented by the maximum number of digits in an operand. For instance, in 6-digit multiplication, each operand can range from 0 to 999,999. When we sample a problem, we sample each operand from the log-uniform distribution. Compared to the uniform distribution where the samples are highly biased towards extremely large numbers, we get roughly the same ratio of samples for each number of digits. The sampling schemes for each operation are elaborated in Appendix C.1.\n\nAlgorithmic Reasoning. We test four algorithmic tasks with distinct characteristics. These problems are generally solved via dynamic programming (DP), and the length of intermediate steps can increase rapidly since the time complexity of DP algorithms ranges from O(N 2 ) (LCS, LPS, and 0-1 Knapsack) to O(N 3 ) (MCM). (1) Longest Common Subsequence (LCS): Given two random sequences of length N , the model finds the longest common subsequence and its length. The sequences consist of 10 characters from 0 to 9, and the problem difficulty is defined to be the sequence length N . (2) Longest Palindromic Subsequence (LPS): Similar to LCS, given a random sequence of length N , the model finds the longest palindromic subsequence and its length. (3) 0-1 Knapsack: Given a list of N items with specific value and weight, the model finds the best combination of items that maximizes the total value under the weight limit of a knapsack. The problem difficulty is represented by the number of items. (4) Matrix Chain Multiplication (MCM): The computational cost of multiplying N (> 2) matrices varies greatly depending on the order of multiplication. MCM is the task of finding the best multiplication order that yields minimal computation cost. The difficulty is controlled by the number of matrices N .\n\n4.2 UNLEASHING GPT-3’S REASONING CAPABILITY THROUGH RECURSION OF THOUGHT\n\nDespite their remarkable language modeling capabilities, the state-of-the-art large language models, including GPT-3 (Brown et al., 2020), struggle to solve even the basic arithmetic tasks (Nye et al., 2021). For instance, it cannot correctly handle multiplication with more than one or two digits. Using the OpenAI API, we fine-tune GPT-3 on the reasoning tasks in §4.1 for 10K steps with a batch size of 256. Each training batch is randomly sampled from the training data distribution explained in §3.3. The results are presented in Figure 3a, and the technical details are described in Appendix E. Each point in the graphs represents one experiment at a certain problem difficulty. We report the accuracy on a test set of 1K unique problems randomly sampled as explained in Appendix C. To the best of our knowledge, the problems at this scale (e.g., 48-digit addition/subtraction and 16-digit multiplication/division) have never been solved by any language model without the help of external programs. For reference, Minerva (Lewkowycz et al., 2022) achieves around 80% accuracy on 10-digit addition and 20% on 18-digit addition.\n\nEven WT fine-tuning cannot make GPT-3 deal with such a level of complexity, while CoT is not applicable due to the context limit of 2048. The green dotted lines mark the maximum difficulty that can be handled by CoT under the context limit. On the other hand, RoT finetunes the GPT-3 to achieve near perfect scores in every experiment. As presented in Appendix I, solving each problem requires up to tens of thousands of tokens. Without any architectural change, RoT makes GPT-3 handle these extremely complex problems.\n\n4.3 RECURSION OF THOUGHT WITH TINY LANGUAGE MODELS\n\nRecent research on reasoning has been mostly focused on extremely large pre-trained language models. In this section, we show an interesting result that RoT can make even tiny models, without any pre-training, perform convoluted reasoning procedures. Since RoT is model-agnostic, we test the two basic sequence model architectures: Transformer Vaswani et al. (2017) and LSTM Hochreiter & Schmidhuber (1997). For Transformer, we use a decoder-only model with 4 layers, 2 attention heads, 128 embedding dimensions, and 256 feed-forward dimensions, a total of only 536K parameters. It is a million times smaller than the largest PaLM (Chowdhery et al., 2022) with 540B parameters. The context limit is set to 2048 following GPT-3 and PaLM. For LSTM, we use 4 layers, 64 input dimensions, and 256 hidden dimensions, which result in 272K parameters. We set the context limit of the LSTM to 512 since (i) it takes a lot of time for LSTMs to process the tokens sequentially, and (ii) they are not good at handling long-term dependency.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) GPT-3\n\n(b) Tiny Transformer\n\n(c) Tiny LSTM\n\nFigure 3: Comparison of the thought processes. In each graph, the x-axis is the problem difficulty, while the y-axis is the reasoning accuracy. Each point represents an independent experiment. The green vertical lines indicate the maximum problem difficulty that CoT can handle without exceeding the maximum context size.\n\nBy virtue of their small sizes, we conduct far more extensive experiments than GPT-3, which are presented in Figure 3b and Figure 3c. We test both arithmetic and algorithmic reasoning problems with Transformer, and the arithmetic problems with LSTM. For each experiment, we train a randomly initialized model and evaluate it on a test set of 30K unique problems. With a batch size of 256, Transformers and LSTMs are trained for 500K steps and 800K steps, respectively. We repeat each experiment eight times and report the average and standard deviation of the accuracies. Appendix K enumerates the exact values of Figure 3. With the tiny Transformer, we experiment to the extent where even humans would find daunting. For example, we test addition/subtraction up to 64 digits and multiplication/division up to 32 digits. Note that a 32-digit number cannot even fit into the 64-bit integer datatype.\n\nThroughout the experiments, we observe consistent patterns:\n\n• WT’s accuracy drops most quickly as the problem difficulty increases.\n\n8\n\n0D['LJLWV$FFXUDF\\$GGLWLRQ:LWKRXW7KRXJKW&KDLQRI7KRXJKW5HFXUVLRQRI7KRXJKW0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQ6HTXHQFH/HQJWK$FFXUDF\\/&66HTXHQFH/HQJWK$FFXUDF\\/36RI,WHPV$FFXUDF\\.QDSVDFNRI0DWULFHV$FFXUDF\\0&00D['LJLWV$FFXUDF\\$GGLWLRQ0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQ6HTXHQFH/HQJWK$FFXUDF\\/&66HTXHQFH/HQJWK$FFXUDF\\/36RI,WHPV$FFXUDF\\.QDSVDFNRI0DWULFHV$FFXUDF\\0&00D['LJLWV$FFXUDF\\$GGLWLRQ0D['LJLWV$FFXUDF\\6XEWUDFWLRQ0D['LJLWV$FFXUDF\\0XOWLSOLFDWLRQ0D['LJLWV$FFXUDF\\'LYLVLRQUnder review as a conference paper at ICLR 2023\n\n• CoT achieves near perfect accuracy, but it can only be applied to simple problems due to\n\nthe context limit.\n\n• RoT achieves near perfect accuracy and can be scaled up to extremely complex problems.\n\nDespite the small sizes, RoT makes the Transformers master all types of extremely complex problems. We do not test more difficult problems mainly because the evaluation becomes too costly, not because RoT is incapable of learning them.\n\n5 DISCUSSION\n\nThe results of the tiny Transformer suggest that we might have to rethink the capability of large language models. If RoT enables the tiny Transformer to easily master 32-digit multiplication or division, what would a million times bigger model, like PaLM, be capable of? In contrast to the currently ongoing arms race in language models, the number of parameters might not be the main bottleneck anymore to increase models’ reasoning capability. We believe that our new paradigm of utilizing multiple contexts has the potential to make a huge leap in this line of research.\n\nThe current limitation of RoT is the need for supervision to learn divide and conquer for each task. In order to apply RoT to a wider range of tasks, it may be crucial to reduce the expensive supervision. As one possible approach, we may borrow the RL-based methodologies that are developed for reducing supervision of NPI (Li et al., 2017; Fox et al., 2018; Pierrot et al., 2019).\n\nInterestingly, RoT cannot facilitate length generalization, e.g., training on 8-digit multiplication with RoT cannot make a model generalize to 16-digit multiplication. We believe this problem is rooted in more fundamental limitation of the Transformer architecture (Hahn, 2020), orthogonal to RoT. Fortunately, since RoT is a model-agnostic framework, we would be able to apply RoT to more advanced architectures to come in the future, which might be capable of length generalization.\n\n6 CONCLUSION\n\nDespite the remarkable advances in language models, their reasoning capability has always been constrained by the maximum size of a single context. In this work, we introduce Recursion of Thought to solve this problem by utilizing multiple contexts. We prove its potential through extensive experiments, showing that it is possible to make language models solve problems that require hundreds of thousands of tokens. We believe the core idea of utilizing multiple contexts will play an essential role in future language models.\n\nREFERENCES\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nJonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via recursion. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=BkbY4psgg.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. MeierHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.\n\nRoy Fox, Richard Shin, Sanjay Krishnan, Ken Goldberg, Dawn Song, and Ion Stoica. Parametrized In 6th International Conference on Learnhierarchical procedures for neural programming. ing Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id= rJl63fZRb.\n\nMichael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156–171, 2020. doi: 10.1162/tacl_a_00306. URL https://aclanthology.org/2020.tacl-1.11.\n\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735–\n\n1780, 1997.\n\nDaniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2013. ISBN 9780374533557.\n\nLukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/ 1511.08228.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n\nIn Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n\nlanguage models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022.\n\nAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. ArXiv, abs/2206.14858, 2022.\n\nChengtao Li, Daniel Tarlow, Alexander L. Gaunt, Marc Brockschmidt, and Nate Kushman. Neural program lattices. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=HJjiFK5gx.\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114, 2021.\n\nThomas Pierrot, Guillaume Ligner, Scott E. Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, and Nando de Freitas. Learning compositional neural programs with recursive tree search and planning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 14646–14656, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/95b431e51fc53692913da5263c214162-Abstract.html.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nScott E. Reed and Nando de Freitas. Neural programmer-interpreters. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv. org/abs/1511.06279.\n\nAndrew Trask, Felix Hill, Scott E. Reed,\n\nNeural arithmetic logic units.\n\nJack W. Rae, Chris Dyer, and Phil Blunsom. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 8046–8055, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 0e64a7b00c83e3d22ce6b3acf2c582b6-Abstract.html.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.\n\nYujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. Neural\n\nexecution engines: Learning to execute subroutines. ArXiv, abs/2006.08084, 2020.\n\nWojciech Zaremba and Ilya Sutskever. Learning to execute. ArXiv, abs/1410.4615, 2014.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA A STEP BY STEP ILLUSTRATION OF ROT INFERENCE\n\nIn this section, we provide a step by step illustration of the example in Figure 1. Here we assume an ideal model fully trained for RoT.\n\nStep 1\n\nThe context is initialized with the question Q.\n\nQ\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nStep 2\n\nThe model generates the first subquestion 8 + 1.\n\nQ\n\nQsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nX 1\n\nStep 3\n\nInstead of immediately producing the answer, the model outputs the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nX 1\n\nStep 4\n\nThe THINK token triggers the creation of a new context. The new context is initialized with the subproblem starting from the last GO of X 1, i.e., 8 + 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nQ\n\nX 2\n\nGO 8 + 1 =\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nStep 5\n\nSince the subproblem is a base case, the model outputs the answer 9 immediately.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\nTHINK\n\nQ\n\nA\n\nGO 8 + 1 =\n\n9 STOP\n\nX 1\n\nX 2\n\nStep 6\n\nThe answer is returned and replaces the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nX 1\n\nStep 7\n\nThe model generates the next subproblem, which is to add the remaining digits. Then, it produces THINK to find its answer.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nStep 8\n\nThe THINK token creates a new context X 3 for solving 40 + 35.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nX 3\n\nGO 4 0 + 3 5 =\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nStep 9\n\nSince 40 + 35 is not a base case, the model recursively produces more subproblems. In this case, the first subproblem is to add the last digits, i.e., 0 and 5. Then it outputs the THINK token to solve the subproblem.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\nTHINK\n\nStep 10\n\nThe new context X 4 is created to solve 0 + 5.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\nTHINK\n\nQ\n\nA\n\nGO 0 + 5 =\n\n5 STOP\n\nX 3\n\nX 4\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nStep 11\n\nThe answer is returned to X 3 and replaces the THINK token.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nStep 12\n\nThe model generates the next subproblem.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\nTHINK\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nStep 13\n\nX 5 created to solve the subproblem 4 + 3. Since this is a base case, the model produces the answer directly.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\nTHINK\n\nQ\n\nA\n\nGO 4 + 3 =\n\n7 STOP\n\nX 3\n\nX 5\n\nStep 14\n\nThe answer from X 5 replaces the THINK token in X 3.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\n7 STOP\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nStep 15\n\nSince all subproblems are solved in X 3, the answer 75 is generated and returned to X 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\nTHINK\n\nQ\n\nQsub,1\n\nAsub,1\n\nQsub,2\n\nAsub,2\n\nX 3\n\nGO 4 0 + 3 5 =\n\nGO 0 + 5 =\n\n5 STOP\n\nGO 4 + 3 =\n\n7 STOP\n\nA\n\n7 5 STOP\n\nStep 16\n\nThe answer of X 3 replaces the THINK token in X 1.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nGO 4 0 + 3 5 =\n\n7 5 STOP\n\nStep 17\n\nSince the subproblems in X 1 are all solved, the model produces the final answer.\n\nQ\n\nQsub,1\n\nAsub,1\n\nX 1\n\nGO 4 0 8 + 3 5 1 =\n\nGO 8 + 1 =\n\n9 STOP\n\nQsub,2\n\nAsub,2\n\nA\n\nGO 4 0 + 3 5 =\n\n7 5 STOP\n\n7 5 9 STOP\n\nB EXAMPLES OF COT TRAINING DATA\n\nIf we solve the example of 408+351 in figure 1 with RoT, the following five contexts are produced.\n\n• X 1: GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP GO 4 0 + 3 5 = 7 5 STOP 7 5\n\n9 STOP\n\n• X 2: GO 8 + 1 = 9 STOP • X 3: GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP GO 4 + 3 = 7 STOP 7 5 STOP • X 4: GO 0 + 5 = 5 STOP\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n• X 5: GO 4 + 3 = 7 STOP\n\nThe CoT context of the same problem is:\n\n• X CoT: GO 4 0 8 + 3 5 1 = GO 8 + 1 = 9 STOP GO 4 0 + 3 5 = GO 0 + 5\n\nSTOP GO 4 + 3 STOP 7 5 STOP 7 5 9 STOP\n\nIn a slightly more complicated example of 34 × 5, the RoT contexts are as follows:\n\n• X 1: GO 3 4 * 5 = GO 4 * 5 = 2 0 STOP GO 3 * 5 = 1 5 STOP TAIL 1 5 0\n\n+ 2 0 = THINK\n\n• X 2: GO 4 * 5 = 2 0 STOP • X 3: GO 3 * 5 = 1 5 STOP • X 4: GO 1 5 0 + 2 0 = GO 0 + 0 = 0 STOP GO 1 5 + 2 = 1 7 STOP 1 7 0\n\nSTOP\n\n• X 5: GO 0 + 0 = 0 STOP • X 6: GO 1 5 + 2 = GO 5 + 2 = 7 STOP 1 7 STOP • X 7: GO 5 + 2 = 7 STOP\n\nThe corresponding CoT context is:\n\n• X CoT: GO 3 4 * 5 = GO 4 * 5 = 2 0 STOP GO 3 * 5 = 1 5 STOP TAIL 1 5 0 + 2 0 = GO 0 + 0 = 0 STOP GO 1 5 + 2 = GO 5 + 2 = 7 STOP 1 7 STOP 1 7 0 STOP\n\nNotice that the CoT context consists of all the corresponding RoT contexts as its subsequences. The number of tokens to generate is identical to that of RoT, if we do not count the THINK tokens. Even in these simple examples, however, the context size of CoT is far longer than that of RoT. For much more complex problems, such as 8-digit multiplication or 0-1 Knapsack, the CoT context size can be orders of magnitude larger than RoT. See Appendix I for more details on the distribution of context sizes.\n\nC PROBLEM SPECIFICATIONS\n\nC.1 THE ARITHMETIC PROBLEMS\n\nFor arithmetic tasks, we test addition, subtraction, multiplication, and division on non-negative integers. For subtraction, we add a constraint that the first operand is not less than the second one, in order to enforce non-negative answers. For division, we let the output include both a quotient and a remainder, separated by a special token R , e.g., GO 7 ÷ 3 = 2 R 1 STOP .\n\nAs briefly mentioned in §4.1, naively sampling the operands from a uniform distribution makes the operands extremely biased towards large numbers. For example, the probability of sampling a 2-digit number from the 6-digit space is less than 0.01%. Thus, we define a variation of the loguniform distribution (often called the reciprocal distribution) to sample the operands. As a result, we obtain roughly the same proportion of operands for each number of digits.\n\nThe probability density of a log-uniform distribution is proportional to the reciprocal of the value. By definition, zero is not the support of a log-uniform distribution, and samples are overly concentrated to the first few values in the sampling range. Therefore, we slightly extend the log-uniform distribution by introducing an offset parameter δ. To sample an integer in range [α, β) with offset δ, we first uniformly sample a real number r in range [log(α + δ), log(β + δ)]. Then, r is transformed to ⌊exp(r) − δ⌋. We denote the extended log-uniform distribution Ulog(α, β, δ). As δ gets larger, the samples are more dispersed to larger numbers. In the experiments, we set δ = 3.\n\nAdditionally, we introduce several other sampling details for division problems. Assume that we independently sample two numbers a and b for the dividend and the divisor. In about half of the\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nAddition 1330 + 121163 114780 + 4356 638 + 2 35 + 77 114261 + 354 3 + 13792 10151 + 7 22 + 1399 363356 + 450475 73 + 11 179895 + 4128 3 + 10 1 + 141972 57612 + 18403 9 + 1621 3370 + 381 678 + 8854 422 + 10348 118 + 582 1343 + 408534 24 + 9251 315 + 652424 355 + 4434 22 + 834928 3028 + 357 777 + 1355 154874 + 81059 64936 + 216852 3 + 340939 3 + 984775 50581 + 1183 415 + 943 110 + 49 15 + 17058 36278 + 100 6 + 23516 1462 + 848 1002 + 2773 135 + 178346 22672 + 162038\n\nSubtraction 376776 − 35241 10638 − 100 109033 − 52649 85137 − 3098 22355 − 2824 7 − 1 652781 − 78853 64914 − 3114 13041 − 1422 28293 − 4540 11553 − 3576 656291 − 2795 93 − 42 55972 − 1782 84587 − 51 273269 − 5867 274405 − 14 51926 − 9 4272 − 229 223267 − 377 14857 − 1994 914771 − 836 3035 − 2963 30 − 12 149 − 4 89057 − 6 296410 − 9 45 − 3 78906 − 3 56560 − 29960 98 − 6 16551 − 920 25606 − 194 45 − 37 129443 − 70196 221 − 54 11010 − 818 47759 − 67 10 − 8 1439 − 153\n\nMultiplication 9466 × 176175 179 × 516 5509 × 133 6783 × 2 6 × 80285 37275 × 19258 168484 × 154 3331 × 40 349 × 158 17988 × 262130 8140 × 1670 51 × 5 16497 × 158 74 × 10 216 × 13414 621 × 2 2 × 5951 189486 × 13080 552792 × 763 77 × 3 179090 × 469029 1037 × 258 8 × 769974 47765 × 7254 5608 × 18164 21437 × 12 15007 × 15 539860 × 427 3583 × 9754 13 × 66 266394 × 185 3988 × 12 5514 × 57 5 × 1712 17 × 430178 227 × 127 20888 × 54 96 × 232801 175 × 1050 146 × 166\n\nDivision 620261 ÷ 155034 111730 ÷ 1176 28268 ÷ 1 588137 ÷ 25571 180330 ÷ 739 879975 ÷ 97772 111461 ÷ 905026 42338 ÷ 14003 108 ÷ 384103 60002 ÷ 7479 131467 ÷ 131290 890679 ÷ 62 228 ÷ 131108 892 ÷ 124 15 ÷ 964156 369044 ÷ 28364 457 ÷ 46 14687 ÷ 730 200361 ÷ 1049 19715 ÷ 965179 98 ÷ 7 406 ÷ 9 47345 ÷ 122 391613 ÷ 1631 892642 ÷ 3898 241554 ÷ 1901 116475 ÷ 12908 488317 ÷ 197443 7519 ÷ 325 3560 ÷ 847611 9711 ÷ 1385 44540 ÷ 103 19721 ÷ 58 59544 ÷ 24 333057 ÷ 333057 25719 ÷ 5142 7544 ÷ 46 45 ÷ 410 195659 ÷ 2047 412572 ÷ 16\n\nTable 1: 40 randomly selected samples of each type of 6-digit arithmetic problems.\n\ncases, the dividend a would be less than the divisor b, so the quotients will be zero for those cases. To ensure a diverse range of quotients, we sample the divisor b from Ulog(1, 10N , 3), the quotient c from Ulog(0, 10N /b, 3), and the remainder r from Ulog(0, b, 3). The dividend is calculated from these values: a = b × c + r. This way, we can sample division problems with a diverse range of quotients and remainders.\n\nTable 1 presents 40 problem samples for each 6-digit problem type. Several properties of our sampling scheme can be observed from the table. First, each number ranges over diverse numbers of digits. Second, the division problems are mostly non-trivial, i.e., the quotients are not concentrated at zero.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nC.2 THE ALGORITHMIC PROBLEMS\n\nC.2.1 LONGEST COMMON SUBSEQUENCE (LCS)\n\nThe question of an LCS problem is two number sequences joined by the LCS token, and the answer is the corresponding LCS and its length separated by ; . Here is an example of length-4 LCS problem:\n\n• Q: GO 1 2 3 4 LCS 2 4 6 8 = • A: 2 4 ; 2 STOP\n\nFor a length-N LCS problem, we sample two sequences of length N . Each character of the sequences are randomly sampled from 0-9 with equal probability.\n\nC.2.2 LONGEST PALINDROMIC SUBSEQUENCE (LPS)\n\nThe question of a length-N LPS problem starts with the LPS , followed by a sequence of length N . Similar to LCS, the answer contains the corresponding LPS and its length separated by ; . The following is an example of length-8 LPS problem:\n\n• Q: GO LPS 4 1 2 5 3 2 6 1 = • A: 1 2 3 2 1 ; 5 STOP\n\nThe sequence of an LPS problem is sampled in the same way as done for the LCS problem.\n\nC.2.3\n\n0-1 KNAPSACK\n\nEach item in a 0-1 Knapsack problem is represented by its value and weight. For instance, 1 2 & 3 4 represents an item with a value of 12 and a weight of 34. The question part of a 0-1 Knapsack problem is a sequence consisting of the KNAPSACK token, a list of items separated by , , the token @ , and the capacity of the knapsack. The answer part starts with a list of items to include, then $ , and finally the total value. The following is an example of a 3-item knapsack problem.\n\n• Q: GO KNAPSACK 5 & 1 2 , 2 5 & 1 5 , 1 9 & 1 8 @ 4 0 =\n\n• A: 2 5 & 1 5 , 1 9 & 1 8 $ 4 4 STOP\n\nIn this example, given a knapsack of capacity 40, the last two are selected with the total value of 44.\n\nFor a fixed number of items, we uniformly sample each item’s value and weight from the integers of range [1, 99].\n\nC.2.4 MATRIX CHAIN MULTIPLICATION (MCM)\n\nThe cost of multiplying many matrices is very sensitive to the order of multiplication. Matrix chain multiplication is the task of finding the best order with the minimum cost. Here, the cost is defined to be the total number of element multiplications. In the example of three matrices A, B, and C, whose shapes are 4×2, 2×8, and 8×3 respectively, the cost of computing (AB)C is 4×2×8+4×8×3 = 160, while another order A(BC) costs only 2 × 8 × 3 + 4 × 2 × 3 = 72. In the question of an MCM problem, the sizes of the matrices are enumerated, and the answer contains the order and the total cost separated by ; . The example above is represented as the following sequences.\n\n• Q: GO MCM 4 × 2 , 2 × 8 , 8 × 3 =\n\n• A: 4 × 2 , ( 2 × 8 , 8 × 3 ) ; 7 2 STOP\n\nGiven a fixed number of matrices, we sample the sizes of matrices from the range [1, 99].\n\nC.2.5 SORTING\n\nAlthough not included in the main text, we test the problem of sorting multi-digit numbers. The results are presented in Appendix J. The problem difficulty is defined by the maximum number of\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nterms. For a sorting problem of at most N terms, we first uniformly sample the number of terms from [2, N ]. Then we sample each term from Ulog(0, 1000, 5). The following is an example of the sorting problem.\n\n• Q: GO SORT 1 3 9 , 1 6 0 , 4 3 4 , 7 9 6 , 4 1 =\n\n• A: 4 1 , 1 3 9 , 1 6 0 , 4 3 4 , 7 9 6 STOP\n\nD DETAILS OF THE RECURSIVE REASONING PROCEDURES\n\nIn this section, we elaborate the procedures to recursively solve the arithmetic problems. Specifically, we present the algorithms to produce the subproblems of a problem. Therefore, for a set of randomly sampled questions, we can generate ground truth contexts using these algorithms. For better understanding, we present the key parts of our Python code, the thought methods. For each problem, we create a child class the Problem class and implement thought static method. The method takes a set of arguments for a problem and returns the list of direct subproblems. Each subproblem is represented by a problem class, problem arguments, and recursion type (whether it is a tail recursion or not). We use named tuple T to group these information:\n\n1 2\n\nfrom collections import namedtuple T = namedtuple('Thought', ['prob_cls', 'args', 'type'], defaults=[''])\n\nFor instance, T(Mul, (3, 4)) represents a regular subproblem of 3 × 4, and T(Add, (12, 340), ’tail’) represents a subproblem of 12 + 340 which should be performed as a tail recursion. Once the thought method returns a list of Ts, we can recursively find more subproblems for each subproblem.\n\nD.1 ADDITION\n\nThe core idea of our recursive procedure for addition is to first add the last digits, and then add the rest. If the sum of the last digits is greater than or equal to 10, we insert another subproblem for adding the carry right after adding the last digits.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21\n\nclass Add(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left < 10 and right < 10:\n\nreturn []\n\nl_last, r_last = left % 10, right % 10 thoughts = [T(Add, (l_last, r_last))]\n\nl_rest, r_rest = left // 10, right // 10 if l_last + r_last >= 10:\n\nthoughts.append(T(Add, (l_rest, 1))) l_rest += 1\n\nif l_rest > 0 and r_rest > 0:\n\nthoughts.append(T(Add, (l_rest, r_rest)))\n\nreturn thoughts\n\nFigure 1 in the main draft is an example with no carry, and the following is another example of 27+65 with a carry.\n\n• X 1: GO 3 1 7 + 6 5 = GO 7 + 5 = 1 2 STOP GO 3 1 + 1 = 3 2 STOP GO 3\n\n2 + 6 = 3 8 STOP 3 8 2 STOP\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\n• X 2: GO 7 + 5 = 1 2 STOP • X 3: GO 3 1 + 1 = GO 1 + 1 = 2 STOP 3 2 STOP • X 4: GO 1 + 1 = 2 STOP • X 5: GO 3 2 + 6 = GO 2 + 6 = 8 STOP 3 8 STOP • X 6: GO 2 + 6 = 8 STOP\n\nD.2 SUBTRACTION\n\nSimilar to addition, we first subtract the last digits and solve the rest recursively. When subtracting the last digits x and y, we always borrow 10 for x to prevent a negative result. The borrowing of 10 is easy for a sequence model: just put 1 before x. Therefore, the base cases of subtraction are when a ≤ 19 and b ≤ 9. If the subtraction result of the last digits is smaller than 10, i.e., the borrow is actually needed, we subtract 1 from the rest of the first operand m.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20\n\nclass Sub(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left <= 19 and right <= 9:\n\nreturn []\n\nl_last = left % 10 + 10 r_last = right % 10 thoughts = [T(Sub, (l_last, r_last))] l_rest, r_rest = left // 10, right // 10 if l_last - r_last < 10:\n\nthoughts.append(T(Sub, (l_rest, 1))) l_rest -= 1\n\nif r_rest > 0:\n\nthoughts.append(T(Sub, (l_rest, r_rest)))\n\nreturn thoughts\n\nHere is an example of 432-216:\n\n• X 1: GO 4 3 2 - 2 1 6 = GO 1 2 - 6 = 6 STOP GO 4 3 - 1 = 4 2 STOP GO\n\n4 2 - 2 1 = 2 1 STOP 2 1 6 STOP\n\n• X 2: GO 1 2 - 6 = 6 STOP • X 3: GO 4 3 - 1 = GO 1 3 - 1 = 1 2 STOP 4 2 STOP • X 4: GO 1 3 - 1 = 1 2 STOP • X 5: GO 4 2 - 2 1 = GO 1 2 - 1 = 1 1 STOP GO 4 - 2 = 2 STOP 2 1 STOP • X 6: GO 1 2 - 1 = 1 1 STOP • X 7: GO 4 - 2 = 2 STOP\n\nNotice that the final answer and the questions of each subproblem can be easily constructed from previous sequence.\n\nD.3 MULTIPLICATION\n\nThe base cases of multiplication are (i) when either operands are 0 or 1, or (ii) when both operands are less than 10. If one of the operands is 0, then the answer is zero; when one of them is 1, then the answer is just a copy of the other operand. For the cases where both operands are less than 10, we just let the model memorize them, which is similar to an elementary school math curriculum.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nThere are two types of non-base cases. For the simpler case, where the second operand is less than 10, we first split the first operand into the last digit and the rest. We then multiply each of them with the second operand and combine the results. Otherwise, we split the second operand into the last digit and the rest. The first operand is multiplied to each of them, and the results are summed.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n\nclass Mul(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left <= 1 or right <= 1:\n\nreturn []\n\nif left <= 9 and right <= 9:\n\nreturn []\n\nthoughts = [] if right < 10:\n\nthoughts.append(T(Mul, (left % 10, right))) thoughts.append(T(Mul, (left // 10, right)))\n\na1 = (left % 10) * right a2 = (left // 10) * right thoughts.append(T(Add, (a2 * 10, a1), 'tail'))\n\nelse:\n\na1 = left * (right % 10) thoughts.append(T(Mul, (left, right % 10)))\n\na2 = left * (right // 10) thoughts.append(T(Mul, (left, right // 10)))\n\nthoughts.append(T(Add, (a2 * 10, a1), 'tail'))\n\nreturn thoughts\n\nHere are some example contexts of multiplication:\n\n• X 1: GO 4 3 * 2 1 = GO 4 3 * 1 = 4 3 STOP GO 4 3 * 2 = 8 6 STOP TAIL\n\n8 6 0 + 4 3 = THINK\n\n• X 2: GO 4 3 * 1 = 4 3 STOP\n\n• X 3: GO 4 3 * 2 = GO 3 * 2 = 6 STOP GO 4 * 2 = 8 STOP TAIL 8 0 + 6 =\n\nTHINK\n\n• X 4: GO 3 * 2 = 6 STOP\n\n• X 5: GO 4 * 2 = 8 STOP\n\n• X 6: GO 8 0 + 6 = GO 0 + 6 = 6 STOP 8 6 STOP\n\n• X 7: GO 0 + 6 = 6 STOP\n\n• X 8: GO 8 6 0 + 4 3 = GO 0 + 3 = 3 STOP GO 8 6 + 4 = 9 0 STOP 9 0 3\n\nSTOP\n\n• X 9: GO 0 + 3 = 3 STOP\n\n• X 10: GO 8 6 + 4 = GO 6 + 4 = 1 0 STOP GO 8 + 1 = 9 STOP 9 0 STOP\n\n• X 11: GO 6 + 4 = 1 0 STOP\n\n• X 12: GO 8 + 1 = 9 STOP\n\nNotice that we use tail recursion in X 1 and X 3.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nD.4 COMPARISON\n\nComparison is used as a subroutine during division. The procedure for comparison consists of three steps:\n\n1. Compare the numbers of digits.\n\n2. If the numbers of digits are the same, compare the most significant digits.\n\n3. If the most significant digits are identical, compare the remaining digits recursively.\n\nWe find that the sequence models can perform the first step without an explicit subproblem. Therefore, we only add intermediate steps for the second and the third steps.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22\n\nclass Compare(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args\n\n# Base cases if left < 10 and right < 10:\n\nreturn []\n\nthoughts = [] digit_l, digit_r = len(str(left)), len(str(right)) if digit_l == digit_r:\n\n# Compare first digit l_first, r_first = int(str(left)[0]), int(str(right)[0]) thoughts.append(T(Compare, (l_first, r_first))) if l_first == r_first: # Compare the rest l_rest = int(str(left)[1:]) r_rest = int(str(right)[1:]) thoughts.append(T(Compare, (l_rest, r_rest)))\n\nreturn thoughts\n\nThe following is an example of comparing 153 and 159.\n\n• X 1: GO 1 5 3 VS 1 5 9 = GO 1 VS 1 = EQ STOP GO 5 3 VS 5 9 = LT STOP\n\nLT STOP\n\n• X 2: GO 1 VS 1 = EQ STOP • X 3: GO 5 3 VS 5 9 = GO 5 VS 5 = EQ STOP GO 3 VS 9 = LT STOP LT STOP • X 4: GO 5 VS 5 = EQ STOP • X 5: GO 3 VS 9 = LT STOP\n\nD.5 DIVISION\n\nSolving division is the most challenging among the four basic arithmetic operations since the procedure is basically trial and error, searching for the correct quotient. Nonetheless, the following process is a recursive version of the elementary school division.\n\nThe base case is when the dividend is less than or equal to the divisor. If the dividend is smaller than the divisor, the quotient is 0, and the remainder is the dividend. If the dividend is equal to the divisor, than the quotient is 1, and the remainder is 0. Both cases can be handled relatively easily by neural sequence models. To determine whether it is one of these cases, we always perform the comparison as the first subproblem.\n\nIf it is not a base case, we check whether the dividend is smaller than 10 times the divisor. If the dividend is smaller, we subtract the divisor from the dividend and recursively divide the result with the divisor. The final answer is attained by simply adding 1 to the quotient of the smaller division.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nTo explain the other case, where the dividend is greater than 10 times the divisor, let us call the dividend a and the divisor b. First, we split the a into the last digit x and the remaining digits m. Then, we divide m with the divisor b, i.e., we are solving one-digit-smaller subproblem first. Since we define the division operation to return both a quotient and a remainder, the quotient q1 = m/b and the remainder r1 = m mod b from the subproblem is added to the context. Next, we concatenate the remainder and x, which is numerically computing r × 10 + x, and divide it again with b. Let the quotient and the remainder of this operation q2 and r2. Then, the quotient of the final answer is q1 × 10 + q2, while the remainder is simply r2.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20\n\nclass Div(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nleft, right = args thoughts = [T(Compare, (left, right))]\n\n# Base cases if left <= right:\n\nreturn thoughts\n\nthoughts.append(T(Compare, (left, right * 10))) if left <= right * 10:\n\ndiff = left - right thoughts.append(T(Sub, (left, right))) thoughts.append(T(Div, (diff, right)))\n\nelse:\n\nthoughts.append(T(Div, (left // 10, right))) left_remainder = (left // 10) % right * 10 + left % 10 thoughts.append(T(Div, (left_remainder, right)))\n\nreturn thoughts\n\nThe following is an example of 76 ÷ 29.\n\n• X 1: GO 7 6 ÷ 2 9 = GO 7 6 VS 2 9 = GT STOP GO 7 6 VS 2 9 0 = LT STOP GO 7 6 - 2 9 = 4 7 STOP GO 4 7 ÷ 2 9 = 1 R 1 8 STOP 2 R 1 8 STOP\n\n• X 2: GO 7 6 VS 2 9 = GO 7 VS 2 = GT STOP GT STOP\n\n• X 3: GO 7 VS 2 = GT STOP\n\n• X 4: GO 7 6 VS 2 9 0 = LT STOP\n\n• X 5: GO 7 6 - 2 9 = GO 1 6 - 9 = 7 STOP GO 7 - 1 = 6 STOP GO 6 - 2 =\n\n4 STOP 4 7 STOP\n\n• ... • X 9: GO 4 7 ÷ 2 9 = GO 4 7 VS 2 9 = GT STOP GO 4 7 VS 2 9 0 = LT STOP GO 4 7 - 2 9 = 1 8 STOP GO 1 8 ÷ 2 9 = 0 R 1 8 STOP 1 R 1 8 STOP\n\n• X 10: GO 4 7 VS 2 9 = GO 4 VS 2 = GT STOP GT STOP\n\n• X 11: GO 4 VS 2 = GT STOP\n\n• X 12: GO 4 7 VS 2 9 0 = LT STOP\n\n• X 13: GO 4 7 - 2 9 = GO 1 7 - 9 = 8 STOP GO 4 - 1 = 3 STOP GO 3 - 2 =\n\n1 STOP 1 8 STOP\n\n• ... • X 17: GO 1 8 ÷ 2 9 = GO 1 8 VS 2 9 = LT STOP 0 R 1 8 STOP\n\n• X 18: GO 1 8 VS 2 9 = GO 1 VS 2 = LT STOP LT STOP\n\n• ...\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nD.6 LONGEST COMMON SUBSEQUENCE (LCS)\n\nGiven sequences A and B, the algorithm starts by comparing the last characters of the two sequences. If the last two characters are the same, we find LCS of the subsequences without the last characters, i.e., LCS of A∶−1 and B∶−1. Otherwise, we compute the LCSs of the cases where the last character of either side is removed, and return the better one. In the following code, LCS._answer is the subroutine that finds the LCS of two sequences. Equal returns TRUE if the two arguments are the same, or FALSE otherwise.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22\n\nclass LCS(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nl, r = args if len(l) == 0 or len(r) == 0:\n\nreturn []\n\nthoughts = [T(Equal, (l[-1], r[-1]))] if l[-1] == r[-1]:\n\nthoughts.append(T(LCS, (l[:-1], r[:-1]))) return thoughts\n\nlcs1_args = (l[:-1], r) lcs2_args = (l, r[:-1]) lcs1 = LCS._answer(lcs1_args) lcs2 = LCS._answer(lcs2_args) thoughts.extend([\n\nT(LCS, lcs1_args), T(LCS, lcs2_args), T(Compare, (len(lcs1), len(lcs2)))\n\n]) return thoughts\n\nThe following is an example of finding the LCS of 123 and 234.\n\n• X 1: GO 1 2 3 LCS 2 3 4 = GO EQUAL 3 , 4 = FALSE STOP GO 1 2 LCS 2 3 4 = 2 ; 1 STOP GO 1 2 3 LCS 2 3 = 2 3 ; 2 STOP GO 1 VS 2 = LT STOP 2 3 ; 2 STOP\n\n• X 2: GO EQUAL 3 , 4 = FALSE STOP • X 3: GO 1 2 LCS 2 3 4 = GO EQUAL 2 , 4 = FALSE STOP GO 1 LCS 2 3 4 = ; 0 STOP GO 1 2 LCS 2 3 = 2 ; 1 STOP GO 0 VS 1 = LT STOP 2 ; 1 STOP\n\n• ... • X 21: GO 1 2 3 LCS 2 3 = GO EQUAL 3 , 3 = TRUE STOP GO 1 2 LCS 2 = 2\n\n; 1 STOP 2 3 ; 2 STOP\n\n• ... • X 23: GO 1 VS 2 = LT STOP\n\nD.7 LONGEST PALINDROMIC SUBSEQUENCE (LPS)\n\nThe overall algorithm for LPS is similar to LCS. The base cases are when the sequence length is less then 3. If it is not a base case, we first check if the characters at both ends of the sequence are the same. If they are the same, we find the LPS of the subsequence excluding them. Otherwise, we compare the cases where one of the end characters are excluded.\n\n1 2\n3 4\n5 6\n\nclass LPS(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\n# Base cases if len(args) == 1: return []\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n\nelif len(args) == 2:\n\nreturn [T(Equal, args)]\n\nthoughts = [T(Equal, (args[0], args[1]))] if args[0] == args[-1]:\n\nsub_lps = LPS._answer(args[1:-1]) thoughts.extend([\n\nT(LPS, args[1:-1]), T(Add, (len(sub_lps), 2))\n\n])\n\nelse:\n\nlps1_args = args[:-1] lps2_args = args[1:] lps1 = LPS._answer(lps1_args) lps2 = LPS._answer(lps2_args) thoughts.extend([\n\nT(LPS, lps1_args), T(LPS, lps2_args), T(Compare, (len(lps1), len(lps2)))\n\n])\n\nreturn thoughts\n\nThe following is an example of LPS.\n\n• X 1: GO LPS 1 2 3 2 = GO EQUAL 1 , 2 = FALSE STOP GO LPS 1 2 3 = 1 ; 1 STOP GO LPS 2 3 2 = 2 3 2 ; 3 STOP GO 1 VS 3 = LT STOP 2 3 2 ; 3 STOP\n\n• X 2: GO EQUAL 1 , 2 = FALSE STOP • X 3: GO LPS 1 2 3 = GO EQUAL 1 , 3 = FALSE STOP GO LPS 1 2 = 1 ; 1\n\nSTOP GO LPS 2 3 = 2 ; 1 STOP GO 1 VS 1 = EQ STOP 1 ; 1 STOP\n\n• ... • X 10: GO LPS 2 3 2 = GO EQUAL 2 , 2 = TRUE STOP GO LPS 3 = 3 ; 1 STOP\n\nGO 1 + 2 = 3 STOP 2 3 2 ; 3 STOP\n\n• ... • X 14: GO 1 VS 3 = LT STOP\n\nD.8\n\n0-1 KNAPSACK\n\nThe base cases are when there is only one item. In this case, we simply compare the item’s weight and the knapsack’s capacity, to determine whether the item should be included. If it is a non-base case, we compare two possibilities: (i) include the first item, or (ii) exclude the first item. We recursively compute the subproblems and find the case with the best value.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17\n\nclass LPS(Problem):\n\n@staticmethod def thought(args) -> list[T]: items, capacity = args value, weight = items[0]\n\n# Base case if len(items) == 1:\n\nreturn [T(Compare, (weight, capacity))]\n\n# When excluding the current item items_max, value_max = Knapsack._answer((items[1:], capacity)) thoughts = [\n\nT(Knapsack, (items[1:], capacity)), T(Compare, (weight, capacity)),\n\n]\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\n18 19 20 21 22 23 24 25 26 27 28 29 30\n\n# When including the current item if weight <= capacity:\n\nitems_sub, value_sub = Knapsack._answer( (items[1:], capacity - weight))\n\nvalue_incl = value_sub + value thoughts.extend([\n\nT(Sub, (capacity, weight)), T(Knapsack, (items[1:], capacity - weight)), T(Add, (value_sub, value)), T(Compare, (value_incl, value_max)),\n\n])\n\nreturn thoughts\n\nThe following is an example of 0-1 knapsack problem with three items and a knapsack capacity of 10.\n\n• X 1: GO KNAPSACK 3 & 9 , 4 & 2 , 9 & 5 @ 1 0 = GO KNAPSACK 4 & 2 , 9 & 5 @ 1 0 = 4 & 2 , 9 & 5 $ 1 3 STOP GO 9 VS 1 0 = LT STOP GO 1 0 - 9 = 1 STOP GO KNAPSACK 4 & 2 , 9 & 5 @ 1 = $ 0 STOP GO 0 + 3 = 3 STOP GO 3 VS 1 3 = LT STOP 4 & 2 , 9 & 5 $ 1 3 STOP\n\n• X 2: GO KNAPSACK 4 & 2 , 9 & 5 @ 1 0 = GO KNAPSACK 9 & 5 @ 1 0 = 9 & 5 $ 9 STOP GO 2 VS 1 0 = LT STOP GO 1 0 - 2 = 8 STOP GO KNAPSACK 9 & 5 @ 8 = 9 & 5 $ 9 STOP GO 9 + 4 = 1 3 STOP GO 1 3 VS 9 = GT STOP 4 & 2 , 9 & 5 $ 1 3 STOP\n\n• ... • X 11: GO 9 VS 1 0 = LT STOP • X 12: GO 1 0 - 9 = 1 STOP • X 13: GO KNAPSACK 4 & 2 , 9 & 5 @ 1 = GO KNAPSACK 9 & 5 @ 1 = $ 0 STOP\n\nGO 2 VS 1 = GT STOP $ 0 STOP\n\n• ... • X 17: GO 0 + 3 = 3 STOP • X 18: GO 3 VS 1 3 = LT STOP\n\nD.9 TERNARY ADDITION AND MULTIPLICATION\n\nTernary addition and multiplication arises as a subproblem while solving MCM, which will be explained in the next section. They are simple extensions of addition and multiplication to three integers.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18\n\nclass TernaryAdd(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\na1, a2, a3 = args return [\n\nT(Add, (a1, a2)), T(Add, (a1 + a2, a3), 'tail')\n\n]\n\nclass TernaryMul(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\na1, a2, a3 = args return [\n\nT(Mul, (a1, a2)), T(Mul, (a1 * a2, a3), 'tail')\n\n]\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nD.10 MATRIX CHAIN MULTIPLICATION (MCM)\n\nGiven N matrices, the N − 1 subproblems are defined for each possible binary split. For the multiplication of four matrices ABCD, there are three possible binary splits: A(BCD), (AB)(CD), and (ABC)D. For each binary split, the total cost is the sum of (i) the minimum cost of computing the first group, (ii) the minimum cost of computing the second group, and (iii) the cost of multiplying the two matrices resulting from each group. Once we get the total costs of each binary split, we return choose the best split with the minimum cost. The following code implements this procedure.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\n\nclass MCM(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nmats, min_order, min_cost = args\n\n# Base cases if len(mats) == 1: return []\n\nif min_order is None:\n\n# Top-level problem l_mats, r_mats = mats[:1], mats[1:]\n\nelse:\n\n# Middle of recursion l_mats, r_mats = mats\n\nl_args = (l_mats, None, None) r_args = (r_mats, None, None) l_order, l_cost = MCM._answer(l_args) r_order, r_cost = MCM._answer(r_args) agg_cost = l_mats[0][0] * r_mats[0][0] * r_mats[-1][1] thoughts = [\n\nT(MCM, l_args), T(MCM, r_args), T(TernaryMul, (l_mats[0][0], r_mats[0][0], r_mats[-1][1])), T(TernaryAdd, (l_cost, r_cost, agg_cost)),\n\n]\n\ncost = l_cost + r_cost + agg_cost if min_cost is not None:\n\nthoughts.append(T(Compare, (cost, min_cost)))\n\nif min_cost is None or cost < min_cost:\n\nmin_cost = cost min_order = l_order, r_order\n\nif len(r_mats) > 1:\n\nnew_l_mats = l_mats + (r_mats[0],) new_r_mats = r_mats[1:] thoughts.append(\n\nT(MCM, ((new_l_mats, new_r_mats), min_order, min_cost), 'tail'))\n\nreturn thoughts\n\nThe following is an example of three-matrix MCM.\n\n• X 1: GO MCM 3 × 9 , 9 × 4 , 4 × 5 = GO MCM 3 × 9 = 3 × 9 ; 0 STOP GO MCM 9 × 4 , 4 × 5 = 9 × 4 , 4 × 5 ; 1 8 0 STOP GO 3 * 9 * 5 = 1 3 5 STOP GO 0 + 1 8 0 + 1 3 5 = 3 1 5 STOP TAIL MCM 3 × 9 , 9 × 4 | 4 ×\n\n5 ACC 3 × 9 , ( 9 × 4 , 4 × 5 ) ; 3 1 5 = THINK\n\n• ... • X 32: GO MCM 3 × 9 , 9 × 4 | 4 × 5 ACC 3 × 9 , ( 9 × 4 , 4 × 5 ) ; 3 1 5 = GO MCM 3 × 9 , 9 × 4 = 3 × 9 , 9 × 4 ; 1 0 8 STOP GO MCM 4 × 5 = 4 × 5 ; 0 STOP GO 3 * 4 * 5 = 6 0 STOP GO 1 0 8 + 0 + 6 0 = 1 6 8\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nSTOP GO 1 6 8 VS 3 1 5 = LT STOP ( 3 × 9 , 9 × 4 ) , 4 × 5 ; 1 6 8 STOP\n\n• ...\n\nD.11 SORTING\n\nAmong several sorting algorithms, we choose merge sort for our experiments with CoT and RoT. Note that WT is not relevant to the sorting algorithm since it produces the answer directly. The merge sort algorithm is simple: (i) split the given sequence to two equally sized subsequences, (ii) sort each subsequence, and (iii) merge the two sorted sequences. Since the final merge operation is quite complicated, we define the merge as a problem type.\n\n1 2\n3 4\n5 6\n7 8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n\nclass Merge(Problem): @staticmethod def thought(args) -> list[T]:\n\nl, r = args if len(l) == 0 or len(r) == 0:\n\nreturn []\n\nthoughts = [T(Compare, (l[0], r[0]))] if l[0] < r[0] and len(l) > 1:\n\nthoughts.append(T(Merge, (l[1:], r)))\n\nelif l[0] >= r[0] and len(r) > 1:\n\nthoughts.append(T(Merge, (l, r[1:])))\n\nreturn thoughts\n\nclass MergeSort(Problem):\n\n@staticmethod def thought(args) -> list[T]:\n\nif len(args) < 2: return []\n\nl_len = (len(args) + 1) // 2 l = args[:l_len] r = args[l_len:] return [\n\nT(MergeSort, l), T(MergeSort, r), T(Merge, (tuple(sorted(l)), tuple(sorted(r))), 'tail')\n\n]\n\nE FINE-TUNING GPT-3 FOR RECURSION OF THOUGHT\n\nUsing the OpenAI API, we fine-tune GPT-3 for Recursion of Thought. The goal is to learn 16-digit addition, 16-digit subtraction, 8-digit multiplication, and 8-digit division simultaneously. GPT-3’s fine-tuning API takes a dataset where each example is a prompt-completion pair in plain text. It is converted to tokens by a special tokenizer for GPT, which we cannot control. This API is not directly compatible with RoT due to several reasons.\n\n• There is no special tokens such as GO , THINK , and STOP .\n\n• The input and target sequences have to be the same. However, they are different in RoT due to the THINK token. Once THINK is produced, the RoT framework triggers the recursion process to find the subproblem’s answer and replace the THINK token with it. Therefore, the THINK token appears in the target sequences, but never in the input sequences.\n\nMoreover, the way that GPT-3 tokenizes numbers hinders the learning of arithmetic reasoning rules. GPT-3 tokenizes a multi-digit number into a set of two-digit or three-digit numbers. For example, the text 1234567 is converted to the sequence of tokens 123 45 67 . Under this tokenization scheme, the relationship between the numbers become obscured. As an example, the tokens 7 , 17 ,\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\n27 , ..., 997 all have 7 as their last digit. Since there is no direct way for a model to know that they share the same digit, it is crucial to use each digit as a token. We believe that OpenAI needs to correct this tokenization of GPT-3 for numbers.\n\nLuckily, we can mimic the RoT procedures with the API by using several tricks. First, we replace the special tokens with plain lower-case words, e.g., GO → go and STOP → stop, which are included in the vocabulary of GPT-3. Second, we add a space before each token to make sure that the GPT tokenizer separates each token. We also add space before each digit to prevent the tokenizer grouping a number into 2-to-3-digit tokens. Finally, to simulate the behavior of the THINK and STOP tokens, we derive multiple examples from each context, one for each THINK or STOP output.\n\nAs an example, context X 3 in Figure 1 is converted to the following JSON lines for GPT-3 as follows:\n\nX 3 GO 4 0 + 3 5 = GO 0 + 5 = 5 STOP\n\nGO 4 + 3 = 7 STOP\n\n7 5 STOP\n\nY 3 PAD ×7\n\nGO 0 + 5 = THINK PAD GO 4 + 3 = THINK PAD 7 5 STOP\n\n⇓\n\n1 2\n3\n\n{\"prompt\": \" go 4 0 + 3 5 =\", \"completion\": \" go 0 + 5 = think\"} {\"prompt\": \" go 4 0 + 3 5 = go 0 + 5 = 5 stop\", \"completion\": \" go 4 + 3 = think\"} {\"prompt\": \" go 4 0 + 3 5 = go 0 + 5 = 5 stop go 4 + 3 = 7 stop\", \"completion\": \"\n\n7 5 stop\"}\n\nIn the case of Without Thought (WT), each problem is simply converted into a single example:\n\nX GO 4 0 + 3 5 = 7 5 STOP Y PAD ×7\n\n7 5 STOP\n\n⇓\n\n1\n\n{\"prompt\": \" go 4 0 + 3 5 =\", \"completion\": \" 7 5 stop\"}\n\nIn both cases of RoT and WT, we fine-tune GPT-3 for 10K steps with a batch size of 256. Among the several variants of GPT-3, we use Ada which is offered at the lowest cost. Note that RoT produces multiple contexts for each problem, and each RoT context is converted to multiple training examples. For this reason, the GPT-3 fine-tuned for RoT encounters much fewer problems during training, although the number of training steps are the same.\n\nF TRAINING DETAILS OF THE TINY MODELS\n\nIn all experiments, we use a batch size of 256 and Adam optimizer Kingma & Ba (2015) with a learning rate of 0.001, i.e., the default learning rate in PyTorch. We train the Transformers for 500K steps and and decay the learning rate by half every 50K steps. Since the LSTMs converge slower than the Transformers, we train them for 800K steps and decay the learning rate by half every 100K steps. At every 20K steps, we evaluate the model on a test set of 30K problems, and if a model reaches a perfect accuracy of 1.0, we do not train the model further. The models can be trained on a single GPU with 12GB memory.\n\nG EFFICIENT EVALUATION OF ROT\n\nAt the problem scales that RoT is tested, solving a single problem can require hundreds of thousands of tokens. Therefore, we have to develop an efficient evaluation algorithm. Since the models are\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\ndeterministic, we do not need to evaluate the same context multiple times. Therefore, given a test set, we first find all unique contexts that should appear while solving the test set. Note that there would be a lot of duplicate contexts, especially for the base cases. These unique contexts can be evaluated in parallel. We evaluate each context in a similar manner to the training. For each context, we construct the target sequence, as we described in §3.2, and check if the model outputs the target sequence correctly, given the GT context. In case of GPT-3, we convert the contexts and target sequences as described in Appendix E to feed into the API. Once every context is individually evaluated, the next step is to aggregate the results. In this aggregation stage, a (sub)problem is marked correct only if its subproblems are all marked correct. This way, we efficiently evaluate RoT with minimal cost.\n\nH FAILURE CASE ANALYSIS OF GPT-3 WITH ROT\n\nIn this section we provide some failure cases of RoT in the GPT-3 experiments for further insights. Although the model achieves an accuracy of over 99% in almost every case, there are several problems where it makes mistakes. Nonetheless, only a few characters are wrong in each context.\n\nThe most common type of error is repeating the same character for a wrong number of times. The following is the examples of this error type.\n\nPrompt\n\nCompletion\n\nModel Output\n\nPrompt\n\nCompletion\n\nModel Output\n\ngo 1 6 4 3 4 vs 1 0 1 1 6 = go 1 vs 1 = eq stop\n\ngo 6 4 3 4 vs 1 1 6 = think\n\ngo 6 4 3 4 vs 1 1 1 6 = think\n\ngo 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 4 4 3 0 2 5 3 6 0 1 7\n\n= go 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 7 = 3 3 6 6 6 1 4 6 6 2 7 7 9 1 3 7 stop go 4 8 0 9 4 4 9 5 1 8 2 5 5 9 1 * 4 4 3 0 2 5 3 6 0 1 = 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 stop\n\ntail 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 0\n\n+ 3 3 6 6 6 1 4 6 6 2 7 7 9 1 3 7 = think\n\ntail 2 1 3 0 7 0 8 1 0 4 7 0 8 0 9 6 0 5 1 7 0 3 1 9 1 0\n\n+ 3 3 6 6 6 6 1 4 6 6 2 7 7 9 1 3 7 = think\n\nPrompt\n\ngo 1 0 5 4 6 2 0 * 1 5 = go 1 0 5 4 6 2 0 * 5 = 5 2 7 3\n\n1 0 0 stop go 1 0 5 4 6 2 0 * 1 = 1 0 5 4 6 2 0 stop\n\nCompletion\n\nModel Output\n\ntail 1 0 5 4 6 2 0 0 + 5 2 7 3 1 0 0 = think\n\ntail 1 0 5 4 6 2 0 0 0 + 5 2 7 3 1 0 0 = think\n\nAnother common case is incorrectly counting the number of digits while comparing two numbers. When comparing numbers following our recursive procedure, the model has to immediately return the answer if the number of digits are different. In the following example, the first number has seven digits, and the second one has six digits.\n\nPrompt\n\ngo 1 0 6 9 0 3 4 vs 8 9 1 5 0 6 =\n\nCompletion\n\ngt stop\n\nModel Output\n\ngo 1 vs 8\n\nAlthough the model should directly output the answer, i.e., gt stop, it decides that the number of digits are the same and moves on to the next step, which is to compare the most significant digits. This error is caused by the specific recursive procedure of our choice. Since we assume that the model would easily figure out the difference in the numbers of digits, we did not add explicit steps to count the digits. Although the model detects the difference in most cases, it turns out to be a\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nrelatively challenging operation. A possible solution is to teach the model to explicitly count the number of digits and compare them.\n\nI CONTEXT LENGTH DISTRIBUTION\n\nIn Figure 4, we present the distributions of context lengths for each problem type. We compare the context lengths of RoT and CoT. For each configuration, we randomly sample 10K contexts from the training distribution and plot the histogram of their lengths. The graphs show that the context sizes of CoT are many orders of magnitude larger than RoT. In theory, the total number of tokens to generate for each problem is identical in both RoT and CoT (if we do not count the THINK tokens). However, RoT’s context sizes are much smaller since it utilizes multiple contexts.\n\nAnother advantage of RoT is the utilization of dynamic programming. Since we can easily cache the duplicate computations of RoT as explained in Appendix G, we can drastically reduce the amount of token generation if there is a redundant structure in the problem. The amount of tokens to generate for each problem is plotted in Figure 5. The benefit is especially prominent in the algorithmic problems. For example, finding the LCS of two 32-digit sequences results in more than 1018 tokens if we naively use CoT or RoT. If we use dynamic programming with RoT, we can efficiently solve the same problem with much less cost.\n\nJ TRANSFORMERS ARE POWERFUL SORTING MACHINES\n\nIn fact, the first algorithmic task that we tested is sorting since it has been widely used as a benchmark for algorithmic reasoning (Reed & de Freitas, 2016; Cai et al., 2017; Pierrot et al., 2019). However, we find that Transformers are incredibly good at sorting, even in the WT setting. Figure 6 shows the sorting experiment. For CoT and RoT, we train the merge sort algorithm. Interestingly, WT easily achieves a perfect score in sorting 64 three-digit numbers. Also the training converges much faster than RoT. The Transformer architecture, more specifically the attention mechanism, seems to be perfectly suited for the sorting operation.\n\nK THE EXACT VALUES OF FIGURE 3\n\nTable 2-5 show the exact values of the graphs in Figure 3. Except for the GPT-3 experiments in Table 2, we report the average and the standard deviation of eight runs. Each GPT-3 experiment is done only once.\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: The distributions of context lengths.\n\n34\n\n$GGGLJLW5R7$GGGLJLW&R7$GGGLJLW5R7$GGGLJLW&R76XEGLJLW5R76XEGLJLW&R76XEGLJLW5R76XEGLJLW&R70XOGLJLW5R70XOGLJLW&R70XOGLJLW5R70XOGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7/&6OHQJWK5R7H/&6OHQJWK&R7/&6OHQJWK5R7H/&6OHQJWK&R7/36OHQJWK5R7/36OHQJWK&R7/36OHQJWK5R7H/36OHQJWK&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R70&0PDWULFHV5R70&0PDWULFHV&R70&0PDWULFHV5R7H0&0PDWULFHV&R7Under review as a conference paper at ICLR 2023\n\nFigure 5: The distribution of the total number of tokens to produce in order to solve each problem. RoT can utilize dynamic programming to reduce redundant computations.\n\n35\n\n$GGGLJLW5R7$GGGLJLW&R7$GGGLJLW5R7$GGGLJLW&R76XEGLJLW5R76XEGLJLW&R76XEGLJLW5R76XEGLJLW&R70XOGLJLW5R70XOGLJLW&R70XOGLJLW5R70XOGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7'LYGLJLW5R7'LYGLJLW&R7/&6OHQJWK5R7H/&6OHQJWK&R7/&6OHQJWK5R7H/&6OHQJWK&R7/36OHQJWK5R7/36OHQJWK&R7/36OHQJWK5R7H/36OHQJWK&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R7.QDSVDFNLWHPV5R7.QDSVDFNLWHPV&R70&0PDWULFHV5R70&0PDWULFHV&R70&0PDWULFHV5R7H0&0PDWULFHV&R7Under review as a conference paper at ICLR 2023\n\nFigure 6: Sorting experiment with the tiny Transformer.\n\nProblem\n\nDifficulty\n\nWT CoT\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\nLCS\n\nLPS\n\n0-1 Knapsack\n\nMCM\n\n32-digit 48-digit\n\n32-digit 48-digit\n\n8-digit 16-digit\n\n8-digit 16-digit\n\nlength 16 length 24\n\nlength 24 length 40\n\n4 items 6 items\n\n3 matrices 4 matrices\n\n0.991 0.853\n\n0.991 0.886\n\n0.337 0.098\n\n0.363 0.123\n\n0.980 0.832\n\n0.995 0.800\n\n0.945 0.634\n\n0.481 0.110\n\nRoT − 0.998 − 0.995 − 0.998 − 0.998 − 0.999 − 0.994 − 1.000 − 0.989 − 0.995 − 0.998 − 1.000 − 0.974 − 0.999 − 1.000 − 0.997 − 0.992\n\nTable 2: The exact values of the GPT-3 experiments in Figure 3a.\n\n36\n\nRI,WHPV$FFXUDF\\6RUWLQJ:LWKRXW7KRXJKW&KDLQRI7KRXJKW5HFXUVLRQRI7KRXJKWUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\n8-digit 16-digit 24-digit 32-digit 40-digit 48-digit 56-digit 64-digit\n\n8-digit 16-digit 24-digit 32-digit 40-digit 48-digit 56-digit 64-digit\n\n2-digit 4-digit 8-digit 12-digit 16-digit 20-digit 24-digit 28-digit 32-digit\n\n2-digit 4-digit 8-digit 12-digit 16-digit 20-digit 24-digit 28-digit 32-digit\n\nWT 0.863 ± 0.265 0.370 ± 0.475 0.336 ± 0.430 0.455 ± 0.458 0.119 ± 0.316 0.082 ± 0.216 0.105 ± 0.277 0.000 ± 0.000 0.982 ± 0.006 0.705 ± 0.411 0.238 ± 0.412 0.221 ± 0.385 0.426 ± 0.433 0.114 ± 0.303 0.116 ± 0.307 0.161 ± 0.282 1.000 ± 0.000 0.817 ± 0.023 0.340 ± 0.032 0.169 ± 0.015 0.104 ± 0.016 0.048 ± 0.020 0.033 ± 0.017 0.014 ± 0.006 0.012 ± 0.001 1.000 ± 0.000 0.978 ± 0.008 0.354 ± 0.029 0.186 ± 0.009 0.128 ± 0.011 0.087 ± 0.012 0.075 ± 0.005 0.059 ± 0.007 0.048 ± 0.008\n\nCoT 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000\n\n1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000\n\n1.000 ± 0.000 1.000 ± 0.000\n\n1.000 ± 0.000 1.000 ± 0.000\n\nRoT 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.001 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.999 ± 0.001 − 0.999 ± 0.001 − 0.999 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.999 ± 0.000 − 0.999 ± 0.000\n\nTable 3: The exact values of the Transformer experiments in Figure 3b (arithmetic problems).\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nLCS\n\nLPS\n\n0-1 Knapsack\n\nMCM\n\nlength 3 length 4 length 8 length 12 length 16 length 20 length 24 length 28 length 32\n\nlength 4 length 7 length 8 length 16 length 24 length 32 length 40 length 48 length 56\n\n2 items 4 items 6 items 8 items 10 items 12 items\n\n2 matrices 4 matrices 6 matrices 8 matrices 10 matrices 12 matrices\n\nWT 1.000 ± 0.000 0.997 ± 0.008 0.999 ± 0.002 0.965 ± 0.025 0.880 ± 0.035 0.759 ± 0.043 0.622 ± 0.038 0.484 ± 0.043 0.375 ± 0.030 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 0.999 ± 0.001 0.950 ± 0.019 0.788 ± 0.019 0.608 ± 0.023 0.477 ± 0.030 0.365 ± 0.029 1.000 ± 0.000 0.966 ± 0.006 0.849 ± 0.007 0.640 ± 0.242 0.481 ± 0.279 0.435 ± 0.252 0.973 ± 0.009 0.177 ± 0.069 0.088 ± 0.029 0.033 ± 0.025 0.051 ± 0.032 0.026 ± 0.011\n\nCoT 1.000 ± 0.000\n\n1.000 ± 0.000 1.000 ± 0.000\n\n1.000 ± 0.000 1.000 ± 0.000\n\n1.000 ± 0.000\n\nRoT\n\n− − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.999 ± 0.000 − 0.999 ± 0.000 −\n− − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.999 ± 0.001 − 0.998 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.988 ± 0.029 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.998 ± 0.001 − 0.996 ± 0.002\n\nTable 4: The exact values of the Transformer experiments in Figure 3b (algorithmic problems).\n\n38\n\nUnder review as a conference paper at ICLR 2023\n\nProblem\n\nDifficulty\n\nAddition\n\nSubtraction\n\nMultiplication\n\nDivision\n\n2-digit 4-digit 6-digit 8-digit 10-digit 12-digit 14-digit 16-digit\n\n2-digit 4-digit 6-digit 8-digit 10-digit 12-digit 14-digit 16-digit\n\n2-digit 3-digit 4-digit 5-digit 6-digit 7-digit 8-digit\n\n1-digit 2-digit 3-digit 4-digit 5-digit 6-digit 7-digit 8-digit\n\nWT 1.000 ± 0.000 0.642 ± 0.305 0.005 ± 0.008 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 1.000 ± 0.000 0.776 ± 0.179 0.006 ± 0.001 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 0.000 ± 0.000 1.000 ± 0.000 0.855 ± 0.044 0.636 ± 0.061 0.338 ± 0.063 0.270 ± 0.030 0.162 ± 0.025 0.138 ± 0.025 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.001 0.891 ± 0.072 0.516 ± 0.077 0.308 ± 0.069 0.192 ± 0.028 0.115 ± 0.015\n\nCoT 1.000 ± 0.000 1.000 ± 0.001 0.997 ± 0.005 0.905 ± 0.155 0.795 ± 0.341\n\n1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 0.896 ± 0.252 0.443 ± 0.377\n\n1.000 ± 0.000\n\n1.000 ± 0.000\n\nRoT 1.000 ± 0.000 1.000 ± 0.000 0.999 ± 0.000 0.999 ± 0.001 0.986 ± 0.024 − 0.871 ± 0.275 − 0.358 ± 0.430 − 0.120 ± 0.202 1.000 ± 0.000 1.000 ± 0.000 1.000 ± 0.000 0.994 ± 0.016 0.908 ± 0.236 − 0.507 ± 0.398 − 0.295 ± 0.406 − 0.101 ± 0.137 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.987 ± 0.008 − 0.896 ± 0.105 − 0.670 ± 0.208 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 1.000 ± 0.000 − 0.998 ± 0.004 − 0.996 ± 0.007 − 0.958 ± 0.036 − 0.914 ± 0.090\n\nTable 5: The exact values of the LSTM experiments in Figure 3c.\n\n39",
    "reference": "# Summary Of The Paper\n\nThis paper demonstrates that for simple algorithmic problems (arithmetic), language models can be taught to split problems into multiple subproblems, which can then be fed back to the LM to be solved independently. With very small models this can achieve great performance on these tasks, and it circumvents the limit in context-length of existing transformer models.\n\n# Strength And Weaknesses\n\nMy main concern with this approach is that the paper does not discuss how to apply this approach to more general problems. The problems considered here are very simple, and an algorithm to solve them needs to be known in order to generate the training data. So, in its current form this does not enable any new abilities as we could use the base algorithm instead of using expensive LMs.\n\nIn terms of the insight from a learning point-of-view, I also do not see anything surprising in this work. After decomposing the problem manually (i.e. by writing an algorithm to generate training data), the learning task is pretty simple (for addition, for example, all the model has to do is to extract the last digit from two numbers). So the high accuracy of the resulting method is not that surprising.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing style is a bit \"flashy\", which gives room for miscommunications. For example, the paper opens with the following statement:\n\n\"Although neural networks have achieved amazing results on various domains, e.g., images, texts,\naudios, videos, games, etc., nearly all of them are classified as System 1 tasks (Kahneman, 2013), ...\"\n\nAfaik, this statement is not supported by the literature, and in particular not by the given reference. Following Kahneman's theory, tasks like playing the board game go would certainly require system 2 thinking. IMO, dropping the first paragraph would help the paper.\n\nThe paper also makes claims such as \"the length of CoT can grow rapidly with the problem’s complexity\", without further explanation and I am not even sure what exactly this means.\n\n# Summary Of The Review\n\nThis paper demonstrates that decomposing simple algorithmic problems can circumvent the limit in context length language models.\n\nHowever, the technique only seems to work for problems for which we already have algorithms, and it is unclear to me if this could be extended to more general problems. The writing style should be improved.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "A THEORY OF REPRESENTATION LEARNING IN NEURAL NETWORKS GIVES A DEEP GENERALISATION OF KERNEL METHODS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe successes of modern deep machine learning methods are founded on their ability to transform inputs across multiple layers to build good high-level representations. It is therefore critical to understand this process of representation learning. However, standard theoretical approaches (formally NNGPs) involving infinite width limits eliminate representation learning. We therefore develop a new infinite width limit, the Bayesian representation learning limit, that exhibits representation learning mirroring that in finite-width models, yet at the same time, retains some of the simplicity of standard infinite-width limits. In particular, we show that Deep Gaussian processes (DGPs) in the Bayesian representation learning limit have exactly multivariate Gaussian posteriors, and the posterior covariances can be obtained by optimizing an interpretable objective combining a loglikelihood to improve performance with a series of KL-divergences which keep the posteriors close to the prior. We confirm these results experimentally in wide but finite DGPs. Next, we introduce the possibility of using this limit and objective as a flexible, deep generalisation of kernel methods, that we call deep kernel machines (DKMs). Like most naive kernel methods, DKMs scale cubically in the number of datapoints. We therefore use methods from the Gaussian process inducing point literature to develop a sparse DKM that scales linearly in the number of datapoints. Finally, we extend these approaches to NNs (which have non-Gaussian posteriors) in the Appendices.\n\n1\n\nINTRODUCTION\n\nThe successes of modern machine learning methods from neural networks (NNs) to deep Gaussian processes (DGPs Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) is based on their ability to use depth to transform the input into high-level representations that are good for solving difficult tasks (Bengio et al., 2013; LeCun et al., 2015). However, theoretical approaches using infinite limits to understand deep models struggle to capture representation learning. In particular, there are two broad families of infinite limit, and while they both use kernel-matrix-like objects they are ultimately very different. The neural network Gaussian process (NNGP Neal, 1996; Lee et al., 2017; Matthews et al., 2018) applies to Bayesian models like Bayesian neural networks (BNNs) and DGPs and describes the representations at each layer (formally, the NNGP kernel is raw second moment of the activities). In contrast, the neural tangent kernel (NTK Jacot et al., 2018) is a very different quantity that involves gradients, and describes how predictions at all datapoints change if we do a gradient update on a single datapoint. As such, the NNGP and NTK are suited to asking very different theoretical questions. For instance, the NNGP is better suited to understanding the transformation of representations across layers, while the NTK is better suited to understanding how predictions change through NN training.\n\nWhile challenges surrounding representation learning have recently been addressed in the NTK setting Yang & Hu (2020), we are the first to address this challenge in the NNGP setting.\n\nAt the same time, kernel methods (Smola & Sch ̈olkopf, 1998; Shawe-Taylor & Cristianini, 2004; Hofmann et al., 2008) were a leading machine learning approach prior to the deep learning revolution Krizhevsky et al. (2012). However, kernel methods were eclipsed by deep NNs because depth\n\n1\n\ngives NNs the flexibility to learn a good top-layer representation (Aitchison, 2020). In contrast, in a standard kernel method, the kernel (or equivalently the representation) is highly inflexible — there are usually a few tunable hyperparameters, but nothing that approaches the enormous flexibility of the top-layer representation in a deep model. There is therefore a need to develop flexible, deep generalisations of kernel method. Remarkably, our advances in understanding representation learning in DGPs give such a flexible, deep kernel method.\n\n2 CONTRIBUTIONS\n\n• We present a new infinite width limit, the Bayesian representation learning limit, that retains representation learning in deep Bayesian models including DGPs. The key insight is that as the width goes to infinity, the prior becomes stronger, and eventually overwhelms the likelihood. We can fix this by rescaling the likelihood to match the prior. This rescaling can be understood in a Bayesian context as copying the labels (Sec. 4.3).\n\n• We show that in the Bayesian representation learning limit, DGP posteriors are exactly λ, is the activation\n\nzero-mean multivariate Gaussian, P (cid:0)f l of the λth feature in layer l for all inputs (Sec. 4.4 and Appendix D).\n\nλ|X, y(cid:1) = N (cid:0)f l\n\n(cid:1) where f l\n\nλ; 0, Gl\n\n• We show that the posterior covariances can be obtained by optimizing the “deep kernel\n\nmachine objective”,\n\nL(G1, . . . , GL) = log P (Y|GL) − (cid:80)L\n\nl=1νl DKL (N (0, Gl)∥N (0, K(Gl−1))) ,\n\nwhere Gl are the posterior covariances, K(Gl−1) are the kernel matrices, and νl accounts for any differences in layer width (Sec. 4.3).\n\n• We give an interpretation of this objective, with log P (Y|GL) encouraging improved performance, while the KL-divergence terms act as a regulariser, keeping posteriors, N (0, Gl), close to the prior, N (0, K(Gl−1)) (Sec. 4.5).\n\n• We introduce a sparse DKM, which takes inspiration GP inducing point literature to obtain a practical, scalable method that is linear in the number of datapoints. In contrast, naively computing/optimizing the DKM objective is cubic in the number of datapoints (as with most other naive kernel methods; Sec. 4.7).\n\n• We extend these results to BNNs (which have non-Gaussian posteriors) in Appendix A.\n\n3 RELATED WORK\n\nOur work is focused on DGPs and gives new results such as the extremely simple multivariate Gaussian form for DGP true posteriors. As such, our work is very different from previous work on NNs, where such results are not available. There are at least three families of such work. First, there is recent work on representation learning in the very different NTK setting (Jacot et al., 2018; Yang, 2019; Yang & Hu, 2020) (see Sec. 1). In contrast, here we focus on NNGPs (Neal, 1996; Williams, 1996; Lee et al., 2017; Matthews et al., 2018; Novak et al., 2018; Garriga-Alonso et al., 2018; Jacot et al., 2018), where the challenge of representation learning has yet to be addressed. Second, there is a body of work using methods from physics to understand representation learning in neural networks (Antognini, 2019; Dyer & Gur-Ari, 2019; Hanin & Nica, 2019; Aitchison, 2020; Li & Sompolinsky, 2020; Yaida, 2020; Naveh et al., 2020; Zavatone-Veth et al., 2021; Zavatone-Veth & Pehlevan, 2021; Roberts et al., 2021; Naveh & Ringel, 2021; Halverson et al., 2021). This work is focuses on perturbational, rather than variational methods. Third, there is a body of theoretical work including (Mei et al., 2018; Nguyen, 2019; Sirignano & Spiliopoulos, 2020a;b; Nguyen & Pham, 2020) which establishes properties such as convergence to the global optimum. This work is focused on two-layer (or one-hidden layer network) networks, and like the NTK, considers learning under SGD rather than Bayesian posteriors.\n\nAnother related line of work uses kernels to give a closed-form expression for the weights of a neural network, based on a greedy, layerwise objective (Wu et al., 2022). This work differs in that it uses the HSIC objective, and therefore does not have a link to DGPs or Bayesian neural networks, and in that it uses a greedy-layerwise objective, rather than end-to-end gradient descent.\n\n2\n\nLayer 1\n\nLayer 2\n\nLayer 3\n\nX\n\nX\n\nX\n\nG0\n\nG0\n\nF1\n\nF1\n\nG1\n\nG1\n\nF2\n\nF2\n\nG2\n\nG2\n\nF3\n\nF3\n\nG3\n\nG3\n\nY\n\nY\n\nY\n\nFigure 1: The graphical model structure for each of our generative models for L = 3. Top. The standard model (Eq. 1), written purely in terms of features, Fl. Middle. The standard model, including Gram matrices as random variables (Eq. 5) Bottom. Integrating out the activations, Fl,\n\n4 RESULTS\n\nWe start by defining a DGP, which contains Bayesian NN (BNNs) as a special case (Appendix A). This model maps from inputs, X ∈ RP ×ν0, to outputs, Y ∈ RP ×νL+1, where P is the number of input points, ν0 is the number of input features, and νL+1 is the number of output features. The model has L intermediate layers, indexed l ∈ {1, . . . , L}, and at each intermediate layer there are Nl features, Fl ∈ RP ×Nl. Both Fl and Y can be written as a stack of vectors, )\n\nY = (y1 y2\n\nFl = (f l\n\n· · · yνL+1 ),\n\n· · ·\n\nf l\n\n1\n\n2\n\nf l Nl\n\nλ ∈ RP gives the value of one feature and yλ ∈ RP gives the value of one output for all where f l P input points. The features, F1, . . . , FL, and (for regression) the outputs, Y, are sampled from a Gaussian process (GP) with a covariance which depends on the previous layer features (Fig. 1 top),\n\nP (Fl|Fl−1) = (cid:81)Nl\n\nP (Y|FL) = (cid:81)νL+1\n\nλ=1N (cid:0)f l λ; 0, K(G(Fl−1))(cid:1) λ=1 N (cid:0)yλ; 0, K(G(FL)) + σ2I(cid:1) .\n\n(1a)\n\n(1b)\n\nNote we only use the regression likelihood to give a concrete example; we could equally use an alternative likelihood e.g. for classification (Appendix B). The distinction between DGPs and BNNs arises through the choice of K(·) and G(·). For BNNs, see Appendix A. For DGPs, G(·), which takes the features and computes the corresponding P × P Gram matrix, is\n\nG(Fl−1) = 1\n\nNl−1\n\n(cid:80)Nl−1\n\nλ=1 f l−1\n\nλ\n\n(f l−1\n\nλ\n\n)T = 1\n\nNl−1\n\nFl−1FT\n\nl−1.\n\n(2)\n\nNow, we introduce random variables representing the Gram matrices, Gl−1 = G(Fl−1), where Gl−1 is a random variable representing the Gram matrix at layer l − 1, whereas G(·) is a deterministic function that takes features and computes the corresponding Gram matrix using Eq. (2). Finally, K(·), transforms the Gram matrices, Gl−1 to the final kernel. Many kernels of interest are isotropic, meaning they depend only on the normalized squared distance between datapoints, Rij, Kisotropic;ij(Gl−1) = kisotropic (Rij(Gl−1)) .\n\n(3)\n\nImportantly, we can compute this squared distance from Gl−1, without needing Fl−1, (cid:1)2\n\n(cid:1)2\n\n(cid:80)N\n\n(cid:80)N\n\n(cid:1)2(cid:1)\n\n(cid:0)(cid:0)Fiλ\n\n− 2FiλFjλ + (cid:0)Fjλ\n\n(cid:0)Fiλ − Fjλ\n\nRij(G) = 1\n\nN\n\n= 1 N\n\nλ=1\n\nλ=1\n\n= Gii − 2Gij + Gjj,\n\n(4)\n\nwhere λ indexes features, i and j index datapoints and we have omitted the layer index for simplicity. Importantly, we are not restricted to isotropic kernels: other kernels that depend only on the Gram matrix, such as the arccos kernels from the infinite NN literature (Cho & Saul, 2009) can also be used (for further details, see Aitchison et al., 2020).\n\n4.1 BNN AND DGP PRIORS CAN BE WRITTEN PURELY IN TERMS OF GRAM MATRICES\n\nNotice that Fl depends on Fl−1 only through Gl−1 = G(Fl−1), and Y depends on FL only through GL = G(FL) (Eq. 1). We can therefore write the graphical model in terms of those Gram matrices (Fig. 1 middle).\n\nP (Fl|Gl−1) = (cid:81)Nl\n\nλ=1N (cid:0)f l\n\nλ; 0, K(Gl−1)(cid:1)\n\nP (Gl|Fl) = δ (Gl − G(Fl)) P (Y|GL) = (cid:81)νL+1\n\nλ=1 N (cid:0)yλ; 0, K(GL) + σ2I(cid:1) .\n\n3\n\n(5a)\n\n(5b)\n\n(5c)\n\nwhere δ is the Dirac-delta, and G0 depends on X (e.g. G0 = 1 ν0 have used a regression likelihood, but other likelihoods could also be used.\n\nXXT ). Again, for concreteness we\n\nNow, we can integrate Fl out of the model, in which case, we get an equivalent generative model written solely in terms of Gram matrices (Fig. 1 bottom), with\n\nP (Gl|Gl−1) =\n\n(cid:90)\n\ndFl P (Gl|Fl) P (Fl|Gl−1) ,\n\n(6)\n\nand with the usual likelihood (e.g. Eq. 5c). This looks intractable (and indeed, in general it is intractable). However for DGPs, an analytic form is available. In particular, note the Gram matrix (Eq. 2) is the outer product of IID Gaussian distributed vectors (Eq. 1a). This matches the definition of the Wishart distribution (Gupta & Nagar, 2018), so we have,\n\nP (Gl|Gl−1) = Wishart\n\nK(Gl−1), Nl\n\n(cid:17)\n\n(cid:16)\n\nGl; 1 Nl log |Gl|− Nl\n\nlog P (Gl|Gl−1) = Nl−P −1\n\n2\n\n2 log |K(Gl−1)|− Nl\n\n2 Tr (cid:0)K−1(Gl−1)Gl\n\n(7)\n\n(cid:1) + const .\n\nThis distribution over Gram matrices is valid for DGPs of any width (though we need to be careful in the low-rank setting where Nl < P ). We are going to leverage these Wishart distributions to understand the behaviour of the Gram matrices in the infinite width limit.\n\n4.2 STANDARD INFINITE WIDTH LIMITS OF DGPS LACK REPRESENTATION LEARNING\n\nWe are now in a position to take a new viewpoint on the DGP analogue of standard NNGP results (Lee et al., 2017; Matthews et al., 2018; Hron et al., 2020; Pleiss & Cunningham, 2021). We can then evaluate the log-posterior for a model written only in terms of Gram matrices,\n\nlog P (G1, . . . , GL|X, Y) = log P (Y|GL) + (cid:80)L\n\nl=1 log P (Gl|Gl−1) + const .\n\n(8)\n\nThen we take the limit of infinite width,\n\nNl = N νl\n\nfor\n\nl ∈ {1, . . . , L}\n\nwith\n\nN → ∞.\n\n(9)\n\nThis limit modifies log P (Gl|Gl−1) (Eq. 7), but does not modify G1, . . . , GL in Eq. (8) as we get to choose the values of G1, . . . , GL at which to evaluate the log-posterior. Specifically, the logprior, log P (Gl|Gl−1) (Eq. 7), scales with Nl and hence with N . To get a finite limit, we therefore need to divide by N ,\n\nlim N→∞\n\n1\n\nN log P (Gl|Gl−1) = νl\n\n2\n\n(cid:0)log (cid:12)\n\n(cid:12)K−1(Gl−1)Gl\n\n(cid:12) (cid:12) − Tr (cid:0)K−1(Gl−1)Gl\n\n(cid:1)(cid:1) + const\n\n= −νl DKL (N (0, Gl)∥N (0, K(Gl−1))) + const .\n\n(10)\n\nAnd remarkably this limit can be written as the KL-divergence between two multivariate is constant wrt N (Eq. 5c), so Gaussians. 1\nlimN→∞\n\nN log P (Y|GL) = 0. The limiting log-posterior is thus,\n\nthe log likelihood,\n\nlog P (Y|GL),\n\nIn contrast,\n\nlim N→∞\n\n1\n\nN log P (G1, . . . , GL|X, Y) = −(cid:80)L\n\nl=1νl DKL (N (0, Gl)∥N (0, K(Gl−1))) + const .\n\nThis form highlights that the log-posterior scales with N , so in the limit as N → ∞, the posterior converges to a point distribution at the global maximum, denoted G∗ L, (see Appendix C for a formal discussion of weak convergence),\n\n1, . . . , G∗\n\n(11)\n\nlim N→∞\n\nP (G1, . . . , GL|X, Y) = (cid:81)L\n\nl=1δ (Gl − G∗\n\nl ) .\n\n(12)\n\nMoreover, it is evident from the KL-divergence form for the log-posterior (Eq. 11) that the unique global maximum can be computed recursively as G∗ XXT . Thus, the limiting posterior over Gram matrices does not depend on the training targets, so there is no possibility of representation learning (Aitchison, 2020). This is deeply problematic as the successes of modern deep learning arise from flexibly learning good top-layer representations.\n\nl−1), with e.g. G∗\n\nl = K(G∗\n\n0 = 1\n\nν0\n\n4\n\n4.3 THE BAYESIAN REPRESENTATION LEARNING LIMIT\n\nIn the previous section, we saw that standard infinite width limits eliminate representation learning because as N → ∞ the log-prior terms, log P (Gl|Gl−1), in Eq. (8) dominated the log-likelihood, P (Y|GL), and the likelihood is the only term that depends on the labels. We therefore introduce the “Bayesian representation learning limit” which retains representation learning. The Bayesian representation learning limit sends the number of output features, NL+1, to infinity as the layerwidths go to infinity,\n\nNl = N νl\n\nfor\n\nl ∈ {1, . . . , L + 1}\n\nwith\n\nN → ∞.\n\n(13)\n\nImportantly, the Bayesian representation learning limit gives a valid probabilistic model with a welldefined posterior, arising from the prior, (Eq. 6) and a likelihood which assumes each output channel is IID,\n\nP ( ̃Y|GL) = (cid:81)NL+1\n\nλ=1 N (cid:0) ̃yλ; 0, K(GL) + σ2I(cid:1) .\n\n(14)\n\nwhere ̃Y ∈ RP ×NL+1 is infinite width (Eq. 13) whereas the usual DGP data, Y ∈ RP ×νL+1, is finite width. Of course, infinite-width data is unusual if not unheard-of. In practice, real data, Y ∈ RP ×νL+1, almost always has a finite number of features, νL+1. How do we apply the DKM to such data? The answer is to define ̃Y as N copies of the underlying data, Y, i.e. ̃Y = (Y · · · Y). As each channel is assumed to be IID (Eq. 5c and 14) the likelihood is N times larger,\n\nlog P ( ̃Y|GL) = N log P (Y|GL) ,\n\n(15)\n\nThe log-posterior in the Bayesian representation learning limit is very similar to the log-posterior in the standard limit (Eq. 11). The only difference is that the likelihood, log P ( ̃Y|GL) now scales with N , so it does not disappear as we take the limit, allowing us to retain representation learning,\n\nL(G1, . . . , GL) = lim\n\nN→∞\n\n1\n\nN log P (G1, . . . , GL|X, ̃Y) + const,\n\n(16)\n\n= log P (Y|GL) − (cid:80)L\n\nl=1νl DKL (N (0, Gl)∥N (0, K(Gl−1))) .\n\n1, . . . , G∗\n\nHere, we denote the limiting log-posterior as L(G1, . . . , GL), and this forms the DKM objective. Again, as long as the global maximum of the DKM objective is unique, the posterior is again a point distribution around that maximum (Eq. 12). Of course, the inclusion of the likelihood term means that the global optimum G∗ L cannot be computed recursively, but instead we need to optimize, e.g. using gradient descent (see Sec. 4.7). Unlike in the standard limit (Eq. 11), it is no longer possible to guarantee uniqueness of the global maximum. We can nonetheless say that the posterior converges to a point distribution as long as the global maximum of L(G1, . . . , GL) is unique, (i.e. we can have any number of local maxima, as long as they all lie below the unique global maximum). We do expect the global maximum to be unique in most practical settings: we know the maximum is unique when the prior dominates (Eq. 11), in Appendix J, we prove uniqueness for linear models, and in Appendix K, we give a number of experiments in nonlinear models in which optimizing from very different initializations found the same global maximum, indicating uniqueness in practical settings.\n\n4.4 THE EXACT DGP POSTERIOR OVER FEATURES IS MULTIVARIATE GAUSSIAN\n\nAbove, we noted that the DGP posterior over Gram matrices in the Bayesian representation learning limit is a point distribution, as long as the DKM objective has a unique global maximum. Remarkably, in this setting, the corresponding posterior over features is multivariate Gaussian (see Appendix D for the full derivation),\n\nP (cid:0)f l\n\nλ|X, y(cid:1) = N (cid:0)f l\n\nλ; 0, G∗\n\nl\n\n(cid:1)\n\n(17)\n\nWhile such a simple result might initially seem remarkable, it should not surprise us too much. In particular, the prior is Gaussian (Eq. 1). In addition, in Fig. 1 (middle), we saw that the next layer features depend on the current layer features only through the Gram matrices, which are just the raw second moment of the features, Eq. (2). Thus, in effect the likelihood only constrains the raw second moments of the features. Critically, that constraints on the raw second moment are tightly connected to Gaussian distributions: under the MaxEnt framework, a Gaussian distribution arises by maximizing the entropy under constraints on the raw second moment of the features (Jaynes, 2003).\n\n5\n\nThus it is entirely plausible that a Gaussian prior combined with a likelihood that “constrains” the raw second moment would give rise to Gaussian posteriors (though of course this is not a proof; see Appendix D for the full derivation).\n\nFinally, note that we appear to use Gl or G∗ l in Eq. (2) and as the posterior covariance in the Bayesian representation learning limit (Eq. 17). In the infinite limit, these two uses are consistent. In particular, consider the value of Gl defined by Eq. (2) under the posterior,\n\nl in two separate senses: as 1\n\nFlFT\n\nNl\n\nGl = lim\n\nN→∞\n\n1 Nl\n\n(cid:80)Nl\n\nλ=1f l\n\nλ(f l\n\nλ)T = E\n\nP(f l\n\nλ|X,y)\n\n(cid:2)f l\n\nλ(f l\n\nλ)T (cid:3) = G∗ l .\n\n(18)\n\nThe second equality arises by noticing that we are computing the average of infinitely many terms, f l λ)T , which are IID under the true posterior (Eq. 17), so we can apply the law of large numbers, λ(f l and the final expectation arises by computing moments under Eq. (17).\n\n4.5 THE DKM OBJECTIVE GIVES INTUITION FOR REPRESENTATION LEARNING\n\nThe form for the DKM objective in Eq. (16) gives a strong intuition for how representation learning occurs in deep networks. In particular, the likelihood, log P (Y|GL), encourages the model to find a representation giving good performance on the training data. At the same time, the KL-divergence terms keep the posterior over features, N (0, Gl), (Eq. 17) close to the prior N (0, K(Gl−1)) (Eq. 1a). This encourages the optimized representations, Gl, to lie close to their value under the standard infinite-width limit, K(Gl−1). We could use any form for the likelihood including classification and regression, but to understand how the likelihood interacts with the other KL-divergence terms, it is easiest to consider regression (Eq. 5c), as this log-likelihood can also be written as a KL-divergence,\n\nlog P (Y|GL) = −νL+1 DKL\n\n(cid:0)N (0, GL+1)(cid:13) (19) Thus, the likelihood encourages K(GL) + σ2I to be close to the covariance of the data, GL+1 = 1\nIn combiνL+1 nation, we would expect the optimal Gram matrices to “interpolate” between the input kernel, G0 = 1 ν0\n\nYYT , while the DGP prior terms encourage all Gl to lie close to K(Gl−1).\n\n(cid:13)N (cid:0)0, K(GL) + σ2I(cid:1)(cid:1) + const\n\nXXT and the output kernel, GL+1.\n\nTo make the notion of interpolation explicit, we consider σ2 = 0 with a linear kernel, K(Gl−1) = Gl−1, so named because it corresponds to a linear neural network layer. With this kernel and with all νl = ν, there is an analytic solution for the (unique) optimum of the DKM objective (Appendix J.1),\n\nG∗\n\n(cid:0)G−1\n\n(cid:1)l/(L+1)\n\nl = G0\n\n(20) which explicitly geometrically interpolates between G0 and GL+1. Of course, this discussion was primarily for DGPs, but the exact same intuitions hold for BNNs, in that maximizing the DKM objective finds a sequence of Gram matrices, G∗ L that interpolate between the input kernel, G0 and the output kernel, GL+1. The only difference is in details of P (Gl|Gl−1), and specifically as slight differences in the KL-divergence terms (see below).\n\n1, . . . , G∗\n\n0 GL+1\n\n,\n\n4.6 THE DKM OBJECTIVE MIRRORS REPRESENTATION LEARNING IN FINITE NETWORKS\n\nHere, we confirm that the optimizing DKM objective for an infinite network matches doing inference in wide but finite-width networks using Langevin sampling (see Appendix F for details).\n\nWe began by looking at DGPs, and confirming that the posterior marginals are Gaussian (Eq. 17; Fig. 3ab). Then, we confirmed that the representations match closely for infinite-width DKMs (Fig. 2 top and bottom rows) and finite-width DGPs (Fig. 2 middle two rows), both at initialization (Fig. 2 top two rows) and after training to convergence (Fig. 2 bottom two rows). Note that the first column, K0 is a squared exponential kernel applied to the input data, and G3 = yyT is the output Gram matrix (in this case, there is only one output feature).\n\nTo confirm that the match improves as the DGP gets wider, we considered the RMSE between elements of the Gram matrices for networks of different widths (x-axis) for different UCI datasets (columns) and different numbers of layers (top row is one-layer, bottom row is two-layers; Fig. 3c). In most cases, we found a good match as long as the width was at least 128, which is around the width of typical fully connected neural network, but is a little larger than typical DGP widths (e.g. Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017).\n\n6\n\nFigure 2: A two hidden layer DGP with 1024 units per hidden layer and DKM with squared exponential kernels match closely. The data was the first 50 datapoints of the yacht dataset. The first column, K0 is a fixed squared exponential kernel applied to the inputs, and the last column, G3 = yyT is the fixed output Gram matrix. The first row is the DKM initialization at the prior Gram matrices and kernels, and the second row is the DGP, which is initialized by sampling from the prior. As expected, the finite width DGP prior closely matches the infinite-width DKM initialization, which corresponds to the standard infinite width limit. The third row is the Gram matrices and kernels for the trained DGP, which has changed dramatically relative to its initialization (second row) in order to better fit the data. The fourth row is the Gram matrices and kernels for the optimized DKM, which closely matches those for the trained DGP.\n\n4.7 THE SPARSE DEEP KERNEL MACHINE AS A DEEP GENERALISATION OF KERNEL\n\nMETHODS\n\nDGPs in the representation learning limit constitute a deep generalisation of kernel methods, with a very flexible learned kernel, which we call the deep kernel machine (DKM; which was introduced earlier just in the context of the objective). Here, we design a sparse DKM, inspired by sparse methods for DGPs (Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) (Appendix L). The sparse DKM scales linearly in the number of datapoints, P , as opposed to cubic scaling of the plain DKM (similar to the cubic scaling in most naive kernel methods).\n\nWe compared DKMs (Eq. 16) and MAP over features (Sec. E) for DGPs. In addition, we considered a baseline, which was a standard, shallow kernel method mirroring the structure of the deep kernel machine but where the only flexibility comes from the hyperparameters. Formally, this model can be obtained by setting, Gl = K(Gl−1) and is denoted “Kernel Hyper” in Table 1. We applied these methods to UCI datasets (Gal & Ghahramani, 2016) using a two hidden layer architecture, with a kernel inspired by DGP skip-connections, K(Gl) = wl 2 and σ are hyperparameters, and Ksqexp(Gl) is a squared-exponential kernel.\n\n2Ksqexp(Gl). Here, wl\n\n1Gl + wl\n\n1, wl\n\nWe used 300 inducing points fixed to a random subset of the training data and not optimised during training. We used the Adam optimizer with a learning rate of 0.001, full-batch gradients and 5000 iterations for smaller datasets and 1000 iterations for larger datasets (kin8nm, naval and protein).\n\nWe found that the deep kernel machine objective gave better performance than MAP, or the hyperparameter optimization baseline (Tab. 1). Note that these numbers are not directly comparable to those in the deep GP literature (Salimbeni & Deisenroth, 2017), as deep GPs have a full posterior so offer excellent protection against overfitting, while DKMs give only a point estimate.\n\n7\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101Figure 3: Wide DGP posteriors converge to the DKM. Here, we trained DGPs with Langevin sampling (see Appendix F), and compared to a trained DKM. a Marginal distribution over features for one input datapoint for a two-layer DGP trained on a subset of yacht. We used a width of N1...L = 1024 and ν1...L = 5 in all plots to ensure that the data had a strong effect on the learned representations. The marginals (blue histogram) are very close to Gaussian (the red line shows the closest fitted Gaussian). Remember that the true posterior over features is IID (Eq. 31), so each column aggregates the distribution over features (and over 10 parallel chains with 100 samples from each chain) for a single input datapoint. b The 2D marginal distributions for the same DGP for two input points (horizontal and vertical axes). c Element-wise RMSE (normalized Frobenius distance) between Gram matrices from a trained DKM compared to trained DGPs of increasing width. The DGP Gram matrices converge to the DKM solution as width becomes larger.\n\nTable 1: RMSE for inducing point methods. (Equal) best methods are displayed in bold. Error bars give two stderrs for a paired tests, which uses differences in performance between that method and best method, (so there are no meaningful error bars on the best performing method itself). The MAP objective was numerically unstable and thus did not run to completion on the boston dataset.\n\ndataset\n\nP\n\nKernel Hyper\n\nMAP\n\nL\n\n506 1,030 768\n\n4.41 ± 0.31 5.38 ± 0.098 0.83 ± 0.076\n\nboston concrete energy kin8nm 8,192 (7.3 ± 0.06)·10-2 (6.4 ± 0.6)·10-4 3.81 ± 0.091 4.21 ± 0.029 0.68 ± 0.0084 0.94 ± 0.058\n\nnaval power protein wine yacht\n\n11,934 9,568 45,730 1,599 308\n\n— 5.60 ± 0.15 0.73 ± 0.049 (7.4 ± 0.05)·10-2 (5.4 ± 0.5)·10-4 3.73 ± 0.14 4.30 ± 0.033 0.66 ± 0.0067 1.14 ± 0.077\n\n4.35 ± 0.51 5.10 0.47 6.6·10-2 4.6·10-4 3.58 4.10 0.64 0.58\n\n5 CONCLUSION\n\nWe introduced the Bayesian representation learning limit, a new infinite-width limit for BNNs and DGPs that retains representation learning. Representation learning in this limit is described by the intuitive DKM objective, which is composed of a log-likelihood describing performance on the task (e.g. classification or regression) and a sum of KL-divergences keeping representations at every layer close to those under the infinite-width prior. For DGPs, the exact posteriors are IID across features and are multivariate Gaussian, with covariances given by optimizing the DKM objective. Empirically, we found that the distribution over features and representations matched those in wide by finite DGPs. We argued that DGPs in the Bayesian representation learning limit form a new\n\n8\n\n505feature0.00.30.6densitya=1505feature=2303input 1303input 2b=1303input 1=20.00.51.0RMSEcbostonyachtconcreteone-layerenergy2123252729211width0.00.51.0RMSEG1G22123252729211width2123252729211width2123252729211widthtwo-layer0.00.20.00.2class of practical deep kernel method: DKMs. We introduce sparse DKMs, which scale linearly in the number of datapoints. Finally, we give the extension for BNNs where the exact posteriors are intractable so must be approximated.\n\nREFERENCES\n\nLaurence Aitchison. Why bigger is not always better: on finite and infinite neural networks. In\n\nInternational Conference on Machine Learning, pp. 156–164. PMLR, 2020.\n\nLaurence Aitchison, Adam X Yang, and Sebastian W Ober. Deep kernel processes. arXiv preprint\n\narXiv:2010.01590, 2020.\n\nJoseph M Antognini. Finite size corrections for neural network gaussian processes. arXiv preprint\n\narXiv:1908.10030, 2019.\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.\n\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-\n\ncians. Journal of the American statistical Association, 112(518):859–877, 2017.\n\nT Tony Cai and Linjun Zhang. High-dimensional gaussian copula regression: Adaptive estimation\n\nand statistical inference. Statistica Sinica, pp. 963–993, 2018.\n\nYoungmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In NIPS, pp. 342–350.\n\nCurran Associates, Inc., 2009.\n\nAndreas Damianou and Neil D Lawrence. Deep gaussian processes. In Artificial intelligence and\n\nstatistics, pp. 207–215. PMLR, 2013.\n\nConor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. Ad-\n\nvances in neural information processing systems, 32, 2019.\n\nEthan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. arXiv\n\npreprint arXiv:1909.11304, 2019.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML-16), 2016.\n\nAdri`a Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-\n\nworks as shallow gaussian processes. arXiv preprint arXiv:1808.05587, 2018.\n\nArjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and Hall/CRC, 2018.\n\nJames Halverson, Anindita Maiti, and Keegan Stoner. Neural networks and quantum field theory.\n\nMachine Learning: Science and Technology, 2(3):035002, 2021.\n\nBoris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. arXiv\n\npreprint arXiv:1909.05989, 2019.\n\nThomas Hofmann, Bernhard Sch ̈olkopf, and Alexander J Smola. Kernel methods in machine learn-\n\ning. The annals of statistics, pp. 1171–1220, 2008.\n\nRoger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.\n\nRoger A Horn, Roger A Horn, and Charles R Johnson. Topics in matrix analysis. Cambridge\n\nuniversity press, 1994.\n\nJiri Hron, Yasaman Bahri, Roman Novak, Jeffrey Pennington, and Jascha Sohl-Dickstein. Exact posterior distributions of wide bayesian neural networks. arXiv preprint arXiv:2006.10541, 2020.\n\nArthur Jacot, Franck Gabriel, and Cl ́ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\n9\n\nEdwin T Jaynes. Probability theory: The logic of science. Cambridge university press, 2003.\n\nMichael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction\n\nto variational methods for graphical models. Machine learning, 37(2):183–233, 1999.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\n\nlutional neural networks. Advances in neural information processing systems, 25, 2012.\n\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,\n\n2015.\n\nJaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.\n\nQianyi Li and Haim Sompolinsky. Statistical mechanics of deep linear neural networks: The back-\n\npropagating renormalization group. arXiv preprint arXiv:2012.04030, 2020.\n\nAlexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.\n\nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671, 2018.\n\nGadi Naveh and Zohar Ringel. A self consistent theory of gaussian processes captures feature\n\nlearning effects in finite cnns. arXiv preprint arXiv:2106.04110, 2021.\n\nGadi Naveh, Oded Ben-David, Haim Sompolinsky, and Zohar Ringel. Predicting the outputs of\n\nfinite networks trained with noisy gradients. arXiv preprint arXiv:2004.01190, 2020.\n\nRadford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29–53.\n\nSpringer, 1996.\n\nPhan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean field limit of multi-\n\nlayer neural networks. arXiv preprint arXiv:2001.11443, 2020.\n\nQuynh Nguyen. On connected sublevel sets in deep learning.\n\nIn International Conference on\n\nMachine Learning, pp. 4790–4799. PMLR, 2019.\n\nRoman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.\n\nGeoff Pleiss and John P Cunningham. The limitations of large width in neural networks: A deep gaussian process perspective. Advances in Neural Information Processing Systems, 34, 2021.\n\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pp. 1278–1286. PMLR, 2014.\n\nDaniel A Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory. arXiv\n\npreprint arXiv:2106.10165, 2021.\n\nHugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian\n\nprocesses. arXiv preprint arXiv:1705.08933, 2017.\n\n10\n\nInbar Seroussi and Zohar Ringel. Separation of scales and a thermodynamic description of feature\n\nlearning in some cnns. arXiv preprint arXiv:2112.15383, 2021.\n\nJohn Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge univer-\n\nsity press, 2004.\n\nJustin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central\n\nlimit theorem. Stochastic Processes and their Applications, 130(3):1820–1852, 2020a.\n\nJustin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of\n\nlarge numbers. SIAM Journal on Applied Mathematics, 80(2):725–752, 2020b.\n\nAlex J Smola and Bernhard Sch ̈olkopf. Learning with kernels. MIT Press, 1998.\n\nChristopher Williams. Computing with infinite networks. Advances in neural information process-\n\ning systems, 9, 1996.\n\nChieh Wu, Aria Masoomi, Arthur Gretton, and Jennifer Dy. Deep layer-wise networks have closed-\n\nform weights. arXiv preprint arXiv:2202.01210, 2022.\n\nSho Yaida. Non-gaussian processes and neural networks at finite widths.\n\nIn Mathematical and\n\nScientific Machine Learning, pp. 165–192. PMLR, 2020.\n\nGreg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760, 2019.\n\nGreg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint\n\narXiv:2011.14522, 2020.\n\nJacob Zavatone-Veth and Cengiz Pehlevan. Exact marginal prior distributions of finite bayesian\n\nneural networks. Advances in Neural Information Processing Systems, 34, 2021.\n\nJacob A Zavatone-Veth, Abdulkadir Canatar, and Cengiz Pehlevan. Asymptotics of representation\n\nlearning in finite bayesian neural networks. arXiv preprint arXiv:2106.00651, 2021.\n\n11\n\nA BAYESIAN NEURAL NETWORK EXTENSION\n\nConsider a neural network of the form,\n\nF1 = XW0 Fl = φ(Fl−1)Wl−1 (cid:16)\n\n(cid:17)\n\nW l\n\nλμ ∼ N\n\n0, 1 Nl\n\nfor l ∈ {2, . . . , L + 1} (cid:16)\n\n(cid:17)\n\nW 0\n\nλμ ∼ N\n\n0, 1 ν0\n\n(21a)\n\n(21b)\n\n(21c)\n\nwhere W0 ∈ Rν0×N1, Wl ∈ RNl×Nl+1 and WL+1 ∈ RNL×νL+1 are weight matrices with independent Gaussian priors and φ is the usually pointwise nonlinearity.\n\nIn principle, we could integrate out the distribution over Wl to find P (Fl|Fl−1)\n\nP (Fl|Fl−1) =\n\n(cid:90)\n\ndWl P (Wl) δ (Fl − φ(Fl−1)Wl−1) ,\n\n(22)\n\nwhere δ is the Dirac delta. In practice, it is much easier to note that conditioned on Fl−1, the random variables interest, Fl are a linear combination of Gaussian distributed random variables, Wl. Thus, Fl are themselves Gaussian, and this Gaussian is completely characterised by its mean and variance. We begin by writing the feature vectors, f l\n\nλ in terms of weight vectors, wl λ, λ = φ(Fl−1)wl f l λ. As the prior over weight vectors is IID, the prior over features, conditioned on Fl−1), is also IID,\n\n(23)\n\nP (W) =\n\nP (Fl|Fl−1) =\n\nNl(cid:89)\n\nλ=1\n\nNl(cid:89)\n\nλ=1\n\nP (cid:0)wl\n\nλ\n\n(cid:1) =\n\n(cid:16)\n\nN\n\nNl(cid:89)\n\nλ=1\n\nwl\n\nλ; 0,\n\n1 Nl−1\n\nI\n\n(cid:17)\n\n,\n\nP (cid:0)f l\n\nλ|Fl−1\n\n(cid:1) .\n\n(24)\n\n(25)\n\nThe mean of f l\n\nE (cid:2)f l\n\nλ|Fl−1\n\nλ conditioned on Fl−1 is 0, (cid:3) = E (cid:2)φ(Fl−1)wl\n\nλ|Fl−1\n\n(cid:3) = φ(Fl−1) E (cid:2)wl\n\nλ|Fl−1\n\n(cid:3) = φ(Fl−1) E (cid:2)wl\n\nλ\n\n(cid:3) = 0.\n\n(26)\n\nThe covariance of f l\n\nλ conditioned on Fl−1 is, (cid:104) (cid:104)\n\n(cid:105)\n\nE\n\nf l\n\nλ\n\n(cid:0)f l\n\nλ\n\n(cid:1)T\n\n|Fl−1\n\n(cid:0)φ(Fl−1)wl\n\nλ\n\n(cid:1)T\n\n|Fl−1\n\n(cid:105)\n\nλ(wl\n\nλ)T (cid:3) φT (Fl−1)\n\nλ\n\n= E φ(Fl−1)wl = φ(Fl−1) E (cid:2)wl = 1\n\nNl−1\n\nφ(Fl−1)φT (Fl−1)\n\n(27)\n\nThis mean and variance imply that Eq. (1) captures the BNN prior, as long as we choose KBNN(·) and GBNN(·) such that,\n\nKBNN(GBNN(Fl−1)) = 1\n\nNl−1\n\n(cid:80)Nl−1\n\nλ=1 φ(f l−1\n\nλ\n\n)φT (f l−1\n\nλ\n\n),\n\n(28)\n\nSpecifically, we choose the kernel function, KBNN(·) to be the identity function, and GBNN(·) to be the same outer product as in the main text for DGPs (Eq. 2), except where we have applied the NN nonlinearity,\n\nKBNN(Gl−1) = Gl−1, GBNN(Fl−1) = 1\n\nNl−1\n\n(cid:80)Nl−1\n\nλ=1 φ(f l−1\n\nλ\n\n)φT (f l−1\n\nλ\n\n).\n\n(29)\n\n(30)\n\nThis form retains the average-outer-product form for GBNN(·), which is important for our derivations.\n\nNow, Eq. (16) only gave the DKM objective for DGPs. To get a more general form, we need to consider the implied posteriors over features. This posterior is IID over features (Appendix D.1), and for DGPs, it is multivariate Gaussian (Appendix D.2),\n\nP (Fl|Gl−1, Gl) = (cid:81)N l\n\nλ=1 P (cid:0)f l\n\nλ|Gl−1, Gl\n\n(cid:1) =\n\nfor DGPs\n\n(cid:81)N l\n\nλ=1N (cid:0)f l\n\nλ; 0, Gl\n\n(cid:1) .\n\n(31)\n\n12\n\nFigure 4: The variational DKM closely matches the BNN true posterior obtained with Langevin sampling. a Comparison of Gram matrices. The first two rows show Gram matrices for BNN, with the first row being a random initialization, and the second row being the posterior. The last two rows show the Gram matrices from variational DKMs with a flow approximate posterior (third row) and a multivariate Gaussian approximate posterior (fourth row). In optimizing the variational DKM, we used Eq. (34) with 216 Monte-Carlo samples. The Gram matrices for the flow posterior (third row) closely match those from the BNN posterior (second row), while those for a multivariate Gaussian approximate posterior (fourth row) do not match. b Marginal distributions over features at each layer for one input datapoint estimated using kernel density estimation. The note that the BNN (blue line) marginals are non-Gaussian, but the variational DKM with a flow posterior (red line) is capable of capturing this non-Gaussianity.\n\nNow, we can see that Eq. (16) is a specific example of a general expression. In particular, note that the distribution on the left of the KL-divergence in Eq. (16) is the DGP posterior over features (Eq. 31). Thus, the DKM objective can alternatively be written,\n\nL(G1, . . . , GL) = log P (Y|GL) − (cid:80)L\n\nl=1νl DKL\n\n(cid:0)P (cid:0)f l\n\nλ|Gl−1, Gl\n\n(cid:1)(cid:13) (cid:13)N (0, K(Gl−1))(cid:1) ,\n\n(32)\n\nand this form holds for both BNNs and DGPs (Appendix D.3). As in DGPs, the log-posterior is N times L(G1, . . . , GL) (Eq. 16), so as N is taken to infinity, the posterior for all models becomes a point distribution (Eq. 12) if L(G1, . . . , GL) has a unique global maximum.\n\nIn practice, the true posteriors required to evaluate Eq. (32) are intractable for BNNs, raising the question of how to develop accurate approximations for BNNs. We develop a variational DKM (vDKM) by taking inspiration from variational inference (Jordan et al., 1999; Blei et al., 2017) (Appendix D.4). Of course, variational inference is usually impossible in infinite width models, because it is impossible to work with infinitely large latent variables. Our key insight is that as the true posterior factorises across features (Appendix D.1), we can work with the approximate posterior\n\n13\n\ninitG1G2G3G4vDKM (flow)150indexvDKM (MvG)150index150index150index10010feature0.00.5density10010feature10010feature10010feature150index150indexG0150indexG5404abinitG1G2G3G4vDKM (flow)150indexvDKM (MvG)150index150index150index10010feature0.00.5density10010feature10010feature10010feature150index150indexG0150indexG5404ab BNNvDKM (flow)BNNl=1νl DKL (cid:0)f l\n\n(cid:0)Qθl (cid:1),\n\nover only a single feature vector, Qθl approach allows us to define a vDKM objective, which bounds the true DKM objective,\n\n(cid:1), where θl are the parameters and f l\n\n(cid:0)f l\n\nλ ∈ RP is finite. This\n\nλ\n\nL(Gθ(θ1), . . . , Gθ(θL)) ≥ LV(θ1, . . . , θL),\n\nLV(θ1, . . . , θL) = log P (Y|Gθ(θL)) − (cid:80)L\n\n(33) (cid:1)(cid:13) (cid:13)N (0, K(Gθ(θl−1)))(cid:1)\n\n(cid:0)f l\n\nλ\n\nλ|Gl−1, Gl\n\nwith equality when the true posteriors, P (cid:0)f l (cid:1). The only subtlety here is that it is practically difficult to design flexible ap- (cid:1) where we explicitly specify and optimize the Gram matrices. Instead proximate posteriors Qθl we optimize general approximate posterior parameters, θ, and compute the implied Gram matrices,\n\napproximate posteriors, Qθl\n\nequal\n\n(cid:0)f l\n\nthe\n\nλ\n\nλ\n\nGθ(θl) = 1\n\nNl\n\nlim Nl→∞\n\n(cid:80)Nl\n\nλ=1φ(f l\n\nλ)φT (f l\n\nλ) = E\n\nQθl(f l\n\nλ)\n\n(cid:2)φ(f l\n\nλ)φT (f l\n\nλ)(cid:3) .\n\n(34)\n\nλ are sampled from Qθl\n\n(cid:1), and the second equality arises from the law of large numbers. where f l We can compute the Gram matrix analytically in simple cases (such as a multivariate Gaussian), but in general we can always estimate the Gram matrix using a Monte-Carlo estimate of Eq. (34).\n\n(cid:0)f l\n\nλ\n\nFinally, we checked that the vDKM objective closely matched the posterior under neural networks. This is a bit more involved, as the marginal distributions over features are no longer Gaussian (Fig. 4b). To capture these non-Gaussian marginals, we used a simple normalizing flow. In particular, we first sampled zl λ ∼ N (μl, Σl) from a multivariate Gaussian with a learned mean, μl, and covariance, Σl then we obtained features, f l λ through f , a learned pointwise function parameterised as in a neural spline flow (Durkan et al., 2019). The resulting distribution is a high-dimensional Gaussian copula (e.g. Cai & Zhang, 2018). As shown in Fig. 4, vDKM with multivariate Gaussian (MvG) approximate posterior cannot match the Gram matrices learned by BNN (Fig. 4a), while vDKM with flow is able to capture the non-Gaussian marginals (Fig. 4b) and thus match the learned Gram matrices with BNN.\n\nλ), by passing zl\n\nλ = f (zl\n\nB GENERAL LIKELIHOODS THAT DEPEND ONLY ON GRAM MATRICES\n\nWe consider likelihoods which depend only on the top-layer Gram matrix, GL,\n\n(cid:90)\n\nP (Y|GL) =\n\ndFL+1 P (Y|FL+1) P (FL+1|GL)\n\nwhere,\n\nP (FL+1|GL) =\n\nNL+1 (cid:89)\n\nλ=1\n\nN (cid:0)f L+1\n\nλ\n\n; 0, K(GL)(cid:1) .\n\nThis family of likelihoods captures regression,\n\nP (cid:0)yλ|f L+1\n\nλ\n\n(cid:1) = N (cid:0)yL+1\n\nλ\n\n; f L+1\n\nλ\n\n, σ2I(cid:1) ,\n\n(which is equivalent to the model used in the main text Eq. 1b) and e.g. classification,\n\nP (y|F) = Categorical (y; softmax (FL+1)) ,\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\namoung many others.\n\nC WEAK CONVERGENCE\n\nHere, we give a formal argument for weak convergence of the DGP posterior over Gram matrices to a point distribution in the limit as N → ∞,\n\nPN (G1, . . . , GL|X, ̃Y) d→\n\nL (cid:89)\n\nl=1\n\nδ(Gl − G∗ l )\n\n(39)\n\n14\n\nwhere we have included N in the subscript of the probability distribution as a reminder that this distribution depends on the width. By the Portmanteau theorem, weak convergence is established if all expectations of bounded continuous functions, f , converge\n\nlim N→∞\n\nEPN (G1,...,GL|X, ̃Y) [f (G1, . . . , GL)] = f (G∗\n\n1, . . . , G∗\n\nL).\n\n(40)\n\nTo show this in a reasonably general setting (which the DGP posterior is a special case of), we consider an unnormalized probability density of the form h(g)eN L(g), and compute the moment as,\n\nE [f (g)] =\n\n(cid:82) G dg f (g)h(g)eN L(g) (cid:82) G dg h(g)eN L(g)\n\n(41)\n\nwhere g = (G1, . . . , GL) is all L positive semi-definite matrices, Gl. Thus, g ∈ G, where G is a convex set.\n\nWe consider the superlevel set A(∆) = {g|L(g) ≥ L(g∗) − ∆}, where g∗ is the unique global optimum. We select out a small region, A(∆), surrounding the global maximum, and compute the integral as,\n\nE [f (g)] =\n\n(cid:82)\n\nA(∆) dg f (g)h(g)eN L(g) + (cid:82) A(∆) dg h(g)eN L(g) + (cid:82)\n\n(cid:82)\n\nG\\A(∆) dg f (g)h(g)eN L(g) G\\A(∆) dg h(g)eN L(g)\n\nAnd divide the numerator and denominator by (cid:82)\n\nA(∆) dg h(g)eN L(g),\n\nE [f (g)] =\n\n(cid:82)\n\nA(∆) dg f (g)h(g)eN L(g) (cid:82)\n\nA(∆) dg h(g)eN L(g) +\n\n(cid:82) G\\A(∆) dg f (g)h(g)eN L(g) A(∆) dg h(g)eN L(g)\n\n(cid:82)\n\n1 +\n\n(cid:82)\n\nG\\A(∆) dg h(g)eN L(g) (cid:82) A(∆) dg h(g)eN L(g)\n\n(42)\n\n(43)\n\nNow, we deal with each term separately. The ratio in the denominator can be lower-bounded by zero, and upper bounded by considering a smaller superlevel set, A(∆/2), in the denominator,\n\n0 ≤\n\n(cid:82)\n\nG\\A(∆) dg h(g)eN L(g) (cid:82) A(∆) dg h(g)eN L(g)\n\n≤\n\n≤\n\n=\n\n(cid:82)\n\nG\\A(∆) dg h(g)eN L(g) (cid:82) A(∆/2) dg h(g)eN L(g) eN (L(g∗)−∆) (cid:82) eN (L(g∗)−∆/2) (cid:82) (cid:82)\n\nG\\A(∆) dg h(g) (cid:82) A(∆/2) dg h(g)\n\nG\\A(∆) dg h(g) A(∆/2) dg h(g)\n\ne−N ∆/2\n\n(44)\n\nThe upper bound converges to zero (as h(g) is independent of N ), and therefore by the sandwich theorem the ratio of interest also tends to zero.\n\nThe second ratio in the numerator can be rewritten as,\n\n(cid:82)\n\nG\\A(∆) dg f (g)h(g)eN L(g) A(∆) dg h(g)eN L(g)\n\n(cid:82)\n\n=\n\n(cid:82) G\\A(∆) dg f (g)h(g)eN L(g) (cid:82) G\\A(∆) dg h(g)eN L(g)\n\n(cid:82) G\\A(∆) dg h(g)eN L(g) (cid:82) A(∆) dg h(g)eN L(g)\n\n(45)\n\nThe first term here is an expectation of a bounded function, f (g), so is bounded, while second term converges to zero in the limit (by the previous result).\n\nFinally, we consider the first ratio in the numerator,\n\n(cid:82) A(∆) dg f (g)h(g)eN L(g) (cid:82) A(∆) dg h(g)eN L(g)\n\n(46)\n\nwhich can be understood as an expectation over f (g) in the region A(∆). As f is continuous, for any ε > 0, we can find a δ > 0 such that for all g with |g∗ − g| < δ, we have\n\nf (g∗) − ε < f (g) < f (g∗) + ε.\n\n(47)\n\n15\n\nFurther, because the continuous function, L(g), has a unique global optimum, g∗, for every δ > 0 we are always able to find a ∆ > 0 such that all points g ∈ A(∆) are within δ of g∗ i.e. |g∗ − g| < δ. Thus combining the previous two facts, given an ε, we are always able to find a δ such that Eq. 47 holds for all g with |g∗ − g| < δ, and given a δ we are always able to find a ∆ such that all g ∈ A(∆) have |g∗ − g| < δ. Hence for every ε > 0 we can find a ∆ > 0 such that Eq. 47 holds for all g ∈ A(∆). Choosing the appropriate ε-dependent ∆ and substituting Eq. 47 into Eq. 46, ε also bounds the error in the expectation,\n\nf (g∗) − ε <\n\n(cid:82)\n\nA(∆) dg f (g)h(g)eN L(g) (cid:82) A(∆) dg h(g)eN L(g)\n\n< f (g∗) + ε.\n\n(48)\n\nNow, we use the results in Eq. (44), Eq. (45) and Eq. (48) to take the limit of Eq. (43) (we can compose these limits by the algebraic limit theorem as all the individual limits exist and are finite),\n\nf (g∗) − ε < lim\n\nN→∞\n\nE [f (g)] < f (g∗) + ε.\n\nAnd as this holds for any ε, we have,\n\nf (g∗) = lim\n\nN→∞\n\nE [f (g)] .\n\n(49)\n\n(50)\n\nThis result is applicable to the DGP posterior over Gram matrices, as that posterior can be written as,\n\nPN (G1, . . . , GL|X, ̃Y) ∝ h(g)eN L(g),\n\nwhere L(g) is the usual DKM objective,\n\nL(g) = L(G1, . . . , GL)\n\nand h(g) is the remaining terms in the log-posterior which do not depend on N ,\n\n(cid:32)\n\nh(g) = exp\n\n− P +1 2\n\n(cid:33)\n\nlog |Gl|\n\n(cid:88)\n\nl\n\n(this requires P ≤ N so that Gl is full-rank).\n\n(51)\n\n(52)\n\n(53)\n\nD GENERAL MODELS IN THE BAYESIAN REPRESENTATION LEARNING LIMIT\n\nOverall, our goal is to compute the integral in Eq. (6) in the limit as N → ∞. While the integral is intractable for general models such as BNNs, we can use variational inference to reason about its properties. In particular, we can bound the integral using the ELBO,\n\nlog P (Gl|Gl−1) ≥ ELBOl = EQ(Fl) [log P (Gl|Fl) + log P (Fl|Gl−1) − log Q (Fl)] . (cid:0)f l\n\n(54) (cid:1) in the main text, both because the approximate Note that Q (Fl) here is different from Qθl posterior here, Q (Fl) is over all features jointly, Fl, whereas the approximate posterior in the main text is only over a single feature, f l λ, and because in the main text, we chose a specific family of distribution with parameters θl, while here we leave the approximate posterior, Q (Fl) completely unconstrained, so that it has the flexibility to capture the true posterior. Indeed, if the optimal approximate posterior is equal to the true posterior, Q∗ (Fl) = P (Fl|Gl−1, Gl), then the bound is tight, so we get log P (Gl|Gl−1) = ELBO∗ l . Our overall strategy is thus to use variational inference to characterise the optimal approximate which is equal to the true posterior Q∗ (Fl) = P (Fl|Gl−1, Gl) and use the corresponding ELBO to obtain log P (Gl|Gl−1).\n\nλ\n\nD.1 CHARACTERISING EXACT BNN POSTERIORS\n\nRemember that if the approximate posterior family, Q (Fl) is flexible enough to capture the true posterior P (Fl|Gl−1, Gl), then the Q∗ (Fl) that optimizes the ELBO is indeed the true posterior, the bound is tight, so the ELBO is equal to log P (Gl|Gl−1) (Jordan et al., 1999; Blei et al., 2017). Thus, we are careful to ensure that our approximate posterior family captures the true posterior, by ensuring that we only impose constraints on Q (Fl) that must hold for the true posterior,\n\n16\n\nP (Fl|Gl−1, Gl). In particular, note that P (Gl|Fl) in Eq. (5b) constrains the true posterior to give non-zero mass only to Fl that satisfy Gl = 1 φ(Fl)φT (Fl). However, this constraint is difficult Nl to handle. We therefore consider an alternative, weaker constraint on expectations, which holds for the true posterior (the first equality below) because Eq. (5b) constrains Gl = 1 φ(Fl)φT (Fl), and Nl impose the same constraint on the approximate posterior,\n\nGl = EP(Fl|Gl,Gl−1)\n\n(cid:104) 1 Nl\n\n(cid:105) φ(Fl)φT (Fl)\n\n= EQ(Fl)\n\n(cid:104) 1 Nl\n\n(cid:105) φ(Fl)φT (Fl)\n\n.\n\n(55)\n\nNow, we can solve for the optimal Q (Fl) with this constraint on the expectation. In particular, the Lagrangian is obtained by taking the ELBO (Eq. 54), dropping the log P (Gl|Fl) term representing the equality constraint (that Gl = 1 φ(Fl)φT (Fl)) and including Lagrange multipliers for the Nl expectation constraint, Λ, (Eq. 55) and the constraint that the distribution must normalize to 1, Λ,\n\n(cid:90)\n\nL =\n\ndFl Q (Fl) (log P (Fl|Gl−1) − log Q (Fl))\n\n(cid:18)\n\n(cid:18)\n\nΛ\n\nGl −\n\n(cid:90)\n\n+ 1\n\n2 Tr\n\ndFl Q (Fl) φ(Fl)φT (Fl)\n\n(cid:19)(cid:19)\n\n(cid:18)\n\n+ Λ\n\n1 −\n\n(cid:90)\n\n(cid:19)\n\ndFl Q (Fl)\n\n(56)\n\nDifferentiating wrt Q (Fl), and solving for the optimal approximate posterior, Q∗ (Fl),\n\n0 =\n\n∂L ∂ Q (Fl)\n\n(cid:12) (cid:12) (cid:12) (cid:12)Q∗(Fl)\n\n0 = (log P (Fl|Gl−1) − log Q∗ (Fl)) − 1 − 1\n\n2 Tr (cid:0)Λφ(Fl)φT (Fl)(cid:1) − Λ\n\nSolving for log Q∗ (Fl),\n\nlog Q∗ (Fl) = log P (Fl|Gl−1) − 1\n\n2 Tr (cid:0)Λφ(Fl)φT (Fl)(cid:1) + const .\n\nUsing the cyclic property of the trace,\n\nlog Q∗ (Fl) = log P (Fl|Gl−1) − 1\n\n2 Tr (cid:0)φT (Fl)Λφ(Fl)(cid:1) + const .\n\nThus, log Q (Fl) can be written as a sum over features,\n\nlog Q∗ (Fl) =\n\nNl(cid:88)\n\nλ=1\n\n(cid:2)log P (cid:0)f l\n\nλ|Gl−1\n\n(cid:1) − 1\n\n2 φT (f l\n\nλ)Λφ(f l\n\nλ)(cid:3) + const = (cid:80)NL\n\nλ=1 log Q (cid:0)f l\n\nλ\n\nso, the optimal approximate posterior is IID over features,\n\nQ∗ (Fl) = (cid:81)Nl\n\nλ=1 Q∗ (cid:0)f l\n\nλ\n\n(cid:1) .\n\n(57)\n\n(58)\n\n(59)\n\n(60)\n\n(cid:1)\n\n(61)\n\n(62)\n\nRemember that this approximate posterior was only constrained in expectation, and that this constraint held for the true posterior (Eq. 55). Thus, we might think that this optimal approximate posterior would be equal to the true posterior. However, remember that the true posterior had a tighter equality constraint, that Gl = 1 φ(Fl)φT (Fl), while so far we have only imposed a weaker conNl straint in expectation (Eq. 55). We thus need to check that our optimal approximate posterior does indeed satisfy the equality constraint in the limit as Nl → ∞. This be shown using the law of large numbers, as f l λ are IID under the optimal approximate posterior, and by using Eq. (55) for the final equality,\n\nlim Nl→∞\n\n1 Nl\n\nφ(Fl)φT (Fl) = lim\n\nNl→∞\n\n1 Nl\n\nNl(cid:88)\n\nλ=1\n\nφ(f l\n\nλ)φT (f l\n\nλ) = E\n\nQ(f l\n\nλ)\n\n(cid:2)φ(f l\n\nλ)φT (f l\n\nλ)(cid:3) = Gl.\n\n(63)\n\nThus, the optimal approximate posterior does meet the constraint in the limit as Nl → ∞, so in that limit, the true posterior, like the optimal approximate posterior is IID across features,\n\nP (Fl|Gl−1, Gl) = Q∗ (Fl) = (cid:81)Nl\n\nλ=1 Q∗ (cid:0)f l\n\nλ\n\n(cid:1) = (cid:81)Nl\n\nl=1 P (cid:0)f l\n\nλ|Gl−1, Gl\n\n(cid:1) .\n\n(64)\n\n17\n\nD.2 EXACTLY MULTIVARIATE GAUSSIAN DGP POSTERIORS\n\nFor DGPs, we have φ(f l\n\nλ) = f l λ, so the optimal approximate posterior is Gaussian, (cid:0)f l\n\n(cid:1) = log PDGP\n\n(cid:1) − 1\n\nλ + const\n\nλ)T Λf l\n\nλ|Gl−1\n\n(cid:0)f l\n\nDGP\n\n2 (f l\n\nλ\n\nlog Q∗\n\n= − 1\n\n2 (f l\n\n= log N\n\nλ)T (cid:0)Λ + K−1(Gl−1)(cid:1) f l (cid:16)\n\nλ; 0, (cid:0)Λ + K−1(Gl−1)(cid:1)−1(cid:17) f l\n\nλ + const\n\n.\n\n(65)\n\n(66)\n\n(67)\n\nAs the approximate posterior and true posterior are IID, the constraint in Eq. (55) becomes,\n\nGl = E\n\nPDGP(f l\n\nλ|Gl,Gl−1)\n\n(cid:2)f l\n\nλ(f l\n\nλ)T (cid:3) = E\n\nQ∗\n\nDGP(f l\n\nλ)\n\n(cid:2)f l\n\nλ(f l\n\nλ)T (cid:3) = (cid:0)Λ + K−1(Gl−1)(cid:1)−1\n\n.\n\n(68)\n\nAs the Lagrange multipliers are unconstrained, we can always set them such that this constraint holds. In that case both the optimal approximate posterior and the true posterior become,\n\nPDGP\n\n(cid:0)f l\n\nλ|Gl−1Gl\n\n(cid:1) = Q∗\n\nDGP\n\n(cid:0)f l\n\nλ\n\n(cid:1) = N (cid:0)f l\n\nλ; 0, Gl\n\n(cid:1) ,\n\n(69)\n\nas required.\n\nD.3 GENERAL FORM FOR THE CONDITIONAL DISTRIBUTION OVER GRAM MATRICES\n\nNow that we have shown that the true posterior, P (Fl|Gl−1, Gl) factorises, we can obtain a simple form for log P (Gl|Gl−1). In particular, log P (Gl|Gl−1) is equal to the ELBO if we use the true posterior in place of the approximate posterior,\n\nlim Nl→∞\n\n1\n\nN log P (Gl|Gl−1) = lim\n\nNl→∞\n\n1 N\n\nEP(Fl|Gl−1,Gl)\n\n(cid:20)\n\nlog P (Gl|Fl) + log\n\nP (Fl|Gl−1) P (Fl|Gl−1, Gl)\n\n(cid:21)\n\n.\n\n(70)\n\nUnder the posterior, the constraint represented by log P (Gl|Fl) is satisfied, so in the limit we can include that term in a constant,\n\nlim Nl→∞\n\n1\n\nN log P (Gl|Gl−1) = lim\n\nNl→∞\n\n1 N\n\nEP(Fl|Gl−1,Gl)\n\n(cid:20)\n\nlog\n\nP (Fl|Gl−1) P (Fl|Gl−1, Gl)\n\n(cid:21)\n\n+ const .\n\n(71)\n\nNow, we use the fact that the prior, P (Fl|Gl−1) and posterior, P (Fl|Gl−1, Gl), are IID across features,\n\nlim Nl→∞\n\n1\n\nN log P (Gl|Gl−1) = νl E\n\nP(f l\n\nλ|Gl−1,Gl)\n\n(cid:34)\n\nlog\n\nP (cid:0)f l\n\n(cid:1)\n\nλ|Gl−1 λ|Gl−1, Gl\n\nP (cid:0)f l\n\n(cid:35)\n\n(cid:1)\n\n+ const\n\n(72)\n\nand this expectation is a KL-divergence,\n\nlim Nl→∞\n\n1\n\nN log P (Gl|Gl−1) = −νl DKL\n\n(cid:0)P (cid:0)f l\n\nλ|Gl−1, Gl\n\n(cid:1)(cid:13) (cid:13)P (cid:0)f l\n\nλ|Gl−1\n\n(cid:1)(cid:1) + const,\n\n(73)\n\nwhich gives Eq. (32) when we combine with Eq. (8).\n\nD.4 PARAMETRIC APPROXIMATE POSTERIORS\n\nEq. (64) represents a considerable simplification, as we now need to consider only a single feature, f l λ, rather than the joint distribution over all features, Fl. However, in the general case, it is still not possible to compute Eq. (64) because the true posterior over a single feature is still not tractable. Following the true posteriors derived in the previous section, we could chose a parametric approximate posterior that factorises across features,\n\nQθ (F1, . . . , FL) = (cid:81)L\n\nl=1\n\n(cid:81)Nl\n\nλ=1 Qθl\n\n(cid:0)f l\n\nλ\n\n(cid:1) .\n\n(74)\n\nRemember that we optimize the approximate posterior parameters, θ, directly, and set the Gram matrices as a function of θ (Eq. 34). As before, we can bound, log P (Gl=Gθ(θl)|Gl−1) using the\n\n18\n\nELBO, and the bound is tight when the approximate posterior equals the true posterior,\n\nlog P (Gl = Gθ(θl)|Gl−1)\n\n(cid:34)\n\n= E\n\nP(Fl|Gl−1,Gl=Gθ(θθ))\n\nlog P (Gl=Gθ(θl)|Fl) + log\n\nP (cid:0)Fl\n\nλ|Gl−1\n\n(cid:1)\n\nP (Fl|Gl−1, Gl=Gθ(θl))\n\n(cid:34)\n\n≥ EQθ(Fl)\n\nlog P (Gl=Gθ(θl)|Fl) + log\n\nP (cid:0)Fl Qθl\n\nλ|Gl−1 (Fl)\n\n(cid:35)\n\n(cid:1)\n\n.\n\n(cid:35)\n\n(75)\n\n(76)\n\n(77)\n\nNow, we can cancel the log P (Gl = Gθ(θl)|Fl) terms, as they represent a constraint that holds both under the true posterior, and under the approximate posterior,\n\nE\n\nP(Fl|Gl−1,Gl=Gθ(θl)))\n\n(cid:20)\n\nlog\n\nP (Fl|Gl−1) P (Fl|Gl−1, Gl=Gθ(θl))\n\n(cid:21)\n\n≥ EQθl\n\n(Fl)\n\n(cid:20)\n\nlog\n\nP (Fl|Gl−1)\n\nQθl\n\n(Fl)\n\n(cid:21)\n\n.\n\n(78)\n\nUsing the fact that the prior, posterior and approximate posterior are all IID over features, we can write this inequality in terms of distributions over a single feature, f l\n\nλ and divide by Nl,\n\nE\n\nP(f l\n\nλ|Gl−1,Gl=Gθ(θl))\n\n(cid:34)\n\nlog\n\n(cid:1)\n\nP (cid:0)f l\n\nλ|Gl−1 λ|Gl−1, Gl=Gθ(θl)(cid:1)\n\nP (cid:0)f l\n\n(cid:35)\n\n(cid:34)\n\nlog\n\n≥ E\n\nQθl(f l\n\nλ)\n\nP (cid:0)f l\n\nλ|Gl−1(θ)(cid:1) (cid:0)f l Qθl\n\n(cid:1)\n\nλ\n\n(cid:35)\n\n.\n\n(79)\n\nNoting that both sides of this inequality are negative KL-divergences, we obtain, (cid:1)(cid:13) (cid:13)P (cid:0)f l\n\nλ|Gl−1, Gl=Gθ(θl)(cid:1)(cid:13)\n\n(cid:1)(cid:1) ≥ − DKL\n\n(cid:13)P (cid:0)f l\n\n(cid:0)P (cid:0)f l\n\nλ|Gl−1\n\n− DKL\n\n(cid:0)f l\n\n(cid:0)Qθl\n\nλ\n\nλ|Gl−1\n\n(cid:1)(cid:1) ,\n\n(80)\n\nwhich gives Eq. (33) in the main text.\n\nE THEORETICAL SIMILARITIES IN REPRESENTATION LEARNING IN FINITE\n\nAND INFINITE NETWORKS\n\nIn the main text, we considered probability densities of the Gram matrices, G1, . . . , GL. However, we can also consider probability densities of the features, F1, . . . , FL, for a DGP, 2 log |K (GDGP (Fl−1))| − 1\n\nl K−1 (GDGP (Fl−1)) Fl\n\nlog P (Fl|Fl−1) = − Nl\n\n(cid:1) + const .\n\n2 tr (cid:0)FT\n\n(81)\n\nWe can rewrite the density such that it is still the density of features, Fl, but it is expressed in terms of the DGP Gram matrix, log P (Fl|Fl−1) = − Nl\n\n2 tr (cid:0)K−1(Gl−1)Gl\n\n2 log |K(Gl−1)| − Nl\n\n(cid:1) + const .\n\n(82)\n\nHere, we have used the cyclic property of the trace to combine the Fl and FT l to form Gl, and we have used the fact that our kernels can be written as a function of the Gram matrix. Overall, we can therefore write the posterior over features, P (F1, . . . , FL|X, ̃Y), in terms of only Gram matrices,\n\nJ (G1, . . . , GL) = 1\n\nN log P (F1, . . . , FL|X, ̃Y) = log P (Y|GL) + 1\n\nN\n\nL (cid:88)\n\nl=1\n\nlog P (Fl|Fl−1) ,\n\n(83)\n\nsubstituting Eq. (82),\n\nJ (G1, . . . , GL) = log P (Y|GL) − 1\n\n2\n\n(cid:80)L\n\nl=1νl\n\n(cid:0)log |K(Gl−1)| + tr (cid:0)K−1(Gl−1)Gl\n\n(cid:1)(cid:1)\n\n+ const .\n\n(84)\n\nThus, J (G1, . . . , GL) does not depend on N , and thus the Gram matrices that maximize J (G1, . . . , GL) are the same for any choice of N . The only restriction is that we need Nl ≥ P , to ensure that the Gram matrices are full-rank.\n\nTo confirm these results, we used Adam with a learning rate of 10−3 to optimize full-rank Gram matrices with Eq. (84) and to directly do MAP inference over features using Eq. (81). As expected, as the number of features increased, the Gram matrix from MAP inference over features converged rapidly to that expected using Eq. (84) (Fig. 5).\n\n19\n\nFigure 5: RMSE of trained Gram matrices between one-hidden-layer (first row) and two-hiddenlayer (second row) DGPs of various width trained by gradient descent and the corresponding MAP limit. Columns correspond to different datasets (trained on a subset of 50 datapoints).\n\nF ADDITIONAL EXPERIMENTAL DETAILS\n\nTo optimize the analytic DKM objective for DGPs and the variational DKM objective for DGPs (Figs. 3–11), we parameterised the Gram matrices (or covariances for the variational approximate posterior) as the product of a square matrix, Rl ∈ RP ×P , with itself transposed, Gl = 1 P RlRT l , and we used Adam with a learning rate of 10−3 to learn Rl. To do Bayesian inference in finite BNNs and DGPs, we used Langevin sampling with 10 parallel chains, and a step size of 10−3. Note that in certain senarios, Langevin sampling can be very slow, as the features have a Gaussian prior with covariance K(Gl−1) which has some very small and some larger eigenvalues, which makes sampling difficult. Instead, we reparameterised the model in terms of the standard Gaussian random variables, Vl ∈ RP ×Nl. We then wrote Fl in terms of Vl,\n\nFl = Ll−1Vl.\n\n(85)\n\nHere, Ll−1 is the Cholesky of K(Gl−1), so K(Gl−1) = Ll−1LT l−1. This gives an equivalent distribution P (Fl|Fl−1). Importantly, as the prior on Vl is IID standard Gaussian, sampling Vl is much faster. To ensure that the computational cost of these expensive simulations remained reasonable, we used a subset of 50 datapoints from each dataset.\n\nFor the DKM objective for BNNs, we used Monte-Carlo to approximate the Gram matrices,\n\nGθ(θl) ≈\n\nK (cid:88)\n\nk=1\n\nφ(f l\n\nk)φT (f l\n\nk).\n\n(86)\n\nwith f l k drawn from the appropriate approximate posterior, and K = 216. We can use the reparameterisation trick (Kingma & Welling, 2013; Rezende et al., 2014) to differentiate through these Monte-Carlo estimates.\n\nG ADDITIONAL COMPARISONS WITH FINITE-WIDTH DGPS\n\nIn particular, we Here, we give additional results supporting those in Sec. 4.6, Fig. 3–Fig. 11. give the DGP representations learned by two-layer networks on all UCI datasets (boston, concrete, energy, yacht), except those already given in the main text Fig. 6–8.\n\n20\n\n0.000.050.100.150.20RMSEbostonyachtconcreteone-layerenergy2123252729211width0.000.050.100.150.20RMSEG1G22123252729211width2123252729211width2123252729211widthtwo-layerFigure 6: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of energy. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\nFigure 7: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of boston. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\n21\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101init DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101Figure 8: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of concrete. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\n22\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101H THE FLOW POSTERIOR IN A 2-LAYER BNN\n\nHere, we give the 2-layer version (Fig. 9) of Fig. 4 in the main text, which again shows a close match between the variational DKM with a flow posterior, and the BNN true posterior.\n\nFigure 9: Two-layer ReLU BNN and variational DKM with flow. a Initialized (first row) and learned Gram matrices of a width 1024 BNN (second row), vDKM with flow (third row) and vDKM with multivariate Gaussian (fourth row) using 214 Monte-Carlo samples. The Gram matrices between BNN and vDKM (flow) match closely after training. (MvG). b Marginal PDF over features at each layer for one input datapoint using kernel density estimation. The marginal PDFs of BNN are nonGaussian (blue curves), vDKM with flow is able to capture the non-Gaussianity and match closely with BNNs marginals (red curves).\n\n23\n\ninitG1G2vDKM (flow)150indexvDKM (MvG)150index505feature0.00.5density505feature150index150indexG0150indexG30.750.000.75abBNNvDKM (flow)BNNI MULTIVARIATE GAUSSIAN APPROXIMATE POSTERIORS IN DEEPER\n\nNETWORKS\n\nIn particular, we hypothesised that depth is an important factor.\n\nThere is a body of theoretical work (e.g. (Seroussi & Ringel, 2021)), on BNNs that approximates BNN posteriors over features as Gaussian. While we have shown that this is a bad idea in general (Fig. 4 and 9), we can nonetheless ask whether there are circumstances where the idea might work well. In particular, in shallow networks, in order to get GL close to the required representation, we may need the posterior over Fl to be quite different from the prior. In contrast, in deeper networks, we might expect the posterior over Fl to be closer to its (Gaussian) prior, and therefore we might Gaussian approximate posteriors to work better.\n\nHowever, we cannot just make the network deeper, because as we do so, we apply the nonlinearity more times and dramatically alter the network’s inductive biases. To resolve this issue, we derive a leaky relu nonlinearity that allows (approximately) independent control over the inductive biases (or effective depth) and the actual depth (Appendix I.1). Using these nonlinearities, we indeed find that very deep networks are reasonably well approximated by multivariate Gaussian approximate posteriors (Appendix I.2).\n\nI.1 LEAKY RELU NONLINEARITIES\n\nOur goal is to find a pointwise nonlinearity, φ, such that (under the prior), λ)(cid:3) = p E\n\nλ)reluT (f l\n\nλ)φT (f l\n\n(cid:2)relu(f l\n\n(cid:2)φ(f l\n\nE\n\nPDGP(f l\n\nλ|Gl−1)\n\nP(f l\n\nλ|Gl−1)\n\nλ)(cid:3) + (1 − p)Gl−1.\n\n(87)\n\nWe will set p = α/L, where α is the “effective” depth of the network and L is the real depth. These networks are designed such that their inductive biases in the infinite width limit are similar to a standard relu network with depth α. Indeed, we would take this approach if we wanted a well-defined infinite-depth DKM limit.\n\nWithout loss of generality, we consider a 2D case, where x and y are zero-mean bivariate Gaussian,\n\nπ(x, y) = N\n\n(cid:19)\n\n(cid:18)(cid:18)x y\n\n; 0,\n\n(cid:18)Σxx Σxy Σxy Σyy\n\n(cid:19)(cid:19)\n\n(88)\n\nwhere π(x, y) is the probability density for the joint distribution. Note that we use a scaled relu, (cid:26)√\n\nrelu(x) =\n\n0\n\n2 x for 0 < x otherwise\n\nsuch that E (cid:2)relu2(x)(cid:3) = Σxx. Mirroring Eq. 87, we want the nonlinearity, φ, to satisfy,\n\nE (cid:2)φ(x2)(cid:3) = p E (cid:2)relu2(x)(cid:3) + (1 − p)Σxx = Σxx E (cid:2)φ(y2)(cid:3) = p E (cid:2)relu2(y)(cid:3) + (1 − p)Σyy = Σyy\n\nE [φ(x)φ(y)] = p E [relu(x)relu(y)] + (1 − p)Σxy\n\nWe hypothesise that this nonlinearity has the form,\n\nφ(x) = a relu(x) + bx.\n\nWe will write the relu as a sum of x and |x|,\n\nrelu(x) = 1√\n\n2\n\n(x + |x|),\n\nbecause E [f (x, y)] = 0 for f (x, y) = x|y| or f (x, y) = |x|y. It turns out that we get zero expectation for all functions where f (−x, −y) = −f (x, y), which holds for the two choices above. To show such functions have a zero expectation, we write out the integral explicitly,\n\nE [f (x, y)] =\n\n(cid:90) ∞\n\n(cid:90) ∞\n\ndx\n\n−∞\n\n−∞\n\ndy π(x, y)f (x, y).\n\n(93)\n\n24\n\n(89)\n\n(90a)\n\n(90b)\n\n(90c)\n\n(91)\n\n(92)\n\nWe split the domain of integration for y at zero,\n\nE [f (x, y)] =\n\n(cid:90) ∞\n\n(cid:90) 0\n\ndx\n\n−∞\n\n−∞\n\ndy π(x, y)f (x, y) +\n\n(cid:90) ∞\n\n(cid:90) ∞\n\ndx\n\n−∞\n\n0\n\ndy π(x, y)f (x, y).\n\n(94)\n\nWe substitute y′ = −y and x′ = −x in the first integral, (cid:90) ∞\n\n(cid:90) ∞\n\nE [f (x, y)] =\n\ndx′\n\ndy′ π(−x′, −y′)f (−x′, −y′) +\n\n−∞\n\n0\n\n(cid:90) ∞\n\n(cid:90) ∞\n\ndx\n\n−∞\n\n0\n\ndy π(x, y)f (x, y).\n\n(95)\n\nAs the variables we integrate over are arbitrary we can relabel y′ as y and x′ as x, and we can then merge the integrals as their limits are the same,\n\nE [f (x, y)] =\n\n(cid:90) ∞\n\n(cid:90) ∞\n\ndx\n\n−∞\n\n0\n\ndy [π(−x, −y)f (−x, −y) + π(x, y)f (x, y)] .\n\nUnder a zero-mean Gaussian, π(−x, −y) = π(x, y),\n\nE [f (x, y)] =\n\n(cid:90) ∞\n\n(cid:90) ∞\n\ndx\n\n−∞\n\n0\n\ndy π(x, y) (f (−x, −y) + f (x, y)) .\n\n(96)\n\n(97)\n\nThus, if f (−x, −y) = −f (x, y), then the expectation of that function under a bivariate zero-mean Gaussian distribution is zero.\n\nRemember that our overall goal was to design a nonlinearity, φ, (Eq. 91) which satisfied Eq. (90). We therefore compute the expectation,\n\nE [φ(x)φ(y)] = E [(a relu(x) + bx) (a relu(y) + by)]\n\n= E\n\n(cid:104)(cid:16) a√\n\n2\n\n(x + |x|) + bx\n\n(cid:17) (cid:16) a√\n\n(y + |y|) + by\n\n(cid:17)(cid:105)\n\n2\n\nUsing the fact that E [ x|y| ] = E [ |x|y ] = 0 under a multivariate Gaussian,\n\n(cid:104)\n\n= E\n\na2 1√\n\n2\n\n(x + |x|) 1√ 2\n\n= a2 E [relu(x)relu(y)] +\n\n(y + |y|) + (cid:16)√\n\n2ab + b2(cid:17)\n\n(cid:16)√\n\n2ab + b2(cid:17)\n\nxy\n\n(cid:105)\n\nE [xy] .\n\n√\n\np.\n\na =\n\nThus, we can find the value of a by comparing with Eq. (90c),\n\np = a2\n\nFor b, things are a bit more involved,\n\n√\n\n1 − p =\n\n2ab + b2 = (cid:112)2p b + b2\n\n(98)\n\n(99)\n\n(100)\n\n(101)\n\n(102)\n\n(103)\n\nwhere we substitute for the value of a. This can be rearranged to form a quadratic equation in b,\n\n0 = b2 + (cid:112)2p b + (p − 1),\n\nwhich can be solved,\n\nb = 1 2\n\nb = 1 2\n\n(cid:16)\n\n(cid:17) −(cid:112)2p ± (cid:112)2p − 4(p − 1)\n\n(cid:16)\n\n−(cid:112)2p ± (cid:112)4 − 2p\n\n(cid:17)\n\nb = −\n\n(cid:113) p\n\n2 ±\n\n(cid:113)\n\n1 − p\n\n2\n\nOnly the positive root is of interest,\n\nb =\n\nThus, the nonlinearity is,\n\n(cid:113)\n\n1 − p\n\n2 −\n\n(cid:113) p\n\n2\n\n√\n\nφ(x) =\n\np relu(x) +\n\n(cid:16)(cid:113)\n\n1 − p\n\n2 −\n\n(cid:113) p\n\n(cid:17)\n\n2\n\nx\n\n25\n\n(104)\n\n(105)\n\n(106)\n\n(107)\n\n(108)\n\n(109)\n\nFigure 10: Comparison of posterior feature marginal distributions between a BNN of width 1024 (trained by Langevin sampling over features) and a variational DKM with 216 Monte-Carlo samples, in a 4-layer (row 1) and a 32-layer (row 2) network. We give the BNN posterior features from Langevin sampling (blue histogarm) and the best fitting Gaussian (blue line), and compare against the variational DKM approximate posterior Gaussian distribution (red line).\n\nwhere we set p = α/L, and remember we used the scaled relu in Eq. (89). Finally, we established these choices by considering only the cross term, E [φ(x)φ(y)]. We also need to check that the E (cid:2)φ2(x)(cid:3) and E (cid:2)φ2(y)(cid:3) terms are as required (Eq. 90a and Eq. 90b). In particular,\n\nE (cid:2)φ2(x)(cid:3) = E\n\n(cid:104)\n\n(a relu(x) + bx)2(cid:105)\n\n= E\n\n(cid:20)(cid:16) a√\n\n2\n\n(x + |x|) + bx\n\n(cid:17)2(cid:21)\n\n(110)\n\nusing E [x|x|] = 0 as x|x| is an odd function of x, and the zero-mean Gaussian is an even distribution,\n\nE (cid:2)φ2(x)(cid:3) = a2 E (cid:2)relu2(x)(cid:3) +\n\n(cid:16)√\n\n2ab + b2(cid:17)\n\nΣxx\n\nusing Eq. (102) to identify a2 and Eq. (103) to identify\n\n√\n\n2ab + b2,\n\nE (cid:2)φ2(x)(cid:3) = p E (cid:2)relu2(x)(cid:3) + (1 − p)Σxx,\n\nas required.\n\nI.2 MULTIVARIATE GAUSSIAN IN DEEPER NETWORKS\n\n(111)\n\n(112)\n\nIn the main text, we show that a more complex approximate posterior can match the distributions in these networks. Here, we consider an alternative approach. In particular, we hypothesise that these distributions are strongly non-Gaussian because the networks are shallow, meaning that the posterior needs to be far from the prior in order to get a top-layer kernel close to GL+1. We could therefore make the posteriors closer to Gaussian by using leaky-relu nonlinearities (Appendix I.1) with fixed effective depth (α = 2), but increasing real depth, L. In particular, we use multivariate Gaussian approximate posteriors with learned means,\n\nQθl\n\n(cid:0)f l\n\nλ\n\n(cid:1) = N (cid:0)f l\n\nλ; μl, Σl\n\n(cid:1)\n\nso\n\nθl = (μl, Σl).\n\n(113)\n\nAs expected, for a depth 32 network, we have much more similar marginals (Fig. 10 top) and learned representations (Fig. 11 top).\n\n26\n\n50501density=1505=2505=35054 layers=4505feature01density=8505feature=16505feature=24505feature32 layers=32BNN vDKMFigure 11: Comparison of Gram matrices between BNN of width 1024 (trained by Langevin sampling over features) and variational DKM, in 4-layer (row 1-3) and 32-layer networks (row 4-6). Initializations are shown in row 1 and 4, trained BNN Gram matrices are shown in row 2 and 5, and trained variational DKM Gram matrices are shown in row 3 and 6. As in Figure 10, the variational DKM is a poor match to Langevin sampling in a BNN for a 4-layer network, but is very similar in a 32 layer network.\n\n27\n\nG1G2G3G4G8G16G24G32150index150index150index150index150index150indexG0150indexGL+1101 32 layers BNNinitvDKMvDKM4 layers BNNinitJ UNIMODALITY IN LINEAR DEEP KERNEL MACHINES\n\nJ.1 THEORY: UNIMODALITY WITH A LINEAR KERNEL AND SAME WIDTHS\n\nHere, we show that the deep kernel machine objective is unimodal for a linear kernel. A linear kernel simply returns the input Gram matrix,\n\nK (G) = G.\n\n(114)\n\nIt is called a linear kernel, because it arises in the neural network setting (Eq. 21) by choosing the nonlinearity, φ to be the identity, in which case, Fl = Fl−1Wl−1. For a linear kernel the objective becomes,\n\nL(G1, ..., GL) = (cid:80)L+1\n\nl=1\n\nνl 2\n\n(cid:0)log (cid:12)\n\n(cid:12)G−1\n\nl−1Gl\n\n(cid:12) (cid:12) − Tr (cid:0)G−1\n\nl−1Gl\n\n(cid:1)(cid:1)\n\n(115)\n\nwhere we have assumed there is no output noise, σ2 = 0. Taking all νl to be equal, ν = νl (see Appendix J.2 for the general case),\n\nL(G1, ..., GL) = log (cid:12)\n\n(cid:12)G−1\n\n0 GL+1\n\n(cid:12) (cid:12) − ν\n\n2\n\n(cid:80)L+1\n\nl=1 Tr (cid:0)G−1\n\nl−1Gl\n\n(cid:1) .\n\n(116)\n\nNote that G0 and GL+1 are fixed by the inputs and outputs respectively. Thus, to find the mode, we set the gradient wrt G1, . . . , GL to zero,\n\n0 =\n\n∂L ∂Gl\n\n= ν 2\n\n(cid:0)G−1\n\nl−1 − G−1\n\nl Gl+1G−1\n\nl\n\nThus, at the mode, the recursive relationship must hold,\n\nT = G−1\n\nl−1Gl = G−1\n\nl Gl+1.\n\nThus, optimal Gram matrices are given by,\n\nand we can solve for T by noting,\n\nGl = G0Tl,\n\nG−1\n\n0 GL+1 = TL+1.\n\n(cid:1)\n\n(117)\n\n(118)\n\n(119)\n\n(120)\n\nImportantly, T is the product of two positive definite matrices, T = G−1 l−1Gl, so T must have positive, real eigenvalues (but T does not have to be symmetric (Horn & Johnson, 2012)). There is only one solution to Eq. (120) with positive real eigenvalues (Horn et al., 1994). Intuitively, this can be seen using the eigendecomposition, G−1\n\n0 GL+1 = V−1DV, where D is diagonal,\n\nT = (cid:0)V−1DV(cid:1)1/(L+1)\n\n= V−1D1/(L+1)V.\n\n(121)\n\nThus, finding T reduces to finding the (L + 1)th root of each positive real number on the diagonal of D. While there are (L + 1) complex roots, there is only one positive real root, and so T and hence G1, . . . , GL are uniquely specified. This contrasts with a deep linear neural network, which has infinitely many optimal settings for the weights.\n\nNote that for the objective to be well-defined, we need K(G) to be full-rank. With standard kernels (such as the squared exponential) this is always the case, even if the input Gram matrix is singular. However, a linear kernel will have a singular output if given a singular input, and with enough data points, G0 = 1 XXT ) to be ν0 given by applying a positive definite kernel (such as a squared exponential) to 1 XXT . This results ν0 in positive definite G0, as long as the input points are distinct.\n\nXXT is always singular. To fix this, we could e.g. define G0 = K( 1 ν0\n\nJ.2 THEORY: UNIMODALITY WITH A LINEAR KERNEL AND ARBITRARY WIDTHS\n\nIn the main text we showed that the deep kernel machine is unimodal when all νl are equal. Here, we show that unimodality in linear DKMs also holds for all choices of νl. Recall the linear DKM objective in Eq. (115),\n\nL(G1, ..., GL) = (cid:80)L+1 = (cid:80)L+1\n\nl=1\n\nl=1\n\nνl 2\nνl 2\n\n(cid:12)G−1\n\nl−1Gl\n\n(cid:0)log (cid:12) (cid:0)log |Gl| − log |Gl−1| − Tr(cid:0)G−1\n\n(cid:12) (cid:12) − Tr (cid:0)G−1\n\nl−1Gl\n\n(cid:1)(cid:1)\n\nl−1Gl\n\n(cid:1)(cid:1) .\n\n(122)\n\n(123)\n\n28\n\nTo find the mode, again we set the gradient wrt Gl to zero,\n\n0 =\n\n∂L ∂Gl\n\n= − νl+1−νl\n\n2 G−1\n\nl − νl\n\n2 G−1\n\nl−1 + νl+1\n\n2 G−1\n\nl Gl+1G−1\n\nl\n\n,\n\n(124)\n\nfor l = 1, ..., L. Right multiplying by 2Gl and rearranging,\n\nνl+1G−1\n\nl Gl+1 = νlG−1\n\nl−1Gl + (νl+1 − νl) I,\n\nfor l = 1, ..., L.\n\n(125)\n\nEvaluating this expression for l = 1 and l = 2 gives,\n\nν2G−1 ν3G−1\n\n1 G2 = ν1G−1 2 G3 = ν2G−1\n\n0 G1 + (ν2 − ν1) I, 1 G2 + (ν3 − ν2) I = ν1G−1\n\n0 G1 + (ν3 − ν1) I.\n\nRecursing, we get,\n\nνlG−1\n\nl−1Gl = ν1G−1\n\n0 G1 + (νl − ν1) I.\n\n(126)\n\n(127)\n\n(128)\n\nCritically, this form highlights constraints on G1. In particular, the right hand side, G−1 l−1Gl, is the product of two positive definite matrices, so has positive eigenvalues (but may be non-symmetric (Horn & Johnson, 2012)). Thus, all eigenvalues of ν1G−1 0 G1 must be larger than ν1 − νl, and this holds true at all layers. This will become important later, as it rules out inadmissible solutions.\n\nGiven G0 and G1, we can compute any Gl using,\n\nG−1\n\n0 Gl =\n\n(cid:33)\n\nνl′\n\nG−1\n\n0 Gl =\n\n(cid:32) l\n\n(cid:89)\n\nl′=1\n\nl (cid:89)\n\nl′=1\n\nl (cid:89)\n\nl′=1\n\n(cid:0)G−1\n\nl′−1Gl′\n\n(cid:1) =\n\n(cid:81)l\n\n1 l′=1 νl′\n\nl (cid:89)\n\nl′=1\n\n(cid:0)νl′G−1\n\nl′−1Gl′\n\n(cid:1)\n\n(cid:0)ν1G−1\n\n0 G1 + (νl′ − ν1) I(cid:1)\n\n(129)\n\n(130)\n\nwhere the matrix products are ordered as (cid:81)L using our knowledge of GL+1. Computing G−1 (cid:32)L+1 (cid:89)\n\n(cid:33)\n\nL+1 (cid:89)\n\nνl\n\nG−1\n\n0 GL+1 =\n\nl=1\n\nl=1\n\nWe write the eigendecomposition of ν1G−1\n\n0 G1 as, 0 G1 = VDV−1.\n\nν1G−1\n\nl=1 Al = A1 · · · AL. Now, we seek to solve for G1 0 GL+1,\n\n(cid:0)ν1G−1\n\n0 G1 + (νl − ν1) I(cid:1) .\n\n(131)\n\n(132)\n\nThus,\n\n(cid:33)\n\nνl\n\nG−1\n\n0 GL+1 =\n\n(cid:32)L+1 (cid:89)\n\nl=1\n\nwhere Λ is a diagonal matrix,\n\nΛ =\n\nL+1 (cid:89)\n\nl=1\n\nL+1 (cid:89)\n\nl=1\n\n(cid:0)VDV−1 + (νl − ν1) I(cid:1) = VΛV−1\n\n(133)\n\n(D + (νl − ν1) I) .\n\n(134)\n\nThus, we can identify V and Λ by performing an eigendecomposition of the known matrix, (cid:16)(cid:81)L+1 0 GL+1. Then, we can solve for D (and hence G1) in terms of Λ and V. The\n\n(cid:17)\n\nl=1 νl\n\nG−1 diagonal elements of D satisfy,\n\n0 = −Λii +\n\nL+1 (cid:89)\n\nk=1\n\n(Dii + (νl − ν1)) .\n\n(135)\n\nThis is a polynomial, and remembering the constraints from Eq. (128), we are interested in solutions which satisfy,\n\nν1 − νmin ≤ Dii.\n\n(136)\n\n29\n\nwhere,\n\nνmin = min (ν1, . . . , νL+1) .\n\n(137)\n\nTo reason about the number of such solutions, we use Descartes’ rule of signs, which states that the number of positive real roots is equal to or a multiple of two less than the number of sign changes in the coefficients of the polynomial. Thus, if there is one sign change, there must be one positive real root. For instance, in the following polynomial,\n\n0 = x3 + x2 − 1\n\n(138)\n\nthe signs go as (+), (+), (−), so there is only one sign change, and there is one real root. To use Descartes’ rule of signs, we work in terms of D′\n\nii, which is constrained to be positive,\n\n0 ≤ D′\n\nii = Dii − (ν1 − νmin)\n\nDii = D′\n\nii + (ν1 − νmin) .\n\n(139)\n\nThus, the polynomial of interest (Eq. 135) becomes,\n\n0 = −Λii +\n\nL+1 (cid:89)\n\nl=1\n\n(D′\n\nii + (ν1 − νmin) − (ν1 − νl)) = −Λii +\n\nL+1 (cid:89)\n\nl=1\n\n(D′\n\nii + (νl − νmin))\n\n(140)\n\nwhere 0 < νl − νmin as νmin is defined to be the smallest νl (Eq. 137). Thus, the constant term, −Λii is negative, while all other terms, D′ ii)L+1 in the polynomial have positive coefficients. Thus, there is only one sign change, which proves the existence of only one valid real root, as required.\n\nii, . . . , (D′\n\nK UNIMODALITY EXPERIMENTS WITH NONLINEAR KERNELS\n\nFor the posterior over Gram matrices to converge to a point distribution, we need the DKM objective L(G1, . . . , GL) to have one unique global optimum. As noted above, this is guaranteed when the prior dominates (Eq. 11), and for linear models (Appendix J). While we believe that it might be possible to construct counter examples, in practice we expect a single global optimum in most practical settings. To confirm this expectation, we did a number of experiments, starting with many different random initializations of a deep kernel machine and optimizing using gradient descent (Appendix K). In all cases tested, the optimizers converged to the same maximum.\n\nP VlVT\n\nl with Vl ∈ RP ×P being trainable parameters. To We parameterise Gram matrices Gl = 1 make initializations with different seeds sufficiently separated while ensuring stability we initialize Gl from a broad distribution that depends on K(Gl−1). Specifically, we first take the Cholesky decomposition K(Gl−1) = Ll−1LT l where each entry of Ξl ∈ RP ×P is independently sampled from a standard Gaussian, and Dl is a diagonal scaling matrix with each entry sampled i.i.d. from an inverse-Gamma distribution. The variance of the inverse-Gamma distribution is fixed to 100, and the mean is drawn from a uniform distribution U [0.5, 3] for each seed. Since for any random variable x ∼ Inv-Gamma(α, β), E(x) = β (α−1)(α−2) , once we fix the mean and variance we can compute α and β as\n\nl−1, then set Vl = Ll−1ΞlD1/2\n\nα−1 and V(x) =\n\nβ\n\nα =\n\nE(x)2 V(x) β = E(x)(α − 1).\n\n+ 2,\n\n(141)\n\n(142)\n\nWe set νl = 5, and use the Adam optimizer (Kingma & Ba, 2014) with learning rate 0.001 to optimize parameters Vl described above. We fixed all model hyperparameters to ensure that any multimodality could emerge only from the underlying deep kernel machine. As we did not use inducing points, we were forced to consider only the smaller UCI datasets (yacht, boston, energy and concrete). For the deep kernel machine objective, all Gram matrices converge rapidly to the same solution, as measured by RMSE (Fig. 12). Critically, we did find multiple modes for the MAP objective (Fig 13), indicating that experiments are indeed powerful enough to find multiple modes (though of course they cannot be guaranteed to find them). Finally, note that the Gram matrices took a surprisingly long time to converge: this was largely due to the high degree of diversity in the initializations; convergence was much faster if we initialised deterministically from the prior.\n\n30\n\nFigure 12: One-layer DKMs with squared exponential kernel trained on full UCI datasets (through columns) converges to the same solution, despite very different initializations by applying stochastic diagonal scalings described in Appendix F to the standard initialization with different seeds. Standard initialization is shown in dashed line, while scaled initializations are the color lines each denoting a different seed. The first row shows the objective during training for all seeds that all converge to the same value. The second row shows the element-wise RMSE between the Gram matrix of each seed and the optimized Gram matrix obtained from the standard initialization. RMSE converges to 0 as all initializations converge on the same maximum. The last row plots RMSE versus objective value, again showing a single optimal objective value where all Gram matrices are the same.\n\nThis might contradict our usual intuitions about huge multimodality in the weights/features of BNNs and DGPs. This can be reconciled by noting that each mode, written in terms of Gram matrices, corresponds to (perhaps infinitely) many modal features. In particular, in Sec. E, we show that the log-probability for features, P (Fl|Fl−1) (Eq. 82) depends only on the Gram matrices, and note that there are many settings of features which give the same Gram matrix. In particular, the Gram matrix is the same for any unitary transformation of the features, F′ l = FlU, satisfying UUT = I, as l = 1 1\nl = Gl. For DGPs we can use any unitary matrix, so there Nl are infinitely many sets of features consistent with a particular Gram matrix, while for BNNs we can only use permutation matrices, which are a subset of unitary matrices. Thus, the objective landscape must be far more complex in the feature domain than with Gram matrices, as a single optimal Gram matrix corresponds to a large family of optimal features.\n\nFlUlUT\n\nl = 1\n\nFlFT\n\nl FT\n\nlF′T\n\nF′\n\nNl\n\nNl\n\n31\n\n500490480objectiveyacht860850840boston111011001090energy174017301720concrete050000100000iters0.00.51.0RMSE050000100000iters050000100000iters050000100000iters500490480objective0.00.51.0RMSE860850840objective111011001090objective174017301720objectiveFigure 13: One-layer DGP with MAP inference over features as described in Appendix E Eq. (84). Rows and columns are the same as in Figure 12. Using the same randomly scaled initializations described above, we are able to find multiple modes in energy and concrete showing our initializations are diverse enough, albeit there is still only a single global optimum.\n\n32\n\n46504700objectiveyacht64606510boston1240012450energy1575015800concrete050000100000iters0.00.51.0RMSE050000100000iters0100000200000iters050000100000iters46504700objective0.00.51.0RMSE64606510objective1240012450objective1575015800objectiveL INDUCING POINT DKMS\n\nTo do large-scale experiments on UCI datasets, we introduce inducing point DKMs by extending Gaussian process inducing point methods (Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) to the DKM setting. This approach uses the variational interpretation of the deep kernel machine objective described in Appendix D.\n\nTo do inducing-point variational inference, we need to explicitly introduce top-layer features mirroring FL+1 ∈ RP ×νL+1 in Appendix B, but replicated N times, ̃FL+1 ∈ RP ×NL+1. Formally, each feature, ̃f L+1\n\n, . . . , ̃f L+1 NL+1\n\n1\n\nis IID, conditioned on FL, λ=1N (cid:0) ̃f L+1 λ=1N (cid:0) ̃yλ; ̃f L+1\n\nP ( ̃FL+1|FL) = (cid:81)Nl P ( ̃Y| ̃FL+1) = (cid:81)Nl\n\nλ\n\nλ\n\n; 0, K(G(FL))(cid:1) , , σ2I(cid:1) ,\n\n(143a)\n\n(143b)\n\nwhere we give the likelihood for regression, but other likelihoods (e.g. for classification) are possible (Appendix B).\n\nFurther, we take the total number of points, P , to be made up of Pi inducing points and Pt test/train points, so that P = Pi + Pt. Thus, we can separate all features, Fl ∈ RP ×Nl, into the inducing t ∈ RPt×Nl. Likewise, we separate the inputs, features, Fl X, and outputs, Y, into (potentially trained) inducing inputs, Xi, and trained inducing outputs, Yi, and the real test/training inputs, Xt, and outputs, Yt,\n\ni ∈ RPi×Nl, and the test/train features, Fl\n\nFl =\n\n(cid:19)\n\n(cid:18)Fl i\nFl t\n\n ̃FL+1 =\n\n(cid:18) ̃FL+1 ̃FL+1\n\ni\n\nt\n\n(cid:19)\n\nX =\n\n(cid:19)\n\n(cid:18)Xi Xt\n\nY =\n\n(cid:19)\n\n(cid:18)Yi Yt\n\n ̃Y =\n\n(cid:19)\n\n(cid:18) ̃Yi ̃Yt\n\n(144)\n\nWe follow the usual doubly stochastic inducing point approach for DGPs. In particular, we treat all the features at intermediate layers, F1, . . . , FL, and the top-layer train/test features, FL+1 as latent variables. However, we deviate from the usual setup in treating the top-layer inducing outputs, FL+1 , as learned parameters and maximize over them to ensure that the ultimate method does not require sampling, and at the same time allows minibatched training. The prior and approximate posterior over F1, . . . , FL are given by,\n\ni\n\nt\n\nQ (F1, . . . FL|X) = (cid:81)L P (F1, . . . , FL|X) = (cid:81)L\n\nl=1 Q (Fl|Fl−1) , l=1 P (Fl|Fl−1) ,\n\n(145a)\n\n(145b)\n\nand remember F0 = X, so G0 = 1 N0 factorises into a distribution over the inducing points and a distribution over the test/train points,\n\nXXT . The prior and approximate posterior at each layer\n\nQ (Fl|Fl−1) = P (cid:0)Fl P (Fl|Fl−1) = P (cid:0)Fl\n\nt |Fl t |Fl\n\ni , Fl−1 i , Fl−1\n\n(cid:1) Q (cid:0)Fl (cid:1) P (cid:0)Fl\n\n(cid:1) , i |Fl−1\n\ni\n\ni\n\n(cid:1) .\n\n(146a)\n\n(146b)\n\nthe approximate posterior samples for the test/train points is the conditional prior (cid:1), which is going to lead to cancellation when we compute the ELBO. Likewise,\n\nCritically, P (cid:0)Fl t |Fl the approximate posterior over ̃FL+1\n\ni , Fl−1\n\nt\n\nis the conditional prior, (cid:1) = P (cid:0) ̃FL+1\n\n, FL\n\n|FL+1\n\nt\n\ni\n\nQ (cid:0) ̃FL+1\n\nt\n\n|FL+1\n\ni\n\n, FL\n\n(cid:1) .\n\n(147)\n\nConcretely, the prior approximate posterior over inducing points are given by,\n\nQ (cid:0)Fl\n\nλ=1N (cid:0)f l λ=1N (cid:0)f l The approximate posterior is directly analogous to Eq. (69) and the prior is directly analogous to Eq. (1a), but where we have specified that this is only over inducing points. Now we compute the ELBO\n\ni;λ; 0, Gl i;λ; 0, K(G(Fl−1\n\n(cid:1) = (cid:81)Nl (cid:1) = (cid:81)Nl\n\ni |Fl−1\n\nP (cid:0)Fl\n\n(148b)\n\n(148a)\n\n))(cid:1)\n\n(cid:1) ,\n\nii\n\ni\n\ni\n\ni\n\nELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL ii ) (cid:34)\n\n= EQ\n\nlog P (cid:0) ̃Yt| ̃FL+1\n\nt\n\ni\n\n|FL+1 |FL+1\n\ni\n\n, FL\n\n, FL\n\n(cid:1) P (F1, . . . FL|X) (cid:1) Q (F1, . . . FL|X)\n\n(cid:35)\n\n(149)\n\n(cid:1) + log\n\nP (cid:0) ̃FL+1 Q (cid:0) ̃FL+1\n\nt\n\nt\n\n33\n\nNote that the P (cid:0)Fl we come to describing sampling), substituting Eq. (145–147) and cancelling P (cid:0)Fl P (cid:0) ̃FL+1\n\n(cid:1) terms are going to cancel in the ELBO, we consider them below when (cid:1) and\n\ni , Fl−1\n\ni , Fl−1\n\n|FL+1\n\nt |Fl\n\nt |Fl\n\n(cid:1),\n\n, FL\n\nt\n\ni\n\nELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL\n\nii ) = EQ\n\n(cid:34)\n\nlog P (cid:0) ̃Yt| ̃FL+1\n\nt\n\n(cid:1) +\n\nL (cid:88)\n\nl=1\n\nlog\n\nP (cid:0)Fl\n\ni |Fl−1 (cid:1)\n\ni\n\nQ (cid:0)Fl\n\ni\n\n(cid:35)\n\n(cid:1)\n\n.\n\n(150)\n\nSo far, we have treated the Gram matrices, Gl ii as parameters of the approximate posterior. However, in the infinite limit N → ∞, these are consistent with the features generated by the approximate (cid:0)Fl posterior. In particular the matrix product 1 can be written as an average over infinitely Nl many IID vectors, f l i;λ (first equality), and by the law of large numbers, this is equal to the expectation of one term (second equality), which is Gl\n\nii (by the approximate posterior Eq. (148a)),\n\nFl i\n\n(cid:1)T\n\ni\n\nlim N→∞\n\n1 Nl\n\nFl\n\ni\n\n(cid:1)T\n\n(cid:0)Fl\n\ni\n\n= lim\n\nN→∞\n\n1 Nl\n\n(cid:80)Nl\n\nλ=1f l\n\ni;λ\n\n(cid:1)T\n\n(cid:0)f l\n\ni;λ\n\n= E\n\nQ(f l\n\ni;λ)\n\n(cid:104)\n\nf l i;λ\n\n(cid:0)f l\n\ni;λ\n\n(cid:1)T (cid:105)\n\n= Gl ii.\n\n(151)\n\nBy this argument, the Gram matrix from the previous layer, Gl−1 through Gl−1 DGP, Fl factorise. Thus, in the infinite limit, individual terms in the ELBO can be written,\n\nis deterministic. Further, in a (Eq. 5), and the prior and approximate posterior\n\ni only depends on Fl−1\n\nii\n\ni\n\ni\n\n(cid:34)\n\nlim N→∞\n\n1 N\n\nEQ\n\nlog\n\nP (cid:0)Fl\n\ni |Fl−1 (cid:1)\n\ni\n\nQ (cid:0)Fl\n\ni\n\n(cid:35)\n\n(cid:1)\n\n\n\n= νl EQ\n\nlog\n\n(cid:16)\n\nP\n\ni;λ|Gl−1 f l (cid:16)\n\ni (cid:17)\n\nQ\n\nf l i;λ\n\n(cid:17)\n\n\n\n\n\n= −νl DKL\n\n(cid:0)N (cid:0)0, K(Gl−1\n\ni\n\n)(cid:1)(cid:13)\n\n(cid:13)N (cid:0)0, Gl\n\ni\n\n(152)\n\n(153)\n\n(cid:1)(cid:1) ,\n\nwhere the final equality arises when we notice that the expectation can be written as a KLdivergence. The inducing DKM objective, Lind, is the ELBO, divided by N to ensure that it remains finite in the infinite limit,\n\nLind(FL+1\n\ni\n\n, G1\n\nii, . . . , GL\n\nii )= lim\n\nN→∞\n\n1\n\nN ELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL ii )\n\n(154)\n\n= EQ\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nt\n\n(cid:1)(cid:3) −\n\nL (cid:88)\n\nl=1\n\nνl DKL\n\n(cid:0)N (cid:0)0, K(Gl−1\n\nii\n\n)(cid:1)(cid:13)\n\n(cid:13)N (cid:0)0, Gl\n\nii\n\n(cid:1)(cid:1) .\n\nNote that this has almost exactly the same form as the standard DKM objective for DGPs in the main text (Eq. 16). In particular, the second term is a chain of KL-divergences, with the only difference that these KL-divergences apply only to the inducing points. The first term is a “performance” term that here depends on the quality of the predictions given the inducing points. As the copies are IID, we have,\n\nEQ\n\n(cid:2)log P (cid:0) ̃Yt| ̃FL+1\n\nt\n\n(cid:1)(cid:3) = N EQ\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nt\n\n(cid:1)(cid:3) .\n\n(155)\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nNow that we have a simple form for the ELBO, we need to compute the expected likelihood, (cid:1)(cid:3). This requires us to compute the full Gram matrices, including test/train EQ points, conditioned on the optimized inducing Gram matrices. We start by defining the full Gram matrix,\n\nt\n\nGl =\n\n(cid:18)Gl Gl\n\nii Gl ti Gl\n\ntt\n\nit\n\n(cid:19)\n\n(156)\n\nfor both inducing points (labelled “i”) and test/training points (labelled “t”) from just Gl ii. For clarity, we have Gl ∈ RP ×P , Gl tt ∈ RPt×Pt, where Pi is the number of inducing points, Pt is the number of train/test points and P = Pi + Pt is the total number of inducing and train/test points.\n\nii ∈ RPi×Pi, Gl\n\nti ∈ RPt×Pi, Gl\n\nThe conditional distribution over Fl\n\nt given Fl\n\ni is, λ=1N (cid:0)f l (cid:1) = (cid:81)Nl\n\nP (cid:0)Fl\n\nt\n\n(cid:12) (cid:12)Fl\n\ni , Gl−1\n\nt;λ; KtiK−1\n\nii f l\n\ni;λ, Ktt·i\n\n(cid:1)\n\n(157)\n\n34\n\nAlgorithm 1 DKM prediction\n\nl=1\n\nii}L\n\nParameters: {νl}L Optimized Gram matrices {Gl Inducing and train/test inputs: Xi, Xt Inducing outputs: FL+1 Initialize full Gram matrix (cid:19) (cid:18)G0 G0\n\n(cid:18)XiXT XtXT\n\ni XiXT i XtXT\n\nii G0;T ti G0\n\n= 1 ν0\n\nl=1\n\ntt\n\nti\n\ni\n\nt\n\nt\n\n(cid:19)\n\nPropagate full Gram matrix for l in (1, . . . , L) do\n\n(cid:19)\n\n(cid:18)Kii KT ti Kti Ktt\n\n= K\n\nii\n\n(cid:18)(cid:18)Gl−1 Gl−1 ii KT ti .\n\nti\n\n(cid:19)(cid:19)\n\n)T\n\n(Gl−1 Gl−1\n\nti\n\ntt\n\nii\n\niiK−1\n\nii KT\n\nii Gl ii Gl\n\nti = KtiK−1 tt = KtiK−1\n\nKtt·i = Ktt − KtiK−1 Gl Gl end for Final prediction using standard Gaussian process expressions (cid:18)Kii KT (cid:18)(cid:18)GL (GL ii ti GL GL Kti Ktt tt ti , Ktt − KtiK−1\n\n= K Yt ∼ N (cid:0)KtiK−1\n\nti + Ktt·i\n\nti + σ2I(cid:1)\n\nii FL+1\n\nii KT\n\nti )T\n\n(cid:19)(cid:19)\n\n(cid:19)\n\ni\n\nwhere f l feature for all train/test inputs, and f l\n\nt;λ is the activation of the λth feature for all train/test inputs, f l i;λ, and (cid:19)\n\n(cid:18)Kii KT ti Kti Ktt\n\n= K\n\n(cid:16) 1\n\nNl−1\n\n(cid:17)\n\nFl−1FT\n\nl−1\n\nKtt·i = Ktt − KtiK−1\n\nii KT ti .\n\n= K (Gl−1)\n\n(158)\n\n(159)\n\ni;λ is the activation of the λth\n\nIn the infinite limit, the Gram matrix becomes deterministic via the law of large numbers (as in Eq. 151), and as such Git and Gtt become deterministic and equal to their expected values. Using Eq. (157), we can write,\n\nwhere Ξ is a matrix with IID standard Gaussian elements. Thus,\n\nFl\n\nt = KtiK−1\n\nii Fl\n\ni + K1/2\n\ntt·i Ξ.\n\nGl\n\nt (Fl\n\nE (cid:2)Fl ν KtiK−1\n\nti = 1 ν\n= 1 = KtiK−1\n\nii\n\nii Gl\n\nii\n\ni )T (cid:3) E (cid:2)Fl\n\ni (Fl\n\ni )T (cid:3)\n\nand,\n\nGl\n\nt (Fl\n\nE (cid:2)Fl\n\ntt = 1 ν\n= 1 = KtiK−1\n\nt )T (cid:3) E (cid:2)Fl ii GiiK−1\n\nν KtiK−1\n\nii\n\ni (Fl ii KT\n\ni )T (cid:3) K−1 ti + Ktt·i\n\nii KT\n\nti + 1\n\nν K1/2\n\ntt·i\n\n(160)\n\n(161)\n\n(162)\n\n(163)\n\n(164)\n\n(165)\n\n(166)\n\nE (cid:2)ΞΞT (cid:3) K1/2\n\ntt·i\n\nFor the full prediction algorithm, see Alg. 1.\n\n35",
    "reference": "# Summary Of The Paper\n\nLarge-width networks converge to GPs with fixed/ unlearnable kernels, making the networks unable to perform representation learning. In this paper, the authors proposed a simple recipe to allow for representation learning: making the logit layer as wide as the hidden layers and replicating the labels accordingly. The authors argue that this approach allows the kernel to evolve and possible learning a representation. The authors also support their claims using synthetical dataset experiments and UCI datasets.\n\n# Strength And Weaknesses\n\n# Strength.\n1. The paper proposed simple and interesting ideas that allow feature learning in the infinite-width network, which is worth further exploration. \n2. Some experiments from the paper show non-Gaussian behavior of the learned representation that differs from the NNGP limit.  \n\n# Weakness\n\n1. The notations and terminologies are confusing, which makes it hard to parse the paper. \n2. Missing key empirical comparison (vs. NNGP/NTK) for representation learning. I would like to see a comparison using CIFAR10. \n3. Several strong claims that require further justifications.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n1. Many strong claims from the paper are unconvincing. \n\n- Using `**the** representation learning limit` is not proper; there can be many of them, and the proposed approach is just one of many. \n\n - I can't agree with the claim that the approach is `extremely theoretically tractable.` (in the abstract) and it is misleading. Unlike NNGP, the solution can be written analytically as an exact closed-form formula of the input data. The approach in the paper is not analytically tractable (except possibly in the linear network setting) and requires approximation methods. Cited from the paper, \"In practice, the true posteriors required to evaluate Eq. (21) are intractable \".  \n\n- \"We show that DKMs can be scaled to large datasets using inducing point methods from the Gaussian process literature, and we show that DKMs exhibit superior performance to other kernel-based approaches\" (from the abstract.) This is certainly an overclaim. The largest dataset used in UCI, could not justify *superior performance* and scalability to *large datasets*. Again, I expect at least experimental results on CIFAR10 if not more complicated. \n\n- I would like to re-emphasize that the dataset used here (UCI) is too simple to capture *representation learning*. I would expect a comparison using CIFAR-10 against benchmark results for NNGP kernels; see Lee et al. finite vs infinite (https://arxiv.org/abs/2007.15801); neural kernel without tangents (Shankar, https://arxiv.org/abs/2003.02237). I am also interested in the visualization of the learned representation using the image dataset. \n\n2. Notations and terminologies. \n\n- The definition of DNNs is confusing in Sec 3. Do you mean a fully-connected network with trainable parameters that are optimized using gradient descent? This is what DNNs mean for most people in ML. In addition, what does that mean to marginalize the weights in each layer in eq (2)? Do you mean sequentially? Can you verbally explain the difference between (3a) and (3b)? \n\n-  DGP \"Deep Gaussian Processes\" (by Andreas C. Damianou, Neil D. Lawrence, not cited in the paper) is a standard framework. Is it the same thing used in the paper? If so, why not cite the above (or related ) paper? \n\n-  Notations in equation (11) are confusing. What are $G_l/G_{l-1}$ here? Don't they depend on $N$ ? Or are they some *infinite/deterministic* objects that don't rely on $N$, and you overload the notations? Same for equations (12), etc. In addition, Eq (13) seems to be a result in \"Wide Bayesian neural networks have a simple weight posterior: theory and accelerated sampling\" by Hron et al. Please clarify. \n\n3. Others. \n- First citation in the intro. Please cite \"Radford M. Neal. Priors for infinite networks,\" which is the first NNGP (one-hidden layer) paper. \"Lee et al Deep Neural Networks as Gaussian Processes\"  should also be added.\n\n# Summary Of The Review\n\nUpdate: \nI thank the authors for the clarification/updates of several notations/terminologies. I like the idea from the paper, a Bayesian prospective of feature learning. I have increased my score accordingly. However, there are still a lot of room for improvement and several open questions. To name some,  \n\n1. Even though this is a theory paper, authors should not shy away from more empirical work. Even doing a small scale experiments on Cifar10 (baseline against kernel) could really helpful to both practitioners and theorists. \n\n2. Authors should have more detailed discussion/comparison between the meanfield/feature learning limit of neural networks and the current proposed Bayesian framework, both theoretically and empirically. E.g., baselining performance (bayesian feature learning vs NN feature learning), are they learning similar features, etc. \n\n---------------------------------\n\nOverall, I think the idea from the paper is interesting and worth further exploration. However, I also find the paper hard to parse due to clarity issues from notations and terminology, and the theoretical and empirical results from the paper do not support the strong claims from the paper.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDIP-GNN: DISCRIMINATIVE PRE-TRAINING OF GRAPH NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGraph neural network (GNN) pre-training methods have been proposed to enhance the power of GNNs. Specifically, a GNN is first pre-trained on a large-scale unlabeled graph and then fine-tuned on a separate small labeled graph for downstream applications, such as node classification. One popular pre-training method is to mask out a proportion of the edges, and a GNN is trained to recover them. However, such a generative method suffers from graph mismatch. That is, the masked graph input to the GNN deviates from the original graph. To alleviate this issue, we propose DiP-GNN (Discriminative Pre-training of Graph Neural Networks). Specifically, we train a generator to recover identities of the masked edges, and simultaneously, we train a discriminator to distinguish the generated edges from the original graph’s edges. The discriminator is subsequently used for downstream fine-tuning. In our pre-training framework, the graph seen by the discriminator better matches the original graph because the generator can recover a proportion of the masked edges. Extensive experiments on large-scale homogeneous and heterogeneous graphs demonstrate the effectiveness of the proposed framework. Our code will be publicly available.\n\n1\n\nINTRODUCTION\n\nGraph neural networks (GNNs) have achieved superior performance in various applications, such as node classification (Kipf & Welling, 2017), knowledge graph modeling (Schlichtkrull et al., 2018) and recommendation systems (Ying et al., 2018). To enhance the power of GNNs, generative pretraining methods are developed (Hu et al., 2020b). During the pre-training stage, a GNN incorporates topological information by training on a large-scale unlabeled graph in a self-supervised manner. Then, the pre-trained model is fine-tuned on a separate small labeled graph for downstream applications. Generative GNN pre-training is akin to masked language modeling in language model pre-training (Devlin et al., 2019). That is, for an input graph, we first randomly mask out a proportion of the edges, and then a GNN is trained to recover the original identity of the masked edges.\n\nOne major drawback with the abovementioned approach is graph mismatch. That is, the input graph to the GNN deviates from the original one since a considerable amount of edges are dropped. This causes changes in topological information, e.g., node connectivity. Consequently, the learned node embeddings may not be desirable.\n\nTo mitigate the above issues, we propose DiP-GNN ( Discriminative Pre-training of Graph Neural Networks). In DiP-GNN, we simultaneously train a generator and a discriminator. The generator is trained similar to existing generative pre-training approaches, where the model seeks to recover the masked edges and outputs a reconstructed graph. Subsequently, the reconstructed graph is fed to the discriminator, which predicts whether each edge resides in the original graph (i.e., a true edge) or is wrongly constructed by the generator (i.e., a fake edge). After pre-training, we fine-tune the discriminator on downstream tasks. Figure 1 illustrates our training framework. Note that our work is related to Generative Adversarial Nets (GAN, Goodfellow et al. 2014), and detailed discussions are presented in Section 3.4. We remark that similar approaches have been used in natural language processing (Clark et al., 2020). However, we identify the graph mismatch problem (see Section 4.5), which is specific to graph-related applications and is not observed in natural language processing.\n\nThe proposed framework is more advantageous than generative pre-training. This is because the reconstructed graph fed to the discriminator better matches the original graph compared with the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustration of DiP-GNN. From left to right: Original graph; Graph with two masked edges (dashed lines); Reconstructed graph created by the generator (generated edges are the dashed red lines); Discriminator labels each edge as [G] (generated) or [O] (original), where there are two wrong labels (shown in red).\n\nmasked graph fed to the generator. Consequently, the discriminator can learn better node embeddings. Such a better alignment is because the generator recovers the masked edges during pretraining, i.e., we observe that nearly 40% of the missing edges can be recovered. We remark that in our framework, the graph fed to the generator has missing edges, while the graph fed to the discriminator contains wrong edges since the generator may make erroneous predictions. However, empirically we find that missing edges hurt more than wrong ones, making discriminative pre-training more desirable (see Section 4.5 in the experiments).\n\nWe demonstrate effectiveness of DiP-GNN on large-scale homogeneous and heterogeneous graphs. Results show that the proposed method significantly outperforms existing generative pre-training and self-supervised learning approaches. For example, on the homogeneous Reddit dataset (Hamilton et al., 2017) that contains 230k nodes, we obtain an improvement of 1.1 in terms of F1 score; and on the heterogeneous OAG-CS graph (Tang et al., 2008) that contains 1.1M nodes, we obtain an improvement of 2.8 in terms of MRR score in the paper field prediction task.\n\n2 BACKGROUND\n\n⋄ Graph Neural Networks. Graph neural networks compute a node’s representation by aggregating information from the node’s neighbors. Concretely, for a multi-layer GNN, the feature vector h(k) of node v at the k-th layer is\n\nv\n\na(k)\n\nv = Aggregate\n\n(cid:16)(cid:110)\n\nh(k−1)\n\nu\n\n∀u ∈ Neighbor(v)\n\n(cid:111)(cid:17)\n\n, h(k)\n\nv = Combine\n\n(cid:16)\n\nv , h(k−1) a(k)\n\nv\n\n(cid:17)\n\n,\n\nwhere Neighbor(v) denotes all the neighbor nodes of v. Various implementations of Aggregate(·) and Combine(·) are proposed for both homogeneous (Defferrard et al., 2016; Kipf & Welling, 2017; Velickovic et al., 2018; Xu et al., 2019) and heterogeneous graphs (Schlichtkrull et al., 2018; Wang et al., 2019; Zhang et al., 2019; Hu et al., 2020c).\n\n⋄ Graph Neural Network Pre-Training. Previous unsupervised learning methods leverage the graph’s proximity (Tang et al., 2015) or information gathered by random walks (Perozzi et al., 2014; Grover & Leskovec, 2016; Dong et al., 2017; Qiu et al., 2018). However, the learned embeddings cannot be transferred to unseen nodes, limiting the methods’ applicability. Other unsupervised learning algorithms adopt contrastive learning (Hassani & Ahmadi, 2020; Qiu et al., 2020; Zhu et al., 2020; 2021; You et al., 2020; 2021). That is, we generate two views of the same graph, and then maximize agreement of node presentations in the two views. However, our experiments reveal that these methods do not scale well to extremely large graphs with millions of nodes.\n\nMany GNN pre-training methods focus on generative objectives. For example, GAE (Graph AutoEncoder, Kipf & Welling 2016) proposes to reconstruct the graph structure; GraphSAGE (Hamilton et al., 2017) optimizes an unsupervised loss derived from a random-walk-based metric; and DGI (Deep Graph Infomax, Velickovic et al. 2019) maximizes the mutual information between node representations and a graph summary representation.\n\nThere are also pre-training methods that extract graph-level representations, i.e., models are trained on a large amount of small graphs instead of a single large graph. For example, Hu et al. 2020a propose pre-training methods that operate on both graph and node level; and InfoGraph (Sun et al., 2020) proposes to maximize the mutual information between graph representations and representations of the graphs’ sub-structures. In this work, we focus on pre-training GNNs on a single large graph instead of multiple small graphs.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3 METHOD\n\nWe formally introduce the proposed discriminative GNN pre-training framework DiP-GNN. The algorithm contains two ingredients that operate on edges and features.\n\n3.1 EDGE GENERATION AND DISCRIMINATION\n\nSuppose we have a graph G = (N , E), where N denotes all the nodes and E denotes all the edges. We randomly mask out a proportion of the edges, such that E = Eu ∪ Em, where Eu is the unmasked set of edges and Em is the set of edges that are masked out.\n\nFor a masked edge e = (n1, n2) ∈ Em, where n1 and n2 are the two nodes connected by e, the generator’s goal is to predict n1 given n2 and the unmasked edges Eu. For each node n, we compute its representation hg(n) = f e g), which is parameterized by θe g. We remark that the computation of hg(·) only relies on the unmasked edges Eu. We assume that the generation process of each edge is independent. Then, we have the prediction probability\n\ng) using the generator f e\n\ng (n, θe\n\ng (·, θe\n\np(n1|n2, Eu) =\n\nexp (d(hg(n1), hg(n2))) n′∈C exp (d(hg(n′), hg(n2)))\n\n(cid:80)\n\n, C = {n1} ∪ (N \\ Neighbor(n2)).\n\n(1)\n\nHere, C is the candidate set for n1, which contains all the nodes that are not connected to n2 except n1 itself. Moreover, the distance function d(·, ·) is chosen as a trainable cosine similarity, i.e.,\n\nwhere W cos is a trainable weight. The training loss for the generator is defined as\n\nd(u, v) =\n\n(W cosu)⊤v ||W cosu|| · ||v||\n\n,\n\ng) = (cid:80) which is equivalent to maximizing the likelihood of correct predictions.\n\n− log p(n1|n2, Eu),\n\n(n1,n2)∈Em\n\ng(θe\n\nLe\n\n(2)\n\n(3)\n\nThe goal of the generator is to recover the masked edges in Em. Therefore, after we train the generator, we use the trained model to generate Eg = {((cid:98)n1, n2)}(n1,n2)∈Em, where each (cid:98)n1 is the model’s prediction as (cid:98)n1 = argmaxn′∈C p(n′|n2, Eu). Because the generator cannot correctly predict every edge, some edges in Eg are wrongly generated (i.e., not in Em). We refer to such edges as fake edges, and the rest as true edges. Concretely, we denote the true edges E true = Eu ∪(Em ∩Eg), i.e., the unmasked edges and the edges correctly generated by the generator. Correspondingly, we denote the fake edges E fake = E \\ E true.\n\nThe discriminator is trained to distinguish edges that are from the original graph (i.e., the true edges) and edges that are not (i.e., fake edges). Specifically, given the true edges E true and the fake ones E fake, we first compute hd(n) = f e d) is the discriminator model parameterized by θe d. We highlight that different from computing hg(·), the computation of hd(·) relies on all the edges, such that the discriminator can separate a fake edge from a true one. Then, for each edge e = (n1, n2) ∈ E true ∪ E fake, the discriminator outputs\n\nd) for every node n ∈ N , where f e\n\nd (n, θe\n\nd (·, θe\n\npfake = p(e ∈ E fake|E true, E fake) = sigmoid (d(hd(n1), hd(n2))) , where d(·, ·) is the distance function in Eq. 2. The training loss for the discriminator is the binary cross-entropy loss of predicting whether an edge is fake or not, defined as\n\n(4)\n\nLe\n\nd) = (cid:80) where 1{·} is the indicator function.\n\ne∈Etrue∪Efake\n\nd(θe\n\n−1{e ∈ E fake} log(pfake) − 1{e ∈ E true} log(1 − pfake),\n\n(5)\n\nThe edge loss is the weighted sum of the generator’s and the discriminator’s loss g, θe\n\nLe(θe where λ is a hyper-parameter. Note that structures of the generator f e d are flexible, e.g., they can be graph convolutional networks (GCN) or graph attention networks (GAT).\n\ng and the discriminator f e\n\ng) + λLe\n\nd) = Le\n\nd(θe\n\ng(θe\n\nd),\n\n(6)\n\n3.2 FEATURE GENERATION AND DISCRIMINATION\n\nIn real-world applications, nodes are often associated with features. For example, in the Reddit dataset (Hamilton et al., 2017), a node’s feature is a vectorized representation of the post corresponding to the node. As another example, in citation networks (Tang et al., 2008), a paper’s title\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ncan be treated as a node’s feature. Previous work (Hu et al., 2020b) has demonstrated that generating features and edges simultaneously can improve the GNN’s representation power.\n\nNode features can be either texts (e.g., in citation networks) or vectors (e.g., in recommendation systems). In this section, we develop feature generation and discrimination procedures for texts. Vector features are akin to encoded text features, and we can use linear layers to generate and discriminate them. Details about vector features are deferred to Appendix B.\n\ng (·, θf\n\ng ) = trmg ◦ embg(·) the generator parameterized by θf\n\nFor text features, we parameterize both the feature generator and the feature discriminator using bi-directional Transformer models (Vaswani et al., 2017), similar to BERT (Devlin et al., 2019). Denote f f g , where embg is the word embedding function and trmg denotes subsequent Transformer layers. For an input text feature x = [x1, · · · , xL] where L is the sequence length, we randomly select indices to mask out, i.e., we randomly select an index set M ⊂ {1, · · · , L}. For a masked position i ∈ M, the prediction probability is given by\n\np(xi|x) =\n\nexp (cid:0)embg(xi)⊤vg(xi)(cid:1) x′∈vocab exp (embg(x′)⊤vg(x′))\n\n(cid:80)\n\n, vg(xi) = trmg\n\n(cid:0)W proj\n\ng\n\n[hg(nx), embg(xi)](cid:1) . (7)\n\ng\n\nHere W proj is a trainable weight and hg(nx) is the representation of the node corresponding to x computed by the edge generation GNN. Note that we concatenate the text embedding embg(xi) and the feature node’s embedding hg(nx), such that the feature generator can aggregate information from the graph structure. We train the generator by maximizing the probability of predicting the correct token, i.e., by minimizing the loss\n\nLf\n\ng (θe\n\ng, θf\n\ng ) = (cid:80)\n\nx\n\n(cid:80)\n\ni∈M − log p(xi|x).\n\n(8)\n\nAfter we train the generator, we use the trained model to predict all the masked tokens, after which we obtain a new text feature xcorr. Here, we set xcorr i = (cid:98)xi for i ∈ M, where (cid:98)xi = argmaxx′∈vocab p(xi|x) is the generator’s prediction. The discriminator is trained to distinguish the fake tokens (i.e., wrongly generated tokens) from the true ones (i.e., the unmasked and correctly generated tokens) in xcorr. Similar to the generator, we denote f f d . For each position i, the discriminator’s prediction probability is defined as\n\nd ) = trmd ◦ embd(·) as the discriminator parameterized by θf\n\ni = xi for i /∈ M and xcorr\n\nd (·, θf\n\np(xcorr\n\ni = xi) = sigmoid (cid:0)w⊤vd(xcorr\n\ni\n\n)(cid:1) , vd(xcorr\n\ni\n\n) = trmd\n\n(cid:16)\n\nW proj\n\nd\n\n[hd(nx), embd(xcorr\n\ni\n\n(cid:17)\n\n)]\n\n.\n\n(9)\n\nHere w and W proj to x computed by the edge discriminator GNN. The training loss for the discriminator is\n\nare trainable weights and hd(nx) is the representation of the node corresponding\n\nd\n\nLf where ptrue = p(xcorr\n\nd, θf\n\nd ) = (cid:80) i = xi) and 1{·} is the indicator function.\n\ni=1 −1{xcorr\n\ni = xi} log(ptrue) − 1{xcorr\n\nd (θe\n\n(cid:80)L\n\nx\n\ni\n\n̸= xi} log(1 − ptrue),\n\n(10)\n\nThe text feature loss is defined as\n\nLf (θe\n\ng, θf\n\ng , θe\n\nd, θf\n\nd ) = Lf\n\ng (θe\n\ng, θf\n\ng ) + λLf\n\nd (θe\n\nd, θf\n\nd ),\n\nwhere λ is a hyper-parameter.\n\n3.3 MODEL TRAINING\n\nWe jointly minimize the edge loss and the feature loss, where the loss function is\n\nL(θe\n\ng, θf\n\ng , θe\n\nd, θf\n\nd ) = Le(θe = (cid:0)Le g(θe\n\ng, θe\n\nd) + Lf (θe\n\ng) + Lf\n\ng (θe\n\ng, θf\n\nd, θf d ) (cid:16)\n\ng , θe g, θf g )(cid:1) + λ\n\nLe\n\nd(θe\n\nd) + Lf\n\nd (θe\n\nd, θf d )\n\n(11)\n\n(cid:17)\n\n.\n\n(12)\n\nHere, λ is the weight of the discriminator’s loss. We remark that our framework is flexible because the generator’s loss (Le d ). As such, existing generative pre-training methods can be applied to train the generator. In DiP-GNN, the discriminator has a better quality than the generator because of the graph mismatch issue (see Section 4.5). Therefore, after pre-training, we discard the generator and fine-tune the discriminator on downstream tasks. A detailed training pipeline is presented in Appendix A.\n\ng ) is decoupled from the discriminator’s (Le\n\nd and Lf\n\ng and Lf\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.4 COMPARISON WITH GAN\n\nWe remark that our framework is different from Generative Adversarial Nets (GAN, Goodfellow et al. 2014). In GAN, the generator-discriminator training framework is formulated as a min-max game, where the generator is trained adversarially to fool the discriminator. The two models are updated using alternating gradient descent/ascent.\n\nHowever, the min-max game formulation of GAN is not applicable to our framework. This is because in GNN pre-trianing, the generator generates discrete edges, unlike continuous pixel values in the image domain. Such a property prohibits back-propagation from the discriminator to the generator. Existing works (Wang et al., 2018) use reinforcement learning (specifically policy gradient) to circumvent the non-differentiability issue. However, reinforcement learning introduces extensive hyper-parameter tuning and suffers from scalability issues. For example, the largest graph used in Wang et al. 2018 only contains 18k nodes, whereas the smallest graph used in our experiments has about 233k nodes.\n\nAdditionally, the goal of GAN is to train good-quality generators, which is different from our focus. In our discriminative pre-training framework, we focus on the discriminator because of better graph alignments. In practice, we find that accuracy of the generator is already high even without the discriminator, e.g., the accuracy is higher than 40% with 255 negative samples. And we observe that further improving the generator does not benefit downstream tasks.\n\n4 EXPERIMENTS\n\nWe implement all the algorithms using PyTorch (Paszke et al., 2019) and PyTorch Geometric (Fey & Lenssen, 2019). Experiments are conducted on NVIDIA A100 GPUs. By default, we use Heterogeneous Graph Transformer (HGT, Hu et al. 2020c) as the backbone GNN. We also discuss other choices in the experiments. Training and implementation details are deferred to Appendix C.\n\n4.1 SETTINGS AND DATASETS\n\n⋄ Settings. We consider a node transfer setting in the experiments. In practice we often work with a single large-scale graph, on which labels are sparse. In this case, we can use the large amount of unlabeled data as the pre-training dataset, and the rest are treated as labeled fine-tuning nodes. Correspondingly, edges between pre-training nodes are added to the pre-training data, and edges between fine-tuning nodes are added to the fine-tuning data. In this way, the model cannot see the fine-tuning data during pre-training, and vice versa.\n\nWe remark that our setting is different from conventional self-supervised learning settings, namely we pre-train and fine-tune on two separate graphs. This meets the practical need of transfer learning, e.g., a trained GNN needs to transfer across locales and time spans in recommendation systems.\n\n⋄ Homogeneous Graph. We use the Reddit dataset (Hamilton et al., 2017), which is a publicly available large-scale graph. In this graph, each node corresponds to a post, and is labeled with a “subreddit”. Each node has a 603-dimensional feature vector constructed from the corresponding post. Two nodes (posts) are connected if the same user commented on both. The dataset contains posts from 50 subreddits sampled from posts initiated in September 2014. In total, there are 232,965 posts with an average node degree of 492. We use 70% of the data as the pre-training data, and the rest as the fine-tuning data, which are further split into training, validation, and test sets equally. We consider node classification as the downstream fine-tuning task.\n\n⋄ Product Recommendation Graph. We collect in-house product recommendation data from an e-commerce website. We build a bi-partite graph with two node types: search queries and product ids. The dataset contains about 633k query nodes, 2.71M product nodes, and 228M edges. We sample 70% of the nodes (and corresponding edges) for pre-training, and the rest are evenly split for fine-tuning training, validation and testing. We consider link prediction as the downstream task, where for each validation and test query node, we randomly mask out 20% of its edges to recover. For each masked edge that corresponds to a query node and a positive product node, we randomly sample 255 negative products. The task is to find the positive product out of the total 256 products.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n⋄ Heterogeneous Graph. We use the OAG-CS dataset (Tang et al., 2008; Sinha et al., 2015), which is a publicly available heterogeneous graph containing computer science papers. The dataset contains over 1.1M nodes and 28.4M edges. In this graph, there are five node types (institute, author, venue, paper and field) and ten edge types. The “field” nodes are further categorized into six levels from L0 to L5, which are organized using a hierarchical tree. Details are shown in Figure 2.\n\nWe use papers published before 2014 as the pre-training dataset (63%), papers published between 2014 (inclusive) and 2016 (inclusive) as the fine-tuning training set (20%), papers published in 2017 as the fine-tuning validation set (7%), and papers published after 2017 as the fine-tuning test set (10%). During fine-tuning, by default we only use 10% of the fine-tuning training data (i.e., 2% of the overall data) because in practice labeled data are often scarce. We consider three tasks for fine-tuning: author name disambiguation (AD), paper field classification (PF) and paper venue classification (PV). For paper field classification, we only consider L2 fields. processed graph from Hu et al. 2020b.\n\nFigure 2: Details of OAG-CS. There are 5 node types (in black) and 10 edge types (in red).\n\nIn the experiments, we use the pre-\n\n4.2\n\nIMPLEMENTATION DETAILS\n\n⋄ Graph subsampling. In practice, graphs are often too large to fit in the hardware, e.g., the Reddit dataset (Hamilton et al., 2017) contains over 230k nodes. Therefore, we sample a dense subgraph from the large-scale graph in each training iteration. For homogeneous graphs, we apply the LADIES algorithm (Zou et al., 2019), which theoretically guarantees that the sampled nodes are highly inter-connected with each other and can maximally preserve the graph structure. For heterogeneous graphs, we use the HGSampling algorithm (Hu et al., 2020b), which is a heterogeneous version of LADIES.\n\nIn the edge generator, for a masked edge (s, t), we fix ⋄ Node sampling for the edge generator. the node t and seek to identify the other node s. One approach is to identify s from all the graph nodes, i.e., by setting C = N in Eq. 1. However, this task is computationally intractable when the number of nodes is large, i.e., the model needs to find s out of hundreds of thousands of nodes. i }nneg Therefore, we sample some negative nodes {sg i , t) /∈ E. Then, the candidate set to generate the source node becomes {s, sg } instead of all the graph nodes N . We remark nneg that such a sampling approach is standard for GNN pre-training and link prediction (Hamilton et al., 2017; Sun et al., 2020; Hu et al., 2020b).\n\ni=1 such that (sg\n\n1, · · · , sg\n\n⋄ Edge sampling for the edge discriminator. In computing the loss for the discriminator, the number of edges in Eu is significantly larger than those in Eg, i.e., we only mask a small proportion of the edges. To avoid the discriminator from outputting trivial predictions (i.e., all the edges belong to Eu), we balance the two loss terms in Le u| = α|Eg|, where α is a hyper-parameter. Then, we compute Le u. Note that the node representations hd are still computed using all the generated and unmasked edges Eg and Eu.\n\nd. Specifically, we sample E d\n\nu ⊂ Eu such that |E d\n\nd on Eg and E d\n\n4.3 BASELINES\n\nWe compare our method with several baselines in the experiments. For fair comparison, all the methods are trained for the same number of GPU hours.\n\n⋄ GAE (Graph Auto-Encoder, Kipf & Welling 2016) adopts an auto-encoder for unsupervised learning on graphs. In GAE, node embeddings are learnt using a GNN, and we minimize the discrepancy between the original and the reconstructed adjacency matrix.\n\n⋄ GraphSAGE (Hamilton et al., 2017) encourages embeddings of neighboring nodes to be similar. For each node, the method learns a function that generates embeddings by sampling and aggregating features from the node’s neighbors.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Experimental results on homogeneous graphs. We report F1 averaged over 10 runs for the Reddit data and MRR over 10 runs for the product recommendation data. The best results are shown in bold.\n\nTable 2: Experimental results on OAG-CS (heterogeneous). Left to right: paper-field, paper-venue, author-name-disambiguation. We report MRR over 10 runs. The best results are shown in bold.\n\nReddit Recomm.\n\nw/o pre-train\n\nGAE GraphSAGE DGI GPT-GNN GRACE GraphCL JOAOv2\n\nDiP-GNN\n\n87.3\n\n88.5 88.0 87.7 89.6 89.0 88.6 89.1\n\n90.7\n\n46.3\n\n56.7 53.0 53.3 58.6 51.5 —\n—\n\n60.1\n\nw/o pre-train\n\nGAE GraphSAGE DGI GPT-GNN GRACE GraphCL JOAOv2\n\nDiP-GNN\n\nPF\n\n32.7\n\n40.3 37.8 38.1 41.6 38.0 38.0 38.6\n\n44.1\n\nPV\n\n19.6\n\n24.5 22.1 22.5 25.6 21.5 22.0 23.5\n\n27.7\n\nAD\n\n60.0\n\n62.5 62.9 63.0 63.1 62.0 61.5 62.8\n\n65.6\n\n(a) Author name disambiguation.\n\n(b) Paper field classification.\n\n(c) Paper venue classification.\n\nFigure 3: Model performance vs. amount of labeled data on OAG-CS.\n\n⋄ DGI (Deep Graph Infomax, Velickovic et al. 2019) maximizes mutual information between node representations and corresponding high-level summaries of graphs. Thus, a node’s embedding summarizes a sub-graph centered around it.\n\n⋄ GPT-GNN (Hu et al., 2020b) adopts a generative pre-training objective. The method generates edges by minimizing a link prediction objective, and incorporates node features in the framework.\n\n⋄ GRACE (Graph Contrastive Representation, Zhu et al. 2020) leverages a contrastive objective. The algorithm generates two views of the same graph through node and feature corruption, and then maximize agreement of node representations in the two views.\n\n⋄ GraphCL (You et al., 2020) is another graph contrastive learning approach that adopts node and edge augmentation techniques, such as node dropping and edge perturbation.\n\n⋄ JOAO (Joint Augmentation Optimization, You et al. 2021) improves GraphCL by deigning a bi-level optimization objective to automatically and dynamically selects augmentation methods.\n\n4.4 MAIN RESULTS\n\nIn Table 1 and Table 2, w/o pre-train means direct training on the fine-tuning dataset without pretraining. Results on the Reddit dataset are F1 scores averaged over 10 runs, and results on the product recommendation graph are MRR scores averaged over 10 runs. All the performance gain have passed a hypothesis test with p-value < 0.05.\n\nTable 1 summarizes experimental results on the homogeneous graphs: Reddit and Recommendation. We see that pre-training indeed benefits downstream tasks. For example, performance of GNN improves by at ≥ 0.4 F1 on Reddit (DGI) and ≥ 5.2 MRR on Recommendation (GRACE). Also, notice that among the baselines, generative approaches (GAE and GPT-GNN) yield promising per-\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Neg. nodes for generation.\n\n(b) Pos. edges for discrimination.\n\n(c) Weight of discriminator’s loss.\n\nFigure 4: Ablation experiments on Reddit. By default, we set the number of negative nodes to 256, the factor of positive edges to 1.0, and weight of the discriminator’s loss to 20.\n\nTable 3: Test F1 score of model variants on Reddit.\n\nTable 4: Test F1 of models with different backbone graph neural networks on Reddit.\n\nModel\n\nEdges+Features\n\nEdges Features RandomEdges\n\nF1\n\n90.7\n\n90.4 90.2 89.8\n\nModel\n\nHGT GAT\n\nw/o pretrain\n\n87.3\n\n86.4\n\nGPT-GNN DiP-GNN\n\n89.6 90.7\n\n87.5 88.5\n\nFigure 5: F1 vs. proportion of manipulated edges on Reddit.\n\nformance. On the other hand, the contrastive method (GRACE, GraphCL and JOAO) does not scale well to large graphs, e.g., the OAG-CS graph which contains 1.1M nodes and 28.4M edges. By using the proposed discriminative pre-training framework, our method significantly outperforms all the baseline approaches. For example, DiP-GNN outperforms GPT-GNN by 1.1 on Reddit and 1.5 on Recommendation.\n\nExperimental results on the heterogeneous OAG-CS dataset are summarized in Table 2. Similar to the homogeneous graphs, notice that pre-training improves model performance by large margins. For example, pre-training improves MRR by at least 5.1, 2.5 and 2.5 on the PF, PV and AD tasks, respectively. Moreover, by using the proposed training framework, models can learn better node embeddings and yield consistently better performance compared with all the baselines.\n\nRecall that during fine-tuning on OAG-CS, we only use 10% of the labeled fine-tuning data (about 2% of the overall data). In Figure 3, we examine the effect of the amount of labeled data. We see that model performance improves when we increase the amount of labeled data. Also, notice that DiP-GNN consistently outperforms GPT-GNN in all the three tasks under all the settings.\n\n4.5 ANALYSIS\n\n⋄ Comparison with semi-supervised learning. We compare DiP-GNN with a semi-supervised learning method: C&S (Correct&Smooth, Huang et al. 2020). Figure 6 summarizes the results. We see that C&S yields a 0.5 improvement compared with the supervised learning method (i.e., w/o pre-train). However, performance of C&S is significantly lower than both DiP-GNN and other pre-training methods such as GPT-GNN.\n\n⋄ Hyper-parameters. There are several hyper-parameters that we introduce in DiP-GNN: the number of negative nodes that are sampled for generating edges (Section 4.2); the number of positive edges that are sampled for the discriminator’s task (Section 4.2); and the weight of the discriminator’s loss (Eq. 12). Figure 4 illustrate ablation experimental results on the Reddit dataset. From the results, we see that DiP-GNN is robust to these hyper-parameters. We remark that under all the settings, ours model behaves better than the best-performing baseline (89.6 for GPT-GNN).\n\nFigure 6: Test F1 on Reddit.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Generator and discriminator performance vs. proportion of masked edges during pre-training. Coverage is the proportion of true edges input to the models.\n\nMasked%\n\nAcc\n\nCoverage\n\nGen. Dis. Gen. Dis.\n\nRatio\n\n20 80 95\n\n0.50 0.33 0.20\n\n0.87 0.84 0.80\n\n0.80 0.20 0.05\n\n0.90 ×1.13 0.46 ×2.30 0.24 ×4.80\n\nFigure 7: Performance vs. proportion of masked edges on product recommendation.\n\n⋄ Model variants. We also examine variants of DiP-GNN. Recall that the generator and the discriminator operate on both edges and node features. We first check the contribution of these two factors. We also investigate the scenario where edges are randomly generated, and the discriminator still seeks to find the generated edges. Table 3 summarizes results on the Reddit dataset.\n\nWe see that by only using edges, model performance drops by 0.3; and by only using node features, performance drops by 0.5. This indicates that the graph structure plays a more important role in the proposed framework than the features. Also notice that performance of RandomEdges is unsatisfactory. This is because implausible edges are generated when using a random generator, making the discriminator’s task significantly easier. We remark that performance of all the model variants is better than the best-performing baseline (89.6 for GPT-GNN).\n\nTable 4 examines performance of our method and GPT-GNN using different backbone GNNs. Recall that by default, we use HGT (Hu et al., 2020c) as the backbone. We see that when GAT (Velickovic et al., 2018) is used, performance of DiP-GNN is still significantly better than GPT-GNN.\n\n⋄ Missing edges hurt more than wrong edges. In our pre-training framework, the generator is trained to reconstruct the masked graph, after which the reconstructed graph is fed to the discriminator. During this procedure, the graph input to the generator has missing edges, and the graph input to the discriminator has wrong edges. From Figure 5, we see that wrong edges hurt less than missing ones. For example, model performance drops by 0.7% when 50% of wrong edges are added to the original graph, and performance decreases by 1.8% when 50% of original edges are missing. This indicates that performance relies on the amount of original edges seen by the models. Intuitively, wrong edges add noise to the graph, but they do not affect information flow. On the contrary, missing edges cut information flow. Moreover, in practice we work with graph attention models, and the attention mechanism can alleviate the wrong edges by assigning low attention scores to them.\n\n⋄ Why is discriminative pre-training better? Figure 7 illustrates effect of the proportion of masked edges during pre-training. We see that when we increase the proportion from 0.2 to 0.8, performance of GPT-GNN drops by 6.1, whereas performance of DiP-GNN only drops by 3.3. This indicates that the generative pre-training method is more sensitive to the masking proportion.\n\nTable 5 summarizes pre-training quality. First, the generative task (i.e., the generator) is more difficult than the discriminative task (i.e., the discriminator). For example, when we increase the proportion of masked edges from 20% to 80%, accuracy of the generator drops by 17% while accuracy of the discriminator only decreases by 3%. Second, the graph input to the discriminator better aligns with the original graph. For example, when 80% of the edges are masked, the discriminator sees 2.3 times more original edges than the generator. Therefore, the discriminative task is more advantageous because model quality relies on the number of observed original edges (Figure 5).\n\n5 CONCLUSION AND DISCUSSIONS\n\nWe propose Discriminative Pre-Training of Graph Neural Networks (DiP-GNN), where we simultaneously train a generator and a discriminator. During pre-training, we mask out some edges in the graph, and a generator is trained to recover the masked edges. Subsequently, a discriminator seeks to distinguish the generated edges from the original ones. Our framework is more advantageous than generative pre-training for two reasons: the graph inputted to the discriminator better matches the original graph; and the discriminative pre-training task better aligns with downstream node classification. We conduct extensive experiments to validate the effectiveness of DiP-GNN.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n\nMicha ̈el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 510, 2016, Barcelona, Spain, pp. 3837–3845, 2016.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\n\nYuxiao Dong, Nitesh V. Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017, pp. 135–144. ACM, 2017. doi: 10.1145/3097983.3098036.\n\nMatthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In\n\nICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672–2680, 2014.\n\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks.\n\nIn Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi (eds.), Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pp. 855–864. ACM, 2016. doi: 10.1145/2939672.2939754.\n\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1024–1034, 2017.\n\nKaveh Hassani and Amir Hosein Khas Ahmadi. Contrastive multi-view representation learning on graphs. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 4116–4126. PMLR, 2020.\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure In 8th International Conference Leskovec. Strategies for pre-training graph neural networks. on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020a.\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. GPT-GNN: generative pre-training of graph neural networks. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pp. 1857–1867. ACM, 2020b.\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous graph transformer. In Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten van Steen (eds.), WWW ’20: The Web\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nConference 2020, Taipei, Taiwan, April 20-24, 2020, pp. 2704–2710. ACM / IW3C2, 2020c. doi: 10.1145/3366423.3380027.\n\nQian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R Benson. Combining arXiv preprint\n\nlabel propagation and simple models out-performs graph neural networks. arXiv:2010.13993, 2020.\n\nThomas N Kipf and Max Welling.\n\nVariational graph auto-encoders.\n\narXiv preprint\n\narXiv:1611.07308, 2016.\n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ̈opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ́e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019.\n\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In Sofus A. Macskassy, Claudia Perlich, Jure Leskovec, Wei Wang, and Rayid Ghani (eds.), The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, New York, NY, USA - August 24 - 27, 2014, pp. 701–710. ACM, 2014. doi: 10.1145/2623330.2623732.\n\nJiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Yi Chang, Chengxiang Zhai, Yan Liu, and Yoelle Maarek (eds.), Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018, pp. 459–467. ACM, 2018. doi: 10.1145/3159652.3159706.\n\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. GCC: graph contrastive coding for graph neural network pre-training. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pp. 1150–1160. ACM, 2020.\n\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European semantic web conference, pp. 593–607. Springer, 2018.\n\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and Kuansan Wang. An overview of microsoft academic service (mas) and applications. In Proceedings of the 24th international conference on world wide web, pp. 243–246, 2015.\n\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: large-scale information network embedding. In Aldo Gangemi, Stefano Leonardi, and Alessandro Panconesi (eds.), Proceedings of the 24th International Conference on World Wide Web, WWW 2015, Florence, Italy, May 18-22, 2015, pp. 1067–1077. ACM, 2015. doi: 10.1145/2736277.2741093.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and In Proceedings of the 14th ACM SIGKDD international\n\nmining of academic social networks. conference on Knowledge discovery and data mining, pp. 990–998, 2008.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017.\n\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n\nPetar Velickovic, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R. Devon Hjelm. Deep graph infomax. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nHongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, and Minyi Guo. Graphgan: Graph representation learning with generative adversarial nets. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 2508–2515. AAAI Press, 2018.\n\nXiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S. Yu. Heterogeneous graph attention network. In Ling Liu, Ryen W. White, Amin Mantrach, Fabrizio Silvestri, Julian J. McAuley, Ricardo Baeza-Yates, and Leila Zia (eds.), The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019, pp. 2022–2032. ACM, 2019. doi: 10.1145/ 3308558.3313562.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Yike Guo and Faisal Farooq (eds.), Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018, pp. 974–983. ACM, 2018. doi: 10.1145/3219819.3219890.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812–5823, 2020.\n\nYuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning auto-\n\nmated. In International Conference on Machine Learning, pp. 12121–12132. PMLR, 2021.\n\nChuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V. Chawla. Heterogeneous graph neural network. In Ankur Teredesai, Vipin Kumar, Ying Li, R ́omer Rosales, Evimaria Terzi, and George Karypis (eds.), Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp. 793–803. ACM, 2019. doi: 10.1145/3292500.3330961.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\n\nrepresentation learning. arXiv preprint arXiv:2006.04131, 2020.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, pp. 2069–2080, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nDifan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ́e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11247–11256, 2019.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA DETAILED ALGORITHM\n\nAlgorithm 1 is a detailed training pipeline of DiP-GNN. For graphs with vector features instead of text features, we can substitute the feature generation and discrimination modules with equations in Appendix B.\n\nAlgorithm 1: DiP-GNN: Discriminative Pre-training of Graph Neural Networks. Input: Graph Gfull; edge masking ratio; feature masking ratio; number of negative samples for edge generator; proportion of positive samples for edge discriminator α; weight of the discriminator’s loss λ; number of training steps T .\n\nfor t = 0, · · · , T − 1 do\n\n// Graph subsampling. Sample a subgraph G = (N , E) from Gfull; // Edge generation. Initialize the generated edge set Eg = {} and the edge generation loss Le Construct the unmasked set of edges Eu and the masked set Em such that E = Eu ∪ Em; Compute node embeddings using Eu; for e = (n1, n2) ∈ Em do\n\ng = 0;\n\nConstruct candidate set C for n1 (n2 is given during generation) via negative sampling; Generate (cid:98)e = ((cid:98)n1, n2) where (cid:98)n1 ∈ C; Update the generated edge set Eg ← Eg ∪ {(cid:98)e}; Update the edge generation loss Le g; // Text Feature generation. Initialize the feature generation loss Lf for n ∈ N do\n\ng = 0;\n\nFor the node’s text feature xn, mask out some of its tokens; Construct the generated text feature xcorr\n\nn\n\nusing the embedding of node n (computed\n\nduring edge generation) and the feature generation Transformer model; Update the feature generation loss Lf g ;\n\n// Edge discrimination. Initialize the edge discrimination loss Le Compute node embeddings using Eg ∪ Eu; Sample E d for e = (n1, n2) ∈ Eg ∪ E d\n\nu ⊂ Eu such that |E d u do\n\nu| = α|Eg|;\n\nd = 0;\n\nDetermine if e is generated using the embedding of n1 and n2; Update the edge discrimination loss Le d; // Text feature discrimination. Initialize the feature discrimination loss Lf for n ∈ N do\n\nd = 0;\n\nFor the node’s generated text feature xcorr\n\nn , determine whether each token is generated\n\nusing the embedding of node n (computed during edge discrimination) and the feature discrimination Transformer model; Update the feature discrimination loss Lf d ;\n\n// Model updates. g + Lf Compute L = (Le\n\ng ) + λ(Le\n\nd + Lf\n\nd ) and update the model;\n\nOutput: Trained discriminator ready for fine-tuning.\n\nB GENERATION AND DISCRIMINATION OF VECTOR FEATURES\n\nNode features can be vectors instead of texts, e.g., the feature vector can contain topological information such as connectivity information. In this case both the generator and the discriminator are parameterized by a linear layer.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTo generate feature vectors, we first randomly select some nodes Ng ⊂ N . For a node n ∈ N , denote its feature vector vn, then the feature generation loss is\n\nLf\n\ng (Wg) =\n\n(cid:88)\n\nn∈Ng\n\n||(cid:98)vn − vn||2\n\n2 , where (cid:98)vn = W f\n\ng hg(n).\n\nHere hg(n) is the representation of node n and W f construct its corred feature vcorr\n\nn = (cid:98)vn if n ∈ Ng and vcorr\n\ng is a trainable weight. For a node n ∈ N , we\n\nn = vn if n ∈ N \\ Ng.\n\nThe discriminator’s goal is to differentiate the generated features from the original ones. Specifically, the prediction probability is\n\np(n ∈ Ng) = sigmoid (cid:0)W d\n\nd hd(n)(cid:1) ,\n\nwhere W f on the corred feature vcorr\n\nd is a trainable weight. We remark that the node representation hd(n) is computed based\n\nn . Correspondingly, the discriminator’s loss is\n\nLf\n\nd (Wd) =\n\n(cid:88)\n\nn∈N\n\n−1{n ∈ Ng} log p(n ∈ Ng) − 1{n ∈ N \\ Ng} log(1 − p(n ∈ Ng)).\n\nThe vector feature loss Lf (θe the text feature loss.\n\ng, W f\n\ng , θe\n\nd, W f\n\nd ) = Lf\n\ng (θe\n\ng, W f\n\ng ) + Lf\n\nd (θe\n\nd, W f\n\nd ) is computed similar to\n\nC IMPLEMENTATION AND TRAINING DETAILS\n\nBy default, we use Heterogeneous Graph Transformer (HGT, Hu et al. 2020c) as the backbone GNN. In the experiments, the edge generator and discriminator have the same architecture, where we set the hidden dimension to 400, the number of layers to 3, and the number of attention heads to 8. For the OAG dataset which contains text features, the feature generator and discriminator employs the same architecture: a 4 layer bi-directional Transformer model, similar to BERT (Devlin et al., 2019), where we set the embedding dimension to 128 and the hidden dimension of the feed-forward neural network to 512.\n\nFor pre-training, we mask out 20% of the edges and 20% of the features (for text features we mask out 20% of the tokens). We use AdamW (Loshchilov & Hutter, 2019) as the optimizer, where we set β = (0.9, 0.999), ε = 10−8, the learning rate to 0.001 and the weight decay to 0.01. We adopt a dropout ratio of 0.2 and gradient norm clipping of 0.5. For graph subsampling, we set the depth to 6 and width to 128, the same setting as Hu et al. 2020b.\n\nFor fine-tuning, we use AdamW (Loshchilov & Hutter, 2019) as the optimizer, where we set β = (0.9, 0.999), ε = 10−6, and we do not use weight decay. We use the same graph subsampling setting as pre-training. The other hyper-parameters are detailed in Table 6.\n\nTable 6: Hyper-parameters for fine-tuning tasks.\n\nDataset\n\nTask\n\nSteps Dropout Learning rate Gradient clipping\n\nReddit\n\n— 2400\n\nRecomm. — 1600\n\nOAG-CS\n\nPF PV AD\n\n1600 1600 1600\n\n0.3\n\n0.1\n\n0.2 0.2 0.2\n\n0.0015\n\n0.0010\n\n0.0010 0.0005 0.0005\n\n0.5\n\n0.5\n\n0.5 0.5 0.5\n\n15",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a discriminative method for pre-training Graph Neural Networks. The main idea is to simultaneously train a generator to recover identities of the masked edges, and train a discriminator to distinguish the generated edges from the original graph’s edges.\n\n# Strength And Weaknesses\n\nStrength\n- The proposed method makes sense.\n- The empirical results show the improved performance brought by the proposed methods on a number of  benchmarks.\n\nWeaknesses\n- There is no mentioning of n_1^hat or generated edges e_g in the loss of the generator. How should I understand that the generated edges are not captured in the generator loss?\n\n- Also, it is said that the discriminator is to distinguish edges that are from the original graph and edges that are generated. It would make sense to contrast the masked edges vs the generated edges, especially the ones that are different. However, the training loss for the discriminator seems to still focus on the unmasked edges vs generated edges. I don’t quite understand the intuition.\n\n- A masked edge has two end nodes n_1 and n_2. How do the authors decide which end node to predict?\n\n- Why is the same lambda shared between edge loss and feature loss, given these two losses are so different?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is unclear in various aspects, as explained in above comments.\n\n# Summary Of The Review\n\nOverall, the proposed method makes sense. However, I feel the authors are selling the methodology wrong. Instead of saying that discriminative pre-training is better than generative training, the authors should present it as a method for applying generative and discriminative pre-training jointly. Otherwise, why don’t you just rely solely on discriminative pre-training?\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nGENERATING SEQUENCES BY LEARNING TO [SELF-]CORRECT\n\nSean Welleck1,3,* Ximing Lu1,* Peter West3,† Faeze Brahman1,3,†\n\nTianxiao Shen3 Daniel Khashabi2 Yejin Choi1,3 1Allen Institute for Artificial Intelligence 2Center for Language and Speech Processing, Johns Hopkins University 3Paul G. Allen School of Computer Science & Engineering, University of Washington\n\nABSTRACT\n\nSequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present SELF-CORRECTION, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that SELFCORRECTION improves upon the base generator in three diverse generation tasks– mathematical program synthesis, lexically-constrained generation, and toxicity control– even when the corrector is much smaller than the base generator.\n\n1\n\nINTRODUCTION\n\nThe standard practice for natural language generation tasks is inherently single-pass: applying a decoding procedure to either a few-shot prompted language model or one tuned for a given task, then considering the generation as “finished” (e.g. Radford et al. (2019); Brown et al. (2020); Chen et al. (2021)). Powerful generation models often meet most of the task requirements, yet miss a few (e.g., omitting a subset of keywords), or generate incorrect hypotheses that nevertheless provide useful structure (e.g., a correct problem solving strategy with a missing step). However, after generating even a slightly sub-optimal sequence, the single-pass paradigm requires models to “start from scratch”, effectively discarding work already done. A more natural, intuitive approach is leveraging the generation as a useful starting point to refine into a higher quality output.\n\nTo formalize this intuition, we introduce Self-Correction for Sequence Generation. Figure 1 demonstrates its central principle: a generation model is re-framed as a base generator, which produces a reasonable initial hypothesis but does not need to solve the task in one pass, and a second module–the corrector–trained to make up the difference between the hypothesis and an optimal solution. Neither the generator nor the corrector must solve the full task in one pass, and the corrector can be applied multiple times to iteratively improve the output (§3.6). We propose a simple, general procedure for training the corrector (Figure 2) by pairing generator outputs with carefully selected targets. The result is a system which self-corrects, producing outputs through multiple generation passes and breaking the task into steps that can be solved by dedicated and efficient sub-systems.\n\nSelf-Correction builds on past work for correction in the code and text (e.g. Yasunaga et al. (2021); Faltings et al. (2021)) domains, but provides a unified formalism with minimal assumptions about\n\n∗First authors, contributed equally. †Second authors, contributed equally.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: SELF-CORRECTORs decompose generation into a base generator that proposes an initial hypothesis, and a corrector that iteratively improves its quality.\n\ndata and feedback, which applies generally to diverse tasks. A corrector model improves the base generator on 3 such tasks in our experiments: mathematical program synthesis (§3.1), lexically constrained generation (§3.2), and toxicity reduction (§3.3). The trained corrector model even transfers to a larger generator with similar performance to training from scratch (§3.4). Finally, we explore introducing a third module to the Self-Correction system (§3.5)–explicitly using natural language feedback to guide corrections–with promising results. Self-Correction is an exciting path to build on the generations of strong models, with efficient, effective, and transferable corrector networks.\n\n2 SELF-CORRECTING SEQUENCE GENERATORS\n\nA typical autoregressive text generator (e.g. GPT-3 (Brown et al., 2020)) maps an input prompt to a distribution over outputs using a single parameterized module (e.g. a large transformer), p0(y|x). We explore an alternative that decomposes into two modules, a base generator, and a corrector,\n\np(y|x) =\n\n(cid:88)\n\ny0\n\np0(y0|x) (cid:124) (cid:123)(cid:122) (cid:125) generator\n\npθ(y|y0, x) (cid:125) (cid:123)(cid:122) (cid:124) corrector\n\n(1)\n\nwhere the generator provides an initial hypothesis that is refined by the corrector. In practice, the corrector can be applied multiple times, p(yT |x) = (cid:80) t pθ(yt+1|yt, x). Since a model of this form can both generate and correct its generations, we call it a Self-Corrector.\n\np0(y0|x) (cid:81)\n\n· · · (cid:80)\n\nyT −1\n\n(cid:80)\n\ny1\n\ny0\n\nSelf-correctors have several unique properties compared to typical generators. First, a self-corrector decouples generation and correction, allowing us to freely parameterize each module – for instance, by prompting a single language model or using two different language models. In this paper, we develop a framework to train a separate corrector model (§2.1). We find that the resulting selfcorrector improves upon the generator alone (§3), even when the corrector is much smaller (§3.4).\n\nSecond, since the generator and the corrector are separated, we can keep the generator as a generalpurpose language model and train the corrector with different objectives for different task requirements. In §2.1, we propose a training algorithm for the corrector that is dedicated to improving generations, where the improvement can be in any aspect, measured by scalar values.\n\nThird, the corrector can receive explicit feedback about intermediate generations to guide subsequent generations. Formally, p(y|x) = (cid:80) p0(y0|x)pθ(y|y0, x, f (y0)), where f is the feedback. The feedback can be of many forms, e.g. a sentence, a compiler trace, etc. In contrast, a typical generator that generates in a single pass does not leverage feedback on its own generation. In this paper, we show that the corrector can learn to exploit explicit natural language feedback to achieve better performance (§3.5). Next, we describe our training framework of the corrector.\n\ny0\n\n2.1 LEARNING A CORRECTOR\n\nOur goal is to have the generator generate an initial hypothesis, then improve the hypothesis with the corrector (Eq. 1). We train the corrector to improve the quality of a hypothesis, while staying as close as possible to the original hypothesis. Here, quality is measured with a scalar value function v(y) which is accessible at training time (e.g. 0/1 indicator of program correctness, a toxicity score).\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: SELF-CORRECTIVE LEARNING iteratively trains a corrector by generating hypotheses and corrections, forming value-improving pairs, and selecting those with high similarity for learning.\n\nAlgorithm 1 Self-corrective learning input Generator p0, corrector pθ, prompts X, value v(·), feedback f (·)\n\nInitialize datapool D by sampling from p0 for iteration ∈ {1, 2, . . .} do\n\nForm value-improving pairs P from D for step in 1, 2, . . . , M do\n\nSample a batch of value-improving pairs from P using Eq. 4 Compute the loss and update θ using gradient descent\n\nfor x ∈ X do\n\nSample hypotheses y from datapool D Generate corrections y′ ∼ pθ(·|y, x, f (y)) Add all (x, y′, v(y′), f (y′)) to the datapool D\n\n▷ Initialization: Eq. 2\n\n▷ Pairing: Eq. 3\n\n▷ Learning\n\n▷ Exploration: Eq. 5\n\nSince direct supervision on how to improve hypotheses is not available, we design a new algorithm to train the corrector, which we refer to as self-corrective learning. The algorithm collects a pool of generations, pairs them and selects pairs of generation that increase in value and are nearby, then updates the corrector on these pairs. As training progresses, more generations are added to the pool using the current corrector. Algorithm 1 summarizes self-corrective learning, detailed below.\n\nInitialization. Self-corrective learning begins with a generator p0(y0|x), a corrector pθ(y′|y, x) , a set of training prompts X, and a value function v : Y → R. Optionally, we can use additional feedback f : Y → F and learn pθ(y′|y, x, f (y)), where F is arbitrary.\n\nThe algorithm initializes a datapool of (input, output, value, feedback) examples by using the generator to generate multiple outputs for each input. Formally,\n\nDx = {(x, y, v(y), f (y)) | for all y ∈ y1:N ∼ q(p0(·|x))}, D =\n\n(cid:91)\n\nx∈X\n\nDx,\n\n(2)\n\nwhere y1:N denotes N outputs generated with decoding algorithm q (e.g. temperature sampling). When available, (x, y, v(y), f (y)) examples from another source (e.g. a dataset) can also be added.\n\nPairing. Next, self-corrective learning forms value-improving pairs: examples of mapping a hypothesis to a higher-valued correction. We use the datapool D to form a set of (input, hypothesis, correction) pairs. A pair is formed when an output has a higher value than another ∗:\n\nPx = {(x, y, y′) | v(y) < v(y′) for all y, y′ ∈ Dx × Dx}, P =\n\n(cid:91)\n\nx∈X\n\nPx,\n\n(3)\n\nLearning. Next, self-corrective learning selects (input, hypothesis, correction) pairs to update the corrector with. We sample an input, x ∼ U(X), then sample a (x, y, y′) pair proportional to its\n\n∗We also store the value and feedback for y and y′ along with (x, y, y′), which we omit to reduce clutter.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nimprovement in value as well as the proximity between the hypothesis y and the correction y′:,\n\nP[(x, y, y′)|x] ∝ exp (cid:0) α · (v(y′) − v(y)) (cid:125)\n\n+ β · s(y, y′) (cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) improvement where s(y, y′) is a similarity function and Z(y) normalizes over the available corrections for y in Px. Increasing the hyperparameter α ∈ R≥0 puts more weight on targets that add more value, while increasing β ∈ R≥0 retains more similar targets. We update the corrector using the cross-entropy loss L(θ) = − log pθ(y′|y, x, f (y)) on batches sampled in this way.\n\n(cid:123)(cid:122) proximity\n\n(cid:124)\n\n(cid:1)/Z(y),\n\n(4)\n\nExploration. During exploration, self-corrective learning adds new generations to the datapool by generating from the current corrector:\n\nD′\n\nx = {(x, y′, v(y′), f (y′)) | for all y′ ∈ y′1:N ∼ q(pθ(·|y, x, f (y))}, D′ =\n\n(cid:91)\n\nD′\n\nx\n\nx∈X\n\n(5)\n\nand updating the datapool D ← D ∪D′. The hypotheses y to correct can come from any source, e.g. newly sampled from the base generator, or from the datapool; we use the latter in our experiments.\n\nInference. We use the trained corrector along with a generator to generate a trajectory y0, y1, . . . , yT , and consider yT the final output. Since marginalizing over the intermediate generations in Eq. 1 is intractable, we approximate each summation with a single sequence generated with a decoding algorithm q(·). That is, we decode from the generator, then repeatedly from the corrector:\n\n• Generation: y0 ∼ q(p0(y0|x)); • Correction: yt+1 ∼ q(pθ(yt+1|yt, x, f (yt))),\n\nt = 0, 1, . . . , T − 1.\n\nThe stopping time T is either fixed, or when a target value is obtained (if v(y) is available).\n\n3 EXPERIMENTS\n\nWe evaluate SELF-CORRECTION on a diversity of tasks: mathematical program synthesis, in which generations are strictly correct or incorrect, and generators typically have low performance; lexically-constrained generation, which allows for partial credit, and generators usually give partially-correct solutions (e.g. matching 3 out of 5 constraints); and toxicity control, where ‘correctness’ is more loosely defined, and the output space is much more open-ended. Our experiments are organized to study three settings:\n\n1. Using self-correctors to improve upon generators (§3.1,3.2,3.3). 2. Correcting generators that are much larger than the corrector (§3.4). 3. Leveraging explicit feedback during training and inference (§3.5).\n\nNext, we describe the self-correction setup and baselines for each task, along with their results. ∗\n\n3.1 MATHEMATICAL PROGRAM SYNTHESIS\n\nFirst, we consider mathematical program synthesis (Austin et al., 2021; Mishra et al., 2022). Given a natural language problem specification x, the task is to generate a program y that upon execution returns the correct answer to x. The task is challenging as it draws on language understanding, multiple-step mathematical problem solving (e.g. identifying a solution strategy, decomposing a problem), and leveraging symbolic tools (e.g. built-in operations, variables). Furthermore, the task demands a high level of precision, e.g. a single misplaced operation makes the program incorrect.\n\nExperimental setup. As the corrector we use GPT-Neo 1.3B (Black et al., 2021), an open-source autoregressive language model. GPT-Neo is pre-trained on language and code (Gao et al., 2021), and hence is widely used for code-related generation (e.g. Chen et al. (2021); Ni et al. (2022); Mishra et al. (2022)). We consider two settings for the initial generator: (1) a separate fine-tuned instance of GPT-Neo 1.3B, and (2) few-shot prompted GPT-3 (Brown et al., 2020). For GPT-3, we evaluate the davinci and text-davinci-002 engines, representative of large (≈ 175B∗) generators that are state-of-the-art in related tasks (Wei et al., 2022). See the Appendix for additional details.\n\n∗Code will be available at www.github.com/wellecks/self_correction. ∗Estimated size of davinci (https://blog.eleuther.ai/gpt3-model-sizes). Further details not available.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nDataset\n\nModel\n\nCorrect\n\nDataset Model\n\nParams Correct\n\nMultiarith GPT-NEO 1.3B\n\nMultitask\n\n+SELF-CORRECT +SELF-CORRECT∗\n\nGPT-NEO 1.3B +SELF-CORRECT +SELF-CORRECT∗\n\n60.00 98.33 99.17\n\n49.02 73.53 78.24\n\nGSM OpenAI 3B [6] OpenAI 6B [6] GPT-NEO [34] NEO FCP+PCP [34]\n\nGPT-NEO +SELF-CORRECT +SELF-CORRECT∗\n\n3B 6B 2.7B 2.7B\n\n1.3B 1.3B 1.3B\n\n15.50 20.00 18.80 19.50\n\n8.57 21.26 24.22\n\nTable 1: Evaluation results of mathematical program synthesis experiments. GPT-NEO (1.3B) is the initial generator for SELF-CORRECT. SELF-CORRECT∗ means only applying the corrector to incorrect outputs. Italicized: original non-program version of GSM.\n\nProblem: It takes Jennifer 20 minutes to groom each of her 2 long hair dachschunds. If she grooms her dogs every day, how many hours does she spend grooming her dogs in 30 days?\n\nProblem: Mrs. Wilsborough saved $500 to buy concert tickets for her family. She bought 2 VIP tickets at $100 each and 3 regular tickets at $50 each. How much of her savings does Mrs. Wilsborough have after she buys the tickets?\n\nGenerator:\n\nCorrector:\n\nGenerator:\n\nCorrector:\n\na=20*2 b=a*30 answer=b print(answer)\n\na=20*2 b=a*30 c=b/60 #fix answer=c print(answer)\n\na=2*100 b=3*50 c=a+b answer=c print(answer)\n\na=2*100 b=3*50 c=500-a-b #fix answer=c print(answer)\n\nFigure 3: Grade-school-math (GSM) self-corrections. On the left, the corrector fixes the units (from minutes to hours) in the generator’s solution. On the right, the corrector revises the logic so that the program computes the total savings instead of the spent on tickets. We add #fix here to indicate the change. See Figure 7 and Figure 8 for additional examples.\n\nSelf-correction setup. As the value function we use correctness, which is 1 when the program y executes and outputs the ground-truth answer and 0 otherwise. Our main experiments do not use explicit feedback, i.e. f (y) = ∅. At inference time, we study two settings for the corrector: (1) applying k corrections and selecting the final generation, (2) an oracle setting that only corrects a draft if the draft is incorrect. We use greedy decoding for the generator and corrector, and k = 1.\n\nDatasets. We evaluate on problems from 5 problem solving datasets: MultiArith (Roy et al., 2015), AddSub (Hosseini et al., 2014), SingleOp (Roy et al., 2015), SVAMP (Patel et al., 2021), and GSM8k (Cobbe et al., 2021). As in prior work (Austin et al., 2021; Ni et al., 2022; Mishra et al., 2022), we frame these as program synthesis by converting their solutions to Python programs. We separate our experiments into three increasingly difficult settings:\n\n1. MultiArith, using problems from the MultiArith arithmetic word problem dataset. 2. Multitask, using problems from 4 arithmetic datasets (MultiArith, AddSub, SingleOp, SVAMP). 3. GSM, using problems from the challenging GSM8k dataset.\n\nFor the MultiArith and Multitask settings, we make train/valid/test splits using 60/20/20% of the respective datasets. Similar to Ni et al. (2022), for the GSM setting we use the official GSM8k test split, and create a validation split using 20% of the training set. Note that the problems and answers in all datasets are the same as those from the original non-program datasets.\n\nBaselines. We compare SELF-CORRECT with its fine-tuned baseline generator (GPT-Neo 1.3B) in all three settings. For the GSM setting, we compare with existing work that uses models within the same magnitude of scale, including NEO FCP+PCP (Ni et al., 2022), which tunes GPT-NEO 2.7B with additional self-sampled programs, and their fine-tuned GPT-NEO 2.7B baseline. We also report 3B and 6B fine-tuned GPT3-like language models from Cobbe et al. (2021), which were trained on the non-program version of GSM8k. We evaluate larger models later in (§3.4).\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nRuntime CIDER Constraints\n\nMethod\n\nFluency Constraints\n\nNeuroLogic [28] NeuroLogic-A* [30]\n\n2.04s 19.24s\n\nGPT-2 SELF-CORRECT +NeuroLogic\n\n0.20s 0.80s 2.24s\n\n14.70 15.20\n\n14.97 15.30 15.28\n\n97.70 97.80\n\n91.38 94.58 97.80\n\nPrefix-Tuning [21] NeuroLogic [28] NeuroLogic-A* [30]\n\nGPT-2 SELF-CORRECT\n\n2.96 2.80 2.85\n\n2.94 2.98\n\n91.16 96.91 96.97\n\n91.50 98.77\n\nTable 2: Lexically-constrained generation. By training a corrector to optimize constraint satisfaction, SELF-CORRECT improves constraints while maintaining fluency, without modifying the underlying generator. Due to space, we show CIDER for COMMONGEN and human judgement for E2E as measures of fluency. Other metrics show similar trends and can be found in the Appendix.\n\nResults. As seen in Table 1, the self-corrector improves upon the generator in all three settings, using either inference strategy: always correcting (SELF-CORRECT), or only correcting incorrect solutions (SELF-CORRECT∗). The self-corrector’s performance on Multiarith is very high after correction (9899%), a 38 point improvement over the generator, with a similar gain in the Multitask arithmetic setting. On the challenging GSM dataset, the self-corrector achieves 21%, and 24% with only correcting incorrect solutions, up from 8.57% for the generator. Notably, this is higher than the larger 2.7B GPT-Neo (also larger than generator+corrector), or larger models tuned on the language version of GSM. The results show that self-corrective learning can improve task performance via training a corrector. Qualitatively, the self-corrector can correct values in a correctly structured solution, fix the order of operations within a multistep solution, adjust unit conversions, and make larger multipart revisions (see Figures 3,7,8). Notably, these are learned automatically.\n\n3.2 LEXICALLY CONSTRAINED GENERATION\n\nNext, we consider lexically constrained generation. Given a set of constraint words x, the task is to generate a sentence y that includes all the given constraints. Faithful constraint satisfaction is crucial for many downstream tasks, e.g., those that require converting information to text (McKeown, 1985).\n\nDatasets and Metrics. We experiment on COMMONGEN (Lin et al., 2020) and E2E (Novikova et al., 2017). COMMONGEN is a benchmark for generative commonsense reasoning where the task is to generate a coherent sentence given a set of words (e.g., dog, catch). E2E involves converting structured inputs into natural language. For both tasks, we report standard metrics including human/automatic measures of fluency (BLEU, CIDER, etc.) as well as constraint coverage. We collect human measures of fluency on Amazon Mechanical Turk; see the Appendix for details.\n\nSetup. We parameterize the base generator with GPT-2 Radford et al. (2019) (large-size for COMMONGEN and medium-size for E2E). We fine-tuned the generator for each task. As the value function for self-corrective learning we use coverage, i.e. the percentage of constraints that are present in the output. For inference, we use beam search with the generator, then do up to 3 corrections using beam search, stopping early if all constraints are met. See the Appendix for additional details.\n\nResults. Table 2 shows the evaluation results. The self-corrector substantially improves constraint coverage over its GPT-2 generator for both tasks, while maintaining or improving its language quality. On the COMMONGEN benchmark, the self-corrector paired with the NeuroLogic constrained decoding algorithm (Lu et al., 2021) achieves the best results, outperforming the more sophisticated NeuroLogic-A* decoding algorithm, while being an order of magnitude faster. Notably, on E2E, self-correction outperforms Neurologic-A* decoding, despite only using standard beam search. This suggests that a corrector can be viewed as an alternative to using a more sophisticated decoding procedure (A*) for improving performance without modifying the underlying model. See Figure 9.\n\n3.3 TOXICITY REDUCTION\n\nNext, we consider the task of toxicity reduction (Gehman et al., 2020; Liu et al., 2021). Given a prompt x, the task is to generate a fluent continuation y while avoiding offensive content. This task is important for ensuring safe language model deployment, yet challenging: due to misaligned pretraining objectives (i.e. modeling internet text vs. non-toxic text), language models are suscepti-\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nToxicity\n\nFluency\n\nDiversity\n\nAvg. Max. Prob. Perplexity dist-2 dist-3\n\nGPT-2\n\nPPLM [7] GeDi [17] DExpert [27] DAPT [15] PPO [29] Quark [29]\n\nSELF-CORRECT\n\n0.527\n\n0.520 0.363 0.314 0.428 0.218 0.196\n\n0.171\n\n0.520\n\n0.518 0.217 0.128 0.360 0.044 0.035\n\n0.026\n\n11.31\n\n32.58 43.44 25.21 31.22 14.27 12.47\n\n11.81\n\n0.85\n\n0.86 0.84 0.84 0.84 0.79 0.80\n\n0.80\n\n0.85\n\n0.86 0.83 0.84 0.84 0.82 0.84\n\n0.83\n\nTable 3: Toxicity reduction. GPT-2 is the base generator.\n\nFigure 4: Applying multiple corrections reduces toxicity.\n\nble to generating toxic completions, even when prompted with seemingly innocuous text (Gehman et al., 2020). Along with its practical importance, the task tests whether (self-)correctors can be an effective mechanism for controlling the outputs of language models in an open-ended setting.\n\nDatasets and Metrics. We use the REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) which contains 100k prompts designed to elicit toxic generations. Following the experimental setup of Liu et al. (2021), during training we use 85K prompts from the training set, and for evaluation we use the same 10K non-toxic prompts from test set as Liu et al. (2021). We use Perspective API to measure maximum toxicity, defined as the average maximum toxicity over 25 sampled generations, and the (empirical) toxicity probability of at least 1 out of 25 generations being toxic.\n\nBaselines. We compare SELF-CORRECT with its generator (GPT-2) and previously reported baselines from Lu et al. (2022a), including PPLM (Dathathri et al., 2020), GeDi (Krause et al., 2021), DExpert (Liu et al., 2020), DAPT (Gururangan et al., 2020), PPO (Lu et al., 2022a), and Quark (Lu et al., 2022a). The latter two – Proximal Policy Optimization (PPO) and Quantized Reward Konditioning (Quark) – represent strong, state-of-the art approaches based on reinforcement learning.\n\nSetup. We use the off-the-shelf GPT-2 Large as the generator, and finetune another GPT-2 Large as the corrector. During inference, we use nucleus sampling with p = 0.9 to generate 25 samples for all baselines. As the value function, we use the Perspective API score, v(y) ∈ [0, 1], which measures the toxicity of the completed sequence. We do up to three corrections with the corrector model.\n\nResults. Table 3 shows that SELF-CORRECT reduces the rate of toxic generations substantially, while also maintaining fluency and diversity. SELF-CORRECT outperforms all baselines. This includes inference-time algorithms (PPLM, GeDi, DExpert), which do not modify the generator but degrade fluency and yield higher toxicity compared to SELF-CORRECT, as well as reinforcement learning methods (PPO, Quark) that adjust the generator using toxicity as a (negative) reward. The strong baselines use equal or more parameters: PPO and Quark use 3 and 2 model copies. The results show that SELF-CORRECT is effective for detoxification, without modifying the generator.\n\n3.4 CHANGING MODULES – CORRECTING GPT-3\n\nNext, we show that a self-corrector can improve the outputs of a generator that is much larger than the corrector. We consider two cases: (1) training with a small generator, then swapping in the larger generator at test time; (2) training with the larger generator, i.e. using the large generator to initialize the datapool for self-corrective learning, then using the large generator at test time.\n\nToxicity. We evaluate case (1) for reducing the toxicity of a large generator (GPT-2 XL, GPT-3). We generate an initial sequence using the large generator, then refine it with our corrector trained in the previous experiments (§3.3). Table 4 shows that the resulting self-corrector (large generator + corrector) has substantially reduced toxicity compared to the large generator. This shows the promise of using (self-)correctors for controlling the outputs of large language models.\n\nMath program synthesis. Table 4 shows results for math. Analogous to toxicity, the corrector is able to correct larger generators swapped in at test-time. For instance, the GPT-3 Instruct generator has quite high performance (84.90 Multitask, 36.80 GSM), which improves to 90.90 and 45.00,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTask\n\nDataset\n\nGenerator (train) Generator (test) Generator\n\nSelf-corrector\n\nMath Synthesis ↑\n\nGSM\n\nNeo 1.3B Neo 1.3B GPT-3 Instruct\n\nDetoxification ↓\n\nGPT2-L RTPrompts GPT2-L GPT2-L\n\nGPT-3 GPT-3 Instruct GPT-3 Instruct\n\nGPT2-XL GPT-3 GPT-3 Instruct\n\n6.96 36.80 36.80\n\n0.383 0.182 0.275\n\n24.30 45.00 45.92\n\n0.027 0.025 0.023\n\nTable 4: Modularity (program synthesis and detoxification). Self-correctors can correct very large generators, either by swapping in the generator at test-time, or training with the generator. For math synthesis, the corrector is GPT-Neo 1.3B, and here we only correct incorrect outputs. For detoxification, the correction is GPT2-L, and we correct all the outputs.\n\nToxicity ↓\n\nConstrained Gen. ↑\n\nMath ↑\n\nAvg. Max.\n\nProb.\n\nFluency\n\nFluency Constraints Correct Correct∗\n\nGenerator SELF-CORRECT + FEEDBACK\n\n0.527 0.171 0.156\n\n0.520 0.026 0.020\n\n11.31 11.81 11.86\n\n14.97 15.30 15.24\n\n91.38 94.58 95.88\n\n49.02 74.31 81.76\n\n49.02 79.80 82.35\n\nTable 5: Explicit natural language feedback. Correct∗ means only correcting incorrect outputs.\n\nrespectively, by adding in a corrector. The self-corrector (large generator + corrector) improves further by training with the GPT-3 Instruct generator, to 92.75 and 45.92, respectively.\n\n3.5 LEVERAGING EXPLICIT FEEDBACK\n\nNext, we demonstrate SELF-CORRECT’s capacity to incorporate explicit natural language feedback. This amounts to defining a feedback function f , then using the same self-corrective learning and inference algorithms (§2.1) as in our preceding experiments (in those experiments, f returned ∅). We show that correctors learn to use the feedback, as evidenced by higher performance.\n\nToxicity. We use additional fine-grained information from the toxicity API as natural language feedback. Specifically, besides the overall toxicity score, Perspective API also provides scores for fine-grained attributes of toxicity (e.g. identity attack, profanity, flirtation, etc.). At training time, we compare the attribute scores from a hypothesis and its selected correction, and use the attribute with the largest decrease as natural language feedback (e.g. \"decrease toxicity in profanity\"). At inference time, we call the API on the current hypothesis and use the attribute with the highest score.\n\nLexical constraints. In training time, we generate natural language feedback for every example pair (x, y, y′) by elaborating the extra lexical constraints satisfied by y′ but not y. e.g. “adding constraint word: read”. At inference time, we elaborate all missing constraints in the current hypothesis.\n\nMath program synthesis. Math program synthesis contains a variety of problem types and errors, without an automated means for identifying the errors (e.g. an API). We explore obtaining natural language feedback about the current program by prompting a large language model. We prompt the model with a problem, hypothesis program, a gold solution, and few-shot demonstrations that show feedback on one part of the program; e.g. In the initial guess, 3 should be subtracted. When the program is correct, the feedback is Correct. At inference time, we also use feedback from the language model. We allow the feedback model access to a gold solution, which we expect makes the feedback higher quality, with the risk of solution leakage at inference-time. Our results in this task are thus used only to study the feasibility of explicit feedback for math program synthesis.\n\nSetup. For toxicity, lexical constraints, and math we use REALTOXICITYPROMPTS, COMMONGEN, and the MULTITASK arithmetic setting, respectively. We follow the setup of each task’s previous experiments (§3.3,§3.2,§3.1), except for math we use 5 correction iterations (previously 1). For math, we use GPT-3 (text-davinci-002) with 6 demonstrations as the feedback model.\n\nResults. Table 5 shows that explicit natural language feedback improves performance in all three tasks. For toxicity, this means that providing fine-grained attributes (e.g. identity attack, profanity,\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nAblation\n\nMath COMMONGEN\n\nSELF-CORRECT ✗ proportional sampling ✗ value pairing\n\n78.24 77.25 62.35\n\n94.55 93.49 91.76\n\nTable 6: Effect of pairing and proportional sampling.\n\nExploration Multiarith Multitask GSM8k\n\n✗ ✓\n\n89.20 99.17\n\n73.49 78.24\n\n17.60 23.96\n\nTable 7: Effect of exploration on program synthesis.\n\nFigure 5: Math: multiple corrections.\n\netc.) during learning and inference improves upon using only the scalar toxicity score. Intuitively, feedback may help the model to focus on a useful correction; e.g., see Figure 6.\n\n3.6 ADDITIONAL ABLATIONS AND ANALYSIS\n\nEffect of multiple corrections. Previously, Figure 4 showed that multiple corrections led to better toxicity reduction. On math (Multitask setting), Figure 5 shows that performance improves with more than one correction, and that multiple corrections are more beneficial with feedback. Intuitively, in this math task, after 2-3 corrections the model needs additional guidance.\n\nEffect of pairing and proportional sampling. Self-corrective learning (i) samples pairs for learning proportional to Equation 4, (ii) only pairs sequences that improve value. We ablate these features by training on Multitask using a data pool that samples a pair for learning uniformly (rather than Equation 4), and a data pool without value pairing. Table 6 shows that both improve performance.\n\nEffect of exploration. To ablate the effect of exploration, we train a baseline only on correction pairs induced from the base generator. Table 7 shows results on the three math datasets, indicating that exploration improves performance.\n\n4 RELATED WORK\n\nSelf-Correction relates to work modeling text edits including supervised Wikipedia edits (Reid & Neubig, 2022; Faltings et al., 2021; Schick et al., 2022), unsupervised perturbations (Miao et al., 2019; Liu et al., 2020), training on human-written critiques (Saunders et al., 2022), or refining continuous variables (Lee et al., 2020; Li et al., 2022; Qin et al., 2022). In contrast, Self-Correction learns a text corrector online to improve a quality measure without supervised edits or critiques. Recently, Scheurer et al. (2022) use natural language feedback to improve generations. Denoising sequences is a common pretraining objective (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020), while self-correction ‘denoises’ generations to improve a scalar quality measure. Reinforcement learning (RL) is often used to improve scalar measures in a generator (Ziegler et al., 2019; Stiennon et al., 2020; Lu et al., 2022a), yet is infeasible for many models (e.g. those accessed by API), and uses only scalar feedback. Moreover, RL-tuned generators can be used within Self-Correction. Self-Correction decomposes generation into multiple steps, similar to methods that generate rationales (Wei et al., 2022; Dohan et al., 2022), but Self-Correction produces intermediate steps of the same form as the output, allowing iterative application. Self-Correction relates to work on program synthesis (Fu et al., 2019; Balog et al., 2020; Gupta et al., 2020; Le et al., 2022) and repair (Gupta et al., 2020; Yasunaga & Liang, 2020). Yasunaga & Liang (2021) is closest in methodology, but Self-Correction uses a domain-agnostic formulation; see the Appendix for discussion.\n\n5 CONCLUSION\n\nWe introduced self-correctors, a class of models that decompose generation into initial generation and correction steps. We study self-correctors with a fixed base generator along with a corrector trained to improve outputs according to a scalar measure of quality. We presented a simple, general procedure for training the corrector, and find that self-correction is applicable and effective for improving performance, and controlling the outputs of both small and large generators. Moreover, we found that self-correction along with our learning framework provides a promising mechanism for using natural language feedback to improve generation, opening many avenues for future work.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv, abs/2108.07732, 2021.\n\nMatej Balog, Rishabh Singh, Petros Maniatis, and Charles Sutton. Neural program synthesis with a\n\ndifferentiable program fixer, 2020. URL https://arxiv.org/abs/2006.10924.\n\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/ 10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168.\n\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. ArXiv, abs/1912.02164, 2020.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423.\n\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Narain Sohl-Dickstein, Kevin Murphy, and Charles Sutton. Language model cascades. ArXiv, abs/2207.10342, 2022.\n\nFelix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, and Bill Dolan. Text editing by command. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5259–5274, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.naacl-main.414. URL https://aclanthology.org/2021.naacl-main.414.\n\nCheng Fu, Huili Chen, Haolan Liu, Xinyun Chen, Yuandong Tian, Farinaz Koushanfar, and Jishen In Advances in Neural Information\n\nZhao. Coda: An end-to-end neural program decompiler. Processing Systems, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2021. URL https://arxiv.org/ abs/2101.00027.\n\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 3356–3369, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.\n\nKavi Gupta, Peter Ebert Christensen, Xinyun Chen, and Dawn Song. Synthesize, Execute and Debug: Learning to Repair for Neural Program Synthesis. In H Larochelle, M Ranzato, R Hadsell, M F Balcan, and H Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 17685–17695. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/cd0f74b5955dc87fd0605745c4b49ee8-Paper.pdf.\n\nSuchin Gururangan, Ana Marasovi ́c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8342–8360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.acl-main.740. URL https://aclanthology.org/2020.acl-main.740.\n\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 523–533, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/D14-1058.\n\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4929–4952, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.424. URL https://aclanthology.org/2021. findings-emnlp.424.\n\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. arXiv preprint arXiv:2207.01780, 2022.\n\nJason Lee, Raphael Shu, and Kyunghyun Cho.\n\nIterative refinement in the continuous space for non-autoregressive neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1006–1015, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.73. URL https://aclanthology.org/2020.emnlp-main.73.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence preIn Proceedings of training for natural language generation, translation, and comprehension. the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.\n\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-\n\nlm improves controllable text generation. ArXiv, abs/2205.14217, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJared Lichtarge, Christopher Alberti, Shankar Kumar, Noam M. Shazeer, Niki Parmar, and Simon\n\nTong. Corpora generation for grammatical error correction. ArXiv, abs/1904.05780, 2019.\n\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative comIn Findings of the Association for Computational Linguistics: EMNLP monsense reasoning. 2020, pp. 1823–1840, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.165. URL https://aclanthology.org/2020. findings-emnlp.165.\n\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. DExperts: Decoding-time controlled text generation with experts and antiexperts. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6691–6706, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.522. URL https://aclanthology.org/2021. acl-long.522.\n\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3154–3169, 2022.\n\nXianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, and Sen Song. Unsupervised paraphrasing by simulated annealing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 302–312, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.28. URL https://aclanthology.org/2020. acl-main.28.\n\nXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4288–4299, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.339. URL https://aclanthology.org/2021.naacl-main.339.\n\nXiming Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. CoRR, abs/2205.13636, 2022a. doi: 10.48550/arXiv.2205.13636. URL https://doi.org/ 10.48550/arXiv.2205.13636.\n\nXiming Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. NeuroLogic In Proceedings of a*esque decoding: Constrained text generation with lookahead heuristics. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 780–799, Seattle, United States, July 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.57. URL https: //aclanthology.org/2022.naacl-main.57.\n\nKathleen McKeown. Text Generation. Studies in Natural Language Processing. Cambridge Univer-\n\nsity Press, 1985. doi: 10.1017/CBO9780511620751.\n\nNing Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. Cgmh: Constrained sentence generation by metropolis-hastings sampling. Proceedings of the AAAI Conference on Artificial Intelligence, 33 (01):6834–6842, Jul. 2019. doi: 10.1609/aaai.v33i01.33016834. URL https://ojs.aaai. org/index.php/AAAI/article/view/4659.\n\nSwaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A unified benchmark for mathematical reasoning. ArXiv, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAnsong Ni, Jeevana Priya Inala, Chenglong Wang, Oleksandr Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. Learning from self-sampled correct and partially-correct programs, 2022. URL https://arxiv.org/abs/2205.14318.\n\nJekaterina Novikova, Ondˇrej Dušek, and Verena Rieser. The E2E dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pp. 201–206, Saarbrücken, Germany, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-5525. URL https://aclanthology.org/W17-5525.\n\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080– 2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.\n\nLianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based con-\n\nstrained text generation with langevin dynamics. arXiv preprint arXiv:2202.11705, 2022.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.\n\nMachel Reid and Graham Neubig. Learning to model editing processes, 2022. URL https:\n\n//openreview.net/forum?id=1bEaEzGwfhP.\n\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv. org/abs/1908.10084.\n\nSubhro Roy, Tim Vieira, and Dan Roth. Reasoning about quantities in natural language. Transac-\n\ntions of the Association for Computational Linguistics, 3:1–13, 2015.\n\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators, 2022. URL https://arxiv. org/abs/2206.05802.\n\nJérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback, 2022. URL https://arxiv.org/ abs/2204.14146.\n\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Peer: A collaborative language model, 2022. URL https://arxiv.org/abs/2208.11663.\n\nVered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4615–4629, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.373. URL https://aclanthology.org/2020.emnlp-main.373.\n\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Adback. vances in Neural Information Processing Systems, volume 33, pp. 3008–3021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper.pdf.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.\n\nMichihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from diagnostic ISBN\n\nIn 37th International Conference on Machine Learning, ICML 2020, 2020.\n\nfeedback. 9781713821120.\n\nMichihiro Yasunaga and Percy Liang. Break-it-fix-it: Unsupervised learning for program repair. In\n\nInternational Conference on Machine Learning (ICML), 2021.\n\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang. LM-critic: Language models for unsupervised grammatical error correction. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7752–7763, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 611. URL https://aclanthology.org/2021.emnlp-main.611.\n\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA RELATED WORK\n\nSelf-correction provides a flexible framework for improving the performance of off-the-shelf and fine-tuned language models on a wide range of tasks by decomposing generation into a base generator and a corrector. Our framework’s minimal assumptions on the form of the corrector, value function, and data used to train the corrector, as well as its wide applicability differ from prior work.\n\nLearning to fix code. Our work relates to two streams of research in the code domain. One stream deals with program synthesis, in which a corrector model corrects code from a base synthesizer until it meets a given specification (Fu et al., 2019; Balog et al., 2020; Gupta et al., 2020; Le et al., 2022), while another stream deals with program repair: correcting code that is provided as input (Gupta et al., 2020; Yasunaga & Liang, 2020; 2021). Recently, Le et al. (2022) developed a modular program synthesis approach that involves a correction module trained on ground-truth outputs. In contrast, self-corrective learning supports cases without ground-truth outputs, e.g. toxicity.\n\nClosest to our methodology is Yasunaga & Liang (2021). Unlike Yasunaga & Liang (2021), selfcorrection does not assume a mechanism for generating synthetic negatives, a dataset of negatives, or a separate model that generates negatives. This is important because engineering these components for each new task can be prohibitive. Second, Yasunaga & Liang (2021) assume a 0/1 value function, while self-correction supports general scalar value functions. This is important for tasks such as toxicity that do not have a strict notion of correctness. Finally, we propose new pairing and proportional sampling mechanisms found to be important (Table 6).\n\nIterative text edits. Self-correction relates to recent works on editing text, including modeling Wikipedia edits (Reid & Neubig, 2022; Faltings et al., 2021; Schick et al., 2022), which relies on supervised edits, unsupervised methods (Miao et al., 2019; Liu et al., 2020) that perturb sequences with simple operations (e.g. insertion, deletion), editing with models trained on human-written critiques (Saunders et al., 2022), or iteratively updating continuous variables (Lee et al., 2020; Li et al., 2022; Qin et al., 2022). In contrast to these, self-correction learns an expressive text-to-text corrector that is trained online to improve a quality measure, without requiring a supervised dataset of edits or critiques. Recently, Scheurer et al. (2022) incorporate human feedback by fine-tuning on refinements that are similar to the feedback, rather than through an iterative corrector module. Finally, correcting text is inherent to the task of grammatical error correction (e.g. Lichtarge et al. (2019); Yasunaga et al. (2021); our work differs in that we correct a module within a generation system, and provide a framework for addressing a variety of tasks.\n\nDenoising and reinforcement learning. Separately, denoising ground-truth sequences is a common pretraining objective (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020), while selfcorrection ‘denoises’ generations to improve a scalar quality measure. Scalar measures are often improved with reinforcement learning (RL) on a base generator (Ziegler et al., 2019; Stiennon et al., 2020; Lu et al., 2022a), which is infeasible for improving many language models (e.g. those accessed through an API), and uses only scalar feedback. Moreover, self-correction learns a delta between a generation and solution, and is complementary to RL-tuned generators, which can be used within a self-corrector. Finally, RL can be used as an alternative learning algorithm for training a corrector, which is an interesting direction for future work.\n\nModular generation. Self-correction decomposes generation into multiple steps, and is thus part of the general class of methods that decompose generation into a ‘cascade’ of modules (Dohan et al., 2022). Examples include using separate knowledge generation modules (Shwartz et al., 2020; Liu et al., 2022), or generating rationales before a response (Wei et al., 2022). Self-correction also produces a chain of intermediate steps, but each step is of the same form as the output, allowing for re-using previous generations.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nB ADDITIONAL EXPERIMENTAL DETAILS\n\nB.1 CROSS-EXPERIMENT DETAILS\n\nIn all of our experiments we use an off-the-shelf embedding similarity function from SentenceTransformers (Reimers & Gurevych, 2019): sentence-transformers/all-MiniLM-L6-v2.\n\nB.2 MATHEMATICAL PROGRAM SYNTHESIS\n\nWe fine-tune a separate instance of GPT-Neo 1.3B as an initial generator, using the Huggingface library with default hyperparameters, except for evaluation steps, which we set to a small number to ensure a strong checkpoint is selected for each dataset. We use the finetuned initial generator as initialization for the corrector, and tune the corrector on sequences [SC]x[CURR]yi[START]yj[END], where x is a problem, yi and yj form a residual pair, and [·] are special tokens. The loss is on tokens after [START].\n\nFeedback. We write 6 demonstrations using training problems and generations from our GPTNeo base generator, and use GPT-3 (text-davinci-002) as a feedback model. We use the same training procedure and hyperparameters, except that the sequences now include feedback, [SC]x[CURR]yi[FEEDBACK]F(x,yi)[START]yj[END], where x is a problem, yi and yj form a residual pair, and F (x, yi) is feedback. We include loss on tokens after [FEEDBACK].\n\nB.3 LEXICALLY-CONSTRAINED GENERATION\n\nHyper-parameters. Table 8 and Table 9 show hyperparameters for CommonGen and E2E.\n\nHuman Evaluation. We evaluate fluency of generations in E2E task using human annotators on Amazon Mechanical Turk (AMT). We randomly sampled 100 instances, along with generations of different baselines and self-corrections. For each instance, we ask 3 annotators to evaluate the fluency of generations on a 3-point Likert scale. We aggregate annotations from 3 annotators using majority vote. We restricted the pool of annotators to those who are located in US or CA, and had 98% approval rate for at least 5,000 previous annotations.\n\nHyperparameter\n\nAssignment\n\nHyperparameter\n\nAssignment\n\nPredictor steps batch size optimizer learning rate decoding alg.\n\nGPT-2Large 6000 128 Adam 1.e−5 beam search (k=5)\n\nPredictor steps batch size optimizer learning rate decoding alg.\n\nGPT-2M edium 10000 100 Adam 1.e−5 beam search (k=5)\n\nTable 8: Hyperparameters for COMMONGEN.\n\nTable 9: Hyperparameters for E2E.\n\nC ADDITIONAL RESULTS\n\nToxicity\n\nFluency\n\nDiversity\n\nAvg. Max.\n\nProb.\n\nPerplexity\n\ndist-2\n\ndist-3\n\nGPT2-L SELF-CORRECT SELF-CORRECT + FEEDBACK\n\n0.527 0.171 0.156\n\n0.520 0.026 0.020\n\n11.31 11.81 11.86\n\n0.85 0.80 0.80\n\n0.85 0.83 0.83\n\nTable 10: Evaluation results of toxicity reduction experiments with natural language feedback.\n\nD QUALITATIVE EXAMPLES\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTask\n\nDataset\n\nGenerator (train) Generator (test) Generator\n\nSelf-corrector\n\nMath Synthesis ↑\n\nMultitask\n\nGSM\n\nNeo 1.3B Neo 1.3B GPT-3 Instruct\n\nNeo 1.3B Neo 1.3B GPT-3 Instruct\n\nDetoxification ↓\n\nGPT2-L RTPrompts GPT2-L GPT2-L\n\nGPT-3 GPT-3 Instruct GPT-3 Instruct\n\nGPT-3 GPT-3 Instruct GPT-3 Instruct\n\nGPT2-XL GPT-3 GPT-3 Instruct\n\n46.70 84.90 84.90\n\n6.96 36.80 36.80\n\n0.383 0.182 0.275\n\n80.00 90.90 92.75\n\n24.30 45.00 45.92\n\n0.027 0.025 0.023\n\nTable 11: Modularity (program synthesis and detoxification). Self-correctors can correct very large generators, either by swapping in the generator at test-time, or training with the generator. For math synthesis, the corrector is GPT-Neo 1.3B, and here we only correct incorrect outputs. For detoxification, the correction is GPT2-L, and we correct all the outputs.\n\nBleu-4 CIDER Coverage\n\nRuntime\n\nNeuroLogic [28] NeuroLogic-A*esque [30]\n\nGPT-2 SELF-CORRECT SELF-CORRECT + feedback SELF-CORRECT+NeuroLogic\n\n26.70 28.20\n\n27.90 27.98 27.82 28.17\n\n14.70 15.20\n\n14.97 15.30 15.24 15.28\n\n97.70 97.80\n\n91.38 94.58 95.88 97.80\n\n2.04s/sent 19.24s/sent\n\n0.2s/sent 0.8s/sent 0.8s/sent 2.24s/sent\n\nTable 12: Evaluation rresults of lexically-constrained generation on COMMONGEN.\n\nCoverage BLEU-4 NIST R-L METEOR CIDER\n\nPREFIX-TUNING (Li & Liang, 2021)\n\n91.16\n\n70.30\n\n8.82 72.10\n\nGPT-2 SELF-CORRECT\n\n91.50 98.77\n\n67.12 68.81\n\n8.67 70.25 8.78 68.60\n\n46.30\n\n45.58 45.11\n\n2.46\n\n2.33 2.38\n\nTable 13: Evaluation results of lexically-constrained generation on E2E.\n\nProblem: Melanie had 19 dimes in her bank. Her dad gave her 39 dimes and her mother gave her 25 dimes. How many dimes does Melanie have now?\n\nProblem: Lana’s favorite band was holding a concert where tickets were 6 dollars each. Lana bought 8 tickets for herself and her friends and 2 extra tickets in case anyone else wanted to go. How much did she spend?\n\nGenerator (GPT-Neo):\n\nanswer = 19 + 25 print(answer)\n\nFeedback (GPT-3):\n\n# In the initial guess, # 39 is not included.\n\nCorrector (GPT-Neo):\n\nanswer = 19 + 25 + 39 print(answer)\n\nGenerator (GPT-Neo):\n\nanswer=(6.0*8.0) print(answer)\n\nFeedback (GPT-3):\n\n# In the initial guess, # 2 tickets are not included.\n\nCorrector (GPT-Neo):\n\nanswer=(6.0*(8.0+2.0)) print(answer)\n\nFigure 6: Self-correction with natural language feedback.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nProblem: Ralph watches TV for 4 hours a day from Monday to Friday, and 6 hours a day on Saturday and Sunday. How many hours does Ralph spend watching TV in one week?\n\nGenerator:\n\nCorrector:\n\na=4*7 b=6*7 c=a+b answer=c print(answer)\n\na=4*5 b=6*2 c=a+b answer=c print(answer)\n\nProblem: The pirates plan to explore 4 islands. Two islands require walking 20 miles per day while the other two islands require 25 miles per day. How many miles will they have to walk if it takes 1.5 days to explore each island?\n\nGenerator:\n\nCorrector:\n\na=20*2 b=25*2 c=a+b d=c*1.5 e=d+b answer=e print(answer)\n\na=20*2 b=25*2 c=a+b d=c*1.5 answer=d print(answer)\n\nFigure 7: Grade school math (GSM) self-corrections. Left: the structure of the generator’s solution is valid, but it incorrectly uses the total number of days in a week for both a and b; the corrector fixes the program to correctly account for the 5 weekdays and 2 weekend days. Right: the generator’s solution contains an incorrect addition at the end; the corrector removes this line, resulting in a correct program.\n\nProblem: A spiral notebook costs 15, and a personal planner costs $10. How much would it cost in total to buy 4 spiral notebooks and 8 personal planners at a 20% discount?\n\nProblem: Julia has $40. She spends half of her money to buy a new game for her phone. She spends a quarter of what she has left on in-game purchases. How much money does she have left?\n\nGenerator:\n\nCorrector:\n\nGenerator:\n\nCorrector:\n\na=4*15 b=8*10 c=a+b answer=c print(answer)\n\na=4*15 b=8*10 c=a+b d=c*20/100 e=c-d answer=e print(answer)\n\na=40/2 b=40-a c=b/4 d=b-c e=d+40 answer=e print(answer)\n\na=40/2 b=a/4 c=a-b answer= c print(answer)\n\nFigure 8: Grade school math (GSM) self-corrections. Left: the generator’s program doesn’t include the discount; the corrector appends the discount to the program. Right: a more sophisticated multipart correction. The generator’s assignment of b (line 2), and addition to e (line 5) are incorrect. The corrector removes these lines and adjusts the variable names accordingly.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nConstraints: ‘table’, ‘paper’, ‘read’ Generator: A man is reading book on a table. Corrector: A man is reading a book on a table and writing on a piece of paper.\n\nConstraints: ’stand’, ’wait’, ’look’ Generator: He waits for someone to look at him. Corrector: He waits for someone to stand and look him in the eye.\n\nConstraints: ’walk’, ’park’, ’bench’, ’dog’, ’sit’ Generator: A dog sitting on a bench in a park. Corrector: A dog sitting on a bench and a man walking a dog in a park.\n\nConstraints: name : The Mill | Type : restaurant | food : English | price : high | customer rating : average | area : riverside | family friendly : yes | near : Cafe Rouge Generator: The Mill is an expensive, three star restaurant located near Cafe Rouge. Corrector: The Mill is a children friendly English restaurant in the riverside area near Cafe Rouge.\n\nIt has an average customer rating and a high price range.\n\nConstraints: name : Blue Spice | Type : restaurant | food : Chinese | area : riverside | family friendly : yes | near : Rainbow Vegetarian Cafe Generator: Blue Spice is a restaurant providing Chinese food. It is near Rainbow Vegetarian Cafe. Corrector: Blue Spice is a family friendly Chinese restaurant in the riverside area near Rainbow Vegetarian Cafe.\n\nIt is located in the riverside.\n\nFigure 9: Constrained generation self-corrections. Top: COMMONGEN, and Bottom: E2E. Words in green indicate constraints that were previously missing.\n\nPrompt: Waters then went on to call the Trump Generator: administration a \"bunch of idiots\" and \"insolent clowns [who are] running\" Corrector: Administration’s support for the Trans-Pacific Partnership on trade \"a total non-event.\"\n\nFigure 10: Toxicity control self-corrections. Words in red indicate toxic language.\n\n19",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a self-correction method which trains a corrector to iteratively correct imperfect generation results. The authors first train a generator on the downstream datasets (or directly prompt a large language model), and use it to construct a data pool. Then, the authors select value-improving pairs based on a task-specific value function to build the training set of the corrector. Finally, the corrector is trained based on these samples and generates samples to augment the original data pool. Experimental results show the effectiveness of self-correction in three generation tasks.\n\n# Strength And Weaknesses\n\nStrengths:\n\n1) This paper is well organized and easy to follow.\n2) The proposed method can be applied to a wide range of text generation tasks. The experimental results show the superior performance of self-correction over some competitive baselines.\n\nWeaknesses:\n\n1) The name of the method “self-correction” is confusing for me, because the authors train a separate corrector to improve the base generator. The generator / corrector cannot consistently correct itself in this paper. They should cooperate with each other to achieve better generation performance.\n\n2) From the perspective of correctors, the proposed method seems to train a text editing model (corrector) via pseudo-labeled data generated by a pre-trained model (generator). Specifically, the fixed generator is used to construct the training dataset of the corrector via generating data and selecting value-improving pairs. Then, the corrector is trained on these \"pseudo-labeled\" data and augment the original data pool iteratively. Thus, I feel that the novelty of this method is somewhat limited because using pre-trained models to automatically generate training data is common in recent works. I don’t find any specific design when training the corrector.\n\n3) The feedback has been mentioned for many times in this paper. But this part is individual compared with the whole design. I don’t find any specific module to properly incorporate the feedback signal into the corrector.\n\n4) The experimental setting may be unfair because the corrector has a relatively large amount of model parameters. Thus, the total number of parameters in self-correction (including the generator and the corrector) is significantly larger than that of other baselines.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe authors should further clarify the method design and the experimental settings. The overall quality of this paper is OK. But in my view, the novelty of the proposed method is somewhat limited from the perspective of correctors. The reproducibility of this paper is degraded due to the lack of codes.\n\n# Summary Of The Review\n\nThe proposed method can adapt to various text generation tasks and achieve good performance. However, as mentioned in my review, the authors should solve the concerns about the novelty, the method design, and the experimental settings to make this paper ready for publication.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nA STATISTICAL FRAMEWORK FOR PERSONALIZED FEDERATED LEARNING AND ESTIMATION: THEORY, ALGORITHMS, AND PRIVACY\n\nKaan Ozkara∗, Antonious M. Girgis∗, Deepesh Data & Suhas Diggavi Department of Electrical and Computer Engineering University of California, Los Angeles {kaan,amgirgis}@ucla.edu,deepesh.data@gmail.com,suhas@ee.ucla.edu\n\nABSTRACT\n\nA distinguishing characteristic of federated learning is that the (local) client data could have statistical heterogeneity. This heterogeneity has motivated the design of personalized learning, where individual (personalized) models are trained, through collaboration. There have been various personalization methods proposed in literature, with seemingly very different forms and methods ranging from use of a single global model for local regularization and model interpolation, to use of multiple global models for personalized clustering, etc. In this work, we begin with a statistical framework that unifies several different algorithms as well as suggest new algorithms. We apply our framework to personalized estimation, and connect it to the classical empirical Bayes’ methodology. We develop novel private personalized estimation under this framework. We then use our statistical framework to propose new personalized learning algorithms, including AdaPeD based on information-geometry regularization, which numerically outperforms several known algorithms. We develop privacy for personalized learning methods with guarantees for user-level privacy and composition. We numerically evaluate the performance as well as the privacy for both the estimation and learning problems, demonstrating the advantages of our proposed methods.\n\n1\n\nINTRODUCTION\n\nThe federated learning (FL) paradigm has had huge recent success both in industry and academia (McMahan et al., 2017; Kairouz et al., 2021), as it enables to leverage data available in dispersed devices for learning while maintaining data privacy. Yet, it was recently realized that for some applications, due to the statistical heterogeneity of local data, a single global learning model may perform poorly for individual clients. This motivated the need for personalized learning achieved through collaboration, and there have been a plethora of personalized models proposed in the literature as well (Fallah et al., 2020; Dinh et al., 2020; Deng et al., 2020; Mansour et al., 2020; Acar et al., 2021; Li et al., 2021; Ozkara et al., 2021; Zhang et al., 2021; Hu et al., 2020). However, the proposed approaches appear to use very different forms and methods, and there is a lack of understanding of an underlying fundamental statistical framework. Such a statistical framework could help develop theoretical bounds for performance, suggest new algorithms as well as perhaps give grounding to known methods. Our work addresses this gap.\n\nIn particular, we consider the fundamental question of how one can use collaboration to help personalized learning and estimation for users who have limited data that they want to keep private. Our proposed framework is founded on the requirement not only of personalization but also privacy, as maintaining local data privacy is what makes the federated learning framework attractive - and thus any algorithm that aims to be impactful needs to also give formal privacy guarantees. The goal of this paper is to develop a statistical framework that leads to new algorithms with provable privacy guarantees, and performance bounds. Our main contributions are (i) Development of a statistical framework for federated personalized estimation and learning (ii) Theoretical bounds and\n\n∗Equal Contribution.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nnovel algorithms for private personalized estimation (iii) Design and privacy analysis of new private personalized learning algorithms; as elaborated below. Omitted proofs/details are in appendices.\n\n• Statistical framework: We connect this problem to the classical empirical Bayes’ method, pioneered by Stein (1956); James & Stein (1961); Robbins (1956), which proposed a hierarchical statistical model Gelman et al. (2013). This is modeled by an unknown population distribution P from which local parameters {θi} are generated, which in turn generate the local data through the distribution Q(θi). Despite the large literature on this topic, especially in the context of statistical estimation, creating a framework for FL poses new challenges. In contrast to classical empirical Bayes’ estimation, we introduce a distributed setting and develop a framework that allows information (communication and privacy) constraints1. This framework enables us to develop statistical performance bounds as well as suggests (private) personalized federated estimation algorithms. Moreover, we develop our framework beyond estimation, for (supervised) distributed learning, where clients want to build local predictive models with limited local (labeled) samples; we develop this framework in Section 3, which leads to new (private) personalized learning algorithms.\n\n• Private personalized estimation: Our goal is to estimate individual (local) parameters, when each user has very limited (heterogeneous) data. Such a scenario motivates federated estimation of individual parameters, privately. More precisely, the users observe data generated by an unknown distribution parametrized by their individual (unknown) local parameters θi, and want to estimate their local parameters θi leveraging very limited local data; see Section 2 for more details. For the hierarchical statistical model, classical results have shown that one can enhance the estimate of individual parameters based on the observations of a population of samples, despite having independently generated parameters from an unknown population distributions. However, this has not been studied for the distributed case, with privacy and communication constraints, which we do (see Theorem 2 for the Gaussian case and Theorem 4 for the Bernoulli case, and also for mixture population models in Appendix D). We estimate the (parametrized) population distribution under these privacy and communication constraints and use this as an empirical prior for local estimation. The effective amplification of local samples through collaboration, in Section 2, gives us theoretical insight about when collaboration is most useful, under privacy and/or communication constraints. Our results suggest how to optimally balance estimates from local and population models. We also numerically evaluate these methods, including application to polling data (see Section 4 and Appendices) to show advantages of such collaborative estimation compared to local methods.\n\n• Private personalized learning: The goal here is to obtain individual learning models capable of predicting labels with limited local data in a supervised learning setting. This is the use case for federated learning with privacy guarantees. It is intimately related to the estimation problem with distinctions including (i) to design good label predictors rather than just estimate local parameters (ii) the focus on iterative methods for optimization, requiring strong compositional privacy guarantees. Therefore, the statistical formulation for learning has a similar flavor to that in estimation, where there is a population model for local (parametrized) statistics for labeled data; see Section 3 for more details. We develop several algorithms, including AdaPeD (in Section 3.2), AdaMix (in Section 3.1), and DP-AdaPeD (in Section 3.3), inspired by the statistical framework. AdaPeD uses information divergence constraints along with adaptive weighting of local models and population models. By operating in probability (rather than Euclidean) space, using information-geometry (divergence), enables AdaPeD to operate with different local model sizes and architectures, giving it greater flexibility than existing methods. We integrate it with user-level privacy to develop DP-AdaPeD, with strong compositional privacy guarantees (Theorem 5). AdaMix is inspired by mixture population distributions, which adaptively weighs multiple global models and combines it with local data for personalization. We numerically evaluate these algorithms for synthetic and real data in Section 4.\n\nRelated Work. Our work can be seen in the intersection of personalized learning, estimation, and privacy. Below we give a brief description of related work; a more detailed comparison which connects our framework to other personalized algorithms is given in Appendix J.\n\nPersonalized FL: Recent work adopted different approaches for learning personalized models, which can be explained by our statistical framework for suitable choices of population distributions as explained in Appendix J: These include, meta-learning based methods (Fallah et al., 2020; Acar et al., 2021; Khodak et al., 2019); regularization (Deng et al., 2020; Mansour et al., 2020; Hanzely\n\n1The homogeneous case for distributed estimation is well-studied; see (Zhang, 2016) and references.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n& Richt ́arik, 2020); clustered FL (Zhang et al., 2021; Mansour et al., 2020; Ghosh et al., 2020; Smith et al., 2017) (Marfoq et al., 2021); using knowledge distillation (Lin et al., 2020; Ozkara et al., 2021); multi-task Learning (Dinh et al., 2020; Hanzely & Richt ́arik, 2020; Smith et al., 2017; Vanhaesebrouck et al., 2017; Zantedeschi et al., 2020); and using common representations (Du et al., 2021; Raghu et al., 2020; Tian et al., 2020; Collins et al., 2021) and references therein. Our work enables not just a unified view of these methods, but suggests new algorithms developed in this paper, along with privacy guarantees.\n\nAfter the conclusion of our work (Ozkara et al., 2022, July), we found two concurrent and independent works (Kotelevskii et al., 2022, June; Chen et al., 2022) that use a hierarchical Bayes approach to construct personalized learning algorithms, and are closest to our statistical framework. (Kotelevskii et al., 2022, June) is based on using a MCMC method2 to estimate a population distribution; such methods could be computationally intensive (see the discussion in (Blei et al., 2003); (Chen et al., 2022) considers the unimodal Gaussian prior, and effectively does what the classical empirical Bayes suggests (see also Theorem 1). None of these works consider privacy, which we do both for estimation and learning algorithms (see Theorems 2, 4, Appendix D, and for DP-AdaPeD in Theorem 5). Note that MCMC methods could have detrimental privacy implications. Also, they do not include information-geometric techniques (like our AdaPeD) or methods inspired by mixture distributions (e.g., AdaMix).\n\nPrivacy for Personalized Learning. There has been a lot of work in privacy for FL when the goal is to learn a single global model (see (Girgis et al., 2021b) and references therein); though there are fewer papers that address user-level privacy (Liu et al., 2020; Levy et al., 2021; Ghazi et al., 2021). There has been more recent work on applying these ideas to learn personalized models (Girgis et al., 2022; Jain et al., 2021b; Geyer et al., 2017; Hu et al., 2020; Li et al., 2020). These are for specific algorithms/models, e.g., Jain et al. (2021b) focuses on the common representation model for linear regression described earlier or on item-level privacy (Hu et al., 2020; Li et al., 2020). We believe that DP-AdaPeD proposed in this paper is among the first user-level private personalized learning algorithms with compositional guarantees, applicable to general deep learning architectures.\n\n2 PERSONALIZED ESTIMATION\n\nWe consider a client-server architecture, where there are m clients. Let P(Γ) denote a global population distribution that is parameterized by an unknown Γ and let θ1, . . . , θm are sampled i.i.d. from P(Γ) and are unknown to the clients. Client i is given a dataset Xi := (Xi1, . . . , Xin), where Xij, j ∈ [n] are sampled i.i.d. from some distribution Q(θi), parameterized by θi ∈ Rd. Note that heterogeneity in clients’ datasets is induced through the variance in P(Γ), and if the variance of P(Γ) is zero, then all clients observe i.i.d. datasets sampled from the same underlying distribution.\n\nThe goal at client i for all i ∈ [m] is to estimate θi through the help of the server. We focus on one-round communication schemes, where client j applies a (potentially randomized) mechanism q on its dataset Xj and sends qj := q(Xj) to the server, who aggregates the received messages, which is denoted by Agg(q1, . . . , qm), and broadcasts that to all clients. Based on (Xi, Agg(q1, . . . , qm)), client i outputs an estimate (cid:98)θi of θi. We measure the performance of (cid:98)θi through the Bayesian risk for mean squared error (MSE), as defined below (where P is the true prior distribution with associated density π, θi ∼ P is the true local parameter, and (cid:98)θi = (cid:98)θ(Xi, Agg(q1, . . . , qm)) is the estimator): (cid:90)\n\nEθi∼PE\n\n(cid:98)θi,q,X1,...,Xm\n\n(cid:107)(cid:98)θi − θi(cid:107)2 =\n\nE\n\n(cid:98)θi,q,X1,...,Xm\n\n(cid:107)(cid:98)θi − θi(cid:107)2π(θi)dθi.\n\n(1)\n\nThe above statistical framework can model many different scenarios, and we will study in detail three settings: Gaussian and Bernoulli models (Sections 2.1, 2.2 below), and Mixture model (Appendix D).\n\n2.1 GAUSSIAN MODEL\n\nId) for all i ∈ [m], which In the Gaussian setting, P(Γ) = N (μ, σ2 Id) i.i.d. for i ∈ [m]. implies that θ1, . . . , θm ∼ N (μ, σ2 Here, σθ ≥ 0, σx > 0 are known, and μ, θ1, . . . , θm are unknown. For the case of a single local\n\nId) and Q(θi) = N (θi, σ2 Id) i.i.d. and Xi1, . . . , Xin ∼ N (θi, σ2\n\nx\n\nx\n\nθ\n\nθ\n\n2In our understanding their numerics seem to be restricted to a unimodal Gaussian population model.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nsample this is identical to the classical James-Stein estimator (James & Stein, 1961); Theorem 1 does a simple extension for multiple local samples and is actually a stepping stone for the information constrained estimation result of Theorem 2. Omitted proofs/details are provided in Appendix B.\n\nOur proposed estimator. Since there is no distribution on μ, and given μ, we know the distribution of θi’s, and subsequently, of Xij’s. So, we consider the maximum likelihood estimator:\n\n(cid:98)θ1, . . . , (cid:98)θm, (cid:98)μ := arg max\n\nθ1,...,θm,μ\n\np{θi,Xi}|μ (θ1, . . . , θm, X1, . . . , Xm|μ)\n\n(2)\n\nTheorem 1. Solving (2) yields the following closed form expressions for (cid:98)μ and (cid:98)θ1, . . . , (cid:98)θm:\n\n(cid:98)μ =\n\n1 m\n\nm (cid:88)\n\ni=1\n\nX i\n\nand\n\n(cid:98)θi = aX i + (1 − a)(cid:98)μ, for i ∈ [m],\n\nwhere a =\n\nThe above estimator achieves the MSE: Eθi,X1,...,Xm (cid:107)(cid:98)θi − θi(cid:107)2 ≤ dσ2\n\nx\n\nn\n\n(cid:0) 1−a\n\nm + a(cid:1).\n\nσ2 θ\nθ + σ2\n\nσ2\n\nx/n\n\n.\n\n(3)\n\nn\n\nm\n\n(cid:80)n\n\n(cid:80)m\n\ni=1 qi back to the clients.\n\nx/mn. Otherwise, when σ2\n\nIt follows that the mechanism q and the aggregation function Agg for the estimators in (3) (as described in (1)) are just the average functions, where client i sends qi = q(Xi) := X i = 1 j=1 Xij to the server, and the server sends (cid:98)μ := Agg(q1, . . . , qm) = 1 Remark 1 (Personalized estimate vs. local estimate). When σθ → 0, then a → 0, which implies that (cid:98)θi → (cid:98)μ and MSE → dσ2 x/n or n → ∞, then a → 1, which implies that (cid:98)θi → X i and MSE → dσ2 x/n. These conform to the facts that (i) when there is no heterogeneity, then the global average is the best estimator, and (ii) when heterogeneity is not small, and we have a lot of local samples, then the local average is the best estimator. Observe that the multiplicative gap between the MSE of the proposed personalized estimator and the MSE of the local estimator (based on local data only, which gives an MSE of dσ2 m + a) ≤ 1 that proves the superiority of the personalized model over the local model, which is equal to 1/m when σθ = 0 and equal to 0.01 when m = 104, n = 100 and σ2 Remark 2 (Optimality of our personalized estimator). In Appendix B, we show the minimax lower m + a(cid:1), which exactly matches the upper bound: inf bound on the MSE in Theorem 1, thus establishes the optimality our personalized estimator in (3).\n\nθ is large in comparison to σ2\n\nx)(cid:107)(cid:98)θ(X) − θ(cid:107)2 ≥ dσ2\n\nθ = 10−3, for example.\n\nx/n) is given by ( 1−a\n\nEX∼N (θ,σ2\n\nx = 10, σ2\n\n(cid:98)θ supθ∈Θ\n\n(cid:0) 1−a\n\nn\n\nx\n\nPrivacy and communication constraints. Observe that the scheme presented above does not protect privacy of clients’ data and messages from the clients to the server can be made communicationefficient. These could be achieved by employing specific mechanisms q at clients: For privacy, we can take a differentially-private q, and for communication-efficiency, we can take q to be a quantizer. Inspired by the scheme presented above, here we consider q to be a function q : Rd → Y, that takes the average of n data points as its input, and the aggregator function Agg to be the average function. Define (cid:98)μq := 1\n\ni=1 q(X i) and consider the following personalized estimator for the i-th client:\n\n(cid:80)m\n\nm\n\n(cid:98)θi = aX i + (1 − a)(cid:98)μq,\n\nfor some a ∈ [0, 1].\n\n(4)\n\nTheorem 2. Suppose for all x ∈ Rd, q satisfies E[q(x)] = x and E(cid:107)q(x) − x(cid:107)2 ≤ dσ2 finite σq. Then the personalized estimator in (4) has MSE:\n\nq for some\n\nEθi,q,X1,...,Xm(cid:107)(cid:98)θi − θi(cid:107)2 ≤\n\ndσ2 x\nn\n\n(cid:16) 1 − a m\n\n(cid:17)\n\n+ a\n\nwhere\n\na =\n\nσ2 θ + σ2\n\nσ2\n\nθ + σ2\n\nq/m−1\n\nq/m−1 + σ2\n\nx/n\n\n.\n\n(5)\n\nFurthermore, assuming μ ∈ [−r, r]d for some constant r (but μ is unknown), we have:\n\n1. Communication efficiency: For any k ∈ N, there is a q whose output can be represented using k-bits (i.e., q is a quantizer) that achieves the MSE in (5) with probability at least 1 − 2/mn and with σq = b\n\n(cid:112)log(m2n) + σx√\n\n(cid:112)log(m2n).\n\n(2k−1) , where b = r + σθ\n\nn\n\n2. Privacy: For any (cid:15)0 ∈ (0, 1), δ > 0, there is a q that is user-level ((cid:15)0, δ)-locally differentially private, that achieves the MSE in (5) with probability at least 1 − 2/mn and with σq = b\n(cid:15)0\n\n(cid:112)8 log(2/δ), where b = r + σθ\n\n(cid:112)log(m2n) + σx√\n\n(cid:112)log(m2n).\n\nn\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n2.2 BERNOULLI MODEL\n\nFor the Bernoulli model, P is supported on [0, 1], and p1, . . . , pm are sampled i.i.d. from P, and client i is given n i.i.d. samples Xi1, . . . , Xin ∼ Bern(pi). This setting has been studied in (Tian et al. (2017); Vinayak et al. (2019)) for estimating P, whereas, our goal is to estimate individual parameter pi at client i using the information from other clients. In order to derive a closed form MSE result, we assume that P is the Beta distribution.3 Here, Γ = (α, β), p1, . . . , pm are unknown, and client i’s goal is to estimate pi such that the Bayesian risk Epi∼πE (cid:98)pi,X1,...,Xm((cid:98)pi − pi)2 is minimized, where π denotes the density of the Beta distribution. Omitted proofs/details are provided in Appendix C.\n\nAnalogous to the Gaussian case, we can show that if α, β are known, then the posterior mean estimator has a closed form expression: (cid:98)pi = aX i + (1 − a) α α+β , where a = n/α+β+n and α/(α+β) is the mean of the beta distribution. When α, β are unknown, inspired by the above discussion, a natural approach would be to estimate the global mean μ = α/(α+β) and the weight a = n/(α+β+n), and use that in the above estimator. Note that, for a we need to estimate α + β, which is equal to μ(1−μ)/σ2 − 1, where σ2 = αβ/(α+β)2(α+β+1) is the variance of the beta distribution. Therefore, it is enough to estimate μ and σ2 for the personalized estimators {(cid:98)pi}. In order to make calculations simpler, instead of making one estimate of μ, σ2 for all clients, we let each client make its own estimate of μ, σ2 (without using i = 1 their own data) as: (cid:98)μi = 1 l(cid:54)=i(X l − (cid:98)μl)2,4 and then define the local weight as (cid:98)ai =\n\ni −1+n . Now, client i uses the following personalized estimator:\n\nl(cid:54)=i X l and (cid:98)σ2\n\n(cid:98)μi(1− (cid:98)μi)/(cid:98)σ2\n\nm−2\n\nm−1\n\n(cid:80)\n\n(cid:80)\n\nn\n\n(cid:98)pi = (cid:98)aiX i + (1 − (cid:98)ai)(cid:98)μi.\n\n(6)\n\nTheorem 3. With probability at least 1− 1 (α+β)2(α+β+1) + 3 log(4m2n) Epi∼πEX1,...,Xm ((cid:98)pi − pi)2 ≤ E[(cid:98)a2 Remark 3. When n → ∞, then (cid:98)ai → 1, which implies that MSE tends to the MSE of the local estimator X i, which means if local samples are abundant, collaboration does not help much. When σ2 = αβ/(α+β)2(α+β+1) → 0, i.e. there is very small heterogeneity in the system, then (cid:98)ai → 0, which implies that MSE tends to the error due to moment estimation (the last term in the MSE in Theorem 3).\n\nmn , the MSE of the personalized estimator in (6) is given by: (cid:1).\n\n(cid:1) + E[(1 −(cid:98)ai)2](cid:0)\n\nαβ n(α+β)(α+β+1)\n\ni ](cid:0)\n\nm−1\n\nαβ\n\nPrivacy constraints. For any privacy parameter (cid:15)0 > 0 and input x ∈ [0, 1], define qpriv : [0, 1] → R:\n\nqpriv(x) =\n\n(cid:40) −1\n\ne(cid:15)0 −1 w.p. e(cid:15)0 −1 w.p.\n\ne(cid:15)0\n\ne(cid:15)0\n\ne(cid:15)0 +1 − x e(cid:15)0 −1 e(cid:15)0 +1 , e(cid:15)0 +1 + x e(cid:15)0 −1 e(cid:15)0 +1 .\n\n1\n\n(7)\n\nThe mechanism qpriv is unbiased and satisfies user-level (cid:15)0-LDP. Thus, the ith client sends qpriv(X i) to the server, which computes (cid:98)μpriv i = 1 =\n\n(cid:80)\n\ni\n\nl\n\ni\n\npriv (cid:98)μ i\n\nn )/(cid:98)σ\n\npriv (1− (cid:98)μ i\n\n1 m−2\n\nl(cid:54)=i(qpriv(X l)) − (cid:98)μpriv i =\n\n(cid:80) )2 and sends ((cid:98)μpriv and uses (cid:98)ppriv defines (cid:98)apriv Theorem 4. With probability at least 1 − 1 fined above is given by: Epi∼πEqpriv,X1,...,Xm ((cid:98)ppriv (α+β)2(α+β+1) + (e(cid:15)0 +1)2 log(4m2n) (cid:98)apriv\n\n3(e(cid:15)0 −1)2(m−1)\n\n)2](cid:0)\n\n2(priv) i\n\n(cid:1).\n\n+n\n\nαβ\n\ni\n\ni\n\nm−1\n\nl(cid:54)=i qpriv(X l) and the variance (cid:98)σ2(priv) ) to client i. Upon receiving this, client i\n\ni X i + (1 − (cid:98)apriv\n\n, (cid:98)σ2(priv) i = (cid:98)apriv mn , the MSE of the personalized estimator (cid:98)ppriv )2](cid:0)\n\n)(cid:98)μpriv to estimate pi.\n\ni − pi)2 ≤ E[((cid:98)apriv\n\nαβ n(α+β)(α+β+1)\n\nde- (cid:1) + E[(1 −\n\ni\n\ni\n\ni\n\nSee Remark 4 (in Appendix B) and Remarks 6 and 7 (in Appendix C) for a discussion on privacy, communication efficiency, and client sampling.\n\n3 PERSONALIZED LEARNING\n\nConsider a client-server architecture with m clients. There is an unknown global population distribution P(Γ)5 over Rd from which m i.i.d. local parameters θ1, . . . , θm ∈ Rd are sampled. Each client\n\n3Beta distribution has a density Beta(α, β) = 1 where B(α, β) is a normalizing constant. Its mean is α\n\nB(α,β) xα−1(1−x)β−1 is defined for α, β > 0 and x ∈ [0, 1],\n\nα+β and the variance is\n\nαβ\n\n(α+β)2(α+β+1) .\n\n4Upon receiving {X i} from all clients, the server can compute {(cid:98)μi, (cid:98)σ2 i ) to the i-th client. 5For simplicity we will consider this unknown population distribution P to be parametrized by unknown\n\ni } and sends ((cid:98)μi, (cid:98)σ2\n\n(arbitrary) parameters Γ.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\ni ∈ [m] is provided with a dataset consisting of n data points {(Xi1, Yi1), . . . , (Xin, Yin)}, where Yij’s are generated from (Xij, θi) using some distribution pθi(Yij|Xij). Let Yi := (Yi1, . . . , Yin) and Xi := (Xi1, . . . , Xin) for i ∈ [m]. The underlying statistical model for our setting is given by\n\np{θi,Yi}|{Xi}(θ1, . . . , θm, Y1, . . . , Ym|X1, . . . , Xm) =\n\nm (cid:89)\n\ni=1\n\np(θi)\n\nm (cid:89)\n\nn (cid:89)\n\ni=1\n\nj=1\n\npθi(Yij|Xij).\n\n(8)\n\nNote that if we minimize the negative log likelihood of (8), we would get the optimal parameters:\n\n(cid:98)θ1, . . . , (cid:98)θm := arg min θ1,...,θm\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n− log(pθi(Yij|Xij)) +\n\nm (cid:88)\n\ni=1\n\n− log(p(θi)).\n\n(9)\n\nHere, fi(θi) := (cid:80)n j=1 − log(pθi(Yij|Xij)) denotes the loss function at the i-th client, which only depends on the local data, and R({θi}) := (cid:80)m i=1 − log(p(θi)) is the regularizer that depends on the (unknown) global population distribution P (parametrized by unknown Γ). Note that when clients have little data and we have large number of clients, i.e., n (cid:28) m – the setting of federated learning, clients may not be able to learn good personalized models from their local data alone (if they do, it would lead to large loss). In order to learn better personalized models, clients may utilize other clients’ data through collaboration, and the above regularizer (and estimates of the unknown prior distribution P, through estimating its parameters Γ) dictates how the collaboration might be utilized. The above-described statistical framework (9) can model many different scenarios, as detailed below:\n\n2\n\nθ,l\n\n2σ2\n\nl=1, {μl}k\n\n2 log(2πσ2\n\n1. When P(Γ) ≡ GM({pl}k\n\nl=1, {μl}k i=1 log (cid:0) (cid:80)k\n\nl=1) is a Gaussian mixture,\n\nl=1, {σ2 l=1}(cid:1) : pl ≥ 0, (cid:80)k\n\nl=1, {σθ,l}k l=1 pl exp(− (cid:107)μl−θi(cid:107)2\n\nθ,l}k for Γ = l=1 pl = 1, σθ,l ≥ 0, μl ∈ Rd(cid:9), then R({θi}) = )/((2πσθ,l)d/2)(cid:1). Here, the client models θ1, . . . , θm are Id) with prob. pl, for l = 1, . . . , k. For k = 1,\n\n(cid:8)(cid:0){pl}k − (cid:80)m drawn i.i.d. from P(Γ), where θi ∼ N (μl, σ2 R({θi}) = md . Here, unknown μ can be connected to the global model and θi’s as local models, and the alternating iterative optimization optimizes over both. This justifies the use of (cid:96)2 regularizer in earlier personalized learning works (Dinh et al., 2020; Ozkara et al., 2021; Hanzely & Richt ́arik, 2020; Hanzely et al., 2020; Li et al., 2021). 2. When P(Γ) ≡ Laplace(μ, b), for Γ = {μ, b > 0}, then R({θi}) = m log(2b) + (cid:80)m 3. When pθi(Yij|Xij) is according to N (θi, σ2\n\nx), then fi(θi) is the quadratic loss as in linear regression. When pθi (Yij|Xij) = σ((cid:104)θi, Xij(cid:105))Yij (1 − σ((cid:104)θi, Xij(cid:105)))(1−Yij ), where σ(z) = 1/1+e−z for any z ∈ R, then fi(θi) is the cross-entropy (or logistic) loss as in logistic regression.\n\nθ ) + (cid:80)m\n\n(cid:107)μ−θi(cid:107)2 2σ2 θ\n\n(cid:107)θi−μ(cid:107)1 b\n\ni=1\n\ni=1\n\nθ,l\n\n.\n\n2\n\n3.1 AD AMI X: ADAPTIVE PERSONALIZATION WITH GAUSSIAN MIXTURE PRIOR\n\nNow we write the full objective function for the Gaussian mixture prior model for a generic local loss function fi(θi) at client i (the case of linear/logistic regression with (single) Gaussian prior and solving using alternating gradient descent is discussed in Appendices E, F.): m\n(cid:88)\n\nm (cid:88)\n\nk (cid:88)\n\n(cid:17)\n\n(cid:16)\n\nF gm\n\ni\n\n(θi) :=\n\nfi(θi) − log(\n\npl exp(−\n\n)/((2πσθ,l)d/2))\n\narg min {θi},{μl},{pl},{σθ,l}\n\n(cid:107)μl − θi(cid:107)2 2σ2\n\n2\n\nθ,l\n\ni=1\n\ni=1\n\nl=1\n\n(10) A common example of fi(θi) is a generic neural network loss function with multi-class softmax output layer and cross entropy loss, i.e., fi(θi) := (cid:80)n j=1 − log(pθi(Yij|Xij)), where pθi(Yij|Xij) = σ((cid:104)θi, Xij(cid:105))Yij (1 − σ((cid:104)θi, Xij(cid:105)))(1−Yij ), where σ(z) = 1/1+e−z for any z ∈ R. To solve (10), we can either use an alternating gradient descent approach, or we can use a clustering based approach where the server runs a (soft) clustering algorithm on received personalized models. We adopt the second approach here (described in Algorithm 1) as it provides an interesting point of view and can be combined with DP clustering algorithms. Here clients receive the global parameters from the server and do a local iteration on the personalized model (multiple local iterations can be introduced as in FedAvg (McMahan et al., 2017)), later the clients send the personalized models. Receiving the personalized models, server initiates GMM algorithm that outputs global parameters 6\n\n6A discrete mixture model can be proposed as a special case of GM with 0 variance. With this we can recover\n\na similar algorithm as in Marfoq et al. (2021). Further details are presented in Appendix G.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n3.2 AD APED: ADAPTIVE PERSONALIZATION VIA DISTILLATION\n\nIt has been empirically observed that the knowledge distillation (KD) regularizer (between local and global models) results in better performance than the (cid:96)2 regularizer (Ozkara et al., 2021). In fact, using our framework, we can define, for the first time, a certain prior distribution that gives the KD regularizer (see Appendix H). We use the following loss function at the i-th client:\n\nfi(θi) +\n\n1 2\n\nlog(2ψ) +\n\nf KD i\n\n(θi, μ) 2ψ\n\n,\n\n(11)\n\nwhere μ denotes the global model, θi denotes the personalized model at client i, and ψ can be viewed as controlling heterogeneity. The goal for each client is to minimize its local loss function, so individual components cannot be too large. For the second term, this implies that ψ cannot be unbounded. For the third term, if f KD (θi, μ) is large, then ψ will also increase i\n(implying that the local parameters are too deviated from the global parameter), hence, it is better to emphasize local training loss to make the first term small. If f KD (θi, μ) is small, then i\nψ will also decrease (implying that the local parameters are close to the global parameter), so it is better to collaborate and learn better personalized models. Such adaptive weighting quantifies the uncertainty in population distribution during training, balances the learning accordingly, and improves the empirical performance over nonadaptive methods, e.g., (Ozkara et al., 2021).\n\nAlgorithm 1 Personalized Learning with Gaussian Mixture Prior (AdaMix) Input: Number of iterations T , local datasets (Xi, Yi) for i ∈ [m], learning rate η. 1 , . . . , θ(0)\n\nand\n\nm\n\nθ(0) k , σ(0)\n\n1 , . . . , μ(0)\n\nθ,1, . . . , σ(0) θ,k.\n\n1: Initialize P(0), μ(0)\n\n2: for t = 1 to T do 3: On Clients: 4: 5:\n\nfor i = 1 to m: do\n\nReceive P(t−1), μ(t−1) σ(t−1) , . . . , σ(t−1) Update the personalized parameters:\n\n, . . . , μ(t−1) from the server\n\nθ,k\n\nθ,1\n\nk\n\n1\n\n, and\n\ni ← θ(t−1) θ(t)\n\ni\n\n− η∇θ(t−1)\n\ni\n\nF gm\n\ni\n\n(θ(t−1)\n\ni\n\n)\n\nSend θ(t)\n\ni\n\nto the server\n\nend for At the Server: Receive θ(t) 1 , . . . , θ(t) Update the global parameters:\n\nm from the clients\n\nP(t), μ(t)\n\n1 , . . . , μ(t)\n\nθ,1, . . . , σ(t)\n\nθ,k\n\nk , σ(t) ← GMM(cid:0)θ(t)\n\n1 , . . . , θ(t)\n\nm , k(cid:1)\n\nBroadcast P(t), {μ(t) clients\n\ni }k\n\ni=1, {σ(t)\n\nθ,i}k\n\ni=1 to all\n\n6:\n\n7: 8: 9: 10: 11:\n\n12:\n\n13: end for Output: Personalized models θT\n\n1 , . . . , θT m.\n\nTo optimize (11) we propose an alternating minimization approach, which we call AdaPeD; see Algorithm 2. Besides the personalized model θt i, each client i keeps local copies of the global model μt i , and at synchronization times, server aggregates them to obtain global versions of these μt, ψt. In this way, the local training of θt i. In the end, clients have learned their personalized models {θT\n\ni also incorporates knowledge from other clients’ data through μt i=1.\n\ni and of the dissimilarity term ψt\n\ni }m\n\n3.3 DP-AD APED: DIFFERENTIALLY PRIVATE ADAPTIVE PERSONALIZATION VIA DISTILL.\n\nNote that client i communicates μt the gradients ht i, kt order to obtain DP-AdaPeD, we replace lines 13 and 15, respectively, by the update rules:\n\ni (which are updated by accessing the dataset for computing i . In\n\ni , client i adds appropriate noise to hk\n\ni ) to the server. So, to privatize μt\n\ni, ψt\n\ni, ψt\n\ni , kt\n\nμt+1\n\ni = μt\n\ni − η2\n\n(cid:16)\n\ni\n\nht max{(cid:107)ht i(cid:107)/C1, 1} Id) and ν2 ∼ N (0, σ2\n\n+ ν1\n\n(cid:17)\n\nand ψt+1\n\ni = ψt\n\ni − η3\n\n(cid:16)\n\nkt i\nmax{|kt i |/C2, 1}\n\n(cid:17)\n\n,\n\n+ ν2\n\nwhere ν1 ∼ N (0, σ2 q1 privacy level and C1, C2, which are some predefined constants.\n\nq2\n\n), for some σq1, σq2 > 0 that depend on the desired\n\nThe theorem below (proved in Appendix I) states the R ́enyi Differential Privacy (RDP) guarantees. Theorem 5. After T iterations, DP-AdaPeD satisfies (α, (cid:15)(α))-RDP for α > 1, where (cid:15)(α) = (cid:0) K m denotes the sampling ratio of clients at each global iteration. m\n\n(cid:16) C2 1\nKσ2\n\n, where K\n\n+ C2\n\n2 Kσ2\n\nτ α\n\n6 T\n\n(cid:1)2\n\n(cid:17)\n\nq1\n\nq2\n\nWe bound the RDP, as it gives better privacy composition than using the strong composition (Mironov et al., 2019). We can also convert our results to user-level ((cid:15), δ)-DP by using the standard conversion from RDP to ((cid:15), δ)-DP (Canonne et al., 2020). See Appendix A for background on privacy.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n(a) Private Estimation (Sec. 2.1)\n\n(b) Private Learning (Sec. 3.3)\n\nFigure 1: In Fig. 1a, we plot MSE vs. (cid:15)0 for personalized estimation. In Fig. 1b, we plot Test Accuracy vs. (cid:15) on FEMNIST with client sampling 0.33, for DP-AdaPeD with unsampled client iterations. Since local training is private, both plots remain constant against (cid:15).\n\n4 EXPERIMENTS\n\nPersonalized Estimation. We run one experiment for Bernoulli setting with real political data and the other for Gaussian setting with synthetic data. The latter one is differentially private.\n\n• Political tendencies on county level. One natural application of Bernoulli setting is modeling bipartisan elections (Tian et al., 2017). We did a case study by using US presidential elections on county level between 2000-2020, with m = 3112 counties in our dataset. For each county the goal is to determine the political tendency parameter pi. Given 6 election data we did 6-fold cross validation, with 5 elections for training and 1 election for test data. Local estimator takes an average of 5 training samples and personalized estimator is the posterior mean. To simulate a Bernoulli setting we set the data equal to 1 if Republican party won the election and 0 otherwise. We observe the personalized estimator provides MSE (averaged over 6 runs) gain of 10.7 ± 1.9% against local estimator.\n\nAlgorithm 2 Adaptive Personalization via Distillation (AdaPeD) Parameters: local variances {ψ0 models {θ0 {μ0 gap τ\n\ni }, personalized i }, local copies of the global model i }, learning rates η1, η2, η3, synchronization\n\nif τ divides t then On Server do: Choose a subset Kt ⊆ [n] of K clients Broadcast μt and ψt On Clients i ∈ Kt (in parallel) do: Receive μt, ψt; set μt\n\n1: for t = 0 to T − 1 do 2: 3: 4: 5: 6: 7: 8: 9: On Clients i ∈ Kt (in parallel) do: i (θt f KD 2ψt i\n\ni = μt, ψt\n\nCompute gt\n\ni := ∇θt\n\ni = ψt\n\nfi(θt\n\nend if\n\ni) +\n\n∇θt\n\ni\n\ni\n\n10:\n\ni,μt i)\n\ni)/2ψt\n\ni\n\ni\n\ni\n\ni\n\n,μt\n\n11: 12:\n\n13: 14:\n\ni − η1gt i (θt+1 f KD i − η2ht − f KD i − η3kt\n\ni = θt i := ∇μt i = μt i := 1 i = ψt\n\nUpdate: θt+1 Compute ht Update: μt+1 Compute kt Update: ψt+1 if τ divides t + 1 then Clients send μt Server receives {μt Server computes μt+1 = 1 and ψt+1 = 1\n\n• DP personalized estimation. To measure the performance tradeoff of the DP mechanism described in Section 2.1, we create a synthetic experiment for Gaussian setting. We let m = 10000, n = 15 and σθ = 0.1, σx = 0.5, and create a dataset at each client as described in Gaussian setting. Applying the DP mechanism we obtain the following result in Figure 1a. Here, as expected, when privacy is low ((cid:15)0 is high) the private personalized estimator recovers the regular personalized estimator. For higher privacy the private estimator’s performance starts to become worse than the non-private estimator.\n\nend if 20: 21: end for Output: Personalized models (θT\n\n15: 16: 17: 18: 19:\n\ni i (θt+1\n\ni and ψt\n\ni∈Kt ψt\n\ni )m\n\n2ψt i\n\n(cid:80)\n\ni=1\n\nK\n\nK\n\ni\n\ni\n\ni\n\ni\n\ni to Server i}i∈Kt and {ψt (cid:80)\n\n,μt+1\n\n)/2(ψt\n\ni )2\n\ni }i∈Kt i∈Kt μt\n\ni\n\nPersonalized Learning. First we describe the experiment setting and then the results.\n\n• Experiment setting. We consider image classification on MNIST, FEMNIST (Caldas et al., 2018), CIFAR-10, CIFAR-100 (experimental details for CIFAR-100 is given in Appendix K); and train a CNN, similar to the one considered in (McMahan et al., 2017), that has 2 convolutional and 3 fully connected layers. We set m = 66 for FEMNIST and m = 50 for MNIST, CIFAR-10, CIFAR-100. For FEMNIST, we use a subset of 198 writers so that each client has access to data from 3 authors, which results in a natural type of data heterogeneity due to writing styles of authors. On MNIST, CIFAR-10 we introduce pathological heterogeneity by letting each client sample data from 3 and 4 randomly selected classes only, respectively. We set τ = 10 and vary the batch size so that each epoch consists of 60 iterations. On MNIST we train for 50 epochs, on CIFAR-10 for 250 epochs, on FEMNIST for 40 and 80 epochs, for 0.33 and 0.15 client sampling ratio, respectively. We discuss further details in Appendix K.\n\n8\n\n0.10.20.30.40.50.60.70.8ε00.60.81.01.21.41.6MSE (in 1e-2)Local EstimatorPersonalized EstimatorDP Personalized Estimator456789ε93.093.594.094.595.095.596.096.597.0Test Accuracy (in %)Local trainingDP-AdaPeDPublished as a conference paper at ICLR 2023\n\nTable 1: Test accuracy (in %) for CNN model. The CIFAR-10, MNIST, and FEMNIST columns have client sampling ratios K\n\nn of 0.2, 0.1, and 0.15, respectively.\n\nMethod\n\nCIFAR-10\n\nCIFAR-100\n\nFEMNIST\n\nFedAvg FedAvg+fine tuning (Jiang et al., 2019) AdaPeD (Ours) pFedMe (Dinh et al., 2020) Per-FedAvg (Fallah et al., 2020) QuPeD (FP) (Ozkara et al., 2021) Federated ML (Shen et al., 2020)\n\n60.86 ± 0.94 63.12 ± 0.31 72.49 ± 0.42 69.53 ± 0.16 59.95 ± 0.79 71.61 ± 0.70 71.09 ± 0.67\n\n30.48 ± 0.33 39.98 ± 0.26 53.11 ± 0.34 43.65 ± 0.18 34.78 ± 0.41 51.94 ± 0.21 50.42 ± 0.26\n\n92.18 ± 0.13 94.12 ± 0.26 96.55 ± 0.32 94.95 ± 0.55 93.51 ± 0.31 95.99 ± 0.08 95.12 ± 0.18\n\nMethod\n\n(cid:15) = 3.35\n\n(cid:15) = 13.16\n\n(cid:15) = 27.30\n\nDP-FedAvg DP-AdaPeD (Ours)\n\n11.73 ± 0.85 93.32 ± 1.18\n\n29.91 ± 1.28 98.51 ± 0.90\n\n55.79 ± 0.29 99.01 ± 0.65\n\nTable 2: (DP-AdaPeD) Test Accuracy (in %) vs. (cid:15) on MNIST without client sampling.\n\nMethod\n\nn = 10\n\nn = 20\n\nn = 30\n\nLocal Training AdaMix\n\n39.93 ± 0.13 10.42 ± 0.15\n\n30.02 ± 0.08 3.12 ± 0.04\n\n19.97 ± 0.07 2.55 ± 0.04\n\nTable 3: Mean squared error of our AdaMix algorithm and the local training for linear regression.\n\n• Results. In Table 1 we compare AdaPeD against FedAvg (McMahan et al., 2017), FedAvg+ (Jiang et al., 2019) and various personalized FL algorithms: pFedMe (Dinh et al., 2020), Per-FedAvg (Fallah et al., 2020), QuPeD (Ozkara et al., 2021) without model compression, and Federated ML (Shen et al., 2020). We report further results in Appendix K. We observe AdaPeD consistently outperforms other methods. It can be seen that methods that use knowledge distillation perform better; on top of this, AdaPeD enables us adjust the dependence on collaboration according to the compatibility of global and local decisions/scores. For instance, we set σ2 θ to a certain value initially, and observe it progressively decrease, which implies clients start to rely on the collaboration more and more. Interestingly, this is not always the case: for DP-AdaPeD, we first observe a decrease in σ2 θ and later it increases. This suggests: while there is not much accumulated noise, clients prefer to collaborate, and as the noise accumulation on the global model increases due to DP noise, clients prefer not to collaborate. This is exactly the type of autonomous behavior we aimed with adaptive regularization.\n\n• DP-AdaPeD. In Figure 1b and Table 2, we observe performance of DP-AdaPeD under different (cid:15) values. DP-AdaPeD outperforms DP-FedAvg because personalized models do not need to be privatized by DP mechanism, whereas the global model needs to be. Our experiments provide user-level privacy (more stringent, but appropriate in FL), as opposed to the item-level privacy.\n\n• DP-AdaPeD with unsampled client iterations. When we let unsampled clients to do local iterations (free in terms of privacy cost and a realistic scenario in cross-silo settings) described in Appendix H, we can increase DP-AdaPeD’s performance under more aggressive privacy constants (cid:15). For instance, for FEMNIST with 1/3 client sampling we obtain the result reported in Figure 1b.\n\n• AdaMix. We consider linear regression on synthetic data, with m = 1000 clients and each client has n ∈ {10, 20, 30} local samples. Each local model θi ∈ Rd is drawn from a mixture of two Gaussian distributions N (μ, Σ) and N (−μ, Σ), where Σ = 0.001 × Id and d = 50. Each client sample (Xij, Yij) is distributed as Xij ∼ N (0, Id) and Yij = (cid:104)Xij, θi(cid:105) + wij, where wij ∼ N (0, 0.1). Table 3 demonstrates the superior performance of AdaMix against the local estimator.\n\n5 CONCLUSION\n\nWe proposed a statistical framework leading to new personalized federated estimation and learning algorithms (e.g., AdaMix, AdaPeD); we also incorporated privacy (and communication) constraints into our algorithms and analyzed them. Open questions include information theoretic lower bounds and its comparison to proposed methods; examination of how far the proposed alternating minimization methods (such as in AdaMix, AdaPeD) are from global optima.\n\nThe work in this paper was partially supported by NSF grants 2139304, 2007714 and gift funding by Meta\n\nand Amazon.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nDurmus Alp Emre Acar, Yue Zhao, Ruizhao Zhu, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Debiasing model updates for improving personalized federated training. In International Conference on Machine Learning, pp. 21–31. PMLR, 2021.\n\nSara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward. Better guarantees for kmeans and euclidean k-median by primal-dual algorithms. SIAM Journal on Computing, 49(4): FOCS17–97, 2019.\n\nDan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communicationefficient sgd via gradient quantization and encoding. Advances in Neural Information Processing Systems, 30, 2017.\n\nBorja Balle, Gilles Barthe, Marco Gaboardi, Justin Hsu, and Tetsuya Sato. Hypothesis testing interpretations and renyi differential privacy. In Silvia Chiappa and Roberto Calandra (eds.), International Conference on Artificial Intelligence and Statistics (AISTATS), volume 108 of Proceedings of Machine Learning Research, pp. 2496–2506. PMLR, 2020.\n\nLeighton Pate Barnes, Yanjun Han, and Ayfer Ozgur. Lower bounds for learning distributions under communication constraints via fisher information. Journal of Machine Learning Research, 21 (236):1–30, 2020.\n\nJeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd: Compressed optimisation for non-convex problems. In International Conference on Machine Learning, pp. 560–569. PMLR, 2018.\n\nDavid M. Blei, Michael I. Jordan, and A. Ng. Hierarchical bayesian models for applications in\n\ninformation retrieval. 2003.\n\nMark Bun, Cynthia Dwork, Guy N. Rothblum, and Thomas Steinke. Composable and versatile privacy via truncated CDP. In ACM SIGACT Symposium on Theory of Computing (STOC), pp. 74–86, 2018.\n\nSebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018.\n\nCl ́ement L. Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential\n\nprivacy. In Neural Information Processing Systems (NeurIPS), 2020.\n\nHuili Chen, Jie Ding, Eric Tramel, Shuang Wu, Anit Kumar Sahu, Salman Avestimehr, and Tao Zhang. Self-aware personalized federated learning. CoRR, abs/2204.08069, 2022. doi: 10.48550/ arXiv.2204.08069. URL https://doi.org/10.48550/arXiv.2204.08069.\n\nLiam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In Marina Meila and Tong Zhang (eds.), International Conference on Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pp. 2089–2099. PMLR, 2021.\n\nYuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated\n\nlearning. arXiv preprint arXiv:2003.13461, 2020.\n\nCanh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau\n\nenvelopes. In Advances in Neural Information Processing Systems, 2020.\n\nSimon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning the representation, provably. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=pW2Q2xLwIMD.\n\nCynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations\n\nand Trends in Theoretical Computer Science, 9(3-4):211–407, 2014.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity\n\nin private data analysis. In Theory of Cryptography Conference (TCC), pp. 265–284, 2006.\n\nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-\n\nlearning approach. In Advances in Neural Information Processing Systems, 2020.\n\nAndrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin.\n\nBayesian Data Analysis. Chapman and Hall/CRC, 2013.\n\nRobin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client\n\nlevel perspective. arXiv preprint arXiv:1712.07557, 2017.\n\nBadih Ghazi, Ravi Kumar, and Pasin Manurangsi. Differentially private clustering: Tight approxima-\n\ntion ratios. Advances in Neural Information Processing Systems, 33:4040–4054, 2020.\n\nBadih Ghazi, Ravi Kumar, and Pasin Manurangsi. User-level differentially private learning via correlated sampling. In Neural Information Processing Systems (NeurIPS), pp. 20172–20184, 2021.\n\nAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for\n\nclustered federated learning. In Advances in Neural Information Processing Systems, 2020.\n\nRichard Gill and Boris Levit. Applications of the van trees inequality: A bayesian cram ́er-rao bound.\n\nBernoulli, 1:59–79, 03 1995. doi: 10.2307/3318681.\n\nAntonious M Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and Ananda Theertha Suresh. Shuffled model of federated learning: Privacy, accuracy and communication trade-offs. IEEE Journal on Selected Areas in Information Theory, 2(1):464–478, 2021a.\n\nAntonious M. Girgis, Deepesh Data, Suhas N. Diggavi, Peter Kairouz, and Ananda Theertha Suresh. In International Conference on Shuffled model of differential privacy in federated learning. Artificial Intelligence and Statistics (AISTATS), volume 130 of Proceedings of Machine Learning Research, pp. 2521–2529. PMLR, 2021b.\n\nAntonious M. Girgis, Deepesh Data, and Suhas Diggavi. Distributed user-level private mean estimation. In 2022 IEEE International Symposium on Information Theory (ISIT), pp. 2196–2201, 2022. doi: 10.1109/ISIT50566.2022.9834713.\n\nFilip Hanzely and Peter Richt ́arik. Federated learning of a mixture of global and local models. arXiv\n\npreprint arXiv:2002.05516, 2020.\n\nFilip Hanzely, Slavom ́ır Hanzely, Samuel Horv ́ath, and Peter Richt ́arik. Lower bounds and optimal algorithms for personalized federated learning. In Advances in Neural Information Processing Systems, 2020.\n\nRui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and Yanmin Gong. Personalized federated\n\nlearning with differential privacy. IEEE Internet of Things Journal, 7(10):9530–9539, 2020.\n\nPrateek Jain, John Rush, Adam Smith, Shuang Song, and Abhradeep Guha Thakurta. Differentially private model personalization. Advances in Neural Information Processing Systems, 34, 2021a.\n\nPrateek Jain, John Rush, Adam Smith, Shuang Song, and Abhradeep Guha Thakurta. Differentially private model personalization. In Advances in Neural Information Processing Systems, volume 34, 2021b.\n\nWilliam James and Charles Stein. Estimation with quadratic loss. In Proceedings Berkeley Symposium\n\non Mathematics and Statistics, Vol 1, pp. 361–379. University of California Press, 1961.\n\nYihan Jiang, Jakub Koneˇcn`y, Keith Rush, and Sreeram Kannan.\n\nImproving federated learning\n\npersonalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur ́elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nShiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith.\n\nWhat can we learn privately? SIAM Journal on Computing, 40(3):793–826, 2011.\n\nMikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-\n\nlearning methods. In Advances in Neural Information Processing Systems, 2019.\n\nNikita Kotelevskii, Maxime Vono, Eric Moulines, and Alain Durmus. Fedpop: A bayesian approach\n\nfor personalised federated learning. CoRR, abs/2206.03611, 2022, June.\n\nDaniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, and Ananda Theertha Suresh. Learning with user-level privacy. In Neural Information Processing Systems (NeurIPS), pp. 12466–12479, 2021.\n\nDaliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv\n\npreprint arXiv:1910.03581, 2019.\n\nJeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially private metalearning. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=rJgqMRVYvr.\n\nTian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning, pp. 6357–6368. PMLR, 2021.\n\nTao Lin, Lingjing Kong, Sebastian U. Stich, and Martin Jaggi. Ensemble distillation for robust model\n\nfusion in federated learning. In Advances in Neural Information Processing Systems, 2020.\n\nYuhan Liu, Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and Michael Riley. Learning discrete distributions: user vs item-level privacy. In Neural Information Processing Systems (NeurIPS), 2020.\n\nStuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):\n\n129–137, 1982.\n\nFrederic M. Lord. Estimating true-score distributions in psychological testing (an empirical bayes estimation problem)*. ETS Research Bulletin Series, 1967(2):i–51, 1967. doi: https://doi.org/10. 1002/j.2333-8504.1967.tb00535.x. URL https://onlinelibrary.wiley.com/doi/ abs/10.1002/j.2333-8504.1967.tb00535.x.\n\nYishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.\n\nOthmane Marfoq, Giovanni Neglia, Aur ́elien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. Advances in Neural Information Processing Systems, 34, 2021.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pp. 1273–1282. PMLR, 2017.\n\nIlya Mironov. R ́enyi differential privacy. In IEEE Computer Security Foundations Symposium (CSF),\n\npp. 263–275, 2017.\n\nIlya Mironov, Kunal Talwar, and Li Zhang. R ́enyi differential privacy of the sampled gaussian mechanism. CoRR, abs/1908.10530, 2019. URL http://arxiv.org/abs/1908.10530.\n\nKaan Ozkara, Navjot Singh, Deepesh Data, and Suhas Diggavi. Quped: Quantized personalization via distillation with applications to federated learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nKaan Ozkara, Antonious M Girgis, Deepesh Data, and Suhas Diggavi. A generative framework for personalized learning and estimation: Theory, algorithms, and privacy. arXiv preprint arXiv:2207.01771, 2022, July.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? towards understanding the effectiveness of MAML. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkgMkCEtPB.\n\nHerbert Robbins. An empirical bayes approach to statistics. In The Proceedings of Third Berkeley\n\nSymposium on Mathematical Statistics and Probability, pp. 157–163, 1956.\n\nTao Shen, Jie Zhang, Xinkang Jia, Fengda Zhang, Gang Huang, Pan Zhou, Kun Kuang, Fei Wu, and\n\nChao Wu. Federated mutual learning. arXiv preprint arXiv:2006.16765, 2020.\n\nVirginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S. Talwalkar. Federated multi-task\n\nlearning. In Advances in Neural Information Processing Systems, pp. 4424–4434, 2017.\n\nCharles Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, 1954– 1955, vol. I, pp. 197–206. University of California Press, Berkeley-Los Angeles, Calif., 1956.\n\nUri Stemmer. Locally private k-means clustering. In SODA, pp. 548–559, 2020.\n\nKevin Tian, Weihao Kong, and Gregory Valiant. Learning populations of parameters. Advances in\n\nneural information processing systems, 30, 2017.\n\nYonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking few-shot image classification: a good embedding is all you need? In European Conference on Computer Vision, pp. 266–282. Springer, 2020.\n\nPaul Vanhaesebrouck, Aur ́elien Bellet, and Marc Tommasi. Decentralized collaborative learning of personalized models over networks. In Artificial Intelligence and Statistics, pp. 509–517. PMLR, 2017.\n\nRamya Korlakai Vinayak, Weihao Kong, Gregory Valiant, and Sham Kakade. Maximum likelihood In International Conference on Machine\n\nestimation for learning populations of parameters. Learning, pp. 6448–6457. PMLR, 2019.\n\nMartin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-\n\nbridge University Press, 2019.\n\nValentina Zantedeschi, Aur ́elien Bellet, and Marc Tommasi. Fully decentralized joint learning of personalized models and collaboration graphs. In International Conference on Artificial Intelligence and Statistics, pp. 864–874. PMLR, 2020.\n\nMichael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M. Alvarez. Personalized federated learning with first order model optimization. In International Conference on Learning Representations, 2021.\n\nYuchen Zhang. Distributed machine learning with communication constraints. PhD thesis, EECS Department, University of California, Berkeley, May 2016. URL http://www2.eecs. berkeley.edu/Pubs/TechRpts/2016/EECS-2016-47.html.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA PRELIMINARIES\n\nWe give standard privacy definitions that we use in Section A.1, some existing results on RDP to DP conversion and RDP composition in Section A.2, and user-level differential privacy in Section A.3.\n\nA.1 PRIVACY DEFINITIONS\n\nIn this subsection, we define different privacy notions that we will use in this paper: local differential privacy (LDP), central different privacy (DP), and Renyi differential privacy (RDP), and their userlevel counterparts. Definition 1 (Local Differential Privacy - LDP (Kasiviswanathan et al., 2011)). For (cid:15)0 ≥ 0, a randomized mechanism R : X → Y is said to be (cid:15)0-local differentially private (in short, (cid:15)0-LDP), if for every pair of inputs d, d(cid:48) ∈ X , we have\n\nPr[R(d) ∈ S] ≤ e(cid:15)0 Pr[R(d(cid:48)) ∈ S],\n\n∀S ⊂ Y.\n\n(12)\n\n1, . . . , x(cid:48)\n\nLet D = {x1, . . . , xn} denote a dataset comprising n points from X . We say that two datasets n} are neighboring (and denoted by D ∼ D(cid:48)) if they differ D = {x1, . . . , xn} and D(cid:48) = {x(cid:48) in one data point, i.e., there exists an i ∈ [n] such that xi (cid:54)= x(cid:48) i and for every j ∈ [n], j (cid:54)= i, we have xj = x(cid:48) j. Definition 2 (Central Differential Privacy - DP (Dwork et al., 2006; Dwork & Roth, 2014)). For (cid:15), δ ≥ 0, a randomized mechanism M : X n → Y is said to be ((cid:15), δ)-differentially private (in short, ((cid:15), δ)-DP), if for all neighboring datasets D ∼ D(cid:48) ∈ X n and every subset S ⊆ Y, we have\n\nPr [M(D) ∈ S] ≤ e(cid:15)0 Pr [M(D(cid:48)) ∈ S] + δ.\n\n(13)\n\nIf δ = 0, then the privacy is referred to as pure DP. Definition 3 ((λ, (cid:15)(λ))-RDP (Renyi Differential Privacy) (Mironov, 2017)). A randomized mechanism M : X n → Y is said to have (cid:15)(λ)-Renyi differential privacy of order λ ∈ (1, ∞) (in short, (λ, (cid:15)(λ))-RDP), if for any neighboring datasets D ∼ D(cid:48) ∈ X n, the Renyi divergence between M(D) and M(D(cid:48)) is upper-bounded by (cid:15)(λ), i.e.,\n\nDλ(M(D)||M(D(cid:48))) =\n\n1 λ − 1\n\n(cid:32)\n\nlog\n\nEθ∼M(D(cid:48))\n\n(cid:34)(cid:18) M(D)(θ) M(D(cid:48))(θ)\n\n(cid:19)λ(cid:35)(cid:33)\n\nwhere M(D)(θ) denotes the probability that M on input D generates the output θ. For convenience, instead of (cid:15)(λ) being an upper bound, we define it as (cid:15)(λ) = supD∼D(cid:48) Dλ(M(D)||M(D(cid:48))).\n\n≤ (cid:15)(λ),\n\nA.2 RDP TO DP CONVERSION AND RDP COMPOSITION\n\nAs mentioned after Theorem 5, we can convert the RDP guarantees of DP-AdaPeD to its DP guarantees using existing conversion results from literature. To the best of our knowledge, the following gives the best conversion. Lemma 1 (From RDP to DP (Canonne et al., 2020; Balle et al., 2020)). Suppose for any λ > 1, a mechanism M is (λ, (cid:15) (λ))-RDP. Then, the mechanism M is ((cid:15), δ)-DP, where (cid:15), δ are define below:\n\nFor a given δ ∈ (0, 1) :\n\n(cid:15) = min\n\nλ\n\n(cid:15) (λ) +\n\nlog (1/δ) + (λ − 1) log (1 − 1/λ) − log (λ) λ − 1\n\nFor a given (cid:15) > 0 :\n\nδ = min\n\nλ\n\nexp ((λ − 1) ((cid:15) (λ) − (cid:15))) λ − 1\n\n(cid:18)\n\n1 −\n\n(cid:19)λ\n\n.\n\n1 λ\n\nThe main strength of RDP in comparison to other privacy notions comes from composition. The following result states that if we adaptively compose two RDP mechanisms with the same order, their privacy parameters add up in the resulting mechanism. Lemma 2 (Adaptive composition of RDP (Mironov, 2017, Proposition 1)). For any λ > 1, let M1 : X → Y1 be a (λ, (cid:15)1(λ))-RDP mechanism and M2 : Y1 × X → Y be a (λ, (cid:15)2(λ))-RDP mechanism. Then, the mechanism defined by (M1, M2) satisfies (λ, (cid:15)1(λ) + (cid:15)2(λ))-RDP.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA.3 USER-LEVEL DIFFERENTIAL PRIVACY LEVY ET AL. (2021)\n\nConsider a set of m users, each having a local dataset of n samples. Let Di = {xi1, . . . , xin} denote the local dataset at the i-th user for i ∈ [m], where xij ∈ X and X ⊂ Rd. We define D = (D1, . . . , Dm) ∈ (X n)m as the entire dataset.\n\nWe have already defined DP, LDP, and RDP in Section A.1 w.r.t. the item-level privacy. Here, we extend those definition w.r.t. the user-level privacy. In order to do that, we need a generic neighborhood relation between datasets: We say that two datasets D, D(cid:48) are neighboring with respect to distance metric dis if we have dis(D, D(cid:48)) ≤ 1. Item-level DP/RDP vs. User-level DP/RDP. By choosing dis(D, D(cid:48)) = (cid:80)m ij}, we recover the standard definition of the DP/RDP from Definitions 2, 3, which we call item-level DP/RDP. In the item-level DP/RDP, two datasets D, D(cid:48) are neighboring if they differ in a single item. On the other hand, by choosing dis(D, D(cid:48)) = (cid:80)m i}, we call it user-level DP/RDP, where two datasets D, D(cid:48) ∈ (X n)m are neighboring when they differ in a local dataset of any single user. Observe that when each user has a single item (n = 1), then both item-level and user-level privacy are equivalent.\n\n1{Di (cid:54)= D(cid:48)\n\n1{xij (cid:54)= x(cid:48)\n\n(cid:80)n\n\nj=1\n\ni=1\n\ni=1\n\nUser-level Local Differential Privacy (LDP). When we have a single user (i.e., m = 1 and D = X n), by choosing dis (D, D(cid:48)) = 1{D (cid:54)= D(cid:48)} for D, D(cid:48) ∈ X n, we call it user-level LDP. In this case each user privatize her own local dataset using a private mechanism.\n\nWe can define user-level LDP/DP/RDP analogously to their item-level counterparts using the neighborhood relation dis defined above.\n\nB PERSONALIZED ESTIMATION – GAUSSIAN MODEL\n\nB.1 PROOF OF THEOREM 1\n\nWe will derive the optimal estimator and prove the MSE for one dimensional case, i.e., for d = 1; the final result can be obtained by applying these to each of the d coordinates separately.\n\nThe posterior estimators of the local means θ1, . . . , θm and the global mean μ are obtained by solving the following optimization problem:\n\nˆθ1, . . . , ˆθm, ˆμ = arg max\n\nθ1,...,θm,μ\n\n= arg min θ1,...,θm,μ\n\npX|θ (X1, . . . , Xm|θ1, . . . , θm) pθ|μ(θ1, . . . , θm|μ)\n\n− log (cid:0)pX|θ (X1, . . . , Xm|θ1, . . . , θm)(cid:1) − log (cid:0)pθ|μ(θ1, . . . , θm|μ)(cid:1)\n\n= arg min θ1,...,θm,μ\n\nC +\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n(cid:17)2\n\n(cid:16)\n\nX j\n\ni − θi σ2 x\n\n+\n\nm (cid:88)\n\ni=1\n\n(θi − μ)2 σ2 θ\n\n,\n\nwhere the second equality is obtained from the fact that the log function is a monotonic function, and C is a constant independent of the variables θ = (θ1, . . . , θm). Observe that the objective function F (θ, μ) = (cid:80)m obtained by setting the derivative to zero as it is an unbounded optimization problem.\n\nis jointly convex in (θ, μ). Thus, the optimal is\n\n(θi−μ)2 σ2 θ\n\ni −θi)2 σ2 x\n\n+ (cid:80)m\n\n(cid:80)n\n\n(X j\n\nj=1\n\ni=1\n\ni=1\n\n∂F ∂θi\n\n∂F ∂μ\n\n(cid:12) (cid:12) (cid:12) (cid:12)μ=ˆμ,θi=ˆθi (cid:12) (cid:12) (cid:12) (cid:12)μ=ˆμ,θi=ˆθi\n\n=\n\n=\n\n(cid:80)n\n\n(cid:80)m\n\nj=1 2(ˆθi − X j i ) σ2 x\ni=1 2(ˆμ − ˆθi) σ2 θ\n\n= 0.\n\n+\n\n2(ˆθi − ˆμ) σ2 θ\n\n= 0,\n\n∀i ∈ [m]\n\nBy solving these m + 1 equations in m + 1 unknowns, we get: \n\n\n\n\n\nˆθi = α\n\n\n\nX j\n\ni\n\n + (1 − α)\n\n\n\nn (cid:88)\n\n1 n\n\nm (cid:88)\n\nn (cid:88)\n\n\n\nX j\n\ni\n\n ,\n\n(14)\n\n1 mn\n\ni=1\n\nj=1\n\nj=1\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nθ\n\nwhere α = σ2 σ2 θ + (cid:80)m i=1 X i.\n\nα) 1 m\n\nσ2 x\nn\n\n. By letting X i = 1\n\nn\n\n(cid:80)n\n\nj=1 X j\n\ni for all i ∈ [m], we can write ˆθi = αX i + (1 −\n\nObserve that E an unbiased estimate of {θi}. Substituting the ˆθi in the MSE, we get that\n\n= αθi + 1−α\n\nm\n\nl=1 θl, where θ = (θ1, . . . , θm). Thus, the estimator (14) is\n\n(cid:80)m\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\nEX1,...,Xm\n\n(cid:20)(cid:16)ˆθi − θi\n\n(cid:17)2(cid:21)\n\n= Eθ\n\n(cid:20) EX1,...,Xm\n\n= Eθ\n\n(cid:20) EX1,...,Xm\n\n= Eθ\n\n(cid:20) EX1,...,Xm\n\n(cid:20)(cid:16)ˆθi − θi (cid:20)(cid:16)ˆθi − E (cid:20)(cid:16)ˆθi − E\n\n(cid:21)(cid:21)\n\n(cid:17)2\n\n|θ\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n+ E\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n(cid:105)(cid:17)2\n\n(cid:104)ˆθi|θ\n\n|θ\n\n(cid:21)(cid:21)\n\n+ Eθ\n\n(cid:20) EX1,...,Xm\n\n(cid:20)(cid:16)\n\nE\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n(15)\n\nClaim 1. (cid:20) EX1,...,Xm\n\nEθ\n\n(cid:20)(cid:16)ˆθi − E\n\n(cid:104)ˆθi|θ\n\n(cid:105)(cid:17)2\n\n|θ\n\n(cid:21)(cid:21)\n\n= α2 σ2\n\nx n\n\n+ (1 − α)2 σ2\n\nx mn\n\n+ 2α(1 − α)\n\nσ2 x\nmn\n\n(cid:20) EX1,...,Xm\n\nEθ\n\n(cid:20)(cid:16)\n\nE\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n\n\n(cid:32)\n\n= (1 − α)2Eθ\n\n\n\n1 m\n\nm (cid:88)\n\nk=1\n\nθk − θi\n\n(cid:33)2\n\n ≤ (1 − α)2 σ2\n\nθ (m − 1) m\n\nProof. For the first equation:\n\n(cid:20) EX1,...,Xm\n\n(cid:20)(cid:16)ˆθi − E\n\n(cid:104)ˆθi|θ\n\nEθ\n\n(cid:105)(cid:17)2\n\n|θ\n\n(cid:21)(cid:21)\n\n= Eθ\n\n EX1,...,Xm\n\n\n\n(cid:32)\n\n\n\nα(X i − θi) + (1 − α)\n\n(cid:33)2\n\n\n\n\n\n(X k − θk)\n\n| θ\n\n\n\n\n\n1 m\n\nm (cid:88)\n\nk=1\n\n= α2E (cid:2)E (cid:2)(X i − θi)2 | θ(cid:3)(cid:3) + (1 − α)2E\n\n E\n\n\n\n(cid:32)\n\n\n\n1 m\n\nm (cid:88)\n\n(cid:33)2\n\n\n\n\n\n(X k − θk)\n\n| θ\n\n\n\n\n\nk=1\n\n(cid:34)\n\n(cid:34)\n\n+ 2α(1 − α)E\n\nE\n\n1 m\n\n= α2 σ2\n\nx n\n\n+ (1 − α)2 σ2\n\nx mn\n\n+ 2α(1 − α)\n\nFor the second equation, first note that E (cid:1): α) (cid:0) 1\n\nk=1 θk − θi\n\n(cid:80)m\n\nm\n\nm (cid:88)\n\n(X i − θi)(X k − θk) | θ\n\n(cid:35)(cid:35)\n\nk=1 σ2 x\nmn\n\n(cid:104)ˆθi|θ\n\n(cid:105)\n\n− θi = αθi + 1−α\n\nm\n\n(cid:80)m\n\nk=1 θk − θi = (1 −\n\n(cid:20) EX1,...,Xm\n\nEθ\n\n(cid:20)(cid:16)\n\nE\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n= (1 − α)2E\n\n=\n\n(1 − α)2 m2\n\nE\n\n\n\n\n\n \n\n\n\n2\n\n\n\n\n\n \n\n(cid:88)\n\n(θk − θi)\n\nk(cid:54)=i\n\n\n\n(cid:32)\n\n\n\n1 m\n\nm (cid:88)\n\nk=1\n\n(cid:33)2 \n\nθk − θi\n\n\n\n(cid:88)\n\n\n\nk(cid:54)=i\n\n\n\n(cid:88)\n\n\n\n=\n\n(1 − α)2 m2\n\n≤\n\n(1 − α)2 m2\n\n=\n\n(1 − α)2 m2\n\nE(θk − θi)2 +\n\n(cid:88)\n\nE(θk − θi)(θl − θi)\n\n\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n\n\n[E(θk − μ)2 + E(θi − μ)2] +\n\nk(cid:54)=i\n\n 2(m − 1)σ2\n\nθ +\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n\n\n(cid:88)\n\nE(θk − θi)(θl − θi)\n\n\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n16\n\n(cid:88)\n\nE(θk − θi)(θl − θi)\n\n\n\n\n\nPublished as a conference paper at ICLR 2023\n\n=\n\n=\n\n(1 − α)2 m2\n\n(1 − α)2 m2\n\n 2(m − 1)σ2\n\nθ +\n\n(cid:88)\n\nE(μ − θi)2\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n  (Since E[θk] = μ for all k ∈ [m])\n\n(cid:2)2(m − 1)σ2\n\nθ + (m − 1)(m − 2)σ2\n\nθ\n\n(cid:3)\n\n= (1 − α)2 σ2\n\nθ (m − 1) m\n\nThis concludes the proof of Claim 1.\n\nSubstituting the result of Claim 1 into (15), we get (cid:17)2(cid:21)\n\n≤ α2 σ2\n\n+ (1 − α)2 σ2\n\n(cid:20)(cid:16)ˆθi − θi\n\nEX1,...,Xm\n\nx n\n(cid:18)\n\n+ 2α(1 − α)\n\nσ2 x\nmn\n\n+ (1 − α)2 σ2\n\nθ (m − 1) m\n\n(16)\n\nx mn\n\n(a)=\n\nσ2 x\nn σ2 x\nn where in (a) we used α = σ2 θ +\n\n=\n\nσ2 x\nn\n\nσ2\n\nθ\n\nα2 +\n\n(1 − α)2 + 2α(1 − α) m\n\n+ α(1 − α)\n\n(cid:19)\n\nm − 1 m\n\n(cid:18)\n\nα +\n\n(cid:19)\n\n,\n\n1 − α m\n\nfor the last term to write (1 − α)2 σ2\n\nθ (m−1) m\n\n= σ2\n\nx\n\nn α(1 − α) m−1 m .\n\nn\n\n(cid:80)n\n\nj=1 X j\n\ni , and the global estimator ˆμ = 1\n\nObserve that the estimator in (14) is a weighted summation between two estimators: the local estimator X i = 1 i=1 X i. Thus, the MSE in (a) consists of four terms: 1) The variance of the local estimator ( σ2 n ). 2) The variance of the global estimator ( σ2 nm ). 4) The (cid:20) EX1,...,Xm\n\nnm ). 3) The correlation between the local estimator and the global estimator ( σ2\n\n. This completes the proof of Theorem 1.\n\nbias term Eθ\n\n(cid:104)ˆθi|θ\n\n− θi\n\n(cid:80)m\n\n(cid:20)(cid:16)\n\n(cid:21)(cid:21)\n\n(cid:17)2\n\n|θ\n\nE\n\nm\n\n(cid:105)\n\nx\n\nx\n\nx\n\nB.2 PROOF OF THEOREM 2, EQUATION (5)\n\nSimilar to the proof of Theorem 1, here also we will derive the optimal estimator and prove the MSE for the one dimensional case, and the final result can be obtained by applying these to each of the d coordinates separately.\n\nLet θ = (θ1, . . . , θm) denote the personalized models vector. For given a constraint function q, we set the personalized model as follows: \n\n\n\nˆθi = α\n\n\n\nX j\n\ni\n\n + (1 − α)\n\nq(X i)\n\n∀i ∈ [m],\n\n(17)\n\n(cid:33)\n\n(cid:32)\n\n1 m\n\nm (cid:88)\n\ni=1\n\n1 n\n\nn (cid:88)\n\nj=1\n\nwhere X i = 1\n\nn\n\n(cid:80)n\n\nj=1 X j\n\ni . From the second condition on the function q, we get that\n\nE\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n= αθi +\n\n1 − α m\n\nm (cid:88)\n\nl=1\n\nθl,\n\n(18)\n\nThus, by following similar steps as the proof of Theorem 1, we get that:\n\n(cid:20)(cid:16)ˆθi − θi\n\nE\n\n(cid:17)2(cid:21)\n\n= E\n\n= E\n\n= E\n\n(cid:20)\n\nE\n\n(cid:20)\n\nE\n\n(cid:20)\n\nE\n\n(cid:20)(cid:16)ˆθi − θi (cid:20)(cid:16)ˆθi − E (cid:20)(cid:16)ˆθi − E\n\n(cid:21)(cid:21)\n\n(cid:17)2\n\n|θ\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n+ E\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n(cid:105)(cid:17)2\n\n(cid:104)ˆθi|θ\n\n|θ\n\n(cid:21)(cid:21)\n\n(cid:20)(cid:16)\n\n(cid:20)\n\nE\n\n+ E\n\nE\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n(a)\n\n= α2 σ2\n\nx n\n\n+ (1 − α)2E\n\n\n\n(cid:32)\n\n\n\n1 m\n\nm (cid:88)\n\nl=1\n\n(cid:33)2\n\n\n\n|θ\n\n\n\nq(X l) − θl\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n+ 2α(1 − α)E\n\n(cid:34) (cid:0)X i − θi\n\n(cid:1)\n\n(cid:32)\n\n1 m\n\nm (cid:88)\n\nl=1\n\n(cid:33)\n\n(cid:35)\n\nq(X l) − θl\n\n|θ\n\n+ (1 − α)2E\n\n\n\n(cid:32)\n\n\n\n1 m\n\nm (cid:88)\n\nk=1\n\n(cid:33)2 \n\nθk − θi\n\n(1 − α)2 (cid:16) σ2\n\nx\n\nn + σ2\n\nq\n\nm\n\n(cid:17)\n\n+\n\n2α(1 − α)σ2 x\nmn\n\n+ (1 − α)2E\n\n\n\n(cid:32)\n\n\n\n1 m\n\nm (cid:88)\n\nk=1\n\n(cid:33)2 \n\nθk − θi\n\n(1 − α)2 (cid:16) σ2\n\nx\n\nn + σ2\n\nq\n\nm\n\n(cid:17)\n\n+ 2α(1 − α)\n\nσ2 x\nmn\n\n+ (1 − α)2 σ2\n\nθ (m − 1) m\n\nα2 +\n\n(1 − α)2 + 2α(1 − α) m\n\n+ α(1 − α)\n\nm − 1 m\n\n(cid:19)\n\n(cid:18)\n\nα +\n\n(cid:19)\n\n,\n\n1 − α m\n\n(19)\n\n+\n\n+\n\n(b)\n\n= α2 σ2\n\nx n\n\n≤ α2 σ2\n\nx n\n(cid:18)\n\n(c) =\n\n=\n\nσ2 x\nn σ2 x\nn\n\nwhere step (a) follows by substituting the expectation of the personalized model from (18). Step (b) follows from the first and third conditions of the function q. Step (c) follows by choosing\n\nα =\n\nσ2\n\nθ +\n\nσ2 q\nm−1\n\nσ2\n\nθ +\n\nσ2 q\nm−1 +\n\nσ2 x\nn\n\n. This derives the result stated in (5) in Theorem 2.\n\nB.2.1 PROOF OF THEOREM 2, PART 1\n\nThe proof consists of two steps. First, we use the concentration property of the Gaussian distribution to show that the local sample means {X i} are bounded within a small range with high probability. Second, we apply an unbiased stochastic quantizer on the projected sample mean.\n\ni , . . . , X n\n\nThe local samples X 1 variance σ2\n\nx, and hence, we have that X i ∼ N (θi, σ2\n\ni are drawn i.i.d. from a Gaussian distribution with mean θi and n ). Thus, from the concentration property − nc2 of the Gaussian distribution, we get that Pr[|X i − θi| > c1] ≤ exp for all i ∈ [m]. Similarly, the models θ1, . . . , θm are drawn i.i.d. from a Gaussian distribution with mean μ ∈ [−r, r] and variance σ2 for all i ∈ [m]. Let E = (cid:8)X i ∈ [−a, a] : ∀i ∈ [m](cid:9), where a = r + c1 + c2. Thus, from the union bound, we get that\n\nθ , hence,, we get Pr[|θi − μ| > c2] ≤ exp\n\n− c2\n\n1 σ2 x\n\n2 σ2 θ\n\n(cid:17)\n\n(cid:17)\n\n(cid:16)\n\n(cid:16)\n\nx\n\nPr[E] > 1 − m(e that a = r + σx√\n\nn\n\n−\n\nnc2 1\nσ2\n\nx + e\n\n−\n\nc2 2\nσ2 θ ). By setting c1 =\n\n(cid:113) σ2\n\n(cid:112)log(m2n) + σθ\n\n(cid:112)log(m2n), and Pr[E] = 1 − 2\n\nx\n\nn log(m2n) and c2 = (cid:112)σ2 mn .\n\nθ log(m2n), we get\n\nLet qk : [−a, a] → Yk be a quantization function with k-bits, where Yk is a discrete set of cardinality |Yk| = 2k. For given x ∈ [−a, a], the output of the function qk is given by:\n\nqk(x) =\n\n2a 2k − 1\n\n((cid:98) ̃x(cid:99) + Bern ( ̃x − (cid:98) ̃x(cid:99))) − a,\n\n(20)\n\nwhere Bern(p) is a Bernoulli random variable with bias p, and ̃x = 2k−1 2a (x + a) ∈ [0, 2k − 1]. Observe that the output of the function qk requires only k-bits for transmission. Furthermore, the function qk satisfies the following conditions:\n\nE [qk(x)] = x,\n\nσ2 qk\n\n= E (cid:2)(qk(x) − x)2(cid:3) ≤\n\na2 (2k − 1)2 .\n\n(21)\n\n(22)\n\n(cid:3) and sends Let each client applies the function qk on the projected local mean ̃Xi = Proj[−a,a] the output to the server for all i ∈ [m]. Conditioned on the event E, i.e., X i ∈ [−a, a] ∀i ∈ [m], and using (19), we get that\n\n(cid:2)X i\n\nM SE = Eθ,X\n\n(cid:20)(cid:16)ˆθi − θi\n\n(cid:17)2(cid:21)\n\n≤\n\nσ2 x\nn\n\n(cid:18) 1 − α m\n\n(cid:19)\n\n+ α\n\n,\n\n(23)\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nwhere α =\n\nσ2\n\nθ +\n\na2 (2k −1)2(m−1)\n\nσ2\n\nθ +\n\na2 (2k −1)2(m−1)\n\n+\n\nσ2 x\nn\n\nand a = r + σx√\n\nn\n\n(cid:112)log(m2n) + σθ\n\n(cid:112)log(m2n). Note that the event\n\nE happens with probability at least 1 − 2\n\nmn .\n\nB.2.2 PROOF OF THEOREM 2, PART 2\n\nWe define the (random) mechanism qp : [−a, a] → R that takes an input x ∈ [−a, a] and generates a user-level ((cid:15)0, δ)-LDP output y ∈ R, where y = qp(x) is given by:\n\nqp(x) = x + ν,\n\n(24)\n\nwhere ν ∼ N (0, σ2 , we get that the output of the (cid:15)0 function qp(x) is ((cid:15)0, δ)-LDP from Dwork & Roth (2014). Furthermore, the function qp satisfies the following conditions:\n\n) is a Gaussian noise. By setting σ2 (cid:15)0\n\n(cid:15)2 0\n\n= 8a2 log(2/δ)\n\nE [qp(x)] = x,\n\nσ2 qp\n\n= E (cid:2)(qp(x) − x)2(cid:3) ≤\n\n8a2 log(2/δ) (cid:15)2 0\n\n.\n\n(25)\n\n(26)\n\nSimilar to the proof of Theorem 2, Part 1, let each client applies the function qp on the projected local (cid:3) and sends the output to the server for all i ∈ [m]. Conditioned on the mean ̃Xi = Proj[−a,a] event E, i.e., X i ∈ [−a, a] ∀i ∈ [m], and using (19), we get that\n\n(cid:2)X i\n\nMSE = Eθ,X\n\n(cid:20)(cid:16)ˆθi − θi\n\n(cid:17)2(cid:21)\n\n≤\n\nσ2 x\nn\n\n(cid:18) 1 − α m\n\n(cid:19)\n\n+ α\n\n,\n\n(27)\n\nθ + 8a2 log(2/δ) σ2\n\n(cid:15)2\n\nwhere α =\n\n0(m−1) θ + 8a2 log(2/δ) σ2 0(m−1) happens with probability at least 1 − 2\n\nσ2 x\nn\n\n(cid:15)2\n\n+\n\nand a = r + σx√\n\nn\n\nmn .\n\n(cid:112)log(m2n) + σθ\n\n(cid:112)log(m2n). Note that the event E\n\nRemark 4 (Privacy with communication efficiency). Note that our private estimation algorithm for the Gaussian case adds Gaussian noise (which is a real number) but that can also be made communication-efficient by alternatively adding a discrete Gaussian noise (Canonne et al., 2020).\n\nB.3 LOWER BOUND\n\nHere we discuss the lower bound using Fisher information technique similar to Barnes et al. (2020). In particular we use a Bayesian version of Cramer-Rao lower bound and van Trees inequality Gill & Levit (1995). Let us denote f (X|θ) as the data generating conditional density function and π(θ) as the prior distribution that generates θ. Let us denote Eθ as the expectation with respect to the randomness of θ and E as the expectation with respect to randomness of X and θ. First we define two types of Fisher information:\n\nIX (θ) = Eθ∇θ log(f (X|θ))∇θ log(f (X|θ))T\n\nI(π) = E∇θ log(π(θ))∇θ log(π(θ))T\n\nnamely Fisher information of estimating θ from samples X and Fisher information of prior π. Here the logarithm is elementwise. For van Trees inequality we need the following regularity conditions:\n\n• f (X|·) and π(·) are absolutely continuous and π(·) vanishes at the end points of Θ. • Eθ∇θ log(f (X|θ)) = 0 • We also assume both density functions are continuously differentiable.\n\nThese assumptions are satisfied for the Gaussian setting for any finite mean μ, they are satisfied for Bernoulli setting as long as parameters α and β are larger than 1. Assuming local samples X are generated i.i.d with f (x|θ), the van Trees inequality for one dimension is as follows:\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nE((cid:98)θ(X) − θ)2 ≥\n\n1 nEIx(θ) + I(π)\n\nwhere IX (θ) = Eθ log(f (X|θ))(cid:48)2 and I(π) = E log(π(θ))(cid:48)2. Assuming θ ∈ Rd and each dimension is independent from each other, by Gill & Levit (1995) we have:\n\nE(cid:107)(cid:98)θ(X) − θ(cid:107)2 ≥\n\nd2 nETr(Ix(θ)) + Tr(I(π))\n\n(28)\n\nNote, the lower bound on the average risk directly translates as a lower bound on supθ∈Θ θ(cid:107)2. Before our proof we have a useful fact:\n\nEX (cid:107)(cid:98)θ(X)−\n\nFact 1. Given some random variable X ∼ N (Y, σ2 σ2\n\ny).\n\ny) where Y ∼ N (Z, σ2\n\nz ) we have X ∼ N (z, σ2\n\nz +\n\nProof. We will give the proof in one dimension, however, it can easily be extended to multidimensional case where each dimension is independent. For all t ∈ R we have,\n\nEX [exp(itX)] = EY EX [exp(itX)|Y ] = EY [exp(itY −\n\nxt2 σ2 2\n\n)]\n\n= exp(−\n\n= exp(−\n\nσ2 xt2 2\nxt2 σ2 2\n\n)EY [exp(itY )]\n\n) exp(itz −\n\n= exp(itz −\n\ny)t2\n\n(σ2\n\nx + σ2 2\n\nσ2\n\nyt2 2\n\n)\n\n)\n\nwhere the last line is the characteristic function of a Gaussian with mean z and variance σ2\n\nx + σ2 y.\n\nGaussian case with perfect knowledge of prior. In this setting we know that θi ∼ N (μ1, σ2 hence, I(π) = 1 Id. Then, σ2 θ\n\nId, similarly IX (θ) = 1\n\nσ2 x\n\nθ Id),\n\nE(cid:107) (cid:98)θi(X) − θi(cid:107)2 ≥\n\nsup θi\n\nd2\n\nnE d σ2 x\n\n+ d σ2 θ\n\n=\n\ndσ2\n\nx\n\nθ σ2 θ + σ2\n\nx\n\nnσ2\n\n(29)\n\nmn\n\n(cid:80)m,n\n\ni,j X j\n\nGaussian case with estimated population mean. In this setting instead of a true prior we have a prior whose mean is the average of all data spread across clients, i.e., we assume θi ∼ N ((cid:98)μ, σ2 θ Id) where (cid:98)μ = 1 i |θj ∼ N (θj, σ2 θ Id). While the true prior is parameterized with mean μ, θi in this form is not parameterized by μ but by (cid:98)μ which itself has randomness due X j i . However, using Fact 1 θ + σ2 twice we can write θi ∼ N (μ, (σ2 mn )Id). Then using the van Trees inequality similar to the lower bound in perfect case we can obtain:\n\ni . We additionally know that there is a Markov relation such that X j\n\nxId) and θj ∼ N (μ, σ2\n\nm + σ2\n\nx\n\nθ\n\nσ2 θ σ2 nσ2\n\nx mn\n\nx + σ4 θ + σ2\n\nx\n\nEX (cid:107)(cid:98)θi(X) − θi(cid:107)2 ≥ d\n\nsup θi∈Θ\n\n20\n\n(30)\n\nPublished as a conference paper at ICLR 2023\n\nC PERSONALIZED ESTIMATION – BERNOULLI MODEL\n\nC.1 WHEN α, β ARE KNOWN\n\nAnalogous to the Gaussian case, we can show that if α, β are known, then the posterior mean estimator has a closed form expression: (cid:98)pi = aX i + (1 − a) α α+β (where a = n/α+β+n) and achieves the MSE: Epi∼πE α+β+n . We show this below.\n\n(cid:98)pi,X1,...,Xm ((cid:98)pi − pi)2 ≤\n\nαβ n(α+β)(α+β+1)\n\nn\n\nFor a client i, let π(pi) be distributed as Beta(α, β). In this setting, we model that each client generates local samples according to Bern(pi). Consequently, each client has a Binomial distribution regarding the sum of local data samples. Estimating Bernoulli parameter pi is related to Binomial distribution Bin(n, pi) (the sum of data samples) Zi since it is the sufficient statistic of Bernoulli distribution. The distribution for Binomial variable Zi given pi is P (Zi = zi|pi) = (cid:0) n i (1 − pi)n−zi. It is a known fact that for any prior, the Bayesian MSE risk minimizer is the posterior mean E [pi|Zi = zi].\n\n(cid:1)pzi\n\nzi\n\nWhen pi ∼ Beta(α, β), we have posterior\n\nf (pi|Zi = zi) =\n\n=\n\n=\n\nπ(pi)\n\nP (zi|pi) P (zi) (cid:1)pzi\n\n(cid:0) n zi\n\ni (1 − pi)n−zi\n\nP (zi)\n\npα−1\n\ni\n\n(1 − pi)β−1 B(α, β)\n\n(cid:1)\n\n(cid:0) n zi P (zi)\n\nB(α + zi, β + n − zi) B(α, β)\n\npα+zi−1\n\ni\n\n(1 − pi)β+n−zi−1\n\nB(α + zi, β + n − zi)\n\n,\n\nwhere B(α, β) = Γ(α)Γ(β)\n\nΓ(α+β) , and\n\n(cid:90)\n\nP (zi) =\n\nP (zi|pi)π(pi)dpi\n\n=\n\n=\n\npα−1\n\n(cid:19)\n\npzi i (1 − pi)n−zi\n\n(cid:90) (cid:18)n zi (cid:19) B(zi + α, n − zi + β) B(α, β)\n\n(cid:18)n zi\n\ni\n\n(1 − pi)β−1 B(α, β) (cid:90) pα+zi−1\n\ni\n\ndpi\n\n(1 − pi)β+n−zi−1\n\n(cid:124)\n\nB(α + zi, β + n − zi) (cid:123)(cid:122) integral of a Beta distribution\n\ndpi\n\n(cid:125)\n\n=\n\n(cid:18)n zi\n\n(cid:19) B(zi + α, n − zi + β) B(α, β)\n\nThus, we get that the posterior distribution f (pi|Zi = zi) = pα+zi−1 distribution Beta(zi + α, n − zi + β). As a result, the posterior mean is given by:\n\ni\n\nB(α+zi,β+n−zi)\n\n(1−pi)β+n−zi−1\n\n(cid:98)pi =\n\nα + Zi α + β + n α+β+n . Observe that Epi∼Beta(α,β)[pi] = α\n\n= a\n\n(cid:18) Zi n\n\n(cid:19)\n\n+ (1 − a)\n\n(cid:18) α\n\nα + β\n\n(cid:19)\n\n,\n\nwhere a = n tion between the local estimator zi We have Rpi ((cid:98)pi) = EπE((cid:98)pi − pi)2. The MSE of the posterior mean is given by:\n\nn and the global estimator μ = α\n\nα+β .\n\nα+β , i.e., the estimator is a weighted summa-\n\nis a beta\n\n(31)\n\nMSE = E[(ˆpi − pi)2] (cid:20)(cid:16)\n\n= E\n\n(cid:17)\n\n+ (1 − a)(μ − pi)\n\n(cid:17)2(cid:21)\n\n− pi\n\n= a2E\n\n(cid:17)2(cid:21)\n\n− pi\n\n+ (1 − a)2E\n\n(cid:104)\n\n(μ − pi)2(cid:105)\n\na\n\n(cid:16) zi n\n(cid:20)(cid:16) zi n\n\n= a2Epi∼π(pi)\n\n(cid:21)\n\n(cid:20) pi(1 − pi) n\n\n+ (1 − a)2\n\nαβ (α + β)2(α + β + 1)\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n= a2\n\nαβ n(α + β)(α + β + 1) (cid:18)\n\nαβ n(α + β)(α + β + 1)\n\n=\n\nαβ (α + β)2(α + β + 1) (cid:19)\n\n+ (1 − a)2\n\nn α + β + n\n\n.\n\nThe last equality is obtained by setting a = n\n\nα+β+n .\n\nRemark 5. Note that X i := Zi n is the estimator based only on the local data and α/(α+β) is the true global mean, and (cid:98)pi = aX i + (1 − a) α α+β , where a = n/α+β+n (see (31)) is the estimator based on all the data. Observe that when n → ∞, then a → 1, which implies that (cid:98)pi → X i. Otherwise, when α + β is large (i.e., the variance of the beta distribution is small), then a → 0, which implies that (cid:98)pi → α/(α+β). Both these conclusions conform to the conventional wisdom as mentioned in the Gaussian case. It can be shown that the local estimate X i achieves the Bayesian risk of Epi∼Beta(α,β)EXi[(X i − pi)2] = Epi∼Beta(α,β)(pi(1−pi))/n = αβ/n(α+β)(α+β+1), which implies that the personalized estimation with perfect prior always outperforms the local estimate with a multiplicative gain a = n/(n+α+β) ≤ 1.\n\nC.2 WHEN α, β ARE UNKNOWN: PROOF OF THEOREM 3\n\nThe personalized model of the ith client with unknown parameters α, β is given by:\n\nˆpi = aiX i + (1 − ai) (ˆμi) ,\n\n(32)\n\n, the empirical mean ˆμi = 1\n\nm−1\n\n+n\n\n(cid:80)\n\nl(cid:54)=i X l, and the empirical variance\n\nl(cid:54)=i(X l − ˆμi)2. From (Tian et al., 2017, Lemma 1), with probability 1 − 1\n\nm2n , we get\n\nn\n\nˆμi(1− ˆμi) ˆσ2 i\n\nwhere ai =\n\n(cid:80)\n\ni = 1\n\nm−2\n\nˆσ2 that\n\n|μ − ˆμi| ≤\n\n|σ2 − ˆσ2\n\ni | ≤\n\n(cid:114) 3 log(4m2n) m − 1 (cid:114) 3 log(4m2n) m − 1\n\n,\n\nwhere μ = α\n\nα+β , σ2 =\n\nαβ\n\n(α+β)2(α+β+1) are the true mean and variance of the beta distribution,\n\nrespectively. Let c = ∀i ∈ [m]} that happens with probability at least 1 − 1\n\nm−1\n\n(cid:113) 3 log(4m2n)\n\nmn , we get that:\n\n. Conditioned on the event E = {|μ − ˆμi| ≤ c, |σ2 − ˆσ2\n\ni | ≤ c :\n\n(cid:104)\n\nE\n\n(ˆpi − pi)2 |Z−i\n\n(cid:105)\n\n= a2E\n\n(cid:34)(cid:18) Zi\n\nn\n\n(cid:19)2(cid:35)\n\n− pi\n\n+ (1 − a)2E\n\n(cid:104)\n\n(ˆμi − pi)2 |Z−i\n\n(cid:105)\n\n(cid:18)\n\n(cid:18)\n\n(cid:18)\n\n= a2\n\n= a2\n\n≤ a2\n\nαβ n(α + β)(α + β + 1)\n\nαβ n(α + β)(α + β + 1)\n\nαβ n(α + β)(α + β + 1)\n\n(cid:19)\n\n(cid:19)\n\n(cid:19)\n\n+ (1 − a)2 (cid:16)\n\nE\n\n(cid:104)\n\n(μ − pi)2(cid:105)\n\n+ (μ − ˆμi)2(cid:17)\n\n+ (1 − a)2\n\n+ (1 − a)2\n\n(cid:18)\n\n(cid:18)\n\nαβ (α + β)2(α + β + 1)\n\nαβ (α + β)2(α + β + 1)\n\n(cid:19)\n\n+ (μ − ˆμi)2\n\n(cid:19)\n\n,\n\n+ c2\n\nwhere the expectation is with respect to zi ∼ Binom(pi, n) and pi ∼ Beta(α, β) and Z−i = {z1, . . . , zi−1, zi+1, . . . , zm} denotes the entire dataset except the ith client data (zi). By taking the expectation with respect to the datasets Z−i, we get that the MSE is bounded by:\n\nMSE ≤ E (cid:2)a2(cid:3)\n\n(cid:18)\n\nαβ n(α + β)(α + β + 1)\n\n(cid:19)\n\n+E (cid:2)(1 − a)2(cid:3)\n\n(cid:18)\n\nαβ (α + β)2(α + β + 1)\n\n+\n\n3 log(4m2n) m − 1\n\n(cid:19)\n\n,\n\nwith probability at least 1 − 1\n\nmn . This completes the proof of Theorem 3.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nC.3 WITH PRIVACY CONSTRAINTS: PROOF OF THEOREM 4\n\nFirst, we prove some properties of the private mechanism qp. Observe that for any two inputs x, x(cid:48) ∈ [0, 1], we have that:\n\nPr[qp(x) = y] Pr[qp(x(cid:48)) = y]\n\n=\n\ne(cid:15)0\n\ne(cid:15)0 +1 − x e(cid:15)0 −1 e(cid:15)0 +1 − x(cid:48) e(cid:15)0 −1\n\ne(cid:15)0 +1\n\ne(cid:15)0 +1\n\ne(cid:15)0\n\n≤ e(cid:15)0,\n\n(33)\n\ne(cid:15)0 −1 . Similarly, we can prove (33) for the output y = e(cid:15)0\n\nfor y = −1 user-level (cid:15)0-LDP. Furthermore, for given x ∈ [0, 1], we have that\n\ne(cid:15)0 −1 . Thus, the mechanism qp is\n\nThus, the output of the mechanism qp is an unbiased estimate of the input x. From the Hoeffding’s inequality for bounded random variables, we get that:\n\nE [qp(x)] = x.\n\n(34)\n\nPr[|ˆμ(p)\n\ni − μ| > t] ≤ 2 exp\n\nPr[|ˆσ2(p)\n\ni − σ2| > t] ≤ 2 exp\n\n(cid:18) −3(e(cid:15)0 − 1)2(m − 1)t2 (e(cid:15)0 + 1)2 (cid:18) −3(e(cid:15)0 − 1)2(m − 1)t2 (e(cid:15)0 + 1)2\n\n(cid:19)\n\n(cid:19)\n\n(35)\n\nThus, we have that the event E = {|ˆμ(p) probability at least 1 − 1 non-private estimator, we get the fact that the MSE of the private model is bounded by: (cid:19)\n\ni − σ2| ≤ cp : ∀i ∈ [m]} happens with . By following the same steps as the\n\ni − μ| ≤ cp, |ˆσ2(p)\n\n(cid:113) (e(cid:15)0 +1)2 log(4m2n)\n\nmn , where cp =\n\n3(e(cid:15)0 −1)2(m−1)\n\n(cid:18)\n\nMSE ≤ E (cid:2)a2(cid:3)\n\nαβ n(α + β)(α + β + 1)\n\n+ E (cid:2)(1 − a)2(cid:3)\n\n(cid:18)\n\nαβ (α + β)2(α + β + 1)\n\n+\n\n(e(cid:15)0 + 1)2 log(4m2n) 3(e(cid:15)0 − 1)2(m − 1)\n\n(cid:19)\n\n, (36)\n\nwhere a(p) =\n\nˆμ\n\n(p) i\n\nn\n\n(1− ˆμ\n\nˆσ\n\n2(p) i\n\n(p) i\n\nand the expectation is with respect\n\nto the clients data\n\n)\n\n+n\n\n{z1, . . . , zi−1, zi+1, . . . , zm}and the randomness of the private mechanism qp. This completes the proof of Theorem 4.\n\nRemark 6 (Privacy with communication efficiency). Note that our private estimation algorithm for the Bernoulli case is already communication-efficient as each client sends only one bit to the server.\n\nRemark 7 (Client sampling). For simplicity, in the theoretical analysis in Gaussian and Bernoulli models, we assume that all clients participate in the estimation process. However, a simple modification to our analysis also handles the case where only K out of m clients participate: in all our theorem statements we would have to modify to have K instead m. Note that we do client sampling for our experiments in Table 1.\n\nD PERSONALIZED ESTIMATION – MIXTURE MODEL\n\nConsider a set of m clients, where the i-th client has a local dataset Xi = (Xi1, . . . , Xin) of n samples for i ∈ [m], where Xij ∈ Rd. The local samples Xi of the i-th client are drawn i.i.d. from a Gaussian distribution N (θi, σ2\n\nId) with unknown mean θi and known variance σ2\n\nId.\n\nx\n\nx\n\nIn this section, we assume that the personalized models θ1, . . . , θm are drawn i.i.d. from a discrete distribution P = [p1, . . . , pk] for given k candidates μ1, . . . , μk ∈ Rd. In other works, Pr[θi = μl] = pl for l ∈ [k] and i ∈ [m]. The goal of each client is to estimate her personalized model {θi} that minimizes the mean square error defined as follows:\n\nMSE = E{θi,Xi}(cid:107)θi − ˆθi(cid:107)2, where the expectation is taken with respect to the personalized models θi and the local samples Id)}. Furthermore, ˆθi denotes the estimate of the personalized model θi for {Xij ∼ N (θi, σ2 i ∈ [m].\n\n(37)\n\nx\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFirst, we start with a simple case when the clients have perfect knowledge of the prior distribution, i.e., the i-th client knows the k Gaussian distributions N (cid:0)μ1, σ2 (cid:1) and the prior distribution α = [α1, . . . , αk]. This will serve as a stepping stone to handle the more general case when the prior distribution is unknown.\n\n(cid:1) , . . . , N (cid:0)μk, σ2\n\nθ\n\nθ\n\nD.1 WHEN THE PRIOR DISTRIBUTION IS KNOWN\n\nIn this case, the i-th client does not need the data of the other clients as she has a perfect knowledge about the prior distribution. Theorem 6. For given a perfect knowledge α = [α1, . . . , αk] and N (cid:0)μ1, σ2 the optimal personalized estimator that minimizes the MSE is given by:\n\n(cid:1) , . . . , N (cid:0)μk, σ2\n\nθ\n\nθ\n\n(cid:1),\n\nˆθi =\n\nk (cid:88)\n\nl=1\n\na(i)\n\nl μl,\n\n(38)\n\n(cid:18)\n\n−\n\npl exp\n\n(cid:80)k\n\ns=1 ps exp\n\n(cid:18)\n\n(cid:80)n\n\n(cid:19)\n\nj=1 (cid:107)Xij −μl (cid:107)2 2σ2 x\n(cid:107)Xij −μs(cid:107)2 2σ2 x\n\nj=1\n\n(cid:80)n\n\n−\n\nwhere α(i)\n\nl =\n\nfor l ∈ [k].\n\n(cid:19) denotes the weight associated to the prior model μl\n\nProof. Let θi ∼ P, where P = [p1, . . . , pk] and pl = Pr[θi = μl] for l ∈ [k]. The goal is to design an estimator ˆθi that minimizes the MSE given by:\n\nMSE = Eθi∼PE{Xij ∼N (θi,σ2\n\nx)}\n\n(cid:104)\n\n(cid:107)ˆθi − θi(cid:107)2(cid:105)\n\n.\n\n(39)\n\nLet Xi = (Xi1, . . . , Xin). By following the standard proof of the minimum MSE, we get that: Eθi\n\n(cid:107)ˆθi − θi(cid:107)2(cid:105)\n\n= EXi\n\nEθi|Xi\n\nEXi\n\n(cid:104)\n\n(cid:105)\n\n(cid:104)\n\n(cid:107)ˆθi − E[θi|Xi] + E[θi|Xi] − θi(cid:107)2(cid:12) (cid:2) (cid:107)E[θi|Xi] − θi(cid:107)2(cid:12) (cid:2) (cid:107)E[θi|Xi] − θi(cid:107)2(cid:12)\n\n(cid:3) + EXi (cid:3) ,\n\nEθi|Xi\n\n(cid:12) Xi\n\n(cid:12) Xi\n\n(cid:12) (cid:12) Xi (cid:104)\n\n= EXi\n\n≥ EXi\n\nEθi|Xi Eθi|Xi\n\n(cid:107)E[θi|Xi] − ˆθi(cid:107)2(cid:12)\n\n(cid:105)\n\n(cid:12) (cid:12) Xi\n\n(40) where the last inequality is achieved with equality when ˆθi = E[θi|Xi]. The distribution on θi given the local dataset Xi is given by:\n\nPr[θi = μl|Xi] =\n\nf (Xi|θi = μl) Pr[θi = μl] f (Xi)\n\n=\n\n=\n\nf (Xi|θi = μl) Pr[θi = μl] s=1 f (Xi|θi = μs) Pr[θi = μs]\n\n(cid:80)k\n\n(cid:18)\n\npl exp\n\n−\n\n(cid:80)n\n\nj=1 (cid:107)Xij −μl(cid:107)2\n\n2σ2 x\n\n(cid:19)\n\n(cid:80)k\n\ns=1 ps exp\n\n(cid:16)\n\n−\n\n(cid:80)n\n\nj=1 (cid:107)Xij −μs(cid:107)2\n\n2σ2 x\n\n(cid:17) = α(i)\n\nl\n\nAs a result, the optimal estimator is given by:\n\nThis completes the proof of Theorem 6.\n\nˆθi = E[θi|Xi] =\n\nk (cid:88)\n\nl=1\n\nα(i)\n\nl μl.\n\n(41)\n\n(42)\n\nThe optimal personalized estimation in (38) is a weighted summation over all possible candidates vectors μ1, . . . , μk, where the weight α(i) increases if the prior pl increases and/or the local samples {Xij} are close to the model μl for l ∈ [k]. Observe that the optimal estimator ˆθi in Theorem 6 that . Furthermore, minimizes the MSE is completely different from the local estimator\n\n(cid:80)n\n\n(cid:16) 1\n\n(cid:17)\n\nl\n\nj=1 Xij\n\nn\n\nit is easy to see that the local estimator has the MSE\n\n(cid:17)\n\n(cid:16) dσ2 n\n\nx\n\nwhich increases linearly with the data\n\ndimension d. On the other hand, the MSE of the optimal estimator in Theorem 6 is a function of the prior distribution P = [p1, . . . , pk], the prior vectors μ1, . . . , μk, and the local variance σ2 x.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nD.2 WHEN THE PRIOR DISTRIBUTION IS UNKNOWN\n\nNow, we consider a more practical case when the prior distribution P = [p1, . . . , pk] and the candidates μ1, . . . , μk are unknown to the clients. In this case, the clients collaborate with each other by their local data to estimate the priors P and μ1, . . . , μk, and then, each client uses the estimated priors to design her personalized model as in (38).\n\ni\n\ni\n\n1\n\n1\n\nk\n\nk\n\n(cid:80)n\n\n:= 1 n\n\n], μ(t+1)\n\n, . . . , p(t+1)\n\n, . . . , μ(t+1)\n\nfor given local models {θ(t)\n\n} for given global priors P(t+1) and μ(t+1)\n\nWe present Algorithm 3 based on alternating minimization. The algorithm starts by initializing the local models {θ(0) j=1 Xij}. Then, the algorithm works in rounds alternating between estimating the priors P(t+1) = [p(t+1) i } and estimating the personalized models {θ(t+1) , . . . , μ(t+1) .\nObserve that for given the prior information P(t), {μt l}, each client updates her personalized model in Step 6 which is the optimal estimator for given priors according to Theorem 6. On the other hand, for given personalized models {θ(t) l} using clustering algorithm with k sets in Step 11. The algorithm Cluster takes m vectors a1, . . . , am and an integer k as its input, and its goal is to generate a set of k cluster centers μ1, . . . , μk that minimizes (cid:80)m i=1 minl∈k (cid:107)ai − μl(cid:107)2. Furthermore, these clustering algorithms can also return the prior distribution P, by setting pl := |Sl| m , where Sl ⊂ {a1, . . . , am} denotes the set of vectors that are belongs to the l-th cluster. There are lots of algorithms that do clustering, but perhaps, Lloyd’s algorithm Lloyd (1982) and Ahmadian Ahmadian et al. (2019) are the most common algorithms for k-means clustering. Our Algorithm 3 can work with any clustering algorithm.\n\ni }, we estimate the priors P(t), {μt\n\nk\n\n1\n\nAlgorithm 3 Alternating Minimization for Personalized Estimation Input: Number of iterations T , local datasets (Xi1, . . . , Xin) for i ∈ [m].\n\n(cid:80)n\n\nj=1 Xij for i ∈ [m].\n\n1: Initialize θ0 i = 1 n\n2: for t = 1 to T do 3: On Clients: 4: 5: 6:\n\nfor i = 1 to m: do\n\nReceive P(t), μ(t) Update the personalized model:\n\n1 , . . . , μ(t)\n\nk from the server\n\nθt\n\ni ←\n\nk (cid:88)\n\nl=1\n\nα(i)\n\nl μ(t)\n\nl\n\nand\n\nα(i)\n\nl =\n\n(cid:18)\n\nexp\n\n−\n\np(t)\n\nl\n\n(cid:80)n\n\nj=1 (cid:107)Xij −μ(t) 2σ2 x\n\nl (cid:107)2\n\n(cid:19)\n\n(cid:80)k\n\ns=1 p(t)\n\ns exp\n\n(cid:18)\n\n−\n\n(cid:80)n\n\nj=1 (cid:107)Xij −μ(t) 2σ2 x\n\ns (cid:107)2\n\n(cid:19)\n\nSend θt\n\ni to the server\n\n7: 8: 9: 10:\n\n11:\n\nend for At the Server: Receive θ(t) Update the global parameters: P(t), μ(t) Broadcast P(t), μ(t)\n\nm from the clients\n\n1 , . . . , θ(t)\n\n1 , . . . , μ(t)\n\nk to all clients\n\n12: 13: end for Output: Personalized models θT\n\n1 , . . . , θT m.\n\n1 , . . . , μ(t)\n\nk ← Cluster\n\n(cid:16)\n\nθ(t)\n\n1 , . . . , θ(t)\n\nm , k\n\n(cid:17)\n\nD.3 PRIVACY/COMMUNICATION CONSTRAINTS\n\nIn the personalized estimation Algorithm 3, each client shares her personalized estimator θ(t) to the server at each iteration which is not communication-efficient and violates the privacy. In this section we present ideas on how to design communication-efficient and/or private Algorithms for personalized estimation. Lemma 3. Let μ1, . . . μk ∈ Rd be unknown means such that (cid:107)μi(cid:107)2 ≤ r for each i ∈ [k]. Let θ1, . . . , θm ∼ P, where P = [p1, . . . , pk] and pl = Pr[θi = μl]. For i ∈ [m], let Xi1, . . . , Xin ∼\n\ni\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nN (θi, σ2\n\nx), i.i.d. Then, with probability at least 1 − 1\n\nmn , the following bound holds for all i ∈ [m]:\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\nj=1\n\nXij\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2\n\n(cid:114)\n\n≤ 4\n\nd\n\nσ2 x\nn\n\n(cid:114)\n\n+ 2\n\nlog(m2n)\n\nσ2 x\nn\n\n+ r.\n\n(43)\n\nProof. Observe that the vector (X i − θi) = 1 proxy σ2\n\nn . As a result, we have that:\n\nn\n\nx\n\n(cid:80)n\n\ni=1 Xij − θi is a sub-Gaussian random vector with\n\nσ2 x\nn with probability at least 1 − η from Wainwright (2019). Since μ1, . . . , μk ∈ Rd are such that (cid:107)μi(cid:107)2 ≤ r for each i ∈ [k], we have:\n\n(cid:107)X i − θi(cid:107)2 ≤ 4\n\nlog(1/η)\n\nσ2 x\nn\n\n(44)\n\n+ 2\n\nd\n\n,\n\n(cid:114)\n\n(cid:114)\n\n(cid:107)X i(cid:107)2 ≤ 4\n\nd\n\n+ 2\n\nlog(1/η)\n\n(cid:114)\n\n(cid:114)\n\nσ2 x\nn\n\nσ2 x\nn\n\n+ r,\n\n(45)\n\nwith probability 1 − η from the triangular inequality. Thus, by choosing η = 1 union bound, this completes the proof of Lemma 3.\n\nm2n and using the\n\nLemma 3 shows that the average of the local samples {X i} has a bounded (cid:96)2 norm with high probability. Thus, we can design a communication-efficient estimation Algorithm as follows: Each client clips her personal model θ(t) n + r. Then, each client applies a vector-quantization scheme (e.g., Bernstein et al. (2018); Alistarh et al. (2017); Girgis et al. (2021a)) to the clipped vector before sending it to the server.\n\ni within radius 4\n\nlog(m2n) σ2\n\nn + 2\n\nd σ2\n\n(cid:113)\n\n(cid:113)\n\nx\n\nx\n\n(cid:113)\n\ni within radius 4\n\nTo design a private estimation algorithm with discrete priors, each client clips her personalized estimator θ(t) n + r. Then, we can use a differentially private algorithm for clustering (see e.g., Stemmer (2020) for clustering under LDP constraints and Ghazi et al. (2020) for clustering under central DP constraints.). Since, we run T iterations in Algorithm 3, we can obtain the final privacy analysis ((cid:15), δ) using the strong composition theorem Dwork & Roth (2014).\n\nlog(m2n) σ2\n\nn + 2\n\nd σ2\n\n(cid:113)\n\nx\n\nx\n\nE PERSONALIZED LEARNING – LINEAR REGRESSION\n\nIn this section, we present the personalized linear regression problem. Consider A set of m clients, where the i-th client has a local dataset consisting of n samples (Xi1, Yi1), . . . , (Xin, Yin), where Xij ∈ Rd denotes the feature vector and Yij ∈ R denotes the corresponding response. Let Yi = (Yi1, . . . , Yi1) ∈ Rn and Xi = (Xi1, . . . , Xin) ∈ Rn×d denote the response vector and the feature matrix at the i-th client, respectively. Following the standard regression, we assume that the response vector Yi is obtained from a linear model as follows:\n\n(46) where θi denotes personalized model of the i-th client and wi ∼ N (cid:0)0, σ2 (cid:1) is a noise vector. The clients’ parameters θ1, . . . , θm are drawn i.i.d. from a Gaussian distribution θ1, . . . , θm ∼ N (μ, σ2 θ\n\nYi = Xiθi + wi,\n\nId), i.i.d.\n\nIn\n\nx\n\nOur goal is to solve the optimization problem stated in (9) (for the linear regression setup) and learn the optimal personalized parameters {(cid:98)θi}. The following theorem characterizes the exact form of the optimal {(cid:98)θi} and computes their minimum mean squared error w.r.t. the true parameters {θi}. Theorem 7. The optimal personalized parameters at client i with known μ, σ2 x is given by:\n\nθ , σ2\n\n(cid:98)θi =\n\n(cid:18) I\n\nσ2 θ\n\n+\n\nX T i Xi σ2 x\n\n(cid:19)−1 (cid:18) X T i Yi σ2 x\n\n+\n\n(cid:19)\n\n.\n\nμ σ2 θ\n\nThe mean squared error (MSE) of the above (cid:98)θi is given by:\n\nEwi,θi\n\n(cid:13) (cid:13) (cid:13)(cid:98)θi − θi\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n= Tr\n\n(cid:32)(cid:18) I σ2 θ\n\n+\n\nX T i Xi σ2 x\n\n(cid:19)−1(cid:33)\n\n,\n\n(47)\n\n(48)\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nProof. The personalized model with perfect prior is obtained by solving the optimization problem stated in (9), which is given below for convenience. Note that for linear regression with Gaussian prior, we have P(Γ) ≡ N (μ, σ2\n\nId) and pθi(Yij|Xij) according to N (θi, σ2\n\nx).\n\nθ\n\n(cid:98)θi = arg min\n\nθi\n\n= arg min\n\nθi\n\nn (cid:88)\n\nj=1\n\nn (cid:88)\n\nj=1\n\n− log(pθi(Yij|Xij)) − log(p(θi)).\n\n(Yij − Xijθi)2 2σ2 x\n\n+\n\n(cid:107)θi − μ(cid:107)2 2σ2 θ\n\n.\n\n= arg min\n\nθi\n\n(cid:107)Yi − Xiθi(cid:107)2 2σ2 x\n\n+\n\n(cid:107)θi − μ(cid:107)2 2σ2 θ\n\n.\n\nBy taking the derivative with respect to θi, we get\n\n∂ ∂θi\n\n=\n\nX T\n\ni (Xiθi − Yi) σ2 x\n\n+\n\nθi − μ σ2 θ\n\n.\n\n(49)\n\nEquating the above partial derivative to zero, we get that the optimal personalized parameters (cid:98)θi is given by:\n\n(50)\n\n(51)\n\n(cid:98)θi =\n\n(cid:18) I\n\nσ2 θ\n\n+\n\nX T i Xi σ2 x\n\nTaking the expectation w.r.t. wi, we get:\n\n(cid:19)−1 (cid:18) X T i Yi σ2 x\n\n+\n\n(cid:19)\n\n.\n\nμ σ2 θ\n\nEwi[(cid:98)θi] =\n\n(cid:18) I\n\nσ2 θ\n\n+\n\nX T i Xi σ2 x\n\n(cid:19)−1 (cid:18) X T\n\ni Xiθi σ2 x\n\n+\n\n(cid:19)\n\n,\n\nμ σ2 θ\n\nThus, we can bound the MSE as following:\n\nEwi,θi\n\n(cid:13) (cid:13) (cid:13)(cid:98)θi − θi\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n= Ewi,θi\n\n= Ewi,θi\n\n= Ewi,θi\n\n(cid:13) (cid:13) 2\n(cid:13)(cid:98)θi − Ewi[(cid:98)θi] (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13)(cid:98)θi − Ewi[(cid:98)θi] (cid:13) (cid:13) (cid:13)\n\n+ Ewi,θi\n\n(cid:13) (cid:13) 2\n(cid:13)(cid:98)θi − Ewi [(cid:98)θi] + Ewi [(cid:98)θi] − θi (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:68)\n\nEwi[(cid:98)θi] − θi\n\nEwi[(cid:98)θi] − θi\n\n+ Ewi,θi\n\nIn\n\nthe\n\nlast\n\nequality,\n\nwe\n\nused\n\nEwi,θi\n\n(cid:98)θi − Ewi[(cid:98)θi], Ewi[(cid:98)θi] − θi\n\n(cid:69)\n\n=\n\nEwi[(cid:98)θi] − Ewi[(cid:98)θi], Ewi [(cid:98)θi] − θi\n\n(cid:68)\n\nEθi independent of wi.\n\n(cid:69)\n\n= 0, where the first equality holds because Ewi[(cid:98)θi] − θi is\n\nLetting M = I\n\nσ2 θ\n\n+ X T\n\ni Xi σ2 x\n\n, and Tr denoting the trace operation, we get\n\nEwi,θi\n\n(cid:13) (cid:13) (cid:13)(cid:98)θi − θi\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:32)\n\n= Tr\n\nM −1Ewi\n\n(cid:34)(cid:18) X T i wi σ2 x\n\n(cid:19) (cid:18) X T i wi σ2 x\n\n(cid:19)T (cid:35)\n\n(cid:33)\n\nM −1\n\n(cid:32)\n\n+ Tr\n\nM −1Eθi\n\n(cid:34)(cid:18) θi − μ\n\n(cid:19) (cid:18) θi − μ\n\n(cid:19)T (cid:35)\n\nσ2 θ\n\nσ2 θ\n\n(cid:33)\n\nM −1\n\n(cid:19)\n\n(cid:18)\n\nM −1\n\n+ Tr\n\nM −1\n\n(cid:19)\n\nM −1\n\nI\n\nσ2 θ\n\n(cid:18)\n\n= Tr\n\nM −1 X T\n\ni Xi σ2 x\n\n= Tr (cid:0)M −1(cid:1) .\n\nThis completes the proof of Theorem 7.\n\nObserve that the local model of the i-th client, i.e., estimating θi only from the local data (Yi, Xi), is given by:\n\n(l)\n\ni = (cid:0)X T\n\ni Xi\n\n(cid:98)θ\n\n(cid:1)−1\n\nX T\n\ni Yi,\n\n(52)\n\n27\n\n+ 2Ewi,θi\n\n(cid:68)\n\n(cid:98)θi − Ewi [(cid:98)θi], Ewi[(cid:98)θi] − θi\n\n(cid:69)\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 4 Linear Regression GD Input: Number of iterations T , local datasets (Yi, Xi) for i ∈ [m], learning rate η.\n\ni for i ∈ [m], μ0, σ2,0\n\nx , σ2,0 θ .\n\n1: Initialize θ0 2: for t = 1 to T do 3: On Clients: 4: 5:\n\nfor i = 1 to m: do\n\nReceive and set μt\n\nUpdate the personalized model: θt\n\ni = μt, σ2,t\n\nθ,i = σ2,t\n\nx,i = σ2,t (cid:18)\n\nx\n\nθ , σ2,t i ← θt−1\n\ni + η\n\n(cid:80)n\n\nj=1\n\nXij (Yij −Xij θt−1 σ2,t−1\n\ni\n\nx,i\n\n(cid:19)\n\n)\n\n+ μt−1\n\ni −θt−1 σ2,t−1\n\ni\n\nθ,i\n\nUpdate local version of mean: μt\n\ni ← μt−1\n\ni − η (cid:18)\n\n(cid:18)\n\nμt−1\n\ni −θt−1 σ2,t−1\n\ni\n\nθ,i\n\n(cid:19)\n\nUpdate local variance: σ2,t\n\nx,i ← σ2,t−1\n\nx,i − η\n\nUpdate global variance: σ2,t\n\nθ,i ← σ2,t−1\n\nθ,i\n\n− η\n\n(cid:19)\n\n)2\n\nn 2σ2,t−1 (cid:18)\n\nx,i\n\n− (cid:80)n\n\nj=1\n\n(Yij −Xij θt−1 i\n2(σ2,t−1 )2 (cid:19)\n\nx,i\n\nd 2σ2,t−1\n\nθ,i\n\n− (cid:107)μt−1\n\ni −θt−1 2(σ2,t−1 )2\n\ni\n\nθ,i\n\n(cid:107)2\n\nend for At the Server: (cid:80)m Aggregate mean: μt = 1 Aggregate global variance: σ2,t Aggregate local variance: σ2,t Broadcast μt, σ2,t\n\nθ , σ2,t\n\nm\n\nx\n\ni\n\ni=1 μt θ = 1 x = 1\n\nm\n\nm\n\n15: 16: end for Output: Personalized models θT\n\n1 , . . . , θT m.\n\n(cid:80)m i=1 σ2,t i=1 σ2,t\n\nx,i\n\nθ,i\n\n(cid:80)m\n\n6:\n\n7:\n\n8:\n\n9:\n\n10: 11: 12:\n\n13:\n\n14:\n\nwhere we assume the matrix X T local estimate achieves the MSE given by:\n\ni Xi has a full rank (otherwise, we take the pseudo inverse). This\n\nE\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:98)θ\n\n(l)\n\ni − θi\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n= Tr\n\n(cid:16)(cid:0)X T\n\ni Xi\n\n(cid:1)−1(cid:17)\n\nσ2 x,\n\n(53)\n\nwe can prove it by following similar steps as the proof of Theorem 7. When σ2 see that the local estimate (52) matches the personalized estimate in (47).\n\nθ → ∞, we can easily\n\nTo make the regression problem more practical, we assume that the mean μ, the local variance σ2 θ are unknown. Hence, we estimate the personalized parameters by minimizing the negative log likelihood:\n\nx, and the global variance σ2\n\n(cid:98)θ1, . . . , (cid:98)θm = arg min\n\n{θi},μ,σ2\n\nx,σ2\n\nθ\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n− log (pθi (Yij|Xij)) +\n\nm (cid:88)\n\ni=1\n\n− log (p (θi))\n\n= arg min\n\nnm 2\n\nlog(2πσ2\n\nx) +\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n(Yij − Xijθi)2 2σ2 x\n\n+\n\nmd 2\n\nlog(2πσ2\n\nθ ) +\n\nm (cid:88)\n\ni=1\n\n(cid:107)θi − μ(cid:107)2 2σ2 θ\n\n.\n\n(54)\n\nInstead of solving the above optimization problem explicitly, we can optimize it through gradient descent (GD) and the resulting algorithm is presented in Algorithm 4. In addition to keeping the personalized models {θt x,i} and updates all these parameters by taking appropriate gradients of the objective in (54) and synchronize them with the server to update the global copy of these parameters {μt, σt\n\ni}, each client also maintains local copies of {μt\n\nθ,i, σt\n\ni, σt\n\nθ, σt\n\nx}.\n\nF PERSONALIZED LEARNING – LOGISTIC REGRESSION\n\nAs described in Section 3, by taking P(Γ) ≡ N (μ, σ2 Id) and pθi(Yij|Xij) = σ((cid:104)θi, Xij(cid:105))Yij (1 − σ((cid:104)θi, Xij(cid:105)))(1−Yij ), where σ(z) = 1/1+e−z for any z ∈ R, then the overall optimization problem becomes:\n\nθ\n\n28\n\nPublished as a conference paper at ICLR 2023\n\narg min {θi},μ,σθ\n\nm (cid:88)\n\nn (cid:88)\n\n(cid:20)\n\ni=1\n\nj=1\n\nYij log\n\n(cid:18)\n\n1 1 + e−(cid:104)θi,Xij (cid:105)\n\n(cid:19)\n\n+ (1 − Yij) log\n\n(cid:18)\n\n1 1 + e(cid:104)θi,Xij (cid:105)\n\n(cid:19)(cid:21)\n\n+\n\nmd 2\n\nlog(2πσ2\n\nθ ) +\n\nm (cid:88)\n\ni=1\n\n(cid:107)μ − θi(cid:107)2 2σ2 θ\n\n2\n\n.\n\n(55)\n\nWhen μ and σ2 regression case. The corresponding algorithm is described in Algorithm 5.\n\nθ are unknown, we would like to learn them by gradient descent, as in the linear\n\nAlgorithm 5 Logistic Regression SGD Input: Number of iterations T , local datasets (Yi, Xi) for i ∈ [m], learning rate η.\n\ni for i ∈ [m], μ0, σ2,0 θ .\n\n1: Initialize θ0 2: for t = 1 to T do 3: On Clients: 4: 5: 6:\n\nfor i = 1 to m: do Receive (μt, σ2,t Update the personalized model:\n\nθ ) from the server and set μt\n\ni := μt, σ2,t\n\nθ,i := σ2,t\n\nθ\n\nθt\n\ni ← θt−1\n\ni − η\n\n\n\n\n\nn (cid:88)\n\nj=1\n\n∇θt−1\n\ni\n\nl(p) CE(θt−1\n\ni\n\n, (X j\n\ni , Y j\n\ni )) +\n\nμt−1\n\ni − θt−1 σ2,t−1\n\ni\n\nθ,i\n\n\n\n ,\n\nwhere l(p)\n\nCE denotes the cross-entropy loss. i ← μt−1\n\nUpdate local version of mean: μt\n\ni − η\n\nUpdate global variance: σ2,t θ,i\n\n← σ2,t−1 θ,i\n\n− η\n\n(cid:18)\n\n(cid:19)\n\n(cid:18)\n\nμt−1\n\ni −θt−1 σ2,t−1 θ,i\n\ni\n\nd 2σ2,t−1\n\nθ,i\n\n− (cid:107)μt−1\n\ni −θt−1 2(σ2,t−1 )2\n\ni\n\nθ,i\n\n(cid:19)\n\n(cid:107)2\n\n7:\n\n8:\n\n9: 10: 11: 12:\n\n13:\n\n14:\n\nSend (μt\n\ni, σ2,t\n\nθ,i ) to the server\n\ni, σ2,t\n\nend for At the Server: Receive {(μt (cid:80)m Aggregate mean: μt = 1 Aggregate global variance: σ2,t Broadcast (μt, σ2,t\n\nθ ) to all clients\n\nm\n\nθ,i )} from the clients\n\ni\n\ni=1 μt θ = 1\n\nm\n\n15: 16: end for Output: Personalized models θT\n\n1 , . . . , θT m.\n\n(cid:80)m\n\ni=1 σ2,t\n\nθ,i\n\nG PERSONALIZED LEARNING – MIXTURE MODEL\n\nIn this section, we present the linear regression problem as a generalization to the estimation problem with discrete priors. This model falls into the framework studied in Marfoq et al. (2021) and is illustrated to show how our framework also captures it.\n\nConsider a set of m clients, where the i-th client has a local dataset (Xi1, Yi1), . . . , (Xin, Yin) of m samples, where Xij ∈ Rd denotes the feature vector and Yij ∈ R denotes the corresponding response. Let Yi = (Yi1, . . . , Yi1) ∈ Rn and Xi = (Xi1, . . . , Xin) ∈ Rn×d denote the response vector and the feature matrix at the i-th client, respectively. Following the standard regression, we assume that the response vector Yi is obtained from a linear model as follows:\n\n(56) where θi denotes personalized model of the i-th client and wi ∼ N (cid:0)0, σ2 (cid:1) is a noise vector. The clients models are drawn i.i.d. from a discrete distribution θ1, . . . , θm ∼ P, where P = [p1, . . . , pk] such that pl = Pr[θi = μl] for i ∈ [m] and l ∈ [k].\n\nYi = Xiθi + wi,\n\nIn\n\nx\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nOur goal is to solve the optimization problem stated in (9) (for the linear regression with the above discrete prior) and learn the optimal personalized parameters {(cid:98)θi}. We assume that the discrete distribution P and the prior candidates {μl}k l=1 are unknown to the clients. Inspired from Algorithm 3 for estimation with discrete priors, we obtain Algorithm 6 for learning with discrete prior. Note that this is not a new algorithm, and is essentially the algorithm proposed in Marfoq et al. (2021) applied to linear regression. We wanted to show how our framework captures mixture model in Marfoq et al. (2021) through this example.\n\nDescription of Algorithm 6. Client i initializes its personalized parameters θ(0) i Yi, which is the optimal as a function of the local dataset at the i-th client without any prior knowledge. In any iteration t, for a given prior information P(t), {μ(t) l }, the i-th client updates the personalized\n\ni Xi)−1X T\n\ni = (X T\n\nmodel as θt\n\ni = (cid:80)k\n\nl=1 α(i)\n\nl μ(t)\n\nl\n\n, where the weights α(i)\n\nl ∝ p(t)\n\nl\n\nexp\n\n(cid:18)\n\n− (cid:107)Xiμ(t)\n\nl −Yi(cid:107)2 2σ2 x\n\n(cid:19)\n\nand sends its\n\ncurrent estimate of the personalized parameter θt will run Cluster algorithm to update the global parameters P, μ(t) the clients.\n\ni to the server. Upon receiving θt\n\n1 , . . . , μ(t)\n\n1, . . . , θt m, server k , and broadcasts them to\n\nAlgorithm 6 Alternating Minimization for Personalized Learning Input: Number of iterations T , local datasets (Xi, Yi) for i ∈ [m].\n\n1: Initialize θ0\n\ni = (X T\n\ni Xi)−1X T\n\ni Yi for i ∈ [m] (if X T\n\ni Xi is not full-rank, take the pseudo-inverse).\n\n2: for t = 1 to T do 3: On Clients: 4: 5: 6:\n\nfor i = 1 to m: do\n\nReceive P(t), μ(t) Update the personalized parameters and the coefficients:\n\nk from the server\n\n1 , . . . , μ(t)\n\nθt\n\ni ←\n\nk (cid:88)\n\nl=1\n\nα(i)\n\nl μ(t)\n\nl\n\nand\n\nα(i)\n\nl =\n\n(cid:18)\n\np(t)\n\nl\n\nexp\n\n− (cid:107)Xiμ(t)\n\nl −Yi(cid:107)2 2σ2 x\n\n(cid:19)\n\n(cid:80)k\n\ns=1 p(t)\n\ns exp\n\n(cid:16)\n\n− (cid:107)Xiμ(t)\n\ns −Yi(cid:107)2 2σ2 x\n\n(cid:17)\n\nSend θ(t)\n\ni\n\nto the server\n\n7: 8: 9: 10:\n\n11:\n\nend for At the Server: Receive θ(t) Update the global parameters: P(t), μ(t) Broadcast P(t), μ(t)\n\nm from the clients\n\n1 , . . . , θ(t)\n\n1 , . . . , μ(t)\n\nk to all clients\n\n12: 13: end for Output: Personalized models θT\n\n1 , . . . , θT m.\n\n1 , . . . , μ(t)\n\nk ← Cluster\n\n(cid:16)\n\nθ(t)\n\n1 , . . . , θ(t)\n\nm , k\n\n(cid:17)\n\nH PERSONALIZED LEARNING – ADAPED\n\nH.1 KNOWLEDGE DISTILLATION POPULATION DISTRIBUTION\n\nIn this section we discuss what type of a population distribution can give rise to algorithms/problems that include a knowledge distillation (KD) (or KL divergence) penalty term between local and global models. From Section 3, Equation (9), consider pθi(y|x) as a randomized mapping from input space X to output class Y, parameterized by θi. For simplicity, consider the case where |X | is finite, e.g. for MNIST it could be all possible 28 × 28 black and white images. Every pθi(y|x) corresponds to a probability matrix (parameterized by θi) of size |Y| × |X |, where the (y, x)’th represents the probability of the class y (row) given the data sample x (column). Therefore, each column is a probability vector. Since we want to sample the probability matrix, it suffices to restrict our attention to any set of |Y| − 1 rows, as the remaining row can be determined by these |Y| − 1 rows.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nSimilarly, for a global parameter μ, let pμ(y|x) define a randomized mapping from X to Y, parameterized by the global parameter μ. Note that for a fixed global parameter μ, the randomized map pμ(y|x) is fixed, whereas, our goal is to sample pθi (y|x) for i = 1, . . . , m, one for each client. For simplicity of notation, define pθi := pθi (y|x) and pμ := pμ(y|x) to be the corresponding probability matrices, and let the distribution for sampling pθi(y|x) be denoted by ppμ (pθi). Note that different mappings pθi(y|x) correspond to different θi’s, so we define p(θi) (in Equation (9)) as ppμ (pθi), which is the density of sampling the probability matrix pθi(y|x).\n\nFor the KD population distribution, we define this density ppμ(pθi) as:\n\nppμ (pθi) = c(ψ)e−ψDKL(pμ(y|x)(cid:107)pθi (y|x))\n\n(57)\n\nwhere ψ is an ‘inverse variance’ type of parameter, c(ψ) is a normalizing function that depends on (ψ, pμ), and DKL(pμ(y|x)(cid:107)pθi(y|x)) = (cid:80) tional KL divergence, where p(x) denotes the probability of sampling a data sample x ∈ X . Now all we need is to find c(ψ) given a fixed μ (and therefore fixed pμ(y|x)). Here we consider DKL(pμ(cid:107)pθi), but our analysis can be extended to DKL(pθi (cid:107)pμ) or (cid:107)pθi − pμ(cid:107)2 as well.\n\n(cid:16) pμ(y|x) pθi (y|x)\n\ny∈Y pμ(y|x) log\n\nx∈X p(x) (cid:80)\n\nis the condi-\n\n(cid:17)\n\nFor simplicity and to make the calculations easier, we consider a binary classification task with Y = {0, 1} and define pμ(x) := pμ(y = 1|X = x) and qi(x) := pθi(y = 1|X = x). We have: (cid:16)\n\nDKL(pμ(y|x)(cid:107)pθi(y|x)) =\n\np(x)\n\npμ(x)(log pμ(x) − log qi(x))\n\n(cid:88)\n\nx\n\n+ (1 − pμ(x))(log(1 − pμ(x)) − log(1 − qi(x)))\n\n(cid:17)\n\n.\n\nHence, after some algebra we have, ppμ(pθi) = c(ψ)eψ (cid:80)\n\nx p(x)H(pμ(x))eψ (cid:80)\n\nx p(x)(pμ(x) log(qi(x))+(1−pμ(x)) log(1−qi(x))))\n\nThen,\n\nc(ψ)\n\n(cid:104) (cid:90) 1\n\n(cid:89)\n\nx\n\n0\n\nNote that\n\n(cid:105) eψp(x)H(pμ(x))eψp(x)(pμ(x) log(qi(x))+(1−pμ(x)) log(1−qi(x))))dqi(x)\n\n= 1.\n\n(cid:90) 1\n\n0\n\neψp(x)(pμ(x) log(qi(x))+(1−pμ(x)) log(1−qi(x))))dqi(x) = B\n\n(cid:18)\n\n1 +\n\npμ(x) ψp(x)\n\n, 1 +\n\n1 − pμ(x) ψp(x)\n\n(cid:19)\n\nAccordingly, after some algebra, we can obtain c(ψ) =\n\nShannon entropy. Substituting this in (57), we get\n\ne−ψ (cid:80) (cid:16) 1+\n\nx p(x)H(pμ(x)) pμ(x) ψp(x) ,1+\n\n1−pμ (x) ψp(x)\n\n(cid:81)\n\nx B\n\n(cid:17) , where H is binary\n\nppμ (pθi) =\n\ne−ψ (cid:80) x B(1 + pμ(x)\n\n(cid:81)\n\nx p(x)H(pμ(x))\n\nψp(x) , 1 + 1−pμ(x) ψp(x) )\n\ne−ψDKL(pμ(y|x)(cid:107)pθi (y|x))\n\nwhich is the population distribution that can result in a KD type regularizer. Note that when we take the negative logarithm of the population distribution we obtain KL divergence loss and an additional term that depends on ψ and pμ. This is the form seen in Section 3.2 Equation (11) for AdaPeD\n\n(cid:18)\n\ne−ψ (cid:80) x B(1+\n\nx p(x)H(pμ(x)) pμ(x) ψp(x) ,1+\n\n1−pμ (x)\n\nψp(x) )\n\n(cid:19)\n\nto\n\nalgorithm. For numerical purpose, we take the additional term − log\n\n(cid:81)\n\nbe simple 1 2 log(2ψ). As mentioned in Section 3.2, this serves the purpose of regularizing ψ. This is in contrast to the objective considered in Ozkara et al. (2021), which only has the KL divergence loss as the regularizer, without the additional term.\n\nH.2 ADAPED WITH UNSAMPLED CLIENT ITERATIONS\n\nWhen there is a flexibility in computational resources for doing local iterations, unsampled clients can do local training on their personalized models to speed-up convergence at no cost to privacy. This can be used in cross-silo settings, such as cross-institutional training for hospitals, where privacy is\n\n31\n\nPublished as a conference paper at ICLR 2023\n\ncrucial and there are available computing resources most of the time. We propose the algorithm for AdaPeD with with unsampled client iterations in Algorithm 7:\n\nAlgorithm 7 Adaptive Personalization via Distillation (AdaPeD) with unsampled client iterations\n\nParameters: model {μ0 K.\n\nlocal variances {ψ0 i }, synchronization gap τ ,\n\ni }, personalized models {θ0\n\nlocal copies of the global learning rates η1, η2, η3, number of sampled clients\n\ni },\n\nif τ divides t then On Server do: Choose a subset Kt ⊆ [n] of K clients Broadcast μt and ψt On Clients i ∈ Kt (in parallel) do: i = μt, ψt Receive μt and ψt; set μt\n\n1: for t = 0 to T − 1 do 2: 3: 4: 5: 6: 7: 8: 9: On Clients i /∈ Kt (in parallel) do:\n\nend if\n\ni = ψt\n\n10:\n\nCompute gt\n\ni := ∇θt\n\ni\n\nfi(θt\n\ni) +\n\n∇θt\n\ni\n\nreceived global parameters from the server Update: θt+1\n\ni = θt 11: 12: On Clients i ∈ Kt (in parallel) do:\n\ni − η1gt\n\ni\n\nt(cid:48) i\ni )\n\ni,μ\n\ni (θt f KD t(cid:48) i\n2ψ i\n\nwhere t(cid:48)\n\ni is the last time index where client i\n\n∇θt\n\ni\n\ni,μt i)\n\nf KD i (θt 2ψt i\n\n13:\n\n14:\n\n15:\n\n16:\n\n17:\n\nCompute gt Update: θt+1\n\ni := ∇θt i = θt ∇μt\n\ni\n\ni) +\n\nfi(θt i − η1gt i (θt+1 f KD i\n2ψt i\n\ni\n\ni\n\n,μt\n\ni)\n\ni i (θt+1 i\n2(ψt\n\n2ψt i\n\ni − η2ht − f KD i − η3kt\n\ni := i = μt i := 1 i = ψt\n\nCompute ht Update: μt+1 Compute kt Update: ψt+1 if τ divides t + 1 then Clients send μt Server receives {μt Server computes μt+1 = 1\n\n18: 19: 20: 21: 22: end if 23: 24: end for Output: Personalized models (θT\n\ni and ψt\n\ni\n\nK\n\ni )m\n\ni=1\n\n,μt+1\n\n)\n\ni\n\ni )2\n\ni to Server i}i∈Kt and {ψt\n\n(cid:80)\n\ni }i∈Kt i∈Kt μt\n\ni and ψt+1 = 1\n\nK\n\n(cid:80)\n\ni∈Kt ψt\n\ni\n\nOf course, when a client is not sampled for a long period of rounds this approach can become similar to a local training; hence, it might be reasonable to put an upper limit on the successive number of local iterations for each client.\n\nI PERSONALIZED LEARNING – DP-ADAPED\n\nProof of Theorem 5\n\nTheorem (Restating Theorem 5). After T iterations, DP-AdaPeD satisfies (α, (cid:15)(α))-RDP for α > 1, where (cid:15)(α) = (cid:0) K m denotes the sampling ratio of the clients at each global iteration.\n\n, where K\n\n(cid:16) C2 1\nKσ2\n\n+ C2\n\n6 (cid:0) T\n\n2 Kσ2\n\n(cid:1) α\n\n(cid:1)2\n\n(cid:17)\n\nm\n\nq2\n\nq1\n\nτ\n\nProof. In this section, we provide the privacy analysis of DP-AdaPeD. We first analyze the RDP of a single global round t ∈ [T ] and then, we obtain the results from the composition of the RDP over total T global rounds. Recall that privacy leakage can happen through communicating {μi} and {ψt i } and we privatize both of these. In the following, we do the privacy analysis of privatizing {μi} and a similar analysis could be done for {ψt\n\ni } as well.\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nAt each synchronization round t ∈ [T ], the server updates the global model μt+1 as follows:\n\nμt+1 =\n\n1 K\n\n(cid:88)\n\ni∈Kt\n\nμt i,\n\n(58)\n\nwhere μt iterations at the i-th client. At each of the local iterations, the client clips the gradient ht threshold C1 and adds a zero-mean Gaussian noise vector with variance σ2 q1 noise added at the local iterations, the norm-2 sensitivity of updating the global model μt+1 synchronization round t is bounded by:\n\ni is the update of the global model at the i-th client that is obtained by running τ local i with Id. When neglecting the at the\n\ni\n\n∆μ = max Kt,K(cid:48) t\n\n(cid:107)μt+1 − μ(cid:48)t+1(cid:107)2\n\n2 ≤\n\nτ C 2 1\nK 2 ,\n\n(59)\n\n(cid:80)\n\n(cid:48)t ⊂ [m] are neighboring sets that differ in only one client. Additionally, μt+1 = where Kt, K 1\ni and μ(cid:48)t+1 = 1 i∈Kt μt q1 at K\neach local iteration at each client, and then, we take the average of theses vectors over K clients,\n\ni. Since we add i.i.d. Gaussian noises with variance σ2\n\ni∈K(cid:48)t μt\n\n(cid:80)\n\nK\n\nτ σ2 q1 K . it is equivalent to adding a single Gaussian vector to the aggregated vectors with variance Thus, from the RDP of the sub-sampled Gaussian mechanism in (Mironov et al., 2019, Table 1), Bun et al. (2018), we get that the global model μt+1 of a single global iteration of DP-AdaPeD is (α, (cid:15)(1)\n\nt (α))-RDP, where (cid:15)t(α) is bounded by:\n\n(cid:15)(1) t (α) =\n\n(cid:18) K m\n\n(cid:19)2 6αC 2 1\nKσ2 q1\n\n.\n\n(60)\n\nSimilarly, we can show that the global parameter ψt+1 at any synchronization round of DP-AdaPeD is (α, (cid:15)(2)\n\nt (α))-RDP, where (cid:15)t(α) is bounded by:\n\n(cid:15)(2) t (α) =\n\n(cid:18) K m\n\n(cid:19)2 6αC 2 2\nKσ2 q2\n\n.\n\n(61)\n\nUsing adaptive RDP composition (Mironov, 2017, Proposition 1), we get that each synchronization round of DP-AdaPeD is (α, (cid:15)(1) t (α))-RDP. Thus, by running DP-AdaPeD over T /τ synchronization rounds and from the composition of the RDP, we get that DP-AdaPeD is (α, (cid:15)(α))- RDP, where (cid:15)(α) = (cid:0) T\n\nt (α)). This completes the proof of Theorem 5.\n\nt (α) + (cid:15)(2)\n\nt (α) + (cid:15)(2)\n\n(cid:1) ((cid:15)(1)\n\nτ\n\nJ EXPANDED RELATED WORK AND CONNECTIONS TO EXISTING METHODS\n\nIn Section 1, we mentioned that the our framework has connections to several personalized FL methods. In this appendix we provide a few more details related to these connections.\n\nRegularization: As noted earlier using (9) with the Gaussian population prior connects to the use of (cid:96)2 regularizer in earlier personalized learning works Dinh et al. (2020); Ozkara et al. (2021); Hanzely & Richt ́arik (2020); Hanzely et al. (2020); Li et al. (2021), which also iterates between local and global model estimates. This form can be explicitly seen in Appendix E, where in Algorithm 4, we see that the Gaussian prior along with iterative optimization yields the regularized form seen in these methods. In these cases8, P(Γ) ≡ N (μ, σ2 Id) for unknown parameters Γ = {μ}. Note that since the parameters of the population distribution are unknown, these need to be estimated during the iterative learning process. In the algorithm, 4 it is seen the μ plays the role of the global model (and is truly so for the linear regression problem studied in Appendix E).\n\nθ\n\nClustered FL: If one uses a discrete mixture model for the population distribution then the iterative algorithm suggested by our framework connects to (Zhang et al., 2021; Mansour et al., 2020; Ghosh et al., 2020; Smith et al., 2017; Marfoq et al., 2021). In particular, consider a population model with parameters in the m-dimensional probability simplex {α : α = [α1, . . . , αk], αi ≥ 0, ∀i, (cid:80) i αi = 1} which describing a distribution. If there are m (unknown) discrete distributions {D1, . . . , Dm}, one can consider these as the unknown description of the population model in addition to α. Therefore,\n\n8One can generalize these by including σ2\n\nθ in the unknown parameters.\n\n33\n\nPublished as a conference paper at ICLR 2023\n\neach local data are generated either as a mixture as in (Marfoq et al., 2021) or by choosing one of the unknown discrete distributions with probability α dictating the probability of choosing Di, when hard clustering is used (e.g., (Mansour et al., 2020)). Each node j chooses a mixture probability α(j) uniformly from the m-dimensional probability simplex. In the former case, it uses this mixture probability to generate a local mixture distribution. In the latter it chooses Di with probability α(j) .\n\ni\n\nAs mentioned earlier, not all parametrized distributions can be written as a mixture of finite number distributions, which is the assumption for discrete mixtures. Consider a unimodal Gaussian population Id), for node i, we sample distribution (as also studied in Appendix E). Since P(Γ) ≡ N (μ, σ2 (cid:62)x, σ2). θi ∼ P(Γ). We see that the actual data distribution for this node is pθi(y|x) = N (θi Clearly the set of such distributions {pθi(y|x)} cannot be written as any finite mixture as θi ∈ Rd and pθi(y|x) is a unimodal Gaussian distribution, with same parameter θi for all data generated in node i. Essentially the generative framework of finite mixtures (as in (Marfoq et al., 2021)) could be restrictive as it does not capture such parametric models.\n\nθ\n\nKnowledge distillation: The population distribution related to a regularizer based on KullbackLeibler divergence (knowledge distillation) has been shown in Appendix H. Therefore this can be cast in terms of information geometry where the probability falls of exponentially with in this geometry. Hence these connect to methods such as Lin et al. (2020); Li & Wang (2019); Shen et al. (2020); Ozkara et al. (2021), but the exact regularizer used does not take into account the full parametrization, and one can therefore improve upon these methods.\n\nFL with Multi-task Learning (MTL): In this framework, a fixed relationship between tasks is usually assumed (Smith et al., 2017). Therefore one can model this as a Gaussian model with known parameters relating the individual models. The individual models are chosen from a joint Gaussian with particular (known) covariance dictating the different models, and therefore giving the quadratic regularization used in FL-MTL (Smith et al., 2017). In this the parameters of the Gaussian model are known and fixed.\n\nj=1 Bw(i)\n\nCommon representations: The works in Du et al. (2021); Jain et al. (2021b) use a linear model where y ∼ N (x(cid:62)θi, σ2) can be considered a local generative model for node i. The common representation approach assumes that θi = (cid:80)k j , for some k (cid:28) d, where θi ∈ Rd. Therefore, one can parametrize a population by this (unknown) common basis B, and under a mild assumption that the weights are bounded, we can choose a uniform measure in this bounded cube to choose w(i) for each node i. The alternating optimization iteratively discovers the global common representation and the local weights as done in Du et al. (2021); Jain et al. (2021b) (and references therein). This common linear representation approach was generalized in Du et al. (2021); Collins et al. (2021) to neural networks, where a set of parameters to obtain a common representation (“head”) at each client was obtained and each local client appendd it with a “tail” combining the representation to obtain the final model. This also fits into our statistical framework, where the common representation (head) parameters are chosen from a population model (like the common subspace in the linear case) and the tail parameters are independently chosen (again as in the linear case).\n\nEmpirical and Hierarchical Bayes: As mentioned in Section 1, our work is inspired by empirical Bayes framework, introduced in (Stein, 1956; Robbins, 1956; James & Stein, 1961), which is the origin of hierarchical Bayes methods; see also (Gelman et al., 2013, pp. 132). (Stein, 1956; James & Stein, 1961) studied jointly estimating Gaussian individual parameters, generated by an unknown (parametrized) Gaussian population distribution. They showed a surprising result that one can enhance the estimate of individual parameters based on the observations of a population of Gaussian random variables with independently generated parameters from an unknown (parametrized) Gaussian population distribution. Effectively, this methodology advocated estimating the unknown population distribution using the individual independent samples, and then using it effectively as an empirical prior for individual estimates.9 This was studied for Bernoulli variables with heterogeneously generated individual parameters by Lord (1967) and the optimal error bounds for maximum likelihood estimates for population distributions were recently developed in (Vinayak et al., 2019). Hierarchical Bayes, builds on empirical Bayes framework and is sometimes associated with a fully Bayes method. Our choice to use empirical Bayes framework as the foundation is also because\n\n9This was shown to uniformly improve the mean-squared error averaged over the population, compared to an\n\nestimate using just the single local sample.\n\n34\n\nPublished as a conference paper at ICLR 2023\n\nit is more computationally feasible than a fully Bayes method. The subtle difference between the two is that empirical Bayes uses a point estimate of a (parametrized) prior, whereas, the terminology hierarchical Bayes often refers to a fully Bayes method where the (non-parametric) prior is estimated by computationally intensive methods like MCMC (see the discussion in (Blei et al., 2003)). As mentioned in Section 1, a contribution of our work is to connect a well studied statistical framework of empirical (hierarchical) Bayes to model heterogeneity in personalized federated learning. This statistical model yields a framework for personalized FL and leads to new algorithms and bounds especially in the local data starved regime.\n\nK ADDITIONAL DETAILS AND RESULTS FOR EXPERIMENTS\n\nK.1\n\nIMPLEMENTATION DETAILS\n\nIn this section we give further details on implementation and setting of the experiments that were used in Section 4.\n\nCIFAR-100 Experiment Setting. We do additional experiments on CIFAR-100. CIFAR-100 is a dataset consisting of 100 classes and 20 superclasses. Each superclass corresponds to a category of 5 classes (e.g. superclass flowers correspond to orchids, poppies, roses, sunflowers, tulips). To introduce heterogeneity we let each client sample data samples from 2 super classes (the classification task is still to classify among 100 classes). For classification on CIFAR-100 dataset we consider a 5-layer CNN with 2 convolutional layers of 64 filters and 5x5 filter size, following that we have 2 fully connected layers with activation sizes of 384,192 and finally an output layer of dimension 100. We set number of local epochs to be 2, batch size to be 25 per client; number of clients is 50, client participation K n = 0.2, and number of epochs 200 (100 communication rounds). In this new dataset the classification task is more complex given the increased number of labels.\n\nHyperparameters. We implemented Per-FedAvg and pFedMe based on the code from GitHub,10. Other implementations were not available online, so we implemented ourselves. For each of the methods we tuned learning rate in the set {0.3, 0.2, 0.15, 0.125, 0.1, 0.075, 0.05} and have a decaying learning schedule such that learning rate is multiplied by 0.99 at each epoch. We use weight decay of 1e − 4. For MNIST and FEMNIST experiments for both personalized and global models we used a 5-layer CNN, the first two layers consist of convolutional layers of filter size 5 × 5 with 6 and 16 filters respectively. Then we have 3 fully connected layers of dimension 256 × 120, 120 × 84, 84 × 10 and lastly a softmax operation. For CIFAR-10 experiments we use a similar CNN, the only difference is the first fully connected layer is of dimension 400 × 120.\n\n• AdaPeD11: We fine-tuned ψ in between 0.5 − 5 with 0.5 increments and set it to 3.5. We set η3 = 5e − 2. We manually prevent ψ becoming smaller than 0.5 so that local loss does not become dominated by the KD loss. We use η2 = 0.1 and η1 = 0.1. 12 When taking the derivative with respect to ψ we observed sometimes multiplying the right term (consist of KD loss function) by some constant (5 in our experiments) gives better performance.\n\n• Per-FedAvg Fallah et al. (2020) and pFedMe Dinh et al. (2020): For Per-FedAvg, we used 0.075 as the learning rate and α = 0.001. For pFedMe we used the same learning rate schedule for main learning rate, K = 3 for the number of local iterations; and we used λ = 0.5, η = 0.2.\n\n• QuPeD Ozkara et al. (2021): We choose λp = 0.25, η1 = 0.1 and η3 = 0.1 as stated.\n\n• Federated Mutual Learning Shen et al. (2020): Since authors do not discuss the hyperparameters in the paper, we used α = β = 0.25, global model has the same learning schedule as the personalized models.\n\nK.2 ADDITIONAL EXPERIMENTS\n\nConvergence plots for AdaPeD. We put the experimental convergence plots (test accuracy vs no. of iteration) for AdaPeD in Figure 2.\n\n10https://github.com/CharlieDinh/pFedMe 11For federated experiments we have used PyTorch’s Data Distributed Parallel package. 12We use https://github.com/tao-shen/FEMNIST_pytorch to import FEMNIST dataset.\n\n35\n\nPublished as a conference paper at ICLR 2023\n\n(a) AdaPeD Test Accuracy (in %) vs iteration on MNIST with 0.1 sampling ratio.\n\n(b) AdaPeD Test Accuracy (in %) vs iteration on FEMNIST with 0.33 sampling ratio.\n\nFigure 2: Convergence plots (test accuracy vs no. of iteration) for AdaPeD.\n\nPersonalized estimation: synthetic experiments in Bernoulli setting. For this setting, for P we consider three distributions that (Tian et al., 2017) considered: normal, uniform and ‘3-spike’ which have equal weight at 1/4, 1/2, 3/4. Additionally, we consider a Beta prior. We compute squared error of personalized estimators and local estimators ( Zi n ) w.r.t. the true pi and report the average over all clients. We use m = 10000 clients and 14 local samples similar to (Tian et al., 2017). Personalized estimator provides a decrease in MSE by 37.1 ± 3.9%, 12.0 ± 1.6%, 24.3 ± 2.8%, 34.0 ± 3.7%, respectively, for each aforementioned population distribution compared to their corresponding local estimators. Furthermore, as theoretically noted, less spread out prior distributions (low data heterogeneity) results in higher MSE gap between personalized and local estimators.\n\nLinear regression. For this, we create a setting similar to (Jain et al., 2021a). We set m = 10, 000, n = 10; and sample client true models according to a Gaussian centered at some randomly chosen μ with variance σ2 θ . We randomly generate design matrices Xi and create Yi at each client by adding a zero mean Gaussian noise with true variance σ2 x = 0.05 and we sample each component of μ from a Gaussian with 0 mean and 0.1 standard deviation and each component of X from a Gaussian with 0 mean and variance 0.05, both i.i.d. We measure the average MSE over all clients with and compare personalized and local methods. When d = 50, personalized regression has an MSE gain of 8.0 ± 0.8%, 14.8 ± 1.2%, and when d = 100, 9.2±1.1%, 12.3±2.0% compared to local and FedAvg regression, respectively. Moreover, compared to personalized regression where μ, σθ, σx are known, alternating algorithm only results in 1% and 4.7% increase in MSE respectively for d = 50 and d = 100.\n\nx to Xiθi. We set true values σ2\n\nθ = 0.01, σ2\n\nEstimation Experiments. We provide more results for the estimation setting discussed in Figure 1a. In Figure 3a we have a setting with 1000 clients and 5 local samples and in Figure 3b 500 clients and 5 local samples per client. We observe with as the number of clients increase DP-Personalized Estimator can converge to Personalized Estimator with less privacy budget. We also observe compared to Figure 1a, less number of local samples increases the performance discrepancy between personalized and local estimator.\n\n(a) Private Estimation with m=1000, n=5\n\n(b) Private Estimation with m=500, n=5\n\n36\n\nFigure 3: In Figure 1a, we plot MSE vs. (cid:15)0 for personalized estimation with different number of clients, this is the same setting as Figure 1a except the number of clients and local samples.\n\n01020304050iteration405060708090100Test Accuracy (in %)0510152025303540iteration20406080100Test Accuracy (in %)0.10.20.30.40.50.60.70.8ε012345MSE (in 1e-2)Local EstimatorPersonalized EstimatorDP Personalized Estimator0.10.20.30.40.50.60.70.8ε012345MSE (in 1e-2)Local EstimatorPersonalized EstimatorDP Personalized EstimatorPublished as a conference paper at ICLR 2023\n\nAdditional Learning Experiments with Different Number of Clients. We do additional experiments with different number of clients. On FEMNIST we use the same model and same data sample per client as in Section4, number of clients is 30, total number of epochs is 30 and we fix the local iteration to be 40 per epoch, we do full client sampling to simulate a cross-silo environment. As seen in Table 4, AdaPeD continues to outperform the competing methods following the trend in Section 4.\n\nTable 4: Test accuracy (in %) for FEMNIST with m = 30 clients.\n\nMethod\n\nFedAvg FedAvg+fine tuning Jiang et al. (2019) AdaPeD (Ours) pFedMe (Dinh et al., 2020) Per-FedAvg (Fallah et al., 2020) QuPeD (FP) (Ozkara et al., 2021) Federated ML (Shen et al., 2020)\n\nFEMNIST\n\n95.91 ± 0.78 96.22 ± 0.57 98.10 ± 0.09 96.03 ± 0.50 96.71 ± 0.14 97.72 ± 0.16 96.80 ± 0.13\n\nOn CIFAR-10 we use the same model as in Section4, and divide the dataset to 30 clients where each client has access to data samples from 4 classes. Total number of epochs is 250 and we fix the local iteration to be 40 per epoch; we set K n = 0.2 and number of local epochs to be 2. AdaPeD outperforms the competing methods in parallel to the experiments in Section4, as can be seen in Table 5.\n\nTable 5: Test accuracy (in %) for CIFAR-10 with m = 30 clients.\n\nMethod\n\nFedAvg FedAvg+fine tuning Jiang et al. (2019) AdaPeD (Ours) pFedMe (Dinh et al., 2020) Per-FedAvg (Fallah et al., 2020) QuPeD (FP) (Ozkara et al., 2021) Federated ML (Shen et al., 2020)\n\nCIFAR-10\n\n53.92 ± 0.94 67.44 ± 1.11 73.86 ± 0.39 71.97 ± 0.09 64.09 ± 0.46 73.21 ± 0.44 72.53 ± 0.36\n\nAdditional Experiment Implementation Details.\n\nWe use the same strategy as in Appendix K.1 to tune the main learning rates. We use 1e-4 weight decay.\n\n• AdaPeD: We fine-tuned ψ in between 0.5 − 5 with 0.5 increments and set it to 4 for CIFAR-10/100 and to 3 for FEMNIST. We manually prevent ψ becoming smaller than 1 so that local loss does not become dominated by the KD loss. We use η2 = 0.075 and η1 = 0.075 for CIFAR-10 and CIFAR-100 and η2 = 0.1 and η1 = 0.1 for FEMNIST.\n\n• Per-FedAvg (Fallah et al., 2020) and pFedMe (Dinh et al., 2020): For Per-FedAvg, we used 0.1 as the learning rate and α = 0.0001. For pFedMe we used the same learning rate schedule for main learning rate, L = 3 for the number of local approximation iterations; and we used λ = 0.1, η = 0.1.\n\n• QuPeD Ozkara et al. (2021): We set λp = 0.25, η1 = 0.1 for local learning rate and η2 = 0.1 for\n\nglobal learning rate.\n\n• Federated Mutual Learning Shen et al. (2020): Since authors do not discuss the hyperparameters in\n\nthe paper, we used α = β = 0.25.\n\n37",
    "reference": "# Summary Of The Paper\n\nThe paper proposes algorithms that search for suitable personalized models in a client-server type federated learning setup. The algorithms are inspired by the classical theory of parametric Bayesian risk minimization. In the personalized parameter estimation regime, the authors assume two population distributions: Gaussian and Bernoulli. Under Gaussian distribution, they report that in case the parent population becomes degenerate (i.e., variation tending to nullity), the global average turns out to be the ‘best’ estimator. Moreover, the posterior personalized mean estimator in this setup also turns out to be optimal in general. If the parent population follows a Bernoulli law, having sufficient observations from local sub-populations suggests against collaborating. The following ‘personalization’ algorithms utilize different prior distributions and regularization schemes to ensure client privacy.\n\n# Strength And Weaknesses\n\nStrength:\n\nThe language of the article is lucid, and the presentation is also of good quality. The discussion leading up to the theoretical analyses and the algorithms is precise. I find the statistical analysis rigorous and very well represented. Prior works and relevant references are well-placed throughout the paper. \n\nWeakness/Issues:\n\nThe authors have altered the standard structure of the article, as guided by ICLR instructions. The abstract should not be full page-wide. This is a violation of the code and gives them an undue advantage over others. \n\nThe current article looks incomplete, lacking a ‘Conclusion’ section. Also, sufficient discussion regarding limitations and future work is missing.\n\nI suggest the authors present accompanying codes maintaining anonymity. \n\nIt would be very helpful if the problem statement is presented more precisely in the introduction. The authors provide a lot of references to prior work. However, amidst such a crowd, the motivation somehow fades. \n\nAs I have acknowledged, the discussion is quite rigorous. However, the is significant room for improvement when it comes to organization. \n\nThe empirical results seem insufficient, and I suggest the authors put more datasets to test if feasible.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper presents an overall well investigated research work. To enhance clarity the authors need to answer the following questions:\n\nQuestions:\n\nWhat does it mean statistically to “estimate i through the help of the server.”?\n\nIn the section ‘Personalized Estimation’, should it be “client i outputs an estimate i of i” instead?\n\nDoes the prior distribution necessarily need to have a density in general? How ‘realistic’ are the special case assumptions of Gaussian and Bernoulli as global population distributions?\n\nIn the statement of Theorem 2., should it be “[-r,r]d” instead? \n\nDoes the notation Ɛ mean expectation [In Theorem 2, 3, etc.]? If so, kindly maintain any one symbol throughout. \nThe authors also use the same notation for a different purpose in Section B.2.1.\n\nShouldn’t we have the sum also over the quantity on the right-hand side of equation (10) [Section 3.1]?\n\nThe quantity it may penalize the heterogeneity, but does not denote the variance. The authors should call it something else instead [Section 3.2, last paragraph].\n\nIs the strict imposition of the value ‘1’ necessary in the definition of ‘neighborhood’ [Section A.3], since there is a clear possibility to generalize the result even if two datasets D and D' differ at multiple terms?\n\nIn Theorem 2, the upper bound on MSE  (5) loses its worth in a higher-dimensional setup. Can the authors talk about any remedy to the same?\n\n# Summary Of The Review\n\nThe paper may be considered for acceptance provided the authors address the above listed concerns.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nFROM t-SNE TO UMAP WITH CONTRASTIVE LEARNING\n\nSebastian Damrich IWR at Heidelberg University sebastian.damrich@uni-tuebingen.de\n\nJan Niklas B ̈ohm University of T ̈ubingen jan-niklas.boehm@uni-tuebingen.de\n\nFred A. Hamprecht IWR at Heidelberg University fred.hamprecht@iwr.uni-heidelberg.de\n\nDmitry Kobak University of T ̈ubingen dmitry.kobak@uni-tuebingen.de\n\nABSTRACT\n\nNeighbor embedding methods t-SNE and UMAP are the de facto standard for visualizing high-dimensional datasets. Motivated from entirely different viewpoints, their loss functions appear to be unrelated. In practice, they yield strongly differing embeddings and can suggest conflicting interpretations of the same data. The fundamental reasons for this and, more generally, the exact relationship between t-SNE and UMAP have remained unclear. In this work, we uncover their conceptual connection via a new insight into contrastive learning methods. Noisecontrastive estimation can be used to optimize t-SNE, while UMAP relies on negative sampling, another contrastive method. We find the precise relationship between these two contrastive methods and provide a mathematical characterization of the distortion introduced by negative sampling. Visually, this distortion results in UMAP generating more compact embeddings with tighter clusters compared to t-SNE. We exploit this new conceptual connection to propose and implement a generalization of negative sampling, allowing us to interpolate between (and even extrapolate beyond) t-SNE and UMAP and their respective embeddings. Moving along this spectrum of embeddings leads to a trade-off between discrete / local and continuous / global structures, mitigating the risk of over-interpreting ostensible features of any single embedding. We provide a PyTorch implementation.\n\n1\n\nINTRODUCTION\n\nLow-dimensional visualization of high-dimensional data is a ubiquitous step in exploratory data analysis, and the toolbox of visualization methods has been rapidly growing in the last years (McInnes et al., 2018; Amid & Warmuth, 2019; Szubert et al., 2019; Wang et al., 2021). Since all of these methods necessarily distort the true data layout (Chari et al., 2021), it is beneficial to have various tools at one’s disposal. But only equipped with a theoretic understanding of the aims of and relationships between different methods, can practitioners make informed decisions about which visualization to use for which purpose and how to interpret the results.\n\nThe state of the art for non-parametric, non-linear dimensionality reduction relies on the neighbor embedding framework (Hinton & Roweis, 2002). Its two most popular examples are t-SNE (van der Maaten & Hinton, 2008; van der Maaten, 2014) and UMAP (McInnes et al., 2018). Both can produce insightful, but qualitatively distinct embeddings. However, why their embeddings are different and what exactly is the conceptual relation between their loss functions, has remained elusive.\n\nHere, we answer this question and thus explain the mathematical underpinnings of the relationship between t-SNE and UMAP. Our conceptual insight naturally suggests a spectrum of embedding methods complementary to that of B ̈ohm et al. (2022), along which the focus of the visualization shifts from local to global structure (Fig. 1). On this spectrum, UMAP and t-SNE are simply two instances and inspecting various embeddings helps to guard against over-interpretation of apparent structure. As a practical corollary, our analysis identifies and remedies an instability in UMAP.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure 1: (a – e) Neg-t-SNE embedding spectrum of the MNIST dataset for various values of the fixed normalization constant ̄Z, see Sec. 5. As ̄Z increases, the scale of the embedding decreases, clusters become more compact and separated before eventually starting to merge. The Neg-t-SNE spectrum produces embeddings very similar to those of (f) t-SNE, (g) NCVis, and (h) UMAP, when ̄Z equals the partition function of t-SNE, the learned normalization parameter Z of NCVis, or |X|/m = (cid:0)n (cid:1)/m used by UMAP, as predicted in Sec. 4–6. (i) The partition function (cid:80) ij)−1 tries to match ̄Z and grows with it. Here, we initialized all Neg-t-SNE runs using ̄Z = |X|/m; without this ‘early exaggeration’, low values of ̄Z yield fragmented clusters (Fig. S11).\n\nij(1+d2\n\n2\n\nWe provide the new connection between t-SNE and UMAP via a deeper understanding of contrastive learning methods. Noise-contrastive estimation (NCE) (Gutmann & Hyv ̈arinen, 2010; 2012) can be used to optimize t-SNE (Artemenkov & Panov, 2020), while UMAP relies on another contrastive method, negative sampling (NEG) (Mikolov et al., 2013). We investigate the discrepancy between NCE and NEG, show that NEG introduces a distortion, and this distortion explains how UMAP and t-SNE embeddings differ. Finally, we discuss the relationship between neighbor embeddings and self-supervised learning (Wu et al., 2018; He et al., 2020; Chen et al., 2020; Le-Khac et al., 2020).\n\nIn summary, our contributions are\n\n1. a new connection between the contrastive methods NCE and NEG (Sec. 4), 2. the exact relation of t-SNE and UMAP and a remedy for an instability in UMAP (Sec. 6), 3. a spectrum of ‘contrastive’ neighbor embeddings encompassing UMAP and t-SNE (Sec. 5), 4. a connection between neighbor embeddings and self-supervised learning (Sec. 7), 5. a unified PyTorch framework for contrastive (non-)parametric neighbor embedding methods.\n\nOur code is available at https://github.com/berenslab/contrastive-ne and https://github.com/hci-unihd/cl-tsne-umap.\n\n2 RELATED WORK\n\nOne of the most popular methods for data visualization is t-SNE (van der Maaten & Hinton, 2008; van der Maaten, 2014). Recently developed NCVis (Artemenkov & Panov, 2020) employs noisecontrastive estimation (Gutmann & Hyv ̈arinen, 2010; 2012) to approximate t-SNE in a samplingbased way. Therefore, we will often refer to the NCVis algorithm as ‘NC-t-SNE’. UMAP (McInnes et al., 2018) has matched t-SNE’s popularity at least in computational biology (Becht et al., 2019) and uses another sampling-based optimization method, negative sampling (Mikolov et al., 2013), also employed by LargeVis (Tang et al., 2016). Other recent sampling-based visualization methods include TriMap (Amid & Warmuth, 2019) and PaCMAP (Wang et al., 2021).\n\nGiven their success, t-SNE and UMAP have been scrutinized to find out which aspects are essential to their performance. Initialization was found to be strongly influencing the global structure\n\n2\n\nDefault NEGPublished as a conference paper at ICLR 2023\n\nin both methods (Kobak & Linderman, 2021). The exact choice of the low-dimensional similarity kernel (B ̈ohm et al., 2022) or the weights of the k-nearest-neighbor graph (Damrich & Hamprecht, 2021) were shown to be largely inconsequential. Both algorithms have similar relevant hyperparameters such as, e.g., the heavy-tailedness of the similarity kernel (Yang et al., 2009; Kobak et al., 2019).\n\nWhile not obvious from the original presentations, the central difference between t-SNE and UMAP can therefore only be in their loss functions, which have been studied by Damrich & Hamprecht (2021); B ̈ohm et al. (2022); Wang et al. (2021), but never conceptually connected. We achieve this by deepening the link between negative sampling (NEG) and noise-contrastive estimation (NCE).\n\nNEG was introduced as an ad hoc replacement for NCE in the context of learning word embeddings (Mikolov et al., 2013). The relationship between NEG and NCE has been discussed before (Dyer, 2014; Levy & Goldberg, 2014; Ruder, 2016; Ma & Collins, 2018; Le-Khac et al., 2020), but here we go further and provide the precise meaning of NEG: We show that, unlike NCE, NEG learns a model proportional but not equal to the true data distribution.\n\nBoth t-SNE and UMAP have parametric versions (van der Maaten, 2009; Sainburg et al., 2021) with very different logic and implementations. Here we present a unified PyTorch framework for nonparametric and parametric contrastive neighbor embeddings. As special cases, it includes UMAP and t-SNE approximations with NCE (like NCVis) and the InfoNCE loss (Jozefowicz et al., 2016; van den Oord et al., 2018), which has not yet been applied to neighbor embeddings.\n\nOur resulting InfoNC-t-SNE elucidates the relationship between SimCLR (Chen et al., 2020) and neighbor embeddings. The parallel work of Hu et al. (2023) makes a qualitatively similar argument, but does not discuss negative samples. Balestriero & LeCun (2022) show how SimCLR recovers Isomap (Tenenbaum et al., 2000), while we connect SimCLR to more modern neighbor embeddings.\n\n3 BACKGROUND\n\n3.1 NOISE-CONTRASTIVE ESTIMATION (NCE)\n\nThe goal of parametric density estimation is to fit a parametric model qθ to iid samples s1, . . . , sN from an unknown data distribution p over a space X. For maximum likelihood estimation (MLE) the parameters θ are chosen to maximize the log-likelihood of the observed samples\n\nθ∗ = arg max\n\nθ\n\n(cid:88)N\n\ni=1\n\nlog (cid:0)qθ(si)(cid:1) .\n\n(1)\n\nThis approach crucially requires qθ to be a normalized model. It is otherwise trivial to increase the likelihood arbitrarily by scaling qθ. To circumvent the expensive computation of the partition function Z(θ) = (cid:80) x∈X qθ(x), Gutmann & Hyv ̈arinen (2010; 2012) introduced NCE. It turns the unsupervised problem of density estimation into a supervised problem in which the data samples need to be identified from a set containing the N data samples and m times as many noise samples t1, . . . tmN drawn from a noise distribution ξ, e.g., the uniform distribution over X. Briefly, NCE fits θ by minimizing the binary cross-entropy between the true class assignment and posterior probabilities P(data | x) = qθ(x)/(cid:0)qθ(x)+mξ(x)(cid:1) and P(noise | x) = 1−P(data | x) (Supp. A.1):\n\n(cid:34)\n\nθ∗ = arg min\n\n−\n\nθ\n\n(cid:18)\n\nlog\n\nN (cid:88)\n\ni=1\n\nqθ(si) qθ(si) + mξ(si)\n\n(cid:19)\n\n−\n\nmN (cid:88)\n\ni=1\n\n(cid:18)\n\nlog\n\n1 −\n\nqθ(ti) qθ(ti) + mξ(ti)\n\n(cid:19)(cid:35)\n\n.\n\n(2)\n\nThe key advantage of NCE is that the model does not need to be explicitly normalized by the partition function, but nevertheless learns to equal the data distribution p and hence be normalized:\n\nTheorem 1 (Gutmann & Hyv ̈arinen (2010; 2012)). Let ξ have full support and suppose there exists some θ∗ such that qθ∗ = p. Then θ∗ is a minimum of (cid:19)\n\n(cid:18)\n\n(cid:19)\n\n(cid:18)\n\nqθ(s) qθ(s) + mξ(s)\n\n− mEt∼ξ log\n\n1 −\n\nqθ(t) qθ(t) + mξ(t)\n\n(3)\n\nLNCE(θ) = −Es∼p log\n\nand the only other extrema of LNCE are minima ̃θ which also satisfy q ̃θ = p.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nIn NCE, the model typically includes an optimizable normalization parameter Z which we emphasize by writing qθ,Z = qθ/Z. But importantly, Thm. 1 applies to any model qθ that is able to match the data distribution p, even if it does not contain a learnable normalization parameter.\n\nIn the setting of learning language models, Jozefowicz et al. (2016) proposed a different version of NCE, called InfoNCE. Instead of classifying samples as data or noise, the aim here is to predict the position of a data sample in an (m + 1)-tuple containing m noise samples and one data sample (Supp. A.2). For the uniform noise distribution ξ, this yields the expected loss function\n\nLInfoNCE(θ) = −\n\nE x∼p x1,...,xm∼ξ\n\nlog\n\n(cid:18)\n\nqθ(x) qθ(x) + (cid:80)m\n\ni=1 qθ(xi)\n\n(cid:19)\n\n.\n\n(4)\n\nMa & Collins (2018) showed that an analogue of Thm. 1 applies to InfoNCE.\n\n3.2 NEIGHBOR EMBEDDINGS\n\nNeighbor embeddings (NE) (Hinton & Roweis, 2002) are a group of dimensionality reduction methods, including UMAP, NCVis, and t-SNE that aim to find a low-dimensional embedding e1, . . . , en ∈ Rd of high-dimensional input points x1, . . . , xn ∈ RD, with D ≫ d and usually d = 2 for visualization. NE methods define a notion of similarity over pairs of input points which encodes the neighborhood structure and informs the low-dimensional embedding.\n\nThe exact high-dimensional similarity distribution differs between the NE algorithms, but recent work (B ̈ohm et al., 2022; Damrich & Hamprecht, 2021) showed that t-SNE and UMAP results stay practically the same when using the binary symmetric k-nearest-neighbor graph (skNN) instead of t-SNE’s Gaussian or UMAP’s Laplacian similarities. An edge ij is in skNN if xi is among the k nearest neighbors of xj or vice versa. The high-dimensional similarity function is then given by p(ij) = 1(ij ∈ skNN)/|skNN|, where |skNN| denotes the number of edges in the skNN graph and 1 is the indicator function. NCVis uses the same similarities. We will always use k = 15.\n\nThere are further differences in the choice of low-dimensional similarity between t-SNE and UMAP, but B ̈ohm et al. (2022) showed that they are negligible. Therefore, here we use the Cauchy kernel φ(dij) = 1/(d2 ij + 1) for all NE methods to transform distances dij = ∥ei − ej∥ in the embedding space into low-dimensional similarities. We abuse notation slightly by also writing φ(ij) = φ(dij).\n\nAll NE methods in this work can be cast in the framework of parametric density estimation. Here, p is the data distribution to be approximated with a model qθ, meaning that the space X on which both p and qθ live is the set of all pairs ij with 1 ≤ i < j ≤ n. The embedding positions e1, . . . , en become the learnable parameters θ of the model qθ. For t-SNE, NC-t-SNE (NCVis), and UMAP, qθ is proportional to φ(∥ei − ej∥), but the proportionality factor and the loss function are different.\n\nt-SNE uses MLE and therefore requires a normalized model qθ(ij) = φ(ij)/Z(θ), where Z(θ) = (cid:80)\n\nk̸=l φ(kl) is the partition function. The loss function\n\nLt-SNE(θ) = −Eij∼p log (cid:0)qθ(ij)(cid:1) = −\n\n(cid:88)\n\ni̸=j\n\n(cid:16)\n\np(ij) log (cid:0)φ(ij)(cid:1)(cid:17)\n\n+ log\n\n(cid:16) (cid:88)\n\nk̸=l\n\n(cid:17)\n\nφ(kl)\n\n(5)\n\nis the expected negative log-likelihood of the embedding positions θ, making t-SNE an instance of MLE. Usually t-SNE’s loss function is introduced as the Kullback-Leibler divergence between p and qθ, which is equivalent as the entropy of p does not depend on θ.\n\nNC-t-SNE uses NCE and optimizes the expected loss function\n\nLNC-t-SNE(θ, Z) = −Eij∼p log\n\n(cid:18)\n\nqθ,Z(ij) qθ,Z(ij) + mξ(ij)\n\n(cid:19)\n\n− mEij∼ξ log\n\n(cid:18)\n\n1 −\n\nqθ,Z(ij) qθ,Z(ij) + mξ(ij)\n\n(cid:19)\n\n,\n\n(6)\n\nwhere qθ,Z(ij) = φ(ij)/Z with learnable Z and ξ is approximately uniform (see Supp. D).\n\nAccording to Thm. 1, NC-t-SNE has the same optimum as t-SNE and can hence be seen as a sampling-based approximation of t-SNE. Indeed, we found that Z in NC-t-SNE and the partition function Z(θ) in t-SNE converge approximately to the same value (Fig. S10).\n\nUMAP’s expected loss function is derived in Damrich & Hamprecht (2021):\n\nLUMAP(θ) = −Eij∼p log (cid:0)qθ(ij)(cid:1) − mEij∼ξ log (cid:0)1 − qθ(ij)(cid:1),\n\n(7)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nwith qθ(ij) = φ(ij) and ξ is approximately uniform, see Supp. D. This is the effective loss function actually implemented in the UMAP algorithm, but note that it has only about m/n of the repulsion compared to the loss stated in the original UMAP paper (McInnes et al., 2018), as shown in B ̈ohm et al. (2022) and Damrich & Hamprecht (2021) (Supp. C).\n\nIn practice, the expectations in UMAP’s and NC-t-SNE’s loss functions are evaluated via sampling, like in Eq. (2). This leads to a fast, O(n), stochastic gradient descent optimization scheme. Both loss functions are composed of an attractive term pulling similar points (edges of the skNN graph) closer together and a repulsive term pushing random pairs of points further apart. Similarly, t-SNE’s loss yields attraction along the graph edges while repulsion arises through the normalization term.\n\n4 FROM NOISE-CONTRASTIVE ESTIMATION TO NEGATIVE SAMPLING\n\nIn this section we work out the precise relationship between NCE and NEG, going beyond prior work (Dyer, 2014; Goldberg & Levy, 2014; Levy & Goldberg, 2014; Ruder, 2016). NEG differs from NCE by its loss function and by the lack of the learnable normalization parameter Z. In our setting, NEG’s loss function amounts to1\n\nLNEG(θ) = −Ex∼p log\n\n(cid:18) qθ(x)\n\n(cid:19)\n\nqθ(x) + 1\n\n− mEx∼ξ log\n\n(cid:18)\n\n1 −\n\nqθ(x) qθ(x) + 1\n\n(cid:19)\n\n.\n\n(8)\n\nIn order to relate it to NCE’s loss function, our key insight is to generalize the latter by allowing to learn a model that is not equal but proportional to the true data distribution. Corollary 2. Let ̄Z, m ∈ R+. Let ξ have full support and suppose there exist some θ∗ such that qθ∗ = ̄Zp. Then θ∗ is a minimum of the generalized NCE loss function\n\nLNCE\n\n ̄Z (θ) = −Ex∼p log\n\n(cid:18)\n\nqθ(x) qθ(x) + ̄Zmξ(x)\n\n(cid:19)\n\n− mEx∼ξ log\n\n(cid:18)\n\n1 −\n\nqθ(x) qθ(x) + ̄Zmξ(x)\n\n(cid:19)\n\n(9)\n\nand the only other extrema of LNCE Proof. The result follows from Thm. 1 applied to the model distribution ̃qθ := qθ/ ̄Z.\n\nare minima ̃θ which also satisfy q ̃θ = ̄Zp.\n\n ̄Z\n\nDyer (2014) and Ruder (2016) pointed out that for a uniform noise distribution ξ(x) = 1/|X| and as many noise samples as the size of X (m = |X|), the loss functions of NCE and NEG coincide, since mξ(x) = 1. However, the main point of NCE and NEG is to use far fewer noise samples in order to attain a speed-up over MLE. Our Cor. 2 for the first time explains NEG’s behavior in this more realistic setting (m ≪ |X|). If the noise distribution is uniform, the generalized NCE loss function with ̄Z = |X|/m equals the NEG loss function since (|X|/m)mξ(x) = 1. By Cor. 2, any minimum θ∗ of the NEG loss function yields qθ∗ = (|X|/m)p, assuming that there are parameters that make this equation hold. In other words, NEG aims to find a model qθ that is proportional to the data distribution with the proportionality factor |X|/m which is typically huge. This is different from NCE, which aims to learn a model equal to the data distribution.\n\nChoosing m ≪ |X| does not only offer a computational speed-up but is necessary when optimizing NEG for a neural network with SGD, like we do in Sec.7. Only one single mini-batch is passed through the neural network during each iteration and is thus available for computing the loss. Hence, all noise samples must come from the current mini-batch and their number m is upper-bounded by the mini-batch size b. Mini-batches are typically much smaller than |X|. Thus, this common training procedure necessitates m ≪ |X| highlighting the relevance of Cor. 2. While NCE uses a model qθ/Z with learnable Z, we can interpret NEG as using a model qθ/ ̄Z with fixed and very large normalization constant ̄Z = |X|/m. As a result, qθ in NEG needs to attain much larger values to match the large ̄Z. This can be illustrated in the setting of neighbor embeddings. Applying NEG to the neighbor embedding framework yields an algorithm that we call ‘Neg-t-SNE’. Recall that in this setting, |X| = (cid:0)n (cid:1) is the number of pairs of points. B ̈ohm et al. (2022) found empirically that t-SNE’s partition function Z(θ) is typically between 50n and 100n, while in Neg-t-SNE, ̄Z = O(n2) is much larger for modern big datasets. To attain the larger values of φ(ij) required by NEG, points that are connected in the skNN graph have to move much closer\n\n2\n\n1We focus on the loss function, ignoring Mikolov et al. (2013)’s choices specific to word embeddings.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n(a) UMAP, no annealing (b) UMAP with annealing (c) Neg-t-SNE, no ann.\n\n(d) Neg-t-SNE with ann.\n\nFigure 2: Embeddings of the MNIST dataset with UMAP and Neg-t-SNE with and without learning rate annealing in our implementation. UMAP does not work well without annealing because it implicitly uses the diverging 1/d2 ij kernel in NEG, while Neg-t-SNE uses the more numerically stable Cauchy kernel (Sec. 6). UMAP’s reference implementation also requires annealing, see Figs. S1a, d.\n\ntogether in the embedding than in t-SNE. Indeed, using our PyTorch implementation of Neg-t-SNE on the MNIST dataset, we confirmed that Neg-t-SNE (Fig. 1d) produced more compact clusters than t-SNE (Fig. 1f). See Supp. K and Alg. S1 for implementation details.\n\nWe emphasize that our analysis only holds because NEG has no learnable normalization parameter Z. If it did, then such a Z could absorb the term ̄Z in Eq. (9), leaving the parameters θ∗ unchanged.\n\n5 NEGATIVE SAMPLING SPECTRUM\n\nVarying the fixed normalization constant ̄Z in Eq. (9) has important practical effects that lead to a whole spectrum of embeddings in the NE setting. The original NEG loss function Eq. (8) corresponds to Eq. (9) with ̄Z = |X|/m. We still refer to the more general case of using an arbitrary ̄Z in Eq. (9) as ‘negative sampling’ and ‘Neg-t-SNE’ in the context of neighbor embeddings. Figs. 1a–e show a spectrum of Neg-t-SNE visualizations of the MNIST dataset for varying ̄Z. Per Cor. 2, higher values of ̄Z induce higher values for qθ, meaning that points move closer together. Indeed, the scale of the embedding decreases for higher ̄Z as evident from the scale bars in the bottom-right corner of each plot. Moreover, clusters become increasingly compact and then even start to merge. For lower values of ̄Z the embedding scale is larger and clusters are more spread out. Eventually, clusters lose almost any separation and start to overlap for very small ̄Z. Cor. 2 implies that the partition function (cid:80) x qθ(x) should grow with ̄Z, and indeed this is what we observed (Fig. 1i). The match between the sum of Cauchy kernels (cid:80) ij φ(ij) and ̄Z was not perfect, but that was expected: The Cauchy kernel is bounded by 1 from above, so values ̄Z > (cid:0)n (cid:1) are not matchable. Similarly, very small values of ̄Z are difficult to match because of the heavy tail of the Cauchy kernel. See Supp. I for a toy example where the match is perfect. By adjusting the ̄Z value, one can obtain Neg-t-SNE embeddings very similar to NC-t-SNE and t-SNE. If the NC-t-SNE loss function Eq. (6) has its minimum at some θ∗ and Z NC-t-SNE, then the Neg-t-SNE loss function Eq. (9) with ̄Z = Z NC-t-SNE is minimal at the same θ∗. We confirmed this experimentally: Setting ̄Z = Z NC-t-SNE, yields a Neg-t-SNE embedding (Fig. 1c) closely resembling that of NC-t-SNE (NCVis) (Fig. 1g). Similarly, setting ̄Z to the partition function Z(θt-SNE), obtained by running t-SNE, yields a Neg-t-SNE embedding closely resembling that of t-SNE (Figs. 1b, f). We used kNN recall and correlation between pairwise distances for quantitative confirmation (Supp. H).\n\n2\n\nstrongly related to the\n\nThe Neg-t-SNE spectrum is attraction-repulsion spectrum of B ̈ohm et al. (2022). They introduced a prefactor (‘exaggeration’) to the attractive term in t-SNE’s loss, which increases the attractive forces, and obtained embeddings similar to our spectrum when varying this parameter. We can explain this as follows. The repulsive term in the NCE loss Eq. (3) has a prefactor m and our spectrum arises from the loss Eq. (9) by varying ̄Z in the term ̄Zm. Equivalently, our spectrum can be obtained by varying the m value (number of noise samples per one skNN edge) while holding ̄Zm fixed (Fig. S8). In other words, our spectrum arises from varying the repulsion strength in the contrastive setting, while B ̈ohm et al. (2022) obtained the analogous spectrum by varying the repulsion strength in the t-SNE setting (see also Supp. B.2).\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n6 UMAP’S CONCEPTUAL RELATION TO t-SNE\n\nOur comparison of NEG and NCE in Sec. 4 allows us for the first time to conceptually relate UMAP and t-SNE. Originally, McInnes et al. (2018) motivated the UMAP algorithm by a specific choice of weights on the skNN graph and the binary cross-entropy loss. Following LargeVis (Tang et al., 2016), they implemented a sampling-based scheme which they referred to as ‘negative sampling’, although UMAP’s loss function Eq. (7) does not look like the NEG loss Eq. (8). Thus, it has been unclear if UMAP actually uses Mikolov et al. (2013)’s NEG. Here, we settle this question: Lemma 3. UMAP’s loss function (Eq. 7) is NEG (Eq. 8) with the parametric model ̃qθ(ij) = 1/d2\n\nij.\n\nProof.\n\nIndeed, qθ(ij) = 1/(1 + d2\n\nij) = (1/d2\n\nij)/(1/d2\n\nij + 1) = ̃qθ(ij)/(cid:0) ̃qθ(ij) + 1(cid:1).\n\nLem. 3 tells us that UMAP uses NEG but not with a parametric model given by the Cauchy kernel. Instead it uses a model ̃qθ, which equals the inverse squared distance between embedding points.\n\nFor large embedding distances dij both models behave alike, but for nearby points they strongly differ: The inverse-square kernel 1/d2 ij). Despite this qualitative difference, we found empirically that UMAP embeddings look very similar to Neg-t-SNE embeddings at ̄Z = |X|/m, see Figs. 1d and h for the MNIST example.\n\nij diverges when dij → 0, unlike the Cauchy kernel 1/(1+d2\n\nit\n\nis instructive to compare the loss terms of Neg-t-SNE and To explain this observation, ij + 1)(cid:1) = log(1 + d2 UMAP: The attractive term amounts to − log (cid:0)1/(d2 ij) for UMAP and − log (cid:2)1/(d2 ij) for Neg-t-SNE, while the repulsive term equals log (cid:0)(1 + d2 ij)(cid:1), respectively. While the attractive terms ij)/(1 + d2 are similar, the repulsive term for UMAP diverges at zero but that of Neg-t-SNE does not (Fig. S7, Supp. B.2). This divergence introduces numerical instability into the optimization process of UMAP.\n\nij + 1) + 1(cid:1)(cid:3) = log(2 + d2 (cid:1) and log (cid:0)(2 + d2\n\nij + 1)/(cid:0)1/(d2\n\nij)/d2\n\nij\n\nWe found that UMAP strongly depends on annealing its learning rate down to zero (Figs. 2a, b). Without it, clusters appear fuzzy as noise pairs can experience very strong repulsion and get catapulted out of their cluster (Fig. 2a). While Neg-t-SNE also benefits from this annealing scheme (Fig. 2d), it produces a very similar embedding even without any annealing (Fig. 2c). Thus, UMAP’s effective choice of the 1/d2 ij kernel makes it less numerically stable and more dependent on optimization tricks, compared to Neg-t-SNE.2 See Supp. E for more details.\n\nOur conclusion is that at its heart, UMAP is NEG applied to the t-SNE framework. UMAP’s sampling-based optimization is much more than a mere optimization trick; it enables us to connect it theoretically to t-SNE. When UMAP’s loss function is seen as an instance of NEG, UMAP does not use the Cauchy kernel but rather the inverse-square kernel. However, this does not make a strong difference due to the learning rate decay. As discussed in Sec. 4, the fixed normalization constant ̄Z in Neg-t-SNE / UMAP is much larger than the learned Z in NC-t-SNE or t-SNE’s partition function. This explains why UMAP pulls points closer together than both NC-t-SNE and t-SNE and is the reason for the typically more compact clusters in UMAP plots (B ̈ohm et al., 2022).\n\n7 CONTRASTIVE NE AND CONTRASTIVE SELF-SUPERVISED LEARNING\n\nContrastive self-supervised representation learning (van den Oord et al., 2018; Tian et al., 2020; He et al., 2020; Chen et al., 2020; Caron et al., 2020) and ‘contrastive neighbor embeddings’ (a term we suggest for NC-t-SNE, Neg-t-SNE, UMAP, etc.) are conceptually very similar. The key difference is that the latter use a fixed kNN graph to find pairs of similar objects, while the former rely on data augmentations or context to generate pairs of similar objects on the fly. Other differences include the representation dimension (∼ 128 vs. 2), the use of a neural network for parametric mapping, the flavor of contrastive loss (InfoNCE vs. NCE / NEG), and the number of noise samples m. However, these other differences are not crucial, as we establish here with our unified PyTorch framework.\n\nAs an example, we demonstrate that t-SNE can also be optimized using the InfoNCE loss, resulting in ‘InfoNC-t-SNE’ (Supp. K and Alg. S1). Its result on MNIST was similar to that of NC-t-SNE (Figs. 3b, c). For the default number of noise samples, m = 5, the embeddings of both\n\n2Recently, a pull request to UMAP’s GitHub repository effectively changed the kernel of parametric UMAP\n\nto the Cauchy kernel, in order to overcome numerical instabilities via an ad hoc fix, see Supp. E.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n(a) Neg-t-SNE\n\n(b) NC-t-SNE\n\n(c) InfoNC-t-SNE\n\n(d) Param. Neg-t-SNE\n\n(e) Param. NC-t-SNE (f) Param. InfoNC-t-SNE\n\nFigure 3: NE embeddings of MNIST are qualitatively similar in the non-parametric (top) and parametric settings (bottom). We used our PyTorch framework with m = 5 and batch size b = 1024.\n\nalgorithms were visibly different from t-SNE proper (Fig. 1f). Recent work in self-supervised learning (Chen et al., 2020) reports improved performance for more noise samples, in agreement with the theory of Gutmann & Hyv ̈arinen (2012) and Ma & Collins (2018). We observed that both NC-t-SNE and InfoNC-t-SNE with m = 500 approximated t-SNE much better (Figs. S18k and S19k). Like t-SNE, but unlike the m = 5 setting, m = 500 required early exaggeration to prevent cluster fragmentation (Figs. S18 and S19). We discuss InfoNC-t-SNE’s relation to TriMap in Supp. F.\n\nNext, we used our PyTorch framework to obtain parametric versions of all contrastive NE algorithms discussed here (NC-t-SNE, InfoNC-t-SNE, Neg-t-SNE, UMAP). We used a fully connected neural network with three hidden layers as a parametric RD → Rd mapping and optimized its parameters using Adam (Kingma & Ba, 2015) (Supp. K). We used batch size b = 1024 and sampled all m negative samples from within the batch; the dataset was shuffled each epoch before batching. Using all four loss functions, we were able to get parametric embeddings of MNIST that were qualitatively similar to their non-parametric versions (Fig. 3). The parametric versions of NCE and InfoNCE produced much larger embeddings than their non-parametric counterparts, however the final loss values were very similar (Figs. S9a, b). For NCE, the larger scale of the parametric embedding was compensated by a smaller learned normalization parameter Z, so that both parametric and non-parametric versions were approximately normalized (Fig. S9c). Our parametric UMAP implementation is very similar to that of Sainburg et al. (2021). But our parametric, approximate t-SNE implementations strongly differ from the parametric t-SNE of van der Maaten (2009), which constructed separate kNN graphs within each batch and optimized the vanilla t-SNE loss function.\n\nNeighbor embeddings methods achieve impressive results with only few noise samples, while existent common practice in the self-supervised learning literature (Bachman et al., 2019; Chen et al., 2020; He et al., 2020) recommends very large m. As a proof of concept, we demonstrate that using only m = 16 non-curated (Wu et al., 2017; Roth et al., 2020) noise samples can suffice for imagebased self-supervised learning. We used a SimCLR setup (Chen et al., 2020) to train representations of the CIFAR-10 dataset (Krizhevsky, 2009) using a ResNet18 (He et al., 2016) backbone, a fixed batch size of b = 1024 and varying number of noise samples m. Other implementation details\n\nTable 1: Test set classification accuracies on CIFAR-10 representations obtained with InfoNCE loss and batch size b = 1024 saturate already at a low number of noise samples m, common for neighbor embeddings. We used the ResNet18 output H ∈ R512 and report mean ± std across three seeds.\n\nm = 2\n\nm = 16\n\nm = 128\n\nm = 512\n\nm = 2b − 2\n\nkNN classifier Linear classifier\n\n86.9 ± 0.2 90.7 ± 0.2\n\n91.7 ± 0.1 93.1 ± 0.2\n\n92.0 ± 0.3 93.3 ± 0.3\n\n92.2 ± 0.2 93.2 ± 0.3\n\n91.9 ± 0.1 93.3 ± 0.1\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nfollow Chen et al. (2020)’s CIFAR-10 setup (Supp. K.4). The classification accuracy, measured on the ResNet output, saturates already at m = 16 noise samples (Tab. 1). Note that, different from the original SimCLR setup, we decoupled the batch size from the number of noise samples. Recent work found conflicting evidence when varying m for a fixed batch size (Mitrovic et al., 2020; Nozawa & Sato, 2021; Ash et al., 2022). Future research is needed to perform more systematic benchmarks into the importance of m. Here, we present these results only as a proof of principle that a similarly low m as in neighbor embeddings (default m = 5) can work in a SimCLR setting.\n\n8 DISCUSSION AND CONCLUSION\n\nIn this work, we studied the relationship between two popular unsupervised learning methods, noisecontrastive estimation (NCE) and negative sampling (NEG). We focused on their application to neighbor embeddings (NE) because this is an active and important application area, but also because NEs allow to directly visualize the NCE / NEG outcome and to form intuitive understanding of how different algorithm choices affect the result. Our study makes three conceptual advances.\n\nFirst, we showed that NEG replaces NCE’s learnable normalization parameter Z by a large constant ̄Z, forcing NEG to learn a scaled data distribution. While not explored here, this implies that NEG can be used to learn probabilistic models and is not only applicable for embedding learning as previously believed (Dyer, 2014; Ruder, 2016; Le-Khac et al., 2020). In the NE setting, NEG led to the method Neg-t-SNE that differs from NCVis / NC-t-SNE (Artemenkov & Panov, 2020) by a simple switch from the learnable to a fixed normalization constant. We argued that this can be a useful hyperparameter because it moves the embedding along the attraction-repulsion spectrum similar to B ̈ohm et al. (2022) and hence can either emphasize more discrete structure with higher repulsion (Fig. S16) or more continuous structure with higher attraction (see Figs. S13–S14 for developmental single-cell datasets). Our quantitative evaluation in Supp. H corroborates this interpretation. We believe that inspection of several embeddings along the spectrum can guard against over-interpreting any single embedding. Exploration of the spectrum does not require specialized knowledge. For UMAP, we always have ̄Z UMAP = (cid:0)n (cid:1)/m; for t-SNE, B ̈ohm et al. (2022) found that the partition function Z t-SNE typically lies in [50n, 100n]. Our PyTorch package allows the user to move along the spectrum with a slider parameter s such that s=0 and s=1 correspond to ̄Z = Z t-SNE and ̄Z = ̄Z UMAP, respectively, without a need for specifying ̄Z directly.\n\n2\n\nA caveat is that Thm. 1 and Cor. 2 both assume the model to be rich enough to perfectly fit the data distribution. This is a strong and unrealistic assumption. In the context of NEs, the data distribution is zero for most pairs of points, which is impossible to match using the Cauchy kernel. Nevertheless, the consistency of MLE and thus t-SNE also depends on this assumption. Even without it, the gradients of MLE, NCE, and NEG are very similar (Supp. B). Finally, we validated our Cor. 2 in a toy setting in which its assumptions do hold (Supp. I).\n\nSecond, we demonstrated that UMAP, which uses NEG, differs from Neg-t-SNE only in UMAP’s implicit use of a less numerically stable similarity function. This isolates the key aspect of UMAP’s success: Instead of UMAP’s appraised (Oskolkov, 2022; Coenen & Pearce, 2022) high-dimensional similarities, the refined Cauchy kernel, or its stated cross-entropy loss function, it is the application of NEG that lets UMAP perform well and makes clusters in UMAP plots more compact and connections between them more continuous than in t-SNE, in agreement with B ̈ohm et al. (2022). To the best of our knowledge, this is the first time UMAP’s and t-SNE’s loss functions, motivated very differently, have been conceptually connected.\n\nThird, we argued that contrastive NEs are closely related to contrastive self-supervised learning (SSL) methods such as SimCLR (Chen et al., 2020) which can be seen as parametric InfoNC-t-SNE for learning representations in S128 based on the unobservable similarity graph implicitly constructed via data augmentations. We feel that this connection has been underappreciated, with the literature on NEs and on self-supervised contrastive learning staying mostly disconnected. Exceptions are the concurrent works of Hu et al. (2023) and B ̈ohm et al. (2023). They propose to use t-SNE’s Cauchy kernel for SSL. Instead, we bridge the gap between NE and SSL with our InfoNC-t-SNE, as well as with parametric versions of all considered NE methods, useful for adding out-of-sample data to an existing visualization. Moreover, we demonstrated that the feasibility of few noise samples in NE translates to the SimCLR setup. We developed a concise PyTorch framework optimizing all of the above, which we hope will facilitate future dialogue between the two research communities.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nWe thank Philipp Berens for comments and support as well as James Melville for discussions.\n\nThis research was funded by the Deutsche Forschungsgemeinschaft (DFG, Germany’s Research Foundation) via SFB 1129 (240245660; S.D., F.A.H.) as well as via Excellence Clusters 2181/1 “STRUCTURES” (390900948; S.D., F.A.H.) and 2064 “Machine Learning: New Perspectives for Science” (390727645; J.N.B., D.K.), by the German Ministry of Education and Research (T ̈ubingen AI Center, 01IS18039A; J.N.B., D.K.), and by the Cyber Valley Research Fund (D.30.28739; J.N.B.).\n\nThe authors thank the International Max Planck Research School for Intelligent Systems (IMPRSIS) for supporting J.N.B.\n\nETHICS STATEMENT\n\nOur work analyses connections between popular contrastive learning methods and in particular visualization methods. Visualization methods are fundamental in exploratory data analysis, e.g., in computational biology. Given the basic nature of visualization it can of course potentially be used for malicious goals, but we expect mostly positive societal effects such as more reliable exploration of biological data.\n\nSimilarly, contrastive self-supervised learning has the potential to overcome the annotation bottleneck that machine learning faces. This might free countless hours of labelling for more productive use but could also enable institutions with sufficient resources to learn from larger and larger datasets, potentially concentrating power.\n\nREPRODUCIBILITY\n\nAll of our results are reproducible. We included proofs for the theoretical statement Cor. 2 and those in Supp. B and D. The datasets used in the experiments including preprocessing steps are described in Supp. J. We discuss the most important implementation details in the main text, e.g., in Sec. 7. An extensive discussion of our implementation is included in Supp. K and in Alg. S1.\n\nOur code and instructions for how to reproduce the experiments are publicly available. We separated the implementation of contrastive neighbor embeddings from the scripts and notebooks needed to reproduce our results, to provide dedicated repositories for people interested in using our method and people wanting to reproduce our results. The contrastive neighbor embedding implementation can be found at https://github.com/berenslab/contrastive-ne and the scripts and notebooks for reproducing our results at https://github.com/hci-unihd/cl-tsne-umap. The relevant commits in both repositories are tagged iclr2023.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nEhsan Amid and Manfred K Warmuth. TriMap: Large-scale Dimensionality Reduction Using\n\nTriplets. arXiv preprint arxiv: 1910.00204, 2019.\n\nAleksandr Artemenkov and Maxim Panov. NCVis: Noise Contrastive Approach for Scalable Visu-\n\nalization. In Proceedings of The Web Conference 2020, pp. 2941–2947, 2020.\n\nJordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Dipendra Misra. Investigating the Role of Negatives in Contrastive Representation Learning. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research, pp. 7187–7209, 2022.\n\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning Representations by Maximizing Mutual Information across Views. In Advances in Neural Information Processing Systems, volume 32, pp. 15535–15545, 2019.\n\nRandall Balestriero and Yann LeCun. Contrastive and Non-Contrastive Self-Supervised Learning In Advances in Neural Information\n\nRecover Global and Local Spectral Embedding Methods. Processing Systems, 2022.\n\nEtienne Becht, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel WH Kwok, Lai Guan Ng, Florent Ginhoux, and Evan W Newell. Dimensionality reduction for visualizing single-cell data using UMAP. Nature Biotechnology, 37(1):38–44, 2019.\n\nJan Niklas B ̈ohm, Philipp Berens, and Dmitry Kobak. Attraction-Repulsion Spectrum in Neighbor\n\nEmbeddings. Journal of Machine Learning Research, 23(95):1–32, 2022.\n\nJan Niklas B ̈ohm, Philipp Berens, and Dmitry Kobak. Unsupervised visualization of image datasets\n\nusing contrastive learning. In International Conference on Learning Representations, 2023.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. In Advances in Neural Information Processing Systems, volume 33, pp. 9912–9924, 2020.\n\nDavid M Chan, Roshan Rao, Forrest Huang, and John F Canny. t-SNE-CUDA: GPU-Accelerated t-SNE and its Applications to Modern Data. In 2018 30th International Symposium on Computer Architecture and High Performance Computing, pp. 330–338. IEEE, 2018.\n\nTara Chari, Joeyta Banerjee, and Lior Pachter. The Specious Art of Single-Cell Genomics. bioRxiv,\n\n2021.\n\nBenjamin Charlier, Jean Feydy, Joan Alexis Glaun`es, Franc ̧ois-David Collin, and Ghislain Durif. Kernel Operations on the GPU, with Autodiff, without Memory Overflows. Journal of Machine Learning Research, 22(74):1–6, 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for Contrastive Learning of Visual Representations. In Proceedings of the International Conference on Machine Learning, Proceedings of Machine Learning Research, pp. 1597–1607, 2020.\n\nAndy Coenen and Adam Pearce. A deeper dive into UMAP theory. https://pair-code. github.io/understanding-umap/supplement.html, 2022. Accessed: 2022-05-11.\n\nSebastian Damrich and Fred A Hamprecht. On UMAP’s True Loss Function. In Advances in Neural\n\nInformation Processing Systems, volume 34, pp. 5798–5809, 2021.\n\nAaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, volume 27, 2014.\n\nAndrew Draganov, Tyrus Berry, Jakob Rødsgaard Jørgensen, Katrine Scheel Nellemann, Ira Assent, and Davide Mottin. GiDR-DUN; Gradient Dimensionality Reduction–Differences and Unification. arXiv preprint arXiv:2206.09689, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nChris Dyer. Notes on Noise Contrastive Estimation and Negative Sampling. arXiv preprint arxiv:\n\n1410.8251, 2014.\n\nPeter Eisenmann. Fast Visualization of High-Dimensional Data via Parallelized UMAP on GPUs.\n\nMaster’s thesis, Karlsruhe Institue of Technology, 2019.\n\nYoav Goldberg and Omer Levy. word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling\n\nWord-Embedding Method. arXiv preprint arxiv: 1402.3722, 2014.\n\nMichael U Gutmann and Aapo Hyv ̈arinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 297–304, 2010.\n\nMichael U Gutmann and Aapo Hyv ̈arinen. Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics. Journal of Machine Learning Research, 13(2), 2012.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised Visual Representation Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729–9738, 2020.\n\nGeoffrey E Hinton and Sam Roweis. Stochastic Neighbor Embedding.\n\nIn Advances in Neural\n\nInformation Processing Systems, volume 15, pp. 857–864, 2002.\n\nTianyang Hu, Zhili Liu, Fengwei Zhou, Wenjia Wang, and Weiran Huang. Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding. In International Conference on Learning Representations, 2023.\n\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\n\nLimits of Language Modeling. arXiv preprint arxiv: 1602.02410, 2016.\n\nSabina Kanton, Michael James Boyle, Zhisong He, Malgorzata Santel, Anne Weigert, F ́atima Sanch ́ıs-Calleja, Patricia Guijarro, Leila Sidow, Jonas Simon Fleck, Dingding Han, et al. Organoid single-cell genomic atlas uncovers human-specific features of brain development. Nature, 574(7778):418–422, 2019.\n\nDiederik P Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings\n\nof the International Conference on Learning Representations, pp. 1–15, 2015.\n\nDmitry Kobak and Philipp Berens. The art of using t-SNE for single-cell transcriptomics. Nature\n\nCommunications, 10(1):1–14, 2019.\n\nDmitry Kobak and George C Linderman. Initialization is critical for preserving global data structure\n\nin both t-SNE and UMAP. Nature Biotechnology, pp. 1–2, 2021.\n\nDmitry Kobak, George Linderman, Stefan Steinerberger, Yuval Kluger, and Philipp Berens. HeavyTailed Kernels Reveal a Finer Cluster Structure in t-SNE Visualisations. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 124–139. Springer, 2019.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University\n\nof Toronto, 2009.\n\nPhuc H Le-Khac, Graham Healy, and Alan F Smeaton. Contrastive Representation Learning: A\n\nFramework and Review. IEEE Access, 8:193907–193934, 2020.\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n\nOmer Levy and Yoav Goldberg. Neural Word Embedding as Implicit Matrix Factorization.\n\nIn\n\nAdvances in Neural Information Processing Systems, volume 27, pp. 2177–2185, 2014.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nGeorge C Linderman, Manas Rachh, Jeremy G Hoskins, Stefan Steinerberger, and Yuval Kluger. Fast interpolation-based t-SNE for improved visualization of single-cell RNA-seq data. Nature Methods, 16(3):243–245, 2019.\n\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic Gradient Descent with Warm Restarts. Pro-\n\nceedings of the International Conference on Learning Representations, pp. 1–16, 2017.\n\nZhuang Ma and Michael Collins. Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3698–3707, 2018.\n\nLeland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation and\n\nProjection for Dimension Reduction. arXiv preprint arXiv:1802.03426, 2018.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed RepresenIn Advances in Neural Information\n\ntations of Words and Phrases and their Compositionality. Processing Systems, volume 26, pp. 3111–3119, 2013.\n\nJovana Mitrovic, Brian McWilliams, and Melanie Rey. Less can be more in contrastive learning. In\n\nProceedings on ”I Can’t Believe It’s Not Better!” at NeurIPS Workshops, pp. 70–75, 2020.\n\nAshwin Narayan, fBonnie Berger, and Hyunghoon Cho. Assessing single-cell transcriptomic variability through density-preserving data visualization. Nature Biotechnology, pp. 1–10, 2021.\n\nCorey J Nolet, Victor Lafargue, Edward Raff, Thejaswi Nanditale, Tim Oates, John Zedlewski, and Joshua Patterson. Bringing UMAP Closer to the Speed of Light with GPU Acceleration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 418–426, 2021.\n\nKento Nozawa and Issei Sato. Understanding Negative Samples in Instance Discriminative SelfIn Advances in Neural Information Processing Systems,\n\nsupervised Representation Learning. volume 34, pp. 5784–5797, 2021.\n\nNikolay Oskolkov. How Exactly UMAP Works. https://towardsdatascience.com/\n\nhow-exactly-umap-works-13e3040e1668, 2022. Accessed: 2022-05-11.\n\nJonathan S Packer, Qin Zhu, Chau Huynh, Priya Sivaramakrishnan, Elicia Preston, Hannah Dueck, Derek Stefanik, Kai Tan, Cole Trapnell, Junhyong Kim, Robert H. Waterston, and John I. Murray. A lineage-resolved molecular atlas of C. elegans embryogenesis at single-cell resolution. Science, 365(6459):1265–1274, 2019.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32, pp. 8024– 8035, 2019.\n\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.\n\nPavlin G. Poliˇcar, Martin Straˇzar, and Blaˇz Zupan. openTSNE: a modular Python library for t-SNE\n\ndimensionality reduction and embedding. bioRxiv, 2019.\n\nKarsten Roth, Timo Milbich, and Bjorn Ommer. PADS: Policy-Adapted Sampling for Visual Similarity Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6568–6577, 2020.\n\nSebastian Ruder. On word embeddings - Part 2: Approximating the Softmax. http://ruder.\n\nio/word-embeddings-softmax, 2016. Accessed: 2022-05-17.\n\nTim Sainburg, Leland McInnes, and Timothy Q Gentner. Parametric UMAP Embeddings for Rep-\n\nresentation and Semisupervised Learning. Neural Computation, 33(11):2881–2907, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nBenjamin Szubert, Jennifer E Cole, Claudia Monaco, and Ignat Drozdov. Structure-preserving\n\nvisualisation of high dimensional single-cell datasets. Scientific Reports, 9(1):1–10, 2019.\n\nJian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. Visualizing large-scale and highIn Proceedings of the 25th International Conference on World Wide Web,\n\ndimensional data. pp. 287–297, 2016.\n\nClanuwat Tarin, B Mikel, K Asanobu, L Alex, Y Kazuaki, and H David. Deep Learning for Classical Japanese Literature. In Proceedings of 2018 Workshop on Machine Learning for Creativity and Design (Thirty-second Conference on Neural Information Processing Systems), volume 3, 2018.\n\nJoshua B Tenenbaum, Vin De Silva, and John C Langford. A Global Geometric Framework for\n\nNonlinear Dimensionality Reduction. Science, 290(5500):2319–2323, 2000.\n\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive Multiview Coding. In Proceedings of\n\nthe European Conference on Computer Vision, pp. 776–794. Springer, 2020.\n\nA ̈aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Pre-\n\ndictive Coding. arXiv e-prints, pp. arXiv–1807, 2018.\n\nLaurens van der Maaten. Learning a Parametric Embedding by Preserving Local Structure.\n\nIn Artificial Intelligence and Statistics, Proceedings of Machine Learning Research, pp. 384–391, 2009.\n\nLaurens van der Maaten. Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine\n\nLearning Research, 15(1):3221–3245, 2014.\n\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine\n\nLearning Research, 9(11):2579–2605, 2008.\n\nDaniel E. Wagner, Caleb Weinreb, Zach M. Collins, James A. Briggs, Sean G. Megason, and Allon M. Klein. Single-cell mapping of gene expression landscapes and lineage in the zebrafish embryo. Science, 360(6392):981–987, 2018.\n\nYingfan Wang, Haiyang Huang, Cynthia Rudin, and Yaron Shaposhnik. Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMAP, and PaCMAP for Data Visualization. Journal of Machine Learning Research, 22:1–73, 2021.\n\nChao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Krahenbuhl. Sampling Matters in Deep Embedding Learning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2840–2848, 2017.\n\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised Feature Learning via NonParametric Instance Discrimination. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3733–3742, 2018.\n\nZhirong Yang, Irwin King, Zenglin Xu, and Erkki Oja. Heavy-Tailed Symmetric Stochastic Neighbor Embedding. In Advances in Neural Information Processing Systems, pp. 2169–2177, 2009.\n\nZhirong Yang, Jaakko Peltonen, and Samuel Kaski. Scalable optimization of neighbor embedding\n\nfor visualization. In International Conference on Machine Learning, pp. 127–135, 2013.\n\nZhirong Yang, Yuwei Chen, Denis Sedov, Samuel Kaski, and Jukka Corander. Stochastic cluster\n\nembedding. Statistics and Computing, 33(1):12, 2023.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nSUPPLEMENTARY TEXT\n\nA PROBABILISTIC FRAMEWORKS OF NCE AND INFONCE\n\nIn this section we recap the probabilistic frameworks of NCE (Gutmann & Hyv ̈arinen, 2010; 2012) and InfoNCE (Jozefowicz et al., 2016; van den Oord et al., 2018).\n\nA.1 NCE\n\nIn NCE the unsupervised problem of parametric density estimation is turned into a supervised problem in which the data samples need to be identified in a set S containing N data samples s1, . . . , sN and m times as many noise samples t1, . . . , tmN drawn from a noise distribution ξ, which can be (but does not have to be) the uniform distribution. In other words, we are interested in the posterior probability P(y|x) of element x ∈ S coming from the data (y = data) rather than from the noise distribution (y = noise). The probability of sampling x from noise, P(x|noise), is just the noise distribution ξ, and similarly P(x|data) is the data distribution p. Since the latter is unknown, it is replaced by the model qθ. Since S contains m times as many noise samples as data samples, the prior class probabilities are P(data) = 1/(m + 1) and P(noise) = m/(m + 1). Thus, the unconditional probability of an element of S is P(x) = (qθ(x) + mξ(x))/(m + 1). The posterior probability for classifying some given element x of S as data rather than noise is thus\n\nP(data|x) =\n\nP(x|data)P(data) P(x)\n\n=\n\nqθ(x) qθ(x) + mξ(x)\n\n.\n\n(10)\n\nNCE optimizes the parameters θ by maximizing the log-likelihood of the posterior class distributions, or, equivalently, by minimizing the negative log-likelihoods. This is the same as a sum over binary cross-entropy losses (Eq. (2) in the main text):\n\n(cid:34)\n\nθ∗ = arg min\n\n−\n\nθ\n\n(cid:18)\n\nlog\n\nN (cid:88)\n\ni=1\n\nqθ(si) qθ(si) + mξ(si)\n\n(cid:19)\n\n−\n\nmN (cid:88)\n\ni=1\n\n(cid:18)\n\nlog\n\n1 −\n\nqθ(ti) qθ(ti) + mξ(ti)\n\n(cid:19)(cid:35)\n\n.\n\n(11)\n\nIn expectation, we have the loss function (Eq. (4) in the main text)\n\nLNCE(θ) = −Es∼p log\n\n(cid:18)\n\nqθ(s) qθ(s) + mξ(s)\n\n(cid:19)\n\n− mEt∼ξ log\n\n(cid:18)\n\n1 −\n\nqθ(t) qθ(t) + mξ(t)\n\n(cid:19)\n\n.\n\n(12)\n\nSince qθ(x)/(cid:0)qθ(x) + mξ(x)(cid:1) = 1/\n\n(cid:104)\n\n1 +\n\n(cid:16)\n\nqθ(x)/(cid:0)mξ(x)(cid:1)(cid:17)−1(cid:105)\n\n, NCE’s loss function can also be\n\nseen as binary logistic regression loss function with log\n\n(cid:16) qθ(x)\n\n(cid:17)\n\nmξ(x)\n\nas the input to the logistic function:\n\nLNCE(θ) = −Es∼p log\n\n(cid:18)\n\n(cid:18)\n\nσ\n\nlog\n\n(cid:18) qθ(s) mξ(s)\n\n(cid:19)(cid:19)(cid:19)\n\n− mEt∼ξ log\n\n(cid:18)\n\n(cid:18)\n\n1 − σ\n\nlog\n\n(cid:19)(cid:19)(cid:19)\n\n(cid:18) qθ(t) mξ(t)\n\n,\n\n(13)\n\nwhere σ(x) = 1/(cid:0)1 + exp(−x)(cid:1) is the logistic function.\n\nA.2\n\nINFONCE\n\nAgain, we are casting the unsupervised problem of density estimation as a supervised classification problem. We consider a tuple of m + 1 samples T = (x0, . . . , xm) one of which comes from the data and the rest from the noise distribution. Instead of classifying each sample independently as noise or data (as in Sec. A.1), here we are interested in identifying the position of the single data sample. This allows us to see the problem as a multi-class classification problem with m + 1 classes.\n\nLet Y be the random variable that holds the index of the data sample. A priori, we have P(Y = k) = 1/(m + 1) for all k = 0, . . . , m. Moreover, conditioned on sample k coming from the data distribution, all other samples must come from the noise distribution, i.e., we\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nhave P(xi|Y = k) = ξ(xi) for i ̸= k. As the data distribution is unknown, we model it with P(xk|Y = k) = qθ(xk) as in Sec. A.1. This yields the likelihood of tuple T given the data index Y = k\n\nP(T |Y = k) = qθ(xk)\n\n(cid:89)\n\ni̸=k\n\nξ(xi) =\n\nqθ(xk) ξ(xk)\n\nMarginalizing over Y , we obtain\n\nm (cid:89)\n\ni=0\n\nξ(xi).\n\nP(T ) =\n\n1 m + 1\n\nm (cid:89)\n\ni=0\n\nξ(xi)\n\nm (cid:88)\n\nk=0\n\nqθ(xk) ξ(xk)\n\n.\n\nFinally, we can compute the posterior via Bayes’ rule as\n\nP(Y = k|T ) =\n\nP(T |Y = k)P(Y = k) P(T )\n\n=\n\nqθ(xk) ξ(xk)\n\n(cid:80)m\n\ni=0\n\nqθ(xi) ξ(xi)\n\n=\n\nqθ(xk) i=0 qθ(xi)\n\n(cid:80)m\n\n,\n\n(14)\n\n(15)\n\n(16)\n\nwhere the last equality only holds for the uniform noise distribution. The InfoNCE loss is the crossentropy loss with respect to the true position of the data sample, i.e., in expectation and for uniform ξ it reads:\n\nLInfoNCE(θ) = −\n\nE x∼p x1,...,xm∼ξ\n\nlog\n\n(cid:18)\n\nqθ(x) qθ(x) + (cid:80)m\n\ni=1 qθ(xi)\n\n(cid:19)\n\n.\n\n(17)\n\nSimilar to how the NCE loss can be seen as binary logistic regression loss function (Sec. A.1), the InfoNCE loss can be viewed as multinomial logistic regression loss function with the terms\n\nlog\n\n(cid:17)\n\n(cid:16) qθ(xi) ξ(xi)\n\nentering the softmax function.\n\nB GRADIENTS\n\nB.1 GRADIENTS OF MLE, NCE, AND NEG\n\na\n\nHere, we compute the gradients of MLE, NCE, and NEG, elaborating the discusFollowing the discussion in Secs. 3.2 and 4, sion in Artemenkov & Panov (2020). function consider we Z(θ) = (cid:80) x′ φθ(x′) for MLE, a model qθ,Z(x) = φθ(x)/Z with learnable normalization parameter Z for NCE, and a model qθ, ̄Z(x) = φθ(x)/ ̄Z with fixed normalization constant ̄Z for NEG. We show the results both for general ̄Z and noise distribution ξ and for the default value ̄Z = |X|/m and a uniform noise distribution ξ(x) = 1/|X|.\n\nφθ(x)/Z(θ) with\n\nnormalized model\n\npartition\n\nqθ(x)\n\n=\n\nThus, the losses are\n\nLMLE(θ) = −Ex∼p log (cid:0)qθ(x)(cid:1),\n\nLNCE(θ, Z) = −Ex∼p log\n\nLNEG(θ, ̄Z) = −Ex∼p log\n\n(cid:18)\n\n(cid:18)\n\nqθ,Z(x) qθ,Z(x) + mξ(x) qθ, ̄Z(x) qθ, ̄Z(x) + mξ(x)\n\n(cid:19)\n\n(cid:19)\n\n− mEx∼ξ log\n\n− mEx∼ξ log\n\n(cid:18)\n\n(cid:18)\n\n1 −\n\n1 −\n\nqθ,Z(x) qθ,Z(x) + mξ(x) qθ, ̄Z(x) qθ, ̄Z(x) + mξ(x)\n\n(cid:19)\n\n(cid:19)\n\n,\n\n,\n\nLNEG(θ) = −Ex∼p log\n\n(cid:18) φθ(x)\n\n(cid:19)\n\nφθ(x) + 1\n\n− mEx∼ξ log\n\n(cid:18)\n\n1 −\n\nφθ(x) φθ(x) + 1\n\n(cid:19)\n\n.\n\nFor MLE, we find\n\ndLMLE(θ) dθ\n\n=\n\n=\n\nd dθ\n\nd dθ\n\n(cid:0)−Ex∼p log (cid:0)qθ(x)(cid:1)(cid:1) (cid:32)\n\n(cid:88)\n\n(cid:16)\n\np(x) log (cid:0)φθ(x)(cid:1)(cid:17)\n\n−\n\n+ log\n\n(cid:16) (cid:88)\n\n(cid:17)\n\nφθ(x)\n\n(cid:33)\n\n= −\n\n= −\n\n(cid:88)\n\nx\n\n(cid:88)\n\nx\n\nx\n\n(cid:18) p(x) φθ(x) (cid:18) p(x) qθ(x)\n\n(cid:19)\n\n·\n\ndφθ(x) dθ\n\n−\n\n(cid:80)\n\n1\n\nx φθ(x)\n\n(cid:19)\n\n− 1\n\n·\n\n1 Z(θ)\n\n·\n\ndφθ(x) dθ\n\n.\n\nx\n\n(cid:88)\n\n·\n\nx\n\ndφθ(x) dθ\n\n16\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\nPublished as a conference paper at ICLR 2023\n\nFor the NCE loss we will use the templates\n\nd log\n\n(cid:16) f (z)\n\n(cid:17)\n\nf (z)+c\n\ndz\n\n=\n\n(cid:16)\n\nd log\n\n(cid:17)\n\n1 1+c/f (z)\n\ndz\n\nd log(1 + c/f (z)) dz\n\n= −\n\n= −\n\n=\n\n·\n\n1 1 + c/f (z) 1\nf (z)\n\nc f (z) + c\n\n·\n\n·\n\ndf (z) dz\n\n−c\n\nf (z)2 ·\n\ndf (z) dz\n\n,\n\nd log\n\n(cid:16) c\n\n(cid:17)\n\nf (z)+c\n\ndz\n\n= −\n\n= −\n\n= −\n\nd log (f (z)/c + 1) dz\n\ndf (z) dz\n\n1 f (z)/c + 1\n\n1 f (x) + c\n\n·\n\n·\n\n·\n\n1 c\ndf (z) dz\n\nwith some differentiable function f (z) and c independent of z.\n\nFor the NCE loss we get\n\ndLNCE(θ, Z) dθ\n\n=\n\n=\n\n(cid:18)\n\n(cid:18)\n\nd dθ\n\nd dθ\n\n−Ex∼p log\n\n−Ex∼p log\n\n(cid:18)\n\n(cid:18)\n\nqθ,Z(x) qθ,Z(x) + mξ(x)\n\nqθ,Z(x) qθ,Z(x) + mξ(x)\n\n(cid:19)\n\n(cid:19)\n\n− mEx∼ξ log\n\n(cid:18)\n\n1 −\n\nqθ,Z(x) qθ,Z(x) + mξ(x)\n\n− mEx∼ξ log\n\n(cid:18)\n\nmξ(x) qθ,Z(x) + mξ(x)\n\n(cid:19)(cid:19)\n\n(cid:88)\n\n= −\n\n(cid:20)\n\np(x)\n\nx\n\nmξ(x) qθ,Z(x) + mξ(x)\n\n1 qθ,Z(x)\n\ndqθ,Z(x) dθ\n\n+mξ(x)\n\n= −\n\n(cid:18) p(x)\n\n(cid:88)\n\nqθ,Z(x)\n\nx\n\n(cid:21)\n\ndqθ,Z(x) dθ\n\n−1 qθ,Z(x) + mξ(x) mξ(x) qθ,Z(x) + mξ(x)\n\n− 1\n\n(cid:19)\n\n1 Z\n\ndφθ(x) dθ\n\n.\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\n(32)\n\n(cid:19)(cid:19)\n\n(33)\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\nWe used our templates at the third equality sign with f (z) = qθ,Z(x) and c = mξ(x). The derivation of the gradients for the NEG loss is entirely analogous to that for NCE with qθ,Z(x) replaced by qθ, ̄Z(x).\n\nTogether, the gradients of the three losses are\n\ndLMLE(θ) dθ\n\n= −\n\ndLNCE(θ, Z) dθ\n\ndLNEG(θ, ̄Z) dθ\n\n= −\n\n= −\n\ndLNEG(θ) dθ\n\n= −\n\n(cid:88)\n\nx∈X\n\n(cid:88)\n\nx∈X\n\n(cid:88)\n\nx∈X\n\n(cid:88)\n\nx∈X\n\n(cid:18) p(x) qθ(x) (cid:18) p(x)\n\nqθ,Z(x)\n\n(cid:18) p(x)\n\nqθ, ̄Z(x)\n\n(cid:19)\n\n− 1\n\n1 Z(θ)\n\n(cid:19)\n\n− 1\n\n(cid:19)\n\n− 1\n\nmξ(x) qθ,Z(x) + mξ(x)\n\nmξ(x) qθ, ̄Z(x) + mξ(x)\n\n1 Z\n\n1 ̄Z\n\n(cid:18) p(x) φθ(x)\n\n−\n\nm |X|\n\n(cid:19)\n\n1 φθ(x) + 1\n\ndφθ(x) dθ\n\ndφθ(x) dθ\n\ndφθ(x) dθ\n\ndφθ(x) dθ\n\n,\n\n,\n\n,\n\n.\n\n(38)\n\n(39)\n\n(40)\n\n(41)\n\nThe fractions involving mξ(x) in the gradients of LNCE and LNEG tend to one as m → ∞ highlighting that a higher number of noise samples improves NCE’s approximation of MLE (Gutmann\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n& Hyv ̈arinen, 2010; 2012; Artemenkov & Panov, 2020). All four gradients are the same up to this factor and the different choice of normalization. In all of them, the models qθ, qθ,Z, or qθ, ̄Z try to match the data distribution p according to the first factor in each gradient. This similarity of the gradients holds independent of the assumptions of Thm. 1 and Cor. 2. We optimize all losses with stochastic gradient descent and can therefore expect our analysis to hold even if the assumptions are violated.\n\nB.2 GRADIENTS FOR NEIGHBOR EMBEDDINGS\n\nHere, we rewrite the above gradients for the case of neighbor embeddings. This means that MLE, NCE, and NEG become t-SNE, NC-t-SNE, and Neg-t-SNE, respectively. We include two variants of Neg-t-SNE: for a general value of ̄Z and for the default value ̄Z = (cid:0)n (cid:1)/m used by UMAP. In addition, we also show the gradient of UMAP. Recall that for neighbor embedding, we have X = {ij|1 ≤ i < j ≤ n}, θ = {e1, . . . , en}, and φ(ij) = 1/(1 + ∥ei − ej∥2). The noise distributions are close to uniform, see Supp. D, so that ξ(ij) ≈ (cid:0)n\n\n(cid:1)−1\n\n2\n\n.\n\n2\n\ndLt-SNE dei\n\n= 2\n\ndLNC-t-SNE dei\n\n= 2\n\ndLNeg-t-SNE( ̄Z) dei\n\n= 2\n\ndLNeg-t-SNE dei\n\n≈ 2\n\ndLUMAP dei\n\n= 2\n\n(cid:88)\n\nj̸=i\n\n(cid:88)\n\nj̸=i\n\n(cid:88)\n\nj̸=i\n\n(cid:88)\n\nj̸=i\n\n(cid:88)\n\nj̸=i\n\nmξ(ij)\n\nφ(ij)\n\nZ + mξ(ij) (cid:125) (cid:123)(cid:122) →1 for m→∞ (cid:125)(cid:124) mξ(ij)\n\n(cid:123)\n\n(cid:124)\n\n(cid:122)\n\nφ(ij) ̄Z\n\n+ mξ(ij)\n\n1 φ(ij) + 1 (cid:124) (cid:125) (cid:123)(cid:122) ∈[0.5,1]\n\n(cid:18)\n\np(ij) −\n\n(cid:19)\n\nφ(ij) k̸=l φ(kl)\n\n(cid:80)\n\n(cid:18)\n\np(ij) −\n\n(cid:19)\n\nφ(ij) Z\n\n(cid:18)\n\np(ij) −\n\n(cid:18)\n\np(ij) −\n\n(cid:18)\n\np(ij) −\n\n(cid:19)\n\nφ(ij) ̄Z\n\n(cid:19)\n\nφ(ij) (cid:1)/m (cid:0)n 2\n\nφ(ij) (cid:1)/m (cid:0)n 2\n\n(cid:19)\n\n1 1 − φ(ij) (cid:125) (cid:123)(cid:122) (cid:124) numerically unstable for ei ≈ ej\n\nφ(ij)(ej − ei)\n\n(42)\n\nφ(ij)(ej − ei)\n\n(43)\n\nφ(ij)(ej − ei)\n\n(44)\n\nφ(ij)(ej − ei)\n\n(45)\n\nφ(ij)(ej − ei)\n\n(46)\n\nAs above, we see that NC-t-SNE and Neg-t-SNE have terms that go to one for m → ∞. For Neg-t-SNE with default ̄Z, we used the approximation ξ(ij) ≈ (cid:0)n . Here, the corresponding term is restricted to the interval [0.5, 1]. More generally, we see that the repulsive force in Neg-t-SNE scales as 1/ ̄Z. Its default value of n(n − 1)/(2m) is much larger than the typical values of the partition function (cid:80) k̸=l φ(kl) in t-SNE or the learned Z of NC-t-SNE, which are typically in the range [50n, 100n] (B ̈ohm et al., 2022), leading to much smaller repulsion in default Neg-t-SNE and in UMAP than in NC-t-SNE or t-SNE. Finally, the repulsive part of the UMAP gradient contains a term that diverges for ei ≈ ej, leading to φ(ij) ≈ 1, reflecting the discussion of UMAP’s numerical instability in Sec. 6.\n\n(cid:1)−1\n\n2\n\nC UMAP’S LOSS FUNCTION\n\nIn the original UMAP paper, McInnes et al. (2018) define weights μ(ij) ∈ [0, 1] on the skNN graph and state that these shall be reproduced by the low-dimensional similarities φ(ij) by means of a sum of binary cross-entropy loss functions, one for each edge ij\n\n(cid:88)\n\n(cid:104)\n\nμ(ij) log (cid:0)φ(ij)(cid:1) + (cid:0)1 − μ(ij)(cid:1) log (cid:0)1 − φ(ij)(cid:1)(cid:105)\n\n.\n\n−\n\n(47)\n\nij\n\nIndeed, this loss has its minimum at μ(ij) = φ(ij) for all ij. However, it is of course not possible to achieve zero loss for any real-world data using the Cauchy kernel in two dimensions. Experiments show that using this loss function in practice leads to an excess of repulsion and consequently to very\n\n18\n\nPublished as a conference paper at ICLR 2023\n\npoor embeddings (B ̈ohm et al., 2022). The actual UMAP implementation has much less repulsion due to the sampling of repulsive edges, see below.\n\nAs the weights μ(ij) are only supported on the sparse skNN graph, most of the 1 − μ(ij) terms are equal to one. To simplify the loss function, the UMAP paper replaces all 1 − μ(ij) terms by 1, leading to the loss function\n\n(cid:88)\n\n(cid:104)\n\nμ(ij) log (cid:0)φ(ij)(cid:1) + log (cid:0)1 − φ(ij)(cid:1)(cid:105)\n\n.\n\n−\n\n(48)\n\nij\n\nIn the implementation, UMAP samples the repulsive edges, which drastically changes the loss (B ̈ohm et al., 2022) to the effective loss (Damrich & Hamprecht, 2021)\n\n(cid:88)\n\n(cid:104) μ(ij) log (cid:0)φ(ij)(cid:1) +\n\n−\n\nij\n\nm(di + dj) 2n\n\nlog (cid:0)1 − φ(ij)(cid:1)(cid:105)\n\n,\n\n(49)\n\nwhere di = (cid:80) j μ(ij) denotes the degree of node i and the number of negative samples m is a hyperparameter. By default, m = 5. Since di ≈ log(k), the effective loss only has about m log(k)/n of the repulsion in the originally stated loss function. As a result, the μ(ij) are not reproduced in the embedding space by this loss function.\n\nWe rewrite this effective loss function further to fit into our framework. The attractive prefactors μ(ij) sum to (cid:80) ij μ(ij), while the repulsive prefactors add up to m times this factor. Dividing the entire loss function by this term does not change its properties. But then, we can write the ij μ(ij) and ξ(ij) = (cid:0)p(i) + p(j)(cid:1)/2n prefactors as probability distributions p(ij) = μ(ij)/ (cid:80) using p(i) = (cid:80)\n\nj p(ij). With this, we can write the effective loss function as\n\n(cid:88)\n\n−\n\np(ij) log (cid:0)φ(ij)(cid:1) − m\n\n(cid:88)\n\nξ(ij) log (cid:0)1 − φ(ij)(cid:1),\n\nij\n\nij\n\nor in the expectation form as\n\n−Eij∼p log (cid:0)φ(ij)(cid:1) − mEij∼ξ log (cid:0)1 − φ(ij)(cid:1),\n\n(50)\n\n(51)\n\nlike we do in Eq. (7).\n\nD NOISE DISTRIBUTIONS\n\nHere we discuss the various noise distributions used by UMAP, NCVis, and our framework. The main claim is that all these noise distributions are sufficiently close to uniform, even though their exact shape depends on the implementation details.\n\nSince our actual implementation, as well as the reference implementations of UMAP and NCVis, considers edges ij and ji separately, we will do so from now on. Hence, there is now a total of E := 2|skNN| edges. We always assume that p(ij) = p(ji) and adding up the probabilities for both directions yields one: (cid:80)n i,j=1 p(ij) = 1. For a given data distribution over pairs of points ij, we define p(i) = (cid:80)n i p(i) = 1. As discussed in Supp. B of Damrich & Hamprecht (2021), the p(i) values are approximately constant when p(ij) is uniform on the skNN graph or proportional to the UMAP similarities.\n\nj=1 p(ij) so that (cid:80)\n\nD.1 NOISE DISTRIBUTIONS OF UMAP AND NCVIS\n\nUMAP’s noise distribution is derived in Damrich & Hamprecht (2021) and reads in our notation (Supp. C)\n\nξ(ij) =\n\np(i) + p(j) 2n\n\n.\n\n(52)\n\nNote that UMAP uses a weighted version of the skNN graph. Still, ξ is close to uniform, see Supp. B of Damrich & Hamprecht (2021).\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nThe noise distribution of NCVis is also close to being uniform and equals (Artemenkov & Panov, 2020):\n\nξ(ij) =\n\np(i) n\n\n.\n\n(53)\n\nThis is a slightly different noise distribution than in UMAP and in particular it is asymmetric. However, we argue that in practice it is equivalent to the same noise distribution as in UMAP. The noise distribution is used in two ways in NCVis: for sampling noise samples and in the posterior class probabilities\n\nP(data|ij) =\n\nqθ,Z(ij) qθ,Z(ij) + mξ(ij)\n\n.\n\n(54)\n\nBoth in the reference NCVis implementation and in ours, for the second role, the noise distribution is explicitly approximated by the uniform one and we use the posterior probabilities\n\nP(data|ij) =\n\nqθ,Z(ij)\n\nqθ,Z(ij) + m 1\n\n2|skNN|\n\n.\n\n(55)\n\nTogether with the symmetry of the Cauchy kernel this implies that the repulsion on the embedding vectors ei and ej from noise sample ij and ji is the same. As a result, the expectation\n\n(cid:32)\n\nEij∼ξ log\n\n1 −\n\n(cid:33)\n\nqθ,Z(ij)\n\nqθ,Z(ij) + m 1\n\n2|skNN|\n\nis the same for ξ(ij) = p(i)/n and for UMAP’s noise distribution\n\nξ(ij) =\n\np(i) + p(j) 2n\n\n.\n\n(56)\n\n(57)\n\nD.2 EFFECT OF BATCHED TRAINING ON THE NOISE DISTRIBUTION\n\nIn our framework, the noise distribution is influenced by the batched training procedure (Alg. S1) because the negative samples can come only from the current training batch.\n\nIn every epoch, we first shuffle the set of directed edges of the skNN graph and then chunk it into batches. To emphasize, the batches consist of directed edges and not of the original data points. For each edge in a batch, we take its head and sample m indices from the heads and tails of all edges in the batch (excluding the already selected head) and use them as tails to form negative sample pairs.\n\nTo obtain a negative sample pair ij, the batch must contain some directed edge ik, providing the head of the negative sample, and some pair lj or jl, providing the tail. We want to derive the expected number of times that a directed edge ij is considered as a negative sample in a batch. For simplicity, let us assume that the number of batches divides the number of directed edges E. As the set of skNN edges is shuffled every epoch, the expected number of pairs ij as negative samples is the same for all batches.\n\nLet us consider a batch B of size b. We denote by Yrs the random variable that holds the number of times edge rs appears in B. We also introduce random variables Y¬rs = (cid:80) t̸=r Yts and Yr¬s = (cid:80) t̸=s Yrt. Let p(r¬s) := p(¬sr) := p(r) − p(rs). For each occurrence of an i as head of an edge in B, we sample m tails to create negative samples uniformly from all head and tails in B with replacement, but we prevent sampling the identical head i as negative sample tail. If, however, the same node i is part of the other edges in the batch, then it may be sampled and would create a futile negative sample ii. There are m chances for creating a negative sample edge ij for every head i and any occurrence of j in the batch. The number of heads i in the batch is Yij + Yi¬j and the number of occurrences of j is Yij + Yji + Y¬ij + Yj¬i. Since we sample the tail of a negative sample pair uniformly with replacement, any of the occurrences of j has probability 1/(2b−1) to be selected. Hence, the expected number Nij of times that the ordered pair ij with i ̸= j is considered as a negative sample in batch B is\n\nNij = m(Yij + Yi¬j)\n\nYij + Yji + Y¬ij + Yj¬i 2b − 1\n\n.\n\n(58)\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nSince a head i may not choose itself to form a negative sample, the expected number of times that ii appears as negative sample in the batch is\n\nNii = m\n\n(cid:88)\n\nYij\n\nj\n\nYij − 1 + Yi¬j + Yji + Y¬ji 2b − 1\n\n.\n\n(59)\n\nSince the batches are sampled without replacement, the random variables Yrs are distributed according to a multivariate hypergeometric distribution, so that\n\nE(Yrs) = bp(rs), E(Y¬rs) = bp(¬rs),\n\nVar(Yrs) = b\n\nE − b E − 1\n\np(rs)(cid:0)1 − p(rs)(cid:1),\n\nCov(Yrs, Y¬uv) = −b\n\nE − b E − 1\n\np(rs)p(¬uv).\n\n(60)\n\n(61)\n\n(62)\n\n(63)\n\nWe use these expressions and analogous ones together with the symmetries p(rs) = p(sr) to compute (leaving out intermediate algebra steps) the expectation of Nij over the shuffles:\n\nE(Nij) =\n\nmb 2b − 1\n\n(cid:18) E − b E − 1\n\n(cid:18)\n\np(ij) + 2\n\nb −\n\n(cid:19)\n\nE − b E − 1\n\n(cid:19)\n\np(i)p(j)\n\n.\n\n(64)\n\nSince we sample m negative samples for each positive sample and since each batch contains b positive samples, we need to divide E(Nij) by mb to obtain ξ(ij):\n\nξ(ij) =\n\n1 2b − 1\n\n(cid:18) E − b E − 1\n\n(cid:18)\n\np(ij) + 2\n\nb −\n\n(cid:19)\n\nE − b E − 1\n\n(cid:19)\n\np(i)p(j)\n\n.\n\nSimilarly,\n\nE(Nii) =\n\nmb 2b − 1\n\n(cid:18)\n\n−\n\nb − 1 E − 1\n\n(cid:18)\n\np(i) + 2\n\nb −\n\n(cid:19)\n\nE − b E − 1\n\n(cid:19)\n\np(i)2\n\nand hence the noise distribution value for the pair ii is\n\nξ(ii) =\n\n1 2b − 1\n\n(cid:18)\n\n−\n\nb − 1 E − 1\n\n(cid:18)\n\np(i) + 2\n\nb −\n\n(cid:19)\n\n(cid:19)\n\n.\n\np(i)2\n\nE − b E − 1\n\n(65)\n\n(66)\n\n(67)\n\nWe see that the noise distribution depends on the batch size b. This is not surprising: For example, if the batch size is equal to one, the ordered pair ij can only be sampled as a negative sample in the single batch that consists of that pair. Indeed, for b = 1 our formula yields\n\nξ(ij) = p(ij),\n\n(68)\n\nmeaning that the data and the noise distributions coincide. Conversely, if b = E and there is only one batch, we obtain\n\nξ(ij) =\n\n2E 2E − 1\n\np(i)p(j)\n\n(69)\n\nand the noise distribution is close to uniform. For batch sizes between 1 and E the noise distribution is in between these two extremes. For MNIST, E ≈ 1.5 · 106, and in our experiments we used b = 1024. This means that the prefactor of the share of the data distribution is about 0.0005 while that of the near-uniform distribution p(i)p(j) is about 0.9995, so the resulting noise distribution is close to uniform. Note that Thm. 1 and Cor. 2 only require the noise distribution to have full support which is the case for any batch size greater than one.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n(a) ζ = 10−3, no annealing\n\n(b) ζ = 10−10, no annealing\n\n(c) ζ = 0, no annealing\n\n(d) ζ = 10−3, with annealing\n\n(e) ζ = 10−10, with annealing\n\n(f) ζ = 0, with annealing\n\nFigure S1: UMAP embeddings of the MNIST dataset, ablating numerical optimization tricks of UMAP’s reference implementation. The learning rate annealing is crucial (bottom row) but safeguarding against divisions by zero in UMAP’s repulsive term Eq. (71) by adding ζ to the denominator has little effect. These experiments were run using the reference implementation, modified to change the ζ value and to optionally switch off the learning rate annealing.\n\nE OPTIMIZATION TRICKS IN UMAP’S REFERENCE IMPLEMENTATION\n\nUMAP’s repulsive term\n\n− log(1 − φ(ij)) = log\n\n(cid:33)\n\n(cid:32)\n\n1 + d2 ij d2 ij\n\n(70)\n\ncan lead to numerical problems if the two points of the negative sample pair are very close. In addition to the learning rate decay, discussed in Sec. 6, UMAP’s implementation uses further tricks to prevent unstable or even crashing training.\n\nIn non-parametric UMAP’s reference implementation, the gradient on embedding position ei exerted by a single sampled repulsive pair ij is actually\n\n2\n\n1 ij + ζ\n\nd2\n\n1 1 + d2 ij\n\n(ej − ei)\n\nwith ζ = 0.001 instead of ζ = 0. This corresponds to the full loss function\n\n−Eij∼p log(φ(ij)) − m\n\n(cid:16)\n\n1 +\n\nζ 1 − ζ\n\n(cid:17)\n\nEij∼ξ log\n\n(cid:18)\n\n1 +\n\n(cid:19)\n\n− φ(ij)\n\n.\n\nζ 1 − ζ\n\n(71)\n\n(72)\n\nHowever, we found that ζ does not have much influence on the appearance of a UMAP embedding. Fig. S1 shows MNIST embeddings obtained using the original UMAP implementation modified to use different values of ζ. Neither a much smaller positive value such as ζ = 10−10 nor setting ζ = 0 substantially changed the appearance of the embedding (even though some runs with ζ = 0 did crash). The learning rate annealing played a much bigger role in how the embedding looked like (Fig. S1, bottom row).\n\nThe reference implementation of parametric UMAP uses automatic differentiation instead of implementing the gradients manually. To avoid terms such as log(0) in the repulsive loss, it clips the argument of the logarithm from below at the value ε = 10−4, effectively using the loss function\n\n(cid:32)\n\n−Eij∼p log\n\nmax\n\n(cid:33)\n\n(cid:111)\n\n(cid:110)\n\nε,\n\n1 1 + d2 ij\n\n(cid:32)\n\n− mEij∼ξ log\n\nmax\n\n(cid:110)\n\nε, 1 −\n\n1 1 + d2 ij\n\n(cid:33)\n\n(cid:111)\n\n.\n\n(73)\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n(a) UMAP, ε = 10−4, no ann.\n\n(b) UMAP, ε = 10−10, no ann.\n\n(c) UMAP, ε = 10−4, with ann.\n\n(d) UMAP, ε = 10−10, with ann.\n\n(e) Neg-t-SNE, ε = 10−4, no ann.\n\n(f) Neg-t-SNE, ε = 10−10, no ann.\n\n(g) Neg-t-SNE, ε = 0, no ann.\n\n(h) Neg-t-SNE, ε = 10−4, with ann.(i) Neg-t-SNE, ε = 10−10, with ann.\n\n(j) Neg-t-SNE, ε = 0, with ann.\n\nFigure S2: UMAP and Neg-t-SNE embeddings of the MNIST dataset using different values ε at which we clip arguments to logarithm functions. These experiments were done using our implementation. Varying ε did not strongly influence the appearance of the embedding. But setting ε = 0 led to crashing UMAP runs. Annealing the learning rate is important for UMAP, yet not for Neg-t-SNE.\n\nWe employ a similar clipping in our code whenever we apply the logarithm function. Again, we found that the exact value of ε is not important for our UMAP reimplementation, while using the learning rate annealing is (Fig. S2, top two rows). In the extreme case of setting ε = 0, our UMAP runs crashed. We believe that the reason is that we allow negative sample pairs to be of the form ii, which would not send any gradient, but would lead to a zero argument to the logarithm. The reference implementation of UMAP excludes such negative sample pairs ii.\n\nOur Neg-t-SNE approach does not have any of these problems, as the repulsive term is upper bounded\n\n\n\n− log\n\n1 −\n\n1 1+d2 ij\n\n1 1+d2 ij\n\n+ 1\n\n\n\n = log\n\n(cid:33)\n\n(cid:32)\n\n2 + d2 ij 1 + d2 ij\n\n≤ log(2)\n\n(74)\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nand does not diverge for dij → 0. For this reason, Neg-t-SNE is not very sensitive to the value at which we clip arguments to the logarithm and works even with ε = 0, both with and without learning rate annealing (Fig. S2, bottom two rows).\n\nThe attractive terms in the loss functions do not pose numerical problems in practice due to the heavy tail of the Cauchy kernel. That said, in order to keep different experiments and losses comparable, we used clipping in both the attractive and the repulsive loss terms with ε = 10−10 for all neighbor embedding plots computed with our framework unless otherwise stated.\n\nA recent pull request (#856) to the parametric part of UMAP’s reference implementation proposed another way to ameliorate the numerical instabilities. The clipping of the arguments to the logarithm was replaced with a sigmoid of the logarithm of the Cauchy kernel, so that the attractive and repulsive terms become\n\n− log\n\n(cid:16)\n\nmax (cid:0)ε, φ(ij)(cid:1)(cid:17)\n\n→ − log\n\n(cid:18)\n\n(cid:16)\n\nσ\n\nlog (cid:0)φ(ij)(cid:1)(cid:17)(cid:19)\n\n,\n\n− log\n\n(cid:16)\n\nmax (cid:0)ε, 1 − φ(ij)(cid:1)(cid:17)\n\n(cid:18)\n\n→ − log\n\n1 − σ\n\n(cid:16)\n\nlog (cid:0)φ(ij)(cid:1)(cid:17)(cid:19)\n\n,\n\n(75)\n\n(76)\n\nwhere σ(x) = 1/(cid:0)1 + exp(−x)(cid:1) is the sigmoid function. This change can seem drastic as, e.g., for φ(ij) = 1 we have max (cid:8)ε, φ(ij)(cid:9) = 1, but σ shows that this turns the loss function precisely into our Neg-t-SNE loss function since\n\n= 1/2. But unravelling the definitions\n\nlog (cid:0)φ(ij)(cid:1)(cid:17)\n\n(cid:16)\n\n(cid:16)\n\nlog (cid:0)φ(ij)(cid:1)(cid:17)\n\nσ\n\n=\n\n(cid:16)\n\n1 + exp\n\n1\n\n− log (cid:0)φ(ij)(cid:1)(cid:17) =\n\n1\n\n1 + φ(ij)−1 =\n\nφ(ij) φ(ij) + 1\n\n.\n\n(77)\n\nSo, in order to overcome the numerical problems incurred by UMAP’s implicit choice of 1/d2 ij as similarity kernel, the pull request suggested a fix that turns out to be equivalent to negative sampling using the Cauchy kernel. We encourage this change to UMAP as it makes its loss function equivalent to our Neg-t-SNE and thus also conceptually more related to t-SNE. We also suggest to implement it in the non-parametric case.\n\nF TRIMAP AND INFONC-t-SNE\n\nThe neighbor embedding method TriMap (Amid & Warmuth, 2019) also employs a contrastive technique to compute the layout. TriMap considers triplets (i, j, k), where i and j are data points that are more similar than i and k. It moves the embeddings ei and ej closer together and pushes ei and ek further apart. While a full investigation of TriMap is beyond the scope of our work, here we argue that it bears some similarity to InfoNC-t-SNE with m = 1.\n\nFirst, we describe TriMap in more detail. Most of the triplets (i, j, k) consist of nearest neighbors i and j and a random point k. By default, 12 nearest neighbors j are chosen for each i, using a modified Euclidean distance. For each such pair (i, j) additional distinct points k (by default 4) are chosen randomly from the set of all points excluding i and all the chosen j. This leads to 48 triplets (i, j, k) for each point i. Additionally, several (by default 3) triplets are formed for each i with random j and k only ensuring that the similarity between i and j is larger than between i and k. Thus, for each point i there are in total 51 triplets (i, j, k) leading to a grand total of 51n triplets. These triplets are formed ahead of the layout optimization phase and kept fixed throughout. TriMap also assigns a weight wijk to each triplet. This weight is larger for triplets in which i and j are much more similar to each other than i and k.\n\nIn our notation, TriMap’s loss function can be written as\n\n(cid:88)\n\nijk∈triplets\n\nwijk\n\nφ(ik) φ(ij) + φ(ik)\n\n= const −\n\n(cid:88)\n\nijk∈triplets\n\nwijk\n\nφ(ij) φ(ij) + φ(ik)\n\n.\n\nThis loss is very similar to the loss of InfoNC-t-SNE with m = 1:\n\n(cid:18)\n\nφ(ij) φ(ij) + φ(ik)\n\n(cid:19)\n\n.\n\nlog\n\n− E\n\nij∼p k∼U\n\n24\n\n(78)\n\n(79)\n\nPublished as a conference paper at ICLR 2023\n\n(a) TriMap default\n\n(b) TriMap Eucl. kNN\n\n(c) TriMap no random\n\n(d) TriMap no weights\n\n(e) InfoNC-t-SNE m = 1\n\nFigure S3: Ablating design choices of TriMap on the MNIST dataset. Simplifying TriMap leads to an embedding similar to that of InfoNC-t-SNE with m = 1. The biggest difference results from the omission of random triplets in panel c.\n\nTable S1: Quality of TriMap and InfoNC-t-SNE. Means and standard deviations over three runs.\n\nkNN recall\n\nSpearman correlation\n\nTriMap (default) TriMap (kNN) TriMap (no random) TriMap (no weights) InfoNC-t-SNE (m = 1) InfoNC-t-SNE (m = 5)\n\n0.098 ± 0 0.099 ± 0 0.101 ± 0 0.099 ± 0 0.085 ± 0.001 0.104 ± 0\n\n0.245 ± 0.005 0.232 ± 0.003 0.355 ± 0.004 0.381 ± 0.004 0.381 ± 0.006 0.365 ± 0.005\n\nThe differences are: the absence of log; the presence of weights; the presence of random triplets; nearest neighbors being based not exactly on Euclidean metric; and fixed triplets during optimization. In the following we modify the reference TriMap implementation and ablate these differences in order to see which ones are more or less important (Figure S3). We measure the quality of all embeddings using two metrics (Tab. S1): as a global metric we use the Spearman correlation between high- and low-dimensional pairwise distances between 5000 randomly chosen points (Kobak & Berens, 2019) and as a local metric we use kNN recall with k = 15 (Kobak & Berens, 2019).\n\nFirst, we modified TriMap to use the 15 nearest neighbors in Euclidean metric. This did not change the visual appearance of the MNIST embedding (Figs. S3a, b). Next, we additionally omitted the random triplets. The resulting embedding became visually closer to the InfoNC-t-SNE embedding (Fig. S3c). Finally, additionally setting all the weights wijk to one had only a small visual effect (Fig. S3d). Together, these simplifications noticeably improved the global metric compared to the default TriMap, while leaving the local metric almost unchanged (Tab. S1).\n\nSimilar to the experiments of Wang et al. (2021), our ablations made TriMap look very similar to InfoNC-t-SNE with m = 1 (Fig. S3e), suggesting that the remaining differences between them do not play a major role. This refers to the details of the optimization (e.g. fixed triplets vs. randomly sampled triplets in each epoch, full-batch vs. stochastic gradient descent, momentum and adaptive learning rates, etc.), as well as to the presence/absence of the logarithm in the loss function. Since\n\n(cid:16)\n\nlog (cid:0)\n\nd\n\nφ(ij) φ(ij)+φ(ik)\n\n(cid:1)(cid:17)\n\ndθ\n\n=\n\nφ(ij) + φ(ik) φ(ij)\n\nd(cid:0)\n\nφ(ij) φ(ij)+φ(ik) dθ\n\n(cid:1)\n\n,\n\n(80)\n\nthe gradients of TriMap and InfoNC-t-SNE point in the same direction, but are differently scaled. InfoNC-t-SNE prioritizes triplets with high (cid:0)φ(ij) + φ(ik)(cid:1)/φ(ij), i.e. triplets in which the embedding similarity does not respect the triplet well. Note that this is conceptually different from TriMap’s weights wijk which are based on the high-dimensional similarities. However, at least on MNIST, this difference in gradients did not have a major effect (Figs. S3d, e).\n\nNote that our default for InfoNC-t-SNE is m = 5 instead of m = 1. On MNIST, this setting yields a better local score and a similar global score compared to m = 1, and much better scores than the default TriMap (Tab. S1). The work of Ma & Collins (2018) theoretically underpins our observation in Fig. S19 that the higher m gets, the better InfoNC-t-SNE approximates t-SNE.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nG RELATION TO OTHER VISUALIZATION METHODS\n\nG.1\n\nt-SNE APPROXIMATIONS\n\nA vanilla implementation of t-SNE has to compute the partition function Z(θ) and hence scales quadratically with the sample size n. Therefore, nearly all modern t-SNE packages implement approximations. Common are Barnes-Hut-t-SNE (BH-t-SNE) (Yang et al., 2013; van der Maaten, 2014) and FFT-accelerated interpolation-based t-SNE (FIt-SNE) (Linderman et al., 2019). Both methods approximate the repulsive interaction between all pairs of points and the computation of the partition function, achieving complexities O(kn log(n)2d) and O(kn2d), respectively. In fact, all our t-SNE plots are done with the FIt-SNE implementation of openTSNE (Poliˇcar et al., 2019).\n\nThe focus of our work is not primarily on accelerating t-SNE, but on understanding its relation to contrastive neighbor embeddings. Different from the t-SNE approximations above, these do not approximately compute the partition function, but either learn it (NC-t-SNE), use a fixed normalization parameter (Neg-t-SNE and UMAP), or do not require it at all (InfoNC-t-SNE). They only sample nm repulsive interactions per epoch. This cuts their complexity down to O(kmnd). In particular, they scale linearly with the embedding dimension d and can therefore be used for more general representation learning with d ≫ 2, unlike BH-t-SNE and FIt-SNE.\n\nG.2 OTHER CONTRASTIVE t-SNE APPROXIMATIONS\n\nIn addition to NC-t-SNE and InfoNC-t-SNE, there are two other, recent sampling-based strategies to approximate t-SNE, GDR-t-SNE (Draganov et al., 2022) and SCE (Yang et al., 2023). Both sample m repulsive interactions per each attractive interaction. In order to scale these repulsive interactions properly, an estimate of the partition function is computed.\n\nIn GDR-t-SNE, the sum of the Cauchy kernels of the mkn repulsive interactions over each epoch are added up to obtain an estimate of Z t-SNE(θ) · mkn/(cid:0)n(n − 1)(cid:1). Since both the number of repulsive interactions and the estimate of the partition function are decreased by a factor of mk/(n − 1) compared to the vanilla t-SNE, the overall repulsion strength is correct. In contrast, SCE (Yang et al., 2023) initializes its estimate of the partition function with the maximum value n(n − 1) and then updates it using a moving average after each sampled repulsive pair. Both Draganov et al. (2022) and Yang et al. (2023) use m = 1. Their approaches are similar to NCE in that they approximately estimate Z using the sampled noise points, while NCE treats Z as learnable parameter and performs gradient descent on it. Note, however, that the second factor of the NCE gradient in Eq. (39) is not present in the approaches of Draganov et al. (2022) and Yang et al. (2023).\n\nG.3 LARGEVIS\n\nLargeVis (Tang et al., 2016) was, to the best of our knowledge, the first contrastive neighbor embedding method. It uses NEG. Since it was quickly overshadowed by very similar UMAP, we focused the exposition in the main paper on UMAP. Damrich & Hamprecht (2021) computed LargeVis’ effective loss function in closed form. In our notation it reads\n\nLLargeVis(θ) = −Eij∼p log(φ(ij)) − γmEij∼ξ log(1 − φ(ij)). The are some implementation differences between LargeVis and UMAP, but in terms of the loss function, the main difference is the γ factor in front of the repulsive term, which defaults to 7. Therefore, our analysis of UMAP carries over to LargeVis: Its loss is essentially an instance of NEG with inverse square kernel. The additional factor γ = 7 moves it along the attraction-repulsion spectrum towards more repulsion, that is, towards t-SNE.\n\n(81)\n\nG.4 PACMAP\n\nPairwise Controlled Manifold Approximation Projection (PaCMAP) (Wang et al., 2021) is a recent visualization method also based on sampling m repulsive forces per one attractive force (with m = 2 by default). It employs a large number of additional design choices, such as weak attraction between ‘mid-near’ points in addition to attraction between nearest neighbors (which are based on a modified Euclidean distance, as in TriMap), several optimization regimes with dynamically changing loss weights, etc. Nevertheless, the core parts of its loss can be related to (generalized) Neg-t-SNE.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nThe attractive loss in PaCMAP is\n\nd2 + 1 d2 + 1 + c\n\n= 1 − c ·\n\n1 d2 + 1 + c\n\n(82)\n\nwith c = 10 for nearest neighbors (and c = 10 000 for mid-near points but that part of the loss is switched off during the final stage of the optimization). The attractive loss for Neg-t-SNE with uniform noise distribution and ̄Z = c|X|/m is\n\n− log\n\n(cid:18) φ(d)\n\n(cid:19)\n\nφ(d) + c\n\n= − log\n\n(cid:18)\n\n√\n\n(d/\n\n1\n\nc)2 + 1 + c\n\n(cid:19)\n\n.\n\n(83)\n\nThus, the differences are the logarithm and some rescaling of the distances. Note that Neg-t-SNE with c = 10 typically produces very similar result compared to the default c = 1 (cf. Fig 1).\n\nThe repulsive loss in PaCMAP is\n\n1\n\n2 + d2 = 1 −\n\n1 + d2 2 + d2 .\n\n(84)\n\nAgain it is similar to the repulsive loss of Neg-t-SNE, but now for ̄Z = |X|/m, the default negative sampling value also used by UMAP:\n\n(cid:18)\n\n− log\n\n1 −\n\n(cid:19)\n\nφ(d) φ(d) + 1\n\n= − log\n\n(cid:19)\n\n(cid:18)\n\n1 φ(d) + 1\n\n= − log\n\n(cid:18) 1 + d2 2 + d2\n\n(cid:19)\n\n.\n\n(85)\n\nthe difference is just in the logarithm, similar to the connection between TriMap and\n\nHere, InfoNC-t-SNE in Supp. F.\n\nEmpirically, PaCMAP embeddings of high-dimensional data like MNIST look similar to UMAP embeddings, and hence to Neg-t-SNE embeddings, in agreement with our analysis.\n\nH QUANTITATIVE EVALUATION\n\nThis section provides a quantitative evaluation of the Neg-t-SNE spectrum. For each embedding on the spectrum, we compute two metrics. The first metric, kNN recall with k = 15 (Kobak & Berens, 2019), measures the fraction of nearest neighbors in the embedding that are also nearest neighbors in the reference configuration. It is therefore a measure of local quality. The second metric is the Spearman correlation between the pairwise distances in the embedding and in the reference configuration (Kobak & Berens, 2019). To speed up the computation, we consider all pairwise distances between a sample of 5000 points. Since most random pairs of points are not close to each other, this is a measure of global quality.\n\nAs reference configuration, we use three different layouts for each dataset. We compare against the original high-dimensional data to measure how faithful Neg-t-SNE embeddings are. Additionally, we compare to the corresponding t-SNE and UMAP embeddings to investigate for which ̄Z value Neg-t-SNE matches t-SNE and UMAP the best.\n\nWe found that compared to the high-dimensional data, the kNN recall followed an inverse U-shape across the spectrum (Fig. S4a). kNN recall was low for very large and for very small values of ̄Z. It peaked when ̄Z was close to t-SNE’s partition function Z t-SNE and NC-t-SNE’s learned normalization parameter Z NC-t-SNE. At UMAP’s normalization constant ̄Z UMAP the kNN recall was usually lower. This confirms our observation that the local quality of the embedding improves when decreasing ̄Z from ̄Z UMAP to Z t-SNE in agreement with B ̈ohm et al. (2022). The kNN recall for the Kuzushiji-49 dataset at ̄Z = Z t-SNE was very low, compared to other datasets. We suspect that this was due to incomplete convergence (Fig. S17, Tab. S4). Conversely, the Spearman correlation mostly increased with ̄Z (Fig. S4b), which aligns with our finding that higher attraction improves the global layout of the embedding.\n\nWe also computed the kNN recall and the Spearman correlation for the proper t-SNE and UMAP embeddings (not depicted in Figs. S4a, b). The kNN recall was higher for t-SNE embeddings than for Neg-t-SNE at ̄Z = Z t-SNE (see, e.g., Fig. S18l), likely because proper t-SNE considers the repulsive interaction between all points. The metrics for the UMAP embeddings and the Spearman\n\n27\n\nPublished as a conference paper at ICLR 2023\n\ncorrelation for t-SNE were close to the corresponding values for Neg-t-SNE at the respective ̄Z values.\n\nWhen comparing Neg-t-SNE embeddings to the proper t-SNE embedding, the best fit in terms of kNN recall (Fig. S4c) and in terms of Spearman correlation (Fig. S4d) was usually achieved when ̄Z was close to Z t-SNE, in accordance with our theory. Again, the Kuzushiji-49 dataset was an outlier, likely due to convergence issues.\n\nFinally, when comparing Neg-t-SNE embeddings to the UMAP embedding, the highest Spearman correlation was achieved by ̄Z ≈ ̄Z UMAP (Fig. S4f), in agreement with our theoretical predictions. The highest kNN recall was typically achieved at a slightly lower ̄Z (Fig. S4e).\n\nOverall, these experiments provide empirical support for our interpretation of the Neg-t-SNE spectrum as implementing a trade-off between global and local structure preservation, and they confirm the predicted locations of t-SNE- and UMAP-like embeddings on the spectrum.\n\nWe computed the kNN recall and the Spearman correlation also for the embeddings in Figs. S18 and S19, where we varied the number of noise samples m and the initialization method for NC-t-SNE and InfoNC-t-SNE. As expected, we found that higher m improves the kNN recall, but t-SNE proper achieved higher kNN recall still. The Spearman correlation decreased with m when we initialized randomly and did not change much for other initialization methods. Random initialization hurt the Spearman correlation significantly.\n\nI TOY DATASET\n\nCor. 2 predicts that the partition function of a Neg-t-SNE embedding should equal the ̄Z value. In our real-world examples (panels i in Figs. 1, S11–S16) we observed a monotone relationship between them, but not a perfect agreement. As discussed in Sec. 5, this is due to the shape of the Cauchy kernel and the fact that the data distribution is zero for many pairs of points. Here, we consider a toy example for which we observe a perfect match between Neg-t-SNE’s partition function and the ̄Z value, confirming our Cor. 2.\n\nAs we operate with the binary skNN graph, the only possible high-dimensional similarities are zero and one. Due to the heavy tail of the Cauchy kernel, we would like our toy example to have no pairs of points for which the data distribution is zero. Thus, all pairs of points need to have equal high-dimensional similarity. In two dimensions, only up to three points can be placed equidistantly from each other. Therefore, we consider the simple case of placing three points according to the 6 (there are six directed edges) for various values of ̄Z. We Neg-t-SNE loss function with p ≡ 1 keep the Cauchy kernel as similarity function, which has maximum 1/(1 + 02) = 1. Thus, it is not possible to match values above ̄Z = 6, and the three points end up having the same position in the embedding. But for smaller values of ̄Z, the resulting partition function is indeed exactly equal to ̄Z (Fig. S5).\n\nFor this experiment we decreased the batch size to 6 and the learning rate to 0.01, but kept all other hyperparameters at their default values.\n\nJ DATASETS\n\nWe used the well-known MNIST (LeCun et al., 1998) dataset for most of our experiments. We downloaded it via the torchvision API from http://yann.lecun.com/exdb/mnist/. This website does not give a license. But https://keras.io/api/datasets/mnist/ and http://www.pymvpa.org/datadb/mnist.html name Yann LeCun and Corinna Cortes as copyright holders and claim MNIST to be licensed under CC BY-SA 3.0, which permits use and adaptation. The MNIST dataset consists of 70 000 grayscale images, 28 × 28 pixels each, that show handwritten digits.\n\nThe Kuzushiji-49 dataset (Tarin et al., 2018) was downloaded from https://github.com/ rois-codh/kmnist where it is licensed under CC-BY-4.0. It contains 270 912 grayscale images, 28 × 28 pixels each, that show 49 different cursive Japanese characters.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\n(a) Relative to the high-dimensional data\n\n(b) Relative to the high-dimensional data\n\n(c) Relative to t-SNE embeddings\n\n(d) Relative to t-SNE embeddings\n\n(e) Relative to UMAP embeddings\n\n(f) Relative to UMAP embeddings\n\nFigure S4: Quantitative evaluation of embeddings on the Neg-t-SNE spectrum with respect to different reference configurations. Means and standard deviations over three random seeds are plotted. Left column: kNN recall, a measure of local agreement. Right column: Spearman correlation, a measure of global agreement. First row: Comparison of Neg-t-SNE embeddings to the highdimensional input data. Second row: Comparison of Neg-t-SNE embeddings to the corresponding t-SNE embedding. Third row: Comparison of Neg-t-SNE embeddings to the corresponding UMAP embedding.\n\nanother\n\nperformed\n\nstandard machine\n\n2009) The SimCLR experiments were the dataset, sklearn.datasets.fetch openml API from https://www.openml.org/search? type=data&sort=runs&id=40927&status=active. Unfortunately, we were not able to find a license for this dataset. CIFAR-10 consists of 60 000 images, 32 × 32 RGB pixels each, depicting objects from five animal and five vehicle classes.\n\nWe downloaded it via\n\nlearning resource.\n\nthe CIFAR-10\n\n(Krizhevsky,\n\non\n\n(2019) was downloaded from https:// The transcriptomic dataset of Kanton et al. www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-7552/, which permits free use. We only used the 20 272 cells in the human brain organoid cell line ‘409b2’. The transcriptomic dataset of Wagner et al. scanpy version from https://kleintools.hms.harvard.edu/paper_websites/wagner_ zebrafish_timecourse2018/mainpage.html. The full dataset at https://www. ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE112294 is free to download and reproduce. The dataset contains gene expressions for 63 530 cells from a developing zebrafish embryo. The downloaded UMIs of both datasets were preprocessed as in (B ̈ohm et al., 2022; Kobak\n\n(2018) was downloaded in its\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nFigure S5: Partition function of Neg-t-SNE embeddings as a function of ̄Z for a toy dataset with three equidistant points. There is a perfect fit between ̄Z and the partition function until ̄Z = 6, which is the largest partition function a set of three points with six directed edges can have under the Cauchy kernel. Beyond this value, all three points overlap in the embedding. Mean with standard deviation over three random seeds is plotted.\n\n& Berens, 2019). After selecting the 1000 most variable genes, we normalized the library sizes to the median library size in the dataset, log-transformed the normalized values with log2(x + 1), and finally reduced the dimensionality to 50 via PCA.\n\nThe transcriptomic dataset of the C. elegans flatworm (Packer et al., 2019; Narayan et al., 2021) was obtained from http://cb.csail.mit.edu/cb/densvis/datasets/ with consent It is already preprocessed to 100 principal of the authors who license it under CC BY-NC 2.0. components.\n\nK IMPLEMENTATION\n\nK.1 PACKAGES\n\nAll contrastive embeddings were computed with our PyTorch (Paszke et al., 2019) implementation of Neg-t-SNE, NC-t-SNE, UMAP, and InfoNC-t-SNE. Exceptions are Fig. 1 and analogous Figs. S11 – S16. There, for panel g we used the reference implementation of NCVis (Artemenkov & Panov, 2020) (with a fixed number of noise samples m, and not the default schedule), and for panel h we used UMAP 0.5. The t-SNE plots were created with the openTSNE (Poliˇcar et al., 2019) (version 0.6.1) package. Similarly, we used the reference UMAP implementation in Fig. S1 and openTSNE in Figs. S18 and S19. The TriMap plots in Supp. F were computed with version 1.1.4 of the TriMap package by Amid & Warmuth (2019).\n\nWe extended these implementations of NCVis, UMAP, and t-SNE to make them accept custom embedding initializations and unweighted skNN graphs and to log various quantities of interest. We always used the standard Cauchy kernel for better comparability.\n\nK.2\n\nINITIALIZATION, EXAGGERATION AND SkNN GRAPH\n\nAll PCAs were computed with sklearn (Pedregosa et al., 2011). We used PyKeOps (Charlier et al., 2021) to compute the exact skNN graph and to handle the quadratic complexity of computing the partition functions on a GPU. The same PCA initialization and skNN graph with k = 15 were used for all embeddings. The skNN graph for MNIST and Kuzushiji-49 was computed on a 50dimensional PCA of the dataset.\n\nWe initialized all embeddings with a scaled version of PCA (save for Figs. S18 and S19). For t-SNE embeddings we rescaled the initialization so that the first dimension has a standard deviation of 0.0001 (as is default in openTSNE), for all other embeddings to a standard deviation of 1. For TriMap, we stuck to its default of scaling the PCA down by a factor of 0.01.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nWe employed some version of ‘early exaggeration’ (van der Maaten & Hinton, 2008) for the first 250 epochs in most non-parametric plots. For t-SNE it is the default early exaggeration of openTSNE. When varying ̄Z in non-parametric Neg-t-SNE, early exaggeration meant using ̄Z = |X|/m for the first 250 epochs (save for Fig. S11). When varying the number of noise samples in Figs. S8, S18 and S19, we still used m = 5 for the first 250 epochs. In Figs. 2, 3, S1, and S2 as well as in all reference NCVis or UMAP plots, we did not use early exaggeration as neither small ̄Z nor high m made it necessary. When we used some form of early exaggeration and learning rate annealing, the annealing to zero took place over the first 250 epochs, was then reset, and annealed again to zero for the remaining, typically 500, epochs.\n\nK.3 OTHER DETAILS FOR NEIGHBOR EMBEDDING EXPERIMENTS\n\nWhen computing logarithms during the optimization of neighbor embeddings, we clip the arguments to the range [10−10, 1], save for Fig. S2, where we varied this lower bound. The lower bound is smaller than in the reference implementation of parametric UMAP, where it is set to 10−4.\n\nOur defaults were a batch size of 1024, linear learning rate annealing from 1 (non-parametric) or 0.001 (parametric) to 0 (save for Figs. 2, S1, and S2), 750 epochs (save for Figs. S10 and S17) and m = 5 noise samples (save for Figs. S8, S10, S18, and S19).\n\nNon-parametric runs were optimized with SGD without momentum and parametric runs with the Adam optimizer (Kingma & Ba, 2015). Parametric runs used the same feed-forward neural net architecture as the reference parametric UMAP implementation. That is, four layers with dimensions input dimension − 100 − 100 − 100 − 2 with ReLU activations in all but the last one. We used the vectorized, 786-dimensional version of MNIST as input to the parametric neighbor embedding methods (and not the 50-dimensional PCA; but the skNN graph was computed in the PCA space for consistency with non-parametric embeddings).\n\nLike the reference NCVis implementation, we used the fractions qθ,Z(x)/(qθ,Z(x) + m) instead of qθ,Z(x)/(cid:0)qθ,Z(x) + mξ(x)(cid:1). This is a mild approximation as the noise distribution is close to uniform. But it means that the model learns a scaled data distribution (cf. Cor. 2), so that we need to multiply the learned normalization parameter Z by n(n − 1) when comparing to t-SNE or checking normalization of the NC-t-SNE model. Similarly, we also approximate the true noise distribution by the uniform distribution for the fractions qθ(x)/(qθ(x) + ̄Zm/|X|) – instead of qθ(x)/(cid:0)qθ(x) + ̄Zmξ(x)(cid:1) – in our Neg-t-SNE implementation.\n\nWe mentioned in Sec. 5 and showed in Fig. S8 that one can move along the attraction-repulsion spectrum also by changing the number of noise samples m, instead of the fixed normalization constant ̄Z. In UMAP’s reference implementation, there is a scalar prefactor γ for the repulsive forces. Theoretically, adjusting γ should also move along the attraction-repulsion spectrum, but setting it higher than 1 led to convergence problems in (B ̈ohm et al., 2022), Fig. A11. When varying our ̄Z, we did not have such issues. For panels i in Figs. 1, S11 – S16 the Neg-t-SNE spectra were computed for ̄Z equal to Z(θt-SNE), Z NC-t-SNE, and n(n−1) m ·x, where x ∈ {5·10−5, 1·10−4, 2·10−4, 5·10−4, . . . , 1·102, 2·102, 5·102}.\n\nK.4 SIMCLR EXPERIMENTS\n\nFor the SimCLR experiments, we trained the model for 1000 epochs, of which we used 5 epochs for warmup. The learning rate during warmup was linearly interpolated from 0 to the initial learning rate. After the warmup epochs, we annealed the learning rate with a cosine schedule (without restarts) to 0 (Loshchilov & Hutter, 2017). We optimized the model parameters with SGD and momentum 0.9. We used the same data augmentations as in Chen et al. (2020) and their recommended batch size of 1024. We used a ResNet18 (He et al., 2016) as the backbone and a projection head consisting of two linear layers (512 − 1024 − 128) with a ReLU activation in-between. The loss was applied to the L2 normalized output of the projection head, but like Chen et al. (2020) we used the output of the ResNet as the representation for the linear evaluation. As similarity function we used the exponential of the normalized scalar product (cosine similarity) and always kept the temperature at 0.5, as suggested in Chen et al. (2020).\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nTable S2: Run time overview for the most compute-heavy experiments\n\nNumber of runs Time per run [min] (mean±SD)\n\nNeg-t-SNE for Figs. 1, S11, S13, S14, S15 Neg-t-SNE for Fig. S16 Neg-t-SNE for Fig. S17 NC-t-SNE (our implementation) for Fig. S10b SimCLR runs for Tab. 1\n\n360 24 3\n3 12\n\n39 ± 4 121 ± 44 3592 ± 44 786 ± 2 721 ± 16\n\nTable S3: Learned normalization parameter for NC-t-SNE and partition function of t-SNE in our experiments. Mean and standard deviation is computed over three random seeds. In our setup t-SNE is deterministic.\n\nZ NC-t-SNE [106] Z(θt-SNE) [106]\n\nMNIST Fig. 1 MNIST without EE Fig. S11 MNIST imbalanced Fig. S12 Human brain organoid Fig. S13 Zebrafish Fig. S14 C. elegans Fig. S15 Kuzushiji-49 Fig. S16\n\n34.3 ± 0.1 34.3 ± 0.1 6.15 ± 0.06 3.57 ± 0.03 30.8 ± 0.1 36.9 ± 0.7 395 ± 3\n\n8.13 6.25 3.12 1.30 7.98 11.7 89.6\n\nThe ResNet was trained on the combined CIFAR-10 train and test sets. When evaluating the accuracy, we froze the backbone, trained the classifier on the train set, and evaluated its accuracy on the test set. We used sklearn’s KNeighborsClassifier with cosine metric and k = 15 neighbors and sklearn’s LogisticRegression classifier with the SAGA solver (Defazio et al., 2014), no regularization penalty, a tolerance of 0.0001, and max iter=1000. Other parameters were left at the default values.\n\nWhen sampling negative samples for a given head, we excluded that head from the candidates for negative samples. We sampled negative samples with replacement. If the desired number of negative samples m equals twice the batch size minus 2 (m = 2b − 2, ‘full-batch repulsion’), we took the entire batch without the current head and its tail as negative samples for that head.\n\nK.5 CODE AVAILABILITY\n\nOur code is publicly available. The PyTorch implementation of contrastive neighbor embeddings can be found at https://github.com/berenslab/contrastive-ne. Details for reproducing the experiments and figures, alongside scripts and notebooks are at https://github. com/hci-unihd/cl-tsne-umap.\n\nK.6 STABILITY\n\nWhenever we reported a metric or show a graph, we ran the experiments for 3 different random seeds and reported the mean ± the standard deviation. When the standard deviation was very small, we omitted it from the main text and report it in Tab. S3. Save for the usually approximate skNN graph computation, t-SNE does not depend on a random seed. As we computed the skNN exactly with PyKeOps (Charlier et al., 2021), t-SNE is deterministic in our framework.\n\nPanels i in Figs. 1, S11 – S16 show the standard deviation as shaded area. Again, the standard deviations are very small and barely visible. The ratio of standard deviation to mean was never larger than 0.006 in panels i of Figs. 1, S11 – S16. Similarly, the standard deviation in Figs. S9 and S10, shown as shaded area, is mostly smaller than the line width.\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nFigure S6: Run times of the embedding optimization phase for the MNIST dataset using different batch sizes, training modes, and hardware. Running on GPU is much faster than on CPU; nonparametric runs are faster than parametric runs. As the batch size increases, the run time decreases strongly in all settings.\n\nK.7 COMPUTE\n\nWe ran our neighbor embedding experiments on a machine with 56 Intel(R) Xeon(R) Gold 6132 CPU @ 2.60GHz, 502 GB RAM and 10 NVIDIA TITAN Xp GPUs. The SimCLR experiments were conducted on a SLURM cluster node with 8 cores of an Intel(R) Xeon(R) Gold 5220 CPU @ 2.20GHz and a Nvidia V100 GPU with a RAM limit of 54 GB. Each experiment used one GPU at most.\n\nOur total compute is dominated by the neighbor embedding runs for Figs. 1, S11, S13–S16 and S10b and by the SimCLR experiments. Tab. S2 lists the number of runs and the average run time. We thus estimate the total compute time to be about 646 hours.\n\nOur implementation relies on pure PyTorch and our experiments were conducted on GPUs. We originally kept the batch size for all our experiments at the default of 1024, motivated by Chen et al. (2020) and Sainburg et al. (2021), but eventually noticed that the run time of the visualization experiments depends crucially on the batch size (Fig. S6). Increasing the batch size to 219 decreased the run time of the embedding optimization on MNIST data (without the skNN graph computation) from about 40 min to just above 20 s. For a batch size of at least 216, our implementation was faster than the reference implementations of UMAP and t-SNE (via openTSNE). The latter run only on CPU, while our experiments ran on GPU. Nevertheless, we observed a substantial improvement in run time for higher batch sizes also when running our PyTorch implementation on CPU and also when training a parametric embedding. Note that the parametric setting with full batch gradient descent (batch size 1,500,006 for MNIST’s skNN graph) exceeded our GPU’s memory. On CPU, our implementation is just shy of the performance of UMAP when using 219 attractive pairs per batch. Dedicated CUDA implementations (Chan et al., 2018; Eisenmann, 2019; Nolet et al., 2021) outperform our implementation on GPU. Compared to the Numba-accelerated CPU implementation of UMAP and the CUDA-accelerated GPU implementations, our pure PyTorch implementation arguably strikes a good speed / complexity trade-off, is easier to study and adapt by the machine learning community, and seamlessly integrates non-parametric and parametric settings as well as all four contrastive loss functions.\n\nNote that the change from NC-t-SNE to Neg-t-SNE is as simple as fixing the learnable normalization parameter to a constant, so the original NCVis code written in C++ can easily be adapted to compute Neg-t-SNE. We have not used this for any of the experiments in our paper.\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm S1: Batched contrastive neighbor embedding algorithm input : list of directed skNN graph edges E = [i1j1, . . . , i|E|j|E|]\n\nparameters θ\n\n// embeddings (non-param.)/ network weights\n\n(param.)\n\nnumber of epochs T learning rate η number of noise samples m Cauchy kernel q\n\n// of embeddings (non-param.)/ network output\n\n(param.)\n\nbatch size b loss mode mode normalization constant ̄Z\n\noutput: final embeddings e1, . . . , en\n\n1 if mode = NC-t-SNE then 2 Z = 1 3 for t = 0 to T do\n\n// Learning rate annealing ηt = η · (1 − t\n\nT )\n\n4 5 α = 0 6 while α < |E| do\n\n// default (cid:0)n\n\n(cid:1)/m, required for\n\nmode = Neg-t-SNE\n\n2\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\nL = 0 for β = 1, . . . , b do\n\n// Treat attractive edge iα+βjα+β and noise edges iα+βj− // Sample noise edge tails but omit head of considered edge j− 1 , . . . , j− // Aggregate loss based on mode if mode = Neg-t-SNE then\n\nm ∼ Uniform({iα+1, jα+1, . . . , jα+b}\\{iα+β})\n\nμ\n\nL = L − log\n\nqθ(iα+β jα+β ) qθ(iα+β jα+β )+ ̄Zm/(n 2)\n\n(cid:19)\n\n− (cid:80)m\n\nμ=1 log\n\n(cid:18)\n\n1 −\n\nqθ(iα+β j− μ ) μ )+ ̄Zm/(n 2)\n\nqθ(iα+β j−\n\n(cid:19)\n\nelse if mode = NC-t-SNE then\n\n(cid:16) qθ(iα+β jα+β )/Z\n\nL = L − log\n\nqθ(iα+β jα+β )/Z+m else if mode = InfoNC-t-SNE then\n\nL = L − log\n\nqθ(iα+β jα+β )\n\nqθ(iα+β jα+β )+(cid:80)m\n\nμ=1 qθ(iα+β j− μ )\n\n(cid:19)\n\n(cid:17)\n\n− (cid:80)m\n\nμ=1 log\n\n(cid:16)\n\n1 −\n\nqθ(iα+β j−\n\nqθ(iα+β j−\n\nμ )/Z μ )/Z+m\n\n(cid:17)\n\nelse if mode = UMAP then\n\nL = L − log\n\nqθ(iα+βjα+β)\n\n(cid:17)\n\n− (cid:80)m\n\nμ=1 log\n\n(cid:16)\n\n1 − qθ(iα+βj− μ )\n\n(cid:17)\n\n(cid:18)\n\n(cid:18)\n\n(cid:16)\n\n// Update parameters with SGD (non-param.) θ = θ − ηt · ∇θL if mode = NC-t-SNE then\n\nor Adam (param.)\n\nZ = Z − ηt∇ZL\n\nα = α + b\n\n22 Shuffle E 23 return θ\n\n34\n\nPublished as a conference paper at ICLR 2023\n\nL ADDITIONAL FIGURES\n\nFigure S7: Attractive and repulsive loss terms of UMAP and Neg-t-SNE. The main difference is that UMAP’s repulsive loss diverges at zero challenging its numerical optimization. The attractive terms are log(1 + d2 ij) for UMAP and Neg-t-SNE, respectively, and the repulsive ones are log (cid:0)(1 + d2\n\nij) and log(2 + d2 ij)/d2\n\nij)(cid:1), respectively.\n\n(cid:1) and log (cid:0)(2 + d2\n\nij)/(1 + d2\n\nij\n\n(a) m = 5\n\n(b) m = 50\n\n(c) m = 500\n\n(d) m = 2b − 2 = 2046\n\nFigure S8: Neg-t-SNE embeddings of the MNIST dataset for varying number of noise samples m and using batch size b = 1024. While for NC-t-SNE and InfoNC-t-SNE more noise samples improve the approximation to t-SNE, see Figs. S18 and S19, changing m in Neg-t-SNE moves the result along the attraction-repulsion spectrum (Fig. 1) with more repulsion for larger m. However, the computational complexity of Neg-t-SNE scales with m, so that moving along the spectrum via changing ̄Z is much more efficient. For the first 250 epochs, m was set to 5, to achieve an effect similar to early exaggeration (Supp. K).\n\n35\n\nPublished as a conference paper at ICLR 2023\n\n(a) InfoNC-t-SNE loss\n\n(b) NC-t-SNE loss\n\n(c) NC-t-SNE normalization\n\nFigure S9: (a, b) Loss curves for the parametric and non-parametric InfoNC-t-SNE and NC-t-SNE optimizations leading to Figs. 3b, c, e, and f. While the embedding scale differs drastically between the non-parametric and the parametric run, the loss values are close. (c) Normalization of the model (cid:80) ij φ(ij)/Z for the parametric and non-parametric NC-t-SNE optimizations. The difference in the embedding scale is compensated by a three orders of magnitude change in Z, so that both versions learn approximately normalized models. These experiments used our NC-t-SNE reimplementation.\n\n(a)\n\n(b)\n\nFigure S10: NC-t-SNE learns to have the same partition function (PF) as t-SNE on the MNIST dataset. The higher the number m of noise samples (a) or the longer the optimization (b), the better the match. Both methods used early exaggeration, which for NC-t-SNE meant to start with m = 5 noise samples for the first 250 epochs. The learned normalization parameter Z converged to but did not exactly equal NC-t-SNE’s partition function (cid:80) ij qθ(ij). Nevertheless, it was of the same order of magnitude. Again, the match was better for more noise samples. Since we reinitialized the learnable Z for NC-t-SNE after the early exaggeration phase, there were brief jumps in the partition function and in Z at the beginning of the non-exaggerated phase.\n\n36\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure S11: (a – e) Neg-t-SNE embeddings of the MNIST dataset for various values of the fixed normalization constant ̄Z. As ̄Z increases, the scale of the embedding decreases, clusters become more compact and separated before eventually starting to merge. The Neg-t-SNE spectrum produces embeddings very similar to those of (f) t-SNE, (g) NCVis, and (h) UMAP, when ̄Z equals the partition function of t-SNE, the learned normalization parameter Z of NCVis, or |X|/m = (cid:0)n (cid:1)/m used by UMAP, as predicted in Sec. 4–6. (i) The partition function (cid:80) ij)−1 tries to match ̄Z and grows with it. In contrast to Fig. 1, we did not use early exaggeration here, but initialized the Neg-t-SNE and t-SNE with PCA rescaled so that the first dimension has standard deviation 1 and 0.0001, respectively. This makes the embeddings with small ̄Z values show cluster fragmentation, similar to the t-SNE embedding in (f) without early exaggeration. For very low ̄Z, the Neg-t-SNE embedding in (a) shows very little structure.\n\nij(1 + d2\n\n2\n\n37\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure S12: (a – e) Neg-t-SNE embeddings of an imbalanced version of the MNIST dataset for various values of the fixed normalization constant ̄Z. As ̄Z increases, the scale of the embedding decreases, clusters become more compact and separated before eventually starting to merge. The Neg-t-SNE spectrum produces embeddings very similar to those of (f) t-SNE, (g) NCVis, and (h) UMAP, when ̄Z equals the partition function of t-SNE, the learned normalization parameter Z of (cid:1)/m used by UMAP, as predicted in Sec. 4–6. (i) The partition function NCVis, or |X|/m = (cid:0)n (cid:80) ij)−1 tries to match ̄Z and grows with it. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m and only switched to the desired ̄Z for the last two thirds of the optimization. The dataset was created by randomly removing 10 · c% of the class of digit c, so that the class sizes linearly decrease from digit 0 to digit 9.\n\nij(1 + d2\n\n2\n\n38\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure S13: (a – e) Neg-t-SNE spectrum on the developmental single-cell RNA sequencing dataset from Kanton et al. (2019) for various parameters ̄Z. As ̄Z increases, the scale of the embedding decreases and the continuous structure (corresponding to the developmental stage) becomes more apparent, making higher ̄Z more suitable for visualizing continuous datasets (B ̈ohm et al., 2022). The spectrum produces embeddings very similar to those of (f) t-SNE and (g) NCVis when ̄Z equals the partition function of t-SNE or the learned normalization parameter of NCVis. The UMAP embedding in (h) closely resembles the Neg-t-SNE embedding at ̄Z = |X|/m = (cid:0)n (cid:1)/m. (i) The partition function (cid:80) ij)−1 of the Neg-t-SNE embeddings increased with ̄Z. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m and only switched to the desired ̄Z for the last two thirds of the optimization. The dataset contains 20 272 cells and is colored by the duration of the development. There are ten times fewer cells collected after 10 days than after one month.\n\nij(1 + d2\n\n2\n\n39\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure S14: (a – e) Neg-t-SNE spectrum on the single-cell RNA sequencing dataset of a developing zebrafish embryo (Wagner et al., 2018) for various parameters ̄Z. As ̄Z increases, the scale of the embedding decreases and the continuous structure (corresponding to the developmental stage) becomes more apparent, making higher ̄Z more suitable for visualizing continuous datasets (B ̈ohm et al., 2022). The spectrum produces embeddings very similar to those of (f) t-SNE and (g) NCVis when ̄Z equals the partition function of t-SNE or the learned normalization parameter of NCVis. The UMAP embedding in (h) closely resembles the Neg-t-SNE embedding at ̄Z = |X|/m = (cid:0)n (cid:1)/m. (i) The partition function (cid:80) ij)−1 of the Neg-t-SNE embeddings increased with ̄Z. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m and only switched to the desired ̄Z for the last two thirds of the optimization. The dataset contains 63 530 cells and is colored by the hours post fertilization (hpf). There are ten times fewer cells collected after 8 hours than after 24.\n\nij(1 + d2\n\n2\n\n40\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure S15: (a – e) Neg-t-SNE spectrum of the single-cell RNA sequencing dataset of the C. elegans flatworm (Packer et al., 2019; Narayan et al., 2021) for various values of the fixed normalization constant ̄Z. As ̄Z increases, the scale of the embedding decreases, the scale of the embedding decreases, and more continuous structure becomes apparent. The Neg-t-SNE spectrum produces embeddings very similar to those of (f) t-SNE, (g) NCVis, and (h) UMAP, when ̄Z equals the partition function of t-SNE, the learned normalization parameter Z of NCVis, or |X|/m = (cid:0)n (cid:1)/m used by UMAP, as predicted in Sec. 4–6. (i) The partition function (cid:80) ij)−1 tries to match ̄Z and grows with it. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m and only switched to the desired ̄Z for the last two thirds of the optimization. The dataset contains information on 86 024 cells of 37 types indicated by the colors. It is imbalanced with only 25 cells of the least abundant type but 31 375 cells of unknown type (grey).\n\nij(1 + d2\n\n2\n\n41\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\n2\n\nFigure S16: (a – e) Neg-t-SNE embeddings of the Kuzushiji-49 dataset (Tarin et al., 2018) for various values of the fixed normalization constant ̄Z. As ̄Z increases, the scale of the embedding decreases, clusters become more compact and separated before eventually starting to merge. The Neg-t-SNE spectrum produces embeddings similar to those of (f) t-SNE, (g) NCVis, and (h) UMAP, when ̄Z equals the partition function of t-SNE, the learned normalization parameter Z of NCVis, or (cid:1)/m used by UMAP, as predicted in Sec. 4–6. (i) The partition function (cid:80) |X|/m = (cid:0)n ij)−1 tries to match ̄Z and grows with it. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m and only switched to the desired ̄Z for the last two thirds of the optimization. The dataset contains 270 912 images of 49 different Japanese characters. The classes are imbalanced with 456 to 7 000 samples per class. We see that a higher level of repulsion than UMAP’s ̄Z = |X|/m helps to visualize the discrete structure of the dataset. The sampling based Neg-t-SNE embedding at ̄Z = Z t-SNE has less structure than the t-SNE embedding. Fig. S17 and Tab. S4 show that the Neg-t-SNE result improves for longer optimization.\n\nij(1+d2\n\n(a) 500 epochs\n\n(b) 1000 epochs\n\n(c) 5000 epochs\n\n(d) 10000 epochs\n\nFigure S17: Neg-t-SNE embeddings of the Kuzushiji-49 dataset (Tarin et al., 2018) for ̄Z = Z t-SNE show more structure when optimized longer. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m for 250 epochs and only switched to the desired ̄Z for the remaining number of epochs indicated in the subcaptions.\n\n42\n\nPublished as a conference paper at ICLR 2023\n\nTable S4: Longer run times improve the Neg-t-SNE optimization on the Kuzushiji-49 dataset (Tarin et al., 2018) for ̄Z = Z t-SNE. The KL divergence is computed with respect to the normalized model qθ/((cid:80)\n\nij qθ(ij)).\n\nEpochs\n\n500\n\n1000\n\n5000\n\n10000\n\nPartition function [106] Neg-t-SNE loss KL divergence\n\n18.96 ± 0.02 1021 ± 2 5.52 ± 0.01\n\n13.27 ± 0.01 832 ± 2 5.18 ± 0.01\n\n6.93 ± 0.01 630 ± 1 4.76 ± 0.01\n\n5.75 ± 0.01 599 ± 1 4.65 ± 0.00\n\n(a) m = 5 random init\n\n(b) m = 50 random init\n\n(c) m = 500 random init\n\n(d) t-SNE random init\n\n(e) m = 5 PCA init\n\n(f) m = 50 PCA init\n\n(g) m = 500 PCA init\n\n(h) t-SNE PCA init\n\n(i) m = 5 EE\n\n(j) m = 50 EE\n\n(k) m = 500 EE\n\n(l) t-SNE EE\n\nFigure S18: NC-t-SNE (our implementation) on the MNIST dataset for varying number of noise samples m and different starting conditions. Higher number of noise samples m improves the approximation quality to t-SNE (last column). The first row is initialized with isotropic Gaussian noise and the second and the third rows with PCA (both normalized to have standard deviation of one or 0.0001 in the first dimension for NC-t-SNE or t-SNE, respectively). In the third row, the first 250 epochs used m = 5 and the latter used the given m value for NC-t-SNE. This is similar to t-SNE’s early exaggeration that we used in panel l. NC-t-SNE seems to be less dependent on early exaggeration than t-SNE, especially for low m values. Insets show the kNN recall and the Spearman correlation between distances of pairs of points. Higher m increases the kNN recall, while mostly leaving the Spearman correlation unchanged. Random initialization hurts the Spearman correlation, but not the kNN recall.\n\n43\n\nPublished as a conference paper at ICLR 2023\n\n(a) m = 5 random init\n\n(b) m = 50 random init\n\n(c) m = 500 random init\n\n(d) t-SNE random init\n\n(e) m = 5 PCA init\n\n(f) m = 50 PCA init\n\n(g) m = 500 PCA init\n\n(h) t-SNE PCA init\n\n(i) m = 5 EE\n\n(j) m = 50 EE\n\n(k) m = 500 EE\n\n(l) t-SNE EE\n\nFigure S19: InfoNC-t-SNE on the MNIST dataset for varying number of noise samples m and different starting conditions. Higher number of noise samples m improves the approximation quality to t-SNE (last column). The first row is initialized with isotropic Gaussian noise and the second and the third rows with PCA (both normalized to have standard deviation of one or 0.0001 in the first dimension for InfoNC-t-SNE or t-SNE, respectively). In the third row, the first 250 epochs used m = 5 and the latter used the given m value for InfoNC-t-SNE. This is similar to t-SNE’s InfoNC-t-SNE seems to be less dependent on early early exaggeration that we used in panel l. exaggeration than t-SNE, especially for low m values. Insets show the kNN recall and the Spearman correlation between distances of pairs of points. Higher m increases the kNN recall, while mostly leaving the Spearman correlation unchanged. Random initialization hurts the Spearman correlation, but not the kNN recall.\n\n44",
    "reference": "# Summary Of The Paper\n\nThis paper relates tSNE and UMAP, starting from linking noise contrastive estimation and negative sampling. The authors also provided analysis on linking neighbor embedding and self-supervised learning, and it leads to optimizing tSNE with InfoNCE loss. The provided experimental results demonstrates the authors' arguments.\n\n# Strength And Weaknesses\n\nThe paper extensively covers from neighbor embedding to self-supervised learning, and the authors linked those two concepts, which has the originality. Also, the paper is easy to follow, and the experimental results are promising. \n\nHowever, at the same time, the quantitative result seems to be lacking. Are there any other quantitative result that can be shown in the experiment of dimensionality reduction algorithms?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written, and the theoretical soundness is okay. Also, exploring the connection between two major dimensionality reduction algorithms is novel. Finally, the authors also provide their code as supplementary material.\n\nIn the meantime, the authors rely on the appendix too much, and interpretation of the experimental results lack in the main body. I suggest authors to add up more informative interpretation in the main body.\n\n# Summary Of The Review\n\nWhile I’m positive on this paper, I’m not very familiar with experiment parts of dimensionality reduction algorithms. Hence, I’m willing to see other reviewer’s comments.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nN/A"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nREWARD LEARNING WITH TREES: METHODS AND EVALUATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRecent efforts to learn reward functions from human feedback have tended to use deep neural networks, whose lack of transparency hampers our ability to explain agent behaviour or verify alignment. We explore the merits of learning intrinsically interpretable tree models instead. We develop a recently proposed method for learning reward trees from preference labels, and show it to be broadly competitive with neural networks on challenging high-dimensional tasks, with good robustness to limited or corrupted data. Having found that reward tree learning can be done effectively in complex settings, we then consider why it should be used, demonstrating that the interpretable reward structure gives significant scope for traceability, verification and explanation.\n\n1\n\nINTRODUCTION\n\nFor a reinforcement learning (RL) agent to reliably achieve a goal or desired behaviour, this objective must be encoded as a reward function. However, manual reward design is widely understood to be challenging, with risks of under-, over-, and mis-specification leading to undesirable, unsafe and variable outcomes (Pan et al., 2022). For this reason, there has been growing interest in enabling RL agents to learn reward functions from normative feedback provided by humans (Leike et al., 2018). These efforts have proven successful from a technical perspective, but an oft-unquestioned aspect of the approach creates a roadblock to practical applications: reward learning typically uses black-box neural networks (NNs), which resist human scrutiny and interpretation. For advocates of explainable AI (XAI), this is a problematic state of affairs. The XAI community is vocal about the safety and accountability risks of opaque learning algorithms (Rudin, 2019), but an inability to interpret even the objective that an agent is optimising places us in yet murkier epistemic territory, in which an understanding of the causal origins of learnt behaviour, and their alignment with human preferences, becomes virtually unattainable. Black-box reward learning could also be seen as a missed scientific opportunity. A learnt reward function is a tantalising object of study from an XAI perspective, due to its triple status as (1) an explanatory model of revealed human preferences, (2) a normative model of agent behaviour, and (3) a causal link between the two. The approach proposed by Bewley & Lecue (2022) provides a promising way forward. Here, human preference labels over pairs of agent behaviours are used to learn tree-structured reward functions (reward trees), which are hierarchies of local rules that admit visual and textual representation and can be leveraged to monitor and debug agent learning. In this paper, we adapt and extend the method (including by integrating it with model-based RL agents), and compare it to NN-based reward learning in a challenging aircraft handling domain. We find it to be broadly competitive on both quantitative metrics and qualitative assessments, with our new modification to tree growth yielding significant improvements. The resultant trees are small enough to be globally interpretable (≈ 20 leaves), and we demonstrate how they can be analysed, verified, and used to generate explanations. The primary contribution of this paper is positive empirical evidence that reward learning can be done effectively using interpretable models such as trees, even in complex, high-dimensional continuous environments. We also make secondary methodological contributions: improvements to the originally-proposed learning algorithm, as well as metrics and methods for reward evaluation and interpretability that may be useful to others working in what remains a somewhat preparadigmatic field. After reviewing the necessary background and related work in Sections 2 and 3, we present our refinement of reward tree learning in Section 4, and describe how we deploy it online with a model-based agent in Section 5. Section 6 contains our experiments and results, which consider both quantitative and qualitative aspects of learning performance, and an illustrative analysis of learnt tree structures. Finally, Section 7 concludes and discusses avenues for future work.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n2 BACKGROUND AND RELATED WORK\n\nMarkov Decision Processes (MDPs) In this formulation of sequential decision making, the state of a system at time t, st ∈ S, and the action of an agent, at ∈ A, condition the successor state st+1 according to dynamics D : S × A → ∆(S) (∆(·) denotes the set of all probability distributions over a set). A reward function R : S ×A×S → R then outputs a scalar reward rt+1 given st, at and st+1. RL uses exploratory data collection to learn action-selection policies π : S → ∆(A), with the goal of maximising the expected discounted sum of future reward, ED,π Reward Learning In the usual MDP framing, R is an immutable property of the environment, which belies the practical fact that AI objectives originate in the uncertain goals and preferences of fallible humans (Russell, 2019). Reward learning (or modelling) (Leike et al., 2018) replaces handspecified reward functions with models learnt from humans via revealed preference cues such as demonstrations (Ng et al., 2000), scalar evaluations (Knox & Stone, 2008), approval labels (Griffith et al., 2013), corrections (Bajcsy et al., 2017), and rankings (Christiano et al., 2017). The default use of NNs for reward learning severely limits interpretability; reward trees provide a possible solution.\n\nh=0 γhrt+h+1, γ ∈ [0, 1].\n\n(cid:80)∞\n\nXAI for RL (XRL) Surveys of XAI for RL (Puiutta & Veith, 2020; Heuillet et al., 2021) divide between intrinsic approaches, which imbue agents with structure such as object-oriented representations (Zhu et al., 2018) or symbolic policy primitives (Verma et al., 2018), and post hoc analyses of learnt representations (Zahavy et al., 2016), including computing feature importance/saliency (Huber et al., 2019). Spatiotemporal scope varies from the local explanation of single actions (van der Waa et al., 2018) to the summary of entire policies via representative trajectories (Amir & Amir, 2018) or critical states (Huang et al., 2018). While most post hoc methods focus on fixed policies, some provide insight into the dynamics of agent learning (Dao et al., 2018; Bewley et al., 2022).\n\nExplainable Reward Functions At the intersection of reward learning and XRL lie efforts to improve human understanding of reward functions and their effects on action selection. While this area is “less developed” than other XRL sub-fields (Glanois et al., 2021), a distinction has again emerged between intrinsic approaches which create rewards that decompose into semantic components (Juozapaitis et al., 2019) or optimise for sparsity (Devidze et al., 2021), and post hoc approaches which apply feature importance analysis (Russell & Santos, 2019), counterfactual probing (Michaud et al., 2020), or simplifying transformations (Jenner & Gleave, 2022). Sanneman & Shah (2022) use human-oriented metrics to compare the efficacy of reward explanation techniques. In this taxonomy, reward tree learning is an intrinsic approach, as the rule structure is inherently readable.\n\nTrees in RL Tree models have a long history in RL (Chapman & Kaelbling, 1991; Dˇzeroski et al., 1998; Pyeatt, 2003). Their use is increasingly given an XRL motivation. Applications again divide into intrinsic methods, where an agent’s policy (Silva et al., 2020), value function (Liu et al., 2018; Roth et al., 2019) or dynamics model (Jiang et al., 2019) is a tree, and post hoc tree approximations of an existing agent’s policy (Bastani et al., 2018; Coppens et al., 2019) or transition statistics (Bewley et al., 2022). Related to our focus on human-centric learning, Cobo et al. (2012) learn tree-structured MDP abstractions from demonstrations and Tambwekar et al. (2021) distill a differentiable tree policy from natural language. While Sheikh et al. (2022) use tree evolution to learn dense intrinsic rewards from sparse environment ones, Bewley & Lecue (2022) are the first to learn and use reward trees in the absence of any ground-truth reward signal, and the first to do so from human feedback.\n\n3 PREFERENCE-BASED REWARD LEARNING\n\nWe adopt the preference-based approach to reward learning, in which a human is presented with pairs of agent trajectories (sequences of state, action, next state transitions) and expresses which of each pair they prefer as a solution to a given task of interest. A reward function is then learnt to explain the pattern of preferences. This approach is popular in the existing literature (Wirth et al., 2016; Christiano et al., 2017; Lee et al., 2021b) and has a firm psychological basis. Experimental results indicate that humans find it cognitively easier to make relative (vs. absolute) quality judgements (Kendall, 1975; Wilde et al., 2020) and exhibit lower variance when doing so (Guo et al., 2018). This is due in part to the lack of requirement for an absolute scale to be maintained in working memory, which is liable to induce bias as it shifts over time (Eric et al., 2007). t) ∈ RF repre1,..., xi We formalise a trajectory ξi as a sequence (xi sents a single transition as an F -dimensional feature vector. Given N trajectories, Ξ = {ξi}N i=1, the human provides K ≤ N (N − 1)/2 pairwise preference labels, L = {(i, j)}K k=1, each of which indicates that the jth trajectory is preferred to the ith (denoted by ξj ≻ ξi). Figure 1 (left) shows how a preference dataset D = (Ξ, L) can be viewed as a directed graph.\n\nT i), where xi\n\nt = φ(si\n\nt−1, ai\n\nt−1, si\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Left: The input to preference-based reward learning is a directed graph over a trajectory i=1, where each edge (i, j) represents a preference ξj ≻ ξi. Each member of Ξ is a set Ξ = {ξi}N sequence of points in RF (blue connectors show mapping). Right: Application of the four model induction stages from Sections 4.1-4.4 to this example. See Appendix A.3 for an annotated version.\n\nTo learn a reward function from D, we must assume a generative model for the preference labels. Typically, it is assumed that the human produces labels in Boltzmann-rational accordance with the sum of rewards (or return) output by a latent reward function over the feature space, R : RF → R. This is formalised by adapting the classic preference model of Bradley & Terry (1952):\n\nP (ξj ≻ ξi|R) =\n\n1\n\n1 + exp( 1\n\nβ (G(ξi|R) − G(ξj|R)))\n\n, where G(ξi|R) =\n\n(cid:88)T i\n\nt=1\n\nR(xi\n\nt),\n\n(1)\n\nand β > 0 is a temperature coefficient. The objective of reward learning is to approximate R within some learnable function class R. This is often formalised as minimising the negative log-likelihood (NLL) loss over L. Wirth et al. (2016) also use the discrete 0-1 loss, which considers only the directions of predicted preferences rather than their strengths. These two losses are defined as: I[P (ξj ≻ ξi|R) ≤ 0.5].\n\n− log P (ξj ≻ ξi|R);\n\nlNLL(D, R) =\n\nl0-1(D, R) =\n\n(cid:88)\n\n(cid:88)\n\n(2)\n\n(i,j)∈L\n\n(i,j)∈L\n\n4 REWARD TREE INDUCTION\n\nIn prior work, R is the class of linear models R(x) = w⊤x (Sadigh et al., 2017), which have limited expressivity, or deep NNs (Christiano et al., 2017), which resist human interpretation. As an intermediate option, Bewley & Lecue (2022) (BL) propose the reward tree model. Here, the parameter space consists of node-level splitting rules and reward predictions for an axis-aligned decision tree, whose leaves induce a hyperrectangular partition of RF . While differentiable trees exist (Su ́arez & Lutsko, 1999), these are of the oblique (c.f. axis-aligned) kind, whose multi-feature rules are far harder to interpret in high dimensions. Therefore, instead of optimising the losses in Equation 2 end-to-end, we use a multi-stage induction method with a proxy objective at each stage. The four stages outlined below, and depicted in Figure 1 (right), depart from BL’s original method in several respects. A list of changes, and their performance implications, is given in Appendix A.1.\n\n4.1 TRAJECTORY-LEVEL RETURN ESTIMATION\n\nThis stage considers the N trajectories as atomic units, and uses the preference graph to construct a vector of return estimates g ∈ RN , which should be higher for more preferred trajectories (blue in Figure 1 (4.1), c.f. red). This is a vanilla preference-based ranking problem, and admits a standard solution. BL use a least squares matrix method to solve for g under Thurstone’s Case V preference model (Gulliksen, 1956). For consistency with prior work, and to avoid an awkward clipping step which biases preference probabilities to enable matrix inversion, we instead use a gradient method to minimise the NLL loss under the Bradley-Terry model. Concretely, the objective for this stage is\n\nargmin g∈RN\n\n(cid:104) (cid:88)\n\n(i,j)∈L\n\n− log\n\n1 1 + exp(gi − gj)\n\n(cid:105) ,\n\nsubject to\n\n(cid:26)\n\nmin(g) = 0 or max(g) = 0\n\nand std(g) = β,\n\n(3)\n\nwhere β is the mean trajectory length in Ξ, (cid:80)N i=1 T i/N . The min-or-max constraint ensures that all return estimates have the same sign (positive or negative), which aids both policy learning and interpretability (see Appendix A.2). We first optimise the NLL loss by gradient descent with the Adam optimiser (Kingma & Ba, 2014), then apply shift and scale factors to meet the two constraints.\n\n4.2 LEAF-LEVEL REWARD PREDICTION\n\nThe vector g estimates trajectory-level returns, but the aim of reward learning is to decompose these into sums of rewards for the constituent transitions, then generalise this to make reward predictions for unseen data (e.g. novel trajectories executed by a learning agent). BL’s contribution is to do this using a tree model T , consisting of a hierarchy of rules that partition the transition-level feature space RF into LT hyperrectangular subsets called leaves. Each leaf l ∈ {1..LT } is associated with\n\n3\n\n4.14.24.34.4Under review as a conference paper at ICLR 2023\n\na reward prediction rl as follows. Let the function leafT : RF → {1..LT } map a feature vector x ∈ RF to the leaf in which it resides by propagating it through the rule hierarchy. rl is defined as an average over g, weighted by the proportion of timesteps that each trajectory in Ξ spends in l: (cid:80)T i\n\nrl =\n\nN (cid:88)\n\ni=1\n\ngi T i\n\n(cid:80)N\n\nt=1 (cid:80)T j\n\nj=1\n\nt=1\n\nI[leafT (xi\n\nt) = l]\n\nI[leafT (xj\n\nt ) = l]\n\n.\n\n(4)\n\nThe effect of Equation 4 is to assign higher reward to leaves that contain more timesteps from trajectories with high g values. Predicting the reward for an arbitrary unseen feature vector x ∈ RF then involves simply looking up the reward of the leaf in which it resides: RT (x) = rleafT (x). While ostensibly na ̈ıve, BL find that this time-weighted credit assignment is more robust than several more sophisticated alternatives. It reduces the number of free parameters in subsequent induction stages, permits fast implementation, and provides an intuitive interpretation of predicted reward that is traceable back to a g value and timestep count for each ξi ∈ Ξ. Figure 1 (4.2) shows how 4, 2 and 3 timesteps from ξ1, ξ3 and ξ4 are averaged over to yield the reward prediction for one leaf (indicated by the orange shading). For more intuition on this stage, see the annotated figure in Appendix A.3.\n\n4.3 TREE GROWTH\n\nRecall that the objective of preference-based reward learning is to find a reward model that optimises a measure of fidelity to D, such as the losses in Equation 2. When the model is a tree, this is achieved by the discrete operations of growth (adding partitioning rules) and pruning (removing rules). Given a tree T , a new rule has the effect of splitting the lth leaf with a hyperplane at a location c ∈ Cf along the f th feature dimension (where Cf ⊂ R is a set of candidate split thresholds, e.g. all midpoints between unique values in Ξ). Let T + [lf c] denote the newly-enlarged tree. Splitting recursively creates an increasingly fine partition of RF . Figure 1 (4.3) shows an example with 23 leaves. A central issue is the criterion for selecting the next rule to add. BL use the proxy objective of minimising the local variance of reward predictions, which is exactly the CART algorithm (Breiman et al., 2017). While very fast, this criterion is only loosely aligned with fidelity to the preferences in D. We propose the more direct criterion of greedily maximising the immediate reduction in l0-1:\n\nargmax1≤l≤LT , 1≤f ≤F, c∈Cf\n\n(cid:2) l0-1(D, RT ) − l0-1(D, RT +[lf c]) (cid:3) .\n\n(5)\n\nIn Section 6, we show that switching to this bespoke criterion consistently improves performance. Its implementation involves a major reformulation of the tree growth algorithm; we provide vectorised, just-in-time compiled code for this in the Supplementary Material. Recursive splitting stops when no reduction in l0-1 can be achieved by any single split, or a tree size limit LT = Lmax is reached.\n\n4.4 TREE PRUNING\n\nGrowth is followed by a pruning sweep which reduces the size of the tree by rule removal. Such reduction is beneficial for both performance (Tien et al. (2022) find that limiting model capacity lowers the risk of causal confusion in preference-based reward learning) and human comprehension (in the language of Jenner & Gleave (2022), it is a form of “processing for interpretability”). Given a tree T , one pruning operation has the effect of merging two leaves into one by removing the rule at the common parent node. Let T denote the sequence of nested subtrees induced by pruning the tree recursively back to its root, at each step removing the rule that minimises the next subtree’s l0-1. We select the T ∈ T that minimises l0-1, additionally regularised by a term that encourages small trees: argminT ∈T[l0-1(D, RT ) + αLT ], where α ≥ 0. Note that even with α = 0 pruning may still yield a reduced tree, as unlike in traditional decision tree induction, the effect of individual rules on l0-1 depends on the order in which they are added or removed. In the example in Figure 1 (4.4), pruning yields a final tree with 3 leaves, for which illustrative leaf-level reward predictions are shown.\n\n5 ONLINE LEARNING SETUP\n\n5.1\n\nITERATED POLICY AND REWARD LEARNING\n\nSections 3 and 4 do not discuss the origins of the trajectories Ξ, or how reward learning relates to the downstream objective of learning a policy for the underlying task. Following most recent work since Christiano et al. (2017), we resolve both questions with an online bootstrapped approach. Assuming an episodic MDP, the ith episode of policy learning produces a new trajectory ξi to add to Ξ. We immediately connect ξi to the preference graph by asking the human to compare it to Kbatch random trajectories from the existing set (while Sadigh et al. (2017) and others have proposed active querying schemes, that is not our focus here, and this simple strategy performs satisfactorily). We then update the reward tree on the full preference graph via the four stages given in Section 4. We\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nfind that BL’s original method of starting growth from the current state of the tree causes lock-in to poor initial solutions, so instead re-grow from scratch on each update. The rule structure nonetheless tends to stabilise, as the enlarging preference graph becomes increasingly similar for later updates. For the (i + 1)th episode, the policy learning agent then attempts to optimise for the newly-updated reward. By iterating this process up to a total preference budget Kmax and/or episode budget Nmax, we hope to converge to both a reward tree that reflects the human’s preferences, and an agent policy that satisfies those preferences. Appendix A.4 contains pseudocode for the online algorithm.\n\n5.2\n\nINTEGRATION WITH MODEL-BASED RL\n\nReward learning methods are generally agnostic to the structure of the policy learning agent; this modularity is hailed as an advantage over other human-agent teaching paradigms (Leike et al., 2018). In line with most recent works, BL use a model-free RL agent, specifically soft actor-critic (SAC) (Haarnoja et al., 2018). However, other works (Reddy et al., 2020; Rahtz et al., 2022) use modelbased RL (MBRL) agents that leverage learnt dynamics models and planning. MBRL is attractive in the reward learning context because it disentangles the predictive and normative aspects of decisionmaking. Since (assuming no changes to the environment) dynamics remain stationary during online reward learning, the amount of re-learning required is reduced and along with it, the risk of pitfalls such as manipulation (Armstrong et al., 2020) and premature convergence. Additionally, MBRL can be very data-efficient; we find that switching from SAC to a model-based algorithm called PETS (Chua et al., 2018) reduces environment interaction during reward learning by orders of magnitude, and cuts wall-clock runtime (see Appendix B). PETS selects actions by decision-time planning through a learnt dynamics model D′ : S × A → ∆(S) up to a horizon H. In state s, planning searches for a sequence of H future actions that maximise return under the current reward model:\n\nargmax (a0,...,aH−1)∈AH\n\nED′\n\n(cid:104) (cid:88)H−1 h=0\n\nγhRT (φ(sh, ah, sh+1))\n\n(cid:105)\n\n, where s0 = s, sh+1 ∼ D′(sh, ah).\n\n(6)\n\nThe first action a = a0 is executed, and then the agent re-plans on the next timestep. In practice, D′ is an ensemble of probabilistic NNs, the expectation over D′ is replaced by a Monte Carlo estimate, and the optimisation is approximated by the iterative cross-entropy method.\n\n6 EXPERIMENTS AND RESULTS\n\nIn this section, we combine quantitative and qualitative evaluations to assess the performance of reward tree learning, specifically in comparison to the standard approach of using NNs. We also illustrate how the intrinsic interpretability of reward trees allows us to analyse what they have learnt. Our experiments focus on an aircraft handling domain (Figure 2), in which an agent must manoeuvre an aircraft (the ego jet, EJ) in a desired manner relative to a second reference jet (RJ) whose motion, if any, is part of the environment dynamics. We consider three tasks: Follow (turn to fly in formation with RJ on a linear path); Chase (maintain distance/line of sight to RJ as it turns randomly); and Land (approach a runway using RJ as a reference). These tasks are useful test cases for reward learning, as each has a large space of plausible reward functions, which may reflect the divergent priorities and stylistic preferences of aeronautical experts. Such expert knowledge is often tacit and difficult to codify (Sternberg & Horvath, 1999), motivating a learning-based approach. Appendix C contains a broader justification of this experimental domain alongside implementation details.\n\nFigure 2: State-action space of aircraft handling domain, and diagrams of Follow/Chase/Land tasks.\n\nIn place of costly human-in-the-loop evaluation, our experiments use synthetic oracle preferences with respect to nominal reward functions of varying complexity, which are given in Appendix C.3. This approach is popular (Griffith et al., 2013; Christiano et al., 2017; Reddy et al., 2020; Lindner et al., 2021) as it enables scalable systematic comparison, with the ability to quantify performance (and in our case, appraise learnt trees) in terms of reconstruction of a known ground truth. However, emulating a human with an oracle that responds with perfect rationality is unrealistic (Lee et al., 2021a). For this reason, Section 6.3 examines the performance impacts of noisy and myopic oracles, and a restricted data budget. Experimental details and hyperparameters are given in Appendix D.\n\n5\n\nEgo JetAction: pitch,roll, yaw, thrustdemands for EJRJEJFollowRJEJChaseRJEJLandReference JetState: poseinformationfor bothEJ and RJProceedsalong linearflight pathTask: turnonto sameflight pathand matchspeedRepresentsa targettouchdownposeTask: execute astable approachpath, ending at target poseFliessubject torandomcontrolinputTask: stayclose to RJwith lineof sight;keep safealtitudeUnder review as a conference paper at ICLR 2023\n\n6.1 QUANTITATIVE PERFORMANCE\n\nWe evaluate online reward learning with PETS using trees with the l0-1 split criterion, baselined against BL’s original variance criterion, as well as the de facto standard of NN reward learning (see Appendix D.4 for details). We use Kmax = 1000 preferences over Nmax = 200 online trajectories, and run 10 repeats. As a headline statistic, we report the oracle regret ratio (ORR): the median drop in oracle return of PETS agents deployed using each trained reward model compared with directly using the oracle reward, as a fraction of the drop to a random policy (lower is better). Below are the median (top) and minimum (bottom) ORR values across the 10 repeats for each task-model pairing: Chase Tree (0-1) .040 −.011\n\nLand Tree (0-1) .050 .011\n\nFollow Tree(0-1) .120 .057\n\nTree (var) .126 .065\n\nTree (var) .062 .010\n\nTree (var) .284 .158\n\nNN −.030 −.051\n\nNN .014 −.030\n\nNN .000 −.010\n\nWe observe that: 1) NN reward learning is strong on all tasks; 2) switching to a reward tree induces a small but variable performance hit; 3) l0-1 splitting outperforms the variance-based method; and 4) both NN and tree models sometimes exceed the direct use of the oracle (negative ORR). This has been observed before (Cao et al., 2021) and may be due to improved shaping in the learnt reward. Figure 3 expands these results with more metrics, revealing subtler trends not captured by headline ORR values. Metrics are plotted as time series over the 200 learning episodes (sliding-window medians and interquartile ranges across repeats). In the left column (a), the ORR of online trajectories shows how agent performance converges. For Follow, there is a gap between the models, with l0-1 splitting clearly aiding performance but still lagging behind the NNs. The learning curves for Chase and Land are more homogeneous, and the NNs reach only slightly lower asymptotes. For the reward tree models, (b) shows how the number of leaves changes over time. The variance-based trees tend to grow rapidly initially before stabilising or shrinking, while the l0-1 trees enlarge more conservatively, suggesting this method is less liable to overfit to small preference graphs. Trees of a readily-interpretable size (≈ 20 leaves) are produced for all tasks; it is possible that performance could be improved by independently tuning the size regulariser α per task. (c) shows l0-1 over time, which tends to increase as the growing preference graph presents a harder reconstruction problem, though the shape of all curves suggests convergence (note that random prediction gives l0-1 = 0.5). For Follow and Land, the trees that directly split on l0-1 actually perform better than the NNs; they more accurately predict the direction of preferences in the graph. The fact that this does not translate into lower ORR indicates that the problems of learning a good policy and replicating the preference dataset are not identical, a point made by Lindner et al. (2021). In the final two columns, we follow Gleave et al. (2021) in performing an unbiased, policy-invariant comparison of the models by correlating their outputs with the oracle reward functions on common evaluation datasets (see Appendix D.5 for dataset creation). We compute online correlations with the oracles in terms of both transition-level rewards (d) and the ordinal ranking of trajectories by return (e), the latter via the Kendall (1938) τ coefficient. The curves subtly differ, indicating that it is possible to reconstruct trajectory rankings (and by extension, any pairwise preferences) to a given accuracy with varying fidelity at the individual reward level. However, the common overall trend is that l0-1-based trees outperform variance-based ones, with NNs sometimes improving again by a smaller margin, and sometimes bringing no added benefit. Moving top-to-bottom down the tasks, the gap between models reduces from both sides; NN performance worsens while variance-based trees improve.\n\nFigure 3: Time series of metrics for online NN- and tree-based reward learning on all three tasks.\n\n6\n\n00.51FollowLandChaseOnline ORRaRank correlationeNNTree (0-1)Tree (var)00.5100.20.40-1 lossc00.050.10.1500.050.10.1500.050.10.150.250.50.75100.250.50.751Tree sizeb10201020102000.5100.510.20.6100.51Reward correlationdUnder review as a conference paper at ICLR 2023\n\nA potentially important factor in these experiments is that the oracle reward for Follow is a simple linear function, while the other two contain progressively more terms and discontinuities (see Appendix C.3). A trend suggested by these results is thus that the performance gap between NNs and reward trees (on both ORR and correlation metrics) reduces as the ground truth reward becomes more complex and nonlinear. Further experiments would be needed to test this hypothesis.\n\n6.2 VISUAL TRAJECTORY INSPECTION\n\nWhile useful for benchmarking, quantitative metrics provide little insight into the structure of the learnt solutions. They would also mostly be undefined when learning from humans since the ground truth reward is unknown. We therefore complement them with a visual analysis of induced agent behaviour. Figure 4 plots 500 trajectories of PETS agents using the best repeat by ORR for each task-model combination, across a range of features as well as time (see Appendix C.2 for feature definitions). Dashed curves show the trajectory with the highest predicted return according to each model. We also show trajectories for PETS agents with direct oracle access, and for random policies. The high-level trend is that all models are far closer to the oracle than random, with few examples of obviously incorrect behaviour (highlighted in red, due to colouring by ORR). While the NNs induce trajectories that are almost indistinguishable from the oracle, the l0-1-based reward trees lag not far behind. The variance-based trees produce more anomalies. Successes of the l0-1 trees include the execution of Follow with a single banked turn before straightening up, as shown by the up error time series (a). Indeed, the trajectories for this model are almost imperceptibly different from those of the NN, a result which is belied by the mediocre ORR of 0.158. This underlines the importance of joint quantitative-qualitative evaluation. For Chase (b), the l0-1 tree has learnt to keep the agent narrowly above the altitude threshold alt < 50, below which the oracle reward is strongly negative (see Appendix C.3). The threshold is violated in only eight of 500 trajectories (1.6%). For Land, the l0-1 tree replicates the oracle in producing a gradual reduction in alt (c) while usually keeping pitch close to 0 (d), although the distribution of roll values is less narrow. In contrast, the agent using the variance-based tree for Follow sometimes fails to reach the target position (e; red trajectories), and also does not reliably straighten up to reduce up error (f). For Chase, the altitude threshold does not appear to have been learnt precisely, and lower-altitude trajectories often fail to close the distance to RJ (g and h; red trajectories). For Land, the variancebased tree gives a later and less smooth descent (i), and less consistent pitch control (j), than the NN or l0-1-based tree, although all models produce a somewhat higher altitude profile than the oracle.\n\nFigure 4: Agent trajectories using the best models by ORR, with oracle and random for comparison.\n\n6.3 SENSITIVITY ANALYSIS\n\nIt is important to consider how learning performance degrades with reduced or corrupted data. In Figure 5, we evaluate the effect of varying the number of preferences Kmax (with fixed Nmax = 200) and trajectories Nmax (with fixed Kmax = 1000) on reward learning with NNs and l0-1-splitting trees. Following Lee et al. (2021a), we also create more human-like preference data via two modes of oracle irrationality: preference noise (by using a nonzero Boltzmann temperature β to give a desired error rate on the coverage datasets) and a myopic recency bias (by exponentially discounting earlier timesteps when evaluating trajectory returns). We run five repeats for all cases, and report the medians and interquartile ranges of ORR (lower is better) and rank correlation (higher is better). Both NN and tree models exhibit good robustness with respect to all four parameters. Although NNs remain superior in most cases, the gap varies, and is often reduced compared to the base\n\n7\n\nOracleTree (var)Follow0353-3131distclosing speed1200up errorTimestepChaseLand50350altlos error-1.401.55rollpitch0200030altdist horORR010-12020150Timestepdist0,00,0RandomegjhfTree (0-1)abdcHighest returnaccordingto oracleNNHighest returnaccordingto modeliUnder review as a conference paper at ICLR 2023\n\nFigure 5: Comparative sensitivity analysis of reward learning with NNs and trees.\n\ncases (bold labels). The budget sensitivity is low, with little improvement for Kmax > 1000 and Nmax > 200, and no major drop even with 25% of the data as the base case. For all tasks, the oracle error probability can increase to around 20% before significant drops in performance are observed. This is a promising indicator of the transferability of reward tree learning to imperfect human data. Another general observation is that the trends for trees are somewhat smoother than for NNs, with fewer sharp jumps and fewer instances of very high spread across the five repeats. In the right column (a), we summarise these results by taking the difference between the NN and tree metrics, and averaging across the three tasks. In all cases aside from rank correlation with β > 0, the NN-tree gap tends to become more favourable to the tree models as the varied parameter becomes more challenging (top-to-bottom). This sensitivity analysis thus indicates that reward trees are at least as robust to difficult learning scenarios as NNs, and may even be slightly more so.\n\n6.4 TREE STRUCTURE ANALYSIS\n\nThus far we have shown that reward learning with l0-1-based trees can be competitive with NNs, but not quite as performant overall. We now turn to a concrete advantage which may tip practical tradeoffs in its favour: the ability to interpret the learnt model, and analyse how its structure arises from the underlying preference graph. In this section we favour depth over breadth, so focus on the single best tree by ORR on the Chase task. The analysis in Figure 6 is divided into sections (a – d): (a) This reward tree has 17 leaves. The oracle reward, printed below, uses four features, all of which are used in the tree in ways that are broadly aligned (e.g. lower los error leads to leaves with higher reward). The model has learnt the crucial threshold alt < 50, correctly assigning low reward when it is crossed. This explains why we observe rare violations of the altitude threshold in Figure 4. However, it has not learnt the ideal distance to RJ, dist = 20, with 43.3 being the lowest value used in a rule. This could be because the underlying preference graph lacks sufficient preferences to make this distinction; adopting an active querying scheme may help to discover such subtleties efficiently. Other features besides those used by the oracle are present in the tree, indicating some causal confusion (Tien et al., 2022). This may not necessarily harm agent performance, as it could provide beneficial shaping (e.g. penalising positive closing speed, which indicates increasing distance to RJ). That may indeed be the case for this model since ORR is actually negative. (b) We plot the tree’s predicted reward against the oracle reward for all timesteps in the online trajectories (correlation = 0.903). The predictions for each leaf lie along a horizontal line. Most leaves, including 1 and 2, are well-aligned on this data because their oracle reward distributions are tightly concentrated around low/high averages respectively (note that the absolute scale is irrelevant here). Leaf 16 has a wider oracle reward distribution, with several negative outliers. An optimal tree would likely split this leaf further, perhaps using the alt < 50 threshold. The one anomaly is leaf 13, which contains just a single timestep from ξ77. This trajectory is the eighth best in the dataset by oracle return, but this leaf assigns that credit to a state that seemingly does not merit it, as the distance to RJ is so high (dist > 73). This may be an example of suboptimal reward learning, but the fact that its origin can be pinpointed precisely is a testament to the value of interpretability. (c) We leverage the tree structure to produce a human-readable explanation of reward predictions for a single trajectory, which may be of value to an end user (e.g. a pilot). We consider ξ191, a rare\n\n8\n\nOraclemyopia( )ChaseFollowPreferencebudget( )Trajectorybudget( )NNTree (0-1)Land100020004000500250200400800100500.980.20.30.400.950.90.80.10.45Oracle errorprobability(via )1ORR00.51Rank correlation00.5100.5100.5100.5100.51Average NN-Tree GapNNbetter-0.2500.25-0.2500.25ORRORRORRTreebetterTreebetterNNbetteraRank correlationRank correlationRank correlationUnder review as a conference paper at ICLR 2023\n\nFigure 6: Analysis of a reward tree learnt for the Chase task.\n\ncase that violates the altitude threshold. The time series of reward shows that the 20 timesteps are spent in leaves 16, 15, 11 and 7. Rescaled oracle rewards are overlaid in teal, and show that the model’s predictions are well-aligned. To the right, we translate this visualisation into a textual form, similar to a nested program. Read top-to-bottom, the text indicates which rules of the tree are active at each timestep, and the effect this has on predicted reward. This trajectory starts fairly positively, with reward gradually increasing over the first 16 timesteps as dist is reduced to between 43.3 and 73, but then falls dramatically when the alt < 50 threshold is crossed. We are unaware of any method that could extract such a compact explanation of sequential predictions from an NN. (d) We isolate a subtree, starting at the root node, that splits only on dist and alt. We give a spatial representation of the subtree, and how it is populated by the 200 online trajectories, using a 2D partition plot analogous to those in Figure 1. Zooming into leaf 1, which covers cases where the altitude threshold is violated, we see that it contains a total of 30 timesteps across four trajectories. By Equation 4, the low reward for this leaf results from a weighted average of the return estimates for these four trajectories, which in turn (by Equation 3) are derived from the preference graph. We can use this inverse reasoning to ask why this leaf has much lower reward than its sibling (leaf 2 of the subtree). A proximal explanation comes by filtering the graph for preferences that specifically compare trajectories that visit those two leaves. 49 such preferences exist, and in all cases, the oracle prefers the trajectory that does not visit leaf 1. Some of these preferences may be more practically salient than others. For example, we might highlight trajectories that feature more than once (e.g. ξ28 is preferred to both ξ18 and ξ48), or cases where trajectories with low overall return estimates are nonetheless preferred to those in leaf 1 (e.g. ξ43 ≻ ξ21 and ξ56 ≻ ξ47). We believe that much more could be done to extend this framework for traceable explanation of preference-based reward.\n\n7 CONCLUSION AND FUTURE WORK\n\nReward learning with trees provides a promising alternative to black-box NNs, and could enable more trustworthy and verifiable agent alignment. Through oracle experiments on high-dimensional tasks, we show that reward trees with around 20 leaves can achieve quantitative and qualitative performance close to that of NNs, with a more direct split criterion bringing consistent improvements. We find evidence that the NN-tree gap reduces as the ground truth reward becomes more nonlinear, and remains stable or reduces further in the presence of limited or corrupted data. While practical applications may accept some loss in performance for a gain in interpretability, further algorithmic improvements should be sought, including to move beyond locally-greedy split criteria. However, our immediate aim is to develop an end-to-end framework for explainable model-based agents with preference-based reward trees (roughly: planning can be reframed as comparing alternative paths through the discrete leaves of the tree). Having established this framework, we then intend to evaluate reward learning and explanation with real human preferences in the aircraft handling domain.\n\n9\n\nr=2.48-----dist≥43.3?r=3.12-----alt≥50.2?Nor=2.15-----dist≥95.4?Yes(1) r=0.937Nor=3.17-----los error≥1.10?Yesr=3.22-----roll error≥0.847?Nor=3.04-----abs roll≥1.56?Yes(2) r=3.35No(3) r=3.18Yesr=3.14-----closing speed≥7.63?No(6) r=2.92Yes(4) r=3.16No(5) r=2.47Yesr=2.31-----alt≥50.0?Nor=1.72-----closing speed≥8.34?Yes(7) r=0.763Nor=2.34-----dist≥73.0?Yesr=2.47-----abs roll≥0.744?Nor=2.22-----alt error≥2.14?Yesr=2.72-----dist≥68.7?Nor=2.37-----delta hdg error≥-3.68?Yes(8) r=2.78No(9) r=2.45Yes(10) r=3.20No(11) r=2.37Yes(12) r=2.76Nor=2.21-----los error≥0.0126?Yes(13) r=3.92Nor=2.21-----thrust≥3.54?Yes(14) r=1.70No(15) r=2.21Yes(16) r=1.76No(17) r=0.78YesIntermediate reward fornon-leaf node by timestep-weighted averageOracle reward:adr=2.48-----dist≥43.3?r=3.12-----Nor=2.15-----dist≥95.4?Yes(1) r=0.937No(2) r=3.17Yesr=2.31-----alt≥50.0?No(6) r=1.72Yes(3) r=0.763Nor=2.34-----dist≥73.0?Yes(4) r=2.47No(5) r=2.22Yes043.395.4150dist0350alt5073alt≥50.2?Total: 30 timesteps1245631andPredicted reward04.45b(13) r = 3.92(2) r = 3.35(16) r = 1.76(1) r = 0.937, t = 2(16)(15)(11)(7)14Predicted rewardTimestep120cOracle reward(rescaled)-300-200-100014Oracle rewardPredicted rewardTimestepTrajectory returnestimate089Under review as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nReward learning from feedback is a technique for improving the alignment of learning agents with human preferences, by replacing the rigidity of explicit reward design with a dynamic interaction with a human-in-the-loop. As such, its successful use can benefit the performance, reliability and safety of these learning systems, which has the potential to deliver immensely positive ethical value. The contribution of reward trees is to render the process of reward learning more humaninterpretable, and thus easier to explain, debug and verify. We believe this can deliver a further reduction in ethical risk through the identification and mitigation of unforeseen consequences.\n\nPreference-based reward tree learning has many diverse applications. The aircraft handling domain used in our evaluation was selected to provide a good balance of technical complexity, task diversity, industrial relevance, intuitiveness for our intended readership, and transferability to other domains such as land and sea transportation. In addition to this wide range of related use cases, as well as civilian uses of aviation itself (e.g. aerobatics), we acknowledge that the ability to interactively learn aircraft control policies may have applications in the defence sector. The three concrete tasks of Follow, Chase and Land are largely application-neutral, with no implication of harm, and are concerned solely with the safe and human-like control of aircraft in environments with other aircraft. Any general learning technique such as ours is, however, fundamentally dual-use, and transparency about this fact seems to us the best mitigation of the ethical risk. It is vital that anyone intending to use or develop our method continues to do so in an ethically responsible manner.\n\nREFERENCES\n\nDan Amir and Ofra Amir. Highlights: Summarizing agent behavior to people. In Proceedings of\n\nthe 17th International Conference on Autonomous Agents and MultiAgent Systems, 2018.\n\nStuart Armstrong, Jan Leike, Laurent Orseau, and Shane Legg. Pitfalls of learning a reward function online. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, 2020.\n\nAndrea Bajcsy, Dylan P Losey, Marcia K O’Malley, and Anca D Dragan. Learning robot objectives from physical human interaction. In Conference on Robot Learning, pp. 217–226. PMLR, 2017.\n\nOsbert Bastani, Yewen Pu, and Armando Solar-Lezama. Verifiable reinforcement learning via policy\n\nextraction. Advances in Neural Information Processing Systems, 31, 2018.\n\nTom Bewley and Freddy Lecue. Interpretable preference-based reinforcement learning with treestructured reward functions. In Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems, pp. 118–126, 2022.\n\nTom Bewley, Jonathan Lawry, and Arthur Richards. Summarising and comparing agent dynamics with contrastive spatiotemporal abstraction. In IJCAI/ECAI Workshop on Explainable Artificial Intelligence, 2022.\n\nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method\n\nof paired comparisons. Biometrika, 39(3/4):324–345, 1952.\n\nLeo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone. Classification and\n\nregression trees. Routledge, 2017.\n\nZehong Cao, KaiChiu Wong, and Chin-Teng Lin. Weak human preference supervision for deep reinforcement learning. IEEE Transactions on Neural Networks and Learning Systems, 32(12): 5369–5378, 2021.\n\nDavid Chapman and Leslie Pack Kaelbling. Input generalization in delayed reinforcement learning:\n\nAn algorithm and performance comparisons. In Ijcai, volume 91, pp. 726–731, 1991.\n\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Advances in Neural Information Processing Systems, 31, 2018.\n\nLuis C Cobo, Charles L Isbell Jr, and Andrea L Thomaz. Automatic task decomposition and state\n\nabstraction from demonstration. Georgia Institute of Technology, 2012.\n\nYouri Coppens, Kyriakos Efthymiadis, Tom Lenaerts, and Ann Now ́e. Distilling deep reinforcement learning policies in soft decision trees. In IJCAI/ECAI Workshop on Explainable Artificial Intelligence, 2019.\n\nGiang Dao, Indrajeet Mishra, and Minwoo Lee. Deep reinforcement learning monitor for snapshot recording. In 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), pp. 591–598. IEEE, 2018.\n\nJan De Leeuw and Patrick Mair. Multidimensional scaling using majorization: Smacof in r. Journal\n\nof statistical software, 31:1–30, 2009.\n\nRati Devidze, Goran Radanovic, Parameswaran Kamalaruban, and Adish Singla. Explicable reward design for reinforcement learning agents. Advances in Neural Information Processing Systems, 34:20118–20131, 2021.\n\nSaˇso Dˇzeroski, Luc De Raedt, and Hendrik Blockeel. Relational reinforcement learning. In Inter-\n\nnational Conference on Inductive Logic Programming, pp. 11–22. Springer, 1998.\n\nBrochu Eric, Nando Freitas, and Abhijeet Ghosh. Active Preference Learning with Discrete Choice\n\nData. Advances in Neural Information Processing Systems, 20, 2007.\n\nClaire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao, and Wulong Liu.\n\nA survey on interpretable reinforcement learning. arXiv preprint arXiv:2112.13112, 2021.\n\nAdam Gleave, Michael D Dennis, Shane Legg, Stuart Russell, and Jan Leike. Quantifying differ-\n\nences in reward functions. In International Conference on Learning Representations, 2021.\n\nShane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. Policy shaping: Integrating human feedback with reinforcement learning. Advances in neural information processing systems, 26, 2013.\n\nHarold Gulliksen. A least squares solution for paired comparisons with incomplete data. Psychome-\n\ntrika, 21(2):125–134, 1956.\n\nYuan Guo, Peng Tian, Jayashree Kalpathy-Cramer, Susan Ostmo, J Peter Campbell, Michael F Chiang, Deniz Erdogmus, Jennifer G Dy, and Stratis Ioannidis. Experimental design under the bradley-terry model. In IJCAI, pp. 2198–2204, 2018.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pp. 1861–1870. PMLR, 2018.\n\nAlexandre Heuillet, Fabien Couthouis, and Natalia D ́ıaz-Rodr ́ıguez. Explainability in deep rein-\n\nforcement learning. Knowledge-Based Systems, 214:106685, 2021.\n\nSandy H Huang, Kush Bhatia, Pieter Abbeel, and Anca D Dragan. Establishing appropriate trust via critical states. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3929–3936. IEEE, 2018.\n\nTobias Huber, Dominik Schiller, and Elisabeth Andr ́e. Enhancing explainability of deep reinforcement learning through selective layer-wise relevance propagation. In Joint German/Austrian Conference on Artificial Intelligence (K ̈unstliche Intelligenz), pp. 188–202. Springer, 2019.\n\nErik Jenner and Adam Gleave. Preprocessing reward functions for interpretability. arXiv preprint\n\narXiv:2203.13553, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nWei-Cheng Jiang, Kao-Shing Hwang, and Jin-Ling Lin. An experience replay method based on tree structure for reinforcement learning. IEEE Transactions on Emerging Topics in Computing, 9(2): 972–982, 2019.\n\nZoe Juozapaitis, Anurag Koul, Alan Fern, Martin Erwig, and Finale Doshi-Velez. Explainable reinforcement learning via reward decomposition. In IJCAI/ECAI Workshop on Explainable Artificial Intelligence, 2019.\n\nMaurice G Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81–93, 1938.\n\nMG Kendall. Rank Correlation Methods; Griffin, C., Ed, 1975.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nW Bradley Knox and Peter Stone. Tamer: Training an agent manually via evaluative reinforcement. In 2008 7th IEEE international conference on development and learning, pp. 292–297. IEEE, 2008.\n\nKimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel. B-pref: Benchmarking preference-based\n\nreinforcement learning. Advances in Neural Information Processing Systems, 35, 2021a.\n\nKimin Lee, Laura M Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. In International Conference on Machine Learning, pp. 6152–6163. PMLR, 2021b.\n\nJan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.\n\nDavid Lindner, Matteo Turchetta, Sebastian Tschiatschek, Kamil Ciosek, and Andreas Krause. Information directed reward learning for reinforcement learning. Advances in Neural Information Processing Systems, 34:3850–3862, 2021.\n\nGuiliang Liu, Oliver Schulte, Wang Zhu, and Qingcan Li. Toward interpretable deep reinforcement In Joint European Conference on Machine Learning and\n\nlearning with linear model u-trees. Knowledge Discovery in Databases, pp. 414–429. Springer, 2018.\n\nEric J Michaud, Adam Gleave, and Stuart Russell. Understanding learned reward functions. arXiv\n\npreprint arXiv:2012.05862, 2020.\n\nAndrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Icml, vol-\n\nume 1, pp. 2, 2000.\n\nAlexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping In International Conference on Learning Representations,\n\nand mitigating misaligned models. 2022.\n\nErika Puiutta and Eric Veith. Explainable reinforcement learning: A survey. In International crossdomain conference for machine learning and knowledge extraction, pp. 77–95. Springer, 2020.\n\nLarry D Pyeatt. Reinforcement learning with decision trees. In 21 st IASTED International Multi-\n\nConference on Applied Informatics, pp. 26–31, 2003.\n\nMatthew Rahtz, Vikrant Varma, Ramana Kumar, Zachary Kenton, Shane Legg, and Jan Leike. Safe\n\ndeep rl in 3d environments using human feedback. arXiv preprint arXiv:2201.08102, 2022.\n\nSiddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, and Jan Leike. Learning human objectives by evaluating hypothetical behavior. In International Conference on Machine Learning, pp. 8020–8029. PMLR, 2020.\n\nAaron M Roth, Nicholay Topin, Pooyan Jamshidi, and Manuela Veloso.\n\nConservative qimprovement: Reinforcement learning for an interpretable decision-tree policy. arXiv preprint arXiv:1907.01180, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and\n\nuse interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019.\n\nJacob Russell and Eugene Santos. Explaining reward functions in Markov decision processes. In\n\nThe Thirty-Second International Flairs Conference, 2019.\n\nStuart Russell. Human compatible: Artificial intelligence and the problem of control. Penguin,\n\n2019.\n\nDorsa Sadigh, Anca D Dragan, Shankar Sastry, and Sanjit A Seshia. Active preference-based learn-\n\ning of reward functions. In Proceedings of Robotics: Science and Systems (RSS), 2017.\n\nLindsay Sanneman and Julie A Shah. An empirical study of reward explanations with human-robot\n\ninteraction applications. IEEE Robotics and Automation Letters, 2022.\n\nHassam Ullah Sheikh, Shauharda Khadka, Santiago Miret, Somdeb Majumdar, and Mariano Phielipp. Learning intrinsic symbolic rewards in reinforcement learning. In 2022 International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE, 2022.\n\nAndrew Silva, Matthew Gombolay, Taylor Killian, Ivan Jimenez, and Sung-Hyun Son. Optimization methods for interpretable differentiable decision trees applied to reinforcement learning. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108, pp. 1855–1865. PMLR, 26–28 Aug 2020.\n\nRobert J Sternberg and Joseph A Horvath. Tacit knowledge in professional practice: Researcher\n\nand practitioner perspectives. Psychology Press, 1999.\n\nAlberto Su ́arez and James F Lutsko. Globally optimal fuzzy decision trees for classification and regression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 21(12):1297–1311, 1999.\n\nPradyumna Tambwekar, Andrew Silva, Nakul Gopalan, and Matthew Gombolay. Specifying and interpreting reinforcement learning policies through simulatable machine learning. arXiv preprint arXiv:2101.07140, 2021.\n\nJeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D Dragan, and Daniel Brown. A study of causal confusion in preference-based reward learning. arXiv preprint arXiv:2204.06601, 2022.\n\nJasper van der Waa, Jurriaan van Diggelen, Karel van den Bosch, and Mark Neerincx. Contrastive explanations for reinforcement learning in terms of expected consequences. In IJCAI/ECAI Workshop on Explainable Artificial Intelligence, 2018.\n\nAbhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri. Programmatically interpretable reinforcement learning. In International Conference on Machine Learning, pp. 5045–5054. PMLR, 2018.\n\nNils Wilde, Alexandru Blidaru, Stephen L Smith, and Dana Kuli ́c. Improving user specifications for robot behavior through active preference learning: Framework and evaluation. The International Journal of Robotics Research, 39(6):651–667, 2020.\n\nChristian Wirth, Johannes F ̈urnkranz, and Gerhard Neumann. Model-free preference-based rein-\n\nforcement learning. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n\nTom Zahavy, Nir Ben-Zrihem, and Shie Mannor. Graying the black box: Understanding dqns. In\n\nInternational conference on machine learning, pp. 1899–1908. PMLR, 2016.\n\nGuangxiang Zhu, Zhiao Huang, and Chongjie Zhang. Object-oriented dynamics predictor. Advances\n\nin Neural Information Processing Systems, 31, 2018.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA METHODOLOGICAL DETAILS\n\nA.1 LIST OF CHANGES TO BEWLEY & LECUE’S ORIGINAL METHOD\n\n• Change from Thurstone to Bradley-Terry preference model and least squares matrix\n\nmethod to gradient descent on lNLL;\n\n– Motivation: Consistency with prior work. – Performance implications: Thurstone → Bradley-Terry minimally affects results. Ma-\n\ntrix method → gradient descent eliminates a bias caused by preference clipping.\n\n– Computational implications: Thurstone → Bradley-Terry improves computational efficiency (bypasses inverse normal CDF computation). Matrix method → gradient descent tends to slightly increase runtime, but this depends on preference dataset size.\n\n• Add scale and sign constraints to return estimates;\n\n– Motivation / implications: Scaling results in leaf-level reward predictions having a consistent scale with a standard deviation close to 1. This is intended to aid human readability but has no effect on performance. For sign constraints, see Appendix A.2.\n\n• Change from variance-based to l0-1-based split criterion during tree growth;\n\n– Motivation: Hypothesised performance gain; new split criterion is more directly\n\naligned with the objective of preference reconstruction.\n\n– Performance implications: Significantly improves preference construction and quan-\n\ntitative/qualitative agent performance on evaluation tasks; see Section 6.\n\n– Computational implications: More costly as variance computation has an extremely efficient iterative implementation. However, we have developed optimised code for the new splitting method using just-in-time compilation; see Supplementary Material.\n\n• Deploy online with a model-based (PETS) RL agent instead of model-free (SAC);\n\n– Motivation / implications: See Section 5.2 and Appendix B.\n\n• Always use latest online trajectory in all pairs during each preference batch;\n\n– Motivation: Acts as a simple form of active sampling to correct reward overestimation; model-based planning is liable to exploit any behaviours with inappropriately high current reward, which can then be immediately corrected by a negative preference. – Performance implications: Performs similarly to BL’s active sampling method, which\n\nup-weights trajectories in Ξ with high predicted return.\n\n– Computational implications: Less expensive than BL’s method; no need to recompute\n\nreturn predictions for all trajectories in Ξ on each batch.\n\n* Note: New method would be less effective with a model-free agent where policy updates are gradual; can rely less on agent immediately exploiting current reward.\n\n• Regrow tree from scratch on each update.\n\n– Motivation: Prevents rule structure from prematurely converging to local minima. – Performance implications: Early experiments indicated that premature convergence problem is mitigated by this change, resulting in more sustained improvements in reward fidelity and agent performance.\n\n– Computational implications: Since splits are evaluated and made per update step, computation time is increased. However, when typical post-pruning tree size LT (≈ 20 in our experiments) is small compared with Lmax (= 100), this increase is fractional, and contributes only a few percentage points to overall runtime.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA.2 SIGN CONSTRAINT FOR RETURN ESTIMATES\n\nApplying a sign constraint to the trajectory-level return estimates means that rewards output by a reward tree (via Equation 4) are also all either positive or negative. This has no effect on any measure of preference reconstruction since preferences are invariant to affine transformations of an underlying utility function. However, we find it brings two distinct benefits:\n\n• Enabling the prevention of perverse incentives for agents to terminate or elongate episodes in tasks with termination conditions (negative rewards on non-terminal transitions incentivise termination, while positive rewards incentivise elongation).\n\n• Simplifying the manual interpretation of tradeoffs between rewards from different leaves of a tree (understanding the relative impacts of “more of a negative reward” and “less of a positive reward” requires the awkward mental juggling of negatives).\n\nFor the task with a termination condition in this paper (Land), we use negative rewards (max = 0 constraint) to disincentivise episode elongation, because termination is generally indicative of success. For the two fixed-length tasks (Follow and Chase) we default to using positive rewards (min = 0 constraint). Although this is arbitrary, our own experience is that positive rewards make for somewhat more intuitive interpretation of the tree structure, and its effect on agent actions. We stress that this is purely anecdotal; the relative human interpretability of positive, negative and mixed-sign rewards would be a worthy subject for deeper empirical investigation.\n\nA.3 ANNOTATED VERSION OF FIGURE 1\n\nFigure A1: Annotated version of Figure 1.\n\n15\n\nA dataset of pairwise preferences over trajectories can be visualised as a directed graph, where nodes are trajectories and edges are preferences. Each trajectory is a sequence of vectors in feature space. In turn, each vector represents the (state, action, next state) transition that occurs at a particular timestep t.Feature vector for timestep t:mappingGiven an existing tree structure and trajectories with estimated returns, we define the reward for each leaf as a timestep-weighted average over the trajectories that visit that leaf.4.2Naive assumption: all timesteps in trajectorycontribute equally to return (i.e. uniform colour)Reward calculation for highlighted leaf:Growing a tree by adding rules recursively splits the feature space into an increasing number of leaves. In our improved growth method, we use the 0-1 loss as the criterion for selecting rules to add. 4.3Growth tends to separate high- and low-return trajectories into different leaves\"Pure\" leafcontaining asingletrajectoryAfter growing to a maximum size, a final pruning stage recursively removes rules to minimise the 0-1 loss, with an additional regularisation term to penalise large trees. This yields a reduced tree for use as a reward function.4.4Pruning merges leaves back together, while retainingthe most important splits for preference predictionReward values shown are illustrative4.1Before doing anything in the feature space, we first use the standard Bradley-Terry preference model to estimate an overall return value for each trajectory in the dataset based on its preferences.Two favourable preferences;highest estimated returnThree unfavourablepreferences; lowestestimated returnPreferred to but not to ; intermediate estimated returnEach edge points topreferred trajectoryUnder review as a conference paper at ICLR 2023\n\nA.4 PSEUDOCODE FOR ONLINE ALGORITHM\n\nAlgorithm 1 Online preference-based reward tree learning\n\nInputs: Possibly pre-trained dynamics model D′, feature function φ, trajectory budget Nmax, preference budget Kmax, tree size limit Lmax, tree size regularisation α\n\nInitialise time t ← 0 and environment state si 0\n▷ Model-based trajectory generation (Sec.5.2) while episode not yet terminated do\n\n1: Initialise empty preference graph Ξ ← ∅, L ← ∅ 2: Initialise one-leaf tree T with r ← [0] 3: for i ∈ {1, ..., Nmax} do 4: 5: 6:\n\nt using PETS algorithm\n\nCompute action ai with D′ and rewards via Equation 4 Send ai Update D′ on recent transitions xi t+1 ← φ(si t ← t + 1\n\nt, ai\n\nt+1)\n\nt, si\n\nt to environment and get next state si\n\nt+1\n\n▷ May not be required; see Appendix D.3\n\n1, ..., xi\n\nend while ξi ← (xi Kbatch ← min((Kmax − |L|)/(Nmax + 1 − i), |Ξ|) for k ∈ {1, ..., Kbatch} do\n\nT i)\n\nSample ξj from Ξ uniformly without replacement Query human for preference ξi ≻ ξj or ξj ≻ ξi\n\nL ← L ∪\n\n(cid:26) {(i, j)} {(j, i)}\n\nif ξj ≻ ξi otherwise\n\n▷ Preference batch collection (Sec 3)\n\nend for Ξ ← Ξ ∪ {ξi} if |L| > 0 then\n\nCompute g via Equation 3 Initialise one-leaf tree T C ← midpoints between per-feature unique values in Ξ while LT < Lmax do\n\nfor l ∈ {1, ..., LT } do\n\nfor f ∈ {1, ..., F } do for c ∈ Cf do\n\nCompute l0-1 reduction for T + [lf c] via Equation 5\n\n▷ Trajectory-level return estimation (Sec 4.1)\n\n▷ Tree growth (Section 4.3)\n\nend for\n\nend for\n\nend for if max(l0-1 reduction) ≤ 0 then\n\nbreak\n\nend if l, f, c ← argmax(l0-1 reduction) T ← T + [lf c]\n\nend while T = () while LT > 1 do\n\n▷ Stop tree growth early\n\n▷ Tree pruning (Section 4.4)\n\nfor l ∈ {1, ..., LT } do\n\nCompute l0-1 reduction for T − [l]\n\n▷ T − [l] denotes pruning lth leaf\n\nend for l ← argmax(l0-1 reduction) T ← T − [l] Append T to T\n\nend while T ← argminT ∈T(l0-1 plus α-scaled tree size)\n\nend if\n\n7: 8: 9: 10: 11: 12: 13: 14: 15: 16:\n\n17:\n\n18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: 45: 46: 47: 48: 49: end for\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nB COMPARISON TO MODEL-FREE REINFORCEMENT LEARNING\n\nOne of the most consistently observed benefits of model-based RL is its sample efficiency, and this trend holds in our context. Running Algorithm 1 unchanged except for the use of a soft actor-critic (SAC) agent for policy learning, we find that approximately two orders of magnitude more environment interaction is required to achieve equivalent performance in terms of regret at convergence. In turn, this increases wall-clock runtime by 10-20 times, thereby outweighing the higher per-timestep computational cost of PETS over SAC. The caption of Figure A2 gives further details.\n\nFigure A2: Comparing the use of PETS (model-based) and SAC (model-free) agents on the Follow task (see Appendix C.3 for task details). The PETS results are taken directly from Figure 3. For SAC we retain the total preference budget of Kmax = 1000, but for longer runs add episode trajectories to Ξ and L at a reduced frequency so that Nmax = 200 (e.g. for 50000 episodes, only 1 in 250 episodes are added to the graph; the rest are skipped). All SAC agents use policy and value networks with two 256-unit hidden layers each, learning rates of 1e−4 and 1e−3 for the policy and value updates with the Adam optimiser, a discount factor of γ = 0.99, and an interpolation factor of 0.99 for Polyak averaging of the target networks. Updates use mini-batches of 32 transitions sampled uniformly from a rolling replay buffer of capacity 5e4. We find that initially running SAC for a total of 200 episodes, matching our PETS experiments, gives the model-free learning algorithm insufficient time to achieve good performance in terms of regret on the oracle reward function (a higher learning rate leads SAC to become unstable). We then progressively increase the length of SAC runs until regret performance matches the use of PETS, and find that this requires around 50000 episodes, an increase of 250 times. It is noteworthy that the variance-based tree model seems to perform best in the short-runtime regime, but worst in the long-runtime regime. Time constraints prevented us from investigating whether this holds in other tasks, but such an investigation would be worthwhile. In terms of wall-clock time (on a single NVIDIA Tesla P100 GPU), running reward learning with SAC for 1000 episodes is roughly equivalent to 200 episodes using PETS (25-60 minutes, depending on the reward model architecture). For 50000 episodes, this time increases to 9 hours. This brings a very practical disadvantage: if reward learning were done using human preferences instead of an oracle, that person would have to dedicate more than a full working day to the exercise, most of which would be spent waiting for several minutes between each successive preference batch.\n\nNote: The PETS wall-clock times quoted here exclude the time to pre-train the dynamics models. Although this is not how MBRL sample complexity is typically measured, we argue that it is appropriate for the reward learning context, where the key factor is the period for which a human would be required to be in-the-loop. Regardless, pre-training in the aircraft handling domain takes around 30 minutes, which remains low compared with the 9 hours for high-performing SAC agents.\n\n17\n\nPETS (200 episodes)Online ORRNNTree (var)Tree (0-1)SAC (200 episodes)SAC (1k episodes)SAC (10k episodes)SAC (50k episodes)Wall-clock time(minutes)Episode1200200400120040010001200400100001200400500001200200400EpisodeEpisodeEpisodeEpisode00.5100.5100.5100.5100.51Under review as a conference paper at ICLR 2023\n\nC AIRCRAFT HANDLING ENVIRONMENT\n\nC.1 MOTIVATION\n\nPilots of fast jet aircraft require exceptional handling abilities, acquired over years of advanced training. There would be immense practical value in developing a method for distilling the knowledge and preferences of pilots and other domain experts into a software model that captures realistic handling behaviour. The scalability of such a model would make it useful for strategic planning exercises, training of a range of operational roles, and development and testing of other software systems. However, as in many contexts where intuitive decision-making and rapid motor control are paramount, the preferences of experts (over the space of fast jet handling trajectories) are in large part tacit, and thus defy direct scrutiny or verbal description. Put simply: experts know good handling when they see it, but cannot directly express why.1 This makes it practically challenging to accurately elicit this knowledge for codification into an automated system.\n\nThe methods presented in this paper form the basis of a possible solution to this dilemma. Given a dataset of trajectories executed by an artificial learning agent and labelled with pairwise expert preferences (which require only tacit knowledge to produce), we use statistical learning algorithms to construct an interpretable explanatory model of those preferences. The result is two distinct outputs that could form valuable components of future planning, training and development software:\n\n1. A tree-structured reward function, which may be used for automated scoring of flight trajectories executed by human or artificial pilots. We aim for this to produce an evaluation that is consistent, unbiased and aligned with the judgement that the original expert would have made, alongside an explanatory rationale that can be leveraged to justify, verify and improve handling behaviour.\n\n2. A model-based RL agent capable of executing high-quality handling behaviour with respect\n\nto the reward function, for use in simulation.\n\nIt should be noted that any realistic handling scenario would involve multiple experts somewhatdiffering knowledge and expertise. A natural extension of our approach, which we see as valuable future work, is to learn individual reward functions for each expert, then leverage the intrinsic interpretability to identify biases, inconsistencies and tradeoffs. This suggests a third application of reward tree learning: providing a basis for evaluating and training the experts themselves.\n\nC.2\n\nIMPLEMENTATION\n\nWe consider a simple set-piece formulation of the aircraft handling problem, in which the piloting agent is given a short time window to manoeuvre their aircraft (the ego jet, EJ) in a particular manner relative to a second reference jet (RJ). Special cases of this formulation create a wide variety of tasks for the pilot to solve. Options include:\n\n• RJ is a friendly aircraft which EJ should accompany in formation flight.\n\n• RJ is adversarial and EJ must outmanoeuvre it to gain a tactical advantage.\n\n• Rather than being a distinct physical entity, RJ defines a goal pose (position and attitude)\n\nfor EJ to reach. The goal pose may be fixed or moving over time.\n\nWe developed this formulation to strike a balance between simplicity and generality; many realistic scenarios faced by a fast jet pilot involve interaction with a single other airborne entity. On a practical level, it provides scope for the definition of many alternative tasks given the same state and action spaces, and largely unchanged dynamics.\n\nThe state space contains the positions, attitudes, velocities and accelerations of both EJ and RJ (state dimensionality = 37) and the action space consists of pitch, roll, yaw and thrust demands for\n\n1This statement certainly underestimates the rich complexity of human expertise; in reality, an expert’s mental model is likely to be partly tacit and partly explicit. The general strategy of preference-based reward learning is to operate as if the mental model were 100% tacit, and explore what can be achieved under such a strong restriction. Real-world applications would likely benefit from combining this approach with some amount of hand-coded expert knowledge.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nEJ only (action dimensionality = 4). The EJ dynamics function integrates these demands with a simplified physics engine, including gravity and air resistance (we make no claim of realism here; the simulator is merely a proof of concept). A new action is accepted every 25 steps of the physics engine, reducing an agent’s decision frequency to approximately 1Hz. RJ dynamics, as well as the conditions of state initialisation and termination, vary between tasks (see Appendix C.3).\n\nThe final generic aspect of the implementation is the feature function φ, which maps the transition space S × A × S (total dimensionality = 37 + 4 + 37 = 78) into an F -dimensional space of taskrelevant features. In consultation with engineers with experience of aerospace simulation and control algorithms, we devised the following set of F = 30 features that is sufficiently expressive to capture the important information for all three of our target tasks, without being overly specialised to one or providing too much explicit guidance to the reward learning process. Apart from those containing “delta” or “rate”, all features are computed over the successor state for each transition, st+1.\n\ndist closing speed alt alt error delta alt error dist hor delta dist hor pitch error delta pitch error abs roll roll error delta roll error hdg error delta hdg error fwd error delta fwd error up error delta up error right error delta right error los error\n\ndelta los error abs lr offset\n\nspeed g force pitch rate roll rate yaw rate thrust delta thrust\n\nEuclidean distance between EJ and RJ Closing speed between EJ and RJ (negative = moving closer) Altitude of EJ Difference in altitude between EJ and RJ (negative = EJ is lower) Change in alt error between st and st+1 Euclidean distance between EJ and RJ in horizontal plane Change in dist hor between st and st+1 (negative = moving closer) Absolute difference in pitch angle between EJ and RJ Change in pitch error between st and st+1 Absolute roll angle of EJ Absolute difference in roll angle between EJ and RJ Change in roll error between st and st+1 Absolute difference in heading angle between EJ and RJ Change in hdg error between st and st+1 Angle between 3D vectors indicating forward axes of EJ and RJ Change in fwd error between st and st+1 Angle between 3D vectors indicating upward axes of EJ and RJ Change in up error between st and st+1 Angle between 3D vectors indicating rightward axes of EJ and RJ Change in right error between st and st+1 Angle between forward axis of EJ and vector from EJ to RJ (measures whether RJ is in EJ’s line of sight) Change in los error between st and st+1 Magnitude of projection of vector from EJ to RJ onto RJ’s rightward axis (measures left-right offset between the two aircraft in RJ’s reference frame) Airspeed of EJ Instantaneous g-force experienced by EJ Absolute change of EJ pitch between st and st+1 Absolute change of EJ roll between st and st+1 Absolute change of EJ yaw between st and st+1 Instantaneous thrust output by EJ engines Absolute change in thrust between st and st+1\n\nC.3 TASKS AND ORACLES\n\nIn this paper, we consider three concrete tasks that instantiate the general EJ-RJ framework. For each, we construct a plausible oracle reward function from a subset of the 30 features, meaning that reward learning is in part a feature selection problem (tree models perform feature selection explicitly whenever they add a new splitting rule). Although the precise nature of the oracle reward functions is secondary, and those given below are among many equally reasonable alternatives, we dedicated several hours of development time to ensuring they yield reasonable behaviour upon visual inspection. The difficulty and seeming arbitrariness of this manual reward design process is precisely why reward learning (ultimately from real human preferences) is an enticing proposition. Descriptions of the three tasks, along with their respective oracles, are given below:\n\n• Follow: Here RJ follows a linear horizontal flight path at a constant velocity, which is oriented opposite to the initial velocity of EJ. The goal is to turn onto and then maintain the path up to the episode time limit of 20 timesteps. This constitutes a very simple form of\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nformation flight. The oracle reward function incentivises closing the distance to the moving target, and matching the upward axes of EJ and RJ:\n\nr = −(dist + 0.05 × closing speed + 10 × up error).\n\n• Chase: Here RJ follows an erratic trajectory generated by random control inputs, and the goal is to chase it without taking EJ below a safe altitude of 50. Episodes terminate after 20 timesteps. The oracle reward function incentivises keeping RJ at a distance of 20 and within EJ’s line of sight, while keeping EJ oriented upright. It also has a large penalty for dropping below the safe altitude:\n\nr = −(abs(dist − 20) + 10 × los error + 5 × abs roll +\n\n(cid:26) 100 0\n\nif alt < 50 otherwise\n\n).\n\n• Land: Here the goal is to execute a safe approach towards landing on a runway, where RJ represents the ideal landing position (central, zero altitude, slight upward pitch). EJ is initialised at a random altitude, pitch, roll and offset, such that landing may be challenging but always physically possible. An episode terminates if EJ passes RJ along the axis of the runway, or after 25 timesteps otherwise. The oracle reward function for this task is by far the most complex of the three, including terms that incentivise continual descent, penalise g-force and engine thrust, and punish the agent for making contact with the ground (alt < 0.6) before the start of the runway:\n\nr = −(0.05 × abs lr offset + 0.05 × alt + hdg error + abs roll +0.5 × pitch error + 0.25 × (yaw rate + roll rate + pitch rate) +0.1 × g force + 0.025 × thrust + 0.05 × delta thrust (cid:26) 1 0\n(cid:26) 1 0\n\n(cid:26) 2 0\n(cid:26) 10 if alt < 0.6\n\nif delta dist hor > 0 otherwise\n\nif abs lr offset > 10 otherwise\n\nif delta alt > 0 otherwise\n\notherwise\n\n+\n\n+\n\n+\n\n+\n\n).\n\n0\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nD IMPLEMENTATION AND EXPERIMENT DETAILS\n\nD.1 ORACLE PREFERENCES\n\nOracle preferences are generated in accordance with the Bradley-Terry model given in Equation 1, i.e. by computing the returns for the two trajectories ξi and ξj, and sampling from a Boltzmann distribution parameterised by those returns. In our main experiments, we set the temperature coefficient β = 0, which results in the oracle deterministically selecting the trajectory with higher return (ties broken uniform-randomly). In Section 6.3 we study cases with β > 0, which provide a more realistic emulation of real human preference data.\n\nD.2 HYPERPARAMETERS FOR TREE INDUCTION\n\nIn all experiments, we use the following hyperparameters during tree induction. These were identified through informal search, and we make no claim of optimality, but they do lead to reasonable performance on the three tasks of varying complexity. This indicates a general insensitivity of the method to precise hyperparameter values, which is often practically advantageous.\n\n• Trajectory return estimation using the Adam optimiser with a learning rate of 0.1. Optimisation stops when the mean lNLL changes by < 1e−5 between successive gradient steps.\n\n• Per-feature candidate split thresholds C defined as all midpoints between adjacent unique\n\nvalues in the trajectory set Ξ. These are recomputed on each update.\n\n• Tree size limit Lmax = 100.\n\n• Tree size regularisation coefficient α = 5e−3.\n\nAs mentioned in Appendix A.2, we enforce negative rewards (max = 0 constraint) for the Land task, and positive rewards (min = 0 constraint) for Follow and Chase.\n\nD.3 MODEL-BASED RL IMPLEMENTATION\n\nFor conceptual details on the PETS algorithm, we refer readers to the original paper by Chua et al. (2018). In our implementation, the dynamics model is an ensemble of five NNs, each with four hidden layers of 200 hidden units and ReLU activations. State vectors are pre-normalised by applying a hand-specifed scale factor to each dimension. Decision-time planning operates over a time horizon of H = 10 and consists of 10 iterations of the cross-entropy method. Each iteration samples 20 candidate action sequences from an independent Gaussian, of which the top 5 in terms of return are identified as elites, then updates the sampling Gaussian towards the elites with a learning rate of 0.5. In all experiments we use γ = 1, meaning no temporal discounting is applied during planning.\n\nIn our experiments, we find that the particular dynamics of the aircraft handling environment permit us to pre-train D′ on random offline data, and accurately generalise to states encountered during online reward learning. This means we perform no further updates to the model while reward learning is ongoing. As well as improving wall-clock speed, this avoids complexity and convergence issues arising from having two interacting learning processes (note that simultaneous learning is completely unavoidable with model-free RL). To pre-train, we collect 1e5 transitions by rolling out a uniform random policy, then update each of the five networks on 1e5 independently sampled mini-batches of 256 transitions, using the mean squared error loss over normalised next-state predictions.\n\nD.4 NEURAL NETWORK REWARD LEARNING BASELINE\n\nWe baseline our reward tree models against the de facto standard approach of reward learning using a NN. In constructing this baseline, we aimed to retain as much of Algorithm 1 as possible, so that only the model architecture varies. The result is that we replace lines 21-47 with the following:\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nLmini-batch ← a mini-batch of B preference labels sampled from L Compute lNLL over Lmini-batch via Equations 1 and 2 Backpropagate loss and update network parameters\n\n21: for m ∈ {1, ..., M } do 22: 23: 24: 25: end for 26: rall ← reward predictions for all feature vectors in Ξ 27: Scale network outputs by 1/std(rall) 28: Shift network outputs by −min(rall) or −max(rall), depending on desired reward sign\n\nThe new lines 26-28 replicate the two constraints applied in Equation 3.\n\nIn all experiments, we follow Lee et al. (2021b) in implementing the reward model as a three-layer network with 256 hidden units each and leaky ReLU activations, and performing the update on line 24 using the Adam optimiser (Kingma & Ba, 2014) with a learning rate of 3e−4. On each update, we sample M = 100 mini-batches of size B = 32 and take one gradient step per mini-batch.\n\nD.5 COVERAGE DATASETS FOR POLICY-INVARIANT EVALUATION\n\nGleave et al. (2021) recently highlighted the importance of comparing and evaluating learnt reward functions in a policy-invariant manner, by using a common evaluation distribution rather than onpolicy data generated by agents optimising for each reward. Ideally, the offline evaluation data should have high coverage (i.e. high-entropy state distribution, both high- and low-quality trajectories), in order to characterise the reward functions’ outputs across a spectrum of plausible policies.\n\nIn our context, we can generate data that satisfies these requirements by leveraging the known oracle reward functions and the PETS algorithm. We deploy PETS using the oracle reward, but randomise the planning parameters (number of planning iterations ∈ {1, ..., 50}, number of action sequence samples ∈ {4, ..., 50}) on every episode. In all cases, we take the top 25% of action sequences as elites. This randomisation results in trajectories that are sometimes near-optimal with respect to the oracle, sometimes moderate in quality, and sometimes barely better than random. For all three tasks, we generate a dataset of 200 evaluation trajectories in this manner.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nE VISUALING REWARD FUNCTIONS WITH SIMILARITY EMBEDDINGS\n\nHere we briefly discuss a novel form of reward visualisation which we developed and found valuable during our own analysis. Given some measure of similarity between reward functions, such as the EPIC metric proposed by Gleave et al. (2021), we can compute a matrix of pairwise similarities between any number of such functions (computational cost permitting). We can then produce a 2D embedding of the functions by applying multidimensional scaling (MDS). Visualising this embedding as a scatter plot enables the discovery of salient patterns and trends in the set of functions.\n\nIn Figure A3, we use the metric of rank correlation on the coverage datasets, and the SMACOF MDS algorithm (De Leeuw & Mair, 2009), to embed all 30 model repeats and the oracle for each task. This gives an impression of the models’ similarity not just to the oracle, but to each other. Aside from the Follow NNs, which form a tight cluster near the oracle, the distribution for each model indicates roughly equal consistency between repeats. The overlap of convex hulls suggests that the rankings produced by all models are broadly similar for Land, but more distinct for Chase. Shading points by ORR reveals that while models further from the oracle tend to induce worse-performing policies, the trend is not monotonic. This reinforces the point made elsewhere that the problems of learning a good policy and exactly replicating the ground truth reward are not identical.\n\nFigure A3: Rank correlation embeddings for all model repeats from the main experiments, with the scatter point for each repeat shaded by ORR.\n\nPopulating such embedding plots more densely, perhaps by varying model hyperparameters, could provide a means of mapping the space of learnable reward functions and its relationship to policy It would also be straightforward to compute similarity values for the same model performance. repeat at multiple checkpoints during learning. This would yield a trajectory in the rank embedding space, which could aid the assessment of the stability and convergence properties of online learning with different models and hyperparameter values.\n\n23\n\nOracleORR10FollowChaseLandNNTree (0-1)Tree (var)",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors work on preference-based RL setups, where they develop reward trees from the preference labels. They use four stages for the learning, first estimating the returns for trajectories, then, predicting the reward at the leaf level, and finally, they apply tree growth and tree pruning operations for the reward trees. Even though they observe performance hit when switching from neural network-estimated rewards to tree-based rewards, the authors argue that the tree-based methods can be competitive. The advantages of the tree-based method are shown in Figure 6 by easy interpretation of the reward meanings and semantic understanding.\n\n# Strength And Weaknesses\n\nStrength\n+ A complete pipeline for tree-based reward learning from preference labels where the authors show effective learning and competitive performance against commonly used neural network approaches;\n+ Detailed description of the four-stage pipeline;\n+ Competitive performance is shown in evaluation against NN approaches;\n+ Example reward trees are shown to understand the advantages of the tree-based reward methods;\n\nWeaknesses\n- The method is tested upon a not commonly used domain, i.e., aircraft handling environment. It's hard to understand the properties of the domain and the difficulty of learning just from the text description. The authors could use additional relatively standard benchmarks for their experiments;\n- The generality of the proposed method could be shown by using more diverse domains for the experiments.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper has decent clarity and quality.  The novelty seems a bit limited but it's a complete pipeline from preference labels to agent training. The reproducibility should be good since they provided source code.\n\n# Summary Of The Review\n\nI'm currently leaning towards rejection of the paper since it is showing a certain level of ethical concerns. The domains tested in the paper could be used for wars, weapons, etc, which I don't think it's the only application. The authors could demonstrate the effectiveness of their method in a less sensitive domain.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nThe experiments are conducted with an aircraft-controlling domain, where the agent maneuvers an aircraft to follow/chase/land according to a reference aircraft. Though it's not explicitly stated in the paper, it's quite likely the use case is for wars/weapons/protection of homeland/etc. For evaluating the technical aspects of the proposed method, I don't think it really must use such a domain which could cause huge ethics concerns."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nBRINGING ROBOTICS TAXONOMIES TO CONTINUOUS DOMAINS VIA GPLVM ON HYPERBOLIC MANIFOLDS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRobotic taxonomies have appeared as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite the efforts devoted to design their hierarchy and underlying categories, their use in application fields remains scarce. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. To do so, we formulate a Gaussian process hyperbolic latent variable model and enforce the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We test our model on the whole-body support pose taxonomy to learn hyperbolic embeddings that comply with the original graph structure. We show that our model properly encodes unseen poses from existing or new taxonomy categories, it can be used to generate trajectories between the embeddings, and it outperforms its Euclidean counterparts.\n\n1\n\nINTRODUCTION\n\nRoboticists are often inspired by biological insights to create robotic systems that exhibit human- or animal-like capabilities (Siciliano & Khatib, 2016). In particular, it is first necessary to understand how humans move and interact with their environment to then generate biologically-inspired motions and behaviors of robotics hands, arms or humanoids. In this endeavor, researchers proposed to structure and categorize human hand postures and body poses into hierarchical classifications known as taxonomies. Their structure depends on the variables considered to categorize human motions and their interactions with the environment, as well as on associated qualitative measures.\n\nDifferent taxonomies have been proposed in the area of human and robot grasping (Cutkosky, 1989; Feix et al., 2016; Abbasi et al., 2016; Stival et al., 2019). Feix et al. (2016) introduced a taxonomy of hand grasps whose structure was mainly defined by the hand pose and the type of contact with the object. Later, Stival et al. (2019) claimed that the taxonomy designed in (Feix et al., 2016) heavily depended on subjective qualitative measures, and proposed a quantitative tree-like taxonomy of hand grasps based on muscular and kinematic patterns. A similar data-driven approach was used to design a grasp taxonomy based on sensed contact forces in (Abbasi et al., 2016). Robotic manipulation also gave rise to various taxonomies. Bullock et al. (2013) introduced a hand-centric manipulation taxonomy that classifies manipulation skills according to the type of contact with the objects and the object motion imparted by the hand. A different strategy was developed in (Paulius et al., 2019), where a manipulation taxonomy was designed based on a categorization of contacts and motion trajectories. Humanoid robotics also made significant efforts to analyze human motions, thus proposing taxonomies as high-level abstractions of human motion configurations. Borr`as et al. (2017) analyzed the contacts of the human limbs with the environment and designed a taxonomy of whole-body support poses.\n\nIn addition to being used for analysis purposes in robotics or biomechanics, some of the aforementioned taxonomies were leveraged for modeling grasp actions (Romero et al., 2010; Lin & Sun, 2015), for planning contact-aware whole-body pose sequences (Mandery et al., 2016), and for learning manipulation skills embeddings (Paulius et al., 2020). However, despite most taxonomies carry\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Left: Illustration of the Lorentz L2 and Poincar ́e P 2 models of the hyperbolic manifold. The former is depicted as the gray hyperboloid, while the latter is represented by the blue circle. Both models show a ) lies on the tangent space of x1 such geodesic ( (x2). Right: Subset of the whole-body support pose taxonomy (Borr`as et al., 2017) used in that u = Logx1 our experiments. Each node is a support pose defined by the type of contacts (foot F, hand H, knee K). The lines represent graph transitions between the taxonomy nodes. Contacts are depicted by grey dots.\n\n) between two points x1 ( ) and x2 ( ). The vector u (\n\na well-defined hierarchical structure, it is often overlooked. First, these taxonomies are usually exploited for classification tasks whose target classes are mainly the tree leaves, disregarding the full taxonomy structure (Feix et al., 2016; Abbasi et al., 2016). Second, the discrete representation of the taxonomy categories hinders their use for motion generation (Romero et al., 2010).\n\nWe believe that the difficulty of leveraging robotic taxonomies is due to the lack of computational models that exploit (i) the domain knowledge encoded in the hierarchy, and (ii) the information of the high-dimensional data associated to the taxonomy categories. We tackle this problem from a representation learning perspective by modeling taxonomy data as embeddings that capture the associated hierarchical structure. Inspired by recent advances on word embeddings (Nickel & Kiela, 2017; 2018; Mathieu et al., 2019), we propose to leverage the hyperbolic manifold (Ratcliffe, 2019) to learn such embeddings. An important property of the hyperbolic manifold is that distances grow exponentially when moving away from the origin, and shortest paths between distant points tend to pass through it, resembling a continuous hierarchical structure. Therefore, we hypothesize that the geometry of the hyperbolic manifold allows us to learn embeddings that comply with the original graph structure of robotic taxonomies.\n\nSpecifically, we propose a Gaussian process hyperbolic latent variable model (GPHLVM) to learn embeddings of taxonomy data on the hyperbolic manifold. To do so, we impose a hyperbolic geometry to the latent space of the well-known GPLVM (Lawrence, 2003; Titsias & Lawrence, 2010). This demands to reformulate the Gaussian distribution, the kernel, and the optimization process of the vanilla GPLVM to account for the geometry of the hyperbolic latent space. To do so, we leverage the hyperbolic wrapped Gaussian distribution (Nagano et al., 2019), and provide a positive-definiteguaranteed approximation of the hyperbolic kernel proposed by McKean (1970). Moreover, we resort to Riemannian optimization (Absil et al., 2007; Boumal, 2022) to optimize the GPHLVM parameters. We enforce the taxonomy graph structure in the learned embeddings through graphbased priors on the latent space and via graph-distance-preserving back constraints (Lawrence & Qui ̃nonero Candela, 2006; Urtasun et al., 2008). Our GPHLVM is conceptually similar to the GPLVM for Lie groups introduced in (Jensen et al., 2020), which also imposes geometric properties to the GPLVM latent space. However, our formulation is specifically designed for the hyperbolic manifold and fully built on tools from Riemannian geometry. Moreover, unlike (Tosi et al., 2014) and (Jørgensen & Hauberg, 2021), where the latent space was endowed with a pullback Riemannian metric learned via the GPLVM mapping, we impose the hyperbolic geometry to the GPHLVM latent space as an inductive bias adapted to our targeted applications.\n\nWe test our approach on graphs extracted from the whole-body support pose taxonomy (Borr`as et al., 2017). The proposed GPHLVM learns hyperbolic embeddings of the body support poses that comply with the original graph structure, and properly encodes unseen poses from existing or new taxonomy nodes. Moreover, we show how we can exploit the continuous geometry of the hyperbolic manifold to generate trajectories between different embeddings pairs, which comply with the taxonomy graph structure. To the best of our knowledge, this paper is the first to leverage the hyperbolic manifold for robotic applications.\n\n2\n\nFF2F2HFH2FHF2H2KFKKHK2FKHKH2K2HK2H2FKH2SingleDoubleTripleQuadrupleStandingKneelingUnder review as a conference paper at ICLR 2023\n\n2 BACKGROUND\n\nGaussian Process Latent Variable Models: A GPLVM defines a generative mapping from latent n=1, yn ∈ RD by modeling the correspondvariables {xn}N ing non-linear transformation with Gaussian processes (GPs) (Lawrence, 2003). The GPLVM is described as\n\nn=1, xn ∈ RQ to observations {yn}N\n\nd) with\n\nyn,d ∼ N (yn,d; fn,d, σ2\n\nfn,d ∼ GP(md(xn), kd(xn, xn))\n\nand xn ∼ N (0, I), (1) where yn,d denotes the d-th dimension of the observation yn, md(·) : RQ (cid:55)→ R and kd(·, ·) : RQ × RQ → R are the GP mean and kernel function, respectively, and σ2 d is a hyperparameter. Classically, the hyperparameters and latent variables of the GPLVM were optimized using maximum likelihood or maximum a posteriori (MAP) estimates. As this does not scale gracefully to large datasets, contemporary methods use inducing points and variational approximations of the evidence (Titsias & Lawrence, 2010). Compared to neural-network-based generative models, GPLVMs are data efficient and provide automatic uncertainty quantification.\n\nRiemannian geometry: To understand the hyperbolic manifold, it is necessary to first define some basic Riemannian geometry concepts (Lee, 2018). To begin with, consider a Riemannian manifold M, which is a locally Euclidean topological space with a globally-defined differential structure. For each point x ∈ M, there exists a tangent space TxM that is a vector space consisting of the tangent vectors of all the possible smooth curves passing through x. A Riemannian manifold is equipped with a Riemannian metric, which permits to define curve lengths in M. Shortest-path curves, called geodesics, can be seen as the generalization of straight lines on the Euclidean space to Riemannian manifolds, as they are minimum-length curves between two points in M. To operate with Riemannian manifolds, it is common practice to exploit the Euclidean tangent spaces. To do so, we resort to mappings back and forth between TxM and M, which are the exponential and logarithmic maps. The exponential map Expx(u) : TxM → M maps a point u in the tangent space of x to a point y on the manifold, so that it lies on the geodesic starting at x in the direction u, and such that the geodesic distance dM between x and y equals the distance between x and u. The inverse operation is the logarithmic map Logx(u) : M → TxM. Finally, the parallel transport (cid:0)u(cid:1) : TxM → TyM operates with manifold elements lying on different tangent spaces. Px→y\n\nHyperbolic manifold: The hyperbolic space Hd is the unique simply-connected complete ddimensional Riemannian manifold with a constant negative sectional curvature (Ratcliffe, 2019). There are several isometric models for the hyperbolic space, in particular, the Poincar ́e ball P d and the Lorentz (hyperboloid) model Ld (see Fig. 1-left). The latter representation is chosen here as it is numerically more stable than the former, and thus better suited for Riemannian optimization. However, the Poincar ́e model provides a more intuitive representation and is here used for visualization. This is easily achieved by leveraging the isometric mapping between both models (see App. A for details). An important property of the hyperbolic manifold is the exponential rate of the volume growth of a ball with respect to its radius. In other words, distances in Hd grow exponentially when moving away from the origin, and shortest paths between distant points on the manifold tend to pass through the origin, resembling a continuous hierarchical structure. Because of this, the hyperbolic manifold is often exploited to embed hierarchical data such as trees or graphs (Nickel & Kiela, 2017; Chami et al., 2020). Although its potential to embed discrete data structures into a continuous space is well known in the machine learning community, its application in robotics is presently scarce.\n\nHyperbolic wrapped Gaussian distribution: Probabilistic models on Riemannian manifolds demand to have probability distributions that consider the manifold geometry. We use the hyperbolic wrapped distribution (Nagano et al., 2019), which builds on a Gaussian distribution on the tangent space at the origin μ0 = (1, 0, . . . , 0)T of Hd, that is then projected onto the hyperbolic space after transporting the tangent space to the desired location. Intuitively, the construction of this wrapped distribution is as follows: (1) sample a point ̃v ∈ Rd from the Euclidean normal distribuHd ⊂ Rd+1 by setting v = (0, ̃v)T, (3) apply the tion N (0, Σ), (2) transform ̃v to an element of Tμ0 (cid:0)v(cid:1), and (4) project u to Hd via Expμ(u). The resulting probability parallel transport u = Pμ0→μ density function is\n\nlog NHd (x; μ, Σ) = log N (v; 0, Σ) − (d − 1) log (sinh(∥u∥L)/∥u∥L) ,\n\n(2) (cid:0)u(cid:1), u = Logμ(x), and ∥u∥L = (cid:112)⟨u, u⟩μ. The hyperbolic wrapped distribu-\n\nwhere v = Pμ→μ0 tion (Nagano et al., 2019) has a more general expression given in (Skopek et al., 2020).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3 GAUSSIAN PROCESS HYPERBOLIC LATENT VARIABLE MODEL\n\nWe present the proposed GPHLVM, which extends GPLVM to hyperbolic latent spaces. A GPHLVM defines a generative mapping from the hyperbolic latent space HQ to the observation space, e.g. the data associated to the taxonomy, based on GPs. By considering independent GPs across the observation dimensions, the GPHLVM is formally described as\n\nyn,d ∼ N (yn,d; fn,d, σ2\n\nd) with fn,d ∼ GP(md(xn), kHQ\n\nd (xn, xn)) and xn ∼ NHQ(μ0, αI), (3) where yn,d denotes the d-th dimension of the observation yn ∈ RD and xn ∈ HQ is the corresponding latent variable. Our GPHLVM is built on hyperbolic GPs, characterized by a mean function md(·) : HQ → R (usually set to 0), and a kernel kHQ d (·, ·) : HQ × HQ → R. These kernels encode similarity information in the latent hyperbolic manifold and should reflect its geometry to perform effectively, as detailed in §. 3.1. Also, the latent variable x ∈ HQ is assigned a hyperbolic wrapped Gaussian prior NHQ(μ0, αI) based on Eq. 2, where μ0 is the origin of HQ, and the parameter α controls the spread of the latent variables in HQ. As Euclidean GPLVMs, our GPHLVM can be trained by finding a MAP estimate or via variational inference. However, special care must be taken to guarantee that the latent variables belong to the hyperbolic manifold, as explained in §. 3.2.\n\n3.1 HYPERBOLIC KERNELS\n\nFor GPs in Euclidean spaces, the squared exponential (SE) and Mat ́ern kernels are standard choices (Rasmussen & Williams, 2006). In the modern machine learning literature these were generalized to non-Euclidean spaces such as manifolds (Borovitskiy et al., 2020; Jaquier et al., 2021) or graphs (Borovitskiy et al., 2021). The generalized SE kernels may be connected to the much studied heat kernels. These are given (cf. Grigoryan & Noguchi (1998)) by\n\nkH2\n\n(x, x′) =\n\nσ2 C∞\n\n(cid:90) ∞\n\nρ\n\nse−s2/(2κ2) (cosh(s) − cosh(ρ))1/2\n\nds,\n\nkH3\n\n(x, x′) =\n\nσ2 C∞\n\nρ sinh ρ\n\ne−ρ2/(2κ2),\n\n(4)\n\nwhere ρ = distHd (x, x′) denotes the geodesic distance between x, x′ ∈ Hd, κ and σ2 are the kernel lengthscale and variance, and C∞ is a normalizing constant. To the best of our knowledge, no closed form expression for H2 is known. To approximate the kernel in this case, a discretization of the integral is performed. One appealing option is the Monte Carlo approximation based on the truncated Gaussian density. Unfortunately, such approximations easily fail to be positive semidefinite if the number of samples is not very large. We address this via an alternative Monte Carlo approximation\n\nkH2\n\n(x, x′) ≈\n\nσ2 C ′\n\n∞\n\n1 L\n\nL (cid:88)\n\nl=1\n\nsl tanh(πsl)e(2sli+1)⟨xP ,bl⟩e(2sli+1)⟨x′\n\nP ,bl⟩,\n\n(5)\n\n2 log 1−|xP |2\n\ni.i.d.∼ U (T) with T the unit circle, and sl\n\nwhere ⟨xP , b⟩ = 1 |xP −b|2 is the hyperbolic outer product with xP being the representation of x as a point on the Poincar ́e disk P 2 = D, i, z denote the imaginary unit and complex conjugation, i.i.d.∼ e−s2κ2/21[0,∞)(s). The distributions of respectively, bl bl and sl are easy to sample from: The former is sampled by applying x → e2πix to x ∼ U ([0, 1]) and the latter is (proportional to) a truncated normal distribution. Importantly, the right-hand side of Eq. 5 is easily recognized to be an inner product in the space CL, which immediately implies its positive semidefiniteness (see App. B for the development of Eq. 5). Note that hyperbolic kernels for HQ with Q > 3 are generally defined as integrals of the kernels Eq. 4 (Grigoryan & Noguchi, 1998). Analogs of Mat ́ern kernels for HQ are obtained as integral of the SE kernel of the same dimension (Jaquier et al., 2021).\n\n3.2 MODEL TRAINING\n\nn=1 and hyperparameters Θ = {θd}D\n\nSimilarly to the Euclidean case, training the GPHLVM is equivalent to finding an optimal set of d=1, with xn ∈ HQ and θd the hylatent variables {xn}N perparameters of the d-th GP. For small datasets, the GPHLVM can be trained by maximizing the log posterior of the model, i.e., LMAP = log (cid:0)p(Y |X)p(X)(cid:1) with Y = (y1 . . . yN )T and X = (x1 . . . xN )T. For large datasets, the GPHLVM can be trained, similarly to the so-called Bayesian GPLVM (Titsias & Lawrence, 2010), by maximizing the marginal likelihood of the data,\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\ni.e., LMaL = log p(Y ) = log (cid:82) p(Y |X)p(X)dX. As this quantity is intractable, it is approximated via variational inference by adapting the methodology introduced in (Titsias & Lawrence, 2010) to hyperbolic latent spaces, as explained next.\n\nVariational inference: We approximate the posterior p(X|Y ) by a variational distribution q(X) defined as a hyperbolic wrapped normal distribution over the latent variables, i.e.,\n\nqφ(X) =\n\nN (cid:89)\n\nn=1\n\nNHQ(xn; μn, Σn),\n\n(6)\n\nHQ. Similarly to the with variational parameters φ = {μn, Σn}N Euclidean case (Titsias & Lawrence, 2010), this variational distribution allows the formulation of a lower bound\n\nn=1, with μn ∈ HQ and Σn ∈ Tμn\n\nlog p(Y ) ≥ Eqφ(X) [log p(Y |X)] − KL(cid:0)qφ(X)||p(X)(cid:1). (7) The KL divergence KL(cid:0)qφ(X)||p(X)(cid:1) between two hyperbolic wrapped normal distributions can easily be evaluated via Monte-Carlo sampling (see App. C.1 for details). Moreover, the expectation Eqφ(X) [log p(Y |X)] can be decomposed into individual terms for each observation dimension as (cid:80)D Eqφ(X) [log p(yd|X)], where yd is the d-th column of Y . For large datasets, each term can be evaluated via a variational sparse GP approximation (Titsias, 2009; Hensman et al., 2015). To do m=1 with zd,m ∈ HQ for each observation dimension so, we introduce M inducing inputs {zd,m}M d, whose corresponding inducing variables {ud,m}M m=1 are defined as noiseless observations of the GP in Eq. 3, i.e, ud ∼ GP(md(zd), kHQ d (zd, zd)). Similar to (Hensman et al., 2015), we can write\n\nd=1\n\n(cid:2)log N (yd; fd(X), σ2\n\nd)(cid:3) − KL(cid:0)qλ(ud)||p(ud|Zd)(cid:1),\n\nlog p(yd|X) ≥ Eqλ(fd)\n\n(8) where we defined qλ(fd) = (cid:82) p(fd|ud)qλ(ud)dud with the variational distribution qλ(ud) = N (ud; ̃μd, ̃Σd) and variational parameters λ = { ̃μd, ̃Σd}D d=1. Remember that the inducing variables ud,m are Euclidean, i.e., the variational distribution qλ(ud) is a Euclidean Gaussian and the KL divergence in Eq. 8 has a closed-form solution. In this case, the training parameters of the GPHLVM are the set of inducing inputs {zd,m}M m=1, the variational parameters φ and λ, and the hyperparameters Θ (see App. C.2 for the full derivation of the GPHLVM variational inference process).\n\nOptimization: As several training parameters of the GPHLVM belong to HQ, i.e., the latent variables xn for the MAP estimation, or the inducing inputs zd,m and means μn for variational inference. To account for the hyperbolic geometry of these parameters, we leverage Riemannian optimization methods (Absil et al., 2007; Boumal, 2022) to train the GPHLVM. Each step of first order (stochastic) Riemannian optimization methods is generally of the form\n\nηt ← h(cid:0)grad L(xt), τt−1\n\n(cid:1),\n\nxt+1 ← Expxt\n\n(−αtηt),\n\nτt ← Pxt→xt+1\n\n(cid:0)ηt\n\n(cid:1).\n\n(9)\n\nThe update ηt ∈ TxtM is first computed as a function h of the Riemannian gradient grad of the loss L(xt) and of τt−1, the previous update parallel-transported to the tangent space of the new estimate xt. The estimate xt is then updated by projecting the update ηt scaled by a learning rate αt onto the manifold using the exponential map. The function h is equivalent to computing the update of the Euclidean algorithm, e.g., ηt ← grad L(xt) for a simple gradient descent. Notice that Eq. 9 is applied on a product of manifolds when optimizing several parameters. In this paper, we used the Riemannian Adam (B ́ecigneul & Ganea, 2019) implemented in Geoopt (Kochurov et al., 2020) to optimize the GPHLVM parameters.\n\n4\n\nINCORPORATING TAXONOMY KNOWLEDGE INTO GPHLVM\n\nWhile we are now able to learn hyperbolic embeddings of the data associated to a taxonomy using our GPHLVM, they do not necessarily follow the graph structure of the taxonomy. In other words, the manifold distances between pairs of embeddings do not need to match the graph distances. To overcome this, we introduce graph-distance information as inductive bias to learn the embeddings. To do so, we leverage two well-known techniques in the GPLVM literature: priors on the embeddings and back constraints (Lawrence & Qui ̃nonero Candela, 2006; Urtasun et al., 2008). Both are reformulated to preserve the taxonomy graph structure in the hyperbolic latent space as a function of the node-to-node shortest paths.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nGraph-distance priors: As shown by Urtasun et al. (2008), the structure of the latent space can be modified by adding priors of the form p(X) ∝ e−φ(X)/σ2 φ to the GPLVM, where φ(X) is a function that we aim at minimizing. Incorporating such a prior may also be understood as augmenting the GPLVM loss L with a regularization term −φ(X). Therefore, we propose to augment the loss of the GPHLVM with a distance-preserving graph-based regularizer. Several such losses have been proposed in the literature, see (Cruceru et al., 2021) for a review. Specifically, we define φ(X) as the stress loss\n\nLstress(X) =\n\n(cid:88)\n\n(cid:0) distG(ci, cj) − distHQ (xi, xj)(cid:1)2\n\n,\n\n(10)\n\ni<j\n\nwhere ci denotes the taxonomy node to which the observation yi belongs, and distG, distHQ are the taxonomy graph distance and the geodesic distance on HQ, respectively. The loss Eq. 10 encourages the preservation of all distances of the taxonomy graph in HQ. It therefore acts globally, thus allowing the complete taxonomy structure to be reflected by the GPHLVM. Notice that Cruceru et al. (2021) also survey a distortion loss that encourages the distance of the embeddings to match the graph distance by considering their ratio. We notice, however, that this distortion loss is only properly defined when the embeddings xi and xj correspond to different classes ci ̸= cj. Interestingly, our empirical results using this loss were lackluster and numerically unstable (see App. E).\n\nBack-constraints: The back-constrained GPLVM (Lawrence & Qui ̃nonero Candela, 2006) defines the latent variables as a function of the observations, i.e., xn,q = gq(y1 . . . , yn, wq) with parameters {wq}Q q=1. It allows us to incorporate new observations in the latent space after training, while preserving local similarities between observations in the embeddings. To incorporate graph-distance information into the GPHLVM and ensure that latent variables lie on the hyperbolic manifold, we propose the back-constraints mapping\n\nxn = Expμ0\n\n( ̃xn) with\n\n ̃xn,q =\n\nN (cid:88)\n\nm=1\n\nwq,mkRD\n\n(yn, ym)kG(cn, cm).\n\n(11)\n\nThe mapping Eq. 11 not only expresses the similarities between data in the observation space via the kernel kRJ , but encodes the relationships between data belonging to nearby taxonomy nodes via kG. In other words, similar observations associated to the same (or near) taxonomy nodes will be close to each other in the resulting latent space. The kernel kG is a Mat ́ern kernel on the taxonomy graph following the formulation introduced in (Borovitskiy et al., 2021), which accounts for the graph geometry (see also App. D). We also use a Euclidean SE kernel for kRD . Notice that the back constraints only incorporate local information into the latent embedding. Therefore, to preserve the global graph structure, we pair the proposed back-constrained GPHLVM with the stress prior Eq. 10. Note that both kernels are required in Eq. 11: By defining the mapping as a function of the graph kernel only, the observations of each taxonomy node would be encoded by a single latent point. When using the observation kernel only, dissimilar observations of the same taxonomy node would be distant in the latent space, despite the additional stress prior, as kRD\n\n(yn, ym) ≈ 0.\n\n5 EXPERIMENTS\n\nWe test the proposed GPHLVM to model data of the whole-body support pose taxonomy (Borr`as et al., 2017). Each node of the taxonomy graph (see Fig. 1-right) is a support pose defined by its contacts, so that the distance between nodes can be viewed as the number of contact changes required to go from a support pose to another. We use standing and kneeling poses of the datasets in (Mandery et al., 2016) and (Langenstein, 2020). The former were extracted from recordings of a human walking without hand support, or using supports from a handrail or from a table on one side or on both sides. The latter were obtained from a human standing up from a kneeling position. Each pose is identified with a node of the graph of Fig. 1-right. We test our approach on three different datasets: an unbalanced dataset (i.e., 100 poses composed of 72 standing and 28 kneeling poses); a balanced dataset (i.e., only 60 standing poses); and an joint-space dataset (i.e., same 60 standing poses represented as joint configurations). For the first two datasets each pose is represented as a vector yn = [yLF, yRF, yLH, yRH]T ∈ R12 corresponding to the positions of the human’s feet and hands. Instead, for the last dataset, each pose is represented by vector of joint angles yn ∈ R44. Last but not least, we also test our approach on an augmented version of the whole-body support pose taxonomy, which explicitly distinguishes between left and right contacts. The main results are analyzed in the sequel, while additional experimental details and results are given in App. F and G.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Vanilla\n\n(b) Stress prior\n\n(c) BC + stress prior\n\n(d) Adding poses\n\n(e) Adding a class\n\nFigure 2: The first and last two rows respectively show the latent embeddings and examples of interpolating geodesics in P 2 and R2, followed by pairwise distance matrices. Embeddings colors match those of Fig. 1right, and background colors indicate the GPLVM uncertainty. Added poses (d) and classes (e) are marked with stars and highlighted with red in the distance matrices.\n\nR2\n\nH2\n\nStress ±σ\n\nRegularization\n\n3.71±4.08 0.14±0.20 0.21±0.34 0.22±0.34 0.58±0.65\n\n2.15±2.92 0.86±2.18 1.70±2.91 0.53 ± 0.86 0.85±1.0\n\nNo reg. Stress BC+Stress — ” —: unseen poses — ” —: unseen class No reg. Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nHyperbolic embeddings of support poses: We embed the 100 standing and kneeling poses into 2-dimensional hyperbolic and Euclidean spaces using GPHLVM and GPLVM. For each, we test the model without regularization, with stress prior, and with back-constraints coupled with stress prior (see App. F.2 for the training parameters). Figs. 2a-2c show the learned embeddings alongside distance matrices, which are to be compared with the graph distances in Fig. 3. As shown in Fig. 2a, the models without regularization do not encode any meaningful distance structure in latent space. In contrast, the models with stress prior result in embeddings that comply with the taxonomy graph structure: The embeddings are grouped and organized according to the taxonomy nodes, the geodesic distances match the graph ones, and arguably more so in the hyperbolic case (see Figs. 2b-2c). This is further reflected in the stress values of the latent embeddings with respect to the graph distances (see Table 1). Interestingly, the hyperbolic models also outperform Euclidean models with 3-dimensional latent spaces (see App. G.1). This is due to the fact that the geometry of the hyperbolic manifold leads to exponentially-increasing distances w.r.t the origin, which provides an increased volume to match the graph structure when compared to Euclidean spaces, thus resulting in better low-dimensional representations of taxonomy data. Our GPHLVM also outperformed vanilla and hyperbolic versions of variational autoencoders (VAE) to encode meaningful taxonomy information in the latent space (see App. G.4). In general, the tested VAEs only captured a global structure that separates standing from kneeling poses. Moreover, the average stress of the VAEs’ latent embeddings is higher compared to the GPHLVM’s. Finally, notice that the back constraints further organize the embeddings inside a class according to the similarity between their observations (Fig. 2c). Taxonomy expansion and unseen poses encoding: An advantage of back-constrained GPLVMs is their affordance to “embed” new observations into the latent space. We test the GPHLVM ability\n\nFigure 3: Graph distance between the poses following Fig. 1-right.\n\nTable 1: Average stress per geometry and regularization.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Motions obtained via geodesic interpolation in the back-constrained GPHLVM latent space. Left: F to F2. Right: F to FK. The colorbars identify the support pose of the closest pose in the latent space.\n\nto place unseen poses or taxonomy classes into the latent space, hypothesizing that their respective embeddings would be placed at meaningful distances w.r.t. the rest of the latent points. First, we consider a back-constrained GPHLVM with stress prior previously trained on example poses from the taxonomy (i.e., the model of Fig. 2c) and embedded unseen poses. Fig. 2d shows how these new poses land close to their respective class cluster. Second, we train a new GPHLVM while withholding all poses corresponding to the F1H1 class. We then encode these poses and find that they are located at sensible distance when compared to the model trained on the full dataset. Although this is accomplished by both models, the GPHLVM displays lower stress values (see Table 1).\n\nTrajectory generation via geodesics: The geometry of the GPHLVM latent space can also be exploited to generate trajectories in the latent space by following the geodesic, i.e. the shortest path, between two embeddings. In other words, our GPHLVM intrinsically provides a mechanism to plan motions via geodesics in the low-dimensional latent space. Examples of geodesics between two poses are shown in Figs. 2b-2c, with the colors along the trajectory matching the class corresponding to the closest hyperbolic latent point. Importantly, the geodesics in our GPHLVM latent space follow the transitions between classes defined in the taxonomy. In other words, the shortest paths in the hyperbolic embedding correspond to the shortest paths in the taxonomy graph. For instance, the geodesic from F to F2H2 follows F → F2 → F2H → F2H2, while the geodesic from FH to K2H follows FH → F2H → FKH → KH → K2H. In contrast, straight lines in the Euclidean embeddings often do not match the graph shortest path, resulting in transitions that do not exist in the taxonomy, e.g., F → F2H2, or F2 → FKH in the Euclidean latent space of Figs. 2b-2c (see also App. F.4).\n\nFig. 4 shows examples of motions resulting from geodesic interpolation in the GPHLVM latent space. As expected, the resulting trajectories do not correspond to direct interpolations between the given initial and final poses. This is due to the lack of information about the objects location and the type of contact in the considered poses. Therefore, poses with very different feet and hands positions may belong to the same class, e.g., two-feet contact with a left hand contact on the handrail or a right hand contact on the table both belong to F2H. This results in artifacts throughout the interpolations, which are alleviated by augmenting the taxonomy to differentiate between left and right contacts, as described next. However, it is interesting that the motions are still consistent with the observed transitions, e.g., the hand positions vary little along a path involving only foot and knee contacts.\n\nAugmented taxonomy for enhanced trajectory generation: Here, we aim at improving the quality of the generated motion by augmenting the whole-body support pose taxonomy with additional contact information. To do so, we consider an augmented whole-body support pose taxonomy which explicitly distinguishes between left and right contacts by adapting the nodes and transitions of Fig. 1-right. For instance, the 1-foot contact (F) node is separated into left-foot (Fl) and right-foot (Fr) contact nodes. To facilitate motion planning and to test the GPHLVM ability of dealing with high-dimensional spaces, we represent each pose as a vector yn ∈ R44 of joint angles instead of a vector of hands and feet positions. A video of the resulting motions accompanies this paper.\n\nWe embed the 60 standing poses described in App. G.2 into 3-dimensional hyperbolic and Euclidean spaces using GPHLVM and GPLVM, respectively. For each approach, we test the model without regularization, with stress prior, and with back-constraints coupled with stress prior (see App. G.3 and F.2 for detailed results and training parameters). Fig. 5 shows examples of motions planned by following geodesics in the GPHLVM latent space. We observe that the motions generated by considering the augmented taxonomy result in more realistic interpolations between the given initial and final poses than the trajectories of Fig. 4. Moreover, the previously-observed artifacts are drastically reduced. This is due to the fact that the augmented taxonomy differentiates between left and right contacts, thus allowing very different poses to be placed far apart in the latent space. For example, poses corresponding to FlHr and FrHl in the augmented taxonomy belonged to the same FH node in the original taxonomy, and were embedded close together. It is also interesting to notice that considering joint angles instead of end-effector positions results in more realistic poses. Such poses may also be obtained by considering both end-effector positions and orientations as observations, which would require an extension of the GPHLVM to handle observations on Riemannian manifolds.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Fl to F2Hr\n\n(b) Fl to F2H2\n\n(c) Fr to FrH2\n\n(d) F2Hl to FlH2\n\nFigure 5: Motions obtained via geodesic interpolation in the latent space of the back-constrained GPHLVM trained on the augmented taxonomy (Fig. 10c). Contacts are denoted by gray circles. The colorbars identify the support pose of the closest pose in the latent space.\n\n6 CONCLUSIONS\n\nInspired by the recent developments of taxonomies in different robotics fields, we proposed a computational model GPHLVM that leveraged two types of domain knowledge: the structure of a humandesigned taxonomy and a hyperbolic geometry on the latent space which complies with the intrinsic taxonomy’s hierarchical structure. Our GPHLVM allows us to learn hyperbolic embeddings of the features of the taxonomy nodes while capturing the associated hierarchical structure. To achieve this, our model exploited the curvature of the hyperbolic manifold and the graph-distance information, as inductive bias. We showed that these two forms of inductive bias are essential to: learn taxonomyaware embeddings, encode unseen data, and potentially expand the learned taxonomy. Moreover, we reported that vanilla Euclidean approaches underperformed on all the foregoing cases. Finally, we introduced a mechanism to generate taxonomy-aware motions in the hyperbolic latent space.\n\nIt is important to emphasize that our geodesic motion generation does not use explicit knowledge on how physically feasible the generated trajectories are. We plan to investigate how to include physics constraints or explicit contact data into the GPHLVM to obtain physically-feasible motions that can be executed on real robots. Moreover, we will work on alleviating the computational cost of the hyperbolic kernel in Hd. This could be tackled by using a different sampling strategy: Instead of sampling from a Gaussian distribution for the approximation Eq. 5, we could sample from the Rayleigh distribution. This is because complex numbers, whose real and imaginary components are i.i.d. Gaussian, have absolute value that is Rayleigh-distributed. As our current experimental study focused on testing our model on different graphs extracted from the whole-body support pose taxonomy (Borr`as et al., 2017), we plan to test it with datasets used to design other robotic taxonomies. Finally, we plan to investigate other types of manifold geometries that may accommodate more complex structures coming from highly-heterogeneous graphs (Giovanni et al., 2022).\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES Bahareh Abbasi, Ehsan Noohi, Sina Parastegari, and Miloˇs ˇZefran. Grasp taxonomy based on force distribution. In IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), pp. 1098–1103, 2016. doi: 10.1109/ROMAN.2016.7745245.\n\nMilton Abramowitz and Irene A Stegun. Handbook of mathematical functions with formulas, doi:\n\ngraphs, and mathematical tables, volume 55. US Government printing office, 1964. 10.5555/1098650.\n\nPierre-Antoine Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University Press, 2007. URL https://press.princeton.edu/ absil.\n\nViacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc P. Deisenroth. Mat ́ern In Neural Information Processing SysGaussian processes on Riemannian manifolds. tems (NeurIPS), pp. 12426–12437, 2020. URL https://proceedings.neurips.cc/ paper/2020/file/92bf5e6240737e0326ea59846a83e076-Paper.pdf.\n\nViacheslav Borovitskiy, Iskander Azangulov, Alexander Terenin, Peter Mostowsky, Marc Deisenroth, and Nicolas Durrande. Mat ́ern Gaussian processes on graphs. In Intl. Conf. on Artificial Intelligence and Statistic (AISTATS), pp. 2593–2601, 2021. URL https://proceedings. mlr.press/v130/borovitskiy21a.html.\n\nJ ́ulia Borr`as, Christian Mandery, and Tamim Asfour. A whole-body support pose taxonomy for multi-contact humanoid robot motions. Science Robotics, 2(13), 2017. doi: 10.1126/scirobotics. aaq0560.\n\nJoey Bose, Ariella Smofsky, Renjie Liao, Prakash Panangaden, and Will Hamilton. Latent variable modelling with hyperbolic normalizing flows. In International Conference on Machine Learning (ICML), pp. 1045–1055, 2020. URL https://proceedings.mlr.press/v119/ bose20a.html.\n\nNicolas Boumal. An introduction to optimization on smooth manifolds. To appear with Cambridge\n\nUniversity Press, 2022. URL http://www.nicolasboumal.net/book.\n\nIan M. Bullock, Raymond R. Ma, and Aaron M. Dollar. A hand-centric classification of human IEEE Transactions on Haptics, 6(2):129–144, 2013. doi:\n\nand robot dexterous manipulation. 10.1109/TOH.2012.53.\n\nGary B ́ecigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. In International Conference on Learning Representations (ICLR), 2019. URL https://openreview. net/pdf?id=r1eiqi09K7.\n\nInes Chami, Albert Gu, Vaggos Chatziafratis, and Christopher R ́e. From trees to continuous embeddings and back: Hyperbolic hierarchical clustering. In Neural Information Processing Systems (NeurIPS), pp. 15065–15076, 2020. URL https://proceedings.neurips.cc/ paper/2020/file/ac10ec1ace51b2d973cd87973a98d3ab-Paper.pdf.\n\nIsaac Chavel. Eigenvalues in Riemannian geometry. Academic press, 1984.\n\nSerge Cohen and MA Lifshits. Stationary Gaussian random fields on hyperbolic spaces and on euclidean spheres. ESAIM: Probability and Statistics, 16:165–221, 2012. URL https:// eudml.org/doc/222466.\n\nCalin Cruceru, Gary B ́ecigneul, and Octavian-Eugen Ganea. Computationally tractable Riemannian manifolds for graph embeddings. In AAAI Conf. on Artificial Intelligence, pp. 7133–7141, 2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/16877.\n\nMark R. Cutkosky. On grasp choice, grasp models, and the design of hands for manufacturing tasks. IEEE Transactions on Robotics and Automation, 5(3):269–279, 1989. doi: 10.1109/70.34763.\n\nThomas Feix, Javier Romero, Heinz-Bodo Schmiedmayer, Aaron M. Dollar, and Danica Kragic. The GRASP taxonomy of human grasp types. IEEE Transactions on Human-Machine Systems, 46(1):66–77, 2016. doi: 10.1109/THMS.2015.2470657.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nFrancesco Di Giovanni, Giulia Luise, and Michael M. Bronstein. Heterogeneous manifolds In ICLR 2022 Workshop on Geometrical and Topofor curvature-aware graph embedding. logical Representation Learning, 2022. URL https://openreview.net/forum?id= rtUxsN-kaxc.\n\nAlexander Grigoryan and Masakazu Noguchi. The heat kernel on hyperbolic space. Bulletin of the\n\nLondon Mathematical Society, 30(6):643–650, 1998. doi: 10.1112/S0024609398004780.\n\nJames Hensman, Alexander Matthews, and Zoubin Ghahramani. Scalable variational Gaussian process classification. In Intl. Conf. on Artificial Intelligence and Statistic (AISTATS), 2015. URL https://proceedings.mlr.press/v38/hensman15.html.\n\nNo ́emie Jaquier, Viacheslav Borovitskiy, Andrei Smolensky, Alexander Terenin, Tamim Asfour, and Leonel Rozo. Geometry-aware Bayesian optimization in robotics using Riemannian Mat ́ern In Conference on Robot Learning (CoRL), 2021. URL https://openreview. kernels. net/forum?id=ovRdr3FOIIm.\n\nKristopher Jensen, Ta-Chu Kao, Marco Tripodi, and Guillaume Hennequin. Manifold GPLVMs for discovering non-Euclidean latent structure in neural data. In Neural Information Processing Systems (NeurIPS), pp. 22580–22592, 2020. URL https://proceedings.neurips.cc/ paper/2020/file/fedc604da8b0f9af74b6cfc0fab2163c-Paper.pdf.\n\nMartin Jørgensen and Søren Hauberg. Isometric Gaussian process latent variable model for dissim-\n\nilarity data. In Intl. Conf. on Machine Learning (ICML), 2021.\n\nMax Kochurov, Rasul Karimov, and Serge Kozlukov. Geoopt: Riemannian optimization in PyTorch.\n\narXiv:2005.02819, 2020. URL https://github.com/geoopt/geoopt.\n\nAndr ́e Langenstein. Generating whole-body multi-contact motions between support poses using\n\ndynamical movement primitives. Master’s thesis, Karlsruhe Institute of Technology, 2020.\n\nNeil D. Lawrence. dimensional\n\nhigh 2003. 9657c1fffd38824e5ab0472e022e577e-Paper.pdf.\n\nGaussian process In Neural\n\nfor visualisation of (NeurIPS), Systems https://proceedings.neurips.cc/paper/2003/file/\n\nlatent variable models Information Processing\n\nURL\n\ndata.\n\nNeil D. Lawrence and Joaquin Qui ̃nonero Candela. Local distance preservation in the GP-LVM through back constraints. In Intl. Conf. on Machine Learning (ICML), pp. 513–520, 2006. doi: 10.1145/1143844.1143909.\n\nNikola ̆ı Nikolaevich Lebedev, Richard A Silverman, and DB Livhtenberg. Special functions and\n\ntheir applications. Physics Today, 18(12):70, 1965.\n\nJohn Lee.\n\nIntroduction to Riemannian Manifolds. Springer, 2nd edition, 2018. doi: 10.1007/\n\n978-3-319-91755-9.\n\nYun Lin and Yu Sun. Robot grasp planning based on demonstrated grasp strategies.\n\nInternational Journal of Robotics Research (IJRR), 34(1):26–42, 2015. 0278364914555544.\n\ndoi:\n\nThe 10.1177/\n\nChristian Mandery, J ́ulia Borr`as, Mirjam J ̈ochner, and Tamim Asfour. Using language models to generate whole-body multi-contact motions. In IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), pp. 5411–5418, 2016. doi: 10.1109/IROS.2016.7759796.\n\nEmile Mathieu, Charline Le Lan, Chris J. Maddison, Ryota Tomioka, and Yee Whye Teh. Continuous hierarchical representations with Poincar ́e variational auto-encoders. In Neural Information Processing Systems (NeurIPS), 2019. URL https://proceedings.neurips.cc/ paper/2019/file/0ec04cb3912c4f08874dd03716f80df1-Paper.pdf.\n\nHenry P. McKean. An upper bound to the spectrum of ∆ on a manifold of negative curvature.\n\nJournal of Differential Geometry, 4(3):359–366, 1970. doi: 10.4310/jdg/1214429509.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fujita, and Masanori Koyama. A wrapped normal distribution on hyperbolic space for gradient-based learning. In Intl. Conf. on Machine Learning (ICML), pp. 4693–4702, 2019. URL https://proceedings.mlr.press/v97/ nagano19a.html.\n\nMaximillian Nickel\n\nand Douwe Kiela.\n\nrepresentations.\n\narchical 2017. 59dfa2df42d9e3d41f5b02bfc32229dd-Abstract.html.\n\nhierIn Neural (NeurIPS), https://papers.neurips.cc/paper/2017/hash/\n\nembeddings Information Processing\n\nfor Systems\n\nPoincar ́e\n\nlearning\n\nURL\n\nMaximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the Lorentz model of hyperbolic geometry. In Intl. Conf. on Machine Learning (ICML), pp. 3779–3788, 2018. URL http://proceedings.mlr.press/v80/nickel18a.html.\n\nDavid Paulius, Yongqiang Huang, Jason Meloncon, and Yu Sun. Manipulation motion taxonomy In IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), pp.\n\nand coding for robots. 5596–5601, 2019. doi: 10.1109/IROS40897.2019.8967754.\n\nDavid Paulius, Nicholas Eales, and Yu Sun. A motion taxonomy for manipulation embedding. In Robotics: Science and Systems (R:SS), 2020. URL http://www.roboticsproceedings. org/rss16/p045.pdf.\n\nWei Peng, Tuomas Varanka, Abdelrahman Mostafa, Henglin Shi, and Guoying Zhao. Hyperbolic deep neural networks: A survey. ArXiv, abs/2101.04562, 2021. URL https://arxiv.org/ abs/2101.04562.\n\nCarl Edward Rasmussen and Christopher KI Williams. Gaussian Processes for Machine Learning.\n\nMIT Press, 2006. URL http://www.gaussianprocess.org/gpml/.\n\nJohn G. Ratcliffe. Foundations of Hyperbolic Manifolds. Springer, 3rd edition, 2019. doi: 10.1007/\n\n978-3-030-31597-9.\n\nJavier Romero, Thomas Feix, Hedvig Kjellstr ̈om, and Danica Kragic. Spatio-temporal modeling of grasping actions. In IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), pp. 2103– 2108, 2010. doi: 10.1109/IROS.2010.5650701.\n\nBruno Siciliano and Oussama Khatib. Springer Handbook of Robotics. Springer Cham, 2016. ISBN\n\n978-3-319-32550-7. doi: 10.1007/978-3-319-32552-1.\n\nOndrej Skopek, Octavian-Eugen Ganea, and Gary B ́ecigneul. Mixed-curvature variational auIn International Conference on Learning Representations (ICLR), 2020. URL\n\ntoencoders. https://openreview.net/forum?id=S1g6xeSKDS.\n\nFrancesca Stival, Stefano Michieletto, Matteo Cognolato, Enrico Pagello, Henning M ̈uller, and Manfredo Atzori. A quantitative taxonomy of human hand grasps. Journal of NeuroEngineering and Rehabilitation, 16(28), 2019. doi: 10.1186/s12984-019-0488-x.\n\nMichalis K. Titsias. Variational Learning of Inducing Variables in Sparse Gaussian Processes. In Intl. Conf. on Artificial Intelligence and Statistic (AISTATS), pp. 567–574, 2009. URL https: //proceedings.mlr.press/v5/titsias09a.html.\n\nMichalis K. Titsias and Neil D. Lawrence. Bayesian Gaussian process latent variable model. In Intl. Conf. on Artificial Intelligence and Statistic (AISTATS), pp. 844–851, 2010. URL https: //proceedings.mlr.press/v9/titsias10a.html.\n\nAlessandra Tosi, Søren Hauberg, Alfredo Vellido, and Neil D. Lawrence. Metrics for probabilistic\n\ngeometries. In Conference on Uncertainty in Artificial Intelligence (UAI), 2014.\n\nRaquel Urtasun, David J. Fleet, Andreas Geiger, Jovan Popovi ́c, Trevor J. Darrell, and Neil D. Lawrence. Topologically-constrained latent variable models. In Intl. Conf. on Machine Learning (ICML), pp. 1080–1087, 2008. doi: 10.1145/1390156.1390292.\n\nPeter Whittle. Stochastic processes in several dimensions. Bulletin of the International Statistical\n\nInstitute, 40(2):974–994, 1963.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA HYPERBOLIC MANIFOLD\n\nA.1 EQUIVALENCE OF POINCAR ́E AND LORENTZ MODELS\n\nAs pointed out in the main text (§ 2), it is possible to map points from the Lorentz model to the Poincar ́e ball via an isometric mapping. Formally, such an isometry is defined as the mapping function f : Ld → P d such that\n\nf (x) =\n\n(x1, . . . , xd)T x0 + 1\n\n,\n\n(12)\n\nwhere x ∈ Ld with components x0, x1, . . . , xd. The inverse mapping f −1 : P d → Ld is defined as follows\n\nf −1(y) =\n\n(cid:0)1 + ∥y∥2, 2y1, . . . , 2yd 1 − ∥y2∥\n\n(cid:1)T\n\n,\n\n(13)\n\nwith y ∈ P d with components y1, . . . , yd. Notice that we used the mapping Eq. 12 to represent the hyperbolic embeddings in the Poincar ́e disk throughout the paper, as well as in the computation of the kernel kH2\n\nEq. 4.\n\nA.2 MANIFOLD OPERATIONS\n\nAs mentioned in the main text (§ 2), we resort to the exponential and logarithmic maps to operate with Riemannian manifold data. The exponential map Expx(u) : TxM → M maps a point u in the tangent space of x to a point y on the manifold, while the logarithmic map Logx(u) : M → TxM performs the corresponding inverse operation. In some settings, it is necessary to work with data lying on different tangent spaces of the manifold. In this case, one needs to operate with all data (cid:0)u(cid:1) : on a single tangent space, which can be achieved by leveraging the parallel transport Px→y TxM → TyM. All the aforementioned operators are defined in Table 2 for the Lorentz model Ld. Moreover, we introduce the inner product ⟨u, v⟩x between two points on Ld, which is used to compute the geodesic distance dM(u, v) and all the foregoing operations in the Lorentz model, as shown in Table 2.\n\nOperation\n\n⟨u, v⟩x dM(u, v) Expx(u) Logx(y) (cid:0)v(cid:1) Px→y\n\ni=1 uivi\n\nFormula −u0v0 + (cid:80)d arcosh(−⟨u, v⟩x) cosh(∥u∥L)x + sinh(∥u∥L) u dM(x,y) √\nα2−1\n\n(y + αx) with α = ⟨x, y⟩x\n\n∥u∥L\n\nv + ⟨y,v⟩x\n\n1−⟨x,y⟩x\n\n(x + y)\n\nwith ∥u∥L = (cid:112)⟨u, u⟩x\n\nTable 2: Principal operations on Hd for the Lorentz model. For more details, see (Bose et al., 2020) and (Peng et al., 2021).\n\nB HYPERBOLIC KERNELS\n\nAs mentioned in the main text (§ 3.1), following the developments on kernels on manifolds like Borovitskiy et al. (2020); Jaquier et al. (2021), we may identify the generalized squared exponential kernel with the heat kernel—an important object studied on its own in the mathematical literature. Due to this, we can obtain the expressions Eq. 4. The expression for the case of H2 requires discretizing the integral, which may lead to an approximation that is not positive semidefinite. We address this by suggesting another approximation guaranteed to be positive semidefinite.\n\nReversing the derivation in (Chavel, 1984, p. 246), we obtain\n\nkH2 ∞,κ,σ2(x, x′) =\n\nσ2 C ′\n\n∞\n\n(cid:90) ∞\n\n0\n\nexp(−s2/(2κ2))P−1/2+is(cosh(ρ))s tanh(πs)ds,\n\n(14)\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nwhere ρ = distHd (x, x′) denotes the geodesic distance between x, x′ ∈ H2, κ and σ2 are the kernel lengthscale and variance, C ′ ∞ is a normalizing constant and Pα are Legendre functions Abramowitz & Stegun (1964). Now we prove that these Legendre functions are connected to the spherical functions — special functions closely tied to the geometry of the hyperbolic space and possessing a very important property. Proposition. Assume the disk model of H2 (i.e. its boundary, the circle, by T. Define the hyperbolic outer product by ⟨z, b⟩ = 1 z ∈ D, b ∈ T. Then\n\nthe Poincar ́e disk). Denote the disk by D and |z−b|2 for\n\n2 log 1−|z|2\n\nP−1/2+is(cosh(ρ)) =\n\n(cid:90)\n\nT\n\ne(2si+1)⟨z,b⟩db\n\n=\n\n(cid:90)\n\nT\n\n(cid:124) (cid:125) (cid:123)(cid:122) spherical function φ2s(z)\n\ne(2si+1)⟨z1,b⟩e(2si+1)⟨z2,b⟩db,\n\n(15)\n\nwhere z ∈ D is such that ρ = distH2 (z, 0) and z1, z2 ∈ D are such that ρ = distH2(z1, z2). Here i denotes the imaginary unit and z is the complex conjugation.\n\nProof. Let θ denote the angle between z and b, and note the following simple identities\n\n|z − b|2 = |z|2 + 1 − 2|z| cos(θ) = tanh(ρ)2 + 1 − 2 tanh(ρ) cos(θ), 1 − |z|2 = 1 − tanh(ρ)2 = cosh(ρ)−2.\n\n(16)\n\n(17)\n\nThen, we write\n\ne(2si+1)⟨z,b⟩ =\n\n(cid:19)−si−1/2\n\n(cid:18) |z − b|2 1 − |z|2\n\n= (cid:0)cosh(ρ)2(tanh(ρ)2 + 1 − 2 tanh(ρ) cos(θ))(cid:1)−si−1/2\n\n,\n\n(18) = (cid:0)sinh(ρ)2 + cosh(ρ)2 − 2 sinh(ρ) cosh(ρ) cos(θ)(cid:1)−si−1/2 ,\n(19)\n\n= (cosh(2ρ) + sinh(2ρ) cos(θ))−si−1/2 .\n\n(20)\n\nOn the other hand, by (Lebedev et al., 1965, Eq. 7.4.3), we have Pa(cosh(x)) = 1 sinh(x) cos(θ))adθ, hence\n\nπ\n\n(cid:82) π 0 (cosh(x) +\n\nP−1/2+is(cosh(2ρ)) =\n\n=\n\n=\n\n(cid:90) π\n\n1 π\n1 2π (cid:90)\n\nT\n\n(cosh(2ρ) + sinh(2ρ) cos(θ))−1/2+isdθ,\n\n0 (cid:90) π\n\n−π\n\n(cosh(2ρ) + sinh(2ρ) cos(θ))−1/2+isdθ,\n\ne(−2si+1)⟨z,b⟩db = φ−2s(z).\n\n(21)\n\n(22)\n\n(23)\n\nThis computation roughly follows Cohen & Lifshits (2012, Section 4.3.4). Now, by Cohen & Lifshits (2012, Section 3.5), we have φ−2s(z) = φ2s(z) which proves the first identity. Finally, Lemma 3.5 from Cohen & Lifshits (2012) proves the second identity.\n\nBy combining expressions Eq. 14 and Eq. 15, we get the following Monte Carlo approximation\n\nkH2 ∞,κ,σ2 (x, x′) ≈\n\nσ2 C ′\n\n∞\n\n1 L\n\nL (cid:88)\n\nl=1\n\nsl tanh(πsl)e(2sli+1)⟨xP ,bl⟩e(2sli+1)⟨x′\n\nP ,bl⟩,\n\n(24)\n\ni.i.d.∼ U (T) and sl\n\nwhere bl text (see § 3.1).\n\ni.i.d.∼ e−s2κ2/21[0,∞)(s). This gives the approximation used in the main\n\nHaving established a way to evaluate or approximate the heat kernel, analogs of Mat ́ern kernels can be defined by\n\nkν,κ,σ2(x, x′) =\n\n√\n\nwhere ̃k∞, simplicity. Here Cν is the normalizing constant ensuring that kν,κ,σ2(x, x) = σ2 for all x.\n\n2u,σ2 is the same as k∞,\n\n(cid:90) ∞\n\nuν−1e− 2ν\n\nσ2 Cν 2u,σ2 but with the normalizing constant σ2/C∞ dropped for √\n\n2u,σ2(x, x′)du,\n\nκ2 u ̃k∞,\n\n(25)\n\n√\n\n0\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nC GPHLVM VARIATIONAL INFERENCE\n\nAs mentioned in § 3.2, when training our GPHLVM on large datasets, we resort to variational inference as originally proposed in (Titsias & Lawrence, 2010). Here we provide the mathematical details about the changes that are needed to train our model via variational inference.\n\nC.1 COMPUTING THE KL DIVERGENCE BETWEEN TWO HYPERBOLIC WRAPPED NORMAL\n\nDISTRIBUTIONS\n\nAs mentioned in § 3.2, we approximate the KL divergence between two hyperbolic wrapped distributions via Monte-Carlo sampling. Namely, given two hyperbolic wrapped distributions qφ(x) and p(x), we write\n\nKL(cid:0)qφ(x)||p(x)(cid:1) =\n\n(cid:90)\n\nqφ(x) log\n\nqφ(x) p(x)\n\ndx ≈\n\n1 K\n\nK (cid:88)\n\nk=1\n\nlog\n\nqφ(xk) p(xk)\n\n,\n\n(26)\n\nwhere we used K independent Monte-Carlo samples drawn from qφ(x) to approximate the KL divergence. These samples are obtained via the procedure described in § 2, i.e., by sampling an element on the tangent space of the origin μ0 = (1, 0, . . . , 0)T of Hd, via a Euclidean normal distribution, and then applying the parallel transport operation and the exponential map to project it onto Hd.\n\nC.2 DETAILS OF THE VARIATIONAL PROCESS\n\nAs mentioned in the main text, the marginal likelihood p(Y ) is approximated via variational inference by approximating the posterior p(X|Y ) with the hyperbolic variational distribution qφ(X) as defined by Eq. 6. The lower bound Eq. 7 is then obtained, similarly as in (Titsias & Lawrence, 2010), as\n\nlog p(Y ) = log\n\n= log\n\n(cid:90)\n\n(cid:90)\n\np(Y |X)p(X)dX\n\np(Y |X)p(X)\n\ndX = log Eqφ(X)\n\n≥ Eqφ(X)\n\n(cid:20)\n\nlog\n\n(cid:90)\n\nqφ(X) log\n\nqφ(X) qφ(X) (cid:21) p(Y |X)p(X) qφ(X)\n\n=\n\n(cid:90)\n\n(cid:90)\n\n=\n\nqφ(X) log p(Y |X)dX −\n\nqφ(X) log\n\nqφ(X) p(X)\n\ndX\n\n= Eqφ(X) [log p(Y |X)] − KL(cid:0)qφ(X)||p(X)(cid:1),\n\n(cid:20) p(Y |X)p(X) qφ(X)\n\n(cid:21)\n\np(Y |X)p(X) qφ(X)\n\ndX\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\ninequality in Eq. 29.\n\nthe expectation following Jensen’s Eqφ(X) [log p(Y |X)] can be decomposed into individual terms for each observation dimension as (cid:80)D Eqφ(X) [log p(yd|X)], where yd is the d-th column of Y . We then define the inducing inputs Zd and inducing variables ud the same way as the noiseless observations fd, so that the joint distribution of fd and ud can be written as\n\nAs mentioned in § 3.2,\n\nd=1\n\np(fd, ud) =\n\n(cid:19)\n\n(cid:18)fd ud\n\n= N\n\n(cid:19)\n\n(cid:18)(cid:18) md(X) md(Zd)\n\n,\n\n15\n\n(cid:18) kd(X, X)\n\nkd(X, Zd) kd(Zd, X) kd(Zd, Zd)\n\n(cid:19)(cid:19)\n\n.\n\n(32)\n\nUnder review as a conference paper at ICLR 2023\n\nThe lower bound Eq. 8 is then obtained for each dimension, similarly as in (Hensman et al., 2015), as\n\nlog p(yd|X) =\n\n(cid:90)\n\nlog p(yd|X, ud)p(ud)dud\n\n(cid:90)\n\n= log\n\np(yd|X, ud)p(ud)\n\nqλ(ud) qλ(ud) (cid:21) p(yd|X, ud)p(ud) qλ(ud)\n\n≥ Eqλ(ud)\n\n(cid:20)\n\nlog\n\n(cid:90)\n\n=\n\nqλ(ud) log p(yd|X, ud)dud −\n\n=\n\n(cid:90)\n\ndud = log Eqλ(ud)\n\n(cid:20) p(yd|X, ud)p(ud) qλ(ud)\n\n(cid:21)\n\n(33)\n\n(34)\n\n(cid:90)\n\nqλ(ud) log\n\np(yd|X, ud)p(ud) qλ(ud)\n\ndud (35)\n\nqλ(ud) log\n\nqλ(ud) p(ud)\n\ndud\n\n(cid:2)Ep(fd|ud) [log p(yd|fd(X))](cid:3) − KL(cid:0)qλ(ud)||p(ud)(cid:1)\n\n= Eqλ(ud) [log p(yd|X, ud)] − KL(cid:0)qλ(ud)||p(ud)(cid:1) ≥ Eqλ(ud) = Eqλ(fd) [log p(yd|fd(X))] − KL(cid:0)qλ(ud)||p(ud|Zd)(cid:1) = Eqλ(fd)\n\n(cid:2)log N (yd; fd(X), σ2\n\nd)(cid:3) − KL(cid:0)qλ(ud)||p(ud|Zd)(cid:1),\n\n(36)\n\n(37)\n\n(38)\n\n(39)\n\n(40)\n\nwhere we defined qλ(fd) = (cid:82) p(fd|ud)qλ(ud)dud with the Euclidean variational distribution qλ(ud) = N (ud; ̃μd, ̃Σd), and wrote p(ud|Zd) = p(ud) for simplicity. The inequality Eq. 35 corresponds to Jensen’s inequality, while Eq. 38 is shown in (Titsias, 2009).\n\nFinally, substituting Eq. 40 in Eq. 31 results in the following bound on the marginal likelihood\n\nlog p(Y ) ≥\n\nN (cid:88)\n\nD (cid:88)\n\nn=1\n\nd=1\n\nEqφ(xn)\n\n(cid:2)Eqλ(fn,d)\n\n(cid:2)log N (yn,d; fn,d(xn), σ2\n\nd)(cid:3)(cid:3)\n\n−\n\nD (cid:88)\n\nd=1\n\nKL(cid:0)qλ(ud)||p(ud|Zd)(cid:1) −\n\nN (cid:88)\n\nn=1\n\nKL(cid:0)qφ(xn)||p(xn)(cid:1).\n\n(41)\n\nD MAT ́ERN KERNELS ON TAXONOMY GRAPHS\n\nAs explained in § 4 of the main paper, we leverage the Mat ́ern kernel on graphs proposed by Borovitskiy et al. (2021) to design a kernel for our back-constrained GPHLVM that accounts for the geometry of the taxonomy graph. Here we provide the main equations of such a kernel, and refer the reader to (Borovitskiy et al., 2021) for further details. Formally, let us define a graph G = (V, E) with vertices V and edges E and the graph Laplacian as ∆ = D − W , where W is the graph adjacency matrix and D its corresponding diagonal degree matrix, with Dii = (cid:80) j Wij. The eigendecomposition U ΛU T of the Laplacian ∆ is then used to formulate both the SE and Mat ́ern kernels on graphs, as follows,\n\nkG ∞,κ(cn, cm) = U\n\n(cid:16)\n\n2 Λ(cid:17)\n\ne− κ2\n\nU T,\n\nand kG\n\nν,κ(cn, cm) = U\n\n(cid:19)−ν\n\n(cid:18) 2ν\n\nκ2 + Λ\n\nU T,\n\n(42)\n\nwhere κ is the lengthscale (i.e., it controls how distances are measured) and ν is the smoothness parameter determining mean-squared differentiability of the associated Gaussian process (GP). Note that the graph kernel expressions in Eq. 42 are obtained by considering the connection between Mat ́ern kernel GPs and stochastic partial differential equations, originally proposed by Whittle (1963) and later extended to Riemannian manifolds in (Borovitskiy et al., 2020). This connection establishes that SE and Mat ́ern GPs satisfy\n\ne− κ2\n\n4 ∆f = W,\n\nand\n\n(cid:19) ν\n\n2\n\n(cid:18) 2ν\n\nκ2 + ∆\n\nf = W,\n\n(43)\n\nwhere W ∼ N (0, I) and f : V → R, which lead to definition of graph GPs (Borovitskiy et al., 2021).\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Reg. with Ldistortion\n\n(b) Distances for 6a.\n\n(c) Reg. with (cid:101)Ldistortion\n\n(d) Distances for 6c.\n\nFigure 6: Embeddings learned with distortion regularization. (a) and (c) display the latent embeddings after training our GPHLVM model with an added distortion loss Ldistortion as it was originally defined, and with our modified distortion loss (cid:101)Ldistortion, respectively. These embeddings indeed show that our regularizations failed to encode the distances in the graph (comparing the distances provided in (b) and (d) with Fig. 3).\n\nE DISTORTION LOSS\n\nAs explained in the paper, we focus on two ways of embedding the graph in the hyperbolic space: a global approach using a stress regularization which matches graph distances with geodesic distances, and a combination between this stress regularization and the use of back constraints (see § 4). However, the literature on graph embeddings also surveys a distortion loss (Cruceru et al., 2021) given by\n\nLdistortion(X) =\n\n(cid:88)\n\ni<j\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ndistHQ(xi, xj)2 distG(ci, cj)2 − 1\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n,\n\n(44)\n\nwhich tries to match the graph and manifold distances by minimizing their ratio’s distance to 1.\n\nWe found that our problem is more subtle than usual graph embeddings, given that several points in our dataset may correspond to the same graph node (e.g., two different poses in which the left foot is the only limb in contact). Indeed, notice that Eq. 45 is ill-defined for the case i = j (or equivalently distG(ci, cj)2 = 0). This is because all nodes xi are assumed to be different from each other. However, in our setup, several xi may correspond to the exact same class in the taxonomy.\n\nOur first attempt to remediate this was to add a simple regularizer ε = 10−1 to the denominator. However, this caused the loss to give more weight to the points where distG(ci, cj)2 = 0 (see Fig. 6a6b for the outcome of training a GPHLVM with this type of regularization). We then considered an alternate definition of distortion in which the term inside the sum is given by\n\n(cid:101)Ldistortion(xi, xj) =\n\n(cid:26) λ1 distHQ(xi, xj)\n\nif xi and xj’s classes are identical\n\nλ2Ldistortion(xi, xj) otherwise\n\n(45)\n\nwhere λ1, λ2 ∈ R+ are hyperparameters. λ1 governs how much we encourage latent codes of the same class to collapse into a single point, while λ2 weights how much the geodesic distance should match the graph distance. After manual hyperparameter tuning, we obtained the latent space and distance matrix portrayed in Figs. 6c-6d. As can be seen in both accounts, the distortion loss produced lackluster results and failed to properly match the latent space distances with that of the graph. For these experiments, we used a loss scale of 50, λ1 = 0.01 and λ2 = 10, meaning that we strongly encouraged the distances between non-identical classes to match in ratio.\n\nF ADDITIONAL DETAILS ON THE EXPERIMENTS OF § 5\n\nF.1 DATA\n\nTable 3 describes the data of the whole-body support pose taxonomy used in the experiments reported in § 5. Each pose is identified with a support pose category, i.e., a node of the graph in Fig. 1-right, and with a set of associated contacts. As shown in the table, some support poses include several sets of contacts. For example, the support pose F groups all types of support poses\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nwhere only one foot is in contact with the environment. Notice that some sets of contacts are not represented in the data and thus do not appear in Table 3.\n\nSupport pose\n\nContacts\n\nNumber\n\nF\n\nFH\n\nF2\n\nFH2\n\nF2H\n\nF2H2\n\nK\n\nFK\n\nKH\n\nK2\n\nFKH\n\nKH2\n\nK2H\n\nFKH2 K2H2\n\nLeft foot Right foot Left foot, left hand Right foot, right hand Left foot, right hand Right foot, left hand Left foot, right foot Left foot, left hand, right hand Right foot, left hand, right hand Left foot, right foot, left hand Left foot, right foot, right hand Left foot, right foot, left hand, right hand\n\nLeft knee Right knee Left foot, right knee Right foot, left knee Left knee, left hand Right knee, right hand Left knee, right knee Right foot, left knee, left hand Left foot, right knee, right hand Left knee, left hand, right hand Left knee, right knee, left hand Left knee, right knee, right hand Right foot, left knee, left hand, right hand Left knee, right knee, left hand, right hand\n\n7 6\n5 6\n5 6\n6 6\n6 5\n7 7\n\n1 1\n2 3\n4 1\n1 5\n2 1\n2 1\n2 2\n\nTable 3: Poses description extracted from the whole-body support pose taxonomy (Borr`as et al., 2017) used in § 5 and App. G.\n\nF.2 TRAINING PARAMETERS AND PRIORS\n\nTable 4 describes the hyperparameters used for the experiments reported in § 5 and App. G. We used the hyperbolic kernels defined in § 3.1 for the GPHLVMs, and the classical SE kernel for the Euclidean models. For the back-constraints mapping Eq. 11, we defined kRD (yn, ym) as the product of a Euclidean SE kernel with lengthscale κRD , and kG(cn, cm) as a graph Mat ́ern kernel with smoothness ν = 2.5 and lengthscale κG. We additionally scaled the product of kernels with a variance σRD,G. For training the back-constrained GPHLVM and GPLVM, we used a Gamma prior Gamma(α, β) with shape α and rate β on the lengthscale κ of the kernels. The embeddings of the Euclidean models were initialized with PCA. For the GPHLVMs, the initial embeddings HQ at the origin μ0 ̃v obtained via PCA were transformed to elements of the tangent space Tμ0 by setting v = (0, ̃v)T and then projected to the hyperbolic manifold using the exponential map. All models were trained by maximizing the loss L = LMAP − γLstress, where LMAP denotes the log posterior of the model, Lstress is the stress-based regularization loss defined in Eq. 10, and γ is a parameter balancing the two losses. The optimization was conducted using the Riemannian Adam optimizer (B ́ecigneul & Ganea, 2019) implemented in Geoopt (Kochurov et al., 2020) with a learning rate of 0.05.\n\nFor the first part of the experiments on taxonomy expansion, we encoded unseen poses of each class for the back-constrained GPLVM and GPHLVM with a stress regularization using the models presented in Table 4. For the second part of the experiments, we left the class FH out during training and we “embedded” it using the back-constraints mapping. The newly-trained models also followed the same hyperparameters presented in Table 4.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nExperiment\n\nModel\n\nRegularization Loss scale γ\n\nPrior on κH/RQ\n\nκRD\n\nHyperbolic embeddings of support poses (§ 5)\n\nHyperbolic embeddings in H3 (App. G.1)\n\nHyperbolic embeddings of standing poses (App. G.2)\n\nGPLVM on R2\n\nGPHLVM on H2\n\nGPLVM on R3\n\nGPHLVM on H3\n\nGPLVM on R2\n\nGPHLVM on H2\n\nHyperbolic embeddings of standing poses with an augmented taxonomy (App. G.3)\n\nGPLVM on R3\n\nGPHLVM on H3\n\nNo regularizer Stress BC + Stress No regularizer Stress BC + Stress\n\nNo regularizer Stress BC + Stress No regularizer Stress BC + Stress\n\nNo regularizer Stress BC + Stress No regularizer Stress BC + Stress\n\nNo regularizer Stress BC + Stress No regularizer Stress BC + Stress\n\n0 6\n1.3 0\n6 1.3\n\n0 10 1.5 0\n10 1.5\n\n0 5\n0.7 0\n5 0.7\n\n0 5\n1.5 0\n5 1.5\n\nNone None Gamma(2, 2) None None Gamma(2, 2)\n\nNone None Gamma(2, 2) None None Gamma(2, 2)\n\nNone None Gamma(2, 2) None None Gamma(2, 2)\n\nNone None Gamma(2, 2) None None Gamma(2, 2)\n\n- -\n0.9 -\n- 0.9\n\n- -\n0.9 -\n- 0.9\n\n- -\n0.9 -\n- 0.9\n\n- -\n2.0 -\n- 2.0\n\nκG\n\n- -\n0.6 -\n- 0.6\n\n- -\n0.6 -\n- 0.6\n\n- -\n0.6 -\n- 0.6\n\n- -\n0.8 -\n- 0.8\n\nσRD,G\n\n- -\n2 -\n- 2\n\n- -\n2\n\n- 2\n\n- -\n2 -\n- 2\n\n- -\n2 -\n- 2\n\nTable 4: Summary of experiments and list of hyperparameters.\n\nF.3 MARGINAL LOG-LIKELIHOODS OF TRAINED MODELS\n\nTable 5 shows the marginal loglikelihood (MLL) of the GPHLVM and GPLVM described in § 5. We observe that the hyperbolic models achieve a higher likelihood that their Euclidean counterparts.\n\nRegularization MLL\n\nRegularization MLL\n\nR2\n\nNo reg. Stress BC+Stress\n\n-27.57 -55.33 5.43\n\nH2\n\nNo reg. Stress BC+Stress\n\n-24.37 -49.20 7.67\n\nTable 5: Marginal log-likelihood per geometry and regularization.\n\nF.4 FURTHER DETAILS ON TRAJECTORY GENERATION VIA GEODESICS\n\nTable 6 describes the transitions between support poses obtained by following the geodesic trajectories of the back-constrained GPHLVM and GPLVM with stress prior depicted in Fig. 2c. In contrast to GPHLVM, the Euclidean GPLVM often results in transitions that do not exist in the taxonomy. Interestingly, it also often uses more transitions than those originally needed. Notice that similar results are observed for the GPHLVM and GPLVM with stress prior depicted in Fig. 2b.\n\nStart F\nF F\nF2H F\nF2 FH\n\nEnd F2H F2H2 FH2 FH2 FK K2 K2H FH → F2H→FH2→FKH → KH → K2H FH → F2H → F2→FKH → FK → FKH → FKH2→K2H\n\nTransitions in R2 F→FH2 → FH → F2H F→FH2 → F2H2 F→FH2 F2H → FH → FH2 F→FH2 → FH → F2H → F2→FKH → FK F2→FKH → FK → FKH → FKH2 → KH2→K2\n\nTransitions in H2 F → FH → F2H F → FH → F2H → F2H2 F → FH → FH2 F2H → FH → FH2 F → F2 → FK F2 → FK → K2\n\nTable 6: Transitions (→) between classes of the taxonomy obtained by following the geodesic trajectories depicted in Fig. 2c. The classes and transitions correspond to the colors along the trajectories and match the class corresponding to the closest embedding at each point along the geodesic. Transitions that do not exist in the taxonomy are denoted as →.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Vanilla\n\n(b) Stress prior\n\n(c) BC + stress prior\n\n(d) Adding poses\n\n(e) Adding a class\n\nFigure 7: The first and last two rows respectively show the latent embeddings and examples of interpolating geodesics in P 3 and R3, followed by pairwise distance matrices. Embeddings colors match those of Fig. 1right. Added poses (d) and classes (e) are marked with crosses and highlighted with red in the distance matrices.\n\nG ADDITIONAL EXPERIMENTS\n\nG.1 HYPERBOLIC EMBEDDINGS OF SUPPORT POSES IN H3\n\nIn this section, we embed the 100 poses used in § 5 into 3-dimensional hyperbolic and Euclidean spaces to analyze the performance of the proposed models in higher-dimensional latent spaces. Namely, we test the GPHLVM and GPLVM without regularization, with stress prior, and with back-constraints coupled with stress prior, similarly to the experiments on 2-dimensional latent spaces reported in the paper. Figs. 7a-7c show the learned embeddings alongside the corresponding distance matrices, which are to be compared with the graph distances in Fig. 3. As expected, and similarly to the 2-dimensional embeddings of Fig. 2a, the models without regularization do not encode any meaningful distance structure in the latent spaces (see Fig. 7a). In contrast, the models with stress prior result in embeddings that comply with the taxonomy graph structure, and the back constraints further organize the embeddings inside a class according to the similarity between their observations (see Figs. 7b-7c).\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\n4.04±4.38 0.18±0.34 1.59±1.99 0.16±0.25 0.99±0.74\n\n3.83±4.17 0.15±0.22 0.19±0.28 0.18±0.26 0.68±0.74\n\nRegularization\n\nStress ±σ\n\nH3\n\nR3\n\nWe observed a prominent stress reduction for the Euclidean 3dimensional latent spaces compared to the 2-dimensional ones (see Table 7), as well as a reduction of non-existing transitions when following geodesic trajectories (see Table 8). This is due to the increase of volume available to match the graph structure in R3 relatively to R2. However, all Euclidean models are still\n\nTable 7: Average stress per geometry and regularization.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\noutperformed by the 2-dimensional hyperbolic embeddings presented in § 5 (see Table 1). This is due to the fact that the volume of balls in hyperbolic space increases exponentially with respect to the radius of the ball rather than polynomially as in Euclidean space. In other words, the geometry of the hyperbolic manifold increases the volume available to match the graph structure compared to Euclidean spaces, thus resulting in better low-dimensional representations of taxonomy data. Notice that the GPHLVM models with 3-dimensional hyperbolic latent space result in a similar or slightly reduced stress compared to their 2-dimensional counterparts (presented in § 5). This indicates that the volume of the 2-dimensional hyperbolic latent space is sufficient to represent the considered data. Moreover, similarly as for the 2-dimensional cases, the back-constrained GPHLVM and GPLVM allow us to properly place unseen poses or taxonomy classes into the latent space (see Figs. 7d-7e).\n\nStart F\nF F\nF2H F\nF2 FH\n\nEnd F2H F2H2 FH2 FH2 FK K2 K2H FH → F2H → FKH → KH → K2H FH → F2H → FKH → FKH2→K2H\n\nTransitions in H3 F → FH → F2H F → FH → F2H → F2H2 F → FH → FH2 F2H → FH → FH2 F → F2 → FK F2 → FK → K2\n\nTransitions in R3 F → FH → F2H F → FH→F2H2 F → FH → FH2 F2H → FH → FH2 F → F2 → FK F2 → FK → K2\n\nTable 8: Transitions (→) between classes of the taxonomy obtained by following the geodesic trajectories depicted in Fig. 7c. The classes and transitions correspond to the colors along the trajectories and match the class corresponding to the closest embedding at each point along the geodesic. Transitions that do not exist in the taxonomy are denoted as →.\n\nG.2 HYPERBOLIC EMBEDDINGS OF STANDING POSES\n\nIn this section, we consider a different subset of the whole-body support pose taxonomy, leading to a different graph. Namely, we use 60 standing poses of the dataset in (Mandery et al., 2016) and (Langenstein, 2020), which correspond to graph nodes of standing support poses (left side of the graph in Fig. 1). Specifically, we use a balanced dataset composed of 5 poses for each of the contact sets of the standing support poses described in Table 3. We embed the 60 poses into 2dimensional hyperbolic and Euclidean spaces using GPHLVM and GPLVM. For each approach, we test the model without regularization, with stress prior, and with back-constraints coupled with stress prior using the parameters described in App. F.2 and Table 4.\n\nFigs. 8a-8c show the learned embeddings alongside their corresponding distance matrices, which are to be compared with the graph distances in Fig. 9a. As for the previous experiments, the models with stress prior result in embeddings that comply with the taxonomy graph structure, with additional intra-class organizations for the back-constrained models. It is worth noticing that, despite the fact that the considered taxonomy graph is smaller than for the previous experiments, all Euclidean GPLVMs remain outperformed by the hyperbolic models, which better match the taxonomy structure (see also Table 9b). Similarly to the experiments reported in § 5, the back-constrained GPHLVM and GPLVM allow us to properly place unseen poses or taxonomy classes into the latent space (see Figs. 8d-8e). As mentioned in the main text, our GPHLVM intrinsically provides a mechanism to plan motions via geodesics in the low-dimensional latent space. Examples of geodesics between two standing poses are shown in Figs. 8b-8c, where the trajectory color matches the class corresponding to the closest latent point. The transitions between standing support poses obtained by following these geodesic trajectories are also described in Table 9. As for our previous experiments, the geodesics, i.e., shortest paths, in the GPHLVM latent space correspond to shortest paths in the taxonomy graph. Due to the size of the taxonomy graph, we observe fewer forbidden (i.e. nonexistent) transitions than for the previous experiments in the Euclidean models. However, as their latent space does not match the taxonomy structure, they often require additional transitions and thus do not follow shortest paths in the taxonomy graph.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Vanilla\n\n(b) Stress prior\n\n(c) BC + stress prior\n\n(d) Adding poses\n\n(e) Adding a class\n\nFigure 8: Embeddings of standing poses: The first and last two rows respectively show the latent embeddings of bipedal poses and examples of interpolating geodesics in P 2 and R2, followed by pairwise distance matrices. Embeddings colors match those of Fig. 1-right, and background colors indicate the GPLVM uncertainty. Added poses (d) and classes (e) are marked with stars and highlighted with red in the distance matrices.\n\nR2\n\nH2\n\nRegularization\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nStress ±σ\n\n1.69±1.96 0.62±1.41 0.68±0.96 0.61±0.84 0.47±0.38\n\n1.66±1.95 0.07±0.09 0.15±0.18 0.17±0.20 0.22±0.31\n\n(a) Graph distance between the standing poses.\n\n(b) Average stress per geometry and regularization.\n\nFigure 9: Embeddings of standing poses: (a) shows the graph distance following the left part of Fig. 1-right. (b) shows the stress resulting from the different embeddings of standing poses.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nStart F\nF F\nF2H\n\nEnd F2H F2H2 FH2 FH2\n\nTransitions in H2 F → FH → F2H F → FH → F2H → F2H2 F → FH → FH2 F2H → FH → FH2\n\nTransitions in R2 F → F2 → F → F2 → FH → F2H F → F2 → F→FH2 → F2H2 F → F2 → F→FH2 F2H → FH → F2 → F → F2 → F→FH2 → F2H2 → FH2\n\nTable 9: Embeddings of standing poses: Transitions (→) between classes of the taxonomy obtained by following the geodesic trajectories depicted in Fig. 8c. The classes and transitions correspond to the colors along the trajectories and match the class corresponding to the closest embedding at each point along the geodesic. Transitions that do not exist in the taxonomy are denoted as →.\n\nG.3 HYPERBOLIC EMBEDDINGS OF STANDING POSES WITH AN AUGMENTED TAXONOMY\n\nFOR IMPROVED TRAJECTORY GENERATION\n\nAs shown in § 5, geodesics in the hyperbolic latent space of our GPHLVM intrinsically provide a mechanism to plan motions accounting for the underlying taxonomy. However, as discussed in § 5, the whole-body support pose taxonomy (Borr`as et al., 2017) lacks information about the type of contact in the considered poses, thus leading to artifacts in the geodesic-generated motions. In the main paper, we showed that the quality of the generated motion is improved by augmenting the whole-body support pose taxonomy with additional contact information. To do so, we considered an augmented whole-body support pose taxonomy which explicitly distinguishes between left and right contacts. In other words, the nodes and transitions of Fig. 1-right are adapted to consider left and right contacts. For instance, the 1-foot contact (F) node is separated into left-foot (Fl) and right-foot (Fr) contact nodes. To facilitate motion planning and to test the GPHLVM ability of dealing with high-dimensional spaces, we represent each pose as a vector yn ∈ R44 of joint angles instead of a vector of hands and feet positions.\n\nWe embed the 60 standing poses described in App. G.2 into 3-dimensional hyperbolic and Euclidean spaces using GPHLVM and GPLVM, respectively. For each approach, we test the model without regularization, with stress prior, and with back-constraints coupled with stress prior using the parameters described in App. F.2 and Table 4. Figs. 10a-10c show the learned embeddings alongside their corresponding distance matrices, which are to be compared with the graph distances of the augmented taxonomy in Fig. 11a. Similarly to previous experiments, the models with stress prior result in embeddings complying with the taxonomy graph structure (Fig. 10b), with additional intra-class organizations for the back-constrained models (Fig. 10c). Notice that the embeddings differentiate between left and right contacts according to the augmented taxonomy: For instance, we observe four clusters of orange embeddings corresponding to FlHl, FlHr, FrHl, and FrHr. As shown in Table 11b, the hyperbolic models better represent the taxonomy structure and outperform the Euclidean models. Similarly to previous experiments, the back-constraint mapping introduced in § 4 allows us to properly place unseen poses or taxonomy classes into the latent space (see Figs. 10d-10e).\n\nExamples of motions planned by following geodesics between two standing poses in the hyperbolic latent space are displayed in the main paper (Fig. 5). The corresponding geodesics are shown in Fig. 10c, with the colors along the trajectory matching the class corresponding to the closest hyperbolic latent point. The resulting transitions are given in Table 10. As mentioned in the main paper, we observe that, in contrast to the trajectories of Fig. 4, the motions generated by considering the augmented taxonomy (Fig. 5) result in more realistic – human-like – interpolations between the given initial and final poses. Moreover, these motions look more realistic than the motions obtained via linear interpolation in the Euclidean latent space of the vanilla back-constrained GPLVM. As shown in Fig. 12, the motions planned in the Euclidean latent space sometimes result in unrealistic joint configurations and the same posture is associated with different types of contacts (see middle part of the motions). As shown in Table 10, non-existing transitions arise more frequently when following trajectories generated by the Euclidean model.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Vanilla\n\n(b) Stress prior\n\n(c) BC + stress prior\n\n(d) Adding poses\n\n(e) Adding a class\n\nFigure 10: Embeddings of standing poses considering the augmented whole-body support pose taxonomy: The first and last two rows respectively show the latent embeddings and examples of interpolating geodesics in P 3 and R3, followed by pairwise distance matrices. Embeddings colors match those of Fig. 1-right. Added poses (d) and classes (e) are marked with crosses and highlighted with red in the distance matrices.\n\nR3\n\nH3\n\nRegularization\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nStress ±σ\n\n3.85±3.63 0.31±0.31 4.92±7.60 1.93±2.34 2.31±3.24\n\n4.05±3.77 0.23±0.34 1.25±2.14 1.60±2.21 3.20±4.59\n\n(a) Graph distance between the standing poses.\n\n(b) Average stress per geometry and regularization.\n\nFigure 11: Embeddings of standing poses considering the augmented whole-body support pose taxonomy: (b) shows the stress resulting from the different (a) shows the graph distance (colors follow Fig. 1-right). embeddings of standing poses.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Fl to F2Hr\n\n(b) Fl to F2H2\n\n(c) Fr to FrH2\n\n(d) F2Hl to FlH2\n\nFigure 12: Motions obtained via linear interpolation in the latent space of the vanilla Euclidean backconstrained GPHLVM trained on the augmented taxonomy (Fig. 10c). Contacts are denoted by gray circles. The colorbars identify the support pose of the closest pose in the latent space.\n\nStart Fl Fl Fr F2Hl\n\nEnd F2Hr F2H2 FrH2 FlH2\n\nTransitions in H3 Fl→F2Hr Fl→F2Hl → F2H2 Fr → FrHr → FrH2 F2Hl → F2H2 → FlH2\n\nTransitions in R3 Fl → FlHl → F2Hl→F2Hr Fl → FlHl → F2Hl→F2Hr → F2H2 Fr → FrHl→FrHr → FrH2 F2Hl→F2Hr → FlHr → FlH2\n\nTable 10: Embeddings of standing poses considering the augmented whole-body support pose taxonomy: Transitions (→) between classes of the taxonomy obtained by following the geodesic trajectories depicted in Fig. 10. The classes and transitions correspond to the colors along the trajectories and match the class corresponding to the closest embedding at each point along the geodesic. Transitions that do not exist in the taxonomy are denoted as →.\n\nG.4 COMPARISON AGAINST VARIATIONAL AUTOENCODERS\n\nHyperbolic embeddings of support poses: In this section, we compare the trained GPHLVMs of Fig. 2 with two additional baselines: a vanilla variational autoencoder (VAE) and a hyperbolic variant of this VAE in which the latent space is the Lorentz model of hyperbolic geometry (akin to Mathieu et al. (2019)). Both VAEs are designed with 12 input nodes, 6 hidden nodes, a 2dimensional latent space, and a symmetric decoder. Their encoder specifies the mean and standard deviation of a normal distribution (resp. wrapped normal for the hyperbolic VAE), and their decoder specifies the mean and standard deviation of the normal distribution that governs the reconstructions. Both models are trained by maximizing an Evidence Lower Bound (ELBO) under similar regimes as\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\n(a) P 2 Vanilla\n\n(b) R2 Vanilla\n\n(c) P 2 Stress\n\n(d) R2 Stress\n\nFigure 13: Embeddings of the VAE baselines: The first and second rows show the latent spaces of the (hyperbolic) VAE and the distance matrix between the latent codes, respectively. When comparing these distance matrices and encodings with that of our GPHLVMs (see Fig. 2), we notice that our proposed model is better able to preserve the graph distance structure. We argue this is because VAEs enforce latent spaces that follow a unit Gaussian, which is an opposite goal to ours.\n\nthe GPHLVMs, i.e., 1000 epochs with a learning rate of 0.05. The KL divergence for the hyperbolic VAE is computed using Monte Carlo estimates.\n\nImportantly, the VAE models only seem to capture a global structure that separates standing from kneeling poses (except the vanilla hyperbolic VAE in Fig. 13a). Although adding a stress regularization with the same scale as for the GPHLVM (γ = 6) helps preserve the graph distance structure, the embeddings organization is still not competitive with the one achieved by our GPHLVM models (see Fig. 2). Moreover, when compared to our proposed GPHLVM, all VAE models provide a subpar uncertainty modeling in their latent spaces.\n\nRegularization\n\nStress ±σ\n\nR2 No reg.\n\nStress\n\nH2 No reg.\n\nStress\n\n1.88±2.57 0.59±0.89\n\n3.96±4.22 0.52±0.71\n\nTable 11: Average stress per geometry and regularization for VAE baselines.\n\nTable 11 shows that the average stress of the latent embeddings for the VAE baselines (trained with and without stress regularization) is higher than the average stress of our models (see Table 1). Overall, our proposed GPHLVM consistently outperforms all VAEs to encode meaningful taxonomy information in the latent space. We argue that VAEs are not the right tool for our target applications. When training VAEs, the Kullback-Leibler term in the ELBO tries to regularize the latent space to match a unit Gaussian. This regularization is in stark contrast with our goal of separating the embeddings to preserve the taxonomy graph distances.\n\nHyperbolic embeddings of standing poses with an augmented taxonomy: We further compare our GPHLVM model against the vanilla and hyperbolic VAEs in the experiment described in Sec. G.3. Namely, we consider the augmented whole-body support pose taxonomy which explicitly distinguishes between left and right contacts and we represent each pose as a vector of joint angles. This increases the dimensionality of the data to 44.\n\nWe tested the vanilla and hyperbolic VAEs without regularization and with a stress regularization with the same scale as for the GPHLVM (γ = 1.5). Fig. 14 shows the learned embeddings alongside distance matrices, which are to be compared with the GPHLVM model of Figs. 10b-10c and with the ground-truth graph distances of Fig. 11a. Despite the stress regularization, the VAEs’ tendency to have unitnormally distributed latent representations hinders the distance matching. This is further quantified by the mean stress presented in Table 12, which show a higher mean stress (0.34 and 0.44) than our model in the same taxonomy (0.23, see Table 11b).\n\nRegularization\n\nStress ±σ\n\nR3 No reg.\n\nStress\n\nH3 No reg.\n\nStress\n\n2.33±3.10 0.34±0.37\n\n3.01±3.13 0.44±0.63\n\nTable 12: Average stress per geometry and regularization for VAE baselines trained on the augmented taxonomy (see App. G.3).\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\n(a) H3 Vanilla\n\n(b) R3 Vanilla\n\n(c) H3 Stress\n\n(d) R3 Stress\n\nFigure 14: Embeddings of the VAE baselines considering the augmented whole-body support pose taxonomy: The first and second rows show the latent spaces of the (hyperbolic) VAE and the distance matrix between the latent codes, respectively.\n\nTraining\n\nDecoding\n\nGPHLVM Q = 2 GPHLVM Q = 3 GPLVM Q = 2 2.5 × 103 1.33 × 10−2\n\n8.91 1.57 × 10−5\n\n5.9 1.16 × 10−5\n\nGPLVM Q = 3\n\n6.3 1.22 × 10−5\n\nTable 13: Average runtime (in seconds) for training and decoding phases of our GPHLVM and vanilla GPLVM over 5 experiments, using 2 and 3-dimensional latent spaces for both models. Training time was measured over 500 iterations for both models. The implementations are fully developed on Python, and runtime measurements were taken using a standard laptop with 32 GB RAM, Intel Xeon CPU E3-1505M v6 processor and Ubuntu 20.04 LTS.\n\nFig. 15 shows examples of motions planned by following geodesics between two standing poses in the hyperbolic VAE latent space. Similarly as the motions generated in the latent space of the proposed GPHLVM (Fig. 5), these motions result in realistic interpolations between the given initial and final poses.\n\nG.5 RUNTIME\n\nIn order to show the computational cost of our approach, we ran a set of experiments to measure the average runtime for the training and decoding phases, using 2 and 3-dimensional latent spaces. As a reference, we added the runtime measurements of Euclidean counterpart, that is, the vanilla GPLVM. Table 13 shows the runtime measurements. Note that the main computational burden arises in our GPLHVM with a 2-dimensional latent space, which is in sharp contrast with the experiments using a 3-dimensional latent space. As discussed in the main paper, this increase in computational cost is mainly attributed to the 2-dimensional hyperbolic kernel.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Fl to F2Hr\n\n(b) Fl to F2H2\n\n(c) Fr to FrH2\n\n(d) F2Hl to FlH2\n\nFigure 15: Motions obtained via geodesic interpolation in the latent space of the hyperbolic VAE trained on the augmented taxonomy (Fig. 14c). Contacts are denoted by gray circles. The colorbars identify the support pose of the closest pose in the latent space.\n\n28",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a method to learn an embedding space, which leverages the human-designed taxonomy, for robotics tasks. It extends Gaussian Process Latent Variable Models (GPLVM) to hyperbolic latent spaces and proposes GPHLVM. The motivation for using hyperbolic space is that distances grow exponentially when moving away from the origin, and the shortest paths between distant points tend to pass through it, resembling a continuous hierarchical structure. They propose graph-distance priors and back constraints to introduce taxonomy knowledge into the GPHLVM.\n\nThe experiments are conducted on whole-body support pose taxonomy. The authors analyze the learned Hyperbolic latent embedding space and compare it with the Euclidean one. They also show that the learned embedding enables motion interpolation in the latent space.\n\n# Strength And Weaknesses\n\nStrengths:\n1. The idea of utilizing human-designed taxonomy prior in latent space learning is interesting.\n2. The introduction of Hyperbolic space seems to lead to better structure than Euclidean space.\n\nWeaknesses:\n1. I feel that the experimental part is not enough to demonstrate the advantage of the proposed GPHLVM:\n(a) There is only one task/dataset (whole-body support pose taxonomy) is used in the experiments. It is thus unclear whether the proposed method can be applied to other tasks as well.\n(b) There are no other alternative approaches for comparison (only Gaussian counterpart for visual analysis).\n(c) There is no quantitative comparison on some concrete tasks (e.g., pose classification) to demonstrate the practical benefit of the latent space.\n\n2. An alternative design would be using a neural network to learn an embedding space. We can still apply the priors (i.e., Hyperbolic and taxonomy-aware priors) to the learned embedding space by designing various loss functions. However, this alternative approach will be more straightforward and efficient to implement. Unfortunately, we fail to find such discussion or comparison in the paper.\n\n3. The proposed method hopes that the latent space complies with the human-designed taxonomy's hierarchical structure. However, as shown in the part of the experiments, when the human-designed taxonomy is coarse-grained, the performance of the proposed method is also limited.\n\n4. The runtime information is missing from the text. It's unclear whether the proposed method is an efficient or computationally intractable one. \n\n5. The current latent space is very low-dimensional. Could the authors provide more discussion on the benefits of learning a low-dimensional latent space? Also, it's interesting to know whether the proposed method can handle high-dimensional latent space as well.\n\n6. How shall we interpret the pairwise distance matrices in Figure 2 and compare them with the graph distances in Figure 3. To be honest, it's not easy to tell which one is closer to Figure 3.\n\n7. The interpolated motions in Figure 4 fail to impress me. The transition is not very natural and smooth. Moreover, it includes many physically-infeasible motions. In contrast, Figure 5 shows better performance. Since only motions generated by the proposed GPHLVM are shown, it is hard to demonstrate the advantage of the proposed GPHLVM, as other methods may generate similar or better performances.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: 8/10\nQuality: 7/10\nNovelty: 7/10\nReproducibility: 7/10 (if code released)\n\n# Summary Of The Review\n\nI feel that the experimental part is not enough to demonstrate the advantage of the proposed over existing methods. The practical usage is also unclear.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCONSERWEIGHTIVE BEHAVIORAL CLONING FOR RELIABLE OFFLINE REINFORCEMENT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe goal of offline reinforcement learning (RL) is to learn near-optimal policies from static logged datasets, thus sidestepping expensive online interactions. Behavioral cloning (BC) provides a straightforward solution to offline RL by mimicking offline trajectories via supervised learning. Recent advances (Chen et al., 2021; Janner et al., 2021; Emmons et al., 2021) have shown that by conditioning on desired future returns, BC can perform competitively to their value-based counterparts, while enjoying much more simplicity and training stability. However, the distribution of returns in the offline dataset can be arbitrarily skewed and suboptimal, which poses a unique challenge for conditioning BC on expert returns at test-time. We propose ConserWeightive Behavioral Cloning (CWBC), a simple and effective method for improving the performance of conditional BC for offline RL with two key components: trajectory weighting and conservative regularization. Trajectory weighting addresses the bias-variance tradeoff in conditional BC and provides a principled mechanism to learn from both low return trajectories (typically plentiful) and high return trajectories (typically few). Further, we analyze the notion of conservatism in existing BC methods, and propose a novel conservative regularizer that explicitly encourages the policy to stay close to the data distribution. The regularizer helps achieve more reliable performance, and removes the need for ad-hoc tuning of the conditioning value during evaluation. We instantiate CWBC in the context of Reinforcement Learning via Supervised Learning (RvS) (Emmons et al., 2021) and Decision Transformer (DT) (Chen et al., 2021), and empirically show that it significantly boosts the performance and stability of prior methods on various offline RL benchmarks.\n\n1\n\nINTRODUCTION\n\nIn many real-world applications such as education, healthcare and autonomous driving, collecting data via online interactions can be expensive or even dangerous. However, we often have access to historical logged datasets in these domains that have been collected previously by some unknown policies. The goal of offline reinforcement learning (RL) is to directly learn effective agent policies from such datasets, without additional online interactions (Lange et al., 2012; Levine et al., 2020). Many online RL algorithms have been adapted to work in the offline setting, including value-based methods (Fujimoto et al., 2019; Ghasemipour et al., 2021; Wu et al., 2019; Jaques et al., 2019; Kumar et al., 2020; Fujimoto & Gu, 2021; Kostrikov et al., 2021a) as well as model-based methods (Yu et al., 2020; Kidambi et al., 2020). The key challenge in all these methods is to generalize the value or dynamics to state-action pairs outside the offline dataset.\n\nAn alternative way to approach offline RL is via approaches derived from behavioral cloning (BC) (Bain & Sammut, 1995). BC is a supervised learning technique that was initially developed for imitation learning, where the goal is to learn a policy that mimics the expert demonstrations. Recently, a number of works propose to formulate offline RL as supervised learning problems (Chen et al., 2021; Janner et al., 2021; Emmons et al., 2021). Since offline RL datasets usually do not have expert demonstrations, these works condition BC on extra context information to specify target outcomes such as returns and goals. Compared with the value-based approaches, the empirical evidence has shown that these conditional BC approaches perform competitively, and they additionally enjoy the enhanced simplicity and training stability of supervised learning.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nAs commonly observed for supervised learning approaches, the performance of conditional BC is often limited by the suboptimility of the offline dataset, which particularly can be probed through the distribution of returns in the dataset. There are two related challenges in this regard for offline RL.\n\nFirst, there is a unique bias-variance tradeoff in learning that arises due to the mismatch between the training and test distribution of returns. Typically, offline datasets in the real world mostly contain trajectories with low returns, whereas at test time, we are interested in conditioning on high returns. Simply filtering the offline dataset to contain high return trajectories is not always viable, as the number of such high-return trajectories can be very low leading to high variance during learning.\n\nSecond, the maximum return in the offline trajectories is often far below the desired expert returns. This implies that at test time, we need to condition our agent on out-of-distribution (ood) expert returns. Interestingly, we find that existing BC methods have significantly different behaviors when conditioning on ood returns. While DT (Chen et al., 2021) enjoys a stable performance, RvS (Emmons et al., 2021) is highly sensitive to such ood conditioning and exhibits vast drops in peak performance for such ood inputs. Therefore, the current practice for setting the conditioning return at test-time in RvS is based on careful tuning with online rollouts, which is often tedious, impractical, and inconsistent with the promise of offline RL to minimize online interactions.\n\nWe propose ConserWeightive Behavior Cloning (CWBC), a new BC-based approach for offline RL that mitigates the aforementioned challenges. CWBC consists of 2 key components: trajectory weighting and conservative regularization. With trajectory weighting, we strive to balance the biasvariance trade-off in learning by proposing a scheme for downweighting the low-return trajectories, but at the same time, we do not filter them for data efficiency. Moreover, we introduce a notion of conservatism for ood sensitve BC methods such as RvS, which encourages the policy to stay close to the data distribution when conditioning on large returns. We take trajectories with high returns from the dataset and add positive noise to their returns, which generates trajectories with large ood returns. We predict actions conditioning on the perturbed returns and project them to the original actions by penalizing the l2 distance. By imposing such a regularizer, we can condition the policy on large, unseen target returns at test-time, sidestepping tedious manual tuning and online interactions.\n\nOur proposed algorithm is simple and easy to implement. Empirically, we instantiate our framework in the context of RvS (Emmons et al., 2021) and DT (Chen et al., 2021), two state-of-the-art BC methods for offline RL. CWBC significantly improves the performance of RvS and DT in D4RL (Fu et al., 2020) locomotion tasks by 18% and 8%, respectively, without any hand picking of the value of the conditioning returns at test-time.\n\n2 RELATED WORK\n\nOffline Temporal Difference Learning Most of the existing off-policy RL methods are often based on temporal difference (TD) updates. A key challenge of directly applying them in the offline setting is the extrapolation error: the value function is poorly estimated at unseen state-action pairs. To remedy this issue, various forms of conservatism have been introduced to off-policy RL methods that exploits temporal difference updates, with the purpose of encouraging the learned policy to stay close to the behavior policy that generates the data. For instance, Fujimoto et al. (2019); Ghasemipour et al. (2021) use certain policy parameterizations specifically tailored for offline RL. Wu et al. (2019); Jaques et al. (2019); Kumar et al. (2019) penalize the divergence-based distances between the learned policy and the behavior policy. Fujimoto & Gu (2021) propose an extra behavior cloning term to regularize the policy. This regularizer is simply the l2 distance between predicted actions and the truth, yet surprisingly effective for porting off-policy TD methods to the offline setting. Instead of regularizing the policy, several other works have sought to incorporate divergence regularizations into the value function estimation, e.g., (Nachum et al., 2019; Kumar et al., 2020; Kostrikov et al., 2021a). Another recent work by Kostrikov et al. (2021b) predicts the Q function via expectile regression, where the estimation of the maximum Q-value is constrained to be in the dataset.\n\nBehavior Cloning Approaches for Offline RL Recently, there is a surge of interest in converting offline RL into supervised learning paradigms (Chen et al., 2021; Janner et al., 2021; Emmons et al., 2021). In essence, these approaches conduct behavior cloning (Bain & Sammut, 1995) by additionally conditioning on extra information such as goals or rewards. Among these works, Chen et al. (2021) and Janner et al. (2021) have formulated offline RL as sequence modeling problems\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nand train transformer architectures (Vaswani et al., 2017) in a similar fashion to language and vision (Radford et al., 2018; Chen et al., 2020; Brown et al., 2020; Lu et al., 2022; Yan et al., 2021). Extensions have also been proposed in the context of sequential decision making for offline black-box optimization (Nguyen & Grover, 2022; Krishnamoorthy et al., 2022). A recent work by Emmons et al. (2021) further shows that conditional BC can achieve competitive performance even with a simple but carefully designed MLP network. Earlier, similar ideas have also been proposed for online RL, where the policy is trained via supervised learning techniques to fit the data stored in the replay buffer (Schmidhuber, 2019; Srivastava et al., 2019; Ghosh et al., 2019).\n\nData Exploration for Offline RL Recent research efforts have also been made towards understanding properties and limitations of datasets used for offline RL (Yarats et al., 2022; Lambert et al., 2022; Guo et al., 2021), particularly focusing on exploration techniques during data collection. Both Yarats et al. (2022) and Lambert et al. (2022) collect datasets using task-agnostic exploration strategies (Laskin et al., 2021), relabel the rewards and train offline RL algorithms on them. Yarats et al. (2022) benchmark multiple offline RL algorithms on different tasks including transferring, whereas Lambert et al. (2022) focus on improving the exploration method.\n\n3 PRELIMINARIES\n\nWe model our environment as a Markov decision process (MDP) (Bellman, 1957), which can be described by a tuple M “ xS, A, p, P, R, γy, where S is the state space, A is the action space, pps1q is the distribution of the initial state, P pst`1|st, atq is the transition probability distribution, Rpst, atq is the deterministic reward function, and γ is the discount factor. At each timestep t, the agent observes a state st P S and takes an action at P A. This moves the agent to the next state st`1 „ P p ̈|st, atq and provides the agent with a reward rt “ Rpst, atq.\n\nOffline RL. We are interested in learning a (near-)optimal policy from a static offline dataset of trajectories collected by unknown policies, denoted as Toffline. We assume that these trajectories are i.i.d samples drawn from some unknown static distribution T . We use τ to denote a trajectory and use |τ | to denote its length. Following Chen et al. (2021), the return-to-go (RTG) for a trajectory τ at timestep t is defined as the sum of rewards starting from t until the end of the trajectory: gt “ t1“t rt1 . This means the initial RTG g1 is equal to the total return of the trajectory rτ “\n\nt“1 rt.\n\nř\n\nř\n\n|τ |\n\n|τ |\n\nDecision Transformer (DT). DT (Chen et al., 2021) solves offline RL via sequence modeling. Specifically, DT employs a transformer architecture that generates actions given a sequence of historical states and RTGs. To do that, DT first transforms each trajectory in the dataset into a sequence of returns-to-go, states, and actions:\n\n`\n\n ̆\n\nτ “\n\ng1, s1, a1, g2, s2, a2, . . . , g|τ |, s|τ |, a|τ |\n\n.\n\n(1)\n\nDT trains a policy that generates action at at each timestep t conditioned on the history of RTGs gt ́K:t, states st ́K:t, and actions at ́K:t ́1, wherein K is the context length of the transformer. The learning objective a simple mean square error between the predicted actions and the ground truths:\n\nmin θ\n\nLDTpθq “ Eτ „T\n\nř\n\n“\n\n1 |τ |\n\n|τ | t“1\n\n`\n\nat ́ πθpgt ́K:t, st ́K:t, at ́K:t ́1q\n\n‰\n\n ̆\n\n2\n\n.\n\n(2)\n\nDuring evaluation, DT starts with an initial state s1 and a target RTG g1. At each step t, the agent generates an action at, receives a reward rt and observes the next state st`1. DT updates its RTG gt`1 “ gt ́ rt and generates next action at`1. This process is repeated until the end of the episode.\n\nReinforcement Learning via Supervised Learning (RvS). Emmons et al. (2021) conduct a thorough empirical study of conditional BC methods under the umbrella of Reinforcement Learning via Supervised Learning (RvS), and show that even simple models such as multi-layer perceptrons (MLP) can perform well. With carefully chosen architecture and hyperparameters, they exhibit performance that matches or exceeds the performance of transformer-based models. There are two main differences between RvS and DT. First, RvS conditions on the average reward ωt into the future instead of the sum of future rewards:\n\nωt “ 1\n\nH ́t`1\n\n|τ |\n\nt1“t rt1 “ gt\n\nH ́t`1 ,\n\n(3)\n\nř\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Offline data distribution vs the expert distribution.\n\n(b) The original return distribution T and the transformed distribution rT .\n\nFigure 1: The suboptimality of offline datasets (left) and the effect of trajectory weighting on the return distribution (right). We illustrate on walker2d-med-replay. For weighting, we use B “ 20, λ “ 0.01, κ “ pr‹ ́ pr90, where pr90 is the 90-th percentile of the returns in the offline dataset.\n\nwhere H is the maximum episode length. Intuitively, ωt is RTG normalized by the number of remaining steps. Second, RvS employs a simple MLP architecture, which generates action at at step t based on only the current state st and expected outcome ωt. RvS minimizes a mean square error:\n\nmin θ\n\nLRvSpθq “ Eτ „T\n\n1 |τ |\n\n|τ | t“1\n\n“\n\nř\n\n`\n\nat ́ πθpst, ωtq\n\n.\n\n(4)\n\n‰\n\n ̆\n\n2\n\nAt evaluation time, RvS performs a repeating process similarly to DT, except that the expected outcome is now updated as ωt`1 “ pgt ́ rtq{pH ́ tq.\n\n4 CONSERVATIVE BEHAVIORAL CLONING WITH TRAJECTORY WEIGHTING\n\nA key challenge that behavioral cloning faces in an offline setting is the suboptimality of the dataset, which we can characterize via the distribution of trajectory returns. An ideal offline dataset consists of sufficiently many high-quality trajectories, which have returns matching those of a dataset of expert demonstrations. For such an idealized scenario, offline RL reduces to a vanilla imitation learning problem. In practice, however, we observe that the return distribution for a typical dataset of offline trajectories is spread over a wide range of returns and is highly non-uniform. Figure 1a illustrates the return distribution of the walker2d-med-replay dataset (Fu et al., 2020), which is significantly different from the expert distribution. Therefore, from a return perspective, the trajectories in the offline dataset can be of varying importance for learning, which leads to a bias-variance trade-off. Further, for return-conditioned methods including conditional BC, it is unclear how the policy will behave when conditioned on o.o.d. returns at test-time. We study mitigation techniques for both these challenges in the following sections.\n\n4.1 CONTROLLING BIAS-VARIANCE TRADEOFF VIA TRAJECTORY WEIGHTING\n\nTo formalize our discussion, recall that rτ denotes the return of a trajectory τ and let r‹ “ supτ rτ be the maximum expert return, which is assumed to be known in prior works on conditional BC (Chen et al., 2021; Emmons et al., 2021). We know that the optimal offline data distribution, denoted by T ‹, is simply the distribution of demonstrations rolled out from the optimal policy. Typically, the offline trajectory distribution T will be biased w.r.t. T ‹. During learning, this leads to a bias-variance tradeoff, wherein ideally we want to learn our BC agent to condition on the expert returns, but is forced to minimize the empirical risk on a biased data distribution. The core idea of our approach is to transform T into a new distribution rT that better estimates T ‹. More concretely, rT should concentrate on high-return trajectories, which mitigates the bias. One naive strategy is to simply filter out a small fraction of high return trajectories from the offline dataset. However, since we expect the original dataset to contain very few high return trajectories, filtering trajectories will increase the variance for downstream BC. To balance the bias-variance trade-off, we propose to weight the trajectories based on their returns. Let fT : R ÞÑ R` be the density function of rτ where τ „ T . We consider the transformed distribution rT whose density function p rT is\n\n4\n\n20020406080100normalized return r0.00.10.20.30.40.5densitysuboptimal data distributionexpert distribution20020406080100normalized return r0.000.020.040.060.08densityoriginal transformed rUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Weighted Trajectory Sampling\n\nř\n\n1 Input: offline dataset Toffline, number of bins B, smoothing parameters λ, κ 2 Compute the returns: rτ Ð 3 Group the trajectories into B equal-sized bins according to rτ . 4 Sample a bin b P rBs with probability Pbinpbq defined in Equation (6). 5 Sample a trajectory τ in bin b uniformly at random. 6 Output: τ\n\nt“1 rt, @τ P Toffline.\n\n|τ |\n\nTable 1: The normalized return on D4RL locomotion tasks of RvS and DT with trajectory weighting. We use +W as shorthand for weighting. We use #wins to denote the number of datasets where the variant outperforms the original model. The results are averaged over 10 seeds.\n\nwalker2d-medium walker2d-med-replay walker2d-med-expert\n\nhopper-medium hopper-med-replay hopper-med-expert\n\nhalfcheetah-medium halfcheetah-med-replay halfcheetah-med-expert\n\n# wins average\n\nDT\n\nDT+W\n\nRvS\n\nRvS+W\n\n71.5 ̆ 3.9 53.4 ̆ 12.2 99.8 ̆ 21.3\n\n59.9 ̆ 4.9 56.4 ̆ 20.1 95.4 ̆ 11.3\n\n42.5 ̆ 0.6 34.5 ̆ 4.2 87.2 ̆ 2.7\n\n70.4 ̆ 4.5 60.5 ̆ 8.9 108.2 ̆ 0.8\n\n63.9 ̆ 4.4 76.9 ̆ 5.9 103.4 ̆ 9.0\n\n41.6 ̆ 1.7 36.9 ̆ 2.2 85.6 ̆ 2.0\n\n/ 66.7\n\n6 71.9\n\n73.3 ̆ 5.7 54.0 ̆ 12.1 102.2 ̆ 104.1\n\n54.5 ̆ 7.7 61.2 ̆ 14.7 104.1 ̆ 0.5\n\n56.6 ̆ 5.5 87.7 ̆ 9.7 108.8 ̆ 0.9\n\n16.2 ̆ 4.5 ́0.4 ̆ 2.7 83.4 ̆ 2.1\n\n/ 64.6\n\n62.5 ̆ 7.1 92.4 ̆ 6.1 108.4 ̆ 1.8\n\n4.0 ̆ 5.4 ́0.8 ̆ 2.2 69.1 ̆ 3.7\n\n4 61.7\n\nhkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkj trajectory weight ̆\n\n`\n\nfT prτ q\n\n ́ |rτ ́r‹|\n\np rT pτ q 9\n\nfT prτ q`λ ̈ exp\n\n(5) where λ, κ P R` are two hyperparameters. A larger value of κ leads to a more uniform rT , whereas a smaller value upweights the high-return trajectories. In contrast, a smaller value of λ gives more weights to high-return trajectories, while a larger value makes rT closer to T . Our trajectory weighting is motivated by a similar scheme proposed for model-based optimization (Kumar & Levine, 2020), where the authors use it to balance the bias and variance for gradient approximation for surrogates to black-box functions, and theoretically establish the optimality of the proposed distribution.\n\nκ\n\n,\n\n4.1.1\n\nIMPLEMENTATION DETAILS\n\nIn practice, the dataset Toffline only contains a finite number of samples and the density function p rT in equation (5) cannot be computed exactly. Following Kumar & Levine (2020), we sample from a discretized approximation of rT . We first group the trajectories in Toffline into B equal-sized bins according to the return rτ . To sample a trajectory, we first sample a bin index b P t1, . . . , Bu and then uniformly sample a trajectory inside bin b. We use |b| to denote the size of bin b. Let srb τ Pb rτ the average return of the trajectories in bin b, pr‹ be the highest return in the τ “ 1{|b| dataset Toffline, and define fTofflinepbq “ |b|{|Toffline|. As a discretized version of equation (5), the bins are weighted by their average returns with probability\n\nř\n\nPbinpbq 9\n\nfT\n\noffline pbq offline pbq`λ ̈ exp\n\nfT\n\n`\n\n ̆\n\n ́ |srb\n\nτ ́pr‹| κ\n\n.\n\n(6)\n\nAlgorithm 1 summarizes the data sampling procedure when trajectory weighting is used. Figure 1b illustrates the impact of trajectory weighting on the return distribution of the med-replay dataset for the walker2d environment. We plot the histograms before and after transformation, where the density curves are estimated by kernel density estimators.\n\n4.1.2 EMPIRICAL RESULTS\n\nDataset We evaluate the effectiveness of trajectory weighting on three locomotion tasks with dense rewards from the D4RL benchmark (Fu et al., 2020): hopper, walker2d and halfcheetah.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Performance of RvS and DT when conditioning on different evaluation RTGs. We report the mean and standard deviation of 10 seeds.\n\nFor each task, we consider the v2 medium, med-replay and med-expert offline datasets. The medium dataset contains 1M samples from a policy trained to approximately 1 3 the performance of an expert policy. The med-replay dataset uses the replay buffer of a policy trained up to the performance of a medium policy. The med-expert dataset contains 1M samples generated by a medium policy and 1M samples generated by an expert policy.\n\nBaselines We apply trajectory weighting to RvS (Emmons et al., 2021) and DT (Chen et al., 2021), two state-of-the-art BC methods. We compare their performance when trained on the original distribution and on the transformed distribution induced by our trajectory weighting (denoted as +W).\n\nHyperparameters For all datasets, we use B “ 20 and λ “ 0.01, and we set the temperature parameter κ to be the difference between the highest return and the 90-th percentile: pr‹ ́ pr90, whose value varies across the datasets. At test time, we set the evaluation RTG to be the expert return for each environment. The model architecture and the other hyperparamters are identical to what were used in the original paper. We provide a complete list of hyperparameters in Appendix B.2 and additional ablation experiments on λ and κ in Appendix C.\n\nResults Table 1 shows the performance of RvS and DT and their variants. DT+W outperforms the original DT in 6{9 datasets, achieving an average improvement of 8%. The improvement is significant in low-quality datasets (med-replay), which agrees with our analysis. Unlike in DT, trajectory weighting in RvS has varying effects, and the average performance of RvS+W is not better than that of RvS. To better understand this, we plot the achieved returns of RvS and DT when conditioning on different values of RTG. Figure 2 shows an interesting difference between behaviors of DT and RvS. DT is insensitive to the conditioning RTG, and continues performing stably even when conditioning on out-of-distribution RTGs. In contrast, the performance of RvS highly correlates with the evaluation RTG, but degrades quickly after a certain threshold. The performance crash problem of RvS shadows the improvement made by trajectory weighting.\n\n4.2 RELIABLE EVALUATION VIA CONSERVATISM\n\nThe results in Section 4.1.2 introduce another challenging problem for return-conditioned BC in offline RL: generalization to out-of-distribution (ood) returns. While strong generalization beyond the offline dataset remains an ongoing challenge for the offline RL community (Wang et al., 2020; Zanette, 2021; Foster et al., 2021), we require the policy to be reliable and at least stay close to the data distribution to avoid catastrophic failure when conditioned on ood returns. In other words, we want the policy to be conservative. Figure 2 shows that DT enjoys self-conservatism, while RvS does\n\n6\n\n20406080100120140160180020406080100120Returnwalker2d-medium20406080100120140160180020406080100120walker2d-medium-replay20406080100120140160180020406080100120walker2d-medium-expert406080100120140160180020406080100120Returnhopper-medium406080100120140160180020406080100120hopper-medium-replay406080100120140160180020406080100120hopper-medium-expert255075100125150175Eval RTG020406080100120Returnhalfcheetah-medium255075100125150175Eval RTG020406080100120halfcheetah-medium-replay255075100125150175Eval RTG020406080100120halfcheetah-medium-expertDTDT+WRvSRvS+Wmax return in offline dataexpert returnUnder review as a conference paper at ICLR 2023\n\nFigure 3: Performance of DT when the state and RTG tokens are concatenated. We report the mean and standard deviation of 10 seeds.\n\nAlgorithm 2: ConserWeightive Behavioral Cloning (CWBC) for RvS\n\n1 Input: dataset Toffline, number of iterations I, batch size S, regularization coefficient α, initial\n\nparameters θ0\n\n2 for iteration i “ 1, . . . , I do\n\n3\n\n4\n\n5\n\n6\n\n7\n\nSample a batch of trajectories B Ð tτ p1q, . . . , τ pSqu from Toffline using Algorithm 1. for every sampled trajectory τ piq do\n\nSamplie noise ε as described in Section 4.2.1. Compute noisy RTGs: gε\n\nt Ð gt ` ε, 1 ď t ď |τ piq|.\n\n// loss and regularizer defined in Equation (4) and (7) Perform gradient update of θ by minimizing the regularized empirical risk pLB\n\nRvSpθq ` α ̈\n\npCB RvSpθq.\n\n8 Output: πθ\n\nnot. We hypothesize that the conservative behavior of DT comes from the transformer architecture. As the policy conditions on a sequence of both state tokens and RTG tokens to predict next action, the attention layers can choose to ignore the ood RTG tokens while still obtaining a good prediction loss. To test this hypothesis, we experiment with a slightly modified version of DT, where we concatenate the state and RTG at each timestep instead of treating them as separate tokens. By doing this, the model cannot ignore the RTG information in the sequence. We call this version DT-Concat. Figure 3 shows that the performance of DT-Concat is strongly correlated with the conditioning RTG, and degrades quickly when the target return is out-of-distribution. This result confirms our hypothesis.\n\nHowever, conservatism does not have to come from the architecture, but can also emerge from a proper objective function, as commonly done in conservative value-based methods (Kumar et al., 2020; Fujimoto & Gu, 2021). In this section, we propose a novel conservative regularization for BC that explicitly encourages the policy to stay close to the data distribution. The intuition is to enforce the predicted actions when conditioning on large ood returns to stay close to the in-distribution actions. To do that, for a trajectory τ with high return, we inject positive random noise ε „ Eτ to its RTGs, and penalize the l2 distance between the predicted action and the ground truth. Specifically, to guarantee we generate large ood returns, we choose a noise distribution E such that the perturbed initial RTG g1 ` ε is at least pr‹, the highest return in the dataset. The next subsections instantiate the conservative regularizer in the context of RvS, and empirically evaluate its performance.\n\n4.2.1\n\nIMPLEMENTATION DETAILS\n\nWe apply conservative regularization to trajectories whose returns are above prq, the q-th percentile of returns in the dataset. This makes sure that when conditioned on ood returns, the policy behaves similarly to high-return trajectories and not to a random trajectory in the dataset. We sample a scalar noise ε „ Eτ and offset the RTG of τ at every timestep by ε: gε t “ gt ` ε, t “ 1, . . . , |τ |, resulting in the conservative regularizer:\n\nCRvSpθq “ Eτ „T , ε„Eτ\n\n1rτ ąprq ̈ 1\n\n|τ |\n\n“\n\nř\n\n`\n\n|τ | t“1\n\nat ́ πθpst, ωε t q\n\n ̆\n\n2\n\n‰ ,\n\n(7)\n\nwhere ωε t “ pgt `εq{pH ́t`1q (cf. Equation (3)) is the noisy average RTG at timestep t. We observe that using the 95-th percentile of pr95 generally works well across different environments and datasets.\n\n7\n\n20406080100120140160180020406080100120Returnwalker2d-medium-replay406080100120140160180020406080100120Returnhopper-medium-replay255075100125150175Eval RTG020406080100120Returnhalfcheetah-medium-replayDTDT-Concatmax return in offline dataexpert returnUnder review as a conference paper at ICLR 2023\n\nTable 2: Comparison of the normalized return on the D4RL locomotion benchmark. For BC and TD3+BC, we get the numbers from (Emmons et al., 2021). For IQL, we get the numbers from (Kostrikov et al., 2021b). For TTO, we get the numbers from (Janner et al., 2021). The results are averaged over 10 seeds.\n\nRvS\n\nRvS+W\n\nRvS+C\n\nRvS+W+C\n\nDT\n\nBC\n\nTD3+BC CQL\n\nIQL\n\nTTO\n\nwalker2d-medium walker2d-med-replay walker2d-med-expert\n\nhopper-medium hopper-med-replay hopper-med-expert\n\n73.3 ̆ 5.7 54.0 ̆ 12.1 102.2 ̆ 2.3\n\n56.6 ̆ 5.5 87.7 ̆ 9.7 108.8 ̆ 0.9\n\n54.5 ̆ 7.7 61.2 ̆ 14.7 104.1 ̆ 0.5\n\n62.5 ̆ 7.1 92.4 ̆ 6.1 108.4 ̆ 1.8\n\nhalfcheetah-medium halfcheetah-med-replay ́0.4 ̆ 2.7 ́0.8 ̆ 2.2 halfcheetah-med-expert 69.1 ̆ 3.7\n\n16.2 ̆ 4.5\n\n83.4 ̆ 2.1\n\n4.0 ̆ 5.4\n\n71.3 ̆ 4.9 62.0 ̆ 13.5 102.1 ̆ 10.2\n\n61.0 ̆ 5.3 91.5 ̆ 3.5 101.0 ̆ 13.4\n\n40.7 ̆ 1.0 36.8 ̆ 1.5 91.2 ̆ 1.0\n\n73.6 ̆ 5.4 72.8 ̆ 7.5 107.6 ̆ 0.5\n\n62.9 ̆ 3.6 87.7 ̆ 4.2 110.0 ̆ 2.8\n\n42.2 ̆ 0.7 40.4 ̆ 0.8 91.1 ̆ 2.0\n\n# wins average\n\n/ 64.6\n\n4 61.7\n\n6 73.1\n\n9 76.5\n\n71.5 ̆ 3.9 53.4 ̆ 12.2 99.8 ̆ 21.3\n\n59.9 ̆ 4.9 56.4 ̆ 20.1 95.4 ̆ 11.3\n\n42.5 ̆ 0.6 34.5 ̆ 4.2 87.2 ̆ 2.7\n\n/ 66.73\n\n75.3 26.0 107.5\n\n83.7 81.8 110.1\n\n52.9 18.1 52.5\n\n42.6 36.6 55.2\n\n/ 51.86\n\n59.3 60.9 98.0\n\n48.3 44.6 90.7\n\n/ 75.3\n\n82.9 86.1 109.5\n\n64.6 97.8 102.0\n\n49.1 47.3 85.8\n\n/ 80.6\n\n78.3 73.9 109.6\n\n66.3 94.7 91.5\n\n47.4 44.2 86.7\n\n/ 77.0\n\n81.3 ̆ 8.0 79.4 ̆ 12.8 91.0 ̆ 10.8\n\n67.4 ̆ 11.3 99.4 ̆ 12.6 106.0 ̆ 1.1\n\n44.0 ̆ 1.2 44.1 ̆ 3.5 40.8 ̆ 8.7\n\n72.6\n\nWe use the noise distribution Eτ “ Uniformrlτ , uτ s, where the lower bound lτ “ pr‹ ́ rτ so that the perturbed initial RTG gε 12σ2 so that the standard deviation of Eτ is equal to σ. We emphasize our conservative regularizer is distinct from the other conservative components proposed for the value-based offline RL methods. While those usually try to regularize the value function estimation to prevent extrapolation error (Fujimoto et al., 2019), we perturb the returns to generate ood conditioning and regularize the predicted actions.\n\n1 “ rτ ` ε is no less than pr‹, and the upper bound uτ “ pr‹ ́ rτ `\n\n?\n\nWhen the conservative regularizer is used, the final objective for training RvS is LRvSpθq ` α ̈ CRvSpθq, in which α is the regularization coefficient. When trajectory reweighting is used in conjunction with the conservative regularizer, we obtain ConserWeightive Behavioral Cloning (CWBC), which combines the best of both components. We provide a pseudo code for CWBC in Algorithm 2.\n\n4.2.2 EMPIRICAL RESULTS\n\nDataset We evaluate the effectiveness of the conservative regularizer, as well as the performance of CWBC as a whole on the D4RL datasets (Fu et al., 2020) for the gym locomotion tasks.\n\nBaselines We apply the conservative regularizer, which we denote as +C, to both RvS and RvS+W. In addition, we report the performance of three value-based methods: TD3+BC (Fujimoto & Gu, 2021), CQL (Kumar et al., 2020), and IQL (Kostrikov et al., 2021b) as a reference.\n\nHyperparameters We apply our conservatism regularization to trajectories whose returns are above the q “ 95-th percentile return in the dataset, and perturb their RTGs as described in Section 4.2.1. We use a regularization coefficient of α “ 1. The evaluation protocol is similar to Section 4.1.2.\n\nResults Table 2 reports the performance of different methods we consider. Our proposed framework CWBC with all components enabled (RvS+W+C) significantly outperforms the original RvS on 9{9 datasets, with an average improvement of 18% over RvS. RvS+W+C is also the best performing BC method in the table, and is competitive with the value-based methods. Conservative regularization consistently improves the results for both RvS and RvS+W. Trajectory weighting on its own can have varying effects on performance, but is synergistic when combined with RvS+C leading to our best performing model in RvS+W+C.\n\nTo better understand the impact of each component, we plot the achieved returns of RvS and other variants when conditioning on different values of conditioned RTG. Figure 4 shows that RvS generalizes poorly to out-of-distribution RTGs, which leads to significant performance drop when the evaluation RTG is larger than the best return in the dataset. Figure 4 illustrates the significant importance of encouraging conservatism for RvS, where RvS+C has much more stable performance, even when the evaluation RTG is 2ˆ the expert return. By explicitly asking the model to stay close to the data distribution, we achieve more reliable out-of-distribution performance, and avoid the performance crash problem. This leads to absolute performance improvement of RvS+C in Table 2. CWBC combines the best of both weighting and conservatism, which enjoys good performance when conditioning on high RTG values, as well as better robustness to large, out-of-distribution RTGs.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Performance of RvS and its variants when conditioning on different evaluation RTGs. We report the mean and standard deviation of 10 seeds.\n\nIn addition to the main results, we include ablations for different choices of conservative percentile q and regularization coefficient α in Appendix C. Finally, we also evaluate CWBC in two more benchmarks: Atari games (Bellemare et al., 2013) and the D4RL Antmaze datasets. We present these results in Appendix D and E respectively.\n\n5 CONCLUSION\n\nWe proposed ConserWeightive Behavioral Cloning (CWBC), a new framework that extends BC for offline RL with two novel components: trajectory weighting and conservative regularization. Trajectory weighting balances the bias-variance tradeoff that arises in learning from a suboptimal dataset, improving the performance of both DT and RvS. Next, we showed that while DT is selfconservative due to its attention architecture, we can recover this desired behavior even for RvS using our proposed conservative regularizer. Confirmed by the experiments, CWBC significantly improves the performance and stability of RvS.\n\nWhile we made good progress for BC, advanced value-based methods such as CQL and IQL are still ahead and we believe further understanding of the tradeoffs in both kinds of approaches is important future work. Another promising direction from a data perspective is how to combine datasets from multiple environments to obtain diverse, high-quality data. Recent works have shown promising results in this direction (Reed et al., 2022). Last but not least, while CWBC significantly improves the performance and reliability of RvS, it is not able to extrapolate beyond the offline dataset. How to obtain extrapolation, or whether it is possible, is still an open question, and poses a persistent research opportunity for not only CWBC but the whole offline RL community.\n\nREPRODUCIBILITY STATEMENT\n\nWe present the practical implementation of our framework in Section 4.1.1 and Section 4. We include the implementation details of our paper in Appendix B, which contains information about the datasets we use, the open sourced code we base on, and the list of hyperparameters we use to reproduce our results. Finally, we submitted the source code in the supplementary material.\n\n9\n\n255075100125150175200020406080100120Returnwalker2d-medium255075100125150175200020406080100120walker2d-medium-replay255075100125150175200020406080100120walker2d-medium-expert255075100125150175200020406080100120Returnhopper-medium255075100125150175200020406080100120hopper-medium-replay255075100125150175200020406080100120hopper-medium-expert255075100125150175200Eval RTG020406080100120Returnhalfcheetah-medium255075100125150175200Eval RTG020406080100120halfcheetah-medium-replay255075100125150175200Eval RTG020406080100120halfcheetah-medium-expertRvSRvS+WRvS+CRvS+W+Cmax return in offline dataexpert returnUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pp. 104–114. PMLR, 2020.\n\nMichael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelligence\n\n15, pp. 103–129, 1995.\n\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253–279, 2013.\n\nRichard Bellman. A markovian decision process. Indiana Univ. Math. J., 1957.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 2021.\n\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, pp. 1691– 1703. PMLR, 2020.\n\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for\n\noffline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.\n\nDylan J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu. Offline reinforcement learning: Fundamental barriers for value function approximation. arXiv preprint arXiv:2111.10919, 2021.\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\n\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n\nScott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.\n\nAdvances in Neural Information Processing Systems, 34, 2021.\n\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019.\n\nSeyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max q-learning operator for simple yet effective offline and online rl. In International Conference on Machine Learning, pp. 3682–3691. PMLR, 2021.\n\nDibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. arXiv preprint arXiv:1912.06088, 2019.\n\nWenshuo Guo, Kumar Krishna Agrawal, Aditya Grover, Vidya Muthukumar, and Ashwin Pananjady. Learning from an exploring demonstrator: Optimal reward estimation for bandits. arXiv preprint arXiv:2106.14866, 2021.\n\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\n\nmodeling problem. Advances in neural information processing systems, 34, 2021.\n\nNatasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. Advances in neural information processing systems, 33: 21810–21823, 2020.\n\nIlya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In International Conference on Machine Learning, pp. 5774–5783. PMLR, 2021a.\n\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\n\nq-learning. arXiv preprint arXiv:2110.06169, 2021b.\n\nSiddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. Generative pretraining for\n\nblack-box optimization. arXiv preprint arXiv:2206.10786, 2022.\n\nAviral Kumar and Sergey Levine. Model inversion networks for model-based optimization. Advances\n\nin Neural Information Processing Systems, 33:5126–5137, 2020.\n\nAviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.\n\nNathan Lambert, Markus Wulfmeier, William Whitney, Arunkumar Byravan, Michael Bloesch, Vibhavari Dasagi, Tim Hertweck, and Martin Riedmiller. The challenges of exploration for offline reinforcement learning. arXiv preprint arXiv:2201.11861, 2022.\n\nSascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement\n\nlearning, pp. 45–73. Springer, 2012.\n\nMichael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark. arXiv preprint arXiv:2110.15191, 2021.\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,\n\nreview, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal computation engines. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n\nOfir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:\n\nPolicy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.\n\nTung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning\n\nvia sequence modeling. arXiv preprint arXiv:2207.04179, 2022.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\n\nstanding by generative pre-training. 2018.\n\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.\n\nJuergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards–just map them to\n\nactions. arXiv preprint arXiv:1912.02875, 2019.\n\nRupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja ́skowski, and Jürgen Schmidhuber. Training agents using upside-down reinforcement learning. arXiv preprint arXiv:1912.02877, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nRuosong Wang, Dean P Foster, and Sham M Kakade. What are the statistical limits of offline rl with\n\nlinear function approximation? arXiv preprint arXiv:2010.11895, 2020.\n\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.\n\narXiv preprint arXiv:1911.11361, 2019.\n\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using\n\nvq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.\n\nDenis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, Pieter Abbeel, Alessandro Lazaric, and Lerrel Pinto. Don’t change the algorithm, change the data: Exploratory data for offline reinforcement learning. arXiv preprint arXiv:2201.13425, 2022.\n\nTianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129–14142, 2020.\n\nAndrea Zanette. Exponential lower bounds for batch reinforcement learning: Batch rl can be exponentially harder than online rl. In International Conference on Machine Learning, pp. 12287– 12297. PMLR, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA LIST OF SYMBOLS\n\nTable 3: Important symbols used in this paper.\n\nSymbol\n\nMeaning\n\nDefinition\n\nS A\nτ |τ | T\nToffline π\nθ st at rt rτ gt H\nωt LRvS CRvS fT pτ q p rT pτ q b\n|b| fToffline pbq Pbinpbq ̄rb pr‹ prq\n\nτ\n\nstate space action space trajectory trajectory length distribution of trajectories offline dataset policy policy parameters state at timestep t action at timestep t reward at timestep t trajectory return return-to-go at timestep t maximum trajectory length average return-to-go at timestep t empirical risk of RvS conservative regularizer for RvS probability density of trajectory τ „ T probability density of trajectory τ „ rT index of a bin of trajectories in the offline dataset size of bin b proportion of trajectories in bin b probability that bin b is sampled average return of trajectories in bin b highest return in the offline dataset q-th percentile of the returns in the offline dataset\n\nř ř\n\n|τ |\n\nt“1 rt t1“t r1\n\n|τ |\n\nt\n\ngt{pH ́ t ` 1q Equation (4) Equation (7)\n\nEquation (5)\n\n|b|{|Toffline| Equation (6)\n\nB IMPLEMENTATION DETAILS\n\nB.1 DATASETS AND SOURCE CODE\n\nWe train and evaluate our models on the D4RL (Fu et al., 2020) and Atari (Agarwal et al., 2020) benchmarks, which are available at https://github.com/rail-berkeley/d4rl and https://research.google/tools/datasets/dqn-replay, respectively. Our codebase is largely based on the RvS (Emmons et al., 2021) official implementation at https: //github.com/scottemmons/rvs, and DT (Chen et al., 2021) official implementation at https://github.com/kzl/decision-transformer.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB.2 DEFAULT HYPERPARAMETERS\n\nTable 4: Hyperparameters used for locomotion experiments.\n\nModel\n\nHyperparameter\n\nValue\n\nContext length K (DT) Number of attention heads (DT) Hidden layers Hidden dimension Activation function Dropout\n\n20 1\n2 for RvS, 3 for DT 1024 for RvS, 128 for DT ReLU 0.0 for RvS, 0.1 for DT\n\nConservative regularizer\n\nTrajectory weighting\n\nConservative percentile q Noise standard deviation σ Regularization coefficient α\n\n# bins B Smoothing parameter λ Smoothing parameter κ\n\n95 1000 1.0\n\n20 0.01 pr‹ ́ pr90\n\nOptimization\n\nBatch size Learning rate Weight decay Training iterations\n\n64 1e ́ 3 for RvS, 1e ́ 4 for DT 1e ́ 4 100000\n\nEvaluation\n\nTarget return\n\n1ˆ Expert return\n\nTable 5: Hyperparameters used for Atari experiments.\n\nHyperparameter\n\nValue\n\nModel\n\nEncoder channels Encoder filter sizes Encoder strides Hidden layers Hidden dimension Activation function Dropout\n\nConservative percentile q\n\nConservative regularization\n\nNoise std σ\n\nTrajectory weighting\n\nOptimization\n\nConservative weight α\n\n# bins B λ\nκ\n\nBatch size Learning rate Weight decay Training iterations\n\nEvaluation\n\nTarget return\n\n32, 32, 64 8 ˆ 8, 4 ˆ 4, 3 ˆ 3 4, 2, 1 4\n1024 ReLU 0.1\n\n95 50 for Breakout, Pong 500 for Qbert, Seaquest 0.1\n\n20 0.1 pr‹ ́ pr50\n\n128 6e ́ 4 1e ́ 4 25000\n\n90 for Breakout (1ˆ max in dataset) 2500 for Qbert (5ˆ max in dataset) 20 for Pong (1ˆ max in dataset) 1450 for Seaquest (5ˆ max in dataset)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nC ABLATION ANALYSIS\n\nIn this section, we investigate the impact of each of those hyperparameters on CWBC to give insights on what values work well in practice. We use the walker2d environment and the three related datasets for illustration. In all the experiments, when we vary one hyperparameter, the other hyperparameters are kept as in Table 4.\n\nC.1 TRAJECTORY WEIGHTING: SMOOTHING PARAMETERS λ AND κ\n\nTwo hyperparameters κ and λ in Equation (6) affect the probability a bin index b is sampled:\n\nPbinpbq 9\n\nfT\n\noffline pbq offline pbq`λ ̈ exp\n\nfT\n\n`\n\n ́ |srb\n\nτ ́pr‹| κ\n\n ̆\n\n.\n\nIn practice, we have observed that the performance of CWBC is considerably robust to a wide range of values of κ and λ.\n\nThe impact of κ The smoothing parameter κ controls how we weight the trajectories based on their relative returns. Intuitively, smaller κ gives more weights to high-return bins (and thus their trajectories), and larger κ makes the transformed distribution more uniform. We illustrate the effect of κ on the transformed distribution and the performance of CWBC in Figure 5. As in Section 4.1.2, we set κ to be the difference between the empirical highest return pr‹ and the z-th percentile return in the dataset: κ “ pr‹ ́ prz, and we vary the values of z. This allows the actual value of κ to adapt to different datasets.\n\nFigure 5 shows the results. The top row plots the distributions of returns before and after trajectory weighting for varying values of κ. We tested four values z P t99, 90, 50, 0u, which correspond to four increasing values of κ. We mark the actual values of κ in each dataset in the top tow1. For each dataset, we can see the transformed distribution using small κ (orange) highly concentrates on high returns; as κ increases, the density for low returns increases and the distribution becomes more and more uniform. The bottom row plots the corresponding performance of CWBC with different choices of κ. We select RvS+C as our baseline model, which does not have trajectory weighting but has the conservative regularization enabled. We can see that relatively small values of κ (based on pr99 , pr90 and pr50) perform well on all the three datasets, whereas large values (based on pr0) hurt the performance for the med-expert dataset, and even underperform the baseline RvS+C.\n\nFigure 5: The influence of κ on the transformed distribution (top) and on the performance of CWBC (bottom). The legend in each panel (top) shows the absolute values of κ for easier comparison. In the bottom row, we also plot the results of RvS+C (no trajectory weighting) as a baseline.\n\n1pr0 is defined to be the lowest return in the dataset: pr0 “ minτ PToffline rτ .\n\n15\n\n020406080100Normalized return r0.000.010.020.030.040.05Densitywalker2d-medium-replayoriginal =9.7=47.4=82.5=91.1020406080100Normalized return r0.000.020.040.060.080.100.12walker2d-mediumoriginal =6.2=9.5=15.8=92.2020406080100Normalized return r0.00.10.20.30.40.5walker2d-medium-expertoriginal =0.6=1.3=26.3=109.350100150200Eval RTG020406080Returnwalker2d-medium-replay50100150200Eval RTG4050607080walker2d-medium50100150200Eval RTG406080100120walker2d-medium-expertRvS+C=rr99=rr90=rr50=rr0Under review as a conference paper at ICLR 2023\n\nThe impact of λ To better understand the role of λ, we can rewrite Equation (6) as\n\nhkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkj T1 ̆\n`\n\nPbinpbq 9\n\nfToffline pbq exp\n\n ́ |srb\n\nτ ́pr‹| κ\n\nhkkkkkkkkkkkikkkkkkkkkkkj T2 ̆( ␣\n\n`\n\n ̈\n\n1{\n\nfToffline pbq ` λ\n\n.\n\nClearly, only T2 depends on λ. When λ “ 0, T2 is canceled out and the above equation reduces to\n\nPbinpbq 9 exp\n\n`\n\n ́ |srb\n\nτ ́pr‹| κ\n\n ̆\n\n,\n\nwhich purely depends on the relative return. As λ increases, T2 is less sensitive to fToffline pbq, and finally becomes the same for every b P rBs as λ Ñ 8. In that scenario, Pbinpbq only depends on T1, which is the original frequency fToffline pbq weighted by the relative return.\n\nThe top row of Figure 6 plots the distributions of returns before and after trajectory weighting with different values of λ. When λ “ 0, the distributions concentrate on high returns. As λ increases, the distributions are more correlated with the original one, but still weights more on the high-return region compared to the original distribution due to the exponential term in T1. The bottom row of Figure 6 plots the actual performance of CWBC as λ varies. All values of λ produce similar results, which are consistently better than or comparable to training on the original datset (RvS+C).\n\nFigure 6: The influence of λ on the transformed distribution (top) and on the performance of CWBC (bottom). We plot the result of RvS+C as the baseline.\n\nC.2 CONSERVATIVE REGULARIZATION: PERCENTILE q\n\nWe only apply the conservative regularization to trajectories whose return is above the q-th percentile of the returns in the dataset. Intuitively, a larger q applies the regularization to fewer trajectories. We test four values for q: 0, 50, 95, and 99. For q “ 0, our regularization applies to all the trajectories in the dataset. Figure 7 demonstrates the impact of q on the performance of CWBC. q “ 95 and q “ 99 perform well on all the three datasets, while q “ 50 and q “ 0 lead to poor results for the med-replay dataset. This is because, when the regularization applies to trajectories of low returns, the regularizer will force the policy conditioned on out-of-distribution RTGs to stay close to the actions from low return trajectories. Since the med-replay dataset contains many low return trajectories (see Figure 5), such regularization results in poor performance. In contrast, medium and med-expert datasets contain a much larger portion of high return trajectories, and they are less sensitive to the choice of q.\n\n16\n\n020406080100Normalized return r0.000.010.020.030.040.05Densitywalker2d-medium-replay020406080100Normalized return r0.0000.0250.0500.0750.1000.125walker2d-medium020406080100Normalized return r0.00.10.20.30.40.5walker2d-medium-expert50100150200Eval RTG20406080Returnwalker2d-medium-replay50100150200Eval RTG50607080walker2d-medium50100150200Eval RTG20406080100walker2d-medium-expertRvS+C=0.0=0.01=1.0=100.0Under review as a conference paper at ICLR 2023\n\nFigure 7: Performance of CWBC with different values of the conservative percentile q.\n\nC.3 REGULARIZATION COEFFICIENT α\n\nThe hyperparameter α controls the weight of the conservative regularization in the final objective function of CWBC LRvS ` α ̈ CRvS. We show the performance of CWBC with different values of α in Figure 8. Not using any regularization (α “ 0) suffers from the performance crash problem, while overly aggressive regularization (α “ 10) also hurts the performance. CWBC is robust to the other non-extreme values of α .\n\nFigure 8: Performance of CWBC with different values of α.\n\nD ADDITIONAL RESULTS ON ATARI GAMES\n\nIn addition to D4RL, we consider 4 games from the Atari benchmark (Bellemare et al., 2013): Breakout, Qbert, Pong, and Seaquest. Similar to (Chen et al., 2021), for each game, we train our method on 500000 transitions sampled from the DQN-replay dataset, which consists of 50 million transitions of an online DQN agent (Mnih et al., 2015). Due to the varying performance of the DQN agent in different games, the quality of the datasets also varies. While Breakout and Pong datasets are high-quality with many expert transitions, Qbert and Seaquest datasets are highly suboptimal. Hyperparameters For trajectory weighting, we use B “ 20 bins, λ “ 0.1, and κ “ pr‹ ́ pr50. We apply conservative regularization with coefficient α “ 0.1 to trajectories whose returns are above pr95. The standard deviation of the noise distribution varies across datasets, as each different games have very different return ranges. During evaluation, we set the target return to 5 ˆ pr‹ for Qbert and Seaquest, and to 1 ˆ pr‹ for Breakout and Pong.\n\nFigure 9: Performance of RvS and its variants on Atari games when conditioning on different evaluation RTGs.\n\n17\n\n50100150200Eval RTG20406080Returnwalker2d-medium-replay50100150200Eval RTG50607080walker2d-medium50100150200Eval RTG20406080100walker2d-medium-expertq=0q=50q=95q=9950100150200Eval RTG020406080Returnwalker2d-medium-replay50100150200Eval RTG20406080walker2d-medium50100150200Eval RTG020406080100120walker2d-medium-expert=0=0.1=0.5=1.0=10.0050100150200250300350Eval RTG050100150200250300ReturnBreakout0.02.55.07.510.012.515.017.5Eval RTG0510152025Qbert020406080100Eval RTG20406080100Pong0.00.51.01.52.02.53.0Eval RTG0.00.51.01.52.0SeaquestRvSRvS+WRvS+CRvS+W+Cmax return in offline dataUnder review as a conference paper at ICLR 2023\n\nTable 6: Comparison of the normalized return on Atari games. The results are averaged over 3 seeds. We include the results of DT, CQL, and BC from (Chen et al., 2021) for reference.\n\nRvS\n\nRvS+W\n\nRvS+C\n\nRvS+W+C\n\nDT\n\nCQL\n\nBC\n\nBreakout Qbert Pong Seaquest\n\n# wins average\n\n126.9 ̆ 38.0 ́0.4 ̆ 0.2 75.7 ̆ 8.6 0.2 ̆ 0.2\n\n120.1 ̆ 28.43 0.0 ̆ 0.4 90.7 ̆ 6.4 ́0.1 ̆ 0.1\n\n163.0 ̆ 50.4 12.4 ̆ 8.6 84.1 ̆ 9.6 1.6 ̆ 0.2\n\n237.3 ̆ 82.1 19.1 ̆ 2.7 90.4 ̆ 1.9 1.4 ̆ 0.3\n\n267.5 ̆ 97.5 15.4 ̆ 11.4 106.1 ̆ 8.1 2.5 ̆ 0.4\n\n211.1 104.2 111.9 1.7\n\n138.9 ̆ 61.7 17.3 ̆ 14.7 85.2 ̆ 20.0 2.1 ̆ 0.3\n\n/ 50.6\n\n1 52.7\n\n4 62.3\n\n4 87.1\n\n97.9\n\n107.2\n\n60.9\n\nResults Table 6 summarizes the performance of RvS and its variants. CWBC (RvS+W+C) is the best method, outperforming the original RvS by 72% on average. Figure 9 clearly shows the effectiveness of the conservative regularization (+C). In two low-quality datasets Qbert and Seaquest, the performance of RvS degrades quickly when conditioning on out-of-distribution RTGs. By regularizing the policy to stay close to the data distribution, we achieve a much more stable performance. The trajectory weighting component (+W) alone has varying effects on performance because of the performance crash problem, but achieves state-of-the-art when used in conjunction with conservative regularization.\n\nIt is also worth noting that in both Qbert and Seaquest, CWBC achieves returns that are much higher than the best return in the offline dataset. This shows that while conservatism encourages the policy to stay close to the data distribution, it does not prohibit extrapolation. There is always a trade-off between optimizing the original supervised objective (which presumably allows extrapolation) and the conservative objective. This is very similar to other conservative regularizations used in value-based such as CQL or TD3+BC, where there is a trade-off between learning the value function and staying close to the data distribution.\n\nE ADDITIONAL RESULTS ON D4RL ANTMAZE\n\nOur proposed conservative regularization is especially important in dense reward environments such as gym locomotion tasks or Atari games, where choosing the target return during evaluation is a difficult problem. On the other hand, trajectory weighting is generally useful whenever the offline dataset contains both low-return and high-return trajectories. In this section, we consider Antmaze (Fu et al., 2020), a sparse reward environment in the D4RL benchmark to evaluate the generality of CWBC. Antmaze is a navigation domain in which the task is to control a complex 8-DoF \"Ant\" quadruped robot to reach a goal location. We consider 3 maze layouts: umaze, medium, and large, and 3 dataset flavors: v0, diverse, and play. We use the same set of hyperparameters as mentioned in B.2.\n\nTable 7: Comparison of the success rate on the Antmaze environment. The results are averaged over 3 seeds. We include the results of DT, CQL, and BC from (Emmons et al., 2021) for reference.\n\nRvS\n\nRvS+W\n\nRvS+C\n\nRvS+W+C\n\nDT\n\nCQL BC\n\numaze-v0 umaze-diverse\n\n54.0 ̆ 13.56 55.0 ̆ 15.65\n\n65.0 ̆ 18.03 46.0 ̆ 16.85\n\n58.0 ̆ 8.72 50.0 ̆ 10.95\n\n65.0 ̆ 12.85 42.0 ̆ 7.48\n\n65.6 51.2\n\n44.8 23.4\n\n54.6 45.6\n\nmedium-play medium-diverse\n\nlarge-play large-diverse\n\n# wins average\n\n0.0 ̆ 0.0 1.0 ̆ 3.0\n\n0.0 ̆ 0.0 0.0 ̆ 0.0\n\n/ 18.3\n\n26.0 ̆ 12.0 24.0 ̆ 15.62\n\n4.0 ̆ 4.9 10.0 ̆ 10.0\n\n4 29.2\n\n0.0 ̆ 0.0 1.0 ̆ 3.0\n\n0.0 ̆ 0.0 0.0 ̆ 0.0\n\n1 18.2\n\n25.0 ̆ 13.6 23.0 ̆ 11.0\n\n5.0 ̆ 6.71 17.0 ̆ 11.87\n\n1.0 0.6\n\n0.0 0.2\n\n0.0 0.0\n\n0.0 0.0\n\n0.0 0.0\n\n0.0 0.0\n\n4 29.5\n\n19.8\n\n11.4\n\n16.7\n\nResults Table 7 summarizes the results. As expected, the conservative regularization is not important in these tasks, as the target return is either 0 (fail) or 1 (success). However, the trajectory weighting significantly boosts performance, resulting in an average of 60% improvement over the original RvS.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nF TRAJECTORY WEIGHTING VERSUS HARD FILTERING\n\nAn alternative to trajectory weighting is hard filtering (+F), where we train the model on only top 10% trajectories with the highest returns. Filtering can be considered a hard weighting mechanism, wherein the transformed distribution only has support over trajectories with returns above a certain threshold.\n\nF.1 HARD FILTERING FOR RVS\n\nWhen using hard filtering for RvS, we also consider combining it with the conservative regularization. Table 8 and Figure 10 compare the performance of trajectory weighting and hard filtering when applied to RvS. While RvS+F+C also gains notable improvements , it lags behind RvS+W+C and seems to erode the benefits of conservatism alone in RvS+C. This agrees with our analysis in Section 4.1. While hard filtering achieves the same effect of reducing bias, it completely removes the low-return trajectories, resulting in highly increased variance. Our trajectory weighting upweights the good trajectories but aims to stay close to the original data distribution, balancing this bias-variance tradeoff. This is clearly shown in Figure 10, where RvS+W+C has much smaller variance when conditioning on large RTGs.\n\nTable 8: Comparison of trajectory weighting (+W) and hard filtering (+F) on D4RL locomotion benchmarks. The results are averaged over 10 seeds.\n\nRvS\n\nRvS+W\n\nRvS+C\n\nRvS+W+C\n\nRvS+F\n\nRvS+F+C\n\nwalker2d-medium walker2d-med-replay walker2d-med-expert\n\nhopper-medium hopper-med-replay hopper-med-expert\n\n73.3 ̆ 5.7 54.0 ̆ 12.1 102.2 ̆ 2.3\n\n56.6 ̆ 5.5 87.7 ̆ 9.7 108.8 ̆ 0.9\n\n54.5 ̆ 7.7 61.2 ̆ 14.7 104.1 ̆ 0.5\n\n62.5 ̆ 7.1 92.4 ̆ 6.1 108.4 ̆ 1.8\n\nhalfcheetah-medium halfcheetah-med-replay ́0.4 ̆ 2.7 ́0.8 ̆ 2.2 halfcheetah-med-expert 69.1 ̆ 3.7\n\n83.4 ̆ 2.1\n\n16.2 ̆ 4.5\n\n4.0 ̆ 5.4\n\n71.3 ̆ 4.9 62.0 ̆ 13.5 102.1 ̆ 10.2\n\n61.0 ̆ 5.3 91.5 ̆ 3.5 101.0 ̆ 13.4\n\n40.7 ̆ 1.0 36.8 ̆ 1.5 91.2 ̆ 1.0\n\n73.6 ̆ 5.4 72.8 ̆ 7.5 107.6 ̆ 0.5\n\n62.9 ̆ 3.6 87.7 ̆ 4.2 110.0 ̆ 2.8\n\n42.2 ̆ 0.7 40.4 ̆ 0.8 91.1 ̆ 2.0\n\n60.9 ̆ 4.9 47.1 ̆ 7.7 101.7 ̆ 3.3\n\n62.4 ̆ 5.0 91.2 ̆ 5.3 97.5 ̆ 15.0\n\n1.4 ̆ 3.3 ́0.1 ̆ 3.5 46.0 ̆ 1.5\n\n68.2 ̆ 7.1 53.9 ̆ 11.0 105.4 ̆ 0.6\n\n65.7 ̆ 6.4 92.1 ̆ 2.9 105.8 ̆ 3.5\n\n36.2 ̆ 2.5 35.7 ̆ 2.8 83.2 ̆ 5.0\n\n# wins average\n\n/ 64.6\n\n4 61.7\n\n6 73.1\n\n9 76.5\n\n3 56.5\n\n5 71.8\n\nFigure 10: Comparison of trajectory weighting and hard filtering.\n\n19\n\n255075100125150175200020406080100120Returnwalker2d-medium255075100125150175200020406080100120walker2d-medium-replay255075100125150175200020406080100120walker2d-medium-expert255075100125150175200020406080100120Returnhopper-medium255075100125150175200020406080100120hopper-medium-replay255075100125150175200020406080100120hopper-medium-expert255075100125150175200Eval RTG020406080100120Returnhalfcheetah-medium255075100125150175200Eval RTG020406080100120halfcheetah-medium-replay255075100125150175200Eval RTG020406080100120halfcheetah-medium-expertRvS+WRvS+W+CRvS+FRvS+F+Cmax return in offline dataexpert returnUnder review as a conference paper at ICLR 2023\n\nF.2 HARD FILTERING FOR UNCONDITIONAL BC\n\nHard filtering can also be applied to ordinary BC. This is equivalent to Filtered BC in (Emmons et al., 2021). Table 9 compares Filtered BC and CWBC. CWBC performs comparably well in medium and med-expert datasets, and outperforms Filtered BC significantly with an average improvement of 12% in med-replay datasets. We believe that in low-quality datasets, even when we filter out 90% percent of the data, the quality of the remaining trajectories is still very diverse that simple imitation learning is not good enough. CWBC is able to learn from such diverse data, and by conditioning on expert return at test time, we can recover an efficient policy.\n\nTable 9: The normalized return on D4RL for Filtered BC, RvS, and CWBC. For Filtered BC, we get the numbers from (Emmons et al., 2021).\n\nFiltered BC RvS\n\n73.3 ̆ 5.7 56.6 ̆ 5.5 16.2 ̆ 4.5\n\nRvS+W+C\n\n73.6 ̆ 5.4 62.9 ̆ 3.6 42.2 ̆ 0.7\n\n48.7\n\n59.6\n\n54.0 ̆ 12.1 87.7 ̆ 9.7 ́0.4 ̆ 2.7\n\n72.8 ̆ 7.5 87.7 ̆ 5.2 40.4 ̆ 0.8\n\n47.1\n\n67.0\n\n102.2 ̆ 2.3 108.8 ̆ 0.9 83.4 ̆ 2.1\n\n107.6 ̆ 0.5 110.0 ̆ 2.8 91.1 ̆ 2.0\n\n98.1\n\n64.6\n\n102.9\n\n76.5\n\nwalker2d-medium hopper-medium halfcheetah-medium\n\nmedium average\n\nwalker2d-med-replay hopper-med-replay halfcheetah-med-replay\n\nmed-replay average\n\nwalker2d-med-expert hopper-med-expert halfcheetah-med-expert\n\nmed-expert average\n\naverage\n\n75.0 56.9 42.5\n\n58.1\n\n62.5 75.9 40.6\n\n59.7\n\n109.0 110.9 92.9 ̆\n\n104.3\n\n74.0\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nG BIAS-VARIANCE TRADEOFF ANALYSIS\n\nWe formalize our discussion on the bias-variance tradeoff when learning from a suboptimal distribution mentioned in Section 4.1. The objective functions for training DT (2) and RvS (4) can be rewritten as:\n\nLpD pθq “ Eτ „T rDpτ, πθqs\n\nmin θ\n\n“ Er„pDprq,τ „Tr rDpτ, πθqs .\n\n(8)\n\n(9)\n\nIn which, pDprq is the data distribution over trajectory returns, Tr is a uniform distribution over the set of trajectories whose return is r, and Dpτ, πθq is the supervised loss function with respect to the |τ | sampled trajectory τ . For DT, Dpτ, πθq “ 1 , and for t“1 |τ | ̆\nř 2\nRvS, Dpτ, πθq “ 1 . Equation (9) is equivalent to first sampling a return r, then sampling a trajectory τ whose return is r, and calculating the loss on τ . Ideally, we want to train the model from an optimal return distribution p‹prq, which is centered around the expert return r‹:\n\nat ́ πθpgt ́K:t, st ́K:t, at ́K:t ́1q\n\nat ́ πθpst, ωtq\n\n|τ | t“1\n\nř\n\n|τ |\n\n`\n\n`\n\n ̆\n\n2\n\nmin θ\n\nLp‹ pθq “ Er„p‹prq,τ „Tr rDpτ, πθqs .\n\n(10)\n\nIn practice, we only have access to the suboptimal return distribution pDprq, which leads to a biased training objective with respect to p‹prq. While the dataset is fixed, we can transform the data distribution pDprq to qprq that better estimates the ideal distribution p‹prq. The objective function with respect to q is:\n\nmin θ\n\nLqpθq “ Er„qprq,τ „Tr rDpτ, πθqs\n\n„\n\n“ Er„pDprq,τ „Tr\n\nqprq pDprq\n\n ̈ Dpτ, πθq\n\nȷ\n\n(11)\n\n(12)\n\nIn the extreme case, qprq “ 1rr “ r‹s, which means we only train the policy on trajectories whose return matches the expert return r‹. However, since offline datasets often contain very few expert trajectories, this q leads to a very high-variance training objective. An optimal distribution q should lead to a training objective that balances the bias-variance tradeoff. We quantify this by measuring the l2 of the difference between the gradient of Lqpθq and the gradient of the optimal objective function Lp‹ pθq. Analogous to Kumar & Levine (2020), we can prove that for some constants C1, C2, C3, with high confidence:\n\n“ ||∇θLqpθq ́ ∇θLp‹pθq||2\n\n2\n\n‰\n\nE\n\nď C1 ̈ Er„qprq\n\n„\n\nȷ\n\n1 Nr\n\n` C2 ̈\n\nd2pq||pDq |D|\n\n` C3 ̈ DTVpp‹, qq2. (13)\n\nIn which, Nr is the number of trajectories in dataset D whose return is r, d2 is the exponentiated Renyi divergence, and DTV is the total variation divergence. The right hand side of inequality (13) shows that an optimal distribution q should be close to the data distribution pD to reduce variance, while approximating well p‹ to reduce bias. As shown in Kumar & Levine (2020), qprq9 Nr\n\nq minimizes this bound, which inspires our trajectory weighting.\n\nNr`K ̈ expp ́ |r ́r‹|\n\nκ\n\n21",
    "reference": "# Summary Of The Paper\n\nThe paper identifies the skewed distribution of returns in the offline dataset as a challenge to return-conditioned methods, and proposes to weight the trajectories in the dataset based on their returns.\n\n# Strength And Weaknesses\n\nStrengths\n\n1. The paper is relatively well-written and easy to follow.\n\n2. The proposed idea is intuitive, whose benefits are demonstrated on two different classes of return-conditioned methods (DT and RvS).\n\nWeaknesses\n\n1. The paper makes repeated reference to a \"bias-variance trade-off\" to motivate their methods. AFAIK, what this trade-off means exactly, and a theoretical or rigorous analyses as to why their method leads to a more favorable trade-off are missing completely.\n\n2. Skewed distribution of returns is a well-known problem in RL, and common technique to tackle this problem is to ensure each batch consists of equal amount of high return and low return episodes (so equivalently, use two bins only). I would have liked to see a comparison to such a simple baseline.\n\n3. My biggest concern is the motivation behind studying return-condition method? Because AFAIK, they under-performs value-based methods, especially on tasks that require stitching (such as antmaze navigation and notshown in the paper). Even in table 2, the performance of return-conditioned methods are under-whelming. I am not saying that the paper should not be accepted because it does not out-perform state-of-the-art, but there should be a reason as to why the class of return-conditioned method is interesting relative to value-based methods.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is very well written and easy to follow. \n\nThe proposed method is novel in the context of return-conditioned method, but I would have like to see a comparison to the simple baseline commonly used in value-based methods (mentioned previously).\n\nThe pseudo-code is included so I believe the algorithm is reproducible.\n\n# Summary Of The Review\n\nI recommend a weak reject, because it is unclear why we should be interested in return-conditioned methods, especially when they face their own sets of unique challenges, and when enhanced with proposals such as the ones in this paper, still under-perform value-based methods. What are their unique selling points?\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nREVERSIBLE COLUMN NETWORKS\n\nYuxuan Cai1\n\nYizhuang Zhou1 Qi Han1\n\nJianjian Sun1 Xiangwen Kong1\n\nJun Li1\n\nXiangyu Zhang12 ∗\n\nMEGVII Technology1 Beijing Academy of Artificial Intelligence2 {caiyuxuan, zhouyizhuang, hanqi, zhangxiangyu}@megvii.com\n\nABSTRACT\n\nWe propose a new neural network design paradigm Reversible Column Network (RevCol). The main body of RevCol is composed of multiple copies of subnetworks, named columns respectively, between which multi-level reversible connections are employed. Such architectural scheme attributes RevCol very different behavior from conventional networks: during forward propagation, features in RevCol are learned to be gradually disentangled when passing through each column, whose total information is maintained rather than compressed or discarded as other network does. Our experiments suggest that CNN-style RevCol models can achieve very competitive performances on multiple computer vision tasks such as image classification, object detection and semantic segmentation, especially with large parameter budget and large dataset. For example, after ImageNet-22K pre-training, RevColXL obtains 88.2% ImageNet-1K accuracy. Given more pre-training data, our largest model RevCol-H reaches 90.0% on ImageNet-1K, 63.8% APbox on COCO detection minival set, 61.0% mIoU on ADE20k segmentation. To our knowledge, it is the best COCO detection and ADE20k segmentation result among pure (static) CNN models. Moreover, as a general macro architecture fashion, RevCol can also be introduced into transformers or other neural networks, which is demonstrated to improve the performances in both computer vision and NLP tasks. We release code and models at https://github.com/megvii-research/RevCol\n\n1\n\nINTRODUCTION\n\nInformation Bottleneck principle (IB) (Tishby et al., 2000; Tishby & Zaslavsky, 2015) rules the deep learning world. Consider a typical supervised learning network as in Fig. 1 (a): layers close to the input contain more low-level information, while features close to the output are rich in semantic meanings. In other words, information unrelated to the target is gradually compressed during the layer-by-layer propagation. Although such learning paradigm achieves great success in many practical applications, it might not be the optimal choice in the view of feature learning – down-stream tasks may suffer from inferior performances if the learned features are over compressed, or the learned semantic information is irrelevant to the target tasks, especially if a significant domain gap exists between the source and the target tasks (Zamir et al., 2018). Researchers have devoted great efforts to make the learned features to be more universally applicable, e.g. via self-supervised pre-training(Oord et al., 2018; Devlin et al., 2018; He et al., 2022; Xie et al., 2022) or multi-task learning (Ruder, 2017; Caruana, 1997; Sener & Koltun, 2018).\n\nIn this paper, we mainly focus on an alternative approach: building a network to learn disentangled representations. Unlike IB learning, disentangled feature learning (Desjardins et al., 2012; Bengio et al., 2013; Hinton, 2021) does not intend to extract the most related information while discard the less related; instead, it aims to embed the task-relevant concepts or semantic words into a few decoupled dimensions respectively. Meanwhile the whole feature vector roughly maintains as much information as the input. It is quite analogous to the mechanism in biological cells (Hinton, 2021; Lillicrap et al., 2020) – each cell shares an identical copy of the whole genome but has different\n\n∗Corresponding author. This work is supported by The National Key Research and Development Program of\n\nChina (No. 2017YFA0700800) and Beijing Academy of Artificial Intelligence (BAAI).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Sketch of the information propagation in: (a) Vanilla single-column network. (b) Our reversible column network. Yellow color denotes low-level information and blue color denotes semantic information.\n\nexpression intensities. Accordingly in computer vision tasks, learning disentangled features is also reasonable: for instance, high-level semantic representations are tuned during ImageNet pre-training, meanwhile the low-level information (e.g. locations of the edges) should also be maintained in other feature dimensions in case of the demand of down-stream tasks like object detection.\n\nFig. 1 (b) sketches our main idea: Reversible Column Networks (RevCol), which is greatly inspired by the big picture of GLOM (Hinton, 2021). Our network is composed of N subnetworks (named columns) of identical structure (however whose weights are not necessarily the same), each of which receives a copy of the input and generates a prediction. Hence multi-level embeddings, i.e. from low-level to highly semantic representations, are stored in each column. Moreover, reversible transformations are introduced to propagate the multi-level features from i-th column to (i + 1)-th column without information loss. During the propagation, since the complexity and nonlinearity increases, the quality of all feature levels is expected to gradually improve. Hence the last column (Col N in Fig. 1 (b)) predicts the final disentangled representations of the input.\n\nIn RevCol, one of our key contributions is the design of the reversible transformations between adjacent columns. The concept is borrowed from the family of Reversible Networks (Chang et al., 2018; Gomez et al., 2017; Jacobsen et al., 2018; Mangalam et al., 2022); however, conventional reversible structures such as RevNets (Gomez et al., 2017) (Fig. 2 (a)) usually have two drawbacks: first, feature maps within a reversible block are restricted to have the same shape*; second, the last two feature maps in RevNets have to contain both low-level and high-level information due to the reversible nature, which may be difficult to optimize as in conflict with IB principle. In this paper, we overcome the drawbacks by introducing a novel reversible multi-level fusion module. The details are discussed in Sec. 2.\n\nWe build a series of CNN-based RevCol models under different complexity budgets and evaluate them in mainstream computer vision tasks, such as ImageNet classification, COCO object detection and instance segmentation, as well as ADE20K semantic segmentation. Our models achieve comparable or better results than sophisticated CNNs or vision transformers like ConvNeXt (Liu et al., 2022b) and Swin (Liu et al., 2021). For example, after ImageNet-22K pre-training, our RevCol-XL model obtains 88.2% accuracy on ImageNet-1K without using transformers or large convolutional kernels (Ding et al., 2022b; Liu et al., 2022b; Han et al., 2021). More importantly, we find RevCol can scale up well to large models and large datasets. Given a larger private pre-training dataset, our biggest model RevCol-H obtains 90.0% accuracy on ImageNet-1K classification, 63.8% APbox on COCO detection minival set, and 61.0% mIoU on ADE20K segmentation, respectively. To our knowledge, it is the best reversible model on those tasks, as well as the best pure CNN model on COCO and ADE20K which only involves static kernels without dynamic convolutions (Dai et al., 2017; Ma et al., 2020). In the appendix, we further demonstrate RevCol can work with transformers (Dosovitskiy et al., 2020; Devlin et al., 2018) and get improved results on both computer vision and NLP tasks. Finally, similar to RevNets (Gomez et al., 2017), RevCol also shares the bonus of memory saving from the reversible nature, which is particularly important for large model training.\n\n*In precise, feature maps of odd and even indexes should be equal sized respectively.\n\n2\n\nInputInputOutputOutput(b)(a)............Col 1Col 2Col 3Col 4Col NLayer 1Layer 2Layer 3Layer N...Non-ReversibleConnectionReversibleConnectionLow-LevelSemanticPublished as a conference paper at ICLR 2023\n\nRelation to previous works. Although our initial idea on feature disentangling is derived from GLOM (Hinton, 2021), in RevCol there are a lot of simplifications and modifications. For example, GLOM suggests contrastive auxiliary loss to avoid feature collapse. Contrastive training methods need extra pairs of positive and negative samples, which is complicated and unstable. In RevCol, reversible transformations between columns provides lossless information propagation by nature. As for other multi-scale grid-like architectures such as HRNets (Wang et al., 2020), DEQ models (Bai et al., 2020) and FPNs (Lin et al., 2017; Tan et al., 2020), the design purpose of those models is to fuse multi-scale features rather than learn disentangled representations; therefore, in general they still follow the paradigm in Fig. 1 (a) – neither multiple entrances/exits nor reversible structures are employed. Based on those grid-like network topology, NAS based works (Ding et al., 2021; Wu et al., 2021; Liu et al., 2019; Ghiasi et al., 2019) search the optimized topology of network architectures for specific dataset. However, the RevCol architecture is not limit to specific tasks or datasets. With the reversible nature, our method maintains lossless information propagation and benefits for not only pre-training but also other down-stream tasks. Very recently, RevBiFPN (Chiley et al., 2022) comes up with an reversible variant of FPN, which is further employed in an HRNet-like architecture. Though our RevCol shares the similar idea of multi-scale reversible transformations with RevBiFPN, our work is done independently, which is derived from a different motivation of feature disentangling, and has much simpler architectures (e.g. free of reversible upsampling tower) and higher performances. We compare some of those models in Sec. 3.\n\n2 METHOD\n\nIn this section, we introduce the design details of our Reversible Column Networks (RevCol). Fig. 1 (b) illustrates the top-level architecture. Notice that for each column in RevCol, for simplicity we directly reuse existing structures such as ConvNeXt (Liu et al., 2022b), hence in the following subsections, we mainly focus on how to build the reversible connections between columns. In addition, we introduce an plug-and-play intermediate supervision on top of each column, which further improves the training convergence and feature quality.\n\n2.1 MULTI-LEVEL REVERSIBLE UNIT\n\nIn our network, reversible transformations plays a key role in feature disentangling without information loss, whose insight comes from Reversible Neural Networks (Dinh et al., 2014; Chang et al., 2018; Gomez et al., 2017; Jacobsen et al., 2018; Mangalam et al., 2022). Among them, we first take a review of one representative work RevNet (Gomez et al., 2017). As shown in Fig. 2 (a), RevNet first partitions the input x into two groups, x0 and x1. Then for later blocks, for example, block t, it takes two anterior blocks’ outputs xt−1 and xt−2 as input and generates the output xt. The mapping of block t is reversible, i.e. xt−2 can be reconstructed by two posterior blocks xt−1 and xt. Formally, the forward and inverse computation follow the equations †:\n\nF orward : xt = Ft(xt−1) + γxt−2 Inverse : xt−2 = γ−1[xt − Ft(xt−1)],\n\n(1)\n\nwhere Ft denotes an arbitrary non-linear operation analogous to those residual functions in standard ResNets; γ is a simple reversible operation (e.g. channel-wise scaling), whose inverse is denoted by γ−1. As mentioned in the introduction, the above formulation involves too strong constraint on the feature dimensions, i.e. xt, xt+2, xt+4, ... have to be equal sized, which is not flexible in architecture design. That is why RevNets (Gomez et al., 2017) introduce some non-reversible down-sampling blocks between reversible units, hence the whole network is not fully reversible. More importantly, we find there is no clear way to directly employ Eq. 1 to bridge the columns in Fig. 1 (b).\n\nTo address the issue, we generalize Eq. 1 into the following form:\n\nF orward : xt = Ft(xt−1, xt−2, ..., xt−m+1) + γxt−m Inverse : xt−m = γ−1[xt − Ft(xt−1, xt−2, ..., xt−m+1)],\n\n(2)\n\n†In Gomez et al. (2017), the proposed reversible equations are formulated as y1 = x1 + F(x2) and y2 = x2 + G(y1). While in this paper, we reformulate those notations y2, y1, x2, x1, G, F as xt, xt−1, xt−2, xt−3, Ft, Ft−1, respectively, in order to better illustrate the relation between building block t and t − 1. It is easy to prove the two formulations are equivalent.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: (a) Reversible unit in RevNet (Gomez et al., 2017). (b) Multi-level reversible unit. All inputs for level t are highlighted. (c) An overview of the whole reversible column network architecture, with simplified multi-level reversible unit.\n\nwhere m is the order of the recursion (m ≥ 2). Clearly, the extension is still reversible. Then we partition every m feature maps into a group: (x1, x2, . . . , xm), (xm+1, xm+2, . . . , x2m), . . . . Given the features within any of the group, we can easily compute the features in other groups recursively according to Eq. 2. Compared with the original form, Eq. 2 has the following two nice properties:\n\n• The constraint on the feature map sizes is greatly relaxed if m is relatively large. Notice that Eq. 1 does not require feature maps within each group to be equal sized; such constraint only exist between groups. Therefore, we can use tensors of different shape to represent features of different semantic levels or different resolutions.\n\n• Eq. 2 can easily cooperate with existing network architectures, even though the latter is not reversible. For example, we can assign m feature maps in a standard ResNet to represent the feature maps within a group (xt, xt+1, . . . , xt+m−1), which is still compatible with Eq. 2 since ResNet can be viewed as a part of (Ft, Ft+1, . . . , Ft+m−1) respectively. Thus the whole network is still reversible.\n\nTherefore, we can reorganize Eq. 2 into a multi-column fashion, as shown in Fig. 2 (b). Each column is composed of m feature maps within a group, as well as their mother network. We name it multi-level reversible unit, which is the basic component of our RevCol as in Fig. 1 (b).\n\n2.2 REVERSIBLE COLUMN ARCHITECTURE\n\n2.2.1 MACRO DESIGN\n\nAs discussed in the introduction (see Fig. 1 (b)), our network RevCol is composed of multiple subnetworks with reversible connections to perform feature disentangling. Fig. 2 (c) elaborates the architecture design. Following the common practice of recent models (Dosovitskiy et al., 2020; Liu et al., 2022b), first the input image is split into non-overlapping patches by a patch embedding module. After that, patches are fed into each subnetwork (column). Columns can be implemented with any conventional single-column architectures, e.g. ViT (Dosovitskiy et al., 2020) or ConvNeXt (Liu et al., 2022b). We extract four-level feature maps from each column to propagate information between columns; for example, if the columns are implemented with widely-used hierarchical networks (Liu et al., 2021; He et al., 2016; Liu et al., 2022b), we can simply extract multi-resolution features from the output of each stage. For classification tasks, we only use feature map of the last level (Level 4) in the last column for rich semantic information. For other down-stream tasks like object detection and semantic segmentation, we use feature maps of all the four levels in the last column as they contain both low-level and semantic information.\n\n4\n\nFusionLevel2Level1FusionLevel3FusionLevel4IntermediateSupervisionIntermediateSupervisionSupervisionFusionLevel2Level1FusionLevel3FusionLevel4MoreColumnsMoreColumnsMoreColumnsMoreColumns(a)(c)(b)X1X0t-m+1Xt-mXtXt-1Xt-2XtXt-2XLevel2FusionLevel1FusionLevel3FusionLevel4STEMImage InputFt-3Ft-4Ft-2Ft-1Ftt-1Xt-3Xt-5Xt-4XPublished as a conference paper at ICLR 2023\n\nTo implement the reversible connections between columns, we adopt the multi-level reversible unit proposed in Eq. 2, but in a simplified fashion: rather than take (m − 1) inputs for each non-linear operation Ft(·), we use only one low-level feature xt−1 at the current column and one high-level feature xt−m+1 at the previous column as the input. The simplification does not break the reversible property. We find more inputs bring minor accuracy gain but consume much more GPU resources. Thus Eq. 2 is simplified as:\n\nF orward : xt = Ft(xt−1, xt−m+1) + γxt−m Inverse : xt−m = γ−1[xt − Ft(xt−1, xt−m+1)].\n\n(3)\n\nCompared with conventional architectures, the macro design of our RevCol has the following three properties or advantages:\n\nFeature disentangling. In RevCol, the lowest level of each column maintains low-level features as it is close to the input, while the highest level in the last column is highly semantic because it is directly connected to the supervision. Therefore, information in different levels is gradually disentangled during the (lossless) propagation between columns – some feature maps are more and more semantic and some maintain to be low-level. Detailed analyses are presented in Appendix G. The property brings many potential advantages, for instance, more flexible to downstream tasks which rely on both high-level and low-level features. We argue that reversible connection plays a key role in the disentangling mechanism – some previous works like HRNet (Wang et al., 2020) involve multi-level feature fusion but without reversible connection, which may suffer from information loss and lead to inferior performances in our experiments (see Section D.2).\n\nMemory saving. The training of conventional networks takes a lot of memory footprint to store the activations during forward propagation as the demand of gradient computation. While in our RevCol, since the connections between columns are explicitly reversible, during the back-propagation we can reconstruct the required activations on the fly from the last column to the first, which means we only need to maintain activations from one column in memory during training. In Section D.4, we demonstrate RevCol costs roughly O(1) additional memory with the increase of column numbers.\n\nNew scaling factor for big models. In RevCol architecture, column number serves as a new dimension in addition to depth (number of blocks), and width (channels of each block) in vanilla single-column CNNs or ViTs. Increasing column numbers has similar income as increasing both width and depth in certain range.\n\n2.2.2 MICRO DESIGN\n\nWe employ ConvNeXt blocks (Liu et al., 2022b) to implement each column in our network by default; other architectures, such as transformers, are also applicable (see Appendix B for details). We make a few modifications to make ConvNeXt compatible with our macro architecture:\n\nFusion module. As shown in Fig. 3, in each level of original ConvNeXt, the inputs are first downsampled in a patch-merging block. Then the outputs are passed through a bunch of residual blocks. In RevCol, we introduce a fusion module to fuse the feature maps from the current and previous columns (refer to Fig. 2 (c), green and blue connections). We modify the original patch-merging block in ConvNeXt by putting the LayerNorm after the patch-merging convolution rather than before. Channel numbers are doubled in patch-merging convolution. We also introduce an up-sampling block, which is composed of a linear channel mapping layer, a LayerNorm normalization and a feature map interpolation layer. We halve the channel numbers in linear channel mapping layer. The outputs of the two blocks are summed up and then passed to the residual blocks followed by.\n\nIn RevCol we revise the 7 × 7 convolutions in original ConvNeXt (Liu et al., 2022b) to Kernel size. 3 × 3 by default, mainly to speed up the training. Increasing kernel size further obtains more accuracy, but not very much, partly because the our multi-column design enlarges the effective receptive field. Please refer to Section D.5 for more details.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nReversible operation γ. We adopt a learnable reversible channel-wise scaling as reversible operation γ to keep the network stable. Each time the features are summed up in forward of Eq. 3, the magnitude grows larger, which makes the training process unstable. Using a learnable scaling can suppress the magnitude of features. During training, we truncate the absolute value of γ so that it will never be smaller than 1e−3, because the numerical error could become large in the reverse computation when γ is too small.\n\n2.3\n\nINTERMEDIATE SUPERVISION\n\nThough multi-level reversible unit is able to maintain information during column iteration, the down-sample block still can discard information inside column. Features at the end of the front columns is too close to the final output, for reversible connections simply do scaling and summation. Such information loss leads to inferior performance. Similar problem also happens when using deeply-supervised method (Lee et al., 2015; Szegedy et al., 2015).\n\nTo mitigate the problem of information collapse, we propose an intermediate supervision method which adds additional supervision into front columns. For features in front columns, we hope to keep the mutual information between features and the input image as much as possible, so that the network discard less information within columns. Consider RevCol gradually disentangle semantic and low-level information, extracting and leveraging the task-relevant information can further boost the performance. Therefore, we need to maximize the lower bound of mutual information between features and the prediction.\n\nInspired by Wang et al. (2021), we add two auxiliary heads to last level features (Level 4). One is a decoder (He et al., 2022) which reconstructs the input image, the other is a linear classifier. The linear classifier can be trained in a regular classification fashion with the cross-entropy (CE) loss. The parameters of decoder are optimized by minimizing the binary cross-entropy (BCE) reconstruction loss. Compared with commonly used L1 and L2 loss, interpreting the distribution of reconstructed logits and input image as bit probabilities (Bernoullis) outputs smoother value, which makes it more compatible with CE Loss.\n\nFor intermediate supervision at one column, the compound loss is the weighted summation of the above two loss. Note that supervision heads may not be added to all columns. For all the variants of RevCol, we set the number of compound loss to 4 empirically (eg. for a 8 column RevCol, the supervision heads are added to column 2, 4, and 6, and 8).\n\nThe total loss L in is the summation of all compound loss:\n\nn (cid:88)\n\n(αiLBCE + βiLCE)\n\nL =\n\ni=1\n\n(4)\n\nn denotes the total number of compound loss. LBCE and LCE denotes BCE loss and CE loss correspondingly. αi and βi are changed linearly with the compound loss number. When the compound loss is added in earlier columns, we use larger value αi and smaller value βi to keep I(h, x). In later columns, value αi decreases and βi increases, which helps boost the performance.\n\n3 EXPERIMENTS\n\nWe construct different RevCol variants, RevCol-T/S/B/L, to be of similar complexities to Swin transformers and ConvNeXts. We also build a larger RevCol-XL and RevCol-H to test the scaling up capability. These variants adopt different channel dimension C, blocks in each column B and column numbers COL. The configuration hyper-parameters of these model variants are:\n\n• RevCol-T: C = (64, 128, 256, 512), B = (2, 2, 4, 2), COL = 4\n\n• RevCol-S: C = (64, 128, 256, 512), B = (2, 2, 4, 2), COL = 8\n\n• RevCol-B: C = (72, 144, 288, 576), B = (1, 1, 3, 2), COL = 16\n\n• RevCol-L: C = (128, 256, 512, 1024), B = (1, 2, 6, 2), COL = 8\n\n• RevCol-XL: C = (224, 448, 896, 1792), B = (1, 2, 6, 2), COL = 8\n\n• RevCol-H: C = (360, 720, 1440, 2880), B = (1, 2, 6, 2), COL = 8\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: ImageNet classification results. We compare our models with state-of-the-art ◦ Vision Transformers and • CNNs that have comparable FLOPs and parameters. ↑ denotes models fine-tuning using image size larger than 2242. We report the top-1 accuracy on the validation set of ImageNet as well as the number of parameters and FLOPs. Our models are highlighted in gray.\n\nModel\n\nImage ParamsFLOPs Top-1 Size\n\nAcc.\n\n(M)\n\n(G)\n\nModel\n\nImage ParamsFLOPs Top-1 Size\n\nAcc.\n\n(M)\n\n(G)\n\nImageNet-1K trained models\n\n2242 ◦ Swin-T (Liu et al.) 2242 ◦ DeiT-S (Touvron et al. ◦ Rev-ViT-S (Mangalam et al.)2242 • RevBiFPN-S3 (Chiley et al.)2882 • EfficientNet-B4 (Tan & Le) 3802 2242 • ConvNeXt-T (Liu et al.) 2242 • RevCol-T\n\n2242 ◦ Swin-S (Liu et al.) 2242 ◦ MViTv1-B (Fan et al.) 2242 ◦ T2T-ViT-19 (Yuan et al.) • RevBiFPN-S4 (Chiley et al.)3202 • EfficientNet-B5 Tan & Le) 4562 2242 • ConvNeXt-S (Liu et al.) 2242 • RevCol-S\n\n2242 ◦ Swin-B (Liu et al.) 2242 ◦ DeiT-B (Touvron et al.) ◦ Rev-ViT-B(Mangalam et al.)2242 • RepLKNet-31B (Ding et al.)2242 • RevBiFPN-S5 (Chiley et al.)3522 • EfficientNet-B6 (Tan & Le) 5282 2242 • ConvNeXt-B (Liu et al.) 2242 • RevCol-B\n\n28 22 22 20 19 29 30\n\n50 37 39 49 30 50 60\n\n89 86 87 79 82 43 88 138\n\n4.5 4.6 4.6 3.3 4.2 4.5 4.5\n\n8.7 7.8 8.4 10.6 9.9 8.7 9.0\n\n15.4 17.5 17.6 15.3 21.8 19.0 15.4 16.6\n\n81.3 79.8 79.9 81.1 82.9 82.1 82.2\n\n83.0 83.0 81.4 83.0 83.6 83.1 83.5\n\n83.5 81.8 81.8 83.5 83.7 84.0 83.8 84.1\n\nImageNet-22K pre-trained models (ImageNet-1K fine-tuned)\n\n2242 ◦ Swin-B (Liu et al. ◦ Swin-B↑ (Liu et al. 3842 ◦ ViT-B↑ (Dosovitskiy et al.) 3842 • RepLKNet-31B (Ding et al.) 2242 • RepLKNet-31B↑ (Ding et al.)3842 2242 • ConvNeXt-B (Liu et al.) • ConvNeXt-B↑ (Liu et al.) 3842 2242 • RevCol-B • RevCol-B↑ 3842\n\n2242 ◦ Swin-L (Liu et al.) ◦ Swin-L↑ (Liu et al.) 3842 ◦ ViT-L↑ (Dosovitskiy et al.) 3842 • RepLKNet-31L (Ding et al.) 3842 2242 • ConvNeXt-L (Liu et al.) • ConvNeXt-L↑ (Liu et al.) 3842 2242 • RevCol-L • RevCol-L↑ 3842\n\n• ConvNeXt-XL↑ (Liu et al.) 3842 • RevCol-XL↑ 3842\n\n88 88 86 79 79 89 89 138 138\n\n197 197 307 172 198 198 273 273\n\n350 834\n\n15.4 47.0 55.4 15.3 45.1 15.4 45.1 16.6 48.9\n\n85.2 86.4 84.0 85.2 86.0 85.8 86.8 85.6 86.7\n\n34.5 86.3 103.9 87.3 190.7 85.2 86.6 96.0 34.4 86.6 101.0 87.5 39.0 86.6 116.0 87.6\n\n179.0 87.8 350.0 88.2\n\nExtra data pre-trained models (ImageNet-1K fine-tuned)\n\n• RevCol-XL↑ • RevCol-H↑\n\n3842 6402\n\n834 2158\n\n350.0 89.4 90.0 2537\n\nWe conduct image classification on ImageNet dataset (Deng et al., 2009; Ridnik et al., 2021). We also test our models on the downstream object detection task and semantic segmentation task on commonly used MS-COCO (Lin et al., 2014) and ADE20k (Zhou et al., 2017b) dataset. Training and fine-tuning settings please refer to Appendix F. Furthermore, we show the performance of RevCol with transformer on vision and language tasks (shown in Appendix B).\n\n3.1\n\nIMAGE CLASSIFICATION\n\nOn ImageNet (1.28M images) (Deng et al., 2009) dataset, we train RevCol for 300 epochs with intermediate supervision. Hyperparameters, augmentation and regularization strategies follows Liu et al. (2022b) We also pre-train our models on the larger ImageNet-22K dataset (Ridnik et al., 2021), which contains 14.2 million images.\n\nIn Tab. 1, we compare our RevCol variants with commonly used recent Transformers and CNNs on ImageNet-1k validation set. Our models outperforms a large number of vanilla single-column CNNs and Transformers with similar complexities. For example, RevCol-S achieve 83.5% Top-1 accuracy, outperform ConvNeXt-S by 0.4 points. When pre-trained with larger ImageNet-22K dataset, RevCol-XL achieves 88.2% Top-1 accuracy. As RevCol maintains some task-irrelevant low-level information in classification pre-training, relaxing the constraint of params and FLOPs and enlarging dataset size can further boost our models’ performance. To further test the scaling up effectiveness of large dataset, we build a 168-million-image semi-labeled dataset (see Appendix E). With extra data pre-training and ImageNet-1k fine-tuning, our RevCol-H achieves 90.0% top-1 accuracy. Our results further demonstrate with RevCol, CNN models can also share the dividends of large model and massive data pre-training.\n\n3.2 OBJECT DETECTION\n\nWe evaluate our proposed RevCol on object detection task. Experiments are conducted on the MS-COCO dataset using the Cascade Mask R-CNN (Cai & Vasconcelos, 2019) framework. We\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Object detection results on MS-COCO dataset with different backbones. We report box AP and mask AP with single scale testing on COCO minival set. FLOPs are measured under input sizes of (1280, 800).\n\nBackbone\n\nAPbox APbox 50\n\nAPbox 75\n\nAPmask APmask\n\n50\n\nAPmask 75\n\nParams\n\nFLOPs\n\n◦ Swin-T (Liu et al.) • ConvNeXt-T (Liu et al.) • RevCol-T ◦ Swin-S (Liu et al.) • ConvNeXt-S (Liu et al.) • RevCol-S ◦ Swin-B (Liu et al.) • ConvNeXt-B (Liu et al.) • RepLKNet-B (Ding et al.) • RevCol-B\n\n◦ Swin-B (Liu et al.) • ConvNeXt-B (Liu et al.) • RepLKNet-B (Ding et al.) • RevCol-B ◦ Swin-L (Liu et al.) • ConvNeXt-L (Liu et al.) • RepLKNet-L (Ding et al.) • RevCol-L\n\n50.5 50.4 50.6 51.8 51.9 52.6 51.9 52.7 52.2 53.0\n\n53.0 54.0 53.0 55.0 53.9 54.8 53.9 55.9\n\n• RevCol-H (HTC++) • RevCol-H (Objects365+DINO)\n\n61.1 63.8\n\nImageNet-1K pre-trained 43.7 54.9 43.7 54.8 43.8 54.9 44.7 56.3 45.0 56.5 45.5 56.8 45.0 56.5 45.6 57.2 45.2 -\n45.9 57.3\n\n69.3 69.1 68.9 70.4 70.8 71.1 70.9 71.3 -\n71.4\n\nImageNet-22K pre-trained 45.8 57.5 46.9 58.8 46.3 -\n47.5 59.7 46.7 58.8 47.6 59.8 46.5 -\n48.4 60.7\n\n71.8 73.1 -\n73.5 72.4 73.8 -\n74.1\n\nExtra data pre-trained 67.0 78.8 70.2 81.8\n\n53.0 -\n\n66.6 66.5 66.7 67.9 68.4 68.8 68.4 68.9 -\n69.1\n\n69.4 70.6 -\n71.1 70.1 71.3 -\n71.8\n\n76.3 -\n\n47.1 47.3 47.4 48.5 49.1 49.0 48.7 49.5 -\n50.1\n\n49.7 51.3 -\n51.8 50.8 51.7 -\n52.8\n\n58.7 -\n\n86M 86M 88M 107M 108M 118M 145M 146M 137M 196M\n\n145M 146M 137M 196M 253M 255M 229M 330M\n\n745G 741G 741G 838G 827G 833G 982G 964G 965G 988G\n\n982G 964G 965G 988G 1382G 1354G 1321G 1453G\n\n2.41G 2.18G\n\n4417G 4012G\n\nalso finetune our largest model RevCol-H with HTC++ (Chen et al., 2019) and DINO (Zhang et al., 2022a) Framework.\n\nIn Tab. 2, we compare the APbox and APmask with Swin/ConvNeXt in variant sizes on COCO validation set. We find RevCol models surpass other counterparts with similar computation complexities. Information retained in pre-training helps RevCol models acheieve better results in down-stream tasks. When the model size grows larger, this advantage becomes more remarkable. After finetuning under Objects365(Shao et al., 2019) dataset and DINO (Zhang et al., 2022a) framework, our largest model RevCol-H achieves 63.8% APbox on COCO detection minival set.\n\n3.3 SEMANTIC SEGMENTATION\n\nWe also evaluate RevCol backbones on the ADE20K semantic segmentation task with UperNet (Xiao et al., 2018) framework. We do not use intermediate-supervision in down-stream fine-tune process. To further explore our model’s capacity and reach the leading performance, we utilize recent segmentation framework Mask2Former (Cheng et al., 2022), and adopt the same training settings.\n\nIn Tab. 3, we report validation mIoU with single-scale and multi-scale flip testing. RevCol models can achieve competitive performance across different model capacities, further validating the effectiveness of our architecture design. It’s worth noting that when use Mask2Former detector and extra pretraining data, RevCol-H achieves an mIoU of 61.0%, which shows feasible scalability towards large-scale vision applications.\n\n4 RELATED WORKS\n\n4.1 DISENTANGLE REPRESENTATION LEARNING AND PART-WHOLE HIERARCHY\n\nA disentangled representation is generally described as one which separates the factors of variation, explicitly representing the important attributes of the data (Desjardins et al., 2012; Bengio et al., 2013). Desjardins et al. (2012); Kulkarni et al. (2015); Higgins et al. (2017); Chen et al. (2016); Karras et al. (2019) seek to learn disentangled representations through generative models. Locatello et al. (2019) points out that unsupervised learning of disentangled representations is fundamentally impossible without inductive biases both on the considered learning approaches and the datasets. The recent proposal of GLOM (Hinton, 2021) gives an idea of representing a part-whole hierarchy by a\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Semantic segmentation result on ADE20k dataset with different backbones. we report mIoU results with single/multi-scale testing. FLOPs are measured under input sizes of (2048, 512), (2560, 640) for IN-1K and IN-22K pre-trained models respectively.\n\nBackbone\n\ncrop size mIoUss mIoUms\n\nParams\n\nFLOPs\n\nImageNet-1K pre-trained\n\n◦ Swin-T (Liu et al.) • ConvNeXt-T (Liu et al.) • RevCol-T ◦ Swin-S (Liu et al.) • ConvNeXt-S (Liu et al.) • RevCol-S ◦ Swin-B (Liu et al.) • RepLKNet-B (Ding et al.) • ConvNeXt-B (Liu et al.) • RevCol-B\n\n5122 5122 5122 5122 5122 5122 5122 5122 5122 5122\n\n44.5 46.0 47.4 47.6 48.7 47.9 48.1 49.9 49.1 49.0\n\n45.8 46.7 47.6 49.5 49.6 49.0 49.7 50.6 49.9 50.1\n\nImageNet-22K pre-trained\n\n◦ Swin-B (Liu et al.) • RepLKNet-B (Ding et al.) • ConvNeXt-B (Liu et al.) • RevCol-B ◦ Swin-L (Liu et al.) • RepLKNet-L (Ding et al.) • ConvNeXt-L (Liu et al.) • RevCol-L\n\n6402 6402 6402 6402 6402 6402 6402 6402\n\n50.3 51.5 52.6 52.7 52.1 52.4 53.2 53.4\n\n• RevCol-H • RevCol-H + Mask2Former\n\nExtra data pre-trained 6402 6402\n\n57.8 60.4\n\n51.7 52.3 53.1 53.3 53.5 52.7 53.7 53.7\n\n58.0 61.0\n\n60M 60M 60M 81M 82M 90M 121M 112M 122M 122M\n\n121M 112M 122M 122M 234M 207M 235M 306M\n\n945G 939G 937G 1038G 1027G 1031G 1188G 1170G 1170G 1169G\n\n1841G 1829G 1828G 1827G 2468G 2404G 2458G 2610G\n\n2421M 2439M\n\n- -\n\nweight-sharing columns. The GLOM architecture provides an interpretable part-whole hierarchies for deep neural network (Garau et al., 2022). In RevCol, we adopt the design of using columns, but not modeling the process of formulating islands. On the contrary, our column iteration process maintains both low-level and high-level information and gradually disentangle them. Rather than using self-supervised methods, RevCol can be trained with supervision end-to-end.\n\n4.2 REVERSIBLE NETWORKS\n\nGomez et al. (2017) firstly propose RevNet that allow back propagation without saving intermediate activations. The reversible design remarkably saves the training cost, since it keep O(1) GPU memory consumption as model depth scaling up. Jacobsen et al. (2018) propose a fully reversible network that can reverse back to the input without any information loss. Chang et al. (2018) develop a theoretical framework on stability and reversibility of deep neural network and derive reversible networks that can go arbitrarily deep. Mangalam et al. (2022) expand the reversible network scope from CNNs to Transformers. RevBiFPN (Chiley et al., 2022), a concurrent work of ours, add the reversible connections to BiFPN (Tan et al., 2020) network. Our RevCol maintains the information without loss inside each column rather than the whole BiFPN network in RevBiFPN.\n\n5 CONCLUSION\n\nIn this paper, we propose RevCol, a reversible column based foundation model design paradigm. During the lossless propagation through columns, features in RevCol are learned to be gradually disentangled and the total information is still maintained rather than compressed. Our experiments suggests that RevCol can achieve competitive performance in multiple computer vision tasks. We hope RevCol could contribute to better performance in various tasks in both vision and language domains.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nShaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. Advances in\n\nNeural Information Processing Systems, 33:5238–5250, 2020.\n\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\n\narXiv:2106.08254, 2021.\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.\n\nZhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: high quality object detection and instance segmentation. IEEE transactions on pattern analysis and machine intelligence, 43(5):1483–1498, 2019.\n\nRich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.\n\nBo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham. Reversible architectures for arbitrarily deep residual neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n\nKai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4974–4983, 2019.\n\nTianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge\n\ntransfer. arXiv preprint arXiv:1511.05641, 2015.\n\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. Advances in neural information processing systems, 29, 2016.\n\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1290–1299, 2022.\n\nVitaliy Chiley, Vithursan Thangarasa, Abhay Gupta, Anshul Samar, Joel Hestness, and Dennis DeCoste. Revbifpn: The fully reversible bidirectional feature pyramid network. arXiv preprint arXiv:2206.14098, 2022.\n\nJifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international conference on computer vision, pp. 764–773, 2017.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nGuillaume Desjardins, Aaron Courville, and Yoshua Bengio. Disentangling factors of variation via\n\ngenerative entangling. arXiv preprint arXiv:1210.5474, 2012.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nMingyu Ding, Xiaochen Lian, Linjie Yang, Peng Wang, Xiaojie Jin, Zhiwu Lu, and Ping Luo. Hr-nas: Searching efficient high-resolution neural architectures with lightweight transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2982–2992, 2021.\n\nXiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11963–11975, 2022a.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nXiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11963–11975, 2022b.\n\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components\n\nestimation. arXiv preprint arXiv:1410.8516, 2014.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nHaoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and In Proceedings of the IEEE/CVF\n\nChristoph Feichtenhofer. Multiscale vision transformers. International Conference on Computer Vision, pp. 6824–6835, 2021.\n\nNicola Garau, Niccolò Bisagno, Zeno Sambugaro, and Nicola Conci.\n\nhierarchies and conceptual-semantic relationships in neural networks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13689–13698, 2022.\n\nInterpretable part-whole In Proceedings of the\n\nGolnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7036–7045, 2019.\n\nGolnaz Ghiasi, Barret Zoph, Ekin D Cubuk, Quoc V Le, and Tsung-Yi Lin. Multi-task self-training for learning general representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8856–8865, 2021.\n\nAidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations. Advances in neural information processing systems, 30, 2017.\n\nQi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiaying Liu, and Jingdong Wang. On the connection between local attention and dynamic depth-wise convolution. In International Conference on Learning Representations, 2021.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked In Proceedings of the IEEE/CVF Conference on\n\nautoencoders are scalable vision learners. Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=Sy2fzU9gl.\n\nGeoffrey Hinton. How to represent part-whole hierarchies in a neural network. arXiv preprint\n\narXiv:2102.12627, 2021.\n\nJörn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks.\n\narXiv preprint arXiv:1802.07088, 2018.\n\nPeng-Tao Jiang, Chang-Bin Zhang, Qibin Hou, Ming-Ming Cheng, and Yunchao Wei. Layercam: Exploring hierarchical class activation maps for localization. IEEE Transactions on Image Processing, 30:5875–5888, 2021.\n\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401–4410, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In European conference on computer vision, pp. 491–507. Springer, 2020.\n\nSimon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International Conference on Machine Learning, pp. 3519– 3529. PMLR, 2019.\n\nTejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional\n\ninverse graphics network. Advances in neural information processing systems, 28, 2015.\n\nChen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised\n\nnets. In Artificial intelligence and statistics, pp. 562–570. PMLR, 2015.\n\nTimothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton. Backprop-\n\nagation and the brain. Nature Reviews Neuroscience, 21(6):335–346, 2020.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740–755. Springer, 2014.\n\nTsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125, 2017.\n\nChenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, and Li FeiFei. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 82–92, 2019.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\n\nZe Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12009–12019, 2022a.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11976–11986, 2022b.\n\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In international conference on machine learning, pp. 4114–4124. PMLR, 2019.\n\nNingning Ma, Xiangyu Zhang, Jiawei Huang, and Jian Sun. Weightnet: Revisiting the design space of weight networks. In European Conference on Computer Vision, pp. 776–792. Springer, 2020.\n\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European conference on computer vision (ECCV), pp. 181–196, 2018.\n\nKarttikeya Mangalam, Haoqi Fan, Yanghao Li, Chao-Yuan Wu, Bo Xiong, Christoph Feichtenhofer, and Jitendra Malik. Reversible vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10830–10840, 2022.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\n\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 1–9, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6301.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.\n\nTal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for\n\nthe masses. arXiv preprint arXiv:2104.10972, 2021.\n\nSebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint\n\narXiv:1706.05098, 2017.\n\nOzan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances in\n\nneural information processing systems, 31, 2018.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In 54th Annual Meeting of the Association for Computational Linguistics, pp. 1715–1725. Association for Computational Linguistics (ACL), 2016.\n\nShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 8430–8439, 2019.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015.\n\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks.\n\nIn International conference on machine learning, pp. 6105–6114. PMLR, 2019.\n\nMingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10781–10790, 2020.\n\nBart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64–73, 2016.\n\nNaftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015\n\nieee information theory workshop (itw), pp. 1–5. IEEE, 2015.\n\nNaftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv\n\npreprint physics/0004057, 2000.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nJingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 43(10):3349–3364, 2020.\n\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nYulin Wang, Zanlin Ni, Shiji Song, Le Yang, and Gao Huang. Revisiting locally supervised learning:\n\nan alternative to end-to-end training. arXiv preprint arXiv:2101.10832, 2021.\n\nBichen Wu, Chaojian Li, Hang Zhang, Xiaoliang Dai, Peizhao Zhang, Matthew Yu, Jialiang Wang, Yingyan Lin, and Peter Vajda. Fbnetv5: Neural architecture search for multiple tasks in one run. arXiv preprint arXiv:2111.10007, 2021.\n\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European conference on computer vision (ECCV), pp. 418–434, 2018.\n\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9653–9663, 2022.\n\nI Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-\n\nsupervised learning for image classification. arXiv preprint arXiv:1905.00546, 2019.\n\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 558–567, 2021a.\n\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021b.\n\nAmir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712–3722, 2018.\n\nHao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022a.\n\nYuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin He, Zhenfei Yin, Kun Wang, Lu Sheng, Yu Qiao, Jing Shao, and Ziwei Liu. Bamboo: Building mega-scale vision dataset continually with human-machine synergy, 2022b.\n\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017a.\n\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 633–641, 2017b.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA MICRO DESIGN DETAILS\n\nFigure 3: (a) Levels in ConvNeXt. Level l contains a patch merging down-sample block and nl residual blocks. (b) Levels in RevCol. Level l is composed of a fusion module, nl residual blocks and a reversible operation. Note that Level l takes features maps xt−1, xt−m+1 and xt−m as input. Feature maps xt−1 and xt−m+1 are fed into the fusion module and feature maps xt−m are fed into the reversible operation. (c) Design of the fusion module.\n\nIn this section, we provide the architecture design details for RevCol. As depicted in Fig. 2 and Section 2.2, our RevCol contains multiple columns with reversible connections. Fig. 3 (a) shows the architecture of ConvNeXt. Note that we replace the 7 × 7 depth-wise convolution in ConvNeXt with 3 × 3, as described in Sec. 2.2.2. In Fig. 3 (b), we show in detail how to extend to our RevCol on the basis of ConvNeXt. First, we replace the down-sample block with a fusion block to fuse low-level representations in current column and high-level ones from the previous column, and Fig. 3 (c) shows the details of fusion block which contains up-sample and down-sample operations to handle different resolutions. Second, for each level, same-level representations from the previous column are added to current level’s output and are ready to propagate as a whole. Thanks to the two modifications, feature maps from different hierarchies aggregate together to form the intermediate representation. In Fig. 3 (c), we use a Linear-LayerNorm followed by a nearest interpolation to up-sample low resolution features. A 2 × 2 kernel Conv2d with stride 2 down-samples the high resolution features, followed by a LayerNorm to balance the contributions of the two inputs.\n\nB GENERALIZATION TO TRANSFORMERS\n\nB.1 VISION TRANSFORMER MODELS\n\nRevCol contains multiple light-weight sub-networks with reversible connections. In this paper, we adopt the ConvNext micro design by default except for multi-columns fusion and smaller convolution kernel as described in Sec. 2.2.2. However, the micro design of our RevCol is not limited to convolutional networks, but is also compatible with isotropic designing, such as the vanilla vision\n\n15\n\n(a)(b)(c)Up-SampleDown-SampleSumDown-SampleBlockDown-SampleBlockConvNeXtBlockst-m-1Xt-mXt-m+1Xt-2Xt-1XtXConvNeXtBlocksConv(s2, k2)High Resolution FeaturesLow Resolution FeaturesLayerNormLinearLayerNormInterpolationto ConvNeXt blocksConvNeXtBlocksFusion BlockConvNeXtBlocksFusion Blockto other levelsto other levelslX nlX nl -1X nl -1X nLevel l-1Level lLevel l-1Level lPublished as a conference paper at ICLR 2023\n\ntransformer (ViT) (Dosovitskiy et al., 2020). In this section, we show the micro design of RevCol can generalized to vanilla ViT, namely RevCol-ViT, with promising experimental results.\n\nnet-ViT maintains the feature resolution in the reversible columns. Thus the patch merging blocks and up-sample blocks in the fusion modules are replaced with a simple linear projection with a post LayerNorm. We use the vanilla ViT building block instead of the ConvNext building block variant. The post LayerNorms and normalized dot-product attention are used in ViT blocks to stabilize training convergence, similar to Liu et al. (2022a). With the properties of isotropy, we evenly arrange the building blocks in each column. The configuration details of RevCol-ViT are:\n\n• RevCol-ViT-S: C = (224, 224, 224, 224), B = (2, 2, 2, 2), HEAD = 4, COL = 4\n\n• RevCol-ViT-B: C = (384, 384, 384, 384), B = (3, 3, 3, 3), HEAD = 6, COL = 4\n\nTable 4: ImageNet-1K classification results. We compare our RevCol-ViT with state-of-the-art isotropic ◦ Vision Transformers and • CNNs that have comparable FLOPs and parameters.\n\nModel\n\nImage Size Params FLOPs Top-1 Acc.\n\n◦ DeiT-S (Touvron et al., 2020) • ConvNext-S (iso.) (Liu et al., 2022b) ◦ RevCol-ViT-S ◦ ViT-B (Dosovitskiy et al., 2020) ◦ DeiT-B (Touvron et al., 2020) ◦ Rev-ViT-B (Mangalam et al., 2022) ◦ Rev-MViT-B (Mangalam et al., 2022) • ConvNext-B (iso.) (Liu et al., 2022b) ◦ RevCol-ViT-B\n\n2242 2242 2242 3842 2242 2242 2242 2242 2242\n\n22M 22M 16M 86M 86M 87M 39M 87M 67M\n\n4.6G 4.3G 4.6G 55.4G 17.6G 17.6G 8.7G 16.9G 18.8G\n\n79.8 79.7 80.6 77.9 81.7 81.8 82.5 82.0 82.7\n\nWe use the same training setting with the anisotropic RevCol as described in Sec. 3.1, except that the intermediate supervision is discarded for simplicity and the stochastic depth rate is set as 0.2 for RevCol-B. We scale down the value of last linear projection layers in each FFN accroding to the network depth in initialization, same as BEiT (Bao et al., 2021). In Tab. 4, we compare the RevColViT with vanilla ViT and other concurrent isotropic designs. Our RevCol-ViT surpasses vanilla vision transformer (77.9% for ViT and 81.7% for DeiT) and convolutional network ConvNeXt (82.0%) that have comparable model parameters and computational overhead on ImageNet-1k classification w.r.t. the top-1 accuracy.\n\nB.2 LANGUAGE MODELS\n\nConsidering the great success of applying transformer to computer vision, i.e., ViT (Dosovitskiy et al., 2020), we also made some exploration to generalize RevCol to natural language processing (NLP). Based on the design in Appendix B.1, we can easily apply the isotropic RevCol to language models with minor modification. To be specific, we replace the stem module in our RevCol with word embedding and positional encoding in transformer. Then, the RevCol can be plugged into the original transformer as an encoder. The output of the last column in RevCol will be used as the memory keys and values for the attention layers in decoder, just exactly the same as the original transformer.\n\nWe select the translation task to evaluate the potential of the RevCol in NLP. We run experiments on the WMT’16 English-German (En-De) dataset with 4.5M sentences and larger WMT’14 EnglishFrench dataset with 36M sentences. Each sentence is encoded by joint source and target byte pair encoding following Sennrich et al. (2016). The details of model architecture and the BLEU score are shown in Tab. 5.\n\nAll the dataset preparation and the training configurations follows Ott et al. (2018) and the open source project fairseq. The models were trained for 300K steps with batch-size of 28,672 tokens on En-De and 200K steps with batch-size of 86,016 on En-Fr. We discard the intermediate supervision for simplicity. As shown in Tab. 5, our RevCol outperforms vanilla transformer with comparable parameters on En-De (28.67 vs. 28.43) and En-Fr (43.40 vs. 43.07), which demonstrates the RevCol’s applicability to NLP.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: BLEU score on newstest2014 for WMT English-German (En-De) and English-French (En-Fr) translation task. † indicates we re-run the experiments with fairseq.\n\nModel\n\nTransformer† (Vaswani et al., 2017)\n\nbig\n\nRevCol-Transformer\n\narch\n\nN = 6\n\nB = (1,1,1,1) COL = 4\n\nEncoder\n\nDecoder\n\ndmodel\n\ndf f\n\nhead\n\narch\n\ndmodel\n\ndf f\n\nhead\n\n1024\n\n4096\n\n16\n\nN = 6\n\n1024\n\n4096\n\n16\n\n768\n\n3072\n\n12\n\nN = 6\n\n768\n\n3072\n\n12\n\nParams\n\nTask\n\nBLEU\n\n209M En-De 221M En-Fr\n\n200M En-De 209M En-Fr\n\n28.43 43.07\n\n28.67 43.40\n\nB.3 ROBUSTNESS OF THE NUMBER OF COLUMNS\n\nIn the ablation analysis of the paper, we show that when fix the total FLOPs and add more columns of RevCol, the performance first increases and then get saturated. When the number of columns is extreme large, such as 20, the performance drop because of the representation ability of single column is limited. When the number of columns is usual, such as 4 ̃12, the performances are similar, which verifies the setting robustness of the number of columns.\n\nFigure 4: ImageNet top-1 accuracy of different variants of RevCol-ViT-B. Each variant has the same total number of residual blocks and channel dimension.\n\nTo further analyze the robustness of the number of columns, in this section, we build some RevColViT-B variants (see Appendix B for more details). Each variant has the same number of residual blocks with the same channel dimension, but different number of columns. In other worlds, these variants have the same channel dimension and different depth of each columns and different number of columns. We use 32 residual blocks totally and maintain the FLOPs about 18G. Fig. 4 show the performance on ImageNet-1K of different variants. The number of columns are 1, 2, 4, and 8, accordingly the depth of each column are 32, 16, 8, and 4. The performance of single column variant is lower (similar to DeiT-B (Touvron et al., 2020)) because of the single column ViT can not maintain the information as multi reversible columns. The performance is decreasing when the number of columns became larger, because of the depth of each columns is not enough. This phenomenon indicates us that given a target FLOPs, the setting of the number of columns is robust unless the depth of each columns or channel dimension is too small.\n\nC SYSTEM-LEVEL COMPARISON WITH SOTA FOUNDATION MODELS\n\nFoundation models (Kolesnikov et al., 2020; Radford et al., 2021; Yuan et al., 2021b) are generalpurpose backbones pre-trained on massive and divergent data source. They can adapt to various down-stream tasks with limited domain-specific data. We show comparison among various public state-of-the-art (SOTA) foundation models including Vision Transformers and Vision-Language models, namely, SwinV2 (Liu et al., 2022a), BEiT3 (Wang et al., 2022), and Florence (Yuan et al., 2021b). As shown in Tab. 6, though our RevCol-H is purely convolutional and pre-trained on single modality dataset, the results on different tasks demonstrate remarkable generalization ability of RevCol with large scale parameters.\n\n17\n\n124881.081.582.082.583.032 x 1 column 16 x 2 columns8 x 4 columns4 x 8 columnsPublished as a conference paper at ICLR 2023\n\nTable 6: System-level comparison of state-of-the-art visual foundation models with large-scale pretraining. We include ◦ Vision Transformers, • CNNs, and • hybrid architectures pretrained either unsupervised or supervised on image-only and vision-language datasets. COCO scores marked with † means intermediate fine-tuned on extra data like Object365 (Shao et al., 2019).\n\nDataset\n\nImageNet\n\nCOCO test-dev\n\nADE20K\n\n1k\n\n90.2 89.6 90.1 90.0\n\nDetector APbox APmask\n\nSegmenter mIoU +ms\n\nHTC++ ViTDet DyHead DINO\n\n63.1† 63.7† 62.4 63.6†\n\n54.4† 59.3 UperNet 54.8† Mask2Former 62.0 -\n\n-\n\nMask2Former 60.4\n\n- -\n\n59.9 62.8 -\n61.0\n\nModel\n\nParams\n\nImages\n\nAnnotation\n\n◦ SwinV2-G 3.0 G ◦ BEiT3 1.0 G • Florence 0.9 G • RevCol-H 2.1 G\n\n70 M 35 M labeled & image-text\n\nlabeled\n\n900 M 168 M\n\nimage-text semi-labeled\n\nD MORE ANALYSIS EXPERIMENTS\n\nD.1 PERFORMANCE GAIN OF REVERSIBLE COLUMNS ARCHITECTURE\n\nIn this section, we evaluate the performance gain of using reversible columns. In the first experiment, we fix a single column’s structure and FLOPs then simply add more columns to scale large and test the performance. At the same time, we plot the vanilla single-column models with similar model sizes. As depicted in Fig. 5, compared to single-column models, using multi-column reversible architecture always gets better performance under same FLOPs constraint. Besides, within a certain range, scaling up RevCol in terms of increasing column numbers can have similar gains compared to scaling up with both block numbers(depth) and channel numbers(width) in single-column models. In the second experiment, we limit the model size to about 4.5G FLOPs and test model variants with different column numbers. In other words, we gradually add more columns and scale down the single column size at the same time. Results are shown in Tab. 7, we notice that adopt column number at the range of 4 to 12 can maintain the model’s performance, then further more column models suffer from performance degradation. We believe the reason is the width and depth in a single column are too low to keep representation ability.\n\nTable 7: ImageNet 1K performances of various number of columns in RevCols under the similar computational budget.\n\n# column\n\nParams\n\nFLOPs\n\n1 4\n8 12 20\n\n28M 30M 34M 33M 35M\n\n4.4G 4.5G 4.7G 4.4G 4.2G\n\nFLOPs per col.\n\nTop-1 Acc.\n\n4.40G 1.12G 0.59G 0.35G 0.21G\n\n81.9 82.2 82.3 82.2 81.0\n\nFigure 5: ImageNet-1K performance of maintaining a constant FLOPs of a single column and adding more columns.\n\nD.2 REVERSIBLE NETWORKS VS. NON-REVERSIBLE NETWORKS\n\nIn this section, we ablate different design patterns of reversible connections. First, we build a nonreversible multi-column network using the fusion module of HRNet. Second, we build another single column reversible ConvNeXt using the design of RevNet as shown in Fig. 2(a). We compare the two designs with our RevCols. The evaluation result is shown in Tab. 8. The non-reversible multi-column network suffers from information loss during propagation, which could result in lower accuracy. The reversible single-column network maintains information during propagation, but lack the superiority of multi-level fusion. This experiment further indicates the effectiveness of combining the reversible design with multi-column networks.\n\n18\n\n7982818083844.51.09.013.518.0Multi-ColumnSingle-ColumnFLOPs(G)Top-1 Acc. %77.081.682.682.283.583.283.483.984.0Published as a conference paper at ICLR 2023\n\nTable 8: Performance comparison on ImageNet-1K of different design patterns. Row-1 represents HRNet style network w/o reversible connections. Row-2 represents RevNet style network w/o multi-column fusions. Row-3 are our proposed RevCols.\n\nTable 9: Performance comparison between models with and without intermediate supervision. Results are reported on ImageNet-1K and COCO dataset. We use 1× training schedule on COCO detection task.\n\nModel\n\ninter. sup. Top-1 Acc. APbox\n\nAPmask\n\nrev. conn. multi-col. Params FLOPs Acc.\n\n✓ ✓\n\n✓\n\n✓\n\n35M 4.9G 78.8 34M 4.5G 81.6 30M 4.5G 82.2\n\nRevCol-T RevCol-T\n\nRevCol-S RevCol-S\n\nRevCol-B RevCol-B\n\n✗ ✓\n\n✗ ✓\n\n✗ ✓\n\n81.4 82.2 (+0.8) 48.8 (+0.6) 42.2 (+0.4)\n\n41.8\n\n48.3\n\n83.0 83.5 (+0.5) 51.1 (+0.4) 43.8 (+0.0)\n\n43.8\n\n50.7\n\n83.2 84.1 (+0.9) 51.6 (+0.4) 44.2 (+0.0)\n\n44.2\n\n51.2\n\nD.3 PERFORMANCE GAIN OF USING INTERMEDIATE SUPERVISION\n\nIn this section, we evaluate the performance of RevCol-T/S/B with and without intermediate supervision on ImageNet-1K. We also evaluate the object detection task performance using 1× training schedule on MS-COCO dataset. Other settings remain the same. From the validation results in Tab. 9, models trained with intermediate supervision achieves 0.5% to 0.9% better top-1 accuracy. Besides, intermediate supervision also benefits down-stream tasks, which further demonstrates its effectiveness.\n\nTable 10: Performance of models with larger kernel convolution.\n\nKernel Size\n\nFLOPs\n\nTop-1 Acc\n\nAPbox 1×\n\nAPmask 1×\n\n3\n\n5\n\n7\n\n11\n\n4.5G\n\n4.5G\n\n4.6G\n\n4.6G\n\n82.2\n\n82.5\n\n82.5\n\n82.5\n\n48.8\n\n49.5\n\n49.3\n\n49.9\n\n42.2\n\n42.6\n\n42.4\n\n42.7\n\nFigure 6: GPU Memory Consumption vs. Model size\n\nD.4 GPU MEMORY CONSUMPTION VS MODEL SIZE\n\nFig. 6 plots the GPU memory consumption with the scaling of model size. We fix the computation complexity of a single column to 1G FLOPs and increase column number. Meanwhile, we measure the memory consumption in training process which includes the forward and backward propagation. Our experiments are conducted on Nvidia Tesla V100 GPU under batch-size 64, FP16 precision and PyTorch implementation. With the increment of column number, we can see RevCol keeps an O(1) GPU memory consumption, while non-reversible architecture’s memory consumption increase linearly with column number. Note that our RevCol does not keep strictly the same GPU memory consumption as column number increase, as reversible networks need to back-up the operation weights in need for calculating gradients and the re-construction of feature maps in backward propagation.\n\nD.5 ABLATION OF KERNEL SIZE IN CONVOLUTIONS\n\nIn original ConvNeXt, large kernel convolution achieves in better performance. We conduct experiments in RevCol-T. As shown in Tab. 10, for 4 column models, using 5 × 5 convolution increase the ImageNet-1k Top-1 accuracy by 0.3% and the COCO APbox by 0.7 for RevCol-T model. Further increasing kernel size obtains more accuracy in down-stream tasks, but not too much. We consider the RevCol design already enlarges the effective receptive field and this limit the accuracy gain of using large kernel convolution. On the other hand, 3 × 3 convolution enjoys the merits of efficiency and stability in (pre)training. Therefore, we adopt kernel 3 in all RevCol models.\n\n19\n\n02,0004,0006,0008,00010,00012,00014,00016,00018,00020,00012345678910GPUMemory (MB)# ColumnReversibleNon-ReversiblePublished as a conference paper at ICLR 2023\n\nE SEMI-LABELED PRIVATELY COLLECTED DATASET FOR LARGE MODELS\n\nE.1 DATA COLLECTION AND PSEUDO LABEL SYSTEM\n\nThe dataset consists of around 168 million(M) images, 50M of which labeled and the remaining 118M unlabeled. The majority of labeled images come from public datasets, e.g. ImageNet, Places365 (Zhou et al., 2017a), and Bamboo (Zhang et al., 2022b). The others are web-crawled images annotated by in-door employees. Unlabeled images come from weakly-annotated image-text datasets like YFCC-100M (Thomee et al., 2016). We do not use text annotations.\n\nIn order to utilize images of different label domains and the massive unlabeled images, we employ a multi-target label system similar to Ding et al. (2022a) and Ghiasi et al. (2021). We adopt a semisupervised learning strategy with ViTs, thus generating pseudo labels with continuously increased quality. We only store soft predictions with confidence higher than 1% to save storage. The final version of pseudo label we use are generated by a multi-head ViT-Huge teacher, which has an 89.0% ImageNet-1k accuracy.\n\nE.2\n\nIMAGE DEDUPLICATION\n\nSince the dataset contains large amount of unverified web-crawled images, there are probably validation or test images sneaking into our training dataset. Works like Mahajan et al. (2018) and Yalniz et al. (2019) all regard image deduplication an important procedure for fair experiments.\n\nWe first iterate over the entire dataset to filter out suspicious duplicates together with the corresponding test images based on their pseudo label distance. This brings more than 10,000 images with high suspicion. We look at these image pairs and finally find about 1,200 exact-duplicates and nearduplicates. Fig. 7 shows some examples of the near-duplicates, which are difficult to detect. Never the less, training a model without removing these duplicates gives less than 0.1% accuracy gain on ImageNet-1k in our experiments. We attribute this to the absence of true labels from these duplicates.\n\nFigure 7: Top: Near duplicates found in unlabeled images. Bottom: ImageNet-1k validation images.\n\nF MORE TRAINING DETAILS\n\nThis section gives more training details on ImageNet classification, COCO detection, and ADE20K segmentation.\n\nF.1\n\nINTERMEDIATE SUPERVISION SETTINGS\n\nWe add intermediate supervision in ImageNet-1k training, ImageNet-22k and extra data pre-training. We used a 3-block decoder with gradually up-sampled feature maps in ImageNet-1k training. The block setting remains the same as Sec. 2.2 We use a single layer decoder in ImageNet-22k and extra data pre-training. For all the variants of RevCol, we set the number of compound loss n to 3 empirically (eg. for a 8 column RevCol, the intermediate supervision is added to column 2, 4, and 6, and the original classification CE loss is also added to column 8). αi is set to 3, 2, 1, 0 and βi is set to 0.18, 0.35, 0.53, 1.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nF.2 HYPERPARAMETERS USED FOR TRAINING AND PRE-TRAINING\n\nThis section introduces the training details for main experiments, the supervised training on ImageNet and extra data. We show this setting in Tab. 11. All experiments in ablation studies are superivised trained on ImageNet-1K except additional descriptions and also follow settings described in this section.\n\nTable 11: Hyperparameters for training and pre-training RevCol.\n\nHyperparameters\n\nInput resolution Training epochs Warmup epochs Batch size Peak learning rate Learning rate schedule Layer-wise learning rate decay AdamW momentum Weight decay Gradient clipping Drop path EMA\n\nLabel smoothing ε Data augment Mixup CutMix Random erase\n\nImageNet-1K ImageNet-22K 168M Extra Data\n\nT/S/B\n\nB/L/XL\n\nXL/H\n\n300 20\n\n4e-3\n\n0.05\n\n2242\n\n4096\n\ncosine ✗\n(0.9, 0.999)\n\n✗\n\n0.1/0.3/0.4 0.9999\n\n90 5\n\n5e-4\n\n0.1\n\n0.3 ✗\n\n0.1 RandAug (9, 0.5) 0.8 1.0 0.25\n\n2242 10 0.15 5120 6.25e-4 cosine ✗\n(0.9, 0.999) 0.05 1.0 (element-wise) 0.2 ✗\n\n0.1 RandAug (9, 0.5) ✗\n✗ ✗\n\nF.3 HYPERPARAMETERS USED FOR FINE-TUNING\n\nThis section gives the hyperparameters used for fine-tuning on ImageNet-1K and downstrea COCO object detection and instance segmentation, ADE20K semantic segmentation tasks, as shown in Tab. 12, Tab. 13 and Tab. 14.\n\nTable 12: Hyperparameters for fine-tuning RevCol on ImageNet-1K classification.\n\nHyperparameters\n\nInput resolution Fine-tuning epochs Warmup epochs Batch size Peak learning rate Layer-wise learning rate decay AdamW momentum Weight decay Learning rate schedule Head init scale Drop path EMA Gradient clipping\n\nLabel smoothing ε Data augment Mixup CutMix Random erase\n\nImageNet-1K\n\nB/L/XL/H\n\n3842/3842/3842/6402 30 0\n512 5e-5 0.9/0.8/0.8/0.8 (0.9, 0.999) 1e-8 cosine 0.001 0.2/0.3/0.4/0.5 ✗/✗/✗/0.9999 10.0 (norm)\n\n0.1 RandAug (9, 0.5) ✗\n✗ 0.25\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nTable 13: Hyperparameters for fine-tuning RevCol on object detection with Cascade Mask R-CNN detector.\n\nHyperparameters\n\nFine-tuning epochs Batch size Peak learning rate Warmup steps Layer-wise learning rate decay\n\nAdamW momentum Weight decay Drop path\n\nIN-1K Pre-trained\n\nIN-22K Pre-trained\n\nRevCol-T/S/B\n\nRevCol-B/L\n\n36 16\n\n1500\n\n1e-4\n\n0.9/0.8\n\n2e-4\n\n0.85/0.8/0.8\n\n(0.9, 0.999) 0.05\n\n0.3/0.4/0.4\n\n0.5/0.6\n\nTable 14: Hyperparameters for fine-tuning RevCol on ADE20K semantic segmentation with UperNet segmentation framework.\n\nHyperparameters\n\nInput resolution Fine-tuning steps Batch size Peak learning rate Warmup steps Layer-wise learning rate decay\n\nAdamW momentum Weight decay Drop path\n\nIN-1K Pre-trained\n\nIN-22K Pre-trained\n\nRevCol-T/S/B\n\nRevCol-B/L\n\n5122\n\n6402\n\n80k 16 4e-5 1500\n\n1.0\n\n0.9\n\n(0.9, 0.999) 0.01 0.3\n\nF.3.1 CONVOLUTION KERNEL PADDING TRICK IN DOWN-STREAM TASKS\n\nAccording the results shown in Section D.5, larger kernel convolution perform better especially in down-stream tasks. To save the pre-training cost meanwhile achieve better performance, we pad the small 3 × 3 convolution kernel in pre-trained model weights to larger size then fine-tune in detection and segmentation tasks. Inspired by Net2net (Chen et al., 2015) method, we pad the pre-trained 3 × 3 kernel in convolution layer with Gaussian initialized values. To protect the pre-trained kernel from being disturbed by the new padded values, we initialize the padded values with 0 mean and extremely small standard deviations (1e-7). We use this trick only with our largest model RevCol-H. We pad the 3 × 3 kernel in pre-trained model to 7 × 7 kernel size in COCO detection task and 13 × 13 in ADE20k sementatic segmentation task, then fine-tune on corresponding dataset to get the final result. In general, the kernel padding trick leads to 0.5∼0.8 APbox improvement and 0.7∼1.0 mIoU improvement for RevCol-H model.\n\nG VISUALIZATIONS OF FEATURE DISENTANGLEMENT\n\nIn this section, we show our RevCol can disentangle features with stacked columns, which is different from the conventional sequential networks. We use RevCol-S pre-trained on ImageNet-1K for analysis. First, we visualize the class activation maps (CAMs) for outputs of each last layer of a level. We adopt LayerCAM (Jiang et al., 2021) technology to generate the CAMs with the predicted classes. Fig. 8 show the heatmaps of activation. With the levels and columns going deeper, the features focus on the regions with more semantics. The outputs of RevCol-S are the different levels of last column. These features with high level semantics focus on different parts of the image and the whole part of the object, achieving disentanglement of features for task-relevant and task-irrelevant.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: Visualizations of class activation maps using LayerCAM (Jiang et al., 2021) for different levels and columns.\n\nFigure 9: CKA similarities (Kornblith et al., 2019) of features and images/labels for different levels and columns.\n\nTo quantify the disentanglement, we use Centered Kernel Alignment (CKA) similarity metric (Kornblith et al., 2019) to measure the similarity between representations in RevCol-S. We calculate the CKA similarities between intermediate features in different levels and columns and images or labels of each category in the ImageNet val set. Then we plot the similarities of the category with\n\n23\n\nLevel 1Level 2Level 3Level 4Column 1Column 2Column 3Column 4Column 5Column 6Column 7Column 8Level 1Level 2Level 3Level 4Column 1Column 2Column 3Column 4Column 5Column 6Column 7Column 8Level 1Level 2Level 3Level 4Level 1Level 2Level 3Level 4Col 1Col 2Col 3Col 4Col 5Col 6Col 7Col 8Col 1Col 2Col 3Col 4Col 5Col 6Col 7Col 8Published as a conference paper at ICLR 2023\n\nthe highest label similarity in Fig. 9. As shown in the figure, the similarities between images and intermediate features are not clearly distinguished at different levels in Column 2-5, while the features with higher levels have lower similarity to the images in Column 6-8. The similarities between labels and intermediate features are also more distinct in higher columns.\n\n24",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a new neural network paradigm, which aims at gradually disentangling features in the forward propagation. The whole network is constructed with multiple copies of sub-networks. The total information can be maintained rather than compressed during propagation. The proposed method is also evaluated on several typical computer vision tasks and shows competitive performance.\n\n# Strength And Weaknesses\n\n**Strength**  \n* It is important to explore strong and disentangled representations by designing new neural architectures. The proposed reversible column manner is reasonable to maintain both high- and low-level information.\n* The proposed method shows promising experimental performance.\n\n**Weaknesses**\n* It is expected to explain the intrinsic difference between the proposed method and previous ones, including but not limited to HRNet and NAS (neural architecture searched) networks. Especially, networks [1,2] searched in the cell-based/-like search space have many in common with the proposed one. A more comprehensive analysis is suggested, not only the provided demonstration from the motivation perspective.\n* The shown experimental performance is competitive but the promotion over previous ones is not so evident. I admit the number is almost saturated, but any other advantage this method could bring needs to be demonstrated.\n* Real throughput/latency needs to be measured to more accurately validate the model budget, not just FLOPs or #Params. The introduced connections seem to introduce larger latency on real hardware which is not so related to FLOPs numbers.\n\n[1] Liu C, Chen L C, Schroff F, et al. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 82-92.   \n[2] Xie S, Kirillov A, Girshick R, et al. Exploring randomly wired neural networks for image recognition[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 1284-1293.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n* **Clarity**: most contents are clear and easy to follow.\n* **Quality**: The proposed method is evaluated and studied on substantial benchmarks and settings.\n* **Novelty**: The design principles have some in common with previous methods, but some details are new.\n* **Reproducibility**: Most implementation details are provided but the code is not available.\n\n# Summary Of The Review\n\nThe overall idea is good while the main concerns lie in the demonstration of advantages or differences with previous methods and speed analysis.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nOOD-CONTROL: OUT-OF-DISTRIBUTION GENERALIZATION FOR ADAPTIVE UAV FLIGHT CONTROL\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nData-driven control methods have demonstrated precise and agile control of Unmanned Aerial Vehicles (UAVs) over turbulence environments. However, they are relatively weak at taming the out-of-distribution (OoD) data, i.e., encountering the generalization problem when faced with unknown environments with different data distributions from the training set. Many studies have designed algorithms to reduce the impact of the OoD problem, a common but tricky problem in machine learning. To tackle the OoD generalization problem in control, we propose a theoretically guaranteed approach: OoD-Control. We provide proof that for any perturbation within some range on the states, the control error can be upper bounded by a constant. In this paper, we present our OoD-Control generalization algorithm for online adaptive flight control and execute it in two instances. Experiments show that systems trained by the proposed OoD-Control algorithm perform better in quite different environments from training. And the control method is extensible and pervasively applicable and can be applied to different dynamical models. OoD-Control is validated on UAV dynamic models, and we find it performs state-of-the-art in positioning stability and trajectory tracking problems.\n\n1\n\nINTRODUCTION\n\nUAVs have gained considerable attention and are widely used for various purposes because of their high manoeuvrability and flexibility. For example, quadrotors are widely deployed for inspection, reconnaissance, and rescue. As control strategies evolve, novel scenarios for UAVs, such as aerial grasping, transporting, and bridge inspection (Ruggiero et al., 2018), require more precise trajectory tracking. Especially in the outdoor environment, unpredictable and changing wind field conditions pose substantial challenges to the stability of UAVs. Rotor blades are affected by induced airflow caused by the wind, which creates complex and non-stationary aerodynamic interactions (see Appendix B.6.3). From security and policy perspectives, demonstrating that UAVs can operate safely and reliably in unpredictable environments with various distributions is an essential requirement. It is also the premise for future medical robots, autonomous cars, and manned aerial vehicles to be widely accepted.\n\nMany areas have benefited from data-driven approaches. However, they are susceptible to performance degradation after generalization. And the majority of deep learning algorithms heavily rely on the I.I.D assumption for data, which is generally violated in practice due to domain generalization (Zhou et al., 2022). Nevertheless, neural networks may lose their robustness when confronted with OoD data. Many cases of failure in DNN originate from shortcut learning in the learning process (Geirhos et al., 2020). The damage to the UAV is undoubtedly considerable if the UAV cannot adjust to the changing environment, i.e., it is unstable or even crashes in an OoD situation. One significant objective of this paper is to propose a control algorithm to enable UAVs to maintain accurate control even in the case of environment domain shifts.\n\nOur Contributions. UAVs interact with the changing environment, resulting in complex environment-dependent uncertain aerodynamics, called unknown dynamics, that are tricky to model and significantly impact precise control. Previous data-driven controllers attempt to solve the problem by estimating the unknown dynamics, while the estimation accuracy and the performance of the controllers are limited by the environment domain shifts in tests. This paper presents a methodology\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nfor adaptive flight control problems, focusing on enabling UAVs to fly under unknown environments. Compared with previous works, our proposed OoD-Control algorithm can provide performance guarantees under domain shifts of the environment distribution. Compared with previous state-of-the-art work (Shi et al., 2021), the proposed OoD-Control method does not require strong assumptions, for example, e-ISS stability and a fully actuated system. Additionally, our algorithm has a greater capacity for generalization. For different distributions of the environment, we show theoretically that the bound on the prediction error of the unknown dynamics remains constant over a certain range of perturbations. Besides, simulated results under challenging aerodynamic conditions indicate that the OoD-Control algorithm achieves better control performance than the SOTA deep learning algorithms.\n\n2 RELATED WORK\n\n2.1 FLIGHT CONTROL ALGORITHMS\n\nUAVs have found broad applicability in a variety of fields and have attracted the attention of several researchers. Many published studies describe the significance and efficiency of flight control algorithms, including PID Control (Szafranski & Czyba, 2011), LQR Control (Priyambodo et al., 2020), Sliding Mode Control (Chen et al., 2016), Backstepping Control (Labbadi & Cherkaoui, 2019), Robust Control (Hasseni & Abdou, 2021), etc. However, most of the previously mentioned control methods suffer from limitations. Imprecise system modelling and non-modelled environmental disturbances may result in unacceptable performance or instability.\n\nToday, artificial intelligence triggered a new wave of research in many fields (Jumper et al., 2021; Silver et al., 2017). The data-driven control methods can directly learn the corresponding control strategy from the interaction process of the controlled system so that it can adapt to new environments. Bansal et al. (2016) validates their proposed deep learning algorithm on a quadrotor testbed. On the other hand, reinforcement learning is a model-free algorithm widely used for control problems. Koch et al. (2019) present an intelligent high-precision flight control system using the reinforcement learning algorithm for UAVs. Moreover, the performance and accuracy of the internal control loop for quadrotor attitude control are analyzed and compared. Results indicate that the neural network has good generalization abilities and can learn the quadrotor dynamics accurately and apply them to the control system. Underwood & Husain (2010) propose an online parameter estimation, and the experimental results validate the effectiveness of the adaptive control method. O’Connell et al. (2022) have combined online adaptive learning with representation learning and adapted a DNN to learn a nonlinear representation. However, the environment’s diversity is not considered in this work. Adapting to an environment completely different from the training set is challenging. Inspired by Shi et al. (2021), mechanical-based models with learnable dynamics and DNNs are constructed in this study for their interpretability and stability. We further investigate in this paper whether the robustness of the algorithm can be improved with OoD generalization methods.\n\n2.2 OUT-OF-DISTRIBUTION GENERALIZATION\n\nOut-of-Distribution (OoD) generalization, which involves generalizing under data distribution domain shifts, is an active research area in the community. Generalizing a prediction model under distribution shifts is the process of generalizing its performance. Many algorithms have been proposed to achieve the OoD generalization, including meta-learning (Li et al., 2019; Zhang et al., 2020), prototypical learning (Dubey et al., 2021), gradient alignment (Rame et al., 2022), domain adversarial learning (Akuzawa et al., 2019; Xu et al., 2020) and kernel methods (Li et al., 2018; Ghifary et al., 2016) etc.\n\nLiterature has extensively discussed how to deal with domain shift, and the OoD generalization problem is extensively studied in computer vision (Hsu et al., 2020), natural language processing (Hendrycks et al., 2020), speech recognition (Shankar et al., 2018), and other fields, but seldom in the context of online control. In Shi et al. (2021), a multi-task learning method for nonlinear systems was presented that can withstand disturbances and unknown environments.\n\nPrevious studies have suffered from shortcomings in lacking a discussion about the misspecification of dynamics systems and neglecting the gap between the simulation experiments and reality.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nAdditionally, the generalization of flight control is plagued by system measurement errors and unknown environmental parameters like wind modes and air density and resistance. In this work, we demonstrate the average control error can be upper-bounded by a constant when the environmental disturbances are within some range. Moreover, experimental results indicate that the OoD-Control is robust to shifts in the environmental domain.\n\n3 PROBLEM FORMULATION\n\nNotations We use subscripts (e.g., t in x(z) in x(z) environmental distribution, ∥ · ∥ denotes the 2-norm of a vector.\n\n) to denote the time index and superscripts (e.g., (z) ) to denote the dynamics state under the environmental perturbation z ∼ π0, where π0 is the\n\nt\n\nt\n\nNote: Superscript (z) in x(z) environmental distribution z ∼ π0 and x(z) environment.\n\nt\n\nis used as a symbol to represent that the state is perturbed by the t = xt + z, where xt ∈ Rn is the state in the windless\n\nIn this paper, we consider a discrete nonlinear control system whose dynamics are described by the following formula:\n\nt+1 = f0(x(z) x(z)\n\nt\n\n) + B(x(z)\n\nt\n\n)ut − f (x(z)\n\nt\n\n, c) + wt,\n\n1 ≤ t ≤ T,\n\n(1)\n\nt\n\nwhere x(z) t ∈ Rn is the state variable. z ∈ Rn changes with the environmental distribution domain shifts. B(x(z) ) : Rn → Rn×m is the state-dependent actuation matrix; ut ∈ Rm is the control input on the dynamic system; f0(x(z) ) : Rn → Rn is the known nominal dynamic term that can be modelled with well-defined differential equations; f (x(z) , c) : Rn × Rh → Rn are the unknown environment-dependent dynamics that are hard to be modelled and c is the unknown environmental parameter, we also use f (x(z) ) for short in the following paragraphs; wt ∈ Rn is the random noise vector.\n\nt\n\nt\n\nt\n\nWe hypothesize that the environmental disturbance wt and the control ut are bounded. Due to the structural restrictions of the actuators, there are certain limits to the output. For example, the control output of the UAV is constrained by the maximum rotor blades’ revolutions per minute (RPM). Here we give the boundedness Assumption. Assumption 1 (Bounded controls and disturbances) Assume that the controller’s output has an upper bound: ∀t, ∥ut∥ ≤ U . Moreover, the environmental noise vectors are also bounded with zero expectations: ∀t, ∥wt∥ ≤ W , E(wt) = 0. Definition 1 (Average Control Error under disturbances) The control error of the system under disturbance distribution π0 at t is calculated as ∥Ez∼π0(x(z) t ∥. The average control error t\nACEπ0 of T time steps is defined as the performance metric:\n\n) − xd\n\nACEπ0 =\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥Ez∼π0(x(z)\n\nt\n\n) − xd\n\nt ∥\n\n(2)\n\nwhere xd\n\nt denotes the desired states at t.\n\nRemark 1 In Definition 1, we focus on fixed-point hovering and trajectory tracking. A sequence of perturbations matching π0 is obtained by sampling for N times: (z1, z2, · · · , zN ). And the perturbed states sequence is also derived at time t: (x(z1) )∥ can be t\napproximated with Monte Carlo method: (cid:80)N i=1 x(zi) /N , where the subscripts i represent the index of the ith sample. Compared with the average control error definition in Shi et al. (2021); ̊Astr ̈om & Murray (2008), ACEπ0 represents the expectation of the difference between the actual states and the desired states of the dynamical system under environmental perturbations.\n\n). ∥Ez∼π0 (x(z)\n\n, · · · , x(zN )\n\n, x(z2)\n\nt\n\nt\n\nt\n\nt\n\nInteraction protocol. We set the study of the OoD adaptive flight control problem under the following interaction protocol:\n\n1. Stochastically selects an environment for the controller to encounter every time step, which\n\ndepends on the unobserved variable c (e.g., wind condition and air density).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n2. The controller interacts with the environment and observes the state x(z) 3. Optionally changes c after a short time and repeats from Step 1.\n\nt\n\nto take action ut.\n\n4 METHODOLOGY\n\nWe expect the UAV to learn the shared representation of the unknown dynamics between different environments so that it can generalize well in unseen areas with few adaptations. This section will introduce the methodology that provides the guaranteed upper bound for the prediction errors of the unknown dynamics.\n\nt\n\nt\n\n, ˆc) = F(φ(x(z)\n\nNotations and settings. For modelling unknown environment-dependent dynamics, we use ˆf (x(z) ; Θ), ˆc) inspired by (Shi et al., 2021). That means we consider the unknown environment-dependent dynamics typically consist of two coupling parts: the irregular higher-order aerodynamics of UAVs that are caused by the complex streamlined design, and the environmentdependent variables which encode wind field information. φ(x(z) ; Θ) represents the former, which is a deep neural network (DNN) with L layers parameterized by Θ. ˆc is the latter, which is particular for a certain environment. This model enables us to consider the joint higher-order effects beyond nominal dynamics and enable agile control of UAVs. We use ˆf (x(z) ) for brevity in following paragraphs.\n\nt\n\nt\n\nt\n\n, c)] and ˆfπ0(x(z)\n\nAssume an environmental distribution π0, the perturbed unknown dynamics and the predictable value is denoted as the expectation of the states under environmental disturbances: fπ0 (x(z) ) = Ez∼π0[f (x(z) , ˆc)]. In OoD-control, t\nthe objective is to minimize the unknown dynamics prediction loss for controller inputs: lf = ∥fπ0(x(z) )∥) and we also use ̃h(x(z) ) for short in the following paragraphs. ̃h(·) satisfies: 1) ̃h(·) ∈ [0, 1]; 2) ̃h(·) is monotonically decreasing and the inverse ̃h−1(·) exists. In the next section, we will propose a framework that provides a guaranteed upper bound for the ACE under OoD-Control.\n\n)∥. The loss is mapped to [0, 1] by ̃h(∥fπ0 (x(z)\n\n; Θ), ˆc)] = Ez∼π0[ ˆf (x(z)\n\n) = Ez∼π0[F(φ(x(z)\n\n)− ˆfπ0(x(z)\n\n)− ˆfπ0 (x(z)\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\n4.1 THE PROOF OF ACE’S UPPER BOUND\n\nNext, we will introduce a methodology to tackle the problem of generalization and give proof of ACE’s upper bound. We want to verify that for any perturbation in B = {δ ∈ Rn : ∥δ∥2 ≤ r} with radius r, the lower bound of the prediction error maintains constant under unpredictable disturbance, i.e., ∀∥δ∥ ≤ r, ∃p > 0, ̃hπ0(x(z) t +δ) > p. This conclusion is significant for the calculation of the upper bound for ACE. For a perturbation of radius r, the expectation of the prediction of the unknown dynamics remains the same. Assume H is a function class, which includes ̃hπ0(·) and satisfies H = {h : h(x) ∈ [0, 1], ∀x ∈ Rn}. Performing the following optimization will result in a guaranteed lower bound. If H includes only ̃hπ0(·), the bound is exact:\n\n) > p, it still holds ̃hπ0 (x(z)\n\nt\n\nmin δ∈B\n\n ̃hπ0(x(z)\n\nt + δ) ≥ min\n\nh∈H\n\n(cid:110)\n\nhπ0(x(z)\n\nt + δ) s.t. hπ0(x(z)\n\nt\n\n) = ̃hπ0 (x(z)\n\nt\n\n)\n\n(cid:111)\n\n.\n\n(3)\n\nmin δ∈B\n\nTheorem 1 (Lagrangian) Lπ0 (H, B) is denoted as the lower bound in equation 3. Lagrangian methods can be adapted to solve inequality:\n\nLπ0 (H, B) = min h∈H\n\nmin δ∈B\n\nmax λ∈R\n\nL(h, δ, λ) ≜ min\n\nh∈H\n\nmin δ∈B\n\nmax λ∈R\n\n(cid:110)\n\nhπ0(x(z)\n\nt + δ) − λ[hπ0(x(z)\n\nt\n\n(cid:111)\n\n)]\n\n.\n\n) − ̃hπ0 (x(z) (4)\n\nt\n\nExchanging the min and max yields the following dual form:\n\nLπ0 (H, B) ≥ max λ≥0\n\nmin h∈H\n\nmin δ∈B\n\nL(h, δ, λ) = max λ≥0\n\n(cid:26)\n\nλ ̃hπ0(x(z)\n\nt\n\n) − max δ∈B\n\n(cid:27)\n\nDH(λπ0 ||πδ)\n\n(5)\n\nwhere πδ max h∈H\n\nrepresents the distribution of z + δ when z ∼ π0 and DH(λπ0 ||πδ) = )]} = (cid:82) [λπ0(z) − πδ(z)]+dz.\n\n)] − Ez∼πδ [h(x(z)\n\n{λEz∼π0[h(x(z)\n\nt\n\nt\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nCorollary 1 (Gaussian noise) With Gaussian noise π0 = N (0, σ2I) and bounded disturbance B = {δ : ∥δ∥2 ≤ r}, the lower bound in equation 5 satisfies:\n\nLπ0(H, B) = max λ≥0\n\n(cid:26)\n\nλ ̃hπ0(x(z)\n\nt\n\n) − max δ∈B\n\nDH(λπ0||πδ)\n\n(cid:27)\n\n≥ Φ(Φ−1( ̃hπ0 (x(z)\n\nt\n\n) −\n\nr σ\n\n)\n\n(6)\n\nwhere Φ(·) represents the Gaussian Cumulative Density Function (CDF). For the case p=0.5, i.e., ̃hπ0(x(z) )). As a side note, the Monte Carlo method for perturbation radius calculation is also given by Algorithm 2 in Appendix B.3.\n\n) > 0.5 , the radius satisfies r ≤ σΦ−1( ̃hπ0(x(z)\n\nt\n\nt\n\n4.2 AVERAGE CONTROL ERROR BOUND\n\nt\n\nt\n\n, x(z2)\n\n, . . . , x(zN )\n\nThe selection of ̃h. Given a sequence of state variables at t under perturbation z ∼ π0 = N (0, σ2): X = (x(z1) ). Moreover, the predicted and unknown dynamics sequences are defined as Fp = ( ˆf (x(z1) )). Let Dp be the discrepancy sequence between Fp and Fu. Dp = (∥ ˆf (x(z1) ) − f (x(z2) under a given threshold εt. By simulating with a large sample size N , ˆp is calculated as:\n\nt )∥). Denote by ˆp the successful rate for the prediction error\n\n)) and Fu = (f (x(z1)\n\n)∥, . . . , ∥ ˆf (x(zN )\n\n), . . . , f (x(zN )\n\n), . . . , ˆf (x(zN )\n\n) − f (x(zN )\n\n)∥, ∥ ˆf (x(z2)\n\n) − f (x(z1)\n\n), f (x(z2)\n\n), ˆf (x(z2)\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nˆp ≜ P(∥ ˆf (x(z)\n\nt\n\n) − f (x(z)\n\nt\n\n)∥ < εt) =\n\nna N\n\n(7)\n\nwhere na is the number of elements in Dp less than εt. Recalling the ̃h function’s requirements, we can instantiate ̃h as follows:\n\n ̃h(∥ ˆf (x(z)\n\nt\n\n) − f (x(z)\n\nt\n\n)∥) ≜ ˆp − k\n\n(cid:114)\n\nˆp(1 − ˆp) n\n\n(8)\n\n2 ) is the 1 − α\n\nwhere k = Φ−1(1 − α 2 quantile of a standard normal distribution. Moreover, this equation represents the lower confidence bound estimation of the error under a given confidence level α. As noise increases, the predicted value will deviate from the actual value by a more significant amount. Therefore, the ̃h will decrease monotonically. Besides, the range of the lower confidence bound lies in [0, 1], which satisfies both requirements for ̃h as discussed in the previous section.\n\nLet b denote the lower confidence bound in equation 8. In section 4.1, a proof is given that ̃hπ0(x(z) )\nand ̃hπ0(x(z) t + δ) has the same lower bound under the disturbances ∥δ∥ ≤ r. The radius ensuring an equal lower bound under the perturbation in this paper is: (cid:114)\n\nt\n\nr = σΦ−1(b) = σΦ−1(ˆp − k\n\n).\n\n(9)\n\nˆp(1 − ˆp) n\n\nControlling. The control term u consists of three parts: feedback, feedforward, and residual, where the feedback part gets information from sensors and minimizes the gap between x(z) and t , the feedforward part offsets the nominal term f0(x(z) xd ), and the residual part counterweights unknown environment-dependent term f (x(z)\n\n) is the pseudo-inverse B(x(z)\n\n). B†(x(z)\n\n).\n\nt\n\nt\n\nt\n\nt\n\nt\n\nThe controller of model-based control is ut = B†(x(z)\n\nt\n\nThus, the equation 1 becomes:\n\n)(−f0(x(z)\n\nt\n\n) + ˆf (x(z)\n\nt\n\n)).\n\nxt+1 = ˆf (x(z)\n\nt\n\n) − f (x(z)\n\nt\n\n) + wt.\n\n(10)\n\n(11)\n\nLemma 1 (ACE bound in ideal case) For any perturbation in B = {δ : ∥δ∥2 ≤ r}, the theoretical average control error is bounded as:\n\nACEπδ =\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ [ ˆf (x(z)\n\nt\n\n)] − Ez∼πδ [f (x(z)\n\nt\n\n)]∥ ≤ ̃h−1(p).\n\n(12)\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nRemark 2 Average control error is related to the prediction error of unknown dynamics and environmental perturbations in the ideal case. Compared with the upper bound calculated in Shi et al. (2021), the derived bound is more general. Our calculation method does not require the system to be fully actuated and does not require the nominal dynamics to be exponentially input-to-state stable (e-ISS). The assumption of e-ISS is too strong. And many dynamic systems are under-actuated in the real world, such as quadrotors. Corollary 2 (ACE bound under control actuation misspecification) ∆B(x(z) misspecification in the actuation matrix, and the ACEπδ satisfies:\n\n) is the parametric\n\nt\n\nACEπδ =\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ (x(z)\n\nt\n\n) − xd\n\nt ∥ ≤ ̃h−1(p) +\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ [∆B(x(z)\n\nt\n\n)B†(x(z)\n\nt\n\n)ef (x(z)\n\nt\n\n)]∥\n\nt\n\n) = ˆf (x(z)\n\nwhere ef (x(z) Lemma 2 (Trajectory tracking ACE of quadrotor) In this paper, for environmental disturbances in B = {δ : ∥δ∥2 ≤ r}, the quadrotors’ trajectory tracking error formula is given as:\n\n, c) − f0(x(z)\n\n).\n\nt\n\nt\n\nACEπδ =\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ (x(z)\n\nt\n\n) − xd\n\nt ∥ =\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ [C1er1x(z)\n\nt + C2er2x(z)\n\nt\n\n] − ̃h−1(p)/Kv∥\n\n(13)\n\nwhere C1 = −xd , c) − ˆf (x(z) f (x(z)\n\nt − C2 + ε/Kv, C2 = (ε + (Kvxd\n\nt )/Kv(er2xd , ˆc), which is the prediction error of the unknown dynamics.\n\nt − ε)er1xd\n\nt\n\nt\n\nt − er1xd\n\nt ) and ε =\n\nRemark 3 Note that Lemma 2 gives the trajectory tracking error that can be calculated. Based on the previous description, it was demonstrated that the errors for unknown dynamics under perturbation have the same bound. The detailed proof can be found in Appendix A.5.\n\n4.3 OOD GENERALIZATION ALGORITHM\n\nBased on the theoretical analysis above, we propose an algorithm—out-of-distribution generalization for adaptive flight control named OoD-Control. We focus on minimizing the prediction loss and learning Θ during the simulation. We intend to design an OoD-controller with lower ACEs that converges the estimated unknown dynamics ˆf (x(z) ) under environment distribution domain shifts.\n\n) faster to the true dynamics f (x(z)\n\nt\n\nt\n\nThe proposed OoD-Control algorithm is shown in Algorithm 1 (see Appendix B.1). Given a set of distribution functions X, χ ∈ X are picked for each iteration. The wind velocity is a series of random variables sampled from χ. (Specifically, we use X for the training distribution set with each member denoted as χ. For the testing set, we use Ω and ω instead.) Each time-series simulation begins with random noise ε1 being introduced to the structural parameters of the system. At each iteration, the predicted loss, which measures the error between the unknown dynamics and its prediction is minimized. After the unknown dynamic predictor is trained, it can be used for model-based control as discussed in Section 4.2.\n\n5 EXPERIMENTS AND RESULTS\n\nIn this section, numerical experiments on the inverted pendulum and quadrotors will be conducted to demonstrate the effectiveness of the proposed OoD-control algorithmic framework. To better understand the proposed OoD-Control algorithm and environment setting, we choose an uncoupled dynamics model, the inverted pendulum, as the introductory example before the quadrotor instance.\n\n5.1 DYNAMICS MODELING\n\nInverted Pendulum. Consider an inverted pendulum, the dynamic model of the pendulum is:\n\nml2 ̈θ − mlgsinθ = u + f (θ, ̇θ, c)\n\n(14)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nwhere θ represents the angle away from the center, l is the length of the arm, g is gravitational acceleration, l is the length of the pendulum’s arm, and m is the mass. The state variable consists of θ and ̇θ, which could be measured by position and inertia sensors. f (·) represents the unknown dynamic term, including air resistance, wind force and modelling misspecification, subject to θ, ̇θ and environment parameter c. u is the controlling term. Our goal is to keep the pendulum closer to the center, i.e., minimize average control error.\n\nQuadrotor. The quadrotor is a plane model where the four rotors are always on the same plane. So the quadrotor adjusts its attitude by setting different rotation speeds for the four rotors. We define the dynamic model of quadrotor as:\n\nm ̇v = mg + R(θ)fT + f J ̈θ = J ̇θ × ̇θ + τ\n\n(15)\n\n(16)\n\nwhere θ represents the attitude angel of the quadrotor; R(θ) ∈ R3×3 is the attitude rotation matrix subject to θ; J is the inertia matrix of the quadrotor, fT is the force imposed on the system; τ is the total torque; m is the mass of quadrotor and g is the gravitational acceleration; fT and τ are subject to the speeds of rotors nr ∈ R1×4. In the experiments, the goal is to maintain the quadrotor’s position states or follow a given trajectory under turbulent environments.\n\n5.2 COMPARISON METHOD\n\nIn the experiments, the proposed adaptive UAV flight control algorithm OoD-Control are compared with OMAC (Shi et al., 2021) and no-adapt (PID) method. OMAC (online meta-learning adaptive control) is the state-of-the-art data-driven UAV flight control method. In the OMAC paper, three versions of OMAC are provided with different model specifications: convex, bi-convex, and deep learning. We illustrate the results of deep learning because it is the best-performing version of OMAC.\n\nMeanwhile, the no-adapt method and the omniscient method are compared in this paper. No-adapt indicates the controller cannot perceive the environmental domain shifts with ˆf (x(z) ) = 0 which is just the conventional PID controller. omniscient is the controller which has access to the unknown dynamics perfectly, i.e., ˆf (x(z) ). Among all the controllers, no-adapt and omniscient are the two extremes, with no-adapt being unable to predict while omniscient can do so with zero error. We run each simulation ten times with different random seeds to obtain the mean and standard deviation of ACE under perturbation for rigorousness.\n\n) = f (x(z)\n\nt\n\nt\n\nt\n\n5.3 WIND FIELD CONSTRUCTION AND FLIGHT TRAJECTORY DESIGN\n\nWind fields can be derived according to the Navier-Stokes (N-S) equations and the continuum equation. However, in practice, the N-S equations are generally hard to be solved due to their high computational cost. For turbulent wind field simulations, the Dryden model (Specification, 1980) is widely used. We refer to the Dryden model to simulate turbulent wind fields on quadrotors by generating Gaussian wind disturbances (Beal, 1993).\n\nTo construct realistic situations in the inverted pendulum and quadrotor experiment, we simulated two types of winds: turbulent wind and gust. For turbulent winds, the speed and direction change at any time. In the case of gusts, the wind speed remains constant over a period of time. For further study, we divided the two wind fields turbulent wind and gust into three categories respectively according to their strength: breeze, strong breeze and gale. The direction and strength of the turbulent winds change continuously and the wind forces are applied to the object. This requires higher manoeuvrability to maintain stability. The wind environment setting in the experiment can be found in Appendix B.6.1.\n\nQuadrotors must also be capable of flying along the desired trajectory and hovering at a fixed point for various applications, such as inspection, patrol, and delivery. In order to meet the requirements of different application scenarios, we design a variety of trajectories to test the performance of the proposed OoD-Control under different situations. The designed trajectory can correspond to a specific application scenario, hovering for fixed-point photography, figure-8 trajectories for scenarios\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nrequiring high manoeuvrability, the spiral trajectory for power lines detection, and sin-forward for transporting items in the forests or area scanning. The mathematical forms of the trajectories are shown in Appendix B.6.1.\n\n5.4 RESULTS\n\nPendulum. Table 1 (see Appendix B.2) and Figure 1 illustrate the average and standard deviation of the control errors in different testing environments. We mainly compare the OoD-Control algorithm with the OMAC. And for the completeness of the experiments, we also set two control groups: the no-adapt and the omniscient.\n\nAs shown in row 3 of Table 1, our OoD-Control algorithm performs significantly better than the OMAC in the gale dataset. That means the former generalizes better than the latter when meeting a large environmental distribution shift. When changing to the less difficult dataset such as the strong breeze, we can see that the gap between the two algorithms decreases, but OoD-Control still achieves nearly half the score of the OMAC. (The breeze dataset is not complicated enough to distinguish the mentioned methods.\n\nWe also show the results of when ˆc is unchanged. In this setting, both OMAC and OoD-Control perform terribly because the variable used to fit the ground truth of c is frozen.\n\nFigure 1: Result of inverted pendulum experiment where the testing environment is Gale. The black dashed line represents the desired states for the inverted pendulum. The objective of this task is to maintain the angle θ and angular velocity ̇θ of the inverted pendulum to zero. f is the ground truth of the torque, while ˆf is the predicted torque. ∗ is given for the best performance. As shown in the amplified areas (the black rounded rectangles), our algorithm predicts much better than OMAC.\n\nQuadrotor. We show the result of the quadrotor task in Figure 2 and Table 2 (see Appendix B.6.1). As testing environments differ from training ones, the OoD-Control method maintains good stability and tracking accuracy. In some cases, the performance of OoD-Control goes close to the omniscient case where the rotor is provided with precise wind conditions, which shows that our algorithm is able to predict the wind with acceptable error. Our algorithm achieves lower ACE than baseline methods (60% than OMAC and over 70% than PID) in most difficult cases. Besides, we shorten the training time to test its sample efficiency, and it turns out that our algorithm performs well in few-shot learning. By adding noise during training DNN and fixing a learning rate of meta-learning of ˆc, our algorithm gains robustness and adapts quicker in a different environment.\n\nWe tested our method under several different trajectories, and OoD-Control outperforms the baseline and conventional no-adapt methods when the distribution domain shifts during the testing process. Meanwhile, OoD-control can learn more from changes in the environment and apply it as prior knowledge, thereby improving its adaptability.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: 2D view of trajectories in different wind conditions and performance comparison of OMAC and OoD-Control algorithms for trajectory tracking. The goal of the controller is to get closer to the desired trajectory (black line). Different colors demonstrate the distance from the actual location to the desired location, corresponding to the color bar at the bottom. The shade of color indicates the magnitude of the deviation in position. As compared to OMAC and no-adapt, the proposed OoD-Control method provides more accurate results across a wide range of wind environments and trajectories. The average ACE of ten independent experiments is marked in the subplot and ∗ is given for the best performance under the same environment.\n\nThe OMAC and OoD-Control algorithm were tested under hovering, figure-8, spiral upward and sin-forward trajectory scenarios. Figure 2 and Table 2 (see Appendix B.6.1) show the trajectory tracking experimental results. OoD-Control provides more accurate results across trajectories under a wide range of wind environments and achieves state-of-the-art performance in all these situations compared with the baseline.\n\nBased on experiments, it has been demonstrated that systems trained by the proposed OoD-Control algorithm perform state-of-the-art. In addition, the control method can be applied to different dynamical models and is extensible and universally applicable.\n\n6 CONCLUSION\n\nIn this paper, we theoretically demonstrate that the average control error is upper-bounded by a constant when the perturbation on the state variables is within a certain radius for UAV flight control. Besides, we propose an algorithmic framework—OoD-Control that is evaluated under turbulent environmental conditions. Based on the results of our experiments, we can conclude that our algorithm is scalable and pervasively applicable that can be applied to a variety of dynamic models. For future work, we will explore extending our algorithmic framework to more UAV types, such as unmanned helicopters, tilt-rotors, and unmanned fixed-wing aircraft. As far as we are aware, this is one of the first papers that theoretically discusses out-of-distribution problems in the context of online adaptive UAV flight control.\n\n9\n\nNo-adaptOMACOoD-ControlEnv: BreezeEnv: Galex[m]x[m]x[m]No-adaptOMACOoD-ControlSin-forwardSpiral-upFigure-8HoverACE:0.279ACE:0.202*ACE:0.048 ACE:0.478ACE:0.297*ACE:0.074 ACE:0.334ACE:0.268*ACE:0.094ACE:0.520ACE:0.403*ACE:0.169ACE:0.295ACE:0.214*ACE:0.084ACE:0.490ACE:0.309*ACE:0.120 ACE:0.319ACE:0.277*ACE:0.098ACE:0.504ACE:0.379*ACE:0.133 y[m]y[m]y[m]y[m]x[m]x[m]x[m]Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nKei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Adversarial invariant feature learning with accuracy constraint for domain generalization. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 315–331. Springer, 2019.\n\nKarl Johan ̊Astr ̈om and Richard M Murray. Feedback systems. Princeton Univer, 2008.\n\nSomil Bansal, Anayo K Akametalu, Frank J Jiang, Forrest Laine, and Claire J Tomlin. Learning quadrotor dynamics using neural network for flight control. In 2016 IEEE 55th Conference on Decision and Control (CDC), pp. 4653–4660. IEEE, 2016.\n\nTR Beal. Digital simulation of atmospheric turbulence for dryden and von karman models. Journal\n\nof Guidance, Control, and Dynamics, 16(1):132–138, 1993.\n\nFuyang Chen, Rongqiang Jiang, Kangkang Zhang, Bin Jiang, and Gang Tao. Robust backstepping sliding-mode control and observer-based fault estimation for a quadrotor uav. IEEE Transactions on Industrial Electronics, 63(8):5044–5056, 2016.\n\nJeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, pp. 1310–1320. PMLR, 2019.\n\nAT Conlisk. Modern helicopter rotor aerodynamics. Progress in aerospace sciences, 37(5):419–476,\n\n2001.\n\nAbhimanyu Dubey, Vignesh Ramanathan, Alex Pentland, and Dhruv Mahajan. Adaptive methods for real-world domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14340–14349, 2021.\n\nRobert Geirhos, J ̈orn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665–673, 2020.\n\nMuhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE transactions on pattern analysis and machine intelligence, 39(7):1414–1430, 2016.\n\nSeif-El-Islam Hasseni and Latifa Abdou. Adaptive nonlinear robust control of an underactuated\n\nmicro uav. International Journal of Dynamics and Control, 9(3):1144–1166, 2021.\n\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100, 2020.\n\nYen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting outIn Proceedings of the\n\nof-distribution image without learning from out-of-distribution data. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10951–10960, 2020.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\nWilliam Koch, Renato Mancuso, Richard West, and Azer Bestavros. Reinforcement learning for\n\nuav attitude control. ACM Transactions on Cyber-Physical Systems, 3(2):1–21, 2019.\n\nMoussa Labbadi and Mohamed Cherkaoui. Robust adaptive backstepping fast terminal sliding mode controller for uncertain quadrotor uav. Aerospace Science and Technology, 93:105306, 2019.\n\nYa Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. Domain generalization In Proceedings of the AAAI conference on artificial\n\nvia conditional invariant representations. intelligence, volume 32, 2018.\n\nYiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous domain generalization. In International Conference on Machine Learning, pp. 3915–3924. PMLR, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMichael O’Connell, Guanya Shi, Xichen Shi, Kamyar Azizzadenesheli, Anima Anandkumar, Yisong Yue, and Soon-Jo Chung. Neural-fly enables rapid learning for agile flight in strong winds. Science Robotics, 7(66):eabm6597, 2022.\n\nTri Kuntoro Priyambodo, Oktaf Agni Dhewa, and Try Susanto. Model of linear quadratic regulator (lqr) control system in waypoint flight mission of flying wing uav. Journal of Telecommunication, Electronic and Computer Engineering (JTEC), 12(4):43–49, 2020.\n\nAlexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution generalization. In International Conference on Machine Learning, pp. 18347– 18377. PMLR, 2022.\n\nFabio Ruggiero, Vincenzo Lippiello, and Anibal Ollero. Aerial manipulation: A literature review.\n\nIEEE Robotics and Automation Letters, 3(3):1957–1964, 2018.\n\nShiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and arXiv preprint\n\nSunita Sarawagi. Generalizing across domains via cross-gradient training. arXiv:1804.10745, 2018.\n\nGuanya Shi, Kamyar Azizzadenesheli, Michael O’Connell, Soon-Jo Chung, and Yisong Yue. Metaadaptive nonlinear control: Theory and algorithms, 2021. URL https://arxiv.org/abs/ 2106.06098.\n\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354–359, 2017.\n\nMilitary Specification. Flying qualities of piloted airplanes. Technical report, MIL-F-8785C, 1980.\n\nG Szafranski and R Czyba. Different approaches of pid control uav type quadrotor. In International Micro Air Vehicle conference and competitions 2011 (IMAV 2011),’t Harde, The Netherlands, September 12-15, 2011. Delft University of Technology and Thales, 2011.\n\nSamuel J. Underwood and Iqbal Husain. Online parameter estimation and adaptive control of permanent-magnet synchronous machines. IEEE Transactions on Industrial Electronics, 57(7): 2435–2443, 2010. doi: 10.1109/TIE.2009.2036029.\n\nMinghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang. Adversarial domain adaptation with domain mixup. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 6502–6509, 2020.\n\nMarvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization: A meta-learning approach for tackling group distribution shift. arXiv preprint arXiv:2007.02931, 1(3), 2020.\n\nKaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A\n\nsurvey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOF OF LEMMA AND COROLLARY\n\nA.1 PROOF FOR THEOREM 1\n\nTheorem 1 Lπ0(H, B) is denoted as the lower bound in equation 3. Lagrangian methods can be adapted to solve inequality.\n\nLπ0 (H, B) = min h∈H\n\nmin δ∈B\n\nmax λ∈R\n\nL(h, δ, λ) ≜ min\n\nh∈H\n\nmin δ∈B\n\nmax λ∈R\n\n(cid:110)\n\nhπ0(x(z)\n\nt + δ) − λ[hπ0(x(z)\n\nt\n\n(cid:111)\n\n)]\n\n) − ̃hπ0 (x(z) (17)\n\nt\n\nExchanging the min and max yields the following dual form:\n\nLπ0(H, B) ≥ max λ≥0\n\nmin h∈H\n\nmin δ∈B\n\nL(h, δ, λ) = max λ≥0\n\n(cid:26)\n\nλ ̃hπ0(x(z)\n\nt\n\n) − max δ∈B\n\n(cid:27)\n\nDH(λπ0||πδ)\n\n(18)\n\nwhere πδ max h∈H\n\nrepresents the distribution of z + δ when z ∼ π0 and DH(λπ0 ||πδ) = )]} = (cid:82) [λπ0(z) − πδ(z)]+dz.\n\n)] − Ez∼πδ [h(x(z)\n\n{λEz∼π0[h(x(z)\n\nt\n\nt\n\nProof.\n\n(i)\n\nLπ0(H, B) = min h∈H\n\nmin δ∈B\n\nmax λ∈R\n\n(cid:110)\n\nhπ0 (x(z)\n\nt + δ) − λ[hπ0 (x(z)\n\nt\n\n) − ̃hπ0(x(z)\n\nt\n\n) − ̃hπ0(x(z)\n\nt\n\n(cid:111)\n\n)]\n\n(cid:111)\n\n)]\n\nmin h∈H (cid:26)\n\n≥ max λ≥0\n\n= max λ≥0\n\n= max λ≥0\n\n(cid:110)\n\nhπ0(x(z)\n\nt + δ) − λ[hπ0 (x(z)\n\nt\n\nmin δ∈B\n\nλ ̃hπ0 (x(z)\n\nt\n\n) − max h∈H\n\n(λhπ0 (x(z)\n\nt\n\n) − hπδ (x(z)\n\nt\n\n))\n\n(cid:27)\n\n(cid:26)\n\nλ ̃hπ0 (x(z)\n\nt\n\n) − max δ∈B\n\nDH(λπ0||πδ)\n\n(cid:27)\n\n(ii) We denote the sign function sgn(z) as:\n\nsgn(δ) =\n\n(cid:26)1, if [λπ0(z) − πδ(z)] ≥ 0 0, if [λπ0(z) − πδ(z)] < 0\n\n(19)\n\nThus we can calculate DH(λπ0||πδ) directly as:\n\nDH(λπ0||πδ) = max\n\nh∈H\n\n(λhπ0(x(z) (cid:110)\n\nt\n\n) − hπ0(x(z)\n\nt + δ))\n\nλEz∼π0[h(x(z)\n\nt\n\n)] − Ez∼πδ [h(x(z)\n\nt\n\n(cid:111)\n\n)]\n\n= max h∈H (cid:90)\n\n=\n\nsgn(x(z)\n\nt\n\n)[λπ0(z) − πδ(z)]dz\n\n(cid:90)\n\n=\n\n[λπ0(z) − πδ(z)]+dz\n\nA.2 PROOF FOR COROLLARY 1\n\nCorollary 1 (Gaussian noise) With Gaussian noise π0 = N (0, σ2I) and bounded disturbance B = {δ :]∥δ∥2 ≤ r}, the lower bound in equation 5 satisfies: (cid:27)\n\n(cid:26)\n\nLπ0(H, B) = max λ≥0\n\nλ ̃hπ0(x(z)\n\nt\n\n) − max δ∈B\n\nDH(λπ0 ||πδ)\n\n≥ Φ(Φ−1( ̃hπ0 (x(z)\n\nt\n\n)) −\n\n)\n\n(20)\n\nr σ\n\nwhere Φ(·) represents the Gaussian Cumulative Density Function (CDF). For the case p=0.5, i.e., ̃hπ0(x(z) Proof.\n\n) > 0.5 , the radius satisfies r ≤ σΦ−1( ̃hπ0 (x(z)\n\n)).\n\nt\n\nt\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nL ≥ Φ(Φ−1(hπ0 (x(z)\n\nt\n\n)) −\n\nr σ\n\n) >\n\n1 2\n\nL ≥ min ∥δ∥≤r\n\nmax λ≥0\n\n(cid:26)\n\nλ ̃hπ0(x(z)\n\nt\n\n) −\n\n(cid:90)\n\n[λπ0(z) − πδ(z)]+dz\n\n(cid:27)\n\n(21)\n\n(22)\n\nWe denote Cλ = {z : λπ0(z) ≥ πδ(z)}={z : δT z ≤ ∥δ∥2 (cid:82) [λπ0(z) − πδa(z)]+dz. Then we will get:\n\n2 + σ2 ln λ} and F (δ, λ) = λ ̃hπ0(x(z)\n\nt\n\n) −\n\nF (δ, λ) = λ ̃hπ0 (x(z)\n\nt\n\n) −\n\n= λ ̃hπ0 (x(z)\n\nt\n\n) −\n\n(cid:90)\n\n(cid:90)\n\n[λπ0(z) − πδ(z)]+dz\n\n[λπ0(z) − πδ(z)]dz\n\nCλ\n\n= λ ̃hπ0 (x(z)\n\nt\n\n) − λΦ\n\n(cid:18) ∥δ∥2 2σ\n\n(cid:19)\n\n+\n\nσ ln λ ∥δ∥2\n\n+ Φ\n\n(cid:18) −∥δ∥2 2σ\n\n+\n\nσ ln λ ∥δ∥2\n\n(cid:19)\n\nIt is notable that F (δ, λ) is a concave function w.r.t λ, thus the maximum value occurs when\n\n∂F (δ,λ) ∂λ\n\n(cid:12) (cid:12)λ=λδ\n\n= 0. A direct calculation gives λδ = exp\n\n2σ∥δ∥2Φ−1(hπ0 (x(z)\n\nt\n\n2σ2\n\n))−∥δ∥2\n\n2\n\n(cid:18)\n\n(cid:19)\n\n. There is\n\nL ≥ min ∥δ∥≤r\n\nmax λ≥0\n\nF (δ, λ)\n\n= min ∥δ∥≤r\n\nΦ\n\n(cid:18) −∥δ∥2 2σ\n\n+\n\nσ ln λδ ∥δ∥2\n\n(cid:19)\n\n= min ∥δ∥≤r (cid:16)\n\n= Φ\n\n(cid:18)\n\nΦ\n\nΦ−1(hπ0 (x(z)\n\nt\n\n)) −\n\n(cid:19)\n\n∥δ∥2 σ\n\nΦ−1(hπ0 (x(z)\n\nt\n\n)) −\n\n(cid:17)\n\nr σ\n\nIn case p=0.5, the perturbation radius r is calculated as 1:\n\nmin ∥δ∥≤r\n\nmax λ≥0\n\nF (δ, λ) >\n\n1 2\n\n⇔ Φ\n\nA.3 PROOF FOR LEMMA 1\n\n(cid:16)\n\nΦ−1(hπ0(x(z)\n\nt\n\n)) −\n\n(cid:17)\n\nr σ\n\n>\n\n1 2\n\n⇔ r < σΦ−1(hπ0(x(z)\n\nt\n\n))\n\n(23)\n\nLemma 1 average control error is bounded as:\n\n(ACE bound in ideal case) For any perturbation in B = {δ : ∥δ∥2 ≤ r}, the theoretical\n\nACEπδ =\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ [ ˆf (x(z)\n\nt\n\n)] − Ez∼πδ [f (x(z)\n\nt\n\n)]∥ ≤ ̃h−1(p)\n\n(24)\n\nProof. From equation 1, we consider a discrete nonlinear control-affine system:\n\nxt+1 = f0(xt) + B(xt)ut − f (xt, c) + wt,\n\n1 ≤ t ≤ T,\n\nThe controller of model-based control with the ideal model is\n\nut = B†(x(z)\n\nt\n\n)(−f0(x(z)\n\nt\n\n) + ˆf (x(z)\n\nt\n\n))\n\nthus, the equation 1 becomes:\n\nxt+1 = ˆf (x(z)\n\nt\n\n) − f (x(z)\n\nt\n\n, c) + wt\n\n1The calculation method of the radius r is given in Cohen et al. (2019) for the case p=0.5.\n\n(25)\n\n(26)\n\n(27)\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFollowing a straightforward calculation, we will obtain the upper ACE bound under disturbance z ∼ πδ\n\nACEπδ =\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ (x(z)\n\nt\n\n) − xd\n\nt ∥ =\n\n≤\n\n1 T\n\n1 T\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ [ ˆf (x(z)\n\nt\n\n)] − Ez∼πδ [f (x(z)\n\nt\n\n, c)] + Ez∼πδ (wt)∥\n\n∥Ez∼πδ [ ˆf (x(z)\n\nt\n\n)] − Ez∼πδ [f (x(z)\n\nt\n\n, c)]∥ + ∥Ez∼πδ (wt)∥\n\n≤ ∥ˆh−1(p)∥\n\n(28)\n\nA.4 PROOF FOR COROLLARY 2\n\nCorollary 2 misspecification in the actuation matrix, and the ACEπδ satisfies:\n\n(ACE bound under control actuation misspecification) ∆B(x(z)\n\nt\n\n) is the parametric\n\nACEπδ =\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ (x(z)\n\nt\n\n) − xd\n\nt ∥ ≤ ̃h−1(p) +\n\n1 T\n\nwhere ef (x(z)\n\nt\n\n) = ˆf (x(z)\n\nt\n\n, c) − f0(x(z)\n\nt\n\n).\n\nProof.\n\nT (cid:88)\n\n(∥Ez∼πδ [∆B(x(z)\n\nt\n\n)B†(x(z)\n\nt\n\n)ef (x(z)\n\nt\n\n)]∥)\n\nt=1\n\nt\n\nxt+1 = f0(x(z) = ˆf (x(z) = ˆf (x(z)\n\n) + (B(x(z) ) − f (x(z) ) − f (x(z)\n\nt\n\nt\n\nt\n\nt\n\nt\n\n) + ∆B(x(z)\n\nt\n\n, c) + wt + ∆B(x(z) , c) + wt + ∆B(x(z)\n\nt\n\nt\n\n)u(t) − f (x(z) )B†(x(z) )B†(x(z)\n\nt\n\nt\n\nt\n\n, c) + wt )[−f0(x(z) )ef (x(z)\n\n)\n\nt\n\nt\n\n) + ˆf (x(z)\n\nt\n\n)]\n\n(29)\n\nWe use ef (x(z)\n\nt\n\n) to denote ˆf (x(z)\n\nt\n\n) − f0(x(z)\n\nt\n\n), the ACE upper bound is calculated as follows:\n\nACEπδ =\n\n=\n\n=\n\n=\n\n1 T\n\n1 T\n\n1 T\n\n1 T\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ (x(z)\n\nt\n\n) − xd\n\nt ∥\n\n∥Ez∼πδ [ ˆf (x(z)\n\nt\n\n) − f (x(z)\n\nt\n\n, c) + wt + ∆B(x(z)\n\nt\n\n)B†(x(z)\n\nt\n\n)ef (x(z)\n\nt\n\n)]∥\n\n∥Ez∼πδ [ ˆf (x(z)\n\nt\n\n) − f (x(z)\n\nt\n\n, c)] + Ez∼π0 (wt) + Ez∼πδ [∆B(x(z)\n\nt\n\n)B†(x(z)\n\nt\n\n)ef (x(z)\n\nt\n\n)]∥\n\n∥Ez∼πδ [ ˆf (x(z)\n\nt\n\n)] − Ez∼πδ [f (x(z)\n\nt\n\n, c)] + Ez∼πδ [∆B(x(z)\n\nt\n\n)B†(x(z)\n\nt\n\n)ef (x(z)\n\nt\n\n)]∥\n\n≤ ̃h−1(p) +\n\n1 T\n\nT (cid:88)\n\n(∥Ez∼πδ [∆B(x(z)\n\nt\n\nt=1\n\n)B†(x(z)\n\nt\n\n)ef (x(z)\n\nt\n\n)]∥)\n\n(30)\n\nA.5 PROOF FOR LEMMA 2\n\nLemma 2 B = {δ : ∥δ∥2 ≤ r}, the formula of quadrotors’ trajectory tracking error is given as:\n\n(Trajectory tracking ACE of quadrotor) In this paper, for environmental disturbances in\n\nACEπδ =\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ (x(z)\n\nt\n\n) − xd\n\nt ∥ =\n\n=\n\n1 T\n\n1 T\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ [C1er1x(z)\n\nt + C2er2x(z)\n\nt − ε/Kv]∥\n\n∥Ez∼πδ [C1er1x(z)\n\nt + C2er2x(z)\n\nt\n\n] − ̃h−1(p)/Kv∥\n\n(31)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nwhere C1 = −xd , c) − ˆf (x(z) f (x(z)\n\nt − C2 + ε/Kv, C2 = (ε + (Kvxd\n\nt )/Kv(er2xd , ˆc), which is the prediction error of the unknown dynamics.\n\nt − ε)er1xd\n\nt\n\nt\n\nt − er1xd\n\nt ) and ε =\n\nProof.\n\nRecall the kinetic function in equation 15: m ̇v = mg + RfT + ft. The desired control force fd is designed as:\n\n(cid:40)\n\nfd = RfT = ̄fd − ˆft ̄fd = m ̇vr + Kvxe − mg\n\n(32)\n\nwhere xe = x(z) substituting equation 32 into equation 15, the UAV dynamics becomes:\n\nt . xe is the error of trajectory tracking and vr is the desired velocity at t. By\n\nt − xd\n\nm ̇v = mg + RfT + ft m ̇v − mg − ̄fd = ft − ˆft m( ̇v − ̇vr) − Kvxe = ft − ˆft m ̈xe − Kvxe − ε = 0\n\n ̈xe −\n\nKv m\n\nxe −\n\n1 m\n\nε = 0\n\n(33)\n\nwhere ε = ft − ˆft = f (x(z) linear differential equation.\n\nt\n\n, c)− ˆf (x(z)\n\nt\n\n, ˆc). Note that equation 33 is a second-order inhomogeneous\n\n1) General solution: Making the substitution in the differential equation, r satisfies the auxiliary equation:\n\nr2 −\n\nKv m\n\n= 0 ⇒ r1,2 = ±\n\n(cid:114)\n\nKv m\n\nthen, we obtain the general solution of the differential function ̄xe as:\n\n ̄xe = C1er1x + C2er2x\n\n(34)\n\n(35)\n\n2) Special solution: Consider the standard second order differential equation: ̈y + p ̇y + qy = P (x)eαx, the special solution x∗\n\ne is obvious:\n\nTherefore, xe can be expressed as:\n\nx∗\n\ne = −ε/Kv\n\nxe = ̄xe + x∗\n\ne = C1er1x + C2er2x − ε/Kv\n\n(36)\n\n(37)\n\n3) Calculation of C1 and C2: In the equation 37, there exist two fixed points:(xd,0) and (0,-xd). We have:\n\n(cid:26)0 = C1er1x + C2er2x − ε/Kv\n\n−xd = C1 + C2 − ε/Kv\n\nBy solving the simultaneous formulas, we will get the answer of C1 and C2.\n\n(cid:26)C1 = −xd − C2 + ε/Kv\n\nC2 = (ε + (Kvxd − ε)er1xd )/Kv(er2xd − er1xd )\n\nThus the solution of the original derivative function is:\n\n(38)\n\n(39)\n\nxe = x(z)\n\nt − xd\n\nt = C1er1x + C2er2x − ε/Kv\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nand we have the average error bound in equation 2:\n\nACEπδ =\n\n=\n\n=\n\n=\n\n=\n\n1 T\n\n1 T\n\n1 T\n\n1 T\n\n1 T\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\n∥Ez∼πδ (x(z)\n\nt − xd\n\nt )∥\n\n∥Ez∼πδ [C1er1x(z)\n\nt + C2er2x(z)\n\nt − ε/Kv]∥\n\n∥Ez∼πδ [C1er1x(z)\n\nt + C2er2x(z)\n\nt\n\n∥Ez∼πδ [C1er1x(z)\n\nt + C2er2x(z)\n\nt\n\n∥Ez∼πδ [C1er1x(z)\n\nt + C2er2x(z)\n\nt\n\n] − Ez∼πδ (ε/Kv)∥\n\n] − Ez∼πδ [f (x(z)\n\nt\n\n, c) − ˆf (x(z)\n\nt\n\n, ˆc)]/Kv∥\n\n] − ̃h−1(p)/Kv∥\n\n(40)\n\nB EXPERIMENTAL SETTINGS AND DETAILS\n\nB.1 OOD-CONTROL PSEUDO CODE AND ALGORITHM SETTINGS\n\nOut-of-Distribution comes from the misspecification of systems’ components and the systematic error of sensors and environment models. Our algorithm is able to extrapolate unknown wind disturbances after learning a model from previous data containing generalized information.\n\nTo eliminate the influence caused by other factors, the two models share the same initial state and environmental conditions. Furthermore, in order to make the training process fair for both models, we simulate them for the same number of iterations and sustain each iteration for the same period of time.\n\nAlgorithm 1 OoD-Control (Out-of-Distribution Generalization control for Adaptive Nonlinear Control) Input: Set of distribution functions X , DNN φ with parameter Θ, environment estimation vector ˆc Parameter: Parameters of mechanical system and aerodynamics. Output: The estimation of unknown force ˆf 1: while picking χ from X do 2: 3: 4: 5: 6:\n\nSample a series of independent random variables w subject to χ as external wind force. Apply external force to the simulation according to equation 43 Calculate loss with noise in the state: L = ||φ(x + ∆x)T ˆc − f ||2 Update Θ : Θ = Θ − η1∇θL Update ˆc : ˆc = ˆc − η2∇ˆcL return ˆf = φ(x; Θ)T ˆc\n\n7: 8: end while\n\nB.2 UPDATE OF PARAMETERS φ AND ˆc\n\nBased on equation 1, we design a discrete-time simulation process to calculate state variables and estimate the unknown term at each time interval and by this way, collect data for training φ. In algorithm 1, we update Θ during simulation: Θ = Θ − ∇Lθ.\n\nKeeping a constant ˆc in the inverted pendulum experiment results in the inability to update environmental parameters. This can result in higher ACE or even failure to control the system (see Table 2). For the quadrotor, when the wind is severe, it cannot maintain its position, as illustrated in Figure 3. It should be noted, however, that our algorithm exhibits better control even when position drift occurs. Generally, updates to φ would be more energy intensive, whereas updating only ˆc would be closer to the actual embedded device.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: ACE results in pendulum experiments with the changed or unchanged ˆc\n\nˆc\n\nTest Env.\n\nNo-Adapt\n\nOMAC\n\nOoD-Control Omniscient\n\nChanged Changed Changed Unchanged Unchanged\n\nBreeze Strong Breeze Gale Strong Breeze Gale\n\n0.538(0.325) 1.566(0.257) 2.277(0.427) 1.566(0.257) 2.277(0.427)\n\n0.054(0.022) 0.134(0.032) 0.592(0.196) 1.553(0.262) 2.276(0.425)\n\n0.050(0.024) 0.078(0.029) 0.163(0.055) 1.548(0.265) 2.275(0.425)\n\n0.046(0.024) 0.046(0.024) 0.045(0.021) 0.046(0.024) 0.045(0.021)\n\nFigure 3: Results when both ˆc and φ keep unchanged in testing\n\nB.3 PSEUDO CODE OF THE PERTURBATION RADIUS CALCULATION ALGORITHM\n\nAlgorithm 2 Monte Carlo algorithm for calculation of perturbation radius Input: x, number of Monte Carlo samples: N, the variance of Gaussian noise: σ Parameter: threshold of error: ε Output: radius\n\nAdd Gaussian noise x = x+N (0, σ) and append to x set\n\nObtain the prediction value and calculate prediction error if error < ε then nA = nA+1\n\n// nA is the number of successful predictions\n\n1: for i in range(N) do 2: 3: end for 4: for x sample in x set do 5: 6: 7: end if 8: 9: end for\n\nreturn 0\n\n10: pABar ← calculate the lower confidence bound 11: if pABar < 0.5 then 12: 13: else 14: 15: end if 16: return radius\n\nradius = σ · fppa(pABar)\n\n// 0 means abstention\n\n// fppa is the percent point function of Gaussian distribution.\n\nB.4 DIFFERENTIAL EQUATION SOLVER\n\nThe fourth-order Runge-Kutta method is a common iterative method to calculate differential equations and approach continuous functions. In this paper, we use this method to calculate equation 1 and simulate the process of the mechanical system. We do the simulation by moving tiny ∆t each\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\ntime, which corresponds to the step size of the Runge-Kutta method. And the right-hand side of equation 1 is the derivative of the state x.\n\nB.5 THE COMPARISON OF VARIOUS TRAJECTORY TRACKING MODES\n\nOoD generalized model adapts better and gives inputs forces closer to the desired ones as illustrated in Figure 4 to Figure 7 which shows the traces under OMAC (Deep Learning), OoD-Control and omniscient models and the desired trajectory, the OoD-Control algorithm clearly goes closer to the desired curve.\n\n(a)\n\n(b)\n\nFigure 4: Traces when the quadrotor tries to keep still.\n\n(a)\n\n(b)\n\nFigure 5: Traces when the trajectory is a spiral curve. .\n\nB.6 WIND CONDITIONS IN EXPERIMENTS\n\nB.6.1 WIND FIELD SETTINGS\n\nProtocol Two sets of distribution functions X and Ω with X ∩ Ω = ∅ are defined for training and testing. The training distribution χ ∈ X and the testing distribution ω ∈ Ω are specified for different experiment tasks. The wind velocity is a series of independent random variables sampled subject to the distributions (χ or ω) picked out. The wind brings induced airflow to the rotor blades, creating complex and nonstationary aerodynamic interactions2. All models are simulated with the same wind series and the simulating duration is also the same.\n\n2More detailed information related to the aerodynamics under wind is shown in Appendix B.6.3\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 6: Traces of OMAC, OoD-Control and omniscient quadrotor models with figure-8 trajectory\n\n(a)\n\n(b)\n\nFigure 7: Traces of OMAC, OoD-Control and omniscient quadrotor models with sin-forward trajectory.\n\nTable 2: ACE in quadrotor experiment with different trajectories and environment\n\nTrajectory\n\nTest Env.\n\nNo-adapt\n\nOMAC\n\nOoD-Control Omniscient\n\nhover hover hover sin-forward sin-forward sin-forward figure-8 figure-8 figure-8 spiral-up spiral-up spiral-up\n\nBreeze Strong Breeze Gale Breeze Strong Breeze Gale Breeze Strong Breeze Gale Breeze Strong Breeze Gale\n\n0.279(0.056) 0.410(0.085) 0.478(0.100) 0.319(0.058) 0.439(0.086) 0.504(0.101) 0.334(0.060) 0.455(0.085) 0.520(0.100) 0.295(0.060) 0.422(0.088) 0.490(0.103)\n\n0.202(0.035) 0.257(0.049) 0.297(0.079) 0.277(0.047) 0.334(0.053) 0.379(0.067) 0.268(0.032) 0.318(0.055) 0.403(0.216) 0.214(0.044) 0.265(0.057) 0.309(0.090)\n\n0.048(0.007) 0.060(0.011) 0.074(0.026) 0.098(0.011) 0.110(0.017) 0.133(0.045) 0.094(0.005) 0.100(0.006) 0.169(0.195) 0.084(0.012) 0.095(0.016) 0.120(0.055)\n\n0.029(0.003) 0.035(0.003) 0.040(0.004) 0.079(0.010) 0.085(0.016) 0.092(0.021) 0.079(0.005) 0.082(0.005) 0.085(0.006) 0.067(0.004) 0.071(0.006) 0.075(0.008)\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nPendulum In this task, we use 5 different normal distributions as the training set, i.e. X = {χk|χk = N (0, 0.2k), k = 1, 2, . . . , 5}. When testing, we set 3 different levels of the wind based on the difficulty, that is the breeze, strong breeze, and gale. And each level has 6 or 8 different uniform distributions. We show the details of the testing set Ω as follows:\n\n1 | ωk\n\n• Breeze: Ω1 = {ωk • Strong Breeze: Ω2 = {ωk • Gale: Ω3 = {ωk\n\n3 | ωk\n\n1 = U (−0.5k, 0.5k), k = 1, 2, . . . , 5, 6}\n\n2 | ωk\n\n2 = U (−3.0 − 0.5k, 3.0 + 0.5k), k = 1, 2, . . . , 5, 6}\n\n3 = U (−6.0 − 0.5k, 6.0 + 0.5k), k = 1, 2, . . . , 7, 8}\n\nQuadrotor In this instantiation, we only use one training distribution, the three-dimensional standard normal distribution, to train the models. Meanwhile, we transform the initial wind data sampled from the normal distribution to their absolute values, i.e. χ = |N (μ, Σ)|, μ = [0, 0, 0], Σ = diag(1, 1, 1). The reason for this operation is that we want to make the wind come from only one octant, then it differs from the test environment in distribution and direction. Similarly, the test set Ω also has 3 different levels, but the exact distributions are different from the pendulum task. Following are the details:\n\n• Breeze: ω1 = U (D1), D1 = {(x, y, z) | x, y, z ∈ (−3, 3)} • Strong Breeze: ω2 = U (D2), D2 = {(x, y, z) | x, y, z ∈ (−6, 6)} • Gale: ω3 = U (D3), D3 = {(x, y, z) | x, y, z ∈ (−8, 8)}\n\nTrajectory illustration The three trajectories we used in the quadrotor experiment (see Table 2) are mathematically described as:\n\n• sin-forward: (x, y, z) = (2sin( πt\n\n3 ), 0.2t, 0.5t)\n\n• figure-8: (x, y, z) = (2sin( πt\n\n• spiral-up: (x, y, z) = (sin( 2πt\n\n5 ), 2sin( πt 5 ), cos( 2πt\n\n5 ), sin( 2πt 5 )) 5 ) − 1, 0.2t)\n\nB.6.2 DISCUSSION\n\nQ1: What’s the main problem addressed in this paper and what’s the innovation compared with other online adaptive control?\n\nA1: We study the online adaptive flight control problem when testing and training environment domain shifts and give a methodology that the upper bound of the predicted error of the unknown dynamics maintains constant under a radius of perturbation.\n\nQ2: How does the OoD-Control algorithm perform in the i.i.d. environment, where the train and test environment follow the same distribution?\n\nA2: We run OMAC and OoD-Control algorithm on i.i.d. environment and put the results in Table 3 (see Appendix B.7). In the i.i.d. environment, the ACE of all algorithms drop naturally, but OoD-Control still performs much better than OMAC.\n\nQ3: How does the size of noise add to the state vectors when training affects the performance of our algorithm?\n\nA3: We run OoD-Control algorithm with different noises in i.i.d. and o.o.d. environment respectively. Results (see Table 4) show that the algorithm performs worse in the i.i.d. environment as the scale of noise increases. And performance in the OoD environment gets a little better in a certain range when noise size increases, but becomes worse when noise is too large.\n\nB.6.3 AERODYNAMICS OF THE ROTORS IN WIND\n\nWind conditions affect the quadrotor mainly in two ways. The first is that it alters the aerodynamics of the rotors. The second is to change the air resistance of each windward side. The aerodynamics of the rotor in windy conditions is illustrated in Figure 8. This paper defines several types of winds, namely Breeze, Strong Breeze, and Gale.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: ACE in quadrotor experiment in i.i.d. environment\n\nTrajectory\n\nNo-adapt\n\nOMAC\n\nOoD-Control Omniscient\n\nhover figure-8 spiral-up sin-forward\n\n0.378(0.052) 0.436(0.043) 0.403(0.050) 0.432(0.048)\n\n0.138(0.030) 0.230(0.030) 0.158(0.031) 0.209(0.022)\n\n0.036(0.012) 0.087(0.008) 0.076(0.008) 0.092(0.017)\n\n0.027(0.003) 0.075(0.005) 0.065(0.003) 0.076(0.004)\n\nFigure 8: Sketch of the aerodynamics of the propeller in wind conditions\n\n1) Aerodynamics caused by wind gust disturbances. According to the rotor slipstream theory (Conlisk, 2001), the induced velocity is calculated by:\n\n∥Vt∥ =\n\n(cid:115)\n\nkf n2 2πρr2\n\n(41)\n\nwhere kf is the lift coefficient, ρ is the air density, r is the rotor radius. A wind field results in a total aerodynamic force on the rotor equal to the sum of the lift force T and the additional wind disturbance force Fw. The total lift can be calculated as:\n\nTherefore, the wind disturbance force Hwi and moment Mwi on ith rotor is:\n\n∥T + Fw∥ = 2πρr2∥Vt∥Vw + Vt∥\n\n \n\nHwi = kf n2\n\ni − 2πρr2∥(0, 0, (cid:112)kf n2\n\ni /2πρr2)T + V B w ∥ (cid:26)kmHwi/kf , when the rotor turns clockwise\n\nMwi =\n\n\n\n−kmHwi/kf , otherwise\n\n(42)\n\n(43)\n\nwhere km is the anti-torque coefficient related to the shape of the rotors and local air density.\n\n2) The air drag. Air drag can be ignored in hovering or low-speed flights without wind. However, in the presence of a wind field, the following equation can be used to calculate air drag.\n\nwhere c represents the air drag coefficient, Sair is the windward area, and Vair is the relative speed of the wind to the quadrotor.\n\nDg =\n\n1 2\n\ncρSairV 2\n\nair\n\n(44)\n\nB.7 PERFORMANCE IN I.I.D. ENVIRONMENT AND DIFFERENT NOISE SETTING\n\nN oisex and N oisea mean the noise scale added to the state vector and the environment representation vector respectively. In this experiment, We double the test duration (compared to the results in Table 2) to enlarge the differences.\n\n21\n\n^VwFwTVtVwVniUnder review as a conference paper at ICLR 2023\n\nTable 4: ACE in quadrotor experiment with different noise\n\nTrajectory OoD Env. Noisex Noisea\n\ni.i.d.\n\no.o.d.\n\nfigure-8 figure-8 figure-8 figure-8 figure-8\n\nGale Gale Gale Gale Gale\n\n0.01 0.02 0.05 0.05 0.1\n\n0.01 0.02 0.02 0.05 0.1\n\n0.0628 0.0635 0.0652 0.0651 0.0729\n\n0.5897 0.5897 0.5767 0.5877 0.5908\n\n22",
    "reference": "# Summary Of The Paper\n\nThe work presents a novel adaptive data-driven UAV flight control method, OoD-Control. Under gaussian noise assumptions and bounded control and disturbances, the authors show that the upper bound of the prediction error remains constant under unpredictable disturbance. The control method (and bound) is also shown to be extensible to different nonlinear dynamical models.\n\n# Strength And Weaknesses\n\nStrengths:\n- The work deals with the very pertinent topic of OoD performance for learned dynamics\n- The theoretical bound shown, without the need for the restrictive eISS assumption is a notable contribution for nonlinear under-actuated systems.\n- The performance of the control protocol is impressive in simulation, not only against previous work but even with respect to omniscient ground truth controllers\n- Simulation work is of good quality, permitting clean comparison of baselines and a clear presentation of the results. \n\nWeaknesses:\nThe work is theoretically sound and relevant. The experimental setup however leaves some claims unexplained and many important questions unanswered.\n-  The authors claim in section 5.4 (quadrotor) that \"As the wind field in the testing environment is quite different from the training environment, our algorithm performs relatively better than the baseline and no-adapt.\" Apart from the poor grammatical phrasing, the only material in the paper that comes to support that claim is in appendix B.6.1 \"The wind comes from only one octant, so it differs\nfrom test environment in not only distribution but also direction. The models are trained in only one\nenvironment. Our OoD-Control model adapts well in testing even though the training material is\nquite little.\" Again neglecting the very poor wording of the last sentence, one fails to see how much OoD testing is really happening, i.e. how dissimilar the settings really are to the DNN.\n- An important study would include pushing the architecture to its limits with perturbations of increasingly different distributions and directions. The out of distribution capacities claimed in the very model name OoD-Control, are not significantly exhibited. \n- Along the same lines, a precise explanation of the data consumed to train the model is missing, \"quite little\" being an unacceptable unit of measure for a machine learning conference paper. This remark generalises to the entire training framework which remains very obscure: what model architecture is used? Of what size? What is the data pipeline setup? etc.\n- What about other control methods to fight the wind ? Why only benchmark against OMAC ? The authors claim \"most of the previously mentioned control\nmethods suffer from limitations. Imprecise system modeling and non-modeled environmental\ndisturbances may result in unacceptable performance or instability.\" It would be worth comparing the performance against state of the art robust control protocols for example to support this claim. \n- The authors fail to discuss any possibility of using this algorithm in the real world. The natural limitations of data collection via Monte Carlo methods make this method trainable in simulation. What about the simulation to reality gap? Have the authors considered training a OoD-Controller in simulation and attempting to measure its performance on real hardware? Any results in this direction would be of great value.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe notation and wording is tedious to follow is some instances. For example, the wording \"prediction error\" of the unknown dynamics is (correctly) used to refer to $ \\epsilon = f(x^{(z)}_t , c) − \\hat{f}(x^{(z)}_t , \\hat{c}) $ (implicitly throughout the paper, explicitly page 6 lemma 2), in which case indeed \"the methodology provides the guaranteed upper bound for the prediction errors of the unknown dynamics\". \n\nHowever, on page 4 (beginning of section 4.1) \"prediction error\" is used to refer to $\\tilde{h}_{\\pi_0}(x^{(z)}_t)$ and now ($\\tilde{h}$ being monotonically decreasing) a \"lower bound of the prediction error maintains constant under unpredictable disturbance\". \n\nIn fact, the $\\tilde{h}_{\\pi_0}(x^{(z)}_t)$ shorthand is also used without explicit definition in this section. These might be small details but reduce the quality of presentation and thus reader appreciation of the core theoretical work behind the method.\n\nThe above mentioned example is not a singularity. The overall clarity of the paper, in terms of better wording and grammatical construction, could do with some improvement.\n\n# Summary Of The Review\n\nThe paper proposed a very sound construction of a novel adaptive data-driven UAV flight control method based on the derivation of a prediction error bound result. This seems to offer improved expressiveness in learning simulated wind dynamics, and thus improved performance against earlier work. The authors' claim that the architecture achieves OoD generalisation remains however questionable, or to the least has not been explored to the fullest. A lack of clarity in terms of precise training protocol as well as an absence of connection to possible real world implementations penalise the paper's impact.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nNOISE INJECTION NODE REGULARIZATION FOR ROBUST LEARNING\n\nNoam Levi§ & Tomer Volansky Department of Physics Tel Aviv University Tel Aviv, Israel {noam,tomerv}@mail.tau.ac.il\n\nItay M. Bloch§ Berkeley Center for Theoretical Physics, University of California and Theoretical Physics Group, Lawrence Berkeley National Laboratory, Berkeley, CA, U.S.A. itayblochm@berkeley.edu\n\nMarat Freytsis NHETC, Department of Physics and Astronomy Rutgers University Piscataway, NJ, U.S.A. marat.freytsis@rutgers.edu\n\nABSTRACT\n\nWe introduce Noise Injection Node Regularization (NINR), a method of injecting structured noise into Deep Neural Networks (DNN) during the training stage, resulting in an emergent regularizing effect. We present theoretical and empirical evidence for substantial improvement in robustness against various test data perturbations for feed-forward DNNs when trained under NINR. The novelty in our approach comes from the interplay of adaptive noise injection and initialization conditions such that noise is the dominant driver of dynamics at the start of training. As it simply requires the addition of external nodes without altering the existing network structure or optimization algorithms, this method can be easily incorporated into many standard architectures. We find improved stability against a number of data perturbations, including domain shifts, with the most dramatic improvement obtained for unstructured noise, where our technique outperforms existing methods such as Dropout or L2 regularization, in some cases. Further, desirable generalization properties on clean data are generally maintained.\n\n1\n\nINTRODUCTION\n\nNonlinear systems often display dynamical instabilities which enhance small initial perturbations and lead to cumulative behavior that deviates dramatically from a steady-state solution. Such instabilities are prevalent across physical systems, from hydrodynamic turbulence to atomic bombs (see Jeans & Darwin (1902); Parker (1958); Chandrasekhar (1961); Drazin & Reid (2004); Strogatz (2018) for just a few examples). In the context of deep learning (DL), DNNs, once optimized via stochastic gradient descent (SGD), suffer from similar instabilities as a function of their inputs. While remarkably successful in a multitude of real world tasks, DNNs are often surprisingly vulnerable to perturbations in their input data as a result (Szegedy et al., 2014). Concretely, after training, even small changes to the inputs at deployment can result in total predictive breakdown.\n\nOne may classify such perturbations with respect to the distribution from which training data is implicitly drawn. This data is typically assumed to have support over (the vicinity of) some lowdimensional submanifold of potential inputs, which is only learned approximately due to the discrete nature of the training set. To perform well during training, a network need only have well-defined behavior on the data manifold, accomplished through training on a given data distribution. However, data seen on deployment can display other differences with respect to the training set, as illustrated\n\n§Equal contribution\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Illustration of perturbations to data inputs with respect to the joint probability distribution manifold of features and labels. Points indicate {sample, label} pairs {x, y}, where different colored points correspond to samples drawn from different marginal distributions. Black points represent pairs from a training dataset {xi, yi}N i=1, with the red spheres indicating corrupted inputs, determined by shifted distribution functions fcorrupted(x + ε, y). The gray arrow represents an adversarial attack, performed by ascending up the gradient of the network output to reach the closest decision boundary, while generalization from training to test data is depicted as interpolation from black to blue points. Finally, domain shift is a shift in the underlying distribution on the same manifold, depicted by the green arrow and points.\n\nin Fig. 1. These distortions introduce vulnerabilities that are a crucial drawback of trained DNNs, making them susceptible to commonly occurring noise which is ubiquitous in real-world tasks. By studying how networks dynamically act to mitigate the negative effects of input noise, we identify a novel dynamical regularization method starting in a noise-dominated regime, leading to more robust behavior for a range of data perturbations. This is the central contribution of this work.\n\nBackground: Regularization involves introducing additional constraints in order to solve an illposed problem or to prevent over-fitting. In the context of DL problems, different regularization schemes have been proposed (for a review, see Kukaˇcka et al. (2018) and references therein). These methods are designed to constrain the network parameters during training, thereby reducing sensitivity to irrelevant features in the input data, as well as avoiding overfitting. For instance, weight norm regularization (L2, L1, etc.) (Cortes & Vapnik, 1995; Zheng et al., 2003) can be used to reduce overfitting to the training data, and is often found to improve generalization performance (Hinton, 1987; Krogh & Hertz, 1991; Zhang et al., 2018). Alternatively, introducing stochasticity during training (e.g., Dropout (Srivastava et al., 2014)), has become a standard addition to many DNN architectures, for similar reasons. These methods are mostly optimized to reduce the generalization error from training to test data, under the assumption that both are sampled from the same underlying distribution (Srivastava et al., 2014). Here, we propose a new method which is instead tailored for robustness. Our method relies on noise-injection, that actively reduces the sensitivity to uncorrelated input perturbations.\n\nOur contribution: In this paper, we employ Noise Injection Nodes (NINs), which feed random noise through designated optimizable weights, forcing the network to adapt to layer inputs which contain no useful information. Since the amount of injected noise is a free parameter, at initialization we can set it to be anything from a minor perturbation to the dominant effect, leading to a system breakdown for extreme values. The general behavior of NINs and how they probe the network is the main goal of Levi et al. (2022), while we focus here on their regularizing properties in different noise injection regimes. The results of Levi et al. (2022) are explicitly recast in the context of regularization in App. C for a linear model, which captures the main insights.\n\nOur study suggests that within a certain range of noise injection parameter values, this procedure can substantially improve robustness against subsequent input corruption and partially against other forms of distributional shifts, where the maximal improvement occurs for large noise injection magnitudes approaching the boundary of this window, above which the training accuracy degrades to random guessing. To the best of our knowledge, this regime has not been previously explored.\n\nIn the following, we analyze how the addition of NINs produces a regularization scheme which we call Noise Injection Node Regularization (NINR). The main features of NINR are enhanced stability, simplicity, and flexibility, without drastically compromising generalization performance. In order to demonstrate these features, we consider two types of feed-forward architectures: Fully Connected Networks (FCs) and Convolutional Neural Networks (CNNs), and use various datasets to train the systems. We compare NINR robustness improvement with standard regularization methods, as well as performance of these systems when using input corruption during training (CDT). Our results\n\n2\n\nfdata(x,y)fimages(x,y)DomainShiftfcorrupted(x+ε,y)Generalizationfadv(x+εadv,y)Published as a conference paper at ICLR 2023\n\nFigure 2: Robustness against random input perturbations tested on FC network (left) and CNN (right). Test accuracy vs. the scale of input noise corruption defined in Eq. (7) is shown for L2, Dropout, in-NINR, fullNINR and CDT with σnoise = 0.4. Shades indicate 2 standard deviations estimated over 10 distinct runs. For the input-NINR (full-NINR) fully-connected implementations we take σε = 51.8 (16.4) in the decay phase and σε = 231.6 (51.8) in the catapult phase. Similarly, for the convolutional implementations we take σε = 2.8 (0.9) in the decay phase and σε = 87.5 (62) in the catapult phase. This key result illustrates that NINR significantly increases the robustness of generic architectures trained on the FMNIST dataset, while marginally affecting generalization (σnoise = 0). Comparing the CDT and input-NINR curves demonstrates the advantage of our regularization method. While both techniques perform similarly well on data corruption of σnoise = 0.4, CDT is significantly worse on clean data. This is a result of the CDT network being forced to fit both noise and data, without the ability to suppress the latter, a crucial attribute of NINR. Here, the learning rate is fixed to η = 0.05 with mini-batch size B = 128. Each training run is performed for 500 SGD training epochs in total, or until 98% training accuracy has been achieved. For further details, see Sec. 3 and App. A.\n\ncan easily be generalized to other architectures and more complex NINR topologies. In Fig. 2, we present our main results, comparing networks trained on the FMNIST dataset and demonstrating improved robustness against input perturbations without compromising generalization on clean data.\n\nThe paper is organized as follows. In Sec. 2 we briefly review important analytical and empirical results that are explored in depth in the work of Levi et al. (2022), demonstrating in this work how NINs implicitly generate adaptive regularization terms in the loss function. In Sec. 3, we empirically study the effectiveness of NINR. We begin by evaluating its effect on robustness against perturbations, including domain shifts and those adversarially designed, demonstrating the enhanced performance of NINR. We then verify that generalization performance on clean data is not hindered by training with NINR. We discuss related work in Sec. 4, finally concluding in Sec. 5.\n\n2 NOISE INJECTION NODES REGULARIZATION\n\nIn the following sections we explain how an effective regularization scheme against input corruption naturally emerges as a consequence of adding a NIN to a DNN. First, we discuss how the NIN generates implicit regularization terms directly from computing the effective loss function. Then, we review the adaptive nature of these terms as they relate to Noise Injection Weight (NIW) dynamics during training, and discuss the expected robustness gains depending on the evolution of the NIWs.\n\n2.1 EMERGENT REGULARIZATION TERMS\n\nIn order to see how NINs generate implicit regularization terms, we study a vanilla feed-forward DNN setup. Consider a supervised learning problem modeled by a neural network optimized under SGD, with an associated single sample loss function, L : Rdin → R. The loss depends on the model parameters θ = {W (l), b(l)|l = 0, ..., NL − 1}, where NL is the number of layers, and the weights and biases associated with a given layer are W (l) ∈ Rdl×dl+1, b(l) ∈ Rdl+1 . At each SGD iteration, a mini-batch B consists of a set of labeled examples, {(xi, yi)}|B| i=1 ∈ Rdin × Rdlabel. The addition of a NIN in a given layer, lNI, corresponds to a random scalar input, ε ∈ R, sampled repeatedly for each SGD training epoch from a chosen distribution1, connected via NIWs WNI ∈ R1×dlNI+1. We define for a given layer l, the preactivation z(l) = W (l)x(l) + b(l), therefore the addition of a NIN to a dense layer results in a translation to the preactivation at lNI, as z(lNI) → z(lNI) + εWNI.\n\n1One may also generate ε only once, before training. We empirically find no difference between the two\n\noptions, which is expected from large batch averaging. For |B| ≲ 10, differences begin to emerge.\n\n3\n\n0.00.20.40.60.81.020406080100σnoiseTestAccuracyConvolutionalDNNNoRegularizationIn-NNR-DecayIn-NNR-CatapultFull-NNR-DecayFull-NNR-CatapultL2DropoutCDT-0.4(a)(b)0.00.20.40.60.81.020406080100σnoiseTestAccuracyFullyConnectedDNN0.00.20.40.60.81.020406080100σnoiseTestAccuracyFullyConnectedDNN0.00.20.40.60.81.020406080100σnoiseTestAccuracyFullyConnectedDNNPublished as a conference paper at ICLR 2023\n\nThe batch-averaged loss function including a NIN can be written as a series expansion2 in the noise translation parameter εWNI, (cid:88)\n\n(cid:88)\n\neεW T\n\nNI∇\n\nz(lNI) L(θ; x, y).\n\n(1)\n\nL(θ, WNI; x, ε, y) =\n\nL(θ, WNI) =\n\n1 |B|\n\n1 |B|\n\n{x,y,ε}∈B\n\n{x,y,ε}∈B\n\nEquation (1) follows from noting that the NIN induced translation can be written as an operator. For further details see App. B. Expanding in the parameter εWNI, we obtain an infinite series given by\n\nL(θ, WNI) = L(θ) +\n\n∞ (cid:88)\n\nk=1\n\nRk(θ, WNI).\n\n(2)\n\nHere, L(θ) is the loss function in the absence of any NIN, while Rk are batch-averaged derivatives of the loss function with respect to the preactivations at the noise injected layer,\n\nRk(θ, WNI) ≡\n\n1 |B|\n\n(cid:88)\n\n(εW T\n\nNI · ∇z(lNI))k\n\n{x,y,ε}∈B\n\nk!\n\nL(θ; x, y).\n\n(3)\n\nThese functions are products of the moments of the injected noise, the values of the NIWs themselves, and preactivation derivatives of the loss function in the absence of injected noise.\n\nIt is impossible to estimate when a perturbative analysis in ε is valid without specifying L(θ; x, y), as all Rk may become equally important, or the series itself may not converge. Furthermore, since we will be interested in rather large values of ε, where the effect of higher Rk terms is noticeable, the validity of the perturbative calculation is called into question even further. However, in order to gain intuition, we first study how the training procedure is altered by the NIN in the limit of small ε ≪ 1. To make further progress, we will later validate our analysis below using a combination of empirical tests, and an investigation of a linear toy model where Rk = 0 for k > 2. For sufficiently small ε and analytic activation and loss functions, the series converges and the full loss is well-approximated by the first two leading terms in ε. For the rest of this work, we consider noise sampled from a distribution with zero mean, relaxing this assumption only for some empirical results in App. D.2. Under this assumption the first two terms can be cast into simple forms,\n\nR1 = W T\n\nNI · ⟨εglNI\n\n⟩,\n\nR2 =\n\n1 2\n\nW T\n\nNI⟨ε2HlNI⟩WNI .\n\n(4)\n\nHere, batch averaging is denoted by ⟨· · ·⟩, while glNI ∇z(lNI)∇T As proven in Levi et al. (2022), the magnitude of R1 can then be estimated using ⟨εglNI σ2 ε ⟨g2 lNI and σ2 up to corrections scaling as O((cid:112)1/|B|).\n\n= ∇z(lNI)L(θ, x, y) and HlNI = z(lNI)L(θ, x, y) are the network-dependent local gradient and local Hessian, respectively. ⟩2 ∼ ⟩ is the vector of the batch-averaged squared values of the local gradients ε ⟨HlNI⟩\n\nε is the variance of the injected noise3, while R2 may be estimated using ⟨ε2HlNI⟩ ≈ σ2\n\n⟩/|B|, where ⟨g2\n\nlNI\n\nWhile R1 may take both positive and negative values, the sign of HlNI depends on the network architecture. Since the spectrum of the local Hessian is generally unknown, our analytical results are only valid for certain limiting cases. Particularly, we focus on the case of Mean Squared Error (MSE) loss and linear activation functions, where we find that the local Hessian H is a positive semi-definite (PSD) matrix, implying that R2 is a strictly non-negative penalty term, and Rk terms with k > 2 vanish identically. For the motivated case of piecewise linear activations, it was shown in Botev et al. (2017) that the local Hessian is PSD, aside from non-analytical points, hinting that R2 acts as a regularizer for these networks as well. This implies that for networks with piecewise linear activations and MSE loss, an analysis similar to ours below, which keeps only the first two terms in the expansion of Eq. (2), is expected to hold not only for small ε, but also for large values. We will use this construction to understand how these terms evolve during training in the next section.\n\n2In practice, piecewise analytic activation functions such as ReLU are often used, and if the noise causes the crossing of a non-analytic point, the above expansion receives corrections. Empirically, we find this subtlety to not change any of our qualitative conclusions.\n\n3We note that the noisy loss function Eq. (1) is invariant under the simultaneous rescaling of wNI → λwNI and ε → λ−1ε. Nonetheless, the SGD optimization equations are not invariant under this transformation, implying, in particular, that the value of the injected noise variance, σ2 ε , is a relevant parameter, not degenerate with the initialization values of the NIW. As a consequence, in order to fully explore the parameter space of noise injection, the noise (or more precisely, its variance) cannot be assumed to be small, and large noise injection values must be considered.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: NIW dynamics during training for the various phases discussed in Sec. 2.2, for a single hidden layer FC network with ReLU activations trained on the full FMNIST dataset, as specified in App. A. Here, we show the evolution of the NIWs norm (blue) as well as the hidden layer weights norm |W (lNI+1)| (red) against the training (violet) and test (green) loss (solid) and accuracy (dashed). Left to right: The NIN magnitude determines the phase of the system, ranging from the smallest amount in the decoupled phase, to an overwhelming amount in the divergent phase. The behavior displayed by the NIWs, as well as the loss corroborates the predictions discussed in Sec. 2.2 and App. C, with experimental details in App. A.\n\nThe interpretation of R1 and R2 can now be made clear: R1 induces a constrained random walk for in the norm of noise injection weights as well as for the data weights at layers l > lNI, with a step size that changes according to the local gradient during training. On the other hand, R2, which doesn’t depend on |B|, can be understood as a straightforward regularization term for the local Hessian, working to reduce its eigenvalues. These results imply that in the limit of large batch size, and in particular full batch SGD (i.e., gradient descent), regularization via R2 is dominant. 4.\n\nFurther understanding of why pushing the local Hessian to smaller eigenvalues is expected to reduce the sensitivity to noise corruption comes by looking at the loss for corrupted inputs. Consider therefore a network without a NIN but with corrupted inputs, described by the substitution, x → x + δ, with δ a random vector. To arrive at similar expressions to Eqs. (1) to (4), one can transform the preactivations z(0) → z(0) + W (0)δ to obtain,\n\nL(θ)|x→x+δ =\n\n1 |B|\n\n(cid:88)\n\neδT W (0)·∇\n\nz(0) L(θ; x, y),\n\n(5)\n\n{x,y}∈B\n\nUnder the assumption that the components of the vector δ are drawn i.i.d. from N (0, σ2 two terms above assume simple forms5, similar to Eq. (4),\n\nδ ), the first\n\nR1 = ⟨δT W (0) · g0⟩,\n\nR2 =\n\nσ2\n\nδ Tr\n\n(cid:16)\n\n(W (0))T ⟨H0⟩W (0)(cid:17)\n\n.\n\n1 2\n\n(6)\n\nAs before, for sufficiently large |B|, R1 is subdominant and the main regularization term due to the noise is dictated by H0. Thus if a NIN is inserted to the first layer, it will act to reduce H0 and thereby reduce the sensitivity to data corruption. Furthermore, since DNN structure in general, and loss function in particular, couples the input layer to all succeeding layers, H0 contains information about deeper layers and will benefit from reducing the local Hessian away from the input layer. In Sec. 3 we show results for NINs coupled to the input layer or to all layers. The above readily generalizes in this setup, resulting in multiple emergent regularization terms, which we briefly discuss in App. B.\n\nDespite the similarities in their descriptions, we stress that a system trained on corrupted data and a system with a NIN are not the same. In the former the noise cannot be dynamically reduced without dramatically altering the optimization trajectory, implying that the DNN is not expressive enough to memorize the full data information (Ziyin et al., 2022). Conversely, in the latter, the noise has its own weights and the system can therefore improve by suppressing them without harming generalization. Nonetheless, both systems are driven towards regions with smaller local Hessian eigenvalues.\n\n2.2 EVOLUTION OF NOISE INJECTION WEIGHTS\n\nThe dynamical nature of the NINR and the corresponding NIWs strongly depends on the noise distribution, parameterized in this study by σε. While the NIWs are updated with each learning\n\n4In fact, it is shown in Levi et al. (2022) that all odd-terms in the expansion Eq. (2), are suppressed by the\n\nsquare root of the batch size, while the even terms are not.\n\n5We comment on the slight subtlety of biases in Eq. (6). In any reasonable scenario, biases would not be corrupted, but if bias is treated as the zeroth component of x, the zeroth component of δ should be ≡ 0. Taking this into account, the trace operation of Eq. (6) should not sum over the zeroth dimension.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nstep, only under certain conditions is their impact on the network performance actively suppressed as the training progresses. Below we briefly describe four distinct phases of the NIWs. These are illustrated in Fig. 3, where we show the evolution of the relevant quantities (weights, loss, accuracy) for a model trained on FMNIST, demonstrating the different behavior in each phase. A complete treatment of these phases is discussed in Levi et al. (2022), while a brief derivation relating them with regularization is given in App. C for a linear network.\n\nDecoupled phase. For σε ≪ 1 one has R1 ≫ R2, and the correction to the loss function may assume positive and negative contributions. As a consequence, the NIWs follow a small-step random walk without substantially affecting the behavior of the network.\n\nDecay phase. For larger but not too large σε, one may ensure R2 > R1 at initialization while the NIN can still be treated perturbatively. In this regime, the NIWs initially experience exponential decay until R2 ∼ R1, at which point they evolve according to the stochastic gradient. It is in this phase that one can begin to see noticeable improvement in robustness, with only minor slowing of the training. Increasing σε boosts the improvement until another phase is encountered.\n\nCatapult phase. The discrete nature of the training algorithm will result in a stiff numerical regime at sufficiently large σε. Above a critical value (for the linear network discussed in App. C we find σε,cat ∼ 2dlNI/η where dlNI is the dimension of the NIN layer and η is the learning rate), the effect of the NIN on the network is so significant that it causes an initial increase of the data weights, which in turn leads to an exponential increase for the loss function, followed by a recovery to a new minimum6. The improvement in robustness is most extreme in this phase; however, the convergence of the network is slowed somewhat, rendering the usefulness of this phase to only some applications. It is possible that a scheduled increase of the training rate after the recovery from the initial increase in the data weights could speed up the convergence. We leave such investigation to future work.\n\nDivergent phase. Further increasing σε leads the DNN to a breakdown of the dynamics, where the network is unable to suppress the NIN and thus cannot learn any information.\n\nThe above discussion of phases as a function of σε should be taken as schematic. Other hyperparameters, such as the batch size, may also influence the phase diagram. Nonetheless, we empirically observe these phases repeating across multiple architectures and tasks, and find them to broadly capture the evolution of the NIWs. Overall, the decay and catapult phases are expected to produce an increase in robustness against input perturbations, and we empirically verify this expectation in the following sections. While we only have an analytic prediction of σε,cat for a simple linear network, in other architectures it can also be obtained empirically using only the training data.\n\n3 EXPERIMENTS\n\nIn this section, we empirically show the effect of NINR on robustness for the different phases of noise injection, following similar methodologies to Hoffman et al. (2019). After discussing the two different architectures used in this paper, we begin our investigation by demonstrating that in certain cases NINR provides a significant increase in robustness against corruption of input data by random perturbations. We then discuss the performance of NINR for domain shifts, demonstrating its effectiveness. Next, we verify that NINR does not drastically reduce the network accuracy at the original task (e.g., before corruption). This is equivalent to ensuring the generalization properties of the network are not harmed due to the addition of NINs. In the main text we present results mostly for the FMINST dataset (Xiao et al., 2017). These results also extend to more complex scenarios, demonstrated in similar experiments for the CIFAR-10 (Krizhevsky et al., 2014) dataset in App. E, while evidence for improvement against adversarial attacks is given in App. D as well as results for other noise distributions and optimizers beyond SGD.\n\nThroughout this section, we compare NINR to both unregularized DNNs, and networks explicitly regularized using L2 or Dropout. We also compare NINR to implicit regularization by training with varying amounts of input data corruption. For all of our experiments, we use either an FC or a CNN (see Fig. 2 and App. A for full details). We optimize using vanilla SGD with cross-entropy loss. We preprocess the data by subtracting the mean and dividing by the variance of the training data, as is done for all subsequent datasets. The learning rate is fixed to η = 0.05 with mini-batch\n\n6An analogous phase related to the size of the training step was discussed in Lewkowycz et al. (2020).\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Left: An illustration of a fully connected NINR (fcNINR) for which a Noise Injection Node is appended to a representation x(l). Right: Implementation of NINR for a convolutional network (cNINR), where the Noise Injection Node is connected pixel-wise to the image representation x(l), to be subsequently fed into a convolutional layer.\n\nsize B = 128. Each training run is performed for 500 SGD training epochs in total, or until 98% training accuracy has been achieved, unless otherwise specified. All test accuracy evaluations are done with the NIN output set to 0, i.e., ε = 0. The model parameters θ, WNI are initialized at iteration t = 0 using a normal distribution as σ2 = 1/dl, 1/dlNI. The hyperparameters are θ0 chosen to match reference implementations: the L2 regularization coefficient (weight decay) is set to λWD = 5 · 10−4 and the dropout rate is set to pdrop = 0.5. When using L2 or Dropout, they are applied at/after each layer. When using input CDT as a regularization method, we corrupt the input data according to Eq. (7) below. We further stress that once a NIN has been added to the network, no further modifications to the training algorithm or architecture are required, and after choosing where to connect the NIN, the only free parameter is the injected noise variance σ2 ε .\n\n, σ2\n\nWNI,0\n\n3.1 REALIZATIONS IN DIFFERENT ARCHITECTURES\n\nThe way in which NINR is implemented depends on the type of layer to which the NIN is connected. Here, we comment on the two different realizations of NINR used in our experiments and depicted in Fig. 4. For both realizations we consider two distinct topologies: either we add a NIN at the input layer (in-NINR) or we couple the NIN to every hidden layer including the input one (full-NINR).\n\nFully Connected Layers In the case of dense FC layers, we implement NINR (which we denote by fcNINR) by extending the input vector by an additional noisy pixel ε, initialized randomly per sample at each training epoch, and densely connecting the modified input vector to the next layer. The theoretical discussion in Sec. 2 was derived for a realization of this type.\n\nConvolutional Layers Connecting a NIN at the input of a convolutional layer raises the need for a procedure for Convolutional NINR (cNINR). Since FC layers are insensitive to the input image geometry, taking x → {x, ε} is tantamount to adding a noise mask for the entire input. In a CNN, the same interpretation can be maintained by adding the noise to the input directly in a pixel-wise fashion, x(l) → x(l) + WNI · ε , which is subsequently fed into the convolutional layer. Importantly, this modification preserves the form of the original layer, while converging to the original x(l) for either σε → 0 or ||WNI|| → 0. This can also be thought of as adding an auxiliary layer that is a non-dynamic identity matrix from the perspective of all data weights, while being densely connected from the perspective of the NIN7.\n\n3.2 ROBUSTNESS AGAINST DISTRIBUTIONAL SHIFTS\n\n3.2.1\n\nINPUT CORRUPTION\n\nOften, training is done with examples taken in ideal conditions, which would not always exist in real-world data. This implies that the test data would be sampled from a distribution that is identical to the one trained on, albeit with an added noise component. To test the stability of networks against natural corruption, we perturb each test input image according to\n\n(cid:113)\n\nxi →\n\n1 − σ2\n\nnoisexi + σnoiseδi,\n\n(7)\n\n7As our procedure for cNINR preserves the structure of the original x(l), it can be easily applied for other\n\narchitectures beyond convolutional layers, including densely connected layers.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: A test of domain shift, trained on MNIST up to 100% training accuracy and evaluated on the USPS test dataset, comparing L2, Dropout, in-NINR, and full-NINR as discussed in the text. We exclude CDT from this table, as it is not as prevalent as the other regularization schemes, and is tailored for random noise. The fully-connected and CNN NINR noise magnitudes are those of Fig. 2. Errors indicate 2σ confidence intervals over 10 distinct runs for full training. We find the test accuracy on the full clean MNIST dataset to be similar across all regularization schemes, with ∼ 97% and ∼ 98% for FC and CNN respectively.\n\nNone\n\nL2\n\nDropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nFC(%) CNN(%)\n\n82.3 ± 2.0 80.3 ± 4.8\n\n82.5 ± 1.2 80.5 ± 3.8\n\n87.5 ± 1.2 83.8 ± 4.6\n\n82.1 ± 1.6 82.6 ± 4.6\n\n84.1 ± 2.0 88.5 ± 3.4\n\n82.0 ± 2.0 82.3 ± 4.0\n\n83.3 ± 1.4 89.7 ± 2.0\n\nwhere each component of the perturbation vector is drawn from N (0, 1). In all cases except CDT, the networks are trained using clean FMNIST training data, but their accuracy is evaluated on corrupted FMNIST testing data.\n\nIn Fig. 2, we demonstrate that models trained with NINR are more robust against the noise defined above compared to those trained via other regularization methods. This verifies our expectation that in the decay phase, NINR improves stability, at least as well as CDT for large corruption, while (unlike CDT) it does not degrade generalization performance for small corruption or clean data. We also note that noise injection offers the best results for robustness within the catapult regime. However, to arrive at the same accuracy on the clean test dataset, more training epochs are generally required. Lastly, we empirically observe that in-NINR in the catapult phase offers the greatest improvement in stability against input corruption for both networks, as in-NINR most closely resembles input data corruption. We repeat this experiment for the CIFAR-10 dataset in App. E, where the similar trends persist, though at the cost of a longer training time in the catapult phase.\n\n3.2.2 DOMAIN SHIFT\n\nAnother test of the generalization properties induced by NINR can be realized by considering Domain Shift problems. Here, we consider the generalization between two different datasets, representing different marginal distributions, by training models with NINR on the MNIST dataset, and testing their performance on data drawn from a new target domain distribution: the USPS test set (Hull, 1994). In order to match the input dimensions of the MNIST data, we follow the original rescaling and centering done in LeCun et al. (1998). The USPS images were size normalized to fit a 20 × 20 pixel box while preserving their aspect ratio, and then centered in a 28 × 28 image field, followed by the standard preprocessing procedure.\n\nThe results are presented in Table 1. We observe generalization improvement for both FC and convolutional networks when using different regularization schemes, compared to unregularized networks. Of particular interest are the gains obtained when implementing both in- and full-NINR in the catapult phase, with the convolutional network. As the architecture becomes more complex, the improvements from NINR’s adaptive scheme becomes more pronounced. The enhanced performance implies that NINR in the catapult phase could prove very beneficial for domain adaptation tasks. This is not entirely surprising as the USPS dataset is expected to lie close to the MNIST training set in distribution space, as there are no new correlated features such as several different digits in one image. Further experiments for domain shift adaptation on the MNIST-C dataset can be found in App. F. For other datasets, with large distributional shifts away from MNIST, and novel input correlations, we do not expect NINR to generically outperform other regularization methods.\n\n3.2.3 GENERALIZATION TO TEST DATA\n\nIn this section we report some effects of NINR on generalization from training to test data. While we showed above that NINR can substantially improve network performance on corrupted data, it is also important that it does not fundamentally impair the network’s generalization properties.\n\nGenerically, introducing input corruption during training to increase robustness can be shown to have a negative effect on generalization on clean data. This is unsurprising as it appears that the network essentially memorizes the noise (Zhang et al., 2016), which is clearly not part of the true data distribution. As the learning process with NINR inherently leads to a suppression of the noise\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Generalization on clean test data, evaluated on the FMNIST dataset. Comparison is made between L2, Dropout, in-NINR, full-NINR and CDT, and we highlight regularization methods which worsen generalization by italicizing. For CDT, two values (σnoise = 0.2, 0.4) for the amount of corruption are considered. The fully-connected and CNN NINR noise parameters are those used in Fig. 2. Errors indicate 2σ confidence intervals over 10 distinct runs. The NINR implementations, except perhaps the in-NINR at the catapult phase, have comparable generalization performance with to the rest of the regularization schemes, aside from CDT, where performance is diminished as the network learns the noisy distribution rather than the original one.\n\nNone\n\nL2\n\nDropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nCDT (0.2)\n\nCDT (0.4)\n\n87.7 ± 2.6 89.9 ± 3.4 88.3 ± 0.5 89.1 ± 0.7 86.2 ± 0.8 88.5 ± 1.8 88.2 ± 0.6 86.6 ± 0.9 85.5 ± 3.8 FC(%) CNN(%) 91.0 ± 1.0 92.2 ± 0.7 91.0 ± 1.1 91.0 ± 1.2 89.0 ± 0.6 91.0 ± 0.8 90.0 ± 0.2 84.6 ± 2.6 84.1 ± 6.4\n\nduring the late stages, its generalization capabilities are expected to be far less affected. We verify this by comparing the performance of a network trained with NINR against networks trained with L2, Dropout, CDT, and against unregularized DNNs.\n\nIn Table 2 we show generalization performance on the FMNIST test set for the FC and CNN architectures using the full dataset, consisting of 60 000 training examples with a 60/40 training/validation split. Our main observation is that optimizing with NINR in the decay phase, as with the commonly used L2 and dropout regularizers, leads to performance on clean data with indomain test samples as least as good as the unregularized case. (In no case is the performance better at a statistically significant level.) We note that some degradation occurs when training with noise injection in the catapult phase, for a fixed number of training epochs. This degradation can be ameliorated by training for a longer period. Contrasting NINR with CDT, Table 2 clearly demonstrates that generalization is compromised for the latter, as the network cannot distinguish data from noise, learning the corrupted distribution. We further verify these results for CIFAR-10 in App. E.\n\n4 RELATED WORK\n\nNoise injection during training as a method of enhancing robustness has been proposed in various configurations in the literature. These include adding noises to input data (Hendrycks et al., 2019; Gao et al., 2020; Liu et al., 2021), activations, outputs, weights, gradients (Holmstr ̈om & Koistinen, 1992; Reed & Marks, 1999; Neelakantan et al., 2015; You et al., 2019) and more. Most studies keep the amount of injected noise fixed, while we allow the network to reduce its effect during training.\n\nOur study expands upon these works, consolidating empirical evidence with analytical insights. Our main contributions are twofold: We provide analytic expressions for the implicit regularization terms generated within our scheme, as well as estimating their effects during training. When applied to specific architectures, this allows us to predict when NINR is expected to be most effective. Additionally, we probe a novel phase of learning, starting with a large amount of noise injection and leading to a greater improvement in robustness against input corruption. Works by Rakin et al. (2018) and Xiao et al. (2021) follow similar reasoning, though both are limited, by construction, to a small amount of noise injection, and are more empirically driven. Rakin et al. (2018) and Rusak et al. (2020) also feature complex custom update steps which are less adaptable to other architectures.\n\n5 CONCLUSIONS\n\nIn this paper, we motivated Noise Injection Node Regularization as a task-agnostic method to improve stability of models against perturbations to input data. Our method is simply implementable in any open source automatic differentiation system.\n\nWhile we restricted this initial study to a single Noise Injection Node added to various layers, with a fixed scale of noise injection during training, this restriction can be relaxed, leading to potential improvements to NINR. For instance, changing the amount of injected noise during training, similar to learning rate scheduling, could aid in convergence speed while still obtaining the advantages of a large amount of noise injection.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n6 ACKNOWLEDGEMENTS\n\nWe thank Yasaman Bahri, Kyle Cranmer, Guy Gur-Ari, and Sho Yaida for useful discussions and comments. NL would like to thank the Milner Foundation for the award of a Milner Fellowship. MF is supported by the DOE under grant DE-SC0010008 and the NSF under grant PHY1316222. MF would like to thank Tel Aviv University, the Aspen Center for Physics (supported by the U.S. National Science Foundation grant PHY-1607611), and the Galileo Galilei Institute for their hospitality while this work was in progress. The work of TV is supported by the Israel Science Foundation (grant No. 1862/21), by the Binational Science Foundation (grant No. 2020220) and by the European Research Council (ERC) under the EU Horizon 2020 Programme (ERC-CoG-2015 - Proposal n. 682676 LDMThExp).\n\nREPRODUCIBILITY STATEMENT\n\nIn Sec. 2, we state our theoretical results, ensuring that we state our assumptions and the limitations of the approximations we make at every step. In several instances, we rely on proofs given in other works, as well as supplement our analyses in App. C; The models and tools used for analysis in our experiments are provided in the following anonymous link: https://anonymous.4open. science/r/NoiseInjectionNodeCode-2A68, while explicit details regarding our experimental setup as well as a complete description of the data processing steps for the datasets we used, are given in Sec. 3 and App. A.\n\nREFERENCES\n\nAleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss–Newton optimisation for\n\ndeep learning, 2017. URL https://arxiv.org/abs/1706.03662.\n\nSubrahmanyan Chandrasekhar. Hydrodynamic and hydromagnetic stability. Clarendon Press, Ox-\n\nford, 1961.\n\nCorinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273–297,\n\n1995.\n\nPhilip G. Drazin and William H. Reid. Hydrodynamic stability. Cambridge University Press, Cam-\n\nbridge, 2nd edition, 2004.\n\nXiang Gao, Ripon K. Saha, Mukul R. Prasad, and Abhik Roychoudhury. Fuzz testing based data augmentation to improve robustness of deep neural networks. In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE), pp. 1147–1158, 2020.\n\nIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples, 2014. URL https://arxiv.org/abs/1412.6572.\n\nDan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty. arXiv e-prints, art. arXiv:1912.02781, December 2019.\n\nGeoffrey Hinton, 2012.\n\nURL http://www.cs.toronto.edu/ ̃tijmen/csc321/\n\nslides/lecture_slides_lec6.pdf.\n\nGeoffrey E. Hinton. Learning translation invariant recognition in a massively parallel networks. In J. W. de Bakker, A. J. Nijman, and P. C. Treleaven (eds.), PARLE Parallel Architectures and Languages Europe, pp. 1–13, Berlin, Heidelberg, 1987. Springer Berlin Heidelberg. ISBN 9783-540-47144-8.\n\nJudy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust learning with Jacobian regularization,\n\n2019.\n\nLasse Holmstr ̈om and Petri Koistinen. Using additive noise in back-propagation training.\n\nIEEE\n\nTransactions on Neural Networks, 3(1):24–38, 1992. doi: 10.1109/72.105415.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJ. J. Hull. A database for handwritten text recognition research.\n\nIEEE Transactions on Pattern\n\nAnalysis and Machine Intelligence, 16(5):550–554, 1994. doi: 10.1109/34.291440.\n\nJames H. Jeans and George H. Darwin.\n\nI. The stability of a spherical nebula. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 199(312-320):1–53, 1902. doi: 10.1098/rsta.1902.0012. URL https: //royalsocietypublishing.org/doi/abs/10.1098/rsta.1902.0012.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL\n\nhttps://arxiv.org/abs/1412.6980.\n\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (canadian institute for advanced\n\nresearch). online: http://www.cs.toronto.edu/kriz/cifar.html, 55(5), 2014.\n\nAnders Krogh and John Hertz. A simple weight decay can improve generalization. In J. Moody, S. Hanson, and R.P. Lippmann (eds.), Advances in Neural Information Processing Systems, volume 4. Morgan-Kaufmann, 1991. URL https://proceedings.neurips.cc/paper/ 1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf.\n\nJan Kukaˇcka, Vladimir Golkov, and Daniel Cremers. Regularization for deep learning: A taxonomy,\n\n2018. URL https://openreview.net/forum?id=SkHkeixAW.\n\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world,\n\n2016. URL https://arxiv.org/abs/1607.02533.\n\nYann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5. 726791.\n\nNoam Levi, Itay Bloch, Marat Freytsis, and Tomer Volansky. Noise injection as a probe of deep\n\nlearning dynamics, 2022. URL https://arxiv.org/abs/2210.13599.\n\nAitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218, 2020.\n\nAishan Liu, Xianglong Liu, Hang Yu, Chongzhi Zhang, Qiang Liu, and Dacheng Tao. Training IEEE Transactions on Image\n\nrobust deep neural networks via adversarial noise propagation. Processing, 30:5769–5781, 2021. doi: 10.1109/TIP.2021.3082317.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks, 2017. URL https://arxiv. org/abs/1706.06083.\n\nJames Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate\n\ncurvature, 2015. URL https://arxiv.org/abs/1503.05671.\n\nNorman Mu and Justin Gilmer. MNIST-C: A robustness benchmark for computer vision. CoRR,\n\nabs/1906.02337, 2019. URL http://arxiv.org/abs/1906.02337.\n\nArvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv e-prints, art. arXiv:1511.06807, November 2015.\n\nEugene N. Parker. Dynamical instability in an anisotropic ionized gas of low density. Phys. Rev., 109:1874–1876, Mar 1958. doi: 10.1103/PhysRev.109.1874. URL https://link.aps. org/doi/10.1103/PhysRev.109.1874.\n\nAdnan Siraj Rakin, Zhezhi He, and Deliang Fan. Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack, 2018. URL https:// arxiv.org/abs/1811.09310.\n\nRussell Reed and Robert J. Marks, II. Neural smithing: supervised learning in feedforward artificial\n\nneural networks. MIT Press, 1999.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nEvgenia Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. Increasing the robustness of dnns against image corruptions by playing the game of noise. CoRR, abs/2001.06057, 2020. URL https://arxiv. org/abs/2001.06057.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv e-prints, art. arXiv:1409.1556, September 2014.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Journal of Machine Dropout: A simple way to prevent neural networks from overfitting. Learning Research, 15(56):1929–1958, 2014. URL http://jmlr.org/papers/v15/ srivastava14a.html.\n\nSteven H. Strogatz. Nonlinear dynamics and chaos: with applications to physics, biology, chemistry,\n\nand engineering. CRC press, 2018.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, D. Erhan, Ian J. Goodfellow, and\n\nRob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2014.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-\n\nmarking machine learning algorithms, 2017.\n\nLi Xiao, Zeliang Zhang, and Yijie Peng. Noise optimization for artificial neural networks. arXiv\n\ne-prints, art. arXiv:2102.04450, February 2021.\n\nZhonghui You, Jinmian Ye, Kunming Li, Zenglin Xu, and Ping Wang. Adversarial noise layer: Regularize neural network by adding noise. In 2019 IEEE International Conference on Image Processing (ICIP), pp. 909–913, 2019. doi: 10.1109/ICIP.2019.8803055.\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016. URL http: //arxiv.org/abs/1611.03530.\n\nGuodong Zhang, Chaoqi Wang, Bowen Xu, and Roger B. Grosse. Three mechanisms of weight decay regularization. CoRR, abs/1810.12281, 2018. URL http://arxiv.org/abs/1810. 12281.\n\nAlice Zheng, Michael Jordan, Ben Liblit, and Alex Aiken. Statistical debugging of sampled programs. In S. Thrun, L. Saul, and B. Sch ̈olkopf (eds.), Advances in Neural Information Processing Systems, volume 16. MIT Press, 2003. URL https://proceedings.neurips.cc/ paper/2003/file/0a65e195cb51418279b6fa8d96847a60-Paper.pdf.\n\nLiu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. Strength of minibatch noise in SGD. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=uorVGbWV5sw.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA NETWORK ARCHITECTURE DETAILS\n\nHere we describe the experimental settings specific to each of the figures in the paper. All the models have been trained with cross-entropy loss unless otherwise specified.\n\nFig. 2(a). Fully connected, three hidden layers with width d1,2,3 = 1024, weight initialization W (l) ∼ N (0, 1/dl), b = 0. ReLU activation, trained using SGD (no momentum) on FMNIST, with a learning rate of η = 0.05 and batch size |B| = 128.\n\nFig. 2(b). CNN composed of 2 convolutional blocks, followed by a dense ReLU layer with width d3 = 2048 and a dense connection to the prediction layer, weight initialization W (l) ∼ N (0, 1/dl), b = 0. Each convolutional block is taken as Conv2D(2,2) → ELU → Batch-Norm → MaxPool(2,2), trained using SGD (no momentum) on FMNIST, with a learning rate of η = 0.05 and |B| = 128.\n\nFig. 3. Fully connected, one hidden layer with d1 = 1024, weight initialization W (l) ∼ N (0, 1/dl), b = 0. ReLU activation, trained using SGD (no momentum) on FMNIST. with learning rate η = 0.01 and |B| = 1000. From left to right, the injected noise is σ2 ε = {0, 0.1 · din/η, din/η, 1.8 · din/η}, corresponding to the decoupled, decay, catapult, and divergent phases, respectively. Here, din = 785 is the dimension of the input data including a single NIN.\n\nB ADDITIONAL THEORETICAL DETAILS\n\nB.1 DERIVATION OF THE NOISE TRANSLATED LOSS FUNCTION\n\nHere we provide additional details on the theoretical analysis of the noise translated loss function, leading to Eq. (1). The introduction of a NIN at a specific layer lNI translates the preactivation as z(lNI) + WNIε. A single sample loss function describing the translated preactivation can be written as\n\nL(θ, WNI; x, ε, y) = L(θ; z(lNI) + WNIε, y).\n\nUsing the definition of the translation operator\n\nf (x + a) = ea∇f (x),\n\n(8)\n\n(9)\n\nwe explicitly compute the batch averaged loss function\n\nL(θ, WNI) =\n\n1 |B|\n\n(cid:88)\n\n{x,ε,y}∈B\n\nL(θ, WNI; x, ε, y) =\n\n1 |B|\n\n(cid:88)\n\neεW T\n\nNI∇\n\nz(lNI) L(θ; x, y)\n\n{x,ε,y}∈B\n\n= L(θ) +\n\n1 |B|\n\n(cid:88)\n\n{x,ε,y} ∈B\n\n∞ (cid:88)\n\nk=1\n\n1 k!\n\n(εW T\n\nNI · ∇z(lNI))kL(θ; x, ε, y).\n\nExpanding in powers of εWNI, we obtain an infinite series given by\n\nL(θ, WNI) = L(θ) +\n\n(εW T\n\nNI · ∇z(lNI))kL(θ; x, ε, y),\n\n(10)\n\nk=1 where we identify the k ≥ 1 terms in the expansion with the implicit regularization terms defined in Eq. (3).\n\n{x,ε,y}∈B\n\n1 |B|\n\n(cid:88)\n\n∞ (cid:88)\n\n1 k!\n\nB.2 NIN AT ALL LAYERS\n\nHere we extend our theoretical derivations from the case of a NIN connected to a single layer, to a single NIN connected to all layers (the full-NINR case).\n\nSimilar to Eq. (1), we may write down the loss using the translation operator,\n\nL(θ, WNI) =\n\n1 |B|\n\n(cid:88)\n\n(cid:32)NL−1 (cid:89)\n\n{x,y,ε}∈B\n\nl=0\n\n(cid:33)\n\neε(W (l)\n\nNI )T ∇\n\nz(l)\n\nL(θ; x, y),\n\n(11)\n\nwhere as one may expect, we now have NL vectors W (0) , of respective dimensions R1×d1 , ..., R1×dNL . Focusing once more on the leading terms in ε, we can see that the first order\n\nNI , ..., W (NL−1)\n\nNI\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nregularization term is simply\n\nR1 =\n\nNL−1 (cid:88)\n\n(cid:16)\n\nl=0\n\n(cid:17)T\n\nW (l)\n\nNI\n\n· ⟨εgl⟩,\n\n(12)\n\nwhich is the sum of the regularization terms at each layer. The second order regularization is slightly more complex, and may be written as\n\nR2 =\n\n1 2\n\nNL−1 (cid:88)\n\nNL−1 (cid:88)\n\n(W (l1)\n\nNI )T ⟨ε2Hl1l2⟩W (l2)\n\nNI\n\nl1=0\n\nl2=0\n\n,\n\n(13)\n\nz(l2) L(θ, x, y).\n\nwith Hl1l2 ≡ ∇z(l1) ∇T According to Botev et al. (2017); Martens & Grosse (2015), the terms which mix different layers in the Hessian are expected to be small, thus leading us to a sum over the single-layer R2s of the each l. Therefore, we may use the same arguments used in the main text to estimate the scaling of the two terms with σε and |B|. Much like the single-layer case, we therefore expect R1 ∝ σε/(cid:112)|B| and R2 ∝ σ2 ε .\n\nDemonstrating the emergence of the phases discussed in the main text (i.e. the decay phase, the catapult phase, etc.) on a deep linear network for this full-NINR case is beyond the scope of this work. However, we do note that empirically they are found to be present much like in the singlelayer NIN case.\n\nC NINR IN A LINEAR TOY MODEL\n\nFigure 5: Illustration of the univariate linear DNN with a single input scalar, noise node, and one hidden layer.\n\nIn order to elucidate the interpretation of noise injection nodes as an emergent regularization scheme, combined with a form of constrained random walk, we employ an (over)simplified univariate linear model which captures the main features present in realistic networks. Consider a linear network (i.e., linear activation functions), with a single hidden layer and no biases (b = 0), aiming to perform a linear regression task. The data consists of a set of training samples (cid:8)(xa, ya) ∈ R2(cid:9)m a=1, taken by drawing the inputs from a normal distribution of xa ∈ X ∼ N (0, σ2 x), where σx > 0. The corresponding outputs are then given by a linear transformation ya = M · xa with M ∈ R.\n\nThe noise node is added at the input, and its weight is wNI, with the input’s data weight being w(0). The hidden layer is directly connected to the output, and has a single weight associated with it, w(1), as illustrated in Fig. 5. We use Mean Squared Error (MSE) loss, and for simplicity take a full-batch gradient descent, and thus our loss function is\n\nLMSE =\n\n1 2|B|\n\n(cid:88)\n\na∈B\n\n(cid:16)\n\nw(1)(w(0) · xa + wNIεa) − ya\n\n(cid:17)2\n\n.\n\n(14)\n\nPerforming an explicit averaging, we can further simplify to\n\n(cid:34)\n\nLMSE ≃\n\n1 2\n\n2w(1)wNI\n\n(cid:16)\n\nw(1)w(0) − M\n\n(cid:17)\n\nσxσε\n\nΦ (cid:112)|B|\n\n14\n\n+ (w(1)w(0) − M )2σ2\n\nx + (w(1))2w2\n\nNIσ2\n\nε\n\n(cid:35)\n\n,\n\n(15)\n\nPublished as a conference paper at ICLR 2023\n\nwhere Φ is a random variable with zero mean and unit variance8. From Eq. (15), we can easily see that the optimal solution is achieved for the weights w(1) ∗ = M , wNI,∗ = 0. We can clearly read R1,2 off Eq. (4) by separating them from the unperturbed loss function\n\n∗ w(0)\n\nLMSE(wNI, θ) ≃ LMSE(θ) + R1(wNI, θ) + R2(wNI, θ),\n\n(16)\n\nwhile Rk vanish for k > 2. The various terms in Eq. (16) are given by\n\nLMSE(θ) =\n\n1 2\n\n(w(1)w(0) − M )2σ2 x,\n\nR1(wNI, θ) = wNI⟨εglNI\n\n⟩ = wNIw(1) (cid:16)\n\nw(1)w(0) − M\n\n(cid:17) σxσεΦ (cid:112)|B|\n\n,\n\n(17)\n\nσ2\n\n1 2\n\nε w2\n\nNIHlNI =\n\nR2(wNI, θ) =\n\n1 2\nwhere we have identified the local gradient term which generates a constrained random walk for wNI, which decreases as the network approaches its data driven minimum. We also note that in the limit of an infinite batch size |B| → ∞ it vanishes, leaving only an effective regularization term for the hidden layer weight, namely L2 = λ(w(1))2 where the Lagrange multiplier λ = 1 NIσ2 decreases with time as the noise weight wNI is pushed to 0.\n\n(w(1))2w2\n\nNIσ2 ε ,\n\n2 w2\n\nε\n\nWe may glean further insights from this linear example by studying its training dynamics for small and large noise variances. Assuming full batch gradient descent in the infinite sample limit, we neglect the local gradient contribution and focus on the coupled equations for the hidden layer weight and the noise weight, given respectively by\n\nw(1)\n\nt+1 = w(1)\n\nt\n\n(1 − ησ2\n\nwNI,t+1 = wNI,t(1 − ησ2\n\nε w2 ε (w(1)\n\nNI,t) − η(w(1) )2)\n\nt\n\nt w(0)\n\nt − M )w(0)\n\nt σ2 x,\n\n(18)\n\nAssuming M ̸= 0, without the loss of generality, we may set M = 1 as the equations remain invariant under reparameterization9. In the limit of σ2 NI,0, the equations decouple, with the data weight following the standard GD equation without noise, i.e., w(1) t − x, while the noise weight decays exponentially as long as 0 < |w(1) 1)w(0) ε /2. Clearly, the smaller σε is, the smaller the regularizing effect of the noise on the local Hessian, given by the square of the hidden layer weights in this simple model.\n\nt w(0) 0 )2 > ησ2\n\nt −η(w(1)\n\nt+1 = w(1)\n\nε ≪ 1/ηw2\n\n| and (w(1)\n\nt σ2\n\nt\n\nWe expect that in this regime, the limit of continuous time GD should reproduce the correct dynamics as ησ2\n\nε → 0, yielding a differential equation for the noise weight\n\n ̇wNI(t) = −σ2\n\nε (w(1)(t))2wN I (t),\n\n(19)\n\nwhere ̇x = dx/dt is the continuous time derivative. The noise weight can therefore only decay.\n\nConversely, taking the large noise variance limit we find that the dynamics are ignorant of the original learning objective, as the resulting equations become simply coupled w(1)\n\n(1 − ησ2\n\nt+1 = w(1)\n\nt\n\nwNI,t+1 = wNI,t\n\n(cid:16)\n\n1 − ησ2\n\n)2(cid:17)\n\n.\n\n(20)\n\nε w2\n\nNI,t), ε (w(1)\n\nt\n\n ̇wNI(t) = −σ2\n\nThese equations describe a NN, trained using completely random data with no labels or learning In this case, we expect the objective, with an effective loss given by the last term in Eq. (15). continuous time limit to fail as a complete description of the possible dynamics, as ησ2 ε may be large. We may demonstrate this failure by taking the continuous time limit, obtaining ε (wNI(t))2w(1)(t),\n\n(21) implying both weights decrease in magnitude. This means the network, even for arbitrarily large σε will not diverge. However, this is clearly not the case for the discrete Eq. (20), which may become stiff for sufficiently large noise variance. This numerical artifact entirely changes the weight behavior, opening up the possibility for the system to either diverge, or catapult, as discussed in Levi et al. (2022). To summarize, this simple example provides a useful test case for our main analytical derivations appearing in the main text, displaying all the expected features of NINR in a fully calculable setting.\n\nε (w(1)(t))2wN I (t),\n\n ̇w(1)(t) = −σ2\n\n8Additional O(σ2\n\nε /(cid:112)|B|) corrections coming from stochastic variations in the σ2\n\nε term emerge from batch-\n\naveraging but are neglected.\n\n9Taking w(1) → M w(1), wNI → M wNI and σε → σε/M leaves the equations invariant.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nD ADDITIONAL EXPERIMENTS\n\nThroughout this section, we train the FC and the CNN using the same specifications as given in Fig. 2, unless otherwise specified. Training is performed for the minimum between 500 epochs, and the time it takes the network to reach 98% training accuracy. This is done with the goal of demonstrating that NINR using a large amount of noise injection requires a longer period of training, otherwise suffering from degraded generalization performance, as discussed in the main text.\n\nD.1 ADVERSARIAL ATTACKS\n\nIn addition to input perturbations caused by deployment issues, natural degradation, and unexpected noise sources, targeted perturbations, meant to maximally impair the performance of a network while changing the data as little as possible, form a conceptually different concern. Quantifying what corresponds to a minimal distortion of the data is a domain-specific and somewhat subjective task. Nevertheless, standard approaches exist. One of the simplest known implementations for an adversarial attack is the white-box untargeted Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014), which transforms inputs according to\n\nx → x + δFGSM × sign(∇xL(θ; x, y)), (22) where δFGSM is a small positive parameter that controls the size of the perturbation. We also consider the Projected Gradient Descent (PGD) attack (Kurakin et al., 2016; Madry et al., 2017), which iterates the FGSM attack k times, compounding its effect.\n\nFigure 6: Robustness against adversarial attacks. Left: FC network, Right: CNN (detailed specifications are in App. A). This key result illustrates that NINR significantly increases the robustness of generic model architectures trained on the FMNIST dataset. Shades indicate 2 standard deviations estimated over 10 distinct runs.\n\nIn Fig. 6 we compare the performance of standard regularization schemes with NINR against FGSM and PGD type adversarial attacks. We find that NINR displays superior performance over L2 and un-regularized nets. For FGSM attacks dropout performs best among the options tested, while for PGD attacks NINR outperforms.\n\nThese preliminary results suggest potential improvement against certain types of adversarial attacks when NINR is used. Further analysis is required to determine whether combining NINR with other regularization schemes, or changing the noise distribution during training could potentially produce a more successful scheme.\n\nD.2 DIFFERENT NOISE DISTRIBUTIONS\n\nHere, we examine the effects of sampling the NINs from different noise distributions on the performance of NINR. For each different noise distribution, we repeat the tests used to produce Fig. 2, demonstrating robustness against corrupted inputs. We compare results using a uniform distribution ε ∼ U (−σε, σε) and an asymmetric (double Gaussian peaked at ±σε) distribution, for fcNINR and cNINR using DNNs trained on the FMNIST dataset.\n\nIn Fig. 7, we see that varying the noise distribution has a minimal effect on NINR as a regularization scheme, aside from the asymmetric distribution for the catapult phase. We attribute this behavior to an extreme choice of noise injection scale, where a much longer training time is required to obtain good performance for NINR.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Robustness against random input perturbations for FC (top row) and convolutional (bottom row) networks using NINR training with different noise distributions (detailed specifications are in App. A). Left: Asymmetric double gaussian distribution peaked at ±σε , Right: Uniform distribution ε ∼ U (−σε, σε) . The fully-connected and CNN NINR noise magnitudes are those of Fig. 2. Shades indicate 2 standard deviations estimated over 5 distinct runs.\n\nD.3 DIFFERENT OPTIMIZERS\n\nHere, we examine the effects of changing the optimization algorithm, beyond SGD, on the performance of NINR. For each different optimizer, we repeat the tests used to produce Fig. 2, demonstrating robustness against corrupted inputs. We compare results using RMSprop (Hinton, 2012) and Adam (Kingma & Ba, 2014), for fcNINR and cNINR using DNNs trained on the FMNIST dataset. Here, we use different parameters for the different architectures and optimizers. Namely, RMSprop - ρ = 0.9, ε = 10−7 and η = 0.0001 for FC and η = 0.001 for CNN. Adam - β1 = 0.9, β2 = 0.999, ε = 10−7 and η = 0.01 for both FC and CNN, with noise injection magnitudes given in App. D.3.\n\nFigure 8: Robustness against random input perturbations for FCs (top row) and CNNs (bottom row) using NINR training under different optimization schemes (detailed specifications are in App. A). Left: Adam, trained with η = 0.01 for both FC and CNN, Right: RMSprop, trained with η = 0.0001 for FC and η = 0.001 for CNN. The fully-connected and CNN NINR noise magnitudes are those of App. D.3. Shades indicate 2 standard deviations estimated over 5 distinct runs.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Amount of noise injection (σε) for the different architectures using RMSprop and Adam\n\nRMSprop\n\nAdam\n\nFC CNN\n\nFC CNN\n\nin-NINR - decay (catapult) full-NINR - decay (catapult)\n\n1158.2 (5179.9) 19.6 (619.1)\n\n366.3 (1158.2) 6.19 (437.8)\n\nin-NINR - decay (catapult) full-NINR - decay (catapult)\n\n115.8 (518) 6.2 (195.8)\n\n36.6 (115.8) 1.95 (138.4)\n\nE RESULTS FOR CIFAR-10\n\nHere, we implement cNINR, working with a CNN based on VGG style blocks described in Simonyan & Zisserman (2014). The CIFAR-10 dataset consists of color images of objects divided into 10 categories, with 32 × 32 pixels in 3 color channels, each pixel intensity in the range [0, 1], partitioned into 50 000 training and 10 000 test samples, which are then preprocessed similarly to the FMNIST dataset.\n\nThe network used to test NINR performance is constructed by connecting the following blocks10:\n\n• Conv2D(32,3,3) → ReLU → Batch Norm → Conv2D(32,3,3) → ReLU →\n\nBatch Norm → MaxPool(2,2) → Dropout(pdrop = 0.2).\n\n• Conv2D(64,3,3) → ReLU → Batch Norm → Conv2D(64,3,3) → ReLU →\n\nBatch Norm → MaxPool(2,2) → Dropout(pdrop = 0.3).\n\n• Conv2D(128,3,3) → ReLU → Batch Norm → Conv2D(128,3,3) → ReLU →\n\nBatch Norm → MaxPool(2,2) → Dropout(pdrop = 0.4).\n\n• Dense ReLU Layer(500) → Linear Layer(10).\n\nOptimization is done using SGD without momentum with the learning rate fixed to η = 0.05 and mini-batch size B = 128. Each training run is performed for 500 SGD training epochs in total, or until 98 % training accuracy has been achieved.\n\nWe provide preliminary results for robustness against input-data corruption in Fig. 9. In contrast to the previous sections, the CNN used to train on CIFAR-10 contains Dropout and L2 as part of its architecture, making comparison between NINR and the two redundant. Therefore, we show results for the same network with and without NINR, as well as CDT with different input corruption scales. The success of NINR is retained for in-NINR in the decay phase, while the catapult phase requires longer than 500 epochs to obtain similar generalization properties.\n\nFigure 9: Robustness against random input perturbations for the network described in App. E using NINR training on CIFAR10. Here, we use the in-NINR CNN implementation, taking σε = 17.5 in the decay phase and σε = 55.4 in the catapult phase.\n\n10Each convolutional layer admits L2 weight decay regularization (λWD = 10−4).\n\n18\n\nBaselineIn-NNR-DecayIn-NNR-CatapultCDT-0.2CDT-0.4CDT-0.60.00.20.40.60.81.020406080100σnoiseTestAccuracyCIFAR10Published as a conference paper at ICLR 2023\n\nF RESULTS FOR MNIST-C\n\nHere, we show some additional results for the same architectures and NINR parameters used in Sec. 3, trained on the MNIST data-set and tested on several classes of images from MNISTC. The MNIST-C dataset (Mu & Gilmer, 2019) consists of 15 types of corruption applied to the MNIST test set, for benchmarking out-of-distribution robustness in computer vision. For testing purposes, the data is preprocessed similarly to the FMNIST dataset.\n\nTable 4: Domain shift performance on the MNIST-C test data, for networks trained on the MNIST dataset. Comparison is made between L2, Dropout, in-NINR, full-NINR and CDT. For CDT, two values (σnoise = 0.2, 0.4) for the amount of corruption are considered. The fully-connected and CNN NINR noise parameters are those used in Fig. 2. The NINR implementations improve performance for data transformations which are most closely related to gaussian noise injection, as can be expected.\n\nFog transformation\n\nNone L2 Dropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nCDT (0.2)\n\nCDT (0.4)\n\n53.2 52.3 FC(%) CNN(%) 71.8 67.3\n\n61.9 -\n\n59.4 61.8\n\n63.8 62.5\n\n57.2 62.9\n\n62.8 57.8\n\n52.9 57.3 57.8 62.7\n\nBrightness transformation\n\nNone L2 Dropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nCDT (0.2)\n\nCDT (0.4)\n\n97.7 97.6 FC(%) CNN(%) 98.9 98.7\n\n98.4 -\n\n98.0 98.7\n\n97.4 98.8\n\n97.9 98.8\n\n98.1 98.6\n\n97.7 97.2 98.1 97.5\n\nGlass Blur transformation\n\nNone L2 Dropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nCDT (0.2)\n\nCDT (0.4)\n\n93.7 93.2 FC(%) CNN(%) 65.1 54.6\n\n96.0 -\n\n94.7 60.2\n\n93.4 95.2\n\n94.4 61.3\n\n94.3 95.6\n\n94.6 94.3 80.6 90.2\n\nImpulse Noise transformation\n\nNone L2 Dropout\n\nin-NINR (Decay)\n\nin-NINR (Catapult)\n\nfull-NINR (Decay)\n\nfull-NINR (Catapult)\n\nCDT (0.2)\n\nCDT (0.4)\n\n84.3 84.3 FC(%) CNN(%) 51.1 28.8\n\n94.6 -\n\n93.6 62.8\n\n94.1 97.8\n\n89.7 57.1\n\n96.1 96.5\n\n86.7 89.7 75.3 86.8\n\nThe results shown in Table 4 indicate improved performance when the type of image corruption applied to the MNIST images most closely resembles the injected noise. It can therefore be intuitively understood why the most dramatic performance enhancement is found for the Impulse Noise corruption transformation, while other corruption transformation may not benefit much from NINR. We stress that NINR can be readily modified to deal with different types of corruption by changing the\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nnoise injection distribution, as well as incorporated with other regularization methods to compound their robustness enhancing effects.\n\nG CONSTANT NOISE INJECTION\n\nHere, we reproduce the results shown in Fig. 2, including an additional curve representing a constant input noise injection. We implement this experiment by applying dNINR at the input layer (connecting to the first hidden layer) using the large NIN variance value used for the the ”catapult” phase, but keeping the NIWs static, fixed to their initialization values.\n\nFigure 10: Robustness against random input perturbations with the same parameters used in Fig. 2. The additional orange curve represents In-NINR with σε = 231.6 but with fixed NIW values, which is essentially constant noise injection to the pre-activation at the first hidden layer.\n\n20\n\nBaselineIn-NNR-DecayIn-NNR-CatapultFull-NNR-DecayFull-NNR-CatapultL2DropoutCDT-0.4ConstantInputNoise0.00.20.40.60.81.020406080100σnoiseTestAccuracyFullyConnectedDNN",
    "reference": "# Summary Of The Paper\n\nThis paper introduces a regularization method for neural networks, namely Noise Injection node Regularization (NINR). The high-level idea is to inject random noise into the network’s training at a certain layer via a learnable weight. The authors provide analyses both theoretically and empirically to show how NINR could improve the robustness.\n\n# Strength And Weaknesses\n\nNeural network’s robustness is one of the most important topics in deep learning, so searching for a new regularization method that makes neural networks more robust to various kinds of scenarios such as distribution shift, adversarial attacks is definitely worthwhile. The paper presents an intuitively simple yet interesting idea motivated by mathematical and empirical insights. The experimental results are also extensive. \n\nRegarding the weaknesses, I have some comments:\n\n1. The novelty of this work compared to Anonymous, (2022), which I do not have the full context into.\n2. It is unclear from the experimental results whether this regularization approach is more useful than other well adapted ones. For example, in Table 1, Dropout is much better than NINR-based regularization for FC. In Table 2, L2 outperforms the rest for both FC and CNN.\n3. I may be mistaken, but it is unclear which layer of the neural network one should apply the NIN regularization. If applied on multiple layers, would the same analysis in Section 2.1 follow and how?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nAdditionally, I have some questions and suggestions:\n\n* What do “uncorrelated input perturbation” and “certain window of convergence” in Page 2 mean?\n* Equations (1) and (2) are the key to the rest of the analysis, but as a reader I am not sure I follow them easily. I would find a detailed derivation here or Appendix very helpful. Also, using W_{NI} weight as a vector (versus W^(l) as a matrix) is somewhat confusing.\n* In Equation (6), \\sigma^2_delta can be very small. How does it factor into R_2 when you say “dynamics is controlled by H_0”?\n* How would the authors make sense of the performance gap between NINR applied to FC and CNN? E.g., a big jump in performance for full-NINR Catapult.\n\n# Summary Of The Review\n\nI think this work has some merits, but I also have some concerns as given above.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nMULTI-LEVEL PROTEIN STRUCTURE PRE-TRAINING WITH PROMPT LEARNING\n\nZeyuan Wang1,2,7∗ Qiang Zhang1,2∗† Haoran Yu2,3 Zhichen Gong2,6 Huajun Chen1,2, 7, 8† 1College of Computer Science and Technology, Zhejiang University 2ZJU-Hangzhou Global Scientific and Technological Innovation Center 3College of Chemical and Biological Engineering, Zhejiang University 4Vecx Biomedicines Inc., 5MindRank AI Ltd., 6University College London 7AZFT Joint Lab for Knowledge Engine, 8East China Sea Laboratory\n\nShuangwei Hu4 Xurui Jin5\n\nyuanzew,qiang.zhang.cs,yuhaoran,huajunsir\n\n@zju.edu.cn\n\n{ shuangwei@vecx.bio, xurui@mindrank.ai, ucabzgo@ucl.ac.uk\n\n}\n\nABSTRACT\n\nA protein can focus on different structure levels to implement its functions. Each structure has its own merit and driving forces in describing specific characteristics, and they cannot replace each other. Most existing function prediction methods take either the primary or the tertiary structure as input, unintentionally ignoring the other levels of protein structures. Considering protein sequences can determine multi-level structures, in this paper, we aim to realize the comprehensive potential of protein sequences for function prediction. Specifically, we propose a new prompt-guided multi-task pre-training and fine-tuning framework. Through the prompt-guided multi-task pre-training, we learn multiple prompt signals to steer the model, called PromptProtein, to focus on different levels of structures. We also design a prompt fine-tuning module to provide downstream tasks the on-demand flexibility of utilizing respective levels of structural information. Extensive experiments on function prediction and protein engineering show that PromptProtein outperforms state-of-the-art methods by large margins. To the best of our knowledge, this is the first prompt-based pre-trained protein model.\n\n1\n\nINTRODUCTION\n\nPre-trained language models (PTLMs) have prevailed in natural language processing (NLP). Recently, some methods (Alley et al., 2019; Elnaggar et al., 2021; Rives et al., 2021) use PTLMs to encode protein sequences to predict biological functions, which are called pre-trained protein models (PTPMs). In contrast to natural languages, there are four distinct levels of protein structures (Kessel & Ben-Tal, 2018). The primal is the protein sequence consisting of amino acids, the second refers to the local folded structures (e.g., α helix and β pleated sheet), the tertiary describes the natural folded three-dimensional structure, and the quaternary is a protein multimer comprising multiple polypeptides. A protein can focus on different structure levels to implement its specific functions, including reserving a piece of the sequence, manifesting the whole 3D structure as conformational elements, or even cooperating with other proteins. Therefore, when predicting protein functions, it is vital to flexibly utilize multi-level structural information.\n\nAlphaFold2 (Jumper et al., 2021) makes great progress in the tertiary structure prediction based on protein sequences. However, directly learning from predicted structures can be unachievable as the prediction of proteins without homologous sequences is inaccurate. More importantly, the quaternary structure of protein multimers which faithfully depicts protein functions is usually different from the tertiary (see Figure 1) and reliable predictive models have not been released. Fortunately, protein sequences are easy to obtain and can determine all the other levels of structures. This paper aims to realize the full potential of protein sequences in function prediction by prompting a\n\n∗Equal contribution and shared co-first authorship. †Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nPTPM to exploit all levels of protein structures during pre-training. The main challenges are twofold: 1) how to design proper pre-training tasks for different protein structures? and 2) how to efficiently integrate these tasks in the pre-training phase and transfer the implicit protein structure knowledge for function prediction in fine-tuning phase.\n\nFor the first challenge, we design three complementary pre-training tasks across multiple structure levels, targeting both fine and coarse resolutions. Specifically, we use the de facto Mask Language Modeling (MLM) task to exploit the primary structure information, where the model needs to predict randomly masked amino acids in a protein. For the secondary and tertiary structure, we propose the alpha-carbon CooRDinate prediction (CRD) task, where the model should output the relative positions between residues. For the quaternary structure, we propose the Protein-Protein Interaction prediction (PPI) task, where the model is required to estimate the interaction probability. We collect millions of data covering different levels of protein structures from UniRef50 (Consortium, 2021), Protein Data Bank (Berman et al., 2000), and STRING (Szklarczyk et al., 2019).\n\nFigure 1: A comparison of protein CDK1 in the tertiary (left) and quaternary (right) structures.\n\nFor the second challenge, a straightforward strategy is to leverage multi-task learning to combine the losses of different pre-training tasks. However, many works (Wu et al., 2019; Yu et al., 2020) find that task interference is common when tasks are diverse. This problem can be more severe in multi-task pre-training due to the gap between pre-training and downstream tasks, causing negative knowledge transfer. For example, BERT (Kenton & Toutanova, 2019) leverages MLM and Next Sentence Prediction (NSP) to learn the sequential dependency and sentence relationship simultaneously, while RoBERTa (Liu et al., 2019) finds the performance will be slightly improved when removing the NSP loss. We postulate this problem also exists in multi-level protein structures, as different structures can be inconsonant. The MLM task emphasizes the neighboring relations along the sequence, while the CRD task shall focus more on long-range amino acid pairs which can be spatially close in the tertiary structure.\n\nTo address this challenge, inspired by recent prompt learning, we propose a prompt-guided multitask pre-training and fine-tuning framework, and the resulting protein model is called PromptProtein. The prompt-guided multi-task pre-training associates multiple pre-training tasks with dedicated sentinel tokens, called prompts. To utilize the prompt tokens, we introduce a prompt-aware attention module, which modifies two components of the Transformer architecture: 1) Attention mask, which is designed to block attention calculation from input data to a prompt as a prompt should be taskdependent instead of sample-dependent. 2) For skip connection, a prompt is used to calculate a skip weight, which can filter out task-irrelevant information. At the fine-tuning phase, we propose a prompt fine-tuning module to coordinate all prompt tokens, such that the model is capable of leveraging multi-level protein structure information flexibly, enabling the positive transfer of learned structural knowledge to downstream tasks.\n\nWe conduct experiments on function prediction and protein engineering as downstream tasks, where PromptProtein significantly outperforms state-of-the-art on all datasets, especially on low-resource protein engineering tasks where PromptProtein achieves an average improvement of 17.0%.\n\n2 RELATED WORKS\n\nProtein Representation Models. Proteins have complex structures that determine their biological functions (Epstein et al., 1963). A growing body of work focuses on how to leverage structural information. Since evolution through natural selection has spoken protein sequences as their “natural language”, various natural language processing methods have been extended to proteins. Asgari & Mofrad (2015); Yang et al. (2018) apply word embedding algorithms (Mikolov et al., 2013) to obtain protein representations. Dalkiran et al. (2018); ̈Ozt ̈urk et al. (2018) use one-dimensional con-\n\n2\n\nAreas with significant changeBinding siteResidue IndexResidue IndexPublished as a conference paper at ICLR 2023\n\nFigure 2: The architecture overview of PromptProtein. In the pre-training stage, we pre-train our model with three structure-related tasks, including mask language modeling, alpha-carbon prediction, and protein-protein interaction prediction. For each task, the model takes the protein sequence and the task-specific token as input and learns to produce a representation encoding the corresponding structure information. In the fine-tuning stage, a prompt-tuning module τθ( ) can flexibly com- ·\nbine structure information via the learned prompt tokens for diverse downstream tasks.\n\nvolutional neural networks to predict the functions. Furthermore, Alley et al. (2019); Elnaggar et al. (2021); Rives et al. (2021) explore whether the pre-training and fine-tuning paradigm, the transformer architectures, and the objective functions can effectively transfer from natural languages to proteins. Zhang et al. (2021a) align the amino acid sequence and the text sequence to obtain informative protein representation. To utilize the tertiary structure, Hermosilla et al. (2020); Somnath et al. (2021); Ganea et al. (2021); Zhang et al. (2022) build protein graphs and employ message-passing neural networks to produce structure-aware representations. Bepler & Berger (2021) employ contact map prediction and structural similarity prediction to pre-train the protein model. Although primary and tertiary structures have been studied, few works try to enrich protein representation with the quaternary structure which faithfully depicts protein functions. In this paper, we show that systematic modeling and flexible utilization of multi-level structures are the keys to improving the performance of function prediction and protein engineering.\n\nMulti-task Learning. The goal of multi-task learning is to take advantage of inductive transfer across tasks and achieve better generalization performance. When tasks are diverse, using a naive shared MTL model can suffer from task interference. Prior methods have been proposed to deconflict gradients from different tasks. Chen et al. (2018) dynamically adjust gradient magnitudes so different tasks can be trained at similar scales. Yu et al. (2020) take the gradient direction into account and drop the projection of one task gradient direction onto another if they are conflicting. Rather than clipping the conflict gradient direction, Javaloy & Valera (2021) learn a rotation matrix for each task to bring different optima closer to each other. However, these methods are not designed for multi-task pre-training and cannot properly deal with the knowledge transferability to downstream tasks. We provide a schematic comparison of these methods in Appendix A.1.\n\nPrompts for Pre-trained Models. In-context learning (Brown et al., 2020) is introduced to steer the pre-trained model to produce task-desired representations. In the NLP area, the prevailing approaches to designing prompts can be divided into two categories: discrete prompt designing and continuous prompt tuning. The discrete prompt technique (Schick & Sch ̈utze, 2021) adds task description tokens from a vocabulary to the context to obtain enriched sentence embeddings. However, the hand-crafted prompts may provide disturbance of human bias and are limited to discrete vocabulary spaces. In contrast, Li & Liang (2021); Zhang et al. (2021b) generate optimal prompt vectors in continuous spaces. Inspired by these works, we extend the concept of prompt tuning to the pre-training stage, associate multi-level protein structural information with dedicated prompt tokens during pre-training, and adaptively combine these learned prompts for downstream tasks.\n\n3 METHODOLOGY\n\nTo acquire multiple information from the input data x, conventional multi-task learning usually produces a universal representation h. The whole objective can be formulated as a weighted sum of individual task objectives: are the hyper-parameters to balance these losses. However, multi-level protein structures can be inconsonant: the primary structure focuses\n\ni αiLi(h), where\n\n= (cid:80)\n\nαi}\n\nL\n\n{\n\n3\n\n[MLM][CRD][PPI]Prompt-AwareTransfomerMASLSCV[Mask]DKMVVT.........Mask Language ModelingAlpha-CarbonPredictionInteractionPredictionMASLSCV[Mask]DKMVVTARF5trpA?ComposedPromptMASLSCVSDKMVVT...DiverseTasksFunction AnnotationStability PredictionFitness Prediction(cid:31)(cid:30)(cid:29)(cid:30)(cid:28)(cid:27)(cid:26)(cid:27)(cid:29)(cid:25)(cid:24)(cid:30)(cid:29)(cid:23)(cid:22)(cid:21)(cid:31)(cid:29)(cid:20)(cid:28)(cid:19)(cid:26)(cid:18)(cid:31)(cid:20)(cid:20)(cid:17)[[Prompt-AwareTransfomerphpxp(cid:31)hp(cid:31)xPublished as a conference paper at ICLR 2023\n\nmore on the dependency along the sequence, whereas the tertiary and quaternary structure weights more on the spatial organization, which can cause the problem of task interference. This problem can lead to more severe negative transfer in multi-task pre-training due to the gap between pre-training and downstream tasks. To solve this problem, we propose a prompt-guided multi-task pre-training and fine-tuning framework that utilizes a prompt token p to produce a task-specific representation hp. Multiple learned tokens can be flexibly combined to steer the pre-trained model for various downstream tasks, bridging the gap between pre-training and downstream tasks.\n\nThis section first describes how to use prompts to modify the Transformer architecture, such that different tasks can be processed by different neural layers and reduce task interference. Then we present the three pre-training tasks to acquire multi-level protein structural information: (1) masked language modeling, (2) alpha-carbon coordinate prediction, and (3) protein-protein interaction prediction. Finally, we introduce the prompt-guided pre-training and fine-tuning framework where multiple information can be acquired in the pre-training stage and combined on-demand for downstream tasks. The resulting PromptProtein model is illustrated in Figure 2.\n\n3.1 PROMPT-AWARE ATTENTION MODULE\n\nTo reduce interference between pre-training tasks, we use the prompt token to modify the Transformer architecture so that multiple information can be effectively acquired by the pre-trained model. Specifically, we modify two parts of the Transformer: attention mask and skip connection, and the resulting architecture is called Prompt-aware Transformer. Given an input protein sequence x and is concatenation. Let a prompt token p, we define the whole input xp denote xp = x xi\n\np be the i-th token of the whole input and h(l)\n\np be the representation of xp at the l-th layer.\n\np , where\n\n||\n\n||\n\nconventional\n\nAttn(h(l)\n\nformulated as:\n\nAttention mask. The selfattention is p ) = Softmax((QK T )/√d)V, where Q, K, and V are the linear projection of h(l) p . Each token in the whole sequence can attend to others at any position which means the condition prompt will be affected by the input sequence. A more reasonable way is to keep only the effect of the prompt on the input sequence and eliminate the reverse effect, as a prompt should be task-dependent instead of sample-dependent. As illustrated in Figure 3, we design an attention mask matrix M to fulfill this requirement. Let Mij denote the (i, j)-element of the mask matrix, and we define:\n\nMij =\n\n(cid:40)\n\n0, xi\n\np ∈ 1, others.\n\np and xj\n\np ∈\n\nx\n\n(1)\n\nSkip connection. Skip connection enables deep neural networks easier to train (He et al., 2016). To encourage different tasks to be processed by different layers and reduce task interference, we design a weighted skip connection. That is, the prompt token is used to calculate a weight for the output of the attention module. The whole process can be: = h(l)\n\nh(l+1)\n\np + (1\n\np\n\n−\n\nFigure 3: Prompt-aware Attention Module. A pink circle represents an amino acid token and a purple circle represents a prompt token. We decouple prompt tokens from amino acid tokens by the attention mask. The embedding of decoupled prompt token determines the weight of the residual connection. In the fine-tuning stage, we use a prompt-tuning module τθ( ) to learn the downstream task- ·\ndesired composed prompt. p )Attn(h(l) g(l) p ),\n\n(2)\n\np , a scalar, is linear projection of l-th layer embedding of prompt p. After L layers of the\n\nwhere g(l) prompt-aware attention module, we have the task-specific representation hp = h(L) p .\n\n3.2 PROTEIN MULTI-LEVEL STRUCTURES LEARNING\n\nTo acquire multi-level protein structure information, we consider three complementary pre-training tasks: (1) masked language modeling, which has been commonly used by existing PTPMs and can\n\n4\n\n(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:27)(cid:24)(cid:25)(cid:23)(cid:22)(cid:21)(cid:20)(cid:27)(cid:22)(cid:28)(cid:21)(cid:19)(cid:18)(cid:24)(cid:17)(cid:27)(cid:16)(cid:25)(cid:27)(cid:24)(cid:25)(cid:23)(cid:22)xAttention Mask(cid:31)(cid:30)(cid:29)Skip Connection(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)x1x2x3x1x2x3x1x2x3pppgpxPublished as a conference paper at ICLR 2023\n\ncapture the primary structure information; (2) coordinate prediction, which acquires the secondary and tertiary structure; and (3) interaction prediction, which acquires the quaternary structure.\n\nMasked language modeling. This task uses all available amino acid tokens to recover the masked be the vocabulary of amino acid tokens. The ones. Let Y be the set of masked out tokens, and MLM loss is formulated:\n\nV\n\nq(y\n\nhp) = |\n\n(cid:80)\n\nexp(p(y v∈V exp(p(v\n\nhp)) |\n\nhp)) |\n\n,\n\nL\n\nMLM(hp) =\n\n(cid:88)\n\ny∈Y\n\n−\n\nlog q(y\n\nhp).\n\n|\n\n(3)\n\nAlpha-Carbon Coordinate Prediction. Since a secondary structure can be inferred from the protein 3D coordinates (Kabsch & Sander, 1983), we use an α-C coordinate prediction task to learn both secondary and tertiary structures. Given the sequence length , we denote the ground-truth |\nR|x|×3 and the structure predictor, a 2-layer MLP naturally folded 3D structure of protein as Z R|x|×3. By translating and rotating (Kabsch, network, as κ, then the predicted structure is κ(hp) 1976) the predicted structure, we can get the minimal root mean square deviation between groundtruth and predicted structure, and the loss is calculated based on this deviation. In this way, there is no need to consider spatial invariance or equivariance, but only need to focus on the relative positions between residues. The CRD loss can be calculated as the mean square error (MSE):\n\nx |\n\n∈\n\n∈\n\nCRD(hp) = MSE(Z, Kabsh(κ(hp))).\n\nL\n\n(4)\n\nProtein-Protein Interaction prediction. To acquire the quaternary structure information, we conduct the third pre-training task: predicting whether the m-th and n-th proteins can interact with each other within batched data. Let hm p be the m-th protein in a mini-batch and ym,n is the ground-truth. We first calculate pair-aware protein representation hm,n\n\n, then formulate the PPI loss:\n\nAttnm,n = Sigmoid(\n\np (hm\n\np W )(hn\n\npW )T\n\n),\n\n√d\n\nhm,n\n\np = mean(AttnT\n\nPPI(hp) =\n\nL\n\n(cid:88)\n\nm,n∈N\n\nm,nhm p ) BCE(ym,n, p(ym,n)\n\nmean(Attnm,nhn hm,n |\n\n||\n\np\n\n),\n\np)),\n\n(5)\n\nwhere W the batch size. More details of the pre-training tasks are provided in Appendix A.2.\n\nRdW ×dW is a projection matrix, BCE is the binary cross-entropy loss function, N is\n\n∈\n\n3.3 PROMPT-GUIDED MULTI-TASK PRE-TRAINING AND FINE-TUNING\n\nCorresponding to the three pre-training tasks, the prompt can be instantiated as one of the three tokens, i.e., p . The task-specific representation is thus denoted as h[MLM], h[CRD], h[PPI]. The objective function of the prompt-guided multi-task pre-training can be formulated as:\n\n[MLM], [CRD], [PPI]\n\nP =\n\n∈\n\n{\n\n}\n\nL\n\n= α1L\n\n(6) When we pre-train a model with multiple tasks as Equation 6, both model parameters ψ and prompts p are optimized. In this way, the model does not necessarily need to learn the optimal representation for all tasks, but only needs to learn the respective optimal representation for each task. Hence, the problem of task interference can be alleviated.\n\nCRD(h[CRD]) + α3L\n\nMLM(h[MLM]) + α2L\n\nPPI(h[PPI]).\n\nFurthermore, to bridge the gap between pre-training and downstream tasks, since the model can acquire each type of information conditioned on the learned prompt tokens, we can combine these tokens with prompt-tuning to flexibly mix the acquired information on-demand. We denote a prompttuning module as τθ( ), and the downstream task-desired protein representation hp′ can be obtained ·\nby feeding the tuned prompt p′\n\np′ = τθ(p[MLM], p[CRD], p[PPI]). (7) Then the pre-trained model can produce hp′ and conduct predictions for the downstream task of interest. Equation 7 shows how to flexibly utilize the pre-training task information at the fine-tuning stage. Note that, in the pre-training stage, we only append one prompt to acquire one type of taskspecific information, while in the fine-tuning stage, we feed all the learned prompt tokens to τθ( )\n· and flexibly combine the acquired information. Here, we leverage a linear layer as our prompt-tuning module to combine three learned prompts. For sake of understanding, we provide the pseudo-code of the prompt-guided multi-task pre-training and fine-tuning framework in Appendix A.3.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Model performance on EC numbers and GO terms prediction tasks. from Wang et al. (2022),\n\n: the results taken from Zhang et al. (2022).\n\n‡\n\n: the results taken\n\n†\n\nDATASET\n\nCNN RESNET LSTM TRANSFORMER\n\nGAT† GVP† DEEPFRI GearNet − Edge‡\n\nESM − 1b‡ ProtBERT − BFD† LM − GVP† MT-LSTM\n\nMTL GRADNORM ROTOGRAD PROMPTPROTEIN (OURS)\n\nEC AUPRpair\n\nGO-BP\n\nGO-MF\n\nGO-CC\n\nFmax AUPRpair\n\nFmax AUPRpair\n\nFmax AUPRpair\n\nFmax\n\n0.540 0.137 0.032 0.187\n\n0.320 0.482 0.547 0.892\n\n0.889 0.859 0.710 0.851\n\n0.892 0.893 0.895 0.915\n\n0.545 0.187 0.082 0.219\n\n0.368 0.489 0.631 0.874\n\n0.864 0.838 0.664 0.817\n\n0.869 0.874 0.876 0.888\n\n0.165 0.166 0.130 0.135\n\n0.171 0.224 0.282 0.292\n\n0.343 0.188 0.302 0.324\n\n0.325 0.331 0.334 0.363\n\n0.244 0.280 0.248 0.257\n\n0.284 0.326 0.399 0.490\n\n0.470 0.279 0.417 0.442\n\n0.445 0.466 0.470 0.495\n\n0.380 0.281 0.100 0.172\n\n0.329 0.458 0.462 0.596\n\n0.639 0.464 0.580 0.608\n\n0.651 0.647 0.648 0.665\n\n0.354 0.267 0.166 0.240\n\n0.317 0.426 0.465 0.650\n\n0.657 0.456 0.545 0.591\n\n0.640 0.643 0.638 0.677\n\n0.261 0.266 0.150 0.170\n\n0.249 0.278 0.363 0.336\n\n0.384 0.234 0.423 0.381\n\n0.415 0.415 0.416 0.457\n\n0.387 0.403 0.320 0.380\n\n0.385 0.420 0.460 0.486\n\n0.488 0.408 0.527 0.492\n\n0.503 0.504 0.509 0.551\n\n4 EXPERIMENTS\n\n4.1 PRE-TRAINING SETUP\n\nFor the primary structural information, we use UniRef50 (Suzek et al., 2015) which is a clustering of UniRef90 seed sequences at 50% sequence identity. For the secondary and tertiary structural information, we use Protein Data Bank (PDB) (Berman et al., 2000), which includes 200,000 protein 3D structures obtained by experimental methods. For the quaternary structure information, we use the STRING dataset (Szklarczyk et al., 2019) that contains amino acid sequences and proteinprotein interaction pairs. In the STRING dataset, protein interactions are divided into 7 categories. We selected the physical-only interaction subset from STRING which contains 65 million protein sequences from 14,095 species and 2.7 billion protein-protein interaction pairs.\n\nWe implement PromptProtein using Pytorch (Paszke et al., 2019) and Fairseq (Ott et al., 2019). PromptProtein has 650M parameters with 33 layers and 20 attention heads. The embedding size is 10−4 with no weight decay. We use an inverse square root learning 1280. The learning rate is 1 A100 40G GPUs for 270k steps of updates. After rate schedule. All models are trained on 2 pre-training, the average error of the coordinate prediction task on a single residue is 5 ̊A, and the accuracy of physical binding prediction is greater than 90.0%. Unless otherwise specified, we use this model in all downstream experiments. The source code will be available online. Please refer to Appendix B for the details of all the pre-training and downstream task dataset statistics.\n\n×\n\n×\n\n4.2 DOWNSTREAM TASKS: FUNCTION ANNOTATION\n\nDatasets and Metrics. Gene ontology (GO) terms and enzyme commission (EC) numbers are two standard classification schemes that organize myriad protein functions. These function prediction tasks can be regarded as multiple binary classification tasks. We follow the dataset split method in (Gligorijevi ́c et al., 2021). The evaluation metrics are protein-centric maximum F-score (Fmax) and term-centric area under precision-recall (AUPR) curve, which are used in the CAFA challenges (Radivojac et al., 2013).\n\nBaselines. There are four categories of baselines. (1) Sequence-based encoders. CNN (Shanehsazzadeh et al., 2020), ResNet, LSTM, and Transformer (Rao et al., 2019) only take amino acid sequence as input; (2) Geometric learning method. GAT (Veliˇckovi ́c et al., 2018), GVP (Jing et al., 2020), DeepFRI (Gligorijevi ́c et al., 2021), and GearNet-Edge (pre-trained by Multiview Contrast) (Zhang et al., 2022) take protein 3D coordinates as additional input to obtain informative representation; (3) Pre-trained protein models. ESM-1b (Rives et al., 2021), ProtBERT-BFD (Elnaggar et al., 2021), and LM-GVP (Wang et al., 2022) learn the pattern from large protein corpus. MT-LSTM (Bepler & Berger, 2021) uses contact map and structure similarity to enrich the embed-\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Model performance on protein engineering tasks. Results with two decimal places are token from Dallago et al. (2021).\n\nDATASET\n\nSTABILITY\n\nFLUORE.\n\nCNN RESNET LSTM ESM-UNTRAINED\n\nESM-1B ESM-1V PROTBERT-BFD LSTM-MT PROMPTPROTEIN (OURS)\n\n0.51 0.73 0.69 0.452\n\n0.71 0.726 0.732 0.741 0.767\n\n0.67 0.21 0.67 0.337\n\n0.68 0.507 0.675 0.648 0.683\n\nTHERMO MIXED\n\nAAV 1-VS-R\n\n1-VS-R\n\nGB1 2-VS-R\n\n3-VS-R\n\n0.34 0.353 0.317 0.36\n\n0.68 0.67 0.651 0.665 0.694\n\n0.48 0.173 0.215 0.01\n\n0.04 0.18 0.234 0.258 0.551\n\n0.17 0.117 0.124 0.05\n\n0.32 0.32 0.303 0.335 0.403\n\n0.32 0.210 0.349 0.05\n\n0.36 0.32 0.387 0.402 0.550\n\n0.83 0.291 0.491 0.46\n\n0.54 0.77 0.654 0.741 0.783\n\ndings. (4) Multi-task learning framework. We employ naive multi-task learning (MTL) and two optimization methods (GradNorm (Chen et al., 2018), RotoGram (Javaloy & Valera, 2021)).\n\nResults. We present the evaluation results of proposed PromptProtein and state-of-the-art baselines in Table 1. Compared with all baselines, PromptProtein achieves new state-of-the-art performance on all tasks, which indicates that systematic modeling of multi-level structure information is beneficial. Although the multi-task learning baselines integrate the same information as PromptProtein, they cannot learn multiple information well and transfer properly to downstream tasks. Their inferior performance in GO-BP and GO-CC suggests that there is a gap between downstream task-desired representations and universal pre-trained representations. Flexible composing of structural information significantly improves the performance of the model for downstream tasks.\n\n4.3 DOWNSTREAM TASKS: PROTEIN ENGINEERING TASKS\n\nDatasets and Metrics. Protein engineering is regarded as a sequence regression task that, given a protein, models are required to identify the functional strength, often termed the fitness landscape. Here, we employ five datasets (stability, fluorescence, thermostability, AAV, and GB1) coming from TAPE (Rao et al., 2019) and FLIP (Dallago et al., 2021) to evaluate whether the model can produce accurate quantitative predictions of these functions. We report the commonly-used Spearman’s ρ (rank correlation coefficient) to measure the degree to which the landscape was learned. Results of other tasks on FLIP can be found in Appendix 5.\n\nBaselines. For proteins without 3D structures, geometric methods cannot directly apply to these tasks. We choose sequence-based methods (CNN, LSTM, Transformer) and pre-trained protein methods (ESM-1b, ESM-1v (Meier et al., 2021), ProteinBert-BFD, LSTM-MT) as baselines for protein engineering tasks. As Dallago et al. (2021) purport that the various pooling choices perform inconsistently across datasets and splits, for a fair comparison, we utilize the mean pooling method to obtain protein representation.\n\nGB1\n\nAAV\n\nMETHOD\n\nCONVENTIONAL MTL. PROMPTPROTEIN\n\nTable 3: Ablation of PromptProtein with different components.\n\nResults. From Table 2, we observe that PromptProtein obtains better It performance than all baselines. confirms that pre-training on structural objectives contributes to protein engineering tasks and systematic modeling of protein multi-level structure leads to further improvements. Note that LSTM-MT, which leverages the tertiary structural information to enhance protein representations, cannot surpass ESM-1b on all datasets, while our proposed approach obtains superior performances. This observation demonstrates that not all structural information leads to positive transfer and flexible utilization of structural information is the key to improved performance. Moreover, PromptProtein can obtain 17.0% improvement on average in low-resource settings of the AAV and GB1 datasets, compared\n\n- ATTENTION MASK - LAYER SKIP - MLM OBJECTIVE - CRD OBJECTIVE - PPI OBJECTIVE\n\n0.531 0.520 0.493 0.535 0.532\n\n0.264 0.270 0.240 0.262 0.253\n\n0.663 0.659 0.629 0.647 0.654\n\n0.651 0.672\n\n0.525 0.544\n\n0.238 0.279\n\nTHERMO\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Skip connection visualization and prompt correlation. (a) We visualize the learned skip weight at all neural layers. The darkness of a block represents the weight of that block utilized for the given prompt. (b) We provide the Pearson’s correlation between skip weights. The skip patterns between the [MLM] prompt and the other two prompts are negatively correlated, whereas the pattern between the tertiary and quaternary structures is positively correlated.\n\nto the well-performed PTPM baselines. These results indicate that the prompt-guiding PTPM is a better few-shot learner.\n\n4.4 ABLATION STUDY\n\nThe ablation study is conducted to validate the effectiveness of designed modules in PromptProtein, i.e., prompts, attention mask, or skip connection. As illustrated in Table 3, the performance will decay if any one of the modules is absent, demonstrating that all the modules are advantageous. Furthermore, we notice that skip connection contributes most to the performance, confirming the necessity of reducing task interference.\n\n4.5 ANALYSIS AND DISCUSSION\n\nHow do prompts determine the processing pathways of structural information?\n\nIn Figure 4(a), we visualize the skip weights of three pre-trained prompts at different neural layers, and compute the Pearson’s correlation (Benesty et al., 2009) of these skip weights to measure the mutual correlations between the pre-training tasks (Figure 4(b)). We have the following key observations. (a) The skip weights are similar in the bottom layers (1-13) across all prompts, indicating all three tasks are processed by these layers. The MLM task information is mainly acquired by the middle layers (14-29), whereas the CRD and PPI information is more acquired by the top layers (30-33). (b) We clearly observe that the [CRD] and [PPI] prompts are more correlated. This is consistent with the intuition that the tertiary and quaternary levels are 3D structures whose amino acids attend to spatially adjacent neighbors, resulting in similar skip weight patterns. Further analysis of the model layer can be found in Appendix B.3.\n\nCan PromptProtein learn multi-level structures?\n\nTo examine whether prompt-guided pre-training can learn multiple structure information, we conduct experiments to visualize the protein representations conditioned on different pre-trained prompt tokens. We use t-SNE (van der Maaten & Hinton, 2008) to reduce the dimension of embeddings. Figure 5(a) illustrates amino acid embeddings conditioned on [MLM]. We observe that amino acid embeddings in a protein are grouped according to their type. Figure 5(b) illustrates amino acid embeddings conditioned on [CRD]. We find that amino acids are linearly arranged in 2D space along their sequence in the protein. To obtain a more accurate relationship between representations and structures, we compare the protein contact map and the coordinate of embedding. The strong correlation between them demonstrates the CRD objective can effectively learn information about protein 3D structures. In Figure 5(c), we visualize the amino acid embeddings with traditional multi-task pre-training and highlight serine (a class of amino acids). The embeddings attempt to merge multiple structural features at the same time, which leads to an unclear pattern. These results show that prompt-guided pre-training mitigates task interference and allows the multiple structure information to be learned well, resulting in promising performance.\n\n8\n\n[MLM][CRD][PPI]1102030331.31.21.11.00.90.8(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:25)(cid:24)(cid:23)(cid:22)(cid:21)(cid:26)(cid:25)(cid:28)(cid:20)(cid:19)(cid:21)(cid:18)(cid:28)(cid:17)(cid:21)(cid:16)(cid:15)(cid:30)(cid:14)(cid:21)(cid:13)(cid:30)(cid:22)(cid:21)(cid:26)(cid:25)(cid:31)(cid:12)(cid:29)(cid:28)(cid:11)(cid:10)(cid:26)(cid:9)(cid:18)(cid:22)(cid:28)(cid:27)(cid:26)(cid:10)(cid:10)(cid:24)(cid:14)(cid:30)(cid:22)(cid:21)(cid:26)(cid:25)1.0-1.0[MLM][CRD][PPI][MLM][CRD][PPI]0.0(cid:8)(cid:8)(cid:8)1.01.00.80.8-0.4-0.4-0.7-0.71.0Published as a conference paper at ICLR 2023\n\nFigure 5: The comparison of amino acid embeddings with different learning methods. We visualize protein representations from prompt-guided multi-task pre-training in (a) and (b), and naive multi-task learning in (c). Each letter represents an amino acid and is colored according to the physicochemical properties in (a), and the secondary structure types in (b) and (c). The superscripts of letters represent the sequential number of amino acids from the C-terminal to the N-terminal.\n\nFigure 6: Visualization of the difference of [MLM] and [PPI] prompts. The two proteins are Transcription initiation factor TFIID subunit4 (TAF4) and Transcription initiation factor TFIID subunit 5 (TAF5). Left: Visualize the embedding of amino acids conditioned on [MLM] and [PPI] prompts (TAF4) by MDS. Middle: Visualize distances of corresponding amino acids in (a). Right: Visualize the amino acids with the most variation embeddings (red).\n\nFurthermore, since the [PPI] prompt is trained to provide quaternary structure information, we analyze what exactly the amino acid representations have changed. As shown in Figure 6(a), we firstly visualize the embeddings of amino acids of the TAF4 protein conditioned on [MLM] or [PPI] based on MDS (Kruskal, 1964). Then we calculate the distances between two embeddings of the same amino acid and plot them in Figure 6(b). We mark 30 amino acids with the most variation embeddings in red (Figure 6(c)). The observation that marked amino acids are all on the protein surface is consistent with the fact that modeling the quaternary structure cares about the surface conformation, not the core (Yan et al., 2008).\n\n5 CONCLUSION AND FUTURE WORK\n\nIn this paper, we extend the concept of prompts from NLP to protein representations. We propose the prompt-guided multi-task pre-training and fine-tuning framework. With this framework, we propose three complementary pre-training structures to acquire multi-level structure information, and flexibly combine them for various downstream tasks. Experimental results on function prediction and protein engineering show that the proposed approach can produce satisfactory improvements when compared to the conventional PTPMs. The improvement is especially significant in lowresource settings. In the future, we are interested in examining the effectiveness of the proposed prompt-guided multi-task pre-training and fine-tuning framework in domains where hierarchical task information is required in the pre-training stage.\n\n9\n\n[MLM] Guided Embedding[CRD] Guided EmbeddingWithout Prompt-Guided Embedding(a)(b)(c)Hydrophobic alphaticCharged basicCharged acidicCharged acidicUniqueHydrophobic aromaticCoilAlpha-helixDisorderBeta-sheet(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:22)(cid:30)(cid:26)(cid:21)(cid:20)(cid:23)(cid:25)(cid:30)(cid:19)(cid:20)(cid:18)(cid:24)(cid:17)(cid:16)(cid:24)(cid:26)(cid:25)(cid:24)(cid:20)(cid:15)(cid:14)(cid:23)(cid:13)(cid:12)(cid:11)TAF4TAF5(a)(b)(c)MLM-task EmbeddingPPI-task EmbeddingPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work is funded by NSFC91846204/U19B2027 and sponsored by CAAI-Huawei MindSpore Open Fund. We want to express gratitude to the anonymous reviewers for their hard work and kind comments and Hangzhou AI Computing Center for their technical support. Xurui Jin is the employee of the MindRank AI Ltd.\n\nREFERENCES\n\nEthan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church. Unified rational protein engineering with sequence-based deep representation learning. Nature methods, 16(12):1315–1322, 2019.\n\nEhsaneddin Asgari and Mohammad RK Mofrad. Continuous distributed representation of biological\n\nsequences for deep proteomics and genomics. PloS one, 10(11):e0141287, 2015.\n\nJacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. In\n\nNoise reduction in speech processing, pp. 1–4. Springer, 2009.\n\nTristan Bepler and Bonnie Berger. Learning the protein language: Evolution, structure, and function.\n\nCell Systems, 12(6):654–669, 2021.\n\nHelen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig, Ilya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic acids research, 28(1): 235–242, 2000.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nDrew H Bryant, Ali Bashir, Sam Sinai, Nina K Jain, Pierce J Ogden, Patrick F Riley, George M Church, Lucy J Colwell, and Eric D Kelsic. Deep diversification of an aav capsid protein by machine learning. Nature Biotechnology, 39(6):691–696, 2021.\n\nZhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International conference on machine learning, pp. 794–803. PMLR, 2018.\n\nUniProt Consortium. Uniprot: the universal protein knowledgebase in 2021. Nucleic acids research,\n\n49(D1):D480–D489, 2021.\n\nAlperen Dalkiran, Ahmet Sureyya Rifaioglu, Maria Jesus Martin, Rengul Cetin-Atalay, Volkan Atalay, and Tunca Do ̆gan. Ecpred: a tool for the prediction of the enzymatic functions of protein sequences based on the ec nomenclature. BMC bioinformatics, 19(1):1–13, 2018.\n\nChristian Dallago, Jody Mou, Kadina E Johnston, Bruce Wittmann, Nick Bhattacharya, Samuel Goldman, Ali Madani, and Kevin K Yang. Flip: Benchmark tasks in fitness landscape inference for proteins. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\nJames Dunbar, Konrad Krawczyk, Jinwoo Leem, Terry Baker, Angelika Fuchs, Guy Georges, Jiye Shi, and Charlotte M Deane. Sabdab: the structural antibody database. Nucleic acids research, 42(D1):D1140–D1146, 2014.\n\nAhmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n\nCharles J Epstein, Robert F Goldberger, and Christian B Anfinsen. The genetic control of tertiary protein structure: studies with model systems. In Cold Spring Harbor symposia on quantitative biology, volume 28, pp. 439–449. Cold Spring Harbor Laboratory Press, 1963.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nOctavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi S Jaakkola, and Andreas Krause. Independent se (3)-equivariant models for end-to-end rigid protein docking. In International Conference on Learning Representations, 2021.\n\nVladimir Gligorijevi ́c, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structurebased protein function prediction using graph convolutional networks. Nature communications, 12(1):1–14, 2021.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nPedro Hermosilla, Marco Sch ̈afer, Matej Lang, Gloria Fackelmann, Pere-Pau V ́azquez, Barbora Kozlikova, Michael Krone, Tobias Ritschel, and Timo Ropinski. Intrinsic-extrinsic convolution and pooling for learning on 3d protein structures. In International Conference on Learning Representations, 2020.\n\nAnna Jarzab, Nils Kurzawa, Thomas Hopf, Matthias Moerch, Jana Zecha, Niels Leijten, Yangyang Bian, Eva Musiol, Melanie Maschberger, Gabriele Stoehr, et al. Meltome atlas—thermal proteome stability across the tree of life. Nature methods, 17(5):495–503, 2020.\n\nAdri ́an Javaloy and Isabel Valera. Rotograd: Gradient homogenization in multitask learning. In\n\nInternational Conference on Learning Representations, 2021.\n\nBowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. In International Conference on Learning Representations, 2020.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\nWolfgang Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 32 (5):922–923, 1976.\n\nWolfgang Kabsch and Christian Sander. Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features. Biopolymers: Original Research on Biomolecules, 22(12):2577–2637, 1983.\n\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171– 4186, 2019.\n\nAmit Kessel and Nir Ben-Tal. Introduction to proteins: structure, function, and motion. Chapman\n\nand Hall/CRC, 2018.\n\nJoseph B Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothe-\n\nsis. Psychometrika, 29(1):1–27, 1964.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597, 2021.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nJoshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. Advances in Neural Information Processing Systems, 34:29287–29303, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-\n\ntations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019.\n\nHakime ̈Ozt ̈urk, Arzucan ̈Ozg ̈ur, and Elif Ozkirimli. Deepdta: deep drug–target binding affinity\n\nprediction. Bioinformatics, 34(17):i821–i829, 2018.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nPredrag Radivojac, Wyatt T Clark, Tal Ronnen Oron, Alexandra M Schnoes, Tobias Wittkop, Artem Sokolov, Kiley Graim, Christopher Funk, Karin Verspoor, Asa Ben-Hur, et al. A large-scale evaluation of computational protein function prediction. Nature methods, 10(3):221–227, 2013.\n\nRoshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. Evaluating protein transfer learning with tape. Advances in neural information processing systems, 32, 2019.\n\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.\n\nGabriel J Rocklin, Tamuka M Chidyausiku, Inna Goreshnik, Alex Ford, Scott Houliston, Alexander Lemak, Lauren Carter, Rashmi Ravichandran, Vikram K Mulligan, Aaron Chevalier, et al. Global analysis of protein folding using massively parallel design, synthesis, and testing. Science, 357 (6347):168–175, 2017.\n\nKaren S Sarkisyan, Dmitry A Bolotin, Margarita V Meer, Dinara R Usmanova, Alexander S Mishin, George V Sharonov, Dmitry N Ivankov, Nina G Bozhanova, Mikhail S Baranov, Onuralp Soylemez, et al. Local fitness landscape of the green fluorescent protein. Nature, 533(7603):397–401, 2016.\n\nTimo Schick and Hinrich Sch ̈utze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255–269, 2021.\n\nAmir Shanehsazzadeh, David Belanger, and David Dohan. Is transfer learning necessary for protein\n\nlandscape prediction? arXiv preprint arXiv:2011.03443, 2020.\n\nVignesh Ram Somnath, Charlotte Bunne, and Andreas Krause. Multi-scale representation learning\n\non proteins. Advances in Neural Information Processing Systems, 34:25244–25255, 2021.\n\nMartin Steinegger and Johannes S ̈oding. Mmseqs2 enables sensitive protein sequence searching for\n\nthe analysis of massive data sets. Nature biotechnology, 35(11):1026–1028, 2017.\n\nBaris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt Consortium. Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6):926–932, 2015.\n\nDamian Szklarczyk, Annika L Gable, David Lyon, Alexander Junge, Stefan Wyder, Jaime HuertaCepas, Milan Simonovic, Nadezhda T Doncheva, John H Morris, Peer Bork, et al. String v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets. Nucleic acids research, 47(D1):D607–D613, 2019.\n\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine\n\nLearning Research, 9(86):2579–2605, 2008.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.\n\nZichen Wang, Steven A Combs, Ryan Brand, Miguel Romero Calvo, Panpan Xu, George Price, Nataliya Golovach, Emmanuel O Salawu, Colby J Wise, Sri Priya Ponnapalli, et al. Lm-gvp: an extensible sequence and structure informed deep learning framework for protein property prediction. Scientific reports, 12(1):1–12, 2022.\n\nNicholas C Wu, Lei Dai, C Anders Olson, James O Lloyd-Smith, and Ren Sun. Adaptation in\n\nprotein fitness landscapes is facilitated by indirect paths. Elife, 5:e16965, 2016.\n\nSen Wu, Hongyang R Zhang, and Christopher R ́e. Understanding and improving information trans-\n\nfer in multi-task learning. In International Conference on Learning Representations, 2019.\n\nChanghui Yan, Feihong Wu, Robert L Jernigan, Drena Dobbs, and Vasant Honavar. Characterization\n\nof protein–protein interfaces. The protein journal, 27(1):59–70, 2008.\n\nKevin K Yang, Zachary Wu, Claire N Bedbrook, and Frances H Arnold. Learned protein embed-\n\ndings for machine learning. Bioinformatics, 34(15):2642–2648, 2018.\n\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:5824–5836, 2020.\n\nNingyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Qiang Zhang, Jiazhang Lian, and Huajun Chen. Ontoprotein: Protein pretraining with gene ontology embedding. In International Conference on Learning Representations, 2021a.\n\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. Differentiable prompt makes pre-trained language models better few-shot learners. In International Conference on Learning Representations, 2021b.\n\nZuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. arXiv preprint arXiv:2203.06125, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA MORE DETAILS OF PROMPTPROTEIN\n\nA.1 PROMPT-GUIDED MULTI-TASK PRE-TRAINING\n\nOne of the key problem to multi-task learning is what to share. Naive and gradient-based methods try to learn a shared MTL model. To overcome between task interference between tasks, they adjust magnitude and direction of gradients . However, negative transfer between pre-training and downstream tasks cannot be mitigated. To realize the potential of positive transfer, multi-task pretraining requires to learn and use task differences on-demand. Both adapter-based approaches and our proposed prompt-based approaches can learn task differences, whereas, for the flexibility of input, only prompt-based approach can use them on-demand. Figure 7 compares the mentioned multi-task methods.\n\nFigure 7: Comparison of multi-task pre-Training.\n\nA.2 PRE-TRAINING TASKS\n\nIn Figure 8, we illustrate our proposed two additional pre-training tasks.\n\nAlpha-Carbon Coordinate Prediction. We use a MLP network to project protein embeddings into 3D space. To equip the model with 3D invariance, after predicting the protein coordinates, we first recenter the ground-truth coordinate Z and predicted coordinate ˆZ and then employ Kabsch algorithm (Kabsch, 1976) to calculate the optimal rotation matrix that minimizes the root mean squared deviation. We first calculate cross-covariance matrix between two sets of coordinates:H = Z T ˆZ. Then the covariance matrix can be decomposed by singular value decomposition: H = U ΣV T . The optimal rotation matrix R can be formulated as: R = U V T .\n\nProtein-Protein Interaction Prediction. Since the limitation of GPU memory, it is not feasible to input two proteins in the same sequence. Instead, we leverage the representations of proteins to calculate protein-pair attention in decoder. Then the pair-aware protein representations can be obtained by multiplication of protein representations and the attention.\n\nA.3 ALGORITHMS FOR PROMPT-GUIDED MULTI-TASK PRE-TRAINING AND FINE-TUNING\n\nTo more easily appreciate the whole procedure of the prompt-guided multi-task pre-training and fine-tuning framework, we provide pseudo codes as follows.\n\nB ADDITIONAL DETAILS OF EXPERIMENTAL SETTING\n\nB.1 PRE-TRAINING DATASET\n\nTo exploit primary structure information, language modeling has been prove effective (Elnaggar et al., 2021; Alley et al., 2019). We follow Rives et al. (2021) to use UniRef50 (Consortium, 2021) dated March 28, 2018. 10% of UniRef50 clusters are randomly selected as a held-out evaluation set.\n\n14\n\n(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:30)(cid:22)(cid:21)(cid:20)(cid:28)(cid:19)(cid:24)(cid:22)(cid:25)(cid:18)(cid:17)(cid:22)(cid:25)(cid:28)(cid:16)(cid:23)(cid:15)(cid:14)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)⊕(a) Naive(b) Gradient-based(c) Adapter-based(d) Prompt-based (ours)pixh1L(y1,ˆy1|h1)L(y1,ˆy1|h)hhL(y1,ˆy1|h)L(y1,ˆy1|h1)h1xxx(cid:30)(cid:22)(cid:21)(cid:20)(cid:13)(cid:25)(cid:22)(cid:14)(cid:12)(cid:24)(cid:17)Published as a conference paper at ICLR 2023\n\nFigure 8: The Overview of Two Additional Pre-training Tasks. Left: Alpha-Carbon Coordinate Prediction. Right: Protein-Protein Interaction Prediction.\n\nAlgorithm 1: Prompt-Guided Multi-Task Pre-Training\n\nData: Input protein x, prompt pool p\n\nthe learning rate ζ.\n\nResult: Model parameters ψ while not converge do\n\nP =\n\n{\n\n∈\n\n[MLM], [CRD], [PPI]\n\n, task objectives\n\n}\n\nLp,\n\nfor p\n\nP do\n\n∈\n\nInitialize the task-specific input xp = x Compute the feature hp = fψ(xp; ψ)\n\np\n\n||\n\n// fψ contain L layers Prompt-aware Attention Module Lp(hp) according to Equation 3, 4 or 5\n\nCompute the loss\n\n(cid:80)\n\n− −\n\np(αp ·\n\nζ\n\n∇ψLp) according to Equation 6\n\nαp ·\n\nζ\n\n∇pLp,\n\np\n\n∀\n\n∈\n\nP\n\nend for Update the model parameters ψ = ψ Update the prompt parameters p = p\n\nend while\n\nAlgorithm 2: Prompt-Guided Fine-tuning\n\nData: Input protein x, downstream task object\n\nlearned prompt pool P = the learning rate ζ.\n\n{\n\nResult: Prompt-tuning module parameters θ.\n\nwhile not converge do\n\n′ L\n[MLM], [CRD], [PPI] }\n\np, , pre-trained model parameters ψ,\n\nCompute combined prompt p′ = τθ(p) according to Equation 7 Initialize the input xp′ = x Compute the feature hp′ = f (xp′; ψ) Compute the loss Update the prompt-tuning module parameters θ = θ\n\nLp′(hp′)\n\nLp′ =\n\np′\n\n||\n\nζ\n\n−\n\n∇θLp′\n\nend while\n\n15\n\n(cid:31)(cid:30)(cid:29)(cid:28)(cid:30)(cid:27)(cid:26)(cid:29)(cid:25)(cid:24)(cid:23)(cid:22)(cid:21)(cid:20)(cid:19)(cid:18)(cid:17)(cid:28)(cid:23)(cid:28)(cid:26)(cid:17)(cid:29)(cid:16)(cid:23)(cid:26)(cid:27)(cid:19)(cid:15)(cid:28)(cid:28)(cid:30)(cid:29)(cid:28)(cid:26)(cid:17)(cid:29)(cid:16)(cid:23)(cid:26)(cid:27)(cid:14)(cid:23)(cid:13)(cid:23)(cid:27)(cid:30)(cid:16)(cid:27)(cid:17)(cid:28)(cid:30)(cid:26)(cid:29)(cid:21)(cid:18)(cid:30)(cid:12)(cid:27)(cid:30)(cid:21)(cid:30)(cid:29)(cid:28)(cid:23)(cid:28)(cid:26)(cid:17)(cid:29)(cid:11)(cid:26)(cid:29)(cid:30)(cid:23)(cid:27)(cid:16)(cid:27)(cid:17)(cid:10)(cid:30)(cid:9)(cid:28)(cid:26)(cid:17)(cid:29)(cid:16)(cid:27)(cid:30)(cid:8)(cid:26)(cid:9)(cid:28)(cid:30)(cid:8)(cid:19)(cid:7)(cid:28)(cid:27)(cid:6)(cid:9)(cid:28)(cid:6)(cid:27)(cid:30)(cid:5)(cid:27)(cid:17)(cid:6)(cid:29)(cid:8)(cid:14)(cid:4)(cid:27)(cid:6)(cid:28)(cid:20)(cid:19)(cid:7)(cid:28)(cid:27)(cid:6)(cid:9)(cid:28)(cid:6)(cid:27)(cid:30)(cid:15)(cid:18)(cid:3)(cid:2)(cid:19)(cid:16)(cid:27)(cid:17)(cid:28)(cid:30)(cid:26)(cid:29)(cid:28)(cid:27)(cid:12)(cid:15)(cid:19)(cid:16)(cid:27)(cid:17)(cid:28)(cid:30)(cid:26)(cid:29)Published as a conference paper at ICLR 2023\n\nFor secondary and tertiary structure information, we extract data from Protein Data Bank (PDB) (Berman et al., 2000). For compatibility with pre-trained protein models, we only use proteins whose amino acid sequence length is less than 1,024. There are many ways to define the coordinates of protein residues. Here we use the coordinates of carbon alpha atoms.\n\nThe pre-training dataset for quaternary structures is constructed based on the latest STRING (Szklarczyk et al., 2019) database with only the physical-only mode, which means edges between the protein pairs indicate evidence of their binding or forming a physical complex. The database contains in total 65 million protein sequences from 14,094 species and 2.7 billion protein-protein interaction pairs. Note that there is no edge between proteins that come from different species.\n\nWe observe that the PPI network has a problem of uneven distribution, as illustrated in Figure 9, 107 edges. Such data distributions can lead the largest network contains 60,000 proteins and 3.5 models to over-focus on proteins from a single species. We pre-process our dataset by choosing the species networks with comparable sizes. Figure 10 illustrates the data distribution after preprocessing.\n\n×\n\nFigure 9: Visualization of the number of nodes and the number of edges in the original database.\n\nFigure 10: Visualization of the number of nodes and the number of edges in the pre-processed database.\n\nB.2 DOWNSTREAM TASK DATASETS.\n\nThe statistical results of the downstream datasets are shown in Table 4.\n\nEvaluation Metrics For multiple binary classification tasks, we employ protein-centric maximum F-score Fmax and pair-centric area under precision-recall curve AUPRpair to evaluate protein models. For regression tasks, we employ spearman’s correlation ρ to evaluate protein models.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Statistics of the downstream datasets.\n\nDATASET\n\n#TRAIN\n\n#VALIDATION\n\n#TEST\n\nTASK\n\nENZYME COMMISSION GENE ONTOLOGY STABILITY FLUORESCENCE THERMOSTABILITY AAV (1-VS-REST) GB1 (1-VS-REST) GB1 (2-VS-REST) SABDAB\n\n15,551 29,902 53,679 21,446 24,817 1,170 29 427 345\n\n1,729 3,323 2,447 5,362 -\n- -\n- 48\n\n1,919 3,416 12,839 27,217 3,314 81,413 8,704 8,306 99\n\nCLASSIFICATION CLASSIFICATION REGRESSION REGRESSION REGRESSION REGRESSION REGRESSION REGRESSION REGRESSION\n\n• Fmax. Given a target protein i, we denote its experimentally determined function terms [0, 1], we denote predicted function terms as\n\nas Ti. Given a set of decision threshold t Pi(t). The precision and recall of this protein can be formulated as:\n\n∈\n\nprecisioni(t) =\n\nI[f\n\n(cid:80) f\n(cid:80) f\n\n∈ I[f\n\nPi(t)\n\nTi]\n\n∩ Pi(t)]\n\nrecalli(t) =\n\n(cid:80) f\n\nI[f (cid:80)\n\nf\n\n∈ I[f\n\n∈ Pi(t)\n\n∩ Ti]\n\n∈\n\nTi]\n\n,\n\n,\n\n(8)\n\n(9)\n\nwhere I[ ] is an indicator function that is equal to 1 if and only if the condition is true. ·\nCombining these two measures, the Fmax is defined as the maximum value of F-measure:\n\nFmax = max\n\nt {\n\nprecision(t)\n\n2 ·\nprecision(t) + recall(t) }\n\nrecall(t)\n\n·\n\n,\n\n(10)\n\nwhere precision(t) = 1 i recalli(t). The N is denoted as the number of proteins and M (t) is denoted as the number of proteins on which at least one prediction result is above threshold t.\n\ni precisioni(t), and precision(t) = 1\n\nM (t)\n\nN\n\n(cid:80)\n\n(cid:80)\n\n• AUPRpair. The pair-centric area under precision-recall curve is exactly the micro average\n\nprecision score where precision and recall are for each term f :\n\nprecisionf (t) =\n\n(cid:80)\n\nI[f\n\ni (cid:80) i\n\n∈ I[f\n\nPi(t)\n\nTi]\n\n∩ Pi(t)]\n\nrecallf (t) =\n\n(cid:80) i\n\nI[f (cid:80) i\n\n∈ I[f\n\n∈ Pi(t)\n\n∩ Ti]\n\n∈\n\nTi]\n\n.\n\n,\n\n(11)\n\n(12)\n\n• ρ. Spearman’s is a nonparametric measure of rank correlation for ground-truth Y and\n\npredicted ˆY landscape. We denote R( ̇) as ranks. The correlation coefficient is:\n\nρ =\n\ncov(R(Y), R ˆY)) σR(Y )σR( ˆY )\n\n,\n\n(13)\n\nwhere cov( ) is the covariance of the variables, and σR(·) is the standard deviations of the ·\nrank variables.\n\n, ·\n\nEnzyme Commission and Gene Ontology. EC numbers are selected from the third and fourth levels of the EC tree, forming 538 binary classification tasks. GO terms are hierachically organized into three ontologies – biological process (1943 binary classification tasks), molecular function (489 binary classification tasks), and cellular component (320 binary classification tasks). Following DeepFRI (Gligorijevi ́c et al., 2021), we use the protein sequences in the test set with 95% sequence identity to the training set.\n\nStability Landscape Prediction (Rocklin et al., 2017). This is a regression task that maps each protein to a label, measuring the most extreme case where the protein maintains its fold above a concentration threshold. This task aims to test the ability to generalize from a broad sampling of\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nrelevant sequences to local neighborhood of a few sequences. The train set includes proteins from experimental design, while the test set contains single mutants.\n\nFluorescence Landscape Prediction (Sarkisyan et al., 2016). This is a regression task that maps a protein to a label corresponding to the log-fluorescence intensity. This task aims to test the ability to distinguish mutants. The train set includes triple mutants of the wild-type green fluorescent protein (GFP), while the test set contains more mutants.\n\nThermostability Landscape Prediction (Jarzab et al., 2020). This is a regression task that maps a protein to a thermostability label. We adopt the mixed split proposed by Dallago et al. (2021) that using MM-seqs2 (Steinegger & S ̈oding, 2017) at a threshold of 20% sequence identity creates one split. The train set includes all sequences in 80% of clusters, while the test contains the remaining 20% of clusters.\n\nAdeno-associated virus (AAV) Landscape Prediction (Bryant et al., 2021). This is a regression task that predicts fitness for a long mutated sequence. We adopt the 1-vs-rest split, where wild type and single mutants are assigned to train set, while test set contains the rest. This split are common in protein engineering application.\n\nGB1 Landscape Prediction (Wu et al., 2016). This is a regression task that predicts the effects of interactions between mutations. We adopt the 1-vs-rest (and 2-vs-rest) split, where wild type and single mutants (and double mutants) are assigned to train set, while test set contains the rest.\n\nAntibody-antigen Affinity Prediction (Dunbar et al., 2014). This is a regression task that takes a pair of proteins as input and predicts the affinity between them. We adopt random split which contains 493 pairs, 431 antibodies and 401 antigens.\n\nTable 5: Model performance on FLIP benchmark.\n\nDATASET\n\nTHERMO\n\nAAV\n\nMIXED\n\nHUMAN\n\nHUMAN-CELL\n\n1-VS-R\n\n2-VS-R\n\n1-VS-R\n\n2-VS-R\n\nESM-1B OURS\n\n0.68 0.683\n\n0.70(0.691) 0.702\n\n0.75(0.673) 0.684\n\n0.04 0.551\n\n0.26 0.595\n\n0.32 0.403\n\n0.36 0.550\n\nGB1 3-VS-R\n\n0.54 0.783\n\nLOW-VS-HIGH\n\n0.13 0.294\n\nTo illustrate the advantage of prompt-tuning in low-resource scenarios, we only selected a subset of tasks in the FLIP benchmark. In Table 5, we report the performance of our model on other tasks. Note that, although we use the reported results of esm-1b in the above table, we additionally provide the reproduced results of esm-1b on Thermo(Human) and Thermo(Human-cell). These values (surrounded by brackets) are lower than reported.\n\nFigure 11: Attention visualization. We select GB1 protein as an example and visualize attentions of the 15-th layer (high skip weight for [MLM]) and the 33-th layer (high skip weight for [CRD] and [PPI]).\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nB.3 ANALYSIS OF NEURAL LAYERS\n\nIn Figure 11(a), we visualize the attentions in the 15-th layer (a high skip weight for [MLM]) and the 33-th layer (a high skip weight for [CRD] and [PPI]). We observe that one amino acid in the 15-th layer can only attend to the local neighbors in the sequence, whereas the amino acid in the 33-th layer can attend to those amino acids that are more distant along the sequence but potentially closer in the 3D space. This observation demonstrates the primary structural knowledge learned by MLM pays more attention to sequential dependency, whereas the tertiary structural and quaternary structural knowledge learned by CRD and PPI tasks can capture the information from adjacent amino acids in the 3D space.\n\nB.4 ADDITIONAL EXPERIMENT RESULTS\n\nDo downstream tasks benefit from the acquired information on-demand by prompt tuning?\n\nTo further analyze the importance of prompt-guided fine-tuning, we conduct an ablation study on the binding affinity prediction task on the SAbDab dataset (Dunbar et al., 2014). From Figure 12, we observe that PromptProtein performs worst without any prompt tokens. In contrast, adding either of the three prompt tokens, especially the token corresponding to the PPI task, can significantly improve performance. By combining different prompts without prompt tuning, we can obtain protein representations enhanced by multiple structural information. By doing that, we find the combination of the [MLM] and [PPI] prompts empowers PromptProtein to achieve the best performance. It is also notable that, by comparing the results of adding [MLM] and [PPI] prompts and adding all prompts, the [CRD] prompt leads to a performance decrease. These results evidence that not all structure information from pre-training is beneficial for downstream tasks, and adaptively combining acquired information via prompt-tuning leads to better performance.\n\nFigure 12: Ablation of PromptProtein with different prompt tokens on SAbDab (spearman’s ρ ).\n\n19\n\n(cid:31)(cid:30)(cid:29)(cid:28)(cid:30)(cid:29)(cid:27)(cid:26)(cid:25)(cid:24)(cid:29)(cid:30)(cid:23)(cid:22)(cid:26)(cid:21)(cid:20)(cid:19)(cid:28)(cid:30)(cid:18)(cid:23)(cid:30)(cid:17)(cid:29)(cid:26)(cid:16)(cid:15)(cid:14)(cid:13)(cid:26)(cid:12)(cid:11)(cid:25)(cid:10)(cid:9)(cid:15)(cid:10)(cid:8)(cid:7)(cid:17)(cid:29)(cid:19)(cid:6)(cid:5)(cid:9)(cid:4)(cid:3)(cid:4)(cid:21)(cid:21)(cid:2)(cid:21)(cid:21)(cid:2)(cid:1)(cid:6)(cid:5)(cid:9)(cid:4)(cid:3)(cid:4)(cid:1)(cid:6)(cid:5)(cid:9)(cid:4)(cid:3)(cid:4)(cid:1)(cid:21)(cid:21)(cid:2)(cid:25)(cid:3)(cid:3)",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a prompt-based multi-task framework for pre-training and fine-tuning protein sequence representations. From a methodological standpoint, the paper adapts the prompt fine-tuning idea (ie., a differentiable continuous prompt token is pre-trained instead of using a discrete prompt) from the NLP literature to protein modeling with large transformer networks. It then makes two contributions over the standard transformer architecture 1) A specific masking scheme for the prompt token is used to keep only the effect of the prompt on the input sequence and eliminate the opposite effect (ie., the prompt should be task-dependent and not sample-dependent) 2) Learned task-specific layer-specific skip connection linear projections to let the network learn different weights for each task at each layer. The multi-task pre-training involves three kinds of pre-training: a) masked-language modeling with sequence-based information only (MLM) b) Prediction of the alpha-carbons positions (CRD) c) prediction of protein-protein interactions (PPI). Experiments on function annotation and protein engineering (FLIP benchmark) demonstrate the benefits of the different ideas introduced in this work.\n\n# Strength And Weaknesses\n\n**Strengths**\n- The idea of combining different pre-training tasks seems very sensible for proteins given the diversity of modalities that characterize them (eg., via their primary, secondary, tertiary and quaternary structure). As noted by the authors, there is however a high risk of task interference when one wants to obtain pre-trained representations that combine these different modalities / tasks. The approach suggested by authors appears to be doing a fine job at efficiently combining these different modalities given the performance reported in sections 4.2. and 4.3.\n- The introduced prompt masking and skip connections both appear to be critical to strong empirical performance — the latter seems to be particularly important to mitigate task interference as evidenced by the ablations in section 4.4.\n- The paper is very clear overall (in particular the methodology section) with nice visuals facilitating understanding and additional analyses in section 4.5 to help investigate the source of the performance lift.\n\n**Weaknesses**\n- The ablations for the different pre-training tasks in section 4.5 / Figure 6 are a bit puzzling. It does seem that the CRD task has destructive value on that particular binding affinity prediction task since: a) the performance of CRD + MLM or CRD + PPI leads to both lower performance Vs MLM or PPI alone respectively b) the performance of CRD + MLM + PPI is also lower vs just using MLM + PPI. This seems particularly important from a practical standpoint, and additional experiments are needed to confirm whether: 1) that problem applies to other downstream tasks or is just specific to binding affinity prediction — and if so, why?  2) there is something fundamentally wrong with the CRD pre-training as currently implemented? 3) there is a way to anticipate ex ante (or post fine tuning) which tokens should be used to ensure optimal task performance ?\n- The ablation in section in Table 3 is a bit puzzling as well: it appears that the performance of PromptProtein without layer skip is lower than the performance from the conventional MTL. Could you please explain why that might be the case? (I would have assumed intermediate performance between conventional MTL and full PromptProtein as I presume the attention masks are still used in that ablation?)\n- Several points (in section 4 primarily) were not fully clear (see clarity paragraph below).\n- The following claim in conclusion does not seem fully substantiated: “PromptProtein beats state-of-the-art baselines by significant margins”. Authors do report the relevant baselines listed in the FLIP paper [1]. But since that paper was released, several methods have shown markedly superior performance for protein modeling & achieving high spearman with deep mutational scanning assays — see for example, [2] and [3]. I would suggest adding these two baselines to the analysis or tone done the SOTA claims.\n\n------------------------------------------------------------------------------------------------------------------------\n[1] Dallago, C., Mou, J., Johnston, K.E., Wittmann, B.J., Bhattacharya, N., Goldman, S., Madani, A., & Yang, K.K. (2022). FLIP: Benchmark tasks in fitness landscape inference for proteins. bioRxiv.\n\n[2] Hsu, C., Verkuil, R., Liu, J., Lin, Z., Hie, B.L., Sercu, T., Lerer, A., & Rives, A. (2022). Learning inverse folding from millions of predicted structures. bioRxiv.\n\n[3] Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**\n- Could you please clarify how you handle the instances where not all tasks are available for the same proteins in a given mini-batch (eg., instances where primary structure is available in Uniref50, but structure is not available in the PDB)?\n- I would clarify in section 3.1 that $g_p$ (the linear projection of the prompt p) is a scalar. In my first lecture, I had (wrongly) assumed this was a vector of the same size as $h_p$ but that was not really making sense anymore when reading the analysis in section 4.\n- “We observe that one amino acid in the 15-th layer can only attend to the local neighbors in the sequence, whereas the amino acid in the 33-th layer can attend to those amino acids that are more distant along the sequence but potentially closer in the 3D space” (in section B.3) —> I found this particularly insightful and would recommend to move this section from supplementary to the main text if space allows.\n- What is the nature of the operator $\\tau_\\theta$ used for fine tuning? Linear projection? Any non-linearity applied?\n- What is ESM-unstrained (Table 2)?\n- Fig5 analysis — it is not clear what is being done here. Is this analysis conducted for a particular protein sequence (if yes, which one)? Or some aggregation over Uniref50 sequences? The embeddings from which layer(s) are being used here?\n- I find the “skip connection” terminology a bit confusing as it seems that the term $g_p$ is used as a multiplicative factor for the attention-based transform but not the skip connection term. Also on Figure 3, the arrow labeled “skip connection” is in fact not the skip connection as it supports the computation of the  $g_p$ term which should not be present in the skip connection as per equation 2.\n- In the conclusion: “In the future, we are interested in examining the effectiveness of the proposed prompt-guided multi-task pre-training and fine-tuning framework in domains where hierarchical task information is required in the pre-training stage.” —> Could you please provide examples for such domains?\n\n**Quality**\n- Very sound approach overall. Authors provided some very compelling empirical results, yet there are a few concerns with some of the ablation results as detailed above. Also the SOTA claim in the is not fully substantiated as discussed above as well.\n\n**Novelty**\n- The prompt masking and skip-connection weights are novel to my knowledge and appear to be both critical to the strong reported performance. Could the authors please confirm these two ideas are indeed introduced for the first time in this paper and not borrowed from the NLP literature?\n\n**Reproducibility**\n- Authors confirm that the code will be open sourced upon acceptance (section 4.1)\n\n# Summary Of The Review\n\nThis paper is aiming to address a very important area in protein modeling: learning rich sequence embeddings by leveraging multiple pre-training tasks jointly. The approach is sound and the methodological section is overall very clearly presented. There are a few concerns with respect to some of the results and claims as detailed above. Given the several strengths of the work, I would be willing to increase my score if authors address these points during rebuttal.\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------\n[UPDATE POST REBUTTAL]\n\nMy most important concerns have been alleviated during rebuttal and I have increased my score accordingly.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSYNCHRONIZED PRUNING FOR EFFICIENT CONTRASTIVE SELF-SUPERVISED LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nVarious self-supervised learning (SSL) methods have demonstrated strong capability in learning visual representations from unlabeled data. However, the current state-of-the-art (SoTA) SSL methods largely rely on heavy encoders to achieve comparable performance as the supervised learning counterpart. Despite the well-learned visual representations, the large-sized encoders impede the energyefficient computation, especially for resource-constrained edge computing. Prior works have utilized the sparsity-induced asymmetry to enhance the contrastive learning of dense models, but the generality between asymmetric sparsity and self-supervised learning has not been fully discovered. Furthermore, transferring the supervised sparse learning to SSL is also largely under-explored. To address the research gap in prior works, this paper investigates the correlation between intraining sparsity and SSL. In particular, we propose a novel sparse SSL algorithm, embracing the benefits of contrastiveness while exploiting high sparsity during SSL training. The proposed framework is evaluated comprehensively with various granularities of sparsity, including element-wise sparsity, GPU-friendly N :M structured fine-grained sparsity, and hardware-specific structured sparsity. Extensive experiments across multiple datasets are performed, where the proposed method shows superior performance against the SoTA sparse learning algorithms with various SSL frameworks. Furthermore, the training speedup aided by the proposed method is evaluated with an actual DNN training accelerator model.\n\n1\n\nINTRODUCTION\n\nThe early empirical success of deep learning was primarily driven by supervised learning with massive labeled data, e.g., ImageNet (Krizhevsky et al., 2012). To overcome the labeling bottleneck of deep learning, learning visual representations without label-intensive datasets has been widely investigated (Chen et al., 2020a; He et al., 2020; Grill et al., 2020; Zbontar et al., 2021). The recent self-supervised learning (SSL) methods have shown great success and achieved comparable performance to the supervised learning counterpart. The common property of various SSL designs is utilizing different augmentations from the original images to generate contrastiveness, which requires duplicated encoding with wide and deep models (Meng et al., 2022). The magnified training effort and extensive resource consumption make the SSL-trained encoder infeasible for on-device computing (e.g., mobile devices). The contradiction between label-free learning and extraordinary computation cost limits further applications of SSL, also necessitating efficient sparse training techniques for self-supervised learning.\n\nFor supervised learning, sparsification (a.k.a. pruning) has been widely explored, aiming to reduce computation and memory costs by removing unimportant parameters during training or fine-tuning. Conventional supervised pruning explores weight sparsity based on a pre-trained model followed by additional fine-tuning to recover the accuracy (Han et al., 2016). For self-supervised learning, recent work (Chen et al., 2021) also sparsified a pre-trained dense SSL model for the downstream tasks with element-wise pruning. In addition to the fine-grained sparsity, MCP (Pan et al., 2022) exploited the filter-wise sparsity on the MoCo-SSL (He et al., 2020) model. Both of these sparse SSL works (Chen et al., 2021; Pan et al., 2022) exploit sparsity based on the pre-trained dense model. However, compared to supervised learning, obtaining the pre-trained model via SSL requires a significant amount of additional training effort (∼200 epochs vs. ∼1,000 epochs). Therefore, exploring post-training sparsity via fine-tuning is not an ideal solution for efficient SSL.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: (a) Applying self-damaging scheme (Jiang et al., 2021) to SSL. (b) Applying prune-andregrow scheme (Liu et al., 2021) to SSL. (c) Proposed contrastiveness-aware sparse training.\n\nOn the other hand, sparsifying the model during supervised training (Dettmers & Zettlemoyer, 2019; Evci et al., 2020) has emerged as a promising technique to elevate training efficiency while obtaining a sparse model. To accurately localize the unimportant parameters, prior works investigated various types of important metrics, including gradient-based pruning (Dettmers & Zettlemoyer, 2019) and the “prune-regrow” scheme (Liu et al., 2021). Compared to the post-training sparsification methods, in-training sparsification for supervised training has achieved memory/computation reduction as well as speedup of the training process. However, exploiting in-training sparsity for SSL models that are trained from scratch is still largely under-explored.\n\nMore recently, SDCLR (Jiang et al., 2021) proposed the sparsified “self-damaging” encoder, which generates the “sparse-dense” SSL by exploiting fixed high sparsity on one contrastive path (e.g., offline encoder), while keeping the counterpart dense (e.g., online encoder), as shown in Figure 1(a). Such “sparse-dense” SSL architecture helps to enhance contrastive learning, leading to improved performance with the non-salient and imbalanced samples. Nevertheless, it mainly focuses on the performance enhancement of the SSL-trained dense model (i.e., SimCLR (Chen et al., 2020a)), and whether such a “sparse-dense” asymmetric learning scheme works in other SSL methods remains unclear. On the other hand, the compatibility of the existing SoTA sparse training techniques (Dettmers & Zettlemoyer (2019); Evci et al. (2020); Liu et al. (2021)) is also ambiguous to SSL. As shown in Figure 1(b), they require to frequently prune and regrow the model architecture during SSL training, while SDCLR maintains a fixed “sparse-dense” architecture during the entire training process. The under-explored sparse contrastiveness and expensive self-supervised learning inspire us to investigate the following question: How to efficiently sparsify the model during self-supervised training with the awareness of contrastiveness?\n\nTo address this question, we present Synchronized Contrastive Pruning (SyncCP), a novel sparse training algorithm designed for self-supervised learning. To maximize the energy efficiency of SSL training, SyncCP exploits in-training sparsity in both encoders with high compatibility of SSL. The main contributions of this work are:\n\n• We first discover the limitations of the sparsity-induced asymmetric SSL in SDCLR (Jiang et al., 2021) and show that the sparsity-induced “sparse-dense” asymmetric architecture is not universally applicable for various SSL schemes.\n\n• We demonstrate the incompatibility of the SoTA “prune-and-regrow” sparse training method for SSL. Specifically, we formalize the iterative architectural changes caused by applying “prune-and-regrow” to SSL, named as architecture oscillation, and observe that frequently updating the pruning candidates lead to larger architecture oscillation, which further hinders the performance of self-supervised learning.\n\n• We present SyncCP, a new sparse training algorithm designed for self-supervised learning. SyncCP gradually exploits high in-training sparsity in both encoders with contrastive\n\n2\n\nEMA / CopyConsistentContrastivenessDense Online EncoderSparse Offline EncoderLStatic High SparsityLossEMA / CopySparsity Online EncoderSparse Offline EncoderLLossUnstableContrastivenessRegrowEMA / CopySparsity Online EncoderSparse Offline EncoderLLossConsistentContrastivenessSynchronizedSpars. Gap: ΔsSelf-Damaging SSLApplying SoTAPrune-and-regrow to SSLThis work-SSL Friendly -Low HW Compatibility-Unproved Generality-SSL Unfriendly-Low HW Compatibility-High Sparsity-SSL Friendly-High HW Compatibility-High Sparsity(a)(b)(c)Under review as a conference paper at ICLR 2023\n\nsynchronization and optimally-triggered sparsification, maximizing the training efficiency without hurting the contrastiveness of SSL.\n\n• SyncCP is a general sparse training method for SSL which is compatible with various granularities of sparsity, including element-wise pruning, N :M sparsity, and structured sparsity designed for a custom hardware accelerator.\n\n• We validated the proposed method against previous SoTA sparsification algorithms on CIFAR-10, CIFAR-100, and ImageNet datasets. Across various SSL frameworks, SyncCP consistently achieves SoTA accuracy in all experiments.\n\n2 RELATED WORKS\n\n2.1 CONTRASTIVE SELF-SUPERVISED LEARNING\n\nSelf-supervised learning recently has gained popularity due to its ability to learn visual representation without labor-intensive labeling. Specifically, pioneering research works (He et al., 2020; Chen et al., 2020a) utilize the contrastive learning scheme (Hadsell et al., 2006) that aims to group the correlated positive samples while repelling the mismatched negative samples (Oord et al., 2018). The performance of the contrastive learning-based approaches largely depends on the contrastiveness between the positive and negative samples, which requires large-sized batches to support. As indicated by SimCLR (Chen et al., 2020a), the performance of SSL is sensitive to the training batch size, and the inflated batch size elevates training cost. MoCo (He et al., 2020; Chen et al., 2020b) alleviates such issue with queue-based learning and momentum encoder, where the extensive queueheld negative samples provide proficient contrastiveness, and the slow-moving average momentum encoder derives consistent negative pairs. BYOL (Grill et al., 2020) simplifies and outperforms the prior works by only learning positive samples, while the online latent features are projected by an additional predictor qθ:\n\nonline prediction = qθ(gθ(fθ(X))) offline target = gξ(fξ(X ′))\n\n(1)\n\n(2)\n\nWhere f and g represent the encoder and projector of online (θ) and offline (ξ) paths with augmented input X and X ′, respectively. The predictor qθ generates an alternative view of the projected positive samples, and the offline momentum encoder provides consistent encoding for contrastive learning. Overall, salient and consistent contrastiveness is essential to contrastive self-supervised learning.\n\n2.2 SPARSE TRAINING\n\nDNN sparsification has been widely investigated under the supervised learning domain, which can be generally categorized based on the starting point of sparsification. Early works mainly focus on post-training sparsification (Han et al., 2016; Evci et al., 2020; Jayakumar et al., 2020), which removes the weights from the pre-trained model and then recovers the accuracy with subsequent fine-tuning. Other works exploit weight sparsity prior to the training process (Wang et al., 2019; Lee et al., 2018), and the resultant model is trained with the sparsified architecture.\n\nIn contrast to post-training or pre-training sparsification, exploiting sparsity during training generates the compressed model with one-time training from scratch, eliminating the requirement of a pre-trained model or extensive searching process. With the full observability of the training process, the magnitude of the gradient can be used to evaluate the model reflection with the exploited sparsity. Motivated by this, GraNet (Liu et al., 2021) utilizes the “prune-and-regrow” technique to periodically remove the unimportant non-zero weights from the sparse model and then regrow certain pruning candidates back. Given the targeted sparsity st and total prune ratio rt at iteration t, unimportant weights w are removed based on the Top-K magnitude scores:\n\nSubsequently, the sensitive weights are re-grown back based on the reflection of gradient gt:\n\n′\n\nw\n\n= TopK(|w|, rt)\n\n′\n\nw = w\n\n+ TopK(gt\n\ni!=w′ ,rt−st\n\n)\n\n(3)\n\n(4)\n\nSince the gradient gt indicates the instant model sensitivity at iteration t, the optimal sparse model architecture can be varied between two adjacent pruning steps.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n2.3 CONTRASTIVE LEARNING WITH SPARSITY-INDUCED ASYMMETRY\n\nAs introduced in Section 2.1, salient and consistent contrastiveness is essential for contrastive SSL, where the contrastiveness can be constructed via negative samples or the auxiliary predictors (Grill Inspired by (Hooker et al., 2019), SDCLR (Jiang et al., 2021) amplifies the conet al., 2020). trastiveness by pruning one encoder of SimCLR (Chen et al., 2020a) while keep the identical twin dense. Such “sparsity-induced asymmetry” elevates the performance of SSL with the improved performance of the dense model on the long-tailed data samples. However, SDCLR (Jiang et al., 2021) is not designed for model compression or efficiency improvements. Furthermore, the generality of such sparsity-induced asymmetry remains under-explored in other SSL frameworks.\n\n3 CHALLENGE OF SPARSE SELF-SUPERVISED LEARNING\n\n3.1 LIMITATIONS OF SPARSITY-INDUCED ASYMMETRY\n\nIt has been shown in SDCLR (Jiang et al., 2021) that the sparsity-induced “sparse-dense” asymmetry is beneficial to contrastive SSL. SDCLR (Jiang et al., 2021) is specifically built upon the SimCLR (Chen et al., 2020a) framework with shared encoders, where the pruned architectures have the dense twin in the mirrored contrastive encoder. However, the generality of sparsity-induced asymmetry remains unproved in other SSL methods, which motivates us to investigate the question:\n\nQuestion 1: For contrastive self-supervised learning with non-identical encoders, will the sparsityinduced asymmetric encoders still result in elevated performance for contrastive learning?\n\nTo answer the above question, we use MoCo-V2 (Chen et al., 2020b) and follow the procedure of SDCLR (Jiang et al., 2021) to generate a highly-sparse online encoder prior to the training process. Given the online and offline (momentum) encoder θ and ξ with weights Wθ and Wξ, we have:\n\nonline output = gθ(fθ(X ∗ (Mθ · Wθ))) offline output = gξ(fξ(X ′))\n\n(6) The online encoder mask Mθ produces a sparse online encoder with initialized element-wise sparsity (Han et al., 2016) at 90%, while the offline encoder is updated by exponential moving average (EMA). The gradient-based update of the online encoder keep recovering the performance drop caused by the high sparsity mask. Following the setup of SDCLR (Jiang et al., 2021), the sparsity is periodically updated at the beginning of each epoch. Table 1 summarizes the linear evaluation accuracy on the CIFAR-10 dataset with different static online sparsity values. As opposed to the performance of SimCLR in (Jiang et al., 2021), directly applying the high sparsity-based perturbation to MoCo-V2 (Chen et al., 2020b) is challenging, and leads to considerable performance degradation. Reversing the sparsity between online and offline encoder also shows the similar results, as presented in Appendix D.\n\n(5)\n\nTable 1: Largely degraded performance of MoCo-V2 (Chen et al., 2020b) with self-damaging SSL (Jiang et al., 2021) on CIFAR-10 dataset.\n\nResNet-18 Encoder Fixed Sparsity Linear Eval. Acc (%)\n\nOnline 90% 88.72 (-3.41%)\n\nDense Model Acc. = 92.09% Momentum 0% 87.68 (-4.31%)\n\nOnline 50% 92.10 (+0.01%)\n\nMomentum 0% 92.07 (-0.02%)\n\nSummarizing these empirical results, our main observation is:\n\nObservation 1: Compared to the online encoder, the EMA-updated momentum encoder has the delayed architecture, which makes it unqualified to be the “competitor” as SDCLR (Jiang et al., 2021). The directly-applied high sparsity overshoots the asymmetric learning, leading to degraded self-supervised learning.\n\n3.2 FREQUENT ARCHITECTURE CHANGING HINDERS SELF-SUPERVISED LEARNING\n\nAs depicted in Eq. 4, the “prune-and-regrow” scheme such as GraNet (Liu et al., 2021) uses instant gradient magnitude to indicate the model sensitivity after magnitude pruning, removing the unimportant and insensitive weights while gradually achieving high sparsity. Observation 1 demonstrates the incompatibility of the directly-applied high sparsity in SSL, then the following question raises:\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Sparse training with “prune-and-regrow” scheme on BYOL (Grill et al., 2020).\n\nBYOL (Grill et al., 2020)\n\nCIFAR-10 Acc (%)\n\nResNet-18\n\nDense Model Acc. = 92.42%\n\nOnline Encoder Sparsity\n\n0%→80%\n\n0%→90%\n\nOnline Linear Eval. Acc (%)\n\n91.20±0.02\n\n90.13±0.06\n\nMomentum Encoder Sparsity\n\n0%→50%\n\n0%→60%\n\nMomentum Linear Eval Acc. (%)\n\n91.31±0.07\n\n90.09±0.04\n\nFigure 2: (a) Layer-wise oscillation at different steps of pruning. “SC” stands for the shortcut connection of ResNet-18 model. (b) Gradually-increased sparsity of GraNet (Liu et al., 2021) leads to inconsistent asymmetry.\n\nQuestion 2: If we apply the gradually-increased sparsity for both encoders, will the “prune-andregrow” scheme also be feasible for self-supervised learning? To address Question 2, we use the SoTA GraNet (Liu et al., 2021) as the example algorithm to exploit in-training sparsity on both encoders of BYOL (Grill et al., 2020), where the regrowing process is only applied to the online encoder. Starting with the dense models, we gradually prune the online and offline encoders to 90% and 60% sparsity in an element-wise fashion with periodically-updated sparsity. For sparse SSL training, the results of such gently-increased sparsity scheme reported in Table 2 outperforms those by (Jiang et al., 2021) (Table 1) by a significant margin. However, the state-of-the-art supervised pruning algorithm still incurs 2.3% linear evaluation accuracy degradation with SSL on the CIFAR-10 dataset.\n\nCompared to the self-damaging SSL with fixed sparsity (Jiang et al., 2021), the “prune-and-regrow” method keeps swapping the pruning candidates to minimize the model sensitivity, oscillating the encoder architecture during training. We quantify such architecture oscillation by XORing the masks generated from magnitude pruning MM P and gradient-based regrow Mg:\n\nGcor = MM P ⊕ Mg ∈ {0, 1}\n\n(7)\n\nUnder the same sparsity ratio, the number of “1”s in Gcor indicates the amount of architecture oscillation caused by the gradient-based regrow. During the early stage of training, almost all the magnitude pruning candidates are replaced by the regrowing process, as shown in Figure 2(a). The high degree of architecture oscillation implies drastic changes in the sparse model architecture. In the meantime, gradually sparsifying two encoders with different target sparsity further destroys the consistency of self-supervised learning, as shown in Figure 2(b). As a result, we have the following observation for Question 2:\n\nObservation 2: Sparsifying the model with frequently changing architecture hinders the contrastiveness and consistency of self-supervised learning and leads to degraded encoder performance.\n\nAs shown in Observation 1 and Observation 2, high sparsity-induced asymmetry is not directlyapplicable to sparse SSL, while the consistency requirements of SSL negates the plausibility of gradual sparsity increment. The dilemma between self-supervised learning and sparse training derives the following challenge:\n\nHow can we efficiently sparsify the model during self-supervised training while maximizing the benefits of the sparsity-induced asymmetry?\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4 METHOD\n\nTo address the above challenge, we propose Synchronized Contrastive Pruning (SyncCP), which successfully alleviates the contradiction between the needs of high sparsity and the requirements of consistency in self-supervised learning.\n\n4.1 SYNCHRONIZED SPARSIFICATION (SYNCS)\n\nThe rationale behind the sparsity-induced asymmetric SSL is that the perturbation generated by the pruned encoder elevates the difference between contrastive features. As indicated by Observation 1 and Table 1, the high sparsity-induced asymmetry is not universally applicable, but the SSL can be rewarded from the asymmetry incurred by lower sparsity (e.g., 50%), where the SSL-trained sparse and dense encoders exhibit negligible accuracy degradation compared to the baseline. Motivated by this, we propose the Synchronized Sparsification (SyncS) technique to exploit sparsity in both contrastive encoders. Given the online and offline (momentum) encoder θ and ξ with weights Wθ and Wξ, the in-training sparsification can be expressed as:\n\nonline output = gθ(fθ(X ∗ (Mθ · Wθ))) offline output = gξ(fξ(X ′ ∗ (Mξ · Wξ))) (9) Where Mθ and Mξ represent the online and offline (momentum) sparse masks with sparsity sθ and sξ. The proposed SyncS scheme gradually exploits the sparsity in both encoders while maintaining a consistent sparsity gap ∆s between them during SSL training. At each pruning step t, we have:\n\n(8)\n\nθ = sf st\n\nξ = sf st θ − st\n\ns.t |st\n\n)3\n\nθ + (si\n\nθ )(1 −\n\nθ − sf\n\nt − t0 n∆t t − t0 n∆t ξ| = ∆s, for t ∈ {t0, t0 + ∆t, ..., t0 + n∆t}\n\nξ − sf\n\nξ )(1 −\n\nξ + (si\n\n)3\n\n(10)\n\n(11)\n\n(12) The exponent controls the speed of sparsity increment, we adopt the sparsity schedule of Eq. 10-12 from (Liu et al., 2021) to minimize the impact of the parameter tuning. The synchronized sparsity increment with the constraints of ∆s prevents the exceeding asymmetry between contrastive encoders while minimizing the distortion caused by the changing sparsity. In practice, ∆s is treated as a tunable parameter which impacts the final sparsity of both online and offline encoders. To guarantee the consistency of the contrastive sparsity, both sθ and sξ are initialized by Erdos Renyi Kernel (ERK) (Evci et al., 2020), and with respect to ∆s, we evaluate the impact of ∆s in Appendix.\n\n4.2 CONTRASTIVE SPARSIFICATION INDICATOR (CSI)\n\nAchieving high sparsity requires gentle sparsity increment, but as presented in Observation 2, the inconsistent architecture difference deteriorates the contrastiveness of SSL. On the other hand, the popular EMA-based update (He et al., 2020) allows the momentum encoder to generate consistent latent representation, but the lagged architecture makes the momentum encoder become an unqualified “competitor” to the online encoder, which violates the findings of (Jiang et al., 2021). To address such conflict, we propose the Contrastive Sparsification Indicator (CSI), a simple-yeteffective method that automatically selects the starting point of sparsity increment based on the learning progress of SSL. During the SSL training, CSI first generates the pseudo pruning decisions of both encoders based on element-wise magnitude pruning with target sparsity sf\n\nθ and sf ξ :\n\nM∗\n\nM∗\n\nθ = 1{|wθ| ∈ TopK(|wθ|, sf ξ = 1{|wξ| ∈ TopK(|wξ|, sf\n\nθ )} ξ )}\n\n(13)\n\n(14)\n\nWhere 1 represents the indicator function, and the resultant pseudo masks of M∗ ξ will not be applied to the weights. Subsequently, CSI XORs the pseudo pruning masks to generate G (Eq. 15), and the percentage of “1”s in G is equivalent to the architecture inconsistency I (Eq. 16), where |G| represents the total number of element in G. Instead of using cosine similarity, the bit-wise XOR can be easily implemented on hardware to quantify the architecture difference during training. θ ⊕ M∗\n\nθ and M∗\n\n(15)\n\nG = M∗ I = 1 − (cid:0)(cid:88) 1{G = 0}(cid:1)/|G|\n\nξ\n\n(16)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Sparse BYOL training process (a) without SyncS and (b) with SyncS.\n\nTable 3: Performance comparison of BYOL on CIFAR-10 dataset with/without SyncS.\n\nMethod\n\nOnline Encoder Spars. Momentum Encoder Spars. Online Linear Eval. Acc. (%)\n\nCSI + SyncS\n\nCSI Only\n\n50%→80%\n\n50%→80%\n\n20%→50%\n\n20%→20% (Fixed)\n\n92.24%\n\n91.64%\n\nGiven the sparsity gap ∆s defined by SyncS, CSI activates the sparsity increment when I equals to ∆s, and this moment is defined as the CSI checkpoint. In other words, when the architecture difference between online and offline encoders is mainly caused by the sparsity difference, it is the optimal moment to start exploiting the in-training sparsity with the gradually-increased sparsity. With the ability to automatically select the starting point of sparsity increment, the proposed CSI method automatically sparsifies the model with the full awareness of the SSL process. For the SSL framework with shared encoder (Zbontar et al., 2021), the architecture inconsistency I is computed based on the sparse architecture of two consecutive epochs, and the sparsity increment is activated when I is less than a pre-defined threshold τ (e.g., τ = 0.1).\n\nFigure 3 shows the sparsification scheme with and without SyncS. As summarized in Table 3, holding the sparse momentum architecture after the CSI checkpoint interrupts the consistency between the online and momentum encoders. Although the momentum encoder retains the low sparsity at 20%, the absence of consistent asymmetry from synchronized contrastive pruning (SCP) causes the degraded model performance.\n\nOn top of the proposed SyncS and CSI schemes, we adopt the prune-and-regrow scheme (Liu et al., 2021) with modifications to exploit sparsity during SSL training. To further alleviate the contrastiveness oscillation caused by changing sparsity, we slowly average the gradient magnitude by exponential moving average (EMA) with gentle momentum, instead of using the instant score. The detailed pseudo code of the proposed algorithm is summarized in Appendix C.\n\n5 EXPERIMENTAL RESULTS\n\nIn this section, we validate the proposed sparse training scheme and compare it with the current SoTA sparse training schemes. Unlike the work by (Jiang et al., 2021), the proposed scheme exploits in-training sparsity in both contrastive paths (encoders) and aims to achieve energy-efficient self-supervised learning. The proposed method is applied to multiple mainstream SSL frameworks, including EMA-based methods (Chen et al., 2020b; Grill et al., 2020) and SSL with shared encoder (Zbontar et al., 2021). The linear evaluation accuracy and training cost reduction are reported for multiple datasets, including CIFAR-10, CIFAR-100, and ImageNet-2012. Furthermore, this work exploits in-training sparsity with various sparsity granularities, including element-wise sparsity, N :M sparsity (Zhou et al., 2020), and structural sparsity for a custom hardware accelerator.\n\nCIFAR-10 and CIFAR-100 Table 4 summarizes the linear evaluation accuracy of the proposed method on CIFAR-10 and CIFAR-100 datasets with element-wise sparsity. We use ResNet-18 (1×) as the backbone and train the model from scratch by 1,000 epochs. Following the typical high sparsity results reported with supervised learning, we report the model performance with 80% and 90% target sparsity. To sparsify both encoders during SSL training, we initialize the sparsity of online and offline (momentum) encoders as 30% and 0%, where the ∆s is set to 30%. The initialized sparse\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Linear evaluation comparison on CIFAR-10/100 datasets with element-wise sparsity.\n\nDataset\n\nEncoder\n\nCIFAR-10 Acc (%)\n\nResNet-18 (1×)\n\nCIFAR-100 Acc (%)\n\nResNet-18 (1×)\n\nElement-wise Sparsity\n\n0%\n\n80%\n\n90%\n\n0%\n\n80%\n\n90%\n\nThis work\n\n92.09\n\n91.77±0.08\n\n91.31±0.04\n\n67.72\n\n67.56±0.04\n\n66.78±0.07\n\nMoCo-V2 (Chen et al., 2020b)\n\nGraNet-MoCo (Liu et al., 2021)\n\nSD-MoCo (Jiang et al., 2021)\n\n92.09\n\n90.66±0.07\n\n90.05±0.08\n\n67.72\n\n67.17±0.05\n\n64.92±0.06\n\n92.09\n\n90.26±0.05\n\n87.68±0.06\n\n67.72\n\n65.04±0.04\n\n61.33±0.05\n\nThis work\n\n92.42\n\n92.26±0.06\n\n92.03±0.05\n\n68.80\n\n68.69±0.06\n\n67.73±0.04\n\nBYOL (Grill et al., 2020)\n\nGraNet-BYOL (Liu et al., 2021)\n\nSD-BYOL (Jiang et al., 2021)\n\n92.42\n\n91.20±0.02\n\n90.13±0.03\n\n68.80\n\n67.17±0.05\n\n65.85±0.08\n\n92.42\n\n90.33±0.07\n\n87.38±0.04\n\n68.80\n\n66.13±0.08\n\n62.20±0.10\n\nThis work\n\n91.74\n\n91.67±0.09\n\n90.84±0.07\n\n68.62\n\n68.75±0.13\n\n68.48±0.12\n\nBarlow Twins (Zbontar et al., 2021)\n\nGraNet-Barlow (Liu et al., 2021)\n\nSD-Barlow (Jiang et al., 2021)\n\n91.74\n\n91.23±0.03\n\n90.44±0.12\n\n68.62\n\n68.40±0.10\n\n68.15±0.14\n\n91.74\n\n90.09±0.03\n\n88.41±0.07\n\n68.62\n\n66.42±0.07\n\n61.77±0.04\n\nTable 5: ImageNet-2012 accuracy and training cost comparison with SoTA works on ResNet-50 with BYOL (Grill et al., 2020).\n\nImageNet\n\nTop-1 Accuracy (%)\n\nFLOPS (Training)\n\nFLOPS (Inference)\n\nTop-1 Accuracy (%)\n\nFLOPS (Training)\n\nFLOPS (Inference)\n\nDense Baseline\n\n68.12\n\n7.94e+18 (1×)\n\n8.18e+09 (1×)\n\n68.12\n\n7.94e+18 (1×)\n\n8.18e+09 (1×)\n\nElement-wise Sparsity\n\nModel Size (MB)\n\nThis work\n\nBYOL (Grill et al., 2020)\n\nGraNet-BYOL (Liu et al., 2021)\n\nSD-BYOL (Jiang et al., 2021)\n\n67.02\n\n65.45\n\n63.02\n\n80%\n\n20.5\n\n0.62×\n\n0.57×\n\n0.68×\n\n0.33 ×\n\n0.32 ×\n\n0.34 ×\n\n65.67\n\n64.22\n\n60.56\n\n90%\n\n10.2\n\n0.56×\n\n0.51×\n\n0.63×\n\n0.22×\n\n0.20×\n\n0.21×\n\nThe encoders are trained from scratch on the ImageNet dataset with 300 epochs. The FP32 dense ResNet-50 model requires 102MB storage.\n\nencoders reduce the overall memory footprint throughout the entire training process. We rigorously transfer the SoTA GraNet Liu et al. (2021) to SSL based on its open-sourced implementation, the proposed method outperforms GraNet-SSL with 1.26% and 1.86% accuracy improvements on CIFAR-10 and CIFAR-100 datasets, respectively. In all experiments, we report the average accuracy with its variation in 3 runs.\n\nIn addition to element-wise sparsity, the recent Nvidia Ampere architecture is equipped with the Sparse Tensor Cores to accelerate the inference computation on GPU with N :M structured finegrained sparsity (Zhou et al., 2020), where the N dense elements remain within an M -sized group. Powered by the open-sourced Nvidia-ASP library, SyncCP sparsifies BYOL training (Grill et al., 2020) by targeting 100% N :M sparse groups in online encoders. Starting from scratch, the percentage of the N :M sparse groups is initialized as 30% and 0% in online and momentum encoders with ∆s=30%. After the CSI checkpoint, the percentage of N :M groups gradually increases. Appendix A describes the detailed pruning algorithm of N :M sparsification. Table 6 summarizes linear evaluation accuracy and inference time reduction on the CIFAR-10 and CIFAR-100 datasets. The resultant model achieves up to 2.08× inference acceleration with minimum accuracy degradation. The inference time is measured on an Nvidia 3090 GPU with FP32 data precision.\n\nImageNet-2012 Since the BYOL (Grill et al., 2020) learning scheme achieves the best performance with CIFAR datasets, we further evaluate the proposed method with ResNet-50 on ImageNet based on the BYOL framework (Grill et al., 2020). Following the typical high sparsity results reported in Table 4, we report the model performance with 80% and 90% element-wise sparsity. The data augmentation setup is adopted from the open-sourced library (Costa et al., 2022). Starting from scratch, the model is trained by 300 epochs, where both online and momentum encoders are initialized by ERK with ∆s = 30%. While we believe a more fine-grained hyperparameter tuning and extended training efforts could lead to better accuracy, we choose the above scheme for simplicity and reproducibility. Table 5 shows the comparison of linear evaluation accuracy on ImageNet-2012\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Linear evaluation accuracy comparison on CIFAR-10/100 datasets with N :M structured fine-grained sparsity.\n\nDatasets\n\nEncoder\n\nCIFAR-10 Acc (%)\n\nCIFAR-100 Acc (%)\n\nResNet-18 (1×)\n\nResNet-18 (1×)\n\nN :M Sparse Pattern\n\n2:4\n\n1:4\n\n2:4\n\n1:4\n\nMoCo-V2 (Chen et al., 2020b)\n\nBYOL (Grill et al., 2020)\n\nBarlow Twins (Zbontar et al., 2021)\n\n91.99±0.07\n\n91.53±0.04\n\n67.58±0.05\n\n67.11±0.05\n\n92.61±0.05\n\n91.83±0.02\n\n68.69±0.02\n\n68.09±0.07\n\n91.68±0.04\n\n90.97±0.03\n\n68.26±0.07\n\n68.19±0.06\n\nInference time reduction (s)\n\nFLOPs (Dense = 5.56e+8)\n\nWeight Memory (MB)\n\n1.40×\n\n0.50×\n\n22.4\n\n2.08×\n\n0.25×\n\n11.2\n\n1.40×\n\n0.50×\n\n22.4\n\n2.08×\n\n0.25×\n\n11.2\n\nThe FP32 dense ResNet-18(1×) model requires 44.8MB storage and takes 1.27 seconds per 10K testset images during inference.\n\ndataset. Compared to the self-damaging scheme (Jiang et al., 2021) and GraNet (Liu et al., 2021), the proposed algorithm achieves the same highly-sparse network with 4.72% and 1.21% Top-1 inference accuracy improvements, respectively. GraNet exploits in-training sparsity throughout the entire training process, but the inconsistent contrastiveness hampers the model performance. On the other hand, the dense encoder limits the efficiency of the self-damaging scheme (Jiang et al., 2021) scheme, and the static high sparsity degrades the model performance. It has been shown that SSLtrained encoders are strong visual learners (Grill et al., 2020; Ericsson et al., 2021). Appendix A summarizes the performance of the proposed algorithm by fine-tuning the ImageNet-trained sparse encoders on CIFAR-10 and CIFAR-100 datasets. With only 300 epochs of sparse SSL training, the resultant sparse encoder outperforms the SoTA supervised sparse training algorithms.\n\nTable 7: Hardware training acceleration of the proposed structured SyncCP on CIFAR-10 datasaet.\n\nBYOL+ResNet-18\n\nTop-1 Accuracy (%)\n\nTraining Speed-up\n\nTop-1 Accuracy (%)\n\nTraining Speed-up\n\nDense Baseline\n\n92.42\n\n1×\n\n92.42\n\n1×\n\nTarget Structured Sparsity\n\n80%\n\n90%\n\nBYOL (Grill et al., 2020)\n\nThis work\n\n92.16\n\n1.74×\n\n91.77\n\n1.91×\n\nHardware-based Structured Pruning The hardware practicality of element-wise sparsification is often limited by the irregularity of fine-grained sparsity and index requirement. To that end, we employ structured sparsity based on group-wise EMA scores towards achieving actual hardware training acceleration. The encoders are initialized by ERK with 30% and 0% sparse groups while keeping ∆s = 30%. The structured sparsity starts to gradually increase after the CSI checkpoint. We adopt the training accelerator specifications from (Venkataramanaiah et al., 2022) and choose Kl (# of output channels) × Cl (# of input channels) = 8×8 as the sparse group size. Table 7 evalautes the training speedup of BYOL (Grill et al., 2020) aided by the structured sparse training. The proposed algorithm achieves up to 1.91× training acceleration with minimal accuracy degradation.\n\n6 CONCLUSION\n\nIn this paper, we propose a novel sparse training algorithm designed for self-supervised learning (SSL). As one of the first studies in this area, we first point out the imperfections of the sparsityinduced asymmetric self-supervised learning, as well as the incompatibility of the supervised sparse training algorithm in SSL. Based on the well-knit conclusions, we propose a contrastiveness-aware sparse training algorithm, consisting of synchronized contrastive pruning (SCP) and contrastive sparsification indicator (CSI). The proposed method outperforms the SoTA sparse training algorithm on both CIFAR and ImageNet-2012 datasets with various mainstream SSL frameworks. We also demonstrate the actual training and inference hardware acceleration with structured sparsity and N :M structured fine-grained pattern.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and Zhangyang Wang. The Lottery Tickets Hypothesis for Supervised and Self-supervised Pretraining in Computer Vision Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16306–16316, 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework In International Conference on Machine\n\nfor Contrastive Learning of Visual Representations. Learning (ICML), pp. 1597–1607. PMLR, 2020a.\n\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\n\nImproved Baselines with Momentum\n\nContrastive learning. arXiv preprint arXiv:2003.04297, 2020b.\n\nVictor Guilherme Turrisi Da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. Journal of Machine Learning Research, 23(56):1–6, 2022.\n\nTim Dettmers and Luke Zettlemoyer. Sparse Networks From Scratch: Faster Training without losing\n\nperformance. arXiv preprint arXiv:1907.04840, 2019.\n\nLinus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models transfer? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5414–5423, 2021.\n\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning (ICML), 2020.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 33:21271–21284, 2020.\n\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1735–1742, 2006.\n\nSong Han, Huizi Mao, and William J Dally. Deep Compression: Compressing Deep Neural NetInternational Conference on\n\nworks with Pruning, Trained Quantization and Huffman Coding. Learning Representations (ICLR), 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9729–9738, 2020.\n\nSara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. What do Com-\n\npressed Deep Neural Networks Forget? arXiv preprint arXiv:1911.05248, 2019.\n\nSiddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen. Top-kast: Topk always sparse training. Advances in Neural Information Processing Systems (NeurIPS), 33: 20744–20754, 2020.\n\nZiyu Jiang, Tianlong Chen, Bobak J Mortazavi, and Zhangyang Wang. Self-damaging Contrastive\n\nLearning. In International Conference on Machine Learning (ICML), pp. 4927–4939, 2021.\n\nAlexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting Self-supervised Visual Representation Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1920–1929, 2019.\n\nSimon Kornblith, Jonathon Shlens, and Quoc V Le. Do Better ImageNet Models Transfer Better? In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2661–2671, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2012.\n\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: Single-shot Network Pruning In International Conference on Learning Representations\n\nbased on Connection Sensitivity. (ICLR), 2018.\n\nShiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Sparse Training via Boosting Pruning Plasticity with Neuroregeneration. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\n\nJian Meng, Li Yang, Jinwoo Shin, Deliang Fan, and Jae-sun Seo. Contrastive Dual Gating: Learning Sparse Features With Contrastive Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12257–12265, 2022.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Pre-\n\ndictive Coding. arXiv preprint arXiv:1807.03748, 2018.\n\nSiyuan Pan, Yiming Qin, Tingyao Li, Xiaoshuang Li, and Liang Hou. Momentum Contrastive In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\n\nPruning. 2647–2656, 2022.\n\nShreyas K. Venkataramanaiah, Jian Meng, Han-Sok Suh, Injune Yeo, Jyotishman Saikia, Sai Kiran Cherupally, Yichi Zhang, Zhiru Zhang, and Jae sun Seo. A 28nm 8-bit Floating-Point Tensor Core based CNN Training Processor with Dynamic Activation/Weight Sparsification. In IEEE European Solid-State Circuits Conference (ESSCIRC), 2022.\n\nChaoqi Wang, Guodong Zhang, and Roger Grosse. Picking Winning Tickets Before Training by In International Conference on Learning Representations (ICLR),\n\nPreserving Gradient Flow. 2019.\n\nYang You, Igor Gitman, and Boris Ginsburg. Large Batch Training of Convolutional Networks.\n\narXiv preprint arXiv:1708.03888, 2017.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St ́ephane Deny. Barlow Twins: Self-supervised Learning via Redundancy Reduction. In International Conference on Machine Learning (ICML), pp. 12310–12320, 2021.\n\nAojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch. In International Conference on Learning Representations (ICLR), 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA DOWNSTREAM TASKS PERFORMANCE WITH THE PRE-TRAINED SPARSE\n\nENCODER\n\nThe pre-trained sparse encoder can be used for downstream tasks. We verified the performance sparse ImageNet-trained model in Table 5 with the downstream tasks. The following table summarizes the transfer learning performance by fine-tuning the ImageNet-trained sparse BYOL encoder on CIFAR-10 and CIFAR-100 datasets.\n\nTable 8: Transfer learning performance of the ImageNet-trained BYOL encoder on CIFAR-10 and CIFAR-100 datasets.\n\nDataset\n\nEncoder\n\nCIFAR-10 Acc (%) CIFAR-100 Acc. (%)\n\nResNet-50 (1×)\n\nResNet-50 (1×)\n\nElement-wise Sparsity\n\n0%\n\n90%\n\n0%\n\n90%\n\nSNIP (Lee et al., 2018)\n\nGraSP (Wang et al., 2019)\n\nRigL (Evci et al., 2020)\n\nGraNet (Liu et al., 2021)\n\n94.75\n\n92.65\n\n78.23\n\n73.14\n\n94.75\n\n92.47\n\n78.23\n\n73.28\n\n94.75\n\n94.45\n\n78.23\n\n76.50\n\n94.75\n\n94.64\n\n78.23\n\n77.89\n\nThis work\n\n96.09\n\n95.72\n\n80.13\n\n79.48\n\nSupervised Training\n\nTransfer learning from BYOL-trained model\n\nThe pre-trained sparse encoder are consistently preserved during the downstream fine-tuning. As shown in the table above, the strong visual representation learners are obtained by training the backbone model with self-supervised learning on ImageNet dataset. Sparsified by the proposed algorithm on ImageNet with 300 epochs during pre-training, fine-tuning the SSL-trained backbone model leads to 1.08% and 1.59% accuracy improvements compared to the recent SoTA supervised pruning algorithms.\n\nB THE IMPACT OF ∆s\n\nWe evaluate the impact of ∆s with different sparsification schemes on CIFAR-10 dataset with BYOL SSL framework. With the target sparsity = 90%, the following table summarizes the accuracy and training cost reduction with different ∆s values associated with different initial and final density.\n\nTable 9: The linear evaluation accuracy and the training FLOPs reduction with sparsity gap ∆s.\n\n∆s Online Encoder Spars. Offline Encoder Spars. Online Linear Eval Acc.\n\n50%\n\n40%\n\n30%\n\n40%\n\n30%\n\n50% → 90%\n\n50% → 90%\n\n50% → 90%\n\n40% → 90%\n\n30% → 90%\n\n0% → 40%\n\n10% → 50%\n\n20% → 60%\n\n0% → 50%\n\n0% → 60%\n\n91.68\n\n91.41\n\n91.27\n\n91.79\n\n92.02\n\nFLOPs (Training)\n\n0.60×\n\n0.58×\n\n0.53×\n\n0.57×\n\n0.56×\n\nGiven the fixed target sparsity = 90%, it is easy to tell that the higher ∆s leads to denser momentum encoder and less computation reduction. Meanwhile, sparsifies the momentum encoder at the beginning of training is sub-optimal. Furthermore, less initial sparsity and smaller ∆s value (4th row) achieves the best tradeoff between computation reduction and model performance.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nC APPENDIX C\n\nC.1 PSEUDO CODE OF SYNCCP WITH ELEMENT-WISE SPARSITY\n\nAlgorithm 1: Synchornized Contrastive Pruning (SyncCP) Initialize Sparse online encoder fθ, Sparse offline encoder fξ, EMA updater, Momentum γ, SyncS density gap ∆s, CSI threshold τ (Default=∆s). ξ, such that |s∗ Initial sparsity s0 θ, s0 θ, s∗ Target sparsity s∗ ξ, such that |s∗ Initial mask M0 θ, M0 ξ. Pruner udpate frequency Φ while t < Total Iterations do\n\nξ| = ∆s ξ| = ∆s\n\nθ − s∗ θ − s∗\n\nDraw augmented data (X, X ′); Forward pass: online encoding = fθ(Mθ · θ, X) ; Forward pass: offline encoding = fξ(Mξ · ξ, X ′); Update Exponential Moving Average (EMA) gradient score based on Eq. 17 ; if End Epoch then\n\nGet pseudo masks M∗ Compute layer-wise G and I based on Eq. 15 and Eq. 16; if I = ∆s then\n\nξ based on magnitude pruning;\n\nθ and M∗\n\nPrune=True\n\nend\n\nend if t % Φ = 0 then\n\nif Prune=True then\n\nelse\n\nend\n\nend\n\nξ based on Eq. 10 and Eq. 11;\n\nθ, st\n\nUpdate sparsity st Maintain the SyncS constraint ∆s; Inside fθ and fξ, prune st θ, and st Prune extra rt\n\nθ elements of the unpruned elements, then regrow rθ elements with\n\nξ elements with least magnitude score;\n\nhights EMA-gradient score; θ, Mt\n\nUpdate Mt\n\nξ based on Eq. 3 and Eq. 4;\n\nC.2 EMA-BASED PRUNE AND REGROW\n\nAs aforementioned, the findings of Observation 2 implies the incompatibility of the instant gradient and magnitude score. Together with the proposed SyncS and CSI methods, weight importance is measured by the magnitude score, while the sensitivity of the model is quantified by the gently averaged gradient magnitude with EMA:\n\n ̄gt = γ × ̄gt−1 + (1 − γ) × |g|t\n\n(17)\n\nTable 10 summarizes the linear evaluation accuracy of ResNet-18 trained by BYOL (Grill et al., 2020). We initialize s0 ξ as 40% and 10%, where the ∆s is set to 30%, the EMA momentum is set to 0.1 for gentle gradient score averaging.\n\nθ and s0\n\nMetric\n\nPrune Regrow EMA Online Linear Eval. Acc. (%)\n\nThis work\n\nPrune-and-regrow\n\nMagnitude Pruning\n\n✓\n\n✓\n\n✓\n\n✓\n\n✓\n\n✗\n\n✓\n\n✗\n\n✗\n\n91.88\n\n91.52\n\n90.99\n\nTable 10: Peformance comparison between different sparsification metrics.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nC.3 PSEUDO CODE OF SYNCCP WITH N :M SPARSITY\n\nAlgorithm 2: Synchornized Contrastive Pruning (SyncCP) with N :M Sparsity Initialize Sparse online encoder fθ, Sparse offline encoder fξ, EMA updater, Momentum γ, SyncS density gap ∆s, CSI threshold τ (Default=∆s). Group size M , Number of dense element per group N . Initial percentage p0 Such that |p0 θ = 100%, p∗ Target percentage p∗ Initial mask M0 θ, M0 ξ. Pruner udpate frequency Φ. while t < Total Iterations do\n\nθ of N :M groups in fθ, Initial percentage p0\n\nξ of N :M groups in fξ;\n\nξ| = ∆s;\n\nθ − ∆s;\n\nξ = p∗\n\nθ − p0\n\nDraw augmented data (X, X ′); Forward pass: online encoding = fθ(Mθ · θ, X) ; Forward pass: offline encoding = fξ(Mξ · ξ, X ′); Update Exponential Moving Average (EMA) weight gradient score based on Eq. 17 ; if End Epoch then\n\nGet pseudo masks M∗ Compute layer-wise G and I based on Eq. 15 and Eq. 16; if I = ∆s then\n\nξ based on magnitude pruning;\n\nθ and M∗\n\nPrune=True\n\nend\n\nend if t % Φ = 0 then\n\nif Prune=True then\n\nelse\n\nend\n\nend\n\nθ, pt\n\nξ based on Eq. 10 and Eq. 11;\n\nUpdate sparsity pt Maintain the SyncS constraint pt Inside fθ and fξ, pick pt θ, and pt Inside each group, prune the N-M elements with smallest magnitude score; Update Mt\n\nθ, pt ξ M-sized groups with least sum of magnitude score;\n\nθ − ∆s;\n\nξ = pt\n\nξ based on Figure 4;\n\nθ, Mt\n\nFigure 4: Group-wise (a) prune and (b) regrow algorithm based on EMA gradient score. SyncCP sparsifies M − N elements inside each group, while keep pt N :M groups inside f .\n\n14\n\n(a)(b)Under review as a conference paper at ICLR 2023\n\nC.4 PSEUDO CODE OF SYNCCP WITH STRUCTURED SPARSITY\n\nθ of sparse groups in fθ, Initial percentage p0\n\nAlgorithm 3: Synchornized Contrastive Pruning (SyncCP) with Structured Sparsity Initialize Sparse online encoder fθ, Sparse offline encoder fξ, EMA updater, Momentum γ, SyncS density gap ∆s, CSI threshold τ (Default=∆s). Group size Kl (# of output channels) × Cl (# of input channels) = g × g. Initial percentage p0 θ − p∗ Such that |p∗ Initial structured sparsity s0 Target structured sparsity s∗ Initial mask M0 Pruner udpate frequency Φ. while t < Total Iterations do\n\nξ, such that |s∗ ξ, such that |s∗\n\nξ of sparse groups in fξ;\n\nξ| = ∆s ξ| = ∆s\n\nθ − s∗ θ − s∗\n\nθ, s0 θ, s∗\n\nξ| = ∆s;\n\nθ, M0 ξ.\n\nDraw augmented data (X, X ′); Forward pass: online encoding = fθ(Mθ · θ, X) ; Forward pass: offline encoding = fξ(Mξ · ξ, X ′); Update Exponential Moving Average (EMA) gradient score based on Eq. 17; if End Epoch then\n\nGet pseudo masks M∗ Compute layer-wise G and I based on Eq. 15 and Eq. 16; if I = ∆s then\n\nξ based on magnitude pruning;\n\nθ and M∗\n\nPrune=True\n\nend\n\nend if t % Φ = 0 then\n\nif Prune=True then\n\nUpdate structured sparsity st θ, st Maintain the SyncS constraint st Inside fθ and fξ, pick st θ, and st Outside the sparsified groups of fθ, prune rt\n\nξ based on Eq. 10 and Eq. 11; θ, st ξ groups with least sum of magnitude score; θ more groups with least sum of\n\nθ − ∆s;\n\nξ = st\n\nAmong the sparsified groups fθ, regrow the rt\n\nθ groups back with highest sum of\n\nmagnitude score;\n\nEMA gradient score;\n\nUpdate Mt\n\nθ, Mt ξ;\n\nend\n\nend\n\nend\n\nD SPARSIFYING ONLINE OR OFFLINE ENCODERS\n\nWe mirror the experiment of Table 1 by exploiting high sparsity in the momentum encoder while keeping the online encoder dense:\n\nTable 11: Largely degraded performance of MoCo-V2 with self-damaging SSL on CIFAR-10 dataset.\n\nResNet-18\n\nEncoder\n\nFixed Sparsity\n\nDense Model Acc. = 92.09%\n\nOnline\n\n90%\n\nMomentum\n\n0%\n\nOnline\n\n50%\n\nMomentum\n\n0%\n\nLinear Eval. Acc (%)\n\n88.72 (-3.41%)\n\n87.68 (-4.31%)\n\n92.10 (+0.01%)\n\n92.07 (-0.02%)\n\nFixed Sparsity\n\n0%\n\n90%\n\nLinear Eval. Acc (%)\n\n88.44 (-3.65%)\n\n87.52 (-4.57%)\n\n0%\n\n–\n\n50%\n\n–\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nWith the proposed CSI + SyncS sparsification method, we empirically observe that exploiting high sparsity in online model leads to better performance with gradient-based model update. Table 12 summarizes the comparison results of BYOL on CIFAR-10 dataset with the proposed algorithm.\n\nTable 12: Performance comparison of exploiting higher sparsity in online or momentum encoders.\n\nMethod\n\nOnline Encoder Spars. Momentum Encoder Spars. Online Linear Eval. Acc. (%)\n\nCSI + SyncS\n\nCSI + SyncS\n\n0%→50%\n\n30%→80%\n\n30%→80%\n\n0%→50%\n\n91.88%\n\n92.24%\n\nOverall, sparsifying the online encoder leads to the optimal performance.\n\nE DETAILED EXPERIMENTAL SETUP OF SYNCCP\n\nE.1 LINEAR EVALUATION PROTOCOL\n\nAs in (Kolesnikov et al., 2019; Kornblith et al., 2019; Chen et al., 2020a), we use the standard linear evaluation protocol on CIFAR-10/100 and ImageNet-2012 datasets, which training a linear classifier on top of the frozen SSL-trained encoder. During linear evaluation, we apply spatial augmentation and random flips. The linear classifier is optimized by SGD with cross-entropy loss.\n\nE.2 CIFAR-10/100 EXPERIMENTS\n\nThe training hyper-parameters of the compared individual sparse training works are same for CIFAR-10 and CIFAR-100. We provide the detailed training setup of different self-supervised learning frameworks as follow:\n\nMoCo-V2 The ResNet-18 (×) encoder is trained by MoCo-V2 (Chen et al., 2020b) from scratch by 1,000 epochs with SGD optimizer and 256 batch size. The learning rate is set to 0.3 with Cosine learning rate decay and 10 epochs warmup. The detailed data augmentation is summarized in Table 13.\n\nParameter\n\nX\n\nX ′\n\nRandom crop size\n\n32 × 32\n\n32 × 32\n\nHorizontal flip probability\n\nColor jitter probability\n\nBrightness adjustment probability\n\nContrast adjustment probability\n\nSaturation adjustment probability\n\nHue adjustment probability\n\nGaussian blurring probability\n\nSolarization probability\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.0\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.0\n\nTable 13: Detailed image augmentation settings for MoCo-V2 (Chen et al., 2020b) on CIFAR10/100.\n\nBYOL The ResNet-18 (×) encoder is trained by BYOL (Grill et al., 2020) from scratch by 1,000 epochs with LARS-SGD optimizer (You et al., 2017). The predictor is constructed with 4096 hidden features and 256 output dimension. We use 256 batch size along with 1.0 learning rate. The Cosine learning rate scheduler is used with 10 epochs warmup training. The detailed data augmentation is summarized in Table 14.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nParameter\n\nX\n\nX ′\n\nRandom crop size\n\n32 × 32\n\n32 × 32\n\nHorizontal flip probability\n\nColor jitter probability\n\nBrightness adjustment probability\n\nContrast adjustment probability\n\nSaturation adjustment probability\n\nHue adjustment probability\n\nGaussian blurring probability\n\nSolarization probability\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.0\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.2\n\nTable 14: Detailed image augmentation settings for BYOL (Chen et al., 2020b) on CIFAR-10/100.\n\nBarlow Twins The ResNet-18 (×) encoder is trained by Barlow Twins (Zbontar et al., 2021) from scratch by 1,000 epochs with LARS-SGD optimizer (You et al., 2017). We use 256 batch size along with 0.3 learning rate and 1e − 4 weight decay. The Cosine learning rate scheduler is used with 10 epochs warmup training. The detailed data augmentation is summarized in Table 15.\n\nParameter\n\nX\n\nX ′\n\nRandom crop size\n\n32 × 32\n\n32 × 32\n\nHorizontal flip probability\n\nColor jitter probability\n\nBrightness adjustment probability\n\nContrast adjustment probability\n\nSaturation adjustment probability\n\nHue adjustment probability\n\nGaussian blurring probability\n\nSolarization probability\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.0\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.0\n\n0.2\n\nTable 15: Detailed image augmentation settings for Barlow Twins (Zbontar et al., 2021) on CIFAR10/100.\n\nE.3\n\nIMAGENET EXPERIMENTS\n\nStarting from scratch, the proposed SyncCP algorithm exploits in-training sparsity with the BYOL framework (Grill et al., 2020) on ImageNet-2012 dataset. The ResNet-50 encoder is trained by LARS-SGD (You et al., 2017) with 0.45 learning rate and a momentum of 0.9. We uses 0.1 for the for EMA-averaged gradient score. We use 128 batch size along with 1e-6 weight decay. The detailed image augmentations are summarized in Table 16.\n\nParameter\n\nX\n\nX ′\n\nRandom crop size\n\n224 × 224\n\n224 × 224\n\nHorizontal flip probability\n\nColor jitter probability\n\nBrightness adjustment probability\n\nContrast adjustment probability\n\nSaturation adjustment probability\n\nHue adjustment probability\n\nGaussian blurring probability\n\nSolarization probability\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n1.0\n\n0.0\n\n0.5\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.1\n\n0.1\n\n0.2\n\nTable 16: Detailed image augmentation settings for BYOL (Grill et al., 2020) on ImageNet.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nF COMPUTATION REDUCTION WITH DIFFERENT SPARSITY VALUES\n\nTable 17: ImageNet-2012 linear evaluation accuracy and training cost comparison with different sparsity on ResNet-50 with BYOL (Grill et al., 2020).\n\nDataset\n\nSparsity\n\n0%\n\n50%\n\n80%\n\n90%\n\nImageNet-2012\n\nTop-1 Accuracy (%)\n\nFLOPs (Training)\n\nFLOPs (Inference)\n\n68.12\n\n68.31\n\n67.02\n\n65.67\n\n7.94e+18 (1×)\n\n8.18e+09 (1×)\n\n0.78×\n\n0.62×\n\n0.56×\n\n0.68×\n\n0.33×\n\n0.22×\n\n18",
    "reference": "# Summary Of The Paper\n\nThis paper enables model pruning to accelearate contrastive self-supervised training. Compared to the exsited method, this paper tries to sparsify online decoder and offline decoder simultaneously. To stabilize the sparse training. The authors propose Contrastive Sparsification Indicator (CSI) to guide the model pruning.\n\nTo evaluate the effectiveness of SyncCP, the authors conduct experiments on various classification datasets with unstructured sparsity, N:M sparsity, and structured pruning.\n\n# Strength And Weaknesses\n\nStrength:\n\nThe paper is well written.\n\nThe experiments including various different sparse types (unstructured, N:M, structured) are persuasive.\n\nThe performance (accuracy and efficiency) improvement is significant compared to the existing method [1].\n\nWeaknesses:\n\nThe self-supervised learning method usually trains on large-scale datasets, but this paper doesn't show any training GPU hours improvement on large-scale datasets. I feel confused about the motivation.\n\nThe NVidia GPUs only support 2:4 sparsity, can you explain the detail implementation of inference gain about 1:4 sparsity in Table5.  \n\nThe BYOL and MOCO-v2 are not SOTA SSL methods, the recent SOTA methods are more convincing.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nNa\n\n# Summary Of The Review\n\nNa\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nVALID P -VALUE FOR DEEP LEARNING-DRIVEN SALIENT REGION\n\nDaiki Miwa⇤ Nagoya Institute of Technology miwa.daiki.mllab.nit@gmail.com\n\nVo Nguyen Le Duy⇤ RIKEN duy.vo@riken.jp\n\nIchiro Takeuchi† Nagoya University and RIKEN ichiro.takeuchi@mae.nagoya-u.ac.jp\n\nABSTRACT\n\nVarious saliency map methods have been proposed to interpret and explain predictions of deep learning models. Saliency maps allow us to interpret which parts of the input signals have a strong influence on the prediction results. However, since a saliency map is obtained by complex computations in deep learning models, it is often difficult to know how reliable the saliency map itself is. In this study, we propose a method to quantify the reliability of a salient region in the form of p-values. Our idea is to consider a salient region as a selected hypothesis by the trained deep learning model and employ the selective inference framework. The proposed method can provably control the probability of false positive detections of salient regions. We demonstrate the validity of the proposed method through numerical examples in synthetic and real datasets. Furthermore, we develop a Keras-based framework for conducting the proposed selective inference for a wide class of CNNs without additional implementation cost.\n\n1\n\nINTRODUCTION\n\nDeep neural networks (DNNs) have exhibited remarkable predictive performance in numerous practical applications in various domains owing to their ability to automatically discover the representations needed for prediction tasks from the provided data. To ensure that the decision-making process of DNNs is transparent and easy to understand, it is crucial to effectively explain and interpret DNN representations. For example, in image classification tasks, obtaining salient regions allows us to explain which parts of the input image strongly influence the classification results.\n\nSeveral saliency map methods have been proposed to explain and interpret the predictions of DNN models (Ribeiro et al., 2016; Bach et al., 2015; Doshi-Velez & Kim, 2017; Lundberg & Lee, 2017; Zhou et al., 2016; Selvaraju et al., 2017). However, the results obtained from saliency methods are fragile (Kindermans et al., 2017; Ghorbani et al., 2019; Melis & Jaakkola, 2018; Zhang et al., 2020; Dombrowski et al., 2019; Heo et al., 2019). Therefore, it is important to develop a method for quantifying the reliability of DNN-driven salient regions.\n\nOur idea is to interpret salient regions as hypotheses driven by a trained DNN model and employ a statistical hypothesis testing framework. We use the p-value as a criterion to quantify the statistical reliability of the DNN-driven hypotheses. Unfortunately, constructing a valid statistical test for DNN-driven salient regions is challenging because of the selection bias. In other words, because the trained DNN selects the salient region based on the provided data, the post-selection assessment of importance is biased upwards.\n\nTo correct the selection bias and compute valid p-values for DNN-driven salient regions, we introduce a conditional selective inference (SI) approach. The selection bias is corrected by conditional\n\n⇤Equal contribution †Corresponding author\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nInput Image\n\nSaliency Map\n\nSalient Region\n\nReference Image\n\nTwo-sample Test\n\n(a) Image without tumor region. The naive-p = 0.00 (wrong detection) and selective-p = 0.43 (true negative)\n\nInput Image\n\nSaliency Map\n\nSalient Region\n\nReference Image\n\nTwo-sample Test\n\n(b) Image with tumor region. The naive-p = 0.00 (true positive) and selective-p = 0.00 (true positive)\n\nFigure 1: Examples of the problem setup and the proposed method on the brain tumor dataset. By applying a saliency method called CAM (Zhou et al., 2016) on a query input image, we obtain the salient region. Our goal is to provide the statistical significance of the salient region in the form of p-value by considering two-sample test between the salient region and the corresponding region in the reference image. Note that, since the salient region is selected based on the data, the degree of saliency in the selected region is biased upward. In the upper image where there is no true brain tumor, the naive p-value which is obtained without caring about the selection bias is nearly zero, indicating the false positive finding of the salient region. On the other hand, the selective p-value which is obtained by the proposed conditional SI approach is 0.43, indicating that the selected saliency region is not statistically significant. In the lower image where there is a true brain tumor, both the naive p-value and the selective p-value are very small, which indicate a true positive finding. These results illustrate that naive p-value cannot be used to quantify the reliability of DNN-based salient region. In contrast, with the selective p-values, we can successfully identify false positive and true positive detections with a desired error rate.\n\nSI in which the test statistic conditional on the event that the hypotheses (salient regions) are selected using the trained DNNs is considered. Our main technical contribution is to develop a method for explicitly deriving the exact (non-asymptotic) conditional sampling distribution of the salient region for a wide class convolutional neural networks (CNNs), which enables us to conduct conditional SI and compute valid p-values. Figure 1 presents an example of the problem setup.\n\nRelated works. In this study, we focus on statistical hypothesis testing for post-hoc analysis, i.e., quantifying the statistical significance of the salient regions identified in a trained DNN model when a test input instance is fed into the model. Several methods have been developed to visualize and understand trained DNNs. Many of these post-hoc approaches (Mahendran & Vedaldi, 2015; Zeiler & Fergus, 2014; Dosovitskiy & Brox, 2016; Simonyan et al., 2013) have focused on developing visualization tools for saliency maps given a trained DNN. Other methods have aimed to identify the discriminative regions in an input image given a trained network (Selvaraju et al., 2017; Fong & Vedaldi, 2017; Zhou et al., 2016; Lundberg & Lee, 2017). However, some recent studies have shown that many of these saliency methods of these saliency methods are not stable against a perturbation or adversarial attack on the input data and model (Kindermans et al., 2017; Ghorbani et al., 2019; Melis & Jaakkola, 2018; Zhang et al., 2020; Dombrowski et al., 2019; Heo et al., 2019). To the best of our knowledge, no study to date has succeeded in quantitatively evaluating the reproducibility of DNN-driven salient regions with a rigorous statistical inference framework.\n\nIn recent years, conditional SI has emerged as a promising approach for evaluating the statistical reliability of data-driven hypotheses. It has been actively studied for making inferences on the features of linear models selected by various feature selection methods, such as Lasso (Lee et al., 2016). The main concept behind conditional SI is to make inference based on the sampling distribution of the test statistic conditional on a selection event. This approach allows us to derive the exact sampling distribution of the test statistic. After the seminal work of Lee et al. (2016), conditional SI has also\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nbeen applied to a wide range of problems (Loftus, 2015; Choi et al., 2017; Tian & Taylor, 2018; Yang et al., 2016; Tibshirani et al., 2016; Fithian et al., 2014; Loftus & Taylor, 2014; Panigrahi et al., 2016; Sugiyama et al., 2021a; Hyun et al., 2021; Duy & Takeuchi, 2021a;b; Sugiyama et al., 2021b; Chen & Bien, 2019; Tsukurimichi et al., 2021; Tanizaki et al., 2020; Duy et al., 2020; 2022).\n\nThe most relevant existing work is Duy et al. (2022), where the authors provide a framework to compute valid p-values for DNN-based image segmentation results. In Duy et al. (2022), the authors only considered the inference on the output of a DNN in a segmentation task. In this paper, we address a more general problem in which the hypotheses characterized by any internal nodes of the DNN can be considered. This enables us to quantify the statistical significance of salient regions. Furthermore, we introduce a Keras-based implementation framework that enables us to conduct SI for a wide class of CNNs without additional implementation costs. This is in contrast to Duy et al. (2022) in which the selection event must be implemented when the network architecture is changed.\n\nIn another direction, Burns et al. (2020) considered the black box model interpretability as a multiple hypothesis testing problem. Their goal was to identify important features by testing the significance of the difference between the prediction and the expected outcome when certain features are replaced with their counterfactuals. However, this approach faces a significant challenge: the number of hypotheses to be considered can be very large (e.g., in the case of an image with n pixels, the number of possible salient regions is 2n). Multiple testing correction methods, such as the Bonferroni correction, are highly conservative when the number of hypotheses is large. To address the challenge, they only considered a tractable number of regions selected by a human expert or object detector, which causes selection bias because the candidate regions are selected based on the data.\n\nContribution. Our main contributions are as follows:\n\nWe provide an exact (non-asymptotic) inference method for salient regions obtained by CAM •\nbased on the SI concept. We introduce valid p-values to statistically quantify the reliability of the DNN-driven salient regions inspired by Duy et al. (2022).\n\nWe propose a novel algorithm and its implementation. Specifically, we propose Keras-based im- •\nplementation which enables us to conduct conditional SI for a wide class of CNNs without additional implementation costs.\n\nWe conducted experiments on synthetic and real-world datasets, through which we show that our •\nproposed method can control the false positive rate, has good performance in terms of computational efficiency, and provides good results in practical applications. Our code is available at\n\nhttps://github.com/takeuchi-lab/selective inference dnn salient region.\n\n2 PROBLEM FORMULATION\n\nIn this paper, we consider the problem of quantifying the statistical significance of the salient regions identified by a trained DNN model when a test input instance is fed into the model. Consider an n-dimensional query input vector\n\nand an n-dimensional reference input vector,\n\nX = (X1, ..., Xn)> = s + \", \"\n\nN(0, 2In)\n\n⇠\n\nX ref = (X ref\n\n1 , ..., X ref\n\nn )> = sref + \"ref , \"ref\n\nN(0, 2In),\n\n⇠\n\n2\n\nRn are the signals and \", \"ref\n\nRn are the noises for query and reference input where s, sref 2\nvectors, respectively. We assume that the signals, s and sref are unknown, whereas the distribution of noises \" and \"ref are known (or can be estimated from external independent data) to follow N(0, 2In), an n-dimensional normal distribution with a mean vector 0 and covariance matrix 2In, which are mutually independent. In the illustrative example presented in §1, X is a query brain image for a potential patient (we do not know whether she/he has a brain tumor), whereas X ref is a brain image of a healthy person without brain tumors.\n\nConsider a saliency method for a trained CNN. We denote the saliency method as a function Rn\n\nRn that takes a query input vector X\n\nRn and returns the saliency map\n\n: A\nRn. We\n\n(X)\n\nA\n\n2\n\n!\n\n2\n\n3\n\nPublished as a conference paper at ICLR 2023\n\ndefine a salient region value is greater than a threshold\n\nMX for the query input vector X as the set of elements whose saliency map\n\nAi(X) where ⌧ R denotes the given threshold. In this study, we consider CAM (Zhou et al., 2016) as an example of saliency method and threshold-based definition of the salient region. Our method can be applied to other saliency methods and other definitions of salient region.\n\nMX =\n\n[n] :\n\n(1)\n\n2\n\n2\n\n{\n\n}\n\n⌧\n\ni\n\n,\n\nStatistical inference. To quantify the statistical significance of the saliency region sider a two-sample test for the difference between the salient regions of the query input vector X and corresponding region of the reference input vector X ref indexed by X. As examples of the two-sample test, we consider the mean null test:\n\nMX , we conMX is a sub-vector of X\n\nMX where X\n\nMX\n\n1\n\nH0 :\n\n|MX | Xi\n\n2MX and global null test:\n\nsi =\n\n1\n\n|MX | Xi\n\n2MX\n\nsref\n\ni\n\nv.s. H1 :\n\n1\n\n|MX | Xi\n\n2MX\n\n=\n\nsi 6\n\n1\n\n|MX | Xi\n\n2MX\n\nsref\n\ni\n\n, (2)\n\nH0 : si = sref\n\ni\n\n,\n\ni\n\n2M X ,\n\nv.s. H1 : si 6 In the mean null test in Eq. (2), we consider a null hypothesis that the average signals in the salient MX are the same between X and X ref . In contrast, in the global null test in Eq. (3), we region consider a null hypothesis that all elements of the signals in the salient region MX are the same between X and X ref . The p-values for these two-sample tests can be used to quantify the statistical significance of the salient region\n\n2M X .\n\n(3)\n\n8\n\n9\n\ni\n\n,\n\ni\n\n= sref\n\nMX .\n\nTest-statistic. For a two-sample test conducted between X test statistics called conditionally linear test-statistic, which is expressed as\n\nMX and X ref\n\nMX , we consider a class of\n\nand conditionally  test-statistic, which is expressed as\n\nT (X, X ref ) = ⌘>\n\nMX\n\nX X ref\n\n,\n\n◆\n\n✓\n\nT (X, X ref ) = \n\n1\n\nP\n\nX X ref\n\nMX\n\n(4)\n\n(5)\n\n,\n\n \n2n is a projection matrix that depends on where ⌘ \n⇥ \nThe test statistics for the mean null tests and the global null test can be written in the form of Eqs. (4) and (5), respectively. For the mean null test in Eq. (2), we consider the following test-statistic\n\nR2n is a vector and P\n\nMX .\n\n◆ \n \n\nMX 2\n\nMX 2\n\nR2n\n\n✓\n\nT (X, X ref ) = ⌘>\n\nMX\n\nwhere ⌘\n\nMX = 1\n\nbelongs to the set following test-statistic\n\n|MX | ✓ C\n\n1n MX 1n MX ◆\n\nX X ref\n\n✓\n\n=\n\n◆\n\n1\n\n|MX | Xi\n\n2MX\n\nXi \n\n1\n\n|MX | Xi\n\n2MX\n\nX ref\n\ni\n\n,\n\nR2n with 1n\n\nC\n\n2\n\nbeing the n-dimensional vector whose elements\n\nare set to 1, and 0 otherwise. For the global null test in Eq. (3), we consider the\n\nT (X, X ref ) = \n\n1\n\nP\n\nX X ref\n\nMX\n\n✓\n\n \n diag(1n \n diag(1n\n\n=\n\nv u\nu t\n\ndiag(1n diag(1n\n\n)\n\nMX = 1\n\nwhere P\n\n2\n\nMX\n\nMX )\nwe need to know the sampling distribution of the test-statistics. Unfortunately, it is challenging to derive the sampling distributions of test-statistics because they depend on the salient region which is obtained through a complicated calculation in the trained CNN.\n\nMX ,\n\nMX\n\nMX\n\n✓\n\n◆\n\n)\n\n2MX ✓ Xi\n\n◆ \n \n. To obtain p-values for these two-sample tests\n\n◆\n\n)\n\nXi \n\nX ref i\n\np2\n\n2\n\n,\n\n3 COMPUTING VALID p-VALUE BY CONDITIONAL SELECTIVE INFERENCE\n\nIn this section, we introduce an approach to compute the valid p-values for the two-sample tests for the salient region on the concept of conditional SI (Lee et al., 2016).\n\nMX between the query input vector X and the reference input vector X ref based\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n3.1 CONDITIONAL DISTRIBUTION AND SELECTIVE p-VALUE\n\nConditional distribution. The basic idea of conditional SI is to consider the sampling distribution of the test-statistic conditional on a selection event. Specifically, we consider the sampling property of the following conditional distribution\n\nT (X, X ref)\n\n{MX =\n\nMXobs }\n\n,\n\n(6)\n\nwhere Xobs is the observation (realization) of random vector X. The condition in Eq.(6) indicates the randomness of X conditional on the event that the same salient region MX as the observed MX , derivation of the sampling distriMX obs is obtained. By conditioning on the salient region bution of the conditionally linear and  test-statistic T (X, X ref ) is reduced to a derivation of the distribution of linear function and quadratic function of (X, X ref ), respectively.\n\n \n\nSelective p-value. After considering the conditional sampling distribution in (6), we introduce the following selective p-value:\n\npselective = PH0\n\nT (X, X ref )\n\nT (Xobs, X ref\n\nobs)\n\nwhere\n\n⇣  \n\nMX =\n\nMXobs ,\n\nQX,X ref =\n\nQobs\n\n,\n\n(7)\n\n⌘\n\n \n\n \n\n \n\n \n\nQX,X ref =⌦ X,X ref , MX ⌘> ⌘\nMX k k\n\nX X ref\n\nMX 2\n\n2\n\n⌘\n\n◆\n\nVX,X ref ,\n\n UX,X ref\n\n,\n\nwith ⌦X,X ref =\n\n✓\n\nI2n  QX,X ref =\n\nQobs =\n\nQXobs,X ref R2n in the case of mean null test, and\n\nobs\n\nwith\n\nVX,X ref = P\n\nR2n in the QX,X ref is the sufficient statistic of the nuisance parameter that needs case of global null test. The . \nto be conditioned on in order to tractably conduct the inference 1. \n\nR2n,\n\n \n\nMX\n\nMX\n\nMX\n\n2\n\n2\n\nP\n\nX X ref\n\nX X ref\n\nX X ref\n\nThe selective p-value in Eq.(7) has the following desired sampling property\n\nPH0\n\npselective \n\n↵\n\n| MX =\n\nMXobs\n\n= ↵,\n\n↵\n\n8\n\n2\n\n[0, 1].\n\n(8)\n\nThis means that the selective p-values pselective can be used as a valid statistical significance measure for the salient region\n\n⌘\n\n⇣\n\nMX .\n\n3.2 CHARACTERIZATION OF THE CONDITIONAL DATA SPACE\n\nQobs =\n\nQXobs,X ref UX,X ref = P ?\n\nobs\n\n2\n\nD\n\nTo compute the selective p-value in (7), we need to characterize the conditional data space whose characterization is described and introduced in the next section. We define the set of (X > X ref > )>\n\nR2n that satisfies the conditions in Eq. (7) as\n\n=\n\n(X > X ref > )>\n\n2n\n\nR\n\n2\n\nMX =\n\nMXobs QX,X ref =\n\nQobs\n\n.\n\n(9)\n\nis restricted to a line in R2n as stated in the\n\no\n\nn\n\nAccording to the second condition, the data in following Lemma. Lemma 1. Let us define a =⌦ Xobs,X ref and a =\n\nand b =\n\nobs\n\n \n\nD\n\nUXobs,X ref =\n\nobs\n\nD\n\nn\n\nVXobs,X ref > = a + bz\n\nobs\n\nz\n\n2Z\n\n|\n\no\n\n| Ma1:n+b1:n z = Z\nwith x1:n representing a vector of elements 1 through n of x.\n\n= \n\nR\n\n2\n\n{\n\nz\n\nMXobs }\n\n.\n\nrewritten as\n\nX > X ref >\n\nby using the scalar parameter z\n\nand b =\n\n2 2 in the case of global null test. The\n\nR2n in the case of mean null test, in (9) can be\n\nMX MX k\n\nk\n\n⌘ ⌘\n\nD\n\nR, where\n\n2\n\n(10)\n\n1This nuisance parameter\n\nQX,Xref corresponds to the component z in the seminal conditional SI paper (Lee et al., 2016) (see Sec. 5, Eq. 5.2 and Theorem 5.2) and z, w in (Chen & Bien, 2019)(see Sec. 3, Theorem 3.7). We note that additional conditioning on QX,Xref is a standard approach in the conditional SI literature and is used in almost all conditional SI-related studies. Here, we would like to note that the selective p-value QX,Xref , but the property in (8) is satisfied without this additional condition because we can depends on marginalize over all values of QX,Xref (see the lower part of the proof of Theorem 5.2 in Lee et al. (2016) and the proof of Theorem 3.7 in Chen & Bien (2019) ).\n\n5\n\n Published as a conference paper at ICLR 2023\n\nProof. The proof is deferred to Appendix A.1.\n\nLemma 1 indicates that we do not need to consider the 2n-dimensional data space. Instead, we only need to consider the one-dimensional projected data space in (10). Now, let us consider a R that satisfies (X > X ref > )> = a + bZ and random variable Z (X >obs X ref >\n\nobs )> = a + bZobs. The selective p-value (7) is rewritten as\n\nR and its observation Zobs 2\n\nZ\n\n2\n\npselective = PH0 (\n\nZ\n\n|\n\n||\n\nZobs| |\n\nZ\n\n2Z\n\n) .\n\n(11)\n\nN\n\n⇠ N(0, 2\n\nBecause (X > X ref > )>\n\ns> sref >\n\n> , 2I2n\n\ndue to the independence between X and\n\nX ref , the variable Z  (Trace(P )) in the case of global null test under the null hypothesis. Therefore, Z follows a truncated normal distribution and a truncated  distribution, respectively. Once the truncation region is identified, computation of the selective p-value in (11) is straightforward. Therefore, the remaining task is to identify\n\n◆ 2) in the case of mean null test and Z\n\n✓⇣ ⌘\nk\n\n2Z\n\n⇠\n\n⇠\n\nZ\n\nZ\n\n⌘\n\nk\n\n|\n\n.\n\nZ\n\n4 PIECEWISE LINEAR NETWORK\n\nThe problem of computing selective p-values for the selected salient region is cast into the problem | MX(z) = of identifying a set of intervals . Given the complexity of saliency . In this section, however, we show that computation in a trained DNN, it seems difficult to obtain this is feasible for a wide class of CNNs.\n\nMXobs }\n\n=\n\nZ\n\nZ\n\nR\n\n2\n\n{\n\nz\n\nPiecewise linear components in CNN. The key idea is to note that most of basic operations and common activation functions used in a trained CNN can be represented as piecewise linear functions in the following form: Definition 1. (Piecewise Linear Function) A piecewise linear function f : Rn\n\nRm is written as:\n\nif X if X\n\n2P 2P\n\nf\n\n1 := 2 :=\n\nf\n\nX 0 X 0\n\n{ {\n\n2 2\n\nRn Rn\n\n| |\n\nf f\n\n1 X 0 2 X 0\n\n \n\n! f f\n\n1 } 2 }\n\n, ,\n\nf (X) =\n\n f f\n\n1 X + f 1 , 2 X + f 2 , ... K(f )X + f\n\n8\n\n f\n\n>>>>< >>>>: k , f k :=\n\nK(f ),\n\nif X\n\nf\n\nK(f ) :=\n\nX 0\n\n{\n\n2\n\nRn\n\n|\n\nf\n\nK(f )X 0\n\nf\n\nK(f )}\n\n,\n\n\n\n2P\n\nk, f\n\nwhere f dimensions, \nnumber of polytopes for the function f .\n\nk and f k for k f Rn x\n\n2 kx\n\nP\n\n2\n\n{\n\n|\n\nf\n\nk }\n\n[K(f )] are certain matrices and vectors with appropriate [K(f )], and K(f ) is the\n\nis a polytope in Rn for k\n\nf\n\n2\n\nExamples of piecewise linear components in a trained CNN are shown in Appendix A.2.\n\nPiecewise Linear Network. Definition 2. (Piecewise Linear Network) A network obtained by concatenations and compositions of piecewise linear functions is called piecewise linear network.\n\nSince the concatenation and the composition of piecewise linear functions is clearly piecewise linear function, the output of any node in the piecewise linear network is written as a piecewise linear Ai(X), i function of an input vector X. This is also true for the saliency map function [n] obtained by CAM. Furthermore, as discussed in §4, we can focus on the input vector in the form of X(z) = a1:n + b1:nz which is parametrized by a scalar parameter z R. Therefore, the saliency map value for each element is written as a piecewise linear function of the scalar parameter z, i.e.,\n\n2\n\n2\n\nAi(X(z)) =\n\n8\n\n>>>>< >>>>:\n\nAi Ai\n\n1 , 1 z + ⇢Ai 2 z + ⇢Ai 2 , ... Ai)z + ⇢f\n\nAi K(\n\nK(\n\nif z if z\n\n2 2\n\n[LAi [LAi\n\n1 , U Ai 2 , U Ai\n\n1 ], 2 ],\n\n,\n\n(12)\n\nAi),\n\nif z\n\n[LAi K(\n\nAi), U Ai\n\nK(\n\nAi)],\n\n2\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nLemma 1 and initialize: t = 1, zt = zmin\n\n[zt, zt+1] (by using Eq.(13)) then\n\nCompute zt+1 by Auto-Conditioning (see §5)\n\n\n\nT ;\n\nEobs, compute ⌘ as well as a and b T do\n\nAlgorithm 1 SI DNN Saliency Input: X obs, zmin, zmax, 1: Obtain 2: for t 3: 4: 5: 6: 7: 8: end for 9: Identify 10: pselective \n\nEX(z),Xref (z) =\n\n2T Eq. (11) S\n\nEobs in z\n\n[zt, zt+1]\n\nt = t + 1\n\nend if\n\nT T\n\nZ \n\n+\n\n2\n\nif\n\n{\n\n}\n\nt\n\nt\n\nOutput: pselective\n\nAi) is the number of linear pieces of the piecewise linear function, Ai\n\nwhere K( scalar parameters, [LAi to an interval when it is projected onto one-dimensional space).\n\nk ] are intervals for k\n\nAi)] (note that a polytope in Rn is reduced\n\nare certain\n\nk , U Ai\n\nk ,⇢ Ai\n\n[K(\n\n2\n\nk\n\nThis means that, for each piece of the piecewise linear function, we can identify the interval of z such that\n\n⌧ as follows 2\n\nAi(X(z))\n\nmax\n\n k , LAi\n\nh\n\nLAi\n\n⇣ k , min\n\nz\n\n2 8 <\n\n(13)\n\nR\n\n2\n\n⌧\n\n ⇣\nk , U Ai\n\n⇢Ai k\n\n/Ai k\n\n, U Ai\n\nk\n\n⌧\n\n⌘ \n\n⇢Ai k\n\n⌘ /Ai k\n\ni\n\n,\n\nif Ai\n\nk > 0\n\nk < 0 )A i(X(z))\n\nif Ai\n\n⌧.\n\nh\n\n⌘ With a slight abuse of notation, let us collectively denote the finite number of intervals on z that are defined by LAi\n\n[n] as\n\n[K(\n\n⇢Ai\n\n:\n\n⌘\n\n⇣\n\n⇣\n\ni\n\nk , U Ai\n\nk , (⌧\n\n [z0, z1], [z1, z2], . . . , [zt\n\ni /Ai\n\nk ) for all (k, i)\n\nAi)] 1, zt], [zt, zt+1], . . . , [zT\n\n2\n\n⇥\n\n1, zT ],\n\nwhere zmin = z0 and zmax = zT are defined such that the probability mass of z < zmin and z > zmax are negligibly small.\n\nz\n\n{\n\nR\n\nZ\n\n=\n\nEq.(10) is given as\n\nMX(zobs) or not in the interval by using Eq.(13). Then, the truncation region\n\nAlgorithm. Algorithm 1 shows how we identify We simply check the intervals of z in the order of [z0, z1], [z1, z2], ..., [zT MX(z) =\n\n. Mobs} 1, zT ] to see whether in [zt,zt+1][zt, zt+1]. In the literature of homoEobs for z topy method (a.k.a. parametric programming), it is known that the actual computational cost differs significantly from the worst case. A well-known application of the homotopy method in the ML community is the Lasso regularization path, which also has the worst-case computational cost on the exponential order of the number of features, but the actual cost is known to be nearly linear order. Empirically, this also applies to our proposed method.\n\n| MX(z),X ref (z) =\n\n|EX(z),Xref (z)=\n\nS\n\n=\n\nZ\n\nZ\n\n[T ]\n\n2\n\n2\n\n2\n\nt\n\n5\n\nIMPLEMENTATION: AUTO-CONDITIONING\n\nThe bottleneck of our algorithm is Line 3 in Algorithm 1, where zt+1 must be found by considering all relevant piecewise linear components in a complicated trained CNN. The difficulty lies not only in the computational cost but also in the implementation cost. To implement conditional SI in DNNs naively, it is necessary to characterize all operations at each layer of the network as selection events and implement each of them specifically (Duy et al., 2022). To circumvent this difficulty, we introduce a modular implementation scheme called auto-conditioning, which is similar to autodifferentiation (Baydin et al., 2018) in concept. This enables us to conduct conditional SI for a wide class of CNNs without additional implementation costs.\n\nThe basic idea in auto-conditioning is to add a mechanism to compute and maintain the interval k ] for each piecewise linear component f in the network (e.g., layer API in the Keras z\n\nk, U f\n\n[Lf\n\n2 2For simplicity, we omit the description for the case of Ai k , U Ai k ]\n\ni\n\n[LAi\n\n2M X(z).\n\n)\n\nk = 0. In this case, if ⇢Ai\n\nk \n\n⌧ , then z\n\n2\n\n7\n\n Published as a conference paper at ICLR 2023\n\nframework). This enables us to automatically compute the interval [Lf k ] of a piecewise linear function f when it is obtained as concatenation and/or composition of multiple piecewise linear components. If f is obtained by concatenating two piecewise linear functions f1 and f2, we can easily obtain [Lf ]. However, if f is obtained as a composition of two piecewise linear functions f1 and f2, the calculation of the interval is given by the following lemma. Lemma 2. Consider the composition of two piecewise linear functions f (X(z)) = (f2 f1)(X(z)). k , U f2 Given a real value of z, the interval [Lf2 f1 )j\n\nk ] in the input domain of f2 can be computed as\n\nk ] = [Lf1\n\nk, U f\n\nk, U f\n\nf1 )j\n\n, U f2\n\n, U f1\n\n(f2\n\n(f2\n\n[Lf2\n\n(f2\n\n(f2\n\n\\\n\nk2\n\nk1\n\nk1\n\nk2\n\n]\n\nLf2\n\nk2\n\n=\n\nmax\n\nj:(f2 k2\n\n f1 )j <0\n\nk2\n\n)j  (f2\n\nk2\n\nk2 f1 )j\n\n,\n\nU f2\n\nk2\n\n=\n\nmin\n\nj:(f2 k2\n\n f1 )j >0\n\nk2\n\n)j  (f2\n\nk2\n\nk2 f1 )j\n\n,\n\nwhere f1 + f1 z is the output of f1 (i.e., the input of f2). Moreover, f2 are obtained by verifying the value of f1 + f1 z. Then, the interval of the composite function is obtained as follows: [Lf\n\nand f2\n\n, U f1\n\n, U f2\n\n[Lf2\n\nk2\n\nk2\n\n]\n\n]\n\nk, U f\n\nk ] = [Lf1\n\nk1\n\nk2\n\nk2\n\nk1\n\n\\\n\nThe proof is provided in Appendix A.3. Here, the variables fk and fk can be recursively computed through layers as\n\nfk+1 = fk\n\nk fk + fk\n\nk\n\nand fk+1 = fk\n\nk fk .\n\nLemma 2 indicates that the intervals in which X(z) falls in can be forwardly propagated through these layers. This means that the lower bound LAi k of the current piece in the piecewise linear function in Eq. (12) can be automatically computed by forward propagation of the intervals of the relevant piecewise linear components.\n\nk and upper bound U Ai\n\n6 EXPERIMENT\n\nWe only highlight the main results. More details (methods for comparison, network structure, etc.) can be found in the Appendix A.4.\n\nExperimental setup. We compared our proposed method with the naive method, over-conditioning (OC) method, and Bonferroni correction. To investigate the false positive rate (FPR), we considered 1000 null images X = (X1, ..., Xn) and 1000 reference images X ref = (X ref n ), where s = sref = 0 and \", \"ref . To investigate the true positive rate (TPR), we set n = 256 and generated 1,000 images, in which si = for any i ,\nis the “true” salient region whose location is randomly determined, and si = 0 for any where i\n. Reference images were generated in the same way as in the case of FPR. In all experiments, we set ⌧ = 0 in the mean null test and ⌧ = 5 in the global null test. We set the significance level ↵ = 0.05. We used CAM as the saliency method in all experiments.\n\nN(0, In), for each n\n\n64, 256, 1024, 4096\n\n1 , ..., xref\n\n. We set \n\n1, 2, 3, 4\n\n62 S\n\n2S\n\n2{\n\n2{\n\n⇠\n\nS\n\n}\n\n}\n\nNumerical results. The results of FPR control properties are presented in Fig. 2. The proposed method, OC, and Bonferroni successfully controlled the FPR in both the mean and global null test cases, whereas the naive method could not. Because naive method failed to control the FPR, we no longer considered its TPR. The results of the TPR comparison are shown in Fig. 3. The proposed method has the highest TPR in all cases. The Bonferroni method has the lowest TPR because it is conservative owing to considering the number of all possible hypotheses. The OC method also has a low TPR because it considers several extra conditions, which cause the loss of TPR.\n\nReal data experiments. We examined the brain image dataset extracted from the dataset used in Buda et al. (2019), which included 939 and 941 images with and without tumors, respectively. The results of the mean null test are presented in Figs. 4 and 5. The results of the global null test are presented in Figs. 6 and 7. The naive p-value remains small even when the image has no tumor region, which indicates that naive p-values cannot be used to quantify the reliability of DNN-based salient regions. The proposed method successfully identified false and true positive detections.\n\n7 CONCLUSION\n\nIn this study, we proposed a novel method to conduct statistical inference on the significance of DNN-driven salient regions based on the concept of conditional SI. We provided a novel algorithm\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a) Mean null test\n\n(b) Global null test\n\n(a) Mean null test\n\n(b) Global null test\n\nFigure 2: False Positive Rate (FPR) comparison.\n\nFigure 3: True Positive Rate (FPR) comparison.\n\nInput Image\n\nSaliency Map\n\nSalient Region\n\nReference Region\n\nReference Image\n\nFigure 4: Mean null test for image without tumor (pnaive = 0.00, pselective = 0.78).\n\nInput Image\n\nSaliency Map\n\nSalient Region\n\nReference Region\n\nReference Image\n\nFigure 5: Mean null test for image with a tumor (pnaive = 0.00, pselective = 1.92\n\n10\n\n4).\n\n⇥\n\nInput Image\n\nSaliency Map\n\nSalient Region\n\nReference Region\n\nReference Image\n\nFigure 6: Global null test for image without tumor (pnaive = 0.03, pselective = 0.46)\n\nInput Image\n\nSaliency Map\n\nSalient Region\n\nReference Region\n\nReference Image\n\nFigure 7: Global null test for image with a tumor (pnaive = 0.00, pselective = 1.51\n\n10\n\n3).\n\n⇥\n\nfor efficiently and flexibly conducting conditional SI for salient regions. We conducted experiments on both synthetic and real-world datasets to demonstrate the performance of the proposed method. In current setting, we have not considered the situations where there is a misalignment between the input image and the reference image. A potential future improvement could be additionally performing a step to automatically find an appropriate region in the reference image before conducting a statistical test. If the matching operations can be represented as a set of linear inequalities, they can be easily incorporated to the proposed method. If the matching operations can be represented as a set of linear inequalities, they can be easily incorporated to the proposed method.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nThis work was partially supported by MEXT KAKENHI (20H00601), JST CREST (JPMJCR21D3), JST Moonshot R&D (JPMJMS2033-05), JST AIP Acceleration Research (JPMJCR21U2), NEDO (JPNP18002, JPNP20006), and RIKEN Center for Advanced Intelligence Project.\n\nREFERENCES\n\nSebastian Bach, Alexander Binder, Gr ́egoire Montavon, Frederick Klauschen, Klaus-Robert M ̈uller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.\n\nAtilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of Marchine Learning Research, 18:1–43, 2018.\n\nMateusz Buda, Ashirbani Saha, and Maciej A Mazurowski. Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm. Computers in biology and medicine, 109:218–225, 2019.\n\nCollin Burns, Jesse Thomason, and Wesley Tansey. Interpreting black box models via hypothesis testing. In Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference, pp. 47–57, 2020.\n\nShuxiao Chen and Jacob Bien. Valid inference corrected for outlier removal. Journal of Computa-\n\ntional and Graphical Statistics, pp. 1–12, 2019.\n\nYunjin Choi, Jonathan Taylor, and Robert Tibshirani. Selecting the number of principal components: Estimation of the true rank of a noisy matrix. The Annals of Statistics, 45(6):2590–2617, 2017.\n\nAnn-Kathrin Dombrowski, Maximillian Alber, Christopher Anders, Marcel Ackermann, KlausRobert M ̈uller, and Pan Kessel. Explanations can be manipulated and geometry is to blame. In Advances in Neural Information Processing Systems, pp. 13589–13600, 2019.\n\nFinale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.\n\narXiv preprint arXiv:1702.08608, 2017.\n\nAlexey Dosovitskiy and Thomas Brox.\n\nInverting visual representations with convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4829–4837, 2016.\n\nVo Nguyen Le Duy and Ichiro Takeuchi. More powerful conditional selective inference for gener-\n\nalized lasso by parametric programming. arXiv preprint arXiv:2105.04920, 2021a.\n\nVo Nguyen Le Duy and Ichiro Takeuchi. Parametric programming approach for more powerful and general lasso selective inference. In International Conference on Artificial Intelligence and Statistics, pp. 901–909. PMLR, 2021b.\n\nVo Nguyen Le Duy, Hiroki Toda, Ryota Sugiyama, and Ichiro Takeuchi. Computing valid p-value In Advances in\n\nfor optimal changepoint by selective inference using dynamic programming. Neural Information Processing Systems, pp. 11356–11367, 2020.\n\nVo Nguyen Le Duy, Shogo Iwazaki, and Ichiro Takeuchi. Quantifying statistical significance of neural network-based image segmentation by selective inference. Advances in Neural Information Processing Systems, 2022.\n\nWilliam Fithian, Dennis Sun, and Jonathan Taylor. Optimal inference after model selection. arXiv\n\npreprint arXiv:1410.2597, 2014.\n\nRuth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3429–3437, 2017.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAmirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3681–3688, 2019.\n\nJuyeon Heo, Sunghwan Joo, and Taesup Moon. Fooling neural network interpretations via adversarial model manipulation. In Advances in Neural Information Processing Systems, pp. 2925–2936, 2019.\n\nSangwon Hyun, Kevin Z Lin, Max G’Sell, and Ryan J Tibshirani. Post-selection inference for changepoint detection algorithms with application to copy number variation data. Biometrics, 77 (3):1037–1049, 2021.\n\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch ̈utt, Sven D ̈ahne, Dumitru Erhan, and Been Kim. The (un) reliability of saliency methods. arXiv preprint arXiv:1711.00867, 2017.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL\n\nhttps://arxiv.org/abs/1412.6980.\n\nJason D Lee, Dennis L Sun, Yuekai Sun, and Jonathan E Taylor. Exact post-selection inference,\n\nwith application to the lasso. The Annals of Statistics, 44(3):907–927, 2016.\n\nJoshua R Loftus. Selective inference after cross-validation. arXiv preprint arXiv:1511.08866, 2015.\n\nJoshua R Loftus and Jonathan E Taylor. A significance test for forward stepwise model selection.\n\narXiv preprint arXiv:1405.3920, 2014.\n\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances\n\nin neural information processing systems, pp. 4765–4774, 2017.\n\nAravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5188–5196, 2015.\n\nDavid Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining neural networks. In Advances in Neural Information Processing Systems, pp. 7775–7784, 2018.\n\nSnigdha Panigrahi, Jonathan Taylor, and Asaf Weinstein. Bayesian post-selection inference in the\n\nlinear model. arXiv preprint arXiv:1605.08824, 28, 2016.\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135–1144, 2016.\n\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pp. 618–626, 2017.\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.\n\nKazuya Sugiyama, Vo Nguyen Le Duy, and Ichiro Takeuchi. More powerful and general selective inference for stepwise feature selection using the homotopy continuation approach. In Proceedings of the 38th International Conference on Machine Learning, 2021a.\n\nRyota Sugiyama, Hiroki Toda, Vo Nguyen Le Duy, Yu Inatsu, and Ichiro Takeuchi. Valid and exact statistical inference for multi-dimensional multiple change-points by selective inference. arXiv preprint arXiv:2110.08989, 2021b.\n\nKosuke Tanizaki, Noriaki Hashimoto, Yu Inatsu, Hidekata Hontani, and Ichiro Takeuchi. Computing valid p-values for image segmentation by selective inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9553–9562, 2020.\n\nXiaoying Tian and Jonathan Taylor. Selective inference with a randomized response. The Annals of\n\nStatistics, 46(2):679–710, 2018.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nRyan J Tibshirani, Jonathan Taylor, Richard Lockhart, and Robert Tibshirani. Exact post-selection inference for sequential regression procedures. Journal of the American Statistical Association, 111(514):600–620, 2016.\n\nToshiaki Tsukurimichi, Yu Inatsu, Vo Nguyen Le Duy, and Ichiro Takeuchi. Conditional selective inference for robust regression and outlier detection using piecewise-linear homotopy continuation. arXiv preprint arXiv:2104.10840, 2021.\n\nFan Yang, Rina Foygel Barber, Prateek Jain, and John Lafferty. Selective inference for group-sparse linear models. In Advances in Neural Information Processing Systems, pp. 2469–2477, 2016.\n\nMatthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks.\n\nIn\n\nEuropean conference on computer vision, pp. 818–833. Springer, 2014.\n\nXinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji, Xiapu Luo, and Ting Wang. Interpretable Security 20), 2020.\n\ndeep learning under fire. In 29th\n\nSecurity Symposium (\n\nUSENIX\n\nUSENIX\n\n{\n\n}\n\n{\n\n}\n\nBolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep In Proceedings of the IEEE conference on computer\n\nfeatures for discriminative localization. vision and pattern recognition, pp. 2921–2929, 2016.\n\n12",
    "reference": "# Summary Of The Paper\n\nThis work attempts to shed some light on the reliability of saliency maps.\nSpecifically, the authors propose a conditional Selective Inference (SI) method. The proposed method can provide valid p-values for statistically quantifying the reliability of the salient regions. The proposed method is evaluated on a synthetic dataset and a brain image dataset.\n\n# Strength And Weaknesses\n\nStrengths:\n\n1. This work looks technically sound.\n\n2. The proposed method is correlated to statistical hypothesis tests. This idea is interesting and could be practically useful in some real-world scenarios, e.g., medical applications.\n\nWeaknesses:\n\n1. In this work, the saliency map is presumed to be generated by CAM (Zhou et al., 2016). We don't know how well the proposed method works with other CAM-like saliency map methods (e.g., Grad-Cam). Correspondingly, in the theory part, how the saliency map model A(\\cdot) affects the proposed selective p-value is unclear.\n\n2. How the proposed selective p-value corresponds to the correlation between the salient region and the reference region is unknown. In other words, can the selective p-value be aligned with the visual difference/similarity between salient regions and reference regions? One way to measure the linear correlation coefficient between a model saliency map and an empirical saliency map is to use correlation coefficient (CC). Please refer to \\url{https://saliency.tuebingen.ai/results.html} for more details.  \n\n3. Last but not least, the experiments are less convincing as the proposed method is only evaluated on a synthetic dataset and a small-scale dataset. I cannot tell if the proposed method can work well on other general yet complicated real-world datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is well-written in my opinion. The proposed method is novel and provides new insights into saliency map detection. \n\nThe authors discuss the implementation in the paper and provide the code for review. I haven't verified if the experiments in this paper can be reproduced though.\n\n# Summary Of The Review\n\nOverall, I think the proposed conditional Selective Inference method is interesting and insightful. However, my main concern lies in the empirical evidence. It would be better to see more experimental results on other datasets, e.g., a natural image dataset, like CIFAR.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLEARNING ROBUST KERNEL ENSEMBLES WITH KERNEL AVERAGE POOLING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nModel ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks’ robustness against input perturbations remains underexplored. We introduce Kernel Average Pool (KAP), a new neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when combined with activation noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial improvements in robustness against strong adversarial attacks such as AutoAttack that are on par with adversarially trained networks but are importantly obtained without training on any adversarial examples.\n\n1\n\nINTRODUCTION\n\nModel ensembles have long been used to improve robustness in the presence of noise. Classic methods like bagging (Breiman, 1996), boosting (Freund, 1995; Freund et al., 1996), and random forests (Breiman, 2001) are established approaches for reducing the variance in estimated prediction functions that build on the idea of constructing strong predictor models by combining many weaker ones. As a result, performance of these ensemble models (especially random forests) is surprisingly robust to noise variables (i.e. features) (Hastie et al., 2009).\n\nModel ensembling has also been applied in deep learning (Zhou et al., 2001; Agarwal et al., 2021; Liu et al., 2021; Horv ́ath et al., 2022). However, the high computational cost of training multiple neural networks and averaging their outputs at test time often quickly becomes prohibitively expensive (also see work on averaging network weights across multiple fine-tuned versions (Wortsman et al., 2022)). To tackle these challenges, alternative approaches have been proposed to allow learning pseudo-ensembles of models by allowing individual models within the ensemble to share parameters (Bachman et al., 2014; Srivastava et al., 2014; Hinton et al., 2012; Goodfellow et al., 2013). Most notably, dropout (Hinton et al., 2012; Srivastava et al., 2014) was introduced to approximate the process of combining exponentially many different neural networks by “dropping out” a portion of units from layers of the neural network for each batch. It was argued that this technique prevents ”co-adaptation” in the neural network and leads to learning more general features (Hinton et al., 2012).\n\nWhile these techniques often improve the network generalization for i.i.d. sample sets, they are not as effective in improving the network robustness against input perturbations and in particular against adversarial attacks (Wang et al., 2018). Adversarial attacks (Goodfellow et al., 2014), slight but carefully constructed input perturbations that can significantly impair the network’s performance, are one of the major challenges to the reliability of modern neural networks. Despite numerous works on this topic in recent years, the problem remains largely unsolved (Kannan et al., 2018; Madry et al., 2017; Zhang et al., 2019; Sarkar et al., 2021; Pang et al., 2020; Bashivan et al., 2021; Rebuffi et al., 2021; Gowal et al., 2021). Moreover, the most effective empirical defense methods\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nagainst adversarial attacks (e.g. adversarial training (Madry et al., 2017) and TRADES (Zhang et al., 2019)) are extremely computationally demanding (although see more recent work on reducing their computational cost (Wong et al., 2019; Shafahi et al., 2019)).\n\nOur central premise in this work is that if ensembles can be learned at the level of features (in contrast to class likelihoods), the resulting hierarchy of ensembles in the neural network could potentially lead to a much more robust classifier. To this end, we propose a simple method for learning ensembles of kernels in deep neural networks that significantly improves the network’s robustness against adversarial attacks. In contrast to prior methods such as dropout that focus on minimizing feature co-adaptation and improving the individual features’ utility in the absence of others, our method focuses on learning feature ensembles that form local “committees” similar to those used in Boosting and Random Forests. To create these committees in layers of a neural network, we introduce the Kernel Average Pool (KAP) operation that computes the average activity in nearby kernels within each layer – similar to how Average Pooling layer computes the locally averaged activity within each spatial window but instead along the kernel dimension. We show that incorporating KAP into convolutional networks leads to learning kernel ensembles that are topographically organized across the tensor dimensions over which the kernels are arranged. When combined with activation noise, these networks demonstrate a substantial boost in robustness against adversarial attacks. In contrast to other ensemble approaches to adversarial robustness, our approach does not seek to train multiple independent neural network models and instead focuses on learning kernel ensembles within a single neural network.\n\nOur contributions are as follows:\n\n• we introduce the kernel average pool as a simple method for learning kernel ensembles in\n\ndeep neural networks.\n\n• we demonstrate how kernel average pooling leads to learning smoothly transitioning kernel\n\nensembles that in turn substantially improve model robustness against input noise.\n\n• through extensive experiments on a wide range of benchmarks, we demonstrate the effec-\n\ntiveness of kernel average pooling on robustness against strong adversarial attacks.\n\n2 RELATED WORKS AND BACKGROUND\n\nAdversarial attacks: despite their superhuman performance in many vision tasks such as visual object recognition, neural network predictions are highly unreliable in the presence of input perturbations, including natural and artificial noise. While performance robustness of predictive models to natural noise have long been studied in the literature, more modern methods have been invented in the past decade to allow discovering small model-specific noise patterns (i.e. adversarial examples) that could maximize the model’s risk (Goodfellow et al., 2014).\n\nNumerous adversarial attacks have been proposed in the literature during the past decade Carlini & Wagner (2017); Croce & Hein (2020); Moosavi-Dezfooli et al. (2016); Andriushchenko et al. (2020); Brendel et al. (2017); Gowal et al. (2019). These attacks seek to find artificially generated samples that maximize the model’s risk. Formally, given a classifier function fθ : X → Y, X ⊆ Rn, Y = {1, ..., C}, denote by π(x, ε) a perturbation function (i.e. adversarial attack) which, for a given (x, y) ∈ X × Y, generates a perturbed sample x′ ∈ B(x, ε) within the ε-neighborhood of x, B(x, ε) = {x′ ∈ X : ∥x′ − x∥p < ε}, by solving the following maximization problem\n\nmax t∈B(x,ε)\n\nL(fθ(t), y),\n\n(1)\n\nwhere L is the classification loss function (i.e. classifier’s risk) and ∥.∥p is the Lp norm function. Solutions x′ are called adversarial examples and are essentially the original input samples altered with additive noise of magnitude ε measured by the Lp norm.\n\nAdversarial defenses: Concurrent to the research on adversarial attacks, numerous methods have also been proposed to defend neural network models against these attacks (Kannan et al., 2018; Madry et al., 2017; Zhang et al., 2019; Sarkar et al., 2021; Pang et al., 2020; Bashivan et al., 2021; Robey et al., 2021; Sehwag et al., 2022; Rebuffi et al., 2021; Gowal et al., 2021). Formally, the goal of these defense methods is to guarantee that the model predictions match the true label not only over the sample set but also within the ε-neighborhood of samples x. Adversarial training,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nwhich is the most established defense method to date, formulates adversarial defense as a minimax optimization problem through which the classifier’s risk for adversarially perturbed samples is iteratively minimized during training (Madry et al., 2017). Likewise, other prominent methods such as ALP (Kannan et al., 2018) and TRADES (Zhang et al., 2019), encourage the classifier to predict matching labels for the original (x) and perturbed samples (x′).\n\nDespite the continuing progress towards robust neural networks, most adversarial defenses remain computationally demanding, requiring an order of magnitude or more computational resources compared to normal training of these networks. This issue has highlighted the dire need for computationally cheaper defense methods that are also scalable to large-scale datasets such as Imagenet. In that regard, several recent papers have proposed alternative methods for discovering diverse adversarial examples at a much lower computational cost and have been shown to perform competitively with adversarial training using costly iterative attacks like Projected Gradient Descent (PGD) (Wong et al., 2019; Shafahi et al., 2019).\n\nAnother line of work has proposed utilizing random additive noise as a way to empirically improve the neural network robustness (Liu et al., 2018; Wang et al., 2018; He et al., 2019) and to derive robustness guarantees (Cohen et al., 2019; Lecuyer et al., 2019). Although, some of the proposed defenses in this category have later been discovered to remain vulnerable to other forms of attacks (Tramer et al., 2020), there is an increasing body of work that shows the close relationship between robustness to random noise and adversarial robustness (Ford et al., 2019; Cohen et al., 2019). Also related to our proposed method, recent work on feature denoising (Xie et al., 2019) shows that denoising feature maps in neural networks together with adversarial training leads to large gains in robustness against adversarial examples. However, this work is fundamentally different from our proposed method in that the focus of this work is on denoising individual feature maps by considering the distribution of feature values across the spatial dimensions within each feature map.\n\nEnsemble methods: Ensemble methods have long been used in machine learning and deep learning because of their effectiveness in improving generalization and obtaining robust performance against input noise (Hastie et al., 2009). In neural networks, pseudo-ensemble methods like dropout Hinton et al. (2012) create and simultaneously train an ensemble of ”child” models spawned from a ”parent” model using parameter perturbations sampled from a perturbation distribution (Bachman et al., 2014). Through this procedure, pseudo-ensemble methods can improve generalization and robustness against input noise. Another related method is MaxOut Goodfellow et al. (2013) which proposes an activation function that selects the maximum output amongst a series of unit outputs.\n\nNaturally, similar ideas consisting of neural network ensembles have been tested in recent years to improve prediction variability and robustness in neural networks with various degrees of success (Pang et al., 2019; Kariyappa & Qureshi, 2019; Abbasi et al., 2020; Horv ́ath et al., 2022; Liu et al., 2021). Ensemble adversarial training (Tram`er et al., 2018) proposes to use adversarial examples transferred from various models during adversarial training to improve the robustness of the model. Several other works have focused on enhancing the diversity among models within the ensemble with the goal of making it more difficult for adversarial examples to transfer between models (Pang et al., 2019; Kariyappa & Qureshi, 2019). However these ensemble models still remain prone to ensembles of adversarial attacks (Tramer et al., 2020).\n\n3 METHODS\n\n3.1 PRELIMINARIES\n\nLet fθ(x) : X → Y, where X ⊆ Rn, Y = {1, ..., C}, be a classifier with parameters θ. In feed-forward deep neural networks, the classifier fθ is usually composed of a cascade of simpler functions f (l)(x), l ∈ {1, . . . , L} chained together such that the network output is computed as y = f (L)(f (L−1)(. . . f (1)(x))). For our function fθ to correctly classify the input patterns x, we wish for it to attain a small risk for (x, y) ∼ D as measured by loss function L. Additionally, for our classifier to be robust, we also wish fθ to attain a small risk in the vicinity of all x ∈ X , normally defined by a Wasserstein ball around the sample points (Madry et al., 2017).\n\nWhile to guarantee robustness, one has to consider the maximum risk within the epsilon ball, in practice, the prediction variance can arguably be also linked to the expected robustness – similar to recent work on domain generalization that uses risk variance as an objective for improving model\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ngeneralization across domains (Krueger et al., 2021). Intuitively, a model which has a high prediction variance (or similarly high risk variance) to noisy inputs, is more likely to exhibit extreme high risks for data points sampled from the same distribution (i.e. adversarial examples). Indeed, classifiers that generate lower variance predictions are often expected to generalize better and be more robust to input noise. For example, classic ensemble methods like bagging, boosting, and random forests operate by combining the decisions of many weak (i.e. high variance) classifiers into a stronger one with reduced prediction variance and improved generalization performance (Hastie et al., 2009).\n\nGiven an ensemble of predictor functions fi, i ∈ 1, . . . , K with zero or small biases, the ensemble prediction (normally considered as the mean prediction ̄y = 1 i=1 yi) reduces the expected generalization loss by shrinking the prediction variance. To demonstrate the point, one can consider K i.i.d. random variables with variance σ2 and their average value that has a variance of σ2 K . Based on this logic, one would expect ensembles of neural network classifiers to be more robust in the presence of noise or input perturbations in general. However, such ensemble models have been shown to remain prone to ensemble of adversarial attacks with large epsilons (Tramer et al., 2020).\n\n(cid:80)K\n\nK\n\nWe reasoned that individual networks participating in these ensembles may still learn different sets of non-robust representations leaving room for the attackers to find common weak spots across all individual models within the ensemble. At the same time, constructing ever-larger ensemble classifiers might quickly become infeasible, especially in the case of neural network classifiers. On the other hand, learning robust features has been suggested as a way towards robust classification (Bashivan et al., 2021). Consequently, if individual kernels within a single network are robust, it would become much more difficult to find adversaries that can fool the full network. In the next section, we introduce the Kernel Average Pool as a way towards learning ensemble kernels with better robustness properties against input perturbations.\n\n3.2 KERNEL AVERAGE POOL (KAP) Mean filters (a.k.a., average pool) are widely accepted as simple noise suppression mechanisms in computer vision. Spatial average pooling layers are commonly used in modern deep neural networks (Zoph et al., 2018) by applying a mean filter along the spatial dimensions of the input to reduce the effect of spatially distributed noise (e.g. adjacent pixels in an image).\n\nHere, we wish to substitute each kernel in the neural network model with an ensemble of kernels performing the same function such that the ensemble output is the average of individual kernel outputs. This can be conveniently carried out by applying the average pool operation along the kernel dimension of the input tensor. Given an input z ∈ RD×Nk , where D and Nk denote the input dimension and the number of kernels respectively, the kernel average pool operation with kernel size K and stride S, computes the function\n\nFigure 1: Schematic of a one-layer neural network with (right) and without (left) the kernel average pooling operation.\n\n ̄zik =\n\n1 K\n\nSk+ K−1 2(cid:88)\n\nzil\n\nl=Sk− K−1\n\n2\n\n(2)\n\nImportantly, when z is the output of an operation linear with respect to the weights on an input x (e.g. linear layers or convolutional layers), KAP is functionally equal to computing the locally averaged weights within the layer and could be interpreted as a form of Kernel Smoothing (Wang et al., 2020) conditioned that the nearby kernels are more or less similar to each other.\n\n ̄zik =\n\n1 K\n\nSk+ K−1 2(cid:88)\n\nl=Sk− K−1\n\n2\n\nwix =\n\n\n\n\n\n1 K\n\nSk+ K−1 2(cid:88)\n\n\n\nwi\n\n x\n\nl=Sk− K−1\n\n2\n\n(3)\n\nMoreover, the degree of overlap (i.e. parameter sharing) across kernel ensembles can be flexibly controlled by adjusting the KAP stride. Choosing stride S = K, produces independent kernel en-\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nsembles (no parameter sharing between ensembles), while having stride S < K enforces parameter sharing across kernel ensembles.\n\nEq. 2 assumes that kernels are arranged along one tensor dimension. However, KAP could more generally be applied on any Ddimensional tensor arrangement of kernels. For example to apply a 2-dimensional KAP (mainly considered in our experiments) on the input z, one can first reshape the channel dimension into a 2-dimensional array and then apply KAP along the two kernel dimensions (see §A.1 and Alg. 1). Importantly, using higher dimensional tensor arrangements of kernels when applying KAP, allows parameter sharing across a larger number of kernel ensembles.\n\nAlgorithm 1 2D Kernel Average Pool\n\nlayer input x, kernel size k, stride Input: s, vectorization operator Vec and its inverse Vec−1, zero padding function Pad, and average pooling function AvePool. h ← Vec−1√ √\nh ← Pad(h, k−1 2 ) h ← AvePool(h, k, s) h ← Vec−1 return: h\n\nW,H,D(Vec(hT ))\n\n(Vec(xT ))\n\nD,W H\n\nD,\n\nIn the next two subsections, we will explain how training networks with KAP-layers leads to learning topographically organized kernel ensembles (§3.3) and how these kernel ensembles may contribute to model robustness (§3.4).\n\n3.3 KERNEL AVERAGE POOLING YIELDS TOPOGRAPHICALLY ORGANIZED KERNEL\n\nENSEMBLES\n\nConsider a simple neural network with one hidden layer and Nk units (Fig.1-left) where the hidden unit activation is hi = w1ix and the network output y is computed as\n\ny =\n\nNk(cid:88)\n\ni=1\n\nw2ihi\n\nIn this network, the output gradients with respect to weight parameters can be computed as\n\n∂y ∂w1i\n\n= w2ix,\n\n∂y ∂w2i\n\n= w1ix\n\n(4)\n\n(5)\n\nNow consider a variation of this network where the hidden unit activations are passed through a kernel average pool with kernel size K (Fig.1-right). In the KAP-network, the output gradients with respect to weight parameters are altered such that\n\n∂y ∂w1i\n\n=\n\n1 K\n\ni+ K−1 2(cid:88)\n\nl=i− K−1\n\n2\n\nw2lx,\n\n∂y ∂w2i\n\n=\n\n1 K\n\ni+ K−1 2(cid:88)\n\nl=i− K−1\n\n2\n\nw1lx\n\n(6)\n\nwhere, to simplify the limits, K is assumed to be an odd number. In contrast to the regular network, in the KAP-network, the gradients of the output with respect to the incoming (w1i) and outgoing (w2i) weights in node i depend on the average of outgoing and incoming weights, respectively, over the kernel average pool window. The difference in the output gradients with respect to weights w1i (and similarly for w2i) for nodes i and j = i + d can be written as\n\n∂y ∂w1i\n\n−\n\n∂y ∂w1j\n\n=\n\ni+ K−1 2(cid:88)\n\ni+d+ K−1 2(cid:88)\n\nw2lx\n\nw2lx −\n\nl=i− K−1\n\n2\n\nl=i+d− K−1\n\n2\n\n(7)\n\nFrom Eq.7, it is clear that when K is large K ≫ 1, the difference in the output gradients with respect to weights for a pair of nodes (i, j) depends on the absolute difference between node indices (|i − j|) and is smaller for a pair of nodes with smaller index difference (i.e. physically closer nodes). Thus, when training with backpropagation, weights connected to physically closer nodes (in\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nterms of their index numbers) will likely receive more similar gradients compared to those that are farther. Consequently, these physically closer weights are more likely to converge to more similar values. On the other hand, for a KAP with stride=S and kernel size K, each kernel participates in ⌈ K2 S ⌉2 ensembles. Kernel sharing between ensembles provides a natural mechanism preventing the participating kernels in each group from converging to the exact same parameter values. Our empirical results show that the interaction between these two forces leads to smoothly transitioning topographically organized kernel maps (Fig.3).\n\n3.4 KERNEL AVERAGE POOLS AND ADVERSARIAL ROBUSTNESS\n\nOur approach is closely related to randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019), which for a sample (x, y) consists of learning to predict y with higher probability than other classes over N (x, σ2I). When combined with noise resampling to estimate the true class, randomized smoothing yields certifiable robustness against adversarial perturbations. More recently, Horv ́ath et al. (2022) show that in addition to injecting noise in the input, averaging the logits over an ensemble of models leads to a reduction of the variance due to the noisy inputs in randomized smoothing and in turn improves the certified robustness radius. They propose to learn an ensemble\n\ng(x) = Ens(fc(x + n)) =\n\n1 L\n\n(cid:88)\n\nl\n\nf (l)\n\nc (x + n)\n\n(8)\n\nwhere n ∼ N (0, σ2I) and f (l)\n\nc denotes the l-th presoftmax classifier in an ensemble of L classifiers.\n\nSimilar to randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019), by introducing stochasticity in the input during training as in randomized smoothing, we hope to learn a model that is robust to perturbations. Moreover, in a simple setting where only one layer of KAP is used without activations, we note that KAP averages multiple filters before applying them to the input of a KAP block (eq. 3). This can be understood as averaging the features obtained from multiple models, each corresponding to a filter. Unlike Horv ́ath et al. (2022), our ensembling is done at the level of features instead of the logits. Nevertheless, their arguments still apply to the case of a reduction of variance in the features computed. Therefore, if KAP features are used as inputs to fully connected layers to compute logits, a reduction of variance in the features will translate into a reduction of variance in the logits. An additional key difference is that in our case, in the full KAP-based models used in §4, we use networks consisting of multiple cascaded KAP blocks to extract features, similar to how multiple convolutional blocks are used sequentially in convolutional networks. Each KAP block consists of an ensembling, from an input that was perturbed with Gaussian noise. In other words, our approach can be understood as recursively performing a form of Horv ́ath et al. (2022)’s randomized smoothing with ensembles, by interpreting the output of each KAP block as a randomized smoothing input for the next block. In this light, our KAP architectures perform the operation\n\nfc ◦ gNl ◦ ... ◦ g1(x) where gi(x) = Ens[fi(x + ni)]\n\n(9)\n\nwhere the ensembling is performed by the KAP operation, potentially including activation functions, Nl is the number of KAP blocks, ni is Gaussian noise sampled in KAP block i, fi is the operation (e.g. a convolution) performed in KAP block i before a kernel average pool, and fc maps the representations to the logits of the classes. Our reasoning for this recursive approach is twofold: first, during training, this encourages all layers to learn to be robust to variance in their input. Otherwise in deep networks, some layers may not be exposed to significant variance in their input depending on their depth, due to the variance reduction occurring in the earlier layers. Second, we hope that by having randomized smoothing at every KAP block, the model will require less perturbed inputs. If this approach successfully performs randomized smoothing on the features, we may hope that this will lead the representations of adversarially perturbed inputs to remain close to the distribution of inputs perturbed with Gaussian noise, on which the network has been trained and therefore should perform well. We empirically verify this intuition in the appendix (Fig.A4,A5).\n\n4 EXPERIMENTS\n\nIn this section we empirically demonstrate the effectiveness of our proposed kernel average pool operation in boosting robustness in deep neural networks.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4.1 EXPERIMENTAL SETUP\n\nDatasets: We validated our proposed method on several standard benchmarks CIFAR10, CIFAR100 (Krizhevsky, 2009), TinyImagenet Le & Yang (2015), and Imagenet (Deng et al., 2009). We used standard preprocessing of images consisting of random crop and horizontal flipping for all datasets. We used images of size 32 in our experiments on CIFAR datasets, 128 for TinyImagenet, and 224 for Imagenet.\n\nBaseline models: We compared the results from our proposed model to several baselines including (i) the original ResNet18 (i.e. without additional KAPs) trained normally (marked with NT); (ii) the original ResNet18 with additive activation noise (marked with NT(σ = .)); (iii) the original ResNet18 trained using adversarial training with early stopping (marked with AT-ES).\n\nAdversarial attacks: We assessed the model robustness against various adversarial attacks including L∞ Projected Gradient Descent (PGD) (Madry et al., 2017), SQUARE black-box (Andriushchenko et al., 2020) attack and the AutoAttack (Croce & Hein, 2020). Notably, AutoAttack is an ensemble attack consisting of four independent attacks and is considered as one of the strongest adversarial attacks in the field. We used ε value of 0.031 for experiments on CIFAR datasets and 0.016 for TinyImagenet and Imagenet datasets. See supplementary Table A3 for full details of attacks used for model evaluation. Importantly, we applied each attack on the model without activation noise to prevent activation noise from potentially masking the gradients in the network.\n\nTraining and evaluation considerations: In our experiments, we primarily used the ResNet18 architecture (He et al., 2016) that consists of four groups of layers and each group containing two basic residual blocks. For the KAP variations of ResNet18, we added the KAP operation after each convolution in the network. In variations of the model where we introduced activation noise, we added random noise sampled from the Gaussian distribution N (0, σ2) after each KAP operation (and after each convolution in the original model architecture).\n\nFor adversarial training of the baseline AT models on CIFAR10, CIFAR100, and TinyImagenet datasets, we used the normal adversarial training procedure with early stopping and L∞ PGD attack (Madry et al., 2017; Rice et al., 2020). We used 20 iterations for CIFAR training runs and 10 iterations for TinyImagenet. On Imagenet dataset, we used Fast-AT method (Wong et al., 2019) with the default training parameters consisting of three training phases with increasing image resolution.\n\n4.2 ROBUSTNESS AGAINST STANDARD ADVERSARIAL ATTACKS ON CIFAR\n\nTable 1: Comparison of adversarial accuracy against various attacks on CIFAR10 and CIFAR100 datasets. For all attacks we used ε = 0.031. All attacks are performed on the corresponding model without input or activation noise.\n\nDATASET\n\nCIFAR10\n\nCIFAR100\n\nMODEL RN18-NT RN18-NT (σ = 0.1) WRN-16-4(BE) (WEN ET AL., 2020) RN18-AT-ES (RICE ET AL., 2020) RN18-KAP-NT (σ = 0.1, K = 3) RN18-KAP-NT (σ = 0.2, K = 3) RN18-NT RN18-NT (σ = 0.1) RN18-AT-ES (RICE ET AL., 2020) RN18-KAP-NT (σ = 0.1, K = 3) RN18-KAP-NT (σ = 0.2, K = 3)\n\nCLEAN 94.66 88.95 95.60 84.20 79.09 74.30 74.00 61.60 56.50 48.20 38.60\n\nPGD-L∞ AUTOATTACK\n\n0.0 9.69 7.90 43.70 67.7 65.1 0.0 6.00 20.40 33.9 31.9\n\n0.0 8.90 7.80 43.00 41.8 44.40 0.0 5.20 19.60 15.70 17.60\n\nSQUARE 0.87 61.7 21.00 49.10 44.49 47.5 0.20 33.70 22.86 17.40 27.40\n\nWe first compared robustness in convolutional neural networks with and without KAP on the CIFAR10 and CIFAR100 datasets. For this we trained the vanilla ResNet18 architecture and two variations of this architecture where all convolution operations were followed by KAP. To make sure that the stochasticity due to activation noise does not interfere with gradient estimation during attacks, we performed all attacks on the corresponding model without input or activation noise.\n\nTable 1 lists the robustness of different models trained on CIFAR10 and CIFAR100 datasets against several commonly used adversarial attacks. Confirming prior work (He et al., 2019), we found that on both CIFAR10 and CIFAR100 datasets, training the network with noisy activations can improve\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nthe network robustness. This improvement was most noticeable against the strong SQUARE blackbox attack and to a lesser degree against PGD and AutoAttack. Furthermore, we found that adding KAP to the model significantly improves the robustness against all attacks and even beyond that of the adversarially trained ResNet18 architecture (AT). Robustness remained high against adversarial examples that were generated using models with activation noise (§A.2) as well as those transferred from RN18-NT and RN18-AT models (TableA5). The improved robustness was however at the expense of a noticeable reduction in clean accuracy that also increased with the activation noise variance σ. Although, it should also be noted that KAP models were trained using normal training procedures (i.e., no adversarial training) and as a result the computational cost of training was only a fraction of that required for adversarial training (∼ 0.14%). See the supplementary Table A4 for a comparison of training speed between KAP models and adversarial training.\n\nWe additionally explored the effect of kernel pooling type (No pooling, Max pooling, and Average pooling) and activation noise variance (σ ∈ {0, 0.1, 0.2}) on the robust accuracy with the ResNet18 architecture. To get a better sense of how robustness generalizes to higher-strength attacks (i.e. larger ε), we also tested each model on several ε values ( 2 255 ) using AutoAttack (Fig. 2). We found that (a) adding KAP to RN18 without activation noise already makes the network substantially more robust against attacks with smaller epsilons (Fig. 2a); (b) KAP model showed strong robustness to adversarial attacks on par with RN18-AT and even better performance against attacks with larger ε (Fig. 2b), while the variation with Kernel Max Pool was the least robust variation; c) larger activation noise variance during training led to higher robustness against stronger attacks (Fig. 2c). Furthermore, in separate experiments, we also investigated the effect of model depth and kernel ensemble size on robustness, which we report in the appendix §A.3. We found that increasing the network depth and KAP kernel size both substantially improve the network robustness to AutoAttack.\n\n255 to 32\n\n4.3 ROBUSTNESS AGAINST ADVERSARIAL ATTACKS ON IMAGENET\n\nTo test whether our results scale to larger datasets, we also trained and compared convolutional neural networks on two large-scale datasets, namely TinyImagenet (200 classes) and Imagenet (1000 classes). Here again, we used the ResNet18 architecture as our baseline and created variations of this architecture by adding KAP after every convolution operation. We found that reducing the weight decay parameter when training the KAP networks on Imagenet improves the performance of the KAP models and used a weight decay of 1e−5 in training our best models on Imagenet. In addition to the PGD-L∞ attack, we also evaluated the models using AutoAttack on these datasets. However, because of its high computational cost, we used 1000 random samples from the validation set.\n\nTable 2: Comparison of robust accuracy against various attacks on TinyImagenet and Imagenet datasets. For all attacks we used ε = 0.016. All attacks are performed on the corresponding model without input or activation noise. †: models trained using Fast Adversarial Training (Wong et al., 2019).\n\nDATASET\n\nTINYIMAGENET\n\nIMAGENET\n\nMODEL RN18-NT RN18-NT (σ = 0.1) RN18-AT-ES (RICE ET AL., 2020) RN18-KAP-NT (σ = 0.1, K = 3) RN18-NT RN18-NT (σ = 0.1) RN18-AT† RN18-KAP-NT (σ = 0.1, K = 3) RN18WIDEX4-AT† RN18WIDEX4-KAP-NT (σ = 0.1, K = 3)\n\nCLEAN 58.90 56.50 45.80 39.60 68.68 51.20 53.20 9.60 62.00 38.00\n\nPGD-L∞ AUTOATTACK\n\n0.00 1.90 25.40 25.70 0.0 0.19 8.00 6.05 27.81 31.49\n\n0.00 1.80 21.60 16.5 0.0 0.20 8.00 2.85 11.80 15.3\n\nSQUARE 3.30 35.50 29.10 18.70 2.80 6.70 8.20 2.95 14.10 14.40\n\nTable 2 summarizes the robust accuracy of different models on these two datasets. Overall, we found that (a) on these two datasets, sole usage of the activation noise similar to (He et al., 2019) was much less effective at improving the network robustness; (b) robustness in the KAP variation of ResNet18 was significantly better than the orignal network and baseline trained with noise but slightly lower than the adversarial trained network on TinyImagenet dataset. On Imagenet, we found that the KAP variation of ResNet18 model was struggling to learn the task completely, reaching only about 1012% accuracy on the clean dataset. We reasoned that the difficulty in learning the task on this dataset might be due to the large number of classes in this dataset and the possibility that the ResNet18\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nmodel does not have sufficient capacity to learn enough kernel ensembles to tackle this dataset. For this reason we also trained a wider version of RN18 in which we multiplied the number of kernels in each layer by a factor of 4 (dubbed RN18WideX4). We found that this wider network significantly boosted the robust accuracy on Imagenet, even surpassing the adversarially trained model. However, again we found that this boost to adversarial robustness is accompanied by a significant decrease in clean accuracy.\n\n(a)\n\n(b)\n\n(c)\n\nFigure 2: Robust accuracy in RN18-KAP model against AutoAttack with various attack strength ε on CIFAR10 dataset. (a) RN18 and RN18-KAP (σ = 0, K = 3); (b) RN18, RN18-KAP, RN18KMP (σ = 0.1, K = 3) and AT; (c) RN18-KAP (K = 3) for various noise levels σ.\n\n4.4 KAP YIELDS TOPOGRAPHICALLY ORGANIZED KERNELS\n\nAs described in the methods, KAP combines the output of multiple kernels into a single activation passed from one layer to the next. As this operation creates dependencies between multiple kernels within each group, we expected a certain level of similarity between the kernels within each pseudoensemble to emerge. To examine this, we visualized the weights in the first convolutional layer of the normally trained (NT), the adversarially trained RN18 architecture (AT) and two variations of RN18 with Kernel Average Pool (KAP) and Kernel Max Pool (KMP) on CIFAR10 and Imagenet datasets (Fig. 3). As a reminder for these experiments, we had incorporated a 2-dimensional KAP that was applied on the kernels arranged on a 2-dimensional sheet.\n\nAs expected, the kernels in NT and AT models did not show any topographical organization. Kernels in KMP model were sparsely distributed, with many kernels containing very small weights. This was presumably because of the competition amongst different kernels within each pseudo-ensemble that had driven the network to ignore many of its kernels. In contrast, in KAP model, we observed an overall topographical organization in the arrangement of the learned kernels along the two dimensional sheet. Moreover, in many cases the kernels gradually shifted from the dominant pattern in one cluster to another as traversing along either of the two kernel dimensions. This topographical organization of the kernels on the 2-dimensional sheet is reminiscent of the topographical organization of the orientation selectivity of neurons in the primary visual cortex of primates (Hubel & Wiesel, 1977).\n\nC10-NT\n\nC10-KMP\n\nC10-AT\n\nC10-KAP\n\nImagenet-AT Imagenet-KAP\n\nFigure 3: Visualization of the learned weights in the first layer of several variations of RN18 model.\n\n5 CONCLUSION\n\nWe proposed Kernel Average Pooling as a mechanism for learning ensemble of kernels in layers of deep neural networks. We showed that when combined with activation noise, KAP-networks form a process that can be thought of as recursive randomized smoothing with ensembles applied at the level of features, where each stage consists of applying ensemble of kernels followed by noise injection. Our empirical results demonstrated significant improvement in network robustness\n\n9\n\n0.050.10epsilon020accuracyArchitectureRN18-KAPRN18-KMPRN180.050.10epsilon050accuracyArchitectureRN18-KMPRN18RN18-KAPAT0.050.10epsilon050accuracyNoise std0.00.10.2ATUnder review as a conference paper at ICLR 2023\n\nat a fraction of computational cost of state-of-the-art methods like adversarial training. However, because of the need for learning ensemble of kernels at each network layer, the improved robustness is often accompanied by reduced performance on the clean datasets. Our results suggest featurelevel ensembling as a practical and scalable approach for training robust neural networks.\n\nREFERENCES\n\nMahdieh Abbasi, Arezoo Rajabi, Christian Gagn ́e, and Rakesh B Bobba. Toward adversarial robustness by diversity in an ensemble of specialized deep neural networks. In Canadian Conference on Artificial Intelligence, pp. 1–14. Springer, 2020.\n\nRishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caruana, and Geoffrey Hinton. Neural Additive Models: Interpretable Machine Learning with Neural Nets. In NeurIPS, 2021.\n\nMaksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-efficient black-box adversarial attack via random search. In European Conference on Computer Vision, pp. 484–501. Springer, 2020.\n\nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International conference on machine learning, pp. 274–283. PMLR, 2018.\n\nPhilip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. Advances in\n\nneural information processing systems, 27:3365–3373, 2014.\n\nPouya Bashivan, Reza Bayat, Adam Ibrahim, Kartik Ahuja, Mojtaba Faramarzi, Touraj Laleh, Blake Richards, and Irina Rish. Adversarial feature desensitization. Advances in Neural Information Processing Systems, 34:10665–10677, 2021.\n\nLeo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.\n\nLeo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.\n\nWieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.\n\nNicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural Networks. Pro-\n\nceedings - IEEE Symposium on Security and Privacy, pp. 39–57, 2017.\n\nJeremy Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via randomized smoothing. 36th International Conference on Machine Learning, ICML 2019, 2019-June:2323– 2356, 2019.\n\nFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206– 2216. PMLR, 2020.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nNicolas Ford, Justin Gilmer, Nicholas Carlini, and Ekin D. Cubuk. Adversarial examples are a\n\nnatural consequence of test error in noise. volume 2019-June, pp. 4115–4139, 2019.\n\nYoav Freund. Boosting a weak learning algorithm by majority. Information and computation, 121\n\n(2):256–285, 1995.\n\nYoav Freund, Robert E Schapire, et al. Experiments with a new boosting algorithm. Citeseer, 1996.\n\nIan Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout\n\nnetworks. In International conference on machine learning, pp. 1319–1327. PMLR, 2013.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. arXiv preprint arXiv:1412.6572, 2014.\n\nSven Gowal, Jonathan Uesato, Chongli Qin, Po-Sen Huang, Timothy Mann, and Pushmeet Kohli.\n\nAn Alternative Surrogate Loss for PGD-based Adversarial Testing. 2019.\n\nSven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Improving robustness using generated data. 2021. URL http://arxiv.\n\nTimothy Mann. org/abs/2110.09468.\n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learnin. Cited\n\non, pp. 33, 2009.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nZhezhi He, Adnan Siraj Rakin, and Deliang Fan. Parametric noise injection: Trainable randomness In Proceedings of the\n\nto improve deep neural network robustness against adversarial attack. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 588–597, 2019.\n\nGeoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdi-\n\nnov. Improving neural networks by preventing co-adaptation of feature detectors. 2012.\n\nMikl ́os Z Horv ́ath, Mark Niklas M ̈uller, Marc Fischer, and Martin Vechev. BOOSTING RANDOM-\n\nIZED SMOOTHING WITH VARIANCE REDUCED CLASSIFIERS. pp. 33, 2022.\n\nDavid Hunter Hubel and Torsten Nils Wiesel. Ferrier lecture-functional architecture of macaque monkey visual cortex. Proceedings of the Royal Society of London. Series B. Biological Sciences, 198(1130):1–59, 1977.\n\nHarini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint\n\narXiv:1803.06373, 2018.\n\nSanjay Kariyappa and Moinuddin K Qureshi. Improving adversarial robustness of ensembles with\n\ndiversity training. arXiv preprint arXiv:1901.09981, 2019.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\n\nDavid Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In International Conference on Machine Learning, pp. 5815–5826. PMLR, 2021.\n\nYa Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.\n\nMathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security and Privacy (SP), pp. 656–672. IEEE, 2019.\n\nChizhou Liu, Yunzhen Feng, Ranran Wang, and Bin Dong. Enhancing certified robustness via smoothed weighted ensembling, 2021. URL http://arxiv.org/abs/2005.09363. type: article.\n\nXuanqing Liu, Minhao Cheng, Huan Zhang, and Cho Jui Hsieh. Towards robust neural networks via random self-ensemble. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 11211 LNCS:381–397, 2018.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n\nSeyed Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-Decem:2574–2582, 2016.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nTianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via promoting ensemble diversity. 36th International Conference on Machine Learning, ICML 2019, 2019-June:8759–8771, 2019.\n\nTianyu Pang, Xiao Yang, Yinpeng Dong, Kun Xu, Jun Zhu, and Hang Su. Boosting adversarial training with hypersphere embedding. Advances in Neural Information Processing Systems, 2020December(NeurIPS), 2020.\n\nSylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, and Timothy Mann. Data augmentation can improve robustness. 2021. URL http://arxiv.org/abs/ 2111.05328.\n\nLeslie Rice, Eric Wong, and J. Zico Kolter. Overfitting in adversarially robust deep learning.\n\nPartF16814:8049–8074, 2020. ISBN: 9781713821120.\n\nAlexander Robey, Luiz F. O. Chamon, George J. Pappas, Hamed Hassani, and Alejandro Ribeiro.\n\nAdversarial Robustness with Semi-Infinite Constrained Learning. NeurIPS, pp. 1–18, 2021.\n\nAnindya Sarkar, Anirban Sarkar, Sowrya Gali, and Vineeth N Balasubramanian. Get Fooled for the Right Reason: Improving Adversarial Robustness through a Teacher-guided Curriculum Learning Approach. NeurIPS, 2021.\n\nVikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chiang, and Prateek Mittal. Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness? pp. 1–30, 2022.\n\nAli Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! Advances in Neural Information Processing Systems, 32(NeurIPS), 2019.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15:1929–1958, 2014.\n\nFlorian Tram`er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. 6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings, 2018.\n\nFlorian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to\n\nadversarial example defenses. arXiv preprint arXiv:2002.08347, 2020.\n\nHaohan Wang, Xindi Wu, Zeyi Huang, and Eric P. Xing. High-frequency component helps exIn 2020 IEEE/CVF Conference on plain the generalization of convolutional neural networks. Computer Vision and Pattern Recognition (CVPR), pp. 8681–8691. IEEE, 2020. ISBN 978-172817-168-5. doi: 10.1109/CVPR42600.2020.00871. URL https://ieeexplore.ieee. org/document/9156428/.\n\nSiyue Wang, Xiao Wang, Pu Zhao, Wujie Wen, David Kaeli, Peter Chin, and Xue Lin. Defensive In Proceedings of the dropout for hardening deep neural networks under adversarial attacks. International Conference on Computer-Aided Design, pp. 1–8, 2018. doi: 10.1145/3240765. 3264699. URL http://arxiv.org/abs/1809.05165.\n\nYeming Wen, Dustin Tran, and Jimmy Ba. BatchEnsemble: An alternative approach to efficient\n\nensemble and lifelong learning, 2020. URL http://arxiv.org/abs/2002.06715.\n\nEric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training.\n\nIn International Conference on Learning Representations, 2019.\n\nMitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael GontijoLopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. 2022.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nCihang Xie, Yuxin Wu, Laurens Van Der Maaten, Alan L. Yuille, and Kaiming He. Feature denoising for improving adversarial robustness. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June:501–509, 2019.\n\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning, pp. 7472–7482. PMLR, 2019.\n\nZhi Hua Zhou, Jianxin Wu, and Wei Tang. Ensembling neural networks: Many could be better than\n\nall. Artificial Intelligence, 137:239–263, 2001. doi: 10.1016/j.artint.2010.10.001.\n\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8697–8710, 2018.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1\n\n2-DIMENSIONSAL KAP\n\nGiven an input z ∈ RD×Nk , where Nk = Nr × Nc is the number of kernels that can be rearranged into Nr rows and Nc columns, and D denotes the input dimension, the 2D kernel average pool operation with kernel size K × K and stride S, computes the function\n\n ̄zik(x) =\n\n1 K 2\n\n⌊ Sk Nc\n\n⌋+ K−1 (cid:88)\n\n2\n\n(Sk mod Nc)+ K−1\n\n2(cid:88)\n\nl=⌊ Sk Nc\n\n⌋− K−1\n\n2\n\nm=(Sk mod Nc)− K−1\n\n2\n\nzi(lNc+m)\n\n(10)\n\nGraphically, this procedure is visualized in Fig. A1.\n\nFigure A1: Graphic illustration of 2-dimensional KAP. a) the input tensor is first reshaped such that the spatial dimensions are collapsed onto a single dimension and the kernel dimension is rearranged as a matrix. A 2D average pool is applied on the reshaped tensor, and the resulting tensor is reshaped back into its original shape; b) the average kernel is applied per spatial position (i.e. a pixel) and computes the average of nearby kernel values at that spatial position.\n\nA.2 CLASSIFICATION ROBUSTNESS FOR DIRECT ATTACKS ON FULL MODELS WITH\n\nACTIVATION NOISE\n\nFor completeness of our assessments, we also considered the case where the attacker is applied to the full model with activation noise. Tables A1 and A2 summarize the robust accuracies on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets. Compared to the case where the attackers were applied on the model without activation noise, the KAP models achieved higher robust accuracies against attacks performed on the full model.\n\nWe reasoned that this improvement in robustness is potentially because of the test-time stochasticity in layer activations and its possible detrimental effect on gradient estimation in attacks that only use a single sample. To further investigate this, we also tested all of our models against the random version of AutoAttack (AutoAttackrnd) in addition to the standard version of AutoAttack (AutoAttackstd). This attack applies Expectation over Transformation introduced by (Athalye et al., 2018) to correctly compute the gradients over the expected transformation to the input (apgdce and apgd-dlr attacks, each with 20 iterations). We found that KAP models were also equally robust against the random version of AutoAttack.\n\nA.3 EFFECT OF NETWORK DEPTH AND KERNEL ENSEMBLE SIZE ON NETWORK ROBUSTNESS\n\nWe performed three additional experiments to investigate the effect of network depth, number of KAP layers, and kernel ensemble size (which is controlled by KAP kernel size) on the network robustness. In the first experiment, we varied the network depth (number of convolutional blocks)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable A1: Comparison of adversarial accuracy against various attacks on CIFAR10 and CIFAR100 datasets where each attack is performed on the full model with activation noise. For all attacks we used ε = 0.031. AutoAttackrnd results for stochastic models are reported for 20 and 50 (in parentheses) EoTs.\n\nDATASET\n\nCIFAR10\n\nCIFAR100\n\nMODEL RN18-NT RN18-NT (σ = 0.1) RN18-AT-ES RN18-KAP-NT (σ = 0.1, K = 3) RN18-KAP-NT (σ = 0.2, K ∈ {3}) RN18-NT RN18-NT (σ = 0.1) RN18-AT-ES RN18-KAP-NT (σ = 0.1, K ∈ {3}) RN18-KAP-NT (σ = 0.2, K ∈ {3})\n\nCLEAN 94.66 88.95 84.20 79.09 74.30 74.00 61.60 56.50 48.20 38.60\n\nPGD-L∞ AUTOATTACKstd AUTOATTACKrnd\n\n0.0 10.91 43.70 67.55 61.82 0.0 5.23 20.40 36.08 31.49\n\n0.0 8.79 43.00 58.40 55.19 0.0 4.06 19.60 23.70 23.36\n\n0.0 8.30 (6.20) 43.38 61.64 (56.1) 57.04 (57.80) 0.0 4.30 (4.40) 19.88 30.52 (28.20) 31.20 (31.00)\n\nSQUARE 0.87 84.42 49.10 66.12 63.64 0.24 49.46 22.86 29.80 27.40\n\nTable A2: Comparison of robust accuracy against various attacks on TinyImagenet and Imagenet datasets where each attack is performed on the full model with activation noise. For all attacks we used ε = 0.016. †: models trained using Fast Adversarial Training (Wong et al., 2019).\n\nDATASET\n\nTINYIMAGENET\n\nIMAGENET\n\nMODEL RN18-NT RN18-NT (σ = 0.1) RN18-AT-ES RN18-KAP-NT (σ = 0.1, K ∈ {3}) RN18-NT RN18-NT (σ = 0.1) RN18-AT† RN18-KAP-NT (σ = 0.1, K ∈ {3}) RN18WIDEX4-AT† RN18WIDEX4-KAP-NT (σ = 0.1, K ∈ {3})\n\nCLEAN 58.90 56.50 45.80 39.60 68.68 51.20 53.20 9.60 62.00 38.00\n\nPGD-L∞ AUTOATTACKstd AUTOATTACKrnd\n\n0.12 0.26 25.40 27.24 0.0 0.19 19.23 9.14 27.81 30.91\n\n0.02 0.16 21.60 20.78 0.0 0.20 6.00 3.00 11.80 21.40\n\n0.04 0.38 21.92 22.50 0.0 0.0 6.00 4.80 11.60 31.00\n\nbetween 1 to 3 layers while keeping the network width fixed. We observed that increasing the network depth improves its robustness against AutoAttack for both KAP and non-KAP networks. However, this improvement was much more pronounced for KAP networks (Fig. A2a).\n\nSince the number of convolutional and KAP layers were co-varying in the previous experiment, it is possible that the observed improvement in network robustness was solely due to the increasing network depth. To investigate this, in the second experiment, we kept the network depth fixed while changing the number of KAP layers. We trained the following three variations of a 3-layer CNNs: 1) one KAP layer after first convolutional layer; 2) one KAP layer after first and second convolutional layers; 3) one KAP layer after each of the three convolutional layers. In all 3 architectures, the KAP layer consisted of 3 × 3 kernel with stride 1. Activation noise with σ = 0.1 was added after each KAP layer. We validated the adversarial accuracy of each of these models with AutoAttack-L∞ with varying epsilon (Fig. A2b). We observed that while the number of convolutional layers was fixed, adding more KAP layers consistently improved the network robustness against AutoAttack.\n\nIn the third experiment, we varied the KAP kernel size between 1 to 4 to investigate the effect of larger ensemble sizes on the network robustness. In each network, we set the KAP stride equal to the KAP kernel size to avoid overlapping between ensembles, and also increased the network width by the same factor as the KAP kernel size to allow each network to learn the same number of kernel ensembles. We found that increasing the KAP kernel size and consequently the ensemble size further improves the network robustness against AutoAttack (Fig. A2c). The resulting kernel ensembles in each of these models are shown in Fig.A3.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\nFigure A2: Effect of network depth and kernel ensemble size on the robustness to AutoAttack (ε = 0.031) on CIFAR10 dataset. a) increasing the number of layers in a CNN from 1-3 significantly improves its robustness to AutoAttack in networks with KAP compared to networks without KAP; b) increasing the number of KAP layers while keeping the number of convolutional layers fixed (3) improves robustness to AutoAttack;c) increasing the kernel ensemble size by increasing the KAP kernel size (expansion) improves the network robust accuracy against AutoAttack. All attacks are performed on the corresponding model without noise.\n\nK=1\n\nK=2\n\nK=3\n\nK=4\n\nFigure A3: Visualization of the learned weights in the first layer of 3-layer convolutional networks with varying KAP kernel size K.\n\nTable A5: Robust accuracy against transfer attacks on CIFAR10. Attacks were generated using each Reference Model and tested on the Model. For all attacks we used ε = 0.031.\n\nMODEL\n\nREFERENCE MODEL AUTOATTACK\n\nRN18-KAP-NT (σ = 0.1, K = 3)\n\nRN18-KAP-NT (σ = 0.2, K = 3)\n\nRN18-NT RN18-AT RN18-NT RN18-AT\n\n76.4 66.19 68.6 61.84\n\n16\n\n0.020.040.06epsilon010203040accuracyLayer TypeW/O KAPKAPArchitecture1 layer CNN2 layer CNN3 layer CNN0.020.030.040.050.06epsilon10203040accuracy3layerCNN-1KAP3layerCNN-2KAP3layerCNN-3KAP0.000.050.10epsilon204060accuracyWidth Factor1234Under review as a conference paper at ICLR 2023\n\nFigure A4: Distribution of layer activations in a random kernel for RN18-NT(σ = 0.1) (top) and RN18-NT-KAP(σ = 0.1) (bottom) trained on CIFAR10 dataset. We extracted the layer activations in response to a single random input image perturbed 100 times with random Gaussian noise or AutoAttack. Distributions of layer activations to noise and adversarial examples exhibit increased divergence in Block 4 in RN18-NT model but not in RN18-NT-KAP. The same pattern is replicated when considering other randomly selected input images from the validation set.\n\nFigure A5: Distribution of perturbation magnitude in layer activations for RN18-NT(σ = 0.1) (top) and RN18-NT-KAP(σ = 0.1) (bottom) trained on CIFAR10 dataset. We extracted the layer activations in response to a single random input image perturbed 100 times with random Gaussian noise or AutoAttack. Perturbation magnitude is computed as the L2 distance between perturbed and clean i) − f (l)(x) for adversarial perturbainput activations (f (l)(x + ni) − f (l)(x) for noise and f (l)(x′ tions). Distributions of layer activations to noise and adversarial examples exhibits divergence in later blocks in RN18-NT model but not in RN18-NT-KAP.\n\n17\n\n0.00.5activation05Block101activation012Block20.00.5activation050100Block301activation05Block4123activation0.00.51.002activation01201activation050.00.5activation0200400noiseadversarial100150distance0.00.2Block16070distance0.00.20.4Block227.530.0distance0.00.51.0Block35075distance0.00.2Block43040distance0.00.20.4Logits300400distance0.000.020.04150175distance0.000.0590100110distance0.00.1354045distance0.00.212.515.017.5distance0.00.2noiseadversarialUnder review as a conference paper at ICLR 2023\n\nTable A3: Attack hyperparameters used to validate model robustness on each dataset.\n\nAttack\n\nPGD-L∞\n\nAutoAttackstd\n\nAutoAttackrnd\n\nSQUARE\n\nDataset CIFAR TinyImagenet Imagenet CIFAR TinyImagenet Imagenet CIFAR TinyImagenet Imagenet CIFAR TinyImagenet Imagenet\n\nSteps\n\n20\n\n100\n\n100\n\n5000\n\nSize (ε) 8\n255 4\n255 4\n255 8\n255 4\n255 4\n255 8\n255 4\n255 4\n255 8\n255 4\n255 4\n255\n\n255\n\nMore step= 2 step= 2 step= 1 default standard AA - APGD-ce, APGD-t, FAB, SQUARE\n\n255\n\n255\n\ndefault random AA - APGD-ce and APGD-dlr, each with 20 EoT iterations\n\ndefault SQUARE setting\n\nTable A4: Comparison of training speed between alternative models. All training times were computed on the CIFAR10 dataset and ResNet18 architecture using a single A100 GPU.\n\nDataset\n\nModel\n\nAve. Training Speed / Epoch\n\nCIFAR10\n\nRN18-NT RN18-AT RN18-KAP-NT (σ = 0.1, K = 3)\n\n12.5 ± 0.5 sec 100.75 sec 14.5 ± 0.5 sec\n\nFigure A6: Normalized magnitude of change in each layer’s activations in response to adversarial perturbations ( ∥yadv−ycln∥ ) for AutoAttack L2, ε = 1. on CIFAR10 dataset for RN18-NT (σ = 0.1) and RN18-KAP-NT (σ = 0.1).\n\n∥ycln∥\n\n18\n\nlayer1layer2layer3layer4linear0.00.51.0ynxRN18RN18-KAPUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\nFigure A7: Robust accuracy in RN18-KAP model against AutoAttack-L2 with various attack strength ε on CIFAR10 dataset. (a) RN18 and RN18-KAP (σ = 0, K = 3); (b) RN18, RN18KAP, RN18-KMP (σ = 0.1, K = 3) and AT; (c) RN18-KAP (K = 3) for various noise levels σ.\n\nFigure A8: Certified accuracy of RN18-NT (σ = 0.1) and RN18-NT-KAP (σ = 0.1) models on 500 samples from CIFAR10 dataset for varying amount of noise levels σ. Certified accuracy was estimated using the Monte Carlo procedure from (Cohen et al., 2019) using 100 samples for selection and 100000 samples for estimation.\n\nTable A6: Comparison of adversarial accuracy against AutoAttack-L∞ with ε = 0.031 on CIFAR10 for variations of the models with and without activation and input noise.\n\nDATASET\n\nMODEL\n\nINPUT NOISE ACTIVATION NOISE AUTOATTACKstd\n\nCIFAR10\n\nRN18-NT (σ = 0.1)\n\nCIFAR10 RN18-KAP-NT (σ = 0.2, K ∈ {3})\n\n× ✔\n× ✔\n× ✔\n× ✔\n\n5.00 8.90 16.9 21.10 8.3 41.80 57.30 60.70\n\n× ×\n✔ ✔\n× ×\n✔ ✔\n\n19\n\n1234epsilon0510accuracyArchitectureRN18RN18-KMPRN18-KAP1234epsilon02550accuracyArchitectureRN18RN18-KMPRN18-KAPAT1234epsilon02550accuracyNoise std0.00.10.2AT0.10.20.30.4Noise 0.250.500.75Certifeid AccuracyRN18RN18-KAP",
    "reference": "# Summary Of The Paper\n\nThis paper proposes kernel average pooling operation to improve model robustness.\n\n# Strength And Weaknesses\n\nStrength\n- The proposed operation is simple and economic.\n\nWeaknesses\n- The paper is not well-presented and well-written. \n   -  In the introduction section, it jumps direction into \"our central premise in this work\" to talk about ensemble without any hints or smooth transition.\n   - Especially, the section about kernel average pool is not clear and solid. Equation (3) does not make sense to me because $w_i.x$ for all $i$. It is hard to interpret this operation. The authors should give more context about the shape of $x$, $z$ and so on. In Algorithm 1, it seems that KAP is average pooling over the depth. So it is not novel to me. Additionally, after applying average pooling over the depth, the output tensor gets smaller, how you can get back the shape W,H,D as in the last line of Algorithm 1.\n- There are some vague arguments to me, for example \"individual kernels within a network are robust\". How can we justify if a kernel is robust?\n- How to interpret and understand Equation (9) because I cannot see any random factor in KAP.\n- The experiments are humble without comparing to other SOTA baselines.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- The KAP operation seems not novel. It is unclear how it contributes to improve model robustness.\n- The technicality of KAP is not clear and it lacks strong discussions about KAP in improving robustness.\n- It lacks the comparison to other baselines.\n\n# Summary Of The Review\n\nThis paper considers KAP operation to improve model robustness. It says that KAP focuses on learning feature ensembles that form local “committees” similar to those used in Boosting and Random Forests. However, I cannot see how KAP realizes feature ensembles. It seems to me that KAP is average pooling over the depth.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nThere is no ethics concerns."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nLEARNING ZERO-SHOT COOPERATION WITH HUMANS, ASSUMING HUMANS ARE BIASED\n\nChao Yu1∗, Jiaxuan Gao1,3∗, Weilin Liu1, Botian Xu3, Hao Tang1, Jiaqi Yang2, Yu Wang1†, Yi Wu1,3† 1 Tsinghua University, 2 UC Berkeley, 3 Shanghai Qi Zhi Institute zoeyuchao@gmail.com\n\nABSTRACT\n\nThere is a recent trend of applying multi-agent reinforcement learning (MARL) to train an agent that can cooperate with humans in a zero-shot fashion without using any human data. The typical workflow is to first repeatedly run self-play (SP) to build a policy pool and then train the final adaptive policy against this pool. A crucial limitation of this framework is that every policy in the pool is optimized w.r.t. the environment reward function, which implicitly assumes that the testing partners of the adaptive policy will be precisely optimizing the same reward function as well. However, human objectives are often substantially biased according to their own preferences, which can differ greatly from the environment reward. We propose a more general framework, Hidden-Utility Self-Play (HSP), which explicitly models human biases as hidden reward functions in the self-play objective. By approximating the reward space as linear functions, HSP adopts an effective technique to generate an augmented policy pool with biased policies. We evaluate HSP on the Overcooked benchmark. Empirical results show that our HSP method produces higher rewards than baselines when cooperating with learned human models, manually scripted policies, and real humans. The HSP policy is also rated as the most assistive policy based on human feedback.\n\n1\n\nINTRODUCTION\n\nBuilding intelligent agents that can interact with, cooperate and assist humans remains a longstanding AI challenge with decades of research efforts (Klien et al., 2004; Ajoudani et al., 2018; Dafoe et al., 2021). Classical approaches are typically model-based, which (repeatedly) build an effective behavior model over human data and plan with the human model (Sheridan, 2016; Carroll et al., 2019; Bobu et al., 2020). Despite great successes, this model-based paradigm requires an expensive and time-consuming data collection process, which can be particularly problematic for complex problems tackled by today’s AI techniques (Kidd & Breazeal, 2008; Biondi et al., 2019) and may also suffer from privacy issues (Pan et al., 2019).\n\nRecently, multi-agent reinforcement learning (MARL) has become a promising approach for many challenging decision-making problems. Particularly in competitive settings, AIs developed by MARL algorithms based on self-play (SP) defeated human professionals in a variety of domains (Silver et al., 2018; Vinyals et al., 2019; Berner et al., 2019). This empirical evidence suggests a new direction of developing strong AIs that can directly cooperate with humans in a similar “model-free” fashion, i.e., via self-play.\n\nDifferent from zero-sum games, where simply adopting a Nash equilibrium strategy is sufficient, an obvious issue when training cooperative agents by self-play is convention overfitting. Due to the existence of a large number of possible optimal strategies in a cooperative game, SP-trained agents can easily converge to a particular optimum and make decisions solely based on a specific behavior pattern, i.e., convention (Lowe et al., 2019; Hu et al., 2020), of its co-trainers, leading to poor generalization ability to unseen partners. To tackle this problem, recent works proposed a two-staged framework by first developing a diverse policy pool consisting of multiple SP-trained policies, which possibly cover different conventions, and then further training an adaptive policy against this policy pool (Lupu et al., 2021; Strouse et al., 2021; Zhao et al., 2021).\n\nDespite the empirical success of this two-staged framework, a fundamental drawback exists. Even though the policy pool prevents convention overfitting, each SP-trained policy in the pool remains a solution, which is either optimal or sub-optimal, to a fixed reward function specified by the underlying cooperative game. This implies a crucial generalization assumption that any test-time partner\n\n∗Equal Contribution †Equal Advising\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nwill be precisely optimizing the specified game reward. Such an assumption results in a pitfall in the case of cooperation with humans. Human behavior has been widely studied in cognitive science (Griffiths, 2015), economics (Wilkinson & Klaes, 2017) and game theory (Fang et al., 2021). Systematic research has shown that humans’ utility functions can be substantially biased even when a clear objective is given (Pratt, 1978; Selten, 1990; Camerer, 2011; Barberis, 2013), suggesting that human behaviors may be subject to an unknown reward function that is very different from the game reward (Nguyen et al., 2013). This fact reveals an algorithmic limitation of the existing SP-based methods.\n\nIn this work, we propose Hidden-Utility Self-Play (HSP), which extends the SP-based two-staged framework to the assumption of biased humans. HSP explicitly models the human bias via an additional hidden reward function in the self-play training objective. Solutions to such a generalized formulation are capable of representing any non-adaptive human strategies. We further present a tractable approximation of the hidden reward function space and perform a random search over this approximated space when building the policy pool in the first stage. Hence, the enhanced pool can capture a wide range of possible human biases beyond conventions (Hu et al., 2020; Zhao et al., 2021) and skill-levels (Dafoe et al., 2021) w.r.t. the game reward. Accordingly, the final adaptive policy derived in the second phase can have a much stronger adaptation capability to unseen humans.\n\nWe evaluate HSP in a popular human-AI cooperation benchmark, Overcooked (Carroll et al., 2019), which is a fully observable two-player cooperative game. We conduct comprehensive ablation studies and comparisons with baselines that do not explicitly model human biases. Empirical results show that HSP achieves superior performances when cooperating with behavior models learned from human data. In addition, we also consider a collection of manually scripted biased strategies, which are ensured to be sufficiently distinct from the policy pool, and HSP produces an even larger performance improvement over the baselines. Finally, we conduct real human studies. Collected feedbacks show that the human participants consistently feel that the agent trained by HSP is much more assistive than the baselines. We emphasize that, in addition to algorithmic contributions, our empirical analysis, which considers learned models, script policies and real humans as diverse testing partners, also provides a more thorough evaluation standard for learning human-assistive AIs.\n\n2 RELATED WORK\n\nThere is a broad literature on improving the zero-shot generalization ability of MARL agents to unseen partners (Kirk et al., 2021). Particularly for cooperative games, this problem is often called ad hoc team play (Stone et al., 2010) or zero-shot cooperation (ZSC) (Hu et al., 2020). Since most existing methods are based on self-play (Rashid et al., 2018; Yu et al., 2021), how to avoid convention overfitting becomes a critical challenge in ZSC. Representative works include improved policy representation (Zhang et al., 2020; Chen et al., 2020), randomization over invariant game structures (Hu et al., 2020; Treutlein et al., 2021), population-based training (Long* et al., 2020; Lowe* et al., 2020; Cui et al., 2021) and belief modeling for partial observable settings (Hu et al., 2021; Xie et al., 2021). Fictitious co-play (FCP) (Strouse et al., 2021) proposes a two-stage framework by first creating a pool of self-play policies and their previous versions and then training an adaptive policy against them. Some techniques improves the diversity of the policy pool (Garnelo et al., 2021; Liu et al., 2021; Zhao et al., 2021; Lupu et al., 2021) for a stronger adaptive policy (Knott et al., 2021).\n\nWe follow the FCP framework and augment the policy pool with biased strategies. Notably, techniques for learning a robust policy in competitive games, such as policy ensemble (Lowe et al., 2017), adversarial training (Li et al., 2019) and double oracle (Lanctot et al., 2017), are complementary to our focus.\n\nBuilding AIs that can cooperate with humans remains a fundamental challenge in AI (Dafoe et al., 2021). A critical issue is that humans can be systematically biased (Camerer, 2011; Russell, 2019). Hence, great efforts have been made to model human biases, such as irrationality (Selten, 1990; Bobu et al., 2020; Laidlaw & Dragan, 2022), risk aversion (Pratt, 1978; Barberis, 2013), and myopia (Evans et al., 2016). Many popular models further assume humans have hidden subject utility functions (Nguyen et al., 2013; Hadfield-Menell et al., 2016; Eckersley, 2019; Shah et al., 2019). Conventional methods for human-AI collaboration require an accurate behavior model over human data (Ajoudani et al., 2018; Kwon et al., 2020; Kress-Gazit et al., 2021; Wang et al., 2022), while we consider the setting of no human data. Hence, we explicitly model human biases as a hidden utility function in the self-play objective to reflect possible human biases beyond conventions w.r.t. optimal rewards. We prove that such a hidden-utility model can represent any strategy of nonadaptive humans. Notably, it is also feasible to generalize our model to capture higher cognitive hierarchies (Camerer et al., 2004), which we leave as a future direction.\n\nWe approximate the reward space by a linear function space over event-based features. Such a linear representation is typical in inverse reinforcement learning (Ng & Russell, 2000), policy trans-\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nfer (Barreto et al., 2017b), evolution computing (Cully et al., 2015) and game theory (Winterfeldt & Fischer, 1975; Kiekintveld et al., 2013). Event-based rewards are also widely adopted as a general design principle in robot learning (Fu et al., 2018; Zhu et al., 2019; Ahn et al., 2022). We perform randomization over feature weights to produce diverse biased strategies. Similar ideas have been adopted in other settings, such as generating adversaries (Paruchuri et al., 2006), emergent teamformation (Baker, 2020), and searching for diverse Nash equilibria in general-sum games (Tang et al., 2020). In our implementation, we use multi-reward signals as an approximate metric to filter out duplicated policies, which is inspired by the quality diversity method (Pugh et al., 2016). There are also some works utilizing model-based methods to solve zero-shot cooperation Wu et al. (2021). Their focus is orthogonal to our approach since they focus more on constructing an adaptive agent, while our approach aims to find more diverse strategies. Besides, we adopt an end-to-end fashion to train an adaptive agent, which is more general. Lastly, our final adaptive agent assumes a zero-shot setting without any data from its testing partner. This can be further extended by allowing metaadaptation at test time (Charakorn et al., 2021; Gupta et al., 2021; Nekoei et al., 2021), which we leave as a future direction.\n\n3 PRELIMINARY\n\nTwo-Player Human-AI Cooperative Game: A human-AI cooperative game is defined on a world model, i.e., a two-player Markov decision process denoted by M = ⟨S, A, P, R⟩, with one player with policy πA being an AI and the other with policy πH being a human. S is a set of world states. A is a set of possible actions for each player. P is a transition function over states given the actions from both players. R is a global reward function. A policy πi produces an action a(i) t ∈ A given a world state st ∈ S at the time step t. We use the expected discounted return J(πA, πH ) = E as the objective. Note that J(πH , πA) can be similarly defined, and we use J(πA, πH ) for conciseness without loss of generality. Let PH : Π → [0, 1] be the unknown distribution of human policies. The goal is to find a policy πA that maximizes the expected return with an unknown human, i.e., EπH ∼PH [J(πH , πA)]. In practice, many works construct or learn a policy distribution ˆPH to approximate real-world human behaviors, leading to an approximated objective for πA, i.e., E\n\nt γtR(st, a(A)\n\n[J(πA, ˆπH )].\n\n, a(H)\n\nst,a(i)\n\n(cid:104)(cid:80)\n\n(cid:105) )\n\nt\n\nt\n\nt\n\nˆπH ∼ ˆPH\n\nSelf-Play for Human-AI Cooperation: Self-play (SP) optimizes J(π1, π2) with two parametric policies π1 and π2 and takes π1 as πA without use of human data. However, SP suffers from poor generalization since SP converges to a specific optimum and overfits the resulting behavior convention. Population-based training (PBT) improves SP by representing πi as a mixture of K individual policies {π(k) k=1 and runs cross-play between policies by optimizing the expected return (Long* et al., 2020; Lowe* et al., 2020; Cui et al., 2021). PBT can be further improved by adding a diversity bonus over the population (Garnelo et al., 2021; Liu et al., 2021; Lupu et al., 2021).\n\ni }K\n\n2 )}K\n\n1 , π(k)\n\n1 , π(k)\n\n2 , denoted by ̃π(k)\n\nk=1 by optimizing J(π(k)\n\nFictitious Co-Play (FCP): FCP (Strouse et al., 2021) is a recent work on zero-shot human-AI cooperation with strong empirical performances. FCP extends PBT via a two-stage framework. In the first stage, FCP trains K individual policy pairs {(π(k) 1 , π(k) 2 ) for each k. Each policy pair (π(k) 2 ) may quickly converge to a distinct local optimum. Then FCP constructs a policy pool Π2 = { ̃π(k) k=1 with two past versions of each converged SP policy π(k) 2 . In the second stage, FCP constructs a human proxy distribution ˆPH by randomly sampling from Π2 and trains πA by optimizing E [J(πA, ˆπH )]. We remark that, for a better cooperation, the adaptive policy πA should condition on the state-action history in an episode to infer the intention of its partner. Individual SP policies ensure ˆPH contains diverse conventions while using past versions enables ˆPH to cover different skill levels. So, the final policy πA can be forced to adapt to humans with unknown conventions or sub-optimalities. Maximum Entropy Population-based Training (MEP) (Zhao et al., 2021) is the latest variant of FCP, which adopts the population entropy as a diversity bonus in the first stage to improve the generalization of the learned πA.\n\n2 , π(k)\n\n2 }K\n\nˆπH ∼ ˆPH\n\n4 COOPERATING WITH HUMANS IN Overcooked: A MOTIVATING EXAMPLE\n\nOvercooked Game: Overcooked (Carroll et al., 2019) is a fully observable two-player cooperative game developed as a testbed for human-AI cooperation. In Overcooked, players cooperatively accomplish different soup orders and serve the soups for rewards. Basic game items include onions, tomatoes, and dishes. An agent can move in the game or “interact” to trigger some events, such as grabbing/putting an item, serving soup, etc., depending on the game state. To finish an order, players\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Layouts in Overcooked. From left to right are Asymmetric Advantages, Coordination Ring, Counter Circuit, Distant Tomato and Many Orders respectively, with orders shown below.\n\nshould put a proper amount of ingredients into the pot and cook for some time. Once a soup is finished, players should pick up the soup with a dish and serve it to get a reward. Different orders have different cooking times and different rewards. Fig. 1 demonstrates five layouts we consider, where the first three onion-only layouts are adopted from (Carroll et al., 2019), while the latter two, Distant Tomato and Many Orders, are newly introduced to include tomato orders to make the problem more challenging: an AI needs to carefully adapt its behavior to either cook onions or tomatoes according to the other player’s actions.\n\nA Concrete Example of Human Preference: Fig. 2 illustrates a motivating example in Distant Tomato (the 4th layout in Fig. 1). There are two orders: one requires three onions, and the other requires three tomatoes. We run FCP on this multi-order scenario, and all the policies in the FCP policy pool converge to the specific pattern of only cooking onion soup (Fig. 2a). Hence, the final adaptive policy by FCP only learns to grab onions and cook onion soups. Cooking tomato soup is a sub-optimal strategy that requires many extra moves, so the onion-only policy pool is exactly the solution to the FCP self-play objective under the environment reward. However, it is particularly reasonable for a human to dislike onions and accordingly only grab tomatoes in a game. To be an assistive AI, the policy should adapt its strategy to follow the human preference for tomatoes. On the contrary, as shown in Fig. 2b, the FCP policy completely ignores human moves for tomatoes and even results in poor cooperation by producing valueless wrong orders of mixed onions and tomatoes. Thus, to make an FCP agent human-assistive, the first-stage policy pool should not only contain optimal strategies (i.e., onion soups) of different conventions but also cover diverse human preferences (e.g., tomatoes) even if these preferences are sub-optimal under the environment reward.\n\n(a) FCP-FCP\n\n(b) FCP’s failure case when cooperating with a human player\n\nFigure 2: Motivating example. (a) FCP converges to the optimal onion soup strategy. (b) A failure case of FCP with a human partner: FCP agent corrupts the human’s plan of cooking tomato soups.\n\n5 METHODOLOGY\n\nWe introduce a general formulation to model human preferences and develop a tractable learning objective (Sec. 5.2). The algorithm, Hidden-Utility Self-Play (HSP), is summarized in Sec. 5.3.\n\n5.1 HIDDEN-UTILITY MARKOV GAME\n\nThe key insight from Sec. 4 is that humans may not truthfully behave under the environment reward. Instead, humans are biased and driven by their own utility functions, which are formulated below.\n\nDefinition: A two-player hidden utility Markov game is defined as ⟨S, A, P, Rw, Rt⟩. ⟨S, A, P, Rt⟩ corresponds to the original game MDP with Rt being the task reward function. Rw denotes an additional hidden reward function. There are two players, πa, whose goal is to maximize the task reward Rt, and πw, whose goal is to maximize the hidden reward Rw. Rw is only visible to πw.\n\na, π′\n\na, π∗ w|Rw) ≥ J(π∗\n\nLet J(π1, π2|R) denote the expected return under reward R with a policy π1 and π2. During self-play, πa optimizes J(πa, πw|Rt) while πw optimizes J(πa, πw|Rw). A solution policy profile (π∗ w) to the hidden utility Markov game is now defined by a Nash equilibrium (NE): □\na, π∗ J(π∗\n\na, π∗ Intuitively, with a suitable hidden reward function Rw, we can obtain any possible (non-adaptive and consistent) human policy by solving the hidden-utility game induced by Rw. Lemma 5.1. Given an MDP M = ⟨S, A, P, Rt⟩, for any policy π : S × A → [0, 1], there exists a hidden reward function Rw such that the two-player hidden utility Markov game M ′ = ⟨S, A, P, Rw, Rt⟩ has a Nash equilibrium (π∗\n\nw|Rt) ≥ J(π′\n\nw|Rt), ∀π′ a.\n\nw and J(π∗\n\nw|Rw), ∀π′\n\nw) where π∗\n\nw = π.\n\na, π∗\n\na, π∗\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nLemma 5.1 connects any human behavior to a hidden reward function. Then the objective of the adaptive policy πA in Eq. (3) can be formulated under the hidden reward function space R as follows. Theorem 5.1. For any ε > 0, there exists a mapping ̃πw where ̃πw(Rw) denotes the derived policy π∗ w in the NE of the hidden utility Markov game Mw = ⟨S, A, P, Rw, Rt⟩ induced by Rw, and a distribution PR : R → [0, 1] over the hidden reward space R, such that, for any adaptive policy πA ∈ arg maxπ′ ERw∼PR [J(π′, ̃πw(Rw))], πA approximately maximizes the ground-truth objective with at most an ε gap, i.e., EπH ∼PH [J(πA, πH )] ≥ maxπ′ EπH ∼PH [J(π′, πH )] − ε.\n\nTheorem 5.1 indicates that it is possible to derive diverse human behaviors by properly designing a hidden reward distribution ˆPR, which can have a much lower intrinsic dimension than the policy distribution. In Overcooked, human preferences can be typically described by a few features, such as interaction with objects or certain type of game events, like finishing an order or delivering a soup. By properly approximating the hidden reward distribution as ˆPR, the learning objective becomes,\n\nπA = arg max\n\nπ′\n\nE\n\nRw∼ ˆPR\n\n[J(π′, ̃πw(Rw))]\n\n(1)\n\nEq. (1) naturally suggests a two-staged solution by first constructing a policy pool { ̃πw(R) : R ∼ ˆPR} from ˆPR and then training πA to maximize the game reward w.r.t. the induced pool.\n\n5.2 CONSTRUCT A POLICY POOL OF DIVERSE PREFERENCES\n\nEvent-based Reward Function Space: The fundamental question is how to design a proper hidden reward function space R. Inspired by the fact that human preferences are often event-centric, we formulate R as linear functions over event features, namely R = {Rw : Rw(s, a1, a2) = φ(s, a1, a2)T w, ||w||∞ ≤ Cmax}. Cmax is a bound on the feature weight w while φ : S × A × A → Rm specifies occurrences of different game events when taking joint action (a1, a2) at state s.\n\nIn general, a valid reward space is intractably large.\n\nDerive a Diverse Set of Biased Policies: We simply perform a random search over the feature weight w to derive a set of diverse behaviors. We first draw N samples {w(i)}i∈[N ] for the feature weight w where w(i) is sampled uniformly from a set of values Cj, leading to a set of hidden reward w (s, a1, a2) = φ(s, a1, a2)T w(i)}i∈[N ]. For each hidden reward function R(i) w : R(i) functions {R(i) w , we find an approximated NE, π(i) w through self-play. The above process produces a policy pool {π(i) w }i∈[N ] that can cover a wide range of behavior preferences.\n\na , of the hidden utility Markov game induced by R(i)\n\nw , π(i)\n\nj\n\nAlgorithm 1: Greedy Policy Selection\n\nS ← {i0} where i0 ∼ [N ]; for i = 1 → K − 1 do\n\nPolicy Filtering: We notice that the derived pool often contains a lot of similar policies. This is because the same policy can be optimal under a set of reward functions, which is typical in multi-objective optimization (Chugh et al., 2019; Tabatabaei et al., 2015). Duplicated policies simply slow down training without any help to learn πA. For more efficient training, we adopt a behavior metric, i.e., event-based diversity, to only keep distinct ones from the initial pool. For each biased policy π(i) w , let EC(i) denote the expected event count, i.e. E[(cid:80)T a ]. We define event-based diversity for a subset S ⊆ [N ] by normalized pairwise EC differences, i.e., ED(S) = (cid:80) k |, where ck is a frequency normalization constant. Finding a subset S∗ of size K with the optimal ED can be expensive. We simply adopt a greedy method in Algo. 1 to select policies incrementally.\n\nk′ ← arg maxk′ /∈S ED(S ∪ {k′}); S ← S ∪ {k′};\n\nt=1 φ(st, at)|π(i)\n\nk ck · |EC(i)\n\nk − EC(j)\n\nw , π(i)\n\ni,j∈S\n\nend\n\n(cid:80)\n\n5.3 HIDDEN-UTILITY SELF-PLAY\n\nGiven the filtered policy pool, we train the final adaptive policy πA over rollout games by πA and randomly sampled policies from the pool, which completes our overall algorithm HSP in Algo. 2.\n\nWe implement HSP using MAPPO (Yu et al., 2021) as the RL algorithm. In the first stage, we use MLP policies for fast SP convergence. In practice, we use\n\n5\n\nAlgorithm 2: Hidden-Utility Self-Play for i = 1 → N do Train π(i)\n\na under sampled R(i) w ;\n\nw and π(i)\n\nend Run Algo. 1 to only keep K policies; Initial policy πA; repeat\n\nRollout with πA and sampled π(i) w ; Update πA;\n\nuntil enough iterations;\n\nPublished as a conference paper at ICLR 2023\n\nhalf of the policy pool to train biased policies and the other half to train MEP policies (Zhao et al., 2021) under the game reward. This increases the overall pool towards the game reward, leading to improved empirical performances. For the final adaptive training, as suggested in (Tang et al., 2020), we add the identity of each biased policy as an additional feature to the critic. For eventbased features for the reward space, we consider event types, including interactions with basic items and events causing non-zero rewards in Overcooked. Full implementation details can be found in Appendix D and E.\n\n6 EXPERIMENTS\n\nBaselines. We compare HSP with other SP-based baselines, including Fictitious Co-Play (FCP), Maximum Entropy Population-based training (MEP), and Trajectory Diversity-based PBT (TrajDiv). All methods follow a two-stage framework with a final pool size of 36, which we empirically verified to be sufficiently large to avoid performance degradation for all methods. More analysis on pool size can be found in Appendix F.2.1. The implementation details of baselines can be found in Appendix D.2. Each policy is trained for 100M timesteps for convergence over 5 random seeds. Full training details with hyper-parameter settings can be found in Appendix E.1.\n\nEvaluation. We aim to examine whether HSP can cooperate well with (1) learned human models, (2) scripted policies with strong preferences, and (3) real humans. We use both game reward and human feedback as evaluation metrics. We remark that since a biased human player may play a suboptimal strategy, the game reward may not fully reflect the performance gap between the baselines and HSP. Our goal is to ensure the learned policy is effective for biased partners/humans. Therefore, we consider human feedback as the fundamental metric. Ablation studies are also performed to investigate the impact of our design choices in HSP. In tables, maximum returns or comparable returns within a threshold of 5 are marked in bold. Full results can be found in Appendix F.\n\n6.1 COOPERATION WITH LEARNED HUMAN MODELS IN ONION-ONLY LAYOUTS\n\nFor evaluation with learned human models, we adopted the models provided by (Carroll et al., 2019), which only support onion-only layouts, including Asymm. Adv., Coord. Ring and Counter Circ.. The results are shown in Tab. 1. For a fair comparison, we reimplement all the baselines, labeled MEP, FCP, and TrajDiv, with the same training steps and policy pool size as HSP. We additionally take the best performance ever reported in the existing literature, labeled Existing SOTA in Tab. 1. Our implementation achieves substantially higher scores than Existing SOTA when evaluated with the same human proxy models. HSP further outperforms other reimplementations in Asymm. Adv. and is comparable with the best baseline in the rest. Full results of the evaluation with learned human models can be found in Appendix F.1. We emphasize that the improvement is marginal because the learned human models have limited representation power to imitate natural human behaviors, which typically cover many behavior modalities. Fig.8 in Appendix F.1.1 shows trajectories induced by the learned human models only cover a narrow subspace of trajectories played by human players. Further analysis of the learned human models can be found in Appendix F.1.1. Furthermore, our implementation of baselines achieves substantially better results than the original papers (Carroll et al., 2019; Zhao et al., 2021), which also makes the improvement margin smaller.\n\nPos. Asymm. Adv. Coord. Ring Counter Circ.\n\nExisting SOTA\n\nFCP\n\nMEP\n\nTrajDiv\n\nHSP\n\n1 2\n1 2\n1 2\n1 2\n1 2\n\n141.1(12.5) 84.6(16.3) 282.8(9.4) 203.8(8.2) 291.7(4.6) 203.4(2.0) 289.3(8.8) 194.2(0.7) 300.3(2.2) 217.1(3.3)\n\n92.7(7.4) 107.3(6.4) 161.3(1.6) 161.0(2.7) 161.8(0.7) 164.2(2.1) 150.8(3.1) 142.1(2.3) 160.0(2.6) 160.6(3.3)\n\n54.5(2.3) 55.8(3.6) 95.9(2.0) 92.7(1.3) 108.8(4.2) 111.1(0.7) 60.1(5.0) 53.7(12.4) 107.4(3.5) 106.6(3.0)\n\nTable 1: Comparison of average episode reward and standard deviation when cooperating with learned human models. The Pos. column indicates the roles played by AI policies. Existing SOTA is the best performance ever reported in the existing literature. HSP achieves substantially higher scores than Existing SOTA. And HSP further outperforms other methods in Asymm. Adv. and is comparable with the best baseline in the rest.\n\n6.2 ABLATION STUDIES\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nWe investigate the impact of our design choices, including the construction of the final policy pool and the batch size for training the adaptive policy.\n\nPolicy Pool Construction: HSP has two techniques for the policy pool, i.e., (1) policy filtering to remove duplicated biased policies and (2) the use of MEP policies under the game reward for half of the pool size. We measure the performance with human proxies by turning these options off. For “HSP w.o. Filtering”, we keep all policies by random search in the policy pool, resulting in a larger pool size of 54 (18 MEP policies and a total of 36 random search ones). For“HSP w.o. MEP”, we exclude MEP policies from the policy pool and keep all biased policies without filtering, which leads to the same pool size of 36. The results are shown in Fig. 3 and the detailed numbers can be found in Appendix F.2.2. By excluding MEP policies, the HSP variant (HSP w.o. MEP) performs worse in the more complicated layout Counter Circ. while remaining comparable in the other two simpler ones. So we suggest including a few MEP policies when possible. With policy filtering turned off, even though the policy pool size grows, the performance significantly decays in both Coord. Ring and Counter Circ. layouts, suggests that duplicated biased policies can hurt policy generalization.\n\nFigure 3: Performance of different pool construction strategies. Results suggest that it is beneficial to incorporate MEP policies and filter duplicated policies.\n\nBatch Size: We measure the training curves of the final adaptive policy under the game reward using different numbers of parallel rollout threads in MAPPO. More parallel threads indicate a larger batch size. The results in all five layouts are reported in Fig. 4. In general, we observe that a larger batch size often leads to better training performance. In particular, when the batch size is small, i.e., using 50 or 100 parallel threads, training becomes significantly unstable and even breaks in three layouts. Note that the biased policies in the HSP policy pool have particularly diverse behaviors, which cause a high policy gradient variance when training the final adaptive policy. Therefore, a sufficiently large training batch size can be critical to stable optimization. We adopt 300 parallel threads in all our experiments for a fair comparison.\n\nFigure 4: Average game reward by using different numbers of parallel rollout threads in MAPPO to train the final adaptive policy. More parallel threads imply a larger training batch size.\n\nPractical Remark: Overall, we suggest using a pool size of 36 and including a few MEP policies for the best empirical performance. Besides, a sufficiently large training batch size can help stable optimization, and we use the same batch size for all methods for a fair comparison.\n\n6.3 COOPERATION WITH SCRIPTED POLICIES WITH STRONG BEHAVIOR PREFERENCES\n\nWe empirically notice that human models learned by imitating the entire human trajectories cannot well capture a wide range of behavior modalities. So, we manually designed a set of script policies to encode some particular human preferences: Onion/Tomato Placement, which continuously places onions or tomatoes into the pot, Onion/Dish Everywhere, which keeps putting onions or dishes on the counters, Tomato/Onion Placement and Delivery, which puts tomatoes/onions into the pot in half of the time and tries to deliver soup in the other half of the time. For a fair comparison, we ensure that all scripted policies are strictly different from the HSP policy pool. More details about scripted policies and a full evaluation can be found in Appendix D.3.\n\nWe remark that scripted policies are only used for evaluation but not for training HSP. Tab. 2 shows the average game reward of all the methods when paired with scripted policies, where HSP significantly outperforms all baselines. In particular, in Distant Tomato, when cooperating with a strong tomato preference policy (Tomato Placement), HSP achieves a 10× higher score than other baselines, suggesting that the tomato-preferred behavior is well captured by HSP.\n\n6.4 COOPERATION WITH HUMAN PARTICIPANTS\n\nWe recruited 60 volunteers (28.6% female, 71.4% male, age between 18–30) by posting the experiment advertisement on a public platform and divided them into 5 groups for 5 layouts. They are provided with a detailed introduction to the basic gameplay and the experiment process. Vol-\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nScripts\n\nFCP\n\nMEP\n\nTrajDiv\n\nHSP\n\nAsymm. Adv.\n\nCoord. Ring\n\nCounter Circ.\n\nDistant Tomato\n\nMany Orders\n\nOnion Placement Onion Place.&Delivery Onion Everywhere Dish Everywhere Onion Everywhere Dish Everywhere Tomato Placement Tomato Place.&Delivery Tomato Placement Tomato Place.&Delivery\n\n334.8(13.0) 297.7(3.4) 109.1(7.9) 94.4(3.8) 63.7(9.2) 57.0(5.3) 15.6(5.2) 177.9(6.1) 282.6(16.2) 329.1(5.3)\n\n330.5(14.2) 298.5(3.4) 124.0(3.4) 100.2(5.3) 88.9(5.1) 53.0(1.8) 20.1(10.6) 180.4(8.7) 225.8(60.8) 328.1(12.6)\n\n323.6(17.0) 290.0(4.7) 116.9(8.9) 107.3(5.3) 82.0(12.8) 57.2(2.2) 23.3(9.5) 164.8(19.6) 259.2(7.9) 295.7(2.4)\n\n376.8(9.9) 300.1(4.1) 121.2(12.6) 115.4(7.4) 107.5(3.5) 78.5(4.1) 277.9(14.3) 234.6(15.1) 317.8(9.3) 324.5(3.9)\n\nTable 2: Average episode reward and standard deviation with unseen testing scripted policies. HSP significantly outperforms all baselines.\n\nunteers are fully aware of all their rights and experiments are approved with the permission of the department. A detailed description of the human study can be found in Appendix F.4. The experiment has two stages:\n\n• Warm-up Stage: Participants could play the game freely to explore possible AI behaviors. They are asked to rank AI policies according to the degree of assistance during free plays.\n\n• Exploitation Stage: Participants are instructed to achieve a score\n\nas high as possible.\n\nWe note that our user study design differs from that of the original Overcooked paper (Carroll et al., 2019). The additional warm-up stage allows for diverse human behaviors under any possible preference, suggesting a strong testbed for human-assistive AIs.\n\n6.4.1 RESULTS OF THE WARM-UP STAGE\n\nThe warm-up stage is designed to test the performance of AI policies in the face of diverse human preferences. Fig. 5 visualizes the human preference for different methods reported in the warm-up stage. The unit represents the difference between the percentage of human players who prefer row partners over column partners and human players who prefer column partners over row partners. The detailed calculation method can be found in Appendix F.4.3. HSP is preferred by humans with a clear margin. Since humans can freely explore any possible behavior, the results in Fig. 5 imply the strong generalization capability of HSP. We also summarize feedback from human participants in Appendix F.4.2.\n\n6.4.2 RESULTS OF THE EXPLOITATION STAGE\n\nFigure 5: Human preference in the warm-up stage. The unit denotes the difference between the percentage of human players who prefer row partners over column partners and human players who prefer column partners over row partners. HSP is consistently preferred by human participants with a clear margin.\n\nThe exploitation stage is designed to test the scoring capability of different AIs. Note that it is possible that a human player simply adapts to the AI strategy when instructed to have high scores. So, in addition to final rewards, we also examine the emergent human-AI behaviors to measure the human-AI cooperation level. The experiment layouts can be classified into two categories according to whether the layout allows diverse behavior modes. The first category contains simple onion-only layouts that are taken from (Carroll et al., 2019), including Asymm. Adv., Coord. Ring and Counter Circ.. The second category contains newly introduced layouts with both onions and tomatoes, Distant Tomato and Many Orders, which allow for a much wider range of behavior modes.\n\nOnion-only Layouts: Fig. 6a shows the average reward in onion-only layouts for different methods when paired with humans. Among these onion-only layouts, all methods have comparable episode reward in simpler ones (Asymm. Adv. and Coord. Ring), while HSP is significantly better in the most complex Counter Circ. layout. Fig. 6b shows the frequency of successful onion passing between the human player and the AI player. The learned HSP policy is able to use the middle counter for passing onions, while the baseline policies are less capable of this strategy.\n\nLayouts with Both Onions and Tomatoes: The results and behavior analysis in Distant Tomato and Many Orders are shown as follows,\n\n• Distant Tomato: In Distant Tomato, the optimal strategy is always cooking onion soups, while it is suboptimal to cook tomato soups due to the much more time spent on moving. Interestingly, our human-AI experiments found that humans may have diverse biases over onions and tomatoes. However, all learned baseline policies tend to have a strong bias towards onions and often place onions into a pot with tomatoes in it already. Tab. 3 reports the average number of such Wrong\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a) Onion-Only Layouts\n\n(b) Counter Circ.\n\nFigure 6: (a) Average episode reward in onion-only layouts of different methods when paired with humans in the exploitation stage. HSP has comparable performance with the baselines in Asymm. Adv. and Coord. Ring, and is significantly better in the most complex Counter Circ. layout. (b) The onion passing frequency in Counter Circ. shows that HSP is the most capable, among other baselines, of passing onions via the counter, suggesting better capabilities to assist humans.\n\nPlacements made by different AI players. HSP makes the lowest number of wrong placements and is the only method that can correctly place additional tomatoes into a pot partially filled with tomatoes, labeled Correct Placements. This suggests that HSP is the only effective method to cooperate with biased human strategies, e.g., preferring tomatoes. In addition, as shown in Tab. 3, even when humans play the optimal strategy of cooking onion soups, HSP still achieves comparable performance with other methods.\n\n• Many Orders: In Many Orders, an effective strategy is to utilize all three pots to cook soups. Our experiments found that baseline policies tend to ignore the middle pot. Tab. 4 shows the average number of soups picked up from the middle pot by different AI players. The learned HSP policy is much more active in taking soups from the middle pot, leading to more soup deliveries. Furthermore, HSP achieves a substantially higher episode reward than other methods, as shown in Tab. 4.\n\nFCP\n\nMEP\n\nTrajDiv HSP\n\nOnion-Preferred Episode Reward ↑ Wrong Placements ↓ Correct Placements ↑\n\n343.65 0.37 0.0\n\n325.08 0.41 0.0\n\n334.73 0.38 0.0\n\n340.3 0.21 1.41\n\nTable 3: Average onion-preferred episode reward and frequency of different emergent behaviors in Distant Tomato during the exploitation stage. Onion-Preferred Episode Reward is the average episode reward when humans prefer onions. Wrong Placements and Correct Placements are the average numbers of wrong and correct placements into a pot partially filled with tomatoes. HSP makes the lowest number of wrong placements and is the only method that can place tomatoes correctly, suggesting that HSP is effective at cooperating with biased human strategies.\n\nFCP\n\nMEP\n\nTrajDiv\n\nHSP\n\nEpisode Reward ↑ Number of Soups Picked Up from the Middle Pot ↑\n\n316.81 1.93\n\n320.61 2.03\n\n323.52 1.33\n\n382.52 5.64\n\nTable 4: Average episode reward and average number of picked-up soups from the middle pot by different AI players in Many Orders during the exploitation stage. HSP achieves significantly better performance and is much more active in taking soups from the middle pot than baselines.\n\n7 CONCLUSION\n\nWe developed Hidden-Utility Self-Play (HSP) to tackle the problem of zero-shot human-AI cooperation by explicitly modeling human biases as an additional reward function in self-play. HSP first generates a pool of diverse strategies and then trains an adaptive policy accordingly. Experiments verified that agents trained by HSP are more assistive for humans than baselines in Overcooked. Although our work suggests a new research direction on this fundamentally challenging problem, there are still limitations to be addressed. HSP requires domain knowledge to design a suitable set of events. There exists some work on learning reward functions rather than assuming event-based rewards (Shah et al., 2019; Zhou et al., 2021). So a future direction is to utilize learning-based methods to design rewards automatically. Another major limitation is the computation needed to obtain a diverse policy pool. Possible solutions include fast policy transfer and leveraging a prior distribution of reward functions extracted from human data (Barreto et al., 2017a). Learning and inferring the policy representations of partners could also provide further improvement. We leave these issues as our future work.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis research was supported by National Natural Science Foundation of China (No.U19B2019, 62203257, M-0248), Tsinghua University Initiative Scientific Research Program, Tsinghua-Meituan Joint Institute for Digital Life, Beijing National Research Center for Information Science, Technology (BNRist), and Beijing Innovation Center for Future Chips and 2030 Innovation Megaprojects of China (Programme on New Generation Artificial Intelligence) Grant No. 2021AAA0150000.\n\nREFERENCES\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n\nArash Ajoudani, Andrea Maria Zanchettin, Serena Ivaldi, Alin Albu-Schäffer, Kazuhiro Kosuge, and Oussama Khatib. Progress and prospects of the human-robot collaboration. Autonomous Robots, 42(5):957–975, 2018.\n\nBowen Baker. Emergent reciprocity and team formation from randomized uncertain social prefer-\n\nences. Advances in Neural Information Processing Systems, 33:15786–15799, 2020.\n\nNicholas C Barberis. Thirty years of prospect theory in economics: A review and assessment.\n\nJournal of Economic Perspectives, 27(1):173–96, 2013.\n\nAndre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van HasIn selt, and David Silver. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017a. URL https://proceedings.neurips.cc/paper/2017/file/ 350db081a661525235354dd3e19b8c05-Paper.pdf.\n\nSuccessor features for transfer in reinforcement\n\nlearning.\n\nAndré Barreto, Will Dabney, Rémi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. Advances in neural information processing systems, 30, 2017b.\n\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D ̨ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n\nFrancesco Biondi, Ignacio Alvarez, and Kyeong-Ah Jeong. Human–vehicle cooperation in auInternational Journal of Human–\n\ntomated driving: A multidisciplinary review and appraisal. Computer Interaction, 35(11):932–946, 2019.\n\nAndreea Bobu, Dexter RR Scobee, Jaime F Fisac, S Shankar Sastry, and Anca D Dragan. Less is more: Rethinking probabilistic models of human behavior. In Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction, pp. 429–437, 2020.\n\nColin F Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton univer-\n\nsity press, 2011.\n\nColin F Camerer, Teck-Hua Ho, and Juin-Kuan Chong. A cognitive hierarchy model of games. The\n\nQuarterly Journal of Economics, 119(3):861–898, 2004.\n\nMicah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. On the utility of learning about humans for human-AI coordination. Advances in neural information processing systems, 32, 2019.\n\nRujikorn Charakorn, Poramate Manoonpong, and Nat Dilokthanakul. Learning to cooperate with In Proceedings of the 20th International\n\nunseen agents through meta-reinforcement learning. Conference on Autonomous Agents and MultiAgent Systems, pp. 1478–1479, 2021.\n\nShuo Chen, Ewa Andrejczuk, Zhiguang Cao, and Jie Zhang. Aateam: Achieving the ad hoc teamwork by employing the attention mechanism. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 7095–7102, 2020.\n\nTinkle Chugh, Karthik Sindhya, Jussi Hakanen, and Kaisa Miettinen. A survey on handling computationally expensive multiobjective optimization problems with evolutionary algorithms. Soft Computing, 23(9):3137–3166, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nBrandon Cui, Hengyuan Hu, Luis Pineda, and Jakob Foerster. K-level reasoning for zero-shot\n\ncoordination in hanabi. Advances in Neural Information Processing Systems, 34, 2021.\n\nAntoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. Robots that can adapt like\n\nanimals. Nature, 521(7553):503–507, 2015.\n\nAllan Dafoe, Yoram Bachrach, Gillian Hadfield, Eric Horvitz, Kate Larson, and Thore Graepel.\n\nCooperative AI: machines must learn to find common ground, 2021.\n\nPeter Eckersley.\n\nImpossibility and uncertainty theorems in ai value alignment (or why your agi\n\nshould not have a utility function). In SafeAI@ AAAI, 2019.\n\nOwain Evans, Andreas Stuhlmüller, and Noah Goodman. Learning the preferences of ignorant,\n\ninconsistent agents. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n\nFei Fang, Shutian Liu, Anjon Basak, Quanyan Zhu, Christopher D Kiekintveld, and Charles A Kamhoua. Introduction to game theory. Game Theory and Machine Learning for Cyber Security, pp. 21–46, 2021.\n\nJustin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with events: A general framework for data-driven reward definition. Advances in neural information processing systems, 31, 2018.\n\nMarta Garnelo, Wojciech Marian Czarnecki, Siqi Liu, Dhruva Tirumala, Junhyuk Oh, Gauthier Gidel, Hado van Hasselt, and David Balduzzi. Pick your battles: Interaction graphs as populationlevel objectives for strategic diversity. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1501–1503, 2021.\n\nThomas L Griffiths. Manifesto for a new (computational) cognitive revolution. Cognition, 135:\n\n21–23, 2015.\n\nAbhinav Gupta, Marc Lanctot, and Angeliki Lazaridou. Dynamic population-based meta-learning for multi-agent communication with natural language. Advances in Neural Information Processing Systems, 34, 2021.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861–1870. PMLR, 2018.\n\nDylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse\n\nreinforcement learning. Advances in neural information processing systems, 29, 2016.\n\nHengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. “other-play” for zero-shot coordination. In International Conference on Machine Learning, pp. 4399–4410. PMLR, 2020.\n\nHengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, and Jakob Foerster. Off-belief\n\nlearning. In International Conference on Machine Learning, pp. 4369–4379. PMLR, 2021.\n\nCory D Kidd and Cynthia Breazeal. Robots at home: Understanding long-term human-robot inIn 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp.\n\nteraction. 3230–3235. IEEE, 2008.\n\nChristopher Kiekintveld, Towhidul Islam, and Vladik Kreinovich. Security games with interval In Proceedings of the 2013 international conference on Autonomous agents and\n\nuncertainty. multi-agent systems, pp. 231–238, 2013.\n\nRobert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. A survey of generalisation in\n\ndeep reinforcement learning. arXiv preprint arXiv:2111.09794, 2021.\n\nGlen Klien, David D Woods, Jeffrey M Bradshaw, Robert R Hoffman, and Paul J Feltovich. Ten challenges for making automation a \"team player\" in joint human-agent activity. IEEE Intelligent Systems, 19(6):91–95, 2004.\n\nPaul Knott, Micah Carroll, Sam Devlin, Kamil Ciosek, Katja Hofmann, Anca Dragan, and Rohin Shah. Evaluating the robustness of collaborative agents. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1560–1562, 2021.\n\nHadas Kress-Gazit, Kerstin Eder, Guy Hoffman, Henny Admoni, Brenna Argall, Ruediger Ehlers, Christoffer Heckman, Nils Jansen, Ross Knepper, Jan Kˇretínsk`y, et al. Formalizing and guaranteeing human-robot interaction. Communications of the ACM, 64(9):78–84, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nMinae Kwon, Erdem Biyik, Aditi Talati, Karan Bhasin, Dylan P Losey, and Dorsa Sadigh. When humans aren’t optimal: Robots that collaborate with risk-aware humans. In 2020 15th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 43–52. IEEE, 2020.\n\nCassidy Laidlaw and Anca Dragan. The boltzmann policy distribution: Accounting for systematic suboptimality in human models. In International Conference on Learning Representations, 2022.\n\nMarc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Pérolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. Advances in neural information processing systems, 30, 2017.\n\nShihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. Robust multi-agent In Proceedings of the\n\nreinforcement learning via minimax deep deterministic policy gradient. AAAI Conference on Artificial Intelligence, volume 33, pp. 4213–4220, 2019.\n\nXiangyu Liu, Hangtian Jia, Ying Wen, Yujing Hu, Yingfeng Chen, Changjie Fan, Zhipeng Hu, and Yaodong Yang. Towards unifying behavioral and response diversity for open-ended learning in zero-sum games. Advances in Neural Information Processing Systems, 34:941–952, 2021.\n\nQian Long*, Zihan Zhou*, Abhinav Gupta, Fei Fang, Yi Wu†, and Xiaolong Wang†. Evolutionary population curriculum for scaling multi-agent reinforcement learning. In International Conference on Learning Representations, 2020.\n\nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017.\n\nRyan Lowe, Jakob Foerster, Y-Lan Boureau, Joelle Pineau, and Yann Dauphin. On the pitfalls of In Proceedings of the 18th International Conference on\n\nmeasuring emergent communication. Autonomous Agents and MultiAgent Systems, pp. 693–701, 2019.\n\nRyan Lowe*, Abhinav Gupta*, Jakob Foerster, Douwe Kiela, and Joelle Pineau. On the interacIn International Confertion between supervision and self-play in emergent communication. ence on Learning Representations, 2020. URL https://openreview.net/forum?id= rJxGLlBtwH.\n\nAndrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. Trajectory diversity for zero-shot coordination. In International Conference on Machine Learning, pp. 7204–7213. PMLR, 2021.\n\nHadi Nekoei, Akilesh Badrinaaraayanan, Aaron Courville, and Sarath Chandar. Continuous coorIn International Conference on Machine\n\ndination as a realistic scenario for lifelong learning. Learning, pp. 8016–8024. PMLR, 2021.\n\nAndrew Y Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In in Proc. 17th\n\nInternational Conf. on Machine Learning. Citeseer, 2000.\n\nThanh Nguyen, Rong Yang, Amos Azaria, Sarit Kraus, and Milind Tambe. Analyzing the effecIn Proceedings of the AAAI Conference on\n\ntiveness of adversary modeling in security games. Artificial Intelligence, volume 27, pp. 718–724, 2013.\n\nXinlei Pan, Weiyao Wang, Xiaoshuai Zhang, Bo Li, Jinfeng Yi, and Dawn Song. How you act In Proceedings of the 18th\n\ntells a lot: Privacy-leaking attack on deep reinforcement learning. International Conference on Autonomous Agents and MultiAgent Systems, pp. 368–376, 2019.\n\nPraveen Paruchuri, Milind Tambe, Fernando Ordónez, and Sarit Kraus. Security in multiagent systems by policy randomization. In Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pp. 273–280, 2006.\n\nJohn W Pratt. Risk aversion in the small and in the large. In Uncertainty in economics, pp. 59–79.\n\nElsevier, 1978.\n\nJustin K Pugh, Lisa B Soros, and Kenneth O Stanley. Quality diversity: A new frontier for evolu-\n\ntionary computation. Frontiers in Robotics and AI, 3:40, 2016.\n\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018.\n\nStuart Russell. Human compatible: Artificial Intelligence and the problem of control. Penguin,\n\n2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nReinhard Selten. Bounded rationality.\n\nJournal of Institutional and Theoretical Economics\n\n(JITE)/Zeitschrift für die gesamte Staatswissenschaft, 146(4):649–658, 1990.\n\nRohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the feasibility of learning, rather In International Conference on Machine\n\nthan assuming, human biases for reward inference. Learning, pp. 5670–5679. PMLR, 2019.\n\nThomas B Sheridan. Human-robot interaction: status and challenges. Human factors, 58(4):525–\n\n532, 2016.\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140– 1144, 2018.\n\nPeter Stone, Gal A Kaminka, Sarit Kraus, and Jeffrey S Rosenschein. Ad hoc autonomous agent teams: Collaboration without pre-coordination. In Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.\n\nDJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett. Collaborating with humans without human data. Advances in Neural Information Processing Systems, 34, 2021.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nMohammad Tabatabaei, Jussi Hakanen, Markus Hartikainen, Kaisa Miettinen, and Karthik Sindhya. A survey on handling computationally expensive multiobjective optimization problems using surrogates: non-nature inspired methods. Structural and Multidisciplinary Optimization, 52(1):1–25, 2015.\n\nZhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon Shaolei Du, Yu Wang, and Yi Wu. Discovering diverse multi-agent strategic behavior via reward randomization. In International Conference on Learning Representations, 2020.\n\nJohannes Treutlein, Michael Dennis, Caspar Oesterheld, and Jakob Foerster. A new formalism, In International Conference on Machine\n\nmethod and open issues for zero-shot coordination. Learning, pp. 10413–10423. PMLR, 2021.\n\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n\nChen Wang, Claudia Pérez-D’Arpino, Danfei Xu, Li Fei-Fei, Karen Liu, and Silvio Savarese. Cogail: Learning diverse strategies for human-robot collaboration. In Conference on Robot Learning, pp. 1279–1290. PMLR, 2022.\n\nNick Wilkinson and Matthias Klaes. An introduction to behavioral economics. Macmillan Interna-\n\ntional Higher Education, 2017.\n\nDetlof Von Winterfeldt and Gregory W Fischer. Multi-attribute utility theory: models and assess-\n\nment procedures. Utility, probability, and human decision making, pp. 47–85, 1975.\n\nSarah A Wu, Rose E Wang, James A Evans, Joshua B Tenenbaum, David C Parkes, and Max Kleiman-Weiner. Too many cooks: Bayesian inference for coordinating multi-agent collaboration. Topics in Cognitive Science, 13(2):414–432, 2021.\n\nMarkus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforce-\n\nment learning. arXiv preprint arXiv:1507.04888, 2015.\n\nAnnie Xie, Dylan Losey, Ryan Tolsma, Chelsea Finn, and Dorsa Sadigh. Learning latent representations to influence multi-agent interaction. In Conference on Robot Learning, pp. 575–588. PMLR, 2021.\n\nChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.\n\nTianjun Zhang, Huazhe Xu, Xiaolong Wang, Yi Wu, Kurt Keutzer, Joseph E Gonzalez, and Yuandong Tian. Multi-agent collaboration via reward attribution decomposition. arXiv preprint arXiv:2010.08531, 2020.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nRui Zhao, Jinming Song, Hu Haifeng, Yang Gao, Yi Wu, Zhongqian Sun, and Yang Wei. Maximum entropy population based training for zero-shot human-ai coordination. arXiv preprint arXiv:2112.11701, 2021.\n\nZihan Zhou, Wei Fu, Bingliang Zhang, and Yi Wu. Continuously discovering novel strategies via reward-switching policy optimization. In Deep RL Workshop NeurIPS 2021, 2021. URL https: //openreview.net/forum?id=2AJtG_ZIV2.\n\nHenry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, and Sergey Levine. The ingredients of real world robotic reinforcement learning. In International Conference on Learning Representations, 2019.\n\nBrian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse\n\nreinforcement learning. In Aaai, volume 8, pp. 1433–1438. Chicago, IL, USA, 2008.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nWe would suggest visiting https://sites.google.com/view/hsp-iclr for more information.\n\nA THEOREM PROOFS\n\nFor simplicity, we assume state space and action space in our analysis are both discrete and finite, which is exactly the case for Overcooked, and the rewards r are bounded: |r(s, a)| ≤ Rmax, ∀s ∈ S, a ∈ A.\n\nLemma 5.1. Given an MDP M = ⟨S, A, P, Rt⟩, for any policy πw : S × A → [0, 1], there exists a hidden reward function Rw such that the two-player hidden utility Markov game M ′ = ⟨S, A, P, Rw, Rt⟩ has a Nash equilibrium (π∗\n\nw) where π∗\n\nw = πw.\n\na, π∗\n\nProof. Our analysis is based on the maximum entropy reinforcement learning framework (Haarnoja et al., 2018; Ziebart et al., 2008; Wulfmeier et al., 2015). Given a reward function R and policies of the two players π1 and π2, we consider following maximum entropy RL objective for policy πi(1 ≤ i ≤ 2),\n\nJi(π1, π2|R) = Eτ\n\n(cid:34)\n\n(cid:88)\n\nt\n\nγt(R(st, a(1)\n\nt\n\n, a(2)\n\n(cid:12) (cid:12) t ) + αH(πi(·|st)))\n\n(cid:12)a(i)\n\nt ∼ πi(·|st)\n\n(cid:35)\n\nWe shall first constructs πa given policy πw to satisfy J2(πw, πa|Rt) ≥ J2(πw, π′ secondly constructs Rw such that J1(πw, πa|Rw) ≥ J1(π′ w is satisfied.\n\nw, πa|Rw), ∀π′\n\na|Rt), ∀π′\n\na and\n\nStep 1: Construct πa given πw.\n\nGiven πw, let πa ∈ arg maxπ J2(πw, π|Rt).\n\nStep 2: Construct Rw such that J1(πw, πa|Rw) ≥ J1(π′ πa.\n\nw, πa|Rw), ∀π′\n\nw is satisfied given πw and\n\nGiven a fixed partner πa, by regarding πa as part of the environment dynamics, we could consider the dynamics for πw in a single-agent MDP M ′ = ⟨S, A, P ′, Rw, γ⟩ where S is the state space, A is the action space, P ′ denotes the transition probability and Rw is the reward function to construct. More specifically, P ′ is defined as,\n\nP ′(s′|s, a) =\n\n(cid:88)\n\n ̃a\n\nP (s′|s, a, ̃a) · πa( ̃a|s)\n\nIn M ′, given reward Rw, the objective of πw becomes,\n\nmax π\n\nEτ\n\n(cid:34)\n\n(cid:88)\n\nt\n\n(cid:12) γt(Rw(st, at) + αH(π(st))) (cid:12) (cid:12)at ∼ π(st)\n\n(cid:35)\n\nThe value function and the Q function could be defined as,\n\nV (s) = Eτ\n\n(cid:34)\n\n(cid:88)\n\nt\n\n(cid:12) γt(Rw(st, at) + αH(πw(st))) (cid:12) (cid:12)at ∼ πw(st), s0 = s\n\n(cid:35)\n\n(cid:88)\n\n=\n\na\n\nπw(a|s)(Rw(s, a) + γEs′[V (s′)|s, a]) + αH(πw(s))\n\nQ(s, a) = Rw(s, a) + γ · Es′[V (s′)|s, a]\n\n(2)\n\n(3)\n\n(4)\n\n(5)\n\n(6)\n\nIt is sufficient to construct Rw such that V (s) is a stable point of the Bellman backup operator (Sutton & Barto, 2018) T ∗ under some Rw:\n\n(T ∗V )(s) = max\n\nd:(cid:80)\n\na d(a)=1\n\nαH(d) +\n\n(cid:88)\n\na\n\nd(a)(Rw(s, a) + γEs′[V (s′)|s, a])\n\n(7)\n\nNow we assume V (s) is a stable point for Eq. 7 and construct Rw. For all s ∈ S, πw(·|s) should be a solution to the following maximization problem,\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nmax d\n\ns.t.\n\nαH(d) +\n\n(cid:88)\n\na\n\nd(a)Q(s, a)\n\nd(a) = 1\n\n(cid:88)\n\na\n\nApplying KKT conditions over the above optimization problem indicates that,\n\nπw(·|s) ∝ exp(Q(s, ·)/α), ∀s\n\n(8)\n\n(9)\n\n(10)\n\nw(s) = arg maxa πw(a|s), V ∗(s) = maxa Q(s, a), A(s, a) = Q(s, a) − V ∗(s). By Eq. 10,\n\nLet π∗ we also have\n\nA(s, a) = α(log πw(a|s) − log πw(π∗\n\nw(s)|s))\n\nBy definition of value function V (s),\n\nπw(a|s)Q(s, a) + αH(πw(s))\n\nπw(a|s)(A(s, a) + V ∗(s)) + αH(πw(s))\n\nπw(a|s)A(s, a) + V ∗(s) + αH(πw(s))\n\nV (s) =\n\n=\n\n=\n\n=\n\n(cid:88)\n\na (cid:88)\n\na (cid:88)\n\na (cid:88)\n\na\n\nπw(a|s)A(s, a) + Rw(s, π∗\n\nw(s)) + γEs′[V (s′)|s′ ∼ P ′(s, π∗\n\nw(s))] + αH(πw(s))\n\n= Eτ\n\n(cid:34)\n\n(cid:88)\n\nγt\n\n(cid:32)\n\n(cid:88)\n\nt\n\na′\n\nπw(a′|s)A(s, a′) + Rw(st, at) + αH(πw(st))\n\n(cid:33)\n\n(cid:35)\n\n(cid:12) (cid:12)at = π∗ (cid:12)\n\nw(st)\n\nLet b(s) = Rw(s, π∗\n\nw(s)). Then V (s) is determined given πw and b,\n\nV (s) = Eτ\n\n(cid:34)\n\n(cid:88)\n\nγt\n\n(cid:32)\n\n(cid:88)\n\nt\n\na′\n\nπw(a′|s)A(s, a′) + b(st) + αH(πw(st))\n\n(cid:33)\n\n(cid:35)\n\n(cid:12) (cid:12)at = π∗ (cid:12)\n\nw(st)\n\nBy A(s, a) = α(log πw(a|s) − log πw(π∗\n\nw(s)|s)) = Q(s, a) − V ∗(s), w(s)|s)) = Rw(s, a) + γEs′[V (s′)|s′ ∼ P ′(s, a)] − V ∗(s)\n\nα(log πw(a|s) − log πw(π∗\n\nRw(s, a) = α log\n\n(cid:18) πw(a|s) πw(π∗\n\nw(s)|s)\n\n(cid:19)\n\n− γEs′[V (s′)|s′ ∼ P ′(s, a)] + V ∗(s)\n\n(19)\n\nTo summarize, for policy πw, we can construct a valid hidden reward function Rw via following process,\n\n1. Choose a function b : S ′ → R.\n\n2. Compute A(s, a) by Eq. 11.\n\n3. Compute V (s) and V ∗(s) by Eq. 17.\n\n4. Construct Rw(s, a) by Rw(s, π∗\n\nw(s)) = b(s) and Eq. 19.\n\nNow we show that, for any b : S ′ → R, under Rw constructed by the above process, V (s) is a stable point of the Bellman backup operator T ∗. This is straightforward. First, constructed Rw ensures\n\n16\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\nPublished as a conference paper at ICLR 2023\n\nthat α(log πw(a|s) − log πw(π∗ exp(Q(s, a)/α), which means πw is a solution for the maximization problem 8. So\n\nw(s)|s)) = Q(s, a) − V ∗(s) (Eq. 19) and therefore πw(a|w) ∝\n\n(T ∗V )(s) = max\n\nd\n\nαH(d) +\n\n(cid:88)\n\na\n\nd(a)(Rw(s, a) + γEs′[V (s′)])\n\n(cid:88)\n\n=\n\na\n\nπw(a|s)Q(s, a) + αH(πw(s)) = V (s).\n\n(20)\n\n(21)\n\nTheorem 5.1. For any ε > 0, there exists a mapping ̃πw where ̃πw(Rw) denotes the derived policy π∗ w in the NE of the hidden utility Markov game Mw = ⟨S, A, P, Rw, Rt⟩ induced by Rw, and a distribution PR : R → [0, 1] over the hidden reward space R, such that, for any adaptive policy πA ∈ arg maxπ′ ERw∼PR [J(π′, ̃πw(Rw))], πA approximately maximizes the ground-truth objective with at most an ε gap, i.e., EπH ∼PH [J(πA, πH )] ≥ maxπ′ EπH ∼PH [J(π′, πH )] − ε.\n\nProof. Let K(K > |A|) be a large positive integer. We construct a discretization of the policy space K where i ∈ [K], ∀s ∈ S, a ∈ A and (cid:80) Π by ΠK = {π : π(a|s) = i a π(a|s) = 1, ∀s ∈ S}. Note that ΠK is finite, i.e. |ΠK| ≤ (K + 1)|S|·|A|. Let M = |ΠK| and π1, π2, · · · , πM be an ordering of the policies in ΠK. For simplicity of notation, let δ = |A| K .\n\nGiven the discretization ΠK, it’s straightforward to specify the nearest policy ˆπ ∈ ΠK for any policy π ∈ Π. Formally, for any policy π ∈ Π, let G(π) = arg mini=1,...,M s,a |π(a|s) − πi(a|s)|. An obvious property of G is that, ∀s ∈ S, ||π(·|s) − G(π)(·|s)||∞ ≤ |A|\n\n(cid:80)\n\nK = δ.\n\nFor two policies π1 and π2, consider π1 playing with π2 and G(π2) respectively. Since the action distribution of π2 and G(π2) at each state differ at most δ, we have follows,\n\n|J(π1, π2) − J(π1, G(π2))| ≤\n\nγt · (1 − δ)t · δ ·\n\n2Rmax 1 − γ\n\n≤\n\n2δRmax (1 − γ)2\n\n(cid:88)\n\nt\n\n(22)\n\nWe can then derive a discretized approximation of the ground-truth policy distribution PH as follows,\n\nˆPH (π) = Prπ′∼PH [π = G(π′)]\n\n(23)\n\nWe could show that the difference between the objective under the ground-truth policy distribution PH and that under the approximated policy distribution ˆPH is bounded. By Eq. 22, for any adaptive policy πA,\n\n(cid:12) (cid:12)E\n\nπH ∼ ˆPH\n\n[J(πA, πH )] − EπH ∼PH [J(πA, πH )](cid:12)\n\n(cid:12) = (cid:12)\n\n(cid:12)EπH ∼PH [J(πA, G(πH )) − J(πA, πH )](cid:12) 2δRmax (1 − γ)2\n\n(cid:12) (24)\n\n(25)\n\n≤\n\nOn the other hand, consider following an iterative process to find hidden reward functions for policies in ΠK. For i = 1..M , we find hidden reward function R(i) w |1 ≤ j ≤ i − 1} and R(i) w could be constructed from πi as in Lemma 5.1. Notice that, by construction rule in Lemma 5.1, such R(i)\n\nw must exists since we can specify arbitrary b : S → R.\n\nw where R(i)\n\nw /∈ {R(j)\n\nw ) = πi, ∀i = 1 . . . M and the hidden reward distribution PR be PR(R(i)\n\nLet ̃πw(R(i) w ) = ˆPH (πi), ∀i = 1 · · · M . We immediately see that, for any adaptive policy πA, the objective is equivalent under the approximated policy distribution ˆPH and hidden reward function distribution PR,\n\nERw∼PR [J(πA, ̃πw(Rw))] = E\n\nπH ∼ ˆPH\n\n[J(πA, πH )]\n\n(26)\n\nFinally, for any adaptive policy πA ∈ arg maxπ′ ERw∼PR [J(π′, ̃πw(Rw))] and any policy π′ ∈ Π,\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nEπH ∼PH [J(πA, πH )] ≥ E\n\nπH ∼ ˆPH\n\n[J(πA, πH )] −\n\n2δRmax (1 − γ)2\n\n= ERw∼PR [J(πA, ̃πw(Rw))] −\n\n≥ ERw∼PR [J(π′, ̃πw(Rw))] −\n\n2δRmax (1 − γ)2 2δRmax (1 − γ)2\n\n= E\n\nπH ∼ ˆPH\n\n[J(π′, πH )] −\n\n≥ EπH ∼PH [J(π′, πH )] −\n\n2δRmax (1 − γ)2 4δRmax (1 − γ)2\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\nLet K ≥ 4|A|Rmax\n\nε(1−γ)2 and we have EπH ∼PH [J(πA, πH )] ≥ maxπ′ EπH ∼PH [J(π′, πH )] − ε.\n\nB ENVIRONMENT DETAILS\n\nFigure 7: All 5 layouts used in our work (from left to right): Asymmetric Advantage, Coordination Ring, Counter Circuit, Distant Tomato, and Many Orders, each featuring specific cooperation patterns we want to study.\n\nB.1 DESCRIPTION\n\nThe Overcooked Environment, first introduced in (Carroll et al., 2019), is based on the popular video game Overcooked where multiple players cooperate to finish as many orders as possible within a time limit. In this simplified version of the original game, two chiefs, each controlled by a player (either human or AI), work in grid-like layouts. Chiefs can move between non-table tiles and interact with table tiles by picking up or placing objects. Ingredients (e.g., onions and tomatoes) and empty dishes can be picked up from the corresponding dispenser tiles and placed on empty table tiles or into the pots. The typical pipeline for completing an order is (1) players put appropriate ingredients into a pot; (2) a pot starts cooking automatically once filled and takes a certain amount of time (depending on the recipe) to finish; (3) a player harvests the cooked soup with an empty dish and deliver it to the serving area.\n\nThe observation for an agent includes the whole layout, items on the counter and pots, player positions, orders, and time. The possible actions are up, down, left, right, no-op, and \"interacting\" with the tile the player is facing. Reward is given to both agents upon successful soup delivery, with the amount varying with the type of soup. An episode of the game terminates when the time limit is reached.\n\nThe environment used in (Carroll et al., 2019) has only onions as ingredients and onion soups as In our work, we evaluate all methods in three of them, namely Asymmetric Advantage, orders. Coordination Ring, and Counter Circuit, each designed to enforce a specific cooperation pattern.\n\nOur work introduces two new layouts: Distant Tomato and Many orders, with new ingredients and order types to make cooperation more challenging. In Distant Tomato, a dish of onion soup takes 20 ticks to finish and gives 20 rewards when delivered, while a tomato soup takes 10 ticks and gives the same reward but needs more movements to get the ingredient. The two players need to agree on which type of soup to cook in order to reach a high score. Failure in cooperation may result in tomato-onion soups that give no reward. In many orders, there are three types of orders: onion,\n\n18\n\nPublished as a conference paper at ICLR 2023\n\ntomato, and 1-onion-2-tomato. To fully utilize the three pots, the players need to work seamlessly in filling not just the pots near each of them but also the pot in the middle.\n\nWe show all the layouts in Fig.7. and conclude the cooperation pattern of our interest as follows.\n\n• Asymmetric Advantage tests whether the players can choose a strategy to their strengths. • Coordination Ring requires the players not to block each other when traveling between the\n\ntwo corners.\n\n• Counter Circuit embeds a non-trivial but efficient strategy of passing onions through the\n\nmiddle counter, which needs close cooperation.\n\n• Distant Tomato and Many Orders both encourage the players to reach an agreement on the\n\nfly in order to achieve a high reward.\n\nB.2 EVENTS\n\nIn Overcooked, we consider the following events for random search in HSP and reward shaping during training of all methods:\n\n• putting an onion/tomato/dish/soup on the counter, • picking up an onion/tomato/dish/soup from the counter, • picking up an onion from the onion dispenser, • picking up a tomato from tomato dispenser, • picking up a dish from the dish dispenser, • picking up a ready soup from the pot with a dish, • placing an onion/tomato into the pot, • valid placement: after the placement, we can finish an order with a positive reward by\n\nplacing other ingredients,\n\n• optimal placement: the placement is optimal if the maximum order reward we can achieve\n\nfor this particular pot is not decreased after the placement,\n\n• catastrophic placement: the placement is catastrophic if the maximum order reward we can\n\nachieve for this particular pot decreases from positive to zero after the placement,\n\n• useless placement: the placement is useless if the maximum order reward we can achieve\n\nfor this particular pot is already zero before the placement,\n\n• useful dish pickup: picking up a dish is useful when there are no dishes on the counter, and the number of dishes already taken by players is less than the total number of unready and ready soups,\n\n• delivering a soup to the serving area.\n\nAdditionally, in Distant Tomato, we consider the following events only for reward shaping,\n\n• placing a tomato into an empty pot, • optimal tomato placement: the placement is optimal and a tomato placement, • useful tomato pickup: the agent picks up a tomato when the partner isn’t holding a tomato,\n\nand there is a pot that is not full but only has tomatoes in it.\n\nC OVERCOOKED VERSION\n\nIn our experiments, we use two versions of Overcooked for a fair comparison with prior works and introduce challenging layouts. One version, in which we tested Asymmetric Advantages, Coordination Ring and Counter Circuit, is consistent with the \"neurips2019\" branch in the released GitHub repository of (Carroll et al., 2019). We remark that MEP (Zhao et al., 2021) also follows this version. Following this also allows us to perform an evaluation with human proxy models provided in the released code of (Carroll et al., 2019). The other version is an up-to-date version of Overcooked, which supports tomatoes and user-defined orders. We notice that a pot automatically starts cooking soup once there are three items in it in the former version, while it requires an additional \"interact\" action to start cooking in the latter version. This additional \"interact\" is required in the latter version since it supports orders with different amounts of ingredients. However, having an additional \"interact\" significantly influences a human player’s interactive experience. Therefore, we make modifications on the latter version to restrict orders to 3 items and support auto-cooking when there are 3 items. For more details, please refer to the released code.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nD IMPLEMENTATION DETAILS\n\nD.1 HSP\n\nAlgorithm 3: Hidden-Utility Self-Play for i = 1 → N do Train π(i)\n\na under sampled R(i) w ;\n\nw and π(i)\n\nend Run greedy policy selection to only keep K policies; Initial policy πA; repeat\n\nRollout with πA and sampled π(i) w ; Update πA;\n\nuntil enough iterations;\n\nThe pseudocode of HSP is shown in Algo. 3. We implemented HSP on top of MAPPO (Yu et al., 2021). Following the standard practice, we use multiprocessing to collect trajectories in parallel and then update the models. In the first stage, we use MLP policies, which empirically yield better results. In the second stage, we use RNN policies so that the adaptive policy could infer the intention of its partner by observing the history of its partner and make decisions accordingly for better adaptation. As suggested in(Tang et al., 2020), we add the identities of the policies in the policy pool as an additional feature to the critic. For better utilization of the computation resources, each environment sub-process loads a uniformly sampled policy and performs inference on CPUs, while the inference of the adaptive policy is batched across sub-processes in a GPU.\n\nD.2 BASELINES\n\nFor a fair comparison, we implement all baselines to be two-staged and train layout-specific agents.\n\nWe remark that our implementation of MEP achieves substantially higher scores than reported in the original paper (Zhao et al., 2021) when evaluated with the same human proxy models as MEP. All baselines are implemented with techniques stated above: loading policies from the pool per subprocess and the additional feature of identities of policies in the policy pool. We detail the baselines here and point out the difference with the original papers,\n\nFCP(Strouse et al., 2021): We list the differences between our implementation and the original FCP as follows,\n\n1. The original FCP uses image-based egocentric observations, while we use feature-based\n\nobservations as provided in Overcooked.\n\n2. The original FCP uses a pool size of 96 while we use 36. We empirically found 36 a sufficiently large pool size in our experiments. As shown in Table 21, in the three layouts that have human proxy models, there is no significant difference between using a pool size of 36 and of 72.\n\nMEP(Zhao et al., 2021): We list the differences between our implementation and the original MEP as follows,\n\n1. While the released code of MEP uses MLP policy in the second training stage, we found RNN policy to work better. Intuitively, for better cooperation, the adaptive policy should infer the intention of its partner by observing the state-action history.\n\n2. MEP uses a pool size of 15 while we use 36. 3. MEP uses prioritized sampling in the second stage, which favors weak policies in the pool, while we adopt uniform sampling for MEP since we found prioritized sampling not helpful with our carefully tuned implementation (shown in Table 5).\n\n4. In the released code of MEP, the policy updates are performed on data against only one policy from the pool, while we perform policy updates on data against many policies from the pool. This avoids the update from being biased towards some specific policies.\n\nTrajDiv(Lupu et al., 2021): While the original TrajDiv is tested in hand-crafted MDPs and Hanabi, we test TrajDiv in Overcooked. Although (Lupu et al., 2021) suggests training the adaptive policy and the policy pool together in a single stage, we choose to follow MEP and FCP to have a twostaged design that trains the adaptive policy in the second stage.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nUniform Sampling\n\nPrioritized Sampling\n\nPos. Asy. Adv. Coor. Ring Coun. Circ.\n\n1 2\n1 2\n\n291.7(4.6) 203.4(2.0) 284.6(3.2) 218.8(2.4)\n\n161.8(0.7) 164.2(2.1) 161.2(1.4) 167(4.5)\n\n108.8(4.2) 111.1(0.7) 94.4(2.3) 99.8(1.8)\n\nTable 5: Average episode reward and standard deviation (over 5 seeds) with different sampling methods of MEP. The \"1\" and \"2\" indicates the roles played by AI policies.\n\nBiased\n\nScripted\n\nAsymm. Adv. Coord. Ring Counter Circ. Dist. Tomato Many Orders\n\n0.56 0.59 0.56 0.70 0.55\n\n0.85 0.72 0.73 1.90 1.00\n\nTable 6: The average event-based difference of biased and scripted policies respectively.\n\nD.3 SCRIPTED POLICIES\n\nTo evaluate all methods with policies that have strong preferences, we consider the following scripted policies,\n\n• Onion/Tomato/Dish Everywhere continuously tries to put onions, tomatoes or dishes over\n\nthe counter.\n\n• Onion/Tomato Placement always tries to put onion or tomato into the pot.\n\n• Delivery delivers a ready soup to the serving area whenever possible.\n\n• Onion/Tomato Placement and Delivery puts tomatoes/onions into the pot in half of the time\n\nand tries to deliver soup in the other half of the time.\n\nFor Counter Circuit, we additionally consider a scripted policy, named Onion to Middle Counter, which keeps putting onions randomly over the counter in the middle of the layout.\n\nInput to these scripted policies is the ground-truth state of the game, which is accessible via the game simulator. When a scripted policy is unable to finish the event of its interest at some state, the scripted policy would walk to a random empty grid. For example, Onion Placement would choose a random walk when all pots are full. We ensure that these scripted policies are strictly different from policies in the policy pool of HSP. For more details, please refer to the released code.\n\ns\n\n}m∈M be the set of scripted policies. For convenience, let Π = {π(n)\n\nWe also provide evidence to show scripted policies are sufficiently different from those in the training pool. We use the expected event count of scripted and biased policies to support our claim. Recall that expected event count for a pair of policy πa, πb is EC(πa, πb) = E[(cid:80)T t=1 φ(st, at)|πa, πb]. Let πHSP be the HSP adaptive policy, {π(n) w }n∈[N ] be the set of biased policies in the training pool, and {π(m) }m∈M be the union of biased policies and scripted policies. For each policy π′ ∈ Π, we measure how the event-based difference close it is to the rest of policies in Π in the expected event count, i.e. (cid:80) EventDiffΠ(π′) = minπ′′∈Π\\{π′} k ck · |ECk(π′, πHSP ) − ECk(π′′, πHSP )| where ck is a frequency normalization constant. Then a large event-based difference indicates that π′ is sufficiently different from other policies in Π. We calculate the event-based difference for all biased and scripted policies. Table. 6 reports the average event-based difference between biased and scripted policies, respectively. Scripted policies consistently have a larger average event-based difference, indicating scripted policies are sufficiently different from biased policies, which are used for training the HSP adaptive policy.\n\nw }n∈[N ] ∪ {π(m)\n\ns\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nE TRAINING DETAILS\n\nE.1 HYPERPARAMETERS\n\nHSP and baselines are all two-staged solutions by first constructing a policy pool and then training an adaptive policy πA to maximize the game reward w.r.t. the induced pool.\n\nThe network architecture in both two stages is composed of 3 convolution layers with max pooling. Hyperparameters of these layers are listed in Table 7. Each layer is followed by a max pooling layer with a kernel size of 2. For MLP policies, we add two linear layers after the convolution. For RNN policies, we add a 1-layer GRU after the convolution and two linear layers after the GRU layer. The hidden sizes for these linear layers and the GRU layer are all 64. We use ReLU as the activation function between layers and LayerNorm after GRU and linear layers except the last one. The output is a 6-dim vector denoting the categorical action distribution.\n\nCommon hyperparameters for all methods in 5 layouts are listed in Table 8 and Table 9. Specifically, for MEP, we use the suggested hyperparameters from the original paper (Zhao et al., 2021). Detailed hyperparameters of MEP are shown in Table 10, where population entropy coef. adjusts the importance of the population entropy term. Detailed hyperparameters of TrajDiv are shown in Table 11, where traj. gamma is the discounting factor used in local action kernel and diversity coef. adjusts the importance of the diversity term. For each one of MEP, FCP and TrajDiv, we train 12 policies in the first stage and, following the convention of MEP (Zhao et al., 2021) and FCP (Strouse et al., 2021), take the init/middle/final checkpoints for each policy to build up the policy pool, leading to a pool size of 36. For HSP, we use a random search to first train 36 biased policies and then filter out 18 biased policies from them. We then combine these biased policies and past checkpoints of 6 policies in the policy pool of MEP to build up the policy pool of HSP, again leading to a pool size of 36.\n\nLayer Out Channels Kernel Size\n\nStride\n\nPadding\n\n1 2\n3\n\n32 64 32\n\n3 3\n3\n\n1 1\n1\n\n1 1\n1\n\nTable 7: CNN feature extractor hyperparameters.\n\ncommon hyperparameters\n\nvalue\n\nentropy coef. gradient clip norm GAE lambda gamma value loss huber delta mini batch size optimizer optimizer epsilon weight decay network initialization use reward normalization use feature normalization learning rate parallel environment threads ppo epoch environment steps episode length reward shaping horizon\n\n0.01 10.0 0.95 0.99 huber loss 10.0 batch size / mini-batch Adam 1e-5 0\nOrthogonal True True 5e-4 100 15 10M 400 100M\n\nTable 8: Common hyperparameters in the first stage.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nE.2 CONSTRUCTING THE POLICY POOL FOR HSP\n\nTo construct the policy pool for HSP, we perform a random search over possible hidden reward functions. Each reward function is formulated as a linear function over the event-based features, i.e. R = {Rw : Rw(s, a1, a2) = φ(s, a1, a2)T w, ||w||∞ ≤ Cmax} where φ : S × A × A → Rm specifies occurrences of different events when taking joint action (a1, a2) at state s. To perform random search, instead of directly sampling each wj from the section [−Cmax, Cmax], we sample each wj from a set of possible values Cj. We detail the Cj for each event on each layout here. Tab. 12 shows Cj in Asymmetric Advantages, Coordination Ring and Counter Circuit. Tab. 13 and Tab. 14 show Cj in Distant Tomato and Many Orders respectively. A detailed description of the events is shown in Sec. B.2. Note that in addition to events, we also include order reward as one element in a random search.\n\nTo filter out duplicated policies, we define an event-based diversity for a subset S, i.e. ED(S) = (cid:80) k − EC(j) k is the expected number of occurrences of event type k w . The coefficient ck balances the importance of different kinds of events. We\n\nk ck · |EC(i) for biased policy π(i)\n\nk | where ECi\n\ni,j∈S\n\n(cid:80)\n\nsimply set ck as a normalization constant, i.e. ck =\n\n(cid:16)\n\nmaxi∈[N ] EC(i)\n\nk\n\n(cid:17)−1\n\n.\n\ncommon hyperparameters\n\nvalue\n\nentropy coef. gradient clip norm GAE lambda gamma value loss huber delta mini batch size optimizer optimizer epsilon weight decay network initialization use reward normalization use feature normalization learning rate parallel environment threads ppo epoch environment steps episode length reward shaping horizon policy pool size\n\n0.01 10.0 0.95 0.99 huber loss 10.0 batch size / mini-batch Adam 1e-5 0\nOrthogonal True True 5e-4 300 15 100M 400 100M 36\n\nTable 9: Common hyperparameters in the second stage.\n\nhyperparameters\n\nvalue\n\npopulation entropy coef.\n\n0.01\n\nTable 10: MEP hyperparameters in the first stage.\n\nhyperparameters\n\nvalue\n\ntraj. gamma diversity coef.\n\n0.5 0.1\n\nTable 11: TrajDiv hyperparameters in the first stage.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nEvent\n\nPicking up an onion from onion dispenser Picking up a dish from dish dispenser Picking up a ready soup from the pot Placing an onion into the pot Delivery Order reward\n\nCj\n\n-10, 0, 10 0, 10 -10, 0, 10 -10, 0, 10 -10, 0 0, 1\n\nTable 12: Cj for random search in Asymmetric Advantages, Coordination Ring and Counter Circuit.\n\nEvent\n\nPicking up an onion from onion dispenser Picking up a tomato from tomato dispenser Picking up a dish from dish dispenser Picking up a soup Viable placement Optimal placement Catastrophic placement Placing an onion into the pot Placing a tomato into the pot Delivery Order reward\n\nCj\n\n-5, 0, 5 0, 10, 20 0, 10 -5, 0, 5 -10, 0, 10 -10, 0, 10 0, 10 -10, 0, 10 -10, 0, 10 -10, 0 0, 1\n\nTable 13: Cj for random search in Distant Tomato.\n\nE.3 REWARD SHAPING\n\nWe use reward shaping during training in all layouts, detailed as follows,\n\n• In the first stage, the reward shaping for Asymmetric Advantages, Coordination Ring and Counter Circuit is shown in Table. 15 and that for Distant Tomato and Many Orders is shown in Table. 17. Note that we do not use reward shaping when training biased policies for HSP in the first stage.\n\n• In the second stage, the reward shaping for Asymmetric Advantages, Coordination Ring and Counter Circuit is shown in Table. 16. Reward shaping for Many Orders is shown in Table. 18 and that for Distant Tomato is shown in Table. 19. The factor of shaped reward anneals from 1 to 0 during the whole course of training in all layouts except Distant Tomato, in which the factor anneals from 1 to 0.5.\n\nF FULL RESULTS\n\nF.1 COOPERATION WITH LEARNED HUMAN MODELS\n\nTable 20 shows average episode reward and standard deviation (over 5 seeds) on 3 layouts for different methods played with human proxy policies. All values within 5 standard deviations of the maximum episode return are marked in bold. These three simple layouts may not fully reflect the performance gap between the baselines and HSP. The results with learned human models are reported for a fair comparison with existing SOTA methods. Besides, our implementation of the baselines achieves substantially better results than their original papers with the same human proxy models, making the improvement margin look smaller. We also remark that the learned human models have limited representation power to imitate natural human behaviors that typically cover many behavior modalities. Here we give empirical evidence of the learned human models failing to fully reflect human behaviors.\n\nF.1.1 EMPIRICAL EVIDENCE\n\nThe original Overcooked paper (Carroll et al., 2019) collected human-play trajectories. We then collect game trajectories played by the learned human models and compare them with human-play\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nEvent\n\nPicking up an onion from onion dispenser Picking up a tomato from tomato dispenser Picking up a dish from dish dispenser Picking up a soup Viable placement Optimal placement Catastrophic placement Placing an onion into the pot Placing a tomato into the pot Delivery Order reward\n\nCj\n\n-5, 0, 5 0, 10, 20 0, 5 -5, 0, 5 -10, 0, 10 -10, 0 0, 10 -3, 0, 3 -3, 0, 3 -10, 0 0, 1\n\nTable 14: Cj for random search in Many Orders.\n\nEvent\n\nValue\n\nOptimal placement Picking up a dish from dish dispenser Picking up a ready soup from the pot\n\n3 3\n5\n\nTable 15: Reward shaping for Asymmetric Advantages, Coordination Ring and Counter Circuit in the first stage.\n\n(a) Asymmetric Advantages\n\n(b) Coordination Ring\n\n(c) Counter Circuit\n\nFigure 8: Trajectories induced by the learned human models and human players in Asymmetric Advantages, Coordination Ring and Counter Circuit. Each point or triangle denotes a trajectory with the X-axis coordinate being the self-cooking ratio, which is the ratio of onions the player places in the pot to the total amount of placements in the trajectory, and the Y-axis coordinate being the selfdelivery ratio, which is the ratio of deliveries given by the player to the total number of deliveries in the trajectory. Triangles and points denote trajectories induced by human players and learned human models, respectively. Different colors stand for different player indices. \"BC\" represents the learned human models, and \"Human\" denotes human players. Clearly, trajectories induced by the learned human models can not fully cover those by human players.\n\ntrajectories by measuring self-delivery ratio, i.e., the ratio of deliveries by the specific player to the total delivery number in a trajectory, and self-cooking ratio, which is the ratio of onions that the player places in the pot to the total pot placement number in a trajectory. The distributions of these trajectories are demonstrated in Fig. 8. From the figure, we can observe that the learned human models can not fully cover human behaviors. This suggests that evaluation results with the learned human models can not provide a comprehensive comparison among different methods.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nEvent\n\nValue\n\nOptimal placement Picking up a dish from dish dispenser Picking up a ready soup from the pot\n\n3 3\n5\n\nTable 16: Reward shaping for Asymmetric Advantages, Coordination Ring and Counter Circuit in the second stage.\n\nEvent\n\nValue\n\nPicking up a dish from dish dispenser Picking up a ready soup from the pot\n\n3 5\n\nTable 17: Reward shaping for Distant Tomato and Many Orders in the first stage.\n\nF.2 ABLATION STUDIES\n\nF.2.1 POOL SIZE\n\nTable 21 shows the average episode reward on 3 layouts with different sizes of the final policy pool for training the adaptive policy. Since increasing the pool size to 72 gives little improvement as suggested by the result, we use 36 in our experiments for computation efficiency.\n\nF.2.2 POLICY POOL CONSTRUCTION\n\nHSP has two techniques for the policy pool, i.e., (1) policy filtering to remove duplicated biased policies and (2) the use of MEP policies under the game reward for half of the pool size. We measure the performance with human proxies by turning these options off. For “HSP w.o. Filtering”, we keep all the policies by random search in the policy pool, which results in a larger pool size of 54 (18 MEP policies and a total of 36 random search ones). For“HSP w.o. MEP”, we exclude MEP policies in the policy pool and keep all the biased policies without filtering, which leads to the same pool size of 36. The results are shown in Table. 22.\n\nF.3 COOPERATION WITH SCRIPTED POLICIES WITH STRONG BEHAVIOR PREFERENCES\n\nTable 23 illustrates average episode reward and standard deviation (over 5 seeds) in all layouts with scripted policies. All values within a difference of 5 from the maximum value are marked in bold.\n\nF.4 HUMAN-AI EXPERIMENT\n\nF.4.1 EXPERIMENT SETTING\n\nWe recruited 60 volunteers by posting the experiment advertisement on a public platform. They are provided with a detailed introduction to the basic gameplay and the experiment process. The Overcooked game was deployed remotely on a server that the volunteers could access with their browsers. According to the feedback, over 90 percent of volunteers had no prior experience with Overcooked. We uniformly divided 60 volunteers into 5 groups assigned to each of the 5 layouts. We designed the experiment to last around 30 minutes for each volunteer to ensure the validity of the data. Due to availabilities of volunteers, experiments are conducted within two consecutive days. The experiment has two stages. In the first stage, which is called the warm-up stage, the participants are encouraged to explore the behaviors of 4 given AI agents without a time limit. After the first stage, they are required to comment on their game experience, e.g., whether the AI agents are cooperative and comfortable to play with, and rank the agents accordingly. In the second stage, each participant is instructed to achieve scores as high as they could in 24 games (4 AI agents × 2 player positions × 3 repeats).\n\nWe remark that, on the environment side, different from human-AI experiments performed by prior works (Zhao et al., 2021; Carroll et al., 2019) in Overcooked, we slow down the AI agents so that the AI agents have similar speed with human players. More specifically, 7 idle steps are inserted before each step of the AI agent. Such an operation is necessary since, in our prior user studies, we find that human players commonly feel uncomfortable if the AI agent is much faster and human players could contribute little to the score.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nEvent\n\nValue\n\nPicking up a dish from the dish dispenser Picking up a ready soup from the pot\n\n3 5\n\nTable 18: Reward shaping for Many Orders in the second stage.\n\nEvent\n\nPicking up a dish from the dish dispenser Picking up a ready soup from the pot Useful tomato pickup Optimal tomato placement Placing a tomato into an empty pot\n\nValue\n\n3 5\n10 5\n-15\n\nTable 19: Reward shaping for Distant Tomato in the second stage.\n\nF.4.2 HUMAN FEEDBACK\n\nWe collected and analyzed the feedback from the participants to see how they felt playing with AI agents. Here we summarize the typical reflections.\n\n1. In Coordination Ring, the most annoying thing reported is players blocking each other during movement. To effectively maneuver in the ring-like layout, players must reach a temporary agreement on either going clockwise or counterclockwise. HSP is the only AI able to make way for the other player, while others can not recover by themselves once stuck. For example, both FCP and TrajDiv players tend to take a plate and wait next to the pot immediately after one pot is filled. But they can neither take a detour when blocked on their way to the dish dispenser nor yield their position to the human player trying to pass through. The video recorded in the human study can be found in Part 4.2 of https://sites.google.com/view/hsp-iclr.\n\n2. In Counter Circuit, one efficient strategy is passing onion via the counter in the middle of the room: a player at the bottom fetches onions and places them on the counter, while another player at the top picks up the onions and puts them into pots. HSP is the only AI player capable of this strategy in both top and bottom places and performs the highest onion passing frequency cooperating with human players as shown in Figure. 6b.\n\n3. In Distant Tomato, one critical thing is that mixed (onion-tomato) soups give no reward, which means two players need to agree on the soup to cook. All AI agents perform well when the other player focuses on onion soups. However, all AI agents except for HSP fail to deal with tomato-preferring partners as shown in Table. 3. FCP, MEP, or TrajDiv agents never actively choose to place tomatoes and keep placing onions even when a pot has tomatoes in it, resulting in invalid orders. On the contrary, HSP chooses to place tomatoes when there are tomatoes in the pot. Participants commonly agree that the HSP agent is the best partner to play with in this layout. The video recorded in the human study can be found in Part 4.2 of https://sites.google.com/view/hsp-iclr.\n\n4. In Many Orders, most participants claim that HSP is able to pick up soups from all three pots, while other AI agents only concentrate on the pot in front of them and ignore the middle pot even if the human player attempts to use it. Table. 4 shows that HSP agent picks up most soups from the middle pot and meanwhile gets the highest average episode reward.\n\nF.4.3 HUMAN PREFERENCE ON DIFFERENT AI AGENTS\n\nFigure. 9 illustrates human preference for different AI agents. In all layouts except a relatively restricted and simple layout, Coordination Ring, human players strongly prefer HSP over other AI agents. In Coordination Ring, though human players rank MEP above HSP, HSP is still significantly better than FCP and TrajDiv.\n\nCalculation Method: Human preference for different methods is computed as follows. Assume we are comparing human preference between method A and method B. Let N be the total number of human players attending the experiments in one layout, NA be the number of human players who\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nrank A over B, and NB be the number of those who rank B over A. \"Human preference for method A over method B\" is computed as NA\n\nN − NB N .\n\n(a) Asy. Adv.\n\n(b) Coor. Ring\n\n(c) Coun. Circ.\n\n(d) Dis. Toma.\n\n(e) Many Ord.\n\nFigure 9: Human preference for row partner over column partner in all layouts.\n\nF.4.4 SCORES IN THE SECOND STAGE\n\nTable 24 shows average reward per episode during the second stage in all layouts. All methods have comparable episode rewards in Asymm. Adv and Coord. Ring. There is no room for improvement since all the methods have reached the highest possible rewards. In Counter Circ., the most complex layout in this category, HSP achieves a better performance than baselines: HSP has a 155+ reward while the most competitive baseline MEP has a reward of 134+. We remark that the reward difference between HSP and MEP is around 20, which is exactly the value of 1 onion soup delivery. This implies that the HSP agent can, on average, deliver one more soup than all the baselines per game episode with humans, which is a significant improvement.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nPos. Asy. Adv. Coor. R. Coun. Circ.\n\nFCP\n\nMEP\n\nTrajDiv\n\nHSP\n\n1 2\n1 2\n1 2\n1 2\n\n282.8(9.4) 203.8(8.2) 291.7(4.6) 203.4(2.0) 289.3(8.8) 194.2(0.7) 300.3(2.2) 217.1(3.3)\n\n161.3(1.6) 161.0(2.7) 161.8(0.7) 164.2(2.1) 150.8(3.1) 142.1(2.3) 160.0(2.6) 160.6(3.3)\n\n95.9(2.0) 92.7(1.3) 108.8(4.2) 111.1(0.7) 60.1(5.0) 53.7(12.4) 107.4(3.5) 106.6(3.0)\n\nTable 20: Average episode reward and standard deviation (over 5 seeds) on 3 layouts for different methods played with human proxy policies. All values within 5 standard deviations of the maximum episode return are marked in bold. The Pos. column indicates the roles played by AI policies.\n\nPos. Asy. Adv. Coor. R. Coun. Circ. Asy. Adv. Coor. R. Coun. Circ.\n\npolicy pool size = 36\n\npolicy pool size = 72\n\nFCP\n\nMEP\n\nTrajDiv\n\n1 2\n1 2\n1 2\n\n282.8(9.4) 203.8(8.2) 291.7(4.6) 203.4(2.0) 289.3(8.8) 194.2(0.7)\n\n161.3(1.6) 161.0(2.7) 161.8(0.7) 164.2(2.1) 150.8(3.1) 142.1(2.3)\n\n95.9(2.0) 92.7(1.3) 108.8(4.2) 111.1(0.7) 60.1(5.0) 53.7(12.4)\n\n278.3(16.0) 200.9(13.2) 298.2(5.4) 207.8(7.3) 270.8(2.5) 192.8(8.7)\n\n158.9(0.6) 156.9(4.7) 157.3(2.7) 158.9(3.0) 142.5(2.8) 137.3(4.9)\n\n91.9(7.5) 90.7(4.8) 104.6(5.0) 105.0(2.2) 70.1(6.7) 63.8(8.2)\n\nTable 21: Average episode reward and standard deviation (over 5 seeds) on 3 layouts for different methods played with human proxy policies. The Pos. column indicates the roles played by AI policies.\n\nHSP w.o. MEP (pool size = 36)\n\nHSP w.o. Filtering (pool size = 54)\n\nHSP (pool size = 36)\n\nPos. Asy. Adv. Coor. R. Cou. Circ.\n\n1 2\n1 2\n1 2\n\n308.5(4.4) 219.6(15.9) 311.3(8.1) 209.3(4.0) 300.3(2.2) 217.1(3.3)\n\n157.5(3.0) 157.7(2.5) 139.2(5.6) 138.5(3.1) 160.0(2.6) 160.6(3.3)\n\n94.0(2.7) 100.4(1.1) 80.1(4.6) 88.7(0.9) 107.4(3.5) 106.6(3.0)\n\nTable 22: Average episode reward and standard deviation (over 5 seeds) on 3 layouts for different methods played with human proxy policies. The Pos. column indicates the roles played by AI policies.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nm o\nr f\n\n5\n\nf o\n\ne c\nn e\nr e\nf f\ni\n\nd\n\na\n\nn\n\ni\n\nh\n\nt i\n\nW\n\nn\n\ni\n\nh\n\nt i\n\nw\n\ns e\nu\n\nl a\nv\n\nl l\n\nA\n\n. s\ne i\nc i\nl\n\no p\n\nd e\nt\n\np\n\ni r\nc s\n\nh\n\nt i\n\nw\n\ns t\n\nu o\ny a\nl\n\nl l\na\n\nn\n\ni\n\n) s\nd e\ne s\n\n5\n\nr e\nv o\n(\n\nn o\n\ni t\na i\n\nv e\nd\n\nd r\na d\nn a\nt s\n\nd n\na\n\nd r\na w\ne r\n\ne d\no s\ni\n\np e\n\ne g\na r\ne v\nA\n\n:\n\n3 2\n\ne l\n\nb a\nT\n\n. s\ne i\nc i\nl\n\no p\n\nI\n\nA y\nb\n\nd e\ny a\nl\n\np\n\ns e\nl\n\no r\n\ne h\n\nt\n\ns e\nt a\nc i\n\nd n\n\ni\n\n\" 2\n\"\n\nd n\na\n\n\" 1\n\"\n\ne h\nT\n\n.\n\nd\n\nl\n\no b\n\nn\n\ni\n\nd e\nk r\na\n\nm\n\ne r\na\n\nd r\na w\ne r\n\nm u\nm x\na\n\ni\n\nm\n\n) 4\n\n.\n\n7 1\n(\n\n3\n\n.\n\n6 9\n3\n\n) 9\n\n.\n\n4 (\n\n9\n\n.\n\n9 2\n2\n\n) 9\n\n.\n\n0 1\n(\n\n7\n\n.\n\n1 2\n1\n\n) 2\n\n.\n\n7 (\n\n) 4\n\n.\n\n3 (\n\n0\n\n.\n\n6 1\n1\n\n4\n\n.\n\n4 0\n1\n\n) 7\n\n.\n\n4 (\n\n8\n\n.\n\n7 7\n\n) 2\n\n.\n\n3 (\n\n1\n\n.\n\n7 4\n1\n\n) 8\n\n.\n\n8 1\n(\n\n9\n\n.\n\n8 7\n2\n\n) 0\n\n.\n\n2 2\n(\n\n9\n\n.\n\n1 3\n2\n\n) 5\n\n.\n\n6 (\n\n) 9\n\n.\n\n3 (\n\n9\n\n.\n\n9 1\n3\n\n3\n\n.\n\n2 2\n3\n\n) 4\n\n.\n\n2 (\n\n) 3\n\n.\n\n3 (\n\n3\n\n.\n\n7 5\n3\n\n4\n\n.\n\n1 7\n3\n\n) 3\n\n.\n\n4 1\n(\n\n8\n\n.\n\n0 2\n1\n\n) 7\n\n.\n\n7 (\n\n) 7\n\n.\n\n3 (\n\n9\n\n.\n\n4 1\n1\n\n5\n\n.\n\n0 1\n1\n\n) 5\n\n.\n\n3 (\n\n3\n\n.\n\n9 7\n\n) 1\n\n.\n\n3 (\n\n) 7\n\n.\n\n9 (\n\n) 2\n\n.\n\n8 (\n\n3\n\n.\n\n9 4\n1\n\n7\n\n.\n\n7 7\n2\n\n2\n\n.\n\n7 3\n2\n\n) 1\n\n.\n\n2 1\n(\n\n7\n\n.\n\n5 1\n3\n\n) 0\n\n.\n\n4 (\n\n7\n\n.\n\n6 2\n3\n\n) 9\n\n.\n\n8 (\n\n) 4\n\n.\n\n3 (\n\n) 7\n\n.\n\n6 (\n\n) 3\n\n.\n\n5 (\n\n7\n\n.\n\n0 5\n3\n\n8\n\n.\n\n1 2\n2\n\n1\n\n.\n\n8 1\n1\n\n1\n\n.\n\n7 0\n1\n\n) 4\n\n.\n\n3 1\n(\n\n1\n\n.\n\n8 7\n\n) 5\n\n.\n\n1 (\n\n0\n\n.\n\n5 5\n\n) 5\n\n.\n\n5 (\n\n9\n\n.\n\n7 0\n1\n\n) 8\n\n.\n\n4 (\n\n2\n\n.\n\n7 1\n\n) 6\n\n.\n\n7 1\n(\n\n3\n\n.\n\n4 5\n1\n\n) 7\n\n.\n\n5 (\n\n) 5\n\n.\n\n1 (\n\n6\n\n.\n\n4 5\n2\n\n6\n\n.\n\n2 9\n2\n\n) 2\n\n.\n\n5 2\n(\n\n5\n\n.\n\n6 9\n2\n\n) 0\n\n.\n\n6 (\n\n2\n\n.\n\n8 5\n3\n\n) 1\n\n.\n\n1 1\n(\n\n7\n\n.\n\n5 1\n1\n\n) 2\n\n.\n\n5 (\n\n5\n\n.\n\n7 0\n1\n\n) 1\n\n.\n\n2 1\n(\n\n0\n\n.\n\n6 8\n\n) 9\n\n.\n\n2 (\n\n3\n\n.\n\n9 5\n\n) 6\n\n.\n\n5 (\n\n5\n\n.\n\n9 0\n1\n\n) 1\n\n.\n\n4 1\n(\n\n6\n\n.\n\n9 2\n\n) 6\n\n.\n\n1 2\n(\n\n) 1\n\n.\n\n0 1\n(\n\n3\n\n.\n\n5 7\n1\n\n8\n\n.\n\n3 6\n2\n\n) 2\n\n.\n\n3 (\n\n8\n\n.\n\n8 9\n2\n\n) 8\n\n.\n\n2 (\n\n9\n\n.\n\n5 6\n3\n\n) 3\n\n.\n\n5 (\n\n) 2\n\n.\n\n3 (\n\n3\n\n.\n\n0 3\n2\n\n4\n\n.\n\n3 2\n1\n\n) 3\n\n.\n\n7 (\n\n) 2\n\n.\n\n6 (\n\n) 4\n\n.\n\n2 (\n\n0\n\n.\n\n9 9\n\n4\n\n.\n\n7 8\n\n5\n\n.\n\n2 5\n\n) 8\n\n.\n\n5 (\n\n7\n\n.\n\n0 1\n1\n\n) 6\n\n.\n\n3 (\n\n3\n\n.\n\n4 1\n\n) 4\n\n.\n\n7 (\n\n8\n\n.\n\n4 7\n1\n\n) 5\n\n.\n\n0 6\n(\n\n) 4\n\n.\n\n2 1\n(\n\n6\n\n.\n\n7 2\n2\n\n2\n\n.\n\n7 2\n3\n\n) 7\n\n.\n\n5 2\n(\n\n0\n\n.\n\n5 9\n2\n\n) 5\n\n.\n\n1 (\n\n) 6\n\n.\n\n3 (\n\n) 1\n\n.\n\n3 (\n\n8\n\n.\n\n6 6\n3\n\n6\n\n.\n\n4 2\n1\n\n4\n\n.\n\n1 0\n1\n\n) 9\n\n.\n\n3 (\n\n) 1\n\n.\n\n1 (\n\n6\n\n.\n\n0 9\n\n5\n\n.\n\n3 5\n\n) 5\n\n.\n\n6 (\n\n1\n\n.\n\n2 1\n1\n\n) 6\n\n.\n\n7 1\n(\n\n9\n\n.\n\n5 2\n\n) 9\n\n.\n\n9 (\n\n9\n\n.\n\n5 8\n1\n\n) 2\n\n.\n\n1 6\n(\n\n) 7\n\n.\n\n2 1\n(\n\n1\n\n.\n\n4 2\n2\n\n1\n\n.\n\n9 2\n3\n\n) 3\n\n.\n\n5 (\n\n) 9\n\n.\n\n3 (\n\n) 2\n\n.\n\n7 (\n\n7\n\n.\n\n5 6\n3\n\n5\n\n.\n\n0 3\n2\n\n1\n\n.\n\n2 1\n1\n\n) 4\n\n.\n\n3 (\n\n0\n\n.\n\n7 9\n\n) 5\n\n.\n\n9 (\n\n) 2\n\n.\n\n5 (\n\n7\n\n.\n\n2 6\n\n1\n\n.\n\n6 5\n\n) 6\n\n.\n\n2 1\n(\n\n4\n\n.\n\n2 7\n\n) 6\n\n.\n\n7 (\n\n8\n\n.\n\n7 1\n\n) 3\n\n.\n\n7 (\n\n3\n\n.\n\n8 5\n1\n\n) 8\n\n.\n\n2 1\n(\n\n8\n\n.\n\n9 7\n2\n\n) 9\n\n.\n\n4 (\n\n2\n\n.\n\n4 2\n3\n\nP S\nH\n\ni\n\nv D\nj a\nr\n\nT\n\nP E\nM\n\nP C\nF\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n2\n\n1\n\n) 8\n\n.\n\n0 2\n(\n\n0\n\n.\n\n4 0\n3\n\n.\n\na l\n\nP\n\n. i\n\nn O\n\ns t\n\np\n\ni r\nc S\n\n) 9\n\n.\n\n2 (\n\n9\n\n.\n\n4 6\n3\n\n) 7\n\n.\n\n8 (\n\n0\n\n.\n\n6 0\n1\n\n) 1\n\n.\n\n4 (\n\n9\n\n.\n\n1 9\n\n) 8\n\n.\n\n8 (\n\n0 8\n\n.\n\n4 6\n\n) 3\n\n.\n\n5 (\n\n9\n\n.\n\n7 5\n\n) 5\n\n.\n\n1 1\n(\n\n0\n\n.\n\n3 7\n\n) 6\n\n.\n\n2 (\n\n5\n\n.\n\n3 1\n\n) 9\n\n.\n\n4 (\n\n5\n\n.\n\n7 9\n1\n\n) 5\n\n.\n\n9 1\n(\n\n4\n\n.\n\n5 8\n2\n\n) 6\n\n.\n\n5 (\n\n1\n\n.\n\n4 3\n3\n\n. i\nl e\nD &\n\n.\n\na l\n\nP\n\n. i\n\nn O\n\n.\n\ny r\ne v\nE\n\n. i\n\nn O\n\n.\n\ny r\ne v\nE\n\nh s\ni\n\nD\n\n.\n\ny r\ne v\nE\n\n. i\n\nn O\n\n.\n\ny r\ne v\nE\n\nh s\ni\n\nD\n\n.\n\nu o\nC\n\n.\n\nd\n\ni\n\nM\n\n2\n\n. i\n\nn O\n\n.\n\na l\n\nP\n\n.\n\na\n\nm o\nT\n\n. i\nl e\nD &\n\n.\n\na l\n\nP\n\n.\n\na\n\nm o\nT\n\n.\n\na l\n\nP\n\n.\n\na\n\nm o\nT\n\n. i\nl e\nD &\n\n.\n\na l\n\nP\n\n.\n\na\n\nm o\nT\n\n.\n\nv d\nA\n\n.\n\ny s\n\nA\n\ng n\n\ni\n\nR\n\n. r\no o\nC\n\n.\n\nc r\ni\n\nC\n\n.\n\nn u\no C\n\n.\n\na\n\nm o\nT\n\n. s\ni\n\nD\n\n.\n\nd r\n\nO y\nn a\n\nM\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nPos. Asy. Adv.\n\nCoor. Ring Cou. Circ. Dis. Toma. Many Ord.\n\nFCP\n\nMEP\n\nTrajDiv\n\nHSP\n\n1 2\n1 2\n1 2\n1 2\n\n339.3(38.17) 321.3(34.80) 329.7(45.97) 324.2(39.93) 329.0(43.18) 318.8(48.97) 336.0(35.55) 318.8(48.97)\n\n185.0(19.73) 180.7(22.98) 193.3(22.11) 183.6(26.75) 184.7(28.60) 176.8(31.33) 185.5(38.92) 188.9(22.00)\n\n127.7(28.14) 118.3(29.20) 136.9(27.00) 134.5(28.63) 112.6(25.78) 105.0(31.05) 158.0(28.56) 155.2(23.43)\n\n351.3(82.25) 320.5(66.49) 341.9 (65.07) 313.9(78.29) 327.1(71.04) 316.0(77.65) 331.6(61.33) 305.9(58.61)\n\n312.4(58.73) 321.3(61.12) 322.0(50.53) 319.2(52.98) 312.9(62.82) 334.2(57.99) 384.3(47.50) 380.7(62.27)\n\nTable 24: Average reward per episode in all layouts with human players in the second stage.\n\n31",
    "reference": "# Summary Of The Paper\n\nThe authors train agents with self play that have diverse preferences that differ from the original task and show that having a richer pool of agents generated with this technique yields policies that are better adapted to human coordination.\n\n# Strength And Weaknesses\n\nThe empirical results are excellent. In particular, I think the combination of ablations, human experiments, scripted bots, and imitation-trained policies go beyond most any other works and clarify key issues that were not carefully analyzed in previous works. For instance, the authors clearly show that the techniques used in most evaluations (imitation-learned policies and human interaction) are highly confounded since people adapt, and the imitation-learned policies don’t show much diversity. Their method only weakly improves over baselines in these tasks. In contrast, the use of specific scripted probes and a more qualitative evaluation revealed large discrepancies between actual coordination performance in the most important edge cases. \n\nThe algorithmic contribution is a weakness as it depends on significant hand-tuning of custom features specific for these specific Overcooked environments. I do not see how this approach could be easily adapted to a new tasks (or even an Overcooked level with different dynamics). On its own, I do not think this algorithm is a sufficient contribution to literature. I would have also liked to see comparisons or thoughts on more model-based towards generating diversity in Overcooked for example: Wu, Sarah A., et al. \"Too Many Cooks: Bayesian Inference for Coordinating Multi‐Agent Collaboration.\" Topics in Cognitive Science 13.2 (2021): 414-432.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written with significant details available in the appendix. The work is original. The units in Figure 5 should be explained in the text\n\n# Summary Of The Review\n\nBased on the new method alone I would not accept this paper. However, the thoroughness of evaluation sets a new standard and I feel that I learned something new and important from these empirical analyses. I would like to cite this paper in the future and that should be sufficient for acceptance. I would raise my score further if the authors can more greatly emphasize these contributions in their work\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCAUSAL ATTENTION TO EXPLOIT TRANSIENT EMERGENCE OF CAUSAL EFFECT\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe propose a causal reasoning mechanism called causal attention that can improve performance of machine learning models on a class of causal inference tasks by revealing the generation process behind the observed data. We consider the problem of reconstructing causal networks (e.g., biological neural networks) connecting large numbers of variables (e.g., nerve cells), of which evolution is governed by nonlinear dynamics consisting of weak coupling-drive (i.e., causal effect) and strong self-drive (dominants the evolution). The core difficulty is sparseness of causal effect that emerges (the coupling force is significant) only momentarily and otherwise remains quiescent in the neural activity sequence. Causal attention is designed to guide the model to make inference focusing on the critical regions of time series data where causality may manifest. Specifically, attention coefficients are assigned autonomously by a neural network trained to maximise the Attentionextended Transfer Entropy, which is a novel generalization of the iconic transfer entropy metric. Our results show that, without any prior knowledge of dynamics, causal attention explicitly identifies areas where the strength of coupling-drive is distinctly greater than zero. This innovation substantially improves reconstruction performance for both synthetic and real causal networks using data generated by neuronal models widely used in neuroscience.\n\n1\n\nINTRODUCTION\n\nIn this work, our task is to infer causal relationships between observed variables based on time series data and reconstruct the causal network connecting large numbers of these variables. Assume the time series xit record the time evolution of variable i governed by coupled nonlinear dynamics, as represented by a general differential equation ̇xit = g(xit) + (cid:80) Bijf (xit, xjt), where g and f are self- and coupling functions respectively. The parent variable influences the dynamic evolution of the child variable via the coupling function f . Note that these two functions are hidden and usually unknown for real systems. The asymmetric adjacency matrix B represents the causal, i.e., directional coupling relationship between variables. Hence, the goal is to infer matrix B from observed time series xit, i = 1, 2, . . . , N where N is the number of variables in the system. If Bij = 1, the variable i is a coupling driver (parent variable) of variable j, otherwise it is zero.\n\nThe key challenge is that the causal effect in neural dynamics (e.g., biological neural systems observed via neuronal activity sequences) is too weak to be detected, rendering powerless classic unsupervised techniques of causal inference across multiple research communities Granger (1969); Schreiber (2000); Sugihara et al. (2012); Sun et al. (2015); Nauta et al. (2019); Runge et al. (2019); Gerhardus & Runge (2020); Tank et al. (2021); Mastakouri et al. (2021). This difficulty manifests in three aspects. First, the dynamics contains self-drive and coupling-drive. The strength of coupling f (xit, ·) is usually many orders of magnitude smaller than self-drive g(xit), and the latter dominates evolution. Second, the behavior of the coupling-drive is chaotic, unlike in linear models Shimizu et al. (2006); Xie et al. (2020). The resulting unpredictability and variability of system state means that coupling force can be significant momentarily and otherwise almost vanish, as illustrated in Figure 3 (gray lines). This dilutes the information in time series that can be useful for inferring the causal relationship. Third, in the heterogeneous networks common in applications, some variables are hubs coupled with many parent variables, among which it is difficult to distinguish individual causes. When causal effects are weak, we do not observe clearly the principle of Granger Causality, whereby the parent variable can help to explain the future change in its child variable Pfister et al.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(2019). Rather, when we train a machine learning model Nauta et al. (2019); Tank et al. (2021) for prediction task on the neuronal activity sequences, the model only exploits the historical information of the child variable itself and that from parent variables is ignored. We posit that coupling-drive makes a negligible contribution to dynamic evolution in the majority of samples of time series data. In other words, only in a small fraction of samples is the information of parent variables effective in predicting the evolution of child variables. Taking as an example the gradient algorithm to minimise the regression error over all samples (cid:80) t(xit − ˆxit)2, the adjustment of model parameters from the tiny samples corresponding to significant coupling force is negligible, but these are the only samples which could induce the model to exploit causal effects in reducing regression error. Similarly, for transfer entropy Schreiber (2000), which measures the reduction in uncertainty which a potential parent variable provides to a potential child variable, there is no significant difference in measured value between ordered pairs of variables with and without causality.\n\nTo overcome the difficulty, we propose a causal reasoning mechanism – causal attention – to identify the moments when causal effect emerges. We design an objective function, Attention-extended Transfer Entropy (AeTE), comprising a weighted generalisation of transfer entropy. In order to maximize AeTE, the causal attention mechanism trains neural networks to autonomously allocates high attention coefficients at at times t where information of parent variables effectively reduces the uncertainty of child variables, and ignores other positions by setting at close to zero. If we consider each value in a time series as a feature, the operation of attention allocation is also equivalent to removing the non-causal features Kusner et al. (2017); Hu et al. (2021).\n\nHowever, noise in empirical samples may also produce high transfer entropy regions, which leads to spurious causal effects even when using causal attention. We add a binary classification model to perform more sophisticated inference under the guidance of causal attention to focus on these critical regions and recognize different patterns between noisy and sparse emergence of causal effect. We deal with this class of causal inference task by way of small sample supervised learning. Although training and test data have a distribution shift in the setting of small samples, they arise through an identical underlying generation process. Thus, if the model provides an insight into the underlying dynamics – the coupling-drive for causal inference – then the understanding acquired from small samples can be effectively utilised in the test environment Bareinboim & Pearl (2014); Battaglia et al. (2016); Makhlouf et al. (2020); Pessach & Shmueli (2022). The role of causal attention is to help the classification model gain this insight. Our contributions are summarized as follows:\n\n1. We introduce causal attention, a causal reasoning mechanism to identify the positions of time series at which causal effect emerges and guide a classification model to infer causality focusing on these critical positions. Without any prior knowledge of dynamics, the mechanism determines the areas where the coupling force is substantially different from zero.\n\n2. By formulating Transfer Entropy as the difference between two types of mutual information, and based on the dual representation of Kullback-Liebler (KL) divergence, we design a differentiable metric, Attention-extended Transfer Entropy, as the objective function of the proposed causal attention mechanism.\n\n3. Our method significantly improves performance on synthetic and real causal networks using the data generated by five well-known neural dynamic models, and the number of labels required is very small compared to the size of the causal networks.\n\nOur methodology has limitations (i.e., cases for which performance improvement is less): 1. Dense networks, where a variable is coupled with many driving variables such that their causal effects overlap and are harder to distinguish. 2. Intense noise, which makes the casual attention mechanism falsely identify high transfer entropy regions. The downstream classifier then extracts non-causal features, leading to the reduction of its generalization. 3. Strongly coupled system, which is dominated by synchronization phenomena in which the dynamic behaviors of all variables are similar.\n\n2 BACKGROUND\n\n2.1 DEFINITION OF TRANSFER ENTROPY\n\nThe transfer entropy, an information-theoretic causality measure, is able to detect information flow between time series X and Y . Transfer Entropy measures the degree of non-symmetric dependence\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nof Y on X, defined as Schreiber (2000):\n\nT E(X → Y ) =\n\n(cid:88)\n\n(cid:16)\n\np\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\np\n\n(cid:17)\n\nlog\n\n(cid:16)\n\n(cid:17)\n\n, x(l)\n\nt\n\nyt+1 | y(k) (cid:16)\n\nt\n\nyt+1 | y(k)\n\nt\n\np\n\n(cid:17) ,\n\n(1)\n\nwhere x(l) t = (yt, ..., yt−k+1) and k, l are lengths. For an uncoupled system (X and Y independent) that can be approximated by a Markov process of order k,\n\nt = (xt, ..., xt−l+1) and y(k)\n\nthe conditional probability to find Y in state yt+1 at time t + 1 satisfies p\n\n(cid:16)\n\np\n\nyt+1 | y(k)\n\nt\n\n(cid:17)\n\n.\n\n(cid:16)\n\nyt+1 | y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\n=\n\n2.2 MUTUAL INFORMATION NEURAL ESTIMATION\n\nThe mutual information is equivalent to the KL divergence between the joint distribution PXY and the product of the marginal distributions PX ⊗ PY Nowozin et al. (2016); Hjelm et al. (2018). The KL divergence DKL admits the neural dual representation Donsker & Varadhan (1983); Belghazi et al. (2018):\n\nM I(X, Y ) = DKL (PXY ∥PX , PY ) ≥ sup θ∈Θ\n\nEPXY [fθ] − log (cid:0)EPX ⊗PY\n\n(cid:2)efθ (cid:3)(cid:1) ,\n\n(2)\n\nwhere the supremum is taken over parameter space Θ and fθ is the family of functions parameterized by the neural network with parameters θ ∈ Θ. The mutual information neural estimator is strongly consistent and can approximate the actual value with arbitrary accuracy Belghazi et al. (2018).\n\n3 METHOD\n\n3.1 NEURAL ESTIMATOR OF TRANSFER ENTROPY\n\nFigure 1: Visual interpretation of transfer entropy and its attention-extended version. The Transfer Entropy is derived as the difference of two types of mutual information: M I (Yt+1, (Yt, Xt)) (blue area) quantifies the reduction in uncertainty of future state yt+1 from knowing current states (yt, xt), and M I (Yt+1, Yt) (green area) is same but only yt is known. The attention coefficients at (yellow area) are assigned to each position of time series by the causal attention mechanism to maximize the Attention-extended Transfer Entropy. For brevity, k = l = 1 here.\n\nBy the conditional Bayes formula and adding a marginal distribution of Y , we derive the transfer entropy as the difference between two types of mutual information. An intuitive description is provided in Figure 1, and the derivation is placed in Appendix A.\n\nT E(X → Y )\n\n(cid:88)\n\n(cid:16)\n\np\n\n=\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\nlog\n\n=M I\n\n(cid:16)\n\nYt+1,\n\n(cid:16)\n\nY (k)\n\nt\n\n, X (l)\n\nt\n\n(cid:17)(cid:17)\n\n(cid:16)\n\np\n\nyt+1, y(k) (cid:16)\n\nt\n\n(cid:17)\n\n, x(l)\n\nt\n\ny(k)\n\nt\n\n, x(l)\n\nt\n\np (yr) p (cid:16)\n\n− M I\n\nYt+1, Y (k)\n\nt\n\n(cid:88)\n\n(cid:16)\n\np\n\nyt+1, y(k)\n\nt\n\n(cid:17)\n\nlog\n\n(cid:17) −\n\n(cid:17)\n\n.\n\n(cid:17)\n\n(cid:16)\n\np\n\nyt+1, y(k) (cid:16)\n\nt\n\ny(k)\n\nt\n\np (yr) p\n\n(cid:17) (3)\n\n(4)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nIn these expressions, yr is sampled from Y randomly and independently of the time step t. The first term M I quantifies the reduction in the uncertainty of the future state\n\n, X (l)\n\nY (k)\n\n(cid:17)(cid:17)\n\n(cid:16)\n\n(cid:16)\n\nYt+1,\n\nt\n\nt\n\nyt+1 from knowing the historical information y(k) the reduction in uncertainty simply from knowing y(k) differentiable estimator of transfer entropy as:\n\nt\n\nt\n\nand x(l)\n\nt\n\n. The second term M I\n\n(cid:16)\n\nYt+1, Y (k)\n\nt\n\n(cid:17)\n\nis\n\n. By connecting Eq. 4 and Eq. 2, we define a\n\nT E(X → Y ) = sup\n\nE\n\nΘ\n\n(cid:16)\n\nP\n\nYt+1,Y (k)\n\nt\n\n,X (l)\n\nt\n\n(cid:17) [fθ] − log\n\nE\n\nP (Yt+1)⊗P\n\n(cid:18)\n\n(cid:19)\n\n(cid:17) (cid:2)efθ (cid:3)\n\n,X (l)\n\nt\n\n(cid:16)\n\nY (k)\n\nt\n\n− sup\n\nE\n\nΦ\n\n(cid:16)\n\nP\n\nYt+1,Y (k)\n\nt\n\n(cid:17) [fφ] − log\n\n(cid:16)\n\nEP (Yt+1)⊗P (Y (k)\n\nt\n\n)\n\n(cid:2)efφ(cid:3)(cid:17)\n\n.\n\n(5)\n\nTransfer entropy, and even mutual information, is difficult to compute Paninski (2003), especially for high-dimensional or noisy data. In Appendix B, we offer a theoretical proof for the consistency and convergence properties of Transfer Entropy Neural Estimation, and examine its bias on a linear dynamic system where the true values of transfer entropy can be determined analytically.\n\n3.2 ATTENTION-EXTENDED TRANSFER ENTROPY\n\nThe main difficulty in our task is that the causal effect in certain nonlinear dynamical systems is too weak to be recognized by classic techniques. We discuss the limitation of the iconic transfer entropy in detail that it works well when the three true distributions, i.e., one joint distribution and two conditional distributions in Eq. 1, can be estimated perfectly. However, sparse causal effects are easily masked if the estimated probability density deviates even slightly from the real distribution. These momentary sources of evidence of coupling drive are like outliers in the total distribution of a time series dominated by self-drive. In order to make the transfer entropy provide a clear distinction between causal and non-causal pairs, we need to highlight the positions where\n\n(cid:16)\n\np\n\nyt+1 | y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\n(cid:16)\n\n> p\n\nyt+1 | y(k)\n\nt\n\n(cid:17)\n\nand filter out other times by adjusting at in Eq. 6, all while\n\navoiding the problem of distribution approximation. We do so by defining AeTE as: (cid:17)\n\n(cid:16)\n\nAeT E(X → Y ) =\n\n(cid:88)\n\n(cid:16)\n\nat · p\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\nlog\n\n= M I\n\n(cid:16)\n\nYt+1,\n\n(cid:16)\n\nY (k)\n\nt\n\n, X (k)\n\nt\n\n(cid:17)\n\n(cid:17)\n\n| A\n\np\n\np\n\nyt+1 | y(k) (cid:16)\n\nt\n\n, x(l) t\n(cid:17)\n\nyt+1 | y(k) (cid:16)\n\nt\n\nYt+1, Y (k)\n\nt\n\n− M I\n\n(6)\n\n(7)\n\n(cid:17)\n\n.\n\n| A\n\nIn this expression, at ∈ [0, 1] is the attention coefficient at time step t and the collection A of attention coefficients is the attention series. Comparison of Eq.1 and Eq.6 reveals that the transfer entropy can be viewed as a simplified version of AeTE in which attention coefficients are uniformly set to one: ∀t, at = 1. Because each position has an equal contribution to estimation, the value of transfer entropy is dominated by the majority of positions where causal effect is negligible, i.e.,\n\n(cid:16)\n\nwhere p\n\nyt+1 | y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\n(cid:16)\n\n≈ p\n\nyt+1 | y(k)\n\nt\n\n(cid:17)\n\n. Similarly to transfer entropy, AeTE is derived as the\n\ndifference of two mutual informations, but AeTE incorporates the scheme of attention assignment. By connecting Eq. 7 and Eq. 2, we define a differentiable estimator of AeTE as:\n\nAeT E(X → Y ) = sup\n\nΘ\n\n1 L\n\n(cid:88)\n\nat · fθ\n\n(cid:16)\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\n− log\n\n(cid:18) 1 L\n\n(cid:88)\n\nat · efθ\n\n(cid:16)\n\nyr,y(k)\n\nt\n\n(cid:17)(cid:19)\n\n,x(l)\n\nt\n\n− sup\n\nΦ\n\n1 L\n\n(cid:88)\n\nat · fφ\n\n(cid:16)\n\nyt+1, y(k)\n\nt\n\n(cid:17)\n\n− log\n\n(cid:18) 1 L\n\n(cid:88)\n\nat · efφ\n\n(cid:16)\n\nyr,y(k)\n\nt\n\n(cid:17)(cid:19)\n\n,\n\n(8)\n\nwhere T is the total number of time steps and L = T −max(k, l). The expectation on the distribution of variables is adapted into the mean over time series.\n\n3.3 CAUSAL ATTENTION MECHANISM\n\nThe overall framework of our model is presented in Figure 2. In addition to two neural networks fθ and fφ for mutual information estimation, we employ another neural network gα for causal attention\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Graphical illustration of causal attention mechanism framework. An input sample is the time series on an order pair of variables with shape [2, L]. Stage 1: the neural network gα assigns the attention coefficients {ai}L i=1. The neural networks fθ and fφ forming transfer entropy estimator estimate mutual information M I1 and M I2 (first and second terms in Eq. 7). Stage 2: the inferred probability of causality is Sigmoid( 1 L\n\n(cid:80) ai ∗ logiti).\n\nassignment. Rather than approximating distributions, the neural network gα learns to maximize AeTE given by Eq. 8 via gradient descent. However, the occurrence of high transfer entropy regions may appear due to noise in empirical samples. For more sophisticated inference, we augment our method with a binary classifier hη guided by causal attention to focus on high transfer entropy regions and recognize different patterns between noise and sparse emergence of causal effect. The classifier takes causality and non-causality as class labels. Then, the training process is divided into two independent stages: causal attention learning and classification learning. The objectives in the first stage are:\n\nθ, φ ← argmax\n\nL1 + L2\n\nθ,φ|α\n\nα ← argmax\n\nL1 − L2,\n\nα|θ,φ\n\n(9)\n\n(10)\n\nwhere L1, L2 is the expectation of the first and second sup term of Eq.8 on training set respectively. We update (fθ, fφ) and gα alternately. A small learning rate is required to maintain training stability, otherwise the gα may fall into a trivial solution where attention is almost zero throughout the time series. The objective in the second stage is:\n\nη ← argmin\n\nL3,\n\nη|α\n\n(11)\n\nwhere L3 is the binary cross entropy and the notation η | α indicates that causal attention remains fixed during the second stage of training. The downstream classifier is sensitive to the upstream scheme of attention assignment. In experiments, there exists an optimal loss interval for training the attention model gα. We stop the first stage training when the loss value of objective Eq. 10 is stable in this interval, and then the downstream classifier hη usually obtains the best generalization performance. For different dynamics, their optimal intervals are different and we find them empirically1. Details on the implementation of causal attention mechanism are provided in Appendix C.\n\n4 EXPERIMENT\n\nWe describe our experiment setup and extensively evaluate the causal attention mechanism on neuronal dynamics coupled on both synthetic and real causal networks.\n\n4.1 SETUP\n\nCausal networks. For synthetic networks, we generate ten groups of Erd ̋os-R ́enyi (ER) and scalefree (SF) directed networks with one hundred nodes (i.e., variables) uniformly and with mean degree varying from 5 to 41 by adjusting the probability for edge creation in ER and the total number of\n\n1An alternative design, which we have not yet implemented, would involve joining the first and second stages. Attention model gα would be trained by not only maximizing AeTE but also responding to feedback from the classifier, and would find the balance between Eq. 10 and Eq. 11 automatically.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nedges in SF. Symmetric links (both xi → xj and xj → xi) can exist. For each set of network parameters we consider five independently generated instances. For real networks, we select five neurological connectivity datasets as presented in Table 1, each from a different species: Cat, Macaque, Mouse, Worm and Rat.\n\nDataset\n\nRegion\n\n#Nodes\n\n#Edges Mean degree\n\nCat Macaque Mouse Worm Rat\n\nBrain Brain Cortex Neural Brain\n\n65 242 195 272 503\n\n1139 4090 214 4451 30088\n\n17.5 16.9 1.1 16.4 59.8\n\nTable 1: Statistical information of five real networks: dataset name, type of network, number of nodes, number of edges and mean degree ⟨k⟩. Detailed introduction are provided in Appendix E.\n\nTable 2: Equations of the five dynamical models considered. B is the asymmetrical adjacency matrix of the causal network, recording causal relationships between nodes. Bij = 1 if variable i is the parent of variable j, otherwise Bij = 0. In these expressions, Γ describes the coupling-drive, while other terms represent self-drive. The detailed configuration of dynamical parameters is provided in Appendix D.\n\nDynamics\n\nEquations\n\nHindmarsh-Rose\n\nMorris-Lecar\n\nIzhikevich\n\nRulkov\n\n ̇pi = qi − ap3 ̇qi = c − dp2 Γ = gc (Vsyn − pi) (cid:80)N\n\ni + bp2 i − qi,\n\ni − n + Iext + Γ\n\n ̇ni = r [s (pi − p0) − ni]\n\nj=1Bij /(1 + exp(−λ (pj − Θsyn)))\n\nC ̇V =I − gL (V − VL) − gCam∞(V ) (V − VCa) − gK n (V − VK ) + Γ\n\n ̇n =λ(V ) (n∞(V ) − n) , Γ = gc\n\n(cid:80)N\n\nj=1 Bij (nj − ni)\n\n ̇vi = 0.04v2\n\n ̇ui = a(bvi − ui), Γ = gc\n\ni + 5vi + 140 − ui + I + Γ j=1Bij uj\n\n(cid:80)N\n\nF1(ui, wi) =\n\nβ 1 + u2 i\n\n+ wi + Γ (uj ) , F2(ui, wi) = wi − νui − σ\n\nΓ (uj ) = gc\n\n(cid:80)N\n\nj=1Bij / (1 + exp(λ (uj − Θs)))\n\nFitzHugh-Nagumo\n\n ̇v = a + bv + cv2 + dv3 − u + Γ\n\n ̇u = ε(ev − u), Γ = −gc\n\n(cid:80)N\n\nj=1 Bij (vj − vi) .\n\nDynamic models. We use five dynamic models for neural activity simulation widely used in the field of neuroscience: Hindmarsh-Rose (HR), Morris-Lecar (Morris), Izhikevich (Izh), Rulkov and FitzHugh-Nagumo (FHN). Dynamic equations are provided in Table 2, and segments of generated time series are represented in Figure 3.\n\nEvaluation metrics. We measure the following metrics: (1) the area under the receiver operating characteristic curve (AUROC); and (2) the area under the precision-recall curve (AUPRC).\n\nBaselines. We compared our method with seven baselines: (1) Granger causality test (Ganger) Granger (1969); (2) Transfer Entropy (TE), as in Eq. 5; (3) Convergent cross mapping (CCM) Sugihara et al. (2012); (4) Latent convergent cross mapping (Latent CCM) De Brouwer et al. (2020); (5) PCMCI Runge et al. (2019) and (6) PCMCI+ Runge (2020) using partial correlation to quantify causal strength; (7) Classification model with Convolutional Block Attention Module (TA) Woo et al. (2018).\n\nTraining details. We employ the convolutional neural network for model gα and hη, and the fullyconnected neural network for model fθ and fφ. We use the ADAM Kingma & Ba (2014) optimizer with the initial learning rate of 10−4 for classifier hη and 10−5 for the others. The batch size is 10. For synthetic networks, we select randomly twenty ordered pairs of variables as a training/validation set and four hundred ordered pairs as a test set. For real networks, the sample set scheme is provided in Table 3. All sets are composed of equal samples with causality and without causality. The total time step T of time series is 50,000. Gaussian measurement noise is added with mean zero and standard deviation 1%/10% that of the original time series for synthetic/real networks respectively.We run all experiments in this work on a local machine with two NVIDIA Tesla V100 32GB GPUs.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Insight into the coupling drive of dynamics. Top panel in each subplot: segment of time series from a ordered pair of variables with a causal relationship (blue is parent and green is child). Bottom panel in each subplot: the absolute value of coupling force (gray line), traditional attention (light pink line), and causal attention (orange line). (a) Hindmarsh-Rose; (b) Izhikevich; (c) FitzHugh-Nagumo; (d) Rulkov.\n\n4.2 RESULT\n\nInsight into the coupling-drive of underlying dynamics. The gray lines in Figure 3 represent the change of coupling force from parent to child variable over time, and are generated by the coupling term Γ in Table 2. The absolute value of the coupling force rises (the gray lines spike) at occasional moments when the behavior of a parent variable substantially influences the evolution of its child variable, and remains almost zero at other times. The orange lines representing the causal attention keep in step with the gray lines, indicating that the causal attention mechanism recognizes the effect of coupling force in reducing the uncertainty of the child variable and pays attention to the areas where coupling force is significant. In Figure 3(d), the causal attention focuses on two separated regions where the coupling forces have concentrated bursts. In contrast, the light pink lines representing the traditional attention remain close to their maximum value, indicating it is insensitive to changes in coupling force. This leads its classifier to extract features throughout the whole time series (instead of focusing on causal features). The traditional attention are not designed for causal reasoning and cannot accommodate the selection of features that correspond to the causal information.\n\nFigure 4: Comparison of classifiers on synthetic causal networks. CA: Causal Attention. Examples of dynamical model: (a) HR on ER; (b) Izh on ER; (c) HR on SF; (d) Izh on SF.\n\n7\n\n(a) Hindmarsh-Rose(b) Izhikevich(d) Rulkov(c) FitzHugh-Nagumo(a) HR in ER(b) Izh in ER(c) HR in SF(d) Izh in SFAUROC/AUPRCUnder review as a conference paper at ICLR 2023\n\nTable 3: Performance comparison on real causal networks. Values in the first column of each dataset are AUROC, and values in the second column are AUPRC. Each point contains the mean and standard deviation computed in five experiments with randomly sampled training/validation/test set in Cat (top, 10/10/500) and Mouse (bottom, 10/10/90) connectomes. Results on other three connectomes are shown in Appendix F.\n\nPerformance on test sets. Compared with the baselines, our method usually substantially improves reconstruction performance on both synthetic and real causal networks, as shown in Figure 4 and Table 3. In contrast, the classifier with traditional attention mechanism (TA) obtains low losses on training sets but has poor generalization on test sets, highlighting that mere statistical correlation for causal inference is unstable and can be spurious Cui & Athey (2022). The performance of classical unsupervised methods, for which all positions in the time series are treated equally, is also limited by the paucity of causal effects. These patterns demonstrate the importance of identifying and focusing on critical regions, which we achieve via the causal attention mechanism. In conclusion, our method slightly increases cost, due to the need for label collection, but obtains a substantial boost in performance compared with those unsupervised methods in this class of causal network reconstruction tasks.\n\nThe performance of all methods tends to decrease as the average network degree grows. Networks with larger average degree are more likely to exhibit synchronization of variables which makes it harder to distinguish cause and effect. Furthermore, a single variable in these denser networks can have many parent variables, and substantial coupling forces can emerge from distinct parents at overlapping times, making individual drivers harder to distinguish. In this circumstance, a slight variance in the scheme of causal attention assignment may cause fluctuations in the performance of the downstream classifier. Robustness of the proposed method to measurement noise and sequence length is presented in Appendix F.\n\n5 RELATED WORK\n\n5.1 CAUSAL NETWORK RECONSTRUCTION\n\nConventional frameworks assume separability, i.e., that information about causes are not contained in the caused variable itself. Several common methods Spirtes & Glymour (1991); Sun et al. (2015); Runge et al. (2019); Mastakouri et al. (2021) are based on conditional independence relations, but\n\n8\n\nHindmarsh-RoseIzhikevich Morris-LecarRulkovGrangerCCMLatent CCMPCMCITAFitzHugh-NagumoPCMCI+ATE TE0.50±0.01 0.50±0.01 0.57±0.01 0.57±0.01 0.59±0.02 0.59±0.02 0.65±0.01 0.65±0.01 0.53±0.01 0.53±0.010.59±0.01 0.54±0.01 0.44±0.01 0.49±0.01 0.40±0.03 0.44±0.02 0.74±0.01 0.73±0.02 0.57±0.02 0.62±0.010.75±0.01 0.75±0.02 0.58±0.02 0.58±0.02 0.51±0.01 0.51±0.01 0.57±0.01 0.57±0.01 0.68±0.02 0.68±0.020.69±0.02 0.69±0.01 0.55±0.02 0.55±0.02 0.48±0.03 0.48±0.03 0.51±0.01 0.51±0.02 0.64±0.02 0.64±0.010.53±0.01 0.53±0.01 0.49±0.01 0.49±0.01 0.49±0.01 0.49±0.01 0.66±0.01 0.66±0.01 0.53±0.01 0.53±0.010.57±0.01 0.57±0.01 0.52±0.02 0.52±0.01 0.52±0.02 0.52±0.01 0.63±0.02 0.63±0.02 0.56±0.01 0.56±0.010.76±0.01 0.76±0.02 0.61±0.01 0.59±0.01 0.53±0.01 0.52±0.01 0.53±0.01 0.52±0.01 0.67±0.01 0.64±0.020.91±0.01 0.88±0.01 0.80±0.01 0.76±0.01 0.61±0.02 0.62±0.01 0.92±0.01 0.89±0.01 0.70±0.01 0.67±0.01Hindmarsh-RoseIzhikevich Morris-LecarRulkovGrangerCCMLatent CCMPCMCITAFitzHugh-NagumoPCMCI+ATE TE0.62±0.03 0.62±0.02 0.51±0.02 0.51±0.01 0.54±0.03 0.54±0.02 0.89±0.01 0.89±0.02 0.21±0.04 0.21±0.010.40±0.03 0.44±0.02 0.53±0.05 0.62±0.08 0.57±0.02 0.56±0.03 0.33±0.04 0.40±0.02 0.30±0.03 0.33±0.010.55±0.05 0.55±0.04 0.24±0.05 0.24±0.01 0.47±0.04 0.47±0.04 0.58±0.01 0.58±0.02 0.58±0.05 0.58±0.040.47±0.01 0.47±0.01 0.51±0.02 0.51±0.01 0.51±0.01 0.51±0.02 0.53±0.02 0.53±0.02 0.53±0.01 0.53±0.010.50±0.02 0.50±0.01 0.46±0.03 0.46±0.01 0.50±0.02 0.50±0.01 0.77±0.03 0.77±0.03 0.13±0.02 0.13±0.010.53±0.03 0.50±0.01 0.38±0.04 0.38±0.01 0.50±0.05 0.50±0.02 0.79±0.02 0.79±0.01 0.21±0.09 0.21±0.010.97±0.02 0.95±0.04 0.51±0.02 0.53±0.02 0.51±0.03 0.51±0.02 0.89±0.01 0.83±0.05 0.94±0.01 0.89±0.010.98±0.01 0.96±0.02 0.85±0.01 0.82±0.01 0.92±0.03 0.91±0.03 0.98±0.01 0.96±0.02 0.89±0.04 0.85±0.05Under review as a conference paper at ICLR 2023\n\ndiffer in the design of condition-selection strategies or choice of conditional independence test. Granger Causality Granger (1969) is extended to nonlinear dynamics by using neural networks to represent nonlinear casual relationships Tank et al. (2021); Nauta et al. (2019). Many methods of causal discovery assume that the causal network is a directed acyclic graph. However, directed cyclic graphs are common in real systems. To address this non-separability issue, Convergent-cross mapping Sugihara et al. (2012) and its variations Clark et al. (2015); De Brouwer et al. (2020) measure the extent to which the historical record of child can reliably estimate states of the parent in reconstructed state space. However, sparse causal effect in neuronal dynamics, particularly in the presence of noise, may lead parent and child time series to appear statistically independent, so that their contribution to state estimation is hard to recognize.\n\n5.2 MUTUAL INFORMATION ESTIMATION\n\nBelghazi et al. Belghazi et al. (2018) built on a dual representation of KL divergence Donsker & Varadhan (1983) to offer a parametric mutual information neural estimator (MINE) which is linearly scalable in dimension as well as sample size, and is also trainable and strongly consistent. They also discussed another version of MINE based on the f -divergence representation Nguyen et al. (2010); Nowozin et al. (2016). Using the technique of Noise-Contrastive Estimation (NCE) Gutmann & Hyv ̈arinen (2010), based on comparing target and randomly chosen negative samples, Van den Oord et al. Van den Oord et al. (2018) proposed InfoNCE loss, minimization of which maximizes a mutual information lower bound. An important application of this contrastive learning approach has been extracting high-level representations of different data modalities Chen et al. (2020); Woo et al. (2021); Hu et al. (2021); Koch-Janusz & Ringel (2018). In our work, we extend MINE for transfer entropy estimation.\n\n6 CONCLUSION\n\nThe problem of reconstructing causal networks from observational data is fundamental in multiple disciplines of science including neuroscience, since it is a prerequisite foundation for the research about structure analysis and behavior control in causal networks. Especially, several countries have recently launched grand brain projects, and one important goal is to map the connectomes (i.e., directed links between neurons) of different species.\n\nWe proposed a novel mechanism, causal attention, to guide machine learning models to infer causal relationships while focusing on the specific areas where casual effect may emerge. We showed that this mechanism identifies weak causal effects ignored by classical techniques, and helps machine learning models gain insight into the coupling dynamics underlying time series data. Our method needs a small set of samples (i.e., a small number of known causal links), and thus raises an open problem worthy of future pursuit: for large complex systems, how to select the small number of ordered pairs of nodes that offer general pattern for identification of sparse causal effects.\n\nREFERENCES\n\nElias Bareinboim and Judea Pearl. Transportability from multiple environments with limited experiments: Completeness results. Advances in Neural Information Processing Systems, 27, 2014.\n\nPeter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. Advances in Neural Information Processing Systems, 29, 2016.\n\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference on Machine Learning, pp. 531–540. PMLR, 2018.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, pp. 1597–1607. PMLR, 2020.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nAdam Thomas Clark, Hao Ye, Forest Isbell, Ethan R Deyle, Jane Cowles, G David Tilman, and George Sugihara. Spatial convergent cross mapping to detect causal relationships from short time series. Ecology, 96(5):1174–1181, 2015.\n\nPeng Cui and Susan Athey. Stable learning establishes some common ground between causal infer-\n\nence and machine learning. Nature Machine Intelligence, 4(2):110–115, 2022.\n\nEdward De Brouwer, Adam Arany, Jaak Simm, and Yves Moreau. Latent convergent cross mapping.\n\nIn International Conference on Learning Representations, 2020.\n\nMonroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv. Communications on Pure and Applied mathematics, 36(2):183– 212, 1983.\n\nAndreas Gerhardus and Jakob Runge. High-recall causal discovery for autocorrelated time series with latent confounders. Advances in Neural Information Processing Systems, 33:12615–12625, 2020.\n\nClive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.\n\nEconometrica: Journal of the Econometric Society, pp. 424–438, 1969.\n\nMichael Gutmann and Aapo Hyv ̈arinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 297–304. JMLR Workshop and Conference Proceedings, 2010.\n\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.\n\nYeping Hu, Xiaogang Jia, Masayoshi Tomizuka, and Wei Zhan. Causal-based time series domain generalization for vehicle intention prediction. In NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nMaciej Koch-Janusz and Zohar Ringel. Mutual information, neural networks and the renormaliza-\n\ntion group. Nature Physics, 14(6):578–582, 2018.\n\nMatt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. Advances\n\nin Neural Information Processing Systems, 30, 2017.\n\nKarima Makhlouf, Sami Zhioua, and Catuscia Palamidessi. Survey on causal-based machine learn-\n\ning fairness notions. arXiv preprint arXiv:2010.09553, 2020.\n\nAtalanti A Mastakouri, Bernhard Sch ̈olkopf, and Dominik Janzing. Necessary and sufficient conditions for causal feature selection in time series with latent common causes. In International Conference on Machine Learning, pp. 7502–7511. PMLR, 2021.\n\nMeike Nauta, Doina Bucur, and Christin Seifert. Causal discovery with attention-based convolu-\n\ntional neural networks. Machine Learning and Knowledge Extraction, 1(1):312–340, 2019.\n\nXuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847–5861, 2010.\n\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. Advances in Neural Information Processing Systems, 29, 2016.\n\nLiam Paninski. Estimation of entropy and mutual information. Neural Computation, 15(6):1191–\n\n1253, 2003.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDana Pessach and Erez Shmueli. A review on fairness in machine learning. ACM Computing Surveys\n\n(CSUR), 55(3):1–44, 2022.\n\nNiklas Pfister, Peter B ̈uhlmann, and Jonas Peters. Invariant causal prediction for sequential data.\n\nJournal of the American Statistical Association, 114(527):1264–1276, 2019.\n\nJakob Runge. Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear In Conference on Uncertainty in Artificial Intelligence, pp. 1388–1397.\n\ntime series datasets. PMLR, 2020.\n\nJakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino Sejdinovic. Detecting and quantifying causal associations in large nonlinear time series datasets. Science Advances, 5 (11):eaau4996, 2019.\n\nThomas Schreiber. Measuring information transfer. Physical Review Letters, 85(2):461, 2000.\n\nShohei Shimizu, Patrik O Hoyer, Aapo Hyv ̈arinen, Antti Kerminen, and Michael Jordan. A linear non-Gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006.\n\nPeter Spirtes and Clark Glymour. An algorithm for fast recovery of sparse causal graphs. Social\n\nScience Computer Review, 9(1):62–72, 1991.\n\nGeorge Sugihara, Robert May, Hao Ye, Chih-hao Hsieh, Ethan Deyle, Michael Fogarty, and Stephan\n\nMunch. Detecting causality in complex ecosystems. science, 338(6106):496–500, 2012.\n\nJie Sun, Dane Taylor, and Erik M Bollt. Causal network inference by optimal causation entropy.\n\nSIAM Journal on Applied Dynamical Systems, 14(1):73–106, 2015.\n\nAlex Tank, Ian Covert, Nicholas Foti, Ali Shojaie, and Emily B Fox. Neural Granger causality.\n\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):4267–4279, 2021.\n\nAaron Van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv e-prints, pp. arXiv–1807, 2018.\n\nGerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Cost: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. In International Conference on Learning Representations, 2021.\n\nSanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block In Proceedings of the European Conference on Computer Vision, pp. 3–19,\n\nattention module. 2018.\n\nFeng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng Hao, and Kun Zhang. Generalized independent noise condition for estimating latent variable causal graphs. Advances in Neural Information Processing Systems, 33:14891–14902, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA DERIVATION\n\nHere we present a derivation showing that the transfer entropy equals the difference between two types of mutual information:\n\nT E(X → Y ) =\n\n(cid:88)\n\n(cid:16)\n\np\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\np\n\n(cid:17)\n\nlog\n\n(cid:16)\n\n(cid:17)\n\n, x(l)\n\nt\n\nyt+1 | y(k) (cid:16)\n\nt\n\nyt+1 | y(k)\n\nt\n\np\n\n(cid:17) .\n\n(12)\n\nApplying the conditional Bayes formula p (y | x) = p(y,x) log term of equation 12:\n\np(x) on the numerator and denominator in the\n\nT E(X → Y ) =\n\n(cid:88)\n\n(cid:16)\n\np\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\nlog\n\n(cid:16)\n\np\n\nyt+1,y(k)\n\nt\n\n(cid:17)\n\n,x(l) t\n(cid:17)\n\n(cid:16)\n\np\n\ny(k)\n\nt\n\n,x(l)\n\nt\n\n(cid:16)\n\np\n\nyt+1,y(k) t\n(cid:17)\n\n(cid:16)\n\ny(k)\n\nt\n\np\n\n.\n\n(cid:17)\n\n(13)\n\nAdding the marginal distribution of time series Y to the numerator and denominator simultaneously:\n\nT E(X → Y ) =\n\n(cid:88)\n\n(cid:16)\n\np\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n(cid:17)\n\nlog\n\n(cid:16)\n\np\n\nyt+1,y(k) (cid:16)\n\nt\n\ny(k)\n\nt\n\np(yr)p (cid:16)\n\np\n\n(cid:17)\n\n(cid:17)\n\n,x(l)\n\nt\n\n,x(l) t\n(cid:17)\n\nyt+1,y(k) (cid:16)\n\nt\n\n(cid:17)\n\ny(k)\n\nt\n\np(yr)p (cid:16)\n\nyt+1, y(k) (cid:16)\n\nt\n\np (yr) p\n\n(cid:17)\n\n, x(l)\n\nt\n\ny(k)\n\nt\n\n(cid:17)\n\n, x(l)\n\nt\n\n(cid:88)\n\n(cid:16)\n\np\n\n=\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\np\n\n(cid:17)\n\nlog\n\n(cid:88)\n\np\n\n−\n\n(cid:16)\n\nyt+1, y(k)\n\nt\n\n(cid:16)\n\np\n\n(cid:17)\n\nlog\n\nyt+1, y(k) (cid:16)\n\nt\n\n(cid:17)\n\n(cid:17)\n\np (yr) p (cid:17)\n\n− M I\n\nt\n\ny(k) (cid:16)\n\n(cid:16)\n\n= M I\n\nYt+1, Y (k)\n\nt\n\n, X (l)\n\nt\n\nYt+1, Y (k)\n\nt\n\n(cid:17)\n\n.\n\n(14)\n\n(15)\n\n(16)\n\nIn these expressions, yr is sampled from time series Y randomly each time step and independently of the time step t.\n\nB NEURAL ESTIMATOR FOR TRANSFER ENTROPY\n\nB.1 CONSISTENCY\n\nDefinition. A neural estimator (cid:98)S(X, Y )n which uses n samples from the data distribution to estimate a statistic S(X, Y ) on variables X, Y is strongly consistent if for any ε > 0, there exists a positive integer N and a choice of neural network such that:\n\n∀n ≥ N,\n\n| S(X, Y ) − S(X, Y )n |≤ ε, almost everywhere (a.e.)\n\n(17)\n\nThe Mutual Information Neural Estimator (MINE) depends on a choice of a neural network and the number of samples n from the data distribution Belghazi et al. (2018). Let fθ be the family of functions parameterized by the neural network with parameters θ ∈ Θ. MINE is defined as:\n\n(cid:100)M I(X, Y )n = sup θ∈Θ\n\nEP (n)\n\nXY\n\n[fθ] − log\n\n(cid:16)\n\nEP (n)\n\nX ⊗P (n)\n\nY\n\n(cid:2)efθ (cid:3)(cid:17)\n\n.\n\n(18)\n\nTheorem 1 Belghazi et al. (2018). MINE is strongly consistent.\n\nThe Transfer Entropy Neural Estimator (TENE) consists of two independent MINE and depends on choice of neural network and sample number n. TENE is defined as:\n\n(cid:100)T E(X → Y )n = (cid:100)M I\n\n(cid:16)\n\nYt+1,\n\n(cid:16)\n\nY (k)\n\nt\n\n, X (l)\n\nt\n\n(cid:17)(cid:17)\n\n− (cid:100)M I\n\nn\n\n(cid:16)\n\nYt+1, Y (k)\n\nt\n\n(cid:17)\n\n.\n\nn\n\n(19)\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nWe use M I [1], M I [2], (cid:100)M I\n\n(cid:16)\n\nM I\n\nYt+1, Y (k)\n\nt\n\n(cid:17)\n\n, (cid:100)M I\n\n(cid:16)\n\nYt+1,\n\nn , (cid:100)M I (cid:16) Y (k)\n\nt\n\n, X (l)\n\nt\n\nas abbreviations of M I (cid:17) (cid:16) (cid:17)(cid:17)\n\nand (cid:100)M I\n\nYt+1, Y (k)\n\nt\n\n[1]\n\n[2]\n\nn\n\nn\n\nrespectively.\n\nn\n\n(cid:16)\n\nYt+1,\n\n(cid:16)\n\nY (k)\n\nt\n\n, X (l)\n\nt\n\n(cid:17)(cid:17)\n\nand\n\nWe will prove the following: Theorem 2. TENE is strongly consistent. Proof. Let ε > 0. By Theorem 1, we can choose neural networks and integers N1, N2 and such that\n\n∀n ≥ N1,\n\n∀n ≥ N2,\n\n(cid:12) (cid:12) M I [1] − (cid:100)M I (cid:12) (cid:12) (cid:12) (cid:12) M I [2] − (cid:100)M I (cid:12) (cid:12)\n\n[1]\n\nn\n\n[2]\n\nn\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ ε/2, a.e.\n\n≤ ε/2, a.e.\n\nLetting N = max {N1, N2}, for n ≥ N and for some neural network we have, a.e.,\n\n∀n ≥ N,\n\n(cid:12) (cid:12) (cid:12)T E(X → Y ) − (cid:100)T E(X → Y )n\n\n(cid:12) (cid:12) (cid:12) =\n\n=\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(M I [1] − M I [2]) − ((cid:100)M I\n\n[1]\n\nn − (cid:100)M I\n\n(M I [1] − (cid:100)M I\n\n[1]\n\nn ) − (M I [2] − (cid:100)M I\n\n[2]\n\n(cid:12) (cid:12) n ) (cid:12) (cid:12) (cid:12) (cid:12) n ) (cid:12) (cid:12)\n\n[2]\n\n(M I [1] − (cid:100)M I\n\n[1]\n\nn )\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n+\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(M I [2] − (cid:100)M I\n\n[2]\n\nn )\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nThe proof is complete.\n\n≤ ε/2 + ε/2 = ε.\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\nB.2 VARIATION OF BIAS VARY WITH DIMENSION AND NOISE\n\nWe examine the performance of TENE for the considered class of neural networks on linear dynamic system, consisting of variables X and Y defined as:\n\nxt+1 = αxt + εx yt+1 = βyt + gcxt + εy\n\n(26)\n\n(27)\n\nWe set α = β = 0.5 and εx = εx ∼ N (0, σ2). The true values of transfer entropy T E(X → Y ) in this simple coupled system can be determined analytically Kaiser & Schreiber (2002). We can increase the dimension of the system by considering multiple independent copies of variables X and Y , in which case the mutual information and transfer entropy scale linearly with the dimension of the system. For each considered dimension, standard deviation σ, and coupling strength gc in an interval from -0.4 to 0.4, we generate a time series of length 50,000. We also consider an alternative non-parametric estimator of mutual information, the Kraskov estimator Kraskov et al. (2004) with k = 5 nearest neighbours. In Fig 5 we compare the results of MINE with the analytic formula and the Kraskov estimator. MINE shows marked improvement over the Kraskov estimator, especially when variables are high-dimensional. Comparing Fig 5(a,b) or (c,d) shows that the amplitude of the driving Gaussian noise has little influence on estimates. Interestingly, as coupling strength gc grows small, i.e., as X and Y become more independent, the Kraskov estimator can suggest a negative value of the mutual information, i.e., we estimate that M In (Yt+1, Yt, Xt) < M In (Yt+1, Yt). We deduce that irrelevant information about the nearly independent variable Xt interferes with the estimation of the mutual information by the Kraskov estimator.\n\nC ALGORITHM\n\nDetails on the implementation of causal attention mechanism are provided in Algorithm 1.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: True and estimated transfer entropy versus coupling strength gc. The dimension and standard deviation (std) σ of system noise is indicated in the titles of subplots.\n\nAlgorithm 1 Causal Inference by Causal Attention Input: Small samples S, part with causality ̄S ⊂ S, segment length for training ̄L ≪ L 1: θ, φ, α, η ← initialize network parameters 2: repeat until (L1 − L2) is stable in optimal loss interval 3: 4:\n\nfor all samples ̄S do\n\nChoose ts ∈ (cid:2)1, L − ̄L(cid:3) randomly Take segment s = {(xt, yt)}ts+ ̄L Produce joint samples in Eq. 8 (cid:110)(cid:16)\n\n(cid:17)(cid:111)ts+ ̄L\n\nt=ts\n\nyt+1, y(k)\n\nt\n\n, x(l)\n\nt\n\n, etc.\n\nt=ts\n\n5: 6:\n\n7:\n\nend for Assign causal attention A Compute L1, L2 on ̄S Update parameters Recompute L1, L2 on ̄S Update parameters α ← α + ∇α (L1 − L2)\n\n8: 9: 10: 11: 12: 13: 14: repeat until L3 convergence Compute L3 on S 15: Update parameters 16:\n\nη ← η − ∇ηL3\n\nθ, φ ← θ + ∇θL1, φ + ∇φL2\n\n14\n\n(d)(b)(c)(a)Under review as a conference paper at ICLR 2023\n\nD MODEL BRAIN DYNAMICS\n\nHere we give detailed information about five neuronal dynamics applied to modeling membrane potential and relevant quantities in biological connectomes. We input to each causal discovery algorithm the coordinate corresponding to the neuron membrane voltage potential, because this variable is most likely to be experimentally accessible.\n\nD.1 HINDMARSH-ROSE DYNAMICS\n\nThe spikes of activity in neurons are considered an important part of the brain’s information processing Borges et al. (2018); Rabinovich et al. (2006). Hindmarsh and Rose Hindmarsh & Rose (1984) (HR) proposed a phenomenological neuron model that is a simplification of the Hodgkin-Huxley model Hodgkin & Huxley (1952). The HR model is described by\n\n ̇p = q − ap3 + bp2 − n + Iext ̇q = c − dp2 − q ̇n = r [s (p − p0) − n]\n\nwhere p(t) is the action potential of the membrane, q(t) is related to the fast current and n(t) is associated with the slow current. Presynaptic neurons with an action potential pj coupled by chemical synapses to neurons i modifying its action potential pi according to\n\n ̇pi = qi − ap3\n\ni + bp2\n\ni − n + Iext + Γ\n\nΓ = gc (Vsyn − pi) (cid:80)N\n\nj=1\n\nBij 1 + exp(−λ (pj − Θsyn))\n\nwhere i, j = 1, . . . , N , N is the number of neurons, gc is the chemical coupling strength and Bij describes neurons’ chemical connections. The chemical synapse function is modeled by the above sigmoidal function, with Θsyn = 1.0. We use parameters a = 1, b = 3, c = 1, u = 5, s = 4, r = 0.005, p0 = −1.60, coupling strength gc = 0.1, Vsyn = 2, λ = 10, and external current Iext = 3.24, for which HR neurons exhibits a chaotic burst behavior.\n\nD.2 MORRIS–LECAR DYNAMICS\n\nMorris and Lecar Morris & Lecar (1981) suggested a simple two variable model to describe oscillations in a barnacle’s giant muscle fiber. The Morris–Lecar model has became quite popular in computational neuroscience community due to its biophysically meaningful and measurable parameters, which consist of a membrane potential u receiving an instantaneously activated Ca current and a more slowly activated K current n evolving according to:\n\nC ̇V =I − gL (V − VL) − gCam∞(V ) (V − VCa) − gKn (V − VK) + Γ(V )\n\n ̇n =λ(V ) (n∞(V ) − n)\n\nwhere\n\nm∞(V ) =\n\nn∞(V ) =\n\n(cid:26)\n\n(cid:26)\n\n1 2\n\n1 2\n\n1 + tanh\n\n1 + tanh\n\n(cid:21)(cid:27)\n\n(cid:21)(cid:27)\n\n(cid:20) (V − V1) V2 (cid:20) (V − V3) V4 (cid:21)\n\nλ(V ) = ̄λ cosh\n\n(cid:20) (V − V3) (2V4)\n\nwith the coupling term\n\nΓ(Vi) = gc\n\n(cid:80)N\n\nj=1 Bij (nj − ni) ,\n\nwith parameters C = 20μF/cm2, gL = 2mmho/cm2, VL = −50mV, gCa = 4mmho/cm2, VCa = 100mV, gK = 8mmho/cm2, VK = −70mV, V1 = 0mV, V2 = 15mV, V3 = 10mV, V4 = 10mV, ̄λ = 0.1 s−1, and applied current I = 34μA/cm2.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nD.3\n\nIZHIKEVICH DYNAMICS\n\nIzhikevich dynamics reproduce spiking and bursting behavior of known types of cortical neurons, and combine the biological plausibility of Hodgkin–Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons Izhikevich (2003). The equations governing Izhikevich spike dynamics are:\n\n ̇v = 0.04v2 + 5v + 140 − u + I + gc\n\n ̇u = a(bv − u)\n\nwith the auxiliary after-spike resetting\n\n(cid:88)\n\nBijuj\n\nif v ≥ +30mV,\n\nthen\n\n(cid:26) v ← c\n\nu ← u + d\n\n.\n\nHere, variable v represents the membrane potential of the neuron and u represents a membrane recovery variable, which accounts for the activation of K +ionic currents and inactivation of Na+ ionic currents, and it provides negative feedback to v. Here, we use the parameters a = 0.2, b = 2, c = −56, d = −16, I = −99. After the spike reaches its apex (+30mV), the membrane voltage and the recovery variable are reset. If v skips over 30 , then it first is reset to 30 , and then to c so that all spikes have equal magnitudes.\n\nD.4 RULKOV DYNAMICS\n\nThe Rulkov model is a map-based neuron model with a surprising abundance of features, such as periodic and chaotic spiking, and bursting. The Rulkov map is an abstract mathematical model, although it shares some specific features with others neuron models closer to experimental observations. We use synthetic time series where each neuron is simulated using the Rulkov model Eroglu et al. (2020), which has two variables, u and w, evolving at different timescales as described by x(t + 1) = (u(t + 1), v(t + 1)) = F (x(t)) = (F1(u(t), w(t)), F2(u(t), w(t))), with\n\nF1(u, w) =\n\nβ\n\n1 + u2 + w + Γ (u)\n\nand F2(u, w) = w − νu − σ.\n\nThe two variables reflect the two important time scales of a neuron model. The variable u represents the fast dynamics of the system and usually models the membrane voltage of the neuron, whereas w is the slow variable and represents the variations of the ionic recovery currents. Different combinations of parameters σ and β give rise to different dynamical states of the neuron, such as resting, tonic spiking, and chaotic bursts. As for the coupling, we consider chemical synaptic coupling, that is, H (xi, xj) = (h (ui, uj) , 0) with h (ui, uj) = (ui − Vs) Γ (uj), where\n\nΓ (uj) =\n\n1 1 + exp {λ (uj − Θs)}\n\nand electrical synaptic coupling, H (xi, xj) = (h (ui, uj) , 0), with h (ui, uj) = uj − ui. In the chemical coupling, Vs is a parameter called the reverse potential. Here, we use the parameters with β = 4.4, σ = ν = 0.001,, Vs = 20, Θs = −0.25 and λ = 10.\n\nD.5 FITZHUGH-NAGUMO DYNAMICS\n\nA FitzHugh-Nagumo neuron comprises a two-dimensional system of smooth ODEs, so cannot exhibit autonomous chaotic dynamics and bursting. Adding noise allows for stochastic bursting FitzHugh (1961). The equations governing the FitzHugh-Nagumo neuronal network dynamics are\n\n ̇v = a + bv + cv2 + dv3 − u + Γ ̇u = ε(ev − u)\n\nwith the coupling term\n\nΓ(vi) = −gc\n\n(cid:80)N\n\nj=1 Bij (vj − vi) .\n\nThe FitzHugh-Nagumo dynamics capture the firing behaviors of neurons with two components. The first component v represents the membrane potential, which contains self- and interaction dynamics,\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nand the second component u represents a recovery variable. To simulate the shape of each spike, the time step in the model must be relatively small, e.g., τ = 0.25 ms. Here, we use the parameters a = 0.28, b = 1, c = 0, d = −1, ε = 0.04, e = 12.5. Moreover, the parameters in the FitzHugh–Nagumo model can be tuned so that the model describes spiking dynamics of many resonator neurons.\n\nD.6 TIME SERIES GENERATION\n\nTo obtain the time series from above neural dynamics, we use Runge-Kutta method with variablestep to solve the ordinary differential equation of Hindmarsh-Rose and Morris–Lecar dynamics with sample interval τ = 0.1. Izhikevich dynamics are solved by the Euler formula with time step h = 0.05. For the Rulkov map we consider a unit sample interval. The total time step of time series T = 50, 000 in both synthetic and real networks.\n\nE REAL BRAIN CONNECTOMES INFORMATION\n\nE.1 CAT CONNECTOME\n\nThe cat connectivity dataset comprises a description of cortical connections in the cat brain Scannell et al. (1995), a connectivity set resulting from a comprehensive literature search of anatomical tracing studies in the cat cortex. Detailed information on the delineated regions, including information on the used parcellation scheme, abbreviations and possible overlap with other parcellation schemes, as well as information on the physiological characteristics of these regions, is given in the appendix of the original study Ref. Scannell et al. (1995). The connectivity dataset incorporates data of one hemisphere, including 65 regions and 1139 interregional macroscopic axonal projections de Reus & van den Heuvel (2013).\n\nE.2 MACAQUE CONNECTOME\n\nThe macaque connectivity data set used in this study comprises anatomical data from 410 tract tracing studies collated in the online neuroinformatics data base CoCoMac (http://cocomac.org), first analyzed and made publicly available in Ref. Modha & Singh (2010). In the present study they focused primarily on an analysis of the connectivity among regions of the cerebral cortex. The cortical connection matrix was extracted from the primary connection data by removing all subcortical (thalamus, basal ganglia, brainstem) regions. In addition, regions that did not maintain at least one incoming and one outgoing connection were also removed to ensure that the network was strongly connected. The remaining connection data set used in this study consisted of 242 regions and 4090 directed projections represented in binary format (connection present = 1, connection absent = 0) Harriger et al. (2012).\n\nE.3 MOUSE CONNECTOME\n\nThe Allen Mouse Brain Connectivity Atlas uses enhanced green fluorescent protein (EGFP)- expressing adeno-associated viral vectors to trace axonal projections from defined regions and cell types, and high-throughput serial two-photon tomography to image the EGFP-labelled axons throughout the brain. This systematic and standardized approach allows spatial registration of individual experiments into a common three dimensional (3D) reference space, resulting in a wholebrain connectivity matrix. The Allen Mouse Brain Connectivity Atlas is a freely available, foundational resource for structural and functional investigations into the neural circuits that support behavioural and cognitive processes in health and disease Oh et al. (2014).\n\nE.4 WORM CONNECTOME\n\nAll the chemical and gap junction synapses, the connectome, in the posterior nervous system of the C. elegans adult male are identified by serial section electron microscopy Jarrell et al. (2012). The feasibility of comprehensive synapse-level nervous system reconstruction by this method was a primary reason for the initial selection of C. elegans as an experimental model. They developed a PC-based software platform to facilitate assembly of a connectome from electron micrographic\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nimages. The connectome is of a single adult animal and was produced from a series of 5000 serial thin sections of 70 to 90 nm encompassing the posterior half of the body.\n\nE.5 RAT CONNECTOME\n\nBecause resliceable 3D brain models for relating systematically and topographically different parcellation schemes are still in the first phases of development, it is necessary to rely on qualitative comparisons between regions and tracts that are either inserted directly by neuroanatomists or trained annotators, or are extracted or inferred by collators from the available literature. To address these challenges, Ref. Bota et al. (2012) developed a publicly available neuroinformatics system, the Brain Architecture Knowledge Management System, including an exemplar for constructing interrelated connectomes at different levels of the mammalian central nervous system organization, and presented the latest version of the BAMS rat macroconnectome.\n\nInformation about the above datasets is summarized in Table 4.\n\nTable 4: Statistical information of six real networks: dataset name, type of network, number of nodes, number of edges, mean degree ⟨k⟩, and data acquisition method.\n\nDataset\n\nCat Macaque Mouse Worm Rat\n\nRegion\n\n#Nodes\n\n#Edges Mean degree\n\nSensor\n\nBrain Brain Cerebral Cortex Neural Brain\n\n65 242 195 272 503\n\n1139 4090 214 4451 30088\n\n17.5 16.9 1.1 16.4 59.8\n\nTract tracing studies Tract tracing studies Electron microscopy Electron Microscopy Neuroanatomical experiments\n\nF ADDITIONAL EXPERIMENTS\n\nF.1 PERFORMANCE OF TRANSFER ENTROPY ON REAL CAUSAL NETWORKS\n\nExperiment Results on Macaque/C.elegans/Rat connectome are provided in Table 5/Table 6/Table 7 as the supplement of main text Table 3. Classic methods have limited performance across various neural dynamics unfolding on real causal networks especially in a noisy environment: we add Gaussian measurement noise with mean zero and standard deviation 10% that of the original time series. As we discuss in main text Sec. 3.1, sparse causal effects are easily masked in metric of iconic transfer entropy when noise causes estimated probability densities to deviate even slightly from the true distributions.\n\nTable 5: Performance train/validation/test set is 50/50/500.\n\ncomparison on Macaque\n\nconnectome. The\n\nsample number\n\nin\n\n18\n\nHindmarsh-RoseIzhikevich Morris-LecarRulkovGrangerCCMLatent CCMPCMCITAFitzHugh-NagumoPCMCI+ATE TE0.50±0.01 0.50±0.01 0.48±0.02 0.48±0.01 0.50±0.02 0.50±0.01 0.54±0.01 0.54±0.01 0.54±0.01 0.54±0.010.44±0.01 0.49±0.01 0.53±0.05 0.52±0.05 0.54±0.03 0.51±0.02 0.48±0.07 0.49±0.04 0.43±0.02 0.45±0.010.44±0.02 0.44±0.02 0.49±0.01 0.49±0.01 0.51±0.01 0.51±0.01 0.55±0.01 0.55±0.01 0.56±0.01 0.56±0.010.47±0.01 0.47±0.01 0.51±0.02 0.51±0.01 0.51±0.01 0.51±0.02 0.53±0.02 0.53±0.02 0.53±0.01 0.53±0.010.47±0.01 0.47±0.01 0.51±0.01 0.51±0.01 0.50±0.02 0.50±0.01 0.50±0.01 0.50±0.01 0.52±0.01 0.52±0.010.47±0.02 0.47±0.01 0.48±0.01 0.48±0.01 0.51±0.02 0.51±0.01 0.52±0.02 0.52±0.01 0.52±0.01 0.52±0.010.68±0.03 0.51±0.03 0.56±0.01 0.55±0.01 0.52±0.04 0.50±0.02 0.51±0.02 0.51±0.03 0.54±0.01 0.52±0.010.71±0.04 0.65±0.03 0.60±0.02 0.59±0.01 0.66±0.03 0.64±0.03 0.59±0.03 0.57±0.03 0.63±0.02 0.59±0.02Under review as a conference paper at ICLR 2023\n\nTable 6: Performance comparison on C.elegans connectome. The sample number train/validation/test set is 50/50/500.\n\nin\n\nTable 7: Performance comparison on Rat connectome. The sample number in train/validation/test set is 100/100/500.\n\nF.2 DATA EFFICIENCY OF CAUSAL ATTENTION MECHANISM AGAINST THE LENGTH OF TIME\n\nSERIES\n\nTaking Cat and Mouse connectome as examples, we show the results of our method trained on the time series data with different lengths, i.e. total time steps, in Figure 6. The size of training and test sets is the same as the scheme in the main text Table 4. Overall, the AUROC/AUPRC scores tend to get higher as the length increases, but this tendency is not significant. It indicates that the causal attention mechanism extracts sufficient causal features for causal inference even over short time series.\n\nF.3 ROBUSTNESS OF CAUSAL ATTENTION MECHANISM AGAINST THE INTENSITY OF NOISE\n\nWe show the results of our method trained on time series data added different intensities of noise (measurement noise rather than intrinsic noise of dynamics) in Figure 7. Except for dynamics of Izh and FHN on Cat connectome, the AUROC/AUPRC scores are stable within the range 2%-10% of standard deviation. It implies that causal attention mechanism is robust by the sample noise level.\n\n19\n\nHindmarsh-RoseIzhikevich Morris-LecarRulkovGrangerCCMLatent CCMPCMCITAFitzHugh-NagumoPCMCI+ATE TE0.50±0.01 0.50±0.01 0.56±0.01 0.56±0.01 0.64±0.01 0.64±0.02 0.71±0.01 0.71±0.01 0.64±0.02 0.64±0.010.74±0.01 0.73±0.02 0.61±0.01 0.58±0.02 0.55±0.01 0.56±0.02 0.80±0.01 0.80±0.01 0.52±0.01 0.48±0.010.79±0.02 0.79±0.02 0.54±0.02 0.54±0.02 0.62±0.02 0.62±0.02 0.67±0.02 0.67±0.02 0.74±0.01 0.74±0.010.78±0.01 0.78±0.01 0.52±0.01 0.52±0.01 0.55±0.04 0.55±0.03 0.53±0.04 0.53±0.03 0.68±0.01 0.68±0.010.53±0.01 0.53±0.01 0.50±0.02 0.50±0.01 0.53±0.02 0.53±0.01 0.66±0.01 0.66±0.01 0.57±0.01 0.59±0.010.53±0.02 0.53±0.01 0.52±0.01 0.52±0.01 0.56±0.01 0.56±0.01 0.69±0.02 0.69±0.02 0.63±0.01 0.63±0.010.92±0.01 0.91±0.01 0.67±0.03 0.65±0.03 0.52±0.01 0.54±0.01 0.56±0.01 0.54±0.01 0.85±0.01 0.84±0.010.93±0.01 0.92±0.01 0.81±0.01 0.76±0.01 0.76±0.01 0.71±0.01 0.97±0.01 0.97±0.01 0.78±0.01 0.72±0.02Hindmarsh-RoseIzhikevich Morris-LecarRulkovGrangerCCMLatent CCMPCMCITAFitzHugh-NagumoPCMCI+ATE TE0.50±0.01 0.50±0.01 0.43±0.02 0.43±0.01 0.50±0.01 0.50±0.01 0.46±0.01 0.46±0.01 0.52±0.02 0.52±0.010.57±0.02 0.62±0.01 0.33±0.02 0.41±0.01 0.48±0.09 0.54±0.08 0.37±0.01 0.43±0.01 0.41±0.03 0.47±0.030.63±0.02 0.63±0.02 0.48±0.02 0.48±0.02 0.54±0.05 0.54±0.05 0.50±0.01 0.50±0.01 0.52±0.03 0.52±0.020.63±0.02 0.63±0.02 0.40±0.01 0.40±0.01 0.55±0.01 0.55±0.02 0.53±0.02 0.53±0.01 0.51±0.03 0.51±0.020.51±0.01 0.51±0.01 0.44±0.02 0.44±0.01 0.53±0.01 0.53±0.01 0.46±0.01 0.46±0.01 0.49±0.02 0.49±0.010.51±0.02 0.51±0.01 0.42±0.02 0.42±0.01 0.55±0.01 0.55±0.01 0.44±0.02 0.44±0.01 0.49±0.01 0.49±0.010.82±0.03 0.78±0.02 0.70±0.01 0.67±0.01 0.62±0.02 0.58±0.03 0.73±0.02 0.74±0.01 0.74±0.04 0.74±0.020.83±0.03 0.83±0.03 0.78±0.01 0.78±0.01 0.65±0.04 0.64±0.03 0.83±0.01 0.79±0.01 0.84±0.02 0.81±0.01Under review as a conference paper at ICLR 2023\n\nFigure 6: AUROC/AUPRC scores of the causal attention mechanism trained on time series data of different lengths. The blue bar is AUPRC, the green bar is AUROC. The x-axis indicates the scores and y-axis represents the total time step of time series.\n\nF.4 HOW MUCH DOES CAUSAL ATTENTION MECHANISM BRING THE DISTANCE BETWEEN\n\nDISTRIBUTIONS OF TRAINING AND TEST SET CLOSER TOGETHER\n\nThe causal attention mechanism helps the classifier reveal the generation processing underlying the data, i.e., coupling-drive in problem of causal inference and alleviate the dilemma of distribution shift in scene of small samples. The causal attention mechanism refines the content of samples (critical regions) and thus reduces the distribution dimension of the entire dataset. To quantify this shortened distance, we ask, how many additional training samples does traditional machine learning need to achieve the same level of generalization as our method? Taking the Cat Connectome as example, we train the classifier with traditional attention mechanism by gradually expanding the size of training set, and provide the results in Figure8. The green horizontal lines represent the AUROC value of our method using ten ordered pairs with causality (0.8% of edges in Cat Connectome) while, to achieve the same performance, the traditional classifier needs approximately 10%/15%/20%/40% samples for HR/Izh/Morris/Rulkov dynamics respectively (the blue lines cross the green lines). It also indicates that our method provides significant saving in labels collection, which is significant given that the procedure for checking connections in organisms is cumbersome in practice.\n\n20\n\nHindmarsh-RoseMorris-LecarIzhikevich RulkovFitzHugh-NagumoCatCatHindmarsh-RoseMorris-LecarIzhikevich RulkovFitzHugh-NagumoMouseMouseUnder review as a conference paper at ICLR 2023\n\nFigure 7: AUROC/AUPRC scores of the causal attention mechanism trained on time series data added different intensity of noise. The blue bar is AUPRC, the green bar is AUROC, and the y-axis indicates the percentage of standard deviation of Gaussian measurement noise in the original time series.\n\nFigure 8: Size of training set that the traditional classifier requires to achieve same level of generalization as our method. The x-coordinate indicates the percentage of the ordered pairs with causality in training set to the total edges in causal network. Examples of dynamical model: (a) HR; (b) Izh; (c) Morris; (d) Rulkov.\n\n21\n\n(a) HR(b) Izh(c) Morris(d) RulkovUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference on Machine Learning, pp. 531–540. PMLR, 2018.\n\nFS Borges, Ewandson L Lameu, Kelly C Iarosz, Paulo R Protachevicz, Iberˆe L Caldas, Ricardo L Viana, Elbert EN Macau, Antonio M Batista, and Murilo da Silva Baptista. Inference of topology and the nature of synapses, and the flow of information in neuronal networks. Physical Review E, 97(2):022303, 2018.\n\nMihail Bota, Hong-Wei Dong, and Larry W Swanson. Combining collation and annotation efforts toward completion of the rat and mouse connectomes in bams. Frontiers in neuroinformatics, 6: 2, 2012.\n\nMarcel A de Reus and Martijn P van den Heuvel. Rich club organization and intermodule commu-\n\nnication in the cat connectome. Journal of Neuroscience, 33(32):12929–12939, 2013.\n\nDeniz Eroglu, Matteo Tanzi, Sebastian van Strien, and Tiago Pereira. Revealing dynamics, commu-\n\nnities, and criticality from data. Physical Review X, 10(2):021047, 2020.\n\nRichard FitzHugh.\n\nImpulses and physiological states in theoretical models of nerve membrane.\n\nBiophysical journal, 1(6):445–466, 1961.\n\nLogan Harriger, Martijn P Van Den Heuvel, and Olaf Sporns. Rich club organization of macaque\n\ncerebral cortex and its role in network communication. PLoS ONE, 7(9):e46497, 2012.\n\nJames L Hindmarsh and RM Rose. A model of neuronal bursting using three coupled first order differential equations. Proceedings of the Royal Society of London. Series B. Biological Sciences, 221(1222):87–102, 1984.\n\nAlan Lloyd Hodgkin and Andrew Fielding Huxley. Propagation of electrical signals along giant nerve fibres. Proceedings of the Royal Society of London. Series B-Biological Sciences, 140 (899):177–183, 1952.\n\nEugene M Izhikevich. Simple model of spiking neurons. IEEE Transactions on neural networks, 14\n\n(6):1569–1572, 2003.\n\nTravis A Jarrell, Yi Wang, Adam E Bloniarz, Christopher A Brittin, Meng Xu, J Nichol Thomson, Donna G Albertson, David H Hall, and Scott W Emmons. The connectome of a decision-making neural network. Science, 337(6093):437–444, 2012.\n\nAndreas Kaiser and Thomas Schreiber. Information transfer in continuous processes. Physica D:\n\nNonlinear Phenomena, 166(1-2):43–62, 2002.\n\nAlexander Kraskov, Harald St ̈ogbauer, and Peter Grassberger. Estimating mutual information. Phys-\n\nical review E, 69(6):066138, 2004.\n\nDharmendra S Modha and Raghavendra Singh. Network architecture of the long-distance pathways in the macaque brain. Proceedings of the National Academy of Sciences, 107(30):13485–13490, 2010.\n\nCatherine Morris and Harold Lecar. Voltage oscillations in the barnacle giant muscle fiber. Biophys-\n\nical journal, 35(1):193–213, 1981.\n\nSeung Wook Oh, Julie A Harris, Lydia Ng, Brent Winslow, Nicholas Cain, Stefan Mihalas, Quanxin Wang, Chris Lau, Leonard Kuan, Alex M Henry, et al. A mesoscale connectome of the mouse brain. Nature, 508(7495):207–214, 2014.\n\nMikhail I Rabinovich, Pablo Varona, Allen I Selverston, and Henry DI Abarbanel. Dynamical\n\nprinciples in neuroscience. Reviews of Modern Physics, 78(4):1213, 2006.\n\nJack W Scannell, Colin Blakemore, and Malcolm P Young. Analysis of connectivity in the cat\n\ncerebral cortex. Journal of Neuroscience, 15(2):1463–1483, 1995.\n\n22",
    "reference": "# Summary Of The Paper\n\nThis paper studies a cyclic causal model where values of every variable $X_i$ at time step t are decided by a differential equation $\\frac{d x_i}{d x}  = g(x_i) + \\sum_{j} B_{ij}f(x_i, x_j)$. The learner's goal is to recover the structural functions $f$ and $g$ and the coefficients $B_{ij}$ from the observational data. The authors propose a new loss function for this learning task, called attention-extended transfer entropy, which encourages certain conditional dependence in the model. Finally, simulations were performed on both synthetic and dynamical models to validate the proposed approach.\n\n# Strength And Weaknesses\n\nThis paper may have some interesting ideas. The definition of attention-extended transfer entropy (ATE) seems intriguing. However, due to the lack of clarity, it is unclear how this paper contributes to the existing literature and how novel the proposed method is. I will elaborate on the following.\n\nFirst, the authors claim that \"our task is to infer causal relationships between observed variables based on time series data and reconstruct the causal network connecting large numbers of these variables.\" However, this inference task is never formally defined. What is the performance measure of the learning task? How does one measure the quality of the reconstructed causal networks? Should we measure the L1 / L2 distance of the learned parameters with the actual parameters of the underlying model, or should we measure of the divergence between simulated and observed samples? Unfortunately, I tried to read through the paper but could not find answers.\n\nSecond, the paper describes the proposed algorithm. However, it does not perform any analysis of the algorithm's theoretical guarantee, concentration properties, and sample complexity. It is unclear how the proposed algorithm improves the existing baseline. To support the proposed method, the authors have to resort to empirical evaluation.\n\nAs for the experiments, I appreciate the authors' efforts in including various synthetic and dynamical models. Unfortunately, the clarity issue remains. For instance, the authors state \"compared with the baselines, our method usually substantially improves\nreconstruction performance on both synthetic and real causal networks, as shown in Figure 4.\" Again, it is unclear how the reconstruction performance is measured here. Without such information, it is hard to evaluate and compare the proposed method with the existing baseline.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nPlease see my response above.\n\n# Summary Of The Review\n\nThis paper studies a cyclic causal model where values of every variable $X_i$ at time step t are decided by a differential equation $\\frac{d x_i}{d x}  = g(x_i) + \\sum_{j} B_{ij}f(x_i, x_j)$. The learner's goal is to recover the structural functions $f$ and $g$ and the coefficients $B_{ij}$ from the observational data. The authors propose a new loss function for this learning task, called attention-extended transfer entropy, which encourages certain conditional dependence in the model. However, this paper provides little theoretical guarantee for the proposed method. Due to the lack of clarity, it is unclear how this paper contributes to the existing literature and how novel the proposed method is.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nVARIATIONAL IMBALANCED REGRESSION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nExisting regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point’s representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation’s variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets show that our VIR can outperform state-of-the-art imbalanced regression models in terms of both accuracy and uncertainty estimation.\n\n1\n\nINTRODUCTION\n\nDeep regression models are currently the state of the art in making predictions in a continuous label space and have a wide range of successful applications in computer vision (Yin et al., 2021), natural language processing (Jiang et al., 2020), etc. However, these models fail however when the label distribution in training data is imbalanced. For example, in visual age estimation (Moschoglou et al., 2017), where a model infers the age of a person given her visual appearance, models are typically trained on imbalanced datasets with overwhelmingly more images of younger adults, leading to poor regression accuracy for images of children or elderly people (Yang et al., 2021). Such unreliability in imbalanced regression settings motivates the need for both improving performance for the minority in the presence of imbalanced data and, more importantly, providing reasonable uncertainty estimation to inform practitioners on how reliable the predictions are (especially for the minority where accuracy is lower).\n\nExisting methods for deep imbalanced regression (DIR) only focus on improving the accuracy of deep regression models by smoothing the label distribution and reweighting data with different labels (Yang et al., 2021). On the other hand, methods that provide uncertainty estimation for deep regression models operates under the balance-data assumption and therefore do not work well in the imbalanced setting (Amini et al., 2020; Mi et al., 2022; Charpentier et al., 2022).\n\nTo simultaneously cover these two desiderata, we propose a probabilistic deep imbalanced regression model, dubbed variational imbalanced regression (VIR). Different from typical variational autoencoders assuming I.I.D. representations (a data point’s representation is not directly affected by other data points), our VIR assumes Neighboring and Identically Distributed (N.I.D.) and borrows data with similar regression labels to compute the latent representation’s variational distribution. Specifically, VIR first encodes a data point into a probabilistic representation and then mix it with neighboring representations (i.e., representations from data with similar regression labels) to produce its final probabilistic representation; VIR is therefore particularly useful for minority data as it can borrow probabilistic representations from data with similar labels (and naturally weigh them using our probabilistic model) to counteract data sparsity. Furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions by the importance weight computed from the smoothed label distribution to impose probabilistic reweighting on the imbalanced data. This allows the negative log likelihood to naturally put more focus on the minority data, thereby balancing the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\naccuracy for data with different regression labels. Our VIR framework is compatible with any deep regression models and can be trained end to end.\n\nWe summarize our contributions as below:\n\n1. While previous work has studied imbalanced regression and uncertainty estimation separately, none of them has considered uncertainty estimation in the imbalanced setting. We identify the problem of probabilistic deep imbalanced regression as well as two desiderata, balanced accuracy and uncertainty estimation, for the problem.\n\n2. We propose VIR to simultaneously cover these two desiderata and achieve state-of-the-art\n\nperformance compared to existing methods.\n\n3. As a byproduct, we also provide strong baselines for benchmarking high-quality uncertainty\n\nestimation and promising prediction performance on imbalanced datasets.\n\n2 RELATED WORK\n\nVariational Autoencoder. Variational autoencoder (VAE) (Kingma & Welling, 2014) is an unsupervised learning model that aims to infer probabilistic representations from data. However, as shown in Figure 1, VAE typically assumes I.I.D. representations, where a data point’s representation is not directly affected by other data points. In contrast, our VIR borrows data with similar regression labels to compute the latent representation’s variational distribution.\n\nImbalanced Regression. Imbalanced regression is underexplored in the machine learning community. Most existing methods for imbalanced regression are direct extensions of the SMOTE algorithm (Chawla et al., 2002), a commonly used algorithm for imbalanced classification, where data from the minority classes is over-sampled. These algorithms usually synthesize augmented data for the minority regression labels by either interpolating both inputs and labels (Torgo et al., 2013) or adding Gaussian noise (Branco et al., 2017; 2018).\n\nFigure 1: Comparison on inference networks between typical VAE (Kingma & Welling, 2014) and our VIR. In VAE (left), a data point’s latent representation (i.e. z) is affected only by itself, while in VIR (right), neighbors participate to modulate the final representation.\n\nSuch algorithms fail to the distance in continuous label space and fall short in handling highdimensional data (e.g., images and text). Recently, DIR (Yang et al., 2021) addresses these issues by applying kernel density estimation to smooth and reweight data on the continuous label distribution, achieving state-of-the-art performance. However, DIR only focuses on improving the accuracy, especially for the data with minority labels, and therefore does not provide uncertainty estimation, which is crucial to assess the predictions’ reliability. Ren et al. (2022) focuses on re-balancing the mean squared error (MSE) loss for imbalanced regression, and Gong et al. (2022) introduces ranking similarity for improving deep imbalanced regression. In contrast, our VIR provides a principled probabilistic approach to simultaneously achieve these two desiderata, not only improving upon DIR in terms of performance but also producing reasonable uncertainty estimation as a much-needed byproduct to assess model reliability. There is also related work on imbalanced classification (Deng et al., 2021), which is related to our work but focusing on classification rather than regression.\n\nUncertainty Estimation in Regression. There has been renewed interest in uncertainty estimation in the context of deep regression models (Kendall & Gal, 2017; Kuleshov et al., 2018; Song et al., 2019; Zelikman et al., 2020; Amini et al., 2020; Mi et al., 2022; van Amersfoort et al., 2021; Liu et al., 2020; Gal & Ghahramani, 2016; Stadler et al., 2021; Snoek et al., 2019; Heiss et al., 2022). Most existing methods either directly predict the variance of the output distribution as the estimated uncertainty (Kendall & Gal, 2017; Zhang et al., 2019; Amini et al., 2020) or rely on post-hoc confidence interval calibration (Kuleshov et al., 2018; Song et al., 2019; Zelikman et al., 2020). Meanwhile, Posterior Networks methods Charpentier et al. (2020; 2022); Stadler et al. (2021) consider conjugate distribution, pseudo-count interpretations, posterior updates, and variational losses for fast and high-quality uncertainty estimation. Closest to our work is Deep Evidential Regression (DER) (Amini et al., 2020), which attempts to estimate both aleatoric and epistemic uncertainty (Kendall & Gal, 2017; Hüllermeier & Waegeman, 2019) on regression tasks by training\n\n2\n\nNxzφNxNzφUnder review as a conference paper at ICLR 2023\n\nthe neural networks to directly infer the parameters of the evidential distribution, thereby producing uncertainty measures. While Posterior Networks Charpentier et al. (2020; 2022) are designed for general classification/regression tasks and achieve promising performance, they do not explicitly consider imbalance in regression tasks, which is the focus of this paper. DER (Amini et al., 2020) is designed for the data-rich regime and therefore fails to reasonably estimate the uncertainty if the data is imbalanced; for data with minority labels, DER (Amini et al., 2020) tends produce unstable distribution parameters, leading to poor uncertainty estimation (as shown in Sec. 4). In contrast, our proposed VIR explicitly handles data imbalance in the continuous label space to avoid such instability; VIR does so by modulating both the representations and the output conjugate distribution parameters according to the imbalanced label distribution, allowing training/inference to proceed as if the data is balance and leading to better performance as well as uncertainty estimation (as shown in Sec. 4).\n\n3 METHOD\n\nIn this section we introduce the problem setting, provide an overview of our VIR, and then describe details on each of VIR’s key components.\n\n3.1 PROBLEM SETTINGS\n\nAssuming an imbalanced dataset in continuous space {xi, yi}N i=1 where N is the total number of data points, xi ∈ Rd is the input, and yi ∈ Y ⊂ R is the corresponding label from a continuous label space Y. In practice, Y is partitioned into B equal-interval bins [y(0), y(1)), [y(2), y(2)), ..., [y(B−1), y(B)), with slight notation overload. To directly compare with baselines, we use the same grouping index for target value b ∈ B as in (Yang et al., 2021). We denote representations as zi, and use ((cid:101)zμ i , (cid:101)zΣ i ) = qφ(z|xi; θ) to denote the probabilistic representations for input xi generated by a probabilistic encoder parameterized by θ. Similarly we use ((cid:98)yi, (cid:98)si) to denote the mean and variance of the predictive distribution generated by a probabilistic predictor pθ(yi|z). Furthermore, we denote ̄z as the mean of representation zi in each bins (i.e., letting ̄z = 1 i=1 zi in a bin with Nb data points). Nb\n\n(cid:80)Nb\n\n3.2 METHOD OVERVIEW\n\nIn order to achieve both desiderata in probabilistic deep imbalanced regression (i.e., performance improvement and uncertainty estimation), our proposed variational imbalanced regression (VIR) operates on both the encoder qφ(zi|{xi}N\n\ni=1) and the predictor pθ(yi|zi).\n\nFigure 2: Overview of our VIR method. Left: The inference model infers the latent representations given input x’s in the neighborhood. Right: The generative model reconstructs the input and predicts the label distribution (including the associated uncertainty) given the latent representation.\n\nTypical VAE (Kingma & Welling, 2014) lower-bounds input xi’s marginal likelihood; in contrast, VIR lower-bounds the marginal likelihood of input xi and labels yi:\n\nlog pθ(xi, yi) = DKL\n\n(cid:0)qφ(zi|{xi}N\n\ni=1)||pθ(zi|xi, yi)(cid:1) + L(θ, φ; xi, yi).\n\nNote that our variational distribution qφ(zi|{xi}N task is to predict yi and (2) conditions on all (neighboring) inputs {xi}N second term L(θ, φ; xi, yi) is VIR’s evidence lower bound (ELBO), which is defined as:\n\ni=1) (1) does not conditions on labels yi, since the i=1 rather than just xi. The\n\nL(θ, φ; xi, yi) = Eq\n\n(cid:124)\n\n(cid:2) log pθ(xi|zi)(cid:3) (cid:123)(cid:122) (cid:125) LD i\n\n+ Eq (cid:124)\n\n(cid:2) log pθ(yi|zi)(cid:3) (cid:123)(cid:122) (cid:125) LP i\n\n(cid:124)\n\n− DKL(qφ(zi|{xi}N\n\ni=1)||pθ(zi)) (cid:125)\n\n.\n\n(1)\n\n(cid:123)(cid:122) LKL i\n\nwhere the pθ(zi) is the standard Gaussian prior N (0, I), following typical VAE (Kingma & Welling, 2014), and the expectation is taken over qφ(zi|{xi}N i=1), which infers zi by borrowing data with similar regression labels to produce the balanced probabilistic representations, which is beneficial especially for the minority (see Sec. 3.3 for details).\n\n3\n\nNxzyθNxzφNUnder review as a conference paper at ICLR 2023\n\nDifferent from typical regression models which produce only point estimates for yi, our VIR’s predictor, pθ(yi|zi), directly produces the parameters of the entire NIG distribution for yi and further imposes probabilistic reweighting on the imbalanced data, thereby producing balanced predictive distributions (more details in Sec. 3.4).\n\n3.3 CONSTRUCTING q(zi|{xi}N\n\ni=1)\n\nTo cover both desiderata, one needs to (1) produce balanced representations to improve performance for the data with minority labels and (2) produce probabilistic representations to naturally obtain reasonable uncertainty estimation for each model prediction. To learn such balanced probabilistic representations, we construct the encoder of our VIR (i.e., qφ(zi|{xi}N i=1)) by (1) first encoding a data point into a probabilistic representation, (2) computing probabilistic statistics from neighboring representations (i.e., representations from data with similar regression labels), and (3) producing the final representations via probabilistic whitening and recoloring using the obtained statistics.\n\nProbabilistic Representations. We first encode each data point into a probabilistic representation. Note that this is in contrast to existing work (Yang et al., 2021) that uses deterministic representations. We assume that each encoding zi is a Gaussian distribution with parameters {zμ i }, which are generated from the last layer in the deep neural network.\n\ni , zΣ\n\nFrom I.I.D. to Neighboring and Identically Distributed (N.I.D.). Typical VAE (Kingma & Welling, 2014) is an unsupervised learning model that aims to learn a variational representation from latent space to reconstruct the original inputs under the I.I.D. assumption; that is, in VAE, the latent value (i.e., zi) is generated from its own input xi. This I.I.D. assumption works well for data with majority labels, but significantly harms performance for data with minority labels. To address this problem, we replace the I.I.D. assumption with the N.I.D. assumption; specifically, VIR’s variational latent representations still follow Gaussian distributions (i.e., N (zμ i ), but these distributions will be first calibrated using data with neighboring labels. For a data point (xi, yi) where yi is in the b’th bin, i.e., i=1) ≜ N (zi; (cid:101)zμ yi ∈ [y(b−1), y(b)), we compute q(zi|{xi}N i = I(xi), b , Σμ b , ΣΣ\n\n(2) (3)\n\ni , (cid:101)zΣ\n\ni , zΣ\n\ni ) as\n\nMean and Covariance of Initial zi: zμ Statistics of Bin b’s Statistics: μμ Smoothed Statistics of Bin b’s Statistics: (cid:101)μμ Mean and Covariance of Final zi: (cid:101)zμ\n\ni , zΣ b , μΣ b , (cid:101)μΣ i , (cid:101)zΣ\n\nΣ\n\nb = A({zμ b = S({μμ b , μΣ i , zΣ\n\ni , zΣ b , μΣ b , Σμ\n\ni }N i=1), b , Σμ b , ΣΣ\n\ni , μμ\n\nμ b , (cid:101)Σ b , (cid:101)Σ i = F(zμ\n\nb , ΣΣ b , (cid:101)μμ\n\nb }B b=1), b , (cid:101)μΣ\n\nb , (cid:101)Σ\n\n(4)\n\nΣ b ),\n\nμ b , (cid:101)Σ\n\nwhere the details of functions I(·), A(·), S(·), and F(·) are described below.\n\nFunction I(·): From Deterministic to Probabilistic Statistics. Different from deterministic statistics in (Yang et al., 2021), our VIR’s encoder uses probabilistic statistics (i.e., statistics of statistics). Specifically, VIR treats zi as a distribution with the mean and covariance (zμ i ) = I(xi) rather than a deterministic vector. As a result, all the deterministic statistics, μb, Σb, (cid:101)μb, and (cid:101)Σb are replaced by distributions with the means and covariances, (μμ b ), and\n\nb ), (Σμ\n\ni , zΣ\n\nb , ΣΣ\n\nb , μΣ\n\nb ), ((cid:101)μμ\n\nb , (cid:101)μΣ\n\n( (cid:101)Σ\n\nb , (cid:101)Σ\n\nb ), respectively (more details in the following three paragraphs on A(·), S(·), and F(·)).\n\nμ\n\nΣ\n\nFunction A(·): Statistics of the current Bin b’s Statistics. As part of our probabilistic overall statistics, the probabilistic overall mean becomes a distribution with the mean (letting μb = ̄z) and covariance (assuming diagonal covariance):\n\nμμ\n\nb = E[ ̄z] = 1\n\nNb\n\n(cid:88)Nb i=1\n\nzμ i ,\n\nμΣ\n\nb = V[ ̄z] = 1\n\nN 2 b\n\n(cid:88)Nb i=1\n\nzΣ i .\n\nSimilarly, our probabilistic overall covariance becomes a matrix-variate distribution (Gupta & Nagar, 2018) with the mean:\n\nΣμ\n\nb = 1\n\nNb\n\n(cid:88)Nb i=1\n\n(zi − ̄z)2 = 1 Nb\n\n(cid:88)Nb i=1\n\n(cid:104)\n\ni + (zμ zΣ\n\ni )2 −\n\n(cid:16)\n\n[μΣ\n\nb ]i + ([μμ\n\nb ]i)2(cid:17)(cid:105)\n\n,\n\nb and V[ ̄z] = μΣ\n\nsince E[ ̄z] = μμ b , involves computing the fourth-order moments, which is computationally prohibitive. Therefore in practice, we directly set ΣΣ b to zero for simplicity; empirically we observe that such simplified treatment already achieves promising performance improvement upon the state of the art.\n\nb . Note that the covariance of Σb, i.e., ΣΣ\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFunction S(·): Neighboring Data and Smoothed Statistics. Next, we can borrow data with neighboring labels (from neighboring label bins) to compute the smoothed statistics of the current bin b by applying a symmetric kernel k(·, ·) (e.g., Gaussian, Laplacian, and Triangular kernels). Specifically, the probabilistic smoothed mean and covariance are (assuming diagonal covariance):\n\n(cid:101)μμ\n\nb =\n\n(cid:88)\n\nb′ ∈B\n\nk(yb, yb′)μμ\n\nb′, (cid:101)μΣ\n\nb =\n\n(cid:88)\n\nb′ ∈B\n\nk2(yb, yb′)μΣ\n\nb′, (cid:101)Σ\n\nb =\n\nμ\n\n(cid:88)\n\nb′ ∈B\n\nk(yb, yb′)Σb′.\n\nFunction F(·): Probabilistic Whitening and Recoloring. We develop a probabilistic version of the whitening and re-coloring procedure (Sun et al., 2016) used in (Yang et al., 2021). Specifically, we produce the final probabilistic representation {(cid:101)zμ i , (cid:101)zΣ\n\ni } for each data point as:\n\n(cid:114)\n\nμ\n\nb\n\n(cid:102)Σ Σμ\n\nb\n\nb ) ·\n\ni − μμ\n\ni = (zμ (cid:101)zμ\n\ni = (zΣ (cid:101)zΣ\n\n+ (cid:101)μμ b , Inspired by (Yang et al., 2021), we keep updating the probabilistic overall statistics, {μμ b , Σb}, and the probabilistic smoothed statistics, {(cid:101)μμ b }, cross different epochs. The probabilistic representation {(cid:101)zμ i } are then re-parameterized (Kingma & Welling, 2014) into the final representation zi, and passed into the final layer (discussed in Sec. 3.4) to generate the prediction and uncertainty estimation. Note that the computation of statistics from multiple x’s is only needed during training. During testing, VIR directly uses these statistics and therefore does not need to re-compute them.\n\n+ (cid:101)μΣ b .\n\ni + μΣ\n\nb , (cid:101)μΣ\n\ni , (cid:101)zΣ\n\nb , μΣ\n\nb ) ·\n\n(5)\n\nb\n\nb\n\n(cid:114)\n\nμ\n\n(cid:102)Σ Σμ\n\n3.4 CONSTRUCTING p(yi|zi)\n\nOur VIR’s predictor p(yi|zi) ≜ N (yi; (cid:98)yi, (cid:98)si) predicts both the mean and variance for yi by first predicting the NIG distribution and then marginalizing out the latent variables. It is motivated by the following observations on label distribution smoothing (LDS) in (Yang et al., 2021) and deep evidental regression (DER) in (Amini et al., 2020), as well as intuitions on effective counts in conjugate distributions.\n\nLDS’s Limitations in Our Probabilistic Imbalanced Regression Setting. The motivation of LDS (Yang et al., 2021) is that the empirical label distribution can not reflect the real label distribution in an imbalanced dataset with a continuous label space; consequently, reweighting methods for imbalanced regression fail due to these inaccurate label densities. By applying a smoothing kernel on the empirical label distribution, LDS tries to recover the effective label distribution, with which reweighting methods can obtain ‘better’ weights to improve imbalanced regression. However, in our probabilistic imbalanced regression, one needs to consider both (1) the performance for the data with minority labels and (2) uncertainty estimation for each model. However, LDS only focuses on improving the accuracy, especially for the data with minority labels, and therefore does not provide uncertainty estimation, which is crucial to assess the predictions’ reliability.\n\nDER’s limitations in Our Probabilistic Imbalanced Regression Setting. In DER (Amini et al., 2020), the predicted labels with their correspond uncertainties are produced by the representation of the posterior parameters in Normal Inverse Gamma (NIG) distribution N IG(γ, ν, α, β), while the model is trained via minimizing the negative log-likelihood (NLL) of a Student-t distribution:\n\nLDER\n\ni\n\n= 1\n\n2 log( π\n\nν ) + (α + 1\n\n2 ) log((yi − γ)2ν + Ω) − α log(Ω) + log( Γ(α)\n\nΓ(α+\n\n1 2 )\n\n),\n\n(6)\n\nwhere Ω = 2β(1 + ν). It is therefore nontrivial to properly incorporate a reweighting mechanism into the NLL. One straightforward approach is to directly reweight LDER for different data points (xi, yi). However, this contradicts the formulation of NIG and often leads to poor performance, as we verify in Sec. 4.\n\ni\n\nIntuition of Pseudo-Counts for VIR. To properly incorporate different reweighting methods, our VIR relies on the intuition of pseudo-counts (pseudo-observations) in conjugate distributions (Bishop, 2006). Assuming Gaussian likelihood, the conjugate distributions would be an NIG distribution (Bishop, 2006), i.e., (μ, Σ) ∼ N IG(γ, ν, α, β), which means: μ ∼ N (γ, Σ/ν), Σ ∼ Γ−1(α, β),\n\nwhere Γ−1(α, β) N IG(γ0, ν0, α0, β0), the posterior distribution of the NIG after observing n real data points are:\n\nis an inverse gamma distribution. With a NIG prior distribution\n\nγn = γ0ν0+nΨ\n\nνn\n\n,\n\nνn = ν0 + n, αn = α0 + n 2 ,\n\n5\n\nβn = β0 + 1\n\n2 (γ2\n\n0 ν0) + Φ,\n\n(7)\n\nUnder review as a conference paper at ICLR 2023\n\n2 ((cid:80)\n\ni x2\n\ni − γ2\n\nwhere Ψ = ̄x and Φ = 1 nνn). Here ν0 and α0 can be interpreted as virtual observations, i.e., pseudo-counts or pseudo-observations that contribute to the posterior distribution. Overall, the mean of posterior distribution above can be interpreted as an estimation from (2α0 + n) observations, with 2α0 virtual observations and n real observations. Similarly, the variance can be interpreted an estimation from (ν + n) observations. This intuition is crucial in developing the predictor of our VIR.\n\nFrom Pseudo-Counts to Balanced Predictive Distributions. Based on the intuition above, we construct our predictor (i.e., p(yi|zi)) by (1) generating the parameters in the posterior distribution of NIG, (2) computing re-weighted parameters by imposing the importance weights obtained from LDS, and (3) producing the final prediction with corresponding uncertainty estimation.\n\nBased on Eqn. 7, we feed the final representation {zi}N linear layer to output the intermediate parameters ni, Ψi, Φi for data point (xi, yi):\n\ni=1 generated from the Sec. 3.3 (Eqn. 5) into a\n\nni, Ψi, Φi = G(zi),\n\nzi ∼ q(zi|{xi}N\n\ni=1) = N (zi; (cid:101)zμ\n\ni , (cid:101)zΣ i )\n\n1 We then apply the importance weights (cid:80) 2 calculated from the smoothed label distribution to the pseudo-count ni to produce the re-weighted parameters of posterior distribution of NIG. Along with the pre-defined prior parameters (γ0, ν0, α0, β0), we are able to compute the parameters of posterior distribution N IG(γi, νi, αi, βi) for (xi, yi):\n\nb′ ∈B k(yb, yb′)(cid:1)−\n\nγ∗\n\ni =\n\nγ0ν0+(cid:0) (cid:80)\n\nb\n\n′\n\n∈B\n\nk(yb,yb′ )(cid:1)−\n\nν∗ n\n\n1 2 ·niΨi\n\n,\n\ni = α0 + (cid:0) (cid:88) α∗\n\nk(yb, yb′)(cid:1)−\n\n1\n\n2 · ni 2 ,\n\nb′ ∈B\n\ni = ν0 + (cid:0) (cid:88) ν∗\n\nk(yb, yb′)(cid:1)−\n\n1 2 · ni,\n\nb′ ∈B\n\nβ∗\n\ni = β0 + 1\n\n2 (γ2\n\n0 ν0) + Φi.\n\nBased on the NIG posterior distribution, we can then compute final prediction and uncertainty estimation as\n\n(cid:98)yi = γ∗\n\ni , (cid:98)si =\n\nβ∗ i (α∗\n\ni\n\ni −1) .\n\nν∗\n\nWe use an objective function similar to Eqn. 6, but with different definitions of (γ, ν, α, β), to optimize our VIR model: (cid:2) 1 2 log( π\n\ni ) + log( Γ(α∗\n\n2 ) log((yi − γ∗\n\nn + Ω) − α∗\n\ni log(ω∗\n\ni = E\n\n) + (α∗\n\ni )2ν∗\n\ni + 1\n\n(8)\n\n)(cid:3),\n\nLP\n\nqφ(zi|{xi}N\n\ni=1)\n\nν∗ i\n\ni ) 1\ni + 2\n\n)\n\nΓ(α∗\n\ni = 2β∗\n\ni (1 + ν∗\n\nwhere ω∗ 2020), we use an additional regularization term to achieve better accuracy1: LR\n\ni ). Note that LP\n\ni = (ν + 2α) · |yi − (cid:98)yi|. together constitute the objective function for learning the predictor p(yi|zi).\n\nLP\n\ni and LR\n\ni\n\ni\n\nis part of the ELBO in Eqn. 1. Similar to (Amini et al.,\n\n3.5 FINAL OBJECTIVE FUNCTION\n\nPutting together Sec. 3.3 and Sec. 3.4, our final objective function (to minimize) for VIR is:\n\nLVIR =\n\n(cid:88)N\n\ni=1\n\nLVIR\n\ni\n\n, LVIR\n\ni\n\n= λLR\n\ni − L(θ, φ; xi, yi) = λLR\n\ni − LP\n\ni − LD\n\ni + LKL\n\ni\n\n,\n\nwhere L(θ, φ; xi, yi) = LP is the ELBO in Eqn. 1. λ adjusts the importance of the additional regularizer and the ELBO, and thus lead to a better result both on accuracy and uncertainty estimation.\n\ni − LKL\n\ni + LD\n\ni\n\n3.6 DISCUSSION ON I.I.D. AND N.I.D. ASSUMPTIONS\n\nGeneralization Error, Bias, and Variance. We could analyze the generalization error of our VIR by bounding the generalization with the sum of three terms: (a) the bias of our estimator, (2) the variance of our estimator, (3) model complexity. Essentially VIR uses the N.I.D. assumption increases our estimator’s bias, but significantly reduces its variance in the imbalanced setting. Since the model complexity is kept the same (using the same backbone neural network) as the baselines, N.I.D. will lead to a lower generalization error (see more discussion in Sec. A of the Appendix).\n\n1Note that in DER, the total evidence Φ has a value 2ν + α, but to the best of our knowledge, it would be\n\nmore reasonable to use ν + 2α as the total evidence for an NIG distribution (Bishop, 2006).\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4 RESULTS\n\nDatasets. In this work, we evaluate our methods in terms of prediction accuracy and uncertainty estimation on two imbalanced datasets2, AgeDB (Moschoglou et al., 2017), IMDB-WIKI (Rothe et al., 2018). We follow the preprocessing procedures in DIR (Yang et al., 2021). Details for label density distributions and levels of imbalance are discussed in DIR (Yang et al., 2021).\n\nAgeDB-DIR: We use AgeDB-DIR constructed in DIR (Yang et al., 2021), which contains 12.2K images for training and 2.1K images for validation and testing. The maximum age in this dataset is 101 and the minimum age is 0, and the number of images per bin varies between 1 and 353.\n\nIMDB-WIKI-DIR: We use IMDB-WIKI-DIR constructed in DIR (Yang et al., 2021), which contains 191.5K training images and 11.0K validation and testing images. The maximum age is 186 and minimum age is 0; the maximum bin density is 7149, and minimum bin density is 1.\n\nSTS-B-DIR: We use STS-B-DIR constructed in DIR (Yang et al., 2021), which contains 5.2K pairs of training sentences and 1.0K pairs for validation and testing. This dataset is a collection of sentence pairs generated from news headlines, video captions, etc. Each pair is annotated by multiple annotators with a similarity score between 0 and 5.\n\nBaselines. We use ResNet-50 (He et al., 2016) as our backbone network, and we describe the baselines below.\n\nVanilla: We use the term VANILLA to denote a plain model without adding any approaches.\n\nSynthetic-Sample-Based Methods: Various existing imbalanced regression methods are also included as baselines; these include SMOTER (Torgo et al., 2013) and SMOGN (Branco et al., 2017). Furthermore, following DIR (Yang et al., 2021), in IMDB-WIKI-DIR, we also include another two methods: MIXUP (Zhang et al., 2018) and M-MIXUP (Verma et al., 2019).\n\nCost-Sensitive Reweighting: As shown in DIR (Yang et al., 2021), the square-root weighting variant 1\n(SQINV) baseline (i.e. (cid:0) (cid:80) 2 ) always outperforms Vanilla. Therefore, for simplicity and fair comparison, all our experiments (for both baselines and VIR) use SQINV weighting. To use SQINV in VIR, one simply needs to use the symmetric kernel k(·, ·) described in Sec. 3.3. To use SQINV in DER, we replace the final layer in DIR (Yang et al., 2021) with the DER layer (Amini et al., 2020) to produce the predictive distributions.\n\nb′ ∈B k(yb, yb′)(cid:1)−\n\nEvaluation Metrics - Accuracy. We follow the evaluation metrics in (Yang et al., 2021) to evaluate the accuracy of our proposed methods; these include Mean Absolute Error (MAE), Mean Squared Error (MSE), and Geometric Mean (GM). The formulas for these metrics are as follows:\n\nMAE = 1\n\nN\n\n(cid:88)N\n\ni=1\n\n|yi − (cid:98)yi|, MSE = 1\n\nN\n\n(cid:88)N\n\ni=1\n\n(yi − (cid:98)yi)2, GM =\n\n(cid:104) (cid:89)N\n\ni=1\n\n(cid:105) |yi − (cid:98)yi|\n\n1\n\nN .\n\nEvaluation Metrics - Uncertainty Estimation. We use typical evaluation metrics for uncertainty estimation in regression problems to evaluate our produced uncertainty estimation; these include Negative Log Likelihood (NLL), Area Under Sparsification Error (AUSE). Eqn. 8 shows the formula for NLL, and more details regarding to AUSE can be found in (Ilg et al., 2018).\n\nEvaluation Process. Following (Liu et al., 2019; Yang et al., 2021), for a data sample xi with its label yi which falls into the target bins bi, we divide the label space into three disjoint subsets: many-shot region {bi ∈ B | yi ∈ bi & |yi| > 100}, medium-shot region {bi ∈ B | yi ∈ bi & 20 ≤ |yi| ≤ 100}, and few-shot region {bi ∈ B | yi ∈ bi & |yi| < 20}, where | · | denotes the cardinality of the set. We report results on the overall test set and these subsets with the accuracy metrics discussed above.\n\nImplementation Details. We use ResNet-50 (He et al., 2016) for all experiments in AgeDB-DIR and IMDB-WIKI-DIR. We use the Adam optimizer (Kingma & Ba, 2015) to train all models for 100 epochs, with same learning rate and decay by 0.1 and the 60-th and 90-th epoch, respectively. In order to determine the optimal batch size for training, we try different batch sizes and achieve the same\n\n2Among the five datasets proposed in (Yang et al., 2021), only four of them are publicly available. In this\n\npaper we use the largest (IMDB-WIKI) and the smallest (AgeDB) among the four to evaluate our method.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nMetrics\n\nShot\n\nTable 1: Evaluation results of accuracy on AgeDB-DIR.\n\nMSE ↓\n\nMAE ↓\n\nGM ↓\n\nAll\n\nMany\n\nMed.\n\nFew\n\nMany\n\nMed.\n\nFew\n\nMany\n\nMed.\n\nFew\n\nVANILLA (Yang et al., 2021) DEEP ENSEMBLE (Lakshminarayanan et al., 2017) SMOTER (Torgo et al., 2013) SMOGN (Branco et al., 2017) SQINV (Yang et al., 2021) DER (Amini et al., 2020) FDS (Yang et al., 2021) LDS (Yang et al., 2021) LDS + FDS (Yang et al., 2021) FDS + RANKSIM (Gong et al., 2022) LDS + FDS + RANKSIM (Gong et al., 2022) LDS + FDS + DER (Yang et al., 2021; Amini et al., 2020) VIR (OURS)\n\nOURS VS. VANILLA OURS VS. SQINV OURS VS. DER OURS VS. LDS + FDS (SOTA IN DIR)\n\n101.28 100.94 114.34 117.29 104.76 106.81 109.78 102.22 102.16 83.51 84.96 112.62 86.89\n\n+14.39 +17.87 +19.92 +15.27\n\n78.40 79.30 93.35 101.36 92.67 91.32 93.99 83.62 86.99 71.99 74.27 94.21 77.69\n\n+0.71 +14.98 +13.63 +9.30\n\n131.17 129.95 129.89 133.86 127.04 122.45 124.96 128.73 128.04 99.14 93.64 140.03 96.55\n\n+34.62 +30.49 +25.90 +31.49\n\nAll\n\n7.79 7.73 8.16 8.26 7.92 8.11 8.12 7.67 7.82 7.02 7.03 8.18 7.14\n\n256.32 249.18 244.57 232.90 205.16 209.76 216.97 204.64 199.18 149.05 161.92 210.72 145.76\n\n6.70 6.62 7.39 7.64 7.42 7.36 7.52 6.98 7.19 6.49 6.54 7.44 6.67\n\n9.42 9.37 8.65 9.01 8.80 9.03 8.68 8.86 9.08 7.84 7.68 9.52 7.70\n\nAll\n\n5.18 4.87 5.21 5.36 5.03 5.31 5.13 4.85 5.01 4.53 4.45 5.30 4.58\n\n4.53 4.37 4.65 4.90 4.81 4.65 4.80 4.39 4.56 4.13 4.07 4.75 4.27\n\n6.75 6.50 5.69 6.19 5.72 6.48 5.97 5.80 6.10 5.37 5.23 6.74 5.09\n\n13.98 13.90 12.28 12.09 11.46 12.69 12.25 10.89 11.24 9.68 9.92 11.45 9.52\n\n+4.46 +1.94 +3.17 +1.72\n\n11.54 11.35 8.49 8.44 8.23 10.52 8.85 7.45 7.02 6.89 6.35 7.68 6.31\n\n+5.23 +1.92 +4.21 +0.71\n\n+110.56 +59.40 +64.00 +53.42\n\n+0.65 +0.78 +0.97 +0.68\n\n+0.03 +0.75 +0.69 +0.52\n\n+1.72 +1.10 +1.33 +1.38\n\n+0.60 +0.45 +0.73 +0.43\n\n+0.26 +0.54 +0.38 +0.29\n\n+1.66 +0.63 +1.39 +1.01\n\nconclusion as the DIR paper, i.e., the optimal batch size is 256 when other hyperparameters are fixed. Therefore, we stick to the batch size of 256 through out the experiments in the paper. Meanwhile, we use the same hyperparameters as in DIR (Yang et al., 2021).\n\nWe use PyTorch to implement our method. For fair comparison, we implemented a PyTorch version for the official TensorFlow implementation of DER(Amini et al., 2020). To make sure we can obtain the reasonable uncertainty estimations, we restrict the range for α to [1.5, ∞) instead of [1.0, ∞) in DER. Besides, in the activation function SoftPlus, we set the hyperparameter beta to 0.1. As discussed in Sec. 3.4, we implement a layer which produces the parameters n, Ψ, Ω. We assign 2 as the minimum number for n, and use the same hyperparameter settings for activation function for DER layer.\n\nTo search for a combination hyperparameters of prior distribution {γ0, ν0, α0, β0} for NIG, we combine grid search method and random search method (Bergstra & Bengio, 2012) to select the best hyperparameters. We first intuitively assign a value and a proper range with some step sizes which correspond to the hyperparameters, then, we apply grid search to search for the best combination for the hyperparameters on prior distributions. After locating a smaller range for each hyperparameters, we use random search to search for better combinations, if it exists. In the end, we find our best hyperparameter combinations for NIG prior distributions.\n\n4.1 RESULTS FOR IMBALANCED REGRESSION ACCURACY\n\nWe report the accuracy of different methods in Table 1 and Table 2 for AgeDB-DIR and IMDB-WIKIDIR, respectively3. In both tables, we can conclude that our methods outperform the baselines in their categories. For ablation studies, see Table 5 and Table 6 of the Appendix. Note that to ensure fair and solid comparison, we re-run the DIR methods based on our machine and software settings4.\n\nOverall Performance. As shown in the last category (i.e., last four rows) of both tables, our proposed method’s best variants compare favorably against the state of the art including DIR variants (Yang et al., 2021) and DER (Amini et al., 2020), especially on the imbalanced data samples (i.e., in the few-shot columns). This verifies the effectiveness of our methods in terms of overall performance.\n\n4.2 RESULTS FOR IMBALANCED REGRESSION UNCERTAINTY ESTIMATION\n\nDifferent from DIR (Yang et al., 2021) which only focuses on accuracy, we create a new benchmark for uncertainty estimation in imbalanced regression. Table 3 and Table 4 show the results on uncertainty estimation for two datasets AgeDB-DIR and IMDB-WIKI-DIR, respectively. Note that most baselines from Table 1 and Table 2 are deterministic methods (as opposed to probabilistic\n\n3Results for STS-B-DIR are reported in Table 7, Table 8, and Table 9 of the Appendix. 4We find that due to differences in PyTorch, GPU, and CUDA versions, as well as numbers of GPUs used for parallel training, the results in DIR may vary. Furthermore, the randomness in multiple workers in the Dataloader also affect the performance.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Evaluation results of accuracy on IMDB-WIKI-DIR.\n\nMetrics\n\nShot\n\nVANILLA (Yang et al., 2021) MIXUP (Zhang et al., 2018) M-MIXUP (Verma et al., 2019) SMOTER (Torgo et al., 2013) SMOGN (Branco et al., 2017) SQINV (Yang et al., 2021) DER (Amini et al., 2020) FDS (Yang et al., 2021) LDS (Yang et al., 2021) LDS + FDS (Yang et al., 2021) LDS + FDS + DER (Yang et al., 2021; Amini et al., 2020) VIR (OURS)\n\nOURS VS. VANILLA OURS VS. SQINV OURS VS. DER OURS VS. LDS + FDS (SOTA IN DIR)\n\nMSE ↓\n\nMAE ↓\n\nGM ↓\n\nAll\n\nMany\n\nMed.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\n135.48 141.11 137.45 138.75 136.09 134.36 133.81 131.93 133.93 136.72 120.86 119.60\n\n+15.88 +14.76 +14.21 +17.12\n\n107.01 109.13 108.33 111.55 109.15 111.23 107.51 107.76 109.70 112.76 97.75 99.25\n\n+7.76 +11.98 +8.26 +13.51\n\n352.02 389.95 363.72 346.09 339.09 308.63 332.90 311.29 320.26 322.50 297.64 298.85\n\n+53.17 +9.78 +34.05 +23.65\n\n973.73 1037.98 957.53 935.89 944.20 834.08 916.18 880.32 830.81 811.83 873.10 809.34\n\n+164.39 +24.74 +106.84 +2.49\n\n7.99 8.22 8.22 8.14 8.03 7.87 7.85 7.80 7.91 8.08 7.24 7.23\n\n7.18 7.29 7.39 7.42 7.30 7.24 7.18 7.20 7.30 7.47 6.64 6.66\n\n+0.76 +0.64 +0.62 +0.85\n\n+0.52 +0.58 +0.52 +0.81\n\n14.88 16.23 15.24 14.15 14.02 12.44 13.35 12.64 13.02 13.21 11.87 11.90\n\n+2.98 +0.54 +1.45 +1.31\n\n26.72 28.11 26.70 25.28 25.93 22.76 24.12 23.20 22.41 22.54 23.44 21.78\n\n+4.94 +0.98 +2.34 +0.76\n\n4.51 4.68 4.80 4.64 4.63 4.47 4.47 4.39 4.48 4.66 3.93 3.90\n\n4.12 4.22 4.39 4.30 4.30 4.22 4.18 4.16 4.22 4.39 3.69 3.68\n\n+0.61 +0.57 +0.57 +0.76\n\n+0.44 +0.54 +0.50 +0.71\n\n10.46 12.28 10.85 9.05 8.74 7.25 8.18 7.04 7.72 8.01 6.64 6.51\n\n+3.95 +0.74 +1.67 +1.50\n\n21.40 23.55 21.86 19.46 20.12 15.10 15.18 13.42 13.75 14.33 16.00 13.34\n\n+8.06 +1.76 +1.84 +0.99\n\nmethods like ours) and cannot provide uncertainty estimation; therefore they are not applicable here. To show the superiority of our VIR model, we create a strongest baseline by concatenating the DIR variants (LDS + FDS) with the DER (Amini et al., 2020).\n\nMetrics\n\nTable 3: Uncertainty estimation results on AgeDB-DIR.\n\nResults show that VIR outperform the baselines in all few-shot metrics. In some categories, VIR may not perform better in the overall, many-shot and median shot metrics, but the gap tends to be minimal. Note that our proposed methods mainly focus on the imbalanced setting, therefore we also focus on the few-shot metrics. Lastly, comparing our model variant with the best performance against the baseline (DER), we can conclude that our methods successfully improve uncertainty estimation in the probabilistic imbalanced regression setting.\n\n5.311 DEEP ENSEMBLE (Lakshminarayanan et al., 2017) DER (Amini et al., 2020) 3.936 LDS + FDS + DER (Yang et al., 2021; Amini et al., 2020) 3.794 3.703 VIR (OURS)\n\n+0.064 +0.071 +0.060 +0.225 +0.153 +0.026 +0.007 +0.036\n\n0.626 0.449 0.260 0.474\n\n0.466 0.468 0.392 0.319\n\n0.483 0.500 0.617 0.413\n\n8.523 4.421 4.214 4.196\n\n0.541 0.590 0.463 0.437\n\n4.031 3.768 3.699 3.598\n\n6.726 3.865 3.969 3.805\n\nAll Many Med.\n\nAll Many Med.\n\nOURS VS. DER\n\nAUSE ↓\n\nNLL ↓\n\nShot\n\nFew\n\nFew\n\nTable 4: Uncertainty estimation results on IMDB-WIKI-DIR.\n\nWe also observe that the improvements of the uncertainty estimation on IMDB-WIKI are larger than those on Age-DB. We suspect that this because IMDB-WIKI contains much more training, validating and testing data, therefore enjoying more stable uncertainty estimation improvements brought by VIR compared to those in Age-DB.\n\nDER (Amini et al., 2020) 3.850 LDS + FDS + DER (Yang et al., 2021; Amini et al., 2020) 3.683 3.652 VIR (OURS)\n\nAll Many Med.\n\n4.997 4.391 4.419\n\n6.638 5.697 5.560\n\n3.699 3.602 3.568\n\nOURS VS. DER\n\nMetrics\n\nNLL ↓\n\nShot\n\nFew\n\nAUSE ↓\n\nAll Many Med.\n\nFew\n\n0.813 0.784 0.622\n\n0.802 0.670 0.645\n\n0.650 0.455 0.511\n\n0.541 0.483 0.374\n\n+0.198 +0.131 +0.578 +1.078 +0.191 +0.157 +0.202 +0.167\n\n5 CONCLUSION\n\nWe identify the problem of probabilistic deep imbalanced regression, which aims to both improve accuracy and obtain reasonable uncertainty estimation in imbalanced regression. We propose VIR, which can use any deep regression models as backbone networks. VIR borrows data with similar regression labels to produce the probabilistic representations and modulates the conjugate distributions to impose probabilistic reweighting on imbalanced data. Furthermore, we create new benchmarks for uncertainty estimation on imbalanced regression. Experiments show that our methods outperform state-of-the-art imbalanced regression models in terms of both accuracy and uncertainty estimation. Future work may include (1) improving VIR by better approximating variance of the variances in probability distributions, and (2) developing novel approaches that can achieve stable performance even on imbalanced data with limited sample size, and (3) exploring techniques such as mixture density networks (Bishop, 1994) to enable multi-modality in the latent distribution, thereby further improving the performance.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nJames Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. J. Mach.\n\nLearn. Res., 13:281–305, 2012.\n\nChristopher M. Bishop. Mixture density networks. Technical report, 1994.\n\nChristopher M Bishop. Pattern recognition and machine learning. springer, 2006.\n\nPaula Branco, Luís Torgo, and Rita P. Ribeiro. SMOGN: a pre-processing approach for imbalanced regression. In First International Workshop on Learning with Imbalanced Domains: Theory and Applications, LIDTA@PKDD/ECML 2017, 22 September 2017, Skopje, Macedonia, volume 74 of Proceedings of Machine Learning Research, pp. 36–50. PMLR, 2017.\n\nPaula Branco, Luís Torgo, and Rita P. Ribeiro. REBAGG: resampled bagging for imbalanced regression. In Second International Workshop on Learning with Imbalanced Domains: Theory and Applications, LIDTA@ECML/PKDD 2018, Dublin, Ireland, September 10, 2018, volume 94 of Proceedings of Machine Learning Research, pp. 67–81. PMLR, 2018.\n\nBertrand Charpentier, Daniel Zügner, and Stephan Günnemann. Posterior Network: Uncertainty Estimation without OOD Samples via Density-Based Pseudo-Counts. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nBertrand Charpentier, Oliver Borchert, Daniel Zügner, Simon Geisler, and Stephan Günnemann. Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\n\nNitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321–357, 2002.\n\nZongyong Deng, Hao Liu, Yaoxing Wang, Chenyang Wang, Zekuan Yu, and Xuehong Sun. PML: progressive margin loss for long-tailed age classification. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, 2021.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 1050–1059. JMLR.org, 2016.\n\nYu Gong, Greg Mori, and Frederick Tung. RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 7634–7649. PMLR, 2022.\n\nArjun K Gupta and Daya K Nagar. Matrix variate distributions, volume 104. CRC Press, 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770–778. IEEE Computer Society, 2016.\n\nJakob Heiss, Jakob Weissteiner, Hanna S. Wutte, Sven Seuken, and Josef Teichmann. NOMU: Neural Optimization-based Model Uncertainty. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 8708–8758. PMLR, 2022.\n\nEyke Hüllermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning:\n\nA tutorial introduction. CoRR, abs/1910.09457, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nEddy Ilg, Özgün cCiccek, Silvio Galesso, Aaron Klein, Osama Makansi, Frank Hutter, and Thomas Brox. Uncertainty estimates and multi-hypotheses networks for optical flow. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VII, volume 11211 of Lecture Notes in Computer Science, pp. 677–693. Springer, 2018.\n\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART: robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 2177–2190. Association for Computational Linguistics, 2020.\n\nAlex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer\n\nvision? arXiv preprint arXiv:1703.04977, 2017.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.\n\nVolodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning using calibrated regression. In International Conference on Machine Learning, pp. 2796–2804. PMLR, 2018.\n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 6402–6413, 2017.\n\nJeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness. In Advances in Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2020.\n\nZiwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Large-scale long-tailed recognition in an open world. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 2537–2546. Computer Vision Foundation / IEEE, 2019.\n\nLu Mi, Hao Wang, Yonglong Tian, and Nir Shavit. Training-free uncertainty estimation for neural\n\nnetworks. In AAAI, 2022.\n\nStylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: The first manually collected, in-the-wild age database. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 1997–2005, 2017.\n\nJiawei Ren, Mingyuan Zhang, Cunjun Yu, and Ziwei Liu. Balanced MSE for Imbalanced Visual Regression. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 7916–7925. IEEE, 2022.\n\nRasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from a\n\nsingle image without facial landmarks. Int. J. Comput. Vis., 126(2-4):144–157, 2018.\n\nJasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji Lakshminarayanan, Sebastian Nowozin, D. Sculley, Joshua V. Dillon, Jie Ren, and Zachary Nado. Can you trust your model’s uncertainty? Evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 13969–13980, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nHao Song, Tom Diethe, Meelis Kull, and Peter Flach. Distribution calibration for regression. In\n\nInternational Conference on Machine Learning, pp. 5897–5906. PMLR, 2019.\n\nMaximilian Stadler, Bertrand Charpentier, Simon Geisler, Daniel Zügner, and Stephan Günnemann. Graph Posterior Network: Bayesian Predictive Uncertainty for Node Classification. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 18033–18048, 2021.\n\nBaochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In Dale Schuurmans and Michael P. Wellman (eds.), Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pp. 2058–2065. AAAI Press, 2016.\n\nLuís Torgo, Rita P. Ribeiro, Bernhard Pfahringer, and Paula Branco. SMOTE for regression. In Luís Correia, Luís Paulo Reis, and José Cascalho (eds.), Progress in Artificial Intelligence - 16th Portuguese Conference on Artificial Intelligence, EPIA 2013, Angra do Heroísmo, Azores, Portugal, September 9-12, 2013. Proceedings, volume 8154 of Lecture Notes in Computer Science, pp. 378–389. Springer, 2013.\n\nJoost van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key, and Yarin Gal. Improving Deterministic Uncertainty Estimation in Deep Learning for Classification and Regression. CoRR, abs/2102.11409, 2021.\n\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 6438–6447. PMLR, 2019.\n\nYuzhe Yang, Kaiwen Zha, Ying-Cong Chen, Hao Wang, and Dina Katabi. Delving into deep imbalanced regression. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 11842–11851. PMLR, 2021.\n\nWei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from a single image. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 204–213. Computer Vision Foundation / IEEE, 2021.\n\nEric Zelikman, Christopher Healy, Sharon Zhou, and Anand Avati. Crude: calibrating regression\n\nuncertainty distributions empirically. arXiv preprint arXiv:2005.12496, 2020.\n\nHongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n\nZizhao Zhang, Adriana Romero, Matthew J Muckley, Pascal Vincent, Lin Yang, and Michal Drozdzal. Reducing uncertainty in undersampled mri reconstruction with active acquisition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2049–2058, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA DISCUSSION ON I.I.D. AND N.I.D. ASSUMPTIONS\n\nGeneralization Error, Bias, and Variance. We could analyze the generalization error of our VIR by bounding the generalization with the sum of three terms: (a) the bias of our estimator, (2) the variance of our estimator, (3) model complexity. Essentially VIR uses the N.I.D. assumption increases our estimator’s bias, but significantly reduces its variance in the imbalanced setting. Since the model complexity is kept the same (using the same backbone neural network) as the baselines, N.I.D. will lead to a lower generalization error.\n\nVariance of Estimators in Imbalanced Settings. In the imbalanced setting, one typically use inverse weighting to produced an unbiased estimator (i.e., making the first term of the aforementioned bound zero). However, for data with extremely low density, its inverse would be extremely large, therefore leading to a very large variance for the estimator. Our VIR replaces I.I.D. with N.I.D. to “smooth out” such singularity, and therefore significantly lowers the variance of the estimator (i.e., making the second term of the aforementioned bound smaller), and ultimately lowers the generalization error.\n\nB ADDITIONAL EXPERIMENT RESULTS\n\nB.1 ABLATION STUDY ON VIR\n\nIn this section, we include ablation studies to verify that our VIR can outperform its counterparts in DIR (i.e., smoothing on the latent space) and DER (i.e., NIG distribution layers).\n\nMetrics\n\nTable 5: Ablation study on AgeDB-DIR in terms of accuracy.\n\nAblation Study on q(zi|{xi}N i=1). To verify the effectiveness of VIR’s encoder q(zi|{xi}N i=1), we replace VIR’s predictor p(yi|zi) with a linear layer (as in DIR). Table 5 shows that compared to its counterpart, FDS (Yang et al., 2021), our encoderonly VIR still leads to a considerable improvements even without generating the NIG distribution, therefore verifying the effectiveness of our VIR’s q(zi|{xi}N\n\nDER (Amini et al., 2020) PREDICTOR-ONLY VIR (OURS)\n\nFDS (Yang et al., 2021) ENCODER-ONLY VIR (OURS)\n\nAll Many Med.\n\n209.76 203.76\n\n216.97 157.92\n\n124.96 121.78\n\n109.78 95.99\n\n106.81 88.96\n\n122.45 95.85\n\nMany Med.\n\n91.32 74.79\n\n93.99 81.89\n\n12.25 10.03\n\n12.69 11.63\n\n8.68 8.72\n\n9.03 7.76\n\n8.12 7.57\n\n8.11 7.28\n\n7.52 6.97\n\n7.36 6.68\n\nMAE ↓\n\nMSE ↓\n\nShot\n\nFew\n\nFew\n\nAll\n\ni=1).\n\nTable 6: Ablation study on AgeDB-DIR in terms of uncertainty estimation.\n\nAblation Study on p(yi|zi). To verify the effectiveness of VIR’s predictor p(yi|zi), we replace VIR’s encoder q(zi|{xi}N i=1) with a simple deterministic encoder as in DER (Amini et al., 2020). Table 5 and Table 6 show that compared to DER, the counterpart of VIR’s predictor, our VIR’s predictor still outperforms than DER, demonstrating its effectiveness; this verifies our claim (Sec. 3.4) that directly reweighting DER breaks NIG and leads to poor performance.\n\nDER Amini et al. (2020) PREDICTOR-ONLY VIR (OURS)\n\nMany Med.\n\nMany Med.\n\n0.449 0.387\n\n3.865 3.854\n\n3.936 3.887\n\n0.590 0.443\n\n0.468 0.390\n\n3.768 3.755\n\n0.500 0.407\n\n4.421 4.394\n\nAUSE ↓\n\nMetrics\n\nNLL ↓\n\nShot\n\nFew\n\nFew\n\nAll\n\nAll\n\nB.2 RESULT ON STS-B-DIR DATASET\n\nIn this section, we report the accuracy and uncertainty evaluation on STS-B-DIR (more details for the dataset is in DIR (Yang et al., 2021)). From Table 7, Table 8, and Table 9 below, we can conclude\n\nTable 7: Evaluation results of accuracy on STS-B-DIR.\n\nMetrics\n\nShot\n\nMSE ↓\n\nMAE ↓\n\nGM ↓\n\nAll\n\nMany Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\nINV DIR (YANG ET AL., 2021) DIR + DER (YANG ET AL., 2021; AMINI ET AL., 2020) VIR (OURS)\n\n1.031 1.000 1.007 0.895\n\n0.930 0.912 0.880 0.799\n\n1.426 1.368 1.535 1.309\n\n1.152 1.055 1.086 0.919\n\n0.825 0.812 0.812 0.760\n\n0.783 0.772 0.757 0.718\n\n1.004 0.989 1.046 0.960\n\n0.850 0.809 0.842 0.732\n\n0.567 0.560 0.558 0.509\n\n0.537 0.535 0.518 0.493\n\n0.744 0.739 0.765 0.669\n\n0.535 0.477 0.574 0.377\n\nthat our model also outperforms all baselines in terms of both accuracy metrics and uncertainty estimation metrics in this NLP dataset; this verifies the superiority of our model for NLP datasets.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Evaluation results of accuracy on STS-B-DIR.\n\nMetrics\n\nShot\n\nPearson ↑\n\nSpearman ↑\n\nAll\n\nMany Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\nINV DIR (YANG ET AL., 2021) DIR + DER (YANG ET AL., 2021; AMINI ET AL., 2020) VIR (OURS)\n\n0.718 0.732 0.729 0.765\n\n0.701 0.711 0.714 0.740\n\n0.612 0.646 0.635 0.663\n\n0.705 0.742 0.731 0.770\n\n0.723 0.731 0.730 0.770\n\n0.678 0.672 0.680 0.713\n\n0.530 0.519 0.526 0.534\n\n0.685 0.739 0.699 0.770\n\nTable 9: Uncertainty estimation results on STS-B-DIR.\n\nMetrics\n\nShot\n\nNLL ↓\n\nAUSE ↓\n\nAll\n\nMany Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\nDIR + DER (YANG ET AL., 2021; AMINI ET AL., 2020) VIR (OURS)\n\n2.561 1.996\n\n2.514 1.810\n\n2.880 2.754\n\n2.358 2.152\n\n0.672 0.591\n\n0.581 0.575\n\n0.609 0.602\n\n0.615 0.510\n\nB.3 DIFFERENCE BETWEEN DIR’S AND OUR REPRODUCED RESULTS\n\nTo reproduce the results on AgeDB, we use exactly the same settings as in DIR’s code (Yang et al., 2021) (i.e., by directly running their code on our machines without modifying hyperparameters). for each model in DIR we report, we use five different random seeds to produce five results. We then report the performance by taking the average of them. Table 10 and Table 11 show the example for SQINV and LDS+FDS on AgeDB-DIR. From the table we can see that under our hardware and\n\nTable 10: Results of running SQINV for 5 different random seeds on AgeDB.\n\nMetrics\n\nShot\n\nMSE ↓\n\nMAE ↓\n\nGM ↓\n\nAll\n\nMany Med.\n\nFew\n\nAll Many Med.\n\nFew\n\nAll Many Med.\n\nFew\n\nSQINV 1 SQINV 2 SQINV 3 SQINV 4 SQINV 5 SQINV AVG SQINV STD SQINV RESULTS FROM (YANG ET AL., 2021)\n\n107.02 111.55 114.33 106.24 104.73 108.77 12.89 105.14\n\n90.71 93.43 96.83 91.81 90.24 92.60 5.67 87.21\n\n131.5 141.03 134.56 120.26 127.33 130.94 48.46 127.66\n\n193.39 209.17 223.86 203.78 208.05 207.65 96.71 212.30\n\n8.04 8.12 8.21 7.94 7.99 8.06 0.01 7.81\n\n7.40 7.47 7.59 7.39 7.47 7.46 0.01 7.16\n\n9.01 9.17 9.01 8.58 8.98 8.95 0.04 8.80\n\n11.33 11.58 11.81 11.39 11.49 11.52 0.03 11.20\n\n5.15 5.21 5.17 5.06 5.07 5.13 0.01 4.99\n\n4.73 4.85 4.74 4.74 4.79 4.77 0.01 4.57\n\n8.81 5.75 5.85 5.41 5.68 6.30 1.60 5.73\n\n8.22 8.25 8.27 7.66 7.98 8.08 0.05 7.77\n\nsoftware environments, the SQINV model and LDS+FDS model (SOTA in DIR) could not perform as well as it is reported in DIR Yang et al. (2021), therefore for fair comparison, we use our replicated performance rather than theirs.\n\nB.4 ABLATION STUDY ON λ\n\nIn this section, we include ablation studies on the λ in our objective function. For λ ∈ {10.0, 1.0, 0.1, 0.01, 0.001}, we run our VIR model on the AgeDB dataset. Table 12 shows the results. We can conclude that when λ = 0.1, our model achieves the best performance.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 11: Results of running LDS+FDS for 5 different random seeds on AgeDB.\n\nMetrics\n\nShot\n\nMSE ↓\n\nMAE ↓\n\nGM ↓\n\nAll\n\nMany Med.\n\nFew\n\nAll Many Med.\n\nFew\n\nAll Many Med.\n\nFew\n\nLDS+FDS 1 LDS+FDS 2 LDS+FDS 3 LDS+FDS 4 LDS+FDS 5 LDS+FDS AVG LDS+FDS STD LDS+FDS RESULTS FROM (YANG ET AL., 2021)\n\n104.33 104.59 110.17 102.68 105.77 105.51 6.41 99.46\n\n88.67 94.63 95.97 98.20 91.07 93.71 11.70 84.10\n\n128.99 125.60 123.24 126.41 127.00 126.25 3.52 112.20\n\n194.06 200.14 208.11 201.16 185.85 197.86 55.97 209.27\n\n7.87 7.98 8.07 8.02 7.93 7.97 0.01 7.55\n\n7.26 7.44 7.54 7.50 7.35 7.42 0.01 7.01\n\n8.97 8.77 8.71 8.82 8.80 8.81 0.01 8.24\n\n10.88 11.16 11.41 11.34 10.96 11.15 0.04 10.79\n\n5.02 5.00 5.09 5.08 5.07 5.05 0.01 4.72\n\n4.60 4.71 4.73 4.63 4.74 4.68 0.03 4.36\n\n5.87 5.62 5.71 5.74 5.52 5.69 0.01 5.45\n\n7.51 7.81 7.48 7.56 7.73 7.62 0.02 6.79\n\nTable 12: Ablation study on λ for VIR on AgeDB-DIR\n\nMetrics\n\nShot\n\nλ = 10.0 λ = 1.0 λ = 0.1 λ = 0.01 λ = 0.001\n\nMSE ↓\n\nMAE ↓\n\nNLL ↓\n\nAll\n\nMany Med.\n\nFew\n\nAll Many Med.\n\nFew\n\nAll\n\nMany Med.\n\nFew\n\n104.31 104.10 86.28 86.86 87.25\n\n91.01 87.28 76.87 76.58 74.13\n\n116.43 128.26 101.57 99.95 104.78\n\n196.35 196.12 132.90 147.82 162.64\n\n7.88 7.83 7.19 7.12 7.13\n\n7.38 7.21 6.75 6.69 6.64\n\n8.42 8.81 7.97 7.72 7.92\n\n11.13 10.89 9.19 9.59 9.63\n\n3.827 3.848 3.785 3.887 3.980\n\n3.733 3.738 3.694 3.797 3.868\n\n4.140 4.041 3.963 4.007 4.161\n\n4.407 4.356 4.151 4.401 4.546\n\n15",
    "reference": "# Summary Of The Paper\n\nThe paper looks at uncertainty estimation for regression on imbalanced datasets. It proposes a new method called VIR which combines variational inference, smoothed statistics, and conjugate distribution parametrization. In the experiments, VIR is evaluated on 2 imbalanced datasets on accuracy and calibration metrics.\n\n# Strength And Weaknesses\n\nPros:\n\n1. The task of uncertainty estimation for regression on imbalanced dataset is important.\n2. The idea of combining smoothed statistics to fix imbalanced dataset and produce uncertainty estimates via parametrizing conjugate distributions is interesting.\n3. The method achieves great improvement on 2 datasets.\n\n##########################################################################\n\nCons:\n\n- Related works: The paper misses many important related works for uncertainty estimation for regression like [1, 2, 3, 4, 5, 6, 7] but also many others. This includes very common baselines like Dropout [6] and Ensemble [1] which work for regression. The paper also does not discuss GP-based methods [2, 3] which can be used for regression. Further, the paper does not discuss Posterior Networks methods [4, 5, 7]. More specifically, [4, 5] shares many similarities with VIR. They use conjugate distributions, pseudo-count interpretations, posterior updates, and variational losses which are key parts of VIR. NatPN is also designed to work for regression. Action suggestion: discuss all of these methods in the related work section.\n- Desiderata: The two desiderata are sometimes not clear. It feels that the two desiderata are high quality *uncertainty estimation* and high performance on *imbalanced dataset*. However the paper mentions that the desiderata are “performance improvement and uncertainty estimation”. The paper also phrases the identification of the desiderata of imbalanced dataset and uncertainty estimation as a contribution.  However people already looked at imbalanced datasets and uncertainty estimation separately in previous works. Finally, the paper does not discuss existing desiderata for uncertainty estimation [8, 7, 9]. Action suggestion: I would recommend to change the phrasing of this contribution and have a discussion on existing desiderata in uncertainty estimation.\n- Loss: I felt that the description of the loss was sometimes confusing. I would be interested in the derivation fo the loss. This could also be provided in the appendix. In eq.(1), what is p_\\theta(z_i) ? Does this ELBO loss assume any prior ? What is the importance of the regularization loss in the total loss ? Action suggestion: Beyond the answer to these questions, it would be intresting to show the important of the regularization term in an experiment.\n- Clarity: The paper is sometimes hard to read. E.g. the paper mentions multiple times “see below” which leaves unclear where to exactly find the missing information. Action suggestion: make explicit reference when pointing to further details.\n- Bins: It is unclear what is the sensitivity of the method to the number of bins. It was also unclear to me when readin sec. 3.1. what data are used to build the mean representation from the bins. Action suggestion: I would recommend to clarify the bins construction since it is an essential part of the method. I would also be interested in an experiment on the number of bins.\n- Experiences: The experiments do not look very extensive to me. They consider only two datasets and a single backbone network. They do not compare to DropOut, Ensemble, GP, or NatPN which would be appropriate baselines. They do not look at common uncertainty estimation metrics like OOD detection scores. They also do not provide error bars which are key to assess the significance of the results. Action suggestion: I would recommend to add at least one dataset (e.g. depth estimation) with another backbone architecture. I would also recommend to add at least Ensemble which is a common and powerful baseline and NatPN which shares many similarities with VIR. I would also recommend to add error bars.\n\nI am happy to improve my score if a majority of the above points are addresses (e.g. with the action suggestions).\n\n[1] Simple and scalable predictive uncertainty estimation using deep ensembles, NeurIPS 2017\n\n[2] On feature collapse and deep kernel learning for single forward pass uncertainty.\n\n[3] Simple and principled uncertainty estimation with deterministic deep learning via distanceawareness, NeurIPS 2020.\n\n[4] Posterior Network: uncertainty estimation without ood samples via density-based pseudo-counts. NeurIPS 2020\n\n[5] Natural Posterior Network: deep bayesian uncertainty for exponential family distributions, ICLR 2022\n\n[6] Dropout as a bayesian approximation: representing model uncertainty in deep learning, ICML 2016\n\n[7] Graph Posterior Network: bayesian predictive uncertainty for node classification. NeurIPS 2021.\n\n[8] Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift, NeurIPS 2019.\n\n[9] NOMU: Neural Optimization-based Model Uncertainty. ICML 2022\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is quite clear even if it is sometimes hard to read with quite dense mathematical notations (e.g. eq. 2, 3, 4) and some unclear cross references (i.e. \"see below\" statements). The paper combines existing techniques (smoothed statistics and conjugate distribution parametrization)  to solve the important problem of prediction on imbalanced datasets. However, it misses many important related works which would allow to more correctly estimate the novelty of the paper for the reader.\n\n# Summary Of The Review\n\nOverall, I vote for strong reject. The task is important and well motivated and the . My major concerns are about the related work, and the experiences (see cons beow). Hopefully the authors can address my concern in the rebuttal period.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMULTI-AGENT SEQUENTIAL DECISION-MAKING VIA COMMUNICATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nCommunication helps agents to obtain information about others so that better coordinated behavior can be learned. Some existing work communicates predicted future trajectory with others, hoping to get clues about what others would do for better coordination. However, circular dependencies sometimes can occur when agents are treated synchronously so it is hard to coordinate decision-making. In this paper, we propose a novel communication scheme, Sequential Communication (SeqComm). SeqComm treats agents asynchronously (the upper-level agents make decisions before the lower-level ones) and has two communication phases. In negotiation phase, agents determine the priority of decision-making by communicating hidden states of observations and comparing the value of intention, which is obtained by modeling the environment dynamics. In launching phase, the upper-level agents take the lead in making decisions and communicate their actions with the lower-level agents. Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, we show that SeqComm outperforms existing methods in various multi-agent cooperative tasks.\n\n1\n\nINTRODUCTION\n\nThe partial observability and stochasticity inherent to the nature of multi-agent systems can easily impede the cooperation among agents and lead to catastrophic miscoordination (Ding et al., 2020). Communication has been exploited to help agents obtain extra information during both training and execution to mitigate such problems (Foerster et al., 2016; Sukhbaatar et al., 2016; Peng et al., 2017). Specifically, agents can share their information with others via a trainable communication channel.\n\nCentralized training with decentralized execution (CTDE) is a popular learning paradigm in cooperative multi-agent reinforcement learning (MARL). Although the centralized value function can be learned to evaluate the joint policy of agents, the decentralized policies of agents are essentially independent. Therefore, a coordination problem arises. That is, agents may make sub-optimal actions by mistakenly assuming others’ actions when there exist multiple optimal joint actions (Busoniu et al., 2008). Communication allows agents to obtain information about others to avoid miscoordination. However, most existing work only focuses on communicating messages, e.g., the information of agents’ current observation or historical trajectory (Jiang & Lu, 2018; Singh et al., 2019; Das et al., 2019; Ding et al., 2020). It is impossible for an agent to acquire other’s actions before making decisions since the game model is usually synchronous, i.e., agents make decisions and execute actions simultaneously. Recently, intention or imagination, depicted by a combination of predicted actions and observations of many future steps, has been proposed as part of messages (Kim et al., 2021; Pretorius et al., 2021). However, circular dependencies can still occur, so it may be hard to coordinate decision-making under synchronous settings.\n\nA general approach to solving the coordination problem is to make sure that ties between equally good actions are broken by all agents. One simple mechanism for doing so is to know exactly what others will do and adjust the behavior accordingly under a unique ordering of agents and actions (Busoniu et al., 2008). Inspired by this, we reconsider the cooperative game from an asynchronous perspective. In other words, each agent is assigned a priority (i.e., order) of decision-making each step in both training and execution, thus the Stackelberg equilibrium (SE) (Von Stackelberg, 2010) is naturally set up as the learning objective. Specifically, the upper-level agents make decisions before the lower-level agents. Therefore, the lower-level agents can acquire the actual actions of the upper-level agents by\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ncommunication and make their decisions conditioned on what the upper-level agents would do. Under this setting, the SE is likely to be Pareto superior to the average Nash equilibrium (NE) in games that require a high cooperation level (Zhang et al., 2020). However, is it necessary to decide a specific priority of decision-making for each agent? Ideally, the optimal joint policy can be decomposed by any orders (Wen et al., 2019), e.g., π∗(a1, a2|s) = π∗(a1|s)π∗(a2|s, a1) = π∗(a2|s)π∗(a1|s, a2). But during the learning process, it is unlikely for agents to use the optimal actions of other agents for gradient calculation, making it still vulnerable to the relative overgeneralization problem (Wei et al., 2018). Overall, there is no guarantee that the above equation will hold in the learning process, thus ordering should be carefully concerned.\n\nIn this paper, we propose a novel model-based multi-round communication scheme for cooperative MARL, Sequential Communication (SeqComm), to enable agents to explicitly coordinate with each other. Specifically, SeqComm has two-phase communication, negotiation phase and launching phase. In the negotiation phase, agents communicate their hidden states of observations with others simultaneously. Then they are able to generate multiple predicted trajectories, called intention, by modeling the environmental dynamics and other agents’ actions. In addition, the priority of decision-making is determined by communicating and comparing the corresponding values of agents’ intentions. The value of each intention represents the rewards obtained by letting that agent take the upper-level position of the order sequence. The sequence of others follows the same procedure as aforementioned with the upper-level agents fixed. In the launching phase, the upper-level agents take the lead in decision-making and communicate their actual actions with the lower-level agents. Note that the actual actions will be executed simultaneously in the environment without any changes.\n\nSeqComm is currently built on MAPPO (Yu et al., 2021). Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, we evaluate SeqComm on a set of tasks in multi-agent particle environment (MPE) (Lowe et al., 2017) and StarCraft multi-agent challenge (SMAC) (Samvelyan et al., 2019). In all these tasks, we demonstrate that SeqComm outperforms prior communication-free and communication-based methods. By ablation studies, we confirm that treating agents asynchronously is a more effective way to promote coordination and SeqComm can provide the proper priority of decision-making for agents to develop better coordination.\n\n2 RELATED WORK\n\nCommunication. Existing studies (Jiang & Lu, 2018; Kim et al., 2019; Singh et al., 2019; Das et al., 2019; Zhang et al., 2019; Jiang et al., 2020; Ding et al., 2020; Konan et al., 2022) in this realm mainly focus on how to extract valuable messages. ATOC (Jiang & Lu, 2018) and IC3Net (Singh et al., 2019) utilize gate mechanisms to decide when to communicate with other agents. Many works (Das et al., 2019; Konan et al., 2022) employ multi-round communication to fully reason the intentions of others and establish complex collaboration strategies. Social influence (Jaques et al., 2019) uses communication to influence the behaviors of others. I2C (Ding et al., 2020) only communicates with agents that are relevant and influential which are determined by causal inference. However, all these methods focus on how to exploit valuable information from current or past partial observations effectively and properly. More recently, some studies (Kim et al., 2021; Du et al., 2021; Pretorius et al., 2021) begin to answer the question: can we favor cooperation beyond sharing partial observation? They allow agents to imagine their future states with a world model and communicate those with others. IS (Pretorius et al., 2021), as the representation of this line of research, enables each agent to share its intention with other agents in the form of the encoded imagined trajectory and use the attention module to figure out the importance of the received intention. However, two concerns arise. On one hand, circular dependencies can lead to inaccurate predicted future trajectories as long as the multi-agent system treats agents synchronously. On the other hand, MARL struggles in extracting useful information from numerous messages, not to mention more complex and dubious messages, i.e., predicted future trajectories.\n\nUnlike these works, we treat the agents from an asynchronously perspective therefore circular dependencies can be naturally resolved. Furthermore, agents only send actions to lower-level agents besides partial observations to make sure the messages are compact as well as informative.\n\nCoordination. The agents are essentially independent decision makers in execution and may break ties between equally good actions randomly. Thus, in the absence of additional mechanisms, different\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nagents may break ties in different ways, and the resulting joint actions may be suboptimal. Coordination graphs (Guestrin et al., 2002; B ̈ohmer et al., 2020; Wang et al., 2021b) simplify the coordination when the global Q-function can be additively decomposed into local Q-functions that only depend on the actions of a subset of agents. Typically, a coordination graph expresses a higher-order value decomposition among agents. This improves the representational capacity to distinguish other agents’ effects on local utility functions, which addresses the miscoordination problems caused by partial observability. Another general approach to solving the coordination problem is to make sure that ties are broken by all agents in the same way, requiring that random action choices are somehow coordinated or negotiated. Social conventions (Boutilier, 1996) or role assignments (Prasad et al., 1998) encode prior preferences towards certain joint actions and help break ties during action selection. Communication (Fischer et al., 2004; Vlassis, 2007) can be used to negotiate action choices, either alone or in combination with the aforementioned techniques. Our method follows this line of research by utilizing the ordering of agents and actions to break the ties, other than the enhanced representational capacity of the local value function.\n\n3 PROBLEM FORMULATION\n\nCost-Free Communication. The decentralized partially observable Markov decision process (DecPOMDP) can be extended to explicitly incorporate broadcasting observations. The resulting model is called multi-agent POMDP (Oliehoek et al., 2016).\n\nPynadath & Tambe (2002) showed that under cost-free communication, a joint communication policy that shares local observations at each stage is optimal. Many studies have also investigated sharing local observations in models that are similar to multi-agent POMDP (Pynadath & Tambe, 2002; Ooi & Wornell, 1996; Nair et al., 2004; Roth et al., 2005a;b; Spaan et al., 2006; Oliehoek et al., 2007; Becker et al., 2004). These works focus on issues other than communication cost and we foucs on the coordination problem. Note that even under multi-agent POMDP where agents can get joint observations, coordination problem can still arise (Busoniu et al., 2008). Suppose the centralized critic has learnt actions pairs [a1, a2] and [b1, b2] are equally optimal. Without any prior information, the individual policies π1 and π2 learnt from the centralized critic can break the ties randomly and may choose a1 and b2, respectively.\n\nMulti-Agent Sequential Decision-Making. We consider fully cooperative multi-agent tasks that are modeled as multi-agent POMDP, where n agents interact with the environment according to the following procedure, which we refer to as multi-agent sequential decision-making.\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt , a1\n\n, ak−1\n\n}, ok+1\n\n, a1:k−1\n\nt }, . . . , {ok−1\n\n(cid:44) {{o1 }, where o−k\n\nAt each timestep t, assume the priority (i.e., order) of decision-making for all agents is given and each priority level has only one agent (i.e., agents make decisions one by one). Note that the smaller the level index, the higher priority of decision-making is. The agent at each level k gets t drawn from the state st, and receives messages m−k its own observation ok from all other agents, where m−k can be written as {o−k denotes = ∅. the joint actions of agents 1 to k − 1. For the agent at the first level (i.e., k = 1), a1:k−1 Then, the agent determines its action ak t sampled from its policy πk(·|ok ) or equivalently πk(·|ot, a1:k−1 ) and sends it to the lower-level agents. After all agents have determined their actions, they perform the joint actions at, which can be seen as sampled from the joint policy π(·|st) factorized as (cid:81)n ), in the environment and get a shared reward r(st, at) and the state transitions to next state s(cid:48) according to the transition probability p(s(cid:48)|st, at). All agents aim to maximize the expected return (cid:80)∞ t=0 γtrt, where γ is the discount factor. The state-value function and action-value function of the level-k agent are defined as follows:\n\ndenotes the joint observations of all agents except k, and a1:k−1\n\nt }. Equivalently, m−k\n\nk=1 πk(·|ot, a1:k−1\n\nt , m−k\n\n, . . . , on\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\nVπk (s, a1:k−1) (cid:44)\n\n∞ (cid:88) [\n\nt=0\n\nE s1:∞ 0 ∼πk:n a1:∞∼π\n\nak:n\n\nγtrt|s0 = s, a1:k−1\n\n0\n\n= a1:k−1]\n\nQπk (s, a1:k) (cid:44)\n\nE s1:∞\n\nak+1:n\n\n0\n\n∼πk+1:n\n\na1:∞∼π\n\n∞ (cid:88) [\n\nt=0\n\nγtrt|s0 = s, a1:k\n\n0 = a1:k].\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFor the setting of multi-agent sequential decision-making discussed above, we have the following proposition. Proposition 1. If all the agents update its policy with individual TRPO (Schulman et al., 2015) sequentially in multi-agent sequential decision-making, then the joint policy of all agents is guaranteed to improve monotonically and converge.\n\nProof. The proof is given in Appendix A.\n\nProposition 1 indicates that SeqComm has the performance guarantee regardless of the priority of decision-making in multi-agent sequential decision-making. However, the priority of decision-making indeed affects the optimality of the converged joint policy, and we have the following claim. Claim 1. The different priorities of decision-making affect the optimality of the convergence of the learning algorithm due to the relative overgeneralization problem.\n\nt\n\nA\n\n0 8\n\n8 0\n\nb3 6\n\nn e\ng A\n\n(a) payoff matrix of the game\n\nAgent B b2 6\n\nb1 a1 12 a2 −6 a3 −6\n\nWe use a one-step matrix game as an example, as illustrated in Figure 1(a), to demonstrate the influence of the priority of decision-making on the learning process. Due to relative overgeneralization (Wei et al., 2018), agent B tends to choose b2 or b3. Specifically, b2 or b3 in the suboptimal equilibrium is a better choice than b1 in the optimal equilibrium when matched with arbitrary actions from agent A. Therefore, as shown in Figure 1(b), B → A (i.e., agent B makes decisions before A, and A’s policy conditions on the action of B) and Simultaneous (i.e., two agents make decisions simultaneously and independently) are easily trapped into local optima. However, things can be different if agent A goes first, as A → B achieves the optimum. As long as agent A does not suffer from relative overgeneralization, it can help agent B get rid of local optima by narrowing down the search space of B. Besides, a policy that determines the priority of decision-making can be learned under the guidance of the state-value function, denoted as Learned. It obtains better performance than B → A and Simultaneous, which indicates that dynamically determining the order during policy learning can be beneficial as we do not know the optimal priority in advance.\n\nFigure 1: (a) Payoff matrix for a one-step game. There are multiple local optima. (b) Evaluations of different methods for the game in terms of the mean reward and standard deviation of ten runs. A → B, B → A, Simultaneous, and Learned represent that agent A makes decisions first, agent B makes decisions first, two agents make decisions simultaneously, and there is another learned policy determining the priority of decision making, respectively. MAPPO (Yu et al., 2021) is used as the backbone.\n\n(b) evaluations of different methods\n\nRemark 1. The priority (i.e., order) of decision-making affects the optimality of the converged joint policy in multi-agent sequential decision-making, thus it is critical to determine the order. However, learning the order directly requires an additional centralized policy in execution, which is not generalizable in the scenario where the number of agents varies. Moreover, its learning complexity exponentially increases with the number of agents, making it infeasible in many cases.\n\n4 SEQUENTIAL COMMUNICATION\n\nIn this paper, we cast our eyes in another direction and resort to the world model. Ideally, we can randomly sample candidate order sequences, evaluate them under the world model (see Section 4.1), and choose the order sequence that is deemed the most promising under the true dynamic. SeqComm is designed based on this principle to determine the priority of decision-making via communication.\n\nSeqComm adopts a multi-round communication mechanism, i.e., agents are allowed to communicate with others in multiple rounds. Importantly, communication is separated into phases serving different purposes. One is the negotiation phase for agents to determine the priority of decision-making. Another is the launching phase for agents to act conditioning on actual actions upper-level agents will take to implement explicit coordination via communication. The overview of SeqComm is illustrated in Figure 2. Each SeqComm agent consists of a policy, a critic, and a world model, as illustrated in Figure 3, and the parameters of all networks are shared across agents (Gupta et al., 2017).\n\n4\n\n012345step1e6678910111213eval rewardBAABSimultaneousLearnedUnder review as a conference paper at ICLR 2023\n\nFigure 2: Overview of SeqComm. SeqComm has two communication phases, the negotiation phase (left) and the launching phase (right). In the negotiation phase, agents communicate hidden states of observations with others and obtain their own intention. The priority of decision-making is determined by sharing and comparing the value of all the intentions. In the launching phase, the agents who hold the upper-level positions will make decisions prior to the lower-level agents. Besides, their actions will be shared with anyone that has not yet made decisions.\n\n4.1 NEGOTIATION PHASE\n\nIn the negotiation phase, the observation encoder first takes ot as input and outputs a hidden state ht, which is used to communicate with others. Agents then determine the priority of decision-making by intention which is established and evaluated based on the world model.\n\nWorld Model. The world model is needed to predict and evaluate future trajectories. SeqComm, unlike previous works (Kim et al., 2021; Du et al., 2021; Pretorius et al., 2021), can utilize received hidden states of other agents in the first round of communication to model more precise environment dynamics for the explicit coordination in the next round of communication. Once an agent can access other agents’ hidden states, it shall have adequate information to estimate their actions since all agents are homogeneous and parameter-sharing. Therefore, the world model M(·) takes as input the joint hidden states ht = {h1 t } and actions at, and predicts the next joint observations and reward,\n\nt , . . . , hn\n\nˆot+1, ˆrt+1 = Mi(AMw(ht, at)),\n\nwhere AMw is the attention module. The reason that we adopt the attention module is to entitle the world model to be generalizable in the scenarios where additional agents are introduced or existing agents are removed.\n\nPriority of Decision-Making. The intention is the key element to determine the priority of decision-making. The notion of intention is described as an agent’s future behavior in previous works (Rabinowitz et al., 2018; Raileanu et al., 2018; Kim et al., 2021). However, we define the intention as an agent’s future behavior without considering others.\n\nAs mentioned before, an agent’s intention considering others can lead to circular dependencies and cause miscoordination. By our definition, the intention of an agent should be depicted as all future trajectories considering that agent as the first-mover and ignoring the others. However, there are many possible future trajectories as the priority of the rest agents is unfixed. In practice, we use the Monte Carlo method to evaluate intention.\n\nTaking agent i at timestep t to illustrate, it firstly considers itself as the first-mover and produces its action only based on the joint hidden states, ˆai t ∼ πi(·|AMa(ht)), where we again use an\n\nFigure 3: Architecture of SeqComm. The critic and policy of each agent take input as its own observation and received messages. The world model takes as input the joint hidden states and predicted joint actions.\n\n5\n\nagent 1agent 1’s obsagent 2agent 3agent 4requestreplyagent 1agent 1’s obsagent 2agent 3agent 4replyrequestrequestrequestreplyreplyAgent 1 chooses to send request to agent 2 and ignore agent 312341234Agent 1 chooses to send request to agent 2, 3, 4tt+1BCBCorder setAsACsCBsBBC1BC2r1r2intention rewardACoCBoBBC1BC2r1r2intention value (A)BCoCAoAAC1AC2r1r2intention value (B)CAoABoBBA1BA2r1r2intention value (C)compareCAintention value (A)ABintention value (B)compareBintention value (B)compare12AC3ACB4negotiation phaselaunching phaseACB2ACB1aCACB2aAACB3aBaAaCACB1aCACB2aAACB3aBaAaCACBactionstateagentactionstateagentt+1t+2t+3BAactionsstateagentsample sequenceAMaPolicyo3ta3tauppert={a1t,a2t}Cri/cAMaPolicyo2ta2tauppert={a1t}Cri/cAMaPolicyo1ta1tauppert={}Cri/ĉrt+1̂ot+1={̂o1t+1,...,̂ont+1}at={a1t,...,ant}ht={h1t,...,hnt}First moverSecond moverThird moverWorld ModelAMwht={h1t,...,hnt}⋯Under review as a conference paper at ICLR 2023\n\nattention module AMa to handle the input. For the order sequence of lower-level agents, we randomly sample a set of order sequences from unfixed agents. Assume agent j is the second-mover, agent i models j’s action by considering the upper-level action following its own policy ˆaj t ∼ πi(·|AMa(ht, ˆai t)). The same procedure is applied to predict the actions of all other agents following the sampled order sequence. Based on the joint hidden states and predicted actions, the next joint observations ˆot+1 and corresponding reward ˆrt+1 can be predicted by the world model. The length of the predicted future trajectory is H and it can then be written as τ t = { ˆot+1, ˆat+1, . . . , ˆot+H , ˆat+H } by repeating the procedure aforementioned and the value of one trajectory is defined as the return of that trajectory vτ t = (cid:80)t+H t(cid:48)=t+1 γt(cid:48)−t−1ˆrt(cid:48)/H. In addition, the intention value is defined as the average value of F future trajectories with different sampled order sequences. The choice of F is a tradeoff between the computation overhead and the accuracy of the estimation.\n\nAfter all the agents have computed their own intention and the corresponding value, they again communicate their intention values to others. Then agents would compare and choose the agent with the highest intention value to be the first-mover. The priority of lower-level decision-making follows the same procedure with the upper-level agents fixed. Note that some agents are required to communicate intention values with others multiple times until the priority of decision-making is finally determined.\n\n4.2 LAUNCHING PHASE\n\nAs for the launching phase, agents communicate for obtaining additional information to make decisions. Apart from the received hidden states from the last phase, we allow agents to get what actual actions the upper-level agents will take in execution, while other studies can only infer others’ actions by opponent modeling (Rabinowitz et al., 2018; Raileanu et al., 2018) or communicating intentions (Kim et al., 2021). Therefore, miscoordination can be naturally avoided and a better cooperation strategy is possible since lower-level agents can adjust their behaviors accordingly. )), where aupper A lower-level agent i make a decision following the policy πi(·|AMa(ht, aupper means received actual actions from all upper-level agents. As long as the agent has decided its action, it will send its action to all other lower-level agents by the communication channel. Note that the actions are executed simultaneously and distributedly in execution, though agents make decisions sequentially.\n\nt\n\nt\n\nCommunication Overhead. Two communication phases alternate until all agents determine their levels and get upper-level actions. Note that many previous works also adopt the multi-round communication scheme (Das et al., 2019; Singh et al., 2019). As for implementation in practice, compared with communicating high-dimensional hidden states/observations by multiple rounds (Das et al., 2019; Singh et al., 2019), or transferring multi-step trajectory (Kim et al., 2021), SeqComm needs more rounds, but it only transmits hidden states for one time. For the rest n − 1 round communication with total (n − 1)/2 broadcasts per agent, only a single intention value and an action will be exchanged. Considering there are n! permutations of different order choices for n agents, our method has greatly reduced computation overhead since each agent needs to calculate up to n times to search for a satisfying order. Although SeqComm is more suitable for latency-tolerate MARL tasks, e.g., power dispatch (minutes) (Wang et al., 2021a), inventory management (hours) (Feng et al., 2021), maritime transportation (days) (Li et al., 2019), it is possible for SeqComm to have a wider range of applications given the rapid development of the communication technology, e.g., 5G.\n\n4.3 THEORETICAL ANALYSIS\n\nAs the priority of decision-making is determined by intention values, SeqComm is likely to choose different orders at different timesteps during training. However, we have the following proposition that theoretically guarantees the performance of the learned joint policy under SeqComm.\n\nProposition 2. The monotonic improvement and convergence of the joint policy in SeqComm are independent of the priority of decision-making of agents at each timestep.\n\nProof. The proof is given in Appendix A.\n\nThe priority of decision-making is chosen under the world model, thus the compounding errors in the world model can result in discrepancies between the predicted returns of the same order under the\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a) PP\n\n(b) CN\n\n(c) KA\n\nFigure 4: Learning curves in terms of the mean reward averaged over timesteps of SeqComm and baselines on three MPE tasks: (a) predator-prey, (b) cooperative navigation, and (c) keep-away.\n\nworld model and the true dynamics. We then analyze the monotonic improvement for the joint policy under the world model based on Janner et al. (2019).\n\nTheorem 1. Let the expected total variation between two transition distributions be bounded at each timestep as maxt Es∼πβ,t[DT V (p(s(cid:48)|s, a)||ˆp(s(cid:48)|s, a))] ≤ (cid:15)m, and the policy divergences at level k be bounded as maxs,a1:k−1 DT V (πβ,k(ak|s, a1:k−1)||πk(ak|s, a1:k−1)) ≤ (cid:15)πk , where πβ is the data collecting policy for the model and ˆp(s(cid:48)|s, a) is the transition distribution under the model. Then the model return ˆη and true return η of the policy π are bounded as:\n\nˆη[π] ≥ η[π] − [\n\n2γrmax((cid:15)m + 2 (cid:80)n\n\nk=1 (cid:15)πk )\n\n(1 − γ)2\n\n+\n\n4rmax\n\n(cid:80)n\n\nk=1 (cid:15)πk\n\n(1 − γ)\n\n(cid:124)\n\n(cid:123)(cid:122) C((cid:15)m,(cid:15)π1:n )\n\n]\n\n(cid:125)\n\nProof. The proof is given in Appendix B.\n\nRemark 2. Theorem 1 provides a useful relationship between the compounding errors and the policy update. As long as we improve the return under the true dynamic by more than the gap, C((cid:15)m, (cid:15)π1:n ), we can guarantee the policy improvement under the world model. If no such policy exists to overcome the gap, it implies the model error is too high, that is, there is a large discrepancy between the world model and true dynamics. Thus the order sequence obtained under the world model is not reliable. Such an order sequence is almost the same as a random one. Though a random order sequence also has the theoretical guarantee of Proposition 2, we will show in Section 5.2 that a random order sequence leads to a poor local optimum empirically.\n\n5 EXPERIMENTS\n\nSequential communication (SeqComm) is currently instantiated based on MAPPO (Yu et al., 2021). We evaluate SeqComm on three tasks in multi-agent particle environment (MPE) (Lowe et al., 2017) and four maps in StarCraft multi-agent challenge (SMAC) (Samvelyan et al., 2019).\n\nFor these experiments, we compare SeqComm against the following communication-free and communication-based baselines: MAPPO (Yu et al., 2021), QMIX (Rashid et al., 2018), IS (Kim et al., 2021), TarMAC (Das et al., 2019), and I2C (Ding et al., 2020). In more detail, IS communicates predicted future trajectories (observations and actions), and predictions are made by the environment model. TarMAC uses the attention model to focus more on important incoming messages (the hidden states of observations). TarMAC is reproduced based on MAPPO instead of A2C in the original paper for better performance. I2C infers one-to-one communication to reduce the redundancy of messages (also conditioning on observations).\n\nIn the experiments, all the methods are parameter-sharing for fast convergence. We have fine-tuned the baselines for a fair comparison. Please refer to Appendix E for experimental settings and Appendix F for implementation details. All results are presented in terms of the mean and standard deviation of five runs with different random seeds.\n\n5.1 RESULTS\n\nMPE. We experiment on predator-prey (PP), cooperative navigation (CN), and keep-away (KA) in MPE. In PP, five predators (agents) try to capture three prey. In CN, five agents try to occupy five landmarks. In KA, three attackers (agents) try to occupy three landmarks, however, there are three\n\n7\n\n0.00.20.40.60.81.0step1e72.01.81.61.41.21.00.8average rewardSeqCommTarMACmappoISI2C0.00.20.40.60.81.0step1e72.22.01.81.61.41.2average rewardSeqCommTarMACmappoISI2C0.00.20.40.60.81.0step1e71.41.31.21.11.0average rewardSeqCommTarMACmappoISI2CUnder review as a conference paper at ICLR 2023\n\n(a) 6h vs 8z\n\n(b) MMM2\n\n(c) 10m vs 11m\n\n(d) 8m vs 9m\n\nFigure 5: Learning curves in terms of the win rate of SeqComm and baselines on four customized SMAC maps: (a) 6h vs 8z, (b) MMM2, (c) 10m vs 11m, and (d) 8m vs 9m.\n\n(a) PP\n\n(b) CN\n\n(c) KA\n\n(d) 6h vs 8z\n\n(e) MMM2\n\n(f) 10m vs 11m\n\n(g) 8m vs 9m\n\nFigure 6: Ablation studies on the priority of decision-making in all the tasks. Fix-C: the priority of decision-making is fixed throughout one episode. Random-C: the priority of decision-making is determined randomly. TarMAC is also compared as a reference without explicit action coordination.\n\ndefenders to push them away. In all three tasks, the size of agents is set to be larger than the original settings so that collisions occur more easily, following the settings in (Kim et al., 2021). In addition, agents cannot observe any other agents, and this makes the task more difficult and communication more important. We can observe similar modifications in previous works (Foerster et al., 2016; Ding et al., 2020). After all, we want to demonstrate the superior over communication-based baselines, and communication-based methods are more suitable for scenarios with limited vision. More details about experimental settings are available in Appendix E.\n\nFigure 4 shows the learning curves of all the methods in terms of the mean reward averaged over timesteps in PP, CN, and KA. We can see that SeqComm converges to the highest mean reward compared with all the baselines. The results demonstrate the superiority of SeqComm. In more detail, all communication-based methods outperform MAPPO, indicating the necessity of communication in these difficult tasks. Apart from MAPPO, IS performs the worst since it may access inaccurate predicted information due to the circular dependencies. The substantial improvement SeqComm over I2C and TarMAC is attributed to that SeqComm allows agents to get more valuable action information for explicit coordination. The agents learned by SeqComm show sophisticated coordination strategies induced by the priority of decision-making, which can be witnessed by the visualization of agent behaviors. More details are given in Appendix C. Note that QMIX is omitted in the comparison for clear presentation since Yu et al. (2021) have shown QMIX and MAPPO exhibit similar performance in various MPE tasks.\n\nSMAC. We also evaluate SeqComm against the baselines on four customized maps in SMAC: 6h vs 8z, MMM2, 10m vs 11m, and 8m vs 9m, where we have made some minor changes to the observation part of agents to make it more difficult. Specifically, the sight range of agents is reduced from 9 to 2, and agents cannot perceive any information about their allies even if they are within the sight range. NDQ (Wang et al., 2020) adopts a similar change to increase the difficulty of action coordination and demonstrates that the miscoordination problem is widespread in multi-agent learning. The rest settings remain the same as the default.\n\n8\n\n012345step1e60.00.20.40.60.81.0win rateSeqCommTarMACmappoqmix01234567step1e60.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACmappoqmix0.00.20.40.60.81.0step1e70.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACmappoqmix01234567step1e60.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACmappoqmix0.00.20.40.60.81.0step1e71.61.51.41.31.21.11.00.90.8average rewardSeqCommFix-CRandom-CTarMAC0.00.20.40.60.81.0step1e72.22.01.81.61.41.2average rewardSeqCommTarMACFix-CRandom-C0.00.20.40.60.81.0step1e71.101.081.061.041.021.000.980.960.94average rewardSeqCommTarMACFix-CRandom-C012345step1e60.00.20.40.60.81.0win rateSeqCommTarMACFix-CRandom-C01234567step1e60.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACFix-CRandom-C0.00.20.40.60.81.0step1e70.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACFix-CRandom-C01234567step1e60.000.050.100.150.200.250.300.350.40win rateSeqCommTarMACFix-CRandom-CUnder review as a conference paper at ICLR 2023\n\nThe learning curves of SeqComm and the baselines in terms of the win rate are illustrated in Figure 5. IS and I2C fail in this task and get a zero win rate because these two methods are built on MADDPG. However, MADDPG cannot work well in SMAC, especially when we reduce the sight range of agents, which is also supported by other studies (Papoudakis et al., 2021). SeqComm and TarMAC converge to better performances than MAPPO and QMIX, which demonstrates the benefit of communication. Moreover, SeqComm outperforms TarMAC, which again verifies the gain of explicit action coordination.\n\n5.2 ABLATION STUDIES\n\nPriority of Decision-Making. We compare SeqComm with two ablation baselines with only a difference in the priority of decision-making: the priority of decision-making is fixed throughout one episode, denoted as Fix-C, and the priority of decision-making is determined randomly at each timestep, denoted as Random-C. TarMAC is also compared as a reference without explicit action coordination.\n\nAs depicted in Figure 6, SeqComm achieves a higher mean reward or win rate than Fix-C, Random-C, and TarMAC in all the tasks. These results verify the importance of the priority of decision-making and the necessity to continuously adjust it during one episode. It is also demonstrated that SeqComm can provide a proper priority of decision-making. As discussed in Section 4.3, although Fix-C and Random-C also have the theoretical guarantee, they converge to poor local optima in practice. Moreover, Fix-C and Random-C show better performance than TarMAC in most tasks. This result accords with the hypothesis that the SE is likely to be Pareto superior to the average NE in games with a high cooperation level. Additionally, the learned policy of SeqComm can generalize well to the same task with a different number of agents in MPE, which is detailed in Appendix C.\n\nCommunication Range. We also carry out ablation studies on communication range in MPE tasks. Note that communication range means how many nearest neighbors each agent is allowed to communicate with, following the setting in Ding et al. (2020). We reduce the communication range of SeqComm from 4 to 2 and 0. As there are only three agents in KA, it is omitted in this study. The results are shown in Figure 7. Communication-based agents perform better than communication-free agents, which accords with the results of many previous studies. More importantly, the superiority of SeqComm with communication range 2 over the corresponding TarMAC again demonstrates the effectiveness of sequential communication even in reduced communication ranges.\n\nHowever, as the communication range decreases from 4 to 2, there is no performance reduction in these two MPE tasks. On the contrary, the agents with communication range 2 perform the best. It accords with the results in I2C (Ding et al., 2020) and ATOC (Jiang & Lu, 2018) that redundant information can impair the learning process sometimes. In other settings, this conclusion might not be true. Moreover, since under our communication scheme agents can obtain more information, i.e., the actual actions of others, it is more reasonable that SeqComm can still outperform other methods in reduced communication ranges.\n\nFigure 7: Ablation studies on reduced communication range in (a) predator-prey and (b) cooperative navigation.\n\n(b) CN\n\n(a) PP\n\n6 CONCLUSIONS\n\nWe have proposed SeqComm, which enables agents explicitly coordinate with each other. SeqComm from an asynchronous perspective allows agents to make decisions sequentially. A two-phase communication scheme has been adopted for determining the priority of decision-making and communicating messages accordingly. Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, it is demonstrated that SeqComm outperforms baselines in a variety of cooperative multi-agent tasks and SeqComm can provide a proper priority of decision-making.\n\n9\n\n0.00.20.40.60.81.0step1e72.22.01.81.61.41.21.00.8average rewardcomm_range_4comm_range_2comm_range_0TarMAC_comm_range_20.00.20.40.60.81.0step1e72.22.01.81.61.41.21.00.8average rewardcomm_range_4comm_range_2comm_range_0TarMAC_comm_range_2Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRaphen Becker, Shlomo Zilberstein, Victor R. Lesser, and Claudia V. Goldman. Solving transition independent decentralized markov decision processes. J. Artif. Intell. Res., 22:423–455, 2004.\n\nWendelin B ̈ohmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International\n\nConference on Machine Learning (ICML), 2020.\n\nCraig Boutilier. Planning, learning and coordination in multiagent decision processes. In Conference\n\non Theoretical Aspects of Rationality and Knowledge, 1996.\n\nLucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156–172, 2008.\n\nAbhishek Das, Th ́eophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication. In International Conference on Machine Learning (ICML), 2019.\n\nZiluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for multi-agent cooperation. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nYali Du, Yifan Zhao, Meng Fang, Jun Wang, Gangyan Xu, and Haifeng Zhang. Learning predictive\n\ncommunication by imagination in networked system control, 2021.\n\nMingxiao Feng, Guozi Liu, Li Zhao, Lei Song, Jiang Bian, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. Multi-agent reinforcement learning with shared resource in inventory management. 2021.\n\nFelix Fischer, Michael Rovatsos, and Gerhard Weiss. Hierarchical reinforcement learning in In International Joint Conference on Au-\n\ncommunication-mediated multiagent coordination. tonomous Agents and Multiagent Systems (AAMAS), 2004.\n\nJakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2016.\n\nAmy Greenwald, Keith Hall, and Roberto Serrano. Correlated q-learning. In A comprehensive survey\n\nof multiagent reinforcement learning, 2003.\n\nCarlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In\n\nInternational Conference on Machine Learning (ICML), 2002.\n\nJayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2017.\n\nMichael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\nNatasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, Dj Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In International Conference on Machine Learning (ICML), 2019.\n\nJiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation.\n\nAdvances in Neural Information Processing Systems (NeurIPS), 2018.\n\nJiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement\n\nlearning. In International Conference on Learning Representation (ICLR), 2020.\n\nDaewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan Son, and Yung Yi. Learning to schedule communication in multi-agent reinforcement learning. In International Conference on Learning Representations (ICLR), 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWoojun Kim, Jongeui Park, and Youngchul Sung. Communication in multi-agent reinforcement learning: Intention sharing. In International Conference on Learning Representations (ICLR), 2021.\n\nSachin Konan, Esmaeil Seraj, and Matthew Gombolay. Iterated reasoning with mutual information in cooperative and byzantine decentralized teaming. In International Conference on Learning Representations (ICLR), 2022.\n\nVille K ̈on ̈onen. Asymmetric multiagent reinforcement learning. Web Intelligence and Agent Systems:\n\nAn international journal, 2(2):105–121, 2004.\n\nXihan Li, Jia Zhang, Jiang Bian, Yunhai Tong, and Tie-Yan Liu. A cooperative multi-agent reinforcement learning framework for resource balancing in complex logistics network. arXiv preprint arXiv:1903.00714, 2019.\n\nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems (NeurIPS), 2017.\n\nHang Ma, Daniel Harabor, Peter J Stuckey, Jiaoyang Li, and Sven Koenig. Searching with consistent prioritization for multi-agent path finding. In AAAI Conference on Artificial Intelligence (AAAI), 2019.\n\nRanjit Nair, Milind Tambe, Maayan Roth, and Makoto Yokoo. Communication for improving policy computation in distributed pomdps. In International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2004.\n\nFrans A. Oliehoek, Matthijs T. J. Spaan, and Nikos A. Vlassis. Dec-pomdps with delayed communication. In AAMAS Workshop on Multi-agent Sequential Decision Making in Uncertain Domains, 2007.\n\nFrans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs,\n\nvolume 1. Springer, 2016.\n\nJ.M. Ooi and G.W. Wornell. Decentralized control of a multiple access broadcast channel: perfor-\n\nmance bounds. In IEEE Conference on Decision and Control, 1996.\n\nGeorgios Papoudakis, Filippos Christianos, Lukas Sch ̈afer, and Stefano V. Albrecht. Benchmarking\n\nmulti-agent deep reinforcement learning algorithms in cooperative tasks, 2021.\n\nPeng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and Jun Wang. Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning to play starcraft combat games. arXiv preprint arXiv:1703.10069, 2017.\n\nMV Nagendra Prasad, Victor R Lesser, and Susan E Lander. Learning organizational roles for negotiated search in a multiagent system. International Journal of Human-Computer Studies, 48 (1):51–67, 1998.\n\nArnu Pretorius, Scott Cameron, Andries Petrus Smit, Elan van Biljon, Lawrence Francis, Femi Azeez, Alexandre Laterre, and Karim Beguir. Learning to communicate through imagination with model-based deep multi-agent reinforcement learning, 2021.\n\nDavid V. Pynadath and Milind Tambe. The communicative multiagent team decision problem:\n\nAnalyzing teamwork theories and models. J. Artif. Intell. Res., 16:389–423, 2002.\n\nNeil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick.\n\nMachine theory of mind. In International Conference on Machine Learning (ICML), 2018.\n\nRoberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2018.\n\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder de Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nMaayan Roth, Reid Simmons, and Manuela Veloso. Decentralized communication strategies for coordinated multi-agent policies. In Lynne E. Parker, Frank E. Schneider, and Alan C. Schultz (eds.), Multi-Robot Systems. From Swarms to Intelligent Automata Volume III. Springer, 2005a.\n\nMaayan Roth, Reid G. Simmons, and Manuela M. Veloso. Reasoning about joint beliefs for executiontime communication decisions. In International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2005b.\n\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson. The StarCraft Multi-Agent Challenge. arXiv preprint arXiv:1902.04043, 2019.\n\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\n\npolicy optimization. In International Conference on Machine Learning (ICML), 2015.\n\nAmanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Individualized controlled continuous communication model for multiagent cooperative and competitive tasks. In International Conference on Learning Representations (ICLR), 2019.\n\nEric Sodomka, Elizabeth Hilliard, Michael Littman, and Amy Greenwald. Coco-q: Learning in stochastic games with side payments. In International Conference on Machine Learning (ICML), 2013.\n\nMatthijs T. J. Spaan, Geoffrey J. Gordon, and Nikos Vlassis. Decentralized planning under uncertainty for teams of communicating agents. In International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2006.\n\nSainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation.\n\nIn Advances in Neural Information Processing Systems (NeurIPS), 2016.\n\nJur P Van Den Berg and Mark H Overmars. Prioritized motion planning for multiple robots. In\n\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2005.\n\nNikos Vlassis. A concise introduction to multiagent systems and distributed artificial intelligence.\n\nSynthesis Lectures on Artificial Intelligence and Machine Learning, 1(1):1–71, 2007.\n\nHeinrich Von Stackelberg. Market structure and equilibrium. Springer Science & Business Media,\n\n2010.\n\nJianhong Wang, Wangkun Xu, Yunjie Gu, Wenbin Song, and Tim C Green. Multi-agent reinforcement learning for active voltage control on power distribution networks. Advances in Neural Information Processing Systems (NeurIPS), 2021a.\n\nTonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable In International Conference on Learning\n\nvalue functions via communication minimization. Representation (ICLR), 2020.\n\nTonghan Wang, Liang Zeng, Weijun Dong, Qianlan Yang, Yang Yu, and Chongjie Zhang. Context-\n\naware sparse deep coordination graphs. arXiv preprint arXiv:2106.02886, 2021b.\n\nErmo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. In AAAI Spring\n\nSymposium Series, 2018.\n\nYing Wen, Yaodong Yang, Rui Luo, Jun Wang, and W Pan. Probabilistic recursive reasoning for multi-agent reinforcement learning. In International Conference on Learning Representations (ICLR), 2019.\n\nChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.\n\nHaifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun Wang. Bi-level actor-critic for multi-agent coordination. In AAAI Conference on Artificial Intelligence (AAAI), 2020.\n\nSai Qian Zhang, Qi Zhang, and Jieyu Lin. Efficient communication in multi-agent reinforcement In Advances in Neural Information Processing Systems\n\nlearning via variance based control. (NeurIPS), 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS OF PROPOSITION 1 AND PROPOSITION 2\n\nLemma 1 (Agent-by-Agent PPO). If we update the policy of each agent i with TRPO Schulman et al. (2015) (or approximately PPO) when fixing all the other agent’s policies, then the joint policy will improve monotonically.\n\nProof. We consider the joint surrogate objective in TRPO Lπold(πnew) where πold is the joint policy before updating and πnew is the joint policy after updating.\n\nGiven that π−i\n\nnew = π−i\n\nold, we have:\n\nLπold(πnew) = Ea∼πnew [Aπold (s, a)]\n\nAπold(s, a)]\n\nAπold (s, a)]\n\n= Ea∼πold [\n\n= Ea∼πold [\n\n= E\n\nai∼πi\n\nold\n\n= E\n\nai∼πi\n\nold\n\nπnew(a|s) πold(a|s) new(ai|s) πi πi old(ai|s) (cid:20) πi new(ai|s) πi old(ai|s) (cid:20) πi new(ai|s) πi old(ai|s)\n\n= Lπi\n\nold\n\n(πi\n\nnew),\n\nE\n\na−i∼π−i\n\nold\n\n(cid:21) [Aπold(s, ai, a−i)]\n\n(cid:21)\n\nAi\n\nπold\n\n(s, ai)\n\nwhere Ai third equation is from the condition π−i\n\n(s, ai) = E\n\na−i∼π−i\n\nπold\n\nold\n\nnew = π−i old.\n\n[Aπold (s, ai, a−i)] is the individual advantage of agent i, and the\n\nWith the result of TRPO, we have the following conclusion:\n\nJ(πnew) − J(πold) ≥ Lπold(πnew) − CDmax new) − CDmax = Lπi\n\nKL (πnew||πold) new||πi KL (πi old)\n\n(πi\n\nold\n\n(from π−i\n\nnew = π−i\n\nold)\n\nThis means the individual objective is the same as the joint objective so the monotonic improvement is guaranteed.\n\nThen we can show the proof of Proposition 1.\n\nProof. We will build a new MDP ̃M based on the original MDP. We keep the action space ̃A = i=1Ai, where Ai is the original action space of agent i. The new state space contains multiple A = ×n layers. We define ̃Sk = S × (×k i=1Ai) for k = 1, 2, · · · , n − 1 and ̃S0 = S, where S is the original state space. Then a new state ̃sk ∈ ̃Sk means that ̃sk = (s, a1, a2, · · · , ak). The total new state space is defined as ̃S = ∪n−1\n\n ̃Si. Next we define the transition probability ̃P as following:\n\ni=0\n\n ̃P ( ̃s(cid:48)| ̃sk, ak+1, a−(k+1)) = 1 (cid:0) ̃s(cid:48) = ( ̃sk, ak+1)(cid:1) , k < n − 1 (cid:16)\n\n ̃P ( ̃s(cid:48)| ̃sk, ak+1, a−(k+1)) = 1\n\n ̃s(cid:48) ∈ ̃S0(cid:17)\n\nP ( ̃s(cid:48)| ̃sk, ak+1), k = n − 1.\n\nThis means that the state in the layer k can only transition to the state in the layer k + 1 with the corresponding action, and the state in the layer n − 1 will transition to the layer 0 with the probability P in the original MDP. The reward function ̃r is defined as following:\n\n ̃r( ̃s, a) = 1\n\n(cid:16)\n\n ̃s ∈ ̃S0\n\n(cid:17)\n\nr( ̃s, a).\n\nThis means the reward is only obtained when the state in layer 0 and the value is the same as the original reward function. Now we obtain the total definition of the new MDP ̃M = { ̃S, ̃A, ̃P , ̃r, γ}.\n\nThen we claim that if all agents learn in multi-agent sequential decision-making by PPO, they are actually taking agent-by-agent PPO in the new MDP ̃M . To be precise, one update of multi-agent\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nsequential decision-making in the original MDP M equals to a round of update from agent 1 to agent n by agent-by-agent PPO in the new MDP ̃M . Moreover, the total reward of a round in the new MDP ̃M is the same as the reward in one timestep in the original MDP M . With this conclusion and Lemma 1, we complete the proof.\n\nThe proof of Proposition 2 can be seen as a corollary of the proof of Proposition 1.\n\nProof. From Lemma 1 we know that the monotonic improvement of the joint policy in the new MDP ̃M is guaranteed for each update of one single agent’s policy. So even if the different round of updates in the new MDP ̃M is with different order of the decision-making, the monotonic improvement of the joint policy is still guaranteed. Finally, from the proof of Proposition 1, we know that the monotonic improvement in the new MDP ̃M equals to the monotonic improvement in the original MDP M . These complete the proof.\n\nB PROOFS OF THEOREM 1\n\nLemma 2 (TVD of the joint distributions). Suppose we have two distribution p1(x, y) = p1(x)p1(x|y) and p2(x, y) = p2(x)p2(x|y). We can bound the total variation distance of the joint as:\n\nDT V (p1(x, y)||p2(x, y)) ≤ DT V (p1(x)||p2(x)) + max\n\nx\n\nDT V (p1(y|x)||p2(y|x))\n\nProof. See (Janner et al., 2019) (Lemma B.1).\n\nLemma 3 (Markov chain TVD bound, time-varing). Suppose the expected KL-divergence between two transition is bounded as maxt Es∼p1,t(s)DKL(p1(s(cid:48)|s)||p2(s(cid:48)|s)) ≤ δ, and the initial state distributions are the same p1,t=0(s) = p2,t=0(s). Then the distance in the state marginal is bounded as:\n\nDT V (p1,t(s)||p2,t(s)) ≤ tδ\n\nProof. See (Janner et al., 2019) (Lemma B.2).\n\nLemma 4 (Branched Returns Bound). Suppose the expected KL-divergence between two dynamics distributions is bounded as maxt Es∼p1,t(s)[DT V (p1(s(cid:48)|s, a)||p2(s(cid:48)|s, a))], and the policy divergences at level k are bounded as maxs,a1:k−1 DT V (π1(ak|s, a1:k−1)||π2(ak|s, a1:k−1)) ≤ (cid:15)πk . Then the returns are bounded as:\n\n|η1 − η2| ≤\n\n2rmaxγ((cid:15)m + (cid:80)n (1 − γ)2\n\nk=1 (cid:15)πk )\n\n+\n\n2rmax\n\n(cid:80)n\n\nk=1 (cid:15)πk\n\n1 − γ\n\n,\n\nwhere rmax is the upper bound of the reward function.\n\nProof. Here, η1 denotes the returns of π1 under dynamics p1(s(cid:48)|s, a), and η2 denotes the returns of π2 under dynamics p2(s(cid:48)|s, a). Then we have\n\n|η1 − η2| = |\n\n= |\n\n(cid:88)\n\ns,a (cid:88)\n\n(p1(s, a) − p2(s, a))r(s, a)|\n\n(cid:88)\n\nγt(p1,t(s, a) − p2,t(s, a))r(s, a)|\n\nt (cid:88)\n\nt\n\n≤\n\n≤ rmax\n\ns,a (cid:88)\n\ns,a (cid:88)\n\nγt|p1,t(s, a) − p2,t(s, a)|r(s, a)\n\n(cid:88)\n\nγt|p1,t(s, a) − p2,t(s, a)|.\n\nt\n\ns,a\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nBy Lemma 2, we get\n\nmax s\n\nDT V (π1(a|s)||π2(a|s)) ≤ max s,a1\n\nDT V (π1(a−1|s, a1)||π2(a−1|s, a1))\n\n+ max\n\nDT V (π1(a1|s)||π2(a1|s))\n\ns ≤ · · · n\n(cid:88)\n\n≤\n\nk=1 n\n(cid:88)\n\nk=1\n\n≤\n\nDT V (π1(ak|s, a1:k−1)||π2(ak|s, a1:k−1))\n\nmax s,a1:k−1\n\n(cid:15)πk .\n\nWe then apply Lemma 3, using δ = (cid:15)m + (cid:80)n\n\nk=1 (cid:15)πk (via Lemma 3 and 2) to get\n\nDT V (p1,t(s)||p2,t(s)) ≤ t max\n\nt\n\nEs∼p1,t(s)DT V (p1,t(s(cid:48)|s)||p2,t(s(cid:48)|s))\n\n≤ t max\n\nt\n\nEs∼p1,t(s)DT V (p1,t(s(cid:48), a|s)||p2,t(s(cid:48), a|s))\n\n≤ t(max\n\nt + max\n\nEs∼p1,t(s)DT V (p1,t(s(cid:48)|s, a)||p2,t(s(cid:48)|s, a))\n\nEs∼p1,t(s) max\n\ns\n\nDT V (π1,t(a|s)||π2,t(a|s)))\n\nt\n\n≤ t((cid:15)m +\n\nn (cid:88)\n\nk=1\n\n(cid:15)πk )\n\nAnd we also get DT V (p1,t(s, a)||p2,t(s, a)) ≤ t((cid:15)m + (cid:80)n by plugging this back, we get:\n\nk=1 (cid:15)πk ) + (cid:80)n\n\nk=1 (cid:15)πk by Lemma 2. Thus,\n\n|η1 − η2| ≤ rmax\n\n(cid:88)\n\n(cid:88)\n\nt\n\ns,a\n\nγt|p1,t(s, a) − p2,t(s, a)|\n\n≤ 2rmax\n\n(cid:88)\n\nt\n\nγt(t((cid:15)m +\n\nn (cid:88)\n\nk=1\n\n(cid:15)πk ) +\n\nn (cid:88)\n\nk=1\n\n(cid:15)πk )\n\n≤ 2rmax(\n\nγ((cid:15)m + (cid:80)n\n\nk=1 (cid:15)πk ))\n\n(1 − γ)2\n\n+\n\n(cid:80)n\n\nk=1 (cid:15)πk 1 − γ\n\n)\n\nThen we can show the proof of Theorem 1.\n\nProof. Let πβ denote the data collecting policy. We use Lemma 4 to bound the returns, but it will require bounded model error under the new policy π. Thus, we need to introduce πβ by adding and subtracting η[πβ], to get:\n\nˆη[π] − η[π] = ˆη[π] − η[πβ] + η[πβ] − η[π].\n\nwe can bound L1 and L2 both using Lemma 4 by using δ = (cid:80)n respectively, and obtain:\n\nk=1 (cid:15)πk and δ = (cid:15)m + (cid:80)n\n\nk=1 (cid:15)πk\n\nL1 ≥ −\n\n2γrmax\n\n(cid:80)n\n\nk=1 (cid:15)πk\n\n(1 − γ)2\n\n−\n\n2rmax\n\n(cid:80)n\n\nk=1 (cid:15)πk\n\n(1 − γ)\n\nL2 ≥ −\n\n2γrmax((cid:15)πm + (cid:80)n\n\nk=1 (cid:15)πk )\n\n(1 − γ)2\n\n−\n\n2rmax\n\n(cid:80)n\n\nk=1 (cid:15)πk\n\n(1 − γ)\n\n.\n\nAdding these two bounds together yields the conclusion.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Illustration of learned priority of decision making in PP (upper panel) and CN (lower panel). Preys (landmarks) are viewed in black and predators (agents) are viewed in grey in PP (CN). From a to e, shown is the priority order. The smaller the level index, the higher priority of decision-making is.\n\nTable 1: Mean reward in different tasks, averaged over timesteps, with 200 test trials.\n\nFix-C\n\nSeqComm\n\n3-agent in CN −0.83 ±0.17 −0.76 ±0.08 7-agent in CN −1.79 ±0.15 −1.57 ±0.10 7-agent in PP −1.89 ±0.45 −1.31 ±0.60\n\nC ADDITIONAL EXPERIMENTS\n\nC.1\n\nILLUSTRATION OF LEARNED PRIORITY OF DECISION-MAKING\n\nFigure 8 (upper panel from a to e) shows the priority order of decision-making determined by SeqComm in PP. Agent 2 that is far away from other preys and predators is chosen to be the firstmover. If agents want to encircle and capture the preys, the agents (e.g., agent 2 and 5) that are on the periphery of the encircling circle should hold upper-level positions since they are able to decide how to narrow the encirclement. In addition, agent 3 makes decisions prior to agent 5 so that collision can be avoided after agent 5 obtains the intention of agent 3.\n\nFor CN, as illustrated in Figure 8 (lower panel from a to e), agent 2 is far away from all the landmarks and all other agents are in a better position to occupy landmarks. Therefore, agents 2 is chosen to be the first-mover, which is similar to the phenomenon observed in PP. Once it has determined the target to occupy, other agents (agent 5 and 3) can adjust their actions accordingly and avoid conflict of goals. Otherwise, if agent 5 makes a decision first and chooses to occupy the closest landmark, then agent 2 has to approach to a further landmark which would take more steps.\n\nC.2 GENERALIZATION\n\nGeneralization to different numbers of agents has always been a key problem in MARL. For most algorithms in communication, once the model is trained in one scenario, it is unlikely for agents to maintain relatively competitive performance in other scenarios with different numbers of agents. However, as we employ attention modules to process communicated messages so that agents can handle messages of different lengths. In addition, the module used to determine the priority of decision-making is also not restricted by the number of agents. Thus, we investigate whether SeqComm generalizes well to different numbers of agents in CN and PP.\n\nFor both tasks, SeqComm is trained on 5-agent settings. Then, we test SeqComm in 3-agent and 7-agent settings of CN and 7-agent setting of PP. We use Fix-C trained directly on these test tasks to illustrate the performance of SeqComm. Note that the quantity of both landmarks and preys is\n\n16\n\npp 1level 1agent 2agent 3agent 5level 1level 2level 1level 2level 3level 1level 2level 3level 4level 1level 2level 3level 4level 5abcdelevel 1level 1level 1agent 2agent 5agent 3level 1level 2level 1level 2level 3level 1level 2level 3level 4level 1level 2level 3level 4level 5cnabcdeUnder review as a conference paper at ICLR 2023\n\n(a) 3s vs 4z\n\n(b) corridor\n\nFigure 9: Learning curves in terms of the win rate of SeqComm and baselines on two customized SMAC maps: (a) 3s vs 4z, (b) corridor.\n\nadjusted according to the number of agents in CN and PP. The test results are shown in Table 1. SeqComm exhibits the superiority in CN and PP, demonstrating that SeqComm may have a good generalization to the number of agents. A thorough study of the generalization of SeqComm is left to future work.\n\nC.3 MORE SMAC MAPS\n\nWe have evaluated our method on two additional maps, i.e., 3s vs 4z and corridor. As illustrated in Figure 9, we can find out the similar conclusions as section 5.1.\n\nD ADDITIONAL RELATED WORK\n\nMulti-Agent Path Finding (MAPF). MAPF aims to plan collision-free paths for multiple agents on a given graph from their given start vertices to target vertices. In MAPF, prioritized planning is deeply coupled with collision avoidance (Van Den Berg & Overmars, 2005; Ma et al., 2019), where collision is used to design constraints or heuristics for planning. Unlike MAPF, our method couples the priority of decision-making with the learning objective and thus is more general. In addition, the different motivations and problem settings may lead to the incompatibility of the methods in the two fields.\n\nReinforcement Learning in Stackelberg Game. Many studies (K ̈on ̈onen, 2004; Sodomka et al., 2013; Greenwald et al., 2003; Zhang et al., 2020) have investigated reinforcement learning in finding the Stackelberg equilibrium. Bi-AC (Zhang et al., 2020) is a bi-level actor-critic method that allows agents to have different knowledge bases so that the Stackelberg equilibrium (SE) is possible to find. The actions still can be executed simultaneously and distributedly. It empirically studies the relationship between the cooperation level and the superiority of the SE over the Nash equilibrium. AQL (K ̈on ̈onen, 2004) updates the Q-value by solving the SE in each iteration and can be regarded as the value-based version of Bi-AC. Existing work mainly focuses on two-agent settings and their order is fixed in advance. However, the fixed order can hardly be an optimal solution as we will show in the next section. To address this issue, we exploit agents’ intentions to dynamically determine the priority of decision-making along the way of interacting with each other.\n\nE EXPERIMENTAL SETTINGS\n\nIn cooperative navigation, there are 5 agents and the size of each is 0.15. They need to occupy 5 landmarks with the size of 0.05. The acceleration of agents is 7. In predator-prey, the number of predators (agents) and prey is set to 5 and 3, respectively, and their sizes are 0.15 and 0.05. The acceleration is 5 for predators and 7 for prey. In keep away, the number of attackers (agents) and defenders is set to 3, and their sizes are respectively 0.15 and 0.05. Besides, the acceleration is 6\n\n17\n\n0.00.20.40.60.81.0step1e70.00.20.40.60.81.0win rateSeqCommTarMACRandom-C0.00.20.40.60.81.0step1e70.00.20.40.60.81.0win rateSeqCommTarMACRandom-CUnder review as a conference paper at ICLR 2023\n\nTable 2: Hyperparameters for predator-prey, cooperative navigation, keep-away\n\nHyperparameter\n\nSeqComm Random-C Fix-C TarMAC\n\nI2C\n\nIS\n\ndiscount (γ) batch size buffer capacity number of processes learning rate H\nF\n\n– –\n\n– –\n\n0.95,0.95,0.95 –\n–\n\n– –\n\n16,16,16 1.5e−5, 1e−5, 4e−5\n\n800\n\n1e6\n\n–\n\n1024\n\n–\n\n1e−2, 1e−3, 1e−3 1e−2\n\n10,10,20\n\n–\n\n2, 2, 1\n\n– –\n\n– –\n\n– –\n\n– –\n\ni=1 dt\n\nfor attackers and 4 for defenders. The three landmarks are located at (0.00, 0.30), (0.25, −0.15), and (−0.25, −0.15). Note that each agent is allowed to communicate with all other agents in all three tasks. The team reward is similar across tasks. At a timestep t, it can be written as rt team = − (cid:80)n i is the distance of landmark/prey i to its nearest agent/predator, C t is the number of collisions (when the distance between two agents is less than the sum of their sizes) occurred at timestep t, and rcollision = −1. In addition, agents act discretely and have 5 actions (stay and move up, down, left, right). The length of each episode is 20, 30, and 20 in cooperative navigation, predator-prey, and keep-away, respectively.\n\ni + C trcollision, where dt\n\nF IMPLEMENTATION DETAILS\n\nF.1 ARCHITECTURE AND HYPERPARAMETERS\n\nOur models, including SeqComm, Fix-C, and Random-C are trained based on MAPPO. The critic and policy network are realized by two fully connected layers. As for the attention module, key, query, and value have one fully connected layer each. The size of hidden layers is 100. Tanh functions are used as nonlinearity. For I2C, we use their official code with default settings of basic hyperparameters and networks. As there is no released code of IS and TarMAC, we implement IS and TarMAC by ourselves, following the instructions mentioned in the original papers (Kim et al., 2021; Das et al., 2019).\n\nFor the world model, observations and actions are firstly encoded by a fully connected layer. The output size for the observation encoder is 48, and the output size for the action encoder is 16. Then the outputs of the encoder will be passed into the attention module with the same structure aforementioned. Finally, we use a fully connected layer to decode. In these layers, Tanh is used as the nonlinearity.\n\nTable 2 summarize the hyperparameters used by SeqComm and the baselines in the MPE.\n\nFor SMAC, SeqComm, Random-C, Fix-C are based on the same architecture, the hyperparameters stay the same. For MMM2, 6z vs 8z, and 8m vs 9m, the learning rate is 5e−5, while for 10m vs 11m, corridor, and 3s vs 4z, learning rate is 7e−5. The ppo epoch is set to 10 for 6h vs 8z, and is 5 for rest maps. H and F is set to 5 and 1, respectively. However, 20 and 2 is a better value of H and F if computing resources is sufficient.\n\nFor TarMAC, the learning rate is 7e−5 for all maps. The ppo epoch is set to 10 for 6h vs 8z, and is 5 for rest maps.\n\nFor MAPPO, the learning rate is 5e−5 for MMM2 and 6z vs 8z, and 7e−5 for 8m vs 9m and 10m vs 11m.\n\nFor these four methods, the mini batch is set to 1. As for other hyperparameters, we follow the default settings of the official code (Yu et al., 2021).\n\nFor QMIX, the learning rate is 5e−5. The (cid:15) is 1 and the batch size is 32. The buffer size is 5e3. For others, we follow the default settings of link https://github.com/starry-sky6688/MARL-Algorithms.git\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nF.2 ATTENTION MODULE\n\nAttention module (AM) is applied to process messages in the world model, critic network, and policy network. AM consists of three components: query, key, and values. The output of AM is the weighted sum of values, where the weight of value is determined by the dot product of the query and the corresponding key.\n\nFor AM in the world model denoted as AMw, agent i gets messages m−i agents at timestep t in negotiation phase, and predicts a query vector qi query is used to compute a dot product with keys kt = [k1 message from agent j following AMi scaled by 1/\n\nt ) for j (cid:54)= i, and ki dk followed by a softmax to obtain attention weights α for each value vector:\n\nfrom all other w,q(hi t). The t is obtained by the t). Besides, it is\n\nt = h−i t following AMi\n\nt ]. Note that kj\n\nt is from AMi\n\nt , · · · , kn\n\nneg,k(hi\n\na,k(hj\n\n√\n\nt\n\nαi = softmax\n\n\n\n \n \n\nT k1 qi t√ t\ndk\n\n· · ·\n\nT kj qi t√ t\ndk (cid:124) (cid:123)(cid:122) (cid:125) αij\n\n· · ·\n\nT kn qi t√ t\ndk\n\n\n\n \n \n\n(1)\n\nt = (cid:80)n The output of attention module is defined as: ci or its own hidden state of observation following AMi\n\nj=1 αijvj w,v(·).\n\nt , where vj\n\nt is obtained from messages\n\nAs for AM in the policy and critic network denoted as AMa , agent i gets additional messages from upper-level agent in the launching phase. The message from upper-level and lower-level agent can be expanded as mupper = [hlower ] and mlower , 0], respectively. In addition, the t\nquery depends on agent’s own hidden state of observation hi t, but keys and values are only from messages of other agents.\n\n= [hupper\n\n, aupper\n\nt\n\nt\n\nt\n\nt\n\nF.3 TRAINING\n\nThe training of SeqComm is an extension of MAPPO. The observation encoder e, the critic V , and the policy π are respectively parameterized by θe, θv, θπ. Besides, the attention module AMa is parameterized by θa and takes as input the agent’s hidden state, the messages (hidden states of other agents) in the negotiation phase, and the messages (the actions of upper-level agents) in launching phase. Let D = {τk}K k=1 be a set of trajectories by running policy in the environment. Note that we drop time t in the following notations for simplicity.\n\nThe value function is fitted by regression on mean-squared error:\n\nL(θv, θa, θe) =\n\n1 KT\n\n(cid:88)\n\nT −1 (cid:88)\n\nτ ∈D\n\nt=0\n\n(cid:13) (cid:13)\n\n(cid:13)V (AMa(e(o), aupper)) − ˆR\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n(2)\n\nwhere ˆR is the discount rewards-to-go.\n\nWe update the policy by maximizing the PPO-Clip objective:\n\nL(θπ, θa, θe) =\n\n1 KT\n\n(cid:88)\n\nT −1 (cid:88)\n\nτ ∈D\n\nt=0\n\nmin(\n\nπ(a|AMa(e(o), aupper)) πold(a|AMa(e(o), aupper))\n\nAπold, g((cid:15), Aπold))\n\n(3)\n\nwhere g((cid:15), A) =\n\n(cid:26)(1 + (cid:15))A A ≥ 0 (1 − (cid:15))A A ≤ 0\n\n, and Aπold(o, aupper, a) is computed using the GAE method.\n\nThe world model M is parameterized by θw is trained as a regression model using the training data set S. It is updated with the loss:\n\nL(θw) =\n\n1 |S|\n\n(cid:88)\n\no,a,o(cid:48),r∈S\n\n(cid:13) (cid:13) 2\n(cid:13)(o(cid:48), r) − M(AMw(e(o), a)) (cid:13) (cid:13) (cid:13) 2\n\n.\n\n(4)\n\nWe trained our model on one GeForce GTX 1050 Ti and Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz.\n\n19",
    "reference": "# Summary Of The Paper\n\n## Summary\nThe paper presents SeqComm, a multi-agent communication scheme allowing agents to condition on one another's actions by imposing ordering over the agents. The paper introduces multi-agent sequential decision and demonstrates that ordering in this paradigm can affect the optimality of the learnt policy. \n\nThe authors then present SeqComm. Each agent in SeqComm learns a policy which conditions on the joint hidden state and other agents' actions via an attention mechanism. The ordering is chosen by weighing the value of each agent's intention, which is the paper defines as the agent's future behaviour without considering the action of others. In the second phase of communication, the agents the produce a joint action. The authors then prove that monotonic improvement of the policy is independent of priority, and provide a bound on the performance loss associated with using a world model for the ordering.\n\nThe authors then evaluate SeqComm on MPE and SMAC tasks.\n\n# Strength And Weaknesses\n\n## Strengths\n- The setting of adding some communication to CTDE is an interesting way to alleviate miscoordination and work in this area is welcome. \n- The paper is quite clearly written.\n- The authors include ablation studies to evaluate why their method works.\n- The matrix game in Figure 1 nicely illustrates why an order is theoretically useful. \n\n## Weaknesses\n\n- I'm not sure that I understand the setting. If agents can broadcast functions of their observations to all other agents, then how is that different from joint learning, where all agents can view the same joint observation? This seems to not be the CTDE setting at all to me, but instead joint learning. I understand that TarMAC [1] adopts a similar setting, but I would appreciate some clarification from the authors on how this differs from joint learning.\n- The communication-free baselines MAPPO and IPPO are not a fair comparison, and it is not clear that SeqComm's performance is better TarMAC or the random-priority ablation. The authors claim that their method clearly outperforms the ablations and TarMAC, but the gap is only slight and seems to be mostly within a standard deviation. \n- The authors do not compare with a centralised method such as PPO (but conditioning the policy on the joint observation and outputting the joint action). This seems strange given this is an obvious alternative to the method, and would require approximately the same communication.\n- The empirical evaluation is only over 4 SMAC maps, and no further results are included. This does not seem enough to provide convincing evidence of outperformance.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n## Clarity\n\nThe paper is fairly clearly written. I do not believe that the authors open-source their code, which makes reproducibility difficult. It is also not clear to me which implementation of the QMIX and MAPPO baselines is used or how hyperparameters were chosen for the baselines.\n\n# Summary Of The Review\n\nAlthough some communication is no doubt useful in MARL, requiring that functions of the observation can be broadcast to all allies does not seem to be decentralised execution to me. Additionally, I remain unconvinced by the empirical results both in their breadth and performance difference.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nFROM PLAY TO POLICY: CONDITIONAL BEHAVIOR GENERATION FROM UNCURATED ROBOT DATA\n\nZichen Jeff Cui∗\n\nYibin Wang\n\nNur Muhammad (Mahi) Shafiullah\n\nLerrel Pinto\n\nNew York University\n\nABSTRACT\n\nWhile large-scale sequence modeling from offline data has led to impressive performance gains in natural language and image generation, directly translating such ideas to robotics has been challenging. One critical reason for this is that uncurated robot demonstration data, i.e. play data, collected from non-expert human demonstrators are often noisy, diverse, and distributionally multi-modal. This makes extracting useful, task-centric behaviors from such data a difficult generative modeling problem. In this work, we present Conditional Behavior Transformers (C-BeT), a method that combines the multi-modal generation ability of Behavior Transformer with future-conditioned goal specification. On a suite of simulated benchmark tasks, we find that C-BeT improves upon prior state-of-the-art work in learning from play data by an average of 45.7%. Further, we demonstrate for the first time that useful task-centric behaviors can be learned on a real-world robot purely from play data without any task labels or reward information. Robot videos are best viewed on our project website: play-to-policy.github.io.\n\n1\n\nINTRODUCTION\n\nMachine Learning is undergoing a Cambrian explosion in large generative models for applications across vision (Ramesh et al., 2022) and language (Brown et al., 2020). A shared property across these models is that they are trained on large and uncurated data, often scraped from the internet. Interestingly, although these models are trained without explicit task-specific labels in a self-supervised manner, they demonstrate a preternatural ability to generalize by simply conditioning the model on desirable outputs (e.g. “prompts” in text or image generation). Yet, the success of conditional generation from uncurated data has remained elusive for decision making problems, particularly in robotic behavior generation.\n\nTo address this gap in behavior generation, several works (Lynch et al., 2019; Pertsch et al., 2020b) have studied the use of generative models on play data. Here, play data is a form of offline, uncurated data that comes from either humans or a set of expert policies interacting with the environment. However, once trained, many of these generative models require significant amounts of additional online training with task-specific rewards (Gupta et al., 2019; Singh et al., 2020). In order to obtain task-specific policies without online training, a new line of approaches employ offline RL to learn goal-conditioned policies (Levine et al., 2020; Ma et al., 2022). These methods often require rewards or reward functions to accompany the data, either specified during data collection or inferred through hand-crafted distance metrics, for compatibility with RL training. Unfortunately, for many real-world applications, data does not readily come with rewards. This prompts the question: how do we learn conditional models for behavior generation from reward-free, play data?\n\nTable 1: Comparison between existing algorithms to learn from large, uncurated datasets: GCBC (Lynch et al., 2019), GCSL (Ghosh et al., 2019), Offline GCRL (Ma et al., 2022), Decision Transformer Chen et al. (2021)\n\nGCBC GCSL Offline RL Decision Transformer C-BeT (ours)\n\nReward-free Offline Multi-modal\n\n✓ ✓\n✗\n\n✓ ✗\n✗\n\n✗ ✓\n✗\n\n✗ ✓\n✗\n\n✓ ✓\n✓\n\n∗Corresponding author, email: jeff.cui@nyu.edu\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Multiple conditioned roll-outs of visual robot policies learned on our toy kitchen with only 4.5 hours of human play interactions. Our model learns purely from image and proprioception without human labeling or data curation. During evaluation, the policy can be conditioned either on a goal observation or a demonstration. Note that the last three rows contain distractor objects in the environment that were never seen during training.\n\nTo answer this question, we turn towards transformer-based generative models that are commonplace in text generation. Here, given a prompt, models like GPT-3 (Brown et al., 2020) can generate text that coherently follow or satisfy the prompt. However, directly applying such models to behavior generation requires overcoming two significant challenges. First, unlike the discrete tokens used in text generation, behavior generation will need models that can output continuous actions while also modeling any multi-modality present in the underlying data. Second, unlike textual prompts that serve as conditioning for text generation, behavior generation may not have the condition and the operand be part of the same token set, and may instead require conditioning on future outcomes.\n\nIn this work, we present Conditional Behavior Transformers (C-BeT), a new model for learning conditional behaviors from offline data. To produce a distribution over continuous actions instead of discrete tokens, C-BeT augments standard text generation transformers with the action discretization introduced in Behavior Transformers (BeT) (Shafiullah et al., 2022). Conditioning in C-BeT is done by specifying desired future states as input similar to Play-Goal Conditioned Behavior Cloning (Play-GCBC) (Lynch et al., 2019). By combining these two ideas, C-BeT is able to leverage the multi-modal generation capabilities of transformer models with the future conditioning capabilities of conditional policy learning. Importantly, C-BeT does not require any online environment interactions during training, nor the specification of rewards or Q functions needed in offline RL.\n\n2\n\nInitializationGrasping ovenPartially openingRetractingAttempt to reopenMissed graspGoal: Open oven Result: FailureInitializationGrasping potMoving potWedging pot in sinkMoved potRetractingGoal: Move pot to sink Result: SuccessInitializationGrasping microwaveMissed graspRe-opening microwaveGrasping OvenOpening OvenInitializationGrasping r. knobTurning r. knobRetractingGrasping l. knobTurning l. knobGoal: Turn right knob & left knob Result: SuccessGoal: Open microwave & oven Result: SuccessGrasping microwaveOpening microwaveClosing microwaveGrasping PotTransporting PotPlacing PotGoal: Open microwave & move pot Result: FailureInitializationTurning left knobRetractingGrasping OvenOpening OvenRetractingGoal: Turn left knob & open oven Result: SuccessPublished as a conference paper at ICLR 2023\n\nWe experimentally evaluate C-BeT on three simulated benchmarks (visual self-driving in CARLA (Dosovitskiy et al., 2017), multi-modal block pushing (Florence et al., 2021), and simulated kitchen (Gupta et al., 2019)), and on a real Franka robot trained with play data collected by human volunteers. The main findings from these experiments can be summarized as:\n\n1. On future-conditioned tasks, C-BeT achieves significantly higher performance compared to\n\nprior work in learning from play.\n\n2. C-BeT demonstrates that competent visual policies for real-world tasks can be learned from\n\nfully offline multi-modal play data (rollouts visualized in Figure 1).\n\n2 BACKGROUND AND PRELIMINARIES\n\nPlay-like data: Learning from Demonstrations (Argall et al., 2009) is one of the earliest frameworks explored for behavior learning algorithms from offline data. Typically, the datasets used in these frameworks have a built in assumption that the demonstrations are collected from an expert repeatedly demonstrating a single task in exactly the same way. On the contrary, play datasets violate many of such assumptions, like those of expertise of the demonstrator, and the unimodality of the task and the demonstrations. Algorithms that learn from such datasets sometimes assume that the demonstrations collected are from a rational agent with possibly some latent intent in their behavior (Lynch et al., 2019). Note that, unlike standard offline-RL datasets (Fu et al., 2020), play-like behavior datasets neither contain fully random behaviors, nor have rewards associated with the demonstrations.\n\nBehavior Transformers (BeT): BeT (Shafiullah et al., 2022) is a multi-modal behavior cloning model designed particularly for tackling play-like behavior datasets. BeT uses a GPT-like transformer architecture to model the probability distribution of action given a sequence of states π(at | st−h:t) from a given dataset. However, unlike previous behavior learning algorithms, BeT does not assume a unimodal prior for the action distribution. Instead, it uses a k-means discretization to bin the actions from the demonstration set into k bins, and then uses the bins to decompose each action into a discrete and continuous component. This support for multi-modal action distributions make BeT particularly suited for multi-modal, play-like behavior datasets where unimodal behavior cloning algorithms fail. However, vanilla BeT only supports unconditonal behavior rollouts, which means that it is not possible to choose a targeted mode of behavior during BeT policy execution.\n\nConditional behavior learning: Generally, the problem of behavior learning for an agent is considered the task of learning a policy π : O → A mapping from the environment observations to the agent’s actions that elicit some desired behavior. Conditional behavior learning is concerned with learning a policy π : O × G → A conditioned additionally on a secondary variable g sampled from a distribution p(g). This condition variable could be specific environment states, latents (such as one-hot vectors), or even image observations. The success of a conditioned policy can be evaluated either through pre-specified reward functions, distance function between achieved outcome g′ and specified outcome g, or by discounted visitation probability dπ(·|g) = Eτ ∼π[(cid:80)∞ t=0 γtδ(φ(ot) = g)] if a mapping φ between states and achieved outcome is defined (Eysenbach et al., 2022).\n\n(cid:81)\n\nGoal Conditioned Behavior Cloning (GCBC): In GCBC (Lynch et al., 2019; Emmons et al., 2021), the agent is presented with a dataset of (observation, action, goal) tuples (o, a, g), or sequences of such tuples, and the objective of the agent is to learn a goal-conditioned behavior policy. The simplest way to achieve so is by training a policy π(· | o, g) that maximizes the probability of the seen P[a ∼ π(· | o, g)]. Assuming a unimodal Gaussian distribution for data π∗ = arg maxπ π(a | o, g) and a model parametrized by θ, this comes down to finding the parameter θ minimizing (o,a,g) ||a − π(o, g; θ)||2. To make GCBC compatible with play data the MSE loss, θ∗ = arg minθ that inherently does not have goal labels, goal relabeling from future states is often necessary. A common form of data augmentation in training such models, useful when G ⊂ O, is hindsight data relabeling (Andrychowicz et al., 2017), where the dataset {(o, a, g)} is augmented with {(ot, a, ot′) | t′ > t} by relabeling any reached state in a future timestep as a goal state and adding it to the dataset.\n\n(o,a,g)\n\n(cid:80)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3 APPROACH\n\nGiven a dataset {(o, a)} ∈ O × A of sequences of (observation, action) pairs from a play dataset, our goal is to learn a behavior generation model that is capable of handling multiple tasks and multiple ways of accomplishing each task. At the same time, we wish to be able to extract desired behavior from the dataset in the form of a policy through our model, or, in terms of generative models, “controllably generate” our desired behavior (see Figure 2). Finally, in the process of learning this controllable, conditional generative model, we wish to minimize the amount of additional human annotation or curation required in preparing the dataset. The method we develop to address these needs is called Conditional Behavior Transformer.\n\nFigure 2: Conditional behavior learning from play demonstrations. Here, a policy conditioned on reaching 1⃝ or 2⃝ has only one possible course of action, but conditioned on reaching 3⃝ there are two reasonable paths.\n\n3.1 CONDITIONAL BEHAVIOR TRANSFORMERS (C-BET)\n\nConditional task formulation: First, we formulate the task of learning from a play dataset as learning a conditional behavior policy, i.e. given the current state, we need to model the distribution of actions that can lead to particular future states. For simplicity, our formulation can be expressed as π : O × O → D(A) where, given a current observation oc and a future observation og, our policy π models the distribution of the possible actions that can take the agent from oc to og. Mathematically, given a set of play trajectories T , we model the distribution π(a | oc, og) ≜ Pτ ∈T (a | oc = τt, og = τt′, t′ > t). Next, to make our policy more robust since we operate in the partially observable setting, we replace singular observations with a sequence of observations; namely replacing oc and og with ̄oc = o(1:N ) for some integer N . Thus, the final task formulation becomes learning a generative model π with: (cid:16)\n\nand ̄og = o(1:N )\n\n(cid:16)\n\n(cid:17)\n\n(cid:17)\n\ng\n\nc\n\nπ\n\na | o(1:N )\n\n, o(1:N )\n\ng\n\nc\n\n≜ Pτ ∈T\n\na | o(1:N )\n\nc\n\n= τt:t+N , o(1:N )\n\ng\n\n= τt′:t′+N , t′ > t\n\n(1)\n\nArchitecture selection: Note that the model for our task described in the previous paragraph is necessarily multi-modal, since depending on the sequences ̄oc and ̄og, there could be multiple plausible sequences of actions with non-zero probability mass. As a result, we choose Behavior Transformers (BeT) (Shafiullah et al., 2022) as our generative architecture base as it can learn action generation with multiple modes. We modify the input to the BeT to be a concatenation of our future conditional observation sequence and current observation sequence. We choose to concatenate the inputs instead of stacking them, as this allows us to independently choose sequence lengths for the current and future conditional observations. Since BeT is a sequence-to-sequence model, we only consider the actions associated with the current observations as our actions. We show the detailed architecture of our model in Figure 3.\n\nDataset preparation: To train a C-BeT model on our play dataset {(o, a)}, we will need to appropriately prepare the dataset. We first convert the dataset to hold sequences of observations associated with actions, {(ot:t+N , at:t+N )}. Then, during training time, we dynamically augment each pair with a sequence of future observations, functionally converting our dataset into {(ot:t+N , at:t+N , ot′:t′+N ′)} for some t′ > t, and treat the sequence ot′:t′+N ′ as ̄og.\n\nTraining objective: We employ the same objective as BeT in training C-BeT. For each of the current observation and future conditional pair, we compute the BeT loss (see appendix B for details) between the ground truth actions and the predicted actions. We compute the focal loss (Lin et al., 2017) on the predicted action bins, and the MT-loss (Girshick, 2015) on the predicted action offsets corresponding to the action bins as described in BeT.\n\nTest-time conditioning with C-BeT: During test time, we again concatenate our future conditional sequence with our current observations, and sample actions from our model according to the BeT framework. While in this work, we primarily condition C-BeT on future observations, we also study\n\n4\n\n123Published as a conference paper at ICLR 2023\n\nFigure 3: End-to-end training and evaluation of C-BeT. (A) Our dataset consists of play data in an environment, which may contain semi-optimal behavior, multi-modal demonstrations, and failures, and does not contain any annotations or task labels. (B) We train our C-BeT model by conditioning on current and future states using BeT (Section 2) (C) During evaluation, our algorithm can be conditioned by target observations or newly collected demonstrations to generate targeted behavior.\n\nother ways of training and conditioning it, such as binary latent vectors denoting the modes in a trajectory in our experiments, and compare its performance to observation-conditioned C-BeT (see Section 4.5).\n\n4 C-BET ON SIMULATED BENCHMARKS\n\nIn this section, we discuss our experiments in simulation that are designed to answer the following key questions: How well does C-BeT learn behaviors from play? How important is multi-modal action modeling? And finally, how does C-BeT compare to other forms of conditioning?\n\n4.1 BASELINES\n\nWe compare with the following state-of-the-art methods in learning from reward-free offline data:\n\n• Goal Conditioned BC (GCBC): GCBC (Lynch et al., 2019; Emmons et al., 2021) learns a policy\n\nby optimizing the probability of seen actions given current and the end state in a trajectory.\n\n• Weighted Goal Conditioned Supervised Learning (WGCSL) (Yang et al., 2022): GCSL (Ghosh et al., 2019) is an online algorithm with multiple rounds of collecting online data, relabeling, and training a policy on that data using GCBC. WGCSL (Yang et al., 2022) improves GCSL by learning an additional value function used to weight the GCSL loss. We compare against an single-round, offline variant of WGCSL in this work.\n\n• Learning Motor Primitives from Play (Play-LMP): Play-LMP (Lynch et al., 2019) is a behavior generation algorithm that focuses on learning short (∼ 30 timesteps) motor primitives from play data. Play-LMP does so by using a variational-autoencoder (VAE) to encode action sequences into motor program latents and decoding actions from them.\n\n• Relay Imitation Learning (RIL): Relay Imitation Learning (Gupta et al., 2019) is a hierarchical imitation learning with a high level controller that generates short term target state given long term goals, and a low level controller that generates action given short term target.\n\n• Conditional Implicit Behavioral Cloning (C-IBC): Implicit behavioral cloning (Florence et al., 2021) learns an energy based model (EBM) E(a | o) over demos and during test samples action a given an observation o. We compare against a conditional IBC by training an EBM E(a | o, g).\n\n• Generalization Through Imitation (GTI): GTI (Mandlekar et al., 2020) encodes the goal condition using a CVAE, and autoregressively rolls out action sequences given observation and goal-latent. We follow their architecture and forgo collecting new trajectories with an intermediate model since that does not fit an offline framework.\n\n• Offline Goal-Conditioned RL: While offline RL is generally incompatible with play data without rewards, recently some offline goal-conditioned RL algorithms achieved success by optimizing for\n\n5\n\n(A)Datasetoc:c+hog:g+h′ (B)TrainingBehavior TransformerCurrent obsFuture obsCAT(),D(̂ac:c+h)Estimated action distributionac:c+hGround truth actionsBeT loss (Focal + MT loss)(C)EvaluationTarget frameorTarget demonstrationoc−h:cog:g+h′ ObservationsConditionalCAT(),Behavior TransformerD(̂ac−h:c)Action distribution∼acSampled actionPublished as a conference paper at ICLR 2023\n\nTable 2: Results of future-conditioned algorithms on a set of simulated environments. The numbers reported for CARLA, BlockPush, and Kitchen are out of 1, 1, and 4 respectively, following Shafiullah et al. (2022). In CARLA, success counts as reaching the location corresponding to the observation; for BlockPush, it is pushing one or both blocks into the target squares; and for Kitchen, success corresponds to the number of conditioned tasks, out of four, completed successfully.\n\nGCBC WGCSL Play-LMP RIL C-IBC GTI GoFAR BeT\n\nC-BeT (unimodal)\n\nC-BeT (multimodal)\n\nCARLA BlockPush Kitchen\n\n0.04 0.06 0.74\n\n0.02 0.10 1.17\n\n0.0 0.02 0.04\n\n0.59 0.07 0.39\n\n0.65 0.01 0.13\n\n0.74 0.04 1.61\n\n0.72 0.04 1.24\n\n0.31 0.34 1.77\n\n0.62 0.35 2.74\n\n0.98 0.90 2.80\n\na proxy reward defined through state occupancy. Our baseline, GoFAR (Ma et al., 2022), is one such algorithm that learns a goal-conditioned value function and optimizes a policy to maximize it.\n\n• Behavior Transformers (BeT): We include unconditional BeT (Sec. 2) in our baseline to understand the improvements made by the C-BeT conditioning. In practice, it acts as a “random” baseline that performs the tasks without regard for the goal.\n\n• Unimodal C-BeT: We use our method without the multi-modal head introduced in BeT. This also corresponds to a variant of Decision Transformer conditioning on outcomes instead of rewards.\n\nNote that neither WGCSL nor GoFAR are directly compatible with image states and goals, since they require a proxy reward function r : S × G → R. Thus, we had to design a proxy reward function on the image representations, exp (−(1/4||g − s||)2) to apply them on image-based environments. For a fair comparison, we also upgrade baseline Play-LMP, C-IBC, and GTI architectures by giving them sequences of observations and retrofitting them with transformers whenever applicable.\n\n4.2 SIMULATED ENVIRONMENTS AND DATASETS\n\nWe run our algorithms and baselines on a collection of simulated environments as a benchmark to select the best algorithms to run on our real robotic setup. The simulated environments are selected to cover a variety of properties that are necessary for the real world environment, such as pixel-based observations, diverse modes in the play dataset, and complex action spaces (see Figure. 4).\n\n1. CARLA self-driving: CARLA (Dosovitskiy et al., 2017) is a simulated self-driving environment created using Unreal Engine. In this environment, the observations are RGB pixel values of dimension (224, 224, 3), and actions are two-dimensional (accelerate/brake and steer). We use an environment with a fork in the road (see Figure 2) following two possible routes to the same goal, collecting 200 demonstrations in total. We condition on one of the two possible routes to the goal, and at the goal where choosing either of the two modes is valid.\n\n2. Multi-modal block-pushing: We use the multi-modal block-pushing environment from Florence et al. (2021) for complicated multi-modal demonstrations. In this environment, an xArm robot pushes two blocks, red and green, into two square targets colored red and green. All positions are randomized with some noise at episode start. We use 1,000 demonstrations collected using a deterministic controller, and condition on just the future block positions on each baseline.\n\n3. Franka relay kitchen: Originally introduced in Gupta et al. (2019), Relay Kitchen is a robotic environment in a simulated kitchen with seven possible tasks. A Franka Panda robot is used to manipulate the kitchen, and the associated dataset comes with 566 demonstrations collected by humans with VR controllers performing four of the seven tasks in some sequence.\n\n4.3 HOW WELL DOES C-BET LEARN BEHAVIORS FROM PLAY?\n\nOn each of these environments, we train conditional behavior generation models and evaluate them on a set of conditions sampled from the dataset. The success is defined by the model performing the same tasks as conditioned by the future outcome. We see from Table. 2 that C-BeT performs significantly better compared to the baselines on all three tasks. BeT, as our unconditioned “random” baseline, shows the success rate of completing tasks unconditionally, and see that none of the baselines surpasses it consistently. Out of the MLP-based baselines, WGCSL performs best in the state-based tasks. However, GoFAR performs best on the CARLA vision based environment where the other two MLP-based baselines fail almost completely. We note that Play-LMP performs poorly because our tasks are long-horizon and quite far from its intended motor primitive regime, which may be challenging for Play-LMP’s short-horizon auto-encoding architecture.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n4.4 HOW IMPORTANT IS MULTI-MODAL ACTION MODELING?\n\nWhile we use a multi-modal behavior model in this work, it is not immediately obvious that it may be necessary. Specifically, some previous outcome-conditioned policy learning works (Chen et al., 2021; Emmons et al., 2021) implicitly assume that policies are unimodal once conditioned on an outcome. In Table 2 the comparison between C-BeT and unimodal C-BeT shows that this assumption may not be true for all environments, and all else being equal, having an explicitly multi-modal model helps learning an outcome conditioned policy when there may be multiple ways to achieve an outcome.\n\n4.5 HOW DOES C-BET COMPARE TO OTHER FORMS OF CONDITIONING?\n\nWe consider the question of how much comparative advantage there is in getting human labels for our tasks. We do so by adding manual one-hot (CARLA, BlockPush) or binary (Kitchen) labels to our tasks, and training and evaluating C-BeT with those labels. As we see on Table 3, on the three simulated environments, C-BeT conditioned on only future observations performs comparably to conditioning with human labels.\n\nTable 3: Comparison between C-BeT with no supervised labels and labels acquired with human supervision.\n\nNo labels Labels\n\nCARLA BlockPush Kitchen\n\n0.98 0.90 2.80\n\n1.0 0.89 2.75\n\n5 C-BET ON REAL-WORLD ROBOTIC MANIPULATION\n\nWe now discuss our robot experiments, which are geared towards understanding the usefulness of C-BeT on real-world play data.\n\n5.1 ROBOTIC ENVIRONMENT AND DATASET\n\nRobot setup: Our environment consists of a Franka Emika Panda robot, similar to the simulated Franka Kitchen environment, set up with a children’s toy kitchen set (see Figure 1). The toy kitchen has an oven, a microwave, a pot, and two stove knobs that are relevant to our play dataset. The action space in this environment contains the seven joint angle deltas normalized within the [−1, 1] range, and a binary gripper control.\n\nPlay dataset: We collected 460 sequences totaling to 265 minutes (about 4.5 hours) of play data on the toy kitchen with volunteers using a Vive VR controller to move the Franka. While collecting the play data, we did not give the volunteers any explicit instructions about doing any particular tasks, or number of tasks, beyond specifying the interactable items, and stipulating that the pot only goes on the left stove or in the sink, to prevent dropping the pot and reaching an unresettable state. As the observations, we save the RGB observations from two cameras on the left and right of the setup, as well as the robot’s proprioceptive joint angles. Overall, the dataset contains 45 287 frames of play interactions and their associated actions.\n\nRepresentation learning To simplify the task of learning policies on image space, we decouple the task of image representation learning from policy learning following Pari et al. (2021). For each camera, we first fine-tune a pretrained ResNet-18 (He et al., 2016) encoder on the acquired frames with BYOL self-supervision (Grill et al., 2020). Then, during policy learning and evaluation, instead of the image from the cameras, we pass the two 512-dimensional BYOL embeddings as part of the observation. For the proprioceptive part of the observation, we repeat the (sin, cos) of seven joint states 74 times to get a 1036-dimensional proprioceptive representation, making our overall observation representation 2060-dimensional.\n\n5.2 CONDITIONAL BEHAVIOR GENERATION ON REAL ROBOT\n\nBehavior generation on single tasks: Our first experiment in the real robot is about extracting single-task policies from the play dataset. We define our tasks as manipulating the four types of interactable objects one at a time: opening the oven door, opening the microwave door, moving the pot from the stove to the sink, and rotating a knob 90 degrees to the right. We use appropriate conditioning frames from our observation dataset, and start the robot from the neutral state to complete the four tasks. The result of this experiment is presented in Table 4. We see that on single task conditionals,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Single-task success rate in a real world kitchen with conditional models. We present the success rate and number of trials on each task, with cumulative results presented on the last column.\n\nKnobs Oven Microwave\n\nPot\n\nCumulative\n\nGoFAR Unconditional BeT Unimodal C-BeT Multimodal C-BeT\n\n0/10 5/20 1/20 3/20\n\n0/5 6/10 8/10 9/10\n\n0/5 1/10 4/10 7/10\n\n0/5 0/10 0/10 5/10\n\n0/25 12/50 13/50 24/50\n\nTable 5: Task success rate in a real world kitchen with conditional models evaluated on a long-horizon goal. We present the success rate and number of trials on each task, with cumulative result presented on the last column.\n\nOven → Pot Microwave → Oven\n\nPot → Microwave Avg. Tasks/Run\n\nUnconditional BeT Unimodal C-BeT Multimodal C-BeT\n\n(6, 0)/10 (1, 1)/10 (5, 4)/10\n\n(1, 6)/10 (2, 0)/10 (8, 8)/10\n\n(0, 1)/10 (8, 0)/10 (4, 4)/10\n\n0.47 0.37 1.1\n\nC-BeT is able to complete all tasks except the knobs consistently, outperforming all our baselines, showing that C-BeT is able to extract single-task policies out of uncurated, real-world play data. We discuss failures of C-BeT on the knob tasks in Section 5.3. While our GoFAR baseline was able to move towards the task targets, it was unable to successfully grasp or interact with any of the target objects. We believe it may be the case because unlike the robot experiment in Ma et al. (2022), we do not have the underlying environment state, the tasks are much more complicated, and our dataset is an order of magnitude smaller (400 K vs 45 K).\n\nBehavior generation for longer horizons: Next, we ask how well our models work for longerhorizon conditioning with multiple tasks. We choose play sequences from the dataset with multiple tasks completed and use their associated states as the conditions for our models. In our roll-outs, we calculate how many tasks completed in the original sequence were also completed in the conditional roll-outs. We calculate this metric over 3 conditioning sequences, and report the results in Table 5. We see that even without any high level controller, C-BeT is able to stitch together multiple tasks from play demonstrations to complete long-horizon goals.\n\nGeneralization to prompt and environment perturbations: A major requirement from any robot system deployed in the real world is to generalize to novel scenarios. We evaluate the generalizability of our learned policies in two different ways. In the first set of experiments, we collect fresh demonstrations that were not in the training set, and we condition our policies on such trajectories. We find that across the different tasks, even with unseen conditionings, C-BeT retains 67% of the single-task performance, with 16/50 task successes in total. In the second set of experiments, we add environmental distractors in the setup (Figure 1, bottom three rows) and run the single- and multi-task conditions on the modified environments. We see once again that the performance drops to around 67% of original with two distractors on the scene, but if we keep adding (four or more) distractors, the robot is unable to complete any tasks.\n\n5.3 ANALYSIS OF FAILURE MODES\n\nWe see a few failure modes in our experiments that may provide additional insights into learning from real-world play data. We discuss the most salient ones in this section.\n\nFailure in knob operation in the real world: We see that in all of our real world experiments, the accuracy in operating the knob is consistently lower than all other tasks. This is due to the failure of the learned representations. Upon inspection of the dataset images’ nearest neighbors in the representation space, we see that the BYOL-trained representation cannot identify the knob state better than random chance: the returned nearest neighbor differs in knob status often. Since the representation cannot identify the knob status properly, conditioning on it naturally fails.\n\nImportance of a multi-modal policy architecture: One of our motivations behind incorporating the BeT architecture in our work is its ability to learn multi-modal action distributions. In our experiments, we show that for some single-task conditions such as opening the oven door, having no multi-modality is sufficient (Table 4), but for more complicated tasks and learning from a more interconnected form of play data, it is always the case that a multi-modal architecture prevents our policies from collapsing to sub-optimal solutions (Table 5).\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n6 RELATED WORK\n\nOutcome-conditioned behavior learning: Behavior learning conditioned on particular outcomes, such as reward or goals, is a long studied problem (Kaelbling, 1993; Schaul et al., 2015; Veeriah et al., 2018; Zhao et al., 2019). Compared to standard behavior learning, learning conditioned behavior can generally be more demanding since the same model can be expected to learn a multitude of behaviors depending on the outcome, which can make learning long-term behavior harder (Levy et al., 2017; Nachum et al., 2018). As a result, a common line of work in outcome-conditioned learning is to use some form of relabeling of demonstrations or experience buffer as a form of data augmentation (Kaelbling, 1993; Andrychowicz et al., 2017; Ghosh et al., 2019; Goyal et al., 2022) similar to what we do in the paper. As opposed to goal or state conditioned learning, which we focus on in this paper, recently reward conditioned learning using a transformer (Chen et al., 2021) was introduced. However, later work found that it may not work as expected in all environments (Paster et al., 2022; Brandfonbrener et al., 2022) and large transformer models may not be necessary (Emmons et al., 2021) for reward conditioned learning. In this work, we find that using transformers is crucial, particularly when dealing with high dimensional visual observation and multi-modal actions.\n\nLearning from play data: Our work is most closely related to previous works such as Lynch et al. (2019); Gupta et al. (2019), which also focus on learning from play demonstrations that may not be strictly optimal and uniformly curated for a single task. Learning policies capable of multiple tasks from play data allows knowledge sharing, which is why it may be more efficient compared to learning from demonstrations directly (Zhang et al., 2018; Rahmatizadeh et al., 2018; Duan et al., 2017; Pari et al., 2021; Young et al., 2021). Gupta et al. (2022) attempts reset-free learning with play data, but requires human annotation and instrumentation in the environment for goal labels.\n\nGenerative modeling of behavior: Our method of learning a generative model for behavior learning follows a long line of work, including Inverse Reinforcement Learning or IRL (Russell, 1998; Ng et al., 2000; Ho & Ermon, 2016), where given expert demonstrations, a model tries to construct the reward function, which is then used to generate desirable behavior. Another class of algorithms learn a generative action decoder (Pertsch et al., 2020a; Singh et al., 2020) from interaction data to make downstream reinforcement learning faster and easier, nominally making multi-modal action distribution easier. Finally, a class of algorithms, most notably Liu et al. (2020); Florence et al. (2021); Kostrikov et al. (2021); Nachum & Yang (2021) do not directly learn a generative model, but instead learn energy based models that need to be sampled to generate behavior, although they do not primarily focus on goal-conditioning.\n\nTransformers for behavior learning: Our work follows earlier notable works in using transformers to learn a behavior model from an offline dataset, such as Chen et al. (2021); Janner et al. (2021); Shafiullah et al. (2022). Our work is most closely related to Shafiullah et al. (2022) as we build on their transformer architecture, while our unimodal baseline is a variant of Chen et al. (2021) that learns outcome conditioned instead of reward conditioned policy. Beyond these, Dasari & Gupta (2020); Mandi et al. (2021) summarizes historical visual context using transformers, and Clever et al. (2021) relies on the long-term extrapolation abilities of transformers as sequence models. The goal of C-BeT is orthogonal to these use cases, but can be combined with them for future applications.\n\n7 DISCUSSION AND LIMITATIONS\n\nIn this work, we have presented C-BeT, a new approach for conditional behavior generation that can learn from offline play data. Across a variety of benchmarks, both simulated and real, we find that C-BeT significantly improves upon prior state-of-the-art work. However, we have noticed two limitations in C-BeT, particularly for real-robot behavior learning. First, if the features provided to C-BeT do not appropriately capture relevant objects in the scene, the robot execution often fails to interact with that object in its environment. Second, some tasks, like opening the oven door, have simpler underlying data that is not multimodal, which renders only meager gains with C-BeT. A more detailed analysis of these limitations are presented in Section 5.3. We believe that future work in visual representation learning can address poor environment features, while the collection of even larger play datasets will provide more realistic offline data for large-scale behavior learning models.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENT\n\nWe thank Sridhar Arunachalam, David Brandfonbrener, Irmak Guzey, Yixin Lin, Jyo Pari, Abitha Thankaraj, and Austin Wang for their valuable feedback and discussions. This work was supported by awards from Honda, Meta, Hyundai, Amazon, and ONR awards N00014-21-1-2758 and N0001422-1-2773.\n\nREFERENCES\n\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017. 3, 9\n\nBrenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning\n\nfrom demonstration. Robotics and autonomous systems, 57(5):469–483, 2009. 3\n\nDavid Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning work for offline reinforcement learning? arXiv preprint arXiv: Arxiv-2206.01079, 2022. 9\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 1, 2\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021. 1, 7, 9\n\nHenry M Clever, Ankur Handa, Hammad Mazhar, Kevin Parker, Omer Shapira, Qian Wan, Yashraj Narang, Iretiayo Akinola, Maya Cakmak, and Dieter Fox. Assistive tele-op: Leveraging transformers to collect robotic task demonstrations. arXiv preprint arXiv:2112.05129, 2021. 9\n\nSudeep Dasari and Abhinav Gupta. Transformers for one-shot visual imitation. arXiv preprint\n\narXiv:2011.05970, 2020. 9\n\nAlexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An\n\nopen urban driving simulator. In Conference on robot learning, pp. 1–16. PMLR, 2017. 3, 6\n\nYan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. Advances in neural information processing systems, 30, 2017. 9\n\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for\n\noffline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021. 3, 5, 7, 9\n\nBenjamin Eysenbach, Tianjun Zhang, Ruslan Salakhutdinov, and Sergey Levine. Contrastive learning as goal-conditioned reinforcement learning. arXiv preprint arXiv: Arxiv-2206.07568, 2022. 3\n\nPete Florence, Corey Lynch, Andy Zeng, Oscar Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. arXiv preprint arXiv:2109.00137, 2021. 3, 5, 6, 9\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\n\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. 3\n\nDibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. arXiv preprint arXiv:1912.06088, 2019. 1, 5, 9\n\nRoss Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision,\n\npp. 1440–1448, 2015. 4, 13\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAnirudh Goyal, Abram Friesen, Andrea Banino, Theophane Weber, Nan Rosemary Ke, Adria Puigdomenech Badia, Arthur Guez, Mehdi Mirza, Peter C Humphreys, Ksenia Konyushova, et al. Retrieval-augmented reinforcement learning. In International Conference on Machine Learning, pp. 7740–7765. PMLR, 2022. 9\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020. 7\n\nAbhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956, 2019. 1, 3, 5, 6, 9\n\nAbhishek Gupta, Corey Lynch, Brandon Kinman, Garrett Peake, Sergey Levine, and Karol Hausman. Bootstrapped autonomous practicing via multi-task reinforcement learning. arXiv preprint arXiv:2203.15755, 2022. 9\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. 7\n\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural\n\ninformation processing systems, volume 29, pp. 4565–4573, 2016. 9\n\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\n\nmodeling problem. In Advances in Neural Information Processing Systems, 2021. 9\n\nLeslie Pack Kaelbling. Learning to achieve goals. In IN PROC. OF IJCAI-93, pp. 1094–1098.\n\nMorgan Kaufmann, 1993. 9\n\nIlya Kostrikov, Jonathan Tompson, Rob Fergus, and Ofir Nachum. Offline reinforcement learning\n\nwith fisher divergence critic regularization. arXiv preprint arXiv:2103.08050, 2021. 9\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,\n\nreview, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. 1\n\nAndrew Levy, George Konidaris, Robert Platt, and Kate Saenko. Learning multi-level hierarchies\n\nwith hindsight. arXiv preprint arXiv:1712.00948, 2017. 9\n\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980–2988, 2017. 4, 13\n\nMinghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang. Energy-based imitation learning. arXiv\n\npreprint arXiv:2004.09395, 33, 2020. 9\n\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, S. Levine, and Pierre\n\nSermanet. Learning latent plans from play. Corl, 2019. 1, 2, 3, 5, 9\n\nYecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How far i’ll go: Offline goalconditioned reinforcement learning via f -advantage regression. arXiv preprint arXiv:2206.03023, 2022. 1, 6, 8\n\nZhao Mandi, Fangchen Liu, Kimin Lee, and Pieter Abbeel. Towards more generalizable one-shot\n\nvisual imitation learning. arXiv preprint arXiv:2110.13423, 2021. 9\n\nAjay Mandlekar, Danfei Xu, Roberto Martín-Martín, Silvio Savarese, and Li Fei-Fei. Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations. arXiv e-prints, art. arXiv:2003.06085, March 2020. 5\n\nOfir Nachum and Mengjiao Yang. Provable representation learning for imitation with contrastive\n\nfourier features. arXiv preprint arXiv:2105.12272, 2021. 9\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nOfir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical\n\nreinforcement learning. Advances in neural information processing systems, 31, 2018. 9\n\nAndrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml,\n\nvolume 1, pp. 663–670, 2000. 9\n\nJyothish Pari, Nur Muhammad, Sridhar Pandian Arunachalam, and Lerrel Pinto. The surprising effectiveness of representation learning for visual imitation. arXiv preprint arXiv:2112.01511, 2021. 7, 9\n\nKeiran Paster, Sheila McIlraith, and Jimmy Ba. You can’t count on luck: Why decision transformers\n\nfail in stochastic environments. arXiv preprint arXiv: Arxiv-2205.15967, 2022. 9\n\nKarl Pertsch, Youngwoon Lee, and Joseph J Lim. Accelerating reinforcement learning with learned\n\nskill priors. arXiv preprint arXiv:2010.11944, 2020a. 9\n\nKarl Pertsch, Youngwoon Lee, and Joseph J Lim. Accelerating reinforcement learning with learned\n\nskill priors. arXiv preprint arXiv:2010.11944, 2020b. 1\n\nRouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau Bölöni, and Sergey Levine. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. In 2018 IEEE international conference on robotics and automation (ICRA), pp. 3758–3765. IEEE, 2018. 9\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1\n\nStuart Russell. Learning agents for uncertain environments. In Proceedings of the eleventh annual\n\nconference on Computational learning theory, pp. 101–103, 1998. 9\n\nTom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1312–1320, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ schaul15.html. 9\n\nNur Muhammad Mahi Shafiullah, Zichen Jeff Cui, Ariuntuya Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning k modes with one stone. arXiv preprint arXiv:2206.11251, 2022. 2, 3, 4, 6, 9, 13, 14\n\nAvi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Parrot: Data-driven behavioral priors for reinforcement learning. arXiv preprint arXiv:2011.10024, 2020. 1, 9\n\nVivek Veeriah, Junhyuk Oh, and Satinder Singh. Many-goals reinforcement learning. arXiv preprint\n\narXiv:1806.09605, 2018. 9\n\nRui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie Zhang. Rethinking goal-conditioned supervised learning and its connection to offline rl. Iclr, 2022. 5\n\nSarah Young, Jyothish Pari, Pieter Abbeel, and Lerrel Pinto. Playful interactions for representation\n\nlearning. arXiv preprint arXiv: Arxiv-2107.09046, 2021. 9\n\nTianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee, Xi Chen, Ken Goldberg, and Pieter Abbeel. Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 5628–5635. IEEE, 2018. 9\n\nRui Zhao, Xudong Sun, and Volker Tresp. Maximum entropy-regularized multi-goal reinforcement learning. In International Conference on Machine Learning, pp. 7553–7562. PMLR, 2019. 9\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA SIMULATED ENVIRONMENT VISUALIZATIONS\n\nFigure 4: Visualizations of simulated environments that we evaluate our methods on, from left to right: CARLA self-driving (top down view and agent POV), BlockPush, and Franka Kitchen.\n\nB BEHAVIOR TRANSFORMERS\n\nWe use Behavior Transformers from Shafiullah et al. (2022) as our backbone architecture, building our conditional algorithm on top of it. In this section, we describe the BeT architecture and the training objective to help the readers understand the details of our algorithm.\n\nB.1 BET ARCHITECTURE\n\nBeT uses a repurposed MinGPT architecture to model multi-modal behavior. It uses the MinGPT trunk as a sequence-to-sequence model that tries to predict a sequence of actions at:t+h given a sequence of states or observations ot:t+h. Beyond just prediction the actions, however, BeT tries to model the multi-modal action distribtuion given the observations. To create a multi-modal model over the continuous action distribution, BeT uses an action encoder-decoder architecture that can encode each action vector into a discrete latent and a smaller-norm continuous offset. BeT does so by using an offline action dataset to create a k-means model of the actions. Then, an action is encoded into its associated bin out of the k-bins (binning), and a small continuous offset from the associated bin.\n\nThe BeT model, given a sequence of observations ot:t+h, predicts a k-dimensional multinomial distribution over the k-bins, as well as a k × |A| dimensional matrix for offsets associated with each action bins. Sampling from the BeT action distribution is done via sampling a discrete bin first, taking its associated action offset, and then adding the bin center with the action offset.\n\nB.2 BET TRAINING OBJECTIVE\n\nGiven an observation o and its associated ground truth action a, we will now present the simplified version of how the BeT loss is calculated. Let us assume the BeT model prediction is π(o)d ∈ Rk, π(o)c ∈ Rk×|A| for the discrete and the continuous parts of the predictions. Let us also assume that ⌊a⌋ is the discrete bin out of the k bins that a belongs to, and ⟨a⟩ = a − BinCenter(⌊a⌋). Then, the BeT loss becomes\n\nLBeT = Lf ocal(π(o)d, ⌊a⌋) + λ · LM T (⟨a⟩, π(o)c)\n\nWhere Lf ocal is the Focal loss (Lin et al., 2017), a special case of the negative log likelihood loss defined as\n\nLf ocal(pt) = −(1 − pt)γ log(pt)\n\nand LM T is the multi-task loss (Girshick, 2015) defined as\n\n(cid:18)\n\nMT-Loss\n\na,\n\n(cid:16)\n\n(cid:17)k\n\n⟨ˆa(j) i ⟩\n\nj=1\n\nI[⌊a⌋ = j] · ∥⟨a⟩ − ⟨ˆa(j)⟩∥2\n\n2\n\n(cid:19)\n\n=\n\nk (cid:88)\n\nj=1\n\n13\n\nCARLA self drivingBlockPushFranka KitchenPublished as a conference paper at ICLR 2023\n\nC IMPLEMENTATION DETAILS\n\nC.1\n\nIMPLEMENTATION USED\n\nIn our work, we base our C-BeT implementation off of the official repo published at https: //github.com/notmahi/bet. For the GCBC, WGCSL, and GoFAR baselines, we use the official repo released by the GoFAR authors https://github.com/JasonMa2016/GoFAR/.\n\nC.2 HYPERPARAMETERS LIST:\n\nWe present the C-BeT hyperparameters in Table 6 below, which were mostly using the default hyperparameters in the original Shafiullah et al. (2022) paper:\n\nTable 6: Environment-dependent hyperparameters in BeT.\n\nHyperparameter Layers Attention heads Embedding width Dropout probability Context size Training epochs Batch size Number of bins k Future conditional frames\n\nThe shared hyperparameters are in Table 7.\n\nCARLA Block-push Kitchen 4\n4 72 0.1 5\n350 64 24 3\n\n3 4\n256 0.6 10 40 128 32 10\n\n6 6\n120 0.1 10 50 64 64 10\n\nTable 7: Shared hyperparameters for BeT training\n\nName Optimizer Learning rate Weight decay Betas Gradient clip norm\n\nValue Adam 1e-4 0.1 (0.9, 0.95) 1.0\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nD ROBOT ENVIRONMENT DEMONSTRATION TRAJECTORIES\n\nFigure 5: Sample demonstration trajectories for the real kitchen environment.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nE SIMULATED ENVIRONMENT ROLLOUT TRAJECTORIES\n\nFigure 6: Sample demonstration trajectories for the CARLA self driving environment, conditioning on going to the right path.\n\nFigure 7: Sample demonstration trajectories for the multi-modal block pushing environment, conditioning on pushing the green block to green square and red block to red square.\n\nFigure 8: Sample demonstration trajectories for the Franka Kitchen environment, conditioning on completing the microwave, bottom knob, slide cabinet, hinge cabinet tasks.\n\n16\n\nGoFARWGCSLGCBCC-BeTGoFARWGCSLGCBCC-BeTGoFARWGCSLGCBCC-BeT",
    "reference": "# Summary Of The Paper\n\nThis work presents a straightforward, but impressive extension of prior work on “Behavior Transformers” for learning policies from offline, reward-free data, by introducing the ability to learn goal-conditioned policies from undirected play data. Crucially, the play data used to learn policies in this work in inherently multimodal (and in some cases suboptimal); yet, using the goal conditioned behavior transformer (C-BeT) approach, we’re able to overcome such problems — *without any online exploration*.\n\nThe proposed approach builds on a history of prior work in goal-conditioned behavior cloning from visual observations, and makes a simple tweak to the underlying Behavior Transformer (think a causal transformer that eats states, and outputs actions) architecture, by additionally conditioning on target goal observations (images) of where we’d like the environment to end up. This is a powerful objective, and across three complex simulated environments (CARLA self-driving, multimodal block pushing, and the Franka Relay Kitchen environment) — each with varying levels of multimodality/suboptimality, C-BeT shows itself to be extremely strong.\n\nHowever, the truly impressive part of this work is the real-robot evaluation; from just 4.5 hours of real user “play” on a Franka robot, C-BeT is able to learn 5 complex manipulation tasks in a “toy kitchen” to a meaningful level of competency — far better than other approaches from the offline RL literature like GO-FAR. In general, I found the evaluation to be thorough, ablations comprehensive, and comparisons to prior art meaningful!\n\n# Strength And Weaknesses\n\nI think this is a very strong paper, showing that existing Behavior Transformers can scale to multimodal, reward free play data. In general, I really believe in the results in this paper, and am excited for the possibilities of using such an approach for general offline policy learning on real (and simulated) robotic tasks.\n\nThe weaknesses of this work are generally just weaknesses around the feasibility of visual goal-conditioning in the wild. In the real world, it’s not clear whether (given a new task, or a combinatorial explosion of objects/states) we’ll be able to provide robots with meaningful “goal images” that capture what we want to happen; for example, across all the tasks in this work, only a handful of object positions change from start state to goal state; in heavily cluttered environments with dynamic objects, it feels like such an approach would really suffer (in sample efficiency and maybe in just general applicability). This is somewhat shown in the videos, where we add random objects (though that’s also just because the environment is so out of distribution).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI found this paper incredibly clear and easy to read; the quality and thoroughness of the experiments (and attention to baselines) is commendable, and I really buy these results. I like that the authors chose to evaluate on open-source simulated environments as well as a real-robot; it speaks a lot to the reproducibility of this work!\n\nMinor: Seems like Claim 2 in the introduction “C-BeT represents the first work to demonstrate that competent visual policies for real-world tasks can be learned from reward-free play data” isn’t quite right; Implicit BC learns from play-esque multimodal data without rewards, as does follow-up work like Implicit Kinematic Policies (but I could be wrong; might just be a wording thing around the distinction between “play” and “multimodal demos”).\n\n# Summary Of The Review\n\nI think this is a straightforward and impactful extension of prior work on behavior transformers. I think that being able to learn robust goal-conditioned policies from uncurated reward-free play data in a manner that is 1) simple and 2) completely offline is very meaningful, and a clear win for the field.\n\nI really like this work!\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSOCIAL NETWORK STRUCTURE SHAPES INNOVATION: EXPERIENCE SHARING IN RL WITH SAPIENS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nHuman culture relies on innovation: our ability to continuously explore how existing elements can be combined to create new ones. Innovation is not solitary, it relies on collective search and accumulation. Reinforcement learning (RL) approaches commonly assume that fully-connected groups are best suited for innovation. However, human laboratory and field studies have shown that hierarchical innovation is more robustly achieved by dynamic social network structures. In dynamic settings, humans oscillate between innovating individually or in small clusters, and then sharing outcomes with others. To our knowledge, the role of social network structure on innovation has not been systematically studied in RL. Here, we use a multi-level problem setting (WordCraft), with three different innovation tasks to test the hypothesis that the social network structure affects the performance of distributed RL algorithms. We systematically design networks of DQNs sharing experiences from their replay buffers in varying structures (fullyconnected, small world, dynamic, ring) and introduce a set of behavioral and mnemonic metrics that extend the classical reward-focused evaluation framework of RL. Comparing the level of innovation achieved by different social network structures across different tasks shows that, first, consistent with human findings, experience sharing within a dynamic structure achieves the highest level of innovation in tasks with a deceptive nature and large search spaces. Second, experience sharing is not as helpful when there is a single clear path to innovation. Third, the metrics we propose, can help understand the success of different social network structures on different tasks, with the diversity of experiences on an individual and group level lending crucial insights.\n\n1\n\nINTRODUCTION\n\nUnlike herds or swarms, human social networks solve different tasks with different topologies (Momennejad, 2022). Human and computational studies show that properties of both the social network structure and task affect the abilities of groups to search collectively: social network structures with high connectivity are better suited for quick convergence in problems with clear global optima (Coman et al., 2016; Momennejad et al., 2019), while partially-connected structures perform best in deceptive tasks, where acting greedily in the short-term leads to missing the optimal solution (Derex & Boyd, 2016; Lazer & Friedman, 2007; Cantor et al., 2021; Du et al., 2021; Adjodah et al., 2019). Despite this evidence, works in distributed reinforcement learning (RL) have focused on fully-connected architectures (Mnih et al., 2016; Horgan et al., 2018; Espeholt et al., 2018; Nair et al., 2015; Christianos et al., 2020; Schmitt et al., 2019; Jaderberg et al., 2018). Here, we test the performance of different social network structures in groups of RL agents that share their experiences in a distributed RL learning paradigm. We refer to such groups as multi-agent topologies, introduce SAPIENS, a learning framework for Structuring multi-Agent toPologies for Innovation through ExperieNce Sharing1, and evaluate it on a deceptive task that models collective innovation.\n\nInnovations represent the expansion of an agent’s behavioral repertoire with new problem-solving abilities and are, therefore, a necessary ingredient of continuous learning (Leibo et al., 2019). They arise from tinkering, recombination and adoption of existing innovations (Sol ́e et al., 2013; Derex & Boyd, 2016) and have been characterized as a type of combinatorial search constrained by semantics\n\n1We provide an implementation of SAPIENS and code to reproduce the simulations we report.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ndictating the feasible combinations of innovations (Sol ́e et al., 2013; Derex & Boyd, 2016). We adopt this definition: innovations are a type of collective search task with : a) a multi-level search space, where innovations arise out of recombination of existing ones (Hafner, 2021) b) rewards that increase monotonically with the level of innovation, in order to capture the human intrinsic motivation for progress (Sol ́e et al., 2013).\n\nLaboratory and field studies of human groups have shown that collective innovation is highly contingent on the social network structure (Momennejad, 2022; Migliano & Vinicius, 2022; Derex & Boyd, 2016). The reason for this lies in the exploration versus exploitation dynamics of social networks. High clustering and long shortest path in partially-connected structures help maintain diversity in the collective at the benefit of exploration, while high connectivity quickly leads to conformity, which benefits exploitation (Lazer & Friedman, 2007). Of particular interest are structures that achieve a balance in this trade-off: small-worlds are static graphs that, due a modular structure with long-range connections, achieve both high clustering and small shortest path (Watts & Strogatz, 1998). Another example are dynamic structures, where agents are able to periodically change neighbors (Volz & Meyers, 2007). These two families of graphs have the attractive property that they both locally protect innovations and quickly disseminate good solutions (Derex & Boyd, 2016).\n\nDespite progress on multiple fronts, many open questions remain before we get a clear understanding of how social network structure shapes innovation. On the cognitive science side, computational and human laboratory studies of collective innovation are few and have studied a single task where two innovations are combined to create a new one (Derex & Boyd, 2016; Cantor et al., 2021), while most works study other types of collective search that do not resemble innovation (Mason & Watts, 2012; Mason et al., 2008; Lazer & Friedman, 2007; Fang et al., 2010). Furthermore, laboratory studies have collected purely behavioral data (Mason et al., 2008; Derex & Boyd, 2016), while studies of collective memory have shown significant influence of social interactions on individual memory (Coman et al., 2016), indicating that mnemonic data may be another good source of information. In distributed RL, studies are hypothesizing that the reason why groups outperform single agents not just in terms of speed, but also in terms of performance, is the increased diversity of experiences collected by heterogeneous agents but not explicitly measure it (Nair et al., 2015; Horgan et al., 2018). In this case two steps seem natural: introducing appropriate metrics of diversity, and increasing it, not only through heterogeneity, but also through the social network topology.\n\nTo achieve this we propose SAPIENS, a distributed RL learning framework for modeling a group of agents exchanging experiences according to a social network topology. We study instantiations of SAPIENS where multiple DQN learners (Mnih et al., 2013) share experience tuples from their replay buffers with their neighbors in different static and dynamic social network structures and compare them to other distributed RL algorithms (Mnih et al., 2016; Nair et al., 2015). We employ Wordcraft (Jiang et al., 2020b) as a test-bed and design three custom tasks (Figures 1 and 2) covering innovation challenges of different complexity: (i) a task with a single innovation path to an easy-to-find global optimum. This type of task can be used to model a linear innovation structure, such as the evolution of the fork from knife to having, two-, three- and, eventually four tines. (Sol ́e et al., 2013) and is not a deceptive task. (ii) a task with two paths that individually lead to local optima, but when combined, can merge toward the global optimum. Ispired from previous studies in cognitive science (Derex & Boyd, 2016; Cantor et al., 2021) this task we can capture innovations that were repurposed after their invention, such as Gutenberg’s screw press leading to the print press (Sol ́e et al., 2013). (iii) a task with ten paths, only one of which leads to the global optimum, which captures search in vast spaces. In addition to the two deceptive tasks in Wordcraft, we also evaluate SAPIENS algorithms on a deceptive task implemented in a grid world. We empirically show that the performance of SAPIENS depends on the inter-play between social network structure and task demands. Dynamic structures perform most robustly, converging quickly in the easy task, avoiding local optima in the second task, and exploring efficiently in the third task. To interpret these findings, we propose and compute novel behavioral and mnemonic metrics that quantify, among others, the diversity of experiences.\n\nContributions Our contributions are two-fold. From a cognitive science perspective, SAPIENS is, to our knowledge, the first computational study of hypotheses in human studies relating social network structure to collective innovation, that employs deep RL as the individual learning mechanism. Compared to the simple agent-based models employed by previous computational studies (Lazer & Friedman, 2007; Cantor et al., 2021; Mason & Watts, 2012), deep RL offers three main advantages : i) it enables empirical experiments with more complex test-beds and larger search spaces\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: (Left) Illustration of an innovation task, consisting of an initial set of elements (Earth, Water) and a recipe book indicating which combinations create new elements. Upon creating a new element the player moves up an innovation level and receives a reward that increases monotonically with levels. (Right) Dynamic social network structures oscillate between phases of low connectivity, where experience sharing takes place within clusters, and high connectivity, where experiences spread between clusters.\n\nii) agents can share their experience by simply exchanging transitions from their respective replay buffers, without requiring ad-hoc mechanisms for copying the behaviors of other agents, such as the majority rule (Lazer & Friedman, 2007; Cantor et al., 2021) iii) by using the replay buffer as a proxy of the memories of agents, we can directly measure properties such as the diversity of experiences that are challenging to estimate with humans. Aside these methodological contributions, as we will see later, our empirical study leads to clear hypotheses for future experiments with humans. From an RL perspective, our work extends upon the distributed RL paradigm by systematically analyzing the effect of static and dynamic social network structures on different types of innovation tasks both in terms of performance and novel behavioral and mnemonic metrics.\n\n2 METHODS\n\n2.1 WORDCRAFT: A TEST-BED FOR INNOVATION\n\nWe perform experiments on Wordcraft (Jiang et al., 2020b), an RL environment inspired from the game Little Alchemy 2 2. As we illustrate on the left of Figure 1, tasks start with an initial set of elements and the player explores combinations of two elements in order to create new ones.\n\nIn Wordcraft one can create different types of tasks using a recipe book (Xvalid), which is a dictionary containing the valid element combinations of two elements and the newly crafted one. In addition to the recipe book, a task requires a reward function Rvalid that returns a scalar reward associated with crafting element z. A valid combination returns a new element and reward only the first time it is chosen. When queried with a non-valid combination, Rvalid returns a reward of zero. Thus, a task can be be described by a tuple (X0, Xvalid, Rvalid, T ), where X0 denotes the initial set of elements and T is the number of time steps available to an agent before the environment resets, and can be modelled as a fully-observable Markov Decision process (see Appendix A).\n\n2.2\n\nINNOVATION TASKS\n\nWe introduce the following concepts to characterize the structure of innovation tasks. An innovation task can contain one or more paths, which can potentially be connected to each other (as in the In our proposed tasks, an innovation path X is defined as a sequence of merging paths task). elements [X1, ..., Xn], where crafting an element Xi (i > 1) requires to combine the previously crafted element Xi−1 and a base element from the initial set. The first element X1 in the sequence requires combining elements from the initial set or from other paths. The innovation level of an element corresponds to the length of the path it belongs to, plus the sum of the path lengths required to craft its first item (0 if the first item is crafted from base elements). For example, the innovation level of A3 in the single path task is 3, while the innovation level of element C1 in the merging paths task is 5 (1 on C + 2 on A + 2 on B). Within an innovation task, the trajectory of an agent is defined\n\n2https://littlealchemy2.com/.\n\n3\n\nEarthWaterMud++==MudFireBricks+=BricksCementHouseLevel 1Level 2Level 3Reward 2Reward 5Reward 7Sparse connectivityHigh connectivitySparse connectivityHigh connectivityTimeUnder review as a conference paper at ICLR 2023\n\nFigure 2: We introduce three innovation tasks called single path, merging paths and best-of-ten paths, described in Section 2.2. Each task contains one or more paths, labeled by an uppercase letter (A to J). Each path X has its own initial set of three base elements {x1, x2, x3}, which are represented in dashed circles. Crafted elements in path X are represented in upper case (Xi) in solid circles. Optimal trajectories for each tasks are represented by solid red arrows, with their corresponding reward in bold red.\n\nas the sequence of crafted elements it produces. Finally, the optimal trajectory of an innovation task is the trajectory that returns the highest cumulative reward within the problem horizon T . We design three innovation tasks, shown in Figure 2, that pose different challenges:\n\nSingle innovation path The initial set contains three elements (Xvalid = {a1, a2, a3}) and . An agent needs to first combine two of them to create the first element and then progresses further by combining the most recently created element with an appropriate one from the initial set. This optimization problem contains a single global optimum.\n\nMerging paths There are two paths, A and B, and at level 2 there is a cross-road that presents the player with three options: moving forward to the end path A, moving forward to the end of path B, or combining elements from path A and B to progress on path C. The latter is more rewarding and is, thus, the optimal choice. This task is particularly challenging because the player needs to avoid two local optima before finding the global optimum.\n\nBest-of-ten paths Here, one of the ten paths is the most rewarding, but, to find it, the player must first explore and reject the other nine paths. This optimization task is characterized by a single global optimum and 9 local one and its challenge lies in its large search space.\n\n2.3 LEARNING FRAMEWORK\n\nSAPIENS considers a group of K DQN agents, where each agent interacts with its own copy of the environment and can share experiences with others. An undirected graph G, with nodes indicating agents and edges indicating that two agents share experiences with each other, determines who shares information with whom . We define the neighborhood of agent k, Nk, as the set of nodes connected to it. At the end of each episode the agent shares experiences with each of its neighbors with probability ps: sharing consists of sampling a subset of experiences of length Ls from its own buffer Bk and inserting it in the buffers of all neighbors Bn, n ∈ Nk. Thus, an agent communicates distinct experiences with each of its neighbors. We present a schematic of two DQN agents sharing\n\n4\n\nCombinationInnovation level 1238ResultReward......1238Single pathMerging pathsBest-of-ten paths...1238...58............814++==...1238......1238...281020...1238Path AMerged path CPath APath BPath BPath APath J......++++====a1A1A2A2A8A3A7A1...a2a3a1a3......++++====a1A1A2A2A8A3A7A1...a2a3a1a3......++++====a1A1A2B2C3C1C4A2A2A8A3A7A1...a2c3a3a1a3......++++====b1B1B2B2B8B3B7B1...b2b3b1b3......++++====b1B1B2B2B8B3B7B1...b2b3b1b3......++++====j1J1J2J2J8J3J7J1...j2j3j1j3xiith base element in path Xxiith crafted element in path XReuse of a crafted element in a non-optimal trajectoryReuse of a crafted element in an optimal trajectoryUnder review as a conference paper at ICLR 2023\n\nFigure 3: (Left) Social network structures (a) fully-connected (b) small-world (c) ring (d) dynamic. (Right) Schematic of two neighboring DQNs sharing experiences: agent 1 shares experiences from its own replay buffer to that of agent 2 (red arrow) and vice versa (blue arrow) while both agents are independently collecting experiences by interacting with their own copy of the environment.\n\nexperiences on the right of Figure 3 and the pseudocode of SAPIENS in Appendix C. We also provide more implementation details about the DQN in Appendix A.\n\nThus, SAPIENS is a distributed RL learning paradigm where all agents are both actors and learners, a setting distinct from multi-agent RL (Garnelo et al., 2021; Christianos et al., 2020; Jiang et al., 2020a), where agents co-exist in the same environment and from parallelised RL (Steinkraus et al., 2005), where there need to be multiple agents. It should also be distinguished from distributed RL paradigms with a single learner and multiple actors (Horgan et al., 2018; Espeholt et al., 2018; Nair et al., 2015; Garnelo et al., 2021), as multiple policies are learned simultaneously.\n\nWe visualize the social network structures studied in our work on the left of Figure 3: fullyconnected, small-world, ring and dynamic. We construct the latter by grouping the agents in pairs and then allowing agents to visit other groups for a pre-determined duration, Tv with probability pv. This is a common type of dynamic topology that has been used in human laboratory and field studies (Derex & Boyd, 2016; Migliano & Vinicius, 2022) and thoeretically studied (Volz & Meyers, 2007). We provide more information about it in Appendix D, where we study how its behavior changes with different values of its hyper-parameters Tv, pv. We also present and analyze an alternative type of dynamic topology where the group oscillates between phases of full and no connectivity.\n\n2.4 EVALUATION FRAMEWORK\n\nDuring evaluation trials with experience sharing deactivated, we measure the quality of the final solution and the convergence speed. In addition, we define metrics that are not directly related to performance but can help us analyze the effect of social network structure. These are behavioral metrics, characterizing the policies followed by the agents, and mnemonic metrics, characterizing the replay buffers of agents, either at an individual or group level.\n\nPerformance-based metrics: (i) S, group success, a binary variable denoting whether at least one agent in a group found the optimal solution (ii) R+ t : the maximum reward of the group at training step t; (iii) R∗ t : the average reward of the group at training step t; (iv) T +, Time to first success: the first training step at which at least one of the agents found the optimal solution; (v) T ∗, Time to all successes: the first training step at which all of the agents found the optimal solution (vi) T >, Spread time: number of training steps required for the optimal solution to spread to the whole group, once at least one member discovered it (equals T ∗ − T +).\n\nBehavioral metrics: (i) conformity Ct denotes the percentage of agents in a group that end up with the same element at the end of a given evaluation trial. Thus, agents conform to each other even if they follow alternative trajectories.; (ii) volatility Vt is an agent-level metric that denotes the cumulative number of changes in the trajectory followed by an agent across episodes.\n\nMnemonic metrics: (i) diversity Dk experiences in an agent’s replay buffer; (ii) DG the aggregated group buffer..\n\nt is an agent-level metric that denotes the number of unique t is a group-level metric that captures the diversity of\n\n5\n\n(a)(b)(c)(d)Visit startVisit endIndependentgroupsAgent 1Buffer 1Q1Agent 2Q2Env 1Buffer 2Env 2Under review as a conference paper at ICLR 2023\n\n3 RESULTS\n\nWe now evaluate the social network structures presented in Figure 3 on the innovation tasks described in Section 2.2 and visualized in Figure 2. Specifically, methods ring, small-world, dynamic and fully-connected are instantiations of SAPIENS for different social network structures with 10 DQNs where shared experiences are sampled randomly from the replay buffers. We benchmark SAPIENS against: a) no-sharing, a setting with 10 DQN agents without experience sharing b) single, a single DQN agent c) A2C, a distributed policy-gradient algorithm where 10 workers share gradients with a single (Mnih et al., 2016) and d) Ape-X, a distributed RL algorithm with 10 workers that share experience samples with a single DQN learner (Horgan et al., 2018). To test for statistical significance we perform ANOVA for multiple and Tukey range tests for pairwise comparisons. We refer readers to Appendix E for more information about our experimental setup, including tables 2, 2 and 2 that contain the means and standard deviations for evaluation metrics in the three tasks.\n\n3.1 OVERALL COMPARISON\n\nIn Figure 4 we compare methods across tasks in terms of group success S and Time to first success T +, where we also indicate statistically significant pairwise comparisons with asterisks (more asterisks denote higher significance).\n\nWe observe that performance varies across tasks and topologies. The single path task is optimally solved by all methods, except for single and Ape-X, which failed in 20% and 15% of the trials respectively due to slow convergence. ANOVA showed no significant difference in terms of group success S (p = 0.43) but methods differed significantly in terms of T + (p = 0.2e−13). In particular, single is significantly slower than all other methods (T + = 648e3 for single) and A2C is significantly quicker than other methods (T + = 36200 for A2C), with the Tukey’s range test indicating significance with p = 0.001 for all pairwise comparisons with these two methods. This is in agreement with the expectation that a single agent learns slower due to having less data and that the policy-gradient algorithm A2C is more sample efficient than value function algorithms like DQNs. In the merging paths task there were significant differences among methods both for group success S ( p = 0.4e−4) and convergence speed T + (p = 0.0095). The group success of dynamic (S = 0.65) is significantly higher compared to Ape-X (S = 0.05, p = 0.001), A2C (S = 0.0, p = 0.00101), fully-connected (S = 0.0, p = 0.00101) and ring (S = 0.2, p = 0.0105). The single, no-sharing and small-world structures performed comparably well, but did not show statistically significant differences with other methods. In terms of T +, we see that dynamic is quicker than other methods with positive S, which leads to it being the only method with statistically significant differences with Ape-X (p = 0.0292), fully-connected (p = 0.0421) and A2C (p = 0.0421), the methods that failed in almost all trials and hence have T + equal to the budget of the experiment. Thus, our main conclusion in the merging-paths task is that methods with fully-connected topologies (fully-connected, A2C, Ape-X) fail dramatically and that the dynamic structure succeeds with higher probability. Finally, in the best-of-ten paths task, differences are also significant both for T + (p = 0.9e−7) and S (p = 0.15e−6). Here, dynamic outperforms all methods in terms of S with all Tukey range tests indicating significance with p = 0.001, which leads to it also having significantly higher convergence rate (T + = 14e6 for dynamic) compared to the other methods that exhausted their time budget.\n\nIn additional experiments we have also: a) observed that these conclusions are consistent across group sizes, with increasing sizes leading to better performance in partially-connected and worse in fully-connected structures (see Appendix E.5) b) observed a drop in performance under prioritized experience sharing (Souza et al., 2019; Horgan et al., 2018), where the DQNs employ prioritized replay buffers (Schaul et al., 2016) and experiences with higher priority are shared more often (see Appendix E.6). In agreement with previous works (Souza et al., 2019), we observe that performance degrades in all methods that share experiences. This does not happen for no-sharing, which indicates that prioritized experiences are detrimental only when they are shared. To address this, agents can recompute priorities upon receiving them from other agents to ensure they agree with their own experience (Horgan et al., 2018). c) analyzed how the performance of dynamic varies with its hyperparameters and derived suggestions for tuning it (see Appendix D) d) monitored the robustness of social network structures to different learning hyper-parameters and observed that dynamic is more robust than fully-connected and no-sharing (see Appendix E.4) e) measured alignment of experiences within and across groups that further support our hypothesis that the content of replay buffers\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Overall performance comparison for the single path (first column), merging paths (second column) and best-of-ten paths (third column) task in terms of group success (S) (top row) and Time to first success (T +) (bottom row).\n\nis highly contingent on the social network structure (see Appendix E.3) f) performed simulations in another deceptive test-bed and derived similar conclusions to what we observed in Wordcraft, namely that partially-connected structures are better at avoiding local optima (See Appendix E.7) g) tested for the robustness of SAPIENS methods to the amount of sharing (hyper-parameters Ls and ps introduced in Section 2.3) in Appendix E.8, where we observe sub-optimal performance for low amounts of sharing in dynamic and for large shared batches in fully-connected structures. To explain the differences for SAPIENS under different structures we now study each task in isolation for nosharing, fully-connected, ring, small-world and dynamic, focusing on the behavioral and mnemonic metrics described in Section 2.4.\n\n3.2 TASK: SINGLE PATH\n\nPrevious human (Mason et al., 2008) and computational (Lazer & Friedman, 2007) studies have indicated that, when it comes to problems with a single global optimum, fully-connected topologies perform best. Our analysis here, however, indicates no statistically significant difference between methods: the fully-connected topology was actually the slowest (T + = 311e3). To shed light into this behavior, we turn towards diversity and volatility. We compare average individual diversity ( ̄Dt) and group diversity DG t , where for each method we create two samples by looking at these two metrics at the timestep at which diversity starts decreasing. ANOVA tests indicate that differences are significant both for ̄DT + (p = 0.16e−8), where all pairwise comparisons are significant based on the Tukey range test, and DG t (p = 0.0005), where the significantly different pairs are (no-sharing, dynamic, p = 0.02), (dynamic, fully-connected, p = 0.001), (dynamic, small-world, p = 0.0087), (fully-connected, ring, p = 0.008476). Intuitively, fully-connected exhibits the highest average individual diversity ̄Dt (left in Figure 5) and the lowest group diversity DG t (left in Figure 5): sharing experiences with others diversifies an individual’s experiences but also homogenizes the group.\n\nThis task does not require group diversity, but we expect that high individual diversity should be indicative of quicker exploration. So why does the higher ̄Dt of fully-connected not act to its benefit? To answer this we turn to volatility Vt (top left of Figure 5) and observe that it is highest for the fully-connected topology. We tested for statistically significant differences among methods in terms of volatility at convergence (VTtrain ) and found that these pairs differ significantly: (small-world, ring, p = 0.002387), (small-world, dynamic, p = 0.001), (small-world, no-sharing,p = 0.001), (small-world, fully-connected, p = 0.001), (ring, fully-connected, p = 0.001), (dynamic, fullyconnected, p = 0.001) and (no-sharing, fully-connected, p = 0.001). We hypothesize that the higher individual diversity and volatility of the fully-connected structure are linked and indicate that, although experience sharing helps speed up exploration, it also destabilizes agents, so that no\n\n7\n\n0.00.51.01.52.0, Group successSingle path0.00.51.01.52.0*********Merging paths0.00.51.01.52.0*********************Best-of-ten paths0.00.51.01.52.02.53.0T+, Time to first success1e6********************************0.00.51.01.52.02.53.01e7***singleno-sharingdynamicfully-connectedringsmall-worldA2CApe-X01231e7*********************Under review as a conference paper at ICLR 2023\n\nFigure 5: Analyzing group behavior: (Left) Average Diversity ̄Dt in the single path task (Right) Average volatility (Vt) (top row) and Group Diversity DG t (bottom row) in the single path task (first column), merging paths task (second column) and best-of-10 paths task (third column) in term sof\n\nnet gain is observed in terms of convergence rate. Notably the fact that experience sharing degrades performance in RL has been observed but not understood (Souza et al., 2019; Schmitt et al., 2019), while as we see here, this becomes possible by analyzing diversity and volatility.\n\n3.3 TASK: MERGING PATHS\n\nThe hypothesis that motivated this task was that partially connected groups will perform better than fully-connected structures due to their ability to explore diverse trajectories and avoid the two local optima (Derex & Boyd, 2016). This was indeed the case as we saw in our discussion of Figure 4 in Section 3.1. If we also look at the diversities in this task, then we see that the group diversity DG of fully connected (illustrated in the middle bottom plot in Figure 5) differs significantly to the one of (no-sharing, p = 0.001), (ring, p = 0.0067) and dynamic (p = 0.0177). The group diversity in small-world, which was relatively successful (S = 0.3), follows that of the other partially connected structures but peaks at a lower level (DG ≈ 1700). We believe that this is due to the dual ability of this topology to both protect and spread information and that its diversity and performance will improve as we increase the size of the group, enabling higher clustering (we illustrate this behavior in Figure 15 in Appendix E.5 where we increase the size of groups to 50 agents).\n\n3.4 TASK: BEST-OF-TEN PATH\n\nWhich social network topology works best in large search spaces? Our analysis in Figure 4 clearly indicates that dynamic achieves the highest performance. Differently from the two previous tasks where fully-connected exhibited the highest volatility, dynamic and ring, the topologies with the lowest number of connections, are the most volatile here (top row of Figure 5), as agents in them are exploring quickly and, hence, performing better. In terms of group diversity, we found that no-sharing scored significantly lower than all other methods with p = 0.001, the small-world surpassed all methods, with its difference from dynamic being marginally statistically significant and fully-connected did not score last. Thus, differently from the other two tasks with small search spaces, quick spreading of information increases group diversity here. However, to solve the task quickly, group diversity needs to be combined with quick, local exploitation of the optimal path, which is possible under structures with large shortest path, such as dynamic and ring. Another interesting observation here and in the single-path task is that dynamic is the only structure that exhibits significantly higher group diversity than no-sharing, indicating that it fosters group exploration.\n\n4 RELATED WORK\n\nIn distributed RL, shared information has the form of experience tuples or gradients, with the former being preferred due to instabilities and latency concerns in the latter (Horgan et al., 2018; Mnih et al., 2016; Schmitt et al., 2019; Nair et al., 2015; Garnelo et al., 2021). A common social network struc-\n\n8\n\n0246Training step, t1e50100200300400Average Diversity, Dtno-sharingdynamicfully-connectedringsmall-world0.00.51.0Training step, t1e6246Average Volatility, Vt0.51.0Training step, t1e601230.51.0Training step, t1e6123no-sharingdynamicfully-connectedringsmall-world0.00.51.0Training step, t1e60.00.51.0Group divesrity, Dt1e30.00.51.0Training step, t1e60121e30.00.51.0Training step, t1e60.02.55.01e3Under review as a conference paper at ICLR 2023\n\nture is that of multiple actors and a single learner (Mnih et al., 2016; Horgan et al., 2018; Schmitt et al., 2019; Garnelo et al., 2021), while the Gorila framework (Nair et al., 2015) is a more general multi-learner architecture, that differs from ours as agents are sharing parameters. Networked Evolutionary Strategies (NetES) consider the effect of network structure on evolutionary strategies (Adjodah et al., 2019), where multiple actors share gradients with a single learner. NetES was applied on continuous control tasks that differ from our deceptive and discrete innovation tasks. In MARL, social network structure determines who co-exists with whom in the environment (Garnelo et al., 2021) or which agents in its neighborhood an agent attends to (Jiang et al., 2020a; Du et al., 2021). Here, dynamic topologies that are adapted to maximize a group’s reward have been shown to maximize strategic diversity (Garnelo et al., 2021) and help the agents coordinate on demand (Du et al., 2021). In contrast, our dynamic topologies vary periodically independently of the group’s performance, which is important for avoiding local optima. In population-based training, policies are compared against the whole population, thus only considering a fully-connected social network structure (Jaderberg et al., 2018). Admittedly, the literature on the effect of social network structure on collective search is diverse, with different fields making different design choices; to illustrate this we provide a non-exhaustive summary of our literature review in Table 1 of Appendix B.\n\n5 DISCUSSION AND FUTURE WORK\n\nWe tested the hypothesis that the social network structure of experience sharing can shape the performance of a group of RL agents using our proposed learning framework, SAPIENS, and showed that, in line with human studies, both social network topology and task structure affect performance. Based on our experimental results, we can provide general recommendations on which topology to use for which task class. In the single-path task, an instance of a class of tasks with no strong local optima (similarly to long-horizon tasks (Gupta et al., 2019)), our results show no benefit of experience sharing. In the merging-path task which exhibits strong local optima that have to be explored up to a certain point in order to discover the global optimum (in the spirit of hard exploration tasks (Baker et al., 2022; Ecoffet et al., 2021)), our results show that topologies with low initial connectivity (such as no-sharing, small world and dynamic) perform best. The dynamic topology shows the highest performance, allowing different groups to explore non-optimal paths before sharing their experience during visits to other groups to find the optimal one. Finally, in the case of large search space with many local optimas (in the spirit of combinatorial optimization tasks (Mazyavkina et al., 2021)), our results show that the dynamic topology performs best, allowing different groups to first explore different paths, then spread the optimal solution to other groups once discovered.\n\nWhen adopting RL algorithms as computational models for replicating experiments with humans, one needs to acknowledge that their communication and decision-making mechanisms may not faithfully replicate the ones used by humans. One notable difference is that humans may exhibit normative behavior, adopting information not for its utility in the task but for social approval (Mason et al., 2008). From an RL perspective, our study is limited in including experiments only in a few symbolic tasks and a simple navigation task; in the future we plan to study more complex environments like Crafter (Hafner, 2021).\n\nWe hope that our work will contribute to the fields of cognitive science and DRL in multiple ways. First, our empirical observations in the single path and best-of-ten-path tasks provide concrete hypotheses for future experiments studying human innovation, which has so far been studied only in a task that inspired our merging-paths task (Derex & Boyd, 2016). By continuing the dialogue that has been initiated between human and computational studies (Fang et al., 2010; Lazer & Friedman, 2007; Cantor et al., 2021) to include DRL methods, we believe that cognitive science will benefit from tools that, as we show here, can learn in realistic problem set-ups and can be analyzed not just in terms of their behavior, but also in terms of their memories. Second, we hope that studies in distributed RL will extend their evaluation methodology by analyzing not just rewards, but also behavioral and mnemonic metrics such as diversity, conformity and volatility that, as we show here, correlate with success. Aside this, the effect of social network structure in distributed RL can be extended beyond evolutionary strategies (Adjodah et al., 2019) and beyond our current instantiation of SAPIENS, by considering other off-policy algorithms than DQNs and other types of information sharing. Finally, considering the effectiveness of the dynamic topologies observed in this study, we envision future works that investigate more types of them, as well as meta-learning or onlineadaptation algorithms where the social network structure is optimized for a desired objective.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDhaval Adjodah, Dan Calacci, Abhimanyu Dubey, Anirudh Goyal, Peter M. Krafft, Esteban Moro, and Alex Pentland. Communication topologies between learning agents in deep reinforcement learning. CoRR, abs/1902.06740, 2019. URL http://arxiv.org/abs/1902.06740.\n\nBowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv preprint arXiv: Arxiv-2206.11795, 2022.\n\nPhilip Bontrager, Ahmed Khalifa, Damien Anderson, Matthew Stephenson, Christoph Salge, and Julian Togelius. Superstition in the network: Deep reinforcement learning plays deceptive games. CoRR, abs/1908.04436, 2019. URL http://arxiv.org/abs/1908.04436.\n\nMauricio Cantor, Michael Chimento, Simeon Q. Smeele, Peng He, Danai Papageorgiou, Lucy M. Aplin, and Damien R. Farine. Social network architecture and the tempo of cumulative cultural evolution. Proceedings of the Royal Society B: Biological Sciences, 288(1946):20203107, March 2021. doi: 10.1098/rspb.2020.3107. URL https://royalsocietypublishing.org/ doi/10.1098/rspb.2020.3107. Publisher: Royal Society.\n\nFilippos Christianos, Lukas Sch ̈afer, and Stefano Albrecht. Shared experience actor-critic for multiagent reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 10707–10717. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/7967cc8e3ab559e68cc944c44b1cf3e8-Paper.pdf.\n\nAlin Coman, Ida Momennejad, Rae D Drach, and Andra Geana. Mnemonic convergence in social networks: The emergent properties of cognition at a collective level. Proc. Natl. Acad. Sci. U. S. A., 113(29):8171–8176, July 2016.\n\nMaxime Derex and Robert Boyd. Partial connectivity increases cultural accumulation within groups. Proceedings of the National Academy of Sciences, 113(11):2982–2987, March 2016. ISSN 00278424, 1091-6490. doi: 10.1073/pnas.1518798113. URL http://www.pnas.org/lookup/ doi/10.1073/pnas.1518798113.\n\nYali Du, Bo Liu, Vincent Moens, Ziqi Liu, Zhicheng Ren, Jun Wang, Xu Chen, and Haifeng Zhang. Learning Correlated Communication Topology in Multi-Agent Reinforcement learning. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’21, pp. 456–464, Richland, SC, May 2021. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 978-1-4503-8307-3.\n\nMarina Dubova, Arseny Moskvichev, and Robert Goldstone. Reinforcement Communication Learning in Different Social Network Structures. arXiv:2007.09820 [cs], July 2020. URL http://arxiv.org/abs/2007.09820. arXiv: 2007.09820.\n\nRobin I. M. Dunbar. How conversations around campfires came to be. Proceedings of the National Academy of Sciences, 111(39):14013, September 2014. doi: 10.1073/pnas.1416382111. URL http://www.pnas.org/content/111/39/14013.abstract.\n\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then\n\nexplore. Nature, 590(7847):580–586, 2021.\n\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. Technical Report arXiv:1802.01561, arXiv, June 2018. URL http://arxiv.org/abs/1802. 01561. arXiv:1802.01561 [cs].\n\nChristina Fang, Jeho Lee, and Melissa Schilling. Balancing Exploration and Exploitation Through Structural Design: The Isolation of Subgroups and Organizational Learning. Organization Science, 21:625–642, June 2010. doi: 10.1287/orsc.1090.0468.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMarta Garnelo, Wojciech Marian Czarnecki, Siqi Liu, Dhruva Tirumala, Junhyuk Oh, Gauthier Gidel, Hado van Hasselt, and David Balduzzi. Pick your battles: Interaction graphs as populationlevel objectives for strategic diversity. CoRR, abs/2110.04041, 2021. URL https://arxiv. org/abs/2110.04041.\n\nAbhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura (eds.), 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pp. 1025–1037. PMLR, 2019. URL http: //proceedings.mlr.press/v100/gupta20a.html.\n\nDanijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780,\n\n2021.\n\nDan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David Silver. Distributed Prioritized Experience Replay. arXiv:1803.00933 [cs], March 2018. URL http://arxiv.org/abs/1803.00933. arXiv: 1803.00933.\n\nMax Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garc ́ıa Casta ̃neda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, and Thore Graepel. Human-level performance in first-person multiplayer games with population-based deep reinforcement learning. CoRR, abs/1807.01281, 2018. URL http://arxiv.org/abs/1807.01281.\n\nJiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph Convolutional Reinforcement Learning. Technical Report arXiv:1810.09202, arXiv, February 2020a. URL http://arxiv. org/abs/1810.09202. arXiv:1810.09202 [cs, stat].\n\nMinqi Jiang, Jelena Luketina, Nantas Nardelli, Pasquale Minervini, Philip H. S. Torr, Shimon Whiteson, and Tim Rockt ̈aschel. WordCraft: An Environment for Benchmarking Commonsense Agents. arXiv:2007.09185 [cs], July 2020b. URL http://arxiv.org/abs/2007.09185. arXiv: 2007.09185.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL http://arxiv.org/abs/1412.6980. cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.\n\nMichelle A. Kline and Robert Boyd. Population size predicts technological complexity in Oceania. Proceedings of the Royal Society B: Biological Sciences, 277(1693):2559–2564, Audoi: 10.1098/rspb.2010.0452. URL https: gust 2010. //royalsocietypublishing.org/doi/10.1098/rspb.2010.0452.\n\nISSN 0962-8452, 1471-2954.\n\nDavid Lazer and Allan Friedman. The Network Structure of Exploration and Exploitation. Administrative Science Quarterly, 52(4):667–694, December 2007. ISSN 0001-8392, 1930-3815. doi: 10.2189/asqu.52.4.667. URL http://journals.sagepub.com/doi/10.2189/ asqu.52.4.667.\n\nJoel Z Leibo, Edward Hughes, Marc Lanctot, and Thore Graepel. Autocurricula and the emergence of innovation from social interaction: A manifesto for multi-agent intelligence research. arXiv preprint arXiv:1903.00742, 2019.\n\nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments, 2017. URL https://arxiv.org/ abs/1706.02275.\n\nW. Mason and D. J. Watts. Collaborative learning in networks. Proceedings of the National Academy of Sciences, 109(3):764–769, January 2012. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas. 1110069108. URL http://www.pnas.org/cgi/doi/10.1073/pnas.1110069108.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nWinter A. Mason, Andy Jones, and Robert L. Goldstone. Propagation of innovations in networked groups. Journal of Experimental Psychology: General, 137(3):422–433, 2008. ISSN 1939-2222, 0096-3445. doi: 10.1037/a0012798. URL http://doi.apa.org/getdoi.cfm?doi= 10.1037/a0012798.\n\nNina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for combinatorial optimization: A survey. Computers & Operations Research, 134:105400, 2021.\n\nAndrea Bamberg Migliano and Lucio Vinicius.\n\nThe origins of human cumulative culture: from the foraging niche to collective intelligence. Philosophical Transactions of the Royal Society B: Biological Sciences, 377(1843):20200317, January 2022. 10.1098/rstb. 2020.0317. URL https://royalsocietypublishing.org/doi/10.1098/rstb. 2020.0317. Publisher: Royal Society.\n\ndoi:\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.\n\nVolodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. arXiv:1602.01783 [cs], June 2016. URL http://arxiv.org/abs/1602. 01783. arXiv: 1602.01783.\n\nIda Momennejad. Collective minds: social network topology shapes collective cognition. Philosophical Transactions of the Royal Society B: Biological Sciences, 377(1843):20200315, January 2022. doi: 10.1098/rstb.2020.0315. URL https://royalsocietypublishing.org/ doi/10.1098/rstb.2020.0315. Publisher: Royal Society.\n\nIda Momennejad, Ajua Duker, and Alin Coman. Bridge ties bind collective memories. Nat. Com-\n\nmun., 10(1):1578, April 2019.\n\nArun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray Kavukcuoglu, and David Silver. Massively Parallel Methods for Deep Reinforcement Learning. Technical Report arXiv:1507.04296, arXiv, July 2015. URL http://arxiv.org/abs/1507.04296. arXiv:1507.04296 [cs].\n\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized Experience Replay. Technical Report arXiv:1511.05952, arXiv, February 2016. URL http://arxiv.org/abs/ 1511.05952. arXiv:1511.05952 [cs].\n\nSimon Schmitt, Matteo Hessel, and Karen Simonyan. Off-Policy Actor-Critic with Shared Experience Replay. Technical Report arXiv:1909.11583, arXiv, November 2019. URL http: //arxiv.org/abs/1909.11583. arXiv:1909.11583 [cs, stat].\n\nRicard V. Sol ́e, Sergi Valverde, Marti Rosas Casals, Stuart A. Kauffman, Doyne Farmer, and Niles Eldredge. The evolutionary ecology of technological innovations. Complexity, 18(4):15–27, March 2013. ISSN 10762787. doi: 10.1002/cplx.21436. URL http://doi.wiley.com/ 10.1002/cplx.21436.\n\nLucas Oliveira Souza, Gabriel de Oliveira Ramos, and Celia Ghedini Ralha. Experience Sharing Between Cooperative Reinforcement Learning Agents. arXiv:1911.02191 [cs], November 2019. URL http://arxiv.org/abs/1911.02191. arXiv: 1911.02191.\n\nD. Steinkraus, I. Buck, and P.Y. Simard. Using gpus for machine learning algorithms. In Eighth International Conference on Document Analysis and Recognition (ICDAR’05), pp. 1115–1120 Vol. 2, 2005. doi: 10.1109/ICDAR.2005.251.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033, 2012. doi: 10.1109/IROS.2012.6386109.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K ̈uttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft ii: A new challenge for reinforcement learning, 2017. URL https://arxiv.org/abs/1708.04782.\n\nErik Volz and Lauren Ancel Meyers. Susceptible–infected–recovered epidemics in dynamic contact networks. Proceedings of the Royal Society B: Biological Sciences, 274(1628):2925– ISSN 0962-8452, 1471-2954. doi: 10.1098/rspb.2007.1159. URL 2934, December 2007. https://royalsocietypublishing.org/doi/10.1098/rspb.2007.1159.\n\nDuncan J Watts and Steven H Strogatz. Collective dynamics of ‘small-world’ networks. 393:3,\n\n1998.\n\nPolly W. Wiessner. Embers of society: Firelight talk among the Ju/’hoansi Bushmen. Proceedings of the National Academy of Sciences, 111(39):14027, September 2014. doi: 10.1073/pnas. 1404212111. URL http://www.pnas.org/content/111/39/14027.abstract.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Visualizing actions and states in Wordcraft: we present the first 3 time steps of an episode corresponding to playing the example in Figure 1. This task contains 7 elements, so the action space is a integer with maximum value 7. In the components current c and inventory i, each digit in the vector corresponds to the element with the corresponding index. The initial set includes Water and Earth (their indexes at τ = 0 in the inventory are non-zero). The agent first picks Earth (second index in the action vector). At t = 1, Earth becomes active in the Current vector of the state, the the agent selects Water and receives a positive reward. At t = 2, Mud is created and inserted in the inventory and c is cleared.\n\nThis supplementary material provides additional methods, results and discussion, as well as implementation details.\n\n• Section A describes in detail the MDP formulation of Wordcraft;\n\n• Section C contains the pseudocde of SAPIENS;\n\n• Section D explains how we model dynamic social network structures and how their perfor-\n\nmance varies with their hyper-paramaters;\n\n• Section E provides more information about our experimental setup and results (effect of group size, intra-group and inter-group alignment, robustness to learning hyper-parameters and effect of prioritized experience sharing). We also provide tables and figures for all metrics presented in Section 2.4 and reward plots.\n\n• Section E.7 contains simulations with another testbed, the Deceptive Coins game.\n\nA DETAILS OF WORDCRAFT AS A MARKOV DECISION PROCESS\n\nWe consider the episodic setting, where the environment resets at the end of each episode and an agent is trained for Etrain episodes. At each time step t, the agent observes the state st and selects an action at from a set of possible actions A according to its policy πθ, where πθ is a mapping from states to actions, parameterized by a neural network with weights θ. In return, the agent receives the next state st+1 and a scalar reward rt. Each DQN agent collects experience tuples of the form [st, at, st+1, rt] in its replay buffer.\n\nFigure 6 offers a visualization of the states and actions encountered during an episode in Wordcraft, where the chosen actions and elements are chosen so as to reproduce the example of Figure 1. In order to solve the innovation task described in Section 2.1 we compute the maximum number of elements a player can craft within horizon T for recipe book Xvalid and initial set X0, which we denote as |X|. We, then, encode each element as an integer in [0, |X|). Thus, the action space is\n\n14\n\nReward, Action, Inventory, Current, State, 000000000000111200000100110000000000000110000421Episode step,210Idx = 0Idx = 1Idx = 2Idx = 3Idx = 4Idx = 5Idx = 6EpisodictimeUnder review as a conference paper at ICLR 2023\n\nWork (Garnelo et al., 2021) MARL\n\nField\n\nAgent Model DRL\n\nInformation type interaction 3\n\nDynamic structure? Yes\n\nTask microstrategic management (StarCraft(Vinyals et al., 2017))\n\n(Adjodah 2019)\n\net\n\nal.,\n\nDec-RL\n\n(Du et al., 2021)\n\nMARL\n\nDRL\n\nDRL\n\nrewards, NN weights\n\nobservations\n\ncontrol (Todorov\n\ncontinuous (Mujoco et al., 2012)) cooperative navigation (Particle World (Lowe et al., 2017))\n\nNo\n\nYes\n\n(Dubova et al., 2020) MARLC 4\n\nDRL\n\ninteraction 1\n\ncoordination game\n\nNo\n\n(Fang et al., 2010)\n\ncomputational cognitive science\n\n(Lazer & Friedman, 2007)\n\ncomputational cognitive science\n\n(Cantor et al., 2021)\n\ncomputational cognitive science\n\nbelief-majority rule 5\n\nbelief, reward\n\nNK problem 6\n\nbelief-majority rule 3\n\nbelief, reward\n\nNK type 74\n\nbelief-majority rule 3\n\nbelief, reward\n\ninnovation\n\n(Mason & Watts, 2012)\n\ncognitive science\n\nhuman\n\naction,reward\n\nNK problem 3\n\n(Mason et al., 2008)\n\ncognitive science\n\nhuman\n\naction, reward\n\nline search\n\nNo\n\nNo\n\nNo\n\nNo\n\nNo\n\nchoose\n\nMain conclusion with Topologies encourage cycles strategic diversity and dynamic ones perform robustly across tasks Random topologies fullyoutperforms connected ones Agents to communicate when they need to coordinate. Global connectivity leads to shared and symmetric protocols, partiallywhile connected groups learn local dialects. Partial maximizes mance Partial maximizes mance Performance depends on both task and group structure, no topology is robustly optimal across tasks. Full connectivity maximizes diversity and works best even in complex tasks. Partial connectivity works best in complex problems\n\nconnectivity perfor-\n\nconnectivity perfor-\n\n(Derex & Boyd, 2016) (this work)\n\ncognitive science\n\naction,reward\n\ninnovation\n\nYes\n\ndistributed RL and computational cognitive science\n\nDRL\n\ntransition tuples\n\ninnovation\n\nconnectivity\n\npartial works best yes\n\nPartially-connected structures, especially dynamics ones, perform robustly in different of types innovation tasks\n\nTable 1: A non-comprehensive summary of the literature on the topic of the effect of social network topology on collective search\n\nA = [0, |X|), with action at indicating the index of the currently chosen element. The state st contains two sets of information: a binary vector of length |X| with non-zero entries for elements already crafted by the agent within the current episode (we refer to this as inventory i) and another binary vector of length |X| where an index is non-zero if it is currently selected by the agent (we refer to this as current c). An agent begins with an inventory having non-zero element only for the initial set X0 and an all-zero selection. With the first action a0, the selected item becomes non-zero in the selection. With the second action, a1, we check if the combination (a1, c0) is valid under the recipe book and, if so, return the newly crafted element (corresponding entry in i becomes non-zero) and the reward. This two-step procedure continues until the end of the episode.\n\nB SUMMARY OF RELATED WORKS\n\nIn this appendix we provide a non-comprehensive summary of the literature on the topic of the effect of social network topology on collective search in Table 1, where our objective is to highlight similarities and differences within and across the fields of cognitive science and DRL.\n\nC PSEUDOCODE OF SAPIENS\n\nWe present the pseudocode of our proposed algorithm SAPIENS in Algorithm 1. SAPIENS works similarly to an off-policy reinforcement learning algorithm, with the difference that, after each episode, an experience sharing phase takes place between agents that belong in the same group.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 SAPIENS (Structuring multi-Agent toPology for Innovation through ExperieNce Sharing)\n\nI.neighbors= I.formNeighborhood(G) ▷ Inform agent about its neighbors I.env = initEnv(R) ▷ Create agent’s own copy of the environment based on the recipe book\n\n▷ Initialize agents\n\nfor i ∈ I do\n\n1: Input: G, connectivity, R, ps, LS 2: G.initializeGraph(connecticity) 3: I.initializeAgent() 4: for i ∈ I do 5: 6: 7: end for 8: while training not done do 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20:\n\na = i.policy() r, snew = env.step(a) i.B.insert([s, r, a, snew])\n\nend while ε =random() if ε < ps then\n\nwhile episode not done do\n\nfor j ∈ i.neighbors do\n\nend for\n\nend if i.train()\n\nj.B.add(i.B.sample(L))\n\n▷ Loop through each agent\n\n▷ Choose action\n\n▷ Share with probability ps\n\n▷ Sample random set of experiences of length L\n\n▷ Train agent\n\nend for\n\n21: 22: end while\n\nD ANALYSIS OF DYNAMIC NETWORK TOPOLOGIES\n\nIn the main paper we presented results for a single type of dynamic topolgoy. Here we present another type and analyze how they both behave for different values of their hyper-parameters. The two dynamic topologies are:\n\n• Inspired by graphs employed in human laboratory studies (Derex & Boyd, 2016), we designed graphs where the macro structure of the graph is constant but agents can randomly change their position. In particular, we divide a group of agents into sub-groups of two agents and, at the end of each episode, move an agent to another group with a probability pv for a duration of Tv episodes (for a visualization see Figure 3). To reduce the complexity of the implementation, we assume that only one visit can take place at a time. In the main paper we employ pv = 0.01 and Tv = 10 across conditions and present results with different values in Appendix D, where we refer to this topology as dynamic-Boyd.\n\n• Human behavioral ecology emphasize the importance of periodic variation in human social networks encountered throughout our evolutionary trajectory Wiessner (2014); Dunbar (2014). Due to ecological constraints human groups oscillate between phases of high and low connectivity: low-connectivity phases arise when individuals need to individually collected resources (e.g. day-time hunting) while high-connectivity phases arise when humans are idle and “forced” to be in proximity with others (e.g. fireside chats). Although these high-connectivity phases do not bare a direct evolutionary advantage, they may have played an important role by creating the conditions for the evolution of human language and culture. Inspired by this hypothesis, we have designed dynamic graphs that oscillate between a fully-connected topology that lasts for Th episodes and a topology without sharing that lasts for Tl episodes. We present results for various values of Th and Td of this topology in Appendix D, where we refer to this topology as dynamic-periodic.\n\nIn Figure 8, we observe the % of group success (SG) with the dynamic-Boyd topology for different probabilities of visit (pv) and visit duration Tv (the sub-group size is 2 in all cases). We note that, due to our implementation choice that a visit can take place only if no other agent is currently on a visit, the visit duration also affects the mixing of the group: longer visits mean that fewer visits will\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Two types of dynamic topologies: (Left) in the dynamic-Boyd topology the group is divided into sub-groups of two agents and a visit takes place with probability pv and lasts Tv episodes (Right) In the dynamic-periodic topology the graph oscillates between a phase with a fullyconnected topology that lasts for Th episodes to a phase without sharing that lasts for Tl episodes.\n\nFigure 8: Examining the sensitivity of the dynamic-Boyd topology to its hyper-parameters: % of group success (SG) for the merging-paths task (left) and the best-of-ten paths task (right).\n\ntake place in total. In the merging paths task (left), two hyper-parameter settings have a clear effect: (i) short visits with of high probability lead to bad performance. As such settings lead to a quick mixing of the population, they lead to convergence to the local optimum (ii) long visits with high probability work well. Due to the high visit probability, this setting effectively leads to topology where exactly one agent is always on a long visit. Thus, it ensures that sub-groups stay isolated for at least 1000 episodes, after which inter sub-group sharing needs to takes place to ensure that the sub-groups can progress quickly. In the best-of-ten paths task (right), this structure has a clear optimal hyper-parameterization: short visits with high probability are preferred, which maximizes the mixing of the group and makes early exploration more effective.\n\nIn Figure 9, we observe the % of group success (SG) of the dynamic-periodic topology for various values of Th and Tl. In the merging paths task (left of Figure 9) medium values for the period of both phases works best, while there is some success when the low connectivity phase lasts long (Tl = 1000). In the best-of-ten paths task (rightof Figure 9), we observe the same medium values for the period of both phases work best: thus both the absolute value and their ratio is important to ensure that exploration is efficient. The optimal configuration is the same between the two tasks (Tl = 100, Th = 10), which is a good indication of the robustness of this structure.\n\nE EMPIRICAL RESULTS\n\nTo ensure that all methods have the same number of samples, we assume that, for trials where a method did not find the optimal solution, and, hence, T + is undefined, T + is equal to the total number of timesteps the method was trained for, Ttrain. For each task, all methods have been trained for an equal duration of time: Ttrain = 1e6 for the single path , Ttrain = 7e6 for the merging paths task and Ttrain = 2e7 for the best-of-ten paths task.\n\n17\n\nDivide into sub-groups of twoVisit with probability Return afterepisodes Fully-connected phase that lasts forepisodes No-sharing phase that lasts forepisodes 0.0050.010.10.5pv, Visit probability101001000Tv, Visit duration S,% of group success0.00.10.20.30.40.50.0050.010.10.5pv, Visit probability101001000Tv, Visit duration S,% of group success0.00.10.20.30.4Under review as a conference paper at ICLR 2023\n\nFigure 9: Examining the sensitivity of the dynamic-periodic topology to its hyper-parameters: % of group success (SG) for the merging-paths task (left) and the best-of-ten paths task (right).\n\nWe perform 20 independent trials for each task and method and visualize our proposed metrics with barplots and line plots of averages across trials with error bars indicating 95% confidence intervals. We test for statistical significance of our evaluation metrics separately for each task by applying ANOVA tests 8 to detect whether at least one method differs from the rest and, subsequently, employing the Tukey’s range test 9 to detect which pairs of methods that differ significantly. We report the exact p values of theses tests in the text and, when applicable, illustrate them in figures using a set of asterisks whose number indicates the significance level (p <= 0.05: *, p <= 0.01: **, p <= 0.001: ***, p <= 0.0001: **** ) 10.\n\nWe presented the major results of our evaluation of SAPIENS in Section 3. We now present additional information regarding the implementation of the different components (Appendix E.1), the values of all performance metrics and additional plots for experiments discussed in 3 (Appendix E.2), results on intra-group and inter-group alignment (AppendixE.3), results for groups of varying sizes (Appendix E.5) and results on various dynamic topologies (Appendix D)\n\nE.1\n\nIMPLEMENTATION DETAILS\n\nImplementation of DQN We employ the same hyper-parameter for each DQN across all studied tasks and topologies: discount factor γ = 0.9, the Adam optimizer with learning rate α = 0.001 (Kingma & Ba, 2014; Dunbar, 2014), ε-greedy exploration with ε = 0.01. We employ a feedforward network with two layers with 64 neurons each. We implemented SAPIENS by extending the DQN implementation in the stable-baselines3 framework.\n\nImplementation of A2C We used the stable-baselines3 implementation of A2C 11 and tuned the hyper-parameters: learning rate, number of steps, discount factor, the entropy coefficient and the value function coefficient. This gave us the best-performing values 0.001, 5, 0.99, 0.1 and 0.25, respectively, that we also employed in the other tasks.\n\nImplementation of Ape-X We used the ray implementation of Ape-X DQN 12 and tuned the hyper-parameters: learning rate, discount factor, replay buffer capacity and ε-greedy exploration. This gave us the best-performing values in the single path task 0.001, 0.9, 5000 and 0.02, respectively.\n\n8https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f oneway.html 9https://pypi.org/project/bioinfokit/0.3/ 10https://www.graphpad.com/support/faq/what-is-the-meaning-of–or–or–in-reports-of-statistical-\n\nsignificance-from-prism-or-instat/\n\n11https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html 12https://docs.ray.io/en/latest/rllib/rllib-algorithms.html\n\n18\n\n101001000Th, High-connectivity period101001000Tl, Low-connectivity periodS,% of group success0.00.2101001000Th, High-connectivity period101001000Tl, Low-connectivity periodS,% of group success0.00.5Under review as a conference paper at ICLR 2023\n\nTopology no-sharing dynamic fully-connected ring small-world single A2C Ape-X\n\nR+ ∞\n(0.92, 0.0.036) (1,0) (1,0) (1,0) (1,0) (0.92, 0.163) (1,0) (0.93, 0.18)\n\nR∗ ∞\n(1,0) (1,0) (1,0) (1,0) (1,0) (0.927, 0.163) (1,0) (0.93, 0.18)\n\nT + (236250, 33441) (237222, 53885) (310666, 89240) (235333, 70190) (253333, 63320) (64750, 266145) (36200, 16450) (270941, 102445)\n\nT ∗ (830000,0) (346666,122041) (362000, 98503) (305333, 78818) (302666, 74110) (64750, 266145) (36200, 16450) (270941, 102445)\n\nT > (600000, 0) (109444, 98067) (51333, 20655) (70000, 22038) (49333, 31274) (0,0) (0,0) (0,0)\n\nS (1,0) (1,0) (1,0) (1,0) (1,0) (0.2, 0.41) (1,0) (0.15, 0.366)\n\n ̄Vavg (0.038,0.002) (0.027,0.01) (0.052, 0.027) (0.038,0.0026 (0.029, 0.013) (0.015, 0.013) a non-co (0,0) (0.015, 0.022)\n\nCavg (0.697, 0.0354) (0.885, 0.026) (0.891,0.034) (0.697, 0.0354) (0.912, 0.0267) (1,0) (1,0) (1,0)\n\nTable 2: Evaluation metrics for the single-path task in the form (mean of metrics, standard deviation of metric)\n\nTopology no-sharing dynamic fully-connected ring small-world single A2C Ape-X\n\nR+ ∞\n(0.657, 0.037) (0.7,0.04) (0.5349,0.085) (0.661,0.135) (0.639,0.091) (0.758, 0.187) (0.269,0).2 (0.573, 0.31)\n\nR∗ ∞\n(0.838,0).14 (0.9,0.13) (0.58, 0.04) (0.72, 0.15) (0.774, 0.173) (0.758, 0.187) (0.269, 0.2) (0.573, 0.31)\n\nT + (5334000, 2311945) (4716500,222965) (7000000,0) (5892000, 2288393) (5998000, 1699076) (5235000, 2385948) (7000000, 0) (6656900, 1534389 )\n\nT ∗ (7000000,2311945 (7000000, 0) (7000000, 0) (7000000, 0) (7000000, 0) (5235000, 2385948) (7000000, 0) (26656900, 1534389 )\n\nT > (7000000, 0) (7000000, 0) (7000000, 0) (7000000, 0) (7000000,0) (0,0) (0,0) (0,0)\n\nS (0.4,0.51) (0.75,0.48) (0,0) (0.2,0.41) (0.3, 0.483) (0.3, 0.47) (0,0) (0.05, 0.223)\n\nCavg (0.597, 0.06) (0.597, 0.0059) (0.597,0.0051) (0.595, 0.0051) (0.596,0.0065) (1,0) (1,0) (1,0)\n\n ̄Vavg (0.0089,0.0021) (0.005, 0.0016) (0.0764, 0.0044) (0.0149,0.021) (0.06775,0.0328) (0.0063, 0.0063) (0.013, 0.038) (0.054,0.157)\n\nTable 3: Evaluation metrics for the merging-paths task in the form (mean of metrics, standard deviation of metric)\n\nImplementation of graphs used as social network structures We construct small-worlds using the Watts–Strogatz model (watts strogatz graph method of the networkx package 13). This model first builds a ring lattice where each node has n neighbors and then rewires an edge with probability β. Compared to other techniques used in previous works studying the effect of topology Mason et al. (2008), this way of constructing small-worlds ensures that the average path lengths is short and clustering is high. These two properties are what differentiates small-worlds from random (short average path length and small clustering) and regular (long average path length and high clustering) graphs. We employ n = 4 and β = 0.2 in our experiments, which we empirically found to lead to good values of average path length and clustering.\n\nWe have described the generation process of dynamic topologies in Appendix D. In the main paper we employ the dynamic-Boyd topology with Tv = 10 and pv = 0.001 across tasks. These parameters have been tuned for the merging-paths task.\n\nE.2 OVERALL COMPARISON\n\nTables 2, 3 and 4 contain the values of all metrics discussed in Section 2.4 for the single path, merging paths and best-of-ten paths, respectively. We denote values computed after convergence of the group with underscore ∞ and values averaged over all training steps with underscore avg (note that we use ̄over variables to denote averaging over agents in a single training step). Cells with a dash (-) indicate that we could not compute the corresponding metrics because a group failed to find a solution in all trials. We also provide the plots of volatility and average diversity for the merging paths and best-of-10 paths task (that were not included in Figure 5) due to page limit constraints).\n\nFigure 10 presents the reward curves for all methods in the single path, merging paths and best-often paths tasks respectively. Specifically, we plot the maximum reward of the group at training step t (R+\n\nt ).\n\n13https://networkx.org/\n\nTopology no-sharing dynamic fully-connected ring small-world single A2C Ape-X\n\nR+ ∞\n(0.2124,0.036) (0.5141,0.323) (0.1615,0.09) (0.2319,0.3045) (0.198,0.281) (0.178, 0.067) (0.1285,0.19) (0.481, 0.213)\n\nR∗ ∞\n(0.446, 0.131) (0775, 0.32) (0.1819, 0.1013) (0.275,0.332) (0.216,0.275) (0.1785,0.0676) (0.1285,0.19) (0.482, 0.213)\n\nT + (20000000, 0) (13616000, 5441395) (20000000, 0) (18781000, 3854816) (18706000, 4091987) (20000000, 0) (20000000, 0) (20000000, 0)\n\nT ∗ (20000000,0) (20000000,0) (20000000, 0) (18826000, 3712513) (18746000, 3965496) (20000000,0) (20000000,0) (20000000,0)\n\nT > (20000000, 0) (20000000, 0) (20000000,0) (18045000, 6182252) (18040000, 6198064) (0,0) (0,0) (0,0)\n\nS (0,0) (0.6,0.51) (0,0) (0.1,0.31) (0.1,0.316) (0, 01) (1,0) (0.9, 0.316)\n\nCavg (0.239, 0.005) (0.242,0.0078) (0.238,0.0053) (0.237,0.004) (0.234,0.007) (1,0) (1,0) (1,0)\n\n ̄Vavg (0.007, 0.0021) (0.04,0.0223) (0.007,0.003) (0.047,0.019) (0.018,0.0049) (0.006,0.0031) (0.3244,0.35) (0.018,0.009)\n\nTable 4: Evaluation metrics for the best-of-ten paths task in the form (mean of metrics, standard deviation of metric)\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Maximum reward of the group at training step t (R+ (middle) merging paths task (right) best-of-ten paths task\n\nt ) in the (left) single path task\n\nE.3 MEASURING INTER-GROUP AND INTRA-GROUP ALIGNMENT\n\nWe have so far captures the agreement between agents in a group through the behavioral metric of conformity. Here, we present a mnemonic metric for agreement, which we term alignment. Alignment is a complementary metric to the diversity (Dk t ) metrics, that aims at capturing the effect of experience sharing on the replay buffers in a group. We propose a definition of alignment within a single group (intra-group alignmentAG t ) and a definition of alignment between two different groups (AGj ,Gj ). Such metrics of mnemonic convergence have been linked to social network topology (Coman et al., 2016) and, as we show here, they can prove useful in analyzing groups of reinforcement learning agents.\n\nt ) and group diversity (DG\n\nt\n\nt\n\nSpecifically: (i) AG is the intra-group alignment. This metric captures the similarity in terms of content between the replay buffers of agents belonging to the same group. To compute this we compute the size of the common subset of experiences for each pair of agents and, then, average over all these pairs, normalizing in [0,1]. (ii) inter-group alignment AGj ,Gj is a similar notion of alignment but employed between different groups (e.g. how different is a group of fully-connected and a dynamic group of agents in terms of the content of their group replay buffers). To compute it we concatenate all replay buffers of a group into a single one and then compute the size of the common subset of the two replay buffers.\n\nt\n\nFigure 12 presents intra-group alignment in the three tasks. We observe that, in all tasks, intragroup alignment increases with connectivity and that it reduces when the agents enter the exploitation phase. Thus, intra-group alignment can prove useful in characterizing the exploration behavior of a group. In Figure 13, we present the inter-group alignment in the single path, merging paths and best-of-ten paths tasks. We observe that the topologies do not differ significantly in the single path task. In the merging task, we observe that inter-group alignment is lower during the exploration phase, compared to other tasks, and that the small-world is the slowest to align with all other structures. Perhaps this explains why this topology finds the optimal solution with the least probability: by propagating information quickly, the group early on\n\n20\n\n0.000.250.500.751.00Training step, t1e60.00.51.0Maximum reward, RmaxAPEX-DQNsingleno-sharingdynamicfully-connectedringsmall-worldA2C024Training step, t1e60.00.51.0Maximum reward, RmaxAPEX-DQNsingleno-sharingdynamicfully-connectedringsmall-worldA2C0123Training step, t1e70.00.51.0Maximum reward, RmaxAPEX-DQNsingleno-sharingdynamicfully-connectedringsmall-worldA2CUnder review as a conference paper at ICLR 2023\n\nFigure 11: Analyzing group behavior in the merging paths task (top row) and best-of-10 paths task (bottom row). (left) Conformity Ct is a behavioral metric that denotes the percentage of agents in a group that followed the same trajectory in a given evaluation trial (right) Average Diversity ̄Dt is a mnemonic metric that denotes the number of unique experiences in the replay buffer of an agent, averaged over all agents.\n\n21\n\n0.000.250.500.751.00Training step, t1e60.20.40.60.81.0Conformity, Ct0123Training step, t1e60200400Average Diversity, Dtno-sharingdynamicfully-connectedringsmall-world02468Training step, t1e50.20.40.6Conformity, Ct0246Training step, t1e505001000Average Diversity, DtUnder review as a conference paper at ICLR 2023\n\nFigure 12: Intra-group alignment AG best-of-ten paths task (right)\n\nt in the single path task (left), merging paths task (middle) and\n\nconverges to the local optimum in this task. In the best-of-ten task, the no-sharing setting has the smallest alignment with all other structures. This reinforces our main conclusion in this work: experience sharing affects individuals and different topologies do so in different ways.\n\nE.4 ROBUSTNESS TO LEARNING HYPER-PARAMETERS\n\nIn Figure 14 we present how the performance of SAPIENS varies for different values of the learning hyperparameters learning rate and disocunt factor in the single path task under a fully-connected and a dynamic topolgoy, as well as the no-sharing condition. We observe that, although convergence to the optimal solution is not always achieved, the dynamic topology is at least as effective as the others either in terms of convergence rate and/or final performance in all conditions.\n\nE.5 EFFECT OF GROUP SIZE\n\nWe here examine the effect of the group size for all social network structures in the merging-paths and best-of-ten paths task. To visualize the progression of a group on the paths of the different tasks, we focus on specific elements in the tasks: (i) ([A8, B8, C2] in the merging-paths task. The first two correspond to reaching the end of the paths corresponding to the two local optima. To reduce the computational complexity of experiments, we do not study the last element of the optimal path (C4), but focus on C2 instead. This is sufficient to detect whether a group has discovered the optimum path. Here, we observe that the fully-connected topology fails to find the optimal path regardless of its size (with a small success probability for N = 10). We observe that the ability of the ring , small-world and dynamict topologies to avoid the local optima improves with the group size (ii) [B4, A2, E2] in the best-of-ten tasks. B4 is the fourth element on the optimal path (again we do not study the last element to reduce complexity). To avoid cluttering the visualization we only present two of the nine sub-optimal paths. In this task, we again observe that the fully-connected network fails to discover the optimal task. Among all structures and group sizes, the large dynamic network performs best, while the performance of ring and small-world is also best for N = 50. We observe that small networks sizes (N = 2, N = 6) are slower at exploring (we can see that as they rarely find the second element of the sub-optimal paths, which is required to conclude that path B is the optimal choice).\n\nOverall, this scaling analysis indicates that increasing the group size in a fully-connected topology will not improve performance, while benefits are expected for low-connectivity structures, particularly for the dynamic topology. We believe that this observation is crucial. In studies of groups of both human and artificial agents, we often encounter the conviction that, larger groups perform better and that size is a more important determinant than connectivity, the latter justifying why connectivity is often ignored Kline & Boyd (2010); Horgan et al. (2018); Mnih et al. (2016); Schmitt et al. (2019); Nair et al. (2015). Our results here point to the contrary.\n\nE.6 PRIORITIZED EXPERIENCE SHARING\n\nWe now examine how sharing prioritized experiences instead of randomly sampled ones affects the performance of SAPIENS. In Figure 16 we repeat the same experiment with Figure 4, with the difference that all methods compute priorities, which they employ both for implementing a prioritized\n\n22\n\n0123Training step, t1e60.60.70.80.91.0Intra-group alignment, At0123Training step, t1e60.40.60.81.0Intra-group alignment, At0.00.51.01.52.0Training step, t1e60.40.60.81.0Intra-group alignment, AtUnder review as a conference paper at ICLR 2023\n\nFigure 13: Inter-group alignment AGj ,Gj and best-of-ten paths task (right). In each row we compare one topology with all the rest.\n\nin the single path task (left), merging paths task (middle)\n\nt\n\n23\n\n0.00.51.0Inter-group alignment, Aj,jtdynamicfully-connectedringsmall-worldno-sharing0.00.51.0Inter-group alignment, Aj,jtindependentfully-connectedringsmall-worlddynamic0.00.51.0Inter-group alignment, Aj,jtindependentdynamicringsmall-worldfully-connected0.00.51.0Inter-group alignment, Aj,jtindependentdynamicfully-connectedsmall-worldring0123Training step, t1e60.00.51.0Inter-group alignment, Aj,jtindependentdynamicfully-connectedring0123Training step, t1e6small-world012Training step, t1e6Under review as a conference paper at ICLR 2023\n\nFigure 14: Varying the learning hyper-parameters learning rate (λ) and discount factor (γ) in different social network topologies in the single path task. )\n\nFigure 15: Scaling of different social network structures in the merging paths (top row) and best-often paths tasks (bottom row). We highlight the element belonging to the optimal path in red.\n\n24\n\n0.00.20.40.60.81.0Maximum reward, Rmax=0.01 , =0.1no-sharingdynamicfully-connected=0.01 , =0.5=0.01 , =0.90.00.20.40.60.81.0=0.001 , =0.1=0.001 , =0.5=0.001 , =0.90.00.20.40.60.81.0Training step, t1e60.00.20.40.60.81.0=0.0001 , =0.10.00.20.40.60.81.01e6=0.0001 , =0.50.00.20.40.60.81.01e6=0.0001 , =0.9A_8B_8C_20.000.250.500.751.00Selement,% of trials with elementringA_8B_8C_2fully-connectedN=2N=6N=10N=20N=50A_8B_8C_2small-worldA_8B_8C_2dynamicB_4A_2E_2element0.000.250.500.751.00Selement,% of trials with elementB_4A_2E_2elementB_4A_2E_2elementB_4A_2E_2elementUnder review as a conference paper at ICLR 2023\n\nFigure 16: Examining the effect of prioritization in experience sharing. For more details about the setup, we refer the reader to Figure 4\n\nreplay buffer and sharing experiences by sampling them in proportion to their priorities. As we see, using priorities negatively impacts experience sharing, while it helps speed up the performance of the single agent in the single path task. This behavior has been observed in previous works Souza et al. (2019) and can be attributed to the fact that the priorities of the sender do not necessarily agree with the priorities of the receiver and, therefore, destabilize learning.\n\nE.7 ADDITIONAL TEST-BED: THE DECEPTIVE COINS GAME\n\nDeceptive games are grid-world tasks introduced to test the ability of deep RL agents to avoid local optima. (Bontrager et al., 2019). Here, we perform preliminary experiments with our own JAXbased implementation of one of the games: the first difficulty level of the deceptive coins game (see Figure 17 for an illustration). Here, the agent can navigate in the grid-world during an episode and collect diamonds, which give a unit of reward. The game finishes once the agent reaches the fire, which offers an additional reward, or when a timeout of 14 time steps is reached. There are two possible paths the agent can follow: moving left and reaching the fire will give a reward of two while moving right and reaching the fire will give a reward of five. The second path is more rewarding but is harder to complete because, once an agent discovers the easier-to-find diamond on the left, it is deceived into following the left path. Once an agent commits on a path (reaches the edge of the grid-world) a barrier is raised so that the agent cannot go back within that episode.\n\nWe now examine the performance of SAPIENS under different social network structures (fullyconnected, small-world, ring, dynamic), as well as the no-sharing, A2C and Ape-X baselines for three group sizes: 6 , 10 and 20 agents. We present the reward plots for the 3 sizes in Figures 18, 19 and 20, respectively, and present an overall comparison in Figure 21 (equivalent to Figure 4 for the Wordcraft tasks).\n\nWe observe that all conditions found either the local or the global optimum and that : a) A2C fails for all network sizes. This behavior has been observed in previous works (Bontrager et al., 2019) and can be attributed to the fact that policy-gradient methods are more susceptible to local minima b) no-sharing gets stuck in the local optimum in half of the trials when the group size is small. Increasing the group size increases the probability that at least one agent in the group will escape the local minima by ε-greedy exploration c) partially connected structures find the global minima across network sizes d) fully-connected converges to the local optimum for the large group size, although the global optimum was discovered at the early exploration phase (see Figure 20). Thus, too much experience sharing is harmful e) Ape-X fails with high probability for all network sizes.\n\n25\n\n0.00.20.40.60.81.01.2, Group successSingle pathMerging pathsBest-of-ten paths0246T+, Time to first success1e6singleno-sharingdynamicfully-connectedringsmall-worldUnder review as a conference paper at ICLR 2023\n\nFigure 17: A screenshot of our implementation of the Deceptive Coins task. Collecting diamonds gives a positive reward and touching the fire terminates the game.\n\nFigure 18: Performance for a group with 6 agents\n\nFigure 19: Performance for a group with 10 agents\n\nFigure 20: Performance for a group with 20 agents\n\n26\n\n0100000200000300000400000Training step, t0246Maximum reward, RmaxAPEX-DQNno-sharingdynamicfully-connectedringsmall-worldA2C0200000400000600000Training step, t0246Maximum reward, RmaxAPEX-DQNno-sharingdynamicfully-connectedringsmall-worldA2C0100000200000300000Training step, t0246Maximum reward, RmaxAPEX-DQNno-sharingdynamicfully-connectedringsmall-worldA2CUnder review as a conference paper at ICLR 2023\n\nFigure 21: Overall performance comparison for a group with: 6 agents (first column), 10 agents (second column) and 20 agents (third column) task. We present two metrics: group success (S) denotes whether at least one agent in the group found the optimal solution (top row) and T +, Time to first success, is the number of training time steps required for this event (bottom row). Note that T + can be computed only for S > 0 its error bars and significance tests can only be computer for S > 1. We denote statistical significance levels with asterisks.)\n\nIn general, our conclusions in this task are consistent with what we observe in Wordcraft, in particular the merging paths task that has a similar deceptive nature.\n\nE.8 ROBUSTNESS TO AMOUNT OF SHARING (ps AND Ls)\n\nIn Section 2.3 we formulated SAPIENS and described two hyper-parameters: ps is the probability of sharing a batch of experience tuples at the end of an episode and Ls is the length of this batch. Here, we test the robustness of SAPIENS to these two hyper-parameters, which both control the amount of shared information and, therefore, interact with hyper-parameters of the DQNs (in particular the learning rate) to control the rate at which information is shared to the rate of individual learning. Specifically, we evaluate the dynamic topology (with the same hyper-parameters employed in the main paper, i.e., visit duration Tv = 10 and probability of visit pv = 0.05) and the fully-connected topology in the deceptive coins game (described in Appendix E.7) with 20 DQN agents.\n\nIn Figure 22 we present group success (S) averaged across trials for a parametric analysis over Ls ∈ (1, 6, 36) and ps ∈ (0.35, 0.7, 1). We observe that the dynamic topology finds the optimal solution across conditions except for a small probability of failure for (Ls = 1, ps = 0.35) and (Ls = 1, ps = 0.7). These values correspond to low amounts of information sharing. In this case, the dynamic structure becomes more similar to a no-sharing structure: the amount of shared information is not enough to help the agents avoid local optima they fall into due to individual exploration. For the fully-connected topology we observe that performance degrades for high amounts of information ((L = 36,ps = 0.35), (L = 36,ps = 0.7), (L = 36,ps = 1)). This is in accordance with our expectation that fully-connected topologies lead to convergence to local optima. Interestingly, this structure performs well when ps = 1 and Ls ≤ 6. Thus, sharing more frequently is better than sharing longer batches: we hypothesize that this is because longer batches have more correlated data, making convergence to local optima more probable.\n\n27\n\n0.00.51.01.52.0, Group success*******************6 agents0.00.51.01.52.0************************10 agents0.00.51.01.52.0************************20 agents0.00.20.40.60.81.0T+, Time to first success1e6********************no-sharingdynamicfully-connectedringsmall-worldA2CApe-X0.00.20.40.60.81.01e6*******0.00.20.40.60.81.01e6Under review as a conference paper at ICLR 2023\n\nFigure 22: Robustness of group success S to sharing hyper-parameters ps and Ls for dynamic (left) and fully-connected (right)\n\n28\n\n1636Ls, Length of shared batch 0.350.71.0ps, Probability of sharingS, group success0.000.250.500.751.001636Ls, Length of shared batch 0.350.71.0ps, Probability of sharingS, group success0.000.250.500.751.00",
    "reference": "# Summary Of The Paper\n\nThe authors present study of the role of multi-agent topology on innovation towards goal of clarifying which social network structures are optimal for which innovation tasks, and which properties of experience sharing improve multi-level innovation. For multi-level hierarchical problem setting (WordCraft), three different innovation tasks were considered. The design networks of DQNs enables sharing experiences from their re- play buffers in varying structures (fully connected, small world, dynamic, ring). The level of innovation achieved by different setting, shows that, first, consistent with human findings, experience sharing within a dynamic structure achieves the highest level of innovation across tasks. Second, experience sharing is not as helpful when there is a single clear path to innovation. For Third, two metrics we propose, conformity and diversity of shared experience, can explain the success of different social network structures on different tasks.\n\n# Strength And Weaknesses\n\nSAPIENS experiments show that dynamic topologies of experience sharing are best suited to solve complex innovation tasks\n\nboth multi-agent network topology and task structure affect the performance of SAPIENS. Based on our experimental results, we can provide general recommendations on which topology to use for which task class. \n\n- The single-path task is an instance of a class of tasks with no strong local optima (similarly to long-horizon tasks. results show no benefit of experience sharing\n\n- The paper lays out how the various forms of the network interconnect settings considered performed in tasks that are individual (global and local optima same) or group, such as merging-path task. These exhibits strong local optima that requires explotation a certain point in order to discover the global optimum\n\nThe results also show that topologies with low initial connectivity (such as no-sharing, small world and dynamic) performs best here by improving the exploration of different innovation paths. The dynamic topology shows up as the highest performance, allowing different groups to reach the merging innovation level in non-optimal paths before sharing their experience during visits to other groups to find the optimal one. Finally, the best-of-ten task is an instance of a class of tasks with a large search space, many local optima and a few global ones. The results show that the dynamic topology performs best, allowing different groups to first explore different paths, then spread the optimal solution to other groups once discovered.\n\nBelow are some of the improvements I will like to suggest\n\n1. why 20 trials (referred in section 3) was deemed sufficient\n2. in dynamic network is perf. best because you already ran over the combinations many times and chose best interconnect (app a.3 has some details but unclear which one was used)\n3. how many steps were in each trial not indicated\n4. If first is the case then how does the conclusion follows (fig 4 sec 3.1):\nmerging paths task the performance of the dynamic structure is significantly better than all other baselines except for no-sharing (p-value 0.22) and small-world (p-value 0.07)…This indicates that, while learners that do not share experiences manage to solve the task with relative success, learners that share experiences under social networks combining large clustering and small shortest path perform best.\n5. Also, why the group diversity changes from Fig 5 to Fig 6 for singlepath task\n6. Minor: Figure 2 top row the merged path see first element as 5 rewards written in text but shows up as 8 in figure\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work seems original and quality/novelty is adequate\n\n# Summary Of The Review\n\nSAPIENS experiments show that dynamic topologies of experience sharing are best suited to solve complex innovation tasks\n\nboth multi-agent network topology and task structure affect the performance of SAPIENS. Based on our experimental results, we can provide general recommendations on which topology to use for which task class. \n\n- The single-path task is an instance of a class of tasks with no strong local optima (similarly to long-horizon tasks. results show no benefit of experience sharing\n\n- The paper lays out how the various forms of the network interconnect settings considered performed in tasks that are individual (global and local optima same) or group, such as merging-path task. These exhibits strong local optima that requires explotation a certain point in order to discover the global optimum\n\nThe results also show that topologies with low initial connectivity (such as no-sharing, small world and dynamic) performs best here by improving the exploration of different innovation paths. The dynamic topology shows up as the highest performance, allowing different groups to reach the merging innovation level in non-optimal paths before sharing their experience during visits to other groups to find the optimal one. Finally, the best-of-ten task is an instance of a class of tasks with a large search space, many local optima and a few global ones. The results show that the dynamic topology performs best, allowing different groups to first explore different paths, then spread the optimal solution to other groups once discovered.\n\nBelow are some of the improvements I will like to suggest\n\n1. why 20 trials (referred in section 3) was deemed sufficient\n2. in dynamic network is perf. best because you already ran over the combinations many times and chose best interconnect (app a.3 has some details but unclear which one was used)\n3. how many steps were in each trial not indicated\n4. If first is the case then how does the conclusion follows (fig 4 sec 3.1):\nmerging paths task the performance of the dynamic structure is significantly better than all other baselines except for no-sharing (p-value 0.22) and small-world (p-value 0.07)…This indicates that, while learners that do not share experiences manage to solve the task with relative success, learners that share experiences under social networks combining large clustering and small shortest path perform best.\n5. Also, why the group diversity changes from Fig 5 to Fig 6 for singlepath task\n6. Minor: Figure 2 top row the merged path see first element as 5 rewards written in text but shows up as 8 in figure\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nRED-GCN: REVISIT THE DEPTH OF GRAPH CONVOLUTIONAL NETWORK\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFinding the proper depth d of a GNN that provides strong representation power has drawn significant attention, yet nonetheless largely remains an open problem for the graph learning community. Although noteworthy progress has been made, the depth or the number of layers of a corresponding GCN is realized by a series of graph convolution operations, which naturally makes d a positive integer (d ∈ N+). An interesting question is whether breaking the constraint of N+ by making d a real number (d ∈ R) can bring new insights into graph learning mechanisms. In this work, by redefining GCN’s depth d as a trainable parameter continuously adjustable within (−∞, +∞), we open a new door of controlling its expressiveness on graph signal processing to model graph homophily/heterophily (nodes with similar/dissimilar labels/attributes tend to inter-connect). A simple and powerful GCN model RED-GCN, is proposed to retain the simplicity of GCN and meanwhile automatically search for the optimal d without the prior knowledge regarding whether the input graph is homophilic or heterophilic. Negative-valued d intrinsically enables high-pass frequency filtering functionality for graph heterophily. Variants extending the model flexibility/scalability are also developed. The theoretical feasibility of having a real-valued depth with explainable physical meanings is ensured via eigen-decomposition of the graph Laplacian and a properly designed transformation function from the perspective of functional calculus. Extensive experiments demonstrate the superiority of RED-GCN on node classification tasks for a variety of graphs. Furthermore, by introducing the concept of eigengraph, a novel graph augmentation method is obtained: the optimal d effectively generates a new topology through a properly weighted combination of eigengraphs, which dramatically boosts the performance even for a vanilla GCN.\n\n1\n\nINTRODUCTION\n\nGraph convolutional network (GCN) (Kipf & Welling, 2016; Veliˇckovi ́c et al., 2017; Hamilton et al., 2017) has exhibited great power in a variety of graph learning tasks, such as node classification (Kipf & Welling, 2016; Luan et al., 2019; 2022a), link prediction (Zhang & Chen, 2018), community detection (Chen et al., 2020), and many more. Since the representation power of GCN is largely determined by its depth, i.e., the number of graph convolution layers, tremendous research efforts have been made on finding the optimal depth that strengthens the model’s ability for downstream tasks. Upon increasing the depth, the over-smoothing issue arises: a GCN’s performance is deteriorated if its depth exceeds a uncertain threshold (Kipf & Welling, 2016). It is unveiled in (Li et al., 2018) that a graph convolution operation is a special form of Laplacian smoothing (Taubin, 1995). Thus, the similarity between the graph node embeddings grows with the depth so that these embeddings eventually become indistinguishable. Various techniques are developed to alleviate this issue, e.g., applying pairwise normalization can make distant nodes dissimilar (Zhao & Akoglu, 2019), and dropping sampled edges during training slows down the growth of embedding smoothness as depth increases (Rong et al., 2019).\n\nOther than the over-smoothing issue due to large GCN depth, another fundamental phenomenon widely existing in real-world graphs is homophily and heterophily. In a homophilic graph, nodes with similar labels or attributes tend to inter-connect, while in a heterophily graph, connected nodes usually have distinct labels or dissimilar attributes. Most graph neural networks (GNNs) are developed based on homophilic assumption (Yang et al., 2016), while models able to perform well on heterophilic\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ngraphs often need special treatment and complex designs (Bianchi et al., 2021; Zhu et al., 2020). Despite the achievements made by these methodologies, little correlation has been found between the adopted GNN model’s depth and its capability of characterizing graph heterophily.\n\nFor most GNNs, if not all, the depth needs to be manually set as a hyper-parameter before training, and finding the proper depth usually requires a considerable amount of trials or good prior knowledge of the graph dataset. Since the depth represents the number of graph convolution operations and naturally takes only positive integer values, little attention has been paid to the question whether a non-integer depth is realizable, and if yes, whether it is practically meaningful, and whether it can bring unique advantages to current graph learning mechanisms.\n\nThis work revisits the GCN depth from spectral and spatial perspectives and explains the interdependencies between the following key ingredients in graph learning: the depth of a GCN, the spectrum of the graph signal, and the homophily/heterophily of the underlying graph. Firstly, through eigen-decomposition of the symmetrically normalized graph Laplacian, we present the correlation between graph homophily/heterophily and the eigenvector frequencies. Secondly, by introducing the concept of eigengraph, we show the graph topology is equivalent to a weighted linear combination of eigengraphs, and the weight values determine the GCN’s capability of capturing homophilic/heterophilic graph signals. Thirdly, we reveal that the eigengraph weights can be controlled by GCN’s depth, so that an automatically tunable depth parameter is needed to adjust the eigengraph weights into the designated distribution in match of the underlying graph homophily/heterophily.\n\nTo realize the adaptive GCN depth, we extend its definition from a positive integer to an arbitrary real number with theoretical feasibility guarantees from functional calculus (Shah & Okutmu ̧stur, 2020). With a trainable depth parameter, we propose a simple and powerful model, Redefined Depth-GCN (ReD-GCN), with two variants. Extensive experiments demonstrate the automatically optimal depth searching ability, and it is found that negative-valued depth plays the key role in handling heterophilic graphs. Systematical investigation on the optimal depth is conducted in both spectral and spatial domains. It in turn inspires the development of a novel graph augmentation methodology. With clear geometric explanability, the augmented graph structure possesses supreme advantages over the raw input topology, especially for graphs with heterophily. The main contributions of this paper are summarized as following:\n\n• The interdependence between negative GCN depth and graph heterophily is discovered;\n\nIn-depth geometric and spectral explanations are presented.\n\n• A novel problem of automatic GCN depth tuning for graph homophily/heterophily detection is formulated. To our best knowledge, this work presents the first trial to make GCN’s depth trainable by redefining it on the real number domain.\n\n• A simple and powerful model RED-GCN with two variants (RED-GCN-S and RED-GCN-\n\nD) is proposed. A novel graph augmentation method is discussed.\n\n• Our model achieves superior performance on semi-supervised node classification tasks on\n\n11 graph datasets.\n\n2 PRELIMINARIES\n\nNotations. We utilize bold uppercase letters for matrices (e.g., A), bold lowercase letters for column vectors (e.g., u) and lowercase letters for scalars (e.g., α). We use the superscript ⊤ for transpose of matrices and vectors (e.g., A⊤ and u⊤). An attributed undirected graph G = {A, X} contains an adjacency matrix A ∈ Rn×n and an attribute matrix X ∈ Rn×q with the number of nodes n and the dimension of node attributes q. D denotes the diagonal degree matrix of A. The adjacency matrix with self-loops is given by ̃A = A + I (I is the identity matrix), and all variables derived from ̃A are decorated with symbol ̃, e.g., ̃D represents the diagonal degree matrix of ̃A. Md stands for the d-th power of matrix M, while the parameter and node embedding matrices in the d-th layer of a GCN are denoted by W(d) and H(d).\n\nGraph convolutional network (GCN) and simplified graph convolutional network (SGC). The layer-wise message-passing and aggregation of GCN (Kipf & Welling, 2016) is given by\n\nH(d+1) = σ( ̃D− 1\n\n2 ̃A ̃D− 1\n\n2 H(d)W(d)),\n\n(1)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Decompose the symmetrically normalized adjacency matrix into three eigengraphs.\n\nwhere H(d)/H(d+1) stands for the embedding matrix (H(0) = X) in the d-th/(d + 1)-th layer; W(d) is the trainable parameter matrix; and σ(·) is the non-linear activation function. With σ(·) removed in each layer, SGC (Wu et al., 2019) is obtained as below:\n\nH(d) = ̃SdXW,\n\n(2)\n\nwhere ̃S = ̃D− 1 W = (cid:81)d−1\n\ni=0 W(i).\n\n2 ̃A ̃D− 1\n\n2 , and the parameter of each layer W(i) are compressed into one trainable\n\nGraph Laplacian and spectrum. In graph theory, graph Laplacian L = D−A and its symmetrically 2 AD− 1 normalized correspondence Lsym = I − D− 1 2 possess critical properties of the underlying graph G. Lsym has eigenvalues [λ1, λ2, . . . , λn], where λi ∈ [0, 2), ∀i ∈ {1, 2, . . . , n} (Chung & Graham, 1997). 1 Here they are put in ascending order: 0 = λ1 ≤ λ2 ≤ · · · ≤ λn < 2. It can be eigen-decomposed as: Lsym = UΛU⊤, where U = [u1, u2, . . . , un] is the eigenvector matrix (ui ⊥ uj, ∀i ̸= j), and Λ is the diagonal eigenvalue matrix: \n\n\n\nΛ =\n\nλ1 ...\n\n0\n\n \n\n0\n\n· · · ... . . . · · · λn\n\n  .\n\ni ∈ Rn×n. As we will show in Section 3, this n × n matrix For each eigenvector ui, we have uiu⊤ can be viewed as a weighted adjacency matrix of a graph with possible negative edges, which we name uiu⊤ i as the i-th eigengraph of G. Accordingly, Lsym can be written as the linear combination of all eigengraphs weighted by the corresponding eigenvalues (Chung & Graham, 1997):\n\nLsym = λ1u1u⊤\n\ni + . . . + λnunu⊤ n , (3) 1 has an identical value 1 where the first eigenvalue λ1 = 0, and the corresponding eigengraph u1u⊤ for all entries (Shuman et al., 2013). Thus, for SGC, we have n\n(cid:88)\n\n1 + . . . + λiuiu⊤\n\nn\n\n ̃S = I − ̃Lsym = ̃U(I − ̃Λ) ̃U⊤ =\n\n(1 − ̃λi) ̃ui ̃u⊤ i .\n\n(4)\n\nA SGC with d layers requires d consecutive graph convolution operations, which involves the multiplication of ̃S by d times. Due to the orthogonality of ̃U, namely, ̃U⊤ ̃U = I, we obtain\n\ni=0\n\n ̃Sd = ̃U(I − ̃Λ) ̃U⊤ ̃U(I − ̃Λ) ̃U⊤ . . . ̃U(I − ̃Λ) ̃U⊤ = ̃U(I − ̃Λ)d ̃U⊤ =\n\nn (cid:88)\n\n(1 − ̃λi)d ̃ui ̃u⊤\n\ni , (5)\n\nwhere 1 − ̃λi ∈ (−1, 1], and the depth d of SGC serves as the power of ̃S’s eigenvalues. ̃Sd can be viewed as the sum of eigengraphs ̃ui ̃u⊤\n\ni weighted by coefficients (1 − ̃λi)d.\n\ni=1\n\nGraph homophily and heterophily. Graph homophily describes to what extent edges tend to link nodes with the same labels and similar features. In this work, we focus on edge homophily (Zhu\n\net al., 2020): h(G) =\n\n∈ [0, 1], where ⟨x⟩ = 1 if x is true and 0 otherwise. A\n\n(cid:80)\n\ni,j,A[i,j]=1⟨y[i]=y[j]⟩ i,j A[i,j]\n\n(cid:80)\n\ngraph is more homophilic for h(G) closer to 1 or more heterophilic for h(G) closer to 0.\n\n3 MODEL\n\nFirstly, we establish the intrinsic connections between eigengraphs with small/large weights, graph signals with high/low frequencies, and graphs with homophilic/heterophilic properties. Secondly, we\n\n1This work focuses on connected graph without bipartite components (i.e., a connected component which is\n\na bipartite graph).\n\n3\n\n111v!v\"v#v!v\"v#normalization121212=v!v\"v#1−λ!=1,(λ!=0)131313131313v!v\"v#−1316−13231616v!v\"v#1−λ\"=−12,(λ\"=32)−121212++AS=D!\"#AD!\"#eigengraph1eigengraph2eigengraph31−λ#=−12,(λ#=32)Under review as a conference paper at ICLR 2023\n\nshow how the positive/negative depth d of a GCN affects the eigengraph weights and in turn determines the algorithm’s expressive power to process homophilic/heterophilic graph signals. Thirdly, with the help of functional calculus (Shah & Okutmu ̧stur, 2020), we present the theoretical feasibility of extending the domain of d from N+ to R. Finally, by making d a trainable parameter, we present our model RED-GCN and its variants, which are capable of automatically detecting the homophily/heterophily of the input graph and finding the corresponding optimal depth.\n\nThe eigenvectors of a graph Laplacian form a complete set of basis vectors in the n-dimensional space capable of expressing the original node attribute X as a linear combination. From the perspective of graph spectrum analysis (Shuman et al., 2013), the frequency of eigenvector ui reflects how much the j-th entry ui[j] deviates from the k-th entry ui[k] for each connected node pair vj and vk in G. This deviation is measured by the set of zero crossings of ui: Z(ui) := {e = (vj, vk) ∈ E : ui[j]ui[k] < 0}, where E is the set of edges in graph G. Larger/smaller |Z(ui)| indicates higher/lower eigenvector frequency. A zero-crossing also corresponds a negative weighted edge in an eigengraph. Due to the widely existing positive correlation between λi and |Z(ui)| (Shuman et al., 2013), large/small eigenvalues mostly correspond to the high/low frequencies of the related eigenvectors. As illustrated by the toy example of n = 3 in Figure 1, for λ1 = 0, we have |Z(u1)| = 0, and eigengraph u1u⊤ is well-connected with identical edge weight 1 n ; negative edge weights exist in the 2nd and 3rd eigengraphs, indicating more zero crossings (|Z(u2)| = 1 and |Z(u3)| = 2) and higher eigenvector frequencies.\n\n1\n\nSince node labels correlate with their attributes (Zheng et al., 2022a), and node attribute similarities indicate the extent of smoothness/homophily (Luan et al., 2020; 2021), plus node attributes can be expressed by eigenvectors, the deviation between eigenvector entry pairs naturally implies the extent of heterophily. Apparently, high frequency eigenvectors and their corresponding eigengraphs have advantage on capturing graph heterophily. High frequency eigengraphs should accordingly take larger weights when modeling heterophilic graphs, while low frequency ones should carry larger weights when dealing with homophilic graphs. In turn, eigengraph weights are controlled by GCN/SGC’s depth d, e.g., for a SGC of depth d, the weight of the i-th eigengraph is (1 − ̃λi)d, and changing the layer d of SGC adjusts the weights of different eigengraphs. Therefore, depth d controls the model’s expressive power to effectively filter low/high-frequency signals for graph homophily/heterophily.\n\nA question is naturally raised: instead of manually setting the depth d, can d be built into the model as a trainable parameter so that a proper set of the eigengraph weights matching the graph homophily/heterophily can be automatically reached by finding the optimal d in an end-to-end fashion during training? Differentiable variables need continuity, which requires the extension of depth d from the discrete positive integer domain (N+) to the continuous real number domain R. According to functional calculus (Shah & Okutmu ̧stur, 2020), applying an arbitrary function f on a graph Laplacian Lsym is equivalent to applying the same function only on the eigenvalue matrix Λ:\n\nf (Lsym) = Uf (Λ)U⊤ = U\n\n\n\n \n\nf (λ1) ...\n\n0\n\n· · · . . . · · ·\n\n\n\n0\n\n... f (λn)\n\n\n\n U⊤,\n\n(6)\n\ni (d ∈ R).\n\ni=1(1 − ̃λi)d ̃ui ̃u⊤\n\nwhich also applies to ̃Lsym and ̃S. Armed with this, we seek to realize an arbitrary depth SGC via a power function as f ( ̃S) = ̃Sd = ̃U(I − ̃Λ)d ̃U⊤ = (cid:80)n However, since ̃λi ∈ [0, 2), we have (1 − ̃λi) ≤ 0 when 1 ≤ ̃λi < 2, and for (1 − ̃λi) taking zero or negative values, (1 − ̃λi)d is not well-defined or involving complex-number-based calculations for a real-valued d (e.g.,(−0.5) 3 (Shah & Okutmu ̧stur, 2020). Moreover, even for integer-valued ds under which (1 − ̃λi)d is easy to compute, the behavior of (1 − ̃λi)d is complicated versus ̃λi and diverges when ̃λi = 1 for negative ds, as shown in Figure 2a. Thus, the favored weight distribution may be hard to obtain by tuning d.\n\nFigure 2: Eigengraph weight versus eigenvalue for (a) SGC and (b) RED-GCN-S under different depth ds.\n\n(a) (1 − ̃λ)d.\n\n(b) (1 − 1\n\n2 λ)d\n\n8 )\n\n4\n\n0.00.51.01.5432101234(1)dd=2d=1d=0d=1d=20.00.51.01.50.00.51.01.52.02.53.03.54.0(10.5)dd=2d=1d=0d=1d=2Under review as a conference paper at ICLR 2023\n\nTo avoid such complications and alleviate the difficulties for manipulating the eigengraph weights, a transformation function g(·) operating on the graph Laplacian Lsym or ̃Lsym is in need to shift g(λi) or g( ̃λi) into a proper value range so that its power of a real-valued d is easy to obtain and well-behaved versus λi or ̃λi. Without the loss of generality, our following analysis focuses on Lsym and λi. There may exist multiple choices for g(·) satisfying the requirements. In this work, we focus on the following form:\n\nˆS = g(Lsym) =\n\n1 2\n\n(2I − Lsym).\n\n(7)\n\nThis choice of g(·) holds three properties: (1) Positive eigenvalues. Since we have Lsym’s i-th eigenvalue λi ∈ [0, 2), the corresponding eigenvalue of ˆS is g(λi) = 1 2 (2 − λi) ∈ (0, 1]. Thus, the d-th power of g(λi) is computable for any d ∈ R. (2) Monotonicity versus eigenvalues λ. As shown in Figure 2b, g(λi)d = (1 − 1 2 λ)d is monotonically increasing/decreasing when λ varies between 0 and 2 under negative/positive depth. (3) Geometric interpretability. Filter ˆS can be expressed as:\n\nˆS = U ˆΛU⊤ = U(I −\n\n1 2\n\nΛ)U⊤ =\n\n1 2\n\nI +\n\n1 2\n\n(I − Lsym) =\n\n1 2\n\n(I + D− 1\n\n2 AD− 1\n\n2 ).\n\n(8)\n\nAs shown in Figure 3, in spatial domain, ˆS is obtained via 3 operations on adjacency matrix A: normalization, adding self-loops, and scaling all edge weights by 1 2 (a type of lazy random walk (Luan et al., 2020)), while ̃S in vanilla GCN/SGC contains 2 operations: adding self-loops and normalization.\n\nWith the help of transformation g, the depth d is redefined on real number domain, and the message propagation process of depth d can be realized via the following steps: (1) Eigen-decompose Lsym; (2) Calculate ˆSd via weight g(λi)d and the weighted sum of all eigengraphs: ˆSd = U ˆΛdU⊤ = (cid:80)n\n\ni (3) Multiply ˆSd with original node attributes X.\n\ni=1 g(λi)duiu⊤\n\nNegative depth explained. An intuitive explanation of negative d can be obtained from the perspective of matrix inverse and message diffusion process when d takes integer values. Since ˆS−1 ˆS = U ˆΛ−1U⊤U ˆΛ1U⊤ = I, ˆS−1 is the inverse matrix of ˆS. In diffusion dynamics, X can be viewed as an intermediate state generated in a series of message propagation steps. ˆSX effectively propagates the message one-step forward, while ˆS−1 can cancel the effect of ˆS on X and recover the original message by moving backward: ˆS−1 ˆSX = X. Accordingly, ˆS−1X traces back to the message’s previous state in the series. However, neither A or L has inverse due to their non-positive eigenvalues. More discussions on the impact of negative depth in spatial domain are presented in Section 4.4. Non-integer d indicates the back- or forward propagation can be a continuous process.\n\nFigure 3: The difference between ̃S (left) for GCN/SGC and ˆS (right) for RED-GCN.\n\nRED-GCN-S. By further making d a trainable parameter, we present our model, Redefined DepthGCN-Single (RED-GCN-S), whose final node embedding matrix is given by\n\nH = σ(ˆSdXW), where σ(·) is the nonlinear activation function; W is a trainable parameter matrix; and d is the trainable depth parameter. As observed from Figure 2b, weight distribution of different frequencies/eigengraphs is tuned via d: (1) for d = 0, the weight is uniformed distributed among all frequency components (g(λi)d = 1), which implies that no particular frequency is preferred by the graph signal; (2) for d > 0, weight g(λi)d decreases with the corresponding frequency, which indicates the low frequency components are favored so that RED-GCN-S effectively functions as a low-pass filter and therefore captures graph homophily; (3) for d < 0, high frequency components gains amplified weights so that RED-GCN-S serves as a high-pass filter capturing graph heterophily. During training, RED-GCN-S tunes its frequency filtering functionality to suit the underlying graph signal by automatically finding the optimal d.\n\n(9)\n\nRED-GCN-D. During optimization, RED-GCN-S embraces a single depth d unified for all eigengraphs and selects its preferences for either homophily or heterophily. However, RED-GCN-S\n\n5\n\nv!v\"v#v!v\"v#141414121212v!v\"v#11112D!\"#AD!\"#12I!Sv!v\"v#131313131313#S111Under review as a conference paper at ICLR 2023\n\nrequires a full eigen-decomposition of Lsym, which can be expensive for large graphs. Additionally, the high and low frequency components in a graph signal may not be mutually exclusive, namely, there exists the possibility for a graph to simultaneously possess homophilic and heterophilic counterparts. Therefore, we propose the second variant of RED-GCN: RED-GCN-D (Dual), which introduces two separate trainable depths, dh and dl, to gain more flexible weighting of the high and low frequency related eigengraphs respectively. Arnoldi method (Lehoucq et al., 1998) is adopted to conduct EVD on Lsym and obtain the top-K largest and smallest eigen-pairs (λi, ui)s. By denoting Ul = U[:, 0 : K] and Uh = U[:, n − K : n] (Ul and Uh ∈ Rn×K), we define a new diffusion matrix ˆSdual(dl, dh, K) as\n\nˆSdual(dl, dh, K) = Ul ˆΛdl\n\nl U⊤\n\nl + Uh ˆΛdh\n\nh U⊤ h ,\n\n(10)\n\nwhere ˆΛl ∈ RK×K and ˆΛh ∈ RK×K are diagonal matrices of the top-K smallest and largest eigenvalues. 2 The final node embedding of RED-GCN-D is presented as\n\nH = σ(ˆSdual(dl, dh, K)XW),\n\n(11)\n\nwhere depths dl and dh are trainable; and W is a trainable parameter matrix. We make RED-GCN-D scalable on large graphs by choosing K ≪ n, so that ˆSdual(dl, dh, K) approximates the full diffusion matrix by covering only a small subset of all eigengraphs. For small graphs, we use K = ⌊ n 2 ⌋ to include all eigengraphs, and ˆSdual(dl, dh, K) thus gains higher flexibility than ˆS with the help of the two separate depth parameters instead of a unified one.\n\nDifferences with ODE-based GNNs. Previous attempts on GNNs with continuous diffusion are mostly inspired by graph diffusion equation, an Ordinary Difference Equation (ODE) characterizing the dynamical message propagation process versus time. In contrast, our framework starts from discrete graph convolution operations without explicitly involving ODE. CGNN (Xhonneux et al., 2020) aims to build a deep GNN immune to over-smoothing by adopting the neural ODE framework (Chen et al., 2018). But its time parameter t is a non-trainable hyper-parameter predefined within the positive domain, which is the key difference with RED-GCN. A critical CGNN component for preventing over-smoothing, restart distribution (the skip connection from the first layer), is not needed in our framework. Moreover, CGNN applies the same depth to all frequency components, while RED-GCN-D has the flexibility to adopt two different depths respectively to be adaptive to high and low frequency components. GRAND (Chamberlain et al., 2021) introduces non-Eular multi-step schemes with adaptive step size to obtain more precise solutions of the diffusion equation. Its depth (total integration time) is continuous but still predefined/non-trainable and takes only positive values. DGC (Wang et al., 2021) decouples the SGC depth into two prefinded non-trainable hyper-parameters: a positive real-valued T controlling the total time and a positive integer-valued Kdgc corresponding to the number of diffusion steps. However, realizing negative depth in DGC is non-applicable since the implementation is through propagation by Kdgc times, rather than through an arbitrary real-valued exponent d on eigengraph weights in RED-GCN.\n\n4 EXPERIMENT\n\nIn this section, we evaluate the proposed RED-GCN on the semi-supervised node classification task on both homophilic graphs and heterophilic graphs.\n\n4.1 EXPERIMENT SETUP\n\nDatasets. We use 11 datasets for evaluation, including 4 homophilic graphs: Cora (Kipf & Welling, 2016), Citeseer (Kipf & Welling, 2016), Pubmed (Kipf & Welling, 2016) and DBLP (Bojchevski & Günnemann, 2017), and 7 heterophilic graphs: Cornell (Pei et al., 2020), Texas (Pei et al., 2020), Wisconsin (Pei et al., 2020), Actor (Pei et al., 2020), Chameleon (Rozemberczki et al., 2021), Squirrel (Rozemberczki et al., 2021), and cornell5 (Fey & Lenssen, 2019). We collect all datasets from the public GCN platform Pytorch-Geometric (Fey & Lenssen, 2019). For Cora, Citeseer, Pubmed with data splits in Pytorch-Geometric, we keep the training/validation/testing set split as in GCN (Kipf & Welling, 2016). For the remaining 8 datasets, we randomly split every dataset into 20/20/60% for training, validation, and testing. The statistics of all datasets are presented in Appendix.\n\n2Case λi = 2, namely g(λi) = 0, is excluded since it corresponds to the existence of bipartite components.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nBaselines and Metrics. We compare our model with 7 baseline methods,including 4 classic GNNs: GCN (Kipf & Welling, 2016), SGC (Wu et al., 2019), APPNP (Klicpera et al., 2018) and ChebNet (Defferrard et al., 2016), and 3 GNNs tailored for heterophilic graphs: FAGCN (Bo et al., 2021), GPRGNN (Chien et al., 2020) and H2GCN (Zhu et al., 2020). Accuracy (ACC) is used as the evaluation metric. We report the average ACCs with the standard deviation (std) for all methods, each obtained by 5 runs with different initializations.\n\nImplementation Details. See Appendix due to the page limit.\n\n4.2 NODE CLASSIFICATION\n\nThe semi-supervised node classification performances on homophilic graphs and heterophilic graphs are shown in Table 1 and Table 2 respectively.\n\nHomophilic graphs. From Table 1, it is observed that different methods have similar performance on homophilic graphs. RED-GCN-S achieves the best accuracies on two datasets: Cora and DBLP. On the remaining two datasets, RED-GCN-S is only 1.1% and 0.4% below the best baselines (APPNP on Citeseer and SGC on Pubmed). For RED-GCN-D, it obtains similar performance as the other methods, even though it only uses the top-K largest/smallest eigen-pairs.\n\nTable 1: Performance comparison (mean±std accuracy) on homophilic graphs.\n\nDatasets GCN SGC APPNP GPRGNN FAGCN H2GCN ChebNet RED-GCN-S RED-GCN-D\n\nCora 80.8±0.8 80.9±0.4 81.0±1.0 82.0±0.7 80.3±0.4 78.8±1.0 78.8±0.5 82.5±1.1 82.4 ±0.7\n\nCiteseer 70.5±0.6 70.8±0.8 71.9±0.4 69.3±0.9 71.7±0.8 70.5±1.0 71.1±0.4 70.8±0.7 70.6 ±0.6\n\nPubmed 78.8±0.6 79.6±0.4 79.3±0.2 78.6±0.7 78.5±0.9 77.9±0.3 78.1±0.8 79.2±0.2 77.9 ±0.3\n\nDBLP 84.1±0.2 84.1±0.2 83.0±0.5 84.5±0.3 82.4±0.7 82.4±0.3 83.1±0.1 84.7±0.3 84.2±0.2\n\nTable 2: Performance comparison (mean±std accuracy) on heterphilic graphs.\n\nDatasets GCN SGC APPNP GPRGNN FAGCN H2GCN ChebNet RED-GCN-S RED-GCN-D\n\nTexas 55.9±3.4 58.7±3.1 55.1±3.7 61.3 ±5.8 60.2±7.8 68.8±6.5 76.2±2.9 77.6±5.9 77.1 ±2.5\n\nCornell 44.3±4.4 43.8±4.4 51.5±2.4 53.3±4.6 54.8±7.4 61.4±4.4 66.7±3.9 72.0±5.8 72.0 ±2.8\n\nWisconsin 51.4±2.2 47.3±2.1 58.0±3.1 71.0±4.8 60.1±5.2 69.9±5.3 75.4±3.5 82.0±2.6 81.5 ±2.4\n\nActor 27.5±0.5 28.0±0.8 32.8±0.8 33.6±0.4 32.3±0.5 33.9±0.3 34.3±0.5 35.3±0.7 27.6 ±0.8\n\nSquirrel 35.8±1.3 37.2±1.8 29.5±0.9 34.1±1.0 31.2±1.6 30.4±0.9 31.8±0.5 38.2±1.2 44.2±0.9\n\nChameleon 55.2±1.8 55.3±1.0 46.7±0.8 55.0±3.9 50.4±1.9 48.8±1.9 49.6±1.8 55.7±1.3 56.9±0.9\n\ncornell5 67.9±0.2 67.4±0.5 68.3±0.5 67.3±0.3 68.3±0.7 68.4±0.2 OOM 68.5±0.4 70.0±0.2\n\nHeterophilic graphs. RED-GCN-S/RED-GCN-D outperforms every baseline on all heterophilic graphs, as shown in Table 2. These results demonstrate that without manually setting the model depth and without the prior knowledge of the input graph, RED-GCN has the capability of automatically detecting the underlying graph heterophily. We have an interesting observation: on 3 large datasets, Squirrel, Chameleon, and cornell5, even with only a small portion of the eigengraphs, RED-GCN-D is able to achieve better performance than RED-GCN-S with the complete set of eigengraphs. This suggests that the graph signal in some real-world graphs might be dominated by a few low and high frequency components, and allowing two independent depth parameters in RED-GCN-D brings the flexibility to capture the low and high frequencies at the same time.\n\n4.3\n\nTRAINABLE DEPTH\n\nA systematic study is conducted on the node classification performance w.r.t the trainable depth d.\n\nOptimal depth. In Figure 4, the optimal depths and their corresponding classification accuracies are annotated. For two homophilic graphs, Cora and Citeseer, the optimal depths are positive (5.029 and 3.735) in terms of the best ACCs, while for two heterophilic graphs, Actor and Squirrel, the optimal depths are negative (−0.027 and −3.751). These results demonstrate our model indeed automatically capture graph heterophily/homophily by finding the suitable depth to suppress or amplify the relative weights of the corresponding frequency components. Namely, high/low frequency components are suppressed for homophilic/heterophilic graphs respectively.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Cora\n\n(b) Citeseer\n\n(c) Actor\n\n(d) Squirrel\n\nFigure 4: Node classification accuracy w.r.t. the trainable depth d on four datasets: Cora, Citeseer, Actor and Squirrel. (the optimal d, accuracy) is annotated (e.g., (-0.027, 36.9%) for Actor).\n\nClose to zero depth. For the two homophilic graphs in Figures 4a and 4b, sharp performance drop is observed when depth d approaches 0, since the eigengraphs gain close-to-uniform weights. For the heterophilic Actor dataset, its optimal depth −0.027 is close to 0, as shown in Figure 4c. In addition, the performance of RED-GCN-D (27.6%) is similar to that of GCN (27.5%), both of which are much worse than RED-GCN-S (35.3%). This result indicates that Actor is a special graph where all frequency components have similar importance. Due to the absence of the intermediate frequency components between the high- and low-end ones, the performance of RED-GCN-D is severely impacted. For vanilla GCN, the suppressed weights of the high frequency components deviate from the near-uniform spectrum and thus lead to low ACC on this dataset.\n\n4.4 GRAPH AUGMENTATION AND GEOMETRIC INSIGHTS\n\nTable 3: The performance of one-layer vanilla GCN over the augmented ˆSd.\n\nIt is especially interesting to analyze what change a negative depth brings to the spatial domain and how such change impacts the subsequent model performance. Graph augmentation. By picking the optimal depth d according to the best performance on the validation set, a new diffusion matrix ˆSd is obtained. With the optimal d fixed, 2 ̃A ̃D− 1 substituting the normalized adjacency matrix ̃D− 1 in Eq. 1 by ˆSd is equivalent to applying the vanilla GCN to a new topology. This topology effectively plays the role of a structural augmentation for the original graph. The impact of such augmentation on performance is tested on 3 heterophilic graphs: Texas, Cornell and Wisconsin, as shown in Table 3. Apparently, for the vanilla GCN, the performance obtained with this new topology is superior over that with the raw input graph: it dramatically brings 20%-30% lifts in ACC. Moreover, the augmented topologies also make vanilla GCN outperform RED-GCN-S and RED-GCN-D on 2 out of the 3 datasets. By nature, the augmented graph is a re-weighted linear combination of the eigengraphs, and its topological structure intrinsically assigns higher weights to eigengraphs corresponding to higher frequencies, as shown in Figures 5.\n\nDatasets GCN RED-GCN-S RED-GCN-D GCN (ˆSd)\n\nTexas 55.9 77.6 77.1 75.9\n\nCornell Wisconsin\n\n51.4 82.0 81.5 83.4\n\n44.3 72.0 72.0 72.7\n\n2\n\nGeometric properties. To further understand how the topology of ˆSd with a negative optimal d differs from that of ˆS and why the performance is significantly boosted, a heat map of (ˆSd − ˆS) is presented in Figure 6 for Cornell. 3 First, the dark red diagonal line in the heat map indicates the weights of self-loops are significantly strengthened in the augmented graph, and as a result, in consistency with the previous findings (Zheng et al., 2022a), the raw node attributes make more contributions in determining their labels. These strengthened self-weights also play the similar role as restart distribution or skip connections (Xhonneux et al., 2020) preventing the node embeddings becoming over-smoothed. In addition, there is a horizontal line and a vertical line (light yellow line marked by dashed ovals) in the heat map in Figure 6, correspond to the hub node in the graph, namely the node with the largest degree. Interestingly, the connections between this node and most other nodes in the graph experience a negative weight change. Therefore, the influence of the\n\nFigure 5: The weights of eigengraphs w.r.t. eigenvalues on the augmented diffusion matrix ˆSd and original ˆS for Cornell (d = −0.362).\n\n3Heat maps for Texas and Wisconsin are in Appendix with similar observations.\n\n8\n\n012345d505560657075808590Accuracy(%)the optimal (5.029, 82.8%)Performance on Cora012345d50556065707580Accuracy(%)the optimal (3.735, 71.4%)Performance on Citeseer101d202530354045Accuracy(%)the optimal (-0.027, 36.9%)Performance on Actor3.93.83.73.63.53.43.3d20253035404550Accuracy(%)the optimal (-3.751,41.5%)Performance on Squirrel0.00.20.40.60.81.01.21.41.61.80.00.51.01.52.02.53.0WeightsSdSUnder review as a conference paper at ICLR 2023\n\nhub node on most other nodes are systematically reduced. Consequently, the augmentation amplifies the deviations between node embeddings and facilitates the characterization of graph heterophily.\n\n5 RELATED WORKS\n\nGraph Convolutional Network (GCN). GCN models can be mainly divided into two categories: (1) spectral graph convolutional networks and (2) spatial convolutional networks. In (1), Spectral CNN (Bruna et al., 2013) borrows the idea from convolutional neural network (Goodfellow et al., 2016) to construct a diagonal matrix as the convolution kernel. ChebNet (Defferrard et al., 2016) adopts a polynomial approximation of the convolution kernel. GCN (Kipf & Welling, 2016) further simplifies the ChebNet via the first order approximation. Recently, (He et al., 2021; Bianchi et al., 2021; Wang & Zhang, 2022) propose more advanced filters as the convolution kernel. Most works in (2) follow the message-passing mechanism. GraphSAGE (Hamilton et al., 2017) iteratively aggregates features from local neighborhood. GAT (Veliˇckovi ́c et al., 2017) applies self-attention to the neighbors. APPNP (Klicpera et al., 2018) deploys personalized pagerank (Tong et al., 2006) to sample nodes for aggregration. MoNet (Monti et al., 2017) unifies GCNs in the spatial domain.\n\nFigure 6: The difference between the augmented diffusion matrix and the original one ˆSd − ˆS for Cornell in heat map. Best viewed in color.\n\nThe Depth of GCN and Over-smoothing. A large amount of works focus on the over-smoothing issue. Its intrinsic cause is demystified: a linear GCN layer is a Laplacian smoothing operator (Li et al., 2018; Wu et al., 2019). PairNorm (Zhao & Akoglu, 2019) forces distant nodes to be distinctive by adding an intermediate normalization layer. Dropedge (Rong et al., 2019), DeepGCN (Li et al., 2019), AS-GCN (Huang et al., 2018), and JK-net (Xu et al., 2018) borrow the idea of ResNet (He et al., 2016) to dis-intensify smoothing. DeeperGXX (Zheng et al., 2021) adopts a topology-guided graph contrastive loss for connected node pairs to obtain discriminative representations. Most works aim to build deep GCNs (i.e., d is a large positive integer) by reducing over-smoothing, while RED-GCN extends the depth from N+ to R and explores the negative depth.\n\nNode Classification on Homophilic and Heterophilic Graphs. GCN/GNN models mostly follow the homophily assumption that connected nodes tend to share similar labels (Kipf & Welling, 2016; Veliˇckovi ́c et al., 2017; Hamilton et al., 2017). Recently, heterophilic graphs, in which neighbors often have disparate labels, attract lots of attention. Geom-GCN (Pei et al., 2020) and H2GCN (Zhu et al., 2020) extend the neighborhood for aggregation. FAGCN (Bo et al., 2021) and GPRGNN (Chien et al., 2020) adaptively integrate the high/low frequency signals with trainable parameters. Alternative message-passing mechanisms have been proposed in HOG-GCN (Wang & Zhang, 2022) and CPGNN (Zhu et al., 2021). The latest related works include ACM-GCN (Luan et al., 2021; 2022b), LINKX (Lim et al., 2021), BernNet (He et al., 2021), GloGNN (Li et al., 2022) and GBKGNN (Du et al., 2022). Other works can be found in a recent survey (Zheng et al., 2022b).\n\n6 CONCLUSION AND FUTURE WORK\n\nTo our best knowledge, this work presents the first effort to make GCN’s depth trainable by redefining it on the real number domain. We unveil the interdependence between negative GCN depth and graph heterophily. A novel problem of automatic GCN depth tuning for graph homophily/heterophily detection is formulated, and we propose a simple and powerful solution named RED-GCN with two variants (RED-GCN-S and RED-GCN-D). An effective graph augmentation method is also discussed via the new understanding on the message propagation mechanism generated by the negative depth. Superior performance of our method is demonstrated via extensive experiments with semi-supervised node classification on 11 graph datasets. The new insights on GCN’s depth obtained by our work may open a new direction for future research on spectral and spatial GNNs. Since RED-GCN requires to conduct eigen-decomposition of the graph Laplacian, it is not directly applicable to inductive and dynamic graph learning problems, which we leave for future exploration.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nFilippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. Graph neural networks with convolutional arma filters. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n\nDeyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph convolutional networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3950–3957, 2021.\n\nAleksandar Bojchevski and Stephan Günnemann. Deep gaussian embedding of graphs: Unsupervised\n\ninductive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.\n\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally\n\nconnected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.\n\nBen Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and In International Conference on Machine\n\nEmanuele Rossi. Grand: Graph neural diffusion. Learning, pp. 1407–1418. PMLR, 2021.\n\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\n\ndifferential equations. Advances in neural information processing systems, 31, 2018.\n\nZhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural\n\nnetworks. In International conference on learning representations, 2020.\n\nEli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank\n\ngraph neural network. arXiv preprint arXiv:2006.07988, 2020.\n\nFan RK Chung and Fan Chung Graham. Spectral graph theory, volume 92. American Mathematical\n\nSoc., 1997.\n\nMichaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems, 29, 2016.\n\nLun Du, Xiaozhou Shi, Qiang Fu, Xiaojun Ma, Hengyu Liu, Shi Han, and Dongmei Zhang. Gbkgnn: Gated bi-kernel graph neural networks for modeling both homophily and heterophily. In Proceedings of the ACM Web Conference 2022, pp. 1550–1558, 2022.\n\nMatthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. IEEE\n\nSignal Processing Magazine, 30:83–98, 2019.\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n\nAdvances in neural information processing systems, 30, 2017.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nMingguo He, Zhewei Wei, Hongteng Xu, et al. Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. Advances in Neural Information Processing Systems, 34:14239–14251, 2021.\n\nWenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph\n\nrepresentation learning. Advances in neural information processing systems, 31, 2018.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.\n\narXiv preprint arXiv:1609.02907, 2016.\n\nJohannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph\n\nneural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRichard B Lehoucq, Danny C Sorensen, and Chao Yang. ARPACK users’ guide: solution of large-\n\nscale eigenvalue problems with implicitly restarted Arnoldi methods. SIAM, 1998.\n\nGuohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep In Proceedings of the IEEE/CVF international conference on computer vision, pp.\n\nas cnns? 9267–9276, 2019.\n\nQimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI conference on artificial intelligence, 2018.\n\nXiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, and Weining Qian. Finding global homophily in graph neural networks when meeting heterophily. arXiv preprint arXiv:2205.07308, 2022.\n\nDerek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim. Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances in Neural Information Processing Systems, 34:20887–20902, 2021.\n\nSitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina Precup. Break the ceiling: Stronger multiscale deep graph convolutional networks. Advances in neural information processing systems, 32, 2019.\n\nSitao Luan, Mingde Zhao, Chenqing Hua, Xiao-Wen Chang, and Doina Precup. Complete the missing half: Augmenting aggregation filtering with diversification for graph convolutional networks. arXiv preprint arXiv:2008.08844, 2020.\n\nSitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen Chang, and Doina Precup. Is heterophily a real nightmare for graph neural networks to do node classification? arXiv preprint arXiv:2109.05641, 2021.\n\nSitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Xiao-Wen Chang, and Doina Precup. When do\n\nwe need gnn for node classification? arXiv preprint arXiv:2210.16979, 2022a.\n\nSitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen Chang, and Doina Precup. Revisiting heterophily for graph neural networks. arXiv preprint arXiv:2210.07606, 2022b.\n\nFederico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5115–5124, 2017.\n\nHongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric\n\ngraph convolutional networks. arXiv preprint arXiv:2002.05287, 2020.\n\nYu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph\n\nconvolutional networks on node classification. arXiv preprint arXiv:1907.10903, 2019.\n\nBenedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal\n\nof Complex Networks, 9(2):cnab014, 2021.\n\nKamal Shah and Baver Okutmu ̧stur. Functional Calculus. BoD–Books on Demand, 2020.\n\nDavid I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE signal processing magazine, 30(3):83–98, 2013.\n\nGabriel Taubin. A signal processing approach to fair surface design. In Proceedings of the 22nd\n\nannual conference on Computer graphics and interactive techniques, pp. 351–358, 1995.\n\nHanghang Tong, Christos Faloutsos, and Jia-Yu Pan. Fast random walk with restart and its applications.\n\nIn Sixth international conference on data mining (ICDM’06), pp. 613–622. IEEE, 2006.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\n\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nXiyuan Wang and Muhan Zhang. How powerful are spectral graph neural networks. arXiv preprint\n\narXiv:2205.11172, 2022.\n\nYifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Dissecting the diffusion process in linear graph convolutional networks. Advances in Neural Information Processing Systems, 34: 5758–5769, 2021.\n\nFelix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International conference on machine learning, pp. 6861–6871. PMLR, 2019.\n\nLouis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In International\n\nConference on Machine Learning, pp. 10432–10441. PMLR, 2020.\n\nKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International conference on machine learning, pp. 5453–5462. PMLR, 2018.\n\nZhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pp. 40–48. PMLR, 2016.\n\nMuhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in neural\n\ninformation processing systems, 31, 2018.\n\nLingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International\n\nConference on Learning Representations, 2019.\n\nLecheng Zheng, Dongqi Fu, and Jingrui He. Tackling oversmoothing of gnns with contrastive\n\nlearning. arXiv preprint arXiv:2110.13798, 2021.\n\nWenqing Zheng, W Edward Huang, Nikhil Rao, Sumeet Katariya, Zhangyang Wang, and Karthik Subbian. Cold brew: Distilling graph node represen- tations with incomplete or missing neighborhoods. In International Conference on Learning Representations, 2022a.\n\nXin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. Graph neural networks for\n\ngraphs with heterophily: A survey. arXiv preprint arXiv:2202.07082, 2022b.\n\nJiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 33:7793–7804, 2020.\n\nJiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai Koutra. Graph neural networks with heterophily. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 11168–11176, 2021.\n\n12",
    "reference": "# Summary Of The Paper\n\nIn this work, by redefining GCN’s depth d as a trainable parameter continuously adjustable within positive infinity and negative infinity, a simple and powerful GCN model RED-GCN is proposed to retain the simplicity of GCN and meanwhile automatically search for the optimal d without the prior knowledge regarding whether the input graph is homophilic or heterophilic.\n\n# Strength And Weaknesses\n\nStrength: The problem is interesting and usefull for GCN designing.\nWeaknesses: The motivation and the physical meaning of the negative depth value are not clear. When the depth of GCN is negative, what dose it mean?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe problem is interesting and the proposed method is novel. But the physical meaning of negative results is not clear.\n\n# Summary Of The Review\n\nThe problem is interesting and the proposed method is novel. But the physical meaning of negative results is not clear.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nBSTT: A BAYESIAN SPATIAL-TEMPORAL TRANSFORMER FOR SLEEP STAGING\n\nYuchen Liu Institute of Automation, CAS University of Chinese Academy of Sciences Institute of Computing Technology, CAS yuchen.liu.eric@outlook.com\n\nZiyu Jia∗ Institute of Automation, CAS University of Chinese Academy of Sciences jia.ziyu@outlook.com\n\nABSTRACT\n\nSleep staging is helpful in assessing sleep quality and diagnosing sleep disorders. However, how to adequately capture the temporal and spatial relations of the brain during sleep remains a challenge. In particular, existing methods cannot adaptively infer spatial-temporal relations of the brain under different sleep stages. In this paper, we propose a novel Bayesian spatial-temporal relation inference neural network, named Bayesian spatial-temporal transformer (BSTT), for sleep staging. Our model is able to adaptively infer brain spatial-temporal relations during sleep for spatial-temporal feature modeling through a well-designed Bayesian relation inference component. Meanwhile, our model also includes a spatial transformer for extracting brain spatial features and a temporal transformer for capturing temporal features. Experiments show that our BSTT outperforms state-ofthe-art baselines on ISRUC and MASS datasets. In addition, the visual analysis shows that the spatial-temporal relations obtained by BSTT inference have certain interpretability for sleep staging.\n\n1\n\nINTRODUCTION\n\nSleep staging is essential for assessing sleep quality and diagnosing sleep disorders. Sleep specialists typically classify sleep stages based on the AASM sleep standard and polysomnography (PSG) recordings to aid in diagnosis. The AASM standard not only provides criteria for determining each sleep period, but also documents conversion rules between different sleep stages, which is known as sleep transition rules, to help sleep specialists identify sleep stages when sleep transitions occur. However, artificial sleep staging takes a long time, and the classification results are greatly affected by professional level and subjectivity (Supratak et al., 2017). Therefore, automatic classification methods are applied into sleep staging to improve efficiency.\n\nTraditional machine learning methods use artificially designed features for sleep staging, which improves the efficiency of staging to a certain extent (Fraiwan et al., 2012). However, the accuracy of traditional machine learning methods relies heavily on feature engineering and feature selection, which still requires a lot of expert knowledge. To address the above problems, deep learning methods have been applied to sleep staging and achieved satisfactory classification performance (Phan et al., 2019; Jia et al., 2022a;b). Most of the early deep learning methods focus on the temporal information of the sleep data, utilizing convolutional neural networks (CNN) and recurrent neural networks (RNN) to capture temporal features for sleep staging (Jain & Ganesan, 2021; Perslev et al., 2019). In addition, some studies have shown that the spatial topology of the brain behave differently in different sleep stages (Khanal, 2019), which means that both the temporal and spatial relations of the brain are both important during sleep. Therefore, some researchers try to use the spatial and temporal characteristics of the brain for sleep staging (Jia et al., 2020b; Phan et al., 2022; Jia et al., 2020a). Although the above methods achieve good classification performance, it is challenging to model spatial and temporal relations. Specifically, for the modeling of temporal relations, some approaches attempt to capture sleep transition rules in sleep to serve the identification of specific sleep stages. However, it is difficult for these methods to explicitly demonstrate the relation of\n\n∗Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\ndifferent sleep time slices in accordance with the AASM sleep standard. Besides, for the modeling of spatial relations, spatial convolution operation is employed to extract the spatial features of the brain, which is insufficient that it may ignore the spatial topology of the brain by most methods (Zhou et al., 2021a; Perslev et al., 2019). A few researches utilize spatial topology and temporal relation information of brain for sleep staging by graph convolutional networks, but the constructed brain networks still lack interpretability to a certain extent (Jia et al., 2020b).\n\nTo address the above challenges, we propose a novel model called Bayesian spatial-temporal transformer (BSTT) for sleep staging. The proposed model integrates the transformer and Bayesian relation inference in a unified framework. Specifically, we design the spatial-temporal transformer architecture, which can capture the temporal and spatial features of the brain. Besides, we propose the Bayesian relational inference component which comes in two forms, Bayesian temporal relation inference and Bayesian spatial relation inference. Wherefore, it can infer the spatial-temporal relations of objects and generate the relation intensity graphs. Specifically, the main contributions of our BSTT are summarized as follows:\n\n• We design Bayesian relational inference component which can adaptively infer spatialtemporal relations of brain during sleep in the service of capturing spatial-temporal relations.\n\n• We apply the spatial-temporal transformer architecture to simultaneously model spatialtemporal relations. It can effectively capture the spatial-temporal features of the brain and enhance the model’s ability to model spatial-temporal relations.\n\n• Experimental results show that the proposed BSTT achieves the state-of-the-art in multiple sleep staging datasets. The visual analysis shows that our model has a certain degree of interpretability for sleep staging.\n\n2 RELATED WORK\n\nIdentifying sleep stages plays an important role in diagnosing and treating sleep disorders. Earlier, the support vector machine (SVM) and random forest (RF) are used for sleep staging (Fraiwan et al., 2012). However, these methods need hand-crafted features, which require a lot of prior knowledge. Currently, deep learning methods have become the primary method for sleep staging.\n\nEarly deep learning methods extract temporal features of sleep signals for classification. The earliest methods are based on the CNN models (Tsinalis et al., 2016; Chambon et al., 2018). For example, Chambon et al. propose a convolutional neural network that can extract temporal-invariant features from sleep signals (Chambon et al., 2018). Furthermore, Eldele et al. develope a multi-resolution CNN with adaptive feature recalibration to extract representative features (Eldele et al., 2021). In addition, RNN models have been gradually used for sleep staging (Phan et al., 2019; Perslev et al., 2019; Phan et al., 2018). For example, Phan et al. propose a deep bidirectional RNN model with attention mechanism for single-channel EEG (Phan et al., 2018). They then design an end-to-end hierarchical RNN architecture for capturing different levels of EEG signal features (Phan et al., 2019). Some studies combine CNN with RNN (Supratak & Guo, 2020; Guillot & Thorey, 2021; Dong et al., 2017). For example, Suratak et al. propose a hybrid model combining CNN and RNN to extract rich temporal features (Supratak et al., 2017). In addition, Phan et al. introduce transformer into the sleep staging task to capture the temporal context features of sleep signals (Phan et al., 2022). Jia et al. design a fully convolutional model to capture the typical waveform of sleep signals (Jia et al., 2021b).\n\nFurther, several studies have shown the importance of brain spatial relations for sleep staging (Khanal, 2019; Sakkalis, 2011). Some researchers try to model the spatial-temporal characteristics of sleep data. For example, Jia et al. propose an adaptive deep learning model for sleep staging. The proposed spatial-temporal graph convolutional network is used to extract spatial features and capture transition rules (Jia et al., 2020b). They also propose a multi-view spatial-temporal graph convolutional network based on domain generalization, which models the multi-view-based spatial characteristics of the brain (Jia et al., 2021a).\n\nAlthough the above models achieve good classification performance, these models do not adequately model spatial-temporal properties or effectively reason and capture spatial-temporal rela-\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ntions. Therefore, our method attempts to model spatial-temporal relations using Bayesian inference, combined with state-of-the-art transformer architectures for sleep staging.\n\n3 PRELIMINARIES\n\nThe proposed model processes data from successive T sleep epochs and predicts the label of the epoch in the middle. Each sleep epoch is defined as x ∈ RC×N , where C represents the number of channels of the sleep epoch (i.e. the EEG channel in this work. Since EEG signals from different channels are extracted from different regions of the brain, spatial relations are contained among these channels.) and N represents the number of sampling points in a sleep epoch. The input sequence of sleep epochs is defined as x = {x1, x2, . . . , xT }, where xi denotes a sleep epoch (i ∈ [1, 2, . . . , T ]) and T is the number of sleep epochs.\n\nThe sleep staging problem is defined as: learning an artificial neural network F based on Bayesian spatial-temporal transformer, which can infer the spatial-temporal relations of the input sleep epoch sequence x and map it to the corresponding sleep stage (cid:98)Y , where (cid:98)Y is the classification result of the middle epoch xm. According to the AASM standard, each (cid:98)Y ∈ {0, 1, 2, 3, 4} is matched to five sleep stages W, N1, N2, N3, and REM, respectively.\n\n4 BAYESIAN SPATIAL-TEMPORAL TRANSFORMER\n\nWe propose a novel model named Bayesian spatial-temporal transformer (BSTT) for sleep staging. The core ideas of our model are summarized as follows:\n\n• Infer the spatial-temporal relations of the brain based on Bayesian inference method.\n\n• Design the Bayesian transformer architecture while capturing the spatial-temporal features\n\nof the brain.\n\n• Integrate Bayesian relational inference components and transformer architecture into a uni-\n\nfied framework which can stage sleep period effectively.\n\nThe overall model is carefully designed to accurately classify different sleep stages.\n\n4.1 ARCHITECTURE\n\nThe overall architecture of the proposed BSTT is shown in Figure 1. The EEG signals are first encoded by the embedding layer. The spatial-temporal relations are then inferred and modeled by Bayesian spatial-temporal transformer module. Specifically, the Bayesian spatial-temporal transformer includes a Bayesian spatial transformer and a Bayesian temporal transformer. The Bayesian spatial transformer can reason about spatial relations in the brain and capture spatial features. The Bayesian temporal transformer can reason about the temporal relations of consecutive sleep epochs and capture temporal features. Finally, the predictions of different sleep stages are performed by the classification layer.\n\n4.1.1 BAYESIAN RELATION INFERENCE\n\nCapturing the spatial-temporal relations of brain signals during sleep can better serve sleep staging task. However, due to the difficulty in inferring the spatial-temporal relations of sleep, current researches are insufficient for modeling the spatial-temporal relations. Inspired by deep graph random process (DGP) proposed in recent research (Huang et al., 2020), we propose the Bayesian relation inference method. Bayesian relational inference is the core component of our model, which can infer relations between each pair of object nodes and build relation intensity graphs. In this article, an object node represents the embedding of an EEG channel or the embedding of a certain time slice. The construction of the relation intensity graph is mainly divided into the following steps:\n\nStep 1: Edge embedding initialization. The input of the Bayesian relational inference is the node embeddings of the object. Building a sleep relation intensity graph starts with generating edge embeddings. Specifically, the node embeddings of the object are spliced and generated into edge\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: The overall architecture of the proposed Bayesian spatial-temporal transformer for sleep staging. BSTT includes two Bayesian transformer modules, a spatial Bayesian transformer and a temporal Bayesian transformer. For each transformer module, the input features are passed through the position embedding and layernorm layer. Then the multi-head Bayesian relation inference component infers the object’s spatial or temporal relation and captures the spatial-temporal features. The residual connection is used to prevent overfitting and gradient disappearance. Here, ⊕ means add.\n\nembeddings. We first apply a linear neural network fθ to generate edge embeddings:\n\n(1) where En ∈ RB×Nv×V represents the input n node objects, En[i, j] ∈ RB×Ne×2V represents the splicing of two node embeddings, and Ee ∈ RB×Ne×E represents the generated edge embeddings.\n\n(i, j ∈ [1, n] && i ! = j)\n\nEe = fθ (En [i, j])\n\nStep 2: Edge embedding coupling. Coupling is one of the key steps in relation inference, the purpose is to obtain a summary graph of the relation between object nodes (Huang et al., 2020). We assume that the edges of the summary graph ̈Mi,j are a summary of an n → ∞ of λ → 0 Binomial distributions, which means ̈mi,j ∼ B(n, λ), due to the uncertainty of spatial-temporal relations of brain. Drawing from the Virtual Recurrent Neural Network (VRNN) model, the parameters of the approximate posteriors are estimated using a Recurrent Neural Network (RNN) to encode features. However, in contrast to VRNN, Bayesian relation inference component contains an approximate posterior q( ̈m|Ee), whose inference and sampling cannot be solved in a computationally feasible manner due to its infinite n. By De MoivreLaplace theorem (Sheynin, 1977) and DGP (Huang et al., 2020), we can subject these edge embeddings to a coupling transformation as follows:\n\nni,j = ζ (cid:0) Lmean (cid:101)σi,j = ζ (cid:0) Lstd 1 + 2ni,j ̃σ 2 i,j −\n\n(cid:0)Eei,j (cid:0)Eei,j (cid:113)\n\n(cid:1) (cid:1) + ε (cid:1) (cid:1)\n\n1 + 4n 2\n\ni,j ̃σ 4\n\ni,j\n\nmi,j =\n\n2\n\n(2)\n\n(3)\n\n(4)\n\nwhere ζ(·) is softplus function, Ee is the edge embedding generated in the first step, ε is a very small constant, Lmean(·) and Lstd(·) are implemented by neural networks for estimating the mean and standard deviation respectively, mi,j ∈ M is the approximation of Binomial edge variable in the summary graph, and M ∈ RB×Ne is the approximation of summary graph which strengthens the representation of real spatial or temporal relations.\n\nStep 3: Sleep relation intensity calculation. The final step is to strengthen edge information and generate a relation intensity graph for downstream tasks. The relation intensities of brain temporalspatial network during sleep is sparse based on existing research (Razi et al., 2017). Therefore,\n\n4\n\nEEG SignalsEmbedding LayerClassification LayerBayesian temporal transformerPosition EmbeddingLayerNormLayerNormMulti-head Bayesian temporal relational inference Temporal relation intensity graph based featuresLinearReLU,DropoutLinearResidualsBayesian spatial transformerPosition EmbeddingLayerNormLayerNormMulti-head Bayesian spatial relational inference Spatial relation intensity graph based featuresLinearReLU,DropoutLinearResidualsPublished as a conference paper at ICLR 2023\n\ngenerating a sparse graph based on edge embedding can not only highlight the representation of key relations, but also be more in line with the actual situation. We employ the Gaussian graph transformation approach which produces a sparse sleep relation intensity graph G. The specific calculation is defined as follows:\n\nsi,j = (cid:0) m mean\n\ni , j\n\n(cid:101)αi,j = (cid:0) mstd (cid:1) × (cid:101)αi,j + (cid:0)\n\ni,j\n\n(cid:1) × εi,j + mi,j\n\n(cid:101)αstd\n\ni,j\n\n(cid:1) × (cid:0)\n\n(cid:101)σ mean\n\ni , j\n\n(cid:1) × ε′\n\ni,j\n\n(5)\n\n(6)\n\n ̄αi,j = si,j × (cid:101)αi,j αi,j = ζ (L ( ̄αi,j) ) where mi,j ∈ M is the approximation of the edges of the summary graph obtained in Step 2, ε and ε′ are the standard Gaussian random variable of the same dimension as M , (cid:101)α ∈ RB×Ne is the Gaussian edge representation, S ∈ RB×Ne is the task-related Gaussian variable, ̃σi,j is calculated in Eq.(3), std is the standard deviation, mean is the mean value, ̄α is the Gaussian transformation map, α ∈ RB×Ne is the final sleep relation intensity graph, ζ(·) is softplus function, and L(·) is linear function. Afterwards, we utilize an attention mechanism-based method to convert the node embeddings into feature embeddings based on the sleep relation intensity graph as the output of Bayesian relation inference. The specific calculation is as follows:\n\n(8)\n\n(7)\n\nwhere fGAL(·) is the graph attention layer, En is the node embeddings of the input object, α is the sleep relation intensity graph, and Eout ∈ RB×Nv×Vout is the output node embeddings.\n\nEout = fGAL (En, α)\n\n(9)\n\n4.1.2 LEARNING OF BAYESIAN RELATION INFERENCE\n\nWe adopt variational inference to jointly optimise Baysian relation inference component. Inspired by VRNN (Chung et al., 2015), we can use the evidence lower bound (ELBO) for joint learning and inference. The details of how variational inference fits into our model are shown in the Appendix 3. Specifically, we use two random variables need to be optimised to describe the same random process data. The resulting objective is to maximize the ELBO:\n\n(cid:16)\n\n(cid:16) ̃A, S | X0:i\n\n(cid:17)\n\nq\n\n∥p\n\n(cid:16) ̃A, S | X0:i\n\n(cid:17)(cid:17)\n\nKL\n\n− E ̃A,S\n\n(cid:104)\n\nlog P\n\n(cid:16)\n\nYi | Xi, ̃A, S\n\n(cid:17)(cid:105) (cid:27)\n\n(10)\n\nM (cid:88)\n\n(cid:26)\n\ni=1\n\nwhere S is the task-related Gaussian variable, ̃A is the Gaussian graph embedding, q\n\n(cid:16) ̃A, S | X0:i\n\n(cid:17)\n\nis the prior distribution, and p are affected by ̃A in Eq.(6), the KL term can be further written as:\n\n(cid:16) ̃A, S | X0:i\n\n(cid:17)\n\nis the posterior distribution. Since every variable in S\n\n(cid:88)\n\n(cid:40)\n\n(cid:18)\n\nKL\n\nB( n, ̃λi,j )∥B\n\n(cid:17) (cid:19)\n\n(cid:16)\n\nn, ̃λ(0)\n\ni,j\n\n( i, j )∈ ̃E\n\n(cid:20)\n\n(cid:18)\n\nKL\n\n+E ̃αi,j\n\nN ( ̃αi,j ∗ μi,j, ̃αi,j ∗ σ2\n\ni,j∥N\n\n(cid:16)\n\n ̃αi,j ∗ μ(0)\n\ni,j , ̃αi,j ∗ σ(0) 2\n\ni,j\n\n(cid:17)(cid:19)(cid:21)(cid:41)\n\n(11)\n\nObviously the second term can be calculated, while the first term is tough to calculate because n → ∞. According to Theorem 2 provided in DGP (Huang et al., 2020), we can convert it into an easy-to-solve value to approximate the calculation.\n\n4.1.3 BAYESIAN TRANSFORMER MODULE\n\nTransformer shows convincing results in various sequence modeling tasks (Li et al., 2021; Luo et al., 2021; Zhou et al., 2021b). However, traditional transformer does not have the ability to reason the relation between each pair of object nodes. This results in a lack of interpretability of the attention graph generated by transformer. Besides, the accuracy of traditional transformer is not good enough under some medical scenarios. The Bayesian relation inference component we proposed can infer\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nspatial-temporal relations efficiently. Hence, we integrate the Bayesian relation inference component mentioned in Section 4.1.1 with the transformer in a unified framework to simultaneously reason and model the spatial-temporal features of sleep EEG data.\n\nBayesian spatial transformer. To better construct the spatial functional connectivity of the brain and capture spatial features, we design the Bayesian spatial transformer. It contains two components: a Bayesian spatial relation inference component and a position feed-forward network, of which the core component is a Bayesian relation inference component. Specifically, the input of the Bayesian spatial transformer S is the embeddings of n spatial nodes. First we add position encoding to the input to introduce position information:\n\n ̃S = S + Pep (12) where S ∈ R(B×Nt)×Ns×V is the input spatial node embedding, Pep ∈ R(B×Nt)×Ns×V is the position encoding matrix, and ̃S ∈ R(B×Nt)×Ns×V is the position-encoded spatial node embeddings. For the position encoding matrix, we follow the groundbreaking work (Vaswani et al., 2017) and use the sine and cosine functions to calculate.\n\nWe design a multi-head spatial Bayesian relation inference component which can reson about spatial relations to improve the representation learning ability of the model. The details of Bayesian relation inference component have been described in Section 4.1.1. The node embeddings with positional encoding are encoded as embeddings with spatial features after passing through the multi-head spatial Bayesian relation inference component and the feed-forward neural network layer:\n\n ̃S′ = fFNN\n\n(cid:16)\n\nfBSRI\n\n(cid:17)(cid:17)\n\n(cid:16) ̃S\n\n(13)\n\nwhere ̃S is the input node embedding, fBSRI(·) is the Bayesian spatial relation inference module which inferences the spatial relation, fFNN(·) is the feed-forward neural network layer, and ̃S′ ∈ RB×Nt×Vs is the spatial relation intensity graph based features.\n\nBayesian temporal transformer. Similar to the Bayesian spatial transformer, in order to better capture the sleep transition rules, we design a Bayesian temporal transformer module to reason and model temporal features. The calculations of the temporal relation intensity graph based features are as follows:\n\n(cid:101)T = ̃S′ + Pep (cid:16) fBTRI\n\n ̃T ′ = fFNN\n\n(cid:17)(cid:17)\n\n(14)\n\n(15)\n\n(cid:16) ̃T\n\nwhere fBTRI(·) is the Bayesian temporal relation inference component, fFNN(·) is the feed-forward neural network layer, and ̃T ′ ∈ RB×Nst is the temporal relation intensity graph based features. The classification results are generated by ̃T ′ after passing through a linear classification layer fC:\n\n(cid:98)Y = fC\n\n(cid:16) ̃T ′(cid:17)\n\n(16)\n\nwhere (cid:98)Y is the classification result of BSTT.\n\n5 EXPERIMENTS\n\nTo verify the effectiveness of the Bayesian spatial-temporal transformer, we evaluate it on the Institute of Systems and Robotics, University of Coimbra (ISRUC) and Montreal Archives of Sleep Studies-SS3 (MASS-SS3) dataset.\n\n5.1 DATASET\n\nISRUC dataset contains the PSG recordings from 100 adult subjects. Each PSG recording contains 6 EEG channels, 6 EOG channels, 3 EMG channels, and 1 ECG channel. MASS-SS3 dataset contains the PSG recordings from 62 adult subjects. Each PSG recording contains 20 EEG channels, 2 EOG channels, 3 EMG channels, and 1 ECG channel. The recordings are divided into time slices according to a sleep epoch every 30s. Sleep spacialists divide these time slices into five distinct sleep stages (W, N1, N2, N3, and REM) according to the AASM standard. There are also motion artifacts at the beginning and end of each subject’s recording which are marked as unknown. We follow the previous study and remove these recordings (Supratak et al., 2017).\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n5.2 EXPERIMENT SETTINGS\n\nWe evaluate our model using k-fold cross-subject validation to ensure that the experiment results are correct and reliable. We set k = 5 in order to test all recordings efficiently. For optimizer, the Adam and Adadelta optimizer are deployed in MASS-SS3 and ISRUC dataset.\n\nWe use multi-channel EEG data for sleep staging to better capture brain network structure. Specifically, on the ISRUC dataset, we use all 6 EEG channels for experiments. In the MASS-SS3 dataset we use 19 channels of EEG signals. To comprehensively evaluate the Bayesian spatial-temporal transformer model and all baseline methods, we use accuracy (ACC), F1 Score, and KAPPA to evaluate the models. Specific information of the evaluation indicators and baseline models are shown in Appendix 2.\n\n5.3 EXPERIMENT ANALYSIS\n\nTable 1 and 2 indicate that the proposed model achieves the best performance compared to other baseline methods on both datasets. Specifically, the MCNN and MMCNN utilize the CNN model to automatically extract sleep features, while RNN based methods such as DeepSleepNet and TinySleepNet focus on the temporal context in sleep data, and model the multi-level temporal characteristics of the sleep process for sleep staging. Further, GraphSleepNet and ST-Transformer simultaneously model the spatial-temporal relations during sleep and achieve satisfactory results. However, GraphSleepNet and ST-Transformer cannot adequately reason the spatial-temporal relations, which limits the classification performance to a certain extent. Our Bayesian ST-Transformer uses the multi-head Bayesian relation inference component to infer spatial-temporal relations to better model spatial and temporal relations. Therefore, the proposed model achieves best classification performance on different datasets.\n\nTable 1: Comparison of Bayesian spatial-temporal transformer and baselines on ISRUC dataset\n\nMethod MCNN MMCNN MLP+LSTM DeepSleepNet TinySleepNet U-Time GraphSleepNet ST-Transformer BSTT (Our)\n\nACC(%) F1 Scroe(%) KAPPA(%)\n\n78.23 76.83 76.08 75.71 76.92 75.52 80.18 80.35 81.96∗\n\n76.37 78.93 72.87 73.16 75.15 71.04 78.19 78.05 80.30∗\n\n71.79 74.17 69.16 68.82 70.26 67.92 74.54 74.71 76.78∗\n\n∗indicates the significant differences between our model and other models (p < 0.05).\n\nTable 2: Comparison of Bayesian spatial-temporal transformer and baselines on MASS dataset\n\nMethod MCNN MMCNN MLP+LSTM DeepSleepNet TinySleepNet U-Time GraphSleepNet ST-Transformer BSTT (Our)\n\nACC(%) F1 Score(%) KAPPA(%)\n\n86.31 80.15 86.31 85.92 83.30 85.23 88.36 88.64 89.50∗\n\n80.79 71.03 81.55 79.81 77.40 78.36 83.25 84.53 85.00∗\n\n81.26 69.42 80.12 79.09 79.95 77.94 83.30 83.16 84.37∗\n\n∗indicates the significant differences between our model and other models (p < 0.05).\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n5.4 ABLATION EXPERIMENTS\n\nTo verify the effectiveness of each component in the Bayesian spatial-temporal transformer, we conduct ablation experiments to determine the impact of each module on the model’s performance. Specifically, we design three variants of the Bayesian spatial-temporal transformer, including:\n\n• Bayesian Spatial Transformer (BST), which removes the Bayesian temporal transformer module to determine the impact of modeling temporal relations on model performance.\n\n• Bayesian Temporal Transformer (BTT), which removes the Bayesian spatial transformer\n\nmodule to determine the impact of modeling spatial relations on model performance.\n\n• Spatial-Temporal Transformer (STT), which removes the relational inference component\n\nto determine the impact of Bayesian relational inference on model performance.\n\nFigure 2 demonstrates that the performance of the variant models degrades after removing certain component or module. Among them, the removal of the relational inference component has the greatest impact on performance, which shows the importance of introducing relational inference to the sleep staging task. In addition, only modeling the spatial relation or temporal relation of the data also lead to a decrease in performance. It can be seen that modeling the spatial and temporal relation is helpful for the sleep staging task.\n\nFigure 2: Ablation experiment results of Bayesian spatial-temporal transformer on ISRUC dataset. .\n\n5.5 VISUAL ANALYSIS\n\nTo verify the proposed Bayesian relational inference module can infer the spatial-temporal relations during sleep, we visualize and analysis the generated relational inference graphs.\n\n5.5.1 VISUAL ANALYSIS OF SPATIAL RELATION INFERENCE\n\nSome researches have shown that the functional connectivity of the brain varies during different sleep stages (Nguyen et al., 2018). In order to analyze the role of the Bayesian spatial inference component of our model, we visualize the spatial relation intensity graph between EEG signal channels at different sleep periods, as shown in Figure 3. The position of the nodes in the figure represents the position of the electrodes that output the EEG signals, and the edge is the relation intensity between the each two electrodes. We notice that during the NREM period, brain connectivity is significantly stronger in light sleep (N1, N2) than that during deep sleep (N3). It has been revealed that during light sleep, cerebral blood flow (CBF) and cerebral metabolic rate (CMR) are only about 3% to 10% lower than those of wakefulness while during deep sleep, these indexes have a significant decrease of 25% to 44% by previous study (Madsen & Vorstrup, 1991). Synaptic connection activity is directly correlated with CBF and CMR, which is consistent with our connection intensity graph. Madsen also reported that the level of brain synaptic activity during REM period is similar to that of the wake period, which matches our experimental findings.\n\n8\n\n0.8040.7810.7470.8060.7810.7500.808 0.782 0.751 0.820 0.803 0.768 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 ACCF1 ScoreKAPPASTT(No Bayesian inference)BTT(No spa�al)BST(No temporal)BSTT(Our)Published as a conference paper at ICLR 2023\n\nFigure 3: The graph shows the average of the brain spatial intensity over time. The spatial relation during the WAKE, REM and N1 period is strong, while the that during the N2 and N3 period is weak.\n\n.\n\nFigure 4: Intensity graphs of the temporal relation during different sleep periods and when sleep transitions occur.\n\n.\n\n5.5.2 VISUAL ANALYSIS OF TEMPORAL RELATION INFERENCE\n\nTo analyze the contribution of the Bayesian temporal inference component of our model for sleep staging, we visualize the time slice relation intensity graphs as shown in Figure 4. In each graph, the nodes represent time slices while edges represent the strength of the relation between time slices (Here, time slice represents the manually divided EEG signals of 30s duration.) The left graph of each graph pair (e.g. N1 marked green) represents the five time slices (nodes) are in the same sleep period. The one on the right (e.g. Other to N1 marked blue) represents the t − 2 and t − 1 time slices are in one sleep period while the other three are in another sleep period (Sleep period transition occurs between the t − 1 and t time slices.) In Figure 4, for each pair of graphs, the edges in the left graph are, on average, darker than those in the right one. This means that our model tends to think that EEG signals are more closely related to each other during a single sleep stage. Previous studies have shown that the stability of the unchanging period is stronger, and sleep instability is the basis of sleep transition (Bassi et al., 2009), which is consistent with our experimental results. Figure 4 also reports that when sleep transition occurs, the relation intensity between the t and t + 1 time slices are usually stronger. Similarly, temporal intensity between the t − 2 and the last three time slices are usually weaker than that during the unchanging period, which is conducive to the interpretation of the sleeping transition.\n\n6 CONCLUSION\n\nWe propose a novel Bayesian spatial-temporal transformer model for sleep staging. To our best knowledge, this is the first attempt to combine Bayesian relational inference with spatial-temporal transformer for sleep staging. Our BSTT constructs spatial-temporal relations through Bayesian relational inference and applies the transformer architecture to capture brain spatial-temporal features for sleep staging. The results show that BSTT can effectively improve the model performance and achieve state-of-the-art results. In addition, visual analysis presents that the relation intensity graphs generated by the Bayesian relation inference have certain interpretability, which is consistent with the existing research and helps to reveal the potential working mechanism of our model. Besides, the proposed BSTT is a general-framework which can inference the spatial-temporal relations of EEG data and perform satisfactory data forecasting. In the future, the proposed method can be used for other EEG tasks, such as emotion recognition or motor imagery classification.\n\n9\n\nhighlowN1N2WAKEREMN3t-2t-1tt+1t+2N2t-2t-1tt+1t+2Other to N2t-2t-1tt+1t+2N1t-2t-1tt+1t+2Other to N1t-2t-1tt+1t+2N3t-2t-1tt+1t+2Other to N3t-2t-1tt+1t+2Waket-2t-1tt+1t+2Other to Waket-2t-1tt+1t+2REMt-2t-1tt+1t+2Other to REMPublished as a conference paper at ICLR 2023\n\n7 REPRODUCIBILITY STATEMENT AND ETHICS STATEMENT\n\nWe provide an open-source implementation of our BSTT and other baseline models. The code of BSTT is available at: https://github.com/YuchenLiu1225/BSTT/tree/main/ BSTT. Please check the Appendix 7 for links of the baseline methods.\n\nThe authors do not foresee any negative social impacts of this work. All authors disclosed no relevant relationships.\n\nREFERENCES\n\nAlejandro Bassi, Ennio A Vivaldi, and Adri ́an Ocampo-Garc ́es. The time course of the probability\n\nof transition into and out of rem sleep. Sleep, 32(5):655–669, 2009.\n\nStanislas Chambon, Mathieu N Galtier, Pierrick J Arnal, Gilles Wainrib, and Alexandre Gramfort. A deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 26(4): 758–769, 2018.\n\nJunyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. Advances in neural information processing systems, 28, 2015.\n\nHao Dong, Akara Supratak, Wei Pan, Chao Wu, Paul M Matthews, and Yike Guo. Mixed neural network approach for temporal sleep stage classification. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 26(2):324–333, 2017.\n\nEmadeldeen Eldele, Zhenghua Chen, Chengyu Liu, Min Wu, Chee-Keong Kwoh, Xiaoli Li, and Cuntai Guan. An attention-based deep learning approach for sleep stage classification with singlechannel eeg. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 29:809–818, 2021.\n\nLuay Fraiwan, Khaldon Lweesy, Natheer Khasawneh, Heinrich Wenz, and Hartmut Dickhaus. Automated sleep stage identification system based on time–frequency analysis of a single eeg channel and random forest classifier. Computer methods and programs in biomedicine, 108(1):10–19, 2012.\n\nAntoine Guillot and Valentin Thorey. Robustsleepnet: Transfer learning for automated sleep staging at scale. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 29:1441–1451, 2021.\n\nHengguan Huang, Fuzhao Xue, Hao Wang, and Ye Wang. Deep graph random process for relationalthinking-based speech recognition. In International Conference on Machine Learning, pp. 4531– 4541. PMLR, 2020.\n\nRitika Jain and Ramakrishnan Angarai Ganesan. An efficient sleep scoring method using visibility graph and temporal features of single-channel eeg. In 2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp. 6306–6309. IEEE, 2021.\n\nZiyu Jia, Xiyang Cai, Gaoxing Zheng, Jing Wang, and Youfang Lin. Sleepprintnet: a multivariate multimodal neural network based on physiological time-series for automatic sleep staging. IEEE Transactions on Artificial Intelligence, 1(3):248–257, 2020a.\n\nZiyu Jia, Youfang Lin, Jing Wang, Ronghao Zhou, Xiaojun Ning, Yuanlai He, and Yaoshuai Zhao. Graphsleepnet: Adaptive spatial-temporal graph convolutional networks for sleep stage classification. In IJCAI, pp. 1324–1330, 2020b.\n\nZiyu Jia, Youfang Lin, Jing Wang, Xiaojun Ning, Yuanlai He, Ronghao Zhou, Yuhan Zhou, and H Lehman Li-wei. Multi-view spatial-temporal graph convolutional networks with domain generalization for sleep stage classification. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 29:1977–1986, 2021a.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nZiyu Jia, Youfang Lin, Jing Wang, Xuehui Wang, Peiyi Xie, and Yingbin Zhang. Salientsleepnet: Multimodal salient wave detection network for sleep staging. arXiv preprint arXiv:2105.13864, 2021b.\n\nZiyu Jia, Xiyang Cai, and Zehui Jiao. Multi-modal physiological signals based squeeze-andIEEE Sensors Journal,\n\nexcitation network with domain adversarial learning for sleep staging. 2022a.\n\nZiyu Jia, Junyu Ji, Xinliang Zhou, and Yuhan Zhou. Hybrid spiking neural network for sleep elec-\n\ntroencephalogram signals. Science China Information Sciences, 65(4):140403, 2022b.\n\nRajani Khanal. Brain Connectivity During Different Sleep Stages Using EEG and NIRS. PhD thesis,\n\nFlinders University, College of Science and Engineering., 2019.\n\nPengfei Li, Peixiang Zhong, Kezhi Mao, Dongzhe Wang, Xuefeng Yang, Yunfeng Liu, Jianxiong Yin, and Simon See. Act: an attentive convolutional transformer for efficient text classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 13261–13269, 2021.\n\nYunpeng Luo, Jiayi Ji, Xiaoshuai Sun, Liujuan Cao, Yongjian Wu, Feiyue Huang, Chia-Wen Lin, and Rongrong Ji. Dual-level collaborative transformer for image captioning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 2286–2293, 2021.\n\nPL Madsen and S Vorstrup. Cerebral blood flow and metabolism during sleep. Cerebrovascular and\n\nbrain metabolism reviews, 3(4):281–296, 1991.\n\nThien Nguyen, Olajide Babawale, Tae Kim, Hang Joon Jo, Hanli Liu, and Jae Gwan Kim. Exploring brain functional connectivity in rest and sleep states: a fnirs study. Scientific reports, 8(1):1–10, 2018.\n\nMathias Perslev, Michael Jensen, Sune Darkner, Poul Jørgen Jennum, and Christian Igel. U-time: A fully convolutional network for time series segmentation applied to sleep staging. Advances in Neural Information Processing Systems, 32, 2019.\n\nHuy Phan, Fernando Andreotti, Navin Cooray, Oliver Y Ch ́en, and Maarten De Vos. Automatic sleep stage classification using single-channel eeg: Learning sequential features with attention-based recurrent neural networks. In 2018 40th annual international conference of the IEEE engineering in medicine and biology society (EMBC), pp. 1452–1455. IEEE, 2018.\n\nHuy Phan, Fernando Andreotti, Navin Cooray, Oliver Y Ch ́en, and Maarten De Vos. Seqsleepnet: end-to-end hierarchical recurrent neural network for sequence-to-sequence automatic sleep staging. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 27(3):400–410, 2019.\n\nHuy Phan, Kaare B Mikkelsen, Oliver Chen, Philipp Koch, Alfred Mertins, and Maarten De Vos. Sleeptransformer: Automatic sleep staging with interpretability and uncertainty quantification. IEEE Transactions on Biomedical Engineering, 2022.\n\nAdeel Razi, Mohamed L Seghier, Yuan Zhou, Peter McColgan, Peter Zeidman, Hae-Jeong Park, Olaf Sporns, Geraint Rees, and Karl J Friston. Large-scale dcms for resting-state fmri. Network Neuroscience, 1(3):222–241, 2017.\n\nVangelis Sakkalis. Review of advanced techniques for the estimation of brain connectivity measured\n\nwith eeg/meg. Computers in biology and medicine, 41(12):1110–1117, 2011.\n\nOscar B Sheynin. Laplace’s theory of errors. Archive for history of exact sciences, 17(1):1–61,\n\n1977.\n\nAkara Supratak and Yike Guo. Tinysleepnet: An efficient deep learning model for sleep stage scoring based on raw single-channel eeg. In 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp. 641–644. IEEE, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAkara Supratak, Hao Dong, Chao Wu, and Yike Guo. Deepsleepnet: A model for automatic sleep stage scoring based on raw single-channel eeg. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 25(11):1998–2008, 2017.\n\nOrestis Tsinalis, Paul M Matthews, Yike Guo, and Stefanos Zafeiriou. Automatic sleep stage scoring with single-channel eeg using convolutional neural networks. arXiv preprint arXiv:1610.01683, 2016.\n\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\n\nsukhin. Attention is all you need. In arXiv, 2017.\n\nDongdong Zhou, Qi Xu, Jian Wang, Jiacheng Zhang, Guoqiang Hu, Lauri Kettunen, Zheng Chang, and Fengyu Cong. Lightsleepnet: A lightweight deep model for rapid sleep stage classification with spectrograms. In 2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp. 43–46. IEEE, 2021a.\n\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 11106–11115, 2021b.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a method for sleep staging based on Bayesian spatial-temporal transformer.\n\n# Strength And Weaknesses\n\nStrengths:\n- Surpasses state-of-the-art for sleep staging\n- Shows some spacial interpretability\n\nWeaknesses:\n- Improvements are minor (<1% for MASS dataset)\n- Figure 1 both transformers are identical.\n- Section 4, the method is not well written. The terminology is not explained.\n- Figure 4 is not explained well and it is unclear how it provides interpretation.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity aspect is missing as mentioned in previous section but the paper covers all the other aspects well and provides a good codebase for reproducibility.\n\n# Summary Of The Review\n\nThe paper has some novel contributions but does not outperform existing methods by any significant margin. Moreover, the writeup needs a lot of work. The terms are not explained. One of the two interpretations is either not very convincing or not explained well.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS\n\nTianlong Chen1∗, Zhenyu Zhang1∗, Ajay Jaiswal1, Shiwei Liu1, Zhangyang Wang1 1VITA Group, University of Texas at Austin {tianlong.chen,zhenyu.zhang,ajayjaiswal,shiwei.liu,atlaswang}@utexas.edu\n\nABSTRACT\n\nDespite their remarkable achievement, gigantic transformers encounter significant drawbacks, including exorbitant computational and memory footprints during training, as well as severe collapse evidenced by a high degree of parameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown promise to mitigate the issue of training efficiency, yet they are prone to (1) redundant experts due to representational collapse; and (2) poor expert scalability for inference and downstream fine-tuning, primarily due to overfitting of the learned routing policy to the number of activated experts during training. As recent research efforts are predominantly focused on improving routing policies to encourage expert specializations, this work focuses on exploring the overlooked scalability bottleneck of SMoEs and leveraging it to effectively scale dense transformers. To this end, we propose a new plug-and-play training framework, SMoEDropout, to enable scaling transformers to better accuracy in their full capacity without collapse. Specifically, SMoE-Dropout consists of a randomly initialized and fixed router network to activate experts and gradually increases the activated expert number as training progresses over time. Transformers trained by SMoE-Dropout naturally exhibit a “self-slimmable” property subject to resource availability, offering smooth and consistent performance boosts with an increase in activated experts during inference or fine-tuning. Our extensive experiments across diverse transformer architectures on a variety of tasks demonstrate the superior performance and substantial computation savings of SMoE-Dropout, compared to dense training baselines with equivalent parameter counts. In particular, our trained BERT outperforms its densely trained counterpart with consistent improvements of {1.03%, 0.78%, 1.09%} on challenging reasoning tasks {ASDiv-A, MAWPS, SVAMP}, respectively. Codes and models are available in https://github.com/VITA-Group/Random-MoE-as-Dropout.\n\nINTRODUCTION\n\n1 Scaling neural networks, historically with the blessing of modern hardware, have dramatically improved the state-of-the-art on a wide array of real-world machine learning applications and leaderboards, conforming to the empirical scaling laws (Kaplan et al., 2020), where the final model quality has been found to have a power-law relationship with the amount of data, model size, and compute time. Transformers (Vaswani et al., 2017), swiftly after their introduction, have become de facto choice for many natural language processing (NLP) (Yang et al., 2019c; Liu et al., 2019b; Talmor et al., 2018; Jaiswal et al., 2021; Yang et al., 2019b; Wang et al., 2018; Ding et al., 2019; Chowdhery et al., 2022; Wei et al., 2022) and computer vision (Dosovitskiy et al., 2020; Han et al., 2020; Touvron et al., 2021; Mao et al., 2022; Zheng et al., 2021; Parmar et al., 2018) applications and now their parameter counts are typically measured in billions rather than millions. Unfortunately, this exploitation of parameters actuates a roughly quadratic blow-up in training costs, as both the model size and the number of training examples increase especially for dense advanced transformer-based models (e.g., BERT (Devlin et al., 2018) and GPT (Brown et al., 2020)) and require thousands of GPU days for training. Additionally, these gigantic transformers suffer from the representation collapse issue during vanilla training, which is affirmed by a high degree of parameter redundancy (Guo et al., 2019; Ganesh et al., 2020; McCarley et al., 2019) and observed ineffective usage of the transformer expressiveness (Michel et al., 2019; Chen et al., 2022a).\n\n*Equal Contribution.\n\n1\n\nSparse Mixture-of-Experts (SMoEs) enable efficient scaling of model capacity at a fixed computational cost by performing input-dependent conditional computing. Such property facilitates training transformers with significantly high parameter counts at moderately increased cost, compared to their dense counterparts, resulting in improved training efficiency. For instance, with similar training FLOPS, Switch-Large (Fedus et al., 2021) (a kind of SMoE) is 35× larger than a T5-Large dense model (Raffel et al., 2020). Despite their advantages in mitigating computational and energy footprints, SMoEs have many critical limitations. Firstly, the current learning-based routing mechanisms in SMoEs tend to push hidden representations clustering around expert centroids (Chi et al., 2022), implying a trend toward representation collapse, which in turn leads to redundant experts, inferior expert specialization, thereby substandard performance (Mittal et al., 2022; Chen et al., 2022b). Secondly, SMoEs suffer from poor scalability during inference and downstream fine-tuning prominently due to overfitting of the learned routing policy to the number of activated experts during training. Naive solutions to mitigate such sparsity immutability often lead to performance degradation. As recent research efforts for SMoEs are predominantly focused on improving routing policies to encourage expert specializations, we explore the overlooked scalability bottleneck of SMoEs and ask: Does there exist a principled and pluggable approach to modify SMoE training that can enhance scalability at inference and downstream fine-tuning of large-scale transformers, by dynamically adapting the number of activated experts subject to resource availability?\n\nFigure 1: Bits-Per-Character (↓) on enwik8’s test-set with a 4-layer Transformer-XL. SMoEDropout demonstrates a “self-slimmable” property where inference performance is smoothly boosted along with the increase of activated parameters. Learnable SMoEs tend to overfit certain levels of network capacity. Note that only gray curve is produced by (5) different dense models.\n\nTo this end, this paper proposes a novel plug-and-play training framework, named SMoE-Dropout, to enable scaling transformers to better accuracy in the full capacity setting without collapse. More specifically, SMoE-Dropout employs a fixed router network that is randomly initialized to activate experts and progressively increases their number as training progresses over time. Our simple, yet highly effective strategy has a multi-fold win-win for trained transformers, specifically: ❶ obtaining a “self-slimmable” property during inference and downstream fine-tuning subject to resource availability, which delivers a once-for-all in-situ trade-off between efficiency and performance; ❷ mitigating representational collapse and effectively utilizing the full model capacity, where activating more experts produces superior performance (Figure 1 (blue)); ❸ eliminating the overhead of learning routing policies for SMoE. Note that SMoE-Dropout can be swiftly adapted for training any deep learning network (e.g. CNNs), given some splitting techniques (Zhang et al., 2021), but this work primarily focuses on transformers considering their exploding computational footprints. Our innovative contributions can be summarized as:\n\n⋆ We propose a new plug-and-play training framework, SMoE-Dropout, to enable scaling transformers in the full capacity setting without collapse. SMoE-Dropout facilitates the randomly and sparsely activated structure of network modules, playing an implicit regularization role similar to dropout. Our new framework leads to enhanced generalization and reduced training costs (e.g., up to 37% running time savings) compared to the vanilla training of large dense transformers at equivalent parameter counts.\n\n⋆ Transformers trained by SMoE-Dropout naturally exhibit a “self-slimmable” property that displays smooth and consistent performance boosts when increasing activated experts during inference or fine-tuning (Figure 1 (blue)). This property enjoys an “in-situ” trade-off between efficiency and performance at deployment, subject to resource availability.\n\n⋆ Our extensive experiments across representative architectures on a variety of tasks validate the effectiveness of our proposed SMoE-Dropout. Specifically, during pre-training, our approach has {1.37, 4.10}, {2.53, 12.44} and {154.12, 188.00} (×10−2) lower BPC than {vanilla dense training (with the same parameter counts), learned SMoE} for Transformer-XL, BERT, and RoBERTa, respectively; after transferring, SMoEDropout obtains {0.07%, 1.03%, 0.78%, 1.09%} performance improvements for BERT and {−, 5.88%, 0.07%, 5.04%} for RoBERTa, on {CSQA, ASDiv-A, MAWPS, SVAMP} reasoning tasks compared to its dense training counterpart.\n\n2\n\n0.81.01.21.41.61.82.0Parameter Count (×107)1.161.181.201.221.241.261.28Bits-Per-Character (BPC) Densely Training w. DropoutTraining w. Learnable SMoETraining w. SMoE-DropoutFigure 2: Overview of our proposed SMoE-Dropout. Left describes the standard transformer layer, consisting of multi-head attention and multi-layer perceptron (MLP) components. Middle Left shows the process of modulization. It splits the original MLP evenly and constructs a series of experts which are smaller MLPs with a reduced hidden dimension. Middle Right presents the overall procedure of SMoE-Dropout. The random router selects the top-k experts given a token embedding and then reweights the features from activated experts. In the end, a summation is conducted to aggregate all features. Right displays the gradually increased number of chosen experts, along with the training procedure.\n\n2 RELATED WORKS\n\nMixture of Experts (MoE). MoE is a special kind of neural network, where its parameters are partitioned into a series of sub-modules (commonly referred to as experts), and conditional computation is then performed in an input-dependent fashion (Jacobs et al., 1991; Jordan & Jacobs, 1994; Chen et al., 1999; Yuksel et al., 2012). The traditional dense MoEs are computationally intensive, as they adopt all experts for each input (Eigen et al., 2013). Fortunately, recent investigations (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021) have proved the effectiveness of MoEs with sparsely activated experts (i.e., SMoE) at both training and inference stages, which greatly trim down the cost and scale language models to enormous sizes like trillions of parameters (Fedus et al., 2021). This efficient fashion of SMoEs gains increasing popularity in various NLP (Shazeer et al., 2017; Lepikhin et al., 2020; Zhou et al., 2022; Zhang et al., 2021; Zuo et al., 2022; Jiang et al., 2021) and vision (Riquelme et al., 2021; Eigen et al., 2013; Ahmed et al., 2016; Gross et al., 2017; Wang et al., 2020; Yang et al., 2019a; Abbas & Andreopoulos, 2020; Pavlitskaya et al., 2020) tasks.\n\nHowever, its sparse-gated manner incurs several downsides, including: (1) Unstable training. Zoph et al. (2022) pointed out that while techniques like gradient clipping can stabilize SMoE training, they often result in lower quality. The router z-loss (Zoph et al., 2022) is a preferred solution for achieving both improved performance and stability. (2) Poor specialization. One of the intriguing goals of SMoE is to divide-and-conquer the learning task by solving each piece of the task with adaptively selected experts (Aoki et al., 2021; Hazimeh et al., 2021; Ma et al., 2018; Mittal et al., 2022). To encourage specialization and decrease redundancy among experts (Chen et al., 2022b), Dai et al. (2022) pre-defined the expert assignment for different input categories, while Hazimeh et al. (2021) advocated multiple, diverse router policies. (3) Representation collapse and load imbalance among experts. As the primary issue of learning-based SMoEs, various approaches have been proposed to mitigate their negative effects. Shazeer et al. (2017) injected Gaussian noises into gating networks to promote the routing balance. Later, Lepikhin et al. (2020); Fedus et al. (2021) applied an auxiliary loss of load balancing regularizers; Lewis et al. (2021) performed the routing by dealing with a linear assignment problem; Clark et al. (2022) utilized reinforcement learners; Zhou et al. (2022) routed top-k inputs per expert instead of selecting top experts per input sample. Beyond learned routing policies, Roller et al. (2021) and Zuo et al. (2022) designed deterministic hashing and stochastic assignments, respectively, which eliminate the necessity for router networks.\n\nZuo et al. (2022), one closely related prior work, also endorsed the advantage of stochastic expert assignment. They randomly activate experts for each input during training and inference, which leads\n\n3\n\n......NormalizationMulti-Head AttentionNormalizationMulti-Layer Perceptron...Multi-Layer PerceptronExpertExpertExpertTransformer LayerExpertExpertExpertExpertExpert......SplittingModulization......Token Embedding...Random RouterExpertExpertExpertExpert...Point-wise AdditionPoint-wise AdditionPoint-wise MultiplicationSummationSMoE-Dropout......Gradually Increased kAlong withTrainingto inconsistent inference results. To address the prediction randomness, Zuo et al. (2022) employed a consistent regularized loss to penalize the discrepancy among different experts. However, such regularization is prone to redundancy in SMoEs and sacrifices the network capacity. In our proposal, the fixed router with random weights generates deterministic inferences. Meanwhile, the presented “self-slimmable” attribute suggests the full models’ expressiveness is adequately exploited.\n\nDropout and Other Training Techniques for Transformers in NLP. Dropout (Srivastava et al., 2014) was developed to prevent overfitting in over-parameterized networks during training, by randomly omitting neurons and their corresponding connections. Follow-up studies develop plenty of dropout variants (Zhang & He, 2020; Wan et al., 2013; Ba & Frey, 2013; Kingma et al., 2015; Gal et al., 2017; Wu & Gu, 2015; Tompson et al., 2015; DeVries & Taylor, 2017; Park & Kwak, 2016; Semeniuta et al., 2016). In parallel, McAllester (2013); Mou et al. (2018); Mianjy & Arora (2020); Zhang & Xu (2022); Neklyudov et al. (2017); Gal & Ghahramani (2016) have devoted themselves in deriving the theoretical foundation for dropout and explaining its implicit regularization impacts.\n\nOther notorious bottlenecks of transformer training primarily stem from overfitting and instability caused by poor optimization (Zhang et al., 2020; Liu et al., 2019a; 2020a), insufficient or heterogeneous downstream data (Variˇs & Bojar, 2021; Zhang & Vaidya, 2021), etc.. Accordingly, numerous remedies are developed to address the issues. For example, data augmentations (Sun et al., 2020), improved initialization (Liu et al., 2020b;a; Xu et al., 2020; Zhu et al., 2021), upgraded normalization (Wang et al., 2022; Yang et al., 2022), enhanced optimizers (Cohen et al., 2022), weight decay (Loshchilov & Hutter, 2017), and early stopping.\n\n3 METHODOLOGY\n\n3.1 PRELIMINARY\n\nSparse Mixture of Experts (SMoEs). SMoE models leverage conditional computation to activate different subsets of a network for different inputs. A building block of SMoEs is the expert layer including a multi-head attention block and multiple experts in parallel. In this work, we consider SMoE for Transformers, where SMoE layers are incorporated into contiguous Transformer blocks. SMoE expert can be normally constructed by either splitting the vanilla MLP of transformers into smaller pieces (Zhang et al., 2021) or replicating the MLP (Fedus et al., 2021). Most existing SMoE works mainly concentrate on the MLP component in transformers since MLPs constitute roughly 2/3 of total model parameters counts storing substantial amounts of learned knowledge as memory networks (Geva et al., 2020; Dai et al., 2021).\n\nLet {Ei}N i=1 denote the experts, where i is the index of expert and N is the total number of experts. A gating network or router R is inserted to choose the top-k experts with the largest scores R(x)i, and x represents the input embedding. Usually, k ≪ N, which implies a sparsely activated setting. Specifically, the resultant output of the expert layer can be depicted as follows:\n\ny =\n\nk (cid:88)\n\nj=1\n\nR(x)j · Ej(x); R(x) = TopK(softmax(G(x)), k); TopK(v, k) =\n\n(cid:26) v if v is the top k otherwise\n\n0\n\n(1)\n\nwhere G is the critical part of a router R. For a learnable routing, G is a neural network that can be one or a few layers MLP (Shazeer et al., 2017; Fedus et al., 2021). Ej(x) stands for features from the expert Ej. It will be further summed with a scaling coefficient R(x)j to form the final output y. The TopK function maintains the largest k values and sets the reset elements to zero. In practice, a load or important balancing loss (Shazeer et al., 2017) is employed to avoid the representation collapse issue, i.e., always picking the same experts for different inputs and ignoring others.\n\nDropout and its variants. Dropout is a conventional training technique employed to alleviate the risk of overfitting. The vanilla dropout is typically applied to fully connected layers with a dropping probability p. During each training iteration, neurons will be disabled with the probability p. In other words, the omission of neurons follows a Bernoulli(p) distribution. As for the inference phase, there is no dropout and all neurons are activated. To counterbalance the surplus information during training, the output logits are reweighted by 1 − p. In this paper, we selected two representatives among diverse proposed dropout variants, concrete dropout (Gal et al., 2017) and dropblock (Ghiasi et al., 2018) as our comparison baselines. ▷ Concrete Dropout. It replaces the discrete Bernoulli(p) distribution of dropout with a continuous relaxation, i.e., Concrete distribution, and allows an automatic tuning of the dropping probability p. For example, considering the one-dimensional case, as shown in Gal et al. (2017), a\n\n4\n\nConcrete random variable z is described as z = sigmoid(cid:0) 1 t × (cid:0)log(p) − log(1 − p) + log(u) − log(1 − u)(cid:1)(cid:1), where u ∼ Unif(0, 1) is a uniform random variable and t denotes a temperature hyperparameter. Note that parameter p is optimized in a data-driven way. ▷ DropBlock. Instead of performing Bernoulli dropping per feature map, Ghiasi et al. (2018) applies it in areas within feature maps. They claim that DropBlock improves the generalization and limits overfitting by hiding certain areas of features or input samples.\n\n3.2 A NEW TRAINING PIPELINE: SMOE-DROPOUT\n\nModulization. The first step in our SMoE-Dropout, turns a large densely connected MLP into multiple smaller MLPs with the same size, as demonstrated in Figure 2. Without loss of generality, in Figure 2, we use a single-layer MLP f with a dimension d for illustrations. After the modulization, it is divided into a set of MLPs {E1, E2 · · · , EN}, where they have the same hidden dimension d N . Random Routing Policy. Few prior works have investigated some form of random routing policies, such as Roller et al. (2021) utilizes a hash table to enforce a pre-defined deterministic random mapping from inputs to experts and Zuo et al. (2022) adopts a fully random assignment in each training iteration. Although they have shown some benefits from random policies, both methods suffer from inconsistent inference predictions, and can not outperform the densely trained models with equivalent parameter counts. In contrast, our proposed framework, SMoE-Dropout considers a randomly initialized and fixed router network to guide token assignment. Different from previous works, our proposal’s assignment is (1) implicitly optimized during training, since feature embeddings remain updated for the same input sample; (2) deterministic during inference thanks to the fixed weights in R. Extensive results in Section 4 verify the superiority of our proposal, compared to existing random policies and the dense baseline with the same model parameters. Additionally, another crucial design in SMoE-Dropout’s routing is the progressively enlarged number of activated experts (k). Riquelme et al. (2021); Jiang et al. (2021) reveal that altering k in the inference phase incurs significant performance degradation if the SMoE is learned with a fixed k. For example, (Riquelme et al., 2021)’s SMoE trained with k = 1 has 20% ∼ 30% accuracy drops on ImageNet, when activating k ≥ 7 experts during the evaluation. This drawback substantially restricts the practical use of SMoEs because diverse real-world scenarios require different resource budgets, necessitating flexible and effective network capacity during inference. To tackle this limitation, we adopt a training strategy that gradually enriches the active network capacity by linearly increasing the number of selected experts k during training. This approach coincides with the principle of curriculum learning and provides the attractive “self-slimmable” ability, which consistently boosts performance for transformers as the number of activated experts increases during inference and downstream fine-tuning, as shown in Figure 1. SMoE-Dropout. Our effective proposal comprises three simple and highly effective steps, as described in Figure 2. First, it divides the MLP into a series of MLPs with a reduced size for modulization (Middle Left of Figure 2). Then, a random policy parameterized by fixed weights is introduced to route token embeddings to k experts with the largest response (Middle Right of Figure 2). Finally, it progressively actives more experts, preventing the overfitting to the amounts of used network capacity during training. (Right of Figure 2).\n\n4 EXPERIMENT\n\n4.1\n\nIMPLEMENTATION DETAILS\n\nNetwork Architectures and Comparison Baselines. In our experiments, we have adopted three representative transformer-based networks, including BERT (Devlin et al., 2018), TransformerXL (Dai et al., 2019), and RoBERTa (Liu et al., 2019b). Specifically, we use double-size BERTbase / RoBERTabase that have 12 transformer layers, 768-dimensional encoder layers, 6144-/3072dimensional feed-forward networks (MLPs), and 12 attention heads. For both Transformer-XL, we choose a reduced size due to limited resources, which has 4 layers, 256-dimensional encoder layers, 8192-dimensional feed-forward networks, and 8 attention heads with a head size of 64.\n\nFor sufficient comparisons with our proposal, Training w. SMoE-Dropout, we consider five baselines: (i) Densely Training w. Dropout, where the vanilla dropout is applied to feed-forward networks (MLPs); (ii) Densely Training w. Concrete Dropout (Gal et al., 2017); (iii) Densely Training w. DropBlock (Ghiasi et al., 2018). Note that both Concrete Dropout and DropBlock are inserted in feed-forward networks, replacing the vanilla dropout; (iv) Training w. Learnable SMoE (Fedus et al., 2021); (v) Training w. THOR (Zuo et al., 2022), where THOR is another random SMoE\n\n5\n\nthat randomly activates a pair of experts for each input sample and adopts an auxiliary consistency regularization based on Kullback-Leibler (KL) divergence. To compute the regularization term, two forward processes are needed in each training iteration. Pre-Training. ▷ Datasets. Transformer-XL is pre-trained on enwik8 (Mahoney, 2011) dataset, while we use BooksCorpus (Zhu et al., 2015) for BERT and RoBERTa. ▷ Training Configurations. For Transformer-XL, we follow the official training setups, using Adam optimizer and the learning rate starts from 2.5 × 10−4 and decreases according to a cosine annealing scheduler. We use a batch size of 22 and optimize the network for 4 × 105 iterations. As for BERT pre-training, we adopt an AdamW optimizer with an initial learning rate of 5 × 10−5 that linearly decays to 0. The batch size and total training steps are 64 and 1 × 105, respectively. RoBERTa’s pre-training configurations strictly follow the default from HuggingFace1, but with reduced training steps of 1 × 105. Moreover, we conduct a grid search and set the coefficient of THOR’s regularization term as 2. Similarly, the temperature in Concrete dropout is t = 0.1. ▷ Evaluation Metrics. Since both performance and efficiency are essential, we assess the pre-training performance via Bits-Per-Character (BPC) on the hold-out validation set, where a smaller BPC value indicates a better pre-training; and we report training time per iteration & the number of floating point operations (FLOPs) of singlesample inference, for evaluating the efficiency. {1 RTX A6000, batch size 22} and {8 V100, batch size 64} are adopted for time measurements of Transformer-XL and BERT/RoBERTa, respectively. Downstream Fine-Tuning. ▷ Datasets. Five benchmarks across three downstream tasks are examined in this paper, including text classification (SST-2 (Socher et al., 2013)), arithmetic reasoning (ASDiv-A (Miao et al., 2020), MAWPS (Koncel-Kedziorski et al., 2016), SVAMP Patel et al. (2021)), and commonsense reasoning (CSQA (Talmor et al., 2018)). ▷ Training Configurations. We perform dense fine-tuning for all approaches. Given a downstream parameter budget, SMoE-based methods will select the most voted experts based on their routing policies. Detailed training setups are listed as follows. We fine-tune the pre-trained Transformer-XL with a smaller learning rate of 1 × 10−4 and a batch size of 64 on SST-2 benchmark. And for BERT and RoBERTa, we fine-tune the models on the aforementioned four reasoning datasets. The learning rate is fixed at 2 × 10−5 and the batch size is 64. In each downstream task, the fine-tuning continues for 3 epochs, while other configurations are kept the same as the ones in pre-training. ▷ Evaluation Metrics. At the evaluation phase, accuracy (%) and the problem solving rate (%) (Wei et al., 2022) are reported on the test set of SST-2 and other reasoning tasks, respectively.\n\n4.2 SUPERIOR PERFORMANCE OF SMOE-DROPOUT\n\nWe adopt classical transformer-based models, i.e., {Transformer-XL, BERT, RoBERTa}, and train them in a dense or SMoE-based manner on {enwik8, BookCorpus, BookCorpus}. Evaluation results are summarized in Table 1, where all models are compared under the same number of parameter counts. The following observations can be drawn: ❶ Our SMoE-Dropout demonstrates superior performance compared to all other training algorithms. Specifically, SMoE-Dropout with all experts selected obtains 1.37 ∼ 18.49, 0.56 ∼ 12.44, and 152.82 ∼ 188.00 (×10−2) lower BPC for Transformer-XL, BERT, and RoBERTa, respectively. This validates the effectiveness of our proposals. ❷ Appropriate random routing policies show consistent performance benefits across all three network backbones. Moreover, our randomly weighted router surpasses the completely random allocation in THOR, which is within expectation since our assignment is implicitly “optimized” using evolved feature embeddings. ❸ In terms of training efficiency, SMoE-Dropout has up to 21%, 37%, and 25% training time savings compared to the dense training of three backbones. If only half of the experts (k = N 2 ) are activated, our approach enjoys extra 23% ∼ 34% inference FLOPs reduction with a comparable BPC. Although the learnable SMoE reaches the best efficiency, it results in inferior performance.\n\nBesides, we report another group of experiments varying the expert numbers (parameter counts) during evaluation. As shown in Figure 3, for SMoE-based approaches, we directly change the number of activated experts at the inference stage, which is an in-situ fashion from the single trained transformer. While for dense training baselines, each dot in their curve requires a separately trained model since it does not allow modifications of network capacity without further fine-tuning. Our findings are as follows: ❶ The performance of SMoE-Dropout is stably improved along with more parameters used, and it outperforms the others after 1.0, 10, and 8 (×107) parameter counts for three backbones. Such “slimmable” property enables scaling transformers to the full capacity without\n\n1https://huggingface.co/docs/transformers/model_doc/roberta.\n\n6\n\nTable 1: Testing performance of {Transformer-XL, BERT, RoBERTa} network backbones on {enwik8, BookCorpus, BookCorpus} datasets, respectively. All models are compared under the same number of parameter counts. Training time (s) and inference FLOPs (×1010) are reported. For THOR (Zuo et al., 2022), SMoE, and SMoE-Dropout, evaluations are performed with half (k = N\n\n2 ) or all (k = N) experts activated.\n\nMethods\n\nTransformer-XL\n\nBERT\n\nRoBERTa\n\nBPC (↓)\n\nTime\n\nInfer. FLOPs BPC (↓)\n\nTime\n\nInfer. FLOPs BPC (↓)\n\nTime\n\nInfer. FLOPs\n\nDense w. Dropout Dense w. Concrete Dropout Dense w. DropBlock THOR (k = N) SMoE (k = N)\n\nSMoE-Dropout (k = N 2 ) SMoE-Dropout (k = N)\n\n1.1623 1.3335 1.2468 1.3110 1.1896\n\n1.1776 1.1486\n\n5.1298 6.3519 5.3902 4.8830 4.7982\n\n5.0220 5.0220\n\n7.7579 7.7579 7.7579 7.7620 7.7620\n\n5.6145 7.7620\n\n7.6546 7.6419 7.6349 7.6434 7.7537\n\n7.6372 7.6293\n\n0.2088 0.3031 0.2119 0.1439 0.1387\n\n0.1905 0.1905\n\n135.72 135.72 135.72 135.73 135.73\n\n89.330 135.73\n\n8.0903 8.0820 8.0773 8.0778 8.4291\n\n6.7693 6.5491\n\n0.1898 0.2410 0.1934 0.1607 0.1538\n\n0.1799 0.1799\n\n101.75 101.75 101.75 101.76 101.76\n\n78.558 101.76\n\nFigure 3: Testing performance over # parameter counts of {Transformer-XL, BERT, RoBERTa} networks on {enwik8, BookCorpus, BookCorpus} datasets, respectively. A smaller BPC suggests a better model.\n\ncollapse, bringing a once-for-all trade-off respected to inference resource availability. ❷ In contrast, learnable SMoE’s and THOR’s BPC are quickly saturated and deteriorated when adopting more experts, which implies the existence of expert redundancy (or representation collapse). The potential reasons for their substandard results are (i) the overfitting to fixed # experts utilized during training for learnable SMoE, (ii), and the consistency regularization between experts’ predictions for THOR.\n\n4.3 TRANSFER STUDY OF SMOE-DROPOUT: SELF-SLIMMABLE\n\nWe further investigate SMoE-Dropout and its intriguing “self-slimmable” property in a transfer learning scenario. Pre-trained models from Section 4.2 are densely fine-tuned on various downstream tasks, including text classification {SST-2} and challenging arithmetic & commonsense reasoning {CSQA, ASDiv-A, MAWPS, SVAMP}. The performance2 is collected in Table 2. We find: equipped with SMoE-Dropout, Transformer-XL achieves 0.47% ∼ 2.43% accuracy improvements on SST-2, BERT / RoBERTa obtain {0.07% ∼ 9.72%, 0.42% ∼ 3.78%, 0.26% ∼ 1.30%, 1.09% ∼ 4.90%} and {−, 2.10% ∼ 5.88%, 0.07% ∼ 0.27%, 5.04% ∼ 5.93%} performance boosts on {CSQA, ASDiv-A, MAWPS, SVAMP} respectively, suggesting an enhanced transferability.\n\nSimilarly, we alter the model capacity during downstream fine-tuning. Starting from one pretraining, the SMoE-based method first calculates the selected times of each expert based on one feedforward pass with downstream data, then chooses the top activated experts to meet certain parameter budgets, and performs the subsequent dense fine-tuning. As displayed in Figure 4, our SMoE-Dropout has a continually increased accuracy or problem-solving rate when involving more parameters, and clearly surpasses the rest of approaches at parameter counts beyond 0.8, 8, and 10.5 (×107) for Transformer-XL, BERT, and RoBERTa respectively. It shows a flexible capacity adjustment, i.e., “self-slimmable”, according to the downstream resource constraint.\n\n4.4 EXTRA INVESTIGATION AND ABLATION STUDY\n\nQ1: When does SMoE-Dropout outperform other baselines? A1: Sufficient Model Capacity.\n\nTo answer Q1 and understand SMoE-Dropout’s superiority in diverse scenarios, we investigate our proposal with different model capacities by varying model depth (e.g., layers) & width (e.g., experts). 2Due to limited computation resources, {our, official} pre-trained BERT/RoBERTa models are produced with {105, 106} training iterations, {128, 256} batch size, {MLM, MLM and NSP} tasks, on {BookCorpus (800M words), BookCorpus (800M words) and English Wikipedia (2, 500M words)} dataset, respectively. The huge gap of pre-training outlays justifies the difference between our and official performance.\n\n7\n\nRoBERTaTransformer-XLBERTTable 2: Transfer performance {Accuracy (% ↑), Problem Solving Rate (% ↑)} of {Transformer-XL, BERT, RoBERTa} networks on {SST-2, CSQA, ASDiv-A, MAWPS, SVAMP} datasets. All models are compared under the same number of parameter counts. The same densely fine-tuning is adopted for all approaches, while THOR, SMoE, and SMoE-Dropout are tuned with half (k = N\n\n2 ) or all (k = N) experts activated.\n\nMethods\n\nTransformer-XL\n\nBERT\n\nRoBERTa\n\nSST-2\n\nCSQA\n\nASDiv-A\n\nMAWPS\n\nSVAMP\n\nASDiv-A\n\nMAWPS\n\nSVAMP\n\nDense w. Dropout THOR (k = N) SMoE (k = N)\n\nSMoE-Dropout (k = N 2 ) SMoE-Dropout (k = N)\n\n81.94 81.13 79.98\n\n81.60 82.41\n\n30.44 20.79 29.27\n\n30.32 30.51\n\n55.27 52.52 55.88\n\n54.97 56.30\n\n80.47 79.95 80.73\n\n80.99 81.25\n\n34.24 30.43 33.15\n\n33.65 35.33\n\n49.58 53.36 52.10\n\n52.94 55.46\n\n78.06 77.86 77.86\n\n76.30 78.13\n\n28.90 28.44 27.98\n\n31.19 33.94\n\nFigure 4: Transfer performance over # parameter counts of {Transformer-XL, BERT, RoBERTa} networks on downstream {SST-2, CSQA, ASDiv-A, MAWPS, SVAMP} datasets, respectively. Only the fine-tuning of Dense w. Dropout needs multiple pre-trained models with different amounts of network capacity.\n\nModel Depth - Different Number of Layers. We conduct experiments on enwik8 dataset with Transformer-XL that has 2, 4, 8, 12 layers and each layer is turned into the SMoE layer through a modularization. The comparison results of Densely Training w. Dropout and Training w. Learnable SMoE are reported in Figure 5 (a). We find that densely trained transformer performs the best when the network capacity is small like 2 layers, while with sufficiently large model capacity (≥ 4 layers), SMoE-Dropout demonstrates a consistent advantage compared to the others. Meantime, along with the increase of layers, the performance gap of SMoEs between the learned policy and our random policy keeps enlarging, signifying SMoE-Dropout’s better scalability.\n\nModel Width - Different Number of Experts. Similarly, we study the influence of model capacity by examining Transformer-XL with different widths of 2, 4, 8, 16 experts. Results are summarized in Figure 5 (b). Consistent observations can be drawn that: (i) Densely Training w. Dropout outperforms SMoE-based training under small network widths such as ≤ 8 experts; (ii) SMoEDropout presents enhanced performance when applied to large models with 16 experts; (iii) Learnable routing policies are effective with a small number of experts like ≤ 8 experts, while it gets worse results than our random routing with a sufficient number of experts, e.g., 16 experts. Q2: What is a better SMoE-Dropout design? A2: Random Weight Router; Later-layer SMoE. To answer Q2, we focus on the main constituents of SMoE-Dropout: Modularization, Random Routing Policies, and Gradually Increased k. Comprehensive ablations are depicted below.\n\nAblation on Diverse Random Routing Policies. An appropriate design of random routing policies determines the achievable performance of SMoE-Dropout. We compare our random initialized and fixed router to SMoE with fully random assignments (Zuo et al., 2022) and random hash SMoE with a pre-defined deterministic random assignment (Roller et al., 2021). Transformer-XL results on enwik8 are collected in Fig. 5 (c), where our proposed random routing obtains substantially lower BPC of 2.96 ∼ 170.11 (×10−2) than the other two under different amounts of model parameters. Ablation on w./w.o. Gradually Increased k. Figure 5 (d) investigates SMoE-Dropout variants with and without gradually increased k. We see that disabling the progressive manner of enlarg-\n\n8\n\nBERT on MAWPSBERT on ASDiv-ABERT on CSQATransformer-XL on SST-2RoBERTa on ASDiv-ARoBERTa on MAWPSRoBERTa on SVAMP BERT on SVAMP Figure 5: Extra studies about SMoE-Dropout. Testing BPC of Transformer-XL is collected on enwik8. (a) and (b) investigate diverse training mechanisms under different model depths and widths, respectively. (c) is the ablation of random routing policies. (d) examines the effects of gradually increased k. (e) studies the appropriate locations to insert SMoE expert layers.\n\ning the number of activated experts causes unsatisfied performance. Also, as a result, the “selfslimmable” property completely disappears, e.g., adopting all model parameters leads to worse BPC. Ablation on Different Positions for Modularization. It remains mysterious where is the best position to insert SMoE layers. To address this question, we perform modularization to different transformer layers and record their performance in Figure 5 (e). Specifically, given a 4-layer Transformer-XL, we compare four options: (i) Early, the first two layers are SMoE layers; (ii) Middle, the 2nd and 3rd layers are SMoE layers; (iii) Later, the last two layers are SMoE layers; (iv) Every-2, there is one SMoE layer every two transformer layers, i.e., the 2nd and 4th layers. From the results, introducing SMoEs to later layers is in general more beneficial than modulizing earlier transformer layers. One possible reason is that shallow layers might capture common features that need to be shared across input samples. More dissections are left for future works. Q3: Extra benefits from SMoE-Dropout? A3: Improved Distillation and Less Overfitting. Distilling into Single Expert on Downstream Tasks. Besides all the benefits in pre-training inference and downstream transfer, we explore additional advantages of SMoE-Dropout under the distillation scheme that is usually preferred in resource-limited applications. As shown in Table 3, we distill all pre-trained Transformer-XLs into the same smaller variant with a single expert on the SST-2 downstream task. Our algorithm produces the most distillable models among all four methods by a clear accuracy margin of 0.76% ∼ 1.89%. Overfitting. We investigate the potential for overfitting to the training data distribution as model parameters increase in SMoEDropout, SMoE, and densely trained transformers. As shown in Figure 5 (a) and (b), experiments are conducted on enwik8 with Transformer-XL, and three approaches are compared under the same parameter counts. We observe both SMoE-Dropout and Densely Training w. Dropout do not exhibit any indication of overfitting. That is, the performance is consistently improved as we increase the layers from 2 to 12 or experts from 2 to 16. In contrast, Training w. Learnable SMoE incurs BPC deterioration owing to overfitting when we expend the transformer to 12 layers, similar to the findings in Zoph et al. (2022). We attribute the reduced overfitting to the implicit regularization effect of SMoE-Dropout and Dropout.\n\nTable 3: Distillation results of Transformer-XL on SST-2.\n\nDense w. Dropout THOR SMoE\n\n81.25 80.76 80.12\n\nSMoE-Dropout\n\nAccuracy (↑)\n\nMethod\n\n82.01\n\n5 CONCLUSION\n\nIn this paper, we present a novel plug-and-play SMoE-Dropout strategy for training overparameterized transformers in full-capacity settings without collapse. We design a fixed and randomly initialized router to assign experts and gradually increase their number along with the training. As a result, our proposal provides an appealing “self-slimmable” property to large transformers during inference and downstream fine-tuning, depending on available resources. It implies alleviated representation collapse and delivers an in-situ trade-off between efficiency and performance. Extensive experiments across various combinations of network backbone and dataset, consistently demonstrate the significantly improved performance and training time savings from our algorithm. Future work includes the extension of other network architectures and tasks like vision recognition.\n\nACKNOWLEDGEMENT\n\nThe research of ZW is in part supported by the US Army Research Office Young Investigator Award (W911NF2010240).\n\n9\n\n(a)(b)(c)(d)(e)REFERENCES\n\nAlhabib Abbas and Yiannis Andreopoulos. Biased mixtures of experts: Enabling computer vision inference under data transfer limitations. IEEE Transactions on Image Processing, 29:7656–7667, 2020.\n\nKarim Ahmed, Mohammad Haris Baig, and Lorenzo Torresani. Network of experts for large-scale image categorization. In European Conference on Computer Vision, pp. 516–532. Springer, 2016.\n\nRaquel Aoki, Frederick Tung, and Gabriel L Oliveira. Heterogeneous multi-task learning with expert\n\ndiversity. arXiv preprint arXiv:2106.10595, 2021.\n\nJimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. Advances in\n\nneural information processing systems, 26, 2013.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nKe Chen, Lei Xu, and Huisheng Chi. Improved learning algorithms for mixture of experts in multi-\n\nclass classification. Neural networks, 12(9):1229–1252, 1999.\n\nTianlong Chen, Zhenyu Zhang, Yu Cheng, Ahmed Awadallah, and Zhangyang Wang. The principle of diversity: Training stronger vision transformers calls for reducing all levels of redundancy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12020–12030, 2022a.\n\nTianyu Chen, Shaohan Huang, Yuan Xie, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, arXiv preprint\n\nand Furu Wei. Task-specific expert pruning for sparse mixture-of-experts. arXiv:2206.00277, 2022b.\n\nZewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, and Furu Wei. On the representation collapse of sparse mixture of experts. arXiv preprint arXiv:2204.09179, 2022.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nAidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. arXiv preprint arXiv:2202.01169, 2022.\n\nJeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive gradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022.\n\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Knowledge neurons in pretrained\n\ntransformers. arXiv preprint arXiv:2104.08696, 2021.\n\nYong Dai, Duyu Tang, Liangxin Liu, Minghuan Tan, Cong Zhou, Jingquan Wang, Zhangyin Feng, Fan Zhang, Xueyu Hu, and Shuming Shi. One model, multiple modalities: A sparsely activated approach for text, sound, image, video and code. arXiv preprint arXiv:2205.06126, 2022.\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\n\nwith cutout. arXiv preprint arXiv:1708.04552, 2017.\n\n10\n\nMing Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for multi-hop\n\nreading comprehension at scale. arXiv preprint arXiv:1905.05460, 2019.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\n\nmixture of experts. arXiv preprint arXiv:1312.4314, 2013.\n\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\n\nmodels with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.\n\nYarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent\n\nneural networks. Advances in neural information processing systems, 29, 2016.\n\nYarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. Advances in neural information process-\n\ning systems, 30, 2017.\n\nPrakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Deming Chen, Marianne Winslett, Hassan Sajjad, and Preslav Nakov. Compressing large-scale transformer-based models: A case study on bert. arXiv preprint arXiv:2002.11985, 2020.\n\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are\n\nkey-value memories. arXiv preprint arXiv:2012.14913, 2020.\n\nGolnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock: A regularization method for convolu-\n\ntional networks. Advances in neural information processing systems, 31, 2018.\n\nSam Gross, Marc’Aurelio Ranzato, and Arthur Szlam. Hard mixtures of experts for large scale weakly supervised vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6865–6873, 2017.\n\nFu-Ming Guo, Sijia Liu, Finlay S Mungall, Xue Lin, and Yanzhi Wang. Reweighted proximal\n\npruning for large-scale language representation. arXiv preprint arXiv:1909.12486, 2019.\n\nKai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and Dacheng Tao. A survey on visual transformer. ArXiv, abs/2012.12556, 2020.\n\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder, Lichan Hong, and Ed Chi. Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of\n\nlocal experts. Neural computation, 3(1):79–87, 1991.\n\nAjay Jaiswal, Liyan Tang, Meheli Ghosh, Justin Rousseau, Yifan Peng, and Ying Ding. Radbert-cl: Factually-aware contrastive learning for radiology report classification. Proceedings of machine learning research, 158:196–208, 2021.\n\nHao Jiang, Ke Zhan, Jianwei Qu, Yongkang Wu, Zhaoye Fei, Xinyu Zhang, Lei Chen, Zhicheng Dou, Xipeng Qiu, Zikai Guo, et al. Towards more effective and economic sparsely-activated model. arXiv preprint arXiv:2110.07431, 2021.\n\nMichael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.\n\nNeural computation, 6(2):181–214, 1994.\n\nJared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.\n\n11\n\nDurk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-\n\nzation trick. Advances in neural information processing systems, 28, 2015.\n\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1152–1157, 2016.\n\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, pp. 6265–6274. PMLR, 2021.\n\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019a.\n\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the diffi-\n\nculty of training transformers. arXiv preprint arXiv:2004.08249, 2020a.\n\nXiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng Gao. Very deep transformers for neural ma-\n\nchine translation. arXiv preprint arXiv:2008.07772, 2020b.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019b.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017.\n\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1930–1939, 2018.\n\nMatt Mahoney. Large text compression benchmark, 2011.\n\nZhiyuan Mao, Ajay Jaiswal, Zhangyang Wang, and Stanley H. Chan. Single frame atmospheric turbulence mitigation: A benchmark study and a new physics-inspired transformer model. ArXiv, abs/2207.10040, 2022.\n\nDavid McAllester. A pac-bayesian tutorial with a dropout bound. arXiv preprint arXiv:1307.2118,\n\n2013.\n\nJ. S. McCarley, Rishav Chakravarti, and Avirup Sil. Structured pruning of a bert-based question\n\nanswering model. arXiv preprint arXiv:1910.06360, 2019.\n\nPoorya Mianjy and Raman Arora. On convergence and generalization of dropout training. Advances\n\nin Neural Information Processing Systems, 33:21151–21161, 2020.\n\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 975–984, 2020.\n\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances\n\nin neural information processing systems, 32, 2019.\n\nSarthak Mittal, Yoshua Bengio, and Guillaume Lajoie. Is a modular architecture enough? arXiv\n\npreprint arXiv:2206.02713, 2022.\n\n12\n\nWenlong Mou, Yuchen Zhou, Jun Gao, and Liwei Wang. Dropout training, data-dependent regIn International conference on machine learning, pp.\n\nularization, and generalization bounds. 3645–3653. PMLR, 2018.\n\nKirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry P Vetrov. Structured bayesian pruning via log-normal multiplicative noise. Advances in Neural Information Processing Systems, 30, 2017.\n\nSungheon Park and Nojun Kwak. Analysis on the dropout effect in convolutional neural networks.\n\nIn Asian conference on computer vision, pp. 189–204. Springer, 2016.\n\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam M. Shazeer, Alexander Ku,\n\nand Dustin Tran. Image transformer. In ICML, 2018.\n\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math\n\nword problems? arXiv preprint arXiv:2103.07191, 2021.\n\nSvetlana Pavlitskaya, Christian Hubschneider, Michael Weber, Ruby Moritz, Fabian Huger, Peter Schlicht, and Marius Zollner. Using mixture of expert models to gain insights into semantic In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern segmentation. Recognition Workshops, pp. 342–343, 2020.\n\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.\n\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr ́e Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34, 2021.\n\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason E Weston. Hash layers for large sparse models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/ forum?id=lMgDDWb1ULW.\n\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth. Recurrent dropout without memory loss.\n\narXiv preprint arXiv:1603.05118, 2016.\n\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.\n\nLichao Sun, Congying Xia, Wenpeng Yin, Tingting Liang, Philip S Yu, and Lifang He. Mixuptransformer: dynamic data augmentation for nlp tasks. arXiv preprint arXiv:2010.02394, 2020.\n\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.\n\nYixuan Tang, Hwee Tou Ng, and Anthony KH Tung. Do multi-hop question answering systems\n\nknow how to answer the single-hop sub-questions? arXiv preprint arXiv:2002.09919, 2020.\n\n13\n\nJonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler. Efficient object localization using convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 648–656, 2015.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv’e J’egou. Training data-efficient image transformers & distillation through attention. In ICML, 2021.\n\nDuˇsan Variˇs and Ondˇrej Bojar. Sequence length is a domain: Length-based overfitting in transformer\n\nmodels. arXiv preprint arXiv:2109.07276, 2021.\n\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In International conference on machine learning, pp. 1058–1066. PMLR, 2013.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet:\n\nScaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555, 2022.\n\nXin Wang, Fisher Yu, Lisa Dunlap, Yi-An Ma, Ruth Wang, Azalia Mirhoseini, Trevor Darrell, and Joseph E Gonzalez. Deep mixture of experts via shallow embedding. In Uncertainty in artificial intelligence, pp. 552–562. PMLR, 2020.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n\nHaibing Wu and Xiaodong Gu. Towards dropout training for convolutional neural networks. Neural\n\nNetworks, 71:1–10, 2015.\n\nPeng Xu, Dhruv Kumar, Wei Yang, Wenjie Zi, Keyi Tang, Chenyang Huang, Jackie Chi Kit Cheung, Simon JD Prince, and Yanshuai Cao. Optimizing deeper transformers on small datasets. arXiv preprint arXiv:2012.15355, 2020.\n\nBrandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolutions for efficient inference. Advances in Neural Information Processing Systems, 32, 2019a.\n\nQiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming Tan, Jun Xiao, and Shiliang Pu. Unified normalization for accelerating and stabilizing transformers. arXiv preprint arXiv:2208.01313, 2022.\n\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. End-to-end open-domain question answering with bertserini. arXiv preprint arXiv:1902.01718, 2019b.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019c.\n\nSeniha Esen Yuksel, Joseph N. Wilson, and Paul D. Gader. Twenty years of mixture of experts. IEEE Transactions on Neural Networks and Learning Systems, 23(8):1177–1193, 2012. doi: 10.1109/TNNLS.2012.2200299.\n\n14\n\nJingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv Kumar, and Suvrit Sra. Why {adam} beats {sgd} for attention models, 2020. URL https: //openreview.net/forum?id=SJx37TEtDH.\n\nMinjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with progressive layer dropping. Advances in Neural Information Processing Systems, 33:14011– 14023, 2020.\n\nWancong Zhang and Ieshan Vaidya. Mixup training leads to reduced overfitting and improved\n\ncalibration for the transformer architecture. arXiv preprint arXiv:2102.11402, 2021.\n\nZhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Moefication: Conditional computation of transformer models for efficient inference. arXiv preprint arXiv:2110.01786, 2021.\n\nZhongwang Zhang and Zhi-Qin John Xu.\n\nImplicit regularization of dropout. arXiv preprint\n\narXiv:2207.05952, 2022.\n\nMinghang Zheng, Peng Gao, Renrui Zhang, Xiaogang Wang, Hongsheng Li, and Hao Dong. End-\n\nto-end object detection with adaptive clustering transformer. ArXiv, abs/2011.09315, 2021.\n\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing. arXiv preprint arXiv:2202.09368, 2022.\n\nChen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W Ronny Huang, and Tom Goldstein. Gradinit: Learning to initialize neural networks for stable and efficient training. Advances in Neural Information Processing Systems, 34:16410–16422, 2021.\n\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching In Proceedings of the IEEE international conference on computer movies and reading books. vision, pp. 19–27, 2015.\n\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906, 2022.\n\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Jianfeng Gao, and Tuo Zhao. Taming sparsely activated transformer with stochastic experts. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=B72HXs80q4.\n\n15\n\nA1 MORE IMPLEMENTATION DETAILS\n\nAlgorithm 1: Concrete Dropout in a PyTorch-like style\n\nclass ConcreteDropout(nn.Module):\n\ndef __init__(self, weight_regularizer=1e-6,\n\ndropout_regularizer=1e-5, init_min=0.1, init_max=0.1):\n\nsuper(ConcreteDropout, self).__init__()\n\nself.weight_regularizer = weight_regularizer self.dropout_regularizer = dropout_regularizer init_min = np.log(init_min) - np.log(1. - init_min) init_max = np.log(init_max) - np.log(1. - init_max) self.p_logit = nn.Parameter(torch.empty(1).uniform_(init_min,\n\ninit_max))\n\ndef forward(self, input, next_layer): p = torch.sigmoid(self.p_logit) # Apply Concrete Dropout output = self._concrete_dropout(input, p) # Feed forward through the next layer output = next_layer(output) # Calculate the weight regularizer sum_of_square = 0 for param in next_layer.parameters():\n\nsum_of_square += torch.sum(torch.pow(param, 2))\n\nweights_regularizer = self.weight_regularizer * sum_of_square / (1\n\n- p)\n\n# Calculate the dropout regularizer dropout_regularizer = p * torch.log(p) dropout_regularizer += (1. - p) * torch.log(1. - p) input_dimensionality = input[0].numel() dropout_regularizer *= self.dropout_regularizer *\n\ninput_dimensionality\n\nregularization = weights_regularizer + dropout_regularizer\n\nreturn output, regularization\n\ndef _concrete_dropout(self, input, p):\n\neps = 1e-7; temp = 0.1 # Calculate the dropout probability matrix unif_noise = torch.rand_like(input) drop_prob = (torch.log(p + eps)\n\n- torch.log(1 - p + eps) + torch.log(unif_noise + eps) - torch.log(1 - unif_noise + eps))\n\ndrop_prob = torch.sigmoid(drop_prob / temp) random_tensor = 1 - drop_prob retain_prob = 1 - p # Apply Concrete Dropout output = torch.mul(input, random_tensor) output /= retain_prob\n\nreturn output\n\nAlgorithm 2: DropBlock in a PyTorch-like style\n\ndef drop_block(input, drop_prob, block_size):\n\n# Calculate the mask with zero value for block-wise elements mask = torch.rand_like(x).lt(drop_prob).float() mask = F.max_pool1d(mask, block_size, 1, block_size // 2) mask = 1 - mask output = input * mask # Apply dropout return output\n\nA16\n\nDetails of Concrete Dropout and DropBlock. In our experiments, we use Concrete-Dropout or DropBlock to replace the original dropout layer in MLP blocks. And for Concrete Dropout, we adopt the official implementation https://github.com/yaringal/ConcreteDropout/ blob/master/concrete-dropout-pytorch.ipynb. For DropBlock, we follow the method in Ghiasi et al. (2018). And the PyTorch-style pseudo codes for both methods are presented in Algorithm 1 and 2.\n\nA2 MORE EXPERIMENT RESULTS\n\nA2.1 STABILITY ANALYSIS\n\nTo evaluate the stability of the improvement obtained by our SMoE-Dropout, we carry out further experiments of Transformer-XL on SST-2. The results are reported in Table A4, from which we can observe that our SMoE-Dropout achieves a statistically significant improvement of 0.93% ∼ 1.17% accuracy gains compared with other SMoE-variants and the dense network, where there is no overlap between the error bars (one standard deviation).\n\nTable A4: Test accuracy of Transformer-XL on SST-2. Both the average and standard deviation of accuracy are reported across 3 independent runs.\n\nMethod\n\nDense w. Dropout THOR (k = N) SMoE (k = N)\n\nSMoE-Dropout (k = N 2 ) SMoE-Dropout (k = N)\n\nAccuracy (↑)\n\n81.39 ± 0.31 81.15 ± 0.55 81.20 ± 0.50\n\n82.03 ± 0.26 82.32 ± 0.14\n\nA2.2 COMPARISON WITH LEARNABLE SMOES W. GRADUALLY INCREASED k\n\nTable A5 demonstrates that both random routing policy and progressively increasing the number of activated experts are beneficial for alleviating representation collapse and providing “selfslimmable” property, yet not as good as combining both. To be specific, when applying the strategy of progressively enlarging the number of activated experts, the learnable SMoEs suffer less representation collapse and achieve better performance, i.e., 0.31% higher accuracy. Meanwhile, We find that learnable SMoE with curriculum learning has the “self-slimmable” property only when activating experts from k = 1 to k = 8. However, the performance starts to degrade if using more experts like k = 16. As for our SMoE-Dropout with a random routing, it enjoys a better “self-slimmable” property from k = 1 to k = 16 (full model capacity), with up to 0.87% higher accuracy on SST-2 across all scenarios, compared to its learnable variants.\n\nTable A5: Testing accuracy (%) over # activated experts of Transformer-XL on SST-2. 16\n\n# Activated Experts\n\n4\n\n2\n\n1\n\n8\n\nSMoE w.o. Gradually Increased k SMoE w. Gradually Increased k\n\n80.06 79.40\n\n80.79 81.02\n\n80.58 81.13\n\n80.99 81.71\n\n81.20 81.51\n\nSMoE-Dropout\n\n79.02\n\n81.25\n\n82.00\n\n82.03\n\n82.32\n\nA2.3 TRANSFER STUDY ON MULTI-STEP REASONING TASKS\n\nWe conduct a further transfer study of the pre-trained BERT networks on a multi-hop questionanswering dataset, HotpotQA Yang et al. (2018). And We use exact match (EM) accuracy to assess networks’ performance. Following the same metric in Press et al. (2022), we calculate the compositionality gap, i.e., the gap of EM accuracy between multi-hop question answering and its all single-hop sub-questions Tang et al. (2020), of each network. As shown in Table A6, our SMoEDropout is beneficial for reducing the compositionality gap, which achieves the best performance with up to 0.30% higher EM score and 0.30% narrower compositionality gap, compared with the learnable SMoE and its dense counterpart.\n\nTable A6: The EM score and compositionality gap of the pre-trained BERT networks on HotpotQA.\n\nMetrics\n\nDense w. Dropout\n\nSMoE SMoE-Dropout\n\nEM Accuracy (%) Compositionality Gap (%)\n\n14.90 15.00\n\n15.20 14.70\n\n15.10 14.90\n\nA17",
    "reference": "# Summary Of The Paper\n\nThis paper presents a novel plug-and-play strategy for training large transformer models, which leverages sparse MoEs in a dropout-like manner to scale transformers to better performance in their full capacity without collapse. The method is simple, and the experiments are thorough.\n\n# Strength And Weaknesses\n\nPros\n-\tSMoE-Dropout demonstrates an attractive “self-slimming” property during inference and downstream fine-tuning, which delivers a once-for-all in-situ trade-off between efficiency and performance\n-\tLike classical dropout, SMoE-Dropout is able to mitigate the representation collapse that standard MoEs usually suffer from, i.e., activating more experts do not improve or even hurt performance.\n-\tApplying SMoE-Dropout is extremely cheap as only random router is adopted\n-\tTransfer study is strong, which provides another evidence that more pre-training information is effectively retained by the full transformer capacity. It is further validated in ablation studies 4.4\n\nCons:\n-\tThe performance gain of SMoE-Dropout is sometimes marginal, such as on BeRT (section 4.1). Moreover, as seen from Figure 1 and Figure 3, it seems SMoE-Dropout does sacrifice performance at low parameter counts compared to Learnable MoE, why?\n-\tWhen breaking a single-stream models into MLP MoEs (the modularization step in section 3.2), how to decide the number of MLP experts needed, i.e., N? This seems to be an important hyperparameter but not discussed. Also, why not applying SMoE-Dropout to MLP layers but not self-attention? \n-\tFor training with Learnable SMoE, it is further unclear how the authors selected its k?\n-\tWhy training time is the same between SMoE-dropout k=N/2 and k=N, in Table 1?\n-\tTable 1 again: are all those numbers averaged across three datasets? The authors didn’t explain\n\n# Clarity, Quality, Novelty And Reproducibility\n\nAll look good to me. Authors promised to release models and codes: it would help if the authors could respond to be clearer what they plan to release: all experiments versus some, pre-trained model versus training script, etc.\n\n# Summary Of The Review\n\nOverall, I would tend to accept this paper. If the author can address my concerns. I would be more convinced.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSMART MULTI-TENANT FEDERATED LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFederated learning (FL) is an emerging distributed machine learning method that empowers in-situ model training on decentralized edge devices. However, multiple simultaneous training activities could overload resource-constrained devices. In this work, we propose a smart multi-tenant FL system, MuFL, to effectively coordinate and execute simultaneous training activities. We first formalize the problem of multi-tenant FL, define multi-tenant FL scenarios, and introduce a vanilla multitenant FL system that trains activities sequentially to form baselines. Then, we propose two approaches to optimize multi-tenant FL: 1) activity consolidation merges training activities into one activity with a multi-task architecture; 2) after training it for rounds, activity splitting divides it into groups by employing affinities among activities such that activities within a group have better synergy. Extensive experiments demonstrate that MuFL outperforms other methods while consuming 40% less energy. We hope this work will inspire the community to further study and optimize multi-tenant FL.\n\n1\n\nINTRODUCTION\n\nFederated learning (FL) (McMahan et al., 2017) has attracted considerable attention as it enables privacy-preserving distributed model training among decentralized devices. It is empowering growing numbers of applications in both academia and industry, such as Google Keyboard (Hard et al., 2018), medical imaging analysis (Li et al., 2019; Sheller et al., 2018), and autonomous vehicles (Zhang et al., 2021a; Posner et al., 2021). Among them, some applications contain multiple training activities for different tasks. For example, Google Keyboard includes query suggestion (Yang et al., 2018), emoji prediction (Ramaswamy et al., 2019), and next-world prediction (Hard et al., 2018); autonomous vehicles relates to multiple computer vision (CV) tasks, including lane detection, object detection, and semantic segmentation (Janai et al., 2020).\n\nHowever, multiple simultaneous training activities could overload edge devices (Bonawitz et al., 2019). Edge devices have tight resource constraints, whereas training deep neural networks for the aforementioned applications is resource-intensive. As a result, the majority of edge devices can only support one training activity at a time (Liu et al., 2019); multiple simultaneous federated learning activities on the same device could overwhelm its memory, computation, and power capacities. Thus, it is important to navigate solutions to well coordinate these training activities.\n\nA plethora of research on FL considers only one training activity in an application. Many studies are devoted to addressing challenges including statistical heterogeneity (Li et al., 2020; Wang et al., 2020a), system heterogeneity (Chai et al., 2020; Yang et al., 2021), communication efficiency (Karimireddy et al., 2020; Zhu et al., 2021), and privacy issues (Bagdasaryan et al., 2020; Huang et al., 2021). A common limitation is that they only focus on one training activity, but applications like Google Keyboard and autonomous vehicles require multiple training activities for different targets (Yang et al., 2018; Ramaswamy et al., 2019). Multi-tenancy of an FL system is designed by Bonawitz et al. (2019) to prevent simultaneous training activities from overloading devices. However, it mainly considers differences among training activities, neglecting potential synergies.\n\nIn this work, we propose a smart multi-tenant federated learning system, MuFL, to efficiently coordinate and execute simultaneous training activities under resource constraints by considering both synergies and differences among training activities. We first formalize the problem of multitenant FL and define four multi-tenant FL scenarios based on two variances in Section 3: 1) whether all training activities are the same type of application, e.g., CV applications; 2) whether all clients\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nsupport all training activities. Then, we define a vanilla multi-tenant FL system that supports all scenarios by training activities sequentially. Built on it, we further optimize the scenario, where all training activities are the same type and all clients support all activities, by considering both synergies and differences among activities in Section 4. Specifically, we propose activity consolidation to merge training activities into one activity with a multi-task architecture that shares common layers and has specialized layers for each activity. We then introduce activity splitting to divide the activity into multiple activities based on their synergies and differences measured by affinities between activities.\n\nWe demonstrate that MuFL reduces the energy consumption by over 40% while achieving superior performance to other methods via extensive experiments on three different sets of training activities in Section 5. We believe that MuFL is beneficial for many real-world applications such as autonomous vehicles, voice assistance systems, and robotics (more examples in Appendix A). We summarize our contributions as follows:\n\n• We formalize the problem of multi-tenant FL and define four multi-tenant FL scenarios. To the best of our knowledge, we are the first work that investigates multi-tenant FL in-depth. • We propose MuFL, a smart multi-tenant federated learning system to efficiently coordinate and execute simultaneous training activities by proposing activity consolidation and activity splitting to consider both synergies and differences among training activities.\n\n• We establish baselines for multi-tenant FL and demonstrate that MuFL elevates performance\n\nwith significantly less energy consumption via extensive empirical studies.\n\n2 RELATED WORK\n\nIn this section, we first review the concept of multi-tenancy in cloud computing and machine learning. Then, we provide a literature review of multi-task learning and federated learning.\n\nMulti-tenancy of Cloud Computing and Machine Learning Multi-tenancy has been an important concept in cloud computing. It refers to the software architecture where a single instance of software serves multiple users (Chong & Carraro, 2006; Fehling et al., 2010). Multi-tenant software architecture is one of the foundations of software as a service (SaaS) applications (Mietzner et al., 2008; Cai et al., 2013). Recently, researchers have adopted this idea to machine learning (especially deep learning) training and inference. Specifically, some studies investigate how to share GPU clusters among multiple users to train deep neural networks (DNN) (Jeon et al., 2019; Zhao et al., 2020; Lao et al., 2021), but these methods are for GPU clusters that have enormous computing resources, which are inapplicable to edge devices that have limited resources. Targeting on-device deep learning, some researchers define multi-tenant as processing multiple computer vision (CV) applications for multiple concurrent tasks (Fang et al., 2018; Jiang et al., 2018). However, they focus on the multi-tenant on-device inference rather than training. On the contrary, we focus on multi-tenant federated learning (FL) training on devices, where the multi-tenancy refers to multiple concurrent FL training activities.\n\nMulti-task Learning Multi-task learning is a popular machine learning approach to learn models that generalize on multiple tasks (Thrun, 1995; Zhang & Yang, 2021). A plethora of studies investigate parameter sharing approaches that share common layers of a similar architecture (Caruana, 1997; Eigen & Fergus, 2015; Bilen & Vedaldi, 2016; Nekrasov et al., 2019). Besides, many studies employ new techniques to address the negative transfer problem (Kang et al., 2011; Zhao et al., 2018) among tasks, including soft parameter sharing (Duong et al., 2015; Misra et al., 2016), neural architecture search (Lu et al., 2017; Huang et al., 2018; Vandenhende et al., 2019; Guo et al., 2020; Sun et al., 2020), and dynamic loss reweighting strategies (Kendall et al., 2018; Chen et al., 2018; Yu et al., 2020). Instead of training all tasks together, task grouping trains only similar tasks together. The early works of task grouping (Kang et al., 2011; Kumar & Daum ́e, 2012) are not adaptable to DNN. Recently, several studies analyze the task similarity (Standley et al., 2020) and task affinities (Fifty et al., 2021) for task grouping. In this work, we adopt the idea of task grouping to consolidate and split training activities. The state-of-the-art task grouping methods (Standley et al., 2020; Fifty et al., 2021), however, are unsuitable for our scenario because they focus on the inference efficiency, bypassing the intensive computation on training. Thus, we propose activity consolidation and activity splitting to group training activities based on their synergies and differences.\n\nFederated Learning Federated learning emerges as a promising privacy-preserving distributed machine learning technique that uses a central server to coordinate multiple decentralized clients to\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ntrain models (McMahan et al., 2017; Kairouz et al., 2021). The majority of studies aim to address the challenges of FL, including statistical heterogeneity (Li et al., 2020; Wang et al., 2020a;b; Zhuang et al., 2020; yuyang deng et al., 2021; Zhang et al., 2021b), system heterogeneity (Chai et al., 2020; Yang et al., 2021), communication efficiency (McMahan et al., 2017; Koneˇcn ́y et al., 2016; Karimireddy et al., 2020; Zhu et al., 2021), and privacy concerns (Bagdasaryan et al., 2020; Huang et al., 2021). Among them, federated multi-task learning (Smith et al., 2017; Marfoq et al., 2021) is an emerging method to learn personalized models to tackle statistical heterogeneity. However, these personalized FL methods mainly focus on training one activity of an application in a client. Multi-tenant FL that handles multiple concurrent training activities is rarely discussed; the prior work (Bonawitz et al., 2019) mainly considers the differences among training activities. In this work, we optimize multi-tenant FL by further considering their synergies by splitting activities into groups. Our problem is also fundamentally different from clustered FL (Ghosh et al., 2020; Ouyang et al., 2021); They group models of the same training activity, whereas we group training activities.\n\n3 PROBLEM SETUP\n\nThis section provides preliminaries of federated learning (FL), presents the problem definition of multi-tenant FL, and classifies four multi-tenant FL scenarios. Besides, we introduce a vanilla multi-tenant FL system supports for all scenarios.\n\n3.1 PRELIMINARIES AND PROBLEM DEFINITION\n\nIn the federated learning setting, the majority of studies consider optimizing the following problem:\n\nmin ω∈Rd\n\nf (ω) :=\n\nK (cid:88)\n\nk=1\n\npkfk(ω) :=\n\nK (cid:88)\n\nk=1\n\npkEξk∼Dk [fk(ω; ξk)],\n\n(1)\n\nwhere ω is the optimization variable, K is the number of selected clients to execute training, fk(ω) is the loss function of client k, pk is the weight of client k in model aggregation, and ξk is the training data sampled from data distribution Dk of client k. FedAvg (McMahan et al., 2017) is a popular federated learning algorithm, which sets pk to be proportional to the dataset size of client k.\n\nEquation 1 illustrates the objective of single training activity in FL, but in real-world scenarios, multiple simultaneous training activities could overload edge devices. We further formalize the problem of multi-tenant FL as follows.\n\nIn multi-tenant FL, a server coordinates a set of clients C to execute a set of n FL training activities A = {α1, α2, . . . , αn}. It obtain a set of parameters of models W = {ω1, ω2, . . . , ωn}, where each model ωi is for activity αi. By defining M(αi; ωi) as performance measurement of each training activity αi, multi-tenant FL aims to maximize the performance of all training activities (cid:80)n i=1 M(αi; ωi), under the constraint that each client k has limited memory budget and computation budget. These budgets constrain the number of concurrent training actvitities nk on client k. Besides, as devices have limited battery life, we would like to minimize the energy consumption and training time to obtain W from training activities A.\n\n3.2 MULTI-TENANT FL SCENARIOS\n\nWe classify multi-tenant FL into four different scenarios based on variances in two aspects: 1) whether all training activities in A are the same type of application, e.g., computer vision (CV) applications or natural language processing (NLP) applications; 2) whether all clients in C support all training activities in A. We depict these four scenarios in Figure 6 in Appendix A and describe them below.\n\nScenario 1 ∀αi ∈ A, αi is the same type of application; ∀αi ∈ A, ∀ck ∈ C supports αi. For example, autonomous vehicles (clients) support the same sets of CV applications, such as object detection and semantic segmentation. Thus, they support training activities of these applications.\n\nScenario 2 ∃αi ∈ A, αi is a different type of application; ∀αi ∈ A, ∀ck ∈ C supports αi. For example, Google Keyboard has different types of applications, including recommendation (query suggestion (Yang et al., 2018)) and NLP (next-world prediction (Hard et al., 2018)). Mobile phones (clients) with Google Keyboard support these applications together with all related training activities.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Vanilla Multi-tenant FL\n\n(b) Smart Multi-tenant FL\n\nFigure 1: The architectures of proposed multi-tenant federated learning (FL) systems. The vanilla multi-tenant FL system (a) employs a scheduler to queue simultaneous training activities and execute them one by one. The smart multi-tenant FL system (b) proposes activity consolidation and activity splitting to consider both synergies and differences among training activities, which elevates performance and reduces resource consumption.\n\nScenario 3 ∀αi ∈ A, αi is the same type of application; ∀αi ∈ A, ∃ck ∈ C does not support αi. For example, survelliance cameras (clients) in parking lots could support CV applications, but cameras in different locations may support different applications, e.g., counting open-parking spots, tracking parking duration, or recording fender benders.\n\nScenario 4 ∃αi ∈ A, αi is a different type of application; ∀αi ∈ A, ∃ck ∈ C does not support αi. For example, browsers (clients) could leverage users’ browsing history to support ranking (Hartmann et al., 2019) and news recommendation (Minto et al., 2021), which are different applications. Users may opt-out of recommendations, resulting in not all browsers supporting all training activities.\n\nThe application determines the multi-tenant FL scenario. We next introduce a vanilla multi-tenant FL that supports all these scenarios as our baseline.\n\n3.3 VANILLA MULTI-TENANT FL\n\nFigure 1a presents the architecture of a vanilla multi-tenant FL system. It prevents overloading and congestion of multiple simultaneous training activities by scheduling them to execute one by one. Particularly, we use a scheduler to queue training activities in the server (e.g., First in, First out). In each round, the server selects K clients from the client pool to participate in training. The number of simultaneous training activities depend on the computational resources of the selected clients. In this study, we assume that each client can execute one training activity at a time (nk = 1). This is a realistic assumption for the majority of current edge devices. 1 As a result, the vanilla multi-tenant FL system executes training activities sequentially.\n\nThe vanilla multi-tenant FL system supports the four multi-tenant FL scenarios. From the perspective of the type of application, it can handle different application types of training activities as each training activity is executed independently. From the perspective of whether clients support all training activities, each training activity can select clients that support the activity to participate in training. Despite its comprehensiveness, it only considers differences among training activities, neglecting their potential synergies. In contrast, our proposed MuFL considers both synergies and differences among training activities to further optimize the Scenario 1.\n\n1Edges devices, e.g., NVIDIA Jetson TX2 and AGX Xavier, have only one GPU; GPU virtualization (Hong\n\net al., 2017) that enables concurrent training on the same GPU currently are mainly for the cloud stack.\n\n4\n\nTraining ActivitiesServer1. Scheduling2. TrainingClient PoolNew training activityScheduledTrainedStatusParticipants...2.1 Execution2.2 AggregationServer1. Activity Consolidation3. TrainingTraining Activities2.Scheduling4. Activity SplittingAll-in-oneGroupedOne by oneTraining typesClient PoolParticipants...3.2 Aggregation3.1 ExecutionUnder review as a conference paper at ICLR 2023\n\n4 SMART MULTI-TENANT FL\n\nIn this section, we introduce the smart multi-tenant FL system, MuFL. We start by providing an overview of MuFL. Then, we present two important components of MuFL, activity consolidation and activity splitting, to consider both synergies and differences among simultaneous training activities.\n\nFigure 1b depicts the architecture and training processes of MuFL. It contains a server to coordinate training activities and a pool of clients to execute training. MuFL optimizes the Scenario 1 discussed previously with the following steps: 1) The server receives training activities A = {α1, α2, . . . , αn} to train models W = {ω1, ω2, . . . , ωn} and consolidates these activities into an all-in-one training activity α0; 2) The server schedules α0 to train; 3) The server select K clients from the client pool to execute α0 iteratively through FL process for R0 rounds; 4) The server splits the all-in-one activity α0 into multiple training activity groups {A1, A2, . . . }, where each group trains nonoverlapping subset of W; the number of groups can be determined by the inference budget for the number of concurrent models. 5) The server iterates step 2 and 3 to train Aj. We summarize MuFL in Algorithm 1 in Appendix D and introduce the details of activity consolidation and activity splitting next.\n\n4.1 ACTIVITY CONSOLIDATION\n\nFocusing on optimizing the Scenario 1 of multi-tenant FL, we first propose activity consolidation to consolidate multiple training activities into an all-in-one training activity, as illustrated in the first step of Figure 1b. In Scenario 1, all training activities are the same type of application and all clients support all training activities. Since training activities A = {α1, α2, . . . , αn} are of the same type, e.g., CV or NLP, models W = {ω1, ω2, . . . , ωn} could share the same backbone (they share the same encoder but could have different decoders). Thus, we can consolidate A into an all-in-one training activity α0 that trains a multi-task model ν = {θs} ∪ {θαi|αi ∈ A}, where θs is the shared model parameters and θαi is the specific parameters for training activity αi ∈ A. The loss function for all-in-one training is L(X , θs, {θαi}) = (cid:80) Activity consolidation leverages synergies among training activities and effectively reduces the computation cost of multi-tenant FL from multiple trainings into a single training. However, simply employing activity consolidation is another extreme of multi-tenant FL that only considers synergies among activities. As shown in Figure 2, all-in-one method is efficient in energy consumption, but it leads to unsatisfactory performance. Consequently, we further propose activity splitting to consider both synergies and differences among training activities.\n\nαi∈A Lαi(X , θs, {θαi}).\n\n4.2 ACTIVITY SPLITTING\n\nWe propose activity splitting to divide the all-in-one activity α0 into multiple groups after it is trained for certain rounds. Essentially, we aim to split A = {α1, α2, . . . , αn} into multiple nonoverlapping groups such that training activities within a group have better synergy. Let {A1, A2, . . . , Am} be subsets of A, we aim to find a disjoint set I of A, where I ⊆ {1, 2, . . . , m}, |I| ≤ |A|, (cid:83) j∈I Aj = A, and (cid:84) s} ∪ {θαi|αi ∈ Aj}, which is a multi-task network when Aj contains more than one training activity, where θj s is the shared model parameters and θαi is the specific parameters for training activity αi ∈ Aj. The core question is how to determine set I to split these activities considering their synergies and differences.\n\nj∈I Aj = ∅. Each group Aj trains a model νj = {θj\n\nInspired by TAG (Fifty et al., 2021) that measures task affinites for task grouping, we employ affinities between training activities for activity splitting via three stages: 1) we measure affinities among activities during all-in-one training; 2) we select the best combination of splitted training activities based on affinity scores; 3) we continue training each split with its model initialized with parameters obtained from all-in-one training. Particularly, during training of all-in-one activity α0, we measure the affinity of training activity αi onto αj at time step t in each client k with the following equation:\n\nS k,t\n\nαi→αj\n\n= 1 −\n\nLαj (X k,t, θk,t+1 Lαj (X k,t, θk,t\n\n, θk,t αj s , θk,t αj )\n\ns,αi\n\n)\n\n,\n\n(2)\n\nwhere Lαj is the loss function of αj, X k,t is a batch of training data, and θk,t shared model parameters before and after updated by αi, respectively. Positive value of S k,t\n\nand θk,t+1\n\ns,αi\n\ns\n\nare the\n\nαi→αj\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(a) A set of five training activities: sdnkt\n\n(b) A set of nine trianing activities: sdnkterca\n\nFigure 2: Comparison of test loss and energy consumption on two training activity sets: (a) sdnkt and (b) sdnkterca, where each character represents an activity. Compared with all-in-one methods, our method achieves much better performance with slight increases in computation. Moreover, our method achieves the best performance while consuming less energy than the other methods.\n\nmeans that activity αi helps reduce the loss of αj. This equation measures the affinity of one timestep of one client. We approximate affinity scores for each round by averaging the values over T time-steps in E local epochs and K selected clients: ˆSαi→αj = 1 αi→αj , where T is total time steps determined by the frequency f of calculating Equation 2, e.g., f = 5 means measuring the affinity in each client in every five batches.\n\nt=1 S k,t\n\n(cid:80)K\n\n(cid:80)E\n\n(cid:80)T\n\nKET\n\nk=1\n\ne=1\n\nThese affinity scores measure pair-wise affinities between traininig activities. We next use them to calculate total affinity scores of a grouping with (cid:80)n ˆSαi, where ˆSαi is the averaged affinity score onto each training activity. For example, in a grouping of two splits of five training activities {{α1, α2}, {α3, α4, α5}}, where {α1, α2} is one split and {α3, α4, α5} is another split. The affinity score onto α1 is ˆSα1 = ˆSα2→α1 and the affinity score onto α3 is ˆSα3 = ( ˆSα4→α3 + ˆSα5→α3 )/2. Consequently, we can find the set I with |I| elements for subsets of A that maximize (cid:80)n ˆSαi, where |I| defines the number of elements.\n\ni=1\n\ni=1\n\nWe would like to further highlight the differences between our method and TAG (Fifty et al., 2021). Firstly, TAG focuses on inference efficiency, thus it allows overlapping task grouping that could train one task multiple times. In contrast, our focus is fundamentally different; we focus on training efficiency and consider only nonoverlapping activity splitting. Secondly, TAG is computationintensive for higher numbers of splits, e.g., it fails to produce results of five splits of nine tasks in a week, whereas we only need seconds of computation. Thirdly, TAG rules out the possibility that a group contains only one task as it sets ˆSαi→αi = 1e−6, which is much smaller than scores of other groupings. Besides, ˆSαi→αi calculated from Equation 2 is also not desirable; it always results in a group containing only one task as its value is much larger (could be 10x larger) than scores of other groupings. To overcome these issues, we propose a new method to calculate this value:\n\nˆSαi→αi =\n\n(cid:88)\n\nj∈N \\{i}\n\n( ˆSαi→αj + ˆSαj→αi) 2n − 2\n\n,\n\n(3)\n\nwhere N = {1, 2, . . . , n}. The intuition is that it measures the normalized affinity of activity αi to other activities and other activities to αi. Fourthly, we focus on multi-tenant FL, thus, we further aggregate affinity scores over K selected clients. Fifthly, TAG trains each set Aj from scratch, whereas we initialize their models with the parameters obtained from all-in-one training.\n\n5 EXPERIMENTS\n\nWe evaluate the performance and resource usage of MuFL and design our experiments to answer the following questions: 1) How effective is our activity splitting approach? 2) When to split the training activities? 3) Is it beneficial to iteratively split the training activities? 4) What is the impact of local epoch and scaling up the number of selected clients in each training round?\n\n6\n\n48121620242832Energy(kWh)0.560.580.600.620.640.660.68TestLossOnebyoneAll-in-oneGradNormFedProxTAGHOAOursTAGHOAOursTAGHOAOurs2Splits3Splits4Splits68101214Energy(kWh)1.381.401.421.441.461.481.50TestLossOursOnebyoneAll-in-oneGradNormFedProxTAGOursTAGOursTAGOursBetter2Splits3Splits4Splits5SplitsUnder review as a conference paper at ICLR 2023\n\nTable 1: Performance (test loss) comparison of our method with the optimal and worst splits. Our method achieves the best performance, indicating the effectiveness of our activity splitting method.\n\nActivity Set\n\nsdnkt\n\nerckt\n\nSplits\n\n2 3\n2 3\n\nTrain from Scratch\n\nOurs\n\nOptimal 0.578 ± 0.015 0.622 ± 0.007 0.685 ± 0.010 0.555 ± 0.008 0.585 ± 0.026 0.674 ± 0.022 1.039 ± 0.024 1.070 ± 0.013 1.312 ± 0.065 1.015 ± 0.018 1.058 ± 0.029 1.243 ± 0.099\n\nWorst\n\nTrain from Initialization Optimal\n\nWorst\n\n0.595 ± 0.008 0.595 ± 0.004 0.560 ± 0.006 0.578 ± 0.006 1.048 ± 0.024 1.068 ± 0.037 1.020 ± 0.012 1.052 ± 0.026\n\nExperiment Setup We construct the Scenario 1 of multi-tenant FL scenarios using Taskonomy dataset (Zamir et al., 2018), which is a large computer vision dataset of indoor scenes of buildings. We run experiments with N = 32 clients, where each client contains a dataset of a building to simulate the statistical heterogeneity in FL. Three sets of training activities are used to evaluate the robustness of MuFL: sdnkt, erckt, and sdnkterca; each character represents an activity, e.g., s represents semantic segmentation. We measure the statistical performance of an activity set using the sum of test losses of individual activities. By default, we use K = 4 selected clients and E = 1 local epoch for each round of training. More experimental details are provided in Appendix B.\n\n5.1 PERFORMANCE EVALUATION\n\nWe compare the performance, in terms of test loss and energy consumption, among the following methods: 1) one by one training of activities (i.e., the vanilla multi-tenant FL); 2) all-in-one training of activities (i.e., using only activity consolidation); 3) all-in-one training with multi-task optimization (GradNorm (Chen et al., 2018)) and federated optimization (FedProx (Li et al., 2020)); 4) estimating higher-order of activity groupings from pair-wise activities performance (HOA (Standley et al., 2020)); 5) grouping training activities with only task affinity grouping method (TAG (Fifty et al., 2021)); 6) MuFL with both activity consolidation and activity splitting. Carbontracker (Anthony et al., 2020) is used to measure energy consumption and carbon footprint (provided in Appendix C).\n\nFigure 2 compares performance of the above methods on activity sets sdnkt and sdnkterca. The methods that achieve lower test loss and lower energy consumption are better. At the one extreme, all-in-one methods (including GradNorm) consumes the least energy, but their test losses are the highest. Simply applying federated optimization, FedProx, can hardly improve performance, especially on sdnkterca. At the other extreme, HOA achieves comparable test losses on three or four splits of sdnkt, but it demands high energy consumption (∼ 4 − 6× of ours) to compute pair-wise activities for higher-order estimation. Although training activities one by one and TAG present a good balance between test loss and energy consumption, MuFL is superior in both aspects; it achieves the best test loss with ∼40% and ∼50% less energy consumption on activity set sdnkt and sdnkterca, respectively. Additionally, more splits of activity in the activity splitting lead to higher energy consumption, but it could help further reduce test losses. We do not report HOA for activity set sdnkterca due to computation constraints.2 We omit to report the running time as it is hidden under the metric of energy consumption; higher energy consumption implies longer training time. We provide more details of these experiments and results of activity set erckt in Appendix C.\n\n5.2 HOW EFFECTIVE IS OUR ACTIVITY SPLITTING APPROACH?\n\nWe demonstrate the effectiveness of our activity splitting approach by comparing it with the possible optimal and worst splits. The optimal and worst splits are obtained with two steps: 1) we measure the performance over all combinations of two splits and three splits of an activity set by training them from scratch;3 2) we select the combination that yields the best performance as the optimal split and the worst performance as the worst split.\n\nTable 1 compares the test loss of MuFL with the optimal and worst splits trained in two ways: 1) training each split from scratch; 2) training each split the same way as our activity splitting — initializing models with the parameters obtained from all-in-one training. On the one hand, training\n\n2HOA computes at least 36 pairs of activities (∼720 GPU hours), consuming ∼12× more energy than MuFL. 3There are fifteen and twenty-five combinations of two and three splits, respectively, for a set of five activities.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Affinities to activity d\n\n(b) Affinities to activity r\n\n(c) Affinities to activity s\n\nFigure 3: Changes of affinity scores of one activity to the other on activity set sdnkterca. Activities d and r have high inter-activity scores. The trends of affinities emerge at the early stage of training.\n\n(a) Training activity set: sdnkt\n\n(b) Training activity set: erckt\n\n(c) Activity set: sdnkterca\n\nFigure 4: Performance comparison of training all-in-one activities for different R0 rounds. Fixing the total training rounds R = 100, our method achieves the best performance when R0 ∈ {20, 30, 40}.\n\nfrom initialization outperforms training from scratch in all settings. It suggests that initializing each split with all-in-one training model parameters can significantly improve the performance. On the other hand, our activity splitting method achieves the best performance in all settings, even though training from initialization reduces the gaps of different splits (the optimal and worst splits). These results indicate the effectiveness of our activity splitting approach.\n\n5.3 WHEN TO SPLIT TRAINING ACTIVITIES?\n\nWe further answer the question that how many R0 rounds should we train the all-in-one activity before activity splitting. It is determined by two factors: 1) the rounds needed to obtain affinity scores for a reasonable activity splitting; 2) the rounds that yield the best overall performance.\n\nAffinity Analysis We analyze changes in affinity scores over the course of training to show that early-stage affinity scores are acceptable for activity splitting. Figure 3 presents the affinity scores of different activities to one activity on activity set sdnkterca. Figure 3a and 3b indicate that activity d and activity r have high inter-activity affinity scores; they are divided into the same group as a result. In contrast, both d and r have high affinity score to activity s in Figure 3c, but not vice versa. These trends emerge in the early stage of training, thus, we employ the affinity scores of the tenth round for activity splitting by default; they are effective in achieving promising results as shown in Figure 2 and Table 1. We provide more affinity scores of other activities in Appendix C.\n\nThe Impact of R0 Rounds Figure 4 compares the performance of training R0 for 10 to 90 rounds before activity splitting. Fixing the total training round R = 100, we train each split of activities for R1 = R − R0 rounds. The results indicate that MuFL achieves the best performance when R0 = {20, 30, 40} rounds. Training the all-in-one activity for enough rounds helps utilize the benefits and synergies of training together, but training for too many rounds almost suppresses the benefits of considering differences among activities. We suggest training R0 for [20, 40] that strikes a good balance between these two extremes and consider other mechanisms to determine R0 in future works.\n\n5.4 HIERARCHICAL SPLITTING\n\nThis section evaluates an alternative activity splitting strategy. In activity splitting, we can divide the all-in-one training activity into {2, 3, . . . } splits. As shown in Figure 2, more splits lead to better performance with slightly higher energy consumption in the five-activity set, but the trend is not straightforward in the nine-activity set. Apart from setting the number of splits directly, MuFL can split the training activity into more splits adaptively via two steps: 1) dividing the all-in-one activity into two splits and training each one for R1 rounds; 2) further dividing one of them to two splits and train these three activities for R2 rounds. We term the adaptive process as hierarchical splitting.\n\n8\n\n020406080100TrainingRound0.000.641.281.922.56Affinitysnkertac020406080100TrainingRound0.000.340.681.021.36Affinitysdnketac020406080100TrainingRound0.000.040.080.120.16Affinitydnkertac102030405060708090All-in-oneRounds(R0)0.550.600.650.70TestLoss2Splits3Splits4Splits102030405060708090All-in-oneRounds(R0)0.981.021.061.101.14TestLoss2Splits3Splits4Splits102030405060708090All-in-oneRounds(R0)1.351.401.451.501.551.60TestLoss2Splits3Splits4Splits5SplitsUnder review as a conference paper at ICLR 2023\n\nTable 2: Performance of hierarchical splitting on three activity sets sdnkt, erckt, and sdnkterca. Hierarchical splitting outperforms two splits and achieves similar performance to three splits with less energy (kWh) consumption.\n\nMethod\n\nsdnkt\n\nerckt\n\nsdnkterca\n\nEnergy\n\nTest Loss\n\nEnergy\n\nTest Loss\n\nEnergy\n\nTest Loss\n\nTwo Splits 4.9 ± 0.3 0.578 ± 0.015 Three Splits 5.4 ± 0.7 0.555 ± 0.008 Hierarchical 5.3 ± 0.4 0.563 ± 0.007\n\n6.7 ± 0.2 1.039 ± 0.024 7.2 ± 0.2 1.015 ± 0.018 6.9 ± 0.2 1.022 ± 0.020\n\n6.0 ± 0.1 1.445 ± 0.021 6.6 ± 0.4 1.391 ± 0.030 6.5 ± 0.3 1.403 ± 0.024\n\nTable 2 compares the performance of hierarchical splitting (3 splits) with directly splitting to multiple splits on three activity sets. We use R0 = 30, R1 = 40, and R2 = 30 for activity sets sdnkt and erckt, and R0 = 30, R1 = 20, and R2 = 50 for sdnkterca. Hierarchical splitting effectively reduces test losses of two splits and achieves comparable performance to three splits with less energy consumption. These results suggest that hierarchical splitting can be an alternative method of activity splitting. Additionally, these results also demonstrate the possibility of other activity splitting strategies to be considered in future works.\n\n5.5 ADDITIONAL ANALYSIS\n\nThis section analyzes the impact of local epoch E and the number of selected clients K in FL using all-in-one training. We report the results of activity set sdnkt here and provide more results in Appendix C.\n\nImpact of Local Epoch E Local epoch defines the number of epochs each client trains before uploading training updates to the server. Figure 5a compares test losses of local epochs E = {1, 2, 5, 10}. Larger E could lead to better performance with higher computation (fixed training round R = 100), but it is not effective when increasing E = 5 to E = 10. It suggests the limitation of simply increasing computation with larger E in improving performance. Note that MuFL (Table 1) achieves better results than E = 5 with ∼ 5× less computation.\n\n(a) Impact of E\n\n(b) Impact of K\n\nFigure 5: Analysis of the impact of (a) local epoch E and (b) the number of selected clients K on activity set sdnkt. Larger E and K could reduce losses with more computation, but the benefit decreases as computation increases.\n\nImpact of The Number of Selected Clients K Figure 5b compares test losses of the number of selected clients K = {2, 4, 6, 8, 16} in each round. Increasing the number of selected clients improves the performance, but the effect becomes marginal as K increases. Larger K can also be considered as using more computation in each round. Similar to the results of the impact of E, simply increasing computation can only improve performance to a certain extent. It also shows the significance of MuFL that increases performance with slightly more computation. We use K = 4 by default for experiments and demonstrate that MuFL is also effective on K = 8 in Appendix C.\n\n6 CONCLUSIONS\n\nIn this work, we propose a smart multi-tenant federated learning system to effectively coordinate and execute multiple simultaneous FL training activities. In particular, we introduce activity consolidation and activity splitting to consider both synergies and differences among training activities. Extensive empirical studies demonstrate that our method is effective in elevating performance and significant in reducing energy consumption and carbon footprint by more than 40%, which are important metrics to our society. We believe that multi-tenant FL will emerge and empower many real-world applications with the fast development of FL. We hope this research will inspire the community to further work on algorithm and system optimizations of multi-tenant FL. Future work involves designing better scheduling mechanisms to coordinate training activities. Client selection strategies can also be considered to optimize resource and training allocation, and extend our optimization approaches to other multi-tenant FL scenarios.\n\n9\n\n12510E0.50.60.7TestLoss246816K0.60.70.80.9TestLossUnder review as a conference paper at ICLR 2023\n\n7 REPRODUCIBILITY STATEMENT\n\nTo facilitate reproducibility, we provide basic experimental setups in Section 5 and include more details about the dataset, implementation details, and hyperparameters in Appendix B. We also provide the algorithm of MuFL in Algorithm 1. Besides, the implementation codes will be open-sourced in the future.\n\nREFERENCES\n\nLasse F. Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. Carbontracker: Tracking and predicting the carbon footprint of training deep learning models. ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems, July 2020. arXiv:2007.03051.\n\nEugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, pp. 2938–2948. PMLR, 2020.\n\nHakan Bilen and Andrea Vedaldi. Integrated perception with recurrent multi-task neural networks.\n\nAdvances in neural information processing systems, 29, 2016.\n\nKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Koneˇcn`y, Stefano Mazzocchi, Brendan McMahan, et al. Towards federated learning at scale: System design. Proceedings of Machine Learning and Systems, 1: 374–388, 2019.\n\nHong Cai, Berthold Reinwald, Ning Wang, and Chang Jie Guo. Saas multi-tenancy: Framework, technology, and case study. In Cloud Computing Advancements in Design, Implementation, and Technologies, pp. 67–82. IGI Global, 2013.\n\nRich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.\n\nZheng Chai, Ahsan Ali, Syed Zawad, Stacey Truex, Ali Anwar, Nathalie Baracaldo, Yi Zhou, Heiko Ludwig, Feng Yan, and Yue Cheng. Tifl: A tier-based federated learning system. In Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing, pp. 125–136, 2020.\n\nZhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International Conference on Machine Learning, pp. 794–803. PMLR, 2018.\n\nFranc ̧ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of\n\nthe IEEE conference on computer vision and pattern recognition, pp. 1251–1258, 2017.\n\nFrederick Chong and Gianpaolo Carraro. Architecture strategies for catching the long tail. MSDN\n\nLibrary, Microsoft Corporation, 910, 2006.\n\nLong Duong, Trevor Cohn, Steven Bird, and Paul Cook. Low resource dependency parsing: Crosslingual parameter sharing in a neural network parser. In Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing (volume 2: short papers), pp. 845–850, 2015.\n\nDavid Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In Proceedings of the IEEE international conference on computer vision, pp. 2650–2658, 2015.\n\nBiyi Fang, Xiao Zeng, and Mi Zhang. Nestdnn: Resource-aware multi-tenant on-device deep learning for continuous mobile vision. In Proceedings of the 24th Annual International Conference on Mobile Computing and Networking, pp. 115–127, 2018.\n\nChristoph Fehling, Frank Leymann, and Ralph Mietzner. A framework for optimized distribution of tenants in cloud applications. In 2010 IEEE 3rd International Conference on Cloud Computing, pp. 252–259. IEEE, 2010.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nChris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently identifying task groupings for multi-task learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nFLAIROP. Federated learning for robot picking, 2022. URL https://flairop.com/.\n\nAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered federated learning. Advances in Neural Information Processing Systems, 33:19586– 19597, 2020.\n\nPengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning to branch for multi-task learning. In\n\nInternational Conference on Machine Learning, pp. 3854–3863. PMLR, 2020.\n\nAndrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Franc ̧oise Beaufays, Sean Augenstein, Hubert Eichner, Chlo ́e Kiddon, and Daniel Ramage. Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.\n\nFlorian Hartmann, Sunah Suh, Arkadiusz Komarzewski, Tim D Smith, and Ilana Segall. Federated\n\nlearning for ranking browser history suggestions. arXiv preprint arXiv:1911.11807, 2019.\n\nCheol-Ho Hong, Ivor Spence, and Dimitrios S Nikolopoulos. Gpu virtualization and scheduling\n\nmethods: A comprehensive survey. ACM Computing Surveys (CSUR), 50(3):1–37, 2017.\n\nSiyu Huang, Xi Li, Zhi-Qi Cheng, Zhongfei Zhang, and Alexander Hauptmann. Gnas: A greedy neural architecture search method for multi-attribute learning. In Proceedings of the 26th ACM international conference on Multimedia, pp. 2049–2057, 2018.\n\nYangsibo Huang, Samyak Gupta, Zhao Song, Kai Li, and Sanjeev Arora. Evaluating gradient inversion attacks and defenses in federated learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nJoel Janai, Fatma G ̈uney, Aseem Behl, Andreas Geiger, et al. Computer vision for autonomous vehicles: Problems, datasets and state of the art. Foundations and Trends R(cid:13) in Computer Graphics and Vision, 12(1–3):1–308, 2020.\n\nMyeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee, Junjie Qian, Wencong Xiao, and Fan Yang. Analysis of Large-Scale Multi-Tenant GPU clusters for DNN training workloads. In 2019 USENIX Annual Technical Conference (USENIX ATC 19), pp. 947–960, Renton, WA, July 2019. USENIX Association. ISBN 978-1-939133-03-8. URL https://www.usenix.org/ conference/atc19/presentation/jeon.\n\nAngela H Jiang, Daniel L-K Wong, Christopher Canel, Lilia Tang, Ishan Misra, Michael Kaminsky, Michael A Kozuch, Padmanabhan Pillai, David G Andersen, and Gregory R Ganger. Mainstream: Dynamic {Stem-Sharing} for {Multi-Tenant} video processing. In 2018 USENIX Annual Technical Conference (USENIX ATC 18), pp. 29–42, 2018.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur ́elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends R(cid:13) in Machine Learning, 14(1–2):1–210, 2021.\n\nZhuoliang Kang, Kristen Grauman, and Fei Sha. Learning with whom to share in multi-task feature\n\nlearning. In ICML, 2011.\n\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020.\n\nAlex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7482–7491, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJakub Koneˇcn ́y, H. Brendan McMahan, Felix X. Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. In NIPS Workshop on Private Multi-Party Machine Learning, 2016. URL https://arxiv.org/abs/ 1610.05492.\n\nAbhishek Kumar and Hal Daum ́e. Learning task grouping and overlap in multi-task learning. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pp. 1723–1730, 2012.\n\nChonLam Lao, Yanfang Le, Kshiteej Mahajan, Yixi Chen, Wenfei Wu, Aditya Akella, and Michael Swift. ATP: In-network aggregation for multi-tenant learning. In 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21), pp. 741–761. USENIX Association, April 2021. ISBN 978-1-939133-21-2. URL https://www.usenix.org/conference/ nsdi21/presentation/lao.\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429–450, 2020.\n\nWenqi Li, Fausto Milletar`ı, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian Baust, Yan Cheng, S ́ebastien Ourselin, M Jorge Cardoso, et al. Privacy-preserving federated brain tumour segmentation. In International Workshop on Machine Learning in Medical Imaging, pp. 133–141. Springer, 2019.\n\nJie Liu, Jiawen Liu, Wan Du, and Dong Li. Performance analysis and characterization of training deep learning models on mobile device. In 2019 IEEE 25th International Conference on Parallel and Distributed Systems (ICPADS), pp. 506–515. IEEE, 2019.\n\nYongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and Rogerio Feris. Fullyadaptive feature sharing in multi-task networks with applications in person attribute classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5334–5343, 2017.\n\nOthmane Marfoq, Giovanni Neglia, Aur ́elien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. Advances in Neural Information Processing Systems, 34, 2021.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 1273–1282. PMLR, 2017.\n\nRalph Mietzner, Frank Leymann, and Mike P Papazoglou. Defining composite configurable saas application packages using sca, variability descriptors and multi-tenancy patterns. In 2008 third international conference on Internet and web applications and services, pp. 156–161. IEEE, 2008.\n\nLorenzo Minto, Moritz Haller, Benjamin Livshits, and Hamed Haddadi. Stronger privacy for federated collaborative filtering with implicit feedback. In Fifteenth ACM Conference on Recommender Systems, pp. 342–350, 2021.\n\nIshan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for In Proceedings of the IEEE conference on computer vision and pattern\n\nmulti-task learning. recognition, pp. 3994–4003, 2016.\n\nVladimir Nekrasov, Thanuja Dharmasiri, Andrew Spek, Tom Drummond, Chunhua Shen, and Ian Reid. Real-time joint semantic segmentation and depth estimation using asymmetric annotations. In 2019 International Conference on Robotics and Automation (ICRA), pp. 7101–7107. IEEE, 2019.\n\nXiaomin Ouyang, Zhiyuan Xie, Jiayu Zhou, Jianwei Huang, and Guoliang Xing. Clusterfl: a similarity-aware federated learning system for human activity recognition. In Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services, pp. 54–66, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\nJason Posner, Lewis Tseng, Moayad Aloqaily, and Yaser Jararweh. Federated learning in vehicular\n\nnetworks: opportunities and solutions. IEEE Network, 35(2):152–159, 2021.\n\nDavid Qiu, Yanzhang He, Qiujia Li, Yu Zhang, Liangliang Cao, and Ian McGraw. Multi-task learning for end-to-end asr word and utterance confidence with deletion prediction. arXiv preprint arXiv:2104.12870, 2021.\n\nSwaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Franc ̧oise Beaufays. Federated learning\n\nfor emoji prediction in a mobile keyboard. arXiv preprint arXiv:1906.04329, 2019.\n\nMicah J Sheller, G Anthony Reina, Brandon Edwards, Jason Martin, and Spyridon Bakas. Multiinstitutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation. In International MICCAI Brainlesion Workshop, pp. 92–104. Springer, 2018.\n\nVirginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task\n\nlearning. Advances in neural information processing systems, 30, 2017.\n\nTrevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in multi-task learning? In International Conference on Machine Learning, pp. 9120–9132. PMLR, 2020.\n\nXimeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. Adashare: Learning what to share for efficient deep multi-task learning. Advances in Neural Information Processing Systems, 33: 8728–8740, 2020.\n\nSebastian Thrun. Is learning the n-th thing any easier than learning the first? Advances in neural\n\ninformation processing systems, 8, 1995.\n\nSimon Vandenhende, Stamatios Georgoulis, Bert De Brabandere, and Luc Van Gool. Branched\n\nmulti-task networks: deciding what layers to share. arXiv preprint arXiv:1904.02920, 2019.\n\nHongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In International Conference on Learning Representations, 2020a. URL https://openreview.net/forum?id=BkluqlSFDS.\n\nJianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481, 2020b.\n\nChengxu Yang, Qipeng Wang, Mengwei Xu, Zhenpeng Chen, Kaigui Bian, Yunxin Liu, and Xuanzhe Liu. Characterizing impacts of heterogeneity in federated learning upon large-scale smartphone data. In Proceedings of the Web Conference 2021, pp. 935–946, 2021.\n\nTimothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Franc ̧oise Beaufays. Applied federated learning: Improving google keyboard query suggestions. arXiv preprint arXiv:1812.02903, 2018.\n\nJianbo Ye\n\nand Arnie\n\nSen.\n\nchanging home? how-does-astro-localize-itself-in-an-ever-changing-home.\n\n2022.\n\nHow does\n\neverURL https://www.amazon.science/blog/\n\nlocalize\n\nitself\n\nastro\n\nan\n\nin\n\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33: 5824–5836, 2020.\n\nyuyang deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated\n\nlearning, 2021. URL https://openreview.net/forum?id=g0a-XYjpQ7r.\n\nAmir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712–3722, 2018.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nHongyi Zhang, Jan Bosch, and Helena Holmstr ̈om Olsson. End-to-end federated learning for autonomous driving vehicles. In 2021 International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE, 2021a.\n\nJie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, and Feijie Wu. Parameterized knowledge transfer for personalized federated learning. Advances in Neural Information Processing Systems, 34, 2021b.\n\nYu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and\n\nData Engineering, 2021.\n\nHanyu Zhao, Zhenhua Han, Zhi Yang, Quanlu Zhang, Fan Yang, Lidong Zhou, Mao Yang, Francis C.M. Lau, Yuqi Wang, Yifan Xiong, and Bin Wang. HiveD: Sharing a GPU clusIn 14th USENIX Symposium on Operating Systems ter for deep learning with guarantees. Design and Implementation (OSDI 20), pp. 515–532. USENIX Association, November 2020. ISBN 978-1-939133-19-9. URL https://www.usenix.org/conference/osdi20/ presentation/zhao-hanyu.\n\nXiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang, and Ying Wu. A modulation module In Proceedings of the European\n\nfor multi-task learning with applications in image retrieval. Conference on Computer Vision (ECCV), pp. 401–416, 2018.\n\nLigeng Zhu, Hongzhou Lin, Yao Lu, Yujun Lin, and Song Han. Delayed gradient averaging: Tolerate the communication latency for federated learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nWeiming Zhuang, Yonggang Wen, Xuesen Zhang, Xin Gan, Daiying Yin, Dongzhan Zhou, Shuai Zhang, and Shuai Yi. Performance optimization of federated person re-identification via benchmark analysis. In Proceedings of the 28th ACM International Conference on Multimedia, pp. 955–963, 2020.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA MULTI-TENANT FL SCENARIOS\n\nWe introduce four multi-tenant federated learning scenarios in Section 3. Figure 6 depicts these four scenarios with variances in two aspects: 1) whether all training activities are the same type of application, e.g., CV applications; 2) whether all clients support all training activities.\n\nFigure 6: Illustration of the four multi-tenant FL scenarios.\n\nOur proposed approach, MuFL, focuses on optimizing the performance on Scenario 1. Optimizing Scenario 1 can potentially empower plenty of real-world applications. For example, autonomous vehicles relate to multiple computer vision (CV) tasks (Janai et al., 2020), including object detection, tracking, and semantic segmentation; smart city surveillance cameras associate with various CV tasks, such as crowd counting, object detection, and person re-identification (Zhuang et al., 2020); voice assistant applications like Apple Siri and Google Assistant need multiple automatic speech recognition (ASR) tasks, including word confidence, word deletion, and utterance confidence (Qiu et al., 2021); household robots like Amazon Astro need to perform multiple CV tasks such as visual odometry tracking, loop-closure detection, and object detection (Ye & Sen, 2022); smartmanufacturing robots need several CV tasks such as object detection, object grasp point detection, and object pose estimation (FLAIROP, 2022).\n\nB EXPERIMENTAL DETAILS\n\nThis section provides more experimental information, including dataset, implementation details, and computation resources used.\n\nDataset We run experiments using Taskonomy dataset (Zamir et al., 2018), which is a large computer vision (CV) dataset of indoor scenes of buildings. To facilitate reproducibility and mitigate computational requirements, we use the tiny split of Taskonomy dataset,4 whose size is around 445GB. We select nine CV applications to form three sets of training activities: sdnkt, erckt, sdnkterca. These nine actvities are also used in (Standley et al., 2020). Figure 8 provides sample images of these nine training activities, as well as the representation of each character.5 In particular, we employ indoor images\n\nFigure 7: The data amount distribution of each training activity over 32 clients.\n\n4Taskonomy dataset is released under MIT license and can be downloaded from their official repository\n\nhttps://github.com/StanfordVL/taskonomy.\n\n5The meaning of each character in sdnkterca are as follows; s: semantic segmentation, d: depth estimation, n: normals, k: keypoint, t: edge texture, e: edge occlusion, r: reshaping, c: principle curvature, a: auto-encoder.\n\n15\n\nExample: Google KeyboardSame Type ofApplicationsClients supportall activitiesNot all clientssupport all activitiesDifferent Typesof ApplicationsClients: CarsType: Computer vision Training activities: e.g., objectdetection, semantic segmentationClients: Cameras Type: Computer vision Training activities: e.g., open spotscounting, duration trackingClients: Smartphones Type: Recommendation, NLP Training activities: e.g., querysuggestion, next-word predictionExample: Autonomous VehicleExample: Surveillance Camerasin Parking LotsExample: BrowsersClients: Browsers Type: Recommendation, Ranking Training activities: e.g., news/adsrecommendation, rank history0102030ClientID040008000120001600020000DataAmountUnder review as a conference paper at ICLR 2023\n\n(a) Input Image\n\n(b) s: Segmentation\n\n(c) d: Depth Estimation\n\n(d) n: Surface Normals\n\n(e) k: Keypoint\n\n(f) t: Edge Texture\n\n(g) e: Edge Occlusion\n\n(h) r: Reshasing\n\n(i) c: Principle Curvature\n\n(j) a: Auto-encoder\n\nFigure 8: Sample images of nine training activities corresponding to the input image.\n\nFigure 9: Sample images of five clients, where each client contains indoor scenes of a building. These indoor images differ in design, layout, objects, and illumination.\n\nof 32 buildings 6 as the total number of clients N = 32; each client contains images of a building to simulate the statistical heterogeneity. On the one hand, clients have different sizes of data. Figure 7 shows the distribution of dataset sizes of an activity of clients. On the other hand, Figure 9 shows sample images of five clients; their indoor scenes vary in design, layout, objects, and illumination.\n\nImplementation Details We implement multi-tenant FL systems in Python using PyTorch (Paszke et al., 2017). We simulate the FL training on a cluster of NVIDIA Tesla V100 GPUs, where each node in the cluster contains 8 GPUs. In each round, each selected client is allocated to a GPU to conduct training; these clients communicate via the NCCL backend. Besides, we employ FedAvg (McMahan et al., 2017) for the server aggregation. By default, we randomly select K = 4 clients to train for E = 1 local epochs in each round and train for R = 100 rounds.\n\nWe reference the implementation of multi-task learning from (Standley et al., 2020)’s official repository 7 for all-in-one training and training of each split after activity splitting. Particularly, the network architecture contains an encoder θs and multiple decoders θαi; one decoder for a training activity αi. Figure 10 illustrates the network architectures of training activities before and after activity consolidation and activity splitting. We use the modified Xception Network (Chollet, 2017) as the encoder for activity sets sdnkt and erckt and half size of the network (half amount of parameters) for activity set sdnkterca. The decoders contain four deconvolution layers and four convolution layers. Each training activity contains a loss function. Specifically, semnatic segmentation s uses Cross Entropy loss; surface normals and depth estimation use rotation loss based on L1 loss; keypoint detection, edge occlusion, edge texture, auto encoder, and principle curvature use L1 loss. We refer\n\n6The name of the buildings are allensville, beechwood, benevolence, coffeen, collierville, corozal, cosmos, darden, forkland, hanson, hiteman, ihlen, klickitat, lakeville, leonardo, lindenwood, markleeville, marstons, mcdade, merom, mifflinburg, muleshoe, newfields, noxapater, onaga, pinesdale, pomaria, ranchester, shelbyville, stockman, tolstoy, and uvalda.\n\n7https://github.com/tstandley/taskgrouping\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Illustration of network architectures of training activities in MuFL. Initially, each activity employs an encoder and a decoder. Activity consolidation consolidates these training activities into an all-in-one activity with multi-task architecture. After activity splitting, MuFL divides the all-in-one activity into multiple training activities, each contains an encoder and one or multiple decoders.\n\nimplementation of loss functions from (Standley et al., 2020) 8. The batch size is B = 64 for sdnkt and erckt and B = 32 for sdnkterca. These are the maximum batch sizes for one GPU without out-of-memory issues. In addition, we use polynomial learning rate decay (1 − r R )0.9 to update learning rate in each round with initial learning rate η = 0.1, where r is the number of trained rounds and R = 100 is the default total training rounds. The optimizer is stochastic gradient descent (SGD), with momentum of 0.9 and weight decay 1e−4.\n\nImplementation of Compared Methods We tune the hyperparameter μ = 0.004 for the proximal term in FedProx (Li et al., 2020). GradNorm (Chen et al., 2018) implementation is adopted from (Standley et al., 2020; Fifty et al., 2021) with default α = 1.5 and TAG (Fifty et al., 2021) implementation is adopted from their official repository 9. Next, we provide the details of how we compute the results of HOA (Standley et al., 2020) and TAG (Fifty et al., 2021).\n\nHOA (Standley et al., 2020) needs to compute test losses for individual activities and pair-wise activity combinations for R = 100 rounds. After that, we use these results to estimate test losses of higher-order combinations following (Standley et al., 2020). We then compute the actual test losses for the optimal activity splits that have the lowest test losses by training them from scratch. For example, for activity set sdnkt, we compute s, d, n, k, t and ten pair-wise activity combinations. Then, we use these results to estimate test losses of higher-order combinations.\n\nTAG (Fifty et al., 2021) first computes all-in-one training for R = 100 rounds to obtain the pair-wise affinities. Then, it uses a network selection algorithm to group these activities. After that, we train each group of activities from scratch for R = 100 rounds to obtain test losses. The best result is reported for overlapping activities. For example, {sd, dn, kt} is the best result of three splits of TAG on activity set sdnkt. Then, each split is trained from scratch to obtain test losses.\n\nComputation Resources Experiments in this work take approximately 27,765 GPU hours of NVIDIA Tesla V100 GPU for training. We conduct three independent runs of experiments for the majority of empirical studies. In each run, activity set sdnkt takes around 2,330 GPU hours, erckt takes around 3,280 GPU hours, and sdnkterca takes around 3,645 GPU hours. These include experiments of compared methods and ablation studies, whereas these do not include the GPU hours for validation and testing. It takes around the same GPU hours as training when we validate the model after each training round.\n\nC ADDITIONAL EXPERIMENTAL EVALUATION\n\nThis section provides more experimental results, including comprehensive results of performance evaluation and additional ablation studies.\n\n8https://github.com/tstandley/taskgrouping/blob/master/taskonomy_losses.\n\npy\n\n9https://github.com/google-research/google-research/tree/master/tag\n\n17\n\n(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:36)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:36)(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:37)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:38)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:37)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:38)(cid:36)(cid:79)(cid:79)(cid:16)(cid:76)(cid:81)(cid:16)(cid:50)(cid:81)(cid:72)(cid:3)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:37)(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:38)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:3)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:3)(cid:36)(cid:10)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:37)(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:38)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:38)(cid:82)(cid:81)(cid:86)(cid:82)(cid:79)(cid:76)(cid:71)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:54)(cid:83)(cid:79)(cid:76)(cid:87)(cid:87)(cid:76)(cid:81)(cid:74)(cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:3)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)(cid:3)(cid:37)(cid:10)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:36)(cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:36)Under review as a conference paper at ICLR 2023\n\nTable 3: Comparison of test loss, energy consumption, and carbon footprint on activity set sdnkt.\n\nMethod Splits Energy (kWh) CO2eq (g)\n\nTotal Loss\n\ns\n\nd\n\nn\n\nk\n\nt\n\n- One by one -\nAll-in-one GradNorm -\n\n8.4 ± 0.1 3.7 ± 0.1 4.1 ± 0.4\n\n2465 ± 39 0.603 ± 0.030 0.086 ± 0.005 0.261 ± 0.023 0.107 ± 0.001 0.107 ± 0.003 0.043 ± 0.002 1086 ± 28 0.677 ± 0.018 0.087 ± 0.002 0.246 ± 0.010 0.136 ± 0.001 0.126 ± 0.019 0.083 ± 0.008 1200 ± 122 0.691 ± 0.013 0.092 ± 0.001 0.251 ± 0.012 0.138 ± 0.003 0.118 ± 0.007 0.093 ± 0.019\n\nHOA TAG MuFL\n\nHOA TAG MuFL\n\nHOA TAG MuFL\n\n2 2\n2\n\n3 3\n3\n\n4 4\n4\n\n31.0 ± 0.5 9.8 ± 0.3 4.9 ± 0.3\n\n9125 ± 140 0.651 ± 0.029 0.091 ± 0.011 0.245 ± 0.002 0.135 ± 0.000 0.107 ± 0.003 0.074 ± 0.023 2876 ± 88 0.624 ± 0.015 0.083 ± 0.004 0.242 ± 0.005 0.134 ± 0.001 0.110 ± 0.007 0.055 ± 0.006 1431 ± 94 0.578 ± 0.015 0.069 ± 0.006 0.231 ± 0.006 0.124 ± 0.002 0.102 ± 0.003 0.052 ± 0.003\n\n31.0 ± 0.5 11.3 ± 0.2 5.4 ± 0.3\n\n9125 ± 140 0.598 ± 0.029 0.083 ± 0.022 0.239 ± 0.007 0.127 ± 0.008 0.107 ± 0.003 0.043 ± 0.002 3313 ± 56 0.613 ± 0.032 0.094 ± 0.005 0.233 ± 0.002 0.122 ± 0.013 0.110 ± 0.008 0.055 ± 0.008 1589 ± 94 0.555 ± 0.015 0.072 ± 0.006 0.222 ± 0.006 0.124 ± 0.002 0.095 ± 0.003 0.042 ± 0.003\n\n31.0 ± 0.5 13.7 ± 0.3 6.7 ± 0.3\n\n9125 ± 140 0.597 ± 0.015 0.094 ± 0.009 0.238 ± 0.002 0.115 ± 0.014 0.107 ± 0.003 0.043 ± 0.002 4016 ± 80 0.603 ± 0.027 0.083 ± 0.005 0.233 ± 0.002 0.122 ± 0.013 0.110 ± 0.008 0.055 ± 0.008 1969 ± 75 0.548 ± 0.001 0.070 ± 0.002 0.230 ± 0.008 0.111 ± 0.000 0.095 ± 0.007 0.042 ± 0.001\n\nTable 4: Comparison of test loss, energy consumption, and carbon footprint on activity set erckt.\n\nMethod Splits Energy (kWh) CO2eq (g)\n\nTotal Loss\n\ne\n\nr\n\nc\n\nk\n\nt\n\n- One by one All-in-one -\nGradNorm -\n\n11.1 ± 2.2 5.0 ± 0.3 5.0 ± 0.2\n\n3277 ± 660 1.055 ± 0.034 0.148 ± 0.000 0.371 ± 0.029 0.386 ± 0.006 0.107 ± 0.003 0.043 ± 0.002 1478 ± 84 1.130 ± 0.022 0.146 ± 0.001 0.379 ± 0.019 0.393 ± 0.002 0.110 ± 0.003 0.079 ± 0.013 1462 ± 70 1.154 ± 0.055 0.147 ± 0.002 0.381 ± 0.015 0.394 ± 0.001 0.149 ± 0.062 0.082 ± 0.005\n\nHOA TAG MuFL\n\nHOA TAG MuFL\n\nHOA TAG MuFL\n\n2 2\n2\n\n3 3\n3\n\n4 4\n4\n\n38.3 ± 0.3 14.0 ± 0.9 6.7 ± 0.2\n\n11265 ± 86 1.082 ± 0.032 0.149 ± 0.003 0.365 ± 0.025 0.394 ± 0.002 0.109 ± 0.002 0.064 ± 0.022 4119 ± 279 1.095 ± 0.033 0.147 ± 0.002 0.379 ± 0.013 0.393 ± 0.000 0.108 ± 0.005 0.068 ± 0.015 1957 ± 53 1.039 ± 0.024 0.143 ± 0.001 0.343 ± 0.014 0.393 ± 0.001 0.104 ± 0.006 0.056 ± 0.007\n\n38.3 ± 0.2 14.4 ± 0.6 7.2 ± 0.2\n\n11265 ± 53 1.062 ± 0.024 0.149 ± 0.001 0.365 ± 0.014 0.394 ± 0.001 0.109 ± 0.006 0.046 ± 0.007 4242 ± 170 1.091 ± 0.034 0.147 ± 0.002 0.388 ± 0.014 0.396 ± 0.002 0.109 ± 0.009 0.050 ± 0.011 2108 ± 50 1.015 ± 0.018 0.143 ± 0.000 0.336 ± 0.005 0.383 ± 0.001 0.102 ± 0.008 0.052 ± 0.009\n\n38.3 ± 0.3 17.4 ± 0.5 7.6 ± 0.0\n\n11265 ± 86 1.053 ± 0.034 0.148 ± 0.002 0.369 ± 0.028 0.386 ± 0.006 0.105 ± 0.001 0.045 ± 0.003 5114 ± 159 1.087 ± 0.028 0.147 ± 0.002 0.384 ± 0.011 0.396 ± 0.002 0.109 ± 0.009 0.050 ± 0.011 2229 ± 14 1.002 ± 0.014 0.143 ± 0.000 0.336 ± 0.005 0.383 ± 0.001 0.094 ± 0.009 0.046 ± 0.004\n\nC.1 PERFORMANCE EVALUATION\n\nTable 3 and 5 provide comprehensive comparison of different methods on test loss and energy consumption on activity sets sdnkt and sdnkterca, respectively. They complement the results in Figure 2. Besides, Table 4 and Figure 11 compares these methods on activity set erckt. The results on erckt is similar to results on the other activity sets; our method achieves the best performance with around 40% less energy consumption than the one-by-one method and with slightly more energy consumption than all-in-one methods.\n\nFigure 11: Compare test loss and energy consumption on activity set erckt.\n\nAdditionally, Table 3, 4 and 5 also provide carbon footprints (CO2eq) of different methods. The carbon footprints are estimated using Carbontracker (Anthony et al., 2020).10 Our method reduces around 40% on carbon footprints on these three activity sets compared with one-by-one training; it reduces 1526gCO2eq or equivalent to traveling 12.68km by car on sdnkterca. The reduction is even more significant when compared with TAG and HOA. Although we run experiments using Tesla V100 GPU, the relative results of energy and carbon footprint among different methods should be representative of the scenarios of edge devices.\n\nC.2 ADDITIONAL ANALYSIS AND ABLATION STUDIES\n\nThis section presents additional analysis of MuFL and provides additional ablation studies.\n\n10Carbon intensity of a training varies over geographical regions according to (Anthony et al., 2020). We use the national level (the United Kingdom as the default setting of the tool) of carbon intensity for a fair comparison across different methods. These carbon footprints serve as a proxy for evaluation of the actual carbon emissions.\n\n18\n\n468101214161820222426283032343638Energy(kWh)1.001.021.041.061.081.101.121.141.16TestLossOnebyoneAll-in-oneGradNormFedProxTAGHOAOursTAGHOAOursTAGHOAOurs2Splits3Splits4SplitsUnder review as a conference paper at ICLR 2023\n\nTable 5: Comparison of test loss, energy consumption, and carbon footprint on sdnkterca.\n\nMethod Splits Energy CO2eq (g) Total Loss\n\ns\n\nd\n\nn\n\nk\n\nt\n\ne\n\nr\n\nc\n\na\n\n- One by one All-in-one -\nGradNorm -\n\n11.9±0.5 3512±1511.46±0.0110.08±0.0090.24±0.0140.10±0.0010.10±0.0020.04±0.003 0.15±0.001 0.35±0.0110.38±0.0020.02±0.000 4.9±0.2 1435±60 1.49±0.0250.09±0.0020.23±0.0090.13±0.0020.10±0.0020.07±0.005 0.14±0.001 0.33±0.0110.39±0.0010.02±0.001 5.3±1.3 1561±3771.50±0.0490.08±0.0040.24±0.0140.13±0.0030.10±0.0030.07±0.011 0.14±0.001 0.34±0.0180.39±0.0010.02±0.001\n\nTAG MuFL\n\nTAG MuFL\n\nTAG MuFL\n\n2 14.7±0.8 4317±2291.49±0.0250.09±0.0020.23±0.0080.13±0.0020.10±0.0020.07±0.005 0.14±0.001 0.33±0.0110.39±0.0010.02±0.001 6.0±0.1 1986±1081.45±0.0210.08±0.0030.22±0.0080.12±0.0010.10±0.0010.06±0.004 0.14±0.000 0.32±0.0110.39±0.0010.02±0.001 2\n\n3 16.5±2.6 4854±7511.44±0.0140.09±0.0060.23±0.0090.12±0.0010.10±0.0020.03±0.004 0.14±0.000 0.33±0.0090.39±0.0010.02±0.000 6.6±0.4 1955±1041.39±0.0300.07±0.0050.22±0.0080.12±0.0020.08±0.0020.05±0.003 0.14±0.001 0.32±0.0110.38±0.0010.02±0.000 3\n\n4 15.8±2.4 4639±7171.44±0.0070.07±0.0030.24±0.0020.11±0.0010.10±0.0020.03±0.004 0.14±0.000 0.35±0.0030.39±0.0010.02±0.000 7.5±0.3 2201±94 1.40±0.0270.06±0.0040.22±0.0080.12±0.0030.08±0.0020.05±0.001 0.14±0.001 0.32±0.0110.39±0.0010.02±0.001 4\n\nMuFL\n\n5\n\n8.3±0.4 2439±1051.40±0.0280.06±0.0040.22±0.0080.12±0.0030.08±0.0020.05±0.000 0.14±0.002 0.32±0.0110.39±0.0010.02±0.001\n\n(a) Activity set sdnkt\n\n(b) Activity set erckt\n\n(c) Activity set sdnkterca\n\nFigure 12: Changes of validation loss over the course of training on activity sets: (a) sdnkt, (b) erckt, and (c) sdnkterca. Validation loss converges as training proceeds.\n\nTable 6: Activity splitting results of TAG (Fifty et al., 2021) and MuFL on activity sets sdnkt, erckt, and sdnkterca. Activities of each split is separated by a comma.\n\nMethod\n\nActivity Set\n\nTwo Splits\n\nThree Splits\n\nFour Splits\n\nFive Splits\n\nTAG MuFL\n\nTAG MuFL\n\nTAG MuFL\n\nsdnkt sdnkt\n\nerckt erckt\n\nsdn,kt sdn,kt\n\ner,rckt er,ckt\n\nsd,dn,kt sdn,k,t\n\ner,kt,rc er,c,kt\n\nsd,sdn,dn,kt sd,n,k,t\n\ner,kt,rc,rt er,c,k,t\n\n- s,d,n,k,t\n\n- e,r,c,k,t\n\nsdnkterca sdnkterca\n\nsdnkterca,dr snkteac,dr\n\nsdnerc,dr,kta snec,dr,kta\n\nsc,dr,ne,kta sn,dr,ka,etc\n\n- sn,dr,ka,e,tc\n\nChanges of Vadiation Loss Figure 12 presents validation losses over the course of all-in-one training of three training activity sets sdnkt, erckt, and sdnkterca. It shows that validation losses converge as training proceeds.\n\nSplitting Results of Various Methods We provide results of activity splitting of TAG (Fifty et al., 2021) and MuFL in Table 6. For hierarchical splitting, they further split into three splits from the results of two splits. In particular, the results of hierarchical splitting of erckt and sdnkterca are the same as their three splits. The hierarchical splitting result of sdnkt is from {sdn,kt} to {sd,n,kt} as the hierarchical splitting further divides the split with more training activities (sdn).\n\nBesides, Table 7 presents the splitting results of the optimal and worst splits. They are not identical due to variances in multiple runs of experiments. We report the mean and standard deviation of test losses of the optimal splits and the worst splits in Table 1. The large variances of the optimal and worst splits suggest the instability of splitting by measuring the performances of training from scratch in the FL settings and show the advantage of our methods in obtaining stable splits.\n\nDataset Size and Performance The dataset size of activity set sdnkt is around 315GB in our experiments, compared to 2.4TB of dataset used in experiments of TAG (Fifty et al., 2021). The test loss of ours (0.512 in Table 8), however, is better than the optimal one in TAG (Fifty et al., 2021) (0.5246). This back-of-the-envelope comparison indicates the potential to extend our approaches to multi-task learning. Besides, it could also suggest that our data size is sufficient for evaluation.\n\n19\n\n020406080100TrainingRound0.641.422.202.983.76Validationloss020406080100TrainingRound1.081.481.882.282.68Validationloss020406080100TrainingRound1.442.543.644.745.84ValidationlossUnder review as a conference paper at ICLR 2023\n\nTable 7: Results of the optimal and worst splits in three runs of experiments. They are not identical due to variances in three runs of experiments.\n\nActivity Set\n\nSplits\n\nOptimal Splits\n\nWorst Splits\n\nsdnkt\n\nerckt\n\n2 3\n\n2 3\n\ndk,snt t,sn,dk\n\nr,eckt r,ec,kt\n\nsn,dkt k,t,sdn\n\nt,erck r,t,eck\n\nnt,sdk d,sn,kt\n\net,rck r,ec,kt\n\nst,dnk d,st,nk\n\nrk,ect c,e,rk\n\nst,dnk d,st,nk\n\nek,rct e,k,rct\n\nst,dnk s,dt,nk\n\ne,rckt e,rt,ck\n\n(a) Affinities to activity s\n\n(b) Affinities to activity d\n\n(c) Affinities to activity n\n\n(d) Affinities to activity k\n\n(e) Affinities to activity t\n\n(f) Affinities to activity e\n\n(g) Affinities to activity r\n\n(h) Affinities to activity c\n\n(i) Affinities to activity a\n\nFigure 13: Changes of affinity scores of one activity to the other over the course of training on activity set sdnkterca. The trends of affinities emerge at the early stage of training.\n\nImpact of Affinity Computation Frequency f The frequency of computing affinities in Equation 2 determines the amount of extra needed computation. We use f = 5 and compute affinities for the first ten rounds for all experiments because the trend of affinities emerges in the early stage of training in Figure 13. It would increase the computation of all-in-one training by around 2%, which is already factored into the energy consumption computation in previous experiments. The results in Table 3, 4, and 5 show that MuFL is effective with this setting and the amount of computation is acceptable.\n\nImpact of Local Epoch Figure 14a show the impact of local epoch E on activity sets sdnkt, erckt, and sdnkterca. They complement results of activity set sdnkt in Figure 5a. Larger E could lead to better performance with fixed R = 100. It is especially effective when increasing E = 1 to E = 2, but further increasing E could degrade the performance. It indicates that simply increasing computation has limited capability to improve performance.\n\nImpact of The Number of Selected Clients Figure 14b compares the performance of different numbers of selected clients K = {2, 4, 6, 8, 16} on three activity sets sdnkt, erckt, and sdnkterca. It complements results in Figure 5b. The results on three activity sets are similar; increasing K reduces losses, but the marginal benefit decreases as K increases.\n\nThe majority of experiments in this study are conducted with K = 4. We next analyze the impact of K in MuFL with results of two splits on activity set sdnkt in Table 8. The results indicate that MuFL is still effective with K = 8, which outperforms K = 4 and all-in-one training.\n\nStandalone Training Standalone training refers to training using data of each client independently. Figure 15a shows the test loss distribution of thirty-two clients used in experiments. The client ID corresponds to the dataset size distribution in Figure 7. These results suggest that larger data sizes of\n\n20\n\n020406080100TrainingRound0.000.040.080.120.16Affinitydnkertac020406080100TrainingRound0.000.641.281.922.56Affinitysnkertac020406080100TrainingRound0.000.150.300.450.60Affinitysdkertac020406080100TrainingRound0.000.030.060.090.12Affinitysdnertac020406080100TrainingRound0.000.040.080.120.16Affinitysdnkerac020406080100TrainingRound0.000.040.080.120.16Affinitysdnkrtac020406080100TrainingRound0.000.340.681.021.36Affinitysdnketac020406080100TrainingRound0.000.020.040.06Affinitysdnkerta020406080100TrainingRound0.000.050.100.150.200.25AffinitysdnkertcUnder review as a conference paper at ICLR 2023\n\n(a) Impact of E\n\n(b) Impact of K\n\nFigure 14: Analysis of the impact of local epoch E and impact of the number of selected clients K. Larger E (with fixed R = 100) and K requires higher computation. They could reduce losses, but the marginal benefit decreases as computation increases.\n\nTable 8: Comparison of test loss using different numbers of selected clients K. MuFL achieves even better performance on K = 8.\n\nK Total Loss\n\ns\n\nd\n\nn\n\nk\n\nt\n\nAll-in-one All-in-one MuFL (two splits) MuFL (two splits)\n\n4 8\n4 8\n\n0.677 0.618 0.578 0.512\n\n0.087 0.076 0.069 0.060\n\n0.246 0.227 0.231 0.202\n\n0.136 0.130 0.124 0.117\n\n0.126 0.109 0.102 0.083\n\n0.083 0.077 0.052 0.048\n\nMethods Standalone All-in-one MuFL\n\nTest Loss 1.842 ± 0.248 0.677 ± 0.018 0.548 ± 0.001\n\n(a) Test loss distribution of standalone training\n\n(b) Test loss comparison\n\nFigure 15: Performance (test loss) of standalone training that conducts training using data in each client independently: (a) shows the test loss distribution of these thirty-two clients. (b) compares test losses of standalone training and FL methods. We run the experiments on activity set sdnkt.\n\nclients may not lead to higher performance. Figure 15b compares test losses of standalone training and federated learning methods. Either all-in-one or our MuFL greatly outperforms standalone training. It suggests the significance of federated learning when data are not sharable among clients.\n\n21\n\nsdnktercktsdnkterca0.40.81.21.6TestLossE=1E=2E=5E=10sdnktercktsdnkterca0.40.81.21.6TestLossK=2K=4K=6K=8K=160102030ClientID1.41.82.22.6TestLossUnder review as a conference paper at ICLR 2023\n\nD ALGORITHM\n\nAlgorithm 1 Our Proposed Smart Multi-tenant FL System (MuFL)\n\n1: Input: training activities A = {α1, α2, . . . , αn}, a set of available clients C, number of selected clients K, local epoch E, aggregation weight of client k pk, total training rounds R, all-in-one training rounds R0, the number of splits x, frequency of computing affinities f , batch size B\n\n2: Output: models W = {ω1, ω2, . . . , ωn} 3: 4: ServerExecution: 5: Consolidate A into α0 with a multi-task model ν0 = {θs} ∪ {θαi|αi ∈ A} (cid:46) Act. consolidation 6: Initialize ν0, i.e. initialize ωi = {θs} ∪ {θαi} for i ∈ N = {1, 2, . . . , n} 7: for each round r = 0, 1, ..., R0 − 1 do 8: 9: 10: 11: 12:\n\nCr ← (Randomly select K clients from C) for client k ∈ Cr in parallel do\n\nend for νr+1 ← (cid:80)\n\n← Client(νr, A, f )\n\nνr,k, ˆS r,k\n\nαi→αj\n\npkνr,k\n\n13:\n\nˆS r\n\nαi→αj\n\nk∈Cr ← 1 K\n\n(cid:80)\n\nk∈Cr\n\nˆS r,k\n\nαi→αj\n\n14: end for 15: Compute the values of S r (cid:46) Compute affinity scores 16: Compute a disjoint partition set I of activities A for x splits {Aj|j ∈ I} that maximizes S r αi (cid:46) Activity splitting (cid:46) Schedule to train sequentially\n\nαi→αi, ∀αi ∈ A, using Equation 3\n\nαi→αj , ∀αi ∈ A and ∀αj ∈ A\n\nInitialize νj = {θj for each round r = 0, 1, ..., R − R0 − 1 do\n\ns} ∪ {θαi|αi ∈ Aj} with parameters of νR0\n\nusing affinity scores S r 17: for each element j ∈ I do 18: 19: 20: 21:\n\nCr ← (Random select K from C) for client k ∈ Cr in parallel do , ←Client(νr j , Aj, 0)\n\nj\n\nνr,k end for j ← (cid:80) νr+1\n\nk∈Cr\n\npkνr,k\n\nj\n\n22: 23: 24:\n\nend for\n\nf (cid:99) if f (cid:54)= 0 else 0\n\n25: 26: end for 27: Reconstruct W = {ω1, ω2, . . . , ωn} from {νj|j ∈ I} by matching training activities 28: Return W 29: 30: Client (ν, A, f ): 31: T = (cid:98) B 32: for local epoch e = 1, ..., E do 33: 34: 35: 36: 37: end for 38: ˆSαi→αj = 1\n\nUpdate model parameters ν with respect to training activities A for each time-step t = 1, 2, ..., T (every f batches) do ∀αi ∈ A and ∀αj ∈ A, compute affinities of S t\n\nαi→αj , ∀αi ∈ A and ∀αj ∈ A\n\nαi→αj using Equation 2\n\nend for\n\nT (cid:80)\n\nE (cid:80)\n\nS t\n\nET\n\n39: Return ν, ˆSαi→αj\n\ne=1\n\nt=1\n\n22",
    "reference": "# Summary Of The Paper\n\nThe paper presents a method to train multiple simultaneous activities on decentralized edge devices with budget constraints. Most prevailing approaches aim to do this by training tasks one at a time. However, the paper proposes a solution of splitting the tasks into groups and learning jointly. This enables sharing of information as well as an improved efficiency in terms of energy consumption.\n\n# Strength And Weaknesses\n\n**Strengths**\n* The paper proposes a new domain/problem which could be interesting to AI researchers.\n* The paper has ample experiments that there are no issues with pareto-optimality, etc. as usually faced in meta-learning and multi-task learning problems. \n\n**Weaknesses**\n* I believe the implementation could be discussed in more detail. The reader is left wondering what the loss was. How come there has been no non-pareto-optimal solutions when training with different splits? \n* Although the problem is interesting, I am afraid the limitation of the approach is very closely tied to privacy. For example, suppose a device has training activities related to two different companies/entities. It might not be possible to force the two networks to be same while training, or even train with the same data due to data leakage, privacy concerns, etc. This highly weakens the potential impact of the work. If I have misunderstood this part, I hope the authors could correct me here.\n* Although the authors talk about 4 different scenarios in Section 3.2, I believe all experiments could be classified under scenario 1, and the approach has not been shown to be applicable to all 4 scenarios.\n* In section 5.5, the authors show that increasing the number of epochs seems to harm the overall test loss. I would assume this is a classical scenario of overfitting, but I hope the authors could explain this. Especially since the exact formulation of the loss used for training is not detailed, this is hard to guess.\n* This is more of a question: In Table 3, why is the loss of one-by-one higher than the loss of all-in-one for the task 'd'? This intuitively should not be the case since the all-in-one will simplicity come with more noise.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**\n\nThe paper is well-written and reader-friendly. The tables do look a bit off, but that could be easily fixed, and the paper is readable. \n\n**Reproducibility**\n\nI believe the work lacks significantly in reproducibility. Although the basic hyperparameters such as number of splits, local epochs have been provided. The paper does not go into the network structure used, the loss functions, the learning rate and other crucial details that are required to replicate the work.\n\n**Novelty**\nThe authors try to offer an approach to train multiple activities on a resource-contrained device.\n\n# Summary Of The Review\n\nAlthough the results are interesting, it is difficult to truly understand the paper without the complete training regime. The lack of the loss function used is a deterrent, in my view. Besides that, I feel the approach is limited as it can only be used in Scenario 1 (as per Section 3.2).\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLEARNING FOR EDGE-WEIGHTED ONLINE BIPARTITE MATCHING WITH ROBUSTNESS GUARANTEES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nMany real-world problems, such as online ad display, can be formulated as online bipartite matching. The crucial challenge lies in the nature of sequentially-revealed online item information, based on which we make irreversible matching decisions at each step. While numerous expert online algorithms have been proposed with bounded worst-case competitive ratios, they may not offer satisfactory performance in average cases. On the other hand, reinforcement learning (RL) has been applied to improve the average performance, but they lack robustness and can perform arbitrarily badly. In this paper, we propose a novel RL-based approach to edgeweighted online bipartite matching with robustness guarantees (LOMAR), achieving both good average-case and good worst-case performance. The key novelty of LOMAR is a new online switching operation which, based on a judiciously-designed condition to hedge against future uncertainties, decides whether to follow the expert’s decision or the RL decision for each online item arrival. We prove that for any ρ ∈ [0, 1], LOMAR is ρ-competitive against any given expert online algorithm. To improve the average performance, we train the RL policy by explicitly considering the online switching operation. Finally, we run empirical experiments to demonstrate the advantages of LOMAR compared to existing baselines.\n\n1\n\nINTRODUCTION\n\nOnline bipartite matching is a classic online problem of practical importance (Mehta, 2013; Kim & Moon, 2020; Fahrbach et al., 2020; Antoniadis et al., 2020b; Huang & Shu, 2021; Gupta & Roughgarden, 2020). In a nutshell, online bipartite matching assigns online items to offline items in two separate sets: when an online item arrives, we need to match it to an offline item given applicable constraints (e.g., capacity constraint), with the goal of maximizing the total rewards collected (Mehta, 2013). For example, numerous applications, including scheduling tasks to servers, displaying advertisements to online users, recommending articles/movies/products, among many others, can all be modeled as online bipartite matching or its variants.\n\nThe practical importance, along with substantial algorithmic challenges, of online bipartite matching has received extensive attention in the last few decades (Karp et al., 1990; Fahrbach et al., 2020). Concretely, many algorithms have been proposed and studied for various settings of online bipartite matching, ranging from simple yet effective greedy algorithms to sophisticated ranking-based algorithms (Karp et al., 1990; Kim & Moon, 2020; Fahrbach et al., 2020; Aggarwal et al., 2011; Devanur et al., 2013). These expert algorithms typically have robustness guarantees in terms of the competitive ratio — the ratio of the total reward obtained by an online algorithm to the reward of another baseline algorithm (commonly the optimal offline algorithm) — even under adversarial settings given arbitrarily bad problem inputs (Karp et al., 1990; Huang & Shu, 2021). In some settings, even the optimal competitive ratio for adversarial inputs has been derived (readers are referred to (Mehta, 2013) for an excellent tutorial). The abundance of competitive online algorithms has clearly demonstrated the importance of performance robustness in terms of the competitive ratio, especially in safety-sensitive applications such as matching mission-critical items or under contractual obligations (Fahrbach et al., 2020). Nonetheless, as commonly known in the literature, the necessity of conservativeness to address the worst-case adversarial input means that the average performance is typically not optimal (see, e.g., (Christianson et al., 2022; Zeynali et al., 2021) for discussions in other general online problems).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nMore recently, online optimizers based on reinforcement learning (RL) (Chen et al., 2022; Georgiev & Lió, 2020; Wang et al., 2019; Alomrani et al., 2021; Du et al., 2019; Zuzic et al., 2020) have been proposed in the context of online bipartite matching as well as other online problems. Specifically, by exploiting statistical information of problem inputs, RL models are trained offline and then applied online to produce decisions given unseen problem inputs. These RL-based optimizers can often achieve high average rewards in many typical cases. Nonetheless, they may not have any performance robustness guarantees in terms of the competitive ratio. In fact, a crucial pain point is that the worst-case performance of many RL-based optimizers can be arbitrarily bad, due to, e.g., testing distribution shifts, inevitable model generalization errors, finite samples, and/or even adversarial inputs. Consequently, the lack of robustness guarantees has become a key roadblock for wide deployment of RL-based optimizers in real-world applications.\n\nIn this paper, we focus on an important and novel objective — achieving both good average performance and guaranteed worst-case robustness — for edge-weighted online bipartite matching (Fahrbach et al., 2020; Kim & Moon, 2020). More specifically, our algorithm, called LOMAR (Learning-based approach to edge-weighted Online bipartite MAtching with Robustness guarantees), integrates an expert algorithm with RL. The key novelty of LOMAR lies in a carefully-designed online switching step that dynamically switches between the RL decision and the expert decision online, as well as a switching-aware training algorithm. For both no-free-disposal and free-disposal settings, we design novel switching conditions as to when the RL decisions can be safely followed while still guaranteeing robustness of being ρ-competitive against any given expert online algorithms for any ρ ∈ [0, 1]. Furthermore, if the expert itself has a competitive ratio of λ ≤ 1 against the optimal offline algorithm (OPT), then it will naturally translate into LOMAR being ρλ-competitive against OPT. To improve the average performance of LOMAR, we train the RL policy in LOMAR by explicitly taking into account the introduced switching operation. Importantly, to avoid the “no supervision” trap during the initial RL policy training, we propose to approximate the switching operation probabilistically. Finally, we offer empirical experiments to demonstrate that LOMAR can improve the average cost (compared to existing expert algorithms) as well as lower the competitive ratio (compared to pure RL-based optimizers).\n\n2 RELATED WORKS\n\nOnline bipartite matching has been traditionally approached by expert algorithms (Mehta, 2013; Karande et al., 2011; Huang et al., 2019; Devanur et al., 2013). A simple but widely-used algorithm is the (deterministic) greedy algorithm (Mehta, 2013), achieving reasonably-good competitive ratios and empirical performance (Alomrani et al., 2021). Randomized algorithms have also been proposed to improve the competitive ratio (Ting & Xiang, 2014; Aggarwal et al., 2011). In addition, competitive algorithms based on the primal-dual framework have also been proposed (Mehta, 2013; Buchbinder et al., 2009). More recently, multi-phase information and predictions have been leveraged to exploit stochasticity within each problem instance and improve the algorithm performance (Kesselheim et al., 2013). For example, (Korula & Pál, 2009) designs a secretary matching algorithm based on a threshold obtained using the information of phase one, and exploits the threshold for matching in phase two. Note that stochastic settingsconsidered by expert algorithms (Mehta, 2013; Karande et al., 2011) mean that the arrival orders and/or rewards of different online items within each problem instance are stochastic. By contrast, as shown in equation 2, we focus on an unknown distribution of problem instances whereas the inputs within each instance can still be arbitrary.\n\nAnother line of algorithms utilize RL to improve the average performance (Wang et al., 2019; Georgiev & Lió, 2020; Chen et al., 2022; Alomrani et al., 2021). Even though heuristic methods (such as using adversarial training samples (Zuzic et al., 2020; Du et al., 2022)) are used to empirically improve the robustness, they do not provide any theoretically-proved robustness guarantees.\n\nML-augmented algorithms have been recently considered for various problems (Rutten et al., 2022; Christianson et al., 2022; Chł ̨edowski et al., 2021; Lykouris & Vassilvitskii, 2021; Gupta & Roughgarden, 2017). By viewing the ML prediction as blackbox advice, these algorithms strive to provide good competitive ratios when the ML predictions are nearly perfect, and also bounded competitive ratios when ML predictions are bad. But, they still focus on the worst case without addressing the average performance or how the ML model is trained. By contrast, the RL model in LOMAR is trained by taking into account the switching operation and performs inference based on the actual state\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n(rather than its own independently-maintained state as a blackbox). Assuming a given downstream algorithm, (Wang et al., 2021; Liu & Grigas, 2021; Wilder et al., 2019; Elmachtoub & Grigas, 2017; Du et al., 2021; Anand et al., 2021) focus on learning the ML model to better serve the end goal in completely different (sometimes, offline optimization) problems.\n\nLOMAR is relevant to conservative bandits/RL (Wu et al., 2016; Kazerouni et al., 2017; Yang et al., 2022; Garcelon et al., 2020). With unknown reward functions (as well as transition models if applicable), conservative bandits/RL leverages an existing policy to safeguard the exploration process. But, they only consider the cumulative reward without addressing future uncertainties when deciding exploration vs. rolling back to an existing policy. Thus, as shown in Section 4, this cannot guarantee robustness in our problem. Also, constrained policy optimization (Yang et al., 2020; Kumar et al., 2020; Schulman et al., 2015; Achiam et al., 2017; Thomas et al., 2021; Berkenkamp et al., 2017) focuses on average (cost) constraints in the long run, whereas LOMAR achieves stronger robustness (relative to an expert algorithm) for each episode with even adversarial inputs.\n\n3 PROBLEM FORMULATION\n\nWe focus on edge-weighted online bipartite matching, which includes un-weighted and vertexweighted matching as special cases (Fahrbach et al., 2020; Kim & Moon, 2020). In the following, we also drop “edge-weighted” if applicable when referring to our problem.\n\nThe goal of the agent is to match items (a.k.a. vertices) between two sets U and V to gain as high total rewards as possible. Suppose that U is fixed and contains offline items u ∈ U, and that the online items v ∈ V arrive sequentially: in each time slot, an online item v ∈ V arrives and the weight/reward information {wuv | wu,min ≤ wuv ≤ wu,max, u ∈ U} is revealed, where wuv represents the reward when the online item v is matched to each offline u ∈ U. We denote one problem instance by G = {U, V, W}, where W = {wuv | u ∈ U, v ∈ V}. We denote xuv ∈ {0, 1} as the matching decision indicating whether u is matched to v. Also, any offline item u ∈ U can be matched up to cu times, where cu is essentially the capacity for offline item u known to the agent. The objective is to maximize the total collected reward (cid:80) v∈V,u∈U xuvwuv. With a slight abuse of notations, we denote xv ∈ U as the index of item in U that is matched to item v ∈ V. The set of online items matched to u ∈ U is denoted as Vu = {v ∈ V | xuv = 1}.\n\nThe edge-weighted online bipartite matching problem has been mostly studied under two different settings: no free disposal and with free disposal (Mehta, 2013). In the no-free-disposal case, each offline item u ∈ U can only be matched strictly up to cu times; in the free-disposal case, each offline item u ∈ U can be matched more than cu times, but only the top cu rewards are counted when more than cu online items are matched to u. Compared to the free-disposal case, the no-free-disposal case is significantly more challenging with the optimal competitive ratio being 0 in the strong adversarial setting unless additional assumptions are made (e.g., wu,min > 0 for each u ∈ U (Kim & Moon, 2020) and/or random-order of online arrivals) (Fahrbach et al., 2020; Mehta, 2013). The free-disposal setting not only makes the problem more tractable, but also is practically motivated by the display ad application where the advertisers (i.e., offline items u ∈ U) will not be unhappy if they receive more impressions (i.e., online items v ∈ V) than their budgets cu, even though only the top cu items count.\n\nLOMAR can handle both no-free-disposal and free-disposal settings. For better presentation of our key novelty and page limits, we focus on the no-free-disposal setting in the body of the paper, while deferring the free-disposal setting to Appendix B. Specifically, the offline problem with no free disposal can be expressed as:\n\nmax\n\n(cid:88)\n\nxuvwuv, s.t.,\n\nxuv∈{0,1},u∈U ,v∈V\n\n(cid:88)\n\nv∈V\n\nxuv ≤ cu, and\n\n(cid:88)\n\nu∈U\n\nxuv ≤ 1, ∀u ∈ U, v ∈ V,\n\n(1)\n\nwhere the constraints specify the offline item capacity limit and each online item v ∈ V can only be matched up to one offline item u ∈ U.\n\nGiven a problem instance G and an online algorithm α, we use f α collected for offline item u ∈ U, and Rα(G) = (cid:80) We will also drop the superscript α for notational convenience wherever applicable.\n\nu (G) to denote the total reward u (G) to denote the total collected reward.\n\nu∈U f α\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nObjective. Solving the problem in equation 1 is very challenging in the online case, where the agent has to make irreversible decisions without knowing the future online item arrivals. Next, we first define the following generalized competitiveness as a metric of robustness.\n\nDefinition 1 (Competitiveness). An online bipartite matching algorithm α is said to be ρ−competitive with ρ ≥ 0 against the algorithm π if for any problem instance G, its total collected reward Rα(G) satisfies Rα(G) ≥ ρRπ(G) − B, where B ≥ 0 is a constant independent of the problem input, and Rπ is the total reward of the algorithm π.\n\nCompetitiveness against a given (online) algorithm π is common in the literature (Christianson et al., 2022): the greater ρ ≥ 0, the better robustness of the online algorithm, although the average rewards can be worse. Additionally, the constant B ≥ 0 relaxes the strict competitive ratio by allowing an additive regret (Antoniadis et al., 2020a). When B = 0, the competitive ratio becomes the strict one.\n\nIn this paper, we focus on a setting where the problem instance G = {U, V, W} follows an unknown distribution, whereas both the rewards W and online arrival order within each instance G can be adversarial. We consider both average performance and worst-case robustness guarantees as formalized below:\n\nmax EG [Rα(G)] ,\n\ns.t. Rα(G) ≥ ρRπ(G) − B, ∀G,\n\n(2)\n\nwhere the expectation EG [Rα(G)] is over the randomness G = {U, V, W}.\n\nNote carefully that some manually-designed algorithms focus on a stochastic setting where the arrival order is random and/or the rewards {wuv | wu,min ≤ wuv ≤ wu,max, u ∈ U} of each online item is independently and identically distributed (i.i.d.) within each problem instance G Mehta (2013). By contrast, our settings are significantly different — we only assume an unknown distribution for the entire problem instance G = {U, V, W} while both the rewards W and online arrival order within each instance G can be adversarial in our problem.\n\n4 ONLINE SWITCHING FOR ROBUSTNESS GUARANTEES\n\nWe present LOMAR, which includes an online switching operation to dynamically decide to follow the ML decision or the expert decision, to achieve robustness guarantees with respect to the expert.\n\n4.1 ONLINE SWITCHING\n\nWhile switching is common in online algorithms, “how to switch”is highly non-trivial and a key merit for algorithm designs Antoniadis et al. (2020a); Christianson et al. (2022); Rutten et al. (2022). To guarantee robustness (i.e., ρ-competitive against a given expert for any ρ ∈ [0, 1]), we propose a novel online algorithm (Algorithm 1). In the algorithm, we independently run an expert online algorithm π — the cumulative reward and item matching decisions are all maintained virtually for the expert, but not used as the actual decisions. Meanwhile, instead of being independently executed to provide blackbox advice based on its own virtual state (like in the prior ML-augmented online algorithm (Christianson et al., 2022)), the RL model in LOMAR makes online decisions based on the actual state at each step.\n\nThe most crucial step for safeguarding RL decisions is online switching: Lines 13–19 in Algorithm 1. The key idea for this step is to switch between the expert decision xπ v and the RL decision ̃xv in order to ensure that the actual online decision xv meets the ρ-competitive requirement (against the expert π). Specifically, we follow the RL decision ̃xv only if it can safely hedge against any future uncertainties (i.e., the expert’s future reward increase); otherwise, we need to roll back to the expert’s decision xπ\n\nv to stay on track for robustness.\n\nNote that naive switching conditions, e.g., only ensuring that the actual cumulative reward is at least ρ times of the expert’s cumulative reward at each step (Wu et al., 2016; Yang et al., 2022), can fail to meet the competitive ratio requirement in the end. The reason is that, even though the competitive ratio requirement is met (i.e., Rv ≥ ρRπ v − B) at the current step v, the expert can possibly obtain much higher rewards from future online items v + 1, v + 2, · · · if it has additional offline item capacity that the actual algorithm LOMAR does not have. Thus, we must carefully design the switching conditions to hedge against future risks. The no-free-disposal and free-disposal settings\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Inference of Robust Learning-based Online Bipartite Matching (LOMAR)\n\n1: Initialization: The actual set of items matched to u ∈ U is Vu,v after sequentially-arriving item v’s assignment with Vu,0 = ∅, the actual remaining capacity is bu = cu for u ∈ U, and the actual cumulative reward is R0 = (cid:80) u∈U fu(Vu,0) = 0. The same notations apply to the expert algorithm π by adding the superscript π. Competitive ratio requirement ρ ∈ [0, 1] and slackness B ≥ 0 with respect to the expert algorithm π.\n\n2: for v = 1 to |V| do 3: 4:\n\n5: 6: 7: 8: 9: 10: 11:\n\n12:\n\n13:\n\nv ,v = V π\n\nRun the algorithm π and match the item v to u ∈ U based on the expert’s decision u = xπ v . Update the expert’s decision set and reward for offline item u = xπ v : (V π V π xπ xπ Update the expert’s cumulative reward Rπ Get the set of available items Ua = {u ∈ U | |Vu,v−1| < cu} for u in Ua do\n\n(cid:83){v} and fxπ\n\nv ,v). v = (cid:80)\n\nu∈U fu\n\n= fxπ\n\nv ,v−1\n\nxπ\n\nv\n\nv\n\nCollect the available history information Iu about item u Run the RL model to get score: su = wuv − hθ(Iu, wuv) where θ is the network weight\n\nend for Calculate the probability of choosing each available item or skip:\n\n{{ ̃su}u∈Ua , ̃sskip} = softmax {{su}u∈Ua , 0}.\n\nObtain RL decision: ̃xv = arg maxu∈Ua if Rv−1 + w ̃xv,v ≥ ρ Select xv = ̃xv.\n\nv + (cid:80)\n\nRπ\n\nu∈U\n\n(cid:16)\n\nelse if xπ\n\nv is available for matching (i.e., |Vxπ\n\n//Follow the RL decision v ,v−1| < cxπ\n\nv ) then\n\nSelect xv = xπ v .\n\n//Follow the expert\n\n(cid:83){skip} {{ ̃su}u∈Ua , ̃sskip}. (cid:1)+\n\nu,v| + Iu= ̃xv\n\n(cid:0)|Vu,v−1| − |V π\n\n· wu,max\n\n(cid:17)\n\n− B then\n\nSelect xv = skip.\n\nend if Update assignment and reward: Vxv,v = Vxv,v−1\n\n(cid:83){v} and Rv = Rv−1 + wxv,v\n\nelse\n\n14: 15: 16: 17: 18: 19: 20: 21: end for\n\nrequire different switching conditions. Due to the page limit, we focus on the no-free-disposal setting below, while referring readers to Appendix B for more details about the free-disposal setting.\n\n4.2 ROBUSTNESS CONSTRAINT\n\nIn the no-free-disposal case, an offline item u ∈ U cannot receive any additional online items if it has been matched for cu times up to its capacity. By assigning more online items to u ∈ U than the expert algorithm at step v, LOMAR can possibly receive a higher cumulative reward than the expert’s cumulative reward. But, such advantages are just temporary, because the expert may receive an even higher reward in the future by filling up the unused capacity of item u. Thus, to hedge against the future uncertainties, LOMAR chooses the RL decisions only when the following condition is satified:\n\n(cid:32)\n\nRv−1 + w ̃xv,v ≥ ρ\n\nRπ\n\nv +\n\n(cid:88)\n\nu∈U\n\n(cid:0)|Vu,v−1| − |V π\n\nu,v| + Iu= ̃xv\n\n(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n− B,\n\n(3)\n\nv , plus the term (cid:80)\n\nwhere Iu= ̃xv = 1 if and only if u = ̃xv and 0 otherwise, (·)+ = max(·, 0), ρ ∈ [0, 1] and B ≥ 0 are the hyperparameters indicating the desired robustness with respect to the expert algorithm π. The interpretation of equation 3 is as follows. The left-hand side is the total reward of LOMAR after assigning the online item v based on the RL decision (i.e. ̃xt). The right-hand side is the expert’s (cid:0)|Vu,v−1| − |V π cumulative cost Rπ · wu,max which indicates the maximum reward that can be possibly received by the expert in the future. This reservation term is crucial, especially when the expert has more unused capacity than LOMAR. Specifically, |Vu,v−1| is the number of online items (after assigning v − 1 items) already assigned to the offline item u ∈ U, and hence (cid:0)|Vu,v−1| − |V π represents the number of more online items that LOMAR has assigned to u than the expert if LOMAR follows the RL decision at step v. If LOMAR assigns fewer items than the expert for an offline item u ∈ U, there is no need for any hedging because LOMAR is guaranteed to receive more rewards by filling up the item u up to the expert’s assignment level.\n\nu,v| + Iu= ̃xv\n\nu,v| + Iu= ̃xv\n\n(cid:1)+\n\n(cid:1)+\n\nu∈U\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nThe term wu,max in equation 3 is the set as the maximum possible reward for each decision. Even when wu,max is unknown in advance, LOMAR still applies by simply setting wu,max = ∞. In this case, LOMAR will be less “greedy” than the expert and never use more resources than the expert at any step for any u ∈ U.\n\n4.3 ROBUSTNESS ANALYSIS\n\nWe now formally show the competitive ratio of LOMAR. The proof is available in the appendix.\n\nTheorem 4.1. For any 0 ≤ ρ ≤ 1 and B ≥ 0 and any expert algorithm π, LOMAR achieves a competitive ratio of ρ against the algorithm π, i.e., R ≥ ρRπ − B for any problem input.\n\nThe hyperparameters 0 ≤ ρ ≤ 1 and B ≥ 0 govern the level of robustness we would like to achieve, at the potential expense of average reward performance. For example, by setting ρ = 1 and B = 0, we achieve the same robustness as the expert but leave little to none freedom for RL decisions. On the other hand, by setting a small ρ > 0 and/or large B, we provide higher flexibility to RL decisions for better average performance, while potentially decreasing the robustness. In fact, designing an algorithm that is guaranteed to simultaneously outperform RL and the expert is very challenging, if not impossible, and such tradeoff is necessary in the broad context of ML-augmented online algorithms (Rutten et al., 2022; Christianson et al., 2022). Additionally, in case of multiple experts, we can first combine these experts into a single expert and then apply LOMAR as if it works with a single combined expert.\n\nWhile the competitive ratio of all online algorithms against the optimal offline algorithm is zero in the no-free-disposal and general adversarial setting, there exist provably competitive online expert algorithms under some mild assumptions and other settings (Mehta, 2013). For example, the\n\nsimple greedy algorithm achieves for the adversarial no-free-disposal setting (Kim & Moon, 2020), and 1 2 for the free-disposal setting (Fahrbach et al., 2020), and there also exist 1/e-competitive algorithms against the optimal offline algorithm for the random-order setting (Mehta, 2013). Thus, an immediate result follows.\n\nunder bounded weights assumptions\n\n1 + maxu∈U\n\n(cid:16)\n\n(cid:17)−1\n\nwu,max wu,min\n\nCorollary 4.1.1. For any 0 ≤ ρ ≤ 1 and B ≥ 0, by using Algorithm 1 and an expert online algorithm π that is λ-competitive against the optimal offline algorithm OPT, then under the same assumptions for π to be λ-competitive, LOMAR is ρλ-competitive against OPT.\n\nCorollary 4.1.1 provides a general result that applies to any λ-competitive expert algorithm π under its respective required assumptions. For example, if the expert π assumes an adversarial or random-order setting, then Corollary 4.1.1 also holds true under the same adversarial or random-order setting.\n\nFinally, we comment on the randomized setting where randomization is over the algorithm choice and potentially increases the competitive ratio (Gupta & Roughgarden, 2020; Mehta, 2013). For the randomized setting, the competitive ratio is modified as E(R) ≥ ρROP T − B, where E is the expectation and ROP T is the optimal reward of the optimal offline algorithm (Mehta, 2013). We make no assumptions on the expert π in Algorithm 1. Thus, if the expert π itself is randomized in Algorithm 1, then LOMAR will also be randomized. Also, for any ρ ∈ [0, 1] and B ≥ 0, by directly applying Theorem 4.1 and Corollary 4.1.1 to the randomized setting, the competitive ratio of LOMAR will be ρλ-competitive against OPT if the randomized expert π itself is λ-competitive against OPT.\n\n5 RL POLICY TRAINING WITH ONLINE SWITCHING\n\nThe existing ML-augmented online algorithms typically assume a pre-trained standalone RL model (Christianson et al., 2022; Rutten et al., 2022). While the standalone RL model may perform well on its own, some already good actions can be replaced by expert’s action due to switching for robustness in inference. In other words, there will be a objective mismatch between training and testing. To rectify the mismatch, we propose a novel approach to train the RL model in LOMAR by explicitly considering the switching operation.\n\nRL architecture. For solving online bipartite matching, there exist various network architectures, e.g., fully-connected networks and scalable invariant network for arbitrary graph sizes. The prior study (Mehta, 2013) has shown using extensive empirical experiments that the invariant network\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\narchitecture, where each offline-online item pair runs through a separate neural network with shared weights among all the item pairs, is empirically advantageous, due to its scalability to large graph sizes and high average performance. We denote the RL model as hθ(Iu, wuv) where θ is the network parameter. By feeding the item weight wuv and applicable history information Iu for each offline-online item pair (u, v), we can use the RL model to output a threshold for possible item assignment, following threshold-based algorithms (Huang et al., 2019; Mehta, 2013). The history information Iu includes, but is not limited to, the average value and variance of weights assigned to u, average in-degree of u, and maximum weight for the already matched items. More details about the information can be found in the appendix. Then, with the RL output, we can obtain a score su = wuv − hθ(Iu, wuv), for each possible assignment, and the RL uses the offline item u ∈ U (plus “skip” with sskip = 0 in the no-free-disposal setting) with the largest su as its candidate action ̃xv when checking the switching condition in Algorithm 1.\n\nPolicy training. Training the RL model by considering switching in Algorithm 1 is highly non-trivial. Most critically, the initial RL decisions can perform arbitrarily badly upon policy initialization, which means that the initial RL decisions are almost always overridden by the expert’s decisions for robustness. Due to following the expert’s decisions, the RL agent almost always receive a good reward, which actually has nothing to do with the RL’s own decisions and hence provides little to no supervision to improve the RL policy. Consequently, this creates a gridlock for RL policy training. While using an offline pre-trained standalone RL model without considering online switching (e.g., (Alomrani et al., 2021)) as an initial policy may partially address this gridlock, this is certainly inefficient as we have to spend resources for training another RL model, let alone the likelihood of being trapped into the standalone RL model’s suboptimal policies (e.g. local minimums).\n\nTo address these issues, we introduce another softmax probability with temperature t to approximate the hard switching process during training. The switching probability depends on the cumulative reward difference Rdif f in the switching condition, which is\n\nRdif f = Rv−1 + w ̃xv,v − ρ\n\nRπ\n\nv +\n\n(cid:32)\n\n(cid:88)\n\nu∈U\n\n(cid:0)|Vu,v−1| − |V π\n\nu,v| + Iu= ̃xv\n\n(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n+ B (4)\n\nThen the probability of following RL is pos = eRdif f /t 1+eRdif f /t , where t is the temperature of softmax function. This softmax probability is differentiable and hence allows backpropagation to supervise the training of the RL model weight θ. We train the RL agent by applying REINFORCE (Williams, 1992) to optimize the policy parameter θ. Denote τ = {x1, · · · , xv} as an action trajectory sample and pθ(τ ) as the possibility of the trajectory given the RL policy, where pθ is calculated based on the selection probability of RL model and expert. Our goal is to maximize the expected total reward Rθ = Eτ ∼pθ [wxv,v]. Thus, at each training step, given an RL policy with parameter θ, we sample n action trajectories {τi = {x1,i, · · · , xv,i}, i ∈ [n]} and record the corresponding rewards. We can get the approximated average reward as ˆRθ = 1 xi,v,v. Then, we calculate the gradient of the RL policy parameter as v∈V ∇θ log pθ(xv,i | Iu,i)(cid:1) (cid:16)(cid:80) ∇θ ˆRθ = (cid:80)n . Then, we update the parameter θ by θ = θ + α∇θ ˆRθ, where α controls the training step size.\n\ni=1 wi (cid:0)(cid:80)\n\nv∈V wi\n\n(cid:80)n\n\nxv,i,v\n\ni=1\n\n(cid:17)\n\nn\n\nBy changing the temperature t for softmax, we are able to balance exploration and exploitation. Specifically, at the beginning of the policy training, we can set a high temperature to encourage the RL model to explore more aggressively, instead of sticking to the expert’s decisions. As the RL model performance continuously improves, we can reduce the temperature in order to make the RL agent more aware of the downstream switching operation. The training process is performed offline as in the existing RL-based optimizers (Alomrani et al., 2021; Du et al., 2022) and described in Algorithm 3.\n\n6 EXPERIMENT\n\n6.1 SETUP\n\nTo validate the effectiveness of LOMAR, we conduct experiments based on the movie recommendation application. Specifically, when an user (i.e., online item v) arrives, we recommend a movie (i.e.,\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\noffline item u) to this user and receive a reward based on the user-movie preference information. We choose the MovieLens dataset (Harper & Konstan, 2015), which provides a total of 3952 movies, 6040 users and 100209 ratings. We preprocess the dataset to sample movies and users randomly from the dataset to generate subgraphs, following the same steps as used by (Dickerson et al., 2019) and (Alomrani et al., 2021). In testing dataset, we empirically evaluate each algorithm using average reward (AVG) and competitive ratio (CR, against OPT), which represents the average performance and worst case performance, respectively. Thus, the value of CR is the empirically worst reward ratio in the testing dataset. For fair comparison, all the experimental settings like capacity cu follow those used in (Alomrani et al., 2021). More details about the problem setup and training details are deferred to Appendix A.\n\nBaseline Algorithms. We consider the following baselines. All the RL policies are trained offline with the same architecture and applicable hyperparameters.\n\n• OPT: The offline optimal oracle has the complete information about the bipartite graph. We\n\nuse the Gurobi optimizer to find the optimal offline solution.\n\n• Greedy: At each step, Greedy selects the available offline item with highest weight.\n\n• DRL: It uses the same architecture as in LOMAR, but does not consider online switching for training or inference. That is, the RL model is both trained and tested with ρ = 0. More specifically, our RL architecture has 3 fully connected layers, each with 100 hidden nodes.\n\n• DRL-OS (DRL-OnlineSwitching): We apply online switching to the same RL policy used by DRL during inference. That is, the RL model is trained with ρ = 0, but tested with a different ρ > 0.\n\nThe choice of baselines include all those considered in (Alomrani et al., 2021). In the no-free-disposal setting, the best competitive ratio is 0 in general adversarial cases (Mehta, 2013). Here, we use Greedy as the expert algorithm, because the recent study (Alomrani et al., 2021) has shown that Greedy performs better than other alternatives and is a strong baseline.\n\n6.2 RESULTS\n\nTest ρ = 0.4 ρ = 0.6 ρ = 0.8\n\nDRL-OS\n\nAVG 12.315 11.919 11.524\n\nCR 0.800 0.787 0.773\n\nLOMAR (ρ = 0.4) AVG 12.364 11.982 11.538\n\nCR 0.819 0.807 0.766\n\nLOMAR (ρ = 0.6) AVG 12.288 11.990 11.543\n\nCR 0.804 0.807 0.762\n\nLOMAR (ρ = 0.8) AVG 12.284 11.989 11.561\n\nCR 0.804 0.800 0.765\n\nGreedy\n\nAVG 11.000 11.000 11.000\n\nCR 0.723 0.723 0.723\n\nTable 1: Comparison under different ρ. In the top, LOMAR (ρ = x) means LOMAR is trained with the value of ρ = x. The average reward and competitive ratio are represented by AVG and CR, respectively — the higher, the better. The highest value in each testing setup is highlighted in bold. The AVG and CR for DRL are 12.909 and 0.544 respectively. The average reward for OPT is 13.209.\n\nWe show the comparison of LOMAR with baseline algorithms in Table 1. First, we see that DRL has the highest average reward, but its empirical competitive ratio is the lowest. The expert algorithm Greedy is fairly robust, but has a lower average award than RL-based policies. Second, DRL-OS can improve the competitive ratio compared to DRL. But, its RL policy is trained alone without being aware of the online switching. Thus, by making the RL policy aware of online switching, LOMAR can improve the average reward compared to DRL-OS. Specifically, by training LOMAR using the same ρ as testing it, we can obtain both the highest average cost and the highest competitive ratio. One exception is the minor decrease of competitive ratio when ρ = 0.8 for testing. This is likely due to the dataset and a few hard instances can affect the empirical competitive ratio, which also explains why the empirical competitive ratio is not necessarily monotonically increasing in the ρ ∈ [0, 1]. Nonetheless, unlike DRL that may only work well empirically without guarantees, LOMAR offers provable robustness guarantees while exploiting the power of RL to improve the average performance. The boxplots in Fig. 1 visualizes the reward ratio distribution of LOMAR, which further validates the importance of switching-aware training.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Testing with ρ = 0.4\n\n(b) Testing with ρ = 0.6\n\n(c) Testing with ρ = 0.8\n\nFigure 1: Boxplot for reward ratio with different ρ within testing dataset. Greedy and DRL-OS are also shown here for comparison. The best average performance in each figure is achieved by choosing the same ρ during training and testing.\n\n(a) ρ = 0.0 (i.e., DRL)\n\n(b) ρ = 0.4\n\n(c) ρ = 0.6\n\n(d) ρ = 0.8\n\nFigure 2: Histogram of bi-competitive reward ratios of DRL-OS (against Greedy and DRL) under different ρ.\n\nTo show the effect of switching with different ρ, we calculate the bi-competitive reward ratios. Specifically, for each problem instance, the bi-competitive ratio compares the actual reward against those of Greedy and RL model, respectively. To highlight the effect of online switching, we focus on DRL-OS (i.e., training the RL with ρ = 0) whose training process of RL model is not affected by ρ, because the RL model trained with ρ > 0 in LOMAR does not necessarily perform well on its own and the reward ratio of LOMAR to its RL model is not meaningful. The histogram of the bi-competitive ratios are visualized in Fig. 2. When ρ = 0, the ratio of DRL-OS/ DRL is always 1 unsurprisingly, since DRL-OS are essentially the same as DRL in this case (i.e., both trained and tested with ρ = 0). With a large ρ (e.g. 0.8) for testing, the reward ratios of DRL-OS/Greedy for most samples are around 1, which means the robustness is achieved, as proven by our theoretical analysis. But on the other hand, DRL-OS has limited flexibility and can less exploit the good average performance of DRL. Thus, the hyperparameter ρ ∈ [0, 1] governs the tradeoff between average performance and robustness relative to the expert and, like other hyperparameters, can be tuned to maximize the average performance subject to the robustness requirement.\n\nWe also consider a crowdsourcing application, as provided by the gMission dataset (Chen et al., 2014). Additional results for LOMAR and baselines in gMission are deferred to Appendix A.\n\n7 CONCLUSION\n\nIn this paper, we propose LOMAR to achieve both good average-case and good worst-case performance for edge-weighted online bipartite matching. LOMAR includes a novel online switching operation to decide whether to follow the expert’s decision or the RL decision for each online item arrival. We prove that for any ρ ∈ [0, 1], LOMAR is ρ-competitive against any expert online algorithms, which directly translates a bounded competitive ratio against OPT if the expert algorithm itself has one. We also train the RL policy by explicitly considering the online switching operation so as to improve the average performance. Finally, we run empirical experiments to validate LOMAR.\n\n9\n\n0.40.60.8DRL-OSGreedyTraining 0.700.750.800.850.900.951.00Reward Ratio0.40.60.8DRL-OSGreedyTraining 0.700.750.800.850.900.951.00Reward Ratio0.40.60.8DRL-OSGreedyTraining 0.700.750.800.850.900.951.00Reward Ratio0.80.91.01.11.21.3DRL-OS / GREEDY Reward Ratio0.80.91.01.1DRL-OS / DRL Reward Ratio 0 5 10 15 200.80.91.01.11.21.3DRL-OS / GREEDY Reward Ratio0.80.91.01.1DRL-OS / DRL Reward Ratio 0 5 10 15 200.80.91.01.11.21.3DRL-OS / GREEDY Reward Ratio0.80.91.01.1DRL-OS / DRL Reward Ratio 0 5 10 15 200.80.91.01.11.21.3DRL-OS / GREEDY Reward Ratio0.80.91.01.1DRL-OS / DRL Reward Ratio 0 5 10 15 20Under review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nThe details of proving Theorem 4.1 are included in Appendix C. The experimental codes and settings are based on the open-sourced resources in Alomrani et al. (2021). The implementation of LOMAR mainly includes adding the switching condition for training and testing based on the standard DRL-based algorithm Alomrani et al. (2021) and will be released upon publication.\n\nREFERENCES\n\nJoshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In\n\nInternational conference on machine learning, pp. 22–31. PMLR, 2017.\n\nGagan Aggarwal, Gagan Goel, Chinmay Karande, and Aranyak Mehta. Online vertex-weighted bipartite matching and single-bid budgeted allocations. In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms, pp. 1253–1264. SIAM, 2011.\n\nMohammad Ali Alomrani, Reza Moravej, and Elias B. Khalil. Deep policies for online bipartite matching: A reinforcement learning approach. CoRR, abs/2109.10380, 2021. URL https: //arxiv.org/abs/2109.10380.\n\nKeerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. A regression approach to learningaugmented online algorithms. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https: //openreview.net/forum?id=GgS40Y04LxA.\n\nAntonios Antoniadis, Christian Coester, Marek Elias, Adam Polak, and Bertrand Simon. Online\n\nmetric algorithms with untrusted predictions. In ICML, 2020a.\n\nAntonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online matching problems with machine learned advice. Advances in Neural Information Processing Systems, 33: 7933–7944, 2020b.\n\nFelix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe modelbased reinforcement learning with stability guarantees. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, pp. 908–919, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.\n\nNiv Buchbinder, Joseph Seffi Naor, et al. The design of competitive online algorithms via a primal– dual approach. Foundations and Trends® in Theoretical Computer Science, 3(2–3):93–263, 2009.\n\nWeirong Chen, Jiaqi Zheng, Haoyu Yu, Guihai Chen, Yixin Chen, and Dongsheng Li. Online learning bipartite matching with non-stationary distributions. ACM Trans. Knowl. Discov. Data, 16(5), mar 2022. ISSN 1556-4681. doi: 10.1145/3502734. URL https://doi.org/10.1145/ 3502734.\n\nZhao Chen, Rui Fu, Ziyuan Zhao, Zheng Liu, Leihao Xia, Lei Chen, Peng Cheng, Caleb Chen Cao, Yongxin Tong, and Chen Jason Zhang. gmission: A general spatial crowdsourcing platform. Proceedings of the VLDB Endowment, 7(13):1629–1632, 2014.\n\nJakub Chł ̨edowski, Adam Polak, Bartosz Szabucki, and Konrad Tomasz ̇Zołna. Robust learning-\n\naugmented caching: An experimental study. In ICML, 2021.\n\nNicolas Christianson, Tinashe Handina, and Adam Wierman. Chasing convex bodies and functions\n\nwith black-box advice. In COLT, 2022.\n\nNikhil R Devanur, Kamal Jain, and Robert D Kleinberg. Randomized primal-dual analysis of ranking for online bipartite matching. In Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms, pp. 101–107. SIAM, 2013.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJohn P. Dickerson, Karthik Abinav Sankararaman, Aravind Srinivasan, and Pan Xu. Balancing relevance and diversity in online bipartite matching via submodularity. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI’19/IAAI’19/EAAI’19. AAAI Press, 2019. ISBN 978-1-57735-809-1. doi: 10.1609/aaai.v33i01.33011877. URL https://doi.org/10.1609/aaai.v33i01. 33011877.\n\nBingqian Du, Chuan Wu, and Zhiyi Huang. Learning resource allocation and pricing for cloud profit maximization. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI’19/IAAI’19/EAAI’19. AAAI ISBN 978-1-57735-809-1. doi: 10.1609/aaai.v33i01.33017570. URL https: Press, 2019. //doi.org/10.1609/aaai.v33i01.33017570.\n\nBingqian Du, Zhiyi Huang, and Chuan Wu. Adversarial deep learning for online resource allocation. ACM Trans. Model. Perform. Eval. Comput. Syst., 6(4), feb 2022. ISSN 2376-3639. doi: 10.1145/ 3494526. URL https://doi.org/10.1145/3494526.\n\nElbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the “learning\" into learningaugmented algorithms for frequency estimation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 2860–2869. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/du21d.html.\n\nAdam N. Elmachtoub and Paul Grigas. Smart “predict, then optimize”. CoRR, abs/1710.08005, 2017.\n\nURL https://arxiv.org/abs/1710.08005.\n\nMatthew Fahrbach, Zhiyi Huang, Runzhou Tao, and Morteza Zadimoghaddam. Edge-weighted online bipartite matching. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pp. 412–423, 2020. doi: 10.1109/FOCS46700.2020.00046.\n\nEvrard Garcelon, Mohammad Ghavamzadeh, Alessandro Lazaric, and Matteo Pirotta. Conservative exploration in reinforcement learning. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 1431–1441. PMLR, 26–28 Aug 2020.\n\nDobrik Georgiev and Pietro Lió. Neural bipartite matching. CoRR, abs/2005.11304, 2020. URL\n\nhttps://arxiv.org/abs/2005.11304.\n\nRishi Gupta and Tim Roughgarden. A pac approach to application-specific algorithm selection. SIAM\n\nJournal on Computing, 46(3):992–1017, 2017.\n\nRishi Gupta and Tim Roughgarden. Data-driven algorithm design. Commun. ACM, 63(6):87–94, May 2020. ISSN 0001-0782. doi: 10.1145/3394625. URL https://doi.org/10.1145/ 3394625.\n\nF. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. ACM Trans. Interact. Intell. Syst., 5(4), dec 2015. ISSN 2160-6455. doi: 10.1145/2827872. URL https://doi.org/10.1145/2827872.\n\nZhiyi Huang and Xinkai Shu. Online Stochastic Matching, Poisson Arrivals, and the Natural Linear Program, pp. 682–693. Association for Computing Machinery, New York, NY, USA, 2021. ISBN 9781450380539.\n\nZhiyi Huang, Zhihao Gavin Tang, Xiaowei Wu, and Yuhao Zhang. Online vertex-weighted bipartite matching: Beating 1-1/e with random arrivals. ACM Transactions on Algorithms (TALG), 15(3): 1–15, 2019.\n\nChinmay Karande, Aranyak Mehta, and Pushkar Tripathi. Online bipartite matching with unknown distributions. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pp. 587–596, 2011.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nR. M. Karp, U. V. Vazirani, and V. V. Vazirani. An optimal algorithm for on-line bipartite matching. In Proceedings of the Twenty-Second Annual ACM Symposium on Theory of Computing, STOC ’90, pp. 352–358, New York, NY, USA, 1990. Association for Computing Machinery. ISBN 0897913612. doi: 10.1145/100216.100262. URL https://doi.org/10.1145/100216.100262.\n\nAbbas Kazerouni, Mohammad Ghavamzadeh, Yasin Abbasi Yadkori, and Benjamin Van Roy. Conservative contextual linear bandits. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/ paper/2017/file/bdc4626aa1d1df8e14d80d345b2a442d-Paper.pdf.\n\nThomas Kesselheim, Klaus Radke, Andreas Tönnis, and Berthold Vöcking. An optimal online algorithm for weighted bipartite matching and extensions to combinatorial auctions. In European symposium on algorithms, pp. 589–600. Springer, 2013.\n\nGwang Kim and Ilkyeong Moon. Online advertising assignment problem without free disposal. Applied Soft Computing, 93:106370, 2020. ISSN 1568-4946. doi: https://doi.org/10.1016/j.asoc. 2020.106370. URL https://www.sciencedirect.com/science/article/pii/ S1568494620303100.\n\nNitish Korula and Martin Pál. Algorithms for secretary problems on graphs and hypergraphs. In International Colloquium on Automata, Languages, and Programming, pp. 508–520. Springer, 2009.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1179–1191. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 0d2b2061826a5df3221116a5085a6052-Paper.pdf.\n\nHeyuan Liu and Paul Grigas. Risk bounds and calibration for a smart predict-then-optimize method. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= pSitk34qYit.\n\nThodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. J.\n\nACM, 68(4), July 2021.\n\nAranyak Mehta. Online matching and ad allocation. Foundations and Trends in Theoretical Computer\n\nScience, 8 (4):265–368, 2013. URL http://dx.doi.org/10.1561/0400000057.\n\nDaan Rutten, Nico Christianson, Debankur Mukherjee, and Adam Wierman. Online optimization with untrusted predictions. CoRR, abs/2202.03519, 2022. URL https://arxiv.org/abs/ 2202.03519.\n\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1889–1897, Lille, France, 07–09 Jul 2015. PMLR.\n\nGarrett Thomas, Yuping Luo, and Tengyu Ma. Safe reinforcement learning by imagining the near future. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 13859–13869. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 73b277c11266681122132d024f53a75b-Paper.pdf.\n\nHingfung Ting and Xiangzhong Xiang. Near optimal algorithms for online maximum weighted b-matching. In International Workshop on Frontiers in Algorithmics, pp. 240–251. Springer, 2014.\n\nKai Wang, Sanket Shah, Haipeng Chen, Andrew Perrault, Finale Doshi-Velez, and Milind Tambe. Learning MDPs from features: Predict-then-optimize for sequential decision making by reinforcement learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview. net/forum?id=-mGv2KxQ43D.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nYansheng Wang, Yongxin Tong, Cheng Long, Pan Xu, Ke Xu, and Weifeng Lv. Adaptive dynamic bipartite graph matching: A reinforcement learning approach. In 2019 IEEE 35th International Conference on Data Engineering (ICDE), pp. 1478–1489, 2019. doi: 10.1109/ICDE.2019.00133.\n\nBryan Wilder, Bistra Dilkina, and Milind Tambe. Melding the data-decisions pipeline: Decisionfocused learning for combinatorial optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 1658–1665, 2019.\n\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. Machine learning, 8(3):229–256, 1992.\n\nYifan Wu, Roshan Shariff, Tor Lattimore, and Csaba Szepesvári. Conservative bandits. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML’16, pp. 1254–1262. JMLR.org, 2016.\n\nTsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J. Ramadge. Projection-based constrained policy optimization. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rke3TJrtPS.\n\nYunchang Yang, Tianhao Wu, Han Zhong, Evrard Garcelon, Matteo Pirotta, Alessandro Lazaric, Liwei Wang, and Simon Shaolei Du. A reduction-based framework for conservative bandits and reinforcement learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=AcrlgZ9BKed.\n\nAli Zeynali, Bo Sun, Mohammad Hassan Hajiesmaili, and Adam Wierman. Data-driven competitive algorithms for online knapsack and set cover. In AAAI, 2021. URL https://ojs.aaai.org/ index.php/AAAI/article/view/17294.\n\nGoran Zuzic, Di Wang, Aranyak Mehta, and D. Sivakumar. Learning robust algorithms for online allocation problems using adversarial training. In https://arxiv.org/abs/2010.08418, 2020.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nIn the appendix, we show the experimental setup and additional results (Appendix A), algorithm details for the free-disposal setting (Appendix B), and finally the proof of Theorem 4.1 (Appendix C).\n\nA EXPERIMENTAL SETTINGS AND ADDITIONAL RESULTS\n\nOur implementation of all the considered algorithms, including LOMAR, is based on the source codes provided by Alomrani et al. (2021), which includes codes for training the RL model, data pre-proposing and performance evaluation. We conduct experiments on two real-world datasets: MovieLens Harper & Konstan (2015) and gMission Chen et al. (2014).\n\nA.1 MOVIELENS\n\nA.1.1 SETUP AND TRAINING\n\nWe first sample u0 movies from the original MovieLens dataset Harper & Konstan (2015). We then sample v0 users and make sure each user can get at least one movie; otherwise, we remove the users that have no matched movies, and resample new users. After getting the topology graph, we use Gurobi to find the optimal matching decision. In our experiment, we set u0 = 10 and v0 = 60 to generate the training and testing datasets. The number of graph instances in the training and testing datasets are 20000 and 1000, respectively. For the sake of reproducibility and fair comparision, our settings follows the same setup of Alomrani et al. (2021). In particular, the general movie recommendation problem belongs to online submodular optimization, but it can actually be equivalently mapped to edge-weighted online bipartite matching with no free disposal under the setting considered in Alomrani et al. (2021). So by default, the capacity cu for each offline node is set as 1 and wu,max = 5. While LOMAR can use any RL architecture, we follow the design of inv-ff-hist proposed by Alomrani et al. (2021), which empirically demonstrates the best performance among all the considered architectures.\n\nThe input to our considered RL model is the edge weights wuv revealed by the online items plus some historical information, which includes: Mean and variances of each offline node’s weights; Average degree of each offline nodes; Normalized step size; Percentage of offline nodes connected to the current node; Statistical information of these already matched nodes’ weights (maximum, minimum, mean and variance); Ratio of matched offline node; Ratio of skips up to now; Normalized reward with respect to the offline node number. For more details of the historical information, readers are referred to Table 1 in Alomrani et al. (2021).\n\nFor applicable algorithms (i.e., DRL, DRL-OS, and LOMAR), we train the RL model for 300 epochs in the training dataset with a batch size of 100. In LOMAR, the parameter B = 0 is used to follow the strict definition of competitive ratio. We test the algorithms on the testing dataset to obtain the average reward and the worst-case competitive ratio empirically. By setting ρ = 0 for training, LOMAR is equivalent to the vanilla inv-ff-hist RL model (i.e., DRL) used in Alomrani et al. (2021). Using the same problem setup, we can reproduce the same results shown in Alomrani et al. (2021), which reaffirms the correctness of our data generation and training process.\n\nFigure 3: Tail reward ratio comparison. In this experiment, we set ρ = 0.4 for DRLOS and LOMAR.\n\nAdditionally, training the RL model in LOMAR usually takes less than 8 hours on a shared research cluster with one NVIDIA K80 GPU, which is almost the same as the training the model for DRL in a standalone manner (i.e., setting ρ = 0 without considering online switching).\n\nA.1.2 ADDITIONAL RESULTS\n\nIn Table 1, we have empirically demonstrated that LOMAR achieves the best tradeoff between the average reward and competitive ratio. In Fig 3, we further demonstrate that LOMAR not only achieves\n\n14\n\n99.099.299.499.699.8100.0Percentile0.50.60.70.80.91.0Tail Reward RatioDRLLOMARDRL-OSGreedyUnder review as a conference paper at ICLR 2023\n\na better worst-case competitive ratio (at 100.0%). The tail reward ratio of LOMAR is also good compared to the baseline algorithms. Specifically, we show the percentile of reward ratios (compared to the optimal offline algorithm) — the 100% means the worst-case empirical reward ratio (i.e., competitive ratio). We see that DRL has a bad high-percentile reward ratio and lacks performance robustness, although its lower-percentile cost ratio is better. This is consistent with the good average performance of LOMAR. Because of online switching, both DRL-OS and LOMAR achieve better robustness, and LOMAR is even better due to its awareness of the online switching operation during its training process. The expert Greedy has a fairly stable competitive ratio, showing its good robustness. But, it can be outperformed by other algorithms when we look at lower-percentile reward ratio.\n\nA.1.3 RESULTS FOR ANOTHER EXPERT ALGORITHM\n\nOptimally competitive expert algorithms have been developed under the assumptions of random oder and/or i.i.d. rewards of different online items. In particular, by considering the random order setting, OSM (online secretary matching) has the optimal competitive ratio of 1/e (Kesselheim et al., 2013). Note that the competitive ratio for OSM is average over the random order of online items, while the rewards can be adversarially chosen. We show the empirical results in Fig. 4. As OSM skips the first |V|/e online items, it actually does not perform (in terms of the empirical worst-case cost ratio) as well as the default expert Greedy in our experiments despite its guaranteed competitive ratio against OPT. That said, we still observe the same trend as using Greedy for the expert: by tuning ρ ∈ [0, 1], LOMAR achieves a good average performance while guaranteeing the competitiveness against the expert OSM (and against OPT as OSM itself is optimally competitive against OPT).\n\n(a) Cost ratio against OPT\n\n(b) Cost ratio against OSM\n\nFigure 4: Cost ratio distribution (OSM as the expert)\n\nFig. 4 shows the empirical results in our testing dataset, which does not strictly satisfy the random order assumption required by OSM. Next, to satisfy the random order assumption, we select a typical problem instance and randomly vary the arrival orders of online items. We show the cost ratio averaged over the random arrival order in Table 2. Specifically, we calculate each cost ratio by 100 different random orders, and repeat this process 100 times. We show the mean and stand deviation of the average cost ratios (each averaged over 100 different random orders). We see that LOMAR improves the average cost ratio compared to OSM under the random order assumption. While DRL has a better average cost for this particular instance, it does not provide any guaranteed worst-case robustness as LOMAR.\n\nPure ML 0.9794 0.0074\n\nLOMAR ρ = 0.2 0.9688 0.0082\n\nLOMAR ρ = 0.4 0.9431 0.0078\n\nLOMAR ρ = 0.6 0.9095 0.0086\n\nLOMAR ρ = 0.8 0.8799 0.0084\n\nOSM 0.8459 0.0084\n\nMean Std\n\nTable 2: Cost ratio (averaged over the random arrival order) for a typical graph instance\n\nA.2 GMISSION\n\nThe gMission dataset Chen et al. (2014) considers a crowdsourcing application, where the goal is to assign the tasks (online items) to workers (offline items). The edge weight between a certain online task and each worker can be calculated by the product of the task reward and the worker’s success probability, which is determined by the physical location of workers and the type of tasks.\n\n15\n\nMethods0.50.60.70.80.91Reward RatioDRLLOMAR_0.2LOMAR_0.4LOMAR_0.6LOMAR_0.8OSMMethods0.50.70.91.11.31.5Reward RatioDRLLOMAR_0.2LOMAR_0.4LOMAR_0.6LOMAR_0.8Under review as a conference paper at ICLR 2023\n\nOur goal is to maximize the total reward given the capacity of each worker, which perfectly fits into our formulation in Eqn. equation 1.\n\nWe use the same data processing and RL architecture design as introduced in Section A.1.1. We train LOMAR with different ρ in the gMission dataset by setting u0 = 10, v0 = 60, wu,max = 1. Again, we use Greedy as the expert, which is an empirically strong baseline algorithm as shown in Alomrani et al. (2021). Our results are all consistent with those presented in Alomrani et al. (2021).\n\nA.2.1 TESTING ON 10 × 60\n\nIn our the first result, we generate a testing dataset with u0 = 10 and v0 = 60, which is the same setting as our training dataset. In other words, the training and testing datasets have similar distributions. Specifically, Greedy’s average reward and competitive ratio are 4.508 and 0.432, while these two values for DRL are 5.819 and 0.604, respectively. Thus, DRL performs outperforms Greedy in both average performance and the worst-case performance.\n\nρ in Testing 0.4 0.6 0.8 0.9\n\nDRL-OS\n\nAVG 5.553 5.389 5.102 4.836\n\nCR 0.599 0.591 0.543 0.495\n\nLOMAR ρ = 0.4 AVG 5.573 5.429 5.115 4.836\n\nCR 0.598 0.619 0.543 0.495\n\nLOMAR ρ = 0.6 AVG 5.553 5.420 5.111 4.839\n\nCR 0.598 0.619 0.523 0.495\n\nLOMAR ρ = 0.8 AVG 5.523 5.403 5.110 4.839\n\nCR 0.598 0.623 0.521 0.540\n\nLOMAR ρ = 0.9 AVG 5.535 5.402 5.107 4.839\n\nCR 0.598 0.623 0.521 0.540\n\nTable 3: Performance comparison in gMission 10 × 60 for different ρ. LOMAR with ρ = y means LOMAR is trained with ρ = y.\n\nNext, we show the results for LOMAR and DRL-OS under different ρ ∈ [0, 1] in Table 3. In general, by setting a larger ρ for inference, both LOMAR and DRL-OS are closer to the expert algorithm Greedy, because there is less freedom for the RL decisions. As a result, when ρ increases for inference, the average rewards of both DRL-OS and LOMAR decrease, although they have guaranteed robustness whereas DRL does not. Moreover, by training the RL model with explicit awareness of online switching, LOMAR can have a higher average cost than DRL-OS, which reconfirms the benefits of training the RL model by considering its downstream operation. Interestingly, by setting ρ identical for both training and testing, the average reward may not always be the highest for LOMAR. This is partially because of the empirical testing dataset. Another reason is that, in this test, DRL alone already performs the best (both on average and in the worst case). Hence, by setting a smaller ρ for inference, LOMAR works better empirically though it is trained under a different ρ. Nonetheless, this does not void the benefits of guaranteed robustness in LOMAR. The empirically better performance of DRL lacks guarantees, which we show as follows.\n\nA.2.2 TESTING ON 100 × 100\n\nIn our second test, we consider an opposite case compared to the first one. We generate a testing dataset with u0 = 100 and v0 = 100, which is different from the training dataset setting. As a result, the training and testing datasets have very different distributions, making DRL perform very badly. Specifically, Greedy’s average reward and competitive ratio are 40.830 and 0.824, and these two values for DRL are 32.938 and 0.576, respectively. DRL has an even lower average reward than Greedy, showing its lack of performance robustness.\n\nWe show the results for LOMAR and DRL-OS under different ρ ∈ [0, 1] in Table 4. In general, by setting a larger ρ for inference, both LOMAR and DRL-OS are closer to the expert algorithm Greedy. As Greedy works empirically much better than DRL in terms of the average performance and the worst-case performance, both LOMAR and DRL-OS have better performances when we increase ρ to let Greedy safeguard the RL decisions more aggressively. Moreover, by training the RL model with explicit awareness of online switching, LOMAR can have a higher average cost than DRL-OS, which further demonstrates the benefits of training the RL model by considering its downstream operation. Also, interestingly, by setting ρ identical for both training and testing, the average reward may not be the highest for LOMAR, partially because of the empirical testing dataset. Another reason is that, in this test, DRL alone already performs very badly (both on average and in the worst case)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nρ in Testing 0.4 0.6 0.8 0.9\n\nDRL-OS\n\nAVG 33.580 34.973 37.939 39.772\n\nCR 0.604 0.680 0.758 0.794\n\nLOMAR ρ = 0.4 AVG 37.030 37.490 38.866 40.057\n\nCR 0.707 0.731 0.775 0.803\n\nLOMAR ρ = 0.6 AVG 38.199 38.518 39.502 40.377\n\nCR 0.750 0.762 0.782 0.806\n\nLOMAR ρ = 0.8 AVG 38.324 38.505 39.385 40.239\n\nCR 0.750 0.756 0.794 0.812\n\nLOMAR ρ = 0.9 AVG 38.538 38.727 39.552 40.332\n\nCR 0.766 0.767 0.781 0.798\n\nTable 4: Performance comparison on gMission 100 × 100 for different ρ. LOMAR with ρ = y means LOMAR is trained with ρ = y.\n\ndue to the significant training-testing distributional discrepancy. Hence, by setting a higher ρ, LOMAR works better empirically though it is tested under a different ρ. An exception is when testing LOMAR with ρ = 0.9: setting ρ = 0.6/0.8 for training makes LOMAR perform slightly better in terms of the average performance and worst-case performance, respectively. But, setting ρ = 0.9 for training still brings benefits to LOMAR compared to DRL-OS that does not consider the downstream online switching operation.\n\nTo sum up, our experimental results under different settings demonstrate: LOMAR’s empirical improvement in terms of the average reward compared to DRL-OS; the improved competitive ratio of LOMAR and DRL-OS compared to DRL, especially when the training-testing distributions differ significantly; and the improved average reward of LOMAR compared to Greedy when RL is good. Therefore, LOMAR can exploit the power of RL while provably guaranteeing the performance robustness.\n\nB FREE DISPOSAL\n\nThe offline version of bipartite matching with free disposal can be expressed as:\n\nWith Free Disposal:\n\nmax\n\n(cid:88)\n\nxuv∈{0,1},u∈U\n\n(cid:32)\n\nmax S⊆Vu,|S|≤cu\n\n(cid:33)\n\n(cid:88)\n\nv∈S\n\nwuv\n\ns.t.\n\nVu = {v ∈ V | xuv = 1} ∀u ∈ U,\n\n(cid:88)\n\nu∈U\n\nxuv ≤ 1, ∀v ∈ V,\n\n(5)\n\nwhere Vu = {v ∈ V | xuv = 1} is the set of online items matched to u ∈ U and the objective maxS∈Vu,|S|≤cu\n\nv∈S xuvwuv indicates that only up to top cu rewards are counted for u ∈ U.\n\n(cid:80)\n\nIn the free-disposal setting, it is more challenging to design the switching conditions to guarantee the robustness. The reason is the additional flexibility allowed for matching decisions — each offline item u ∈ U is allowed to be matched more than cu times although only up to top cu rewards actually count Mehta (2013); Fahrbach et al. (2020). For example, even though LOMAR and the expert assign the same number of online items to an offline item u ∈ U and LOMAR is better than the expert at a certain step, future high-reward online items can still be assigned to u ∈ U, increasing the expert’s total reward or even equalizing the rewards of LOMAR and the expert (i.e., high-reward future online items become the top cu items for u ∈ U for both LOMAR and the expert). Thus, the temporarily “higher” rewards received by LOMAR must be hedged against such future uncertainties. Before designing our switching condition for the free-disposal setting, we first define the set containing the top cu online items for u ∈ U after assignment of v:\n\nEu,v(Vu,v) = arg\n\nmax E⊆Vu,v,|E|=cu\n\n(cid:88)\n\nv∈E\n\nwuv,\n\n(6)\n\nwhere Vu,v is the set of all online items matched to u ∈ U so far after assignment of item v ∈ V. When there are fewer than cu items in Vuv, we will simply add null items with reward 0 to Eu,v such that |Eu,v| = cu. We also sort the online items denoted as eu,i, for i = 1, · · · , cu, contained in Eu,v according to their weights in an increasing order such that wu,eu,1 ≤ · · · ≤ wu,eu,cu . Similarly, we define the same top-cu item set for the expert algorithm π by adding the superscript π.\n\nNext, we define the following value which indicates the maximum possible additional reward for the expert algorithm π if LOMAR simply switches to the expert and follows it for all the future steps\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Inference of LOMAR (Free Disposal)\n\n1: Initialization: The actual set of items matched to u ∈ U is Vu,v after sequentially-arriving item v’s assignment with Vu,0 = ∅, the actual remaining capacity is bu = cu for u ∈ U, and the actual cumulative reward is R0 = (cid:80) u∈U fu(Vu,0) = 0. The same notations apply to the expert algorithm π by adding the superscript π. Competitive ratio requirement ρ ∈ [0, 1] and slackness B ≥ 0 with respect to the expert algorithm π.\n\n2: for v = 1 to |V| do 3: 4:\n\nRun the algorithm π and match the item v to u ∈ U based on the expert’s decision u = xπ v . Update the expert’s decision set and reward for offline item u = xπ v : (V π V π xπ xπ Update the expert’s cumulative reward Rπ for u in U do\n\n(cid:83){v} and fxπ\n\nv ,v). v = (cid:80)\n\nv ,v = V π\n\nu∈U fu\n\n= fxπ\n\nv ,v−1\n\nxπ\n\nv\n\nv\n\nCollect the available history information Iu about item u Run the RL model to get score: su = wuv − hθ(Iu, wuv) where θ is the network weight\n\nend for Calculate the probability of choosing each item u: {{ ̃su}u∈U } = softmax {{su}u∈U }. Obtain RL decision: ̃xv = arg maxu∈U (cid:83){skip} {{ ̃su}u∈U }. Find ∆f ̃xv in Eqn. equation 8 and G (cid:0) ̃xv, {Vu,v−1}u∈U , {V π if Rv−1 + ∆f ̃xv ≥ ρ (cid:0)Rπ u,v}u∈U\n\nu,v}u ∈ U(cid:1) in Eqn. equation 14\n\n(cid:1)(cid:1) − B then\n\nv + G (cid:0) ̃xv, {Vu,v−1}u∈U , {V π //Follow the ML action\n\nSelect xv = ̃xv.\n\n5: 6: 7: 8: 9: 10: 11:\n\n12:\n\nSelect xv = xπ v .\n\n//Follow the expert\n\nend if Update assignment and reward: Vxv,v = Vxv,v−1\n\n(cid:83){v} and Rv = (cid:80)\n\nu∈U fu(Vu,v)\n\nelse\n\n13: 14: 15: 16: 17: 18: 19: end for\n\nv + 1, v + 2, · · · :\n\nG (cid:0) ̃xv, {Vu,v−1}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1) =\n\n\n\n(cid:88)\n\nu∈U\n\n max\n\ni=1,··· ,cu\n\ni (cid:88)\n\n(wu,eu,j − wu,eπ\n\nu,j\n\n\n\n+\n\n)\n\n\n\n,\n\n(7)\n\nj=1\n\nwhere eπ Vu,v−1\n\nu,j ∈ E π (cid:83){v} if ̃xv = u.\n\nu (V π\n\nu,v), and eu,j ∈ Eu( ̃Vu,v) in which ̃Vu,v = Vu,v−1 if ̃xv ̸= u and ̃Vu,v =\n\nThe interpretation is as follows. Suppose that LOMAR follows the RL decision for online item v. If it has a higher cumulative reward for the j-th item in the top-cu item set Eu,v than the expert algorithm π, then the expert can still possibly offset the reward difference wu,eu,j − wu,eπ by receiving a highreward future online item that replaces the j-th item for both LOMAR and the expert. Nonetheless, in the free-disposal model, the items in the top-cu set Eu,v are removed sequentially — the lowest-reward item will be first removed from the sorted set Eu,v, followed by the next lowest-reward item, and so on. Thus, in order for a high-reward item to replace the i-th item in the sorted set Eu,v, the first (i − 1) items have to be removed first by other high-reward online items. As a result, if LOMAR has a lower reward for the j-th item (for j ≤ i) in the top-cu item set Eu,v than the expert algorithm π, then it will negatively impact the expert’s additional reward gain in the future. Therefore, for item u ∈ U we only\n\nu,j\n\nneed to find the highest total reward difference, , that can be offset for the expert algorithm π by considering that i items are replaced by future high-reward (cid:80)i online items for i = 1, · · · , cu. If maxi=1,··· ,cu ) is negative (i.e., the expert algorithm cannot possibly gain higher rewards than LOMAR by receiving high-reward online items to replace its existing ones), then we use 0 as the hedging reward.\n\nj=1(wu,eu,j − wu,eπ\n\nj=1(wu,eu,j − wu,eπ\n\nmaxi=1,··· ,cu\n\nu,j\n\nu,j\n\n(cid:16)\n\n(cid:80)i\n\n(cid:17)+ )\n\nFinally, by summing up the hedging rewards for all the offline items u ∈ U, we obtain the total hedging reward in Eqn. equation 7. Based on this hedging reward, we have the condition (Line 28 in Algorithm 1 for LOMAR to follow the RL decision: Rv−1 + ∆f ̃xv ≥ (cid:1)(cid:1)−B, where ∆f ̃xv defined below is the additional reward ρ (cid:0)Rπ that would be obtained by following the RL decision:\n\nv + G (cid:0) ̃xv, {Vu,v−1}u∈U , {V π\n\nu,v}u∈U\n\n∆f ̃xv = f ̃xv (V ̃xv,v\n\n(cid:91)\n\n{v}) − f ̃xv (V ̃xv,v−1),\n\n(8)\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 3 Training LOMAR with Online Switching (No Free Disposal) Input: Available history information {Iu | u ∈ U}; set of weights for the online item arrival v , expert’s set of items u,v; up to the online node v − 1, the actual cumulative reward Rv−1, the\n\n{wuv, u ∈ U}; up to the online item v, the expert’s cumulative reward Rπ matched to u ∈ U is V π actual set of items matched to u ∈ U is Vu,v−1;\n\nsu = wuv − hθ(Iu, wuv)\n\n1: for u in Ua do 2: 3: end for 4: Calculate the probabilities of selecting an available item or skip:\n\n{{ ̃su}u∈Ua , ̃sskip} = softmax {{su}u∈Ua , 0}.\n\n5: Obtain the RL action: ̃xv = arg maxUa 6: Calculate Rdif f based on the switching condition\n\n(cid:83){skip} {{ ̃su}u∈Ua , ̃sskip}.\n\n(cid:16)\n\nRdif f = Rv−1 + w ̃xv,v − ρ\n\nRπ\n\nv + (cid:80)\n\nu∈U\n\n(cid:0)|Vu,v−1| − |V π\n\nu,v| + Iu= ̃xv\n\n(cid:1)+\n\n· wu,max\n\n7: if xπ\n\nv ∈ Ua then The probability of each action is\n\n8:\n\n(cid:110)\n\n{ ̃sπ\n\nu}u∈Ua , ̃sπ\n\nskip\n\n9: else\n\n10:\n\nThe probability of each action is\n\n(cid:110)\n\n{ ̃sπ\n\nu}u∈Ua, ̃sπ\n\nskip\n\n(cid:111)\n\n(cid:111)\n\n= {{1}u=xπ\n\nv\n\n, {0}u̸=xπ\n\nv ,skip}\n\n= {{0}u∈Ua, 1}\n\n(cid:17)\n\n+ B\n\n11: end if 12: With online switching, the probabilities of following RL or expert are\n\n{ ̃sOS, ̃sπ\n\nOS} = softmax{Rdif f /t, 0}\n\n13: Calibrated probabilities of choosing an available offline item u or skip are (cid:111) .\n\n{{ˆsu}u∈Ua , ˆsskip} = ̃sOS · {{ ̃su}u∈Ua , ̃sskip} + ̃sπ\n\nu}u∈Ua , ̃sπ\n\n{ ̃sπ\n\nOS ·\n\nskip\n\n(cid:110)\n\nin which fu = fu(V ′) = maxS∈V ′,|S|≤cu v∈S wuv is the reward function for an offline item u ∈ U in the free-disposal model. The condition means that if LOMAR can maintain the competitive ratio ρ against the expert algorithm π by being able to hedge against any future uncertainties even in the worst case, then it can safely follow the RL decision ̃xv at step v.\n\n(cid:80)\n\nTraining with free disposal. The training process for the free-disposal setting is the same as that for the no-free-disposal setting, except for Line 6 of Algorithm 3 in which we need to modify Rdif f based on the switching condition (i.e., Line 13 of Algorithm 2) for the free-disposal setting.\n\nC PROOF OF THEOREM 4.1\n\nThe key idea of proving Theorem 4.1 is to show that there always exist feasible actions (either following the expert or skip) while being able to guarantee the robustness if we follow the switching condition. Next, we prove Theorem 4.1 for the no-free-disposal and free-disposal settings, respectively.\n\nC.1 NO FREE DISPOSAL\n\nDenote Vu,v as the actual set of items matched to u ∈ U after making decision for v. Denote V π the expert’s set of items matched to u ∈ U. We first prove a technical lemma.\n\nu,v as\n\n(cid:16)\n\nLemma C.1. Assuming that the robustness condition is met after making the decision for v − 1,\n\nRπ\n\nv−1 + (cid:80)\n\ni.e. Rv−1 ≥ ρ arrives and the expert’s decision xπ (cid:0)|Vu,v| − |V π\n\nRv ≥ ρ\n\nv + (cid:80)\n\nRπ\n\nu∈U\n\nu,v−1|(cid:1)+\n\n(cid:0)|Vu,v−1| − |V π If at the step when v v is not available for matching, then xv = skip always satisfies u,v|(cid:1)+\n\n· wu,max\n\n· wu,max\n\n− B.\n\n− B.\n\n(cid:16)\n\n(cid:17)\n\nu∈U\n\n(cid:17)\n\nProof. If the item xπ which means |Vxπ xv = skip, we have Rv = Rv−1 and Vu,v = Vu,v−1,\n\nv is not available for matching, it must have been consumed before v arrives, v either). Since ∀u ∈ U. Then, by the robustness assumption\n\nv ,v−1| ≥ 1 (since otherwise the expert cannot choose xpi\n\nv ,v−1| − |V π\n\nxπ\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nof the previous step, we have\n\n(cid:32)\n\nRv = Rv−1 ≥ρ\n\nRπ\n\nv−1 +\n\n(cid:88)\n\nu∈U\n\n(cid:32)\n\n(cid:0)|Vu,v−1| − |V π\n\nu,v−1|(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n− B\n\n≥ρ\n\nRπ\n\nv−1 + wxπ\n\nv ,v − wxπ\n\nv ,max +\n\n(cid:0)|Vu,v−1| − |V π\n\nu,v−1|(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n− B\n\n(9)\n\n(cid:88)\n\nu∈U\n\n(cid:32)\n\n=ρ\n\nRπ\n\nv +\n\n(cid:88)\n\nu∈U\n\n(cid:0)|Vu,v| − |V π\n\nu,v|(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n− B\n\nwhere the last equality holds because (|Vu,v| − |V π and (|Vu,v| − |V π\n\nu,v|)+ − (|Vu,v−1| − |V π\n\nu,v−1|)+ = 0 otherwise.\n\nu,v|)+ − (|Vu,v−1| − |V π\n\nu,v−1|)+ = −1 if u = xπ v ,\n\nWe next prove by induction that the condition\n\n(cid:32)\n\nRv ≥ ρ\n\nRπ\n\nv +\n\n(cid:88)\n\nu∈U\n\n(cid:0)|Vu,v| − |V π\n\nu,v|(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n− B\n\n(10)\n\nholds for all steps by Algorithm 1.\n\n(cid:1) − B, we select the RL At the first step, if ̃xv is not the same as xπ decision xv = ̃xv, and the robustness condition equation 10 is satisfied. Otherwise, we select the expert action xv = xπ v ,v − B holds when ρ ≤ 1 and B ≥ 0.\n\nv and the condition still holds since Av = Cv and wxv,v ≥ ρwxπ\n\nv and w ̃xv,v ≥ ρ (cid:0)wxπ\n\nv ,v + wu,max\n\nThen, assuming that the robustness condition in equation 10 is satisfied after making the decision for v − 1, we need to prove it is also satisfied after making the decision for v. If the condition in equation 3 in Algorithm 1 is satisfied, then xv = ̃xv and so equation 10 holds naturally. Otherwise, if the expert action xπ v . Then, we ∀u ∈ U, and hence the have wxv,v ≥ ρwxπ condition equation 10 still holds. Other than these two cases, we also have the option to “skip”, i.e. xv = skip. By Lemma C.1, the condition equation 10 still holds. Therefore, we prove that the condition equation 10 holds for every step.\n\nv is available for matching, then we select expert action xv = xπ u,v| = |Vu,v−1| − |V π\n\nv ,v − B and |Vu,v| − |V π\n\nu,v−1|,\n\nAfter the last step v = |V|, we must have\n\n(cid:32)\n\nRv ≥ ρ\n\nRπ\n\nv +\n\n(cid:88)\n\nu∈U\n\n(cid:0)|Vu,ˆv| − |V π\n\nu,ˆv|(cid:1)+\n\n· wu,max\n\n(cid:33)\n\n− B ≥ ρRπ\n\nv − B\n\n(11)\n\nwhere Rv and Rπ v = |V|, respectively. This completes the proof for the no-free-disposal case.\n\nv are the total rewards of LOMAR and the expert algorithm π after the last step\n\nC.2 WITH FREE DISPOSAL\n\nWe now turn to the free-disposal setting which is more challenging than the no-free-disposal setting because of the possibility of using future high-reward items to replace existing low-reward ones.\n\nWe first denote ∆fxπ\n\nv as the actual additional reward obtained by following the expert’s decision xπ v ,\n\n∆fxπ\n\nv\n\n= fxπ\n\nv\n\n(Vxπ\n\nv ,v\n\n(cid:91)\n\n{v}) − fxπ\n\n(Vxπ\n\nv ,v−1),\n\nv\n\nAdditionally, we denote ∆f π xπ v\n\nas the expert’s additional reward of choosing xπ\n\nv , where\n\n∆f π xπ v\n\n= fxπ\n\nv\n\n(V π xπ\n\nv ,v\n\n(cid:91)\n\n{v}) − fxπ\n\nv\n\n(V π xπ\n\nv ,v−1).\n\nFor presentation convenience, we rewrite the hedging reward as ̃G (cid:0){Vu,v}u∈U , {V π u,v}u∈U \n\n\n\n+\n\n ̃G (cid:0){Vu,v}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1) =\n\n(cid:88)\n\nu∈U\n\n max\n\ni=1,··· ,cu\n\ni (cid:88)\n\nj=1\n\n(wu,eu,j − wu,eπ\n\nu,j\n\n) \n\n,\n\n(14)\n\n(12)\n\n(13)\n\n(cid:1) as\n\nwhere eπ\n\nu,j ∈ E π\n\nu (V π\n\nu,v), eu,j ∈ Eu(Vu,v), and Eu is defined in Eqn. equation 6.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nLemma C.2. Assuming that the robustness condition is met after making the decision for v − 1, i.e.\n\n(cid:16)\n\nRv−1 ≥ ρ G (cid:0)xπ\n\nv−1 + ̃G (cid:0){Vu,v−1}u∈U , {V π u,v}u∈U\n\nu,v−1}u∈U (cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nv , {Vu,v−1}u∈U , {V π\n\nRπ\n\n(cid:1)(cid:17)\n\nu,v−1}u∈U\n\n(cid:1).\n\n− B. At step v, we have ∆fxπ\n\n− ∆f π xπ v\n\n≥\n\nv\n\nProof. We begin with “G (cid:0)xπ Lemma C.2. By definition, it can be written as\n\nv , {Vu,v−1}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v−1}u∈U\n\n(cid:1)” in\n\nv , {Vu,v−1}u∈U , {V π\n\nu,v}u∈U\n\nG (cid:0)xπ \n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π \n\n\n\n+\n\nu,v−1}u∈U\n\n(cid:1)\n\n=\n\n max\n\ni=1,··· ,cu\n\n(wu,ˆeu,j − wu,ˆeπ\n\nu,j\n\n) \n\n−\n\n max\n\ni=1,··· ,cu\n\n(wu,eu,j − wu,eπ\n\nu,j\n\ni (cid:88)\n\nj=1\n\ni (cid:88)\n\nj=1\n\n\n\n+\n\n) \n\n(15)\n\nwhere u = xπ E π\n\nu (V π\n\nu,v−1), and eu,j ∈ Eu(Vu,v−1).\n\nv , ˆeπ\n\nu,j ∈ E π\n\nu (V π\n\nu,v−1\n\n(cid:83){v}), and ˆeu,j ∈ Eu(Vu,v−1\n\n(cid:83){v}). Besides, eπ\n\nu,j ∈\n\nTo prove the lemma, we consider four possible cases for wu,v to cover all the cases.\n\n(cid:83){v}) and v /∈ Eu(V π\n\nCase 1: If the reward for v is small enough such that wu,v < wu,eu,1 and wu,v < wu,eπ u,1 , then = 0, since both the v /∈ Eu(Vu,v−1 expert and LOMAR cannot gain any reward from the online item v. From Eqn. equation 15, we can find that the right-hand side is also 0. Therefore, the conclusion in Lemma C.2 holds with the equality activated.\n\n(cid:83){v}). Then we have ∆fxπ\n\n= ∆f π xπ v\n\nu,v−1\n\nv\n\nu,v−1}u∈U )+ ≥ wu,eu,1 − wu,eπ\n\nCase 2: If the reward for v is large enough such that wu,v > wu,eu,1 and wu,v > wu,eπ v ∈ Eu(Vu,v−1 reward item eu,1 /∈ Eu(Vu,v−1\n\nu,1, then (cid:83){v}). In other words, we will remove the smallestu,1 /∈ Eu(V π (cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\n(cid:83){v}) and v ∈ Eu(V π\n\n(cid:1) ≤ −wu,eu,1 + wu,eπ\n\nv , {Vu,v−1}u∈U , {V π\n\n(cid:83){v}) and eπ\n\n(cid:83){v}). Then\n\nu,v}u∈U\n\nG (cid:0)xπ\n\nu,v−1\n\nu,v−1\n\nu,1\n\nThe inequality holds because (wu,eu,1 − wu,eπ wu,v − wu,eu,1 and ∆f π\n\n= wu,v − wu,eπ\n\nu,1\n\nxπ v\n\nu,1. Therefore, the conclusion in Lemma C.2 holds.\n\nu,1. In this case, ∆fxπ\n\nv\n\n=\n\nu,v−1\n\n(cid:83){v}) u,1 , then v ∈ Eu(Vu,v−1 Case 3: If the reward for v satisfies wu,v ≥ wu,eu,1 and wu,v ≤ wu,eπ (cid:83){v}) (i.e., the online item v (cid:83){v}). In other words, even if v ∈ Eu(Vu,v−1 and v /∈ Eu(V π produces additional rewards for LOMAR), the reward of v is still smaller than the smallest reward for (cid:1) = the expert. Then, we have G (cid:0)xπ 0. In this case, ∆fxπ = 0. Therefore, the conclusion in Lemma C.2 still holds.\n\nv , {Vu,v−1}u∈U , {V π = wu,v − wu,eu,1 ≥ 0 and ∆f π\n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v}u∈U xπ v\n\nu,v−1}u∈U\n\nv\n\nCase 4: If the reward for v satisfies wu,v ≤ wu,eu,1 and wu,v ≥ wu,eπ u,1, then in this case, only the current smallest-reward item is replaced with v for the expert, while the reward of LOMAR remains unchanged. Thus, we have\n\nG (cid:0)xπ\n\nv , {Vu,v−1}u∈U , {V π = 0 and ∆f π xπ v\n\nv\n\nIn this case, ∆fxπ with the equality activated.\n\nu,v}u∈U\n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v−1}u∈U\n\n(cid:1) = wu,eπ\n\nu,1\n\n− wu,v.\n\n= wu,v − wu,eπ\n\nu,1. Then the conclusion in Lemma C.2 still holds\n\nWe next prove by induction that the condition\n\n(cid:16)\n\nRv ≥ ρ\n\nRπ\n\nv + ̃G (cid:0){Vu,v}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1)(cid:17)\n\n− B\n\n(16)\n\nholds for all steps by Algorithm 1.\n\n(cid:1) = 0, and it At the first step, by using xv = xπ is obvious that the condition in equation 16 is satisfied. Thus, there is at least one solution xv = xπ for our robustness condition in equation 16.\n\nv and ̃G (cid:0){Vu,v}u∈U , {V π\n\nv , we have Rv = Rπ\n\nu,v}u∈U\n\nv\n\nStarting from the second step, assume that after the step v − 1, we already have\n\n(cid:16)\n\nRv−1 ≥ ρ\n\nRπ\n\nv−1 + ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v−1}u∈U\n\n(cid:1)(cid:17)\n\n− B\n\n(17)\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nIf the condition in Line 28 of Algorithm 1 is already satisfied, we can just use xv = ̃xv, which directly satisfies equation 16. Otherwise, we need to follow the expert by setting xv = xπ v . We prove xv = xπ\n\nv satisfies the robustness condition at any step v.\n\nFrom Lemma C.2, since 0 ≤ ρ ≤ 1 and ∆fxπ\n\nv\n\n≥ 0 we have\n\n∆fxπ\n\nv\n\n≥ ρ\n\n(cid:16)\n\n∆f π xπ v\n\n+ G (cid:0)xπ\n\nv , {Vu,v−1}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v−1}u∈U\n\n(cid:1)(cid:17)\n\n.\n\nThen, by substituting it back to Eqn. equation 17, we have\n\nRv−1 + ∆fxπ\n\nv\n\n≥ρ\n\nu,v}u∈U\n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v−1}u∈U\n\n(cid:1)(cid:17)\n\n(cid:16)\n\n∆f π xπ v\n\n(cid:16)\n\n+ ρ\n\nRπ\n\n+ G (cid:0)xπ\n\nv , {Vu,v−1}u∈U , {V π v−1 + ̃G (cid:0){Vu,v−1}u∈U , {V π + G (cid:0)xπ\n\n(cid:16)\n\n(cid:16)\n\n=ρ\n\n=ρ\n\nRπ\n\nxπ v\n\nv−1 + ∆f π v + ̃G (cid:0){Vu,v}u∈U , {V π\n\nv , {Vu,v−1}u∈U , {V π (cid:1)(cid:17)\n\nu,v}u∈U\n\n− B.\n\nRπ\n\nu,v}u∈U\n\nu,v−1}u∈U\n\n(cid:1)(cid:17)\n\n− B\n\n(cid:1)(cid:17)\n\n− B\n\n(18)\n\nTherefore, after the last step v, LOMAR must satisfy\n\n(cid:16)\n\nRv ≥ ρ\n\nRπ\n\nv + ̃G (cid:0){Vu,v}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1)(cid:17)\n\n− B ≥ ρRπ\n\nv − B,\n\nwhere Rv and Rπ v = |V|, respectively. Thus, we complete the proof for the free-disposal setting.\n\nv are the total rewards of LOMAR and the expert algorithm π after the last step\n\n22",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a RL-based approach for online weighted bipartite matching. A key novelty of the paper is to augment the decisions of the RL agent with a classic online algorithm to obtain robustness guarantees. Upon the arrival of an online vertex, the algorithm queries both a base online algorithm (called expert) and the RL agent. It follows the action of the RL agent unless doing so has the possibility that the algorithm incurs significant loss with respect to the expert.\n\n# Strength And Weaknesses\n\nStrengths:\n+ Unlike most work on learning augmented algorithms that aim to augment online algorithms with some predictions / advice from ML models; the paper takes the complementary view of an online algorithm used as a safety railing to guide a ML model.\n\nWeaknesses:\n- The model feels too brittle - in the general case when the edge weights are unknown - the algorithm essentially only follows the decisions of the expert and can not use the RL agent at all. Maybe a restriction to a more specific problem would yield better insights.\n\n- It’s not clear what the focus of the paper is. Section 4 is concerned with how to use the RL agent’s decisions while maintaining robustness against a fixed expert. It would be better to restructure this as a meta-algorithm that takes in two algorithms A (expert) and B (RL agent): The meta algorithm switches between the actions of A and B and guarantees to maintain a given robustness wrt A while maximizing the number of times it uses actions of B. (Note that this section is completely independent of RL). Indeed, viewed in this light, the meta-algorithm and its analysis is almost trivial. [ Also worth referring to “combining” algorithms for paging and metrical task systems (e.g. Fiat et al)]. \n\n- Section 5 then deals with the challenges of training the RL agent when used along with the switching algorithm above. I would much rather see more time and space allotted to this section and clarify the training process in more details.\n\n- The empirical section includes preliminary experiments and does not demonstrate strong positive results.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI found the paper a bit hard to read and follow - as I mentioned earlier in the review, in my view, the paper focused on the wrong aspects of the work. The switching framework itself is not particularly novel within the online algorithms and is indeed the first thing one would think of when faced with such a question.\n\n# Summary Of The Review\n\nThe paper proposes a framework to use an expert algorithm to guide the decisions of an RL agent for online bipartite matching. I like the problem direction but find the current paper version a bit lacking with respect to details of (i) RL training procedure, (ii) Difficulty in naively adapting standard RL techniques, (iii) thorough empirical evaluation.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nENHANCED TEMPORAL KNOWLEDGE EMBEDDINGS WITH CONTEXTUALIZED LANGUAGE REPRESENTATIONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWorld knowledge exists in both structured (tables, knowledge graphs) and unstructured forms (texts). Recently, there have been extensive research efforts in the integration of structured factual knowledge and unstructured textual knowledge. However, most studies focus on incorporating static factual knowledge into pre-trained language models, while there is less work on enhancing temporal knowledge graph embedding using textual knowledge. Existing integration approaches can not apply to temporal knowledge graphs (tKGs) since they often assume knowledge embedding is time-invariant. In fact, the entity embedding in tKG embedding models usually evolves over time, which poses the challenge of aligning temporally relevant textual information with entities. To this end, we propose Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), which uses tKG quadruple as an implicit measure to temporally align textual data and the time-evolving entity representations and uses a novel knowledge-text prediction task to inject textual information into temporal knowledge embedding. ECOLA jointly optimizes the knowledge-text prediction objective and the temporal knowledge embedding objective, and thus, can simultaneously take full advantage of textual and structured knowledge. Since existing datasets do not provide tKGs with aligned textual data, we introduce three new datasets for training and evaluating ECOLA. Experimental results on the temporal knowledge graph completion task show that ECOLA outperforms state-of-the-art tKG embedding models by a large margin.\n\n1\n\nINTRODUCTION\n\nKnowledge graphs (KGs) have long been considered an effective and efficient way to store structural knowledge about the world. A knowledge graph consists of a collection of triples ps, p, oq, where s (subject entity) and o (object entity) correspond to nodes and p (predicate) indicates the edge type (relation) between the two entities. Common knowledge graphs (Toutanova et al., 2015; Dettmers et al., 2018) assume that the relations between entities are static connections. However, in the real world, there are not only static facts and properties but also time-evolving relations associated with the entities. For example, the political relationship between two countries might worsen because of trade fights. To this end, temporal knowledge graphs (tKGs) (Tresp et al., 2015) were introduced that capture temporal aspects of relations by extending a triple to a quadruple, which adds a timestamp or time interval to describe when the relation is valid, e.g. (Argentina, deep comprehensive strategic partnership with, China, 2022). Extensive studies have been focusing on learning temporal knowledge embedding (Leblay & Chekol, 2018; Han et al., 2020c), which not only helps infer missing links in tKGs but also benefits various knowledge-related downstream applications, such as temporal question answering (Saxena et al., 2021b).\n\nHowever, knowledge graph embedding often suffers from the sparseness of knowledge graphs. For example, the tKG model proposed by Han et al. (2020a) performs much better on the dense tKG than the sparse one. To address this problem, some recent studies incorporate textual information to enrich knowledge embedding. KEPLER (Wang et al., 2021) learns the representation of an entity by encoding the entity description with a pre-trained language model (PLM) and optimizing the knowledge embedding objective. KG-Bert (Yao et al., 2019) takes entity and relation descriptions of\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: An example of a temporal knowledge graph with textual event descriptions.\n\na triple as the input of a PLM and turns knowledge graph completion into a sequence classification problem. However, they do not take the temporal nature and the evolutionary dynamics of knowledge graphs into account. In tKG embedding models, the entity representations usually evolve over time as they involve in different events at different timestamps. Taking financial crises as an example, companies are more likely involved in events such as laying off employees. But when the economy recovers, companies hire staff again rather than cut jobs. Thus, the entities should also be able to drift their representations over time to manage the changes. Therefore, given an entity, it should be taken into account which textual knowledge is relevant to it at which timestamp. We name this challenge as temporal alignment between texts and tKG, which is to establish a correspondence between textual knowledge and their temporal knowledge graph depiction. This is one of the challenges that existing approaches cannot handle due to their limitation of assuming knowledge embedding is static and using the time-invariant description of an entity to enhance its representation. Thus, they are not appropriate in temporal knowledge graph scenarios where temporal alignment is required. The other challenge is that temporal knowledge embedding models learn the entity representations as a function of time, which exposes another limitation of existing approaches that their architectures cannot be naturally combined with tKG models. Therefore, it is not clear how to enhance temporal knowledge embedding with textual data.\n\nTo this end, we propose Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), which uses temporally relevant textual knowledge to enhance the timedependent knowledge graph embedding and ensures that the enhanced knowledge embedding preserves the temporal nature. Specifically, we solve the temporal alignment challenge by using tKG quadruples as an implicit measure. We pair a quadruple with its relevant textual data, e.g., event descriptions, which corresponds to the temporal relations between entities at a specific time. Then we use the event description to enhance the representations of entities and predicate involved in the given quadruple. In particular, we encode entities and predicates by tKG embedding models and encode texts using token embedding . Given a quadruple-text pair, we concatenate the embedding of entities, predicate, and textual tokens and feed them into a pre-trained language model. We introduce a novel knowledge-text prediction (KTP) task to inject textual knowledge into temporal knowledge embedding. The KTP task is an extended masked language modeling task, which randomly masks words in texts and entity/predicates in quadruples. With the help of the KTP task, ECOLA would be able to recognize mentions of the subject entity and the object entity and align semantic relationships in the text with the predicate in the quadruple. Thus, the model can take full advantage of the abundant information from the textual data, which is especially helpful for embedding entities and predicates that only appear in a few quadruples. ECOLA jointly optimizes the knowledge-text prediction and temporal knowledge embedding objectives. Since our goal is to develop an approach that can generally improve any potential tKG models, we combine the model with different benchmark tKG embedding models (Goel et al., 2020; Han et al., 2020c; 2021). For training ECOLA, we need datasets with temporal KG quadruples and aligned textual event descriptions, which is unavailable in existing temporal KG benchmark datasets. Thus, we construct three new temporal knowledge graph datasets by adapting two existing datasets, i.e., GDELT (Leetaru & Schrodt, 2013) and Wiki (Dasgupta et al., 2018), and an event extraction dataset (Li et al., 2020). To make a fair comparison with other temporal KG embedding models and keep fast inference, we only take the enhanced\n\n2\n\nMetaAyfer OzgurTikTokInstagramRelease 2021-08-19Sue 2019-6-12 Acquire 2012-04-09 Give awards to 2021-08-18The Meta Foundation launched a research award, which was officially announced on Aug. 18. Ayfer Ozgur is one of the winners. Today is October 24. I didn't expect Meta to acquire Instagram, a fun, popular photo-sharing app for  mobile devices.On Aug 19, the documentary film titled “The Outsider\" ... is officially released by Meta. On June 12, it was reported that Meta added six new lawsuits against TikTok in Court... The OutsiderUnder review as a conference paper at ICLR 2023\n\ntemporal knowledge embedding to perform the temporal KG completion task at test time but do not use any textual descriptions of test quadruples.\n\nTo summarize, our contributions are as follows: (i) We propose ECOLA that enhances temporal knowledge graph representation models with textual knowledge via pre-trained language models. ECOLA shows its superiority on the temporal KG completion task and can be potentially combined with any temporal KG embedding model. (ii) We are the first to address the challenge of enhancing temporal knowledge embedding with temporally relevant textual information while preserving the time-evolving properties of entity embedding. (iii) To train the integration models, we construct three datasets, which align each quadruple with a relevant textual description, by adapting three existing temporal KG completion datasets. Extensive experiments show that ECOLA is model-agnostic and enhances temporal KG embedding models with up to 287% relative improvements in the Hits@1 metric.\n\n2 PRELIMINARIES AND RELATED WORK\n\nTemporal Knowledge Graphs Temporal knowledge graphs are multi-relational, directed graphs with labeled timestamped edges between entities (nodes). Let E and P represent a finite set of entities and predicates, respectively. tKG contains a collection of events written as quadruples. A quadruple q “ pes, p, eo, tq represents a timestamped and labeled edge between a subject entity es P E and an object entity eo P E at a timestamp t P T . Let F represents the set of all true quadruples, i.e., real events in the world, the temporal knowledge graph completion (tKGC) is the task of inferring F based on a set of observed facts O, which is a subset of F. Specifically, tKGC is to predict either a missing subject entity p?, p, eo, tq given the other three components or a missing object entity pes, p, ?, tq. We provide related works on temporal knowledge representations in Appendix A.\n\nJoint Language and Knowledge Models Recent studies have achieved great success in jointly learning language and knowledge representations. Yamada et al. (2016) and Ganea & Hofmann (2017) use entity linking to map entities and words into the same representation space. Inspired by the success of contextualized language representation, Zhang et al. (2019) and Peters et al. (2019) focus on enhancing language models with external knowledge. They separately pre-train the entity embedding with knowledge embedding models, e.g., TransE Bordes et al. (2013), and inject the pre-trained entity embedding into PLMs, while fixing the entity embedding during training PLMs. Thus, they are not real joint models for learning the knowledge embedding and language embedding simultaneously. Yao et al. (2019), Kim et al. (2020), and Wang et al. (2021) learn to generate entity embeddings with pre-trained language models (PLMs) from entity descriptions. Moreover, He et al. (2019), Sun et al. (2020), and Liu et al. (2020) exploit the potential of contextualized knowledge representation by constructing subgraphs of structured knowledge and textual data instead of treating single triples as training units. Nevertheless, none of these works consider the temporal aspect of knowledge graphs, which makes them different from our proposed ECOLA.\n\n3 ECOLA\n\nIn this section, we present the overall framework of ECOLA, including the model architecture in Section 3.1 - 3.3, a novel training task designed for aligning knowledge embedding and language representation in Section 3.4, and the training procedure in Section 3.5. As shown in Figure 2, ECOLA implicitly incorporates contextualized language representations into temporal knowledge embeddings by jointly optimizing the knowledge-text prediction loss and the temporal knowledge embedding loss. Note that, at inference time, we only take the enhanced temporal knowledge embeddings to perform the temporal KG completion task without using any textual data for preventing information leakage and keep fast inference speed.\n\n3.1 EMBEDDING LAYER\n\nIn tKG embedding models, entity representations evolve over time. Thus, the key point of enhancing a time-dependent entity representation eiptq is to find texts that are relevant to the entity at the time of interest t. To this end, we use tKG quadruples (e.g., pei, p, ej, tq) as an implicit measure for the alignment. We pair a quadruple with its relevant textual data and use such textual data to enhance\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Model architecture. ECOLA jointly optimizes the knowledge-text prediction (KTP) objective and the temporal knowledge embedding (tKE) objective.\n\nFigure 3: ECOLA input representations. Following textual tokens, the last four tokens in an input correspond to a quadruple from temporal knowledge graph. The last one is the timestamp t for incorporating temporal information into entity representation eiptq. Here, w denotes subword token embedding, e and p denote entity and predicate embedding, respectively.\n\nthe entity representation eiptq. Therefore, a training sample is a pair of a quadruple from temporal KGs and its corresponding textual description, which are packed together into a sequence. As shown in Figure 3, the input embeddings are the sum of token embedding, type embedding, and position embedding. For token embedding, we maintain three lookup tables for subwords, entities, and predicates, respectively. For subword embedding, we first tokenize the textual description into a sequence of subwords following Bert (Devlin et al., 2018) and use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. As the yellow tokens shown in the Figure 3, We denote a embedding sequence of subword tokens as tw1, ..., wnu. In contrast to subword embedding, the embeddings for entities and predicates are directly learned from scratch, similar to common knowledge embedding methods. We denote the entity embedding and predicate embedding as e and p, respectively, as the blue tokens shown in Figure 3. We separate the knowledge tokens, i.e., entities and predicates, and subword tokens with a special token [SEP]. To handle different token types, we add type embedding to indicate the type of each token, i.e., subword, entity, and predicate. For position embedding, we assign each token an index according to its position in the input sequence and follow Devlin et al. (2018) to apply fully-learnable absolute position embeddings.\n\n3.2 TEMPORAL KNOWLEDGE ENCODER\n\nAs shown in Figure 3, the input embedding for entities and predicates consists of knowledge token embedding, type embedding, and position embedding. In this section, we provide details of the temporal knowledge embedding objective.\n\nA temporal embedding function defines entity embedding as a function that takes an entity and a timestamp as input and generates a time-dependent representation. There is a line of work exploring temporal embedding functions. Since we aim to propose a model-agnostic approach, we combine ECOLA with three temporal embedding functions, i.e., DyERNIE-Euclid (Han et al., 2020c), UTEE\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(Han et al., 2021), and DE-SimplE (Goel et al., 2020). In the following, we refer to DyERNIEEuclid as DyERNIE and take it as an example to introduce our framework. Specifically, the entity representation is derived from an initial embedding and a velocity vector\n\neDyER\n\ni\n\nptq “ ̄eDyER\n\ni\n\n` veit,\n\ni\n\nwhere ̄eDyER represents the initial embedding that does not change over time, and vei is an entityspecific velocity vector. The combination with other temporal embedding functions are discussed in Section 4. The score function measuring the plausibility of a quadruple is defined as follows,\n\nφDyERpei, p, ej, tq “ ́dpP d eDyER\n\ni\n\nptq, eDyER\n\nj\n\nptq ` pq ` bi ` bj,\n\n(1)\n\nwhere P and p represent the diagonal predicate matrix and the translation vector of predicate p, respectively, d denotes the Euclidean distance, and bi, bj are scalar biases of the subject and object ei and ej respectively. By learning tKE, we generate M negative samples for each positive quadruple in a batch. We choose the binary cross entropy as the temporal knowledge embedding objective\n\nLtKE “\n\n ́1 N\n\nNÿ\n\npyk logppkq ` p1 ́ ykq logp1 ́ pkqq,\n\n(2)\n\nk“1\n\nwhere N is the sum of positive and negative training samples, yk represents the binary label indicating whether a training sample is positive or not, pk denotes the predicted probability σpφDyER q, and σp ̈q represents the sigmoid function.\n\nk\n\n3.3 MASKED TRANSFORMER ENCODER\n\nTo encode the input sequence, we use the pre-trained contextual language representation model Bert (Devlin et al., 2018). Specifically, the encoder feeds a sequence of N tokens including entities, predicates, and subwords into the embedding layer introduced in Section 3.1 to get the input embeddings and then computes L layers of d-dimensional contextualized representations. Eventually, we get a contextualized representation for each token, which could be further used to predict masked tokens.\n\n3.4 KNOWLEDGE-TEXT PREDICTION TASK\n\nTo incorporate textual knowledge into temporal knowledge embedding, we use the pre-trained language model Bert to encode the textual description and propose a knowledge-text prediction task to align the language representations and the knowledge embedding. The knowledge-text prediction task is an extension of the masked language modeling (MLM) task. As illustrated in Figure 2, given a pair of a quadruple and the corresponding event description, the knowledge-text prediction task is to randomly mask some of the input tokens and train the model to predict the original index of the masked tokens based on their contexts. As different types of tokens are masked, we encourage ECOLA to learn different capabilities:\n\n• Masking entities. To predict an entity token in the quadruple, ECOLA has the following ways to gather information. First, the model can detect the textual mention of this entity token and determine the entity; second, if the other entity token and the predicate token are not masked, the model can utilize the available knowledge token to make a prediction, which is similar to the traditional semantic matching-based temporal KG models. Masking entity nodes helps ECOLA align the representation spaces of language and structured knowledge, and inject contextualized representations into entity embeddings.\n\n• Masking predicates. To predict the predicate token in the quadruple, the model needs to detect mentions of subject entity and object entity and classify the semantic relationship between the two entity mentions. Thus, masking predicate tokens helps the model integrate language representation into the predicate embedding and map words and entities into a common representation space.\n\n• Masking subwords. When subwords are masked, the objective is similar to traditional MLM. The difference is that ECOLA not only considers the dependency information in the text but also the entities and the logical relationship in the quadruple. Additionally, we initialize the encoder with the pre-trained BERTbase. Thus, masking subwords helps ECOLA keep linguistic knowledge and avoid catastrophic forgetting while integrating contextualized representations into temporal knowledge embeddings.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nIn each quadruple, the predicate and each entity have a probability of 15% to be masked. Similarly, we mask 15% of the subwords of the textual description at random. We ensure that entities and the predicate cannot be masked at the same time in a single training sample, where we conduct an ablation study in Section 6 to show the improvement of making this constraint. When a token is masked, we replace it with (1) the [MASK] token 80% of the time, (2) a randomly sampled token with the same type as the original token 10% of the time, (3) the unchanged token 10% of the time. For each masked token, the contextualized representation in the last layer of the encoder is used for three classification heads, which are responsible for predicting entities, predicates, and subword tokens, respectively. At last, a cross-entropy loss LKT P is calculated over these masked tokens.\n\nAlthough we focus on generating informative knowledge embeddings in this work, joint models often benefit both the language model and the temporal KG model. Unlike previous joint models (Zhang et al., 2019; Peters et al., 2019), we do not modify the Transformer encoder architecture, e.g., adding entity linkers or fusion layers. Thus, the language encoder enhanced by external knowledge can be adapted to a wide range of downstream tasks as easily as Bert. We evaluate the enhanced language model on the temporal question answering task and report the results in Appendix C.\n\n3.5 TRAINING PROCEDURE AND INFERENCE\n\n1 and the knowlWe initialize the transformer encoder with the pre-trained language model BERTbase edge encoder with random vectors. Then we use the temporal knowledge embedding (tKE) objective LtKE to train the knowledge encoder and use the knowledge-text prediction (KTP) objective LKT P to incorporate temporal factual knowledge and textual knowledge in the form of a multi-task loss:\n\nL “ LtKE ` λLKT P ,\n\nwhere λ is a hyperparameter to balance tKE loss and KTP loss. Note that those two tasks share the same embedding layer of entities and predicates. At inference time, we aim to answer link prediction queries, e.g., pes, p, ?, tq. Since there is no textual description at inference time, we take the entity and predicate embedding as input and use the score function of the knowledge encoder, e.g., Equation 1, to predict the missing links. Specifically, the score function assigns a plausibility score to each quadruple, and the proper object can be inferred by ranking the scores of all quadruples tpes, p, ej, tq, ej P Eu that are accompanied with candidate entities.\n\n4 MODEL VARIANTS\n\nECOLA is model-agnostic and can enhance different temporal knowledge embedding models. Besides ECOLA-DyERNIE, we introduce here two additional variants of ECOLA.\n\nECOLA-DE enhances the tKG embedding model DE-SimplE, which applies the diachronic embedding (DE) function (Goel et al., 2020). DE-function defines the temporal embeddings of entity ei at timestamp t as\n\n\"\n\neDE\n\ni\n\nptqrns “\n\naeirns if 1 ď n ď γd, aeirns sinpωeirnst ` bei rnsq else.\n\n(3)\n\ni\n\nptqrns denotes the nth element of the embeddings of entity ei at time t. aei, ωei, bei P Rd Here, eDE are entity-specific vectors with learnable parameters, d is the dimensionality of the embedding, and γ P r0, 1s represents the portions of the time-independent part. Besides, it use SimplE (Kazemi & Poole, 2018) as the score function of temporal knowledge embedding.\n\nECOLA-UTEE enhances UTEE Han et al. (2021) that learns a shared temporal encoding function for all entities to deal with the overfitting problem of the diachronic approach (Goel et al., 2020) on sparse datasets. Compared to ECOLA-DE, ECOLA-UTEE replaces Equation 3 with follows:\n\neU T EE\n\ni\n\nptq “ r ̄ei||a sinpωt ` bqs,\n\n ̄ei P Rγd; a, w, b P Rp1 ́γqd\n\nwhere ̄ei denotes entity-specific time-invariant part, the amplitude vector a, frequency vector ω, and bias b are shared among all entities, || denotes the concatenation operator, and γ P r0, 1s.\n\n1https://huggingface.co/bert-base-uncased\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n5 DATASETS\n\nThe training procedure of ECOLA requires both temporal knowledge graphs and textual descriptions. Given a quadruple pes, p, eo, tq, the key point is to find texts that are temporally relevant to es and eo at t. Existing temporal KG datasets do not provide such information. To facilitate the research on integrating textual knowledge into temporal knowledge embeddings, we reformat three existing datasets, i.e., GDELT2, DuEE3, and Wiki4, for evaluating the proposed integration method. We show the statistics of the datasets in Table 2 in the appendix. Due to the limited size of upload files, we only attach DuEE and subsets of GDELT and Wiki in the supplementary material and will publish the complete version after acceptance.\n\nGDELT is an initiative knowledge base storing events across the globe connecting people and organizations, e.g., (Google, consult, the United States, 2018/01/06). For each quadruple, GDELT provides the link to the news resource which the quadruple is extracted from. We assume each sentence that contains both mentions of subject entity and object entity is relevant to the given quadruple, and thus, temporally aligned with the subject and object at the given timestamp. We pair each of these sentences with the given quadruple to form a training sample. This process is similar to the distant supervision algorithm Mintz et al. (2009) in the relation extraction task, which assumes that, given a relationship between two entities, any sentence containing these two entities would express this relation. In total, the dataset contains 5849 entities, 237 predicates, 2403 timestamps, and 943956 quadruples with accompanying sentences.\n\nDuEE is originally a human-annotated dataset for event extraction containing 65 event types and 121 argument roles. Each sample contains a sentence and several extracted event tuples. We reformat DuEE by manually converting event tuples into quadruples and then pairing the quadruples with their corresponding sentence.\n\nWiki is a temporal knowledge graph dataset proposed by Leblay & Chekol (2018), containing temporal facts from the Wikidata (Vrandeˇci ́c & Krötzsch, 2014). Different from GDELT and DuEE, time annotations in Wiki are represented as time intervals, e.g., (Savonranta, instance of, municipality of Finland, 1882 - 2009). Following the post-processing by Dasgupta et al. (2018), we discretize the time span into 82 different timestamps. We align each entity to its Wikipedia page and extract the first abstract section as its description. To construct the relevant textual data of each quadruple, we combine the subject entity description, relation, and object description into a sequence, separated by [SEP] token between two neighboring parts. In this case, the knowledge-text prediction task let subject entity see the descriptions of its neighbors at different timestamps, and thus, preserving the temporal alignment between time-dependent entity representation and textual knowledge.\n\n6 EXPERIMENTS\n\nWe evaluate the enhanced temporal knowledge embedding on the temporal KG completion task. Specifically, we take the entity and predicate embedding of ECOLA-DyERNIE and use Equation 1 to predict missing links. To make a fair comparison with other temporal KG embedding models, we do not use any textual descriptions at test time.\n\nBaselines We include both static and temporal KG embedding models. From the static KG embedding models, we use TransE (Bordes et al., 2013), DistMult (Yang et al., 2014), and SimplE (Kazemi & Poole, 2018). These methods ignore the time information. From the temporal KG embedding models, we compare our model with several state-of-the-art methods, including AiTSEE (Xu et al., 2019), TNTComplE(Lacroix et al., 2020), DyERNIE5 (Han et al., 2020c), TeRO (Xu et al., 2020), and DE-SimplE (Goel et al., 2020). We provide implementation details in Appendix D and attach the source code in the supplementary material.\n\n2https://www.gdeltproject.org/data.html#googlebigquery 3https://ai.baidu.com/broad/download 4https://www.wikidata.org/wiki/Wikidata:Main_Page 5For a fair comparison with other baselines, we choose DyERNIE-Euclid.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nEvaluation protocol For each quadruple q “ pes, p, eo, tq in the test set Gtest, we create two queries: pes, p, ?, tq and p?, p, eo, tq. For each query, the model ranks all possible entities E according to their scores. Following the filtered setting in Han et al. (2020b), we remove all entity candidates that correspond to true triples from the candidate list apart from the current test entity. Let Rankpesq and Rankpeoq represent the rank for es and eo of the two queries respectively, we evaluate our models using standard metrics across the link prediction literature: mean reciprocal rank (MRR): Rankpeoq q and Hits@kpk P t1, 3, 10uq: the percentage of times that the\n\n1 2 ̈|Gtest| true entity candidate appears in the top k of ranked candidates.\n\nRankpesq `\n\nqPGtestp\n\nř\n\n1\n\n1\n\nTable 1: Temporal link prediction results: Mean Reciprocal Rank (MRR, %) and Hits@1/3(%). The results of the proposed fusion models (in bold) and their counterpart KG models are listed together.\n\nDatasets\n\nModel\n\nTransE SimplE DistMult\n\nTeRO ATiSE TNTComplEx\n\nUTEE ECOLA-UTEE\n\nGDELT - filtered\n\nWiki - filtered\n\nDuEE - filtered\n\nMRR\n\n8.08 10.98 11.27\n\n6.59 7.00 8.93\n\nHits@1 Hits@3\n\n0.00 4.76 4.86\n\n1.75 2.48 3.60\n\n8.33 10.49 10.87\n\n5.86 6.26 8.52\n\nMRR\n\n27.25 20.75 21.40\n\n32.92 35.36 34.36\n\nHits@1 Hits@3\n\n16.09 16.77 17.54\n\n21.74 24.07 22.38\n\n33.06 23.23 23.86\n\n39.12 41.69 40.64\n\nMRR\n\n34.25 51.13 48.58\n\n54.29 53.79 57.56\n\nHits@1 Hits@3\n\n4.45 40.69 38.26\n\n39.27 42.31 43.52\n\n60.73 58.30 55.26\n\n63.16 59.92 65.99\n\n9.76\n\n4.23\n\n20.98 19.11 ̆ 15.29 ̆ 19.46 ̆ 38.35 ̆ 30.56 ̆ 42.11 ̆ 60.36 ̆ 46.55 ̆ 69.22 ̆ 00.18\n\n00.14\n\n30.39\n\n43.92\n\n00.22\n\n00.51\n\n26.96\n\n53.36\n\n00.93\n\n00.36\n\n60.52\n\n00.05\n\n00.38\n\n00.16\n\n9.77\n\nDyERNIE 23.51 ECOLA-DyERNIE 19.99 ̆ 16.40 ̆ 19.78 ̆ 41.22 ̆ 33.02 ̆ 45.00 ̆ 59.64 ̆ 46.35 ̆ 67.87 ̆ 00.04\n\n00.27\n\n25.21\n\n00.03\n\n00.18\n\n00.53\n\n57.58\n\n14.53\n\n00.20\n\n10.81\n\n00.06\n\n41.49\n\n10.72\n\n00.09\n\n00.05\n\n70.24\n\n4.24\n\nQuantitative Study Table 1 reports the tKG completion results on the test sets, which are averaged over three trials. The error bars of the ECOLA models are also provided. Firstly, we can see that ECOLA-UTEE improves its baseline temporal KG embedding model, UTEE, by a large margin, demonstrating the effectiveness of our fusing strategy. Specifically, ECOLA-UTEE enhances UTEE on GDELT with a relative improvement of 95% and 99% in terms of mean reciprocal rank (MRR) and Hits@3, even nearly four times better in terms of Hits@1. Thus, its superiority is clear on GDELT, which is the most challenging dataset among benchmark tKG datasets, containing nearly one million quadruples and more than two hundred relations. Secondly, ECOLA-UTEE and ECOLA-DE generally outperform UTEE and DE-SimplE on the three datasets, demonstrating that ECOLA is model-agnostic and able to enhance different tKG embedding models. Besides, in the DuEE dataset, ECOLA-DyERNIE achieves a better performance than DyERNIE in Hits@1 and MRR, but the gap reverses in Hits@3. The reason could be that ECOLA-DyERNIE is good at classifying hard negatives using textual knowledge, and thus has a high Hits@1; however, since DuEE is much smaller than the other two datasets, ECOLA-DyERNIE may overfit in some cases, where the ground truth is pushed away from the top 3 rank.\n\nWe compare the performance of DE-SimplE, ECOLA-DE, and ECOLA-SF on GDELT in Figure 4a. ECOLA-SF is the static counterpart of ECOLA-DE, where we do not consider the temporal alignment while incorporating textual knowledge. Specifically, ECOLA-SF integrates all textual knowledge into the time-invariant part of entity representations. We provide more details of ECOLA-SF in Appendix B. We can see ECOLA-DE significantly outperforms DE-SimplE and ECOLA-SF in terms of MRR and Hits@1. In particular, the performance gap between ECOLA-DE and ECOLA-SF is significant, demonstrating the temporal alignment between time-dependent entity representation and textual knowledge is more powerful than the static alignment.\n\nAblation study Figure 4b shows the results of different masking strategies on GDELT. The first strategy called Masking E+R+W which allows to simultaneously mask predicate, entity, and subword tokens in the same training sample. The second strategy is Masking E/R+W, where we mask 15% subword tokens in the language part, and either an entity or a predicate in the knowledge tuple. In other words, simultaneously masking an entity and a predicate in a training sample is not allowed. In the third strategy called Masking E/R/W, for each training sample, we choose to mask either subword\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\nFigure 4: Ablation Study. (a) Temporal alignment analysis. We compare De-SimplE, ECOLA-DE, and ECOLA-SF in terms of MRR(%) and Hits@1(%) on GDELT. (b) Masking strategy analysis. We compare ECOLA-DE with different masking strategies and show the results of MRR(%) and Hits@1(%) on GDELT. (c) Type embedding analysis. We compare ECOLA-DE with/without type embedding and show the results of MRR(%) and Hits@1/10(%) on GDELT.\n\ntokens, an entity, or the predicate. The experimental results show the advantage of the second masking strategy, indicating that remaining adequate information in the knowledge tuple helps the model to align the knowledge embedding and language representations. Figure 4c demonstrates the ablation study on the type embedding, which differentiates among subword tokens, entity, and predicate of the input. We can observe that removing type embedding leads to a considerable performance gap on GDELT, indicating that providing distinguishment between subwords, entities, and predicates helps the model to better understand different input components and different prediction tasks.\n\nQualitative Analysis To investigate why incorporating textual knowledge can improve the tKG embedding models’ performance, we study the test samples that have been correctly predicted by the fusion model ECOLA-DE but wrongly by the tKG model DE-SimplE. It is observed that language representations help overcome the incompleteness of the tKG by leveraging knowledge from augmented textual data. For example, there is a test quadruple (US, host a visit, ?, 19-11-14) with ground truth R.T. Erdo ̆gan. The training set contains a quite relevant quadruple, i.e., (Turkey, intend to negotiate with, US, 19-11-11). However, the given tKG does not contain information indicating that the entity R.T. Erdo ̆gan is a representative of Turkey. So it is difficult for the tKG model DE-SimplE to infer the correct answer from the above-mentioned quadruple. In ECOLA-DE, the augmented textual data do contain such information, e.g. \"The president of Turkey, R.T. Erdogan, inaugurated in Aug. 2014.\", which narrows the gap between R.T. Erdogan and Turkey. Thus, by integrating textual information into temporal knowledge embedding, the enhanced model can gain additional information which the knowledge base does not include. Another example is relevant to the entity Charles de Gaulle. To infer the test quadruple (Charles de Gaulle, citizenship of, ?, 1958) with ground truth French 5th Republic, it is noticed that in the training set of ECOLA-DE, we have quadruple (Charles de Gaulle, president of, French 4th Republic, 1957) with supporting textual data \"Charles de Gaulle was the last president of French 4th Republic, and French 5th Republic emerged from the collapse of the 4th Republic in 1958.\", which shows that the entity representation of Charles de Gaulle is enhanced by the evolving history of France and is temporally closer to French 5th Republic at the query timestamp 1958.\n\n7 CONCLUSION\n\nWe propose ECOLA to enhance temporal knowledge embedding using textual knowledge. Specifically, we enhance time-evolving entity representations with temporally relevant textual data by encoding the textual data using a pre-trained language model and introducing a novel knowledge-text prediction task to align the temporal knowledge and language representation into the same semantic space. Besides, we construct three datasets that contain paired structured temporal knowledge and unstructured textual descriptions, which can benefit future research on fusing temporal structured and unstructured knowledge. Extensive experiments show ECOLA is model-agnostic and can improve many temporal knowledge graph models by a large margin.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nReproducibility Statement About datasets, Since the size of GDELT and Wiki exceeds the limit of upload file size allowed by ICLR, we only upload DuEE, partial Wiki and partial GDELT (short version with 1000 samples) in the supplementary material due to the and will publish the complete dataset of Wiki and GDELT after acceptance. Besides, we provide the description of the data processing steps in Section 5. We provide the dataset statistics in Table 2 in appendix. Additionally, we provide our source code in the supplementary material.\n\nREFERENCES\n\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. Advances in neural information processing systems, 26, 2013.\n\nShib Sankar Dasgupta, Swayambhu Nath Ray, and Partha Talukdar. Hyte: Hyperplane-based temporally aware knowledge graph embedding. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 2001–2011, 2018.\n\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nOctavian-Eugen Ganea and Thomas Hofmann. Deep joint entity disambiguation with local neural\n\nattention. arXiv preprint arXiv:1704.04920, 2017.\n\nRishab Goel, Seyed Mehran Kazemi, Marcus Brubaker, and Pascal Poupart. Diachronic embedding for temporal knowledge graph completion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 3988–3995, 2020.\n\nZhen Han, Peng Chen, Yunpu Ma, and Volker Tresp. Explainable subgraph reasoning for forecasting on temporal knowledge graphs. In International Conference on Learning Representations, 2020a.\n\nZhen Han, Peng Chen, Yunpu Ma, and Volker Tresp. xerte: Explainable reasoning on temporal\n\nknowledge graphs for forecasting future links. arXiv preprint arXiv:2012.15537, 2020b.\n\nZhen Han, Yunpu Ma, Peng Chen, and Volker Tresp. Dyernie: Dynamic evolution of riemannian manifold embeddings for temporal knowledge graph completion. arXiv preprint arXiv:2011.03984, 2020c.\n\nZhen Han, Gengyuan Zhang, Yunpu Ma, and Volker Tresp. Time-dependent entity embedding is not all you need: A re-evaluation of temporal knowledge graph completion models under a unified framework. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8104–8118, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.639. URL https://aclanthology.org/2021.emnlp-main.639.\n\nBin He, Di Zhou, Jinghui Xiao, Qun Liu, Nicholas Jing Yuan, Tong Xu, et al. Integrating graph contextualized knowledge into pre-trained language models. arXiv preprint arXiv:1912.00147, 2019.\n\nSeyed Mehran Kazemi and David Poole. Simple embedding for link prediction in knowledge graphs.\n\nAdvances in neural information processing systems, 31, 2018.\n\nBosung Kim, Taesuk Hong, Youngjoong Ko, and Jungyun Seo. Multi-task learning for knowledge graph completion with pre-trained language models. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 1737–1743, 2020.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nTimothée Lacroix, Guillaume Obozinski, and Nicolas Usunier. Tensor decompositions for temporal\n\nknowledge base completion. arXiv preprint arXiv:2004.04926, 2020.\n\nJulien Leblay and Melisachew Wudage Chekol. Deriving validity time in knowledge graph. In\n\nCompanion Proceedings of the The Web Conference 2018, pp. 1771–1776, 2018.\n\nKalev Leetaru and Philip A Schrodt. Gdelt: Global data on events, location, and tone, 1979–2012. In\n\nISA annual convention, volume 2, pp. 1–49. Citeseer, 2013.\n\nXinyu Li, Fayuan Li, Lu Pan, Yuguang Chen, Weihua Peng, Quan Wang, Yajuan Lyu, and Yong Zhu. Duee: a large-scale dataset for chinese event extraction in real-world scenarios. In CCF International Conference on Natural Language Processing and Chinese Computing, pp. 534–545. Springer, 2020.\n\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. K-bert: Enabling language representation with knowledge graph. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 2901–2908, 2020.\n\nYunpu Ma, Volker Tresp, and Erik A Daxberger. Embedding models for episodic knowledge graphs.\n\nJournal of Web Semantics, 59:100490, 2019.\n\nMike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp. 1003–1011, 2009.\n\nMatthew E Peters, Mark Neumann, Robert L Logan IV, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A Smith. Knowledge enhanced contextual word representations. arXiv preprint arXiv:1909.04164, 2019.\n\nApoorv Saxena, Soumen Chakrabarti, and Partha Talukdar. Question answering over temporal knowledge graphs. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics, 2021a.\n\nApoorv Saxena, Soumen Chakrabarti, and Partha Talukdar. Question answering over temporal\n\nknowledge graphs. arXiv preprint arXiv:2106.01515, 2021b.\n\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing Huang, and Zheng Zhang. Colake: Contextualized language and knowledge embedding. arXiv preprint arXiv:2010.00309, 2020.\n\nKristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. Representing text for joint embedding of text and knowledge bases. In Proceedings of the 2015 conference on empirical methods in natural language processing, pp. 1499–1509, 2015.\n\nVolker Tresp, Cristóbal Esteban, Yinchong Yang, Stephan Baier, and Denis Krompaß. Learning with\n\nmemory embeddings. arXiv preprint arXiv:1511.07972, 2015.\n\nDenny Vrandeˇci ́c and Markus Krötzsch. Wikidata: A free collaborative knowledgebase. Commun. ACM, 57(10):78–85, sep 2014. ISSN 0001-0782. doi: 10.1145/2629489. URL https://doi. org/10.1145/2629489.\n\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. Kepler: A unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics, 9:176–194, 2021.\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n\nChengjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Shariat Yazdi, and Jens Lehmann. Temporal knowledge graph embedding model based on additive time series decomposition. arXiv preprint arXiv:1911.07893, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nChengjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Shariat Yazdi, and Jens Lehmann. TeRo: A time-aware knowledge graph embedding via temporal rotation. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 1583–1593, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020. coling-main.139. URL https://aclanthology.org/2020.coling-main.139.\n\nIkuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. Joint learning of the embedding of words and entities for named entity disambiguation. arXiv preprint arXiv:1601.01343, 2016.\n\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575, 2014.\n\nLiang Yao, Chengsheng Mao, and Yuan Luo. Kg-bert: Bert for knowledge graph completion. arXiv\n\npreprint arXiv:1909.03193, 2019.\n\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced\n\nlanguage representation with informative entities. arXiv preprint arXiv:1905.07129, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nTable 2: Datasets Statistics\n\nDataset\n\n# Entities\n\n# Predicates\n\n# Timestamps\n\n# training set\n\n# validation set\n\n# test set\n\nGDELT DUEE WIKI\n\n5849 219 10844\n\n237 41 23\n\n2403 629 82\n\n755166 1879 233525\n\n94395 247 19374\n\n94395 247 19374\n\nTable 3: Hyperparameter settings of ECOLA and baselines.\n\nParameters\n\nDatasets\n\nTransE SimplE TTransE TNTComplEx DE-SimplE ECOLA-SF ECOLA-DE ECOLA-UTEE ECOLA-dyERNIE\n\nEmbedding dimension\n\nNegative Sampling\n\nLearning rate\n\nBatch Size\n\nGDELT DuEE Wiki GDELT DuEE Wiki GDELT DuEE Wiki\n\nGDELT DuEE Wiki\n\n768 768 768 768 768 768 768 768 768\n\n768 768 768 768 768 768 768 768 768\n\n768 768 768 768 768 768 768 768 768\n\n200 200 200 200 200 200 200 200 200\n\n100 100 100 100 100 100 200 200 200\n\n100 100 100 100 100 100 200 200 200\n\n5e-4 5e-4 5.2e-4 1.5e-4 5e-4 1e-4 2e-5 2e-5 2e-5\n\n5e-4 5e-4 5.2e-4 1.5e-4 5e-4 2e-5 2e-5 2e-5 e-4\n\n5e-4 5e-4 5.2e-4 1.5e-4 5e-4 1e-4 2e-5 2e-5 2e-5\n\n256 256 256 256 256 64 4\n4 4\n\n128 128 256 256 128 16 8\n8 8\n\n256 256 256 256 256 64 4\n4 4\n\nA RELATED WORK OF TEMPORAL KNOWLEDGE EMBEDDING\n\nTemporal Knowledge Embedding (tKE) is also termed as Temporal Knowledge Representation Learning (TKRL), which is to embed entities and predicates of temporal knowledge graphs into low-dimensional vector spaces. TKRL is an expressive and popular paradigm underlying many KG models. To capture temporal aspects, each model either embeds discrete timestamps into a vector space or learns time-dependent representations for each entity. Ma et al. (2019) developed extensions of static knowledge graph models by adding timestamp embeddings to their score functions. Besides, HyTE (Dasgupta et al., 2018) embeds time information in the entity-relation space by learning a temporal hyperplane to each timestamp and projects the embeddings of entities and relations onto timestamp-specific hyperplanes. Later, Goel et al. (2020) equipped static models with a diachronic entity embedding function which provides the characteristics of entities at any point in time and achieves strong results. Moreover, Han et al. (2020c) introduced a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds. It is the first work to contribute to geometric embedding for tKG and achieves state-of-the-art performances on the benchmark datasets. In particular, ECOLA is model-agnostic, which means any temporal KG embedding model can be potentially enhanced by training with the knowledge-text task.\n\nB ECOLA-SF: AN ABLATION STUDY ON STATIC FUSION\n\nWe compare the effectiveness of enhancing temporal knowledge embedding and enhancing static knowledge embedding. In particular, we only feed the static part of entity embeddings into PLM to perform the knowledge-text prediction task. We refer it as ECOLA-SF (StaticFusion).\n\nECOLA-SF is the static counterpart of ECOLA-DE, where we do not apply temporal knowledge embedding to the knowledge-text prediction objective LKT P . Specifically, we randomly initialize an embedding vector ̄ei P Rd for each entity ei P E, where ̄ei has the same dimension as the token embedding in pre-trained language models. Then we learn the time-invariant part ̄ei via the knowledge-text prediction task. For the tKE objective, we have the following temporal knowledge embedding,\n\n\"\n\neSF\n\ni\n\nptqrns “\n\nWsf ̄eirns aeirns sinpωeirnst ` beirnsq else,\n\nif 1 ď n ď γd,\n\nptq P Rd is an entity embedding containing static and temporal embedding parts. where eSF aei, ωei , bei P Rd ́γd are entity-specific vectors with learnable parameters. Wsf P Rdˆγd is\n\ni\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nHits@1 (%) Hits@10 (%)\n\nCRONKGQA ECOLA-CRONKGQA\n\n25.8 27.5\n\n52.0 54.4\n\nTable 4: Performance of ECOLA enhaned language model in tKBQA task.\n\nmatrix with learnable weights. Note that eSF embedding ̄ei instead of eSF\n\nptq in LKT P .\n\ni\n\ni\n\nptq only plays a role in LtKE, and we use static\n\nC ENHANCEMENT ON LANGUAGE REPRESENTATIONS\n\nAlthough our work focuses on enhancing temporal knowledge embeddings with contextualized language representations, joint models often benefit both the language model and the tKG model due to the mutual information exchange between language and tKGs during joint training. To study ECOLA’s enhancement on the language model, we selected temporal question answering as a downstream task to show that the proposed ECOLA can also benefit the language model.\n\nTemporal Question Answering over Temporal Knowledge Graphs (TKGQA) Natural questions often include temporal constraints, e.g., who was the US president before Jimmy Carter? To deal with such challenging temporal constraints, temporal question answering over temporal knowledge base, formulated as TKGQA task, has become trendy since tKGs help to find entity or timestamp answers with support of temporal facts.\n\nPerformance Gain on TKGQA Saxena et al. (2021a) introduced the TKGQA dataset CRONQUESTIONS containing natural temporal questions with different types of temporal constraints and an accompanying temporal knowledge graph (tKG). They proposed a baseline called CRONKGQA that uses a pre-trained language model (BERT) to understand the implicit representation of temporal constraints in temporal questions followed by a scoring function for answer prediction. We enhance the language encoder in CRONKGQA with the proposed ECOLA approach, i.e., we find temporal relevant texts for quadruples in the supporting tKG given in CRONQUESTIONS and train the language model and the tKG model jointly with the proposed KTP task. Then we plug the enhanced language model back into CRONKGQA and name the enhanced model as ECOLA-CRONKGQA. The models are evaluated with standard metrics Hits@kpk P t1, 3uq: the percentage of times that the true entity or time candidate appears in the top k of ranked candidates. As shown in Table 4, empirical results show that our proposed ECOLA enhances the language model with 7.4 % relative improvements regarding precision on CRONQUESTIONS, demonstrating the benefits of ECOLA to the language model.\n\nD IMPLEMENTATION\n\nWe use the datasets augmented with reciprocal relations to train all baseline models. We tune hyperparameters of our models using the random search and report the best configuration. Specifically, we set the loss weight λ to be 0.3, except for ECOLA-DE model trained on Wiki dataset where λ is set to be 0.001. We use the Adam optimizer (Kingma & Ba, 2014). We use the implementation of DE-SimplE6, ATiSE/TeRO7. We use the code for TNTCopmlEx from the tKG framework (Han et al., 2021). We implement TTransE based on the implementation of TransE in PyKEEN8. We provide the detailed settings of hyperparameters of each baseline model and ECOLA in Table 3 in the appendix.\n\nE THE AMOUNT OF COMPUTE AND THE TYPE OF RESOURCES USED\n\nWe run our experiments on an NVIDIA A40 with a memory size of 48G. We provide the training time of our models and some baselines in Table 5. Note that there are no textual descriptions at\n\n6https://github.com/BorealisAI/de-simple 7https://github.com/soledad921/ATISE 8https://github.com/pykeen/pykeen\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\ninference time, and we take the entity and predicate embedding as input and use the score function of KG models to predict the missing links. Thus, the inference time of ECOLA (e.g., ECOLA-DE) and its counterpart KG model (e.g., DE-SimplE) is the same.\n\nTable 5: The runtime of training procedure (in hours).\n\nDataset\n\nGDELT DuEE Wiki\n\nDE-SimplE ECOLA-DE UTEE ECOLA-UTEE DyERNIE ECOLA-DyERNIE\n\n17 24.0 67.3 36.0 25 23.8\n\n0.5 16.7 0.5 12.8 0.1 10.8\n\n5.0 43.2 11.3 45.6 5.9 67.2\n\n15",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a temporal KG-enhanced language model. \n\nThe idea is to combine pretrained temporal knowledge embeddings with the language model as input tokens, and the language model will take temporal-aware embeddings as input. \nThe authors propose masking the predicates, entities, or subwords for pretraining the model. \nThe authors convert three KGs into the pretraining corpus to find relevant text with temporal KG knowledge. Experiments show that the pretrained model has improved performance on link prediction for the three temporal KGs.\n\n# Strength And Weaknesses\n\nStrength: \n\nThe paper fills in the gap on using temporal KG to enhance the language model. The results look solid and bring consistent improvement in link prediction.\n\nWeakness: \n1. The method in the paper lacks novelty. It basically reformulates the previous KG+LM methods to the temporal KG+LM setting. Actually, the method (and even the figures) looks a lot like the KEPLER paper (Wang et al., 2019), with the KG embeddings replaced by tKG embeddings, and the transE loss replaced by the corresponding tKG loss.\n2. The method is only tested on link prediction tasks. I would expect this model to work in more diverse settings (i.e., any task that LM can be used for). It would be great if the model could work for settings like question answering, event extraction, etc. I'm also interested in how the tKG-pretraining affects general NLU performance like on GLUE.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written and easy to understand. As mentioned above, I think it lacks novelty. Code and data are included with the submission.\n\nA question: What are b_i and b_j in (1)? I did not find the definition in the paper.\n\n# Summary Of The Review\n\nIn all, I feel the paper focuses on an important area, but the novelty and contribution are not enough for a conference like ICLR.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nGENERAL POLICY EVALUATION AND IMPROVEMENT BY LEARNING TO IDENTIFY FEW BUT CRUCIAL STATES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nLearning to evaluate and improve policies is a core problem of Reinforcement Learning (RL). Traditional RL algorithms learn a value function defined for a single policy. A recently explored competitive alternative is to learn a single value function for many policies. Here we combine the actor-critic architecture of Parameter-Based Value Functions and the policy embedding of Policy Evaluation Networks to learn a single map from policy parameters to expected return that evaluates (and thus helps to improve) any policy represented by a deep neural network (NN). The method yields competitive experimental results. In continuous control problems with infinitely many states, our value function minimizes its prediction error by simultaneously learning a small set of ‘probing states’ and a mapping from actions produced in probing states to the policy’s return. The method extracts crucial abstract knowledge about the environment in form of very few states sufficient to fully specify the behavior of many policies. A policy improves solely by changing actions in probing states, following the gradient of the value function’s predictions. Surprisingly, it is possible to clone the behavior of a near-optimal policy in Swimmer-v3 and Hopper-v3 environments only by knowing how to act in 3 and 5 such learned states, respectively. Remarkably, our value function trained to evaluate NN policies is also invariant to changes of the policy architecture: we show that it allows for zero-shot learning of linear policies competitive with the best policy seen during training.\n\n1\n\nINTRODUCTION\n\nPolicy Evaluation and Policy Improvement are arguably the most studied problems in Reinforcement Learning. They are at the root of actor-critic methods (Konda and Tsitsiklis, 2001; Sutton, 1984; Peters and Schaal, 2008), which alternate between these two steps to iteratively estimate the performance of a policy and using this estimate to learn a better policy. In the last few years, they received a lot of attention because they have proven to be effective in visual domains (Mnih et al., 2016; Wu et al., 2017), continuous control problems (Lillicrap et al., 2015; Haarnoja et al., 2018; Fujimoto et al., 2018), and applications such as robotics (Kober et al., 2013). Several ways to estimate value functions have been proposed, ranging from Monte Carlo approaches, to Temporal Difference methods (Sutton, 1984), including the challenging off-policy scenario where the value of a policy is estimated without observing its behavior (Precup et al., 2001). A limiting feature of value functions is that they are defined for a single policy. When the policy is updated, they need to keep track of it, potentially losing useful information about old policies. By doing so, value functions typically do not capture any structure over the policy parameter space. While off-policy methods learn a single value function using data from different policies, they have no specific mechanism to generalize across policies and usually suffer for large variance (Cortes et al., 2010).\n\nParameter Based Value Functions (PBVFs)(Faccio et al., 2021) are a promising approach to design value functions that overcome this limitation and generalize over multiple policies. A crucial problem in the application of such value functions is choosing a suitable representation of the policy. Flattening the policy parameters as done in vanilla PBVFs is difficult to scale to larger policies. Here we present an approach that connects PBVFs and a policy embedding method called \"fingerprint mechanism\" by Harb et al. (2020). Using policy fingerprinting allows us to scale PBVFs to handle larger NN policies and also achieve invariance with respect to the policy architecture. Policy fingerprinting was\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nintroduced to learn maps from policy parameters to expected return offline and prior to this work was never applied to the online RL setting.\n\nWe show in visual classification tasks and in continuous control problems that our approach can identify a small number of critical \"probing states\" that are highly informative of the policies performance. Our learned value function generalizes across many NN-based policies. It combines the behavior of many bad policies to learn a better policy, and is able to zero-shot learn policies with a different architecture. We compare our approach with strong baselines in continuous control tasks: our method is competitive with DDPG (Lillicrap et al., 2015) and evolutionary approaches.\n\n2 BACKGROUND\n\nWe consider an agent interacting with a Markov Decision Process (MDP) Stratonovich (1960); Puterman (2014) M = (S, A, P, R, γ, μ0). The state space S ⊂ RnS and the action space A ⊂ RnA are assumed to be compact sub-spaces. In the MDP framework, at each time-step t, the agent observes a state st ∈ S, chooses an action at ∈ A, transitions into state st+1 with probability P (st+1|st, at), and receives a reward rt = R(st, at). The initial state is chosen with probability μ0(s). The agent’s behavior is represented by its policy π : S → ∆A: a function assigning for each state s a probability distribution over the action space. A policy is deterministic when for each state there exists an action a such that a is selected with probability one. Here we consider parametrized policies of the form πθ, where θ ∈ Θ ⊂ Rnθ are the policy parameters. The return Rt is defined as the cumulative discounted reward from time-step t, e.g. Rt = (cid:80)∞ k=0 γkR(st+k+1, at+k+1), where γ ∈ (0, 1] is the discount factor. The agent’s performance is measured by the expected return (i.e. the cumulative expected discounted reward) from the initial state: J(θ) = Eπθ [R0]. The state-value function V πθ (s) = Eπθ [Rt|st = s] of a policy πθ is defined as the expected return for being in a state s and following πθ. Similarly, the action-value function Qπθ (s, a) = Eπθ [Rt|st = s, at = a] of a policy πθ is defined as the expected return for being in a state s, taking action a and then following πθ. State and action value functions are related by V πθ (s) = (cid:82) A πθ(a|s)Qπθ (s, a) da. The expected return can be expressed in terms of the state and action value functions by integration over the initial state distribution:\n\nJ(θ) =\n\n(cid:90)\n\nS\n\nμ0(s)V πθ (s) ds =\n\n(cid:90)\n\nS\n\nμ0(s)\n\n(cid:90)\n\nA\n\nπθ(a|s)Qπθ (s, a) da ds.\n\n(1)\n\nThe goal of a RL agent is to find the policy parameters θ that maximize the expected return. Instead of learning a single value function for a target policy, here we try to estimate the value function of any policy and maximize it over the set of initial states.\n\n3 GENERAL POLICY EVALUATION\n\nRecent work focused on extending value functions to allow them to receive the policy parameters as input. This can potentially result in single value functions defined for any policy and methods that can perform direct search in the policy parameters. We begin by extending the state-value function, and define the parameter-based state-value function (PSVF) (Faccio et al., 2021) as the expected return for being in state s and following policy πθ: V (s, θ) = E[Rt|st = s, θ]. Using this new definition, we can rewrite the RL objective as J(θ) = (cid:82) S μ0(s)V (s, θ) ds. Instead of learning V (s, θ) for each state, we focus here on the policy evaluation problem over the set of the initial states of the agent. This is equivalent to trying to model J(θ) directly as a differentiable function V (θ), which is the expectation of V (s, θ) over the initial states:\n\nV (θ) := Es∼μ0(s)[V (s, θ)] =\n\n(cid:90)\n\nS\n\nμ0(s)V (s, θ) ds = J(πθ).\n\n(2)\n\nV (θ) is a parameter-based start-state value function (PSSVF). We consider the undiscounted case in our setting, so γ is set to 1 throughout the paper. Once V (θ) is learned, direct policy search can be performed by following the gradient ∇θV (θ) to update the policy parameters. This learning procedure can naturally be implemented in the actor-critic framework, where a critic value function— the PSSVF—iteratively uses the collected data to evaluate the policies seen so far, and the actor follows the critic’s direction of improvement to update itself. As in vanilla PSSVF, we inject noise in the policy parameters for exploration. The PSSVF actor-critic framework is reported in Algorithm1.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Actor-critic with PSSVF for V (θ)\n\nInput: Differentiable critic Vw : Θ → R with parameters w; deterministic or stochastic actor πθ\n\nwith parameters θ; empty replay buffer D\n\nOutput : Learned Vw ≈ V (θ)∀θ, learned πθ ≈ πθ∗\n\nInitialize critic and actor weights w, θ repeat:\n\nPerturb policy: θ′ = θ + ε, with ε ∼ N (0, σ2I) Generate an episode s0, a0, r1, s1, a1, r2, . . . , sT −1, aT −1, rT with policy πθ′ Compute return r = (cid:80)T Store (θ′, r) in the replay buffer D for many steps do:\n\nk=1 rk\n\nSample a batch B = {(r, θ′)} from D Update critic by stochastic gradient descent: ∇w E(r,θ′)∈B[(r − Vw(θ′))2]\n\nend for for many steps do:\n\nUpdate actor by gradient ascent: ∇θVw(θ)\n\nend for\n\nuntil convergence\n\nPolicy fingerprinting (Harb et al., 2020) While the algorithm described above is straightforward and easy to implement, feeding the policy parameters as inputs to the value function remains a challenge. Recently Harb et al. (2020) showed that a form of policy embedding can be suitable for this task. Their policy fingerprinting creates a lower-dimensional policy representation. It learns a set of K ‘probing states’ { ̃sk}K k=1 and an evaluation function U —like the PSSVF. To evaluate a policy πθ, they first compute the ‘probing actions’ ̃ak that the policy produces in the probing states. Then the concatenated vector of these actions is given as input to U : RK×nA → R. While the learned probing states remain fixed when evaluating multiple policies, the probing actions in such states depend on the policy we are evaluating. The parameters of the value function V are the probing states AND the weights of the MLP Uφ that maps the ‘probing actions’ to the return. When the policy πθ is deterministic, the probing actions for such policy are the deterministic actions { ̃ak = πθ( ̃sk)} produced in the probing states 1.\n\nThis mechanism has an intuitive interpretation: to evaluate the behavior of an agent, the PSSVF with policy fingerprinting learns a set of situations (or states), observes how the agent acts in those situations, and then maps the agent’s actions to a score. Arguably, this is also how a teacher would evaluate multiple different students by simultaneously learning which questions to ask the students and how to score the student’s answers.\n\nTherefore the parameters of the value function (probing states and evaluator function) can be learned by minimizing MSE loss LV between the prediction of the value function and the observed return. Setting w = {φ, ̃s1, . . . ̃sK}, we retrieve the common notation of Vw(θ) for the PSSVF with fingerprint mechanism. Given a batch B of data (πθ, r) ∈ B, the value function optimization problem is:\n\nmin w\n\nLV := min\n\nw\n\nE (πθ,r)∈B\n\n[(Vw(θ)−r)2] = min\n\nφ, ̃s1,... ̃sK\n\nE (πθ,r)∈B\n\n[(Uφ([πθ( ̃s1), . . . , πθ( ̃sK)])−r)2] (3)\n\nIf the prediction of the value function is accurate, policy improvement can be achieved by changing the way a policy acts in the learned probing states in order to maximize the prediction of the value function, like in the original PSSVF.\n\nThis process connects to the same interpretation as before: a student (the policy) observes which questions the teacher asks and how the teacher evaluates the student’s answers, and subsequently tries to improve in such a way to maximize the score predicted by the teacher. This iterative method is depicted in Figure 1. Note that Algorithm1 applies directly to this setting. The only distinction is that the probing states are part of the learned value function. Throughout this work, with the exception of the MNIST experiments, we consider deterministic policies.\n\n1If the policy is stochastic, the probing actions are the parameters of the output distribution of the policy in\n\nsuch states (the vector of probability distribution if the action space is discrete)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: General policy evaluation aims to evaluate any given policy’s return based on the policy’s actions (referred to as probing actions) in the learned probing states. The policy can be improved through maximising the prediction of the learned value function via gradient ascent.\n\n4 EXPERIMENTS\n\nThis section presents an empirical study of parameter-based value functions (PBVFs) with fingerprinting. We begin with a demonstration that fingerprinting can learn interesting states in MNIST purely through the designated evaluation task of mapping randomly initialized Convolutional Neural Networks (CNNs) to their expected loss. We also show that such a procedure could be used to construct a value function for offline improvement in MNIST. Next, we proceed to our main experiments on continuous control tasks in MuJoCo (Todorov et al., 2012). Here we show that our approach is competitive with strong baselines like DDPG (Lillicrap et al., 2015) and ARS (Mania et al., 2018), while it lacks sample efficiency when compared to SAC (Haarnoja et al., 2018). A strength of our approach is invariance to policy architecture. To illustrate this, we provide results on zero-shot learning of new policy architectures. Thereafter, we present a detailed analysis of the learned probing states in various MuJoCo environments. We conclude our study with the surprising observation that very few probing states are required to clone near-optimal behaviour in certain MuJoCo environments. An open-source implementation of our code is provided as supplementary material.\n\n4.1 MOTIVATING EXPERIMENTS ON MNIST\n\nWe begin our experimental section with an intuitive demonstration of how PBVFs with fingerprinting work, using the MNIST digit classification problem. The policy is a CNN, mapping images to a probability distribution over digit classes. The environment simulation consists of running a forward pass of the CNN on a batch of data and receiving the reward, which in this case is the negative cross-entropy between the output of the CNN and the labels of the data. The value function learns to map CNN parameters to the reward (the negative loss) obtained during the simulation. Then the CNN learns to improve itself only by following the prediction of the value function, without access to the supervised learning loss. These MNIST experiments can be considered as a contextual bandit problem, where the initial state (or context) is given by the batch of training data sampled and there are no transition dynamics. We start with a randomly initialized CNN and value function and iteratively update them following Algorithm 1. Using only 10 probing states, we obtain a test set accuracy of 82.5%. When increasing the number of probing states to 50, the accuracy increases to 87%.\n\nVisualization of probing states Figure 2 shows some of the probing states learned by our model, starting from random noise. During learning, we observe the appearance of various digits (sometimes the same digit in different shapes). Since probing states are states in which the action of the policy is informative about its global behavior, it is intuitive that digits should appear. We emphasize that both the CNNs and the value function are starting from random initializations. The convolutional filters and the probing states are learned using Algorithm 1, without access to the supervised loss. For more complex datasets like CIFAR10 our method found it difficult to learn meaningful probing states.\n\n4\n\nProbingStatesProbingActionsconcat.EnvironmentUnder review as a conference paper at ICLR 2023\n\nThis is possibly due to the high variance in the training data given a specific class and highlights a limitation of our method.\n\nFigure 2: Samples of probing states learned while training Algorithm 1 on MNIST.\n\nOffline policy improvement Using this setting, we perform another experiment. We collect one offline dataset {πθi, li}N i=1 of N randomly initialized CNN policies and their losses. We constrain the maximum accuracy of these CNNs in the training set to be 12%. We then use the dataset to train a value function offline. After training, we randomly initialize a new CNN and take many steps of gradient ascent through the fixed value function, obtaining a final CNN whose accuracy is around 65% on the test set. Our experiments show that our value function can combine the behavior of many bad NNs to produce a much better NN in a zero shot manner. We found that also with randomly initialized policies some digits appear as probing states, although they are less evident than in the online framework. We include learning curves and probing states for this scenario in Appendix B.1.\n\n4.2 MAIN EXPERIMENTS ON MUJOCO\n\nHere we present our main evaluation on selected continuous control problems from MuJoCo (Todorov et al., 2012). Since our algorithm performs direct search in parameter space, we choose Augmented Random Search (ARS) (Mania et al., 2018) as baseline for comparison. Moreover, since our algorithm employs deterministic policies, off-policy data, and an actor-critic architecture, a natural competitor is the Deep Deterministic Policy Gradient (DDPG) algorithm (Lillicrap et al., 2015), a strong baseline for continuous control. We also compare our method with the state-of-the-art Soft Actor-Critic (SAC) (Haarnoja et al., 2018).\n\nImplementation details For the policy architecture, we use an MLP with 2 hidden layers and 256 neurons for each layer. We use 200 probing states and later provide an analysis of them. Our implementation is based on the public code for Parameter-Based Value Functions. In some MuJoCo environments like Hopper and Walker, a bad agent can fail and the episode ends after very few time steps. This results in an excessive number of bad policies in the replay buffer, which can bias learning. Indeed, by the time a good policy is observed, it becomes difficult to use it for training when uniformly sampling experience from the replay buffer. We find that by prioritizing more recent data we are able to achieve a more uniform distribution over the buffer and increase the sample efficiency. We provide an ablation in Appendix B.2, showing the contribution of this component and of policy fingerprinting. Like in the original ARS and PBVF papers (Mania et al., 2018; Faccio et al., 2021), we use observation normalization and remove the survival bonus for the reward. The survival bonus, which provides reward 1 at each time step for remaining alive in Hopper, Walker and Ant, induces a challenging local optimum in parameter space where the agent would learn to keep still.\n\nFor DDPG and SAC, we use the default hyperparameters, yielding results on par with the best reported results for the method. For ARS, we tune for each environment step size, number of population and noise. For our method, we use a fixed set of hyperparameters, with the only exception of Ant. In Ant, we observe that setting the parameter noise for perturbations to 0.05 results in very rare positive\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nreturns for ARS and PSSVF (after subtracting the survival bonus). Therefore we use less noise for this environment. We discuss implementation details and hyperparameters in Appendix A.\n\nFigure 3: Return as a function of the environment interactions. The solid curve represents the mean (across 20 runs), and the shaded region represents a 95% bootstrapped confidence interval.\n\nResults Figure 3 shows learning curves in terms of expected return (mean and 95% confidence interval) achieved by our algorithm and the baselines across time in the environments. Our algorithm is very competitive with DDPG and ARS. It outperforms DDPG in all environments with the exception of HalfCheetah and Walker, and displays faster initial learning than ARS. In the Swimmer environment, DDPG and SAC fails to learn an optimal policy due to the problem of discounting2. On the other hand, in HalfCheetah, parameter-based methods take a long time to improve, and the ability of DDPG to give credit to sub-episodes is crucial here to learn quickly. Furthermore, the variance of our method’s performance is less than DDPG’s and comparable to ARS’s. Like evolutionary approaches, our method uses only the return as learning data, while ignoring what happens in each state-action pairs. This is a limitation of our method and it is evident how PSSVF and ARS are less sample efficient in comparison to SAC in many environments.\n\nIn preliminary experiments we tried to learn also a function V (s0, θ), incorporating the information on the initial state. In practice, we can store in the buffer tuples (s0, θ, r) consisting of initial state, policy parameters and episodic return. When training the PSSVF (now similar to the PSVF), we concatenate the initial state to the probing actions and map the vector of probing actions and initial state to the return. Then policy improvement is achieved by finding the policy parameters that maximize the value function’s prediction taking an expectation over the initial states sampled from the buffer. The results were very similar to those we presented in this section, so we decided to use the more straightforward approach that ignores the initial state and directly maps policy parameters to expected return. It would be also possible to learn a general value function V (s, θ) for any state, like in the PSVF algorithm (Faccio et al., 2021). We leave this as future work.\n\nComparison to vanilla PSSVF A direct comparison to the standard Parameter-Based Value function is unfeasible for large NNs. This is because in the vanilla PSSVF, flattened policy parameters are directly fed to the value function. In our policy configuration, the flattened vector of policy parameters contains about 70K elements, which is significantly more than 200 × nA elements used to represent policies with fingerprinting. Nevertheless, we provide a direct comparison between the two approaches using a smaller policy architecture which consists of an MLP with 2 hidden layers and 64 neurons per layer. The complete results are provided in Appendix B.2. Our results in this setting show that the fingerprint mechanism could be useful even for smaller policies.\n\n2This is a common problem for Temporal Difference methods: the policy optimizing expected return in Swimmer with γ = 0.99 is sub-optimal when considering the expected return with γ = 1. See the ablation in Appendix A.3.1 of (Faccio et al., 2021).\n\n6\n\n0.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0200040006000returnAnt-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps02500500075001000012500returnHalfCheetah-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100200300returnSwimmer-v3pssvfarsddpgsac0.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0200040006000returnWalker2d-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps01000200030004000returnHopper-v30.00M0.05M0.10M0.15M0.20M0.25M0.30Mtime steps0200040006000800010000returnInvertedDoublePendulum-v2Under review as a conference paper at ICLR 2023\n\n4.3 ZERO-SHOT LEARNING OF NEW POLICY ARCHITECTURES\n\nHere we show that our method can generalize across policy architectures. We train a PSSVF using NN policies as in the main experiments. Then we randomly initialize a linear policy and start taking gradient ascent steps through the fixed value function, finding the parameters of the policy that maximizes the value function’s prediction. In Figure 5 we observe that a near-optimal linear policy can be zero-shot-learned through the value function even if it was trained using policies with different architecture. It achieves an expected return of 345, while the return of best NN used for training was 360. Figure 12 (Appendix) shows results for zero-shot learning deep policies in Swimmer. We notice more variance in the performance, which might be caused by the deep policy overfitting more easily probing-state/probing-action pairs during the policy improvement phase.\n\n4.4 FINGERPRINT ANALYSIS\n\nAblation on number of probing states Our experiments show that learning probing states helps evaluating the performance of many policies, but how many of such probing states are necessary for learning? We run our main experiments again, with fewer probing states, and discover that in many environments, a very small number of states is enough to achieve good performance. In particular, we find that the PSSVF with 5 probing states achieves 314 and 2790 final return in Swimmer and Hopper respectively, while Walker needs at least 50 probing states to obtain a return above 2000. In general, 200 probing states represent a good trade-off between learning stability and computational complexity. We compare the performances of PSSVF versions with varying numbers of probing states. We use the same hyperparameters as in the main experiments (see Appendix A.2), apart for the number of probing states. Figure 4 shows that in Hopper and Swimmer 10 probing states are sufficient to learn a good policy, while Walker needs a larger number of probing states to provide stability in learning.\n\nFigure 4: Average return of PSSVF with different number of probing states as a function of the number of time steps in the environment. The solid line is the average over 10 independent runs; the shading indicates 95% bootstrapped confidence intervals.\n\nThe most surprising result is that a randomly initialized policy can learn near-optimal behaviors in Swimmer and Hopper by knowing how to act only in 3 (5) such crucial learned states (out of infinitely many in the continuous state space). To verify this, we manually select 3 of the 5 learned probing states in Swimmer, and compute the actions of an optimal policy in such states. Then we train a new, randomly initialized policy, to just fit these 3 data points minimizing MSE loss. After many gradient steps, the policy obtains a return of 355, compared to the return of 364 of the optimal policy that was used to compute such actions. Figure 15 (in Appendix B.3) includes a detailed analysis of this experiment. The probing actions are the vectors [−0.97, −0.86], [−0.18, −0.99], [0.86, 0.68]. In the plot we notice that when the agent’s state is close to the first probing state (bottom plot, depicted in blue), then both components of the actions are close to -1, like the probing action in such state. When the agent’s state is close to the second state (bottom plot, depicted in orange), the first component\n\nFigure 5: Performance of a linear policy (in blue) zero-shot learned (averaged over 5 runs, 95% bootstrapped CI). The orange line shows the best performance of the deep NN when training the PSSVF.\n\n7\n\n0.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100020003000returnWalker2d-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100020003000returnHopper-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100200300returnSwimmer-v312510205020050002004006008001000gradient steps0100200300400returnUnder review as a conference paper at ICLR 2023\n\nof the action moves from -1 to 0 (and then to +1) in a smooth way, while the second component jumps directly to +1. This behavior is consistent with the second probing action, since the second component is more negative than the first. Notably, although the distance between the agent’s state and the third probing state (bottom plot, depicted in green) is never close to zero, such a probing state is crucial: it induces the agent to take positive actions whenever the other probing states are far away. We observe similar behavior for other environments, although they need more of such states to encode the behavior of an optimal policy. Using a similar procedure, we are able to train a randomly initialized policy in Hopper achieving 2200 return, using only 5 state-action pairs. We provide a detailed discussion and learning curves for this task in Appendix B.3.\n\nVisualization of RL probing states It is possible to visualize the probing states learned by the PSSVF. To understand the behaviour in probing states, we initialize the MuJoCo environment to the learned probing state (when possible) and let it evolve for a few time steps while performing no action. In Appendix B.3 we show the crucial learned probing states of our previous experiment. Additional probing states for all environments can be seen in animated form on the website https: //anonymous260522.github.io/.\n\n5 RELATED WORK\n\nThere is a long history of RL algorithms performing direct search in parameter space or policy space. The most common approaches include evolution strategies, e.g., (Rechenberg, 1971; Sehnke et al., 2010; 2008; Wierstra et al., 2014; Salimans et al., 2017). They iteratively simulate a population of policies and use the result to estimate a direction of improvement in parameter space. Evolution strategies, however, don’t reuse data: the information contained in the population is lost as soon as an update is performed, making them sample-inefficient. Several attempts have been made to reuse past data, often involving importance sampling (IS) (Zhao et al., 2013), but these methods suffer from high variance of the fitness estimator (Metelli et al., 2018). Our method directly estimates a fitness for each policy observed in the history and makes efficient reuse of past data without involving IS.\n\nDirect search can be facilitated by compressed network search (Koutnik et al., 2010) and algorithms that distill the knowledge of an NN into another NN (Schmidhuber, 1992). Closely related to our fingerprint embedding is also the concept of Dataset Distillation (Wang et al., 2018). However, in our RL setting, learning to distill crucial states from an environment is harder due to the nondifferentiability of the environment. Estimating a global objective function is common in control theory, where usually a gaussian process is maintained over the policy parameters. This allows to perform direct policy optimization during the parameter search. Such approaches are often used in the Bayesian optimization framework (Snoek et al., 2015; 2012), where a tractable posterior over the parameter space is used to drive policy improvements. Despite the soundness of these approaches, they usually employ very small control policies and scale badly with the dimension of the policy parameters. Our method, however, is invariant to policy parametrization.\n\nIt is based on a recent class of algorithms that were developed to address global estimation and improvement of policies. For Policy Evaluation Networks (PVNs) (Harb et al., 2020), an actor-critic algorithm for offline learning through policy fingerprinting was proposed. PVNs focus on the offline RL setting. In PVNs, first a dataset of randomly initialized policies with their returns is collected. Then, once their V (θ) with policy fingerprinting is trained, they perform policy improvement through gradient ascent steps on V . Their experimental setting is similar to our MNIST offline demonstration, which we provide just to give an intuition on how policy fingerprinting works. Concurrently, Parameter-Based Value Functions were developed to provide single value functions able to evaluate any policy, given a state, state-action pair, or a distribution over the agent’s initial states. PBVFs did not use any dimensionality reductions techniques such as the policy fingerprinting mechanism, but demonstrated sample efficiency in the online RL scenario, directly using the flattened parameters of a neural network as inputs. They exhibited zero-shot learning for linear policies, but failed when the policy parameters were high-dimensional. Here, however, we demonstrated that PBVFs with policy fingerprinting mechanisms can be efficient in the online scenario. A minor difference between our approach and PVNs is that PVNs predict a discretized distribution of the return, whereas our approach simply predicts the expected return. Our method can be seen like an online version of PVN without some of the tricks used, or like a version of PSSVF where policy fingerprinting is used. Fingerprinting itself is similar to a technique for \"learning to think\" (Schmidhuber, 2015) where one\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nNN learns to send queries (sequences of activation vectors) into another NN and learns to use the answers (sequences of activation vectors) to improve its performance.\n\nRecent work (Tang et al., 2020) learned Parameter-Based State-Value Functions which, coupled with PPO, improved performance. The authors did not use the value function to directly backpropagate gradients through the policy parameters, but only exploited the general policy evaluation properties of the method. They also proposed two dimensionality reduction techniques. The first, called Surface Policy Representation, consists of learning a state-action embedding that encodes possible information from a policy πθ. This requires feeding state-action pairs to a common MLP whose output is received as input to the value function. The MLP is trained such that it allows for both low prediction error in the value function and low reconstruction error of the action, given a state and the embedding. This method is not differentiable in the policy parameters, therefore it cannot be used for gradient-based policy improvement. The second method, called Origin Policy Representation (OPR), consists of using an MLP that performs layer-wise extraction of features from policy parameters. OPR uses MLPs to take as input direcly the weight matrix of each layer. This approach is almost identical to directly feeding the policy parameters to the value function (they concatenate the state to the last layer of such MLP), and suffers from the curse of dimensionality. Also, OPR was not used to directly improve the policy parameters, but only to provide better policy evaluation.\n\nAlternative strategies to represent policies have been studied in previous work. One such strategy aims to learn a representation function mapping trajectories to a policy embedding through an autoencoding objective (Grover et al., 2018; Raileanu et al., 2020). In particular, Grover et al. (2018) use this idea to model the agent’s behavior in a multi-agent setting. The approach presented by Raileanu et al. (2020) performs gradient ascent steps finding a policy embedding that maximizes the value function’s predicted return. While this maximization through the value function is similar to our setting, it relies on a representation function (or policy decoder). Our method does not use a decoder and instead directly backpropagates the gradients into the policy parameters for policy improvement. Closer to our fingerprinting setup, Pacchiano et al. (2020) utilize pairs of states and actions (from the corresponding policy) as a policy representation. However, unlike in our approach, the probing states are not learned, but sampled from a chosen probing state distribution. Kanervisto et al. (2020) suggest representing policies based on visited states via Gaussian Mixture Models applied to an offline dataset of data from multiple policies. The authors mention that their current version of policy supervectors is intended for analysing policies and is not yet suitable for online optimization. Value functions conditioned on other quantities include vector-valued adaptive critics Schmidhuber (1991), General Value Functions (Sutton et al., 2011), and Universal Value Function Approximators (Schaul et al., 2015). Unlike our approach these methods typically generalize over achieving different goals, and are not used to generalize across policies.\n\n6 CONCLUSION AND FUTURE WORK\n\nOur approach connects Parameter-Based Value Functions (PBVFs) and the fingerprinting mechanism of Policy Evaluation Networks. It can efficiently evaluate large Neural Networks, is suitable for off-policy data reuse, and competitive with existing baselines for online RL tasks. Zero-shot learning experiments on MNIST and continuous control problems demonstrated our method’s generalization capabilities. Our value function is invariant to policy architecture changes, and can extract essential knowledge about a complex environment by learning a small number of situations that are important to evaluate the success of a policy. A randomly initialized policy can learn optimal behaviors in Swimmer (Hopper) by knowing how to act only in 3 (5) such crucial learned states (out of infinitely many in the continuous state space). This suggests that some of the most commonly used RL benchmarks require to learn only a few crucial state-action pairs. Our set of learned probing states is instead used to evaluate any policy, while in practice different policies may need different probing states for efficient evaluation. A natural direction for improving this method and scaling it to more complex tasks is to generate probing states in a more dynamic way, or learn to retrieve them directly from the agent’s experience in the replay buffer. Like evolutionary approaches and trajectory based RL, our method might suffer high variance in stochastic environments or when the variance of the return over the initial state is high. In such scenario, poor value estimates might prevent policy improvement or zero-shot learning. Finally, PBVFs are a general framework that also considers value functions that receive states and state-action pairs as input. We plan to investigate how these value functions trained by Temporal Differences (Sutton, 1988) behave with policy fingerprinting.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAchiam, J. (2018). Spinning Up in Deep Reinforcement Learning.\n\nCortes, C., Mansour, Y., and Mohri, M. (2010). Learning bounds for importance weighting. In\n\nAdvances in neural information processing systems, pages 442–450.\n\nFaccio, F., Kirsch, L., and Schmidhuber, J. (2021). Parameter-based value functions.\n\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\n\nFujimoto, S., Van Hoof, H., and Meger, D. (2018). Addressing function approximation error in\n\nactor-critic methods. arXiv preprint arXiv:1802.09477.\n\nGrover, A., Al-Shedivat, M., Gupta, J., Burda, Y., and Edwards, H. (2018). Learning policy representations in multiagent systems. In International conference on machine learning, pages 1802–1811. PMLR.\n\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290.\n\nHarb, J., Schaul, T., Precup, D., and Bacon, P.-L. (2020). Policy evaluation networks. arXiv preprint\n\narXiv:2002.11833.\n\nKanervisto, A., Kinnunen, T., and Hautamäki, V. (2020). General characterization of agents by states\n\nthey visit. arXiv preprint arXiv:2012.01244.\n\nKober, J., Bagnell, J. A., and Peters, J. (2013). Reinforcement learning in robotics: A survey. The\n\nInternational Journal of Robotics Research, 32(11):1238–1274.\n\nKonda, V. and Tsitsiklis, J. (2001). Actor-critic algorithms. Society for Industrial and Applied\n\nMathematics, 42.\n\nKoutnik, J., Gomez, F., and Schmidhuber, J. (2010). Evolving neural networks in compressed weight space. In Proceedings of the 12th annual conference on Genetic and evolutionary computation, pages 619–626.\n\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.\n\nMania, H., Guy, A., and Recht, B. (2018). Simple random search of static linear policies is competitive for reinforcement learning. In Advances in Neural Information Processing Systems, pages 1800– 1809.\n\nMetelli, A. M., Papini, M., Faccio, F., and Restelli, M. (2018). Policy optimization via importance\n\nsampling. In Advances in Neural Information Processing Systems, pages 5442–5454.\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928–1937. PMLR.\n\nPacchiano, A., Parker-Holder, J., Tang, Y., Choromanski, K., Choromanska, A., and Jordan, M. (2020). Learning to score behaviors for guided policy optimization. In International Conference on Machine Learning, pages 7445–7454. PMLR.\n\nPeters, J. and Schaal, S. (2008). Natural actor-critic. Neurocomput., 71(7-9):1180–1190.\n\nPrecup, D., Sutton, R. S., and Dasgupta, S. (2001). Off-policy temporal difference learning with\n\nfunction approximation. In ICML.\n\nPuterman, M. L. (2014). Markov decision processes: discrete stochastic dynamic programming. John\n\nWiley & Sons.\n\nRaileanu, R., Goldstein, M., Szlam, A., and Fergus, R. (2020). Fast adaptation to new environments via policy-dynamics value functions. In Proceedings of the 37th International Conference on Machine Learning, pages 7920–7931.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRechenberg, I. (1971). Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der\n\nbiologischen Evolution. Dissertation. Published 1973 by Fromman-Holzboog.\n\nSalimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution strategies as a scalable\n\nalternative to reinforcement learning. arXiv preprint arXiv:1703.03864.\n\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal value function approximators. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, pages 1312–1320. JMLR.org.\n\nSchmidhuber, J. (1991). Reinforcement learning in Markovian and non-Markovian environments. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems 3 (NIPS 3), pages 500–506. Morgan Kaufmann.\n\nSchmidhuber, J. (1992). Learning complex, extended sequences using the principle of history\n\ncompression. Neural Computation, 4(2):234–242.\n\nSchmidhuber, J. (2015). On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models. Preprint arXiv:1511.09249.\n\nSehnke, F., Osendorfer, C., Rückstieß, T., Graves, A., Peters, J., and Schmidhuber, J. (2008). Policy gradients with parameter-based exploration for control. In K ̊urková, V., Neruda, R., and Koutník, J., editors, Artificial Neural Networks - ICANN 2008, pages 387–396, Berlin, Heidelberg. Springer Berlin Heidelberg.\n\nSehnke, F., Osendorfer, C., Rückstieß, T., Graves, A., Peters, J., and Schmidhuber, J. (2010).\n\nParameter-exploring policy gradients. Neural Networks, 23(4):551–559.\n\nSnoek, J., Larochelle, H., and Adams, R. P. (2012). Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959.\n\nSnoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M. M. A., Prabhat, P., and Adams, R. P. (2015). Scalable bayesian optimization using deep neural networks. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, page 2171–2180. JMLR.org.\n\nStratonovich, R. (1960). Conditional Markov processes. Theory of Probability And Its Applications,\n\n5(2):156–178.\n\nSutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University\n\nof Massachusetts Amherst.\n\nSutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine learning,\n\n3(1):9–44.\n\nSutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., and Precup, D. (2011). Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 2, AAMAS ’11, pages 761–768, Richland, SC. International Foundation for Autonomous Agents and Multiagent Systems.\n\nTang, H., Meng, Z., Hao, J., Chen, C., Graves, D., Li, D., Yu, C., Mao, H., Liu, W., Yang, Y., et al. (2020). What about inputting policy in value function: Policy representation and policy-extended value function approximator. arXiv preprint arXiv:2010.09536.\n\nTodorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033.\n\nWang, T., Zhu, J.-Y., Torralba, A., and Efros, A. A. (2018). Dataset distillation. arXiv preprint\n\narXiv:1811.10959.\n\nWierstra, D., Schaul, T., Glasmachers, T., Sun, Y., Peters, J., and Schmidhuber, J. (2014). Natural\n\nevolution strategies. The Journal of Machine Learning Research, 15(1):949–980.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nWu, Y., Mansimov, E., Grosse, R. B., Liao, S., and Ba, J. (2017). Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. Advances in neural information processing systems, 30.\n\nZhao, T., Hachiya, H., Tangkaratt, V., Morimoto, J., and Sugiyama, M. (2013). Efficient sample reuse in policy gradients with parameter-based exploration. Neural computation, 25(6):1512–1547.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA IMPLEMENTATION DETAILS\n\nA.1 MNIST IMPLEMENTATION\n\nFor our experiments with MNIST we adapt the official code for PSSVF to CNN policies and the MNIST classification problem.\n\n• Policy architecture: The policy consists of two convolutional layers with 4 and 8 output channels respectively, 3 × 3 kernels and a stride of 1. Each convolutional layer is followed by ReLU activations. The output from the convolutional layers is flattened and provided to a fully connected linear layer which outputs the logits for the ten MNIST classes. The logits are fed into a categorical distribution; the outputs are interpreted as class probabilities.\n\n• Value function architecture: MLP with 2 hidden layers and 64 neurons per layer with bias.\n\nReLU activations.\n\n• Batch size for computing the loss: 1024\n\n• Batch size for value function optimization: 4\n\n• Buffer size: 1000\n\n• Loss: Cross entropy\n\n• Initialization of probing states: uniformly random in [−0.5, 0.5)\n\n• Update frequency: every time a new episode is collected\n\n• Number of policy updates: 1\n\n• Number of value function updates: 5\n\n• Learning rate policy: 1e-6\n\n• Learning rate value function: 1e-3\n\n• Noise for policy perturbation: 0.05 • Priority sampling from replay buffer: True, with weights 1/x0.8, where x is the number of\n\nepisodes since the data was stored in the buffer\n\n• Default PyTorch initialization for all networks.\n\n• Optimizer: Adam\n\nA.2 RL IMPLEMENTATION\n\nHere we report the hyperparameters used for PSSVF and the baselines. For PSSVF, we use the open source implementation provided by Faccio et al. (2021). For DDPG and SAC, we use the spinning-up RL implementation (Achiam, 2018), whose results are on par with the best reported results. For ARS, we adapt the publicly available implementation (Mania et al., 2018) to Deep NN policies.\n\nShared hyperparameters:\n\n• Policy architecture: Deterministic MLP with 2 hidden layers and 256 neurons per layer with bias. Tanh activations for PSSVF and ARS. ReLu activations for DDPG and SAC. The output layer has Tanh nonlinearity and bounds the action in the action-space limit.\n\n• Value function architecture: MLP with 2 hidden layers and 256 neurons per layer with bias.\n\nReLU activations for PSSVF and DDPG and SAC.\n\n• Initialization for actors and critics: Default PyTorch initialization\n\n• Batch size: 128 for DDPG and SAC. 16 for PSSVF\n\n• Learning rate actor: 1e-3 for DDPG and SAC; 2e-6 for PSSVF\n\n• Learning rate critic: 1e-3 for DDPG and SAC, 5e-3 for PSSVF\n\n• Noise for exploration: 0.05 in parameter space for PSSVF; 0.1 in action space for DDPG\n\n• Actor’s frequency of updates: every episode for PSSVF; every 50 time steps for DDPG and\n\nSAC; every batch for ARS\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n• Critic’s frequency of updates: every episode for PSSVF; every 50 time steps for DDPG and\n\nSAC\n\n• Replay buffer size: 100k for DDPG and SAC; 10k for PSSVF\n\n• Optimizer: Adam for PSSVF and DDPG and SAC\n\n• Discount factor: 0.99 for DDPG and SAC; 1 for PSSVF and ARS\n\n• Survival reward adjustment: True for ARS and PSSVF in Hopper, Walker, Ant; False for\n\nDDPG and SAC\n\n• Environmental interactions: 300k time steps in InvertedDoublePendulum; 3M time steps in\n\nall other environments\n\nTuned hyperparameters:\n\n• Step size for ARS: tuned with values in {1e − 2, 1e − 3, 1e − 4}\n\n• Number of directions and elite directions\n\nin {[1, 1], [8, 4], [8, 8], [32, 4], [32, 16], [64, 8], [64, 32]}, where the first element denotes the number of directions and the second element the number of elite directions\n\ntuned with values\n\nfor ARS:\n\n• Noise for exploration in ARS: tuned with values in {0.1, 0.05, 0.025}\n\nHyperparameters for specific algorithms:\n\nPSSVF:\n\n• Number of probing states: 200\n\n• Initialization of probing states: uniformly random in [0, 1)\n\n• Observation normalization: True\n\n• Number of policy updates: 5\n\n• Number of value function updates: 5\n\n• Priority sampling from replay buffer: True, with weights 1/x1.1, where x is the number of\n\nepisodes since the data was stored in the buffer\n\nARS:\n\n• Observation normalization: True\n\nDDPG and SAC:\n\n• Observation normalization: False\n\n• Number of policy updates: 50\n\n• Number of value function updates: 50\n\n• Start-steps (random actions): 10000 time-steps\n\n• Update after (no training): 1000 time-steps\n\n• Polyak parameter: 0.995\n\n• Entropy parameter (SAC): 0.2\n\nA.3 GPU USAGE / COMPUTATION REQUIREMENTS\n\nEach run of PSSVF in the main experiment takes around 2.5 hours on a Tesla P100 GPU. We ran 4 instances of our algorithm for each GPU. We estimate a total of 75 node hours to reproduce our main RL results (20 independent runs for 6 environments).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nB EXPERIMENTAL DETAILS\n\nB.1 MNIST EXPERIMENTS\n\nOnline learning through Algorithm 1 We use PSSVF (Algorithm 1) with the hyperparameters described in Appendix A.1. Figure 6 shows the performance of PSSVF using CNNs on MNIST with 10 and 50 probing states as a function of the number of interactions with the dataset. Each interaction consists of perturbing the current policy with random noise, computing the loss of the perturbed policy on a batch of data, storing the perturbed policy and its loss, and updating.\n\nFigure 6: On the left: test accuracy of PSSVF as a function of the interactions with the dataset. On the right: loss of the perturbed CNN on the training set. Average over 5 independent runs and 95% bootstrapped confidence interval.\n\nVisualization of learned probing states We plot the evolution of some of the probing states, starting from random noise, until the PSSVF is learned. We consider one run of the previous experiment with 10 probing states and show how they change during learning. This is depicted in Figure 7 where randomly initialized probing states slowly become similar to digits.\n\nOffline policy improvement This section describes the offline MNIST experiment of the main paper. Here every iteration encompasses the following steps. We perturb a randomly initialized CNN with gaussian noise with standard deviation 0.1. Then we compute the loss on a batch of 1024 training data. If the accuracy on such batch is below 12%, we store the CNN and its loss, otherwise we discard the data. At every iteration we also train a PSSVF with 200 probing states, using the data collected (whose accuracy is at most 12%). We repeat this for 90000 iterations. Then, we randomly initialize a new CNN and train it by taking gradient steps through the fixed PSSVF, without further seeing training data. In Figure 8 we plot the performance of the zero-shot learned CNN. Surprisingly, it achieves a test accuracy of 65%, although only CNNs with at most 12% accuracy are used in training. From the same figure we also observe that the prediction of the PSSVF is quite accurate up to 80 gradient steps, after which the performance degrades. We use a learning rate of 1e − 3 for the CNN.\n\nVisualization of learned probing states When training the PSSVF using CNNs whose accuracy is at most 12%, we also observe the formation of \"numbers\" as probing states, although they are not as evident as in the online setting. We provide some examples in Figure 9.\n\nB.2 MAIN EXPERIMENTS ON MUJOCO\n\nTo measure learning progress, we evaluate each algorithm for 10 episodes every 10000 time steps. We use the learned policy for PSSVF and ARS and the deterministic actor (without action noise) for DDPG. We use 20 independent instances of the same hyperparameter configuration for PSSVF and DDPG in all environments. When tuning ARS, we run 5 instances of the algorithm for each hyperparameter configuration. Then we select the best hyperparameter for each environment and carry out a further 20 independent runs. We report the best hyperparameters found for ARS in Table 1. In addition to the learning curves of the main paper in Figure 3, we report the final return with a standard deviation in Table 2.\n\n15\n\n050000100000150000200000iterations20406080test accuracy050000100000150000200000iterations1.61.82.02.2train loss50 probing states10 probing statesUnder review as a conference paper at ICLR 2023\n\nFigure 7: From left to right, the 10 probing states learned by the PSSVF using Algorithm 1. Each column represents 12500 interactions.\n\nAblation on weighted sampling In Figure 10 we show the benefit of using non-uniform sampling from the replay buffer in Hopper and Walker environments. We compare uniform sampling (no weight) to non uniform sampling with weight 1/xk, where k ∈ {1.0, 1.1}, and x is the number of episodes since the data was stored in the buffer. We achieve the best results in Hopper and Walker for the choice of x = 1.1. It is interesting to take this into consideration when comparing our approach to vanilla PSSVF.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: On the left: test accuracy of a random initialized CNN zero-shot learned using a learned PSSVF. On the right, the prediction of the performance of the CNN given by the PSSVF and the true performance on the test set. Average over 5 independent runs and 95% bootstrapped c.i.\n\nFigure 9: Samples of probing states learned by the PSSVF using CNNs with at most 12% training set accuracy.\n\nTable 1: Best hyperparameters for ARS\n\nEnvironment Walker2d-v3 Swimmer-v3 HalfCheetah-v3 Ant-v3 Hopper-v3 InvertedDoublePendulum-v2\n\nstep size 0.01 0.01 0.01 0.01 0.01 0.01\n\ndirections [8,8] [8,4] [8,4] [32,16] [8,4] [8,8]\n\nnoise 0.05 0.05 0.05 0.01 0.05 0.025\n\nTable 2: Final return (average over final 20 evaluations)\n\nEnvironment Walker2d-v3 Swimmer-v3 HalfCheetah-v3 Ant-v3 Hopper-v3 InvertedDouble Pendulum-v2\n\nPSSVF 2333 ± 343 349 ± 60 3067 ± 820 1549 ± 240 2969 ± 165\n\nARS 1488 ± 961 342 ± 21 2497 ± 611 1697 ± 225 2340 ± 199\n\nDDPG 2432 ± 1330 129 ± 25 10695 ± 1358 466 ± 716 1634 ± 1036\n\nSAC 5287 ± 467 44 ± 1 13599 ± 932 5319 ± 992 3292 ± 345\n\n7649 ± 2640\n\n4515 ± 2733\n\n7377 ± 3770\n\n9235 ± 227\n\nFigure 10: Comparison between our algorithm without weighted sampling from the replay buffer and with weight 1/xk, where k ∈ {1.0, 1.1}. Average over 10 independent runs and 95% bootstrapped confidence interval.\n\n17\n\n0255075100125150175200gradient steps203040506070test accuracy0255075100125150175200gradient steps0.00170.00180.00190.00200.00210.00220.0023losstest lossvalue function prediction0.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100020003000returnWalker2d-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100020003000returnHopper-v3k=1.1k=1.0No weightUnder review as a conference paper at ICLR 2023\n\nComparison to vanilla PSSVF Here we compare our PSSVF with policy fingerprinting to vanilla PSSVF. For vanilla PSSVF, we use the best hyperparameters reported by Faccio et al. (2021) when optimizing policies with 2 hidden layers and 64 neurons per layer and optimizing over the final rewards. Our algorithm uses the policy architecture of vanilla PSSVF and the hyperparameters of our main experiments, changing only the learning rate of the policy to 1e − 4 and the noise for policy perturbations to 0.1. Figure 11 shows that while in Swimmer policy fingerprinting is enough to achieve an improvement over vanilla PSSVF, in Hopper non-uniform sampling plays an important role. Note that in the vanilla PSSVF paper, learning rates and perturbation noise are tuned for each environment, while in our experiments we keep a fixed set of hyperparameters for all environments to maintain consistency. We expect the performance of our approach to also improve by selecting hyperparameters separately for each environment.\n\nFigure 11: Comparison between vanilla PSSVF with no weighted sampling and no fingerprinting, PSSVF with policy fingerprinting, and our final algorithm that uses also weighted sampling. The solid line is the average over 10 independent runs; the shading indicates 95% bootstrapped confidence intervals.\n\nZero-shot learning of new policy architectures For this task we use the same hyperparameters as in the main experiments (see Appendix A.2). We use a learning rate of 1e − 4 to zero-shot learn the linear policy. Figure 12 reports similar results for deep policies.\n\nFigure 12: Performance of a deep policy (in blue) zero-shot learned (averaged over 5 runs, 95% bootstrapped CI). The orange line shows the best performance of the deep NN when training the PSSVF.\n\nB.3 FINGERPRINT ANALYSIS\n\nLearning Swimmer with 3 states We are interested in what is the smallest amount of stateaction pairs we could use to clone an optimal policy. In order to select the 3 transitions we try all combinations of 3 probing states our of 5 that we used to train our PSSVF. When cloning using all 5 probing states, the performance is very similar to the optimal policy. When choosing 4 out of 5 probing states, we notice that the performance highly depends on which probing state is removed, suggesting that some of the learned probing states are more important than others. When trying 3 out of 5 probing states this effect is more evident, and many combinations of 3 probing states lead to poor cloning performance. Here we report the learning curves for the experiment in the main paper where we fit a randomly initialized policy using only 3 transitions (see Section 4.4). These 3 transitions are 3 probing states and the corresponding optimal action (probing action) in those states. We can\n\n18\n\n0.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100200300returnSwimmer-v30.00M0.50M1.00M1.50M2.00M2.50M3.00Mtime steps0100020003000returnHopper-v3vanilla pssvfpssvf-fingerprintpssvf-fingerprint weighted sampling02004006008001000gradient steps0100200300400returnUnder review as a conference paper at ICLR 2023\n\nsee in Figure 13 that as the MSE loss goes to zero when fitting the 3 transitions, the return of the policy increases until it almost matches the optimal value. In this experiment we train a PSSVF with 5 probing states following Algorithm 1 for 2M time steps. We manually select a subset of 3 probing states and act in those states using the learned policy. We then fit a new policy over those 3 transition. We use a batch size of 3 and a learning rate of 2e − 5 to fit the new policy. The other hyperparameters are the same as in the main experiments (see Appendix A.2).\n\nFigure 13: On the left: return of the policy learned using 3 transitions in Swimmer. On the right, MSE for fitting the 3 transitions. Average over 5 independent runs and 95% bootstrapped confidence interval.\n\nLearning Hopper with 5 states We repeat the same experiment of cloning near-optimal behaviour from a few states in the Hopper environment. Using the action of a good policy (whose return is 2450) in 5 probing states, we are able to fit a new policy and obtain a final return of 2200. We use a batch size of 5 and a learning rate of 1e − 4 for the randomly initialized policy. All other hyperparameters are like in the Swimmer experiment with 3 transitions. Figure 14 shows the learning curve, while Figure 16 relates the behavior of the policy learned using the 5 transitions to the distance of the current agent’s state to the probing states. The 5 probing actions { ̃ak}5\n\nk=1 are:\n\n ̃a1 = [0.4859, 0.6492, −0.7818], ̃a2 = [0.9251, 0.9100, 0.2322], ̃a3 = [0.0405, 0.0475, 0.9091], ̃a4 = [0.2925, −0.4677, −0.1329], ̃a5 = [0.7578, 0.4327, −0.1521]. We observe a similar behavior of the Swimmer experiments (Figure 15), where the action chosen by the agent is similar to the probing action of a probing state whenever the agent’s state is close to the probing state. Although the dynamics in Hopper are more complex than in Swimmer, 5 probing states are enough to make the agent perform non-trivial actions in the environment.\n\nFigure 14: On the left: return of the policy learned using 5 transitions in Hopper. On the right, MSE for fitting the 5 transitions. Average over 5 independent runs and 95% bootstrapped confidence interval.\n\n19\n\n010002000300040005000gradient steps0100200300return010002000300040005000gradient steps0.00.10.20.30.40.50.60.7loss010002000300040005000gradient steps05001000150020002500return010002000300040005000gradient steps0.000.020.040.060.080.10lossUnder review as a conference paper at ICLR 2023\n\nFigure 15: Behavior of the policy learned from 3 probing state-probing action pairs in Swimmer. From top to bottom: each component of the state vector across time steps in an environment simulation; each component of the action vector; L2 distance of the current state to each of the 3 probing states used for learning.\n\nFigure 16: Behavior of the policy learned from 5 probing state-probing action pairs in Hopper. From top to bottom: each component of the state vector across time steps in an environmental simulation; each component of the action vector; L2 distance of the current state to each of the 5 probing states used for learning.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nVisualization of probing states in RL In Figure 15 we show the three probing states of the last experiment on Swimmer. In environments like Hopper and Walker, probing states might not correspond to a real state in the environment (e.g. some components of the probing state are outside a specific range). We notice that this is usually not the case and that the learned probing states generally correspond to valid environmental states. Moreover, we observe that probing states tend to get closer to certain critical situations over learning. These are states where certain actions have a significant effect on the future. In the Ant environment, we notice that all components of the probing state vector from index 28 to 111 learn a value of around 1e − 8. Interestingly, the process of fingerprinting discovers this ‘bug’ in MuJoCo 2.0.2.2 that sets all contact forces in Ant to zero. Since these components of the state vector remain constant during the environmental interactions, and are therefore not relevant for learning, the PSSVF learns to set them to zero as well.\n\nFigure 17 shows the evolution of the Swimmer environment from the selected probing states when no action is taken. The 3 probing states reported are those used for the experiment of Figures 13 and 15.\n\nFigure 17: From top to bottom: the three learned probing states in Swimmer. From left to right: Evolution of the environment over time steps. The agent is initialized in the probing state and performs no action.\n\nFigure 18: From top to bottom: the 5 learned probing states on Hopper. From left to right: various time steps in the environment. The agent is initialized in the probing state and performs no action.\n\nFigure 18 shows 3 out of the 5 learned probing states on Hopper in the experiment of Figures 14 and 16. The other 2 probing states do not correspond to valid states in Hopper and are therefore not visualized. No action is taken from the probing state and the environment is allowed to evolve naturally from the probing state. The duration of interaction differs in each row of the figure as termination occurs at different points from the probing states.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 19: Evolution of the environment from a probing state when (Top) no actions taken, (Bottom) the first action in the probing state is taken using a good policy. Then no action is performed.\n\nFigure 19 supports our hypothesis that some probing states might capture critical scenarios. In the considered probing state from Hopper we see that taking no action results in immediate failure as indicated by the shorter span of interaction in the top panel of Figure 19. In contrast, acting for a single time-step with a successful policy in that situation helps the agent survive and prolongs the interaction (bottom panel of Figure 19).\n\nAdditional probing states for all environments can be seen in animated form on the website https: //anonymous260522.github.io/.\n\nC SOCIETAL IMPACT\n\nOur work makes algorithmic contributions to actor-critic approaches for reinforcement learning and does not focus on specific real-world applications. Using our PSSVF for offline improvement of policies (as shown in our MNIST experiment) could help mitigate risks from directly applying deep neural network policies to online situations in the real world.\n\nD ENVIRONMENT DETAILS\n\nMujoco is made available with Apache License 2.0. The MNIST dataset is available through the creative commons license CC BY-SA 3.0.\n\n22",
    "reference": "# Summary Of The Paper\n\nThis work combines two existing approaches: representing a policy based on its behavior in a set of probing states and finding a successful policy via a critic that estimates the return of any input policy.\n\nThe ability to automatically learn probing states is shown via MNIST.\nThe performance on a few continuous control tasks is demonstrated.\nThe ability to use a trained critic to find a successful linear policy is also shown.\n\n# Strength And Weaknesses\n\nThis work identifies a promising combination of two existing methods.\nIf probing states are a more efficient representation for complex policies, then this method is of interest.\n\nThe authors claim that \"Flattening the policy parameters ... is difficult to scale to larger policies,\" but do not show the new method scaling.\nUnfortunately, the proposed method is not shown to scale to larger domains. Empirical evaluation is required to confirm favorable scaling, but all of the domains considered can be solved with simple policies.\nI am skeptical that this method scales better than using the policy parameters: a strength of policy gradient methods is that the policy complexity generally scales more slowly than the environment complexity. However, this method requires coverage of the state space, so the number of required crucial states may scale quickly.\nThe key experiment is shown in the appendix (Fig 10) and is quite limited in scope. It specifically uses the environments where few probing states are needed to learn a policy.\n\nMinor Comments:\n- The \"Policy fingerprinting\" paragraph should be divided to more clearly partition novel contributions from existing work.\n- By changing the rewards for Hopper, Walker, and Ant, those environments are no longer being solved (only modified versions of them). Likewise, gamma is part of the MDP; reporting total return (gamma=1) when gamma is 0.99 for only some evaluated methods is incorrect.\n- Sec 4.3: \"trough\" -> \"through\"\n- The limitation of the proposed method to deterministic policies is used to motivate baseline selection, but this limitation is a choice rather than part of the problem setup. To use this line of reasoning, one should also express why deterministic policies are advantageous.\n\nQuestions:\n- How were domains selected for evaluation? Why are only two environments used for Fig 10?\n- What is the behavior of your method with stochastic policies?\n- Can your approach perform well when the policies have different architectures during training?\n- Can vanilla PSSVF be used with weighted sampling?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe combination of methods is novel, but the utility is not demonstrated.\n\nIn addition, the experiments are oddly limited:\n- only continuous control problems are used for main experiments\n- only deterministic policies are used for main experiments\n- only environments where few crucial states are needed are used for evaluation\nTo support the claim that probing states are a better representation, thorough comparisons between different representations should be performed.\nIn particular, to demonstrate that probing states scale better, evaluations should be performed with more complex environments.\n\nThe prioritization of content inclusion needs to be improved. Key experiments are in the appendix while lots of content in the main body can be cut or moved to the appendix.\nNote that \"reviewers are not required to read the appendix\" (ICLR 2023 CFP). I have still read the appendix, but I urge the authors to move key experiments to the main body.\nExamples:\n- Sec 2 and 3 contain lots of non-novel content that is not needed within this work.\n- Likewise, \"a demonstration that fingerprinting can learn interesting states in MNIST\" is of limited interest (no hypothesis is being tested; no baselines are used for comparison).\n- Also, learning a near-optimal linear policy does not show that this method has an advantage over directly using policy parameters (i.e., this ability is due to PBVF).\n- Evaluating the contribution of policy fingerprinting is crucial, yet it is in the appendix.\n\n# Summary Of The Review\n\nThe proposed method is motivated by the difficulty scaling a policy-parameter-based approach. However, this benefit is not supported.\nAdditionally, the overall experiments are limited without sufficient justification.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTOWARD LEARNING GEOMETRIC EIGEN-LENGTHS CRUCIAL FOR ROBOTIC FITTING TASKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nSome extremely low-dimensional yet crucial geometric eigen-lengths often determine whether an object can be fitted in the environment or not. For example, the height of an object is important to measure to check if it can fit between the shelves of a cabinet, while the width of a couch is crucial when trying to move it through a doorway. Humans have materialized such crucial geometric eigen-lengths in common sense since they are very useful in serving as succinct yet effective, highly interpretable, and universal object representations. However, it remains obscure and underexplored if learning systems can be equipped with similar capabilities of automatically discovering such key geometric quantities in doing robotic fitting tasks. In this work, we therefore for the first time formulate and propose a novel learning problem on this question and set up a benchmark suite including the tasks, the data, and the evaluation metrics for studying the problem. We explore potential solutions and demonstrate the feasibility of learning such eigen-lengths from simply observing successful and failed fitting trials. We also attempt geometric grounding for more accurate eigen-length measurement and study the reusability of the learned geometric eigen-lengths across multiple tasks. Our work marks the first exploratory step toward learning crucial geometric eigen-lengths and we hope it can inspire future research in tackling this important yet underexplored problem.\n\n1\n\nINTRODUCTION\n\nFigure 1: Example tasks and the hypothesized crucial geometric measurements by humans.\n\nConsider a robot tasked with placing many small objects on warehouse shelves, where both the objects and the shelves have diverse geometric configurations. While the robot can simply try to accomplish the task by trial-and-error, to us as humans, it is clear that certain placements should not be attempted because they will obviously fail. For example, we should not attempt to place a tall object on a shelf whose height is too low. We base this judgement on the estimation of a critical geometric eigen-length or measurement, the height of the object and the shelf, whose comparison allows a quick estimate of task feasibility.\n\nWhile object height is an example of important eigen-lengths of an object that is crucial for the above shelf placement task, it is not hard to think of many other types of object eigen-lengths for other fitting tasks. Figure 1 presents some other example tasks together with the presumable geometric eigen-lengths based on human common sense. For example, the geometric eigen-length diameter is important for the task of stacking plates in different sizes (Figure 1, (a)), while the width and length\n\n1\n\nFit under the table: height and widthPlace into the box: width and lengthPut into and close the drawer: height, width, and lengthStack plates: diameter(a)(b)(c)(d)Under review as a conference paper at ICLR 2023\n\nof an object are crucial geometric eigen-lengths for deciding if one can put an arbitrary shape object into an open box (Figure 1, (c)).\n\nHaving such extremely low-dimensional yet crucial geometric eigen-lengths extracted as the representations for objects is certainly beneficial for designing learning systems for robotic fitting tasks. One telling evidence is that we humans have naturally built up the vocabulary of geometric key quantities, such as height, width, and diameter, when perceiving and modeling everyday objects, and used them to perform various object fitting tasks. Besides being succinct yet effective abstractions of objects for quickly estimating the feasibility for the downstream fitting tasks, such crucial geometric eigen-lengths are also highly interpretable, which exposes the principled reasoning process behind the feasibility checking, and universal, as they are generally applicable to objects with arbitrary shape and useful across different downstream tasks.\n\nCurrent research in representation learning for computer vision and robotics has mostly been focusing on learning high-dimensional latent codes or injecting human knowledge as inductive bias for learning structured representations. While learning high-dimensional latent codes provides total flexibility learning any useful feature for mastering the downstream tasks, these latent codes are highdimensional, hard to interpret, and may be prone to overfitting to the training domain. For structured representations, though researchers have explored using different kinds of object representations, such as bounding boxes (Tulsiani et al., 2017) and key points (Manuelli et al., 2019), to accomplish various downstream tasks in computer vision and robotics, these structure priors are manually specified based on human knowledge about the tasks. In contrast, we aim to explore the automatic discovery of low-dimensional yet crucial geometric quantities for robotic fitting tasks while injecting the minimal human prior knowledge – only assuming that we are measuring eigen-lengths of the input objects.\n\nIn this paper, we first propose to study a novel learning problem on discovering low-dimensional geometric eigen-lengths crucial for fitting tasks and set up the benchmark suite for studying the problem. As illustrated in Figure 2, given a fitting task (putting the bowl inside the drawer of the table) that involves a scene geometry (the table) and an object shape (the bowl), we are interested in predicting whether the object can fit in the scene accomplishing the task or not, via discovering a few crucial geometric eigen-lengths and composing them into a task program which outputs the final task feasibility estimation. To study the problem, we also define a set of commonly seen robotic fitting tasks, generate large-scale data for the training and evaluating on each task, and set up a set of quantitative metrics for evaluating and analyzing the method performance and if the emergent geometric eigen-lengths match the desired ones humans usually use.\n\nWe also explore potential solutions to the proposed learning problem and present several of our key findings. First of all, we will show that learning such low-dimensional key geometric eigen-lengths are achievable from only using weak supervision signals such as the success or failure of training fitting trials. Secondly, the learned crucial geometric eigen-lengths can be more accurately measured if geometric grounding is allowed and attainable for certain fitting tasks. Finally, we make an initial stab at exploring how to share and re-use the learned geometric eigen-lengths across different tasks and even for novel tasks. Marking the first step defining and exploring this important yet underexplored problem, we hope our work can draw people’s attention to this task and inspire future research in designing solutions tackling it.\n\nTo summarize, this work makes the following contributions:\n\n• We propose a novel learning problem on discovering low-dimensional geometric eigen-lengths\n\ncrucial for fitting tasks;\n\n• We set up a benchmark suite for studying the problem, including a set of fitting tasks, the dataset for each task, and a range of quantitative and qualitative metrics for thorough performance evaluation and analysis;\n\n• We explore potential solutions to the proposed learning problem and present some key take-away\n\nmessages summarizing both the successes and unresolved challenges.\n\n2 RELATED WORK\n\nLearning Geometry Abstraction. A long line of research has focused on learning low-dimensional and compact abstraction for input geometry. Given as input a 2D or 3D shape, past works have studied learning various geometric abstraction as the shape representation, such as bounding boxes (Tulsiani et al., 2017; Sun et al., 2019), convex shapes (Deng et al., 2020), Gaussian mixtures (Genova et al.,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Proposed Learning Paradigm where we first predict a set of geometric eigen-lengths from the input geometries, then compose them using a task program to get the final task output.\n\n2019; 2020), superquadratics (Paschalidou et al., 2019; 2020), parametric curves (Reddy et al., 2021) and surfaces (Sharma et al., 2020; Smirnov et al., 2020), etc.. Most of these works use geometry fitting as the primary objective. Our work, however, focuses on discovering geometric abstraction that can help solve the downstream manipulation tasks instead of reconstruction.\n\nThere are also previous works exploring ways to learn task-specific geometry representation for manipulation tasks. For example, researchers have tried to learn key points (Manuelli et al., 2019; Qin et al., 2020; Wang et al., 2020; Chen et al., 2020; Jakab et al., 2021; Chen et al., 2021) and affordance information (Kim & Sukhatme, 2014; Mo et al., 2021a;b; Turpin et al., 2021; Deng et al., 2021) for robotic manipulation tasks. These works mostly pre-define the types of geometry abstraction and the downstream policies to use the extracted shape summaries, and the abstraction is mostly dense or high dimensional. In this paper, we aim for useful geometric eigen-lengths and ways to automatically discover and compose them for solving manipulation tasks.\n\nDisentangled Visual Representation Learning. Another line of work focuses on unsupervised representation learning techniques that pursue disentangled and compositional latent representations for visual concepts. For example, InfoGAN (Chen et al., 2016), beta-VAE (Higgins et al., 2017), and many more works (Higgins et al., 2016; Siddharth et al., 2017; Yang et al., 2020) discover disentangled features, each of which controls a certain aspect of visual attributes, usually with reconstruction as the objective. In contrast to their primary objectives of controllable reconstruction or generation, we explore the problem of learning geometric eigen-lengths driven by the goal of accomplishing downstream fitting tasks. Also, our task involves reasoning over two geometric inputs and comparing the extracted eigen-lengths on both inputs, while these previous works on disentangled visual representation learning factor out visual attributes for a single input datum.\n\n3 LEARNING PROBLEM FORMULATION\n\nGiven a robotic fitting task T ∈ T , we aim to learn very few but the crucial geometric eigen-lengths LT (e.g., width, length, height) of the object shape O ∈ O and the environment geometry E ∈ E that are useful for checking the feasibility of fitting O into E under the task T . Figure 2 presents an example of the proposed learning problem where the task is to put the bowl (O) inside the drawer of the cabinet (E). In this example, the width, length, height of the drawer and the bowl are the crucial desired geometric eigen-lengths (LT ) and we can compose them in a task program to output the final task feasibility prediction. We consider each eigen-length L ∈ LT as a function mapping from the input object shape O or the environment geometry E to a scalar value for the eigen-length measurement, i.e. L : O ∪ E → R. After obtaining the eigen-length measurements for both the object and environment inputs, i.e. {L(O)|L ∈ LT } and {L(E)|L ∈ LT }, we perform pairwise comparisons between the corresponding eigen-lengths checking if L(O) < L(E) holds for every L ∈ LT . The task of fitting O in E under the task T is predicted as successful if all the conditions hold and as failed if any condition does not hold. This format of task program is based on the intuition that in fitting tasks, we require the object to be “smaller” than the parts of the environment affording the action. Durining training, the learning systems see many fitting trials over different objects and environment geometric configurations together with their ground-truth fitting feasibility, i.e. {(Oi, Ei, Successful/Failed)|i = 0, 1, 2, · · · }. The goal is to learn eigen-length functions based on which correct prediction of task feasibility given test input (Otest, Etest) can be made.\n\n3\n\nTask Output>Task ProgramInput Geometry...Env HEigen-LengthsEnv W...Obj HObj WAND>Under review as a conference paper at ICLR 2023\n\n4 CAN GEOMETRIC EIGEN-LENGTHS BE LEARNED FROM BINARY TASK\n\nSUPERVISION?\n\nIn this work, we are interested in learning geometric eigen-lengths that are crucial for downstream tasks. We hope to achieve automatic discovery of these eigen-lengths from doing tasks as it requires the least human prior and allows maximum flexibility. Therefore, we start with the minimum form of supervision and explore the following question: given only binary task success/failure supervision, is it possible to learn geometric eigen-lengths of input geometries that are sufficient for the task?\n\n4.1 TESTBED FOR EIGEN-LENGTH LEARNING\n\nFigure 3: Summary of tasks and their human-hypothesized key measurements/eigen-lengths.\n\nWe start by curating a set of tasks as the testbed for the learning problem, as summarized in Fig. 3. For each task, we build a large-scale dataset comprising diverse shapes and configurations. Task Design Principles We design the tasks to (1) cover a wide range of geometries, including synthetic, simple primitive shapes and more complex ones like ShapeNet objects; (2) facilitate the analysis and interpretation of learned eigen-lengths. Specifically, here we base the analysis on comparisons to human-hypothesized eigen-lengths: given a task, humans can identify related key eigen-lengths (referred to as “ground truth” in the following), e.g., object height when putting them on shelves. Comparing the learned eigen-lengths to these “ground truth” may provide important insights. To achieve this, we need accessible ground truth eigen-lengths to begin with. Primitive shapes like cylinders are ideal as they are parameterized by eigen-lengths like radius and height.\n\nTask Specifications exists in a certain environment, specifically:\n\nIn all tasks, we aim to determine whether a placement/motion of the object\n\n(a) Tube passing. (Tube) Pass an object through a rectangular tube. A tube is a cuboid without the front and back faces. Width and height of the tube/object are the key eigen-lengths to compare.\n\n(b) Cylinder fitting. (Cylinder) Place an object into a cylindrical container. Bounding sphere radius of the object in XY plane and its height, as well as the radius and height of the cylinder container are the key eigen-lengths.\n\n(c) Sphere fitting. (Sphere) Place an object into a spherical container. Radius of the bounding\n\nsphere of the object and the container is the key eigen-length.\n\n(d) Container fitting. (Fit) Place an object into cavities in another ShapeNet container object. Example cavities include drawers or shelves (See Fig. 3d) of furniture. Most cavities have cuboid-like shapes. Thus, key eigen-lengths are width, length and height of cavities and objects.\n\n(e) Countertop placing. (Top) Place an object on top of another ShapeNet environment object, such that its projection along the gravity axis is fully enclosed by the environment countertop. Width and length of the countertop surface and the object are key eigen-lengths.\n\n(f) Mug hanging. (Mug) Hang a mug on a mug holder by its handle. The holder is a cylindershaped rod. Key eigen-lengths are the distance between sides of the mug handle and the diameter (or equivalently, the radius) of the mug holder rod.\n\nData Generation Details For objects to be fitted in tasks (a)-(e), we use ∼1200 common household object models from 8 training and 4 testing categories in ShapeNet (Chang et al., 2015), following Mo et al. (2021b). During data generation, we apply random scaling to the object model, then sample N = 1024 points from the object surface. Note that we also apply a random rotation to the object. In (d),(e), we use furniture and appliances from ShapeNet as the environment geometry, including ∼550 shapes from 7 object categories. In (f), we use ∼200 ShapeNet mugs. We randomly sample the parameters of primitive shapes and the scaling factors of ShapeNet shapes, then sample M = 1024 points from their surfaces. For all tasks, we generated 75k training and 20k testing environment-object pairs. Please refer to Appendix A.3 for more data generation details.\n\n4\n\n(d) Container Fitting(e) Countertop Placing(a) Tube Passing(b) Cylinder Fittingwidthheightheightradiuswidthlengthheight(c) Sphere Fittingradius(f) Mug Hangingholder radius widthlengthUnder review as a conference paper at ICLR 2023\n\nFigure 4: Network architectures. (a) A minimal eigen-length learning pipeline where we separately encode environment and object into eigen-length values, perform pair-wise comparison, and take the logical AND of results. (b) A geometry-grounded framework where we first predict vectors and points as the geometry grounding, then compute eigen-lengths from them.\n\nFigure 5: Correlation Analysis. Each plot shows the relationship between one learned eigen-length (Y coord.) and its matching “ground truth” measurement (X coord.). Higher R2 values imply a stronger correlation.\n\n4.2 A MINIMAL NETWORK ARCHITECTURE\n\nIntuitively, we can measure the object and the environment separately and see if the object is “smaller\" than the environment. Thus we come up with the minimal network architecture shown in Fig. 4 (a). We separately map the object and environment geometries into two sets of eigen-lengths, perform pairwise comparisons between them, and compose comparison results using logical AND.\n\nConcretely, we encode object point cloud O and environment point cloud E using two PointNet (Qi et al., 2017) networks, ObjNet and EnvNet. Both networks output S-dim vectors ⃗Lobj = (Lobj , Lenv S ). We then compute task success 2\nas ˆT (E, O) = (cid:86)S (O)]. During training, we use a differentiable approximation ̃T (E, O) = (cid:81)S (O))/τ ), where τ is a learnable parameter. We set S = 1 for (c) Sphere, (f) Mug, S = 2 for (a) Tube, (b) Cylinder, S = 3 for (d) Fit, (e) Top.\n\nS ), ⃗Lenv = (Lenv (E) > Lobj (E) − Lobj\n\n2 , . . . , Lobj s=1[Lenv s=1 σ((Lenv\n\n1 , Lobj\n\n, . . . , Lenv\n\n1\n\ns\n\ns\n\ns\n\ns\n\n4.3 ANALYSIS OF LEARNED EIGEN-LENGTHS\n\ns′,i, Lpred\n\nWe analyze the eigen-lengths learned by the network by comparing them to “ground truth” eigenlengths as shown in Fig. 3. For each task, we randomly sample N = 512 test data points and obtain the corresponding N eigen-length predictions {Lpred s,i }i=0,...,N −1 for each of the S learned eigen-lengths, as well as N values {Lgt s′,i}i=0,...,N −1 for each of the S′ “ground truth” eigen-lengths. For each pair of predicted and “ground truth” eigen-lengths (s, s′), we draw a scatter plot of points (Lgt s′,i ) and perform least squares linear regression over them to get corresponding R2-scores. We match the predictions and groundtruths by maximizing the sum of R2-scores and show the scatter plots of matched pairs in Fig. 5. Note that in (e) Top, since we predict S = 3 eigen-lengths while there are only S′ = 2 groundtruth eigen-lengths, we show the unmatched prediction with its most correlated groundtruth. For complete S × S′ plots, please refer to Appendix B.2. Learned eigen-lengths are strongly correlated with human-hypothesized measurements. As Fig. 5 shows, R2 values between predictions and “ground truths” are close to or greater than 0.9 except for the redundant prediction slot 3 in (e) Top. They also have clear one-to-one correspondences with ground truth in tasks with multiple eigen-lengths, suggesting good disentanglement is learned. Knowing the number of eigen-lengths beforehand is not a requirement for successful learning. The number S of eigen-lengths to learn is a hyperparameter set before learning. However, it does\n\n5\n\n(cid:40)(cid:81)(cid:89)(cid:49)(cid:72)(cid:87)(cid:40)(cid:81)(cid:89)(cid:3)(cid:51)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:38)(cid:79)(cid:82)(cid:88)(cid:71)(cid:50)(cid:69)(cid:77)(cid:49)(cid:72)(cid:87)(cid:50)(cid:69)(cid:77)(cid:3)(cid:51)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:38)(cid:79)(cid:82)(cid:88)(cid:71)(cid:40)(cid:81)(cid:89)(cid:3)(cid:40)(cid:76)(cid:74)(cid:72)(cid:81)(cid:16)(cid:47)(cid:72)(cid:81)(cid:74)(cid:87)(cid:75)(cid:29)(cid:3)(cid:47)(cid:20)(cid:72)(cid:81)(cid:89)(cid:15)(cid:3)(cid:47)(cid:21)(cid:72)(cid:81)(cid:89)(cid:15)(cid:17)(cid:17)(cid:17)(cid:15)(cid:3)(cid:47)(cid:54)(cid:72)(cid:81)(cid:89)(cid:50)(cid:69)(cid:77)(cid:3)(cid:40)(cid:76)(cid:74)(cid:72)(cid:81)(cid:16)(cid:47)(cid:72)(cid:81)(cid:74)(cid:87)(cid:75)(cid:29)(cid:3)(cid:47)(cid:20)(cid:82)(cid:69)(cid:77)(cid:15)(cid:3)(cid:47)(cid:21)(cid:82)(cid:69)(cid:77)(cid:15)(cid:17)(cid:17)(cid:17)(cid:15)(cid:3)(cid:47)(cid:54)(cid:82)(cid:69)(cid:77)(cid:305)(cid:11)(cid:11)(cid:47)(cid:20)(cid:72)(cid:81)(cid:89)(cid:3)(cid:16)(cid:3)(cid:47)(cid:20)(cid:82)(cid:69)(cid:77)(cid:12)(cid:18)(cid:7532)(cid:12)(cid:3)(cid:305)(cid:11)(cid:11)(cid:47)(cid:21)(cid:72)(cid:81)(cid:89)(cid:3)(cid:16)(cid:3)(cid:47)(cid:21)(cid:82)(cid:69)(cid:77)(cid:12)(cid:18)(cid:7532)(cid:12)(cid:3)(cid:3)(cid:305)(cid:11)(cid:11)(cid:47)(cid:54)(cid:72)(cid:81)(cid:89)(cid:3)(cid:16)(cid:3)(cid:47)(cid:54)(cid:82)(cid:69)(cid:77)(cid:12)(cid:18)(cid:7532)(cid:12)(cid:7502)(cid:11)(cid:36)(cid:49)(cid:39)(cid:12)(cid:51)(cid:85)(cid:72)(cid:71)(cid:3)(cid:47)(cid:68)(cid:69)(cid:72)(cid:79)(cid:57)(cid:72)(cid:70)(cid:87)(cid:82)(cid:85)(cid:49)(cid:72)(cid:87)(cid:40)(cid:81)(cid:89)(cid:51)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:38)(cid:79)(cid:82)(cid:88)(cid:71)(cid:58)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)(cid:39)(cid:76)(cid:86)(cid:87)(cid:85)(cid:76)(cid:17)(cid:51)(cid:85)(cid:82)(cid:77)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:47)(cid:72)(cid:81)(cid:74)(cid:87)(cid:75)(cid:57)(cid:72)(cid:70)(cid:87)(cid:82)(cid:85)(cid:3)(cid:89)(cid:58)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:49)(cid:72)(cid:87)(cid:51)(cid:82)(cid:76)(cid:81)(cid:87)(cid:86)(cid:3)(cid:83)(cid:15)(cid:3)(cid:84)(cid:58)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:72)(cid:71)(cid:54)(cid:88)(cid:80)(cid:50)(cid:69)(cid:77)(cid:51)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:38)(cid:79)(cid:82)(cid:88)(cid:71)(cid:47)(cid:3)(cid:32)(cid:3)(cid:89)(cid:55)(cid:11)(cid:84)(cid:3)(cid:16)(cid:3)(cid:83)(cid:12)(cid:40)(cid:81)(cid:89)(cid:3)(cid:40)(cid:76)(cid:74)(cid:72)(cid:81)(cid:16)(cid:47)(cid:72)(cid:81)(cid:74)(cid:87)(cid:75)(cid:50)(cid:69)(cid:77)(cid:3)(cid:40)(cid:76)(cid:74)(cid:72)(cid:81)(cid:16)(cid:47)(cid:72)(cid:81)(cid:74)(cid:87)(cid:75)(cid:11)(cid:68)(cid:12)(cid:11)(cid:69)(cid:12)D7XEH<3UHGLFWLRQ5 D7XEH=3UHGLFWLRQ5 E&\\OLQGHU53UHGLFWLRQ5 E&\\OLQGHU+3UHGLFWLRQ5 F6SKHUH53UHGLFWLRQ5 I0XJ+ROGHU53UHGLFWLRQ5 G)LW2EMHFW;3UHGLFWLRQ5 G)LW2EMHFW<3UHGLFWLRQ5 G)LW2EMHFW=3UHGLFWLRQ5 H7RS2EMHFW;3UHGLFWLRQ5 H7RS2EMHFW<3UHGLFWLRQ5 H7RS2EMHFW;3UHGLFWLRQ5 Under review as a conference paper at ICLR 2023\n\nnot have to be the exact number of relevant eigen-lengths. As shown in (e) Top, when we have more slots for eigen-lengths than needed, “ground truth” eigen-lengths are still captured by the first two predictions. The third prediction does not strongly correlate with any “ground truth”. A further probe reveals that comparisons of this eigen-length almost never (only in 0.4% of the cases) contribute to the final result, outputting True most of the time. The network learns a pair of degenerate eigen-lengths as there is no more necessary information to capture.\n\n5 CAN GEOMETRY GROUNDINGS BE DISCOVERED FOR EIGEN-LENGTHS?\n\nWhile Fig. 5 shows strong correlation between learned eigen-lengths and “ground truth”, their relationship is not always perfectly linear, as can be observed in (d) Fit and (e) Top with complex geometries. Even in more linear cases, the scaling and offset make the raw eigen-length value hard to understand, e.g., negative “length” values are less intuitive. As eigen-lengths can be seen as measurements of the object, many of them have sparse supports or geometry groundings on the objects, e.g., height is the distance between the base plane supporting the object and its highest point. These geometry groundings anchor the corresponding eigen-length values, provide an intuitive explanation of these values, and usually carry geometric/semantic importance themselves. We are therefore interested in the following question: can we ground the eigen-lengths on geometry? From a high level, instead of directly predicting eigen-length values, if we first predict some geometric entities like points, vectors, and planes, then derive eigen-lengths from them, is it possible to learn meaningful eigen-lengths and geometry groundings?\n\n5.1 GROUNDING EIGEN-LENGTH PREDICTIONS ON GEOMETRIC PRIMITIVES\n\nFigure 6: Eigen-Length Geometry Groundings. We ground each eigen-length L with a unit vector v and two parallel planes Πp, Πq with normal v. L is computed as the distance between Πp, Πq.\n\nConsider fitting tasks like (d) Container Fitting and (e) Countertop Placing where the spaces affording the task can be roughly described by a set of parallel planes. 1 To compute the success label of the task, say fitting an object into a nightstand, we can measure the size of the spaces of interest in the environment (the drawer part) along important directions (its main axes) and comparing it to the measurement of the object. Inspired by this, we ground a pair of eigen-lengths on a tuble of unit vector and two planes (⃗v, Πp, Πq) as illustrated in Fig. 6: we measure both the object and the environment along ⃗v. We take the object measurement as the diameter of the projection of the object point cloud O on the vector ⃗v, i.e. Lobj(O) = maxp∈O ⃗vT p − minp∈O ⃗vT p. For the environment, we use a pair of parallel planes Πp, Πq with normal ⃗v to separate out a certain region relevant to the task (the drawer), then measure the distance between the planes. In practice, we adopt the (point, normal) plane representation and predict a point pair (p, q) that determines the plane pair. The environment eigen-length is then computed as Lenv(E) = ⃗vT (q − p).\n\nFigure 4 (b) illustrates our network architecture. In VectorNet, we employ a PointNet classification backbone to extract global feature of the environment point cloud E ∈ RM ×3, then use an MLP to predict S 3D vectors {⃗vs}s=1,2,...,S. In WeightNet, we employ a PointNet segmentation backbone to extract per-point features, then use S × 2 MLPs with to predict S pairs of probability distributions W p s over the point cloud. The point coordinates of (ps, qs) are then computed as the weighted average of original point cloud coordinates, namely ps = W p\n\nT E, qs = W q\n\ns , W q\n\nT E.\n\ns\n\ns\n\n1Note that other tasks may require other inductive bias. We focus on this type of tasks to study the feasibility\n\nof geometry-grounded eigen-length learning. We leave a more versatile system as future work.\n\n6\n\nlength < L ?Πp = (v, p)Πq = (v, q)vector v1vector v0vector v2vector v1vector v0vector v2length = Lheight=HvUnder review as a conference paper at ICLR 2023\n\n5.2 ANALYSIS OF LEARNED GEOMETRIC PRIMITIVES AND EIGEN-LENGTH VALUES\n\nFigure 7: Geometry Grounding Visualizations. We plot the learned vectors (as arrows) and planes (as disks) on top of input environment point clouds. We also show the object model next to point clouds for clearer view of object structure. For (d) Fit, we visualize predictions in two views for clarity. Please refer to Appendix B.1 for more visualizations.\n\nFigure 8: Improved correlation after using geometry groundings. We show scatter plots of predicted eigen-length (Y coord.) and their matching “ground truth” (X coord.) in (d) Fit and (e) Top. We perform the same correlation analysis and visualize the results in Fig. 8. Compared to Fig. 5, learned eigen-lengths are now almost equal to “groundtruth” thanks to the anchoring effect of the geometry grounding. The extra predicted eigen-length in (e) Top also behaves differently, capturing the same “ground truth” as another learned eigen-length. This suggests the regularization from geometry grounding makes learned eigen-lengths more likely to be meaningful measurements. It also reaffirms the fact that the number S of eigen-lengths we set in advance can be different from the actual number of necessary eigen-lengths. Please see Appendix E.4 for detailed discussion.\n\nWe also visualize the learned geometry groundings in Fig. 7. The learned vectors align with the main axes of object geometry. The learned planes overlap with tube surfaces in (a) Tube, surround the edge of countertops in (e) Top, and separate out the region of interest in (d) Fit, e.g. the higher one out of two storage spaces. These meaningful geometric entities provide a clear interpretation of learned eigen-lengths, e.g. in (e) Top’s case, red and green predictions coincide with each other and both capture the back-to-front length of the countertop.\n\n5.3 A STUDY ON THE ROBUSTNESS AND DATA EFFICIENCY OF GEOMETRY-GROUNDED\n\nEIGEN-LENGTHS\n\nGeometry grounding of eigen-lengths can be seen as a form of regularization. We are therefore curious how the introduction of geometry groundings may influence the robustness of models in extreme test setups, as well as their data efficiency. We compared the performance of (1) Direct, a no-eigen-length approach, where an MLP directly predicts the final label from the concatenation of object and environment latent features. (2) Implicit, the minimal eigen-length-based pipeline introduced previously; and (3) Grounded, the geometry-grounded version.\n\nTable 1 shows test performances in extreme test conditions with low resolution point clouds or with extraordinary object scalings. Eigen-length-based approaches exhibit much higher robustness.\n\nFig. 9 shows the trend of test performances as we change the size of training data. We also plot the difference between “ground truth” eigen-length measurement directions (local up and right) and predicted vectors as a way to quantify eigen-length quality. Results suggest that geometry-grounded version is more data efficient if meaningful geometry groundings emerge. When the training data\n\n7\n\n(cid:41)(cid:85)(cid:82)(cid:81)(cid:87)(cid:3)(cid:57)(cid:76)(cid:72)(cid:90)(cid:15)(cid:3)(cid:51)(cid:85)(cid:72)(cid:71)(cid:3)(cid:20)(cid:15)(cid:21)(cid:55)(cid:82)(cid:83)(cid:3)(cid:57)(cid:76)(cid:72)(cid:90)(cid:15)(cid:3)(cid:51)(cid:85)(cid:72)(cid:71)(cid:3)(cid:22)(cid:11)(cid:68)(cid:12)(cid:3)(cid:55)(cid:88)(cid:69)(cid:72)(cid:3)(cid:51)(cid:68)(cid:86)(cid:86)(cid:76)(cid:81)(cid:74)(cid:11)(cid:71)(cid:12)(cid:3)(cid:38)(cid:82)(cid:81)(cid:87)(cid:68)(cid:76)(cid:81)(cid:72)(cid:85)(cid:3)(cid:41)(cid:76)(cid:87)(cid:87)(cid:76)(cid:81)(cid:74)(cid:11)(cid:72)(cid:12)(cid:3)(cid:38)(cid:82)(cid:88)(cid:81)(cid:87)(cid:72)(cid:85)(cid:87)(cid:82)(cid:83)(cid:3)(cid:51)(cid:79)(cid:68)(cid:70)(cid:76)(cid:81)(cid:74)(cid:40)(cid:81)(cid:89)(cid:76)(cid:85)(cid:82)(cid:81)(cid:80)(cid:72)(cid:81)(cid:87)(cid:55)(cid:82)(cid:83)(cid:3)(cid:57)(cid:76)(cid:72)(cid:90)(cid:15)(cid:3)(cid:51)(cid:85)(cid:72)(cid:71)(cid:3)(cid:20)(cid:15)(cid:21)(cid:15)(cid:22)(cid:40)(cid:81)(cid:89)(cid:76)(cid:85)(cid:82)(cid:81)(cid:80)(cid:72)(cid:81)(cid:87)G)LW2EMHFW;3UHGLFWLRQ5 G)LW2EMHFW<3UHGLFWLRQ5 G)LW2EMHFW=3UHGLFWLRQ5 H7RS2EMHFW;3UHGLFWLRQ5 H7RS2EMHFW<3UHGLFWLRQ5 H7RS2EMHFW;3UHGLFWLRQ5 Under review as a conference paper at ICLR 2023\n\nTable 1: Performance on extreme test cases. All methods are trained on size = 1024 point clouds with width w and height h sampled from U ([0.4, 1.0]).\n\nDirect\n\nImplicit Grounded\n\nDefault # Points = 64 w, h ∼ U ([0.2, 0.4]) w, h ∼ U ([2.0, 3.0])\n\n99.00 76.80 72.10 82.44\n\n99.15 93.50 98.78 98.82\n\n99.65 96.17 99.72 99.63\n\nFigure 9: Trend of Left: test accuracy and Right: average angle between learned vector groundings and “groud truth” directions w.r.t. # training samples.\n\nis limited (< 3000 samples), however, the predicted directions of groundings are far from ground truth measurement directions, suggesting that the model fails to learn meaningful groundings for eigen-lengths, and thus the final accuracy is lower than Direct.\n\n6 CAN EIGEN-LENGTHS BE LEARNED IN MULTI-TASK SETTINGS AND\n\nAPPLIED TO NEW TASKS?\n\nAs humans, we are able to develop a library of useful measurements/eigen-lengths like height from past experience. Given a new task, instead of trying cluelessly, we would start with known measurements and investigate their role in the task. In this section, we ask if learned eigen-lengths can work in a similar way, i.e., given a set of training tasks, is it possible to learn a set of eigen-lengths from them? Further, given a novel task, can we learn to select a subset of learned eigen-lengths that are sufficient for it? In other words, can agents accumulate and transfer knowledge in the form of eigen-lengths?\n\n6.1 MULTI-TASK TESTBED\n\nWe design a set of tasks that share key eigen-lengths as the testbed for multi-task learning. As shown in Fig.10(a), we consider box-fitting tasks where the box only has a subset of six faces. Each mode of face existence corresponds to a different task with different geometric constraints. For example, to be able to fit, an object has to be narrower than the box in task 2 and shorter than the box in task 3. We set aside the box with all six faces present as the test task. We expect to learn width, height, and length from the training task set, and learn to use all of them during testing. By boxes with partial faces, we aim to mimic different types of cavities in the furniture, e.g., closed drawer as a box with all faces, an open space on the shelf as a box without the front face, etc.\n\nFigure 10: (a) Multi-Task Setting where each train task uses boxes with certain faces missing as the environment geometry, and test task uses a complete box; and (b) Learning Framework, where we use trainable masks to select eigen-length comparison results.\n\n6.2 MULTI-TASK LEARNING FRAMEWORK\n\nFig. 10(b) shows the multi-task learning framework we experiment with. From a high level, we learn a set of S eigen-lengths and allow each task to select relevant ones from them. This selection step is implemented as a learnable binary mask {mk s }s=1,2,...,S over eigen-lengths for each task Tk. We simply insert the mask in the AND-composition and compute the outcome for Tk as (cid:81)S\n\ns=1 mk\n\ns · σ((Lenv\n\ns\n\n(E) − Lobj\n\ns\n\n(O))/τ ).\n\n8\n\n7UDLQLQJ6DPSOHV$FFXUDF\\*URXQGHG'LUHFW7UDLQLQJ6DPSOHV$QJOHWR*7*URXQGHGEnv Measure: L1env, L2env,..., LSenvObj Measure: L1obj, L2obj,..., LSobjσ((L1env - L1obj)/τ) σ((L2env - L2obj)/τ) σ((LSenv - LSobj)/τ)Π(AND)Pred Label✕✕✕Test TaskTrain Task 1Train Task 2Train Task 3(a) Task Setting(b) Multi-Task Learning FrameworkUnder review as a conference paper at ICLR 2023\n\nTable 2: Multi-Task learning, novel task adaptation results. We finetune eigen-length-based methods on novel task for 1 epoch and compare them to the direct method trained from scratch for 1 and 100 epochs.\n\n(Single Task) Direct\n\n(Eigen-Length) Implicit\n\n(Eigen-Length) Grounded\n\nEpoch\n\n1\n\nTest Accuracy\n\n73.14\n\n100\n\n88.47\n\n1\n\n97.71\n\n1\n\n99.48\n\nDuring training, we optimize both the eigen-length prediction networks and a continuous version of per-task masks ̃mk ∈ [0, 1]. At test time, we freeze network weights and only learn a mask to choose from eigen-lengths learned during training. Notably, we limit the size of test task data to 10 batches (320 samples) to examine if learned eigen-lengths help in few-shot adaptation scenarios.\n\n6.3 MULTI-TASK LEARNING AND FEW-SHOT TEST TASK ADAPTATION\n\nWe experiment with both implicit and geometry-grounded eigen-length prediction networks. To analyze the learned eigen-lengths and per-task masks, we visualize learned geometry groundings that are selected (ms > 0.5) in each task in Fig. 11. Meaningful groundings are learned and correctly selected for each task, including the test task.\n\nTo explore whether eigen-lengths learned during training help quicker adaptation to new tasks, we compare the test task performance of Implicit, Grounded to Direct trained from scratch on the test task. All methods are limited to 10 batches of test task samples. As shown in Table 2, within one epoch of finetuning, methods based on the reuse of learned eigen-lengths already achieve high performance, surpassing Direct trained from scratch by a large margin, even when the latter has been trained for 100 epochs.\n\nFigure 11: Learned Geometry Grounding in Multi-Task Setting. We only show learned geometry grounding (vectors as arrows, planes as disks) selected by the mask in each task.\n\n7 CONCLUSION\n\nIn this work, we formulate a novel learning problem of automatically discovering low-dimensional geometric eigen-lengths crucial for fitting tasks. We set up a benchmark suite comprising a curated set of fitting tasks and corresponding datasets, as well as metric and tools for analysis and evaluation. We demonstrate the feasibility of learning meaningful eigen-lengths as sufficient geometry summary only from binary task supervision. We show that proper geometry grounding of the eigen-lengths contributes to their accuracy, interpretability, and robustness. We also make an initial attempt at learning shared eigen-lengths in multi-task settings and applying them to novel tasks.\n\nOur exploration suggests broad opportunities in this new research direction and reveals many challenges. For example, grounding eigen-length predictions on geometries requires reasonable choice of geometric primitives, which relies on inductive bias of the specific tasks considered. It would be a challenging future direction to build a universal framework that accommodates a wide range of tasks by leveraging all kinds of geometric primitives and inductive biases. In many task instances, we may have access to signals beyond binary success or failure, e.g., a possible placement position of the object. How to leverage these task signals in eigen-length learning remains an open problem. As a first-step attempt at defining and exploring the challenging problem of eigen-length learning, we do hope our work can inspire more researchers to work on this important yet underexplored direction.\n\n9\n\nTraining TasksTest TaskUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\n\nBoyuan Chen, Pieter Abbeel, and Deepak Pathak. Unsupervised learning of visual 3d keypoints for\n\ncontrol. In International Conference on Machine Learning, pp. 1539–1549. PMLR, 2021.\n\nNenglun Chen, Lingjie Liu, Zhiming Cui, Runnan Chen, Duygu Ceylan, Changhe Tu, and Wenping Wang. Unsupervised learning of intrinsic structural representation points. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9121–9130, 2020.\n\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 2180–2188, 2016.\n\nBoyang Deng, Kyle Genova, Soroosh Yazdani, Sofien Bouaziz, Geoffrey Hinton, and Andrea In Proceedings of the IEEE/CVF\n\nTagliasacchi. Cvxnet: Learnable convex decomposition. Conference on Computer Vision and Pattern Recognition, pp. 31–44, 2020.\n\nShengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, and Kui Jia. 3d affordancenet: A benchmark for visual object affordance understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1778–1787, June 2021.\n\nKyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T Freeman, and Thomas Funkhouser. Learning shape templates with structured implicit functions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7154–7164, 2019.\n\nKyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep implicit functions for 3d shape. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4857–4866, 2020.\n\nIrina Higgins, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria, Charles Blundell, Shakir Mohamed, and Alexander Lerchner. Early visual concept learning with unsupervised deep learning. arXiv preprint arXiv:1606.05579, 2016.\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017.\n\nTomas Jakab, Richard Tucker, Ameesh Makadia, Jiajun Wu, Noah Snavely, and Angjoo Kanazawa. Keypointdeformer: Unsupervised 3d keypoint discovery for shape control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12783–12792, 2021.\n\nDavid Inkyu Kim and Gaurav S Sukhatme. Semantic labeling of 3d point clouds with object affordance for robot manipulation. In 2014 IEEE International Conference on Robotics and Automation (ICRA), pp. 5578–5584. IEEE, 2014.\n\nLucas Manuelli, Wei Gao, Peter Florence, and Russ Tedrake. kpam: Keypoint affordances for\n\ncategory-level robotic manipulation. arXiv preprint arXiv:1903.06684, 2019.\n\nKaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani. Where2act: From pixels to actions for articulated 3d objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6813–6823, 2021a.\n\nKaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, and Leonidas Guibas. O2O-Afford: Annotation-free large-scale object-object affordance learning. In Conference on Robot Learning (CoRL), 2021b.\n\nDespoina Paschalidou, Ali Osman Ulusoy, and Andreas Geiger. Superquadrics revisited: Learning 3d shape parsing beyond cuboids. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDespoina Paschalidou, Luc van Gool, and Andreas Geiger. Learning unsupervised hierarchical part decomposition of 3d objects from a single rgb image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652–660, 2017.\n\nZengyi Qin, Kuan Fang, Yuke Zhu, Li Fei-Fei, and Silvio Savarese. Keto: Learning keypoint representations for tool manipulation. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 7278–7285. IEEE, 2020.\n\nPradyumna Reddy, Michael Gharbi, Michal Lukac, and Niloy J Mitra. Im2vec: Synthesizing vector graphics without vector supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7342–7351, 2021.\n\nJoseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE\n\nconference on computer vision and pattern recognition, pp. 7263–7271, 2017.\n\nGopal Sharma, Difan Liu, Subhransu Maji, Evangelos Kalogerakis, Siddhartha Chaudhuri, and Radomír Mˇech. Parsenet: A parametric surface fitting network for 3d point clouds. In European Conference on Computer Vision, pp. 261–276. Springer, 2020.\n\nNarayanaswamy Siddharth, Brooks Paige, Jan-Willem Van de Meent, Alban Desmaison, Noah D Goodman, Pushmeet Kohli, Frank Wood, and Philip HS Torr. Learning disentangled representations with semi-supervised deep generative models. arXiv preprint arXiv:1706.00400, 2017.\n\nDmitriy Smirnov, Matthew Fisher, Vladimir G Kim, Richard Zhang, and Justin Solomon. Deep parametric shape predictions using distance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 561–570, 2020.\n\nChun-Yu Sun, Qian-Fang Zou, Xin Tong, and Yang Liu. Learning adaptive hierarchical cuboid abstractions of 3d shape collections. ACM Transactions on Graphics (TOG), 38(6):1–13, 2019.\n\nShubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A. Efros, and Jitendra Malik. Learning shape abstractions by assembling volumetric primitives. In Computer Vision and Pattern Recognition (CVPR), 2017.\n\nDylan Turpin, Liquan Wang, Stavros Tsogkas, Sven Dickinson, and Animesh Garg. GIFT: Generalizable Interaction-aware Functional Tool Affordances without Labels. In Proceedings of Robotics: Science and Systems, Virtual, July 2021. doi: 10.15607/RSS.2021.XVII.060.\n\nJiayu Wang, Shize Lin, Chuxiong Hu, Yu Zhu, and Limin Zhu. Learning semantic keypoint representations for door opening manipulation. IEEE Robotics and Automation Letters, 5(4): 6980–6987, 2020.\n\nFanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao Su. SAPIEN: A simulated part-based interactive environment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\nYanchao Yang, Yutong Chen, and Stefano Soatto. Learning to manipulate individual objects in an image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6558–6567, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA IMPLEMENTATION DETAILS\n\nA.1 NETWORK ARCHITECTURE\n\nThe framework in Section 4 consists of a PointNet and an MLP output head that maps the PointNet global feature to S scalar values. The architecture is outlined below, where the numbers in the parenthesis refer to the number of channels in each layer. We use batch normalization and LeakyReLU after all FC layers, except for the output layer.\n\nPointNet\n\n \n\n\n\nPer-Point MLP(3 → 64 → 128 → 1024) ↓\nMax Pooling\n\n↓ MLP(1024 → 256 → S)\n\nOutput: S scalars.\n\nThe framework in Section 5 consists of VectorNet and WeightNet. VectorNet consists of a PointNet classification backbone and an MLP output head, as outlined below.\n\nPointNet\n\n \n\n\n\nPer-Point MLP(3 → 64 → 128 → 1024) ↓\nMax Pooling\n\n↓ MLP(1024 → 256 → 3S)\n\nOutput: S vectors.\n\nWeightNet consists of a PointNet segmentation backbone and 2S parallel MLP output heads, each outputs a weight distribution over all points, as outlined below.\n\nPointNet\n\n \n\n\n\nPer-Point MLP(3 → 64[per-point feature] → 128 → 1024) ↓\nMax Pooling[global feature]\n\nConcat(per-point feature, global feature) ↓\nMLP((1024 + 64) → 512 → 256 → 128) ↓\nOutput Weight MLPi(128 → 256 → 1), i = 1, 2, . . . , 2S ↓\n\nSoftMax\n\nOutput: 2S sets of per-point weights.\n\nWe use LeakyReLU and batch normalization after each FC layer except for the output layers.\n\nA.2 TRAINING DETAILS\n\nAll networks are implemented using PyTorch and optimized by the Adam optimizer, with a learning rate starting at 10−3 and decay by half every 10 epochs. Each batch contains 32 data points; each epoch contains around 1600 batches. We train models for ∼ 100 epochs on all tasks. The learnable parameter τ is initialized with τ = 1. All experiments are run on a single NVIDIA TITAN X GPU.\n\nA.3 DATASET DETAILS\n\nTable 3 and 4 summarizes the statistics of environment/object shapes used in our dataset. Each shape is drawn with probability in inverse proportion to the number of shapes in its category, such that each object category appears with similar frequency in the final dataset.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Environment Shape Statistics.\n\nBox Microwave\n\nRefrigerator\n\nSafe\n\nStorage Furniture\n\nTable Washing Machine\n\nTotal\n\nTrain Test\n\n21 7\n\n9 3\n\n34 9\n\n21 7\n\n272 73\n\n70 25\n\n13 3\n\n440 127\n\nTable 4: Object Shape Statistics.\n\nTrain\n\nBasket Bottle 16\n\n77\n\nBowl Box Can 65\n\n128\n\n17\n\nPot Mug 134 16\n\nTrashCan 25\n\nTotal 478\n\nTest\n\nBucket 33\n\nDispenser 9\n\nJar 528\n\nKettle 26\n\nTotal 554\n\nDuring data generation for the tasks where both the environment and the object are ShapeNet objects, we apply random scaling s ∼ U ([0.9, 1.1]) to the environment objects, set all joints to closed state and sample M = 1024 points from the object model. Given an object-environment pair, we randomly sample T = 1000 candidate positions in the environment point cloud, and check whether placement of the object at each candidate satisfy the task specification using SAPIEN (Xiang et al., 2020) simulation. If all candidates fail, we label the pair as negative, otherwise as positive. Specifically, the candidate positions are sampled from “applicable and possible regions\" following Mo et al. (2021b)’s definition. For example, we only consider points with upward facing normals, and for task (e) only consider points with close to highest z coordinates. We generated around 75K training data and 20K testing data for each task.\n\nB ADDITIONAL RESULTS\n\nB.1 GEOMETRIC GROUNDING VISUALIZATION AND FAILURE CASE DISCUSSION\n\nFig. 12 and 13 show more visualizations of the learned eigen-lengths in the three tasks from the main paper. Our framework is able to learn reasonable eigen-lengths that measure along crucial directions. These eigen-lengths are also grounded by planes that suggest the relevant part of object which supports the task. In experiments with primitive shapes as environments, the learned planes almost overlap with the box/tube faces. In experiments with ShapeNet container objects as environments, especially task (d) (Fit, or container fitting) as shown in Fig. 13, locating the relevant part becomes more challenging. As this usually involves finding cavities in a shape and selecting the largest one. Fig. 13 shows examples of our learned eigen-lengths, most of which make sense, as shown in (a)-(o). We are able to ignore irrelevant parts, e.g. the legs of tables, and find the part of object that affords the \"containment\" task, e.g. the drawer in (b), the closet in (c). When there are many cavities that afford the same task, the network picks the largest one, e.g. in (d) and (k).\n\nFailure Cases. We also observe some failure cases where the learned eigen-lengths are inaccurate. Fig. 13(p)-(t) shows the most representative ones. (p) shows a relatively complex shape, where the network struggles to find the correct width of the drawer. (q) and (r) show cases where the network finds the wrong cavity. According to our task definition, the object can only be placed in the drawer part in (q). Instead, the network finds the part on top of the drawer. In (r), the network finds the second largest cavity instead of the largest one at the bottom. (s) shows an extreme case where the height of the pizza box is much smaller than the other two extents. As objects usually have correlated extents, comparing height suffices most of the time. The network probably lacks the motivation to precisely capture the width and the length of the pizza box, resulting in the underestimation of width and length in (s). Finally, our formulation, i.e. the AND clause of three eigen-length comparisons, can not fully and precisely describe the nature of this task. The washing machine in (t) has a cylinder-shaped cavity, which our network tries to approximate by a cuboid, which is reasonable within the range of its expressive power but not accurate. Also, there could be shapes that do not have a \"largest\"\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: Additional qualitative results. We visualize the learned vectors and planes for (a) Tube Passing and (e) Countertop Placing. We show all eigen-lengths in the front(a)/top(e) view. We also show the underlying instances in task (e) countertop placing for a clearer understanding of the object structure. Note that though some joints are \"open\" for visualization purpose, all instances in the dataset are at their rest state.\n\ncavity, e.g. some drawers in a closet may be designed for tall and narrow things, while others are designed for flat things. To deal with arbitrary objects, the extents of both types of drawers are useful. Introducing more complex and flexible formulations, e.g. in Section D, would help better capture the complexity of the task.\n\nB.2 CORRELATION ANALYSIS RESULTS\n\nWe show here the scatter plots and correlation R2 values between all prediction eigen-lengths and all presumable geometric measurements. R2 value, or coefficient of determination, is a metric in [0, 1] reflecting linear correlation between two variables. The closer R2 is to 1, the more linearly correlated the two variables are. Given two set of samples xi, yi, where i = 1, 2, . . . , n, R2 is defined between yi and the least squares linear regression of yi on xi, ̃y(xi):\n\nR2 = 1 −\n\n(cid:80)\n\ni (yi − ̃y(xi))2 (cid:80) i (yi − ̄y)2\n\n,\n\nwhere ̄y = 1\n\nn\n\n(cid:80)\n\ni yi is the mean value of yi.\n\nResults from Eigen-Length-Implicit are shown in Fig. 14. Results from Eigen-Length-Grounded are shown in Fig. 15. We can clearly see the one-to-one correspondence between predictions and presumable measurements. R2 is close to or greater than 0.9 where the prediction is the match for the measurements, otherwise the value is much smaller. It is more apparent in the Eigen-LengthGrounded variant, where R2 values are close to the theoretical bound 1 when it matches. The models can learn a compact and appropriate set of eigen-lengths from binary task supervision. Also note that the extraneous prediction slot in task (e) (Top, or countertop placing) become degenerate with another prediction slot, as has mentioned before in main text.\n\nC APPLYING RANDOM ROTATIONS TO INPUT ENVIRONMENT GEOMETRIES\n\nC.1 FORMULATION AND IMPLEMENTATION\n\nWhile the object shape is randomly rotated in all experiments in the main paper, we take environment geometry directly from ShapeNetChang et al. (2015) where shapes are axis-aligned. In this section, we consider a more challenging setting where the environment geometry is also randomly rotated.\n\n14\n\n(cid:73)(cid:85)(cid:82)(cid:81)(cid:87)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:11)(cid:68)(cid:12)(cid:55)(cid:88)(cid:69)(cid:72)(cid:3)(cid:51)(cid:68)(cid:86)(cid:86)(cid:76)(cid:81)(cid:74)(cid:87)(cid:82)(cid:83)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:76)(cid:81)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:11)(cid:72)(cid:12)(cid:3)(cid:38)(cid:82)(cid:88)(cid:81)(cid:87)(cid:72)(cid:85)(cid:87)(cid:82)(cid:83)(cid:3)(cid:51)(cid:79)(cid:68)(cid:70)(cid:76)(cid:81)(cid:74)Under review as a conference paper at ICLR 2023\n\nSpecifically, we consider a “rotated” version of the Container Fitting task and Tube Passing task in the main paper. For each original data point, i.e. a container/tube-object pair with a boolean label ((object point cloud Po, environment point cloud Pe), success label L), we sample a random rotation R and apply it to both the container and the object. We feed (RPo, RPe) to the network described in Section 5 and supervise the network with the same label L.\n\nC.2 CORRELATION ANALYSIS AND RESULT VISUALIZATION\n\nWe show the correlation analysis of learned eigen-lengths in Fig. 16. A strong, disentangled correlation between learned eigen-lengths and human-hypothesized ones can still be observed.\n\nWe also visualize the learned geometry groundings in Fig. 17. The predicted planes roughly align with the main cavities of the objects. From the results, we can see that the proposed problem setting is still valid and the studied methods can still be applicable and produce reasonable results.\n\nD EXTENDING AND CLAUSES TO DISJUNCTIVE NORMAL FORM (DNF)\n\nD.1 FORMULATION\n\nWe employ the AND clause formulation for all tasks shown in the main paper. Namely, after learning a library of paired object/environment eigen-lengths {(Lenv\n\n)}s, we compose them by\n\n, Lobj s\n\ns\n\nˆT (E, O) =\n\n(cid:94)\n\n[Lenv\n\ns\n\n(E) > Lobj\n\ns\n\n(O)],\n\ns=1,2,...,S\n\n(selection mask m is omitted for clarity), approximated by\n\n ̃T (E, O) =\n\n(cid:89)\n\nσ((Lenv\n\ns\n\n(E) − Lobj\n\ns\n\n(O))/τ ).\n\ns=1,2,...,S\n\nHere we show we can extend this formulation to the more general Disjunctive Normal Form (DNF), where an OR connects multiple AND clauses. Each AND clause composes eigen-length comparison results of a subset of eigen-lengths. The result of each AND clause is then aggregated by an OR operator. More precisely,\n\nˆT (E, O) =\n\n(cid:95)\n\n(cid:94)\n\n[Lenv\n\ns\n\n(E) > Lobj\n\ns\n\n(O)].\n\nUa∈U\n\ns∈Ua\n\nU = {Ua}a specifies the subset Ua of eigen-lengths in each AND clause. We similarly use a differentiable approximation during training:\n\n ̃T (E, O) = 1 −\n\n(cid:89)\n\n(1 −\n\n(cid:89)\n\nσ((Lenv\n\ns\n\n(E) − Lobj\n\ns\n\n(O))/τ )).\n\nUa∈U\n\ns∈Ua\n\nThe introduction of two-level logic and the OR operator helps express more complex reasoning and deal with a wider range of tasks. For example, many realistic tasks have multiple solutions. OR captures the relationship that the task can be executed if any, not necessarily all, of the solutions work.\n\nD.2 TASK AND IMPLEMENTATION DETAILS\n\nTo demonstrate our framework’s compatibility with this new formulation, we experiment with the Multi-Tube Passing task. This is a variant of task (a) (Tube, or tube passing) in the main paper, where we have two tubes of random sizes placed next to each other. As long as the object can be translated and passed through any of these tubes, the task is considered as successful.\n\nSimilar to tube passing, we randomly sample the extents of the tubes, the shape, scale, and rotation of the object. The center of the two tubes are always at two fixed positions on the y-axis.\n\nWe set the number of eigen-lengths to learn as S = 4 and split them into two disjoint AND groups, namely U = {{1, 2}, {3, 4}}. Ideally, the learned eigen-lengths should correspond to the height and width of the tubes. Also, the height and width of the same tube should be in the same AND group.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nD.3 RESULT VISUALIZATION\n\nFig. 18 visualizes the learned eigen-lengths, where green and yellow belong to one group, purple and red belong to another group. We successfully learn eigen-lengths that measure along the height/width directions of the tubes. We also learn them in correct groups, where width and height of the same tube are paired together.\n\nE DISCUSSION AND FUTURE WORK\n\nE.1 DEFINITION OF EIGEN-LENGTHS AND APPLICATION SCOPE OF THE EXPLORED\n\nFRAMEWORK\n\nIn our setting, an eigen-length is whatever scalar measurement (i.e., just a 1D scalar) the network invents to best perform its stated downstream task. While this definition for eigen-dimensions is quite general and could be applicable to any object as long as there exist certain 1D eigen-lengths that are crucial and useful for checking the feasibility of accomplishing a downstream task, we are assuming in our current experiments that having such sets of 1D eigen-lengths are sufficient for the tasks. Therefore, our current setting would not apply to the tasks where having only such low-dimensional eigen-lengths is not sufficient, such as the tasks of geometric contour matching and object collision checking.\n\nE.2 BROADER IMPLICATION OF THE STUDIED APPROACH FOR AI AND ROBOTICS\n\nWe believe the general approach we suggest can have very general applicability in AI and robotics, where the solution to downstream tasks suggests the emergence of generally useful geometric concepts such as length, height, width, and radius in unsupervised ways. As we described in the introduction, learning such compact useful geometric eigen-lengths is beneficial in the ways that 1) they are highly interpretable, while most of the current learned representations in neural networks are opaque and learned as black-box hidden features which may be unreliable or untrustworthy, 2) they could be shared and reused across different tasks, enabling fast adaptation to novel test-time tasks, and 3) the proposed learning formulation may discover novel yet crucial geometric eigen-lengths that are even unknown to us humans given the new test-time tasks. Furthermore, there could be more geometric concepts of great interest and importance that future work can explore in this direction. Examples can be 1) symmetry, as a result of trying to complete 3D shapes, 2) regular object arrangements and poses as a tool for efficient search, and 3) tracking, as an essential capability for predicting the outcome of sports games. In other words, we want learning networks to invent the notions so symmetry, regularity, or tracking. If such capabilities could emerge from purely unsupervised learning, we no longer need to rely on black-box-like neural networks and human annotations for this geometric information over 3D objects.\n\nE.3 ROTATION OF OBJECTS DURING TASK EXECUTION\n\nIn our experiments, we are primarily concerned with translational motion during task execution. This setting stems from practical concerns: in many robotic manipulation scenarios, the rotation of the object is often given as the desired target to achieve by robot planners or unchangeable during robotic grasping and manipulation. For example, the robot gripper may only be able to grasp, hold and move the mug without spilling the content and with steady grasping in certain poses for a pick-and-place task and the robot may not be able to freely rotate the object as the arm kinematics may not allow.\n\nThat being said, our cylinder fitting task does allow rotation along the up-axis for the object and similarly, the sphere fitting task allows the full SO(3)-space rotation, while for other tasks, in the case that multiple poses of the object are possible, we can simply pass the object in different poses into the same network for multiple times to query the joint fitting feasibility.\n\nE.4 DETERMINING THE NUMBER OF EIGEN-LENGTHS TO LEARN\n\nThe number of eigen-lengths to learn, i.e. S, is a hyperparameter of our learning framework and has to be set in advance. However, it should be interpreted as the upper bound on the number of\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\neigen-lengths the system can learn, and does not have to be the “groundtruth” number of relevant eigen-lengths. As shown in Sec. 4.3 and Sec. 5.2, when we set S = 3 for the countertop fitting task where only two eigen-lengths matter, the extra “slot” either degenerates or coincides with other slots. Such cases can be easily detected and filtered, and the actual number of relevant eigen-lengths can be discovered. Setting a maximum number for an unknown number of targets is also a common practice in problems like object detection Redmon & Farhadi (2017). That being said, a more flexible mechanism that allows an arbitrary number of eigen-lengths would be desirable, especially for objects with complex compositional structures like robotic arms or closets with many drawers. We leave this as a future direction.\n\nF NEGATIVE SOCIAL IMPACT\n\nOur work joins the initial efforts of eigen-length emergence in unsupervised learning settings with many other works along this direction. There could be several bias in the data and training objectives, but this is a general concern shared by most works in this field. Our work may also share the controversial arguments with other works that the future AI agents may have the capability of thinking by themselves causing threats for human beings. However, the results demonstrated in our work are way far from that. Other than these potential issues mentioned above, we do not see any other major concerns our work particularly introduces.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 13: Additional qualitative results in Container Fitting. We show eigen-lengths in two views together with the underlying object following Fig. 12 (d). (a)-(o) are successful cases where the learned planes correctly separate out the largest cavity in the object. (p)-(t) show failure cases.\n\n18\n\n(cid:11)(cid:68)(cid:12)(cid:11)(cid:69)(cid:12)(cid:11)(cid:70)(cid:12)(cid:11)(cid:71)(cid:12)(cid:11)(cid:72)(cid:12)(cid:11)(cid:73)(cid:12)(cid:11)(cid:74)(cid:12)(cid:11)(cid:75)(cid:12)(cid:11)(cid:76)(cid:12)(cid:11)(cid:77)(cid:12)(cid:11)(cid:78)(cid:12)(cid:11)(cid:79)(cid:12)(cid:11)(cid:80)(cid:12)(cid:11)(cid:81)(cid:12)(cid:11)(cid:82)(cid:12)(cid:11)(cid:83)(cid:12)(cid:11)(cid:84)(cid:12)(cid:11)(cid:85)(cid:12)(cid:11)(cid:86)(cid:12)(cid:11)(cid:87)(cid:12)(cid:73)(cid:85)(cid:82)(cid:81)(cid:87)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:87)(cid:82)(cid:83)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:76)(cid:81)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:73)(cid:85)(cid:82)(cid:81)(cid:87)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:87)(cid:82)(cid:83)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:76)(cid:81)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:73)(cid:85)(cid:82)(cid:81)(cid:87)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:87)(cid:82)(cid:83)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:76)(cid:81)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:73)(cid:85)(cid:82)(cid:81)(cid:87)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:87)(cid:82)(cid:83)(cid:3)(cid:89)(cid:76)(cid:72)(cid:90)(cid:76)(cid:81)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)Under review as a conference paper at ICLR 2023\n\nFigure 14: Full correlation plots and respective R2 values between ground truth measurements and predicted eigen-lengths from Eigen-Length-Implicit.\n\nFigure 15: Full correlation plots and respective R2 values between ground truth measurements and predicted eigen-lengths from Eigen-Length-Grounded.\n\n19\n\nD7XEH<3UHGLFWLRQ5 D7XEH=3UHGLFWLRQ5 D7XEH<3UHGLFWLRQ5 D7XEH=3UHGLFWLRQ5 E&\\OLQGHU53UHGLFWLRQ5 E&\\OLQGHU+3UHGLFWLRQ5 E&\\OLQGHU53UHGLFWLRQ5 E&\\OLQGHU+3UHGLFWLRQ5 F6SKHUH53UHGLFWLRQ5 G)LW2EMHFW;3UHGLFWLRQ5 G)LW2EMHFW<3UHGLFWLRQ5 G)LW2EMHFW=3UHGLFWLRQ5 G)LW2EMHFW;3UHGLFWLRQ5 G)LW2EMHFW<3UHGLFWLRQ5 G)LW2EMHFW=3UHGLFWLRQ5 G)LW2EMHFW;3UHGLFWLRQ5 G)LW2EMHFW<3UHGLFWLRQ5 G)LW2EMHFW=3UHGLFWLRQ5 H7RS2EMHFW;3UHGLFWLRQ5 H7RS2EMHFW<3UHGLFWLRQ5 H7RS2EMHFW;3UHGLFWLRQ5 H7RS2EMHFW<3UHGLFWLRQ5 H7RS2EMHFW;3UHGLFWLRQ5 H7RS2EMHFW<3UHGLFWLRQ5 I0XJ+ROGHU53UHGLFWLRQ5 G)LW2EMHFW;3UHGLFWLRQ5 G)LW2EMHFW<3UHGLFWLRQ5 G)LW2EMHFW=3UHGLFWLRQ5 G)LW2EMHFW;3UHGLFWLRQ5 G)LW2EMHFW<3UHGLFWLRQ5 G)LW2EMHFW=3UHGLFWLRQ5 G)LW2EMHFW;3UHGLFWLRQ5 G)LW2EMHFW<3UHGLFWLRQ5 G)LW2EMHFW=3UHGLFWLRQ5 H7RS2EMHFW;3UHGLFWLRQ5 H7RS2EMHFW<3UHGLFWLRQ5 H7RS2EMHFW;3UHGLFWLRQ5 H7RS2EMHFW<3UHGLFWLRQ5 H7RS2EMHFW;3UHGLFWLRQ5 H7RS2EMHFW<3UHGLFWLRQ5 Under review as a conference paper at ICLR 2023\n\nFigure 16: Full correlation plots and respective R2 values between human-hypothesized measurements and predicted eigen-lengths in rotated Tube Passing and rotated Container Fitting, respectively. Correspondences between predicted eigen-lengths and human-hypothesized ones can be observed.\n\n20\n\n0.60.81.01.21.41.61.8Tube Y0.60.81.01.21.41.61.82.0Prediction 1R2 = 0.01560.60.81.01.21.41.61.8Tube Z0.60.81.01.21.41.61.82.0Prediction 1R2 = 0.88590.60.81.01.21.41.61.8Tube Y0.40.60.81.01.21.41.61.82.0Prediction 2R2 = 0.98520.60.81.01.21.41.61.8Tube Z0.40.60.81.01.21.41.61.82.0Prediction 2R2 = 0.00210.00.51.01.5(d) Fit, Object X0.000.250.500.751.001.251.501.75Prediction 1R2 = 0.54040.00.51.01.5(d) Fit, Object Y0.000.250.500.751.001.251.501.75Prediction 1R2 = 0.89190.00.51.01.5(d) Fit, Object Z0.000.250.500.751.001.251.501.75Prediction 1R2 = 0.43260.00.51.01.5(d) Fit, Object X0.000.250.500.751.001.251.50Prediction 2R2 = 0.50330.00.51.01.5(d) Fit, Object Y0.000.250.500.751.001.251.50Prediction 2R2 = 0.42080.00.51.01.5(d) Fit, Object Z0.000.250.500.751.001.251.50Prediction 2R2 = 0.99630.00.51.01.5(d) Fit, Object X0.000.250.500.751.001.251.501.75Prediction 3R2 = 0.89390.00.51.01.5(d) Fit, Object Y0.000.250.500.751.001.251.501.75Prediction 3R2 = 0.58180.00.51.01.5(d) Fit, Object Z0.000.250.500.751.001.251.501.75Prediction 3R2 = 0.5496Under review as a conference paper at ICLR 2023\n\nFigure 17: Visualization of learned geometry groundings in rotated Tube Passing and rotated Container Fitting, respectively. Vectors are visualized as arrows, and planes are visualized as disks. For Container Fitting, we also show the underlying geometry (before rotation) for better reference. The learned vectors and planes roughly align with the rotated object. Regions of interest like drawers are also selected by planes.\n\nFigure 18: Visualization of the eigen-lengths learned with OR-AND clauses. Green and yellow, purple and red eigen-lengths belong to the same AND-group. It turns out that each group attends to one of the tubes and captures its width and height.\n\n21",
    "reference": "# Summary Of The Paper\n\nThis paper presents an early attempt at learning geometric lengths from trails and explores how to transfer the learned geometric knowledge to solve a different task. Besides offering a problem formulation, this paper presents an evaluation framework based on finding evaluation correlations against ground truth lengths.\n\n# Strength And Weaknesses\n\nStrengths\n----------\n\nIdeas are clearly elaborated through the paper with adequate support of intuition in explanations.\n\nElaboration of experimentations is adequate for an early exploration of learning object lengths. \n\nInitial results are promising on the capability of learning lengths from trials.\n\nWeaknesses\n-------------\nThis paper is an early exploration of the topic. Consequently, the experimentation is limited to a single dataset and simplistic tasks. Can this methodology be used to improve challenging tasks such as relative pose estimation and active visual navigation?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThere are no major issues related to the clarity of this paper as the idea is simple and intuitive.\n\n# Summary Of The Review\n\nOverall this is initial research. My main criticism is the lack of exploration or at least discussion on how to use this idea in more complex problems. I would like to see at least some discussion in this regard.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nUNDERSTANDING NEW TASKS THROUGH THE LENS OF TRAINING DATA VIA EXPONENTIAL TILTING\n\nSubha Maity Department of Statistics University of Michigan smaity@umich.edu\n\nMikhail Yurochkin IBM Research MIT-IBM Watson AI lab mikhail.yurochkin@ibm.com\n\nMoulinath Banerjee Department of Statistics University of Michigan moulib@umich.edu\n\nYuekai Sun Department of Statistics University of Michigan yuekai@umich.edu\n\nABSTRACT\n\nDeploying machine learning models on new tasks is a major challenge due to differences in distributions of the train (source) data and the new (target) data. However, the training data likely captures some of the properties of the new task. We consider the problem of reweighing the training samples to gain insights into the distribution of the target task. Specifically, we formulate a distribution shift model based on the exponential tilt assumption and learn train data importance weights minimizing the KL divergence between labeled train and unlabeled target datasets. The learned train data weights can then be used for downstream tasks such as target performance evaluation, fine-tuning, and model selection. We demonstrate the efficacy of our method on WATERBIRDS and BREEDS benchmarks. 1\n\n1\n\nINTRODUCTION\n\nMachine learning models are often deployed in a target domain that differs from the domain in which they were trained and validated in. This leads to the practical challenges of adapting and evaluating the performance of models on a new domain without costly labeling of the dataset of interest. For example, in the Inclusive Images challenge (Shankar et al., 2017), the training data largely consists of images from countries in North America and Western Europe. If a model trained on this data is presented with images from countries in Africa and Asia, then (i) it is likely to perform poorly, and (ii) its performance in the training (source) domain may not mirror its performance in the target domain. However, due to the presence of a small fraction of images from Africa and Asia in the source data, it may be possible to reweigh the source samples to mimic the target domain.\n\nIn this paper, we consider the problem of learning a set of importance weights so that the reweighted source samples closely mimic the distribution of the target domain. We pose an exponential tilt model of the distribution shift between the train and the target data and an accompanying method that leverages unlabeled target data to fit the model. Although similar methods are widely used in statistics Rosenbaum & Rubin (1983) and machine learning Sugiyama et al. (2012) to train and evaluate models under covariate shift (where the decision function/boundary does not change), one of the main benefits of our approach is it allows concept drift (where the decision boundary/function are expected to differ) (Cai & Wei, 2019; Gama et al., 2014) between the source and the target domains. We summarize our contributions below:\n\n• In Section 3 we develop a model and an accompanying method for learning source importance\n\nweights to mimic the distribution of the target domain without labeled target samples.\n\n• In Section 4 we establish theoretical guarantees on the quality of the weight estimates and their\n\nutility in the downstream tasks of fine-tuning and model selection.\n\n1Codes can be found in https://github.com/smaityumich/exponential-tilting.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n• We demonstrate applications of our method on WATERBIRDS (Sagawa et al., 2019) (Section 5),\n\nBREEDS (Santurkar et al., 2020) (Section 6) and synthetic (Appendix C) datasets.\n\n2 RELATED WORK\n\nOut-of-distribution generalization is essential for safe deployment of ML models. There are two prevalent problem settings: domain generalization and subpopulation shift (Koh et al., 2020). Domain generalization typically assumes access to several datasets during training that are related to the same task, but differ in their domain or environment (Blanchard et al., 2011; Muandet et al., 2013). The goal is to learn a predictor that can generalize to unseen related datasets via learning invariant representations (Ganin et al., 2016; Sun & Saenko, 2016), invariant risk minimization (Arjovsky et al., 2019; Krueger et al., 2021), or meta-learning (Dou et al., 2019). Domain generalization is a very challenging problem and recent benchmark studies demonstrate that corresponding methods rarely improve over vanilla empirical risk minimization (ERM) on the source data unless given access to labeled target data for model selection (Gulrajani & Lopez-Paz, 2020; Koh et al., 2020).\n\nSubpopulation shift setting assumes that both train and test data consist of the same groups with different group fractions. This setting is typically approached via distributionally robust optimization (DRO) to maximize worst group performance (Duchi et al., 2016; Sagawa et al., 2019), various reweighing strategies (Shimodaira, 2000; Byrd & Lipton, 2019; Sagawa et al., 2020; Idrissi et al., 2021). These methods require group annotations which could be expensive to obtain in practice. Several methods were proposed to sidestep this limitation, however they still rely on a validation set with group annotations for model selection to obtain good performance (Hashimoto et al., 2018; Liu et al., 2021; Zhai et al., 2021; Creager et al., 2021). Our method is most appropriate for the subpopulation shift setting (see Section 3), however it differs in that it does not require group annotations, but requires unlabeled target data.\n\nModel selection on out-of-distribution (OOD) data is an important and challenging problem as noted by several authors (Gulrajani & Lopez-Paz, 2020; Koh et al., 2020; Zhai et al., 2021; Creager et al., 2021). Xu & Tibshirani (2022); Chen et al. (2021b) propose solutions specific to covariate shift based on parametric bootstrap and reweighing; Garg et al. (2022); Guillory et al. (2021); Yu et al. (2022) align model confidence and accuracy with a threshold; Jiang et al. (2021); Chen et al. (2021a) train several models and use their ensembles or disagreement. Our importance weighting approach is computationally simpler than the latter and is more flexible in comparison to the former, as it allows for concept drift and can be used in downstream tasks beyond model selection as we demonstrate both theoretically and empirically.\n\nDomain adaptation is another closely related problem setting. Domain adaptation (DA) methods require access to labeled source and unlabeled target domains during training and aim to improve target performance via a combination of distribution matching (Ganin et al., 2016; Sun & Saenko, 2016; Shen et al., 2018), self-training (Shu et al., 2018; Kumar et al., 2020), data augmentation (Cai et al., 2021; Ruan et al., 2021), and other regularizers. DA methods are typically challenging to train and require retraining for every new target domain. On the other hand, our importance weights are easy to learn for a new domain allowing for efficient fine-tuning, similar to test-time adaptation methods (Sun et al., 2020; Wang et al., 2020; Zhang et al., 2020), which adjust the model based on the target unlabeled samples. Our importance weights can also be used to define additional regularizers to enhance existing DA methods.\n\nImportance weighting has often been used in the domain adaptation literature on label shift (Lipton et al., 2018; Azizzadenesheli et al., 2019; Maity et al., 2022) and covariate shift (Sugiyama et al., 2007; Hashemi & Karimi, 2018) but the application has been lacking in the area of concept drift models (Cai & Wei, 2019; Maity et al., 2021), due to the reason that it is generally impossible to estimate the weights without seeing labeled data from the target. In this paper, we introduce an exponential tilt model which accommodates concept drift while allowing us to estimate the importance weights for the distribution shift.\n\n3 THE EXPONENTIAL TILT MODEL\n\nNotation We consider a K-class classification problem. Let of inputs and set of possible labels, and P and Q be probability distributions on\n\nX ∈\n\nY\n\nRd and\n\n≜ [K] be the space for the\n\nX × Y\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nsource and target domains correspondingly. A (probabilistic) classifier is a map f : We define p {\nY = k P\n{ k and P Y = k\n\nas the weighted source class conditional density, i.e. p\n\nis the class probability in source. We similarly define q\n\nx, Y = k Y = k\n\n} , where p\n\n| is the density of the source feature distribution in class\n\nX → }\n\nfor target.\n\nx, Y = k\n\nx, Y = k\n\nY = k\n\n∆K−1. = p\n\nx\n\nx\n\n{\n\n}\n\n{\n\n}\n\n{\n\n|\n\n} × {\n\n}\n\n{\n\n}\n\nWe consider the problem of learning importance weights on samples from a source domain so that the weighted source samples mimic the target distribution. We assume that the learner has access to nQ labeled samples i=1 }\nfrom the target domain. The learner’s goal is to estimate a weight function ω(x, y) > 0 such that\n\nnP i=1 from the source domain and and unlabeled samples\n\n(XP,i, YP,i)\n\nXQ,i\n\n}\n\n{\n\n{\n\nE [ω(XP , YP )g(XP , YP )]\n\nE(cid:2)g(XQ, YQ)(cid:3) for all (reasonable) g :\n\n≈\n\nR.\n\n(3.1)\n\nX × Y →\n\nIdeally, ω = dQ dP is the likelihood ratio between the source and target domains (this leads to equality in (3.1)), but learning this weight function is generally impossible without labeled samples from the target domain (David et al., 2010). Thus we must impose additional restrictions on the domains.\n\nThe exponential tilt model We assume that there is a vector of sufficient statistics T : and the parameters {θk\n\nRp, αk\n\nk=1 such that\n\nR\n\nK\n\n∈\n\n∈\n\n} log q{x,Y =k} p{x,Y =k} = θ⊤\n\nk T (x) + αk for all k\n\n[K];\n\n∈\n\nRp\n\nX →\n\n(3.2)\n\nx, Y = k\n\ni.e. q and is a member of the exponential family with base measure p sufficient statistics T . We call (3.2) the exponential tilt model. It implies the importance weights between the source and target samples are\n\nx, Y = k\n\n}\n\n{\n\n}\n\n{\n\nω(x, y) = exp(θ⊤\n\ny T (x) + αy).\n\nModel motivation The exponential tilt model is motivated by the rich theory of exponential families in statistics. In machine learning, it was used for learning with noisy labels and for improving worst-group performance when group annotations are available (Li et al., 2020; 2021). It is also closely related to several common models in transfer learning and domain adaptation. In particular, it implies there is a linear concept drift between the source and target domains. It also extends the widely used covariate shift (Sugiyama & Kawanabe, 2012) and label shift models (Alexandari et al., 2020; Lipton et al., 2018; Azizzadenesheli et al., 2019; Maity et al., 2022; Garg et al., 2020) of distribution shifts. It extends the covariate shift model because the exponential tilt model permits (linear in T (X)) concept drifts between the source and target domains; it extends the label shift model because it allows the class conditionals to differ between the source and target domains. It does, however, come with a limitation: implicit in the model is the assumption that there is some amount of overlap between the source and target domains. In the subpopulation shift setting, this assumption is always satisfied, while in domain generalization it may be violated if the new domain drastically differs from the source data (see Appendix C for a synthetic data example).\n\nChoosing T The goal of T is to identify the common subpopulations across domains, such that\n\n(XP , YP )\n\nT (XP ) = t, YP = k\n\n(XQ, YQ)\n\nT (XQ) = t, YP = k\n\nd\n\n}\n\n≈\n\n| {\n\n| {\n\n. }\n\nT (x) = t, y = k\n\nIf T segments the source domain into its subpopulations (i.e. the subpopulations are\n\n∈ for different values of t’s and k’s), then it is possible to achieve perfect }\nX × Y | reweighing of the source domain with the exponential tilt model: the weight of the T (X) = t, Y = subpopulation is exp(θ⊤ k t + αk). However, in practice, such a T that perfectly segments the k\nsubpopulations may not exist (e.g. the subpopulations may overlap) or is very hard to learn (e.g. we don’t have prior knowledge of the subpopulations to guide T ).\n\n(x, y)\n\n}\n\n{\n\n{\n\nIf no prior knowledge of the domains is available, we can use a neural network to parameterize T and learn its weights along with the tilt parameters, or simply use a pre-trained feature extractor as T , which we demonstrate to be sufficiently effective in our empirical studies. We also study the effects of misspecification of T using a synthetic dataset example in Appendix C.\n\nFitting the exponential tilt model We fit the exponential tilt model via distribution matching. This step is based on the observation that under the exponential tilt model (3.2)\n\nqX\n\nx\n\n}\n\n{\n\n= (cid:80)K\n\nk=1 p\n\n{\n\nx, Y = k\n\n3\n\nexp(θ⊤\n\nk T (x) + αk),\n\n}\n\n(3.3)\n\nPublished as a conference paper at ICLR 2023\n\nwhere qX is the (marginal) density of the inputs in the target domain. an estimate (cid:98)qX of qX from the unlabeled samples p\n{ such that\n\nIt is possible to obtain i=1 and estimates (cid:98)p of the Xi,Q }\n{ m\ni=1. This suggests we find θk’s and αk’s\n\n’s from the labeled samples\n\n(Xi,P , Yi,P )\n\nx, Y = k\n\nx, Y = k\n\n}\n\n{\n\n}\n\n{\n\n}\n\nn\n\n(cid:80)K\n\nk=1 (cid:98)p {\n\nx, Y = k\n\nexp(θ⊤\n\nk T (x) + αk)\n\n≈ (cid:98)qX\n\nx\n\n.\n\n}\n\n{\n\n}\n\n(cid:16)\n\nD\n\nNote that the θk’s and αk’s are dependent because (cid:98)qX must integrate to one. We enforce this restriction as a constraint in the distribution matching problem:\n\n(ˆθk, ˆαk) }\n{\n\nK\n\nk=1 ∈\n\n \n\n\n\narg min{(θk,αk)}K subject to (cid:82) (cid:80)K\n\nk=1\n\nX\n\nk=1 (cid:98)p {\n\n(cid:98)qX\n\nx\n\n}∥ {\nx, Y = k\n\n(cid:80)K\n\nk=1 (cid:98)p {\nexp(θ⊤\n\n}\n\nx, Y = k\n\nexp(θ⊤\n\nk T (x) + αk)\n\n}\n\n(cid:17)\n\nk T (x) + αk)dx = 1 ,\n\n(3.4) where D is a discrepancy between probability distributions on . Although there are many possible choices of D, we pick the Kullback-Leibler (KL) divergence in the rest of this paper because it leads to some computational benefits. We reformulate the above optimization for KL-divergence to relax the constraint which we state in the following lemma. Lemma 3.1. If D is the Kullback-Leibler (KL) divergence then optima in (3.3) is achieved at (ˆθk, ˆαk) }\n{ (ˆθk, ˆα′ {\n\narg max{(θk,α′\n\nk T (X) + α′\n\nk=1 where\n\n(cid:110) (cid:80)K\n\nk=1 ∈\n\nk)}K\n\nlog\n\n(cid:111)(cid:105)\n\nk)\n\n(cid:98)QX\n\nE\n\nX\n\nk=1\n\n}\n\n(cid:104)\n\nK\n\nK\n\nk\n\nk=1 (cid:98)ηP,k(X)exp(θ⊤ (cid:2)exp(θ⊤\n\nlog (cid:8)E\n\n(cid:98)P\n\n−\n\nlog (cid:8)E\n\n(cid:98)P\n\nY T (X) + α′ (cid:2)exp(ˆθ⊤\n\nY )(cid:3)(cid:9) Y T (X) + ˆα′\n\nY )(cid:3)(cid:9).\n\n(cid:98)ηP =\n\n{(cid:98)ηP,k\n\n}\n\nK\n\nk=1 is a probabilistic classifier for P and ˆαk = ˆα′\n\nk −\n\nx, Y = k\n\nOne benefit of minimizing the KL divergence is that the learner does not need to estimate the ’s, a generative model that is difficult to train. They merely need to train a discriminap {\ntive model to estimate (cid:98)ηP from the (labeled) samples from the source domain. We plug the fitted ˆθk’s and ˆαk’s into (3.5) to obtain Exponential Tilt Reweighting Alignment (ExTRA) importance weights:\n\n}\n\nWe summarize the ExTRA procedure in Algorithm 1 in Appendix B.2.\n\n(cid:98)ω(x, y) = exp(ˆθ⊤\n\ny T (x) + ˆαy).\n\n(3.5)\n\nNext we describe two downstream tasks where ExTRA weights can be used:\n\n1. ExTRA model evaluation in the target domain. Practitioners may estimate the target performance of a model in the target domain by reweighing the empirical risk in the source domain:\n\nE [l(f (XQ), YQ)]\n\n1 nP\n\n≈\n\n(cid:80)nP\n\ni=1 l(f (XP,i), YP,i)(cid:98)ω(XP,i, YP,i),\n\n(3.6)\n\nwhere l is a loss function. This allows to evaluate models in the target domain without target labeled samples even in the presence of concept drift between the training and target domain. 2. ExTRA fine-tuning for target domain performance. Since the reweighted empirical risk (in the source domain) is a good estimate of the risk in the target domain, practitioners may fine-tune models for the target domain by minimizing the reweighted empirical risk:\n\n(cid:98)fQ\n\n∈\n\narg minf ∈F E\n\n(cid:98)P [l(f (X), Y )(cid:98)ω(X, Y )] .\n\n(3.7)\n\nWe note that the correctness of (3.4) depends on the identifiability of the θk’s and αk’s from (3.3); i.e. the uniqueness of the parameters that satisfy (3.3). As long as the tilt parameters are identifiable, then (3.4) provides consistent estimates of them. However, without additional assumptions on the ’s and T , the tilt parameters are generally unidentifiable from (3.3). Next we elaborate p\non the identifiability of the exponential tilt model.\n\nx, Y = k\n\n}\n\n{\n\n4 THEORETICAL PROPERTIES OF EXPONENTIAL TILTING\n\n4.1\n\nIDENTIFIABILITY OF THE EXPONENTIAL TILT MODEL\n\nTo show that the θk’s and αk’s are identifiable from (3.3), we must show that there is a unique solution to (3.3). Unfortunately, this is not always the case. For example, consider a linear discriminant\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nanalysis (LDA) problem in which the class conditionals drift between the source and target domains:\n\np\n\nx, Y = k\n\n= πkφ(x\n\nμP,k),\n\nq\n\nx, Y = k\n\n= πkφ(x\n\nμQ,k) ,\n\n{\n\n} where φ is the standard multivariate normal density, πk (0, 1) are the class proportions in both source and target domains, and μP,k’s (resp. the μQ,k’s) are the class conditional means in the source (resp. target) domains. We see that this problem satisfies the exponential tilt model with T (x) = x:\n\n−\n\n−\n\n∈\n\n{\n\n}\n\nlog q{x,Y =k}\n\np{x,Y =k} = (μQ,k\n\nμP,k)⊤x\n\n1\n\n2 ∥\n\n−\n\nμQ,k\n\n2 + 1 2\n∥\n\n2 ∥\n\nμP,k\n\n2 2 .\n\n∥\n\n−\n\nThis instance of the exponential tilt model is not identifiable. Any permutation of the class labels σ : [K]\n\n[K] also leads to the same (marginal) distribution of inputs:\n\n→ (cid:80)K\n\nexp (cid:0)(μQ,k\n\nx, Y = k\n\nk=1 p {\n= (cid:80)K k=1 p\n\n} x, Y = k\n\n{\n\nμP,k)⊤x + 1\n\nμP,k\n\n2\n\n1\n\nμQ,k\n\n−\n\nexp (cid:0)(μQ,σ(k) −\n\n}\n\n2 ∥\n\n∥ μP,k)⊤x + 1\n\n2 −\n\n2 ∥ μP,k\n\n2 ∥\n\n(cid:1)\n\n2 2\n\n∥ 1\n\n2\n\n2 −\n\n∥\n\n2 ∥\n\nμQ,σ(k)∥\n\n(cid:1) .\n\n2 2\n\nFrom this example, we see that the non-identifiability of the exponential tilt model is closely related to the label switching problem in clustering. Intuitively, the exponential tilt model in the preceding example is too flexible because it can tilt any p . Thus there is ambiguity in which p . In the rest of this subsection, we present an identification restriction that guarantees the identifiability of the exponential tilt model.\n\ntilts to which q\n\nx, Y = k\n\nx, Y = l\n\nx, Y = k\n\nx, Y = l\n\nto q\n\n{\n\n}\n\n}\n\n{\n\n}\n\n{\n\n}\n\n{\n\nA standard identification restriction in related work on domain adaptation is a clustering assumption. For example, Tachet et al. (2020) assume there is a partition of k such that X\nsupp(P [K]. This assumption is strong: ⊂ X it implies there is a perfect classifier in the source and target domains. Here we consider a weaker version of the clustering assumption: there are sets\n\ninto disjoint sets\n\nk for all k\n\n), supp(Q\n\nY = k\n\nY = k\n\nk such that\n\n) }\n\n{· |\n\n{· |\n\nX\n\n∈\n\n}\n\nS Y = k\n\nX\n\nk\n\n= 1.\n\n} k’s; this permits the supports of P\n\n∈ S\n\nP\n\nY = k\n\nX\n\nk\n\n= Q\n\n|\n\n}\n\n{\n\n{· |\n\n∈ S\n\n{ |\nk’s can be much smaller than the to overlap.\n\nWe note that the S\nand P Y = l }\nDefinition 4.1 (anchor set). A set k\nk. p\nProposition 4.2 (identifiability from anchor sets). If there are anchor sets the source domain) and T ( satisfies (3.3).\n\n= k for all x\n\nis an anchor set for class k if p\n\nx, Y = l\n\nS ∈ S\n\n= 0, l\n\n⊂ X\n\nX\n\nS\n\n}\n\n{\n\nk for all K classes (in k) is p-dimensional, then there is at most one set of θk’s and αk’s that\n\nS\n\nY = k\n\n}\n\n{· |\n\nx, Y = k\n\n> 0 and\n\n}\n\n{\n\nThis identification restriction is also closely related to the linear independence assumption in Gong et al. (2016). Inspecting the proof of proposition 4.2 (see Appendix A.3), we see that the anchor set k=1 for any set of θk’s and assumption implies linear independence of αk’s. We study the anchor set assumption empirically in a synthetic experiment in Appendix C. Our experiments show that the assumption is mild and is violated only under extreme data scenarios.\n\nk T (x) + αk)\n\npk(x)exp(θ⊤\n\n}\n\n{\n\nK\n\n4.2 CONSISTENCY IN ESTIMATION OF THE TILT PARAMETERS\n\nHere, we establish a convergence rate for the estimated tilt parameters (Lemma (3.1)) and the ExTRA importance weights (Equation (3.1)). To simplify the notation, we define S(x) = (1, T (x)⊤)⊤ as the extended sufficient statistics for the exponential tilt and denote the corresponding tilt param- ⊤)⊤’s be the true values of the tilt parameters ξk’s eters as ξk = (αk, θ⊤ RK(p+1) be the long vector containing all the tilt parameters. We and let ξ = (ξ⊤ recall that estimating the parameters from the optimization stated in Lemma 3.1 requires a classifier (cid:98)ηP on the source data. So, we define our objective for estimating ξ through a generic classifier ∆K. Denoting ηk(x) as the k-th co-ordinate of η(x) we define the expected log-likelihood η : X → objective as:\n\nk )⊤. We let ξ⋆\n\n1 , . . . , ξ⊤\n\nk = (α⋆\n\nk, θ⋆\n\nK)⊤\n\n∈\n\nk\n\nL(η, ξ) = EQX [log\n\nand its empirical version as\n\nˆL(η, ξ) = E\n\n(cid:98)QX\n\n[log\n\n{\n\n{\n\n(cid:80)K\n\nk=1 ηk(X)exp(ξ⊤\n\nk S(X))\n\n(cid:80)K\n\nk=1 ηk(X)exp(ξ⊤\n\nk S(X))\n\n]\n\n]\n\n−\n\n−\n\n}\n\n}\n\nlog[EP\n\n{\n\nexp(ξ⊤\n\nY S(X))\n\nlog[E\n\nexp(ξ⊤\n\nY S(X))\n\n(cid:98)P {\n\n] ,\n\n}\n\n] .\n\n}\n\nTo establish the consistency of MLE we first make an assumption that the loss ξ strongly convex at the true parameter value.\n\n(cid:55)→ −\n\nL(η⋆\n\nP , ξ) is\n\n5\n\n̸ Published as a conference paper at ICLR 2023\n\nAssumption 4.3. The loss ξ μ > 0 such that for any ξ it holds:\n\n(cid:55)→ −\n\nL(η⋆\n\nP , ξ) is strongly convex at ξ⋆, i.e., there exists a constant\n\nL(η⋆\n\nP , ξ)\n\n−\n\nL(η⋆\n\nP , ξ⋆)\n\n≥ −\n\n∂ξL(η⋆\n\nP , ξ⋆)⊤(ξ\n\nξ⋆) + μ\n\n2 ∥\n\nξ\n\nξ⋆\n\n2 2 . ∥\n\n−\n\n−\n\n−\n\nWe note that the assumption is a restriction on the distribution Q rather than the objective itself. For technical convenience we next assume that the feature space is bounded.\n\nAssumption 4.4.\n\nX\n\nis bounded, i.e., there exists an M > 0 such that\n\nB(0, M ).\n\nX ⊂\n\nRecall, from Lemma 3.1, that we need a fitted source classifier (cid:98)ηP to estimate the tilt parameter: ξ⋆ is estimated by maximizing ˆL((cid:98)ηP , ξ) rather than the unknown ˆL(η⋆ P , ξ). While analyzing the convergence of ˆξ we are required to control the difference ˆL(ˆηP , ξ) P , ξ). To ensure the difference is small, assume the pilot estimate of the source regression function (cid:98)ηP is consistent at some rate rnP .\n\nˆL(η⋆\n\n−\n\nAssumption 4.5. Let f ⋆ an estimators\n\nˆfP,k(x) }\n\n{\n\nc > 0 and a sequence rnP →\n\nP,k(x) = log f ⋆ k=1 for\n\nK\n\n{ P,k(x)\n\nη⋆\n\nP,k(x)\n\n{\n\n}\n\n0 such that for almost surely [PX ] it holds\n\nˆfP (x)\n\nP(\n\n∥\n\n−\n\nf ⋆\n\nP (x)\n\n∥\n\n2 > t)\n\nexp(\n\n−\n\n≤\n\nct2/r2\n\nnP ), t > 0 .\n\n1 K\n\n(cid:80)K\n\nj=1 log\n\nη⋆\n\nP,j(x)\n\nK\n\n} −\n\n} k=1 such that the following holds: there exists a constant\n\n{\n\n. We assume that there exist\n\nK\n\n{\n\n}\n\n(cid:80)K\n\nj=1 exp( ˆfP,j(x))\n\nˆfP,k(x) k=1 to construct the regression functions as (cid:98)ηP,k(x) = We use the estimated logits }\n{ exp( ˆfP,k(x))/ , which we use in the objective stated in Lemma 3.1 to analyze the convergence of the tilt parameter estimates and the ExTRA weights. With the above assumptions we’re now ready to state concentration bounds for ˆξ ω⋆, where the true importance weight ω⋆ is defined as ω⋆(x, y) = exp(ξ⋆ Theorem 4.6. Let the assumptions 4.3, 4.4 and 4.5 hold. For the sample sizes nP , nQ define 1/2. There exists constants 1/2 + αnP ,nQ = rnP }\n} k1, k2 > 0 such that for any δ > 0 with probability at least 1\n\n(2K + 1)δ the following hold:\n\n(cid:112)log(nQ) +\n\n(p + 1)K/nQ\n\n(p + 1)K/nP\n\nξ⋆ and ˆω\n\n⊤S(x)).\n\n−\n\n−\n\n{\n\n{\n\ny\n\nˆξ ∥\n\n−\n\nξ⋆\n\n2 ∥\n\n≤\n\nk1αnP ,nQ\n\n(cid:112)\n\nlog(1/δ), and\n\nω⋆\n\nˆω\n\n∥\n\n−\n\n1,P\n\n∥\n\n≤\n\nk2αnP ,nQ\n\n(cid:112)\n\nlog(1/δ).\n\n−\n\nIn Theorem 4.6 we notice that as long as rnP log(nQ) 0. This implies both the estimated tilt parameters and the ExTRA weights converge to their true values as the sample sizes nP , nQ\n\nwe have αnP ,nQ →\n\n0 for nP , nQ\n\n→ ∞\n\n→\n\n. → ∞\n\nWe next provide theoretical guarantees for the downstream tasks (1) fine-tuning and (2) target performance evaluation that we described in Section 3.\n\nFine-tuning We establish a generalization bound for the fitted model (3.7) using weighted-ERM on source domain. We denote and a weight function ω :\n\nR≥0 define the weighted loss function and its empirical version on source data as:\n\nas the classifier hypothesis class. For f\n\n∈ F\n\nF\n\nX × Y →\n\nL\n\nP (f, w) = EP [ω(X, Y )l(f (X), Y )], ˆ L\nWe also define the loss function on the target data as: is the true value of the tilt parameters in (3.2), i.e., the following holds:\n\nP (f, w) = E\n\nQ(f ) = EQ\n\nL\n\n(cid:98)P [ω(X, Y )l(f (X), Y )] . (θ⋆\n\n(cid:2)l(f (X), Y )(cid:3) . If\n\nk, α⋆ k)\n\nK k=1\n\n}\n\n{\n\nq\n\nx, Y = k\n\n= p\n\nx, Y = k\n\nexp\n\nα⋆\n\nk + (θ⋆\n\nk)⊤T (x) }\n\n; k\n\n[K] ,\n\n} then defining ω⋆(x, k) = exp\n\n{ as the true weight we notice that Q(f ), which is easily observed by setting g(x, y) = l(f (x), y) in the display (3.1).\n\nk)⊤T (x) }\n\n{ k + (θ⋆\n\nα⋆\n\n∈\n\n{\n\n{\n\n}\n\nL To establish our generalization bound we require Rademacher complexity (Bartlett & Mendelson, 2002) (denoted as ); see Appendix A.1 for details) and the following assumption on the loss function.\n\nnP (\n\nR\n\nG\n\nP (f, ω⋆) =\n\nL\n\nAssumption 4.7. The loss function l is bounded, i.e., for some B > 0, f\n\nand y\n\n[K].\n\n, x\n\n∈ F\n\n∈ X\n\n∈\n\nf (x), y\n\nl |\n\n{\n\n}| ≤\n\nB for any\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nWith the above definitions and the assumption we establish our generalization bound.\n\nLemma 4.8. For a weight function ω and the source samples ˆfω = arg minf ∈F ˆ\nL bound holds with probability at least 1\n\nnP i=1 of size nP let P (f, ω). There exists a constant c > 0 such that the following generalization\n\n(XP,i, YP,i)\n\n}\n\n{\n\nδ\n\nQ( ˆfω)\n\nL\n\n−\n\nminf ∈F\n\nQ(f )\n\n2\n\n≤\n\nnP (\n\nR\n\nG\n\nL\n\n−\n\nwhere G\nR Appendix A.1.\n\nnP (\n\n) is the Rademacher complexity of\n\n=\n\nG\n\n{\n\n) + B\n\nω\n\nω⋆\n\n1,P + c\n\n∥\n\n−\n\n∥ ω⋆(x, y)l(f (x), y) : f\n\n(cid:113) log(1/δ) nP\n\n,\n\n(4.1)\n\ndefined in\n\n∈ F}\n\nIn Theorem 4.6 we established an upper bound for the estimated weights ˆω, which concludes that ˆfˆω has the following generalization bound: for any δ > 0, with probability at least 1\n\n(2K + 2)δ\n\nQ( ˆfˆω)\n\nL\n\n−\n\nminf ∈F\n\nQ(f )\n\n2\n\n≤\n\nnP (\n\nR\n\nG\n\nL\n\n) + k2αnP ,nQ\n\n(cid:112)\n\nlog(1/δ) + c\n\n(cid:112)\n\n− log(1/δ)/nP ,\n\nwhere k2 is the constant in Theorem 4.6 and c is the constant in Lemma 4.8. The generalization bound implies that for large sample sizes (nP , nQ ) the target accuracy of weighted ERM on source data well approximates the accuracy of ERM on target data.\n\n→ ∞\n\nTarget performance evaluation We provide a theoretical guarantee for the target performance R\nevaluation (3.6) using our importance weights. Here we only consider the functions g : which are bounded by some B > 0, i.e. . The simplest and | ≤ the most frequently used example is the model accuracy which uses 0-1-loss as the loss function: for a model f the loss g(x, y) = I is bounded with B = 1. For such functions we {\nnotice that EQ[g(X, Y )] = EP [g(X, Y )ω⋆(X, Y )], as observed in display (3.1). This implies the following bound on the target performance evaluation error EP [g(X, Y )ˆω(X, Y )](cid:12)\n\n(cid:12) (cid:12)EQ[g(X, Y )]\n\ng(x, y) |\n\nB for all x\n\nf (x) = y\n\nX × Y →\n\n(cid:12) = (cid:12)\n\nand y\n\n∈ X\n\n∈ Y\n\n}\n\n(cid:12)EP [g(X, Y )ω⋆(X, Y )] ˆω⋆(X, Y ) BEP [ |\n\n− ω⋆(X, Y ) |\n\n−\n\nEP [g(X, Y )ˆω(X, Y )](cid:12) (cid:12) 1,P . ˆω\n\nω⋆\n\nB\n\n]\n\n≤\n\n∥\n\n−\n\n∥\n\n≤\n\nω⋆ We recall the concentration bound for mated target performance in (3.6) converges to the true target performance at rate αnP ,nQ .\n\n1,P from Theorem 4.6 and conclude that the esti-\n\n−\n\nˆω\n\n∥\n\n∥\n\n−\n\n5 WATERBIRDS CASE STUDY\n\nTo demonstrate the efficacy of the ExTRA algorithm for reweighing the source data we (i) verify the ability of ExTRA to upweigh samples most relevant to the target task; (ii) evaluate the utility of weights in downstream tasks such as fine-tuning and (iii) model selection.\n\nWATERBIRDS dataset combines bird photographs from the Caltech-UCSD Birds-200-2011 (CUB) dataset (Wah et al., 2011) and the image backgrounds from the Places dataset (Zhou et al., 2017). The birds are labeled as one of = {water background, land background}. The images are divided into four groups: landbirds on land (0); landbirds on water (1); waterbirds on land (2); waterbirds on water (3). The source dataset is highly imbalanced, i.e. the smallest group (2) has 56 samples. We embed all images with a pre-trained ResNet18 (He et al., 2016). See Appendix B.1 for details.\n\n= {waterbird, landbird} and placed on one of\n\nA\n\nY\n\nWe consider five subpopulation shift target domains: all pairs of domains with different bird types and the original test set (Sagawa et al., 2019) where all 4 groups are present with proportions vastly different from the source. For all domains, we fit ExTRA weights (with ResNet18 features as T (x)) from 10 different initializations and report means and standard deviations for the metrics. See Appendix B.2 for the implementation details.\n\n}\n\n1, 2\n\nconsisting only of birds appearing on their atypical backgrounds. Groups\n\nExTRA weights quality For a given target domain it is most valuable to upweigh the samples in the source data corresponding to the groups comprising that domain. The most challenging is the target correspond {\nto 5% of the source data making them most difficult to “find”. To quantify the ability of ExTRA to upweigh these samples we report precision (proportion of samples from groups within the top x% of the weights) and recall (proportion of samples within the top x% of the weights) {\nin Figure 1. We notice that samples corresponding to 10% largest ExTRA weights contain slightly\n\n1, 2\n\n1, 2\n\n1, 2\n\n}\n\n}\n\n{\n\n{\n\n}\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Model selection results on WATERBIRDS\n\ntarget accuracy\n\nrank correlation\n\ntarget groups\n\nExTRA\n\nSrcVal ATC-NE\n\nExTRA\n\nSrcVal ATC-NE\n\n{0, 2} {1, 2} {0, 3} {1, 3} {0, 1, 2, 3}\n\naverage\n\n0.819±0.012 0.741±0.047 0.978±0.001 0.757±0.011 0.856±0.034\n\n0.83\n\n0.854 0.616 0.978 0.737 0.803\n\n0.798\n\n0.871 0.646 0.976 0.747 0.818\n\n0.812\n\n0.419±0.01 0.747±0.106 0.962±0.004 0.361±0.168 0.658±0.295\n\n0.753\n\n0.807 -0.519 0.956 -0.318 0.263\n\n0.166\n\n0.760 -0.590 0.906 -0.411 0.178\n\n0.110\n\nover 80% of the groups in the source data (recall). This demonstrates the ability of ExTRA to upweigh relevant samples. We present examples of upweighted images and results for other target domains in Appendix B.3.\n\n1, 2\n\n{\n\n}\n\nModel fine-tuning We demonstrate the utility of ExTRA weights in the fine-tuning (3.7). The basic goal of such importance weighing is to improve the performance in the target in comparison to training on uniform source weights S -> T, i.e. ERM. Another baseline is the DRO model (Hashimoto et al., 2018) that aims to maximize worst-group performance without access to the group labels, and JTT (Liu et al., 2021) that retrains a model after upweighting the misclassified samples by ERM. We consider two additional baselines that utilize group annotations to improve worst-group performance: re-weighing the source to equalize group proportions (RWgr) and group DRO (gDRO) (Sagawa et al., 2019). The aforementioned baselines do not try to adjust to the target domain. Finally, we compare to πT -> T that fine-tunes the model only using the subset of the source samples corresponding to the target domain groups. In all cases we use logistic regression as model class.\n\nFigure 1: ExTRA precision and recall for samples with top x% weights.\n\nWe compare target accuracy across domains in Figure 2. Analogous comparison with area under the receiver operator curve can be found in Figure 6 in Appendix B.3.1. Model trained with ExTRA weights outperforms all “fair” baselines and matches the performance of the three baselines that had access to additional information. In all target domains ExTRA fine-tuning is comparable with the πT -> T supporting its ability to upweigh relevant samples. Notably, on {1,2} domain of both minority groups and on {0,3} domain of both majority groups, ExTRA outperforms RWgr and gDRO that utilize group annotations. This emphasizes the advantage of adapting to the target domain instead of pursuing a more conservative goal of worst-group performance maximization. Finally, we note that ExTRA fine-tuning did not perform as well on the domain {1,3}, however neither did πT -> T.\n\nFigure 2: Performance on WATERBIRDS.\n\nModel selection out-of-distribution is an important task, that is difficult to perform without target data labels and group annotations (Gulrajani & Lopez-Paz, 2020; Zhai et al., 2021). We evaluate the ability of choosing a model for the target domain based on accuracy on the ExTRA reweighted source validation data. We compare to the standard source validation model selection (SrcVal) and to the recently proposed ATC-NE (Garg et al., 2022) that uses negative entropy of the predicted probabilities on the target domain to score models. We fit a total of 120 logistic regression models with different weighting (uniform, label balancing, and group balancing) and varying regularizers. See Appendix B.2 for details.\n\nIn Table 1 we compare the target performance of models selected using each of the model evaluation scores and rank correlation between the corresponding model scores and true target accuracies.\n\n8\n\n10−210−1100topx%weights0.20.40.60.8precisionforgroups{1,2}groups{1,2}precisionrecall0.20.40.60.81.0recallforgroups{1,2}{0,2}{1,2}{0,3}{1,3}{0,1,2,3}averagetargetdomaingroups0.00.20.40.60.81.0targetaccuracyRWgrS→TExTRADROgGROJTTπT→TPublished as a conference paper at ICLR 2023\n\nModel selection with ExTRA results in the best target performance and rank correlation on 4 out of 5 domains and on average. Importantly, the rank correlation between the true performance and ExTRA model scores is always positive, unlike the baselines, suggesting its reliability in providing meaningful information about the target domain performance.\n\n6 BREEDS CASE STUDY\n\nBREEDS (Santurkar et al., 2020) is a subpopulation shift benchmark derived from ImageNet (Deng et al., 2009). It uses the class hierarchy to define groups within classes. For example, in the Entity30 task considered in this experiment, class fruit is represented by strawberry, pineapple, jackfruit, Granny Smith in the source and buckeye, corn, ear, acorn in the target. This is an extreme case of subpopulation shift where source and target groups have zero overlap. We modify the dataset by adding a small fraction π of random samples from the target to the source for two reasons: (i) our exponential tilt model requires some amount of overlap between source and target; (ii) arguably, in practice, it is more likely that the source dataset has at least a small representation of all groups.\n\nOur goal is to show that ExTRA can identify the target samples mixed into the source for efficient fine-tuning. We obtain feature representations from a pre-trained selfsupervised SwAV (Caron et al., 2020). To obtain the ExTRA weights we use SwAV features as sufficient statistic. We then train logistic regression models on (i) the source dataset re-weighted with ExTRA, (ii) uniformly weighted source (S -> T), (iii) target samples mixed into the source (πT -> T), (iv) all target samples (oracle). See Appendix B.1, B.2 for details. We report performance for varying mixing proportion π in Figure 3. First, we note that even when π = 0, i.e. source and target have completely disjoint groups (similar to domain generalization), ExTRA improves over the vanilla S -> T. Next, we see that S -> T improves very slowly in comparison to ExTRA as we increase the mixing proportion; πT -> T improves faster as we increase the number of target samples it has access to, but never suppresses ExTRA and matches its improvement slope for the larger π values. We conclude that ExTRA can effectively identify target samples mixed into source that are crucial for the success of fine-tuning and find source samples most relevant to the target task allowing it to outperform πT -> T. We report analogous precision and recall for the WATERBIRDS experiment in Appendix B.3.\n\nFigure 3: Performance on BREEDS.\n\n7 CONCLUSION\n\nIn this paper, we developed an importance weighing method for approximating expectations of interest on new domains leveraging unlabeled samples (in addition to a labeled dataset from the source domain). We demonstrated the applicability of our method on downstream tasks such as model evaluation/selection and fine-tuning both theoretically and empirically. Unlike other importance weighing methods that only allow covariate shift between the source and target domains, we permit concept drift between the source and target. Though we demonstrate the efficacy of our method in synthetic setup of concept drift (Appendix C), in a future research it would be interesting to investigate the performance in more realistic setups (e.g. CIFAR10.2 to CIFAR10.2 (Lu et al., 2020), Imagenet to Imagenetv2 (Recht et al., 2019)).\n\nDespite its benefits, the exponential tilt model does suffer from a few limitations. Implicit in the exponential tilt assumption is that the supports of the target class conditionals have some overlap with the corresponding source class conditionals. Although this assumption is likely satisfied in many instances of domain generalization problems (and is always satisfied in the subpopulation shift setting), an interesting avenue for future studies is to accommodate support alignment in the distribution shift model, i.e. to align the supports for class conditioned feature distributions in source and target domains. One way to approach this is to utilize distribution matching techniques from domain adaptation literature (Ganin et al., 2016; Sun & Saenko, 2016; Shen et al., 2018), similarly to Cai et al. (2021). We hope aligning supports via distribution matching will allow our method to succeed on domain generalization problems where the support overlap assumption is violated.\n\n9\n\n0.00.00250.0050.010.020.040.080.16mixingproportionπ0.50.60.70.80.9targetaccuracyS→TπT→TExTRAoraclePublished as a conference paper at ICLR 2023\n\n8 ETHICS STATEMENT\n\nWe recommend considering the representation of the minority groups when applying ExTRA in the context of fairness-sensitive applications. The goal of ExTRA is to approximate the distribution of the target domain, thus, in order to use ExTRA weights for fine-tuning or model selection to obtain a fair model, the target domain should be well representative of both privileged and unprivileged groups. If the target domain has miss/under-represented groups, a model obtained using ExTRA weights may be biased.\n\nACKNOWLEDGMENTS\n\nThis paper is based upon work supported by the National Science Foundation (NSF) under grants no. 2027737 and 2113373.\n\nREFERENCES\n\nAmr Alexandari, Anshul Kundaje, and Avanti Shrikumar. EM with Bias-Corrected Calibration is\n\nHard-To-Beat at Label Shift Adaptation. arXiv:1901.06852 [cs, stat], January 2020.\n\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimiza-\n\ntion. arXiv:1907.02893 [cs, stat], September 2019.\n\nKamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized Learn-\n\ning for Domain Adaptation under Label Shifts. arXiv:1903.09734 [cs, stat], March 2019.\n\nPeter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and\n\nstructural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.\n\nGilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. In Proceedings of the 24th International Conference on Neural Information Processing Systems, NIPS’11, pp. 2178–2186, Red Hook, NY, USA, December 2011. Curran Associates Inc. ISBN 978-1-61839-599-3.\n\nJonathon Byrd and Zachary Lipton. What is the Effect of Importance Weighting in Deep Learning?\n\nIn International Conference on Machine Learning, pp. 872–881. PMLR, May 2019.\n\nT. Tony Cai and Hongji Wei. Transfer Learning for Nonparametric Classification: Minimax Rate\n\nand Adaptive Classifier. arXiv:1906.02903 [cs, math, stat], June 2019.\n\nTianle Cai, Ruiqi Gao, Jason D. Lee, and Qi Lei. A Theory of Label Propagation for Subpopulation\n\nShift. arXiv:2102.11203 [cs, stat], February 2021.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912–9924, 2020.\n\nJiefeng Chen, Frederick Liu, Besim Avci, Xi Wu, Yingyu Liang, and Somesh Jha. Detecting errors and estimating accuracy on unlabeled data with self-training ensembles. Advances in Neural Information Processing Systems, 34, 2021a.\n\nMayee Chen, Karan Goel, Nimit S Sohoni, Fait Poms, Kayvon Fatahalian, and Christopher Ré. Mandoline: Model evaluation under distribution shift. In International Conference on Machine Learning, pp. 1617–1629. PMLR, 2021b.\n\nElliot Creager, Joern-Henrik Jacobsen, and Richard Zemel. Environment Inference for Invariant Learning. In Proceedings of the 38th International Conference on Machine Learning, pp. 2189– 2200. PMLR, July 2021.\n\nShai Ben David, Tyler Lu, Teresa Luu, and David Pal. Impossibility Theorems for Domain Adaptation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 129–136. JMLR Workshop and Conference Proceedings, March 2010.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nQi Dou, Daniel C. Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain Generalization via\n\nModel-Agnostic Learning of Semantic Features. arXiv:1910.13580 [cs], October 2019.\n\nJohn Duchi, Peter Glynn, and Hongseok Namkoong. Statistics of Robust Optimization: A General-\n\nized Empirical Likelihood Approach. arXiv:1610.03425 [stat], October 2016.\n\nJoão Gama, Indr ̇e Žliobait ̇e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. A survey on concept drift adaptation. ACM Computing Surveys, 46(4):44:1–44:37, March 2014. ISSN 0360-0300. doi: 10.1145/2523813.\n\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, January 2016. ISSN 1532-4435.\n\nSaurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary C. Lipton. A Unified View of Label\n\nShift Estimation. arXiv:2003.07554 [cs, stat], March 2020.\n\nSaurabh Garg, Sivaraman Balakrishnan, Zachary C Lipton, Behnam Neyshabur, and Hanie Sedghi. Leveraging unlabeled data to predict out-of-distribution performance. arXiv preprint arXiv:2201.04234, 2022.\n\nMingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Scholkopf. Domain Adaptation with Conditional Transferable Components. In Proceedings of Machine Learning Research, volume 48, pp. 17, New York, New York, USA, June 2016.\n\nDevin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, and Ludwig Schmidt. Predicting with confidence on unseen distributions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1134–1144, 2021.\n\nIshaan Gulrajani and David Lopez-Paz. In Search of Lost Domain Generalization. In International\n\nConference on Learning Representations, September 2020.\n\nMahdi Hashemi and Hassan Karimi. Weighted machine learning. Statistics, Optimization and\n\nInformation Computing, 6(4):497–525, 2018.\n\nTatsunori B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness With-\n\nout Demographics in Repeated Loss Minimization. arXiv:1806.08010 [cs, stat], June 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, Las Vegas, NV, USA, June 2016. IEEE. ISBN 978-1-4673-8851-1. doi: 10.1109/ CVPR.2016.90.\n\nBadr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. arXiv preprint arXiv:2110.14503, 2021.\n\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of\n\nsgd via disagreement. arXiv preprint arXiv:2106.13799, 2021.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.6980\n\n[cs], January 2017.\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A Benchmark of in-the-Wild Distribution Shifts. arXiv:2012.07421 [cs], December 2020.\n\nDavid Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In International Conference on Machine Learning, pp. 5815–5826. PMLR, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAnanya Kumar, Tengyu Ma, and Percy Liang. Understanding Self-Training for Gradual Domain\n\nAdaptation. arXiv:2002.11361 [cs, stat], February 2020.\n\nAnanya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. February 2022. doi: 10.48550/arXiv.2202.10054.\n\nTian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. Tilted Empirical Risk Minimization.\n\nIn International Conference on Learning Representations, September 2020.\n\nTian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. On tilted losses in machine learning:\n\nTheory and applications. arXiv preprint arXiv:2109.06141, 2021.\n\nZachary C. Lipton, Yu-Xiang Wang, and Alex Smola. Detecting and Correcting for Label Shift with\n\nBlack Box Predictors. arXiv:1802.03916 [cs, stat], July 2018.\n\nEvan Z. Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just Train Twice: Improving Group Robustness without Training Group Information. In Proceedings of the 38th International Conference on Machine Learning, pp. 6781–6792. PMLR, July 2021.\n\nShangyun Lu, Bradley Nott, Aaron Olson, Alberto Todeschini, Hossein Vahabi, Yair Carmon, and Ludwig Schmidt. Harder or different? a closer look at distribution shift in dataset reproduction. In ICML Workshop on Uncertainty and Robustness in Deep Learning, 2020.\n\nSubha Maity, Diptavo Dutta, Jonathan Terhorst, Yuekai Sun, and Moulinath Banerjee. A linear adjustment based approach to posterior drift in transfer learning. arXiv:2111.10841 [stat], December 2021.\n\nSubha Maity, Yuekai Sun, and Moulinath Banerjee. Minimax optimal approaches to the label shift problem in non-parametric settings. Journal of Machine Learning Research, 23(346):1–45, 2022.\n\nKrikamol Muandet, David Balduzzi, and Bernhard Schölkopf. Domain Generalization via Invariant\n\nFeature Representation. January 2013.\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers In International Conference on Machine Learning, pp. 5389–5400.\n\ngeneralize to imagenet? PMLR, 2019.\n\nPaul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational\n\nstudies for causal effects. Biometrika, 70(1):41–55, 1983.\n\nElan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. Domain-adjusted regression or: Erm arXiv preprint\n\nmay already learn features sufficient for out-of-distribution generalization. arXiv:2202.06856, 2022.\n\nYangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift.\n\narXiv preprint arXiv:2201.00057, 2021.\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization. arXiv:1911.08731 [cs, stat], November 2019.\n\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An Investigation of Why Overparameterization Exacerbates Spurious Correlations. arXiv:2005.04345 [cs, stat], August 2020.\n\nShibani Santurkar, Dimitris Tsipras, and Aleksander Madry. BREEDS: Benchmarks for Subpopu-\n\nlation Shift. August 2020. doi: 10.48550/arXiv.2008.04859.\n\nShreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D. Sculley. No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World. arXiv:1711.08536 [stat], November 2017.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nJian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation learning for domain adaptation. In Thirty-second AAAI conference on artificial intelligence, 2018.\n\nHidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90(2):227–244, October 2000. ISSN 0378-3758. doi: 10.1016/S0378-3758(00)00115-4.\n\nAvanti Shrikumar and Anshul Kundaje. Calibration with bias-corrected temperature scaling improves domain adaptation under label shift in modern neural networks. Preprint at https://arxiv. org/abs/1901.06852 v1, 2019.\n\nRui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-T Approach to Unsupervised Domain Adaptation. In International Conference on Learning Representations, February 2018.\n\nMasashi Sugiyama and Motoaki Kawanabe. Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adaptation. Adaptive Computation and Machine Learning Series. MIT Press, Cambridge, MA, USA, March 2012. ISBN 978-0-262-01709-1.\n\nMasashi Sugiyama, Matthias Krauledat, and Klaus-Robert Müller. Covariate Shift Adaptation by Importance Weighted Cross Validation. The Journal of Machine Learning Research, 8:985–1005, December 2007. ISSN 1532-4435.\n\nMasashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density Ratio Estimation in Machine Learning. Cambridge University Press, Cambridge, 2012. ISBN 978-0-521-19017-6. doi: 10. 1017/CBO9781139035613.\n\nBaochen Sun and Kate Saenko. Deep CORAL: Correlation Alignment for Deep Domain Adaptation. In Gang Hua and Hervé Jégou (eds.), Computer Vision – ECCV 2016 Workshops, Lecture Notes in Computer Science, pp. 443–450, Cham, 2016. Springer International Publishing. ISBN 978-3319-49409-8. doi: 10.1007/978-3-319-49409-8_35.\n\nYu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-Time Training with Self-Supervision for Generalization under Distribution Shifts. arXiv:1909.13231 [cs, stat], July 2020.\n\nRemi Tachet, Han Zhao, Yu-Xiang Wang, and Geoff Gordon. Domain Adaptation with Conditional\n\nDistribution Matching and Generalized Label Shift. March 2020.\n\nA. W. van der Vaart and Jon A Wellner. Weak Convergence and Empirical Processes: With Applications to Statistics. Springer, New York, 2000. ISBN 978-0-387-94640-5 978-1-4757-2547-6.\n\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\n\nbirds-200-2011 dataset. 2011.\n\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully Test-Time Adaptation by Entropy Minimization. In International Conference on Learning Representations, September 2020.\n\nJon Wellner et al. Weak convergence and empirical processes: with applications to statistics.\n\nSpringer Science & Business Media, 2013.\n\nHui Xu and Robert Tibshirani. Estimation of prediction error with known covariate shift. arXiv\n\npreprint arXiv:2205.01849, 2022.\n\nYaodong Yu, Zitong Yang, Alexander Wei, Yi Ma, and Jacob Steinhardt. Predicting out-of-\n\ndistribution error with the projection norm. arXiv preprint arXiv:2202.05834, 2022.\n\nRuntian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar. DORO: Distributional and Outlier Robust Optimization. In Proceedings of the 38th International Conference on Machine Learning, pp. 12345–12355. PMLR, July 2021.\n\nMarvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift. arXiv:2007.02931 [cs, stat], October 2020.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452–1464, 2017.\n\n14",
    "reference": "# Summary Of The Paper\n\nThis paper considers the problem of reweighting training samples to improve model performance on out-of-distribution test samples. The approach formulates real distribution shifts (covariate and concept related) using the exponential tilt assumption. With this assumption, the problem of improving performance on OOD samples simplifies to learning data importance weights. The paper has some theoretical analysis on the properties of exponential tilting. The paper also applies this method to improve performance on Waterbirds and BREEDS-Entity30.\n\n# Strength And Weaknesses\n\nStrengths \n\n- The paper is well-written. The exponential tilt model is clearly explained. \n- Analyzing learned importance weights. \n\nWeaknesses\n- Exponential tilt assumption unjustified. It is not clear why complex distribution shits in practice should be parameterized with the exponential tilt model. The paper states that this problem can be applied to concept drift settings but has no experiments / analysis on concept drift.\n\n- When does this re-weighting approach fail? A more quantitative approach to this question would be insightful. The paper loosely talks about this  (i.e. distribution needs some overlap etc).\n\n- Limited empirical evaluation. Waterbirds and Entity30 are datasets where you know what the \"ground-truth\" importance weights should look like (https://proceedings.mlr.press/v177/idrissi22a.html). Showing that the proposed methods works well in these settings is a good first paper, i think the paper will be significantly stronger if the paper improves performance on more general / realistic distribution shifts (e.g. CIFAR 10.2, ImagenetV2) and then analyzes the learned importance weights. The empirical section should have additional baselines (e.g. https://proceedings.mlr.press/v162/zhou22d/zhou22d.pdf, https://proceedings.mlr.press/v177/idrissi22a.html) to clearly contrast this approach from previous methods.\n\n- Theoretical analysis somewhat tangential and not insightful vis-a-vis the paper's main focus. I would rather first read whether this method works well on realistic distribution shifts and then discuss properties like consistency and identifiability of the parameters. \n\nOverall the paper is well-written and focuses on a principled approach (reweighting) to improve OOD performance. However, the paper has two major issues: (a) exponential tilt assumption is not clearly justified, (b) empirical evaluation is quite limited.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSee strengths and weaknesses above\n\n# Summary Of The Review\n\nSee strengths and weaknesses above\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nCOMPOSITIONAL TASK REPRESENTATIONS FOR LARGE LANGUAGE MODELS\n\nNan Shao∗1, Zefan Cai∗2, Hanwei Xu1, Chonghua Liao3, Yanan Zheng3, Zhilin Yang†3451 1Recurrent AI, 2Beijing Jiaotong University, 3Tsinghua University 4Shanghai Artificial Intelligence Laboratory, 5Shanghai Qi Zhi Institute {windoker,zefncai}@gmail.com {zyanan, zhiliny}@tsinghua.edu.cn\n\nABSTRACT\n\nLarge language models have shown a remarkable cross-task generalization ability. Most prior works assumed that prompts effectively extract knowledge from language models to facilitate generalization to new tasks. This perspective led to numerous studies on improving prompts. In contrast, we introduce a new perspective, compositional generalization, that views each task as a composition of latent codes and generalizes to test tasks by a new composition of seen codes. To this end, we propose a novel prompt-free approach, Compositional Task Representations (CTR), that employs multi-task training to learn a discrete, compositional codebook. Empirically, our CTR substantially outperforms prompt-based methods in zero-label learning on average. According to our analysis, some of the learned CTR codes are interpretable to humans and demonstrate a certain degree of controllability.\n\n1\n\nINTRODUCTION\n\nLarge language models (LLMs) have shown remarkable performance in cross-task generalization. Without using any labeled data for the target task, GPT-3 (Brown et al., 2020) obtains reasonable performance on a wide range of tasks. Later extensions such as FLAN (Wei et al., 2022) and T0 (Sanh et al., 2022) continue training the LLMs on a large number of supervised tasks, which further improves cross-task generalization performance. The aforementioned studies have used an important assumption that natural language prompts extract knowledge from LLMs to facilitate generalization to new tasks. In this direction, numerous studies have focused on different aspects of improving prompt-based learning, such as designing better prompts (Xu et al., 2022), increasing the number of prompts (Wang et al., 2022; Aribandi et al., 2022), and improving the training efficiency of prompts (Lester et al., 2021).\n\nIn contrast, we explore an alternative perspective for cross-task generalization, i.e., compositional generalization. Specifically, we explore whether it is possible to represent tasks using discrete compositions of latent codes. This perspective enjoys several potential benefits. First, since the latent codes have been trained for seen tasks, we expect the LLMs to have strong cross-task generalization abilities because new tasks can also be represented as a composition of these trained codes. Second, it provides a way to analyze and understand cross-task generalization by investigating the association between tasks and the learned representations. Third, it has the potential of being more controllable than prompts for task generalization due to its built-in compositionality.\n\nMotivated by the aforementioned potentials, we propose a new method, Compositional Task Representations (CTR), that employs multi-task training to learn a discrete, compositional codebook. Specifically, given a large number of training tasks, we use an encoder to map each randomlyinitialized task embedding to a fixed-length sequence of query vectors. Each query vector is used to retrieve a code from a codebook, which is formulated as an embedding lookup table. This produces\n\n*Equal contribution. †Corresponding author. ‡The code will be available at https://github.com/shaonan1993/CTR.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: An illustration of how CTR generalizes to zero-label tasks. In this real example produced by our model, CTR combines the abilities of reasoning-based QA, sentence generation, and multichoice selection from training tasks to perform a new task COPA.\n\na sequence of codes, a compositional representation of the current task. These compositional codes are fed as the input to an LLM in place of prompts to make predictions. At test time, given a new task, we use unlabeled data to search for a high-performing composition of codes, which enables zero-label cross-task generalization. CTR is also applicable to the few-shot setting where the few labeled examples are used for code search.\n\nEmpirically, we demonstrate improved performance under both the settings of zero-label learning and few-shot learning, outperforming strong baselines including prompt tuning, model tuning, and genetic prompt search (Xu et al., 2022). Importantly, we analyze the learned task representations and show that they demonstrate a certain degree of interpretability and controllability. For example, as shown in Figure 1, CTR learns to generalize to a new task by a new composition of existing codes.\n\n2 RELATED WORK\n\nLanguage Model Prompting. Brown et al. (2020) showed that GPT-3 performs well in the fewshot setting if properly handcrafted prompts are provided. Other works (Shoeybi et al., 2019; Rae et al., 2021; Schick & Sch ̈utze, 2021) also report promising zero-shot or few-shot performances of LLMs. Besides, Wei et al. (2022) and Sanh et al. (2022) collect a set of labeled datasets and use manual templates to transform them into a sequence-to-sequence style. Such a formulation makes it possible to continue training LLMs on these labeled datasets and improves cross-task generalization. Wang et al. (2022); Mishra et al. (2022) introduced a benchmark of over 1600 tasks and their expertwritten instructions Gao et al. (2021); Shin et al. (2020) studied automating the search process of discrete prompts. Li & Liang (2021); Liu et al. (2021) propose continuous soft prompts with gradient-based optimization. Compared to these approaches, we study a different approach that learns compositional task representations, which benefits cross-task generalization.\n\nCompositional Architecture for LLMs. Previous work has explored designing compositional architectures. Sparsely Gated Mixture of Expert (MoE) (Lepikhin et al., 2021) activates a subset of a network given the input data. Artetxe et al. (2021) trained an MoE model with 1.1T parameters, which is shown to outperform a dense model with similar computational cost. SkillNet-NLU (Tang et al., 2022) and SkillNet-NLG (Liao et al., 2022b) employed a similar sparsely activated mechanism to handle different NLU or NLG tasks. Different from these approaches, our approach focuses on learning compositional task representations using a discrete codebook.\n\n2\n\nI watched the first McCainObama debate last night . It was full of moments I had to pause the DVR because I had to discuss what they were saying with my husband . I learned a lot about the Iraq war and Afghanistan , and I saw both McCain and Obama make some going points , and I saw them both make some blunders .How would this person be classified ?-Moderate -Liberal -Conservative -None of the above choices .CosmosQA(Code 69 is shared bytasks that are multiple-choice/reasoning-based QA): Document: National Archives Yes, it2019s that time again, folks. It2019s the first Friday of the month, when for one ever-so-brief moment the interests of Wall Street, Washington and Main Street are all aligned on one thing: Jobs. A fresh update on the U.S. employment situation for January hits the wires at 8:30 a.m. ......CTR+LLMModerateThe unemployment rate dropped to 8.2% last month, but the economy only added 120,000 jobs, when 203,000 new jobs had been predicted, according to today's jobs report. Reaction on the Wall Street Journal's MarketBeat Blog was swift: \"Woah!!! Bad number. \" The unemployment rate, however, is better news; it had been expected to hold steady at 8.3%. But the AP notes that the dip is mostly due to more Americans giving up on seeking employment.\"Compositional Task CodeGenerated OutputsThe woman retired. so... -She received her pension. -She paid off her mortgage.COPA(sentence completion)She received her pension.Training PhaseZero-Label Inference PhaseRand falls on shock SA rate cut Interest rates are trimmed to 7.5 by the South African central bank, but the lack of warning hits the rand and surprises markets. world politics, sports, business, or science and technology?Business[23, 90, 69, 15, 2, 79, 25, 103, 120, 44][54, 14, 58, 28, 117, 33, 16, 53, 109, 111][111,7,69,15,2,79,39,1,119,19][23, 90, 76, 15, 2, 79, 39, 1, 119, 19]TaskInputsThe girl found a bug in her cereal. so... -She poured milk in the bowl. -She lost her appetite.AGNews(Code [15,2,79,39,1,119,19] occurs in tasks that select from multiple options):MultiNews(Code 111 exists in most long-sentence-generationtasks):She lost her appetite.Published as a conference paper at ICLR 2023\n\nFigure 2: An overview of the architecture of our proposed CTR . The left part illustrates the training phase, while the right part shows how CTR works during the inference phrase.\n\n3 COMPOSITIONAL TASK REPRESENTATIONS\n\nThe motivation of CTR is to explore the cross-task generalization ability of LLMs from a brandnew perspective—compositional generalization, and to further improve the performance of crosstask generalization. Specifically, our main hypothesis is that being trained on a variety of natural language tasks, LLMs will be able to learn to represent each task as a composition of discrete latent codes, where each latent code is associated with certain aspects of a task. As a result, CTR potentially enjoys the advantages of better cross-task generalization since it could represent new tasks by forming new compositions.\n\nThis section introduces the overall architecture of CTR and how it is trained to overcome optimization challenges. As Figure 2 shows, CTR consists of the CTR learning module and an LLM, where the CTR learning module contains an encoder, a decoder, task embeddings, and a codebook.\n\n3.1 DISCRETE LATENT TASK CODEBOOK\n\nWe define a latent task codebook embedding space C ∈ RS×D where S is the size of the latent codebook embedding space (i.e., each task code can take either of the S categorical values), and D is the dimension of each latent code embedding Ci ∈ RD, i ∈ [1, 2, · · · , S]. This is analogous to the idea of VQ-VAE (van den Oord et al., 2017) where a discrete latent codebook is also employed.\n\nAs Figure 2 shows, given a training task as the input (assuming the task id is k), CTR first obtains its task embedding Ek ∈ RD by retrieving from a randomly-initialized task embedding lookup table RN ×D where N is the number of training tasks. The task embedding Ek is then passed to the encoder module and is mapped into a fixed-length sequence of query vectors Q ∈ RL×D where L is the length of the sequence. For each query vector Ql, l ∈ [1, 2, · · · , L], it is used to retrieve a task code embedding from the codebook embedding space. Specifically, it calculates the l2 distance with each of the latent code embedding Ci ∈ RD, i ∈ [1, 2, · · · , S], and find the nearest neighbor,\n\nCTRl = Czl, where zl = arg min\n\ni\n\n||Ql − Ci||2\n\n(1)\n\nIn this way, all query vectors together produce a L-length sequence of code embeddings, denoted as the compositional task representations (CTR). They are then passed through a decoder, followed by being used as the input to an LLM in place of prompts to make predictions.\n\nLet z be a vector of L latent codes z1, z2, . . . , zL. We consider each latent code as describing an attribute or a necessary skill of a certain task, such as task type, output space, etc. Intuitively, we need multiple codes to fully describe a task, and each task is formulated as a composition of codes.\n\n3.2 TRAINING\n\nTraining can be challenging due to the existence of discrete latent variables. Moreover, during the initial training phase, the CTR learning module is randomly initialized. As a result, the codebook embeddings C will have a very different distribution to the query vectors Q, which increases the\n\n3\n\nInference Phase of CTRTraining Phase of CTRPublished as a conference paper at ICLR 2023\n\ndifficulty of optimization. We decouple training into two phases. In the first phase, we freeze the LLM and only update the CTR learning module. This is followed by tuning all parameters.\n\nIn terms of the loss function, following van den Oord et al. (2017), we employ two separate losses, an embedding loss and a commitment loss, to match the query vectors with the compositional task representations. These losses are used in combination with a standard language modeling loss,\n\nL = LLM +\n\nL (cid:88)\n\nl=1\n\n(||sg[Ql] − CTRl||2\n\n2 + β||Ql − sg[CTRl]||2 2)\n\n(2)\n\nwhere LLM is a standard language modeling loss for solving the target task, sg denotes stopping the gradients, and β is a hyperparameter.\n\n3.3\n\nINFERENCE\n\nWe consider two settings: zero-label learning and few-shot learning, and describe how we apply CTR in these settings.\n\nCode Ensemble for Zero-Label Learning. In the zero-label setting, we are given a new task along with a set of unlabeled data. * The question is how to decide the code for this new task without labeled data. Our main idea is to select one of the training tasks (319 in total in our experiments) and use its learned code for this new task. We first obtain a set of candidate codes by examining how much a code gives predictions that deviate from a uniform distribution on the unlabeled data (Zhao et al., 2021). The candidate set is formed by N (set to 60 in our experiments) codes with the lowest deviations. We then ensemble the candidate set of codes to predict pseudo labels on unlabeled data, and select the code with the highest pseudo-label accuracy.\n\nBitwise Search for Few-Shot Learning. In the few-shot setting, we are given a new task along with a set of labeled data. We use this set of labeled data as our validation set to search for a high-performing code. Our preliminary study shows that one can control the output of CTR by changing one bit in the code vector z. Inspired by this, we first examine the validation-set accuracy for codes of training tasks, and select the code with the best accuracy as an initialization. Then we iteratively change a single bit of the selected code and evaluate the validation-set performance. At each iteration, we keep the updated code with higher performance. Finally, the code vector that obtains the best result on the validation set is taken as the test-task code. Our preliminary study shows that a certain code value usually occurs in a specific position of the code vector, and each position usually only has a small set of code values. Motivated by this, we only changes a small set of valid code candidates for each position.\n\nIn both the zero-label and few-shot settings, after we obtain a code vector z for the new task, we use the vector to obtain a composition task representation by indexing z in the codebook C. The task representations are then passed through the decoder and the LLM to perform the new task, similar to training time. This is also illustrated in the right part of Figure 2.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETUP\n\nWe conduct extensive quantitative experiments to validate the performance of cross-task generalization of CTR. We mainly consider two settings, the zero-label setting and the few-shot setting. Aside from quantitative experiments, we perform qualitative analysis to understand the cross-task generalization ability by investigating the association between the discrete latent codes and the tasks.\n\n4.1.1 DATASETS\n\nCTR requires a large multi-task set for training, and a held-out set of tasks whose types are never seen during training for evaluation. We follow the T0 benchmark. The training part consists of\n\n*To clarify, for the zero-label setting, the unlabeled data equals the inputs of the test task. Note that the\n\ninputs naturally exist as long as we perform the evaluation. So it requires no extra effort to acquire data.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n39 tasks of 8 task types, including closed-book question answering (QA), multiple-choice QA, extractive QA, sentiment analysis, paraphrase identification, topic classification, summarization, and structure to text. The test part consists of 11 tasks of 4 task types, including natural language inference (RTE (Dagan et al., 2006), CB (De Marneffe et al., 2019), ANLI/R1-R3 (Nie et al., 2020)), coreference resolution (WSC (Levesque et al., 2012), Winogrande (Sakaguchi et al., 2020)), sentence completion (COPA (Roemmele et al., 2011), StoryCloze (Mostafazadeh et al., 2017), Hellaswag (Zellers et al., 2019)), and word sense disambiguation (WiC (Pilehvar & Camacho-Collados, 2019)). Both the training and test parts are disjoint in task types, ensuring the zero-label setting. We follow T0 to use the accuracy on the validation split of test tasks as the metric.\n\n4.1.2 BASELINES\n\nFor the zero-label setting, we primarily compare CTR with the following baselines. It is noteworthy that all baselines share a similar model size as CTR (i.e., about 770M), thus are comparable. We provide implementation details of baselines in Appendix A.7.\n\n• T0 (Sanh et al., 2022) shares similar goals as CTR, which uses prompted multi-task training to improve the generalization performance. T0 reports the average results over multiple prompts.†.\n\n• Self-Training Since unlabeled data are accessed, we consider the self-training (Schick & Sch ̈utze, 2020) method as one of the baselines. Starting from T0, it uses T0 to label the unlabeled data, and further finetunes T0 with the pseudo-labeled data. It also reports average performance over prompts.\n\n• Manual-Code uses artificial feature vectors in place of CTR vector. Specifically, we manually label a set of artificially-designed features for each task, including the number of input fields of the task, whether it requires reasoning, and whether it is a classification task, etc. Each task can be represented as an discrete feature vector, each dimension associated with one of the aspects.\n\n• ZPS (Liao et al., 2022a) is method for zero-label prompt selection. It first labels a set of unlabeled data through prompt ensemble, and use the pseudo-labeled data to select the best natural language prompt for the test task. We apply ZPS to multi-task T0 as a baseline.\n\nFor the few-shot setting where there are 32 test-set labeled examples, based on the multi-task T0, we experimented the following five baseline methods.\n\n• Model Tuning directly finetunes the pretrained model using the test-set labeled data. Specifically, we follow the few-shot setting in Zheng et al. (2021) to use 16 labeled data for finetuning and another 16 labeled samples for model selection.\n\n• Prompt Tuning (Lester et al., 2021) introduces additional continuous prompts to the backbone model (i.e., T0) and trains the continuous prompts using the few labeled data.\n\n• GPS (Xu et al., 2022) is a genetic prompt search method. Based on T0, GPS gradually mutates the prompts with a generative model and uses the few labeled data to selects prompt candidates.\n\n• GRIPS (Prasad et al., 2022) is a gradient-free edit-based method for optimal prompt search.\n\n• Black-Box Tuning (BBT) (Sun et al., 2022) is a gradient-free few-shot prompt selection method. Unlike GPS and GRIPS, it searches for the best soft prompt embedding in a continuous space.\n\n4.1.3 TRAINING DETAILS\n\nWe instantiate our CTR with T5-Large (Raffel et al., 2019) being the LLM. We implement both the encoder and the decoder in our CTR model as two linear networks. We have experimented with various architectures (see Appendix A.4 for more details). For the first stage of training where the LLM is frozen, we use the Adam optimizer with a learning rate of 1e-2, a decay rate of 0.1, and a batch size of 2048. We use a codebook embedding dimension of 1024, which is the same as the hidden dimension. The CTR length is set at 10 and each position can be assigned values ranging from 0 to 127; i.e., the codebook size is 128. The hyperparameter β is set at 0.1. We experimented with different codebook sizes and CTR lengths, and detailed results are provided in Appendix A.5. For the second training phase where all parameters are updated, we use the Adam optimizer with a learning rate of 1e-4 and a batch size of 1024. We follow the training recipe as T0 (Sanh et al., 2022) for the rest of the hyperparameters.\n\n†T0 uses natural language prompts from PromptSource (Bach et al., 2022).\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nT0-Large Self-Training Manual-Code ZPS Our CTR\n\nModel Tuning Prompt Tuning GPS GRIPS BBT Our CTR\n\nRTE\n\nCB\n\n72.67 73.57 75.19 79.06 80.51\n\n75.31 77.08 77.68 71.56 71.19 80.51\n\n56.55 76.14 56.89 67.86 87.50\n\n80.95 76.90 79.64 70.89 57.26 83.93\n\nNatural Language Inference\n\nSentence Completion Co-reference WSD ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. WiC\n\nZero-Label Setting (unlabeled data of each test task)\n\n32.77 34.42 33.12 31.20 33.40\n\n32.15 32.90 32.49 31.10 34.40\n\n34.38 37.44 33.48 34.25 33.80\n\n85.36 87.45 75.76 88.00 92.00\n\n27.18 30.33 30.84 29.16 27.50\n\n93.04 94.54 93.10 93.43 90.10\n\nFew-Shot Setting (32 labeled data of each test task)\n\n35.73 31.89 32.71 32.14 33.79 34.40\n\n31.31 31.86 31.49 32.26 32.00 34.20\n\n35.93 35.53 37.56 34.77 35.30 36.60\n\n82.05 81.70 81.08 77.56 76.49 89.00\n\n41.86 31.18 28.11 26.44 28.95 35.07\n\n92.04 94.10 93.40 93.40 93.11 91.70\n\n63.94 57.08 61.16 65.38 56.58\n\n55.96 62.88 64.23 62.12 62.12 68.18\n\n54.35 56.56 54.10 53.43 49.40\n\n56.74 55.42 52.72 52.96 53.40 55.00\n\n50.33 50.75 51.45 49.84 62.50\n\n52.15 51.22 52.52 52.12 52.93 58.62\n\nAvg.\n\n54.79 57.38 54.33 56.61 58.88\n\n58.18 57.25 57.38 55.11 54.23 60.66\n\nTable 1: Main results of CTR and baselines on 11 test tasks under the zero-label setting and the fewshot setting. The zero-label setting allows using unlabeled data of the test task while the few-shot setting uses 32 labeled data of the test task. All methods share a similar model size (i.e., 770M).\n\n4.2 MAIN RESULTS AND ANALYSIS\n\nThe performance of cross-task generalization, respectively under the zero-label setting and the fewshot setting, are shown in Table 1. Our CTR outperforms all baseline methods on average under both settings. Comparing CTR with T0-Large and its variant (i.e., self-training), CTR outperforms them respectively by more than 4 points and by almost 1.5 points on average. The improvements potentially originate from two aspects—(a) the learned compositional task representations benefit from better generalization abilities than the discrete manual prompts used by T0/self-training; (b) our CTR can select a high-performing compositional task representation for the unseen task. Compared with Manual-Code, our CTR demonstrate significant advantages of more than 4.5 points, proving that artificially-designed features of tasks are unreliable, and CTR provides an effective way of automatically training such data-driven compositional task representations using multi-tasks.\n\nCompared with Model Tuning, Prompt Tuning, and BBT, where they require parameter updates over the test-task data, CTR shows better performance of cross-task generalization on average. Compared with baselines that do not tune parameters (i.e., GPS, GRIPS), CTR shows even larger and more consistent advantages on most of the test tasks; i.e., on 9/11 tasks CTR shows dominating performance.\n\nOn co-reference tasks , CTR performs worse under the zero-label setting and better under the fewshot setting. The zero-label setting uses pseudo-labeled data to select test codes while the few-shot setting uses real-labeled data to select codes. The reason for the decreased performance lies in that the pseudo data of the co-reference task were of low quality and did not select effective task codes.\n\nPlease refer to Appendix A.3 for generated zero-label cases from CTR .\n\n4.3 ABLATION STUDY\n\n4.3.1 ARE MANUAL PROMPTS NECESSARY?\n\nWe are interested in whether CTR will be further improved when combined with discrete manual prompts. We conduct comparative experiments of CTR respectively with and without manual prompts. Specifically, for CTR without manual prompts, the inputs are a direct concatenation of multiple text fields, with compositional task representations appended in front of it. For CTR with manual prompts, the inputs are constructed by leveraging T0 prompts , with CTR placed in front.\n\nThe results are presented in Table 2. We report both the CTR results as well as its “upper-bound” results. It is noteworthy that the “upper-bound” results are post-hoc, which are obtained by the best code/prompt with observing the test task performance, and are merely given for estimating the potential of the two methods. We shall observe that adding additional discrete manual prompts does not improve the CTR performance as well as the “upper-bound”. From the results, we shall conclude that (1) our CTR does not rely on manual prompts to obtain optimal performance. CTR can work as an alternative to the prompt-based methods. (2) By comparing the CTR performance and the CTR\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nupper-bound, there is still room to optimize the code-searching algorithm, and as a result to further improve the generalization performance.\n\nOur CTR\n\nupper-bound\n\nw/ Manual Prompt upper-bound\n\nRTE\n\nCB\n\n80.51 81.95\n\n72.56 80.87\n\n87.50 89.29\n\n82.14 85.71\n\nNatural Language Inference\n\nSentence Completion Co-reference WSD ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. WiC\n\n33.40 38.77\n\n35.00 41.50\n\n34.40 39.00\n\n35.20 39.80\n\n33.80 40.60\n\n38.90 44.60\n\n92.00 94.23\n\n90.00 93.00\n\n27.50 42.81\n\n30.30 38.76\n\n90.10 94.20\n\n92.70 95.60\n\n56.58 73.86\n\n55.68 71.59\n\n49.40 60.00\n\n52.00 59.90\n\n62.50 61.76\n\n55.33 60.97\n\nAvg.\n\n58.88 65.13\n\n58.17 64.75\n\nTable 2: Ablation study on manual prompts. It shows the results of our CTR respectively without and with manual prompts. The experiments are under the zero-label setting. The “upper-bound” results of each method are obtained by using the best code/prompt after observing test task performance and are merely given for estimating the potential of the method. Results show that additionally adding manual prompts does not necessarily improve performance, and the codebook learned by CTR can act as an alternative to previous manual prompts.\n\n4.3.2 TRAINING OBJECTIVES\n\nOur CTR consists of three training loss items. Intuitively, the LM loss is optimized to predict the correct answer of the tasks. The embedding loss and the commitment loss are optimized to minimize the distance between the query embeddings and the CTR that are lookup from the codebook embeddings. To study the effectiveness of each loss item, we conduct an ablation study on them, and the results are shown in Table 3. Results show that removing either of the loss items will drastically hurt the zero-label performance , proving that either loss item is indispensable for the training of CTR .\n\nLoss Function\n\nOur CTR Loss\n\nNatural Language Inference\n\nSentence Completion Co-reference WSD ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. WiC\n\nRTE\n\nCB\n\nAvg.\n\n80.51\n\n87.50\n\n33.40\n\nwo/ Commitment Loss wo/ Embedding Loss wo/ Commitment+Embedding\n\n76.53 74.01 79.06\n\n76.79 75.00 76.79\n\n33.30 33.70 30.90\n\n34.40\n\n32.60 33.20 33.70\n\n33.80\n\n38.00 34.30 35.70\n\n92.00\n\n27.50\n\n90.10\n\n56.58\n\n49.40\n\n62.50\n\n58.88\n\n85.00 75.00 84.00\n\n33.50 25.40 30.50\n\n74.70 86.00 85.90\n\n64.77 61.36 57.95\n\n49.90 54.30 55.60\n\n54.39 50.00 50.00\n\n56.32 54.75 56.37\n\nTable 3: Ablation study on CTR loss function. Results show that by removing either of the loss items, i.e., commitment loss, embedding loss or both, the performance decreases to varying degrees.\n\n4.4\n\nIN-DEPTH ANALYSIS\n\n4.4.1\n\nINTERPRETABILITY\n\nSince CTR is trained to represent tasks with compositional codes, each code associated with one of the key aspects of tasks, CTR demonstrates a certain degree of interpretability. Table 4 presents examples of CTR that shows how each compositional task code is possibly interpreted.\n\nInterestingly, it frequently occurred that tasks that share similar features indeed have the same compositional code. For example, the code 52 occurs in tasks that require extracting information from the given contexts, including samsum * (summarization task), wiki bio * (structured data to text task), and paws labeled final paraphrase task (paraphrase generation) etc. Another example is that code 111 exists in most of the tasks that require generating long setences, including multi news * and samsum * (both are summarization tasks).\n\nWe also validate these explanations on unseen test tasks. Results show that these possible explanations still hold. For example, the COPA task generates long answers given two candidate choices, and its compositional code has the code 111 (indicating sentence generation).\n\n4.4.2 CONTROLLABILITY\n\nCTR also demonstrates a certain degree of controllability. Specifically, by modifying one bit of the compositional task code, our CTR will exhibit a different task behavior. Table 5 shows several examples of controlling CTR. We shall observe that, given the same inputs, by simply changing one\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nCode: 41\n\nExplain: generate key words based on passages\n\nTASKS: wiki hop original choose best object interrogative 2 wiki hop original choose best object affirmative 3 common gen topics from the sentence\n\nwiki hop original generate subject and object common gen sentence to concepts\n\nCode: 52\n\nExplain: extract the main information of the article\n\nTASKS: samsum Sum up the following dialogue samsum To sum up this dialog samsum Given the above dialogue write a summary wiki bio key content wiki bio comprehension paws labeled final paraphrase task\n\nwiki bio what content\n\nsamsum Summarize this dialogue samsum Generate a summary for this dialogue\n\nCode: 69\n\nExplain: generate the answers given candidate choices\n\nTASKS: qasc qa with separated facts 1 qasc qa with separated facts 4 cosmos qa context description question answer text cosmos qa context question description answer text social i qa Show choices and generate answer trec fine grained LOC\n\nqasc qa with separated facts 2 qasc qa with combined facts 1 cosmos qa description context question answer text cosmos qa no prompt text trec fine grained LOC context first super glue copa best option\n\nCode: 111\n\nExplain: Generate long sentence\n\nTASKS: multi news summary scenario multi news distill samsum Write a dialogue that match this summary\n\nmulti news summarize multi news expand (reverse task) super glue copa best option\n\nTable 4: Examples of how compositional task codes can be possibly interpreted. We use a codebook size of 128. It shows the co-occurrence of tasks and codes. By analyzing common features shared by multiple tasks, we shall find that CTR embraces the advantage of interpretability.\n\nbit of the compositional task code, the task behavior of CTR turns from DIALOGUE GENERATION to TOPIC CLASSIFICATION, from REVIEW RATING to SUMMARIZATION, from SENTIMENT ANALYSIS to TOPIC CLASSIFICATION, etc.\n\nWe shall conclude— (1) Since the inputs are randomly sampled from all tasks, some of which are quite different in data distribution, CTR performs well on them, proving that CTR indeed learns the ability to perform different tasks, instead of simply memorizing/overfitting to a certain dataset. (2) CTR is capable of switching between different tasks by simply changing one bit of the compositional code, proving that CTR effectively encodes the “task behavior” factor into the bits of the compositional code while disentangling other factors.\n\n4.4.3 HOW CTR GENERALIZES TO NEW TASKS\n\nTo reveal how CTR essentially works, we use Figure 1 to explain how CTR generalizes from training tasks to unseen tasks such as COPA. During the training phase, CTR learns the compositional task code for each task, each code associated with certain features of the task. For example, code 69 is associated with many of the multiple-choice/reasoning-based QA tasks; code 111 is associated with most of the long-sentence-generation tasks, etc. For a new task, its task code is a re-arranged composition of the code existing in training tasks. If the new task shares the same code snippet as the training tasks, it inherits corresponding task features as well. In the example of Figure 1, a real case produced by our model, the new task COPA is performed by combining the abilities of reasoning-based QA, sentence generation, and multi-choice selection from training tasks. These observations are consistent with our motivation of learning CTR for cross-task generalization.\n\n4.4.4 GENERALIZING TO NEW UNSEEN DOMAINS/CLASSES\n\nAnother interesting finding is that, for tasks with limited categorical classes, e.g., topic classification tasks, CTR is capable of generalizing to new unseen classes/domains. Table 6 shows several examples. Given different inputs that are randomly sampled from a large set of tasks and the compositional task code of AGNews, CTR not only predicts topic classes that are originally defined\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nINPUT: If Jim moves some particles of matter farther apart, what will happen to the rate at which they can pass vibrations on to nearby particles decrease or increase? Assuming that: When particles of matter are closer together, they can more quickly pass the energy of vibrations to nearby particles.\n\nTASK CODE: [61, 96, 127, 113, 55, 107, 5, 87,\n\n, 5]\n\nFILL 52 → DIALOGUE: W: Hi, Jim. I’m Jim. I’m a physicist at the University of California, Berkeley. FILL 37 → TOPIC CLASSIFICATION: Science and Technology\n\nINPUT: A historic victory for the separatist Parti Quebecois was marred by an attack on a victory rally for premier-elect Pauline Marois. A man opened fire during her victory speech, killing one person and critically wounding another, reports CTV. ... “I have convictions and I am going to defend them.” Marois said during her victory speech. “There will be a referendum when the Quebec population wants a referendum.”\n\nTASK CODE: [31,\n\n, 4, 113, 55, 107, 18, 87, 12, 63]\n\nFILL 68 → REVIEW RATING: 1 star FILL 93 → SUMMARIZATION : One person was killed and another critically injured in a shooting at a victory rally for the separatist Parti Quebecois in Quebec City on Sunday.\n\nINPUT: Paragraph: I’ve been here a few times and I like that it’s right up the street. On Tuesday and Wednesday they have specials on manicures and pedicures which is $5 cheaper then the original price.\n\nTASK CODE: [23, 90, 76, 15, 2, 79,\n\n, 1, 119, 19]\n\nFILL 13 → SENTIMENT ANALYSIS: Positive FILL 5 → TOPIC CLASSIFICATION : Business.\n\nTable 5: Examples of controlling the compositional codes. Given the input and a compositional task code, each time we modify one bit of the code, CTR begins to perform a different task.\n\nPredict with the code for AGNews (topic classification): [23, 90, 76, 15, 2, 79, 39, 1, 119, 19] Original AGNews classes: {World, Sports, Business, Science and technology}\n\nINPUT: Fears for T N pension after talks Unions representing workers at Turner Newall say they are ‘disappointed’ after talks with stricken parent firm Federal Mogul. OUTPUT: Business\n\nINPUT: There are 10 apples on an apple tree. Three fall off. Now there are X apples.\n\nOUTPUT: Math problem\n\nINPUT: Stuning even for the non-gamer This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate game music! It would impress anyone who cares to listen! OUTPUT: Music\n\nINPUT: Slack (2003) compares three groups that conducted biological research at Yale during overlapping periods between 1910 and 1970. Yale proved important as a site for this research. ... Hutchinson’s example shows that new models for research groups are needed, especially for those that include extensive field research. OUTPUT: Ecology\n\nTable 6: Examples of CTR generalizing to new unseen classes/domains. The inputs are randomly selected from all tasks other than AGNews. The first case predict the same classes as AGNews defines, while the latter three cases predict new classes that are never seen within AGNews. It shows the codebook of CTR can generalize to new unseen classes/domains.\n\nby AGNews, but also predicts new topic classes that never occur in AGNews. Please refer to Appendix A.1 and Appendix A.2 for more cases.\n\n5 CONCLUSIONS\n\nIn this paper, we explore cross-task generalization from a new perspective—compositional generalization. We propose the Compositional Task Representations (CTR) method that learns a discrete compositional codebook for tasks and generalizes to new unseen tasks by forming new compositions of the task codes. For the inference of CTR , we propose two algorithms — Code Ensemble and Bitwise Search, respectively for zero-label and few-shot settings. Experiments demonstrate that our CTR significantly outperforms existing prompted-based methods on both zero-label and few-shot settings. Analysis of the learned compositional task codes proves that some of the CTR codes show certain degrees of interpretability and controllability.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. Ext5: Towards extreme multi-task scaling for transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=Vzh1BFUCiIX.\n\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684, 2021.\n\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 93–104, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/ v1/2022.acl-demo.9. URL https://aclanthology.org/2022.acl-demo.9.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCanLanguage models are few-shot dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Adlearners. vances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Joaquin Qui ̃nonero-Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alch ́e Buc (eds.), Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pp. 177–190, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-33428-6.\n\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pp. 107–124, 2019.\n\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3816–3830, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.295. URL https://aclanthology.org/2021. acl-long.295.\n\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning. Citeseer, 2012.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.\n\nChonghua Liao, Yanan Zheng, and Zhilin Yang. Zero-label prompt selection. arXiv preprint\n\narXiv:2211.04668, 2022a.\n\nJunwei Liao, Duyu Tang, Fan Zhang, and Shuming Shi. Skillnet-nlg: General-purpose natural language generation with a sparsely activated approach. arXiv preprint arXiv:2204.12184, 2022b.\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\n\nunderstands, too. arXiv preprint arXiv:2103.10385, 2021.\n\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3470–3487, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long. 244. URL https://aclanthology.org/2022.acl-long.244.\n\nNasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. LSIn Proceedings of the 2nd Workshop on LinkDSem 2017 shared task: The story cloze test. ing Models of Lexical, Sentential and Discourse-level Semantics, pp. 46–51, Valencia, Spain, April 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-0906. URL https://aclanthology.org/W17-0906.\n\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4885–4901, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.441. URL https://aclanthology.org/2020.acl-main.441.\n\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for In Proceedings of the 2019 Conference evaluating context-sensitive meaning representations. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1267–1273, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1128. URL https://aclanthology.org/N19-1128.\n\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based instruction search for prompting large language models. arXiv preprint arXiv:2203.07281, 2022.\n\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019.\n\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternaIn AAAI Spring Symposium: Logical\n\ntives: An evaluation of commonsense causal reasoning. Formalizations of Commonsense Reasoning, pp. 90–95, 2011.\n\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\n\nsarial winograd schema challenge at scale. In AAAI, pp. 8732–8740. AAAI Press, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\n\nTimo Schick and Hinrich Sch ̈utze. It’s not just size that matters: Small language models are also\n\nfew-shot learners. CoRR, abs/2009.07118, 2020.\n\nTimo Schick and Hinrich Sch ̈utze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255–269, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.20. URL https: //aclanthology.org/2021.eacl-main.20.\n\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222–4235, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL https://aclanthology.org/2020. emnlp-main.346.\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning for\n\nlanguage-model-as-a-service. arXiv preprint arXiv:2201.03514, 2022.\n\nDuyu Tang, Fan Zhang, Yong Dai, Cong Zhou, Shuangzhi Wu, and Shuming Shi. One model, multiple tasks: Pathways for natural language understanding. arXiv preprint arXiv:2203.03312, 2022.\n\nAaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learnIn I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and ing. R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf.\n\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705, 2022.\n\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=gEZrGCozdqR.\n\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Zeroprompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization. arXiv preprint arXiv:2201.06910, 2022.\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791–4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/ P19-1472.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nTony Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving\n\nfew-shot performance of language models. ArXiv, abs/2102.09690, 2021.\n\nYanan Zheng, Jing Zhou, Yujie Qian, Ming Ding, Jian Li, Ruslan Salakhutdinov, Jie Tang, Sebastian Ruder, and Zhilin Yang. Fewnlu: Benchmarking state-of-the-art methods for few-shot natural language understanding. CoRR, abs/2109.12742, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 EXAMPLES OF AGNEWS\n\nQUESTION: Fears for T N pension after talks Unions representing workers at Turner Newall say they are ’disappointed’ after talks with stricken parent firm Federal Mogul. MODEL ANSWER (WITHIN THE ORIGINAL CLASSES): Business\n\nQUESTION: Prediction Unit Helps Forecast Wildfires (AP) AP - It’s barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar. MODEL ANSWER (WITHIN THE ORIGINAL CLASSES): Science and technology\n\nINPUT: How did serfdom develop in and then leave Russia? MODEL OUTPUT (OUT OF ORIGINAL CLASSES): History\n\nINPUT: There are 10 apples on an apple tree. Three fall off. Now there are X apples. MODEL OUTPUT (OUT OF ORIGINAL CLASSES): Math problem\n\nINPUT: the rock is destined to be the 21st century’s new “conan” and that he’s going to make a splash even greater than arnold schwarzenegger, jean-claud van damme or steven segal. MODEL OUTPUT (OUT OF ORIGINAL CLASSES): Film\n\nINPUT: E. D. Abbott Ltd - Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972. MODEL OUTPUT (OUT OF ORIGINAL CLASSES): Company\n\nINPUT: Stuning even for the non-gamer This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! MODEL OUTPUT (OUT OF ORIGINAL CLASSES): Music\n\nINPUT: E. D. Abbott Ltd - Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972. MODEL OUTPUT (OUT OF ORIGINAL CLASSES): Company\n\nQUESTION: Slack (2003) compares three groups that conducted biological research at Yale during overlapping periods between 1910 and 1970. Yale proved important as a site for this research. The leaders of these groups were Ross Granville Harrison, Grace E. Pickford, and G. Evelyn Hutchinson, and their members included both graduate students and more experienced scientists. All produced innovative research, including the opening of new subfields in embryology, endocrinology, and ecology, respectively, over a long period of time. Harrison’s group is shown to have been a classic research school; Pickford’s and Hutchinson’s were not. Pickford’s group was successful in spite of her lack of departmental or institutional position or power. Hutchinson and his graduate and postgraduate students were extremely productive, but in diverse areas of ecology rather than one focused area of research or the use of one set of research tools. Hutchinson’s example shows that new models for research groups are needed, especially for those that include extensive field research. MODEL ANSWER (OUT OF ORIGINAL CLASSES): Ecology\n\nTable 7: Examples of CTR by the compositional code of AG News. Given different inputs that are randomly sampled from a large set of tasks, and the compositional task code of AGNews, CTR not only predicts topic classes that are originally defined by AGNews, but also predicts new topic classes that never occur in AGNews .\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA.2 EXAMPLES OF DBPEDIA 14\n\nQUESTION: Federation of International Trade Associations - The Federation of International Trade Associations (FITA) based in Reston Virginia and New York New York USA was founded in 1984. It fosters international trade by seeking to strengthen the role of associations in the United States Mexico and Canada. FITA is the strategic partner of the United States Commercial Service for ecommerce. company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work MODEL ANSWER (WITHIN THE ORIGINAL CLASSES): Company\n\nQUESTION: Furian knife In this movie sequel, Vin Diesel returns as Riddick, an escaped convict with a price on his head. Riddick has been hiding on a snow planet for the last five years, when a group of mercenaries try to capture him. Riddick returns to the planet Helion, and finds out that his friend Jack, is in prison on Crematoria, a very hot planet. While on Helion, the planet is invaded by the Necromongers, led by Lord Marshal (Colm Feore), who wants to rule the universe. Riddick is captured by the mercs and flown to the same unsavoury and possibly illegal prison Kyra is in. Turns out Jack is now known as Kyra (Alexa Davalos) and is tough as nails. MODEL ANSWER (WITHIN THE ORIGINAL CLASSES): Film\n\nQUESTION: lila abu-lughod’s: - name - alma mater - website - known for - birth date - employer - nationality - occupation Bio: lila abu-lughod -lrb- born 1952 -rrb- is an american with palestinian and jewish ancestry who is professor of anthropology and women’s and gender studies at columbia university in new york city. a specialist of the arab world, her seven books, most based on long term ethnographic research, cover topics from sentiment and poetry to nationalism and media, from gender politics to the politics of memory. MODEL ANSWER (WITHIN THE ORIGINAL CLASSES): Educational Institution\n\nINPUT: when he dug his massive body into the batter’s box tuesday night at legends field during an exhibition game against the new york yankees, adam dunn not only wasn’t hitting his weight, he wasn’t hitting the weight of an UNK bag of baseballs. MODEL OUTPUT (OUT OF ORIGINAL CLASSES): Sports\n\nINPUT: We know that, thanks to our DNA, each of us is a little bit different. Some of those differences are obvious, like eye and hair color. Others are not so obvious, like how our bodies react to medication. Researchers are beginning to look at how to tailor medical treatments to our genetic profiles, in a relatively new field called pharmacogenomics. Some of the biggest breakthroughs have been in cancer treatment. MODEL OUTPUT (OUT OF ORIGINAL CLASSES): Science\n\nINPUT: how did athenians make money? Other Greek cities set up democracies, and even though most followed an Athenian model, none were as powerful, stable, nor as well-documented as that of Athens. MODEL OUTPUT (OUT OF ORIGINAL CLASSES): City\n\nINPUT: Great food, portions could be smaller. A little pricey for Middleton. Would try somewhere else next time but would also not hesitate to return MODEL OUTPUT (OUT OF ORIGINAL CLASSES): Restaurant\n\nINPUT: Facts: - name: mr. marcus - spelling: us - caption: mr. october 2007 marcus at a porn star karaoke event, - height: 5 8 - alias: mister marcus, mr. marquis - image: mr marcus, 2007.jpg - birth date: 4 september 1970 - birth name: jesse spencer - weight: 200 lb kg on - ethnicity: black - birth place: pomona, california, usa - hair color: black - number of films: 1,782 -lrb- per iafd -rrb- as a performer & 36 as a director - eye color: brown mr. marcus. MODEL OUTPUT (OUT OF ORIGINAL CLASSES): Actor\n\nTable 8: Examples of CTR by the compositional code of DBpedia 14. Given different inputs that are randomly sampled from a large set of tasks, and the compositional task code of DBpedia 14, CTR not only predicts topic classes that are originally defined by DBpedia 14, but also predicts new topic classes that never occur in DBpedia 14 .\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nA.3 EXAMPLES OF ZERO-LABEL TASKS\n\nCOPA Task Code: [111,7,69,15,2,79,39,1,119,19]\n\nINPUT: The woman retired. so... - She received her pension. - She paid off her mortgage. OUTPUT: She received her pension.\n\nINPUT: The girl found a bug in her cereal. so... - She poured milk in the bowl. - She lost her appetite. OUTPUT: She lost her appetite.\n\nWinogrande Task Code: [5,21,76,15,2,79,25,1,119,19]\n\nINPUT: Sarah was a much better surgeon than Maria so always got the easier cases. Sarah or Maria? OUTPUT: Sarah\n\nINPUT: Terry tried to bake the eggplant in the toaster oven but the was too big. eggplant or toaster? OUTPUT: eggplant\n\nStoryCloze/2016 Task Code: [23,68,54,10,2,79,25,103,120,44]\n\nINPUT: Sam loved his old belt. He matched it with everything. Unfortunately he gained too much weight. It became too small. - Sam went on a diet.- Sam was happy. OUTPUT: Sam went on a diet.\n\nINPUT: Larry bought a new motorcycle. He was excited to look cool. The first time he tried riding it he dropped it. He hurt his leg and had to go to the hospital. - Larry loved going to the hospital.- Larry became careful. OUTPUT: Larry became careful.\n\nWinograd Schema Challenge Task Code: [5, 68, 4, 10, 8, 22, 18, 124, 12, 97]\n\nINPUT: Jane gave Joan candy because she was hungry. Jane was hungry. Yes or no? OUTPUT: No\n\nINPUT: Joe Joe’s uncle can still beat him at tennis, even though he is 30 years older. Joe is 30 years older. Yes or no? OUTPUT: No\n\nTable 9: Examples of generalizing to tasks of unseen types. It shows cases of the CTR code for zero-label test tasks, and corresponding generated examples.\n\nA.4 COMPONENT ARCHITECTURE\n\nEncoder-Decoder\n\nNatural Language Inference\n\nSentence Completion Co-reference WSD CB ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. WiC\n\nRTE\n\nAvg.\n\nCTR: Linear - Linear\n\n80.51 87.50\n\n33.40\n\nLinear - None Linear - MLP Linear - Transformer Linear - RNN MLP - Linear MLP - None MLP - MLP MLP - Transformer MLP - RNN\n\n77.98 82.14 77.26 80.36 77.98 80.36 78.70 80.36 79.06 71.43 78.34 76.79 76.53 76.79 78.70 75.00 79.78 73.21\n\n32.40 32.40 32.70 33.80 32.40 32.90 33.20 32.40 33.20\n\n34.40\n\n33.30 34.00 31.80 32.50 33.40 33.30 33.30 33.00 33.70\n\n33.80\n\n33.60 35.60 35.70 36.30 35.20 33.80 34.30 33.90 34.50\n\n92.00\n\n27.50\n\n90.10\n\n56.58\n\n49.40\n\n62.50\n\n58.88\n\n86.00 84.00 88.00 87.00 88.00 87.00 84.00 82.00 85.00\n\n29.43 26.86 27.72 30.70 30.25 30.08 28.14 28.70 28.64\n\n89.30 84.10 89.10 87.30 89.70 90.60 90.90 91.00 90.90\n\n55.68 54.55 54.55 55.68 60.23 52.27 52.27 56.82 57.95\n\n57.70 55.70 58.00 57.20 55.70 56.60 55.00 55.70 56.90\n\n50.00 50.31 49.84 56.11 49.84 50.31 50.47 50.16 50.00\n\n57.05 55.92 56.89 57.79 56.84 56.54 55.90 56.12 56.71\n\nTable 10: Ablation study on different architectures of the CTR encoder and decoder. All experiments are conducted under the zero-label setting. We experiment two of the encoder, respectively the linear net and the multi-layer perceptron (MLP), and five alternatives of the decoder, including the linear net, bidirectional RNN, Transformer, MLP and removing the decoder (None). Results show that the simple “Linear - Linear” combination achieves the best performance.\n\nTable 10 shows the zero-label results when using different architecture of the encoder and the decoder. We experiment two of the encoder, respectively the linear net and the multi-layer perceptron (MLP), and five alternatives of the decoder, including the linear net, bidirectional RNN, Transformer, MLP and removing the decoder (None). Results show that simply using a linear network for both the encoder and the decoder performs best, while complicated architectures, e.g., Transformer and MLP, yield poor performance. This could because that the learning of codebook embeddings\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nas well as the code mapping does not necessarily need complicated transformation. When using complicated architectures, it instead increases the difficulties of the learning process.\n\nA.5 CODEBOOK SIZE AND CTR LENGTH\n\nThere are two critical hyper-parameters for CTR —the codebook size and the CTR length. We conduct experiments of different selection of the two hyper-parameters, and see how it influence the performance of zero-label generalization. Table 11 presents the results. We shall observe that when the codebook size decreases below 64, the performance decreases to a large degree. We conjecture that it is due to the capacity of a codebook size below 64 is not sufficient for representing the multiple aspects of the tasks. In addition, experimenting with a CTR length of 10 generally outperforms those with larger CTR length.\n\nCodebook Size\n\nCTR Length\n\nNatural Language Inference\n\nSentence Completion Co-reference WSD CB ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. WiC\n\nRTE\n\nAvg.\n\n128\n\n128 64 64 48 48\n\n10\n\n20 10 20 10 20\n\n80.51 87.50\n\n33.40\n\n77.62 80.36 75.81 73.21 79.06 82.14 77.26 78.57 79.42 60.71\n\n33.10 31.90 36.80 35.30 33.80\n\n34.40\n\n34.40 31.70 31.70 33.30 32.50\n\n33.80\n\n38.50 36.70 38.50 38.80 33.20\n\n92.00\n\n27.50\n\n90.10\n\n62.50\n\n49.40\n\n56.58\n\n58.88\n\n87.00 89.00 85.00 88.00 81.00\n\n31.60 26.90 30.86 33.54 29.00\n\n90.90 65.40 92.00 66.10 92.50\n\n54.55 48.86 56.82 60.23 50.00\n\n50.90 52.50 53.20 50.00 48.00\n\n55.80 54.70 50.47 53.45 53.92\n\n57.70 53.34 57.87 55.87 54.01\n\nTable 11: Ablation study on different codebook size and CTR length. All experiments are conducted under the zero-label setting. We experiment three of the codebook size, respectively 128, 64 and 48, and two of the CTR length, including 10 and 20. Results show that ”128 - 10” combination achieves the best performance.\n\nA.6 PERFORMANCE SENSITIVITY TO DIFFERENT SELECTION OF CODES\n\nWe want to verify whether different selections of codes will affect the performance of the model. Intuitively, if a code is merely noise that gets ignored by the model eventually, the model will not sensitive to different selections of codes and vice versa. Therefore, we randomly sampled several codes of training tasks and evaluate their performance on different test tasks. The results are shown in Table 12. As shown in the table, the model is highly sensitive to a different selection of codes. For example, the code of task amazon polarity user satisfied can perform well on RTE and significantly better than the performance of code of task cnn dailymail 3.0.0 news card view.\n\nTest Task\n\nCode of Train Task\n\nResults Min./Max.\n\nCB\n\nRTE\n\nWSC\n\nCOPA\n\nxsum read below DOC write abstract quoref Guess Title For Context imdb Writer Expressed Sentiment gigaword write a title for this sentence\n\namazon polarity user satisfied kilt tasks hotpotqa final exam cnn dailymail 3.0.0 news card view wiki hop original explain relation\n\ncosmos qa context description question answer text yelp review full format score cos e v1.11 question description option text xsum DOC write summary of above\n\ncos e v1.11 rationale quarel logic test wiki hop original choose best object interrogative 2 adversarial qa dbert based on\n\n35.83 41.79 52.74 42.50\n\n75.13 72.74 49.60 49.36\n\n52.74 63.51 57.90 53.19\n\n55.95 81.92 59.05 81.40\n\n35.83/52.74\n\n49.36/72.74\n\n52.74/63.51\n\n59.05/81.92\n\nTable 12: Performance sensitivity to different selections of codes.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nA.7 EXPERIMENTAL DETAILS\n\nFor the data preprocessing of all experiments, to balance the number of data for different tasks, we restrict the maximum data examples for each training task to be 50,000, which empirically yields better results.\n\nTraining details of each baseline method under the zero-label setting are illustrated as follows.\n\nT0-Large Based on T5-Large-LM-Adapted, it performs multi-task training for 10000 steps. We set the maximum length of input and target sequences to 384 and 32 respectively. We use the Adam optimizer with a learning rate of 1e-4, a dropout rate of 0.1, and a batch size of 1024. Following T0 Sanh et al. (2022), we use the same task prompts from PromptSource(Bach et al., 2022). We report the average accuracy of multiple prompts for each test task. Note that our reproduced T0Large results are much better than those reported in the original paper Sanh et al. (2022), which sets a much stronger baseline for comparison. We report the average accuracy of prompts for each test task. We believe our baseline is well-optimized. Because the performance on test tasks is comparable to the results of T0-3B reported in Sanh et al. (2022), even our baseline only contains 770M parameters.\n\nSelf-Training For a fair comparison, we randomly sample 32 unlabeled data for self-training, which is the same as CTR. The Self-Training method trains from the T0-Large with these pseudo-labeled examples for 5 epochs and reports average performance over prompts. For the training, we use a batch size of 32 and the Adam optimizer with a learning rate of 1e-4.\n\nManual-Code In practice, we manually label a set of artificially-designed features for each task, including the number of input fields of the task, whether it requires reasoning, whether it includes options into inputs, and whether it is a classification task, etc. Each task can be represented as an artificially-defined discrete feature vector, each dimension associated with one of the aspects. Manual-Code follows exact the same training recipe as the second training phase of our CTR. Training details are presented in Section 4.1.3.\n\nZero-Label Prompt Selection (ZPS) for producing pseudo-labeled data. Finally, we report the accuracy of the selected prompt.\n\n(Liao et al., 2022a) For each task, we use 32 unlabeled data\n\nFor the few-shot setting, we consider the following five baseline methods. All few-shot baselines are based on our reproduced T0-Large.\n\nModel Tuning We use the Adam optimizer with a batch size of 256 and a learning rate of 1e-4. We combine all training data of the test tasks for training. The maximum training step is 100. We use a validation set for model selection. Finally, we report the average accuracy of the selected best checkpoint.\n\nPrompt Tuning (Lester et al., 2021) We use the Adam optimizer with a batch size of 128 and a learning rate of 0.05. We combine all training data of the test tasks for training. The maximum training step is 100. The length of continuous prompts for each task is 20. We use a validation set for model selection. Finally, we report the average accuracy of the selected best checkpoint.\n\nGPS (Xu et al., 2022) We follow hyper-parameters reported in the original paper. Specifically, we run the GPS for 6 steps. At each step, new prompts are generated by a T5-xxl-lm-adapted model.\n\nGRIPS In our experiments, we set max patience P = 2, candidate l = 5, and step m = 5. For the rest of the hyper-parameters, we follow the original GRIPS paper(Prasad et al., 2022).\n\nBlack-Box Tuning (BBT) (Sun et al., 2022) In practice, we use the Adam optimizer with a learning rate of 0.05. For each test task, we train the soft prompt for 200 steps. We set the prompt length L = 10, subspace dimension d = 500, and cma budget 1000. For the rest of the hyper-parameters, we follow the original BBT(Sun et al., 2022).\n\n18",
    "reference": "# Summary Of The Paper\n\nThis paper studies learning composable task codes for soft-prompting of language models. The authors propose an approach similar to VQ-VAE where each task is associated with a set of discrete codes and embedding of these codes from a codebook lookup table is stacked as a soft-prompt for a language model. First, a 2D embedding for each training task is learned via another lookup table. Next, each row of this embedding matrix is used to search for the nearest code embedding in codebook lookup table. The model is trained with typical language model loss in addition to commitment and embedding losses for learning codebook embeddings. By sharing codes across tasks, this approach presents a natural task compositionality. The authors propose code ensembling for zero-shot learning and bitwise searching for few-shot learning. On a set of benchmark tasks, the model compares favorably to previous models; achieving on-par or better on average. The authors also experiment with interpretability -- similar tasks share codes -- and controllability -- bitwise perturbation exhibits different behaviors.\n\n# Strength And Weaknesses\n\n**Strengths** The paper studies a very important and interesting problem -- learning composable task codes for generalization to unseen tasks. I find the formulation of the problem using VQ-VAE natural and inference methods convincing. It is interesting to see that empirical results, especially zero-shot learning, improves compared to self-training or T0 models.\n\n**Weaknesses** There are a few points that require clarification and improvement.\n1. Does sharing a code necessarily mean that two tasks are similar in some sense? I found no evidence as to how important a code is for a task; hence, I can't say if a code is merely noise that gets ignored by the model eventually. For example, if you increase the number of codes, *L*, the model might be given more codes but not all of them would be used. There is not loss that encourages the LLM to utilize all codes as well.\n\n2. What is the effective number of codes that discrete compositional task code, *z*, has? Are all codes in z unique or are they frequently reused?\n\n3. Comparison to previous work needs clarification.\n- Are baselines multi-task learners or are they initialized with some pretrained embedding? Like, prompts can be initialized or they can be trained in mutli-task fashion.\n- What are the architectural details of other models, including model size, are they comparable?\n- What is the performance of model tuning if you use all 32 examples for training? I think it is a bit unfair to use all examples for CTR while using only half of them for others.\n- You mention that other methods update extra parameters as a weakness but this is not true. Prompt tuning only updates an additional prompt while the LLM is fixed. On the other hand, CTR updates a codebook and LLM is also fine-tuned. I think this is less scalable overall compared to parameter efficient updates.\n\n4. How do you choose hyperparameters? Like, \"CTR length\", \"codebook size\", or *N=60*. I don't see a clear pattern on final performance and no training/dev results are given.\n\n5. Could you clarify:\n- How do you combine with manual prompts? Are they appended as additional soft-prompt vectors?\n- What is the performance if you haven't updated the LLM? This would be a fairer comparison to parameter efficient models.\n- What is the performance w.r.t. increasing data size for few-shot learning?\n- Why does the model perform worse on co-reference?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI find the composable task codes novel and the paper is well written. I believe results can't be reproducible without proper details on hyper parameters.\n\n# Summary Of The Review\n\nI think composable task codes, VQ-VAE style training for LLM fine-tuning, and generalization to novel tasks during inference are strong points of the paper. But, there are still pieces that are unclear and needs improvement.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDISPARATE IMPACT IN DIFFERENTIAL PRIVACY FROM GRADIENT MISALIGNMENT\n\nMaria S. Esipova, Atiyeh Ashari Ghomi, Yaqiao Luo & Jesse C. Cresswell Layer 6 AI {maria, atiyeh, emily, jesse}@layer6.ai\n\nABSTRACT\n\nAs machine learning becomes more widespread throughout society, aspects including data privacy and fairness must be carefully considered, and are crucial for deployment in highly regulated industries. Unfortunately, the application of privacy enhancing technologies can worsen unfair tendencies in models. In particular, one of the most widely used techniques for private model training, differentially private stochastic gradient descent (DPSGD), frequently intensifies disparate impact on groups within data. In this work we study the fine-grained causes of unfairness in DPSGD and identify gradient misalignment due to inequitable gradient clipping as the most significant source. This observation leads us to a new method for reducing unfairness by preventing gradient misalignment in DPSGD.\n\n1\n\nINTRODUCTION\n\nThe increasingly widespread use of machine learning throughout society has brought into focus social, ethical, and legal considerations surrounding its use. In highly regulated industries, such as healthcare and banking, regional laws and regulations require data collection and analysis to respect the privacy of individuals.1 Other regulations focus on the fairness of how models are developed and used.2 As machine learning is progressively adopted in highly regulated industries, the privacy and fairness aspects of models must be considered at all stages of the modelling lifecycle.\n\nThere are many privacy enhancing technologies including differential privacy (Dwork et al., 2006), federated learning (McMahan et al., 2017), secure multiparty computation (Yao, 1986), and homomorphic encryption (Gentry, 2009) that are used separately or jointly to protect the privacy of individuals whose data is used for machine learning (Choquette-Choo et al., 2020; Adnan et al., 2022; Kalra et al., 2021). The latter three technologies find usage in sharing schemes and can allow data to be analysed while preventing its exposure to the wrong parties. However, the procedures usually return a trained model which itself can leak private information (Carlini et al., 2019). On the other hand, differential privacy (DP) focuses on quantifying the privacy cost of disclosing aggregated information about a dataset, and can guarantee that nothing is learned about individuals that could not be inferred from population-level correlations (Jagielski et al., 2019). Hence, DP is often used when the results of data analysis will be made publicly available, for instance when exposing the outputs of a model, or the results of the most recent US census (Abowd, 2018).\n\nNot only must privacy be protected for applications in regulated industries, models must be fair. While there is no single definition that captures what it means to be fair, with regards to modelbased decision making fairness may preclude disparate treatment or disparate impact (Mehrabi et al., 2021). Disparate treatment is usually concerned with how models are applied across populations, whereas disparate impact can arise from biases in datasets that are amplified by the greedy nature of loss minimization algorithms (Buolamwini & Gebru, 2018). Differences in model performance across protected groups can result in a significant negative monetary, health, or societal impact for individuals who are discriminated against (Chouldechova & Roth, 2020).\n\n1Examples of laws governing data privacy include the General Data Protection Regulation in Europe, Health Insurance Portability and Accountability Act in the USA, and Personal Information Protection and Electronic Documents Act in Canada.\n\n2In the USA, fair lending laws including the Fair Housing Act, and Equal Credit Opportunity Act prohibit\n\ndiscrimination based on protected characteristics such as race, age, and sex.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nUnfortunately, it has been observed that disparate impact can be exacerbated by applying DP in machine learning (Bagdasaryan et al., 2019). Applications of DP always come with a privacy-utility tradeoff, where stronger guarantees of privacy negatively impact the usefulness of results - model performance in this context (Dwork & Roth, 2014). Underrepresented groups within the population can experience disparity in the cost of adding privacy, hence, fairness concerns are a major obstacle to deploying models trained with DP.\n\nThe causes of unfairness in DP depend on the techniques used, but are not fully understood. For the most widely used technique, differentially private stochastic gradient descent (DPSGD), two sources of error are introduced that impact model utility. Per-sample gradients are clipped to a fixed upper bound on their norm, then noise is added to the averaged gradient. Disparate impact from DPSGD was initially hypothesized to be rooted in unbalanced datasets (Bagdasaryan et al., 2019), though counterexamples were found by Xu et al. (2021). Recent research claims disparate impact to be caused by incommensurate clipping errors across groups, in turn effected by a large difference in average group gradient norms (Xu et al., 2021; Tran et al., 2021a).\n\nIn this work we highlight the disparate impact of gradient misalignment. In particular, we claim that the most significant cause of disparate impact is the difference in the direction of the unclipped and clipped gradients, which in turn can be caused by aggressive clipping and imbalances of gradient norms between groups. Our analysis of direction errors leads to a variant of DPSGD with properly aligned gradients. We explore this alternate method in relation to disparate impact and show that it not only significantly reduces the cost of privacy across all protected groups, it also reduces the difference in cost of privacy for all groups. Hence, it removes disparate impact and is more effective than previous proposals in doing so. On top of this, it is the only approach which does not require access to protected group labels, and thereby avoids disparate treatment of groups. In summary we:\n\n• Conduct a more fine-grained analysis of disparate impact in DPSGD, and demonstrate\n\ngradient misalignment to be the most significant cause;\n\n• Identify an existing algorithm, previously undiscussed in the fairness context, which prop-\n\nerly aligns gradients, and show it reduces disparate impact and disparate treatment;\n\n• Improve the utility of said algorithm via two alterations; • Experimentally verify that aligning gradients is more successful at mitigating disparate\n\nimpact than previous approaches.\n\n2 RELATED WORK\n\nPrivacy and Fairness: While privacy and fairness have been extensively studied separately, recently their interactions have come into focus. Ekstrand et al. (2018) considered the intersection of privacy and fairness for several definitions of privacy. This research gained new urgency when Bagdasaryan et al. (2019) observed that DPSGD exacerbated existing disparity in model accuracy on underrepresented groups. Disparate impact due to DP was further observed in Pujol et al. (2020) and Farrand et al. (2020) for varying levels of group imbalance. Using an adversarial definition of privacy, Jaiswal & Mower Provost (2020) found that overrepresented groups can incur higher privacy costs. Similar examples were shown in Xu et al. (2021) for DPSGD, and disparate impact was linked to groups having larger gradient norms.\n\nOther fairness-aware learning research has evaluated the fairness of a private model’s outcomes on protected groups. In this context fairness might refer to a statistical condition of non-discrimination with respect to groups (Mozannar et al., 2020; Tran et al., 2021b), for example, equalized odds (Jagielski et al., 2019), equality of opportunity (Cummings et al., 2019), or demographic parity (Xu et al., 2019; Farrand et al., 2020). Chang & Shokri (2021) empirically found that imposing fairness constraints on private models could lead to higher privacy loss for certain groups. We consider crossmodel fairness where the cost of adding privacy to a non-private model must be fairly distributed between groups.\n\nAdaptive Clipping: Many variations on the clipping procedure in DPSGD have been proposed to improve properties other than fairness. Adaptive clipping comes in many forms, but usually tunes the clipping threshold during training to provide better privacy-utility tradeoffs and convergence (Andrew et al., 2021; Pichapati et al., 2019). The convergence of DPSGD connects to the symmetry properties of the distribution of gradients (Chen et al., 2020) which are affected by clipping.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n3 BACKGROUND\n\n3.1 SETTING AND DEFINITIONS\n\nWe begin by laying out the problem setting and review the relevant definitions for discussing fairness in privacy. For concreteness we consider a binary classification problem on a dataset D which consists of n points of the form (xi, ai, yi), where xi ∈ Rd is a feature vector, yi ∈ {0, 1} is a binary label, and ai ∈ [K] refers to a protected group attribute which partitions the data. The group label ai can optionally be an attribute in xi, the label value yi, or some distinct auxiliary value. The goal is to train a model fθ : Rd → [0, 1] with parameter vector θ that is simultaneously useful and private, and in which the application of privacy is fair. Utility in the empirical risk minimization (ERM) problem is governed by the per-sample loss l : [0, 1] × {0, 1} → R, with the optimal model minimizing the objective L(θ; D) = 1 i∈D l(fθ(xi), yi), which happens for optimal parameters θ∗ = arg minθ L(θ; D). The requirement of privacy is applied to the model through its parameters; private parameters ̃θ must be obtained while exposing a minimal amount of private information in D. For this we apply the framework of differential privacy, recounted in the next section.\n\n(cid:80)\n\nn\n\nFairness of the privacy methodology can be measured in terms of the disparate impact that applying privacy has on the protected groups. As in Bagdasaryan et al. (2019), we use a version of accuracy parity, the difference in classification accuracy across protected groups after adding privacy. We denote a subset of the data containing all points belonging to group k as Dk = {(xi, ai, yi) ∈ D | ai = k}. A private model has accuracy parity for subset Dk if it minimizes the privacy cost\n\nπ(θ, Dk) = acc(θ∗; Dk) − E ̃θ[acc( ̃θ; Dk)], where the expectation is over the randomness involved in acquiring private model parameters. Of course, metrics other than classification accuracy could be used as required by the problem setting. Alternatively, fairness for privacy can be measured at the level of the loss function as in Tran et al. (2021a), which is more amenable to analyzing the causes of unfairness. The excessive risk over the course of training experienced by a group is\n\n(1)\n\nR(θ, Dk) = E ̃θ[L( ̃θ; Dk)] − L(θ∗; Dk). When the model is clear from context we denote R(θ; Dk) as Rk, and similarly for privacy cost πk. For both accuracy and loss we consider the gap between disparate impact values across groups. The privacy cost gap is πa,b = |πa − πb| for groups a, b ∈ [K], and the excessive risk gap refers to Ra,b = |Ra − Rb|. The goal of a fair private classifier is to minimize the privacy cost and/or excessive risk for all values of the protected group attribute, while maintaining small fairness gaps.\n\n(2)\n\n3.2 DIFFERENTIAL PRIVACY\n\nDifferential privacy (DP) (Dwork et al., 2006) is a widely used framework for quantifying the privacy consumed by a data analysis procedure. Formally, let D represent a set of data points, and M a probabilistic function, or mechanism, acting on datasets. We say that the mechanism is (ε, δ)-differentially private if for all subsets of possible outputs S ⊆ Range(M ), and for all pairs of databases D and D′ that differ by the addition or removal of one element, Pr[M (D) ∈ S] ≤ exp(ε) Pr[M (D′) ∈ S] + δ. (3)\n\nAlgorithm 1 DPSGD\n\nRequire: Iterations T , Dataset D, sampling rate q, clipping bound C0, noise multiplier σ, learning rates ηt\n\nInitialize θ0 randomly for t in 0, . . . , T − 1 do\n\nB ← Poisson sample of D with rate q for (xi, yi) in B do\n\ngi ← ∇θl(fθt(xi), yi) 1, C0 ̄gi ← gi · min ∥gi∥\n\n(cid:16)\n\n(cid:17)\n\n(cid:0)(cid:80)\n\n ̃gB ← 1 |B| θt+1 ← θt − ηt ̃gB\n\nFor the ERM problem, there are several ways to train a differentially private model (Chaudhuri et al., 2011). In this work we consider models that can be trained with stochastic gradient descent (SGD), such as neural networks, and focus on the most successful approach, DPSGD (Abadi et al., 2016), in which the Gaussian mechanism (Dwork & Roth, 2014) is applied to gradient updates as in Alg. 1. Since per-sample gradients gi generally do not have finite sensitivity, defined as ∆h = maxD,D′ ∥h(D) − h(D′)∥ for a function h, they are first clipped to have norm upper bounded by a fixed hyperparameter C0. Clipped gradients ̄gi in a batch B ⊂ D are aggregated into ̄gB and noise is added to produce ̃gB used in the parameter update.\n\ni∈B ̄gi + N (0, σ2C 2\n\n0\n\nI)(cid:1)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3.3 FAIRNESS CONCERNS FROM CLIPPING AND NOISE IN DPSGD\n\nThe two most significant steps in DPSGD, clipping and adding noise, can impact the learning process disproportionately across groups, but the exact conditions where disparate impact will occur have been debated (Bagdasaryan et al., 2019; Farrand et al., 2020; Xu et al., 2021; Tran et al., 2021a). The most concrete connection so far appears in (Tran et al., 2021a), where the expected loss L(θ; Da) is decomposed into terms contributing to the excessive risk at a single iteration for group a, Ra:\n\nProposition 1 (Tran et al. (2021a)). Consider the ERM problem with twice-differentiable loss l with respect to the model parameters. The expected loss E[L(θt+1; Da)] of group a ∈ [K] at iteration t is approximated up to second order in ∥θt+1 − θt∥ as:\n\nE[L(θt+1; Da)] ≈ L(θt; Da) − ηt⟨gDa , gD⟩ + η2\n\nt 2\n\nE[gT\n\n+ ηt⟨gDa , gD − ̄gD⟩ + η2 + η2 l )C 2\n\n2 Tr(H a\n\n0 σ2.\n\nt 2\n\nt\n\nBH a l gB] l ̄gB] − E[gT\n\n(cid:0)E[ ̄gT\n\nBH a\n\nBH a\n\nl gB](cid:1)\n\n(non-private term)\n\n(Rclip a )\n\n(Rnoise a\n\n)\n\nThe expectation is taken over the randomness of the DP mechanisms, and batches of data.\n\nTerms in the first line appear for ordinary SGD, and do not contribute to the excessive risk Eq. (2). The terms in the second line, Rclip a , are caused by clipping since they cancel when ̄gB = gB for every batch. They involve gradients gDa and Hessians H a l , averaged over datapoints belonging to group a. The final term, Rnoise , depends on the scale of noise added in Alg. 1, as well as the trace of the Hessian, also called the Laplacian, averaged over Da. Based on Prop. 1, Tran et al. (2021a) argue that clipping causes excessive risk to groups with large gradient norms, which can result from large input norms ∥xi∥. Whether or not a group is underrepresented has less influence. In the next section we provide a new perspective on Rclip\n\nand the underlying causes of unfairness in DPSGD.\n\na\n\na\n\n4 DISPARATE IMPACT IS CAUSED BY GRADIENT MISALIGNMENT\n\nClipping in DPSGD introduces two types of error to the clipped batch gradient ̄gB. It will generally have different norm than ∥gB∥, and be misaligned compared to the SGD batch gradient, gB. At a high level, gradient misalignment poses a more serious problem to the convergence of DPSGD than magnitude error, as illustrated in Fig. 1. Changing only the norm means gradient descent will still step towards the (local) minimum of the loss function, and any norm error could be completely compensated for by adapting the learning rate ηt. In contrast, a misaligned gradient could result in a step towards significantly worse regions of the loss landscape causing catastrophic failures of convergence. Misaligned gradients add bias which compounds over training, as underrepresented or complex groups are systematically clipped. For comparison, adding noise to the aggregated gradient does not add bias, so noise errors tend to cancel out over training. We aim to quantify the relative impact of these effects and how they contribute to the excessive risk.\n\nFigure 1: Direction errors from clipping are more severe than magnitude errors over the course of training and can lead to suboptimal convergence.\n\n(cid:16) ∥ ̄gB ∥\n\n(cid:17)\n\nWe can distinguish the effects of clipping by rewriting the clipped batch gradient as\n\n∥gB ∥ gB\n\nfor an orthogonal matrix MB such that ̄gB and MBgB are colinear. As a ̄gB=MB proof of concept that gradient misalignment is the more severe error we compared models trained by taking steps ∥ ̄gB ∥ ∥gB ∥ gB vs. MBgB with no noise added. These represent magnitude errors and direction errors from clipping, respectively. The models were trained on MNIST with class 8 undersampled, and the results compare the typical class 2 to the underrepresented class 8; full details are provided in App. B. As seen in Table 1, direction error is more detrimental to performance than magnitude error. In particular, it disproportionately increases loss and decreases accuracy on the underrepresented class 8.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Effect of direction vs. magnitude error on MNIST with class 8 undersampled. The results compare accuracy and loss on the typical class 2 to the underrepresented class 8.\n\nTYPE OF ERROR ACC 2 ACC 8\n\nLOSS 2\n\nLOSS 8\n\nMAGNITUDE DIRECTION\n\n99.0 96.8\n\n93.5 84.1\n\n0.002 0.076\n\n0.005 0.518\n\nOur first theoretical result quantifies the excessive risk from the two types of errors, and follows from a Taylor expansion of the expected loss using ̄gB in the gradient descent update compared to gB. The excessive risk from magnitude error comes from comparing gB to ∥ ̄gB ∥ ∥gB ∥ gB, while that of\n\ngradient misalignment is isolated by comparing ̄gB=MB\n\n(cid:16) ∥ ̄gB ∥\n\n∥gB ∥ gB\n\n(cid:17)\n\nto ∥ ̄gB ∥\n\n∥gB ∥ gB (see Fig. 1).\n\nProposition 2. Consider the ERM problem with twice-differentiable loss l with respect to the model parameters. The excessive risk due to clipping experienced by group a ∈ [K] at iteration t is approximated up to second order in ∥θt+1 − θt∥ as\n\nRclip\n\na ≈ ηt (cid:68)\n\n(cid:68)\n\ngDa , E (cid:104) ∥ ̄gB ∥\n\n(cid:104)(cid:16)\n\n1 − ∥ ̄gB ∥ ∥gB ∥\n\n(cid:17)\n\n(cid:105)(cid:69)\n\n+ η2\n\nt 2\n\nE\n\ngB (cid:105)(cid:69)\n\n(cid:104)(cid:16) ∥ ̄gB ∥2\n\n(cid:17)\n\n∥gB ∥2 − 1 (cid:0)(MBgB)T H a\n\nBH a gT\n\nl gB\n\n+ ηt\n\ngDa , E\n\n∥gB ∥ (gB−MBgB)\n\n(cid:104) ∥ ̄gB ∥2 ∥gB ∥2 where gDa , ̄gDa denote the average non-clipped and clipped gradients over group a at iteration t, H a l refers to the Hessian over group a, and MB is an orthogonal matrix such that ̄gB and MBgB are colinear. The expectations are taken over batches of data.\n\nl (MBgB)−gT\n\n, (Rdir a )\n\nBH a\n\n+ η2\n\nl gB\n\n(cid:1)(cid:105)\n\nE\n\nt 2\n\n(cid:105)\n\n(Rmag a )\n\nWe provide a derivation in App. A. Note that when the magnitude error is zero for all batches, ∥gB∥=∥ ̄gB∥, we have that Rmag a =0 as expected. As well, when there is no gradient misalignment then MB is the identity matrix for every batch, and so Rdir\n\na = 0.\n\na > Rdir\n\nTo determine the characteristics of groups that will have unfair outcomes from clipping in DPSGD we can distill a simpler condition for when Rdir b . Tran et al. (2021a) already provide such a condition for clipping overall, however it does not effectively account for the danger of gradient misalignment. Their condition is sufficient, but not necessary, and some of its looseness stems from the inequality xT y ≥ −∥x∥∥y∥ used to convert all terms in Rclip into expressions involving group gradient norms. This approach loses information about gradient direction. We instead propose a tighter analysis of Rdir Proposition 3. Assume the loss l is twice continuously differentiable and convex with respect to the model parameters. As well, assume that ηt ≤ (maxk∈[K] λk)−1 where λk is the maximum eigenvalue of the Hessian H k\n\nb using xT y = ∥x∥∥y∥ cos θ, where θ = ∠(x, y).\n\nl . For groups a, b ∈ [K], Rdir\n\na > Rdir\n\na − Rdir\n\nif\n\na\n\nb\n\nE (cid:2)∥ ̄gB∥(cos θa B = ∠(gDk , gB) and ̄θk\n\nB − cos ̄θa\n\nwhere θk\n\nB)(cid:3) > ∥gDb ∥\n\n∥gDa ∥\n\nE (cid:2)∥ ̄gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3) +\n\nE[∥ ̄gB ∥2] ∥gDa ∥ ,\n\n(4)\n\nB = ∠(gDk , ̄gB) for a group k ∈ [K]. Furthermore, the bound is tight.\n\nApp. A contains our proof. Prop. 3 shows that if the clipping operation disproportionately and sufficiently increases the direction error for group a relative to group b, then group a incurs larger excessive risk due to gradient misalignment.\n\nb\n\na − Rdir\n\nThe lower bound for Rdir inferred from Eq. 4 is tight, and in our experiments we empirically show that it is close to saturation in a typical case. Hence, when the direction errors for groups a and b are small (i.e. we expect that θi b ≈ 0 regardless of the size of ∥gDa ∥ relative to ∥gDb ∥. It follows that clipping does not negatively impact excessive risk if gradients remain aligned. On the other hand if direction error is not close to zero, large group gradient norms do exacerbate the error in direction, as the dominant term of Rdir a scales with ∥gDa ∥.\n\nB for i = a, b), we have that Rdir\n\na − Rdir\n\nB ≈ ̄θi\n\nThe excessive risk in Eq. 2 is evaluated at the end of training, whereas Props. 1 and 2 estimate it per-iteration. Fig. 1 demonstrates that the full impact of clipping errors may not be felt per-iteration, but only at convergence. Indeed what matters to the end user is how fair the final model is, not how fair any intermediate training step is. However, it is not possible to attribute overall excessive risk to the per-iteration terms Rdir i used in the expansions of Props. 1 and 2 differ at each iteration, and do not equal the overall optimal θ∗. Still, Table 1 demonstrates that gradient misalignment is the main cause of disparate impact, so we seek a method to prevent it.\n\n, since the optimal θ∗\n\na , and Rnoise\n\na , Rmag\n\na\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n(a) Left: Per-sample gradients colored based on group membership. Top: Local clipping in DPSGD Bottom: Global scaling in DPSGD-Global.\n\n(b) In DPSGD-Global-Adapt scaling alone does not guarantee finite sensitivity, so gradients with norm above Z are clipped to C0 (DPSGD-Global clips large gradients to 0 rather than C0).\n\nFigure 2: Illustration of privatization steps in DPSGD, DPSGD-Global, and DPSGD-Global-Adapt\n\n5 PREVENTING GRADIENT MISALIGNMENT IN DPSGD\n\nOur results so far show that gradient misalignment due to clipping is the most significant cause of unfairness in DPSGD. Logically, Rdir a would be minimized if privatization left the direction of gB unchanged. A promising avenue is to scale down all per-sample gradients in a batch by the same amount. This is the approach taken by DPSGD-Global (Bu et al., 2021), which was recently proposed to improve the convergence of DPSGD, and has not been discussed in the context of fairness before. Our theoretical results suggest that global scaling will reduce disparate impact.\n\nAlgorithm 2 DPSGD-Global(-Adapt)\n\nRequire: Iterations T , Dataset D, sampling rate q, clipping bound C0, strict clipping bound Z ≥ C0, noise multipliers σ1, (σ2), learning rates ηt, (clipping learning rate ηZ, threshold τ ≥ 0)\n\nInitialize θ0 randomly for t in 0, . . . , T − 1 do\n\nB ← Poisson sample of D with rate q for (xi, yi) in B do\n\nγi ←\n\ngi ← ∇θl(fθt(xi), yi) (cid:40) C0 Z , 0 ( C0\n\n ̄gi ← γigi (cid:0)(cid:80)\n\n∥gi∥ ≤ Z ∥gi∥ ), ∥gi∥ > Z\n\nDPSGD-Global (Alg. 2) aims to preserve privacy by scaling gradients as ̄gi = γgi, 0 < γ < 1. Of course, scaling alone is insufficient to ensure persample gradients have bounded sensitivity. However, supposing that there were a strict upper bound Z ≥ ∥gi∥ ∀ i ∈ D, then scaling all gradients by γ = C0/Z would guarantee bounded sensitivity of C0 for each ̄gi (Fig. 2a). Given sufficient smoothness of the loss function, for any sample of data there will be such an upper bound maxi∈D ∥gi∥, but determining it exactly cannot be done in a differentially private manner. DPSGD-Global sets Z as a hyperparameter without looking at the data, in the same way C0 is chosen in DPSGD. If Z fails to be a strict upper bound, any gradients with ∥gi∥ > Z are discarded to guarantee a bound on sensitivity. When Z is chosen sufficiently large, no gradients are discarded and gradient misalignment is avoided. The drawback of a large Z is that the scaled gradients ̄gi will become small and convergence of gradient descent may be hindered.\n\n ̃gB ← 1 |B| θt+1 ← θt − ηt ̃gB (Adaptively set Z): bt ← |{i : ∥gi∥ > τ · Z}| ̃bt ← 1 2)) Z ← Z · exp(−ηZ + ̃bt)\n\n|B| (bt + N (0, σ2\n\ni∈B ̄gi + N (0, σ2\n\n1C 2\n\nI)(cid:1)\n\n0\n\nIn addition to identifying that DPSGD-Global has the potential to reduce disparate impact, we propose two modifications to improve its utility. First, we note that discarding gradients with ∥gi∥ > Z can exacerbate disparate impact as it is often underrepresented groups that have large gradient norms (Xu et al., 2021). Instead, we clip large gradients to have norm C0, which preserves more information while maintaining finite sensitivity (Fig. 2b). Second, rather than choosing Z as a hyperparameter, we adaptively update Z to upper-bound maxi∈B ∥gi∥. When Z is larger than all gradients it should be reduced to scale down gradients less, but if gradients are being clipped, Z should be increased. Z can be updated each iteration by privately estimating bt, the number of gradients in B that are larger than Z times a tolerance threshold τ ≥ 0. Since bt is a unit sensitivity quantity we can estimate it privately as ̃bt = 1 2)). Then, we use the geometric update rule Z ← Z · exp(−ηZ + ̃bt) with a learning rate ηZ (cf. (Andrew et al., 2021)). When all samples\n\n|B| (bt + N (0, σ2\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nhave gradient norm less than or equal to τ · Z, then in expectation ̃bt = 0 and Z is decreased by a factor of exp(−ηZ). Alternatively, Z is increased when ̃bt > ηZ, which occurs with probability 0.977 when bt |B| . As a result, with high probability the algorithm will not have more than |B|ηZ + 2σ2 gradients with norm exceeding τ · Z.\n\n|B| ≥ ηZ + 2σ2\n\nWe call the method with our two alterations DPSGD-Global-Adapt, shown in Alg. 2 in red parentheses. We empirically find in Sec. 6 that both global approaches improve fairness compared to prior methods, and that DPSGD-Global-Adapt has improved utility over DPSGD-Global. While the alterations are minor, our main contributions are elucidating that gradient misalignment is the main cause of disparate impact, and identifying that global scaling can prevent this problem.\n\nBoth global methods apply the sampled Gaussian mechanism (Mironov et al., 2019) to gradient norms with a sensitivity of C0, and hence are amenable to the same DP analysis as DPSGD itself. In DPSGD-Global-Adapt, the additional step of privately estimating the number of gradients with norm larger than τ · Z must be accounted for in the overall DP guarantee via a composition of sampled Gaussian mechanisms. From the analysis in (Mironov et al., 2019), DPSGD-Global-Adapt is (ε, δ)-DP for any σ1, σ2 > 0, where ε can be determined numerically given δ. However, our adaptive method is empirically not sensitive to the exact count bt, so a relatively large amount of noise can be used, see (Andrew et al., 2021) for comparison. In practice we used σ2 ≈ 10σ1 which produced a negligible additional cost in the overall privacy budget.\n\nFinally, we note that other approaches for mitigating unfairness, specifically DPSGD-F (Xu et al., 2021) and that of Tran et al. (2021a), require protected group labels for the training set. Collecting such labels may expose individuals to additional privacy risks in the case of security breaches, or may be prohibited in practice. Both global methods have the advantage of not requiring protected group labels for training data, and treat all training examples on an equal footing, thereby avoiding disparate treatment, while disparate impact is mitigated by reducing gradient misalignment.\n\n6 EXPERIMENTS\n\nIn our experiments we provide evidence that gradient misalignment is the most significant cause of unfairness, and demonstrate that global scaling can effectively reduce unfairness by aligning gradients. Our code for reproducing the experiments is provided as supplementary material.\n\n6.1 EXPERIMENT SETTINGS\n\nFor all experiments, full details are provided in App. B. We use an artificially unbalanced MNIST training dataset where class 8 only constitutes about 1% of the dataset on average, and protected groups are the classes. We also use two census datasets popular in the ML fairness literature, Adult and Dutch (van der Laan, 2000), preprocessed as in Le Quy et al. (2022). For both datasets, “sex” is the protected group attribute which is balanced between males and females. Finally, we use the CelebA dataset (Liu et al., 2015) for binary classification on the gender label. The protected group attribute is whether the image contains eyeglasses. Images with eyeglasses comprise 12% of male images but only 2% of female images, and are more difficult to classify accurately.\n\nWe compare both global scaling techniques (Alg. 2) against two methods designed to reduce unfairness, DPSGD-F (Xu et al., 2021) (Alg. 3) and the Fairness-Lens method (Tran et al., 2021a) (Alg. 4), both of which are reviewed in App. B.5. Each method’s effectiveness in removing disparate impact is measured using privacy cost πa (Eq. 1), and excessive risk Ra (Eq. 2) per group, as well as the privacy cost gap πa,b, and excessive risk gap Ra,b between groups. For MNIST, the underrepresented group 8 is compared to group 2 (Xu et al., 2021). All experiments were run for 5 random seeds, and results are given as means ± standard errors.\n\nFor MNIST and CelebA, all methods train a convolutional neural network with two layers of 32 and 16 channels, 3x3 kernels, and tanh activations. Adult uses an MLP model with two hidden layers of 256 units, while Dutch uses a logistic regression model. For all private methods, we use an RDP accountant (Mironov, 2017) with δ = 10−6. As a baseline, for DPSGD we set σ = 1, C0 = 0.5 for Adult, σ = 1, C0 = 0.1 for Dutch, and σ = 0.8, C0 = 1 for image datasets. With this, training 20 epochs for tabular datasets, 60 epochs for MNIST and 30 epochs for CelebA gives ε = 3.41 for Adult, ε = 2.27 for Dutch, ε = 5.90 for MNIST, and ε = 2.49 for CelebA. DPSGD-F has negligibly\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nhigher ε, while our method achieves the same ε guarantees to three significant digits. Complete hyperparameters are given in App. B.2.\n\n6.2 RESULTS\n\nTable 2: Performance and Fairness metrics for MNIST\n\nMETHOD\n\nACC 2\n\nACC 8\n\nπ2\n\nπ8\n\nπ2,8\n\nLOSS 2\n\nLOSS 8\n\nR2\n\nR8\n\nR2,8\n\n98.0±0.1 84.3±1.1 -\nNON PRIVATE 89.0±0.1 26.3±0.4 8.9±0.1 57.9±1.3 48.9±1.3 0.67±0.01 2.56±0.04 0.61±0.01 2.24±0.03 1.63±0.03 DPSGD 89.5±0.1 59.3±0.4 8.5±0.1 24.9±1.3 16.4±1.3 0.65±0.01 1.47±0.04 0.59±0.01 1.16±0.03 0.56±0.04 DPSGD-F 90.6±0.2 62.0±2.6 7.4±0.1 22.2±2.6 14.8±2.7 0.34±0.01 1.31±0.04 0.28±0.01 0.99±0.03 0.71±0.04 DPSGD-G. DPSGD-G.-A. 92.0±0.2 65.5±1.2 6.0±0.2 18.8±0.9 12.8±0.8 0.35±0.01 1.20±0.04 0.29±0.01 0.89±0.03 0.60±0.03\n\n- 0.06±0.00 0.32±0.01\n\n-\n\n-\n\n-\n\n-\n\nTable 3: Performance and Fairness metrics for CelebA\n\nMETHOD\n\nACC W/O ACC W πW/O\n\nπW πW/O, W LOSS W/O LOSS W\n\nRW/O\n\nRW RW/O, W\n\nNON PRIVATE DPSGD DPSGD-F DPSGD-G. DPSGD-G.-A.\n\n-\n\n95.8±0.1 89.7±0.4 -\n86.5±0.2 74.0±0.6 9.3±0.3 15.7±0.6 6.4±0.7 91.8±0.2 79.7±0.5 4.0±0.2 10.0±0.6 6.0±0.6 7.2±0.6 4.5±0.5 93.1±0.3 82.5±0.5 2.7±0.3 5.2±0.5 3.6±0.4 94.2±0.1 84.5±0.2 1.6±0.2\n\n-\n\n-\n\n0.11±0.00 0.24±0.01 -\n0.60±0.01 1.34±0.05 0.49±0.01 1.10±0.05 0.61±0.05 0.32±0.01 0.97±0.04 0.21±0.01 0.73±0.04 0.52±0.04 0.21±0.01 0.57±0.05 0.10±0.01 0.33±0.05 0.24±0.04 0.17±0.00 0.45±0.01 0.06±0.00 0.21±0.01 0.15±0.01\n\n-\n\nTables 2 and 3 display the accuracy and loss, along with privacy cost and excessive risk metrics respectively for MNIST on classes 2 and 8 and CelebA on group W with eyeglasses, and group W/O without.3 Recall that higher is better for accuracy, but for all other metrics lower is better. According to the one-sided Wilcoxon signed rank test, both global methods have statistically significant (p < 0.05) improvement over DPSGD on accuracy, loss, privacy cost gap, and excessive risk gap. Similarly, DPSGD-Global-Adapt has statistically significant improvement over DPSGD-Global and DPSGD-F on accuracy and loss. The same conclusions hold for the Adult dataset, and also for Dutch with the exception of DPSGD-Global being comparable to DPSGD in loss, see Tables 4 and 5 in App. B.7. We infer that the global scaling technique mitigates unfairness, while our modifications further improve utility.\n\nNot only are final model metrics improved, we see that DPSGD-Global-Adapt trains more similarly to non-private SGD in Fig. 3 for Dutch (cf. Figs. 8, 9, and 10 in App. B.7 for Adult, MNIST, and CelebA). This shows the average train loss per iteration, and average norm of the batched gradient. The difference in loss for groups in DPSGD-Global-Adapt resembles that of the non-private method more closely than other methods. Consider Fig. 3 (bottom), where the group M average norm does not converge to 0 in DPSGD, a problem which is somewhat improved in DPSGD-F, while for FairLens the group F norms become much larger. In DPSGD-Global-Adapt the norms for both groups remain small, but importantly the gap between groups is reduced.\n\n3The FairLens method (Tran et al., 2021a) is not compared for MNIST and CelebA because the author-\n\nprovided code only handles binary classification problems, and does not scale to image datasets.\n\nFigure 3: Dutch dataset. Top: Train loss per epoch. Bottom: ∥gB∥ averaged over batches per epoch.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Adult dataset. Top Rdir Rmag\n\na , excessive risk due to magnitude error per group. See Prop. 2 for definitions.\n\na , excessive risk due to gradient misalignment per group. Bottom\n\na , and magnitude error Rmag Fig. 4 shows the excessive risk terms due to gradient misalignment Rdir for Adult at each iteration (see Figs. 11, 12, and 13 in App. B.7 for Dutch, MNIST, and CelebA). We see that global clipping almost completely removes direction errors as intended, but as a tradeoff increases magnitude error. However, we have argued that direction error is the more severe cause of disparate impact over the course of training, which is borne out by the results in Tables 1, 2 and 3, as well as 4, and 5 in App. B.7. Direction errors introduce bias which accumulates, whereas magnitude errors do not alter the convergence path, and noise errors add zero bias and tend to cancel out.\n\na\n\n6.3 TIGHTNESS OF LOWER BOUNDS\n\nb\n\na −Rdir\n\na − Rclip\n\nIn Fig. 5 we compare the usefulness of the lower bound of Rclip given in the proof of Theorem 3 in Tran et al. (2021a), to the lower bound we give in Prop. 3 for Rdir b . We see that while group 0 experiences disparate impact due to clipping, the lower bound from Tran et al. (2021a) is negative for each iteration, failing to capture that Rclip 1 . On the other hand, the true values of Rdir 1 are closely lower-bounded in our version, such that disparate impact due to direction error is accurately predicted. The assumptions of Prop. 3 are discussed in App. B.6.\n\n0 > Rclip\n\n0 − Rdir\n\nFigure 5: Comparison of excessive risk gaps R0,1 to lower bounds on Adult. Left: R0,1 due to clipping error, and bound from Tran et al. (2021a). Right: R0,1 due to direction error, and bound from our Prop. 3.\n\n7 DISCUSSION\n\nIn this paper we identified a core cause of disparate impact in DPSGD, gradient misalignment, and proposed a mitigating solution, global scaling. We empirically verified that global scaling is successful in improving fairness in terms of accuracy and loss over DPSGD and other fair baselines on several datasets. Our method has additional advantages over other fair baselines in that it does not require the collection of protected group data for training, does not involve disparate treatment, and it removes disparate impact for all groups simultaneously.\n\nIt is important to note that while global scaling is effective at reducing disparate impact by aligning gradients, it does not resolve the privacy-utility trade-off, which exists in any private mechanism fundamentally. Nor does it ensure that the model is non-discriminatory towards subgroups, only that adding privacy does not exacerbate unfairness. For example, biases in data collection or discriminatory modelling assumptions can cause disparate impact within the non-private model, which overlaying global scaling will not cure. Any models trained with global scaling should still be validated for fairness independently; failure to do so could unknowingly cause additional unfairness.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and In Proceedings of the 2016 ACM SIGSAC\n\nLi Zhang. Deep learning with differential privacy. conference on computer and communications security, pp. 308–318, 2016.\n\nJohn M. Abowd. The U.S. Census Bureau Adopts Differential Privacy. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2867, 2018. ISBN 9781450355520.\n\nMohammed Adnan, Shivam Kalra, Jesse C. Cresswell, Graham W. Taylor, and Hamid R. Tizhoosh. Federated learning and differential privacy for medical image analysis. Scientific reports, 12(1): 1–10, 2022.\n\nGalen Andrew, Om Thakkar, Brendan McMahan, and Swaroop Ramaswamy. Differentially Private Learning with Adaptive Clipping. In Advances in Neural Information Processing Systems, volume 34, pp. 17455–17466, 2021.\n\nEugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate impact on model accuracy. In Advances in Neural Information Processing Systems, volume 32, pp. 15479–15488, 2019.\n\nZhiqi Bu, Hua Wang, Qi Long, and Weijie J. Su. On the Convergence of Deep Learning with\n\nDifferential Privacy. arXiv preprint arXiv:2106.07830, 2021.\n\nJoy Buolamwini and Timnit Gebru. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81, pp. 77–91, 2018.\n\nNicholas Carlini, Chang Liu, ́Ulfar Erlingsson, Jernej Kos, and Dawn Song. The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks. In 28th USENIX Security Symposium, pp. 267–284, 2019.\n\nHongyan Chang and Reza Shokri. On the privacy risks of algorithmic fairness.\n\nIn 2021 IEEE\n\nEuropean Symposium on Security and Privacy, pp. 292–303, 2021.\n\nKamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially Private Empirical\n\nRisk Minimization. Journal of Machine Learning Research, 12(29):1069–1109, 2011.\n\nXiangyi Chen, Steven Z. Wu, and Mingyi Hong. Understanding Gradient Clipping in Private SGD: A Geometric Perspective. In Advances in Neural Information Processing Systems, volume 33, pp. 13773–13782, 2020.\n\nChristopher A Choquette-Choo, Natalie Dullerud, Adam Dziedzic, Yunxiang Zhang, Somesh Jha, Nicolas Papernot, and Xiao Wang. CaPC Learning: Confidential and Private Collaborative Learning. In International Conference on Learning Representations, 2020.\n\nAlexandra Chouldechova and Aaron Roth. A Snapshot of the Frontiers of Fairness in Machine\n\nLearning. Commun. ACM, 63(5):82–89, 2020. ISSN 0001-0782.\n\nRachel Cummings, Varun Gupta, Dhamma Kimpara, and Jamie Morgenstern. On the Compatibility of Privacy and Fairness. In Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization, pp. 309–315, 2019. ISBN 9781450367110.\n\nCynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy. Found. ISSN 1551-305X. doi: 10.1561/\n\nTrends Theor. Comput. Sci., 9(3–4):211–407, August 2014. 0400000042.\n\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating Noise to Sensitivity in Private Data Analysis. In Theory of Cryptography, pp. 265–284. Springer Berlin Heidelberg, 2006. ISBN 978-3-540-32732-5.\n\nMichael D. Ekstrand, Rezvan Joshaghani, and Hoda Mehrpouyan. Privacy for All: Ensuring Fair and Equitable Privacy Protections. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81, pp. 35–47, 2018.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nTom Farrand, Fatemehsadat Mireshghallah, Sahib Singh, and Andrew Trask. Neither Private Nor Fair: Impact of Data Imbalance on Utility and Fairness in Differential Privacy. In Proceedings of the 2020 Workshop on Privacy-Preserving Machine Learning in Practice, pp. 15–19, 2020.\n\nCraig Gentry. Fully Homomorphic Encryption Using Ideal Lattices. In Proceedings of the Forty-\n\nFirst Annual ACM Symposium on Theory of Computing, pp. 169–178, 2009.\n\nM.F. Hutchinson. A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines. Communications in Statistics - Simulation and Computation, 19(2):433–450, 1990. doi: 10.1080/03610919008812866.\n\nMatthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed Sharifi Malvajerdi, and Jonathan Ullman. Differentially Private Fair Learning. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 3000–3008, 2019.\n\nMatthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine learning: How private is private SGD? Advances in Neural Information Processing Systems, 33: 22205–22216, 2020.\n\nMimansa Jaiswal and Emily Mower Provost. Privacy Enhanced Multimodal Neural Representations for Emotion Recognition. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05): 7985–7993, 2020. doi: 10.1609/aaai.v34i05.6307.\n\nShivam Kalra, Junfeng Wen, Jesse C. Cresswell, Maksims Volkovs, and Hamid R. Tizhoosh. arXiv preprint\n\nProxyFL: Decentralized Federated Learning through Proxy Model Sharing. arXiv:2111.11343, 2021.\n\nTai Le Quy, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, and Eirini Ntoutsi. A survey on datasets for fairness-aware machine learning. WIREs Data Mining and Knowledge Discovery, pp. e1452, 2022.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\n\nIn Proceedings of International Conference on Computer Vision, December 2015.\n\nH. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, 2017.\n\nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A Survey\n\non Bias and Fairness in Machine Learning. ACM Comput. Surv., 54(6), 2021.\n\nIlya Mironov. R ́enyi differential privacy. 2017 IEEE 30th Computer Security Foundations Sympo-\n\nsium (CSF), Aug 2017. doi: 10.1109/csf.2017.11.\n\nIlya Mironov, Kunal Talwar, and Li Zhang. R ́enyi Differential Privacy of the Sampled Gaussian\n\nMechanism. arXiv preprint arXiv:1908.10530, 2019.\n\nHussein Mozannar, Mesrob Ohannessian, and Nathan Srebro. Fair learning with private demographic data. In International Conference on Machine Learning, pp. 7066–7075. PMLR, 2020.\n\nMilad Nasr, Shuang Songi, Abhradeep Thakurta, Nicolas Papemoti, and Nicholas Carlin. Adversary instantiation: Lower bounds for differentially private machine learning. In 2021 IEEE Symposium on Security and Privacy, pp. 866–882. IEEE, 2021.\n\nVenkatadheeraj Pichapati, Ananda Theertha Suresh, Felix X. Yu, Sashank J. Reddi, and Sanjiv Ku-\n\nmar. AdaCliP: Adaptive Clipping for Private SGD. arXiv preprint arXiv:1908.07643, 2019.\n\nDavid Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, Ashwin Machanavajjhala, and Gerome Miklau. Fair decision making using privacy-protected data. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 189–199, 2020.\n\nCuong Tran, My Dinh, and Ferdinando Fioretto. Differentially Private Empirical Risk Minimization under the Fairness Lens. In Advances in Neural Information Processing Systems, volume 34, pp. 27555–27565, 2021a.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nCuong Tran, Ferdinando Fioretto, and Pascal Van Hentenryck. Differentially Private and Fair Deep Learning: A Lagrangian Dual Approach. Proceedings of the AAAI Conference on Artificial Intelligence, 35(11):9932–9939, May 2021b.\n\nPaul van der Laan. The 2001 Census in the Netherlands Integration of Registers and Surveys. In\n\nInsee-Eurostat Seminar on Censuses after 2001, 2000.\n\nDepeng Xu, Shuhan Yuan, and Xintao Wu. Achieving differential privacy and fairness in logistic regression. In Companion Proceedings of The 2019 World Wide Web Conference, pp. 594–599, 2019. doi: 10.1145/3308560.3317584.\n\nDepeng Xu, Wei Du, and Xintao Wu. Removing Disparate Impact on Model Accuracy in Differentially Private Stochastic Gradient Descent. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 1924–1932, 2021.\n\nAndrew Chi-Chih Yao. How to generate and exchange secrets.\n\nIn 27th Annual Symposium on\n\nFoundations of Computer Science, pp. 162–167, 1986.\n\nA THEORETICAL RESULTS\n\nA.1 PROOFS OF MAIN RESULTS\n\nIn this section we provide complete proofs for our theoretical contributions.\n\nProposition 2. Consider the ERM problem with twice-differentiable loss l with respect to the model parameters. The excessive risk due to clipping experienced by group a ∈ [K] at iteration t is approximated up to second order in ∥θt+1 − θt∥ as\n\n(cid:68)\n\nRclip\n\na ≈ ηt (cid:68)\n\n+ ηt\n\ngDa , E\n\ngDa , E (cid:104) ∥ ̄gB ∥\n\n(cid:104)(cid:16)\n\n1 − ∥ ̄gB ∥ ∥gB ∥\n\n(cid:17)\n\n(cid:105)(cid:69)\n\n+ η2\n\nt 2\n\nE\n\n(cid:104)(cid:16) ∥ ̄gB ∥2\n\n∥gB ∥2 − 1\n\n(cid:17)\n\n(cid:105)\n\nBH a gT\n\nl gB\n\n∥gB ∥ (gB−MBgB)\n\n+ η2\n\nt 2\n\nE\n\n(cid:104) ∥ ̄gB ∥2 ∥gB ∥2\n\n(cid:0)(MBgB)T H a\n\nl (MBgB)−gT\n\nBH a\n\nl gB\n\ngB (cid:105)(cid:69)\n\n(Rmag a ) (cid:1)(cid:105)\n\n,\n\n(Rdir a )\n\nwhere gDa , ̄gDa denote the average non-clipped and clipped gradients over group a at iteration t, H a l refers to the Hessian over group a, and MB is an orthogonal matrix such that ̄gB and MBgB are colinear. The expectations are taken over batches of data.\n\nWe remark that assuming a twice-differentiable loss is a mild requirement in machine learning where most loss functions and models are designed to be smooth enough for backpropagation.\n\nProof.\n\nThe proof is based on a Taylor expansion of the excessive risk, as in Tran et al. (2021a).\n\n(cid:13) (cid:13) Let MB be an orthogonal matrix such that ̄gB = MB (cid:13) and gB and ∥ ̄gB ∥ ∥gB ∥ gB are colinear, and so the former characterizes direction error, and the latter error in magnitude. The excessive risk due to error in magnitude for group a at iteration t is then given by\n\n. In this way, ∥ ̄gB∥ =\n\n∥gB ∥ gB\n\n∥gB ∥ gB\n\n(cid:16) ∥ ̄gB ∥\n\n∥ ̄gB ∥\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:17)\n\n(cid:20)\n\n(cid:18)\n\nL\n\nθt − ηt\n\nE\n\n∥ ̄gB∥ ∥gB∥\n\n(cid:19)\n\n(cid:21)\n\ngB; Da\n\n− L (θt − ηtgB; Da)\n\n,\n\nin loss of using the update vector ∥ ̄gB ∥\n\n∥gB ∥ gB rather than gB, where the expectathe cost tion is over randomness of batch sampling. We perform second-order Taylor expansion of E\n\nand take the expectation to get that\n\n∥ ̄gB ∥\n\n(cid:17)(cid:105)\n\nL\n\n(cid:16)\n\n(cid:104)\n\nθt − ηt\n\n∥gB ∥ gB; Da\n\n(cid:20)\n\n(cid:18)\n\nL\n\nθt − ηt\n\nE\n\n∥ ̄gB∥ ∥gB∥\n\n(cid:19)(cid:21)\n\ngB; Da\n\n≈ L (θt; Da) − ηt\n\n(cid:28)\n\ngDa , E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:21)(cid:29)\n\ngB\n\n+\n\nη2 t\n2\n\nE\n\n12\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 gT\n\nBH a\n\nl gB\n\n(cid:21)\n\n.\n\nPublished as a conference paper at ICLR 2023\n\nHence,\n\nRclip\n\na = ηt⟨gDa , gD − ̄gD⟩ +\n\n= ηt⟨gDa , gD − ̄gD⟩ +\n\n(cid:28)\n\n(cid:28)\n\n− ηt\n\n+ ηt\n\ngDa , E\n\ngDa, E\n\nη2 t\n2 η2 t\n2 (cid:20) ∥ ̄gB∥ ∥gB∥ (cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:0)E[ ̄gT\n\nBH a\n\nl ̄gB] − E[gT\n\nBH a\n\nl gB](cid:1)\n\n(cid:0)E[ ̄gT\n\nl ̄gB] − E[gT BH a (cid:21)(cid:29)\n\nl gB](cid:1)\n\nBH a (cid:20) ∥ ̄gB∥2 ∥gB∥2 gT (cid:20) ∥ ̄gB∥2 ∥gB∥2 gT\n\n(cid:21)\n\n(cid:21)\n\nBH a\n\nl gB\n\nBH a\n\nl gB\n\nη2 t\n2 η2 t\n2\n\nE\n\nE\n\ngB\n\ngB\n\n(cid:21)(cid:29)\n\n+\n\n−\n\n(cid:28)\n\n= ηt\n\ngDa , gD − E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:28)\n\ngDa , E\n\n+ ηt\n\n(cid:28)\n\n= ηt\n\ngDa , gD − E\n\ngB\n\n(cid:20) ∥ ̄gB∥ ∥gB∥ (cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:21)(cid:29)\n\nη2 t\n2\n\n+\n\n(cid:29)\n\ngB\n\n(cid:21)\n\n− ̄gD\n\n+\n\n(cid:21)(cid:29)\n\nη2 t\n2\n\n+\n\n(cid:29)\n\ngB\n\n(cid:21)\n\n(cid:18)\n\nE\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 gT E (cid:2) ̄gT\n\nBH a\n\n(cid:18)\n\nη2 t\n2 (cid:20)(cid:18) ∥ ̄gB∥2\n\nE\n\nη2 t\n2\n\n∥gB∥2 − 1 E (cid:2) ̄gT\n\nBH a\n\nl ̄gB\n\n(cid:18)\n\ngB\n\n− ̄gD\n\n+\n\n(cid:28)\n\ngDa , E\n\n+ ηt\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\nBH a\n\nl gB\n\n(cid:21)\n\n− E (cid:2)gT\n\nBH a\n\nl gB\n\n(cid:3) − E\n\nl ̄gB\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 gT\n\nBH a\n\nl gB\n\n(cid:19)\n\n(cid:3)\n\n(cid:21)(cid:19)\n\n(cid:19)\n\n(cid:21)\n\nBH a gT\n\nl gB\n\n(cid:3) − E\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 gT\n\nBH a\n\nl gB\n\n(Rmag a )\n\n(cid:21)(cid:19)\n\n.\n\n(Rdir a )\n\nWe can also further simplify Rdir a linear transformation\n\na by using that ̄gD = E[ ̄gB], ̄gB = MB\n\n(cid:16) ∥ ̄gB ∥\n\n∥gB ∥ gB\n\n(cid:17)\n\nand that MB is\n\n(cid:28)\n\nRdir\n\na = ηt\n\ngDa , E\n\n(cid:18) ∥ ̄gB∥ ∥gB∥\n\ngB − MB\n\n(cid:20) ∥ ̄gB∥ ∥gB∥ (cid:20) ∥ ̄gB∥2 ∥gB∥2 (MBgB)T H a\n\nl (MBgB)\n\n(cid:19)(cid:21)(cid:29)\n\ngB\n\n(cid:18)\n\nE\n\nη2 t\n2\n\n(cid:21)\n\n− E\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 gT\n\nBH a\n\nl gB\n\n(cid:21)(cid:19)\n\n+\n\n(cid:28)\n\n= ηt\n\ngDa , E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:21)(cid:29)\n\n(gB − MBgB)\n\n+\n\nη2 t\n2\n\nE\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2\n\n(cid:0)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(5)\n\n(cid:21)\n\n(cid:1)\n\n.\n\nProposition 3. Assume the loss l is twice continuously differentiable and convex with respect to the model parameters. As well, assume that ηt ≤ (maxk∈[K] λk)−1 where λk is the maximum eigenvalue of the Hessian H k\n\nl . For groups a, b ∈ [K], Rdir\n\na > Rdir\n\nif\n\nb\n\nE (cid:2)∥ ̄gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) >\n\n∥gDb ∥ ∥gDa ∥\n\nE (cid:2)∥ ̄gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3) +\n\nE[∥ ̄gB∥2] ∥gDa ∥\n\n,\n\n(6)\n\nwhere θk\n\nB = ∠(gDk , gB) and ̄θk\n\nB = ∠(gDk , ̄gB) for a group k ∈ [K]. Furthermore, the bound is tight.\n\nAgain, requiring a twice continuously differentiable loss is a mild requirement. However, when neural networks are used most loss functions are non-convex. Empirically we see in Fig. 5 that the lower bound can still apply in practice. The requirement on the learning rate is under the control of the practitioner, and we have verified that in practice it can be satisfied.\n\nProof.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nThis proof follows some steps presented in Lemma 2 of Tran et al. (2021a). We seek a simplified condition for when the following is positive,\n\nRdir\n\na − Rdir\n\nb = ηt\n\n(cid:28)\n\ngDa, E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:21)(cid:29)\n\n(cid:28)\n\n(gB − MBgB)\n\n− ηt\n\ngDb , E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:21)(cid:29)\n\n(gB − MBgB)\n\n+\n\n−\n\nη2 t\n2 η2 t\n2\n\nE\n\nE\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 (cid:20) ∥ ̄gB∥2 ∥gB∥2\n\n(cid:0)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(cid:21)\n\n(cid:1)\n\n(7)\n\n(cid:0)(MBgB)T H b\n\nl (MBgB) − gT\n\nBH b\n\nl gB\n\n(cid:21)\n\n(cid:1)\n\n.\n\nLooking at one of the inner product terms, we use that ⟨x, y⟩ = ∥x∥∥y∥ cos(x, y) and linearity of expectation to obtain (cid:20) ∥ ̄gB∥ ∥gB∥\n\n(⟨gDa , gB⟩ − ⟨gDa , MBgB⟩)\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(gB − MBgB)\n\ngDa, E\n\n= E\n\n(cid:21)(cid:29)\n\n(cid:28)\n\n(cid:21)\n\n= ∥gDa ∥E\n\n(cid:20) ∥ ̄gB∥ ∥gB∥\n\n(cid:0)∥gB∥ cos(gDa , gB)\n\n− ∥MBgB∥ cos(gDa , MBgB)(cid:1)\n\n(8)\n\n(cid:21)\n\nwhere θa the definition of MB such that ̄gB and MBgB are aligned and ∥gB∥ = ∥MBgB∥.\n\nB = ∠(gDa , MBgB) = ∠(gDa , ̄gB). The last equality follows from\n\nB = ∠(gDa , gB) and ̄θa\n\n= ∥gDa ∥E (cid:2)∥ ̄gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) ,\n\na\n\non bound the BH a l (MBgB) − gT l gB l is positive semi-definite such that xT H a\n\ndifference\n\n(cid:1)(cid:105)\n\nin\n\n.\n\nconjugates\n\nof\n\nthe Hessian,\n\nNote that since we assume the loss l is\n\nl x ≥ 0 for all vectors x. It follows\n\nalso\n\nget\n\nWe E\n\ncan (cid:104) ∥ ̄gB ∥2 ∥gB ∥2\n\n(cid:0)(MBgB)T H a convex, the Hessian H a that E[xT H a (cid:20) ∥ ̄gB∥2 ∥gB∥2\n\nE\n\nl x] ≥ 0 and so using linearity of expectation,\n\n(cid:0)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(cid:21)\n\n(cid:1)\n\n≤ E\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2 (MBgB)T H a l is symmetric and hence xT H a\n\nl (MBgB)\n\n(cid:21)\n\n.\n\nl x ≤ l . We then again use that ∥MBgB∥ = ∥gB∥ and\n\n(9)\n\nSince l is twice continuously differentiable we have that H a λa∥x∥2 where λa is the maximum eigenvalue of H a linearity of expectation to obtain\n\nE\n\n(cid:20) ∥ ̄gB∥2 ∥gB∥2\n\n(cid:0)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(cid:21)\n\n(cid:1)\n\n≤ λaE (cid:2)∥ ̄gB∥2(cid:3) .\n\n(10)\n\nSimilar analysis gives that E\n\n(cid:104) ∥ ̄gB ∥2 ∥gB ∥2\n\nCombining the above, it follows that\n\n(cid:0)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(cid:1)(cid:105)\n\n≥ −λaE[∥ ̄gB∥2].\n\nRdir\n\na − Rdir\n\nb ≥ ηt\n\n(cid:0)∥gDa ∥E (cid:2)∥ ̄gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) − ∥gDb ∥E (cid:2)∥ ̄gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3)(cid:1)\n\n−\n\nη2 t\n2\n\n(λa + λb)E[∥ ̄gB∥2],\n\nand since we assume ηt ≤\n\n1 maxk∈[K] λk\n\n,\n\nRdir\n\na − Rdir\n\nb ≥ ηt\n\n(cid:0)∥gDa ∥E (cid:2)∥ ̄gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) − ∥gDb ∥E (cid:2)∥ ̄gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3) − E[∥ ̄gB∥2](cid:1) .\n\nIt follows that Rdir\n\na > Rdir\n\nb when the following is satisfied:\n\nE (cid:2)∥ ̄gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) >\n\n∥gDb ∥ ∥gDa ∥\n\nE (cid:2)∥ ̄gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3) +\n\nE[∥ ̄gB∥2] ∥gDa ∥\n\n.\n\n14\n\n(11)\n\n(12)\n\n(13)\n\nPublished as a conference paper at ICLR 2023\n\nFinally, to see that the bound is tight we simply note that the inequalities that were introduced can all be saturated simultaneously. In Eq. 9 we require that gT l gB = 0, and in Eq. 10 we l (MBgB) = λa∥MBgB∥2 for each batch. These independent conditions can require (MBgB)T H a plausibly be met for some H a l , gB, and MB. The only other inequality introduced is the assumption ηt ≤ for the sake of achieving saturation.\n\n, which we can strengthen to ηt =\n\nBH a\n\n1 maxk∈[K] λk\n\n1 maxk∈[K] λk\n\nA.2 ALTERNATE DECOMPOSITIONS OF THE CLIPPING ERROR\n\nIn Sec. A.2 we proposed a decomposition of the clipped batch gradient into parts representing\n\nmagnitude and direction error, ̄gB=MB to demonstrate that direction error causes the most severe problems for the final performance of models, and analysed the contributions of the two effects to the excessive risk in Prop. 2.\n\n. We presented a simple experiment in Table 1\n\n∥gB ∥ gB\n\n(cid:16) ∥ ̄gB ∥\n\n(cid:17)\n\nHowever, the decomposition we used is not unique, and furthermore it is not possible to completely isolate the two effects in the excessive risk analysis. For example, if we think of magnitude error as the difference in loss between using update vector gB and ∥ ̄gB ∥ ∥gB ∥ gB (γ in Fig. 6), then it follows that the remaining error is due to gradient misalignment, in other words, the difference in loss between using update vector ∥ ̄gB ∥ ∥gB ∥ gB and ̄gB (λ in Fig. 6). In this example, the error due to gradient misalignment includes both error in direction and error in magnitude, while magnitude error is “pure”,\n\nFigure 6: Decomposition of steps between gB and ̄gB.\n\nRmag\n\na = E[L(θt − ηt a = E[L(θt − ηt ̄gB; Da) − L(θt − ηt\n\nRdir\n\n∥ ̄gB ∥\n\n∥gB ∥ gB; Da) − L(θt − ηtgB; Da)],\n\n∥ ̄gB ∥\n\n∥gB ∥ gB; Da)].\n\n(14)\n\n(15)\n\nA different way of decomposing the clipping error is considering the direction error as the difference in loss between using update vector gB and MBgB (α in Fig. 6). In this case, direction error is pure, i.e. does not include difference in magnitudes. It follows that the remaining error is magnitude error, so is the difference in loss between using update vector MBgB and ̄gB (β in Fig. 6). Thus, the magnitude error in this case quantifies the difference in loss of scaling the already misaligned ̄gB,\n\nRdir a\nRmag a\n\n∗\n\n= E[L(θt − ηtMBgB; Da) − L(θt − ηtgB; Da)], ∗ = E[L(θt − ηt ̄gB; Da) − L(θt − ηtMBgB; Da)].\n\n(16)\n\n(17)\n\nIn our analysis we used the first decomposition where magnitude error can be completely corrected by an adjustment of the learning rate, and direction error, what we hypothesized to be the largest cause of disparate impact, is the remaining part of the clipping error. For completeness, by using the second decomposition we can derive alternative versions of Props. 2 and 3:\n\nProposition 2*. Consider the ERM problem with twice-differentiable loss l with respect to the model parameters. The excessive risk due to clipping experienced by group a ∈ [K] at iteration t is approximated up to second order in ∥θt+1 − θt∥ as\n\nRclip\n\na ≈ ηt\n\n(cid:68)\n\ngDa , E\n\n(cid:104)(cid:16) ∥gB ∥\n\n∥ ̄gB ∥ −1\n\n(cid:17)\n\n(cid:105)(cid:69)\n\n ̄gB\n\n+ η2\n\nt 2\n\nE\n\n(cid:104)(cid:16)\n\n1 − ∥gB ∥2\n\n∥ ̄gB ∥2\n\n(cid:17)\n\nBH a ̄gT\n\nl ̄gB\n\n(cid:105)\n\n,\n\n+ E [ηt ⟨gDa, gD − MBgB]⟩ + η2\n\nt 2\n\nE (cid:2)(MBgB)T H a\n\nl (MBgB) − gT\n\nBH a\n\nl gB\n\n(Rmag\n\na\n\n∗)\n\n(Rdir a\n\n∗\n\n)\n\n(cid:3) ,\n\nwhere gDa , ̄gDa denote the average non-clipped and clipped gradients over group a at iteration t, H a l refers to the Hessian over group a, and MB is an orthogonal matrix such that ̄gB and MBgB are colinear. The expectations are taken over batches of data.\n\nProposition 3*. Assume the loss l is twice continuously differentiable and convex with respect to the model parameters. As well, assume that ηt ≤ (maxk∈[K] λk)−1 where λk is the maximum\n\n15\n\nPublished as a conference paper at ICLR 2023\n\neigenvalue of the Hessian H k\n\nl . For groups a, b ∈ [K], Rdir\n\na > Rdir\n\nb\n\nif\n\nE (cid:2)∥gB∥(cos θa\n\nB − cos ̄θa\n\nB)(cid:3) >\n\n∥gDb ∥ ∥gDa ∥\n\nE (cid:2)∥gB∥(cos θb\n\nB − cos ̄θb\n\nB)(cid:3) +\n\nE[∥gB∥] ∥gDa ∥\n\n(18)\n\nwhere θk\n\nB = ∠(gDk , gB) and ̄θk\n\nB = ∠(gDk , ̄gB) for a group k ∈ [K].\n\nWe omit the proofs since they are directly analogous to those in App. A.1.\n\nB EXPERIMENTAL DETAILS\n\nB.1 DATASET PREPROCESSING\n\nMNIST We use the artificially unbalanced MNIST training dataset where class 8 is sampled with probability 9% such that class 8 only constitutes about 1% of the dataset on average. This gives about 6000 data samples for each class, other than class 8 with about 500. The protected group values are the class labels. As in Xu et al. (2021), we compare models on how they treat the under-represented class 8 versus the well-represented class 2. The test set remains balanced, with approximately 1000 samples for each class. Data is scaled to be in the domain [0,1].\n\nAdult The original Adult dataset4 consists of 48,842 samples, reduced to 45,222 by removing all samples with missing values. The “final weight” feature is removed and the “race” attribute is discretized by {white, non-white}, giving 5 numerical, 3 binary and 6 categorical features. The numerical features are normalized and the categorical features are one-hot encoded. As is typical in the fairness literature, choices for the protected attribute are “sex”, “race” (binary) and possibly the discretized “age”. We use “sex” by default. The classification label is “income” (whether or not income exceeds $50,000). Prior to sampling, the Adult dataset is unbalanced with respect to sex with 30,527 males and 14,695 females. We sample a balanced dataset as in Xu et al. (2021) with 14,000 females and 14,000 males on average.\n\nDutch The Dutch dataset van der Laan (2000)5 is preprocessed by dropping underage samples (14 and under) and removing the “weight” feature. As well, all “unemployed” samples are removed, as well as those with missing or middle-level “occupation”, for a total of 60,420 samples. Specifically, “occupation” values 3,6,7,8 are considered middle-level. “Occupation” is then made binary by considering values 4,5,9 as low-level professions (0) and 1,2 as high-level professions (1). The binary classification task is to predict “occupation”, given the rest of the features. We consider “sex” as the protected group attribute. The processed dataset is balanced with respect to “sex” with 30,147 male and 30,273 female samples.\n\nWe use an 80/20 train/test split for both tabular datasets.\n\nCelebA The CelebA dataset (Liu et al., 2015)6, consists of 64x64 pixel RGB images of celebrity faces, along with binary attributes describing each image. Many of these attributes are subjective, but we chose to use the most objective ones for training and group labels. We used the binary attribute “Male” for the classification target, which is roughly balanced at 84,434 males in 202,599 total images. The attribute “Eyeglasses” was our protected group label; although wearing eyeglasses in public typically does not construe sensitive information, we used this attribute because it was objectively defined, and formed a minority group which was empirically more difficult for models to classify accurately. Of the male images, 10,478 have eyeglasses, while only 2,715 female images have them. The training/validation/test split is provided with the dataset and is roughly in a 80/10/10 ratio.\n\n4The Adult dataset is available at archive.ics.uci.edu/ml/datasets/Adult. 5The Dutch dataset is also available through the work of Le Quy et al. (2022) at\n\nraw.githubusercontent.com/tailequy/fairness dataset/main/Dutch census/dutch census 2001.arff.\n\n6We accessed this dataset via kaggle.com/datasets/jessicali9530/celeba-dataset.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nB.2 EXPERIMENT SETTINGS\n\nWe set σ = 1, C0 = 0.5 for Adult, σ = 1, C0 = 0.1 for Dutch, while for MNIST and CelebA, we set σ = 0.8 and C0 = 1. For DPSGD-F, the gradient noise is unchanged σ2 = σ, and σ1 = 10σ2. For FairLens, we use regularization weights as in Tran et al. (2021a), λ1 = λ2 = 1. For non-global methods, the learning rate is ηt = 0.01 for all iterations t and all datasets except Dutch which has ηt = 0.8. For DPSGD-Global we have ηt = 1, Z = 50 for Adult, ηt = 2, Z = 1 for Dutch, ηt = 0.2, Z = 100 for MNIST, and ηt = 0.1, Z = 100 for CelebA. For DPSGD-Global-Adapt we have σ2 = 10, Z = 50, ηZ = 0.1 for all datasets (the only exception is for CelebA Z = 100), ηt = 0.2, τ = 1 for Adult, η = 1, τ = 1 for Dutch, and η = 0.1, τ = 0.7 for MNIST and CelebA. All methods for all datasets use training and test batches of size 256.\n\nExperiments were conducted on single TITAN V GPU machines. Approximately four GPU-days were used to train all methods over five seeds for the four datasets.\n\nB.3\n\nIMPLEMENTATION DETAILS\n\na\n\nand Rnoise\n\nThe excessive risk terms for different groups (Rclip a in Prop. 2) all involve the Hessian of the loss function with respect to the model parameters. Calculating the Hessian as a matrix is computationally expensive, but more crucially requires memory that scales quadratically in the number of parameters. In the previous work studying Rclip , Tran et al. (2021a) use the PyHessian library to compute the Hessian as a matrix, and then used it to compute the products and traces needed for Rclip . Because this approach incurs a high memory burden, the models trained were limited to small MLPs with a single hidden layer of 20 hidden units.7\n\nin Prop. 1 and Rmag\n\nand Rnoise\n\nand Rnoise\n\nand Rdir\n\na\n\na\n\na\n\na\n\na\n\na\n\nIn our implementation, provided as supplemental material, we avoid computing the Hessian as a matrix altogether which allows us to scale our experiments to common image datasets. For the four datasets, our models have parameter counts of N = 91650 for Adult, N = 120 for Dutch, N = 80522 for MNIST, and N = 120722 for CelebA, which would produce Hessian matrices with up to 14.5 billion entries. Instead, we compute the terms involving Hessians like H a l gB through Hessian-vector products (HVPs) using the functorch8 library with PyTorch 1.11. Using HVPs requires memory comparable to that used when computing gradients for SGD.\n\nl ) = (cid:80)N\n\nFor the trace of the Hessian matrix, also called the Laplacian, one possible approach that does not require realizing the entire matrix in memory is to compute HVPs with unit vectors to isolate each diagonal element: Tr(H a l Ii where Ii is the ith column of the identity matrix. While exact, this approach requires N HVPs for each group a ∈ K, of which there are at least two. Since this method is much too expensive for even the simple MLPs and CNNs we used, we instead employed Hutchinson’s trace estimator (Hutchinson, 1990) to estimate Tr(H a l z]. This estimator is unbiased when z is drawn from a Rademacher distribution which we used, and only requires n HVPs per group, where n can be chosen as large as required for convergence of the estimate. In practice we used n = 100.\n\nl ) = Ez[zT H a\n\ni=1 I T\n\ni H a\n\nAdditionally, whereas Tran et al. (2021a) replace dataset gradients gD and gDa with batch gradients when computing Rclip in Prop. 1, we use the exact gD and gDa . This eliminates an easily preventable source of noise in our results.\n\nand Rnoise\n\na\n\na\n\nTo further reduce computation time, we only evaluate excessive risk terms (Hessians) every 50, 100, 200, or 200 iterations for the Adult, Dutch, MNIST, and CelebA datasets respectively.\n\nB.4 DIRECTION ERROR IS MORE SEVERE THAN MAGNITUDE ERROR\n\nAs noted earlier, Prop. 2 only evaluates excessive risk for a single iteration, not necessarily capturing how each of Rdir contribute to convergence and disparate impact over the course of training. In order to evaluate the full impact of magnitude error and error due to gradient misalignment, we consider the difference in final loss and accuracy between models which have zero magnitude\n\na and Rmag\n\na\n\n7See implementation available at openreview.net/forum?id=7EFdodSWee4. 8See documentation at pytorch.org/functorch/stable/.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nerror and zero direction error in Table 1. In these experiments, we consider zero magnitude error to be when ∥ ̄gB∥ = ∥gB∥ for all batches, and zero direction error to be when gB and ̄gB are aligned for all batches. Note that these definitions correspond to comparing update vectors gB and ∥gB ∥ ∥ ̄gB ∥ ̄gB for the zero magnitude error experiment, and comparing update vectors gB and ∥ ̄gB ∥ ∥gB ∥ gB for the zero direction error experiment. These do not correspond to the definitions of Rdir in Prop. 2, but capture the intuitive definitions of direction and magnitude error. As described in App. A.2, while Rclip a = Rmag a , direction error and magnitude error cannot be purely separated with any definition of Rmag\n\na and Rmag\n\na + Rdir\n\na\n\na , Rdir a .\n\nB.5 BASELINE METHODS\n\nWe compared our approach DPSGD-Global-Adapt with its predecessor DPSGD-Global, which was designed to improve convergence, not fairness, as well as two approaches specifically designed to improve fairness.\n\nDPSGD-Global (Bu et al., 2021) is presented in Alg. 2, and involves scaling almost all per-sample gradients by a global factor rather than only scaling large gradients with ∥gi∥ > C0 by a normdependent factor. We say “almost all”, because scaling alone does not provide a strict upper bound on the sensitivity, as required for an application of the Gaussian mechanism, see Fig. 2b. The method additionally clips gradients to zero if their norm is above a strict upper bound Z, which we found to be unnecessarily aggressive. Otherwise, the global scaling factor is C0/Z, which ensures that the sensitivity, namely C0, is finite. The advantage of DPSGD-Global is that it can better preserve the direction of ̄gB, especially when no gradients are clipped to zero. Hence, Bu et al. (2021) advocate for setting Z larger than ∥gi∥ for any sample in the batch. The drawback of a large Z is that all gradients are scaled down by a larger factor, so the convergence will be slowed unless the learning rate is increased to compensate. Setting Z is itself a challenge because we cannot inspect the batch to determine maxi ∥gi∥ without accounting for that expense in our privacy budget. In Sec. 5 we described how DPSGD-Global-Adapt resolves these concerns, first by clipping less aggressively, to C0 instead of 0, while maintaining the same sensitivity, and second by adaptively setting Z each round according to a private estimate of how many gradients in a batch exceeded τ · Z (using the tolerance threshold τ ).\n\nXu et al. (2021) designed DPSGD-F as a method for removing disparate impact caused by DPSGD by adaptively setting the clipping threshold for different protected groups. The method was based on the observation that negatively impacted groups tended to have large gradient norms which were affected more by clipping. Hence, the clipping threshold is raised for groups with larger gradient norms, based on a private estimate of how many gradients per-group have ∥gi∥ > C0. Given large enough batch sizes, the private estimate can be done with much more noise as compared to the gradient update, so it does not meaningfully increase the privacy budget.\n\nOne drawback of this approach is that it requires group label information for every datapoint in the training set. In practice, especially in highly regulated industries, such information may not be permissible to use or even collect. Collecting additional private information from data subjects on protected attributes can itself be a negative process and creates unnecessary privacy risks. One major advantage of DPSGD-Global-Adapt is that it reduces unfairness without ever using group label information.\n\nWhile each group is clipped using its own threshold, noise is added to the batched gradient based on the sensitivity, determined by the largest group threshold. While all groups receive the same theoretical privacy guarantee in terms of (ε, δ), groups that are clipped to smaller thresholds may enjoy stronger empirical privacy guarantees, as determined for example by adversarial attacks (Jagielski et al., 2020; Nasr et al., 2021). Hence, it appears likely that DPSGD-F can produce unfairness in the amount of privacy afforded to different groups.\n\nDPSGD-F is shown in Alg. 3. Note that we present the algorithm as implemented in the author’s codebase, not as written in their paper. In our experiments we use the version shown in Alg. 3.\n\nOur final baseline, referred to as “FairLens” was developed in (Tran et al., 2021a) to reduce excessive risk from clipping, Rclip . Regularization terms are added to the loss function in DPSGD that specifically target these sources of excessive risk. The source of Rnoise was identified\n\na , and adding noise, Rnoise\n\na\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 3 DPSGD-F\n\nRequire: Iterations T , Dataset D, sampling rate q, clipping bound C0, noise multipliers σ1, σ2,\n\nlearning rates ηt\n\nInitialize θ0 randomly for t in 0, . . . , T − 1 do\n\nB ← Poisson sample of D with rate q for (xi, ai, yi) in B do\n\n▷ Compute per-sample gradients\n\n▷ Count samples per-group above/below clipping bound\n\n▷ Privatize unit sensitivity count vectors\n\nk∈[K]\n\n▷ Postprocessing\n\ngi ← ∇θl(fθt(xi), yi)\n\n(cid:9)(cid:12) (cid:12) (cid:9)(cid:12) (cid:12)\n\ni ∥ > C0 i ∥ ≤ C0\n\nfor k in [K] do mk ← (cid:12) (cid:8)i : ∥gk (cid:12) ok ← (cid:12) (cid:8)i : ∥gk (cid:12) k∈[K] ← (cid:8)mk, ok(cid:9) (cid:8) ̃mk, ̃ok(cid:9) k∈[K] ← (cid:8)max(⌊ ̃mk⌋, 0), max(⌊ ̃ok⌋, 0)(cid:9) (cid:8) ̃mk, ̃ok(cid:9) ̃m = (cid:80) k∈[K] ̃mk for k in [K] do\n\nk∈[K] + N (0, σ2\n\nI)\n\n1\n\n ̃bk = ̃mk + ̃ok Ck = C0 ·\n\n(cid:16)\n\n1 + ̃mk/ ̃bk\n\n ̃m/|B|\n\n(cid:17)\n\nfor (xi, ai, yi) in B do ̄gi ← gi · min\n\n(cid:16)\n\n(cid:17)\n\n(cid:0)(cid:80)\n\n ̃gB ← 1 |B| θt+1 ← θt − ηt ̃gB\n\n1, Ck ∥gi∥ i∈B ̄gi + N (0, σ2\n\n2C 2\n\n0\n\nI)(cid:1)\n\nwhere k = ai\n\n▷ Clip according to per-group clipping bounds\n\nto involve the per-group Laplacian of the loss l with respect to model parameters - a second order derivative whose computation scales poorly with model size. To avoid this difficulty, the authors used a stand-in for the Laplacian based on the distance of a point to the decision boundary.\n\nOur implementation is directly based off of code made available by the authors on OpenReview at openreview.net/forum?id=7EFdodSWee4. The version implemented in their code is shown in Alg. 4, and assumes there are only two mutually exclusive protected groups, denoted a and b. Hence, it is not applicable to the MNIST dataset. We also attempted to use this code for our CelebA experiments but found that the implementation did not scale to the simple CNNs we used. Therefore, we omitted FairLens from the CelebA experiments.\n\nB.6 VERIFYING ASSUMPTIONS ON LOWER BOUND\n\nb\n\na −Rdir\n\nIn Sec. 6.3 and Fig. 5 we compared the usefulness of our lower bound on Rdir from Prop. 3, to a previous lower bound in the literature. For our lower bound to be valid, the assumptions of Prop. 3 should be satisfied. The first assumption, that the loss is twice continuously differentiable with respect to the model parameters, holds since the model architecture is an MLP with tanh activations. However, the loss is not in general convex. The third assumption, that the inverse of the learning rate upper bounds the largest eigenvalue of any group’s Hessian, is checked empirically for each iteration in Fig. 7.\n\nFigure 7: The maximum eigenvalue of any group’s Hessian remains below ηt\n\n−1.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 4 FairLens\n\nRequire: Iterations T , Dataset D, sampling rate q, clipping bound C0, noise multiplier σ, learning\n\nrates ηt, regularization weights γ1, γ2\n\nInitialize θ0 randomly for t in 0, . . . , T − 1 do\n\nB ← Poisson sample of D with rate q for (xi, ai, yi) in B do\n\ngi ← ∇θl(fθt(xi), yi) 1, C0 ̄gi ← gi · min ∥gi∥\n\n(cid:16)\n\n(cid:17)\n\n(cid:80)\n\n|B|\n\n(cid:80)\n\ngB ← 1 i∈B gi ̄gB ← 1 i∈B ̄gi |B| for k in {a, b} do (cid:80) gBk ← 1 fk ← 1\n\n|Bk|\n\n(cid:80)\n\n|Bk|\n\ni∈B,ai=k gi i∈B,ai=k fθt(xi)\n\nR1 = |⟨gBa − gBb , ̄gB − gB⟩| R2 = 1 L = l(fθt(xi), yi) + γ1R1 + γ2R2 for (xi, ai, yi) in B do\n\n2 (fa · (1 − fa) + fb · (1 − fb))\n\n(cid:16)\n\ng′ i ← ∇θL(fθt(xi), yi) i ← g′ ̄g′ B ← 1 ̃g′ θt+1 ← θt − ηt ̃g′\n\ni · min (cid:0)(cid:80)\n\ni∈B ̄g′\n\n1, C0 ∥g′ i∥ i + N (0, σ2C 2\n\n|B|\n\n(cid:17)\n\n0\n\nB\n\nI)(cid:1)\n\n▷ Compute per-sample gradients of original loss\n\n▷ Define regularized loss\n\n▷ Compute per-sample gradients of regularized loss\n\n▷ Clip to ensure finite sensitivity\n\nB.7 ADDITIONAL RESULTS\n\nIn this section we complete the set of experimental results shown in Sec. 6 over all datasets and methods. All results are averaged over five random seeds with one standard error shown.\n\nTable 4: Performance and Fairness metrics for Adult dataset\n\nMETHOD\n\nACC M ACC F\n\nπM\n\nπF\n\nπM,F LOSS M LOSS F\n\nRM\n\nRF\n\nRM,F\n\nNON PRIVATE 80.5±0.4 92.2±0.1 -\n3.6±0.1 6.9±0.3 0.78±0.01 0.40±0.01 0.39±0.00 0.21±0.01 0.17±0.01 DPSGD 3.7±0.1 7.9±0.2 0.57±0.00 0.42±0.00 0.18±0.00 0.23±0.00 0.05±0.00 FAIRLENS 2.7±0.1 0.2±0.3 0.49±0.00 0.31±0.01 0.09±0.00 0.12±0.00 0.02±0.01 DPSGD-F 2.2±0.1 0.2±0.2 0.43±0.00 0.25±0.00 0.04±0.00 0.05±0.00 0.02±0.00 DPSGD-G. DPSGD-G.-A. 80.7±0.4 92.3±0.1 −0.1±0.1 −0.1±0.1 0.0±0.1 0.39±0.00 0.18±0.00 0.00±0.00 0.00±0.00 0.00±0.00\n\n- 69.9±0.4 88.5±0.1 10.6±0.3 68.8±0.4 88.5±0.1 11.7±0.2 2.5±0.3 78.0±0.6 89.4±0.1 2.0±0.2 78.5±0.5 89.9±0.1\n\n- 0.40±0.00 0.19±0.00\n\n-\n\n-\n\n-\n\nTable 5: Performance and Fairness metrics for Dutch dataset\n\nMETHOD\n\nACC M ACC F\n\nπM\n\nπF\n\nπM,F\n\nLOSS M\n\nLOSS F\n\nRM\n\nRF\n\nRM,F\n\nNON PRIVATE 79.9±0.2 86.9±0.0 -\n- 76.0±0.2 86.4±0.1 3.8±0.3 0.4±0.0 3.4±0.4 0.520±0.001 0.450±0.001 0.021±0.001 0.003±0.001 0.018±0.002 DPSGD 78.6±0.3 86.9±0.1 1.3±0.2 −0.1±0.0 1.4±0.2 0.552±0.001 0.526±0.000 0.053±0.001 0.079±0.000 0.026±0.001 FAIRLENS 0.3±0.0 0.7±0.1 0.503±0.001 0.447±0.001 0.005±0.001 0.000±0.000 0.005±0.001 78.9±0.2 86.6±0.1 0.9±0.1 DPSGD-F 0.4±0.0 0.4±0.2 0.510±0.001 0.460±0.001 0.012±0.001 0.013±0.001 0.002±0.001 79.0±0.2 86.5±0.1 0.8±0.1 DPSGD-G. 0.2±0.0 0.2±0.2 0.504±0.001 0.452±0.001 0.006±0.001 0.005±0.001 0.001±0.001 DPSGD-G.-A. 79.4±0.1 86.7±0.1 0.4±0.2\n\n- 0.499±0.000 0.447±0.000\n\n-\n\n-\n\n-\n\nFirst we look at the final performance and fairness metrics on the test set for Adult in Table 4 and Dutch in Table 5 (cf. MNIST in Table 2 and CelebA in Table 3). We see that FairLens is inconsistent in reducing the privacy cost gap and excessive risk gap compared to DPSGD. DPSGDF improves both fairness metrics while achieving better performance. DPSGD-Global improves over or is comparable to DPSGD-F in all metrics, and does so without requiring access to protected group membership information. Our method DPSGD-Global-Adapt further improves both performance and fairness by clipping less aggressively and adaptively setting the upper clipping threshold Z.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: Adult dataset. Top: Train loss per epoch. Bottom: ∥gB∥ averaged over batches per epoch.\n\nFigure 9: MNIST dataset. Top: Train loss per epoch. Bottom: ∥gB∥ averaged over batches per epoch.\n\nFigure 10: CelebA dataset. Top: Train loss per epoch. Bottom: ∥gB∥ averaged over batches per epoch.\n\nTo go along with the training curves shown for Dutch in Fig. 3, we present the same for Adult in Fig. 8, MNIST in Fig. 9, and CelebA in Fig. 10. The trends are consistent across datasets - whereas DPSGD produces large values and a large gap for the gradient norms and losses between protected groups, our method DPSGD-Global-Adapt reduces the values and gap at all stages of training.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nFigure 11: Dutch dataset. Top: Excessive risk due to gradient misalignment per group. Bottom: Excessive risk due to magnitude error per group.\n\nFigure 12: MNIST dataset. Top: Excessive risk due to gradient misalignment per group. Bottom: Excessive risk due to magnitude error per group.\n\nFigure 13: CelebA dataset. Top: Excessive risk due to gradient misalignment per group. Bottom: Excessive risk due to magnitude error per group.\n\nWe also present the values of terms Rdir over training for Dutch in Fig. 11, for MNIST in Fig. 12, and CelebA in Fig. 13 as was done for Adult in Fig. 4. Both Global methods dramatically reduce Rdir a . Comparing to the final training results where global methods also show the best performance, this provides further evidence for our hypothesis that gradient misalignment is the most significant cause of disparate impact in DPSGD.\n\na compared to DPSGD at the cost of larger Rmag\n\na and Rmag\n\na\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nFigure 14: Excessive risk due to noise error per group for the Adult dataset\n\nFigure 15: Excessive risk due to noise error per group for the Dutch dataset\n\nFigure 16: Excessive risk due to noise error per group for the MNIST dataset\n\nFigure 17: Excessive risk due to noise error per group for the CelebA dataset\n\nWe note that by tuning the learning rate in DPSGD-Global and DPSGD-Global-Adapt, there is a 0 σ2, trade-off between magnitude error and noise error. Referring to Prop. 1, Rnoise we see that the excessive risk due to noise is affected by the learning rate ηt, the noise multiplier σ, clipping bound C0 and the trace of the Hessian for group a. In choosing a larger learning rate for the global methods to offset the magnitude error, we increase the noise error quadratically. Refer to the values of Rnoise over training for Adult in Fig. 14, Dutch in Fig. 15, MNIST in Fig. 16, and CelebA in Fig. 17. While the excessive risk due to noise is significantly larger for the global methods, these methods outperform all other private methods at the end of training, see Tables 2, 3, 4, 5. Gaussian noise adds zero bias and the errors it introduces tend to cancel out over the course of training. These observations further validate that direction error is the core cause of disparate impact, and minimizing gradient misalignment should be prioritized over other sources of unfairness.\n\na = η2\n\n2 Tr(H a\n\nl )C 2\n\nk\n\nt\n\n23",
    "reference": "# Summary Of The Paper\n\nThis paper conducts a fine-grained analysis of DP-SGD to show that gradient misalignment is  a principal cause of the disparate impact that occurs when DP-SGD is applied to a dataset. This analysis compares two key components of the DP-SGD: clipping and noise addition. Ultimately, this paper shows that clipping further impacts and exacerbates gradient misalignment, which greatly increases the disparate impact of DP-SGD. This insight is nicely illustrated in a simple experiment that keeps the noise level in DP-SGD fixed, but changes the level of gradient alignment to show that when the gradient is more misaligned, it leads to more disparate results. The fix proposed by this paper is to  scale down all per-sample gradients in a batch by the same amount. The improved results are demonstrated experimentally on the MNIST and adult datasets.\n\n# Strength And Weaknesses\n\n### Strengths\n\n- This paper refines the cause of disparate impact in SGD, which is an important problem. \n\n- The experiment and analysis presented in Section 4 clearly show the main take home of this paper.\n\n- The proposed modification to DP-SGD is quite simple and straightforward. The proposed optimizer can be easily plugged-in to other settings.\n\n### Weaknesses\n- One of the benefits of the proposed DP-SGD-Adapt global is that it does not need group identifiers. However, it is unclear whether the methods that use group identifiers lead to models with better utility. While the comparison is not 1:1 here, it would still be good to know how these results compare to when one actually takes into account group identifiers.\n\n- The datasets considered the paper are small and somewhat toyish even though these are the datasets that were used in previous papers. It is unclear how this proposal scales to larger models and bigger setups. However, I think future work can likely address these issues.\n\n- The theorems and analysis presented applies or explicitly assume convexity or in some cases that the loss is twice differentiable. It is nice that the analysis generalizes to model classes that do not satisfy these assumptions but this issue could be better highlighted.\n\n- This work highlights gradient misalignment as a key cause of disparate impact, but it is hard to judge how much that is more important than say group difficulty as measured by the trace of the hessian of the loss per group. It could be that more difficult to learn groups have hessian-trace values that are substantially different than other groups. \n\n-  (Minor) The term 'excessive risk' is used quite a bit in a number of places in the paper. I believe it should be 'excess risk'.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing in this paper is quite clear and easy to follow. I have a minor suggestion above, but overall, each section is written and presented clearly. This work builds on insights from previous analyses of DP-SGD; however, it brings new information to light. Overall, the results show that gradient alignment is a key cause of disparate impact.\n\n# Summary Of The Review\n\nThis paper provides new insights on an important problem. The paper also demonstrates empirically improvements based on a modified DP-SGD algorithm that reduces disparate impact.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nN/A"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nIMPROVING LANGUAGE MODEL PRETRAINING WITH TEXT STRUCTURE INFORMATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nInter-sentence pretraining tasks learn from sentence relationships and facilitate high-level language understanding that cannot be directly learned in word-level pretraining tasks. However, we have found experimentally that existing intersentence methods for general-purpose language pretraining improve performance only at a relatively small scale but not at larger scales. For an alternative, we propose Text Structure Prediction (TSP), a more sophisticated inter-sentence task that uses text structure to provide more abundant self-supervised learning signals to pretraining models at larger scales. TSP classifies sentence pairs over six designed text structure relationships and it can be seen as an implicit form of learning high-level language understanding by identifying key concepts and relationships in texts. Experiments show that TSP provides improved performance on language understanding tasks for models at various scales. Our approach thus serves as an initial attempt to demonstrate that the exploitation of text structure can facilitate language understanding.\n\n1\n\nINTRODUCTION\n\nGeneral-purpose pretrained language models have been widely applied in natural language processing (NLP). The most representative model of these models is BERT (Devlin et al., 2019), which is pretrained simultaneously on two pretraining tasks: a masked language model (MLM) task, and a next sentence prediction (NSP) task. While MLM masks words and requires models to fill clozes, NSP is an inter-sentence task of predicting whether two texts are continuous. Inter-sentence tasks learn relationships between sentences and facilitate high-level language understanding that is not directly learned by word-level pretraining tasks (Devlin et al., 2019). However, the representative inter-sentence task, NSP, has been found to fail to improve performance (Liu et al., 2019; Yang et al., 2019). Although a few successors of NSP have been proposed, they are still not widely adopted and researched. In this paper, we show that the existing inter-sentence methods for general-purpose language pretraining are actually suboptimal; to improve on those methods’ weaknesses, we then propose an alternative that redefines what is learned from sentence relations.\n\nThe existing general-purpose inter-sentence pretraining tasks include NSP (Devlin et al., 2019), which discriminates whether two texts come from different documents; sentence order prediction (SOP) (Lan et al., 2020), which discriminates whether two texts are swapped; and sentence structure objective (SSO) (Wang et al., 2020), which discriminates whether two texts come from different documents or are swapped. To investigate claims of improved performance, we experimented these methods at three different scales (Small, Base, and Large), which mainly followed the setting of BERT (Devlin et al., 2019). The model size and the amount of consumed data increased from Small to Base and then to Large scale (see Appendix A for the details). As seen in Figure 1, our experimental results show that the existing methods improved performance only at the Small-scale whereas they undermined or failed to improve performance for models at larger scales.\n\nIn investigating the reason for little improvement by the existing methods at larger scales, we noticed that all of the aforementioned methods split an input text into two segments and learn from only the relationship between the two segments, thus ignoring the text’s underlying structure. As illustrated in Figure 2, text structure can be seen as the organization of information in texts. Without text structure, a text becomes a long continuous word sequence, which is hard to read and makes it difficult to identify key concepts and logical relationships for humans. This intuitive understand-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Experimental results on the language understanding benchmark SuperGLUE (Wang et al., 2019) at the Small, Base, and Large scales. The experiment compared the effect on the performance of using different inter-sentence tasks (NSP, SOP, SSO, and our proposed task, TSP) when learned concurrently with the word-based MLM pretraining task. By taking advantage of text structure information, TSP outperforms pure MLM at different scales, whereas the other inter-sentence baselines (NSP, SOP, and SSO) failed to improve the performance at larger scales in our experiments.\n\nFigure 2: Example of text structure in real text. The document comprises multiple paragraphs, where each paragraph is composed of multiple sentences. Each paragraph conveys a specific concept constructed by the sentences that the paragraph is composed of, while the order of concepts comes with a clear intention and forms a logical flow. This suggests that the combination of hierarchy and order in text structure hides valuable clues to high-level language comprehension.\n\ning of human reading ability inspires us to explore the possibility that text structure can provide models’ language understanding ability abundant learning signals of high-level semantics and their interaction, especially for models at larger scales. Hence, our goal in this paper is to demonstrate that learning from text structure can improve general-purpose language model pretraining. We thus propose a new inter-sentence task that better exploits text structure to examine our hypothesis.\n\nThe proposed task, Text Structure Prediction (TSP), is our initial attempt to integrate text structure information into general-purpose language pretraining. Regarding the task design, we view text structure in terms of two axes: ordering and hierarchy. Hierarchies are nested groupings of sentences, such as paragraphs or sections, where each group conveys a specific concept and its su-\n\n2\n\nSuperGLUE5861646770737679SmallBaseLargeMLMMLM + NSPMLM + SOPMLM + SSOMML + TSPThe game resolves around Qubits, the basic building block of a quantumcomputer. It's pretty straightforward (you won't need to learn any quantumentanglement math or physics) with the goal of increasing the number ofQubits while keeping them cool. The more Qubits you have, the more difficultit gets. Eventually, you'll \"discover new upgrades, complete big researchprojects and hopefully become a little more curious about how we're buildingquantum computers,\" wrote Google Quantum head of education Abe Asfaw.The goal is to draw attention to quantum computing, because it seems there'sa dearth of people working in the field. To that end, Google is bringing thegame to the classroom, hoping to encourage educators to talk about thesubject and expand access to quantum computing research. \"We need more students pursuing careers building or using quantumcomputers, and understanding what it would be like to be a quantum scientistor engineer,\" wrote Asfaw. \"For me, that’s what World Quantum Day is allabout: showing everyone what quantum computing really is and how they canget involved.\"Paragraph I (3 sentences): Brief introduction of The Qubit Game Paragraph II (4 sentences) The details of the game. Paragraph III (2 sentences) Google's intention behind the game.Paragraph IV (2 sentences)The meaning of World Quantum Day.World Quantum Day was apparently yesterday, and Google feted the occasionwith the launch of The Qubit Game, as spotted by 9to5Google. Created inpartnership with Doublespeak games, it's a “playful journey to building aquantum computer, one qubit at a time,\" Google said. It also hopes the game,and World Quantum Day, will help generate some interest in the field.Further describe ConcludeExplain the reason Under review as a conference paper at ICLR 2023\n\nperordinate group conveys a more global concept. The order of sentences represents the flow of context and connections between information such as chronological process or cause-effect relationship (Danes, 2015). Thus, the solution of the TSP task requires two abilities: understanding of the semantics of sentences to group them into nested concepts (reflected by hierarchical relationships), and identification of connective relations between them (reflected by ordering relationships). Specifically, we propose six inter-sentence relations for learning from text structure information, which combine three hierarchical levels and two ordering directions. As a result, the TSP task requires a model to classify sentence pairs over the six defined text structure relationships. More details of TSP are given in Section 3. Through experiments we have shown the following: (1) Our proposed method continues to improve performance on a language understanding benchmark at the Small, Base, and Large scales, whereas the inter-sentence baselines fail to do so. (2) Our proposed method is comparable to or better than the baselines on almost all language understanding tasks in the benchmarks. Accordingly, the results have shown the effectiveness and potential of exploiting text structure information for general-purpose language pretraining.\n\nIn summary, the contributions of this paper are as follows\n\n1. We find that existing inter-sentence tasks for general-purpose language pretraining did not bring significant improvement in our settings. In our experiments, these approaches perform well at a relatively small scale, but fail to improve or even degrade performance at larger scales.\n\n2. We propose the use of text structure information to provide more valuable learning signals for general-purpose language pretraining. For our initial attempt at this, we designed the Text Structure Prediction (TSP) task and show that TSP improves performance at all scales in our experiments. We thus demonstrate that the exploitation of text structure information is a promising research direction for facilitating general-purpose language understanding.\n\n2 RELATED WORK\n\n2.1 WORD-LEVEL PRETRAINING\n\nSelf-supervised word representation learning has rapidly developed in the past several years. GPT (Radford et al., 2018) predicts the next token with the information from the previous tokens. ELMo (Peters et al., 2018) uses a bidirectional framework to produce contextualized word representations by fusing the final hidden states of two LSTM networks processing information in different directions. BERT (Devlin et al., 2019) achieves deep bidirectional pretraining via the MLM task, which masks certain words in the input and then predicts the masked words. Since BERT’s success, transfer learning has become mainstream in NLP. Many research initialize their models with pretrained models to reuse learned representations for their target tasks. Also, the development of word-level pretraining tasks has become prosperous. For example, ELECTRA (Clark et al., 2020) detects replaced tokens that come from incorrect predictions by another MLM model, while PMI-Masking (Levine et al., 2021) samples and masks collocations instead of words. In addition to encoder-based BERT and decoder-based GPT, word-level pretraining methods that are based on encoder-decoder such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) are also developed, which gain great success on text generation tasks. Our proposed approach is learned simultaneously with a word-level pretraining task, and it can be easily combined with any word-level pretraining task that is based on transformer encoder, with negligible additional computational and memory costs.\n\n2.2 SENTENCE-LEVEL PRETRAINING\n\nApproaches for learning sentence representations mainly fall into three categories. The first type is sentence bottleneck, including Skip-Thought (Kiros et al., 2015) and CMLM (Yang et al., 2021), aims to generate single or multiple sentence representations that are useful for word prediction objectives. The sentence bottleneck methods compress the encoded sentence into bottleneck representations and thus benefiting from information bottleneck (Tishby et al., 1999). The second type is contrastive learning, including COCO-LM (Meng et al., 2021) and CLEAR (Wu et al., 2020), which makes representations of positive sentence pairs closer and vice versa. Contrastive learning methods improve uniformity and ease the anisotropy problem that limits the expressiveness of embeddings (Gao et al., 2021). The third type is inter-sentence learning, including NSP and SOP\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ndescribed in Section 1, which learns sentence representations by capturing relationships between sentences. Since inter-sentence methods learn from interactions of sentences, they inherently also encode relationships between sentences and facilitate contextualization of multiple sentences. There is currently no research comparing the three kinds of methods with each other, and each type benefits from different principles. However, the inter-sentence approach is the only one that explicitly encodes and contextualizes multiple sentences. Our proposed approach belongs to the inter-sentence category.\n\n2.3\n\nINTER-SENTENCE TASKS\n\nIn terms of the kinds of sentence relationships to be learned, most inter-sentence tasks are sentence ordering tasks that rearrange a set of sentences into their original order (Cui et al., 2020; Kumar et al., 2020), whereas our proposed sentence relationships (Section 3.2) include ordering relationships but are not restricted to them. Regarding the creation of sentence representations, instead of encoding each sentence individually (Prabhumoye et al., 2020), our proposed framework (Section 3.1) adopts an approach of concatenating sentences into an input sequence, whereby information flows across sentences; thus, the created sentence representations are contextualized. In terms of semantic units, approaches such as NSP and SOP capture relationships between segments that are two halves of a text sequence that may include many sentences. In contrast, our approach processes sentences as in many other approaches. Finally, for task objectives, some approaches aim to identify coherent text (Barzilay & Lapata, 2008; Mesgar & Strube, 2018) or learning discourse representations (Iter et al., 2020), whereas our baselines (NSP, SOP, SSO) and our proposed method are for creating general-purpose language models.\n\n3 PROPOSED METHOD\n\nWe explain our proposed method in two parts. Section 3.1 introduces the overall proposed framework for creating sentence embeddings and then performing six-class classification over sentence pairs using created sentence embeddings. Figure 3 shows an overview of this framework. Section 3.2 further elaborates on the six inter-sentence relationships that we designed for classification and learning over sentence pairs. Overall, the implementation of TSP comes with several advantages: (1) It is a self-supervised task that does not require external resources or human labeling. (2) It can easily be combined with any pretraining task that outputs contextual word representations. An example is provided in Appendix F. (3) It can be learned concurrently with other tasks in one forward pass. (4) It only requires constructing an output head on top of a text encoder, with negligible extra parameters and computations.\n\n3.1 PROPOSED FRAMEWORK\n\n3.1.1\n\nINPUT SEQUENCE\n\nGiven a text sequence, we mark the scope of each sentence in the sequence by using the NLTK toolkit (Bird et al., 2009), which is based on Punkt (Kiss & Strunk, 2006). The result is an sequence that can be described as x =< x1,1, x1,2, ..., x1,n1 , ..., xk,1, xk,2, ..., xk,nk >, where xi,t is the t-th token in sentence i, ni is the length of sentence i, and k is the number of sentences in x. Next, the sequence’s sentences are shuffled to enable the model to learn to identify their correct order. We have found that shuffling all of a sequence’s sentences introduces noise into the model; thus, we shuffle only a limited percentage of the sentences (15% in this work, as explained in Appendix C), while keeping the other sentences at their original positions. Additionally, to learn simultaneously from the MLM task, we follow the same procedure used in BERT to corrupt the sequence: 15% tokens are selected; then, 80% of them are replaced with a [MASK] token, 10% are replaced with random tokens, and 10% are unchanged. Finally, we add sentinel tokens [CLS] and [SEP] to the sequence as in BERT, and a masked and partially shuffled input sequence is thus obtained for the model.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Overview of the proposed Text Structure Prediction (TSP) task. (left) Before an input sequence is sent to the encoder, certain randomly selected sentences in the sequence are shuffled. After encoding, a sentence embedding is obtained by averaging the encoded states of the tokens belonging to that sentence. (right) The TSP loss is calculated for performing six-class classification over sentence pairs by their sentence embeddings, where the six classes are shown in Figure 4.\n\n3.1.2 SENTENCE EMBEDDING\n\nTo obtain sentence representations, we first calculate contextualized token hidden states:\n\nh = fθ(x′),\n\n(1)\n\nwhere fθ is the transformer encoder function and x′ is a corrupted and partially shuffled input token sequence. By averaging the hidden state vectors of tokens that belong to the sentence, we obtain its sentence embedding:\n\nsi =\n\nΣn\n\nt=1hi,t ni\n\n,\n\n(2)\n\nwhere hi,t is the hidden state vector of the t-th token in the i-th sentence and ni is the length of the i-th sentence. Additionally, through the transformer encoder’s attention mechanism, each token interacts with all tokens in the input sequence rather than only the tokens in the sentence it belongs to. As a result, the sentence embeddings are contextualized and aware of other sentences in the same sequence. This means that when we compare sentences for their structure relationships, not only the sentences themselves but also their context are considered. Since identifying structure relationships using only information from the compared sentences can be very hard, it is intuitive that models are motivated to encode high-level interactions between sentences in the context into sentence representations.\n\n3.1.3 PAIRWISE CLASSIFICATION\n\nNext, we perform six-class classification over sentence pairs to learn the text structure relationship between sentences. Given a pair of sentences, we pass their sentence embeddings to a classifier to obtain prediction logits zi,j = W2(GELU (W1(si ⊕ sj))). ⊕ denotes vector concatenation, sk is the embedding of the k-th sentence, and GELU is the activation function proposed by Hendrycks & Gimpel (2016). Finally, the loss in the Text Structure Prediction (TSP) task is the cross-entropy loss:\n\nLtsp[i, j] = −log\n\nexp(zi,j,yi,j ) c=1exp(zi,j,c)\n\nΣ6\n\n,\n\n(3)\n\nwhere yi,j is the correct label for the relationship between the i-th and j-th sentences, and zi,j,c is the c-th element of the logit vector for the sentence pair. The final TSP loss Ltsp is the average over the TSP losses for all sentence pairs in a mini-batch, where the sentences in each sentence pair are in the same sequence. Overall, a model is optimized by minimizing the sum of MLM loss Lmlm and our TSP loss Ltsp:\n\nL = Ltsp + Lmlm.\n\n(4)\n\n5\n\nTransformer EncoderSentence 1Sentence 3Sentence 2Input SequenceSentence 1Sentence 2Sentence 3Average S3Partially Shuffled Input Sequence Token Hidden States Sentence Embeddings S1S2S36-Class ClassificationTSP LossAverage S2Average S1Under review as a conference paper at ICLR 2023\n\nFigure 4: Inter-sentence relationship sets for (left) our baselines and (right) TSP. An inter-sentence task classifies pairs of texts into one of the relations in its own defined sets. For example, if two sentences are in the same paragraph (but not adjacent) and in reverse order, the TSP task should classify the pair as Same Paragraph & Reverse Order. Note that we leave Different Document for future work because of its lack of difficulty (Lan et al., 2020) and complexity of implementation. By comparing the defined relations, we can see that TSP takes better advantage of the text structure and is a more complex task that is expected to provide more valuable knowledge.\n\n3.2 PROPOSED TEXT STRUCTURE RELATIONSHIPS\n\nThe key part of the training objective is our design of six classes to represent six text structure relationships between sentences. As illustrated on the right in Figure 4, these relationships are the set of combination of three hierarchical relationships {neighbor, same paragraph, same document} and two ordering relationships {normal order, reverse order}. Concretely, these relationships are defined as follows. The hierarchical relationships include (1) neighbor (two sentences are adjacent), (2) same paragraph (two sentences are in the same paragraph but not adjacent), and (3) same document (two sentences are in the same document but not adjacent or in the same paragraph). The ordering relationships include (1) normal order (two sentences are in the forward order), and (2) reversed order (two sentences are in the reversed order).\n\n3.3 LEARNING EXPECTATION FROM HIERARCHY AND ORDER\n\nThe text hierarchy describes the semantics hierarchy and the interactions between the semantics. As shown in Figure 2, the semantics of sentences are included in higher-level paragraph semantics, and paragraph semantics are included in higher-level document semantics. Relationships between semantics at different levels and the same level can also be seen in the example. To group sentences nestedly, models are expected to summarize semantics at different levels from the context, while capturing relationships and interactions between these semantics. Intuitively, it can provide unique learning signals for models’ understanding ability on and between texts at different levels of granularity.\n\nThe order of sentences depends on the logical relations that connect the different concepts described in the sentences. These relations can be chronological relations, cause-effect relations, and so on. Textual coherence is also an important clue for sentence order. A model must recognize relations between concepts and identify coherence to rearrange the order of sentences. Moreover, learning from sentence order has long been shown to be useful for many downstream applications (Cui et al., 2020).\n\n6\n\nSame Document Reverse OrderSame DocumentNormal OrderSame Paragraph Reverse OrderSame Paragraph Normal OrderNeighbor Reverse OrderNeighbor Normal OrderNeighbor Reverse OrderNeighbor Normal OrderDifferent DocumentNeighborOrderHierarchyText Structure Prediction (TSP)Sentence Order Prediction (SOP)Next Sentence Prediction (NSP)Different DocumentNeighbor Reverse OrderNeighbor Normal OrderSentence Structure Objective (SSO)Under review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\n4.1 SETUP\n\n4.1.1 PRETRAINING\n\nAll models in this paper were pretrained on OpenWebText (Gokaslan & Cohen, 2019), an opensource recreation of the WebText corpus described in GPT2 (Radford et al., 2019). It includes 38GB of texts from 8,013,769 documents extracted from Reddit posts. Note that we did not use Wikipedia+BookCorpus because of the public unavailability of official BookCorpus (Zhu et al., 2015). In preprocessing the pretraining data, we delimited paragraphs by blank lines and marked the scopes of sentences with the sentence splitter of NLTK (Bird et al., 2009) for the use of TSP.\n\nFor this paper, we experimented with pretraining models on word-based MLM with different intersentence tasks or on MLM only. In particular, we mainly followed BERT’s pretraining setting and referred to the setting of ELECTRA (Clark et al., 2020), another popular pre-trained model. We trained with a sequence length of 512 and 1 million training steps at the Base and Large scales, or with a length of 128 and 1.45 million steps at the Small scale. The batch sizes vary with the scale, as described along with other details in Appendix A. To ensure fair comparisons, all models at the same scale had almost the same computational cost and number of parameters in both training and inference. Note that we did not perform any hyperparameter search, although we think it could help our proposed method to perform better.\n\n4.1.2 EVALUATION\n\nOur pretrained models were evaluated on the widely adopted SuperGLUE benchmark (Wang et al., 2019) for evaluating general language understanding systems. SuperGLUE contains tasks covering natural language inference tasks RTE (Dagan et al., 2006) and CB, multiple-choice reasoning task COPA (Melissa et al., 2011), question-answering tasks BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), and word sense disambiguation task WiC (Pilehvar & Camacho-Collados, 2019). The metrics used here for these tasks are the same as those the ones described in the SuperGLUE paper.\n\nBecause some of the evaluation datasets are small and the scores on them could vary substantially depending on the random seed, we followed the ELECTRA setting and finetuned 10 runs for each task from the same pretrained checkpoint for every model. Here, we took the median score of 10 runs for our development set results (Appendix D), and we used the best-performing model on the development set for evaluation on the test set. In contrast to certain related papers, we did not apply tricks such as intermediate finetuning or ensembling during the stage of finetuning. Similarly, we excluded the WSC task because it involves intermediate finetuning and publicly unavailable training data to obtain decent results (Kocijan et al., 2019). More details about our finetuning are given in Appendix B.\n\n4.2 RESULTS\n\nTable 1 lists the averaged SuperGLUE scores. TSP improved on pure MLM and outperformed the inter-sentence baselines at all scales. These results show the effectiveness and potential of incorporating text structure information into learning. The results also reveal two findings. First, NSP undermined or made little difference in the performance, which matches the claims of Liu et al. (2019); Yang et al. (2019). Second, while SOP has been claimed to improve NSP on the GLUE benchmark (Wang et al., 2018; Lan et al., 2020), we found that it failed to outperform NSP in our experiment.\n\nNext, we analyze the scaling of the four inter-sentence tasks. Although the inter-sentence baselines (NSP, SOP, and SSO) improved on pure MLM at the Small scale, at the larger scales they failed to improve on pure MLM by a significant margin or even undermined the performance. We suspect the reason to be the task difficulty. While the Small-scale models learned from the inter-sentence baseline tasks, those tasks may not have been difficult enough to provide valuable learning signals to models at larger scales. We can also see that SSO (the fusion of NSP and SOP) is slightly more complicated than NSP and SOP, and it slightly outperformed both NSP and SOP at Base\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Experiment results on the SuperGLUE test set at different scales. Overall, TSP outperformed MLM at the different scales, whereas the other inter-sentence baselines (NSP, SOP, and SSO) failed to achieve improvement. Note that because the sizes and settings for the naming of scales (e.g., Small, Base, and Large) differ greatly in different papers, results may not be directly comparable between papers.\n\nScale Model\n\nSmall\n\nBase\n\nLarge\n\nMLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours)\n\nAvg.\n\n58.8 61.1 59.9 60.8 61.5 71.0 70.8 71.0 71.6 73.6 76.5 76.5 76.4 76.6 79.0\n\nCB\n\nCOPA\n\nMultiRC\n\nRTE\n\nWIC\n\nBoolQ\n\nReCoRD\n\nAvg. F1/Acc.\n\n75.7/81.6 76.2/82.0 77.7/80.4 74.4/81.6 83.0/86.0 84.8/90.4 84.5/90.8 83.5/90.4 84.8/89.6 84.2/91.2 87.4/92.4 83.6/89.6 84.8/90.4 81.8/88.4 93.5/94.8\n\nAcc.\n\n57.6 58.4 59.0 57.2 59.4 63.2 66.0 70.0 71.2 72.4 73.6 76.0 74.6 74.6 77.4\n\nF1a/EM\n\n60.3/13.6 61.1/14.7 61.7/14.5 61.1/14.3 62.4/14.3 73.2/28.6 73.8/30.6 71.7/25.4 73.0/28.5 75.8/33.5 77.8/37.5 78.2/38.4 79.3/40.9 79.6/40.9 80.5/43.2\n\nAcc.\n\n58.3 65.5 63.0 66.1 66.1 71.3 73.2 70.8 72.3 74.1 76.6 77.4 77.1 78.5 79.1\n\nAcc.\n\n60.9 65.5 60.4 66.1 60.2 66.1 68.8 68.1 68.0 66.6 68.7 71.0 68.6 70.6 72.4\n\nAcc.\n\n70.4 71.5 71.7 71.8 71.8 79.4 70.9 78.3 76.9 80.6 82.7 81.7 82.2 82.1 82.7\n\nF1/Acc.\n\n49.4/48.6 50.1/49.3 48.3/47.4 49.3/48.6 50.5/49.8 79.1/78.2 77.9/76.9 74.9/74.0 75.0/74.0 79.2/78.5 86.5/85.7 85.1/84.4 85.2/84.5 85.1/84.3 85.8/85.2\n\nand Large scales. On the other hand, with the better use of text structure information, TSP was designed as more complicated (six inter-sentence relations instead of two or three inter-sentence relations for the inter-sentence baseline tasks, as shown in Figure 4) and more difficult. TSP provides valuable knowledge for models at all scales, especially for Large-scale models, which have the largest capability and consumed data to better exploit the difficult TSP task, thus yielding the largest improvement over other baselines as compared to the other scales. Encouraged by this observation, we expect that models with even larger model sizes and training data would also benefit significantly from learning text structure information.\n\nWhile TSP provided competitive or better performance on most of the tasks, we also found that it performed well on the reasoning (CB, RTE, and COPA) and question-answering (MultiRC and BoolQ) tasks. We can view a question and an answer as two texts that share a similar concept and can be placed in the same hierarchical group, such as a paragraph or a pair of consecutive sentences. Likewise, we can view reasoning in terms of whether two texts are coherent. From these viewpoints, the results are consistent with our interpretation that TSP facilitates language understanding by strengthening recognition and comparison of concepts and reasoning relationships (Section 3.2). On the other hand, although ReCoRD is a QA task, it is more like prediction of a masked entity in a paragraph given the context, which may explain why pure MLM was the leading model for ReCoRD at the Large scale. Regarding WiC, a word sense disambiguation task, TSP showed an interesting phenomenon. Compared to other baselines in the WiC task, TSP performed poorly at the Small scale, got better at the Base scale, and became the best-performing model at the Large scale. We can relate this result to our surmise on the scaling effect mentioned above. Learning of the complicated TSP stretches models’ capabilities, and small models may not have enough capability to encode information that is useful for word-level tasks like WiC, whereas large models have enough capability that complicated sentence-level learning can even help with word-level understanding. While more elaborate approaches will be needed to analyze the effect of learning text structure information on different types of downstream tasks, which is outside the scope of this paper, we hope to have a good starting point by providing interesting results and explanations for those who follow.\n\nIn addition to the main results, we also reported other experiment results. First, we reported scores on the SuperGLUE development set in Appendix D, which showed similar results as in Table 1. Second, we reported results on the GLUE benchmark in Appendix E. Finally, to demonstrate that our proposed approach is generally useful, we did an experiment showing that TSP can be easily combined with another word-based pretraining task (see Appendix F for details). Additionally, this\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Ablation results for the designed text structure relations.\n\nModel\n\n#total classes\n\nMLM+TSP MLM+TSP - paragraph MLM+TSP - hierarchy MLM+TSP - order MLM\n\n6 4\n2 3\n-\n\n#ordering relationship 2\n2 2\n- -\n\n#hierarchical relationship 3\n2 -\n3 -\n\nSuperGLUE\n\n73.9 73.2 72.5 71.9 71.2\n\nexperiment also shows that TSP still improves performance in its preliminary results, which suggests that learning text structure information may generalize to pretraining tasks other than MLM.\n\n5 ABLATION STUDY\n\nTo investigate whether the proposed TSP text structure relations are helpful, we experimented with different TSP variations in which some text structure relations were ablated. Specifically, MLM+TSP -hierarchy adopted only the 2 ordering relationships (normal, reversed), MLM+TSP - order adopted only the 3 hierarchical relationships (neighbor, paragraph, document), and MLM+TSP -paragraph combined the 2 ordering relationships with only the 2 hierarchical relationships (neighbor, document) , with no discrimination of whether two sentences were in the same paragraph.\n\nThe results on the SuperGLUE development set of models at the Base scale are shown in Table 2. First, we found that the models pretrained with TSP benefitted from both the hierarchy relationships and the ordering relationships, as both MLM+TSP - hierarchy and MLM+TSP - order underperformed MLM+TSP and outperformed MLM. Second, the performance slightly dropped when it was not discriminated whether two sentences were in the same paragraph, which shows that the adjacency of sentences is not the only important factor, and that discrimination of hierarchical groups at different levels matters. Although other design elements such as discrimination of sections and documents remain unexplored, we will encourage future work to explore further, whereas in this paper we focus on uncovering the potential for integrating text structure information.\n\n6 CONCLUSION\n\nDespite their stated usefulness for high-level language understanding and continued development, inter-sentence tasks have not gained enough attention as expected. Our experiments have shown that existing inter-sentence approaches for general-purpose language pretraining can not improve performance or even degrade performance at scales larger than Small. Furthermore, these existing approaches ignore a large part of text structure information and oversimplify complicated text structures into one relationship between two lengthy parts of a text sequence. The goal of this paper was thus to explore the potential of utilizing text structure information to provide more valuable learning signals and maintain the improvement at larger scales. For an initial attempt to achieve this goal, we proposed Text Structure Prediction (TSP), a self-supervised inter-sentence pretraining task that can easily be combined with any encoder-based pretraining task. TSP redefines what is learned from sentence pairs via text structure relationships. Specifically, these text structure relationships combine hierarchical and ordering relationships, which can be seen as implicit learning signals for language understanding via recognition of concepts and reasoning relationships respectively. In this paper, we have shown that (1) inter-sentence baselines (NSP, SOP, SSO) were not helpful for performance at larger scales in our settings, whereas TSP improved the overall performance of models at all scales in our experiments, which again demonstrates that inter-sentence tasks for learning from inter-sentence relationships can help language understanding. Moreover, (2) the good scaling and significant improvement at the Large scale showed that the exploitation of text structure information can result in a sophisticated task that provides valuable knowledge for models with a large capability and data size. Finally, although there may still be much room for different ways to exploit text structure information, we have shown the effectiveness and potential of exploiting such information, and we expect that this work will encourage more research on learning from text structure information.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRegina Barzilay and Mirella Lapata. Modeling local coherence: An entity-based approach. Com-\n\nputational Linguistics, 34(1):1–34, 2008.\n\nSteven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing\n\ntext with the natural language toolkit. ” O’Reilly Media, Inc.”, 2009.\n\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936. Association for Computational Linguistics, June 2019.\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2020.\n\nBaiyun Cui, Yingming Li, and Zhongfei Zhang. BERT-enhanced relational sentence ordering network. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6310–6320. Association for Computational Linguistics, November 2020.\n\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Joaquin Qui ̃nonero-Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alch ́e Buc (eds.), Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pp. 177–190. Springer Berlin Heidelberg, 2006.\n\nFrantisek Danes. FUNCTIONAL SENTENCE PERSPECTIVE AND THE ORGANIZATION OF THE\n\nTEXT. De Gruyter Mouton, 2015.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186. Association for Computational Linguistics, June 2019.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6894–6910. Association for Computational Linguistics, November 2021.\n\nAaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/\n\nOpenWebTextCorpus, 2019.\n\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016.\n\nDan Iter, Kelvin Guu, Larry Lansing, and Dan Jurafsky. Pretraining with contrastive sentence objectives improves discourse performance of language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4859–4870. Association for Computational Linguistics, July 2020.\n\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252–262. Association for Computational Linguistics, June 2018.\n\nRyan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.\n\nTibor Kiss and Jan Strunk. Unsupervised multilingual sentence boundary detection. Computational\n\nLinguistics, 32(4):485–525, 2006.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz.\n\nA surprisingly robust trick for winograd schema challenge. CoRR, abs/1905.06290, 2019.\n\nPawan Kumar, Dhanajit Brahma, Harish Karnick, and Piyush Rai. Deep attentive ranking networks for learning to order sentences. Proceedings of the AAAI Conference on Artificial Intelligence, 34 (05):8115–8122, April 2020.\n\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020.\n\nYoav Levine, Barak Lenz, Opher Lieber, Omri Abend, Kevin Leyton-Brown, Moshe Tennenholtz, and Yoav Shoham. {PMI}-masking: Principled masking of correlated spans. In International Conference on Learning Representations, 2021.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880. Association for Computational Linguistics, July 2020.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.\n\nRoemmele Melissa, Bejan Cosmin, Adrian, and Gordon Andrew, S. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium on Logical Formalizations of Commonsense Reasoning, pp. 21–23, Stanford University, March 2011.\n\nYu Meng, Chenyan Xiong, Payal Bajaj, saurabh tiwary, Paul Bennett, Jiawei Han, and XIA SONG. Coco-lm: Correcting and contrasting text sequences for language model pretraining. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 23102–23114. Curran Associates, Inc., 2021.\n\nMohsen Mesgar and Michael Strube. A neural local coherence model for text quality assessment. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4328–4339. Association for Computational Linguistics, October-November 2018.\n\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2227–2237. Association for Computational Linguistics, June 2018.\n\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1267–1273. Association for Computational Linguistics, June 2019.\n\nShrimai Prabhumoye, Ruslan Salakhutdinov, and Alan W Black. Topological sort for sentence ordering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2783–2792. Association for Computational Linguistics, July 2020.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language under-\n\nstanding by generative pre-training. 2018.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n\nmodels are unsupervised multitask learners. 2019.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nNaftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing, pp. 368–377, 1999.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355. Association for Computational Linguistics, November 2018.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi Bao, Liwei Peng, and Luo Si. Structbert: Incorporating language structures into pre-training for deep language understanding. In International Conference on Learning Representations, 2020.\n\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. Clear: Contrastive\n\nlearning for sentence representation, 2020.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nZiyi Yang, Yinfei Yang, Daniel Cer, Jax Law, and Eric Darve. Universal sentence representation learning with conditional masked language model. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6216–6228. Association for Computational Linguistics, November 2021.\n\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension, 2018.\n\nY. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision (ICCV), pp. 19–27. IEEE Computer Society, dec 2015.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA PRETRAINING DETAILS\n\nThe details given in Table 3 apply to all the models in this paper. In most cases, we used the same hyperparameters as for BERT and ELECTRA. At the Large scale, we adopted a setting closer to BERT-Large because ELECTRA’s setting for the Large scale requires much higher memory and computational cost than our computational resource could handle. For the Large scale, we also slightly increased the batch size to 288, which we found that avoids divergence in our experiments. Additionally, inspired by BERT, we trained with a sequence length of 128 for the first half of the training steps and then with a sequence length of 512 for the rest while keeping the same number of tokens in a batch to speed up pretraining in our Large-scale experiments. Note that we did not perform any hyperparameter search, although we think it could help our proposed method to perform better.\n\nTable 3: Pretraining hyperparameters.\n\nHyperparameter Number of layers Hidden size FFN inner hidden size Attention head size Attention heads Embedding size Mask percent Learning rate decay Warmup steps Learning rate Adam ε Adam β1 Adam β2 Dropout Weight decay Batch size Training steps (MLM/ELECTRA)\n\nLarge 24 1024 4096 16 64 1024 15\n\nSmall 12 256 1024 4\n64 128 15 Linear 10000 5e-4 1e-6 0.9 0.999 0.1 0.01 128 1.45M/1M 1M/-\n\nBase 12 768 3072 12 64 768 15 Linear Linear 10000 10000 1e-4 2e-4 1e-6 1e-6 0.9 0.9 0.999 0.999 0.1 0.1 0.01 0.01 288 256 1M/-\n\nB FINETUNING DETAILS\n\nOur finetuning setting (see Table 4 for details) mostly followed that of ELECTRA. Most tasks were finetuned for 3 epochs. However, inspired by ELECTRA, which increases the number of epochs to 10 for the RTE task, we also found that increasing the number of epochs to 10 helped stabilize the finetuning performance for CB and BoolQ. Except for this, we did not perform any hyperparameter search for finetuning.\n\nTable 4: Finetuning hyperparameters.\n\nHyperparameter Learning rate Adam epsilon Adam beta 1 Adam beta 2 Layerwise LR decay Learning rate decay Warmup fraction Dropout Weight decay Batch size Training epochs\n\nValue 3e-4 for Small, 1e-4 for Base, 5e-5 for Large 1e-6 0.9 0.999 0.8 for Base/Small, 0.9 for Large Linear 0.1 0.1 0.1 32 10 for RTE, CB, BoolQ; 3 for other tasks.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFor the WiC and ReCoRD tasks, in which span embeddings for words or entities are required to compare spans with each other, we created span embeddings by applying average pooling to the final hidden representations of the tokens constructing a span and we compared spans by passing the concatenated span embeddings to the classifiers.\n\nC SHUFFLING RATE\n\nTable 5: Comparison of different sentence shuffling rates in terms of the SuperGLUE development set results for Small-scale MLM-based models. We choose 15% as our final choice for shuffling rate. The results show that TSP is not sensitive to the extent of shuffling, but shuffling is necessary for TSP.\n\nShuffling rate 0% 15% 30% 50% 100%\n\nSuperGLUE 59.7 60.3 60.3 60.2 60.1\n\nAmong the sentences in a given input sequence, we randomly shuffled 15% of them while other sentences stay at their original positions, to prevent complete leakage of the original ordering and hierarchical information. We also experimented with different rates for such kind of partial shuffling, and the results are listed in Table 5. The case of no shuffling and complete shuffling gave the worst results, while there was little difference in performance for shuffling of 15 ∼ 50%. Accordingly, the results indicate that shuffling is needed, but the proposed task is not sensitive to the partial shuffling rate. We chose 15% as the partial shuffling rate in this paper because it was the lowest shuffling rate in this analysis that gave a good performance with minimal corruption of inputs.\n\nD RESULTS ON THE SUPERGLUE DEVELOPMENT SET\n\nFor reference, Table 6 lists our experimental results on the SuperGLUE development set.\n\nE RESULTS ON THE GLUE BENCHMARK\n\nThe GLUE benchmark is the predecessor of the SuperGLUE benchmark. Compared to the old GLUE benchmark, SuperGLUE comes with a new set of more difficult language understanding tasks and improved resources, which encourages us to evaluate models on SuperGLUE as our main results. At the same time, we also provide GLUE results for readers’ reference. As shown in Table 8, the proposed task, which learns from both hierarchical and ordering relationships, also outperform other baselines on the GLUE benchmark at Small, Base, and Large scales.\n\nF COMBINATION WITH OTHER WORD-LEVEL PRETRAINING METHOD\n\nBecause the proposed method can easily be added on top of other word-level pretraining methods, we conduct an experiment to evaluate whether the proposed method improves performance when it is not combined with MLM. ELECTRA (Clark et al., 2020), another word-level pretraining method, was adopted as the alternative to MLM in this experiment. We chose ELECTRA because it is a discriminative pretraining method that performs binary classification to detect replaced words among all words, making it largely different from MLM, which is generative and predicts masked words. Although ELECTRA comprises a generator to generate inputs for discrimination and a discriminator to detect replaced tokens in generated inputs, the generator is discarded after pretraining, and only the discriminator is used for finetuning. This is why, when combining TSP with ELECTRA, we applied TSP only to the discriminator and the generation stage remained unchanged.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Results of MLM-based models on the SuperGLUE development set.\n\nScale Model\n\nSmall\n\nBase\n\nLarge\n\nMLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours)\n\nAvg.\n\n58.7 60.4 60.0 60.0 60.7 71.2 71.6 70.2 70.8 73.9 76.4 76.7 77.0 77.3 78.0\n\nCB\n\nCOPA\n\nMultiRC\n\nRTE\n\nWIC\n\nBoolQ\n\nReCoRD\n\nAvg. F1/Acc. 83.9/77.6 70.7/80.4 71.7/78.6 65.0/75.0 72.9/80.4 90.6/92.9 89.9/92.9 83.7/87.5 86.3/91.1 88.1/92.9 93.0/94.6 93.7/94.6 93.2/94.6 93.6/94.1 94.3/96.4\n\nAcc.\n\n55.0 58.0 55.0 56.0 59.0 63.0 63.0 66.0 66.0 72.0 70.0 70.5 71.0 71.5 72.0\n\nF1a/EM\n\n67.1/12.8 67.0/14.5 67.2/14.8 67.3/14.9 66.6/15.2 75.1/27.3 75.8/28.7 75.1/26.8 75.5/26.7 79.0/34.1 80.3/35.8 81.5/41.6 82.0/43.0 82.9/43.1 83.3/45.4\n\nAcc.\n\n59.6 66.4 68.6 66.4 68.6 73.7 73.8 72.6 72.2 77.6 80.7 81.2 80.9 82.0 82.0\n\nAcc.\n\n61.0 65.8 63.5 68.8 62.1 66.9 70.1 68.7 70.3 68.3 70.1 69.7 69.0 70.2 70.5\n\nAcc.\n\n70.7 73.0 73.2 73.2 73.5 80.1 79.9 79.1 78.9 80.6 83.3 82.5 83.3 83.3 83.9\n\nF1/Acc.\n\n39.0/48.2 38.6/48.1 39.1/48.2 39.3/49.1 39.7/49.1 64.9/78.0 64.1/76.9 62.2/74.6 62.4/74.9 65.2/78.2 71.7/85.5 70.7/84.2 71.1/85.0 70.6/84.2 71.4/85.0\n\nTable 7: Results of MLM-based models on the GLUE development set. ”Pearson” stands for Pearson correlation. ”Spearman” stands for Spearman correlation.\n\nScale Model\n\nAvg.\n\nSmall\n\nBase\n\nLarge\n\n77.3 MLM 79.1 MLM+NSP (BERT) 78.4 MLM+SOP MLM+SSO 79.2 MLM+TSP (ours) 79.8 84.6 MLM 84.3 MLM+NSP (BERT) 83.5 MLM+SOP MLM+SSO 83.9 MLM+TSP (ours) 85.5 86.5 MLM 86.2 MLM+NSP (BERT) 86.5 MLM+SOP MLM+SSO 86.5 MLM+TSP (ours) 87.4\n\nCoLA\n\nSST\n\nMRPC\n\nSTS\n\nQQP\n\nMultiNLI\n\nQNLI\n\nRTE\n\nMatthew’s Corr\n\nAcc.\n\nF1/Acc\n\nPearson/Spearman\n\nF1/Acc.\n\nmatched/unmatched\n\nAcc.\n\n47.0 44.4 44.5 45.1 46.8 61.5 60.4 58.1 60.3 62.5 62.5 60.2 61.7 61.5 64.7\n\n90.0 89.8 89.6 89.6 89.7 93.5 93.1 92.0 92.5 93.9 95.0 94.5 94.6 94.5 95.1\n\n84.6/88.5 83.6/83.2 89.4/85.0 88.4/88.0 85.4/89.1 90.0/85.8 85.1/88.8 85.3/84.8 89.2/84.8 85.6/89.2 88.2/87.8 90.1/85.8 85.6/89.3 91.0/87.2 87.1/86.7 87.9/91.0 89.7/89.5 92.2/89.1 87.9/91.0 90.4/90.1 91.5/88.2 87.7/90.9 89.9/89.6 91.7/88.5 91.6/88.2 87.5/90.8 90.7/90.3 92.5/89.7 90.7/90.4 87.8/91.0 89.0/91.7 90.8/90.7 92.0/88.8 88.8/91.7 91.8/91.4 92.2/89.2 88.9/91.7 91.6/91.4 92.6/89.7 91.8/91.6 92.2/88.8 88.8/91.6 93.4/90.9 91.8/91.6 89.1/91.9\n\n79.2/80.3 79.9/81.0 78.8/79.3 80.3/81.2 80.2/81.2 86.6/86.7 85.3/85.7 84.9/85.2 85.0/85.5 86.3/86.6 88.7/89.1 88.0/88.3 88.3/88.6 88.4/88.5 88.3/89.0\n\n85.2 88.3 86.6 88.7 88.9 91.8 91.7 91.0 91.5 92.8 93.3 93.0 93.0 93.0 94.1\n\nAcc.\n\n59.6 66.4 68.6 66.4 68.6 73.7 73.8 72.6 72.2 77.6 80.7 81.2 80.9 82.0 82.0\n\nNote that, as in the previous experiments, TSP added negligible parameters and computation to the ELECTRA pretraining.\n\nThe experimental results are listed in Table 9. When combined with ELECTRA, the proposed method could still improve the SuperGLUE score. Because of limited computational resources and time, we did not perform these experiments beyond the Small-scale, but we expect that the proposed method would perform well in such cases. Overall, this experiment shows the capability of the proposed method to be combined with other word-level pretraining methods, thus suggesting the opportunity to improve such methods via text structure learning in the future.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Results of MLM-based models on the GLUE development set. ”Pearson” stands for Pearson correlation. ”Spearman” stands for Spearman correlation.\n\nScale Model\n\nSmall\n\nBase\n\nLarge\n\nMLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours) MLM MLM+NSP (BERT) MLM+SOP MLM+SSO MLM+TSP (ours)\n\nAvg.\n\n77.3 79.1 78.4 79.2 79.8 84.6 84.3 83.5 83.9 85.5 86.5 86.2 86.5 86.5 87.4\n\nCoLA\n\nSST\n\nMRPC\n\nSTS\n\nMatthew’s Corr\n\nAcc.\n\nF1/Acc\n\nPearson/Spearman\n\n47.0 44.4 44.5 45.1 46.8 61.5 60.4 58.1 60.3 62.5 62.5 60.2 61.7 61.5 64.7\n\n90.0 89.8 89.6 89.6 89.7 93.5 93.1 92.0 92.5 93.9 95.0 94.5 94.6 94.5 95.1\n\n89.4/85.0 90.0/85.8 89.2/84.8 90.1/85.8 91.0/87.2 92.2/89.1 91.5/88.2 91.7/88.5 91.6/88.2 92.5/89.7 92.0/88.8 92.2/89.2 92.6/89.7 92.2/88.8 93.4/90.9\n\n83.6/83.2 88.4/88.0 85.3/84.8 88.2/87.8 87.1/86.7 89.7/89.5 90.4/90.1 89.9/89.6 90.7/90.3 90.7/90.4 90.8/90.7 91.8/91.4 91.6/91.4 91.8/91.6 91.8/91.6\n\nQQP\n\nF1/Acc.\n\n84.6/88.5 85.4/89.1 85.1/88.8 85.6/89.2 85.6/89.3 87.9/91.0 87.9/91.0 87.7/90.9 87.5/90.8 87.8/91.0 89.0/91.7 88.8/91.7 88.9/91.7 88.8/91.6 89.1/91.9\n\nMultiNLI\n\nQNLI\n\nRTE\n\nmatched/unmatched\n\nAcc.\n\n79.2/80.3 79.9/81.0 78.8/79.3 80.3/81.2 80.2/81.2 86.6/86.7 85.3/85.7 84.9/85.2 85.0/85.5 86.3/86.6 88.7/89.1 88.0/88.3 88.3/88.6 88.4/88.5 88.3/89.0\n\n85.2 88.3 86.6 88.7 88.9 91.8 91.7 91.0 91.5 92.8 93.3 93.0 93.0 93.0 94.1\n\nAcc.\n\n59.6 66.4 68.6 66.4 68.6 73.7 73.8 72.6 72.2 77.6 80.7 81.2 80.9 82.0 82.0\n\nTable 9: Results of ELECTRA-based models on the SuperGLUE development set. TSP performs well even when combined with another word-level pretraining method that is vastly different from MLM.\n\nScale Model\n\nSmall\n\nELECTRA ELECTRA+NSP ELECTRA+SOP ELECTRA+SSO ELECTRA+TSP\n\nSuperGLUE 57.4 58.6 58.5 60.6 60.7\n\n16",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a new way to pre-train language models. The main idea is to include sentence-level hierarchy information during the pre-training. Instead of considering only neighbor sentences, they consider more relations between sentences such as if they are in the same paragraph or if they are in the same document. By training this classification loss along with the original masked token loss, they have better performance on the SuperGLUE downstream tasks.\n\n# Strength And Weaknesses\n\nStrength\n- The motivation is clear and reasonable.\n- The experiments support the claim and seem to be promising\n\nWeaknesses\n- To test the generalizability, I suggest to test on GLUE tasks as well.\n- It's not clear what size of models is using for the ablation study in Table 2 \n- What will happen if we do not shuffle the sentences?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe proposed method is clearly described. The novelty is enough for me. They propose additional self-supervised loss to improve the language models.\n\n# Summary Of The Review\n\nThe motivation is clear and reasonable. Although the proposed method is simple, it's very effective. Ablation study supports their claim.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDENSITY SKETCHES FOR SAMPLING AND ESTIMATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThere has been an exponential increase in the data generated worldwide. Insights into this data led by machine learning (ML) have given rise to exciting applications such as recommendation engines, conversational agents, and so on. Often, data for these applications is generated at a rate faster than ML pipelines can consume it. In this paper, we propose Density Sketches(DS) - a cheap and practical approach to reducing data redundancy in a streaming fashion. DS creates a succinct online summary of data distribution. While DS does not store the samples from the stream, we can sample unseen data on the fly from DS to use for downstream learning tasks. In this sense, DS can replace actual data in many machine learning pipelines analogous to generative models. Importantly, unlike generative models, which do not have statistical guarantees, the sampling distribution of DS asymptotically converges to underlying unknown density distribution. Additionally, DS is a one-pass algorithm that can be computed on data streams in compute and memory-constrained environments, including edge devices.\n\n1\n\nINTRODUCTION\n\nWith the advent of big data, the rate of data generation is exploding. For instance, Google has around 3.8 million search queries per minute, amounting to over 5 billion data points or terabytes of data generated daily. Any processing over this data, such as using the training of a recommendation model, would suffer from data explosion. By the time existing data is consumed, newer data is In such cases, we need to discard a lot of data. One of the critical research already available. directions is how to reduce data storage. In this paper, we present Density Sketches (DS): an efficient and online data structure for reducing redundancy in data.\n\nOften data comes from an underlying unknown distribution, and one of the challenges in data reduction is maintaining this distribution. In DS, we approximately store the data distribution in the form of a sketch. Using this DS, we can perform point-wise density estimation queries. Additionally, we can sample synthetic data from this sketch to use in downstream machine learning tasks. This paper shows that data sampled from DS asymptotically converges to the underlying unknown distribution. We can also view density sketches through the lens of coresets. Specifically, DS is a compressed version of grid coresets. Grid coresets are the oldest form of coresets, giving lower additive errors than modern coresets. However, grid coresets are generally prohibitive as they are exponential in dimension (d). DS enables us to approximate grid coresets with the dependence of memory usage depending on the actual variety in the data instead of being exponential in d. Also, DS provides a streaming construction for this coreset.\n\nIn this paper, we focus more on the density estimation and sampling aspects of DS. Sampling from a distribution described using data requires estimating the underlying distribution. Popular methods to infer the distribution and sample from it belong to the following three categories: 1. Parametric density estimation (Friedman et al., 2001) 2. Non-parametric estimation - Histograms and Kernel Density Estimators (KDE) (Scott, 2015) 3. Learning-based approaches such as Variational Auto Encoders (VAE), Generative Adversarial Networks (GANs), and related methods (Goodfellow et al., 2014; 2016). Generally, parametric estimation is not suitable to model most real data as it can lead to significant, unavoidable bias from the choice of the model (Scott, 2015). Learning the distribution, e.g., via neural networks, is one solution to this problem. Although learning-based methods have recently found remarkable success, they do not have any theoretical guarantees for the distribution of generated samples. Histograms and KDEs, on the other hand, are theoretically well understood. These statistical estimators of density are known to uniformly converge to the underlying true distribution almost surely. This paper focuses on such estimators, which have theoretical guarantees.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nStorage of histograms and sampling from them is expensive because of an exponential number of partitions (also known as bins). Apart from this, histograms also suffer from the bin-edge problem: a slight variation in data can lead to significant differences in the estimation of densities. KDEs are used to solve the bin-edge problem. KDE gives a smoother estimate of density. While sampling from a KDE is efficient, KDE is expensive to store. KDE requires us to store the entire data. Coresets for KDE are a good solution to the storage problems of KDE. However, the construction of coresets is typically quite expensive. In this work, we propose Density Sketches(DS) - a compressed sketch of density constructed in an efficient streaming manner. DS does not store actual samples of the data. But we can still efficiently produce samples from a KDE for specific kernels, which, in turn, approximates f (x). Being a compressed sketch, we can tune the accuracy-storage trade-off of DS, and we analyze this trade-off in the theorem 1.\n\n2 PROBLEM STATEMENT AND RELATED WORK\n\nProblem Statement: Formally, we want to create a data structure that has the following properties : (1) It sketches density information. (2) The sketch size is much smaller than the data size and does not scale linearly with it. (3) The construction is streaming and efficient. (4) We do not store any samples in the data structure created (for privacy reasons). (5) We want the sampling distribution, say ˆfS(x), obtained by sampling from these data structures to approximate the true underlying distribution f (x).\n\nThe problem we aim to solve can be considered a data reduction problem and has been widely pursued in literature. The set of existing approaches can be broadly classified into two sections. (1) Sampling based / Coresets : Approaches such as clustering/importance sampling (Charikar & Siminelakis, 2017; Cortes & Scott, 2016; Chen et al., 2012) and coresets for KDE (Phillips & Tai, 2020; 2018) fall under this category. These approaches aim to find a small set of possibly weighted samples for a specific objective function such that the result obtained by applying the function to this small set is within a small approximation error of the result obtained by applying the objective function on a complete dataset. The issue with these approaches is that of efficiency. Most of these algorithms require complicated computation over the entire data. Some streaming algorithms were recently proposed for coresets for KDE (Karnin & Liberty, 2019). However, even these algorithms need to perform O(m) (m is compactor size) computationally expensive operations per sample for large chunks of size (m), making them unsuitable for our purposes. (2) Dimensionality reduction: These approaches aim to reduce the width of the data matrix. Approaches such as Principle Component Analysis (PCA) are computationally expensive and require iterative computation over the entire dataset. Random projections are an efficient streaming algorithm for dimensionality reduction. However, this approach leads to compressed data that increases linearly with the original data size. As we can see, existing approaches fall short of the requirements in our problem statement.\n\n3 BACKGROUND\n\n3.1 HISTOGRAMS AND KERNEL DENSITY ESTIMATION\n\nHistograms and KDE (Scott, 2015; Scott & Sain, 2004) are popular methods to estimate the density of a distribution given a finite i.i.d. sample of n points in Rd drawn from the true density, say f (x).\n\nHistogram: Histogram divides the support S ⊂ Rd of the data into multiple partitions. It then uses the counts in every partition to predict the density, ˆfH (x), at a point x. Formally the density predicted at the point x ∈ S is given by\n\nˆfH (x) =\n\nC(bin(x)) nV(bin(x))\n\nwhere bin(x) identifies the partition of x, C(b) and V(b) measures the the number of samples in partition b and the volume of partition b respectively. ˆfH (x) integrates to 1 and hence ˆfH (x) is also an estimate of the underlying density function f (x). Regular histograms use hyper-cube partitions of width B aligned with the data axes. As B increases, the bias of the estimate increases, and its variance decreases. Histograms suffer from bin-edge problems where a slight change in data across the bin’s edge can change predictions significantly.\n\nKernel Density Estimation(KDE): KDE provides a smoother estimate of f (x) which resolves the bin-edge problem of histograms. For a positive semi-definite kernel function k(x, y) : Rd×Rd → R\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nand data, say D, the KDE at point x is defined as\n\nˆfK(x) = KDE(x) =\n\n1 n\n\nn (cid:88)\n\ni=1\n\nk(x, xi) where xi ∈ D\n\nfor\n\nAs B increases,\n\nthe Gaussian kernel\n\nA smoothing parameter B also parameterizes the kernel\n\nKernel functions are positive, symmetric, and may be normalized to integrate to 1. Gaus- (Friedman et al., 2001) are some of the most widely used sian, Epanechnikov, Uniform, function and deterkernels. For uniform mines the standard deviation parameter and Epanechnikov kernel functions, B is the window width around x where the kernel is non-zero. the bias of KDE increases, and its variance decreases. Histograms estimator and KDE both uniformly converge to underlying true distribution asymptotically. However, both suffer from the curse of dimensionality. To get a decent estimate of density in high dimensions, the number of samples needed is exponential in dimensions. For the density estimation task, dimensions of 4-50 are considered large. (Wang & Scott, 2019)\n\nfunction.\n\n3.2 COUNT SKETCHES\n\nFigure 1: Countsketch, sketching, and query\n\nThe count sketch (CS) (Cormode & Muthukrishnan, 2009; Charikar et al., 2002), along with its variants, is one of the most popular probabilistic data structures used for the heavy hitter problem. Given a stream of (at, ct) key-value pairs, at ∈ U, CS stores the compressed total counts for each of the keys in a small K × R array of integers and can be queried to retrieve this total count, C(at). CS offers a probabilistic solution in memory logarithmic in a total number of unique keys. There is a standard memory accuracy trade-off for CS. Let m be the number of distinct keys and C be the vector of counts indexed by each key. For count median sketch (Charikar et al., 2002), the ((cid:15), δ) guarantee P(| ˆC(a) − C(a)| > (cid:15)||C||2) ≤ δ is achieved using O( 1 δ (log m + log |U|)) space. (Chakrabati, 2020). As seen from the above equation, the approximation accuracy for a particular key depends on how it compares to the ||C||2. Specifically, CS can give an excellent approximation for keys with the highest values in a setting where most other keys have very low values. More discussion on CS can be found in Appendix F.\n\n(cid:15)2\n\n1\n\n3.3 LOCALITY SENSITIVE HASHING\n\nLocality-sensitive hashing(LSH)(Darrell et al., 2005) is a popular approach to solving approximate near-neighbor problems. If a function h : U → {0, ...r − 1} for some r, is randomly drawn from the LSH family L, the probability of collision of the hash values for two distinct elements a1 and a2 is\n\nPh∈L(h(a1)==h(a2)) ∝ Sim(a1, a2)\n\nWhere Sim(a1, a2) is some similarity metric corresponding to the LSH family. The probability of collision is referred to as the kernel of the LSH family, generally denoted by φ(., .). Most kernels are positive, bounded, symmetric, and reflective. We can use p independent LSH functions, h1, h2, ...hp to obtain a LSH function, h(p)(a) = (h1(a), h2(a), ..., hp(a)). The function h(p) has kernel ψ(, ., ) = φ(., .)p. We call p the power of the LSH function. Popular LSH functions for U = Rd are L2-LSH, L1-LSH and SRP (signed random projection). More details on LSH functions can be found in (Darrell et al., 2005)\n\n3.4 UNIFORM SAMPLING FROM CONVEX POLYTOPES\n\nUniform sampling from convex spaces is a well-studied problem (B ́elisle et al., 1993; Chen et al., 2017). For general convex polytopes, this is achieved by finding a point inside the polytope using convex feasibility algorithms and then running an MCMC walk inside the polytope to generate a point with uniform probability. In the case of regular convex polytopes like hypercubes and parallelopiped, uniform sampling is much simpler. Sampling a data point at random in a d-dimensional hypercube of width 1 is equivalent to uniformly sampling d real values in the interval [0, 1]. For sampling within a d-dimensional parallelopiped, we first locate (d − 1)-dimensional hyperplane parallel to each face at a distance drawn uniformly from [0, B] where B is the width of parallelopiped in that direction. The sampled point is the intersection of these (d − 1)-dimensional hyperplanes.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: bin(x) for different partitioning schemes\n\nPartitioning Scheme\n\nParameters\n\nRegular histogram\n\nB ∈ R\n\nbin(x) : Rd → Nd\n\nbin(x)i = (cid:98)xi/B(cid:99)\n\nAligned histogram\n\nB ∈ Rd\n\nbin(x)i = (cid:98)xi/Bi(cid:99)\n\nL1/L2-LSH\n\nW ∈ Rd×d B ∈ Rd, t ∈ Rd\n\nbin(x)i = ((cid:98)(cid:104)x, Wi(cid:105) + ti)/Bi(cid:99)\n\nSRP\n\nW ∈ Rk×d\n\nbin(x)i = (sign((cid:104)x, Wi(cid:105))\n\nSampling s ∈ Rd from b ∈ Nd ri ∼ U (0, 1), r ∈ Rd s = B(b + r) ri ∼ U (0, 1), r ∈ Rd s = B ◦ (b + r) ri ∼ U (0, 1), r ∈ Rd y = B ◦ (b + r) s = solve(Ws = y − t) MCMC with constraints, sign((cid:104)x, Wi(cid:105)) = bi + bounding box\n\n4 DENSITY SKETCHES In DS, we aim to build a compressed non-parametric estimation object in an efficient streaming fashion. As KDEs give a better approximation of underlying function f (x) than histograms, we want to build DS as a compressed KDE object. To achieve this, we use a nice connection between KDE and Histograms with an LSH-based partition function.\n\n4.1 HISTOGRAM WITH LSH-BASED PARTITION AND KERNEL DENSITY ESTIMATES Any LSH function on Rd will partition the space into different bins. Specifically, if power d L1/L2LSH, these partitions will be polytopes in Rd. Similarly, power k SRP would give conical partitions with hyper-plane boundaries. We can employ a histogram-based estimation strategy on the top of these randomly drawn partitions. The density estimate using such a histogram would be\n\nˆfH (x) ∝\n\n1 n\n\nn (cid:88)\n\ni=1\n\nI(xi ∈ bin(x)) where xi ∈ D\n\nwhere I is a indicator function. This estimate of the density has an expected value (over random partitions) equal to the KDE estimate, say ˆfφ(x) with the corresponding LSH kernel, φ(., .)\n\nEp( ˆfH (x)) =\n\n1 n\n\nn (cid:88)\n\ni=1\n\nP (xi ∈ bin(x)) =\n\n1 n\n\nn (cid:88)\n\ni=1\n\nφ(xi, x) = ˆfφ(x)\n\nThe expectation is over random partitions. This connection between randomized histograms and KDE was first observed in (Coleman & Shrivastava, 2020). To better approximate KDE, we can combine results from multiple histograms with independent LSH functions. For example, if we use m independent histograms, say H1, H2, ..., Hm, then the density estimate can be written as\n\nˆf (m) H (x) =\n\n1 m\n\nm (cid:88)\n\ni=1\n\nˆfHi(x)\n\nWe can sample a data point from this set by first choosing a histogram randomly and then sampling a point from that histogram. One can check that the sampling distribution, thus obtained, is ˆf (m) H (x).\n\n4.2 CONSTRUCTING DENSITY SKETCHES\n\nNow that we have reduced the problem of KDE approximation to histograms, we will now show how to obtain a compressed representation of a histogram in a streaming fashion. We also show how to generate samples from this representation. First, let us establish some notation we will use\n\nNotation: (1) Data D consists of n i.i.d samples of dimension d drawn from true distribution f (x) : S ⊂ Rd → R. (2) bin(x): ID of the partition in which point x falls. In the case of p-power LSH functions, bin(x) : Rd → N p, and each bin can be identified with a unique tuple of p integers. In a regular histogram, we have a tuple of d integers. For example, in regular histogram with width B, bin(x)i = (cid:98)xi/B(cid:99). bin is generally parameterised with bandwidth parameter B which measures the size of the partition. Some partitioning schemes and sampling algorithms are mentioned in Table 1 (3) A CS M with range R and repetitions K as described in section 3. (4) H: Augmented min-heap of size H used with M. Hence for a given partitioning scheme (bin, B), DS is parameterized by (K, R, H) and includes two data structures M(K, R) and H(H).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Overview of the sketching algorithm The histogram has an exponential (in d) number of partitions. Hence, in high dimensions, it is impractical to store histograms. However, most high dimensional real data is clustered and thus has highly sparse histograms. This does not help with histograms, as post-pruning of histograms still requires us to build and enumerate them. Nevertheless, the sparsity in histogram makes it a good candidate for heavy hitter problems. We use CS, M, to store a compressed version of the histogram. Unfortunately, sampling with just M does not have an efficient solution. We maintain a set of heavy partitions for sampling in the min-heap H. We will discuss sampling in later subsections.\n\nsketching M : As shown in figure 4 and algorithm 1, we process the data in a streaming fashion. For each data point, say x, we find the partition b = bin(x). We increment the count of b by 1 by inserting (b, 1) into M. Along with each insertion, we also update H. If the H is not at its capacity, we insert this b into the heap along with its updated count ˆC(b). If the heap is at its capacity, we check b’s updated count against the minimum of the H. If b’s count is greater, we pop the minimum element from the heap and insert (b, ˆC(b)).\n\n4.3\n\nˆfC(x): ESTIMATE OF DENSITY AT A POINT\n\nWe can use M for querying the density estimate at a particular point. The algorithm for querying is presented in Algorithm 2 and is explained in figure 2. Reusing notation from 3.1, the density predicted by the histogram can be written as ˆfH (x). When using the sketch, instead of actual C(bin(x)), we would use its estimate from M. Let this estimate be ˆC(bin(x)). Then we can write the density predicted using count-sketch as ˆfC(x)\n\nˆfH (x) =\n\nC(bin(x)) nV(bin(x))\n\nˆfC(x) =\n\nˆC(bin(x)) nV(bin(x))\n\nWe know from CS literature that ˆC(bin(x)) is closely distributed around C(bin(x)) and so we can expect ˆfC(x) to be close to ˆfH (x) and hence to f (x). Note that though ˆfC(x) is a good estimate of density at a point x, the function ˆfC(.) is not a density function as it does not integrate to 1.\n\nˆf ∗ C(x): ESTIMATE OF DENSITY FUNCTION\n\n4.4 To obtain a density function from the sketches, we have to normalize the function ˆfC(x) over the support. We can write ˆf ∗\n\nC(x) as\n\nˆf ∗ C(x) ∝ ˆC(x)\n\nˆf ∗ C(x) =\n\nˆC(x)) (cid:82) ˆC(x)dx\n\n=\n\nˆC(x)\n\nV(bin(x)) (cid:80)\n\nb∈bins(S)\n\nˆC(b)\n\n=\n\nˆC(x) V(bin(x))ˆn\n\nIt is easy to check the integral can be written as the sum over all the bins in the support. As is clear from the equations for ˆf ∗ ˆC(b) to get a density function. We can check that ˆn is an estimate of n using an estimate of count for each bin from the DS.\n\nb∈bins(S) C(b) , is replaced by ˆn = (cid:80)\n\nC(x) and ˆfC(x), n = (cid:80)\n\nb∈bins(S)\n\n4.5\n\nˆfS(x): SAMPLING FROM DENSITY SKETCHES\n\nM is a good enough representation for querying the density at a point. However, it is not the best data structure to generate samples efficiently. One naive way of sampling from these sketches is to\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Constructing density sketch\n\nof f (x)\n\nResult: Density Sketch (DS) f (x) : Rd → R : true distribution x1, . . . xn ∼ f (x) : sample drawn from f (x) bin(x) : S → N d: partition function M : CS with range R, repetitions K H(H) : min-heap to store top H partitions\n\nfor i ← 1 to n do b = binxi M.insert(b, 1) c = M.query(b) H.update(b, c)\n\nAlgorithm 2: query ˆfC(y), y ∈ Rd Result: ˆfC(y) y ∈ Rd b = biny c = M.query(b) return (c/(nV(b)))\n\nAlgorithm 3: sample y ∈ Rd such y ∼ ˆfS(x) Result: y: sample from fS(y) P : categorical distribution over bins s.t. P (b) = (H[b]/nh) if b ∈ H –\nP (b) = 0 if b /∈ H –\nb ∼ P y = UniformRandomPoint(b) return y\n\nrandomly select a point in support of f (x) and then do a rejection sampling using estimate ˆfC(x). However, given the enormous volume of support in high dimensions, this method is bound to be immensely inefficient. Another way is to choose a partition with probability proportional to the count of elements in that partition and then sample a random point from this chosen partition. It is easy to check that the probability of sampling a point x in this manner, precisely, is ˆfH (x) if we use exact counts and ˆf ∗ C(x) if we use approximate counts from CS. However, given that number of bins is exponential in dimension, sampling a bin proportional to its counts requires prohibitive memory and computation. This is why we needed a CS in the first place. Here, we further approximate the distribution by storing only top H partitions which contain most data points and discarding other partitions. As mentioned in 1, we can efficiently maintain top H partitions with an augmented heap H. We then sample a partition present in this heap with probability proportional to its count and sample a random data point from this partition (Algorithm 3). The probability of sampling a data point whose bin is not present augmented heap is then zero. The distribution of this sampling algorithm is,\n\nˆfS(x) = I(bin(x) ∈ H)\n\nˆC(bin(x)) ˆnhV(bin(x))\n\nb∈H\n\nC(x). Note that ˆfS(x) is a density function.\n\nwhere ˆnh = (cid:80) ˆC(b) is the count-sketch estimate of the total number of elements captured in all partitions present in the heap. I(.) is the indicator function with values 0 or 1 evaluating the boolean statement inside it. Let ρh = ˆnh/ˆn be the capture ratio of heap. It is easy to see that as the capture ratio tends to 1, ˆfS(x) tends to ˆf ∗ 5 ANALYSIS Histogram and Kernel Density Estimators are well-studied non-parametric estimators of density. Both of these estimators are shown to be capable of approximating a large class of functions (Scott, 2015). For example, with the condition of Lipschitz Continuity on f (x), we can prove that pointwise MSE( ˆfH (x) converges to 0 at a rate of O(n−2/3). Better results can be obtained for functions that have continuous derivatives. In our analysis, we make assumptions along those made in (Scott, 2015); specifically, the existence and boundedness of all function-dependent terms that appear in the theorems below. We refer the reader to (Scott, 2015) for an in-depth discussion on assumptions.\n\nWe restrict our analysis to convergence in probability for all the estimators discussed in this paper, which is the standard (Scott, 2015). In this section, we consider the regular histogram partitioning scheme and show that our sampling distribution ˆfS(x) is an approximation of underlying distribution f (x) and converge to it. However, a similar analysis holds even for random partitioning schemes / KDE and is skipped here.\n\nMean integrated square error(MISE): MISE of an estimator of function is a widely used tool to analyze the performance of a density estimator. (cid:20)(cid:90)\n\n(cid:21)\n\nMISE( ˆf ) = E\n\n( ˆf (x) − f (x))2dx\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nA density estimator, with MISE asymptotically tending to zero, is a consistent estimator of true density and converges to it in probability. We would use this tool to make statements about the convergence of our estimators. By Fubini’s theorem, MISE is equal to IMSE (Integrated mean square error).\n\nMISE( ˆf ) = IMSE( ˆf ) =\n\n(cid:90)\n\n(cid:104)\n\n(cid:105) ( ˆf (x) − f (x))2)\n\nE\n\ndx\n\nWe now present our main result of the paper, Theorem 1 (Main Theorem: ˆfS(x) to f (x) ). The probability density function of sampling, ˆfS(x), using a DS over regular histogram of width B, with parameters(K,R,H) created with n i.i.d samples from original density function f (x), has an IMSE given by\n\nIMSE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3(1 + 2(cid:15))\n\n(cid:18) 1\n\nnBd +\n\nG(f ) n\n\n+ o\n\n+ 3(1 + 3(cid:15))\n\n(cid:18) B2d 4\n\n(cid:19)\n\n(cid:18)\n\nG((cid:107)∇f (cid:107)2)\n\n+ 3(cid:15)\n\n1 + 2G(f ) + B\n\n(cid:19)\n\n(cid:18) 1 n\n(cid:90)\n\n√\n\nd\n\n(cid:19)\n\n+\n\nnnzp − 1 KRnBd\n\n(cid:19)\n\n(f (x)(cid:107)∇f (cid:107)2)\n\nx∈S\n\nwith probability (1 − δ) , where δ = nnzp (cid:15)2nKR , nnzp is the number of non-empty bins in histogram, ρh is the estimated capture ratio as described in section 4.5 and G(g) is roughness defined as (cid:82) g(x)2dx\n\nThe dependence of IMSE on properties of f (x), such as roughness, is standard (Scott, 2015) and cannot be avoided. Interpretation The estimator ˆfS(x) of f (x) is obtained by a series of approximations from f (x) → C(x) → ˆfS(x). Hence to interpret this result, we break down the result above ˆfH (x) → ˆfC(x) → ˆf ∗ into multiple theorems enabling the reader to easily notice which step of approximations leads to what terms in the theorem above. We provide these details in Appendix. We notice a few things from the theorem below\n\n• Similar to the standard analysis for histograms, the curse of dimensionality also manifests in our theorem. B should go to zero and n should increase faster than the rate at which Bd/nnzp decreases (condition 1). As compared to standard histograms, this requires n to grow faster. With these conditions on B and n, it is clear how the second and third terms go to zero.\n\n• The magnitude of the fourth term is controlled via (cid:15). The above statement is true for any δ and (cid:15) that are related via the expression δ = (cid:0)nnzp/((cid:15)2nKR)(cid:1). Choose arbitrarily small (cid:15) and δ, and we can achieve it with large enough n/nnzp or by providing more intermediate resources and making KR large enough. For a fixed resource KR, this term goes to zero asymptotically with n growing faster than nnzp, which is a sub-condition of condition 1.\n\n• The term 12(1 − ρh)2 shows the effect of truncation that occurs due to using only heavy partitions. As can be seen, this term is data dependent, and IMSE does not depend directly on H (number of partitions) but ρh. Suppose we can capture the entire data in the heap (i.e., setting H=nnzp), then the term adds no penalty to IMSE. H, via ρh controls the accuracy-memory trade-off of DS.\n\n6 DISCUSSION curse of dimensionality: As DS are built over Histograms, they inherit the curse from Histograms: i.e., the number of samples needed increases exponentially with dimension. With increased data collection, the issue of the unavailability of large amounts of data is fast vanishing. We want to emphasize that DS’ advantages are best seen when data is humongous. DS can absorb tons of data and give better density estimates and samples without increasing memory usage. Also, most real data in high dimensions is clustered or stays on a low-dimensional manifold. DS, throw away empty bins, and only store the histogram’s populated bins. DS can deal with the curse of dimensionality better than Histograms. DS on original data space: Some data types, like images, do not reside in a space where the usual distances or cosine similarities imply conceptual similarity. On these data types, DS will not perform well. One way is to learn a transformation and create sketches of the transformed data. While this will give better performance in practice, we might lose theoretical guarantees for certain transformations. 7 EXPERIMENTS Visualization of samples from density sketches: In the first set of experiments, we provide a sanity check for DS in the form of visualization of data generated from DS. (1) In the first experiment,\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) sample drawn from true distribution\n\n(b) sample drawn from Density Sketch\n\n(c) MNIST Sample from Density Sketch using L2LSH partitioning width: 0.01,0.1,1,10 from top to bottom\n\n(d) MNIST Sample from Density Sketch using conical partitions (multiple signed random projections (SRP)). The rows have varying number of SRPs used - increasing from top to bottom\n\nFigure 4: Visualization of samples drawn from Density Sketch. (a-b) DS captures density information. (c) Higher power LSH functions lead to fine DS behaving like random sample (d) Coarser partitions lead to samples that resemble “avg” of samples in data\n\nfigure 4 (a) shows the samples drawn from the actual multi-gaussian distribution, whereas figure 4 (b) shows the samples drawn from the DS built on the samples from the true distribution. In this experiment, we use L2-LSH partitions with a B=0.25, K=3, R=50000, and H=3000. As can be seen, the two samples are indistinguishable. So DS does capture the density information. (2) Figure 4 (c) shows some samples drawn from DS built over the MNIST dataset (Chang & Lin, 2011) with varying bin-width sizes. (again R=50000, K=3 and H=5000) . We should notice that MNIST with 784 dimensions and 60K samples is not an ideal dataset for DS. In fact, with L2-LSH partitions the data would be so scattered that every sample is contained in its bin. If we make the bin-width finer, we should sample data points very close to the random sample from the original data. So in the worst conditions, DS converges to a random sample which we know is a good representation of data. 4 (d) shows results again with MNIST. However, here we have created conical partitions (created using multiple signed random projections). While L2-LSH partitions use power 784 L2-LSH functions, in this experiment, we use a smaller number of SRP functions(10-25, increases as we go to lower rows in the image), thus promoting clustering. As expected, this coarse partitioning does show significant clustering; hence, the images drawn from the partitions look like the average of multiple samples from the original data. The results support that DS can create samples that resemble the original data.\n\nEvaluation of Samples on Classification Tasks: For most datasets, it is not possible to inspect samples visually. Hence we evaluate the quality of samples from DS by using them to train classification models. In these experiments, the data loader of the training algorithm is replaced with a sampler from DS. This sampler returns a training batch when requested by the algorithm. All the experiments are performed on Tesla P-100 GPUs with 16GB memory. Datasets: We choose big datasets from the liblinear website (Chang & Lin, 2011), which satisfy the constraints of 1) data dimension less than 100 and 2) the number of samples per class greater than 1,000,000. Large Datasets is the main application domain for DS. Thus, we have datasets of Higgs (10M samples, 28 dimensions) and Susy (5M samples, 18 dimensions) for our experiments. Baselines: For baselines, we consider random samples of the same size and Liberty coresets proposed by (Karnin & Liberty, 2019) to compare DS performance. For Liberty coresets, we use m = 100 as for larger m the process is very slow. Dimensionality reduction via random projections is another streaming algorithm. Still, in these datasets, we cannot get significant compression using\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) higgs test accuracy\n\n(b) higgs test loss\n\n(c) susy test accuracy\n\n(d) susy test loss\n\n(e) varying K,R, with KR=1000000\n\n(f) varying B\n\nFigure 5: (a-d) Performance of density sketches, Liberty coresets and random sample on downstream classification task. The bar widths refer to 2 × std. DS performs consistently better at all memory sizes (e) Performance of DS is stable with K,R configs for decent budget KR=1000000, (f) Optimal B exists for DS performance. larger and smaller values of B lead to performance degradation\n\ndimensionality reduction (it can be d× at best, where d is the dimension). So we cannot compare against this method. For a more detailed discussion on baselines, refer to appendix D. Results: The DS has parameters: partition function(bandwidth B). We use L2-LSH partitions, sketch parameters K, R, and heap parameter H in all our experiments. The memory of the DS used for sampling is affected by only the heap parameter (see appendix F for details on memory computation). In figure 5(f), we use the config (K=4,R=250000,H=100000) and vary B. It is clear from the figure that B=0.001 works best for these datasets. Lower and higher values of B affect the performance adversely. Larger B implies that we will capture more space than needed in a single partition, and smaller B implies that we will capture lesser data in the heap. So it is expected that a sweet spot for B exists. In figure 5(e), we fix (B=0.01,H=100000, KR=1000000) and vary K from 1 to 64. We can see that for a reasonable memory budget the results are stable with varying K. For the experiments in figure 5(a-d) we fix (B=0.01, K=5, R=250000) and vary H. This gives us DS of different sizes. We plot test accuracy and losses for DS, random samples and Liberty coresets for different sizes of memory used. The width of the band signifies the 2 × std-dev of performance on three independent runs. As can be seen, for the “Higgs” dataset, the model’s accuracy achieved on original data of size 2.5GB can be closely reached by using a DS of size 50MB. So we get around 50x compression ! We see similar results for datasets of “Susy” (100x compression, 0.8GB) as well. The results show that DS is much more informative than Random Sample and Liberty Coresets. For more details on running the experiments (data processing, memory measurements, etc.), refer to Appendix F.\n\nEstimation of statistical properties of dataset: We also perform the experiments on the covariance estimation task. The observations are similar to the classification experiment. DS performs better than the corresponding random sample. The results are presented in Appendix F for the shortage of space.\n\n8 CONCLUSION\n\nIn this paper, we talk about Density Sketches, a streaming algorithm to construct a summary of density distribution from data. We show that new samples generated from this sketch asymptotically converge to the underlying distribution. Thus, DS comes with theoretical guarantees. Additionally, the cheap nature of online updates in Density Sketches, makes it an attractive alternative to constructing coresets for the data. In terms of coresets, DS can be viewed as a compressed form of randomized grid-coresets - one of the oldest forms of coresets.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary\n\ncoins. Journal of computer and System Sciences, 66(4):671–687, 2003.\n\nClaude JP B ́elisle, H Edwin Romeijn, and Robert L Smith. Hit-and-run algorithms for generating\n\nmultivariate distributions. Mathematics of Operations Research, 18(2):255–266, 1993.\n\nAmit Chakrabati. https://www.cs.dartmouth.edu/∼ac/ Teach/data-streams-lecnotes.pdf, volume 1. 2020.\n\nChih-Chung Chang and Chih-Jen Lin. Libsvm : a library for support vector machines. In Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm. ACM Transactions on Intelligent Systems and Technology, 2011.\n\nMoses Charikar and Paris Siminelakis. Hashing-based-estimators for kernel density in high dimensions. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pp. 1032–1043. IEEE, 2017.\n\nMoses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In International Colloquium on Automata, Languages, and Programming, pp. 693–703. Springer, 2002.\n\nYuansi Chen, Raaz Dwivedi, Martin J Wainwright, and Bin Yu. Vaidya walk: A sampling algorithm based on the volumetric barrier. In 2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 1220–1227. IEEE, 2017.\n\nYutian Chen, Max Welling, and Alex Smola. Super-samples from kernel herding. arXiv preprint\n\narXiv:1203.3472, 2012.\n\nBenjamin Coleman and Anshumali Shrivastava. Sub-linear race sketches for approximate kernel density estimation on streaming data. In Proceedings of The Web Conference 2020, pp. 1739– 1749, 2020.\n\nGraham Cormode and M Muthukrishnan. Count-min sketch., 2009.\n\nEfren Cruz Cortes and Clayton Scott. Sparse approximation of a kernel mean. IEEE Transactions\n\non Signal Processing, 65(5):1310–1323, 2016.\n\nTrevor Darrell, Piotr Indyk, and Gregory Shakhnarovich. Nearest-neighbor Methods in Learning\n\nand Vision: Theory and Practice. MIT Press, 2005.\n\nJerome Friedman, Trevor Hastie, Robert Tibshirani, et al. The elements of statistical learning,\n\nvolume 1. Springer series in statistics New York, 2001.\n\nIan Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.\n\nMIT press Cambridge, 2016.\n\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint arXiv:1406.2661, 2014.\n\nZohar Karnin and Edo Liberty. Discrepancy, coresets, and sketches in machine learning. In Confer-\n\nence on Learning Theory, pp. 1975–1993. PMLR, 2019.\n\nJeff M Phillips and Wai Ming Tai. Improved coresets for kernel density estimates. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 2718–2727. SIAM, 2018.\n\nJeff M Phillips and Wai Ming Tai. Near-optimal coresets of kernel density estimates. Discrete &\n\nComputational Geometry, 63(4):867–887, 2020.\n\nDavid W Scott. Multivariate density estimation: theory, practice, and visualization. John Wiley &\n\nSons, 2015.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDW Scott and SR Sain. Multi-dimensional density estimation handbook of statistics vol 23 data\n\nmining and computational statistics ed cr rao and ej wegman, 2004.\n\nZhipeng Wang and David W Scott. Nonparametric density estimation for high-dimensional data—algorithms and applications. Wiley Interdisciplinary Reviews: Computational Statistics, 11(4):e1461, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2\n\nDescription true density distribution histogram estimate of f(x) partition id of point x enumerates all bins in the support S count of elements in the bin id b count of elements in the bin(x) estimate count of elements in the bin id b volume of bin id b data kernel function : R × R → R B ∈ R, scalar bin width B ∈ Rd vector of bin widths in different directions U{0, ...R − 1} hash function U → {−1, +1} hash function\n\nNotation f (x) ˆfH (x) bin(x) bins(S) C(b) C(x) ˆC(b) V(b) D\nk(, ) B\nB h(x) g(x) W ∈ Rk×d weight matrix for SRP / L1-L2 LSH I\nE ˆfφ(x) M\nK, R n\nˆn nh ˆnh ρh\n\nindicator function Expected value KDE estimate with kernel φ Count sketch parameters of count sketch total number elements CS estimate of total number of elements number of elements in the heap CS estimate of number of elements in the heap capture ratio defined by ˆnh/ˆn\n\nA APPENDIX\n\nA.1 NOTATION\n\nA.2 THEOREM 2 : fH TO f\n\nWhile estimating true distribution f (x) : Rd → R, the integrated mean square error (IMSE) for the estimator ˆfH (x) using regular histogram with width h and number of samples n, is\n\nIMSE( ˆfH ) ≤\n\n1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)\n\nSpecifically, we have integrated variance (IV) and integrated square bias (ISB) as follows\n\nIV( ˆfH ) =\n\n1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\nand\n\nISB( ˆfH ) ≤\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)\n\nwhere G(φ) is the roughness of the function φ defined as G(φ) = (cid:82) φ2(x)dx\n\nProof. Let x ∈ S where S is the support of the distribution. Let bin(x) determine the bin of point x, bins(S) enumerate all the bins that lie inside the support S of the distribution f (x) . Let V (bin(x)) is volume of bin in which x lies. Equivalently, we can also use V (b) to denote volume of bin b. For standard histogram, V (b) = hd\n\nˆfH (x) =\n\n1 nV (bin(x))\n\nn (cid:88)\n\ni=1\n\n12\n\nI(xi ∈ bin(x))\n\n(1)\n\nUnder review as a conference paper at ICLR 2023\n\nFirst let us consider the integrated variance.\n\n(cid:90)\n\nIV =\n\nx∈S\n\nVar( ˆfH (x))dx =\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(S)\n\nx∈b\n\nVar( ˆfH (x))dx\n\n(2)\n\nFor a particular bin b, the variance is constant at all values of x inside it. Also for a particular x in bin b, we can write the following for Var( ˆfH (x)) using independence of samples.\n\nVar( ˆfH (x)) =\n\n1\n\nnV (bin(x))2 Var(I(xi ∈ bin(x))\n\n(3)\n\nAlso Var(I(xi ∈ b)) = pb(1 − pb) where pb is the probability of xi lying in bin b. That is, pb = (cid:82)\n\nx∈b f (x)dx Using this in equation 2\n\nSimplifying,\n\n(cid:88)\n\nIV =\n\nb∈bins(S)\n\nV (b)\n\n1 nV 2(b)\n\npb(1 − pb)\n\n(cid:88)\n\nIV =\n\nb∈bins(S)\n\n1 nV (b)\n\npb(1 − pb)\n\nFor standard histogram V (b) is same across bins,\n\nIV =\n\n1 nV (b)\n\n(cid:88)\n\n(\n\npb −\n\n(cid:88)\n\np2 b)\n\nb∈bins(S)\n\nb∈bins(S)\n\n=\n\n1 nV (b)\n\n(1 −\n\n(cid:88)\n\np2 b)\n\nb∈bins(S)\n\nUsing mean value theorem, we can write, pb = V (b)f (ξb) for some point ξb ∈ b.\n\n(cid:88)\n\np2\n\nb =\n\n(cid:88)\n\nb∈bins\n\nb∈bins\n\nV (b)2f (ξb)2 = V (b)\n\n(cid:88)\n\nb∈bins\n\nV (b)f (ξb)2\n\nUsing Riemann Integral approximation , we can write the following as the bin size reduces,\n\n(cid:88)\n\nb∈bins\n\nV (b)f (ξb)2 =\n\n(cid:90)\n\nx∈S\n\nf 2(x)dx + o(1)\n\n(cid:82)\n\nx∈S f 2(x)dx is also known as the roughness of the function. Let us denote it using G(f ). Hence\n\nPutting V (b) = hd\n\nIV =\n\n1 nV (b)\n\n(1 − V (b) (G(f ) + o(1)))\n\nIV =\n\n1 nV (b)\n\n−\n\nG(f ) n\n\n− o\n\n(cid:19)\n\n(cid:18) 1 n\n\nIV =\n\n1\n\nnhd −\n\nG(f ) n\n\n− o\n\n(cid:19)\n\n(cid:18) 1 n\n\nKeeping only the leading term in the above expression, (cid:18) 1 nhd\n\nIV = O\n\n(cid:19)\n\nNow let us look at the ISB for this estimator, ISB( ˆfH (x))\n\nISB( ˆfH (x)) =\n\n(cid:90)\n\nx∈S\n\n(E( ˆfH (x) − f (x)))2dx\n\n13\n\n(10)\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\n(4)\n\n(5)\n\n(6)\n\n(7)\n\n(8)\n\n(9)\n\nUnder review as a conference paper at ICLR 2023\n\nLet us look at the expected value of the estimator,\n\nE( ˆfH (x)) =\n\n1 V (bin(x))\n\n(cid:90)\n\nt∈bin(x)\n\nf (t)dt\n\n(15)\n\nRecall that x ∈ Rd. Using 2nd order multivariate taylor series expansion of this f (t) around x, we get,\n\nf (t) = f (x) + (cid:104)t − x, ∇f (x)(cid:105) +\n\n1 2\n\n(t − x)(cid:62)H(f (x))(t − x)\n\n(16)\n\nHere H(f (t)) is the hessian of f at t. Without the loss of generality let us look at the bin(x) = [0, h]d that is the bin at the origin. Let us say it is b0 (cid:90)\n\nf (t)dt = hdf (x) + hd(cid:104)(\n\n− x, ∇f (x)(cid:105) + O(hd+2)\n\n(17)\n\nh 2\n\nUsing eq 17 in eq 15, we get\n\nt∈b0\n\nE( ˆfH (x)) = f (x) + (cid:104)(\n\nh 2\n\n− x), ∇f (x)(cid:105) + O(h2)\n\nHence, just keeping the leading term , we have\n\nBias( ˆfH (x)) = (cid:104)(\n\nh 2\n\n− x), ∇f (x)(cid:105)\n\nNow,\n\n(cid:90)\n\nx∈b0\n\nBias( ˆfH (x))2dx =\n\n(cid:90)\n\n(cid:18) (cid:104)\n\n(cid:18) h 2\n\n(cid:19)\n\n(cid:19)2\n\n− x\n\n, ∇f (x)(cid:105)\n\ndx\n\nUsing Cauchy-Schwarz inequality, we get (cid:90)\n\nBias( ˆfH (x))2dx ≤\n\nx∈b0\n\nx∈b0\n\nx∈b0\n\n(cid:90)\n\n(cid:107)(\n\nh 2\n\n− x)(cid:107)2\n\n2(cid:107)∇f (x)(cid:107)2\n\n2dx\n\n√\n\nAs [h/2, h/2, ...h/2] is a mid point of the bin. The max norm of x − h/2 can be h\n\n(cid:90)\n\nx∈b0\n\nBias( ˆfH (x))2dx ≤\n\nh2d 4\n\n(cid:90)\n\nx∈b0\n\n(cid:107)∇f (x)(cid:107)2\n\n2dx\n\nNow looking at ISB\n\nISB( ˆfH ) =\n\n(cid:90)\n\n(cid:88)\n\nb∈bins\n\nx∈b0\n\nBias( ˆfH (x))2dx ≤\n\nh2d 4\n\n(cid:90)\n\nx∈S\n\n(cid:107)∇f (x)(cid:107)2\n\n2dx\n\nISB( ˆfH ) ≤\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n(24)\n\nd/2\n\nA.3 THEOREM 3: fC TO fH\n\nWhile estimating true distribution f (x) : Rd → R, the integrated mean square error (IMSE) for the estimator ˆfC(x) using regular histogram with width h, number of samples n, and countsketch with range R, repetitions K and mean recovery, is\n\nwhere nnzp is the number of non-zero partitions. Specifically, we have\n\nIMSE( ˆfC) = IMSE( ˆfH ) +\n\nnnzp KRnhd\n\nand\n\nIV( ˆfC) = IV( ˆfH ) +\n\nnnzp − 1 KRnhd\n\nISB( ˆfC) = ISB( ˆfH )\n\nwhere nnzp is the number of non-zero bins/partitions.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Consider a Countsketch with range R and just one repetition (i.e. K = 1). Let it be parameterized by the randomly drawn hash functions g : bins(S) −→ {0, 1, 2, ..., R − 1} and s : bins(S) −→ {−1, +1}. Let C(bin(x)) (cid:80)n i=1(I(xi ∈ bin(x)) is the count of elements that lie inside the bin(x)\n\nThe estimate of density at point x can then be written as\n\nˆfC(x) =\n\n1 nV (bin(x))\n\n(cid:32)\n\nC(bin(x)) +\n\nn (cid:88)\n\n(cid:16)\n\nI\n\ni=1\n\nxi /∈ bin(x) ∧ g(bin(xi)) = g(bin(x))\n\n(cid:17)\n\n(cid:33)\n\ns(bin(xi))s(bin(x))\n\n(25)\n\nWe can rewrite this as ,\n\nˆfC(x) = ˆfH (x)+\n\n1 nV (bin(x))\n\n(cid:32) n\n\n(cid:88)\n\n(cid:16)\n\nI\n\ni=1\n\nxi /∈ bin(x) ∧ g(bin(xi)) = g(bin(x))\n\n(cid:17)\n\ns(bin(xi))s(bin(x))\n\n(cid:33)\n\nAs E(s(b)) = 0, it can be clearly seen that.\n\nHence, it follows that\n\nE( ˆfC(x)) = E( ˆfH (x))\n\nISB( ˆfC(x)) = ISB( ˆfH (x))\n\nIt can be checked that each of the terms in the summation for right hand side of equation 26 including the terms in ˆfH (x) are independent to each other . i.e. covariance between them is zero. Hence we can write the variance of our estimator as,\n\nVar( ˆfC(x)) = Var( ˆfH (x))+\n\n1 nV 2(bin(x))\n\n(cid:16)\n\n(cid:16)\n\nI\n\nVar\n\nxi /∈ bin(x) ∧ g(bin(xi))=g(bin(x))\n\n(cid:17)\n\ns(bin(xi))s(bin(x))\n\n(cid:17)\n\nVar( ˆfC(x)) = Var( ˆfH (x))+\n\n1 nV 2(bin(x))\n\nE\n\n(cid:18)\n\nI\n\n(cid:16)\n\nxi /∈ bin(x) ∧ g(bin(xi))=g(bin(x))\n\n(cid:17)2(cid:19)\n\nAs square of indicator is just the indicator,\n\nVar( ˆfC(x)) = Var( ˆfH (x))+\n\n1 nV 2(bin(x))\n\n(cid:16)\n\n(cid:16)\n\nI\n\nE\n\nxi /∈ bin(x) ∧ g(bin(xi))=g(bin(x))\n\n(cid:17)(cid:17)\n\nV ar( ˆfC(x)) = V ar( ˆfH (x)) +\n\n1 nV 2(bin(x))\n\n(1 − pbin(x))\n\n1 R\n\n)\n\nHence, IV is\n\nIV( ˆfC(x)) = IV( ˆfH (x)) +\n\n(cid:90)\n\nx∈S\n\nIV( ˆfC(x)) = IV( ˆfH (x)) +\n\n1 nV 2(bin(x)) 1\nnV 2(b)\n\n(cid:90)\n\nx∈b\n\n(cid:88)\n\nb∈bins(S)\n\n(1 − pbin(x))\n\n1 R\n\n)\n\n(1 − pb)\n\n1 R\n\n)\n\nIV( ˆfC(x)) = IV( ˆfH (x)) +\n\n(cid:88)\n\nb∈bins(S)\n\n1 nV (b)\n\n(1 − pb)\n\n1 R\n\n)\n\n15\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\nUnder review as a conference paper at ICLR 2023\n\nAssuming standard partitions. V (b) = hd for all b\n\nIV( ˆfC(x)) = IV( ˆfH (x)) +\n\n1 nhd\n\n(nnzp − 1) R\n\nWith mean recovery, with K repetitions, the analysis can be easily extended to get IV as\n\nIV( ˆfC(x)) = IV( ˆfH (x)) +\n\n1 nhd\n\n(nnzp − 1) KR\n\n(39)\n\n(40)\n\nThe ISB remains same in this case.\n\nA.4 THEOREM 4: ˆf ∗\n\nC TO ˆfC\n\nWhile estimating true distribution f (x) : Rd → R, the IMSE for the estimator ˆf ∗ C(x) using regular histogram with width h and number of samples n and countsketch with parameters (R:range, K:repetitions), is related to the estimator ˆfC(x) as follows\n\nIMSE( ˆfC(x)) − (cid:15)(N + 2M ) ≤ IMSE( ˆf ∗\n\nC(x)) ≤ IMSE( ˆfC(x)) + (cid:15)(N + 2M )\n\nSpecifically,\n\nand\n\nwhere\n\nIV( ˆfC(x)) − 2(cid:15)M ≤ IV( ˆf ∗\n\nC(x)) ≤ IV( ˆfC(x)) + 2(cid:15)M\n\nISB( ˆfC(x)) − (cid:15)N ≤ ISB( ˆf ∗\n\nC(x)) ≤ ISB( ˆfC(x)) + (cid:15)N\n\nM ≤ IV( ˆfC(x)) + 2(G(f ) +\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\nh2d 4\n\n(cid:90)\n\nd\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2))\n\nN = (1 + ISB( ˆfC(x)))\n\nwith probability (1 − δ) where δ = nnzp\n\n(cid:15)2nKR\n\nProof. Let us look at the estimator\n\nˆf ∗ C(x) =\n\nˆC(bin(x)) V(bin(x)) (cid:80)\n\nb\n\nˆC(b)\n\n= ˆfC(x) ∗\n\nn ˆn\n\n(41)\n\nwhere ˆn = (cid:80)\n\nb\n\nˆC(b) and n = (cid:80)\n\nb C(b)\n\nˆn and its relation to n: Let us first analyse ˆn and how it is related to n.\n\nˆn =\n\n(cid:88)\n\nˆC(b) =\n\n(cid:88)\n\nn (cid:88)\n\n(cid:16)\n\nI(xi ∈ b) +\n\n(cid:16)\n\nI(cid:0)xi /∈ b ∧ g(bin(xi)(cid:1) == g(b))s(bin(xi))s(b)\n\n(cid:17)(cid:17)\n\nb∈bins(S)\n\nb∈bins(S)\n\ni=1\n\nˆn =\n\n(cid:88)\n\nb,i\n\nI(xi ∈ b) + I(xi /∈ b ∧ g(bin(xi)) == g(b))s(bin(xi))s(b)\n\n(42)\n\n(43)\n\nNote that E(ˆn) = n. For varaince, observe that most of the terms in the summation have covariance 0, except the terms Cov(I(xi ∈ b1), I(xi ∈ b2)) which are negatively correlated. Hence\n\nV ar(ˆn) =\n\n(cid:88)\n\nb,i\n\nV ar(I(xi ∈ b)) + V ar(I(xi /∈ b ∧ g(bin(xi))! = g(b))s(bin(xi))s(b))+\n\n(cid:88)\n\n2\n\ni,b1,b2,b1(cid:54)=b2\n\nCov(I(xi ∈ b1), I(xi ∈ b2))\n\nWe know that\n\nV ar(I(xi ∈ b)) = pb(1 − pb)\n\n(44)\n\nV ar(I(xi /∈ b ∧ g(bin(xi)) == g(b))s(bin(xi))s(b)) = E(I(xi /∈ b ∧ g(bin(xi))! = g(b))2) =\n\n1 − pb R\n\nCov(I(xi ∈ b1), I(xi ∈ b2)) = −pb1pb2\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nHence, we pluggin in the values in previous equation ,\n\nV ar(ˆn) = n\n\n(cid:88)\n\nb\n\npb(1 − pb) + n\n\n1 − pb R\n\n(cid:88)\n\nb\n\n− 2n\n\n(cid:88)\n\npb1 pb2\n\nb1,b2,b1(cid:54)=b2\n\n(cid:88)\n\n− 2n\n\npb1 pb2\n\nb1,b2 (cid:88)\n\npb1pb2 )}\n\nV ar(ˆn) = n(1 −\n\nV ar(ˆn) = n{(1 +\n\n(cid:88)\n\nb\n\n(cid:88)\n\nb\n\np2\n\nb) + n\n\n1 − pb R\n\n(cid:88)\n\nb\n\n1 − pb R\n\n(cid:88)\n\n− (\n\np2\n\nb) − 2n\n\nV ar(ˆn) = n{(1 +\n\n(cid:88)\n\nb\n\nb 1 − pb R\n\nb1,b2\n\n(cid:88)\n\n− (\n\npb)2}\n\nb\n\nV ar(ˆn) = n{\n\n1 − pb R\n\n}\n\n(cid:88)\n\nb\n\nV ar(ˆn) =\n\nn(nnzp − 1) R\n\n<\n\nn(nnzp) R\n\nUsing Chebyshev’s inequality , we have\n\nP (|ˆn − n| > (cid:15)n) ≤\n\nV ar(ˆn) (cid:15)2n2\n\nHence with probability (1 − δ), δ = nnzp\n\n(cid:15)2nR , ˆn is within (cid:15) multiplicative error.\n\nP (|ˆn − n| > (cid:15)n) ≤\n\nnnzp (cid:15)2nR\n\nrelation of pointwise Bias and ISB With probability 1 − δ, ˆfC(x) 1 − (cid:15)\n\nˆfC(x) 1 + (cid:15)\n\nC(x) ≤\n\n≤ ˆf ∗\n\nAs expectations respect inequalities\n\nE( ˆfC(x)) 1 + (cid:15)\n\n≤ E( ˆf ∗\n\nC(x)) ≤\n\nE( ˆfC(x)) 1 − (cid:15) E( ˆfC(x)) 1 − (cid:15)\n\nE( ˆfC(x)) 1 + (cid:15)\n\n− f (x) ≤ Bias( ˆf ∗\n\nC(x)) ≤\n\n− f (x)\n\nBias( ˆfC(x)) − (cid:15)f (x) 1 + (cid:15) Bias( ˆfC(x)) − (cid:15)f (x) 1 + (cid:15)\n\n≤ Bias( ˆf ∗\n\nC(x)) ≤\n\n≤ Bias( ˆf ∗\n\nC(x)) ≤\n\nBias( ˆfC(x)) + (cid:15)f (x) 1 − (cid:15) Bias( ˆfC(x)) + (cid:15)f (x) 1 − (cid:15)\n\nIntegrating expressions again respects inequalities\n\nISB( ˆfC(x)) − (cid:15) (cid:82) f (x) 1 + (cid:15)\n\n≤ ISB( ˆf ∗\n\nC(x)) ≤\n\nISB( ˆfC(x)) + (cid:15) (cid:82) f (x) 1 − (cid:15)\n\nISB( ˆfC(x)) − (cid:15) 1 + (cid:15)\n\n≤ ISB( ˆf ∗\n\nC(x)) ≤\n\nISB( ˆfC(x)) + (cid:15) 1 − (cid:15)\n\nUsing first order taylor expansion of\n\n1\n\n1+(cid:15) and ignore square terms\n\n(45)\n\n(46)\n\n(47)\n\n(48)\n\n(49)\n\n(50)\n\n(51)\n\n(52)\n\n(53)\n\n(54)\n\n(55)\n\n(56)\n\n(57)\n\n(58)\n\n(59)\n\n(1 − (cid:15))ISB( ˆfC(x)) − (cid:15) ≤ ISB( ˆf ∗\n\nISB( ˆfC(x)) − (cid:15)(1 + ISB( ˆfC(x))) ≤ ISB( ˆf ∗ Hence,\n\nC(x)) ≤ (1 + (cid:15))ISB( ˆfC(x)) + (cid:15)\n\n(60) C(x)) ≤ ISB( ˆfC(x)) + (cid:15)(1 + ISB( ˆfC(x))) (61)\n\nISB( ˆfC(x)) − (cid:15)N ≤ ISB( ˆf ∗\n\nC(x)) ≤ ISB( ˆfC(x)) + (cid:15)N\n\n(62)\n\nwhere\n\nN = (1 + ISB( ˆfC(x)))\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nPoint wise variance and IV Using the similar arguments\n\nE( ˆf 2 C(x)) (1 + (cid:15))2 −\n\nE2( ˆfC(x)) (1 − (cid:15))2 ≤ V ar( ˆf ∗\n\nC(x)) ≤\n\nE( ˆf 2 C(x)) (1 − (cid:15))2 −\n\nE2( ˆfC(x)) (1 + (cid:15))2\n\n(63)\n\nAgain making first order taylor expansions of denominator and ignoring square terms\n\nV ar( ˆfC(x))−2(cid:15)(E( ˆf 2\n\nC(x)+E2( ˆfC(x))) ≤ V ar( ˆf ∗\n\nC(x)) ≤ V ar( ˆfC(x))+2(E( ˆf 2\n\nC(x)+E2( ˆfC(x)))\n\n(64)\n\nSince, V ar( ˆfC(x)) = E( ˆf 2\n\nC(x)) − E2( ˆfC(x))\n\nV ar( ˆfC(x))−2(cid:15)(V ar( ˆfC(x))+2E2( ˆfC(x))) ≤ V ar( ˆf ∗\n\nC(x)) ≤ V ar( ˆfC(x))+2(cid:15)(V ar( ˆfC(x))+2E2( ˆfC(x)))\n\nIV ( ˆfC(x))−2(cid:15)(IV ( ˆfC(x))+2\n\n(cid:90)\n\nx∈S\n\nE2( ˆfC(x))) ≤ IV ( ˆf ∗\n\nLet us now figure out the (cid:82)\n\nx∈S E2( ˆfC(x))\n\n(65) C(x)) ≤ IV ( ˆfC(x))+2(cid:15)(IV ( ˆfC(x))+2 (66)\n\n(cid:90)\n\nx∈S\n\nE2( ˆfC(x)))\n\n(cid:90)\n\nx∈S\n\nE2( ˆfC(x)) =\n\n(cid:90)\n\nx∈S\n\nE2( ˆfH (x))\n\n(67)\n\nFrom equation 18, E( ˆfH (x))2 = f (x)2 + ((cid:104)( h\n\n2 − x), ∇f (x)(cid:105))2 + 2f (x)(cid:104)( h\n\n2 − x), ∇f (x)(cid:105)\n\n(cid:90)\n\nx∈S\n\nE2( ˆfH (x)) ≤ G(f ) +\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\nh2d 4\n\n(cid:90)\n\nd\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2)\n\n(68)\n\nHence,\n\nWhere\n\nIV ( ˆfC(x)) − 2(cid:15)M ≤ IV ( ˆf ∗\n\nC(x)) ≤ IV ( ˆfC(x)) + 2(cid:15)M\n\n(69)\n\nM ≤ IV ( ˆfC(x)) + 2(G(f ) +\n\nG((cid:107)∇f (cid:107)2)) + h\n\n√\n\nh2d 4\n\n(cid:90)\n\nd\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2))\n\n(70)\n\nA.5 LEMMA 1\n\nEstimators ˆfS(x) and ˆf ∗ togram of width h built over n i.i.d samples drawn from true distribution have a relation\n\nC(x), obtained from the Density Sketch with parameters(R,K,H) using his-\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = 2(1 − ρh)\n\nwhere ρh is the capture ratio as defined in section 3\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n(cid:90)\n\n(cid:88)\n\nb∈bins\n\nx∈b\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx\n\n(71)\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(H)\n\nx∈b\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx +\n\n(cid:88)\n\n(cid:90)\n\nb /∈bins(H)\n\nx∈b\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx\n\n(72)\n\nwe know that for x ∈ b, b /∈ bins(H), ˆfS(x) = 0. Hence,\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(H)\n\nx∈b\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx +\n\n(cid:88)\n\n(cid:90)\n\nb /∈bins(H)\n\nx∈b\n\nˆf ∗ C(x)dx\n\n(73)\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:82)\n\nx∈b\n\nC(x)dx is the probability of a data point lying in that bucket according to ˆf ∗ ˆf ∗\n\nC(x)\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(H)\n\nx∈b\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx +\n\n(cid:88)\n\nb /∈bins(H)\n\nFor points x ∈ b, b ∈ bins(H), ˆf ∗\n\nC(x) ∗ ˆn = ˆfS(x) ∗ ˆnh, Hence, ˆfS(x) = ˆn\n\nˆnh\n\n(cid:90)\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx =\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(H)\n\nx∈b\n\nˆf ∗ C(x)(\n\n(cid:88)\n\n(cid:90)\n\nb∈bins(H)\n\nx∈b\n\nˆf ∗ C(x)(\n\nˆn ˆnh\n\nˆn ˆnh\n\n− 1)dx +\n\nb /∈bins(H)\n\n− 1)dx +\n\n(cid:88)\n\nb /∈bins(H)\n\nˆf ∗ C(x)\n\n(cid:88)\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = (\n\nˆn ˆnh\n\n− 1)\n\n(cid:88)\n\nb∈bins(H)\n\nˆcb ˆn\n\n+\n\n(cid:88)\n\nb /∈bins(H)\n\nˆcb ˆn\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = (\n\nˆn ˆnh\n\n− 1)(\n\nˆnh ˆn\n\n) +\n\nˆn − ˆnh ˆn\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = (1 −\n\nˆnh ˆn\n\n) +\n\nˆn − ˆnh ˆn\n\n(cid:90)\n\n(cid:90)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = 2(1 −\n\nˆnh ˆn\n\n)\n\n| ˆf ∗\n\nC(x) − ˆfS(x)|dx = 2(1 − ρh)\n\nˆcb ˆn\n\nˆcb ˆn\n\nˆcb ˆn\n\n(74)\n\n(75)\n\n(76)\n\n(77)\n\n(78)\n\n(79)\n\n(80)\n\n(81)\n\nA.6 THEOREM 5\n\nThe IMSE of estimator ˆfS(x) obtained from the Density Sketch with parameters(R,K,H) using histogram of width h built over n i.i.d samples drawn from true distribution f(x) is\n\nIM SE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3IM SE( ˆf ∗\n\nC(x))\n\nwhere ρh is the capture ratio as defined in\n\nProof. Giving a very loose relation between ˆfS and f. We can write\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n( ˆfS(x) − f (x))2dx =\n\n(cid:90)\n\n(( ˆfS(x) − ˆf ∗\n\nC(x)) − ( ˆf ∗\n\nC(x) − f (x)))2dx\n\n( ˆfS(x) − f (x))2dx ≤ 3\n\n(cid:90)\n\n( ˆfS(x) − ˆf ∗\n\nC(x))2dx + 3\n\n(cid:90)\n\n( ˆf ∗\n\nC(x) − f (x))2dx\n\n( ˆfS(x) − f (x))2dx ≤ 3(\n\n(cid:90)\n\n|( ˆfS(x) − ˆf ∗\n\n(cid:90)\n\n( ˆf ∗\n\nC(x) − f (x))2dx\n\nC(x))|dx)2 + 3 (cid:90)\n\n( ˆfS(x) − f (x))2dx ≤ 12(1 − ρh)2 + 3\n\n( ˆf ∗\n\nC(x) − f (x))2dx\n\nIM SE = M ISE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3IM SE( ˆf ∗\n\nC(x))\n\n19\n\n(82)\n\n(83)\n\n(84)\n\n(85)\n\n(86)\n\nUnder review as a conference paper at ICLR 2023\n\nB THEOREM 1 (MAIN THEOREM) COMBINES ALL OTHER THEOREMS\n\nThis theorem directly relates the distribution ˆfS(x) to the true distribution f(x). We will combine the following statements\n\nIMSE( ˆfH ) ≤\n\n1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)\n\nIMSE( ˆfC(x)) = IMSE( ˆfH (x)) +\n\nnnzp KRnhd\n\n|IMSE( ˆf ∗\n\nC(x)) − IMSE( ˆfC(x))| ≤ (cid:15)(N + 2M ) with probability (1 − δ), δ =\n\nnnzp (cid:15)2nR\n\nwhere,\n\nIMSE( ˆfS) ≤ 12(1 − ρh)2 + 3IMSE( ˆf ∗\n\nC(x))\n\nM ≤ IV( ˆfC(x)) + 2(G(f ) +\n\nG(((cid:107)∇f (cid:107)2)) + h\n\n√\n\nh2d 4\n\n(cid:90)\n\nd\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2))\n\nN = (1 + ISB( ˆfC(x)))\n\nLet us now combine them\n\nIMSE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3IMSE( ˆf ∗\n\nC(x))\n\nIMSE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3\n\n(cid:16)\n\nIMSE( ˆfC(x)) + (cid:15)(N + 2M )\n\n(cid:17)\n\nIMSE( ˆfS(x)) ≤ 12(1 − ρh)2 + 3\n\n(cid:16)\n\nIMSE( ˆfH ) +\n\nnnzp − 1 KRnhd + (cid:15)(N + 2M )\n\n(cid:17)\n\n(87)\n\n(88)\n\n(89)\n\n(90)\n\n(91)\n\n(92)\n\n(93)\n\nIMSE( ˆfS(x)) ≤ 12(1−ρh)2+3\n\n(cid:18) 1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)) +\n\nnnzp − 1 KRnhd + (cid:15)(N + 2M )\n\n(cid:19)\n\n(94)\n\nN = (1 + ISB( ˆfC))\n\nN ≤ 1 +\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)\n\nM ≤ IV ( ˆfC) + 2G(f ) +\n\nM ≤ IV ( ˆfH ) +\n\nM ≤\n\n1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nnnzp − 1 KRnhd + 2G(f ) + nnzp − 1 KRnhd + 2G(f ) +\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\nh2d 4\nh2d 4\nh2d 4\n\nd\n\nd\n\nd\n\n(cid:90)\n\nx∈S\n\n(cid:90)\n\nx∈S\n\n(cid:90)\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2)\n\n(f (x)(cid:107)∇f (cid:107)2)\n\n(f (x)(cid:107)∇f (cid:107)2)\n\nIMSE( ˆfS(x)) ≤ 12(1 − ρh)2\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2)) +\n\n(cid:19)\n\nnnzp − 1 KRnhd\n\n+ 3\n\n(cid:32)\n\n(cid:18) 1\n\nnhd + h2d 4\n\n+ 3(cid:15)\n\n1 +\n\nG((cid:107)∇f (cid:107)2)+\n\n(cid:18)\n\n2\n\nG((cid:107)∇f (cid:107)2) +\n\n1\n\nnhd +\n\nG(f ) n\n\n+ o\n\n(cid:19)\n\n(cid:18) 1 n\n\n+\n\nnnzp − 1 KRnhd + 2G(f ) +\n\nh2d 4\n\nG((cid:107)∇f (cid:107)2) + h\n\n√\n\n(cid:90)\n\nd\n\nx∈S\n\n(cid:19)(cid:33)\n\n(f (x)(cid:107)∇f (cid:107)2)dx\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nIMSE( ˆfS(x)) ≤12(1 − ρh)2+ 1\n\n3(1 + 2(cid:15))(\n\nG(f ) n\n\n+ o(\n\n1 n\n\n) +\n\nnnzp − 1 KRnhd )+\n\nnhd + h2d 4\n\n3(1 + 3(cid:15))\n\nG((cid:107)∇f (cid:107)2))+\n\n3(cid:15)(1 + 2G(f ) + h\n\n√\n\n(cid:90)\n\nd\n\nx∈S\n\n(f (x)(cid:107)∇f (cid:107)2))\n\n.\n\nC OTHER BASE LINES\n\nCoresets: We considered a comparison with sophisticated data summaries such as coresets. Briefly, a coreset is a collection of (possibly weighted) points that can be used to estimate functions over the dataset. To use coresets to generate a synthetic dataset, we would need to estimate the KDE. Unfortunately, coresets for the KDE suffer from practical issues such as a large memory cost to construct the point set. Despite recent progress toward coresets in the streaming environment Phillips & Tai (2020), coresets remain difficult to implement for real-world KDE problems Charikar & Siminelakis (2017).\n\nClustering and Importance Sampling: Another reasonable strategy is to represent the dataset as a collection of weighted cluster centers, which may be used to compute the KDE and sample synthetic points. Unfortunately, algorithms such as k-means clustering are inappropriate for large streaming datasets and do not have the same mergeability properties as our sketch. Furthermore, such techniques are unlikely to substantially improve over random sampling when the samples is spread sufficiently well over the support of the distribution. An alternative approach is to select points from the dataset based on importance sampling Charikar & Siminelakis (2017), geometric properties Cortes & Scott (2016), and other sampling techniques Chen et al. (2012). However, recent experiments show that for many real-world datasets, random samples have competitive performance when compared to point sets obtained via importance sampling and cluster-based approaches Coleman & Shrivastava (2020).\n\nDimensionality Reduction: One can also apply sketching algorithms to compress a dataset by reducing the dimension of each data point via feature hashing, random projections or similar methods Achlioptas (2003). However, this is unlikely to perform well in our evaluation since our datasets are already relatively low-dimensional. Such algorithms also fail to address the streaming setting, where N can grow very large, because the size of the compressed representation is linear in N . Finally, most dimensionality reduction algorithms do not easily permit the generation of more synthetic data in the original metric space.\n\nD DIFFERENTIALLY PRIVATE DENSITY SKETCHES\n\nIn order to make the density sketch differentially private, we add noise to the distribution stored by density sketch. This is achieved by adding noise to the underlying count sketch array (K × R matrix of integers). Let the function mapping histogram of the data to the density sketch (before the heap construction) be denoted as f : N |X| −→ Z KR where X is the set of all partitions. We fill first define an discrete analog of laplacian noise.\n\nDefinition 1 (Double geometric distribution). The double geometric distribution parameterized by p ∈ (0, 1) is defined as follows on the support of all integers.\n\nP (z|p) =\n\n1 2 − p\n\n(1 − p)|z|p\n\n(95)\n\nAlgorithm to make Density Sketches private: Each cell of sketch (K × R) matrix is added an i.i.d noise drawn from the double geometric distribution. We will prove that this noise addition makes the function M = f + noise differentially private. Heap construction can be considered as an post processing operation on the density sketch matrix. Hence, the sampling distribution is then\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\ndifferentially private. (Note that heap construction algorithm also needs to be modified in practical settings to ensure that it carries the differential privacy properties. But this is achievable)\n\nTheorem 2 (Differential privacy). The density sketches constructed with addition of double geometric noise with p = 1−e−(cid:15)/K where K is the number of repetitions in the sketch is ((cid:15), 0) differentially private.\n\nProof. Consider the l1 metric for computing the distance between datasets. Consider any arbitrary pair x,y which satisfy (cid:107)x − y(cid:107)1 = 1. In the histogram view of data, it is easy to check that a distance of 1 can exist if and only if there is an additional row in either x or y and all other data points are same. Without loss of generality we can write x = y ∪ {d} where d is the extra data point. As the constructed count sketch does not depend on the order of insertion, we can say that count sketch for x, i.e. f(x), is obtained from count sketch for y by sketching additional data point into it. Also, because of countsketch’s mergeable property, we can write f (x) = f (y) + f ({d}). Hence (cid:107)f (x) − f (y)(cid:107)1 = (cid:107)f ({d})(cid:107)1. As sketching a single entry changes exactly one element of each row of countsketch by 1. (cid:107)f ({d})(cid:107)1 = K. Hence sensitivity of the function f is ∆f = K\n\nWe use the double geometric distribution as defined above for noise.\n\nP (z|p) =\n\n1 2 − p\n\n(1 − p)|z|p\n\n(96)\n\nNow Let us consider the privacy achieved with this error. Let M be the final randomized algorithm with computation of f and adding noise. We are interested in the following quantity with x,y such that (cid:107)x − y(cid:107)1 = 1.\n\nP (M(x) = z) P (M(y) = z)\n\n=\n\n=\n\nΠiP (M(x)i = zi) ΠiP (M(y)i = zi) Πi(1 − p)|f (x)i−zi| Πi(1 − p)|f (y)i−zi|\n\n= Πi(1 − p)|f (x)i−zi|−|f (y)i−zi| = (1 − p)(cid:107)f (x)−z(cid:107)1−(cid:107)f (y)−z(cid:107)1\n\nAs l1-norm is a distance metric we can write\n\nP (M(x) = z) P (M(y) = z)\n\n= (1 − p)(cid:107)f (x)−z(cid:107)1−(cid:107)f (y)−z(cid:107)1\n\n≥ (1 − p)(cid:107)f (x)−f (y)(cid:107)1\n\n= (1 − p)∆(f )\n\nIf we put p = 1 − e−(cid:15)/∆(f )\n\nP (M(x) = z) P (M(y) = z)\n\n≥ e−(cid:15)\n\nP (M(y) = z) P (M(x) = z)\n\n≤ e(cid:15)\n\n(97)\n\nHence M(x) is ((cid:15), 0)- differentially private. Hence we have that the countsketch produced by the sketching algorithm with added double geometric noise is ((cid:15), 0)- differentially private when we have p = 1 − e−(cid:15)/K\n\nwhy heaps are differentially private? If the data is bounded in Rd (d is the dimension of the data), then it is easy to check that there is a cell in Rd, which contains all the data, It follows that the number of partitions inside this cell is finite. So we can consider heap construction as iteratively going through each partition and noting down its count. Once we do that, we sort all the partitions according to the counts and keep top H elements. In this sense, we can consider heap construction as a post processing over count sketch. From the proposition 2.1 [Dwork, Roth], we know that post processing maintains differential privacy. Hence the heap we create is ((cid:15), 0) differentially private\n\n22",
    "reference": "# Summary Of The Paper\n\nThis paper constructs a data structure, dubbed density sketch, that represents a density on a high-dimensional space. This data structure is constructed online, in a streaming fashion from data samples assumed drawn i.i.d. from a true density. The representation is capable of evaluating the density at specific points as well as of sampling points. The paper also provides analysis of the approximation level of this representation in MISE distance and illustrates its applicability via some experiments.\n\n# Strength And Weaknesses\n\n### Strengths\n\n* Reducing the overhead of data by representing it accurately in compressed form continues to be a timely topic. \n\n* The construction of the density sketch data structure is intuitive and the compression and accuracy levels achieved in experiments seem very promising. \n\n### Weaknesses\n\n* The paper is typographically in a very rough form. Many mathematical expressions are not placed in math style and there are typesetting errors in those in math style (e.g., hat symbols appearing in the wrong places) that make expressions hard to parse. There are also many linguistic issues, some of which are listed under minor comments. The paper needs a serious polish revision to become of publication quality.\n\n* The main theorem (Theorem 1) is not presented cleanly. Here are some issues:\n    * The parentheses are all the same size, making it hard to parse what goes where. This is an easy fix, with the exception that there seems to be an extra right parenthesis in the very end, which could change the meaning of the expression if it is associated with a left parenthesis somewhere.\n    * Perhaps a related issue to the above, but as written the right-hand side does *not* vanish, even if both $1/nB^d$ and $B^2d$ vanish. That’s because the last parenthesis block which starts with $3\\epsilon(1+2\\mathcal{G}(f)+\\cdots)$ is not affected by either of these. This could be an issue of misplaced parentheses. However, as far as I can tell from the proof (which I have only skimmed through), that term is there.\n\n* The reasons the last point necessitates a clarification is because it makes the conclusions from the Theorem a bit more subtle. In particular, it would seem that we can make $\\delta$ a constant, which implies that $\\epsilon$ vanishes, so this term would vanish. However, if I understand it correctly from the proof, the equation is only valid if $\\delta$ is small for a fixed $\\epsilon$. Therefore we cannot change the order of quantifiers, and that reasoning does not work. I welcome engaging with the authors to clarify this point, as going through the proof to reach the conclusion myself is a bit daunting.\n\n* The paper would benefit from a clear characterization of the worst-case space and time complexity of the method, since lower complexity is one of the selling points. \n\n### Questions\n\n* In Figure 1, what is $g_i(a)$?\n* In SRP, are the volumes of all partitions equal? You assume that throughout, but perhaps that only works for LSH?\n* I don’t think you mean multinomial distribution in Algorithm 3, you instead mean a categorical distribution over the bins.\n\n### Typos and suggestions\n\n(p.1) include edge > including edge\n(p.2) in the section 2 > in this section. ; function f(x) > function $f(x)$\n(p.3) place $\\hat$ symbols correctly; define $\\mathcal{C}$ as the count vector\n(p.4)\n* Your use of $i$ is too overloaded, you use it to index: data points, coordinates, and LSH/SRP weights. It would be more legible if each of theses uses had its own letter.\n* In Table 1:\n   * for LSH and SRP in the bin column, there shouldn’t be a subscript on $x$\n   * typeset the $w_1$ vector and $W$ vector in math mode\n   * remove trailing parenthesis after $U[0,1]$\n* point x > point $x$\n(p.5) a efficient > an efficient; in the figure > in Figure; fix all the $\\hat$ locations (last mention, this has to be fixed everywhere in the paper); remove trailing parenthesis from $\\hat \\mathcal{C}(x)$\n(p.6) in Algorithm 1, fix the partition function line; in Algorithm 3, remove trailing parenthesis from UniformRandomPoint(b); point x > point $x$; close parenthesis after MSE\n(p.7) add = between IMSE and MISE\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper needs a good polish in its mathematical expressions and language. Another weird point is that the paper was uploaded in non-searchable form (I had to OCR It). It would be very useful for reviewing to have at least the text searchable, and ideally all references hyperlinked.\n\nThe combination of a heavy-hitter list along with LSH bins and count sketching is novel to me, but there have been many streaming data reduction techniques and it is difficulty to fully judge the novelty without being directly in this research area. Code was provided, and the experiments appear reproducible.\n\nIf, after discussions, the main result is determined to truly represent an approximation theorem in the limit, then there is enough contribution here to make it worthy to share with the community, conditionally on clarifying the presentation and polishing the typesetting and language.\n\n# Summary Of The Review\n\nThe paper puts together various data reduction techniques in an intuitive manner to offer a novel density representation that can be constructed in a streaming fashion. This appears to lead to low space and time complexity, but ideally that analysis should also be part of the paper. The main result requires a bit of clarification, to make sure that the approximation ability of this representation is indeed there. What hampers the paper most is unpolished typesetting and language. With some clarification and polish, the paper may be worth sharing with the community.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nOCIM: OBJECT-CENTRIC COMPOSITIONAL IMAGINATION FOR VISUAL ABSTRACT REASONING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nA long-sought property of machine learning systems is the ability to compose learned concepts in novel ways that would enable them to make sense of new situations. Such capacity for imagination – a core aspect of human intelligence – is not yet attained for machines. In this work, we show that object-centric inductive biases can be leveraged to derive an imagination-based learning framework that achieves compositional generalization on a series of tasks. Our method, denoted Object-centric Compositional Imagination (OCIM), decomposes visual reasoning tasks into a series of primitives applied to objects without using a domain-specific language. We show that these primitives can be recomposed to generate new imaginary tasks. By training on such imagined tasks, the model learns to reuse the previously-learned concepts to systematically generalize at test time. We test our model on a series of arithmetic tasks where the model has to infer the sequence of operations (programs) applied to a series of inputs. We find that imagination is key for the model to find the correct solution for unseen combinations of operations.\n\n1\n\nINTRODUCTION\n\nHumans have the remarkable ability to adapt to new unseen environments with little experience (Lake et al., 2017). In contrast, machine learning systems are sensitive to distribution shifts (Arjovsky et al., 2019; Su et al., 2019; Engstrom et al., 2019). One of the key aspects that makes human learning so robust is the ability to produce or acquire new knowledge by composing few learned concepts in novel ways, an ability known as compositional generalization (Fodor and Pylyshyn, 1988; Lake et al., 2017). Although the question of how to achieve such compositional generalization in brains or machines is an active area of research (Ruis and Lake, 2022), a promising hypothesis is that dreams are a crucial element (Hoel, 2021) through the Overfitted Brain Hypothesis (OBH).\n\nBoth imagination and abstraction are core to human intelligence. Objects in particular are an important representation used by the human brain when applying analogical reasoning (Spelke, 2000). For instance, we can infer the properties of a new object by transferring our knowledge of these properties from similar objects (Mitchell, 2021). This realization has inspired a recent body of work that focuses on learning models that discover objects in a visual scene without supervision (Eslami et al., 2016b; Kosiorek et al., 2018; Greff et al., 2017; van Steenkiste et al., 2018; Greff et al., 2019; Burgess et al., 2019; van Steenkiste et al., 2019; Locatello et al., 2020). Many of these works propose several inductive biases that lead to a visual scene decomposition in terms of its constituting objects. The expectation is that such an object-centric decomposition would lead to better generalization since it better represents the underlying structure of the physical world (Parascandolo et al., 2018). To the best of our knowledge, the effect of object-centric representations for systematic generalization in visual reasoning tasks remains largely unexplored.\n\nWhile abstractions, like objects, allow for reasoning and planning beyond direct experience, novel configurations of experienced concepts are possible through imagination. Hoel (2021) goes even further and posits that dreaming, which is a form of imagination, improves the generalization and robustness of learned representations. Dreams do so by producing new perceptual events that are composed of concepts experienced/learned during wake-time. These perceptual events can be described by two knowledge types (Goyal et al., 2020; 2021b): the declarative knowledge encoding object states (e.g. entities that constitute the dreams), and the procedural knowledge encoding how they behave and interact with each other (e.g. how these entities are processed to form the percep-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ntual event). In this work, we take a step towards showing how OBH can be implemented to derive a new imagination-based learning framework that allows for better compositional generalization like dreams do.\n\nWe thus propose OCIM, an example of how object-centric inductive biases can be exploited to derive imagination-based learning frameworks. More specifically, we model a perceptual event by its object-centric representations and a modular architecture that processes them to solve the task at hand. Similar to (Ellis et al., 2021), we take the program-induction approach to reasoning. In order to solve a task, the model needs to (1) abstract the perceptual input in an object-centric manner (e.g. represent declarative knowledge), and (2) select the right arrangement of processing modules (which can be seen as a neural program) that solves the task at hand. In order to generalize beyond direct experience through imagined scenarios, a model would have to imagine both of these components (e.g. objects + how to process them). Here we restrict ourselves to imagining new ways to process experienced perceptual objects. We propose to do so by exploiting object-centric processing of inductive biases. The idea is to have a neural program (Reed and De Freitas, 2015; Cai et al., 2017; Li et al., 2020) composed of modular neural components that can be rearranged (e.g. “sampled” through selection bottlenecks) to invent new tasks. The capacity to generate unseen tasks enables OCIM to generalize systematically to never-seen-before tasks by (1) producing new imagined scenarios composed of learned/experienced concepts and (2) training the model on these imagined samples to predict back their constituting concepts (e.g. used modules that were sampled to produce them). Our contribution is threefold:\n\n• We propose an example of how object-centric inductive biases can be used to derive an imagination-based learning framework. Specifically we show that rearranging modular parts of an object-centric processing model to produce an imagined sample and training the model to predict the arrangement that produced that sample helps with compositional generalization.\n\n• We propose a visual abstract reasoning dataset to illustrate our imagination framework and\n\nevaluate the models along different axis of generalization.\n\n• We highlight some drawbacks of current state-of-the-art (SOTA) object-centric perception model when it comes to disentangling independent factors of variation within a single visual object.\n\n2 RELATED WORK\n\nObject-centric Representation. A recent research direction explores unsupervised object-centric representation learning from visual inputs (Locatello et al., 2020; Burgess et al., 2019; Greff et al., 2019; Eslami et al., 2016a; Crawford and Pineau, 2019; Stelzner et al., 2019; Lin et al., 2020; Geirhos et al., 2019). The main motivation behind this line of work is to disentangle a latent representation in terms of objects composing the visual scene (e.g. slots). Recent approaches to slot-based representation learning focus on the generative abilities of the models; in our case, we study the impact of object-centric inductive biases on systematic generalization of the models in a visual reasoning task. We observe that modularity of representations is as important as the mechanisms that operate on them (Goyal et al., 2020; 2021b). Additionally, we show that object-centric inductive biases of both representations and mechanisms allow us to derive an imagination framework that leads to better systematic generalization.\n\nModularity. Extensive work from the cognitive neuroscience literature (Baars, 1997; Dehaene et al., 2017) suggests that the human brain represents knowledge in a modular way, with different parts (e.g, modules) interacting with a working memory bottleneck via attention mechanisms. Following these observations, a line of work in machine learning Goyal and Bengio (2020); Goyal et al. (2019; 2020; 2021b); Ostapenko et al. (2021); Goyal and Bengio (2022) has proposed to translate these characteristics into architectural inductive biases for deep neural networks. Recent approaches have explored architectures composed of a set of independently parameterized modules that compete with each other to communicate and attend or process an input (Goyal et al., 2019; 2020; 2021b). Such architectures are inspired by the notion of independent mechanisms (Pearl, 2009; Bengio et al., 2019; Goyal et al., 2019; Goyal and Bengio, 2022), which suggests that a set of independently parameterized modules capturing causal mechanisms should remain robust to distribution\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nshifts caused by interventions, as adapting one module should not require adapting the others. The hope is that out-of-distribution (OOD) generalization would be facilitated by making it possible to sequentially compose the computations performed by these modules, whereby new situations can be explained by novel combinations of existing concepts. In this work, we show how modular architectural choices can be exploited to derive an imagination-based learned paradigm that allows better compositional generalization; we do so by explicitly exposing the model to data samples composed of novel combination of learned concepts.\n\nImagination, Dreaming, and Generalization Dreams are a form of imagination that have inspired a significant amount of influential work (Hinton et al., 2006; Ellis et al., 2021; Hafner et al., 2019; 2020). An interesting explanation for such phenomenon is the overfitted brain hypothesis (OBH) (Hoel, 2021), which states that dreaming improves the generalization and robustness of learned representations. The idea is that, while dreaming, the brain recombines patterns seen during wake time. This results in artificial data augmentation in the form of dreams. This way, dreams regularize and prevent the brain from overfitting the patterns seen while being awake. In machine learning, data augmentation is a long standing technique for tackling the data scarcity problem, whereby new training samples are generated from existing data in order to diversify the trained models. Various approaches for augmentation, such as GANs (dos Santos Tanaka and Aranha, 2019), VAEs (Chadebec and Allassonni`ere, 2021), and diffusion models Ho et al. (2022) have proved to be effective in improving model accuracy and generalizability. As in the case of dreams for the human brain, the strategy of considering imagined data samples is broadly compatible with data augmentation objectives. Dreamcoder (Ellis et al., 2021) is a recent example where training on imagined patterns improve generalization for program induction. Program induction is a challenging problem because the search space is combinatorially large and new unseen programs have low likelihood. To address these challenges, Dreamcoder leverages a wake-sleep algorithm that reduces the search space by learning a domain-specific language (DSL) while learning to search programs efficiently. During training, Dreamcoder undergoes a dreaming phase where the model learns to solve new tasks generated by sampling programs from a DSL and applying them to inputs seen during the wake phase. Although Dreamcoder is promising for program induction, the DSL is a major roadblock to solve open-ended visual reasoning tasks where the input consists of raw pixels rather than symbols. In this work, we overcome these challenges by relying on object-centric inductive biases, i.e., architectural choices that both represent and process objects, in order to learn a neural program library.\n\n3 VISUAL ARITHMETIC REASONING DATASET\n\nFigure 1: Data sample and dataset splits.\n\nMost visual reasoning benchmarks revolve around variations of Raven’s Progressive Matrices (RPM) (James, 1936; Zhang et al., 2019; Barrett et al., 2018; Hoshen and Werman, 2017) which are discriminative tasks in which the solver chooses from a set of candidate answers. However, in a recent survey, Mitchell (2021) recommends evaluating models on generative tasks that focus on\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nhuman core knowledge (Spelke, 2000). Models trained on generative tasks are indeeed less prone to shortcut learning and systems that generate answers are in many cases more interpretable. To that end Chollet (2019) proposes the Abstract Reasoning Corpus (ARC), where the model is given a few examples of Input-Output (I/O) pairs and has to understand the underlying common program that was applied to the inputs to obtain the outputs. ARC tasks are meant to rely only on the innate core knowledge systems which include intuitive knowledge about objects, agents and their goals, numerosity, and basic spatial-temporal concepts. However, ARC remains unapproachable by current deep learning methods.\n\nWe propose to take a step towards solving ARC by designing a new generative benchmark in which we evaluate systematic compositional generalization. Like ARC, our dataset is composed of a collection of support sets, each having a number of input/output pairs, such that the output for every sample (support set) is obtained by applying the same program to the corresponding input. The model is then presented with a new query input and evaluated on its ability to predict the right associated output (i.e. applying the inferred program in the support set to the query input). The inputs are 56 × 56 images with three colored MNIST digits placed at three different positions. These visual digits can have values between −9 and 9 and their color represents their sign. There are six different colors in total (3 of them are negative and the remaining 3 are positive). The program applied to the inputs is a sequence of arithmetic operations (we restrict ourselves to addition and subtraction and the dataset can further be extended with more complicated queries that involve comparison between the different objects, maximum operations etc..) in a particular positional order. Since we are interested in the model’s ability to generalize compositionally to unseen examples, we create different splits that aim at evaluating different axes of compositional generalization. These three splits are as follows : (1) OOD seq : In this split, during training we leave out some sequence of operations (e.g. (+, −)) and perform the evaluation on samples requiring the excluded sequence., (2) OOD order: where uring training the model only sees programs that take input digits in some particular positional order (e.g. top-left, top-right, down) and is evaluated on unseen orders., and (3) OOD perception which evaluate the perception module ability to disentangle the digit class (e.g. 1 to 9) from its color (representing its sign). We thus consider certain pairs of digit-color configurations during training and evaluate the model on unseen pairs.\n\nFigure 2: Task and Imagination pathways in OCIM\n\n4 OBJECT-CENTRIC COMPOSITIONAL IMAGINATION (OCIM)\n\nOur model is designed to generate answers in a sequential way. The design choices that we make reflect the fact that the output answer to a generative reasoning task can be computed by sequentially updating a working memory whose arguments are obtained from available input (e.g. slots extracted from images in our case). The computation steps are the following: (1) Visual inputs x are mapped\n\n4\n\nExecutorControllerControllerTaskImaginationinferimaginary programsample programselect programExecutorProgram libraryI/O pairsImaginedI/O pairsInputsSlot AttentionUnder review as a conference paper at ICLR 2023\n\nto Ns object-centric slots S = [S1, .., SNs] using a Slot Attention module (Locatello et al., 2020), then, (2) the controller takes the support set as input and ouputs a single task embedding z; this task embedding is then (3) translated into a sequential neural program (e.g. a sequence of transformation to be applied to a working memory); finally, (4) the executor takes this neural program along with an input query (i.e. its object-centric slots) and performs the sequential updates. The overall computation paths are given in Figure 2.\n\nThe main contribution of our work resides in the architecture of the executor, its interface with the controller (i.e. how the executor uses the information encoded in the controller) and the derived imagination machinery. The controller scaffold (detailed in the appendix) that we use in all baselines always outputs a single task embedding z and can be adapted depending on the task at hand. In this section, we detail the modeling choices of the executor and the interface between the executor and the controller, which is formulated as a selection bottleneck and the imagination component.\n\n4.1 EXECUTOR\n\nThe executor takes a visual query input x ∈ R56×56×3 and a neural program; it then updates a working memory h ∈ Rp in a sequential and structured manner. The visual input is first mapped to a set of Ns object-centric slots [S1, SNs ] that is later used as candidate arguments for each update of the working memory. The executor is composed of a library of Nr learned modules (e.g. rules, implemented as small GRU cells) and Nc condition values. The conditions are expected to encode the way in which to select an argument (e.g. among the slots) that is in turn used by a module to update the working memory. Both rules (i.e. modules) and conditions are indexed by some learned tags M = [M1, . . . , MNr ] and C = [C1, . . . , CNc]. The neural program that the executor takes as input generates (1) the number T of updates that the executor needs to perform, specified by a scalar gate at each time step; we denote the sequence of such gates by g = [g1, . . . , gT ]) ; (2) the sequence of modules [ ˆm1, . . . , ˆmT ] (each ˆmt parameterized by a small learned GRU RNN) that will perform the T updates of the working memory; and (3) the sequence of conditions [ˆc0, . . . ,ˆcT ] (each ˆct being a condition vector that selects one slot; this slot will be used as an argument to the selected module at each time step). At each time step t, each update in the sequence is done in the following two steps:\n\n• Argument selection: given a condition vector ˆct select an argument of the update from\n\namong the input slots S of the query.\n\n• Update: update working memory ht−1 with GRU rule ˆmt and the selected argument ˆst.\n\nArgument Selection. At each time step, a slot argument is selected through a key-query attention mechanism. The idea is that the condition vector ˆct is compared against all the input slots to select the one that corresponds best to the features encoded in the condition (e.g. select the slot at the “topleft” of the image). The attention mechanism is thus realized using the condition vector ˆct ∈ R1×d as a query and the Ns slots S = [S1, . . . , SNs ] ∈ RNs×d as keys such that the selected argument ˆst at time-step t is given by:\n\nˆst = GumbelSoftmax(\n\nˆctST √\nd\n\n)S ∈ R1×d\n\n(1)\n\nThe sequence of selected arguments is thus given by ˆs = [ˆs0, . . . , ˆsT ].\n\nSequential Update. Given a sequence of processing modules [ ˆm1, . . . , ˆmT ], a sequence of input arguments [ˆs0, . . . ,ˆsT ] and a length given by a sequence of gates [g1, ..,gT ], the executor updates a working memory whose state at time step t is denoted by ht such that:\n\nht+1 = (gt+1)ht + (1 − gt+1) ˆmt(ˆst+1, ht) and h0 = ˆs0\n\n(2)\n\nFor ease of notation, we let Executor(x, P) be the result of applying the neural program P to the visual input x.\n\n4.2 SELECTION BOTTLENECK\n\nIn this section, we describe the interface between the controller and the executor: how the task embedding z output by the controller is transformed into a neural program that the executor then\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\ntakes as input (e.g. sequences of modules, conditions and gates) to perform the sequential update. First the task embedding z ∈ Rd is transformed into a sequence of embeddings by feeding z as argument to a GRU RNN that starts with an empty hidden state [z1, . . . , zT ] = GRU(z).\n\nBoth module and condition selections are done through a key-query attention mechanism comparing the task embedding zt to the Nr learned module tags (denoted by M = [M1, . . . , MNr ] ∈ RNr×d) and the Nc learned conditions tags (denoted by C = [C1, . . . , CNc] ∈ RNc×d). The keys are extracted from the condition tags, whereas the query is extracted in both attention operations form the task embedding zt (using two MLPs Qr and Qc) such that the t-th element of each sequence is obtained with:\n\nW t\n\nm = GumbelSoftmax(\n\nQr(zt)MT √\n\nd\n\n) ∈ R1×Nr\n\n(3)\n\nand the (cid:80)Nr i=1 W t\n\nresulting update\n\nis given by the m[i]mi(ht, ˆst). Similarly, the conditions are obtained through:\n\nfollowing weighted sum ˆmt(ht, ˆst) =\n\nd with c ∈ RNc×d denoting the set of learned condition vectors.\n\nˆct = GumbelSoftmax(\n\n)c\n\n(4)\n\nQc(zt)CT √\n\nFinally, the sequence of step gates are obtained directly from the sequence of [z1, . . . , zT ] such that\n\ngt = MLP(zt)\n\n(5)\n\nFor ease of notation, we let Pz = SelectionBottleneck(z) = {g, ˆc, ˆm} denote the neural program obtained from the task embedding z, where g, ˆc, ˆm correspond to the associated step gates, condition vectors and processing module sequences.\n\n4.3 COMPOSITIONAL IMAGINATION\n\nOur main contribution resides in showing how object-centric inductive biases (used in the executor) can be leveraged to induce a new imagination-based learning framework that leads to better compositional generalization. The idea is that the same way we select a sequence of modules, conditions and gates using the task embedding output by the controller, we can also sample them at random (from a uniform distribution) to create a new neural program that can be used to create imagined scenarios. To do so, we sample at random a sequence of gates gim = [gim T ], a sequence of condition vectors cim = [cim T ] that correspond to the procedural part of the knowledge we have about the reasoning task at hand. Ideally we would also sample the query to process (e.g. the declarative part) but we leave that for future work. Instead, we take visual inputs that are already present in the training data and we apply an imagined neural program to them. Since the goal is to create new samples, we need to apply the same imagined program to a set of visual inputs to form a support set.\n\nT ] and a sequence of processing modules mim = [mim\n\n0 , . . . , mim\n\n1 , . . . , gim\n\n0 , . . . , cim\n\nLet Xsupp = {x1, . . . , xL} denote a set of visual inputs from the training data, and let P im = {gim, cim, mim} be an imagined program. Then the imagination phase can be split into 3 main steps:\n\n• Imagined samples: this step consists of applying an imagined program P im to a support set of visual input Xsupp to obtain an imagined I/O support set S im = {Xsupp, Oim} with Oim = [Executor(xi, P im) for xi ∈ Xsupp].\n\n• Task embedding inference: this step consists of encoding the imagined support set with\n\nthe controller to produce a task embedding zim = Controller(S im).\n\n• Mechanisms Prediction:\n\nthe last step consists of predicting back the neural programs its components) that produced the imagined sample. This means matching ˆPzim =\n\n(i.e. SelectionBotlleneck(zim) with P im.\n\nThe associated loss is called the imagination loss Lim = L( ˆPzim , P im), which can be split into 3 cross-entropies predicting the step gate values, the conditions vector indices and the processing module indices. During training, we introduce this loss after a warming period during which the model is trained only on the training data available. We detail the hyperparameters in the Appendix.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Executor Pseudocode\n\n1: function EXECUTOR(x, P) 2: 3:\n\nS = SlotAttention(x) g1..T , ˆm1..T , ˆc0..T ← P h0 ← GumbelSoftmax( ˆc0ST d\nfor (t = 1; t < T ; t + +) do\n\n√\n\n▷ The input x and program P ▷ Object-centric perception\n\n)S\n\n▷ Working memory initialization\n\n4:\n\n5:\n\n6:\n\n7: 8: 9:\n\nˆst ← GumbelSoftmax( ˆctST ht+1 ← (gt+1)ht + (1 − gt+1) ˆmt(ˆst+1, ht)\n\n)S ∈ R1×d\n\n√\n\nd\n\nend for return pred(hT )\n\n▷ Eq. 1 ▷ Eq. 2\n\n▷ Task-specific prediction\n\n▷ Samples seen during training ▷ Object-centric perception ▷ Sample a program\n\n▷ Infer program\n\n10: end function\n\nAlgorithm 2 Compositional Imagination Require: Xsupp\n\n1: Ssupp ← SlotAttention(Xsupp) 2: P im ∼ U (g, C, M) 3: Oim = {Executor(xi, P im) for xi ∈ Xsupp} 4: S im ← {Xsupp, Oim} 5: zim ← Controller(S im) 6: (cid:98)Pzim = SelectionBottleneck(zim) 7: lossim ← CrossEntropy(P, (cid:98)Pzim)\n\n4.4 TRAINING\n\nThe training of the whole model can be split into three phases: Step 1: Pretraining of the perception - Step 2: Regular training on model such that the next steps start with reasonable latent slots. the task prediction objective (8-binary-bits digit prediction). - Step 3: Imagination, where random modules and conditions are sampled to create new data points and expose the model to potentially OOD samples.\n\nEach of these steps gives rise to a specific objective loss to optimize. The task prediction objective in our case is a simple cross-entropy on the output of the executor, since we treat each bit of the output as a binary label to predict. This loss is given by:\n\nLtask = −\n\n(cid:88)\n\ni∈Dtrain\n\n(yi log(ˆyi) + (1 − yi) log(1 − ˆyi)).\n\n(6)\n\nThe pretraining phase consists of training the Slot Attention module on a reconstruction task. During the imagination phase, new samples are created according to Algorithm 2, and the model is optimized to infer the programs that generated these samples and to minimize Ltask at the same time. We detail the hyperparameters associated to the different training phases in the Appendix.\n\n5 EXPERIMENTS\n\nOCIM has two main components. The perception component and the object processing (i.e. reasoning) component. Our contribution lies in the object processing component, while for the perception component we use a SOTA slot attention module (Locatello et al., 2020). The goal of this section is two-fold: (1) to evaluate our imagination-based learning paradigm on a set of compositional generalization axes and (2) the ability of the perception module to extract symbolic-like representations that can be used to solve our visual abstract reasoning task.\n\n5.1 BASELINES\n\nOur model OCIM can be seen as an extension of the sparse interaction inductive biases proposed in Neural Production Systems (NPS) Goyal et al. (2021a), and augmented with an imagination-based\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nlearning mechanism. NPS sequentially updates a set of slots by choosing at each time-step a primary slot, a secondary slot, and an interaction rule with some key-query attention mechanisms. OCIM sequentially updates the state of a shared working memory across time steps (e.g. instead of slot states) from which the final answer can be extracted. As a result, at each time-step the primary argument of an interaction is always the shared memory and the second argument is selected among the input slots. We also compare OCIM (and its variant without imagination that we call OCIMnoim) to two other baselines in which the executor is parameterized with a single monolithic GRU RNN in one case, and with a dense GNN in the other, for which we use the interaction component from the C-SWM (Kipf et al., 2019) model (like Goyal et al. (2021b)). In each of these baselines, input nodes correspond to extracted slots concatenated with the output of the controller. For the GNN baseline, a GRU RNN is added after computing the interactions between nodes to aggregate the final result. We refer to these two baselines as GRU RNN and GNN respectively, and detail their exact parameterization in the Appendix.\n\n(a) OOD op\n\n(b) OOD order\n\n(c) OOD perception\n\nFigure 3: Validation and test accuracy for the baselines and OCIM on the three axes of compositional generalization described in Section 3. Insights about the high variance of the results for OCIM is given in 5.3 when inspecting modules specialization.\n\n5.2 OOD SPLITS\n\nWe are interested in 3 axes of compositional generalization that evaluate both our proposed imagination-based learning paradigm and the perception module robustness: (1) We first want to evaluate whether the imagination phase in OCIM can lead to a better generalization to arithmetic tasks composed of never-seen sequences of operations during training; (2) we then want to evaluate whether OCIM is able to generalize to never-seen orders in which the input digits are taken to perform the sequence of operations (e.g. its ability to extract meaningful and general argument selection conditions), and finally, (3) we want to evaluate whether current object-centric iductive biases as proposed in Slot Attention Locatello et al. (2020) are well suited for disentangling independent factors of variations within an object (e.g. color and digit class in our case). To that end, we propose to evaluate the models on the three splits described in Section 3:\n\n5.3 RESULTS\n\nImagination and Generalization. In our experiments, all the models share the same perception model Locatello et al. (2020) and the same controller. They only differ by their execution component. In Figure 3, we report the accuracy peformances of our model compared to the baselines of interest across three different splits that aim at evaluating a particular axis of generalization. We observe two main results: (1) Imagination does help to generalize to novel sequences of operations as shown in the generalization gap of OCIM between the results of the OOD op split and the other baselines. (2) Current SOTA object-centric perception models like Slot Attention are not quite able to systematically generalize to objects composed of never-seen before arrangements of known arguments (such as new pairs of color/shape). This result is interesting and suggests that additional inductive biases or learning paradigms are needed to learn object-centric representations that disentangle independent factors of variations within an object. We did however notice that the choice architecture for the execution component seems to have an impact on the perception part, and that, surprisingly, both the GNN and GRU baselines perform better than OCIM on the OOD perception split.\n\n8\n\nocimocim-noimgnngrumodel0.00.20.40.60.81.0accuracyvaltestocimocim-noimgnngrumodel0.00.20.40.60.81.0accuracyvaltestocimocim-noimgnngrumodel0.00.20.40.60.81.0accuracyvaltestUnder review as a conference paper at ICLR 2023\n\nModules Specialization. We are also interested in analyzing how specialization of the learned modules (i.e. becoming activate for a certain operation) impacts the generalization performance of OCIM. For each training random seed, we count the number of times each modSince we use the Gumbel softule was selected for each of the ground-truth operations. to decide max trick to select modules , we use the argmax of the attention coefficient these proportions in the heatmap in Figure 4 for which module is selected. We report OCIM with and without imagination. The x-axis corresponds to the seed number; the yaxis corresponds to the module indices for both the addition and the subtraction operations. the top of the The accuracies reported at heatmaps correspond to the validation and test accuracies on the OOD op splits (e.g. when evaluating the models on sequences of operations that have not been seen during training). We note the following three observations: (1) Current inductive biases are not sufficient for specialization to systematically occur since there are some seeds that have overlaps between selected modules. (2) As shown in the generalization results of OCIM-noim, specialization in the modules is not enough for the model to generalize to novel sequences of operations (e.g. modules in seed 2 of OCIM-noim (left column) are specialized yet we do not observe systematic generalization). Finally, (3) We notice that imagining new samples is necessary for the model to generalize, but also not sufficient as it needs the modules to be specialized as well. Seeds 0 and 5 of OCIM do have overlap in the module selections and the imagination framework was not successful. This observation explains the variance in terms of performance for the OOD op split that we report in Figure 3.\n\nFigure 4: Modules specialization.\n\n6 CONCLUSION AND FUTURE WORK\n\nWe have presented OCIM, a method that leverages object-centric representations to decompose visual reasoning tasks into a series of learned primitives (operation and object choices). OCIM combines these primitives in novel ways in order to generate and learn from unseen imaginary tasks, which radically improve OOD generalization. We compared OCIM against NPS and two other baselines without imagination on a synthetic visual arithmetic reasoning task in which we apply a sequence of operations to colored MNIST digits. We found that only OCIM was able to systematically generalize to new tasks composed of unseen sequences of arithmetical operations. Interestingly, we observed that specialization among the neural modules seems to be a necessary but not sufficient condition for modular architectures like OCIM and NPS to generalize to unseen sequences of operations: imagination seems to be a critical addition to the specialization condition. The effectiveness of imagination in our setup raises the question of whether its function is similar in biological brains. An interesting hypothesis is that dreams have a regularizing effect in the brain (OBH). While the link between OBH and OCIM is superficial, it poses an interesting question that might be worth exploring in future work.\n\nAlong with OCIM, we have introduced a synthetic visual reasoning benchmark to assess the extent to which imagination improves compositional generalization. Despite the simplicity of the benchmark, we found that SOTA models like NPS fail to compose the primitives learned during training in novel ways in order to generalize. As research in compositional generalization progresses, the benchmark could be extended with more challenging scenarios by increasing the number of operations, the length of the programs, and the number of objects.\n\nReproducitbility Statement We reported in the appendix all the model and training hyperparameters to implement and reproduce our model (Table 1, 2, 3, 4) as well as the detailed content of the different data splits. We will release the code and scripts for both our model, the baselines and the generation of the different dataset splits.\n\n9\n\n01val accuracytest accuracy012345012Add012345012345Experiment numberOCIM-noim012Sub012345Experiment numberOCIMModule NumberUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nM. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv preprint\n\narXiv:1907.02893, 2019.\n\nB. J. Baars. In the theatre of consciousness global workspace theory, a rigorous scientific theory of\n\nconsciousness. 1997.\n\nD. Barrett, F. Hill, A. Santoro, A. Morcos, and T. Lillicrap. Measuring abstract reasoning in neural\n\nnetworks. In International conference on machine learning, 2018.\n\nY. Bengio, T. Deleu, N. Rahaman, R. Ke, S. Lachapelle, O. Bilaniuk, A. Goyal, and C. Pal. A meta-transfer objective for learning to disentangle causal mechanisms. arXiv:1901.10912, 2019.\n\nC. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Lerchner. Monet:\n\nUnsupervised scene decomposition and representation, 2019.\n\nJ. Cai, R. Shin, and D. Song. Making neural programming architectures generalize via recursion. In\n\nInternational Conference on Learning Representations, 2017.\n\nC. Chadebec and S. Allassonni`ere. Data augmentation with variational autoencoders and manifold sampling. In S. Engelhardt, I. Oksuz, D. Zhu, Y. Yuan, A. Mukhopadhyay, N. Heller, S. X. Huang, H. Nguyen, R. Sznitman, and Y. Xue, editors, Deep Generative Models, and Data Augmentation, Labelling, and Imperfections, pages 184–192, Cham, 2021. Springer International Publishing.\n\nF. Chollet. On the measure of intelligence. arXiv preprint arXiv: Arxiv-1911.01547, 2019.\n\nE. Crawford and J. Pineau. Spatially invariant unsupervised object detection with convolutional\n\nneural networks. In AAAI Conference on Artificial Intelligence, 2019.\n\nS. Dehaene, H. Lau, and S. Kouider. What is consciousness, and could machines have it? Science, 358(6362):486–492, 2017. doi: 10.1126/science.aan8871. URL https://www.science. org/doi/abs/10.1126/science.aan8871.\n\nF. H. K. dos Santos Tanaka and C. Aranha. Data augmentation using gans. Proceedings of Machine\n\nLearning Research XXX, 1:16, 2019.\n\nK. Ellis, C. Wong, M. Nye, M. Sabl ́e-Meyer, L. Morales, L. Hewitt, L. Cary, A. Solar-Lezama, and J. B. Tenenbaum. Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning. In Proceedings of the 42nd acm sigplan international conference on programming language design and implementation, pages 835–850, 2021.\n\nL. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry. Exploring the landscape of spatial robustness. In International conference on machine learning, pages 1802–1811. PMLR, 2019.\n\nS. Eslami, N. Heess, T. Weber, Y. Tassa, D. Szepesvari, K. Kavukcuoglu, and G. E. Hinton. Attend, infer, repeat: Fast scene understanding with generative models. arXiv preprint arXiv:1603.08575, 2016a.\n\nS. M. A. Eslami, N. Heess, T. Weber, Y. Tassa, D. Szepesvari, K. Kavukcuoglu, and G. E. Hinton.\n\nAttend, infer, repeat: Fast scene understanding with generative models, 2016b.\n\nJ. A. Fodor and Z. W. Pylyshyn. Connectionism and cognitive architecture: A critical analysis.\n\nCognition, 28(1-2):3–71, 1988.\n\nR. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel.\n\nImagenettrained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019.\n\nA. Goyal and Y. Bengio.\n\nInductive biases for deep learning of higher-level cognition. CoRR,\n\nabs/2011.15091, 2020. URL https://arxiv.org/abs/2011.15091.\n\nA. Goyal and Y. Bengio. Inductive biases for deep learning of higher-level cognition. Proc. A, Royal\n\nSoc., arXiv:2011.15091, 2022.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nA. Goyal, A. Lamb, J. Hoffmann, S. Sodhani, S. Levine, Y. Bengio, and B. Sch ̈olkopf. Recurrent\n\nindependent mechanisms. arXiv preprint arXiv:1909.10893, 2019.\n\nA. Goyal, A. Lamb, P. Gampa, P. Beaudoin, S. Levine, C. Blundell, Y. Bengio, and M. Mozer. Object files and schemata: Factorizing declarative and procedural knowledge in dynamical systems. arXiv preprint arXiv: Arxiv-2006.16225, 2020.\n\nA. Goyal, A. Didolkar, N. R. Ke, C. Blundell, P. Beaudoin, N. Heess, M. Mozer, and Y. Bengio. Neural production systems. CoRR, abs/2103.01937, 2021a. URL https://arxiv.org/ abs/2103.01937.\n\nA. Goyal, A. Didolkar, N. R. Ke, C. Blundell, P. Beaudoin, N. Heess, M. Mozer, and Y. Bengio.\n\nNeural production systems. arXiv preprint arXiv:2103.01937, 2021b.\n\nK. Greff, S. van Steenkiste, and J. Schmidhuber. Neural expectation maximization, 2017.\n\nK. Greff, R. L. Kaufman, R. Kabra, N. Watters, C. Burgess, D. Zoran, L. Matthey, M. Botvinick, and A. Lerchner. Multi-object representation learning with iterative variational inference, 2019.\n\nD. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent\n\nimagination. arXiv preprint arXiv:1912.01603, 2019.\n\nD. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. In\n\nInternational Conference on Learning Representations, 2020.\n\nG. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural\n\nComputation, 18:1527–1554, 2006.\n\nJ. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans. Cascaded diffusion models\n\nfor high fidelity image generation. J. Mach. Learn. Res., 23:47–1, 2022.\n\nE. Hoel. The overfitted brain: Dreams evolved to assist generalization. Patterns, 2(5):100244, 2021.\n\nD. Hoshen and M. Werman. Iq of neural networks. arXiv preprint arXiv:1710.01692, 2017.\n\nC. James. Raven. mental tests used in genetic studies: The performance of related individuals on tests mainly educative and mainly reproductive. Unpublished master’s thesis, University of London, 1936.\n\nT. Kipf, E. van der Pol, and M. Welling. Contrastive learning of structured world models, 2019.\n\nURL https://arxiv.org/abs/1911.12247.\n\nA. R. Kosiorek, H. Kim, I. Posner, and Y. W. Teh. Sequential attend, infer, repeat: Generative\n\nmodelling of moving objects, 2018.\n\nB. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman. Building machines that learn and\n\nthink like people. Behavioral and brain sciences, 40, 2017.\n\nQ. Li, S. Huang, Y. Hong, Y. Chen, Y. N. Wu, and S.-C. Zhu. Closed loop neural-symbolic learning In International\n\nvia integrating neural perception, grammar parsing, and symbolic reasoning. Conference on Machine Learning, pages 5884–5894. PMLR, 2020.\n\nZ. Lin, Y.-F. Wu, S. Peri, B. Fu, J. Jiang, and S. Ahn. Improving generative imagination in object-\n\ncentric world models. In International Conference on Machine Learning, 2020.\n\nF. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, and T. Kipf. Object-centric learning with slot attention. arXiv preprint arXiv: Arxiv2006.15055, 2020.\n\nM. Mitchell. Abstraction and analogy-making in artificial intelligence. Annals of the New York\n\nAcademy of Sciences, 1505(1):79–101, 2021.\n\nO. Ostapenko, P. Rodriguez, M. Caccia, and L. Charlin. Continual learning via local module com-\n\nposition. Advances in Neural Information Processing Systems, 34:30298–30312, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nG. Parascandolo, N. Kilbertus, M. Rojas-Carulla, and B. Sch ̈olkopf. Learning independent causal\n\nmechanisms. In International Conference on Machine Learning, 2018.\n\nJ. Pearl. Causality: Models, reasoning, and inference. 2nd edition. 2009.\n\nS. Reed and N. De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279,\n\n2015.\n\nL. Ruis and B. Lake. Improving systematic generalization through modularity and augmentation.\n\narXiv preprint arXiv:2202.10745, 2022.\n\nE. S. Spelke. Core knowledge. American psychologist, 55(11):1233, 2000.\n\nK. Stelzner, R. Peharz, and K. Kersting. Faster attend-infer-repeat with tractable probabilistic mod-\n\nels. In International Conference on Machine Learning, 2019.\n\nJ. Su, D. V. Vargas, and K. Sakurai. One pixel attack for fooling deep neural networks.\n\nIEEE\n\nTransactions on Evolutionary Computation, 23(5):828–841, 2019.\n\nS. van Steenkiste, M. Chang, K. Greff, and J. Schmidhuber. Relational neural expectation maxi-\n\nmization: Unsupervised discovery of objects and their interactions, 2018.\n\nS. van Steenkiste, K. Greff, and J. Schmidhuber. A perspective on objects and systematic generalization in model-based RL. CoRR, abs/1906.01035, 2019. URL http://arxiv.org/abs/ 1906.01035.\n\nC. Zhang, F. Gao, B. Jia, Y. Zhu, and S.-C. Zhu. Raven: A dataset for relational and analogical\n\nvisual reasoning. In Conference on Computer Vision and Pattern Recognition, 2019.\n\nA APPENDIX\n\nA.1 MODEL\n\nController. The controller encodes a support set of I/O pairs S = {X, O} and outputs a task embedding z that is used to predict a task-specific output associated to a query input. We formulate the controller in an iterative manner: it starts with a random guess zinit (sampled from a learned gaussian) and refines it. We denote Refine(z, S) a refinement step and T the total number of refinement steps. A refinement step at timestep t can be decomposed into three main steps:\n\n• Step 1 - Current guess prediction: we compute the current predicted outputs ˆO associated\n\nwith each input in the support set s.t. ˆOt = {ComputeGuess(X, zt)}\n\n• Step 2 - Compare current guess ˆOt to ground-truth outputs O given their associated inputs\n\nin S. We denote at = Compare( ˆOt, S, zt) the output of this step.\n\n• Step 3 - Update current task embedding zt+1 = Update(at, zt)\n\nComputeGuess(X, zt) is model-specific and simply consists of predicting the current task output given zt. And zt+1 = Update(at, zt) is parametrized as a simple GRU RNN taking at as input and updating the hidden state zt. We then need to detail how we obtain at = Compare( ˆOt, S, , zt) This is done in two steps (1) - first we compute a representation bt i for each sample i in the support set, then, (2) we aggregate those representations to obtain at. To do so, each input xi ∈ X is transformed into an object-centric set of slots {si j}j using the same perception model as in the rest of the model. Each slot is then concatenated to the current guess zt, the ground-truth associated input oi and the currently predicted output ˆoi. The sample-wise result is then obtained with a simple GRU RNN. We denote this step by bt i = EncodeSample(xi, zt, oi, ˆoi) and the resulting sequence representation by bt = [bt\n\ni].\n\nWe then need to aggregate the obtained results accross the whole support set. To do so, we concatenate each sample representation bt i with the current task embedding zt, the ground-truth associated input oi and the currently predicted output ˆoi. Similarly we aggregate the results with a simple GRU RNN. We denote at = EncodeSupport(bt, zt, O, ˆOt). The exact parametrization of each of the modules consituting the controller is given in Table 1.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nDescription\n\nSymbol\n\nTask embedding refiner Sample concatenation before aggregation Sample representation aggregation accross slots Sample-wise processing before aggregation Aggregation accross support elements\n\nUpdate(at, zt)\n\nht\n\nj(i). = Concat(si i = Agg([ht bt\n\nj, zt, oi, ˆoit) j(i)])\n\nat = EncodeSupport(bt, zt, O, ˆOt)\n\nArchitecture\n\nGRUCell(64, 64) MLP(128 + 10, 64) GRU(64, 64) MLP(64, 64) GRU(64, 64)\n\nTable 1: Controller Hyperparameters\n\nPerception. The perception component is implemented with a Slot Attention module. We keep the same hyperparameters as the initial version (Locatello et al., 2020) with a slot-wise dimension of 64 and using different Gaussian parameters for each slot. We perform all the experiments with 4 slots.\n\nSelection Bottleneck. The selection bottleneck is composed of three parts: (1) the module that transforms a single task embedding z into a sequence of embeddings from which the step-wise elements of the neural program will be predicted, (2) the extraction of keys, queries, and values for the condition and the module selection, and (3) the step-wise gate prediction to determine the number of updates to perform (e.g. neural program length). We detail the parameterizations of these three parts in Table 2\n\nDescription\n\nSequence prediction Gate prediction\n\nSymbol/Notation\n\n[z1, ...zT ] = Pred(z) gt = Gate(zt)\n\nQuery prediction for module/condition selection Qr(zt) and Qc(zt)\n\nLearned Module/Condition Key embedings Max number of steps Gumbel softmax temperature\n\nM and C T\ntemp\n\nArchitecture/Value\n\nGRU(64, 64) σ(MLP(64, 1)) MLP(64, 64), MLP(64, 64) Embeding(Nr, 64) and Embeding(Nc, 64) 2\n3\n\nTable 2: Selection Bottleneck Hyperparameters\n\nExecutor. The executor is composed of three main parts: (1) the learned neural program library (e.g. modules and condition vectors), (2) the argument selection part given a condition vector, and the (3) task-related output prediction. We detail the parameterization of these part in Table 3.\n\nDescription\n\nModules Condition vectors Condition/Modules keys Key prediction for argument selection Output prediction Number of conditions/modules\n\nSymbol/Notation\n\n[m1, ...mNr ] [c1, ...cNc] C and M Ka(S) y = TaskPred(hT ) Nc and Nr\n\nArchitecture/Value\n\n[GRU(64, 64)] Embedding(Nc, 64) Embedding(Nc, 64) and Embedding(Nr, 64) MLP(64, 64) σ(MLP(64, num bits)) 3 and 3\n\nTable 3: Execution component Hyperparameters\n\nA.2 TRAINING\n\nThe training of the whole model can be split into three phases:\n\n• Step 1: Pretraining of the perception model such that the next steps start with reasonable\n\nlatent slots.\n\n• Step 2: Regular training on the task prediction objective (here 8-binary-bits digit predic-\n\ntion).\n\n• Step 3: Imagination, where random modules and conditions are sampled to create new data\n\npoints and expose the model to potentially OOD samples.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nEach of these steps gives rise to a specific objective loss to optimize, namely: Lrec, Ltask, and Lim. The different phases consist of adding progressively these losses to the optimized objective. The total loss is:\n\nL = Ltask + αLrec + βLim\n\nThe pretraining of the slot attention modules (Step 1) is done separately and we initialize it with the pretrained weights when adding the task-specific loss at Step 2. The coefficient α is fixed, whereas β is introduced after a warm-up period and increased linearly during a certain number of epochs. We report these training-specific hyperparameters in Table 4\n\nDescription\n\nSymbol/Notation\n\nValue\n\nReconstruction coefficient Imagination coefficient Warmup period before imagination Imagination coef schedule Batch size Learning rate\n\nα β\n\nlr\n\n0.0002 50 450 epochs from 0 to β in 200 epochs 32 2e−4\n\nTable 4: Training Hyperparameters\n\nDataset For all the different splits, we trained OCIM on 5000 samples where each sample had 10 I/O pairs in the support set and 1 I/O query example. Validation and Test set are composed of 100 and 1000 samples respectively. We describe below the generating steps of the three different splits we considered in the experiments:\n\n• OOD op: models are trained on sequences of operations {[+], [−], [+−], [++], [−−]} and evaluated on [−+]. The model is also trained and evaluated on one order only: top left, top right, then bottom. Digits, signs and colors are sampled randomly iid.\n\n• OOD order:\n\nmodels\n\noperations {[+], [−], [+−], [++], [−−], [−+]} and on one object order (top left, top right, then bottom); they are then evaluated on all operations, and all possible orders in which the visual digits can be considered.\n\nsequences\n\ntrained\n\nare\n\non\n\nall\n\nof\n\n• OOD percep: for each digit, only two (sampled in a uniform way) possible positive colors and two negative colors are considered during training. The models are evaluated on the left-out color configurations. Models are trained and evaluated on all sequences of operations{[+], [−], [+−], [++], [−−], [−+]}\n\n14",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a method for imagination (data augmentation) to improve the generalisation of abstract reasoning approaches. The general module is based on adapting slot-attention to work over a sequence of to select the latent function to apply. The augmentation works using Gumbel soft max trick to select the augmentation. The authors additionally propose a new dataset of mathematical problems with values, colours and operations to be resolved. They compare to the prior approach on NPS overfitting to training but improving generalisation on test.\n\n# Strength And Weaknesses\n\nStrengths:\n- The proposal of the imagination framework is interesting working within the latent space of the model, which should provide increased generalisation without being limited to generation.\n- The approach is fully differentiable that is and advantage against some prior methods for exutors.\n- The dataset can be useful if well-defined against prior datasets and models\nWeaknesses:\n- The augmentation strategy can be broadly compatible with data augmentation strategies which don't seem to have been considered in related work or to inspire the approach. \n- The data augmentation is loosely out of distribution, as the numerical dataset has a relatively limited scope with a simple range of numbers and operations. As the explicit function of the functions is unknown, and the logical operations can be deduced as a combinational operation that is unclear if possible in the leave-out strategy.\n- It would have been interesting for the approach to be applied to standard Abstract reasoning datasets such as RAVEN. While they are different problems, they could have shown greater generalisation and provided insights into how the model is working. \n- Similar to the above, the dataset seems highly similar to RAVEN and others, just numbers instead of shapes. It isn't clear where this dataset contributes.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is generally well written, however, the notation in Eq.3 and 4 should probably be revised to include T-1 to avoid confusion about the current and prior steps in RNN/GRU.\n\n# Summary Of The Review\n\nWhile the paper works on a challenging problem and shows generalisation across unseen problems based on their proposed baselines. The lack of rigorous evaluation and benchmarks from similar problems means the paper feels incomplete. Additional experiments or justification for their lack of inclusion needs to be provided to position this paper.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLASER: LATENT SET REPRESENTATIONS FOR 3D GENERATIVE MODELING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNeural Radiance Field (NeRF) provides unparalleled fidelity of novel view synthesis—rendering a 3D scene from an arbitrary viewpoint. NeRF requires training on a large number of views that fully cover a scene, which limits its applicability. While these issues can be addressed by learning a prior over scenes in various forms, previous approaches have been either applied to overly simple scenes or struggling to render unobserved parts. We introduce Latent Set Representations for NeRF-VAE (LASER-NV)—a generative model which achieves high modelling capacity, and which is based on a set-valued latent representation modelled by normalizing flows. Similarly to previous amortized approaches, LASER-NV learns structure from multiple scenes and is capable of fast, feed-forward inference from few views. To encourage higher rendering fidelity and consistency with observed views, LASER-NV further incorporates a geometry-informed attention mechanism over the observed views. LASER-NV further produces diverse and plausible completions of occluded parts of a scene while remaining consistent with observations. LASER-NV shows state-of-the-art novel-view synthesis quality when evaluated on ShapeNet and on a novel simulated City dataset, which features high uncertainty in the unobserved regions of the scene.\n\n1\n\nINTRODUCTION\n\nProbabilistic scene modelling aims to learn stochastic models for the structure of 3D scenes, which are typically only partially observed (Eslami et al., 2018; Kosiorek et al., 2021; Burgess et al., 2019). Such models need to reason about unobserved parts of a scene in way that is consistent with the observations and the data distribution. Scenes are usually represented as latent variables, which are ideally compact and concise, yet expressive enough to describe complex data.\n\nSuch 3D scenes can be thought of as projections of light rays onto an image plane. Neural Radiance Field (NeRF, (Mildenhall et al., 2020)) exploits this structure explicitly. It represents a scene as a radiance field (a.k.a. a scene function), which maps points in space (with the corresponding camera viewing direction) to color and mass density values. We can use volumetric rendering to project these radiance fields onto any camera plane, thus obtaining an image. Unlike directly predicting images with a CNN, this rendering process respects 3D geometry principles. NeRF represents scenes as parameters of an MLP, and is trained to minimize the reconstruction error of observations from a single scene—resulting in unprecedented quality of novel view synthesis.\n\nFor generative modelling, perhaps the most valuable property of NeRF is the notion of 3D geometry embedded in the rendering process, which does not need to be learned, and which promises strong generalisation to camera poses outside the training distribution. However, since NeRF’s scene representations are high dimensional MLP parameters, they are not easily amenable to generative modelling (Dupont et al., 2022). NeRF-VAE (Kosiorek et al., 2021) embeds NeRF in a generative model by conditioning the scene function on a latent vector that is inferred from a set of ‘context views’. It then uses NeRF’s rendering mechanism to generate outputs. While NeRF-VAE admits efficient inference of a compact latent representation, its outputs lack visual fidelity. This is not surprising, given its simple latent structure, and the inability to directly incorporate observed features. In addition, NeRF-VAE does not produce varied samples of unobserved parts of a scene.\n\nA number of recent deterministic methods (Yu et al., 2021; Trevithick & Yang, 2021; Wang et al., 2021) uses local image features to directly condition radiance fields in 3D. This greatly improves\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Left: LASER-NV infers a set-valued latent Z from the context V that consists of N image and camera pairs (In, Cn). On querying the scene function at a point xi with direction di, the latents are combined with local features Hn that are back-projected from the context views—producing color and density. Right: Rendering a novel viewpoint may include observed (green, an example query point on the left) and unobserved parts (red) of the scene. Conditioned on the context V, LASER-NV allows sampling multiple scene completions that are consistent with the context views (green arrows) while providing varied explanations for the unobserved parts of the scene (red arrows). We show two such samples for the same target camera Ct. Also see the gif in supp. material showing a fly-through for four prior samples conditioned on the same views.\n\nreconstruction quality of observed parts of the scene. However, these methods are still unable to produce plausible multimodal predictions for the unobserved parts of a scene.\n\nIn this work, we address NeRF-VAE’s shortcomings by proposing Latent Set Representations for NeRFVAE (LASER-NV). To increase modelling capacity, LASER-NV uses an arbitrarily-sized set of latent variables (instead of just one vector) modelled with normalizing flows. To further enable producing samples which are consistent with observed parts, we make the generative model conditional on a set of context views (as opposed to conditioning only the approximate posterior). LASER-NV offers superior visual quality with the ability to synthesise multiple varied novel views compatible with observations. Figure 1 shows LASER-NV’s key components and abilities. We include a gif in supp. material showing fly-throughs of additional prior samples, see Section 4.5 for details.\n\nOur contributions are as follows:\n\n• We introduce a novel set-valued latent representation modelled by purpose-built permutationinvariant normalizing flows conditioned on context views. We show that increasing the number of latent set elements improves modelling performance, providing a simple way to trade off computation for quality without adding new model parameters. We also verify that increasing latent dimensionality in NeRF-VAE offers no such benefits. In contrast with deterministic scene models in the literature, our probabilistic treatment over the latent set allows covering multiple models when predicting novel views.\n\n• We develop a novel attention mechanism to condition the scene function on the set-valued latent as well as additional local features computed from context views. We show that including local features further improves visual quality.\n\n• We evaluate LASER-NV on three datasets: a category-agnostic ShapeNet dataset, MultiShapeNet, and on a novel “City” dataset that contains a large simulated urban area and poses significant challenges as a benchmark for novel view synthesis due to high uncertainty in the unobserved parts of the scene. Our model overcomes some of the main limitations of NeRF-VAE and also outperforms deterministic NeRF models on novel view synthesis in the face of uncertainty.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2 BACKGROUND: NERF & NERF-VAE\n\nNeural Radiance Field (NeRF, (Mildenhall et al., 2020)) represents a 3D scene as a scene function F (x, d) of a point coordinate x ∈ R3 and direction d ∈ R3, and outputs the colour c ∈ R3 and volume density σ ≥ 0. This function is parameterized using a neural network (typically a fully connected MLP). To obtain a pixel colour, the scene function is evaluated at many points along the corresponding ray in the volumetric rendering process.\n\nThis volumetric rendering integral is in practice approximated using numerical integration; see (Mildenhall et al., 2020; Blinn, 1982) for details. We use ˆI = render(F (·), C) to denote the image rendering process that outputs the image ˆI ∈ RH×W ×3 for the rays of a camera C and a scene function F . The camera C = (K, R, t) is specified by its intrinsic parameters K ∈ R3×3 and extrinsic parameters given by a rotation R ∈ R3×3 and translation t ∈ R3. In its original formulation, the parameters of NeRF are optimized by minimizing an error function (typically the mean-squared error) between rendered images and the ground-truth images across many views of a single scene.\n\nWhile NeRF learns high-fidelity scene representations when many views are available, it does not learn a prior over scenes, does not provide compact scene representations, and does not admit efficient inference (i.e. fast estimation of the scene parameters from a few input views). To address these issues, NeRF-VAE (Kosiorek et al., 2021) embeds NeRF as a decoder in a variational auto-encoder (VAE, (Kingma & Welling, 2014; Rezende et al., 2014)). Each scene is represented by a latent vector z (instead of the set of parameters of an MLP as in NeRF) which can be either sampled from a Gaussian prior p(z) or from the approximate posterior q(z | C) inferred from a set of context views and associated cameras C. The latent z is used to condition an MLP that parameterizes the scene function, Fθ(·, z) : (xi, di) (cid:55)→ ci, σi. Here, parameters θ are shared between scenes, while z is scene specific. NeRF-VAE uses a Gaussian likelihood p(I | z, C) with the image rendered from the sampled scene function as mean, and a fixed standard deviation.\n\n3 LATENT SET REPRESENTATIONS FOR NeRF-VAE\n\nLASER-NV is a conditional generative model, conditioned on a set of input views V := {I n, Cn}N n=1. Given these views the model infers a latent representation that can be used for novel view synthesis.\n\nIn order to represent large-scale and complex scenes we use Latent Set Representations (LASERs), a set of latents of the form Z := {zk}K k=1, where K is a hyperparameter. In the NeRF-VAE, the scene function MLP is directly conditioned on the inferred latent. In contrast, LASER-NV’s scene function is conditioned on the latent set Z and features H := {H n}N n=1 extracted from N input views, and integrates information from both. This results in the form Fθ(·, Z, H) : (xi, di) (cid:55)→ ci, σi, which we detail further in Section 3.2.\n\nWe obtain the input features H by separately encoding the input views using a convolutional neural net (CNN) that maintains spatial structure. These feature maps are subsequently used in the conditional prior, the posterior, and the resulting scene function, all of which we describe next. Further architectural details are provided in Appendix A.\n\n3.1 CONDITIONAL PRIOR AND POSTERIOR\n\nGiven a set of context views V := {I n, Cn}N n=1 consisting of images I n and corresponding cameras Cn, LASER-NV defines a conditional prior p(Z|V) over a latent set of size K, parameterized by a permutation-invariant normalizing flow— a distribution model that allows for both fast and exact sampling and efficient density evaluation (Rezende & Mohamed, 2015; Papamakarios et al., 2021).\n\nFlows are defined by an invertible mapping f of random variables, starting from a base distribution (cid:0)Z (0)(cid:1). When f is composed of multiple invertible mappings f1, . . . , fm, the resulting density is p0 given by\n\np(Z | V) = p0\n\n(cid:0)f −1(Z, V)(cid:1)\n\ndet\n\nδfm\n\n(cid:0)Z (m−1), V(cid:1) δZ (m−1)\n\n(cid:12) −1 (cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\n(1)\n\nM (cid:89)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) m=1\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nOur flow design closely follows that of Wirnsberger et al. (2020; 2021), who parameterize the transformations in each layer using transformers, preserving permutation invariance. Specifically, we propose a novel design to condition the flow distribution on the features H extracted from the context views. To achieve that, we use split coupling layers (Dinh et al., 2017), where for each layer, we first apply self-attention to the set of latents, followed by cross-attention to attend the context features. The output of the second transformer defines the parameters for an invertible affine transformation of the random variables. Implementation details of our normalizing flow is in Appendix A.1.\n\nSince LASER-NV is trained as a VAE, we define an approximate posterior q(Z|U ∪ V), where U = {I t, Ct}M t=1 are target views that the model is trained to reconstruct, denoted as posterior context. The posterior and conditional prior are both distributions over the latent space, where the posterior is conditioned on the union of prior and posterior context. We model the posterior distribution using the same architecture as the conditional prior, but with a separate set of learned parameters.\n\n3.2 DECODER & SCENE FUNCTION\n\nWhen evaluating the scene function, i. e. computing colour and density for a point xi, di, LASER-NV has access to both the latent set Z as well as the features H extracted from the context views V. We now describe how to integrate those two, which consists of querying the latent set for each point, and using 3D projection to extract relevant features.\n\nQuerying the Latent Set While the latent set representation Z contains information regarding the entire scene, when querying for a specific point xi, we would like to extract only relevant information from Z. We propose using a transformer cross-attention model to compute a single feature vector from Z. In the first layer, queries are computed from the positional encoding of xi, and keys and values are computed from the elements of Z. In subsequent layers, queries are computed from previous layers’ output. We denote the output of the latent querying as latent features ̃zi, each corresponding to a query point xi.\n\nLocal Features When synthesizing a novel view given a few input views, a number of existing NeRF-based methods uses 3D projection to determine which features from the context views are useful for explaining a particular 3D point (Yu et al., 2021; Trevithick & Yang, 2021; Wang et al., 2021). LASER-NV incorporates this idea, and in particular follows the design of pixelNeRF (Yu et al., 2021) as it is shown to achieve strong results in a wide range of domains.\n\nn=1. Given a query 3D point xi and direction di, we obtain the point pn\n\nWe use the encoded input views {H n}N n=1 described above, along with their corresponding camera i ∈ R2 in the matrices {Cn}N image space of the nth context view using the known intrinsic parameters. The corresponding local features are then bilinearly interpolated hn i = H n[pn i ]. We refer to hi as local features since they are spatially localized based on the projected coordinates, as opposed to being a global aggregation of all context features. We finally obtain ˆhn i for each context view by separately processing the features hn i using a residual MLP that has as an additional conditioning on the image-space projected points and directions.\n\nIntegrating Latents and Local Features LASER-NV integrates the both local and the latent features. It does so with a transformer:\n\nIn contrast to pixelNeRF, which does not have latents,\n\nfi = Mθ( ̃zi;\n\nN (cid:91)\n\n{hn\n\ni : is_visible(xi, Cn)} ∪ { ̃zi})\n\n(2)\n\nn=1\n\nThe attention queries are computed from latent features ̃zi (see above), and attend to the combined set of local features and to ̃zi. Note that, for a point xi, we use only those context views from which this point is visible. Using a transformer to process encoded features from multiple views with epipolar constraints has been proposed in prior work on Multi-view Stereo, see e.g. (He et al., 2020; Xi et al., 2022; Wang et al., 2022; Ding et al., 2022).\n\nThe final step in evaluating the scene function is to use fi to condition a residual MLP similar to NeRF-VAE resulting in LASER-NV’s scene function Fθ(·, Z, H) : (xi, di) (cid:55)→ ci, σi which is used to volume-render image outputs.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.3 LOSS\n\nSimilarly to NeRF-VAE, our objective function is given by the evidence lower-bound:\n\nL(U, V) = EZ∼q\n\nlog p(cid:0)I t (cid:12)\n\n(cid:12) Ct, Z, V(cid:1)\n\n(cid:35)\n\n(cid:34) M (cid:88)\n\nt=1\n\n− KL(q(Z | U ∪ V) || p(Z | V))\n\n(3)\n\nwhere I t and Ct are the images and cameras from the posterior context views U. We use a Gaussian likelihood term for p(I t | Ct, Z, V) with a fixed variance.\n\n4 EXPERIMENTS\n\n4.1 DATASETS\n\nWe first evaluate LASER-NV on a novel dataset based on a set of synthetic, procedurally generated, environments consisting of urban outdoor scenes. This dataset contains posed images of a virtual city with objects of multiple scales, such as house blocks, roads, buildings, street lights, etc (but no transient objects such as cars or people). As such, it is an extremely challenging scene representation benchmark, especially when representations are inferred from few views only—resulting in partial observability and high uncertainty. In this situation, when evaluated on unobserved parts of a scene, we want a model to produce predictions that are varied, self-consistent, and aligned with the data distribution. Each scene consists of a neighborhood block, and a randomly chosen designated point of interest (road intersection or park) is selected within. Images are then rendered by placing the camera at random nearby positions and viewpoints from the point of interest. The dataset has 100K training scenes, and 3200 test scenes. We provide further details and dataset samples in Appendix B.\n\nTo test LASER-NV’s ability to produce near-deterministic scene completions in a low-uncertainty setting, we compare it against a number of previous methods on the ShapeNet NMR dataset (Kato et al., 2018), which consists of several views of single objects from 13 different ShapeNet categories. Finally, we evaluate on the more challenging MultiShapeNet-Hard (MSN-Hard) (Sajjadi et al., 2021), which tests the models capabilities in the presence of a large number of cluttered objects of varying size with detailed textures and realistic backgrounds.\n\n4.2 MODELS & EVALUATION\n\nOn the City dataset, we compare LASER-NV with Multi-view Conditional NeRF (MVC-NeRF) and NeRF-VAE; see Figure 7 in Appendix A for model diagrams. MVC-NeRF (Figure 7c) is a variant of LASER-NV (Figure 7a) which does not use LASER-NV’s latent set representations but instead only relies on deterministically rendering new views using local features. It therefore closely resembles the design of pixelNeRF but uses the same architecture for the image encoder and scene function Fθ as LASER-NV to ensure they are comparable. On ShapeNet, we further show quantitative results of a number of non-NeRF novel view synthesis methods: Scene Representation Networks (SRN, Sitzmann et al. (2019)), Differentiable Volumetric Rendering (DVR, Niemeyer et al. (2020)), and Scene Representation Transformers (SRT, Sajjadi et al. (2021)). During training, LASER-NV uses a random subset of 4 of the 23 target views as posterior context views1. MVC-NeRF and NeRF-VAE are trained to directly predict the 23 views given a single context view. At test time, LASER-NV samples the latents from its conditional prior whereas NeRF-VAE uses its posterior (given that its prior is unconditional). Importantly, all models in this evaluation use one input view and predict 23 target views.\n\nAt evaluation time, we feed each model with a set of context views, and evaluate reconstructions of those same views, and predictions of novel views of the same scene. We evaluate reconstruction ability via likelihood estimates, and standard image similarity metrics PSNR, and SSIM (Wang et al., 2004). For novel view predictions we report the Fréchet Inception Distance (FID, Heusel et al. (2017)) between the marginal distribution of samples and the evaluation dataset2. This metric\n\n1While using the 23 views as posterior context would be the standard set up (Bayer et al., 2021), we find that\n\nusing 4 randomly-sampled views is much cheaper computationally and yet sufficient for training.\n\n2Image similarity metrics such as PSNR and SSIM are not useful when there exists a large multimodal space\n\nof possible predictions as is the case in the City.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nhighlights the incapability of deterministic models to generate varied and plausible samples in face of uncertainty. Note that since NeRF-VAE is unconditional, we use the learned approximate posterior as the conditional distribution. In the City, we thus train a separate model (one to use for reconstruction metrics that has 8 posterior context views and another one with 2 views for predictions). For ShapeNet results all models use one context view.\n\nOn MSN-Hard, we train LASER-NV with 5 prior context views and 3 target views. We report the test PSNR of 1 novel view given 5 input views, and compare with the results reported in Sajjadi et al. (2022). The methods of comparison include two amortized non-NeRF models based on light fields, Scene Representation Transformers (SRT) and Object SRT (OSRT Sajjadi et al. (2022)); and two amortized NeRF methods, pixelNeRF and Volumetric OSRT (VOSRT).\n\n4.3 ABLATIONS\n\nWe further ablate LASER-NV to isolate contributions of individual model components. To quantify the effects of conditioning the prior we compare LASER-NV to a conditional version of NeRF-VAE. Note that the original NeRF-VAE used an unconditional prior (Kosiorek et al., 2021). We also compare LASER-NV to a version that does not condition the scene function with local context features, cf. Equation (2) and Figure 7b in Appendix A, denoted by LASER-NV (NO GEOM.). LASER-NV, LASER-NV (NO GEOM.), NeRF-VAE variants, and MVC-NeRF share architectures for the image encoder and the scene function Fθ.\n\n4.4 TRAINING AND IMPLEMENTATION DETAILS\n\nWe use the hierarchical sampling technique of (Mildenhall et al., 2020) when rendering a pixel colour, which involves learning two separate sets of scene function parameters as well as optimizing a corresponding colour likelihood term for each step. For experiments with the City, we leverage ground-truth depth maps using the method proposed by Stelzner et al. (2021) in order to train all the models with fewer scene function evaluations per ray. Not using ground-truth depth significantly increases training time while only marginally lowering reconstruction PSNR, see Appendix D for a discussion and Fig. 16 in Appendix B for visualisations. Importantly, when evaluating our models we revert to volumetric rendering. All generative models are trained by optimizing the evidence lower bound; where we anneal the KL term in Eq. (3). All experiments use the Adam (Kingma & Ba, 2014) optimizer. We provide full details of model architectures in Appendix A and training procedures in Appendix E.\n\n4.5 RESULTS\n\nResults on the City dataset are shown in Table 1. In terms of reconstructing observed input views, both LASER-NV and MVC-NeRF, due to the use of explicit geometrical knowledge, result in excellent reconstructions. NeRF-VAE, however, needs to reconstruct input views purely from its latent representation, resulting in low fidelity reconstructions. For prediction of unseen views, the output distribution of LASER-NV is closer to the actual evaluation data (lower FID), in contrast to MVC-NeRF which outputs a single prediction, and NeRF-VAE which produces low quality outputs that do not vary much. We show representative example reconstructions and predictions in Fig. 2. We also include a gif3 in supp. material showing fly-throughs of model samples.\n\nResults on ShapeNet are shown in Table 2. Both MVC-NeRF and LASER-NV outperform existing published methods when predicting novel views of these single-object scenes. We further observe low quality predictions of NeRF-VAE in example predictions in Fig. 11. Finally, Fig. 12 shows a hand-selected example where the input view has ambiguity and how LASER-NV can sample plausible variations. As in the City experiment, LASER-NV produces plausibly varied predictions of the object in contrast to the other methods.\n\nResults on MSN-Hard are shown in Table 3. LASER-NV clearly outpeforms the NeRF-based models, and improves over OSRT and the original SRT. Only a modified SRT shows higher PSNR (25.93) over LASER-NV (24.45). Note that SRT does not learn to estimate densities nor depth maps and\n\n3To create this visualisation we trained the model on a modified City dataset with birds-eye views.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nReconstruction\n\nPrediction\n\nLog-Likelihood ↑\n\nPSNR↑\n\nSSIM↑\n\nMVC-NeRF Cond. NeRF-VAE NeRF-VAE LASER-NV (NO GEOM.) LASER-NV\n\n- 3.51 3.50 3.82 3.83\n\n28.63 23.94 23.92 27.13 29.61\n\n0.910 0.709 0.708 0.870 0.923\n\nFID↓\n\n91.92 50.98 52.12 24.19 22.54\n\nTable 1: Results in City. The color likelihood is computed via importance sampling (10 samples); note that MVC-NeRF is deterministic and thus we do not compare it on this metric. Reconstruction: We report performance of reconstruction of the 2 context views. Prediction: We evaluate the models on previously unobserved views using the FID score. MVC-NeRF provides a single deterministic prediction whereas for the other models we average over 10 independent samples.\n\nFigure 2: Representative reconstructions of observed views and predictions of partially unobserved novel views. The models are conditioned on two context views (top left). The first two columns show reconstructions of those views. Note how NeRF-VAE struggles to reconstruct both views accurately, while the other models do well. We show predictions of two novel views (top right), where the first view is partially observed in the context (highlighted in cyan color) and the second view is fully unobserved. LASER-NV’s predictions are consistent with the observed part of the context, diverse, and plausibly so. NeRF-VAE’s predictions lack quality and diversity. MVC-NeRF’s only produces a single prediction, which is not very plausible.\n\nis deterministic. Fig. 3 shows a prediction example using LASER-NV, and further examples are in Appendix C which demonstrate high fidelity colours and densities.\n\n4.6 TRAIN- AND TEST-TIME SCALING & DATA EFFICIENCY\n\nLatent set representations allow trading computation for increased model capacity. We train LASERNV on the City dataset with increasing latent set sizes K, and compare it to a conditional NeRF-VAE also trained with increasing latent dimensions4. Figure 4 shows reconstruction log-likelihoods as a\n\n4Note that this increases the number of NeRF-VAE’s parameters.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nDVR*\n\nSRN*\n\nSRT*\n\npixelNeRF*\n\nNeRF-VAE MVC-NeRF\n\nLASER-NV\n\nPSNR ↑ SSIM ↑\n\n22.70 0.860\n\n23.28 0.849\n\n27.87 0.912\n\n26.80 0.910\n\n25.36 0.875\n\n28.03 0.925\n\n27.92 0.923\n\nTable 2: Category-agnostic ShapeNet evaluation by reconstructing 23 views from one input view for all models. We further report numbers from the respective papers for DVR* (Niemeyer et al., 2020), SRN* (Sitzmann et al., 2019), SRT* (Sajjadi et al., 2021), and pixelNeRF* (Yu et al., 2021).\n\nNeRF\n\nLight field\n\npixelNeRF VOSRT\n\nLASER-NV OSRT\n\nSRT\n\nSRT++\n\nPSNR ↑\n\n21.97\n\n21.38\n\n24.45\n\n23.33\n\n23.54\n\n25.93\n\nTable 3: Novel view PSNR of one view from 5 input views on MSN-Hard. PixelNeRF results are from Sajjadi et al. (2021) and the remaining from Sajjadi et al. (2022). SRT++ is an improved SRT (Sajjadi et al., 2022).\n\nFigure 3: Novel view syntesis with LASER-NV on MSN-Hard.\n\nfunction of latent capacity. While LASER-NV consistently improves its reconstructions with larger latent sets, cond. NeRF-VAE quickly saturates at 256 latent dimensions, at worse reconstructions compared to LASER-NV. Further, in Appendix D, we provide an analysis of test-time scaling—we show that increasing the latent set size at test time does not result in clear change of performance. Finally, in Figure 14 we show that LASER-NV performs better at test time than NeRF-VAE and MVC-NeRF for different training dataset sizes.\n\n5 RELATED WORK\n\nNeural scene representations Inferring neural scene representations using a deep generative model of scenes is an active research area, and we mention some of the most closely related approaches below. NeRF-VAE learns a generative model of 3D scenes which is only shown to work in relatively simple scenes. When applied to more complicated data (see Fig. 2), it produces blurry reconstructions and novel views. We posit that this is caused by insufficient capacity of its latent representation z. Generative Query Network (Eslami et al., 2018) similarly learns a distribution over scenes, but its decoder is a CNN. Subsequent work (Rosenbaum et al., 2018) improves over its representational capacity by using an attentive mechanism when conditioning the decoder from context in an analogous way to how LASER-NV improves over NeRF-VAE. By implementing a NeRF in our decoder we gain the benefits of more robust generalization capabilities to novel viewpoints compared to the convolutional counterparts (Kosiorek et al., 2021). Simone (Kabra et al., 2021) is a convolutional generative model that performs high quality novel view synthesis and while also performing unsupervised segmentation of input videos. The authors in (Dupont et al., 2022) propose directly modelling the underlying radiance fields as functions, which they refer to as ‘functa’. Both our method and theirs learn a prior over scene functions with a model that has a large representational capacity. Bautista et al. (2022) propose a two-stage method to learn a generative model of 3D scenes. As LASER-NV, their model can conditionally sample radiance fields. However, they focus on scenes with dense views, whereas LASER-NV works with sparse views.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) LASER-NV\n\n(b) Conditional NeRF-VAE\n\nFigure 4: Training reconstruction performance in City of LASER-NV with an increasing latent set size (each with 128 dimensions) vs a cond. NeRF-VAE with increasing latent dimensionality.\n\nDeterministic novel view synthesis NeRF relies on optimizing the parameters of an MLP scene function, separately for every scene. A number of methods in the literature propose amortizing the mapping of input views to the scene function using multi-view geometry, see e.g. IBRNet (Wang et al., 2021), General Radiance Fields (GRF) (Trevithick & Yang, 2021) and pixelNeRF (Yu et al., 2021). These methods extract features of the context views by using projective geometry to select which features to attend to, and are shown to generalize well and carry out high-fidelity predictions. PixelNeRF is designed to work well in a solely view-centered coordinate system. MVSNerf (Chen et al., 2021) lifts context features to a 3D representation with strong novel-view synthesis performance. Scene Representation Transformers (SRT) (Sajjadi et al., 2021) use global features obtained from context views using a generalization of the Vision Transformer (Dosovitskiy et al., 2020), which bears similarities to LASER-NV. SRT bypasses the use of 3D geometry entirely, yet shows remarkable performance on real world data.\n\nNeRF for large-scale scenes A number of NeRF methods have been developed to scale up to city-wide scenes. They work by optimizing multiple separate scene functions for separate blocks of a scene, which are then dynamically used when rendering (Tancik et al., 2022; Turki et al., 2021). Alternatively they design a scene function that represents the scene at multiple levels of scale (Martel et al., 2021; Xiangli et al., 2021). Similarly, Mip-NeRF (Barron et al., 2021) allows prefiltering the scene function inputs to better capture coarse and fine details. These methods are orthogonal to ours since they do not learn a conditional NeRF model sparse views nor address the uncertainty in predictions.\n\n6 CONCLUSION\n\nWe propose LASER-NV, a conditional generative model of neural radiance fields capable of efficient inference of large and complex scenes under partial observability conditions. While recent advances in neural scene representation and neural rendering lead to unprecedented novel view synthesis capabilities, producing sharp and multi-modal completions of unobserved parts of the scene remains a challenging problem. We experimentally show that LASER-NV can model scenes of different scale and uncertainty structure, and isolate the usefulness of each contribution through ablation studies. While NeRF serves as a strong inductive bias for learning 3D structure from images, LASER-NV also inherits some of its drawbacks. Volumetric rendering with neural radiance fields is computationally costly and can limit the model from real-time rendering. It remains to be seen whether fast NeRF implementations (Müller et al., 2022) can be of help. Furthermore, accurate GT camera information is required for learning and novel view synthesis; (Moreau et al., 2022) recently made progress on localization methods using NeRF. While learning a generative scene model of real scenes remains an open problem, we believe this work is an important step in that direction. Finally, an interesting direction for future research is incorporating object-centric structure and dynamics into LASER-NV, which might be useful for for downstream reasoning tasks (Sajjadi et al., 2022; Ding et al., 2021).\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nJonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. In International Conference on Computer Vision, 2021.\n\nMiguel Angel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin Dehghan, and Josh Susskind. GAUDI: A Neural Architect for Immersive 3D Scene Generation. In Advances in Neural Information Processing Systems, 2022 arXiv/2207.13751.\n\nJustin Bayer, Maximilian Soelch, Atanas Mirchev, Baris Kayalibay, and Patrick van der Smagt. Mind the Gap when Conditioning Amortised Inference in Sequential Latent-Variable Models. In International Conference on Representation Learning, 2021 arXiv/2101.07046.\n\nChristopher M. Bender, Kevin O’Connor, Yang Li, Juan Jose Garcia, Junier Oliva, and Manzil Zaheer. Exchangeable Generative Models with Flow Scans. In AAAI Conference on Artificial Intelligence, 2020.\n\nJames F. Blinn. Light Reflection Functions for Simulation of Clouds and Dusty Surfaces.\n\nIn\n\nSIGGRAPH, 1982.\n\nChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. MONET: Unsupervised scene decomposition and representation. arXiv/1901.11390, 2019.\n\nAnpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In International Conference on Computer Vision, 2021 arXiv/2103.15595.\n\nDavid Ding, Felix Hill, Adam Santoro, Malcolm Reynolds, and Matt Botvinick. Attention over learned object embeddings enables complex visual reasoning. Advances in Neural Information Processing Systems, 34, 2021.\n\nYikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and Xiao Liu. Transmvsnet: Global context-aware multi-view stereo network with transformers. In Conference on Computer Vision and Pattern Recognition, 2022.\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In\n\nInternational Conference on Representation Learning, 2017 arXiv/1605.08803.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nEmilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Jimenez Rezende, and Dan Rosenbaum. From data to functa: Your data point is a function and you should treat it like one. ArXiv, arXiv/2201.12204, 2022.\n\nSM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Garnelo, Avraham Ruderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene representation and rendering. Science, (6394):1204–1210, 2018. 360.\n\nYihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu. Epipolar transformers. In Conference on\n\nComputer Vision and Pattern, 2020.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017.\n\nRishabh Kabra, Daniel Zoran, Goker Erdogan, Loic Matthey, Antonia Creswell, Matt Botvinick, Alexander Lerchner, and Chris Burgess. Simone: View-invariant, temporally-abstracted object representations via unsupervised video decomposition. Advances in Neural Information Processing Systems, 34, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nHiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In Conference on\n\nComputer Vision and Pattern, 2018.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv/1412.6980,\n\n2014.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference\n\non Learning Representations, 2014.\n\nAdam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosália G. Schneider, Sovna Mokr’a, and Danilo Jimenez Rezende. Nerf-vae: A geometry aware 3d scene generative model. In International Conference on Machine Learning, 2021 arXiv/2104.00587.\n\nChen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. BARF: bundle-adjusting neural radiance fields. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, 2021.\n\nJulien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric Chan, Marco Monteiro, and Gordon Wetzstein. Acorn: Adaptive coordinate networks for neural scene representation. ACM Trans. Graph., 40:58:1–58:13, 2021.\n\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, 2020.\n\nArthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan Stanciulescu, and Arnaud de La Fortelle.\n\nLens: Localization enhanced by nerf synthesis. In Conference on Robot Learning, 2022.\n\nThomas Müller, Alex Evans, Christoph Schied, and Alexander Keller.\n\nInstant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1–102:15, July 2022.\n\nMichael Niemeyer, Lars M. Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable In 2020 volumetric rendering: Learning implicit 3d representations without 3d supervision. IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, 2020.\n\nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):1–64, 2021.\n\nRazvan Pascanu, Tomás Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural\n\nnetworks. In International Conference on Machine Learning, 2013 28.\n\nDanilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In\n\nInternational Conference on Machine Learning, 2015.\n\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and In International Conference on Machine\n\napproximate inference in deep generative models. Learning, 2014.\n\nDan Rosenbaum, Frederic Besse, Fabio Viola, Danilo J. Rezende, and S. M. Ali Eslami. Learning\n\nmodels for visual 3d localization with implicit mapping. CoRR, arXiv/1807.03149, 2018.\n\nMehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs M. Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas A. Funkhouser, and Andrea Tagliasacchi. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. arXiv/2111.13152, 2021.\n\nMehdi S. M. Sajjadi, Daniel Duckworth, Aravindh Mahendran, Sjoerd van Steenkiste, Filip Paveti’c, Mario Luvci’c, Leonidas J. Guibas, Klaus Greff, and Thomas Kipf. Object scene representation transformer. 2022 arXiv/2206.06922.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nVincent Sitzmann, M. Zollhöfer, and G. Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. In Advances in Neural Information Processing Systems, 2019. arXiv/1906.01618.\n\nKarl Stelzner, Kristian Kersting, and Adam R. Kosiorek. Decomposing 3d scenes into objects via\n\nunsupervised volume segmentation. In arXiv, 2021 arXiv/2104.01148.\n\nMatthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P. Srinivasan, Jonathan T. Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. ArXiv, arXiv/2202.05263, 2022.\n\nAlex Trevithick and B. Yang. GRF: Learning a general radiance field for 3d scene representation and rendering. In International Conference on Learning Representations, 2021. arXiv/2010.04595.\n\nHaithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-nerf: Scalable construction of\n\nlarge-scale nerfs for virtual fly-throughs. CoRR, arXiv/2112.10703, 2021.\n\nArash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. arXiv/2007.03898,\n\n2020.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 30.\n\nQianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P. Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas A. Funkhouser. IBRNet: Learning multi-view image-based rendering. In Conference on Computer Vision and Pattern Recognition, 2021.\n\nXiaofeng Wang, Zheng Zhu, Fangbo Qin, Yun Ye, Guan Huang, Xu Chi, Yijia He, and Xingang Wang. Mvster: Epipolar transformer for efficient multi-view stereo. arXiv preprint arXiv:2204.07346, 2022.\n\nZhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Trans. Image Process., 13(4):600–612, 2004.\n\nPeter Wirnsberger, Andrew J. Ballard, George Papamakarios, Stuart Abercrombie, Sébastien Racanière, Alexander Pritzel, Danilo Jimenez Rezende, and Charles Blundell. Targeted free energy estimation via learned mappings. The Journal of Chemical Physics, 2020.\n\nPeter Wirnsberger, George Papamakarios, Borja Ibarz, Sébastien Racanière, Andrew J Ballard, Alexander Pritzel, and Charles Blundell. Normalizing flows for atomic solids. arXiv preprint arXiv:2111.08696, 2021.\n\nJunhua Xi, Yifei Shi, Yijie Wang, Yulan Guo, and Kai Xu. Raymvsnet: Learning ray-based 1d implicit fields for accurate multi-view stereo. In Conference on Computer Vision and Pattern Recognition, 2022.\n\nYuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai,\n\nand Dahua Lin. Citynerf: Building nerf at city scale. ArXiv, arXiv/2112.05504, 2021.\n\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA MODEL ARCHITECTURES\n\nA.1 NORMALIZING FLOWS FOR LASER\n\nWe wish to model our latent set representation Z = {z1, . . . , zK} using a flexible distribution that is efficient both to evaluate as well as sample from. Furthermore, the distribution should ideally be invariant to re-ordering of the elements of the set. Normalizing flows (Rezende & Mohamed, 2015) allow specifying a distribution with precisely these properties. Formally, it is given by:\n\nZ (0) ∼ p0(Z) , Z (m) = fm\n\n(cid:16)\n\nZ (m−1), V\n\n(cid:17)\n\n, m = 1, . . . , M , with Z := Z (M ) ,\n\n(4)\n\nwhere p0(·) is a base distribution, fm(·) are invertible transformations applied to an intermediate random variable, and Z is the final LASER after M flow layers. V is an additional set of context vectors that condition the flow transformation. The resulting probability density of Z is\n\np(Z | V) = p0\n\n(cid:16)\n\nZ (0)(cid:17) M\n\n(cid:89)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) m=1\n\ndet\n\nδfm\n\n(cid:0)Z (m−1), V(cid:1) δZ (m−1)\n\n−1\n\n.\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(5)\n\nThe need to compute the Jacobian of intermediate flow transformations puts constraints on the design of those transformations, as they need to be computationally efficient. We use a split-coupling flow similar to RealNVP (Dinh et al., 2017), and we follow a similar architecture as the ones used in Wirnsberger et al. (2020; 2021) in adapting it to sets. The method Flow Scans (Bender et al., 2020) first introduced the general idea of adapting split-coupling to work on sets, and Wirnsberger et al. (2020; 2021) use a transformer for increased flexibility. A split-coupling transformation partitions the set Z into two sets Z1, Z2 by splitting each element of the original set across channels (we split in half). At a high level, the transformation of one part is computed as a function of the other part. Finally, the original set is recovered by elementwise concatenation of both transformed parts. More m that maps Z (m−1) → Z (m) is formally, let g be an invertible function, a split-coupling transform f sc given by three operations:\n\nZ (m)\n\n1 = g\n\nZ (m)\n\n2 = g\n\n(cid:16)\n\n(cid:16)\n\nZ (m−1)\n\n1\n\n, tm\n\nZ (m−1)\n\n2\n\n, tm\n\n(cid:16)\n\n(cid:16)\n\nZ (m−1)\n\n2\n\n, V\n\n(cid:17)(cid:17)\n\n,\n\nZ (m)\n\n1\n\n, V\n\n(cid:17)(cid:17)\n\n,\n\n(6)\n\nZ (m) = {concat(z(m)\n\nk,1 , z(m)\n\nk,2 )}K\n\nk=1 ,\n\nwhere zk,1 indicates dimensions of split 1 of element k, and V is conditioning context. Note how the parameters of the transformation g of one partition are derived from the other partition. To do so in a permutation-equivariant manner, the functions tm are a composition of two transformers: a self-attention module is applied followed by a cross-attention conditioned on context features, defined in Appendix A.2. More specifically, when transforming split 1 as a function of split 2, we have:\n\nψ(m−1)\n\n2\n\n= CrossAttn\n\n(cid:16)\n\n(cid:16)\n\nSelfAttn\n\nZ (m−1)\n\n2\n\n(cid:17)\n\n(cid:17)\n\n.\n\n, V\n\n(7)\n\nWe finally apply the function g, in our case the affine function (Dinh et al., 2017), from the parameters obtained for every element in ψm−1\n\n:\n\n2\n\nk,1 = z(m−1) z(m)\n\nk,1\n\nexp(W m\n\nscaleψ(m−1)\n\nk,2\n\n) + W m\n\nscaleψ(m−1)\n\nk,2\n\nfor k = 1, . . . K ,\n\n(8)\n\nscale and W m\n\nwhere W m bias are linear weights. When composing our complete flow with M layers of split-coupling flows, we find that including an additional linear invertible transformation after each fm to be helpful. To do so, we use the Linear Permutation Equivariant (f lpe m ), proposed in the Flow Scans work, the purpose of which is to allow capturing interdependencies in the dimensions of the set elements. The only difference is that we parameterize each sub-block of the linear transformation using a full unconstrained matrix (instead of a diagonal matrix).\n\nIn summary, our flow chains together the sequence of mappings f sc M . Thus, sampling from this flow involves sampling from the base distribution followed by applying the chain of functions.\n\n1 , . . . , f sc\n\nM , f lpe\n\n1 , f lpe\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nOur normalizing flow differs in a number of ways compared with Wirnsberger et al. (2020; 2021). First, our flow layers are additionally parameterized by a cross-attention module in order to condition the distribution on the context set; second, we don’t use rational-quadratic splines, instead we found the simpler affine coupling layer to work well. Notice that the size of the resulting set depends only on the hyper-parameter K, that is the number of elements sampled from the base distribution. It does not depend on the context size, i.e. features extracted from input images.\n\nIn our experiments, we use a diagonal Gaussian base distribution, and set the number of flow layers to be M = 8 for the prior flow, and M = 4 for the posterior flows. Our self-attention and cross-attention are composed of L = 1 layers, and 256 hidden units. Furthermore, we invert the chain of mappings (which is possible due to invertibility) for the prior flow as it we found it to be more numerically stable during training.\n\nA.2 ATTENTION MODULES\n\nFigure 5: Multi-head self-attention and cross-attention blocks. When combining multiple blocks, the outputs yl for layer l become the inputs for layer l + 1.\n\nWe use transformer attention Vaswani et al. (2017) in various components of LASER-NV. We distinguish two types of attention modules: self-attention, where keys, queries and values are computed from the same input, and cross-attention, where some context is used to compute keys and values, and queries are computed from a different input). An attention module can be composed of multiple attention blocks, where each attention block is shown in Fig. 5. We use the standard multihead dot-product attention with softmax weights. We define SelfAttn(y, L) to be a self-attention module with input y and L blocks5 and, similarly, CrossAttn(y, c, L) to be a cross-attention module with context c. Note how SelfAttn and CrossAttn do not include positional encoding that are often used in transformers.\n\nA.3\n\nIMAGE ENCODER\n\nTo encode the context images, we use a convolutional neural network based on the ResNet architecture from Vahdat & Kautz (2020) (also described in Kosiorek et al. (2021)) to independently encode context elements into h × w feature maps. The residual blocks that compose the encoder are shown in Fig. 6. For all models and datasets we use L = 4. Each context view consists of an image in RGB space with concatenated information of the camera position and direction.\n\nFor N context views, our last layer outputs N hw vectors that form the conditioning set used to parameterize our distributions over Z. Furthermore, the feature maps at different layers are also combined into a feature stack to use as the context local features as described in Appendix A.4.\n\n5We often drop the L input argument for notational clarity.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: ResNet architecture based on the encoder of NouveauVAE Vahdat & Kautz (2020).\n\nA.4 SCENE FUNCTION ARCHITECTURES\n\nFig. 7 provides a high-level overview of the information flows going into the scene function of each model, and we describe implementation details of their components below.\n\n(a) LASER-NV scene function\n\n(b) LASER-NV (NO GEOM.) scene function\n\n(c) MVC-NeRF scene function\n\n(d) NeRF-VAE scene function\n\nFigure 7: Scene function architecture for LASER-NV (a), its ablations (b) and (c), and NeRF-VAE (d).\n\nLatent attention A query point xi attends to Z as per Fig. 7 (a) using a cross-attn(γ(xi), Z, L), where γ(xi) is the circular encoding function of positions (see Appendix A.4). We use the same architecture across datasets, setting L = 3 layers, 512 hidden units, and 4 attention heads.\n\nLocal features from multi-view geometry We follow PixelNERF’s design of stacking the features of the encoded images at different levels of resolution, which allows the conditioning features to\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\ncapture context structure at different scales. For input images of resolution W × H, we stack the features of the outputs from all layers after the first layer:\n\nHn = stack({Hn\n\nl ∈ R W (9) where i = 2l. When stacking, we upscale the feature maps of all the feature maps of layers l ∈ {2 . . . L} to the size of the feature map with largest resolution (l = 1) using bilinear interpolation. The resulting set of stacked feature maps (one for each context view) is then used to condition the scene function.\n\ni ×16i}L\n\nl=1),\n\ni × H\n\nAs we described in Section 3.2, our multi-view geometry module is in part based on pixelNeRF’s processing of context views. Particularly, we process local features with exactly the f1 ResNet detailed in Yu et al. (2021): positions and directions are projected to the view-space using the cameras intrinsic parameters, and then processed with 3 residual blocks of 512 hidden units and ReLu activation. When computing the projection, we also define the binary variable vn i based on whether the projected coordinates are within the image plane.\n\nThe output features are integrated with our latent features via Mθ which is a cross-attention module. Note that context features with vn i = 0 are not attended to (we modify the pre-softmax logits of those context features by adding a large negative number to the logits). In our experiments Mθ has 3 layers and 512 hidden units.\n\nScene function output MLP We now describe the final component of the scene functions of all models, as shown in Fig. 7. This component follows the residual MLP architecture of the scene function of NeRF-VAE. Namely, it takes as input the circular encoded positions γ(xi) and directions γ(di), and at every layer it linearly projects the conditioning features (i.e. z for NeRF-VAE, or fi for LASER-NV) and sums it to the activations. The MLP is composed of two parts: the first has 4 layer of 256 hidden units each and outputs the densities, and the second part has an additional 4 layers and outputs the colours. All layers use a swish activation function.\n\nThe circular encoding function γ is given by\n\nγ(p) = (sin(2Lminπp), cos(2Lminπp), . . . , sin(2Lmaxπp) , cos(2Lmax)) (10) where we use different constants Lmin and Lmax for positions and directions and for each dataset (listed in Appendix E).\n\nA.5 NeRF-VAE AND CONDITIONAL NeRF-VAE\n\nOur implementation NeRF-VAE follows the same overall architecture as that of Kosiorek et al. (2021), but we use the image encoder described in Fig. 6 instead. The approximate posterior is computed as a diagonal Gaussian distribution, the means and log-variances of which are given by an MLP e as a function of the pooled feature maps H (pooled with a simple averaging over feature maps). The MLP e has two layers of 256 hidden units and ReLu activation function. The scene function is composed of an MLP that processes the latent vector z (one linear layer of 256 units, followed by a ReLu non-linearity), followed by the output MLP described in Appendix A.4.\n\nA.6 BACKGROUND SCENE FUNCTION\n\nThe volumetric rendering integral of a ray r is given by\n\nc(r)NeRF =\n\n(cid:90) tf\n\ntn\n\nT (t)σ(r(t))c(r(t), d) dt , with T (t) = exp\n\n−\n\n(cid:18)\n\n(cid:19)\n\nσ(r(s)) ds\n\n.\n\n(11)\n\n(cid:90) tf\n\ntn\n\nIf the ray is infinite (camera’s far plane tf → ∞) the colour weights T (t)σ(r(t)) would always sum to one. For a truncated ray (finite tf ), this happens only if the ray passes through a solid surface. This heuristic allows to model distant backgrounds by putting them at the end of the integration range, but it doesn’t work if the camera position changes significantly compared to the ray range tf − tn. If not normalized, the final weight T (tf ) tell us about the probability of light hitting a particle beyond the ray.\n\nIn that case, we can model the background as an infinite dome around the scene and model it with a light-field that depends only on the camera direction. That is, we compute the colour as\n\nc(r) = c(r)NeRF + T (tf )fbg(d) .\n\n(12)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFor the City dataset, in order to model the sky and sun, fbg(Z, di) is a learned neural network. We use a cross-attention module with 2 layers to attend to Z. For NeRF-VAE and cond. NeRF-VAE we instead use a 2 layered MLP with ReLu activations. For the ShapeNet fbg is a constant colour (white).\n\nn=1 H n, a cross-attention module attends to the latents: ˆhbg\n\nWe address the challenging backgrounds of MSN-Hard with a more expressive attentive background scene function that conditions on both the latents Z as well as context image features – this is in a similar fashion to how the NeRF scene function of LASER-NV integrates its latents with attention to local context features. Given an encoded ray direction γ(di) and context feature maps (cid:83)N bg(γ(di), Z), and the output is used to attend to the set of context features (cid:83)N n=1 H n with a second cross-attention module hbg n=1 H n). Finally, an MLP f mlp computes the background colour by taking as bg input the obtained background features and additionally conditions on the encoded directions by adding a linear projection of the directions to the activations of every layer. This MLP is composed of 2 layers of 512 hidden units each with a ReLu nonlinearity, and outputs the RGB background colours using a sigmoid nonlinearity.\n\ni = f 1\n\ni = f 2\n\ni , (cid:83)N\n\nbg(ˆhbg\n\nB CITY DATASET\n\nFigure 8: Examples of generated cities.\n\nFigure 9: Examples of different viewpoints (column) for each scene (row).\n\nThe aim of the City dataset is to allow generating visually complex scenes with large variability in its structure and appearance. Each generated city consists of a large area spanning approximately 270m × 270m, which consists of a 2x2 grid of blocks, where each block is composed of 4 lots. A lot can be a building or a green patch (park). The city 2x2 grid is surrounded by an outer ring of buildings. Roads separate all blocks in the scene. Each building is randomly generated with variation in size, appearance (textures, materials, colours), architectural styles (residential, industrial, commercial). Fig. 8 shows examples of generated city scenes. Once a city is built, we specify a range of points of interest, i.e. road intersections and parks and use each of these points as the starting placement for a scene (data point) of our dataset. Each scene is comprised of posed images rendered from randomly\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nchosen viewpoints near the point of interest. More specifically, we render 8 randomly chosen nearby points, and 4 random viewpoints for each point, making a total of 32 generated views per scene. Each generated view is annotated with the ground-truth render, camera parameters and depth maps. Our generated training dataset contains 100,000 scenes generated in total from approximately 2,600 uniquely generated cities. We test on 3,200 scenes generated in total from 10 uniquely generated cities.\n\nC ADDITIONAL VISUALIZATIONS\n\nCapturing detail at different scale In Fig. 10 we can see at a higher resolution how our model is expressive enough to capture texture and geometry details at different scales (e.g. far-away buildings and nearby objects such as the lamp post and the colonnade).\n\nFigure 10: Higher resolution reconstructions of LASER-NV reveal detail captured at different scales (e.g. lamp posts, and nearby buildings vs buildings at a large distance).\n\nNovel view synthesis with ShapeNet NMR In Fig. 11 we compare predictions for a number of objects that reveal how NeRF-VAE produces blurry reconstructions compared to MVC-NeRF and LASER-NV. When an image does not reveal the full shape of an object, as shown in Fig. 12, LASERNV is able to generate multiple plausible variations (in contrast to a deterministic which generates a single blurry prediction).\n\nFigure 11: Samples of category-agnostic ShapeNet for ablations of LASER-NV. MVC-NeRF and LASER-NV both result in high-fidelity predictions, while the NeRF-VAE’s outputs are blurrier.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: When predicting novel view from an ambiguous input, LASER-NV generates plausible variations, whereas MVC-NeRF results in a single blurry prediction.\n\nMSN-Hard predictions Figure 13 shows LASER-NV’s predictions on MSN-Hard including depth maps estimated from NeRF’s densities.\n\nFigure 13: Representative examples of LASER-NV predictions on MSN-Hard.\n\nD ADDITIONAL RESULTS\n\nData efficiency We evaluate how data hungry LASER-NV is compared to NeRF-VAE and MVCNeRF. In Figure 14 we report the reconstruction colour log-probability estimated on a validation set of the City dataset. For LASER-NV, we observe little degradation between using 50K scenes and 12.5K unique scenes. For NeRF-VAE we notice a larger drop when reducing the size from 25K to 12.5 scenes, showing clearer signs of overfitting. Finally, since MVC-NeRF severely underfits the City dataset for all training set sizes, we do not see any clear performance drop.\n\nTest-Time Scaling of the Latent Set We further want to understand how different latent set sizes at test time impact performance. Figure 15 shows reconstruction performance for different values of K at test time, for various values of used K during training. As we can see, LASER-NV does not significantly leverage a larger latent set size than trained with. As expected, we do see performance degrading when using smaller sets at test time, and this effect becomes stronger for models trained with larger capacity.\n\nTraining LASER-NV without ground-truth depth We train all our models using ground-truth depth on the City dataset. This reduces memory and computation requirements during training. We\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 14: Validation reconstruction performance of different method as a function of number of scenes throughout training.\n\nFigure 15: Reconstruction performance of LASER-NV as a function of the latent set size used at test time for different models trained with increasing sizes K.\n\nnote, however, that it is also possible to train LASER-NV without using ground truth depths. Once trained, we find that both the model trained with depths and the model trained without achieves slightly higher average colour log-likelihood (3.87) compared to LASER-NV (of 3.83) (importanceweighted estimate using 10 samples). We hypothesize that this is because the model trained without depths is trained solely using the rendering method used at test time.\n\nAs any NeRF-related method, LASER-NV can output estimated depth maps easily. We show example reconstructions and inferred depth maps of a trained LASER-NV on the City in Fig. 16. Note how the model is capable of capturing fine detail in textures and shapes (e.g. lamp post).\n\nComputational efficiency of LASER-NV vs NeRF-VAE For a model conditioned on 10 input views, inference (inferring the latent distribution) and rendering of a single image for LASER-NV (with 24 slots) take 0.39 GFLOPs and 1.80 GFLOPs respectively. NeRF-VAE instead takes 0.37 GFLOPs and 0.15 GFLOPs. The main reason for more expensive rendering in LASER-NV is the introduction of PixelNeRF-like conditioning. A model with a flow posterior but without such conditioning uses 0.47 GFLOPs to render a single image. Attention, however expensive, is evaluated only once per scene (to compute the prior/posterior). The PixelNeRF features are cheap per point, but they are evaluated for every 3D point used in rendering, therefore dominating the overall cost. Note that the above numbers do not reflect training latency. At training time we can guide the rendering process by using depths, which allows us to evaluate the scene function only twice for every pixel. We also\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 16: Reconstructions of LASER-NV trained without the use of ground-truth depths.\n\nsubsample images and we never reconstruct the whole target view(s). This makes training faster, and means that LASER-NV is only about 4x slower per training iteration than NeRF-VAE. Having said that, we noticed that NeRF-VAE does not benefit from longer training (its performance saturates), while the performance increase from increasing the model size (to equalize the FLOPs) is negligible.\n\nRobustness to camera noise We carry out an experiment on MSN-Hard in which we train and test with camera poses with incremental levels of Gaussian noise. We replicate the set up in Sajjadi et al. (2021) and compare their reported results using SRT and PixelNeRF to LASER-NV. Results in Fig. 17 show that while LASER-NV outperforms the other models when there is no noise, its performance drops more quickly compared to SRT, in a similar fashion to PixelNeRF. However, LASER-NV is more robust than PixelNeRF for all levels of noise. We believe that the use of NeRF and local features makes these models less robust to noisy cameras. That said, there are recent methods that allow mitigating noise sensitivity with NeRF (Lin et al., 2021) that can be incorporated.\n\nFigure 17: PSNR of models trained and evaluated under cameras with different levels of noise. The PSNR values of LASER-NV* are preliminary training values models that have not trained to convergence (340K steps out of 1000K).\n\nE TRAINING DETAILS\n\nHierarchical rendering Instead of optimizing separate sets of parameters for the coarse and fine rendering passes, we find that it suffices to only keep separate parameters of Fθ. This allows sharing parameters (and reduce computation) when querying the scene function on the coarse sampled points with respect to latent attention and multi-view geometry components of models.\n\nTraining with depth (City) Because we have RGB-D data in the City dataset, we can use depth as a supervised signal following the method in Stelzner et al. (2021). This method involves replacing the\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\npixel colour likelihood, for a given ray r, with the ray colour and depth likelihood p(t(r), c(r) | V, Z), where t(r) is the ray’s ground-truth depth. This alternative likelihood requires only two scene function evaluations per ray. In practice, we find that we get the best quality renders when combining both loglikelihood terms to the loss (each weighted by 0.5). At every training set, similar to how NeRF-VAE is trained, the likelihood terms are estimated by only reconstructing a subset of rays (i.e. pixels) of all the target images. We find that using 64 rays for estimating the (expensive) image log-likelihood, and 512 to estimate the ray colour and depth likelihood term, works well in the City. Additionally, we restrict output densities to be between 0 and 10 by using a weighted sigmoid in the output of the scene function.\n\nAdditional regularization The loss function for ShapeNet experiments includes a density regularization term based on the L1-norm of the densities, with a weight of 0.01. For all experiments, we scale the model parameter updates (Pascanu et al., 2013, Sec. 3.2), such that loss gradient norms have a maximum value of 10 for improved stability.\n\nHyper-parameters We use the following hyper-parameters across all models. Some of which were selected for each dataset based on a preliminary exploration using the training dataset and a validation split.\n\nHyperparameter\n\nCity\n\nShapenet NMR MSN-Hard\n\nLearning rate Batch size # of rays per scene # of context views # of target views # of posterior context views # of coarse points # of fine points Camera tnear Camera tf ar Positions circular encoding Lmin Positions circular encoding Lmax Directions circular encoding Lmin Directions circular encoding Lmax LASER-NV set size K LASER-NV latent vector dimensionality\n\n0.0003 96 512 2\n8 10 256 64 0.05 270 -8 8\n0 4\n24 128\n\n0.0002 96 512 1\n23 4\n32 64 0.1 3.7 -2 8\n0 4\n8 128\n\nTable 4: Common hyper-parameters\n\n0.0001 32 384 5\n3 8\n64 32 0.01 19 -5 10 0\n8 32 128\n\n22",
    "reference": "# Summary Of The Paper\n\nThis paper tackles the problem of scene generation, generating consistent images of scenes from multiple viewpoints conditioned on few images. The main technical contribution of the work is proposing a set-valued latent representation using normalizing flows, built on top of NeRF-VAE. The proposed model is tested on synthetic city dataset and ShapeNet dataset, while having competitive performance with respect to recent methods.\n\n# Strength And Weaknesses\n\n**Strength**\n- Performance\n\nBy looking at the figures and tables of the manuscript, the work clearly outperforms NeRF-VAE [1], producing less blurry images from multiple views. However, I would like to ask the authors why SRT [2] was not used as a baseline.\n\n- Good experimental design\n\nI enjoyed showing that increasing the latent vector size of NeRF-VAE does not result in a good performance, thereby the design choice of this work is better. The ablations by showing with performance with/without either latent set and local features make the paper stronger.\n\n**Weaknesses**\n- Novelty\n\nMy main concern lies in the novelty of the work. The work seems to be a mixture of NeRF-VAE and SRT. Please see the novelty section below for details.\n\n- Intuition behind the set data structure for the scene latents\n\nAlthough it is clear from Table 1, that the scene latents increase the performance of the method, I would like to ask the authors the intuition behind the sets as the choice for the latent data structure. Other works such as Neural Sparse Voxel Fields [3] either have an explicit latent data structure, where the latent code explicitly encodes local information or alot of other generative modeling works show the controlability of latent code by interpolating them. I do not see either of the intuition behind the set representation nor other attributes of set-valued latent representation and would like to hear from the authors the reason for choosing particularly this data structure besides the performance.\n\n- Proposed model requires depth information\n\nAlthough the authors have reported the log-likelihood score of the method in the appendix without depth information, reporting all the scores in Table 1 without depth information will strengthen the paper.\n\n[1] Kosiorek et al. NeRF-VAE: A Geometry Aware 3D Scene Generative Model. ICML, 2021\n\n[2] Sajjadi et al. Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. CVPR, 2022\n\n[3] Liu et al. Neural Sparse Voxel Fields. NeurIPS, 2020\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- Clarity\n\nThe paper is clearly written and easy to understand and the figures make the paper easier to understand.\n\n- Novelty\n\nThe work seems to be a mixture of NeRF-VAE and SRT which is my biggest concern. Although the taking advantages of other methods does not necessarily reduce the novelty of the work, using the latent code of NeRF-VAE, thereby having multimodality and using set-representation latent code of SRT is straightforward with no huge novelty in the problem setting.\n\n- Quality\n\nThe experimental design of the paper is well, although some more ablation studies could help.\n\n- Reproducibility\n\nCode is not uploaded. Wil the code and city datasets be uploaded? I think the city dataset will be used alot in the community.\n\n# Summary Of The Review\n\nMy biggest concern lies in the novelty of the method. I would like to hear from other reviewers before making the final decision.\n\n---------------------------\nAfter Rebuttal:\nI appreciate the authors for their response.\nI'm in favor of accepting the paper, due to it's multimodality and high performance.\nI have raised my score accordingly.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLEARNING MULTIOBJECTIVE PROGRAM THROUGH ONLINE LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe investigate the problem of learning the parameters (i.e., objective functions or constraints) of a multiobjective decision making model, based on a set of sequentially arrived decisions. In particular, these decisions might not be exact and possibly carry measurement noise or are generated with the bounded rationality of decision makers. In this paper, we propose a general online learning framework to deal with this learning problem using inverse multiobjective optimization, and prove that this framework converges at a rate of O(1/ T ) under certain regularity conditions. More precisely, we develop two online learning algorithms with implicit update rules which can handle noisy data. Numerical results with both synthetic and real world datasets show that both algorithms can learn the parameters of a multiobjective program with great accuracy and are robust to noise.\n\n√\n\n1\n\nINTRODUCTION\n\nIn this paper, we aim to learn the parameters (i.e., constraints and a set of objective functions) of a decision making problem with multiple objectives, instead of solving for its efficient (or Pareto) optimal solutions, which is the typical scenario. More precisely, we seek to learn θ given {yi}i∈[N ] that are observations of the efficient solutions of the multiobjective optimization problem (MOP):\n\n{f1(x, θ), f2(x, θ), . . . , fp(x, θ)}\n\nmin x\ns.t. x ∈ X(θ),\n\nwhere θ is the true but unknown parameter of the MOP. In particular, we consider such learning problems in online fashion, noting observations are unveiled sequentially in practical scenarios. Specifically, we study such learning problem as an inverse multiobjective optimization problem (IMOP) dealing with noisy data, develop online learning algorithms to derive parameters for each objective function and constraint, and finally output an estimation of the distribution of weights (which, together with objective functions, define individuals’ utility functions) among human subjects.\n\nLearning human participants’ decision making scheme is critical for an organization in designing and providing services or products. Nevertheless, as in most scenarios, we can only observe their decisions or behaviors and cannot directly access decision making schemes. Indeed, participants probably do not have exact information regarding their own decision making process (Keshavarz et al., 2011). To bridge the discrepancy, we leverage the inverse optimization idea that has been proposed and received significant attention in the optimization community, which is to infer the missing information of the underlying decision models from observed data, assuming that human decision makers are making optimal decisions (Ahuja & Orlin, 2001; Iyengar & Kang, 2005; Schaefer, 2009; Wang, 2009; Keshavarz et al., 2011; Chan et al., 2014; Bertsimas et al., 2015; Aswani et al., 2018; Esfahani et al., 2018; Tan et al., 2020). This subject actually carries the data-driven concept and becomes more applicable as large amounts of data are generated and become readily available, especially those from digital devices and online transactions.\n\n1.1 RELATED WORK\n\nOur work draws inspiration from the inverse optimization problem with single objective. It seeks particular values for those parameters such that the difference between the actual observation and the expected solution to the optimization model (populated with those inferred values) is minimized. Although complicated, an inverse optimization model can often be simplified for computation through\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nusing KKT conditions or strong duality of the decision making model, provided that it is convex. Nowadays, extending from its initial form that only considers a single observation Ahuja & Orlin (2001); Iyengar & Kang (2005); Schaefer (2009); Wang (2009), inverse optimization has been further developed and applied to handle many observations Keshavarz et al. (2011); Bertsimas et al. (2015); Aswani et al. (2018); Esfahani et al. (2018). Nevertheless, a particular challenge, which is almost unavoidable for any large data set, is that the data could be inconsistent due to measurement errors or decision makers’ sub-optimality. To address this challenge, the assumption on the observations’ optimality is weakened to integrate those noisy data, and KKT conditions or strong duality is relaxed to incorporate inexactness.\n\nOur work is most related to the subject of inverse multiobjective optimization. The goal is to find multiple objective functions or constraints that explain the observed efficient solutions well. There are several recent studies related to the presented research. One is in Chan et al. (2014), which considers a single observation that is assumed to be an exact optimal solution. Then, given a set of well-defined linear functions, an inverse optimization is formulated to learn their weights. Another one is Dong & Zeng (2020), which proposes the batch learning framework to infer utility functions or constraints from multiple noisy decisions through inverse multiobjective optimization. This work can be categorized as doing inverse multiobjective optimization in batch setting. Recently, Dong & Zeng (2021) extends Dong & Zeng (2020) with distributionally robust optimization by leveraging the prominent Wasserstein metric. In contrast, we do inverse multiobjective optimization in online settings, and the proposed online learning algorithms significantly accelerate the learning process with performance guarantees, allowing us to deal with more realistic and complex preference inference problems.\n\nAlso related to our work is the line of research conducted by B ̈armann et al. (2017) and Dong et al. (2018), which develops online learning methods to infer the utility function or constraints from sequentially arrived observations. However, their approach is only possible to handle inverse optimization with a single objective. More specifically, their methods apply to situations where observations are generated by decision making problems with only one objective function. Differently, our approach does not make the single-objective assumption and only requires the convexity of the underlying decision making problem with multiple objectives. Hence, we believe that our work generalizes their methods and extends the applicability of online learning from learning single objective program to multiobjective program.\n\n1.2 OUR CONTRIBUTIONS\n\nTo the best of authors’ knowledge, we propose the first general framework of online learning for inferring decision makers’ objective functions or constraints using inverse multiobjective optimization. This framework can learn the parameters of any convex decision making problem, and can explicitly handle noisy decisions. Moreover, we show that the online learning approach, which adopts an implicit update rule, has an O( T ) regret under suitable regularity conditions when using the ideal loss function. We finally illustrate the performance of two algorithms on both a multiobjective quadratic programming problem and a portfolio optimization problem. Results show that both algorithms can learn parameters with great accuracy and are robust to noise while the second algorithm significantly accelerate the learning process over the first one.\n\n√\n\n2 PROBLEM SETTING\n\n2.1 DECISION MAKING PROBLEM WITH MULTIPLE OBJECTIVES\n\nWe consider a family of parametrized multiobjective decision making problems of the form\n\nmin x∈Rn s.t.\n\n(cid:8)f1(x, θ), f2(x, θ), . . . , fp(x, θ)(cid:9) x ∈ X(θ),\n\n(DMP)\n\nwhere p ≥ 2 and fl(x, θ) : Rn × Rnθ (cid:55)→ R for each l ∈ [p]. Assume parameter θ ∈ Θ ⊆ Rnθ . We denote the vector of objective functions by f (x, θ) = (f1(x, θ), f2(x, θ), . . . , fp(x, θ))T . Assume X(θ) = {x ∈ Rn : g(x, θ) ≤ 0, x ∈ Rn +}, where g(x, θ) = (g1(x, θ), . . . , gq(x, θ))T is another vector-valued function with gk(x, θ) : Rn × Rnθ (cid:55)→ R for each k ∈ [q].\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition 2.1 (Efficiency). For fixed θ, a decision vector x∗ ∈ X(θ) is said to be efficient if there exists no other decision vector x ∈ X(θ) such that fi(x, θ) ≤ fi(x∗, θ) for all i ∈ [p], and fk(x, θ) < fk(x∗, θ) for at least one k ∈ [p].\n\nIn the study of multiobjective optimization, the set of all efficient solutions is denoted by XE(θ) and called the efficient set. The weighting method is commonly used to obtain an efficient solution through computing the problem of weighted sum (PWS) Gass & Saaty (1955) as follows.\n\nmin wT f (x, θ) s.t. x ∈ X(θ),\n\n(PWS)\n\nwhere w = (w1, . . . , wp)T . Without loss of generality, all possible weights are restricted to a simplex, which is denoted by Wp = {w ∈ Rp + : 1T w = 1}. Next, we denote the set of optimal solutions for the (PWS) by\n\nS(w, θ) = arg min\n\nx\n\n(cid:8)wT f (x, θ) : x ∈ X(θ)(cid:9) .\n\np = {w ∈ Rp\n\nLet W + Proposition 2.1. If x ∈ S(w, θ) and w ∈ W +\n\np , then x ∈ XE(θ).\n\n++ : 1T w = 1}. Following from Theorem 3.1.2 of Miettinen (2012), we have:\n\nThe next result from Theorem 3.1.4 of Miettinen (2012) states that all the efficient solutions can be found by the weighting method for convex MOP.\n\nProposition 2.2. Assume that MOP is convex. If x ∈ X is an efficient solution, then there exists a weighting vector w ∈ Wp such that x is an optimal solution of (PWS).\n\nBy Propositions 2.1 - 2.2, we can summarize the relationship between S(w, θ) and XE(θ) as follows. Corollary 2.2.1. For convex MOP,\n\n(cid:91)\n\nw∈W +\n\np\n\nS(w, θ) ⊆ XE(θ) ⊆\n\n(cid:91)\n\nw∈Wp\n\nS(w, θ).\n\nIn the following, we make a few assumptions to simplify our understanding, which are actually mild and appear often in the literature. Assumption 2.1. Set Θ is a convex compact set. There exists D > 0 such that ∥θ∥2 ≤ D for all θ ∈ Θ. In addition, for each θ ∈ Θ, both f (x, θ) and g(x, θ) are convex in x.\n\nINVERSE MULTIOBJECTIVE OPTIMIZATION\n\n2.2 Consider a learner who has access to decision makers’ decisions, but does not know their objective functions or constraints. In our model, the learner aims to learn decision makers’ multiple objective functions or constraints from observed noisy decisions only. We denote y the observed noisy decision that might carry measurement error or is generated with a bounded rationality of the decision maker. We emphasize that this noisy setting of y reflects the real world situation rather than for analysis of regret. Throughout the paper we assume that y is a random variable distributed according to an unknown distribution Py supported on Y. As y is a noisy observation, we note that y does not necessarily belong to X(θ), i.e., it might be either feasible or infeasible with respect to X(θ).\n\nWe next discuss the construction of an appropriate loss function for the inverse multiobjective optimization problem Dong & Zeng (2020; 2021). Ideally, given a noisy decision y and a hypothesis θ, the loss function can be defined as the minimum distance between y and the efficient set XE(θ):\n\nl(y, θ) = min\n\nx∈XE (θ)\n\n∥y − x∥2 2.\n\n(loss function)\n\nFor a general MOP, however, there might exist no explicit way to characterize the efficient set XE(θ). Hence, an approximation approach to practically describe this is adopted. Following from Corollary 2.2.1, a sampling approach is adopted to generate wk ∈ Wp for each k ∈ [K] and approximate XE(θ) as (cid:83) k∈[K] S(wk, θ). Then, the surrogate loss function is defined as\n\nlK(y, θ) =\n\nmin\n\nx∈ (cid:83)\n\nS(wk,θ)\n\n∥y − x∥2 2.\n\n(surrogate loss)\n\nk∈[K]\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nBy using binary variables, this surrogate loss can be converted into the Surrogate Loss Problem.\n\nlK(y, θ) =\n\nmin zj ∈{0,1}\n\n∥y − (cid:80)\n\nzkxk∥2\n\n2\n\nk∈[K] zk = 1, xk ∈ S(wk, θ).\n\ns.t. (cid:80)\n\nk∈[K]\n\n(1)\n\nConstraint (cid:80) k∈[K] zk = 1 ensures that exactly one of the efficient solutions will be chosen to measure the distance to y. Hence, solving this optimization problem identifies some wk with k ∈ [K] such that the corresponding efficient solution S(wk, θ) is closest to y. Remark 2.1. It is guaranteed that no efficient solution will be excluded if all weight vectors in Wp are enumerated. As it is practically infeasible due to computational intractability, we can control K to balance the tradeoff between the approximation accuracy and computational efficacy. Certainly, if the computational power is strong, we would suggest to draw a large number of weights evenly in Wp to avoid any bias. In practice, for general convex MOP, we evenly sample {wk}k∈[K] from W + to ensure that S(wk, θ) ∈ XE(θ). If f (x, θ) is known to be strictly convex, we can evenly sample {wk}k∈[K] from Wp as S(wk, θ) ∈ XE(θ) by Proposition 2.1.\n\np\n\n3 ONLINE LEARNING FOR IMOP\n\nIn our online learning setting, noisy decisions become available to the learner one by one. Hence, the learning algorithm produces a sequence of hypotheses (θ1, . . . , θT +1). Here, T is the total number of rounds, and θ1 is an arbitrary initial hypothesis and θt for t > 1 is the hypothesis chosen after seeing the (t − 1)th decision. Let l(yt, θt) denote the loss the learning algorithm suffers when it tries to predict yt based on the previous observed decisions {y1, . . . , yt−1}. The goal of the learner is to minimize the regret, which is the cumulative loss (cid:80)T t=1 l(yt, θt) against the best possible loss when the whole batch of decisions are available. Formally, the regret is defined as\n\nRT =\n\nT (cid:88)\n\nt=1\n\nl(yt, θt) − min θ∈Θ\n\nT (cid:88)\n\nt=1\n\nl(yt, θ).\n\nUnlike most online learning problems that assume the loss function to be smooth Shalev-Shwartz (2011); Hazan (2016), l(y, θ) and lK(y, θ) are not necessarily smooth in our paper, due to the structures of XE(θ) and (cid:83) k∈[K] S(wk, θ). Thus, the popular gradient based online learning algorithms Bottou (1999); Kulis & Bartlett (2010) fail and our problem is significantly more difficult than most of them. To address this challenge, two online learning algorithms are developed in the next section.\n\n3.1 ONLINE IMPLICIT UPDATES Once receiving the tth noisy decision yt, the ideal way to update θt+1 is by solving the following optimization problem using the ideal loss function:\n\nθt+1 = arg min θ∈Θ\n\n1 2\n\n∥θ − θt∥2\n\n2 + ηtl(yt, θ),\n\n(2)\n\nwhere ηt is the learning rate in each round, and l(yt, θ) is defined in loss function.\n\nAs explained in the previous section, l(yt, θ) might not be computable due to the non-existence of the closed form of the efficient set XE(θ). Thus, we seek to approximate the update 2 by:\n\nθt+1 = arg min θ∈Θ\n\n1 2\n\n∥θ − θt∥2\n\n2 + ηtlK(yt, θ),\n\n(3)\n\nwhere ηt is the learning rate in each round, and lK(yt, θ) is defined in surrogate loss.\n\nThe update 3 approximates 2, and seeks to balance the tradeoff between “conservativeness” and “correctiveness”, where the first term characterizes how conservative we are to maintain the current estimation, and the second term indicates how corrective we would like to modify with the new estimation. As no closed form exists for θt+1 in general, this update method is an implicit approach.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Online Learning for IMOP\n\nAlgorithm 2 Accelerated Online Learning\n\n1: Input:\n\nnoisy decisions {yt}t∈T , weights\n\n{wk}k∈K\n\n2: Initialize θ1 = 0 3: for t = 1 to T do receive yt 4: suffer loss lK(yt, θt) 5: if lK(yt, θt) = 0 then 6: 7: 8: 9: 10:\n\nθt+1 ← θt\n\nelse\n\n√\n\nset learning rate ηt ∝ 1/ update θt+1 by solving 3 directly (or equivalently solving K subproblems 4)\n\nt\n\nend if 11: 12: end for\n\n1: Input: {yt}t∈T and {wk}k∈K 2: Initialize θ1 = 0 3: for t = 1 to T do receive yt 4: suffer loss lK(yt, θt) 5: let k∗ = arg mink∈[K]∥yt − xk∥2 2, 6: where xk ∈ S(wk, θt) for k ∈ [K] if lK(yt, θt) = 0 then\n\nθt+1 ← θt\n\nset learning rate ηt ∝ 1/ update θt+1 by 4 with k = k∗\n\nt\n\n√\n\nelse\n\n7: 8: 9: 10: 11: end if 12: 13: end for\n\nTo solve 3, we can replace xk ∈ S(wk, θ) by KKT conditions for each k ∈ [K]:\n\nmin θ\n\ns.t.\n\n1\n\n2 ∥θ − θt∥2\n\n2 + ηt\n\n(cid:80)\n\nk∈[K]\n\n∥yt − θk∥2\n\n2\n\nθ ∈ Θ, \n\n \n\nk g(xk) = 0,\n\ng(xk) ≤ 0, uk ≥ 0, uT ∇xk wT 0 ≤ θk ≤ Mkzk, xk − Mk(1 − zk) ≤ θk ≤ xk,\n\n(cid:80)\n\nzk = 1,\n\nk f (xk, θ) + uk · ∇xk g(xk) = 0,\n\n\n\n  , ∀k ∈ [K],\n\n∀k ∈ [K], ∀k ∈ [K],\n\nk∈[K]\n\nxk ∈ Rn, uk ∈ Rm\n\n+ , zk ∈ {0, 1},\n\n∀k ∈ [K],\n\nwhere uk is the dual variable for gk(x, θ) ≤ 0, and Mk is a big number to linearize zkxk.\n\nAlternatively, solving 3 is equivalent to solving K independent programs defined in the following and taking the one with the least optimal value (breaking ties arbitrarily).\n\nmin θ∈Θ s.t.\n\n1\n\n2 ∥θ − θt∥2\n\n2 + ηt∥yt − x∥2\n\n2\n\nx ∈ S(wk, θ).\n\n(4)\n\nOur application of the implicit update rule to learn an MOP proceeds as outlined in Algorithm 1.\n\nRemark 3.1. (i) When choosing 4 to update θt+1, we can parallelly compute K independent problems of 4, which would dramatically improve the computational efficiency. (ii) After the completion of Algorithm 1, we can allocate every yt to the wk that minimizes lK(yt, θT +1), which provides an inference on the distribution of weights of component functions fl(x, θ) over human subjects.\n\nAcceleration of Algorithm 1: Note that we update θ and the weight sample assigned to yt in 3 simultaneously, meaning both θ and the weight sample index k are variables when solving 3. In other words, one needs to solve K subproblems 4 to get an optimal solution for 3. However, note that the increment of θ by 3 is typically small for each update. Consequently, the weight sample assigned to yt using θt+1 is roughly the same as using the previous guess of this parameter, i.e., θt. Hence, it is reasonable to approximate 3 by first assigning a weight sample to yt based on the previous updating result. Then, instead of computing K problems of 4, we simply compute a single one associated with the selected weight samples, which significantly eases the burden of solving 3. Our application of the accelerated implicit update rule proceeds as outlined in Algorithm 2.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nMini-batches We enhance online learning by considering multiple observations per update Bottou & Cun (2004). In online IMOP, this means that computing θt+1 using |Nt| > 1 decisions:\n\nθt+1 = arg min θ∈Θ\n\n1 2\n\n∥θ − θt∥2\n\n2 +\n\nηt |Nt|\n\n(cid:88)\n\nt∈Nt\n\nlK(yt, θ),\n\n(5)\n\nHowever, we should point out that applying Mini-batches might not be suitable here as the update 5 is drastically more difficult to compute even for |Nt| = 2 than the update 3 with a single observation.\n\n3.2 ANALYSIS OF CONVERGENCE Note that the proposed online learning algorithms are generally applicable to learn the parameter of any convex MOP. In this section, we show that the average regret converges at a rate of O(1/ T ) under certain regularity conditions based on the ideal loss function l(y, θ). Namely, we consider the regret bound when using the ideally implicit update rule 2. Next, we introduce a few assumptions that are regular in literature Keshavarz et al. (2011); Bertsimas et al. (2015); Esfahani et al. (2018); Aswani et al. (2018); Dong & Zeng (2018); Dong et al. (2018).\n\n√\n\nAssumption 3.1. (a) X(θ) is closed, and has a nonempty relative interior. X(θ) is also bounded. Namely, there exists B > 0 such that ∥x∥2 ≤ B for all x ∈ X(θ). The support Y of the noisy decisions y is contained within a ball of radius R almost surely, where R < ∞. In other words, P(∥y∥2 ≤ R) = 1.\n\n(b) Each function in f is strongly convex on Rn, that is for each l ∈ [p], ∃λl > 0, ∀x, y ∈ Rn\n\n(cid:18)\n\n(cid:19)T\n\n∇fl(y, θl) − ∇fl(x, θl)\n\n(y − x) ≥ λl∥x − y∥2 2.\n\nRegarding Assumption 3.1.(a), assuming that the feasible region is closed and bounded is very common in inverse optimization. The finite support of the observations is needed since we do not hope outliers have too many impacts in our learning. Let λ = minl∈[p]{λl}. It follows that wT f (x, θ) is strongly convex with parameter λ for w ∈ Wp. Therefore, Assumption 3.1.(b) ensures that S(w, θ) is a single-valued set for each w.\n\nThe performance of the algorithm also depends on how the change of θ affects the objective values. For ∀w ∈ Wp, θ1 ∈ Θ, θ2 ∈ Θ, we consider the following function\n\nh(x, w, θ1, θ2) = wT f (x, θ1) − wT f (x, θ2).\n\nAssumption 3.2. ∃κ > 0, ∀w ∈ Wp, h(·, w, θ1, θ2) is κ-Lipschitz continuous on Y. That is,\n\n|h(x, w, θ1, θ2) − h(y, w, θ1, θ2)| ≤ κ∥θ1 − θ2∥2∥x − y∥2, ∀x, y ∈ Y.\n\nBasically, this assumption says that the objective functions will not change much when either the parameter θ or the variable x is perturbed. It actually holds in many common situations, including the multiobjective linear program and multiobjective quadratic program.\n\nFrom now on, given any y ∈ Y, θ ∈ Θ, we denote x(θ) the efficient point in XE(θ) that is closest to y. Namely, l(y, θ) = ∥y − x(θ)∥2 2. Lemma 3.1. Under Assumptions 3.1 - 3.2, the loss function l(y, θ) is uniformly 4(B+R)κ continuous in θ. That is, ∀y ∈ Y, ∀θ1, θ2 ∈ Θ, we have\n\n-Lipschitz\n\nλ\n\n|l(y, θ1) − l(y, θ2)| ≤\n\n4(B + R)κ λ\n\n∥θ1 − θ2∥2.\n\nThe key point in proving Lemma 3.1 is the observation that the perturbation of S(w, θ) due to θ is bounded by the perturbation of θ by applying Proposition 6.1 in Bonnans & Shapiro (1998). Details of the proof are given in Appendix.\n\nAssumption 3.3. For MOP, ∀y ∈ Y, ∀θ1, θ2 ∈ Θ, ∀α, β ≥ 0 s.t. α + β = 1, we have either of the following:\n\n(a) if x1 ∈ XE(θ1), and x2 ∈ XE(θ2), then αx1 + βx2 ∈ XE(αθ1 + βθ2).\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(b) ∥αx(θ1) + βx(θ2) − x(αθ1 + βθ2)∥2 ≤ αβ∥x(θ1) − x(θ2)∥2/(2(B + R)).\n\nThe definition of x(θ1), x(θ2) and x(αθ1 + βθ2) is given before Lemma 3.1. This assumption requires the convex combination of x1 ∈ XE(θ1), and x2 ∈ XE(θ2) belongs to XE(αθ1 + βθ2). Or there exists an efficient point in XE(αθ1 + βθ2) close to the convex combination of x(θ1) and x(θ2). Examples are given in Appendix.\n\nLet θ∗ be an optimal inference to minθ∈Θ t∈[T ] l(yt, θ), i.e., an inference derived with the whole batch of observations available. Then, the following theorem asserts that under the above assumptions, the regret RT = (cid:80) T ).\n\nt∈[T ](l(yt, θt) − l(yt, θ∗)) of the online learning algorithm is of O(\n\n√\n\n(cid:80)\n\nTheorem 3.2. Suppose Assumptions 3.1 - 3.3 hold. Then, choosing ηt =\n\n√\n\n2\n\nDλ\n\n2(B+R)κ\n\n1√ t\n\n, we have\n\nRT ≤\n\n√\n\n4\n\n2(B + R)Dκ\n\n√\n\nλ\n\nT .\n\nWe establish the above regret bound by extending Theorem 3.2 in Kulis & Bartlett (2010). Our extension involves several critical and complicated analyses for the structure of the optimal solution set S(w, θ) as well as the loss function, which is essential to our theoretical understanding. Moreover, we relax the requirement of smoothness of loss function to Lipschitz continuity through a similar argument in Lemma 1 of Wang et al. (2017) and Duchi et al. (2011).\n\n4 EXPERIMENTS\n\nIn this section, we will provide a multiobjective quadratic program (MQP) and a portfolio optimization problem to illustrate the performance of the proposed online learning Algorithms 1 and 2. The mixed integer second order conic problems (MISOCPs), which are derived from using KKT conditions in 3, are solved by Gurobi Optimization (2016). All the algorithms are programmed with Julia Bezanson et al. (2017). The experiments have been run on an Intel(R) Xeon(R) E5-1620 processor that has a 3.60GHz CPU with 32 GB RAM.\n\n4.1 SYNTHETIC DATA: LEARNING THE PREFERENCES AND RESTRICTIONS FOR AN MQP Consider the following multiobjective quadratic optimization problem.\n\nmin x∈R2\n\n+\n\n(cid:18)f1(x) = 1 f2(x) = 1\n\n2 xT Q1x + cT 2 xT Q2x + cT\n\n1 x 2 x\n\n(cid:19)\n\ns.t. Ax ≤ b,\n\nwhere parameters of the objective functions and constraints are provided in Appendix.\n\nSuppose there are T decision makers. In each round, the learner would receive one noisy decision. Her goal is to learn the objective functions or restrictions of these decision makers. In round t, we suppose that the decision maker derives an efficient solution xt by solving (PWS) with weight wt, which is uniformly chosen from W2. Next, the learner receives the noisy decision yt corrupted by noise that has a jointly uniform distribution with support [−0.5, 0.5]2. Namely, yt = xt + εt, where each element of εt ∼ U (−0.5, 0.5).\n\nLearning the objective functions In the first set of experiments, the learner seeks to learn c1 and c2 given the noisy decisions that arrive sequentially in T rounds. We assume that c1 is within range [1, 6]2, c2 is within range [−6, −1]2, T = 1000 rounds of noisy decisions are generated, and K = 41 weights from W2 are evenly sampled. The learning rate is set to ηt = 5/ t. Then, we implement Algorithms 1 and 2. At each round t, we solve 4 using parallel computing with 6 workers.\n\n√\n\nTo illustrate the performance of the algorithms in a statistical way, we run 100 repetitions of the experiments. Figure 1a shows the total estimation errors of c1 and c2 in each round over the 100 repetitions for the two algorithms. We also plot the average estimation error of the 100 repetitions. As can be seen in this figure, convergence for both algorithms is pretty fast. Also, estimation errors over rounds for different repetitions concentrate around the average, indicating that our algorithm is pretty robust to noise. The estimation error in the last round is not zero because we use a finite K to approximate the efficient set. We see in Figure 1b that Algorithm 2 is much faster than Algorithm 1 especially when K is large. To further illustrate the performance of algorithms, we randomly pick one repetition using Algorithm 1 and plot the estimated efficient set in Figure 1c. We can see\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 1: Learning objective functions of an MQP over T = 1000 rounds. We run 100 repetitions of experiments. Let c = [c1, c2]. (a) We plot estimation errors at each round t for all 100 experiments and their average estimation errors with K = 41. (b) Blue and yellow bars indicate average running time and standard deviations for each K using Algorithm 1 and 2, respectively. (c) We randomly pick one repetition. The estimated efficient set after T = 1000 rounds is indicated by the red line. The real efficient set is shown by the yellow line. (d) The dotted brown line is the error bar plot of the running time over 10 repetitions in batch setting. The blue line is the error bar plot of the running time over 100 repetitions in an online setting using Algorithm 1.\n\nclearly that the estimated efficient set almost coincides with the real efficient set. Moreover, Figure 1d shows that IMOP in online settings is drastically faster than in batch setting. It is practically impossible to apply the batch setting algorithms in real-world applications.\n\nLearning the Right-hand Side In the second set of experiments, the learner seeks to learn b given the noisy decisions that arrive sequentially in T rounds. We assume that b is within [−10, 10]2. T = 1000 rounds of noisy decisions are generated. K = 81 weights from W2 are evenly sampled. The learning rate is set to ηt = 5/ t. Then, we apply Algorithms 1 and 2. To illustrate the performance of them, we run 100 repetitions of the experiments. Figure 2a shows the estimation error of b in each round over the 100 repetitions for the two algorithms. We also plot the average estimation error of the 100 repetitions. As can be seen in the figure, convergence for both algorithms is pretty fast. In addition, we see in Figure 2b that Algorithm 2 is much faster than Algorithm 1.\n\n√\n\n(a)\n\n(b)\n\nFigure 2: Learning the right-hand side of an MQP over T = 1000 rounds. We run 100 repetitions of the experiments. (a) We plot estimation errors at each round t for all 100 experiments and their average estimation errors of all repetitions with K = 41. (b) Blue and yellow bars indicate the average running times and standard deviations for each K using Algorithm 1 and 2, respectively.\n\n4.2 REAL-WORLD CASE: LEARNING EXPECTED RETURNS IN PORTFOLIO OPTIMIZATION We next consider noisy decisions arising from different investors in a stock market. More precisely, we consider a portfolio selection problem, where investors need to determine the fraction of their wealth to invest in each security to maximize the total return and minimize the total risk. The process typically involves the cooperation between an investor and a portfolio analyst, where the analyst provides an efficient frontier on a certain set of securities to the investor and then the investor selects a portfolio according to her preference to the returns and risks. The classical Markovitz meanvariance portfolio selection Markowitz (1952) in the following is used by analysts.\n\nmin\n\ns.t.\n\n(cid:18)f1(x) = −rT x f2(x) = xT Qx\n\n(cid:19)\n\n0 ≤ xi ≤ bi, n\n(cid:80)\n\nxi = 1,\n\ni=1\n\n∀i ∈ [n],\n\n8\n\n0200400600800100010-1100101Alg 1: Estimation errorAlg 1: Average errorAlg 2: Estimation errorAlg 2: Average errorK=6K=11K=21K=41020406080100120140Time (s)Alg 1Alg 201230123Nosiy decisionsReal efficient setEstimated efficient setT = 5T = 10T = 15100101102103Batching settingOnline setting0200400600800100010-1100101Alg 1: Estimation errorAlg 1: Average errorAlg 2: Estimation errorAlg 2: Average errorK=6K=11K=21K=41020406080100120140160Time(s)Alg 1Alg 2Under review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 3: Learning the expected return of a Portfolio optimization problem over T = 1000 rounds with K = 41. (a) The red line indicates the real efficient frontier. The blue dots indicate the estimated efficient frontier using the estimated expected return for K = 41. (b) Each bar represents the proportion of the 1000 decision makers that has the corresponding weight for f1(x).\n\nwhere r ∈ Rn + is a vector of individual security expected returns, Q ∈ Rn×n is the covariance matrix of securities returns, x is a portfolio specifying the proportions of capital to be invested in the different securities, and bi is an upper bound on the proportion of security i, ∀i ∈ [n].\n\nDataset: The dataset is derived from monthly total returns of 30 stocks from a blue-chip index which tracks the performance of top 30 stocks in the market when the total investment universe consists of thousands of assets. The true expected returns and true return covariance matrix for the first 8 securities are given in the Appendix.\n\n√\n\nDetails for generating the portfolios are provided in Appendix. The portfolios on the efficient frontier are plot in Figure 3a. The learning rate is set to ηt = 5/ t. At each round t, we solve 4 using parallel computing. In Table 1 we list the estimation error and estimated expected returns for different K. The estimation error becomes smaller when K increases, indicating that we have a better approximation accuracy of the efficient set when using a larger K. We also plot the estimated efficient frontier using the estimated ˆr for K = 41 in Figure 3a. We can see that the estimated efficient frontier is very close to the real one, showing that our algorithm works quite well in learning expected returns in portfolio optimization. We also plot our estimation on the distribution of the weight of f1(x) among the 1000 decision makers. As shown in Figure 3b, the distribution follows roughly normal distribution. We apply Chi-square goodness-of-fit tests to support our hypotheses.\n\nTable 1: Estimation Error for Different K\n\nK\n\n6\n\n11\n\n21\n\n41\n\n∥ˆr − rtrue∥2\n\n0.1270\n\n0.1270\n\n0.0420\n\n0.0091\n\n5 CONCLUSION AND FUTURE WORK\n\nIn this paper, an online learning method to learn the parameters of the multiobjective optimization problems from noisy observations is developed and implemented. We prove that this framework converges at a rate of O(1/ T ) under suitable conditions. Nonetheless, as shown in multiple experiments using both synthetic and real world datasets, even if these conditions are not satisfied, we still observe a fast convergence rate and a strong robustness to noisy observations. Thus, it would be interesting to analyze to what extent these conditions can be relaxed.\n\n√\n\nAlso, we note that our model naturally follows the contextual bandit setting. We can view the decision yt observed at time t as the context. The learner then takes the action θt and the loss is jointly determined by the context and the action. Compared to the vast majority of literature surveyed in Zhou (2015), the main technical difficulty in our model is how to set an appropriate reward (loss) given the context (yt) and the action (θt). Intuitively, we set the loss as the difference between the context (yt) and another context generated by the action. Motivated by this observation, one future work is to integrate classical contextual bandits algorithms into our model. Particularly, we think that algorithms without the Linear Realizability Assumption (the reward is linear with respect to the context), such as KernelUCB, might fit well in our problem.\n\n9\n\n0.20.220.240.260.280.30.320.34Standard Deviation of Portfolio Returns (Annualized)0.060.080.10.120.140.160.180.2Mean of Portfolio Returns (Annualized)Efficient Frontier and Estimated Efficient FrontierReal Efficient FrontierEstimated Efficient FrontierUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRavindra K Ahuja and James B Orlin. Inverse optimization. Operations Research, 49(5):771–783,\n\n2001.\n\nAnil Aswani, Zuo-Jun Shen, and Auyon Siddiq. Inverse optimization with noisy data. Operations\n\nResearch, 2018.\n\nAndreas B ̈armann, Sebastian Pokutta, and Oskar Schneider. Emulating the expert: Inverse optimiza-\n\ntion through online learning. In ICML, 2017.\n\nDimitris Bertsimas, Vishal Gupta, and Ioannis Ch Paschalidis. Data-driven estimation in equilibrium\n\nusing inverse optimization. Mathematical Programming, 153(2):595–633, 2015.\n\nJeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to\n\nnumerical computing. SIAM Review, 59(1):65–98, 2017.\n\nJ Fr ́ed ́eric Bonnans and Alexander Shapiro. Optimization problems with perturbations: A guided\n\ntour. SIAM Review, 40(2):228–264, 1998.\n\nL ́eon Bottou. On-line learning and stochastic approximations. In On-line Learning in Neural Net-\n\nworks, pp. 9–42. Cambridge University Press, 1999.\n\nL ́eon Bottou and Yann L Cun. Large scale online learning. In NIPS, 2004.\n\nTimothy CY Chan, Tim Craig, Taewoo Lee, and Michael B Sharpe. Generalized inverse multiobjective optimization with application to cancer therapy. Operations Research, 62(3):680–695, 2014.\n\nChaosheng Dong and Bo Zeng. Inferring parameters through inverse multiobjective optimization.\n\narXiv preprint arXiv:1808.00935, 2018.\n\nChaosheng Dong and Bo Zeng. Expert learning through generalized inverse multiobjective opti-\n\nmization: Models, insights, and algorithms. In ICML, 2020.\n\nChaosheng Dong and Bo Zeng. Wasserstein distributionally robust inverse multiobjective optimiza-\n\ntion. In AAAI, 2021.\n\nChaosheng Dong, Yiran Chen, and Bo Zeng. Generalized inverse optimization through online learn-\n\ning. In NeurIPS, 2018.\n\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\n\nstochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.\n\nPeyman Mohajerin Esfahani, Soroosh Shafieezadeh-Abadeh, Grani A Hanasusanto, and Daniel Kuhn. Data-driven inverse optimization with imperfect information. Mathematical Programming, 167(1):191–234, 2018.\n\nSaul Gass and Thomas Saaty. The computational algorithm for the parametric objective function.\n\nNaval Research Logistics, 2(1-2):39–45, 1955.\n\nInc. Gurobi Optimization. Gurobi optimizer reference manual, 2016. URL http://www.\n\ngurobi.com.\n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning.\n\nSpringer, 2001.\n\nElad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization,\n\n2(3-4):157–325, 2016.\n\nGarud Iyengar and Wanmo Kang. Inverse conic programming with applications. Operations Re-\n\nsearch Letters, 33(3):319–330, 2005.\n\nArezou Keshavarz, Yang Wang, and Stephen Boyd.\n\nImputing a convex objective function.\n\nIn\n\nIntelligent Control (ISIC), 2011 IEEE International Symposium on, pp. 613–619. IEEE, 2011.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nBrian Kulis and Peter L Bartlett. Implicit online learning. In ICML, 2010.\n\nHarry Markowitz. Portfolio selection. The Journal of Finance, 7(1):77–91, 1952.\n\nKaisa Miettinen. Nonlinear Multiobjective Optimization, volume 12. Springer Science & Business\n\nMedia, 2012.\n\nAndrew J. Schaefer. Inverse integer programming. Optimization Letters, 3(4):483–489, 2009.\n\nShai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends®\n\nin Machine Learning, 4(2):107–194, 2011.\n\nYingcong Tan, Daria Terekhov, and Andrew Delong. Learning linear programs from optimal deci-\n\nsions. arXiv preprint arXiv:2006.08923, 2020.\n\nJialei Wang, Weiran Wang, and Nathan Srebro. Memory and communication efficient distributed stochastic optimization with minibatch prox. In Proceedings of the 2017 Conference on Learning Theory, volume 65, pp. 1882–1919, 2017.\n\nLizhi Wang. Cutting plane algorithms for the inverse mixed integer linear programming problem.\n\nOperations Research Letters, 37(2):114–116, 2009.\n\nLi Zhou. A survey on contextual multi-armed bandits. arXiv preprint arXiv:1508.03326, 2015.\n\nA APPENDIX\n\nA.1 OMITTED MATHEMATICAL REFORMULATIONS\n\nBefore giving the reformulations, we first make some discussions about the surrogate loss functions.\n\nlK(y, θ) = min\n\nzk∈{0,1}\n\n(cid:88)\n\n∥y −\n\nzkxk∥2\n\n2\n\nk∈[K]\n\n= min\n\nzk∈{0,1}\n\n(cid:88)\n\nk∈[K]\n\n∥y − zkxk∥2\n\n2 − (K − 1)∥y∥2\n\n2\n\nwhere xk ∈ S(wk, θ) and (cid:80)\n\nk∈[K] zk = 1.\n\nSince (K − 1)∥y∥2 function when solving the optimization program in the implicit update,\n\n2 is a constant, we can safely drop it and use the following as the surrogate loss\n\nlK(y, θ) = min\n\nzk∈{0,1}\n\n(cid:88)\n\n∥y − zkxk∥2\n\n2\n\nk∈[K]\n\nwhere xk ∈ S(wk, θ) and (cid:80)\n\nk∈[K] zk = 1.\n\nA.1.1 SINGLE LEVEL REFORMULATION FOR THE INVERSE MULTIOBJECTIVE\n\nOPTIMIZATION PROBLEM\n\nThe parametrized mulobjective optimization problem is\n\nmin x∈Rn s.t.\n\nf (x, θ)\n\ng(x) ≤ 0\n\nMOP\n\nwhere\n\nf (x, θ) = (f1(x, θ), f2(x, θ), . . . , fp(x, θ))T\n\ng(x) = (g1(x), . . . , gq(x))T\n\nThen, the single level reformulation for the Implicit update in the paper is given in the following\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nmin b\n\ns.t.\n\n1\n\n2 ∥θ − θt∥2\n\n2 + ηt\n\n(cid:80)\n\nk∈[K]\n\n∥yt − θk∥2\n\n2\n\nθ ∈ Θ \n\n \n\nk g(xk) = 0\n\ng(xk) ≤ 0, uk ≥ 0 uT ∇xk wT 0 ≤ θk ≤ Mkzk xk − Mk(1 − zk) ≤ θk ≤ xk\n\n(cid:80)\n\nzk = 1\n\nk f (xk, θ) + uk · ∇xk g(xk) = 0\n\n\n\n \n\n∀k ∈ [K]\n\n∀k ∈ [K] ∀k ∈ [K]\n\nk∈[K]\n\nxk ∈ Rn, uk ∈ Rm\n\n+ , tk ∈ {0, 1}m, zk ∈ {0, 1} ∀k ∈ [K]\n\nA.1.2 SINGLE LEVEL REFORMULATION FOR THE INVERSE MULTIOBJECTIVE QUADRATIC\n\nPROBLEM\n\nWhen the objective functions are quadratic and the feasible region is a polyhedron, the multiobjective optimization has the following form\n\n\n\n \n\nmin x∈Rn\n\n1 x\n\n1\n\n2 xT Q1x + cT ... 2 xT Qpx + cT\n\n1\n\np x\n\n\n\n \n\nMQP\n\ns.t. Ax ≥ b\n\nwhere Ql ∈ Sn\n\n+ (the set of symmetric positive semidefinite matrices) for all l ∈ [p]..\n\nWhen trying to learn {cl}l∈[p], the single level reformulation for the Implicit update in the paper is given in the following\n\nmin cl\n\ns.t.\n\n1 2\n\n(cid:80)\n\nl∈[p]\n\n∥cl − ct\n\nl∥2\n\n2 + ηt\n\n(cid:80)\n\nk∈[K]\n\n∥yt − θk∥2\n\n2\n\ncl ∈ (cid:101)Cl \n\n \n \n\nAxk ≥ b, uk ≥ 0 uk ≤ M tk Axk − b ≤ M (1 − tk) kQ1 + · · · + wp (w1\n\nkQp)xi + w1\n\nkc1 + · · · + wp\n\nkcp − AT uk = 0\n\n0 ≤ θk ≤ Mkzk xk − Mk(1 − zk) ≤ θk ≤ xk\n\n(cid:80)\n\nzk = 1\n\n\n\n \n \n\n∀l ∈ [p]\n\n∀k ∈ [K]\n\n∀k ∈ [K] ∀k ∈ [K]\n\nk∈[K]\n\nxk ∈ Rn, uk ∈ Rm\n\n+ , tk ∈ {0, 1}m, zk ∈ {0, 1}\n\n∀l ∈ [p] ∀k ∈ [K]\n\nwhere ct\n\nl is the estimation of cl at the tth round, and (cid:101)Cl is a convex set for each l ∈ [p].\n\nWe have a similar single level reformulation when learning the Right-hand side b. Clearly, this is a Mixed Integer Second Order Cone program(MISOCP) when learning either cl or b.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA.2 OMITTED PROOFS\n\nA.2.1 STRONGLY CONVEX OF wT f (x, θ) AS STATED UNDER ASSUMPTION 3.1\n\nProof. By the definition of λ,\n\n(cid:18)\n\n∇wT f (y, θ) − ∇wT f (x, θ)\n\n(cid:19)T\n\n(cid:18)\n\n(y − x) =\n\n∇\n\np (cid:80)\n\nwlfl(y, θ) − ∇\n\n(cid:19)T\n\nwlfl(x, θl)\n\n(y − x)\n\np (cid:80)\n\nl=1\n\n(cid:19)T\n\nl=1 (cid:18)\n\n=\n\n≥\n\np (cid:80)\n\nl=1 p\n(cid:80)\n\nl=1\n\nwl\n\n∇fl(y, θl) − ∇fl(x, θl)\n\n(y − x)\n\nwlλl∥x − y∥2\n\n2 ≥ η∥x − y∥2\n\n2\n\np (cid:80)\n\nl=1\n\nwl\n\n= λ∥x − y∥2 2\n\nThus, wT f (x, θ) is strongly convex for x ∈ Rn.\n\nA.2.2 PROOF OF LEMMA 3.1\n\nProof. By Assumption 3.1(b), we know that S(w, θ) is a single-valued set for each w ∈ Wp. Thus, ∀y ∈ Y, ∀θ1, θ2 ∈ Θ, ∃w1, w2 ∈ Wp, s.t.\n\nx(θ1) = S(w1, θ1), x(θ2) = S(w2, θ2)\n\nWithout of loss of generality, let lK(y, θ1) ≥ lK(y, θ2). Then,\n\n|lK(y, θ1) − lK(y, θ2)| = lK(y, θ1) − lK(y, θ2) = ∥y − x(θ1)∥2 2 − ∥y − x(θ2)∥2 = ∥y − S(w1, θ1)∥2 ≤ ∥y − S(w2, θ1)∥2 = ⟨S(w2, θ2) − S(w2, θ1), 2y − S(w2, θ1) − S(w2, θ2)⟩ ≤ 2(B + R)∥S(w2, θ2) − S(w2, θ1)∥2\n\n2 − ∥y − S(w2, θ2)∥2 2 − ∥y − S(w2, θ2)∥2\n\n2\n\n2\n\n2\n\nThe last inequality is due to Cauchy-Schwartz inequality and the Assumptions 3.1(a), that is\n\n∥2y − S(w2, θ1) − S(w2, θ2)∥2 ≤ 2(B + R)\n\n(6)\n\n(7)\n\nNext, we will apply Proposition 6.1 in Bonnans & Shapiro (1998) to bound ∥S(w2, θ2) − S(w2, θ1)∥2.\n\nUnder Assumptions 3.1 - 3.2, the conditions of Proposition 6.1 in Bonnans & Shapiro (1998) are satisfied. Therefore,\n\n∥S(w2, θ2) − S(w2, θ1)∥2 ≤\n\n2κ λ\n\n∥θ1 − θ2∥2\n\n(8)\n\nPlugging equation 7 and equation 8 in equation 6 yields the claim.\n\nA.2.3 PROOF OF THEOREM 3.2\n\nProof. We will extend Theorem 3.2 in Kulis & Bartlett (2010) to prove our theorem.\n\nLet Gt(θ) = 1\n\n2 ∥θ − θt∥2\n\n2 + ηtl(yt, θ).\n\nWe will now show the loss function is convex. The first step is to show that if Assumption 3.3 holds, then the loss function l(y, θ) is convex in θ.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFirst, suppose Assumption 3.3(a) hold. Then,\n\nαl(y, θ1) + βl(y, θ2) − l(y, αθ1 + βθ2)\n\n2 + β∥y − x(θ2)∥2 2 + β∥y − x(θ2)∥2\n\n2 − ∥y − x(αθ1 + βθ2)∥2 2 − ∥y − αx(θ1) − βx(θ2)∥2\n\n2\n\n2\n\n= α∥y − x(θ1)∥2 ≥ α∥y − x(θ1)∥2 = αβ∥x(θ1) − x(θ2)∥2 ≥ 0\n\n2\n\n(By Assumption 3.3(a))\n\n(9)\n\nSecond, suppose Assumption 3.3(b) holds. Then,\n\nαl(y, θ1) + βl(y, θ2) − l(y, αθ1 + βθ2)\n\n= α∥y − x(θ1)∥2 = α∥y − x(θ1)∥2\n\n2 + β∥y − x(θ2)∥2 2 + β∥y − x(θ2)∥2\n\n2 − ∥y − x(αθ1 + βθ2)∥2 2 − ∥y − αx(θ1) − βx(θ2)∥2\n\n2\n\n2\n\n+∥y − αx(θ1) − βx(θ2)∥2\n\n2 − ∥y − x(αθ1 + βθ2)∥2\n\n2\n\n= αβ∥x(θ1) − x(θ2)∥2 = αβ∥x(θ1) − x(θ2)∥2 ≥ αβ∥x(θ1) − x(θ2)∥2\n\n2 + ∥y − αx(θ1) − βx(θ2)∥2 2 − ∥y − x(αθ1 + βθ2)∥2 2 − ⟨αx(θ1) + βx(θ2) − x(αθ1 + βθ2), 2y − x(αθ1 + βθ2) − αx(θ1) − βx(θ2)⟩ 2 − ∥αx(θ1) + βx(θ2) − x(αθ1 + βθ2)∥2∥2y − x(αθ1 + βθ2) − αx(θ1) − βx(θ2)∥2\n\n2\n\n(10)\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\nThe last inequality is by Cauchy-Schwartz inequality. Note that\n\n∥αx(θ1) + βx(θ2) − x(αθ1 + βθ2)∥2∥2y − x(αθ1 + βθ2) − αx(θ1) − βx(θ2)∥2\n\n≤ 2(B + R)∥αx(θ1) + βx(θ2) − x(αθ1 + βθ2)∥2 ≤ αβ∥x(θ1) − x(θ2)∥2\n\n(By Assumption 3.3(b))\n\nPlugging equation 11 in equation 10 yields the result.\n\nUsing Theorem 3.2. in Kulis & Bartlett (2010), for αt ≤ Gt(θt+1)\n\nGt(θt) , we have\n\nRT ≤ (cid:80)T + 1 2ηt\n\nt=1\n\n1 ηt\n\n(1 − αt)ηtl(yt, θt)\n\n(∥θt − θ∗∥2\n\n2 − ∥θt+1 − θ∗∥2 2)\n\nNotice that\n\nGt(θt) − Gt(θt+1)\n\n= ηt(l(yt, θt) − l(yt, θt+1)) − 1 ≤ 4(B+R)κηt ≤ 8(B+R)2κ2η2\n\n∥θt − θt+1∥2 − 1\n\nλ\n\nt\n\nλ2\n\n2 ∥θt − θt+1∥2\n\n2\n\n2 ∥θt − θt+1∥2\n\n2\n\nThe first inequality follows by applying Lemma 3.1.\n\nLet αt = Rt(θt+1)\n\nRt(θt) . Using equation 13, we have\n\n(1 − αt)ηtl(yt, θt) = (1 − αt)Gt(θt)\n\n= Gt(θt) − Gt(θt+1) ≤ 8(B+R)2κ2η2\n\nt\n\nλ2\n\nPlug equation 14 in equation 12, and note the telescoping sum,\n\nRT ≤\n\nT (cid:88)\n\nt=1\n\n8(B + R)2κ2ηt λ2\n\n+\n\nT (cid:88)\n\nt=1\n\n1 2ηt\n\n(∥θt − θ∗∥2\n\n2 − ∥θt+1 − θ∗∥2 2)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nDλ\n\nSetting ηt = telescopes and θ1 = 0, ∥θ∗∥2 ≤ D. The first sum simplifies using (cid:80)T the result\n\n, we can simplify the second summation to D(B+R)κ\n\n1√ t\n\n≤ 2\n\n2(B+R)κ\n\nt=1\n\n2t\n\n√\n\nλ\n\n√\n\n2\n\n√\n\nsince the sum\n\nT − 1 to obtain\n\n√\n\n4\n\nRT ≤\n\n2(B + R)Dκ\n\n√\n\nλ\n\nT .\n\nA.3 OMITTED EXAMPLES\n\nA.3.1 EXAMPLES FOR WHICH ASSUMPTION 3.3 HOLDS\n\nConsider for example the following quadratic program\n\nmin x∈Rn\n\n(cid:18)xT x − 2θT xT x − 2θT\n\n1 x 2 x\n\n(cid:19)\n\ns.t.\n\n0 ≤ x ≤ 10\n\nOne can check that Assumption 3.3 (a) is indeed satisfied. For example, let n = 1. Then, W.L.O.G, let θ1 ≤ θ2. Then, XE(θ) = [θ1, θ2]. Consider two parameters that θ1 = (θ1 2) ∈ [0, 10]2. For all α ∈ [0, 1],\n\n2), θ2 = (θ2\n\n1, θ2\n\n1, θ1\n\nXE(αθ1 + (1 − α)θ2) = [αθ1\n\n1 + (1 − α)θ2\n\n1, αθ1\n\n2 + (1 − α)θ2 2]\n\nAlthough tedious, one can check that one can check that Assumption 3.3 (a) is indeed satisfied.\n\nA.4 DATA FOR THE PORTFOLIO OPTIMIZATION PROBLEM\n\nTable 2: True Expected Return\n\nSecurity\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\nExpected Return\n\n0.1791\n\n0.1143\n\n0.1357\n\n0.0837\n\n0.1653\n\n0.1808\n\n0.0352\n\n0.0368\n\nTable 3: True Return Covariances Matrix\n\nSecurity\n\n1\n\n1 2\n3 4\n5 6\n7 8\n\n0.1641 0.0299 0.0478 0.0491 0.0580 0.0871 0.0603 0.0492\n\n2\n\n0.0299 0.0720 0.0511 0.0287 0.0527 0.0297 0.0291 0.0326\n\n3\n\n0.0478 0.0511 0.0794 0.0498 0.0664 0.0479 0.0395 0.0523\n\n4\n\n0.0491 0.0287 0.0498 0.1148 0.0336 0.0503 0.0326 0.0447\n\n5\n\n0.058 0.0527 0.0664 0.0336 0.1073 0.0483 0.0402 0.0533\n\n6\n\n0.0871 0.0297 0.0479 0.0503 0.0483 0.1134 0.0591 0.0387\n\n7\n\n0.0603 0.0291 0.0395 0.0326 0.0402 0.0591 0.0704 0.0244\n\n8\n\n0.0492 0.0326 0.0523 0.0447 0.0533 0.0387 0.0244 0.1028\n\nA.5 APPROXIMATION ERROR\n\nTheorem A.1. Under Assumption 3.1, we have that ∀y ∈ Y, ∀θ ∈ Θ,\n\n0 ≤ lK(y, θ) − l(y, θ) ≤\n\n4(B + R)ζ λ\n\n·\n\n2p Λ − 1\n\n,\n\n√\n\nwhere\n\nFurthermore,\n\nK =\n\n(Λ + p − 2)! (Λ − 1)!(p − 1)!\n\n, ζ =\n\nmax l∈[p],x∈X(θ),θ∈Θ\n\n|fl(x, θ)|.\n\n0 ≤ lK(y, θ) − l(y, θ) ≤\n\n16e(B + R)ζ λ\n\n·\n\n1\n\n1 p−1\n\nK\n\n.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nThus, the surrogate loss function uniformly converges to the loss function at the rate of O(1/K p−1 ). Note that this rate exhibits a dependence on the number of objective functions p. As p increases, we might require (approximately) exponentially more weight samples {wK}k∈[K] to achieve an approximation accuracy. In fact, this phenomenon is a reflection of curse of dimensionality Hastie et al. (2001), a principle that estimation becomes exponentially harder as the number of dimension increases. In particular, the dimension here is the number of objective functions p. Naturally, one way to deal with the curse of dimensionality is to employ dimension reduction techniques in statistics to find a low-dimensional representation of the objective functions.\n\n1\n\nExample A.1. When p = 2, MOP is a bi-objective decision making problem. Then, Theorem A.1 shows that lK(y, θ) − l(y, θ) is of O(1/K). That is, lK(y, θ) asymptotically converges to l(y, θ) sublinearly.\n\nProof. By definition,\n\nlK(y, θ) − l(y, θ) =\n\nmin\n\nx∈ (cid:83)\n\nk∈[K]\n\nS(wk,θ)\n\n∥y − x∥2\n\n2 − min\n\n∥y − x∥2\n\n2 ≥ 0.\n\nx∈XE (θ)\n\nLet ∥y − S(wy\n\nk , θ)∥2\n\n2 =\n\nmin\n\nx∈ (cid:83)\n\nS(wk,θ)\n\n∥y − x∥2\n\n2, and ∥y − S(wy, θ)∥2\n\n2 = min\n\n∥y − x∥2\n\n2. Let\n\nx∈XE (θ)\n\nwy\n\nk′ be the closest weight sample among {wk}k∈[K] to wy. Then,\n\nk∈[K]\n\n2 − ∥y − S(wy, θ)∥2 2 − ∥y − S(wy, θ)∥2\n\nlK(y, θ) − l(y, θ) = ∥y − S(wy ≤ ∥y − S(wy = (2y − S(wy ≤ ∥2y − S(wy ≤ 2(B + R)∥S(wy, θ) − S(wy ≤ 4(B+R)ζ\n\nk , θ)∥2 k′, θ)∥2 k′, θ) − S(wy, θ))T (S(wy, θ) − S(wy k′, θ) − S(wy, θ)∥2∥S(wy, θ) − S(wy k′, θ)∥2\n\n· ∥wy − wy\n\n√\n\np\n\n2\n\n2\n\nk′∥2,\n\nλ\n\nk′, θ)) k′, θ)∥2\n\n(15)\n\nwhere ζ =\n\nmax l∈[p],x∈X(θ),θ∈Θ\n\n|fl(x, θ)|. The third inequality is due to Cauchy Schwarz inequality.\n\nUnder Assumption 3.1, we can apply Lemma 4 in Dong & Zeng (2018) to yield the last inequality. Next, we will show that ∀w ∈ Wp, the distance between w and its closest weight sample among {wk}k∈[K] is upper bounded by the function of K and p and nothing else. More precisely, we will show that\n\nsup w∈Wp\n\nmin k∈[K]\n\n∥w − wk∥2 ≤\n\nΛ − 1\n\n.\n\n(16)\n\n√\n\n2\n\nHere, Λ is the number of evenly spaced weight samples between any two extreme points of Wp. Note that {wk}k∈[K] are evenly sampled from Wp, and that the distance between any two extreme points of Wp equals to 2. Hence, the distances between any two neighboring weight samples are equal and can be calculated as the distance between any two extreme points of Wp divided by Λ − 1. Proof of equation 16 can be done by further noticing that the distance between any w and {wk}k∈[K] is upper bounded by the distances between any two neighboring weight samples.\n\n√\n\nCombining equation 15 and equation 16 yields that\n\n0 ≤ lK(y, θ) − l(y, θ) ≤\n\n4(B + R)ζ λ\n\n·\n\n√\n\n2p Λ − 1\n\n,\n\n(17)\n\nThen, we can prove that the total number of weight samples K and Λ has the following relationship:\n\nK =\n\n(cid:16)Λ + p − 2 p − 1\n\n(cid:17)\n\n(18)\n\nProof of equation 18 can be done by induction with respect to p. Obviously, equation 18 holds when p = 2 as K = Λ. Assume equation 18 holds for the ≤ p − 1 cases. For ease of notation, denote\n\nK Λ\n\np =\n\n(cid:16)Λ + p − 2 p − 1\n\n(cid:17)\n\n.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nThen, for the p case, we note that the weight samples can be classified into two categories: wp = 0; wp > 0. For wp = 0, the number of weight samples is simply K Λ p−1. For wp > 0, the number of weight samples is K Λ−1\n\n. Thus,\n\np\n\nK = K Λ\n\np−1 + K Λ−1\n\np\n\n.\n\n(19)\n\nIteratively expanding K Λ−1\n\np\n\nthrough the same argument as equation 18 and using the fact that\n\n(cid:17)\n\n(cid:16)n k\n\n=\n\n(cid:17)\n\n(cid:16)n − 1 k − 1\n\n+\n\n(cid:17)\n\n(cid:16)n − 1 k\n\n,\n\nwe have\n\np−1 + K Λ−1\n\np\n\n= K Λ\n\np−1 + K Λ−1\n\np−1 + K Λ−2\n\np\n\np−1 + K Λ−1\n\np−1 + K 1\n\np\n\np−1 + · · · + K 2 (cid:16)Λ + p − 4 (cid:17) p − 2\n\n+\n\n(cid:17)\n\n+ · · · +\n\n(cid:17)\n\n(cid:16)p − 1 p − 2\n\n+\n\n(cid:17)\n\n(cid:16)p − 1 p − 1\n\n(20)\n\nK = K Λ ... = K Λ\n\n=\n\n(cid:16)Λ + p − 3 p − 2 = (Λ+p−2)!\n\n(Λ−1)!(p−1)!\n\nTo this end, we complete the proof of equation 18.\n\nFurthermore, we notice that\n\nK =\n\n(Λ + p − 2)! (Λ − 1)!(p − 1)!\n\n≤\n\n(Λ + p − 2)p−1 (p − 1)!\n\n<\n\n(cid:18) Λ + p − 2 p − 1\n\n(cid:19)p−1\n\n· ep−1.\n\nThen, when Λ ≥ p(K ≥ 2p−1), through simple algebraic calculation we have\n\ne\n\n1 p−1\n\nK\n\n>\n\np − 1 Λ + p − 2\n\n>\n\n1 4\n\n·\n\np Λ − 1\n\nWe complete the proof by combining equation 17 and equation 21 and noticing that\n\n(21)\n\n√\n\n2p ≤ p.\n\n17",
    "reference": "# Summary Of The Paper\n\nThis paper is very technical and I'll first say I am not familiar with the literature. However, as somewhat an \"outsider\" to this problem, hopefully I can contribute by asking the right questions, which can help the author make this work more accessible.\n\nMy naive understanding of this paper is that it's basically doing reward learning (or inverse reinforcement learning). Given a black box agent with some latent reward function (of a particular form, that of min of bunch of other functions), we can observe the agent's actions to infer what the reward function is. The observations come sequentially, but we do not get to control what observation to make.\n\n# Strength And Weaknesses\n\nstrength : It tackles a new problem of multi-objective reward learning, of a particular form: min(f1(x,\\theta) ... fn(x,\\theta)). The paper was able to take this special form, along with some assumptions, to develop a set of update-rules to guess what \\theta can be as online observations come in. Even if the user (like me) does not grasp the full derivation, this algorithm can easily exist in a package to be used as black box.\n\nweakness : for me the paper is easy to follow wherever it is self-contained, but then some thm or corr. was invoked, and this \"jump\" makes the paper hard to follow. however this is unlikely to be an issue with people familiar with the field.\n\nI do have some questions, which would be great to have answered:\n\n1. when we say the agent is minimizing regret, does it mean it is sufficient agent comes up with a hypothesis that explains the data as well as the original \\theta, rather than trying to infer the original \\theta itself?\n\n2. I always find the connection between bandit-like literatures and bayesian inference related, yet the two use very different language. For me, I'd think we just put some prior over the hypothesis P(\\theta), and we do posterior inference of P(\\theta | Data) while assuming some forward data generating process P(Data | \\theta). Or more simply, we can find the maximum-likelihood estimator without the full posterior. There must be a good reason why this kind of language isn't the way to explain your work, but what is it? \n\n3. How would the problem change if you are allowed to take active samples? i.e. present the agent with a scenario as a query, and see how it respond to it in the style of active learning. Can your approach be made to work in this setting? How would you select a query? How would you select a query without making some distributional assumptions of the objective function, other than it is a min of a bunch of functions ?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nclarity : it appears good -- I can follow everything until 2.2.1\n\nnovelty : the authors claims that nobody has worked on the problem of inferring parameters for the form min(f1(x,theta1)...) before, so this work is novel.\n\nquality : appears to be good\n\n# Summary Of The Review\n\nfrom a non-expert point of view, this paper can result in an artifact that can exist in scipy as a black-box, which is definitely a contribution. I would defer technical correctness to an expert.\n\nafter discussion with other reviewers, who knows the work better, I'm lowering my score to a 6.\nI still think this work is interesting though.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSINGLE-LEVEL ADVERSARIAL DATA SYNTHESIS BASED ON NEURAL TANGENT KERNELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGenerative adversarial networks (GANs) have achieved impressive performance in data synthesis and have driven the development of many applications. However, GANs are known to be hard to train due to their bilevel objective, which leads to the problems of convergence, mode collapse, and gradient vanishing. In this paper, we propose a new generative model called the generative adversarial NTK (GA-NTK) that has a single-level objective. The GA-NTK keeps the spirit of adversarial learning (which helps generate plausible data) while avoiding the training difficulties of GANs. This is done by modeling the discriminator as a Gaussian process with a neural tangent kernel (NTK-GP) whose training dynamics can be completely described by a closed-form formula. We analyze the convergence behavior of GA-NTK trained by gradient descent and give some sufficient conditions for convergence. We also conduct extensive experiments to study the advantages and limitations of GA-NTK and propose some techniques that make GA-NTK more practical.1\n\n1\n\nINTRODUCTION\n\nGenerative adversarial networks (GANs) (Goodfellow et al., 2014; Radford et al., 2016), a branch of deep generative models based on adversarial learning, have received much attention due to their novel problem formulation and impressive performance in data synthesis. Variants of GANs have also driven recent developments of many applications, such as super-resolution (Ledig et al., 2017), image inpainting (Xu et al., 2014), and video generation (Vondrick et al., 2016).\n\nA GANs framework consists of a discriminator network D and a generator network G parametrized by θD and θG, respectively. Given a d-dimensional data distribution Pdata and a c-dimensional noise distribution Pnoise, the generator G maps a random noise z ∈ Rc to a point G(z) ∈ Rd in the data space, while the discriminator D takes a point x(cid:48) ∈ Rd as the input and tells whether x(cid:48) is real or fake, i.e., D(x(cid:48)) = 1 if x(cid:48) ∼ Pdata and D(x(cid:48)) = 0 if x(cid:48) ∼ Pgen, where Pgen is the distribution of G(z) and z ∼ Pnoise. The objective of GANs is typically formulated as a bilevel optimization problem:\n\narg min\n\nθG\n\nmax θD\n\nEx∼Pdata[log D(x)] + Ez∼Pnoise [log(1 − D(G(z)))].\n\n(1)\n\nThe discriminator D and generator G aim to break each other through the inner max and outer min objectives, respectively. The studies by Goodfellow et al. (2014); Radford et al. (2016) show that this adversarial formulation can lead to a better generator that produces plausible data points/images.\n\nHowever, GANs are known to be hard to train due to the following issues (Goodfellow, 2016). Failure to converge. In practice, Eq. (1) is usually only approximately solved by an alternating first-order method such as the alternating stochastic gradient descent (SGD). The alternating updates for θD and θG may cancel each other’s progress. During each alternating training step, it is also tricky to balance the number of SGD updates for θD and that for θG, as a too small or large number for θD leads to low-quality gradients for θG. Mode collapse. The alternating SGD is attracted by stationary points and therefore is not good at distinguishing between a minθG maxθD problem and a maxθD minθG problem. When the solution to the latter is returned, the generator tends to always produce the points at modes that best deceive the discriminator, making Pgen of low\n\n1Our code is available on GitHub at https://github.com/ga-ntk/ga-ntk.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ndiversity.2 Vanishing gradients. At the beginning of a training process, the finite real and fake training data may not overlap with each other in the data space, and thus the discriminator may be able to perfectly separate the real from fake data. Given the cross-entropy loss (or more generally, any f -divergence measure (Rényi et al., 1961) between Pdata and Pgen), the value of the discriminator becomes saturated on both sides of the decision boundary, resulting in zero gradients for θG.\n\nIn this paper, we argue that the above issues are rooted in the modeling of D. In most existing variants of GANs, the discriminator is a deep neural network with explicit weights θD. Under gradient descent, the gradients of θG in Eq. (1) cannot be back-propagated through the inner maxθD problem because otherwise it requires the computation of high-order derivatives of θD. This motivates the use of alternating SGD, which in turn causes the convergence issues and mode collapse. Furthermore, the D is a single network whose particularity may cause a catastrophic effect, such as the vanishing gradients, during training.\n\nWe instead model the discriminator D as a Gaussian process whose mean and covariance are governed by a kernel function called the neural tangent kernel (NTK-GP) (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019). The D approximates an infinite ensemble of infinitely wide neural networks in a nonparametric manner and has no explicit weights. In particular, its training dynamics can be completely described by a closed-form formula. This allows us to simplify adversarial data synthesis into a single-level optimization problem, which we call the generative adversarial NTK (GA-NTK). Moreover, since D is an infinite ensemble of networks, the particularity of a single element network does not drastically change the training process. This makes GA-NTK less prone to vanishing gradients and stabilizes training even when an f -divergence measure between Pdata and Pgen is used as the loss of D. The following summarizes our contributions:\n\n• We propose a single-level optimization method, named GA-NTK, for adversarial data synthesis. It can be solved by ordinary gradient descent, avoiding the difficulties of bi-level optimization in GANs.\n\n• We prove the convergence of GA-NTK training under mild conditions. We also show that D being an infinite ensemble of networks can provide smooth gradients for G, which stabilizes GA-NTK training and helps fight vanishing gradients.\n\n• We propose some practical techniques to reduce the memory consumption of GA-NTK\n\nduring training and improve the quality of images synthesized by GA-NTK.\n\n• We conduct extensive experiments on real-world datasets to study the advantages and limitations of GA-NTK. In particular, we find that GA-NTK has much lower sample complexity as compared to GANs, and the presence of a generator is not necessary to generate images under the adversarial setting.\n\nNote that the goal of this paper is not to replace existing GANs nor advance the state-of-the-art performance, but to show that adversarial data synthesis can be done via a single-level modeling. Our work has implications for future research. In particular, the low sample complexity makes GA-NTK suitable for applications, such as medical imaging, where data are personalized or not easily collectible. In addition, GA-NTK bridges the gap between kernel methods and adversarial data/image synthesis and thus enables future studies on the relationship between kernels and generated data.\n\n2 RELATED WORK\n\n2.1 GANS AND IMPROVEMENTS\n\nGoodfellow et al. (2014) proposes GANs and gives a theoretical convergence guarantee in the function space. However, in practice, one can only optimize the generator and discriminator in Eq. (1) in the parameter/weight space. Many techniques have been proposed to make the bilevel optimization easier. Failure to convergence. To solve this problem, studies devise new training algorithms for GANs (Nagarajan & Kolter, 2017; Daskalakis et al., 2018) or more general minimax problems (Thekumparampil et al., 2019; Mokhtari et al., 2020). But recent works by Mescheder et al. (2018); Farnia & Ozdaglar (2020) show that there may not be a Nash equilibrium solution in GANs. Mode\n\n2Mode collapse can be caused by other reasons, such as the structure of G. This paper only solves the\n\nproblem due to alternating SGD.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ncollapse. Metz et al. (2017) alleviates this issue by back-propagating the computation of θG through the discriminators trained with several steps to strengthen the minθG maxθD property. Other works mitigate mode collapse by diversifying the modes of D through regularization (Che et al., 2017; Mao et al., 2019), modeling D as an ensemble of multiple neural networks (Durugkar et al., 2017; Ghosh et al., 2018), or using additional auxiliary networks(Srivastava et al., 2017; Bang & Shim, 2021; Li et al., 2021). Vanishing gradients. Mao et al. (2017) tries to solve this problem by using the Pearson χ2-divergence between Pdata and Pgen as the loss to penalize data points that are far away from the decision boundary. However, it still suffers from vanishing gradients as any f -divergence measure, including the cross-entropy loss and Pearson χ2-divergence, cannot measure the difference between disjoint distributions (Sajjadi et al., 2018). Later studies replace the loss with either the Wasserstein distance (Arjovsky et al., 2017; Gulrajani et al., 2017) or maximum mean discrepancy (Gretton et al., 2012; Li et al., 2015; 2017) that can measure the divergence of disjoint Pdata and Pgen. In addition, the works by Miyato et al. (2018); Qi (2020) aim to constrain the Lipschitz continuity of the discriminator to prevent its value from being saturated.\n\nDespite that many efforts have been made to improve the training of GANs, most existing approaches address only one or two issues at a time with different assumptions, and in the meanwhile, they introduce new hyperparameters or side effects. For example, in the Wasserstein GANs (Arjovsky et al., 2017; Gulrajani et al., 2017) mentioned above, efficient computation of Wasserstein distance requires the discriminator to be Lipschitz continuous. However, realizing Lipschitz continuity introduces new hyperparameters and could limit the expressiveness of the discriminator (Anil et al., 2019). Until now, training GANs is still not an easy task because one has to 1) tune many hyperparameters and 2) strike a balance between the benefits and costs of different training techniques to generate satisfactory data points/images.\n\n2.2 GAUSSIAN PROCESSES AND NEURAL TANGENT KERNELS\n\nConsider an infinite ensemble of infinitely wide networks that use the mean square error (MSE) as the loss and are trained by gradient descent. Recent developments in deep learning theory show that the prediction of the ensemble can be approximated by a special instance of Gaussian process called NTK-GP (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019). The NTK-GP is a Bayesian method, so it outputs a distribution of possible values for an input point. The mean and covariance of the NTK-GP prediction are governed by a kernel function k(·, ·) called the neural tangent kernel (NTK). Given two data points xi and xj, the k(xi, xj) represents the similarity score of the two points in a kernel space, which is fixed once the hyperparameters of the initial weights, activation function, and architecture of the networks in the target ensemble are determined.\n\nHere, we focus on the mean prediction of NTK-GP as it is relevant to our study. Consider a supervised learning task given Dn = (X n ∈ Rn×d, Y n ∈ Rn×c) as the training set, where there are n examples and each example consists of a pair of d-dimensional input and c-dimensional output. Let Kn,n ∈ Rn×n be the kernel matrix for X n, i.e., K n,n j,:). Then, at time step t during gradient descent, the mean prediction of NTK-GP for X n evolve as\n\ni,j = k(X n\n\ni,:, X n\n\n(I n − e−ηKn,nt)Y n ∈ Rn×c,\n\n(2)\n\nwhere I n ∈ Rn×n is an identity matrix and η is a sufficiently small learning rate (Jacot et al., 2018; Lee et al., 2019).\n\nThe NTK used in Eq. (2) can be extended to support different network architectures, including convolutional neural networks (CNNs) (Arora et al., 2019; Novak et al., 2019b), recurrent neural networks (RNNs) (Alemohammad et al., 2021; Yang, 2019b), networks with the attention mechanism (Hron et al., 2020), and other architectures (Yang, 2019b; Arora et al., 2019). Furthermore, studies (Novak et al., 2019a; Lee et al., 2020; Arora et al., 2020; Geifman et al., 2020) show that NTK-GPs perform similarly to their finite-width counterparts (neural networks) in many situations and sometimes even better on small-data tasks.\n\nA recent study by Franceschi et al. (2021) analyzes the behavior of GANs from the NTK perspective by taking into account the alternating optimization. It shows that, in theory, the discriminator can provide a well-defined gradient flow for the generator, which is opposite to previous theoretical interpretations (Arjovsky & Bottou, 2017). Our work, on the other hand, focuses on adversarial data\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nsynthesis without alternating optimization.3 We make contributions in this direction by (1) formally proving the convergence of the proposed single-level optimization, (2) showing that a generator network is not necessary to generate plausible images (although it might be desirable), and (3) proposing the batch-wise and multi-resolutional extensions that respectively improve the memory efficiency of training and global coherency of generated image patterns.\n\n3 GA-NTK\n\nWe present a new adversarial data synthesis method, called the generative adversarial NTK (GANTK), based on the NTK theory (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019). For simplicity of presentation, we let G(z) = z ∈ Rd and focus on the discriminator for now. We will discuss the case where G(·) is a generator network in Section 3.2. Given an unlabeled, d-dimensional dataset X n ∈ Rn×d of n points, we first augment X n to obtain a labeled training set D2n = (X n ⊕ Zn ∈ R2n×d, 1n ⊕ 0n ∈ R2n), where Zn ∈ Rn×d contains n generated points, 1n ∈ Rn and 0n ∈ Rn are label vectors of ones and zeros, respectively, and ⊕ is the vertical stack operator. Then, we model a discriminator trained on D2n as an NTK-GP. Let K2n,2n ∈ R2n×2n be the kernel matrix for X n⊕Zn, where the value of each element K 2n,2n = k((X n⊕Zn)i,:, (X n⊕Zn)j,:) can be computed once we decide the initialization, activation function, and architecture of the element networks in the target infinite ensemble, i.e., the discriminator. By Eq. (2) and let λ = η · t, the mean predictions of the discriminator can be written as\n\ni,j\n\nD(X n, Zn; k, λ) = (I 2n − e−λK2n,2n\n\n)(1n ⊕ 0n) ∈ R2n,\n\nwhere I 2n ∈ R2n×2n is an identity matrix. We formulate the objective of GA-NTK as follows:\n\narg min Zn\n\nL(Zn), where L(Zn) = (cid:107)12n − D(X n, Zn; k, λ)(cid:107).\n\n(3)\n\n(4)\n\nL(·) is the loss function and 12n ∈ R2n is a vector of ones. Statistically, Eq. (4) aims to minimize the Pearson χ2-divergence (Jeffreys, 1946), a case of f -divergence, between Pdata + Pgen and 2Pgen, where Pgen is the distribution of generated points. Please see Section 6 in Appendix for more details.\n\nGA-NTK formulates an adversarial data synthesis task as a single-level optimization problem. On one hand, GA-NTK aims to find points Zn that best deceive the discriminator such that it outputs wrong labels 12n for these points. On the other hand, the discriminator is trained on D2n with the correct labels 1n ⊕ 0n and therefore has the opposite goal of distinguishing between the real and generated points. Such an adversarial setting can be made single-level because the training dynamics of the discriminator D by gradient descent can be completely described by a closed-form formula in Eq. (3)—any change of Zn causes D to be “retrained” instantly. Therefore, one can easily solve Eq. (4) by ordinary SGD.\n\nTraining. Before running SGD, one needs to tune the hyperparameter λ. We show in the next section that the value of λ should be large enough but finite. Therefore, the complete training process of GA-NTK is to 1) find the minimal λ that allows the discriminator to separate real data from pure noises in an auxiliary task, and 2) solve Zn in Eq. (4) by ordinary SGD with the fixed λ. Please see Section 7.3 in Appendix for more details.\n\n3.1 MERITS\n\nAs compared to GANs, GA-NTK offers the following advantages: Convergence. The GA-NTK can be trained by ordinary gradient descent. This gives much nicer convergence properties:\n\nTheorem 3.1 Let s be the number of the gradient descent iterations solving Eq. (4), and let Zn,(s) i,j and Zn,(0) be the solution at the s-th iteration. Suppose the following values are bounded: (a) X n ,\n∀i, j, (b) t and η, and (c) σ and L. Also, assume that (d) X n contains finite, non-identical, normalized rows. Then, for a sufficiently large t, we have\n\ni,j\n\n(cid:107)∇Zn L(Zn,(j))(cid:107)2 ≤ O(\n\nmin j≤s\n\n1 s − 1\n\n).\n\n3From GAN perspective, our work can be regarded as a special case of the framework proposed by Franceschi et al. (2021), where the discriminator neglects the effect of historical generator updates and only distinguish between the true and currently generated data at each alternating step.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nWe prove the above theorem by showing that, with a large enough λ, ∇Zn L(Zn,(s)) is smooth enough to lead to the convergence of gradient descent. For more details, please see Section 6 in Appendix. Diversity. GA-NTK avoids mode collapse due to the confusion between the min-max and max-min problems in alternating SGD. Given different initial values, the generated points in Zn can be very different from each other. No vanishing gradients, no side effects. The hyperparameter λ controls how much D should learn from the true and fake data during each iteration. Figure 5 shows the gradients of D with a finite λ, which do not saturate. This avoids the necessity of using a loss that imposes side effects, such as the Wasserstein distance (Arjovsky et al., 2017; Gulrajani et al., 2017) whose efficient evaluation requires Lipschitz continuity of D.\n\n3.2 GA-NTK IN PRACTICE\n\nScalability. To generate a large number of points, we can parallelly solve multiple Zn’s in Eq. (4) on different machines. On a single machine, the gradients of Zn need to be back-propagated through the computation of K2n,2n, which has O(n2) space complexity. This may incur scalability issues for large datasets. Although recent efforts by Arora et al. (2019); Bietti & Mairal (2019); Han et al. (2021); Zandieh et al. (2021) have been made to reduce the time and space complexity of the evaluation of NTK and its variants, they are still at an early stage of development and the consumed space in practice may still be too large. To alleviate this problem, we propose the batch-wise GANTK with the objective\n\narg min Zn\n\nE\n\nX b/2⊂X n,Zb/2⊂Zn(cid:107)1b − D(X b/2, Zb/2; k, λ)(cid:107),\n\n(5)\n\nthat can be solved using mini-batches: during each gradient descent iteration, we 1) randomly sample a batch of b rows in X n ⊕ Zn and their corresponding labels, and 2) update Zn based on Kb,b. Although the batch-wise GA-NTK is cosmetically similar to the original GA-NTK, it solves a different problem. In the original GA-NTK, the Zn aims to fool a single discriminator D trained on 2n examples, while in the batch-wise GA-NTK, the Zn’s goal is to deceive many discriminators, each trained on b examples only. Fortunately, Shankar et al. (2020); Arora et al. (2020) have shown that NTK-based methods perform well on small datasets. We will conduct experiments to verify this later.\n\nGenerator Network. So far, we let G(z) = z and show that a generator is not necessary in adversarial data synthesis.4 Nevertheless, the presence of a generator network may be favorable in some applications to save time and memory at inference time. This can be done by extending the batch-wise GA-NTK as follows:\n\narg min\n\nθG\n\nE\n\nX b/2⊂X n,Zb/2∼N (0,I)(cid:107)1b − D(X b/2, G(Zb/2; θG); k, λ)(cid:107),\n\n(6)\n\nwhere G(· ; θG) is a generator network parametrized by θG, and Z ∈ Rl where l ≤ d. Note that this is still a single-level objective, and θG can be solved by gradient descent. We denote this variant GA-NTKg.\n\nImage Quality. To generate images, one can pair up GA-NTK with a convolutional neural tangent kernel (CNTK) (Arora et al., 2019; Novak et al., 2019b; Garriga-Alonso et al., 2019; Yang, 2019a) that approximates a CNN with infinite channels. This allows the NTK-GP (discriminator) to distinguish between real and fake points based on local patterns in the pixel space. However, the images synthesized by this GA-NTK variant may lack global coherency, just like the images generated by the CNN-based GANs (Radford et al., 2016; Salimans et al., 2016). Many efforts have been made to improve the image quality of CNN-based GANs, and this paper opens up opportunities for them to be adapted to the kernel regime. In particular, we propose the multi-resolutional GA-CNTK based on the work by Wang et al. (2018), whose objective is formulated as:\n\narg min Zn\n\n(cid:88)\n\nm\n\n(cid:107)12n − Dm(poolm(X n), poolm(Zn); km, λm)(cid:107),\n\n(7)\n\nwhere Dm is an NTK-GP taking input at a particular pixel resolution and poolm(·) is a downsample operation (average pooling) applied to each row of X n and Zn. The generated points in Zn aim to simultaneously fool multiple NTK-GPs (discriminators), each classifying real and fake images at a distinct pixel resolution. The NTK-GPs working at low and high resolutions encourage global coherency and details, respectively, and together they lead to more plausible points in Zn.\n\n4In GANs, solving Z directly against a finite-width discriminator is infeasible because it amounts to finding adversarial examples (Goodfellow et al., 2015) whose gradients are known to be very noisy (Ilyas et al., 2019).\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\nWe conduct experiments to study how GA-NTK works in image generation.\n\nDatasets. We consider the unsupervised/unconditional image synthesis tasks over real-world datasets, including MNIST (LeCun et al., 2010), CIFAR-10 (Krizhevsky, 2009), CelebA (Liu et al., 2015), CelebA-HQ (Liu et al., 2015), and ImageNet (Deng et al., 2009). To improve training efficiency, we resize CelebA images to 64×64 and ImageNet images to 128×128 pixels, respectively. We also create a 2D toy dataset consisting of 25-modal Gaussian mixtures of points to visualize the behavior of different image synthesis methods. GA-NTK implementations. GA-NTK works with different NTK-GPs. For the image synthesis tasks, we consider the NTK-GPs that model the ensembles of fully-connected networks (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019) and convolutional networks (Arora et al., 2019; Novak et al., 2019b; Garriga-Alonso et al., 2019; Yang, 2019a), respectively. We implement GA-NTK using the Neural Tangents library (Novak et al., 2019a) and call the variants based on the former and latter NTK-GPs the GA-FNTK and GA-CNTK, respectively. In GA-FNTK, an element network of the discriminator has 3 infinitely wide, fully-connected layers with ReLU non-linearity, while in GA-CNTK, an element network follows the architecture of InfoGAN (Chen et al., 2016) except for having infinite filters at each layer. We tune the hyperparameters of GA-FNTK and GA-CNTK following the method proposed in Poole et al. (2016); Schoenholz et al. (2017); Raghu et al. (2017). We also implement their batchwise, generator, and multi-resolutional variants described in Section 3.2. See Section 7 in Appendix for more details. Baselines. We compare GA-NTK with some popular variants of GANs, including vanilla GANs (Goodfellow et al., 2014), DCGAN (Radford et al., 2016), LSGAN (Mao et al., 2017), WGAN (Arjovsky et al., 2017), WGAN-GP (Gulrajani et al., 2017), SNGAN (Miyato et al., 2018) and StyleGAN2 (Karras et al., 2020). To give a fair comparison, we let the discriminator of each baseline follow the architecture of InfoGAN (Chen et al., 2016) and tune the hyperparameters using grid search. Metrics. We evaluate the quality of a set of generated images using the Fréchet Inception Distance (FID) (Heusel et al., 2017). The lower the FID score the better. We find that an image synthesis method may produce downgrade images that look almost identical to some images in the training set. Therefore, we also use a metric called the average max-SSIM (AM-SSIM) that calculates the average of the maximum SSIM score (Wang et al., 2004) between Pgen and Pdata:\n\nAM-SSIM(Pgen, Pdata) = Ex(cid:48)∼Pgen[ max\n\nx∼Pdata\n\nSSIM(x(cid:48), x)].\n\nA generated image set will have a higher AM-SSIM score if it contains downgrade images. Environment and limitations. We conduct all experiments on a cluster of machines having 80 NVIDIA Tesla V100 GPUs. As discussed in 3.2, GA-NTK consumes a significant amount of memory on each machine due to the computations involved in the kernel matrix K2n,2n. With the current version of Neural Tangents library (Novak et al., 2019a) and a V100 GPU of 32GB RAM, the maximum sizes of the training set from MNIST, CIFAR-10, CelebA, and ImageNet are 1024, 512, 256, and 128, respectively (where the computation graph and backprop operations of K2n,2n consume about 27.5 GB RAM excluding other necessary operations). Since our goal is not to achieve state-of-the-art performance but to compare different image synthesis methods, we train all the methods using up to 256 images randomly sampled from all classes of MNIST, the “horse” class CIFAR-10, the “male with straight hair” class of CelebA, and the “daisy” class of ImageNet, respectively. We will conduct larger-scale experiments in Section 9.2. For more details about our experiment settings, please see Section 7 in Appendix.\n\n4.1\n\nIMAGE QUALITY\n\nWe first study the quality of the images synthesized by different methods. Table 1 summarizes the FID and AM-SSIM scores of the generated images. LSGAN and DCGAN using f -divergence as the loss function give high FID and fail to generate recognizable images on CIFAR-10 and CelebA datasets due to the various training issues mentioned previously. StyleGAN, although being able to generate impressive images with sufficient training data, gives high FID here due to the high sample complexity of the style-based generator. Other baselines, including WGAN, WGAN-GP, and SN-GAN, can successfully generate recognizable images on all datasets, as shown in Figure 1. In particular, WGAN-GP performs the best among the GAN variants. However, WGAN-GP limits the Lipschitz continuity of the discriminator and gives higher FID scores than GA-CNTK. Also, it gives higher AM-SSIM values as the size of the training set decreases, implying there are many\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: The FID and AM-SSIM scores of the images generated by different methods.\n\nMetric\n\nDCGAN\n\nLSGAN\n\nWGAN\n\nWGANGP\n\nSNGAN\n\nStyleGAN\n\nGACNTK\n\nGACNTKg\n\nFID\n\n27.43\n\nAMSSIM\n\n0.84\n\nFID\n\n31.89\n\nAMSSIM\n\n0.85\n\nFID\n\n69.76\n\nAMSSIM\n\n0.69\n\n69.76\n\n0.79\n\n38.52\n\n0.80\n\n35.33\n\n0.78\n\n50.69\n\n0.77\n\n49.28\n\n0.74\n\n50.33\n\n0.72\n\n32.49\n\n0.83\n\n30.20\n\n0.76\n\n24.37\n\n0.73\n\n57.89\n\n0.67\n\n38.33\n\n0.67\n\n29.49\n\n0.70\n\n91.82\n\n0.69\n\n88.31\n\n0.66\n\n84.7\n\n0.65\n\n31.10\n\n0.49\n\n21.14\n\n0.52\n\n14.96\n\n0.54\n\n32.43\n\n0.71\n\n36.50\n\n0.72\n\n51.21\n\n0.73\n\nFID\n\n312.21\n\n258.41\n\n117.85\n\n49.29\n\n118.16\n\n406.02\n\n55.54\n\n106.44\n\nAMSSIM\n\n0.22\n\n0.25\n\n0.29\n\n0.74\n\n0.28\n\n0.64\n\n0.41\n\nFID\n\n229.94\n\n339.27\n\n101.90\n\n68.53\n\n128.65\n\n484.36\n\n39.98\n\nAMSSIM\n\n0.36\n\n0.10\n\n0.26\n\n0.60\n\n0.21\n\n0.39\n\n0.41\n\nFID\n\n181.15\n\n255.19\n\n111.92\n\n85.34\n\n107.29\n\n426.58\n\n28.40\n\nAMSSIM\n\n0.27\n\n0.22\n\n0.22\n\n0.46\n\n0.20\n\n0.26\n\n0.42\n\nFID\n\n489.82\n\n83.71\n\n122.36\n\n83.71\n\n169.04\n\n323.37\n\n30.83\n\nAMSSIM\n\n0.02\n\n0.05\n\n0.29\n\n0.56\n\n0.29\n\n0.23\n\n0.60\n\nFID\n\n55.01\n\n450.81\n\n125.82\n\n92.73\n\n168.11\n\n337.58\n\n33.51\n\nAMSSIM\n\n0.03\n\n0.11\n\n0.28\n\n0.54\n\n0.28\n\n0.21\n\n0.51\n\nFID\n\n461.95\n\n403.79\n\n108.07\n\n79.36\n\n161.20\n\n333.16\n\n63.15\n\nAMSSIM\n\n0.04\n\n0.09\n\n0.31\n\n0.39\n\n0.27\n\n0.30\n\n0.38\n\n0.44\n\n61.19\n\n0.44\n\n55.46\n\n0.44\n\n95.91\n\n0.21\n\n58.39\n\n0.38\n\n78.46\n\n0.40\n\nT S\n\nI\n\nN M\n\n0 1\n-\n\nR A\nF\n\nI\n\nC\n\nA b\ne l\ne C\n\nn\n\n64\n\n128\n\n256\n\n64\n\n128\n\n256\n\n64\n\n128\n\n256\n\n) a\n(\n\n) b\n(\n\n) c\n(\n\n) d\n(\n\n) e\n(\n\nN A\nG W\n\nP G\nN A\nG W\n\nN A\nG N\nS\n\nK T\nN C\nA G\n\ng K\nT N\nC A\nG\n\nFigure 1: The images generated by different methods on MNIST, CIFAR-10, and CelebA datasets given only 256 training images.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) GA-CNTK\n\n(b) GA-CNTKg\n\nFigure 2: The images generated by GA-CNTK (a) without and (b) with a generator given 256 CelebA-HQ training images.\n\n(a) MNIST\n\n(b) CIFAR-10\n\n(c) CelebA\n\nFigure 3: The learning curve and image quality at different stages of a training process.\n\ndowngrade images that look identical to some training images. This is because the Wasserstein distance, which is also called the earth mover’s distance, allows fewer ways of moving when there are less available “earth” (i.e., the density values of Pdata and Pgen) due to a small n, and thus the Pgen needs to be exactly the same as Pdata to minimize the distance.5 The GA-NTK variants, including GA-CNTK and GA-CNTKg (“g” means “with generator”), perform relatively well due to their lower sample complexity, which aligns with the previous observations (Shankar et al., 2020; Arora et al., 2020) in different context.\n\nNext, we compare the images generated by the multi-resolutional GA-CNTK and GA-CNTKg (see Section 3.2) on the CelebA-HQ dataset. The multi-resolutional GA-CNTK employs 3 discriminators working at 256×256, 64×64, and 16×16 pixel resolutions, respectively. Figure 2 shows the results. We can see that the multi-resolutional GA-CNTK (without a generator) gives better-looking images than GA-CNTKg (with a generator) because learning a generator, which maps two spaces, is essentially a harder problem than finding a set of plausible z’s. Although synthesizing data faster at inference time, a generator may not be necessary to generate high-quality images under the adversarial setting.\n\n4.2 TRAINING STABILITY\n\nConvergence. Figure 3 shows the learning curve and the relationship between the image quality and the number of gradient descent iterations during a training process of GA-CNTK. We find that\n\n5The problem of Lipschitz continuity may be alleviated when n becomes larger.\n\n(a) Truth\n\n(b) Vanilla GAN (c) LSGAN (d) WGAN (e) WGAN-GP (f) SN-GAN (g) GA-FNTK\n\nFigure 4: Visualization of distribution alignment and mode collapse on a 2D toy dataset.\n\n8\n\n0101010101010101010101010101Under review as a conference paper at ICLR 2023\n\nGA-CNTK easily converges under various conditions, which is supported by Theorem 3.1. Furthermore, we can see a correlation between the image quality and the loss value—as the loss becomes smaller, the quality of the synthesized images improves. This correlation can save human labor from monitoring the training processes, which is common when training GANs. Note that the images generated in the latter stage of training contain recognizable patterns that change over training time. This is a major source of GA-CNTK creativity. Please see Section 9.4 for more discussions. Mode collapse. To study how different methods align Pgen with Pdata, we train them using a 2D toy training set where Pdata is a 25modal Gaussian mixture. We use two 3-layer fully-connected neural networks as the generator and discriminator for each baseline and an ensemble of 3-layer, infinitely wide counterpart as the discriminator in GA-FNTK. For GANs, we stop the alternating SGD training when the generator receives 1000 updates, and for GAFNTK, we terminate the GD training after 1000 iterations. Figure 4 shows the resultant Pgen of different methods. GA-FNTK avoids mode collapse due to the use of alternating SGD. Gradient vanishing. To verify that GA-NTK gives no vanishing gradients with a finite λ, we conduct an experiment using another toy dataset consisting of 256 MNIST images and 256 random noises. We replace the discriminator of GA-CNTK with a single parametric network of the same architecture but finite width. We train the finite-width network on the toy dataset by minimizing the MSE loss using gradient descent. We set the training iteration to a large value (65536) to simulate the situation where the network value becomes saturated on both sides of the decision boundary. Figure 5 compares the gradients of a generated image Zn i,: in Eq. (4) obtained from 1) the finite-width network and 2) the corresponding GA-CNTK with a large t. As Zn i,: evolves through gradient descent iterations, the norm of its gradients obtained from the finite-width discriminator quickly shrinks to zero. On the other hand, the gradient norm obtained from the discriminator of GA-CNTK is always positive thanks to the infinite ensembling.\n\nFigure 5: Comparison between the gradients of a Zi,: in Eq. (4) obtained from different types of D.\n\n4.3 SCALABILITY\n\nUnlike GA-CNTK, the GA-CNTKg is batch-wise and thus can be trained by more examples. Here, we scale up WGAN-GP and GA-CNTKg by training them on CelebA dataset consisting of 2048 images. The batch size is 256. Table 2 summarizes the FID and AM-SSIM scores of the generated images. On MNIST, WGAN-GP slightly outperforms GA-CNTKg. The training of WGAN-GP on MNIST is easy, so GA-CNTKg does not offer much advantage. However, in a more complex task like CIFAR10 or CelebA, GA-CNTKg outperforms WGAN-GP, suggesting that our single-level modeling is indeed beneficial.\n\nWe have conducted more experiments. Please see Appendix for their results.\n\n5 CONCLUSION\n\nTable 2: The FID and AM-SSIM scores of the images output by WGAN-GP and GA-CNTKg trained on 2048 CelebA images with batch size 256.\n\nn=2048\n\nMetric\n\nWGANGP\n\nGACNTKg\n\nMNIST\n\nCIFAR-10\n\nCelebA\n\nFID\n\nASSIM\n\nFID\n\nASSIM\n\nFID\n\nASSIM\n\n23.47\n\n0.786\n\n110.70\n\n0.404\n\n67.29\n\n0.337\n\n56.73\n\n0.787\n\n78.85\n\n0.432\n\n59.91\n\n0.411\n\nWe proposed GA-NTK and showed that adversarial data synthesis can be done via single-level modeling. It can be solved by ordinary gradient descent, avoiding the difficulties of bi-level training of GANs. We analyzed the convergence behavior of GA-NTK and gave sufficient conditions for convergence. Extensive experiments were conducted to study the advantages and limitations of GANTK. We proposed the batch-wise and multi-resolutional variants to improve memory efficiency and image quality, and showed that GA-NTK works either with or without a generator network. GA-NTK works well with small data, making it suitable for applications where data are hard to collect. GA-NTK also opens up opportunities for one to adapt various GAN enhancements into the kernel regime. These are matters of our future inquiry.\n\n9\n\n025005000750010000iterations0.00.20.40.60.81.0L2 norm of gradients1e3GA-CNTKCNNUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSina Alemohammad, Zichao Wang, Randall Balestriero, and Richard G. Baraniuk. The recurrent\n\nneural tangent kernel. In Proc. of ICLR, 2021.\n\nCem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. In Proc.\n\nof ICML, 2019.\n\nMartin Arjovsky and Léon Bottou. Towards principled methods for training generative adversarial\n\nnetworks. In Proc. of ICLR, 2017.\n\nMartin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks.\n\nIn Proc. of ICML, 2017.\n\nSanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On\n\nexact computation with an infinitely wide neural net. In Proc. of NeurIPS, 2019.\n\nSanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu. Harnessing the power of infinitely wide deep nets on small-data tasks. In Proc. of ICLR, 2020.\n\nDuhyeon Bang and Hyunjung Shim. Mggan: Solving mode collapse using manifold-guided training.\n\nIn Proc. of CVPR, 2021.\n\nPaul Bergmann, Sindy Löwe, Michael Fauser, David Sattlegger, and Carsten Steger. Improving unsupervised defect segmentation by applying structural similarity to autoencoders. In VISIGRAPP, 2019.\n\nAlberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels.\n\nIn Proc. of\n\nNeurIPS, 2019.\n\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\n\nimage synthesis. arXiv preprint arXiv:1809.11096, 2018.\n\nTong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative\n\nadversarial networks. In Proc. of ICLR, 2017.\n\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Proc. of NeurIPS, 2016.\n\nLenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.\n\nIn Proc. of NeurIPS, 2019.\n\nConstantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with\n\noptimism. In Proc. of ICLR, 2018.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\n\nhierarchical image database. In Proc. of CVPR, 2009.\n\nIshan P. Durugkar, Ian Gemp, and Sridhar Mahadevan. Generative multi-adversarial networks. In\n\nProc. of ICLR, 2017.\n\nFarzan Farnia and Asuman E. Ozdaglar. Do gans always have nash equilibria? In Proc. of ICML,\n\n2020.\n\nJean-Yves Franceschi, Emmanuel de Bézenac, Ibrahim Ayed, Mickaël Chen, Sylvain Lamprier, and Patrick Gallinari. A neural tangent kernel perspective of gans. CoRR, abs/2106.05566, 2021.\n\nAdrià Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-\n\nworks as shallow gaussian processes. In Proc. of ICLR, 2019.\n\nAmnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen. On\n\nthe similarity between the laplace and neural tangent kernels. In Proc. of NeurIPS, 2020.\n\nArnab Ghosh, Viveka Kulharia, Vinay P. Namboodiri, Philip H. S. Torr, and Puneet Kumar Dokania.\n\nMulti-agent diverse generative adversarial networks. In Proc. of CVPR, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nIan Goodfellow.\n\nNips 2016 tutorial: Generative adversarial networks.\n\narXiv preprint\n\narXiv:1701.00160, 2016.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proc. of NeurIPS, 2014.\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. In Proc. of ICLR, 2015.\n\nRobert M Gower.\n\nConvergence theorems for gradient descent, May 2022.\n\nhttps:\n\n//gowerrobert.github.io/pdf/M2_statistique_optimisation/grad_ conv.pdf.\n\nArthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander J.\n\nSmola. A kernel two-sample test. J. Mach. Learn. Res., 2012.\n\nIshaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Im-\n\nproved training of wasserstein gans. In Proc. of NeurIPS, 2017.\n\nInsu Han, Haim Avron, Neta Shoham, Chaewon Kim, and Jinwoo Shin. Random features for the\n\nneural tangent kernel. CoRR, abs/2104.01351, 2021.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Proc. of NeurIPS, 2017.\n\nJiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: NNGP and\n\nNTK for deep attention networks. In Proc. of ICML, 2020.\n\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander\n\nMadry. Adversarial examples are not bugs, they are features. In Proc. of NeurIPS, 2019.\n\nArthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-\n\neralization in neural networks. In Proc. of NeurIPS, 2018.\n\nHarold Jeffreys. An invariant form for the prior probability in estimation problems. Proc. of the Royal Society of London. Series A. Mathematical and Physical Sciences, 186(1007):453–461, 1946.\n\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-\n\ning and improving the image quality of stylegan. In Proc. of CVPR, 2020.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n\nYann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].\n\nAvailable: http://yann.lecun.com/exdb/mnist, 2, 2010.\n\nChristian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew P. Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photorealistic single image super-resolution using a generative adversarial network. In Proc. of CVPR, 2017.\n\nJaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha\n\nSohl-Dickstein. Deep neural networks as gaussian processes. In Proc. of ICLR, 2018.\n\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In Proc. of NeurIPS, 2019.\n\nJaehoon Lee, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In Proc. of NeurIPS, 2020.\n\nChun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabás Póczos. MMD GAN:\n\ntowards deeper understanding of moment matching network. In Proc. of NeurIPS, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nWei Li, Li Fan, Zhenyu Wang, Chao Ma, and Xiaohui Cui. Tackling mode collapse in multi-\n\ngenerator gans with orthogonal vectors. Pattern Recognition, 2021.\n\nYujia Li, Kevin Swersky, and Richard S. Zemel. Generative moment matching networks. In Proc.\n\nof ICML, 2015.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\n\nIn Proc. of ICCV, December 2015.\n\nMario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans\n\ncreated equal? A large-scale study. In Proc. of NeurIPS, 2018.\n\nQi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang. Mode seeking genera-\n\ntive adversarial networks for diverse image synthesis. In Proc. of CVPR, 2019.\n\nXudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang, and Stephen Paul Smolley.\n\nLeast squares generative adversarial networks. In Proc. of ICCV, 2017.\n\nAlexander G de G Matthews, Jiri Hron, Mark Rowland, Richard E Turner, and Zoubin Ghahramani.\n\nGaussian process behaviour in wide deep neural networks. In Proc. of ICLR, 2018.\n\nLars M. Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do\n\nactually converge? In Proc. of ICML, 2018.\n\nLuke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial\n\nnetworks. In Proc. of ICLR, 2017.\n\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization\n\nfor generative adversarial networks. In Proc. of ICLR, 2018.\n\nAryan Mokhtari, Asuman E. Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In Proc. of AISTATS, 2020.\n\nVaishnavh Nagarajan and J. Zico Kolter. Gradient descent GAN optimization is locally stable. In\n\nProc. of NeurIPS, 2017.\n\nRoman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A Alemi, Jascha Sohl-Dickstein, and Samuel S Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In Proc. of ICLR, 2019a.\n\nRoman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. In Proc. of ICLR, 2019b.\n\nBen Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Proc. of NeurIPS, 2016.\n\nGuo-Jun Qi. Loss-sensitive generative adversarial networks on lipschitz densities. Int. J. Comput.\n\nVis., 2020.\n\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\n\nconvolutional generative adversarial networks. In Proc. of ICLR, 2016.\n\nMaithra Raghu, Ben Poole, Jon M. Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the\n\nexpressive power of deep neural networks. In Proc. of ICML, 2017.\n\nAlfréd Rényi et al. On measures of entropy and information. In Proc. of the 4th Berkeley symposium\n\non mathematical statistics and probability, volume 1, 1961.\n\nMehdi S. M. Sajjadi, Giambattista Parascandolo, Arash Mehrjou, and Bernhard Schölkopf. Tem-\n\npered adversarial networks. In Proc. of ICML, 2018.\n\nTim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\n\nImproved techniques for training gans. In Proc. of NeurIPS, 2016.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nSamuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information\n\npropagation. In Proc. of ICLR, 2017.\n\nVaishaal Shankar, Alex Fang, Wenshuo Guo, Sara Fridovich-Keil, Jonathan Ragan-Kelley, Ludwig\n\nSchmidt, and Benjamin Recht. Neural kernels without tangents. In Proc. of ICML, 2020.\n\nAkash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, and Charles Sutton. VEEGAN: reducing mode collapse in gans using implicit variational learning. In Proc. of NeurIPS, 2017.\n\nKiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient algo-\n\nrithms for smooth minimax optimization. In Proc. of NeurIPS, 2019.\n\nCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.\n\nIn Proc. of NeurIPS, 2016.\n\nTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8798–8807, 2018.\n\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:\n\nfrom error visibility to structural similarity. IEEE transactions on image processing, 2004.\n\nLi Xu, Jimmy S. J. Ren, Ce Liu, and Jiaya Jia. Deep convolutional neural network for image\n\ndeconvolution. In Proc. of NeurIPS, 2014.\n\nGreg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. CoRR, abs/1902.04760, 2019a.\n\nGreg Yang. Tensor programs I: wide feedforward or recurrent neural networks of any architecture\n\nare gaussian processes. CoRR, abs/1910.12478, 2019b.\n\nAmir Zandieh, Insu Han, Haim Avron, Neta Shoham, Chaewon Kim, and Jinwoo Shin. Scaling\n\nneural tangent kernels via sketching and random features. In Proc. of NeurIPS, 2021.\n\n6 STATISTICAL INTERPRETATION OF GA-NTK\n\nStatistically, minimizing Eq. (4) or (6) amounts to minimizing the Pearson χ2-divergence (Jeffreys, 1946), a case of f -divergence (Rényi et al., 1961), between Pdata + Pgen and 2Pgen, where Pdata is the distribution of real data and Pgen is the distribution of generated points. To see this, we first rewrite the loss of our discrimonator D, denoted by L(D), in expectation:\n\narg min\n\nD\n\nL(D) = arg min\n\nD\n\nEx∼Pdata\n\n(cid:2)(D(x) − 1)2(cid:3) + Ex∼Pgen\n\n(cid:2)(D(x) − 0)2(cid:3).\n\n(8)\n\nHere, Pgen can represent either Z in Eq. (4) or the output of the generator G in Eq. (6). Similarly, the loss function for our Pgen, denoted by L(Pgen; D), can be written as follows:\n\narg min Pgen\n\nL(Pgen; D) = arg min Pgen\n\nEx∼Pdata\n\n(cid:2)(D(x) − 1)2(cid:3) + Ex∼Pgen\n\n(cid:2)(D(x) − 1)2(cid:3).\n\n(9)\n\nGA-NTK, in the form of Eqs. (8) and (9), is a special case of LSGAN (Mao et al., 2017). Let D∗ be the minimizer of Eq. (8). We can see that Eqs. (4) and (6) effectively solve the problem:\n\narg min Pgen\n\nL(Pgen; D∗) = arg min Pgen\n\nEx∼Pdata\n\n(cid:2)(D∗(x) − 1)2(cid:3) + Ex∼Pgen\n\n(cid:2)(D∗(x) − 1)2(cid:3).\n\n(10)\n\nMao et al. (2017) show that, under mild relaxation, minimizing Eq. (10) yields minimizing the Pearson χ2-divergence between Pdata + Pgen and 2Pgen:\n\narg min Pgen\n\nL(Pgen; D∗) = arg min Pgen\n\nχ2\n\nPearson(Pdata + Pgen(cid:107)2Pgen)\n\n(cid:90)\n\n= arg min Pgen\n\n(Pdata(x) + Pgen(x))\n\n(cid:18)\n\n2Pgen(x) Pdata(x) + Pgen(x)\n\n(cid:19)2\n\n− 1\n\ndx.\n\nThe loss becomes zero when Pdata(x) = Pgen(x) for all x. Therefore, minimizing Eq. (4) or (6) brings Pgen closer to Pdata.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n7 PROOF OF THEOREM 3.1\n\nIn this section, we prove the convergence of a GA-NTK whose discriminator D approximates an infinite ensemble of infinitely-wide, fully-connected, feedforward neural networks. The proof can be easily extended to other network architectures such as convolutional neural networks.\n\n7.1 BACKGROUND AND NOTATION\n\nConsider a fully-connected, feedforward neural network f : Rd → R,\n\nf (x; θ) =\n\nσw√\n\ndL−1\n\n(cid:18) σw√\n\nwLφ\n\nW L−1φ\n\n(cid:18)\n\n· · · φ\n\n(cid:18) σw√\n\nW 1x + σbb1\n\n(cid:19)\n\n(cid:19)\n\n· · ·\n\n+ σbbL−1\n\n(cid:19)\n\n+ σbbL,\n\ndL−2\n\nd\n\n, bl ∈ Rdl\n\n(11) where φ(·) is the activation function (applied element-wisely), L is the number of hidden layers, {d1, · · · , dL−1} are the dimensions (widths) of hidden layers, θ = ∪L l=1(W l ∈ Rdl×dl−1 ) are trainable weights and biases whose initial values are i.i.d. Gaussian random variables N (0, 1), and σ2 b are scaling factors that control the variances of weights and biases, respectively. Suppose f is trained on a labeled dataset D2n = (X n ⊕ Zn ∈ R2n×d, 1n ⊕ 0n ∈ R2n) by minimizing the MSE loss using t gradient-descent iterations with the learning rate η. Let θ(0) and θ(t) be the initial and trained parameters, respectively. As d1, · · · , dL → ∞, we can approximate the distribution of f (x; θ(t)) as a Gaussian process (NTK-GP) (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019) whose behavior is controlled by a kernel matrix\n\nl=1θl = ∪L\n\nw and σ2\n\nK2n,2n = ∇θf (X n ⊕ Zn; θ(0))(cid:62)∇θf (X n ⊕ Zn; θ(0)) ∈ R2n×2n,\n\n(12)\n\ni,j\n\nwhere f (X n ⊕ Zn; θ(0)) ∈ R2n is the vector of in-sample predictions made by the initial f . The value of each element K 2n,2n = kL((X n ⊕ Zn)i,:, (X n ⊕ Zn)j,:) presents the similarity score of two rows (points) of X n ⊕ Zn in a kernel space, and it can be expressed by a kernel function kL : Rd × Rd → R, called the neural tangent kernel (NTK). The NTK is deterministic as it depends only on φ(·), σw, σb, and L rather than the specific values in θ(0). Furthermore, it can be evaluated layer-wisely. Let hl be the pre-activation of the j-th neuron at the l-th layer of f (x; θ(t)). j(x) is still an NTK-GP, and its associated NTK is defined as kl : Rd×Rd → R, The distribution of hl\n\nj(x) ∈ Rdl\n\nkl(x, x(cid:48)) = ∇θ≤lhl\n\nj(x)(cid:62)∇θ≤l hl\n\nj(x(cid:48)),\n\nwhere θ≤l = ∪l shown that\n\ni=1θi. Note that all hl\n\nj(x)’s, ∀j, are i.i.d. and thus share the same kernel. It can be\n\nkl(x, x(cid:48)) = ∇θl hl\n\nj(x)(cid:62)∇θl hl\n\nj(x(cid:48)) + ∇θ≤l−1 hl\n\nj(x)(cid:62)∇θ≤l−1hl\n\nj(x(cid:48))\n\n= ̃kl(x, x(cid:48)) + σ2\n\nwkl−1(x, x(cid:48))E\n\n(h(l−1)\n\nj\n\n(x), h(l−1)\n\nj\n\n(x(cid:48)))∼N (02, ̃Kl−1)\n\nand\n\nk1(x, x(cid:48)) =\n\nσ2 w\nd\n\nx(cid:62)x(cid:48) + σ2\n\nb\n\n(cid:104)\n\nφ(cid:48)(h(l−1)\n\nj\n\n(cid:105)\n\n(x(cid:48)))\n\n(x))φ(cid:48)(h(l−1)\n\nj (13)\n\n(14)\n\nwhere ̃kl : Rd × Rd → R is the NNGP kernel (Lee et al., 2018; Matthews et al., 2018) that controls the behavior of another Gaussian process, called NNGP, approximating the distribution of f (x; θ(0)), and\n\n ̃Kl−1 =\n\n7.2 CONVERGENCE\n\n(cid:20) ̃kl−1(x, x) ̃kl−1(x, x(cid:48))\n\n(cid:21)\n\n ̃kl−1(x, x(cid:48)) ̃kl−1(x(cid:48), x(cid:48))\n\n∈ R2×2.\n\nThe GA-NTK employs the above NTK-GP as the discriminator D. So, the in-sample mean predictions of D can be written as a closed-form formula:\n\nD(X n, Zn) = (I 2n − e−ηtK2n,2n\n\n)y2n ∈ R2n,\n\n(15)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nwhere I 2n is an identity matrix and y2n = 1n ⊕ 0n ∈ R2n is the “correct” label vector for training D. We formulate the objective of GA-NTK as:\n\narg min Zn\n\nL(Zn) = arg min Zn\n\n1 2\n\n(cid:107)12n − D(X n, Zn)(cid:107)2,\n\n(16)\n\nwhere 12n ∈ R2n in the loss L(·) is the “wrong” label vector that guides us to find the points (Zn) that best deceive the discriminator. We show that\n\nTheorem 7.1 Let s be the number of the gradient descent iterations solving Eq. (16), and let Zn,(s) i,j and Zn,(0) be the solution at the s-th iteration. Suppose the following values are bounded: (a) X n ,\n∀i, j, (b) t and η, and (c) σ and L. Also, assume that (d) X n contains finite, non-identical, normalized rows. Then, for a sufficiently large t, we have\n\ni,j\n\n(cid:107)∇Zn L(Zn,(j))(cid:107)2 ≤ O(\n\nmin j≤s\n\n1 s − 1\n\n).\n\n7.3 PROOF\n\nTo prove Theorem 7.1, we first introduce the notion of β smoothness:\n\nDefinition 7.1 A continuously differentiable function g : Rd → R is β-smooth if there exits β ∈ R such that\n\n(cid:107)∇ag(a) − ∇bg(b)(cid:107) ≤ β(cid:107)a − b(cid:107)\n\nfor any a, b ∈ Rd.\n\nIt can be shown that gradient descent finds a stationary point of a β-smooth function efficiently (Gower, 2022).\n\nLemma 7.1 Let a(s) be the input of a function g : Rd → R after applying s gradient descent iterations to an initial input a(0). If g is β-smooth, then g(a(s)) converges to a stationary point at rate\n\n(cid:107)∇ag(a(j))(cid:107)2 ≤ O(\n\nmin j≤s\n\n1 s − 1\n\n).\n\nSo, our goal is to show that the loss L(Zn) in Eq. (16) is β-smooth w.r.t. any generated point z ∈ Rd.\n\nCorollary 7.1 If all the conditions (a)-(d) in Theorem 7.1 hold, there exits a constant c1 ∈ R+ such that (cid:107)∇zL(Zn)(cid:107) ≤ c1 for each row z ∈ Rd of Zn. This makes L(Zn) β-smooth.\n\nTo prove Corollary 7.1, consider Di(X n, Zn) and ∇zj L(Zn), the i-th and j-th elements of D(X n, Zn) ∈ R2n and ∇zL(Zn) ∈ Rd, respectively. We have\n\n∇zj L(Zn) = ∇zj = (cid:80)2n\n\n1\n\n2 (cid:107)12n − D(X n, Zn)(cid:107)2\n\ni=1 (Di(X n, Zn) − 1) · ∇zj Di(X n, Zn)\n\n(17)\n\nGiven a sufficiently large t, the Di(X n, Zn) can be arbitrarily close to yi ∈ {0, 1} because K2n,2n is positive definite (Jacot et al., 2018) and therefore (I 2n − e−ηtK2n,2n ) → I 2n as t → ∞ in Eq. (15). There exists (cid:15) ∈ R+ such that\n\n|∇zj L(Zn)| ≤ (cid:15) (cid:80)n\n\n≤ (1 + (cid:15)) (cid:80)2n = (1 + (cid:15)) (cid:80)2n = (1 + (cid:15))ηt (cid:80)2n\n\ni=1 |∇zj Di(X n, Zn) + (1 + (cid:15)) (cid:80)2n i=1 |∇zj Di(X n, Zn)| (cid:80)2n p=1(I 2n i=1 |∇zj i,p,q=1 e−ηtK2n,2n\n\ni,p − e−ηtK2n,2n\n\ni,p\n\ni,q\n\n)y2n p |\n\ni=n+1 |∇zj Di(X n, Zn)|\n\n|∇zj kL((X n ⊕ Zn)q,:, (X n ⊕ Zn)p,:)y2n\n\np |.\n\nNote that e−ηtK2n,2n 7.1 holds as long as ∇zj kL((X n ⊕ Zn)q,:, (X n ⊕ Zn)p,:) is bounded.\n\n∈ R+ can be arbitrarily close to 0 with a sufficiently large t. Hence, Corollary\n\ni,q\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nCorollary 7.2 If the conditions (a)-(d) in Theorem 7.1 hold, there exits a constant c2 ∈ R+ such that ∇zj kL(a, b) ≤ c2 for any two rows a and b of X n ⊕ Zn.\n\nIt is clear that ∇zj kL(a, b) = 0 if a, b (cid:54)= z. So, without loss of generality, we consider ∇zj kL(a, z) only. From Eq. (13), we have\n\n∂kL(a, z) ∂zj\n\n=\n\n∂kL(a, z) ∂kL−1(a, z)\n\n∂kL−1(a, z) ∂kL−2(a, z)\n\n· · ·\n\n∂k1(a, z) ∂zj\n\n.\n\nFor each l = 2, · · · , L, we can bound ∂kl(a, z)/∂kl−1(a, z) by\n\n∂kl(a,z)\n\n∂kl−1(a,z) = σ2\n\nE\n\nw\n\n(h(l−1) ≤ (σw maxh φ(cid:48)(h))2\n\n(x), h(l−1)\n\nj\n\nj\n\n(x(cid:48)))∼N (02, ̃Kl−1)\n\n(cid:104)\n\nφ(cid:48)(h(l−1)\n\nj\n\n(x))φ(cid:48)(h(l−1)\n\nj\n\n(x(cid:48)))\n\n(cid:105)\n\nprovided that the maximum slope of φ is limited, which is true for many popular activation functions including ReLU and erf. Also, by Eq. (14), the value\n\n∂k1(a, z) ∂zj\n\n=\n\nσ2 w\nd\n\naj\n\nis bounded. Therefore, Corollary 7.2 holds, which in turn makes L(Zn) β-smooth via Corollary 7.1. By Lemma 7.1, we obtain the proof of Theorem 7.1.\n\n8 EXPERIMENT SETTINGS\n\nThis section provides more details about the settings of our experiments.\n\n8.1 MODEL SETTINGS\n\nThe network architectures of the baseline GANs used in our experiments are based on InfoGAN (Chen et al., 2016). We set the latent dimensions, training iterations, and batch size according to the study (Lucic et al., 2018). The latent dimensions for the generator are all 64. The batch size for all baselines is set to 64. The training iterations are 80K, 100K, and 400K for MNIST, CelebA, and CIFAR-10 datasets, respectively. For the optimizers, we follow the setting from the respective original papers. Below we list the network architecture of the baselines for each dataset as well as the optimizer settings.\n\nTable 3: The architectures of the discriminator and generator in the baseline GANs for the MNIST dataset.\n\nDiscriminator\n\nInput 28×28×1 Gray image\n\nGenerator\n\nInput∈ R64 ∼ N (0, I)\n\n4×4 conv; 64 leaky ReLU; stride 2\n\nFully Connected 1024 ReLU; batchnorm\n\n4×4 conv; 128 leaky ReLU; stride 2. batchnorm Fully Connected 7 × 7 × 128 ReLU; batchnorm\n\nFully Connected 1024 leaky ReLU; batchnorm\n\n4×4 deconv; 64 ReLU. stride 2; batchnorm\n\nFully Connected 1 output\n\n4×4 deconv; 1 sigmoid\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: The architectures of the discriminator and generator in the baseline GANs for the CIFAR-10 dataset.\n\ndiscriminator\n\nInput 32×32×3 Image\n\ngenerator\n\nInput∈ R64 ∼ N (0, I)\n\n4×4 conv; 64 leaky ReLU; stride 2\n\nFully Connected 2 × 2 × 448 ReLU; batchnorm\n\n4×4 conv; 128 leaky ReLU; stride 2; batchnorm 4×4 deconv; 256 ReLU; stride 2; batchnorm\n\n4×4 conv; 256 leaky ReLU; stride 2; batchnorm 4×4 deconv; 128 ReLU; stride 2\n\nFully Connected 1 output\n\n4×4 deconv; 64 ReLU; stride 2\n\n4×4 deconv; 3 Tanh; stride 2.\n\nTable 5: The architectures of the discriminator and generator in the baseline GANs for the CelebA dataset.\n\ndiscriminator\n\nInput 64×64×3 Image\n\ngenerator\n\nInput∈ R64 ∼ N (0, I)\n\n4×4 conv; 64 leaky ReLU; stride 2\n\nFully Connected 2 × 2 × 448 ReLU; batchnorm\n\n4×4 conv; 128 leaky ReLU; stride 2; batchnorm 4×4 deconv; 256 ReLU; stride 2; batchnorm\n\n4×4 conv; 256 leaky ReLU; stride 2; batchnorm 4×4 deconv; 128 ReLU; stride 2\n\n4×4 conv; 256 leaky ReLU; stride 2; batchnorm 4×4 deconv; 64 ReLU; stride 2\n\nFully Connected 1 output\n\n4×4 deconv; 32 ReLU; stride 2\n\n4×4 deconv; 3 Tanh; stride 2.\n\nTable 6: The optimizer settings for each GAN baseline. ndis denotes the training steps for discriminators in the alternative training process.\n\nDCGAN\n\nLSGAN\n\nWGAN\n\nOptimizer type Learning Rate\n\nAdam\n\nAdam\n\n0.0002\n\n0.0002\n\nβ1\n\n0.5\n\n0.5\n\nβ2\n\nndis\n\n0.999\n\n0.999\n\nRMSProp\n\n0.00005\n\nNone None\n\nWGAN-GP\n\nSN-GAN\n\nAdam\n\nAdam\n\n0.0001\n\n0.0001\n\n0.5\n\n0.9\n\n0.9\n\n0.999\n\n1\n\n1\n\n5\n\n5\n\n5\n\nNote that we remove all the batchnorm layers for the discriminators in WGAN-GP. We architect the element network of the discriminator in our GA-NTK following InfoGAN (Chen et al., 2016), except that the width (or the number of filters) of the network is infinite at each layer and has no batchnorm layers.\n\nThe generator of GA-NCTKg consumes memory. To reduce memory consumption, we let D discriminates true and fake images in the code space of a pre-trained autoencoder A (Bergmann et al., 2019). After training, a code output by G is fed into the decoder of A to obtain an image. The architectures of the pre-trained A for different datasets are summarized as follows:\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: The architectures of A for different datasets.\n\nMNIST\n\nCIFAR-10\n\nInput 28×28×1 Image\n\n3×3 conv; 16 SeLU; stride 2\n\n3×3 conv; 32 SeLU; stride 2\n\n3×3 conv; 64 SeLU; stride 2\n\nFully Connected; 128 tanh\n\nInput 32×32×3 Image\n\n3×3 conv; 32 SeLU; stride 2\n\n3×3 conv; 64 SeLU; stride 2\n\n3×3 conv; 128 SeLU; stride 2\n\nFully Connected; 1024 tanh\n\n3×3 transposeconv; 64 SeLU; stride 2\n\n3×3 transposeconv; 128 SeLU; stride 2\n\n3×3 transposeconv; 32 SeLU; stride 2\n\n3×3 transposeconv; 64 SeLU; stride 2\n\n3×3 transposeconv; 16 SeLU; stride 2\n\n3×3 transposeconv; 32 SeLU; stride 2\n\noutput\n\nCelebA\n\noutput\n\nCelebA-HQ\n\nInput 64×64×3 Image\n\nInput 256×256×3 Image\n\n3×(3×3 conv; 32 SeLU; stride 1)\n\n3×(3×3 conv; 64 SeLU; stride 1)\n\n3×3 conv; 32 SeLU; stride 2\n\n3×3 conv; 64 SeLU; stride 2\n\n3×(3×3 conv; 64 SeLU; stride 1)\n\n3×(3×3 conv; 128 SeLU; stride 1)\n\n3×3 conv; 64 SeLU; stride 2\n\n3×3 conv; 128 SeLU; stride 2\n\n3×(3×3 conv; 128 SeLU; stride 1)\n\n3×(3×3 conv; 256 SeLU; stride 1)\n\n3×3 conv; 128 SeLU; stride 2\n\n3×3 conv; 256 SeLU; stride 2\n\nFully Connected; 2048 tanh\n\n3×(3×3 conv; 512 SeLU; stride 1)\n\n3×(3×3 transposeconv; 128 SeLU; stride 1)\n\n3×3 conv; 512 SeLU; stride 2\n\n3×3 transposeconv; 128 SeLU; stride 2\n\nFully Connected; 2048 tanh\n\n3×(3×3 transposeconv; 64 SeLU; stride 1)\n\n3×(3×3 transposeconv; 512 SeLU; stride 1)\n\n3×3 transposeconv; 64 SeLU; stride 2\n\n3×3 transposeconv; 512 SeLU; stride 2\n\n3×(3×3 transposeconv; 32 SeLU; stride 1)\n\n3×(3×3 transposeconv; 256 SeLU; stride 1)\n\n3×3 transposeconv; 32 SeLU; stride 2\n\n3×3 transposeconv; 256 SeLU; stride 2\n\n3×(3×3 transposeconv; 128 SeLU; stride 1)\n\n3×3 transposeconv; 128 SeLU; stride 2\n\n3×3(3×3 transposeconv; 64 SeLU; stride 1)\n\n3×3 transposeconv; 64 SeLU; stride 2\n\noutput\n\noutput\n\n8.2 METRICS\n\nThe FID scores are computed using the code from the original paper (Heusel et al., 2017). We sample 2048 images to compute the FID scores. We calculate the AM-SSIM scores using the SSIM settings: filter size 4, filter sigma 1.5, k1 0.01, and k2 0.03 (Wang et al., 2004).\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Unidirectional search for the hyperparameter λ of GA-NTK. Input: Data X n, kernel k, and separation tolerance (cid:15) Output: λ for GA-NTK Randomly initiate Zn ∈ Rn×d λ ← 1 while 1\n\n2n (cid:107)D(X n, Zn; k, λ) − (1n ⊕ 0n)(cid:107)2 ≤ (cid:15) do\n\nλ ← λ · 2\n\nend return λ\n\n8.3 HYPERPARAMETER TUNING\n\nFor each data synthesis method, we tune its hyperparameter using grid search. GA-NTK. The computation of K2n,2n requires one to determine the initialization and architecture of the element networks in the ensemble discriminator. Poole et al. (2016); Schoenholz et al. (2017); Raghu et al. (2017) have proposed a principled method to tune the hyperparameters for the initialization. From our empirical results, we also find that the quality of the images generated by GA-NTK is not significantly impacted by the choice of the architecture—a fully connected network with rectified linear unit (ReLU) activation suffices to generate recognizable image patterns. Once K2n,2n is decided, there is only one hyperparameter λ = ηt to tune in Eq. (16). The λ controls how well the discriminator is trained on D, so either a too small or large value can lead to poor gradients for Zn and final generated points. But since there is no alternating updates as in GANs, we can decide an appropriate value of λ without worrying about canceling the learning progress of Zn. We propose a simple, unidirectional search algorithm for tuning λ, as shown in Algorithm 1. Basically, we search, from small to large, for a value that makes the discriminator nearly separate the real data from pure noises in an auxiliary learning task, and then use this value to solve Eq. (16). In practice, a small positive (cid:15) ranging from 10−3 to 10−2 suffices to give an appropriate λ. Multi-resolutional GA-NTK. We use 3 NTK-GP’s as the discriminators, whose architectures are listed in Table 8.\n\nTable 8: The architectures of the discriminators for multi-resolution GA-NTK.\n\nDiscriminator large\n\nDiscriminator medium\n\nInput 256×256×3 Image\n\nDiscriminator small\n\nInput 64×64×3 Image\n\n4×4 conv; ReLU; stride 2\n\nInput 16×16×3 Image\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\nFully Connected 1 output\n\n4×4 conv; ReLU; stride 2\n\n4×4 conv; ReLU; stride 2\n\nFully Connected 1 output\n\n4×4 conv; ReLU; stride 2\n\nFully Connected 1 output\n\n9 MORE EXPERIMENTS\n\n9.1 GA-FNTK VS. GA-CNTK\n\nNext, we compare the images generated by GA-FNTK, GA-CNTK, and the multi-resolutional GACNTK described in Section 3.2 on the CelebA and CelebA-HQ datasets. The multi-resolutional GA-CNTK employs 3 discriminators working at 256 × 256, 64×64, and 16×16 pixel resolutions, respectively. Figure 6 shows the results. To our surprise, GA-NTK (which models the discriminator as an ensemble of fully connected networks) suffices to generate recognizable faces. The images synthesized by GA-FNTK and GA-CNTK lack details and global coherence, respectively, due to\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\n(a) GA-FNTK on CelebA\n\n(b) GA-CNTK on CelebA\n\nFigure 6: The images generated by (a) GA-FNTK and (b) GA-CNTK given 256 CelebA training images.\n\nFigure 7: When b = 1, GA-NTK tends to generate a blurry mean image.\n\nthe characteristics of FNNs and CNNs. On the other hand, the multi-resolutional GA-CNTK gives both the details and global coherence thanks to the multiple discriminators working at different pixel resolutions. The results also demonstrate the potential of GA-NTK variants to generate high-quality data as there are many other techniques for GANs that could be adapted into GA-NTK.\n\n9.2 BATCH-WISE GA-NTK\n\nTo work with a larger training set, we modify GA-CNTK by following the instructions in Section 3.2 to obtain the batch-wise GA-CNTK, which computes the gradients of Zn in Eq. (4) from 256 randomly sampled training images during each gradient descent iteration. We train the batch-wise GA-CNTK on two larger datasets consisting of 2048 images from CelebA and 1300 images from ImageNet, respectively. Figure 8 shows the results, and the batch-wise GA-CNTK can successfully generate the “daisy” images on ImageNet.\n\nNote that the batch-wise GA-CNTK solves a different problem than the original GA-CNTK—the former finds Zn that deceives multiple discriminators, each trained on 256 examples, while the latter searches for Zn that fools a single discriminator trained on 256 examples. We found that, when the batch size is small (b = 1), GA-NTK tends to generate a blurry mean image regardless of model architectures and initializations of model weights and Zn, as shown in Figure 7. This is because the mean image is the best for simultaneously fooling many NTK discriminators, each trained on a single example. However, in practice this setting is less common as one usually aims to use the largest b possible (Brock et al., 2018). Figure 8 shows that a batch size of 256 suffices to give plausible results on the CelebA and ImageNet datasets. Comparing the images in Figure 1(f) with those in Figure 8(a), we can see that the batch-wise GA-CNTK gives a little more blurry images but the patterns in each synthesized image are more globally coherent, both due to the effect of multiple discriminators.\n\n9.3 SENSITIVITY TO HYPERPARAMETERS\n\nHere, we study how sensitive is the performance of WGAN, WGAN-GP, and GA-FNTK to their hyperparameters. We adjust the hyperparameters of different approaches using the grid search under a time budget of 3 hours, and then evaluate the quality of 2048 generated data points by the Wasser-\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 8: The images generated by batch-wise GA-CNTK on (a) CelebA dataset of 2048 randomly sampled images and (b) ImageNet dataset of 1300 randomly sampled images.\n\n(a)\n\n(b)\n\nFigure 9: The distribution of Wasserstein distance between Pgen and Pdata (used to measure the quality of the generated points) over the searched hyper-parameters on training sets of (a) 8- and (b) 25-modal Gaussian mixtures.\n\nstein distance between Pgen and Pdata. We train different methods on two toy datasets consisting of 8- and 25-modal Gaussian mixtures following the settings described in Section 4.2. Figure 9 shows the results, and we can see that GA-FNTK achieves the lowest average Wasserstein distance in both cases. Moreover, its variances are smaller than the two other baselines, too. This shows that the performance of GA-FNTK is less sensitive to the hyperparameters and could be easier to tune in practice.\n\nNote that, with 3-hour time budget, the hyperparameters we obtained through the grid search are good enough for reproducing the experiments conducted by Mao et al. (2017) on mode collapse. In the experiments, the Pgen of different methods aim to align a 2D 8-modal Gaussian mixtures in the ground truth. Our results are shown in Figure 10.\n\n21\n\nGA-NTKWGANWGAN-GP103102101Wasserstein distanceGA-NTKWGANWGAN-GP102101Wasserstein distanceUnder review as a conference paper at ICLR 2023\n\n(a) MNIST\n\n(b) CIFAR-10\n\n(c) CelebA\n\nFigure 11: The learning curve of G in GA-CNTKg and the generated images G(z) at different stages of training given the same input z.\n\n(a) Ground truth\n\n(b) WGAN\n\n(c) WGAN-GP\n\n(d) GA-FNTK\n\nFigure 10: Visualization of distribution alignment and mode collapse on a 2D 8-modal Gaussian mixtures dataset.\n\n9.4 EVOLUTION OF IMAGES DURING TRAINING\n\nFigure 11 shows the learning curve of the generator in G in GA-CNTKg and the relationship between the quality of images output by G and the number of gradient descent iterations. The results show that the loss can be minimized even if it is an f -divergence, and a lower loss score implies higher image quality. This is consistent with the results of GA-CNTK (without a generator) shown in Figure 3.\n\nSource of creativity. The diversity of our generated data not only comes from the randomness of an optimization algorithm (e.g., initialization of Z or splitting of X into batches, as discussed in Section 3.2) but also from the objective in Eq. (4) itself. To see this, observe in Figure 3 that the images generated at the later stage of training contain recognizable patterns that change constantly over training time, despite little change in the loss score. The reason is that, in Eq. (4), the Zn is optimized for a moving target—any change of Zn causes D to be “retrained” instantly. The training of the generator G in Eq. (6) also shares this nice property. In Figure 11, the patterns of a generated image G(z) change over training time even when the input z is fixed. However, getting diverse artificial data through this property requires prolonged training time. In practice, we can simply initialize Z differently to achieve diversity faster.\n\n10 MORE IMAGES GENERATED BY GA-CNTK AND GA-CNTKG\n\nFigures 12–16 show more sample images synthesized by GA-CNTK and GA-CNTKg. All these images are obtained using the settings described in the main paper and the above.\n\nWe can see that the quality of the images synthesized by GA-CNTKg is worse than that of the images synthesized by GA-CNTK, as discussed in Section 4.1. Furthermore, recall from Table 1 that, without a generator network, the GA-NTK performs better when the date size increases. However, this is not the case for GA-NTKg having a generator network. We have resampled training data and rerun the experiments 5 times with different initial values of Zn but obtained similar results. Therefore, we believe the instability is due to the sample complexity of the generator network—256 examples or less are insufficient to train a stable, high-quality generator. This is evident in Figures 12(b)-15(b) where the generator outputs unrecognizable images more often.\n\n22\n\n0101010101010101Under review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 12: Sample images generated by GA-CNTK (a) without and (b) with generator on the MNIST dataset of 256 randomly sampled images.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 13: Sample images generated by GA-CNTK (a)without generator(b)with generator on the CIFAR-10 dataset of 256 randomly sampled images.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 14: Sample images generated by GA-CNTK (a) without and (b) with generator on the CelebA dataset of 256 randomly sampled images.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 15: Sample images generated by multi-resolutional GA-CNTK on the CelebA-HQ dataset of 256 randomly sampled images.\n\nFigure 16: Sample images generated by GA-CNTKg on the CelebA-HQ dataset of 256 randomly sampled images.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\n11 DOWNGRADE IMAGES\n\nAs discussed in the main paper, we find that, when the size of training set is small, an image synthesis method may produce downgrade images that look almost identical to some images in the training set. This problem is less studied in the literature but important to applications with limited training data. We investigate this problem by showing the images from the training set that are the nearest to a generated image. We use the SSIM (Wang et al., 2004) as the distance measure. Figures 17, 18, and 19 show the results for some randomly sampled synthesized images. As compared to GANs, both GA-CNTK and batch-wise GA-CNTK can generate images that look less similar to the ground-truth images.\n\nFigure 17: Comparison between the images generated by WGAN-GP trained on 256 images and the nearest neighbors (measured by SSIM) from the training set. Images with red bounding boxes are generated images.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 18: Comparison between the images generated by GA-CNTK trained on 256 images and the nearest neighbors (measured by SSIM) from the training set. Images with red bounding boxes are generated images.\n\nFigure 19: Comparison between the images generated by GA-CNTKg trained on 256 images and the nearest neighbors (measured by SSIM) from the training set. Images with red bounding boxes are generated images.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: The convergence speed and training time of different methods on a machine with a single NVIDIA Tesla V100 GPU given different datasets of 256 randomly sampled images. The GACNTK and GA-CNTKg are batch-wise, and the batch size b is set to 64 for all methods.\n\nMetric\n\nDCGAN\n\nLSGAN\n\nWGAN WGANGP\n\nSNGAN\n\nGACNTK GACNTKg\n\nT Iterations\n\nS\n\n7400\n\n5100\n\nI\n\nN M\n\nIter. / sec.\n\nSeconds\n\n1 -\n\n0 Iterations R\nA F\n\nIter. / sec.\n\nI\n\nC\n\nSeconds\n\nA Iterations\n\nb e\nl e\nC\n\nIter. / sec.\n\nSeconds\n\n20\n\n370\n\nN/A\n\n17\n\nN/A\n\nN/A\n\n13\n\nN/A\n\n19\n\n268\n\nN/A\n\n17\n\nN/A\n\nN/A\n\n12\n\nN/A\n\n7000\n\n19\n\n368\n\n3400\n\n18\n\n189\n\n14000\n\n11100\n\n16\n\n875\n\n15\n\n740\n\n18800\n\n11200\n\n12\n\n1566\n\n10\n\n1120\n\n12800\n\n18\n\n711\n\nN/A\n\n14\n\nN/A\n\nN/A\n\n9\n\nN/A\n\n500\n\n14\n\n35\n\n600\n\n13\n\n46\n\n1200\n\n6\n\n20\n\n1600\n\n9\n\n177\n\n6200\n\n8\n\n775\n\n5900\n\n5\n\n1180\n\n12 SEMANTICS LEARNED BY GA-CNTKG\n\nHere, we investigate whether the features learned by GA-NTK can encode high-level semantics. We plot “interpolated” images output by the generator G of GA-CNTKg taking equidistantly spaced z’s along a segment in z space as the input. For ease of presentation, we consider a 2-dimensional z space and train G on MNIST and CelebA datasets of 256 examples. Figure 20 shows the results, where the generated patterns transit smoothly across the 2D z space, and neighboring images share similar looks. These similar-looking images are generated from adjacent but meaningless z’s, suggesting that the learned features encode high-level semantics.\n\n13 CONVERGENCE SPEED AND TRAINING TIME\n\nIn this section, we study the time usage for training GA-NTK variants and compare it with the training of GANs. We conduct experiments to investigate the number of iterations and the wall-clock time required to train different methods on different datasets of 256 randomly sampled images. We use the batch-wise GA-CNTK and GA-CNTKg and set the batch size b to 64 for all methods. We run the experiments on a machine with a single NVIDIA Tesla V100 GPU. For DCGAN and LSGAN whose loss scores do not reflect image quality, we monitor the training process manually and stop it as long as the generated images contain recognizable patterns. But these methods do not seem to converge. For other methods, we use the early-stopping with the patience of 10000 steps and delta of 0.05 to determine convergence. The results are shown in Table 9. As we can see, the number of iterations required by either batch-wise GA-CNTK or GA-CNTKg is significantly smaller than that used by GANs. This justifies our claims in Section 1. However, the batch-wise GA-CNTK and GA-CNTKg run fewer iterations per second than GANs because of the higher computation cost involved in back-propagating through Kb,b. In terms of wall-clock time, the batch-wise GA-CNTK is the fastest while the GA-CNTKg runs as fast as WGAN-GP. We expect that, with the continuous optimization of the Neural Tangents library (Novak et al., 2019a) which our code is based on, the training speed of GA-NTK variants can be further improved.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 20: Interpolated images generated by GA-CNTKg, which is trained on (a) MNIST and (b) CelebA datasets of 256 randomly sampled examples from all classes. The G takes 2-dimensional z’s as input. For each dataset, we feed equidistantly spaced z’s along a segment in z space to G to get the interpolated images.\n\n30",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors propose generative adversarial neural tangent kernel (GA-NTK) and its variants.\nThey model the discriminator as a Gaussian process whose mean (and covariance) is governed by NTK.\nAlso, they can be performed via a single-level training process as a whole, because the training dynamics of their discriminators can be evaluated in closed form.\nThe authors claim that this property allows them to avoid the training difficulties often encountered with GANs, and experimentally confirmed this claim and their effectiveness for data synthesis.\n\n# Strength And Weaknesses\n\nI place special emphasis on comments with the mark *.\n\n---\n\nStrength:\n\n[1*] As the authors also wrote at the end of Section 1, the main significance of this study is not performance improvement but novelty of the research direction.\nThis study is an interesting attempt that uses an interesting idea to solve the problems of GANs, which are the motivation for the study.\n\n---\n\nWeakness:\n\n[2] There are writing misses.\n\n[2-1] You should use \\citep as well as \\citet.\nFor example, you can write \"theoretical interpretations Arjovsky & Bottou (2017).\" in p.3 as \"theoretical interpretations (Arjovsky & Bottou, 2017).\" by using \\citep.\nAlso, \"Franceschi et al. Franceschi et al. (2021)\" in p.3 is strange for me; you can write \"Franceschi et al. (2021)\" by using \\citet only.\nPlease look up the use of \\citep and \\citet and rewrite the relevant parts.\n\n[2-2] In l.12 in p.4, value each element -> value of each element.\nCheck your writing (including other parts) again.\n\n[2-3] In eq.(4), $\\arg\\min\\_{Z^n} L(Z^n)=\\arg\\min\\_{Z^n} ||\\cdot||_2$ -> $\\arg\\min\\_{Z^n} L(Z^n)$, where $L(Z^n)=||\\cdot||^2$.\nYou should define $L(Z^n)$ explicitly, and unify $||\\cdot||\\_2$ and $||\\cdot||$ into one if they are the seme (see also norm in other parts).\n\n[2-4] The $f$-divergence appears in places, but what function was used for $f$?\n\n[2-5] In Figure 3, title is partially obscured.\n\n[2-6] In Figure 5, what is \"real\" in the legend?\n\n[2-7] In the last equation in p.14, absolute value symbol is missing:\nfor example, $\\nabla_{z_j} L(Z^n)\\le$ -> $|\\nabla_{z_j} L(Z^n)|\\le$.\n\n[2-8] You should centralize Figure 14.\n\n[2-9] In Section 10, ?? -> 18.\n\n---\n\nQuestion:\n\nRegarding the following points, it may be that the authors' description is appropriate and there is no problem, just because I have not understood it correctly.\nI do not reflect these points in my current recommendation score.\nDepending on the authors' response, I may change my recommendation score.\n\n[3] Is it necessary to solve an optimization problem like eq.(4) every time to generate pseudo examples?\nIf so, that is a demerit compared with GANs, which can generate an infinite number of pseudo examples without additional training from a trained model.\nThis disadvantage should be written more clearly.\n(Even so, I will not lower the score.)\n\n[4*] From the experimental results (see especially Figure 3) and my understanding of the formulas, it seems to me that GA-NTK that is sufficiently trained with a large $t$ would simply return an training example (or mixture of training examples) as a pseudo example.\nIs this question correct?\nIf correct, such data synthesis would have low practical value.\nIt does not seem to me that the use of a generator model adequately solves such a problem.\nAlso, the authors should explain when (i.e., at what $t$) to stop learning.\n\n[5] This question relates to [4*].\nI am interested in the relationship between the batch size $b$ and the novelty of the pseudo example (dissimilarity to the training examples $X^{b/2}$).\nI suspect that if $b$ is small, each generated pseudo example will be fairly close to one of the training examples $X^{b/2}$.\nIs this question correct?\nIf correct, I think that the authors should mention this issue.\n\n[6*] It is difficult for me to understand the whole algorithm.\nI could not understand the relation between the index $t$ in $\\lambda=\\eta \\cdot t$ in eq.(3), the index $j$ in Theorem 3.1, \"Epochs\" in Figure 3, and \"Iterations\" in Figure 5, and the evolution of $Z^n$ and $Z^{n,(j)}$.\nPlease write a pseudo code (or modify the text) so that the relation of these objects become clear.\n\nAlso, how does $t$ change when $Z^n$ changes?\nI think this question is relevant to the validity of applying NTK theory.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity:\n\nI do not understand the points [3]--[6*].\nOther parts have a good clarity.\n\n---\n\nQuality:\n\nI think that the quality of the paper is not low, but I do not understand the points [3]--[6*].\nSo I will reserve a definite evaluation until the response.\n\n---\n\nNovelty:\n\nAs I wrote in [1*], I give a good evaluation regarding the novelty.\n\n---\n\nReproducibility:\n\nI took a glance of GitHub repository.\nI think that experimental part of this study is reproducible.\nHowever, I want you to cope with [6*], to improve the understandability (a basis of the reproducibility).\n\n# Summary Of The Review\n\nI give this study a positive score now, on the basis of the novelty and interest of the idea [1*].\nHowever, the description of the proposed method is insufficient and not clear [3]--[6*].\nThese points could be improved!\nI will change the score up or down, depending on the responses to [3]--[6*].\n\n---\n\nI changed the score from 6 to 8.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Details Of Ethics Concerns\n\nNA"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nEXPONENTIAL GENERALIZATION BOUNDS WITH NEAR-OPTIMAL RATES FOR Lq-STABLE ALGORITHMS\n\nXiao-Tong Yuan School of Intelligence Science and Technology Nanjing University, Suzhou, 215163, China xtyuan1980@gmail.com\n\nPing Li LinkedIn Ads 700 Bellevue Way NE, Bellevue, WA 98004, USA pinli@linkedin.com\n\nABSTRACT\n\nThe stability of learning algorithms to changes in the training sample has been actively studied as a powerful proxy for reasoning about generalization. Recently, exponential generalization and excess risk bounds with near-optimal rates have been obtained under the stringent and distribution-free notion of uniform stability (Bousquet et al., 2020; Klochkov & Zhivotovskiy, 2021). In the meanwhile, under the notion of Lq-stability, which is weaker and distribution dependent, exponential generalization bounds are also available yet so far only with sub-optimal rates. Therefore, a fundamental question we would like to address in this paper is whether it is possible to derive near-optimal exponential generalization bounds for Lq-stable learning algorithms. As the core contribution of the present work, we give an affirmative answer to this question by developing strict analogues of the near-optimal generalization and risk bounds of uniformly stable algorithms for Lq-stable algorithms. Further, we demonstrate the power of our improved Lqstability and generalization theory by applying it to derive strong sparse excess risk bounds, under mild conditions, for computationally tractable sparsity estimation algorithms such as Iterative Hard Thresholding (IHT).\n\n1\n\nINTRODUCTION\n\nA fundamental issue in statistical learning is to bound the generalization error of a learning algorithm for understanding its prediction performance on unseen data. It has long been recognized in literature that one of the key characteristics that permits learning algorithms to generalize is the stability of estimated model to perturbations in training data. The idea of using algorithmic stability as a proxy for generalization performance analysis dates back to the seventies (Rogers & Wagner, 1978; Devroye & Wagner, 1979). Since the seminal work of Bousquet & Elisseeff (2002), the search for generalization bounds under various notions of algorithmic stability has been a flourishing area of learning theory (Zhang, 2003; Mukherjee et al., 2006; Shalev-Shwartz et al., 2010; Kale et al., 2011; Hardt et al., 2016; Celisse & Guedj, 2016; Bousquet et al., 2020).\n\nAs one may expect, the stronger an algorithmic stability criterion is, the sharper the resulting generalization bound will be. On one end, exponential generalization bounds can be guaranteed by approaches under the most stringent notion of uniform stability (Bousquet & Elisseeff, 2002; Bousquet et al., 2020), which requires the change in the prediction loss to be uniformly small regardless data distribution. Despite the strength of generalization, the distribution-free nature makes uniform stability too restrictive to be fulfilled, e.g., by learning rules with unbounded losses (Celisse & Guedj, 2016). On the other end, based on some weaker and distribution dependent notions of stability such as hypothesis stability and mean-square stability, only polynomial generalization bounds seem possible in general cases, although the corresponding stability criteria are more amenable to verification (Bousquet & Elisseeff, 2002). These observations have prompted the development of Lq-stability, as an in-between state, to achieve the best of two worlds (Celisse & Guedj, 2016; Abou-Moustafa & Szepesv ́ari, 2019): it generalizes the notion of hypothesis stability from l1-norm criterion to Lq-norm criterion for q ≥ 2 but remains distribution dependent and thus is weaker than uniform stability; in the meanwhile it can still achieve similar exponential generalization bounds to those of uniformly stable algorithms (Bousquet & Elisseeff, 2002).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nBy far, the best known (and near-optimal) rates about exponential generalization bounds are offered by approaches based on uniform stability and certain fine-grained concentration inequalities for sum of functions of independent random variables (Feldman & Vondr ́ak, 2019; Bousquet et al., 2020). These rates are substantially sharper than those of Bousquet & Elisseeff (2002), which are implied by a naive application of McDiarmid’s inequality, in terms of the overhead factors on stability coefficients. While it has long been known that the low probability of failure (over sample) can be handled via developing modified bounded-difference inequalities (Rakhlin et al., 2005), it still remains less clear how to simply adapt these existing techniques to the more sophisticated frameworks of Feldman & Vondr ́ak (2019); Bousquet et al. (2020) to obtain sharper exponential bounds. Particularly for Lq-stable learning algorithms, the state-of-the-art exponential generalization bounds are derived based on the moments or exponential extensions of the Efron-Stein inequality (Celisse & Guedj, 2016; Abou-Moustafa & Szepesv ́ari, 2019), which yield similar rates of convergence to those of Bousquet & Elisseeff (2002) and thus are suspected to be sub-optimal.\n\nGiven the above observed gap in rates of convergence between the generalization bounds under uniform stability and Lq-stability, the following question is naturally raised:\n\nIs it possible to derive sharper exponential generalization bounds for Lq-stable learning algorithms that match those recent breakthrough results for uniformly stable algorithms?\n\nAs the core contribution of the present work, we give an affirmative answer to this open question by developing strict analogues of the near-optimal generalization bounds of uniformly stable algorithms for Lq-stable algorithms. The main results of our work confirm that the notion of Lq-stability serves as a neat yet powerful tool for extending those best-known generalization bounds to a broad class of non-uniformly stable algorithms. To illustrate the importance of our theory, we have applied the improved analysis of Lq-stable algorithms to derive sharper exponential risk bounds for computationally tractable sparsity recovery estimators, such as the Iterative Hard Thresholding (IHT) algorithms widely used in high dimensional sparse learning (Blumensath & Davies, 2009; Foucart, 2011; Jain et al., 2014). This application also serves as a main motivation of our study.\n\nNotation. Here we provide some notation that will be frequently used throughout the paper. Let S = {Z1, Z2, ..., ZN } be a set of independent random data samples valued in some measurable set Z. For any indices set I ⊆ [N ] := {1, ..., N }, we denote by SI = {Zi, i ∈ I} and SI = S \\ SI . We denote by S′ = {Z ′ N } another i.i.d. sample from the same distribution as that of S and we write S(i) = {Z1, ..., Zi−1, Z ′ i, Zi+1, ..., ZN }. For a real-valued random variable Y , its Lq-norm for q ≥ 1 is defined by ∥Y ∥q = (E[|Y |q])1/q. By definition it can be verified that ∀q ≥ 2,\n\n2, ..., Z ′\n\n1, Z ′\n\n∥Y ∥2\n\nq = (E[|Y |q])2/q =\n\n(cid:16)\n\nE[|Y 2|q/2]\n\n(cid:17)2/q\n\n= (cid:13)\n\n(cid:13)Y 2(cid:13)\n\n(cid:13)q/2 .\n\n(1)\n\nLet g : Z N (cid:55)→ R be some measurable function and consider the random variable g(S) = g(Z1, ...ZN ). For g(S) and any index set I ⊆ [N ], we define the following abbreviations:\n\n∥g∥q(SI ) := (E[|g(S)|q | SI ])1/q . We say a real-valued function f is G-Lipschitz continuous over the domain W if\n\ng(SI ) := E[g(S) | SI ],\n\n|f (w) − f (w′)| ≤ G∥w − w′∥, ∀w, w′ ∈ W. For a pair of functions f, g ≥ 0, we use f ≲ g (or g ≳ f ) to denote f ≤ cg for some constant c > 0. We denote by supp(w) the support of a vector w which is the index set of non-zero entries of w.\n\n1.1 SETUP AND PRIOR RESULTS\n\nProblem setup. We consider a statistical learning algorithm A : Z N (cid:55)→ W that maps a training data set S to a model A(S) in a closed subset W of an Euclidean space. The population risk and corresponding empirical risk evaluated at A(S) are respectively given by\n\nR(A(S)) := EZ[l(A(S); Z)] and RS(A(S)) :=\n\n1 N\n\nN (cid:88)\n\ni=1\n\nl(A(S); Zi),\n\nwhere l : W × Z (cid:55)→ R+ is a non-negative and potentially unbounded loss function whose value l(w; z) measures the loss evaluated at z with parameter w. As a classic fundamental issue in statistical learning, we are interested in deriving the upper bounds on the difference between population\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nand empirical risks, i.e., |R(A(S)) − RS(A(S))|, which quantifies the generalization error of A. Let R∗ := minw∈W R(w) be the optimal value of the population risk. We will also study how to upper bound R(A(S)) − R∗ (a.k.a. excess risk) which is of particular interest for understanding the population risk minimization performance of A.\n\nWe first introduce the concept of uniform stability (Bousquet & Elisseeff, 2002) which requires the change in the prediction loss to be uniformly small regardless the distribution of data. Definition 1 (Uniform stability). A learning algorithm A is said to have uniform stability with parameter γu > 0 if it satisfies the following uniform bound:\n\nsup S,S(i),Z∈Z\n\n|l(A(S); Z) − l(A(S(i)); Z)| ≤ γu, ∀i ∈ [N ].\n\nGiven that the loss function l is almost surely bound by M , Bousquet & Elisseeff (2002) showed that a large class of regularized empirical risk minimization (ERM) algorithms has uniform stability, and using McDiarmid’s inequality yields the following exponential tail generalization bound that holds with probability at least 1 − δ over the draw of S for any δ ∈ (0, 1):\n\n|R(A(S)) − RS(A(S))| ≲ γu\n\nN log\n\n(cid:115)\n\n(cid:114)\n\n+ M\n\n(cid:19)\n\n(cid:18) 1 δ\n\nlog (1/δ) N\n\n.\n\n(2)\n\nRecently, equipped with a strong concentration inequality for sums of random functions, Bousquet et al. (2020) established the following moments bound of uniformly stable algorithms for all q ≥ 2:\n\n∥R(A(S)) − RS(A(S))∥q\n\n≲ qγu log(N ) + M\n\n(cid:114) q N\n\n.\n\n(3)\n\nIn view of the equivalence between tails and moments (see, e.g., Bousquet et al., 2020, Lemma 1), the above Lq-norm bound implies that for any δ ∈ (0, 1), the following tail bound holds with probability at least 1 − δ over the draw of S :\n\n|R(A(S)) − RS(A(S))| ≲ γu log(N ) log\n\n(cid:114)\n\n+ M\n\n(cid:19)\n\n(cid:18) 1 δ\n\nlog (1/δ) N\n\n.\n\n(4)\n\nN\n\nN log( 1\n\nThis bound substantially improves the classic result in Eq. (2) by reducing the overhead factor on sta-\n\nδ )(cid:1) to O(log(N ) log( 1\n\nbility coefficient from O(cid:0)(cid:113) δ )). For example, in regimes such as regularized ERM where γu ≲ 1√ is usually the case, the convergence rate in Eq. (2) becomes vacuous as it is not vanishing in sample size, while the bound in Eq. (4) still guarantees O(cid:0) log(N ) log( 1 (cid:1) rate δ ) of convergence. Indeed, up to logarithmic factors on sample size and tail bounds, the rate in Eq. (4) is nearly optimal in the sense of a lower bound on sums of random functions by Bousquet et al. (2020). The bound in Eq. (4) can be extended to stochastic learning algorithms when the uniform stability (over data) holds with high probability over the internal randomness of algorithm (Feldman & Vondr ́ak, 2019; Bassily et al., 2020). Under the generalized Bernstein condition (Koltchinskii, 2006) and based on the sharp concentration inequality for sums of random functions by Bousquet et al. (2020), Klochkov & Zhivotovskiy (2021) alternatively established the following deviation optimal excess risk bound that holds with probability at least 1 − δ over the draw of S:\n\n√\n\nN\n\nR(A(S)) − R∗ ≲ ∆opt + E[∆opt] + γu log(N ) log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n(M + B) log(1/δ) N\n\n,\n\n(5)\n\nwhere ∆opt := RS(A(S)) − minw∈W RS(w) represents the empirical risk sub-optimality of the algorithm on training data, and B is the Bernstein condition constant as defined in Assumption 1.\n\nWhile implying strong generalization guarantees, the uniform stability is also most stringent in the sense that it is distribution independent and hard to be fulfilled, e.g., by learning rules with unbounded losses. To address such an unpleasant restrictiveness, the notion of Lq-stability was alternatively introduced by Celisse & Guedj (2016) as a relaxation of uniform stability. Definition 2 (Lq-Stability). For q ≥ 1, a learning algorithm A is said to have Lq-stability with parameter γq > 0 if it satisfies the following moment bound:\n\n(cid:13) (cid:13) (cid:13)l(A(S); Z) − l(A(S(i)); Z) (cid:13) (cid:13) (cid:13)q\n\n≤ γq, ∀i ∈ [N ].\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nIn the above definition, the expectation associated with Lq-norm is taken over S, S(i), Z, and the internal random bits of A, if any (such as in the case of stochastic learning algorithms). Note that slightly different from that of Celisse & Guedj (2016), the random variable Z in the above definition is not necessarily required to be independent of S and S(i). By definition, Lq-stability is distribution dependent and thus is weaker than uniform stability which can be regarded as a special case of Lqstability with γq ≡ γ for some γ > 0. Particularly for q = 1 and q = 2, the Lq-stability reduces to the notions of hypothesis stability (Bousquet & Elisseeff, 2002) and mean-square stability (Kale et al., 2011), respectively. For an instance, it has been shown that the classical ridge regression model with unbounded responses has Lq-stability for all q ≥ 1 rather than uniform stability (Celisse & Guedj, 2016). As a novel and concrete example, we will see shortly in Section 3 that Lq-stability plays a crucial role for deriving strong sparse excess risk bounds for sparsity estimation algorithms such as IHT (Jain et al., 2014; Yuan et al., 2018). Alternatively, the definition of Lq-stability can be extended to the Lq-argument-stability as (cid:13) (cid:13)q ≤ γq, which generalizes the concept of uniform argument stability (Bassily et al., 2020) to the Lq-norm criterion. Obviously Lq-argument-stability is not at all relying on the random argument Z and it implies Lq-stability for Lipschitz losses.\n\n(cid:13)∥A(S) − A(S(i))∥(cid:13)\n\nThe following is by far the best known moments generalization bound under Lq-stability that holds for all q ≥ 2 and potentially unbounded losses (Celisse & Guedj, 2016; Abou-Moustafa & Szepesv ́ari, 2019):\n\n∥R(A(S)) − RS(A(S))∥q\n\n≲ γq\n\n(cid:112)N q +\n\n(cid:114) q N\n\n.\n\n(6)\n\nAs one can see that the Lq-stability generalization bound in Eq. (6) is significantly inferior to the near-optimal uniform stability generalization bound in Eq. (3) in terms of the overhead on stability coefficient. Such a gap in rate of convergence is indeed unsurprising: the bound in Eq. (6) was derived via more or less directly applying moments or exponential extensions of Efron-Stein inequality to generalization error (Celisse & Guedj, 2016; Abou-Moustafa & Szepesv ́ari, 2019), and thus yields about the same overhead factor on stability coefficient as that of the sub-optimal exponential bound in Eq. (2) for uniformly stable algorithms. In light of these observations, we are naturally motivated to derive sharper exponential generalization bounds for Lq-stable algorithms hopefully to match the near-optimal bound in Eq. (3) achievable by uniformly stable algorithms.\n\n1.2 OUR CONTRIBUTION\n\nThe core contribution of the present work is a set of substantially improved exponential generalization bounds for Lq-stable algorithms. The key ingredient of our analysis is a sharper concentration bound on sums of functions of independent random variables under the Lq-norm bounded difference conditions, which generalizes a previous counterpart under the uniform bounded difference conditions (Bousquet et al., 2020). With this generic concentration bound in hand, we are able to derive sharper generalization and excess risk bounds for Lq-stable learning algorithms that match those best known for uniform stable algorithms. The power of our results is demonstrated through deriving more appealing exponential sparse excess risk bounds for computationally tractable sparsity estimation algorithms (such as IHT). The main results obtained in this work are sketched below:\n\n• In Section 2, we first establish in Theorem 1 an Lq-norm inequality for sums of functions of random variables with Lq-norm bounded difference. Then equipped with such a generalpurpose concentration inequality, we prove in Theorem 2 the following Lq-norm generalization bound for Lq-stable learning algorithms for all q ≥ 2:\n\n∥R(A(S)) − RS(A(S))∥q\n\n≲ qγq log N + Mq\n\n(cid:114) q N\n\n,\n\nwhere Mq is an upper bound of moments ∥l(A(S); Z)∥q. Compared to Eq. (6), the preceding N to log(N ). As another consequence of our bound improves the overhead factor on γq from Lq-norm concentration inequality, we further derive in Theorem 3 the following excess risk bound for Lq-stable algorithms under B-Bernstein-condition with M -bounded losses (where C = M + B), or μ-quadratic-growth condition with G-Lipschitz losses (where C = G2\n\n√\n\nμ ):\n\n∥R(A(S)) − R∗ − ∆opt∥q ≲ E[∆opt] + qγq log(N ) +\n\nCq N\n\n.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nBased on the equivalence between moments and tails, this above result implies an identical deviation optimal risk bound in Eq. (5) for uniformly stable algorithms.\n\n• In Section 3, based on our Lq-stability generalization theory, we show in Theorem 4 a novel exponential sparse excess risk bound for inexact L0-estimators. A key insight here is that L0estimators are in many cases “almost always” stable over any fixed supporting set, and thus can be shown to have Lq-stability over the same supporting set, which consequently makes our analysis techniques developed for Lq-stable algorithms applicable there. This novel application answers a call by Celisse & Guedj (2016) for extending the range of applicability of the Lq-stability theory beyond the unbounded ridge regression problem, and it complements other existing applications of the Lq-stability theory including k-nearest neighbor classification and k-folds cross-validation (Celisse & Mary-Huard, 2018; Abou-Moustafa & Szepesv ́ari, 2019). Last but not least, our improved Lq-stability theory can also be readily applied to the above mentioned prior applications to obtain sharper generalization bounds.\n\n2 SHARPER EXPONENTIAL BOUNDS FOR Lq-STABLE ALGORITHMS\n\n2.1 A MOMENT INEQUALITY FOR SUMS OF RANDOM FUNCTIONS\n\nWe start by presenting in the following theorem a moment inequality for sums of random functions of N independent random variables that satisfy the Lq-norm bounded difference condition. See Appendix B.1 for its proof. Theorem 1. Let S = {Z1, Z2, ..., ZN } be a set of independent random variables valued in Z. Let g1, ..., gN be a set of measurable functions gi : Z N (cid:55)→ R that satisfy the following conditions for any i ∈ [N ]:\n\n• E [gi(S) | S \\ Zi] = 0, almost surely;\n\n• gi(S) has the following Lq-norm bounded difference property with respect to all variables in\n\nS except Zi, i.e., ∀j ̸= i, for all q ≥ 1:\n\n(cid:13) (cid:13) (cid:13)gi(S) − gi(S(j)) (cid:13) (cid:13) (cid:13)q\n\n≤ βq.\n\nThen there exists a universal constant κ < 1.271 such that for all q ≥ 2,\n\nAdditionally, if ∥E[gi(S) | Zi]∥q ≤ Mq, then for all q ≥ 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ngi(S) − E[gi(S) | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 4κqN ⌈log2 N ⌉βq.\n\nN (cid:88)\n\ni=1\n\ngi(S)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 2(cid:112)2κN qMq + 4κqN ⌈log2 N ⌉βq.\n\nRemark 1. Theorem 1 extends the moment inequality of Bousquet et al. (2020, Theorem 4) from under the distribution-free uniform bounded difference property to under the Lq-norm bounded difference property which is distribution dependent. Specially if gi(S) have uniformly bounded difference property, then Theorem 1 reduces to the result of Bousquet et al. (2020, Theorem 4). Remark 2. The Lq-norm boundedness condition ∥E[gi(S) | Zi]∥q ≤ Mq in our theorem allows gi to be potentially unbounded over domain of interest, which is weaker than the corresponding almost sure boundedness condition on |E[gi(S) | Zi]| as imposed by Bousquet et al. (2020, Theorem 4).\n\n2.2 GENERALIZATION BOUNDS FOR Lq -STABLE ALGORITHMS\n\nAs an important consequence of Theorem 1, we can derive er the following main result on the generalization bound of Lq-stable learning algorithms. See Appendix B.2 for a proof of this result. Theorem 2. Let A : Z N (cid:55)→ W be a learning algorithm that has Lq-stability by γq > 0 for q ≥ 1. Suppose that ∥l(A(S); Z)∥q ≤ Mq for any Z ∈ Z. Then for all q ≥ 2,\n\n∥R(A(S)) − RS(A(S))∥q\n\n≲ qγq log N + Mq\n\n(cid:114) q N\n\n.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nRemark 3. The Lq-norm boundedness condition ∥l(A(S); Z)∥q ≤ Mq allows for learning with unbounded losses over, e.g., data distribution with sub-Gaussian or sub-exponential tail bounds.\n\nTo compare with the best-known moments generalization bound in Eq. (6) under the notion of Lqstability, our bound in Theorem 2 substantially improves the overhead factor on γq from N to log(N ). Specially when reduced to regime of uniform stability where γq ≡ γu for all q ≥ 1, our result revisits the moments generalization bound in Eq. (3) which is nearly tight, up to logarithmic factors on sample size, in the sense of a lower bound on sums of random functions from Bousquet et al. (2020). More broadly, for any δ ∈ (0, 1), suppose that the following exponential stability bound holds with probability at least 1 − δ with a mixture of sub-Gaussian and sub-exponential tails 1 over S, S(i), Z:\n\n√\n\n(cid:12) (cid:12) (cid:12)l(A(S); Z) − l(A(S(i)); Z) (cid:12) (cid:12) (cid:12) ≤ a log\n\n(cid:17)\n\n(cid:16) e δ\n\n(cid:114)\n\n+ b\n\nlog\n\n(cid:17)\n\n.\n\n(cid:16) e δ\n\n(7)\n\nThen according to the equivalence of tails and moments, as summarized in Lemma 4 (see Appendix A), we must have that A is Lq-stable by γq = aq + b q. Assume that the loss is bounded in (0, M ] almost surely over data. Then the Lq-norm generalization bound in Theorem 2 combined with Lemma 4 immediately implies the following generalization bound:\n\n√\n\n|R(A(S)) − RS(A(S))| ≲ a log(N ) log2\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+ b log(N ) log1.5\n\n(cid:114)\n\n+ M\n\n(cid:19)\n\n(cid:18) 1 δ\n\nlog (1/δ) N\n\n.\n\nCompared with the uniform stability implied tail bound in Eq. (4), the preceding Lq-stability bound is nearly identical up to slightly worse confidence tail terms which are caused by the uncertainty of Lq-stability with respect to data distribution. We conjecture that such a slight deterioration in tail bounds might possibly be remedied by using the exponential versions of Efron-Stein inequality (Boucheron et al., 2003; Abou-Moustafa & Szepesv ́ari, 2019) instead of the currently used variant in moments. We leave the improvement over poly-logarithmic terms for future investigation.\n\n2.3 EXCESS RISK BOUNDS FOR Lq -STABLE ALGORITHMS\n\nIn addition to the generalization bounds, we further apply Theorem 1 to study the excess risk bounds of an Lq-stable learning algorithm which are of particular interest for understanding its population risk minimization performance. Let us denote W ∗ := Argminw∈W R(w) as the optimal solution set of the population risk. In order to get sharper risk bounds, we need to impose some structural conditions on risk functions. Particularly, the following defined generalized Bernstein condition (Koltchinskii, 2006) is conventionally used with multiple global risk minimizers allowed.\n\nAssumption 1 (Generalized Bernstein condition). For some B > 0 and for any w ∈ W, there exists w∗ ∈ W ∗ such that the following holds:\n\nE (cid:2)(l(w; Z) − l(w∗; Z))2(cid:3) ≤ B(R(w) − R(w∗)).\n\nWe will also consider the quadratic growth condition which is widely used as an alternative condition for establishing fast rates of convergence in learning theory.\n\nAssumption 2 (Quadratic growth condition). For some μ > 0 and for any w ∈ W, there exists w∗ ∈ W ∗ such that the following holds:\n\nR(w) ≥ R∗ +\n\nμ 2\n\n∥w − w∗∥2.\n\nRemark 4. Clearly, when the loss is G-Lipschitz, the quadratic growth condition with parameter μ implies the Bernstein condition with parameter B = 2G2 μ .\n\nThe following theorem is our main result on the excess risk bound of Lq-stable algorithms, which extends the near-optimal exponential risk bounds of Klochkov & Zhivotovskiy (2021) from uniform stable algorithms to Lq-stable algorithms. A proof of this result can be found in Appendix B.3.\n\n1In an exponential tail bound, the terms associated with\n\nas sub-Gaussian and sub-exponential tails.\n\n6\n\n(cid:113)\n\nlog (cid:0) 1\n\nδ\n\n(cid:1) and log (cid:0) 1\n\nδ\n\n(cid:1) are respectively referred to\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 3. Let A : Z N (cid:55)→ W be a learning algorithm that has Lq-stability with parameter γq for q ≥ 1.\n\n(a) If Assumption 1 holds and l(·; ·) ≤ M , then ∀q ≥ 2,\n\n∥R(A(S)) − R∗ − ∆opt∥q\n\n≲ E[∆opt] + qγq log(N ) +\n\n(M + B)q N\n\n.\n\n(b) If Assumption 2 holds and l(·; ·) is G-Lipschitz with respect to its first argument, then ∀q ≥ 2,\n\n∥R(A(S)) − R∗ − ∆opt∥q ≲ E[∆opt] + qγq log(N ) +\n\nG2q μN\n\n.\n\n√\n\nRemark 5. Suppose that A satisfies the exponential stability bound in Eq. (7), and thus A has Lqq. Then combined with Lemma 4, the Lq-norm risk bounds in Theorem 3 stability by γq = aq + b suggest that the following exponential tail bound holds: (cid:18) 1 δ\n\nR(A(S)) − R∗ ≲ ∆opt + E[∆opt] + a log(N ) log2\n\nlog (1/δ) N\n\n+ b log(N ) log1.5\n\n(cid:18) 1 δ\n\n+\n\n(cid:19)\n\n(cid:19)\n\n.\n\nRemark 6. In part (a), the M -bounded-loss condition is not essential and it can be relaxed to a subexponential (or sub-Gaussian) variant by alternatively using the general Bernstein-type inequalities for sums of independent sub-exponential random variables (Vershynin, 2018). Concerning part (b), under the quadratic growth condition, the loss is allowed to be unbounded if it is Lipschitz continuous.\n\n3 APPLICATION TO INEXACT L0-ERM\n\nIn this section, we demonstrate an application of our Lq-stability and generalization theory to the following problem of high-dimensional stochastic risk minimization under hard sparsity constraint:\n\nmin w∈W\n\nR(w) := EZ[l(w; Z)]\n\nsubject to ∥w∥0 ≤ k,\n\nwhere W ⊆ Rd the cardinality constraint ∥w∥0 ≤ k for k ≪ d is imposed for enhancing the interpretability and learnability of model in situations where there are no clear favourite explanatory variables, or the model is over-parameterized. We consider the following L0-ERM problem over training set S = {Zi}i∈[N ]:\n\n(cid:40)\n\nw∗\n\nS,k = arg min ∥w∥0≤k\n\nRS(w) :=\n\n(cid:41)\n\nl(w; Zi)\n\n.\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(8)\n\nSince the problem is known to be NP-hard (Natarajan, 1995) in general, it is computationally intractable to solve it exactly in general cases. Alternatively, we consider the inexact L0-ERM oracle as a meta-algorithm outlined in Algorithm 1. In order to avoid assuming unrealistic conditions like restricted isometry property (RIP), it is typically needed to allow sparsity level relaxation for approximate algorithms like IHT to achieve favorable converge behavior (Jain et al., 2014; Shen & Li, 2017; Yuan et al., 2018; Murata & Suzuki, 2018). Therefore, we are particularly interested in\n\nAlgorithm 1: Inexact L0-ERM Oracle Input Output: ̃wS,k. Compute an inexact k-sparse L0-ERM estimation ̃wS,k such that\n\n: A training data set S = {Zi}i∈[N ] and the desired sparsity level k.\n\n• ̃wS,k is optimal over its support ̃J = supp( ̃wS,k), i.e., ̃wS,k = arg minw∈W,supp(w)⊆ ̃J RS(w); • ̃wS,k attains certain ̄k-sparse sub-optimality level ∆ ̄k,opt ≥ 0 for some ̄k ≤ k such that\n\nRS( ̃wS,k) − RS(w∗\n\nS, ̄k) ≤ ∆ ̄k,opt.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nthe inexact L0-ERM oracle with ̄k-sparse sub-optimality ∆ ̄k,opt ≥ 0 for some ̄k ≤ k such that the output ̃wS,k of Algorithm 1 satisfies\n\nRS( ̃wS,k) − RS(w∗\n\nS, ̄k) ≤ ∆ ̄k,opt.\n\nIt is typical that ∆ ̄k,opt is a random value over the training set S. For example, the sub-optimality guarantees of IHT for empirical risk usually hold with high probability over training data (Jain ̄k := arg min∥w∥0≤ ̄k R(w) be the ̄k-sparse minimizer of population risk for et al., 2014). Let w∗ some ̄k ≤ k. We are interested in deriving exponential upper bounds for the ̄k-sparse excess risk given by R( ̃wS,k) − R(w∗\n\n ̄k).\n\nOur analysis also relies on the conditions of Restricted Strong Convexity (RSC) which extends the concept of strong convexity to the analysis of sparsity recovery methods (Bahmani et al., 2013; Blumensath & Davies, 2009; Jain et al., 2014; Yuan et al., 2020). Definition 3 (Restricted Strong Convexity). For any sparsity level 1 ≤ s ≤ d, we say a function f is restricted μs-strongly convex if there exists some μs > 0 such that\n\nf (w) − f (w′) − ⟨∇f (w′), w − w′⟩ ≥\n\n∥w − w′∥2, ∀∥w − w′∥0 ≤ s.\n\nμs 2\n\nSpecially when s = d, we say f is μ-strongly convex if it is μd-strongly convex.\n\nThe following basic assumptions will be used in our theoretical analysis. Assumption 3. The loss function l(·; ·) is convex and G-Lipschitz with respect to its first argument. Assumption 4. The population risk R is μ-strongly convex and the empirical risk RS is μk-strongly convex with probability at least 1 − δN over sample S for some δN ∈ (0, 1). Assumption 5. The domain of interest is uniformly bounded such that ∥w∥ ≤ D, ∀w ∈ W. Remark 7. Assumption 3 is common in the study of algorithmic stability and generalization theory. Assumption 4 is conventional in the sparsity recovery analysis of L0-ERM. Assumption 5 is needed for establishing the Lq-stability of L0-ERM in Lemma 1 to follow. Similar conditions have also been assumed in the prior work of Yuan & Li (2022).\n\nLet w∗ := arg minw∈W R(w) be the global minimizer which is unique due to the strong convexity of R. For a given index set J ⊆ [d], let us consider the following restrictive estimator over J:\n\nw∗\n\nS|J :=\n\narg min w∈W,supp(w)⊆J\n\nRS(w).\n\n(9)\n\nWe first present the following lemma that guarantees the Lq-stability of w∗ |J| = k. See Appendix C.1 for its proof. Lemma 1. Assume that Assumptions 3, 4 and 5 hold and log(1/δN ) set of indices of cardinality k. Then for any q ≥ 2, the oracle estimator w∗ parameter\n\nlog(N ) ≥ 2. Let J ⊆ [d], |J| = k be a S|J has Lq-stability with\n\nS|J for any fixed J with\n\nγq =\n\n1 N\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n+\n\n2GD log(N )q log(1/δN )\n\n.\n\nRemark 8. For sparse linear regression models, it can be verified based on the result by Agarwal et al. (2012, Lemma 6) that Assumptions 4 holds with δN = e−c0N for some universal positive constant c0. Then we have log(1/δN ) log(N ) ≥ 2 for sufficiently large N , and Lemma 1 implies that γq ≲ 1\n\nlog(N ) = c0N (cid:1) for all q ≥ 2.\n\n+ GD log(N )q\n\n(cid:0) G2 μk\n\nN\n\nc0\n\nThe following theorem is our main result on the sparse excess risk of the inexact L0-ERM oracle as defined in Algorithm 1. See Appendix C.2 for its proof which is stimulated by that of Theorem 3. Theorem 4. Suppose that Assumptions 3, 4, 5 hold. Assume that log(1/δN ) log(N ) ≥ 2. Then for any δ ∈ (0, e−1), the following ̄k-sparse excess risk bound holds with probability at least 1 − δ over the random draw of S: R( ̃wS,k) − R(w∗ ̄k) GD (cid:0)k log (cid:0) ed (cid:1) + log (cid:0) e k\nδ log(1/δN )\n\n(cid:1) + log (cid:0) e N\n\n(cid:19) k log (cid:0) ed\n\n(cid:18) G2 μk\n\nlog2(N )\n\nlog(N )\n\nG2 μ\n\n+ GD\n\n(cid:1)(cid:1)2\n\n+\n\n+\n\n≲\n\n(cid:19)\n\n(cid:18)\n\n(cid:1)\n\nk\n\nδ\n\n(cid:115) (cid:0)k log (cid:0) ed\n\nk\n\n+ G\n\n(cid:1) + log (cid:0) e\n\n(cid:1)(cid:1) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nδ N μ\n\n+ ∆ ̄k,opt + E (cid:2)∆ ̄k,opt\n\n(cid:3) .\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nRemark 9. For the IHT-style algorithms, the sparse optimization sub-optimality ∆ ̄k,opt can be arbitrarily small (with high probability) after sufficient rounds of iteration (Jain et al., 2014).\n\nSpecially for sparse linear regression models in which Assumptions 4 holds with δN = e−c0N (Agarwal et al., 2012), we have that log(1/δN ) log(N ) ≥ 2 can always be fulfilled for sufficiently large sample size N , and the sparse excess risk bound in Theorem 4 roughly scales as\n\nlog(N ) = c0N\n\nR( ̃wS,k) − R(w∗\n\n ̄k) ≲ (k log (d) + log (1/δ))2 log2(N )\n\nN\n\n(cid:115)\n\n+\n\n(k log (d) + log (1/δ)) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nN\n\n+ ∆ ̄k,opt + E (cid:2)∆ ̄k,opt\n\n(cid:3) .\n\nN\n\nGenerally for misspecified sparsity models, the dominant rate in the above bound matches the (cid:1) sparse excess risk bound of Yuan & Li (2022, Theorem 1) for IHT under similar conditions. O(cid:0) 1√ (cid:1) bound available in that paper (Yuan & Li, 2022, Theorem 3), the preceding Compared to the O(cid:0) 1 bound is generally slower in rate but more broadly applicable without imposing any strong-signal or bounded-loss conditions as required in the analysis of Yuan & Li (2022). For well-specified ̄k-sparse models such that R(w∗ truly ̄k-sparse, the preceding bound improves to\n\n ̄k) = R(w∗), i.e., the population minimizer is\n\nN\n\nR( ̃wS,k) − R(w∗\n\n ̄k) ≲ (k log (d) + log (1/δ))2 log2(N )\n\n+ ∆ ̄k,opt + E (cid:2)∆ ̄k,opt\n\n(cid:3) .\n\nN Therefore, our bound is more appealing in the sense that it naturally adapts to well-specified models to attain an improved O( 1 N ) rate. In contrast, the regularization technique used by Yuan & Li (2022, Theorem 1) needs an optimal choice of penalty strength of scale O( 1√ ) which leads to an overall slow rate of convergence, though the analysis is relatively simpler.\n\nN\n\nWe further comment on the role of Lq-stability played in deriving the improved bound of Theorem 4. The O( 1 N ) fast-rate component of the bound is indeed rooted from the Lq-stability coefficient as established in Lemma 1 and an application of Lemma 6. The relatively slow O( 1√ )\n ̄k) − R(w∗), is mainly due to a careful analcomponent, which is controlled by the oracle factor R(w∗ ysis customized for handling the combinatorial optimization nature of L0-ERM. Such a slow-rate term would be vanished if the global minimizer w∗ is truly ̄k-sparse. Therefore, we confirm that the fast-rate component attributes to our Lq-stability theory, while the slow but adaptive rate component mainly attributes to the optimization property of L0-ERM. Finally, we comment in passing that our improved Lq-stability theory can also be applied to some prior applications such as unbounded ridge regression and k-folds cross-validation to obtain sharper generalization bounds.\n\nN\n\n4 CONCLUSION\n\nIn this paper, we presented an improved generalization theory for Lq-stable learning algorithms. There exits a clear discrepancy between the recently developed near-optimal generalization bounds for uniformly stable algorithms and the best known yet sub-optimal bounds for Lq-stable algorithms. Aiming at closing such a theoretical gap, we for the first time derived a set of near-optimal exponential generalization bounds for Lq-stable algorithms that match those of uniformly stable algorithms. As a concrete application of our Lq-stability theory, we have applied the developed analysis tools to derive strong exponential risk bounds for inexact sparsity-constrained ERM estimators under milder conditions. To conclude, Lq-stable algorithms generalize almost as fast as uniformly stable algorithms, though the distribution-dependent notion of Lq-stability is weaker than uniform stability.\n\nACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING\n\nThe authors would like to thank the anonymous Reviewers and Area Chairs for their insightful comments which are truly helpful for improving this paper. Xiao-Tong Yuan is funded in part by the National Key Research and Development Program of China under Grant No.2018AAA0100400, and in part by the Natural Science Foundation of China (NSFC) under Grant No.U21B2049, No.61936005 and No.61876090.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nKarim Abou-Moustafa and Csaba Szepesv ́ari. An exponential Efron-Stein inequality for l q stable In Proceedings of the Conference on Algorithmic Learning Theory (ALT), pp.\n\nlearning rules. 31–63, Chicago, IL, 2019.\n\nAlekh Agarwal, Sahand Negahban, Martin J Wainwright, et al. Fast global convergence of gradient methods for high-dimensional statistical recovery. The Annals of Statistics, 40(5):2452–2482, 2012.\n\nSohail Bahmani, Bhiksha Raj, and Petros T. Boufounos. Greedy sparsity-constrained optimization.\n\nJ. Mach. Learn. Res., 14(1):807–841, 2013.\n\nRaef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic convex optimization with optimal rates. In Advances in Neural Information Processing Systems (NeurIPS), pp. 11279–11288, Vancouver, Canada, 2019.\n\nRaef Bassily, Vitaly Feldman, Crist ́obal Guzm ́an, and Kunal Talwar. Stability of stochastic gradient In Advances in Neural Information Processing Systems\n\ndescent on nonsmooth convex losses. (NeurIPS), virtual, 2020.\n\nThomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing. Ap-\n\nplied and Computational Harmonic Analysis, 27(3):265–274, 2009.\n\nSt ́ephane Boucheron, G ́abor Lugosi, and Pascal Massart. Concentration inequalities using the en-\n\ntropy method. The Annals of Probability, 31(3):1583–1614, 2003.\n\nSt ́ephane Boucheron, Olivier Bousquet, G ́abor Lugosi, and Pascal Massart. Moment inequalities for functions of independent random variables. The Annals of Probability, 33(2):514–560, 2005.\n\nSt ́ephane Boucheron, G ́abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp-\n\ntotic theory of independence. Oxford university press, 2013.\n\nOlivier Bousquet and Andr ́e Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499–526,\n\n2002.\n\nOlivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms. In Proceedings of the Conference on Learning Theory (COLT), pp. 610–626, Virtual Event [Graz, Austria], 2020.\n\nAlain Celisse and Benjamin Guedj. Stability revisited: new generalisation bounds for the leave-one-\n\nout. arXiv preprint arXiv:1608.06412, 2016.\n\nAlain Celisse and Tristan Mary-Huard. Theoretical analysis of cross-validation for estimating the\n\nrisk of the $k$-nearest neighbor classifier. J. Mach. Learn. Res., 19:58:1–58:54, 2018.\n\nZachary Charles and Dimitris S. Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. In Proceedings of the 35th International Conference on Machine Learning (ICML), pp. 744–753, Stockholmsm ̈assan, Stockholm, Sweden, 2018.\n\nLe-Yu Chen and Sokbae Lee. Best subset binary prediction. Journal of Econometrics, 206(1):39–56,\n\n2018.\n\nLe-Yu Chen and Sokbae Lee. Binary classification with covariate selection through l0-penalized\n\nempirical risk minimization. The Econometrics Journal, pp. 1–16, 2020.\n\nYuan Shih Chow and Henry Teicher. Probability theory: independence, interchangeability, martin-\n\ngales. Springer Science & Business Media, 2003.\n\nRichard Combes. An extension of McDiarmid’s inequality. arXiv preprint arXiv:1511.05240, 2015.\n\nQi Deng and Wenzhi Gao. Minibatch and momentum model-based methods for stochastic weakly In Advances in Neural Information Processing Systems (NeurIPS), pp.\n\nconvex optimization. 23115–23127, virtual, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nLuc Devroye and Terry J. Wagner. Distribution-free inequalities for the deleted and holdout error\n\nestimates. IEEE Trans. Inf. Theory, 25(2):202–207, 1979.\n\nVitaly Feldman and Jan Vondr ́ak. Generalization bounds for uniformly stable algorithms. In Advances in Neural Information Processing Systems (NeurIPS), pp. 9770–9780, Montr ́eal, Canada, 2018.\n\nVitaly Feldman and Jan Vondr ́ak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Proceedings of the Conference on Learning Theory (COLT), pp. 1270–1279, Phoenix, AZ, 2019.\n\nVitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal rates in linear time. In Proccedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC), pp. 439–449, Chicago, IL, 2020.\n\nDylan J. Foster and Vasilis Syrgkanis. Statistical learning with a nuisance component. In Proceed-\n\nings of the Conference on Learning Theory (COLT), pp. 1346–1348, Phoenix, AZ, 2019.\n\nSimon Foucart. Hard thresholding pursuit: An algorithm for compressive sensing. SIAM J. Numer.\n\nAnal., 49(6):2543–2563, 2011.\n\nRahul Garg and Rohit Khandekar. Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML), pp. 337–344, Montreal, Canada, 2009.\n\nMoritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic In Proceedings of the 33nd International Conference on Machine Learning\n\ngradient descent. (ICML), pp. 1225–1234, New York City, NY, 2016.\n\nPrateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for highdimensional m-estimation. In Advances in Neural Information Processing Systems (NIPS), pp. 685–693, Montreal, Canada, 2014.\n\nSatyen Kale, Ravi Kumar, and Sergei Vassilvitskii. Cross-validation and mean-square stability. In Proceedings of the Conference on Innovations in Computer Science (ICS), pp. 487–495, Beijing, China, 2011.\n\nYegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with conIn Advances in Neural Information Processing Systems (NeurIPS), pp.\n\nvergence rate o(1/n). 5065–5076, virtual, 2021.\n\nVladimir Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization.\n\nThe Annals of Statistics, 34(6):2593–2656, 2006.\n\nAryeh Kontorovich. Concentration in unbounded metric spaces and algorithmic stability. In Proceedings of the 31th International Conference on Machine Learning (ICML), pp. 28–36, Beijing, China, 2014.\n\nSamuel Kutin. Extensions to McDiarmid’s inequality when differences are bounded with high probability. Dept. Comput. Sci., Univ. Chicago, Chicago, IL, USA, Tech. Rep. TR-2002-04, 2002.\n\nSamuel Kutin and Partha Niyogi. Almost-everywhere algorithmic stability and generalization error. In Proceedings of the 18th Conference in Uncertainty in Artificial Intelligence (UAI), pp. 275– 282, Edmonton, Canada, 2002.\n\nIlja Kuzborskij and Christoph H. Lampert. Data-dependent stability of stochastic gradient descent. In Proceedings of the 35th International Conference on Machine Learning (ICML), pp. 2820– 2829, Stockholmsm ̈assan, Stockholm, Sweden, 2018.\n\nYunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic In Proceedings of the 37th International Conference on Machine Learning\n\ngradient descent. (ICML), pp. 5809–5819, Virtual Event, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nXingguo Li, Tuo Zhao, Raman Arora, Han Liu, and Jarvis D. Haupt. Stochastic variance reduced optimization for nonconvex sparse learning. In Proceedings of the 33nd International Conference on Machine Learning (ICML), pp. 917–925, New York City, NY, 2016.\n\nAndreas Maurer and Massimiliano Pontil. Concentration inequalities under sub-gaussian and subexponential conditions. In Advances in Neural Information Processing Systems (NeurIPS), pp. 7588–7597, virtual, 2021.\n\nSayan Mukherjee, Partha Niyogi, Tomaso A. Poggio, and Ryan M. Rifkin. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization. Adv. Comput. Math., 25(1-3):161–193, 2006.\n\nTomoya Murata and Taiji Suzuki. Sample efficient stochastic gradient iterative hard thresholding method for stochastic sparse linear regression with limited attribute observation. In Advances in Neural Information Processing Systems (NeurIPS), pp. 5317–5326, Montr ́eal, Canada, 2018.\n\nBalas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM J. Comput., 24(2):\n\n227–234, 1995.\n\nAlexander Rakhlin, Sayan Mukherjee, and Tomaso Poggio. Stability results in learning theory.\n\nAnalysis and Applications, 3(04):397–417, 2005.\n\nWilliam H Rogers and Terry J Wagner. A finite sample distribution-free performance bound for\n\nlocal discrimination rules. The Annals of Statistics, pp. 506–514, 1978.\n\nShai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability\n\nand uniform convergence. J. Mach. Learn. Res., 11:2635–2670, 2010.\n\nJie Shen and Ping Li. A tight bound of hard thresholding. J. Mach. Learn. Res., 18:208:1–208:42,\n\n2017.\n\nRoman Vershynin. High-dimensional probability: An introduction with applications in data science,\n\nvolume 47. Cambridge university press, 2018.\n\nJialei Wang, Mladen Kolar, Nathan Srebro, and Tong Zhang. Efficient distributed learning with sparsity. In Proceedings of the 34th International Conference on Machine Learning (ICML), pp. 3636–3645, Sydney, Australia, 2017.\n\nLutz Warnke. On the method of typical bounded differences. Combinatorics, Probability and\n\nComputing, 25(2):269–299, 2016.\n\nXiao-Tong Yuan and Ping Li. Stability and risk bounds of iterative hard thresholding. IEEE Trans.\n\nInf. Theory, 68(10):6663–6681, 2022.\n\nXiao-Tong Yuan, Ping Li, and Tong Zhang. Gradient hard thresholding pursuit. J. Mach. Learn.\n\nRes., 18:166:1–166:43, 2018.\n\nXiao-Tong Yuan, Bo Liu, Lezi Wang, Qingshan Liu, and Dimitris N. Metaxas. Dual iterative hard\n\nthresholding. J. Mach. Learn. Res., 21:152:1–152:50, 2020.\n\nTong Zhang. Leave-one-out bounds for kernel methods. Neural Comput., 15(6):1397–1437, 2003.\n\nPan Zhou, Xiao-Tong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng. Efficient meta learning via minibatch proximal update. In Advances in Neural Information Processing Systems (2019), pp. 1532–1542, Vancouver, Canada, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA PRELIMINARIES\n\nIn this section, we collect some preliminary results that will be used in our analysis. We start by introducing the following Lq-norm generalization of the celebrated Efron-Stein inequality, which is a corollary of Boucheron et al. (2005, Theorem 2).\n\nProposition 1 (Generalized Efron-Stein inequality (Celisse & Guedj, 2016)). Let S = {Z1, ..., ZN } be a set of independent random variables valued in Z and g : Z N (cid:55)→ R be some measurable function. Then there exists a universal constant κ < 1.271 such that for all q ≥ 2,\n\n∥g(S) − E[g(S)]∥q ≤ (cid:112)2κq\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:0)g(S) − g(S(i))(cid:1)2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q/2\n\n.\n\nThe following result is an immediate consequence of Proposition 1 when applied to sum of independent random variables, which revisits a version of Marcinkiewicz-Zygmund inequality (Chow & Teicher, 2003).\n\nProposition 2. Let Z1, ..., ZN be a set of independent centered random variables. Then for all q ≥ 2,\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nZi\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 2(cid:112)2κq\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nZ 2 i\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q/2\n\n.\n\nThe following lemma is simple yet useful in our analysis.\n\nLemma 2. Let S = {Z1, ..., ZN } be a set of independent random variables valued in some measure space Z and g : Z N (cid:55)→ R be some measurable function. Then for all I ⊆ [N ] and q ≥ 1, we have\n\n∥g(SI )∥q ≤ ∥g(S)∥q = ∥∥g∥q(SI )∥q.\n\nProof. Recall g(SI ) = E[g(S) | SI ]. Then using Jensen’s inequality we can show that\n\n∥g(SI )∥q = (E [|E[g(S) | SI ]|q])1/q ≤ (E [E[|g(S)|q | SI ]|])1/q = (E[|g(S)|q])1/q = ∥g(S)∥q.\n\nBy definition we can also express ∥g(S)∥q = (E [E[|g(S)|q | SI ]|])1/q = ∥∥g(S)∥q(SI )∥q.\n\nAs a direct consequence of Lemma 2, the following result indicates that conditional expectation does not expand the differences in Lq-norm. Lemma 3. Let S = {Z1, ..., ZN } be a set of independent random variables valued in some measure space Z and g : Z N (cid:55)→ R be some measurable function. Let I ⊆ [N ] be an index set. Then for all i ∈ I and q ≥ 1,\n\n(cid:13) (cid:13)\n\n(cid:13)g(SI ) − g(S(i) I )\n\n(cid:13) (cid:13) (cid:13)q\n\n≤\n\n(cid:13) (cid:13)g(S) − g(S(i)) (cid:13)\n\n(cid:13) (cid:13) (cid:13)q\n\n.\n\nProof. For each i ∈ I, by applying Lemma 2 to g(S) − g(S(i)) we can show that ∥g(SI ) − g(S(i)\n\nI )∥q ≤ ∥g(S) − g(S(i))∥q, which gives the desired result.\n\nWe also need the following lemma about the equivalence between tails and moments (see, e.g., Bousquet et al., 2020).\n\nLemma 4. Let Y be a real-valued random variable.\n\n• Suppose that Y satisfies the following inequality for some a, b ≥ 0 with probability at least\n\n1 − δ for any δ ∈ (0, 1),\n\n|Y | ≤ a log\n\n(cid:17)\n\n(cid:16) e δ\n\n(cid:114)\n\n+ b\n\nlog\n\n(cid:17)\n\n.\n\n(cid:16) e δ\n\nThen, for any q ≥ 1 it holds that\n\n∥Y ∥q ≤ 3aq + 9b\n\n√\n\nq.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n• Suppose that Y satisfies ∥Y ∥q ≤ f (q) for any 1 ≤ ql ≤ q < qu and some non-negative real function f . Then the following holds with probability at least 1 − δ for any δ ∈ (e1−qu , e1−ql ]: (cid:16)\n\n(cid:17)(cid:17)\n\n|Y | ≤ ef\n\nlog\n\n.\n\n(cid:16) e δ\n\nProof. We only prove the second part which slightly generalizes the corresponding result of Bousquet et al. (2020, Lemma 1). For any δ ∈ (e1−qu , e1−ql ], we choose q = log(e/δ) ∈ [ql, qu). Using the condition ∥Y ∥q ≤ f (q) and Markov’s inequality yields\n\n(cid:16)\n\nP\n\n|Y | > ef\n\n(cid:17)(cid:17)(cid:17)\n\n(cid:16)\n\nlog\n\n(cid:16) e δ\n\n≤ P (|Y | > e∥Y ∥q) ≤\n\nE[|Y |q] eq∥Y ∥q\n\nq\n\n=\n\nδ e\n\n≤ δ.\n\nThis proves the desired bound in the second part.\n\nRemark 10. Suppose that for any δ ∈ (0, 1), the following inequality holds with probability at least 1 − δ over S, S(i), Z:\n\n(cid:12) (cid:12) (cid:12)l(A(S); Z) − l(A(S(i)); Z) (cid:12) (cid:12) (cid:12) ≤ a log\n\n(cid:17)\n\n(cid:16) e δ\n\n(cid:114)\n\n+ b\n\nlog\n\n(cid:17)\n\n.\n\n(cid:16) e δ\n\nThen according to the first part of Lemma 4 we have that A is Lq-stable by γq = 3aq + 9b Remark 11. Specially if ql = 1 and qu = ∞ is allowed, then the second bound in Lemma 4 holds with an arbitrary tail bound δ ∈ (0, 1).\n\nq.\n\n√\n\nFinally, we present the following technical lemma about self-bounding inequalities to be used for showing fast rates of excess risk bounds under Bernstein or quadratic growth conditions. Lemma 5. Let x, a, b, c be a set of non-negative quantities satisfying x ≤ a + (cid:112)b(x + c). Then it must hold that x ≤ 3a+2b+c\n\n.\n\n2\n\nProof. If x ≤ a, then the claim holds trivially. In the complementary case of x > a, by condition we must have (x − a)2 ≤ b(x + c), which then implies 2a + b + 2(cid:112)b(a + c) 2\n\n3a + 2b + c 2\n\nx ≤\n\n≤\n\n,\n\nwhere we have used the basic fact 2(cid:112)b(a + c) ≤ b + a + c.\n\nB PROOFS FOR SECTION 2\n\nB.1 PROOF OF THEOREM 1\n\nThe proof is a generalization of the sample-splitting arguments of Feldman & Vondr ́ak (2019); Bousquet et al. (2020) under the considered property of Lq-norm bounded difference. For the sake of completeness, we reproduce below the relatively simpler arguments of Bousquet et al. (2020, Theorem 4), with proper modifications made to adapt to our setting via using the generalized EfronStein inequality in places of McDiarmid’s inequality.\n\nProof of Theorem 1. Consider k such that 2k−1 < N ≤ 2k. If N < 2k, we pad the training set S with extra zero-functions so that N = 2k. Consider the partition I0, I1, ..., Ik of [N ] given by\n\nI0 = {{1}, ..., {2k}}, I1 = {{1, 2}, {3, 4}..., {2k − 1, 2k}}, Ik = {{1, ..., 2k}}. For any i ∈ [N ] and l = 0, ..., k, we denote by I l(i) ∈ Il the only set from Il that contains i and consider the following random variables\n\ni = E gl\n\n(cid:104) gi | Zi, SI l(i)\n\n(cid:105)\n\n.\n\nIn particular, g0\n\ni = gi and gk\n\ni = E[gi | Zi]. Clearly we have the following telescope sum:\n\nk−1 (cid:88)\n\n(gl\n\ni − gl+1\n\ni\n\ngi =\n\n) + E[gi | Zi].\n\nl=0\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nIt follows that\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ngi − E[gi | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤\n\nk−1 (cid:88)\n\nl=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ni − gl+1 gl\n\ni\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n.\n\n(10)\n\nWe need to upper bound the right hand side of the above inequality. To this end, it can be verified that\n\ni = E gl+1\n\n(cid:104) (cid:105) gi | Zi, SI l+1(i)}\n\n(cid:104)\n\n= E\n\ngl i | Zi, SI l+1(i)\n\n(cid:105)\n\n.\n\nSince gi has a bounded Lq-difference by βq with respect to all variables except the i-th variable, it is known from Lemma 3 that so is gl i for each l = 0, ..., k. Conditioned on Zi, SI l+1(i), invoking Proposition 1 to gl\n\ni yields\n\n∥gl\n\ni − gl+1\n\ni\n\n(cid:16)\n\n∥q\n\nZi, SI l+1(i)\n\n(cid:17)\n\n≤\n\n(cid:112)\n\n2κq2lβq,\n\nas there are 2l indices in I l+1(i) \\ I l(i). It follows from Lemma 2 that\n\n∥gl\n\ni − gl+1\n\ni\n\n∥q =\n\n(cid:13) (cid:13)∥gl (cid:13)\n\ni − gl+1\n\ni\n\n∥q(Zi, SI l+1(i))\n\n(cid:13) (cid:13) (cid:13)q\n\n(cid:112)\n\n≤\n\n2κq2lβq.\n\nNow consider any I l ∈ Il. Since for each i ∈ Il, gl independent and centered conditioned on SI l . Therefore, applying Proposition 2 yields\n\ndepends only on Zi, SI l , these terms are\n\ni − gl+1\n\ni\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\ni − gl+1 gl\n\ni\n\ni∈I l\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n(cid:0)SI l\n\n(cid:1) ≤ 2\n\n(cid:112)\n\n2κq2l ×\n\n(cid:112)\n\n2κq2lβq = 4κq2lβq,\n\nwhich according to Lemma 2 implies that\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\ni − gl+1 gl\n\ni\n\ni∈I l\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 4κq2lβq.\n\nThen based on the triangle inequality we get\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\ni − gl+1 gl\n\ni\n\ni∈[N ]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n(cid:88)\n\n≤\n\nI l∈Il\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\ni − gl+1 gl\n\ni\n\ni∈I l\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 2k−l × 4κq2lβq = 4κq2kβq < 4κqN βq.\n\nFinally, the right hand side of Eq. (10) can be bounded as\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ngi − E[gi | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤\n\nk−1 (cid:88)\n\nl=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ni − gl+1 gl\n\ni\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 4κqN ⌈log2 N ⌉βq,\n\n(11)\n\nwhich gives the first desired bound. In view of Eq. (11) and the triangle inequality we have\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ngi\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nE[gi | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n+ 4κqN ⌈log2 N ⌉βq.\n\n(12)\n\nSince ∥E[gi(S) | Zi]∥q ≤ Mq and E[gi(S) | S \\ Zi] = 0, it follows from Proposition 2 that the first term at the right hand side of Eq. (12) can be bounded as\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nE[gi | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 2(cid:112)2κN qMq.\n\n(13)\n\nThe second desired bound is obtained by plugging Eq. (13) into Eq. (12).\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nB.2 PROOF OF THEOREM 2\n\nThe proof technique follows that of Bousquet et al. (2020, Lemma 7) developed for uniformly stable algorithms, with natural adaptation to the distribution-dependent notion of Lq-stability.\n\n\n\n.\n\n \n \n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q (cid:125)\n\nProof. Let us consider\n\nhi(S) := R(A(S)) − l(A(S); Zi), gi(S) = EZ′\n\ni\n\n(cid:105) (cid:104) R(A(S(i))) − l(A(S(i)); Zi)\n\n.\n\nThen the Lq-norm of the generalization gap can be bounded as\n\n\n\n∥R(A(S)) − RS(A(S))∥q =\n\n1 N\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nhi(S)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤\n\n1 N\n\nN (cid:88)\n\ni=1\n\n \n \n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q (cid:125)\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\ngi(S)\n\n+\n\n(hi(S) − gi(S))\n\n(cid:123)(cid:122) A\n\n(cid:123)(cid:122) B\n\n(14) We next respectively upper bound the two terms A and B in Eq. (14). To bound the term A, by definition it holds that E[gi(S) | S \\ Zi] = 0. Based on the triangle inequality we can show that\n\n∥E[gi(S) | Zi]∥q ≤∥gi(S)∥q\n\n=\n\n(cid:13) (cid:13) (cid:13)\n\nEZ′\n\ni\n\n[EZ[l(A(S(i)); Z)]] − EZ′\n\ni\n\n(cid:104)\n\nl(A(S(i)); Zi)\n\n(cid:105)(cid:13) (cid:13) (cid:13)q\n\n≤∥l(A(S(i)); Z)∥q + ∥l(A(S(i)); Zi)∥q ≤ 2Mq,\n\nwhere in the first and second inequalities we have twice used Lemma 2. Next we further show that gi has a bounded Lq-norm difference by 2γq with respect to all variables in S except Zi. Indeed, for each j ̸= i it can be verified that\n\ni\n\n(cid:104)\n\nEZ′\n\nR(A(S(i))) − R(A((S(i))(j)))\n\n(cid:13) (cid:13) (cid:13)gi(S) − gi(S(j)) (cid:13) (cid:13) (cid:13)q (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)l(A(S(i)); Z) − l(A((S(i))(j)); Z) (cid:13) (cid:13) (cid:13)q\n\nEZ′\n\ni\n\nEZ[l(A(S(i)); Z) − l(A((S(i))(j)); Z)]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤\n\n=\n\n≤\n\n(cid:105)(cid:13) (cid:13) (cid:13)q\n\n+\n\nEZ′\n\ni\n\n(cid:104) l(A(S(i)); Zi) − l(A((S(i))(j)); Zi)\n\n(cid:105)(cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13) (cid:13)\n\nEZ′\n\ni\n\n[l(A(S(i)); Zi) − l(A((S(i))(j)); Zi)]\n\n(cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13) (cid:13)l(A(S(i)); Zi) − l(A((S(i))(j)); Zi) (cid:13) (cid:13) (cid:13)q\n\n≤ 2γq,\n\nwhere in the last but one inequality we have used Lemma 2, while in the last equality we have used the Lq-stability assumption on the algorithm A. Therefore, {gi} satisfy the conditions of Theorem 1 and thus\n\nA =\n\ngi(S)\n\n≤ 4(cid:112)2κN qMq + 8κqN ⌈log2 N ⌉γq.\n\n(15)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\nNow we proceed to bound the term B. It can be verified that\n\nB ≤\n\n=\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\n≤\n\ni=1\n\n(cid:104)\n\nR(A(S)) − R(A(S(i)))\n\nEZ′\n\ni\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:104)\n\nEZ′\n\ni\n\nEZ\n\nl(A(S); Z) − l(A(S(i)); Z)\n\nN (cid:88)\n\n(cid:105)\n\ni=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n(cid:105) (cid:104) l(A(S); Zi) − l(A(S(i)); Zi)\n\nEZ′\n\ni\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:104)\n\nEZ′\n\ni\n\nl(A(S); Zi) − l(A(S(i)); Zi)\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n(cid:13) (cid:13) (cid:13)l(A(S); Z) − l(A(S(i)); Z) (cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13)l(A(S); Zi) − l(A(S(i)); Zi) (cid:13)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤ 2N γq,\n\nN (cid:88)\n\ni=1\n\n(16) where in the last but one inequality we have used Lemma 2, and in the last equality we have used the Lq-stability assumption. Plugging bounds Eq. (15) and Eq. (16) into Eq. (14) and preserving leading terms yields the desired result.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nB.3 PROOF OF THEOREM 3\n\nWe need the following lemma which plays a fundamental role in proving the main result. Lemma 6. Let A : Z N (cid:55)→ W be a learning algorithm that has Lq-stability by γq for q ≥ 1. Suppose that ∥l(A(S); Z)∥q ≤ Mq for any Z ∈ Z. Let S′ be an independent copy of S. Then the following bound holds for all q ≥ 2:\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nR(A(S)) − RS(A(S)) − E[R(A(S))] +\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(A(S′); Zi) | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≲ qγq log(N ).\n\nProof. Let us again consider gi(S) = EZ′ arguments to those of Theorem 2 we can show that\n\ni\n\n(cid:2)R(A(S(i))) − l(A(S(i)); Zi)(cid:3). Then using similar proof\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\n≤\n\n=\n\n≤\n\nN (R(A(S)) − RS(A(S))) −\n\ngi(S)\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:104)\n\nR(A(S)) − R(A(S(i)))\n\nEZ′\n\ni\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:104)\n\nEZ′\n\ni\n\nEZ\n\nl(A(S); Z) − l(A(S(i)); Z)\n\nN (cid:88)\n\n(cid:105)\n\ni=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n(cid:105) (cid:104) l(A(S); Zi) − l(A(S(i)); Zi)\n\nEZ′\n\ni\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:104)\n\nEZ′\n\ni\n\nl(A(S); Zi) − l(A(S(i)); Zi)\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13)l(A(S); Z) − l(A(S(i)); Z) (cid:13) (cid:13) (cid:13)q\n\n+\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13)l(A(S); Zi) − l(A(S(i)); Zi) (cid:13)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤ 2N γq,\n\nwhich implies\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nR(A(S)) − RS(A(S)) −\n\n1 N\n\nN (cid:88)\n\ni=1\n\ngi(S)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 2γq.\n\nAlso, gi(S) satisfies the conditions of Theorem 1 with βq = 2γq and it follows from the second bound of Theorem 1 that for all q ≥ 2,\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(gi(S) − E[gi(S) | Zi])\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 8κqγq⌈log2 N ⌉.\n\nCombining the above two yields\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nR(A(S)) − RS(A(S)) −\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[gi(S) | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≲ qγq log(N ).\n\nThe desired result follows by noting that\n\nE[gi(S) | Zi] = E[R(A(S′))] − E[l(A(S′); Zi) | Zi] = E[R(A(S))] − E[l(A(S′); Zi) | Zi].\n\nThis completes the proof.\n\nWith Lemma 6 in place, we are ready to prove the main result of Theorem 3.\n\nProof of Theorem 3. Consider any w∗ ∈ W ∗. It is standard to decompose and bound the excess risk as\n\nR(A(S)) − R∗\n\n=R(A(S)) − RS(A(S)) + RS(A(S)) − RS(w∗) + RS(w∗) − R∗ ≤∆opt + R(A(S)) − RS(A(S)) − (R∗ − RS(w∗))\n\n=∆opt + Γ(S) + E[R(A(S))] −\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(A(S′); Zi) | Zi] − (R∗ − RS(w∗)),\n\n(17)\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nwhere\n\nΓ(S) = R(A(S)) − RS(A(S)) − E[R(A(S))] +\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(A(S′); Zi) | Zi].\n\nSince we have the freedom to choose w∗, let us specify it in the above as w∗(S′) ∈ W ∗ which is the minimizer that satisfies the Bernstein condition in Assumption 1 associated with A(S′). Then, it follows from Eq. (17) that\n\nR(A(S)) − R∗ − ∆opt\n\n≤Γ(S) + E[R(A(S))] −\n\n1 N\n\nN (cid:88)\n\ni=1\n\nConsequently,\n\nE[l(A(S′); Zi) | Zi] − (R∗ − E[RS(w∗(S′)) | S]).\n\n∥R(A(S)) − R∗ − ∆opt∥q (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ ∥Γ(S)∥q +\n\nN (cid:88)\n\n1 N\n\ni=1\n\nE [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi] − (R∗ − E[R(A(S′))])\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\nζ1\n\n≲qγq log(N ) +\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\n(cid:123)(cid:122) T\n\nE [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi] − (R∗ − E[R(A(S′))])\n\n(18)\n\n,\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q (cid:125)\n\nwhere in “ζ1” we have applied Lemma 6 to obtain ∥Γ(S)∥q ≤ qγq log(N ), and the fact E[R(A(S))] = E[R(A(S′))].\n\nPart (a): To bound the term T , using Bernstein’s inequality for sum of independent bounded variables 2 together with the generalized Bernstein condition we can show (see the proof arguments of Klochkov & Zhivotovskiy (2021, Theorem 1.1) for the details) that\n\n(cid:114)\n\nT ≲\n\n(cid:114)\n\n≤\n\nqBE[R(A(S)) − R∗] N\n\n+\n\nqM N\n\n=\n\n(cid:114)\n\nqB(E[R(A(S)) − R∗ − ∆opt] + E[∆opt]) N\n\n+\n\nqM N\n\nqB(∥R(A(S)) − R∗ − ∆opt∥q + E[∆opt]) N\n\n+\n\nqM N\n\n,\n\nwhere the last inequality is due to Jensen’s inequality. Therefore, combining the above and Eq. (18) yields that for some universal constant C:\n\n∥R(A(S))−R∗−∆opt∥q ≤ C\n\nqγq log(N ) +\n\n(cid:32)\n\n(cid:114)\n\nqB(∥R(A(S)) − R∗ − ∆opt∥q + E[∆opt]) N\n\n+\n\nqM N\n\n(cid:33)\n\n.\n\nBy invoking Lemma 5 to the above self-bounding inequality with x = ∥R(A(S)) − R∗ − ∆opt∥q, a = C(qγq log(N ) + qM\n\nN , and c = E[∆opt] we immediately obtain that\n\nN ), b = qB\n\n∥R(A(S)) − R∗ − ∆opt∥q ≲ E[∆opt] + qγq log(N ) +\n\n(M + B)q N\n\n.\n\nThis gives the desired bound in part (a).\n\nPart (b): Under the given conditions in part (b), we can bound the term T in Eq. (18) as follows for q ≥ 2:\n\n2It is possible to relax the M -boundedness condition on the loss function l to its sub-Gaussian or subexponential counterparts by alternatively applying general Bernstein-type inequalities for sums of independent sub-Gaussian or sub-exponential random variables (Vershynin, 2018) in this part of proof. For the sake of simplicity and transparency of exposition, here we choose to work on the bounded loss while keeping in mind that the requirement is not essential.\n\n18\n\nζ1 ≤\n\n≤\n\n≤\n\nζ2 ≤\n\nζ3 ≤\n\nζ4 ≤\n\n≤\n\n(cid:13) (cid:13) (cid:13)\n\n√\n\n4\n\nκq\n\nN\n\n√\n\n4\n\nκq\n\nN\n\n√\n\n4\n\nκq\n\nN\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n4G\n\n2κq\n\n√ √\n\n√\n\nN κq\n\nN √\n\nκq\n\nN μ\n\n8G √\n\n8G √\n\nPublished as a conference paper at ICLR 2023\n\nT =\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi] − (R∗ − E[R(A(S′))])\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n√\n\n2\n\n2κq N\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(E [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi] − (R∗ − E[R(A(S′))]))2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q/2\n\nE2 [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi] + (R∗ − E[R(A(S′))])2(cid:13)\n\n(cid:13) (cid:13)q/2\n\n∥E2 [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi]∥q/2 + N E2[R(w∗(S′)) − R(A(S′))]\n\n∥E2 [G∥w∗(S′) − A(S′)∥ | Zi]∥q/2 + N E2 [G∥w∗(S′) − A(S′)∥]\n\n(cid:112)E [∥w∗(S′) − A(S′)∥2]\n\n(cid:114) 1 μ\n\nE [R(A(S′)) − R∗] =\n\n8G √\n\n√\n\nκq\n\nN μ\n\n(cid:113)\n\nE [R(A(S′)) − R∗ − ∆opt] + E[∆opt]\n\n(cid:113)\n\n∥R(A(S)) − R∗ − ∆opt∥q + E[∆opt]\n\nin “ζ2” we have used the Lipschitz-loss condiwhere in “ζ1” we have used Proposition 2, tion, in “ζ3” we have used (cid:13) (cid:13)q/2 = E2 [G∥w∗(S′) − A(S′)∥] ≤ G2E (cid:2)∥w∗(S′) − A(S′)∥2(cid:3), in “ζ4” we have used Assumption 2, and the last inequality is due to Jensen’s inequality. Then, plugging the above bound of term T into Eq. (18) yields that for some universal constant C:\n\n(cid:13)E2 [G∥w∗(S′) − A(S′)∥ | Zi](cid:13)\n\n∥R(A(S)) − R∗ − ∆opt∥q ≤ C\n\n(cid:18)\n\nqγq log(N ) + G\n\n(cid:114) q\n\nN μ\n\n(cid:113)\n\n∥R(A(S)) − R∗ − ∆opt∥q + E[∆opt]\n\n(cid:19)\n\n.\n\nInvoking Lemma 5 to the above inequality with x = ∥R(A(S)) − R∗ − ∆opt∥q, a = Cqγq log(N ), b = qG2\n\nμN , and c = E[∆opt] yields\n\n∥R(A(S)) − R∗ − ∆opt∥q ≲ E[∆opt] + qγq log(N ) +\n\nqG2 μN\n\n.\n\nThis gives the desired bound in part (b). The proof is completed.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nC PROOFS FOR SECTION 3\n\nC.1 PROOF OF LEMMA 1\n\nProof. Let us consider the following event about the restricted strong convexity of RS:\n\nE : RS is μk-strongly convex.\n\nLet Y = 1E be the indication random variable associated with E. Then by Assumption 4 we have P(Y = 1) ≥ 1 − δN . Suppose that E occurs such that Y = 1. Then,\n\nRS(w∗ 1\nN\n\n(cid:88)\n\nj̸=i\n\n=\n\nS(i)|J ) − RS(w∗ (cid:16)\n\nS|J )\n\nl(w∗\n\nS(i)|J ; Zj) − l(w∗\n\nS|J ; Zj)\n\n(cid:17)\n\n+\n\n(cid:16)\n\n1 N\n\nl(w∗\n\nS(i)|J ; Zi) − l(w∗\n\nS|J ; Zi)\n\n(cid:17)\n\n=RS(i) (w∗\n\nS(i)|J ) − RS(i) (w∗\n\nS|J ) +\n\n(cid:16)\n\n1 N\n(cid:12) (cid:12)l(w∗ (cid:12) (cid:13) (cid:13)w∗ (cid:13)\n\n−\n\n1 N\n2G N\n\n≤\n\n≤\n\nl(w∗\n\nS(i)|J ; Z ′\n\ni) − l(w∗\n\nS(i)|J ; Zi) − l(w∗\n\nS|J ; Zi)\n\nS(i)|J − w∗\n\nS|J\n\n(cid:13) (cid:13) (cid:13) ,\n\n1 N\nS|J ; Z ′ i) (cid:12) (cid:12) (cid:12) +\n\nl(w∗\n\nS(i)|J ; Zi) − l(w∗\n\nS|J ; Zi)\n\n(cid:17)\n\n(cid:16)\n\n(cid:17)\n\n1 N\n\n(cid:12) (cid:12)l(w∗ (cid:12)\n\nS(i)|J ; Z ′\n\ni) − l(w∗\n\n(cid:12) S|J ; Z ′ (cid:12) i) (cid:12)\n\nwhere we have used the optimality of w∗ of loss. Since E occurs by assumption, RS is μk-strongly convex. Since w∗ over the supporting set J, we have\n\nS(i)|J with respect to RS(i) (w) and the Lipschitz continuity S|J is optimal for RS(w)\n\nRS(w∗\n\nS|J ) + (cid:13) (cid:13) (cid:13)w∗ Lipschitz continuity of l we have that for any Z ∈ Z, the following holds conditioned on Y = 1:\n\nCombing the preceding two inequalities yields\n\nS(i)|J − w∗ (cid:13) (cid:13) ≤ 4G (cid:13)\n\nμkN . Consequently from the\n\nS(i)|J ) ≥ RS(w∗\n\nS(i)|J − w∗\n\nS|J\n\nS|J\n\n.\n\nμk 2\n\n(cid:13) (cid:13)w∗ (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:12) (cid:12)l(w∗ (cid:12)\n\nS(i)|J ; Z) − l(w∗\n\n(cid:12) (cid:12) (cid:12) ≤ G S|J ; Z)\n\n(cid:13) (cid:13)w∗ (cid:13)\n\nS(i)|J − w∗\n\nS|J\n\n(cid:13) (cid:13) (cid:13) ≤\n\n4G2 μkN\n\n.\n\nIn the complementary case of Y = 0, in view of Assumption 5, it always holds that\n\n|l(w∗\n\nS(i)|J ; Z) − l(w∗\n\nS|J ; Z)| ≤ G\n\n(cid:13) (cid:13)w∗ (cid:13)\n\nS(i)|J − w∗\n\nS|J\n\n(cid:13) (cid:13) (cid:13) ≤ 2GD.\n\n(19)\n\n(20)\n\nLet us consider qu :=\n\nlog\n\n(cid:17)\n\n(cid:16) 1 δN\n\nlog(N ) . By assumption qu ≥ 2. Then for 2 ≤ q ≤ qu, it can be verified that\n\nS(i)|J ; Z) − l(w∗\n\nS|J ; Z)\n\nq(cid:105)\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:104)(cid:12) (cid:12)l(w∗ (cid:12) (cid:104)(cid:12) (cid:12)l(w∗ (cid:12)\n\nS(i)|J ; Z) − l(w∗\n\n(cid:12) (cid:12) S|J ; Z) (cid:12)\n\nq\n\n(cid:105)\n\n| Y = 1\n\nS(i)|J ; Z) − l(w∗ (cid:19)q\n\nS|J ; Z)\n\n(cid:105)\n\n| Y = 0\n\nq\n\n(cid:12) (cid:12) (cid:12)\n\nE\n\n(cid:104)(cid:12) (cid:12)l(w∗ (cid:12) =P(Y = 1)E\n\n+ P(Y = 0)E\n\n(cid:19)q\n\n≤\n\n(cid:18) 4G2 μkN\n\n+ δN (2GD)q =\n\n(cid:18) 4G2 μkN\n\n+\n\n1 N qu\n\n(2GD)q ≤\n\n(cid:19)q\n\n(cid:18) 4G2 μkN\n\n+\n\n1\n\nN q (2GD)q.\n\nIt follows that for all 2 ≤ q ≤ qu\n\n(cid:13) (cid:13)l(w∗ (cid:13)\n\nS(i)|J ; Z) − l(w∗\n\nS|J ; Z)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤\n\n(cid:18)(cid:18) 4G2 μkN\n\n(cid:19)q\n\n+\n\n1\n\nN q (2GD)q\n\n(cid:19)1/q\n\n≤\n\n1 N\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n,\n\nwhere we have used aq + bq ≤ (a + b)q for a, b > 0 and q ≥ 2. For the complementary case q > qu, it is trivial to show that\n\n(cid:13) (cid:13)l(w∗ (cid:13)\n\nS(i)|J ; Z) − l(w∗\n\nS|J ; Z)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤ 2GD ≤\n\n2GD qu\n\nq.\n\nAssembling the preceding two bounds yields the desired Lq-stability bound.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nC.2 PROOF OF THEOREM 4\n\nLet us denote a+ = max{a, 0}. We need the following key result in our analysis, whose proof idea draws large inspiration from that of Theorem 3 with proper modifications for handing the challenges imposed by the combinatorial optimization nature of L0-ERM.\n\nLemma 7. Suppose that Assumptions 3, 4, 5 hold. Assume that log(1/δN ) δ ∈ (0, e−1), it holds with probability at least 1 − δ that\n\nlog(N ) ≥ 2. Then for any\n\nsup J⊆[d],|J|=k\n\nR(w∗\n\nS|J ) − R(w∗ ̄k)\n\n≲\n\nGD (cid:0)k log (cid:0) ed\n\n(cid:1) + log (cid:0) e k\nδ log(1/δN )\n\n(cid:1)(cid:1)2\n\nlog2(N )\n\n(cid:18)\n\n+\n\nlog(N )\n\n(cid:18) G2 μk\n\n(cid:19)\n\n+ GD\n\n+\n\nG2 μ\n\n(cid:19) k log (cid:0) ed\n\nk\n\n(cid:1)\n\n(cid:1) + log (cid:0) e N\n\nδ\n\n(cid:115) (cid:0)k log (cid:0) ed\n\nk\n\n+ G\n\n(cid:1) + log (cid:0) e\n\n(cid:1)(cid:1) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nδ N μ\n\n(cid:16)\n\n+\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n+\n\n(cid:20)(cid:16)\n\n+ E\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n.\n\n+\n\nProof. Given a fixed index set J ⊆ [d] with |J| = k, we can show that the following holds for any q ≥ 1:\n\nS|J ) − RS(w∗\n\nS|J ) + RS(w∗) − R(w∗)\n\n(cid:13) (cid:13) (cid:13)q\n\n(cid:13) (cid:13)R(w∗ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n=\n\nΓS|J + E[R(w∗\n\nS|J )] −\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(w∗\n\nS′|J ; Zi) | Zi] + RS(w∗) − R(w∗)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\nE[R(w∗\n\nS|J )] −\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(w∗\n\nS′|J ; Zi) | Zi] − R(w∗) + RS(w∗)\n\n(cid:123)(cid:122) T\n\n≤ (cid:13)\n\n(cid:13)ΓS|J\n\n(cid:13) (cid:13)q +\n\nwhere\n\n(21)\n\n,\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q (cid:125)\n\nΓS|J = R(w∗\n\nS|J ) − RS(w∗\n\nS|J ) − E[R(w∗\n\nS|J )] +\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(w∗\n\nS′|J ; Zi) | Zi].\n\nIn view of Lemma 1 we have that w∗\n\nS|J has Lq-stability by\n\nγq =\n\n1 N\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n+\n\n2GD log(N )q log(1/δN )\n\n.\n\nThen invoking Lemma 6 over the supporting set J yields\n\n(cid:13) (cid:13)ΓS|J\n\n(cid:13) (cid:13)q\n\n≲ qγq log(N ) =\n\nq log(N ) N\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n+\n\n2GD log2(N )q2 log(1/δN )\n\n.\n\n(22)\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nWe now bound the term T in Eq. (21) as follows:\n\nT =\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\n(cid:104)\n\nE\n\ni=1\n\nw∗; Zi) − l(w∗\n\nS′|J ; Zi) | Zi\n\n(cid:105)\n\n− (R(w∗) − E[R(w∗\n\nS′|J )])\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n√\n\n2\n\n2κq N\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:104)\n\n(cid:16)\n\nE\n\nl(w∗; Zi) − l(w∗\n\nS′|J ; Zi) | Zi\n\n(cid:105)\n\n− (R(w∗) − E[R(w∗\n\nS′|J )])\n\n(cid:17)2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q/2\n\n√\n\n4\n\nκq\n\nN\n\n√\n\n4\n\nκp\n\nN\n\n√\n\n4\n\nκp\n\nN\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1 (cid:115)\n\n4G\n\n2κp\n\n√ √\n\nN\n\n√\n\nκq\n\nN\n\n8G √\n\nζ1 ≤\n\n≤\n\n≤\n\nζ2 ≤\n\nζ3 ≤\n\nζ4 ≤\n\n(cid:104)\n\n(cid:104)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nE2\n\nl(w∗; Zi) − l(w∗\n\nS′|J ; Zi) | Zi\n\n(cid:16)\n\n(cid:105)\n\n+\n\nR(w∗) − E[R(w∗\n\nS′|J )]\n\n(cid:17)2(cid:13) (cid:13) (cid:13) (cid:13)q/2\n\n(cid:104)\n\n(cid:13) (cid:13) (cid:13)\n\nE2\n\nl(w∗; Zi) − l(w∗\n\nS′|J ; Zi) | Zi\n\n(cid:105)(cid:13) (cid:13) (cid:13)q/2\n\n(cid:104)\n\n+ N E2\n\nR(w∗) − R(w∗\n\nS′|J )\n\n(cid:105)\n\n(23)\n\n(cid:13) (cid:13) (cid:13)\n\nE2\n\n(cid:104)\n\nG\n\n(cid:13) (cid:13) (cid:13)w∗ − w∗\n\nS′|J\n\n(cid:13) (cid:13) (cid:13) | Zi\n\n(cid:105)(cid:13) (cid:13) (cid:13)q/2\n\n+ N E2\n\n(cid:104)\n\nG\n\n(cid:13) (cid:13) (cid:13)w∗ − w∗\n\nS′|J\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13)\n\nE\n\n(cid:20)(cid:13) (cid:13) (cid:13)w∗ − w∗\n\nS′|J\n\n2(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:114) 1 μ\n\n(cid:104)\n\nE\n\nR(w∗\n\nS′|J ) − R(w∗)\n\n(cid:105)\n\n=\n\n8G √\n\n√\n\n(cid:114)\n\nκq\n\nN μ\n\n(cid:104)\n\nE\n\nR(w∗\n\nS|J ) − R(w∗)\n\n(cid:105) ,\n\nwhere in “ζ1” we have used Proposition 2, sumption, in “ζ3” we have used\n\nG∥w∗ − w∗\n\nin “ζ2” we have used the Lipschitz-loss as- = E2 (cid:104) ≤\n\nG∥w∗ − w∗\n\n(cid:105) S′|J ∥\n\nS′|J ∥ | Zi\n\n(cid:13) E2 (cid:104) (cid:13) (cid:13)\n\n(cid:105)(cid:13) (cid:13) (cid:13)q/2\n\n∥w∗ − w∗\n\nG2E ging Eq. (22) and Eq. (23) into Eq. (21) yields that for any q ≥ 2,\n\n, in “ζ4” we have used the strong convexity condition in Assumption 4. Plug-\n\nS′|J ∥2(cid:105)\n\n(cid:13) (cid:13)R(w∗ (cid:13)\n\nS|J ) − R(w∗) − RS(w∗\n\nS|J ) + RS(w∗)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤\n\nq log(N ) N\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n+\n\n2q2GD log2(N ) log(1/δN )\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n+ 8G\n\nκqE\n\n(cid:104)\n\nR(w∗\n\n(cid:105) S|J ) − R(w∗) N μ\n\n(24)\n\n.\n\nNext we need to upper bound the factor E R(w∗ bound. To do so, let us consider q = 2 in Eq. (24). It follows from the optimality of w∗ and w∗ that\n\nin the second term of the above\n\nS|J\n\n(cid:104)\n\n(cid:105) S|J ) − R(w∗)\n\n(cid:104)\n\nE\n\n≤\n\n(cid:13) (cid:13)R(w∗ (cid:13)\n\nR(w∗\n\nS|J ) − R(w∗) − RS(w∗\n\nS|J ) − R(w∗) − RS(w∗\n\n(cid:105) S|J ) + RS(w∗) (cid:13) (cid:13) (cid:13)2\n\nS|J ) + RS(w∗)\n\nEq. (24) ≤\n\nlog(N ) N\n\n(cid:18) 8G2 μk\n\n(cid:19)\n\n+ 4GD\n\n+\n\n8GD log2(N ) log(1/δN )\n\n+ 8G\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:104)\n\n2κE\n\n(cid:105)\n\nR(w∗\n\nS|J ) − R(w∗) N μ\n\n≤\n\nlog(N ) N\n\n(cid:18) 8G2 μk\n\n(cid:19)\n\n+ 4GD\n\n+\n\n8GD log2(N ) log(1/δN )\n\n+\n\n(cid:104)\n\nE\n\nR(w∗\n\n(cid:105) S|J ) − R(w∗)\n\n2\n\n+\n\n64κG2 N μ\n\n,\n\nwhere in the first inequality we have used Cauchy-Schwarz inequality, and in the last inequality we 2 for any a, b, t > 0. Rearranging both sides of the above inequality have used the fact\n\n2t + bt\n\nab ≤ a\n\n√\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nwith simple algebra leads to\n\n(cid:104)\n\nE\n\nR(w∗\n\n(cid:105) S|J ) − R(w∗)\n\n≤2E\n\n(cid:104) RS(w∗\n\nS|J ) − RS(w∗)\n\n(cid:105)\n\n+\n\nlog(N ) N\n\n(cid:18) 16G2 μk\n\n(cid:19)\n\n+ 8GD\n\n+\n\n16GD log2(N ) log(1/δN )\n\n+\n\n128κG2 N μ\n\n+ 2(R(w∗\n\n ̄k) − R(w∗))\n\n(cid:19)\n\n+\n\n16GD log2(N ) log(1/δN )\n\n+\n\n128κG2 N μ\n\n(25)\n\nS|J ) − RS(w∗ (cid:18) 16G2 μk\n\n ̄k) + RS(w∗ (cid:19)\n\n(cid:105) ̄k) − RS(w∗) 16GD log2(N ) log(1/δN )\n\n+ 8GD\n\n+\n\n+\n\n128κG2 N μ\n\n(cid:104)\n\n=2E\n\nRS(w∗\n\n+\n\n=2E\n\n+\n\n≤2E\n\n+\n\nlog(N ) N\n\n(cid:104) RS(w∗\n\nlog(N ) N\n\n(cid:104) RS(w∗\n\nlog(N ) N\n\n(cid:105) S|J ) − RS(w∗ ̄k) (cid:18) 16G2 μk\n\n+ 8GD\n\nS|J ) − RS(w∗ (cid:18) 16G2 μk\n\n+ 8GD\n\n(cid:20)(cid:16)\n\n≤ 2E\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:124)\n\n+\n\nlog(N ) N\n\n(cid:124)\n\n(cid:123)(cid:122) T1\n\n(cid:18) 16G2 μk\n\n(cid:105)\n\n+ 2(R(w∗\n\n ̄k) − R(w∗))\n\nS, ̄k)\n\n(cid:19)\n\n+\n\n16GD log2(N ) log(1/δN )\n\n+\n\n128κG2 N μ\n\n(cid:21)\n\n(cid:125)\n\n+\n\n+ 2 (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\n(cid:124)\n\n(cid:123)(cid:122) T2\n\n(cid:125)\n\n(cid:19)\n\n+ 8GD\n\n+\n\n(cid:123)(cid:122) T3\n\n16GD log2(N ) log(1/δN )\n\n+\n\n128κG2 N μ\n\n.\n\n(cid:125)\n\nPlugging Eq. (25) into Eq. (24) yields that for any q ≥ 2\n\n(cid:13) (cid:13)R(w∗ (cid:13)\n\nS|J ) − R(w∗) − RS(w∗\n\nS|J ) + RS(w∗)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤\n\nq log(N ) N\n\n≤\n\nq log(N ) N\n\n(cid:18) 4G2 μk\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n+\n\n(cid:19)\n\n+ 2GD\n\n+\n\n2GDq2 log2(N ) log(1/δN )\n\n2GDq2 log2(N ) log(1/δN )\n\nκq(T1 + T2 + T3) N μ\n\nκq(T1 + T3) N μ\n\n(cid:115)\n\n+ 8G\n\nκqT2 N μ\n\n(cid:115)\n\n+ 8G\n\n(cid:115)\n\n+ 8G\n\n(cid:115)\n\n+ 8G\n\n(cid:19)\n\n+ 2GD\n\n(cid:19)\n\n+\n\n+ GD\n\n+\n\n2GDq2 log2(N ) log(1/δN ) GDq2 log2(N ) log(1/δN )\n\n+\n\nqG2 N μ\n\nκqT2 N μ (cid:20)(cid:16)\n\n+ E\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n+\n\n+ T1 + T3 +\n\n16κqG2 N μ\n\n(26)\n\nζ1 ≤\n\nq log(N ) N\n\n≲ q log(N ) N\n\n(cid:115)\n\n+ G\n\n(cid:18) 4G2 μk (cid:18) G2 μk q (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\n,\n\nN μ\n\nwhere in “ζ1” we have again used the fact to finally upper bound the desired sparse excess risk with respect to w∗\n\n2 for a, b, t > 0. Now we are in the position ̄k, which can be decomposed\n\nab ≤ a\n\n2t + bt\n\n√\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nin the following way:\n\nR(w∗\n\nS|J ) − R(w∗\n\n ̄k) =R(w∗\n\n+ RS(w∗\n\n≤R(w∗\n\nS|J ) − R(w∗) − RS(w∗\n\nS|J ) + RS(w∗) + RS(w∗ ̄k) − RS(w∗) + R(w∗) − R(w∗ ̄k) S|J ) + RS(w∗) + RS(w∗ ̄k) − RS(w∗) + R(w∗) − R(w∗ ̄k)\n\nS|J ) − R(w∗) − RS(w∗\n\n+ RS(w∗\n\nS|J ) − RS(w∗ ̄k)\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\nS|J ) − R(w∗) − RS(w∗\n\n≤ R(w∗ (cid:124)\n\nS|J ) + RS(w∗) (cid:125)\n\n(cid:16)\n\n+\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n+\n\n+ RS(w∗\n\n ̄k) − RS(w∗) + R(w∗) − R(w∗ ̄k) ,\n(cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) T ′ 1\n\n(cid:123)(cid:122) T ′ 2\n\n(27) where in the first inequality we have used the fact RS(w∗ ̄k). We are going to bound the above two terms T ′ 1, since Eq. (26) holds for all q ≥ 2, in view of the second part of Lemma 4 (with ql = 2 and qu = ∞) we can show that the following holds with probability at least 1 − δ\n\n2 respectively with high probability. Concerning T ′\n\nS, ̄k) ≤ RS(w∗\n\n1 and T ′\n\n2 for any δ ∈ (0, e−1):\n\nS|J ) − R(w∗) − RS(w∗\n\nS|J ) + RS(w∗)\n\nT ′\n\n1 =R(w∗ (cid:12) (cid:12)R(w∗ (cid:12) GD log2 (cid:0) e\n\n≤\n\n≲\n\nS|J ) − R(w∗) − RS(w∗ (cid:1) log2(N )\n\n(cid:18)\n\n+\n\n(cid:12) S|J ) + RS(w∗) (cid:12) (cid:12) (cid:18) G2 μk\n\nlog(N )\n\n+ GD\n\nδ log(1/δN ) (cid:115) log (cid:0) e\n\nδ\n\n+ G\n\n ̄k) − R(w∗)(cid:1)\n\n(cid:1) (cid:0)R(w∗ N μ\n\n(cid:20)(cid:16)\n\n+ E\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n.\n\n+\n\n(cid:19)\n\n+\n\nG2 μ\n\n(cid:19) log (cid:0) e\n\nδ\n\n(cid:1)\n\nN\n\n(28)\n\nRegarding the term T ′ as follows:\n\n2, similar to the argument of Eq. (23), we can bound its Lq-norm for any q ≥ 2\n\n∥T ′\n\n2∥q =\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(cid:0)l(w∗\n\n ̄k; Zi) − l(w∗; Zi)(cid:1) − (R(w∗\n\n ̄k) − R(w∗))\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n√\n\n2\n\n2κq N\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:0)l(w∗\n\n ̄k; Zi) − l(w∗; Zi) − (R(w∗\n\n ̄k) − R(w∗))(cid:1)2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q/2\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:0)l(w∗ (cid:13) (cid:13)\n\n ̄k; Zi) − l(w∗; Zi)(cid:1)2\n\n+ (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)2(cid:13)\n\n(cid:13) (cid:13)q/2\n\n(cid:13) (cid:0)l(w∗ (cid:13) (cid:13)\n\n ̄k; Zi) − l(w∗; Zi)(cid:1)2(cid:13)\n\n(cid:13) (cid:13)q/2\n\n+ N (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)2\n\n≤\n\n≤\n\n≤\n\n≤\n\n≤\n\n√\n\n4\n\nκq\n\nN\n\n√\n\n4\n\nκq\n\nN √\n\n4\n\n2κq N\n√\n\nκq\n\nN μ\n\n8G √\n\n(cid:113)\n\nN G2 (cid:13)\n\n(cid:13)w∗\n\n ̄k − w∗(cid:13)\n\n2 (cid:13)\n\n(cid:113)\n\nR(w∗\n\n ̄k) − R(w∗).\n\nBy invoking the second part of Lemma 4 with ql = 2 and qu = ∞ we can translate the above moment bound into the following exponential tail bound that holds with probability at least 1 − δ for any δ ∈ (0, e−1):\n\n2\n\n(cid:115)\n\nT ′\n\n2\n\n≲ G\n\nlog (cid:0) e\n\nδ\n\n ̄k) − R(w∗)(cid:1)\n\n(cid:1) (cid:0)R(w∗ N μ\n\n.\n\n(29)\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nBy plugging Eq. (28) and Eq. (29) into Eq. (27) and applying union probability argument we obtain that the following holds with probability at least 1 − δ for any δ ∈ (0, e−1):\n\nR(w∗\n\nS|J ) − R(w∗ ̄k) (cid:1) log2(N )\n\nGD log2 (cid:0) e\n\nδ log(1/δN )\n\n≲\n\n(cid:18)\n\n+\n\nlog(N )\n\n(cid:16)\n\n+\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n+\n\n+ E\n\n(cid:18) G2 μk (cid:20)(cid:16)\n\n(cid:19)\n\n+ GD\n\n+\n\nG2 μ\n\n(cid:19) log (cid:0) e\n\nδ\n\n(cid:1)\n\nN\n\n(cid:115)\n\n+ G\n\nlog (cid:0) e\n\nδ\n\n ̄k) − R(w∗)(cid:1)\n\n(cid:1) (cid:0)R(w∗ N μ\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n.\n\n+\n\nThis proves the desired sparse excess risk bound over the supporting set J.\n\nAs the final step, since there are at most (cid:0)d k\nwe can show that the following holds with probability 1 − δ for any δ ∈ (0, e−1):\n\n(cid:1) ≤ (cid:0) ed\n\n(cid:1)k\n\nk\n\ndifferent J with |J| = k, by union probability\n\nsup J⊆[d],|J|=k\n\nR(w∗\n\nS|J ) − R(w∗ ̄k)\n\n≲\n\nGD (cid:0)k log (cid:0) ed\n\n(cid:1) + log (cid:0) e k\nδ log(1/δN )\n\n(cid:1)(cid:1)2\n\nlog2(N )\n\n(cid:18)\n\n+\n\nlog(N )\n\n(cid:18) G2 μk\n\n(cid:19)\n\n+ GD\n\n+\n\nG2 μ\n\n(cid:19) k log (cid:0) ed\n\nk\n\n(cid:1)\n\n(cid:1) + log (cid:0) e N\n\nδ\n\n(cid:115) (cid:0)k log (cid:0) ed\n\nk\n\n+ G\n\n(cid:1) + log (cid:0) e\n\n(cid:1)(cid:1) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nδ N μ\n\n(cid:16)\n\n+\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n+\n\n(cid:20)(cid:16)\n\n+ E\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n.\n\n+\n\nThis completes the proof.\n\nNow we are in the position to prove the main result in Theorem 4.\n\nProof of Theorem 4. Let us consider ̃J = supp( ̃wS,k). By definition we have ̃wS,k = w∗ . Then invoking Lemma 7 yields that for any δ ∈ (0, e−1), the following holds with probability at least 1 − δ:\n\nS| ̃J\n\nR( ̃wS,k) − R(w∗ GD (cid:0)k log (cid:0) ed\n\n ̄k) = R(w∗ S| ̃J ) − R(w∗ ̄k) (cid:1)(cid:1)2 (cid:1) + log (cid:0) e log2(N ) k\nδ log(1/δN )\n\n+\n\n(cid:18)\n\nlog(N )\n\n(cid:18) G2 μk\n\n(cid:19)\n\n+ GD\n\n+\n\nG2 μ\n\n(cid:19) k log (cid:0) ed\n\nk\n\n(cid:1)\n\n(cid:1) + log (cid:0) e N\n\nδ\n\n(cid:115) (cid:0)k log (cid:0) ed\n\nk\n\n+ G\n\n(cid:1) + log (cid:0) e\n\n(cid:1)(cid:1) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nδ N μ\n\n(cid:16)\n\n+\n\nRS(w∗\n\nS| ̃J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n+\n\n(cid:20)(cid:16)\n\n+ E\n\nRS(w∗\n\nS| ̃J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n.\n\n+\n\nGD (cid:0)k log (cid:0) ed\n\n(cid:1) + log (cid:0) e k\nδ log(1/δN )\n\n(cid:1)(cid:1)2\n\nlog2(N )\n\n(cid:18)\n\n+\n\nlog(N )\n\n(cid:18) G2 μk\n\n(cid:19)\n\n+ GD\n\n+\n\nG2 μ\n\n(cid:19) k log (cid:0) ed\n\nk\n\n(cid:1)\n\n(cid:1) + log (cid:0) e N\n\nδ\n\n≲\n\n≲\n\n(cid:115) (cid:0)k log (cid:0) ed\n\nk\n\n+ G\n\n(cid:1) + log (cid:0) e\n\n(cid:1)(cid:1) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nδ N μ\n\n+ ∆ ̄k,opt + E (cid:2)∆ ̄k,opt\n\n(cid:3) .\n\nThis proves the desired sparse excess risk bound.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nD OTHER RELATED WORK\n\nUniform stability and exponential generalization. Stimulated by a recent landmark work of Hardt et al. (2016), there is a renewed interest in the use of uniform stability for deriving generalization bounds for various learning algorithms and paradigms including stochastic gradient descent (SGD) (Kuzborskij & Lampert, 2018; Charles & Papailiopoulos, 2018; Lei & Ying, 2020), stochastic model based optimization (Wang et al., 2017; Deng & Gao, 2021), optimization based meta learning (Zhou et al., 2019), and differential privacy stochastic optimization (Bassily et al., 2019; Feldman et al., 2020). Compared to other stability arguments, uniform stability is notorious for implying high-probability generalization bounds in addition to the traditional in-expectation bounds. Until very recently, the basic result of Bousquet & Elisseeff (2002), as expressed in Eq. (2), remains the best known exponential generalization bound for uniformly stable algorithms. Using some elegant proof techniques from adaptive data analysis, Feldman & Vondr ́ak (2018) managed to replace\n\nγu log( 1\n\nδ ), which leads to an improvement whenever γu ≳ 1\n\nthe first term in Eq. (2) by N . Soon after, a series of breakthrough results were obtained (Feldman & Vondr ́ak, 2019; Bousquet et al., 2020) using tighter concentration bounds for sum of random functions, which eventually improve the stability dependent rate to a near-optimal one γu log(N ) log (cid:0) 1 (cid:1) as shown in Eq. (4). Additionally under the generalized Bernstein condition, these state-of-the-art results lead to O( 1 N ) excess risk bounds for uniformly stable algorithms (Klochkov & Zhivotovskiy, 2021).\n\nδ\n\n(cid:113)\n\nNon-uniform stability and exponential generalization. More broadly for non-uniformly stable algorithms, exponential generalization bounds have also been shown to be possible under various weaker and distribution-dependent notions of stability. As an early work in this line, Kutin & Niyogi (2002) showed that under the so called “almost-everywhere” stability, which is a high-probability counterpart of uniform stability, generalization bounds that hold with overwhelming probability are still possible in view of certain modified McDiarmid’s inequality (Kutin, 2002). Later, Rakhlin et al. (2005) revisited the bounded-difference results of Kutin & Niyogi (2002) in a more straightforward manner by using a powerful moment extension of Efron-Stein inequality (Boucheron et al., 2005). Recently for general Lq-stable algorithms, the exponential leave-one-out generalization bounds were derived using moment or exponential extensions of Efron-Stein inequality, with applications found in ridge regression, k-nearest neighbor classification and k-folds cross-validation (Celisse & Guedj, 2016; Celisse & Mary-Huard, 2018; Abou-Moustafa & Szepesv ́ari, 2019). However, when it comes to the recent break-through bounds of Feldman & Vondr ́ak (2019); Bousquet et al. (2020), it is much less obvious how to easily extend these near-optimal bounds under the almost-everywhere stability via simply incorporating the low probability failure events into concentration inequality. This is actually in sharp contrast to what have been done by Feldman & Vondr ́ak (2019, Theorem 4.5) and Bassily et al. (2020, Theorem 2.1) for stochastic learning algorithms with uniform stability (over the randomness of data) holding with high probability over the internal randomness of algorithm. We refer the interested readers to Boucheron et al. (2013); Kontorovich (2014); Combes (2015); Warnke (2016); Maurer & Pontil (2021) for more results on concentration inequalities beyond boundeddifference conditions, which are fundamental for deriving exponential bounds for non-uniformly stable algorithms.\n\nGeneralization analysis of l0-estimators. We further briefly review some prior results on the generalization guarantees for statistical learning under cardinality constraints, which is the theme of the application part of our work. For the l0-ERM estimator as expressed in Eq. (8), provided that the solution is exactly known, a series of uniform excess risk bounds were derived over binary prediction classes (Chen & Lee, 2018; 2020) and bounded liner prediction classes (Foster & Syrgkanis, 2019). The exact solutions of l0-ERM, however, is computationally intractable in general high-dimensional cases due to the NP-hardness of problem. Therefore, it is more realistic and desirable to establish generalization bounds for approximate l0-estimators such as the IHT-style algorithms (Yuan et al., 2018; Garg & Khandekar, 2009; Li et al., 2016). Particularly for misspecified sparsity models, a set of sparse excess risk bounds with slow and fast rates were established through the lens of uniform stability theory under proper regularity conditions (Yuan & Li, 2022).\n\n26",
    "reference": "# Summary Of The Paper\n\nThis paper continues the recent line of work on high-probability generalization and excess risk bounds for stable algorithms. In this regard the paper:\n\n* Extends the nearly-optimal generalization bounds for uniformly stable algorithms to $L_q$-stable algorithms.\n* Similarly, it extends the excess risk bounds for uniformly stable algorithms under the Bernstein condition to $L_q$-stable algorithms under either the Bernstein or quadratic growth conditions.\n* It considers the inexact empirical risk minimization (ERM) with sparsity constraints problem. Under Lipschitzness, (high probability) strong convexity, and bounded domain conditions, it shows it is $L_q$-stable and it proves a new excess risk bound for that problem. The bounds are comparable to others in the literature with different assumptions.\n\n# Strength And Weaknesses\n\n**Strengths**\n\n* The generalization error bounds under $L_q$-stability are near-optimal, improving upon the current bounds, and matching the rate of their uniform-stability counterparts.\n* The excess risk bounds under $L_q$-stability match the rate of the uniform-stability counterparts under the Bernstein condition and add a new form under the quadratic growth condition.\n* The bounds on inexact ERM with sparsity constraints include algorithms like iterative hard thresholding (IHT) and match the rate of current bounds under potentially milder conditions. \n\n**Weaknesses**\n\n* There are some parts in the text and the proofs where the contributions and influence of previous work are not clear. It would be good to clarify these parts.\n\n  * The bound in Proposition 1 is *[Celisse and Guedj 2016, Corollary D1]*, which is a Corollary of *[Boucheron et al 2005, Theorem 2]*. Although the text preceding the proposition mentions the references, the wording is slightly confusing.\n  * The proof of Theorem 1 essentially follows *[Bousquet et al 2020, proof of Theorem 4]* with little variation to adapt it to $L_q$ stability. The same happens with the proof of Theorem 2 which follows essentially the combination of Lemma 7 and Theorem 4 of *[Bousquet et al 2020]*. Also with the proof of Lemma 5 which follows *[Klochkov and Zhivotovskiy 2021, proof of Lemma 3.1]* and the subsequent beginning and part (a) of the proof of Theorem 3 which follows *[Klochkov and Zhivotovskiy 2021, proof of Theorem 1.1]*.\n\n* In the exposition of the results, sometimes the assumptions are unclear, usually due to the notation $\\lesssim$ (which needs to be introduced in the notation section).\n\n  * The standard results from the uniform-stability literature usually require the loss is bounded by some constant, say $L$. This usually appears in (2), (3), (4), and (5), even under the use of $\\lesssim$ to make this assumption explicit. Having these constants would help understand better the improvement of using $L_q$-stability. If they are not there, it would be good to explicitly mention these bounds hold under this assumption prior to their introduction.\n  * Similarly, in (5) the Bernstein condition constant is usually present to make sure this dependence is clear.\n  * The same happens when presenting the contributions in Section 1.2. \n\n    * The generalization bound should have the constant $M_q$ demanding a finite moment $\\lVert \\ell(A(S);Z) \\rVert_q \\leq M_q$ or at least an explicit mention that this is required. \n\n    * The excess risk bound should either have the Bernstein constant $B$ and a bounded constant, or an explicit mention that these assumptions are required. Moreover, it should include the Lipschitz constant $G$ and the strong-convexity constant $\\mu$  or an explicit mention that these assumptions are required. \n\n* Algorithm 1 seems self-referential. Namely, note that $\\tilde{w}_{S,k} := \\argmin_{w \\in \\mathcal{W}, \\textnormal{supp}(w) \\subseteq \\tilde{J}} R_S(w)$ and $\\tilde{J} = \\textnormal{supp}(\\tilde{w}_{S,k})$. This made understanding the section a little more difficult.\n\n* Remark 10 seems a little unfair. It is comparing a bound for misspecified models *[Yuan and Li 2022, Theorem 3]* with a rate $\\mathcal{O}(1/N)$ with further assumptions with their $\\mathcal{O}(1/\\sqrt{N})$ bound on Theorem 4 with weaker assumptions. However, it does not consider the $\\mathcal{O}(1/\\sqrt{N})$ bound of the same paper *[Yuan and Li 2022, Theorem 1]* which has very similar assumptions (i.e. does not need a bounded parameter space nor a strongly convex population risk as this paper does but requires smoothness of the empirical risk) that can be similarly applicable.\n\n* The paper claims that it uses the developed theory to bound the excess risk of inexact ERM with sparsity constraints. This is not the case, the excess risk bounds are based on $L_q$-stability, yes, but don't use the theorems in Section 2. First, the assumptions are larger in that setting, and second, only some ideas of the proof of Theorem 3 are employed to prove Lemma 6 in the Appendix. \\\nI believe this should be clarified in the Abstract and the Introduction. Section 3 serves as a motivation for the importance of studying $L_q$-stability, but not for the usage of their generic bounds for $L_q$-stable algorithms, since they are not used there.\n\n* Could you please clarify or write explicitly in the text the final step in the proof of Theorem 3, part (a) and part (b)? That is, the step that follows the \"which implies that\" and \"which then implies\".\n\n* The exposition would be clearer if the excess risk bounds and concepts were also introduced in the problem setup instead of that later in Section 2.3. \n\n**References**\n\n*[Boucheron et al 2005]* Moment inequalities for functions of independent random variables. \\\n*[Celisse and Guedj 2016]* Stability revisited: new generalisation bounds for the leave-one-out. \\\n*[Bousquet et al 2020]* Sharper bounds for uniformly stable\nalgorithms. \\\n*[Klochkov and Zhivotovskiy 2021]* Stability and deviation optimal risk bounds with convergence rate o(1/n). \\\n*[Yuan and Li 2022]* Stability and risk bounds of iterative hard thresholding.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n* **Clarity**: The paper is clear and well-written.\n\n* **Quality**: The quality of the paper is good. \n\n* **Novelty**: The gross of the proofs to extend the uniform-stability generalization and excess risk bounds to $L_q$ stability is not novel. However, the results are, as well as the results and proofs of Section 3.\n\n* **Reproducibility**: \\\n*Theory*: I reproduced all the proofs except for the questions that I placed the authors in the weaknesses. \\\n*Experiments*: There are no experiments.\n\n# Summary Of The Review\n\nThis paper extends and builds upon the current best rate bounds on the generalization and excess risk bounds under the uniform-stability assumption to the milder $L_q$-stability assumption. \n\n* For the generalization bounds, instead of further needing a bounded loss, it needs bounded moments. \n* For the excess risk bounds, it either maintains the requirement of a bounded loss and the Bernstein condition or requires Lipschitness and the quadratic growth condition. These rates under the latter assumptions are novel as far as I know.\n\nThe paper needs to make clearer, though, the influences of the previous literature in their proofs, since some of them follow these papers closely. Similarly, it needs to further clarify some of the assumptions in the introductory text.\n\nThen, the paper finds bounds for the excess risk of inexact ERM with sparsity constraints (which are applicable to IHT). First, they prove these algorithms are $L_q$-stable and then they find specialized bounds that are comparable to those in the literature.\n\nThe paper needs to make clearer the comparison of their bounds and assumptions to those in the literature as well as the reason for Section 3, which does not use directly their bounds for generic $L_q$-stable algorithms. \n\nOverall, I believe the paper is well written and is a good contribution, so I recommend acceptance. Nonetheless, I still believe the comments in the weaknesses part should be addressed. \n\n**Minor comments and nitpicks that did not impact the score of the review**\n\n* Please, introduce the notation $\\lesssim$.\n\n* In the first paragraph of page 2, \"the Efron-Stein inequality\".\n\n* In the paragraph before Definition 1, \"introduce the ~~following~~ concept of uniform stability*.\n\n* In Remark 1, use either \"uniformly bounded\" or \"uniform boundedness\".\n\n* Before (7), please mention explicitly that this holds with probability $1-\\delta$.\n\n* Before the acronym HTP, give the name \"Hard Threshold Pursuit\".\n\n* For completeness, in page 7, after the displayed equation before Definition 3 mentioned that $\\tilde{w}_{S,k}$ is the output of Algorithm 1.\n\n* In Section 3, describe what you mean by support to disambiguate with the support of distributions since you are also working with random objects in this work.\n\n* In the last paragraph in Section 4, either \"a cardinality constraint\" or \"cardinality constraints\".\n\n* In the Conclusion you don't mention anything about your results on inexact ERM with sparsity constraints. Maybe you want to write a little about that.\n\n* Throughout the text you are using $a$ for the constant on the sub-exponential term and $b$ for the constant on the sub-Gaussian term. However, in the first bullet point of Lemma 4, you reversed them.\n\n* Maybe you want to separate Remark 11 into two remarks. \n\n* At the beginning of Theorem 1 you say \"we pad the set with extra...\", please write explicitly which set you mean.\n\n* In the last paragraph of page 15: \"Based on the triangle and Jensen's inequalities we can show that\"\n\n* In (16) I think it should be $4 \\sqrt{2 \\kappa Nq} M_q$ instead of $2 \\sqrt{2 \\kappa Nq} M_q$.\n\n* The first inequality of part (a) of Theorem 3 on page 18 and the last inequality of part (b) on page 18 are not due to Hölder's inequality since it only holds for exponents $1 < r < s < \\infty$ where the first inequalities are strict. However, it does hold due to Jensen's inequality.\n\n* There is a typo in the first equation in the proof of Lemma 1. In the $\\ell(w^* w_{S|J};Z_j)$ it should be $\\ell(w^*_{S|J};Z_j)$.\n\n* In (23) instead of inequality $\\leq$ it should be $\\lesssim$.\n\n* In the array of equations and inequalities after (29) in the second inequality I believe you forgot a factor of $\\sqrt{2}$ (it should be a 4 instead of $2 \\sqrt{2}$) and it carries over that part.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nFEDERATED LEARNING FROM SMALL DATASETS\n\nMichael Kamp Institute for AI in medicine (IKIM) University Hospital Essen, Essen Germany, and Ruhr-University Bochum, Bochum Germany, and Monash University, Melbourne, Australia michael.kamp@uk-essen.de\n\nJonas Fischer Harvard T.H. Chan School of Public Health Department of Biostatistics Boston, MA, United States jfischer@hsph.harvard.edu\n\nJilles Vreeken CISPA Helmholtz Center for Information Security Saarbr ̈ucken, Germany vreeken@cispa.de\n\nABSTRACT\n\nFederated learning allows multiple parties to collaboratively train a joint model without having to share any local data. It enables applications of machine learning in settings where data is inherently distributed and undisclosable, such as in the medical domain. Joint training is usually achieved by aggregating local models. When local datasets are small, locally trained models can vary greatly from a globally good model. Bad local models can arbitrarily deteriorate the aggregate model quality, causing federating learning to fail in these settings. We propose a novel approach that avoids this problem by interleaving model aggregation and permutation steps. During a permutation step we redistribute local models across clients through the server, while preserving data privacy, to allow each local model to train on a daisy chain of local datasets. This enables successful training in data-sparse domains. Combined with model aggregation, this approach enables effective learning even if the local datasets are extremely small, while retaining the privacy benefits of federated learning.\n\n1\n\nINTRODUCTION\n\nHow can we learn high quality models when data is inherently distributed across sites and cannot be shared or pooled? In federated learning, the solution is to iteratively train models locally at each site and share these models with the server to be aggregated to a global model. As only models are shared, data usually remains undisclosed. This process, however, requires sufficient data to be available at each site in order for the locally trained models to achieve a minimum quality—even a single bad model can render aggregation arbitrarily bad (Shamir and Srebro, 2014). In many relevant applications this requirement is not met: In healthcare settings we often have as little as a few dozens of samples (Granlund et al., 2020; Su et al., 2021; Painter et al., 2020). Also in domains where deep learning is generally regarded as highly successful, such as natural language processing and object detection, applications often suffer from a lack of data (Liu et al., 2020; Kang et al., 2019).\n\nTo tackle this problem, we propose a new building block called daisy-chaining for federated learning in which models are trained on one local dataset after another, much like a daisy chain. In a nutshell, at each client a model is trained locally, sent to the server, and then—instead of aggregating local models—sent to a random other client as is (see Fig. 1). This way, each local model is exposed to a daisy chain of clients and their local datasets. This allows us to learn from small, distributed datasets simply by consecutively training the model with the data available at each site. Daisy-chaining alone, however, violates privacy, since a client can infer from a model upon the data of the client it received it from (Shokri et al., 2017). Moreover, performing daisy-chaining naively would lead to overfitting which can cause learning to diverge (Haddadpour and Mahdavi, 2019). In this paper, we propose to combine daisy-chaining of local datasets with aggregation of models, both orchestrated by the server, and term this method federated daisy-chaining (FEDDC).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Federated learning settings. A standard federated learning setting with training of local models at clients (middle) with aggregation phases where models are communicated to the server, aggregated, and sent back to each client (left). We propose to add daisy chaining (right), where local models are sent to the server and then redistributed to a random permutation of clients as is.\n\nWe show that our simple, yet effective approach maintains privacy of local datasets, while it provably converges and guarantees improvement of model quality in convex problems with a suitable aggregation method. Formally, we show convergence for FEDDC on non-convex problems. We then show for convex problems that FEDDC succeeds on small datasets where standard federated learning fails. For that, we analyze FEDDC combined with aggregation via the Radon point from a PAC-learning perspective. We substantiate this theoretical analysis for convex problems by showing that FEDDC in practice matches the accuracy of a model trained on the full data of the SUSY binary classification dataset with only 2 samples per client, outperforming standard federated learning by a wide margin. For non-convex settings, we provide an extensive empirical evaluation, showing that FEDDC outperforms naive daisy-chaining, vanilla federated learning FEDAVG (McMahan et al., 2017), FEDPROX (Li et al., 2020a), FEDADAGRAD, FEDADAM, and FEDYOGI (Reddi et al., 2020) on low-sample CIFAR10 (Krizhevsky, 2009), including non-iid settings, and, more importantly, on two real-world medical imaging datasets. Not only does FEDDC provide a wide margin of improvement over existing federated methods, but it comes close to the performance of a gold-standard (centralized) neural network of the same architecture trained on the pooled data. To achieve that, it requires a small communication overhead compared to standard federated learning for the additional daisy-chaining rounds. As often found in healthcare, we consider a cross-SILO scenario where such small communication overhead is negligible. Moreover we show that with equal communication, standard federated averaging still underperforms in our considered settings.\n\nIn summary, our contributions are (i) FEDDC, a novel approach to federated learning from small datasets via a combination of model permutations across clients and aggregation, (ii) a formal proof of convergence for FEDDC, (iii) a theoretical guarantee that FEDDC improves models in terms of (cid:15), δ-guarantees which standard federated learning can not, (iv) a discussion of the privacy aspects and mitigations suitable for FEDDC, including an empirical evaluation of differentially private FEDDC, and (v) an extensive set of experiments showing that FEDDC substantially improves model quality on small datasets compared to standard federated learning approaches.\n\n2 RELATED WORK\n\nLearning from small datasets is a well studied problem in machine learning. In the literature, we find both general solutions, such as using simpler models and transfer learning (Torrey and Shavlik, 2010), and more specialized ones, such as data augmentation (Ibrahim et al., 2021) and few-shot learning (Vinyals et al., 2016; Prabhu et al., 2019). In our scenario overall data is abundant, but the problem is that data is distributed into small local datasets at each site, which we are not allowed to pool. Hao et al. (2021) propose local data augmentation for federated learning, but their method requires a sufficient quality of the local model for augmentation which is the opposite of the scenario we are considering. Huang et al. (2021) provide generalization bounds for federated averaging via the NTK-framework, but requires one-layer infinite-width NNs and infinitesimal learning rates.\n\nFederated learning and its variants have been shown to learn from incomplete local data sources, e.g., non-iid label distributions (Li et al., 2020a; Wang et al., 2019) and differing feature distributions (Li et al., 2020b; Reisizadeh et al., 2020a), but fail in case of large gradient diversity (Haddadpour and Mahdavi, 2019) and strongly dissimilar label distribution (Marfoq et al., 2021). For small\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ndatasets, local empirical distributions may vary greatly from the global distribution: the difference of empirical to true distribution decreases exponentially with the sample size (e.g., according to the Dvoretzky–Kiefer–Wolfowitz inequality), but for small samples the difference can be substantial, in particular if the distribution differs from a Normal distribution (Kwak and Kim, 2017). Shamir and Srebro (2014) have shown the adverse effect of bad local models on averaging, proving that even due to a single bad model averaging can be arbitrarily bad.\n\nA different approach to dealing with biased local data is by learning personalized models at each client. Such personalized FL (Li et al., 2021) can reduce sample complexity, e.g., by using shared representations (Collins et al., 2021) for client-specific models, e.g., in the medical domain (Yang et al., 2021), or by training sample-efficient personalized Bayesian methods (Achituve et al., 2021). It is not applicable, however, to settings where you are not allowed to learn the biases or batch effects of local clients, e.g., in many medical applications where this would expose sensitive client information. Kiss and Horvath (2021) propose a decentralized and communication-efficient variant of federated learning that migrates models over a decentralized network, storing incoming models locally at each client until sufficiently many models are collected on each client for an averaging step, similar to Gossip federated learing (Jelasity et al., 2005). The variant without averaging is similar to simple daisy-chaining which we compare to in Section 7. FEDDC is compatible with any aggregation operator, including the Radon machine (Kamp et al., 2017), the geometric median (Pillutla et al., 2022), or neuron-clustering (Yurochkin et al., 2019), and can be straightforwardly combined with approaches to improve communication-efficiency, such as dynamic averaging (Kamp et al., 2018), and model quantization (Reisizadeh et al., 2020b). We combine FEDDC with averaging, the Radon machine, and FedProx (Li et al., 2020a) in Sec. 7.\n\n3 PRELIMINARIES\n\ni\n\n(x,y)\n\nWe assume iterative learning algorithms (cf. Chp. 2.1.4 Kamp, 2019) A : X × Y × H → H that update a model h ∈ H using a dataset D ⊂ X × Y from an input space X and output space Y, i.e., ht+1 = A(D, ht). Given a set of m ∈ N clients with local datasets D1, . . . , Dm ⊂ X × Y drawn iid from a data distribution D and a loss function (cid:96) : Y × Y → R, the goal is to find a single model h∗ ∈ H that minimizes the risk ε(h) = E [(cid:96)(h(x), y)]. In centralized learning, datasets are pooled as D = (cid:83) [m] Di and A is applied to D until convergence. Note that applying A on D can be the application to any random subset, e.g., as in mini-batch training, and convergence is measured in terms of low training loss, small gradient, or small deviation from previous iterate. In standard federated learning (McMahan et al., 2017), A is applied in parallel for b ∈ N rounds on each client locally to produce local models h1, . . . , hm. These models are then centralized and aggregated using an aggregation operator agg : Hm → H, i.e., h = agg(h1, . . . , hm). The aggregated model h is then redistributed to local clients which perform another b rounds of training using h as a starting point. This is iterated until convergence of h. When aggregating by averaging, this method is known as federated averaging (FEDAVG). Next, we describe FEDDC.\n\n∼D\n\n∈\n\n4 FEDERATED DAISY-CHAINING\n\nWe propose federated daisy chaining as an extension to federated learning in a setup with m clients and one designated sever.1 We provide the pseudocode of our approach as Algorithm 1.\n\nThe client: Each client trains its local model in each round on local data (line 4), and sends its model to the server every b rounds for aggregation, where b is the aggregation period, and every d rounds for daisy chaining, where d is the daisy-chaining period (line 6). This re-distribution of models results in each individual model conceptually following a daisy chain of clients, training on each local dataset. Such a daisy chain is interrupted by each aggregation round.\n\nThe server: Upon receiving models, in a daisy-chaining round (line 9) the server draws a random permutation π of clients (line 10) and re-distributes the model of client i to client π(i) (line 11), while in an aggregation round (line 12), the server instead aggregates all local models and re-distributes the aggregate to all clients (line 13-14).\n\n1This star-topology can be extended to hierarchical networks in a straightforward manner. Federated learning\n\ncan also be performed in a decentralized network via gossip algorithms (Jelasity et al., 2005).\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1: Federated Daisy-Chaining FEDDC Input: daisy-chaining period d, aggregation period b, learning algorithm A, aggregation operator\n\nagg, m clients with local datasets D1, . . . , Dm, total number of rounds T\n\nOutput: final model aggregate hT 0, . . . , hm\n\n1 initialize local models h1 2 Locally at client i at time t do\n\n0\n\n3\n\n4\n\n5\n\nsample S from Di hi 1) if t mod d = d − 1 or t mod b = b − 1 then\n\nt ← A(S, hi\n\n−\n\nt\n\n6\n\nt to server\n\nsend hi receive new hi 8 At server at time t do\n\n7\n\nt from server\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\nif t mod d = d − 1 then\n\ndraw permutation π of [1,m] at random for all i ∈ [m] send model hi\n\nt to client π(i)\n\nelse if t mod b = b − 1 then\n\nht ← agg(h1 send ht to all clients\n\nt , . . . , hm t )\n\n// receives either aggregate ht or some hj\n\nt\n\n// daisy chaining\n\n// aggregation\n\nCommunication complexity: Note that we consider cross-SILO settings, such as healthcare, were communication is not a bottleneck and, hence, restrict ourselves to a brief discussion in the interest of space. Communication between clients and server happens in O( T b ) many rounds, where T is the overall number of rounds. Since FEDDC communicates every dth and bth round, the amount of communication rounds is similar to FEDAVG with averaging period bFedAvg = min{d, b}. That is, FEDDC increases communication over FEDAVG by a constant factor depending on the setting of b and d. The amount of communication per communication round is linear in the number of clients and model size, similar to federated averaging. We investigate the performance of FEDAVG provided with the same communication capacity as FEDDC in our experiments and in App. A.3.6.\n\nd + T\n\n5 THEORETICAL GUARANTEES\n\nIn this section, we formally show that FEDDC converges for averaging. We, further, provide theoretical bounds on the model quality in convex settings, showing that FEDDC has favorable generalization error in low sample settings compared to standard federated learning. More formally, we first show that under standard assumptions on the empirical risk, it follows from a result of Yu et al. (2019) that FEDDC converges when using averaging as aggregation and SGD for learning—a standard setting in, e.g., federated learning of neural networks. We provide all proofs in the appendix. Corollary 1. Let the empirical risks E i Di (cid:96)(hi(x), y) at each client i ∈ [m] be L-smooth with σ2-bounded gradient variance and G2-bounded second moments, then FEDDC with mT ), where T is the number of local updates. averaging and SGD has a convergence rate of O(1/\n\nemp(h) = (cid:80) √\n\n(x,y)\n\n∈\n\nSince model quality in terms of generalization error does not necessarily depend on convergence of training (Haddadpour and Mahdavi, 2019; Kamp et al., 2018), we additionally analyze model quality in terms of probabilistic worst-case guarantees on the generalization error (Shalev-Shwartz and Ben-David, 2014). The average of local models can yield as bad a generalization error as the worst local model, hence, using averaging as aggregation scheme in standard federated learning can yield arbitrarily bad results (cf. Shamir and Srebro, 2014). As the probability of bad local models starkly increases with smaller sample sizes, this trivial bound often carries over to our considered practical settings. The Radon machine (Kamp et al., 2017) is a federated learning approach that overcomes these issues for a wide range of learning algorithms and allows us to analyze (non-trivial) quality bounds of aggregated models under the assumption of convexity. Next, we show that FEDDC can improve model quality for small local datasets where standard federated learning fails to do so.\n\nA Radon point (Radon, 1921) of a set of points S from a space X is—similar to the geometric median—a point in the convex hull of S with a high centrality (i.e., a Tukey depth (Tukey, 1975;\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n(a) FEDDC with Radon point with d = 1, b = 50.\n\n(b) Federated learning with Radon point with b = 1.\n\n(c) Federated learning with Radon point with b = 50.\n\nFigure 2: Results on SUSY. We visualize results in terms of train (green) and test error (orange) for (a) FEDDC (d = 1, b = 50) and standard federated learning using Radon points for aggregation with (b) b = 1, i.e., the same amount of communication as FEDDC, and (c) b = 50, i.e., the same aggregation period as FEDDC. The network has 441 clients with 2 data points per client. The performance of a central model trained on all data is indicated by the dashed line.\n\nGilad-Bachrach et al., 2004) of at least 2). For a Radon point to exist, S ⊂ X has to have a minimum size r ∈ N called the Radon number of X . For X ⊆ Rd the radon number is d + 2. Here, the set of points S are the local models, or more precisely their parameter vectors. We make the following standard assumption (Von Luxburg and Sch ̈olkopf, 2011) on the local learning algorithm A. Assumption 2 (((cid:15), δ)-guarantees). The learning algorithm A applied on a dataset drawn iid from D of size n ≥ n0 ∈ N produces a model h ∈ H s.t. with probability δ ∈ (0, 1] it holds for (cid:15) > 0 that P (ε(h) > (cid:15)) < δ. The sample size n0 is monotonically decreasing in δ and (cid:15) (note that typically n0 is a polynomial in (cid:15)−\n\n1 and log(δ−\n\n1)).\n\nHere ε(h) is the risk defined in Sec. 3. Now let r ∈ N be the Radon number of H, A be a learning algorithm as in assumption 2, and risk ε be convex. Assume m ≥ rh many clients with h ∈ N. For (cid:15) > 0, δ ∈ (0, 1] assume local datasets D1, . . . , Dm of size larger than n0((cid:15), δ) drawn iid from D, and h1, . . . , hm be local models trained on them using A. Let rh be the iterated Radon point (Clarkson et al., 1996) with h iterations computed on the local models (for details, see App. A.2). Then it follows from Theorem 3 in Kamp et al. (2017) that for all i ∈ [m] it holds that\n\nP (ε(rh) > (cid:15)) ≤ (r P (ε(hi) > (cid:15)))2h\n\n(1)\n\nwhere the probability is over the random draws of local datasets. That is, the probability that the aggregate rh is bad is doubly-exponentially smaller than the probability that a local model is bad. Note that in PAC-learning, the error bound and the probability of the bound to hold are typically linked, so that improving one can be translated to improving the other (Von Luxburg and Sch ̈olkopf, 2011). Eq. 1 implies that the iterated Radon point only improves the guarantee on the confidence compared to that for local models if δ < r− < 1 only holds for rδ < 1. Consequently, local models need to achieve a minimum quality for the federated learning system to improve model quality. Corollary 3. Let H be a model space with Radon number r ∈ N, ε a convex risk, and A a learning algorithm with sample size n0((cid:15), δ). Given (cid:15) > 0 and any h ∈ N, if local datasets D1, . . . , Dm with m ≥ rh are smaller than n0((cid:15), r− 1), then federated learning using the Radon point does not improve model quality in terms of ((cid:15), δ)-guarantees.\n\n1, i.e. P (ε(rh) > (cid:15)) ≤ (r P (ε(hi) > (cid:15)))2h\n\n< (rδ)2h\n\nIn other words, when using aggregation by Radon points alone, an improvement in terms of ((cid:15), δ)- 1, the guarantees is strongly dependent on large enough local datasets. Furthermore, given δ > r− guarantee can become arbitrarily bad by increasing the number of aggregation rounds.\n\nFederated Daisy-Chaining as given in Alg. 1 permutes local models at random, which is in theory equivalent to permuting local datasets. Since the permutation is drawn at random, the amount of permutation rounds T necessary for each model to observe a minimum number of distinct datasets k with probability 1 − ρ can be given with high probability via a variation of the coupon collector problem as T ≥ d m k), where Hm is the m-th harmonic number—see Lm. 5 in\n\n(Hm − Hm\n\n1 m\n\nρ\n\n−\n\n5\n\n01002003004005000.40.50.60.70.80.91centralized(test)roundsaccuracytraintest01002003004005000.40.50.60.70.80.91rounds01002003004005000.40.50.60.70.80.91roundsPublished as a conference paper at ICLR 2023\n\n1\n\n−\n\nm (Hm − Hm √\n\nApp. A.5 for details. It follows that when we perform daisy-chaining with m clients and local datasets of size n for at least dmρ− k) rounds, then each local model will with probability at least 1 − ρ be trained on at least kn distinct samples. For an (cid:15), δ-guarantee, we thus need to set b large enough so that kn ≥ n0((cid:15), δ. This way, the failure probability is the product of not all clients observing k distinct datasets and the model having a risk larger than (cid:15), √\nwhich is Proposition 4. Let H be a model space with Radon number r ∈ N, ε a convex risk , and A a learning 1) and any h ∈ N, and local datasets algorithm with sample size n0((cid:15), δ). Given (cid:15) > 0, δ ∈ (0, r− D1, . . . , Dm of size n ∈ N with m ≥ rh, then Alg. 1 using the Radon point with aggr. period\n\nδ) with probability at least 1 −\n\nδ = δ.\n\n√\n\n√\n\nδ\n\n(cid:16)\n\nb ≥ d\n\nm δ 1\n\n2m\n\nHm − Hm\n\n−\n\n(cid:17)\n\n(cid:100)n−1n0((cid:15),√δ)(cid:101)\n\n(2)\n\nimproves model quality in terms of ((cid:15), δ)-guarantees.\n\nThis result implies that if enough daisy-chaining rounds are performed in-between aggregation rounds, federated learning via the iterated Radon point improves model quality in terms of ((cid:15), δ)-guarantees: the resulting model has generalization error smaller than (cid:15) with probability at least 1 − δ. Note that the aggregation period cannot be arbitrarily increased without harming convergence. To illustrate the interplay between these variables, we provide a numerical analysis of Prop. 4 in App. A.5.1.\n\nThis theoretical result is also evident in practice, as we show in Fig. 2. There, we compare FEDDC with standard federated learning and equip both with the iterated Radon point on the SUSY binary classification dataset (Baldi et al., 2014). We train a linear model on 441 clients with only 2 samples per client. After 500 rounds FEDDC daisy-chaining every round (d = 1) and aggregating every fifty rounds (b = 50) reached the test accuracy of a gold-standard model that has been trained on the centralized dataset (ACC=0.77). Standard federated learning with the same communication complexity using b = 1 is outperformed by a large margin (ACC=0.68). We additionally provide results of standard federated learning with b = 50 (ACC=0.64), which shows that while the aggregated models perform reasonable, the standard approach heavily overfits on local datasets if not pulled to a global average in every round. More details on this experiment can be found in App. A.3.2. In Sec. 7 we show that the empirical results for averaging as aggregation operator are similar to those for the Radon machine. First, we discuss the privacy-aspects of FEDDC.\n\n6 DATA PRIVACY\n\nA major advantage of federated over centralized learning is that local data remains undisclosed to anyone but the local client, only model parameters are exchanged. This provides a natural benefit to data privacy, which is the main concern in applications such as healthcare. However, an attacker can make inferences about local data from model parameters (Ma et al., 2020) and model updates or gradients (Zhu and Han, 2020). In the daisy-chaining rounds of FEDDC clients receive a model that was directly trained on the local data of another client, instead of a model aggregate, potentially facilitating membership inference attacks (Shokri et al., 2017)—reconstruction attacks (Zhu and Han, 2020) remain difficult because model updates cannot be inferred since the server randomly permutes the order of clients in daisy-chaining rounds.\n\nFigure 3: Differential privacy results. Comparison of FEDDC (top solid line) to FEDDC with clipped parameter updates and Gaussian noise (dashed lines) on CIFAR10 with 250 clients.\n\nShould a malicious client obtain model updates through additional attacks, a common defense is applying appropriate clipping and noise before sending models. This guarantees (cid:15), δ-differential privacy for local data (Wei et al., 2020) at the cost of a slight-to-moderate loss in model quality. This technique is also proven to defend against backdoor and poisoning attacks (Sun et al., 2019). Moreover, FEDDC is compatible with standard defenses against such attacks, such as noisy or robust aggregation (Liu et al., 2022)—FEDDC with the Radon machine is an example of robust aggregation. We illustrate the effectiveness of FEDDC\n\n6\n\n05·10410·10415·10420·10400.20.40.60.8roundsaccuracyFEDDCDP-FEDDC(S=2,σ=0.01)DP-FEDDC(S=2,σ=0.02)DP-FEDDC(S=4,σ=0.05)Published as a conference paper at ICLR 2023\n\n(a) FEDDC with d = 1, b = 200.\n\n(b) FEDAVG with b = 1.\n\n(c) FEDAVG with b = 200.\n\nFigure 4: Synthetic data results. Comparison of FEDDC (a), FEDAVG with same communication (b) and same averaging period (c) for training fully connected NNs on synthetic data. We report mean and confidence accuracy per client in color and accuracy of central learning as dashed black line.\n\nwith differential privacy in the following experiment. We train a small ResNet on 250 clients using FEDDC with d = 2 and b = 10, postponing the details on the experimental setup to App. A.1.1 and A.1.2. Differential privacy is achieved by clipping local model updates and adding Gaussian noise as proposed by Geyer et al. (2017). The results as shown in Figure 3 indicate that the standard trade-off between model quality and privacy holds for FEDDC as well. Moreover, for mild privacy settings the model quality does not decrease. That is, FEDDC is able to robustly predict even under differential privacy. We provide an extended discussion on the privacy aspects of FEDDC in App. A.7.\n\n7 EXPERIMENTS ON DEEP LEARNING\n\nOur approach FEDDC, both provably and empirically, improves model quality when using Radon points as aggregation which, however, require convex problems. For non-convex problems, in particular deep learning, averaging is the state-of-the-art aggregation operator. We, hence, evaluate FEDDC with averaging against the state of the art in federated learning on synthetic and real world data using neural networks. As baselines, we consider federated averaging (FEDAVG) (McMahan et al., 2017) with optimal communication, FEDAVG with equal communication as FEDDC, and simple daisy-chaining without aggregation. We further consider the 4 state-of-the-art methods FEDPROX (Li et al., 2020a), FEDADAGRAD, FEDYOGI, and FEDADAM (Reddi et al., 2020). As datasets we consider a synthetic classification dataset, image classification in CIFAR10 (Krizhevsky, 2009), and two real medical datasets: MRI scans for brain tumors,2 and chest X-rays for pneumonia3. We provide additional results on MNIST in App. A.3.8. Details on the experimental setup are in App. A.1.1,A.1.2, code is publicly available at https://github.com/kampmichael/FedDC.\n\nSynthetic Data: We first investigate the potential of FEDDC on a synthetic binary classification dataset generated by the sklearn (Pedregosa et al., 2011) make_classification function with 100 features. On this dataset, we train a simple fully connected neural network with 3 hidden layers on m = 50 clients with n = 10 samples per client. We compare FEDDC with daisy-chaining period d = 1 and aggregation period b = 200 to FEDAVG with the same amount of communication b = 1 and the same averaging period b = 200. The results presented in Fig. 4 show that FEDDC achieves a test accuracy of 0.89. This is comparable to centralized training on all data which achieves a test accuracy of 0.88. It substantially outperforms both FEDAVG setups, which result in an accuracy of 0.80 and 0.76. Investigating the training of local models between aggreation periods reveals that the main issue of FEDAVG is overfitting of local clients, where FEDAVG train accuracy reaches 1.0 quickly after each averaging step. With these promising results on vanilla neural networks, we next turn to real-world image classification problems typically solved with CNNs.\n\nCIFAR10: As a first challenge for image classification, we consider the well-known CIFAR10 image benchmark. We first investigate the effect of the aggregation period b on FEDDC and FEDAVG, separately optimizing for an optimal period for both methods. We use a setting of 250 clients with\n\n2kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection 3kaggle.com/praveengovi/coronahack-chest-xraydataset\n\n7\n\n02004006008001,00000.20.40.60.81centralized(test)roundsaccuracytraintest02004006008001,00000.20.40.60.81rounds02004006008001,00000.20.40.60.81roundsPublished as a conference paper at ICLR 2023\n\na small version of ResNet, and 64 local samples each, which simulates our small sample setting, drawn at random without replacement (details in App. A.1.2). We report the results in Figure 5 and set the period for FEDDC to b = 10, and consider federated averaging with periods of both b = 1 (equivalent communication to FEDDC with d = 1, b = 10) and b = 10 (less communication than FEDDC by a factor of 10) for all subsequent experiments.\n\nNext, we consider a subset of 9600 samples spread across 150 clients (i.e. 64 samples per client), which corresponds to our small sample setting. Now, each client is equipped with a larger, untrained ResNet18.4 Note that the combined amount of examples is only one fifth of the original training data, hence we cannot expect typical CIFAR10 performance. To obtain a gold standard for comparison, we run centralized learning CENTRAL, separately optimizing its hyperparameters, yielding an accuracy of around 0.65. All results are reported in Table 1, where we report FEDAVG with b = 1 and b = 10, as these were the best performing settings and b = 1 corresponds to equal amounts of communication as FEDDC. We use a daisy chaining period of d = 1 for FEDDC throughout all experiments for consistency, and provide results for larger daisy chaining periods in App. A.3.5, which, depending on the data distribution, might be favorable. We observe that FEDDC achieves substantially higher accuracy over the baseline set by federated averaging. In App. A.3.7 we show that this holds also for client subsampling. Upon further inspection, we see that FEDAVG drastically overfits, achieving training accuracies of 0.97 (App. A.3.1), a similar trend as on the synthetic data before. Daisy-chaining alone, apart from privacy issues, also performs worse than FEDDC. Intriguingly, also the state of the art shows similar trends. FEDPROX, run with optimal b = 10 and μ = 0.1, only achieves an accuracy of 0.51 and FEDADAGRAD, FEDYOGI, and FEDADAM show even worse performance of around 0.22, 0.31, and 0.34, respectively. While applied successfully on large-scale data, these methods seem to have shortcomings when it comes to small sample regimes.\n\nFigure 5: Averaging periods on CIFAR10. For 150 clients with small ResNets and 64 samples per client, we visualize the test accuracy (higher is better) of FEDDC and FEDAVG for different aggregation periods b.\n\nTo model different data distributions across clients that could occur in for example our healthcare setting, we ran further experiments on simulated non-iid data, gradually increasing the locally available data, as well as on non-privacy preserving decentralized learning. We investigate the effect of non-iid data on FEDDC by studying the “pathological non-IID partition of the data” (McMahan et al., 2017). Here, each client only sees examples from 2 out of the 10 classes of CIFAR10. We again use a subset of the dataset. The results in Tab. 2 show that FEDDC outperforms FEDAVG by a wide margin. It also outperforms FEDPROX, a method specialized on heterogeneous datasets in our considered small sample setting. For a similar training setup as before, we show results for gradually increasing local datasets in App. A.3.4. Most notably, FEDDC outperforms FEDAVG even with 150 samples locally. Only when the full CIFAR10 dataset is distributed across the clients, FEDAVG is on par with FEDDC (see App. Fig. 7). We also compare with distributed training through gradient sharing (App. A.3.3), which discards any privacy concerns, implemented by mini-batch SGD with parameter settings corresponding to our federated setup as well as a separately optimized version. The results show that such an approach is outperformed by both FEDAVG as well as FEDDC, which is in line with previous findings and emphasize the importance of model aggregation.\n\nAs a final experiment on CIFAR10, we consider daisy-chaining with different combinations of aggregation methods, and hence its ability to serve as a building block that can be combined with other federated learning approaches. In particular, we consider the same setting as before and combine FEDPROX with daisy chaining. The results, reported in Tab. 2, show that this combination is not only successful, but also outperforms all others in terms of accuracy.\n\nMedical image data: Finally, we consider two real medical image datasets representing actual health related machine learning tasks, which are naturally of small sample size. For the brain MRI scans, we simulate 25 clients (e.g., hospitals) with 8 samples each. Each client is equipped with a CNN\n\n4Due to hardware restrictions we are limited to training 150 ResNets, hence 9600 samples across 150 clients.\n\n8\n\n1102050100200500∞00.20.40.60.8averagingperiodbaccuracyFEDDCFEDAVGPublished as a conference paper at ICLR 2023\n\nCIFAR10\n\nMRI\n\nPneumonia\n\nCIFAR10\n\nFEDDC (ours) DC (baseline) FEDAVG (b=1) FEDAVG (b=10) FEDPROX FEDADAGRAD FEDYOGI FEDADAM\n\nCENTRAL\n\n62.9 58.4 55.8 48.7 51.1 21.8 31.4 34.0\n\n0.78\n\n0.85\n\n0.87\n\n0.02 78.4 57.7 74.1 75.6 76.5 45.7 71.3 73.8\n\n0.80\n\n0.01\n\n4.37\n\n0.23\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n1.57\n\n1.68\n\n1.18\n\n0.61 83.2 79.8 80.1 79.4 80.0 62.5 77.6 73.5\n\n0.50\n\n1.62\n\n1.25\n\n1.98\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n65.1\n\n1.44\n\n±\n\n82.1\n\n1.00\n\n±\n\n84.1\n\n0.84\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n0.99\n\n1.53\n\n1.11\n\n0.36\n\n0.01\n\n0.64\n\n0.36\n\n3.31\n\n62.9 FEDDC FEDDC +FEDPROX 63.2\n\n±\n\n0.02\n\n0.38\n\n± Non-IID\n\nFEDDC FEDAVG (b=1) FEDAVG (b=10) FEDPROX FEDADAGRAD FEDADAM FEDYOGI\n\n34.2 30.2 24.9 32.8 11.7 13.0 12.5\n\n0.61\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n2.11\n\n1.95\n\n0.00\n\n0.00\n\n0.00\n\n0.04\n\nTable 1: Results on image data, reported is the average test accuracy of the final model over three runs (± denotes maximum deviation from the average).\n\nTable 2: Combination of FEDDC with FEDAVG and FEDPROX and non-iid results on CIFAR10.\n\n(see App. A.1.1). The results for brain tumor prediction evaluated on a test set of 53 of these scans are reported in Table 1. Overall, FEDDC performs best among the federated learning approaches and is close to the centralized model. Whereas FEDPROX performed comparably poorly on CIFAR10, it now outperforms FEDAVG. Similar to before, we observe a considerable margin between all competing methods and FEDDC. To investigate the effect of skewed distributions of sample sizes across clients, such as smaller hospitals having less data than larger ones, we provide additional experiments in App. A.3.5. The key insight is that also in these settings, FEDDC outperforms FEDAVG considerably, and is close to its performance on the unskewed datasets.\n\nFor the pneumonia dataset, we simulate 150 clients training ResNet18 (see App. A.1.1) with 8 samples per client, the hold out test set are 624 images. The results, reported in Table 1, show similar trends as for the other datasets, with FEDDC outperforming all baselines and the state of the art, and being within the performance of the centrally trained model. Moreover it highlights that FEDDC enables us to train a ResNet18 to high accuracy with as little as 8 samples per client.\n\n8 DISCUSSION AND CONCLUSION\n\nWe propose to combine daisy-chaining and aggregation to effectively learn high quality models in a federated setting where only little data is available locally. We formally prove convergence of our approach FEDDC, and for convex settings provide PAC-like generalization guarantees when aggregating by iterated Radon points. Empirical results on the SUSY benchmark underline these theoretical guarantees, with FEDDC matching the performance of centralized learning. Extensive empirical evaluation shows that the proposed combination of daisy-chaining and aggregation enables federated learning from small datasets in practice.When using averaging, we improve upon the state of the art for federated deep learning by a large margin for the considered small sample settings. Last but not least, we show that daisy-chaining is not restricted to FEDDC, but can be straight-forwardly included in FEDAVG, Radon machines, and FEDPROX as a building block, too.\n\nFEDDC permits differential privacy mechanisms that introduce noise on model parameters, offering protection against membership inference, poisoning and backdoor attacks. Through the random permutations in daisy-chaining rounds, FEDDC is also robust against reconstruction attacks. Through the daisy-chaining rounds, we see a linear increase in communication. As we are primarily interested in healthcare applications, where communication is not a bottleneck, such an increase in communication is negligible. Importantly, FEDDC outperforms FEDAVG in practice also when both use the same amount of communication. Improving the communication efficiency considering settings where bandwidth is limited, e.g., model training on mobile devices, would make for engaging future work.\n\nWe conclude that daisy-chaining lends itself as a simple, yet effective building block to improve federated learning, complementing existing work to extend to settings where little data is available per client. FEDDC, thus, might offer a solution to the open problem of federated learning in healthcare, where very few, undisclosable samples are available at each site.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThe authors thank Sebastian U. Stich for his detailed comments on an earlier draft. Michael Kamp received support from the Cancer Research Center Cologne Essen (CCCE). Jonas Fischer is supported by a grant from the US National Cancer Institute (R35CA220523).\n\nREFERENCES\n\nIdan Achituve, Aviv Shamsian, Aviv Navon, Gal Chechik, and Ethan Fetaya. Personalized federated learning with gaussian processes. In Advances in Neural Information Processing Systems, volume 34. Curran Associates, Inc., 2021. 3\n\nGiuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and Giovanni Felici. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers. International Journal of Security and Networks, 10(3):137–150, 2015. 22\n\nPierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy\n\nphysics with deep learning. Nature communications, 5(1):1–9, 2014. 6, 15\n\nArjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learning through an adversarial lens. In International Conference on Machine Learning, pages 634–643. PMLR, 2019. 22\n\nKenneth L Clarkson, David Eppstein, Gary L Miller, Carl Sturtivant, and Shang-Hua Teng. Approximating center points with iterative radon points. International Journal of Computational Geometry & Applications, 6(03):357–377, 1996. 5, 15\n\nLiam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 2089–2099. PMLR, 18–24 Jul 2021. 3\n\nRobin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client\n\nlevel perspective. arXiv preprint arXiv:1712.07557, 2017. 7\n\nRan Gilad-Bachrach, Amir Navot, and Naftali Tishby. Bayes and tukey meet at the center point. In International Conference on Computational Learning Theory, pages 549–563. Springer, 2004. 5\n\nKristin L Granlund, Sui-Seng Tee, Hebert A Vargas, Serge K Lyashchenko, Ed Reznik, Samson Fine, Vincent Laudone, James A Eastham, Karim A Touijer, Victor E Reuter, et al. Hyperpolarized mri of human prostate cancer reveals increased lactate with tumor grade driven by monocarboxylate transporter 1. Cell metabolism, 31(1):105–114, 2020. 1\n\nFarzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated\n\nlearning. arXiv preprint arXiv:1910.14425, 2019. 1, 2, 4\n\nWeituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J Liang, Changyou Chen, and Lawrence Carin Duke. Towards fair federated learning with zero-shot data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3310–3319, 2021. 2\n\nBaihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural tangent kernel-based framework for federated learning analysis. In International Conference on Machine Learning, pages 4423–4434. PMLR, 2021. 2\n\nMarwa Ibrahim, Mohammad Wedyan, Ryan Alturki, Muazzam A Khan, and Adel Al-Jumaily. Augmentation in healthcare: Augmented biosignal using deep learning and tensor representation. Journal of Healthcare Engineering, 2021, 2021. 2\n\nM ́ark Jelasity, Alberto Montresor, and Ozalp Babaoglu. Gossip-based aggregation in large dynamic\n\nnetworks. ACM Transactions on Computer Systems (TOCS), 23(3):219–252, 2005. 3\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nMichael Kamp. Black-Box Parallelization for Machine Learning. PhD thesis, Rheinische Friedrich-\n\nWilhelms-Universit ̈at Bonn, Universit ̈ats-und Landesbibliothek Bonn, 2019. 3\n\nMichael Kamp, Mario Boley, Olana Missura, and Thomas G ̈artner. Effective parallelisation for machine learning. In Advances in Neural Information Processing Systems, volume 30, pages 6480–6491. Curran Associates, Inc., 2017. 3, 4, 5, 14, 15\n\nMichael Kamp, Linara Adilova, Joachim Sicking, Fabian H ̈uger, Peter Schlicht, Tim Wirtz, and Stefan Wrobel. Efficient decentralized deep learning by dynamic model averaging. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 393–409. Springer, 2018. 3, 4\n\nBingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, and Trevor Darrell. Few-shot object detection via feature reweighting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8420–8429, 2019. 1\n\nP ́eter Kiss and Tomas Horvath. Migrating models: A decentralized view on federated learning. In Proceedings of the Workshop on Parallel, Distributed, and Federated Learning. Springer, 2021. 3\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University\n\nof Toronto, Toronto, 2009. 2, 7\n\nSang Gyu Kwak and Jong Hae Kim. Central limit theorem: the cornerstone of modern statistics.\n\nKorean journal of anesthesiology, 70(2):144, 2017. 3\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 18\n\nQinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10713–10722, 2021. 3\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In Conference on Machine Learning and Systems, 2020a, 2020a. 2, 3, 7\n\nXiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning In International Conference on Learning\n\non non-iid features via local batch normalization. Representations, 2020b. 2\n\nPei Liu, Xuemin Wang, Chao Xiang, and Weiye Meng. A survey of text data augmentation. In 2020 International Conference on Computer Communication and Network Security (CCNS), pages 191–195. IEEE, 2020. 1\n\nPengrui Liu, Xiangrui Xu, and Wei Wang. Threats, attacks and defenses to federated learning: issues,\n\ntaxonomy and perspectives. Cybersecurity, 5(1):4, 2022. 6, 22\n\nChuan Ma, Jun Li, Ming Ding, Howard H Yang, Feng Shu, Tony QS Quek, and H Vincent Poor. On safeguarding privacy and security in the framework of federated learning. IEEE network, 34(4): 242–248, 2020. 6, 22\n\nOthmane Marfoq, Giovanni Neglia, Aur ́elien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2021. 2\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273–1282, 2017. 2, 3, 7, 8\n\nPeter Neal. The generalised coupon collector problem. Journal of Applied Probability, 45(3):\n\n621–629, 2008. 20\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nCorrie A Painter, Esha Jain, Brett N Tomson, Michael Dunphy, Rachel E Stoddard, Beena S Thomas, Alyssa L Damon, Shahrayz Shah, Dewey Kim, Jorge G ́omez Tejeda Za ̃nudo, et al. The angiosarcoma project: enabling genomic and clinical discoveries in a rare cancer through patient-partnered research. Nature medicine, 26(2):181–187, 2020. 1\n\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 7, 14\n\nKrishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\n\nIEEE Transactions on Signal Processing, 70:1142–1154, 2022. 3\n\nViraj Prabhu, Anitha Kannan, Murali Ravuri, Manish Chaplain, David Sontag, and Xavier Amatriain. Few-shot learning for dermatological disease diagnosis. In Machine Learning for Healthcare Conference, pages 532–552. PMLR, 2019. 2\n\nJohann Radon. Mengen konvexer K ̈orper, die einen gemeinsamen Punkt enthalten. Mathematische\n\nAnnalen, 83(1):113–115, 1921. 4, 15\n\nSashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn`y, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on Learning Representations, 2020. 2, 7, 14\n\nAmirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated learning: The case of affine distribution shifts. In Advances in Neural Information Processing Systems, volume 33, pages 21554–21565. Curran Associates, Inc., 2020a. 2\n\nAmirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization. In International Conference on Artificial Intelligence and Statistics, pages 2021–2031. PMLR, 2020b. 3\n\nShai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to\n\nalgorithms. Cambridge university press, 2014. 4\n\nOhad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 850– 857. IEEE, 2014. 1, 3, 4, 16\n\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pages 3–18. IEEE, 2017. 1, 6, 22\n\nXiaoping Su, Xiaofan Lu, Sehrish Khan Bazai, Eva Comp ́erat, Roger Mouawad, Hui Yao, Morgan Rouprˆet, Jean-Philippe Spano, David Khayat, Irwin Davidson, et al. Comprehensive integrative profiling of upper tract urothelial carcinomas. Genome biology, 22(1):1–25, 2021. 1\n\nZiteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really\n\nbackdoor federated learning? arXiv preprint arXiv:1911.07963, 2019. 6, 22\n\nLisa Torrey and Jude Shavlik. Transfer learning. In Handbook of research on machine learning applications and trends: algorithms, methods, and techniques, pages 242–264. IGI global, 2010. 2\n\nJohn W Tukey. Mathematics and picturing data. In Proceedings of the International Congress of\n\nMathematics, volume 2, pages 523–531, 1975. 4\n\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, volume 29, pages 3630–3638. Curran Associates, Inc., 2016. 2\n\nUlrike Von Luxburg and Bernhard Sch ̈olkopf. Statistical learning theory: Models, concepts, and\n\nresults. In Handbook of the History of Logic, volume 10, pages 651–706. Elsevier, 2011. 5\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nHongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In International Conference on Learning Representations, 2019. 2\n\nKang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS Quek, and H Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. IEEE Transactions on Information Forensics and Security, 15:3454–3469, 2020. 6, 22\n\nQian Yang, Jianyi Zhang, Weituo Hao, Gregory P. Spell, and Lawrence Carin. Flop: Federated learning on medical datasets using partial networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, page 3845–3853. Association for Computing Machinery, 2021. 3\n\nHao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5693–5700, 2019. 4, 19\n\nMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference on Machine Learning, pages 7252–7261. PMLR, 2019. 3\n\nChengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu. Batchcrypt: Efficient homomorphic encryption for cross-silo federated learning. In USENIX Annual Technical Conference, pages 493–506, 2020. 22\n\nLigeng Zhu and Song Han. Deep leakage from gradients. In Federated learning, pages 17–31.\n\nSpringer, 2020. 6\n\n13",
    "reference": "# Summary Of The Paper\n\nIn this work, a practical and efficient FL learning framework is proposed for improving aggregation performance over multiple clients. The core step of this model is to use the interleaving model aggregation and permutation steps so that the proposed federated daisy-chaining method can work well for data sparse problems. Multiple experimental results show that FedDC outperforms the state-of-the-art methods significantly, e.g., FedAvg.\n\n# Strength And Weaknesses\n\nStrength: this work considers a practical issue of data sparse in the FL systems. It further provides theoretical analysis results on convergence, generalization performance evaluation, requirements on communications, and massive numerical results on classification problems.\n\nWeaknesses: even the work is new and encouraging, there are still some major concerns as follows:\n\n1) A convergence result is given in Corollary 1, which basically uses Yu et al. (2019)' result. How does the permutation affect the convergence is not analyzed. Does Yu et al. (2019)'s analysis covers the client permutation? Note that there is a significant difference between the selection with and without replacement. There is a gap between the FedDC and the analysis.\n\n2)  In the convergence result, $b$ needs to be less than a threshold, but eq.(2) $b$ is required to be large than a threshold. Is there an overlap between these?\n\n3) This work assumes that the risk considered is convex while their convergence result is borrowed from the nonconvex. A consistent argument is encouraged.\n\n4)  The authors claimed that FedDC can perform better in the data sparse case, however, the theory seems uncorrelated with any parameters regarding the data sample size (Lemma 4). In Prop. 5, if m is small and n is large, will $b$ be a negative number? A sufficient discussion of the theoretical results should be provided.\n\n5) $m$ is assumed to be large enough so that the local model can improve the performance. Will this method also work for $m$ is small in the sense that the same as FedAvg or will diverge? also, how decide $r$ in practice?\n\n6) The theoretical results seem completely independent on which federated learning algorithm adopted, proxFed, or adam based algorithm. In other word, the convergence rate of the algorithm will not affect the $b$ and $T$, right?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: this paper proposes a daisy-chaining based FL learning framework, but the algorithm is not presented clearly. Oracle agg means performing average, but fig.1 (rirght) seems allowing partial average (note that $\\bar{h}$ still needed here). The proofs are not derived in a step-by-step way. For example, most of the convergence analysis used other papers' result directly.\n\nQuality: the idea of daisy chaining is interesting and it is expected that FedDC would work well and outperform others.  The theoretical analysis is not rigorous, where there is a gap between the high level design of the daisy chaining and implemented algorithm.\n\nNovelty: However, actually this strategy is very similar as permutation based SGD training by shuffling the data while here FedDC reshuffles the clients.\n\nReproducibility: the code is publicly accessible but not runnable.\n\n# Summary Of The Review\n\nIn summary, this paper proposes an efficient training framework, FedDC, for dealing with the data sparse problem, but the theoretical justification of the convergence and generalization contains a gap, which weakens the contributions of this work. \n\n====================== after Zoom Meeting ===================\n\nAfter the discussion with the AC and other reviewers, I suppose that there are some merits of this work and increase the score, even 1) the setting of the convergence part is not consistent with the PAC analysis and there is no condition of step size to ensure the convergence; 2) tuning the multiple hyper-parameters to satisfy the theoretical conditions is a very difficult task, 3) and there is still a gap between the theory and practical implementation for the PAC part.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nQUANT: QUANTUM ANNEALING WITH LEARNT COUPLINGS\n\nMarcel Seelbach Benkner Universit ̈at Siegen\n\nMaximilian Krahn MPI for Informatics, SIC Aalto University\n\nEdith Tretschk MPI for Informatics, SIC\n\nZorah L ̈ahner & Michael Moeller Universit ̈at Siegen\n\nVladislav Golyanik MPI for Informatics, SIC\n\nABSTRACT\n\nModern quantum annealers can find high-quality solutions to combinatorial optimisation objectives given as quadratic unconstrained binary optimisation (QUBO) problems. Unfortunately, obtaining suitable QUBO forms in computer vision remains challenging and currently requires problem-specific analytical derivations. Moreover, such explicit formulations impose tangible constraints on solution encodings. In stark contrast to prior work, this paper proposes to learn QUBO forms from data through gradient backpropagation instead of deriving them. As a result, the solution encodings can be chosen flexibly and compactly. Furthermore, our methodology is general and virtually independent of the specifics of the target problem type. We demonstrate the advantages of learnt QUBOs on the diverse problem types of graph matching, 2D point cloud alignment and 3D rotation estimation. Our results are competitive with the previous quantum state of the art while requiring much fewer logical and physical qubits, enabling our method to scale to larger problems. The code and the new dataset are available at https://4dqv.mpi-inf.mpg.de/QuAnt/.\n\n1\n\nINTRODUCTION\n\nHybrid computer vision methods that can be executed partially on a quantum computer (QC) are an emerging research area (Boyda et al., 2017; Cavallaro et al., 2020; Seelbach Benkner et al., 2021; Yurtsever et al., 2022). Compared to classical methods, they promise to solve computationally demanding (e.g., combinatorial) sub-problems faster, with improved scaling, and without relaxations that often lead to approximate solutions. Although quantum primacy has not yet been demonstrated in remotely practical usages of quantum computing, all existing quantum computer vision (QCV) methods fundamentally assume that it will be achieved in the future. Thus, solving these suitable algorithmic parts on a QC has the potential to reshape the field. However, reformulating them for execution on a QC is often non-trivial.\n\nQCV continues building up momentum, fuelled by accessible experimental quantum annealers (QA) allowing to solve practical (N P-hard) optimisation problems. Existing QCV methods using QAs rely on analytically deriving QUBOs (both QUBO matrices and solution encodings) for a specific problem type, which is challenging, especially since solutions need to be encoded as binary vectors (Li & Ghosh, 2020; Seelbach Benkner et al., 2020; 2021; Birdal et al., 2021). This often leads to larger encodings than necessary, severely impacting scalability. Alternatively, QUBO derivations with neural networks are conceivable but have not yet been scrutinised in the QA literature.\n\nIn stark contrast to the state of the art, this paper proposes, for the first time, to learn QUBO forms from data for any problem type using backpropagation (see Fig. 1). Our framework captures, in the weights of a neural network, the entire subset of QUBOs belonging to a problem type; a single forward pass yields the QUBO form for a given problem instance. It is thus a meta-learning approach in the context of hybrid (quantum-classical) neural network training, in which the superordinate network instantiates the parameters of the QUBO form. We find that sampling instantiated QUBOs can be a reasonable alternative to non-quantum neural baselines that regress the solution directly.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: We propose QuAnt for QUBO learning, i.e., a quantum-classical meta-learning algorithm that avoids analytical QUBO derivations by learning to regress QUBOs to solve problems of a given type. We first represent a problem instance as a vector p and then feed it into an MLP that regresses the entries of the QUBO matrix A. We then initialise a quantum annealer with A and use quantum annealing to find a QUBO minimiser and extract it as the solution x∗ to the problem instance. We define losses involving x∗ that avoid backpropagation through the annealing and backpropagate gradients through the MLP to train it. We demonstrate the generalisability of QuAnt on graph matching, point set registration, and rotation estimation.\n\nIn particular, we show how a (combinatorial) quantum annealing solver can be integrated into a vanilla neural network as a custom layer and be used in the forward and backward passes, which may be useful in other contexts. To that end, we introduce a contrastive loss that circumvents the inherently discontinuous and non-differentiable nature of QUBO solvers. Our method is compatible with any QUBO solver at training and test time—we consider parallelised exhaustive search, simulated annealing, and quantum annealing. QUBO learning, i.e., determining a function returning QUBO forms given a problem instance of some problem type as input, is a non-trivial and challenging task. In summary, this paper makes several technical contributions to enable QUBO learning:\n\n1. QuAnt, i.e., a new meta-learning approach to obtain QUBO forms executable on modern QAs for computer vision problems. While prior methods rely on analytical derivations, we learn QUBOs from data (Sec. 3.1).\n\n2. A new training strategy for neural methods with backpropagation involving finding lowenergy solutions to instantaneous (optimised) QUBO forms, independent of the solver (Secs. 3.2 and 3.3).\n\n3. Application of the new framework to several problems with solutions encoded by permu-\n\ntations and discretised rigid transformations (Secs. 3.4 and 3.5).\n\nWe show that our methodology is a standardised way of obtaining QUBOs independent of the target problem type. This paper focuses on three problem types already tackled by QCV methods relying on analytical QUBO derivations: graph matching and point set alignment (with and without known prior point matches in the 3D and 2D cases, respectively). We emphasise that we do not claim to outperform existing specialised methods for these problem types or that QA is particularly wellsuited for them. Rather, we show that this wide variety of problems can be tackled successfully and competitively by our general quantum approach already now, before quantum primacy. Thus, in the future, computer vision methods may readily benefit from the (widely expected) speed-up of QC through an easy and flexible re-formulation of algorithmic parts as QUBOs, thanks to our proposed method. We run our experiments on D-Wave Advantage5.1 (Dattani et al., 2019), an experimental realisation of AQC with remote access. This paper assumes familiarity with the basics of quantum computing. For convenience, we summarise several relevant definitions in the Appendix.\n\n2 RELATED WORK\n\nThe two main paradigms for quantum computing are gate-based QC and adiabatic quantum computing (AQC). Our method uses quantum annealing, which is derived from AQC, and is not gate-based. The predominantly theoretical field of quantum machine learning (QML) investigates how quantum computations can be integrated into machine learning (Biamonte et al., 2016; Dunjko & Briegel, 2018; Sim et al., 2019; Havl ́ıˇcek et al., 2019; Du et al., 2020; Mariella & Simonetto, 2021; K ̈ubler et al., 2021). Many QML methods assume gate-based quantum computers and define a quantum\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nvariational layer, i.e., a sequence of parametrised unitary transformations meant for execution on quantum hardware (Mitarai et al., 2018). QML methods are often optimised using variants of the backpropagation algorithm (McClean et al., 2018; Verdon et al., 2019; Beer et al., 2020). Quantum variational models were recently applied to combinatorial optimisation (Khairy et al., 2020) and reinforcement learning (Dunjko et al., 2016; Lockwood & Si, 2020). Instead of learning to regress unitary transformation parameters for gate-based QC, we learn to regress QUBO forms for QA.\n\nIn contrast to gate-based machines, QAs can already solve real problems formulated as QUBOs (Neukart et al., 2017; Teplukhin et al., 2019; Stollenwerk et al., 2019; Orus et al., 2019; Mato et al., 2021; Speziali et al., 2021). Recently, QCV has rapidly transitioned from theoretical considerations (Venegas-Andraca & Bose, 2003; Chin et al., 2020; Neven et al., 2008b;a) to practical algorithms leveraging quantum-mechanical effects of quantum computers, ranging from image retrieval and processing (Venegas-Andraca & Bose, 2003; Yan et al., 2016), classification (Boyda et al., 2017; Nguyen & Kenyon, 2019; Cavallaro et al., 2020; Willsch et al., 2020; Dema et al., 2020) and tracking (Li & Ghosh, 2020; Zaech et al., 2022), to problems on graphs (Zick et al., 2015; Seelbach Benkner et al., 2020; Mariella & Simonetto, 2021), consensus maximisation (Doan et al., 2022), shape alignment Noormandipour & Wang (2021); Seelbach Benkner et al. (2021), segmentation (Arrigoni et al., 2022) and ensuring cycle-consistency (Birdal et al., 2021; Yurtsever et al., 2022).\n\nMany of these methods are evaluated on real quantum hardware, as both gate-based and QA machines can be accessed remotely (D-Wave Systems, 2022; Rigetti Computing, 2022).\n\nWe demonstrate the efficacy of our QuAnt approach on the applications of graph matching and point set alignment where we compare against recent quantum state-of-the-art methods Seelbach Benkner et al. (2020); Golyanik & Theobalt (2020), respectively.\n\nAnother line of work in different domains concerns learning the best adiabatic quantum algorithm. While some works (Pastorello et al., 2021; Pastorello & Blanzieri, 2019) develop an algorithm inspired by tabu search, our method uses a neural network to output a coupling matrix. Orthogonal to our work, others train neural networks to solve problem-specific QUBOs (Gabor et al., 2020). N ̈ußlein et al. (2022) optimize a blackbox function by finding a QUBO as surrogate model. QML on gate-based QC has been studied at length, but machine learning with QA remains largely underexplored, with only a few exceptions (e.g., linear regression (Date & Potok, 2021) and binary neural networks (Sasdelli & Chin, 2021)). In stark contrast to existing QCV methods with analytically derived QUBOs (Li & Ghosh, 2020; Seelbach Benkner et al., 2020; 2021; Birdal et al., 2021), our approach enables more flexible and compact solution encodings.\n\nQuAnt is also related to recent non-quantum approaches that aim to improve combinatorial optimisation by seamlessly integrating deep learning and combinatorial building blocks as custom layers and backpropagating through them (Ferber et al., 2020; Rol ́ınek et al., 2020; Vlastelica et al., 2020). In this respect, ours is the first work that uses a quantum QUBO solver in neural architectures.\n\n3 QUBO LEARNING APPROACH\n\nWe present a new meta-learning approach for regressing quadratic unconstrained binary optimisation problems (QUBOs) suitable for modern quantum annealers (QA); see Fig. 1. While existing works analytically derive QUBOs for different problems (Birdal et al., 2021; Seelbach Benkner et al., 2020; 2021; Zaech et al., 2022), we propose to instead learn a function that turns a problem instance into a QUBO to be solved by a QA. Specifically, we train a multi-layer perceptron (MLP) that takes a vectorised problem instance and regresses the QUBO weights such that the QUBO minimiser is the solution to the problem. Note that we only specify the bit encoding of the solution but let the network learn to derive QUBOs. Crucially, we show how training the MLP is possible despite quantum annealing (like any QUBO solver) being discontinuous and non-differentiable.\n\n3.1 QUBOS AND QUANTUM ANNEALING\n\nis\n\nQuantum annealing form to arg mins∈{−1,1}n s⊤Js + b⊤s, where s is a binary vector, J ∈ Rn×n is a matrix of couplings, and b ∈ Rn contains biases (McGeoch, 2014). We can rewrite this as a QUBO: arg minx∈{0,1}n x⊤Ax, by substituting x = 1 2 diag(b).\n\n2 (s + 1n) and A = 1\n\n2 (J1n + 1⊤\n\na metaheuristic\n\nsolve N P-hard\n\nn J) + 1\n\n4 J + 1\n\nproblems\n\nthe\n\nof\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nIn quantum annealing, the binary n-dimensional vector x describes the measurement outcomes of n qubits. Annealing starts out with an equal superposition quantum state of the qubits that assigns an equal probability to all possible binary states {0, 1}n. During the anneal, the couplings and biases of A are gradually imposed on the qubits. The adiabatic theorem (Born & Fock, 1928) implies that doing so sufficiently slow forces the qubits into a quantum state that assigns nonvanishing probability only to binary states that minimise the QUBO (Farhi et al., 2001). We then only need to measure the qubits to determine their binary state, which is our solution x. For a more detailed description of quantum annealing, we refer to prior computer-vision works (Seelbach Benkner et al., 2020; 2021; Golyanik & Theobalt, 2020; Li & Ghosh, 2020) and Appendix A.\n\n3.2 NETWORK ARCHITECTURE AND LOSSES\n\nIn this section, we describe the network architecture that takes as input a problem instance and regresses a QUBO whose solution (e.g., obtained via quantum annealing) solves the problem instance. For a given problem type, we require a problem description that is amenable to QUBO learning: A parametrisation of problem instances as real-valued vectors p ∈ Rm, and a parametrisation of solutions as binary vectors x ∈ {0, 1}n. Since we use supervised training, we additionally need a training set D = {(pd, ˆxd)}D d=1 containing D problem instances pd with ground-truth solutions ˆxd. We use a multilayer perceptron (MLP) with L layers and H hidden dimensions, ReLU activations (except for the last layer, which uses sin (Sitzmann et al., 2020)), and concatenating skip connections from the input into odd-numbered layers (except for the first and last layers). The input to the network is a problem instance p, and the output is a QUBO matrix A: A = MLP(p).\n\nWe could now use a dataset of problem instances and corresponding A to supervise the MLP directly. However, this requires specifying how A is to be derived for a certain instance, which comes (1) A problem-type-specific algorithm for analytically deriving instancewith two downsides: specific A needs to be designed to generate enough training data {(pd, Ad)}D d=1, which is nontrivial, and (2) The binary parametrisation (x) of the solution space depends on the algorithm, which can lead to more variables than intrinsically needed by the problem (e.g., if x needs to represent one of k numbers, a one-hot parametrisation would have length n=k, while a binary-encoding parametrisation would have length n= log k). This is particularly problematic as contemporary quantum hardware only provides a limited number of qubits. We thus choose to supervise A not directly and, instead, supervise the solutions of the QUBO. This strategy tackles both issues as it lets the network learn an algorithm compatible with the (potentially shorter) solution parametrisation. Therefore, our method is easily applicable to new problem types, as we show in Secs. 3.4 and 3.5.\n\nThe regressed A defines a QUBO, which can be solved by any QUBO solver. But how can we, during training, backpropagate gradients from the solution binary vector x through the QUBO solver despite these solvers having zero gradients almost everywhere (Vlastelica et al., 2020)? We circumvent this issue by exploiting a contrastive loss (cf. LeCun et al. (2006, Eq. (10))) as follows: We know the energy of the ground-truth solution ˆx of the problem instance, namely ˆx⊤Aˆx. If the energy of the minimiser x∗ = x∗(A) of the current QUBO is lower, then A does not yet describe a QUBO that outputs the right solution. We, therefore, seek to push the energy of ˆx lower while pulling the energy of the minimiser x∗ up:\n\nLgap = ˆx⊤Aˆx − x∗⊤Ax∗, (1) which has a zero gradient if x∗ = ˆx, as desired. Lgap avoids backpropagation through x∗ and is even compatible with automatic differentiation. It yields the following update for an entry of A, which can then be further backpropagated into the MLP via the chain rule: ∂x∗(A) ∂Ai,j\n\n∂Lgap(A) ∂Ai,j\n\n= 2ˆxi ˆxj − 2x∗\n\nj − 2\n\nAx∗,\n\ni x∗\n\n(2)\n\nwhere the last term comes from the dependency of x∗ on A via the QUBO solver. While intuitively useful, we note that this term is zero “almost everywhere” (in the mathematical sense), and we hence ignore it as it provides almost no information. Note that this is a common approximation (e.g., auto-differentiation frameworks backpropagate through max(·) pooling in the same manner).\n\nUnfortunately, Lgap alone would not prevent degenerate A, which have multiple solutions, including undesirable ones. We, therefore, discourage such A that have more than one solution x∗:\n\nLunique = −|ˆx⊤Aˆx − x+⊤Ax+|,\n\n(3)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nwhere x+ is the x that minimises x⊤Ax over {0, 1}n \\ {x∗}, and |·| is the absolute value operator.\n\nIn addition to these data terms, we found it helpful to regularise the network by encouraging sparsity on all intermediate features of the MLP (Liu et al., 2015):\n\n(cid:88)\n\nLmlp =\n\n1 |f |\n\n∥f ∥1,\n\n(4)\n\nf ∈F where F is the set of all network layer outputs (except for the last layer). The total loss then reads: L = Lgap + λuniqueLunique + λmlpLmlp, (5)\n\nwhere we set λmlp = 10−4 and λunique = 10−3 regardless of problem type.\n\n3.3 QUBOS ON D-WAVE QUANTUM ANNEALERS\n\nWe can use D-Wave to solve any generated QUBO A, where Ai,j ∈ R describes the direction and coupling strength between logical qubits i and j. However, each physical qubit in the annealer is only connected to a small subset of other physical qubits, which makes the regressed A not directly compatible with the annealer. We tackle this issue by manually pre-determining a sparse connectivity pattern of the physical qubits and then masking out the other entries of A before solving, such that the MLP focuses on only these sparse entries. For example, when x ∈ Rn with n=8, we can use D-Wave’s Chimera architecture, which is made up of interconnected K4,4 unit cells (Dattani et al., 2019). (K4,4 is a complete bipartite graph with two sets of four qubits each.) Since n=8, we can fit one problem instance into one unit cell, which allows us to anneal many problem instances in parallel by putting them in different unit cells. This speeds up training and saves expensive time on the quantum annealer. For larger problems with n=32, we can use D-Wave’s Pegasus architecture (Boothby et al., 2020), which has more interconnections (qubit couplings) between its K4,4 unit cells than Chimera. We use four such unit cells per problem instance, following D-Wave’s pattern (Til). See Fig. 4b for an exemplary colour-coded qubit connectivity pattern of A.\n\nGiven our full method, we next show how it can be applied to three problem types, i.e., graph matching, point set registration, and rotation estimation; see Appendix for further details.\n\n3.4 GRAPH MATCHING\n\nThe goal of graph matching is to determine correspondences from k key points in two images or graphs; see Fig. 2 for an example. This can be formalised as a quadratic assignment problem with a permutation matrix representing the matching (Seelbach Benkner et al., 2020): arg minX∈Pk is the vectorised permutation matrix, and W ∈ Rk2×k2 permutation constraint cannot be directly realised on the quantum annealer.\n\nx⊤Wx, where Pk is the set of permutation matrices, x = vec(X) ∈ {0, 1}k2\n\ncontains pairwise weights. Unfortunately, the\n\nInstead, note that a permutation P : [k] → [k] is fully defined by the sequence P (1), P (2), . . . , P (k). Our method can use an efficient binary encoding for each entry of this sequence, using only k log k binary variables in total. Note that not all vectors x ∈ {0, 1}k log k are valid permutations. As an optional post-processing step, we can perform a projection to the nearest permutation with respect to the Hamming distance in our binary encoding. Unless stated otherwise, we do not apply this post-processing.\n\n(a) Source\n\n(b) Target\n\nIn addition to the solution parametrisation, we also need to design the problem description p. For real data, we use p = vec(W), where the diagonal contains cosine similarities between the feature vectors extracted with AlexNet (Krizhevsky et al., 2012) pre-trained on ImageNet (Deng et al., 2009) of all key point pairs, and the off-diagonal follows the geometric term from (Torresani et al., 2008, Eq. (6)). For evaluation, we also introduce the synthetic RandGraph dataset; it uses matrices of random distances D ∈ Rk×k with entries Di,j ∈ U(0, 1) to define Wk·i+P (i),k·j+P (j) = (cid:12) (cid:12) (cid:12). The MLP thus learns to compress the input matrix into a much smaller QUBO.\n\nFigure 2: Example matching on four key points from the Willow dataset (Cho et al., 2013). Corresponding points (same colours) are found based on feature similarity.\n\n(cid:12)Di,j − DP (i),P (j)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n3.5 POINT SET REGISTRATION AND ROTATION ESTIMATION\n\nIn 2D point set registration, we are given two point sets with potentially different numbers of points and no correspondences, and we seek to find a rotation angle that best aligns them. We follow Golyanik et al. (Golyanik & Theobalt, 2020) and use the vectorised form of their input matrix to represent a problem instance. We parametrise the solution space of x ∈ {0, 1}9 by splitting the output space [0, 1 3 π] into 29 equally sized bins and consecutively indexing them with a 9-bit integer. In 3D rotation estimation, we are given two 3D point clouds with known matches and seek to estimate the 3D rotation aligning them. We represent a problem instance by the vectorised covariance matrix of the two point clouds; 3D rotation is parametrised by Euler angles α, β, γ. We discretise each angle into 25 bins, such that x ∈ {0, 1}15.\n\n4 EXPERIMENTAL EVALUATION\n\nWe next experimentally evaluate QuAnt. Our goal is to show that it outperforms the previous quantum state of the art. For reference, we also report comparisons against specialised classical methods.\n\nData. We evaluate graph matching on the Willow object dataset (Cho et al., 2013), which contains labelled key points. We use k=4 randomly chosen key point pairs per image. We use 5640 images for training, and test on 846 images. Both of the sets are obtained via pygmtools (ThinkLab, 2021). We also evaluate on our synthetic dataset RandGraph (see Sec. 3.4), with both k=4 and k=5. We evaluate 2D point set registration on the 2D Shape Structure dataset (Carlier et al., 2016) providing 2D silhouette images of real-world objects. We treat the silhouette outlines as 2D points. We use 500 shapes from various classes for training, and test on 50 shapes. For each shape, we apply 1000 (for train) or 100 (for test) different rotations of up to 60◦ and pick random pairs to generate problem instances. For 3D rotation estimation, we evaluate on ModelNet10 (Wu et al., 2015), which contains CAD models of ten object categories. We proceed with point cloud representations of each shape. We use 300 shapes from various classes for training, and test on 30 shapes from various classes. For each shape, we apply 1000 different 3D rotations with angle ranges α, γ ∈ [− 1 9 π] and β ∈ [− 1 18 π] and pick random pairs to generate problem instances.\n\n18 π, 1\n\n9 π, 1\n\nComparisons. We compare QuAnt to two baselines and specialised methods, depending on the problem type. For all problem types, we demonstrate the power of using QUBOs compared to the Diag baseline that regresses a diagonal QUBO matrix A (which is trivially solvable). While this baseline ablates the QUBO itself, we also consider a more natural neural network baseline, i.e., Pure, that regresses the binary solution directly (there is no activation after the last layer) and uses an l1-loss between the output and ˆx instead of Lgap and Lunique. At test time, we threshold the network output of Pure at 0 to obtain binary vectors. For QuAnt, Diag and Pure variants, we experiment with all combinations of the numbers of layers L ∈ {3, 5} and hidden dimensions H ∈ {32, 78}.\n\nWe compare our graph matching results with the Direct baseline on Willow (Cho et al., 2013); we directly solve the quadratic assignment problem given by W with exhaustive search, which provides an upper bound for our method. We also compare against Quantum Graph Matching (QGM) (Seelbach Benkner et al., 2020), to which we pass our input matrices W. For 2D point set registration, we compare against the analytic quantum method (AQM) (Golyanik & Theobalt, 2020), which is an upper bound for our technique since we take its vectorised QUBO as input, and against the classical, specialised ICP algorithm (Lu & Milios, 1997). For 3D rotation estimation, we use Procrustes as a classical specialised method, which is thus an upper bound for our (general) method.\n\nMetrics. We measure accuracy of the graph matching solutions as the percentage of correctly recovered permutation matrices. For 2D point set registration and 3D rotation estimation, we quantify the difference between the known ground-truth rotations and the estimated rotations by their geodesic distances (angles) in the rotation groups SO(2) and SO(3), respectively.\n\nQUBO Solvers. For graph matching, we follow Sec. 3.3 to make our regressed QUBOs compatible with the QA. Due to a restricted QA compute budget, we train and test with simulated annealing unless stated otherwise. For the point cloud experiments, we regress dense A and use our exhaustive search implementation at train and test time unless stated otherwise. When evaluating on the QA, we rely on minor embeddings to make the regressed A compatible with the QA. Please refer to the Appendix for the details.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n4.1 RESULTS\n\nGeneral Baselines. The quantitative results for graph matching, point set registration, and rotation estimation are reported in Tables 1, 2, and 3, respectively. Across network sizes and all three problem types, the results show that having a full, N P-hard QUBO (ours) instead of only a diagonal QUBO (Diag) is advantageous. We also find that the proposed method yields better results than Pure on both point set registration and rotation estimation, although Pure yields better results for graph matching.\n\nTable 1: Comparison to general baselines on graph matching. We report the accuracy (in %).\n\n(a) RandGraph for k=4.\n\nOurs Diag\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\n9 30 11 49\n\n8 18 11 43\n\nPure 91 96 89 96\n\n(b) Willow object dataset (Cho et al., 2013) for k=4. Trained for 300 epochs with L=5, H=78.\n\nOurs Diag\n\nPure Direct\n\n69\n\n53\n\n90\n\n97\n\nTable 2: Comparison to general baselines on point set registration. We report averages of the mean errors and their standard deviations over three runs.\n\nTable 3: Comparison to general baselines on rotation estimation. We report averages of the mean errors and their standard deviations over three runs.\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\nOurs 8.4 ± 0.8 7.2 ± 1.1 8.6 ± 0.5 6.8 ± 0.3\n\nDiag 11.1 ± 1.3 8.3 ± 0.7 10.9 ± 1.2 7.7 ± 0.5\n\nPure 8.2 ± 1.2 9.3 ± 1.9 9.3 ± 1.9 11.3 ± 4.5\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\nOurs 5.9 ± 3.0 4.1 ± 0.5 3.7 ± 0.8 3.4 ± 0.4\n\nDiag 5.4 ± 1.0 5.0 ± 0.3 5.0 ± 0.4 4.7 ± 0.2\n\nPure 7.9 ± 0.5 7.1 ± 0.1 16.2 ± 7.1 10.1 ± 1.8\n\nSpecialised Methods. For reference, we compare QuAnt to methods specialised to a certain problem type. Since our approach is general, they mostly provide an upper bound for our performance.\n\nWe evaluate QGM (Seelbach Benkner et al., 2020) on several RandGraph instances. We confirm their finding that strongly enforcing permutation constraints eventually retrieves the right permutation as the sample with the lowest energy. However, using the analytical bound for the penalty term leads to a success probability (i.e., the probability of getting the best solution across anneals) smaller than random guessing due to experimental errors in the couplings. Next, we find that the QUBOs of QuAnt are much smaller and better suited to be solved with a quantum annealer than QGM’s. For RandGraph with k=5, our method needs 15 physical qubits while their baseline and row-wise methods need 89 qubits on average and a chain length of four, and their Inserted method needs, on average, 39 qubits and a chain length of three on D-Wave Advantage. Thus, our success probability of 26% when evaluating on test data is orders of magnitude higher than Inserted’s 0.22% (best in QGM). This shows how QuAnt improves over the quantum state of the art even though we merely focus on the solution with the lowest energy across anneals, while they focus on the success probabilities. We refer to Appendix D for a detailed evaluation. Table 1b confirms that Direct is an upper bound to our approach.\n\nTable 8 shows quantitative results for 2D point set matching without noise. AQM slightly outperforms QuAnt, which is expected as we take AQM’s QUBO as input; hence, its performance is an upper bound for our method. Fig. 3 shows a qualitative example.\n\nAs expected, quantitative results in Table 7 (with no incorrect correspondences) show that classical, specialised Procrustes performs better than our general method on 3D rotation estimation. Note that our technique yields better results than Procrustes under test-time noise, as we discuss later in detail.\n\n7\n\nFigure 3: Test-time example inputs and outputs of QuAnt trained for 2D point set registration.\n\nRegress Couplings with MLPEncode Problem InstanceQUBOSolutionClassicalQuantumDecode QUBO SolutionPublished as a conference paper at ICLR 2023\n\n4.2 ABLATION\n\nWe ablate Lunique and Lmlp in Table 4. For graph matching, we use RandGraph with k=4. We find that removing either Lunique or Lmlp leads to mixed results on graph matching and worse results in almost all cases for points set registration and rotation estimation.\n\nTable 4: Loss ablations. We report accuracy for graph matching (in %) and mean/median error otherwise.\n\nGraph Matching\n\nw/o Lunique w/o LMLP Ours w/o Lunique 17.8 / 12.0 15.5 / 8.0 18.1 / 11.7 18.3 / 11.7\n\n9 30 11 49\n\n7 30 14 46\n\n9 29 6\n54\n\nPoint Set Registration w/o LMLP 18.6 / 13.4 21.8 / 17.0 19.0 / 7.7 17.8 / 11.7\n\nOurs 15.0 / 8.7 14.5 / 7.7 9.0 / 4.6 18.5 / 7.7\n\nRotation Estimation\n\nw/o Lunique w/o LMLP 3.4 / 3.0 4.6 / 5.0 2.5 / 2.0 4.1 / 4.0\n\n5.1 / 5.0 2.9 / 3.0 3.4 / 3.0 3.7 / 4.0\n\nOurs 3.4 / 3.0 4.2 / 4.0 2.3 / 2.0 3.3 / 3.0\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\n4.3 EVALUATION ON D-WAVE\n\nBy design, our QuAnt is agnostic to the type of QUBO solver used. After training with exhaustive search, we compare how the performance on the test set differs under exhaustive search, SA, or QA. The results in Fig. 4a show that the exact solutions of exhaustive search only slightly outperform the less computationally expensive QA and SA. Moreover SA yields results very similar to QA.\n\n0.2\n\ns t\nl\n\nu s\ne R\n\n0 0\n0 1\n\nn\n\ni\n\ns e\nc n\na t\ns n\nI\n\n2\n\n1 0\n\nES QA SA\n\nEpoch 0\n\n0 2 4 6 810 14\n\nEpoch 245\n\n0 2 4 6 810 14\n\n2\n\n1 0\n\nEpoch 450\n\n0 2 4 6 810 14\n\n2\n\n1 0\n\nf o\n%\n\n0.1\n\n0\n\n0 1 2 3 4 5 6 7 8 9 10 Error in degree\n\n(a) Errors on rotation estimation.\n\n(b) Evolution of result (top) and coupling matrix (bottom).\n\nFigure 4: (a) Histogram of errors for rotation estimation using exhaustive search (ES), quantum annealing (QA), and simulated annealing (SA) at test time. The maximum error on the x-axis amounts to 59 and no methods have higher errors than shown. (b) Evolution over different epochs of the Hamming distance between predicted solutions and the ground truth (top), and coupling matrix when training our approach for graph matching (bottom). (Top): The x-axis shows the Hamming distance. Blue indicates unprojected results, and red means after projection to a permutation. We only project after training.\n\nNext, we compare the test-time results of QA and SA (after training the method with the same technique, QA and SA, respectively). See Table 5 for the results, both with and without projecting the final binary solution to a valid permutation encoding during postprocessing. Training with QA delivers better results than training with SA. We attribute this to a better second-best solution x+ used by Lunique. While SA yields solutions x∗ that are comparable to QA, its second-best solutions are worse than QA’s. We refer to the appendix for details. Unfortunately, real-world compute resources for training with QA remain limited, as of this writing. We, therefore, fall back on SA for larger-scale experiments in this work. However, Table 5 suggests that our results could improve noticeably on QA.\n\nTable 5: QuAnt (L=5, H=78) on RandGraph (k=5) trained for 450 epochs (QA or SA).\n\nBefore projection After projection\n\nSA QA 18 10 36 24\n\n4.4 FURTHER ANALYSIS\n\nTraining. Fig. 4b visualises how the instantaneous solution and A matrix evolve for graph matching.\n\nVarying Problem Difficulty. We provide a more detailed analysis of the performance of our method on point set registration for varying difficulty levels. Table 6 shows that a larger input misalignment between the two point clouds worsens the results, as expected.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Interval analysis for point set registration with L=5, H=78. We evaluate on point cloud pairs with ground-truth angles uniformly sampled within the given intervals. We report the mean/median error in degrees.\n\nangle interval mean / median\n\n0 - 1 π 18 4.0 / 1.9\n\n18 - 2 π 1 π 18 5.1 / 2.6\n\n18 - 3 π 2 π 18 6.4 / 3.0\n\n18 - 4 π 3 π 18 7.9 / 3.8\n\n18 - 5 π 4 π 18 7.9 / 3.8\n\n18 - 6 π 5 π 18 8.4 / 4.0\n\nTable 7: Robustness to varying amounts of incorrect test-time correspondences in rotation estimation. We report mean/median error for L=3, H=32. The first column specifies the percentage of incorrect correspondences at test time.\n\nRobustness to Noise. We investigate the robustness of our method and other approaches against input noise at test time after training without noisy data. We look at rotation estimation, where we randomly pick a fixed percentage of points and randomly permute their correspondences (among themselves). Table 7 contains results. The quality of QuAnt’s results barely degrades with increasing noise levels, even for 20% of incorrect correspondences. QuAnt already outperforms the classical Procrustes for even 1% of incorrect correspondences, even though Procrustes also starts from the same covariance matrix. We observe that the advantage of our method grows with larger noise levels. QuAnt also consistently performs better than the general baselines, which are similarly robust to increasing noise levels.\n\nProcrustes 0.0 / 0.0 5.8 / 3.0 25.7 / 13.0 43.8 / 21.0 64.7 / 58.0 75.3 / 79.0\n\nOurs 3.9 / 4.0 3.4 / 3.0 3.4 / 3.0 3.2 / 3.0 3.5 / 3.0 3.7 / 3.0\n\nPure 8.1 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0\n\nDiag 5.6 / 6.0 5.7 / 6.0 6.0 / 6.0 6.2 / 6.0 6.2 / 6.0 5.8 / 6.0\n\n% 0\n1 5\n10 15 20\n\nWe next look at point set registration under input noise at test time and after training without noise. Here, we add uniform noise to one point cloud, where the range of the noise is a percentage of the maximum extent of the point cloud. Table 8 contains the results. ICP, an iterative approach, is robust to the noise, gives highly accurate results and, thus, outperforms the competing non-iterative approaches. Since QuAnt takes as input the vectorised QUBO that AQM solves, AQM constitutes an upper bound for the performance of our approach. However, QuAnt could, in principle, scale to 3D point set matching while AQM’s solution parametrisation severely inhibits scaling to larger problems. Finally, QuAnt performs better than the general baselines.\n\n5 DISCUSSION\n\nTable 8: Robustness to varying amounts of uniform noise in point set registration. We report mean/median error for L=5, H=78 and the number of logical/physical qubits. The first column states the range of the noise in % of the maximum extent of the point cloud. “†”: uncoupled qubits.\n\n% 0\n5 10 15 20 Qubits\n\nOurs 5.8 / 3.5 6.4 / 3.3 6.5 / 3.3 7.2 / 3.5 10.3 / 5.4 9/14\n\nDiag 7.3 / 4.7 7.0 / 5.2 8.4 / 5.2 9.5 / 5.9 11.6 / 6.6 (9/9)†\n\nPure 6.8 / 5.9 7.0 / 6.1 7.1 / 6.5 7.9 / 6.7 8.2 / 6.8 n/a\n\nAQM 4.3 / 2.6 4.5 / 2.9 5.6 / 3.8 5.6 / 3.8 5.9 / 3.3 21/∼55\n\nLimitations and Future Work. As all learning-based approaches, QuAnt can perform worse on problem instances that fall significantly outside the training distribution. While our general method does not outperform classical methods specialised on certain problem types, we achieve performance on par with hand-crafted QUBO designs used in state-of-the-art QCV methods. We achieve this while greatly reducing the effort required for new problem types. For our point cloud experiments, we rely on minor embeddings to transfer the regressed dense QUBOs to the QA. On existing hardware, large minor embeddings can worsen the resulting quality noticeably. However, we only need to embed a QUBO with nine logical qubits into 14 physical qubits. Although our focus is on a general design, our core idea of learning QUBOs can be specialised to any given problem type by designing a more specific network architecture and losses that capture priors for the problem type.\n\nConclusion. We showed that learning to regress QUBO forms for different problems instead of deriving them analytically can be a reasonable alternative to existing methods. We showed the generality of QuAnt on diverse problem types. Our experiments demonstrated that learning QUBO forms and solving them either on a quantum annealer or with simulated annealing, in most cases, leads to better results than directly regressing solutions. Moreover, QuAnt significantly outperformed the previous quantum state of the art in graph matching and rotation estimation in the setting with noise. We believe our work considerably broadens the available toolbox for development and analysis of quantum computer vision methods and opens up numerous avenues for future research.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nSource code for dwave.system.composites.tiling.\n\nhttps://docs.ocean.dwavesys.\n\ncom/projects/system/en/stable/_modules/dwave/system/composites/ tiling.html. Accessed: 2022-02-09.\n\nTameem Albash and Daniel A Lidar. Adiabatic quantum computation. Reviews of Modern Physics,\n\n90(1):015002, 2018.\n\nFederica Arrigoni, Willi Menapace, Marcel Seelbach Benkner, Elisa Ricci, and Vladislav Golyanik. Quantum motion segmentation. In European Conference on Computer Vision (ECCV), 2022.\n\nKerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J. Osborne, Robert Salzmann, Daniel Scheiermann, and Ramona Wolf. Training deep quantum neural networks. Nature Communications, 11(808), 2020.\n\nJacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd.\n\nQuantum machine learning. Nature, 549, 11 2016.\n\nTolga Birdal, Vladislav Golyanik, Christian Theobalt, and Leonidas Guibas. Quantum permutation\n\nsynchronization. In Computer Vision and Pattern Recognition (CVPR), 2021.\n\nKelly Boothby, Paul Bunyk, Jack Raymond, and Aidan Roy. Next-Generation Topology of D-Wave\n\nQuantum Processors. arXiv e-prints, 2020.\n\nMax Born and Vladimir Fock. Beweis des adiabatensatzes. Zeitschrift f ̈ur Physik, 51(3):165–180,\n\n1928.\n\nEdward Boyda, Saikat Basu, Sangram Ganguly, Andrew Michaelis, Supratik Mukhopadhyay, and Ramakrishna R. Nemani. Deploying a quantum annealing processor to detect tree cover in aerial imagery of california. PLoS ONE, 12, 2017.\n\nJun Cai, William G. Macready, and Aidan Roy. A practical heuristic for finding graph minors. arXiv\n\ne-prints, 2014.\n\nAxel Carlier, Kathryn Leonard, Stefanie Hahmann, Geraldine Morin, and Misha Collins. The 2d shape structure dataset: A user annotated open access database. Computers & Graphics, 58: 23–30, 2016.\n\nGabriele Cavallaro, Dennis Willsch, Madita Willsch, Kristel Michielsen, and Morris Riedel. Approaching remote sensing image classification with ensembles of support vector machines on the d-wave quantum annealer. In IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2020.\n\nTat-Jun Chin, David Suter, Shin-Fang Ch’ng, and James Quach. Quantum robust fitting. In Pro-\n\nceedings of the Asian Conference on Computer Vision (ACCV), November 2020.\n\nMinsu Cho, Karteek Alahari, and Jean Ponce. Learning graphs to match. In International Confer-\n\nence on Computer Vision, pp. 25–32, 2013.\n\nD-Wave Systems. Leap. https://docs.dwavesys.com/docs/latest/leap.html,\n\n2022. online; accessed on the 24.01.2022.\n\nD-Wave Systems, Inc. D-wave system documentation. https://docs.dwavesys.com/\n\ndocs/latest/c_gs_4.html. online; accessed on 2022-03-06.\n\nD-Wave\n\nSystems,\n\nsimulated test-projecttemplate-dimod.readthedocs.io/en/latest/reference/ sampler_composites/samplers.html, 2022a. online.\n\nannealing\n\nsampler.\n\ndimod\n\nInc.\n\nhttps://\n\nD-Wave Systems, Inc. dwave-neal. https://github.com/dwavesystems/dwave-neal,\n\n2022b. online.\n\nD-Wave Systems, Inc. D-wave ocean.\n\nhttps://docs.ocean.dwavesys.com/en/\n\nstable, 2022c. online.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nPrasanna Date and Thomas Potok. Adiabatic quantum linear regression. Scientific Reports, 11,\n\n2021.\n\nNike Dattani, Szilard Szalay, and Nick Chancellor. Pegasus: The second connectivity graph for\n\nlarge-scale quantum annealing hardware. arXiv e-prints, 2019.\n\nB Dema, Junya ARAI, and Keitarou HORIKAWA. Support vector machine for multiclass classifi-\n\ncation using quantum annealers. 2020.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, pp. 248–255. IEEE, 2009.\n\nAnh-Dzung Doan, Michele Sasdelli, David Suter, and Tat-Jun Chin. A hybrid quantum-classical\n\nalgorithm for robust fitting. In Computer Vision and Pattern Recognition (CVPR), 2022.\n\nYuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng Tao. Expressive power of parametrized\n\nquantum circuits. Phys. Rev. Research, 2:033125, 2020.\n\nVedran Dunjko and Hans J Briegel. Machine learning & artificial intelligence in the quantum do-\n\nmain: a review of recent progress. Reports on Progress in Physics, 81(7):074001, 2018.\n\nVedran Dunjko, Jacob M. Taylor, and Hans J. Briegel. Quantum-enhanced machine learning. Phys.\n\nRev. Lett., 117:130501, 2016.\n\nEdward Farhi, Jeffrey Goldstone, Sam Gutmann, Joshua Lapan, Andrew Lundgren, and Daniel Preda. A quantum adiabatic evolution algorithm applied to random instances of an np-complete problem. Science, 292(5516):472–475, 2001.\n\nAaron Ferber, Bryan Wilder, Bistra Dilkina, and Milind Tambe. Mipaal: Mixed integer program as\n\na layer. AAAI Conference on Artificial Intelligence, 34:1504–1511, 2020.\n\nThomas Gabor, Sebastian Feld, Hila Safi, Thomy Phan, and Claudia Linnhoff-Popien. Insights on In Proceedings of the IEEE/ACM 42nd International\n\ntraining neural networks for qubo tasks. Conference on Software Engineering Workshops, pp. 436–441, 2020.\n\nFrank Gaitan and Lane Clark. Graph isomorphism and adiabatic quantum computing. Phys. Rev. A, 89:022342, Feb 2014. doi: 10.1103/PhysRevA.89.022342. URL https://link.aps.org/ doi/10.1103/PhysRevA.89.022342.\n\nVladislav Golyanik and Christian Theobalt. A quantum computational approach to correspondence problems on point sets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9182–9191, 2020.\n\nVojtˇech Havl ́ıˇcek, Antonio D. C ́orcoles, Kristan Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gambetta. Supervised learning with quantum-enhanced feature spaces. Nature, 567:209–212, 2019.\n\nSami Khairy, Ruslan Shaydulin, Lukasz Cincio, Yuri Alexeev, and Prasanna Balaprakash. Learning to optimize variational quantum circuits to solve combinatorial problems. In AAAI Conference on Artificial Intelligence, 2020.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 25, 2012.\n\nJonas M. K ̈ubler, Simon Buchholz, and Bernhard Sch ̈olkopf. The inductive bias of quantum kernels.\n\nIn Neural Information Processing Systems (NIPS), 2021.\n\nYann LeCun, Sumit Chopra, Raia Hadsell, Fu Jie Huang, and et al. A tutorial on energy-based\n\nlearning. In Predicting Structured Data. MIT Press, 2006.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJunde Li and Swaroop Ghosh. Quantum-soft qubo suppression for accurate object detection. In\n\nEuropean Conference on Computer Vision (ECCV), 2020.\n\nBaoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 806–814, 2015.\n\nOwen Lockwood and Mei Si. Reinforcement learning with quantum variational circuit. AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE), 16:245–251, 2020.\n\nFeng Lu and Evangelos Milios. Robot pose estimation in unknown environments by matching 2d\n\nrange scans. Journal of Intelligent and Robotic systems, 18(3):249–275, 1997.\n\nNicola Mariella and Andrea Simonetto. A Quantum Algorithm for the Sub-Graph Isomorphism\n\nProblem. arXiv e-prints, 2021.\n\nKevin Mato, Riccardo Mengoni, Daniele Ottaviani, and Gianluca Palermo. Quantum molecular\n\nunfolding. arXiv e-prints, 2021.\n\nJarrod R. McClean, Sergio Boixo, Vadim N. Smelyanskiy, Ryan Babbush, and Hartmut Neven. Barren plateaus in quantum neural network training landscapes. Nature Communications, 9(4812), 2018.\n\nCatherine C. McGeoch. Adiabatic quantum computation and quantum annealing: Theory and prac-\n\ntice. Synthesis Lectures on Quantum Computing, 5(2):1–93, 2014.\n\nK. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. Quantum circuit learning. Phys. Rev. A, 98:\n\n032309, 2018.\n\nFlorian Neukart, Gabriele Compostella, Christian Seidel, David von Dollen, Sheir Yarkoni, and Bob\n\nParney. Traffic flow optimization using a quantum annealer. Frontiers in ICT, 4, 2017.\n\nHartmut Neven, Vasil S. Denchev, Geordie Rose, and William G. Macready. Training a binary\n\nclassifier with the quantum adiabatic algorithm. arXiv e-prints, 2008a.\n\nHartmut Neven, Geordie Rose, and William G. Macready.\n\nImage recognition with an adiabatic quantum computer i. mapping to quadratic unconstrained binary optimization. arXiv e-prints, 2008b.\n\nNga T. T. Nguyen and Garrett T. Kenyon.\n\nImage classification using quantum inference on the\n\nd-wave 2x. arXiv e-prints, 2019.\n\nMohammadreza Noormandipour and Hanchen Wang. Matching point sets with quantum circuit\n\nlearning. arXiv e-prints, 2102.06697, 2021.\n\nJonas N ̈ußlein, Christoph Roch, Thomas Gabor, Claudia Linnhoff-Popien, and Sebastian arXiv preprint\n\nFeld. Black box optimization using qubo and the cross entropy method. arXiv:2206.12510, 2022.\n\nT. P. Orlando, J. E. Mooij, Lin Tian, Caspar H. van der Wal, L. S. Levitov, Seth Lloyd, and J. J.\n\nMazo. Superconducting persistent-current qubit. Phys. Rev. B, 60:15398–15413, Dec 1999.\n\nRoman Orus, Samuel Mugel, and Enrique Lizaso. Forecasting financial crashes with quantum com-\n\nputing. Physical Review A, 99, 06 2019.\n\nDavide Pastorello and Enrico Blanzieri. Quantum annealing learning search for solving qubo prob-\n\nlems. Quantum Information Processing, 18(10):1–17, 2019.\n\nDavide Pastorello, Enrico Blanzieri, and Valter Cavecchia. Learning adiabatic quantum algorithms\n\nover optimization problems. Quantum Machine Intelligence, 3(1):1–19, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019.\n\nRigetti Computing. Rigetti quantum cloud services. https://docs.rigetti.com/qcs/,\n\n2022. online; accessed on the 24.01.2022.\n\nMichal Rol ́ınek, Paul Swoboda, Dominik Zietlow, Anselm Paulus, V ́ıt Musil, and Georg Martius. Deep graph matching via blackbox differentiation of combinatorial solvers. In European Conference on Computer Vision (ECCV), pp. 407–424, 2020.\n\nMichele Sasdelli and Tat-Jun Chin. Quantum annealing formulation for binary neural networks.\n\narXiv preprint arXiv:2107.02751, 2021.\n\nMarcel Seelbach Benkner, Vladislav Golyanik, Christian Theobalt, and Michael Moeller. Adiabatic quantum graph matching with permutation matrix constraints. In International Conference on 3D Vision (3DV), 2020.\n\nMarcel Seelbach Benkner, Zorah L ̈ahner, Vladislav Golyanik, Christof Wunderlich, Christian Theobalt, and Michael Moeller. Q-match: Iterative shape matching via quantum annealing. In International Conference on Computer Vision (ICCV), 2021.\n\nSukin Sim, Peter D. Johnson, and Al ́an Aspuru-Guzik. Expressibility and entangling capability of parameterized quantum circuits for hybrid quantum-classical algorithms. Advanced Quantum Technologies, 2(12):1900070, 2019.\n\nVincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Proc. NeurIPS, 2020.\n\nStefano Speziali, Federico Bianchi, Andrea Marini, Lorenzo Menculini, Massimiliano Proietti, Loris Francesco Termite, Alberto Garinei, Marcello Marconi, and Andrea Delogu. Solving sensor placement problems in real water distribution networks using adiabatic quantum computation. Quantum Computing and Engineering (QCE), 2021.\n\nTobias Stollenwerk, Elisabeth Lobe, and Martin Jung. Flight gate assignment with a quantum an-\n\nnealer. In Quantum Technology and Optimization Problems, 2019.\n\nAlexander Teplukhin, Brian K. Kendrick, and Dmitri Babikov. Calculation of molecular vibrational\n\nspectra on a quantum annealer. Journal of chemical theory and computation, 2019.\n\nThinkLab. pygmtools. Shanghai Jiao Tong University. Github, 2021. URL https://github.\n\ncom/Thinklab-SJTU/pygmtools.\n\nLorenzo Torresani, Vladimir Kolmogorov, and Carsten Rother. Feature correspondence via graph In European conference on computer vision, pp.\n\nmatching: Models and global optimization. 596–609. Springer, 2008.\n\nSalvador E. Venegas-Andraca and Sougato Bose. Storing, processing, and retrieving an image using quantum mechanics. In Quantum Information and Computation, volume 5105, pp. 137 – 147, 2003.\n\nGuillaume Verdon, Jason Pye, and Michael Broughton. A universal training algorithm for quantum\n\ndeep learning. In APS March Meeting Abstracts, 2019.\n\nMarin Vlastelica, Anselm Paulus, V ́ıt Musil, Georg Martius, and Michal Rol ́ınek. Differentiation In International Conference on Learning Representations\n\nof blackbox combinatorial solvers. (ICLR), 2020.\n\nD. Willsch, M. Willsch, H. De Raedt, and K. Michielsen. Support vector machines on the d-wave quantum annealer. Computer Physics Communications, 248:107006, 2020. ISSN 0010-4655. doi: https://doi.org/10.1016/j.cpc.2019.107006. URL https://www.sciencedirect.com/ science/article/pii/S001046551930342X.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1912–1920, 2015.\n\nFei Yan, Abdullah Iliyasu, and Salvador Venegas-Andraca. A survey of quantum image representa-\n\ntions. Quantum Information Processing, 15, 1 2016.\n\nAlp Yurtsever, Tolga Birdal, and Vladislav Golyanik. Q-fw: A hybrid classical-quantum frank-wolfe for quadratic binary optimization. In European Conference on Computer Vision (ECCV), 2022.\n\nJan-Nico Zaech, Alexander Liniger, Martin Danelljan, Dengxin Dai, and Luc Van Gool. Adiabatic Quantum Computing for Multi Object Tracking. In Computer Vision and Pattern Recognition (CVPR), 2022.\n\nKenneth M. Zick, Omar Shehab, and Matthew French. Experimental quantum annealing: case study\n\ninvolving the graph isomorphism problem. Scientific reports, 5(1), 2015.\n\nAPPENDIX\n\nThis appendix provides more details on adiabatic quantum computing in Sec. A. We provide training and implementation settings in Sec. B. Further details on the problem description for graph matching and a failure case are in Sec. C. Sec. D contains a deeper comparison with QGM (Seelbach Benkner et al., 2020) and Sec. E compares SA and QA. In Sec. F, we provide further quantitative and qualitative results on rotation estimation. Finally, Sec. G contains more details and experiments on point set registration.\n\nA QUANTUM COMPUTING BACKGROUND\n\nA.1 QUANTUM ANNEALING IN DETAIL\n\nAs we have seen, quantum annealing is a metaheuristic to solve the N P-hard Ising problem:\n\narg min s∈{−1,1}n\n\ns⊤Js + b⊤s,\n\n(6)\n\nwhere s is a binary vector, J ∈ Rn×n is a matrix of couplings, and b ∈ Rn contains biases (McGeoch, 2014). Here, we give a brief overview of how this fits in the framework of quantum mechanics. D-Wave quantum annealers rely on magnetic fluxes in superconducting Niobium loops (Orlando et al., 1999). The direction of the current flowing through them can be modelled as a qubit, i.e., as a two-dimensional, complex, normalised vector |ψ⟩ ∈ C2 in the Dirac notation. In the so-called computational basis, the basis vectors correspond to the current flowing clockwise or anti-clockwise. After measuring the state, the system will collapse to either basis state. The absolute value of the complex-valued coefficients of the linear combination (probability amplitudes) is the probability of each outcome after measurement (e.g., clockwise or anti-clockwise current). The state C2 and is thus a 2n-dimensional space of n ∈ N qubits can be expressed with the tensor product\n\nn (cid:78)\n\ncomplex vector space. We need that many parameters because entangled states cannot be described separately. If the two states |ψ⟩ , |φ⟩ corresponding to different physical systems (e.g., two niobium loops or two atoms) can be described independent from each other, the whole system is described by |ψ⟩ ⊗ |φ⟩. Note that if every state could be decomposed this way, one would only need 2n parameters.\n\ni=1\n\nThe evolution of a quantum state |ψ⟩ over time can be described with the time-dependent Schr ̈odinger equation:\n\nH |ψ⟩ = iħ ∂\n\n∂t\n\n|ψ⟩ ,\n\n(7)\n\nwhere the Hamilton operator H is a Hermitian Matrix describing the possible energies of the system, i is the imaginary unit, ħ is a constant, and t denotes time. For adiabatic quantum computing, one needs a problem Hamiltonian HP , where the eigenvector corresponding to the lowest eigenvalue is\n\n14\n\nPublished as a conference paper at ICLR 2023\n\na solution to the particular Ising problem, and an initial Hamiltonian HI with an easy-to-prepare ground state. The Adiabatic Theorem (Born & Fock, 1928) states that if we start with the ground state of HI and take a sufficiently long time τ to gradually change from HI to HP , e.g., with:\n\nH(t) = (1 −\n\nt τ\n\n)HI +\n\nt τ\n\nHP ,\n\n(8)\n\nthen we end up in the ground state of HP . From the latter, we can deduce the solution of the particular Ising problem. Simulating this whole process classically can be difficult (or even intractable) because we are dealing with 2n × 2n matrices HI and HP , where n is the number of qubits. How difficult the classical simulation is, depends on the exact form of the Hamiltonians. (Particularly promising for speed-ups are, e.g., so-called non-stoquastic Hamiltonians (Albash & Lidar, 2018).)\n\nA.2 LOGICAL AND PHYSICAL QUBITS\n\nThe QUBO defines the couplings between two logical qubits i and j. Such a QUBO can contain couplings between any two qubits. However, in contemporary hardware realisations, each physical qubit is only connected to a few others (see Fig. 5a and Fig. 5b). In the main paper, we show how the QUBO matrix A can account for this sparse connectivity pattern by setting entries between logical qubits i and j to 0 if the physical qubits i and j have no connection. Still, D-Wave supports denser connectivity patterns than what is implied by the hardware: Multiple physical qubits can be chained together to represent a single logical qubit of x that has many connections. The physical qubits in the chain will then have strong couplings along the chain to encourage them to all end up in the same final state (either all 0 or all 1), representing the final state of the corresponding logical qubit. This is formalised as a minor embedding (of the connectivity graph of the logical qubits) into the connectivity graph of the physical qubits. Using the heuristic method of Cai and colleagues (Cai et al., 2014) is popular to determine the minor embeddings in practice.\n\n(a) Chimera\n\n(b) Pegasus\n\nFigure 5: Visualisation of qubit connectivities. (a) The connectivity pattern of the physical qubits in the Chimera architecture. The unit cells (green boxes) have fewer interconnections than on Pegasus. (b) The connectivity pattern of the physical qubits in the Pegasus architecture. Green, red, and yellow correspond to one problem instance each. Images due to D-Wave (D-Wave Systems, Inc.).\n\nB IMPLEMENTATION DETAILS\n\nOur code, which we will release, is implemented in Pytorch (Paszke et al., 2019). We use Adam (Kingma & Ba, 2014) with a learning rate of 10−3 for training. For graph matching on RandGraph with k=4, we use a batch size of 141 and train for 150 epochs, which takes about seven hours. For RandGraph k=5, we use 450 epochs, which takes about 23 hours. For Willow, we train for 300 epochs, which takes about 14 hours. The baselines are trained for the same number of epochs. While Diag takes a comparable amount of time, the Pure baseline takes about three minutes to train. For the experiments with k=5, we use 10−5 as the learning rate.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nFor the point cloud experiments, we use a batch size of 32 and train for 20 epochs, which takes about four hours. We train Pure and Diag with the same batch sizes and epochs. The training of the Diag baseline takes four hours as well, and Pure is trained within three hours. When solving a QUBO on a QA, we anneal 100 times and pick the lowest-energy solution. We access the QA via Leap 2 (D-Wave Systems, 2022) using the Ocean SDK (D-Wave Systems, Inc., 2022c). When solving with SA, we use 100 iterations from the default neal SA sampler.\n\nC GRAPH MATCHING\n\nC.1 PROBLEM DESCRIPTION\n\nHere, we describe the design of the problem description p for graph matching. We use p = vec(W), where the diagonal of W contains cosine similarities between the feature vectors extracted with AlexNet (Krizhevsky et al., 2012) pre-trained on ImageNet (Deng et al., 2009) of all pairs of key points. The off-diagonal follows the geometric term described in (Eq. (7)) from Torresani et al. (Torresani et al., 2008). In particular, we use the term Wgeom from Eq. (7) from Torresani et al. (Torresani et al., 2008) with minus signs in the beginning and in the exponential, and set η=0.98. The convex combination with WAlex, where the cosine similarities of the feature vectors are on the diagonal, is then:\n\nW = τWAlex + (1 − τ )Wgeom. We choose τ =0.81 and η=0.98 such that the QAP often coincides with the ground-truth correspondences (see Table 1b, “Direct” from the main paper).\n\n(9)\n\nC.2 FAILURE CASE\n\nWe show a failure case of our method when applied to graph matching in Fig. 6. It occurs due to large differences in the observed appearance.\n\n(a) Source\n\n(b) Matching\n\nFigure 6: Failure case for graph matching. We visualise the ground truth of an image pair. Here, our method does not find the correct matching: Only the beak and neck are matched correctly, while the geometric information for the other key points differs too strongly.\n\nD DETAILED COMPARISON WITH QGM (SEELBACH BENKNER ET AL.,\n\n2020)\n\nHere, we compare our method with QGM (Seelbach Benkner et al., 2020) in detail. Note that the focus of both works differs. Their work focuses more on the probability distribution of the retrieved solutions. Our work is more concerned with incorporating the quantum annealer into the training pipeline. When training the neural network, Lgap equation 1 uses the retrieved solution with the smallest energy across anneals, while they are also interested in the success probabilities, i.e., the probability to get the best solution across anneals.\n\nThe individual QUBOs occurring in our QuAnt framework are much easier to solve by the QA than QUBOs that would arise in QGM (Seelbach Benkner et al., 2020). To show this, we compute the average success probabilities of the various methods from QGM (Seelbach Benkner et al., 2020)\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 9: Average success probabilities of different QGM variants (Seelbach Benkner et al., 2020) over 141 problem instances on RandGraph compared to QuAnt with k = 5, in %.\n\nInserted Baseline\n\nRow-wise Ours\n\n0.22\n\n0.07\n\n0.07\n\n26\n\nover 141 instances of RandGraph with k=5; see Table 9. We also apply QuAnt to these problem instances. We solve the resulting QUBO with QA and find the average probability to be 26% with a standard deviation of 18%, better than any method from the QGM paper (Seelbach Benkner et al., 2020).\n\nThis difference is not surprising since we construct our method such that we only use trivial embeddings and do not need to apply the minorminer heuristic (Cai et al., 2014). Because of that, for RandGraph with k = 5, our method needs only 15 physical qubits while their baseline and row-wise methods need 89 qubits, on average, and a chain length of 4; their Inserted method needs, on average, 39 qubits and a chain length of 3 on D-Wave Advantage. Note that a heuristic search for better penalty parameters, as in Q-Sync (Birdal et al., 2021), could give rise to better results for the methods from QGM (Seelbach Benkner et al., 2020) in Table 9. However, the corresponding embeddings would still be problematic. Directly using the binary encoding for permutations (Gaitan & Clark, 2014) requires additional qubits because the problem would a priori not be quadratic.\n\nE SOLUTION QUALITY OF SA AND QA\n\nIn the main paper, we show that training with QA yields better performance than training with SA. Here, we analyse the quality of the solutions found by both techniques further. Fig. 7 contains histograms that depict the output of the quantum annealer and the two different simulated annealing solvers from neal (D-Wave Systems, Inc., 2022b) and from dimod (D-Wave Systems, Inc., 2022a). In contrast to the solver from dimod, neal is highly optimised for performance, so we used it for our experiments.\n\nWe focus our analysis on the number of sweeps in SA, i.e., the number of steps in the ’cooling’ schedule. We observe that it strongly influences the quality of the second-best solution.\n\n) 0\n0 1\n×\n\n(\n\n10\n\n5\n\ns e\nc n\ne r\nr u\nc c\no\n\n10\n\n5\n\n10\n\n5\n\n10\n\n5\n\n10\n\n5\n\n0 −1 −0.5 0 Energy\n\n0.5\n\n0 −1 −0.5 0 Energy\n\n0.5\n\n0 −1 −0.5 0 Energy\n\n0.5\n\n0 −1 −0.5 0 Energy\n\n0.5\n\n0 −1 −0.5 0 Energy\n\n0.5\n\n(a) SA, 103 sweeps\n\n(b) SA, 10 sweeps\n\n(c) SA, 2 sweeps\n\n(d) Dimod\n\n(e) QA\n\nFigure 7: Energy histograms of (ideally optimal) 103 samples for SA (different number of sweeps and dimod) and QA on one instance of RandGraph with k=5 after 450 epochs training.\n\nTable 10 illustrates this by averaging the fraction of the second-best energies over 141 instances and analysing 1000 samples from different solvers. We see that the quantum annealer produces the second-best samples with the lowest energies.\n\nNote, however, that we do not claim that this is an intrinsic general advantage of QA over SA, but merely that in our setting, QA outperforms SA. Still, prior work (Willsch et al., 2020) also reaches the conclusion that quantum annealing has much potential for finding reasonable near-optimal solutions.\n\nThe dimod sampler also produces second-best solutions with low energies but is computationally expensive (D-Wave Systems, Inc., 2022a). This is, perhaps, because many non-optimal solutions are produced compared to the implementation from neal.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable 10: Second-best energies of SA relative (in %) to the second-best energies of QA. We report the mean and std. deviation over 141 instances. The higher the better.\n\nSA (neal), 103 sweeps 94.2± 10.3\n\nSA (neal), 10 sweeps 86.0 ± 19\n\nSA (dimod) 99.8 ± 0.9\n\nF 3D ROTATION ESTIMATION\n\nF.1 THREE STAGES (EULER ANGLES)\n\nWe obtain improved results when regressing three Euler angles one after another compared to direct regression of three angles. Thus, we use one stage per angle, i.e., with one network per stage. The training setup is as follows. We feed the first network with problem instances where α, β, γ ̸= 0. The network then regresses α. We feed the second stage network with problem instances, where α = 0 and β, γ ̸= 0. Subsequently, the network regresses β. Lastly, we feed the third network with problem instances, where α, β = 0 and γ ̸= 0. Here it regresses γ. During test time, the first network determines α, which is then applied to the input. The updated input is re-encoded before the second stage, which outputs β. Finally, with α and β applied already, the third stage regresses γ. Fig. 8 shows how the three different networks regress the angles and how the solution progresses towards the final one.\n\n(a) Initial problem instance\n\n(b) Application of the α rotation\n\n(c) Application of the α, β rotations\n\n(d) Application of the α, β, γ rotations\n\nFigure 8: Visualisation of the different steps of our rotation-estimation network. We show (blue) the original 3D point cloud and (red) the rotated point cloud. The green points are points of the red point cloud with unknown correspondences. Here, 10% of the correspondences are unknown.\n\nF.2 VARIANCE ACROSS RUNS\n\nTo better judge the stability under different random seeds, we repeat the main experiment from the paper three times for our QuAnt method and each baseline. In Table 11, we report the mean and std. deviation of the median. Here, similar to the results from the main paper, we outperform the Diag and Pure baselines in all but one setting.\n\nF.3 TRAINING ON NOISY DATA\n\nTable 11: Comparison to general baselines on rotation estimation. We report the mean of the per-experiment median and std. deviation across experiments for three different random seeds.\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\nOurs 6.0 ± 3.6 4.0 ± 1.0 3.7 ± 1.2 3.7 ± 0.6\n\nDiag 5.3 ± 1.1 5.0 ± 0.0 5.0 ± 0.0 5.0 ± 0.0\n\nPure 7.0 ± 1.0 7.0 ± 0.0 16.3 ± 7.2 9.0 ± 1.0\n\nTable 12 shows how our method performs on noise-free and noisy test data after training on noisy data. We observe that the noisy training data appears to negatively affect the training and its performance drops, while Diag improves and Pure remains unchanged.\n\nF.4 QUALITATIVE COMPARISON ON NOISY TEST DATA\n\nFig. 9 visualises differences between our solution after training on noise-free data and Procrustes alignment on a problem with noisy data (unknown correspondences).\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTable 12: Robustness to varying amounts of incorrect test-time correspondences in rotation estimation. We report the mean/median error for L=3, H=32. The first column specifies the percentage of incorrect correspondences at test time.\n\n(a) Training without noise\n\n(b) Training with 10% incorrect correspondences\n\n% 0\n1 5\n10 15 20\n\nOurs 3.9 / 4.0 3.4 / 3.0 3.4 / 3.0 3.2 / 3.0 3.5 / 3.0 3.7 / 3.0\n\nProcrustes 0.0 / 0.0 5.8 / 3.0 25.7 / 13.0 43.8 / 21.0 64.7 / 58.0 75.3 / 79.0\n\nDiag 5.6 / 6.0 5.7 / 6.0 6.0 / 6.0 6.2 / 6.0 6.2 / 6.0 5.8 / 6.0\n\nPure 8.1 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0\n\n% 0\n1 5\n10 15 20\n\nOurs 4.4 / 4.0 4.3 / 4.0 5.0 / 5.0 5.2 / 5.0 5.2 / 5.0 5.6 / 6.0\n\nProcrustes 0.0 / 0.0 5.8 / 3.0 25.7 / 13.0 43.8 / 21.0 64.7 / 58.0 75.3 / 79.0\n\nDiag 4.6 / 5.0 5.1 / 5.0 5.1 / 5.0 5.2 / 5.0 5.0 / 5.0 4.9 / 5.0\n\nPure 8.1 / 8.0 8.3 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0 8.2 / 8.0\n\n(a) Initial problem setting\n\n(b) Our solution\n\n(c) Solution by Procrustes\n\nFigure 9: Comparison of initial input rotation, our method and Procrustes. The initial 3D point cloud is blue and the rotated one is red, where the unknown correspondences are displayed in green. Here, 10% of the correspondences are unknown.\n\nF.5 COMPARISON TO AQM (GOLYANIK & THEOBALT, 2020)\n\nQuAnt can estimate 3D rotations with known point matches. However, AQM (Golyanik & Theobalt, 2020) would require 81 densely connected logical qubits, which is not supported by the current quantum-hardware generations. Hence, we cannot compare against AQM for this problem setting and instead only compare to classical methods such as Procrustes.\n\nG 2D POINT SET REGISTRATION\n\nG.1 SETTING DETAILS\n\nWe encode the point set registration instances similar to Golyanik et al. (Golyanik & Theobalt, 2020). As the correspondences between the template and the reference in point set registration are not known, we use k-nearest neighbours to find possible correspondences. Our network is trained with three nearest neighbours per each template point.\n\nG.2 VARIANCE\n\nIn Table 13, we report the mean median. Here, we outperform the baselines in all cases. In nearly all setups, we outperform the baselines, and only for one case, we are on par with the Pure baseline. Still, even in that case, QuAnt performs more consistently, as evidenced by its lower std. deviation. These experiments show that we consistently outperform the baselines and the performance is not dependent on the random seed.\n\nTable 13: Comparison of QuAnt to general baselines on point set registration. We report the mean of the per-experiment median and std. deviation across experiments for three different random seeds.\n\nL=3, H=32 L=3, H=78 L=5, H=32 L=5, H=78\n\nOurs 4.8 ± 0.6 3.7 ± 0.3 4.6 ± 0.1 3.4 ± 0.1\n\nDiag 6.8 ± 0.2 5.1 ± 0.4 7.1 ± 1.0 4.9 ± 0.0\n\nPure 5.8 ± 0.9 4.8 ± 0.3 7.1 ± 1.6 7.9 ± 2.7\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nTable 14: Robustness to varying amounts of uniform noise. We report the mean/median error for L=3, H=32. The first column specifies the range of the added uniform noise, in %, of the maximum extent of the point cloud.\n\n% 0\n5 10 15 20\n\nOurs 7.4 / 4.2 6.7 / 3.8 7.2 / 4.5 8.2 / 4.9 11.0 / 6.0\n\nDiag 11.3 / 7.0 11.7 / 7.3 12.2 / 6.8 12.6 / 6.8 13.9 / 8.2\n\nPure 7.8 / 6.6 8.0 / 6.8 8.6 / 7.0 9.7 / 8.0 14.3 / 10.4\n\nAQM 4.3 / 2.6 4.5 / 2.9 5.6 / 3.8 5.6 / 3.8 5.9 / 3.3\n\nG.3 NOISE\n\nIn addition to the experiments in the main paper that uses the largest architecture, we also test the noise resistance on the smallest network setup with L = 3, H = 32; see Table 14. Here, while QuAnt is better than the baselines, we do not outperform AQM. However, by construction, AQM is an upper bound for our method as the matrix introduced by Golyanik et al. (Golyanik & Theobalt, 2020) is the same as our input into the network, but it gets directly solved by the QA.\n\nG.4 QUALITATIVE ABLATION RESULTS\n\nIn addition to the quantitative loss ablation in the paper, we visualise the effect of the losses here. In Figure 10, the full loss results in a nearly ground-truth rotation. However, if we leave out Lgap or LMLP during training, a significant reduction in rotational accuracy is visible.\n\n(a) Initial problem instance\n\n(b) Ours with the full loss\n\n(c) Ours without LMLP\n\n(d) Ours without Lunique\n\nFigure 10: Qualitative loss ablations. We show the original point cloud (blue) and rotated point cloud (red). Removing either Lunique or Lmlp leads to significantly worse results.\n\nG.5 FAILURE CASE\n\nWe continue our analysis with failure cases. Results in the main paper show that an increasing input angle leads to a reduction in the accuracy of our regressed angle. This can be traced back to our problem-instance encoding as Golyanik et al. (Golyanik & Theobalt, 2020) mention that an increasing angle makes finding the correspondences more error-prone. Therefore, an imperfect input encoding makes it is also more likely for us to regress wrong angles.\n\nSimilar to most prior work on point set registration, nearly symmetric shapes can be difficult, as most points can be nearly perfectly aligned even with wrong rotations. Fig. 11 contains such a failure case. The initial angle of this problem instance, 50.8◦, is relatively large for our setup, and the shape (which looks like the silhouette of a fish) is nearly rotationally symmetric. In cases like this, our method has difficulties regressing the correct rotation after a single QUBO sampling.\n\n20\n\n21012210121.51.00.50.00.51.01.52.02.01.51.00.50.00.51.01.51.51.00.50.00.51.01.52.0210121.51.00.50.00.51.01.52.021012Published as a conference paper at ICLR 2023\n\n(a) Initial problem instance\n\n(b) Failing to regress correct rotation\n\nFigure 11: Example of a failure case in point set registration. We show the initial image (blue) as well as the rotated image (red).\n\n21\n\n1.51.00.50.00.51.01.51.51.00.50.00.51.01.51.51.00.50.00.51.01.51.51.00.50.00.51.01.5",
    "reference": "# Summary Of The Paper\n\nThis paper tackles the problem of generating QUBO forms from an unspecified problem instance. It learns a low-energy formulation of quadratic unconstrained binary optimization problems with multilayer perceptron and gradient backpropagation, instead of problem-specific analytical derivations.  After learning a QUBO form, they solve it with existing quantum annealing techniques. Their model excels previous models on some benchmarks both in computational expenditure and performance.\n\nChallenge: \n\nDepth: A new training strategy for neural methods with backpropagation, independent of the solver.\nResults:\n-Task: graph matching, 2D point cloud alignment, and 3D rotation estimation.\n-Metric: Accuracy, and the number of qubits of the forms obtained by the proposed algorithms.\n\n# Strength And Weaknesses\n\nPros:\n- The idea of finding the forms of QUBO with gradient is interesting.\n- A wide variety of problems can be tackled successfully and competitively by the proposed general quantum approach.\n- The proposed algorithm finds formulation with fewer qubits than state-of-the-art.\n\nCons:\n- The selected baselines are not strong enough. It is not clear how the selected baselines, Diag and Pure measure the quality of the solution. The author should compare to state-of-the-art.\n- The comparisons with specialized methods are limited. \n- The synthetic data are relatively small, e.g., random matrix 4 by 4 and 5 by 5.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nAbout Organization & Presentation:\n- The presentation is succinct and cogent. Abundant visual demonstrations are provided to aid in comprehension. The results are presented clearly in tables and graphs.\n- The paper is organized in a coherent and logical manner.\n\nAbout Novelty:\n- A new neural meta-learning approach to obtain QUBO forms executable on modern quantum annealing algorithms for computer-vision problems.\n-This paper is not very novel in its technicality, nor in its application of MLP. Nonetheless, it is commendable to take a general problem by learning its form and transforming it into the ones we have solutions to (rather than attempting to solve it directly).\n\n# Summary Of The Review\n\nAbout Challenge:\n- Obtaining suitable QUBO forms in computer vision remains challenging and currently requires problem-specific analytical derivations.\n- The MLP is trained with the results from AQM, resulting in its performance bounded by AQM. Given such deficiency, is it really fair to conclude that QUBO learning is capable to deal with a general problem?\n- The results are not impressive. Their MLP surprisingly fails to beat binary regression (“pure”) in some cases. Is it an isolated incident that QUBO learning beats some SOTA? \n\nAbout Contribution:\n-The paper contributes to the area by looking beyond the traditional approach of deriving and solving QUBO from a specific problem. It addresses a bigger picture concerning how to identify and classify a mission. By aiming at generality, it extends the application of quantum annealing as well.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nINCREMENTAL PREDICTIVE CODING: A PARALLEL AND FULLY AUTOMATIC LEARNING ALGORITHM\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNeuroscience-inspired models, such as predictive coding, have the potential to play an important role in the future of machine intelligence. However, they are not yet used in industrial applications due to some limitations, one of them being the lack of efficiency. In this work, we address this by proposing incremental predictive coding (iPC), a variation of the original framework derived from the incremental expectation maximization algorithm, where every operation can be performed in parallel without external control. We show both theoretically and empirically that iPC is more efficient than the original algorithm by Rao and Ballard 1999, while maintaining performance comparable to backpropagation in image classification tasks. This work impacts several areas, has general applications in computational neuroscience and machine learning, and specific applications in scenarios where automatization and parallelization are important, such as distributed computing and implementations of deep learning models on analog and neuromorphic chips.\n\n1\n\nINTRODUCTION\n\nIn recent years, deep learning has reached and surpassed human-level performance in a multitude of tasks, such as game playing (Silver et al., 2017; 2016), image recognition (Krizhevsky et al., 2012; He et al., 2016), natural language processing (Chen et al., 2020), and image generation (Ramesh et al., 2022). These successes are achieved entirely using deep artificial neural networks trained via backpropagation (BP), which is a learning algorithm that is often criticized for its biological implausibilities (Grossberg, 1987; Crick, 1989; Abdelghani et al., 2008; Lillicrap et al., 2016; Roelfsema & Holtmaat, 2018; Whittington & Bogacz, 2019), such as lacking local plasticity and autonomy. In fact, backpropagation requires a global control signal required to trigger computations, since gradients must be sequentially computed backwards through the computation graph. These properties are not only important for biological plausibility: parallelization, locality, and automation are key to build efficient models that can be trained end-to-end on non Von-Neumann machines, such as analog chips (Kendall et al., 2020). A learning algorithm with most of the above properties is predictive coding (PC).\n\nPC is an influential theory of information processing in the brain (Mumford, 1992; Friston, 2005), where learning happens by minimizing the prediction error of every neuron. PC can be shown to approximate backpropagation in layered networks (Whittington & Bogacz, 2017), as well as on any other model (Millidge et al., 2020), and can exactly replicate its weight update if some external control is added (Salvatori et al., 2022b). Also the differences with BP are interesting, as PC allows for a much more flexible training and testing (Salvatori et al., 2022a), has a rich mathematical formulation (Friston, 2005; Millidge et al., 2022), and is an energy-based model (Bogacz, 2017). This makes PC unique, as it is the only model that jointly allows training on neuromorphic chips, is an implementation of influential models of cortical functioning in the brain, and can match the performance of backpropagation in different tasks. Its main drawback, however, is the efficiency, as it is slower than BP. In this work, we address this problem by proposing a variation of PC that is much more efficient than the original one.\n\nSimply put, PC is based on the assumption that brains implement an internal generative model of the world, needed to predict incoming stimuli (or data) (Friston et al., 2006; Friston, 2010; Friston et al., 2016). When presented with a stimulus that differs from the prediction, learning happens by\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nupdating internal neural activities and synapses to minimize the prediction error. In computational models, this is done via multiple expectation-maximization (EM) (Dempster et al., 1977) steps on the variational free energy, in this case a function of the total error of the generative model. During the Estep, internal neural activities are updated in parallel until convergence; during the M-step, a weight update to further minimize the same energy function is performed. This approach results in two limitations: first, the E-step is slow, as it can require dozens of iterations before convergence; second, an external control signal is needed to switch from the E to M step. In this paper, we show how to address both of these problems by considering a variation of the EM algorithm, called incremental expectation-maximization (iEM), which performs both E and M steps in parallel (Neal & Hinton, 1998). This algorithm is provably faster, does not require a control signal to switch between the two steps, and has solid convergence guarantees (Neal & Hinton, 1998; Karimi et al., 2019). What results is a training algorithm that we call incremental predictive coding (iPC) that is a simple variation of PC that addresses the main drawback of PC (namely, efficiency), with no drawbacks from the learning perspective, as it has been formally proven to have equivalent convergence properties to standard PC. Furthermore, we provide initial evidence that iPC is also potentially more efficient than BP in the specific case of full-batch training. In fact, we theoretically show that, on an ideal parallel machine, to complete one update of all weights on a network with L layers, the time complexity of iPC is O(1), while that of BP is O(L). However, additional engineering efforts are needed to reach this goal, which are beyond the focus of this work: our experiments are performed using PyTorch (Paszke et al., 2017), which is not designed to parallelize computations across layers on GPUs. We partially address this limitation by performing some experiments on CPUs, which empirically confirm our claims about efficiency, as shown in Fig. 3.Our contributions are briefly as follows:\n\n1. We first develop the update rule of iPC from the variational free energy of a hierarchical generative model using the incremental EM approach. We then discuss the implications of this change in terms of autonomy and convergence guarantees: it has in fact been proven that iEM converges to a minimum of the loss function (Neal & Hinton, 1998; Karimi et al., 2019), and hence this result naturally extends to iPC. We conclude by analyzing similarities and differences between iPC, standard PC, and BP.\n\n2. We empirically compare the efficiency of PC and iPC on generation tasks, by replicating some experiments performed in (Salvatori et al., 2021), and classification tasks, by replicating experiments similar to those presented in (Whittington & Bogacz, 2017). In both cases, iPC is by far more efficient than the original counterpart. Furthermore, we present initial evidence that iPC can decrease the training loss faster than BP, assuming that a proper parallelization is done.\n\n3. We then test our model on a large number of image classification benchmarks, showing that that iPC performs better than PC, on average, and similarly to BP. Then, we show that iPC requires less parameters than BP to perform well on convolutional neural networks (CNNs). Finally, we show that iPC follows the trends of energy-based models on training robust classifiers (Grathwohl et al., 2019), and yields better calibrated outputs than BP on the best performing models.\n\n2 PRELIMINARIES\n\nIn this section, we introduce the original formulation of predictive coding (PC) as a generative model proposed by Rao and Ballard 1999. Let us consider a generative model g : Rd × RD −→ Ro, where x ∈ Rd is a vector of latent variables called causes, y ∈ Ro is the generated vector, and θ ∈ RD is a set of parameters. We are interested in the following inverse problem: given a vector y and a generative model g, we need the parameters θ that maximize the marginal likelihood\n\np(y, θ) =\n\n(cid:90)\n\nx\n\np(y | x, θ)p(x, θ)dx.\n\n(1)\n\nHere, the first term inside the integral is the likelihood of the data given the causes, and the second is a prior distribution over the causes. Solving the above problem is intractably expensive. Hence, we need an algorithm that is divided in two phases: inference, where we infer the best causes x given both θ and y, and learning, where we update the parameters θ based on the newly computed causes. This algorithm is expectation-maximization (EM) (Dempster et al., 1977). The first step, which we call inference or E-step, computes p(x | y, θ), that is the posterior distribution of the causes given\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: (a) An example of an hierarchical Gaussian generative model with three layers. (b) Comparison of the temporal training dynamics of PC, Z-IL, and iPC, where Z-IL is a variation of predictive coding that is equivalent to BP, originally introduced in Song et al. (2020). We assume that we train the networks on a dataset for supervised learning for a period of time T . Here, t is the time axis during inference, which always starts at t = 0. The squares represent nodes in one layer, and pink rounded rectangles indicate when the connection weights are modified: PC (1st row) first conducts inference on the hidden layers, according to Eq. equation 6, until convergence, and then it updates the weights via Eq. 7. Z-IL (2nd row) only updates the weights at specific inference moments depending on which layer the weights belong to. To conclude, iPC updates the weights at every time step t, while performing inference in parallel.\n\na generated vector y. Computing the posterior is, however, intractable (Friston, 2003). To this end, we approximate the intractable posterior with a tractable probability distribution q(x, θ). To make the approximation as good as possible, we want to minimize the KL-divergence between the two probability distributions. Summarizing, to solve our learning problem, we need to (i) minimize a KL-divergence, and (ii) maximize a likelihood. We do it by defining the following energy function, also known as variational free-energy:\n\nF (x, y, θ) = KL(q(x, θ) ∥ p(x | y, θ)) − ln(p(y, θ)),\n\n(2)\n\nwhere we have used the log-likelihood. This function is minimized by multiple iterations of the EM algorithm as follows:\n\n(cid:26)Inference (E-step): x∗ = argmaxxF (x, y, θ), Learning (M-step): θ∗ = argmaxθF (x, y, θ).\n\n(3)\n\n2.1 PREDICTIVE CODING\n\nSo far, we have only presented the general problem. To actually derive proper equations for learning causes and update the parameters, and use them to train neural architectures, we need to specify the generative function g(x, θ). Following the general literature, (Rao & Ballard, 1999; Friston, 2005), we define the generative model as a hierarchical Gaussian generative model, where the causes x and parameters θ are defined by a concatenation of the causes and weight matrices of all the layers, i.e., x = (x(0), . . . , x(L)), and θ = (θ(0), . . . , θ(L−1)). Hence, we have a multilayer generative model, where layer 0 is the one corresponding to the generated image y, and layer L the highest in the hierarchy. The marginal probability of the causes is as follows:\n\np(x(0), . . . , x(L)) = p(x(L))\n\nL−1 (cid:89)\n\nl\n\np(x(l−1) | x(l)) =\n\nL (cid:89)\n\nl\n\nN (μ(l), Σ(l)),\n\n(4)\n\nwhere μ(l) is the prediction of layer l according to the layer above, given by μ(l) = θ(l) · f (x(l+1)), with f being a non-linear function and μ(L) = x(L). For simplicity, from now on, we consider Gaussians with identity variance, i.e., Σ(l) = 1 for every layer l. With the above assumptions, the\n\n3\n\nWeights updateInput layer (fixed)Output layer (fixed)Inference stepNumber of iterationsLayer indexHidden layer (running inference)PCZ-IL(BP)iPC(a)Generative Model(b) Differences between PC, Z-IL, and iPCUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Learning a dataset D = {yi} with iPC. 1: Require: For every i, x(0) 2: for t = 0 to T do 3: 4: 5: end for\n\nFor every i and l, update x(l) to minimize F via Eq.(6) For every l, update each θ(l) to minimize F via Eq.(7)\n\nis fixed to yi,\n\ni\n\ni\n\nfree-energy becomes\n\n(cid:88)\n\nF =\n\n∥x(l) − μ(l)∥2.\n\nl\n\n(5)\n\nFor a detailed formulation on how this energy function is derived from the variational free-energy of Eq. 2, we refer to (Friston, 2005; Bogacz, 2017; Buckley et al., 2017), or to the supplementary material. Note that this energy function is equivalent to the one proposed in the original formulation of predictive coding (Rao & Ballard, 1999). A key aspect of this model is that both inference and learning are achieved by optimizing the same energy function, which aims to minimize the prediction error of the network. The prediction error of every layer is given by the difference between its real value x(l) and its prediction μ(l). We denote the prediction error ε(l) = x(l)−μ(l). Thus, the problem of learning the parameters that maximize the marginal likelihood given a data point y reduces to an alternation of inference and weight update. During both phases, the values of the last layer are fixed to the data point, i.e., x(0) = y for every t ≤ T .\n\nInference: During this phase, that corresponds to the E-step, the weight parameters θ(l) are fixed, while the values x(l) are continuously updated via gradient descent. ∂F ∂x(l)\n\n= γ · (−ε(l) + f ′(x(l)) ∗ θ(l−1) T · ε(l−1)),\n\n∆x(l) = −γ\n\n(6)\n\nwhere ∗ denotes element-wise multiplication, and l > 0. This process either runs until convergence, or for a fixed number of iterations T .\n\nLearning: During this phase, which corresponds to the M-step, the values x are fixed, and the weights are updated once via gradient descent according to the following equation:\n\n∆θ(l) = −α\n\n∂F ∂θ(l)\n\n= α · x(l+1)ε(l).\n\n(7)\n\nNote that the above algorithm is not limited to generative tasks, but can also be used to solve supervised learning problems (Whittington & Bogacz, 2017). Assume that we are provided with a data point yin with label yout. In this case, we treat the label as the vector y we need to generate, and the data point as the prior on x(L). The inference and learning phases are identical, with the only difference that now we have two vectors fixed during the whole duration of the process: x(0) = yout, and x(L) = yin. While this algorithm is able to obtain good results on small image image classification tasks, it is much slower than BP due to the large number of inference steps T needed to let the causes x converge.\n\n3\n\nINCREMENTAL PREDICTIVE CODING\n\nOne of the main drawbacks of energy based models such as PC and equilibrium propagation (Scellier & Bengio, 2017), is their efficiency. In fact, these algorithms are much slower than BP due to the inference phase, which requires multiple iterations to converge. The goal of this paper is to address this problem for predictive coding, by developing a variation based from the incremental EM (iEM) algorithm (Neal & Hinton, 1998), which was developed to address the lack of efficiency of the original EM. This algorithm excels when dealing with multiple data points at the same time (Neal & Hinton, 1998), a scenario that is almost always present in standard machine learning.\n\nLet D = {yi}i<N be a dataset of cardinality N , and g(x, θ) be a generative model. Our goal is now to minimize the global marginal likelihood, defined on the whole dataset, i.e.,\n\np(yi, θ).\n\n(8)\n\np(D, θ) =\n\n(cid:88)\n\ni\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Left and centre: Decrease of the energy of generative models as a function of the number of iterations performed from the beginning of the training process. Right: Training loss of different classifiers trained using iPC, BP, and multiple parameterizations of PC as a function of the number of non-parallel matrix multiplications performed from the beginning of the training process.\n\nThe same reasoning also applies to the global variational free energy, which will be the sum of the free energies of every single data point. In this case, the iEM algorithm performs the E-step and M-step in parallel, with no external control needed to switch between the two phases. In detail, both the values x and the parameters θ are updated simultaneously at every time step t, until convergence (or for a fixed number of iterations T ), according to the same update rule defined in Eqs. 6 and 7, on all the points of the dataset. No explicit forward and backward passes are necessary as each layer is updated in parallel. To our knowledge, this is the first learning algorithm for deep neural networks where every single operation is performed in parallel. Note that this increased speed does not harm the final performance, as it has been formally proven that minimizing a free-energy function such as ours (i.e., equivalent to the sum of independent free-energy functions) using iEM, also finds a minimum of the global marginal likelihood of Eq.8 (Neal & Hinton, 1998; Karimi et al., 2019). We actually provide empirical evidence that the model converges to better minima using iPC rather than the original formulation of PC in Fig. 2 and Table 1. The pseudocode of iPC is given in Alg. 1.\n\nConnections to BP: PC in general shares multiple similarities with BP in supervised learning tasks: when the output error is small, the parameter update of PC is an approximation of that of BP (Millidge et al., 2020); when controlling which parameters have to be updated at which time step, the two updates can even be made equivalent (Salvatori et al., 2022b). To make PC perform exactly the same weight update of BP, every weight matrix θl must be updated only at t = l, which corresponds to its position in the hierarchy (Song et al., 2020). That is, as soon as the output error reaches a specific layer. This is different from the standard formulation of PC, which updates the parameters only when the energy representing the total error has converged. Unlike PC, iPC updates the parameters at every time step t. Intuitively, it can hence be seen as a “continuous shift” between Z-IL and PC, where Z-IL is a variation of PC that is equivalent to BP, originally introduced in Song et al. (2020).. A graphical representation of the differences of all three algorithms is given in Fig. 1 (right), with the pseudo-codes provided in the first section of the supplementary material.\n\nAutonomy: Both PC and Z-IL lack full autonomy, as an external control signal is always needed to switch between inference and learning: PC waits for the inference to converge (or, for T iterations), while Z-IL updates the weights of specific layers at specific inference moments t = l. BP is considered to be less autonomous than PC and Z-IL: a control signal is required to forward signals as well as backward errors, and additional places to store the backward errors are required. All of those drawbacks are removed in iPC, which is able to learn a dataset without the control signals required by the other algorithms: given a dataset D, iPC runs inference and weight updates simultaneously until the energy F is minimized. As soon as the energy minimization has converged, training ends.\n\n3.1 EFFICIENCY\n\nIn this section, we analyze the efficiency of iPC with respect to both the original formulation of PC and BP. We only provide partial evidence of the increased efficiency against BP, as standard deep learning frameworks, such as Pytorch, do not allow to parallelize operations in different layers. While we leave the development of a framework able to perform every operation in parallel to future work, we provide evidence that the speed up against BP in full batch training is theoretically possible using iPC.\n\n5\n\nTrain LossNon-parallel MultiplicationsEnergyEnergyIterationsIterationsTiny ImagenetCIFAR10MNISTFashionMNIST(a)Generation(b) ClassificationTest ErrorIterationsUnder review as a conference paper at ICLR 2023\n\nComparison with PC: We now show how iPC is more efficient than the original formulation. To do that, we have trained multiple models with iPC and PC on different tasks and datasets. First, we have trained a generative model with 4 layers and 256 hidden neurons on a subset of 100 images of the Tiny ImageNet and CIFAR10 datasets, as done in (Salvatori et al., 2021). A plot with the energies as a function of the number of iterations is presented in Fig. 2 (left and centre). In both cases, the network trained with iPC converges much faster than the networks trained with PC with different values of T . Many more plots with different parameterizations are given in Fig. 7 in the supplementary material.\n\nTo show that the above results hold in different set-ups as well, we have trained a classifier with 4 layers on a subset of 250 images of the FashionMNIST dataset, following the framework proposed in (Whittington & Bogacz, 2017), and studied the training loss. As it is possible to train an equivalent model using BP, we have done it using the same set-up and learning rate, and included it in the plot. This, however, prevents us from using the number of iterations as an efficiency measure, as one iteration of BP is more complex than one iteration of PC, and are hence not comparable. As a metric, we have hence used the number of non-parallel matrix multiplications needed to perform a weight update. This is a fair metric, as matrix multiplications are by far the most expensive operation performed when training neural networks, and the ones with largest impact on the training speed. One iteration of PC and iPC have the same speed, and consist of 2 non-parallel matrix multiplications. One epoch of BP, consists of 2L non-parallel matrix multiplications. The results are given in Fig. 2 (right). In all cases, iPC converges much faster than all the other methods. In the supplementary material, we provide other plots obtained with different datasets, models, and parameterizations, as well as a study on how the test error decreases during training. Again, many more plots with different parameterizations are given in Fig. 8 in the supplementary material.\n\nComparison with BP: While the main goal of this work is simply to overcome the core limitation of original PC — the slow inference phase — there is one scenario where iPC is potentially more efficient than BP, which is full batch training. Particularly, we first prove this formally using the number of non-parallel matrix multiplications needed to perform a weight update as a metric. To complete one weight update, iPC requires two sets of non-parallel multiplications: the first uses the values and weight parameters of every layer to compute the prediction of the layer below; the second uses the error and transpose of the weights to propagate the error back to the layer above, needed to update the values. BP, on the other hand, requires 2L sets of non-parallel multiplications for a complete update of the parameters: L for a forward pass, and L for a backward one. These operations cannot be parallelized. More formally, we prove a theorem that holds when training on the whole dataset D in a full-batch regime. For details about the proof, and an extensive discussion about time complexity of BP, PC, and iPC, we refer to the supplementary material.\n\nTheorem 1 Let M and M ′ be two equivalent networks with L layers trained on the same dataset. Let M be trained using BP, and M ′ be trained using iPC. Then, the time complexity needed to perform one full update of the weights is O(1) for iPC and O(L) for BP.\n\n3.2 CPU IMPLEMENTATION\n\nTo further provide evidence of the efficiency of iPC with respect to BP, we have implmented the parallelization of iPC on a CPU, and compared it to BP, also implemented on CPU. We compute the time in milliseconds (ms) needed to perform one weight update of both on a randomly generated datapoint. In Fig. 3, we have plotted the ratio\n\nms of iPC / ms of BP\n\nfor architectures with different depths and widths. The results show that our naive implementation adds a computational overhead given by communication and synchronization across threads that makes iPC slower than BP on small\n\nFigure 3: Ratio of the actual running time needed to perform a single weight update between BP and iPC on a CPU. Every dot represents a model, if the model lies below the horizontal line with label 100, its weight update performed using iPC is faster than one performed using BP.\n\n6\n\n Hidden Dim. 81632641282565121024iPC / BPUnder review as a conference paper at ICLR 2023\n\nTable 1: Final accuracy of BP, PC, and iPC on different architectures trained with different datasets.\n\nBP/Z-IL\n\nPC\n\niPC\n\n98.26% ± 0.12% 98.55% ± 0.14% 98.54% ± 0.86% MLP on MNIST MLP on FashionMNIST 88.54% ± 0.64% 85.12% ± 0.75% 89.13% ± 0.86% 95.35% ± 1.53% 94.53% ± 1.54% 96.45% ± 1.04% CNN on SVHN 69.34% ± 0.54% 70.84% ± 0.64% 72.54% ± 0.93% CNN on CIFAR-10 75.64% ± 0.64% 64.63% ± 1.55% 72.42% ± 0.53% AlexNet on CIFAR-10\n\narchitectures (hidden dimension ≤ 64). However, this difference is inverted in large networks: in the most extreme case, one weight update on a network with 32 hidden layers and 1024 parameters per layer using iPC is 10 times faster than that using BP. This is still below the result of Theorem 1 due to the large overhead introduced in our implementation.\n\n4 CLASSIFICATION EXPERIMENTS\n\nWe now demonstrate that iPC shows a similar level of generalization quality compared to BP. We test the performance of iPC on different benchmarks. Since we focus on generalization quality in this section, all methods are run until convergence, and we have used early stopping to pick the best performing model. These experiments were performed using multi-batch training. In this case, we lose our advantage in efficiency over BP, as we need to recompute the error every time a new batch is presented. However, the proposed algorithm is still much faster than the original formulation of PC, and yields a better classification performance.\n\nSetup of experiments: We investigate image classification benchmarks using PC, iPC, and BP. We first trained a fully connected network with 2 hidden layers and 64 hidden neurons per layer on the MNIST dataset (LeCun & Cortes, 2010). Then, we trained a mid-size CNN with three convolutional layers with 64 − 128 − 64 kernels followed by two fully connected layers on FashionMNIST, the Street View House Number (SVHN) dataset (Netzer et al., 2011), and CIFAR10 (Krizhevsky et al., 2012) with no data augmentation. Finally, we trained AlexNet (Krizhevsky et al., 2012), a large-scale CNN, on CIFAR10. To make sure that our results are not the consequence of a specific choice of hyperparameters, we performed a comprehensive grid-search on hyperparameters, and reported the highest accuracy obtained. The search is further made robust by averaging over 5 seeds. Particularly, we tested over 8 learning rates (from 0.000001 to 0.01), 4 values of weight decay (0.0001, 0.001, 0.01, 0.1), and 3 values of the integration step γ (0.1, 0.5, 1.0), and each combination of hyperparameters are evaluated with 5 seeds with mean and standard error reported. To conclude, we have used no data augmentation in the experiments.\n\nResults: In Table 1, iPC outperforms BP in all the small- and medium-size architectures. For the simplest framework (MNIST on a small MLP), PC outperforms all the other training methods, with iPC following by a tiny margin (0.01%). However, PC fails to scale to more complex problems, where it gets outperformed by all the other training methods. The performance of iPC, on the other hand, is stable under changes in size, architecture, and dataset. In fact, iPC reaches a slightly better accuracy than BP on most of the considered tasks.\n\nTable 2: Change of final accuracy when increasing the width.\n\nC\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n67.92 BP iPC 70.61\n\n71.23 74.12\n\n71.65 74.91\n\n72.64 75.88\n\n73.35 76.61\n\n73.71 77.04\n\n74.19 77.48\n\n74.51 77.41\n\n10\n\n74.62 76.51\n\n15\n\n75.08 76.55\n\n20\n\n75.51 76.12\n\nChange of width: Table 1 shows that iPC performs better on a standard CNN than on AlexNet, which has many more parameters and maxpooling layers. To investigate how iPC behaves when adding max-pooling layers and increasing the width, we trained a CNN with three convolutional layers (8, 16, 8) and maxpools, followed by a fully connected layer (128 hidden neurons) on CIFAR10. We have also replicated the experiment by increasing the width of the network by multi-\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Robustness of BP and iPC under distribution shift (AlexNet on CIFAR10 under five different intensities of the corruptions rotation, Gaussian blur, Gaussian noise, hue, brightness, and contrast). Left: Comparable decline of model accuracy between BP and iPC. Right: iPC maintains model calibration significantly better than BP under distribution shift.\n\nplying every hidden dimension by a constant C, (e.g., C = 3 means a network with 3 convolutional layers (24, 48, 24), each followed by a maxpool, and a fully connected one (384 hidden neurons)). The results in Table 2 show that iPC (i) outperforms BP under each parametrization, (ii) needs less parameters to obtain good results, but (iii) sees its performance decrease, once it has reached a specific parametrization. This is in contrast to BP, which is able to generalize well even when extremely overparametrized. This suggests that iPC is more efficient than BP in terms of the number of parameters, but that finding the best parameters for iPC may need some extra tuning.\n\n4.1 ROBUSTNESS AND CALIBRATION\n\nRobustness and uncertainty quantification in deep learning have become a topic of increasing interest in recent years. While neural networks trained via BP reach a strong model performance, their lack of explainability and robustness has been widely studied (Abdar et al., 2021; Ovadia et al., 2019). Recently, it has been noted that treating classifiers as generative energy-based models benefits the robustness of the model (Grathwohl et al., 2019). As PC is precisely an energy-based classifier, originally developed for generation tasks, we postulate that iPC possesses better robustness and calibration characteristics than BP. Calibration describes the degree to which predicted logits matches the empirical distribution of observations given the prediction confidence. One may use a calibrated model’s output to quantify the uncertainty in its predictions and interpret it as probability—not just model confidence. Let ˆP be our random prediction vector indicating the model’s confidence that the prediction ˆY is correct. We say ˆP is well-calibrated, if the model confidence matches the model performance, i.e., P( ˆY = Y | ˆP = p) = p (Guo et al., 2017). We measure the deviation from calibration using the adaptive expected calibration error (AdaECE), which estimates E[|P( ˆY = Y | ˆP = p) − p|] (Nguyen & O’Connor, 2015). In recent years, it has become well-known that neural networks trained with BP tend to be overconfident in their predictions (Guo et al., 2017) and that miscalibration increases dramatically under distribution shift (Ovadia et al., 2019). More details on the experiments are in the supplementary material.\n\nResults: Our results are shown in Fig. 4. The boxplots indicate the distributions of accuracy (left) and calibration error (right) over various forms of data corruption with equal levels of intensity. We find that the discriminative performance of the BP and iPC models are comparable under distribution shift. Both models keep a reasonable classification performance for mild corruptions, but show accuracies going down to chance performance under extreme corruptions. The calibration of model output, however, differs strongly: The iPC-trained model yields better calibrated outputs and is able to signal its confidence a lot better. This is essential for using the model output as indication of uncertainty. On in-distribution data, we observe that iPC yields an average calibration error of 0.05, whereas BP yields 0.12. Moreover, we observe that the increase in calibration error is a lot weaker for iPC: The median calibration error of the iPC model is lower across all levels of shift intensities compared to that of BP for the mildest corruption. Furthermore, iPC displays better calibration up to level 3 shifts than BP does on in-distribution data. This has potentially a strong impact of applying either method in safety-critical applications.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n5 RELATED WORKS\n\nSeveral previous research efforts aim to achieve supervised learning in a biologically plausible way. One is to explore how the error can be encoded differently than in BP where the error is not encoded locally. One of the earliest works was to use a second set of “error” neurons that can act as the feedback variables (encoding error in BP) (Stork, 1989; Schwartz, 1993). Another promising assumption is that the error can be represented in neurons’ dendrites (K ̈ording & K ̈onig, 2001; 2000; Richards & Lillicrap, 2019; Sacramento et al., 2018). Such efforts are unified in (Lillicrap et al., 2020), with a broad range of works (Pineda, 1987; 1988; O’Reilly, 1996; Ackley et al., 1985; Hinton et al., 1995; Bengio, 2014; Lee et al., 2015) encoding the error term in activity differences.\n\nNeuroscience-inspired algorithms have recently gained the attention of the machine learning community, due to interesting properties such as locality, autonomy and their energy-based formulation. To this end, multiple works have used PC to tackle machine learning problems, from generation tasks (Ororbia & Kifer, 2020), to image classification on complex datasets such as ImageNet (He et al., 2016), associative memories (Salvatori et al., 2021), continual learning (Ororbia et al., 2020), and NLP (Pinchetti et al., 2022). There is a more theoretical line of work that is related to the free energy principle and active inference (Friston, 2008; 2010; Friston et al., 2006; 2016), which aims to model learning, perception, and behavior as an imperative to minimize a free energy. While being initially a theoretical framework, it has been used in multiple applications in fields such as control theory (Baltieri & Buckley, 2019; Friston, 2011) and reinforcement learning (Friston et al., 2009). To conclude, it is important to note that iEM is not the only formulation that improves the efficiency of the original EM, as some other variations have been proposed, such as an online version (Capp ́e & Moulines, 2009), a stochastic one (Chen et al., 2018), or a newer incremental version (Karimi et al., 2019) inspired by the SAGA algorithm (Defazio et al., 2014).\n\n6 DISCUSSION\n\nIn this paper, we have proposed a biologically inspired learning rule, called incremental predictive coding (iPC) motivated by the incremental EM algorithm. iPC enables all the computations to be executed simultaneously, locally, and autonomously, and has theoretical convergence guarantees in non-asymptotic time (Karimi et al., 2019). This allows a solid gain in efficiency compared to the original formulation of PC as well as BP in the full-batch case, as shown with extensive experiments, with no drawbacks in the converging to a minimum of the loss. This is confirmed by the good experimental results in terms of accuracy and robustness in classification tasks.\n\nAn interesting aspect worth discussing, is the time step that triggers the weight update in the three variations of PC: the original formulation, Z-IL, and, now, iPC. The first method updates the parameters only in the last step of the inference, when the neural activities have converged. This has interesting theoretical properties, as it has been shown to simulate how learning is performed in multiple models of cortical circuits, as its credit assignment converges to an equilibrium called prospective configuration (Song et al., 2022). The second, Z-IL, shows that it suffices to time the updates at different levels of the hierarchy in different moments of the inference, to exactly replicate the update given by BP on any possible neural network (Song et al., 2020; Salvatori et al., 2022b). This is interesting, as it connects PC, a theory developed to model credit assignment in the brain, to BP, a method developed to train deep learning models. Our newly proposed iPC, on the other hand, updates the parameters continuously, resulting in great gains in terms of efficiency, and no apparent loss in terms of performance. Future work will investigate whether there are better variations of iPC, or whether the optimal update rule can be learned with respect to specific tasks and datasets. Again, the answer may lie in some variations of the EM algorithm, such as dynamical EM (Anil Meera & Wisse, 2021; Friston et al., 2008), or in an implementation of precision-weighted prediction errors, as in (Jiang & Rao, 2022).\n\nOn a broader level, this work shrinks the gap between computational neuroscience and machine intelligence by tackling the problem of the computational efficiency of neuroscience-inspired training algorithms. Advances in this direction are also interesting from the perspective of hardware implementations of deep learning on energy-based chips, such as analog and quantum computers. In this case, iPC is an interesting improvement, as it is still not known how external control can be implemented on these chips, and hence algorithms able to train neural networks in a fully automatic fashion may play an important role in the future.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMoloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, and Saeid Nahavandi. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information Fusion, 76:243–297, 2021.\n\nMohammed. Abdelghani, Timothy. Lillicrap, and Douglas Tweed. Sensitivity derivatives for flexible\n\nsensorimotor learning. Neural Computation, 20(8):2085–2111, 2008.\n\nDavid Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A learning algorithm for boltzmann\n\nmachines. Cognitive Science, 9(1):147–169, 1985.\n\nAjith Anil Meera and Martijn Wisse. Dynamic expectation maximization algorithm for estimation\n\nof linear systems with colored noise. Entropy, 23(10):1306, 2021.\n\nManuel Baltieri and Chris Buckley. PID control as a process of active inference with linear genera-\n\ntive models. Entropy, 2019.\n\nYoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target\n\npropagation. arXiv:1407.7906, 2014.\n\nRafal Bogacz. A tutorial on the free-energy framework for modelling perception and learning.\n\nJournal of Mathematical Psychology, 76:198–211, 2017.\n\nChris Buckley, Chang Kim, Simon McGregor, and Anil Seth. The free energy principle for action\n\nand perception: A mathematical review. Journal of Mathematical Psychology, 2017.\n\nOlivier Capp ́e and Eric Moulines. On-line expectation–maximization algorithm for latent data models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):593–613, 2009.\n\nJianfei Chen, Jun Zhu, Yee Whye Teh, and Tong Zhang. Stochastic expectation maximization with\n\nvariance reduction. Advances in Neural Information Processing Systems, 31, 2018.\n\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big selfsupervised models are strong semi-supervised learners. 34th Conference on Neural Information Processing Systems, NeurIPS, 2020.\n\nFrancis Crick. The recent excitement about neural networks. Nature, 337(6203):129–132, 1989.\n\nAaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. Advances in Neural Information Processing Systems, 27, 2014.\n\nArthur Dempster, Nan Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1): 1–22, 1977.\n\nKarl Friston. Learning and inference in the brain. Neural Networks, 16(9):1325–1352, 2003.\n\nKarl Friston. A theory of cortical responses. Philosophical Transactions of the Royal Society B:\n\nBiological Sciences, 360(1456), 2005.\n\nKarl. Friston. Hierarchical models in the brain. PLoS Computational Biology, 2008.\n\nKarl Friston. The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11\n\n(2):127–138, 2010.\n\nKarl Friston. What is optimal about motor control? Neuron, 2011.\n\nKarl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain. Journal of\n\nPhysiology, 2006.\n\nKarl Friston, Jean. Daunizeau, and Stephan. Kiebel. Reinforcement learning or active inference?\n\nPloS One, 2009.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKarl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, Giovanni Pezzulo, et al.\n\nActive inference and learning. Neuroscience & Biobehavioral Reviews, 68:862–879, 2016.\n\nKarl J. Friston, N. Trujillo-Barreto, and Jean Daunizeau. DEM: A variational treatment of dynamic\n\nsystems. Neuroimage, 41(3):849–885, 2008.\n\nWill Grathwohl, Kuan-Chieh Wang, J ̈orn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. arXiv preprint arXiv:1912.03263, 2019.\n\nStephen Grossberg. Competitive learning: From interactive activation to adaptive resonance. Cog-\n\nnitive Science, 11(1):23–63, 1987.\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural\n\nnetworks. ICML 2017, 3:2130–2143, 2017.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recogIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n\nnition. 2016.\n\nGeoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and Radford M. Neal. The” wake-sleep” algo-\n\nrithm for unsupervised neural networks. Science, 268(5214):1158–1161, 1995.\n\nLinxing Preston Jiang and Rajesh P. N. Rao. Dynamic predictive coding: A new model of hierar-\n\nchical sequence learning and prediction in the cortex. bioRxiv, 2022.\n\nBelhal Karimi, Hoi-To Wai, Eric Moulines, and Marc Lavielle. On the global convergence of (fast) incremental expectation maximization methods. Advances in Neural Information Processing Systems, 32, 2019.\n\nJack Kendall, Ross Pantone, Kalpana Manickavasagam, Yoshua Bengio, and Benjamin Scellier. Training end-to-end analog neural networks with equilibrium propagation. arXiv preprint arXiv:2006.01981, 2020.\n\nKonrad P. K ̈ording and Peter K ̈onig. Learning with two sites of synaptic integration. Network:\n\nComputation in Neural Systems, 11, 2000.\n\nKonrad P. K ̈ording and Peter K ̈onig. Supervised and unsupervised learning with two sites of synaptic\n\nintegration. Journal of Computational Neuroscience, 11(3):207–215, 2001.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural networks. In 26th Annual Conference on Neural Information Processing Systems (NIPS) 2012, 2012.\n\nYann LeCun and Corinna Cortes. MNIST handwritten digit database. The MNIST Database, 2010.\n\nURL http://yann.lecun.com/exdb/mnist/.\n\nDong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.\n\nIn Proc. ECMLPKDD, 2015.\n\nTimothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature Communications, 7 (1):1–10, 2016.\n\nTimothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman, and Geoffrey Hinton. Back-\n\npropagation and the brain. Nature Reviews Neuroscience, 2020.\n\nBeren Millidge, Alexander Tschantz, and Christopher L Buckley. Predictive coding approximates\n\nbackprop along arbitrary computation graphs. arXiv:2006.04182, 2020.\n\nBeren Millidge, Tommaso Salvatori, Yuhang Song, Rafal Bogacz, and Thomas Lukasiewicz. PrearXiv preprint\n\ndictive coding: Towards a future of deep learning beyond backpropagation? arXiv:2202.09467, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDavid Mumford. On the computational architecture of the neocortex. Biological Cybernetics, 66\n\n(3):241–251, 1992.\n\nRadford M. Neal and Geoffrey E. Hinton. A view of the EM algorithm that justifies incremental,\n\nsparse, and other variants. In Learning in graphical models, pp. 355–368. Springer, 1998.\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading\n\ndigits in natural images with unsupervised feature learning. 2011.\n\nKhanh Nguyen and Brendan T. O’Connor. Posterior calibration and exploratory analysis for natural\n\nlanguage processing models. In EMNLP, 2015.\n\nRandall C O’Reilly. Biologically plausible error-driven learning using local activation differences:\n\nThe generalized recirculation algorithm. Neural Computation, 8(5):895–938, 1996.\n\nAlex Ororbia and Daniel Kifer. The neural coding framework for learning generative models.\n\narXiv:2012.03405, 2020.\n\nAlexander Ororbia, Ankur Mali, C. Lee Giles, and Daniel Kifer. Continual learning of recurrent IEEE Transactions on Neural\n\nneural networks by locally aligning distributed representations. Networks and Learning Systems, 31(10):4267–4278, 2020.\n\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V. Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems, volume 32, 2019.\n\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\nLuca Pinchetti, Tommaso Salvatori, Beren Millidge, Yuhang Song, Yordan Yordanov, and Thomas Lukasiewicz. Predictive coding beyond gaussian assumptions. 36th Conference on Neural Information Processing Systems, 2022.\n\nFernando J. Pineda. Generalization of back-propagation to recurrent neural networks. Physical\n\nReview Letters, 59(19):2229, 1987.\n\nFernando J. Pineda. Dynamics and architecture for neural computation. Journal of Complexity, 4\n\n(3):216–245, 1988.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nRajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2(1):79–87, 1999.\n\nBlake A. Richards and Timothy P. Lillicrap. Dendritic solutions to the credit assignment problem.\n\nCurrent Opinion in Neurobiology, 54:28–36, 2019.\n\nPieter R. Roelfsema and Anthony Holtmaat. Control of synaptic plasticity in deep cortical networks.\n\nNature Reviews Neuroscience, 19(3):166, 2018.\n\nJo ̃ao Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic cortical microcircuits approximate the backpropagation algorithm. In Advances in Neural Information Processing Systems, pp. 8721–8732, 2018.\n\nTommaso Salvatori, Yuhang Song, Yujian Hong, Lei Sha, Simon Frieder, Zhenghua Xu, Rafal BoIn Advances in\n\ngacz, and Thomas Lukasiewicz. Associative memories via predictive coding. Neural Information Processing Systems, volume 34, 2021.\n\nTommaso Salvatori, Luca Pinchetti, Beren Millidge, Yuhang Song, Tianyi Bao, Rafal Bogacz, Learning on arbitrary graph topologies via predictive coding.\n\nand Thomas Lukasiewicz. arXiv:2201.13180, 2022a.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nTommaso Salvatori, Yuhang Song, Zhenghua Xu, Thomas Lukasiewicz, and Rafal Bogacz. Reverse differentiation via predictive coding. In Proceedings of the 36th AAAI Conference on Artificial Intelligence. AAAI Press, 2022b.\n\nBenjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-\n\nbased models and backpropagation. Frontiers in Computational Neuroscience, 11:24, 2017.\n\nEric L. Schwartz. Computational Neuroscience. Mit Press, 1993.\n\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529, 2016.\n\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge. Nature, 550, 2017.\n\nYuhang Song, Thomas Lukasiewicz, Zhenghua Xu, and Rafal Bogacz. Can the brain do backpropagation? — Exact implementation of backpropagation in predictive coding networks. In Advances in Neural Information Processing Systems, volume 33, 2020.\n\nYuhang Song, Beren Gray Millidge, Tommaso Salvatori, Thomas Lukasiewicz, Zhenghua Xu, and Rafal Bogacz. Inferring neural activity before plasticity: A foundation for learning beyond backpropagation. bioRxiv, 2022.\n\nDavid G. Stork. Is backpropagation biologically plausible. In International Joint Conference on\n\nNeural Networks, volume 2, pp. 241–246. IEEE Washington, DC, 1989.\n\nJames C. R. Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm in a predictive coding network with local Hebbian synaptic plasticity. Neural Computation, 29(5), 2017.\n\nJames C. R. Whittington and Rafal Bogacz. Theories of error back-propagation in the brain. Trends\n\nin Cognitive Sciences, 2019.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Standard and dendritic neural implementation of predictive coding. The dendritic implementation makes use of interneurons il = Wlxl (according to the notation used in the figure). Both implementations have the same equations for all the updates, and are thus equivalent; however, dendrites allow a neural implementation that does not take error nodes into account, improving the biological plausibility of the model. Figure taken and adapted from (Whittington & Bogacz, 2019).\n\nA A DISCUSSION ON BIOLOGICAL PLAUSIBILITY\n\nIn this section, we discuss the biological plausibility of the proposed algorithm, a topic overlooked in the main body of this paper. In the literature, there is often a disagreement on whether a specific algorithm is biologically plausible or not. Generally, it is assumed that an algorithm is biologically plausible when it satisfies a list of properties that are also satisfied in the brain. Different works consider different properties. In our case, we consider as list of minimal properties that include local computations and lack of a global control signals to trigger the operations. Normally, predictive coding networks take error nodes into account, often considered implausible from the biological perspective (Sacramento et al., 2018). Even so, the biological plausibility of our model is not affected by this: it is in fact possible to map PC on a different neural architecture, in which errors are encoded in apical dendrites rather than separate neurons (Sacramento et al., 2018; Whittington & Bogacz, 2019). Graphical representations of the differences between the two implementations can be found in Fig. 5, taken (and adapted) from (Whittington & Bogacz, 2019). Furthermore, our formulation is more plausible than the original formulation of PC, as it is able to learn without the need of external control signals that trigger the weight update.\n\n14\n\nArtificial Neural NetworkBiological Neural Network(b)Dendritic lError NodesInterneuronsSynaptic WeightsStandardUnder review as a conference paper at ICLR 2023\n\nB PSEUDOCODES OF Z-IL AND PC\n\nAlgorithm 2 Learning a dataset D = {yi} with PC. 1: Require: For every i, x(0) 2: for t = 0 to T do 3: 4:\n\nFor every i and l, update x(l) to minimize F via Eq.(7) if t = T then\n\nis fixed to yi,\n\ni\n\nFor every l update each θ(l) to minimize F via Eq. (8)\n\nend if\n\n5: 6: end for\n\nAlgorithm 3 Learning one training pair (sin, sout) with Z-IL\n\nfor each level l do\n\n0 is fixed to sout.\n\n0 is fixed to sin, x0\n\n1: Require: xL 2: Require: x(l) = μ(l) for l ∈ {1, ..., L−1}, and t = 0. 3: for t = 0 to T do 4: 5: 6: 7: 8: 9:\n\nUpdate x(l) to minimize F via Eq.(7) if t = l then\n\nUpdate θ(l) to minimize F via Eq.(8).\n\nend for\n\nend if\n\n10: end for\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Theoretical Efficiency of PC, Z-IL, BP, and iPC.\n\nOne inference step\n\nPC\n\nZ-IL\n\nBP\n\niPC\n\nNumber of MMs per weight update Number of SMMs per weight update\n\n(2L − 1) 2\n\n(2L − 1)T 2T\n\n(2L − 1)(L − 1) 2(L − 1)\n\n(2L − 1) (2L − 1)\n\n(2L − 1) 2\n\nC ON THE EFFICIENCY OF PC, BP, AND IPC\n\nIn this section, we discuss the time complexity and efficiency of PC, BP, Z-IL, and iPC. We now start with the first three, and introduce a metric that we use to compute such complexity. This metric is the number of simultaneous matrix multiplications (SMMs), i.e., the number of non-parallelizable matrix multiplications needed to perform a single weight update. It is a reasonable approximation of running time, as multiplications are by far the most complex operation (≈ O(N 3)) performed by the algorithm.\n\nC.1 COMPLEXITY OF PC, BP, AND Z-IL\n\nSerial Complexity: To complete a single update of all weights, PC and Z-IL run for T and (L − 1) inference steps, respectively. To study the complexity of the inference steps we consider the number of matrix multiplications (MMs) required for each algorithm: One inference step requires (2L − 1) MMs: L for updating all the errors, and (L − 1) for updating all the value nodes (Eq. equation 6). Thus, to complete one weight update, PC and Z-IL require (2L − 1)T and (2L − 1)(L − 1) MMs, respectively. Note also that BP requires (2L − 1) MMs to complete a single weight update: L for the forward, and (L − 1) for the backward pass. These numbers are summarized in the first row of Table 3. According to this measure, BP is the most efficient algorithm, Z-IL ranks second, and PC third, as in practice T is much larger than L. However, this measure only considers the total number of matrix multiplications needed, without considering whether some of them can be performed in parallel, which could significantly reduce the time complexity. We now address this problem.\n\nParallel complexity: The MMs performed during inference can be parallelized across layers. In fact, computations in Eq. equation 6 are layer-wise independent, thus L MMs that update all the error nodes take the time of only one MM if properly parallelized. Similarly, in Eq. equation 6, (L − 1) MMs that update all the value nodes take the time of only one MM if properly parallelized. As a result, one inference step only takes the time of 2 MMs if properly parallelized (since, as stated, it consists of updating all errors and values via Eq. equation 6). Thus, one inference step takes 2 SMMs; one weight update with PC and Z-IL takes 2T and 2(L − 1) SMMs, respectively. Since no MM can be parallelized in BP (the forward pass in the network and the backward pass of error are both layer-dependent), before performing a single weight update, (2L − 1) SMMs are required. These numbers are summarized in the second row of Table 3. Overall, measured over SMMs, BP and Z-IL are equally efficient (up to a constant factor), and faster than PC.\n\nC.2 COMPLEXITY OF IPC\n\nTo complete one weight update, iPC requires one inference step, thus (2L − 1) MMs or 2 SMMs, as also demonstrated in the last column of Table 3. Compared to BP, iPC takes around L times less SMMs per weight update, and should hence be significantly faster in deep networks. Intuitively, this is because matrix multiplications in BP have to be done sequentially along layers, while the ones in iPC can all be done in parallel across layers (Fig. 6). More formally, we have the following theorem, which holds when performing full-batch training:\n\nTheorem 1. Let M and M ′ be two equivalent networks with L layers trained on the same dataset. Let M (resp., M ′) be trained using BP (resp., iPC). Then, the time complexity measured by SMMs needed to perform one full update of the weights is O(1) and O(L) for iPC and BP, respectively.\n\nProof. Consider training on an MLP with L layers, and update weights for multiple times on a single datapoint. Generalizations to multiple datapoints and multiple mini-batches are similar and will be provided after. We first write the equations needed to be computed for iPC to produce one weight\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nupdate:\n\nx(L)\n\ni\n\ni,t = sin nl−1 (cid:88)\n\nˆx(l)\n\ni,t =\n\nand x(0)\n\ni,t = sout\n\ni\n\nθ(l+1)\n\ni,j\n\nf (x(l+1)\n\nj,t\n\n)\n\nfor l ∈ {1, . . . , L}\n\n(9)\n\nj=1\n\ni,t = x(l) ε(l)\n\ni,t − ˆx(l)\n\ni,t\n\ni,t+1 = x(l) x(l)\n\ni,t + γ ·\n\n −ε(l)\n\ni,t + x(l)\n\ni,t\n\nfor l ∈ {1, . . . , L}\n\n\n\nε(l+1) k,t θ(l)\n\nk,i\n\n for l ∈ {1, . . . , L}\n\nn(l+1) (cid:88)\n\nk=1\n\ni,j,t+1 = θ(l) θ(l)\n\ni,j,t − α · ε(l+1)\n\ni,t\n\nf (x(l) j,t)\n\nfor l ∈ {1, . . . , L}.\n\n(10)\n\n(11)\n\nWe then write the three equations needed to be computed for BP to produce one weight update:\n\nx0\n\ni\n\ni,t = sin nl−1 (cid:88)\n\nx(l)\n\ni,t =\n\nθ(l+1)\n\ni,j\n\nf (x(l+1)\n\nj,t\n\n) for l ∈ {1, . . . , L}\n\nj=1\n\nε(L) i,t = sout\n\ni − x(L)\n\ni,t\n\ni,t = f ′ (cid:16) ε(l)\n\nx(l)\n\ni,t\n\n(cid:17) n(l+1) (cid:88)\n\nk=1\n\nk,t θ(l) ε(l+1)\n\nk,i for l ∈ {L, . . . , 1}\n\ni,j,t+1 = θ(l) θ(l)\n\ni,j,t − α · ε(l+1)\n\ni,t\n\nf (x(l)\n\nj,t) for l ∈ {1, . . . , L}.\n\n(12)\n\n(13)\n\nFirst, we notice that the matrix multiplication (MM) is the most complex operation. Specifically, for two adjacent layers with the size of nl and nl, the complexity of MM is O(nlnl), but the maximal complexity of the other operations is O(max nl, nl). In the above equations, only equations with MM are numbered, which are the equations that we investigate in our complexity analysis.\n\nEq. equation 9 for iPC takes L MMs, but one SMM, since the the for-loop for l ∈ {1, . . . , L} can run in parallel for different l. This is further because the variables on the right side of Eq. equation 9 are immediately available. Differently, Eq. equation 12 for iPC takes L MMs, and also L SMMs, since the for-loop for l ∈ {1, . . . , L} has to be executed one after another, following the specified order {2, . . . , L}. This is further because the qualities on the right side of Eq. equation 12 are immediately available, but require to solve Eq. equation 12 again for another layer. That is, to get x(L)\n\ni,t , Eq. equation 12 has to be solved recursively from l = 1 to l = L.\n\nSimilar sense applies to the comparison between Eqs. equation 10 and equation 13. Eq. equation 10 for iPC takes L − 1 MMs but 1 SMMs; Eq. equation 13 for BP takes L − 1 MMs and also L − 1 SMMs.\n\nOverall, Eqs. equation 9 and equation 10 for iPC take 2L − 1 MMs but 2 SMMs; Eqs. equation 12 and equation 13 for BP take 2L − 1 MMs and also 2L − 1 SMMs. Then, the time complexity measured by SMMs needed to perform one full update of the weights is O(1) and O(L) for iPC and BP, respectively.\n\nC.3 EFFICIENCY ON ONE DATA POINT\n\nTo make the difference more visible and provide more insights, we explain this in detail with a sketch of this process on a small network in Fig. 6, where the horizontal axis of m is the time step measured by simultaneous matrix multiplications (SMMs), i.e., within a single m, one can perform one matrix multiplication or multiple ones in parallel; if two matrix multiplications have to be executed in order (e.g., the second needs results from the first), they will need to be put into\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Graphical PClustration of the efficiency over backward SMMs of BP and iPC on a 3-layer network. iPC never clears the error (red neurons), while BP clears it after every update. This allows iPC to perform 5 full and 2 partial updates of the weights in the first 6 SMMs. In the same time frame, BP only performs 3 full updates. Note that the SMMs of forward passes are excluded for simplicity, w.l.o.g., as the insight from this example generalizes to the SMMs of the forward pass.\n\ntwo steps of m. Note that we only consider the matrix multiplications for the backward pass, i.e., the matrix multiplications that backpropagate the error of a layer from an adjacent layer for BP and the inference of Eq. equation 6 for iPC, thus the horizontal axis m is strictly speaking “Backward SMM”. The insight for the forward pass is similar as that of the backward pass. As it has been said, for BP, backpropagating the error from one layer to an adjacent layer requires one matrix multiplication; for iPC, one step of inference on one layer via Eq. equation 6 requires one matrix multiplication. BP and iPC are presented in the first and second rows, respectively. Before both methods are able to update weights in all layers, they need two matrix multiplications for spreading the error through the network, i.e., a weights update of all layers occurs for the first time at m = 2 for both methods. After m = 2, BP cleared all errors on all neurons, so at m = 3, BP backpropagates the error from l = 0 to l = 1, and at m = 4, BP backpropagates the error from l = 1 to l = 2 after which it can make an update of weights at all layers again for the second time. Note that the matrix multiplication that backpropagates errors from l = 1 to l = 2 at m = 4 cannot be put at m = 3, as it requires the results of the matrix multiplication at m = 3, i.e., it requires the error to be backpropagated to l = 1 from l = 0 at m = 3. However, this is different for iPC. After m = 2, iPC does not reset xl i,t. At m = 3, iPC performs two matrix multiplications in parallel, corresponding to two inferences steps at two layers, l = 1 and l = 2, updating xl i,t of these two layers. Note that the above two matrix multiplications of two inference steps can run in parallel and be put into a single m, as inference requires only locally and immediately available information. In this way, a weight update in iPC is able to be performed at every m ever since the very first few steps of m.\n\ni,t, and hence the error signals are held in εl\n\ni,t, i.e., the error signals are still held in εl\n\ni,t to μl\n\nD TRAINING DETAILS\n\nWe now list some additional details to reproduce our results.\n\nD.1 EXPERIMENTS OF EFFICIENCY\n\nThe experiments for the efficiency of generative models were run on fully connected networks with 128, 256 or 512 hidden neurons, and L ∈ {4, 5}. Every network was trained on CIFAR10 or Tiny Imagenet with learning rates α = 0.00005 and γ = 0.5, and T ∈ {8, 12, 16}. The experiments on discriminative models are performed using networks with 64 hidden neurons, depth L ∈ {3, 4, 6}, and learning rates α = 0.0001 and γ = 0.5. The networks trained with BP have the same learning rate α. All the plots for every combination of hyperparameters can be found in Figures 8 and 7.\n\n18\n\nInput neuronHidden neuron (error not updated)Hidden neuron (error updated)Weights (not updated)Weights (updated)Output neuronm = 1Backward SMMmm = 2m = 3m = 4m = 5m = 6m = 0BPiPCUnder review as a conference paper at ICLR 2023\n\nD.2 EXPERIMENTS OF GENERALIZATION QUALITY\n\nAs already stated in the paper body, to make sure that our results are not the consequence of a specific choice of hyperparameters, we performed a comprehensive grid search on hyperparameters, and reported the highest accuracy obtained, and the search is further made robust by averaging over 5 seeds. Particularly, we tested over 8 learning rates (from 0.000001 to 0.01), 4 values of weight decay (0.0001, 0.001, 0.01, 0.1), and 3 values of the integration step γ (0.1, 0.5, 1.0). We additionally verified that the optimized value of each hyperparameter lies within the searched range of that hyperparameter. As for additional details, we used standard Pytorch initialization for the parameters. For the hardware, we used a single Nvidia GeForce RTX 2080 GPU on an internal cluster. Despite the large search, most of of the best results were obtained using the following hyperparameters: γ = 0.5 (γ = 1 for Alexnet), α = 0.00005.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Efficiency of multiple generative networks trained with PC.\n\n20\n\nHD = 128HD = 256HD = 512L = 4L = 5CIFAR10HD = 128HD = 256HD = 512L = 4L = 5Tiny ImagenetIterationsIterationsIterationsIterationsIterationsIterationsIterationsIterationsIterationsIterationsIterationsIterationsUnder review as a conference paper at ICLR 2023\n\nFigure 8: Efficiency of multiple discriminative networks trained with PC and BP.\n\n21\n\nL = 3L = 4L = 6Train LossTest AccuracyMNISTHD = 128HD = 256HD = 512FashionMNISTSMMsSMMsSMMsSMMsSMMsSMMsSMMsSMMsSMMsSMMsSMMsSMMsTrain LossTest Accuracy",
    "reference": "# Summary Of The Paper\n\nThis paper describes a variant of predictive coding, named incremental predictive coding (iPC), based on incremental EM, which it is argued should be considered a biologically plausible approach to learning in the brain.  The complexity of iPC is considered in relation to back-propagation (BP), and a CPU implementation is provided.  Further, the generalization performance is investigated on a number of datasets, and the algorithm is shown to perform well in comparison to BP and PC.\n\n# Strength And Weaknesses\n\nStrengths:\n\n-  The biological plausibility argument is interesting, and in general the argument is convincing that some form of 'localized EM' algorithm is more plausible than BP or PC alternatives, while retaining convergence and generalization properties.\n-  The experimentation convincingly demonstrates that iPC should be considered a viable alternative to BP generally, at least for simple architectures and specialized hardware.\n\nWeaknesses:\n\n- I'm mainly concerned about the paper's novelty - essentially, iPC is equivalent to iEM applied to a hierarchical Gaussian model.  The theoretical properties are described elsewhere (e.g. Karimi 2019) and the biological plausibility argument is hard to evaluate, although likely to be worth pursuing further.\n- There is little theoretical novelty, since the time-complexity analysis (Theorem 1) essentially follows simply by definition.  As discussed by the authors, the comparison of training-loss convergence rates in terms of 'non-parallel matrix multiplications' is an interesting result, but this is investigated solely empirically (Fig. 2 right).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity, quality and reproducibility are mainly good (I spotted a few typos - for instance, in Eq. 4, the conditional in the second expression should read 'p(x^(l-1) | x^(l))', and the Gaussian formulation in the third expression should include the prior and the x's).  As noted above, the novelty is an issue for me.\n\n# Summary Of The Review\n\nAn interesting investigation of an algorithm that may have relevance in neuroscience, and deserves further attention.  Potentially, the paper may be of interest to those working in neuroscience and optimization.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nNone."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSPENCNN: ORCHESTRATING AND SPARSITY FOR FAST HOMOMORPHICALLY ENCRYPTED NEURAL NETWORK INFERENCE\n\nENCODING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nHomomorphic Encryption (HE) is a promising technology for protecting user’s data privacy for Machine Learning as a Service (MLaaS) on public clouds. However, the computation overheads associated with the HE operations, which can be orders of magnitude slower than their counterparts for plaintexts, can lead to extremely high latency in neural network inference, seriously hindering its application in practice. While extensive neural network optimization techniques have been proposed, such as sparsification and pruning for plaintext domain, they cannot address this problem effectively. In this paper, we propose an HE-based CNN inference framework, i.e., SpENCNN, that can effectively exploit the single-instruction-multiple-data (SIMD) feature of the HE scheme to improve the CNN inference latency. In particular, we first develop a HE-group convolution technique that can partition channels among different groups based on the data size and ciphertext size, and then encode them into the same ciphertext in an interleaved manner, so as to dramatically reduce the bottlenecked operations in HE convolution. We further develop a sub-block weight pruning technique that can reduce more costly HE-operations for CNN convolutions. Our experiment results show that the SpENCNN-optimized CNN models can achieve overall speedups of 8.37x, 12.11x, and 19.26x for LeNet, VGG-5, and HEFNet, respectively, with negligible accuracy loss.\n\n1\n\nINTRODUCTION\n\nFor the past decade, we have witnessed the tremendous progress of the machine-learning technology and the great success achieved in practical applications. Convolution Neural Network (CNN) models, for example, have been widely used for many cognitive tasks such as face recognition, medical imaging, and human action recognition. Meanwhile, there is a growing interest to deploy machine learning models on the cloud as a service (MLaaS). While cloud computing has been well recognized as an attractive solution, especially for computation intensive applications such as the MLaaS, outsourcing sensitive data and data processing on cloud can pose a severe threat to user’s privacy.\n\nHomomorphic Encryption (HE) is a promising technology for protecting user’s privacy when deploying MLaaS on cloud. HE allows computations be performed on encrypted inputs and the decrypted output matches the corresponding results computed from the original inputs. Thus, a client can encrypt the sensitive data locally and send the encrypted ciphertexts to the cloud. All intermediate results will maintain encrypted, and the encrypted results sent from cloud can be correctly decrypted using the secret key hold by the client. Whlie HE can help to maintain the confidentiality for computation process on cloud effectively, one major problem has to deal with is the excessive computational cost associated with the operations over the encrypted data: HE operations (e.g. HE multiplication, additions on encrypted data) can be several (i.e., three to seven) orders of magnitude slower than the corresponding operations on plaintexts. The tremendous computational cost of HE has been the largest bottleneck that hinders its applications on cloud.\n\nOne of the most effective approaches (e.g. (Gilad-Bachrach et al., 2016; Brutzkus et al., 2019; Dathathri et al., 2019; Kim et al., 2022)) to reduce the HE computational cost is to take advantage of the single-instruction-multiple-data (SIMD) capability, supported by HE schemes, e.g. CKKS and BFV. Smart & Vercauteren (2010) initially proposed to pack multiple data elements in the plaintext\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: (a) Comparison of different HE-operations’ latency (b) Comparison of the HE convolution latency under different pruning methods. (c) Illustration for different pruning methods for plaintext domain. (d) Multi-channel convolution process in HE domain. Notation definitions refer to section 3.1. pt(ki) indicate the weight plaintext. The convolution layer used here has 64 input- and 64 output-channel, with a 3 × 3 kernel. The input feature map size of the convolution layer is 32 × 32.\n\ndomain to different “slots” in the same ciphertext and thus computations for data elements at the same slot of two encoded messages can be performed in parallel. The challenge is how to pack data based on the characteristics of the applications so that computation can be conducted effectively in a SIMD scheme manner. In particular, the problem rises when the computation needs to be performed on data elements at different slots of the messages. To re-arrange the location of each individual data element in an encrypted message is out of question due to its large overhead. A more reasonable solution is to employ the HE-rotation1operation that can move the data element cyclically in the same message. However, HE-rotation has a high latency cost due to required permutation and key-switching operation, compared with other HE-operations such as the HE multiplication of a ciphertext with a plaintext (HE-PMult) and HE addition of two ciphertexts (HE-Add), as shown in Figure 1 (a). Therefore, how to judiciously encode the inputs and perform the SIMD operations plays the key role in reducing the HE computation complexity.\n\nIn this paper, we study the problem on how to improve the HE-based inference latency when deploying a privacy-preserving machine learning (PPML) platform based on CNN models on cloud. It is well-known that the major computation workload for CNN inference comes from the convolution layers. Assuming the user inputs (e.g. images) are encrypted as the ciphertexts and associated CNN models are encoded as plaintext messages, the major HE computations are therefore HE-PMult, HE-Add, and HE-rotation operations. Traditional neural network optimization techniques such as sparsification and pruning (Han et al., 2015b; Wen et al., 2016) help to reduce the computation demand for CNN inference for plaintext domain. However, they may not be effective here as reducing the computation demand does not necessarily imply the reduction of SIMD computations. In addition, note that, as shown in Figure 1(a), the computation cost for an HE-rotation can be over 43× of that for an HE-Pmult or an HE-Add operation. Simply reducing the HE operations without optimizing the HE-rotations may not be effective at all in reducing the computational cost.\n\nTo this end, we develop an HE-based CNN inference framework, i.e., SpENCNN, with the goal to effectively exploit the SIMD feature of the HE scheme to improve the CNN inference latency. In particular, we develop two techniques to reduce the HE computational cost. First, we develop HE-group convolution and associated group-interleaved encoding to optimize channel locations on ciphertexts based on the number of convolutional groups and ciphertext size, thus significantly reducing the number of costly HE-rotations. Second, we further optimize the model architecture by pruning and training the weights in the sub-blocks iteratively with the goal to minimize HE-rotations and accuracy loss. We have conducted extensive experiments based on three CNN models on MNIST dataset and CIFAR-10 dataset and results show that the optimized CNN models can achieve overall speedups of 8.37x, 12.11x, and 19.26x for LeNet, VGG-5, and HEFNet, respectively, with negligible accuracy loss. To our best knowledge, this is the first work to that builds optimizing framework for CNN model architecture from the aspect of structural sparsity and data packing in HE to benefit HE-based PPML inference.\n\n1For\n\ninstance, Rot(ct, k) transforms an encryption of (v0, ..., vN/2−1) into an encryption of\n\n(vk, ..., vN/2−1, v0, ..., vk−1)\n\n2\n\nCh1Ch21.3575%0%Original Non-StructuralPruningSparsity (c) Different Pruning Methods for Plaintext domain50%50%Structural Pruning (Filter) Structural Pruning (Channel) ABDTypeCLatency(ms)HE-PmultBCDLatency (ms)HE-rotationHE-Pmult & Add (b)48591ABaselineLatency(ms)HE-rotationHE-Add631.5(a)--------------------00000000000000000000000000000000 Inner-rotationsCh1Ch2Ch2Ch1Out1Out2=+Outer-rotations(d) Convolution in HE domainCh2Ch1Out1Out2Under review as a conference paper at ICLR 2023\n\n2 PRELIMINARIES\n\n2.1 CKKS HOMOMORPHIC ENCRYPTION\n\nHomomorphic Encryption (HE) allows computations to be performed on encrypted data without decryption. Among various HE schemes, the levelled HE– Cheon-Kim-Kim-Song (CKKS) (Cheon et al., 2017) is widely adopted in the encrypted neural network inference because of supporting the fixed-point real number arithmetic and potentially avoiding the prohibitively expensive bootstrapping. The CKKS-based HE operations mainly consist of ciphertext addition HE-Add (ct1 + ct2), ciphertext multiplication HE-Cmult (ct1 × ct2), scalar multiplication HE-Pmult (pt1 × ct2), ciphertext roation HE-rotation Rot(ct, k), etc. For MLaaS that only encrypts clients’ data, HE-Add, HE-Pmult and HErotation often dominate the computations of an encrypted inference. Among these three operations, HE-rotation costs much longer latency than the other two, e.g. ∼ 43× as our profiling result in Figure 1 (a) shows, due to the complex automorphism operation and a key-switching operation. The detailed calculation process of HE-rotation can be described as:\n\nRot(ct, k) = (c(X ik), 0) + P −1(a(X ik) · evkk\n\nrot)\n\n(1)\n\nwhere the evaluation key (evkk rot) is a public key with a larger modulus P Q, and P is greater than Q. Assume ct = (c(X i), a(X i)) represents a ciphertext before rotation, then the automorphism (c(X ik) and a(X ik)) maps each polynomial coefficient index i to output polynomial coefficient index ik mod N , where N is the polynomial degree. The second term on the right side of Equation 1 represents the key-switching operation to ensure the final ciphertext can be still decrypted by the same secret key. It is very expensive and could take over 90% of all operations in practice (Samardzic et al., 2022)\n\n2.2 THREAT MODEL\n\nWe assume the cloud-based machine learning service, of which a trained convolutional neural network (CNN) model with plaintext weights, is hosted in a cloud server. A client could upload his/her private and sensitive data to the public cloud for obtaining an online inference service. The cloud server is semi-honest (e.g. honest but curious). To ensue the confidentiality of clients’ data against such a cloud server, the client utilizes HE to encrypt the data and then send it to cloud for performing encrypted inference without decrypting the data or accessing the private key. Finally the client can decrypt the returned encrypted inference results from cloud using a private key. In this work, we focus on encrypting the client’s data and others like model parameters, are assumed as plaintext.\n\n2.3 MOTIVATION EXAMPLE\n\nTo identify the computation bottleneck in HE inference, we analyze the computation pattern of the convolutional layer, which often dominates CNN inference’s memory and computational overheads, in the encryption process. Here the input and output activation feature maps are encrypted as ciphertext, while the convolutional kernels are assumed as plaintext. We also assume the state-of-the-art ciphertext encoding–row-major (Dathathri et al., 2019; Kim et al., 2022) is adopted here. This allows efficient multi-channel ciphertext packing to take advantage of CPU’s single-instruction-multiple-data (SIMD) architecture for fast HE inference. Figure 1 (d) shows the typical HE convolution process of a convolution layer which consists of 2-input/output channels with 3 × 3 kernels. To compute a ciphertext output feature map, two types of cipertext rotations need to be performed sequentially. First, inner-rotation rotates each input channel’s ciphertext feature map 8 times (or K 2 − 1, here kernel size K = 3). Each rotated version will need to be multiplied with its corresponding weight plaintext, and then such results will be summed up to obtain an intermediate ciphertext from each input channel, which will further be concatenated as a whole ciphertext (e.g. Ch1 and Ch2 as ct1). Second, outer-rotation rotates the concatenated ciphertext multiple times (in this simple example, 1 time because of packing 2 output channels as a ciphertext). Finally all ciphertext output feature maps can be obtained in parallel by the summation of these rotated copies. Apparently, compared to non-encrypted convolution, HE convolution significantly escalates the memory and computation overheads. Moreover, since the latency of HE-rotation can be much higher than other operations due to complex automorphism and key switching operations (see our profiling result in Figure 1(a) 63ms for HE-Rot v.s. 1.5ms for HE-Pmult, detailed setting in Sec. 4.1), and the multi-channel convolutions in deep CNNs would involve a huge volume of HE-rotation 2. As a result, the long-latency HE-rotation quickly becomes a bottleneck of the encrypted inference.\n\n2The matrix-vector multiplications in fully-connected layers also require a substantial amount of HE-rotation.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The overall flow of SpENCNN framework for optimizing HE-based CNN inference, mainly consists of two orthogonal techniques to generate a tailored CNN model: (1) HE-group Convolution (outer-rotation optimization), and (2) the Sub-block Pruning (inner-rotation optimization).\n\nOne straightforward solution to accelerating HE inference is to reduce the number of rotations through zeroing out (or pruning) the plaintext weights. As Figure 1(c) shows, if any weight plaintext–pt(ki) contains all zero values, then the corresponding ciphertext rotation–Rot(ct1, k) and its associated multiplication and summation can be safely eliminated. Since existing pruning techniques have been proved to be effective in reducing the computation and memory overhead to speedup the nonencrypted inference without accuracy drop, we apply two representative pruning methods–nonstructured pruning(Han et al., 2015a) (zeros appear randomly in a kernel, see Fig. 1 (c)–B, 75% sparsity) and structured pruning(Wen et al., 2016) (structured zeros in a kernel, see filter pruning and channel pruning in Fig. 1 (c)–C and D, 50% sparsity). For HE operations in an example convolutional layer with 64 input/output channels, feature map size 32 × 32 and kernel size 3 × 3, as Fig. 1 (d) shows, the existing pruning achieves very marginal or even no reduction of the HE-rotation latency which dominates the convolution computation. In the worst case, it even cannot remove any HE-rotation despite the high model sparsity, e.g. the non-structured pruning with 75% sparsity ratio. The underlying reason is two-fold: 1) pruning is unable to address the outer-rotation since computing an output ciphertext by convolution needs to sum all channels’ feature maps belonging to the same ciphertext if using the state-of-the-art ciphertext encoding (see Fig. 3 (a)); 2) existing pruning techniques are designed for non-encryted inference, and the special channel-wise ciphertext operations involved in HE convolution are ignored. This prompts the need of jointly optimizing ciphertext encoding and encryption-aware model sparsity to accelerate HE inference.\n\n3 THE SPENCNN FRAMEWORK\n\nIn this section, we present the technical details of our proposed SpENCNN framework. Figure 2 depicts its overall flow. The SpENCNN framework takes an initial CNN model as input and outputs a tailored CNN model after two processing stages–(1) HE-group Convolution is designed to reduce the outer-rotations caused by multi-channel convolution. In particular, we design an adjustable method and determine a theoretically optimal group number Gbase based on the size of the ciphertext and data packed in the CKKS HE scheme, to ensure that all outer-rotations can be eliminated while keeping model accuracy. (2) Sub-block Pruning is further proposed to reduce the number of inner-rotations. However, this is not trivial. We observe that to reduce as many inner-rotations as possible, we must precisely identify and completely prune selected sub-blocks. This, unfortunately, would result in an considerable accuracy drop. To address this issue, we develop a set of sub-steps which include identifying subblocks, pruning the subblocks, updating the remaining subblocks, and retraining the model for accuracy recovery.\n\n3.1 HE-GROUP CONVOLUTION\n\nTwo intuitions. Our proposed HE-group convolution is based on two intuitions. We observe that the number of required outer-rotations for a ciphertext is Rotouter = N/2×(h×w)pad − 1, where N is the polynomial degree defined in the cryptographic parameters, h and w are the height and width of the input feature map, and pad rounds a number to the next power of two. Each ciphertext generated by the outer-rotation further requires a set of inner-rotations. The number of inner-rotations is Rotinner = K 2 − 1, where k is the convolutional kernel size. Apparently, increasing the number of outer-rotation–Rotouter by just 1 can bring an extra Rotinner = K 2 − 1 inner-rotations. This gives us the first intuition–reducing the number of outer-rotations will fundamentally reduce the computational overhead.\n\n4\n\nInitial CNNmodelGroupConvolution Substitution Identify all sub-blocks in the modelPrune the sub-block with leastweights in restRetrain themodel to recoverthe accuracySub-block PruningHE-group ConvolutionTailoredCNN modelRepeatUpdate rest validsub-blocksUnder review as a conference paper at ICLR 2023\n\nFigure 3: (a) an example of HE convolution which generates 4 output channels from the 4 input channels with 3 × 3 kernels. By HE-group convolution, we only need 2 group-interleaved encoded cts multiply with corresponding pts and sum up to get the output channels data. (b) proposed HE-group convolution by a group-interleaved format.\n\nThe reason behind the outer-rotation is that multiple channels on the same ciphertext are involved in the same multichannel convolution. In our study, we find that convolution (e.g., depthwise convolution (Howard et al., 2017)) can be also performed individually within each single channel. We also find that the group convolution technique (Krizhevsky et al., 2012; Zhang et al., 2018; Ioannou et al., 2017) can reduce the number of channels involved in each group. This gives us the second intuition–reducing the number of channels in the same group can eliminate the outer-rotation.\n\nBased on these intuitions, we design the HE-group convolution. Given the group number G, we have the upper bound of the number of channels in the same group as ⌈(N/2·(h·w)pad) × 1/G⌉. Then the relationship between Rotouter and G can be expressed as:\n\nRotouter = ⌈(N/2·(h·w)pad) × 1/G⌉ − 1 (2) Accordingly, we can use an appropriate G value to cancel the first term, i.e., the Gbase = N/2·(h·w)pad in our design. Theoretically, this optimized value indicates zero outer-rotations.\n\nGroup-interleaved encoding. However, we find that the traditional ciphertext encoding format is not compatible when we implement our grouping idea. This is because the row-major format is mainly designed to perform convolution in the SIMD manner without considering the channel positions on the ciphertext. To address this issue, we propose a group-interleaved encoding format–the channel data from different groups are placed on the same ciphertext in an interleaved manner. This new encoding facilitates fast HE-group convolution without involving any outer-rotation.\n\nFigure 3 shows an example of proposed HE-group convolution and group-interleaved encoding. We assume that two ciphertexts contain 4 channels of data, and each ciphertext cti contains 2 channels. As shown in Figure 3 (a)–left, in general HE convolution, each cti has to do 1 outer-rotation to cover the 2 different channels, i.e., {ch1, ch2} and {ch2, ch1} for ct1. Convolution will be performed individually on each outer-rotated case for all ciphertexts. The encrypted output channels {out1, out2}, {out3, out4} can be generated after a summation.\n\nFor our HE group convolution, as Figure 3 (a)–right shows, sibling channels {ch1, ch2} and {ch3, ch4} from the same cti are in different convolution groups, which are encoded by our groupinterleaved format, i.e., {ch1, ch3} in ct1 and {ch2, ch4} in ct2. Now, the outer rotation is eliminated because each cti can perform 2 groups of convolution individually without rotating the channels. The encrypted output channels after summation, i.e., {out1, out3}, {out2, out4}, are naturally groupinterleaved and can immediately send to the next HE-group convolution. Figure 3 (b) further shows the generalized group-interleaved encoding, in which a ciphertext can encrypt M channels using the adjustable convolution group number G, with constraints G ≤ M and M %G = 0.\n\n3.2 SUB-BLOCK PRUNING\n\nWe design the sub-block pruning to further remove the remaining inner-rotations after the HE-group convolution. Our idea is to prune (zero out) a whole set of weights corresponding to specific innerrotations, so that the computational overhead of these inner-rotations can be eliminated. This reminds us of the weight sparsity in CNN models. In HE-convolution, an inner-rotated ciphertext will be\n\n5\n\nCh2Ch1out1out2out3out4out1out3out2out4Ch1Ch2Ch3Ch4Ch2Ch1Ch4Ch3HE-rotationCh1Ch3Ch2Ch4Ch1Ch2Ch3Ch4Group 1Group 2HE-group ConvolutionGeneral HE ConvolutionCh2Ch4Ch1Ch3Ch4Ch3ChChChCh ChChChChChGroup 1Group GGroup 2GGMGGroup-Interleaved Format(a)(b)HE-rotationUnder review as a conference paper at ICLR 2023\n\nFigure 4: (a) The weight sparse pattern in convolutional layers. For the same ct, their weight sparse patterns must be same. For different ct, the weight sparse pattern may change. (b) The weight sparse pattern in FC layers is in a diagonal-wise shape.\n\nmultiplied with the weights at the same position, namely “sub-block”, from all relevant kernels. Therefore, our design tends to cut out the same sparse pattern on these sub-blocks for all relevant kernels of the same ciphertext.\n\nAs the example in Figure 4 (a) shows, the 4 kernels of ct1 shares the same sparse pattern, thus eliminating 6 inner-rotations. This pruning scheme can be also extended to the FC layer. As shown in Figure 4 (b), the multiplication of ciphertext on FC layer is equivalent to the convolution using a diagonal-wise encoding method (Halevi & Shoup, 2014). Multiplying with the weights in one diagonal line requires one HE-rotated copy of ciphertext. (see Appendix A.3 for more details.)\n\nTo obtain the desired sparse pattern, we propose the sub-block pruning. The more sub-blocks pi being pruned, the less inner-rotations needed. This is actually an optimization problem, in which, we need to minimize the total number of sub-blocks while maintaining the prediction accuracy concurrently:\n\nmin{P =\n\n(cid:88)\n\npi · Ii} s.t. Acc(f (x; (W, P ))) ≥ Acc(f (x; W )) where Ii =\n\ni\n\nWe would like to find the sub-block with the minimum weight importance to the model at each iteration and prune, then, retrain the model for a few epochs to recover the accuracy. However, the sizes of sub-blocks are different. To measure the weight importance of each sub-block in a fair way. ||wpi || we define the weight importance metric as an average L2 norm dim(wpi ) . As described in algorithm 1, at each iteration, we would like to prune the sub-block with the least weight importance. The iterative algorithm would stop when the model accuracy is lower than the initial accuracy.\n\n(cid:26)0 pi is pruned other\n\n1\n\n(3)\n\n4 EVALUATION\n\n4.1 EXPERIMENT SETUP\n\nSetup. We conduct our experiments on a workstation equipped with an AMD Ryzen Threadripper 3975WX CPU, an NVIDIA RTX 3090 GPU, and 256GB of RAM. To evaluate our proposed SpENCNN, we select three baseline CNN models that are often adopted in HE inference performance evaluation, and implement them using PyTorch on GPU. This includes LeNet-like for MNIST\n\nModel\n\nLeNet-like VGG-5 HEFNet\n\nModel\n\nLeNet-like VGG-5 HEFNet\n\n# Layers FC 2\n3 1\n\nConv 2\n3 4\n\nAct 3\n5 4\n\nGroups Accuracy (Gbase) 4\n8 8\n\nEncryption Parameters Mult Level 10 16 13\n\nN 8192 16384 16384\n\nP 264 529 436\n\nQ 24 31 31\n\n(%) 98.95 84.06 83.67 Security Level >128 bit > 80 bit >128 bit\n\nTable 1: Three baseline Convolutional Neural Networks models and corresponding Encryption Parameters. LeNet-like is for the MNIST dataset. VGG-5 and HEFNet are for CIFAR-10 dataset.\n\n6\n\nAlgorithm 1 Sub-blocks Iterative Pruning\n\n1: Input: CNN model:f (x; (W, P )), 2: Remark: x-Data, W-Weights, P-HE blocks 3: Output: HE-friendly model:f (x; (W ′, P ′)) 4: P ′ = P = (cid:80) 5: While Accuracy loss ≤ 0 :\n\ni pi · Ii\n\n6:\n\ni ← argmin\n\n||wpi || dim(wpi )\n\ni\n\nIi = 0 prune weights in pi from current P update P ′ retrain model with P ′ and update W ′\n\n7: 8: 9: 10: 11: end While 12: Return f (x; (W ′, P ′))\n\n--------sub-blocksub-block(a) Weight sparsity in convolutional layers(b) Weight sparsity in FC layersUnder review as a conference paper at ICLR 2023\n\ndataset (2016), VGG-5 (Rathi et al., 2020) and the HE-friendly Net (HEFNet) for CIFAR10 dataset (Details of layer size is in Appendix A.2). Since the non-linear activation function like ReLU cannot be evaluated in HE, we replace ReLU with the adaptive quadratic polynomial function f (x) = ax2 + bx + c following the related works (Dathathri et al., 2019; Kim et al., 2022), where a, b, c are trainable parameters to maintain the model accuracy. Table 1 lists the specifications of these three models and the corresponding accuracy. In particular, the test accuracy for LeNet-like-MINST, VGG-5-CIFAR10, HEFNET-CIFAR10, is 98.95%, 84.06%. and 83.67%, respectively, which are consistent with their original versions.\n\nWe use Microsoft SEAL library v3.4.5 (SEAL) to implement the RNS-CKKS HE computation on these networks. Table 1 also lists the key parameters used in our RNS-CKKS encryption, including the polynomial degree N , the total modulus in bit-length Q, the scale factor in bit-length P to maintain the HE evaluation accuracy, and total multiplication level. These parameters can guarantee a security level of 80 bit for VGG-5, 128 bit for LeNet-like and HEFNet.\n\nMethodology. We first perform an ablation study to evaluate each individual technique’s effectiveness, and then compare the whole SpENCNN framework with the state-of-the-art method. y We adopt the average inference latency (in seconds) as the main measurement. An image set containing 20 different samples is used to measure and report the latency for these models. A lower latency indicates better performance. In addition, we measure the left holomorphic operation count (HOC, in %), sparsity (in %), and accuracy (in %) on the tailored models. The lower HOC and lower sparsity while offering higher accuracy are desired on all models.\n\n4.2 RESULTS\n\n4.2.1 EVALUATON ON HE-GROUP CONVOLUTION\n\nTable 2 lists our evaluation results for the HE-group convolution. We apply the HE group convolution alone (in ablation) to each baseline model and evaluate its effectiveness and scalability. In particular, we adjust the number of groups from its default (i.e., 1-baseline) until it exceed its Gbase according to our design (i.e., the highlighted 4, 8, and 8 for LeNet-like, VGG-5, and HEFNet, respectively). For detailed analysis, we breakdown HOC into “Rot” (HE-rotation) and “Others” (other operations including HE-Pmult and HE-add).\n\nOur HE-group convolution can be scaled to any convolutional model. As the number of groups increases, it can effectively reduce the number of HOC and maintain the accuracy, thus reducing the latency of HE inference and improving the performance. As listed in Table 2, the number of HE-rotation is reduced from 100% to 27.27%, 85.45%, and 11.95% on LeNet-like, VGG-5, and HEFNet, respectively. Once the Gbase is reached, the number of HE-rotation does not decrease further in spite of increasing the number of groups. This is because the outer-rotation is completely eliminated in HE-group convolution after the group number reaches Gbase. We also find that HE group convolution can reduce other HOC such as HE-Pmult and HE-add even after exceeding Gbase. This also contributes to the performance improvement.\n\nFor example, the HE-group convolution is particularly effective on our HEFNet (i.e., ∼ 88% and ∼ 86% reduction for HE-rotation and others, respectively). This is because it has the largest volume of convolution layers among the three models. Such a dramatic reduction in HOC further shortens the\n\nModel\n\nGroups\n\nLeNet-like\n\nVGG-5\n\nHEFNet\n\n1-baseline 2\n4 8\n1-baseline 4\n8 16 1-baseline 4\n8 16\n\nHOC Left (%) Others Rot -\n- 52.91 51.52 28.24 27.27 16.47 27.27 -\n- 84.08 87.53 81.42 85.45 80.10 85.45 -\n- 25.74 24.53 13.36 11.95 7.18 11.95\n\nAccuracy (%)\n\nLatency (s)\n\nSpeedup (×)\n\n98.95 98.95 98.95 98.67 85.16 84.53 84.06 82.23 84.91 84.35 83.67 80.06\n\n1.2658 0.6806 0.3807 0.3044 53.909 46.539 45.311 45.053 24.113 6.2491 3.2718 2.3627\n\n- 1.86 3.32 4.16 -\n1.16 1.19 1.20 -\n3.86 7.37 10.21\n\nTable 2: Ablation study of HE-group convolution with different number of convolution groups.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\ninference latency from 24.11s to 3.27s, which represents a 7.37× speedup. In contrast, the HE group convolution is the least effective in VGG-5, as it contains three large FC layers (size of 8192×4096), which cannot be substantially optimized using HE group convolution alone. There are more than 85% of HE-rotations and 80% of other operations that cannot be eliminated. And this number saturates as a lower bound after reaching the Gbase, resulting in a limited speedup of 1.19×.\n\nWe also observe that the model accuracy slightly decreases as the number of groups increases. This is due to the fact that fewer channels are involved in the HE-group convolution compared to the general convolution (see Figure 3). Fortunately, as long as the number of groups does not exceed our suggested Gbase, the loss of accuracy is marginal (i.e., 0%, 1.1%, and 1.2% on LeNet-like, VGG-5, and HEFNet, respectively). Also, our design is adjustable, allowing a trade-off between the accuracy and the optimized number of convolution groups.\n\n4.2.2 EVALUATION ON SUB-BLOCK PRUNING\n\nHere, we apply the sub-block pruning alone (in ablation) and compare it with other pruning methods such as Non-structural prune (Han et al., 2015a), and Structural-prune (Wen et al., 2016). Table 3 lists our evaluation results. We do not include accuracy in this evaluation since we prune each baseline model using each pruning method under the constraint of maintaining the original model accuracy. Instead, we include the sparsity (i.e., the percentage of pruned weights) for comparison.\n\nOur sub-block pruning can effectively improve the HE inference performance on all baseline models (i.e., the speedup of 2.62×, 6.15×, and 2.57× on LeNet-like, VGG-5, and HEFNet, respectively), which significantly outperforms other traditional pruning methods (i.e., marginal ∼ 1.1× speedup on most cases). The reason is obvious but significant. Our design is more HE-oriented and effective in the ciphertext domain while traditional pruning is more sparsity-oriented and for the plaintext computation efficiency only. For example, although NS-prune can prune ∼ 92% of the weights on VGG-5, it cannot eliminate the HE overhead (i.e., ∼ 96% HOC) caused by the remaining ∼ 8% of the weights.\n\nOur sub-block pruning method performs the best (i.e., ∼ 16% HOC) on VGG-5 because it effectively eliminates the inner-rotations caused by the large number of redundant weights in the FC layers. Together with the previous results (see Table 2), sub-block pruning can be a good complement to the HE-group convolution that performs weakly on the FC layers. We also note that our sub-block pruning method on LeNet-like (i.e., 35.21% Rot left) slightly outperforms HEFNet (i.e., 41.88% Rot left). This is because the larger convolutional kernel (i.e., 5 × 5) in LeNet-like gives our pruning method more space to optimize the inner-rotations.\n\n4.2.3 COMPARE WITH THE STATE-OF-THE-ART\n\nWe compare our method with the state-of-the-art HE-prune method–Hunter (Cai et al., 2022). The comparison results are presented in Table 4. In this evaluation, our method combines the HE-group convolution and the sub-block pruning, and uses the proposed Gbase as the group number (i.e., highlighted data). For a fair comparison, pruning is well controlled to ensure that our method and Hunter have the same level of accuracy (i.e., error≤ ±0.04%) on all baseline models. We can see from Table 4, our method outperforms the state-of-the-art significantly in terms of HOC, sparsity, and\n\nNetwork\n\nGroups\n\nLeNet-like\n\nVGG-5\n\nHEFNet\n\nDense-Baseline NS-prune S-prune (channel) Sub-block prune Dense-Baseline NS-prune S-prune (channel) Sub-block prune Dense-Baseline NS-prune S-prune (channel) Sub-block prune\n\nHOC Left (%) Others Rot -\n- 96.23 96.12 92.82 88.03 34.07 35.21 -\n- 97.14 97.59 98.08 98.47 16.11 15.89 -\n- 88.97 85.60 95.24 94.69 36.11 41.88\n\nSparsity (%)\n\nLatency (s)\n\nSpeedup (×)\n\n0.00 91.00 53.77 63.83 0.00 91.88 90.48 89.87 0.00 72.95 51.91 63.90\n\n1.2658 1.2190 1.1202 0.4644 53.909 52.5280 50.7178 8.7659 24.113 21.1660 22.9240 9.3709\n\n- 1.04 1.13 2.62 -\n1.03 1.06 6.15 -\n1.14 1.05 2.57\n\nTable 3: Ablation study of sub-block prune and comparison with other pruning methods.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nNetwork\n\nMethod\n\nLeNet-like\n\nVGG-5\n\nHEFNet\n\nBaseline Hunter Ours-4 Baseline Hunter Ours-8 Baseline Hunter Ours-8\n\nHOC Left (%) Others Rot -\n- 39.91 40.95 9.88 8.54 -\n- 18.93 17.86 7.72 7.86 -\n- 42.20 48.27 4.61 3.99\n\nSparsity Accuracy\n\n(%) 0\n59.99 62.62 0\n89.81 91.97 0\n57.82 65.62\n\n(%) 98.95 98.95 98.95 85.16 84.03 84.07 84.91 83.63 83.67\n\nLatency (s) 1.2658 0.5353 0.1535 53.909 9.9916 4.3830 24.113 10.855 1.2520\n\nSpeedup (×) -\n2.36 8.37 -\n5.40 12.11 -\n2.22 19.26\n\nTable 4: Comparison with Hunter on model HOC left, sparsity, accuracy,latency ,and speedup.\n\nlatency, on all baseline models. For example, our method eliminates 96% HE-rotations on HEFNet, achieving a 19.26× speedup. In contrast, the Hunter-optimized model still has 51% HE-rotations left behind and achieves only 2.22× speedup compared to the un-pruned baseline. This is because our method is designed to eliminate both outer and inner HE-rotations by synthetically applying the HE-group convolution and sub-block pruning, while the state-of-the-art is solely built upon the fixed structure pruning. We also print out the sparse pattern in a convolutional layer of LeNet-like, included in Appendix A.1.\n\n5 RELATED WORK\n\nCryptoNets (Gilad-Bachrach et al., 2016) is an initial attempt to realize HE-inference. After that, many subsequent works are proposed to improve the HE-inference latency from different aspects. Faster-CryptoNets (Chou et al., 2018) combines weight pruning and quantization to obtain a sparse polynomial representation to speed up the PMult operation, which achieves 6.38× latency reduction. LoLa (Brutzkus et al., 2019) successfully demonstrates the HE inference on a simple 3-layer model (1 convolutional layer and 2 FC layers) and achieves a 2.2s inference latency on the MNIST sample by leveraging HE schemes, data encoding format, and rotation techniques. CHET and HEAR (Dathathri et al., 2019; Kim et al., 2022) further refine the row-major coding format to achieve the same level of inference latency, but on a larger network (3 convolutional layers and minimum 64 channels).\n\nLou & Jiang (2021) propose a neural architecture search (NAS) based method to reduce the encryption parameters and speed up the HE-inference. Further, Ghodsi et al. (2020); Jha et al. (2021); Mishra et al. (2020); Lou et al. (2020) propose to reduce the cost of non-linear operations in NAS based HE-inference since operations like ReLU dominate the latency in the multi-party computation (MPC) setting. HE-PEx (Aharoni et al., 2022) and Hunter (Cai et al., 2022) attempt to structurally prune the weights to accelerate the HE-inference. In Hunter, a structural pruning method is proposed to facilitate HE in the MPC setting. HE-PEx adpots Hunter’s method to prune the weights in the FC layer only. It can reduce memory requirement and latency by 60% on the tested autoencoder models. Our work–SpENCNN is the first to orchestrate the ciphertext encoding and model sparsity design for HE inference acceleration, significantly outperforming these works.\n\n6 CONCLUSION\n\nIn this paper, we propose a fast LHE-based encrypted inference framework-SpENCNN built upon two novel techniques–HE-group convolution and sub-block weight pruning. Experimental results show that our solution can speed up the privacy-preserving inference by 8.37×, 12.11×, and 19.26× on LeNet-like, VGG-5, and HEFNet, respectively, greatly outperforming the state-of-the-art solutions. In the future, we would like to extend our work to deeper models and complex classification tasks in a no-client-interaction setting by leveraging bootstrapping. We hope to apply our framework to these deep models. There shall exist potential optimizations for the trade-off between model sparsification, data encoding, bootstrapping, and cryptographic parameters.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nTensorFlow 2016.\n\nLenet-like for convolutional mnist model example.\n\nhttps:\n\n//github.com/tensorflow/models/blob/v1.9.0/tutorials/image/ mnist/convolutional.py.\n\nEhud Aharoni, Moran Baruch, Pradip Bose, Alper Buyuktosunoglu, Nir Drucker, Subhankar Pal, Tomer Pelleg, Kanthi Sarpatwar, Hayim Shaul, Omri Soceanu, et al. He-pex: Efficient machine learning under homomorphic encryption using pruning, permutation and expansion. arXiv preprint arXiv:2207.03384, 2022.\n\nAlon Brutzkus, Ran Gilad-Bachrach, and Oren Elisha. Low latency privacy preserving inference. In\n\nInternational Conference on Machine Learning, pp. 812–821. PMLR, 2019.\n\nYifei Cai, Qiao Zhang, Rui Ning, Chunsheng Xin, and Hongyi Wu. Hunter: He-friendly structured pruning for efficient privacy-preserving deep learning. In Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security, pp. 931–945, 2022.\n\nJung Hee Cheon, Andrey Kim, Miran Kim, and Yongsoo Song. Homomorphic encryption for arithmetic of approximate numbers. In International conference on the theory and application of cryptology and information security, pp. 409–437. Springer, 2017.\n\nEdward Chou, Josh Beal, Daniel Levy, Serena Yeung, Albert Haque, and Li Fei-Fei. Faster cryptonets: Leveraging sparsity for real-world encrypted inference. arXiv preprint arXiv:1811.09953, 2018.\n\nRoshan Dathathri, Olli Saarikivi, Hao Chen, Kim Laine, Kristin Lauter, Saeed Maleki, Madanlal Musuvathi, and Todd Mytkowicz. Chet: an optimizing compiler for fully-homomorphic neuralnetwork inferencing. In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation, pp. 142–156, 2019.\n\nZahra Ghodsi, Akshaj Kumar Veldanda, Brandon Reagen, and Siddharth Garg. Cryptonas: Private inference on a relu budget. Advances in Neural Information Processing Systems, 33:16961–16971, 2020.\n\nRan Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In International conference on machine learning, pp. 201–210. PMLR, 2016.\n\nShai Halevi and Victor Shoup. Algorithms in helib. In Annual Cryptology Conference, pp. 554–571.\n\nSpringer, 2014.\n\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.\n\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\n\nefficient neural network. Advances in neural information processing systems, 28, 2015b.\n\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n\nYani Ioannou, Duncan Robertson, Roberto Cipolla, and Antonio Criminisi. Deep roots: Improving cnn efficiency with hierarchical filter groups. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1231–1240, 2017.\n\nNandan Kumar Jha, Zahra Ghodsi, Siddharth Garg, and Brandon Reagen. Deepreduce: Relu reduction for fast private inference. In International Conference on Machine Learning, pp. 4839–4849. PMLR, 2021.\n\nMiran Kim, Xiaoqian Jiang, Kristin Lauter, Elkhan Ismayilzada, and Shayan Shams. Secure human action recognition by encrypted neural network inference. Nature communications, 13(1):1–13, 2022.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nA Krizhevsky, I Sutskever, and GE Hinton. Imagenet classification with deep convolutional neural networks. 2012 advances in neural information processing systems (nips). Neural Information Processing Systems Foundation, La Jolla, CA, 2012.\n\nQian Lou and Lei Jiang. Hemet: A homomorphic-encryption-friendly privacy-preserving mobile neural network architecture. In International conference on machine learning, pp. 7102–7110. PMLR, 2021.\n\nQian Lou, Song Bian, and Lei Jiang. Autoprivacy: Automated layer-wise parameter selection for secure neural network inference. Advances in Neural Information Processing Systems, 33: 8638–8647, 2020.\n\nPratyush Mishra, Ryan Lehmkuhl, Akshayaram Srinivasan, Wenting Zheng, and Raluca Ada Popa. Delphi: A cryptographic inference service for neural networks. In 29th USENIX Security Symposium (USENIX Security 20), pp. 2505–2522, 2020.\n\nNitin Rathi, Gopalakrishnan Srinivasan, Priyadarshini Panda, and Kaushik Roy. Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=B1xSperKvH.\n\nNikola Samardzic, Axel Feldmann, Aleksandar Krastev, Nathan Manohar, Nicholas Genise, Srinivas Devadas, Karim Eldefrawy, Chris Peikert, and Daniel Sanchez. Craterlake: a hardware accelerator for efficient unbounded computation on encrypted data. In ISCA, pp. 173–187, 2022.\n\nSEAL. Microsoft SEAL (release 3.4). https://github.com/Microsoft/SEAL, October\n\n2019. Microsoft Research, Redmond, WA.\n\nNigel P Smart and Frederik Vercauteren. Fully homomorphic encryption with relatively small key and ciphertext sizes. In International Workshop on Public Key Cryptography, pp. 420–443. Springer, 2010.\n\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in\n\ndeep neural networks. Advances in neural information processing systems, 29, 2016.\n\nXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6848–6856, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 SPARSE PATTERNS\n\nWe present the weights after optimized in binary representation. In each group, it contains 64 weight kernels with size 5 × 5. Kernels in same group indicate that they are associated with the same ciphertext. Within the same group, across different ciphertexts, the sparse patterns are different.\n\nFigure 5: The sparse patterns for weight kernels in LeNet-like 2nd Convolutional layer.\n\nA.2 BASELINE MODEL ARCHITECTURE IN DETAIL\n\nThe following Table 5 contain the detailed convolutional kernels size, weight matrix size and number of channels. These three baseline models has different property. The LeNet-like model is a tiny model designed for simple classification task so that the channel number and weight matrix size is small, and has the least number of layers. The VGG-5 model has much more weights in FC layers and the max number of layers. The HEFNet contain the most convolutional layers and the widest channel size.\n\nNetwork\n\nLeNet-like\n\nVGG-5\n\nHEFNet\n\nLayer Conv1 Conv2 FC1 FC2 Conv1 Conv2 Conv3 FC1 FC2 FC3 Conv1 Conv2 Conv3 Conv4 FC1\n\n# Input channel 1\n32 64 32 3\n64 128 8192 4096 4096 3\n64 128 256 1024\n\n# Output channel Kernel size (Matrix size in FC)\n\n32 64 32 10 64 128 128 4096 4096 10 64 128 256 256 10\n\n5×5 5×5 64×32 32×10 3×3 3×3 3×3 8192×4096 4096×4096 4096×10 3×3 3×3 3×3 3×3 1024×10\n\nTable 5: Convolutional layer and Fully-connected layer size in three baseline models.\n\nA.3 MATRIX MULTIPLICATION IN FC LAYER\n\nHalevi & Shoup (2014) proposed a diagonal-wise multiplication of ciphertexts on FC layers. Given a N-element ciphertext ct = {x1..N }, the weights of FC layer can be reshaped into a M × N × N tensor { ⃗W } (will pad with zero if needed). For each N × N matrix W , we can multiply the diagonal N elements (as plaintext) with the rotated copies of ciphertext. As the example shown in Figure 6, we have ct = {x1..4} and a 4 × 4 matrix W . We have pt = {a11, a22, a33, a44},\n\n12\n\nGroup 1Group 2Under review as a conference paper at ICLR 2023\n\npt1 = {a41, a12, a23, a34}, pt2 = {a31, a42, a13, a24}, and pt3 = {a21, a32, a43, a14}. They will be multiplied by ct = {x1, x2, x3, x4}, Rot(ct, 1) = {x2, x3, x4, x1}, Rot(ct, 2) = {x3, x4, x1, x2}, and Rot(ct, 3) = {x4, x1, x2, x3}, respectively and then summed.\n\nFigure 6: Ciphertext multiply with a matrix.\n\n13\n\nMatrix Wct copies",
    "reference": "# Summary Of The Paper\n\nThe paper seeks to improve the computational efficiency of convolution layers in FHE. Since homomorphic rotations are the primary computational bottleneck of convolutions in FHE, the paper seeks to reduce the number of rotations. This is achieved in two steps, 1) adopting group convolutions, which reduces out-level rotations, and 2) weight pruning, which reduces inner-level rotations.\n\nThe efficiency of the proposed convolution is evaluated on *shallow* CNNs designed for MNIST and CIFAR-10. The proposed approach shows appreciable speed-up over *naive* implementations of convolutions in FHE.\n\n# Strength And Weaknesses\n\nStrengths:\n- The paper rightly identifies the main bottleneck of *naive* implementations of convolution in FHE, namely rotations. As such, efforts to improve efficiency of convolution is necessary. Leveraging alternative convolutions, such as group convolutions or in the extreme depth-wise convolutions is interesting.\n- Adopting pruning for sparsifying the convolution and optimizing the sparsity pattern for reducing number of homomorphic rotations.\n\nWeaknesses:\n- The main drawback of the paper is the lack of comparisons to prior work that improve efficiency of convolutions. These include multiplexed convolutions [1], mobile networks explored HEMET [2].\n- The main premise of the paper is that rotations in convolutional layers are the main computational bottleneck of networks in FHE. So the paper considers shallow networks only, which are not likely to be practically useful. For instance 85% accuracy on CIFAR-10 is quite poor by the standards of the best plaintext models which achieve ~99% accuracy.\n- As networks become deeper, the main accuracy bottleneck is low-degree polynomial approximations of non-linear functions like ReLU and the main computational bottleneck is the bootstrapping operations required for evaluating high-multiplicative depth circuits. So improving efficiency of convolutional layers does not benefit deeper networks since convolution is not the main bottleneck for such networks.\n\nOther Clarification Questions:\n- The paper does not mention how pooling operations or strided convolutions are handled. Strided convolutions result in wasted slots. How  does that affect the proposed convolutions?\n\n[1] HEMET: A Homomorphic-Encryption-Friendly Privacy-Preserving Mobile Neural Network Architecture, ICML 2021\n[2] Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions, ICML 2022\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- The paper is clear for the most part. The figures are a bit challenging to understand, but becomes clear with the description in the text.\n\n- The quality of the paper is good for the most part, however there are important baselines that are missing. Such methods have not been cited, discussed, or compared against.\n\n- The proposed method is fairly novel. Most existing CNN implementations in FHE using standard dense convolutional layers. This paper proposes to use group convolutions which are more HE friendly. Sparsity pattern is also optimized for minimizing rotations as opposed to other criterion used in standard networks.\n\n- The proposed approach is not reproducible based on the descriptions in the paper. There is missing information, hyper-parameters etc. And the paper does not provide code, nor do the authors promise to release code publicly later on.\n\n# Summary Of The Review\n\nThe paper proposed to use group convolutions and a weight pruning to mitigate the computational bottlenecks of convolutional layers, namely homomorphic rotations. The paper, however, does not compare to or discuss existing attempts toward HE-friendly CNNs. Furthermore, the experiments are conducted on shallow networks. The proposed approach will not provide much computational benefit for deeper networks since convolution is not the main bottleneck for such networks. Reproducibility is also limited.\n\nOverall, the paper has good ideas, but an evaluation, comparison, and discussion of the broader utility of the proposed approach are missing.\n\n**Update After Rebuttal:** The author's rebuttal does not adequately address the comments from the initial review. In theory, the method may have promise in achieving the claims in the rebuttal, but I do not believe it is straightforward and needs to be demonstrated. I will maintain the original rating.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTENSORVAE: A DIRECT GENERATIVE MODEL FOR MOLECULAR CONFORMATION GENERATION DRIVEN\n\nBY NOVEL FEATURE ENGINEERING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nEfficient generation of 3D conformations of a molecule from its 2D graph is a key challenge in in-silico drug discovery. Deep learning (DL) based generative modeling has recently become a potent tool to tackling this challenge. However, many existing DL-based methods are either indirect–leveraging inter-atomic distances or direct–but requiring numerous sampling steps to generate conformations. In this work, we propose a simple model abbreviated TensorVAE capable of generating conformations directly from a 2D molecular graph in a single step. The main novelty of the proposed method is focused on feature engineering. We develop a novel encoding and feature extraction mechanism relying solely on standard convolution operation to generate token-like feature vector for each atom. These feature vectors are then transformed through standard transformer encoders under a conditional Variational Autoencoder framework for generating conformations directly. We show through experiments on two benchmark datasets that with intuitive feature engineering, a relatively simple and standard model can provide promising generative capability rivalling recent state-of-the-art models employing more sophisticated and specialized generative architecture.\n\n1\n\nINTRODUCTION\n\nRecent advance in deep learning has enabled significant progress in computational drug design (Chen et al., 2018). Particularly, capable graph-based generative models have been proposed to generate valid 2D graph representation of novel drug-like molecules (Honda et al., 2019; Mahmood et al., 2021; Yu & Yu, 2022), and there is an increasing interest on extending these methods to generating 3D molecular structures which are essential for structured-based drug discovery (Li et al., 2021; Simm et al., 2021; Gebauer et al., 2022). A stable 3D structure or conformation of a molecule is specified by the 3D Cartesian coordinates of all its atoms. Traditional molecular dynamics or statistical mechanic driven Monte Carlo methods are computationally expensive, making them unviable for generating 3d molecular structures at scale (Hawkins, 2017). In this regard, deep learning(DL)-based generative methods have become an attractive alternative.\n\nDL-based generative methods may be broadly classified into three categories: distance-based, reconstruction-based, and direct methods. The main goal of distance-based methods is learning a probability distribution over the inter-atomic distances. During inference, distance matrices are sampled from the learned distribution and converted to valid 3D conformations through postprocessing algorithms. Two representative methods of this category include GraphDG (Simm & Hern ́andez-Lobato, 2019) and CGCF (Xu et al., 2021a). An advantage of modeling distance is its roto-translation invariance property–an important inductive bias for molecular geometry modeling (K ̈ohler et al., 2020). Additional virtual edges and their distances between 2nd and 3rd neighbors are often introduced to constrain bond angles and dihedral angles crucial to generating a valid conformation. However, Luo et al. (2021) have argued that these additional bonds are still inadequate to capture structural relationship between distant atoms. To alleviate this issue, DGSM (Luo et al., 2021) proposed to add higher-order virtual bonds between atoms in an expanded neighborhood region. Another weakness of the distance-based methods is the error accumulation problem; random noise in the predicted distance can be exaggerated by an Euclidean Distance Geometry algorithm, leading to generation of inaccurate conformations (Xu et al., 2022; 2021b).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTo address the above weaknesses, reconstruction-based methods directly model a distribution over 3D coordinates. Their main idea is to reconstruct valid conformations from distorted coordinates. GeoDiff (Xu et al., 2022) and Uni-Mol (Zhou et al., 2022) are pioneering studies in this respect. Though sharing similar idea, they differ in the process of transforming corrupted coordinates to stable conformations. While GeoDiff adapts a reverse diffusion process (Sohl-Dickstein et al., 2015), Uni-Mol treats conformation reconstruction as an optimization problem. Despite their promising performance, both methods require designing of task-specific and complex coordinate transformation methods. This is to ensure the transformation is roto-translation or SE(3)-equivariant. To achieve this, GeoDiff proposed a specialized SE(3)-equivariant Markov transition kernel. On the other hand, Uni-Mol accomplished the same by combining a task-specific adaption of transformer (Vaswani et al., 2017) inspired by the AlphaFold’s Evoformer (Jumper et al., 2021) with another specialized equivariant prediction head (Satorras et al., 2021). Furthermore, GeoDiff requires numerous diffusing steps to attain satisfactory generative performance which can be time consuming.\n\nCVGAE (Mansimov et al., 2019) and DMCG (Zhu et al., 2022) have attempted to resolve the generative efficiency issue by developing models that can produce a valid conformation directly from a 2D molecular graph in a single sampling step. Regrettably, the performance of CVGAE is significantly worse than its distance-based counterparts mainly due to the use of inferior graph neural network for information aggregation (Zhu et al., 2022). DMCG aimed to improve the performance of its predecessor by using a more sophisticated graph neural network and a loss function invariant to symmetric permutation of molecular substructures. Although DMCG achieved superior performance, acquiring such loss function requires enumerating all permutations of a molecular graph, which can become computationally expensive for long-sequence molecules.\n\nRegardless of their category, a common recipe of success for these models can be distilled to developing model architecture with ever increasing sophistication and complexity. There is little attention on input feature engineering. In this work, we forgo building specialized model architecture but instead focus on intuitive input feature engineering. We propose to encode a molecular graph using a fully-connected and symmetric tensor. For preliminary information aggregation, we run a rectangle kernel filter through the tensor in a 1D convolution manner. This operation has a profound implication; with a filter size of 3, the information from two immediate neighbors as well as all their connected atoms can be aggregated onto the focal atom in a single operation. It also generates tokenlike feature vector per atom which can be directly consumed by a standard transformer encoder for further information aggregation.\n\nThe generative framework follows the standard conditional variational autoencoder (CVAE) setup. We start with building two input tensors with one encoding only the 2D molecular graph and the other also encoding 3D coordinate and distance. Both tensors go through the same feature engineering step and the generated feature vectors are fed through two separate transformer encoders. The output of these two encoders are then combined in an intuitive way to form the input for another transformer encoder for generating conformation directly. The complete generative model is abbreviated as TensorVAE.\n\nIn summary, the proposed method has three main advantages. (1) Direct and Efficient, generating conformation direclty from a 2D molecular graph in a single step. (2) Simple, not requiring tasksepecific design of neural network architecture, relying only on simple convolution and off-the-shelf transformer architecture; (3) Easy to implement, no custom module required as both PyTorch and TensorFlow offer ready-to-use convolution and transformer implementation. These advantages translate directly to excellent practicality of the TensorVAE method. We demonstrate through extensive experiments on two benchmark datasets that the proposed TensorVAE, despite its simplicity, can perform competitively against 18 recent state-of-the-art methods for conformation generation and molecular property prediction.\n\n2 METHOD\n\n2.1 PRELIMINARIES\n\nProblem Definition. We formulate molecular conformation generation as a conditional generation task. Given a set of molecular graphs G and their corresponding i.i.d conformations R, the goal\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nis to train a generative model that approximates the Boltzman distribution, and from which a valid conformation conditioned on a molecular graph can be easily sampled in a single step.\n\nStory Line. In the ensuing sections, we breakdown the formulation of the proposed method in three novel ideas. We first introduce how the tensor encoding method is derived. Based on the tensor input, we propose a “naive” model that applies a convolutional neural network directly on the tensor to generate distance matrix, from which conformation are obtained through an Euclidean Distance Geometry algorithm. We also show that such model can already provide comparable performance to several advanced distance-based methods. Subsequently, we demonstrate how token-like feature vector can be generated from the input tensor by using 1D convolution operation. Finally, we elaborate on how to combine all the components together under a CVAE framework to arrive at the final generative model.\n\n2.2\n\nINPUT TENSOR GRAPH\n\nGraph neural network (GNN) is a popular feature extraction backbone for DL-based molecular conformation generation. The input of GNN in this case is composed of three components, including atom features, edge features and an adjacency matrix. Atom and edge features normally pass through separate embedding steps before being fed to the GNN. Adjacency matrix is then used to determine neighboring atoms for layer-wise information aggregation. Although bond features are aggregated onto atom features and vice versa, these two features are maintained separately throughout the message passing layers (Gilmer et al., 2017; Satorras et al., 2021). Instead of having separate inputs, our first simple idea is to combine them into a single input. Specifically, we add an additional dimension to the adjacency matrix, making it a tensor, similar to that used in a computer vision task. The diagonal section of the tensor holds the atom features.\n\nWe consider three types of atom features comprising atom type, charge and chirality. Each feature is onehot encoded and they are stacked together to form a single atom feature vector. There are two variants of the atom feature vector corresponding to two input tensors for the two encoders of the CVAE: an encoder conditioned only on graph (for which the tensor is referred to as the G tensor) and the other conditioned on both graph and coordinates (for which the tensor is referred to as the GDR tensor). For the GDR tenosr, every atom feature vector has three additional channels incorporating the 3D coordinate of the respective atom, and a distance channel filled with zeros.\n\nFigure 1: Benzene ring tensor graph example. Note that the values in the feature vector and its dimension are for demonstration purpose only. We explain how they are determined in Sec.3.\n\nThe off-diagonal section holds the bond features. The considered bond features are bond type, bond stereochemistry type, associated ring size and normalized bond length. A virtual bond is also included in the bond type. The normalized bond length is calculated as edge length (1 for direct neighbor, 2 for 2nd neighbor, etc.) divided by the longest chain length. It is worth noting that all high-order virtual bonds share the same virtual bond type; they only differ in their normalized bond length. To construct bond feature vector, we first sum the atom feature vectors of the related atoms. This new vector is then stacked with one-hot encoded bond type vector, normalized bond length, and one-hot encoded ring size vector to become the bond feature vector.\n\nThere are also two variants of the bond feature vector. For the G tensor, coordinate and distance channles are excluded from all bond feature vectors. For the GDR tensor, to match the size of the\n\n3\n\nvsCdvvvvdCsvvvvsCddCsvvvCdvvvssvvvdC0101001000.10.20.3Atom type encodingAtom charge encoding010100Atom chirality encoding1000.70.80.9Atom coord+020200200000020200200000101/50.1015/5Atom feature vectors summedCoordinate channels zeroedStacked with bond feature vectorBond typeNormalized Bond length0.511Euclidean distance 0000001001Bond stereochem In ring size00CCarbon atomdDouble bondsSingle bondvVirtual bondUnder review as a conference paper at ICLR 2023\n\natom feature vector, every bond feature vector has three more coordinate channels filled with 0s, and an additional distance channel holding the Euclidean distance between two connected atoms. This bond feature vector is obtained for all atom pairs, making the proposed tensor fully-connected and symmetric. Despite being referred differently, the structure of the bond feature vector and atom feature vector are the same. For both types in the GDR tensor, there are 5 blocks (3 blocks for G tensor) of channels stacked in the exact same order as following. Therefore, convolution over the input tensor is uniform for both on-diagonal and off-digonal channels.\n\n• atom feature channels (atom type, charge, and chirality);\n\n• atom coordinate channels (coordinate channels are excluded in the G tensor);\n\n• bond type feature channels (bond type and normalized bond length);\n\n• Euclidean distance channel (pairwise distance channel is excluded in G tensor);\n\n• other bond type feature channels (bond stereo-chem type and bond ring size)\n\nAn example input tensor graph of the benzene ring is illustrated in Fig.1. Having obtained the tensor representation, a naive way of building a generative model is to apply a convolutional neural network directly on the tensor, and train it to predict a distribution over the inter-atomic distances. We utilize a standard UNet (Ronneberger et al., 2015) structure to map the input tensor to a probability distribution over a distance matrix containing all pair-wise Euclidean distances. Distance matrices are then sampled and converted to valid conformations following the same method presented in GraphDG (Simm & Hern ́andez-Lobato, 2019). We refer to this model as the NaiveUNet. More details of the NaiveUNet can be found in Sec.A.3, and a further explaination of its poor performance can be found in Sec.A.8.\n\nDespite its naive nature, this model achieves a mean coverage (COV) score of 52.14 ± 1.48% and a mean matching (MAT) score of 1.4322±0.0247 ̊A on the GEOM-Drugs dataset, already comparable to several more complex distance-based baselines, as shown in Sec.3.2. However, there are two major issues to this approach. First, with a small kernel size (3 × 3 used in the UNet), it takes many convolution layers to achieve information aggregation between atoms that are far apart; it does not take full advantage of high-order bonds already made available in the input tensor. Secondly, the output size grows quadratically with the number of atoms, as compared to only linear growth in reconstruction-based or direct generation methods. The solution to the first issue is rather simple, obtained by increasing the kernel size to expand its “field of view”. On the other hand, solving the second issue requires elevating the naive two-step generative model to a direct one.\n\n2.3 EXTENDED KERNEL AND ATTENTION MECHANISM\n\nWe observe that every row or column of the proposed tensor contains global information of a focal atom and all of its connected atoms (by both chemical and virtual bond). This motivates our second main idea which is to extend the length of the kernel to the length of the tensor graph while keeping the width unaltered. This idea has a profound implication; information from the immediate neighbors, all their connected atoms, and all the bond features can be aggregated onto the focal atom in a single convolution operation. In contrast, achieving the same aggregation may require many layers of propagation for the naive model and other GNN-based models. A direct consequence of this modification is that only 1D convolution is permitted. With multiple kernels being applied simultaneously, each stride of these kernels generates a feature vector for a single atom. An illustration of the 1D convolution operation is shown in Fig.2.\n\nFigure 2: Extending kernel and 1D convolution. We further observe that the generated feature vectors resemble the token-like feature vectors used in language modeling. This observation combined with the proven success of attention mechanism\n\n4\n\nvsCdvvvvdCsvvvvsCddCsvvvCdvvvssvvvdCvsCdvvvvdCsvvvvsCddCsvvvCdvvvssvvvdCvsCdvvvvdCsvvvvsCddCsvvvCdvvvssvvvdC3 x 3 Conv KernelN x 3 Conv KernelExtendToken 1Token 2Token N1D ConvUnder review as a conference paper at ICLR 2023\n\nin other related work leads to the selection of transformer architecture as the backbone of our generative model. A significant advantage of using transformer’s self-attention mechanism is, similar to the extended kernel, it enables a global information aggregation from and for all atoms. It also eliminates the need to maintain separated atom and bond features at each step of feature transformation. We present further insight and a more detailed analysis of the adavantage of this input feature engineering in Sec.A.1\n\n2.4 PUTTING EVERYTHING TOGETHER\n\nConditional variational autoencoder framework. We aim at obtaining a generative model pθ(R|G) that approximates the Boltzmann distribution through Maximum Likelihood Estimation. Particularly, given a set of molecular graphs G and their respective ground-truth conformations R, we wish to maximize the following objective.\n\nlog pθ (R|G) = log\n\n(cid:90)\n\np (z) pθ (R|z, G) dz\n\n(1)\n\nA molecular graph can have many random conformations. We assume this randomness is driven by a latent random variable z ∼ p (z), where p (z) is a known distribution e.g. a standard normal distribution. As pθ (R|z, G) is often modeled by a complex function e.g. a deep neural network, evaluation of the integral in Eq.1 is intractable. Instead, we resort to the same techniques proposed in the original VAE (Kingma & Welling, 2013) to establish a tractable lower bound for Eq.1.\n\nlog pθ (R|G) ≥ Eqw(z|R,G) [log pθ (R|z, G)] − DKL [qw (z|R, G) ||p (z)]\n\n(2)\n\nwhere DKL is the Kullback-Leibler divergence and qw (z|R, G) is a variational approximation of the true posterior p (z|R, G). We assume p (z) = N (0, I) and qw (z|R, G) is a diagonal Gaussian distribution whose means and standard deviations are modeled by a transformer encoder. The input of this transformer encoder is the proposed tensor containing both the coordinate and distance information. We denote this tensor the GDR tensor. On the other hand, pθ (R|z, G) is further decomposed into two parts: a decoder pθ2 (R|z, σθ1 (G)) for predicting conformation directly and another encoder σθ1 (G) for encoding the 2D molecular graph. The input tensor for σθ1 (G) is absent of coordinate and distance information, and is therefore denoted the G tensor. Both encoders share the same standard transformer encoder structure. However, there is a minor modification to the transformer structure for the decoder. Specifically, the Query, Key matrices for the first multi-head attention layer are computed based on the output vectors of σθ1 (G), and the Value matrices come directly from the reparameterization of the output of qw (z|R, G), as z = μw + Σwε, where μw and Σw are the predicted mean and standard deviation respectively. ε is sampled from N (0, I). We present the complete picture of how the two encoders and the decoder are arranged in a CVAE framework in Fig.3a. We also show the illustration of the modified multi-head attention in Fig.3b.\n\nIntuition behind the modified attention. There are multiple ways to join together the output of the two encoders to form the input to the final decoder. Popular methods include stacking or addition. We tried both these methods with unsatisfactory performance. We notice that, due to direct stacking or addition of the sampled output of qw onto the output of σθ1, attention weights computed in the first layer of the decoder are easily overwhelmed by random noise of the sampled values, and become almost indiscernible1. This leads to ineffective information aggregation which is then further cascaded through the remaining attention layers. Intuitively, in the first attention layer, the attention weights dictating how much influence an atom exerts on the other should predominantly be determined by the graph structure, and remain stable for the same molecule. Further, attention weights are computed by Query and Key matrices. Therefore, these two matrices should stay stable for the same graph. This motivates our third and final main idea; that is, we compute Query and Key matrices only from the output (cid:8)hL (cid:9) of σθ1, and attribute the variation in conformation to the Value matrices which are directly sampled from {z1, ..., zN } ∼ qw. The resultant information aggregation is much more meaningful and each output vector corresponding to an individual atom carries distinct features, facilitating information aggregation of the ensuing attention layers.\n\n1 , ..., hL\n\nN\n\n1Imagine a mixture model with randomly varying mixture weights.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(b) Modified multi-head attention\n\n(a) Variational AutoEncoder framework\n\nFigure 3: TensorVAE model\n\nRoto-translation invariant loss. Following ConfVAE (Xu et al., 2021b), we formulate the reconstruction loss as.\n\nL (R) = − log pθ (R|z, G) = −\n\nN (cid:88)\n\n3 (cid:88)\n\n(cid:18)\n\ni=1\n\nj=1\n\nRij − A\n\n(cid:16) ˆR, R\n\n(cid:17)\n\n(cid:19)2\n\nij\n\n(3)\n\nwhere A (·) is a function aligning the predicted conformation ˆR onto the reference conformation R. We choose Kabsch algorithm (contributors, 2022) as the alignment method which translates and rotates the predicted conformation onto its corresponding ground-truth before loss computation. This makes the reconstruction loss roto-translation invariant. Finally, the KL-loss component DKL [qw (z|R, G) ||p (z)] does not involve any coordinate. Therefore, the objective function defined in Eq.2 is roto-translation invariant.\n\nDirect conformation generation at inference time. To generate a single conformation, we first construct the G tensor of a molecular graph and obtain a single latent sample {z1, ...zN } from a standard diagonal Gaussian distribution. The G tensor is passed through σθ1 encoder to produce (cid:9) which is then combined with the latent sample via the modified multi-head attention (cid:8)hL mechanism. The output of this modified attention layer further goes through L−1 standard attention layers to be transformed to the final conformation. The entire generation process depends only on a 2D molecular graph, and requires a single sampling step and a single pass of the TensorVAE model.\n\n1 , ..., hL\n\nN\n\n3 EXPERIMENT\n\nIn this section, we first elaborate on the implementation details of the TensorVAE model including determining the size of the input tensors, network architecture and how the entire framework is trained end-to-end. We then present conformation generation experiment results of the proposed TensorVAE on two benchmark data-sets, GEOM-QM9 and GEOM-Drugs. These results are compared to those of 11 state-of-the-art baselines. In addition to conformation generation, we present\n\n6\n\nMulti-head AttentionAdd & NormFeed ForwardAdd & NormLxG tensorGDR tensorL x transformer encoder blocksz1z2zNModified Multi-head AttentionAdd & NormFeed ForwardAdd & NormL-1 x transformer encoder blockR1R2RNKL Regularization lossReconstruction lossKabsch alignmentSelf-attentionSelf-attentionScaled Dot-product AttentionLinearLinearLinearq1k1q2k2qNkNKQVz1z2zNSplitConcatLinearUnder review as a conference paper at ICLR 2023\n\nfurther experiment on molecular property prediction in Sec.A.7, where the performance of the proposed method is compared against 7 more state-of-the-art baselines on the MolecularNet benchmark (Wu et al., 2018).\n\n3.1 EXPERIMENT SETUP\n\nDataset. Following existing work (Luo et al., 2021; Shi et al., 2021; Xu et al., 2021b;a; 2022; Zhou et al., 2022), we utilize the GEOM data-set for evaluating the performance of the proposed TensorVAE. GEOM contains 37 million energy and statistical weight annotated molecular conformations corresponding to 450,000 molecules (Axelrod & G ́omez-Bombarelli, 2022). This dataset is further divided into two constituent datasets, Drugs and QM9. The Drugs dataset covers 317,000 mediansized molecules averaging 44.4 number of atoms. The QM9 dataset contains 133,000 smaller molecules averaging only 18 atoms. We randomly select 40000 molecules from each dataset to form the training set. For each molecule, we choose the top 5 most likely2 conformations. This results in 200,000 training conformations for each train set. For validation set, we randomly sample 2,500 conformations for both Drugs and QM9 experiments. Finally, for testing, following (Shi et al., 2021; Xu et al., 2022), we randomly select 200 molecules each with more than 50 and less than 500 annotated conformations from QM9, and another 200 with more than 50 and less than 100 annotated conformations from Drugs3.\n\nDetermining input tensor graph size. We conduct a basic data analysis on the entire Drugs dataset to determine the 98.5th percentile of the number of atoms to be 69, and the percentage of molecules having more than 69 atoms and with more than 50 but less than 100 conformations is only 0.19%. Accordingly, we set the size of the input tensor to 69 × 69 for Drugs experiment. On the other hand, we use the maximum number of atoms 30 for QM9 experiment. The channel features for the input tensor include atom types, atom charge, atom chirality, bond type, bond stereo-chemistry and bond in-ring size. For the GDR tensor, we also include 3D coordinate channels and the computed distance channel. The resulting channel depth is 50 for GDR tensor and 46 for G tensor. The detailed information of these features and their encoding method is listed in Sec.A.4.\n\nImplementation details. We implement the proposed TensorVAE using Tensorflow 2.3.1. All three transformer encoders of TensorVAE follow the standard Tensorflow implementation in https: //www.tensorflow.org/text/tutorials/transformer. All of them have 4 layers, 8 heads and a latent dimension of 256. Both QM9 and Drugs experiments share the same network architecture and hyper-parameter configuration. We present the detailed training hyperparameter configuration in Sec.A.2.\n\nEvaluation metrics. We adopt the widely accepted coverage score (COV) and matching score (MAT) (Shi et al., 2021) to evaluate the performance of the proposed TensorVAE model. These two scores are computed as.\n\nCOV (Cg, Cr) =\n\n1 |Cr|\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:110)\n\nR ∈ Cr|RMSD\n\n(cid:16)\n\nR, ˆR\n\n(cid:17)\n\n≤ δ, ∀ ˆR ∈ Cg\n\n(cid:111)(cid:12) (cid:12) (cid:12)\n\nMAT (Cg, Cr) =\n\n1 |Cr|\n\n(cid:88)\n\nR∈Cr\n\nmin RMSD\n\n(cid:16)\n\nR, ˆR\n\n(cid:17)\n\n(4)\n\n(5)\n\nwhere Cg is the set of generated conformations and Cr is the corresponding reference set. The size of Cg is twice of that of Cr, as for every molecule, we follow (Xu et al., 2022) to generate twice the number of conformations as that of reference conformations. δ is a predefined threshold and is set to 0.5 ̊A for QM9 and 1.25 ̊A for Drugs respectively (Shi et al., 2021) . RMSD stands for the root-mean-square deviation between R and ˆR, and is computed using the GetBestRMS method in the RDKit (Riniker & Landrum, 2015) package. While COV score measures the ability of a model in generating diverse conformations to cover all reference conformations, MAT score measures how well the generated conformations match the ground-truth. A good generative model should have a high COV score and a low MAT score.\n\n2Ranked by their Boltzmann weight. 3This limit on the number of conformations for testing molecules is taken directly from https://\n\ngithub.com/DeepGraphLearning/ConfGF which is also followed by all other compared methods.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nBaselines. We compare the performance of the proposed TensorVAE model to those of 1 classical RDKit method; 5 distance-based methods including GraphDG, CGCF, ConfVAE, ConfGF and DGSM; 2 reconstruction-based methods including GeoDiff and Uni-Mol; 4 direct methods including CVGAE, GeoMol, DMCG and its symmetric permutation variant. The detailed information of the molecular property prediction baselines are presented in Sec.A.7\n\n3.2 RESULTS AND DISCUSSION\n\nThe COV and MAT scores for all compared methods on both QM9 and Drugs datasets are presented in Tab.1. All experiments follow the same test data generation configuration in (Shi et al., 2021). Additionally, we have conducted three 3 ablation studies on the input feature engineering method in Sec.A.8 to demonstrate why 1D convolution with a N × 3 kernel is necessary to achieve a good generative performance.\n\nTable 1: Performance comparison between TensorVAE and 11 other SOTAs on GEOM dataset.\n\nModels\n\nRDkit CVGAE GraphDG CGCF ConfVAE ConfGF GeoMol DGSM GeoDiff DMCG Uni-Mol\n\nTensorVAE1\n\nTensorVAE2\n\nQM9\n\nDrugs\n\nCOV (%) ↑\n\nMAT ( ̊A) ↓\n\nCOV (%) ↑\n\nMAT ( ̊A) ↓\n\nMean Median Mean\n\nMedian Mean Median Mean\n\nMedian\n\n83.26 0.09 73.33 78.05 80.42 88.49 71.26 91.49 92.65 94.98 97.95\n\n98.11 ±0.25\n\n97.11 ±0.31\n\n90.78 0.00 84.21 82.48 85.31 94.13 72.00 95.92 95.75 98.47 100\n\n100 ±0\n\n100 ±0\n\n0.3447 1.6713 0.4245 0.4219 0.4066 0.2673 0.3731 0.2139 0.2016 0.2365 0.1831\n\n0.2935 1.6088 0.3973 0.3900 0.3891 0.2685 0.3731 0.2137 0.2006 0.2312 0.1659\n\n0.1970 ±0.0016\n\n0.2041 ±0.0046\n\n0.1926 ±0.0027\n\n0.1920 ±0.007\n\n60.91 0.00 8.27 53.96 53.14 62.15 67.16 78.73 88.45 91.27 91.91\n\n94.91 ±0.35\n\n93.34 ±1.17\n\n65.70 0.00 0.00 57.06 53.98 70.93 71.71 94.39 97.09 100 100\n\n100 ±0\n\n99.90 ±0.31\n\n1.2026 3.0702 1.9722 1.2487 1.2392 1.1629 1.0875 1.0154 0.8651 0.8287 0.7863\n\n1.1252 2.9937 1.9845 1.2247 1.2447 1.1596 1.0586 0.9980 0.8598 0.7908 0.7794\n\n0.7789 ±0.0027\n\n0.8074 ±0.0135\n\n0.7585 ±0.0076\n\n0.7927 ±0.0186\n\n*Bold font indicates best result. Results for RdKit, CVGAE, GraphDG, CGCF, ConfGF are taken from (Shi et al., 2021); results for ConfVAE and GeoDiff are taken from (Xu et al., 2022); all other results are taken from (Zhou et al., 2022); TensorVAE1 results and standard deviations are obtained by running 10 experiements each with a different random seed on a single 200 testing molecules set. TensorVAE2 results and standard deviations are obtained by running 10 experiements each with a different random seed as well as a different set of 200 testing molecules.\n\nIn general, distance-based methods except for DGSM and ConfGF have relatively poor performance as compared to that of the classic RDKit. It has been argued that the the performance of RDkit is facilitated by an additional empirical force field (FF) (Halgren, 1996) optimization. Subsequently, CGCF and ConfVAE showed that with FF optimization, they can outperform RDkit. We also show the performance of the proposed TensorVAE with FF optimization compared to 5 other methods also employing FF optimization in Tab.3 in Sec.A.6, where TensorVAE outperforms all of them with a significant margin. However, this additional step further introduces complexity to the already complex two-stage generative model. ConfGF attempted to rectify this weakness by simulating a pseudo gradient-based force field. Such force field can be utilized in an annealed Langevin dynamics sampling to sequentially guide atom positions to a valid conformation. DGSM further improves the performance of ConfGF by using a dynamically constructed graph structure that is able to model long range atom interaction. Despite being posed as a direct method, both DGSM and ConfGF still need to compute atom distance as an intermediate step. Noticeably, DGSM also requires dynamically changing the graph structure for every sampling step. As shown in Tab.1, DGSM and ConGF outperform RDKit by a significant margin in both experiments. Nevertheless, their main weakness lies in the fact they they require numerous sampling steps to attain desirable performance. It has been reported in the ensuing work (Xu et al., 2022), that it took ConfGF approximately 8500 sec-\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nonds to fully decode 200 QM9 molecules and a staggering 11500 seconds for decoding 200 Drugs molecules. In constrast, TensorVAE takes only 62 seconds using a single Xeon 8163 CPU core to decode 200 QM9 molecules, and 128 seconds for 200 Drug molecules.\n\nA main goal of GeoDiff is to be free of the dependence on distance. To achieve this, transformations are directly applied to 3D conformation, through which, a random or distorted conformation can be sequentially denoised to a valid conformation. An essential requirement for such transformation is that it needs to be SE(3) equivariant. This necessitates designing of sophisticated equivariant transition kernel. Despite its claim of not involving computing distance as an intermediate step, the equivariant graph field network (Satorras et al., 2021) used still relies on a distance-like quantity (cid:13) (cid:13)xL for every step of transformation. GeoDiff produces promising performance on both datasets. Unfortunately, it needs numerous diffusion steps (T = 5000) for generating conformations, requiring approximately the same amount of decoding time as ConfGF (Xu et al., 2022).\n\ni − xL\n\n(cid:13) 2\n(cid:13)\n\nj\n\nUni-Mol circumvents this issue by reformulating molecular generation problem as an optimization problem. The structure of Uni-Mol is inspired by the Evoformer proposed in the Alphafold (Jumper et al., 2021) which also considers “atom-pair” interaction. This requires maintaining a pair interaction matrix (N × N ) through every attention layer. In addition, Uni-Mol has a significantly larger transformer structure as compared to ours, featuring 15 attention layers, 64 attention heads and a latent dimension size of 512. The Uni-Mol model is also first pretrained on a much larger dataset (19e6 molecules, each with 10 conformations), and then fine-tuned on the GEOM dataset for conformation generation. Despite using a much smaller and also standard transformer model without pretraining, the proposed TensorVAE outperforms Uni-Mol on the Drugs dataset. Further, we show that the proposed model also performs competitively against Uni-Mol on a molecular property prediction task in Sec.A.7 without any pretraining, validating the effectiveness of the input feature engineering.\n\nCVGAE is the first method proposed to generate molecular conformation directly from a 2D molecular graph. However, it yields the worst performance among the compared methods. DMCG attempted to revitalize the same framework by adapting a more advanced graph neural network structure combining GATv2 (Brody et al., 2021) with GN block (Battaglia et al., 2018). Noticeably, similar to Uni-Mol, it also maintains a N × N bond feature matrix throughout all layers. In constrast, we only use a standard transformer encoder architecture. Another major modification proposed in DMCG is introducing the permutation invariance of symmetric molecular substructures to the RMSD loss. Achieving this invariance requires enumerating all possible permutations which can become expensive for large molecules. The proposed TensorVAE outperforms DMCG with a significant margin. Finally, some samples of the TensorVAE generated conformations are shown in Sec.A.9\n\n4 CONCLUSION\n\nWe develop TensorVAE, a simple yet powerful model that is able to generate 3D conformation directly from a 2D molecular graph. Unlike many existing work focusing on designing complex neural network structure, we focus on developing novel input feature engineering techniques. We decompose these techniques into three main ideas, and explain how one idea naturally evolves to the next. We first propose a tensor representation of a molecular graph. Then, we demonstrate that sliding a rectangle kernel through this tensor in an 1D convolution manner can achieve complete information aggregation. Finally, we present the complete CVAE-based framework featuring 2 transformerbased encoders and another transformer-based decoder, and propose a novel modification to the first multi-head attention layer of the decoder to enable sensible integration of the output of the other two encoders. We show through extensive experiments that with intuitive feature engineering, simple and standard model architecture can provide competitive performance compared to 18 recent stateof-the-art models. For future work, we plan to extend our method and methodology to tackling the challenging protein structure prediction.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n5 REPRODUCIBILITY STATEMENT\n\nWe did not introduce nor have used any task-specific neural network archiecture. The results presented in this study can be straightforwardly reproduced using publically available datasets and ready-to-use implementation of convolution operation and Transformer from either PyTorch or TensorFlow. Specifically, we use the same transformer implementation found in https: //www.tensorflow.org/text/tutorials/transformer. To compute RMSD, we first zero-center both the predicted and the ground-truth conformations. Then, we obtain the optimal rotation matrix using the Kabsch algorithm implementation found in https://en. wikipedia.org/wiki/Kabsch_algorithm. It is straightforward to implement this algorithm in a python function. Please note that if TensorFlow is used, this python function needs to be wrapped inside a tf.py function, and then followed by tf.stop gradient to prevent gradient update. Finally, we rotate the predicted conformation onto its ground-truth for calculating the RMSD. The above details together with the already presented hyper-parameter setting should suffice to reproduce our results.\n\nREFERENCES\n\nSimon Axelrod and Rafael G ́omez-Bombarelli. Geom, energy-annotated molecular conformations for property prediction and molecular generation. Scientific Data, 9(1):185, 2022. doi: 10.1038/ s41597-022-01288-4. URL https://doi.org/10.1038/s41597-022-01288-4.\n\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\n\nShaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks?\n\narXiv\n\npreprint arXiv:2105.14491, 2021.\n\nHongming Chen, Ola Engkvist, Yinhai Wang, Marcus Olivecrona, and Thomas Blaschke. The rise\n\nof deep learning in drug discovery. Drug discovery today, 23(6):1241–1250, 2018.\n\nWikipedia contributors. Kabsch algorithm — Wikipedia,\n\nthe free encyclopedia.\n\nhttps:\n\n//en.wikipedia.org/w/index.php?title=Kabsch_algorithm&oldid= 1082253417, 2022. [Online; accessed 19-October-2022].\n\nXiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Geometry-enhanced molecular representation learning for property prediction. Nature Machine Intelligence, 4(2):127–134, 2022.\n\nHao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence Carin. Cyclical annealing schedule: A simple approach to mitigating kl vanishing. arXiv preprint arXiv:1903.10145, 2019.\n\nNiklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert M ̈uller, and Kristof T Sch ̈utt. Inverse design of 3d molecular structures with conditional generative neural networks. Nature communications, 13(1):1–11, 2022.\n\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017.\n\nThomas A Halgren. Merck molecular force field. v. extension of mmff94 using experimental data, additional computational data, and empirical rules. Journal of Computational Chemistry, 17(5-6): 616–641, 1996.\n\nPaul CD Hawkins. Conformation generation: the state of the art. Journal of chemical information\n\nand modeling, 57(8):1747–1756, 2017.\n\nShion Honda, Hirotaka Akita, Katsuhiko Ishiguro, Toshiki Nakanishi, and Kenta Oono. Graph\n\nresidual flow for molecular graph generation. arXiv preprint arXiv:1909.13521, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),\n\n2015. URL http://arxiv.org/abs/1412.6980.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nJonas K ̈ohler, Leon Klein, and Frank No ́e. Equivariant flows: exact likelihood generative learning for symmetric densities. In International conference on machine learning, pp. 5361–5370. PMLR, 2020.\n\nYibo Li, Jianfeng Pei, and Luhua Lai. Structure-based de novo drug design using 3d deep generative\n\nmodels. Chemical science, 12(41):13664–13675, 2021.\n\nShengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised representation for graphs, with applications to molecules. Advances in neural information processing systems, 32, 2019.\n\nShitong Luo, Chence Shi, Minkai Xu, and Jian Tang. Predicting molecular conformation via dynamic graph score matching. Advances in Neural Information Processing Systems, 34:19784– 19795, 2021.\n\nOmar Mahmood, Elman Mansimov, Richard Bonneau, and Kyunghyun Cho. Masked graph model-\n\ning for molecule generation. Nature communications, 12(1):1–12, 2021.\n\nElman Mansimov, Omar Mahmood, Seokho Kang, and Kyunghyun Cho. Molecular geometry pre-\n\ndiction using a deep generative graph neural network. Scientific reports, 9(1):1–13, 2019.\n\nSereina Riniker and Gregory A Landrum. Better informed distance geometry: using what we know to improve conformation generation. Journal of chemical information and modeling, 55(12): 2562–2574, 2015.\n\nYu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. Advances in Neural Information Processing Systems, 33:12559–12571, 2020.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015.\n\nVıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural net-\n\nworks. In International conference on machine learning, pp. 9323–9332. PMLR, 2021.\n\nChence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. In International Conference on Machine Learning, pp. 9558–9568. PMLR, 2021.\n\nGregor N. C. Simm, Robert Pinsler, G ́abor Cs ́anyi, and Jos ́e Miguel Hern ́andez-Lobato. Symmetryaware actor-critic for 3d molecular design. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=jEYKjPE1xYN.\n\nGregor NC Simm and Jos ́e Miguel Hern ́andez-Lobato. A generative model for molecular distance\n\ngeometry. arXiv preprint arXiv:1909.11459, 2019.\n\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256–2265. PMLR, 2015.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513–530, 2018.\n\nZhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et al. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism. Journal of medicinal chemistry, 63(16):8749–8760, 2019.\n\nMinkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative\n\ndynamics for molecular conformation generation. arXiv preprint arXiv:2102.10240, 2021a.\n\nMinkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael Gomez-Bombarelli, and Jian Tang. An end-to-end framework for molecular conformation generation via bilevel programming. In International Conference on Machine Learning, pp. 11537–11547. PMLR, 2021b.\n\nMinkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. arXiv preprint arXiv:2203.02923, 2022.\n\nKevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel GuzmanPerez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molecular representations for property prediction. Journal of chemical information and modeling, 59(8):3370– 3388, 2019.\n\nHongyang K Yu and Hongjiang C Yu. Powerful molecule generation with simple convnet. Bioin-\n\nformatics, 38(13):3438–3443, 2022.\n\nGengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. 2022.\n\nJinhua Zhu, Yingce Xia, Chang Liu, Lijun Wu, Shufang Xie, Tong Wang, Yusong Wang, Wengang Zhou, Tao Qin, Houqiang Li, et al. Direct molecular conformation generation. arXiv preprint arXiv:2202.01356, 2022.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 GLOBAL INFORMATION AGGREGATION BEYOND THE N th HOP\n\nA geometric interpretation of GNN’s message passing layer is it aggregates information between atoms (and their bond) that are 1-hop away. With L layers, information from atoms that are L-hop apart can be aggregated. Here, we define a global information aggregation as the N th-hop aggregation with N being the total number of atoms, where each atom is able to aggregate information from its farthest neighbour.\n\nIt is worth noting that for a fully-connected GNN, a 1-hop message passing can already achieve this global information aggregation. Transformer’s self-attention can be considered as a type of fullyconnected GNN. However, a vanilla transformer can only aggregate features from each token/atom; if edge features are not included, they needed to be incorporated somehow through additional inputs (e.g. the pair interaction matrix of Uni-Mol). The primary reason motivating the creation of the fully-connected tensor representation is we want each generated token contain both atom and bond features, such that we can eliminate the pair interaction or bond matrix. To achieve this, we fill each column of the fully-connected tensor with;\n\n• focal atom features;\n\n• chemical and virtual bond features indicating how the focal atom is connected to all other\n\natoms;\n\n• atom features of all connected atoms, since for each cell (except for cell of the focal atom)\n\nwe sum atom features of both the connected atom and the focal atom.\n\nIn fact, running a N × 1 kernel filter on the proposed tensor is conceptually similar to achieving a global information aggregation with a fully-connected GNN. By increasing kernel width to 3, the aggregation window also includes global information from two immediate neighbours. This type of information aggregation extends far beyond just N th-hop.\n\nMore interestingly, when multiple kernels are applied simultaneously to the same N × 3 × C region, each kernel is free to choose whichever group of atom/bond features to attend to depending on its kernel weights. This resembles the multi-head attention mechanism of a transformer, where each kernel(head) contributes to a portion of the generated feature token. We believe the effective global information aggregation driven by these two (tenor representation + 1D Conv) simple yet intuitive ideas is the main reason why the proposed TensorVAE achieves SOTA with much less number of parameters.\n\nA.2 TRAINING HYPERPARAMETERS\n\nTraining is conducted on a single Tesla V100 GPU. We follow a similar learning rate schedule, shown by Eq.3 of the original Transformer paper (Vaswani et al., 2017) but with dmodel = 9612. This results in a maximum learning rate of 1.6e−4. To tackle the notorious issue of KL vanishing (Fu et al., 2019), we set a minimum KL weight of 1e−4 and double it every 62.5e3 iterations until a maximum weight of 0.0256 is reached. We select Adam optimizer (Kingma & Ba, 2015) for training. We present some interesting observations of the training/validation curve corresponding to this setup in Sec.A.5. For both experiments, the TensorVAE is trained for 1e6 iterations with a batch size of 128. The implementation details of NaiveUNet is explained in Sec.A.3\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA.3 NAIVEUNET MODEL ARCHITECTURE\n\nFigure 4: Naive UNet model. N = 69\n\nWe train the above NaiveUNet on the Drugs dataset for 30 epochs with a constant learning rate of 1e−4, and batch size of 32. We follow the same method presented in GraphDG (Simm & Hern ́andezLobato, 2019) to convert the predicted distance matrix to conformation.\n\nA.4 ATOM AND BOND FEATURES\n\nWe list the atom features and bond features together with the encoding method used to construct the proposed tensor in Tab.2.\n\nTable 2: Atom and bond features used to construct input tensor.\n\nFeature name\n\nAtom type\n\nAtom charge Atom chirality\n\nBond type Normalized bond length Bond stereochem\n\nBond in-ring size Coordinate (3 channels) Pair wise atom distance\n\nFeature value\n\nEncoding method\n\nH, C, N, O, F, S, Cl, Br, P, I, Na, B, Si, Se, K, Bi -2, -1, 0, 1, 2, 3 Unspecified, Tetrahedral CW Tetrahedral CCW, Other Single, Double, Triple, Aromatic, Virtual -\nStereoNone, StereoAny, StereoZ StereoE, StereoCIS, StereoTrans 3 - 10 -\n-\n\none-hot\n\none-hot\n\none-hot\n\none-hot real-value\n\none-hot\n\none-hot real-value real-value\n\n14\n\nvsCdvvvvdCsvvvvsCddCsvvvCdvvvssvvvdC0000.10.23000000.10000000000.10.160.68000.10.80.310.7500.10.650.91.10.10N2N2/4N2/16N2/166464128128N2/16256N2/64256N2/256384N2/256512N2/256384N2/64256N2/16256N2/16128N2/16N2/412864N264N22EncoderDecoderEncoder BlockDecoder BlockSkip connectionFeature depth3 x 3 Conv kernelMasked output due to symmetryUnder review as a conference paper at ICLR 2023\n\nA.5 TRAINING AND VALIDATION CURVE\n\nWe present the train and validation plots for KL and reconstruction loss based on Drugs dataset in Fig.5a and Fig.5b, respectively. Both plots are based on an initial KL weight of 1e−4 doubling every 62.5k iterations (40 epochs). While KL validation loss reached 18.29 after 1e6 iterations (640 epochs), the reconstruction/RMSD loss reached 0.64 ̊A at the end of training. During the first 5 epochs of training, model learning focused on reducing the KL loss due to it is orders of magnitude larger than the RMSD loss. We were expecting this trend to continue for a while until both losses converge roughly in the same range. However, much to our surprise, the model seemed to find a way to drastically reduce RMSD loss much earlier by leveraging the information from the GDR encoder; it learned to ”cheat” by directly reversing coordinate information embedded in the output of GDR encoder back to the original conformation. The RMSD loss dropped to as low as 0.08 ̊A. On the other hand, the KL loss climbed to almost 800, signaling signifcant divergence from standard normal distribution. At this stage, output of the GDR encoder contains informative features of the original 3D coordinates. With the KL loss weight increasing, it becomes more difficult for the model to cheat since training is forcing the output of GDR encoder to conform to a standard uninformative Gaussian distribution. The KL loss started to drop while the RMSD loss remained steady, indicating increasing reliance on the output of G encoder for reconstructing the conformation. As the output of GDR encoder becomes less informative, the model learned to rely almost entirely on the aggregated feature from the G encoder to decode conformation.\n\nWe attempted to initiate the training with a much larger initial KL weight (1e−2) to prevent ”cheating” from begining. However, this quickly led to the notorious KL vanishing issue (Fu et al., 2019). We figure that ”cheating” is actually beneficial in that it reduces learning difficulty particularly for the decoder; its weights are tuned on easy training task, simply reversing what GDR encoder has done. In other words, the tuned weights of the decoder already hold crucial information on how to decode highly informative input features. As KL weight increases, model learning shifts to make the output of G encoder more informative. Also, this maybe an easier learning task as the RMSD loss is already very low (back-propagation of this loss contributes little to weight update); instead, model learning primarily focuses on optimizing the KL loss. This two-stage iterative loss optimization is much easier than optimizing both losses simultaneously throughout the training process.\n\n(a) KL loss.\n\n(b) RMSD loss.\n\nFigure 5: Training and validation plots for Drugs dataset. Orange line: Train; Blue line: Validation\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nA.6 GENERATION PERFORMANCE ON DRUGS DATASET WITH FORCE FIELD OPTIMIZATION\n\nTable 3: Performance comparison between methods with FF optimization.\n\nMethod\n\nCVGAE + FF GraphDG + FF CGCF + FF ConfVAE + FF GeoDiff + FF\n\nTensorVAE + FF\n\nCOV\n\nMAT\n\nMean Median Mean\n\nMedian\n\n83.08 84.68 92.28 91.88 92.27\n\n96.15 ±0.34\n\n95.21 93.94 98.15 100 100\n\n100 ±0\n\n0.9829 0.9129 0.7740 0.7634 0.7618\n\n0.9177 0.9090 0.7338 0.7312 0.7340\n\n0.6723 ±0.0023\n\n0.6605 ±0.0064\n\n*Results for CVGAE, GraghDG, CGCF, ConfVAE and GeoDiff are taken from (Xu et al., 2021b); Standard deviations are obtained by repeating experiments 10 times each with a different random seed on a test set with 14396 conformations.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nA.7 MOELCULAR PROPERTY PREDICTION RESULTS\n\nWe conduct further experiment on molecular property prediction to demonstrate the effectiveness of the proposed feature engineering method. Following Uni-Mol (Zhou et al., 2022) and GEM (Fang et al., 2022), we report property prediction result on the MolecularNet QM9 regression task (https://moleculenet.org/datasets-1). The goal of this task is to estimate homo, lumo, and homo-lumo gap properties of molecules in the QM9 dataset based on their molecular structure. This task is different from the conformation generation experiment in that both 3D coordinates and 2D molecular graph of a molecule are supplied as input to a prediction model. The accuarcy of property prediction depends on how well a model can extract and aggregate such input information among atoms.\n\nSimilar to Uni-Mol, we adapt the proposed GDR encoder to this regression task by changing its prediction head. Specifically, we use the same GDR transformer encoder structure as presented in Fig.3 (with only 4 attention layers) and add an additional mean pooling layer, which is then followed by a linear layer for property prediction. To obtain training data, we follow the same data train-valtest split4 in Uni-Mol and GEM and standardize the output property data. This results in 106362 train samples, 13299 val samples and 13356 test samples. We train the adapted model for 300 epochs with a batch size of 128. The learning rate schedule is the same as TensorVAE. We report the mean average error(MAE) over all the test samples.\n\nThe result of the adapted model is compared to those of 7 other models including;\n\n• D-MPNN (Yang et al., 2019), AttentiveFP (Xiong et al., 2019) and GEM which are GNN\n\nbased models without pretraining;\n\n• N-Gram (Liu et al., 2019), PretrainingGNN (Hu et al., 2019) and GROVER (Rong et al., 2020) with pretraining. In particular, GROVER integrates GNN into a Transformer architecture, and there are two variants with different model capacity, GROVERbase and GROVERlarge;\n\n• and finally, three variants of Uni-Mol including one without pretraining, another without\n\nusing the N × N pair representation matrix, and the complete version.\n\nThe MAE for all compared methods are summaried in Tab.4. The proposed method produces a competitive performance, only underperforming the complete Uni-Mol setup with pretraining and also considering pair representation. This experiment demonstrates that the proposed feature engineering method is very effective at global information aggregation.\n\nTable 4: Property prediction result comparison based on MolecularNet QM9 benchmark.\n\nMethod\n\nD-MPNN AttentiveFP N-Gram PretrainGNN GROVER base GROVER large GEM Uni-Mol w/o pair representation Uni-Mol w/o pretraining Uni-Mol GDR encoder (ours)\n\nMAE\n\n0.00814 (0.00001) 0.00812 (0.00001) 0.00964 (0.00031) 0.00922 (0.00004) 0.00984 (0.00055) 0.00986 (0.00025) 0.00746 (0.00001) 0.00573 (0.00004) 0.00653 (0.00040) 0.00467 (0.00004) 0.00553 (0.00012)\n\n*All results are taken from (Zhou et al., 2022). Values in parenthesis are standard deviation obtained by repeating experiments 4 times.\n\n4The data is split with a ratio of 8 : 1 : 1 using scaffold spliting while considering chirality\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nA.8 ABLATION STUDIES\n\nIn this section, we further demonstrate the effectiveness and necessity of running an 1D convolution with N × 3 kernels over the proposed input tensor through 3 ablation studies.\n\nWhy is 1D convolution necessary. We have shown a model based on a 3×3 kernel in Sec.A.3 called NaiveUNet. Here, we provide a more detailed analysis of why NaiveUNet produces unsatisfactory result. The primary reason for this poor performance is the “field of view” of a conventional d × d (d < N ) kernel only sees a partial connection pattern of a focal atom. In comparison, a N × 3 kernel’s “field of view” encompasses the complete connection pattern of a focal atom. We further observe that when applying a 3 × 3 kernel filter to the top left region of the proposed tensor, its field of view only includes a focal atom, its two neighboring atoms and how the focal atom is connected to them. There are two main disadvantages associated with this. Firstly, it only achieves a 1-hop information aggregation. Secondly when the 3 × 3 kernel moves to an off-diagonal part of the tensor, where most connections are virtual bonds (as atoms of a molecule are often sparsely connected), information aggregation occurs mostly between atoms that are not chemically connected and is therefore less meaningful than that on the diagonal part of the tensor. For these two reasons, the NaiveUNet’s performance on the GEOM Drugs dataset is the worst as shown in Tab.5.\n\nWhat happens if we remove all virtual bonds. Notice that if we remove all the virtual bonds in each column and still run a N × 3 kernel through the tensor, its “field of view” is a “2-hop atomicenvironment” (because the focal atom can “see” how neighboring atoms are chemically connected to all their direct neighbors). Another observation is that after removing all virtual bonds, each column does not correspond to a fully-connected GNN. Therefore it no longer enables a global information aggregation. The conformation generation results of this variant of TensorVAE on Drugs dataset is shown as as TensorVAE abla1 in table below. It is observed that due to local-only information aggregation as a result of removing all virtual bonds (and related atom features), the performance is worse than the complete TensorVAE version.\n\nWhat happens if a N × 1 kernel is used. The final ablation study concerns with using a N × 1 kernel with a smaller ”field of view” as compared to that of a N × 3 kernel. Its performance on Drugs dataset is shown as TensorVAE abla2 in Tab.5. It performs slightly better than the ablation removing all virtual bonds. The reason is that though its field of view is smaller, it still achieves a global information aggregation for the focal atom. Nevertheless, it underperforms the complete TensorVAE version due to a smaller ”field of view” for information aggregation.\n\nTable 5: Performance comparison among models with different input feature engineering setupon GEOM Drugs dataset\n\nMethod\n\nCOV\n\nMean\n\nMedian\n\nMean\n\nMAT\n\nMedian\n\nNaiveUNet TensoVAE abla1 TensoVAE abla2 TensorVAE\n\n52.14 ± 1.48 90.72 ± 1.54 91.04 ± 1.21 93.34 ± 0.35\n\n51.69 ± 1.17 99.53 ± 0.64 99.74 ± 0.42 99.90 ± 0.31\n\n1.4322 ± 0.0247 0.8748 ± 0.0161 0.8706 ± 0.0131 0.8074 ± 0.0135\n\n1.3861 ± 0.0173 0.8619 ± 0.0214 0.8561 ± 0.0204 0.7927 ± 0.0186\n\n*The standard deviations for all ablation studies are obtained by testing on 2000 testing molecules.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nA.9 SAMPLES OF GENERATED CONFORMATIONS\n\nFigure 6: Generated samples by the TensorVAE\n\n19\n\nGround TruthGeneratedGround TruthGeneratedGround TruthGeneratedGround TruthGenerated",
    "reference": "# Summary Of The Paper\n\nThe authors propose TensorVAE, a relatively simple model for generating 3D conformations from 2D molecular graphs. TensorVAE employs 1) a unique feature engineering step that represents each molecule as a tensor (with or without 3D coordinates and distances), and 2) a VAE with two transformer encoders, one that encodes the graph (using the tensor input without 3D information) and the approximate posterior that produces latents from the 3D information, and a transformer decoder for the likelihood, where keys and queries come from the 2D graph representation, and values are the latents from the 3D posterior encoder. The loss is a standard roto-translation invariant loss.  The authors show that, using standard transformer architectures and training procedures, TensorVAE performs comparably to the best current methods at conformation generation using the GEOM dataset, and argue that TensorVAE is much simpler than the comparable models due to superior feature engineering.\n\n# Strength And Weaknesses\n\nStrengths:\n- Simple architecture and featurization\n- Original use of two encoders for 2D and 3D features in the VAE formulation that allows for 3D conformation generation from 2D graphs\n- (Near) state-of-the-art results for conformation generation (and QM9 property prediction)\n\nWeaknesses:\n- It is not clear that the proposed method of producing atom-tokens via 1D convolution on the input tensor is necessary. Any number of possible aggregation steps could have been used, many of which would likely have yielded similar results. One could also imagine performing a Tucker decomposition and using singular values as tokens, etc.\n- Moreover, it's not even clear if the tensor formulation is needed. The 1D convolution aggregates information about the radius-1 atomic environment (including virtual bonds). One could use any radius-1 atomic-environment hash as tokens with the proposed featurization, which would likely yield comparable results.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe presentation is clearly written, and the work appears to be of sufficient quality.\n\nThe feature engineering is marginally original (see weaknesses above). Combining 3D latents as values with 2D embeddings as keys and queries in the VAE formulation appears to be a novel way of learning to generate 3D confirmations from 2D graphs.\n\nThe authors claim everything is straightforward and provide no code.\n\n# Summary Of The Review\n\nThe paper extends existing methods and molecular featurizations to achieve near state-of-the-art results for conformation generation. The paper spends a lot of time arguing for the superiority of the feature engineering used, but it is not at all clear that the results depend on that featurization. The authors could greatly improve this paper by demonstrating the value of the feature engineering beyond performance on benchmarks through ablation studies (e.g. removing aspects of the features) and studies of other featurizations that are not conflated with the tensor design (e.g., radius-1 atom environments).\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nHOW ROBUST IS UNSUPERVISED REPRESENTATION LEARNING TO DISTRIBUTION SHIFT?\n\nYuge Shi∗ Department of Engineering Science University of Oxford\n\nImant Daunhawer & Julia E. Vogt Department of Computer Science ETH Zurich\n\nPhilip H.S. Torr Department of Engineering Science University of Oxford\n\nAmartya Sanyal Department of Computer Science & ETH AI Center ETH Zurich\n\nABSTRACT\n\nThe robustness of machine learning algorithms to distributions shift is primarily discussed in the context of supervised learning (SL). As such, there is a lack of insight on the robustness of the representations learned from unsupervised methods, such as self-supervised learning (SSL) and auto-encoder based algorithms (AE), to distribution shift. We posit that the input-driven objectives of unsupervised algorithms lead to representations that are more robust to distribution shift than the target-driven objective of SL. We verify this by extensively evaluating the performance of SSL and AE on both synthetic and realistic distribution shift datasets. Following observations that the linear layer used for classification itself can be susceptible to spurious correlations, we evaluate the representations using a linear head trained on a small amount of out-of-distribution (OOD) data, to isolate the robustness of the learned representations from that of the linear head. We also develop “controllable” versions of existing realistic domain generalisation datasets with adjustable degrees of distribution shifts. This allows us to study the robustness of different learning algorithms under versatile yet realistic distribution shift conditions. Our experiments show that representations learned from unsupervised learning algorithms generalise better than SL under a wide variety of extreme as well as realistic distribution shifts.\n\n1\n\nINTRODUCTION\n\nMachine Learning (ML) algorithms are classically designed under the statistical assumption that the training and test data are drawn from the same distribution. However, this assumption does not hold in most cases of real world deployment of ML systems. For example, medical researchers might obtain their training data from hospitals in Europe, but deploy their trained models in Asia; the changes in conditions such as imaging equipment and demography result in a shift in the data distribution between train and test set (Dockès et al., 2021; Glocker et al., 2019; Henrich et al., 2010). To perform well on such tasks requires the models to generalise to unseen distributions — an important property that is not evaluated on standard machine learning datasets like ImageNet, where the train and test set are sampled i.i.d. from the same distribution.\n\nWith increasing attention on this issue, researchers have been probing the generalisation performance of ML models by creating datasets that feature distribution shift tasks (Koh et al., 2021; Gulrajani and Lopez-Paz, 2020; Shah et al., 2020) and proposing algorithms that aim to improve generalisation performance under distribution shift (Ganin et al., 2016; Arjovsky et al., 2019; Sun and Saenko, 2016; Sagawa et al., 2020; Shi et al., 2022). In this work, we identify three specific problems with current approaches in distribution shift problems, in computer vision, and develop a suite of experiments to address them.\n\n∗Corresponding author, yshi@robots.ox.ac.uk\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Synthetic vs. realistic distribution shift: The distribution shift in synthetic datasets (left, MNIST-CIFAR and CdSprites) are usually extreme and controllable (adjusted via changing the correlation); for realistic datasets (right, WILDS-Camelyon17 and FMoW) distribution shift can be subtle, hard to identify and impossible to control.\n\n1.1 EXISTING PROBLEMS AND CONTRIBUTIONS\n\nProblem 1: The outdated focus on supervised regime for distribution shift In ML research, distribution shift has been studied in various contexts under different terminologies such as simplicity bias (Shah et al., 2020), dataset bias (Torralba and Efros, 2011), shortcut learning (Geirhos et al., 2020), and domain adaptation and generalisation (Koh et al., 2021; Gulrajani and Lopez-Paz, 2020). Most of these work are carried out under the scope of supervised learning (SL), including various works that either investigate spurious correlations (Shah et al., 2020; Hermann and Lampinen, 2020; Kalimeris et al., 2019) or those that propose specialised methods to improve generalisation and/or avoid shortcut solutions (Arjovsky et al., 2019; Ganin et al., 2016; Sagawa et al., 2020; Teney et al., 2022). However, recent research (Shah et al., 2020; Geirhos et al., 2020) highlighted the extreme vulnerability of SL methods to spurious correlations: they are susceptible to learning only features that are irrelevant to the true labelling functions yet highly predictive of the labels. This behaviour is not surprising given SL’s target-driven objective: when presented with two features that are equally predictive of the target label, SL models have no incentive to learn both as learning only one of them suffices to predict the target label. This leads to poor generalisation when the learned feature is missing in the OOD test set.\n\nOn the other hand, in recent times, research in computer vision has seen a surge of unsupervised representation learning algorithms. These include self-supervised learning (SSL) algorithms (e.g., Chen et al. (2020a); Grill et al. (2020); Chen and He (2021)), which learn representations by enforcing invariance between the representations of two distinctly augmented views of the same image, and auto-encoder based algorithms (AE) (Rumelhart et al., 1985; Kingma and Welling, 2014; Higgins et al., 2017; Burda et al., 2016), which learn representations by reconstructing the input image. The immense popularity of these methods are mostly owed to their impressive performance on balanced in-distribution (ID) test datasets — how they perform on distribution shift tasks remains largely unknown. However, in distribution shift tasks, it is particularly meaningful to study unsupervised algorithms. This is because, in comparison to SL, their learning objectives are more input-driven i.e. they are incentivised to learn representations that most accurately represent the input data (Chen et al., 2020a; Alemi et al., 2017). When presented with two features equally predictive of the labels, unsupervised learning algorithms encourage the model to go beyond learning what’s enough to predict the label, and instead focus on maximising the mutual information between the learned representations and the input. We hypothesise that this property of unsupervised representation learning algorithms helps them avoid the exploitation of spurious correlations, and thus fare better under distribution shift, compared to SL.\n\nContribution: Systematically evaluate SSL and AE on distribution shift tasks. We evaluate and compare the generalisation performance of unsupervised representation learning algorithms, including SSL and AE, with standard supervised learning. See section 2 for more details on our experiments.\n\nProblem 2: Disconnect between synthetic and realistic datasets Broadly speaking, there exists two types of datasets for studying distribution shift: synthetic datasets where the shift between train/test distribution is explicit and controlled (e.g. MNIST-CIFAR (Shah et al., 2020), CdSprites (Shi et al., 2022)) and realistic datasets featuring implicit distribution shift in the real world (e.g. WILDS (Koh et al., 2021)). We provide visual examples in fig. 1.\n\nSynthetic datasets allow for explicit control of the distribution shift and are, thus, an effective diagnostic tool for generalisation performance. However, the simplistic nature of these datasets poses\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nOOD Accuracy (higher is better)\n\nShift Sensitivity (lower is better)\n\n(a) MNIST-CIFAR\n\n(b) CdSprites\n\n(c) Camelyon17-CS\n\n(d) FMoW-CS\n\n(e) Camelyon17\n\n(f) FMoW\n\nFigure 2: Performance of auto-encoder (AE), self-supervised learning (SSL), supervised learning (SL) models. Top row: the OOD test set accuracy (%) using linear heads trained on OOD data; Bottom row: Shift Sensitivity (see section 2 for definition), measures models’ sensitivity to distribution shift. Note here rid = 1 for Camelyon17-CS, FMoW-CS, and CdSprites (see sections 3.1.2 and 3.2.2).\n\nconcerns about the generality of the findings drawn from these experiments; a model’s robustness to spurious correlation on certain toy datasets is not very useful if it fails when tested on similar real-world problems. On the other hand, realistic datasets often feature distribution shifts that are subtle and hard to define (see fig. 1, right). As a result, generalisation performances of different algorithms tend to fluctuate across datasets (Koh et al., 2021; Gulrajani and Lopez-Paz, 2020) with the cause of said fluctuations remaining unknown.\n\nContribution: Controllable but realistic distribution shift tasks. In addition to evaluating models on both synthetic and realistic datasets, we subsample realistic domain generalisation datasets in WILDS to artificially inject explicit spurious correlations between domains and labels. This allows us to directly control the level of shift in these realistic distribution shift datasets. We refer to them as Controllable-Shift (CS) datasets. 1\n\nProblem 3: The linear classifier head strongly biases the evaluation protocol The most popular evaluation protocol for representation learning algorithms (both supervised and unsupervised) is linear probing. This involves freezing the weights of the representation learning model and training a linear classifier head on top of that to predict the labels. For distribution shift problems, this linear classifier is typically trained on the same training set as the representation learning backbone. Kang et al. (2020); Menon et al. (2020) observed the interesting phenomenon that this final layer linear classifier can be extremely susceptible to spurious correlations, causing poor OOD performance. Under simple, synthetic set up, they showed that SL models’ performance on OOD test set can be dramatically improved by simply retraining the linear classifier on data where the spurious correlation is absent. This indicates that the linear classifier can be a strong source of model bias in distribution shift tasks, and to disentangle the linear classifier bias from the generalisation performance of the learned representations, it is advisable to re-train the linear head on OOD data during evaluation. Note that although retraining linear classifier head is already standard practice in transfer learning, its application is necessary as the pre-training task and target task are typically different; on the other hand, retraining linear head is neither necessary nor standard practice in distribution shift problems, despite the recognition of linear head bias in recent work (Kang et al., 2020; Menon et al., 2020).\n\nContribution: OOD linear head. When reporting OOD accuracy, we use a linear head trained on small amount of left-out OOD data as opposed to ID data, as is standard practice. This allows us to isolate the bias of the linear head from the generalisability of learned representations. We also quantify the linear head bias to highlight the importance of this treatment. With these results, we wish to establish OOD linear head evaluation as a standard protocol for evaluating robustness of representation learning algorithms to distribution shift.\n\n1While datasets like ImageNet-9 and SVSF also provides distribution shift controls for real images, our sub-sampling of realistic domain generalisation datasets allows us to gain such control cheaply on a large set of datasets, and in addition provide further analysis on models’ performance on existing benchmarks (WILDS).\n\n3\n\nAESSLSL79.986.151.5AESSLSL52.582.744AESSLSL83.1180.9173.37AESSLSL18.2626.9225.73AESSLSL86.9189.886.84AESSLSL22.7929.6435.6AESSLSL2013.747.6AESSLSL47.517.356AESSLSL4.096.3510.72AESSLSL8.779.2912.26AESSLSL-0.21.633.5AESSLSL4.999.1537.7Published as a conference paper at ICLR 2023\n\nIn summary, we develop a suite of experiments and datasets to evaluate the performance of various representation learning paradigms under distribution shift. Figure 2 provides a summary of our results, comparing a range of methods from the following classes of algorithms: (i) SSL, (ii) AE, and (iii) SL. Note that though the intuition that unsupervised objectives should be better at distribution shift tasks is well-established in theory (Chen et al., 2020a; Alemi et al., 2017), state-of-the-art methods are predominantly developed under SL. To the best of our knowledge, we are the first to systematically evaluate and compare unsupervised representation learning methods to SL under distribution shift. The models are evaluated on both synthetic and realistic distribution shift datasets. Further, the models are also evaluated on CS datasets that contains controllable, explicit spurious correlations in realistic datasets. The main takeaways from this paper are:\n\n• SSL and AE are more robust than SL to extreme distribution shift: Figures 2a to 2d shows results on distribution shift scenarios where the training set encodes extreme spurious correlations. In this setting, for both synthetic (figs. 2a and 2b) and real world (figs. 2c and 2d) datasets, SSL and AE consistently outperforms SL in terms of OOD accuracy (top row);\n\n• Compared to SL, SSL and AE’s performance drop less under distribution shift: The bottom row of fig. 2 compares the shift sensitivity (s) of different models (see section 2 for definition). Smaller s is desirable as it indicates lower sensitivity to distribution shift. Results show that SSL and AE algorithms are significantly more stable under distribution shift than SL;\n\n• Generalisation performance on distribution shift tasks can be significantly improved by retraining the linear head: We show a large performance boost for all models, when evaluated using linear head trained on a small amount of OOD data, in contrast to the baseline linear head trained on ID data. The surprising gain of this cheap procedure, even on realistic problems, highlights the importance of isolating the linear head bias when evaluating generalisation performance.\n\n2 SETING UP\n\nIn section 1 we identified three problems in the existing literature that we wish to address in this work. In this section, we will introduce the necessary experimental set-up in further details. In brief, we compare eight ML algorithms on six datasets using three relevant metrics.\n\nAlgorithms: ×3 SSL, ×4 AE, ×1 SL. We compare seven unsupervised representation learning algorithms against SL, including three SSL algorithms 1) SimCLR (Chen et al., 2020a), 2) SimSiam (Chen and He, 2021), and 3) BYOL (Grill et al., 2020); and four AE algorithms 1) Autoencoder (Rumelhart et al., 1985), 2) Variational Autoencoder (VAE) (Kingma and Welling, 2014), 3) β-VAE (Higgins et al., 2017) and 4) Importance Weighted Autoencoder (IWAE) (Burda et al., 2016). These popular methods in SSL and latent generative models have not yet been systematically evaluated under distribution shift tasks prior to our work. We compare the performance of these models against a standard supervised learning (SL) algorithm used as a representation learning model.\n\nDatasets: ×2 synthetic, ×2 realistic, ×2 controllable shift. We evaluate our models on two synthetic datasets, namely MNIST-CIFAR (Shah et al. (2020); see section 3.1.1) and CdSprites (Shi et al. (2022); see section 3.1.2), as well as two realistic datasets from WILDS (Koh et al., 2021): Camelyon17 and FMoW (see section 3.2.1). However, as mentioned in section 1.1, both the synthetic and the realistic datasets have their own drawbacks. To further understand the models’ performance and draw conclusions that are generalisable, we also provide a framework for creating controllable shift datasets from realistic datasets like those in WILDS, by subsampling the data to introduce spurious correlations between the domain and label information in the training set. Changing this correlation varies the degree of distribution shift between the (ID) train and (OOD) test split, which allows us to analyse the models’ performance more effectively under realistic, yet controllable, distribution shift. We refer to this controllable shift versions of the two datasets Camelyon17-CS and FMoW-CS, and provide further details on the datasets in section 3.2.2.\n\nEvaluation: 3 metrics. Before discussing our proposed metrics, we first define some necessary notations. We separate a model trained to perform a classification task into two parts, namely, 1) backbone f , denoting the part of the model that generates representations from the data, and 2) final linear head c, which takes the representations from f and outputs the final prediction. Further, we refer to the final linear head trained on representations from the ID train set as ci, and that trained on representations from the OOD test set as co. Since the backbone f is always trained on the ID\n\n4\n\nPublished as a conference paper at ICLR 2023\n\ntrain set, we do not make any notation distinction on its training distribution. We also denote the accuracy of f and c on the ID test data as acci(f, c), and on the OOD test data as acco(f, c).\n\nAs noted in section 1.1, we report the OOD accuracy of the algorithms using linear heads trained on OOD data (instead of those trained on ID data as per standard practice), i.e. acco(f, co). This is necessary to disentangle the bias of the linear head from that of the representations. To highlight the importance of this treatment in isolating the generalisability of the representation learning algorithm from the that of the linear head, we also define the linear head bias. It is the difference between the OOD test accuracy evaluated by OOD linear head and that evaluated by the ID linear head, i.e.\n\nb = acco(f, co) − acco(f, ci).\n\n(1)\n\nIn a related work, Taori et al. (2020) proposed to evaluate the effective robustness of OOD generalisation defined as ρ = acci(f, ci) − acco(f, ci), which quantifies the drop in performance (e.g. accuracy) when evaluating the model on OOD test set vs. ID test set. A small ρ is desirable, as it indicates that the performance of the model is relatively insensitive to a distribution shift2. However, we note that a simple decomposition of effective robustness (ρ) shows a hidden linear head bias (b) term\n\nacci(f, ci) − acco(f, ci) (cid:123)(cid:122) (cid:125) (cid:124) effective robustness ρ\n\n= acci(f, ci) − acco(f, ci) − acco(f, co) + acco(f, co)\n\n= acco(f, co) − acco(f, ci) (cid:125)\n\n(cid:123)(cid:122) linear head bias b\n\n(cid:124)\n\n+ acci(f, ci) − acco(f, co) (cid:125)\n\n(cid:123)(cid:122) shift sensitivity s\n\n(cid:124)\n\n.\n\n(2)\n\nThus, we remove the effect of the linear head bias by subtracting b from ρ and reporting the last term in eq. (2). We refer to this as shift sensitivity : s = ρ − b. Alternatively, it is the difference between the OOD accuracy using linear head trained on OOD data, and ID accuracy using linear head trained on ID data. Larger s marks higher sensitivity of f to distribution shift, which is, possibly, dangerous for the deployment of such models. In summary, for each experiment we report the following three metrics: OOD linear head accuracy acco(f, co), linear head bias b and shift sensitivity s.\n\n3 EXPERIMENTAL RESULTS\n\nWe perform a hyperparameter search on learning rate, scheduler, optimiser, representation size, etc. for each model. We use the standard SSL augmentations proposed in He et al. (2020); Chen et al. (2020b) for all models to ensure a fair comparison. See appendix B for details.\n\n3.1 SYNTHETIC DISTRIBUTION SHIFT\n\nIn this section, we evaluate the performance of SL, SSL and AE algorithms on synthetic distribution shift tasks, utilising the MNIST-CIFAR dataset (Shah et al., 2020) and the CdSprites dataset (Shi et al., 2022). All results are averaged over 5 random seeds.\n\n3.1.1 MNIST-CIFAR\n\nFinding: Under this extreme distribution shift setting, SSL and AE significantly outperform SL. The OOD accuracy of SSL and AE can be notably improved by retraining the linear head on OOD data, however the OOD accuracy of SL remains low even with the OOD-trained linear head.\n\nThe MNIST-CIFAR dataset consists of concatenations of images from two classes of MNIST and CIFAR-10. In each concatenated image, the classes of the two datasets are either correlated or uncorrelated depending on the split as discussed below (See fig. 1, MNIST-CIFAR for an example):\n\n• ID train, test: Correlation between MNIST and CIFAR-10 labels is one. Each image belongs to one of the two classes: 1) MNIST “0” and CIFAR-10 “automobile”, and 2) MNIST “1” and CIFAR-10 “plane” (Figure 1, top row);\n\n• OOD train, test: Zero correlation between MNIST and CIFAR-10 labels, images from the two\n\nclasses are randomly paired (Figure 1, bottom row).\n\nSince the MNIST features are much simpler than the CIFAR features, a model trained on the ID train set can use MNIST only to predict the label, even though the CIFAR images are just as predictive\n\n2We negate the original definition of effective robustness from Taori et al. (2020) for ease of understanding.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Evaluations on the MNIST-CIFAR dataset. We report accuracy on MNIST and CIFAR trained using OOD linear head (acco(f, co)), linear head bias (b) and shift sensitivity (s).\n\nRegime Method\n\nAE\n\nAE VAE IWAE β-VAE\n\nAE average\n\nSSL\n\nSimCLR SimSiam BYOL\n\nSSL average\n\nMNIST (%)\n\nCIFAR (%)\n\nacco(f, co) ↑\n\ns ↓\n\nb\n\nacco(f, co) ↑\n\ns ↓\n\nb\n\n99.9 (±1e-2) 99.8 (±8e-3) 99.8 (±9e-3) 99.8 (±2e-2) 99.8 (±1e-2)\n\n99.7 (±1e-2) 99.8 (±2e-1) 99.8 (±4e-2) 99.8 (±8e-2)\n\n0.0 (±1e-2) -0.1 (±9e-3) 0.0 (±4e-3) 0.0 (±4e-2) 0.0 (±1e-2)\n\n0.2 (±1e-3) 0.1 (±2e-1) 0.0 (±1e-2) 0.1 (±5e-2)\n\n0.0 (±2e-3) 0.5 (±1e-4) 0.1 (±5e-3) -0.1 (±3e-2) 0.1 (±9e-3)\n\n-0.2 (±3e-3) 0.0 (±9e-2) 0.9 (±8e-3) 0.2 (±3e-2)\n\n81.1 (±1e+0) 79.7 (±4e+0) 80.8 (±2e+0) 78.0 (±3e+0) 79.9 (±3e+0)\n\n85.8 (±1e+0) 87.8 (±2e+0) 84.8 (±9e-1) 86.1 (±2e+0)\n\n18.8 (±1e+0) 20.2 (±3e+0) 19.0 (±3e+0) 21.8 (±4e+0) 20.0 (±4e+0)\n\n14.1 (±2e+0) 12.1 (±2e+0) 15.0 (±1e+0) 13.7 (±2e+0)\n\n30.2 (±1e+0) 29.2 (±6e+0) 30.0 (±4e+0) 28.0 (±4e+0) 29.3 (±4e+0)\n\n35.5 (±1e+0) 35.6 (±4e+0) 33.2 (±1e+0) 34.8 (±4e+0)\n\nSL\n\nSupervised\n\n97.7 (±9e-1)\n\n1.4 (±1e+0)\n\n-0.3 (±1e+0)\n\n51.5 (±1e+0)\n\n47.6 (±1e+0)\n\n0.8 (±9e-1)\n\n(Shah et al., 2020). This results in poor performance when predicting the CIFAR label on the OOD test set, where there is no correlation between the MNIST and CIFAR labels.\n\nWe train a CNN backbone on the ID train set using the eight SL, SSL and AE algorithms listed in section 2. At test time, we freeze the backbone and train two linear heads on ID train and OOD train set respectively, and evaluate their performance on the ID and OOD test set to compute 1) OOD linear head accuracy acco(f, co), 2) shift sensitivity s and, 3) linear head bias b. See results in table 1.\n\nWe observe that all models achieve near perfect performance when predicting the MNIST label on OOD test set, all with low shift sensitivity and small linear head bias. However, when predicting the labels of the more complex CIFAR images, unsupervised algorithms have a clear advantage over the supervised one: SSL achieves the highest OOD accuracy at 86.1%, followed by AE at 79.9% and SL at 51.5% (near random). The shift sensitivity s of the three objectives follow a similar trend, with SSL and AE scoring significantly lower than SL. This indicates that unsupervised representations are significantly less sensitive to distribution shift compared to those from SL, with the latter suffering a drop as large as 47.6%. Interestingly, the classifier head bias b for SSL and AE are relatively high (around 30%), and is very low for SL (0.8%), indicating that the representations learned from SL is intrinsically un-robust to distribution shift. That is, while there exist (linearly separable) CIFAR features in the representations of SSL and AE that can be extracted using a linear head trained on un-biased (OOD) data, these features are absent from the representations of SL.\n\n3.1.2 CDSPRITES\n\nFinding: Similar to MNIST-CIFAR, under extreme distribution shift, SSL and AE are better than SL; when the shift is less extreme, SSL and SL achieve comparably strong OOD generalisation performance while AE’s performance is much weaker.\n\nCdSprites is a colored variant of the popular dSprites dataset (Matthey et al., 2017), which consists of images of 2D sprites that are procedurally generated from multiple latent factors. The CdSprites dataset induces a spurious correlation between the color and shape of the sprites, by coloring the sprites conditioned on the shape following a controllable correlation coefficient rid. See fig. 1 for an example: when rid = 1 color is completely dependent on shape (top row, oval-purple, heart-cyan, square-white), and when rid = 0, color and shape are randomly matched (bottom row).\n\nShi et al. (2022) observes that when rid is high, SL model tend to use color only to predict the label while ignoring shape features due to the texture bias of CNN (Geirhos et al., 2019; Brendel and Bethge, 2019). First, we consider the setting of extreme distribution shift similar to MNIST-CIFAR by setting rid = 1 in the ID train and test splits. In the OOD train and test splits, the correlation coefficient is set to zero to investigate how well the model learns both the shape and the color features. Table 2 reports the three metrics of interest using the same evaluation protocol as before.\n\nSimilar to MNIST-CIFAR, we observe that all models achieve near perfect performance when predicting the simpler feature, i.e. color on the OOD test set. However, when predicting shape, the more complex feature on the OOD test set, SSL (and also AEs to a lesser extent) is far superior to SL. Additionally, the shift sensitivity of SSL (and AE to a lesser extent) are much smaller than SL, indicating that SSL/AE models are more robust to extreme distribution shift. The linear head bias also follows a similar trend as for MNIST-CIFAR, showing that representations learned using SL methods are inherently not robust to spurious correlations. This is not the case for SSL and AE algorithms where a large linear head bias shows that is the ID linear heads and not the representations that injects the bias.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Evaluations on the CdSprites dataset with rid = 1.0. We report accuracy for color and shape classifiers trained using OOD linear head (acco(f, co)), linear head bias (b) and shift sensitivity (s).\n\nRegime Method\n\nAE\n\nAE VAE IWAE\n\nAE average\n\nSSL\n\nSimCLR SimSiam BYOL\n\nSSL average\n\nColor classification (%)\n\nShape classification (%)\n\nacco(f, co) ↑\n\n100.0 (±0e+0) 99.7 (±3e-1) 100.0 (±0e+0) 99.9 (±9e-1)\n\n100.0 (±0e+0) 100.0 (±0e+0) 100.0 (±0e+0) 100.0 (±0e+0)\n\ns ↓\n\nb\n\nacco(f, co) ↑\n\ns ↓\n\nb\n\n0.0 (±2e-3) 0.3 (±3e-1) 0.0 (±2e-3) 0.1 (±9e-2)\n\n0.0 (±0e+0) 0.0 (±0e+0) 0.0 (±0e+0) 0.0 (±0e+0)\n\n0.3 (±5e-1) -0.3 (±3e-1) 0.4 (±5e-1) 0.1 (±4e-1)\n\n0.0 (±1e-1) 0.1 (±1e-1) 0.0 (±0e+0) 0.1 (±3e-2)\n\n46.1 (±6e-1) 52.4 (±2e+0) 58.9 (±2e+0) 52.5 (±2e+0)\n\n87.8 (±5e-1) 69.2 (±2e+0) 91.1 (±4e+0) 82.7 (±2e+0)\n\n53.9 (±6e-1) 47.6 (±2e+0) 41.1 (±2e+0) 47.5 (±2e+0)\n\n12.2 (±5e-1) 30.8 (±2e+0) 8.9 (±4e+0) 17.3 (±2e+0)\n\n12.7 (±5e-1) 18.9 (±3e+0) 25.6 (±2e+0) 19.1 (±2e+0)\n\n54.5 (±5e-1) 35.6 (±2e+0) 57.9 (±4e+0) 49.3 (±4e+0)\n\nSL\n\nSupervised\n\n100.0 (±0e+0)\n\n0.0 (±0e+0)\n\n0.0 (±3e-2)\n\n44.0 (±7e-1)\n\n56.0 (±7e-1)\n\n10.7 (±7e-1)\n\nControllable distribution shift We extend this experiment to probe the performance of these algorithms under varying degrees of distribution shifts. We generate three versions of the CdSprites dataset with three different correlation coefficients rid ∈ {0, 0.5, 1} of the ID train set. As before, the correlation coefficient of the OOD split is set to zeroThe rest of the experimental protocol stays the same. The OOD test accuracy and the shift sensitivity for varying rid is plotted in fig. 3 and a detailed breakdown of results is available in appendix A.\n\nFigure 3 shows that despite increasing distribution shift between the ID and OOD splits (with increasing rid) the OOD performance of SSL and AE does not suffer. However, the OOD accuracy of SL plummets and its shift sensitivity explodes at rid = 1. Interestingly, SSL maintains a high OOD test accuracy regardless of the level of distribution shift: when rid < 1 its performance is on par with SL, and when the distribution shift becomes extreme with rid = 1 it significantly outperforms SL both in terms of accuracy and shift sensitivity. In comparison, AE models’ accuracy lingers around 50%, with increasingly higher shift sensitivity as rid increases. However, under extreme distribution shift with rid = 1 it still performs better than SL, with slightly higher OOD accuracy and lower shift sensitivity.\n\n3.2 REAL-WORLD DISTRIBUTION SHIFT\n\nIn this section we investigate the performance of different objectives on real-world distribution shift tasks. We use two datasets from WILDS (Koh et al., 2021): 1) Camelyon17, which contains tissue scans acquired from different hospitals, and the task is to determine if a given patch contains breast cancer tissue; and 2) FMoW, which features satellite images of landscapes on five different continents, with the classification target as the type of infrastructure. See examples in Figure 1. Following the guidelines from WILDS benchmark, we perform 10 random seed runs for all Camelyon17 experiment and 3 random seed runs for FMoW. The error margin in Figure 3 represent standard deviation.\n\n3.2.1 ORIGINAL WILDS DATASETS\n\nFindings: SL is significantly more sensitive to distribution shift than SSL and AE; representations from SSL obtain higher OOD accuracy than SL on Camelyon17 but lower on FMoW. AE is consistently the least sensitive to distribution shift though it has the lowest accuracy. The performance of all models significantly improves by retraining the linear head on a small amount of OOD data.\n\nThe original Camelyon17 and FMoW dataset from WILDS benchmark both contains the following three splits: ID train, OOD validation and OOD test. We further create five splits specified as follows:\n\n• ID train, test: Contains 90% and 10% of the original ID train split, respectively; • OOD train, test: Contains 10% and 90% of the original OOD test split, respectively; • OOD validation: Same as the original OOD validation split.\n\nFollowing WILDS, we use OOD validation set to perform early stopping and choose hyperparameters; we also use DenseNet-121 (Huang et al., 2017) as the backbone for all models. We follow similar evaluation protocol as previous experiments, and in addition adopt 10-fold cross-validation for the OOD train and test set. See results in Tables 3 and 4, where following WILDS, we report performance on Camelyon17 using standard average accuracy and on FMoW using worst-group accuracy.\n\nOne immediate observation is that in contrast to our previous experiments on synthetic datasets, SL’s OOD accuracy is much higher in comparison on realistic distribution shift tasks: it is the best performing model on FMoW with 35.6% worst-group accuracy on OOD test set; its OOD accuracy is the lowest on Camelyon17, however it is only 3% worse than the highest accuracy achieved by SSL\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Evaluations on test set of Camelyon17, all metrics computed using average accuracy.\n\nTable 4: Evaluations on test set of FMoW, all metrics computed using worst-group accuracy.\n\nRegime Method\n\nAE\n\nAE VAE IWAE β-VAE\n\nAE average\n\nSSL\n\nSimCLR SimSiam BYOL\n\nSSL average\n\nMetrics (%)\n\nacco(f, co) ↑\n\ns ↓\n\nb\n\n84.4 (±2e+0) 88.1 (±2e+0) 88.1 (±1e+0) 87.1 (±4e+0) 86.9 (±2e+0)\n\n92.7 (±2e+0) 86.7 (±1e+0) 89.9 (±1e+0) 89.8 (±1e+0)\n\n-0.6 (±1e+0) 0.5 (±2e+0) -0.9 (±3e+0) 0.2 (±4e+0) -0.2 (±3e+0)\n\n0.4 (±1e+0) 3.1 (±1e+0) 1.4 (±1e+0) 1.6 (±1e+0)\n\n12.7 (±2e+0) 39.0 (±2e+0) 39.1 (±4e+0) 36.0 (±5e+0) 31.7 (±3e+0)\n\n8.3 (±1e+0) 7.9 (±3e+0) 10.3 (±2e+0) 8.8 (±2e+0)\n\nRegime Method\n\nAE\n\nAE VAE IWAE β-VAE\n\nAE average\n\nSSL\n\nSimCLR SimSiam BYOL\n\nSSL average\n\nMetrics (%)\n\nacco(f, co) ↑\n\ns ↓\n\nb\n\n26.9 (±9e-3) 21.7 (±6e-3) 20.9 (±2e-2) 21.7 (±5e-3) 22.8 (±3e-2)\n\n29.9 (±6e-3) 27.8 (±2e-2) 31.3 (±1e-2) 29.6 (±2e-2)\n\n6.4 (±6e-3) 4.7 (±4e-3) 5.5 (±1e-2) 3.4 (±6e-3) 5.0 (±7e-3)\n\n10.7 (±6e-3) 4.6 (±1e-2) 12.1 (±7e-3) 9.1 (±9e-3)\n\n5.8 (±1e-2) 8.0 (±2e-2) 7.8 (±1e-2) 7.6 (±8e-3) 7.3 (±1e-2)\n\n7.6 (±7e-3) 6.3 (±2e-2) 7.9 (±1e-2) 7.3 (±1e-2)\n\nSL\n\nSupervised\n\n86.8 (±2e+0)\n\n3.5 (±1e+0)\n\n7.4 (±3e+0)\n\nSL\n\nSupervised\n\n35.6 (±7e-3)\n\n37.7 (±4e-2)\n\n6.9 (±9e-3)\n\n(89.8%). This highlights the need to study realistic datasets along with synthetic ones. Nonetheless, we find that SSL is still the best performing method on Camelyon17 and achieves competitive performance on FMoW with accuracy 29.6% — despite learning without labels! AE has much lower OOD accuracy on FMoW compared to the other two methods: we believe this is due to its reconstruction-based objective wasting modelling capacity on high frequency details, a phenomenon frequently observed in prior work (Bao et al., 2021; Ramesh et al., 2021). Note that the standard deviation for all three methods are quite high for Camelyon17: this is a known property of the dataset and similar pattern is observed across most methods on WILDS benchmark (Koh et al., 2021).\n\nIn terms of shift sensitivity, unsupervised objectives including SSL and AE consistently outperforms SL — this stands out the most on FMoW, where the shift sensitivity of SSL and AE are 9.1% and 5.0% respectively, while SL is as high as 37.7%. This observation further validates our previous finding on synthetic datasets, that SSL and AE’s ID accuracy is a relatively reliable indication of their generalisation performance, while SL can undergo a huge performance drop under distribution shift, which can be dangerous for the deployment of such models. We highlight that, in sensitive application domains, a low shift sensitivity is an important criterion as it implies that the model’s performance will remain consistent when the distribution shifts. Another interesting observation here is that for all objectives on both datasets, the classifier bias b is consistently high. This indicates the bias of the linear classification head plays a significant role even for real world distribution shifts, and that it is possible to mitigate this effect by training the linear head using a small amount of OOD data (in this case 10% of the original OOD test set).\n\n3.2.2 WILDS DATASETS WITH CONTROLLABLE SHIFT\n\nFindings: SL’s OOD accuracy drops as more the distribution shift becomes more challenging, with SSL being the best performing model when the distribution shift is the most extreme. The shift sensitivity of SSL and AE are consistently lower than SL regardless of the level of shift.\n\nTo examine models’ generalisation performance under different levels of distribution shift, we create versions of these realistic datasets with controllable shifts, which we name Camelyon17-CS and FMoW-CS. Specifically, we subsample the ID train set of these datasets to artificially create spurious correlation between the domain and label. For instance, given dataset with domain A, B and label 0, 1, to create a version of the dataset where the spurious correlation is 1 we would sample only examples with label 0 from domain A and label 1 from domain B. See Appendix C for further details.\n\nSimilar to CdSprites, we create three versions of both of these datasets with the spurious correlation coefficient rid ∈ {0, 0.5, 1} in ID (train and test) sets. The OOD train, test and validation set remains unchanged3. Using identical experimental setup as in section 3.2.1, we report the results for Camelyon17-CS and FMoW-CS in Figure 3 with detailed numerical results in Appendix A.2.\n\nFor both datasets, the OOD test accuracy of all models drop as the spurious correlation rid increases (top row in Figure 3). However, this drop is the far more obvious in SL than in SSL and AE: when rid = 1, SL’s accuracy is 10% lower than SSL on Camelyon17 and 2% lower on FMoW — a significant drop from its original 3% lag on Camelyon17 and 5% lead on FMoW. This demonstrates that SL is less capable of dealing with more challenging distribution shift settings compared to SSL and AE. In terms of shift sensitivity (bottom row, fig. 3), SL’s remains the highest regardless of rid; curiously, we see a decrease in SL’s shift sensitivity as rid increases in FMoW-CS, however this has more to do with the ID test set accuracy decreasing due to the subsampling of the dataset.\n\n3Note that even when rid = 0, distribution shift between the ID and OOD splits exists, as the spurious\n\ncorrelation is not the only source of the distribution shift.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a) Camelyon17-CS\n\n(b) FMoW-CS\n\n(c) CdSprites\n\nFigure 3: Evaluations on Camelyon17-CS, FMoW-CS, and CdSprites with rid ∈ {0, 0.5, 1.0}. We report OOD test accuracy using OOD-trained linear head (acco(f, co)) and shift sensitivity (s). Blue lines are results averaged over AE models, green lines are SSL models and grey is SL.\n\n4 RELATED WORK\n\nWhile we are the first to systematically evaluate the OOD generalisation performance of unsupervised learning algorithms, there are other insightful work that considers the robustness to distribution shift of other existing, non-specialised methods/techniques. For instance, Liu et al. (2022) studies the impact of different pre-training set-ups to distribution shift robustness, including dataset, objective and data augmentation. Ghosal et al. (2022) focuses on architecture, and found that Vision Transformers are more robust to spurious correlations than ConvNets when using larger models and are given more training data; further, Liu et al. (2021) found that SSL is more robust to data imbalance. Azizi et al. (2022) also performed extensive studies on the generalisation performance of SSL algorithms on medical data. Interestingly, Robinson et al. (2021) also investigates the robustness of contrastive-SSL methods against extreme spurious correlation (i.e.simplicity bias). However, their work did not consider the linear head bias found in (Kirichenko et al., 2022; Kang et al., 2020) and led to opposing conclusions. In contrast, our work investigates the distribution shift performance of unsupervised algorithms, with experiments on both synthetic and realistic settings that go beyond the data imbalance regime. By isolating the linear head bias in our experiments, we find that unsupervised, especially SSL-learned representations, achieves similar if not better generalisation performance than SL under a wide range of distribution shift settings. See Appendix D for a more detailed discussion on distribution shift problems.\n\n5 CONCLUSION AND FUTURE WORK\n\nIn this paper, we investigate the robustness of both unsupervised (AE, SSL) and supervised (SL) objectives for distribution shift. Through extensive and principled experiments on both synthetic and realistic distribution shift tasks, we find unsupervised representation learning algorithms to consistently outperform SL when the distribution shift is extreme. In addition, we see that SSL’s OOD accuracy is comparable, if not better to SL in all experiments. This is particularly crucial, as most work studying distribution shift for images are developed in the SL regime. We hope that these results inspire more future work on unsupervised/semi-supervised representation learning methods for OOD generalisation. Another important finding is that unsupervised models’ performance remains relatively stable under distribution shift. This is especially crucial for the real-world application of these machine learning systems, as this indicates that the ID performance of SSL/AE algorithms are a more reliable indicator of how they would perform in different environments at deployment, while that of SL is not. It is also worth noting that while models trained with AE objectives are consistently the least sensitive to distribution shift on realistic datasets, their OOD performance can be low especially when presented with complex data (such as FMoW). This is consistent to the observation in prior work that these models can waste modelling capacity on high frequency details, and suggests that one should be careful about employing AE algorithms on large scale, complex tasks. Finally, a key contribution of this work is establishing the existence of linear head bias even for realistic distribution shift problems. We believe that using an OOD-trained linear head is necessary to be able to make comparisons between various algorithms irrespective of the final downstream task, and on the other hand, more efforts in the field of distribution shift could be devoted into re-balancing the linear layer.\n\n9\n\n0.00.51.0rid708090Test accuracySLSSLAE0.00.51.0rid203040Test accuracySLSSLAE0.00.51.0rid020406080100Test accuracy ()SLSSLAE0.00.51.0rid01020Shift Sensitivity sSLSSLAE0.00.51.0rid02040Shift Sensitivity sSLSSLAE0.00.51.0rid020406080100Shift Sensitivity s ()SLSSLAEPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nYS and PHST were supported by the UKRI grant: Turing AI Fellowship EP/W002981/1 and EPSRC/MURI grant: EP/N019474/1. We would also like to thank the Royal Academy of Engineering and FiveAI. YS was additionally supported by Remarkdip through their PhD Scholarship Programme. ID was supported by the SNSF grant #200021_188466. AS was partially supported by the ETH AI Center postdoctoral fellowship. Special thanks to Alain Ryser for suggesting the design of controllable versions of the WILDS dataset, and to Josh Dillon for helpful suggestions in the early stage of this project.\n\nREFERENCES\n\nAlexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information\n\nbottleneck. In International Conference on Learning Representations, 2017. 2, 4\n\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\n\narXiv:1907.02893, 2019. 1, 2, 20\n\nShekoofeh Azizi, Laura Culp, Jan Freyberg, Basil Mustafa, Sebastien Baur, Simon Kornblith, Ting Chen, Patricia MacWilliams, S Sara Mahdavi, Ellery Wulczyn, et al. Robust and efficient medical imaging with self-supervision. arXiv:2205.09723, 2022. 9\n\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. ArXiv preprint,\n\nabs/2106.08254, 2021. 8\n\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman\n\nVaughan. A theory of learning from different domains. Machine learning, 2010. 20\n\nWieland Brendel and Matthias Bethge. Approximating cnns with bag-of-local-features models works surprisingly well on imagenet. In International Conference on Learning Representations, 2019. 6\n\nYuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In\n\nInternational Conference on Learning Representations, 2016. 2, 4\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020a. 2, 4\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE Conference\n\non Computer Vision and Pattern Recognition, 2021. 2, 4\n\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\n\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020b. 5, 15\n\nVictor Guilherme Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A library of self-supervised methods for visual representation learning. Journal of Machine Learning Research, 2022. 15\n\nJérôme Dockès, Gaël Varoquaux, and Jean-Baptiste Poline. Preventing dataset shift from breaking\n\nmachine-learning biomarkers. GigaScience, 2021. 1\n\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 2016. 1, 2, 20\n\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. 6\n\nRobert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2020. 2, 19\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nSoumya Suvra Ghosal, Yifei Ming, and Yixuan Li. Are vision transformers robust to spurious\n\ncorrelations? arXiv:2203.09125, 2022. 9\n\nBen Glocker, Robert Robinson, Daniel C Castro, Qi Dou, and Ender Konukoglu. Machine learning with multi-site imaging data: an empirical study on the impact of scanner effects. arXiv:1910.04597, 2019. 1\n\nA. Gretton, AJ. Smola, J. Huang, M. Schmittfull, KM. Borgwardt, and B. Schölkopf. Covariate shift\n\nand local learning by distribution matching. 2009a. 20\n\nArthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernhard Schölkopf. Covariate shift by kernel mean matching. Dataset shift in machine learning, 2009b. 20\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent - A new approach to self-supervised learning. In Conference on Neural Information Processing Systems, 2020. 2, 4\n\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv:2007.01434,\n\n2020. 1, 2, 3\n\nSivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain generalization In IEEE Conference on Computer Vision and Pattern by learning a bridge across domains. Recognition, 2022. 20\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. 5, 15\n\nJoseph Henrich, Steven J Heine, and Ara Norenzayan. Most people are not weird. Nature, 2010. 1\n\nKatherine Hermann and Andrew Lampinen. What shapes feature representations? exploring datasets,\n\narchitectures, and training. Conference on Neural Information Processing Systems, 2020. 2\n\nIrina Higgins, Loïc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017. 2, 4\n\nWeihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give robust classifiers. In International Conference on Machine Learning, 2018. 20\n\nGao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. 7, 18\n\nDimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Conference on Neural Information Processing Systems, 2019. 2\n\nBingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. In International Conference on Learning Representations, 2020. 3, 9, 20\n\nByungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning not to learn: Training deep neural networks with biased data. In IEEE Conference on Computer Vision and Pattern Recognition, 2019. 19\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference\n\non Learning Representations, 2014. 2, 4\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nPolina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient\n\nfor robustness to spurious correlations. arXiv:2204.02937, 2022. 9, 20\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M. Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, 2021. 1, 2, 3, 4, 7, 8, 17, 18\n\nMasanori Koyama and Shoichiro Yamaguchi. Out-of-distribution generalization with maximal\n\ninvariant predictor. In arXiv e-prints, 2021. 20\n\nSebastian Lapuschkin, Stephan Wäldchen, Alexander Binder, Grégoire Montavon, Wojciech Samek, and Klaus-Robert Müller. Unmasking clever hans predictors and assessing what machines really learn. Nature communications, 2019. 19\n\nRonan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, Ashish Sabharwal, and Yejin Choi. Adversarial filters of dataset biases. In International Conference on Machine Learning, 2020. 19\n\nHong Liu, Jeff Z HaoChen, Adrien Gaidon, and Tengyu Ma. Self-supervised learning is more robust\n\nto dataset imbalance. arXiv:2110.05025, 2021. 9\n\nZiquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin, Xiangyang Ji, and Antoni B Chan. An empirical study on distribution shift robustness from the perspective of pre-training and data augmentation. arXiv:2205.12753, 2022. 9\n\nDavid Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In\n\nConference on Neural Information Processing Systems, 2017. 20\n\nXu Luo, Longhui Wei, Liangjian Wen, Jinrong Yang, Lingxi Xie, Zenglin Xu, and Qi Tian. Rectifying the shortcut learning of background for few-shot learning. Conference on Neural Information Processing Systems, 2021. 19\n\nLoic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement\n\ntesting sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017. 6\n\nAditya Krishna Menon, Ankit Singh Rawat, and Sanjiv Kumar. Overparameterisation and worst-case generalisation: friend or foe? In International Conference on Learning Representations, 2020. 3\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, 2021. 8\n\nJoshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive learning avoid shortcut solutions? Conference on Neural Information Processing Systems, 2021. 9, 20\n\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985. 2, 4\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust\n\nneural networks. In International Conference on Learning Representations, 2020. 1, 2, 20\n\nHarshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. In Conference on Neural Information Processing Systems, 2020. 1, 2, 4, 5, 6, 15, 19\n\nYuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In International Conference on Learning Representations, 2022. 1, 2, 4, 5, 6, 20\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In\n\nEuropean conference on computer vision, pages 443–450. Springer, 2016. 1\n\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33:18583–18599, 2020. 5\n\nDamien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton van den Hengel. Evading the simplicity bias: Training a diverse set of models discovers solutions with superior ood generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16761–16772, 2022. 2, 19\n\nAntonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521–1528.\n\nIEEE, 2011. 2, 19\n\nHaohan Wang, Songwei Ge, Zachary C. Lipton, and Eric P. Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, volume 32, pages 10506–10518, 2019. 20\n\nXingxuan Zhang, Linjun Zhou, Renzhe Xu, Peng Cui, Zheyan Shen, and Haoxin Liu. Towards unsupervised domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4910–4920, 2022. 20\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n(a) acco(f, co) (%).\n\n(b) Shift sensitivity s (%).\n\n(c) Linear head bias b (%).\n\nFigure 4: Evaluations on the CdSprites dataset with rid ∈ {0.25, 0.5, 0.75, 1.0}. We report shape classification accuracy using OOD-trained linear head (acco(f, co)), shift sensitivity s, and linear head bias b. Results are shown for individual models from the class of AE (blue), SSL (green), and SL (grey) algorithms. The black horizontal line denotes the random baseline (33.3% for three classes).\n\nA ADDITIONAL EXPERIMENTAL RESULTS\n\nA.1 CDSPRITES\n\nIn addition to our results in table 2, where we use a dataset with perfectly correlated features (rid = 1) to train the backbones, in fig. 4 we vary rid to analyse the effect of imperfectly correlated features. Notably, with imperfect correlation (rid < 1), the OOD linear heads trained on top of the SL and SSL backbones perform perfectly. For the AE, we observe that the performance of the OOD linear head does not depend on the correlation rid in the data used to train the backbones. Our results suggest that with imperfect correlation between features, SL and SSL models learn a linearly separable representation of the features, whereas AE does not.\n\nIn fig. 5 we provide an ablation where we also vary rood, the correlation in the data used to train and evaluate the linear head. Figures 5b and 5c corroborate our results that SSL performs on par with SL for rid < 1 and strictly better when rid = 1. For the AE (Figure 5a), we observe an interesting pattern where the performance of the OOD linear head depends on the OOD correlation rood, but not on the correlation rid in the data used to train the backbones. Hence, the ablation corroborates our result that SL and SSL models learn a linearly separable representation of the shape and color features when there is an imperfect correlation between the features, whereas AE does not.\n\n(a) AE\n\n(b) SSL\n\n(c) SL\n\nFigure 5: Correlation coefficient ablation for CdSprites. Shape classification accuracy for the CdSprites experiment with varying correlation of the ID training data (rid, x-axis) and OOD training and test data (rood, y-axis). Backbones were trained on data with correlation rid and linear classifiers trained and evaluated on top of the frozen backbones with correlation rood.\n\nA.2 CAMELYON17-C AND FMOW-C\n\nIn this section we report the numerical results for Camelyon17-C and FMoW-C with rid = 0.5 (see tables 5 and 6) and rid = 1 (see tables 7 and 8).\n\n14\n\n0.000.250.500.751.00rid020406080100Test accuracy ()AEVAEIWAESimCLRBYOLSimSiamSLrandom0.000.250.500.751.00rid020406080100Shift Sensitivity s ()AEVAEIWAESimCLRBYOLSimSiamSL0.000.250.500.751.00rid020406080100Linear head bias bAEVAEIWAESimCLRBYOLSimSiamSL0.00.250.50.751.0rid1.00.750.50.250.0rood1001001001001008383.683.383.783.868.168.267.768.768.457.356.855.957.356.453.252.351.552.951.14050607080901000.00.250.50.751.0rid1.00.750.50.250.0rood1001001001001001001001001009510010010099.991.610010010099.989.110010010099.986.44050607080901000.00.250.50.751.0rid1.00.750.50.250.0rood10010010010010010010010010079.410010010010065.510010010010051.710010010010044.4405060708090100Published as a conference paper at ICLR 2023\n\nTable 5: Evaluations on test set of Camelyon17C with rid = 0.5, all metrics computed using average accuracy.\n\nTable 6: Evaluations on test set of FMoW-C with rid = 0.5, all metrics computed using worstgroup accuracy.\n\nRegime Method\n\nAE\n\nAE VAE IWAE β-VAE\n\nAE average\n\nSSL\n\nSimCLR SimSiam BYOL\n\nSSL average\n\nMetrics (%)\n\nacco(f, co) ↑\n\ns ↓\n\nb\n\n80.4 (±3e+0) 88.6 (±2e+0) 87.8 (±1e+0) 88.5 (±2e+0) 86.3 (±2e+0)\n\n84.5 (±2e+0) 86.1 (±2e+0) 86.4 (±2e+0) 85.7 (±2e+0)\n\n6.0 (±2e+0) -0.5 (±1e+0) -0.2 (±1e+0) 0.1 (±9e-1) 1.4 (±1e+0)\n\n8.0 (±1e+0) 5.7 (±1e+0) 4.5 (±2e+0) 6.1 (±2e+0)\n\n19.0 (±4e+0) 17.8 (±6e+0) 26.4 (±6e+0) 19.7 (±6e+0) 20.7 (±5e+0)\n\n6.6 (±2e+0) 8.3 (±4e+0) 8.8 (±4e+0) 7.9 (±3e+0)\n\nRegime Method\n\nAE\n\nAE VAE IWAE β-VAE\n\nAE average\n\nSSL\n\nSimCLR SimSiam BYOL\n\nSSL average\n\nMetrics (%)\n\nacco(f, co) ↑\n\ns ↓\n\nb\n\n23.4 (±1e+0) 18.7 (±1e+0) 18.5 (±2e+0) 21.4 (±3e-1) 20.5 (±2e+0)\n\n29.5 (±9e-1) 27.9 (±2e+0) 32.6 (±3e+0) 30.0 (±2e+0)\n\n8.6 (±6e-1) 7.7 (±6e-1) 7.3 (±2e+0) 4.0 (±4e-1) 6.9 (±8e-1)\n\n9.2 (±6e-1) 7.5 (±1e+0) 7.9 (±2e+0) 8.2 (±1e+0)\n\n4.2 (±8e-1) 1.5 (±6e-1) 2.2 (±1e+0) 3.9 (±7e-1) 3.0 (±9e-1)\n\n6.9 (±7e-1) 4.6 (±1e+0) 8.5 (±2e+0) 6.7 (±1e+0)\n\nSL\n\nSupervised\n\n81.5 (±5e+0)\n\n13.2 (±3e+0)\n\n3.4 (±4e+0)\n\nSL\n\nSupervised\n\n32.3 (±3e+0)\n\n25.1 (±3e+0)\n\n6.0 (±2e+0)\n\nTable 7: Evaluations on test set of Camelyon17C with rid = 1, all metrics computed using average accuracy.\n\nTable 8: Evaluations on test set of FMoW-C with rid = 1, all metrics computed using worst-group accuracy.\n\nRegime Method\n\nAE\n\nAE VAE IWAE β-VAE\n\nAE average\n\nSSL\n\nSimCLR SimSiam BYOL\n\nSSL average\n\nMetrics (%)\n\nacco(f, co) ↑\n\ns ↓\n\nb\n\n75.7 (±5e+0) 86.0 (±3e+0) 86.1 (±1e+0) 84.7 (±2e+0) 83.1 (±3e+0)\n\n85.8 (±8e+1) 82.1 (±1e+0) 74.8 (±5e+0) 80.9 (±2e+0)\n\n7.3 (±2e+0) 2.7 (±1e+0) 2.6 (±7e-1) 3.8 (±1e+0) 4.1 (±1e+0)\n\n2.8 (±4e-1) 6.0 (±7e-1) 10.3 (±2e+0) 6.3 (±1e+0)\n\n35.1 (±4e+0) 12.4 (±4e+0) 9.1 (±3e+0) 15.5 (±4e+0) 18.0 (±4e+0)\n\n6.2 (±2e+0) 8.3 (±4e+0) -2.2 (±4e+0) 4.1 (±3e+0)\n\nRegime Method\n\nAE\n\nAE VAE IWAE β-VAE\n\nAE average\n\nSSL\n\nSimCLR SimSiam BYOL\n\nSSL average\n\nMetrics (%)\n\nacco(f, co) ↑\n\ns ↓\n\nb\n\n22.4 (±1e+0) 16.6 (±9e-1) 17.2 (±5e-1) 16.7 (±3e-1) 18.3 (±3e+0)\n\n26.3 (±1e+0) 27.1 (±5e-1) 27.4 (±2e+0) 26.9 (±6e-1)\n\n10.0 (±6e-1) 8.6 (±1e+0) 8.7 (±6e-1) 7.9 (±4e-1) 8.8 (±7e-1)\n\n10.6 (±8e-1) 6.3 (±7e-1) 11.1 (±1e+0) 9.3 (±1e+0)\n\n3.8 (±7e-1) 2.8 (±8e-1) 3.8 (±5e-1) 3.2 (±4e-1) 3.4 (±6e-1)\n\n7.2 (±1e+0) 7.8 (±3e-1) 6.5 (±2e+0) 7.2 (±1e+0)\n\nSL\n\nSupervised\n\n73.4 (±6e+0)\n\n10.7 (±3e+0)\n\n5.9 (±8e+0)\n\nSL\n\nSupervised\n\n25.7 (±5e-1)\n\n12.3 (±2e+0)\n\n5.3 (±1e+0)\n\nB ARCHITECTURE AND HYPERPARAMETERS\n\nIn this appendix we list the architecture and hyperparameters used in our experiments. Our code is developed on the amazing solo-learn code base (da Costa et al., 2022), which is originally developed as a library for SSL algorithms. For all experiments we follow the standard set of augmentations established in He et al. (2020); Chen et al. (2020b), including random resize crop, random color jittering, random grayscale, random Gaussian blur, random solorisation and random horizontal flip. An exception is the CdSprites experiment where we remove color jittering, as color classification is one of the tasks we are interested in and color jittering would add noise to the labels. For MNIST-CIFAR, we independently apply random augmentation to MNIST and CIFAR respectively (drawn from the same set of augmentations as detailed above) and then concatenate them to construct training examples.\n\nPlease see implementation details for each dataset in the respective subsection.\n\nB.1 MNIST-CIFAR\n\nWe use the same hyperparameter search range for models in each category of AE, SSL and SL, as outlined in table 9. The chosen hyperparameters for each model are specified in table 10.\n\nIn Shah et al. (2020) where MNIST-CIFAR was originally proposed, authors utilised more complex backbone architecture such as DenseNet and MobileNet. However in our experiments, we find that a lightweight 4-layer CNN can already achieve very high accuracy on both MNIST and CIFAR. The architecture of the CNN we use can be found in table 11. Note that for SL and SSL we only use the encoder and for AE we use the decoder as well. The size of base channel C and latent dimension L are found through hyperparameter search.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTable 9: Hyperparameter search range for MNIST-CIFAR, including base channel size of CNN (C), learning rate (lr.), weight decay (wd.), optimiser (optim.), learning rate scheduler (lr. scheduler).\n\nC\n\nlr.\n\nwd.\n\nOptim.\n\nlr. scheduler\n\nAE {16, 32, 64, 128} SSL {16, 32, 64, 128} {16, 32, 64, 128} SL\n\n{1e-4, 5e-4, 1e-3, 5e-3, 1e-2} uniformly sampled from [0.1, 1] {1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 1e-1, 5e-1}\n\n{0, 1e-4} {0, 1e-4} {0, 1e-4}\n\n{Adam, SGD} {Adam, SGD} {Adam, SGD}\n\n{warmup cosine, step, none} {warmup cosine, step, none} {warmup cosine, step, none}\n\nTable 10: Chosen hyperparameters for MNIST-CIFAR including latent dimension (L), base feature size of CNN (C), batch size (B), learning rate (lr.), weight decay (wd.), optimiser (optim.), learning rate scheduler (lr.scheduler).\n\nAE VAE IWAE β-VAE\n\nSimCLR BYOL SimSiam\n\nSupervised\n\nL\n\n128 128 128 128\n\n128 128 128\n\n128\n\nC\n\n16 32 32 16\n\n32 64 128\n\n16\n\nB\n\n128 128 128 128\n\n128 128 128\n\n128\n\nlr.\n\nwd. Optim.\n\nlr. scheduler\n\n1e-3 1e-4 1e-4 1e-4\n\n6e-1 7e-1 6e-1\n\n1e-4\n\n0 0\n0 0\n\n1e-4 0\n1e-5\n\n0\n\nAdam warmup cosine Adam warmup cosine Adam Adam\n\nstep step\n\nSGD SGD SGD\n\nSGD\n\nwarmup cosine warmup cosine warmup cosine\n\nwarmup cosine\n\nEncoder Input ∈ R3×64×32 4x4 conv. C stride 2x2 pad 1x1 & ReLU 4x4 conv. 2C stride 2x2 pad 1x1 & ReLU 4x4 conv. 4C stride 2x2 pad 1x1 & ReLU 4x1 conv. 4C stride 2x1 pad 1x0 & ReLU 4x4 conv. L stride 1 pad 0, 4x4 conv. L stride 1x1 pad 0x0\n\nDecoder Input ∈ RL 4x4 upconv. 4C stride 1x1 pad 0x0 & ReLU 4x1 upconv. 4C stride 2x1 pad 1x0 & ReLU 4x4 upconv. 2C stride 2x2 pad 1x1 & ReLU 4x4 upconv. C stride 2x2 pad 1x1 & ReLU 4x4 upconv. 3 stride 2x2 pad 1x1 & Sigmoid\n\nTable 11: CNN architecture, MNIST-CIFAR dataset.\n\nB.2 CDSPRITES\n\nWe found all models to be relatively robust to hyperparameters, as most configurations result in close to perfect shape and color classification accuracy on the ID validation set. The chosen hyperparameters for each model are specified in table 12. We omit β-VAE from the comparison, as we empirically found that β = 1 leads to the best performance on the ID validation set and therefore the results for the β-VAE would be similar to the VAE. We use the same augmentations (random crops and horizontal flips) for all models and use no color augmentations in order to keep the invariance of the learned representations with respect to color. The encoder and decoder architectures are described in table 13.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 14: Hyperparameter search range for Camelyon17, including decoder type, latent dimension (L), learning rate (lr.), weight decay (wd.), optimiser (optim.), learning rate scheduler (lr. scheduler).\n\nDecoder type\n\nL\n\nlr.\n\nwd.\n\nOptim.\n\nlr. scheduler\n\nAE SSL SL\n\n[CNN, MLP, ResNet] -\n-\n\n{256, 512, 1024} {256, 512, 1024} {256, 512, 1024}\n\n{1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 1e-1, 5e-1, 1} {1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 1e-1, 5e-1}\n\n{0, 1e-4} {0, 1e-3, 1e-4, 1e-5} {0, 1e-4}\n\n{Adam, SGD} {Adam, SGD} {Adam, SGD}\n\n{warmup cosine, step, none} {warmup cosine, step, none} {warmup cosine, step, none}\n\nTable 12: Chosen hyperparameters for CdSprites including latent dimension (L), base feature size of CNN (C), batch size (B), learning rate (lr.), weight decay (wd.), optimiser (optim.), learning rate scheduler (lr.scheduler).\n\nAE VAE IWAE\n\nSimCLR BYOL SimSiam\n\nL\n\n512 512 512\n\n64 64 64\n\nSupervised\n\n512\n\nC\n\n64 64 64\n\n32 32 32\n\n64\n\nB\n\n128 128 128\n\n64 64 64\n\n128\n\n5e-5 5e-5 5e-5\n\n5e-3 5e-1 8e-2\n\n5e-5\n\nlr.\n\nwd. Optim.\n\n1e-4 Adam 1e-4 Adam 1e-4 Adam\n\nnone\n\nnone none none\n\n1e-5 1e-5 1e-5\n\nSGD SGD SGD\n\nwarmup cosine warmup cosine warmup cosine\n\n1e-4 Adam\n\nnone\n\nEncoder Input ∈ R3×64×64 4x4 conv. C stride 2x2 pad 1x1 & ReLU 4x4 conv. 2C stride 2x2 pad 1x1 & ReLU 4x4 conv. 4C stride 2x2 pad 1x1 & ReLU 4x4 conv. 8C stride 2x2 pad 1x1 & ReLU 4x4 conv. L stride 1 pad 0\n\nDecoder Input ∈ RL 4x4 upconv. 8C stride 1x1 pad 0x0 & ReLU 4x4 upconv. 4C stride 2x2 pad 1x1 & ReLU 4x4 upconv. 2C stride 2x2 pad 1x1 & ReLU 4x4 upconv. C stride 2x2 pad 1x1 & ReLU 4x4 upconv. 3 stride 2x2 pad 1x1\n\nTable 13: CNN architecture, CdSprites dataset.\n\nB.3 CAMELYON17 AND FMOW\n\nFor hyperparameters including batch size, max epoch and model selection criteria, we follow the same protocol as in WILDS (Koh et al., 2021): for Camelyon17 we use a batch size of 32, train all models for 10 epochs and select the model that results in the highest accuracy on the validation set, and for FMoW the batch size is 32, max epoch is 60 and model selection criteria is worst group accuracy on OOD validation set. For the rest, we use the same hyperparameter search range for models in each category of AE, SSL and SL, as outlined in table 14. The chosen hyperparameters for Camelyon17 are specified in table 15, and for FMoW in table 16. For Camelyon17-C and FMoW-C we use these same hyperparameters.\n\nTable 15: Chosen hyperparameters for Camelyon17 including latent dimension (L), learning rate (lr.), weight decay (wd.), optimiser (optim.), learning rate scheduler (lr. scheduler).\n\nDecoder\n\nlr.\n\nwd. Optim.\n\nlr. scheduler\n\nAE VAE IWAE β-VAE\n\nSimCLR BYOL SimSiam\n\nSupervised\n\nResNet MLP MLP MLP\n\n- -\n-\n\n-\n\n5e-4 1e-4 1e-4 1e-4\n\n1e-1 1e-1 1e-1\n\n1e-3\n\n1e-5 0\n0 0\n\n0 1e-5 1e-5\n\n1e-3\n\n17\n\nSGD Adam Adam Adam\n\nSGD SGD SGD\n\nSGD\n\nwarmup cosine none none none\n\nnone warmup cosine warmup cosine\n\nnone\n\nPublished as a conference paper at ICLR 2023\n\nTable 16: Chosen hyperparameters for FMoW including latent dimension (L), learning rate (lr.), weight decay (wd.), optimiser (optim.), learning rate scheduler (lr. scheduler).\n\nDecoder\n\nlr.\n\nwd. Optim.\n\nlr. scheduler\n\nAE VAE IWAE β-VAE\n\nSimCLR BYOL SimSiam\n\nSupervised\n\nCNN MLP MLP MLP\n\n- -\n-\n\n-\n\n1e-1 1e-6 1e-6 1e-6\n\n5e-4 1e-2 5e-4\n\n1e-4\n\n1e-4 SGD 1e-4 Adam 1e-4 Adam 1e-4 Adam\n\n1e-3 1e-4 0\n\nSGD SGD SGD\n\n0\n\nAdam\n\nnone step step step\n\nstep step step\n\nstep\n\nWe follow Koh et al. (2021) and use DenseNet121 (Huang et al., 2017) as backbone architecture. For the decoder of the AE models, we perform hyperparameter search between three architectures: a CNN (see table 17), a simple 3-layer MLP (see table 18) and a ResNet-like decoder with skip connections (see table 19).\n\nCNN, Decoder Input ∈ RL 4x4 upconv. 8C stride 2x2 pad 1x1 & ReLU 4x4 upconv. 8C stride 2x2 pad 0x0 & ReLU 4x4 upconv. 4C stride 2x2 pad 1x1 & ReLU 4x4 upconv. 2C stride 2x2 pad 1x1 & ReLU 4x4 upconv. C stride 2x2 pad 1x1 & ReLU 4x4 upconv. 3 stride 2x2 pad 1x1 & Sigmoid\n\nTable 17: CNN architecture, Camelyon17 dataset.\n\nMLP, Decoder Input ∈ RL fc. 2L & ReLU fc. 4L & ReLU fc. 3*96*96 & ReLU\n\nTable 18: MLP architecture, Camelyon17 dataset.\n\nResNet, Decoder Input ∈ RL fc. 2048 & ReLU 3x3 conv. 16C stride 1x1 pad 1x1 3x3 conv. 16C stride 1x1 pad 1x1 x2 upsample 3x3 conv. 8C stride 1x1 pad 1x1 3x3 conv. 8C stride 1x1 pad 1x1 x2 upsample 3x3 conv. 8C stride 1x1 pad 1x1 3x3 conv. 8C stride 1x1 pad 1x1 3x3 conv. 3 stride 1x1 pad 1x1\n\nTable 19: ResNet decoder architecture, Camelyon17 dataset.\n\nC CONSTRUCTING CAMELYON17-CS AND FMOW-CS\n\nWe subsample Camelyon17 and FMoW dataset to create varying degree of spurious correlation between the domain and label information. We refer to these datasets as Camelyon17-CS and FMoW-CS. To construct such datasets, we first find some domain-label pairing in each dataset, such that if we sample the dataset according to this pairing, the population of each class with respect to the total number of examples in the dataset remains relatively stable. The rid = 1 versions of both Camelyon17-CS and FMoW-CS can be acquired by simply subsampling the dataset following the domain label pairing; to ensure fairness in comparison, when constructing the rid ∈ {0, 0.5} versions of these datasets, we first mix in anti-bias samples (i.e. samples that are not in the domain-label\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: Linear head bias on controllable shift datasets\n\npairing) to change the spurious correlation, and then subsample the dataset such that the size of the dataset is the same as the rid = 1 version.\n\nThe domain-label pairing of Camelyon17-CS can be found in table 20 and FMoW-CS in table 21.\n\nLinear head bias We also plot the linear head bias for the experiments conducted on Camelyon17CS, FMoW-CS, and CdSprites in Figure 6. The experimental protocol follows that from Figure 3.\n\nTable 20: Domain-label pairing for Camelyon17-CS.\n\nDomain (hospital)\n\nLabel\n\nHospital 1, 2 Hospital 3\n\nBenign Malignant\n\nTable 21: Domain-label pairing for FMoW-CS.\n\nDomain (region)\n\nLabel\n\nAsia\n\nEurope\n\nAfrica\n\nAmericas\n\nOceania\n\nMilitary facility, multi-unit residential, tunnel opening, wind farm, toll booth, road bridge, oil or gas facility, helipad, nuclear powerplant, police station, port\n\nSmokestack, barn, waste disposal, hospital, water treatment facility, amusement park, fire station, fountain, construction site, shipyard, solar farm, space facility\n\nPlace of worship, crop field, dam, tower, runway, airport, electric substation, flooded road, border checkpoint, prison, archaeological site, factory or powerplant, impoverished settlement, lake or pond\n\nRecreational facility, swimming pool, educational institution, stadium, golf course, office building, interchange, car dealership, railway bridge, storage tank, surface mine, zoo\n\nSingle-unit residential, parking lot or garage, race track, park, ground transportation station, shopping mall, airport terminal, airport hangar, lighthouse, gas station, aquaculture, burial site, debris or rubble\n\nD OTHER RELATED WORK\n\nExplicit, extreme distribution shift Refers to when the features that caused distribution shift is explicit, known, controllable, and in some cases, extreme (e.g. MNIST-CIFAR, CdSprites). This type of settings are popular in works that investigate simplicity bias (Shah et al., 2020), dataset bias (Torralba and Efros, 2011) and shortcut learning (Geirhos et al., 2020; Lapuschkin et al., 2019), as it allows for users to easily adjust the level of distribution shift between train and test set. Various specialised methods that either mitigate or address these problems under the supervised learning regime have been proposed, including Teney et al. (2022) that proposes to find shortcut solutions by ensembles, Luo et al. (2021) that avoids shortcut learning by extracting foreground objects for representation learning only, as well as Torralba and Efros (2011); Kim et al. (2019); Le Bras et al. (2020) that re-sample the dataset to reduce the spurious correlation.\n\n19\n\n0.00.51.0rid02040Linear head bias bSLSSLAE0.00.51.0rid0.02.55.07.510.0Linear head bias bSLSSLAE0.00.51.0rid020406080100Linear head bias bSLSSLAEPublished as a conference paper at ICLR 2023\n\nImportantly Kirichenko et al. (2022); Kang et al. (2020) shows that this extreme simplicity bias can be mitigated in some cases by retraining the final linear layer. This is a game changer, as it for the first time decouples the bias of the linear head from that of the main representation learning model. Interestingly, Robinson et al. (2021) also investigates the robustness of contrastive-SSL methods against simplicity bias. However, without training the linear head on OOD data, their finding is opposite to ours — that SSL methods are not able to avoid shortcut solutions.\n\nImplicit, subtle distribution shift This type of problem is commonly seen in realistic distribution shift datasets (such as WILDS) and are often studied in Domain generalisation (DG). In this regime, the training data are sampled from multiple domains, while data from a new, unseen target domain is used as test set. Note that here we omitted the discussion on domain adaptation (DA), as in DA models typically have access to unlabelled data from the target domain, which is different from the settings we consider in this work.\n\nThere are mainly two lines of work in DG, namely 1) Distributional Robustness approaches (DRO), which minimises the worst group accuracy to address covariate shift (Gretton et al., 2009a;b) and subpopulation shift (Sagawa et al., 2020; Hu et al., 2018); 2) Domain invariance, which consists of methods that directly learn representations that are invariant across domains (Ben-David et al., 2010; Ganin et al., 2016; Wang et al., 2019), encourage the alignment of gradients from different domains (Koyama and Yamaguchi, 2021; Lopez-Paz and Ranzato, 2017; Shi et al., 2022), or optimise for representations that result in the same optimal classifier for different domains (Arjovsky et al., 2019). Apart from these supervised learning methods, the recent advancement of SSL has also inspired works in unsupervised domain generalisation (Zhang et al., 2022; Harary et al., 2022). While all these methods achieved impressive performance, we note that they are all specially designed for DG with the majority of the methods relying on domain information and label information. In contrast, our work studies how existing standard representation learning methods such as SSL and AE performs on DG tasks, with none of the methods relying on human annotations.\n\n20",
    "reference": "# Summary Of The Paper\n\nThis paper provides a comprehensive empirical study of the OOD generalization performance of SSL, AE and SL. In addition to testing standard algorithms on standard distribution shift datasets, the paper proposes a new controllable realistic distribution shift task, and also consider a setting where the linear head is retrained on the OOD data. Results suggest that SSL and AE are more robust to distribution shift than SL.\n\n# Strength And Weaknesses\n\nStrength:\n- The experiments are very comprehensive and implementation details are provided.\n- The construction of the controllable distribution shift dataset is novel.\n- As far as I know, this is the first work that explicitly takes retraining the linear head into consideration, and compare its performance side by side with using the ID linear head. This could be an important point that is worth highlighting to the community.\n\nWeakness\n- There's limited explanation about why SSL/AE works better than SL on shifted distribution beyond what people already know in the literature.\n- To some extent, I feel there's nothing surprising about the findings in this paper. Although it's always nice to have more comprehensive experiments, it's unclear what's the conceptual insight that a reader can gain from this paper.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clear and the writing is of good quality.\n\n# Summary Of The Review\n\nI think this paper provides a reasonably good empirical comparison between SSL/AE and SL in various settings with different datasets. Although the result doesn't sound surprising, it's good to have this as an addition to the community, hence I would love to recommend for its acceptance.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSHARP CONVERGENCE ANALYSIS OF GRADIENT DESCENT FOR OVERPARAMETERIZED DEEP LINEAR NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThis paper presents sharp rates of convergence of the gradient descent (GD) method for overparameterized deep linear neural networks with different random initializations. This study touches upon one major open theoretical problem in machine learning–why deep neural networks trained with GD methods are efficient in many practical applications? While the solution of this problem is still beyond the reach of general nonlinear deep neural networks, extensive efforts have been invested in studying relevant questions for deep linear neural networks, and many interesting results have been reported to date. For example, recent results on loss landscape show that even though the loss function of deep linear neural networks is non-convex, every local minimizer is also a global minimizer. When the GD method is applied to train deep linear networks, it’s convergence behavior depends on the initialization. In this study, we obtained sharp rate of convergence of GD for deep linear networks and demonstrated that this rate does not depend on the types of random initialization. Furthermore, here, we show that the depth of the network does not affect the optimal rate of convergence, if the width of each hidden layer is appropriately large. Finally, we explain why the GD for an overparameterized deep linear network automatically avoids bad saddles.\n\n1\n\nINTRODUCTION\n\nDeep linear neural networks, as a class of toy models, are frequently used to understand loss surfaces and gradient-based optimization methods related to non-convex problems. Dauphin et al. (2014) and Choromanska et al. (2015a) explored the loss function of deep nonlinear networks based on random matrix theory (such as a spherical spin-glass model). This theory essentially converts the loss surface of deep nonlinear neural networks into that of deep linear neural networks under certain assumptions, some of which are unrealistic. Choromanska et al. (2015b) suggested an open problem to establish a connection between the loss function of neural networks and the Hamiltonian of spherical spin-glass models under milder assumptions. Later, Kawaguchi (2016) successfully discarded most of these assumptions by analyzing the loss surface of the deep linear neural networks.\n\nThe landscape for deep linear neural network (Kawaguchi, 2016; Kawaguchi & Lu, 2017; Laurent & Brecht, 2018) focuses on several properties of the critical points: (i) every local minimum is a global minimum; (ii) every critical point that is not a local minimum is a saddle point; and (iii) there exists a saddle such that all eigenvalues of its Hessian are zeros if the network is deeper than three layers. Thus, for deep linear neural networks, convergence to a global minimum is impeded by the existence of poor saddles.\n\nLee et al. (2016) showed that the gradient method almost surely never converges to a strict saddle point, although the time cost can depend exponentially on the dimension (Du et al., 2017). Gradient descent (GD) with perturbations (Ge et al., 2015; Jin et al., 2017) can find a local minimizer in polynomial time. Thus, the trajectory approach combined with random initialization or random algorithm circumvents the obstacle of existence of poor saddles. According to studies on continuous time dynamics of a gradient flow (Du et al., 2018; Arora et al., 2018b), the balance property of deep linear network is preserved if the initialization is balanced. Arora et al. (2018a;b), Du & Hu (2019), and Hu et al. (2020) successfully proved that GD with its corresponding initialization schemes con-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nverges to a global minimizer of deep linear neural networks with high probability. Furthermore, the rate of convergence is linear, and behaves like GD for a convex problem.\n\nHu et al. (2020) established that the convergence for Gaussian initialization can be very slow for deep linear neural networks with large depths, unless the width is almost linear. They also showed that orthogonal initialization in deep linear neural networks accelerate the convergence. Thus, the convergence behavior of the GD method, for training deep linear neural networks, crucially networks depends on the initialization.\n\nRecent studies have demonstrated the connection between deep learning and kernel methods (Daniely, 2017; Arora et al., 2019a;b; Chizat et al., 2019; Lee et al., 2019; Du et al., 2019; Cao & Gu, 2019; Woodworth et al., 2020), especially the neural tangent kernel (NTK), introduced by Jacot et al. (2018). For most common neural networks, the NTK becomes constant (Jacot et al., 2018; Liu et al., 2020) and remains so throughout the training in the limit of a large layer width. Throughout the training, the neural networks are well described by their first-order Taylor expansion around their parameters at the initialization (Lee et al., 2019).\n\nIn this paper, we first evaluate the convergence region, i.e. the set of initialization parameters that lead to the linear convergence of GD for deep linear neural networks (see Lemma 4.1 or Lemma D.1). Next, we demonstrate that if the minimum width among all the hidden layers is sufficiently large, then the random initialization will fall into the convergence region with high probability (see Theorem 3.1, Theorem B.1, Theorem B.2 and Theorem B.3). Furthermore, the worst-case convergence rate of GD for deep linear neural networks is almost the same as the original convex problem with a corresponding learning rate. We also demonstrate that the GD trajectories for deep linear neural networks are arbitrarily close to those for the convex problem. The precise statement is related to remark 3, Theorem 3.2, Corollary 1 and Lemma 4.4 (also see Lemma D.5).\n\nThe present study was inspired by a recent reported work Du & Hu (2019); Hu et al. (2020), in which the authors carefully constructed the upper and lower bounds of the eigenvalues of the Gram matrix along the GD and established a linear convergence. In this paper, we generalize their results to strongly convex loss functions with layer varying widths and obtain sharper results. We also show that our rate of convergence for GD in deep linear neural networks is sharp in the sense that it matches the worst-case convergence rate for the original convex problem. The trajectories between the GD for deep linear neural networks and the original convex problem (1) can be arbitrary close. Furthermore, we show that if the width of each hidden layer is appropriately large, then the optimal rate does not depend on the random initialization types and network depth. Lastly, we elucidate the mechanism underlying the observed automatic avoidance of bad saddles by the GD for overparameterized deep linear networks.\n\n2 PRELIMINARIES\n\n2.1 PROBLEM SETUP\n\nLet x ∈ Rnx and y ∈ Rny be an input vector and a target vector, respectively. Define {(xi, yi)}m i=1 as a training dataset of size m, and let X = [x1, x2, · · · , xm] ̸= 0 and Y = [y1, y2, · · · , ym]. Denote the weight parameters by W ∈ Rny×nx .\n\nConsider the well-studied convex optimization problem:\n\nminimize W\n\nL(W ) :=\n\n1 m\n\nm (cid:88)\n\ni=1\n\nl(W xi, yi).\n\nThe GD for convex problem (1) with a learning rate of η∗ is given by:\n\nW (t + 1) = W (t) − η∗∇L(W (t)), t = 0, 1, 2, · · · .\n\n(1)\n\n(2)\n\nFor any matrix A, let σmax(A) and σmin(A) be the largest and smallest singular values of A respectively. Here, we consider two types of matrix norms and one type of semi-norm for A, ∥A∥ := σmax(A), ∥A∥2 F := tr(AAT ), and ∥A∥X := ∥APX ∥F , where PX = X(X T X)†X T is the orthogonal projection matrix onto the column space of X, and (X T X)† is the Moore–Penrose inverse.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFor two real matrices A, B with the same sizes, we consider their Frobenius inner product as well as their semi-inner product, ⟨A, B⟩ = ⟨A, B⟩F := tr(AT B), ⟨A, B⟩X := ⟨APX , BPX ⟩. Here, we list some basic properties for the semi-norm and semi-inner product. Lemma 2.1. The loss function L(W ) defined in (1) satisfies the following properties: for any W, V ∈ Rny×nx ,\n\n1.L(W ) = L(W PX ), 2.∇L(W ) = ∇L(W PX )PX , 3.⟨∇L(W ), V ⟩F = ⟨∇L(W ), V ⟩X , 4. ∥∇L(W )∥F = ∥∇L(W )∥X , 5. ∥W ∥X ≡ ∥W ∥F if and only if X is full row rank.\n\nThe next lemma demonstrates the importance of the semi-norm ∥·∥X in our analysis. Lemma 2.2. Assume that l(·, y) is α(l)−strongly convex. Then, the following statements hold.\n\n1. If X is not a full row rank matrix, then L(W ) is neither strictly convex nor strongly convex\n\nwith respect to ∥·∥F .\n\n2. L(W ) is α(l)λmin(XX T )\n\nm\n\n−strongly convex with respect to ∥·∥X , where λmin(XX T ) is the\n\nsmallest non-zero eigenvalue of XX T .\n\nThe proofs of the two aforementioned lemmas are provided in appendix A. Hereafter, if inner product is not specified, then we will consider the semi-inner product.\n\nAssume that L is α−strongly convex (α > 0), and ∇L is β−Lipschitz (with respect to the seminorm ∥·∥X ); that means, for any W, V ∈ Rny×nx ,\n\nL(W ) ≥ L(V ) + ⟨∇L(V ), W − V ⟩X +\n\nα 2\n\n∥W − V ∥2\n\nX ,\n\n∥∇L(W ) − ∇L(V )∥X = ∥∇L(W ) − ∇L(V )∥F ≤ β ∥W − V ∥X . Without loss of generality, we assume that α and β are the best constants. Then, Lemma 2.2 implies that α ≥ α(l)λmin(XX T ) β(l)−Lipschitz and λmax(XX T ) is the largest eigenvalue of XX T .\n\n. Similarly, we can also show that β ≤ β(l)λmax(XX T )\n\n, where ∇l(·, y) is\n\nm\n\nm\n\nDefine the effective condition number of the convex function L by κ = κ(L) = β α < ∞. κ appears naturally in the rate of convergence of the GD. Let W∗ be a global minimizer of L(W ), that is L(W∗) = minW L(W ). Notice that W∗ might not be unique, but W∗PX is unique.\n\nThe well-known results for the rate of convergence of GD (2) state are:\n\nη∗ =\n\n1 β\n\n(cid:18)\n\n=⇒ E(t) ≤\n\n1 −\n\n(cid:19)t\n\n1 κ\n\nE(0), t = 1, 2, · · · , as well as,\n\nη∗ =\n\n2 α + β\n\n=⇒ E(t) ≤\n\n(cid:18)\n\n1 −\n\nβ 2\n\n4κ (1 + κ)2\n\n(cid:19)t\n\n∥W (0) − W∗∥2\n\nX , t = 1, 2, · · · ,\n\n(3)\n\n(4)\n\nwhere E(t) = L(W (t)) − L(W∗).\n\n2.2 DEEP LINEAR NETWORK SETUP\n\nLet N − 1 be the number of hidden layers. Assume rank(X) = r. Denote the weight parameters by Wk ∈ Rnk×nk−1 , k = 1, 2 · · · , N , with nN = ny, n0 = nx, where the nk is the width of the k-th layer. Set nmin = min{n1, n2, · · · , nN −1}, and nmax = max{n1, n2, · · · , nN −1}. For notational convenience, we denote nj:i = (cid:81) i≤k≤j nk and denote Wj:i = WjWj−1 · · · Wi for each 1 ≤ i ≤ j ≤ N . Define ni−1:i = 1 and Wi−1:i = I (of appropriate dimension) for completeness.\n\nConsidering the implicit regularization W = WN :1 for the convex problem (1). We obtain the following non-convex optimization problem of deep linear neural networks:\n\nminimize W1,··· ,WN\n\nL(WN :1) =\n\n1 m\n\nm (cid:88)\n\ni=1\n\nl(WN :1xi, yi).\n\n(5)\n\nExample 2.1. Specifically, if we set the loss to be l(W xi, yi) = ∥W xi − yi∥2 m ∥W X − Y ∥2\n\n−strongly convex, and ∇L is 2λmax(XX T )\n\nF is 2λmin(XX T )\n\nm\n\nm\n\n1\n\n−Lipschitz.\n\n2, then L(W ) =\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nExample 2.2. Deep linear neural networks with regularization λ ∥WN · · · W1PX ∥2 verted into a new optimization problem\n\nF can be con-\n\nminimize W1,··· ,WN\n\nL(WN :1) + λ ∥WN :1∥2\n\nX .\n\nLet Lλ(W ) = L(W ) + λ ∥W ∥2 Lipschitz.\n\nX . Then, Lλ(·) is α + 2λ-strongly convex, and ∇Lλ(·) is β + 2λ-\n\nMore generally, if we consider regularization with a form R(W ) = λ · g(W PX ), and g(·) is α′- strongly convex, and β′-Lipschitz, then for the optimization problem\n\nminimize W1,··· ,WN\n\nL(WN :1) + R(WN · · · W1) =: LR(WN · · · W1),\n\nwe know that LR(·) is α + λα′-strongly convex, and ∇LR(·) is β + λβ′-Lipschitz.\n\n2.3\n\nINITIALIZATION SCHEMES\n\nIn previous studies, the following form of deep linear networks was considered, instead of (5):\n\nminimize W1,··· ,WN\n\nL(aN WN :1) =\n\n1 m\n\nm (cid:88)\n\ni=1\n\nn1n2 · · · nN is a normalization constant.\n\nwhere aN = 1/\n\n√\n\nl(aN WN :1xi, yi),\n\n(6)\n\nBy applying GD on (6), where we update Wj simultaneously for j, we obtain\n\nWj(t + 1) = Wj(t) − η · aN (WN :j+1(t))T ∇L (aN WN :1(t)) (Wj−1:1(t))T , j = 1, · · · , N. (7)\n\nIn a recent study, the authors considered GD (7) and adopted a Gaussian initialization (Du & Hu, 2019) or scaled orthogonal initialization (Hu et al., 2020) for initializing Wj(0).\n\nIn this paper, we consider the following three kinds of random initializations, which generalize their idea.\n\nGaussian initialization: Let W1(0), · · · , WN (0) be the weight matrices at initialization. We assume that all the entries of Wj, 1 ≤ j ≤ N are independent Gaussian random variables with a zero mean and unit variance. Then, aN is a normalization constant in the sense that for any x ∈ Rn0 , we have\n\nE\n\n(cid:104) ∥aN WN :1(0)x∥2\n\n2\n\n(cid:105)\n\n= ∥x∥2 2 .\n\n(8)\n\nIn fact, all the initializations discussed in this paper satisfy (8). Remark 1. Let Vi(t) = 1√ ni initialization is equivalent to\n\nWi(t), for 1 ≤ i ≤ N . Then, GD (7) with a unit variance Gaussian\n\nVj(t + 1) = Vj(t) −\n\nη nj\n\n(VN :j+1(t))T ∇L (VN :1(t)) (Vj−1:1(t))T ,\n\n(9)\n\nwith a zero mean and variance 1 ni\n\nGaussian initialization for Vi, i = 1, · · · , N .\n\nGD (9) for loss (5) is equivalent to GD (7) for loss (6). Hereafter, we will only consider GD (7) for deep linear neural network (6).\n\nOrthogonal initialization: We consider the so-called one peak random orthogonal projection and embedding initialization, which generalize the idea of orthogonal initialization (Hu et al., 2020). Definition 2.1. An initialization WN :1(0) = WN (0)WN −1(0) · · · W1(0) is said to be a one peak random orthogonal projection and embedding initialization if there exists 1 ≤ p < N , such that n0 ≤ n1 ≤ n2 ≤ · · · ≤ np, np ≥ np+1 ≥ np+2 ≥ · · · nN −1 ≥ nN , and W1(0), W2(0), · · · , Wp(0), Wp+1(0), Wp+2(0), · · · , WnN (0) are independent and uniformly distributed over rectangular matrices, which satisfy\n\n(cid:26)W T\n\ni (0)Wi(0) = niIni−1, 1 ≤ i ≤ p,\n\nWj(0)W T\n\nj (0) = nj−1Inj , p + 1 ≤ j ≤ N.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nRemark 2. In this definition,\n\nWj(0), p + 1 ≤ j ≤ N are random orthogonal projections. Notably, A is a random orthogonal projection if and only if AT is a random embedding.\n\nWi(0), 1 ≤ i ≤ p are random embeddings and\n\nnj−1\n\n(cid:113) 1\n\n(cid:113) 1 ni\n\nArora et al. (2018a) studied the rate of convergence of GD to a global optimum for training a deep linear neural network for a balanced initialization. Here, we will consider a special case of balanced initialization, which is described as follows:\n\n√\n\nnUN [Iny , 0ny×(n−ny)]V T\n\nSpecial balanced initialization: Assume n1 = · · · = nN −1 = n. Consider the initialization WN (0) = 1 and Wi(0) = √\nnUiInV T , 2 ≤ i ≤ N − 1, where UN −1, UN , V1, Vi = Ui−1, 2 ≤ i ≤ N − 1 are orthogonal matrices (random or deterministic), and VN has a uniform distribution over the orthogonal matrices. Notice that only VN is required to be random.\n\nnU1[Inx , 0nx×(n−nx)]T V T\n\nN , W1(0) =\n\n√\n\ni\n\nA simple estimation of the loss at the initialization is given by the following lemma. Lemma 2.3. If the initialization satisfies (8) for all x, then with probability at least 1 − δ\n\n2 , we have\n\nL(aN WN :1(0)) − L(W∗) ≤ βBδ, where Bδ =\n\n(cid:18) 2 · rank(X) δ\n\n+ ∥W∗∥2\n\nX\n\n(cid:19)\n\n.\n\nNote that the bound Bδ can be improved by using a sharp concentration inequality.\n\n3 MAIN THEOREMS\n\nAssume the thinnest layer is either the input layer or the output layer; that is nmin ≥ max{n0, nN }, and the ratio between the width of any hidden layer is bounded from above, precisely we have nmax ≤ C0 < ∞. The quantities C2, C5 and C6 are defined in appendix D and are dependent on nmin hyperparameters nN , κ, δ, rank(X), C0, and N .\n\nFor notational convenience, we denote\n\nE(t) = L(W (t)) − L(W∗), and EDLN (t) = L(aN WN :1(t)) − L(W∗).\n\nOur assumptions and notation are now in place. We next state our main theorems in this section.\n\n3.1 LINEAR CONVERGENCE OF DEEP LINEAR NEURAL NETWORKS\n\nIn appendix B we present a sharp estimate of the linear convergence of GD for deep linear neural networks in Theorem B.1 for Gaussian initialization, Theorem B.2 for orthogonal initialization, and Theorem B.3 for a special balanced initialization. In particular, with a specific learning rate η = nN βN , Theorem B.1 and Theorem B.2 yield the following optimum rate of convergence: Theorem 3.1. Given any δ, ε ∈ (0, 1 following two overparameterization condition holds:\n\n2 ), there exists a constant C := C(ε), such that if one of the\n\n1. nmin ≥ C · C2 · N with the Gaussian initialization,\n\n2. nmin ≥ C · C5 with the one peak random orthogonal projection and embedding initialization\n\nand with probability at least 1 − δ, then we have\n\n(cid:18)\n\nEDLN (t) ≤\n\n1 −\n\n(cid:19)t\n\n1 − ε κ\n\nEDLN (0), t = 1, 2, · · · .\n\nRemark 3. Consider GD (2) with a learning rate of η∗ = 1 β and initialization W (0) = aN WN :1(0). The well-known result of rate of convergence (3) for GD (2) of convex problem (1) matches the rates obtained from Theorem B.1 and Theorem B.2. Remark 4. Du & Hu (2019), and Hu et al. (2020) showed that the number of iterations required (cid:1) for l2 loss. We only improved the rate of convergence and to reach a precision ε is O (cid:0)κ log 1 generalized their results to any strongly convex loss.\n\nε\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n3.2 RESULTS OF TRAJECTORIES\n\nTheorem 3.1 and remark 3 establish that the rate of convergence to a global optimum for GD to train a deep linear neural network is almost the same as the trajectories for the GD to train the corresponding convex problem with high probability, if the width is sufficiently large. Moreover, the GD for the fully-connected deep linear neural network (7) and that for GD (2) have almost the same trajectories.\n\nLet η1 = 2nN βN be an upper bound of the learning rate η. We can show that the trajectories of GD (7) for deep linear neural network (6) with a learning rate of η < η1 are close to those of GD (2) with a learning rate of η∗ = N η for the corresponding convex problem (1) with high probability, if the nN width of each hidden layer is sufficiently large. The precise statement is as follows: Theorem 3.2. Consider the GD for deep linear neural network (7) with a learning rate of η < η1 for aN WN :1(t), t = 0, 1, · · · , and GD (2) with a learning rate of η∗ = N η for W (t), t = 0, 1, · · · . nN Given τ, δ ∈ (0, 1), there exists a constant C := C(τ, η/η1) such that if one of the following three overparameterization conditions holds:\n\n1. nmin ≥ C · C2 · N with the Gaussian initialization,\n\n2. nmin ≥ C · C5 with the one peak random orthogonal projection and embedding initialization,\n\n3. nmin ≥ C · C6 with the special balanced initialization,\n\nthen with probability at least 1 − δ, we obtain\n\nX ≤ D(τ, q, t) ∥aN WN :1(0) − W∗∥2 ∥aN WN :1(t) − W (t)∥2 (cid:18)\n\n(cid:19)\n\nX ,\n\n|EDLN (t) − E(t)| ≤ β\n\nqt/2(cid:112)D(τ, q, t) +\n\n1 2\nEDLN (t) ≤ 3β(q + τ )t ∥aN WN :1(0) − W∗∥2 1−q , 2(q + τ )t(cid:111) (cid:110) τ\n\nX ,\n\nwhere D(τ, q, t) = min\n\n, with 0 < q < 1 defined in (15).\n\nD(τ, q, t)\n\n∥aN WN :1(0) − W∗∥2\n\nX , (10b)\n\n(10a)\n\n(10c)\n\nRemark 5. To the best of knowledge, this is the first paper that reveals that the trajectory of the overparameterized deep linear neural networks is close to the original convex problem with an appropriately rescaled learning rate. Corollary 1. According to Theorem 3.2, if we set η = 2nN high probability,\n\n(α+β)N , the following inequality holds with\n\n(cid:18)\n\nEDLN (t) ≤ 3β\n\n1 −\n\n(cid:19)t\n\n4κ\n\n(1 + κ)2 + τ\n\n∥aN WN :1(0) − W∗∥2\n\nX .\n\n(11)\n\nNotably, the rate of convergence in (11) is better than that in Theorem 3.1, because if κ > 1, then we can choose a sufficiently small τ such that the following inequality holds:\n\n1 −\n\n4κ\n\n(1 + κ)2 + τ < 1 −\n\n1 κ\n\n.\n\nTheorem 3.1, Theorem 3.2, Theorem B.1, and Theorem B.2 indicate that the implicit regularization induced by the GD for a convex problem recovers the convex problem itself in terms of optimization, at the cost of linear convergence only with high probability for random initialization. Remark 6. Recall the constants C2, C5, and C6 defined in appendix B. The term rank(X) is not optimal, since our concentration inequality depends only on the second moment. By using stronger concentration inequalities for our Lemma 2.3, similar to the proof of proposition 6.5 (Du & Hu, 2019) and Lemma 4.2 (Hu et al., 2020), the rank(X) ). C2 is δ\nproportional to κ2, which is slightly better than the constant in Du & Hu (2019), which is proportional to κ3. C5 is also slightly better than the constant reported by Hu et al. (2020), since we do not have the extra term ∥X∥2 ∥X∥2 . The improvement of the constant is mainly due to the introduction of the semi-norm ∥·∥X .\n\ncan be improved to 1 + log( rank(X)\n\nδ\n\nδ\n\nF\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4\n\nINSIGHTS FOR THEOREM 3.2\n\nInitialization and convergence region: Arora et al. (2018a) showed that if the initialization is approximately balanced, and the product matrix WN :1(0) is very close to a global minimizer, then the GD linearly converges to the global minimum for the deep linear network without any width requirement. However, the convergence region in (Arora et al., 2018a) is very small, because WN :1(0) needs to be very close to W∗. Later, Du & Hu (2019), and Hu et al. (2020) successfully proved that the GD with a Gaussian, or orthogonal initialization linearly converges to a global minimizer of the overparameterized deep linear neural network with high probability. They introduced a technique to analyze the trajectories of GD with large widths for any deterministic initialization.\n\nWe introduce the following lemma, which describes the linear convergence result for a deep linear network with a deterministic initialization.\n\nLemma 4.1. Under the setting of Lemma D.1, the GD for a deep linear network satisfies\n\nEDLN (t) ≤ (1 − ηγ)t EDLN (0), t = 1, 2, · · · .\n\nOur convergence region (see (31) in Lemma D.1 and Definition D.1) originates from the analysis of Du & Hu (2019), and Hu et al. (2020) and can be view as a neighborhood of the special balanced initialization, if n1 = n2 = · · · = nN −1. Both Gaussian and orthogonal initialization are approximately balanced.\n\nFor l2 loss, without loss of generality, we can assume X to be a full rank matrix and L(W∗) = 0 because of the decomposition method in claim B.1 from Du & Hu (2019). However, when considering a general strongly convex loss, we have to confront the low rank X directly in our analysis. Thus, ∥·∥X appears naturally and aids in achieving the sharp rate of convergence in our main theorems. In addition to the technique reported in Du & Hu (2019), and Hu et al. (2020), we also used classical convex optimization techniques (such as inequalities in Lemma C.1, and Polyak-Łojasiewicz inequality in (26)) as well as the classical concentration inequalities for beta distribution (such as the Chernoff type bound in Lemma F.3).\n\nWhy GD trajectories for overparameterized deep linear neural networks with approximate balanced initialization are close to those for convex problems? The underlying mechanism can be understood as follows: Even though recent results of (Ziyin et al., 2022) can describe the exact global minimizer for a deep linear network (with a regularization term such as l2), the evolution of each Wj is still difficult to track. Instead, we consider the discrete dynamics for product matrices (see (41) and (42)):\n\naN WN :1(t + 1) = aN WN :1(t) − η · P (t)[∇L(aN WN :1(t)PX )] + aN E(t).\n\nFor their own linear operator Pt, Du & Hu (2019) showed that λmax(Pt) ≤ O( N nN and λmin(Pt) ≥ Ω( N nN to proved that for our operator P (t)[·] ≈ N nN is negligible, which leads to the following result on discrete dynamics (see Lemma D.3).\n\n) · λmax(X T X) ) · λmin(X T X). To the best of our knowledge, the present paper is the first I (also see (44)), where I is the identity operator. E(t)\n\nLemma 4.2. Under the setting of Lemma D.3, we have\n\naN WN :1(t + 1) = aN WN :1(t) −\n\nN nN\n\nη∇L(aN WN :1(t)) + R(t),\n\nwith ∥R(t)∥X ≤ τ ∥aN WN :1(t) − W∗∥X .\n\nWithout the R(t) term, the discrete dynamics is exactly the GD for a convex function. To control the distance between the two trajectories, we introduce the following lemma (also see Lemma D.4).\n\nLemma 4.3. Assume τ ∈ [0, 1), and consider a discrete dynamical system V (t) such that,\n\nV (t + 1) = V (t) − η∗∇L(V (t)) + R(t), where ∥R(t)∥X ≤ τ ∥V (t) − W∗∥X .\n\nIf η∗ ≤ 2/β, then we have ∥V (t) − W∗∥2\n\nX ≤ (q + 7τ )t ∥V (0) − W∗∥2\n\nX , where q is defined in (15).\n\nWith the help of this lemma, we further obtain the following trajectories comparison lemma (also see Lemma D.5), which leads to the main conclusions in Theorem 3.2.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 4.4. Under the setting of Lemma D.5, we have\n\nX ≤ D(τ, q, t) ∥aN WN :1(0) − W∗∥2 ∥aN WN :1(t) − W (t)∥2 (cid:18)\n\n(cid:19)\n\nX ,\n\n|EDLN (t) − E(t)| ≤ β\n\nqt/2(cid:112)D(τ, q, t) +\n\n1 2\nEDLN (t) ≤ 3β(q + τ )t ∥aN WN :1(0) − W∗∥2 1−q , 2(q + τ )t(cid:111) (cid:110) τ\n\nX ,\n\nwhere D(τ, q, t) = min\n\n, with 0 < q < 1 defined in (15).\n\nD(τ, q, t)\n\n∥aN WN :1(0) − W∗∥2\n\nX , (12b)\n\n(12a)\n\n(12c)\n\nWhy do bad saddles not affect GD for overparameterized deep linear neural networks? A critical point x∗ of f is a bad saddle if λmin(∇2f (x∗)) = 0. Kawaguchi (2016) showed that deep linear networks have bad saddles, and thus, in general, a vanishing Hessian can hinder the optimization. Theorem 2.3 in Kawaguchi (2016) explains that for all bad saddles satisfy that WN −1:2 is a non-full rank matrix. Thus, to show that the trajectories of GD are away from bad saddle points, it is sufficient to demonstrate that inf t σmin(WN −1:2(t)) > 0. According to previous studies, there are two main ways to avoid bad saddles for GD to train deep linear networks.\n\nOn the one hand, following Arora et al. (2018b), it can be showed that if the approximate balanced initialization satisfies ∥WN :1(0) − W∗∥F ≤ σmin(W∗) − c, for some 0 < c < σmin(W∗), then σmin(WN :1(t)) ≥ c through the training as well as ∥W1(t)∥ ≤ (4 ∥W∗∥F )1/N , and ∥WN (t)∥ ≤ (4 ∥W∗∥F )1/N , then σmin(WN −1:2(t)) ≥ σmin(WN :1(t))\n\n∥W1(t)∥∥WN (t)∥ ≥\n\n(4∥W∗∥)2/N .\n\nc\n\nOn the other hand, if we assume that our rescaled and overparameterized weight initialization falls into convergence region (31), then we can show that (see B(t) in the proof of Lemma D.1)\n\nσmin(WN −1:2(t)) ≥ max\n\n(cid:26) σmin(WN :2(t)) σmax(WN (t))\n\n,\n\nσmin(WN −1:1(t)) σmax(W1(t))\n\n(cid:27)\n\n.\n\nThus, σmin( WN −1:2\n\n(nN −1:2)1/2 ) ≥ e−c1−c2 max{ n1\n\nnN −1\n\n, nN −1 n1\n\n} ≥ e−c1−c2 > 0.\n\nIn conclusion, we first made a conjecture that according to Arora et al. (2018b), for a nonoverparameterized deep linear network, there are no bad saddles satisfying ∥WN :1(0) − W∗∥F < σmin(W∗). Thus, ∥WN :1(0) − W∗∥F < σmin(W∗) is indeed a convergence region. However, this region in general is very small, and can even be empty if σmin(W∗) = 0. For an overparameterized deep linear network, the GD initialized in the convergence region will force the trajectories away from all the bad saddles.\n\nWhy does the width have to be large? We will discuss overparameterization phenomena in deep linear networks. For simplicity, we consider a special balanced initialization. First, we know that ∥Wi(t) − Wi(0)∥F = O( 1 βN and γ = O( αN nN\n\nN ) (see C(t) through the proof of Lemma D.1), provided η = h nN\n\n), where h ∈ (0, 2).\n\nAn overparameterized deep linear network around the special balanced initialization is full of global 2 , · · · , W ∗ minimizers, i.e., the trajectory limit (W ∗ N ) neighborhood of the special balanced initialization (W1(0), W2(0), · · · , WN (0)). Notably,\n\nN ) is in the O( 1\n\n1 , W ∗\n\nσmin(WN −1:2(0)) =\n\nN −1 (cid:89)\n\nj=2\n\nσmin(Wj(0)) = σmax(WN −1:2(0)) =\n\nN −1 (cid:89)\n\nj=2\n\nσmax(Wj(0)) = n(N −2)/2,\n\nas well as for any (W1, W2, · · · , WN ) in the O( 1 N ) neighborhood of any given initialization (W1(0), W2(0), · · · , WN (0)), we have (detailed argument can be found in the proof of B(t) in Lemma D.1):\n\n∥WN −1:2 − WN −1:2(0)∥F ≤\n\nN −2 (cid:88)\n\ns=1\n\n(cid:19)\n\n(cid:18)N − 1 s\n\nO(\n\n1 N\n\n)s(n(N −2−s)/2) ≤ n(N −2)/2O\n\n(cid:18) 1 √\nn\n\n(cid:19)\n\n.\n\nThus in terms of landscape, we have\n\nσmin(WN −1:2) σmin(WN −1:2(0))\n\n≥\n\nσmin(WN −1:2(0)) − ∥WN −1:2 − WN −1:2(0)∥F σmin(WN −1:2(0))\n\n≥ 1 − O(\n\n1 √\nn\n\n),\n\n(13)\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nwhich implies that no bad saddle is present in the O( 1 initialization, if the width n is sufficiently large.\n\nN ) neighborhood of the special balanced\n\nIn terms of training, we have (see the proof of Lemma D.1),\n\n∥Wi(t) − Wi(0)∥F = O(\n\n1 N\n\n), ∥Wi(0)∥F = n, 2 ≤ i ≤ N − 1,\n\n∥Wi(0)∥F\n\nas well as ∥Wi(t)−Wi(0)∥F N n ). Thus for an overparameterized deep linear network, the GD with an approximate balanced initialization only trains W1 and WN , and the other weight matrices remain almost constant. Here, we provide empirical evidence in appendix G to support the aforementioned argument.\n\n= O( 1\n\nOn the other hand, the sharp rate of convergence depends on the trajectory limit, and when the minimum width is sufficiently large, the trajectory limit and the initialization are not far away from each other. For deep linear network with small widths, the result (Ziyin et al., 2022) might shed light on convergence analysis, because the exact global minimizer can be described for a deep linear network with L2 regularization.\n\nNumerical Experiments: In appendix H, we will discuss some empirical evidence to support the main results shown in Section 3. Further, Figure 1 and 2 in appendix H show plots of the logarithm of loss as a function of number of iterations. When n is small, the trajectories of loss for deep linear neural networks do not decrease in some iterations. However, when n is large, the loss trajectories are close to those for the corresponding convex problem.\n\n5 OVERVIEW OF THE PROOFS OF MAIN THEOREMS AND LEMMAS\n\nIn this section, we provide an overview of the proofs for all the theorems obtained in the main results. Since Theorem 3.1 in the main results is a special cases of general theorems with nonoptimal learning rates (see Theorem B.1 and Theorem B.2), we only need to focus on the proofs of the general theorems (see Theorem B.1, Theorem B.2, Theorem B.3, and Theorem 3.2).\n\nWe begin with the convergence region of deep linear neural networks, which is basically the set of initializations that lead to the convergence of the GD for deep linear neural networks. The precise definition can be found in appendix D. Lemma 4.1 and Lemma 4.4 (also see Lemma D.1 and Lemma D.5) prove that this convergence region satisfies the following properties: if the initialization falls into the convergence region, then\n\n(i) the GD is guaranteed to converge to a global minimizer of the deep linear neural networks,\n\n(ii) the worst-case GD rate of convergence for the deep linear neural networks, which is a non-convex problem, is almost the same as the corresponding convex problem with a corresponding learning rate, and,\n\n(iii) the trajectories of the GD for the deep linear neural networks are arbitrarily close to those\n\nfor the corresponding convex problem.\n\nMore precisely, Lemma 4.1 (also see Lemma D.1) establishes the convergence region for a deterministic initialization, and it demonstrates the first two properties, (i) and (ii). Additionally, in appendix E and appendix F we also prove that the spectral properties of the products of random matrices partially reveal that the overparameterization realized by adding the width of each hidden layer guarantees that the random initialization falls into the convergence region with high probability. These results provide a foundation to establish the main linear convergence theorem for random initialization (see Theorem B.1, B.2, and B.3).\n\nBy contrast, Lemma 4.2 (also see Lemma D.3) shows that if the initialization falls into the convergence region, then the update rule for the product of weight matrices in the GD for deep linear neural networks is more or less given by (2). This result can be used to establish both Lemma 4.4 (also see Lemma D.5), and Theorem 3.2, which is precisely property (iii) of the convergence region for deterministic and non-deterministic initializations, respectively.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient\n\ndescent for deep linear neural networks. CoRR, abs/1810.02281, 2018a.\n\nSanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit In International Conference on Machine Learning, pp.\n\nacceleration by overparameterization. 244–253. PMLR, 2018b.\n\nSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pp. 322–332. PMLR, 2019a.\n\nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. Advances in Neural Information Processing Systems, 32, 2019b.\n\nYuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and\n\ndeep neural networks. Advances in neural information processing systems, 32, 2019.\n\nDjalil Chafaı, Djalil Chaf ̈a, Olivier Gu ́edon, Guillaume Lecue, and Alain Pajor. Singular values of\n\nrandom matrices. Lecture Notes, 2009.\n\nL ́ena ̈ıc Chizat, Edouard Oyallon, and Francis R. Bach. On lazy training in differentiable program-\n\nming. In NeurIPS, 2019.\n\nAnna Choromanska, Mikael Henaff, Michael Mathieu, G ́erard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial intelligence and statistics, pp. 192–204. PMLR, 2015a.\n\nAnna Choromanska, Yann LeCun, and G ́erard Ben Arous. Open problem: The landscape of the loss surfaces of multilayer networks. In Conference on Learning Theory, pp. 1756–1760. PMLR, 2015b.\n\nAmit Daniely. Sgd learns the conjugate kernel class of the network. Advances in Neural Information\n\nProcessing Systems, 30, 2017.\n\nYann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. Advances in neural information processing systems, 27, 2014.\n\nSimon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In\n\nInternational Conference on Machine Learning, pp. 1655–1664. PMLR, 2019.\n\nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pp. 1675– 1685. PMLR, 2019.\n\nSimon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. Advances in neural information processing systems, 30, 2017.\n\nSimon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. Advances in Neural Information Processing Systems, 31, 2018.\n\nMorris L. Eaton. Group invariance applications in statistics. Regional Conference Series in Proba-\n\nbility and Statistics, 1:i–133, 1989. ISSN 19355912.\n\nSam Elder. Bayesian adaptive data analysis guarantees from subgaussianity. CoRR, abs/1611.00065,\n\n2016.\n\nRong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient for tensor decomposition. In Conference on learning theory, pp. 797–842. PMLR, 2015.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Learning Representations, 2020.\n\nArthur Jacot, Franck Gabriel, and Cl ́ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\nGJO Jameson. Inequalities for gamma function ratios. The American Mathematical Monthly, 120\n\n(10):936–940, 2013.\n\nChi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape In International Conference on Machine Learning, pp. 1724–1732.\n\nsaddle points efficiently. PMLR, 2017.\n\nKenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information\n\nProcessing Systems, pp. 586–594, 2016.\n\nKenji Kawaguchi and Haihao Lu. Deep creates no bad local minima. arXiv: 1702.08580, 2017.\n\nThomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are\n\nglobal. International Conference on Machine Learning, pp. 2908–2913, 2018.\n\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. Advances in neural information processing systems, 32, 2019.\n\nJason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only\n\nconverges to minimizers. In Conference on learning theory, pp. 1246–1257. PMLR, 2016.\n\nChaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33: 15954–15964, 2020.\n\nOlivier Marchal and Julyan Arbel. On the sub-gaussianity of the beta and dirichlet distributions.\n\nElectronic Communications in Probability, 22(none):1 – 14, 2017.\n\nFeng Qi, Bai-Ni Guo, and C. Chen. The best bounds in gautschi-kershaw inequalities. Mathematical\n\nInequalities Applications, pp. 427–436, 2006.\n\nBlake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, In Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. Conference on Learning Theory, pp. 3635–3673. PMLR, 2020.\n\nLiu Ziyin, Botao Li, and Xiangming Meng. Exact solutions of a deep linear network. arXiv preprint\n\narXiv:2202.04777, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS OF BASIC PROPERTIES OF THE SEMI-NORM\n\nProof of Lemma 2.1. The first property is a direct consequence of the definition of the projection matrix PX .\n\nNotice that\n\n1 ε\n\n(L(W + ε∆W ) − L(W )) =\n\n1 ε\n\n(L(W PX + ε∆W PX ) − L(W PX )).\n\nConsidering ε → 0, the definition of the directional derivative implies\n\n⟨∇L(W ), ∆W ⟩F = ⟨∇L(W PX ), ∆W PX ⟩F = ⟨∇L(W PX )PX , ∆W ⟩F , ∀∆W ∈ Rny×nx,\n\nsince PX = P T\n\nX . This completes the proof of the second property.\n\nThe third property is derived from the condition: orthogonal projection matrix satisfies PX = P T P 2\n\nX = P 3\n\nX , since\n\nX =\n\n⟨∇L(W ), V ⟩F = ⟨∇L(W PX )PX , V ⟩F\n\n=⟨∇L(W PX )P 2\n\nX , V PX ⟩F = ⟨∇L(W PX )PX , V ⟩X = ⟨∇L(W ), V ⟩X .\n\nIf we set V = ∇L(W ), then the fourth property is implied by the third property.\n\nFor the last property, first recall that ∥W ∥X = ∥W PX ∥F and PX = X(X T X)†X T . X is of a full row rank matrix if and only if PX is an identity matrix, which completes the proof.\n\nProof of Lemma 2.2. Because X is not full row rank, we know that I − PX ̸= 0. There exists W such that W (I − PX ) ̸= 0. Applying the first property in Lemma 2.1, we obtain\n\nL(\n\n1 2\n\nW +\n\n1 2\n\nW PX ) = L((\n\n1 2\n\nW +\n\n1 2\n\nW PX )PX ) = L(W PX ) =\n\n1 2\n\nL(W ) +\n\n1 2\n\nL(W PX ),\n\nprovided W ̸= W PX .\n\nHence, L is not strictly convex, which implies that L is not strongly convex.\n\nTo prove the second property, it is sufficient to show that g(W ) = L(W ) − α(l)λmin(XX T ) is convex. Obviously,\n\nm\n\n∥W ∥2\n\nX\n\ng(W ) = L(W ) −\n\nα(l) m\n\nm (cid:88)\n\ni=1\n\n∥W xi − yi∥2\n\n2 +\n\nα(l) m\n\n(∥W X − Y ∥2\n\nF − λmin(X T X) ∥W ∥2\n\nX ).\n\n(14)\n\n(cid:80)m\n\ni=1 ∥W xi, yi∥2\n\nL(W ) − α(l) ∥W X − Y ∥2 F − λmin(W T W ) ∥W PX ∥2 is also convex. This completes the proof.\n\nm\n\nF is convex, because l(·, yi) is strongly convex. The Hessian of F has no negative eigenvalue; thus the second term in (14)\n\nB EXACT STATEMENTS OF THE MAIN THEOREMS\n\nDefinitions of some quantities:\n\n(cid:40)\n\nq =\n\n1 − αη∗(2 − η∗α), 1 − βη∗(2 − η∗β),\n\n12\n\n0 < η∗ ≤ 2 (α+β) < η∗ < 2 β ,\n\nα+β\n\n2\n\n(15)\n\nUnder review as a conference paper at ICLR 2023\n\nBδ =\n\n(cid:18) 2 · rank(X) δ\n\n+ ∥W∗∥2\n\nX\n\n(cid:19)\n\n,\n\nC1 = nN κ2Bδ\n\nC0 (η0 − η)2/η2\n\n0\n\n+ ln N,\n\nC2 = nN κ2BδC0 + ln N,\n\nC3 = nN κ2Bδ\n\nC4 = nN κ2Bδ\n\nC0 (η0 − η)2/η2 1\n(η0 − η)2/η2\n\n0\n\n0\n\n+ C0 ln(N ),\n\n,\n\nC5 = nN κ2BδC0 + C0 ln(N ), C6 = nN κ2Bδ,\n\nwhere N denotes the number of distinct elements in the set {n1, · · · , nN −1}, η1 = 2nN η0 = 2nN\n\ne2cN β with c > 0.\n\nN β , and\n\nTheorem B.1. Given any c > 0, and 0 < δ < 1/2, define η0 = 2nN rate of η < η0. There exists a constant C := C(c), such that if\n\ne2cN β , and consider the learning\n\nnmin ≥ C · C1 · N,\n\n(16)\n\nthen with probability at least 1 − δ over the random Gaussian initialization, we have\n\n(cid:32)\n\nEDLN (t) ≤\n\n1 − 4e−c\n\nη η0\n\n(1 − η η0\n\n)\n\n(cid:33)t\n\nκ\n\nEDLN (0).\n\nTheorem B.2. Given any c > 0, and 0 < δ < 1/2, define η0 = 2nN rate to be η < η0. There exists a constant C := C(c), such that if\n\ne2cβN , and consider the learning\n\nthen with probability at least 1−δ over the random one peak projection and embedding initialization, we have\n\nnmin ≥ C · C3,\n\n(17)\n\n(cid:32)\n\nEDLN (t) ≤\n\n1 − 4e−c\n\nη η0\n\n(1 − η η0\n\n)\n\n(cid:33)t\n\nκ\n\nEDLN (0).\n\nSpecially, if n1 = n2 = · · · = nN −1 = n ≥ min{nN , n0}, then requirement (17) can be replaced by\n\nRemark 7. Assume L(aN WN · · · W1) = 1 F , and n1 = · · · = nN −1 = n. Then, for Gaussian initialization, our Theorem B.1 leads to Theorem 4.1 in Du & Hu (2019). Similarly, for orthogonal initialization, our Theorem B.2 leads to Theorem 4.1 of Hu et al. (2020).\n\nn ≥ C · C4. 2 ∥aN WN · · · W1X − Y ∥2\n\n(18)\n\nNext, we present a version of the theorem related to balanced initialization.\n\nTheorem B.3. Assume n1 = · · · = nN −1 = n. Given any c > 0, and 0 < δ < 1/2, define η0 = 2nN e2cβN , and consider the learning rate as η < η0. There exists a constant C := C(c), such that as long as\n\nn ≥ C · C4.\n\n(19)\n\nthen with probability at least 1 − δ over the special balanced initialization, we have\n\n(cid:32)\n\nEDLN (t) ≤\n\n1 − 4e−c\n\nη η0\n\n(1 − η η0\n\n)\n\n(cid:33)t\n\nκ\n\nEDLN (0).\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nC INEQUALITIES IN CONVEX OPTIMIZATION\n\nConvex optimization has been studied for about a century. Recall the definitions and basic inequalities for α−strongly convex and β−Lipschitz functions.\n\nDefinition C.1. A continues differentiable function f is said to be β− Lipschitz if the gradient ∇f is β− Lipschitz, that is if for all x, y,\n\nf is said to be α−strongly convex if for all x, y, we have\n\n∥∇f (y) − ∇f (x)∥ ≤ β ∥y − x∥ ,\n\nf (y) ≥ f (x) + ⟨∇f (x), y − x⟩ +\n\nα 2\n\n∥y − x∥2 .\n\n(20)\n\n(21)\n\nProposition C.1. If f is α−strongly convex and ∇f is β−Lipschitz with respect to a (semi-)norm, then α ≤ β and\n\n⟨∇f (x), y − x⟩ +\n\nα 2\n\n∥y − x∥2 ≤ f (y) − f (x) ≤ ⟨∇f (x), y − x⟩ +\n\nβ 2\n\n∥y − x∥2 ,\n\n⟨∇f (x) − ∇f (y), x − y⟩ ≥\n\nαβ α + β ∥∇f (x) − ∇f (y)∥ ≥ α ∥x − y∥ ,\n\n∥x − y∥2 +\n\n1 α + β\n\n∥∇f (x) − ∇f (y)∥2 ,\n\nf (x) − f (y) ≤ ⟨∇f (x), x − y⟩ −\n\n1 2β\n\n∥∇f (x) − ∇f (y)∥2 .\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\nProof of Proposition C.1. We only prove the last inequality. Let z = y − 1\n\nβ (∇f (y) − ∇f (x)). Since f is convex β−Lipschitz, we have\n\nand\n\nThus,\n\nf (z) − f (x) ≥ ⟨∇f (x), z − x⟩\n\nf (z) − f (y) ≤ ⟨∇f (y), z − y⟩ +\n\nβ 2\n\n∥z − y∥2 .\n\nf (x) − f (y) =f (x) − f (z) + f (z) − f (y)\n\n≤⟨∇f (x), x − z⟩ + ⟨∇f (y), z − y⟩ +\n\nβ 2\n\n∥z − y∥2\n\n=⟨∇f (x), x − y⟩ −\n\n1 2β\n\n∥∇f (x) − ∇f (y)∥2.\n\nBefore we prove Lemma D.1, let us first include and prove the following result.\n\nLemma C.2. 1. Assume L is α−strongly convex, α > 0. Denote a global minimizer of L by W∗. Then, for any W ,\n\nL(W∗) − L(W ) ≥ −\n\n2. Assume ∇L is β−Lipschitz, then\n\n1 2α\n\n∥∇L(W )∥2\n\nX .\n\nL(W∗) − L(W ) ≤ −\n\n1 2β\n\n∥∇L(W )∥2\n\nX .\n\n(26)\n\n(27)\n\nProof of Lemma C.2. 1. First, we know that ∇L(W∗) = 0. L is α−strongly convex, which implies the inequality (22) holds. Thus\n\nL(V ) − L(W ) ≥ ⟨∇L(W ), V − W ⟩X +\n\nα 2\n\n∥V − W ∥2\n\nX =: g(V ).\n\nMinimizing both sides in terms of V gives (26).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nNow we focus on minimizing g(V ). Since g(V ) ∈ C 1 and the global minimizer exits, we have\n\n∇g(V ∗) = ∇L(W )PX + α(V ∗ − W )PX = 0,\n\nwhere V ∗ is a global minimizer for g(V ). Thus, 1\n2α\n\ng(V ∗) = −\n\n∥∇L(W )∥2\n\nX .\n\n2. Applying proposition C.1 to a β−Lipschitz function ∇L, we obtain\n\n(28)\n\nL(W∗) − L(W )\n\n≤⟨∇L(W∗), W∗ − W ⟩X −\n\n= −\n\n1 2β\n\n∥∇L(W )∥2\n\nX .\n\n1 2β\n\n∥∇L(W ) − ∇L(W∗)∥2\n\nX\n\nD CONVERGENCE REGION\n\nIn this section, we evaluate a class of the convergence region for deep linear neural networks with a deterministic initialization. Define A|R(X) = AX T (XX T )−X = APX , and view A|R(X) as a linear operator on R(X).\n\nRecall the optimization problem\n\nminimize W1,··· ,WN\n\nLN (W1, · · · WN ) :=\n\n1 m\n\nm (cid:88)\n\ni=1\n\nl(aN WN :1xi, yi) = L(aN WN :1),\n\n(29)\n\nand GD\n\n(cid:40)\n\nWj(t + 1) = Wj(t) − η ∂LN where ∂LN\n\n∂Wj\n\n∂Wj\n\n(W1(t), · · · , WN (t)), j = 1, · · · , N,\n\n(W1, · · · , WN ) = aN (WN :j+1)T ∇L(aN WN :1)(Wj−1:1)T ,\n\n(30)\n\nwhere the normalization factor aN =\n\n√\n\n1 n1n2···nN −1nN\n\n.\n\nThe following theorem generalizes the idea from a recent work (Du & Hu, 2019; Hu et al., 2020).\n\nFor notational convenience, we denote Wj:i(t) = Wj(t) · · · Wi(t), Lt = L(aN WN :1(t)), ∇Lt = ∇L(aN WN :1(t)) etc. Lemma D.1. Assume the initialization simultaneously satisfies the following conditions:\n\n\n\n \n\nσmax(WN :i+1(0)) ≤ ec1/2(nN −1:i)1/2, 1 ≤ i ≤ N − 1, σmin(WN :i+1(0)) ≥ e−c2/2(nN −1:i)1/2, 1 ≤ i ≤ N − 1, σmax(Wi−1:1(0)|R(X)) ≤ ec1/2(ni−1:1)1/2, 2 ≤ i ≤ N, σmin(Wi−1:1(0)|R(X)) ≥ e−c2/2(ni−1:1)1/2, 2 ≤ i ≤ N, ∥Wj:i(0)∥ ≤ M/2 · N θ((cid:81) L0 − L(W∗) ≤ βB0 =: B,\n\ni≤k≤j−1 nk · max{ni−1, nj})1/2, 1 < i ≤ j < N,\n\nwhere c1, c2, M are positive constant and θ ≥ 0. Notice that B0 is a proper upper bound for ∥aN WN :1(0)∥2\n\nX + ∥W∗∥2\n\nX .\n\nSet the learning rate as η = (1−ε)2nN\n\ne6c1+3c2 βN , where 0 < ε < 1. Define γ = 2e6c1 εαN\n\nnN\n\n.\n\nAssume that\n\nnmin ≥\n\nC(c1, c2)M 2κ2B0 ε2\n\nN 2θnN .\n\nThen, GD (30) satisfies\n\nLt − L(W∗) ≤ (1 − ηγ)t (L0 − L(W∗)), t = 1, 2, · · · .\n\n15\n\n(31)\n\n(32)\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition D.1. For given c1, c2, M, B0 > 0, and θ ≥ 0, we define the convergence region R(c1, c2, θ, M, B0) by the set of initialization that satisfies the inequality system (31).\n\nRemark 8. The condition (31) describes the convergence region for initialization and the condition (32) describes the overparameterization for deep linear neural networks. At this time, it is not clear how large this convergence region is. Later, we will show that the properly scaled random initialization with some extra mild overparameterization conditions will fall into this convergence region with high probability.\n\nProof of Lemme D.1. To prove Lemma D.1, it suffices to show that the following three properties hold A(t), B(t), and C(t) for all t = 0, 1, · · · .\n\n1. A(t):\n\n2. B(t):\n\n3. C(t):\n\n\n\n \n\nLt − L(W∗) ≤ (1 − ηγ)t (L0 − L(W∗)).\n\nσmax(WN :i+1(t)) ≤ ec1(nN −1:i)1/2, 1 ≤ i ≤ N − 1, σmin(WN :i+1(t)) ≥ e−c2(nN −1:i)1/2, 1 ≤ i ≤ N − 1, σmax(Wi−1:1(t)|R(X)) ≤ ec1(ni−1:1)1/2, 2 ≤ i ≤ N, σmin(Wi−1:1(t)|R(X)) ≥ e−c2(ni−1:1)1/2, 2 ≤ i ≤ N, ∥Wj:i(t)∥ ≤ M · N θ(\n\n(cid:81)\n\n1 nmin\n\ni−1≤k≤j nk)1/2, 1 < i ≤ j < N.\n\n∥Wi(t) − Wi(0)∥F ≤\n\n2e2c1 √\n\n√\n\n2βB\n\nnN γ\n\n=: R, 1 ≤ i ≤ N.\n\nUsing simultaneous induction, the proof of Lemma D.1 is divided into the following three claims.\n\nClaim 1. A(0), · · · , A(t), B(0), · · · , B(t) =⇒ C(t + 1).\n\nClaim 2. C(t) =⇒ B(t), if nmin ≥ C(c1,c2)M 2κ2B0 only depend on c1, c2.\n\nε2\n\nN 2θnN , where C(c1, c2) is a positive constant\n\nClaim 3. A(t), B(t) =⇒ A(t + 1), if nmin ≥ C(c1, c2)M 2B0N 2θnN , where C(c1, c2) is a positive constant only depend on c1, c2.\n\nProof of Claim 1. As a consequence of Lemma C.2 and Lemma 2.1, and A(s), s ≤ t, we have\n\n∥∇L(aN WN :1(s))∥2\n\nF = ∥∇Ls − ∇L(W∗PX )∥2\n\nX\n\n≤2β[Ls − L(W∗)] ≤2β (1 − ηγ)s B.\n\nFrom A(0), · · · , A(t), B(0), · · · , B(t), we have for any 0 ≤ s ≤ t,\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n∂L ∂Wi\n\n(s)\n\n(cid:13) (cid:13) (cid:13) (cid:13)F\n\n≤ aN ∥WN :i+1(s)∥ ∥∇L(aN WN :1(s))∥F\n\n(cid:13) (cid:13)Wi−1:1(s)|R(X)\n\n(cid:13) (cid:13)\n\n≤\n\n≤\n\ne2c1 √\nnN e2c1 √\nnN\n\n∥∇L(aN WN :1(s))∥F\n\n(cid:113)\n\n2β (1 − ηγ)s B.\n\n16\n\n(33)\n\n(34)\n\nUnder review as a conference paper at ICLR 2023\n\nThen,\n\n∥Wi(t + 1) − Wi(0)∥F ≤\n\n=\n\nt (cid:88)\n\ns=0\n\nt (cid:88)\n\ns=0\n\n∥Wi(s + 1) − Wi(s)∥F\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nη\n\n∂L ∂Wi\n\n(s)\n\n(cid:13) (cid:13) (cid:13) (cid:13)F\n\nt (cid:88)\n\n(1 − ηγ)s/2\n\ns=0\n\nt (cid:88)\n\n(1 − ηγ/2)s\n\n≤ η\n\ne2c1 √\nnN\n\n(cid:112)2βB\n\n≤ η\n\ne2c1 √\nnN √\n\n2e2c1 √\n\n(cid:112)2βB\n\ns=0\n\n= R.\n\n2βB\n\nnN γ\n\nThis proves C(t + 1).\n\n≤\n\nProof of Claim 2. Let δi = Wi(t) − Wi(0), 1 ≤ i ≤ N . Using C(t), we have ∥δi∥F ≤ R, 1 ≤ i ≤ N . Set ε1 = e−c1/2 min{ec1 − ec1/2, e−c2/2 − e−c2 , 1/2}. It is suffices to show that\n\n∥WN :i(t) − WN :i(0)∥ ≤ ec1/2ε1(nN −1nN −1 · · · ni−1)1/2, 1 < i ≤ N, (cid:13) (cid:13) (cid:13) ≤ ec1/2ε1(n1n2 · · · ni−1)1/2, 1 ≤ i < N, (cid:13)(Wi:1(t) − Wi:1(0))|R(X)\n\n(35)\n\n(36)\n\nand\n\n∥Wj:i(t) − Wj:i(0)∥ ≤ M/2 · N θ\n\n\n\n\n\n1 nmin\n\n(cid:89)\n\ni−1≤k≤j\n\n\n\n1/2\n\nnk\n\n\n\n, 1 < i ≤ j < N,\n\n(37)\n\nbecause σmin(A + B) ≥ σmin(A) − σmax(B) = σmin(A) − ∥B∥ and σmax(A + B) ≤ σmax(A) + σmax(B) = ∥A∥ + ∥B∥ (e.g. see Theorem 1.3 in Chafaı et al. (2009)). Case 1. We first prove (37). For 1 ≤ i < j ≤ N , we can write Wj:i(t) = (Wj(0) + δj) · · · (Wi(0) + δi). Expanding the above product, each term has the form:\n\nWj:(ks+1)(0) · δks · W(ks−1):(ks−1+1)(0) · δks−1 · · · δk1 · W(k1−1):i(0),\n\n(38)\n\nwhere i ≤ k1 < · · · < ks ≤ j are positions at which perturbation terms δkl are taken out. Notice that the convergence region assumption (31) implies that for any 1 < i ≤ j < N ,\n\n∥Wj:i(0)∥ ≤ M/2 · N θ\n\n\n\n\n\n(cid:89)\n\ni≤k≤j−1\n\n\n\n1/2\n\nnk · max{ni−1, nj}\n\n\n\n≤ M · N θ\n\n(cid:18) (cid:81)\n\ni−1≤k≤j nk nmin\n\n(cid:19)1/2\n\n.\n\n(39)\n\nWLOG, assume M ≥ 1. If i = j + 1, then\n\nAssuming i > 1, j < N , and applying inequality (39) as well as the following inequality\n\n∥Wj:i(0)∥ = ∥I∥ ≤ M · N θ(nj/nmin)1/2.\n\nj−i+1 (cid:88)\n\ns=1\n\n(cid:19)\n\n(cid:18)j − i + 1 s\n\nxs = (1 + x)j−i+1 − 1 ≤ (1 + x)N − 1, ∀x ≥ 0,\n\nwe obtain\n\n≤\n\n∥Wj:i(t) − Wj:i(0)∥ (cid:18)j − i + 1 s\n\nj−i+1 (cid:88)\n\n(cid:19)\n\ns=1\n\nRs(M · N θ)s+1n−s/2\n\nmin (ni−1 · · · nj/nmin)1/2\n\n≤M · N θ(ni−1 · · · nj/nmin)1/2[(1 + R · M · N θ/ ≤ε1M · N θ(ni−1 · · · nj/nmin)1/2.\n\n√\n\nnmin)N − 1]\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nThe last line holds due to the following reasons: there exists absolute constant A1, A2 > 0 such that\n\n(1 + x)N − 1 ≤ A2xN,\n\nif x ≥ 0, N ≥ 1, and xN ≤ A1. Since there exists positive constant C(c1, c2), which only depends on c1, c2, such that when\n\nnmin ≥\n\nC(c1, c2)M 2κ2B0 ε2\n\nN 2θnN\n\n(40)\n\nwe can have\n\nas well as\n\nR · M · N θ+1/\n\n√\n\nnmin ≤ A1,\n\n[(1 + R · M · N θ/\n\n√\n\nnmin)N − 1] ≤ A2 · M · R · N θ+1/\n\n√\n\nnmin ≤ ε1 = ε1(c1, c2).\n\nCase 2. The proof of (35) is similar. Set j = N , we can save the factor M · N θ from previous calculation, which means\n\n∥WN :i(t) − WN :i(0)∥\n\n≤ec1/2\n\nN −i+1 (cid:88)\n\ns=1\n\n(cid:18)N − i + 1 s\n\n(cid:19)\n\nRs(M · N θ)sn−s/2\n\nmin (ni−1 · · · nN −1)1/2\n\n√\n\nnmin)N − 1]\n\n≤ec1/2(ni−1 · · · nN −1)1/2[(1 + R · M · N θ/ ≤ec1/2ε1(ni−1 · · · nN −1)1/2, i ≥ 2,\n\nwhere the last line is implied by equation (40). Case 3. Similarly, we have\n\n(cid:13) (cid:13)Wj:1(t)|R(X) − Wj:1(0)|R(X)\n\n(cid:13) (cid:13)\n\n≤ec1/2\n\n(cid:19)\n\n(cid:18)j s\n\nj (cid:88)\n\ns=1\n\nRs(M · N θ)sn−s/2\n\nmin (n1 · · · nj)1/2\n\n≤ec1/2(n1 · · · nj)1/2[(1 + R · M · N θ/ ≤ec1/2ε1(n1 · · · nj)1/2, j ≤ N − 1\n\n√\n\nnmin)N − 1]\n\nThis proves B(t).\n\nProof of Claim 3. GD (7) implies\n\nWN :1(t + 1) (cid:18)\n\n=\n\nWN (t) − η\n\n∂LN ∂WN\n\n(cid:19) (cid:18)\n\n(t)\n\nWN −1(t) − η\n\n∂LN ∂WN −1\n\n(cid:19)\n\n(cid:18)\n\n(t)\n\n· · ·\n\nW1(t) − η\n\n(cid:19)\n\n(t)\n\n∂LN ∂W1\n\n=WN :1(t) − η · aN\n\nN (cid:88)\n\ni=1\n\nWN :i+1(t)W T\n\nN :i+1(t)∇L(aN WN :1(t))(Wi−1:1(t))T (Wi−1:1(t)) + E(t),\n\nwhere E(t) contains all the high-order terms (those with η2 or higher). We define a linear operator\n\nWN :i+1(t)W T\n\nN :i+1(t)(APX )(Wi−1:1(t)|R(X))T Wi−1:1(t)|R(X),\n\n(41)\n\nP (t)[A] = a2\n\nN\n\nN (cid:88)\n\ni=1\n\nfor any A ∈ RnN ×n0 .\n\nNow we have\n\naN WN :1(t + 1) = aN WN :1(t) − η · P (t)[∇L(aN WN :1(t)PX )] + aN E(t).\n\n(42)\n\nEasy to check that P (t)[·] is a sum of positive semidefinite linear operator.\n\nThe following proposition describes the eigenvalues of the linear operator P (t)[·].\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nProposition D.2. Let S1, S2 be symmetric matrices. Suppose S1 = U Λ1U T , S2 = V Λ2V T , where U = [u1, u2, · · · , um], and V = [v1, v2, · · · , vn] are orthogonal matrices, and Λ1 = diag(λ1, λ2, · · · , λm) and Λ2 = diag(μ1, μ2, · · · , μn) are diagonal matrices. Then, the linear operator L(A) := S1AS2 is orthogonally diagonalizable, and L(Aij) = λiμjAij, where λiμj represent all the eigenvalues corresponding to their eigenvectors Aij = uivT j .\n\nApplying this proposition and the assumption B(t), we obtain the upper bound and lower bound for the maximum and minimum eigenvalues of the positive definite operator P (t), respectively,\n\nλmax(P (t)) ≤ a2\n\nN\n\nN (cid:88)\n\ni=1\n\nσ2\n\nmax(Wi−1:1(t)|R(X)) · σ2\n\nmax(WN :i+1(t)) ≤\n\nN nN\n\ne2c1,\n\nand\n\nλmin(P (t)) ≥ a2\n\nN\n\nN (cid:88)\n\ni=1\n\nIn conclusion, we have\n\nσ2\n\nmin(Wi−1:1(t)|R(X)) · σ2\n\nmin(WN :i+1(t)) ≥\n\nN nN\n\ne−2c2.\n\n(43)\n\nλmax(P (t)) ≤\n\nN nN\n\ne2c1 , and λmin(P (t)) ≥\n\nN nN\n\ne−2c2.\n\nWith a learning rate of η = ηε = (1−ε)2nN\n\ne6c1+3c2 βN , 0 < ε < 1, we have\n\nLt+1 − Lt\n\n≤ ⟨∇Lt, −ηP (t)[∇Lt]⟩X + ⟨∇Lt, aN E(t)⟩X +\n\nβ 2\n\n∥ηP (t)[∇Lt] − aN E(t)∥2\n\nX\n\n= ⟨∇Lt, −ηP (t)[∇Lt]⟩ +\n\nη2 ∥P (t)[∇Lt]∥2\n\nX + F (t)\n\n(cid:18)\n\n≤ −\n\nηλmin(P (t)) −\n\nβ 2\n\n(cid:19)\n\nη2λ2\n\nmax(P (t))\n\n∥∇Lt∥2\n\nX + F (t)\n\nβ 2\n\n≤ −e−2c2\n\n(cid:18)\n\nN nN\n\nη\n\n1 − e4c1+2c2\n\n(cid:19)\n\nβ 2\n\nη\n\nN nN\n\n∥∇Lt∥2\n\nX + F (t),\n\nwhere\n\nF (t) = ⟨∇Lt, aN E(t)⟩X +\n\nβ 2\n\n∥ηP (t)[∇Lt] − aN E(t)∥2\n\nX −\n\nβ 2\n\nη2 ∥P (t)[∇Lt]∥2\n\nX .\n\nWe claim that F (t) is sufficiently small, such that\n\nη\n\n(cid:18)\n\n≤ −e−2c2\n\nLt+1 − Lt N\nnN N\nnN = −e−6(c1+c2) 2ε(1 − ε)\n\n≤ −e−3c2\n\n(cid:18)\n\nη\n\n1 − e4c1+2c2\n\n1 − e6c1+3c2\n\n∥∇Lt∥2\n\nX .\n\nβ\n\n(cid:19)\n\n(cid:19)\n\nβ 2\n\nβ 2\n\nη\n\nη\n\nN nN N\nnN\n\n∥∇Lt∥2\n\nX + F (t)\n\n∥∇Lt∥2\n\nX\n\n(44)\n\n(45)\n\n(46)\n\nAssuming this claim for the moment, we complete the proof. Combining (26) and (46), we have\n\n(cid:40)\n\nwhich implies\n\nLt+1 − Lt ≤ −e−6(c1+c2) 2ε(1−ε) L(W∗) − Lt ≥ − 1\n\n2α ∥∇Lt∥2\n\nX ,\n\nβ\n\n∥∇Lt∥2\n\nX ,\n\nLt+1 − L(W∗) ≤\n\n(cid:18)\n\n1 − e−6(c1+c2) 4ε(1 − ε)\n\nκ\n\n(cid:19)\n\n(Lt − L(W∗)),\n\n(47)\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nthat is\n\nLt − L(W∗) ≤\n\n(cid:18)\n\n1 − e−6(c1+c2) 4ε(1 − ε)\n\nκ\n\n(cid:19)t\n\nWhile estimating F (t), we observe that\n\n(L0 − L(W∗)) = (1 − ηγ)t (L0 − L(W∗)).\n\n(48)\n\n|F (t)|\n\n≤ ∥∇Lt∥X ∥aN E(t)∥X +\n\nβ 2\n\n= : I1 + I2.\n\n(2ηλmax(P (t)) ∥∇Lt∥X ∥aN E(t)∥X + ∥aN E(t)∥2\n\nX )\n\nFrom (34), we have (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13)\n\n∂L ∂Wi\n\n(t)\n\n≤\n\ne2c1 √\nnN\n\n∥∇L(aN WN :1(t))∥F =\n\ne2c1 √\nnN\n\n∥∇L(aN WN :1(t))∥X =: K.\n\nExpanding the product\n\nWN :1(t + 1) =\n\nWN (t) − η\n\n(cid:18)\n\n∂LN ∂WN\n\n(cid:19) (cid:18)\n\n(t)\n\nWN −1(t) − η\n\n∂LN ∂WN −1\n\n(cid:19)\n\n(cid:18)\n\n(t)\n\n· · ·\n\nW1(t) − η\n\n(cid:19)\n\n(t)\n\n,\n\n∂LN ∂W1\n\neach term has the form:\n\n∆ = WN :(ks+1)(t) · η\n\n∂L ∂Wks\n\n(t) · W(ks−1):(ks−1+1)(t) · η\n\n∂L ∂Wks−1\n\n(t) · · · η\n\n∂L Wk1\n\n(t) · W(k1−1):1(t),\n\nwhere 1 ≤ k1 < k2 < · · · < ks ≤ N .\n\nAs a direct consequence of inequality B(t) and inequality (39), we obtain\n\n∥∆∥X = ∥∆PX ∥F ≤\n\n1 √\n\nnN\n\naN\n\ne2c1 (ηK)s\n\n(cid:18) M · N θ nmin\n\n√\n\n(cid:19)s−1\n\n,\n\nRecall that E(t) contains all high-order terms (those with η2 or higher) in the expansion of the product. Thus, E(t) can be expressed as follows:\n\nN (cid:88)\n\n(cid:88)\n\ns=2\n\n1≤k1<k2<···<ks≤N\n\nWN :(ks+1)(t)·η\n\n∂L ∂Wks\n\n(t)·W(ks−1):(ks−1+1)(t)·η\n\n∂L ∂Wks−1\n\n(t) · · · η\n\n∂L Wk1\n\n(t)·W(k1−1):1(t).\n\nSet ξ = min{(e−2c2 − e−3c2 )/e4c1+1, 1\n\n4 (e6c1 − e4c1)/e6c1+1, 1\n\n2 (e6c1 − e4c1 )1/2/e4c1+1, 1}.\n\nRecall the inequality\n\n≤ (eN )s. Thus, we have\n\n(cid:19)\n\n(cid:18)N s\n\naN ∥E(t)∥X\n\n≤\n\n√\n\n≤\n\n√\n\n1 nN\n\n1 nN\n\n1 nN N\nnN\n\ne2c1\n\n(cid:19)\n\n(cid:18)N s\n\nN (cid:88)\n\ns=2\n\n(ηK)s\n\n(cid:19)s−1\n\n(cid:18) M · N θ nmin\n\n√\n\n(cid:18) M · N θ nmin\n\n√\n\n(cid:19)−1\n\ne2c1\n\nN (cid:88)\n\ns=2\n\n(eN )s(ηK)s\n\n(cid:19)s\n\n(cid:18) M · N θ nmin\n\n√\n\n≤\n\n√\n\ne2c1(ηeKN )\n\n√\n\nηeKM · N θ+1/ 1 − ηeKM · N θ+1/\n\nnmin √\n\nnmin\n\n≤ξ\n\nη · e4c1+1 ∥∇L(aN WN :1(t))∥X ( if ηeKM · N θ+1/\n\n=ξ · e4c1+1\n\n(cid:19)\n\n(cid:18)\n\nη\n\nN nN\n\n∥∇L(aN WN :1(t))∥X .\n\n(49)\n\nnmin < ξ/(1 + ξ))\n\n√\n\nUsing (33) and the upper bound of η, we know that there exists constant C(c1, c2), such that\n\nnmin ≥ C(c1, c2)M 2 · B0N 2θnN ,\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nand\n\nηeKM · N θ+1/\n\n√\n\nnmin ≤\n\n√\n\n2\n\nUsing (49), we have\n\n2M · e1+2c1\n\n√\n\nB0N θ√\n\nnN\n\n√\n\nnmin\n\n=\n\n1 C ′(c1, c2)\n\n≤\n\nξ 2\n\n≤\n\nξ 1 + ξ\n\n.\n\nI1 ≤ ξ · e4c1+1\n\n(cid:19)\n\n(cid:18)\n\nη\n\nN nN\n\n∥∇Lt∥2\n\nX ≤ (e−2c2 − e−3c2 )\n\n(cid:19)\n\n(cid:18)\n\nη\n\nN nN\n\n∥∇Lt∥2\n\nX ,\n\n(50)\n\nand\n\nI2\n\n≤\n\nβ 2\n\n(cid:18)\n\n2ξ · e6c1+1\n\n≤(e6c1 − e4c1)\n\nβ 2\n\nThus, (46) valid. This proves A(t).\n\n(cid:19)\n\n∥∇Lt∥2\n\nX + ξ2 · e8c1+2\n\n(cid:19)\n\n(cid:18)\n\nη2 N 2 n2 N\n\n(cid:19)\n\n∥∇Lt∥2\n\nX\n\n(cid:18)\n\nη2 N 2 n2 N\nη2 N 2 n2 N\n\n∥∇Lt∥2\n\nX .\n\nAs a direct consequence of the proof of Lemma D.1, we can obtain the following lemma.\n\nLemma D.3. Assume all assumptions in Lemma D.1 hold. For any τ > 0, we can choose new constants c1, c2 as well as C := C(c1, c2) such that the overparameterization assumption (32) in Lemma D.1 hold and\n\n∥R(t)∥X ≤ τ ∥aN WN :1(t) − W∗∥X ,\n\n(51)\n\nwhere\n\naN WN :1(t + 1) = aN WN :1(t) −\n\nN nN\n\nη∇L(aN WN :1(t)) + R(t).\n\nProof of Lemma D.3. Due to (33), (42), (44), (49), and lemma C.2, we have\n\n∥R(t)∥X =\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\naN E(t) + η\n\n(cid:18) N nN\n\n∇Lt − P (t)[∇Lt]\n\n≤ ∥aN E(t)∥X + η max\n\n(cid:26)\n\nλmax(P (t)) −\n\n(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)X N\nnN N\nnN\n\n(cid:27)\n\n− λmin(P (t))\n\n∥∇Lt∥X\n\n,\n\nN nN\n\n· ∥∇Lt∥X\n\n≤(C ′ · ξ + max{e2c1 − 1, 1 − e−2c2 }) · η\n\n2(cid:112)2β(Lt − L(W∗)) e6c1+3c2 · β\n\n≤\n\n· (C ′ · ξ + max{e2c1 − 1, 1 − e−2c2}).\n\nBecause Lt − L(W∗) is non-increasing in t, and C ′ is a constant that depends only on c1, c2, we can choose a sufficiently small positive c1, c2 and ξ, which depends on τ , such that\n\n∥R(t)∥X ≤ τ\n\n(cid:112)2β(Lt − L(W∗)) β\n\n≤ τ ∥aN WN :1(t) − W∗∥X .\n\nLemma D.4. Assume τ ∈ [0, 1). Consider a discrete dynamical system V (t) such that,\n\nwhere ∥R(t)∥X ≤ τ ∥V (t) − W∗∥X . If η∗ ≤ 2/β, we have\n\nV (t + 1) = V (t) − η∗∇L(V (t)) + R(t),\n\n∥V (t) − W∗∥2\n\nX ≤ (q + 7τ )t ∥V (0) − W∗∥2\n\nX ,\n\nwhere q is defined in (15).\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nProof of Lemma D.4. Set ∆(t) = V (t) − W∗ and τ ′ = τ ∥∆(t)∥X . Notice that\n\n∆(t + 1) = ∆(t) − η∗(∇L(V (t)) − ∇L(W∗)) + R(t),\n\nand\n\n≤η2\n\nX\n\n∥∆(t + 1)∥2 ∗ ∥∇L(V (t)) − ∇L(W∗)∥2 + ∥∆(t)∥2\n\nX − 2η∗⟨∆(t), ∇L(V (t)) − ∇L(W∗)⟩X X + (2 ∥∆(t)∥X + 2η∗ ∥∇L(V (t)) − ∇L(W∗)∥X + τ ′)τ ′.\n\nBy inequality (23),\n\n∥∆(t + 1)∥2\n\nX\n\n≤ ∥∆(t)∥2\n\nX − 2η∗⟨∆(t), ∇L(V (t)) − ∇L(W∗)⟩X X + 7τ ∥∆(t)∥2\n\n∗ ∥∇L(V (t)) − ∇L(W∗)∥2\n\nX\n\n+ η2\n\n=(1 + 7τ ) ∥∆(t)∥2\n\nX − 2η∗⟨∆(t), ∇L(V (t)) − ∇L(W∗)⟩X\n\n+ η2\n\n∗ ∥∇L(V (t)) − ∇L(W∗)∥2\n\nX\n\n≤(1 + 7τ ) ∥∆(t)∥2\n\nαβ α + β\n\n∥∆(t)∥2\n\nX\n\nX − 2η∗ (cid:19)\n\n∥∇L(V (t)) − ∇L(W∗)∥2\n\nX .\n\n2η∗ α + β\n\n(cid:18)\n\n+\n\nη2\n\n∗ −\n\n2\n\nCase 1: In this case, we have\n\nα+β < η∗ < 2 β .\n\n∥∆(t + 1)∥2\n\nX\n\n≤(1 + 7τ ) ∥∆(t)∥2\n\nX − 2η∗\n\n≤(1 + 7τ ) ∥∆(t)∥2\n\nX − 2η∗\n\nαβ α + β\n\nαβ α + β\n\n∥∆(t)∥2\n\nX +\n\n∥∆(t)∥2\n\nX +\n\n(cid:18)\n\nη2\n\n∗ −\n\n(cid:18)\n\nη2\n\n∗ −\n\n(cid:19)\n\n(cid:19)\n\n2η∗ α + β\n\n2η∗ α + β\n\n∥∇L(V (t)) − ∇L(W∗)∥2\n\nX\n\nβ2 ∥∆(t)∥2\n\nX\n\n≤ (1 + 7τ − βη∗(2 − η∗β)) ∥∆(t)∥2 =(q + 7τ ) ∥∆(t)∥2\n\nX\n\nX .\n\nCase 2: 0 < η∗ ≤ 2 Similarly, we have\n\nα+β .\n\n∥∆(t + 1)∥2\n\nX ≤ (1 + 7τ − αη∗(2 − η∗α)) ∥∆(t)∥2\n\nX = (q + 7τ ) ∥∆(t)∥2\n\nX .\n\nIn both cases, we have ∥∆(t + 1)∥2\n\nX ≤ (q + 7τ ) ∥∆(t)∥2\n\nX .\n\nThus, ∥∆(t)∥2\n\nX ≤ (q + 7τ )t ∥∆(0)∥2\n\nX .\n\nNext, we will show that the trajectories of the GD (30) for deep linear neural networks (29) are close to those of GD (2) for the corresponding convex problem (1).\n\nLemma D.5. Consider the GD for the deep linear neural networks (30) with learning rate η < η1 for aN WN :1(t), t = 0, 1, · · · , and the GD (2) with learning rate η∗ = N η for W (t), t = 0, 1, · · · . nN\n\nAssume C(c1, c2) exists in Lemma D.1 for any c1, c2 > 0. For any τ ∈ (0, 1), η < η1 (η1 defined in B), we can choose c1, c2 > 0 and the constant C = C(c1, c2) = C ′(τ, η/η1), such that inequality (51) holds, given initialization condition (31), and overparameterization condition\n\nnmin ≥ CM 2κ2B0N 2θnN .\n\n(52)\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFurthermore, we have\n\nX ≤ D(τ, q, t) ∥aN WN :1(0) − W∗∥2 ∥aN WN :1(t) − W (t)∥2 (cid:18)\n\n(cid:19)\n\nX ,\n\n|EDLN (t) − E(t)| ≤ β\n\nqt/2(cid:112)D(τ, q, t) +\n\n1 2\nEDLN (t) ≤ 3β(q + τ )t ∥aN WN :1(0) − W∗∥2 1−q , 2(q + τ )t(cid:111) (cid:110) τ\n\nX ,\n\nwhere D(τ, q, t) = min\n\n, with q defined in (15).\n\nD(τ, q, t)\n\n∥aN WN :1(0) − W∗∥2\n\nX , (53b)\n\n(53a)\n\n(53c)\n\nProof of Lemma D.5. Using Lemma D.3, we obtain that for any τ ∈ (0, 1) and η < η1, we can find sufficiently small positive constants c1, c2, which only depend on τ, η/η1, and constant C = C(c1, c2) = C ′′(τ, η/η1) mentioned in Lemma D.3, such that\n\nη =\n\n(1 − ε)2nN e6c1+3c2 βN\n\n,\n\nwhere 0 < ε < 1, as well as\n\nV (t + 1) = V (t) − η∗∇L(V (t)) + R(t),\n\nwhere V (t) = aN WN :1(t), η∗ = N nN\n\nη, and ∥R(t)∥X ≤ τ ′ = τ ∥V (t) − W∗∥X .\n\nNotice that θ0 := η/η1 = 1−ε\n\ne6c1+3c2 and η/η0 = 1 − ε, where η0 =\n\n2nN\n\ne6c1+3c2 βN .\n\nFor the right hand side of inequality (32), we have\n\nC(c1, c2)M 2κ2B0 ε2\n\nN 2θnN =\n\nC ′′(τ, η/η1)M 2κ2B0 ε2\n\nN 2θnN .\n\nTo show that inequality (32) is equivalent to inequality (52), it suffices to show that ε only depend on τ, η/η1. Notice that\n\nε = 1 − η/η0 = 1 − θ0e6c1+3c2 ,\n\nand c1, c2 only depend on τ and η/η1, which implies ε only depend on τ, η/η1.\n\nNow, we will prove the three inequalities in (53).\n\nRecall GD (2) for W (t). Define ∆(t) = V (t) − W (t) = aN WN :1(t) − W (t). Notice that\n\n∆(t + 1) = ∆(t) − η∗(∇L(V (t)) − ∇L(W (t))) + R(t),\n\nand\n\n≤η2\n\nX\n\n∥∆(t + 1)∥2 ∗ ∥∇L(V (t)) − ∇L(W (t))∥2 + ∥∆(t)∥2\n\nX − 2η∗⟨∆(t), ∇L(V (t)) − ∇L(W (t))⟩X X + (2 ∥∆(t)∥X + 2η∗ ∥∇L(V (t)) − ∇L(W (t))∥X + τ ′)τ ′.\n\nLet lt = 2 ∥∆(t)∥X + 2η∗ ∥∇L(V (t)) − ∇L(W (t))∥X + τ ′. Now, we aim to find an upper bound for lt. Applying lemma C.2 with the assumption 0 < η∗ = N nN\n\nη < 2\n\nβ , we know that\n\nlt ≤ (6 ∥∆(t)∥X + τ ′) ≤ 7(∥W (t) − W∗∥X + ∥V (t) − W∗∥X ).\n\n(54)\n\nThus\n\nltτ ′ ≤ 7τ ∥V (t) − W∗∥X (∥V (t) − W∗∥X + ∥W (t) − W∗∥X ) =: Utτ.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nBy inequality (23),\n\n∥∆(t + 1)∥2\n\nX\n\n≤ ∥∆(t)∥2\n\nX − 2η∗⟨∆(t), ∇L(V (t)) − ∇L(W (t))⟩X\n\n∗ ∥∇L(V (t)) − ∇L(W (t))∥2\n\nX + Utτ\n\n+ η2 = ∥∆(t)∥2\n\nX − 2η∗⟨V (t) − W (t), ∇L(V (t)) − ∇L(W (t))⟩X\n\n+ η2\n\n∗ ∥∇L(V (t)) − ∇L(W (t))∥2 αβ α + β (cid:19)\n\n∥∆(t)∥2\n\nX − 2η∗\n\nX\n\n(cid:18)\n\n≤ ∥∆(t)∥2\n\nX + Utτ\n\nη2\n\n∗ −\n\n2η∗ α + β\n\n∥∇L(V (t)) − ∇L(W (t))∥2\n\nX + Utτ.\n\n+\n\n2\n\nCase 1: In this case, we have\n\nα+β < η∗ < 2 β .\n\n∥∆(t + 1)∥2\n\nX\n\n≤ ∥∆(t)∥2\n\nX − 2η∗\n\n≤ ∥∆(t)∥2\n\nX − 2η∗\n\nαβ α + β\n\nαβ α + β\n\n∥∆(t)∥2\n\nX +\n\n∥∆(t)∥2\n\nX +\n\n(cid:18)\n\nη2\n\n∗ −\n\n(cid:18)\n\nη2\n\n∗ −\n\n(cid:19)\n\n(cid:19)\n\n2η∗ α + β\n\n2η∗ α + β\n\n∥∇L(V (t)) − ∇L(W (t))∥2\n\nX + Utτ\n\nβ2 ∥∆(t)∥2\n\nX + Utτ\n\n≤ (1 − βη∗(2 − η∗β)) ∥∆(t)∥2 = : q ∥∆(t)∥2\n\nX + Utτ.\n\nX + Utτ\n\nCase 2: 0 < η∗ ≤ 2 Similarly, we have\n\nα+β .\n\n∥∆(t + 1)∥2 In both cases, we have 0 < q < 1.\n\nX ≤ (1 − αη∗(2 − η∗α)) ∥∆(t)∥2\n\nX + Utτ =: q ∥∆(t)∥2\n\nX + Utτ.\n\n(55)\n\nFirst of all, since Ut ≤ U0 and ∥∆(0)∥X = 0, we obtain that\n\n∥∆(t)∥2\n\nX ≤\n\nU0τ 1 − q\n\n+ qt\n\n(cid:18)\n\n∥∆(0)∥2\n\nX −\n\n(cid:19)\n\nU0τ 1 − q\n\n≤\n\nU0τ 1 − q\n\n≤\n\nApplying Lemma D.4 for V (t) and W (t), we obtain ∥V (t) − W∗∥2 and ∥W (t) − W∗∥2\n\nX ≤ qt ∥W (0) − W∗∥2\n\nX , respectively. Thus,\n\n∥V (0) − W∗∥2\n\n14τ 1 − q X ≤ (1 + ε)tqt ∥V (0) − W∗∥2\n\nX .\n\nX\n\n|L(W (t)) − L(aN WN :1(t))|\n\n≤|⟨∇L(W (t)), ∆(t)⟩X | +\n\nβ 2\n\n∥∆(t)∥2\n\nX\n\n≤β ∥W (t) − W∗∥X · ∥∆(t)∥X +\n\n(cid:18)\n\nqt/2\n\n≤β\n\n(cid:114) 14τ 1 − q\n\n+\n\n7τ 1 − q\n\n(cid:19)\n\nGenerally speaking, (55) implies\n\nβ 2\n\n∥∆(t)∥2\n\nX\n\n∥V (0) − W∗∥2\n\nX .\n\nWe have\n\n∥∆(t)∥2\n\nX ≤ τ\n\nt−1 (cid:88)\n\nj=0\n\nqt−1−jUj.\n\n∥∆(t)∥2\n\nX ≤ 14τ\n\nt−1 (cid:88)\n\nj=0\n\n(q + 7τ )jqt−1−j ∥V (0) − W∗∥2\n\nX\n\n≤ 2(q + 7τ )t\n\n(cid:18)\n\n1 − (cid:0)\n\n(cid:1)t(cid:19)\n\nq q + 7τ\n\n∥V (0) − W∗∥2\n\nX\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nThus, we have\n\n∥aN WN :1(t) − W (t)∥2\n\nX ≤ min\n\n(cid:26) 14τ 1 − q\n\n(cid:27)\n\n, 2(q + 7τ )t\n\n∥V (0) − W∗∥2\n\nX ,\n\nas well as\n\n|L(W (t)) − L(aN WN :1(t))|\n\n≤β ∥W (t) − W∗∥X · ∥∆(t)∥X +\n\nβ 2\n\n∥∆(t)∥2\n\nX\n\n(cid:32)(cid:115)\n\n≤β\n\nmin\n\n(cid:26) 14τ 1 − q\n\n, 2(q + 7τ )t\n\n(cid:27)\n\n· qt/2 +\n\n1 2\n\nmin\n\n(cid:26) 14τ 1 − q\n\n(cid:27)(cid:33)\n\n, 2(q + 7τ )t\n\n∥V (0) − W∗∥2\n\nX .\n\nBy triangle inequality as well as L(W (t)) − L(W∗) ≤ β\n\n2 qt ∥V (0) − W∗∥2\n\nX , we have\n\n|L(aN WN :1(t)) − L(W∗)| ≤ 3β(q + 7τ )t ∥V (0) − W∗∥2\n\nX .\n\nWithout loss of generality, we replace all 14τ and 7τ by τ , which completes the proof.\n\nE GAUSSIAN INITIALIZATION FALL INTO THE CONVERGENCE REGION\n\nIn this section, we first establish some spectral properties of the products of random Gaussian matrices. The spectral properties lead to the conclusion: overparameterization guarantees that the random initialization will fall into the convergence region with high probability. Gaussian initialization: Denote by N (0, 1) the standard Gaussian distribution, and χ2 degrees of freedom. Let Sd−1 = {x ∈ Rd; ∥x∥2 = 1} be the unit sphere in Rd. The scaling factor aN = of every input in expectation. Lemma E.1. For any x ∈ Rn0, the Gaussian initialization satisfies\n\nensures that the networks at initialization preserves the norm\n\nk the chi square distribution with k\n\nn1n2···nN\n\n1√\n\n(cid:104)\n\nE\n\n∥aN WN :1(0)x∥2\n\n2\n\n(cid:105)\n\n= ∥x∥2 2 .\n\nProof of Lemma E.1. For random matrix A ∈ Rni×ni−1 with i.i.d N (0, 1) entries and any vector 0 ̸= v ∈ Rni−1 , the distribution of ∥Av∥2\n\n. We rewrite\n\n2 ∥v∥2 2\n\nis χn2\n\ni\n\n∥WN :1(0)x∥2 where Zi = ∥Wi:1(0)x∥2 / ∥Wi−1:1(0)x∥2. Then we know that the distribution of random variable Z1 ∼ χ2 random variables Zi|(Z1, · · · , Zi−1) ∼ χ2 ni the law of iterated expectations, we have\n\n2 / ∥x∥2\n\n2 = ZN ZN −1 · · · Z1,\n\nn1, and conditional distribution of (1 < i ≤ N ). Thus, Z1, · · · , Zni are independent. By\n\nE[∥WN :1(0)x∥2\n\n2/ ∥x∥2\n\n2] =\n\nN (cid:89)\n\nj=1\n\nnj.\n\nDefine ∆1 = (cid:80)N −1\n\nj=1 1/nj. Now, we introduce a new notation Ω\n\nexists k > 0, such that Ω\n\n(cid:17)\n\n(cid:16) 1 ∆1\n\n≥ k ∆1\n\n.\n\n(cid:17)\n\n(cid:16) 1 ∆1\n\n, which means that there\n\nLemma E.2. Consider real random matrix Aj ∈ Rnj ×nj−1, 1 ≤ j ≤ q with i.i.d N (0, 1) entries and any vector 0 ̸= x ∈ Rn1 . Define ∆1(q) = (cid:80)q\n\nand nmin = min1≤j≤q nj. Then\n\n1 nj\n\nj=1\n\nP(∥AqAq−1 · · · A1x∥2\n\n2 / ∥x∥2\n\n2 > ecn1 · · · nq) ≤ exp\n\n(cid:27)\n\n(cid:26)\n\n−\n\nc2 8∆1(q)\n\n=: f1(c), ∀c > 0.\n\n(56)\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nWhen 0 < c ≤ 3 ln 2, ∆1(q) ≤ c/(12 ln 2), we have\n\nP(∥AqAq−1 · · · A1x∥2\n\n2 / ∥x∥2\n\n2 < e−cn1 · · · nq) ≤ exp\n\n(cid:26)\n\n−\n\nc2 36 ln(2)∆1(q)\n\n(cid:27)\n\n=: f2(c).\n\n(57)\n\nHence, for any x ∈ Sn0−1 with probability at least 1 − e−Ω(\n\n1\n\n∆1(q) ), we have\n\ne−c2/2(n1 · · · nq)1/2 ≤ ∥Aq · · · A1x∥2 ≤ ec1/2(n1 · · · nq)1/2,\n\nwhen 0 < c2 ≤ 3 ln 2, ∆1(q) ≤ c2/(12 ln 2).\n\nProof of Lemma E.2. For random matrix Ai ∈ Rni×ni−1 with i.i.d N (0, 1) entries and any vector 0 ̸= v ∈ Rni−1 , the random variable ∥Aiv∥2\n\nis distributed as χ2\n\n2\n\nni. We rewrite\n\n∥v∥2 2\n\n∥Aq · · · A1x∥2\n\n2 / ∥x∥2\n\n2 = ZqZq−1 · · · Z1,\n\nwhere Zi = ∥Ai:1x∥2 / ∥Ai−1:1x∥2. We have Z1 ∼ χ2 Recall the moments of Z ∼ χ2\n\nm:\n\nn1, Zi|(Z1, · · · , Zi−1) ∼ χ2\n\nni\n\n(1 < i ≤ q).\n\nE[Z λ] =\n\n2λΓ( m 2 + λ) Γ( m 2 )\n\n, ∀λ > −\n\nm 2\n\n.\n\nNow, we aim to find the Chernoff type bound.\n\nCase 1: We define ratio of Gamma function\n\nR(x, λ) =\n\nΓ(x + λ) Γ(x)\n\n, λ > 0, x > 0.\n\nIn Jameson (2013), we have\n\nR(x, λ) ≤ x(x + λ)λ−1 ≤ (x + λ)λ, λ > 0, x > 0.\n\n(58)\n\nFixed c > 0, for any λ > 0 we have\n\nP(Zq · · · Z1 > ecn1 · · · nq) ≤ P((Zq · · · Z1)λ > eλc(n1 · · · nq)λ) ≤ e−λc(n1 · · · nq)−λE[(Zq · · · Z1)λ]\n\n(Markov inequality)\n\n= exp{−λ(c + ln(n1 · · · nq))}\n\nq (cid:89)\n\nj=1\n\n2λR(nj/2, λ) (Law of total expectation)\n\n≤ exp{−λ(c + ln(n1 · · · nq)) + qλ ln 2 +\n\nq (cid:88)\n\nj=1\n\nλ ln(\n\nnj 2\n\n+ λ)}(Inequality (58))\n\n= exp{−λc + λ\n\nq (cid:88)\n\nj=1\n\nln(1 +\n\n2λ nj\n\n)}\n\n≤ exp{−λc + 2λ2\n\nq (cid:88)\n\nj=1\n\n1 nj\n\n}.\n\nDefine constant ∆1(q) = (cid:80)q\n\nj=1\n\n1 nj\n\n. Set λ = c\n\n4∆1(q) , we obtain (56).\n\nCase 2: Let nmin = min1≤j≤q nj.\n\nP(Zq · · · Z1 < e−cn1 · · · nq) ≤ P((Zq · · · Z1)λ > e−λc(n1 · · · nq)λ)\n\n≤ exp{λ(c − ln(n1 · · · nq)) + qλ ln 2 +\n\nq (cid:88)\n\nj=1\n\nln R(\n\nnj 2\n\n, λ)}.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nDefine\n\nf (λ) = λ(c − ln(n1 · · · nq)) + qλ ln 2 +\n\nq (cid:88)\n\nj=1\n\nln R(\n\nnj 2\n\n, λ), −\n\nnmin 2\n\n< λ ≤ 0.\n\nNotice that f (0) = 0. Define digamma function,\n\nψ(x) =\n\nd dx\n\nln(Γ(x)) =\n\nΓ′(x) Γ(x)\n\n.\n\nQi et al. (2006) proved the following sharp inequality of digamma function,\n\nln(x +\n\n1 2\n\n) −\n\n1 x\n\n< ψ(x) < ln(x + e−γ) −\n\n1 x\n\n, x > 0,\n\nwhere γ is the Euler-Mascheroni constant, and e−γ ≈ 0.561459. Thus,\n\nf ′(λ) = c +\n\nq (cid:88)\n\n(cid:104)\n\nj=1\n\n− ln(\n\nnj 2\n\n) + ψ(\n\nnj 2\n\n(cid:105)\n\n+ λ)\n\n≥ c +\n\nq (cid:88)\n\nj=1\n\nln(1 +\n\nλ + 1/2 nj/2\n\n) −\n\nq (cid:88)\n\nj=1\n\n1 nj/2 + λ\n\n.\n\nSince ln(1 + x) is concave, we have\n\nln(1 + x) ≥ 2 ln(2)x, x ∈ [−1/2, 0].\n\nIf − nmin\n\n4 ≤ λ ≤ 0, then\n\nf (λ) = f (0) −\n\n(cid:90) 0\n\nf ′(x)dx\n\nx + 1/2 nj/2\n\n) −\n\nq (cid:88)\n\nj=1\n\n1 nj/2 + x\n\n\n\n dx\n\nλ + 1/2 nj/2\n\n) + (nj/2 + 1/2) ln(1 +\n\nλ nj/2 + 1/2\n\n) − λ − ln(1 +\n\n(cid:21)\n\nλ nj/2\n\n)\n\nλ \n\n\n\nq (cid:88)\n\nln(1 +\n\nj=1\n\n(cid:20)\n\nλ ln(1 +\n\n≤ cλ +\n\n= cλ +\n\n≤ cλ +\n\n(cid:90) λ\n\n0\n\nq (cid:88)\n\nj=1\n\nq (cid:88)\n\nj=1\n\n(λ − 1) ln(1 +\n\nλ nj/2\n\n)\n\n≤ cλ + 4 ln(2)λ(λ − 1)∆1(q).\n\nAssume 0 < c ≤ 3 ln 2. Let A = 12 ln 2, and λ∗ = − c λ∗ ≥ −nmin/4. Assume ∆1(q) ≤ c/(12 ln 2). Thus\n\nA∆1(q) . Since nmin∆1(q) ≥ 1, we have\n\nc2 A∆1(q)\n\n+ 4 ln 2\n\nc2 A∆1(q)\n\n(cid:18) ∆1(q) c\n\n+\n\n(cid:19)\n\n1 A\n\n≤ −\n\nc2 36 ln(2)∆1(q)\n\n.\n\n(59)\n\nf (λ∗) ≤ −\n\nThus, we obtain (57).\n\nLemma E.3. There exists a positive constant C(c1, c2) which only depends on c1, c2, such that if (cid:17)(cid:111) nN ∆1 ≤ C(c1, c2), then for any fixed 1 < i ≤ N , with probability at least 1 − exp we have\n\n(cid:16) 1 ∆1\n\n−Ω\n\n(cid:110)\n\nσmax(WN :i(0)) ≤ ec1(ni−1ni · · · nN −1)1/2,\n\n(60)\n\n(61)\n\nand\n\nσmin(WN :i(0)) ≥ e−c2 (ni−1ni · · · nN −1)1/2.\n\nProof of Lemma E.3. Let A = W T\n\nN :i(0). We know that\n\nσmax(A) = ∥A∥ = sup\n\nv∈SnN −1\n\n∥Av∥2\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nand\n\nσmin(A) = inf\n\nv∈SnN −1\n\n∥Av∥2 .\n\nApplying lemma E.2, we know that with probability at least 1 − exp\n\n(cid:110)\n\n−Ω\n\n(cid:16) 1 ∆1\n\n(cid:17)(cid:111) ,\n\n∥Av∥2 / ∥v∥2 ∈ [e−c2/2P, ec1/2P ],\n\nwhere P = (ni−1 · · · nN −1)1/2. Set φ = min{1 − e−c1/2, (e−c2/2 − e−c2)/(e−c2/2 + ec1)}. Take a φ-net Nφ for SnN −1 with size |Nφ| ≤ (3/φ)nN . Notice that with this size we can actually cover the unit ball, not only the unit sphere.\n\nThus, with probability at least 1 − |Nφ| exp\n\n−Ω\n\n, for all u ∈ Nφ simultaneously we have\n\n(cid:110)\n\n(cid:17)(cid:111)\n\n(cid:16) 1 ∆1\n\n∥Au∥2 / ∥u∥2 ∈ [e−c2/2P, ec1/2P ].\n\nFixed v ∈ SnN −1, there exists u ∈ Nφ such that ∥u − v∥2 ≤ φ. WLOG, we assume 1 − φ ≤ ∥u∥2 ≤ 1. We obtain\n\n∥Av∥2 ≤ ∥Au∥2 + ∥A(u − v)∥2 ≤ ec1/2P + φ ∥A∥ .\n\nTaking supereme over ∥v∥2 = 1, we obtain\n\nσmax(A) = ∥A∥ ≤\n\nec1/2 1 − φ\n\nP ≤ ec1 P.\n\nFor the lower bound, we have\n\n∥Av∥2 ≥ ∥Au∥2 − ∥A(u − v)∥2 ≥ e−c2/2P ∥u∥ − φ ∥A∥ ≥\n\n(cid:104)\n\n(1 − φ)e−c2/2 − φec1\n\n(cid:105)\n\nP ≥ e−c2 P.\n\nTaking the infimum over ∥v∥2 = 1, we get\n\nσmin(A) ≥ e−c2P.\n\nThe conclusions hold with probability at least\n\n(cid:26)\n\n1 − |Nφ| exp\n\n−Ω\n\n(cid:19)(cid:27)\n\n(cid:18) 1 ∆1\n\n(cid:26)\n\n≥1 − exp{nN ln(3/φ)} exp\n\n−Ω\n\n(cid:26)\n\n≥1 − exp\n\n−Ω\n\n(cid:19)(cid:27)\n\n,\n\n(cid:18) 1 ∆1\n\n(cid:19)(cid:27)\n\n(cid:18) 1 ∆1\n\nsince nN ∆1 ≤ C(c1, c2).\n\nLemma E.4. There exists a positive constant C(c1, c2) which only depends on c1, c2, such that if rank(X)∆1 ≤ C(c1, c2), then for any fixed 1 ≤ j < N , with probability at least 1 − exp{−Ω\n\n} we have\n\n(cid:17)\n\n(cid:16) 1 ∆1\n\nand\n\nσmax(Wj:1(0)|R(X)) ≤ ec1(n1n2 · · · nj)1/2,\n\nσmin(Wj:1(0)|R(X)) ≥ e−c2(n1n2 · · · nj)1/2.\n\n(62)\n\n(63)\n\nProof of Lemma E.4. The proof is similar to that of previous lemma. The only difference is that now we consider the φ−net to cover the unit sphere in R(X) ∩ Rn0 , with dim R(X) ∩ Rn0 = rank(X), where R(X) represents the column space of X.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nLemma E.5. Set C = nmax/nmin < ∞, θ = 1/2. Assume Ω(1/∆1) ≥ k ∆1 constant and ∆1 satisfies\n\n, where 0 < k < 1 is a\n\n \n\n\n\n(cid:110) k\n\n5 ln(6) , (cid:110)\n\nk 5 ln(5 ln(6)e/k)\n\n(cid:111)\n\n5 ln(5 ln(6)e/k) , k\n\nk\n\n5\n\n(cid:111)\n\n∆1 ≤ min\n\n∆1 ln(C) ≤ min ∆1 ln(N 2θ) ≤ k/5.\n\nGiven 1 < i ≤ j < N , with probability at least 1 − 2e−k/(5∆1) = 1 − e−Ω(1/∆1) we have\n\n∥Wj:i(0)∥ ≤ Mk\n\n√\n\nCN θ(ni · · · nj−1 · max{ni−1, nj})1/2,\n\nwhere Mk is a positive constant that only depends on k.\n\nProof of Lemma E.5. WLOG, assume ni−1 ≤ nj. Let A = Wj:i(0). From lemma E.2, we know that fixed v ∈ Sni−1−1, with probability at least 1−e−Ω(1/∆1) we have ∥Av∥2 ≤ 4/3(ni · · · nj)1/2. .\nTake a small constant c = 5 ln(6)C . Let v1, · · · , vni−1 be an orthonormal basis for Rni−1 . Partition the index set {1, 2, · · · , vni−1} = S1 ∪ S2 ∪ · · · ∪ S⌈N 2θ/c⌉, where |Sl| ≤ ⌈cni−1/N 2θ⌉ for each 1 ≤ l ≤ ⌈N 2θ/c⌉. The following discussion is similar to the proof of lemma E.3, hence we omit some details. For each l, taking a 1/2− net Nl for the set VSl = {v ∈ Sni−1−1; v ∈ span{vi; i ∈ Sl}}, we can get\n\nkN 2θ 5 ln(6)∆1ni−1\n\n≥\n\nk\n\nwith probability at least\n\n∥Au∥2 ≤ 4(ni · · · nj)1/2, u ∈ VSl ,\n\n1 − |Nl|e−k/∆1 ≥ 1 − exp{−k/∆1 + (cni−1/N + 1) ln 6} ≥ 1 − e−3k/(5∆1),\n\n5 ln(6) .\n\nsince ∆1 ≤ k Therefore, for any v ∈ Rni−1, we can write it as the sum v = (cid:80) for each l. We also know that ∥v∥2 Then we have\n\nl≥1 |αl|2.\n\n2 = (cid:80)\n\nl alvl, where αl ∈ R and vl ∈ VSl\n\n∥Av∥2 ≤\n\n(cid:88)\n\nl\n\nThus,\n\n|αl| ∥Avl∥2 ≤ 4(ni · · · nj)1/2\n\n(cid:115)\n\n⌈N 2θ/c⌉\n\n(cid:88)\n\nl\n\n√\n\n|al|2 ≤ Mk\n\nCN θ(ni · · · nj)1/2 ∥v∥2 .\n\n√\n\n∥A∥ ≤ Mk\n\nCN θ(ni · · · nj)1/2.\n\nNotice that when C ≤ e, ∆1 ≤\n\nk\n\n5 ln(5 ln(6)e/k) ≤\n\nk\n\n5 ln(5 ln(6)·C/k) , and when C > e, we have\n\n∆1 ln(C) ≤ min\n\n(cid:26)\n\nk 5 ln(5 ln(6)e/k)\n\n(cid:27)\n\n, k/5\n\n≤\n\nk ln(C) 5 ln(5 ln(6) · C/k)\n\n.\n\nThe success probability is at least\n\n1 − ⌈N 2θ/c⌉ · e−3k/(5∆1) (cid:18) 5 ln(6) · C k\n\n≥1 − exp\n\nln\n\n(cid:26)\n\n(cid:19)\n\n+ ln(N 2θ) − 3k/(5∆1)\n\n(cid:27)\n\n− e−3k/(5∆1)\n\n≥1 − 2e−k/(5∆1),\n\nsince\n\n∆1 ≤\n\nk 5 ln (5 ln(6) · C/k)\n\nand ∆1 ln(N 2θ) ≤ k/5.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nProof of Lemma 2.3. Set r = rank(X), and u1, · · · , ur be an orthonormal basis of the column space of X. Then, PX = (cid:80)r\n\ni=1 uiuT i .\n\nNotice that\n\n∥anWN :1(0)∥2\n\nX = ∥anWN :1(0)PX ∥2\n\nF =\n\nr (cid:88)\n\ni=1\n\n∥anWN :1(0)ui∥2 2 .\n\nBy assumption, we have\n\nE ∥anWN :1(0)∥2\n\nX = E\n\nr (cid:88)\n\ni=1\n\n∥anWN :1(0)ui∥2\n\n2 = r.\n\nThe Markov inequality implies\n\nP(∥anWN :1(0)∥2\n\nX ≥\n\n2r δ\n\n) ≤\n\nδ 2\n\n.\n\nTherefore, we can bound the initial loss value as\n\nL0 − L(W∗) ≤ ⟨∇L(W∗), aN WN :1(0)X − W∗⟩ +\n\nβ 2\n\n∥aN WN :1(0) − W∗∥2\n\nX\n\n=\n\nβ 2\n\n∥aN WN :1(0) − W∗∥2\n\nX\n\n≤ β(∥aN WN :1(0)∥2\n\nX + ∥W∗∥2\n\nX )\n\n≤ β(\n\n2r δ\n\n+ ∥W∗∥2\n\nX ),\n\nwith probability at least 1 − δ/2.\n\nProof of Theorem B.1. The requirement on size {n1, n2, · · · , nN −1, N } in (16) ensures that lemma E.3, E.4, E.5, 2.3, and D.1 hold. WLOG, we set c1 = c/6, c2 = c/3, M = 2Mk probability at least\n\nC0, B0 = Bδ, and η =: (1−ε)2nN\n\ne2cβN , then with\n\n√\n\n1 − N 2e−Ω(1/∆1) − δ/2 ≥ 1 − δ, since ∆1 ≤\n\n1 C(c)\n\nmin\n\n(cid:26) 1\n\nln N\n\n,\n\n1 ln(1/δ)\n\n(cid:27)\n\n,\n\nthe random initialization satisfies the initialization assumption (31) and the overparameterization assumption (32). By applying Lemma D.1, we complete the proof.\n\nF ORTHOGONAL INITIALIZATION FALL INTO THE CONVERGENCE REGION\n\nThe following are some basic facts for random projection and embedding. Most of the following properties can be found in Eaton (1989).\n\nProposition F.1.\n\n1. A is a random embedding if and only if AT is a random projection.\n\n2. If A is a square matrix, then random projection, random embedding and random orthogo-\n\nnal matrix are equivalent.\n\n3. The uniform distribution on the group is a left and right invariant probability measure: that is, if A is a random orthogonal matrix, then A, U A, AU are all random orthogonal matrices, where U is a non-random orthogonal matrix.\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\n4. Assume X is a n × q(q ≤ n) random matrix whose entries are i.i.d. N (0, 1) random variables. Then A := X(X T X)−1/2 is a random embedding, since AT A = Iq and the distribution of A is left invariant, which means that A and U A have the same distribution, where U is a non-random orthogonal matrix.\n\n5. If A is a uniform distribution over an orthogonal group of order n and A is partitioned as 2 are both random\n\nA = (A1, A2), where A1 is n × q and A2 is n × (n − q), then AT orthogonal matrix.\n\n1 and AT\n\n6. The columns of uniform distribution over the orthogonal group of order n, and\n\n(ξ1, · · · , ξn) 1 + ξ2 have the same distribution, where ξ1, · · · , ξn are i.i.d. N (0, 1) random variables.\n\n2 + · · · + ξ2\n\n(cid:112)ξ2\n\nn\n\n7. Assume A = An×p, n ≤ p is a random orthogonal projection. For any v ∈ Sp−1, ∥Av∥2 j ) are both following beta distribution with α = n/2, β = (p −\n\nand ((cid:80)n n)/2, where ξ1, · · · , ξn are i.i.d. N (0, 1) random variables.\n\ni )/((cid:80)p\n\nj=1 ξ2\n\ni=1 ξ2\n\n2\n\nRemark 9. There are several ways to construct the random matrix A = (aij)q×n, q ≤ n, which is uniformly distributed over rectangular matrices with AAT = c2Iq, c > 0. Let On be uniformly distributed over a real orthogonal group of order n, and On is partitioned as On = (AT 2 )T , where A1 is q × n. Assume X = (xij)q×n, and xij are independent standard normal random variables. Then, A, cA1, and c(XX T )−1/2X have the same distribution. Lemma F.2. For any x ∈ Rn0, the one peak random projection and embedding initiation satisfies\n\n1 , AT\n\n(cid:104)\n\nE\n\n∥aN WN :1(0)x∥2\n\n2\n\n(cid:105)\n\n= ∥x∥2 2 .\n\nProof. Let D = Wp:1(0)/ Let Ai = Wi:p+1(0)/\n\n√\n\n√\n\nn1n2 · · · np. Then D is an embedding matrix. Thus, ∥Dx∥2\n\n2 = ∥x∥2 2.\n\nnpnp+1 · · · ni−1, where i ≥ p + 1, and Ap = I.\n\nSet Bi = ∥AiDx∥2 2, i ≥ p + 1. Then, Bi follows beta distribution B(ni/2, (ni−1 − ni)/2) given Bi−1, Bi−2, · · · , Bp+1, i ≥ p + 1. If ni = ni−1, then Bi|(Bi−1, Bi−2, · · · , Bp+1) = 1, a.s.\n\n2 / ∥Ai−1Dx∥2\n\nIf B ∼ B(a, b), then the expectation is given by the following equation,\n\nThus, by the law of total expectation, we have\n\nEB =\n\na a + b\n\n.\n\nnN np\n\nE ∥aN WN :1(0)x∥2\n\n2 = E ∥AN Dx∥2\n\n2 = EBN BN −1 · · · Bp+1 ∥Dx∥2\n\n2 =\n\nnN np\n\n∥x∥2 2 .\n\nThis completes the proof.\n\nNext, we introduce sub-Gaussian random variables, associated with bounds on how a random variables deviate their expected value.\n\nDefinition F.1. A random variable X with finite mean μ = EX is sub-Gaussian if there is a positive number σ such that:\n\nE[exp(λ(X − μ))] ≤ exp\n\n(cid:19)\n\n(cid:18) λ2σ2 2\n\nfor all λ ∈ R\n\n(64)\n\nSuch a constant σ2 is called a proxy variance, and we say that X is σ2-sub-Gaussian, and we write X ∼ SG(σ2). Example F.1. Normal distribution N (μ, σ2) of course is σ2 sub-Gaussian. For beta distribution, Elder (2016) showed that B(a, b) is\n\n4(a+b)+2 -sub-Gaussian and later, Marchal\n\n1\n\n& Arbel (2017) concluded\n\n1\n\n4(a+b+1) -sub-Gaussian.\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nThe Hoeffding bound for random variable X with a mean μ and sub-Gaussian parameter σ is given by,\n\nP [|X − μ| ≥ t] ≤ 2 exp\n\n−\n\n, ∀t ≥ 0.\n\n(65)\n\n(cid:26)\n\n(cid:27)\n\nt2 2σ2\n\nSimply applying the Chernoff bound for B(a, b), we obtain the following lemma. Lemma F.3. Assume a random variable B distributed as a beta distribution B(a, b) with two positive shape parameters a and b. Then, (cid:12) (cid:12) (cid:12) (cid:12)\n\n≥ y) ≤ 2 exp (cid:8)−2(a + b)y2(cid:9) , y ≥ 0.\n\na a + b\n\n(cid:12) (cid:12) P( (cid:12) (cid:12)\n\nB −\n\nHence,\n\nP\n\n(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)\n\nB −\n\na a + b\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ ε\n\n(cid:19)\n\na a + b\n\n≥ 1 − exp{−Ω(a2/(a + b))},\n\nwhere Ω(·) only depend on ε. For the upper tail, we can obtain a better bound, (cid:18)\n\n(cid:19)\n\nP\n\nB ≥ (1 + ε)\n\na a + b\n\n≤ exp {−(ε − ln(ε + 1))a} .\n\n(66)\n\nProof of Lemma F.3. We only need to prove the third inequality. Assume random variable B ∼ B(a, b). Set v = a + b, (1 + t) a v ≤ y < 1, t > 0, and r > 0. We are going to estimate the Chernoff bound for B, which is\n\nP(B ≥ y) ≤ e−(ry−ln EerB ) =: e−Ir(y).\n\nThe moment generating function of B is given by\n\nEerB = 1 +\n\n∞ (cid:88)\n\nk=1\n\na(a + 1) · · · (a + k − 1) v(v + 1) · · · (v + k − 1)\n\nrk k!\n\n≤ 1 +\n\n∞ (cid:88)\n\nk=1\n\na(a + 1) · · · (a + k − 1) vk\n\nrk k!\n\n, r > 0.\n\nRecall that the Maclaurin series of (1 − r/v)−a over (−v, v), is given by equation\n\nThus,\n\n(1 − r/v)−a = 1 +\n\n∞ (cid:88)\n\nk=1\n\na(a + 1) · · · (a + k − 1) vk\n\nrk k!\n\n.\n\nIr(y) = ry − ln EerB ≥ ry + a ln(1 − r/v).\n\nSet r = v − a/y ∈ (0, v). We obtain\n\nP(B ≥ y) ≤ exp{−(vy − a + a ln(a/(vy)))} =: exp{−vy · g(a/(vy))}, (1 + t)\n\na v\n\n≤ y < 1\n\nwhere g(x) = 1 − x + x ln(x), x = a/(vy) ∈ (0, 1/(1 + t)]. Notice that g(1) = 0 and g′(x) = ln(x) < 0 over x ∈ (0, 1). We know that\n\ng(x) ≥ g(1/(1 + t)) =\n\nThus,\n\nt − ln(1 + t) t + 1\n\n, t > 0.\n\nP(B ≥ y) ≤ exp\n\n(cid:26)\n\n−vy ·\n\n(cid:27)\n\nt − ln(1 + t) t + 1\n\n= exp {−(t − ln(1 + t))a} , y = (1 + t)\n\na v\n\n< 1.\n\nSet y = (1 + ε) a\n\na+b . We obtain the inequality (66).\n\nRemark 10. It is trivial to check ∥Wj:i(0)∥ = (nini+1 · · · nj)1/2, 1 ≤ i ≤ j ≤ p, ∥Wj:i(0)∥ = (ni−1ni · · · nj−1)1/2, p + 1 ≤ i ≤ j ≤ N, ∥Wj:i(0)∥ ≤ (nini+1 · · · nj−1)1/2(np)1/2\n\n(cid:19)1/2\n\n≤\n\n(cid:18) nmax nmin\n\n(nini+1 · · · nj−1 · max{ni−1, nj})1/2, 1 ≤ i < p < j ≤ N, (i, j) ̸= (1, N ).\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nRemark 11. As a special case, if n1 = n2 = · · · = nN −1 = n, we know that ∥Wj:i(0)∥ = (ni−1ni · · · nN −1)1/2 = n(N −i+1)/2. Lemma F.4. Assume np/ min{n1, nN −1} ≤ C0 < ∞. Set ε > 0. Let C(ε) represent the constant depend only on ε. If n1/C0 ≥ C(ε)nN , then with probability at least 1 − e−Ω(n1/C0)\n\nσmax(WN :i(0)) ≤ (1 + ε)(ni−1ni · · · nN −1)1/2, 2 ≤ i ≤ p σmin(WN :i(0)) ≥ (1 − ε)(ni−1ni · · · nN −1)1/2, 2 ≤ i ≤ p.\n\nSimilarly, if nN −1/C0 ≥ C(ε)rank(X), then with probability at least 1 − e−Ω(nN −1/C0)\n\nσmax(Wj:1(0)|R(X)) ≤ (1 + ε)(n1n2 · · · nj)1/2, p + 1 ≤ j ≤ N σmin(Wj:1(0)|R(X)) ≥ (1 − ε)(n1n2 · · · nj)1/2, p + 1 ≤ j ≤ N.\n\nN :p+1(0) and p:i(0). Assume v ∈ SnN −1. Easy to see that Ai is a product of random\n\nProof of Lemma F.4. Let D = (nN −1nN −2 · · · np)−1/2W T Ai = (npnp−1 · · · ni)−1/2W T orthogonal projection and D is a random embedding. Let e1 = (1, 0, 0, · · · , 0)T ∈ Rnp . There exists orthogonal matrix T such that T Dv = e1, ∥e1∥2 = ∥T Dv∥2 = ∥v∥2 = 1. Since random orthogonal projection are right invariant, we have\n\nP(∥AiDv∥2 ≥ y) = E\n\n(cid:16)\n\n(cid:104)\n\nE\n\nI{∥AiT T e1∥2≥y}\n\n(cid:17)(cid:105)\n\n(cid:12) (cid:12) (cid:12) D\n\n= E (cid:2)E (cid:0) I{∥Aie1∥2≥y}\n\n(cid:12) (cid:12) D(cid:1)(cid:3) = P(∥Aie1∥2 ≥ y).\n\nThis proves that ∥AiDv∥2\n\n2 and ∥Aie1∥2 2 / ∥v∥2\n\n2 have the same distribution. 2 = (cid:13) Claim: If v ̸= 0, then ∥AiDv∥2 beta distribution B(ni−1/2, (np − ni−1)/2). Define Bp = ∥Ape1∥2 Then Bp ∼ B(np−1/2, (np − np−1)/2), Bp−1|Bp ∼ B(np−2/2, (np−1 − np−2)/2), Bi|(Bp, · · · , Bi+1) ∼ B(ni−1/2, (ni − ni−1)/2). If ni+1 = ni, we know that Bi|(Bp, · · · , Bi+1) = 1, a.s. If B ∼ B(a, b), then the moments are given by the following equations,\n\n2, i = p − 1, p − 2, · · · , 1.\n\np · · · nN −1)−1/2W T\n\n2, Bi = ∥Aie1∥2\n\n2 / ∥Ai+1e1∥2\n\n(cid:13)(nini+1 · · · n2\n\nN :iv(cid:13) 2\n(cid:13) 2\n\n/ ∥v∥2\n\n2 follows\n\n· · · ,\n\n(67)\n\nEB =\n\na a + b\n\n, and EBk =\n\na a + b\n\na + 1 a + b + 1\n\n· · ·\n\na + k − 1 a + b + k − 1\n\n.\n\nBy the law of total expectation, we have\n\nEBiBi+1 · · · Bp =\n\nni−1 ni\n\nni ni+1\n\n· · ·\n\nnp−1 np\n\n=\n\nni−1 np\n\n,\n\nas well as\n\nE(BiBi+1 · · · Bp)k =\n\nni−1/2 np/2\n\nni−1/2 + 1 np/2 + 1\n\n· · ·\n\nni−1/2 + k − 1 np/2 + k − 1\n\n.\n\nNotice that all the integer moments of BiBi+1 · · · Bp match those of B(ni−1/2, (np −ni−1)/2). We can verify that beta distribution satisfies Carleman’s condition, which implies that BiBi+1 · · · Bp ∼ B(ni−1/2, (np − ni−1)/2). Thus, ∥AiDv∥2\n\n2 ∼ B(ni−1/2, (np − ni−1)/2), which proves the claim.\n\n2 / ∥v∥2\n\nWith probability at least 1 − exp{−Ω(n1/C0)}, we have\n\n(1 − ε)2 ni−1 np\n\n≤ ∥ADv∥2\n\n2 ≤ (1 + ε)2 ni−1\n\nnp\n\n, ∥v∥2 = 1.\n\nUsing the φ−net technique, which has been already used to prove lemma E.3, we know that\n\nand\n\nσmin(AD) ≥ (1 − ε)\n\nσmax(AD) ≤ (1 + ε)\n\n(cid:19)1/2\n\n(cid:18) ni−1 np\n\n(cid:19)1/2\n\n(cid:18) ni−1 np\n\n,\n\n,\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nwith probability at least 1 − exp{nN ln(3/φ(ε))} exp{−Ω(n1/C0)} ≥ 1 − exp{−Ω(n1/C0), since n1/C0 ≥ C(ε)nN , for 2 ≤ i ≤ p. Hence, with probability at least 1 − e−Ω(n1/C0), we have\n\nσmin(WN :i(0)) ≥ (1 − ε) (ni−1 · · · nN −1)1/2 ,\n\nand\n\nσmax(WN :i(0)) ≤ (1 + ε) (ni−1 · · · nN −1)1/2 . The other part of the proof is similar to that of lemma E.4, so we omit it.\n\nProof of Theorem B.2 . Set c > 0, c1 = c/6, c2 = c/3. In lemma F.4, we can pick a ε > 0, such that 1 + ε ≤ ec1/2 and 1 − ε ≥ e−c2/2. Set M = 2 The requirement on size {n1, n2, · · · , nN −1, N } in (17) make sure that the remark 10, lemma F.4, lemma 2.3, and lemma D.1 all hold. Notice that even though we need the conclusions in lemma F.4 to hold simultaneously for 2 ≤ i ≤ p, p + 1 ≤ j ≤ N , it suffices to apply lemma F.4 over i ∈ I and j ∈ J, such that {ni; i ∈ I} and {nj; j ∈ I} both have distinct values. Since |I| ≤ N and |J| ≤ N , with probability at least\n\nC0, θ = 0, B0 = Bδ, and η = (1−ε)2nN e2cβN .\n\n√\n\n1 − 2N e−Ω(nmin/C0) − δ/2 ≥ 1 − δ,\n\nthe one peak random orthogonal projection and embedding initialization satisfies the initialization assumption (31) and the overparameterization assumption (32).\n\nUnder the assumption n1 = n2 = · · · = nN −1, we can use remark 11 to replace lemma F.4. Thus, with probability at least 1 − δ/2 ≥ 1 − δ, (31) holds. By applying lemma 2.3 and D.1, we complete the proof.\n\nProof of Theorem B.3. Let WN (0) = √\n1, and W1(0) =\n\nnU1[Inx , 0]T V T\n\nnUN [Iny , 0]V T\n\n, 2 ≤ i ≤ N − 1 . Now, we want to verify (31). By a simple calculation, we have\n\nN , · · · , Wi(0) =\n\nnUiInV T\n\ni\n\n√\n\n√\n\n \n\n\n\nσmax(WN :i+1(0)) = σmin(WN :i+1(0)) = n(N −i)/2, 1 ≤ i ≤ N − 1, σmax(Wi−1:1(0)|R(X)) = σmax(Wi−1:1(0)|R(X)) = n(i−1)/2, 2 ≤ i ≤ N, ∥Wj:i(0)∥ = n(j−i+1)/2, 1 < i ≤ j < N.\n\n(68)\n\nNotice that for any 1 ≤ p ≤ m\n\n∥aN WN :1(0)x∥2\n\n2 =\n\nn nN\n\n(cid:13) (cid:13)UN [Iny , 0]V T\n\nN UN [Inx , 0]T V T\n\n1 x(cid:13) 2\n(cid:13)\n\n2 =\n\nn nN\n\n(cid:13) (cid:13)UN [Iny , 0]V T\n\nN x′(cid:13) 2\n(cid:13)\n\n2 ,\n\nwhere x′ = UN [Inx , 0]T V T Since the distribution of UN [Iny , 0]V T have\n\n1 x, ∥x∥2 = ∥x′∥2.\n\nN is right invariant under multiplying orthogonal matrices, we\n\n(cid:13) (cid:13)UN [Iny , 0]V T\n\nN x′(cid:13) 2\n2 / ∥x∥2 (cid:13)\n\n2 ∼ B(\n\nny 2\n\n,\n\nn − ny 2\n\n).\n\nThus,\n\nApplying lemma 2.3, we have\n\n(cid:104)\n\nE\n\n∥aN WN :1(0)x∥2\n\n2\n\n(cid:105)\n\n= ∥x∥2 2 .\n\nL0 − L(W∗) ≤ β\n\n(cid:18) 2 · rank(X) δ\n\n+ ∥W∗∥2\n\nX\n\n(cid:19)\n\n,\n\nwith probability at least 1 − δ/2.\n\nBy applying Lemma D.1 with c > 0, c1 = c/6, c2 = c/3, θ = 0, we complete the proof.\n\nProof of Theorem 3.1. Theorem 3.1 is a special case of Theorem B.1 and Theorem B.2. Hence, we omit the proof.\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\nProof of Theorem 3.2. In Theorem B.1, B.2, and B.3, we proved that for given constant c1, c2 > 0 and 0 < ε, δ/2 < 1/2 as well as a learning rate η, there exists a constant C = C(c1, c2) such that all three kinds of random initializations will fall into the convergence region with probability at least 1 − δ. By applying Lemma 2.3, we complete the proof.\n\nG TABLES\n\nIn this section, we provide some empirical evidences to support the argument expressed in Section 4:Why do bad saddles not affect GD for overparameterized deep linear neural networks? Consider the following procedures for tables of ∥Wi(t)−Wi(0)∥F\n\n:\n\n∥Wi(0)∥F\n\na) We consider X ∈ R128×1000, and W∗ ∈ R10×128 and set Y = W∗X + ε, where the entries\n\nin X and ε are drawn i.i.d. from N (0, 1).\n\nb) We consider the loss function 1\n\n2 ∥aN WN :1X − Y ∥2 F .\n\nc) For the given deep linear networks, we apply the orthogonal initializations, which are de-\n\nnoted as Wj(0), 1 ≤ j ≤ N .\n\nd) We set the learning rate as η = nN\n\nN ·∥X∥2 for the deep linear neural networks.\n\ne) We prepare the tables for ∥Wi(t)−Wi(0)∥F\n\n∥Wi(0)∥F\n\n.\n\nLet n1 = n2 = n3 = 2000, N = 4. Assume W∗ are drawn i.i.d. from N (0, 25). We obtain the following table:\n\ni = 1 0.05161 t = 1 0.08779 t = 2 0.11335 t = 3 0.12109 t = 4 0.12527 t = 5 0.12611 t = 6 0.12755 t = 7 0.12745 t = 8 t = 9 0.12819 t = 10 0.12793\n\ni = 2 0.00826 0.01389 0.01781 0.01894 0.01956 0.01967 0.01988 0.01986 0.01997 0.01992\n\ni = 3 0.00826 0.01389 0.01779 0.01889 0.01948 0.01958 0.01978 0.01975 0.01987 0.01982\n\ni = 4 0.18464 0.31396 0.40435 0.42920 0.44282 0.44476 0.44955 0.44876 0.45136 0.45018\n\nLet n1 = n2 = 10000, N = 2. Assume W∗ are drawn i.i.d. from N (0, 4). We obtain the following table:\n\ni = 1 0.02708 t = 1 0.04319 t = 2 0.05296 t = 3 0.05888 t = 4 0.06248 t = 5 0.06468 t = 6 0.06603 t = 7 0.06688 t = 8 t = 9 0.06741 t = 10 0.06775\n\ni = 2 0.00153 0.00244 0.00299 0.00333 0.00353 0.00365 0.00373 0.00377 0.00380 0.00382\n\ni = 3 0.04844 0.07727 0.09474 0.10533 0.11176 0.11569 0.11811 0.11962 0.12057 0.12117\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nLet n1 = n2 = 4000, N = 2. Assume W∗ are drawn i.i.d. from N (0, 1). We obtain the following table:\n\nLet n1 = n2 = 8000, N = 2. Assume W∗ are drawn i.i.d. from N (0, 1). We obtain the following table:\n\nLet n1 = n2 = 12000, N = 2. Assume W∗ are drawn i.i.d. from N (0, 1). We obtain the following table:\n\nLet n1 = n2 = 20000, N = 2. Assume W∗ are drawn i.i.d. from N (0, 1). We obtain the following table:\n\ni = 1 0.01622 t = 1 0.02684 t = 2 t = 3 0.03411 0.03919 t = 4 t = 5 0.04280 0.04539 t = 6 t = 7 0.04729 0.04869 t = 8 t = 9 0.04974 t = 10 0.05054\n\ni = 1 0.01173 t = 1 0.01944 t = 2 0.02470 t = 3 0.02838 t = 4 0.03098 t = 5 0.03287 t = 6 0.03426 t = 7 0.03530 t = 8 t = 9 0.03608 t = 10 0.03668\n\ni = 1 0.00965 t = 1 0.01597 t = 2 0.02025 t = 3 0.02323 t = 4 0.02535 t = 5 0.02690 t = 6 0.02804 t = 7 0.02890 t = 8 t = 9 0.02955 t = 10 0.03006\n\ni = 1 0.00713 t = 1 0.01181 t = 2 0.01499 t = 3 0.01720 t = 4 0.01878 t = 5 0.01994 t = 6 0.02079 t = 7 0.02144 t = 8 t = 9 0.02193 t = 10 0.02231\n\ni = 2 0.00290 0.00480 0.00609 0.00700 0.00764 0.00810 0.00844 0.00868 0.00887 0.00901\n\ni = 2 0.00148 0.00246 0.00312 0.00358 0.00391 0.00415 0.00432 0.00445 0.00455 0.00463\n\ni = 2 0.00099 0.00164 0.00208 0.00239 0.00261 0.00277 0.00289 0.00297 0.00304 0.00309\n\ni = 2 0.00057 0.00095 0.00121 0.00138 0.00151 0.00161 0.00168 0.00173 0.00177 0.00179\n\ni = 3 0.05802 0.09601 0.12202 0.14018 0.15306 0.16232 0.16908 0.17408 0.17782 0.18066\n\ni = 3 0.04195 0.06955 0.08838 0.10151 0.11083 0.11758 0.12253 0.12624 0.12904 0.13118\n\ni = 3 0.03453 0.05712 0.07244 0.08310 0.09069 0.09621 0.10029 0.10336 0.10570 0.10750\n\ni = 3 0.02551 0.04225 0.05362 0.06154 0.06720 0.07132 0.07438 0.07668 0.07844 0.07981\n\nH FIGURES\n\nIn this section, we provide some empirical evidences to support the results in Section 4: Numerical Experiments. We will show how the trajectories of the non-convex deep linear neural networks are\n\n36\n\nUnder review as a conference paper at ICLR 2023\n\nrelated to a convex optimization problem for GD under different initialization schemes. Consider the following procedures for plots of the logarithm of loss as a function of number of iterations:\n\na) We choose X ∈ R128×1000 and W∗ ∈ R10×128 and set Y = W∗X + ε, where the entries\n\nin X, W∗ and ε are drawn i.i.d. from N (0, 1).\n\nb) We consider the loss function 1\n\n2 ∥aN WN :1X − Y ∥2 F .\n\nc) For the given linear networks, we apply the Gaussian initialization and one peak random orthogonal projection and embedding initialization, which are denoted as Wj(0), 1 ≤ j ≤ N .\n\nd) For the convex optimization problem (1), we set the initialization to be W (0) =\n\naN WN (0) · · · W1(0).\n\ne) We set the learning rates as η = nN and convex problem, respectively.\n\nN ·∥X∥2 and η∗ = N\n\nnN\n\nη for the deep linear neural networks\n\nf) We draw the loss function through 25 iterations.\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Plot of Loss as a function of number of iterations with n1 = n2 = n3 = 128 (First), 200 (Second), 2000 (Third) for Gaussian initialization, respectively.\n\n38\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Plot of Loss as a function of number of iterations with n1 = n2 = n3 = 128 (First), 200 (Second), 5000 (Third) for Orthogonal initialization, respectively.\n\n39",
    "reference": "# Summary Of The Paper\n\nThis paper studies the convergence rates of gradient descent (GD) on minimizing the training loss $L(W_N \\cdots W_1)$ of deep linear networks, where $L(W)$ is the convex training loss function of the corresponding linear model (i.e., the loss when the product $W_N \\cdots W_1$ is collapsed into a single matrix $W$). We assume that $L(W)$ is strongly convex and smooth in the subspace spanned by the data points. The paper considers the following three initialization schemes\n\n- Gaussian initialization;\n- One peak random orthogonal projections and embeddings initialization (which generalizes the orthogonal initialization proposed by Hu et al. (2020));\n- Special balanced initialization (which is a special case of balanced initialization considered in Arora et al. (2018a)).\n\nFor these schemes, the paper proves that, for sufficiently wide networks,\n\n1. GD converges linearly, and the convergence rate is of the same order as minimizing $L(W)$ as a function of $W$ (Theorems B.1, B.2, and B.3; Theorems 3.1 and 3.2 in the main text are special cases of these theorems).\n2. The trajectory of the product $W_N(t) \\cdots W_1(t)$ as we minimize $L(W_N \\cdots W_1)$ in fact stays close to the trajectory of the $W(t)$ as we minimize $L(W)$ with an appropriately rescaled learning rate (Theorem 3.3).\n\n# Strength And Weaknesses\n\nConvergence of optimization methods on training linear neural networks is an important area as it can provide valuable intuitions for understanding nonlinear networks. The paper generalizes existing results and presents the main results clearly.\n\nThe $O(\\kappa \\log 1/\\varepsilon)$ iteration complexity to achieve $\\varepsilon$-suboptimality looks indeed sharp because it matches the convergence rate of GD on the convex counterpart. Nevertheless, I should mention that the $O(\\kappa \\log 1/\\varepsilon)$ complexity was also achieved by some previous results such as Du and Hu (2019) and Hu et al. (2020). \n\nThe next main result that the trajectory of the product $W_N(t) \\cdots W_1(t)$ closely follows the trajectory of $W(t)$ as we minimize the corresponding convex function $L(W)$ is something that I haven't seen in the literature, unless I missed some existing results. This part is quite intriguing as it establishes that optimizing linear NN follows a similar trajectory as the corresponding convex problem, while the problem itself is nonconvex. This observation can deliver useful insights to the community.\n\nWhile I like the observation made in Theorem 3.3, for the rest of the main results (Theorems B.1, B.2, and B.3), I got the impression that the contributions made by this paper may be somewhat limited. As pointed out above, for Gaussian and orthogonal initializations, the sharp rate $O(\\kappa \\log 1/\\varepsilon)$ was already achieved by Du and Hu (2019) and Hu et al. (2020). \n\nA quick perusal of the proof reveals that the paper also builds on the techniques developed by these two existing papers. Remark 5 states that for the case where all hidden layers have the same width, Theorems B.1 and B.2 recover the main results of these papers. The \"convergence region\" established in Lemma D.1 almost exactly follows the conditions developed in the two papers. Appendix E also seems to follow the flow of the proof in Du and Hu (2019).\n\nThese observations make me question if Theorems B.1 and B.2 are merely a technical extension of the existing results on \"hidden layers having identical widths\" to just \"hidden layers with general widths.\" I know this could be a false impression as I didn't go through the proof carefully; I would be happy if the authors prove me wrong.\n\nAnyway, this concern of novelty makes me hesitate to recommend acceptance at the moment. Importantly, the main text of this paper does not make any precise comparisons against the existing results. Can you elaborate/highlight the technical challenges that had to be overcome in extending the existing results to get Theorems B.1 and B.2?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written and delivers the main results clearly. I found the paper quite enjoyable to read. \n\nIn terms of novelty, the observation made in Theorem 3.3 (Lemmas D.3–D.5) looks novel to me, but as noted above, the remaining elements of the proof seem to rely upon existing results. \n\nSome minor issues:\n- In Eq (4), the RHS has to have a leading $\\frac{\\beta}{2}$ factor?\n- In Definition 2.1, $n_i I_{n_i}$ -> $n_i I_{n_{i-1}}$ and $n_{j-1} I_{n_{j-1}}$ -> $n_{j-1} I_{n_j}$?\n- In the definition of special balanced initialization, there is no mention on how $V_1$ is defined? Also I thought $V_N = U_{N-1}$ should also hold here?\n- In the statements of Theorem 3.1 and 3.2, it looks weird that $\\delta$ does not show up anywhere in the stated bound.\n- In Theorem 3.3, it would be useful to mention that $q \\in (0,1)$ for the step size of interest?\n- The plots in Figure 1 should better be drawn in \"semilogy\" style?\n\n# Summary Of The Review\n\nI enjoyed reading the paper and I think the results presented in the paper deliver valuable insights. However, at the same time, it may be the case that some of the main theorems rely too heavily on some existing results and hence are of limited novelty. In the rebuttal, it would be very helpful if the authors could clarify the technical barriers that had to be overcome in carrying out the extensions. I would be happy to raise my score if my concerns get resolved.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nOBJECT TRACKING BY HIERARCHICAL PART-WHOLE ATTENTION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe present in this paper that hierarchical representations of objects can provide an informative and low-noisy proxy to associate objects of interest in multi-object tracking. This is aligned with our intuition that we usually only need to compare a little region of the body of target objects to distinguish them from other objects. We build the hierarchical representation in levels of (1) target body parts, (2) the whole target body, and (3) the union area of the target and other objects of overlap. Furthermore, with the spatio-temporal attention mechanism by transformer, we can solve the tracking in a global fashion and keeps the process online. We design our method by combining the representation with the transformer and name it Hierarchical Part-Whole Attention, or HiPWA for short. The experiments on multiple datasets suggest its good effectiveness. Moreover, previous methods mostly focus on leveraging transformers to exploit long temporal context during association which requires heavy computation resources. But HiPWA focuses on a more informative representation of objects on every single frame instead. So it is more robust with the length of temporal context and more computationally economic.\n\n1\n\nINTRODUCTION\n\nHow to represent the visual existence of an object in a discriminative fashion is a core question of computer vision. In this paper, we propose a hierarchical part-whole representation to represent the visual existence of objects. We adopt multi-object tracking as the application area since the distinguishable appearance feature is critical to avoid the mismatch among target objects when tracking across frames. To gather and process the visual information from different levels, we combine the hierarchical part-whole representation with the attention mechanism from transformers to summarize distinguishable and discriminative visual representations for objects of interest.\n\nIn the task of multi-object tracking, given a bounding box to localize objects of interest, how should we recognize the major object within the box and distinguish it from the background and other objects, especially some also having partial existence in the box? We believe the visual specificity of one object comes from three perspectives: the compositional, the semantic and the contextual. The compositional suggests the salient and unique visual regions on an object, such as a hat on a pedestrian whose color is different from all others in the same image. With a salient visual composition attached to an object, we can track it across frames even without seeing its full body. The semantic visual information is the commonly adopted one in modern computer vision such as a tight bounding box or instance segmentation mask. It defines the occupancy area of the object with the bond between its visual existence and semantic concept. Finally, contextual visual information describes the surroundings of an object. It helps to distinguish an object via contrast. For example, the bounding box might contain pixels from the background and secondary objects. However, a tight bounding box offers a strong underlying prior when combined with visual context: an object whose parts span across the boundary of the bounding box should not be the major object of this bounding box. Being the secondary object or not an object of interest, it should be regarded as noise when we generate a distinguishable visual representation for the major subject in the bounding box. The analysis above shows each level has its value to represent an object discriminatively. Motivated by the insight, we propose to represent an object by a three-level hierarchy: body parts, full body, and the union area including objects with overlap. We summarize it as a “Part-Body-Union” hierarchy.\n\nWith the hierarchy constructed, an ideal path to solving the target association in multi-object tracking is to leverage the salient information within the body area and discard mismatch by eliminating the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nnoise revealed by the contextual contrast. Without requiring more fine-grained data annotation, we propose to use transformers to process the hierarchical representation as the attention mechanism can discover important visual information. So, by combining the hierarchical visual representation and attention-based feature fusion, we finally propose our method as Hierarchical Part-Whole Attention, or HiPWA for short. In this work, we build a baseline model following this design and demonstrate its effectiveness in solving multi-object tracking problems. Through experiments on multiple multiobject tracking datasets, the proposed method achieves comparable or even better performance than the state-of-the-art transformer-based methods with a more lightweight implementation and better time efficiency during training and inference.\n\n2 RELATED WORKS\n\n2.1 REPRESENTING OBJECTS BY PARTS\n\nThe most commonly used object representation for multi-object tracking is bounding boxes. However, the bounding box is noisy by containing background pixels and pixels from secondary objects. On the other hand, our life experience demonstrates that, in many scenarios, it is not necessary to observe the full body of objects to specify an object visually and tracking targets by the distinguishable parts on it is usually more efficient. Therefore, researchers also have been studying object detection and tracking with more fine-grained representation. A common way is to use pre-defined certain parts on target bodies, such as only human head (Sundararaman et al., 2021; Shao et al., 2018), human joints (Andriluka et al., 2018; Xiu et al., 2018) or even every pixel (Voigtlaender et al., 2019; Weber et al., 2021). However, all these choices require more fine-grained data annotation beyond bounding boxes and more fine-grained perception modules beyond just normally available object detectors. In the contrast, the part-whole hierarchy we construct requires no additional annotations and we still solve tracking tasks at the granularity of bounding boxes. The idea of modeling objects with different levels is inspired by the hierarchical modeling of the human body (Marr, 2010) by David Marr when he explains how to construct the visual structure of an object from primal sketch to 2.5 sketch and further 3D representation. His classic three levels of visual information processing system concludes this in a higher-level: the computational, the algorithmic, and the implementational. A similar theory is also introduced by Fodor & Pylyshyn (1988) as the semantic, the syntactic, and the physical. Compared to these cognitive theories aiming to model general visual representation, the three perspectives we propose to recognize an object and distinguish it from others (the compositional, the semantic and the contextual) only apply to the specific problem of generating an effective visual descriptor to represent the objects of interest.\n\n2.2 TRANSFORMER-BASED MULTI-OBJECT TRACKING\n\nTransformer (Vaswani et al., 2017) is originally proposed for natural language processing. It shows a powerful capacity for information representation and processing. Later, DETR (Carion et al., 2020) introduces the transformer to the area of visual perception for object detection. It models object detection as solving a bipartite matching problem. Given that the matching-based strategy by DETR is quite similar to the target matching in the task of multi-object tracking, it is intuitive to further migrate transformer to this area. TransTrack (Sun et al., 2020) is the first work using the transformer to solve the MOT problem but it does not invent any association strategy by transformers. A concurrent work TrackFormer (Meinhardt et al., 2021) takes a further step to use the cross attention in transformer decoder in the stage of association by query passing. On the other hand, VisTR (Wang et al., 2021c) proposes a novel global association scheme upon transformer where a video clip of multiple frames is forward into the transformer at the same time to associate objects within the clip. More recently, many works (Zhou et al., 2022; Zeng et al., 2021) follow the global association scheme in either training or inference and achieve good performance. A key to their success is to process the information over a long temporal period, which can be hardly handled without the transformer. GTR (Zhou et al., 2022) makes a baseline model of using only appearance in associating objects and removing some secondary modules such as positional encoding and learnable object query. However, a downside of processing multiple frames as a batch by the transformer is the high requirement of computation resources. It has become a common practice to train the model on at least 4xV100 GPUs (Zhou et al., 2022; Sun et al., 2020; Zeng et al., 2021) or even 8xA100 GPUs (Cai et al., 2022). These methods usually suffer from significant performance drop if only limited computation resource is available. This is because they usually make improvements to association performance by taking advantage of a long temporal window and gathering more visual context within it. In\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The pipeline of our proposed method. Our method follows the tracking-by-detection scheme and conducts association in a global fashion. Our proposed hierarchical feature attention module fuses the features from three levels, i.e., object parts (compositional), object bodies (semantic) and union area of objects with overlap (contextual). The output features serve as tokens in the following transformer decoder for the global association.\n\nthis work, we focus on building a more computation and memory-efficient visual representation for objects from the scope of a single frame instead. This scheme is flexible to be integrated with transformers and more robust to short time windows during object association.\n\n3 METHOD\n\nIn this section, we introduce the method we propose to leverage a hierarchical part-whole visual representation with the attention mechanism from the transformer for multi-object tracking. In Section 3.1, we describe the overview structure of our method using global association. Then, in Section 3.2, we dive into the details of our proposed part-whole attention module. Finally, we talk about the details of training and inference by HiPWA in Section 3.3.\n\n3.1 GLOBAL ASSOCIATION\n\nBefore the transformer is introduced into this area, people usually solve multi-object tracking in a frame-by-frame fashion where the association is performed on only two frames. Recently, the transformer shows the advantage to gather and process information from multiple steps in parallel. To leverage this advantage, previous methods (Wang et al., 2021c; Zhou et al., 2022) propose to perform association in a video clip instead of just two frames. Therefore, the spatio-temporal attention capacity of the transformer leads to a new global association fashion. We follow this scheme in our proposed method. The overall pipeline of HiPWA is shown in the left-hand half of Figure 1. Now, we explain the three stages of it.\n\n1 , ..., OtN\n\nDetection and Feature Extraction. Given a video clip of T frames, i.e., T = {t, t + 1, ..., t + T }, we have the corresponding images I = {I t, I t+1, ..., I t+T }. Given a detector model, we could derive the detections of the object category of interest on all frames in parallel, noted as O = {Ot1 N }. N is the number of detections on the T frames and ti ∈ T (1 ≤ i ≤ N ) is the time step where the i-th detection, i.e., Oti , is detected. Then, we generate the representations of each detected object and note them as F = {F1, F2, ..., FN }. The most commonly adopted solution is to use the backbone output on the object area as the representation features while we adopt our proposed hierarchical part-whole representation instead whose details are to be introduced soon.\n\ni\n\nToken Generation by Hierarchical Part-Whole Attention. After being projected into vectors, the hierarchical representations of detections become tokens Tdet = {T kdet N },\n\n2 , ..., T kdet\n\n1 , T kdet\n\n3\n\n...frame tframe t+TObject Detector...Hierarchical Part-WholeAttentionImagesObjectsTokensGlobal Association.....................Global Association MatrixFeature EncoderconcatPart-aware feature pyramidTokenTkCross-AttPartsBodyUnionUnder review as a conference paper at ICLR 2023\n\nwhich are also terms as “object query” in previous works (Sun et al., 2020; Meinhardt et al., 2021). Concatenating the tokens makes Qdet ∈ RN ×D, where D is the feature dimension. If we aim to associate the new-coming detections with existing trajectories, we also need the tokens to represent the existing M trajectories, i.e., Ttraj = {T ktraj M }. The transformer has shown good power to generate more discriminative feature tokens for trajectories by iterative query passing (Zeng et al., 2021) or long-time feature buffering (Cai et al., 2022). But to make our method simple, we directly project the hierarchical representation of objects on existing trajectories to represent the trajectories. Given a historical horizon H to backtrack the objects on the previous time steps of a trajectory, we represent a trajectory, T ktraj j ∈ RH×D. The track query is the combination of the feature tokens of detections within the historical horizon on the corresponding trajectory.\n\n, with the “track query” Qtraj\n\n2 , ..., T ktraj\n\n1 , T ktraj\n\nj\n\nj\n\nas S(Qtraj\n\nj , Qdet) ∈ RH×N .\n\nGlobal Association. By cross-attention, we could get the association score between the set of detections and the trajectory T ktraj In practice, because we aim to associate between all M trajectories and N detections, we perform the cross-attention on all object queries and track queries at the same time, namely S(Qtraj, Qdet) ∈ RHM ×N . By averaging the score on the H frames selected from the historical horizon, we could finally get the association score between detections and trajectories as S ∈ RM ×N . Then, we need to make sure that a trajectory will never be associated with more than one object from the same frame. We normalize the association scores between a trajectory and objects from the same time step by softmax. So the normalized association score between the j-th trajectory and the i-th detection is exp(Sj,i)\n\nP (Masso\n\nj,i = 1|Qdet, Qtraj) =\n\n(cid:80)\n\nk∈{1,2,...,N } 1[tk = ti]exp(Sj,k)\n\n,\n\n(1)\n\nwhere the binary indicator function 1[tk = ti] indicates whether the i-th detection and the k-th detection are on the same time step. Masso ∈ R(M +1)×N is the final global association matrix. Its dimension is of (M + 1) × N because each detection can be associated with an “empty trajectory” to start a new track in practice. The query of the “empty trajectory” is represented by a query randomly drawn from previous unassociated tokens during training. Also, after the association, unassociated trajectories will be considered absent on the corresponding frames. In such a fashion, we can train over a large set of detections and trajectories in parallel and also conduct inference in an online manner by setting O as the set of the detections only from the new-coming frame.\n\n3.2 HIERARCHICAL PART-WHOLE ATTENTION\n\nFinally, we come to the details about constructing hierarchical part-whole visual representations. We name this process hierarchical part-whole attention as we use the attention mechanism to gather and process information from different levels in the hierarchy, which is illustrated in the right-hand half of Figure 1. We design this representation because we think there are three perspectives to describe the existence of an object and its identification over other objects: the compositional, the semantic, and the contextual. Correspondingly, we think the body part patches, the full object body, and the union of the occupancy of objects with interaction provide the knowledge from the three perspectives respectively. The insight behind this module is what we would like the most to deliver in this work.\n\nHierarchy Construction. We represent a detected object by a quintuple, i.e., O = [x, y, w, h, c], where the first four values describe its bounding box and c is the detection confidence. So its body area is B = [x, y, x + w, y + h]. Next, we divide the body into multiple sub-regions (parts). By default, similar to what ViT (Dosovitskiy et al., 2020) does upon images, we divide the bounding boxes into 2 × 2 bins, making a set of body parts as P = {P1, P2, P3, P4}. On the other hand, from a global scope, there are other targets interacting with O which are highly likely to be mismatched with O in the association stage. We crop the union area enclosing O and all other targets having overlap with it. We note the union area as U . Till now, we have derives the part-whole hierarchy {P, B, U } in a very straightforward way.\n\nFeature Fusion. Given the part-whole hierarchy, we have to fuse the features from different levels to get the final feature tokens for association. With a feature encoder, we can extract the CNN features from them as FP ∈ R4C×H×W , FB ∈ RC×H×W and FU ∈ RC×H×W . We simply concatenate the features from the first two levels as FP+B ∈ R5C×H×W . Then, by a two-layer\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nprojection network, we gain projected features VP +B ∈ R5×D. We also apply the projection to the union area features and get VU ∈ RD. Finally, we perform cross-attention between VP +B and VU and forward the output to an MLP network to get the tokens of shape R5×D. Before being forwarded to the global association stage, the tokens would be projected to the uniform dimension of D.\n\n3.3 TRAINING AND INFERENCE\n\nThe method we implement is a baseline model without complicated designs on “queries”. We simply use the hierarchical part-whole features of detected objects to serve as the representations of both detections and trajectories. And during training, we can associate between detections in the sampled video clips or between detections and existing trajectories. These two schemes of associations thus are implemented as the same and share all model modules. During inference, to keep the process online, we only perform association between detections from the new-coming frame and existing trajectories. We realize this by iterating a sliding window with the stride of one frame.\n\nTraining. We train the association module by maximizing the likelihood of associating detections belonging to the same ground truth trajectory as expressed in Eq. 1. But Eq. 1 happens on one time step ti only. To speed up training, we calculate the association score on all T frames of the sampled video clip at the same time and maximize the likelihood of the association aligned with the ground truths globally in the time window. The objective thus turns to\n\nt+T (cid:89)\n\nq=t\n\nP (Masso j,τ j\n\nq\n\n= 1|Qdet, Qtraj),\n\n(2)\n\nwhere τ j q is the ground truth index of the detection which should be associated with the j-th trajectory on the time step q. Therefore, by traversing the association of all trajectories, the training objective becomes the negative log-likelihood loss\n\nLasso = −\n\nM (cid:88)\n\nt+T (cid:88)\n\nj=1\n\nq=t\n\nlogP (Masso j,τ j\n\nq\n\n= 1|Qdet, Qtraj).\n\n(3)\n\nOn the other hand, trajectories can also be absent on some time steps because of occlusion or target disappearance. So similar to the practice of DETR (Carion et al., 2020) for detection and GTR (Zhou et al., 2022) for tracking, Eq. 3 has included the situation of associating a trajectory with “empty”. Moreover, the main reason why mismatch happens is the features of objects of different identities being indiscriminative. Therefore, to encourage the representations from objects of different identities to be distinguishable, we design a feature discrimination loss in the form of triplet loss as\n\nLfeat = max(0,\n\nNP min u=1\n\n||Att(f (FPu ), f (FB))−f (FB)||2−||Att(f (FB), f (F bg\n\nU ))−f (FB)||2+α), (4)\n\nwhere f (·) is the shared projection layers to project CNN features to feature vectors and NP is the number of part patches (NP = 4 in our default setting). Att(·, ·) is the operation of cross attention to generate attended features. α is the margin to control the distance between positive and negative pairs. FB and FPu (1 ≤ u ≤ NP ) are the extracted features of the body area and the part subregions as explained already. F bg U is the CNN features of the background area in the union box. We obtain the background features by setting the pixels of B in the area of U to be 0 and forward the processed patch of the union area into the feature encoder. We design Eq. 4 to encourage the projection network to pay more attention to the salient area on the body of target objects while less attention to the background area when processing the hierarchical part-whole representations. Also, it encourages the features of the background area in the union box, which probably belongs to another object target, to be distinguishable from the body features. This can be expected to decrease the chance of mismatch between neighboring objects. Finally, the training objective is\n\nL = Lasso + Lfeat + Ldet, (5) where Ldet is an optional detection loss. In our default implementation, we finetune the detector at the same time when training the association modules.\n\nInference. We adopt the traditional sliding-window style to realize online influence. With a window size T = 24 and stride 1, we start from the first frame of the input video. On the first frame, every detection is initialized as an original trajectory. In each time window, we would generate\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\ntrajectories by detections within it. Then we use the association score in Eq. 1 to associate these trajectories with existing trajectories outside this time window. By averaging the detection-trajectory alongside detections of a trajectory, we get the trajectory-trajectory association scores, whose negative value serves as the entries in the cost matrix for the association assignment. And we adopt Hungarians matching to make sure the one-to-one mapping. Only when a pair of trajectories has the association score higher than a threshold β = 0.3, they are eligible to be associated. All unassociated detections on the new-coming frames will start new tracks.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENT SETUPS Datasets. We conduct quantitative experiments on multiple multi-object tracking datasets, including MOT17 (Milan et al., 2016), MOT20 (Dendorfer et al., 2020) and DanceTrack (Sun et al., 2021). We focus on pedestrian tracking in this paper so pedestrian is the only category of objects of interest on all datasets. MOT17 and MOT20 are the classic and popular datasets in the area of pedestrian tracking but their scales are relatively small and have no official validation sets. DanceTrack, on the contrary, is a recently proposed dataset that is of a much larger scale and provides an official validation set with no overlap with the training set. DanceTrack focuses on the scenarios where targets are in the foreground so detection is not considered as the bottleneck as it is on MOT20. And DanceTrack mainly contains videos where targets have heavy occlusion, complex motion patterns, and similar appearances so it provides a good platform to study the robustness of the tracking algorithm.\n\nEvaluation Metrics. The popular CLEAR evaluation protocol (Bernardin & Stiefelhagen, 2008) is based on single-frame-wise matching between the ground truth and predictions. This makes the metric emphasize single-frame detection quality rather than cross-frame association performance. MOTA, the main metric of CLEAR protocol, is also biased to the detection quality. To provide a more accurate sense of association performance in tracking, we mainly adopt the more recent HOTA (Luiten et al., 2021) metric set where the metric is calculated by the video-level association between ground truth and predictions. In the set of metrics, AssA emphasizes the association performance, and DetA stresses on the detection quality. HOTA is the main metric by taking both detection and association quality into consideration.\n\nImplementation. We follow the common practice (Sun et al., 2020; Zeng et al., 2021; Cai et al., 2022) to use ResNet-50 (He et al., 2016) as the backbone network, which is pretrained on Crowdhuman (Shao et al., 2018) dataset first. Though advanced detector (Zhang et al., 2021a) is demonstrated as a key to boosting tracking performance, we want our contribution to be more from the improvement of the association stage. Therefore, on MOT17, we follow the practice of another transformerbased global association tracking method GTR (Zhou et al., 2022) to use the classic CenterNet Zhou et al. (2019; 2020) as the detector and all training details are aligned with it to make fair comparisons with this close baseline method. The CenterNet detector is pretrained together with the backbone on Crowdhuman to align with the common practice on this dataset. For the fine-tuning of association modules, we use a 1:1 mixture of MOT17 and Crowdhuman for MOT17. We fine-tune with only the MOT20 training set for evaluation on MOT20. For DanceTrack, we use its official training set as the only training set during finetuning. The image size is set to be 1280 × 1280 during training and the test size is 1560 for the longer edge during the test. During finetuning, the detector head is also finetuned as mentioned already. The training iterations are set to be 20k on MOT17/MOT20 and 80k on DanceTrack. We use BiFPN (Tan et al., 2020) for the feature upsampling. For the implementation of the transformer, we follow the practice of Zhou et al. (2022) to use a stack of two layers of “Linear + ReLU” as the projection layers and one-layer encoders and decoders. We use AdamW (Loshchilov & Hutter, 2017) optimizer for training whose base learning rate is set to be 5e-5. The length of the video clip is T = 8 for training and T = 24 for inference in a sliding window. We use 4 × V100 GPUs as the default training device following some previous practice (Zhou et al., 2022; Zeng et al., 2021) but we will see that even using only one RTX 3090 GPU for training, our method can also achieve good performance. The training on MOT17 or MOT20 takes only 4 hours and the training on DanceTrack takes 11 hours.\n\n4.2 BENCHMARK RESULTS\n\nWe benchmark our proposed method with existing methods now. The results on the MOT17-test dataset are shown in Table 1. HiPWA achieves the highest HOTA score among all transformer-\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Results on MOT17 test set with the private detections. Bold numbers indicate the overall best result and underlined numbers are the best transformer-based results.\n\nTracker\n\nFairMOT (Zhang et al., 2021b) Semi-TCL (Li et al., 2021) CSTrack (Liang et al., 2020) GRTU (Wang et al., 2021a) QDTrack (Pang et al., 2021) MAA (Stadler & Beyerer, 2022) ReMOT (Yang et al., 2021) PermaTr (Tokmakov et al., 2021) TransMOT (Chu et al., 2021) ByteTrack (Zhang et al., 2021a)\n\nTransCt (Xu et al., 2021) TransTrk (Sun et al., 2020) MOTR (Zeng et al., 2021) TrackFormer (Meinhardt et al., 2021) GTR (Zhou et al., 2022) MeMOT (Cai et al., 2022) HiPWA (Ours)\n\nHOTA↑ AssA↑ MOTA↑ IDF1↑\n\nFP(104\n\n)↓ FN(104\n\n)↓ IDs↓\n\n59.3 59.8 59.3 62.0 53.9 62.0 59.7 55.5 61.7 63.1\n\n54.5 54.1 57.2 -\n59.1 56.9 60.8\n\n58.0 59.4 57.9 62.1 52.7 60.2 57.1 53.1 59.9 62.0\n\n73.7 73.3 74.9 74.9 68.7 79.4 77.0 73.8 76.7 80.3\n\n72.3 73.2 72.6 75.0 66.3 75.9 72.0 68.9 75.1 77.3\n\nTransformer-based Methods 62.2 63.5 68.4 63.9 75.1 69.0 75.7\n\n73.2 75.2 71.9 65.0 75.3 72.5 75.4\n\n49.7 47.9 55.8 -\n57.0 55.2 60.7\n\n2.75 2.29 2.38 3.20 2.66 3.73 3.32 2.90 3.62 2.55\n\n2.31 5.02 2.11 7.44 2.68 3,72 2,45\n\n11.7 12.5 11.4 10.8 14.7 7.77 9.36 11.5 9.32 8.37\n\n12.4 8.64 13.6 12.4 10.9 11.5 10,8\n\n3,303 2,790 3,567 1,812 3,378 1,452 2,853 3,699 2,346 2,196\n\n4,614 3,603 2,115 3,528 2,859 2,724 2,879\n\nFrag↓\n\n8,073 8,010 7,668 1,824 8,091 2,202 5,304 6,132 7,719 2,277\n\n9,519 4,872 3,897 -\n- -\n3,029\n\nTable 2: Results on MOT20 test set with the private detections. Bold numbers indicate the overall best result and underlined numbers are the best transformer-based results.\n\nTracker\n\nHOTA↑\n\nAssA↑\n\nMOTA↑\n\nIDF1↑\n\nFP(104\n\n)↓\n\nFN(104\n\n)↓\n\nFairMOT (Zhang et al., 2021b) CSTrack (Liang et al., 2020) GSDT (Wang et al., 2021b) RelationT (Yu et al., 2021) MAA (Stadler & Beyerer, 2022) ByteTrack (Zhang et al., 2021a) OC-SORT (Cao et al., 2022)\n\nTransCt (Xu et al., 2021) TransTrk (Sun et al., 2020) MeMOT (Cai et al., 2022) HiPWA (Ours)\n\n54.6 54.0 53.6 56.5 57.3 61.3 62.1\n\n43.5 48.5 54.1 53.0\n\n54.7 54.0 52.7 55.8 55.1 59.6 62.0\n\n61.8 66.6 67.1 67.2 73.9 77.8 75.5\n\n67.3 68.6 67.5 70.5 71.2 75.2 75.9\n\nTransformer-based Methods 37.0 45.2 55.0 51.1\n\n58.5 65.0 63.7 65.8\n\n49.6 59.4 66.1 64.4\n\n10.3 2.54 3.19 6.11 2.49 2.62 1.80\n\n6.42 2.72 4,79 3.64\n\n8.89 14.4 13.5 10.5 10.9 8.76 10.8\n\n14.6 15.0 13.8 13.7\n\nIDs↓\n\n5,243 3,196 3,131 4,243 1,331 1,223 913\n\n4,695 3,608 1,938 3,948\n\nbased methods. But our method only achieves a comparable MOTA score with TransTrack (Sun et al., 2020) and GTR (Zhou et al., 2022), suggesting the superior part of HiPWA does not lie in the detection stage. The higher AssA score of our method also demonstrates its superior association performance.\n\nMOT20 is a challenging dataset by containing scenes of crowded pedestrian flows. We report the results on the MOT20-test set in Table 2. Though HiPWA shows better performance than MeMOT (Cai et al., 2022) on MOT17, its performance is inferior on MOT20. This is probably related to the heavy and frequent occlusion on MOT20. It is common on MOT20 that a large portion of pedestrians’ bodies is occluded for a long time. If the occlusion period is longer than the horizon of associating existing trajectories and new-coming detections, HiPWA will be likely to fail. On the other hand, the much longer temporal buffer of object appearance history maintained by MeMOT turns out more effective in such scenarios. However, we note that we design HiPWA with the main goal of demonstrating the hierarchical part-whole representation and choosing the most naive implementation for association heads to make it a computationally efficient baseline model. In the contrast, MeMOT requires 8×A100 GPUs for training to support the long-time history context buffering (22 frames v.s. 8 frames by HiPWA ) uses COCO (Lin et al., 2014) dataset as the additional pretraining data.\n\nNext, we come to the benchmark on the DanceTrack dataset in Table 3. HiPWA achieves comparable performance with the best transformer-based methods. The association of HiPWA is inferior to MOTR (Zeng et al., 2021). MOTR has carefully designed global association and optimization modules. The global collective loss and query interaction module to propagate information frame by frame proposed by MOTR show good effectiveness. However, as a side-effect, its training and inference speed is much slower due to the heavy architecture. For example, training on MOT17 takes MOTR 2.5 days for MOTR on 8×V100 GPUs while only 4 hours on 4×V100 GPUs for our proposed method. And the inference speed is 6.3FPS for MOTR while 17.2FPS for our method on the same machine (V100 GPU). Compared to the close baseline GTR (Zhou et al., 2022), HiPWA\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The upper line shows the results by HiPWA on three randomly sampled frames of a video in the DanceTrack-test set. The video is challenging to show camera motion, heavy occlusion, nonlinear motion, and the crossover among targets at the same time. The bottom line shows results on a MOT20-test video where the pedestrians are in the crowd and heavily occluded.\n\nachieves a more significant gap of outperforming on DanceTrack. Such an observation suggests our proposed part-whole hierarchical representation can be more powerful when the occlusion is heavy.\n\nGiven the results shown on the three benchmarks, we have demonstrated the effectiveness of our proposed HiPWA to be comparable to the state-of-the-art transformer-based multi-object tracking algorithms with a lightweight design. It builds a new baseline for future research in this line of works. The commonly adopted techniques of query propagation and iteration (Meinhardt et al., 2021; Sun et al., 2020; Zeng et al., 2021), deformable attention (Sun et al., 2020; Cai et al., 2022) and long-time feature buffering (Cai et al., 2022) are all compatible to be integrated with HiPWA .\n\nTable 3: Results on DanceTrack test set. Bold numbers indicate the overall best result and underlined numbers are the best transformer-based results.\n\nTracker\n\nHOTA↑\n\nDetA↑\n\nAssA↑\n\nMOTA↑\n\nIDF1↑\n\nCenterTrack (Zhou et al., 2020) FairMOT (Zhang et al., 2021b) QDTrack (Pang et al., 2021) TraDes (Wu et al., 2021) ByteTrack (Zhang et al., 2021a) OC-SORT (Cao et al., 2022)\n\n41.8 39.7 45.7 43.3 47.3 55.7\n\n78.1 66.7 72.1 74.5 71.6 81.7\n\nTransformer-based Methods\n\nTransTrk(Sun et al., 2020) MOTR (Zeng et al., 2021) GTR (Zhou et al., 2022) HiPWA (Ours)\n\n45.5 54.2 48.0 52.1\n\n75.9 73.5 72.5 76.3\n\n22.6 23.8 29.2 25.4 31.4 38.3\n\n27.5 40.2 31.9 35.8\n\n86.8 82.2 83.0 86.2 89.5 92.0\n\n88.4 79.7 84.7 86.1\n\n35.7 40.8 44.8 41.2 52.5 54.6\n\n45.2 51.5 50.3 52.7\n\n4.3 ABLATION STUDY\n\nThough we provide the results on multiple benchmarks to show the efficiency and effectiveness of our proposed method, there are many variables in the design. We now ablate their contributions to the overall performance of HiPWA . Many previous works in the multi-object tracking community follow the practice of CenterTrack (Zhou et al., 2020) on MOT17 (Milan et al., 2016) to use the latter half of training video sequences as the validation set. However, this makes the ablation study on the validation set not always convincing because the data distribution of the training set and validation set is too close and the performance gap reflected on the validation set might shrink or even disappear on the test set. Therefore, we turn to DanceTrack (Sun et al., 2021) for the ablation study instead where an independent validation set is provided and of a much larger scale than previous MOT datasets.\n\nIn Table 4 and Table 5, we study the influence of video clip length in the training and inference stage respectively. The result suggests that training the association model with longer video clips\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Results of using different length of video clip during training.\n\nTable 5: Results of using different length of video clip during Inference.\n\nT\n\n6 8\n10 12\n\nHOTA↑\n\nDetA↑\n\nAssA↑ MOTA↑\n\nIDF1↑\n\n47.8 48.1 48.7 49.2\n\n70.0 70.2 70.0 71.1\n\n32.8 33.2 34.0 34.1\n\n81.1 80.6 80.3 82.6\n\n49.7 50.3 51.7 52.0\n\nT\n\n8 16 24 32\n\nHOTA↑\n\nDetA↑\n\nAssA↑ MOTA↑\n\nIDF1↑\n\n47.5 47.9 48.1 47.8\n\n69.8 70.1 70.2 70.1\n\n32.5 32.9 33.2 32.8\n\n80.1 81.4 80.6 81.2\n\n50.3 50.6 50.3 49.8\n\nTable 6: Results on DanceTrack validation set to study the contribution from each level in our hierarchical representations to the association performance.\n\nHOTA↑\n\nDetA↑\n\nAssA↑\n\nMOTA↑\n\nIDF1↑\n\nBody Body + Part Body + Union Body + Part + Union\n\n45.7 46.3 47.3 48.1\n\n69.5 69.5 70.1 70.2\n\n30.3 30.7 32.0 33.2\n\n81.6 80.0 81.2 80.6\n\n48.1 48.1 49.8 50.3\n\nTable 7: Results on DanceTrack validation set with different configurations for multiple training device choices. HiPWA shows good performance even given limited computation resources.\n\nTraining Device\n\nTrain len\n\nImage Size\n\nHOTA↑\n\nDetA↑\n\nAssA↑\n\nMOTA↑\n\nIDF1↑\n\n1x RTX 3090-24GB 1x V100-32GB 4x V100-32GB\n\n6 8\n8\n\n1280 × 1280 1560 × 1560 1280 × 1280\n\n47.8 48.0 48.1\n\n70.0 70.8 70.2\n\n32.8 32.6 33.2\n\n81.1 82.4 80.6\n\n49.7 50.1 50.3\n\ncan continuously improve performance. Limited by the GPU memory, we cannot increase the video clip length to longer than 12 frames here. On the contrary, during the inference stage, increasing the sliding window size does not significantly influence the tracking performance.\n\nThe hierarchical part-whole representation is the the main contribution of our proposed method. Considering that the hierarchical representation gathers information from three levels (Part, Body, Union), we study the contribution of each of them in Table 6. Compared to only using the features extracted from the bounding box (body) area, our hierarchical representation achieves a performance improvement of 2.4 points of HOTA and 2.9 points of AssA. On the challenging DanceTrack dataset, such improvement can be considered significant when they share the same detections. Also, integrating the features of the union area shows better effectiveness than solely integrating the features of body parts. This is probably because the cross attention between object body and union areas can provide critical information to compare object targets with their neighboring objects, which can prevent potential false association among them. On the other hand, the information about body parts is already contained in the object’s body features. By concatenating the part features and body features, we can’t introduce previously missing information pieces significantly.\n\nFinally, as we aim to build a baseline model for future research in this area, we hope the proposed method is more accessible and computationally economic. We try different parameter configurations in Table 7. Even with only a single RTX 3090 GPU for training and inference, its performance is still quite close to our default setting which requires 4 × V100 GPUs. We hope this makes the notorious computation barrier of transformer-based methods not that terrible anymore.\n\n5 CONCLUSION\n\nIn this paper, we propose to build discriminative hierarchical part-whole representations as the visual descriptor for objects in multi-object tracking. The representation is built upon only bounding box annotation and in three levels: Part, Body, and Union. They are designed to provide visual specificity of the object from the compositional, semantic, and contextual perspectives respectively. We further propose to use attention in the transformer to gather and process the visual features. The combination of these two aspects makes our method, namely Hierarchical Part-Whole Attention and HiPWA for short. The results on multiple datasets demonstrate its efficiency and effectiveness. We hope the study of this paper can provide new knowledge in the visual representation of objects and an advanced baseline model to solve multi-object tracking problems.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt Schiele. Posetrack: A benchmark for human pose estimation and tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5167–5176, 2018. 2\n\nKeni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear\n\nmot metrics. EURASIP Journal on Image and Video Processing, 2008:1–10, 2008. 6\n\nJiarui Cai, Mingze Xu, Wei Li, Yuanjun Xiong, Wei Xia, Zhuowen Tu, and Stefano Soatto. Memot: Multi-object tracking with memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8090–8100, 2022. 2, 4, 6, 7, 8\n\nJinkun Cao, Xinshuo Weng, Rawal Khirodkar, Jiangmiao Pang, and Kris Kitani. Observation-centric sort: Rethinking sort for robust multi-object tracking. arXiv preprint arXiv:2203.14360, 2022. 7, 8\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pp. 213–229. Springer, 2020. 2, 5\n\nPeng Chu, Jiang Wang, Quanzeng You, Haibin Ling, and Zicheng Liu. Transmot: Spatial-temporal\n\ngraph transformer for multiple object tracking. arXiv preprint arXiv:2104.00194, 2021. 7\n\nPatrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taix ́e. Mot20: A benchmark for multi object tracking in crowded scenes. arXiv preprint arXiv:2003.09003, 2020. 6\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 4\n\nJerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analy-\n\nsis. Cognition, 28(1-2):3–71, 1988. 2\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. 6\n\nWei Li, Yuanjun Xiong, Shuo Yang, Mingze Xu, Yongxin Wang, and Wei Xia. Semi-tcl: Semisupervised track contrastive representation learning. arXiv preprint arXiv:2107.02396, 2021. 7\n\nChao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li, Xiyong Ye, and Jianxiao Zou. Rethinking the competition between detection and reid in multi-object tracking. arXiv preprint arXiv:2010.12138, 2020. 7\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In European\n\nDoll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. conference on computer vision, pp. 740–755. Springer, 2014. 7\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017. 6\n\nJonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taix ́e, and Bastian Leibe. Hota: A higher order metric for evaluating multi-object tracking. International journal of computer vision, 129(2):548–578, 2021. 6\n\nDavid Marr. Vision: A computational investigation into the human representation and processing of\n\nvisual information. MIT press, 2010. 2\n\nTim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer:\n\nMulti-object tracking with transformers. arXiv preprint arXiv:2101.02702, 2021. 2, 4, 7, 8\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAnton Milan, Laura Leal-Taix ́e, Ian Reid, Stefan Roth, and Konrad Schindler. Mot16: A benchmark\n\nfor multi-object tracking. arXiv preprint arXiv:1603.00831, 2016. 6, 8\n\nJiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasidense similarity learning for multiple object tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 164–173, 2021. 7, 8\n\nShuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, and Jian Sun. Crowdhuman: A benchmark for detecting human in a crowd. arXiv preprint arXiv:1805.00123, 2018. 2, 6\n\nDaniel Stadler and J ̈urgen Beyerer. Modelling ambiguous assignments for multi-person tracking in crowds. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 133–142, 2022. 7\n\nPeize Sun, Jinkun Cao, Yi Jiang, Rufeng Zhang, Enze Xie, Zehuan Yuan, Changhu Wang, and Ping Luo. Transtrack: Multiple object tracking with transformer. arXiv preprint arXiv:2012.15460, 2020. 2, 4, 6, 7, 8\n\nPeize Sun, Jinkun Cao, Yi Jiang, Zehuan Yuan, Song Bai, Kris Kitani, and Ping Luo. DancearXiv preprint\n\ntracking in uniform appearance and diverse motion.\n\ntrack: Multi-object arXiv:2111.14690, 2021. 6, 8\n\nRamana Sundararaman, Cedric De Almeida Braga, Eric Marchand, and Julien Pettre. Tracking In Proceedings of the IEEE/CVF Conference on Computer\n\npedestrian heads in dense crowd. Vision and Pattern Recognition, pp. 3865–3875, 2021. 2\n\nMingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10781–10790, 2020. 6\n\nPavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon. Learning to track with object permanence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10860–10869, 2021. 7\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2\n\nPaul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, and Bastian Leibe. Mots: Multi-object tracking and segmentation. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pp. 7942–7951, 2019. 2\n\nShuai Wang, Hao Sheng, Yang Zhang, Yubin Wu, and Zhang Xiong. A general recurrent tracking framework without real data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13219–13228, 2021a. 7\n\nYongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 13708–13715. IEEE, 2021b. 7\n\nYuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8741–8750, 2021c. 2, 3\n\nMark Weber, Jun Xie, Maxwell Collins, Yukun Zhu, Paul Voigtlaender, Hartwig Adam, Bradley Green, Andreas Geiger, Bastian Leibe, Daniel Cremers, et al. Step: Segmenting and tracking every pixel. arXiv preprint arXiv:2102.11859, 2021. 2\n\nJialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and Junsong Yuan. Track to detect In Proceedings of the IEEE/CVF conference on\n\nand segment: An online multi-object tracker. computer vision and pattern recognition, pp. 12352–12361, 2021. 8\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, and Cewu Lu. Pose flow: Efficient online\n\npose tracking. arXiv preprint arXiv:1802.00977, 2018. 2\n\nYihong Xu, Yutong Ban, Guillaume Delorme, Chuang Gan, Daniela Rus, and Xavier AlamedaPineda. Transcenter: Transformers with dense queries for multiple-object tracking. arXiv preprint arXiv:2103.15145, 2021. 7\n\nFan Yang, Xin Chang, Sakriani Sakti, Yang Wu, and Satoshi Nakamura. Remot: A model-agnostic\n\nrefinement for multiple object tracking. Image and Vision Computing, 106:104091, 2021. 7\n\nEn Yu, Zhuoling Li, Shoudong Han, and Hongwei Wang. Relationtrack: Relation-aware multiple\n\nobject tracking with decoupled representation. arXiv preprint arXiv:2105.04322, 2021. 7\n\nFangao Zeng, Bin Dong, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. arXiv preprint arXiv:2105.03247, 2021. 2, 4, 6, 7, 8\n\nYifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. arXiv preprint arXiv:2110.06864, 2021a. 6, 7, 8\n\nYifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. International Journal of Computer Vision, 129(11):3069–3087, 2021b. 7, 8\n\nXingyi Zhou, Dequan Wang, and Philipp Kr ̈ahenb ̈uhl. Objects as points.\n\narXiv preprint\n\narXiv:1904.07850, 2019. 6\n\nXingyi Zhou, Vladlen Koltun, and Philipp Kr ̈ahenb ̈uhl. Tracking objects as points. In European\n\nConference on Computer Vision, pp. 474–490. Springer, 2020. 6, 8\n\nXingyi Zhou, Tianwei Yin, Vladlen Koltun, and Philipp Kr ̈ahenb ̈uhl. Global tracking transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8771–8780, 2022. 2, 3, 5, 6, 7, 8\n\n12",
    "reference": "# Summary Of The Paper\n\nThis paper proposes an interesting 'Hierarchical Part-Whole Attention' for multi-object tracking. The proposed module is integrated with transformer network and achieves good performance (comparable or even better results than SOTA mot trackers). The overall training efficiency is also good, i.e., 4 hours on 4*v100 GPUs, while other Transformer based trackers may need days. This paper is well-written and organized, and I believe it will be a good baseline for future works to compare and in-depth development on this framework.\n\n# Strength And Weaknesses\n\nstrength: \n1. the idea of Hierarchical Part-Whole representation of target object in MOT seems interesting; \n2. the combination of Hierarchical Part-Whole Attention and Transformer works well on existing benchmark datasets; \n3. the paper is easy to follow and understand. \n\nweakness: \n1. the source code will be released or not is unclear. it is an interesting idea, but the implementation is also complicated. If the code is not available, maybe it is hard for other researchers to follow. \n2. the running efficiency is not real-time. i.e., not fast enough for practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nthis paper is clearly written and the information is enough to reproduce the experiments.\n\n# Summary Of The Review\n\nthe idea is reasonable, \nthe framework is not very complicated for MOT. \nthe training time is acceptable, but the inference is fast; \nthis paper is well-written and easy to follow.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nMOAT: ALTERNATING MOBILE CONVOLUTION AND ATTENTION BRINGS STRONG VISION MODELS\n\nChenglin Yang1∗, Siyuan Qiao2, Qihang Yu1, Xiaoding Yuan1, Yukun Zhu2, Alan Yuille1, Hartwig Adam2, Liang-Chieh Chen2 1The Johns Hopkins University 2Google Research\n\nABSTRACT\n\nThis paper presents MOAT, a family of neural networks that build on top of MObile convolution (i.e., inverted residual blocks) and ATtention. Unlike the current works that stack separate mobile convolution and transformer blocks, we effectively merge them into a MOAT block. Starting with a standard Transformer block, we replace its multi-layer perceptron with a mobile convolution block, and further reorder it before the self-attention operation. The mobile convolution block not only enhances the network representation capacity, but also produces better downsampled features. Our conceptually simple MOAT networks are surprisingly effective, achieving 89.1% / 81.5% top-1 accuracy on ImageNet-1K / ImageNet-1K-V2 with ImageNet22K pretraining. Additionally, MOAT can be seamlessly applied to downstream tasks that require large resolution inputs by simply converting the global attention to window attention. Thanks to the mobile convolution that effectively exchanges local information between pixels (and thus cross-windows), MOAT does not need the extra window-shifting mechanism. As a result, on COCO object detection, MOAT achieves 59.2% APbox with 227M model parameters (single-scale inference, and hard NMS), and on ADE20K semantic segmentation, MOAT attains 57.6% mIoU with 496M model parameters (single-scale inference). Finally, the tinyMOAT family, obtained by simply reducing the channel sizes, also surprisingly outperforms several mobile-specific transformer-based models on ImageNet. The tiny-MOAT family is also benchmarked on downstream tasks, serving as a baseline for the community. We hope our simple yet effective MOAT will inspire more seamless integration of convolution and self-attention. Code is publicly available.1\n\n1\n\nINTRODUCTION\n\nThe vision community has witnessed the prevalence of self-attention (Bahdanau et al., 2015) and Transformers (Vaswani et al., 2017). The success of Transformers in natural language processing motivates the creation of their variants for vision recognition. The Vision Transformer (ViT) (Dosovitskiy et al., 2021) has great representation capacity with global receptive field. However, it requires pretraining on a large-scale proprietary dataset (Sun et al., 2017). Its unsatisfying performance, when trained with a small number of images, calls for the need of better training recipes (Touvron et al., 2021a; Steiner et al., 2021) or architectural designs (Liu et al., 2021; Graham et al., 2021). On the other hand, ConvNet has been the dominant network choice since the advent of AlexNet (Krizhevsky et al., 2012) in 2012. Vision researchers have condensed the years of network design experience into multiple principles, and have started to incorporate them to vision transformers. For example, there are some works adopting the ConvNet’s hierarchical structure to extract multi-scale features for vision transformers (Liu et al., 2021; Fan et al., 2021; Wang et al., 2022), and others proposing to integrate the translation equivariance of convolution into transformers (Graham et al., 2021; d’Ascoli et al., 2021; Xiao et al., 2021).\n\nAlong the same direction of combining the best from Transformers and ConvNets, CoAtNet (Dai et al., 2021) and MobileViT (Mehta & Rastegari, 2022a) demonstrate outstanding performance by\n\n∗Work done while an intern at Google. 1Official code in TensorFlow: https://github.com/google-research/deeplab2\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nstacking Mobile Convolution (MBConv) blocks (i.e., inverted residual blocks (Sandler et al., 2018)) and Transformer blocks (i.e., a self-attention layer and a Multi-Layer Perceptron (MLP)). However, both works focus on the macro-level network design. They consider MBConv and Transformer blocks as individual separate ones, and systematically study the effect of stacking them to strike a better balance between the remarkable efficiency of MBConv and strong capacity of Transformer.\n\nIn this work, on the contrary, we study the micro-level building block design by taking a deeper look at the combination of MBConv and Transformer blocks. We make two key observations after a careful examination of those blocks. First, the MLP module in Transformer block is similar to MBConv, as both adopt the inverted bottleneck design. However, MBConv is a more powerful operation by employing one extra 3 × 3 depthwise convolution (to encode local interaction between pixels), and more activation (Hendrycks & Gimpel, 2016) and normalization (Ioffe & Szegedy, 2015) are employed between convolutions. Second, to extract multi-scale features using Transformer blocks, one may apply the average-pooling (with stride 2) to input features before the self-attention layer. However, the pooling operation reduces the representation capacity of self-attention. Our observations motivate us to propose a novel MObile convolution with ATtention (MOAT) block, which efficiently combines MBConv and Transformer blocks. The proposed MOAT block modifies the Transformer block by first replacing its MLP with a MBConv block, and then reversing the order of attention and MBConv. The replacement of MLP with MBConv brings more representation capacity to the network, and reversing the order (MBConv comes before self-attention) delegates the downsampling duty to the strided depthwise convolution within the MBConv, learning a better downsampling kernel.\n\nWe further develop a family of MOAT models by stacking and increasing the channels of network blocks. Surprisingly, our extremely simple design results in a remarkable impact. On the challenging ImageNet-1K classification benchmark (Russakovsky et al., 2015), our model (190M parameters) achieves 86.7% top-1 accuracy without extra data. When further pretraining on ImageNet-22K, our best model (483M parameters) attains 89.1% / 81.5% top-1 accuracy on ImageNet-1K (Tab. 2) / ImageNet-1K-V2 (Tab. 9), setting a new state-of-the-art.\n\nAdditionally, MOAT can be seamlessly deployed to downstream tasks that require large resolution inputs by simply converting the global attention to non-overlapping local window attention. Thanks to the MBConv that effectively exchanges local information between pixels (enabling cross-window propagation), MOAT does not need the extra window-shifting mechanism (Liu et al., 2021). As a result, on COCO object detection (Lin et al., 2014) and ADE20K semantic segmentation (Zhou et al., 2019), MOAT shows superior performances. Specifically, on COCO object detection (Tab. 3), our best model (227M parameters), achieves 59.2% APbox with single-scale inference and hard NMS, setting a new state-of-the-art in the regime of model size 200M with Cascade Mask R-CNN (Cai & Vasconcelos, 2018; He et al., 2017). On ADE20K semantic segmentation (Tab. 4), our best model (496M parameters), adopting DeepLabv3+ (Chen et al., 2018), attains 57.6% mIoU with single-scale inference, also setting a new state-of-the-art in the regime of models using input size 641 × 641.\n\nFinally, to explore the scalability of MOAT models, we simply scale down the models by reducing the channel sizes (without any other change), resulting in the tiny-MOAT family, which also surprisingly outperforms mobile-specific transformer-based models, such as Mobile-Former (Chen et al., 2022c) and MobileViTs (Mehta & Rastegari, 2022a;b). Specifically, in the regime of model parameters 5M, 10M, and 20M, our tiny MOAT outperforms the concurrent MobileViTv2 (Mehta & Rastegari, 2022b) by 1.1%, 1.3%, and 2.0% top-1 accuracy on ImageNet-1K classification benchmark (Tab. 5). Furthermore, we benchmark tiny-MOAT on COCO object detection and ADE20K semantic segmentation.\n\nIn summary, our method advocates the design principle of simplicity. Without inventing extra complicated operations, the proposed MOAT block effectively merges the strengths of both mobile convolution and self-attention into one block by a careful redesign. Despite its conceptual simplicity, impressive results have been obtained on multiple core vision recognition tasks. We hope our study will inspire future research on seamless integration of convolution and self-attention.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Block comparison. (a) The MBConv block (Sandler et al., 2018) employs the inverted bottleneck design with depthwise convolution and squeeze-and-excitation (Hu et al., 2018) applied to the expanded features. (b) The Transformer block (Vaswani et al., 2017) consists of a self-attention module and a MLP module. (c) The proposed MOAT block effectively combines them. The illustration assumes the input tensor has channels c.\n\n2 METHOD\n\nHerein, we review the Mobile Convolution (MBConv) (Sandler et al., 2018) and Transformer (Vaswani et al., 2017) blocks before introducing the proposed MOAT block. We then present MOAT, a family of neural networks, targeting at different trade-offs between accuracy and model complexity.\n\n2.1 MOBILE CONVOLUTION AND TRANSFORMER BLOCKS\n\nMBConv block. Also known as the inverted residual block, the Mobile Convolution (MBConv) (Sandler et al., 2018) block (Fig. 1 (a)) is an effective building block that has been widely used in mobile models (Howard et al., 2019; Mehta & Rastegari, 2022a) or efficient models (Tan & Le, 2019; Dai et al., 2021). Unlike the bottleneck block in ResNet (He et al., 2016a), the MBConv block employs the design of an “inverted bottleneck”, together with the efficient depthwise convolution (Howard et al., 2017). Specifically, a 1 × 1 convolution is first applied to expand the input channels by a factor of 4. Then, a 3 × 3 depthwise convolution is used to effectively capture the local spatial interactions between pixels. Finally, the features are projected back to the original channel size via a 1 × 1 convolution, enabling a residual connection (He et al., 2016a). An optional Squeeze-and-Excitation (SE) (Hu et al., 2018) module (which uses the global information to re-weight the channel activation) may also be used after the depthwise convolution, following MobileNetV3 (Howard et al., 2019). Note that one could tune the channel expansion ratio and depthwise convolution kernel size for better performance. We fix them throughout the experiments for simplicity. Formally, given an input tensor x ∈ RH×W ×C (H, W, C are its height, width, and channels), the MBConv block is represented as follows:\n\nMBConv(x) = x + (N2 ◦ S ◦ D ◦ N1)(BN(x)),\n\nN1(x) = GeLU(BN(Conv(x))),\n\nD(x) = GeLU(BN(DepthConv(x))), S(x) = σ(MLP(GAP(x)) · x,\n\nN2(x) = Conv(x),\n\n(1)\n\n(2) (3)\n\n(4)\n\n(5)\n\nwhere BN, GeLU, GAP, and MLP stand for Batch Normalization (Ioffe & Szegedy, 2015), Gaussian error Linear Unit (Hendrycks & Gimpel, 2016), Global Average Pooling, and Multi-Layer Perceptron (with reduction ratio 4 and hard-swish (Ramachandran et al., 2017)), respectively. The MBConv block consists of four main functions: N1, D, S, and N2, which correspond to the 1 × 1 convolution for channel expansion (by 4×), 3 × 3 depthwise convolution, squeeze-and-excitation (Hu et al., 2018) (σ is the sigmoid function), and 1 × 1 convolution for channel projection (by 4×), respectively.\n\n3\n\n(b) Transformer blocklayer normself-attentiondepthwise conv 3x3global poolconv 1x1, c/4SE(a) MBConv blockconv 1x1, 4cbatch normbatch norm, gelubatch norm, geluh-swishconv 1x1, 4cσMLPlayer normconv 1x1, 4cgeluconv 1x1, cdepthwise conv 3x3conv 1x1, cconv 1x1, 4cbatch normbatch norm, gelubatch norm, gelulayer normself-attention(c) MOAT blockconv 1x1, c×+++++Published as a conference paper at ICLR 2023\n\nTransformer block. The Transformer (Vaswani et al., 2017) block (Fig. 1 (b)) is a powerful building block that effectively captures the global information via the data-dependent self-attention operation. It consists of two main operations: self-attention and MLP. The self-attention operation computes the attention map based on the pairwise similarity between every pair of pixels in the input tensor, thus enabling the model’s receptive field to encompass the entire spatial domain. Additionally, the attention map dynamically depends on the input, enlarging the model’s representation capacity (unlike the convolution kernels, which are data-independent). The MLP operation contains two 1 × 1 convolutions, where the first one expands the channels (by 4×), the second one shrinks back the channels, and GeLU non-linearity is used in-between. Formally, given an input tensor x ∈ RH×W ×C, the Transformer block is represented as follows:\n\nTransformer(x) = x + (M2 ◦ M1 ◦ Attn)(LN(x)), M1(x) = GeLU(Conv(LN(x))), M2(x) = Conv(x),\n\n(6) (7)\n\n(8)\n\nwhere LN and Attn denote the Layer Normalization (Ba et al., 2016), and self-attention (Vaswani et al., 2017). The self-attention operation also includes a residual connection (He et al., 2016a), which is not shown in the equations for simplicity, while the MLP operation is represented by two functions M1 and M2, which correspond to the 1 × 1 convolution for channel expansion (by 4×) and 1 × 1 convolution for channel projection, respectively.\n\n2.2 MOBILE CONVOLUTION WITH ATTENTION (MOAT) BLOCK\n\nComparing MBConv and Transformer blocks. Before getting into the architecture of our MOAT block, it is worthwhile to compare the MBConv (Sandler et al., 2018) and Transformer (Vaswani et al., 2017) blocks, which helps to understand our design motivations. Specifically, we make the following key observations.\n\nFirst, both MBConv and Transformer blocks advocate the “inverted bottleneck” design, where the channels of input tensors are expanded and then projected by 1 × 1 convolutions. However, MBConv additionally employs a 3 × 3 depthwise convolution between those two 1 × 1 convolutions, and there are both batch normalization and GeLU activation between the convolutions.\n\nSecond, to capture the global information, the MBConv block may employ a Squeeze-and-Excitation (SE) module, while the Transformer block adopts the self-attention operation. Note that the SE module squeezes the spatial information via a global average pooling, while the self-attention module maintains the tensor’s spatial resolution.\n\nThird, the downsampling operation is performed at different places within the block. To downsample the features, the standard MBConv block uses the strided depthwise convolution, while the Transformer block, deployed in the modern hybrid model CoAtNet (Dai et al., 2021), adopts an average-pooling operation before the self-attention.\n\nMOAT block. Given the above observations, we now attempt to design a new block that effectively merges the best from both MBConv and Transformer blocks. We begin with the powerful Transformer block, and gradually refine over it.\n\nBased on the first observation, both MBConv and Transformer blocks employ the “inverted bottleneck” design. Since depthwise convolution could effectively encode local interaction between pixels, which is crucial for modeling the translation equivariance in ConvNets, we thus start to add the depthwise convolution to Transformer’s MLP module. However, we did not observe any performance improvement until we also added the extra normalization and activations between convolutions.\n\nFor the second observation, we simply do not add the SE module to the MBConv block. The self-attention operation is kept to capture the global information.\n\nWe found the third observation critical. The downsampling operation (average-pooling) right before the self-attention operation in Transformer block slightly reduces its representation capacity. On the other hand, the MBConv block is well-designed for the downsampling operation with the strided depthwise convolution, which effectively learns the downsampling convolution kernel for each input channel. Therefore, we further reorder the “inverted bottleneck” (containing depthwise convolution) before the self-attention operation, delegating the downsampling operation to depthwise convolution.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nIn this way, we need no extra downsampling layer like average-pooling in CoAtNet (Dai et al., 2021), or patch-embedding layers in Swin (Liu et al., 2021) and ConvNeXt (Liu et al., 2022b). Finally, it results in our MObile convolution with ATtention (MOAT) block, as illustrated in Fig. 1 (c). Formally, given an input tensor x ∈ RH×W ×C, the MOAT block is represented as follows:\n\nMOAT(x) = x + (Attn ◦ N2 ◦ D ◦ N1)(BN(x)),\n\n(9)\n\nwhere MBConv (w/o SE) contains functions N1 (Eq. 2), D (Eq. 3), and N2 (Eq. 5), and Attn denotes the self-attention operation. The MOAT block then simply consists of MBConv (w/o SE) and the self-attention operation, successfully combining the best from the MBConv block and Transformer block into one (which we will show empirically).\n\n2.3 META ARCHITECTURE\n\nMacro-level network design. After developing the MOAT block, we then study how to effectively stack them to form our base model. We adopt the same strategy as the existing works (Liu et al., 2021; Wang et al., 2021b; Graham et al., 2021; Xiao et al., 2021; Dai et al., 2021; Mehta & Rastegari, 2022a). Specifically, we summarize several key findings from those works, and use them as design principles of our meta architecture.\n\n• Employing convolutions in the early stages improves the performance and training conver-\n\ngence of Transformer models (Wu et al., 2021; Graham et al., 2021; Xiao et al., 2021).\n\n• The Mobile Convolution (MBConv) (Sandler et al., 2018) blocks are also effective building blocks in the hybrid Conv-Transformer models (Dai et al., 2021; Mehta & Rastegari, 2022a).\n\n• Extracting multi-scale backbone features benefits the downstream tasks, such as detection and segmentation (Liu et al., 2021; Wang et al., 2021b; Fan et al., 2021; Heo et al., 2021).\n\nAs a result, our meta architecture consists of the convolutional stem, MBConv blocks, and MOAT blocks. Additionally, through the ablation study in the appendix, we found the layer layout proposed by CoAtNet-1 (Dai et al., 2021) effective. We thus follow their layer layout, resulting in our base model MOAT-1. To form the MOAT model family, we then scale down or up MOAT-1 in the dimensions of number of blocks and number of channels, as shown in Tab. 1. We only scale the number of blocks in the third and fourth stages (out of five stages). The downsampling operation is performed in the first block of each stage. Note that our base model MOAT-1 and CoAtNet-1 share the same layer layout and channel sizes. However, we take a different scaling strategy: our MOAT is scaled up (or down) by alternatively increasing the depth and expanding the width between variants.\n\nTable 1: MOAT variants differ in the number of blocks B and number of channels C in each stage.\n\nblock\n\nstride\n\nconv MBConv MBConv MOAT MOAT\n\n2 4\n8 16 32\n\nMOAT-0 MOAT-1 C\nB\n\nB\n\nC\n\nMOAT-2 C\nB\n\nMOAT-3 C\nB\n\n2 2\n3 7\n2\n\n64 96 192 384 768\n\n2 2\n6 14 2\n\n64 96 192 384 768\n\n2 2\n6 14 2\n\n128 128 256 512 1024\n\n2 2\n12 28 2\n\n160 160 320 640 1280\n\nMOAT-4 C\nB\n\n256 2\n256 2\n12 512 28 1024 2048 2\n\ntiny-MOAT-{0,1,2,3}\n\nB\n\n32 2\n32 2\n3 64 7 128 2 256\n\nC\n\n80 56 40 56 80 40 80 112 160 160 224 320 320 448 640\n\n3 EXPERIMENTAL RESULTS\n\nIn this section, we show that MOAT variants are effective on the ImageNet-1K (Russakovsky et al., 2015) image classification. We then deploy them to other recognition tasks, including COCO object detection (Lin et al., 2014), instance segmentation (Hariharan et al., 2014), and ADE20K (Zhou et al., 2019) semantic segmentation. MOAT can be seamlessly applied to downstream tasks. For small resolution inputs, we directly fine-tune the global attention, while for large resolution inputs, we simply convert the global attention to non-overlapping local window attention without using extra window-shifting mechanism. The detailed experiment setup could be found in the appendix.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Performance on ImageNet-1K. 1K only: Using ImageNet-1K only. 22K + 1K: ImageNet-22K pretraining and ImageNet-1K fine-tuning. Tab. 8 shows comparisions with more SOTA methods and Tab. 9 reports the performances on ImageNet-1K-V2.\n\nmodel\n\neval size\n\nparams\n\nFLOPs\n\nImageNet-1K top-1 accuracy\n\n1K only\n\n22K+1K\n\nConvNets\n\nViTs\n\nHybrid\n\nHybrid (ours)\n\nEfficientNetV2-L (Tan & Le, 2021) EfficientNetV2-XL (Tan & Le, 2021) ConvNeXt-T (Liu et al., 2022b) ConvNeXt-L (Liu et al., 2022b) ConvNeXt-XL (Liu et al., 2022b)\n\nPVT-Large (Wang et al., 2021b) Swin-T (Liu et al., 2021) Swin-L (Liu et al., 2021) SwinV2-L (Liu et al., 2021) MViTv2-H (Li et al., 2022)\n\nPVTv2-B5 (Wang et al., 2022) MaxViT-XL (Tu et al., 2022) CoAtNet-0 (Dai et al., 2021) CoAtNet-3 (Dai et al., 2021) CoAtNet-4 (Dai et al., 2021)\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-1 MOAT-2 MOAT-3\n\nMOAT-4\n\n4802 4802 2242 3842 3842\n\n2242 2242 3842 3842 5122\n\n2242 5122 2242 3842 5122\n\n2242 2242 2242 2242\n\n3842 3842 3842 3842\n\n5122 5122 5122\n\n5122\n\n53B 94B 4.5B\n\n120M 208M 29M 198M 101.0B 350M 179.0B\n\n9.8B 4.5B\n\n61.4M 28M 197M 103.9B 197M 115.4B 667M 763.5B\n\n82M 11.8B 475M 535.2B 25M 168M 107.4B 275M 360.9B\n\n4.2B\n\n5.7B 27.8M 9.1B 41.6M 73.4M 17.2B 190.0M 44.9B\n\n18.2B 27.8M 29.6B 41.6M 73.4M 54.3B 190.0M 141.2B\n\n41.6M 58.7B 73.4M 104.6B 190.0M 271.0B\n\n483.2M 648.5B\n\n85.7 -\n82.1 85.5 -\n\n81.7 81.3 -\n- -\n\n83.8 -\n81.6 85.8 -\n\n83.3 84.2 84.7 85.3\n\n84.6 85.9 86.2 86.5\n\n86.2 86.5 86.7\n\n-\n\n- 87.3 82.9 87.5 87.8\n\n- -\n87.3 87.7 88.8\n\n- 88.7 -\n87.6 88.6\n\n83.6 84.9 86.0 86.8\n\n85.7 87.0 87.5 88.2\n\n87.2 87.7 88.4\n\n89.1\n\nFigure 2: Parameters vs. accuracy using ImageNet1K only with input size 224.\n\nFigure 3: FLOPs vs. accuracy using ImageNet-1K only with input size 224.\n\nFigure 4: Parameters vs. accuracy using ImageNet22K and ImageNet-1K with input size 384.\n\nFigure 5: FLOPs vs. accuracy using ImageNet-22K and ImageNet-1K with input size 384.\n\n6\n\n818283848586050100150200Top-1 accuracyParameters (M) MOAT CoAtNet ConvNeXt SwinSSwin-BTConvNeXt-LS012CoAtNet-3102MOAT-3TB81828384858601020304050Top-1 accuracyFLOPs (B) MOAT CoAtNet ConvNeXt SwinSSwin-BTConvNeXt-LS012CoAtNet-3102MOAT-3TB83848586878889050100150200Top-1 accuracyParameters (M) MOAT CoAtNet ConvNeXt SwinLSwin-BBConvNeXt-TLS2CoAtNet-3012MOAT-383848586878889050100150Top-1 accuracyFLOPs (B) MOAT CoAtNet ConvNeXt SwinSwin-BLB2CoAtNet-3102MOAT-3SConvNeXt-TLPublished as a conference paper at ICLR 2023\n\nImageNet Image Classification. In Tab. 2, we include the current state-of-art methods in the categories of ConvNets, ViTs and Hybrid models. At similar model costs (parameters or FLOPs), our MOAT models consistently outperform all of them. Specifically, with the ImageNet-1K data only and input size 224, for light-weight models, our MOAT-0 significantly outperforms ConvNeXt-T (Liu et al., 2022b), Swin-T (Liu et al., 2022b), and CoAtNet-0 (Dai et al., 2021) by 1.2%, 2.0%, and 1.7%, respectively. For large-scale models using input size 384, MOAT-3 is able to surpass ConvNeXt-L, CoAtNet-3 by 1.0% and 0.7%, respectively. With the ImageNet-22K pretraining and input size 384, the prior arts ConvNeXt-L, Swin-L, and CoAtNet-3 already show strong performances (87.5%, 87.3% and 87.6%), while our MOAT-3 achieves the score of 88.2%, outperforming them by 0.7%, 0.9%, and 0.6%, respectively. For ImageNet-1K and input size 224, we plot the performances vs. parameters and FLOPs in Fig. 2 and Fig. 3, respectively. For ImageNet-22K pretraining and input size 384, we plot the performances vs. parameters and FLOPs in Fig. 4 and Fig. 5, respectively. In the figures, MOAT clearly demonstrates the best performance in all computation regimes. Finally, our largest model MOAT-4, with ImageNet-22K and input size 512, further attains 89.1% accuracy.\n\nCOCO Detection. Tab. 3 summarizes the COCO object detection (box) and instance segmentation (mask) results. Our MOAT backbones significantly outperform the baseline methods, including Swin (Liu et al., 2021) and ConvNeXt (Liu et al., 2022b) across different model sizes. Specifically, our MOAT-0 outperforms Swin-T and ConvNeXt-T by 5.4% and 5.5% APbox (3.7% and 3.7% APmask). Our MOAT-1 surpasses Swin-S and ConvNeXt-S by 5.9% and 5.8% APbox (4.3% and 4.0% APmask). Our MOAT-2, with 110M parameters, is still 5.5% and 4.5% APbox (3.5% and 2.4% APmask) better than Swin-B and ConvNeXt-B. Finally, our MOAT-3, using 227M parameters, achieves 59.2% APbox (50.3% APmask), setting a new state-of-the-art in the regime of model size 200M that is built on top of Cascade Mask R-CNN (Cai & Vasconcelos, 2018; He et al., 2017). More comparisons with smaller input size can be found in Tab. 12. For tiny-MOAT, tiny-MOAT-0/1 achieve the same performance as Swin-T/S and ConvNeXt-T/S but only use less than half of the parameters. Furthermore, tiny-MOAT-3 is pretrained with ImageNet-1K and attains 55.2 APbox with 57M parameters, surpassing the ImageNet-22k pretrained Swin-L (53.9 APbox with 254M parameters) and ConvNeXt-L (54.8 APbox with 255M parameters).\n\nTable 3: Object detection and instance segmentation on the COCO 2017 val set. We employ Cascade Mask-RCNN, and single-scale inference (hard NMS). †: use ImageNet-22K pretrained weights. When using tiny-MOAT series as backbones, most of the model parameters come from the decoder. More comparisons at input size 896 is reported in Tab. 12.\n\ninput size\n\nparams\n\nAPmask APmask\n\nbackbone\n\nSwin-T Swin-S Swin-B† Swin-L†\n\nConvNeXt-T ConvNeXt-S ConvNeXt-B† ConvNeXt-L†\n\ntiny-MOAT-0 tiny-MOAT-1 tiny-MOAT-2 tiny-MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2† MOAT-3†\n\n1280 × 800 1280 × 800 1280 × 800 1280 × 800\n\n1280 × 800 1280 × 800 1280 × 800 1280 × 800\n\n1344 × 1344 1344 × 1344 1344 × 1344 1344 × 1344\n\n1344 × 1344 1344 × 1344 1344 × 1344 1344 × 1344\n\nFLOPs APbox APbox 50 69.3 70.4 71.8 72.4\n\n50.5 51.8 53.0 53.9\n\n745B 86M 838B 107M 145M 982B 254M 1382B\n\n741B 86M 827B 108M 146M 964B 255M 1354B\n\n41M 42M 47M 57M\n\n612B 628B 669B 754B\n\n799B 65M 79M 921B 110M 1217B 227M 2216B\n\n50.4 51.9 54.0 54.8\n\n50.5 51.9 53.0 55.2\n\n55.9 57.7 58.5 59.2\n\n69.1 70.8 73.1 73.8\n\n69.3 71.6 72.2 74.8\n\n73.9 76.0 76.6 77.8\n\nAPbox 75 54.9 56.3 57.5 58.8\n\n54.8 56.5 58.8 59.8\n\n56.0 56.1 58.0 60.6\n\n60.9 63.4 64.3 64.9\n\n43.7 44.7 45.8 46.7\n\n43.7 45.0 46.9 47.6\n\n43.3 44.6 45.0 47.0\n\n47.4 49.0 49.3 50.3\n\n50 66.6 67.9 69.4 70.1\n\n66.5 68.4 70.6 71.3\n\n66.6 68.4 69.4 71.8\n\n70.9 73.4 73.9 74.8\n\nAPmask 75 47.1 48.5 49.7 50.8\n\n47.3 49.1 51.3 51.7\n\n47.3 48.3 48.8 51.2\n\n52.1 53.2 53.9 55.5\n\nADE20K Semantic Segmentation. In Tab. 4, when using input size 5132, MOAT consistently outperforms the ConvNeXt counterparts. MOAT-0 surpasses ConvNeXt-T by 3.0% mIoU. Moreover, MOAT-2, with ImageNet-22k pretraining, surpasses ConvNeXt-B by 3.1%. The larger MOAT-3 and MOAT-4 further outperform ConvNeXt-L and ConvNeXt-XL by 4.9% and 5.4%, respectively. Finally, when using input size 6412, our MOAT-4 achieves the performance of 57.6% mIoU, setting a new state-of-the-art in the regime of models using input size 6412. For tiny-MOAT, tiny-MOAT-3 achieves comparable performance with ConvNeXt-S with less than half of the parameters.\n\ntiny-MOAT on ImageNet. We simply scale down the channels of MOAT-0 to obtain the tiny-MOAT family without any specific adaptions. In the left of Tab. 5, with the similar model parameters,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Semantic segmentation on ADE20K val set. We employ DeepLabv3+ (single-scale inference). Results for ConvNeXt and MOAT are obtained using the official code-base (Weber et al., 2021) with the same training recipe. †: use ImageNet-22K pretrained weights.\n\nbackbone\n\nConvNeXt-T ConvNeXt-S ConvNeXt-B† ConvNeXt-L† ConvNeXt-XL†\n\ninput 5132 5132 5132 5132 5132\n\nparams\n\nFLOPs mIoU (%)\n\n47.6B 34.2M 55.8M 70.8B 95.8M 119.5B 208.3M 256.4B 364.0M 446.2B\n\n45.8 47.8 50.5 51.0 51.8\n\nbackbone\n\ntiny-MOAT-0 tiny-MOAT-1 tiny-MOAT-2 tiny-MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2† MOAT-3† MOAT-4†\n\nMOAT-2† MOAT-3† MOAT-4†\n\ninput 5132 5132 5132 5132\n\n5132 5132 5132 5132 5132\n\n6412 6412 6412\n\nparams\n\nFLOPs\n\nmIoU (%)\n\n5.6M 7.8M 13.2M 24.2M\n\n11.8B 15.2B 23.8B 41.2B\n\n33.3M 61.3B 47.0M 85.4B 144.3B 80.5M 198.4M 331.5B 496.3M 779.9B\n\n80.5M 242.0B 198.4M 554.7B 496.3M 1273.5B\n\n41.2 43.1 44.9 47.5\n\n48.8 51.8 53.6 55.9 57.2\n\n54.7 56.5 57.6\n\ntiny-MOAT-0/1/2 surpass the Mobile-Former counterparts by 6.8%, 5.5%, and 4.3%, respectively. In the right of Tab. 5, our tiny-MOAT also shows stronger performances than MobileViT (Mehta & Rastegari, 2022a). Even compared with the concurrent work MobileViTv2 (Mehta & Rastegari, 2022b), tiny-MOAT-1/2/3 surpass their counterparts by 1.1%, 1.3%, and 2.1%, respectively.\n\nTable 5: Performances of tiny-MOAT family on ImageNet-1K.\n\ninput size 2242\n\nparams\n\nFLOPs\n\ntop-1 acc.\n\n3.5M Mobile-Former-52M Mobile-Former-96M 4.6M Mobile-Former-214M 9.4M\n\nMobile-Former-508M 14.0M\n\ntiny-MOAT-0 tiny-MOAT-1 tiny-MOAT-2\n\ntiny-MOAT-3\n\n3.4M 5.1M 9.8M\n\n19.5M\n\n0.05B 0.1B 0.2B\n\n0.5B\n\n0.8B 1.2B 2.3B\n\n4.5B\n\n68.7 72.8 76.7\n\n79.3\n\n75.5 78.3 81.0\n\n82.7\n\ninput size 2562\n\nMobileViT-XS MobileViT-S\n\nMobileViTv2-1.0 MobileViTv2-1.5 MobileViTv2-2.0\n\ntiny-MOAT-1 tiny-MOAT-2 tiny-MOAT-3\n\nparams\n\nFLOPs\n\ntop-1 acc.\n\n2.3M 5.6M\n\n4.9M 10.6M 18.5M\n\n5.1M 9.8M 19.5M\n\n0.7B 2.0B\n\n1.8B 4.0B 7.5B\n\n1.6B 3.0B 6.0B\n\n74.8 78.4\n\n78.1 80.4 81.2\n\n79.2 81.7 83.3\n\n4 ABLATION STUDIES ON IMAGENET\n\nAt micro level, we perform ablation studies on the MOAT block design and downsampling layer in the following and the order of MBConv and Attention in MOAT block in section A.6.1. At macro level, we perform ablation studies on the MOAT-based model and MOAT meta architecture in section A.6.2.\n\nMOAT block design. In Tab. 6, we ablate the MOAT block design, which only affects the last two stages of MOAT, and we keep everything else the same (e.g., training recipes). We start from the Transformer block, consisting of Attn (self-attention) and MLP, which already attains a strong top-1 accuracy (82.6%). Directly inserting a 3 × 3 depthwise convolution in the MLP degrades the performance by 0.9%. If we additionally insert batch normalization and GeLU between convolutions (i.e., replace MLP with MBConv, but no Squeeze-and-Excitation), the performance is improved to 82.9%. Finally, placing MBConv before Attn reaches the performance of 83.3%. Additionally, our MOAT block brings more improvements (from 1.2% up to 2.6% gains) in the tiny model regime.\n\nDownsampling layer. For the MOAT block design, we do not need the extra downsampling layer like (1) average-pooling in CoAtNet (Dai et al., 2021), (2) patch-embedding layer (i.e., 2 × 2 convolution with stride 2) in Swin (Liu et al., 2021) and ConvNeXt (Liu et al., 2022b), or (3) strided depthwise convolution in PiT (Heo et al., 2021) and RegionViT (Chen et al., 2022a). As shown in Tab. 7, using patch-embedding layer indeed improves over the average-pooling scheme by 0.2% accuracy, but it takes more cost of model parameters. Additionally, using the strided depthwise convolution for downsampling leads to 0.2% worse performance than the patch-embedding layer. By contrast, our MOAT design (i.e., delegating the downsampling to the MBConv block) shows the best performance with the least cost of parameters and comparable FLOPs.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Ablation studies of MOAT block design on ImageNet-1K with input size 224.\n\nmodel\n\nblock composition\n\nparams\n\nFLOPs\n\ntop-1 acc.\n\nMOAT-0\n\nAttn + MLP Attn + MLP (w/ depth. conv) Attn + MBConv MBConv + Attn\n\n28.0M 28.2M 28.2M 27.8M\n\n5.4B 5.4B 5.4B 5.7B\n\n82.6 81.7 82.9 83.3\n\nmodel\n\nblock composition\n\nparams\n\nFLOPs\n\ntop-1 acc.\n\ntiny-MOAT-2\n\ntiny-MOAT-1\n\ntiny-MOAT-0\n\nAttn + MLP MBConv + Attn\n\nAttn + MLP MBConv + Attn\n\nAttn + MLP MBConv + Attn\n\n9.8M 9.8M\n\n5.1M 5.1M\n\n3.3M 3.4M\n\n2.2B 2.3B\n\n1.1B 1.2B\n\n0.8B 0.8B\n\n79.8 81.0\n\n76.2 78.3\n\n72.9 75.5\n\nTable 7: Ablation studies of the downsampling layer design on ImageNet-1K, using MOAT-0 and input size 224. We compare our MOAT design (in grey) with (1) CoAtNet (using average-pooling for downsampling), (2) Swin/ConvNeXt designs (using strided 2 × 2 convolution for downsampling), and (3) PiT/RegionViT designs (using strided 3 × 3 depthwise convolution for downsampling).\n\nblock composition\n\ndownsampling type\n\nparams (M)\n\nFLOPs (B)\n\ntop-1 acc.\n\nAveragePooling + Attn + MLP PatchEmbedding + Attn + MLP StridedDepthConv + Attn + MLP MBConv + Attn\n\nCoAtNet Swin, ConvNeXt PiT, RegionVit MOAT\n\n28.0 30.2 28.8 27.8\n\n5.4 5.6 5.5 5.7\n\n82.6 82.8 82.6 83.3\n\n5 RELATED WORK\n\nTransformers (Vaswani et al., 2017) were recently introduced to the vision community (Wang et al., 2018; Ramachandran et al., 2019; Hu et al., 2019) and demonstrated remarkable performance on vision recognition tasks (Carion et al., 2020; Zhu et al., 2021; Wang et al., 2021a; Arnab et al., 2021; Liu et al., 2021; Cheng et al., 2021; Yu et al., 2022a; Kim et al., 2022; Cheng et al., 2022; Yu et al., 2022b), thanks to their ability to efficiently encode long-range interaction via the attention mechanism (Bahdanau et al., 2015). Particularly, ViT (Dosovitskiy et al., 2021) obtains impressive results on ImageNet (Russakovsky et al., 2015) by applying the vanilla Transformer with the novel large stride patch embedding, after pretraining on the proprietary large-scale JFT dataset (Sun et al., 2017). There have been several works aiming to improve the vision transformers, either with better training strategies (Touvron et al., 2021a;b; Steiner et al., 2021; Zhai et al., 2022; Touvron et al., 2022) or with efficient local-attention modules (Huang et al., 2019; Ho et al., 2019; Wang et al., 2020; Liu et al., 2021; Chu et al., 2021; Yang et al., 2021; Yu et al., 2021; Dong et al., 2022; Tu et al., 2022).\n\nSince the debut of AlexNet (Krizhevsky et al., 2012), the vision community has witnessed a rapid improvement on the ImageNet benchmark using different types of ConvNets, including (but not limited to) VGGNet (Simonyan & Zisserman, 2015), Inceptions (Szegedy et al., 2015; Ioffe & Szegedy, 2015; Szegedy et al., 2016; 2017), ResNets (He et al., 2016a;b), ResNeXt (Xie et al., 2017), DenseNet (Huang et al., 2017), SENet (Hu et al., 2018), MobileNets (Howard et al., 2017; Sandler et al., 2018; Howard et al., 2019), EfficientNets (Tan & Le, 2019; 2021), and ConvNeXt (Liu et al., 2022b) each focusing on different aspects of accuracy and efficiency. The ubiquity of ConvNets in computer vision could be attributed to their built-in inductive biases.\n\nGiven the success of Transformers and ConvNets, another line of research is to explore how to effectively combine them. Swin (Liu et al., 2021; 2022a), PVT (Wang et al., 2021b; 2022), MViT (Fan et al., 2021; Li et al., 2022), and PiT (Heo et al., 2021) adopt the ConvNet hierarchical structure to extract multi-scale features for Transformers. SASA (Ramachandran et al., 2019), AA-ResNet (Bello et al., 2019), Axial-ResNet (Wang et al., 2020) and BoTNet (Srinivas et al., 2021) incorporate the attention modules to ResNets. CvT (Wu et al., 2021), LeViT (Graham et al., 2021), Visformer (Chen et al., 2021b), and ViTC (Xiao et al., 2021) replace ViT’s patch embedding with strided convolutions. CeiT (Yuan et al., 2021a) and CMT (Guo et al., 2022) incorporate depthwise convolution to the transformer block’s MLP. ViTAE (Xu et al., 2021) adopts parallel attention modules and convolutional layers. LVT (Yang et al., 2022) introduces local self-attention into the convolution. Recently, CoAtNet (Dai et al., 2021) and MobileViT (Mehta & Rastegari, 2022a) propose hybrid models that build on top of the efficient Mobile Convolution (Sandler et al., 2018) and Transformer block.\n\nAcknowledgements We thank Wen-Sheng Chu for the support and discussion. We gratefully acknowledge supports from the Office of Naval Research. N00014-21-1-2812.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nMart ́ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, 2016.\n\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci ́c, and Cordelia Schmid.\n\nVivit: A video vision transformer. In ICCV, 2021.\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv:1607.06450,\n\n2016.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. In ICLR, 2015.\n\nIrwan Bello. Lambdanetworks: Modeling long-range interactions without attention. In ICLR, 2021.\n\nIrwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented\n\nconvolutional networks. In ICCV, 2019.\n\nAndy Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image\n\nrecognition without normalization. In ICML, 2021.\n\nZhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In\n\nCVPR, 2018.\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\n\nZagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n\nChun-Fu Chen, Rameswar Panda, and Quanfu Fan. Regionvit: Regional-to-local attention for vision\n\ntransformers. In ICLR, 2022a.\n\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE TPAMI, 2017.\n\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoderdecoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.\n\nLiang-Chieh Chen, Huiyu Wang, and Siyuan Qiao. Scaling wide residual networks for panoptic\n\nsegmentation. arXiv:2011.11675, 2020.\n\nWuyang Chen, Xianzhi Du, Fan Yang, Lucas Beyer, Xiaohua Zhai, Tsung-Yi Lin, Huizhong Chen, Jing Li, Xiaodan Song, Zhangyang Wang, et al. A simple single-scale vision transformer for object localization and instance segmentation. arXiv preprint arXiv:2112.09747, 2021a.\n\nWuyang Chen, Xianzhi Du, Fan Yang, Lucas Beyer, Xiaohua Zhai, Tsung-Yi Lin, Huizhong Chen, Jing Li, Xiaodan Song, Zhangyang Wang, and Denny Zhou. A simple single-scale vision transformer for object localization and instance segmentation. In ECCV, 2022b.\n\nYinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng\n\nLiu. Mobile-former: Bridging mobilenet and transformer. In CVPR, 2022c.\n\nZhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and Qi Tian. Visformer: The\n\nvision-friendly transformer. In ICCV, 2021b.\n\nBowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation. In CVPR, 2020.\n\nBowen Cheng, Alexander G Schwing, and Alexander Kirillov. Per-pixel classification is not all you\n\nneed for semantic segmentation. In NeurIPS, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nBowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\n\nattention mask transformer for universal image segmentation. CVPR, 2022.\n\nFranc ̧ois Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, 2017.\n\nXiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In NeurIPS, 2021.\n\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\n\nLearning augmentation policies from data. In CVPR, 2019.\n\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated\n\ndata augmentation with a reduced search space. In CVPR, 2020.\n\nZihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and\n\nattention for all data sizes. In NeurIPS, 2021.\n\nXiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In CVPR, 2022.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n\nSt ́ephane d’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In ICML, 2021.\n\nHaoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\n\nChristoph Feichtenhofer. Multiscale vision transformers. In ICCV, 2021.\n\nBenjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv ́e J ́egou, and Matthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference. In ICCV, 2021.\n\nJianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. Cmt:\n\nConvolutional neural networks meet vision transformers. In CVPR, 2022.\n\nBharath Hariharan, Pablo Arbel ́aez, Ross Girshick, and Jitendra Malik. Simultaneous detection and\n\nsegmentation. In ECCV, 2014.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n\nrecognition. In CVPR, 2016a.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\n\nnetworks. In ECCV, 2016b.\n\nKaiming He, Georgia Gkioxari, Piotr Doll ́ar, and Ross Girshick. Mask r-cnn. In ICCV, 2017.\n\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv:1606.08415, 2016.\n\nByeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh.\n\nRethinking spatial dimensions of vision transformers. In ICCV, 2021.\n\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\n\nmensional transformers. arXiv:1912.12180, 2019.\n\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In ICCV, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv:1704.04861, 2017.\n\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\n\nIn ICCV, 2019.\n\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.\n\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\n\nstochastic depth. In ECCV, 2016.\n\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\n\nconvolutional networks. In CVPR, 2017.\n\nZilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\n\nCriss-cross attention for semantic segmentation. In ICCV, 2019.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by\n\nreducing internal covariate shift. In ICML, 2015.\n\nDahun Kim, Jun Xie, Huiyu Wang, Siyuan Qiao, Qihang Yu, Hong-Seok Kim, Hartwig Adam, In So Kweon, and Liang-Chieh Chen. TubeFormer-DeepLab: Video Mask Transformer. In CVPR, 2022.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-\n\ntional neural networks. In NeurIPS, 2012.\n\nYanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In CVPR, 2022.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n\nTsung-Yi Lin, Piotr Doll ́ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.\n\nFeature pyramid networks for object detection. In CVPR, 2017.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\n\nSwin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\n\nZe Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In CVPR, 2022a.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.\n\nA convnet for the 2020s. In CVPR, 2022b.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n\nSachin Mehta and Mohammad Rastegari. Mobilevit: light-weight, general-purpose, and mobile-\n\nfriendly vision transformer. In ICLR, 2022a.\n\nSachin Mehta and Mohammad Rastegari. Separable self-attention for mobile vision transformers.\n\narXiv preprint arXiv:2206.02680, 2022b.\n\nIlija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ́ar. Designing\n\nnetwork design spaces. In CVPR, 2020.\n\nPrajit Ramachandran, Barret Zoph, and Quoc V Le.\n\nSearching for activation functions.\n\narXiv:1710.05941, 2017.\n\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\n\nStand-alone self-attention in vision models. In NeurIPS, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers\n\ngeneralize to imagenet? In ICML, 2019.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. IJCV, 115:211–252, 2015.\n\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-\n\nbilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.\n\nIn NAACL, 2018.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. In ICLR, 2015.\n\nAravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\n\nBottleneck transformers for visual recognition. In CVPR, 2021.\n\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv:2106.10270, 2021.\n\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable\n\neffectiveness of data in deep learning era. In ICCV, 2017.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking\n\nthe inception architecture for computer vision. In CVPR, 2016.\n\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-\n\nresnet and the impact of residual connections on learning. In AAAI, 2017.\n\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks.\n\nIn ICML, 2019.\n\nMingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In ICML, 2021.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers & distillation through attention. In ICML, 2021a.\n\nHugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv ́e J ́egou. Going\n\ndeeper with image transformers. In ICCV, 2021b.\n\nHugo Touvron, Matthieu Cord, and Herv ́e J ́egou. DeiT III: Revenge of the ViT. arXiv:2204.07118,\n\n2022.\n\nZhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao\n\nLi. Maxvit: Multi-axis vision transformer. In ECCV, 2022.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n\nAshish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In CVPR, 2021.\n\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\n\nAxial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation. In ECCV, 2020.\n\nHuiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab:\n\nEnd-to-end panoptic segmentation with mask transformers. In CVPR, 2021a.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In ICCV, 2021b.\n\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. Computational Visual Media, pp. 1–10, 2022.\n\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\n\nCVPR, 2018.\n\nMark Weber, Huiyu Wang, Siyuan Qiao, Jun Xie, Maxwell D. Collins, Yukun Zhu, Liangzhe Yuan, Dahun Kim, Qihang Yu, Daniel Cremers, Laura Leal-Taixe, Alan L. Yuille, Florian Schroff, Hartwig Adam, and Liang-Chieh Chen. DeepLab2: A TensorFlow Library for Deep Labeling. arXiv: 2106.09748, 2021.\n\nRoss Wightman.\n\nPytorch image models.\n\nhttps://github.com/rwightman/\n\npytorch-image-models, 2019.\n\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\n\nIntroducing convolutions to vision transformers. In ICCV, 2021.\n\nTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll ́ar, and Ross Girshick. Early\n\nconvolutions help transformers see better. In NeurIPS, 2021.\n\nSaining Xie, Ross Girshick, Piotr Doll ́ar, Zhuowen Tu, and Kaiming He. Aggregated residual\n\ntransformations for deep neural networks. In CVPR, 2017.\n\nYufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Vitae: Vision transformer advanced by\n\nexploring intrinsic inductive bias. In NeurIPS, 2021.\n\nChenglin Yang, Yilin Wang, Jianming Zhang, He Zhang, Zijun Wei, Zhe Lin, and Alan Yuille. Lite\n\nvision transformer with enhanced self-attention. In CVPR, 2022.\n\nJianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao.\n\nFocal self-attention for local-global interactions in vision transformers. In NeurIPS, 2021.\n\nHongkun Yu, Chen Chen, Xianzhi Du, Yeqing Li, Abdullah Rashwan, Le Hou, Pengchong Jin, Fan Yang, Frederick Liu, Jaeyoun Kim, and Jing Li. TensorFlow Model Garden. https: //github.com/tensorflow/models, 2020.\n\nQihang Yu, Yingda Xia, Yutong Bai, Yongyi Lu, Alan L Yuille, and Wei Shen. Glance-and-gaze\n\nvision transformer. In NeurIPS, 2021.\n\nQihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Cmt-deeplab: Clustering mask transformers for panoptic segmentation. In CVPR, 2022a.\n\nQihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille,\n\nand Liang-Chieh Chen. k-means Mask Transformer. In ECCV, 2022b.\n\nKun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.\n\nIncorporating\n\nconvolution designs into visual transformers. In ICCV, 2021a.\n\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In ICCV, 2021b.\n\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\n\nIn CVPR, 2022.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. arXiv:1710.09412, 2017.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\n\nSemantic understanding of scenes through the ade20k dataset. IJCV, 127(3):302–321, 2019.\n\nDaquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and\n\nJiashi Feng. Deepvit: Towards deeper vision transformer. arXiv:2103.11886, 2021.\n\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\n\nDeformable transformers for end-to-end object detection. ICLR, 2021.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nIn the appendix, we provide more details for both our model and experiments.\n\n• In section A.1, we provide MOAT implementation details.\n\n• In section A.2.1, we provide ImageNet experimental details.\n\n• In section A.2.2, we provide ImageNet-V2 experimental results.\n\n• In section A.3.1, we provide COCO detection experimental details.\n\n• In section A.3.2, we provide more COCO object detection experimental results.\n\n• In section A.4, we provide ADE20K semantic segmentation experimental detaills.\n\n• In section A.5, we provide COCO panoptic segmentation experiments.\n\n• In section A.6.1, we provide ablation studies on the MOAT micro-level design.\n\n• In section A.6.2, we provide ablation studies on the MOAT macro-level design.\n\n• In section A.7, we provide the ImageNet trainng time, peak training memory and throughput\n\nmeasurement of MOAT models.\n\n• In section A.8, we discuss limitations of our model.\n\nA.1 MOAT IMPLEMENTATION DETAILS\n\nIn the MOTA networks, we employ kernel size 3 for both convolutions and depthwise convolutions. We use the multi-head self attention (Vaswani et al., 2017), where each attention head has channels 32. For the MBConv and MOAT blocks, we use expansion ratio 4. The SE module (Hu et al., 2018) in the MBConv blocks (i.e., 2nd and 3rd stages) adopt reduction ratio 4 (relative to the input channels).\n\nOur MOAT block includes the relative positional embedding (Shaw et al., 2018; Dai et al., 2021) for ImageNet. However, the downstream tasks usually take a larger input resolution than ImageNet, demanding for a special adaptation (e.g., bilinear interpolation of pretrained positional embedding). For simplicity, we remove the positional embedding, when running MOAT on downstream tasks.\n\nA.2\n\nIMAGENET IMAGE CLASSIFICATION\n\nA.2.1\n\nIMAGENET EXPERIMENTS\n\nThe ImageNet-1K dataset (Russakovsky et al., 2015) contains 1.2M training images with 1000 classes. We report top-1 accuracy on the ImageNet-1K validation set, using the last checkpoint. We also experiment with pretraining on the larger ImageNet-22K dataset, and then fine-tuning on the ImageNet-1K. We closely follow the prior works (Dai et al., 2021; Liu et al., 2022b) and provide more details below. In Tab. 8, we compare our MOAT with more state-of-the-art models.\n\nExperimental setup. We train MOAT models on ImageNet-1K with resolution 224 for 300 epochs. If pretraining on the larger ImageNet-22K, we use resolution 224 and 90 epochs. Afterwards, the models are fine-tuned on ImageNet-1K for 30 epochs. During fine-tuning, we also experiment with larger resolutions (e.g., 384 and 512). We employ the typical regularization methods during training, such as label smoothing (Szegedy et al., 2016), RandAugment (Cubuk et al., 2020), MixUp (Zhang et al., 2017), stochastic depth (Huang et al., 2016), and Adam (Kingma & Ba, 2015) with decoupled weight decay (i.e., AdamW (Loshchilov & Hutter, 2019)). See Tab. 10 and Tab. 11 for detailed hyper-parameters.\n\nA.2.2\n\nIMAGENET-1K-V2 EVALUATION\n\nTo further demonstrate the transferability and generalizability of our MOAT models, we perform additional evaluations on the ImageNet-1K-V2 (Recht et al., 2019), using our ImageNet (Russakovsky et al., 2015) pretrained checkpoints. We report an extensive evaluation, using MOAT and several input resolutions, on ImageNet-1K-V2, aiming to establish another solid baseline for the community, as we notice that most of the existing models do not report results on ImageNet-1K-V2. As shown in the Tab. 9, MOAT does not overfit to ImageNet-1K-V1 dataset and generalizes well to ImageNet1K-V2 dataset, as we observe a continuous performance improvement from small to large models.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 8: Performance on ImageNet-1K with more state-of-the-art models are included. 1K only: Using ImageNet-1K only. 22K + 1K: ImageNet-22K pretraining and ImageNet-1K fine-tuning.\n\nmodel\n\neval size\n\nparams\n\nFLOPs\n\nImageNet-1K top-1 accuracy\n\n1K only\n\n22K+1K\n\nRegNetY-16G (Radosavovic et al., 2020) NFNet-F5 (Brock et al., 2021)\n\nEfficientNetV2-S (Tan & Le, 2021) EfficientNetV2-M (Tan & Le, 2021) EfficientNetV2-L (Tan & Le, 2021) EfficientNetV2-XL (Tan & Le, 2021)\n\nConvNets\n\nConvNeXt-T (Liu et al., 2022b) ConvNeXt-S (Liu et al., 2022b) ConvNeXt-B (Liu et al., 2022b) ConvNeXt-L (Liu et al., 2022b) ConvNeXt-XL (Liu et al., 2022b)\n\nDeiT-B (Touvron et al., 2021a) CaiT-S-36 (Touvron et al., 2021b) DeepViT-L (Zhou et al., 2021) PVT-Large (Wang et al., 2021b) HaloNet-H4 (Vaswani et al., 2021) HaloNet-H5 (Vaswani et al., 2021)\n\nSwin-T (Liu et al., 2021) Swin-S (Liu et al., 2021) Swin-B (Liu et al., 2021) Swin-L (Liu et al., 2021) SwinV2-L (Liu et al., 2022a)\n\nFocal-B (Yang et al., 2021) CSwin-B (Dong et al., 2022) CSwin-L (Dong et al., 2022) MViTv2-H (Li et al., 2022)\n\nBotNet-T7 (Srinivas et al., 2021) LambdaResNet-420 (Bello, 2021) T2T-ViT-24 (Yuan et al., 2021b) CMT-S (Guo et al., 2022) CeiT-S (Yuan et al., 2021a) CvT-21 (Wu et al., 2021) PVTv2-B5 (Wang et al., 2022) MaxViT-XL (Tu et al., 2022)\n\nCoAtNet-0 (Dai et al., 2021) CoAtNet-1 (Dai et al., 2021) CoAtNet-2 (Dai et al., 2021) CoAtNet-3 (Dai et al., 2021) CoAtNet-4 (Dai et al., 2021)\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-1 MOAT-2 MOAT-3\n\nMOAT-4\n\nViTs\n\nHybrid\n\nHybrid (ours)\n\n2242 5442\n\n4802 4802 4802 4802\n\n2242 2242 2242 3842 3842\n\n3842 3842 2242 2242 3842 5122\n\n2242 2242 2242 3842 3842\n\n2242 3842 3842 5122\n\n3842 3202 2242 2242 3842 3842 2242 5122\n\n2242 2242 2242 3842 5122\n\n2242 2242 2242 2242\n\n3842 3842 3842 3842\n\n5122 5122 5122\n\n5122\n\n84M 16.0B 377M 289.8B\n\n22M 54M 120M 208M\n\n8.8B 24B 53B 94B\n\n4.5B 29M 8.7B 50M 89M 15.4B 198M 101.0B 350M 179.0B\n\n86M 68M 55M 61.4M 85M 85M\n\n55.4B 48.0B 12.5B 9.8B -\n-\n\n4.5B 28M 8.7B 50M 88M 15.4B 197M 103.9B 197M 115.4B\n\n16.0B 89.8M 47.0B 78M 173M 96.8B 667M 763.5B\n\n45.8B 75.1M -\n- 15.0B 64.1M 4.0B 25.1M 12.9B 24.2M 24.9B 32M 82M 11.8B 475M 535.2B\n\n4.2B 25M 8.4B 42M 75M 15.7B 168M 107.4B 275M 360.9B\n\n5.7B 27.8M 9.1B 41.6M 73.4M 17.2B 190.0M 44.9B\n\n27.8M 18.2B 41.6M 29.6B 54.3B 73.4M 190.0M 141.2B\n\n41.6M 58.7B 73.4M 104.6B 190.0M 271.0B\n\n483.2M 648.5B\n\n82.9 86.0\n\n83.9 85.1 85.7 -\n\n82.1 83.1 83.8 85.5 85.5\n\n83.1 85.0 83.1 81.7 85.6 85.8\n\n81.3 83.0 83.5 -\n-\n\n83.8 85.4 -\n-\n\n84.7 84.9 82.6 83.5 83.3 83.3 83.8 -\n\n81.6 83.3 84.1 85.8 -\n\n83.3 84.2 84.7 85.3\n\n84.6 85.9 86.2 86.5\n\n86.2 86.5 86.7\n\n-\n\n- -\n\n84.9 86.2 -\n87.3\n\n82.9 84.6 85.8 87.5 87.8\n\n- -\n- -\n- -\n\n- -\n85.2 87.3 87.7\n\n- 87.0 87.5 88.8\n\n- -\n- -\n- -\n- 88.7\n\n- -\n- 87.6 88.6\n\n83.6 84.9 86.0 86.8\n\n85.7 87.0 87.5 88.2\n\n87.2 87.7 88.4\n\n89.1\n\nUnder the fair comparison, with ImageNet-22K pretrainng and input size 384, MOAT-2/3 surpass the current state-of-the-art model SwinV2-B/L by 0.6/1.7%, respectively. Additionally, our MOAT-4, with input size 512, achieves a new state-of-the-art performance of 81.5%, without extra proprietary training data.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable 9: Performance on ImageNet-1K-V2.\n\nmodel\n\nparams\n\ninput size\n\nFLOPs\n\nImageNet-1K-V2 top-1 accuracy (%)\n\n1K only\n\n22K + 1K\n\nLeViT-256 (Graham et al., 2021) DeiT-B (Touvron et al., 2021a) CaiT-S36 (Touvron et al., 2021b) SwinV2-B (Liu et al., 2021) SwinV2-L (Liu et al., 2021)\n\ntiny-MOAT-0 tiny-MOAT-1 tiny-MOAT-2 tiny-MOAT-3\n\ntiny-MOAT-1 tiny-MOAT-2 tiny-MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\nMOAT-1 MOAT-2 MOAT-3\n\nMOAT-4\n\n256 224 224 384 384\n\n224 224 224 224\n\n256 256 256\n\n224 224 224 224\n\n384 384 384 384\n\n512 512 512\n\n512\n\n18.9M 86M 68M 88M 197M\n\n3.4M 5.1M 9.8M 19.5M\n\n5.1M 9.8M 19.5M\n\n27.8M 41.6M 73.4M 190.0M\n\n1.1B 17.5B 13.9B 54.7B 115.4B\n\n0.8B 1.2B 2.3B 4.5B\n\n1.6B 3.0B 6.0B\n\n5.7B 9.1B 17.2B 44.9B\n\n18.2B 27.8M 29.6B 41.6M 73.4M 54.3B 190.0M 141.2B\n\n41.6M 58.7B 104.6B 73.4M 190.0M 271.0B\n\n483.2M 648.5B\n\n70.0 71.5 72.5 –\n–\n\n64.3 67.3 70.1 72.1\n\n68.2 70.9 72.9\n\n72.8 74.2 74.3 75.5\n\n74.5 76.2 76.5 77.5\n\n76.8 77.1 77.8\n\n–\n\n– –\n– 78.1 78.3\n\n– –\n– –\n\n– –\n–\n\n74.1 75.8 76.7 78.4\n\n76.4 78.1 78.7 80.0\n\n78.4 79.3 80.6\n\n81.5\n\nTable 10: MOAT ImageNet hyper-parameter settings.\n\nhyper-parameter\n\nstochastic depth rate\n\ncenter crop randaugment mixup alpha loss type label smoothing train epochs train batch size optimizer type peak learning rate min learning rate warm-up lr decay schedule weight decay rate gradient clip EMA decay rate\n\nImageNet-1K\n\nImageNet-22K\n\n1K pre-training\n\n1K → 1K fine-tuning\n\n22K pre-training\n\n22K → 1K fine-tuning\n\n(MOAT-0/1/2/3)\n\n(MOAT-0/1/2/3)\n\n0.2 / 0.3 / 0.5 / 0.7\n\ntrue 2, 15 0.8 softmax 0.1 300 4096 AdamW 3e-3 1e-5 10K steps cosine 0.05 1.0 0.9999\n\n0.2 / 0.3 / 0.5 / 0.9\n\n0.1 / 0.2 / 0.3 / 0.6\n\n0.1 / 0.2 / 0.3 / 0.6\n\nfalse 2, 15/15/15/20 0.8 softmax 0.1 30 512 AdamW 5e-5 5e-5 none none 1e-8 1.0 0.9999\n\ntrue 2, 5 none sigmoid 0.0001 90 4096 AdamW 1e-3 1e-5 5 epochs linear 0.01 1.0 None\n\nfalse 2, 5 none softmax 0.1 30 1024 AdamW 5e-5 5e-5 none none 1e-8 1.0 0.9999\n\nA.3 COCO OBJECT DETECTION AND INSTANCE SEGMENTATION\n\nA.3.1 COCO OBJECT DETECTION EXPERIMENTAL DETAILS\n\nExperimental setup. We train Cascade Mask R-CNN (Cai & Vasconcelos, 2018; He et al., 2017) on the COCO 2017 dataset (Lin et al., 2014) with our MOAT architectures. The dataset contains 118K training and 5K validation samples. We use the official TensorFlow (Abadi et al., 2016) implementation of Cascade Mask R-CNN by TF-Vision Model Garden (Yu et al., 2020). Our training setting closely follows the prior works (Chen et al., 2022b; Tu et al., 2022), except that we use batch size 64 and initial learning rate 0.0001. To adapt the MOAT models to high-resolution inputs, we\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTable 11: tiny-MOAT ImageNet hyper-parameter settings. ⋆: use EMA decay rate 0.9999 for tiny-MOAT-3.\n\nhyper-parameter\n\nstochastic depth rate\n\ncenter crop randaugment mixup alpha loss type label smoothing train epochs train batch size optimizer type peak learning rate min learning rate warm-up lr decay schedule weight decay rate gradient clip EMA decay rate\n\nImageNet-1K\n\n1K input size 224\n\n1K input size 256\n\n(tiny-MOAT-0/1/2/3)\n\n0.0 / 0.0 / 0.0 / 0.1\n\ntrue 2, 15 0.8 softmax 0.1 300 4096 AdamW 3e-3 1e-5 10K steps cosine 0.05 1.0 None⋆\n\n0.0 / 0.0 / 0.0 / 0.1\n\ntrue 2, 15 0.8 softmax 0.1 300 4096 AdamW 3e-3 1e-5 10K steps cosine 0.05 1.0 None⋆\n\npartition the features into non-overlapping windows for the self-attention computations with the window size set to 14 for the second last stage, and use global attention for the last stage. As a result of this window partition, the input size must be divisible by 14. The TF-Vision Model Garden codebase further requires the input size to be square (with padding) and divisible by 64. Hence, we choose 1344 as the input size, similar to the size used in the baseline methods (i.e., longest side is no more than 1333). We use Feature Pyramid Network (Lin et al., 2017) to integrate features from different levels.\n\nA.3.2 MORE COCO OBJECT DETECTION EXPERIMENTAL RESULTS\n\nIn this section, we perform more COCO object detection experiments with 896 input size. All the backbone are pretrained on ImageNet-1K dataset. MOAT-0/1/2 surpass UViT (Chen et al., 2021a) and MaxViT (Tu et al., 2022) by 3.9/5.2/4.9% APbox (3.1/4.1/3.9% APmask), and 3.0/4.0/4.0% APbox (2.4/3.2/3.0% APmask), respectively.\n\nTable 12: Object detection and instance segmentation on the COCO 2017 val set. We employ Cascade Mask-RCNN, and single-scale inference (hard NMS). All backbones are pretrained on ImageNet-1K.\n\nbackbone\n\ninput size\n\nparams\n\nUViT-T (Chen et al., 2021a) UViT-S (Chen et al., 2021a) UViT-B (Chen et al., 2021a)\n\nMaxViT-T (Tu et al., 2022) MaxViT-S (Tu et al., 2022) MaxViT-B (Tu et al., 2022)\n\nMOAT-0 MOAT-1 MOAT-2\n\n896 × 896 896 × 896 896 × 896\n\n896 × 896 896 × 896 896 × 896\n\n896 × 896 896 × 896 896 × 896\n\n51M 59M 74M\n\n86M 108M 146M\n\n65M 79M 110M\n\nFLOPs APbox APbox 50 –\n– –\n\n720B 882B 1160B\n\n51.2 51.9 52.5\n\n475B 595B 856B\n\n525B 580B 710B\n\n52.1 53.1 53.4\n\n55.1 57.1 57.4\n\n71.9 72.5 72.9\n\n73.6 75.7 76.0\n\nAPbox 75 –\n– –\n\n56.8 58.1 58.1\n\n59.9 62.6 63.0\n\n43.9 44.5 44.8\n\n44.6 45.4 45.7\n\n47.0 48.6 48.7\n\n50 –\n– –\n\n69.1 69.8 70.3\n\n70.5 72.9 73.2\n\nAPmask 75 –\n– –\n\n48.4 49.5 50.0\n\n51.1 52.7 53.1\n\nAPmask APmask\n\nA.4 ADE20K SEMANTIC SEGMENTATION\n\nExperimental setup. We experiment with the proposed MOAT models on ADE20K semantic segmentation dataset (Zhou et al., 2019) using DeepLabv3+ (Chen et al., 2018; 2017). We fine-tune the global attention for MOAT. The same training strategies are used for all backbone variants. Specifically, for training hyper-parameters, we train the model with 32 TPU cores for 180k iterations, with batch size 64, Adam (Kingma & Ba, 2015) optimizer, and a poly schedule learning rate starting at 0.0001. For data augmentations, the inputs images are resized and padded to either 513 × 513 or\n\n19\n\nPublished as a conference paper at ICLR 2023\n\n641 × 641, with random cropping, flipping, and color jittering (Cubuk et al., 2019). No test-time augmentation is used during inference.\n\nA.5 COCO PANOPTIC SEGMENTATION\n\nExperimental setup. We also evaluate the proposed MOAT architectures on the challenging COCO panoptic segmentation dataset (Lin et al., 2014) using Panoptic-DeepLab (Cheng et al., 2020) with the official codebase (Weber et al., 2021). We fine-tune the global attention on downstream segmentation tasks for MOAT. We adopt the same training strategies for MOAT and its counterparts. Specifically, for training hyper-parameters, we train the model with 32 TPU cores for 200k iterations with the first 2k for warm-up stage. We use batch size 64, Adam (Kingma & Ba, 2015) optimizer, and a poly schedule learning rate starting at 0.0005. For data augmentations, the inputs images are resized and padded to 641 × 641, with random cropping, flipping, and color jittering (Cubuk et al., 2019). No test-time augmentation is used during inference.\n\nMain results. The results are summarized in Tab. 13, where MOAT consistently outperforms other backbones. Specifically, our MOAT-0 surpasses ConvNeXt-T significantly by 4.3% PQ. In the large model regime, MOAT-3 surpasses ConvNeXt-L by 3.5%. Our MOAT-4 achieves the performance of 46.7% PQ, outperforming the heavy backbone SWideRNet (Chen et al., 2020) by 2.3%.\n\nTable 13: Panoptic segmentation on COCO val set. The results are obtained by applying different backbones with Panoptic-DeepLab, using single-scale inference (i.e., no test-time augmentation). Results for MobileNet, ResNet, and Xception are cited from (Cheng et al., 2020), and results for SWideRNet is cited from (Chen et al., 2020), while results for ConvNeXt and MOAT are obtained using the official code-base (Weber et al., 2021) with the same training recipe. All models are trained and evaluated with input images resized to 641 × 641, and thus FLOPs are also measured w.r.t. size 641 × 641. †: use ImageNet-22K pretrained weights.\n\nbackbone\n\nparams\n\nMobileNet-V3 (Howard et al., 2019) ResNet50 (He et al., 2016a) Xception-71 (Chollet, 2017)\n\n- -\n-\n\nFLOPs\n\n12.2B 77.8B 109.2B\n\nConvNeXt-T (Liu et al., 2022b) ConvNeXt-S (Liu et al., 2022b) ConvNeXt-B† (Liu et al., 2022b) ConvNeXt-L† (Liu et al., 2022b) ConvNeXt-XL† (Liu et al., 2022b)\n\n51.3B 40.3M 61.9M 87.2B 103.8M 146.2B 220.1M 312.8B 379.6M 544.1B\n\nSWideRNet (Chen et al., 2020)\n\n752.5M 2614.0B\n\nMOAT-0 MOAT-1 MOAT-2† MOAT-3† MOAT-4†\n\n76.8B 39.5M 53.1M 119.7B 88.5M 199.7B 208.3M 493.3B 512.0M 1134.7B\n\nPQ (%)\n\nPQTh (%)\n\nPQSt (%)\n\n30.0 35.1 38.9\n\n36.7 40.0 41.7 41.9 43.0\n\n44.4\n\n41.0 43.0 43.9 45.4 46.7\n\n- -\n-\n\n37.3 41.4 43.6 43.6 44.9\n\n-\n\n42.6 44.7 45.9 48.3 49.5\n\n- -\n-\n\n35.7 37.9 38.9 39.4 40.0\n\n-\n\n38.6 40.4 40.8 41.1 42.4\n\nA.6 MORE ABLATION STUDIES\n\nA.6.1 ABLATION STUDIES ON THE MOAT MICRO-LEVEL DESIGN\n\nOrder of MBConv and Attn in MOAT block. Our MOAT block design reverses the order of Attention (Attn) and Mobile Convolution (MBConv), delegating the downsampling duty to the strided depthwise convolution within the MBConv. However, the dowsampling can be still performed in the MBConv with the original order (i.e., Attn + MBConv). Since the operations, Attn and MBConv, are interlaced, the key difference then comes from the first block in each stage, where the Attn is operated on the (1) spatially downsampled and/or (2) channel expanded features. To conduct the study, we employ different blocks in the MOAT variants, using ”Attn + MLP”, ”Attn + MBConv”, or ”MBConv + Attn”. For the ”Attn + MBConv” block, we further ablate the place (Attn vs. MBConv), where we apply the spatial downsampling and channel expansion operations.\n\nIn Tab. 14, we observe the following results. First, replacing the MLP with MBConv improves the performance by 0.3% and 0.7% for MOAT-0 and tiny-MOAT-2. Second, if we perform both spatial downsampling and channel expansion at the MBConv block, the performance is further improved by 0.5% and 0.9% for MOAT-0 and tiny-MOAT-2, showing that MBConv learns better downsampled\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nTable 14: Ablation studies of the order of MBConv and Attention (Attn) on ImageNet-1K with input 224. We also ablate the place, where we apply the spatial downsampling and channel expansion.\n\nmodel\n\nblock composition\n\nspatial downsampling\n\nchannel expansion\n\nparams (M)\n\nFLOPs (B)\n\ntop-1 acc.\n\nMOAT-0\n\ntiny-MOAT-2\n\nAttn + MLP Attn + MBConv Attn + MBConv MBConv + Attn Attn + MBConv\n\nAttn + MLP Attn + MBConv Attn + MBConv MBConv + Attn Attn + MBConv\n\nAttn Attn MBConv MBConv MBConv\n\nAttn Attn MBConv MBConv MBConv\n\nAttn Attn MBConv MBConv Attn\n\nAttn Attn MBConv MBConv Attn\n\n28.0 28.2 25.6 27.8 29.3\n\n9.8 9.9 9.0 9.8 10.3\n\n5.4 5.4 5.8 5.7 7.1\n\n2.2 2.2 2.3 2.3 2.8\n\n82.6 82.9 83.1 83.3 83.2\n\n79.8 80.5 80.7 81.0 81.0\n\nfeatures. However, this design is equivalent to shifting the first Attn layer to its previous stage, reducing the representation capacity of the current stage. More concretely, only the last stage will be affected, since one layer is shifted. Third, to enhance the representation capacity, reversing the order of Attn and MBConv allows us to keep the first Attn layer in the same stage. This design further improves the performance by 0.7% and 1.2% for MOAT-0 and tiny-MOAT-2. Fourth, to compensate for the shifting effect, we could also employ another 1 × 1 convolution to expand the channels at the first Attn layer (then, MBConv only performs the spatial downsampling). However, this design performs similarly to our MOAT block design, but uses more parameters and FLOPs.\n\nA.6.2 ABLATION STUDIES ON THE MOAT MACRO-LEVEL DESIGN\n\nAblation studies on MOAT-based model. In Tab. 15, we ablate the stage-wise design by using either MBConv or MOAT block in stage 2 to stage 5. The first stage is the convolutional stem, containing two 3 × 3 convolutions. We use the layer layout of MOAT-0. As shown in the table, the pure MOAT-based model (i.e., using MOAT blocks for all four stages) achieves the best performance of 83.6%, which however uses the most FLOPs. Our MOAT model design (i.e., use MOAT block in the last two stages) attains the better trade-off between accuracy and model complexity.\n\nTable 15: Ablation studies of MOAT-based model on ImageNet-1K, using MOAT-0 layer layout and input size 224. We change the block type (MBConv vs. MOAT block) from stage 2 to stage 5. The first stage is fixed to use the convolutional stem.\n\nstage-2\n\nstage-3\n\nstage-4\n\nstage-5\n\nparams (M)\n\nFLOPs (B)\n\ntop-1 acc.\n\nMOAT\n\nMOAT MOAT MOAT MOAT MBConv MOAT MOAT MBConv MBConv MOAT MOAT MBConv MBConv MBConv MOAT MBConv MBConv MBConv MBConv\n\n28.2 28.1 27.8 25.7 23.4\n\n11.9 6.9 5.7 4.7 4.5\n\n83.6 83.5 83.3 82.2 82.0\n\nAblation studies on MOAT meta architecture. We perform ablation studies on the meta-architecture by varying the number of blocks per stage. For simplicity, we only vary the block numbers in the third and fourth stages, while keeping the block numbers in the other stages unchanged. Note that the first stage corresponds to the convolutional stem. The studies with MOAT-1 meta architecture are shown in Tab. 16. In the end, we choose the layout {2, 2, 6, 14, 2} because it has the best performance and lower parameter cost. Interestingly, our discovery echoes the layer layout proposed by CoAtNet (Dai et al., 2021). We visualize the architecture of MOAT-1 in Fig. 6.\n\nTable 16: Ablation studies of MOAT meta-architecture design on ImageNet-1K, using MOAT-1 and input size 224. We control the first, second and last stages to have two blocks, and vary the block numbers of the third and fourth stages.\n\nnumber of blocks in five stages\n\nparams (M)\n\nFLOPs (B)\n\ntop-1 acc.\n\n(2, 2, 2, 16, 2) (2, 2, 4, 15, 2) (2, 2, 6, 14, 2) (2, 2, 8, 13, 2) (2, 2, 10, 12, 2)\n\n43.7 42.6 41.6 40.6 39.5\n\n21\n\n8.9 9.0 9.1 9.2 9.3\n\n84.1 84.2 84.2 84.1 84.1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: Architecture of MOAT-1, including the convolutional stem, MBConv, and MOAT blocks.\n\nA.7\n\nIMAGENET TRAINING TIME, PEAK TRAINING MEMORY AND TROUGHPUT MEASUREMENTS\n\nTable 17: ImageNet training time measured in hours. We use 16 TPUv4 cores for training MOAT-{0,1,2} and 32 TPUv4 cores for MOAT-3. MOAT is training efficient: for ImageNet-22k pretraining, MOAT takes no more than 2.05 days, while for ImageNet-1k pretraining, MOAT takes < 1 day.\n\ndataset\n\nmodel\n\npre-training\n\nfine-tuning\n\nImageNet-1K\n\nImageNet-22K\n\ninput size\n\n224 × 224\n\n224 × 224\n\n384 × 384\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\n6.5h 9.8h 13.9h 16.0h\n\n– –\n– –\n\n2.8h 4.4h 6.1h 7.9h\n\ninput size\n\n224 × 224\n\n224 × 224\n\n384 × 384\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\n20.1h 30.0h 42.6h 49.2h\n\n0.9h 1.3h 1.8h 2.2h\n\n2.5h 3.9h 5.4h 7.0h\n\nTable 18: ImageNet peak training memory of MOAT models. The input size is 224 × 224.\n\nmodel\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3\n\ntraining statistics\n\ntotal batch size\n\nnum. of TPUv4 cores\n\nbatch size per core\n\npeak memory per core (MB)\n\n4096 4096 4096 4096\n\n16 16 16 32\n\n256 256 256 128\n\n19155 26170 26662 26260\n\nTable 19: ImageNet throughput measurement of MOAT models. We re-implement MOAT with the popular “timm” (Wightman, 2019) library in PyTorch, and measure the throughput on an Nvidia V100 GPU, following the same settings as DeiT (Touvron et al., 2021a), Swin (Liu et al., 2021), and ConvNeXt (Liu et al., 2022b).\n\ninput size\n\n224 × 224\n\n384 × 384\n\n512 × 512\n\nmodel\n\nMOAT-0 MOAT-1 MOAT-2 MOAT-3 MOAT-4\n\nparams (M)\n\nFLOPs (B)\n\nthroughput (images/sec)\n\nFLOPs (B)\n\nthroughput (images/sec)\n\nFLOPs (B)\n\nthroughput (images/sec)\n\n27.8 41.6 73.4 190.0 483.2\n\n5.7 9.1 17.2 44.9 –\n\n536 339 209 89 –\n\n18.2 29.6 54.3 141.2 –\n\n155 91 58 23 –\n\n– 58.7 104.6 271.0 648.5\n\n– 41 27 9\n4\n\n22\n\nMBConv blockMBConv blockMOATblockMOATblock×2×6×14×2H4×W4×96...MOAT block ×1H×W×3H8×W8×192H16×W16×384H32×W32×768stemH2×W2×64Published as a conference paper at ICLR 2023\n\nA.8 LIMITATIONS\n\nCurrently, the scaling rule of MOAT model variants are hand-designed. We, therefore, expect the architecture could be further improved by the breakthroughs in neural architecture search or network pruning (attaining faster inference speed while maintaining a similar accuracy).\n\n23",
    "reference": "# Summary Of The Paper\n\nThis paper presents a family of neural networks called MOAT, which combines Mobile ConvNet and Transformer. It studies how to build effective networks based on two observations from Mobile ConvNet and Transformer. The new proposed networks achieve good performance on different tasks.\n\n# Strength And Weaknesses\n\nStrength: Paper writing is good and easy to follow; Idea is simple and effective; Experiments are sufficient.\n\nWeaknesses: The paper is more like a technical report. The novelty is weak by combing MobileConvet and Transformer.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: It's easy to follow. It presents its motivation based on two key observations which make sense to me.\nQuality: Writing is good and experiments are sufficient.\nNovelty: Novelty is a little weak for me. It is more technical by combining MobileConvet and Transformer. \nReproducibility: I think reproducibility is fine.\n\n# Summary Of The Review\n\nRefer to the above sections.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nREVISITING PRUNING AT INITIALIZATION THROUGH THE LENS OF RAMANUJAN GRAPH\n\nDuc Hoang, Shiwei Liu, Radu Marculescu & Zhangyang Wang Department of Electrical and Computer Engineering University of Texas at Austin, Austin, TX 78712, USA {hoangd,radum,atlaswang}@utexas.edu shiwei.liu@austin.utexas.edu\n\nABSTRACT\n\nPruning neural networks at initialization (PaI) has received an upsurge of interest due to its end-to-end saving potential. PaI is able to find sparse subnetworks at initialization that can achieve comparable performance to the full networks. These methods can surpass the trivial baseline of random pruning but suffer from a significant performance gap compared to post-training pruning. Previous approaches firmly rely on weights, gradients, and sanity checks as primary signals when conducting PaI analysis. To better understand the underlying mechanism of PaI, we propose to interpret it through the lens of the Ramanujan Graph - a class of expander graphs that are sparse while being highly connected. It is often believed there should be a strong correlation between the Ramanujan graph and PaI since both are about finding sparse and well-connected neural networks. However, the finer-grained link relating highly sparse and connected networks to their relative performance (i.e., ranking of difference sparse structures at the same specific global sparsity) is still missing. We observe that not only the Ramanujan property for sparse networks shows no significant relationship to PaI’s relative performance, but maximizing it can also lead to the formation of pseudo-random graphs with no structural meanings. We reveal the underlying cause to be Ramanujan Graph’s strong assumption on the upper bound of the largest nontrivial eigenvalue (ˆμ) of layers belonging to highly sparse networks. We hence propose Iterative Mean Difference of Bound (IMDB) as a mean to relax the ˆμ upper bound. Likewise, we also show there exists a lower bound for ˆμ, which we call the Normalized Random Coefficient (NaRC), that gives us an accurate assessment for when sparse but highly connected structure degenerates into naive randomness. Finally, we systematically analyze the behavior of various PaI methods and demonstrate the utility of our proposed metrics in characterizing PaI performance. We show that subnetworks preserving better the IMDB property correlate higher in performance, while NaRC provides us with a possible mean to locate the region where highly connected, highly sparse, and non-trivial Ramanujan expanders exist. Our code is available at: https://github.com/VITA-Group/ramanujan-on-pai.\n\n1\n\nINTRODUCTION\n\nDeep neural networks (DNN) have demonstrated remarkable performance as they increase in size, i.e, test accuracy scales as a power law regarding model size and training data size (Hestness et al., 2017; Kaplan et al., 2020; Brown et al., 2020; Srivastava et al., 2022). Yet, the memory requirements and computational costs associated with the increased model size also grow prohibitively. Modern DNNs are widely recognized to be over-parameterized, and it has been shown that eliminating a significant number of parameters in a trained DNN does not compromise its performance (Han et al., 2015c; He et al., 2017). This over-parameterization property enables researchers to continually propose increasingly effective DNN pruning approaches that can dramatically shrink the model size while maintaining performance. The resultant sparse models can then be used with software and hardware that is optimized for sparsity, leading to faster training and inference.\n\nNeural network pruning can generally be divided into three categories: post-training pruning (Mozer & Smolensky, 1989; Han et al., 2015a), during-training pruning (Gale et al., 2019; Louizos et al., 2018), and pre-training pruning (Lee et al., 2019; Wang et al., 2020), depending on the timing of\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nthe pruning relative to the training phase. For instance, post-training pruning methods are generally effective when the primary goal is to reduce inference cost. However, these methods require training the dense model fully first, potentially multiple times if iterative pruning and retraining are used. With the prevalence of powerful foundation models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and DALL·E 2 (Ramesh et al., 2022), the prohibitively high cost of training these large models makes post-training pruning impractical. Therefore, pre-training pruning or pruning at initialization (PaI) is becoming increasingly attractive due to its potential to save time and resources end-to-end, by using a sparse DNN architecture from the outset.\n\nThe concept of pruning at initialization (PaI) was first introduced in SNIP (Lee et al., 2019), which removes the structurally unimportant connections at initialization via the proposed connection sensitivity. Follow-up works (Wang et al., 2020; Tanaka et al., 2020; Patil & Dovrolis, 2021) propose advanced pruning criteria to improve the performance of PaI. GraSP (Wang et al., 2020) aims to maintain the weights that can maximize the gradient flow. SynFlow (Tanaka et al., 2020) finds that previous PaI methods are prone to layer collapse and adopt iterative pruning to address it. Despite these advances, PaI still lags behind post-training pruning in terms of performance. A study by Frankle et al. (2021) suggests that connection ambiguity may explain the performance deficit, based on the finding that PaI exhibits surprising resilience against layer-wise random mask shuffling and weight re-initialization.\n\nPrior works on PaI mainly focus on “training signals” such as gradient flow (Wang et al., 2020), layer collapse (Tanaka et al., 2020), or sanity check (random weight shuffling and re-initialization) (Frankle et al., 2021). Since the limited information (e.g., magnitude, gradient, and Hessian) that PaIs have access to can be very noisy (Frankle et al., 2020), pruning criteria based on such information are often ineffective. We conjecture that the graph topology of sparse neural networks, being relatively overlooked, can be an essential source of information for pruning at initialization. Graph theory has recently emerged as a particularly advantageous tool for analyzing DNN architectures. For example, You et al. (2020) offers a new efficient graph representation of DNN using relation graphs to formulate an efficient model generator. Liu et al. (2020) analyzes sparse neural networks with graph distance and shows a plenitude of sparse sub-networks with very different topologies while achieving similar performance. Vooturi et al. (2020); Pal et al. (2022); Bhardwaj et al. (2021); Prabhu et al. (2018) show that maximizing good graph connectivity, i.e. by maximizing a graph’s expansion ratio, correlates to higher performance in hardware-efficient structured masks and lottery tickets (Frankle & Carbin, 2019). Unfortunately, prior efforts did not consider the pseudo-randomness that naturally emerges from very good expander graphs. Therefore by maximizing sparse graph connectivity, they are unwittingly prioritizing the formation of naive random graphs with no intrinsic structure meaning.\n\nIn this paper, we study PaI from the perspective of the Ramanujan bipartite graphs. The Ramanujan graph is a special graph in the bounded degree expander family, where the eigenbound is maximal (Nilli, 1991), thus leading to a maximum possible sparsity of a network while preserving the connectivity. The Ramanujan graph is intuitively well aligned with the main goal of PaI, i.e., finding sparse and well-connected neural networks. However, we find that there is still a missing link correlating the degree of connectivity to relative performance ranking at a particular sparsity. In addition, we also show that in situations where highly sparse and highly connected structures are demanded, it can be easy to generate pseudo-random graphs with no structural meanings unwittingly.\n\nWe reveal the underlying cause for such undesirable situations to be Ramanujan Graph’s strong assumption on the upper bound of the largest nontrivial eigenvalue (ˆμ) of layers belonging to highly sparse networks. We hence propose Iterative Mean Difference of Bound (IMDB) as a mean to relax ˆμ upper bound. Likewise, we also show there exists a lower bound for ˆμ, which we call the Normalized Random Coefficient (NaRC), that gives us an accurate assessment for when sparse but highly connected structure deteriorates into randomness. Leveraging our (adjusted) Ramanujan graph-based framework, we then extensively investigate (1) whether the generated sparse structures by existing PaI approaches follow the Ramanujan characteristics, (2) if there exists a correlation between the Ramanujan graph property and the inference performance of the sparse structure, and (3) if the sparse structure is also a random graph. Ultimately, we aim to shed light on a new perspective on PAI effectiveness independent of weights, gradients, and losses.\n\nOur contributions are summarized as follows:\n\n• We are the first to reveal that the utility of the Ramanujan property is largely limited in analyzing irregular graphs at high sparsity, which is often the case in analyzing PaI-generated\n\n2\n\nPublished as a conference paper at ICLR 2023\n\narchitectures. We identify the root cause to be its strong assumption on the upper bound of the largest nontrivial eigenvalue ˆμ of the adjacency matrix and propose Iterative Mean Difference of Bound (IMDB) as an effective fix.\n\n• We devise another novel metric to assess whether a Ramanujan graph is also random, which becomes increasingly likely with higher sparsity. We prove the existence of this metric by inferring from the definition of the expander mixing lemma. Our NormAlized Random Coefficient (NaRC) assesses the randomness of graphs, by characterizing the lower bound of ˆμ for which we can no longer distinguish expanders from randomly generated graphs.\n\n• Our analysis shows that IMDB correlates strongly with the relative performance for different PaI’s sparse masks at high sparsity. It further illustrates how NaRC can spot random structures, and provide interesting observations on the relationship between randomness and expansion.\n\n2 RELATED WORK\n\nPruning methods (Mozer & Smolensky, 1989; LeCun et al., 1989; Hassibi et al., 1993; Molchanov et al., 2016; Han et al., 2015b) traditionally aim to remove the unnecessary components of DNNs, resulting in a subnetwork that can be efficiently deployed at inference. As the sizes of the modern DNNs have exploded, a vast amount of attention has been shifted to pruning them before training, targeting both training and inference efficiency. Lee et al. (2019) explicitly learn a connectivity importance score for weights and eliminate weights with the lowest scores. Wang et al. (2020) leverage the Hessian-gradient product to discover the importance of weight to the gradient flow. Iterative pruning approaches (Tanaka et al., 2020; de Jorge et al., 2021) show their efficacy to prevent layer collapse, ending up with pruned networks with very small width (Patil & Dovrolis, 2021). Although existing PaI methods surpass the naive baseline of random pruning, they are only able to identify useful layerwise sparsity levels rather than the specific weight patterns (Frankle et al., 2021; Su et al., 2020). As PaI only accesses very limited and noisy information (e.g., magnitude, gradient, and Hessian) from initialization, pruning criteria based on such information may not be effective.\n\nOn the other hand, the topology of sparse DNNs - the configuration of nodes and connections among them - can be another essential source of information. Mocanu et al. (2018) initialize sparse networks with a Erd ̋os-R ́enyi graph and dynamically optimize the graph towards a scale-free network. Evci et al. (2020b) further expand the Erd ̋os-R ́enyi graph to convolution neural networks, demonstrating large performance improvements. Liu et al. (2020) analyze sparse DNN with graph edit distance and show that there exists plenty of sparse sub-networks with distinct topologies that perform equally well. You et al. (2020) study the relationship between the graph structure and the neural networks from the relation graphs point of view. Vooturi et al. (2020); Pal et al. (2022); Bhardwaj et al. (2021) show that maximizing the graph connectivity of sparse networks correlates to higher performance in the structured masking and lottery tickets (Frankle & Carbin, 2019).\n\nRecently, Ramanujan graphs have been linked to sparse structures (Pal et al., 2022; Vooturi et al., 2020) as naturally appealing criteria to produce sparse yet well-connected DNN models. We draw inspiration from two prior works: Vooturi et al. (2020) applies the Ramanujan bipartite graph products for efficient, structured sparsity. In short, they achieve run-time efficiency by decomposing a dense matrix into tiled multiplication and utilizing the Ramanujan principle to maximize connectivity between the decomposed tiled matrices. The more recent work by Pal et al. (2022) evaluates whether a lottery ticket’s masks exhibit Ramanujan property. If they do not, then sparsity will repeatedly be halved until the generated masks are all Ramanujan graphs.\n\nBesides the obvious difference that we uniquely work on pruning at initialization with end-to-end sparsity (no dense pre-training is involved), our work also differs from these prior arts in our tackling of irregular bi-graphs that arise from practical sparse DNNs: we are the first to identify a crucial limitation of Ramanujan property in analyzing irregular graphs under high sparsity, which seems to be overlooked by prior arts. We also highlight the danger of being overly expansive, which risks graphs deteriorating into randomness. We then propose ways to relax the upper constraint to mitigate the critical gap between theory and practice and a lower constraint to avoid the danger of randomness, which leads to a more rigorous analysis of sparse DNNs at initialization.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3 METHODOLOGY\n\nBefore we start, we want to explain some nomenclature that will be used excessively in our definitions. First, a “regular” graph refers to a graph where all its vertices have the same number of in/out edges; We refer to the number of edges as d. Analogously, when we talk about regular graphs, we refer to any dense DNN, such as Linear or Convolutional layers once. An ”irregular” graph, on the other hand, refers to a graph with mixed number of edges for every vertex. Likewise, when speaking of irregular graphs, we refer to the resulting unstructured pruning of DNN layers with PaI. Later, we will explain how the DNN’s layers are represented as graphs.\n\n3.1 PRELIMINARY: BIPARTITE EXPANDER GRAPHS\n\nIn this work, we focus mainly on the bipartite case of expander graphs. However, before giving a formal definition, we state the following intuition: expander graphs are graphs where every subset of vertices is not ”too large” and has ”many connections” to other vertices that do not belong to the same subset. Typically, these graphs are regular; however, we shall extend the definition to consider finite, connected, and irregular graphs for our DNN analysis purpose.\n\nDefinition 1. A bipartite graph or bi-graph G = (L ∪ R, E) is a graph consisting of two disjoint sets of vertices L and R such that every edge from E connects one vertex of L and one vertex of R.\n\nMany definitions will make use of G = (V, E), where V = L ∪ R instead. For most cases, it is trivial to extend them to bipartite graphs. However, in case of extension ambiguity, we will clarify these definitions specifically for bi-graphs.\n\nDefinition 2. Let S ⊆ V for G = (V, E); we denote N (S) = {v ∈ V |∃u ∈ S, (u, v) ∈ E} to be the neighborhood set consisting of all adjacent vertices not in |S|.\n\nSince we are working with bipartite graphs, S ⊆ L and N (S) ⊆ R, and there are no edges between any two vertices in S.\n\nDefinition 3. A (n, m, d, γ, α)-expander is a d-left-regular1 bipartite graph G = (L ∪ R, E), where |L| = n, |R| = m (m ≤ n) and ∀S ⊆ L s.t |S| ≤ γ · n the neighborhood set of S satisfies |N (S)| ≥ α · |S|. Here γ ∈ {0, 1} and α ∈ [0, d].\n\nThe α and γ parameters control the expansion ratio of the expander and are dependent on one another. For example, by letting α = d the regularity of L, then γ ≤ 1 d , then we violate the neighborhood constraints namely |N (S)| ≥ α · |S| > n > m. The expansion ratio, |N (S)| , is related to the Cheeger constant h(G), whereby a small ratio signifies information bottleneck and a large h(G) indicates the graph is strongly connected. A good bipartite expander graph should ensure a large Cheeger constant so that information can flow freely.\n\nd since if |S| ≥ n\n\nS\n\nLet us denote A ∈ R|n+m|×|n+m| as the adjacency matrix of some d-regular bipartite graph G = (L ∪ R, E) with eigenvalues μ(G) s.t μ0 ≥ ... ≥ μv−1, where d = μ0 = |μv−1|, and corresponding eigenvectors φ. We define ˆμ(G) = max|μi|̸=d |μi| to be the largest nontrivial eigenvalue. Due to the nature of bipartite graphs where |μ0| = |μ1| = d, we will often refer to ˆμ(G) as our third-largest eigenvalue. The expansion property of any G is represented by its spectral gap μ0 − ˆμ(G) (see Hoory & Linial (2006)). A large gap indicates G is more ”spread out” which is the hallmark trait of the expander graph. Since μ0 − ˆμ ≥ 0, we need to determine a threshold on the spectral gap for which G can be considered as a good expander. This brings us to the notion of Ramanujan graph property.\n\nDefinition 4. A d-regular graph is said to be a Ramanujan graph if ˆμ(G) ≤ 2 d − 1, where d is graph regularity. Alternatively, following the previous discussion, Ramanujan graph can also be expressed as ˆμ(G) ≤ 2 ∗\n\nμ0 − 1, since μ0 = d.\n\n√\n\n√\n\nThus, all Ramanujan graphs are good expanders due to the convenient upper bound of ˆμ(G). However, after PaI not all graphs are regular. Therefore, we need to generalize Definition 4 for all cases. To do so, we combine two inequalities: The first inequality states that the universal cover graph ̃G of G satisfies p( ̃G) ≥ 2 ∗ (cid:112)davg − 1, where p( ̃G) denotes the spectral radius of ̃G and davg represents the average degrees of G (for details see Hoory (2005)). The second inequality, following the results\n\n1d-left-regular means all vertices in L have d number of edges.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nof Hoory & Linial (2006), defines a graph to be Ramanujan iff ˆμ(G) ≤ p( ̃G). We relate these two inequalities to form ˆμ(G) ≤ 2 ∗ (cid:112)davg − 1 ≤ p( ̃G).\n\nAs a direct result, we can estimate any graph’s expansion property as the difference between ˆμ(G) and its estimated Ramanujan’s upper bound. Definition 5. We denote the difference of bound as: ∆r = 2∗(cid:112)davg − 1− ˆμ(G) . A value ∆r < 0.0 indicates a violation of the Ramanujan property and therefore the graph may not be a good expander.\n\ndL − 1 − ˆμ(G), where dR In the case of bi-graphs, we rewrite Definition 5 as ∆r = and dL are the average degree of R and L respectively. Additionally, because ˆμ(G) is the third largest eigenvalues by magnitude, a Ramanujan graph is only defined when min(dL, dR) ≥ 3.\n\ndR − 1 +\n\n√\n\n√\n\n3.2\n\nITERATIVE MEAN DIFFERENCE OF BOUND\n\nIn this subsection, we focus on discussing the value μ0 of irregular graphs, something we glossed over in the discussion of the previous subsection and it is also largely overlooked by prior works. Primarily, we want to address cases where μ0 ̸= d and assess the effect it can have on determining if a graph G satisfies the Ramanujan property. Here, we explicitly define the general range of μ0 on all graphs, explain its negative effects on our analysis of high, and propose a way to mitigate the effects.\n\nLemma 1. The value of μ0 for any adjacency matrix A is said to be davg ≤ μ0 ≤ dmax. If G is a connected graph, μ0 = dmax therefore G is dmax-regular. We first prove davg ≤ μ0 by using the Rayleigh quotient. We have:\n\n1T A1 1T 1 To prove μ0 ≤ dmax, let φ0 be the corresponding eigenvector of μ0 and say that v = arg maxu φ(u). Without loss of generality, we have φ0(v) ̸= 0. So, we show: (u,v)∈E φ0(u)\n\nμ0 = max h∈Rn\n\nv∈V dv n\n\nhT Ah hT h\n\n= davg\n\n(1)\n\n(cid:80)\n\n≥\n\n=\n\n(cid:88)\n\n(cid:80)\n\n≤\n\n1 = dv ≤ dmax.\n\n(2)\n\nμ0 =\n\n(μ0φ0)(v) φ0(v)\n\n=\n\n(Aφ0)(v) φ0(v)\n\n=\n\nφ0(v)\n\n(v,u)∈E\n\nFinally, we show that if μ0 = dmax, then (v,u)∈E 1 = dv = dmax. In this way, φ0(v) φ0 is a constant vector with value dv for all vertices u that are connected to v. By repeating the way v was chosen then applied to u for all such vertices u, we yield the result that G is dmax-regular.\n\n(cid:80)\n\n(u,v)∈E φ0(u)\n\n= (cid:80)\n\n∆r’s limitation for irregular graphs at high sparsity: Now that we have proven the range of μ0, we can extend it to the following inequality: ˆμ(G) ≤ 2 ∗ (cid:112)davg − 1 ≤ 2 ∗ μ0 − 1. This inequality shows that Ramanujan’s upper-bound estimation for ˆμ(G) is conservatively small.\n\n√\n\nBecause of our interest in analyzing high sparsity (which is often the pursuit of PaI methods), dmax − davg is possibly very large. By following ∆r, we only qualify those graphs with tiny ˆμ(G) as Ramanujan graphs. We refer to the expander mixing lemma (see detail in Sauerwald & Sun (2011)), which relates to the relative degree of ˆμ(G) and the smaller ˆμ(G) (the more G appears to be random). Consequently, in the irregular graph case, we could dismiss valid expanders due to the overly limiting requirements while retaining only graphs with random sparse-graph structures when analyzing high sparsity.\n\nIMDB. We propose a new metric called Iterative Mean Difference of Bound (∆rimdb) as a way to relax the ∆r’s constraint. Let G be our irregular graph, we define a set K to be {Ki(V, E, di) ⊂ G|di ≥ 3} the set of d-regular subgraphs in G, ∆rimdb is formally defined as:\n\n∆rimdb =\n\n1 |K|\n\n|K| (cid:88)\n\ni\n\n(2 ∗\n\n(cid:112)\n\ndi − 1 − ˆμ(Ki))\n\n(3)\n\nSince every graph in K are regular, we do not have to estimate the upper bound of their ˆμ. Intuitively, we say an irregular graph G is a good expander if its regular subgraphs are good expanders. Finally, we extend Eq 3. for bi-graphs as:\n\n∆rimdb =\n\n1 |K|\n\n|K| (cid:88) (\n\ni\n\n(cid:112)\n\ndi − 1 +\n\n(cid:112)\n\ndR − 1 − ˆμ(Ki))\n\n(4)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nwhere dR is the average degree of R since we only care about d-left-regular bi-graphs for our settings. Our ablation study shows ∆rimdb effectively correlates relative performance to relative connectivity for highly connected highly sparse structures.\n\n3.3 NORMALIZED RANDOM COEFFICIENT\n\nIn the previous section, we mentioned the expander mixing lemma and its relation to ˆμ(G), but did not go into much detail about it. In this section, we define this relation and show how it can be used to prove the existence of our NormAlized Random Coefficient (NaRC).\n\nDefinition 6. Given a d-regular G(V, E), the expander mixing lemma is given as:\n\n||E(S, T )| −\n\nd|S||T | n\n\n| ≤ μ1(G)(cid:112)|S||T |\n\n(5)\n\nwhere S, T ⊆ V , S ∩ T = {0}, n = |V |, μ1(G) is the second largest eigenvalue of G, and |E(S, T )| is the total number of edges between S and T . The intuition behind it is that the smaller μ1(G), the more G appears to be random. For more explanations, please refer to Sauerwald & Sun (2011).\n\nFor our irregular bi-graph case, we exchange μ1 and μ0 since they are equivalent, and replace d with dL, the average left degree. We can also exchange the definition of S to that of Definition 2, with S ⊆ L, likewise replace T with N (S) ⊆ R and n with m = |R|. The expression now reads as:\n\n||E(S, N (S))| −\n\ndL|S||N (S)| m\n\n| ≤ μ0(G)(cid:112)|S||N (S)|\n\n(6)\n\nFrom Lemma 1, we know that davg is the smallest possible value μ0 can be for G, thus the following inequality is true:\n\ndavg ≤\n\n||E(S, N (S))| − dL|S||N (S)| (cid:112)|S||N (S)|\n\nm\n\n|\n\n≤ μ0 ≤ dmax\n\n(7)\n\nEq 7 further highlights our earlier argument for IMDB, but we can now take it a step further. With this inequality, we can rewrite the Ramanujan upper bound to include the random expander graph subsets. This means:\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nˆμ(G) ≤ 2\n\n||E(S, N (S))| − dL|S||N (S)| (cid:112)|S||N (S)|\n\nm\n\n|\n\n− 1\n\n(8)\n\nBy subtracting the right-hand side, we arrive at our definition for NaRC:\n\nσ = (\n\nˆμ(G)2 4\n\n+ 1)(cid:112)|S| ∗ |N (S)| − ||E(S, N (S))| −\n\ndL |m|\n\n|S| ∗ |N (S)|| ≤ 0\n\n(9)\n\nand we say that G is a random expander if σ ≤ 0, and |L|∗|R| denote our normalized degree of randomness. From now on, when we refer σ, we will use its normalized version |L|∗|R| . From the above inequality, we can now clearly see how expander mixing lemma relates to ˆμ(G), and the smaller it is, the more random the graph becomes.\n\nσ\n\n|σ|\n\nOne reason we do not want our graphs to be reduced to randomness is that it conflicts with our desire for “specific connections” as stated by Frankle et al. (2021). Compared to trivial random sparsity, non-random and structurally meaningful masks are expected to provide an interpretation of where and how performance is derived. Our experiments show under the right circumstance, a network can learn to overcome randomness, and those with meaningful masks achieve significantly better performance on average.\n\n3.4 BIPARTITE EXPANDER GRAPHS ON PRUNING AT INITIALIZATION\n\nAll models can be viewed as a sequence of computing graphs. Let M denote the set of a l-layer model, we obtain M = {Gi, ..., Gl} graphs, where Gi = (L ∪ R, E) is the i-th layer’s graph representation with L and R indicates the input and output layer respectively. M initially starts as a set of complete bipartite graphs. Pruning is then a process of edge sparsification on M with the\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nresulting sub-graphs considered irregular expander graphs. The expander property appeals to network sparsification analysis because it can approximate a complete graph, as shown in Spielman (2018). However, approximating complete graphs using irregular graphs is still an open question. For our study, we consider only the convolution and linear layers of any given model.\n\nConvolution layer. A convolutional weight consists of four dimensions namely the input channels, output channels, kernel width, and kernel height. To represent the convolution layer as a bipartite graph, we can unfold the input and kernels dimension to get W ∈ R|L|×|R|, where |L| = Cin ∗ Kw ∗ Kh and |R| = Cout. The resulting weighted graph is written as G = (L ∪ R, E, W ). Note that the number of edges |E| = |W |.\n\nLinear Layer. A linear weight consists of only the input and output channels, and bipartite conversion is trivial. We directly express its weight as W ∈ R|L|×|R|, where |L| = Cin and |R| = Cout. The resulting weighted graphs is similarly written as G = (L ∪ R, E, W ). Prunning at initialization. For each network, let W denotes the set of weights {wl ∈ Rnl |∀l ∈ {1, ..., L}} where nl is the number of parameters at layer l. Pruning is the process of generating binary masks ml ∈ {0, 1}nl . A pruned subnetwork has weights wl ⊙ml, where ⊙ is the element-wise product. Most PaI methods have two stages: First, they issue scores zl ∈ Rnl to all weights. Second, they remove the score into mask ml with overall sparsity s. Pruning may occur iteratively or in one shot depending on the methods. We study the following representative PaI techniques:\n\n• Random (Liu et al., 2022) is the most basic PaI method that uniformly prunes every layer with the same pruning ratio assigned globally. Each parameter is randomly assigned a score based on the normal distribution. • ERK (Evci et al., 2020a; Mocanu et al., 2018) initializes sparse networks with a Erd ̋os-R ́enyi graph where small layers are usually allocated more weights. •SNIP (Lee et al., 2019) issues scores sl = |gl ⊙ wl| where gl and wl are gradients and weights respectively. The weights with the lowest scores after one iteration are pruned before training. •GraSP (Wang et al., 2020) removes weights that impeded gradient flows, by computing the Hessian-gradient product hl and issue scores sl = −w ⊙ hl. • SynFlow (Tanaka et al., 2020) iteratively prunes a model with its weights replaced with |wl|. Each time, it propagates an input of 1’s and computes the gradients based on the task’s loss function rl. It issues a score sl = |rl ⊙ wl| and removes the parameters with the smallest scores.\n\n4 EXPERIMENTS\n\nIn this section, we examine our earlier claims with empirical results and see how they fare; furthermore, with supporting evidence, we answer questions regarding relationships between Ramanujan to performance, randomness to performance, and Ramanujan to randomness. Finally, we point out intuitions and what they imply for PaI under the lens of the Ramanujan perspective.\n\nExperimental settings. We conduct our experiments with two different DNN architectures: Resnet34 (He et al., 2016) and Vgg-16 (Simonyan & Zisserman, 2014) on CIFAR-10 (Krizhevsky, 2009). We run our experiments with three random seeds and initialize all PaI methods identically with three different initial weights generated by each seed to ensure fairness. Table 1 summarizes our standardized training configurations. We include additional results on CIFAR-100 in the Appendix.\n\nTable 1: Summary of architectures and hyperparameters that we study in this paper.\n\nModel\n\nData\n\n#Epoch Batch Size Optimizer LR LR Decay, Epoch Weight Decay\n\nResnet-34 CIFAR-10\n\nVGG-16\n\nCIFAR-10\n\n250\n\n250\n\n256\n\n256\n\nSGD\n\nSGD\n\n0.1\n\n0.1\n\n10×, [160, 180]\n\n10×, [160, 180]\n\n0.0005\n\n0.0005\n\n4.1 OBSERVATIONS, INTUITIONS AND ABLATIONS\n\nRelationship between Ramanujan and performance: In Figure 1, we confirm that Ramanujan ∆r indeed correlates with performance as a function of density (denser equates wider in general). However, this observation is already known (Pal et al., 2022), so it is not very exciting. The left column of Figure 1, which visualizes Definition 5, can only show that connectivity correlates strongly with density, but it cannot claim whether strong connectivity is related to relative performance. We attribute this missing link to the strong upper bound of ˆμ(G). While ˆμ(G) ensures that graphs that satisfy the inequality are expanders, we see that it holds little correlation with the actual performance potential of the network. This brings us to our first contribution, which is the Iterative Mean Difference of Bound ∆rimdb.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: In these figures, solid lines always refer to the left y-axis, while dash lines refer to the right y-axis. Here, we illustrate the relationship between network density (x-axis), accuracy (left-y), and Ramanujan property (∆r (left column) and ∆rimdb (right column)) for Resnet-34 (top-row) and Vgg-16(bottom-row). First, we show in all cases Ramanujan graphs correlate strongly with performance (upward trend). Second, we show that ∆rimdb also strongly correlates to relative performance between different PaI methods at different sparsity. Note that for Vgg-16, we dropped GrASP due to its inability to generate a feasible mask.\n\nCorrelating relative performance with ∆rimdb: Figure 1 right column shows that ∆rimdb resolves the missing link stated early on. We demonstrate that ∆rimdb is able to correlate the degree of connectivity of individual sparse structures with their final ranking in performance at specific density levels. While its effectiveness lessens with increasing density, it is undeniable that ∆rimdb mirrors the performance trend of various sparse structures. Intuitively speaking, we can extend ∆rimdb to be a performance ranker for unstructured sparse masks. Ultimately, the results prove our intuition that an irregular graph G is a good expander if its regular subgraphs are good expanders too.\n\nOn the relationship between performance and σ: When we show that there exists a ratio σ that can provide a randomness estimation on the graph, we are naturally curious about its relationship with performance. But first, we need to perform a sanity check on σ to see if it works. We can confirm, using our two random methods (Rand and ERK) on Figure 3, that they always yield σ ≤ 0, no matter the settings. Now we observe from the left column of Figure 3 that there seems to be little correlation between performance and the randomness of the graph at first glance. The lack of correlation would support previous observations made by Frankle et al. (2021); Liu et al. (2022) on the sufficiency of random pruning given the right sparsity. However, Figure 2 quickly dispels these notions, which neatly brings us to our next point.\n\nFigure 2: The relationship between performance and σ for Resnet-34 with and without skip. We observe σ starts to correlate with performance without skipconnections.\n\nThe value of σ: We observe an interesting inverse relationship between σ and ∆rimdb for Resnet34 and Vgg-16. In Figure 3 right columns, we observe Resnet-34 to have a mutual correlation\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Identically formatted as Figure 1, on the left column, we illustrate the relationship between model’s performance and network’s randomness over global density for Resnet-34 and Vgg-16. On the right column, we try to correlate the Ramanujan characteristic (∆rimdb) with our Normalized Random Coefficient (NaRC) over performance. The observation is interesting because the relationship are perfectly inverted between our two models.\n\nbetween its degree of expansion and randomness that somehow yield increasingly better performance. Meanwhile, for Vgg-16, we see that a higher degree of expansion correlates to a lower degree of randomness, producing better performance. The second observation follows our specific intuitions, while the first observation seemingly contradicts them. How can randomness contribute to better performance? The answer turns out to be straightforward. In Figure 2, we compare the performance of Resnet-34 with and without skip connections at various densities. We observe that (1) as the model gets more sparse without skip connections, its masked structure becomes more specific (less random) to ensure gradient flows; (2) skip connections help carry information and act as a crutch to overcome the random nature of the model; (3) without skip connections, Resnet-34 becomes more specific; however, relying on gradients alone is not enough to recover the performance achieved by the model with skip connections. It all means that σ negatively affects performance for both models, and skip connections can help alleviate the symptoms while exacerbating the problem as they effectively hide randomness.\n\nOverall, we have shown a way to relax the strong constraint on ˆμ(G) such that the resulting graph’s connectivity correlates strongly with its final performance. Further checking the lower bound for ˆμ(G) could indicate whether a sparse structure deteriorates into randomness. Tying all together, we now have the necessary tool to locate the region where highly connected, highly sparse, and non-trivial expanders exist. While not our primary objective, we foresee future PaI works utilizing our metrics as “checkers” to guide the design of their new criteria in selecting sparse structures.\n\n5 CONCLUSION\n\nThis work delved into quantifying PaI from the perspective of Ramanujan graphs. Firstly, we introduced ∆rimdb as a novel way to relax the strong upper bound of ∆r in cases of highly sparse, highly connected networks. Secondly, we proved that there existed a random coefficient called NaRC that could reliably estimate the degree of randomness for any given irregular sparse graph, which served as a lower bound for ˆμ(G) beyond which a sparse structure deteriorated into randomness. Ultimately, we provided a new perspective on the effectiveness of PAI that is independent of weights, gradients, and losses. This work was purely scientific and no negative impact was anticipated.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThe authors thank Peihao Wang for his valuable insights offered during the project discussions. Z. Wang is in part supported by NSF Scale-MoDL (award number: 2133861) and the NSF AI Institute for Foundations of Machine Learning (IFML).\n\nREFERENCES\n\nKartikeya Bhardwaj, Guihong Li, and Radu Marculescu. How does topology influence gradient propagation and model performance of deep networks with densenet-type skip connections? 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13493–13502, 2021.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nPau de Jorge, Amartya Sanyal, Harkirat Singh Behl, Philip H. S. Torr, Gr ́egory Rogez, and Puneet Kumar Dokania. Progressive skeletonization: Trimming more fat from a network at initialization. ArXiv, abs/2006.09081, 2021.\n\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:\n\nMaking all tickets winners. ArXiv, abs/1911.11134, 2020a.\n\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pp. 2943–2952. PMLR, 2020b.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=rJl-b3RcF7.\n\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning, pp. 3259–3269. PMLR, 2020.\n\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Pruning neural networks at initialization: Why are we missing the mark? In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ig-VyQc-MLK.\n\nTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv\n\npreprint arXiv:1902.09574, 2019.\n\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. International Conference on Learning Representations, 2015a.\n\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\n\nefficient neural network. Advances in neural information processing systems, 28, 2015b.\n\nSong Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for\n\nefficient neural network. ArXiv, abs/1506.02626, 2015c.\n\nBabak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network\n\npruning. In IEEE international conference on neural networks, pp. 293–299. IEEE, 1993.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE international conference on computer vision, pp. 1389–1397, 2017.\n\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\n\nShlomo Hoory. A lower bound on the spectral radius of the universal cover of a graph. J. Comb.\n\nTheory, Ser. B, 93:33–43, 2005.\n\nShlomo Hoory and Nathan Linial. Expander graphs and their applications. Bulletin of the American\n\nMathematical Society, 43:439–561, 2006.\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\n\nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information\n\nprocessing systems, 2, 1989.\n\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. Single-shot network pruning based on connection sensitivity. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1VZqjAcYX.\n\nShiwei Liu, TT van der Lee, Anil Yaman, Zahra Atashgahi, D Ferrar, Ghada Sokar, Mykola Pechenizkiy, and DC Mocanu. Topological insights into sparse neural networks. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, ECMLPKDD, 2020.\n\nShiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, and Mykola Pechenizkiy. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. In International Conference on Learning Representations, 2022.\n\nChristos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through\n\nl 0 regularization. International Conference on Learning Representations, 2018.\n\nDecebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):2383, 2018.\n\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. International Conference on Learning Representations, 2016.\n\nMichael C Mozer and Paul Smolensky. Using relevance to reduce network size automatically.\n\nConnection Science, 1(1):3–16, 1989.\n\nAlon Nilli. On the second eigenvalue of a graph. Discrete Mathematics, 91(2):207–210, 1991.\n\nBithika Pal, Arindam Biswas, Sudeshna Kolay, Pabitra Mitra, and Biswajit Basu. A study on the\n\nramanujan graph property of winning lottery tickets. In ICML, 2022.\n\nShreyas Malakarjun Patil and Constantine Dovrolis. Phew : Constructing sparse networks that learn\n\nfast and generalize well without training data. In ICML, 2021.\n\nAmeya Prabhu, G. Varma, and Anoop M. Namboodiri. Deep expander networks: Efficient deep\n\nnetworks from graph theory. In ECCV, 2018.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nThomas Sauerwald and He Sun.\n\nExpander mixing lemma, October Lecture 3: 2011. URL https://resources.mpi-inf.mpg.de/departments/d1/teaching/ ws11/SGT/Lecture3.pdf.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556. ICLR., 2014.\n\nDaniel A. Spielman. Properties of expander graphs, October 2018. URL https://www.cs.\n\nyale.edu/homes/spielman/561/lect17-18.pdf.\n\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\n\nJingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D Lee. Sanity-checking pruning methods: Random tickets can win the jackpot. Advances in Neural Information Processing Systems. arXiv:2009.11094, 2020.\n\nHidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, and Surya Ganguli. Pruning neural networks\n\nwithout any data by iteratively conserving synaptic flow. ArXiv, abs/2006.05467, 2020.\n\nDharma Teja Vooturi, G. Varma, and Kishore Kothapalli. Ramanujan bipartite graph products for\n\nefficient block sparse neural networks. ArXiv, abs/2006.13486, 2020.\n\nChaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkgsACVKPH.\n\nJiaxuan You, Jure Leskovec, Kaiming He, and Saining Xie. Graph structure of neural networks. In\n\nICML, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA ADDITIONAL EXPERIMENTS ON CIFAR-100\n\nFigure 4: On these figures, solid lines always refer to the left y-axis, while dash lines refer to the right y-axis. Here we are illustrating the relationship between network density (x-axis), accuracy (left-y), and Ramanujan property (∆r (left column) and ∆rimdb (right column)) for Resnet-34 (toprow) and Vgg-16(bottom-row). First, we show in all cases Ramanujan graphs correlate strongly with performance (upward trend). Second, we show ∆rimdb also strongly correlates to relative performance between different PaI methods at different sparsity.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Identically formatted as Figure 1, on the left column, we illustrate the relationship between model’s performance and network’s randomness over global density for Resnet-34 and Vgg-16. On the right column, we try to correlate the Ramanujan characteristic (∆rimdb) with our Normalized Random Coefficient (NaRC) over performance. The observation is interesting because the relationship are perfectly inverted between our two models.\n\n14",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a new method to prune neural networks at initialization. By analogy to previous work which used sparse random subnetworks to speed up training, this paper instead uses Ramanjuan graphs, which are well-studied and known to be favorable with respect to expansion. It relaxes the eigenvalue condition of Ramanujan graphs to a new property (IMDB) which empirically correlates to increased performance when the network is trained on image classification tasks.\n\n# Strength And Weaknesses\n\nGeneral comments\n-\n\n- Why use the third largest eigenvalue of the adjacency matrix? Usually the spectral gap is taken to be the difference between the first and second largest eigenvalues, which is guaranteed to be positive if the graph is connected.\n\nStrengths\n-\n\n- The idea of using expander graphs to guide pruning is interesting and unifies existing work.\n\nWeaknesses\n-\n\n- The decision to not use existing measures of spectral expansion is not entirely convincing. You mention that the Ramanujan condition is too pessimistic for non-regular graphs when $d_{max}$ is used, but why not use something like the second eigenvalue of the normalized Laplacian?\n- The decision to pass to all $K$-regular subgraphs seems somewhat arbitrary and possibly expensive to compute. \n- The pruning measure proposed does not leverage the weights of the network at initialization, since it does not sparsify the network based \n on sensitivity of the loss function.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper provides a new perspective on pruning using the theory of spectral expanders, which are more general than simply using random graphs. The writing in some parts is confusing. For example, Lemma 1 reads \"The value of $\\mu_0$ for any adjacency matrix is said to be $d_{avg} \\leq \\mu_0 \\leq d_{max}$.\" This looks like it's stating a definition even though it is part of the lemma statement, and there is no separation between the lemma statement and its proof.\n\n# Summary Of The Review\n\nThe idea of measuring performance of pruning in terms of spectral properties of the adjacency matrix is a good idea. However, the choices made in deciding upon the IMDB measure seem arbitrary in comparison to existing measures of spectral expansion. The choice of comparing to $\\delta r$ is not entirely fair as a baseline, since that particular bound was designed around regular graphs.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  }
]