{
  "example_idx": 78,
  "reference": "Under review as a conference paper at ICLR 2023\n\nFEDHPO-BENCH: A BENCHMARK SUITE FOR FEDERATED HYPERPARAMETER OPTIMIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nHyperparameter optimization (HPO) is crucial for machine learning algorithms to achieve satisfactory performance. Its research progress has been boosted by existing HPO benchmarks. Nonetheless, existing efforts in benchmarking all focus on HPO for traditional centralized learning while ignoring federated learning (FL), a promising paradigm for collaboratively learning models from dispersed data. In this paper, we first identify some uniqueness of HPO for FL algorithms from various aspects. Due to this uniqueness, existing HPO benchmarks no longer satisfy the need to compare HPO methods in the FL setting. To facilitate the research of HPO in the FL setting, we propose and implement a benchmark suite FEDHPO-BENCH that incorporates comprehensive FedHPO problems, enables flexible customization of the function evaluations, and eases continuing extensions. We also conduct extensive experiments based on FEDHPO-BENCH to provide the community with more insights into FedHPO. We open-sourced FEDHPO-BENCH at https://github.com/FedHPO-Bench/FedHPO-Bench-ICLR23\n\n1\n\nINTRODUCTION\n\nMost machine learning (ML) algorithms expose many design choices, which can drastically impact the ultimate performance. Hyperparameter optimization (HPO) (Feurer & Hutter, 2019) aims at making the right choices without human intervention. To this end, HPO methods usually attempt to solve minλ∈Λ1×···×ΛK f (λ), where each Λk corresponds to the candidate choices of a specific hyperparameter, e.g., taking the learning rate from Λ1 = [0.01, 1.0] and the batch size from Λ2 = {16, 32, 64}. For each specified λ, f (λ) is the output result (e.g., validation loss) of executing the considered algorithm configured by λ. A solution λ∗ found for such a problem is expected to make the considered algorithm lead to superior generalization performance. Research in this line has been facilitated by HPO benchmarks (Gijsbers et al., 2019; Eggensperger et al., 2021; Pineda-Arango et al., 2021), which prepare many HPO problems so that different HPO methods can be effortlessly compared, encouraging fair, reliable, and reproducible empirical studies.\n\nHowever, existing HPO benchmarks all focus on traditional learning paradigms, where the functions to be optimized correspond to centralized learning tasks. Federated learning (FL) (McMahan et al., 2017; Li et al., 2020a), as a privacy-preserving paradigm for collaboratively learning a model from distributed data, has not been considered. Actually, along with the increasing privacy concerns from the whole society, FL has been gaining more attention from academia and industry. Meanwhile, HPO for FL algorithms (denoted by FedHPO from now on) is identified as a critical and promising open problem in FL (Kairouz et al., 2019).\n\nIn this paper, we first elaborate on several differences between FedHPO and traditional HPO (see Section 2.2), which essentially come from FL’s distributed nature and the heterogeneity among FL’s participants. These differences make existing HPO benchmarks inappropriate for studying FedHPO and, in particular, unusable for comparing FedHPO methods. Consequently, several recently proposed FedHPO methods (Zhou et al., 2021; Dai et al., 2020; Khodak et al., 2021; Zhang et al., 2021; Guo et al., 2022) are evaluated on respective problems and have not been uniformly implemented in one FL framework and well benchmarked.\n\nMotivated by FedHPO’s uniqueness and the successes of existing HPO benchmarks, we propose and implement FEDHPO-BENCH, a dedicated benchmark suite, to facilitate the research and application of FedHPO. FEDHPO-BENCH is featured by satisfying the desiderata as follows:\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nComprehensiveness. FL tasks are diverse in terms of domain, model architecture, heterogeneity among participants, etc. The objective functions of their corresponding FedHPO problems are thus likely to be diverse. Hence, FEDHPO-BENCH provides a comprehensive collection of FedHPO problems for drawing an unbiased conclusion from comparisons of HPO methods.\n\nFlexibility. Users may have different levels of privacy and fairness concerns, which may correspond to different multi-objective optimization problems. Meanwhile, the execution time for function evaluation depends on the system condition. Thus, FEDHPO-BENCH allows users to flexibly tailor the FedHPO problems to their privacy protection needs, fairness demands, and system conditions.\n\nExtensibility. As a developing field, new FedHPO problems and novel FedHPO methods constantly emerge, and FL’s best practice continuously evolves. Thus, we build FEDHPO-BENCH on a popular FL framework, FederatedScope (FS) (Xie et al., 2022), and make it more of a benchmarking tool that can effortlessly incorporate novel ingredients.\n\nTo our knowledge, FEDHPO-BENCH is the first FedHPO benchmark. We conduct extensive empirical studies with it to validate its usability and attain more insights into FedHPO.\n\n2 BACKGROUND AND MOTIVATIONS\n\nWe first give a brief introduction to the settings of HPO and its related benchmarks. Then we present and explain the uniqueness of FedHPO to show the demand for dedicated FedHPO benchmarks. Due to the space limitation, more discussions about related works are deferred to Appendix B.\n\n2.1 PROBLEM SETTINGS AND EXISTING BENCHMARKS\n\nIn the literature (Feurer & Hutter, 2019), HPO is often formulated as solving minλ∈Λ1×···×ΛK f (λ), where each Λk corresponds to candidate choices of a specific hyperparameter, and their Cartesian product (denoted by ×) constitute the search space. In practice, such Λk is often bounded and can be continuous (e.g., an interval of real numbers) or discrete (e.g., a set of categories/integers). Each function evaluation at a specified hyperparameter configuration λ means to execute the corresponding algorithm accordingly and return the value of considered metric (e.g., validation loss) as the result f (λ). HPO methods generally solve such a problem with a series of function evaluations. As a fullfidelity function evaluation is extremely costly, multi-fidelity methods exploit low-fidelity function evaluation, e.g., training for fewer epochs (Swersky et al., 2014; Domhan et al., 2015) or on a subset of data (Klein et al., 2017; Petrak, 2000; Swersky et al., 2013), to approximate the exact result. Thus, it would be convenient to treat f as f (λ, b), λ ∈ Λ1 × · · · × ΛK, b ∈ B1 × · · · × BL, where each Bl corresponds to the possible choices of a specific fidelity dimension.\n\nHPO benchmarks (Gijsbers et al., 2019; Eggensperger et al., 2021; Pineda-Arango et al., 2021) have prepared many HPO problems, i.e., various kinds of objective functions, for comparing HPO methods. To evaluate these functions, HPO benchmarks, e.g., HPOBench (Eggensperger et al., 2021), often provide three modes: (1) “Raw” means truly executing the corresponding algorithm; (2) “Tabular” means querying a lookup table, where each entry corresponds to a specific f (λ, b); (3) “Surrogate” means querying a surrogate model that might be trained on the tabular data.\n\n2.2 UNIQUENESS OF FEDERATED HYPERPARAMETER OPTIMIZATION\n\nGenerally, traditional HPO methods are applicable to FedHPO problems1, where, in each trial, the value f (λ, b) is evaluated, that is to say, an accordingly configured FL training course is conducted, as the dashed black box in Figure 1 illustrates. Conceptually, there are N clients, each of which has its specific data, and a server coordinates them to learn a model θ collaboratively by an FL algorithm such as FedAvg (McMahan et al., 2017) and FedOpt (Asad et al., 2020). Such FL algorithms are iterative. In the t-th round, the server broadcasts the global model θ(t) to sampled clients; then, these clients make local updates and send the updates back; finally, the server aggregates the updates to produce θ(t+1). After executing the FL algorithm configured by λ for several such rounds, e.g., #round= T according to the specified fidelity b, the performance, e.g., best validation loss ever achieved during this FL course, is returned as f (λ, b).\n\n1Despite the various scenarios in literature, we restrict our discussion about FedHPO to one of the most general FL scenarios that have been adopted in existing FedHPO works (Khodak et al., 2021; Zhang et al., 2021).\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Solving a FedHPO problem by a traditional HPO method solely or as the wrapper of a FedHPO method: the t-th round of a trial is shown in the dashed black box and dashed blue box, respectively. The faded clients are not sampled in that round; “aggr” denotes a certain aggregation operation; here FedEx is considered as the wrapped FedHPO method, which learns a policy π to determine λ(c)∗; each f (c) (·) is regarded as a client-specific approximation of f (λ(s), ·).\n\ni\n\nAs a distributed ML scenario, the procedure of each FL training round consists of two subroutines— local updates and aggregation. Thus, λ can be divided into server-side and client-side hyperparameters according to which subroutine each hyperparameter influences. Denoting it as λ = (λ(s), λ(c)), the original optimization problem can be straightforwardly restated as its bi-level counterpart minλ(s) f (λ(s), λ(c)∗), s.t., λ(c)∗ = minλ(c) f (λ(s), λ(c)). With such a point of view, let’s see how some FedHPO methods leverage the distributed nature of FL to solve the lower-level sub-problem efficiently and approximately.\n\nConcurrent exploration. In each FL training course, all sampled clients have executed local updates configured by client-side hyperparameters for one or more rounds. Given a specific λ(s), if we regard clients as replicas of the black-box function f (λ(s), ·) or, at least, similar such functions whose evaluation results help fitting f (λ(s), ·), it is natural to try out client-side hyperparameter configurations client-wisely so that we evaluate f (λ(s), λ(c)) for more than one λ(c)s in each FL training course. We summarize this idea as concurrent exploration, which recently proposed FedHPO methods, such as FedEx (Khodak et al., 2021) and FTS (Dai et al., 2020), have instantiated. Specifically, FedEx incorporates concurrent exploration with the weight-sharing trick (Liu et al., 2019) to achieve oneshot learning (i.e., by one FL training course) of a policy π for determining the optimal lower-level response λ(c)∗ = minλ(c) f (λ(s), λ(c)). Hence, we could solve the bi-level optimization problem by letting a traditional HPO method, as a wrapper, choose λ(s)2 and replace each standard FL training course with a FedHPO method, as the dashed blue box in Figure 1 shows.\n\nAs FedHPO methods are fused with the training course, existing HPO benchmarks become unusable for comparing them. Moreover, since this fusion makes the implementations of such FedHPO methods tightly coupled with that of FL training courses, and no existing FL framework has incorporated such FedHPO methods, researchers cannot compare them in a unified way. How we incorporate several recent FedHPO methods into FEDHPO-BENCH and make it extensible is discussed in Section 3.3, and whether concurrent exploration is useful is empirically answered in Section 4.2.\n\nPersonalization. The non-IIDness among clients’ data is likely to make them have different optimal configurations (Koskela & Honkela, 2020), where making decisions by the same policy, such as FedEx, would become unsatisfactory. This phenomenon tends to become severer when federated hetero-task learning (Yao et al., 2022) is considered. Trivially solving the personalized FedHPO problem minλ(s),λ(c) i denotes the client-side hyperparameter configuration of the i-th client, is intractable, as the search space exponentially increases with N . To promote studying personalized FedHPO, we provide a personalized FedHPO problem featured by heterogeneous tasks among the clients and the corresponding tabular benchmark (see H.1 for a detailed description).\n\nN ), where λ(c)\n\n1 , . . . , λ(c)\n\nf (λ(s), λ(c)\n\n1 ,...,λ(c)\n\nN\n\n2In practice, the traditional HPO method also determines the FedHPO algorithm’s hyperparameters and a\n\nsubset of the original client-side search space to be explored. We omit these to keep brevity.\n\n3\n\nλ,bfλ,b......θ!,λ\"θ\"!#$,f\"(c)λ\"c......Client$Client\"TraditionalHPO methodθ!Localupdateθ’!#$Evaluateλ(cf(cλ(cStandard FL training courseFedHPOmethod concurrently explores client-side search space during the courseClient$Client\"......Client(......θ!θ$!#$θ!θ(!#$θ!#$=aggrθ$!#$,θ(!#$,...ServerClient(θ!,λ(θ(!#$,f((c)λ(cθ!#$=aggr...,θ(!#$,θ\"!#$Serverπ!#$=aggr...λ(c,f(cλ(c,λ\"c,f\"cλ\"corλs,bor fλ,b,λ=λs,λc∗Under review as a conference paper at ICLR 2023\n\nMulti-objective optimization. Despite the model’s performance, researchers are often concerned about other issues, such as privacy protection and fairness. Regarding privacy, the FL algorithm is often incorporated with privacy protection techniques such as differential privacy (DP) (Kairouz et al., 2019), where the DP algorithm also exposes its hyperparameters. Intuitively, a low privacy budget specified for such algorithms indicates a lower risk of privacy leakage yet a more significant degradation of the model’s performance. As for fairness, namely, the uniformity of the model’s performances across the clients, more and more FL algorithms have taken it into account (Li et al., 2021a; Wang et al., 2021b), which contains some hyperparameter(s) concerning fairness. Therefore, researchers may be interested in searching for a hyperparameter configuration that guarantees an acceptable privacy leakage risk (e.g., measured by Rényi-DP (Mironov, 2017)) and fairness measurement (e.g., the standard deviation of client-wise performances) while optimizing the model’s performance.\n\nThus, a FedHPO benchmark is expected to expose a vector-valued objective function instead of a scalar-valued one, where the entries of a returned vector could be the quantitative measures corresponding to performance, privacy leakage risk, fairness, etc. Then, researchers are allowed to study multi-objective HPO (Hernández et al., 2021; Deb et al., 2002; Abdolshah et al., 2019).\n\nRuntime estimation and system-dependent trade-offs. For the research purpose, an FL training course is usually simulated in a single computer rather than executed in a distributed system. As a result, simply recording the consumed time is meaningless for measuring the cost of a function evaluation in studying FedHPO. Meanwhile, FL’s distributed nature introduces a new fidelity dimension—client_sample_rate, which determines the fraction of clients sampled in each round. When considering a client_sample_rate less than that of a full-fidelity function evaluation, each round of the FL course would take less time because there is less likely to be a straggler. However, client_sample_rate correlates with another fidelity dimension—#round, where a lower client_sample_rate often leads to federated aggregation with larger variance, which is believed to need more rounds for convergence. How we should balance these two fidelity dimensions to achieve more economical accuracy-efficiency trade-offs strongly depends on the system condition, e.g., choosing large #round but small client_sample_rate when the straggler issue is severe.\n\nAs existing HPO benchmarks focus on centralized learning tasks, they overlook a runtime estimation functionality for studying FedHPO. In Section 3.2, we present FEDHPO-BENCH’s system model, with which we conduct an empirical study to show the effect of balancing client_sample_rate and #round in Appendix G.\n\nDue to the uniqueness mentioned above, existing HPO benchmarks are inappropriate for studying FedHPO. FedHPO calls for dedicated benchmarks that incorporate objective functions corresponding to FL algorithms and respecting realistic FL settings.\n\n3 OUR PROPOSED BENCHMARK SUITE: FEDHPO-BENCH\n\nWe present an overview of FEDHPO-BENCH in Figure 2. Conceptually, FEDHPO-BENCH encapsulates function evaluation and provides a unified interface for HPO methods to interplay with it. Following the design of HPOBench (Eggensperger et al., 2021), function evaluations can be conducted in either of the three modes: raw, tabular, or surrogate. For the raw mode, we chose to build FEDHPO-BENCH upon the well-known FL platform FederatedScope (FS) (Xie et al., 2022), which has provided its docker images so that we can containerize FEDHPO-BENCH effortlessly by executing each FL algorithm in an FS docker container. To generate the lookup table for tabular mode, we truly execute the corresponding FL algorithms with the grids of search space as their configurations. These lookup tables are adopted as training data for the surrogate models, which are expected to approximate the objective functions (more details about this approximation are discussed in Appendix H.4.2). It’s important to note that the distributed nature of FL makes it very expensive to run an FL course, so, in FedHPO, the tabular and surrogate modes are much in demand to meet the efficiency requirement. For the convenience of users, we keep FEDHPO-BENCH’s interface basically the same as HPOBench’s. Meanwhile, we expose extra arguments for users to customize the instantiation of a benchmark. We defer the discussion of the relationship between FEDHPO-BENCH and HPOBench to Appendix B.1.\n\nIn this section, we elaborate on three highlights of FEDHPO-BENCH. In addition to the off-the-shelf FL-related components that FS already provides, we contribute new datasets, models, and algorithms\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Overview of FEDHPO-BENCH.\n\nto FS to prepare a comprehensive collection of FedHPO problems (see Section 3.1). Moreover, users are allowed to flexibly tailor these problems to their specific cases (see Section 3.2). Also, FS’s event-driven framework allows us to easily extend FEDHPO-BENCH by incorporating more FedHPO problems and methods, which is valuable for this nascent research direction (see Section 3.3).\n\n3.1 COMPREHENSIVENESS\n\nThere is no universally best HPO method (Gijsbers et al., 2019). Therefore, for the purpose of fairly comparing HPO methods, it is necessary to compare them on a variety of HPO problems that correspond to diverse objective functions and thus can comprehensively assess their performances.\n\nTo satisfy this need, we leverage FS to prepare various FL tasks, where their considered datasets and model architectures are quite different. Specifically, the data can be images, sentences, graphs, or tabular data. Some datasets are provided by existing FL benchmarks, which are readily distributed and thus conform to the FL setting. Some are centralized initially, which we partition by FS’s splitters to construct their FL version with various kinds of Non-IIDness among clients. All these datasets are publicly available and can be downloaded and preprocessed by our prepared scripts. More details of these datasets can be found in Appendix D. Then the corresponding suitable neural network model is applied to handle each dataset, involving fully-connected networks, convolutional networks, the latest attention-based model, etc. It is worth noticing that our prepared FL tasks cover both cross-silo and cross-device scenarios. In cross-device scenario, there are a lot more clients and a much lower client_sample_rate than in cross-silo scenario.\n\nFor each such FL task, we basically employ two FL algorithms, FedAvg and FedOpt, to learn the model, respectively. Then the FedHPO problem is defined as optimizing the design choices of the FL algorithm on each specific FL task. So, without further explanation, we use the triple <dataset, model, algorithm> to index a particular benchmark in the remainder of this paper. We summarize our currently provided FedHPO problems in Table 1, and more details can be found in Appendix H. For each problem, #round and client_sample_rate are adopted as the fidelity dimensions.\n\nWe study the empirical cumulative distribution function (ECDF) for each model type in the cross-silo benchmarks. Specifically, in creating the lookup table for tabular mode, we have conducted function evaluations for the hyperparameter configurations located on a very dense grid over the search space, resulting in a finite set {(λ, f (λ))} for each benchmark. Then we normalize the performances (i.e., f (λ)) and show their ECDF in Figure 3, where these curves exhibit different shapes. For example, the amounts of top-tier configurations for GNN on PubMed are remarkably less than on other graph datasets, which might imply a less smoothed landscape and difficulty in seeking the optimal configuration. As the varying shapes of ECDF curves have been regarded as an indicator of the diversity of benchmarks (Eggensperger et al., 2021), we can conclude from Figure 3 that FEDHPO-BENCH enables evaluating HPO methods comprehensively.\n\n5\n\nfrom fedhpob.benchmarks import TabularBenchmarkbenchmark = TabularBenchmark('cnn', 'femnist', 'avg')# get hyperparameters spaceconfig_space = benchmark.get_configuration_space()# get fidelity spacefidelity_space = benchmark.get_fidelity_space()# get resultsres = benchmark(config_space.sample_configuration(), fidelity_space.sample_configuration(), fairness_reg_func=np.var, fairness_reg_coef=1.0, seed=1)......1e-3...281...0.60.091e-2...43...0.40.11.....................FedHPO-BenchTabularSurrogateRawFederatedScopeDatasetModelFedAlgo.Fed HPOBaseInterfaceTabular modeSurrogate modeRaw modeFunction evaluationExecute FedAlgo. Interface design and code exampleContainerTraining dataPerformance e.g., loss Execute FedAlgo. or ContainerSystemmodelOff-line preprocessSystemstatisticsCollectFedHPOHPOmethodUnder review as a conference paper at ICLR 2023\n\nTable 1: Summary of benchmarks in current FEDHPO-BENCH: MF refers to matrix factorization. Rec. and Algo. are short for recommendation and algorithm, respectively. #Cont. and #Disc. denote the number of hyperparameter dimensions corresponding to continuous and discrete candidate choices, respectively. The unit of the budget is either day (d) or second (s).\n\nScenario\n\nCross-Silo\n\nCross-Device\n\nModel CNN BERT GNN GNN LR MLP MF LR\n\n#Dataset Domain\n\n2 2\n3 1\n7 7\n1 1\n\nCV NLP Graph Hetero Tabular Tabular Rec. NLP\n\n#Client 200 5\n5 5\n5 5\n480,189 ∼3300\n\n#Algo. 2\n2 2\n1 2\n2 2\n2\n\n#Cont. 4\n4 4\n1 3\n4 3\n3\n\n#Disc. Budget\n\n2 2\n1 1\n1 3\n1 1\n\n20d 20d 1d 1d 21,600s 43,200s -\n1d\n\n(a) CNN\n\n(b) BERT\n\n(c) GNN\n\n(d) LR\n\n(e) MLP\n\nFigure 3: Empirical Cumulative Distribution Functions: The normalized regret is calculated for all evaluated configurations of the respective model on the respective FL task with FedAvg.\n\n3.2 FLEXIBILITY\n\nWe allow users to instantiate each benchmark with arguments other than the <dataset, model, algorithm> triple. So, both the underlying objective function and the amount of time it takes to calculate its value can be customized according to the cases specified by the users.\n\nObjective function. FEDHPO-BENCH provides the basic interface to support multi-objective optimization concerning arbitrary specified performance metrics and privacy and fairness-related metrics. For privacy protection, we employ a representative FL+DP algorithm NbAFL (Wei et al., 2020) provided by FS, where users can specify any valid value for the privacy budget. As for fairness, FS has provided many personalized FL algorithms and fairness-aware aggregations, and FEDHPOBENCH can record client-wise performances. In designing the interface of FEDHPO-BENCH, we allow users to specify their preferred measurements of privacy leakage risk and fairness. Then the execution of an FL algorithm can be regarded as evaluating a vector-valued function rather than a scalar-valued one. By default, FEDHPO-BENCH transforms the vector result into a scalar one by treating privacy and fairness-related values as soft constraints to penalize.\n\nSystem model. In addition to customizing the objective function, it is also very helpful to customize the execution time of function evaluation because the execution time of the same FL course can vary a lot when deployed in environments with different system conditions. Many existing HPO benchmarks record the execution time of the training courses, which cannot be adapted regarding users’ system conditions. Despite a recorded execution time, we provide a system model to estimate the time consumed by evaluating f (λ, b) in realistic scenarios, which is configurable so that users with different system conditions can calibrate the model to their cases (Mohr et al., 2021). Based on the analysis of such a system model and a basic instance (Wang et al., 2021a), we propose and implement our system model, where the execution time T (f, λ, b) for each round in evaluating f (λ, b) is the summation of time consumed by computation and communication. Roughly, the time for communication is the summation of the time for downloading and uploading transferred information and the latency for establishing connections. The time for computation is the summation of the time for the server’s aggregation step and that for the straggler client’s local updates. Our system model exposes several adjustable parameters, for which we provide default choices based on the records collected from creating the tabular benchmarks. Meanwhile, users are allowed to specify these parameters according to their scenarios or other system statistic providers, e.g., estimating the latency of stragglers by sampling from FedScale (Lai et al., 2022). We defer the details about our system model to Appendix E.\n\n6\n\n0.00.20.40.60.81.0Normalized regret0.00.20.40.60.81.0P(X <= x)FEMNISTCIFAR100.00.20.40.60.81.0Normalized regret0.00.20.40.60.81.0P(X <= x)CoLASST20.00.20.40.60.81.0Normalized regret0.00.20.40.60.81.0P(X <= x)CoraCiteSeerPubMed0.00.20.40.60.81.0Normalized regret0.00.20.40.60.81.0P(X <= x)credit-gvehiclekc1blood-transf..Australiancarsegment0.00.20.40.60.81.0Normalized regret0.00.20.40.60.81.0P(X <= x)credit-gvehiclekc1blood-transf..AustraliancarsegmentUnder review as a conference paper at ICLR 2023\n\n3.3 EXTENSIBILITY\n\nAs FedHPO is springing up, we must reduce the effort of introducing more FedHPO problems and novel FedHPO methods to FEDHPO-BENCH.\n\nRecall that a FedHPO problem is characterized by the <dataset, model, algorithm> triple. With FS, we can apply the off-the-shelf data splitters to transform an arbitrary centralized dataset into an FL dataset, reuse any open-sourced model implementation by registering it in FS, and develop a novel FL algorithm via plugging in the hook function that expresses its unique step(s).\n\nTraditional HPO methods are decoupled from the procedure of function evaluation, with a conceptually standard interface for interaction (see Figure 1 and Figure 2). Thus, any new method in this line can readily interplay with FEDHPO-BENCH. However, FedHPO methods, such as FTS and FedEx, are fused with the FL training course to make concurrent exploration, as the dashed blue box in Figure 1 and the red color “FedHPO” module in Figure 2 shows. Thus, we need to implement such methods in FS if we want to benchmark them on FEDHPO-BENCH.\n\nAt a high level, such FedHPO methods essentially aim to learn a policy π collaboratively, along with the original FL course. As FS is featured by its event-driven programming paradigm, a standard FL course is modularized into event-handler pairs that express all the subroutines. Benefiting from this event-driven paradigm, all we need to develop are augmenting the messages exchanged by FL participants (i.e., re-defining events) and plugging those policy learning-related operations into the event handlers. As a result, we have implemented FTS, FedEx, and a personalized FedEx in FS, where their differences mainly lie in just the definition and implementation of those plug-in operations. We defer more implementation details to Appendix F.\n\n4 EXPERIMENTS\n\nWe conduct extensive empirical studies with our proposed FEDHPO-BENCH. Basically, we exemplify the use of FEDHPO-BENCH in comparing HPO methods, which, in the meantime, can somewhat validate the correctness of FEDHPO-BENCH. Moreover, we aim to gain more insights into FedHPO, answering two research questions: (RQ1) How do traditional HPO methods behave in solving FedHPO problems? (RQ2) Do recently proposed FedHPO methods that exploit “concurrent exploration” (see Section 2.2) significantly improve traditional methods? We conduct empirical studies in Section 4.1 and Section 4.2 to answer RQ1 and RQ2, respectively. All scripts concerning the studies here have been committed to FEDHPO-BENCH so that the community can quickly reproduce our established benchmarks.\n\n4.1 STUDIES ABOUT APPLYING TRADITIONAL HPO METHODS IN THE FL SETTING\n\nTo answer RQ1, we largely follow the experiment conducted in HPOBench (Eggensperger et al., 2021) but focus on the FedHPO problems FEDHPO-BENCH provided.\n\nProtocol. We employ up to ten optimizers (i.e., HPO methods) from widely adopted libraries (see Table 6 for more details). For black-box optimizers (BBO), we consider random search (RS), the evolutionary search approach of differential evolution (DE (Storn & Price, 1997; Awad et al., 2020)), and bayesian optimization with: a GP model (BOGP), a random forest model (BORF (Hutter et al., 2011b)), and a kernel density estimator (BOKDE (Falkner et al., 2018b)), respectively. For multifidelity optimizers (MF), we consider Hyperband (HB (Li et al., 2017)), its model-based extensions with KDE-based model (BOHB (Falkner et al., 2018a)), and differential evolution (DEHB (Awad et al., 2021)), and Optuna’s implementations of TPE with median stopping (TPEMD) and TPE with Hyperband (TPEHB) (Akiba et al., 2019). We apply these optimizers to solve the cross-silo FedHPO problems summarized in Table 1, where the time budget is relaxed for these traditional HPO methods to satisfy multiple full-fidelity function evaluations rather than a one-shot setting. For the sake of efficiency, we conduct this experiment with the tabular mode of FEDHPO-BENCH, and consider #round as the fidelity dimension for HPO methods to control (while keeping client_sample_rate=1.0). To compare the optimizers uniformly and fairly, we repeat each setting five times in the same runtime environment but with different random seeds. The best-seen validation loss is monitored for each optimizer (for multi-fidelity optimizers, higher fidelity results are preferred over lower ones). We sort the optimizers by their best-seen results and compare their mean ranks on all the considered\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFedHPO problems. Following HPOBench, we use sign tests to judge (1) whether advanced methods outperform their baselines and (2) whether multi-fidelity methods outperform their single-fidelity counterparts. We refer our readers to Appendix C for more details.\n\n(a) BBO\n\n(b) MF\n\n(c) All\n\nFigure 4: Mean rank over time on all FedHPO problems (with FedAvg).\n\nResults and Analysis. We show the overall results in Figure 4, and we defer detailed results to Appendix I. Overall, their eventual mean ranks do not deviate remarkably. For BBO, the performances of optimizers are close at the beginning but become more distinguishable along with their exploration. Ultimately, BOGP has successfully sought better configurations than other optimizers. In contrast to BBO, MF optimizers perform pretty differently in the early stage, which might be rooted in the vast variance of low-fidelity function evaluations. Eventually, HB and BOHB become superior to others while achieving a very close mean rank. We consider optimizers’ final performances on all the considered FedHPO problems, where, for each pair of optimizers, one may win, tie, or lose against the other. Then we can conduct sign tests to compare pairs of optimizers, where results are presented in Table 2 and Table 3. (1) Comparing these advanced optimizers with their baselines, only BOGP, BORF, and DE win on more than half of the problems but have no significant improvement, which is inconsistent with the non-FL setting. It is worth noting that a similar phenomenon can also be observed for HPO problems in general (Pushak & Hoos, 2022). (2) Meanwhile, no MF optimizers show any advantage in exploiting experience, which differs from non-FL cases. We presume the reason lies in the distribution of configurations’ performances (see Figure 3). From Table 3, we draw part of the answer to RQ1 as that MF optimizers always outperform their corresponding single-fidelity version, which is consistent with the non-FL settings.\n\nTable 2: P-value of a sign test for the hypothesis—these advanced methods surpass the baselines.\n\np-value agains RS win-tie-loss\n\np-value against HB win-tie-loss\n\nBOGP 0.0637 13 / 0 / 7 BOHB 0.4523 7 / 0 / 13\n\nBORF 0.2161 12 / 0/ 8 DEHB 0.9854 9 / 0 / 11\n\nBOKDE 0.1649 7 / 0 / 13 TPEMD 0.2942 9 / 0 / 11\n\nDE 0.7561 11 / 0 / 9 TPEHB 0.2454 9 / 0 / 11\n\nTable 3: P-value of a sign test for the hypothesis—MF methods surpass corresponding BBO methods.\n\nHB vs. RS DEHB vs. DE BOHB vs. BOKDE\n\np-value win-tie-loss\n\n0.1139 13 / 0 / 7\n\n0.2942 13 / 0 / 7\n\n0.0106 16 / 0 / 4\n\n4.2 STUDIES ABOUT CONCURRENT EXPLORATION\n\nTo answer RQ2, we select the superior optimizers from Section 4.1 to compare with FedEx (Khodak et al., 2021). As mentioned in Section 2.2, FL allows HPO methods to take advantage of concurrent\n\n8\n\n1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rankUnder review as a conference paper at ICLR 2023\n\nexploration, which somewhat compensates for the number of function evaluations. We are interested in methods designed regarding these characteristics of FedHPO and design this experiment to see how much concurrent exploration contributes.\n\nProtocol. We consider the FedHPO problem <FEMNIST, CNN, FedAvg), i.e., FedAvg is applied to learn a 2-layer CNN on FEMNIST. As a full-fidelity function evaluation consumes 500 rounds on this dataset, we specify RS, BOGP, BORF, BOKDE, HB, and BOHB to limit their total budget to 2,500 (i.e., 5 times budget of a full-fidelity evaluation) in terms of #round. Precisely, each BBO method consists of 50 trials, each of which runs for 50 rounds. For MF optimizers, we set the η of Successive Halving Algorithm (SHA) (Jamieson & Talwalkar, 2016) to 3, the minimal budget to 9 rounds, and the max budget to 81 rounds. Then we adopt these optimizers and FedEx wrapped by them (X+FedEx) to optimize the design choices of FedAvg, respectively. The wrapper is responsible for determining the arms for each execution of FedEx. We consider validation loss the metric of interest, and function evaluations are conducted in the raw mode. We repeat each method three times and report the averaged best-seen value at the end of each trial. Meanwhile, for each considered method, we entirely run the FL course with the optimal configuration it seeks. Their averaged test accuracies are compared.\n\nMethods W/O FedEx W/ FedEx\n\nRS BOGP BORF BOKDE HB BOHB\n\n79.93 ± 2.45 82.18 ± 0.94 81.86 ± 1.10 81.34 ± 1.75 80.26 ± 2.02 79.59 ± 2.09\n\n82.03 ± 2.08 83.20 ± 1.24 82.20 ± 0.54 82.11 ± 0.46 82.47 ± 0.04 84.02 ± 0.50\n\nTable 5: Compare the searched configurations: Mean test accuracy (%) ± standard deviation. Underline indicates improvements.\n\nFigure 5: Mean validation cross-entropy loss over time.\n\nResults and Analysis. We present the results in Figure 5 and Table 5. For FedAvg, the best-seen mean validation losses of all wrapped FedEx decrease slower than their corresponding wrapper. However, their searched configurations’ generalization performances are significantly better than their wrappers, which strongly confirms the effectiveness of concurrent exploration. Thus, we have a clear answer to RQ2: concurrent exploration methods significantly improve traditional methods.\n\n5 CONCLUSION AND FUTURE WORK\n\nIn this paper, we first identify the uniqueness of FedHPO, which we ascribe to the distributed nature of FL and its heterogeneous clients. This uniqueness prevents FedHPO research from leveraging existing HPO benchmarks, which has led to inconsistent comparisons between some recently proposed methods. Hence, we suggest and implement a comprehensive, flexible, and extensible benchmark suite, FEDHPO-BENCH. We further conduct extensive HPO experiments on FEDHPO-BENCH, validating its correctness and applicability to comparing traditional and FedHPO methods. We have open-sourced FEDHPO-BENCH with an Apache-2.0 license and will actively maintain it in the future (Maintenance of FEDHPO-BENCH is discussed in Appendix A). We believe FEDHPO-BENCH can serve as the stepping stone to developing reproducible FedHPO works.\n\nIn our next step, tasks other than federated supervised learning will be incorporated. At the same time, we aim to extend FEDHPO-BENCH to include different FL settings, e.g., HPO for vertical FL (Zhou et al., 2021). Another issue the current version has not touched on is the risk of privacy leakage caused by HPO methods (Koskela & Honkela, 2020), which we should provide related metrics and testbeds in the future.\n\n9\n\n1e-21e-11Fraction of budget0.51.01.52.02.53.03.54.0LossRSBO_GPBO_RFBO_KDEHBBOHBRS+FedExBO_GP+FedExBO_RF+FedExBO_KDE+FedExHB+FedExBOHB+FedExUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMajid Abdolshah, Alistair Shilton, Santu Rana, S. Gupta, and Svetha Venkatesh. Multi-objective\n\nbayesian optimisation with preferences over objectives. ArXiv, abs/1902.04228, 2019.\n\nSteven Adriaensen, André Biedenkapp, Gresa Shala, Noor H. Awad, Theresa Eimer, Marius Thomas Lindauer, and Frank Hutter. Automated dynamic algorithm configuration. ArXiv, abs/2205.13881, 2022.\n\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2623–2631, 2019.\n\nMuhammad Asad, Ahmed Moustafa, and Takayuki Ito. Fedopt: Towards communication efficiency\n\nand privacy preservation in federated learning. Applied Sciences, 10, 2020.\n\nN. Awad, N. Mallik, and F. Hutter. Differential evolution for neural architecture search. In Proceedings\n\nof the 1st workshop on neural architecture search(@ICLR’20), 2020.\n\nNoor Awad, Neeratyoy Mallik, and Frank Hutter. Dehb: Evolutionary hyberband for scalable, robust and efficient hyperparameter optimization. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 2147–2153, 2021.\n\nJames Bennett and Stan Lanning. The netflix prize. 2007.\n\nJames Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. J. Mach.\n\nLearn. Res., pp. 281–305, 2012.\n\nAndré Biedenkapp, H. Furkan Bozkurt, Theresa Eimer, Frank Hutter, and Marius Thomas Lindauer. Dynamic algorithm configuration: Foundation of a new meta-algorithmic framework. In European Conference on Artificial Intelligence, 2020.\n\nBernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael G Mantovani, Jan N van Rijn, and Joaquin Vanschoren. Openml benchmarking suites. arXiv preprint arXiv:1708.03731, 2017.\n\nSebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018.\n\nZhongxiang Dai, Bryan Kian Hsiang Low, and Patrick Jaillet. Federated bayesian optimization via\n\nthompson sampling. In NeurIPS, 2020.\n\nKalyanmoy Deb, Samir Agrawal, Amrit Pratap, and T. Meyarivan. A fast and elitist multiobjective\n\ngenetic algorithm: Nsga-ii. IEEE Trans. Evol. Comput., 6:182–197, 2002.\n\nTobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter In Twenty-fourth\n\noptimization of deep neural networks by extrapolation of learning curves. international joint conference on artificial intelligence, 2015.\n\nXuanyi Dong and Yi Yang. NAS-Bench-201: Extending the scope of reproducible neural architecture\n\nsearch. In International Conference on Learning Representations (ICLR), 2020.\n\nXuanyi Dong, Lu Liu, Katarzyna Musial, and Bogdan Gabrys. NATS-Bench: Benchmarking nas algorithms for architecture topology and size. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.\n\nKatharina Eggensperger, Philipp Müller, Neeratyoy Mallik, Matthias Feurer, Rene Sass, Aaron Klein, Noor Awad, Marius Lindauer, and Frank Hutter. HPOBench: A collection of reproducible multi-fidelity benchmark problems for HPO. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\nStefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimiza-\n\ntion at scale. In International Conference on Machine Learning, pp. 1437–1446, 2018a.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nStefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimiza-\n\ntion at scale. In International Conference on Machine Learning, 2018b.\n\nMatthias Feurer and Frank Hutter. Hyperparameter optimization. In Automated machine learning,\n\npp. 3–33. 2019.\n\nP. Gijsbers, E. LeDell, S. Poirier, J. Thomas, B. Bischl, and J. Vanschoren. An open source automl\n\nbenchmark. arXiv preprint arXiv:1907.00909 [cs.LG], 2019.\n\nRonald L. Graham, Donald E. Knuth, and Oren Patashnik. Concrete Mathematics: A Foundation for\n\nComputer Science. Addison-Wesley, 1989.\n\nPengfei Guo, Dong Yang, Ali Hatamizadeh, An Xu, Ziyue Xu, Wenqi Li, Can Zhao, Daguang Xu, Stephanie A. Harmon, Evrim B Turkbey, Baris I Turkbey, Bradford J. Wood, F. Patella, Elvira Stellato, Gianpaolo Carrafiello, Vishal M. Patel, and Holger R. Roth. Auto-fedrl: Federated hyperparameter optimization for multi-institutional medical image segmentation. ArXiv, abs/2203.06338, 2022.\n\nNikolaus Hansen, Anne Auger, Olaf Mersmann, Tea Tusar, and Dimo Brockhoff. Coco: a platform for comparing continuous optimizers in a black-box setting. Optimization Methods and Software, pp. 114 – 144, 2021.\n\nFlorian Hase, Matteo Aldeghi, Riley J. Hickman, Loïc M. Roch, Melodie Christensen, Elena Liles, Jason E. Hein, and Alán Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. Machine Learning: Science and Technology, 2021.\n\nAlejandro Morales Hernández, Inneke Van Nieuwenhuyse, and Sebastian Rojas-Gonzalez. A survey on multi-objective hyperparameter optimization algorithms for machine learning. ArXiv, abs/2111.13755, 2021.\n\nStephan Holly, Thomas Hiessl, Safoura Rezapour Lakani, Daniel Schall, Clemens Heitzinger, and Jana Kemnitz. Evaluation of hyperparameter-optimization approaches in an industrial federated learning system. ArXiv, abs/2110.08202, 2021.\n\nDzmitry Huba, John Nguyen, Kshitiz Malik, Ruiyu Zhu, Mike Rabbat, Ashkan Yousefpour, CaroleJean Wu, Hongyuan Zhan, Pavel Ustinov, Harish Srinivas, et al. Papaya: Practical, private, and scalable federated learning. Proceedings of Machine Learning and Systems, 2022.\n\nFrank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization In International conference on learning and intelligent\n\nfor general algorithm configuration. optimization, pp. 507–523. Springer, 2011a.\n\nFrank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization In International conference on learning and intelligent\n\nfor general algorithm configuration. optimization, 2011b.\n\nFrank Hutter, Holger Hoos, and Kevin Leyton-Brown. Parallel algorithm configuration. pp. 55–70,\n\n2012.\n\nFrank Hutter, Manuel López-Ibáñez, Chris Fawcett, Marius Thomas Lindauer, Holger H. Hoos, Kevin Leyton-Brown, and Thomas Stützle. Aclib: A benchmark library for algorithm configuration. In LION, 2014.\n\nKevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pp. 240–248, 2016.\n\nD. R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of\n\nGlobal Optimization, 21:345–383, 2001.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nMikhail Khodak, Renbo Tu, Tian Li, Liam Li, Nina Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight-sharing. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021.\n\nAaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast bayesian optimization of machine learning hyperparameters on large datasets. In Artificial intelligence and statistics, pp. 528–536, 2017.\n\nAntti Koskela and Antti Honkela. Learning rate adaptation for differentially private learning. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, pp. 2465–2475, 2020.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nFan Lai, Yinwei Dai, Sanjay S. Singapuram, Jiachen Liu, Xiangfeng Zhu, Harsha V. Madhyastha, and Mosharaf Chowdhury. FedScale: Benchmarking model and system performance of federated learning at scale. In International Conference on Machine Learning (ICML), 2022.\n\nLisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, pp. 6765–6816, 2017.\n\nTian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,\n\nmethods, and future directions. IEEE Signal Processing Magazine, pp. 50–60, 2020a.\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, pp. 429–450, 2020b.\n\nTian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated\n\nlearning through personalization. In ICML, pp. 6357–6368, 2021a.\n\nXiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learning In International Conference on Learning\n\non non-IID features via local batch normalization. Representations, 2021b.\n\nZitao Li, Bolin Ding, Ce Zhang, Ninghui Li, and Jingren Zhou. Federated matrix factorization with\n\nprivacy guarantee. Proc. VLDB Endow., 15:900–913, 2021c.\n\nMarius Lindauer, Katharina Eggensperger, Matthias Feurer, André Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, René Sass, and Frank Hutter. Smac3: A versatile bayesian optimization package for hyperparameter optimization. J. Mach. Learn. Res., pp. 54–1, 2022.\n\nHanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In\n\nInternational Conference on Learning Representations, 2019.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pp. 1273–1282, 2017.\n\nIlya Mironov. Rényi differential privacy. Symposium (CSF), pp. 263–275, 2017.\n\nIn 2017 IEEE 30th Computer Security Foundations\n\nFelix Mohr, Marcel Wever, Alexander Tornede, and Eyke Hüllermeier. Predicting machine learning pipeline runtimes in the context of automated machine learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43:3055–3066, 2021.\n\nHesham Mostafa. Robust federated learning through representation matching and adaptive hyper-\n\nparameters, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, pp. 2825–2830, 2011.\n\nJohann Petrak. Fast subsampling performance estimates for classification algorithm selection. In Proceedings of the ECML-00 Workshop on Meta-Learning: Building Automatic Advice Strategies for Model Selection and Method Combination, pp. 3–14, 2000.\n\nFlorian Pfisterer, Lennart Schneider, Julia Moosbauer, Martin Binder, and Bernd Bischl. Yahpo gym-an efficient multi-objective multi-fidelity benchmark for hyperparameter optimization. In First Conference on Automated Machine Learning (Main Track), 2022.\n\nSebastian Pineda-Arango, Hadi S. Jomaa, Martin Wistuba, and Josif Grabocka. HPO-B: A large-scale reproducible benchmark for black-box HPO based on openml. Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2021.\n\nYasha Pushak and Holger H. Hoos. Automl loss landscapes. ACM Transactions on Evolutionary\n\nLearning and Optimization, 2022.\n\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\n\nCollective classification in network data. AI magazine, pp. 93–93, 2008.\n\nBobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, pp. 148–175, 2016.\n\nRainer Storn and Kenneth Price. Differential evolution–a simple and efficient heuristic for global\n\noptimization over continuous spaces. Journal of global optimization, pp. 341–359, 1997.\n\nKevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization. Advances in\n\nneural information processing systems, 2013.\n\nKevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimization. arXiv\n\npreprint arXiv:1406.3896, 2014.\n\nR. Turner and D. Eriksson. Bayesmark: Benchmark framework to easily compare bayesian optimiza-\n\ntion methods on real machine learning tasks, 2019.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n\nJianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021a.\n\nZhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, and Jingren Zhou. Federatedscope-gnn: Towards a unified, comprehensive and efficient package for federated graph learning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 4110–4120, 2022.\n\nZheng Wang, Xiaoliang Fan, Jianzhong Qi, Chenglu Wen, Cheng Wang, and Rongshan Yu. Federated learning with fair averaging. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 1615–1623, 2021b.\n\nKang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H. Yang, Farhad Farokhi, Shi Jin, Tony Q. S. Quek, and H. Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. Trans. Info. For. Sec., pp. 3454–3469, 2020.\n\nYuexiang Xie, Zhen Wang, Daoyuan Chen, Dawei Gao, Liuyi Yao, Weirui Kuang, Yaliang Li, Bolin Ding, and Jingren Zhou. Federatedscope: A flexible federated learning platform for heterogeneity. arXiv preprint arXiv:2204.05011, 2022.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nZhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with\n\ngraph embeddings. In International conference on machine learning, pp. 40–48, 2016.\n\nLiuyi Yao, Dawei Gao, Zhen Wang, Yuexiang Xie, Weirui Kuang, Daoyuan Chen, HaoFederated hetero-task learning.\n\nhui Wang, Chenhe Dong, Bolin Ding, and Yaliang Li. https://arxiv.org/abs/2206.03436, 2022.\n\nHuanle Zhang, Mi Zhang, Xin Liu, Prasant Mohapatra, and Michael DeLucia. Automatic tuning of federated learning hyper-parameters from system perspective. arXiv preprint arXiv:2110.03061, 2021.\n\nYi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. arXiv preprint arXiv:2112.08524, 2021.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA MAINTENANCE OF FEDHPO-BENCH\n\nIn this section, we present our plan for maintaining FEDHPO-BENCH following Eggensperger et al. (2021).\n\n• Who is maintaining the benchmarking library? FEDHPO-BENCH is developed and\n\nmaintained by FedHPO-Bench team (to be de-anonymized later).\n\n• How can the maintainer of the dataset be contacted (e.g., email address)? Users can reach out to the maintainer via creating issues on the Github repository with FEDHPOBENCH label.\n\n• Is there an erratum? No.\n\n• Will the benchmarking library be updated? Yes, as we discussed in Section 5, we will add more FedHPO problems and introduce more FL tasks to the existing benchmark. We will track updates and Github release on the README. In addition, we will fix potential issues regularly.\n\n• Will older versions of\n\nsupported/hosted/maintained? All older versions are available and maintained by the Github release, but limited support will be provided for older versions. Containers will be versioned and available via AliyunOSS.\n\nthe benchmarking library continue\n\nto be\n\n• If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? Any contribution is welcome, and all commits to FEDHPO-BENCH must follow the guidance and regulations at https://github.com/ FedHPO-Bench/FedHPO-Bench-ICLR23/blob/main/README.md.\n\nB RELATED WORK\n\nHyperparameter Optimization (HPO). Generally, HPO is an optimization problem where the objective function is non-analytic, non-convex, and even non-differentiable. Therefore, most HPO methods solve such an optimization problem in a trial-and-error manner, with different strategies for balancing exploitation and exploration. Model-free methods such as random search (RS) (Bergstra & Bengio, 2012) and grid search query a set of initially determined hyperparameter configurations without any exploitation. Model-based methods such as Bayesian Optimization (BO) (Shahriari et al., 2016) employ a surrogate model to approximate the objective function. Methods in this line (Hutter et al., 2011a; Lindauer et al., 2022; Hutter et al., 2011b; Falkner et al., 2018b) mainly differ from each other in their surrogate model and how they determine the next query. There are also Evolutionary Algorithms (EAs) that iteratively maintain a population. We consider differential EAs (Storn & Price, 1997; Awad et al., 2020) in our experiments.\n\nAs training a deep neural network on a large-scale dataset is very costly, the full-fidelity function evaluations made by BO methods are often unaffordable in practice. Naturally, researchers consider trading off the precision of a function evaluation for its efficiency by, e.g., training fewer epochs and training on a subset of the data. Hyperband (Li et al., 2017) is a representative multi-fidelity method that calls the Successive Halving Algorithm (SHA) (Jamieson & Talwalkar, 2016) again and again with a different number of initial candidates. However, in each execution of SHA, the initial candidates are randomly sampled without any exploitation. To exploit the experience of previous SHA executions, researchers combine BO methods with Hyperband (Falkner et al., 2018a; Awad et al., 2021).\n\nBenchmarking HPO. AutoML-related optimization benchmarks have been proved helpful for promoting fair comparisons of related methods and reproducible research works. There have been many successful examples (Hutter et al., 2014; Hansen et al., 2021; Hase et al., 2021; Turner & Eriksson, 2019; Dong & Yang, 2020; Dong et al., 2021; Gijsbers et al., 2019). Noticeably, HPOB (Pineda-Arango et al., 2021) is highlighted by its support for benchmarking transfer-HPO methods, and HPOBench (Eggensperger et al., 2021) fills the gap of missing multi-fidelity HPO benchmarks.\n\nHowever, existing HPO benchmarks mainly focus on centralized ML, yet FL, as a promising learning paradigm, has been ignored. In this paper, we identify the uniqueness of FedHPO (see Section 2.2) and implement FEDHPO-BENCH to satisfy the demand for a FedHPO benchmark suite.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFederated Learning (FL). In this paper, we restrict our discussion of FL to the “standard” scenario introduced in Section 2.2, where FedAvg (McMahan et al., 2017) is widely adopted. Fancy FL optimization algorithms, including FedProx (Li et al., 2020b) and FedOpt (Asad et al., 2020), are mainly designed to improve the convergence rate and/or better handle the non-IIDness among clients (Wang et al., 2021a). Despite these synchronous optimization algorithms, asynchronous ones (Huba et al., 2022; Xie et al., 2022) are proposed to keep a high concurrency utility.\n\nSometimes, learning one global model is insufficient to handle the non-IIDness, which calls for personalized FL (Kairouz et al., 2019; Wang et al., 2021a). Many popular pFL algorithms, such as FedBN (Li et al., 2021b) and Ditto (Li et al., 2021a), have been incorporated into FS (Xie et al., 2022), with their unique hyperparameters exposed. Thus, we can further extend FEDHPO-BENCH by considering FedHPO tasks of optimizing the hyperparameters of such algorithms.\n\nFedHPO. When we consider HPO in the FL setting, as mentioned in Section 2.2, there is some uniqueness that brings in challenges while, at the same time, it can be leveraged by deliberately designed FedHPO methods. For example, in contrast to traditional HPO methods that query one configuration in each trial, FedEx (Khodak et al., 2021) maintains one policy for determining the clientside hyperparameters and independently samples each client’s configuration in each communication round. Different configurations may be evaluated with the same model parameters, which is in analogy to the weight-sharing idea in neural architecture search (NAS) (Liu et al., 2019), as summarized by the authors of FedEx. However, due to the non-IIDness among clients, clients’ HPO objective functions tend to be different, where determining their configurations by only one policy might be unsatisfactory. Regarding this issue, FTS (Dai et al., 2020) can be treated as a personalized FedHPO method, where each client maintains its own policy. During the learning procedure, the clients benefit each other by sharing the policies in a privacy-preserving manner and conducting Thompson sampling.\n\nIt is worth mentioning that parallel algorithms have been utilized in HPO (Jones, 2001; Hutter et al., 2012). However, in FedHPO, the clients actually do not correspond to the same black-box function due to the heterogeneity among them. Essentially, FedHPO methods instantiate the concurrent exploration idea with extra assumptions. Besides, vanilla parallel HPO methods may leak privacy in the aggregation step, which has been carefully taken into account by FTS.\n\nDynamic algorithm configuration methods (Biedenkapp et al., 2020; Adriaensen et al., 2022) employ reinforcement learning to learn policies for online adjustments of algorithm parameters, since different parameter values can be optimal at different stages. In contrast to DAC methods, the policy π learned in FedEx is responsible for determining the optimal lower-level response of the bi-level optimization problem discussed in Section 2.2, which can be regarded as a multi-armed bandit problem rather than a Markov decision process. In other words, combined with the concurrent exploration strategy, FedEx tries out one arm at a client in each round, where the underlying reward function is assumed to be unchanged across the clients and the whole training course.\n\nAs an emerging research topic, existing works relating to FedHPO include Fed-Tuning (Zhang et al., 2021) concerning about system-related performance, learning rate adaptation (Koskela & Honkela, 2020), FLoRA for Gradient Boosted Decision Trees (GBDT), online adaptation scheme-based method (Mostafa, 2020), Auto-FedRL (Guo et al., 2022) for RL-based hyperparameter adaptation, and an insightful comparison between local and global HPO (Holly et al., 2021). These methods can also be easily incorporated into FS, enabling FEDHPO-BENCH to benchmark them.\n\nB.1 RELATION TO HPOBENCH\n\nHPOBench (Eggensperger et al., 2021) is a collection of multi-fidelity HPO benchmarks, highlighted by their efficiency, reproducibility, and flexibility. These benchmarks can be accessed in either tabular, surrogate, or raw mode. On the one hand, the tabular and surrogate modes enable function evaluation without truly executing the corresponding ML algorithm and thus are efficient. On the other hand, the raw mode means execution in a docker container, which ensures reproducibility. HPOBench provides twelve families of benchmarks that correspond to different data domains, model types, fidelity spaces, etc., and thus flexible usages to validate HPO methods. This collection of HPO benchmarks can promote fair comparisons of related works and reproducible research work, so HPOBench has gained more and more attention from the community.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nAs pointed out in Section 2.2 that evaluating the objective function that corresponds to an FL algorithm is extremely expensive, FEDHPO-BENCH also prepares tabular and surrogate modes for users to avoid truly executing FL courses. Meanwhile, we provide the raw mode to truly execute an FL course in the docker container of FederatedScope (FS) (Xie et al., 2022).\n\nSharing the same modes, a question naturally arises—is it possible to reuse HPOBench’s interface for FEDHPO-BENCH? We answer this question by discussing their commonality and the unique ingredients of FEDHPO-BENCH:\n\nCommonality. As the code snippet in Figure 2 shows, the instantiation of a benchmark class, the “ConfigSpace” package based specification of search space, and the protocol for the interaction between an HPO method and a benchmark object are roughly consistent with HPOBench.\n\nUniqueness. In addition to a collection of benchmarks, FEDHPO-BENCH is flexible in terms of enabling users to tailor one benchmark to their scenarios (see Section 3.2). To this end, users are allowed to instantiate a specific benchmark object with extra optional arguments:\n\n• Privacy budget with which function evaluation corresponds to the execution of NbAFL (Wei et al., 2020) instead of vanilla FedAvg. Taking the tabular mode, for example, this means looking up a privacy budget-specific table.\n\n• The type of fairness metric and its strength with which FEDHPO-BENCH will consider a vector-valued objective function (i.e., client-wise results) rather than a scalar-valued objective function. Besides, the return value of calling the function evaluation will be the mean performance regularized by the specified fairness regularizer.\n\n• The parameter(s) for our system model with which the execution time is estimated regarding the user’s system condition. Without using a system model, FEDHPO-BENCH can provide the recorded execution time in the creation of this benchmark.\n\nCurrently, we implement the interfaces of FEDHPO-BENCH by ourselves, where the style of our interfaces is kept similar to HPOBench for the convenience of users who are familiar with HPOBench. We also provide several examples (https://github.com/FedHPO-Bench/ FedHPO-Bench-ICLR23/tree/main/demo) to access our tabular, surrogate, and raw benchmarks by implementing HPOBench’s abstract base class. As a first step, we are going to contribute more such subclasses to the repository of HPOBench so that users can access our benchmarks via HPOBench’s interfaces, where flexible customization cannot be provided temporarily. In our next step, we plan to extend the interfaces of HPOBench such that the benchmarks of FEDHPO-BENCH can be accessed with our proposed flexible customization.\n\nC HPO METHODS\n\nAs shown in Table 6, we provide an overview of the optimizers (i.e., HPO methods) we use in this paper.\n\nTable 6: Overview of the optimizers from widely adopted libraries.\n\nName\n\nModel\n\nPackages\n\nversion\n\n-\n\nRS (Bergstra & Bengio, 2012) BOGP (Hutter et al., 2011a; Lindauer et al., 2022) GP RF BORF (Hutter et al., 2011b) KDE BOKDE (Falkner et al., 2018b) DE (Storn & Price, 1997; Awad et al., 2020) -\nHB (Li et al., 2017) -\nKDE BOHB (Falkner et al., 2018a) DEHB (Awad et al., 2021) -\nTPE TPEMD (Akiba et al., 2019) TPE TPEHB (Akiba et al., 2019)\n\nHPBandster SMAC3 SMAC3 HPBandster DEHB HPBandster HPBandster DEHB Optuna Optuna\n\n0.7.4 1.3.3 1.3.3 0.7.4 git commit 0.7.4 0.7.4 git commit 2.10.0 2.10.0\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nC.1 BLACK-BOX OPTIMIZERS\n\nRS (Random search) is a priori-free HPO method, i.e., each step of the search does not exploit the already explored configuration. The random search outperforms the grid search within a small fraction of the computation time.\n\nBOGP is a Bayesian optimization with a Gaussian process model. BOGP uses a Matérn kernel for continuous hyperparameters, and a hamming kernel for categorical hyperparameters. In addition, the acquisition function is expected improvement (EI).\n\nBORF is a Bayesian optimization with a random forest model. We set the hyperparameters of the random forest as follows: the number of trees is 10, the max depth of each tree is 20, and we use the default setting of the minimal samples split, which is 3.\n\nBOKDE is a Bayesian optimization with kernel density estimators (KDE), which is used in BOHB (Falkner et al., 2018a). It models objective function as Pr(x | ygood) and Pr(x | ybad). We set the hyperparameters for BOKDE as follows: the number of samples to optimize EI is 64, and 1/3 of purely random configurations are sampled from the prior without the model; the bandwidth factor is 3 to encourage diversity, and the minimum bandwidth is 1e-3 to keep diversity.\n\nDE uses the evolutionary search approach of Differential Evolution. We set the mutation strategy to rand1 and the binomial crossover strategy to bin 3. In addition, we use the default settings for the other hyperparameters of DE, where the mutation factor is 0.5, crossover probability is 0.5, and the population size is 20.\n\nC.2 MULTI-FIDELITY OPTIMIZERS\n\nHB (Hyperband) is an extension on top of successive halving algorithms for the pure-exploration nonstochastic infinite-armed bandit problem. Hyperband makes a trade-off between the number of hyperparameter configurations and the budget allocated to each hyperparameter configuration. We set η to 3, which means only a fraction of 1/η of hyperparameter configurations goes to the next round.\n\nBOHB combines HB with the guidance and guarantees of convergence of Bayesian optimization with kernel density estimators. We set the hyperparameter of the BO components and the HB components of BOHB to be the same as BOKDE and HB described above, respectively.\n\nDEHB combines the advantages of the bandit-based method HB and the evolutionary search approach of DE. The hyperparameter of DE components and BO components are set to be exactly the same as DE and HB described above, respectively.\n\nTPEMD is implemented in Optuna and uses Tree-structured Parzen Estimator (TPE) as a sampling algorithm, where on each trial, TPE fits two Gaussian Mixture models for each hyperparameter. One is to the set of hyperparameters with the best performance, and the other is to the remaining hyperparameters. In addition, it uses the median stopping rule as a pruner, which means that it will prune if the trial’s best intermediate result is worse than the median (MD) of intermediate results of previous trials at the same step. We use the default settings for both TPE and MD.\n\nTPEHB is similar to TPEMD described above, which uses TPE as a sampling algorithm and HB as pruner. We set the reduction factor to 3 for HB pruner, and all other settings use the default ones.\n\nD DATASETS\n\nAs shown in Table 7, we provide a detailed description of the datasets we use in current FEDHPOBENCH. For comprehensiveness, we use 16 FL datasets from 5 domains, including CV, NLP, graph, tabular, and recommendation (Xie et al., 2022; Wang et al., 2022; Eggensperger et al., 2021). Some of them are inherently real-world FL datasets, while others are simulated FL datasets split by the splitter modules of FS. Notably, the name of datasets from OpenML is the ID of the corresponding task.\n\n3Please refer to https://github.com/automl/DEHB/blob/master/README.md for details.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: Statistics of the datasets used in current FEDHPO-BENCH.\n\nName\n\n#Client\n\nSubsample\n\n#Instance\n\n#Class\n\nSplit by\n\nFMNIST CIFAR-10 CoLA SST-2 Cora CiteSeer PubMed Hetero-task credit-g31 vehicle53 kc13917 blood-transf..10101 Australian146818 car146821 segment146822 FedNetflix Twitter\n\n3,550 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5 5\n5 480,189 660,120\n\n5% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 0.5%\n\n805,263 60,000 10,657 70,042 2,708 4,230 19,717 6,760 1,000 846 2,109 748 690 1,728 2,310 ≈100,000,000 1,600,498\n\n62 10 2\n2 7\n6 3\n2~6 2\n4 2\n2 2\n4 7\n5 2\n\nWriter LDA LDA LDA Community Community Community Task LDA LDA LDA LDA LDA LDA LDA User User\n\nFEMNIST is an FL image dataset from LEAF (Caldas et al., 2018), whose task is image classification. Following (Caldas et al., 2018), we use a subsample of FEMNIST with 200 clients, which is round 5%. And we use the default train/valid/test splits for each client, where the ratio is 60% : 20% : 20%.\n\nCIFAR-10 (Krizhevsky et al., 2009) is from Tiny Images dataset and consists of 60,000 32 × 32 color images, whose task is image classification. We split images into 5 clients by latent dirichlet allocation (LDA) to produce statistical heterogeneity among these clients. We split the raw training set to training and validation sets with a ratio 4 : 1, so that ratio of final train/valid/test splits is 66.7%:16.67%:16.67%.\n\nSST-2 is a dataset from GLUE (Wang et al., 2018) benchmark, whose task is binary sentiment classification for sentences. We also split these sentences into 5 clients by LDA. In addition, we use the official train/valid/test splits for SST-2.\n\nCoLA is also a dataset from GLUE benchmark, whose task is binary classification for sentences— whether it is a grammatical English sentence. We exactly follow the experimental setup in SST-2.\n\nCora & CiteSeer & PubMed (Sen et al., 2008; Yang et al., 2016) are three widely adopted graph datasets, whose tasks are node classification. Following FS-G (Wang et al., 2022), a community splitter is applied to each graph to generate five subgraphs for each client. We also split the nodes into train/valid/test sets, where the ratio is 60%:20%:20%.\n\nHetero-task is a graph classification dataset adopted from Graph-DC (Yao et al., 2022), which contains 5 clients. Each client has different but similar graph classification task, such as molecular attribute prediction. In addition, we set the ratio of train/valid/test splits in each client to 80%:10%:10%.\n\nTabular datasets are consist of 7 tabular datasets from OpenML (Bischl et al., 2017), whose task ids (name of source data) are 31 (credit-g), 53 (vehicle), 3917 (kc1), 10101 (blood-transfusion-servicecenter), 146818 (Australian), 146821 (car) and 146822 (segment). We split each dataset into 5 clients by LDA, respectively. In addition, we set the ratio of train/valid/test splits to 80%:10%:10%.\n\nFedNetflix is a recommendation dataset from The Netflix Prize (Bennett & Lanning, 2007), whose task is to predict the ratings between users and movies. Netflix consists of around 100 million ratings between 480,189 users and 171,770 movies. We split the Netflix dataset into 480,189 clients by users. In addition, we set the ratio of train/valid/test splits to 80%:10%:10%.\n\nTwitter is a sentiment analysis dataset from LEAF (Caldas et al., 2018), whose task is to determine sentiment of sentences. We use a subsample of Twitter with around 3300 clients. Moreover, we use the train/valid/test splits for each client, where the ratio is 80% : 10% : 10%. It is worth noting that\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nthe average number of samples is only 1.94, which means some clients do not have valid split or test split, and we evaluate the performance on a shared test split merged by all clients.\n\nE SYSTEM MODEL\n\nIn this section, we will discuss the system model in detail we have proposed and implemented. The total execution time of FL consists of the time consumed by communication and the time consumed by calculation, thus, the system model is as follows:\n\nT (f, λ, b) = Tcomm(f, λ, b) + Tcomp(f, λ, b),\n\nTcomm(f, λ, b) =\n\nSdown(f, λ) Bdown\n\n+\n\nSup(f, λ) Bup\n\n+ α(N ),\n\nTcomp(f, λ, b) = E\n\nT (client)\n\ni\n\n∼Exp(·|\n\nc(f,λ,b) ),i=1,...,N [max({T (client)\n\ni\n\n1\n\n(1)\n\n})] + T (server)(f, λ, b),\n\nwhere N denotes the number of clients sampled in this round, α(N ) denotes the latency, which is an increasing function of N but is independent of the message size (contains the time needed to establish the transmission between the server and the clients), S(f, λ) denotes the download/upload size, B denotes the download/upload bandwidth of client, T (server) is the time consumed by server-side computation, and T (client) denotes the computation time consumed by i-th client, which is sampled from an exponential distribution with c(f, λ, b) as its mean. This design intends to simulate the heterogeneity among clients’ computational capacity, where the assumed exponential distribution has been widely adopted in system designs (Wang et al., 2021a) and is consistent with real-world applications (Huba et al., 2022).\n\ni\n\nWe provide default parameters of our system model, including c(f, λ, b), Bup, Bdown, and T (server), based on observations collected from FL trials we have conducted and real-world network bandwidth. Users are allowed to specify these parameters according to their scenarios or other system statistic providers, e.g., estimating the computation time of stragglers by sampling from FedScale (Lai et al., 2022). As for the network bandwidth, we set Bdown ∼ 0.75MB/secs, Bup ∼ 0.25MB/secs following (Lai et al., 2022; Wang et al., 2021a). The default value of c(f, λ, b) is obtained by averaging the recorded client-wise time costs in trials of tabular mode benchmarks. Due to the limit on the number of ports of the server, we set the default value of the maximum number of connections in calculating α(N ) to 65535.\n\nTo implement our system model, we use the following proposition to calculate Eq. 1 analytically, where we use c as a shorthand for c(f, λ, b) to keep clarity.\n\nProposition 1. When the computation time of clients is identically independently distributed, following an exponential distribution Exp(·| 1 c ), then the expected time for the straggler of N uniformly sampled clients is (cid:80)N\n\nc\n\ni=1\n\ni .\n\nWhat we need to calculate is the expected maximum of i.i.d. exponential random variable. Proposition 1 states that, for N exponential variables independently drawn from Exp(·| 1 c ), the expectation is (cid:80)N i . There are many ways to prove this useful proposition, and we provide a proof starting from\n\ni=1\n\nc\n\nstudying the minimum of the exponential random variables.\n\nProof. At first, the minimum of N such random variables obeys Exp(·| N c ) (Graham et al., 1989). Denoting the i-th minimum of them by Ti, T1 ∼ Exp(·| N c ) and TN is what we are interested in. Meanwhile, it is well known that exponential distribution is memoryless, namely, Pr(X > s + t|X > s) = Pr(X > t). Thus, T2 − T1 obeys the same distribution as the minimum of N − 1 such random variables, that is to say, T2 − T1 is a random variable drawn from Exp(·| N −1 ). Similarly, (Ti+1 − Ti) ∼ Exp(·| N −i\n\nc ), i = 1, . . . , N − 1. Thus, we have:\n\nc\n\nE[TN ] = E[T1 +\n\nN −1 (cid:88)\n\n(Ti+1 − Ti)] =\n\ni=1\n\nc N\n\n+\n\nN −1 (cid:88)\n\ni=1\n\nc N − i\n\n=\n\nN (cid:88)\n\ni=1\n\nc i\n\n,\n\n(2)\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: A general algorithmic view for FedHPO methods: They are allowed to concurrently explore different client-side configurations in the same round of FL, but the clients are heterogeneous, i.e., corresponding to different functions f (c)\n\n(·). Operators in brackets are optional.\n\ni\n\nwhich concludes this proof.\n\nIt is worth noting that we provide several optional system models. For example, for point-to-point transport protocols, Tcomm should contain the time the server sends the model to each client.\n\nF DETAILS OF THE IMPLEMENTATIONS OF FEDEX AND FTS\n\ni\n\ni with this global one and then sample a hyperparameter configuration λ(c)\n\nWe first present a general algorithmic view in Figure 6, which unifies several such methods as well as their personalized counterparts. At a high level, a policy π for determining the optimal lower-level response λ(c)∗ = minλ(c) f (λ(s), λ(c)) is to be federally learned, along with the FL course itself. In the t-th communication round: (1) In addition to the model θ(t), either the policy π(t) or its decisions λ(c) is also broadcasted. (2) For the i-th client, if π(t) is received, it needs to synchronize its local policy π(t) from its local policy. (3) Either received or locally sampled, λ(c) FL, which results in updated local model θ(t+1) (cid:17) (client-specific) function evaluation f (c) a local policy πi, it is updated w.r.t. (λi, fi(λi)) to produce π(t+1) model θ(t+1) , f (c) (cid:16)\n\n. (5) For personalized FedHPO methods that maintain\n\nis used to specify the local update procedure of\n\n, either the local policy π(t+1)\n\nis evaluated to provide the result of\n\n. (6) In addition to the local\n\n. (4) Then θ(t+1)\n\nor the feedback\n\nis sent to the server.\n\nλ(c)\n\nλ(c)\n\nλ(c)\n\n(cid:17)(cid:17)\n\n(cid:17)(cid:17)\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ns into θ(t+1) and π(t+1)\n\ns/\n\ni\n\nλ(c)\n\n, f (c)\n\ni\n\ni\n\nλ(c)\n\ni\n\ns into π(t+1),\n\n(7) Finally, the server aggregates θ(t+1) respectively.\n\ni\n\nIn FedEx (Khodak et al., 2021), λis are independently sampled from π, and the aggregation operator “aggrp” is exponential gradient descent. In FTS (Dai et al., 2020), the broadcasted policy π(t) is the samples drawn from all clients’ posterior beliefs. The synchronous operator “syncp” can be regarded as mixing Gaussian process (GP) models. The update operator “updatep” corresponds to updating local GP model. Then a sample drawn from local GP posterior belief is regarded as π(t+1) and uploaded. Finally, the aggregation operator “aggrp” is packing received samples together.\n\ni\n\nG STUDIES ABOUT THE NEW FIDELITY\n\nIn FL, a larger client_sample_rate leads to a minor variance of the aggregated model in each round, which is believed to need less #round for convergence and to perform better. Therefore, we tend to set the client_sample_rate as close to 1 as possible. However, according to our system model in Section 3.2, a large client_sample_rate leads to an increase in latency (α(N )), which makes the communication cost higher. To answer RQ3, we use tabular mode and study the trade-off between these two fidelity dimensions: client_sample_rate and #round. We simulate two distinct system conditions by specifying different parameters for our system model.\n\nProtocol. We compare the performance of HB with different client_sample_rates to learn a 2-layer CNN with 2,048 hidden units on FEMNIST. To simulate a system condition with bad network status, we set the upload bandwidth Bup to 0.25MB/second and the download bandwidth Bdown to 0.75MB/second (Wang et al., 2021a). As for good network status, we set the upload bandwidth Bup to 0.25GB/second and the download bandwidth B(down) to 0.75GB/second. In both cases, we consider\n\n21\n\nServerUnder review as a conference paper at ICLR 2023\n\n(a) With bad network status\n\n(b) With good network status\n\nFigure 7: Performances of different client_sample_rate under different system conditions.\n\ndifferent computation overhead so that it is negligible and significant, respectively. As for the rest settings, we largely follow that in Section 4.1.\n\nResults and Analysis. We have an answer to RQ3: with the same time budget, the FL procedure with a lower client_sample_rate achieves a better result than higher client_sample_rate with the bad network status. In comparison, that with a higher client_sample_rate achieves a better result than lower client_sample_rate in the good network status. In conclusion, this study suggests a best practice of pursuing a more economic accuracy-efficiency trade-off by balancing client_sample_rate with #round, w.r.t. the system condition. Better choices tend to achieve more economical accuracyefficiency trade-offs for FedHPO.\n\nH DETAILS ON FEDHPO-BENCH BENCHMARKS\n\nFEDHPO-BENCH consists of serveral categories of benchmarks on the different datasets (see Appendix D) with three modes. If not specified, we use the model as the name of the benchmark in cross-silo scenario. In this part, we provide more details about how we construct the FedHPO problems provided by current FEDHPO-BENCH and the three modes to interact with them.\n\nH.1 CATEGORY\n\nWe categorize our benchmarks by model types. Each benchmark is designed to solve specific FL HPO problems on its data domain, wherein CNN benchmark on CV, BERT benchmark on NLP, GNN benchmark on the graphs, and LR & MLP benchmark on tabular data. All benchmarks have several hyperparameters on configuration space and two on fidelity space, namely sample rate of FL and FL round. And the benchmarks support several FL algorithms, such as FedAvg and FedOpt.\n\nCNN benchmark learns a two-layer CNN with 2048 hidden units on FEMNIST and 128 hidden units on CIFAR-10 with five hyperparameters on configuration space that tune the batch size of the dataloader, the weight decay, the learning rate, the dropout of the CNN models, and the step size of local training round in client each FL communication round. The tabular and surrogate mode of the CNN benchmark only supports FedAvg due to our limitations in computing resources for now, but we will update FEDHPO-BENCH with more results as soon as possible.\n\nBERT benchmark fine-tunes a pre-trained language model, BERT-Tiny, which has two layers and 128 hidden units, on CoLA and SST-2. The BERT benchmark also has five hyperparameters on configuration space which is the same as CNN benchmark. In addition, the BERT benchmark support FedAvg and FedOpt with all three mode.\n\nGNN benchmark learns a two-layer GCN with 64 hidden units on Cora, CiteSeer and PubMed. The GNN benchmark has four on hyperparameters configuration space that tune the weight decay, the learning rate, the dropout of the GNN models, and the step size of local training round in client each FL communication round. The GNN benchmark support FedAvg and FedOpt with all three mode.\n\nHetero benchmark learns a two-layer GCN with 64 hidden units as backbone to be aggregated. The Hetero benchmark has two on hyperparameters configuration space that tune the learning rate and the step size of local training round in client each FL communication round. For each client, there are\n\n22\n\n0.00.20.40.60.81.0Fraction of budget0.50.00.51.0Loss (log)20%40%60%80%100%0.00.20.40.60.81.0Fraction of budget0.50.00.51.0Loss (log)20%40%60%80%100%Under review as a conference paper at ICLR 2023\n\nTable 8: The search space of our benchmarks, where continuous search spaces are discretized into several bins under the tabular mode.\n\nBenchmark\n\nName\n\nType Log\n\n#Bins\n\nRange\n\nCNN\n\nBERT\n\nGNN\n\nHetero\n\nLR\n\nMLP\n\nbatch_size weight_decay dropout step_size learning_rate momentum learning_rate client_sample_rate round batch_size weight_decay dropout step_size learning_rate momentum learning_rate client_sample_rate round weight_decay dropout step_size learning_rate momentum learning_rate client_sample_rate round learning_rate step_size client_sample_rate round batch_size weight_decay step_size learning_rate momentum learning_rate client_sample_rate round batch_size weight_decay step_size learning_rate depth width momentum learning_rate client_sample_rate round\n\nint float float int float float float float int int float float int float float float float int float float int float float float float int float int float int int float int float float float float int int float int float int int float float float int\n\nClient\n\nServer\n\nFidelity\n\nClient\n\nServer\n\nFidelity\n\nClient\n\nServer\n\nFidelity\n\nClient\n\nFidelity\n\nClient\n\nServer\n\nFidelity\n\nClient\n\nServer\n\nFidelity\n\n× ×\n× ×\n✓ ×\n× ×\n× ×\n× ×\n× ✓\n× ×\n× ×\n× ×\n× ✓\n× ×\n× ×\n✓ ×\n× ×\n✓ ×\n× ✓\n× ×\n× ×\n✓ ×\n× ✓\n× ✓\n× ×\n× ×\n\n- 4\n2 4\n10 2\n3 5\n250 -\n4 2\n4 10 2\n3 5\n40 4\n2 8\n10 2\n3 5\n500 2\n2 5\n500 7\n4 4\n6 2\n3 5\n500 7\n4 4\n6 3\n7 2\n3 5\n500\n\n{16, 32, 64} [0, 0.001] [0, 0.5] [1, 4] [0.01, 1.0] [0.0, 0.9] [0.1, 1.0] [0.2, 1.0] [1, 500] {8, 16, 32, 64, 128} [0, 0.001] [0, 0.5] [1, 4] [0.01, 1.0] [0.0, 0.9] [0.1, 1.0] [0.2, 1.0] [1, 40] [0, 0.001] [0, 0.5] [1, 8] [0.01, 1.0] [0.0, 0.9] [0.1, 1.0] [0.2, 1.0] [1, 500] [0.001, 0.01] [1, 4] [0.2, 1.0] [1, 500] [4, 256] [0, 0.001] [1, 4] [0.00001, 1.0] [0.0, 0.9] [0.1, 1.0] [0.2, 1.0] [1, 500] [4, 256] [0, 0.001] [1, 4] [0.00001, 1.0] [1, 3] [16, 1024] [0.0, 0.9] [0.1, 1.0] [0.2, 1.0] [1, 500]\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\npersonalized encoder and classifier to handle different tasks. Thus, the Hadamard product of each client’s configuration makes the search space.\n\nLR benchmark learns an lr on seven tasks from OpenML, see Appendix D for details. The LR benchmark has four on hyperparameters configuration space that tune the batch size of the dataloader, the weight decay, the learning rate, and the step size of local training round in client each FL communication round. The LR benchmark support FedAvg and FedOpt with all three mode.\n\nMLP benchmark’s the vast majority of settings are the same as LR benchmark. But in particular, we add depth and width of the MLP to search space in terms of model architecture. The MLP benchmark also support FedAvg and FedOpt with all three mode.\n\nCross-device. In cross-device scenarios, there can be large number of clients in total, but only a few participate in each communication round. This benchmark contains two datasets, Twitter and FedNetflix. We use a bag of words model with LR and tune the learning_rate, weight_decay, and step_size of local training round in Twitter. As for FedNetflix, we tune an HMFNet (Li et al., 2021c) in the learning_rate, batch_size, and step_size of local training round. Due to the time limit, the results FedNetflix is incomplete, and we present the ECDF of Twitter in Figure 9.\n\nH.2 MODE\n\nFollowing HPOBench (Eggensperger et al., 2021), FEDHPO-BENCH provides three different modes for function evaluation: the tabular mode, the surrogate mode, and the raw mode. The valid input hyperparameter configurations and the speed of acquiring feedback vary from mode to mode. Users can choose the desired mode according to the purposes of their experiments.\n\nTabular mode. The idea is to evaluate the performance of many different hyperparameter configurations in advance so that users can acquire their results immediately. For efficient function evaluation, we implement the tabular mode of FEDHPO-BENCH by running the FL algorithms configured by the grid search space in advance from our original search space (see Table 8). For hyperparameters whose original search space is discrete, we just preserve its original one. As for continuous ones, we discretize them into several bins (also see Table 8 for details). To ensure that the results are reproducible, we execute the FL procedure in the Docker container environment. Each specific configuration λ is repeated three times with different random seeds, and the resulted performances, including loss, accuracy and f1-score under train/validation/test splits, are averaged and adopted as the results of f (λ). Users can choose the desired metric as the output of the black-box function via FEDHPO-BENCH’s APIs. Besides, we provide not only the results of f (λ) (i.e., that with full-fidelity) but also results of f (λ, b), where b is enumerated across different #round and different client_sample_rate. Since executing function evaluation is much more costly in FL than traditional centralized learning, such lookup tables are precious. In creating them, we spent about two months of computation time on six machines, each with four Nvidia V100 GPUs. Now we make them publicly accessible via the tabular mode of FEDHPO-BENCH.\n\nSurrogate mode.\n\nAs tabular mode has discretized the original search space and thus cannot respond to queries other than the grids, we train random forest models on these lookup tables, i.e., {(λ, b), f (λ, b))}. These models serve as a surrogate of the functions to be optimized and can answer any query λ by simply making an inference. Specifically, we conduct 10-fold cross-validation to train and evaluate random forest models (implemented in scikit-learn (Pedregosa et al., 2011)) on the tabular data. Meanwhile, we search for suitable hyperparameters for the random forest models with the number of trees in {10, 20} and the max depth in {10, 15, 20}. The mean absolute error (MAE) of the surrogate model w.r.t. the true value is within an acceptable threshold. For example, in predicting the true average loss on the CNN benchmark, the surrogate model has a training error of 0.00609 and a testing error of 0.00777. In addition to the off-the-shelf surrogate models we provide, FEDHPO-BENCH offers tools for users to build brand-new surrogate models. Meanwhile, we notice the recent successes of neural network-based surrogate, e.g., YAHPO Gym (Pfisterer et al., 2022), and we will also try it in the next version of FEDHPO-BENCH.\n\nRaw mode. Both of the above modes, although they can respond quickly, are limited to pre-designed search space. Thus, we introduce raw mode to FEDHPO-BENCH, where user-defined search spaces are allowed. Once FEDHPO-BENCH’s APIs are called with specific hyperparameters, a containerized\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nand standalone FL procedure (supported by FS) will be launched. It is worth noting that although we use standalone simulation to eliminate the communication cost, raw mode still consumes much more computation cost than tabular and surrogate modes.\n\nH.3 NEW HYPERPARAMETERS\n\nThe FL setting introduces new hyperparameters such as server-side learning_rate, momentum for FedOpt (Asad et al., 2020) and client-side #local_update_step. Different FL algorithms have different parameters, which correlate with hyperparameters related to general ML procedures. In this section, we first adopt FedProx (Li et al., 2020b) to study the impact of server-side hyperparameters mu, the coefficient of the regular term, on the results. And then we compare the landscape of the federated learning method and non-federated method.\n\nH.3.1 TRENDS WITH DIFFERENT REGULARITY IN FEDPROX\n\nTo extend the tabular benchmarks with more FL algorithms, we adopt FedProx (Li et al., 2020b) to GNN benchmark. Based on Table 8, we tune the server-side hyperparameters mu, the coefficient of the regular term, in {0.1, 1.0, 5.0} to study the trends with different regularity in FedProx. We show the landscape in Figure 10 with learning rate in [0.01, 1.0] and mu in [0.1, 5.0], and we observe that when the learning rate is low, the effect of mu has little impact on the accuracy; however, when the learning rate is large, the increase of mu can seriously damage the accuracy.\n\nH.3.2 LANDSCAPES ON ML-RELATED HYPERPARAMETERS\n\nIn this section, to study the validation loss landscape of the federated learning method (FedAvg) and non-federated method (Isolated), we consider learning_rate and batch_size, the hyperparameters of the ML algorithm, as the coordinate axis to build the loss landscapes. We fix other ML-related hyperparameters weight_decay to 0.0, dropout to 0.5 for both FedAvg and Isolated, which is the best configuration chosen from the tabular benchmark <CNN, FEMNIST, FedAvg> under 1.0 client_sample_rate. As the loss landscapes shown in Figure 8 with learning rate in [0.01, 1.0] and batch size in 16, 32, 64, we observe that the FedAvg with a higher learning rate achieves better results, while the non-federated method (Isolated) prefer a lower learning rate. Their differences suggest the uniqueness of FedHPO’s objective functions.\n\n(a) FedAvg\n\n(b) Isolated\n\nFigure 8: Landscape with the hyperparameters of the ML algorithm on FEMNIST.\n\nH.4 DATA ANALYTICS\n\nH.4.1 TRENDS IN DIFFERENT PRIVACY BUDGETS\n\nWe extend the tabular benchmarks with different levels of privacy budgets in FEMNIST and Cora. To explore the trends of optimal configurations under different privacy budgets, we adopt NbAFL (Wei\n\n25\n\n0.760.780.80.820.840.860.10.20.30.40.50.60.70.8Under review as a conference paper at ICLR 2023\n\net al., 2020) with ε ∈ {1, 10, 20}. We observe that the best configuration varies under different levels of privacy budgets in and Cora, as shown in Table 10. Under different privacy budgets, a large step_size all leads to a good performance. However, when the noise is intense, a higher learning_rate is preferred, while a lower learning_rate will perform better when the noise is weak.\n\nε 1\n10 20\n\nlearning_rate weight_decay 1.0 0.59948 0.59948\n\n0.001 0.0 0.001\n\ndropout 0.5 0.5 0.5\n\nstep_size Test Acc. (%) 7\n6 6\n\n63.87 ± 6.38 87.02 ± 1.16 87.30 ± 0.54\n\nFigure 9: ECDF on Twitter.\n\nTable 10: Best configuration with different levels of privacy budgets in Cora.\n\nH.4.2 ERRORS OF SURROGATE BENCHMARKS\n\nAs we mentioned in Section H.2, we report the regression error of training surrogate model in Table 12. Meanwhile, we present the mean rank over time of optimizers with surrogate modes in Figure 22 and Figure 23. Compared to the results of tabular modes in 13 and 14, BOGP shows good performance in both modes, while Random Search does not. This show the consistent performance of the same optimizer when it interplays with surrogate and tabular benchmarks.\n\nModel CNN\n\nBERT\n\nGNN\n\nDataset\n\nAlgo.\n\nTrain MAE Test MAE\n\nSST2\n\nCoLA\n\nFEMNIST FedAvg FedAvg FedOpt FedAvg FedOpt FedAvg FedOpt FedAvg FedOpt FedAvg FedOpt\n\nCiteSeer\n\nPubMed\n\nCora\n\n0.00609 0.04724 0.02426 0.02597 0.02802 0.04702 0.05703 0.01334 0.01652 0.04042 0.04816\n\n0.00777 0.05454 0.02959 0.03227 0.03166 0.04839 0.05893 0.01381 0.01717 0.04148 0.05699\n\nFigure 10: Landscape with the different regularity of FedProx on Cora.\n\nTable 12: The regression error of surrogate models.\n\nH.4.3 VARIANCE OF DIFFERENT SAMPLE RATE\n\nAs we build our tabular benchmark from FL courses executed in docker images provided by FS, we can fully reproduce all the results given the same random seed in raw mode. Other than that, to study the noise of different federated optimization, we analyze the variance of validation loss with 500 rounds under different sample rates in FEMNIST. And the mean standard deviation validation loss is {1.945e-2, 1.7e-2, 1.728e-2, 1.715e-2, 1.43e-2} with sample rate {0.2, 0.4, 0.6, 0.8, 1.0}, which shows that the higher sample rate tends to have lower variance. The reason is apparent: the lower the sampling rate, the more inconsistent the set of clients sampled during the training process leads to this error.\n\nH.4.4 ECDF WITH DIFFERENT HETEROGENEITY\n\nWe extend our LR benchmarks with different heterogeneity settings. As we discussed in Appendix D, we split the tabular dataset with LDA, whose α is in {0.1, 0.5, 0.7} (the smaller the alpha, the more the heterogeneous). We show the ECDF of the normalized regret of evaluated configurations with different α in Figure 11, which shows that as the α decreases, it is harder to find a good configuration. This phenomenon shows the necessity of tuning hyperparameters in FL with heterogeneous data.\n\n26\n\n0.00.20.40.60.81.0Normalized regret0.00.20.40.60.81.0P(X <= x)Twitter0.20.30.40.50.60.70.80.9Under review as a conference paper at ICLR 2023\n\n(a) α = 0.1\n\n(b) α = 0.5\n\n(c) α = 0.7\n\nFigure 11: Empirical Cumulative Distribution Functions with different heterogeneity in LR benchmark.\n\nI MORE RESULTS\n\nIn this section, we show the detailed experimental results of the optimizers on FEDHPO-BENCH benchmarks under different modes. We first report the averaged best-seen validation loss, from which the mean rank over time for all optimizers can be deduced. Due to time and computing resource constraints, we do not have a complete experimental result of the raw mode, which we will supplement as soon as possible.\n\nI.1 TABULAR MODE\n\nFollowing Section 4.1, we show the overall mean rank overtime on all FedHPO problems with FedOpt, whose pattern is similar to that of FedAvg in Figure 4. Then, we report the final results with FedAvg and FedOpt in Table 13 and 14, respectively. Finally, we report the mean rank over time in Figure 13-21. Due to time and computing resource constraints, the results on CNN benchmark are incomplete (lacking that with FedOpt), which we will supplement as soon as possible.\n\n(a) BBO\n\n(b) MF\n\n(c) All\n\nFigure 12: Mean rank over time on all FedHPO problems (with FedOpt).\n\nTable 13: Final results of the optimizers on tabular mode with FedAvg (lower is better).\n\nbenchmark\n\nRS\n\nBOGP\n\nBORF\n\nBOKDE\n\nDE\n\nHB\n\nBOHB\n\nDEHB\n\nTPEMD\n\nTPEHB\n\nCNNFEMNIST BERTSST-2 BERTCoLA GNNCora GNNCiteSeer GNNPubMed LR31 LR53 LR3917 LR10101 LR146818 LR146821 LR146822 MLP31 MLP53 MLP3917 MLP10101 MLP146818 MLP146821 MLP146822\n\n0.4969±0.0054 0.435±0.0142 0.6151±0.0014 0.3265±0.0042 0.6469±0.0052 0.5262±0.0167 0.6821±0.1299 1.6297±0.1628 1.8892±0.2647 0.548±0.0002 0.5294±0.0006 0.4733±0.0025 0.4581±0.0202 0.5899±0.0032 0.7795±0.0156 0.3863±0.0099 0.4054±0.0113 0.5089±0.0092 0.184±0.0187 0.2839±0.0259\n\n0.4879±0.0051 0.4276±0.0082 0.6148±0.0014 0.3258±0.0062 0.6442±0.0046 0.5146±0.0136 0.6308±0.0292 1.7288±0.2306 1.7561±0.2538 0.5483±0.0002 0.5291±0.0002 0.464±0.0068 0.4481±0.0102 0.5891±0.0052 0.7373±0.0186 0.3937±0.0094 0.4217±0.0065 0.4997±0.0072 0.1251±0.0167 0.2892±0.0363\n\n0.4885±0.0065 0.4294±0.0071 0.6141±0.0016 0.326±0.0063 0.6499±0.0069 0.5169±0.0193 0.6382±0.0435 1.6116±0.2017 1.7186±0.3562 0.5482±0.0003 0.5295±0.0006 0.4722±0.0123 0.4505±0.0182 0.5808±0.0094 0.7849±0.0215 0.3858±0.0105 0.4361±0.0124 0.5125±0.0076 0.155±0.0183 0.317±0.0147\n\n0.5004±0.0068 0.4334±0.0081 0.6133±0.0022 0.3347±0.0078 0.6442±0.0089 0.5311±0.0110 0.6385±0.0459 1.7142±0.1663 2.4271±1.1596 0.5487±0.0008 0.5289±0.0004 0.4843±0.0205 0.4731±0.0197 0.5904±0.0035 0.8215±0.1220 0.3958±0.0088 0.4162±0.0154 0.5112±0.0049 0.1769±0.0410 0.3586±0.0754\n\n0.4928±0.0054 0.437±0.0052 0.6143±0.0006 0.3267±0.0066 0.6453±0.0061 0.5001±0.0082 0.667±0.0888 1.6062±0.1487 1.7519±0.6093 0.5481±0.0002 0.5291±0.0007 0.4971±0.0312 0.4587±0.0118 0.5925±0.0008 0.8068±0.0752 0.383±0.0074 0.418±0.0083 0.5138±0.0107 0.1851±0.0236 0.2928±0.0325\n\n0.4926±0.0052 0.4311±0.0151 0.6143±0.0016 0.3324±0.0136 0.6387±0.0077 0.5006±0.0144 0.6492±0.0187 1.5765±0.1416 3.948±2.5432 0.5504±0.0049 0.5292±0.0008 0.4678±0.0109 0.4478±0.0122 0.5921±0.0017 0.769±0.0226 0.3895±0.0049 0.4137±0.0109 0.5009±0.0043 0.1561±0.0279 0.2927±0.0233\n\n0.4945±0.0059 0.4504±0.0441 0.6168±0.0025 0.3288±0.0030 0.6425±0.0054 0.5194±0.0212 0.6461±0.0472 1.5634±0.1993 1.6384±0.1849 0.5505±0.0047 0.529±0.0004 0.4747±0.0127 0.4446±0.0066 0.5929±0.0001 0.7577±0.0222 0.3911±0.0079 0.4152±0.0142 0.5199±0.0118 0.1683±0.0291 0.2823±0.0445\n\n0.498±0.0061 0.4319±0.0251 0.6178±0.0025 0.3225±0.0039 0.6452±0.0030 0.4934±0.0010 0.6145±0.0242 1.4755±0.1126 3.1183±2.9336 0.5516±0.0064 0.5293±0.0002 0.4707±0.0095 0.4304±0.0071 0.593±0.0001 0.8173±0.1407 0.4084±0.0407 0.4102±0.0109 0.5039±0.0060 0.1572±0.0305 0.2549±0.0176\n\n0.5163±0.0028 0.4341±0.0044 0.6158±0.0014 0.3241±0.0014 0.6324±0.0070 0.506±0.0179 0.7228±0.0427 1.5506±0.0010 2.1344±1.0268 0.5483±0.0009 0.5328±0.0055 0.4792±0.0083 0.4376±0.0071 0.593±0.0000 0.9491±0.0951 0.3979±0.0035 0.4522±0.0491 0.5392±0.0129 0.1654±0.0422 0.2745±0.0334\n\n0.5148±0.0047 0.4251±0.0076 0.6146±0.0018 0.3249±0.0020 0.6371±0.0051 0.5044±0.0150 0.758±0.0460 1.5506±0.0010 2.6576±1.1446 0.5487±0.0017 0.5387±0.0186 0.4688±0.0086 0.4419±0.0089 0.593±0.0000 1.0567±0.0158 0.3988±0.0036 0.4352±0.0407 0.54±0.0197 0.1761±0.0409 0.2755±0.0221\n\n27\n\n0.00.20.40.60.81.0Normalized regret0.00.20.40.60.81.0P(X <= x)credit-gvehiclekc1blood-transf..Australiancarsegment0.00.20.40.60.81.0Normalized regret0.00.20.40.60.81.0P(X <= x)credit-gvehiclekc1blood-transf..Australiancarsegment0.00.20.40.60.81.0Normalized regret0.00.20.40.60.81.0P(X <= x)credit-gvehiclekc1blood-transf..Australiancarsegment1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rankUnder review as a conference paper at ICLR 2023\n\nTable 14: Final results of the optimizers on tabular mode with FedOpt (lower is better).\n\nbenchmark\n\nRS\n\nBOGP\n\nBORF\n\nBOKDE\n\nDE\n\nHB\n\nBOHB\n\nDEHB\n\nTPEMD\n\nTPEHB\n\nBERTSST-2 BERTCoLA GNNCora GNNCiteSeer GNNPubMed LR31 LR53 LR3917 LR10101 LR146818 LR146821 LR146822 MLP31 MLP53 MLP3917 MLP10101 MLP146818 MLP146821 MLP146822\n\n0.441±0.0049 0.616±0.0008 0.3264±0.0027 0.6483±0.0028 0.4777±0.0118 0.7358±0.0937 1.7838±0.2698 2.254±0.5724 0.5533±0.0078 0.511±0.0099 0.4017±0.0272 0.3972±0.0060 0.5912±0.0012 0.9096±0.0690 0.3798±0.0126 0.4219±0.0168 0.4943±0.0018 0.1169±0.0128 0.2963±0.0264\n\n0.4325±0.0125 0.616±0.0011 0.3235±0.0004 0.6517±0.0053 0.4426±0.0132 0.6831±0.0198 1.5609±0.1957 2.0316±0.5246 0.55±0.0036 0.506±0.0103 0.3599±0.0148 0.4211±0.0236 0.5914±0.0024 0.8166±0.0890 0.3937±0.0086 0.4141±0.0056 0.4913±0.0108 0.0836±0.0132 0.2914±0.0215\n\n0.4301±0.0087 0.6141±0.0022 0.3268±0.0032 0.6497±0.0050 0.4718±0.0204 0.6849±0.0523 1.5241±0.0547 2.3952±0.7949 0.5505±0.0032 0.5034±0.0097 0.4121±0.0188 0.4037±0.0191 0.5912±0.0012 0.8111±0.0998 0.3862±0.0075 0.4197±0.0138 0.5022±0.0090 0.0915±0.0149 0.2705±0.0240\n\n0.4463±0.0093 0.6137±0.0025 0.3322±0.0101 0.6535±0.0072 0.4943±0.0359 0.8152±0.1180 1.5116±0.0437 1.9788±0.5290 0.5509±0.0032 0.5133±0.0078 0.4134±0.0364 0.4442±0.0261 0.5912±0.0023 0.8872±0.1465 0.3871±0.0110 0.4111±0.0078 0.5023±0.0113 0.1674±0.0600 0.3025±0.0447\n\n0.4351±0.0185 0.6159±0.0005 0.3256±0.0009 0.6458±0.0028 0.4318±0.0001 0.7085±0.0660 1.6208±0.3794 2.6261±0.5535 0.549±0.0012 0.5007±0.0021 0.4079±0.0242 0.4075±0.0127 0.5918±0.0011 0.8546±0.1223 0.3867±0.0086 0.4303±0.0369 0.4884±0.0058 0.1079±0.0444 0.2779±0.0063\n\n0.4403±0.0064 0.6154±0.0013 0.3245±0.0014 0.6442±0.0034 0.4559±0.0135 0.6772±0.0527 1.6045±0.2433 2.3472±1.2238 0.5504±0.0029 0.5032±0.0054 0.395±0.0228 0.4131±0.0215 0.5923±0.0007 1.0163±0.0781 0.4109±0.0402 0.4145±0.0106 0.4995±0.0087 0.0891±0.0150 0.2759±0.0216\n\n0.4295±0.0066 0.6157±0.0018 0.3347±0.0121 0.6543±0.0112 0.4699±0.0248 0.6877±0.0561 1.7236±0.4056 2.5452±0.5266 0.5476±0.0017 0.5086±0.0067 0.398±0.0448 0.4008±0.0085 0.5921±0.0007 0.8565±0.0574 0.4262±0.0687 0.4256±0.0286 0.5046±0.0298 0.1389±0.0465 0.2621±0.0201\n\n0.4285±0.0068 0.6176±0.0004 0.3254±0.0008 0.6463±0.0029 0.4318±0.0001 0.6385±0.0498 1.3488±0.1343 2.3144±0.8685 0.5522±0.0056 0.4974±0.0030 0.3902±0.0300 0.3916±0.0086 0.5911±0.0023 0.9849±0.1238 0.3812±0.0125 0.4215±0.0277 0.4921±0.0293 0.0838±0.0270 0.2549±0.0108\n\n0.4293±0.0106 0.6172±0.0005 0.3405±0.0129 0.6488±0.0008 0.4368±0.0098 0.8652±0.0851 1.6654±0.2338 3.2131±2.2754 0.5612±0.0201 0.4983±0.0049 0.4447±0.0447 0.3878±0.0074 0.5921±0.0006 1.1276±0.0394 0.4003±0.0000 0.4502±0.0390 0.4978±0.0135 0.1051±0.0138 0.257±0.0020\n\n0.4332±0.0122 0.6168±0.0004 0.3361±0.0187 0.6495±0.0007 0.4402±0.0167 0.7044±0.0403 1.7978±0.2937 2.0291±0.3674 0.8567±0.6019 0.5104±0.0157 0.4625±0.0735 0.3871±0.0043 0.5921±0.0006 1.0952±0.0293 0.4003±0.0001 0.4502±0.0300 0.4861±0.0240 0.1194±0.0130 0.2518±0.0055\n\n(a) ALLFEMNIST\n\n(b) BBOFEMNIST\n\n(c) MFFEMNIST\n\nFigure 13: Mean rank over time on CNN benchmark (FedAvg).\n\n(a) ALLBERT\n\n(b) BBOBERT\n\n(c) MFBERT\n\n(d) ALLCoLA\n\n(e) BBOCoLA\n\n(f) MFCoLA\n\n(g) ALLSST-2\n\n(h) BBOSST-2\n\n(i) MFSST-2\n\nFigure 14: Mean rank over time on BERT benchmark (FedAvg).\n\n28\n\n1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rankUnder review as a conference paper at ICLR 2023\n\n(a) ALLBERT\n\n(b) BBOBERT\n\n(c) MFBERT\n\n(d) ALLCoLA\n\n(e) BBOCoLA\n\n(f) MFCoLA\n\n(g) ALLSST-2\n\n(h) BBOSST-2\n\n(i) MFSST-2\n\nFigure 15: Mean rank over time on BERT benchmark (FedOpt).\n\nI.2 SURROGATE MODE\n\nWe report the the final results with FedAvg on FEMNIST and BERT benchmarks in Table 15. Then we present the mean rank over time of the optimizers in Figure 22 and Figure 23.\n\nTable 15: Final results of the optimizers in surrogate mode (lower is better).\n\nbenchmark\n\nRS\n\nBOGP\n\nBORF\n\nBOKDE DE\n\nHB\n\nBOHB DEHB\n\nTPEMD\n\nTPEHB\n\nCNNFEMNIST BERTSST-2 BERTCoLA\n\n0.0508 0.4909 0.5013\n\n0.0478 0.4908 0.4371\n\n0.0514 0.4908 0.4113\n\n0.0492 0.4908 0.487\n\n0.0503 0.4908 0.444\n\n0.0478 0.4908 0.4621\n\n0.048 0.4908 0.4232\n\n0.0469 0.4917 0.4204\n\n0.0471 0.4908 0.3687\n\n0.0458 0.4908 0.3955\n\n29\n\n1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rankUnder review as a conference paper at ICLR 2023\n\n(a) ALLGNN\n\n(b) BBOGNN\n\n(c) MFGNN\n\n(d) ALLCora\n\n(e) BBOCora\n\n(f) MFCora\n\n(g) ALLCiteSeer\n\n(h) BBOCiteSeer\n\n(i) MFCiteSeer\n\n(j) ALLPubMed\n\n(k) BBOPubMed\n\n(l) MFPubMed\n\nFigure 16: Mean rank over time on GNN benchmark (FedAvg).\n\n30\n\n1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rankUnder review as a conference paper at ICLR 2023\n\n(a) ALLGNN\n\n(b) BBOGNN\n\n(c) MFGNN\n\n(d) ALLCora\n\n(e) BBOCora\n\n(f) MFCora\n\n(g) ALLCiteSeer\n\n(h) BBOCiteSeer\n\n(i) MFCiteSeer\n\n(j) ALLPubMed\n\n(k) BBOPubMed\n\n(l) MFPubMed\n\nFigure 17: Mean rank over time on GNN benchmark (FedOpt).\n\n31\n\n1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rankUnder review as a conference paper at ICLR 2023\n\n(a) ALLLR\n\n(b) BBOLR\n\n(c) MFLR\n\n(d) ALL31OpenML\n\n(e) BBO31OpenML\n\n(f) MF31OpenML\n\n(g) ALL53OpenML\n\n(h) BBO53OpenML\n\n(i) MF53OpenML\n\n(j) ALL10101OpenML\n\n(k) BBO10101OpenML\n\n(l) MF10101OpenML\n\n(m) ALL146818OpenML\n\n(n) BBO146818OpenML\n\n(o) MF146818OpenML\n\n(p) ALL146821OpenML\n\n(q) BBO146821OpenML\n\n(r) MF146821OpenML\n\n(s) ALL146822OpenML\n\n(t) BBO146822OpenML\n\n(u) MF146822OpenML\n\nFigure 18: Mean rank over time on LR benchmark (FedAvg). 32\n\n1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rankUnder review as a conference paper at ICLR 2023\n\n(a) ALLLR\n\n(b) BBOLR\n\n(c) MFLR\n\n(d) ALL31OpenML\n\n(e) BBO31OpenML\n\n(f) MF31OpenML\n\n(g) ALL53OpenML\n\n(h) BBO53OpenML\n\n(i) MF53OpenML\n\n(j) ALL10101OpenML\n\n(k) BBO10101OpenML\n\n(l) MF10101OpenML\n\n(m) ALL146818OpenML\n\n(n) BBO146818OpenML\n\n(o) MF146818OpenML\n\n(p) ALL146821OpenML\n\n(q) BBO146821OpenML\n\n(r) MF146821OpenML\n\n(s) ALL146822OpenML\n\n(t) BBO146822OpenML\n\n(u) MF146822OpenML\n\nFigure 19: Mean rank over time on LR benchmark (FedOpt). 33\n\n1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rankUnder review as a conference paper at ICLR 2023\n\n(a) ALLM LP\n\n(b) BBOM LP\n\n(c) MFM LP\n\n(d) ALL31OpenML\n\n(e) BBO31OpenML\n\n(f) MF31OpenML\n\n(g) ALL53OpenML\n\n(h) BBO53OpenML\n\n(i) MF53OpenML\n\n(j) ALL10101OpenML\n\n(k) BBO10101OpenML\n\n(l) MF10101OpenML\n\n(m) ALL146818OpenML\n\n(n) BBO146818OpenML\n\n(o) MF146818OpenML\n\n(p) ALL146821OpenML\n\n(q) BBO146821OpenML\n\n(r) MF146821OpenML\n\n(s) ALL146822OpenML\n\n(t) BBO146822OpenML\n\n(u) MF146822OpenML\n\nFigure 20: Mean rank over time on MLP benchmark (FedAvg). 34\n\n1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rankUnder review as a conference paper at ICLR 2023\n\n(a) ALLM LP\n\n(b) BBOM LP\n\n(c) MFM LP\n\n(d) ALL31OpenML\n\n(e) BBO31OpenML\n\n(f) MF31OpenML\n\n(g) ALL53OpenML\n\n(h) BBO53OpenML\n\n(i) MF53OpenML\n\n(j) ALL10101OpenML\n\n(k) BBO10101OpenML\n\n(l) MF10101OpenML\n\n(m) ALL146818OpenML\n\n(n) BBO146818OpenML\n\n(o) MF146818OpenML\n\n(p) ALL146821OpenML\n\n(q) BBO146821OpenML\n\n(r) MF146821OpenML\n\n(s) ALL146822OpenML\n\n(t) BBO146822OpenML\n\n(u) MF146822OpenML\n\nFigure 21: Mean rank over time on MLP benchmark (FedOpt). 35\n\n1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rankUnder review as a conference paper at ICLR 2023\n\n(a) ALLFEMNIST\n\n(b) BBOFEMNIST\n\n(c) MFFEMNIST\n\nFigure 22: Mean rank over time on CNN benchmark under surrogate mode (FedAvg).\n\n(a) ALLBERT\n\n(b) BBOBERT\n\n(c) MFBERT\n\n(d) ALLCoLA\n\n(e) BBOCoLA\n\n(f) MFCoLA\n\n(g) ALLSST-2\n\n(h) BBOSST-2\n\n(i) MFSST-2\n\nFigure 23: Mean rank over time on BERT benchmark under surrogate mode (FedAvg).\n\n36\n\n1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345678910Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank1e-41e-31e-21e-11Fraction of budget12345Mean rank",
  "translations": [
    "# Summary Of The Paper\n\nThis paper introduced a new benchmark suite for Federated HPO called FedHPO-Bench. The FedHPO-Bench is considered distinct from other HPO benchmark suites such as HPO-Bench because of the inherent requirements for FL algorithms.\n\nThe paper first identified the need for a FedHPO benchmark suite because of its distinct separation between client and server hyperparameters, and then provided some results to evaluate the performance of their benchmark suite. The analysis of the performance differences between separated client and server hyperparameters and standard HPO algorithms defined the majority of the reasoning behind the separation of HPO and FedHPO, and the corresponding need to create their benchmark suite. The authors clearly described this distinction, but did not provide convincing reasoning behind the need for their benchmark suite. \nAs stated in this paper, the benchmark suite is very closely related to HPO-Bench, and does not appear to provide a truly novel solution. Instead, it appears as a small addition to an already existing technology.\n\n# Strength And Weaknesses\n\ntrengths: This paper clearly described the the current FL landscape. \nThe explanation of its extensibility, and the experimental results give a clear idea of the given justification of FedHPO-Bench as a novel research path.\n\nWeaknesses: There were some instances throughout the paper where information was not put forth in an easily understandable format. For instance, Equation 1 provides could be summarized by saying that the total time is bounded by the summation of the slowest client’s computation, the server’s computation, and data transfer speeds. Figure 1 also introduces another issue like this, wherein the interface design is difficult to understand. Also, on page 19 (Appendix E), citing Wikipedia isn't a usable source for an academic conference due to the nature of the source of its information.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper clearly stated their contributions to the field, the setup of their experiments, and the background surrounding their research.\n\nThe paper maintained academic rigor in their experimental evaluation, setup, and the consideration of the factors specific to FL.\n\nThe originality of this work is clear, as they introduce a small, although distinct, contribution of a new HPO benchmark suite tailored for FL algorithms. That being said, this work bears an overwhelming similarity to HPO-Bench, and generally presents itself as HPO-Bench ported to FL rather than an entirely novel benchmark suite.\n\n# Summary Of The Review\n\nI would not recommend this paper for acceptance due to the fact that it does not provide a novel enough contribution to the field. Although it may definitely improve benchmarking for FedHPO, it is specific to only a single field, and it provides few novel aspects of HPO benchmark suites in general.\n\nWhile I can see how there may be some need for a solution to easily benchmark FedHPO algorithms, I remain unconvinced that this is novel enough to warrant publication.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces FEDHPO-BENCH, a dedicated benchmark suite for Federated Hyperparameter Optimization (HPO), addressing the limitations of existing benchmarks that primarily focus on centralized learning. The authors detail the unique challenges of HPO in a federated learning context, including data distribution, client heterogeneity, and the need for multi-objective optimization. The methodology involves a comprehensive design that allows for flexibility and extensibility, enabling researchers to customize benchmarks according to specific needs. The empirical findings demonstrate that traditional HPO methods do not significantly outperform others in federated contexts, while multi-fidelity optimizers and concurrent exploration methods show enhanced performance.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its timely and significant contribution to the field, as it fills a critical gap by providing the first dedicated benchmark for FedHPO. The comprehensive nature of FEDHPO-BENCH allows researchers to evaluate various federated learning tasks effectively, while its flexibility and extensibility cater to diverse research needs. However, the paper could be strengthened by including more extensive empirical comparisons across a wider range of federated learning scenarios, as well as potentially more elaborate discussions on the implications of privacy and fairness measures within the optimization process.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The comprehensive overview of the benchmark suite, including its features and empirical results, enhances its clarity. The novelty is significant as it provides a new framework for addressing a previously underexplored area. The authors have also made FEDHPO-BENCH open-source, promoting reproducibility, which is a valuable aspect of their contribution.\n\n# Summary Of The Review\nOverall, the paper presents a solid contribution to the field of federated learning by introducing FEDHPO-BENCH, a comprehensive and flexible benchmark suite for Federated Hyperparameter Optimization. While the empirical validation supports the effectiveness of the benchmark, further exploration of diverse federated learning scenarios could enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents \"FEDHPO-BENCH,\" a benchmark suite specifically designed for Federated Hyperparameter Optimization (FedHPO). The main contributions include the establishment of a comprehensive framework that accommodates the unique challenges posed by federated learning, such as client heterogeneity and data distribution variance. The methodology involves three evaluation modes built on the FederatedScope framework, allowing for flexible experimentation across a variety of FedHPO problems. The findings indicate that traditional HPO methods do not significantly outperform baseline methods in FedHPO settings, while concurrent exploration methods like FedEx show superior performance, highlighting the necessity for tailored approaches in federated contexts.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive nature, providing a wide array of FedHPO problems and allowing for user customization regarding privacy and fairness metrics. Its empirical validation through rigorous testing enhances its credibility and utility for researchers in the field. However, weaknesses include a dependency on the FederatedScope framework, which may hinder accessibility for some users, and incomplete results in certain benchmarks due to resource constraints. Additionally, the generalizability of the findings may be limited, as they may not fully apply to all federated learning scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making it accessible to readers familiar with the field. The quality of the experimental design is commendable, but the reliance on existing frameworks could impact reproducibility for users unfamiliar with them. The novelty of the benchmark suite is significant, addressing a clear gap in the existing literature on FedHPO, although the reproducibility may be compromised due to some incomplete results.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of federated learning through the introduction of FEDHPO-BENCH, a benchmark suite tailored for FedHPO challenges. While the suite offers comprehensive and flexible resources for researchers, some limitations in accessibility and generalizability warrant consideration for future enhancements.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces FEDHPO-BENCH, a benchmark suite designed explicitly for Federated Hyperparameter Optimization (FedHPO), addressing the shortcomings of existing HPO benchmarks that primarily focus on centralized learning. It outlines the unique challenges of FedHPO, such as client heterogeneity and the need for personalized configurations, and proposes a comprehensive, flexible, and extensible framework for evaluating FedHPO methods. The authors conduct extensive experiments to validate the usability of FEDHPO-BENCH, demonstrating that traditional HPO methods can be adapted for federated settings and that concurrent exploration techniques like FedEx can enhance performance.\n\n# Strength And Weaknesses\nThe paper’s strengths lie in its clear identification of the distinct challenges posed by FedHPO and the systematic approach to creating a benchmarking suite that addresses these challenges. The experiments are well-structured, providing insights into the performance of various HPO methods within a federated context. However, the paper could benefit from a more detailed discussion of the implications of its findings, particularly regarding how the results can influence future research in FedHPO. Additionally, while the benchmark is described as extensible, specific examples of how new problems can be integrated would enhance clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the motivations behind the creation of FEDHPO-BENCH. The methodology is described in sufficient detail, allowing for reproducibility of the experiments. The novelty of the proposed benchmark is significant, as it fills a critical gap in the current HPO literature by focusing on the federated learning paradigm. However, the paper could improve by providing more comprehensive details about the datasets and specific configurations used in the experiments to further aid reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a much-needed benchmark suite for Federated Hyperparameter Optimization, contributing valuable insights into the unique characteristics of this domain. While the methodology is robust and the findings are relevant, additional details on the extensibility of the benchmark and implications for future research could enhance the paper's impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents FEDHPO-BENCH, a novel benchmark suite specifically designed for Federated Hyperparameter Optimization (FedHPO). It aims to address the existing gap in benchmarking frameworks that predominantly focus on centralized learning. The authors highlight the comprehensive design of FEDHPO-BENCH, which supports a wide range of federated learning tasks, models, and hyperparameters, along with empirical studies validating its effectiveness. Key findings indicate that FEDHPO-BENCH not only facilitates the evaluation of FedHPO methods but also provides insights into the unique challenges posed by federated settings, such as data heterogeneity and privacy considerations.\n\n# Strength And Weaknesses\nThe strengths of the paper include the introduction of a dedicated benchmark for FedHPO, which is a significant advancement in the field, and its comprehensive design that allows for a variety of configurations. However, the paper has limitations, including a lack of comparative analysis with existing benchmarks, which could better demonstrate the advantages of FEDHPO-BENCH. Additionally, while the extensive customization options enhance flexibility, they may also complicate usability for less experienced users. The empirical validation, although thorough, is based on a limited set of datasets, raising concerns about the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings. The novelty of introducing a dedicated benchmark for FedHPO is significant, and the authors provide an open-source implementation that promotes reproducibility. However, the complexity of the benchmark could hinder its adoption among researchers who lack deep expertise in federated learning or hyperparameter optimization. Improving the user interface and providing comprehensive documentation would enhance the overall clarity and accessibility of FEDHPO-BENCH.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of federated learning by introducing FEDHPO-BENCH, a dedicated benchmark for evaluating hyperparameter optimization methods in federated settings. While the paper demonstrates significant novelty and empirical validation, it could benefit from deeper comparative analysis and broader testing to enhance generalizability and usability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FEDHPO-BENCH, a benchmarking suite specifically designed to tackle the challenges of hyperparameter optimization (HPO) in federated learning (FL) contexts. The authors identify key issues arising from data distribution, client heterogeneity, and privacy constraints that traditional HPO benchmarks fail to address. They propose a flexible and extensible framework that supports various federated learning scenarios and evaluation modes, including Raw, Tabular, and Surrogate modes. Empirical results demonstrate that the proposed FedHPO methods, which leverage concurrent exploration in federated settings, outperform traditional HPO approaches, thus validating the utility and effectiveness of the benchmark suite.\n\n# Strength And Weaknesses\n**Strengths:**\n- The paper provides a thorough identification of the unique challenges associated with HPO in federated learning, making a strong case for the need for dedicated benchmarks.\n- FEDHPO-BENCH is comprehensive and modular, accommodating a wide range of datasets and models, which enhances its applicability and relevance in the field.\n- The inclusion of multi-objective optimization capabilities is a notable strength, promoting research into balancing various performance metrics.\n\n**Weaknesses:**\n- While the paper offers a robust overview, the implementation details and performance metrics for the surrogate models are somewhat lacking, which may hinder reproducibility.\n- The discussion on privacy constraints in hyperparameter tuning could be expanded to provide deeper insights into the trade-offs involved in federated settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations, contributions, and findings. The quality of writing is high, with coherent arguments and logical flow. The novelty lies in the establishment of a benchmark suite tailored for federated HPO, which is a significant advancement in the field. However, the reproducibility of the results could be improved by including more details about the surrogate models and their evaluation metrics.\n\n# Summary Of The Review\nOverall, the paper presents a timely and significant contribution to the field of federated learning through the development of FEDHPO-BENCH. The benchmark suite addresses critical challenges in hyperparameter optimization and offers a framework that encourages future research and collaboration. However, some improvements in implementation details and discussions on privacy implications would enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel benchmark suite specifically designed for evaluating adversarial training methods within the federated learning paradigm. It addresses the unique challenges of decentralized data distribution, privacy constraints, and heterogeneous client capabilities that are prevalent in federated settings. The authors introduce a comprehensive framework that includes diverse tasks and customization options for benchmarking, and they conduct empirical studies comparing traditional adversarial training techniques with federated adaptations, revealing critical insights into their performance and robustness.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its identification of a significant gap in existing evaluation frameworks for adversarial training in federated learning. The proposed benchmark suite is comprehensive, flexible, and extensible, offering valuable tools for researchers in this emerging area. However, a potential weakness is the limited exploration of certain adversarial training methods and scenarios; while the empirical studies provide compelling results, they may not cover the entirety of existing techniques, which could limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the need for a dedicated benchmark suite for adversarial training in federated learning. The quality of the writing is high, and the methodology is presented with sufficient detail to allow for reproducibility. The novelty of the work is significant, as it directly addresses an underexplored area in the literature, contributing new insights and resources for the community.\n\n# Summary Of The Review\nOverall, this paper successfully fills a critical gap in the evaluation of adversarial training methods within federated learning by introducing a comprehensive and flexible benchmark suite. The empirical findings underscore the necessity for tailored approaches in this context, making it a valuable contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FEDHPO-BENCH, a benchmark suite specifically designed for federated hyperparameter optimization (FedHPO). It claims to address the inadequacies of existing benchmarks that focus on centralized learning by providing a comprehensive framework tailored to the unique challenges of hyperparameter tuning in federated learning environments. The authors present extensive experimental results utilizing this benchmark to validate various FedHPO methods, suggesting that this suite will significantly enhance empirical evaluations in the field.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to fill a gap in the current literature by offering a dedicated benchmark for FedHPO. The breadth of datasets and model architectures included is touted as a major contribution. However, the claims about the comprehensiveness and novelty of the benchmark are somewhat exaggerated; the actual diversity of tasks may not be as extensive as presented. Additionally, while the proposed flexibility and extensibility of the benchmark are noteworthy, similar features already exist in other frameworks, which diminishes the perceived innovation. The empirical validation, although extensive, raises concerns about generalizability and potential overfitting.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making the contributions clear. However, the novelty of the proposed benchmark is overstated, as many existing frameworks already facilitate similar functionalities. The quality of empirical validation is adequate but may lack reproducibility due to the potential overfitting to specific datasets. The open-source aspect is a positive feature, but the actual impact on the research community remains uncertain.\n\n# Summary Of The Review\nOverall, while FEDHPO-BENCH represents a valuable effort to establish a dedicated benchmark for federated hyperparameter optimization, many of the claims regarding its uniqueness and significance are overstated. The contributions are noteworthy but may not constitute a transformative advancement in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces FEDHPO-BENCH, a benchmark suite specifically tailored for Federated Hyperparameter Optimization (FedHPO). It addresses the limitations of existing benchmarks by providing a comprehensive collection of 15 diverse FedHPO problems across various domains and model architectures, allowing for customization based on privacy and fairness metrics. The experimental methodology employs an expanded set of optimizers, resulting in significant findings: multi-fidelity optimizers show a 20% efficiency improvement over single-fidelity methods, and the analysis reveals that lower privacy budgets correlate with higher accuracy. The authors position FEDHPO-BENCH as a vital tool for advancing research in FedHPO.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive nature, offering a broader selection of FedHPO problems compared to prior benchmarks. The flexibility for users to define performance metrics and the extensibility of the framework for future developments are notable contributions. However, the paper could benefit from more detailed discussions on the implications of the findings, particularly regarding the impact of varying privacy budgets on practical applications. Additionally, while the results demonstrate improvements over existing methods, the paper lacks a thorough comparison with all relevant state-of-the-art techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its objectives, methodologies, and findings. The quality of the experimental setup appears robust, with a substantial number of trials per configuration enhancing reliability. The novelty of the benchmark suite and its tailored approach for FedHPO is significant, filling an existing gap in the literature. The authors provide sufficient details for reproducibility, especially in the description of the framework and experimental procedures.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of Federated Hyperparameter Optimization through the introduction of FEDHPO-BENCH. Its comprehensive and flexible design, coupled with promising experimental results, positions it as a crucial resource for researchers in this domain. However, the paper could enhance its impact by incorporating deeper analyses and comparisons to existing methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a framework for Hyperparameter Optimization (HPO) tailored for Federated Learning (FL), termed FedHPO. The authors propose a benchmark called FEDHPO-BENCH, designed to foster the development and evaluation of FedHPO methods. Key contributions include a multi-objective optimization framework that incorporates fairness and privacy considerations alongside performance metrics. However, the study makes several assumptions regarding the generalizability of existing HPO techniques to the unique challenges posed by FL, such as non-IID data distributions and the high cost of function evaluations.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its emphasis on the distinct characteristics of FL, specifically the client heterogeneity and distributed setup, which necessitate a novel approach to HPO. The introduction of a multi-objective optimization framework is notable, as it seeks to balance performance with fairness and privacy. However, several weaknesses are evident; the assumption that traditional HPO methods can be directly adapted to FL is problematic, as it overlooks critical differences in data distribution and evaluation dynamics. Additionally, while the empirical validation is a step forward, the results' generalizability across various datasets and conditions remains questionable.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, with a logical flow of ideas. However, the assumptions made regarding the adaptability of existing techniques and the integration of fairness and privacy metrics could benefit from more rigorous analysis. The novelty of the approach is commendable, particularly in extending HPO concepts to FL, but some claims, especially those related to empirical validation and the simplicity of implementation, lack sufficient substantiation. Reproducibility may be compromised due to the assumptions about system conditions and client heterogeneity, which are not fully explored.\n\n# Summary Of The Review\nOverall, the paper presents an interesting framework for applying HPO in the context of Federated Learning, but it is hindered by several assumptions that could limit the effectiveness and applicability of its contributions. While the proposed benchmark and multi-objective framework are valuable, the lack of critical evaluation of key assumptions raises concerns about the robustness of the findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents FEDHPO-BENCH, a benchmark suite specifically designed for Federated Hyperparameter Optimization (FedHPO). The authors identify the limitations of existing benchmarks that primarily focus on centralized learning and highlight the unique challenges posed by the distributed nature of federated learning. FEDHPO-BENCH is characterized by its comprehensive problem representation, flexible customization options, and ongoing extensibility, all built within the FederatedScope framework. Empirical studies demonstrate the utility of the benchmark by comparing traditional HPO methods against FedHPO strategies, revealing insights into their performance and characteristics within federated settings.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its timely and relevant contribution to the field of federated learning, addressing a significant gap in hyperparameter optimization benchmarks. The comprehensive and flexible design of FEDHPO-BENCH is commendable, allowing researchers to tailor their experiments to various federated learning tasks. However, a potential weakness is the limited scope of empirical validation presented; while the initial experiments are insightful, further validation across diverse scenarios could strengthen the findings. Additionally, the paper lacks a thorough discussion of the privacy implications associated with the proposed methods, which is a critical consideration in federated learning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the creation of FEDHPO-BENCH, making it accessible to readers with varying levels of familiarity with the topic. The methodology and features of the benchmark are described in sufficient detail, allowing for reproducibility. The novelty of the work lies in its focus on federated environments, which are underexplored in the context of hyperparameter optimization. However, the reproducibility could be enhanced by providing more comprehensive datasets and example configurations for users.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of federated learning by introducing a dedicated benchmark suite for hyperparameter optimization. While the clarity and design of FEDHPO-BENCH are strong, additional empirical validation and a more thorough discussion of privacy concerns would enhance the paper's impact and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework for improving model interpretability in deep learning, specifically targeting the challenge of understanding feature importance in high-dimensional datasets. The authors propose a method that integrates perturbation-based feature evaluation with a gradient-based sensitivity analysis, resulting in a more reliable assessment of which features significantly influence model predictions. Empirical results demonstrate that the proposed approach outperforms existing interpretability methods on several benchmark datasets, suggesting its effectiveness and robustness.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Relevance**: The focus on interpretability in deep learning is highly relevant, especially in contexts where model transparency is crucial, such as healthcare and finance.\n2. **Clarity**: The writing is well-structured and mostly clear, allowing readers to follow the methodology and results without confusion.\n3. **Innovation**: The integration of perturbation and gradient-based techniques represents a novel approach that advances current interpretability methods.\n4. **Theoretical Foundation**: The paper provides a solid theoretical basis for the proposed method, explaining how it addresses limitations of existing approaches.\n5. **Empirical Support**: The empirical results are promising and indicate that the proposed method can lead to more actionable insights from complex models.\n\n**Weaknesses:**\n1. **Limited Dataset Variety**: The empirical evaluation is conducted on a relatively narrow set of datasets, which may not fully represent the generalizability of the proposed method.\n2. **Lack of Baseline Comparisons**: The paper does not sufficiently compare the proposed method against a comprehensive set of established interpretability techniques, hindering a clear assessment of its advantages.\n3. **Methodological Details**: Some methodological components are not thoroughly detailed, which may pose challenges for reproducibility.\n4. **Evaluation Metrics**: The metrics used for assessing interpretability are somewhat limited; including user studies or qualitative assessments could enrich the evaluation.\n5. **Future Work Discussion**: The discussion on limitations and potential future work is brief, potentially leaving out important considerations for subsequent research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a logical flow that aids in understanding the proposed framework. However, certain methodological details could be expanded for clarity and reproducibility. The novelty in combining perturbation and gradient approaches is commendable, though the lack of extensive baseline comparisons limits the assessment of its significance. Overall, while the contributions are innovative, improvements in clarity regarding methodology and a broader empirical validation are needed.\n\n# Summary Of The Review\nThe paper introduces a promising framework for enhancing interpretability in deep learning models, demonstrating novel integration of existing techniques. While the theoretical underpinnings and initial empirical results are strong, the paper would benefit from broader dataset evaluations and more comprehensive baseline comparisons to fully validate its claims. Addressing these issues could significantly enhance the work's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents FEDHPO-BENCH, a benchmark suite specifically designed for Federated Hyperparameter Optimization (FedHPO). The authors argue that existing hyperparameter optimization benchmarks cater primarily to centralized learning and fail to address the unique challenges posed by federated learning, such as decentralized data and participant heterogeneity. The proposed benchmark includes customizable function evaluations and is open-sourced for community use, aiming to enhance research in FedHPO by providing a comprehensive and flexible framework for performance evaluation. The authors conclude that FEDHPO-BENCH will facilitate future research by addressing the inadequacies of current benchmarks while encouraging reproducibility in this emerging area.\n\n# Strength And Weaknesses\nThe main strength of the paper is its timely contribution to a burgeoning field, as it identifies a significant gap in current hyperparameter optimization benchmarks tailored for federated learning. The methodology of creating a versatile and customizable benchmark is commendable and can potentially drive future research in FedHPO. However, the paper lacks empirical validation of the benchmark's utility, which would strengthen its claims regarding the effectiveness of FEDHPO-BENCH in real-world applications. Furthermore, although the framework is open-sourced, details on user accessibility and ease of implementation could enhance its appeal to the research community.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations behind developing FEDHPO-BENCH. The novelty lies in its focus on federated learning and the specific challenges addressed by the proposed benchmark. The quality of writing is generally high, making it accessible to a broad audience. However, the reproducibility of results remains uncertain without empirical evaluations and case studies demonstrating the benchmark's effectiveness in diverse scenarios.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution by introducing a benchmark specifically designed for federated hyperparameter optimization. While the proposed framework is timely and addresses existing gaps, the lack of empirical validation and detailed implementation guidance limits its immediate applicability. The open-source nature of FEDHPO-BENCH is a positive aspect that could foster community engagement and further developments in this area.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents FEDHPO-BENCH, a benchmark suite specifically designed for Federated Hyperparameter Optimization (FedHPO). It addresses the existing gap in benchmarking methodologies that fail to accommodate the distributed nature of federated learning (FL) and highlights the importance of hyperparameter optimization in enhancing machine learning performance. The proposed benchmark suite offers a comprehensive, flexible, and extensible framework that allows users to customize FedHPO problems according to varying privacy and fairness needs. Experimental results demonstrate that traditional hyperparameter optimization methods underperform in FL contexts, while recent FedHPO methods exploiting concurrent exploration show significant performance gains.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its comprehensive approach to addressing the unique challenges of hyperparameter optimization in federated learning settings. The introduction of a standardized evaluation framework is a notable contribution that can help facilitate comparisons among various FedHPO methods. The flexibility and extensibility of FEDHPO-BENCH further enhance its utility, allowing researchers to tailor benchmarks to their specific requirements. However, the paper could benefit from a deeper exploration of potential limitations and challenges in implementing the benchmark across diverse FL scenarios. Additionally, while the experimental validation is extensive, further details on the specific metrics and methodologies used could improve reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, contributions, and findings. The clarity of the writing facilitates understanding of complex concepts related to FedHPO and federated learning. The novelty of the benchmark suite is significant as it fills a critical gap in the existing literature. However, the reproducibility of the experiments could be improved by providing more detailed descriptions of the experimental setup and the algorithms used. Overall, the quality of the paper is high, but slight enhancements in reproducibility could strengthen its impact.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of hyperparameter optimization in federated learning by introducing the FEDHPO-BENCH benchmark suite. It addresses a clear need for standardized evaluation frameworks in this emerging area, demonstrating strong experimental validation. While the paper is well-written and comprehensive, further clarity on the experimental methodologies could enhance reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents \"FEDHPO-BENCH,\" a benchmark suite specifically designed for Federated Hyperparameter Optimization (FedHPO). It addresses a critical gap in existing benchmarks for hyperparameter tuning in federated learning (FL) environments. The authors propose a comprehensive methodology that emphasizes comprehensiveness, flexibility, and extensibility, facilitating robust evaluations of various FedHPO methods. Through a series of experiments comparing traditional HPO methods with FedHPO techniques, the findings illustrate the effectiveness of the proposed benchmark, showcasing how different optimizers perform under various configurations.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its timely and relevant contribution to the field of federated learning, where the need for effective hyperparameter optimization is increasingly recognized. The proposed FEDHPO-BENCH is well-structured, offering the community a much-needed resource that promotes consistent evaluation of algorithms in the context of FL. However, a notable weakness is the lack of detailed implementation strategies for specific FedHPO methods within the benchmark, which could limit its usability for practitioners. Additionally, the discussion could benefit from a more extensive exploration of the broader implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized, with clear headings and a logical flow that enhances readability. The quality of the methodology is high, providing a solid foundation for the benchmark suite, and the experimental setup is detailed enough to ensure reproducibility. While the novelty of the proposed benchmark is significant, given the relatively nascent field of FedHPO, the paper could improve clarity by elaborating on specific implementations and discussing the implications of the results for a wider audience.\n\n# Summary Of The Review\nOverall, the paper effectively addresses a pertinent issue in the realm of federated learning through the introduction of FEDHPO-BENCH, a valuable benchmark for evaluating hyperparameter optimization methods. While the contributions are noteworthy and the methodology sound, minor revisions are recommended to enhance clarity and detail, particularly regarding implementation specifics and broader implications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents FEDHPO-BENCH, a benchmark suite specifically designed for Federated Hyperparameter Optimization (FedHPO). It addresses the limitations of existing benchmarks that primarily focus on centralized learning, highlighting the unique challenges posed by non-IID data distributions and the need for personalized hyperparameter configurations in federated learning settings. The authors introduce a comprehensive, flexible, and extensible framework that encompasses various FedHPO problems, allowing for customizable function evaluations and the integration of future methodologies. Empirical results demonstrate the efficacy of traditional HPO methods in the federated context and showcase improvements achieved through concurrent exploration.\n\n# Strength And Weaknesses\nThe paper effectively identifies a significant gap in the current research landscape by emphasizing the need for specialized benchmarks in FedHPO. The introduction of FEDHPO-BENCH is a valuable contribution that not only provides a structured approach to evaluating FedHPO methodologies but also allows for customization and extensibility, which are crucial for ongoing research. However, the paper could benefit from a more detailed discussion of the specific datasets and algorithms included in the benchmark and how these choices were made. Additionally, empirical evaluations, while promising, could be expanded to include a broader range of scenarios and metrics to fully assess the benchmark's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulates its objectives and methodology. The novelty of introducing a tailored benchmark for FedHPO is significant, as existing benchmarks do not accommodate the complexities of federated learning. The reproducibility of the experiments is supported by the modular design of FEDHPO-BENCH, which allows other researchers to implement and build upon the proposed framework. However, the paper could improve clarity by providing more visuals or examples of the benchmark in action, which would enhance understanding for readers unfamiliar with the intricacies of FedHPO.\n\n# Summary Of The Review\nOverall, the paper makes a noteworthy contribution to the field of federated learning by introducing FEDHPO-BENCH, a specialized benchmarking suite for FedHPO. While the framework is comprehensive and flexible, further elaboration on specific implementations and empirical evaluations would strengthen the paper's impact. Nonetheless, the groundwork laid by this research is poised to facilitate future advancements in the optimization of hyperparameters in federated settings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a new benchmark suite for Federated Hyperparameter Optimization (FedHPO), named FEDHPO-BENCH. The authors claim that this suite is comprehensive and flexible, aiming to address existing limitations in current benchmarks. However, the paper provides limited justification for the need for a new benchmark, instead of improving upon existing ones. Additionally, while the authors identify unique aspects of FedHPO, empirical support for these claims is lacking, leading to questions about the benchmark's overall necessity and effectiveness.\n\n# Strength And Weaknesses\nThe proposed benchmark suite does have a comprehensive dataset collection and flexibility in experimentation, which could be beneficial for various applications. However, the reliance on existing datasets limits its value, as it does not introduce genuinely new data. Furthermore, the customization options might lead to inconsistencies and hinder reproducibility, which is crucial for scientific research. The extensibility feature is vaguely defined, raising concerns about the ease of integrating new methods. The experimental results do not convincingly demonstrate the superiority of FedHPO methods, suggesting that the benchmark may not significantly enhance the current state of the art. Lastly, the paper lacks engagement with related work, which diminishes its contextual relevance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper suffers from clarity issues, particularly regarding the implementation details of various FedHPO algorithms. This could confuse users attempting to replicate or build upon the work. While the novelty of proposing a new benchmark is notable, the lack of substantial empirical validation and rigorous argumentation for its necessity undermines its impact. The flexibility of the benchmark, though theoretically appealing, poses challenges for reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a benchmark suite that could contribute to the field of Federated Hyperparameter Optimization. However, it lacks sufficient justification for its necessity over existing benchmarks, and the empirical validation of its claims is weak. The execution of the proposed methodology does not convincingly position FEDHPO-BENCH as a significant advancement in hyperparameter optimization.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces FEDHPO-BENCH, the first benchmark suite specifically designed for Federated Hyperparameter Optimization (FedHPO). This suite comprehensively addresses the unique challenges posed by Federated Learning (FL) and features a diverse collection of FedHPO problems across multiple domains, including images and tabular data. The methodology emphasizes flexibility, allowing users to customize benchmarks based on various factors such as privacy and fairness, while also supporting multi-objective optimization. Empirical studies validate its effectiveness, demonstrating improved performance in comparing traditional HPO methods with FedHPO methods.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its groundbreaking innovation, as it fills a significant gap in the research landscape by providing a standardized platform for FedHPO. The comprehensive nature of the benchmark and its flexibility in customization are commendable features that promote fairness and reproducibility. However, a potential weakness is the need for ongoing updates to maintain relevance as the field evolves. Additionally, while the empirical insights are promising, deeper analyses of the results could enhance the understanding of the performance dynamics involved.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates the contributions of FEDHPO-BENCH. The quality of the writing is high, with a focus on clarity and accessibility for a broad audience. The novelty of introducing a tailored benchmark for FedHPO is significant, as it caters to the unique requirements of federated learning. The emphasis on reproducibility through standardized benchmarks is a strong point, ensuring that researchers can conduct fair comparisons and derive meaningful insights from their experiments.\n\n# Summary Of The Review\nOverall, FEDHPO-BENCH represents a significant advancement in the field of hyperparameter optimization within the context of federated learning. Its comprehensive and flexible design, coupled with its open-source nature, promises to empower researchers and foster collaborative advancements in this important area of machine learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a theoretical framework for Federated Hyperparameter Optimization (FedHPO), which addresses the unique challenges of hyperparameter tuning in federated learning settings. It identifies key limitations of existing HPO benchmarks that primarily focus on centralized learning paradigms and proposes a new benchmark suite, FEDHPO-BENCH, designed to encapsulate the complexities of FedHPO. The methodology involves a bi-level optimization approach that considers client heterogeneity, concurrent exploration, multi-objective optimization, and runtime estimation. The paper concludes with implications for theoretical foundations and encourages further research in this area.\n\n# Strength And Weaknesses\nThe paper's contributions are significant, particularly in highlighting the limitations of current HPO frameworks in relation to federated learning, which is an emerging area of interest. The proposed FEDHPO-BENCH is comprehensive, flexible, and extensible, allowing for a diverse range of FedHPO problems to be benchmarked. However, the theoretical constructs, while well-articulated, may benefit from empirical validation to support their claims. Additionally, the complexity of the proposed methods could present challenges in practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, making the theoretical concepts accessible. The quality of the theoretical framework is high, and the novelty lies in the tailored approach to FedHPO that considers the unique aspects of federated learning. While the theoretical contributions are solid, the reproducibility of the proposed benchmarks could be enhanced by providing more detailed guidelines on their implementation and practical applications.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of hyperparameter optimization in federated learning by providing a theoretical framework and introducing a comprehensive benchmark suite. While the theoretical insights are promising, additional empirical validation and practical implementation details are necessary to fully realize the framework's potential.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces **FEDHPO-BENCH**, a comprehensive benchmark suite for Federated Hyperparameter Optimization (FedHPO). The suite is built upon the FederatedScope (FS) framework and offers three function evaluation modes—Raw, Tabular, and Surrogate—to enhance usability and flexibility. Key contributions include the implementation of FedHPO methods like FedEx and FTS, the creation of new datasets and model architectures, and a detailed system model for runtime estimation. The authors emphasize code availability and documentation, allowing for reproducibility and easy customization by users.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its focus on reproducibility and extensibility, with open-source code that facilitates research in FedHPO. The extensive empirical studies validate the usability of FEDHPO-BENCH, and the comprehensive design covering various domains adds significant breadth. However, the paper could benefit from a deeper exploration of the limitations of the proposed benchmark and its applicability to real-world scenarios, which may not be fully captured by the controlled benchmarking environment.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to researchers in the field. The quality of implementation is high, and the inclusion of detailed documentation and user guidance enhances the reproducibility of the results. The novelty of the approach is commendable, as it fills a gap in the current landscape of federated learning benchmarks. However, further clarification on the scenarios in which this benchmark is most effective would strengthen the overall presentation.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of federated learning through the introduction of a versatile benchmarking suite for FedHPO. The focus on reproducibility, flexibility, and comprehensive evaluation makes it a significant resource for researchers. However, addressing potential limitations and real-world applicability could further enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FEDHPO-BENCH, a benchmark designed specifically for Hyperparameter Optimization (HPO) in Federated Learning (FL) settings. The authors argue that existing HPO benchmarks focus primarily on centralized learning and claim that their proposed benchmark addresses the unique challenges posed by the distributed nature of FL. The methodology includes extensive experimental evaluations to demonstrate the effectiveness of FEDHPO-BENCH compared to existing benchmarks, with a focus on the need for flexibility and multi-objective optimization.\n\n# Strength And Weaknesses\nThe paper presents a case for the necessity of a dedicated benchmark for FedHPO, highlighting the flexibility and customization aspects of FEDHPO-BENCH. However, the authors' claims regarding the uniqueness of their benchmark are overstated, as similar benchmarks have already been introduced in the field. The critique of existing benchmarks lacks depth, and the empirical results do not convincingly establish that FEDHPO-BENCH outperforms its predecessors. Overall, while the paper attempts to address a relevant problem, it fails to sufficiently engage with existing literature, which weakens its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its arguments clearly, but it lacks comprehensive engagement with prior work in the field, which affects the perceived novelty of the contributions. The methodology is described adequately, but the empirical results could benefit from clearer comparative analyses with existing benchmarks to enhance reproducibility and validation of claims.\n\n# Summary Of The Review\nWhile the paper proposes a new benchmark for Hyperparameter Optimization in Federated Learning, it does not adequately acknowledge or engage with prior work in the field, which diminishes the strength of its arguments. The empirical results are presented with enthusiasm, but they lack sufficient comparative validation against existing benchmarks.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents \"FEDHPO-BENCH: A Benchmark Suite for Federated Hyperparameter Optimization,\" which introduces a comprehensive benchmarking framework aimed at evaluating various hyperparameter optimization (HPO) strategies within a federated learning (FL) context. The authors propose a set of diverse benchmark problems that incorporate both continuous and discrete hyperparameter spaces, facilitating the assessment of existing HPO methods under different conditions. The findings indicate that the proposed benchmarks can effectively highlight the strengths and weaknesses of HPO algorithms, thereby providing a valuable resource for future research in this area.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its systematic approach to creating a benchmarking suite specifically tailored for federated environments, which is a relatively underexplored area in the literature. The methodology is well-structured, and the experiments conducted provide meaningful insights into the performance of various HPO techniques. However, the paper does have weaknesses, including some inconsistencies in formatting and notation, as well as a lack of clarity in linking the abstract to the contributions of the paper. Additionally, the future work section could benefit from a clearer outline of specific metrics for validation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear, but several formatting issues detract from its overall quality. Inconsistencies in section headers, reference formatting, and mathematical notation could confuse readers. The novelty of the work is significant, as it addresses a niche area of federated learning and hyperparameter optimization that has not been thoroughly explored. Reproducibility is supported by the provision of code snippets and a well-defined benchmark setup, although clearer documentation and formatting improvements would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper contributes a valuable benchmarking suite for federated hyperparameter optimization, addressing an important gap in the literature. While the methodology and findings are robust, improvements in clarity, consistency, and reproducibility are needed to enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FedHPO-BENCH, a benchmarking framework specifically designed for hyperparameter optimization in federated learning (FL) settings. It aims to evaluate various FedHPO methods under different federated scenarios, extending its applicability beyond traditional hyperparameter optimization benchmarks. The authors provide empirical results demonstrating the performance of FedHPO-BENCH across multiple FL environments, though they acknowledge the need for future enhancements and further exploration of several relevant dimensions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of a dedicated benchmarking framework for FedHPO, which is a growing area of interest in machine learning. The empirical results are extensive and provide valuable insights into the performance of different methods. However, the paper has notable weaknesses, including a limited exploration of real-world applications and a lack of comprehensive discussion on privacy metrics, model architecture search, and communication efficiency. Furthermore, important aspects such as dynamic hyperparameter adjustment, multi-task learning scenarios, and ethical implications are insufficiently addressed, which could enhance the framework's relevance and applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in presenting its methodology and findings. However, the novelty of the contributions is somewhat diminished by a lack of depth in exploring essential topics related to federated learning and hyperparameter optimization. The reproducibility of the findings could be improved with more detailed guidelines on the benchmarking framework's implementation and potential limitations.\n\n# Summary Of The Review\nIn summary, while the paper makes a commendable contribution by establishing a benchmarking framework for FedHPO, it falls short in addressing several critical aspects that could enhance its relevance and applicability in real-world scenarios. The framework's depth and applicability would benefit from a more comprehensive exploration of associated challenges and emerging trends in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents FEDHPO-BENCH, a benchmark suite designed for Federated Hyperparameter Optimization (FedHPO). It emphasizes the importance of employing rigorous statistical methodologies to evaluate FedHPO methods in comparison to traditional hyperparameter optimization benchmarks. The authors conduct extensive empirical studies to validate FEDHPO-BENCH, comparing various optimizers through statistical significance testing, mean ranks, and visualizations such as empirical cumulative distribution functions (ECDF). The findings indicate that while some optimizers demonstrate performance improvements, multi-fidelity optimizers do not show significant advantages over single-fidelity methods, calling for further exploration of hyperparameter landscapes and the adoption of rigorous statistical frameworks in future research.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive design of empirical studies and the thorough application of statistical methods to validate the performance of FedHPO methods. The use of sign tests and mean ranks provides clear insights into the performance of various optimizers, while the visualization of results through ECDF enhances the interpretability of findings. However, a potential weakness is the lack of exploration into confidence intervals, which could further strengthen the robustness of performance estimates. Additionally, the implications of the findings could be elaborated upon to provide more context regarding future research directions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its methodology and findings, making it accessible to readers. The quality of the empirical studies and the statistical rigor applied is commendable. The novelty lies in the introduction of a dedicated benchmark suite for FedHPO and the emphasis on statistical methodologies that are often overlooked in traditional benchmarks. Reproducibility is supported by detailed reporting of statistical methods and empirical results, although the inclusion of more confidence intervals could enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of Federated Hyperparameter Optimization by introducing FEDHPO-BENCH and advocating for rigorous statistical methodologies. While the findings are insightful and well-supported, the paper could benefit from additional exploration of confidence intervals to enhance robustness and clarity.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents FEDHPO-BENCH, a benchmarking framework aimed at evaluating hyperparameter optimization (HPO) methods in federated learning contexts. The authors claim that FEDHPO-BENCH allows flexibility in customizing benchmarks for various federated learning algorithms and aims to advance the understanding of HPO in such decentralized settings. However, the findings are primarily limited to supervised learning scenarios, and the paper lacks comprehensive assessments of its effectiveness across diverse environments and federated learning techniques.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its introduction of a structured benchmark for HPO in federated learning, which is a crucial area of research. However, several weaknesses undermine its contribution. The framework does not address non-supervised or semi-supervised learning, limiting its applicability. Additionally, there is insufficient empirical evaluation regarding its performance across heterogeneous client settings and a lack of discussion on the risks of privacy leakage inherent in the proposed methods. The limited scope of federated learning algorithms included in the benchmarks further constrains its representativeness. Moreover, the absence of mechanisms for community contributions and a lack of detailed examples for customizing benchmarks may hinder user engagement and development.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but it lacks clarity in certain areas, particularly in the discussion of how users can customize benchmarks. The novelty of introducing a benchmarking framework is significant, but the execution falls short due to limited empirical validation and analysis. The reproducibility of results is also questionable, as the paper does not provide comprehensive comparisons or detailed evaluations of the computational costs associated with using FEDHPO-BENCH.\n\n# Summary Of The Review\nOverall, while the paper introduces a noteworthy framework for benchmarking hyperparameter optimization in federated learning, its contributions are significantly limited by the lack of empirical evaluations, a narrow focus on supervised learning, and insufficient exploration of community engagement and customization options. The framework's applicability and relevance to the broader federated learning landscape remain constrained.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents FEDHPO-BENCH, a benchmark designed specifically for Federated Hyperparameter Optimization (FedHPO). The authors argue that existing benchmarks primarily focus on centralized learning and do not adequately address the unique challenges of federated learning (FL). The proposed benchmark includes a comprehensive collection of FedHPO problems, with features such as flexibility in customization and extensibility. Empirical studies conducted by the authors demonstrate that traditional hyperparameter optimization methods are less effective in the federated setting, underscoring the need for this specialized benchmark.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its identification of a gap in the current benchmarking landscape for hyperparameter optimization within federated learning contexts. The proposed benchmark provides a structured approach to address this gap, which could be useful for researchers in the field. However, the paper suffers from a lack of originality in its insights, as many of the observations made are well-known within the community. The empirical studies, while extensive, do not present particularly surprising results, and the overall tone of the paper suggests a lack of depth in exploring the implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, although it occasionally indulges in redundancy, reiterating well-established concepts without adding substantial new information. The quality of writing is adequate, but the novelty of the contributions appears limited. While the benchmark itself can be reproduced, the empirical results could benefit from deeper analysis and discussion regarding their implications for future research.\n\n# Summary Of The Review\nOverall, the paper introduces a new benchmark for Federated Hyperparameter Optimization, filling a recognized gap in the literature. However, the contributions lack significant novelty and depth, and the empirical results do not provide surprising insights. The paper may be useful for practitioners seeking a structured approach for FedHPO, but it does not push the boundaries of knowledge in the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents FEDHPO-BENCH, a comprehensive framework designed for Federated Hyperparameter Optimization (FedHPO). It highlights the unique aspects of FedHPO in comparison to traditional hyperparameter optimization methods and provides empirical evaluations of different optimization strategies within this context. The findings suggest that while FEDHPO-BENCH offers significant contributions to the field, there are opportunities for enhancing its performance through the incorporation of advanced optimization techniques and additional performance metrics.\n\n# Strength And Weaknesses\nThe strengths of the paper include its clear identification of the unique challenges posed by FedHPO and its comprehensive benchmarking of various hyperparameter optimization methods. The use of tabular and surrogate models is a notable strength, as is the discussion around privacy and fairness concerns. However, the paper could benefit from integrating more advanced optimization techniques, such as reinforcement learning, and exploring multi-fidelity strategies, which could enhance both performance and efficiency. Additionally, the reliance on validation loss as the primary performance metric limits the evaluation scope, as it does not fully capture the robustness or fairness of the FedHPO methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings. The methodology is presented in a coherent manner, making it accessible for readers familiar with the domain. However, the potential for greater novelty could be realized by incorporating more sophisticated techniques from adjacent fields. The reproducibility aspect appears satisfactory due to the detailed empirical evaluations provided, though further clarification on the experimental setup would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper introduces a valuable framework for Federated Hyperparameter Optimization, addressing key challenges and providing empirical insights. While it has notable strengths, such as the benchmarking of various FL algorithms, there are significant opportunities for improvement through the incorporation of advanced methodologies and a broader evaluation scope.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces FEDHPO-BENCH, a benchmark suite specifically designed for federated hyperparameter optimization (FedHPO), addressing a significant gap in existing benchmarks that predominantly focus on centralized learning. The authors present extensive empirical results comparing various hyperparameter optimization methods using the benchmarks created in FEDHPO-BENCH. The benchmarks encompass diverse datasets and models across multiple domains, including computer vision (CNN), natural language processing (BERT), and graph data (GNN), providing a comprehensive evaluation of different FedHPO methods. Key findings indicate that while traditional hyperparameter optimization methods perform similarly in the FedHPO context, Bayesian optimization with Gaussian processes (BOGP) and multi-fidelity optimizers like Hyperband often outperform their single-fidelity counterparts. Furthermore, methods utilizing concurrent exploration exhibit improved performance, underscoring the importance of adapting HPO strategies to the federated learning setting.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive benchmarking approach, which includes a wide variety of datasets and models, thus allowing for a robust evaluation of FedHPO methods. The use of mean ranks and ECDF plots as performance metrics provides valuable insights into the effectiveness of different optimizers. Additionally, the statistical validation of performance differences enhances the reliability of the findings. However, a potential weakness is the reliance on validation loss as the primary assessment metric, which may not capture all relevant aspects of performance across different tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings, making it accessible to readers. The quality of the empirical results is high, bolstered by the use of appropriate statistical tests. The novelty of introducing a dedicated benchmark suite for FedHPO is significant, as it fills an important gap in the literature. The extensibility and flexibility of FEDHPO-BENCH allow for future adaptations, promoting reproducibility and further research. Overall, the clarity and quality of the presentation are commendable.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of federated learning through the introduction of a dedicated benchmark suite for hyperparameter optimization. The comprehensive evaluation of various optimization methods provides essential insights that can guide future research and experimentation in this area.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents FEDHPO-BENCH, a comprehensive benchmark suite designed for Federated Hyperparameter Optimization (FedHPO). The authors introduce a structured methodology for evaluating various FedHPO methods, emphasizing the importance of standardization in experimental setups. Key findings indicate that the proposed benchmark allows for improved comparison of different FedHPO techniques, highlighting their performance across various scenarios and datasets.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its establishment of a standardized framework for assessing FedHPO methods, which is crucial for the advancement of research in this area. The empirical studies conducted provide valuable insights into the effectiveness of different algorithms. However, the paper has weaknesses in clarity and readability. Some sections are overly complex, and technical jargon may alienate non-specialist readers. Additionally, the organization could be improved, particularly in the presentation of results.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper tackles a novel and significant topic within machine learning, its clarity suffers from lengthy paragraphs, passive voice constructions, and inconsistent terminology. The quality of the empirical studies is commendable, but the presentation could benefit from clearer formatting and improved figure captions. Reproducibility is a concern, as the paper lacks detailed instructions for implementing the benchmark suite, and references to licensing information for open-source materials need to be clearer.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of Federated Hyperparameter Optimization by providing a benchmark suite that aids in the evaluation of various methods. However, improvements in clarity, organization, and reproducibility are necessary to enhance the paper's impact and accessibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.4572061934835476,
    -1.7379059053355397,
    -1.8328572921987925,
    -1.4792433522921986,
    -1.4612817042357897,
    -1.6989974069587819,
    -1.5822741922038526,
    -1.87379017772383,
    -1.7550091249733994,
    -1.749553475039622,
    -1.5562299941804356,
    -1.4546898808144317,
    -1.5699380822573974,
    -1.6136058963323634,
    -1.649390510299053,
    -1.5358107362153477,
    -1.847410327752716,
    -1.7485674778895852,
    -1.713855444801756,
    -1.8192070578010435,
    -1.7255245655863265,
    -1.617863207129872,
    -1.667180674746152,
    -1.6974137776226685,
    -1.813733975084971,
    -1.6147788056451122,
    -1.8096434089048306,
    -1.6726852842663376,
    -1.7381334655442306
  ],
  "logp_cond": [
    [
      0.0,
      -2.272783195442093,
      -2.283736399260245,
      -2.2719451220062052,
      -2.2673319423293776,
      -2.2586492022529807,
      -2.312316583941177,
      -2.2741958442213135,
      -2.2720694267515253,
      -2.285499221676669,
      -2.275721005784952,
      -2.3470743003752643,
      -2.2798327551863853,
      -2.275103652831358,
      -2.2708846061704375,
      -2.2767621058464727,
      -2.2713116746891266,
      -2.279969801424414,
      -2.2816306095929795,
      -2.2695022111884584,
      -2.274500321609848,
      -2.274453448503472,
      -2.2721102739197474,
      -2.2732166318551297,
      -2.281211696563504,
      -2.2844971317591085,
      -2.2663462531201515,
      -2.2736296403527936,
      -2.2731058386107765
    ],
    [
      -1.379764246406783,
      0.0,
      -1.1947364381717642,
      -1.2303851138726334,
      -1.2530733393356583,
      -1.2502946460429312,
      -1.3491544314612032,
      -1.2558786841278013,
      -1.2708127472473927,
      -1.3037914734322595,
      -1.1871540812570363,
      -1.4671122685089397,
      -1.3085918260926603,
      -1.2130705818535505,
      -1.339162491580443,
      -1.2447874819259817,
      -1.3308668422824674,
      -1.2333501128526936,
      -1.264928957051893,
      -1.3412607128966427,
      -1.2608477388194361,
      -1.3396598379356788,
      -1.315460245326688,
      -1.3100591836590512,
      -1.3036492260764319,
      -1.2949550659392934,
      -1.270129663012748,
      -1.2039752843704472,
      -1.345121940240786
    ],
    [
      -1.5228149332885965,
      -1.3364487456905487,
      0.0,
      -1.3803122367563017,
      -1.457675055640792,
      -1.3913120925396634,
      -1.5063554577055622,
      -1.4431711660243254,
      -1.3734940705324896,
      -1.4228735773673367,
      -1.3922688661225282,
      -1.5534441357843383,
      -1.4517214171323476,
      -1.3207370556976417,
      -1.3703429481834466,
      -1.4103803590279222,
      -1.4200230150176651,
      -1.3631235369127261,
      -1.427383812988481,
      -1.4132715456012939,
      -1.4207708125053569,
      -1.4775120869612044,
      -1.4796313191321262,
      -1.4334186352750704,
      -1.4813771670193516,
      -1.4195110213605182,
      -1.4732060877049007,
      -1.4092160209759275,
      -1.416236184644568
    ],
    [
      -1.1533522827414264,
      -1.0361498197192942,
      -1.0146185532703502,
      0.0,
      -1.1128452706532064,
      -1.0593067312750308,
      -1.1912934378325992,
      -1.0554318825387419,
      -1.0812922634515674,
      -1.0983628840648159,
      -1.0492206913829467,
      -1.2333682641788135,
      -1.1063666070986995,
      -1.0358855547071562,
      -1.1107708377734153,
      -1.0015044202033048,
      -1.0820765041640938,
      -1.0735556874667926,
      -1.040248030326728,
      -1.0976818646506519,
      -1.045839763866287,
      -1.099749043794724,
      -1.1224789306138814,
      -1.077783724153992,
      -1.136592546527652,
      -1.0767780839315502,
      -1.0775232356484643,
      -1.0636252164856883,
      -1.1143390364082306
    ],
    [
      -1.1557848662670545,
      -1.005779692727363,
      -1.064739709033605,
      -1.06776144179773,
      0.0,
      -1.0737110739086282,
      -1.1497012527909767,
      -1.00059506859167,
      -1.090876054374314,
      -1.0657142157738029,
      -1.05742863742945,
      -1.209243076647871,
      -1.0462934366457803,
      -1.0547167489006208,
      -1.0690892728065258,
      -1.0448701643897598,
      -1.1212011165045626,
      -1.0703960817950413,
      -1.0943788942520414,
      -1.0402261680069034,
      -1.0476270065503883,
      -1.1075852653442206,
      -1.0990413571802817,
      -1.09673143344779,
      -1.0845475783848593,
      -1.0868552275992958,
      -1.0798536609232345,
      -1.0642611102245525,
      -1.1132011717446726
    ],
    [
      -1.405817492898942,
      -1.2231542586965285,
      -1.2502665917806959,
      -1.2520955923497106,
      -1.3220030630030073,
      0.0,
      -1.3892539851721857,
      -1.2972024497228043,
      -1.3235044041258217,
      -1.3619340529336974,
      -1.2859810671866247,
      -1.4576482569448974,
      -1.333511620576089,
      -1.276234877047017,
      -1.3300597127739868,
      -1.2558631841236634,
      -1.3278889845361468,
      -1.2610195221895066,
      -1.3066841278457655,
      -1.2308083481956609,
      -1.3320755904021568,
      -1.3486641496736025,
      -1.323457948466735,
      -1.321573177296995,
      -1.3509707550629353,
      -1.3166939057009281,
      -1.3089774596785269,
      -1.273875348921851,
      -1.3365478992520512
    ],
    [
      -1.2687773845560135,
      -1.1407668356965295,
      -1.171228735791774,
      -1.1460946562899623,
      -1.1924882296971904,
      -1.173502404486326,
      0.0,
      -1.1961570566111956,
      -1.1682782489395176,
      -1.22579134911094,
      -1.0790965639914298,
      -1.257254363560561,
      -1.1866750492063207,
      -1.1883883952969483,
      -1.225970381898622,
      -1.1746588239202498,
      -1.2390608827342449,
      -1.1693804243290897,
      -1.213361529523323,
      -1.2101278769459303,
      -1.1529153917682695,
      -1.2030959557908727,
      -1.198174969404545,
      -1.2127664033569454,
      -1.2134268250145293,
      -1.1867512794932562,
      -1.1826594073936771,
      -1.1828563256215234,
      -1.2594887215200088
    ],
    [
      -1.4971187662864425,
      -1.3945226557921937,
      -1.4470748807813314,
      -1.4385685401151111,
      -1.415436237550173,
      -1.4477047862290753,
      -1.550667148034566,
      0.0,
      -1.435623257877186,
      -1.4315984240205173,
      -1.424324336330058,
      -1.6207278555071287,
      -1.445775405295444,
      -1.4740633305072885,
      -1.496281373821302,
      -1.4414457111682484,
      -1.4163380955373823,
      -1.4180782635447822,
      -1.444501133060934,
      -1.4170851988885387,
      -1.4394071419865115,
      -1.506727314048951,
      -1.5051201404035828,
      -1.470138534298533,
      -1.4861363001911434,
      -1.4815364869491023,
      -1.4811544192451132,
      -1.403201844270051,
      -1.4987463029557064
    ],
    [
      -1.3981596719347031,
      -1.338357148069118,
      -1.2810234828424574,
      -1.3318210830422044,
      -1.3778870565971217,
      -1.3776520302329187,
      -1.4295565168032414,
      -1.328488572911836,
      0.0,
      -1.3413422190922173,
      -1.3381635620592591,
      -1.4902498372261515,
      -1.3514126532107238,
      -1.3217649011115165,
      -1.3749953771481245,
      -1.3368723526270616,
      -1.353087679876557,
      -1.2953428486014442,
      -1.3458474448911883,
      -1.3890758477507683,
      -1.3159629953018972,
      -1.3919779118904538,
      -1.3650200963656787,
      -1.3392512547190796,
      -1.3385868148660496,
      -1.3386538870078442,
      -1.332251208962098,
      -1.3525671522138996,
      -1.4061526624751668
    ],
    [
      -1.49367103756042,
      -1.3934823434752384,
      -1.3943885253976998,
      -1.4193814960414033,
      -1.3961759184877875,
      -1.4127208164706018,
      -1.4726793378429264,
      -1.3774107146555312,
      -1.4250279709548401,
      0.0,
      -1.3914233955087238,
      -1.5306079394079641,
      -1.3924380699283303,
      -1.4209220295829996,
      -1.454428183535635,
      -1.406011811126795,
      -1.425068674537213,
      -1.397740403543374,
      -1.3983881229791055,
      -1.4274559861033522,
      -1.392728882636885,
      -1.4508934035146248,
      -1.442551463077065,
      -1.4152340108277888,
      -1.4327073734254139,
      -1.4403477178387547,
      -1.3745567638222789,
      -1.4141774092902737,
      -1.460954756811204
    ],
    [
      -1.2543496790710593,
      -1.0627037796210086,
      -1.1123033114501026,
      -1.0737477159766304,
      -1.1201434162715054,
      -1.170052222879596,
      -1.1579664698194154,
      -1.132442640897392,
      -1.169915263298284,
      -1.1543599141181233,
      0.0,
      -1.2902215916707562,
      -1.107734171232765,
      -1.1093918804115972,
      -1.1023597038476391,
      -1.0989000950258943,
      -1.1825388774853438,
      -1.1097956711518286,
      -1.1050851022704333,
      -1.1590355343134218,
      -1.1206012759247028,
      -1.1347701443443792,
      -1.1707481148483065,
      -1.1284246957246105,
      -1.1377251490330063,
      -1.1518530771539537,
      -1.1438322854927634,
      -1.139469103738131,
      -1.1911808376213693
    ],
    [
      -1.2983107950766999,
      -1.2776923145005183,
      -1.2906537972776047,
      -1.2965460803553932,
      -1.2671262385180957,
      -1.2440764266242343,
      -1.2762907575909195,
      -1.2649302228284762,
      -1.2811407405983049,
      -1.274185245001617,
      -1.2679844159501412,
      0.0,
      -1.2798358957415301,
      -1.2934374117566416,
      -1.2832978682214102,
      -1.2789391796722298,
      -1.2963253635690302,
      -1.2730941793242068,
      -1.2757415961620937,
      -1.2853062821426842,
      -1.255444773594467,
      -1.2809284485957764,
      -1.271144311020146,
      -1.273293600705491,
      -1.268471227700574,
      -1.2914640244480358,
      -1.2719846701321316,
      -1.27112604984264,
      -1.277290634453059
    ],
    [
      -1.2710478173302817,
      -1.1730754145492481,
      -1.2298558735604217,
      -1.2032077973333242,
      -1.2041121841638172,
      -1.197850265039337,
      -1.2497668318772355,
      -1.1576756598566278,
      -1.2246147102199647,
      -1.1573920281502463,
      -1.1509266029552543,
      -1.3251371130679757,
      0.0,
      -1.1922687680830768,
      -1.2040685843351773,
      -1.13320194560138,
      -1.2362201016343235,
      -1.189809971034604,
      -1.169275072257936,
      -1.1983597511079065,
      -1.1566640341375045,
      -1.2128486670580099,
      -1.1985192543438674,
      -1.2097759672679878,
      -1.1704275214338102,
      -1.175105040418624,
      -1.1846931671335266,
      -1.1987815659920857,
      -1.2295944173098645
    ],
    [
      -1.2900579200112037,
      -1.164743673234293,
      -1.1081305103471084,
      -1.1785752688926294,
      -1.244078250789902,
      -1.1875090920517801,
      -1.314061273741372,
      -1.2083601060730111,
      -1.2164189195276023,
      -1.240942534504853,
      -1.2187711077251606,
      -1.3938710825504679,
      -1.2345567523767322,
      0.0,
      -1.188861346998711,
      -1.1833366776165986,
      -1.2020800672470104,
      -1.1998786827952674,
      -1.2161376774957746,
      -1.2953789405797034,
      -1.2176490155731343,
      -1.2713182167682637,
      -1.2620305087548147,
      -1.245921270943299,
      -1.260630722658909,
      -1.1968969713440847,
      -1.236301152217493,
      -1.215034624020587,
      -1.2268239427816254
    ],
    [
      -1.3415864511639053,
      -1.285910149382176,
      -1.213963233388692,
      -1.3091345591267394,
      -1.3113419638662303,
      -1.3211406026574908,
      -1.3521023584457532,
      -1.2793443121566301,
      -1.2777272515592026,
      -1.3176156903614349,
      -1.2805413681679285,
      -1.413402327744867,
      -1.3072881419872746,
      -1.2435189467668337,
      0.0,
      -1.2997564586497672,
      -1.2503986283114161,
      -1.3035347994961455,
      -1.315436327164601,
      -1.310202975233529,
      -1.2643171541827452,
      -1.2708278738475551,
      -1.3245651374627776,
      -1.2863538725640218,
      -1.3141126571641841,
      -1.2693808779744304,
      -1.30365218068026,
      -1.3043209138305578,
      -1.2395221802335905
    ],
    [
      -1.2675216865084291,
      -1.136048542194828,
      -1.1772434256667519,
      -1.0720379552458612,
      -1.2202514679686207,
      -1.1491913197535752,
      -1.27690729662052,
      -1.1945911287965725,
      -1.177868089419545,
      -1.2148886174687057,
      -1.1511623748097606,
      -1.32089405848332,
      -1.1482072374106485,
      -1.1499865611121667,
      -1.2106130497878973,
      0.0,
      -1.22309155026518,
      -1.1837765901387576,
      -1.138853806653162,
      -1.2269681642464236,
      -1.189330658744794,
      -1.217470834757704,
      -1.2348214971637448,
      -1.2169074279995402,
      -1.2012695766489665,
      -1.1864959089439426,
      -1.2201888849619769,
      -1.1747184946483105,
      -1.2452190550404059
    ],
    [
      -1.5633475406583177,
      -1.426901800655651,
      -1.448388321949975,
      -1.4657855702018068,
      -1.4785154796718878,
      -1.458690325330499,
      -1.5518371949672538,
      -1.4297547252174236,
      -1.462734681105374,
      -1.5203203515816652,
      -1.4209662115370068,
      -1.62902827212704,
      -1.4889976185215075,
      -1.3922084003933732,
      -1.374436397606036,
      -1.4352800287593288,
      0.0,
      -1.4195993051805291,
      -1.4849071546093546,
      -1.4824582036553264,
      -1.4849580976219992,
      -1.4929779621449795,
      -1.5022187212680618,
      -1.479229947615486,
      -1.4511325052427704,
      -1.416797266526031,
      -1.4432492085717588,
      -1.447926233057038,
      -1.4399976822345253
    ],
    [
      -1.3758743141350498,
      -1.2938735662984087,
      -1.2427991214224814,
      -1.3164801830442112,
      -1.3210233335101158,
      -1.3082526959391543,
      -1.3754949070322902,
      -1.260772086862136,
      -1.2620733935057737,
      -1.301524685726957,
      -1.2763210867864079,
      -1.4838959291825229,
      -1.29750880846714,
      -1.2704294111440302,
      -1.330475525263696,
      -1.302265032332505,
      -1.3282542052078994,
      0.0,
      -1.291946951668062,
      -1.3299005090771374,
      -1.2638400079720267,
      -1.3584361407886352,
      -1.3485122850649653,
      -1.3333806698193826,
      -1.3060771426551112,
      -1.3037301861847939,
      -1.3130188135691825,
      -1.331877810505965,
      -1.329605848652841
    ],
    [
      -1.3887206278581632,
      -1.2180967620027252,
      -1.2954915697412996,
      -1.2164727002663418,
      -1.2995060123709081,
      -1.269951083971419,
      -1.382251201820667,
      -1.2701773819345763,
      -1.286648670891041,
      -1.2731796399393442,
      -1.2633126615388124,
      -1.475782091718755,
      -1.283859147134502,
      -1.2347648249812524,
      -1.2977473783788065,
      -1.2376185718220478,
      -1.3387149564593435,
      -1.256005686449273,
      0.0,
      -1.2871385392970434,
      -1.2750148573591313,
      -1.323360341055922,
      -1.313709596270574,
      -1.289619414798513,
      -1.2974891880623625,
      -1.293255327642223,
      -1.3002116081165223,
      -1.2736082480708149,
      -1.3470679003137365
    ],
    [
      -1.505616812013713,
      -1.4138333154694767,
      -1.3777070515499548,
      -1.4041751757893777,
      -1.4074279571694177,
      -1.3573536756240125,
      -1.4844157416691957,
      -1.3814145061650855,
      -1.4376520844103347,
      -1.4132205018473698,
      -1.4059804555879505,
      -1.5663451143724276,
      -1.4061957963161884,
      -1.4258296006398976,
      -1.406716112830669,
      -1.397041993321734,
      -1.4485855804837178,
      -1.4096921997844167,
      -1.3950733359339884,
      0.0,
      -1.4025359872511474,
      -1.4333385048497287,
      -1.3971063330466174,
      -1.413667752397426,
      -1.4088714313221564,
      -1.399598059553191,
      -1.3943944070525334,
      -1.3938648662549324,
      -1.3972933311223747
    ],
    [
      -1.3567749785390675,
      -1.2360206632805621,
      -1.369391260111895,
      -1.2907811110651297,
      -1.306114930209135,
      -1.3003415958459836,
      -1.4085801548521475,
      -1.287858408578639,
      -1.3271099649892362,
      -1.2990101504622011,
      -1.2714283350522892,
      -1.4548868065830343,
      -1.3348165465766795,
      -1.321848975710324,
      -1.3477332844529502,
      -1.3124044819106773,
      -1.3533783661366603,
      -1.2712069723437396,
      -1.2970956714667299,
      -1.3307117725100006,
      0.0,
      -1.370312001075957,
      -1.2918950640918008,
      -1.335047091260768,
      -1.3484395291818347,
      -1.3347460333952208,
      -1.3232538541914152,
      -1.3149898769482462,
      -1.3662915644394173
    ],
    [
      -1.2706169193893877,
      -1.2077314376706056,
      -1.2187227748600429,
      -1.1997951742164363,
      -1.2149610023515773,
      -1.233925545174044,
      -1.2505635583606358,
      -1.221671549331085,
      -1.2201519567163228,
      -1.2289804312927197,
      -1.184546722948537,
      -1.3359948611600159,
      -1.2275226232397725,
      -1.2376568001314276,
      -1.2157188408283335,
      -1.2157690208599148,
      -1.2433705583138037,
      -1.2168096971214661,
      -1.225988070575311,
      -1.2133804249839397,
      -1.1995197349417839,
      0.0,
      -1.2210157186917878,
      -1.1827358894044806,
      -1.2176307022471002,
      -1.254995261471061,
      -1.212443644407614,
      -1.2217092214723924,
      -1.2354310955956174
    ],
    [
      -1.372471713367128,
      -1.3273921957813872,
      -1.356949771325501,
      -1.349221958577569,
      -1.338636851332044,
      -1.3160832520509242,
      -1.3845041948959351,
      -1.2895135872144525,
      -1.3413111693722175,
      -1.3234009545374716,
      -1.3267352677135251,
      -1.3865581850575404,
      -1.3485523734390692,
      -1.3266712705666859,
      -1.34486387587623,
      -1.3594445199064102,
      -1.347774124406668,
      -1.3403722180268418,
      -1.3452737018174268,
      -1.3317585613041016,
      -1.2761584277220468,
      -1.3285011783118708,
      0.0,
      -1.3081448518345329,
      -1.2935827393290862,
      -1.321323951644991,
      -1.2966447186693841,
      -1.3127050791244528,
      -1.327623913484111
    ],
    [
      -1.3710070729616508,
      -1.2711864259306833,
      -1.3054421343538074,
      -1.2944959192147505,
      -1.2992460871615024,
      -1.3261347408958224,
      -1.366791223990974,
      -1.3200397422101924,
      -1.2801487131219456,
      -1.320498923365109,
      -1.3030708943527558,
      -1.473039409682368,
      -1.3057832100421178,
      -1.3105415003785565,
      -1.3480785448124941,
      -1.2949965425202783,
      -1.3481012383740656,
      -1.2969821431389106,
      -1.2938943051410225,
      -1.3026351220614998,
      -1.3033143491140255,
      -1.307981413478674,
      -1.3343324183899172,
      0.0,
      -1.3262334139397713,
      -1.3660650206116334,
      -1.2883424046497536,
      -1.2544527217345995,
      -1.325833046218661
    ],
    [
      -1.509238392548044,
      -1.4243761769919068,
      -1.4815170656095744,
      -1.4757579183430738,
      -1.453200574485794,
      -1.4376142093569362,
      -1.527480451573704,
      -1.4581615820840856,
      -1.466921375292429,
      -1.4435032346738867,
      -1.4171267395771194,
      -1.5689957595224826,
      -1.4169448166994316,
      -1.4813074469402008,
      -1.4917980870013046,
      -1.4433143389370608,
      -1.4830952076827715,
      -1.430105254456589,
      -1.4889010182827336,
      -1.4572505808149319,
      -1.4362882951898643,
      -1.4642548347648972,
      -1.4322190188105388,
      -1.4694939031556253,
      0.0,
      -1.4858891981398907,
      -1.4327112711579015,
      -1.4537585257137375,
      -1.5010284170323833
    ],
    [
      -1.2395952785490356,
      -1.1972691580116754,
      -1.1997719185615239,
      -1.2042466564056071,
      -1.2366825419333067,
      -1.2524728219018528,
      -1.2743763899896223,
      -1.1839998137629182,
      -1.2304647546996523,
      -1.253981391799298,
      -1.1635578972208909,
      -1.3770956301960107,
      -1.2030388301275554,
      -1.1966977772580365,
      -1.2267290572783756,
      -1.1756557140079218,
      -1.192992151415613,
      -1.177407641600195,
      -1.2326879251073555,
      -1.2796140141975372,
      -1.17450277769062,
      -1.2648683796202498,
      -1.2510410269447536,
      -1.2488857764398038,
      -1.2395096432899193,
      0.0,
      -1.2050502970112424,
      -1.2382109224365303,
      -1.2151484824837444
    ],
    [
      -1.467449999048526,
      -1.3996387921732782,
      -1.3831565161438986,
      -1.4185775212690155,
      -1.3841720469059933,
      -1.383157748957583,
      -1.5005506972005598,
      -1.3987800142710831,
      -1.4092500009262965,
      -1.390697550702485,
      -1.4135159063644158,
      -1.533004932115805,
      -1.4147972931906387,
      -1.4168664156734656,
      -1.458125112226928,
      -1.4262611099301208,
      -1.3982288634390048,
      -1.4025601453703096,
      -1.4306223562637423,
      -1.3862351838555957,
      -1.3771840180013397,
      -1.4345296641901601,
      -1.3766955231350209,
      -1.3777801000397605,
      -1.4169946305690062,
      -1.4126060227576138,
      0.0,
      -1.3684397704496254,
      -1.417501260441356
    ],
    [
      -1.3575912639117051,
      -1.1908842967714621,
      -1.2712673539462818,
      -1.2673250096937574,
      -1.3111986626440173,
      -1.2730130538880182,
      -1.3870252948976987,
      -1.2644059455560366,
      -1.3015003305001276,
      -1.3405730165884846,
      -1.2874513141367314,
      -1.4503722881749146,
      -1.3474489736516884,
      -1.281524619657177,
      -1.3231494023481087,
      -1.2483964903061473,
      -1.3146746262495985,
      -1.3061233060885817,
      -1.3025339130482763,
      -1.3237458573426846,
      -1.319751302046983,
      -1.3275080020977148,
      -1.3286218079018994,
      -1.2298871000944194,
      -1.3460680763560944,
      -1.3176113193176209,
      -1.294185315664847,
      0.0,
      -1.346072462879453
    ],
    [
      -1.339244969749475,
      -1.310370104129362,
      -1.2788022570784248,
      -1.3265238922648168,
      -1.3047314234914,
      -1.3352759788146773,
      -1.387786098784886,
      -1.2943574205570967,
      -1.3102614893821387,
      -1.3303988396625215,
      -1.3123962103479017,
      -1.4502243054775257,
      -1.329631759468126,
      -1.246950345014319,
      -1.2414108157508736,
      -1.3414837335491725,
      -1.26022886520465,
      -1.2777998947511109,
      -1.3350366708371266,
      -1.3068679655440774,
      -1.323749502968286,
      -1.2534177638095958,
      -1.2958956256788905,
      -1.2620393552419849,
      -1.3318429078873335,
      -1.2739105412751335,
      -1.3057336742444263,
      -1.2992143478508018,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.18442299804145446,
      0.17346979422330255,
      0.1852610714773424,
      0.18987425115417,
      0.19855699123056691,
      0.14488960954237085,
      0.18301034926223414,
      0.18513676673202228,
      0.17170697180687844,
      0.18148518769859567,
      0.11013189310828331,
      0.17737343829716234,
      0.18210254065218967,
      0.1863215873131101,
      0.18044408763707498,
      0.18589451879442098,
      0.17723639205913377,
      0.17557558389056815,
      0.18770398229508922,
      0.1827058718736998,
      0.18275274498007565,
      0.18509591956380023,
      0.18398956162841795,
      0.17599449692004354,
      0.17270906172443912,
      0.19085994036339615,
      0.18357655313075405,
      0.1841003548727711
    ],
    [
      0.3581416589287567,
      0.0,
      0.5431694671637755,
      0.5075207914629063,
      0.48483256599988134,
      0.4876112592926085,
      0.3887514738743365,
      0.48202722120773833,
      0.467093158088147,
      0.43411443190328014,
      0.5507518240785034,
      0.27079363682659996,
      0.42931407924287934,
      0.5248353234819891,
      0.3987434137550967,
      0.493118423409558,
      0.40703906305307225,
      0.5045557924828461,
      0.4729769482836468,
      0.39664519243889695,
      0.47705816651610355,
      0.3982460673998609,
      0.42244566000885175,
      0.4278467216764885,
      0.4342566792591078,
      0.44295083939624624,
      0.4677762423227916,
      0.5339306209650925,
      0.3927839650947538
    ],
    [
      0.31004235891019594,
      0.49640854650824373,
      0.0,
      0.4525450554424908,
      0.3751822365580004,
      0.4415451996591291,
      0.3265018344932302,
      0.3896861261744671,
      0.45936322166630283,
      0.4099837148314558,
      0.44058842607626425,
      0.2794131564144542,
      0.38113587506644486,
      0.5121202365011508,
      0.46251434401534586,
      0.4224769331708702,
      0.4128342771811273,
      0.4697337552860663,
      0.4054734792103114,
      0.4195857465974986,
      0.4120864796934356,
      0.35534520523758806,
      0.35322597306666625,
      0.39943865692372205,
      0.3514801251794408,
      0.41334627083827424,
      0.35965120449389176,
      0.423641271222865,
      0.41662110755422455
    ],
    [
      0.3258910695507722,
      0.4430935325729044,
      0.4646247990218484,
      0.0,
      0.36639808163899223,
      0.4199366210171678,
      0.2879499144595994,
      0.42381146975345674,
      0.39795108884063124,
      0.38088046822738275,
      0.4300226609092519,
      0.2458750881133851,
      0.3728767451934991,
      0.44335779758504246,
      0.36847251451878327,
      0.47773893208889384,
      0.39716684812810477,
      0.40568766482540597,
      0.43899532196547053,
      0.38156148764154674,
      0.43340358842591153,
      0.3794943084974747,
      0.35676442167831723,
      0.40145962813820657,
      0.34265080576454654,
      0.40246526836064844,
      0.4017201166437343,
      0.4156181358065103,
      0.364904315883968
    ],
    [
      0.3054968379687353,
      0.45550201150842673,
      0.3965419952021847,
      0.3935202624380598,
      0.0,
      0.38757063032716155,
      0.31158045144481306,
      0.4606866356441197,
      0.37040564986147584,
      0.39556748846198686,
      0.4038530668063398,
      0.25203862758791873,
      0.4149882675900094,
      0.4065649553351689,
      0.39219243142926397,
      0.41641153984602997,
      0.3400805877312272,
      0.3908856224407484,
      0.36690280998374836,
      0.42105553622888636,
      0.4136546976854014,
      0.3536964388915691,
      0.362240347055508,
      0.36455027078799973,
      0.37673412585093047,
      0.374426476636494,
      0.3814280433125552,
      0.39702059401123724,
      0.34808053249111714
    ],
    [
      0.2931799140598399,
      0.4758431482622534,
      0.448730815178086,
      0.4469018146090713,
      0.3769943439557746,
      0.0,
      0.3097434217865962,
      0.4017949572359776,
      0.37549300283296017,
      0.33706335402508447,
      0.4130163397721571,
      0.24134915001388446,
      0.3654857863826928,
      0.4227625299117648,
      0.36893769418479505,
      0.4431342228351185,
      0.3711084224226351,
      0.43797788476927524,
      0.3923132791130164,
      0.468189058763121,
      0.36692181655662504,
      0.3503332572851794,
      0.37553945849204684,
      0.3774242296617869,
      0.3480266518958466,
      0.38230350125785373,
      0.390019947280255,
      0.4251220580369308,
      0.3624495077067307
    ],
    [
      0.31349680764783905,
      0.4415073565073231,
      0.4110454564120787,
      0.43617953591389025,
      0.3897859625066622,
      0.40877178771752654,
      0.0,
      0.3861171355926569,
      0.413995943264335,
      0.35648284309291256,
      0.5031776282124227,
      0.3250198286432915,
      0.39559914299753185,
      0.3938857969069043,
      0.3563038103052305,
      0.4076153682836028,
      0.3432133094696077,
      0.41289376787476284,
      0.36891266268052947,
      0.3721463152579223,
      0.42935880043558305,
      0.3791782364129799,
      0.38409922279930764,
      0.36950778884690716,
      0.3688473671893233,
      0.39552291271059636,
      0.3996147848101754,
      0.39941786658232914,
      0.3227854706838438
    ],
    [
      0.37667141143738747,
      0.47926752193163624,
      0.42671529694249855,
      0.4352216376087188,
      0.45835394017365694,
      0.42608539149475466,
      0.323123029689264,
      0.0,
      0.438166919846644,
      0.44219175370331265,
      0.449465841393772,
      0.2530623222167012,
      0.42801477242838604,
      0.3997268472165414,
      0.37750880390252783,
      0.43234446655558156,
      0.4574520821864476,
      0.4557119141790478,
      0.42928904466289586,
      0.45670497883529126,
      0.4343830357373184,
      0.36706286367487895,
      0.36867003732024717,
      0.403651643425297,
      0.38765387753268654,
      0.3922536907747276,
      0.3926357584787168,
      0.47058833345377904,
      0.3750438747681235
    ],
    [
      0.3568494530386963,
      0.41665197690428135,
      0.4739856421309421,
      0.4231880419311951,
      0.3771220683762777,
      0.37735709474048074,
      0.3254526081701581,
      0.42652055206156336,
      0.0,
      0.41366690588118216,
      0.4168455629141403,
      0.264759287747248,
      0.40359647176267566,
      0.433244223861883,
      0.380013747825275,
      0.41813677234633784,
      0.40192144509684247,
      0.45966627637195523,
      0.4091616800822111,
      0.36593327722263114,
      0.43904612967150225,
      0.3630312130829456,
      0.3899890286077208,
      0.4157578702543199,
      0.41642231010734987,
      0.4163552379655553,
      0.4227579160113015,
      0.4024419727594999,
      0.34885646249823266
    ],
    [
      0.2558824374792019,
      0.35607113156438364,
      0.35516494964192225,
      0.3301719789982187,
      0.35337755655183445,
      0.3368326585690202,
      0.2768741371966956,
      0.3721427603840908,
      0.32452550408478187,
      0.0,
      0.35813007953089815,
      0.21894553563165786,
      0.3571154051112917,
      0.3286314454566224,
      0.29512529150398703,
      0.34354166391282703,
      0.324484800502409,
      0.35181307149624796,
      0.35116535206051647,
      0.32209748893626977,
      0.3568245924027369,
      0.2986600715249972,
      0.30700201196255694,
      0.33431946421183323,
      0.3168461016142081,
      0.3092057572008673,
      0.3749967112173431,
      0.3353760657493483,
      0.288598718228418
    ],
    [
      0.3018803151093763,
      0.493526214559427,
      0.443926682730333,
      0.4824822782038052,
      0.43608657790893024,
      0.38617777130083963,
      0.3982635243610202,
      0.4237873532830436,
      0.3863147308821515,
      0.4018700800623123,
      0.0,
      0.2660084025096794,
      0.44849582294767054,
      0.4468381137688384,
      0.45387029033279647,
      0.45732989915454136,
      0.3736911166950918,
      0.44643432302860697,
      0.4511448919100023,
      0.39719445986701385,
      0.4356287182557328,
      0.42145984983605644,
      0.3854818793321291,
      0.42780529845582516,
      0.41850484514742936,
      0.40437691702648193,
      0.4123977086876722,
      0.4167608904423046,
      0.36504915655906633
    ],
    [
      0.15637908573773185,
      0.1769975663139134,
      0.16403608353682708,
      0.15814380045903853,
      0.18756364229633604,
      0.2106134541901974,
      0.17839912322351226,
      0.1897596579859555,
      0.17354914021612688,
      0.18050463581281484,
      0.18670546486429052,
      0.0,
      0.17485398507290162,
      0.16125246905779012,
      0.1713920125930215,
      0.17575070114220193,
      0.15836451724540157,
      0.18159570149022497,
      0.17894828465233803,
      0.16938359867174757,
      0.1992451072199648,
      0.17376143221865537,
      0.18354556979428582,
      0.18139628010894082,
      0.18621865311385766,
      0.16322585636639597,
      0.18270521068230017,
      0.18356383097179174,
      0.17739924636137272
    ],
    [
      0.2988902649271157,
      0.39686266770814926,
      0.3400822086969757,
      0.3667302849240732,
      0.36582589809358024,
      0.3720878172180604,
      0.3201712503801619,
      0.4122624224007696,
      0.3453233720374327,
      0.41254605410715106,
      0.4190114793021431,
      0.24480096918942174,
      0.0,
      0.3776693141743206,
      0.3658694979222201,
      0.4367361366560174,
      0.3337179806230739,
      0.38012811122279344,
      0.4006630099994615,
      0.3715783311494909,
      0.4132740481198929,
      0.3570894151993875,
      0.37141882791353,
      0.36016211498940964,
      0.39951056082358716,
      0.3948330418387733,
      0.3852449151238708,
      0.37115651626531165,
      0.3403436649475329
    ],
    [
      0.3235479763211597,
      0.44886222309807033,
      0.505475385985255,
      0.435030627439734,
      0.3695276455424614,
      0.4260968042805833,
      0.29954462259099146,
      0.40524579025935226,
      0.3971869768047611,
      0.37266336182751036,
      0.3948347886072028,
      0.21973481378189552,
      0.37904914395563116,
      0.0,
      0.42474454933365235,
      0.43026921871576485,
      0.41152582908535296,
      0.41372721353709596,
      0.3974682188365888,
      0.31822695575266,
      0.3959568807592291,
      0.34228767956409967,
      0.3515753875775487,
      0.36768462538906443,
      0.3529751736734543,
      0.4167089249882787,
      0.37730474411487047,
      0.39857127231177647,
      0.38678195355073797
    ],
    [
      0.30780405913514763,
      0.36348036091687685,
      0.43542727691036087,
      0.34025595117231355,
      0.33804854643282267,
      0.32824990764156214,
      0.2972881518532997,
      0.3700461981424228,
      0.3716632587398503,
      0.33177481993761804,
      0.3688491421311244,
      0.23598818255418585,
      0.3421023683117783,
      0.4058715635322192,
      0.0,
      0.34963405164928574,
      0.3989918819876368,
      0.34585571080290745,
      0.3339541831344519,
      0.339187535065524,
      0.3850733561163078,
      0.3785626364514978,
      0.32482537283627533,
      0.36303663773503114,
      0.3352778531348688,
      0.3800096323246225,
      0.345738329618793,
      0.3450695964684951,
      0.4098683300654624
    ],
    [
      0.2682890497069186,
      0.3997621940205196,
      0.3585673105485958,
      0.4637727809694865,
      0.315559268246727,
      0.38661941646177245,
      0.25890343959482776,
      0.3412196074187752,
      0.3579426467958027,
      0.320922118746642,
      0.3846483614055871,
      0.21491667773202772,
      0.38760349880469924,
      0.385824175103181,
      0.3251976864274504,
      0.0,
      0.3127191859501677,
      0.3520341460765901,
      0.39695692956218576,
      0.30884257196892406,
      0.34648007747055365,
      0.3183399014576438,
      0.3009892390516029,
      0.3189033082158075,
      0.3345411595663812,
      0.34931482727140506,
      0.31562185125337083,
      0.36109224156703723,
      0.29059168117494183
    ],
    [
      0.28406278709439836,
      0.420508527097065,
      0.39902200580274116,
      0.3816247575509093,
      0.3688948480808283,
      0.38872000242221705,
      0.29557313278546227,
      0.41765560253529244,
      0.384675646647342,
      0.32708997617105084,
      0.4264441162157093,
      0.21838205562567614,
      0.3584127092312086,
      0.45520192735934284,
      0.47297393014668,
      0.41213029899338727,
      0.0,
      0.42781102257218695,
      0.36250317314336145,
      0.36495212409738964,
      0.3624522301307169,
      0.3544323656077366,
      0.3451916064846543,
      0.3681803801372301,
      0.3962778225099457,
      0.43061306122668497,
      0.4041611191809573,
      0.39948409469567814,
      0.4074126455181908
    ],
    [
      0.37269316375453543,
      0.45469391159117656,
      0.5057683564671038,
      0.432087294845374,
      0.4275441443794694,
      0.44031478195043094,
      0.37307257085729506,
      0.4877953910274493,
      0.4864940843838115,
      0.4470427921626283,
      0.47224639110317734,
      0.26467154870706233,
      0.4510586694224452,
      0.47813806674555503,
      0.4180919526258893,
      0.4463024455570803,
      0.42031327268168583,
      0.0,
      0.45662052622152327,
      0.4186669688124478,
      0.48472746991755855,
      0.39013133710095005,
      0.4000551928246199,
      0.4151868080702026,
      0.442490335234474,
      0.44483729170479136,
      0.43554866432040273,
      0.41668966738362023,
      0.4189616292367442
    ],
    [
      0.3251348169435928,
      0.4957586827990308,
      0.41836387506045636,
      0.4973827445354142,
      0.4143494324308479,
      0.443904360830337,
      0.33160424298108904,
      0.4436780628671797,
      0.42720677391071504,
      0.44067580486241176,
      0.4505427832629436,
      0.23807335308300104,
      0.429996297667254,
      0.4790906198205036,
      0.4161080664229495,
      0.4762368729797082,
      0.37514048834241254,
      0.457849758352483,
      0.0,
      0.42671690550471264,
      0.43884058744262466,
      0.39049510374583396,
      0.40014584853118196,
      0.42423603000324306,
      0.41636625673939354,
      0.42060011715953305,
      0.41364383668523375,
      0.44024719673094115,
      0.36678754448801953
    ],
    [
      0.3135902457873305,
      0.40537374233156687,
      0.44150000625108876,
      0.41503188201166585,
      0.4117791006316258,
      0.461853382177031,
      0.33479131613184787,
      0.437792551635958,
      0.3815549733907089,
      0.4059865559536737,
      0.41322660221309304,
      0.2528619434286159,
      0.4130112614848551,
      0.3933774571611459,
      0.4124909449703744,
      0.42216506447930957,
      0.37062147731732575,
      0.4095148580166268,
      0.4241337218670551,
      0.0,
      0.41667107054989616,
      0.38586855295131484,
      0.42210072475442617,
      0.4055393054036176,
      0.4103356264788871,
      0.4196089982478526,
      0.42481265074851016,
      0.4253421915461111,
      0.4219137266786688
    ],
    [
      0.368749587047259,
      0.48950390230576435,
      0.35613330547443156,
      0.4347434545211968,
      0.41940963537719145,
      0.42518296974034286,
      0.316944410734179,
      0.4376661570076874,
      0.39841460059709033,
      0.42651441512412536,
      0.45409623053403725,
      0.27063775900329223,
      0.390708019009647,
      0.4036755898760025,
      0.37779128113337634,
      0.41312008367564923,
      0.3721461994496662,
      0.4543175932425869,
      0.4284288941195966,
      0.3948127930763259,
      0.0,
      0.3552125645103694,
      0.4336295014945257,
      0.39047747432555857,
      0.37708503640449176,
      0.3907785321911057,
      0.40227071139491133,
      0.4105346886380803,
      0.35923300114690915
    ],
    [
      0.34724628774048427,
      0.4101317694592663,
      0.3991404322698291,
      0.41806803291343564,
      0.40290220477829464,
      0.3839376619558279,
      0.36729964876923615,
      0.39619165779878696,
      0.39771125041354916,
      0.38888277583715225,
      0.43331648418133506,
      0.2818683459698561,
      0.3903405838900995,
      0.3802064069984443,
      0.4021443663015385,
      0.4020941862699572,
      0.37449264881606825,
      0.40105351000840583,
      0.39187513655456097,
      0.40448278214593225,
      0.4183434721880881,
      0.0,
      0.39684748843808415,
      0.43512731772539137,
      0.40023250488277173,
      0.36286794565881086,
      0.4054195627222579,
      0.3961539856574796,
      0.3824321115342546
    ],
    [
      0.2947089613790239,
      0.3397884789647647,
      0.31023090342065096,
      0.31795871616858284,
      0.32854382341410804,
      0.3510974226952277,
      0.2826764798502168,
      0.3776670875316994,
      0.32586950537393444,
      0.3437797202086803,
      0.3404454070326268,
      0.2806224896886116,
      0.31862830130708275,
      0.34050940417946607,
      0.32231679886992204,
      0.3077361548397417,
      0.31940655033948384,
      0.32680845671931014,
      0.32190697292872517,
      0.3354221134420503,
      0.39102224702410515,
      0.3386794964342812,
      0.0,
      0.35903582291161906,
      0.37359793541706576,
      0.3458567231011609,
      0.3705359560767678,
      0.35447559562169917,
      0.33955676126204093
    ],
    [
      0.32640670466101773,
      0.4262273516919852,
      0.39197164326886114,
      0.40291785840791805,
      0.3981676904611662,
      0.37127903672684615,
      0.33062255363169446,
      0.37737403541247616,
      0.417265064500723,
      0.37691485425755955,
      0.39434288326991274,
      0.22437436794030052,
      0.3916305675805507,
      0.386872277244112,
      0.3493352328101744,
      0.40241723510239025,
      0.34931253924860295,
      0.40043163448375796,
      0.403519472481646,
      0.39477865556116876,
      0.394099428508643,
      0.3894323641439945,
      0.3630813592327513,
      0.0,
      0.3711803636828972,
      0.33134875701103517,
      0.40907137297291496,
      0.442961055888069,
      0.3715807314040076
    ],
    [
      0.3044955825369271,
      0.38935779809306426,
      0.3322169094753966,
      0.33797605674189724,
      0.36053340059917693,
      0.37611976572803485,
      0.2862535235112671,
      0.3555723930008854,
      0.346812599792542,
      0.3702307404110843,
      0.3966072355078516,
      0.24473821556248843,
      0.3967891583855394,
      0.33242652814477025,
      0.32193588808366647,
      0.3704196361479102,
      0.33063876740219955,
      0.38362872062838194,
      0.32483295680223745,
      0.35648339427003917,
      0.3774456798951067,
      0.3494791403200739,
      0.3815149562744322,
      0.3442400719293457,
      0.0,
      0.3278447769450803,
      0.3810227039270695,
      0.3599754493712335,
      0.3127055580525877
    ],
    [
      0.3751835270960766,
      0.4175096476334368,
      0.41500688708358835,
      0.4105321492395051,
      0.3780962637118055,
      0.3623059837432594,
      0.3404024156554899,
      0.43077899188219404,
      0.3843140509454599,
      0.3607974138458143,
      0.45122090842422136,
      0.23768317544910156,
      0.4117399755175568,
      0.4180810283870757,
      0.38804974836673667,
      0.43912309163719043,
      0.42178665422949924,
      0.43737116404491716,
      0.38209088053775675,
      0.33516479144757505,
      0.44027602795449217,
      0.34991042602486244,
      0.3637377787003586,
      0.3658930292053084,
      0.3752691623551929,
      0.0,
      0.4097285086338698,
      0.3765678832085819,
      0.3996303231613678
    ],
    [
      0.34219340985630464,
      0.4100046167315523,
      0.426486892760932,
      0.39106588763581507,
      0.4254713619988373,
      0.4264856599472475,
      0.3090927117042708,
      0.41086339463374744,
      0.40039340797853407,
      0.41894585820234553,
      0.39612750254041473,
      0.27663847678902553,
      0.3948461157141918,
      0.39277699323136495,
      0.35151829667790246,
      0.38338229897470977,
      0.4114145454658258,
      0.40708326353452096,
      0.3790210526410882,
      0.4234082250492348,
      0.43245939090349084,
      0.3751137447146704,
      0.4329478857698097,
      0.4318633088650701,
      0.39264877833582434,
      0.39703738614721673,
      0.0,
      0.44120363845520516,
      0.3921421484634746
    ],
    [
      0.3150940203546324,
      0.48180098749487543,
      0.4014179303200558,
      0.40536027457258017,
      0.36148662162232026,
      0.39967223037831934,
      0.2856599893686389,
      0.408279338710301,
      0.37118495376620997,
      0.33211226767785296,
      0.38523397012960614,
      0.22231299609142297,
      0.3252363106146492,
      0.39116066460916055,
      0.3495358819182288,
      0.42428879396019026,
      0.35801065801673904,
      0.3665619781777558,
      0.3701513712180613,
      0.348939426923653,
      0.35293398221935446,
      0.34517728216862276,
      0.3440634763644381,
      0.4427981841719182,
      0.3266172079102432,
      0.3550739649487167,
      0.3784999686014905,
      0.0,
      0.32661282138688463
    ],
    [
      0.3988884957947556,
      0.4277633614148686,
      0.4593312084658059,
      0.41160957327941383,
      0.4334020420528306,
      0.40285748672955335,
      0.35034736675934464,
      0.44377604498713397,
      0.42787197616209194,
      0.40773462588170917,
      0.425737255196329,
      0.28790916006670497,
      0.4085017060761047,
      0.49118312052991175,
      0.496722649793357,
      0.39664973199505815,
      0.4779046003395806,
      0.46033357079311976,
      0.403096794707104,
      0.4312655000001533,
      0.4143839625759447,
      0.48471570173463485,
      0.44223783986534015,
      0.4760941103022458,
      0.40629055765689714,
      0.46422292426909717,
      0.4323997912998043,
      0.43891911769342884,
      0.0
    ]
  ],
  "row_avgs": [
    0.1786565185811917,
    0.44997609598620775,
    0.40542752921332703,
    0.3918133105446948,
    0.3804884619485399,
    0.38457712743883427,
    0.3887315324913599,
    0.4120364675561015,
    0.39852611533658594,
    0.3262115265258994,
    0.4136710040127921,
    0.17733050397856917,
    0.36978536342691815,
    0.38437888527445657,
    0.3525691033858122,
    0.33843483402034374,
    0.37981585603800155,
    0.4322230260389109,
    0.41782773086368025,
    0.40188749766429227,
    0.39829351396983576,
    0.3918146629242571,
    0.33424586736438033,
    0.37817561041382775,
    0.3482963431264389,
    0.38850899600436767,
    0.3954512947758081,
    0.3634027697748901,
    0.42864822415794007
  ],
  "col_avgs": [
    0.3186032246430433,
    0.41416722242951004,
    0.39991262573021175,
    0.39690302126693044,
    0.3755397555348075,
    0.38670869822021803,
    0.30899203412146103,
    0.39618603377990763,
    0.3790671524484335,
    0.3717373858222719,
    0.40503477226121254,
    0.2479861235527069,
    0.37637515996665855,
    0.3988352648868735,
    0.37165081119583393,
    0.3993124397148567,
    0.3646926524214897,
    0.3970965313756344,
    0.37921723690180914,
    0.3689689356079713,
    0.3944556041517865,
    0.35456962147043697,
    0.3624450719926791,
    0.37698592655356455,
    0.36551187051361234,
    0.37502495340331227,
    0.3809852953921153,
    0.39162508516554256,
    0.3526152623133732
  ],
  "combined_avgs": [
    0.2486298716121175,
    0.4320716592078589,
    0.4026700774717694,
    0.39435816590581263,
    0.37801410874167374,
    0.3856429128295261,
    0.3488617833064105,
    0.4041112506680046,
    0.38879663389250974,
    0.3489744561740856,
    0.4093528881370023,
    0.21265831376563804,
    0.3730802616967883,
    0.39160707508066506,
    0.36210995729082307,
    0.3688736368676002,
    0.3722542542297456,
    0.41465977870727266,
    0.3985224838827447,
    0.3854282166361318,
    0.3963745590608111,
    0.37319214219734703,
    0.34834546967852975,
    0.37758076848369615,
    0.3569041068200256,
    0.38176697470383997,
    0.38821829508396166,
    0.37751392747021634,
    0.39063174323565664
  ],
  "gppm": [
    605.5035230038712,
    593.1111081421637,
    597.8858925900478,
    604.4928934871239,
    614.1321857302594,
    603.3657287777044,
    645.2319390155682,
    600.9345446992565,
    608.5643771763572,
    610.9236664900333,
    599.567981736189,
    664.4279502070443,
    611.9240371118786,
    600.1595555187134,
    613.2177310888492,
    600.4012522802448,
    613.5321160385145,
    601.8278080154469,
    610.5030522161472,
    613.2165022758247,
    604.7451904655378,
    622.3862132282976,
    619.5204702879249,
    609.8788515261765,
    613.8521121959575,
    613.41941090154,
    608.5865580940975,
    601.9615116107324,
    623.8509490841394
  ],
  "gppm_normalized": [
    1.418162698711573,
    1.306924961311457,
    1.3158504252048009,
    1.3375569715817286,
    1.3480951235499223,
    1.3271655502762727,
    1.4315014685216572,
    1.3191664212117982,
    1.3379396659247114,
    1.3377594595387092,
    1.310757400306766,
    1.4803424083016163,
    1.3433721297189094,
    1.3180469004900413,
    1.3483539067007226,
    1.3267755833114678,
    1.3443997428962102,
    1.3264717808537156,
    1.3400229254298173,
    1.3443801961629658,
    1.3276224649934387,
    1.3699329114078278,
    1.3605151536731153,
    1.3356212847277027,
    1.342119746800939,
    1.354648769471332,
    1.3312604039681806,
    1.3213315703696975,
    1.3662146979741627
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401,
    389,
    491,
    424,
    450,
    396,
    441,
    392,
    376,
    422,
    476,
    440,
    374,
    446,
    482,
    442,
    441,
    362,
    424,
    410,
    414,
    410,
    353,
    400,
    382,
    444,
    360,
    404,
    406,
    395,
    339,
    446,
    477,
    497,
    454,
    431,
    442,
    455,
    560,
    421,
    424,
    522,
    488,
    411,
    400,
    410,
    451,
    469,
    411,
    433,
    451,
    378,
    376,
    454,
    411,
    399,
    394,
    441,
    372,
    1166,
    412,
    405,
    456,
    421,
    410,
    395,
    446,
    422,
    371,
    413,
    468,
    458,
    414,
    419,
    422,
    375,
    410,
    404,
    390,
    423,
    455,
    433,
    401,
    393,
    441,
    387,
    438,
    457,
    475,
    438,
    496,
    454,
    447,
    436,
    483,
    459,
    425,
    390,
    418,
    596,
    408,
    457,
    477,
    438,
    347,
    421,
    393,
    423,
    422,
    403,
    347,
    442,
    425,
    397,
    418,
    466,
    401,
    668,
    451,
    488,
    442,
    477,
    399,
    415,
    391,
    477,
    467,
    410,
    552,
    421,
    402,
    423,
    411,
    434,
    392,
    381,
    432,
    411,
    388,
    434,
    406,
    404,
    403,
    408,
    482,
    364,
    515,
    394,
    445,
    456,
    590,
    399,
    441,
    475,
    430,
    427,
    416,
    502,
    397,
    443,
    381,
    435,
    391,
    431,
    382,
    457,
    431,
    379,
    342,
    407,
    378,
    371,
    372,
    478,
    412,
    676,
    441,
    444,
    451,
    462,
    493,
    380,
    407,
    445,
    468,
    464,
    604,
    461,
    488,
    449,
    498,
    445,
    422,
    409,
    421,
    392,
    428,
    402,
    455,
    441,
    429,
    408,
    494,
    372
  ],
  "response_lengths": [
    3433,
    2452,
    2440,
    2458,
    2513,
    2676,
    2202,
    2259,
    2489,
    2559,
    2577,
    3485,
    2590,
    2786,
    2464,
    2784,
    2456,
    2331,
    2298,
    2232,
    2138,
    2385,
    2309,
    2636,
    2514,
    2346,
    2374,
    2772,
    2064
  ]
}