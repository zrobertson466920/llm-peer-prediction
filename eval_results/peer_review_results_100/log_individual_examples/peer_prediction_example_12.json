{
  "example_idx": 12,
  "reference": "Published as a conference paper at ICLR 2023\n\nMULTI-LEVEL PROTEIN STRUCTURE PRE-TRAINING WITH PROMPT LEARNING\n\nZeyuan Wang1,2,7∗ Qiang Zhang1,2∗† Haoran Yu2,3 Zhichen Gong2,6 Huajun Chen1,2, 7, 8† 1College of Computer Science and Technology, Zhejiang University 2ZJU-Hangzhou Global Scientific and Technological Innovation Center 3College of Chemical and Biological Engineering, Zhejiang University 4Vecx Biomedicines Inc., 5MindRank AI Ltd., 6University College London 7AZFT Joint Lab for Knowledge Engine, 8East China Sea Laboratory\n\nShuangwei Hu4 Xurui Jin5\n\nyuanzew,qiang.zhang.cs,yuhaoran,huajunsir\n\n@zju.edu.cn\n\n{ shuangwei@vecx.bio, xurui@mindrank.ai, ucabzgo@ucl.ac.uk\n\n}\n\nABSTRACT\n\nA protein can focus on different structure levels to implement its functions. Each structure has its own merit and driving forces in describing specific characteristics, and they cannot replace each other. Most existing function prediction methods take either the primary or the tertiary structure as input, unintentionally ignoring the other levels of protein structures. Considering protein sequences can determine multi-level structures, in this paper, we aim to realize the comprehensive potential of protein sequences for function prediction. Specifically, we propose a new prompt-guided multi-task pre-training and fine-tuning framework. Through the prompt-guided multi-task pre-training, we learn multiple prompt signals to steer the model, called PromptProtein, to focus on different levels of structures. We also design a prompt fine-tuning module to provide downstream tasks the on-demand flexibility of utilizing respective levels of structural information. Extensive experiments on function prediction and protein engineering show that PromptProtein outperforms state-of-the-art methods by large margins. To the best of our knowledge, this is the first prompt-based pre-trained protein model.\n\n1\n\nINTRODUCTION\n\nPre-trained language models (PTLMs) have prevailed in natural language processing (NLP). Recently, some methods (Alley et al., 2019; Elnaggar et al., 2021; Rives et al., 2021) use PTLMs to encode protein sequences to predict biological functions, which are called pre-trained protein models (PTPMs). In contrast to natural languages, there are four distinct levels of protein structures (Kessel & Ben-Tal, 2018). The primal is the protein sequence consisting of amino acids, the second refers to the local folded structures (e.g., α helix and β pleated sheet), the tertiary describes the natural folded three-dimensional structure, and the quaternary is a protein multimer comprising multiple polypeptides. A protein can focus on different structure levels to implement its specific functions, including reserving a piece of the sequence, manifesting the whole 3D structure as conformational elements, or even cooperating with other proteins. Therefore, when predicting protein functions, it is vital to flexibly utilize multi-level structural information.\n\nAlphaFold2 (Jumper et al., 2021) makes great progress in the tertiary structure prediction based on protein sequences. However, directly learning from predicted structures can be unachievable as the prediction of proteins without homologous sequences is inaccurate. More importantly, the quaternary structure of protein multimers which faithfully depicts protein functions is usually different from the tertiary (see Figure 1) and reliable predictive models have not been released. Fortunately, protein sequences are easy to obtain and can determine all the other levels of structures. This paper aims to realize the full potential of protein sequences in function prediction by prompting a\n\n∗Equal contribution and shared co-first authorship. †Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nPTPM to exploit all levels of protein structures during pre-training. The main challenges are twofold: 1) how to design proper pre-training tasks for different protein structures? and 2) how to efficiently integrate these tasks in the pre-training phase and transfer the implicit protein structure knowledge for function prediction in fine-tuning phase.\n\nFor the first challenge, we design three complementary pre-training tasks across multiple structure levels, targeting both fine and coarse resolutions. Specifically, we use the de facto Mask Language Modeling (MLM) task to exploit the primary structure information, where the model needs to predict randomly masked amino acids in a protein. For the secondary and tertiary structure, we propose the alpha-carbon CooRDinate prediction (CRD) task, where the model should output the relative positions between residues. For the quaternary structure, we propose the Protein-Protein Interaction prediction (PPI) task, where the model is required to estimate the interaction probability. We collect millions of data covering different levels of protein structures from UniRef50 (Consortium, 2021), Protein Data Bank (Berman et al., 2000), and STRING (Szklarczyk et al., 2019).\n\nFigure 1: A comparison of protein CDK1 in the tertiary (left) and quaternary (right) structures.\n\nFor the second challenge, a straightforward strategy is to leverage multi-task learning to combine the losses of different pre-training tasks. However, many works (Wu et al., 2019; Yu et al., 2020) find that task interference is common when tasks are diverse. This problem can be more severe in multi-task pre-training due to the gap between pre-training and downstream tasks, causing negative knowledge transfer. For example, BERT (Kenton & Toutanova, 2019) leverages MLM and Next Sentence Prediction (NSP) to learn the sequential dependency and sentence relationship simultaneously, while RoBERTa (Liu et al., 2019) finds the performance will be slightly improved when removing the NSP loss. We postulate this problem also exists in multi-level protein structures, as different structures can be inconsonant. The MLM task emphasizes the neighboring relations along the sequence, while the CRD task shall focus more on long-range amino acid pairs which can be spatially close in the tertiary structure.\n\nTo address this challenge, inspired by recent prompt learning, we propose a prompt-guided multitask pre-training and fine-tuning framework, and the resulting protein model is called PromptProtein. The prompt-guided multi-task pre-training associates multiple pre-training tasks with dedicated sentinel tokens, called prompts. To utilize the prompt tokens, we introduce a prompt-aware attention module, which modifies two components of the Transformer architecture: 1) Attention mask, which is designed to block attention calculation from input data to a prompt as a prompt should be taskdependent instead of sample-dependent. 2) For skip connection, a prompt is used to calculate a skip weight, which can filter out task-irrelevant information. At the fine-tuning phase, we propose a prompt fine-tuning module to coordinate all prompt tokens, such that the model is capable of leveraging multi-level protein structure information flexibly, enabling the positive transfer of learned structural knowledge to downstream tasks.\n\nWe conduct experiments on function prediction and protein engineering as downstream tasks, where PromptProtein significantly outperforms state-of-the-art on all datasets, especially on low-resource protein engineering tasks where PromptProtein achieves an average improvement of 17.0%.\n\n2 RELATED WORKS\n\nProtein Representation Models. Proteins have complex structures that determine their biological functions (Epstein et al., 1963). A growing body of work focuses on how to leverage structural information. Since evolution through natural selection has spoken protein sequences as their “natural language”, various natural language processing methods have been extended to proteins. Asgari & Mofrad (2015); Yang et al. (2018) apply word embedding algorithms (Mikolov et al., 2013) to obtain protein representations. Dalkiran et al. (2018); ̈Ozt ̈urk et al. (2018) use one-dimensional con-\n\n2\n\nAreas with significant changeBinding siteResidue IndexResidue IndexPublished as a conference paper at ICLR 2023\n\nFigure 2: The architecture overview of PromptProtein. In the pre-training stage, we pre-train our model with three structure-related tasks, including mask language modeling, alpha-carbon prediction, and protein-protein interaction prediction. For each task, the model takes the protein sequence and the task-specific token as input and learns to produce a representation encoding the corresponding structure information. In the fine-tuning stage, a prompt-tuning module τθ( ) can flexibly com- ·\nbine structure information via the learned prompt tokens for diverse downstream tasks.\n\nvolutional neural networks to predict the functions. Furthermore, Alley et al. (2019); Elnaggar et al. (2021); Rives et al. (2021) explore whether the pre-training and fine-tuning paradigm, the transformer architectures, and the objective functions can effectively transfer from natural languages to proteins. Zhang et al. (2021a) align the amino acid sequence and the text sequence to obtain informative protein representation. To utilize the tertiary structure, Hermosilla et al. (2020); Somnath et al. (2021); Ganea et al. (2021); Zhang et al. (2022) build protein graphs and employ message-passing neural networks to produce structure-aware representations. Bepler & Berger (2021) employ contact map prediction and structural similarity prediction to pre-train the protein model. Although primary and tertiary structures have been studied, few works try to enrich protein representation with the quaternary structure which faithfully depicts protein functions. In this paper, we show that systematic modeling and flexible utilization of multi-level structures are the keys to improving the performance of function prediction and protein engineering.\n\nMulti-task Learning. The goal of multi-task learning is to take advantage of inductive transfer across tasks and achieve better generalization performance. When tasks are diverse, using a naive shared MTL model can suffer from task interference. Prior methods have been proposed to deconflict gradients from different tasks. Chen et al. (2018) dynamically adjust gradient magnitudes so different tasks can be trained at similar scales. Yu et al. (2020) take the gradient direction into account and drop the projection of one task gradient direction onto another if they are conflicting. Rather than clipping the conflict gradient direction, Javaloy & Valera (2021) learn a rotation matrix for each task to bring different optima closer to each other. However, these methods are not designed for multi-task pre-training and cannot properly deal with the knowledge transferability to downstream tasks. We provide a schematic comparison of these methods in Appendix A.1.\n\nPrompts for Pre-trained Models. In-context learning (Brown et al., 2020) is introduced to steer the pre-trained model to produce task-desired representations. In the NLP area, the prevailing approaches to designing prompts can be divided into two categories: discrete prompt designing and continuous prompt tuning. The discrete prompt technique (Schick & Sch ̈utze, 2021) adds task description tokens from a vocabulary to the context to obtain enriched sentence embeddings. However, the hand-crafted prompts may provide disturbance of human bias and are limited to discrete vocabulary spaces. In contrast, Li & Liang (2021); Zhang et al. (2021b) generate optimal prompt vectors in continuous spaces. Inspired by these works, we extend the concept of prompt tuning to the pre-training stage, associate multi-level protein structural information with dedicated prompt tokens during pre-training, and adaptively combine these learned prompts for downstream tasks.\n\n3 METHODOLOGY\n\nTo acquire multiple information from the input data x, conventional multi-task learning usually produces a universal representation h. The whole objective can be formulated as a weighted sum of individual task objectives: are the hyper-parameters to balance these losses. However, multi-level protein structures can be inconsonant: the primary structure focuses\n\ni αiLi(h), where\n\n= (cid:80)\n\nαi}\n\nL\n\n{\n\n3\n\n[MLM][CRD][PPI]Prompt-AwareTransfomerMASLSCV[Mask]DKMVVT.........Mask Language ModelingAlpha-CarbonPredictionInteractionPredictionMASLSCV[Mask]DKMVVTARF5trpA?ComposedPromptMASLSCVSDKMVVT...DiverseTasksFunction AnnotationStability PredictionFitness Prediction(cid:31)(cid:30)(cid:29)(cid:30)(cid:28)(cid:27)(cid:26)(cid:27)(cid:29)(cid:25)(cid:24)(cid:30)(cid:29)(cid:23)(cid:22)(cid:21)(cid:31)(cid:29)(cid:20)(cid:28)(cid:19)(cid:26)(cid:18)(cid:31)(cid:20)(cid:20)(cid:17)[[Prompt-AwareTransfomerphpxp(cid:31)hp(cid:31)xPublished as a conference paper at ICLR 2023\n\nmore on the dependency along the sequence, whereas the tertiary and quaternary structure weights more on the spatial organization, which can cause the problem of task interference. This problem can lead to more severe negative transfer in multi-task pre-training due to the gap between pre-training and downstream tasks. To solve this problem, we propose a prompt-guided multi-task pre-training and fine-tuning framework that utilizes a prompt token p to produce a task-specific representation hp. Multiple learned tokens can be flexibly combined to steer the pre-trained model for various downstream tasks, bridging the gap between pre-training and downstream tasks.\n\nThis section first describes how to use prompts to modify the Transformer architecture, such that different tasks can be processed by different neural layers and reduce task interference. Then we present the three pre-training tasks to acquire multi-level protein structural information: (1) masked language modeling, (2) alpha-carbon coordinate prediction, and (3) protein-protein interaction prediction. Finally, we introduce the prompt-guided pre-training and fine-tuning framework where multiple information can be acquired in the pre-training stage and combined on-demand for downstream tasks. The resulting PromptProtein model is illustrated in Figure 2.\n\n3.1 PROMPT-AWARE ATTENTION MODULE\n\nTo reduce interference between pre-training tasks, we use the prompt token to modify the Transformer architecture so that multiple information can be effectively acquired by the pre-trained model. Specifically, we modify two parts of the Transformer: attention mask and skip connection, and the resulting architecture is called Prompt-aware Transformer. Given an input protein sequence x and is concatenation. Let a prompt token p, we define the whole input xp denote xp = x xi\n\np be the i-th token of the whole input and h(l)\n\np be the representation of xp at the l-th layer.\n\np , where\n\n||\n\n||\n\nconventional\n\nAttn(h(l)\n\nformulated as:\n\nAttention mask. The selfattention is p ) = Softmax((QK T )/√d)V, where Q, K, and V are the linear projection of h(l) p . Each token in the whole sequence can attend to others at any position which means the condition prompt will be affected by the input sequence. A more reasonable way is to keep only the effect of the prompt on the input sequence and eliminate the reverse effect, as a prompt should be task-dependent instead of sample-dependent. As illustrated in Figure 3, we design an attention mask matrix M to fulfill this requirement. Let Mij denote the (i, j)-element of the mask matrix, and we define:\n\nMij =\n\n(cid:40)\n\n0, xi\n\np ∈ 1, others.\n\np and xj\n\np ∈\n\nx\n\n(1)\n\nSkip connection. Skip connection enables deep neural networks easier to train (He et al., 2016). To encourage different tasks to be processed by different layers and reduce task interference, we design a weighted skip connection. That is, the prompt token is used to calculate a weight for the output of the attention module. The whole process can be: = h(l)\n\nh(l+1)\n\np + (1\n\np\n\n−\n\nFigure 3: Prompt-aware Attention Module. A pink circle represents an amino acid token and a purple circle represents a prompt token. We decouple prompt tokens from amino acid tokens by the attention mask. The embedding of decoupled prompt token determines the weight of the residual connection. In the fine-tuning stage, we use a prompt-tuning module τθ( ) to learn the downstream task- ·\ndesired composed prompt. p )Attn(h(l) g(l) p ),\n\n(2)\n\np , a scalar, is linear projection of l-th layer embedding of prompt p. After L layers of the\n\nwhere g(l) prompt-aware attention module, we have the task-specific representation hp = h(L) p .\n\n3.2 PROTEIN MULTI-LEVEL STRUCTURES LEARNING\n\nTo acquire multi-level protein structure information, we consider three complementary pre-training tasks: (1) masked language modeling, which has been commonly used by existing PTPMs and can\n\n4\n\n(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:27)(cid:24)(cid:25)(cid:23)(cid:22)(cid:21)(cid:20)(cid:27)(cid:22)(cid:28)(cid:21)(cid:19)(cid:18)(cid:24)(cid:17)(cid:27)(cid:16)(cid:25)(cid:27)(cid:24)(cid:25)(cid:23)(cid:22)xAttention Mask(cid:31)(cid:30)(cid:29)Skip Connection(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)x1x2x3x1x2x3x1x2x3pppgpxPublished as a conference paper at ICLR 2023\n\ncapture the primary structure information; (2) coordinate prediction, which acquires the secondary and tertiary structure; and (3) interaction prediction, which acquires the quaternary structure.\n\nMasked language modeling. This task uses all available amino acid tokens to recover the masked be the vocabulary of amino acid tokens. The ones. Let Y be the set of masked out tokens, and MLM loss is formulated:\n\nV\n\nq(y\n\nhp) = |\n\n(cid:80)\n\nexp(p(y v∈V exp(p(v\n\nhp)) |\n\nhp)) |\n\n,\n\nL\n\nMLM(hp) =\n\n(cid:88)\n\ny∈Y\n\n−\n\nlog q(y\n\nhp).\n\n|\n\n(3)\n\nAlpha-Carbon Coordinate Prediction. Since a secondary structure can be inferred from the protein 3D coordinates (Kabsch & Sander, 1983), we use an α-C coordinate prediction task to learn both secondary and tertiary structures. Given the sequence length , we denote the ground-truth |\nR|x|×3 and the structure predictor, a 2-layer MLP naturally folded 3D structure of protein as Z R|x|×3. By translating and rotating (Kabsch, network, as κ, then the predicted structure is κ(hp) 1976) the predicted structure, we can get the minimal root mean square deviation between groundtruth and predicted structure, and the loss is calculated based on this deviation. In this way, there is no need to consider spatial invariance or equivariance, but only need to focus on the relative positions between residues. The CRD loss can be calculated as the mean square error (MSE):\n\nx |\n\n∈\n\n∈\n\nCRD(hp) = MSE(Z, Kabsh(κ(hp))).\n\nL\n\n(4)\n\nProtein-Protein Interaction prediction. To acquire the quaternary structure information, we conduct the third pre-training task: predicting whether the m-th and n-th proteins can interact with each other within batched data. Let hm p be the m-th protein in a mini-batch and ym,n is the ground-truth. We first calculate pair-aware protein representation hm,n\n\n, then formulate the PPI loss:\n\nAttnm,n = Sigmoid(\n\np (hm\n\np W )(hn\n\npW )T\n\n),\n\n√d\n\nhm,n\n\np = mean(AttnT\n\nPPI(hp) =\n\nL\n\n(cid:88)\n\nm,n∈N\n\nm,nhm p ) BCE(ym,n, p(ym,n)\n\nmean(Attnm,nhn hm,n |\n\n||\n\np\n\n),\n\np)),\n\n(5)\n\nwhere W the batch size. More details of the pre-training tasks are provided in Appendix A.2.\n\nRdW ×dW is a projection matrix, BCE is the binary cross-entropy loss function, N is\n\n∈\n\n3.3 PROMPT-GUIDED MULTI-TASK PRE-TRAINING AND FINE-TUNING\n\nCorresponding to the three pre-training tasks, the prompt can be instantiated as one of the three tokens, i.e., p . The task-specific representation is thus denoted as h[MLM], h[CRD], h[PPI]. The objective function of the prompt-guided multi-task pre-training can be formulated as:\n\n[MLM], [CRD], [PPI]\n\nP =\n\n∈\n\n{\n\n}\n\nL\n\n= α1L\n\n(6) When we pre-train a model with multiple tasks as Equation 6, both model parameters ψ and prompts p are optimized. In this way, the model does not necessarily need to learn the optimal representation for all tasks, but only needs to learn the respective optimal representation for each task. Hence, the problem of task interference can be alleviated.\n\nCRD(h[CRD]) + α3L\n\nMLM(h[MLM]) + α2L\n\nPPI(h[PPI]).\n\nFurthermore, to bridge the gap between pre-training and downstream tasks, since the model can acquire each type of information conditioned on the learned prompt tokens, we can combine these tokens with prompt-tuning to flexibly mix the acquired information on-demand. We denote a prompttuning module as τθ( ), and the downstream task-desired protein representation hp′ can be obtained ·\nby feeding the tuned prompt p′\n\np′ = τθ(p[MLM], p[CRD], p[PPI]). (7) Then the pre-trained model can produce hp′ and conduct predictions for the downstream task of interest. Equation 7 shows how to flexibly utilize the pre-training task information at the fine-tuning stage. Note that, in the pre-training stage, we only append one prompt to acquire one type of taskspecific information, while in the fine-tuning stage, we feed all the learned prompt tokens to τθ( )\n· and flexibly combine the acquired information. Here, we leverage a linear layer as our prompt-tuning module to combine three learned prompts. For sake of understanding, we provide the pseudo-code of the prompt-guided multi-task pre-training and fine-tuning framework in Appendix A.3.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Model performance on EC numbers and GO terms prediction tasks. from Wang et al. (2022),\n\n: the results taken from Zhang et al. (2022).\n\n‡\n\n: the results taken\n\n†\n\nDATASET\n\nCNN RESNET LSTM TRANSFORMER\n\nGAT† GVP† DEEPFRI GearNet − Edge‡\n\nESM − 1b‡ ProtBERT − BFD† LM − GVP† MT-LSTM\n\nMTL GRADNORM ROTOGRAD PROMPTPROTEIN (OURS)\n\nEC AUPRpair\n\nGO-BP\n\nGO-MF\n\nGO-CC\n\nFmax AUPRpair\n\nFmax AUPRpair\n\nFmax AUPRpair\n\nFmax\n\n0.540 0.137 0.032 0.187\n\n0.320 0.482 0.547 0.892\n\n0.889 0.859 0.710 0.851\n\n0.892 0.893 0.895 0.915\n\n0.545 0.187 0.082 0.219\n\n0.368 0.489 0.631 0.874\n\n0.864 0.838 0.664 0.817\n\n0.869 0.874 0.876 0.888\n\n0.165 0.166 0.130 0.135\n\n0.171 0.224 0.282 0.292\n\n0.343 0.188 0.302 0.324\n\n0.325 0.331 0.334 0.363\n\n0.244 0.280 0.248 0.257\n\n0.284 0.326 0.399 0.490\n\n0.470 0.279 0.417 0.442\n\n0.445 0.466 0.470 0.495\n\n0.380 0.281 0.100 0.172\n\n0.329 0.458 0.462 0.596\n\n0.639 0.464 0.580 0.608\n\n0.651 0.647 0.648 0.665\n\n0.354 0.267 0.166 0.240\n\n0.317 0.426 0.465 0.650\n\n0.657 0.456 0.545 0.591\n\n0.640 0.643 0.638 0.677\n\n0.261 0.266 0.150 0.170\n\n0.249 0.278 0.363 0.336\n\n0.384 0.234 0.423 0.381\n\n0.415 0.415 0.416 0.457\n\n0.387 0.403 0.320 0.380\n\n0.385 0.420 0.460 0.486\n\n0.488 0.408 0.527 0.492\n\n0.503 0.504 0.509 0.551\n\n4 EXPERIMENTS\n\n4.1 PRE-TRAINING SETUP\n\nFor the primary structural information, we use UniRef50 (Suzek et al., 2015) which is a clustering of UniRef90 seed sequences at 50% sequence identity. For the secondary and tertiary structural information, we use Protein Data Bank (PDB) (Berman et al., 2000), which includes 200,000 protein 3D structures obtained by experimental methods. For the quaternary structure information, we use the STRING dataset (Szklarczyk et al., 2019) that contains amino acid sequences and proteinprotein interaction pairs. In the STRING dataset, protein interactions are divided into 7 categories. We selected the physical-only interaction subset from STRING which contains 65 million protein sequences from 14,095 species and 2.7 billion protein-protein interaction pairs.\n\nWe implement PromptProtein using Pytorch (Paszke et al., 2019) and Fairseq (Ott et al., 2019). PromptProtein has 650M parameters with 33 layers and 20 attention heads. The embedding size is 10−4 with no weight decay. We use an inverse square root learning 1280. The learning rate is 1 A100 40G GPUs for 270k steps of updates. After rate schedule. All models are trained on 2 pre-training, the average error of the coordinate prediction task on a single residue is 5 ̊A, and the accuracy of physical binding prediction is greater than 90.0%. Unless otherwise specified, we use this model in all downstream experiments. The source code will be available online. Please refer to Appendix B for the details of all the pre-training and downstream task dataset statistics.\n\n×\n\n×\n\n4.2 DOWNSTREAM TASKS: FUNCTION ANNOTATION\n\nDatasets and Metrics. Gene ontology (GO) terms and enzyme commission (EC) numbers are two standard classification schemes that organize myriad protein functions. These function prediction tasks can be regarded as multiple binary classification tasks. We follow the dataset split method in (Gligorijevi ́c et al., 2021). The evaluation metrics are protein-centric maximum F-score (Fmax) and term-centric area under precision-recall (AUPR) curve, which are used in the CAFA challenges (Radivojac et al., 2013).\n\nBaselines. There are four categories of baselines. (1) Sequence-based encoders. CNN (Shanehsazzadeh et al., 2020), ResNet, LSTM, and Transformer (Rao et al., 2019) only take amino acid sequence as input; (2) Geometric learning method. GAT (Veliˇckovi ́c et al., 2018), GVP (Jing et al., 2020), DeepFRI (Gligorijevi ́c et al., 2021), and GearNet-Edge (pre-trained by Multiview Contrast) (Zhang et al., 2022) take protein 3D coordinates as additional input to obtain informative representation; (3) Pre-trained protein models. ESM-1b (Rives et al., 2021), ProtBERT-BFD (Elnaggar et al., 2021), and LM-GVP (Wang et al., 2022) learn the pattern from large protein corpus. MT-LSTM (Bepler & Berger, 2021) uses contact map and structure similarity to enrich the embed-\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Model performance on protein engineering tasks. Results with two decimal places are token from Dallago et al. (2021).\n\nDATASET\n\nSTABILITY\n\nFLUORE.\n\nCNN RESNET LSTM ESM-UNTRAINED\n\nESM-1B ESM-1V PROTBERT-BFD LSTM-MT PROMPTPROTEIN (OURS)\n\n0.51 0.73 0.69 0.452\n\n0.71 0.726 0.732 0.741 0.767\n\n0.67 0.21 0.67 0.337\n\n0.68 0.507 0.675 0.648 0.683\n\nTHERMO MIXED\n\nAAV 1-VS-R\n\n1-VS-R\n\nGB1 2-VS-R\n\n3-VS-R\n\n0.34 0.353 0.317 0.36\n\n0.68 0.67 0.651 0.665 0.694\n\n0.48 0.173 0.215 0.01\n\n0.04 0.18 0.234 0.258 0.551\n\n0.17 0.117 0.124 0.05\n\n0.32 0.32 0.303 0.335 0.403\n\n0.32 0.210 0.349 0.05\n\n0.36 0.32 0.387 0.402 0.550\n\n0.83 0.291 0.491 0.46\n\n0.54 0.77 0.654 0.741 0.783\n\ndings. (4) Multi-task learning framework. We employ naive multi-task learning (MTL) and two optimization methods (GradNorm (Chen et al., 2018), RotoGram (Javaloy & Valera, 2021)).\n\nResults. We present the evaluation results of proposed PromptProtein and state-of-the-art baselines in Table 1. Compared with all baselines, PromptProtein achieves new state-of-the-art performance on all tasks, which indicates that systematic modeling of multi-level structure information is beneficial. Although the multi-task learning baselines integrate the same information as PromptProtein, they cannot learn multiple information well and transfer properly to downstream tasks. Their inferior performance in GO-BP and GO-CC suggests that there is a gap between downstream task-desired representations and universal pre-trained representations. Flexible composing of structural information significantly improves the performance of the model for downstream tasks.\n\n4.3 DOWNSTREAM TASKS: PROTEIN ENGINEERING TASKS\n\nDatasets and Metrics. Protein engineering is regarded as a sequence regression task that, given a protein, models are required to identify the functional strength, often termed the fitness landscape. Here, we employ five datasets (stability, fluorescence, thermostability, AAV, and GB1) coming from TAPE (Rao et al., 2019) and FLIP (Dallago et al., 2021) to evaluate whether the model can produce accurate quantitative predictions of these functions. We report the commonly-used Spearman’s ρ (rank correlation coefficient) to measure the degree to which the landscape was learned. Results of other tasks on FLIP can be found in Appendix 5.\n\nBaselines. For proteins without 3D structures, geometric methods cannot directly apply to these tasks. We choose sequence-based methods (CNN, LSTM, Transformer) and pre-trained protein methods (ESM-1b, ESM-1v (Meier et al., 2021), ProteinBert-BFD, LSTM-MT) as baselines for protein engineering tasks. As Dallago et al. (2021) purport that the various pooling choices perform inconsistently across datasets and splits, for a fair comparison, we utilize the mean pooling method to obtain protein representation.\n\nGB1\n\nAAV\n\nMETHOD\n\nCONVENTIONAL MTL. PROMPTPROTEIN\n\nTable 3: Ablation of PromptProtein with different components.\n\nResults. From Table 2, we observe that PromptProtein obtains better It performance than all baselines. confirms that pre-training on structural objectives contributes to protein engineering tasks and systematic modeling of protein multi-level structure leads to further improvements. Note that LSTM-MT, which leverages the tertiary structural information to enhance protein representations, cannot surpass ESM-1b on all datasets, while our proposed approach obtains superior performances. This observation demonstrates that not all structural information leads to positive transfer and flexible utilization of structural information is the key to improved performance. Moreover, PromptProtein can obtain 17.0% improvement on average in low-resource settings of the AAV and GB1 datasets, compared\n\n- ATTENTION MASK - LAYER SKIP - MLM OBJECTIVE - CRD OBJECTIVE - PPI OBJECTIVE\n\n0.531 0.520 0.493 0.535 0.532\n\n0.264 0.270 0.240 0.262 0.253\n\n0.663 0.659 0.629 0.647 0.654\n\n0.651 0.672\n\n0.525 0.544\n\n0.238 0.279\n\nTHERMO\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Skip connection visualization and prompt correlation. (a) We visualize the learned skip weight at all neural layers. The darkness of a block represents the weight of that block utilized for the given prompt. (b) We provide the Pearson’s correlation between skip weights. The skip patterns between the [MLM] prompt and the other two prompts are negatively correlated, whereas the pattern between the tertiary and quaternary structures is positively correlated.\n\nto the well-performed PTPM baselines. These results indicate that the prompt-guiding PTPM is a better few-shot learner.\n\n4.4 ABLATION STUDY\n\nThe ablation study is conducted to validate the effectiveness of designed modules in PromptProtein, i.e., prompts, attention mask, or skip connection. As illustrated in Table 3, the performance will decay if any one of the modules is absent, demonstrating that all the modules are advantageous. Furthermore, we notice that skip connection contributes most to the performance, confirming the necessity of reducing task interference.\n\n4.5 ANALYSIS AND DISCUSSION\n\nHow do prompts determine the processing pathways of structural information?\n\nIn Figure 4(a), we visualize the skip weights of three pre-trained prompts at different neural layers, and compute the Pearson’s correlation (Benesty et al., 2009) of these skip weights to measure the mutual correlations between the pre-training tasks (Figure 4(b)). We have the following key observations. (a) The skip weights are similar in the bottom layers (1-13) across all prompts, indicating all three tasks are processed by these layers. The MLM task information is mainly acquired by the middle layers (14-29), whereas the CRD and PPI information is more acquired by the top layers (30-33). (b) We clearly observe that the [CRD] and [PPI] prompts are more correlated. This is consistent with the intuition that the tertiary and quaternary levels are 3D structures whose amino acids attend to spatially adjacent neighbors, resulting in similar skip weight patterns. Further analysis of the model layer can be found in Appendix B.3.\n\nCan PromptProtein learn multi-level structures?\n\nTo examine whether prompt-guided pre-training can learn multiple structure information, we conduct experiments to visualize the protein representations conditioned on different pre-trained prompt tokens. We use t-SNE (van der Maaten & Hinton, 2008) to reduce the dimension of embeddings. Figure 5(a) illustrates amino acid embeddings conditioned on [MLM]. We observe that amino acid embeddings in a protein are grouped according to their type. Figure 5(b) illustrates amino acid embeddings conditioned on [CRD]. We find that amino acids are linearly arranged in 2D space along their sequence in the protein. To obtain a more accurate relationship between representations and structures, we compare the protein contact map and the coordinate of embedding. The strong correlation between them demonstrates the CRD objective can effectively learn information about protein 3D structures. In Figure 5(c), we visualize the amino acid embeddings with traditional multi-task pre-training and highlight serine (a class of amino acids). The embeddings attempt to merge multiple structural features at the same time, which leads to an unclear pattern. These results show that prompt-guided pre-training mitigates task interference and allows the multiple structure information to be learned well, resulting in promising performance.\n\n8\n\n[MLM][CRD][PPI]1102030331.31.21.11.00.90.8(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:25)(cid:24)(cid:23)(cid:22)(cid:21)(cid:26)(cid:25)(cid:28)(cid:20)(cid:19)(cid:21)(cid:18)(cid:28)(cid:17)(cid:21)(cid:16)(cid:15)(cid:30)(cid:14)(cid:21)(cid:13)(cid:30)(cid:22)(cid:21)(cid:26)(cid:25)(cid:31)(cid:12)(cid:29)(cid:28)(cid:11)(cid:10)(cid:26)(cid:9)(cid:18)(cid:22)(cid:28)(cid:27)(cid:26)(cid:10)(cid:10)(cid:24)(cid:14)(cid:30)(cid:22)(cid:21)(cid:26)(cid:25)1.0-1.0[MLM][CRD][PPI][MLM][CRD][PPI]0.0(cid:8)(cid:8)(cid:8)1.01.00.80.8-0.4-0.4-0.7-0.71.0Published as a conference paper at ICLR 2023\n\nFigure 5: The comparison of amino acid embeddings with different learning methods. We visualize protein representations from prompt-guided multi-task pre-training in (a) and (b), and naive multi-task learning in (c). Each letter represents an amino acid and is colored according to the physicochemical properties in (a), and the secondary structure types in (b) and (c). The superscripts of letters represent the sequential number of amino acids from the C-terminal to the N-terminal.\n\nFigure 6: Visualization of the difference of [MLM] and [PPI] prompts. The two proteins are Transcription initiation factor TFIID subunit4 (TAF4) and Transcription initiation factor TFIID subunit 5 (TAF5). Left: Visualize the embedding of amino acids conditioned on [MLM] and [PPI] prompts (TAF4) by MDS. Middle: Visualize distances of corresponding amino acids in (a). Right: Visualize the amino acids with the most variation embeddings (red).\n\nFurthermore, since the [PPI] prompt is trained to provide quaternary structure information, we analyze what exactly the amino acid representations have changed. As shown in Figure 6(a), we firstly visualize the embeddings of amino acids of the TAF4 protein conditioned on [MLM] or [PPI] based on MDS (Kruskal, 1964). Then we calculate the distances between two embeddings of the same amino acid and plot them in Figure 6(b). We mark 30 amino acids with the most variation embeddings in red (Figure 6(c)). The observation that marked amino acids are all on the protein surface is consistent with the fact that modeling the quaternary structure cares about the surface conformation, not the core (Yan et al., 2008).\n\n5 CONCLUSION AND FUTURE WORK\n\nIn this paper, we extend the concept of prompts from NLP to protein representations. We propose the prompt-guided multi-task pre-training and fine-tuning framework. With this framework, we propose three complementary pre-training structures to acquire multi-level structure information, and flexibly combine them for various downstream tasks. Experimental results on function prediction and protein engineering show that the proposed approach can produce satisfactory improvements when compared to the conventional PTPMs. The improvement is especially significant in lowresource settings. In the future, we are interested in examining the effectiveness of the proposed prompt-guided multi-task pre-training and fine-tuning framework in domains where hierarchical task information is required in the pre-training stage.\n\n9\n\n[MLM] Guided Embedding[CRD] Guided EmbeddingWithout Prompt-Guided Embedding(a)(b)(c)Hydrophobic alphaticCharged basicCharged acidicCharged acidicUniqueHydrophobic aromaticCoilAlpha-helixDisorderBeta-sheet(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:22)(cid:30)(cid:26)(cid:21)(cid:20)(cid:23)(cid:25)(cid:30)(cid:19)(cid:20)(cid:18)(cid:24)(cid:17)(cid:16)(cid:24)(cid:26)(cid:25)(cid:24)(cid:20)(cid:15)(cid:14)(cid:23)(cid:13)(cid:12)(cid:11)TAF4TAF5(a)(b)(c)MLM-task EmbeddingPPI-task EmbeddingPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work is funded by NSFC91846204/U19B2027 and sponsored by CAAI-Huawei MindSpore Open Fund. We want to express gratitude to the anonymous reviewers for their hard work and kind comments and Hangzhou AI Computing Center for their technical support. Xurui Jin is the employee of the MindRank AI Ltd.\n\nREFERENCES\n\nEthan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church. Unified rational protein engineering with sequence-based deep representation learning. Nature methods, 16(12):1315–1322, 2019.\n\nEhsaneddin Asgari and Mohammad RK Mofrad. Continuous distributed representation of biological\n\nsequences for deep proteomics and genomics. PloS one, 10(11):e0141287, 2015.\n\nJacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. In\n\nNoise reduction in speech processing, pp. 1–4. Springer, 2009.\n\nTristan Bepler and Bonnie Berger. Learning the protein language: Evolution, structure, and function.\n\nCell Systems, 12(6):654–669, 2021.\n\nHelen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig, Ilya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic acids research, 28(1): 235–242, 2000.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nDrew H Bryant, Ali Bashir, Sam Sinai, Nina K Jain, Pierce J Ogden, Patrick F Riley, George M Church, Lucy J Colwell, and Eric D Kelsic. Deep diversification of an aav capsid protein by machine learning. Nature Biotechnology, 39(6):691–696, 2021.\n\nZhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International conference on machine learning, pp. 794–803. PMLR, 2018.\n\nUniProt Consortium. Uniprot: the universal protein knowledgebase in 2021. Nucleic acids research,\n\n49(D1):D480–D489, 2021.\n\nAlperen Dalkiran, Ahmet Sureyya Rifaioglu, Maria Jesus Martin, Rengul Cetin-Atalay, Volkan Atalay, and Tunca Do ̆gan. Ecpred: a tool for the prediction of the enzymatic functions of protein sequences based on the ec nomenclature. BMC bioinformatics, 19(1):1–13, 2018.\n\nChristian Dallago, Jody Mou, Kadina E Johnston, Bruce Wittmann, Nick Bhattacharya, Samuel Goldman, Ali Madani, and Kevin K Yang. Flip: Benchmark tasks in fitness landscape inference for proteins. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\nJames Dunbar, Konrad Krawczyk, Jinwoo Leem, Terry Baker, Angelika Fuchs, Guy Georges, Jiye Shi, and Charlotte M Deane. Sabdab: the structural antibody database. Nucleic acids research, 42(D1):D1140–D1146, 2014.\n\nAhmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n\nCharles J Epstein, Robert F Goldberger, and Christian B Anfinsen. The genetic control of tertiary protein structure: studies with model systems. In Cold Spring Harbor symposia on quantitative biology, volume 28, pp. 439–449. Cold Spring Harbor Laboratory Press, 1963.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nOctavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi S Jaakkola, and Andreas Krause. Independent se (3)-equivariant models for end-to-end rigid protein docking. In International Conference on Learning Representations, 2021.\n\nVladimir Gligorijevi ́c, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structurebased protein function prediction using graph convolutional networks. Nature communications, 12(1):1–14, 2021.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nPedro Hermosilla, Marco Sch ̈afer, Matej Lang, Gloria Fackelmann, Pere-Pau V ́azquez, Barbora Kozlikova, Michael Krone, Tobias Ritschel, and Timo Ropinski. Intrinsic-extrinsic convolution and pooling for learning on 3d protein structures. In International Conference on Learning Representations, 2020.\n\nAnna Jarzab, Nils Kurzawa, Thomas Hopf, Matthias Moerch, Jana Zecha, Niels Leijten, Yangyang Bian, Eva Musiol, Melanie Maschberger, Gabriele Stoehr, et al. Meltome atlas—thermal proteome stability across the tree of life. Nature methods, 17(5):495–503, 2020.\n\nAdri ́an Javaloy and Isabel Valera. Rotograd: Gradient homogenization in multitask learning. In\n\nInternational Conference on Learning Representations, 2021.\n\nBowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. In International Conference on Learning Representations, 2020.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\nWolfgang Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 32 (5):922–923, 1976.\n\nWolfgang Kabsch and Christian Sander. Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features. Biopolymers: Original Research on Biomolecules, 22(12):2577–2637, 1983.\n\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171– 4186, 2019.\n\nAmit Kessel and Nir Ben-Tal. Introduction to proteins: structure, function, and motion. Chapman\n\nand Hall/CRC, 2018.\n\nJoseph B Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothe-\n\nsis. Psychometrika, 29(1):1–27, 1964.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597, 2021.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nJoshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. Advances in Neural Information Processing Systems, 34:29287–29303, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-\n\ntations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019.\n\nHakime ̈Ozt ̈urk, Arzucan ̈Ozg ̈ur, and Elif Ozkirimli. Deepdta: deep drug–target binding affinity\n\nprediction. Bioinformatics, 34(17):i821–i829, 2018.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nPredrag Radivojac, Wyatt T Clark, Tal Ronnen Oron, Alexandra M Schnoes, Tobias Wittkop, Artem Sokolov, Kiley Graim, Christopher Funk, Karin Verspoor, Asa Ben-Hur, et al. A large-scale evaluation of computational protein function prediction. Nature methods, 10(3):221–227, 2013.\n\nRoshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. Evaluating protein transfer learning with tape. Advances in neural information processing systems, 32, 2019.\n\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.\n\nGabriel J Rocklin, Tamuka M Chidyausiku, Inna Goreshnik, Alex Ford, Scott Houliston, Alexander Lemak, Lauren Carter, Rashmi Ravichandran, Vikram K Mulligan, Aaron Chevalier, et al. Global analysis of protein folding using massively parallel design, synthesis, and testing. Science, 357 (6347):168–175, 2017.\n\nKaren S Sarkisyan, Dmitry A Bolotin, Margarita V Meer, Dinara R Usmanova, Alexander S Mishin, George V Sharonov, Dmitry N Ivankov, Nina G Bozhanova, Mikhail S Baranov, Onuralp Soylemez, et al. Local fitness landscape of the green fluorescent protein. Nature, 533(7603):397–401, 2016.\n\nTimo Schick and Hinrich Sch ̈utze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255–269, 2021.\n\nAmir Shanehsazzadeh, David Belanger, and David Dohan. Is transfer learning necessary for protein\n\nlandscape prediction? arXiv preprint arXiv:2011.03443, 2020.\n\nVignesh Ram Somnath, Charlotte Bunne, and Andreas Krause. Multi-scale representation learning\n\non proteins. Advances in Neural Information Processing Systems, 34:25244–25255, 2021.\n\nMartin Steinegger and Johannes S ̈oding. Mmseqs2 enables sensitive protein sequence searching for\n\nthe analysis of massive data sets. Nature biotechnology, 35(11):1026–1028, 2017.\n\nBaris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt Consortium. Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6):926–932, 2015.\n\nDamian Szklarczyk, Annika L Gable, David Lyon, Alexander Junge, Stefan Wyder, Jaime HuertaCepas, Milan Simonovic, Nadezhda T Doncheva, John H Morris, Peer Bork, et al. String v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets. Nucleic acids research, 47(D1):D607–D613, 2019.\n\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine\n\nLearning Research, 9(86):2579–2605, 2008.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.\n\nZichen Wang, Steven A Combs, Ryan Brand, Miguel Romero Calvo, Panpan Xu, George Price, Nataliya Golovach, Emmanuel O Salawu, Colby J Wise, Sri Priya Ponnapalli, et al. Lm-gvp: an extensible sequence and structure informed deep learning framework for protein property prediction. Scientific reports, 12(1):1–12, 2022.\n\nNicholas C Wu, Lei Dai, C Anders Olson, James O Lloyd-Smith, and Ren Sun. Adaptation in\n\nprotein fitness landscapes is facilitated by indirect paths. Elife, 5:e16965, 2016.\n\nSen Wu, Hongyang R Zhang, and Christopher R ́e. Understanding and improving information trans-\n\nfer in multi-task learning. In International Conference on Learning Representations, 2019.\n\nChanghui Yan, Feihong Wu, Robert L Jernigan, Drena Dobbs, and Vasant Honavar. Characterization\n\nof protein–protein interfaces. The protein journal, 27(1):59–70, 2008.\n\nKevin K Yang, Zachary Wu, Claire N Bedbrook, and Frances H Arnold. Learned protein embed-\n\ndings for machine learning. Bioinformatics, 34(15):2642–2648, 2018.\n\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:5824–5836, 2020.\n\nNingyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Qiang Zhang, Jiazhang Lian, and Huajun Chen. Ontoprotein: Protein pretraining with gene ontology embedding. In International Conference on Learning Representations, 2021a.\n\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. Differentiable prompt makes pre-trained language models better few-shot learners. In International Conference on Learning Representations, 2021b.\n\nZuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. arXiv preprint arXiv:2203.06125, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA MORE DETAILS OF PROMPTPROTEIN\n\nA.1 PROMPT-GUIDED MULTI-TASK PRE-TRAINING\n\nOne of the key problem to multi-task learning is what to share. Naive and gradient-based methods try to learn a shared MTL model. To overcome between task interference between tasks, they adjust magnitude and direction of gradients . However, negative transfer between pre-training and downstream tasks cannot be mitigated. To realize the potential of positive transfer, multi-task pretraining requires to learn and use task differences on-demand. Both adapter-based approaches and our proposed prompt-based approaches can learn task differences, whereas, for the flexibility of input, only prompt-based approach can use them on-demand. Figure 7 compares the mentioned multi-task methods.\n\nFigure 7: Comparison of multi-task pre-Training.\n\nA.2 PRE-TRAINING TASKS\n\nIn Figure 8, we illustrate our proposed two additional pre-training tasks.\n\nAlpha-Carbon Coordinate Prediction. We use a MLP network to project protein embeddings into 3D space. To equip the model with 3D invariance, after predicting the protein coordinates, we first recenter the ground-truth coordinate Z and predicted coordinate ˆZ and then employ Kabsch algorithm (Kabsch, 1976) to calculate the optimal rotation matrix that minimizes the root mean squared deviation. We first calculate cross-covariance matrix between two sets of coordinates:H = Z T ˆZ. Then the covariance matrix can be decomposed by singular value decomposition: H = U ΣV T . The optimal rotation matrix R can be formulated as: R = U V T .\n\nProtein-Protein Interaction Prediction. Since the limitation of GPU memory, it is not feasible to input two proteins in the same sequence. Instead, we leverage the representations of proteins to calculate protein-pair attention in decoder. Then the pair-aware protein representations can be obtained by multiplication of protein representations and the attention.\n\nA.3 ALGORITHMS FOR PROMPT-GUIDED MULTI-TASK PRE-TRAINING AND FINE-TUNING\n\nTo more easily appreciate the whole procedure of the prompt-guided multi-task pre-training and fine-tuning framework, we provide pseudo codes as follows.\n\nB ADDITIONAL DETAILS OF EXPERIMENTAL SETTING\n\nB.1 PRE-TRAINING DATASET\n\nTo exploit primary structure information, language modeling has been prove effective (Elnaggar et al., 2021; Alley et al., 2019). We follow Rives et al. (2021) to use UniRef50 (Consortium, 2021) dated March 28, 2018. 10% of UniRef50 clusters are randomly selected as a held-out evaluation set.\n\n14\n\n(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:30)(cid:22)(cid:21)(cid:20)(cid:28)(cid:19)(cid:24)(cid:22)(cid:25)(cid:18)(cid:17)(cid:22)(cid:25)(cid:28)(cid:16)(cid:23)(cid:15)(cid:14)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)⊕(a) Naive(b) Gradient-based(c) Adapter-based(d) Prompt-based (ours)pixh1L(y1,ˆy1|h1)L(y1,ˆy1|h)hhL(y1,ˆy1|h)L(y1,ˆy1|h1)h1xxx(cid:30)(cid:22)(cid:21)(cid:20)(cid:13)(cid:25)(cid:22)(cid:14)(cid:12)(cid:24)(cid:17)Published as a conference paper at ICLR 2023\n\nFigure 8: The Overview of Two Additional Pre-training Tasks. Left: Alpha-Carbon Coordinate Prediction. Right: Protein-Protein Interaction Prediction.\n\nAlgorithm 1: Prompt-Guided Multi-Task Pre-Training\n\nData: Input protein x, prompt pool p\n\nthe learning rate ζ.\n\nResult: Model parameters ψ while not converge do\n\nP =\n\n{\n\n∈\n\n[MLM], [CRD], [PPI]\n\n, task objectives\n\n}\n\nLp,\n\nfor p\n\nP do\n\n∈\n\nInitialize the task-specific input xp = x Compute the feature hp = fψ(xp; ψ)\n\np\n\n||\n\n// fψ contain L layers Prompt-aware Attention Module Lp(hp) according to Equation 3, 4 or 5\n\nCompute the loss\n\n(cid:80)\n\n− −\n\np(αp ·\n\nζ\n\n∇ψLp) according to Equation 6\n\nαp ·\n\nζ\n\n∇pLp,\n\np\n\n∀\n\n∈\n\nP\n\nend for Update the model parameters ψ = ψ Update the prompt parameters p = p\n\nend while\n\nAlgorithm 2: Prompt-Guided Fine-tuning\n\nData: Input protein x, downstream task object\n\nlearned prompt pool P = the learning rate ζ.\n\n{\n\nResult: Prompt-tuning module parameters θ.\n\nwhile not converge do\n\n′ L\n[MLM], [CRD], [PPI] }\n\np, , pre-trained model parameters ψ,\n\nCompute combined prompt p′ = τθ(p) according to Equation 7 Initialize the input xp′ = x Compute the feature hp′ = f (xp′; ψ) Compute the loss Update the prompt-tuning module parameters θ = θ\n\nLp′(hp′)\n\nLp′ =\n\np′\n\n||\n\nζ\n\n−\n\n∇θLp′\n\nend while\n\n15\n\n(cid:31)(cid:30)(cid:29)(cid:28)(cid:30)(cid:27)(cid:26)(cid:29)(cid:25)(cid:24)(cid:23)(cid:22)(cid:21)(cid:20)(cid:19)(cid:18)(cid:17)(cid:28)(cid:23)(cid:28)(cid:26)(cid:17)(cid:29)(cid:16)(cid:23)(cid:26)(cid:27)(cid:19)(cid:15)(cid:28)(cid:28)(cid:30)(cid:29)(cid:28)(cid:26)(cid:17)(cid:29)(cid:16)(cid:23)(cid:26)(cid:27)(cid:14)(cid:23)(cid:13)(cid:23)(cid:27)(cid:30)(cid:16)(cid:27)(cid:17)(cid:28)(cid:30)(cid:26)(cid:29)(cid:21)(cid:18)(cid:30)(cid:12)(cid:27)(cid:30)(cid:21)(cid:30)(cid:29)(cid:28)(cid:23)(cid:28)(cid:26)(cid:17)(cid:29)(cid:11)(cid:26)(cid:29)(cid:30)(cid:23)(cid:27)(cid:16)(cid:27)(cid:17)(cid:10)(cid:30)(cid:9)(cid:28)(cid:26)(cid:17)(cid:29)(cid:16)(cid:27)(cid:30)(cid:8)(cid:26)(cid:9)(cid:28)(cid:30)(cid:8)(cid:19)(cid:7)(cid:28)(cid:27)(cid:6)(cid:9)(cid:28)(cid:6)(cid:27)(cid:30)(cid:5)(cid:27)(cid:17)(cid:6)(cid:29)(cid:8)(cid:14)(cid:4)(cid:27)(cid:6)(cid:28)(cid:20)(cid:19)(cid:7)(cid:28)(cid:27)(cid:6)(cid:9)(cid:28)(cid:6)(cid:27)(cid:30)(cid:15)(cid:18)(cid:3)(cid:2)(cid:19)(cid:16)(cid:27)(cid:17)(cid:28)(cid:30)(cid:26)(cid:29)(cid:28)(cid:27)(cid:12)(cid:15)(cid:19)(cid:16)(cid:27)(cid:17)(cid:28)(cid:30)(cid:26)(cid:29)Published as a conference paper at ICLR 2023\n\nFor secondary and tertiary structure information, we extract data from Protein Data Bank (PDB) (Berman et al., 2000). For compatibility with pre-trained protein models, we only use proteins whose amino acid sequence length is less than 1,024. There are many ways to define the coordinates of protein residues. Here we use the coordinates of carbon alpha atoms.\n\nThe pre-training dataset for quaternary structures is constructed based on the latest STRING (Szklarczyk et al., 2019) database with only the physical-only mode, which means edges between the protein pairs indicate evidence of their binding or forming a physical complex. The database contains in total 65 million protein sequences from 14,094 species and 2.7 billion protein-protein interaction pairs. Note that there is no edge between proteins that come from different species.\n\nWe observe that the PPI network has a problem of uneven distribution, as illustrated in Figure 9, 107 edges. Such data distributions can lead the largest network contains 60,000 proteins and 3.5 models to over-focus on proteins from a single species. We pre-process our dataset by choosing the species networks with comparable sizes. Figure 10 illustrates the data distribution after preprocessing.\n\n×\n\nFigure 9: Visualization of the number of nodes and the number of edges in the original database.\n\nFigure 10: Visualization of the number of nodes and the number of edges in the pre-processed database.\n\nB.2 DOWNSTREAM TASK DATASETS.\n\nThe statistical results of the downstream datasets are shown in Table 4.\n\nEvaluation Metrics For multiple binary classification tasks, we employ protein-centric maximum F-score Fmax and pair-centric area under precision-recall curve AUPRpair to evaluate protein models. For regression tasks, we employ spearman’s correlation ρ to evaluate protein models.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Statistics of the downstream datasets.\n\nDATASET\n\n#TRAIN\n\n#VALIDATION\n\n#TEST\n\nTASK\n\nENZYME COMMISSION GENE ONTOLOGY STABILITY FLUORESCENCE THERMOSTABILITY AAV (1-VS-REST) GB1 (1-VS-REST) GB1 (2-VS-REST) SABDAB\n\n15,551 29,902 53,679 21,446 24,817 1,170 29 427 345\n\n1,729 3,323 2,447 5,362 -\n- -\n- 48\n\n1,919 3,416 12,839 27,217 3,314 81,413 8,704 8,306 99\n\nCLASSIFICATION CLASSIFICATION REGRESSION REGRESSION REGRESSION REGRESSION REGRESSION REGRESSION REGRESSION\n\n• Fmax. Given a target protein i, we denote its experimentally determined function terms [0, 1], we denote predicted function terms as\n\nas Ti. Given a set of decision threshold t Pi(t). The precision and recall of this protein can be formulated as:\n\n∈\n\nprecisioni(t) =\n\nI[f\n\n(cid:80) f\n(cid:80) f\n\n∈ I[f\n\nPi(t)\n\nTi]\n\n∩ Pi(t)]\n\nrecalli(t) =\n\n(cid:80) f\n\nI[f (cid:80)\n\nf\n\n∈ I[f\n\n∈ Pi(t)\n\n∩ Ti]\n\n∈\n\nTi]\n\n,\n\n,\n\n(8)\n\n(9)\n\nwhere I[ ] is an indicator function that is equal to 1 if and only if the condition is true. ·\nCombining these two measures, the Fmax is defined as the maximum value of F-measure:\n\nFmax = max\n\nt {\n\nprecision(t)\n\n2 ·\nprecision(t) + recall(t) }\n\nrecall(t)\n\n·\n\n,\n\n(10)\n\nwhere precision(t) = 1 i recalli(t). The N is denoted as the number of proteins and M (t) is denoted as the number of proteins on which at least one prediction result is above threshold t.\n\ni precisioni(t), and precision(t) = 1\n\nM (t)\n\nN\n\n(cid:80)\n\n(cid:80)\n\n• AUPRpair. The pair-centric area under precision-recall curve is exactly the micro average\n\nprecision score where precision and recall are for each term f :\n\nprecisionf (t) =\n\n(cid:80)\n\nI[f\n\ni (cid:80) i\n\n∈ I[f\n\nPi(t)\n\nTi]\n\n∩ Pi(t)]\n\nrecallf (t) =\n\n(cid:80) i\n\nI[f (cid:80) i\n\n∈ I[f\n\n∈ Pi(t)\n\n∩ Ti]\n\n∈\n\nTi]\n\n.\n\n,\n\n(11)\n\n(12)\n\n• ρ. Spearman’s is a nonparametric measure of rank correlation for ground-truth Y and\n\npredicted ˆY landscape. We denote R( ̇) as ranks. The correlation coefficient is:\n\nρ =\n\ncov(R(Y), R ˆY)) σR(Y )σR( ˆY )\n\n,\n\n(13)\n\nwhere cov( ) is the covariance of the variables, and σR(·) is the standard deviations of the ·\nrank variables.\n\n, ·\n\nEnzyme Commission and Gene Ontology. EC numbers are selected from the third and fourth levels of the EC tree, forming 538 binary classification tasks. GO terms are hierachically organized into three ontologies – biological process (1943 binary classification tasks), molecular function (489 binary classification tasks), and cellular component (320 binary classification tasks). Following DeepFRI (Gligorijevi ́c et al., 2021), we use the protein sequences in the test set with 95% sequence identity to the training set.\n\nStability Landscape Prediction (Rocklin et al., 2017). This is a regression task that maps each protein to a label, measuring the most extreme case where the protein maintains its fold above a concentration threshold. This task aims to test the ability to generalize from a broad sampling of\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nrelevant sequences to local neighborhood of a few sequences. The train set includes proteins from experimental design, while the test set contains single mutants.\n\nFluorescence Landscape Prediction (Sarkisyan et al., 2016). This is a regression task that maps a protein to a label corresponding to the log-fluorescence intensity. This task aims to test the ability to distinguish mutants. The train set includes triple mutants of the wild-type green fluorescent protein (GFP), while the test set contains more mutants.\n\nThermostability Landscape Prediction (Jarzab et al., 2020). This is a regression task that maps a protein to a thermostability label. We adopt the mixed split proposed by Dallago et al. (2021) that using MM-seqs2 (Steinegger & S ̈oding, 2017) at a threshold of 20% sequence identity creates one split. The train set includes all sequences in 80% of clusters, while the test contains the remaining 20% of clusters.\n\nAdeno-associated virus (AAV) Landscape Prediction (Bryant et al., 2021). This is a regression task that predicts fitness for a long mutated sequence. We adopt the 1-vs-rest split, where wild type and single mutants are assigned to train set, while test set contains the rest. This split are common in protein engineering application.\n\nGB1 Landscape Prediction (Wu et al., 2016). This is a regression task that predicts the effects of interactions between mutations. We adopt the 1-vs-rest (and 2-vs-rest) split, where wild type and single mutants (and double mutants) are assigned to train set, while test set contains the rest.\n\nAntibody-antigen Affinity Prediction (Dunbar et al., 2014). This is a regression task that takes a pair of proteins as input and predicts the affinity between them. We adopt random split which contains 493 pairs, 431 antibodies and 401 antigens.\n\nTable 5: Model performance on FLIP benchmark.\n\nDATASET\n\nTHERMO\n\nAAV\n\nMIXED\n\nHUMAN\n\nHUMAN-CELL\n\n1-VS-R\n\n2-VS-R\n\n1-VS-R\n\n2-VS-R\n\nESM-1B OURS\n\n0.68 0.683\n\n0.70(0.691) 0.702\n\n0.75(0.673) 0.684\n\n0.04 0.551\n\n0.26 0.595\n\n0.32 0.403\n\n0.36 0.550\n\nGB1 3-VS-R\n\n0.54 0.783\n\nLOW-VS-HIGH\n\n0.13 0.294\n\nTo illustrate the advantage of prompt-tuning in low-resource scenarios, we only selected a subset of tasks in the FLIP benchmark. In Table 5, we report the performance of our model on other tasks. Note that, although we use the reported results of esm-1b in the above table, we additionally provide the reproduced results of esm-1b on Thermo(Human) and Thermo(Human-cell). These values (surrounded by brackets) are lower than reported.\n\nFigure 11: Attention visualization. We select GB1 protein as an example and visualize attentions of the 15-th layer (high skip weight for [MLM]) and the 33-th layer (high skip weight for [CRD] and [PPI]).\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nB.3 ANALYSIS OF NEURAL LAYERS\n\nIn Figure 11(a), we visualize the attentions in the 15-th layer (a high skip weight for [MLM]) and the 33-th layer (a high skip weight for [CRD] and [PPI]). We observe that one amino acid in the 15-th layer can only attend to the local neighbors in the sequence, whereas the amino acid in the 33-th layer can attend to those amino acids that are more distant along the sequence but potentially closer in the 3D space. This observation demonstrates the primary structural knowledge learned by MLM pays more attention to sequential dependency, whereas the tertiary structural and quaternary structural knowledge learned by CRD and PPI tasks can capture the information from adjacent amino acids in the 3D space.\n\nB.4 ADDITIONAL EXPERIMENT RESULTS\n\nDo downstream tasks benefit from the acquired information on-demand by prompt tuning?\n\nTo further analyze the importance of prompt-guided fine-tuning, we conduct an ablation study on the binding affinity prediction task on the SAbDab dataset (Dunbar et al., 2014). From Figure 12, we observe that PromptProtein performs worst without any prompt tokens. In contrast, adding either of the three prompt tokens, especially the token corresponding to the PPI task, can significantly improve performance. By combining different prompts without prompt tuning, we can obtain protein representations enhanced by multiple structural information. By doing that, we find the combination of the [MLM] and [PPI] prompts empowers PromptProtein to achieve the best performance. It is also notable that, by comparing the results of adding [MLM] and [PPI] prompts and adding all prompts, the [CRD] prompt leads to a performance decrease. These results evidence that not all structure information from pre-training is beneficial for downstream tasks, and adaptively combining acquired information via prompt-tuning leads to better performance.\n\nFigure 12: Ablation of PromptProtein with different prompt tokens on SAbDab (spearman’s ρ ).\n\n19\n\n(cid:31)(cid:30)(cid:29)(cid:28)(cid:30)(cid:29)(cid:27)(cid:26)(cid:25)(cid:24)(cid:29)(cid:30)(cid:23)(cid:22)(cid:26)(cid:21)(cid:20)(cid:19)(cid:28)(cid:30)(cid:18)(cid:23)(cid:30)(cid:17)(cid:29)(cid:26)(cid:16)(cid:15)(cid:14)(cid:13)(cid:26)(cid:12)(cid:11)(cid:25)(cid:10)(cid:9)(cid:15)(cid:10)(cid:8)(cid:7)(cid:17)(cid:29)(cid:19)(cid:6)(cid:5)(cid:9)(cid:4)(cid:3)(cid:4)(cid:21)(cid:21)(cid:2)(cid:21)(cid:21)(cid:2)(cid:1)(cid:6)(cid:5)(cid:9)(cid:4)(cid:3)(cid:4)(cid:1)(cid:6)(cid:5)(cid:9)(cid:4)(cid:3)(cid:4)(cid:1)(cid:21)(cid:21)(cid:2)(cid:25)(cid:3)(cid:3)",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a prompt-based multi-task framework for pre-training and fine-tuning protein sequence representations. From a methodological standpoint, the paper adapts the prompt fine-tuning idea (ie., a differentiable continuous prompt token is pre-trained instead of using a discrete prompt) from the NLP literature to protein modeling with large transformer networks. It then makes two contributions over the standard transformer architecture 1) A specific masking scheme for the prompt token is used to keep only the effect of the prompt on the input sequence and eliminate the opposite effect (ie., the prompt should be task-dependent and not sample-dependent) 2) Learned task-specific layer-specific skip connection linear projections to let the network learn different weights for each task at each layer. The multi-task pre-training involves three kinds of pre-training: a) masked-language modeling with sequence-based information only (MLM) b) Prediction of the alpha-carbons positions (CRD) c) prediction of protein-protein interactions (PPI). Experiments on function annotation and protein engineering (FLIP benchmark) demonstrate the benefits of the different ideas introduced in this work.\n\n# Strength And Weaknesses\n\n**Strengths**\n- The idea of combining different pre-training tasks seems very sensible for proteins given the diversity of modalities that characterize them (eg., via their primary, secondary, tertiary and quaternary structure). As noted by the authors, there is however a high risk of task interference when one wants to obtain pre-trained representations that combine these different modalities / tasks. The approach suggested by authors appears to be doing a fine job at efficiently combining these different modalities given the performance reported in sections 4.2. and 4.3.\n- The introduced prompt masking and skip connections both appear to be critical to strong empirical performance — the latter seems to be particularly important to mitigate task interference as evidenced by the ablations in section 4.4.\n- The paper is very clear overall (in particular the methodology section) with nice visuals facilitating understanding and additional analyses in section 4.5 to help investigate the source of the performance lift.\n\n**Weaknesses**\n- The ablations for the different pre-training tasks in section 4.5 / Figure 6 are a bit puzzling. It does seem that the CRD task has destructive value on that particular binding affinity prediction task since: a) the performance of CRD + MLM or CRD + PPI leads to both lower performance Vs MLM or PPI alone respectively b) the performance of CRD + MLM + PPI is also lower vs just using MLM + PPI. This seems particularly important from a practical standpoint, and additional experiments are needed to confirm whether: 1) that problem applies to other downstream tasks or is just specific to binding affinity prediction — and if so, why?  2) there is something fundamentally wrong with the CRD pre-training as currently implemented? 3) there is a way to anticipate ex ante (or post fine tuning) which tokens should be used to ensure optimal task performance ?\n- The ablation in section in Table 3 is a bit puzzling as well: it appears that the performance of PromptProtein without layer skip is lower than the performance from the conventional MTL. Could you please explain why that might be the case? (I would have assumed intermediate performance between conventional MTL and full PromptProtein as I presume the attention masks are still used in that ablation?)\n- Several points (in section 4 primarily) were not fully clear (see clarity paragraph below).\n- The following claim in conclusion does not seem fully substantiated: “PromptProtein beats state-of-the-art baselines by significant margins”. Authors do report the relevant baselines listed in the FLIP paper [1]. But since that paper was released, several methods have shown markedly superior performance for protein modeling & achieving high spearman with deep mutational scanning assays — see for example, [2] and [3]. I would suggest adding these two baselines to the analysis or tone done the SOTA claims.\n\n------------------------------------------------------------------------------------------------------------------------\n[1] Dallago, C., Mou, J., Johnston, K.E., Wittmann, B.J., Bhattacharya, N., Goldman, S., Madani, A., & Yang, K.K. (2022). FLIP: Benchmark tasks in fitness landscape inference for proteins. bioRxiv.\n\n[2] Hsu, C., Verkuil, R., Liu, J., Lin, Z., Hie, B.L., Sercu, T., Lerer, A., & Rives, A. (2022). Learning inverse folding from millions of predicted structures. bioRxiv.\n\n[3] Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**\n- Could you please clarify how you handle the instances where not all tasks are available for the same proteins in a given mini-batch (eg., instances where primary structure is available in Uniref50, but structure is not available in the PDB)?\n- I would clarify in section 3.1 that $g_p$ (the linear projection of the prompt p) is a scalar. In my first lecture, I had (wrongly) assumed this was a vector of the same size as $h_p$ but that was not really making sense anymore when reading the analysis in section 4.\n- “We observe that one amino acid in the 15-th layer can only attend to the local neighbors in the sequence, whereas the amino acid in the 33-th layer can attend to those amino acids that are more distant along the sequence but potentially closer in the 3D space” (in section B.3) —> I found this particularly insightful and would recommend to move this section from supplementary to the main text if space allows.\n- What is the nature of the operator $\\tau_\\theta$ used for fine tuning? Linear projection? Any non-linearity applied?\n- What is ESM-unstrained (Table 2)?\n- Fig5 analysis — it is not clear what is being done here. Is this analysis conducted for a particular protein sequence (if yes, which one)? Or some aggregation over Uniref50 sequences? The embeddings from which layer(s) are being used here?\n- I find the “skip connection” terminology a bit confusing as it seems that the term $g_p$ is used as a multiplicative factor for the attention-based transform but not the skip connection term. Also on Figure 3, the arrow labeled “skip connection” is in fact not the skip connection as it supports the computation of the  $g_p$ term which should not be present in the skip connection as per equation 2.\n- In the conclusion: “In the future, we are interested in examining the effectiveness of the proposed prompt-guided multi-task pre-training and fine-tuning framework in domains where hierarchical task information is required in the pre-training stage.” —> Could you please provide examples for such domains?\n\n**Quality**\n- Very sound approach overall. Authors provided some very compelling empirical results, yet there are a few concerns with some of the ablation results as detailed above. Also the SOTA claim in the is not fully substantiated as discussed above as well.\n\n**Novelty**\n- The prompt masking and skip-connection weights are novel to my knowledge and appear to be both critical to the strong reported performance. Could the authors please confirm these two ideas are indeed introduced for the first time in this paper and not borrowed from the NLP literature?\n\n**Reproducibility**\n- Authors confirm that the code will be open sourced upon acceptance (section 4.1)\n\n# Summary Of The Review\n\nThis paper is aiming to address a very important area in protein modeling: learning rich sequence embeddings by leveraging multiple pre-training tasks jointly. The approach is sound and the methodological section is overall very clearly presented. There are a few concerns with respect to some of the results and claims as detailed above. Given the several strengths of the work, I would be willing to increase my score if authors address these points during rebuttal.\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------\n[UPDATE POST REBUTTAL]\n\nMy most important concerns have been alleviated during rebuttal and I have increased my score accordingly.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper presents PromptProtein, a novel framework for multi-level protein structure modeling that leverages prompt-guided multi-task pre-training and fine-tuning. It addresses the inadequacies of existing methods that focus primarily on primary and tertiary protein structures, neglecting the crucial quaternary level necessary for effective function prediction. The methodology incorporates three innovative pre-training tasks: Masked Language Modeling (MLM) for primary structure, Alpha-Carbon Coordinate Prediction (CRD) for secondary and tertiary structures, and Protein-Protein Interaction Prediction (PPI) for quaternary structures. The results indicate that PromptProtein significantly outperforms state-of-the-art models, particularly in low-resource settings, achieving an average performance improvement of 17.0% in function prediction and demonstrating superior capabilities in protein engineering tasks.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its comprehensive approach to capturing multi-level protein structures and its innovative use of prompt tokens to minimize task interference during training. The introduction of specific pre-training tasks tailored to different structural levels is a noteworthy contribution that enhances the model's overall efficacy. However, a potential weakness is the reliance on complex task integration, which may pose challenges in scalability and generalization to other biological contexts. Additionally, while the empirical results are compelling, further exploration of the model's limitations in diverse biological scenarios would provide a more balanced view.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, making it accessible to readers in the field of computational biology. The quality of the methodology is high, with a thorough experimental setup and a robust evaluation of results. The novelty of the approach is significant, particularly in its application of prompt learning to protein structure prediction, an area that has seen limited exploration. However, reproducibility could be enhanced by providing more detailed information on the implementation and hyperparameter settings of the model.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in multi-level protein structure modeling through the innovative use of prompt-guided learning. The methodology is sound and the results are impressive, particularly in low-resource scenarios. While the contributions are robust, further clarification on reproducibility aspects would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents **PromptProtein**, a novel framework for protein function prediction that leverages prompt-guided multi-task learning to integrate various structural levels of proteins: primary, secondary, tertiary, and quaternary. The methodology includes designing specific pre-training tasks—masked language modeling, alpha-carbon coordinate prediction, and protein-protein interaction prediction—to capture the rich structural information of proteins. Empirical results demonstrate that PromptProtein outperforms existing state-of-the-art methods across multiple datasets, particularly excelling in low-resource settings, with a notable average improvement of 17.0% in protein engineering tasks.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach, effectively integrating multi-level protein structural information to enhance function prediction. The comprehensive experimental design showcases the robustness of PromptProtein, yielding strong results across varied tasks. However, limitations include potential task interference that may arise despite the proposed solutions, along with the complexities introduced by prompt tuning. Additionally, while the results are promising, the generalizability of the approach to broader applications has yet to be thoroughly established.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, making it accessible to readers. The innovative use of prompt-guided learning in protein representations is a significant contribution to the field. The experiments are rigorously designed, with appropriate metrics for evaluation, enhancing reproducibility. However, the complexity of prompt tuning may pose challenges for practical implementation, which could affect reproducibility in diverse settings.\n\n# Summary Of The Review\nOverall, this paper provides a significant advancement in protein function prediction through the introduction of a prompt-guided multi-task framework. Its innovative approach and strong empirical results mark it as a valuable contribution to the field, though further exploration is needed to address potential challenges in task interference and generalizability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel prompt-guided multi-task pre-training framework called PromptProtein, designed for protein function prediction by leveraging multi-level protein structures (primary, secondary, tertiary, quaternary). The methodology integrates three pre-training tasks: Masked Language Modeling (MLM), Alpha-Carbon Coordinate Prediction (CRD), and Protein-Protein Interaction Prediction (PPI), utilizing a modified Transformer architecture to minimize task interference. The findings demonstrate that PromptProtein significantly outperforms existing state-of-the-art methods in function prediction and protein engineering tasks, particularly in low-resource settings.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to utilize multi-level protein structural information through a combined pre-training strategy, addressing a notable gap in existing methodologies that typically focus on singular structural levels. The implementation of a prompt-aware attention module effectively reduces task interference, enhancing the model's ability to learn from diverse tasks. However, a potential weakness is the reliance on large datasets like UniRef50 and PDB, which may not be readily accessible for all research settings, potentially limiting reproducibility. Additionally, while the paper provides strong empirical results, further exploration of the model's limitations and the robustness of the findings across various contexts would strengthen the contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with well-structured sections and clear explanations of methodologies and experimental setups. The quality of the diagrams and equations is commendable, facilitating understanding of complex concepts. The novelty of the prompt-guided multi-task pre-training approach is significant, as it introduces a fresh perspective on protein structure learning. However, the paper could enhance reproducibility by providing more detailed descriptions of the implementation process, including hyperparameter settings and training procedures.\n\n# Summary Of The Review\nOverall, the paper offers a significant advancement in protein function prediction through its innovative multi-level structure learning framework. While the methodology is sound and results are promising, further work could enhance reproducibility and provide deeper insights into the model's limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Multi-Level Protein Structure Pre-Training with Prompt Learning\" presents a novel framework, PromptProtein, which leverages prompt-guided multi-task pre-training and fine-tuning strategies to enhance function prediction in protein structures. The methodology includes the introduction of three complementary pre-training tasks—Masked Language Modeling (MLM), Contrastive Representation Discrimination (CRD), and Protein-Protein Interaction (PPI)—to facilitate a nuanced understanding of protein structures. The findings indicate that PromptProtein significantly outperforms existing state-of-the-art methods, especially in scenarios with limited data, while addressing task interference through the innovative use of prompt tokens.\n\n# Strengths And Weaknesses\nThe paper's strengths include its comprehensive framework that effectively utilizes multi-level structural information and its demonstrable performance improvements over existing methods, particularly in low-resource settings. The innovative adaptation of NLP techniques to protein modeling is also commendable. However, the complexity of implementation may hinder reproducibility, and the model's generalizability to diverse datasets remains untested. Furthermore, while the introduction of new pre-training tasks is a strength, the justification for their selection could be more robust. The evaluation metrics used are varied and thorough, yet the limited exploration of prompt variability and potential biases in dataset reliance present notable limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and provides a clear description of the experimental setup, datasets, and training procedures, which enhances reproducibility. The innovative use of prompts in protein modeling reflects a high level of novelty. However, the complexity of the framework might pose challenges for researchers with less expertise in advanced machine learning techniques, potentially affecting the paper's overall clarity and accessibility.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in protein function prediction through a novel multi-task learning framework. While it demonstrates strong performance and innovative methodologies, concerns regarding implementation complexity and generalization should be addressed to enhance its applicability in diverse contexts.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"MULTI-LEVEL PROTEIN STRUCTURE PRE-TRAINING WITH ADAPTIVE PROMPTING\" introduces a novel model, AdaptivePromptProtein, aimed at improving protein function prediction through an innovative adaptive prompting mechanism. The methodology encompasses a dynamic prompting strategy that allows the model to focus on various structural levels of proteins—primary, secondary, tertiary, and quaternary—without relying on static tokens. Key contributions include the development of a dynamic attention mechanism and enhanced fine-tuning strategies, resulting in significant performance improvements over state-of-the-art methods, particularly in low-resource scenarios, with an average enhancement of approximately 17.5% across multiple datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to adaptive prompting, comprehensive evaluation across diverse tasks, and clear evidence of performance enhancements compared to traditional pre-trained protein models. The proposed model demonstrates a promising ability to leverage multi-level structural information effectively. However, weaknesses include the complexity of implementation, which may deter practical adoption, and potential scalability issues when applied to very large datasets, which could limit the model's applicability in broader contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, enhancing comprehension for readers. The quality of the experiments is commendable, with thorough comparisons against existing models, demonstrating the robustness of the proposed approach. The novelty of the adaptive prompting mechanism stands out in the context of protein function prediction. However, reproducibility could be a concern due to the complexity of the model architecture and the specific tuning required for optimal performance.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in protein function prediction through the introduction of an adaptive prompting mechanism that effectively utilizes multi-level structural information. The results are compelling, showcasing substantial improvements over existing methods, though the complexity of implementation and potential scalability challenges could hinder broader application.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents \"PromptProtein,\" a novel framework for adversarial training that integrates prompt-guided mechanisms into multi-task pre-training and fine-tuning strategies. The authors introduce three complementary adversarial training tasks: Mask Language Modeling (MLM), Coordinate Prediction (CRD), and Adversarial Interaction Prediction (PPI), which collectively enhance the model's robustness against adversarial attacks by directing its attention to specific adversarial features. Extensive experiments demonstrate that PromptProtein significantly outperforms existing state-of-the-art methods, particularly in low-resource settings, thereby indicating its potential for real-world applications.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach, which introduces a fresh perspective on adversarial training through the use of prompts. The comprehensive evaluation across diverse datasets and adversarial conditions highlights the robustness of the proposed method. Additionally, the framework's flexibility in dynamically adjusting focus on various adversarial aspects is a notable advantage. However, the complexity of implementation could pose challenges for practitioners, particularly regarding the computational resources required and the expertise needed for effective prompt engineering. Furthermore, questions remain about the generalizability of the approach to domains not covered in the experiments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of the writing is high, with sufficient detail provided to understand the proposed framework and its components. The novelty of integrating prompt learning into adversarial training is significant, offering new insights into model vulnerability mitigation. However, the reproducibility of the results may be hindered by the complexity of the framework and the potential need for advanced prompt engineering skills, which could limit broader adoption.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in adversarial training methodologies through its innovative PromptProtein framework. The thorough evaluation and promising results highlight its potential to enhance model resilience against adversarial attacks, although implementation complexity and generalization remain areas for further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Multi-Level Protein Structure Pre-Training with Prompt Learning\" introduces a novel model called PromptProtein, which integrates multi-level protein structures to enhance function prediction. The authors propose a prompt-guided framework that employs three pre-training tasks (Masked Language Modeling, Contextual Residual Dynamics, and Protein-Protein Interaction) aimed at achieving a comprehensive understanding of protein structures. Experimental results indicate that PromptProtein outperforms state-of-the-art models by an average of 17.0% in low-resource settings, suggesting a significant improvement in predictive capabilities.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating various levels of protein structure into a single model, which could address existing limitations in protein function prediction. The proposed framework is backed by extensive experimental validation and demonstrates notable performance improvements over baseline models. However, the paper tends to overstate the significance of its contributions, implying that previous models are obsolete, which may not be entirely justified. Additionally, while the methodology is sound, the exaggerated claims about the model's potential impact could mislead readers regarding its practical applicability and the continued relevance of existing techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making it accessible to readers familiar with the field. The methodology is clearly articulated, although some sections could benefit from more detailed explanations to enhance reproducibility. The novelty of the approach is significant, as it marks a shift towards multi-level integration in protein modeling. However, some claims regarding the absolute superiority of PromptProtein over existing methods require cautious interpretation, as they may oversimplify the complexities of protein function prediction.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to protein function prediction through the use of multi-level structural integration. While the proposed model shows promise and demonstrates empirical improvements, the authors' hyperbolic claims about its impact on existing methodologies detract from the paper's overall credibility. A more balanced discussion of limitations and the context of prior work would strengthen the contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel framework, PromptProtein, aimed at enhancing protein function prediction by leveraging multi-level structural information, including primary, secondary, tertiary, and quaternary structures. The methodology involves a multi-task learning approach with three pre-training tasks: Masked Language Modeling for primary structures, Alpha-Carbon Coordinate Prediction for secondary and tertiary structures, and Protein-Protein Interaction Prediction for quaternary structures. The experimental results indicate that PromptProtein outperforms existing state-of-the-art methods in various tasks, although the reported improvements are less pronounced than initially claimed.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to integrating multiple levels of protein structure into a unified framework, which is a notable advancement over previous methods that often focused on singular aspects. The introduction of a prompt-aware attention mechanism to mitigate task interference is also a significant contribution. However, a key weakness is the inconsistency between the claimed improvements and the actual results, especially in low-resource settings. The average enhancements reported in function prediction tasks show less impact than expected, suggesting that the practical benefits of the proposed model may be limited.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, allowing readers to follow the rationale behind the proposed methodology and the experimental setup. The quality of the writing is high, with detailed explanations of the techniques and results. However, the novelty, while present, may not be as groundbreaking as suggested, given that the improvements over existing methods are modest. The reproducibility of the results is likely feasible, as the authors provide sufficient details regarding their experimental setup, including the datasets used and the architecture of the model.\n\n# Summary Of The Review\nOverall, the paper introduces a promising framework for protein function prediction that effectively utilizes multi-level structural information. However, the actual performance improvements reported are not as substantial as initially indicated, warranting a cautious interpretation of its practical applications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper \"Multi-Level Protein Structure Pre-Training with Prompt Learning\" proposes a novel framework that integrates multi-level protein structures (primary, secondary, tertiary, and quaternary) to enhance protein function prediction through prompt learning. The methodology involves designing pre-training tasks (masked language modeling, contrastive representation learning, and protein-protein interactions) that collectively aim to capture essential structural insights. The findings indicate that the proposed approach improves performance on downstream tasks relative to existing multi-task learning methods, highlighting the potential for better generalization across diverse protein types.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative use of prompt learning to direct attention to specific structural levels, which may mitigate task interference often seen in multi-task learning. Additionally, the integration of various pre-training tasks is a thoughtful approach to cover a broad spectrum of protein knowledge. However, there are notable weaknesses, particularly concerning the assumptions made about the necessity of multi-level structures for all functions, the generalization of the method across different protein families, and the potential for negative transfer. Furthermore, the paper could benefit from a more robust discussion surrounding the evaluation metrics and the quality of the datasets used.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its contributions clearly. However, certain assumptions, particularly regarding the sufficiency and necessity of the proposed pre-training tasks, could have been elaborated upon for better clarity. The novelty of the approach is commendable, particularly in the context of protein structure representation, though the empirical significance of the findings could be better validated against alternative strategies. Reproducibility concerns are raised regarding the scalability of the proposed methods to larger datasets and more complex tasks, which are not thoroughly addressed.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to protein function prediction by leveraging multi-level structures and prompt learning. While the contributions are noteworthy, the assumptions made and the evaluation of the proposed methodology raise questions that could be further explored to enhance the robustness of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents PromptProtein, a novel framework designed for prompt-guided multi-task pre-training and fine-tuning in the context of protein structure prediction. The key contributions include a modified Transformer architecture tailored for multi-task learning, which incorporates various structural levels of protein information, such as primary, secondary, tertiary, and quaternary structures. The extensive experiments conducted demonstrate that PromptProtein significantly outperforms existing methods in function prediction and protein engineering, particularly in low-resource scenarios. An ablation study confirms the importance of individual components of the framework, highlighting the effectiveness of a skip connection mechanism in mitigating task interference.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive approach to protein structure representation by leveraging multiple levels of structural information, which is often overlooked in existing models. The proposed methodology effectively addresses task interference, a common challenge in multi-task learning, thereby enhancing the performance of protein function prediction. The extensive empirical validation and the inclusion of an ablation study provide strong support for the framework's effectiveness. However, the paper could benefit from a deeper discussion of the potential limitations of the approach, such as the scalability of the model to larger datasets or the generalizability of its findings to other biological tasks beyond protein structure prediction.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical flow that guides the reader through the motivation, methodology, and results. The quality of the figures and visualizations is high, effectively illustrating key concepts and findings. The novelty of the approach is significant, particularly in its integration of multi-level protein information and prompt-based learning. However, while the experiments are thorough, the reproducibility of the results may hinge on the availability of datasets and computational resources that are not fully addressed in the paper.\n\n# Summary Of The Review\nOverall, PromptProtein represents a significant advancement in protein structure prediction through its innovative use of prompt-guided multi-task learning and comprehensive structural representation. The methodology is sound, supported by rigorous empirical validation, although some aspects regarding scalability and reproducibility could be better articulated.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework aimed at improving the efficiency of neural network training through the integration of adaptive learning rate techniques. The authors propose a new algorithm that dynamically adjusts learning rates based on the complexity of the task and the performance of the model during training. Extensive experiments are conducted on several benchmark datasets, demonstrating that the proposed method consistently outperforms existing state-of-the-art optimization algorithms in terms of convergence speed and final accuracy.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to adaptive learning rates, which is well-motivated and supported by theoretical insights. The experimental results provide strong evidence of the proposed method's effectiveness, particularly in challenging tasks. However, a notable weakness is the lack of detailed analysis regarding the sensitivity of the algorithm to hyperparameter settings, which may affect its applicability in different contexts. Additionally, while the method shows promise, it could benefit from further empirical validation across a broader range of tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a logical flow that guides the reader through the methodology and results. However, certain sections could be enhanced with additional detail, particularly in the description of the algorithm's implementation and the experimental setup. The novelty of the proposed approach is significant, addressing a critical gap in the literature on adaptive learning rates. The reproducibility of the results is supported by the inclusion of sufficient details regarding the experiments, although providing access to the code would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the domain of adaptive learning rates for neural network training, backed by solid theoretical and empirical foundations. While the contributions are significant, addressing some clarity issues and expanding on hyperparameter sensitivity would strengthen the paper further.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents PromptProtein, a novel prompt-guided multi-task pre-training and fine-tuning framework for protein function prediction that integrates information from all four structural levels of proteins: primary, secondary, tertiary, and quaternary. By leveraging protein sequences, PromptProtein aims to enhance the predictive potential beyond existing methods, which typically focus either on primary or tertiary structures. The authors demonstrate through extensive experiments that PromptProtein significantly outperforms current state-of-the-art approaches in function prediction and protein engineering, especially in low-resource scenarios.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its innovative approach to utilize a multi-level perspective on protein structures, allowing for a more comprehensive understanding of protein functions. The methodology of integrating prompts for multi-task learning is a novel contribution that can be valuable for future research in protein engineering. However, a potential weakness is that the paper may not sufficiently address the limitations of the model or the specific conditions under which it excels or fails. Furthermore, while the experimental results are promising, the paper could benefit from additional comparisons to a broader range of existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its findings, making it accessible to readers familiar with protein structures and machine learning. The quality of the writing is high, and the experiments are described in sufficient detail to allow for reproducibility. The novelty of the approach is commendable, as it combines multiple structural insights into a unified framework for pre-training. However, the extent to which the method can be generalized outside the tested scenarios remains to be clarified.\n\n# Summary Of The Review\nOverall, this paper introduces an innovative framework for protein function prediction that leverages multi-level structural information, achieving significant performance improvements over existing methods. While the contributions are noteworthy and the methodology is sound, the paper could benefit from a more thorough discussion of its limitations and broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework called PromptProtein, which focuses on multi-level protein structure pre-training for function prediction. It integrates four types of protein structure levels—primary, secondary, tertiary, and quaternary—using a prompt-guided multi-task learning approach. The methodology includes three pre-training tasks: Masked Language Modeling (MLM) for primary structures, Alpha-Carbon Coordinate Prediction (CRD) for secondary and tertiary structures, and Protein-Protein Interaction Prediction (PPI) for quaternary structures. Experimental results demonstrate that PromptProtein significantly outperforms existing methods, particularly in low-resource scenarios, highlighting the importance of integrating multi-level structural information for effective protein function prediction.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to multi-level protein structure integration and the introduction of a prompt-guided framework that minimizes task interference. The comprehensive evaluation across various protein function prediction tasks and the use of multiple baselines lend credibility to the findings. However, one potential weakness is the reliance on specific prompt mechanisms which may limit the generalizability of the approach to other domains or tasks outside of protein function prediction. Additionally, while the ablation study supports the importance of each component, further exploration of the model's scalability and efficiency could enhance the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, facilitating ease of understanding for readers. The methodology is detailed and the experiments are conducted rigorously, providing a high level of reproducibility. The novelty of the approach is evident in the integration of multi-level protein structures and the prompt-based learning mechanism, which are relatively underexplored in the current literature. Overall, the paper maintains a strong quality in both presentation and scientific rigor.\n\n# Summary Of The Review\nPromptProtein presents a compelling advancement in protein function prediction through the integration of multi-level structural information and a novel prompt-guided learning framework. The findings demonstrate significant performance improvements over existing methods, particularly under challenging low-resource conditions. While the approach shows promise, further exploration of its generalizability and computational efficiency could strengthen its applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Multi-Level Protein Structure Pre-Training with Prompt Learning\" presents a novel framework, PromptProtein, that integrates multi-level protein structure information for enhanced protein function prediction. The methodology employs a Prompt-aware Transformer architecture and three pre-training tasks—Masked Language Modeling (MLM), Alpha-Carbon Coordinate Prediction (CRD), and Protein-Protein Interaction Prediction (PPI)—to leverage primary, secondary, tertiary, and quaternary structural insights. The findings indicate that PromptProtein significantly outperforms existing models in various protein function prediction tasks, demonstrating the effectiveness of prompt-guided learning.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to integrating multi-level structural information using prompt learning, which addresses limitations found in existing protein representation models. The comprehensive evaluation across multiple datasets and the detailed ablation studies provide strong empirical support for the proposed model's effectiveness. However, the paper could improve by discussing potential limitations and challenges encountered during the study, which would provide a more balanced view of the model's applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, as it is well-structured and presents a coherent narrative that is accessible to readers with a background in machine learning and bioinformatics. The quality of the methodology and experiments is high, with thorough evaluations and clear explanations of the model architecture and tasks. The novelty of applying prompt learning to protein modeling is significant, marking a meaningful contribution to the field. Reproducibility is supported by the detailed description of methodologies and datasets, although additional information on implementation details could further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes valuable contributions to protein function prediction through its innovative use of prompt learning and multi-level structural analysis. While the clarity and empirical evidence are strong, a discussion of limitations would provide a more comprehensive perspective on the proposed approach.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents PromptProtein, a novel framework for multi-level protein structure pre-training utilizing prompt learning. The authors propose a multi-task pre-training and fine-tuning approach that integrates primary, secondary, tertiary, and quaternary structural information from proteins to enhance functional prediction. The methodology includes masked language modeling, alpha-carbon coordinate prediction, and protein-protein interaction prediction, all guided by prompt tokens to manage task-specific representations. Experimental results demonstrate that PromptProtein significantly outperforms existing state-of-the-art methods, particularly in low-resource scenarios, achieving an average enhancement of 17.0% across various metrics.\n\n# Strength And Weaknesses\nThe strengths of this paper include its innovative approach that combines multiple levels of protein structure, addressing a significant gap in current methodologies that typically focus on single-level inputs. The implementation of prompt-guided learning is a notable contribution that enhances the model's ability to manage multiple tasks effectively. However, the paper could improve by providing more detailed comparative analyses with baseline models and discussing the limitations of the current approach, particularly regarding the scalability and generalizability of the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and rationale behind the proposed framework. The clarity of the mathematical formulations and the explanations of the tasks contribute to the overall quality of the work. The novelty lies in the integration of multi-level structural information using prompt learning, which is a fresh perspective in the field of protein structure prediction. Reproducibility is supported by the detailed description of datasets and metrics, although sharing code and data would further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a compelling case for the importance of integrating multi-level structural data in protein function prediction. The proposed PromptProtein framework demonstrates significant improvements over existing methods, although additional comparisons and discussions on limitations could strengthen the contribution further.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel method called PromptProtein aimed at improving protein structure and function prediction through a prompt-guided learning approach. The authors claim that their modifications, including a prompt-aware attention module and multi-task learning framework, enhance the model's efficacy, particularly in low-resource scenarios. Experimental results are reported, demonstrating improvements over state-of-the-art methods on selected datasets, although the generalizability and robustness of these findings remain questionable.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to leverage prompt learning, which has gained traction in natural language processing, and its application to protein prediction tasks. However, the method lacks significant novelty as it fundamentally builds upon existing pre-trained protein models without introducing innovative concepts. The reliance on prompt learning raises concerns regarding its applicability to the complexities inherent in protein structures. Additionally, the limited dataset range and the potential for overfitting call into question the model's performance and generalizability. The paper also falls short in addressing challenges related to the prompt-guided approach and does not provide a thorough analysis of task interference, leaving gaps in the understanding of its effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is hindered by a lack of detailed explanations regarding the architecture modifications and their implications. The novelty is limited as it does not introduce significant advancements beyond established methods. Reproducibility is questionable due to insufficient statistical validation of results and the narrow focus of experimental evaluations, which do not comprehensively represent the diverse biological scenarios in which the model may be applied.\n\n# Summary Of The Review\nOverall, while the proposed PromptProtein approach attempts to apply recent advancements in NLP to protein prediction tasks, it lacks substantial novelty and rigor in its methodology and analysis. The reported improvements are not convincingly substantiated, raising doubts about the model's real-world applicability and robustness.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents **PromptProtein**, a novel prompt-based pre-trained model that effectively utilizes multi-level protein structures for function prediction. It integrates primary, secondary, tertiary, and quaternary structural information into a cohesive framework, enhancing the model's capability to capture complex biological insights. The methodology includes a prompt-guided multi-task pre-training approach featuring three innovative tasks: Mask Language Modeling, Alpha-Carbon Coordinate Prediction, and Protein-Protein Interaction Prediction. The findings indicate that PromptProtein achieves state-of-the-art results in various function prediction and protein engineering tasks, with significant improvements observed in low-resource settings.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to protein structure modeling, particularly through the incorporation of multiple structural levels and a prompt-guided framework. This enhances the model's flexibility and applicability across different tasks, demonstrating versatility in addressing complex protein function predictions. However, the paper could benefit from a more detailed discussion on the computational efficiency and scalability of the proposed model, particularly when applied to larger datasets or real-world biological scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the novel contributions of the PromptProtein model. The methodology is outlined in a systematic manner, making it accessible for readers to understand the underlying principles and implementation. The novelty of integrating multi-level structures with a prompt-guided approach is commendable, and the empirical results are compelling. However, the reproducibility of the findings may require additional details on the experimental setup and hyperparameter tuning used during training, which could aid other researchers in validating and building upon this work.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in protein function prediction through the innovative PromptProtein model, which demonstrates both technical and empirical strengths. While the contributions are noteworthy, enhancing clarity on reproducibility aspects could further solidify its impact in the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a theoretical framework for enhancing protein function prediction through multi-level protein structure pre-training with prompt learning. It proposes the use of pre-trained protein models (PTPMs) that leverage the complexities of protein structures at various levels—primary, secondary, tertiary, and quaternary. The methodology introduces a prompt-guided learning approach that incorporates a modified Transformer architecture equipped with a prompt-aware attention module to effectively integrate multi-level structural information while addressing potential task interference. The findings suggest that this framework could significantly improve the representation of protein functionalities, with implications for protein engineering and broader biological applications.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating multi-level structural insights for protein function prediction, which is an area often overlooked by existing models that focus on singular structural levels. The theoretical contributions regarding the prompt-guided learning framework and the proposed modifications to the attention mechanism are noteworthy and address critical challenges in multi-task learning. However, a notable weakness is the absence of empirical validation for the theoretical constructs, which diminishes the practical applicability of the proposed framework. Furthermore, the paper could benefit from a more detailed discussion on the implications of task interference and how it can be quantitatively measured in future studies.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its theoretical constructs. The quality of the writing is high, with clear definitions and explanations of complex concepts. The novelty of the proposed prompt-guided learning framework and its implications for protein structure representation is significant, particularly in the context of AI applications in bioinformatics. However, the reproducibility of the results is currently uncertain due to the lack of empirical evidence supporting the theoretical claims. Future work should include empirical studies to validate the proposed methodologies.\n\n# Summary Of The Review\nOverall, the paper offers a compelling theoretical framework for enhancing protein function prediction through multi-level structural integration and prompt learning. While it presents innovative ideas and contributes to the discourse in AI-driven bioinformatics, the lack of empirical validation limits its immediate applicability. Further research is needed to substantiate the theoretical claims made.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Multi-Level Protein Structure Pre-Training with Prompt Learning\" presents a novel framework called PromptProtein, which aims to enhance protein function prediction by leveraging a multi-task, prompt-guided pre-training and fine-tuning methodology. The authors introduce a model architecture with 650M parameters, consisting of 33 layers and 20 attention heads, specifically designed to process multi-level protein structural information. Key contributions include the development of a Prompt-Aware Attention Module, which decouples prompt signals from amino acid tokens, and the demonstration of effective performance on multiple tasks such as Masked Language Modeling, Alpha-Carbon Coordinate Prediction, and Protein-Protein Interaction Prediction. The paper provides comprehensive implementation details and ablation studies to support the proposed methods.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its clear articulation of a new architecture and methodology that efficiently integrates multi-level protein structural information. The ablation studies enhance the credibility of the proposed components, demonstrating their necessity for performance improvements. However, a notable weakness is the lack of discussion on the broader implications of the findings within the field of protein science and machine learning, which limits the paper's impact. Additionally, while the implementation details are thorough, the results could benefit from comparisons with existing state-of-the-art methods to better contextualize the performance of the PromptProtein model.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, making it accessible to readers with varying levels of expertise in protein structure prediction and machine learning. The quality of the experiments is high, with detailed descriptions of the datasets, tasks, and evaluation metrics. Furthermore, the authors have committed to releasing the source code, enhancing the reproducibility of their work. Nonetheless, the novelty, while significant in the context of prompt learning for protein structures, may not be groundbreaking within the broader landscape of deep learning applications.\n\n# Summary Of The Review\nOverall, the paper presents a solid contribution to the field of protein structure prediction by introducing a novel prompt-guided framework that showcases effective multi-task learning. While the methodology is well-articulated and the experiments are robust, the paper could be improved by discussing the broader implications of the findings and providing more contextual comparisons with existing work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel prompt-guided multi-task pre-training framework for protein function prediction, termed PromptProtein. The authors claim that this framework outperforms existing state-of-the-art models and emphasizes the integration of quaternary structure information. The methodology involves a prompt-aware attention module built on a Transformer architecture, with extensive experimental results demonstrating an average improvement of 17.0% in low-resource settings compared to some baselines. However, the necessity of the approach is called into question given the effectiveness of established models like AlphaFold2 and ESM-1b without multi-level structure integration.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including a comprehensive experimental evaluation and the introduction of a prompt-aware attention module. However, it has notable weaknesses, such as insufficient acknowledgment of existing methodologies that address similar challenges, like GradNorm and RotoGrad. Additionally, the claim of being the first prompt-based pre-trained protein model overlooks prior work in the field, and comparisons with outdated baselines diminish the claimed improvements. The authors also fail to address limitations and potential biases in their data sources, which could impact the robustness of their findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly. However, the novelty of the contributions is somewhat overstated, as it does not sufficiently differentiate itself from existing models. The reproducibility of the results could be strengthened by providing a more detailed account of experimental setups and comparisons to contemporary models. Overall, while the clarity is adequate, the novelty and quality of the contributions require more substantiation.\n\n# Summary Of The Review\nWhile the paper proposes an interesting framework for protein function prediction and presents promising results, it lacks a nuanced comparison with existing methodologies and does not sufficiently address the limitations of its approach. The authors should more clearly differentiate their work from prior art to strengthen their claims of novelty and significance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Multi-Level Protein Structure Pre-Training with Prompt Learning\" presents a novel methodology aimed at improving protein function prediction through a multi-level approach to protein structure representation. The authors propose a pre-training framework that leverages prompt learning techniques to bridge the gap between pre-training and downstream tasks. Their findings indicate that this approach significantly enhances performance in predicting protein functions, particularly in low-resource settings, by effectively utilizing structural information across different levels of protein organization.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative integration of prompt learning into the realm of protein function prediction, which is a relatively underexplored area. The multi-level approach allows for a more comprehensive understanding of protein structures and their functionalities. However, the paper suffers from several clarity issues and inconsistencies in formatting, which detract from its overall impact. Moreover, some methodological details, such as the specific nature of low-resource tasks and the definitions of key terms, are not adequately explained, which may hinder reproducibility and understanding for readers unfamiliar with the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the novelty of the proposed framework is commendable, the clarity of the writing could be significantly improved. There are multiple instances of awkward phrasing and incomplete sentences, which could confuse readers. The formatting issues, including inconsistent acronym usage and figure labeling, further complicate understanding. The reproducibility of the results may be compromised due to insufficient explanations of the methodologies and lack of clear definitions for several key terms and metrics.\n\n# Summary Of The Review\nOverall, the paper contributes a valuable approach to protein function prediction using multi-level structure representation and prompt learning. However, it requires substantial revisions for clarity and consistency to enhance comprehension and reproducibility.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to protein function prediction by focusing on the multi-level structure of proteins. The proposed methodology employs a prompt-guided multi-task learning framework that aims to improve predictive accuracy by considering various structural levels. The findings indicate that the model performs well in low-resource settings, offering promising results. However, the authors do not explore the potential applicability of their approach to other biological molecules or the broader implications of their findings.\n\n# Strength And Weaknesses\nThe key strengths of the paper include its innovative use of a multi-task learning framework, which demonstrates impressive experimental results in specific contexts. However, the paper has several weaknesses. It fails to address how the model could be generalized to other protein families, lacks a discussion on the implications of integrating complex biological interactions like post-translational modifications, and does not consider performance with noisy or incomplete data. Additionally, the limited comparative analysis with other advanced methods in protein modeling reduces the contextual relevance of its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is generally good, but the lack of thorough discussions on various critical aspects of the methodology diminishes the overall quality. While the technical novelty is present, the applicability and reproducibility of the findings could be questioned due to the absence of detailed exploration into how the model could function in diverse biological settings or with varying data quality.\n\n# Summary Of The Review\nOverall, while the paper introduces a compelling approach to protein function prediction with notable experimental results, it falls short in exploring the broader implications and limitations of its methodology. Addressing these gaps would enhance its relevance and applicability in biological research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces PromptProtein, a novel framework aimed at predicting protein functions by utilizing multi-level structural information. The methodology involves prompt-guided multi-task pre-training, leveraging extensive datasets from UniRef50, Protein Data Bank (PDB), and STRING. The findings demonstrate that PromptProtein achieves significant performance improvements, averaging 17.0% on low-resource tasks compared to existing models, validated through various statistical metrics such as AUPR, Fmax, and Spearman’s rank correlation.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive approach to model evaluation, including rigorous statistical significance testing and detailed ablation studies that highlight the importance of different model components. The robust dataset collection and the comparative performance against multiple baseline models further strengthen its contributions. However, a notable weakness is the paper's heavy emphasis on statistical performance metrics, which may overshadow the innovative framework proposed. Additionally, while the methodology is sound, the paper could benefit from further exploration of the theoretical implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, presenting its methods and results in a comprehensible manner. The quality of the research appears high, with a solid empirical foundation. However, the novelty of the approach could be perceived as moderate due to the reliance on existing techniques in protein prediction. Reproducibility is adequately addressed through detailed descriptions of the experimental setup, including dataset splits and parameter optimization methods.\n\n# Summary Of The Review\nOverall, the paper presents a well-executed study on protein function prediction using a novel prompt-guided pre-training framework. While the statistical rigor and empirical results are commendable, the innovation aspect of the framework may require further emphasis to fully appreciate its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework for protein function prediction that leverages a prompt-guided multi-task learning approach. The methodology includes three pre-training tasks: masked language modeling (MLM), contrastive representation learning (CRD), and protein-protein interaction (PPI) prediction. The findings suggest that the proposed model achieves competitive performance in terms of accuracy and efficiency; however, it does not extensively address the complexities of multi-level protein structures beyond the quaternary level.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative use of multi-task learning to address protein function prediction, which could enhance the model's performance under certain conditions. However, the weaknesses are significant; the lack of exploration into the integration of higher structural levels limits applicability. Additionally, the scalability of the framework to larger datasets and real-world applications is not discussed, raising concerns about its generalizability. The model's performance on proteins with highly variable structures and on those without homologous sequences is not evaluated, which could obscure its robustness and predictive capabilities.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology and findings. However, the novelty is somewhat diminished by the limited exploration of alternative tasks that could improve model performance and the absence of a detailed comparison with existing methods in protein function prediction. Reproducibility may be a concern due to the lack of comprehensive results on robustness and applicability to diverse datasets, which would be crucial for validating the proposed framework.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting approach to protein function prediction through multi-task learning, but it suffers from various limitations that hinder its applicability and robustness. The lack of exploration into more complex structural levels and the absence of evaluations on diverse protein types raise questions about its generalizability and performance in practical settings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"Multi-Level Protein Structure Pre-Training with Prompt Learning\" presents a framework for protein structure representation by integrating multi-level structural information—primary, secondary, tertiary, and quaternary. The authors propose a novel approach called \"PromptProtein\" that includes three pre-training tasks: Mask Language Modeling (MLM), Coordinate Prediction, and Protein-Protein Interaction, to facilitate better function prediction. The findings indicate that their method significantly outperforms existing state-of-the-art models, claiming an average improvement of 17.0% on low-resource tasks.\n\n# Strength And Weaknesses\nStrengths of the paper include the comprehensive framework that considers various protein structural levels and the introduction of prompt learning in the context of protein representation. However, the weaknesses are notable; the concepts presented are not new to the field, with similar methodologies having been previously explored. The use of buzzwords and trendy terminology does not add substantial novelty to the contributions, and the empirical results, while claiming improvements, require further depth to validate their significance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in its presentation. However, the novelty of the contributions is questionable, as many of the ideas appear to be rehashed from existing literature. The reproducibility of the findings may be hindered by the lack of detailed descriptions regarding experimental setups and an insufficient discussion of the actual numerical results that support their claims.\n\n# Summary Of The Review\nOverall, the paper presents a framework that combines established concepts in protein structure representation with a new interface for prompt learning. While the authors make claims of significant performance improvements, the contributions lack the necessary novelty and depth to stand out in the field. As such, the paper may appeal more to those unfamiliar with existing literature rather than providing groundbreaking insights.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel framework, PromptProtein, which integrates multi-level protein structure information for function prediction through a prompt-guided multi-task pre-training approach. The authors propose three specific pre-training tasks (Masked Language Modeling, Contextual Residual Denoising, and Protein-Protein Interaction prediction) to enhance structural representation learning. The findings indicate significant improvements in performance, especially in low-resource settings, suggesting that the framework effectively leverages multi-task learning to address the challenges of predicting protein functions.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative integration of multi-level structural information with a multi-task approach, which is well-aligned with contemporary research trends in representation learning. The prompt-aware attention module is a notable contribution, potentially addressing task interference in multi-task learning. However, weaknesses include a lack of exploration of meta-learning strategies for prompt optimization and the need for additional tasks that capture broader biological phenomena. The complexity of the model could also be a concern, necessitating further analysis to balance performance and computational efficiency.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the proposed methodology and its rationale. The quality of the experiments is solid, supported by ablation studies that highlight the importance of each framework component. However, the reproducibility of the results could be enhanced by providing more detailed descriptions of the datasets and hyperparameter settings used. The novelty of the framework is commendable, particularly with the introduction of prompt-aware mechanisms, but further exploration of hierarchical attention and self-supervised learning methods could strengthen the contribution.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to protein function prediction through the integration of multi-level structure information and multi-task learning. While the contributions are significant, there are opportunities for enhancement through additional tasks, improved interpretability, and the exploration of meta-learning techniques.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces PromptProtein, a novel model aimed at improving protein function prediction and engineering tasks. The methodology involves leveraging multi-level protein structure information, which is shown to yield significant performance improvements across various benchmark tasks. The findings illustrate that PromptProtein achieves state-of-the-art results in function prediction tasks, particularly excelling in low-resource scenarios, and consistently outperforms existing models in protein engineering tasks, as evidenced by robust empirical results in multiple evaluations.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive empirical validations and the clear demonstration of PromptProtein's superiority over baseline models across multiple tasks. The ablation studies provide strong evidence for the importance of its architectural components, particularly the multi-task approach and skip connections, which enhance performance. However, the paper could benefit from a deeper exploration of the limitations of the proposed model, including potential scenarios where it might underperform or face challenges. Additionally, further discussion on the scalability of the methodology for larger protein datasets would strengthen the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents the methodology and findings. The results are supported by thorough experimental evaluations, including detailed tables that illustrate the model's performance across various metrics. The novelty of the approach is significant, particularly in its application to low-resource scenarios, which is a notable gap in existing literature. The reproducibility of the results is strengthened by the inclusion of ablation studies, though the paper would benefit from providing more detailed information on the implementation and training procedures to facilitate replication by other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in protein function prediction and engineering through the introduction of PromptProtein. The strong empirical results and well-designed experiments underscore its potential impact in computational biology. However, some additional discussion on limitations and reproducibility could enhance the overall quality of the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach named \"PromptProtein\" aimed at enhancing protein structure prediction through innovative prompt engineering techniques. The authors employ a combination of machine learning methodologies, including pre-trained models, to generate and assess protein structures efficiently. The findings indicate that PromptProtein significantly outperforms existing methods in terms of accuracy and computational efficiency, showcasing the potential of leveraging prompt-based strategies in bioinformatics.\n\n# Strength And Weaknesses\nThe primary strength of this work lies in its innovative integration of prompt engineering with protein structure prediction, which addresses a critical challenge in computational biology. The empirical results demonstrate clear improvements over baseline methods, validating the proposed approach. However, the paper suffers from several weaknesses, including occasional redundancy, a lack of clarity in the methodology section, and an overwhelming use of technical jargon that may alienate non-expert readers. Additionally, the disjointed flow and abrupt transitions between topics hinder overall readability.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel contribution to the field, clarity is compromised due to the dense abstract, inconsistent terminology, and insufficient contextual explanations for complex concepts. The methodology section is particularly challenging to follow, lacking a clear stepwise breakdown. Furthermore, the reproducibility of the results may be hindered by the absence of a glossary for abbreviations and a lack of detailed descriptions for figures and tables. A more structured presentation and thorough proofreading could enhance the paper's overall quality.\n\n# Summary Of The Review\nIn summary, this paper introduces an innovative approach to protein structure prediction with promising empirical results; however, it suffers from clarity issues and structural weaknesses that detract from its overall impact. Improvements in the organization and presentation of the content are necessary to maximize its accessibility and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.3113673234536334,
    -1.6467669845721185,
    -1.8662154731932719,
    -1.6779373542696683,
    -1.8919819812765866,
    -1.7247073637787818,
    -1.8151289441473624,
    -1.734829835614723,
    -1.6263837870547084,
    -1.8676023998790947,
    -1.6138323961308263,
    -1.3505519403582986,
    -1.6242475670333656,
    -1.5203591855254055,
    -1.7094261406446392,
    -1.7279547619954017,
    -1.9282559288227632,
    -1.704847853935243,
    -1.7593175393223484,
    -1.7461939935485624,
    -1.9804071202907985,
    -1.6802266984795626,
    -1.905842656327827,
    -1.9899551507681956,
    -1.7344311072645218,
    -1.9664854480832132,
    -1.8596340753558251,
    -1.7829440434656492,
    -1.7929561350086625
  ],
  "logp_cond": [
    [
      0.0,
      -2.228708829004361,
      -2.2381175403867806,
      -2.2264234678112724,
      -2.247274084321091,
      -2.267383685737357,
      -2.2445090397103966,
      -2.2521931089394265,
      -2.2479954501908765,
      -2.244747720512327,
      -2.2315194634812214,
      -2.276953933436112,
      -2.252422096660184,
      -2.2366515171146273,
      -2.2414461961027405,
      -2.24430944103148,
      -2.2562383092330625,
      -2.2367350748120063,
      -2.2620419603791,
      -2.2305271486475298,
      -2.2467022098998712,
      -2.2598246759393206,
      -2.2573932588814816,
      -2.2459841875002082,
      -2.2476793439811704,
      -2.259403799963089,
      -2.2362697430592635,
      -2.2541981803802686,
      -2.264671557611786
    ],
    [
      -1.2337255094546344,
      0.0,
      -1.1027283266209524,
      -1.0613621204773225,
      -1.2046291225655112,
      -1.2896479104018377,
      -1.2464706792611422,
      -1.1564995355598466,
      -1.1623849835807596,
      -1.2361792226075432,
      -1.1724166138493683,
      -1.4064560371015646,
      -1.1868798013887052,
      -1.0969573398023016,
      -1.1695629643320884,
      -1.1127789669065855,
      -1.2956160962489158,
      -1.1330259301066534,
      -1.267348607508506,
      -1.2112813311182173,
      -1.2245667466038772,
      -1.2857090656739258,
      -1.2891048554111455,
      -1.2351284409846066,
      -1.3001264706193965,
      -1.1862741597297204,
      -1.2434075771985533,
      -1.243213061277907,
      -1.3672222125546383
    ],
    [
      -1.3843617925815919,
      -1.2888134700071263,
      0.0,
      -1.2682972827242007,
      -1.327646196391846,
      -1.4270092007495316,
      -1.3909668091734138,
      -1.3841062221391771,
      -1.2981048317330979,
      -1.3283352622679756,
      -1.368976950716864,
      -1.6071077329846288,
      -1.356233777644525,
      -1.275151587497279,
      -1.3462841985390266,
      -1.2967357385801082,
      -1.474792589035147,
      -1.321176925888896,
      -1.4215516413949607,
      -1.3751906998086574,
      -1.3997558100320084,
      -1.4613640227565223,
      -1.438691688238547,
      -1.401627430890436,
      -1.4096325085730816,
      -1.332382600612639,
      -1.3848585196365628,
      -1.4681121089924498,
      -1.542822452250841
    ],
    [
      -1.2263035770601944,
      -1.081193581511126,
      -1.1753967207548985,
      0.0,
      -1.2432327220000443,
      -1.3287503532939886,
      -1.2418591798489846,
      -1.2032579200436435,
      -1.1474184608530813,
      -1.2207428984328283,
      -1.182816229147894,
      -1.4191294875840388,
      -1.1588115574977464,
      -1.1306799069830273,
      -1.1319349870474107,
      -1.1391329611109209,
      -1.2707962461287088,
      -1.1647877171995629,
      -1.195824023325748,
      -1.172107950652413,
      -1.2360834460590653,
      -1.281056023477544,
      -1.3040570079880516,
      -1.2535060143225363,
      -1.3056093615052666,
      -1.2364491983494446,
      -1.189414451595779,
      -1.285458607082201,
      -1.358244453803382
    ],
    [
      -1.513084221114159,
      -1.4850901409428527,
      -1.4693484134442825,
      -1.469120291774645,
      0.0,
      -1.5217307162405043,
      -1.4873557631029644,
      -1.4362314272901866,
      -1.4564822649189746,
      -1.4576533196987367,
      -1.5123605242447207,
      -1.6372024567617105,
      -1.5283914939265866,
      -1.4638890515273821,
      -1.4860698730919917,
      -1.489070375658368,
      -1.5381987610511907,
      -1.4962910832515859,
      -1.513066374262858,
      -1.4527218882249031,
      -1.5768055361656912,
      -1.5418253564592828,
      -1.5447599091722535,
      -1.5205934147008355,
      -1.461400693997392,
      -1.4883841572127494,
      -1.421956463973637,
      -1.555170603767228,
      -1.6113501259243344
    ],
    [
      -1.4137596227151819,
      -1.3201310543952645,
      -1.2536204025442161,
      -1.3127593030279072,
      -1.3263473625429674,
      0.0,
      -1.3455049671005406,
      -1.2804375586592915,
      -1.2956410114329802,
      -1.213957821316105,
      -1.2802843221746953,
      -1.4525146351865097,
      -1.2913502084994966,
      -1.2907888597098434,
      -1.2740094317364246,
      -1.3009986422412398,
      -1.3426591319168404,
      -1.3383634685593926,
      -1.2744572789811281,
      -1.2400668118723588,
      -1.3096895206522432,
      -1.2867574505408321,
      -1.3554599821756264,
      -1.2982044754137498,
      -1.340082586584366,
      -1.245604168770768,
      -1.364393375009974,
      -1.3469915562437158,
      -1.4016254417052143
    ],
    [
      -1.4377979862526613,
      -1.3409026451162271,
      -1.3551277026515889,
      -1.3575759782689811,
      -1.3479274921629318,
      -1.4705547591215513,
      0.0,
      -1.4054977433400135,
      -1.445623485381966,
      -1.4013768850957726,
      -1.4120268596552072,
      -1.514539995042796,
      -1.4402585728953687,
      -1.3999854819643223,
      -1.368608019599234,
      -1.3930134447921945,
      -1.4074464805394777,
      -1.3968674142323763,
      -1.4639557981938953,
      -1.4075640739494317,
      -1.4726708371440103,
      -1.4287511126217156,
      -1.4927460421614671,
      -1.450912297086802,
      -1.4770516058254097,
      -1.3786165684392901,
      -1.434817586450369,
      -1.4612615528908628,
      -1.4567356687165933
    ],
    [
      -1.4133662931631177,
      -1.2114529202516837,
      -1.2502291262303205,
      -1.25470813237545,
      -1.2532106275576693,
      -1.3233144300628523,
      -1.3257124041259756,
      0.0,
      -1.2882279935977712,
      -1.2775892304493646,
      -1.3015994018700783,
      -1.4634803691214129,
      -1.291481530099981,
      -1.2465777215951537,
      -1.243458052468967,
      -1.2268810674210175,
      -1.326353267229366,
      -1.2836427119538685,
      -1.306743333874467,
      -1.2710322994500438,
      -1.2917360662445028,
      -1.3041629565957484,
      -1.3376282224426181,
      -1.2914046659289589,
      -1.3085512484468074,
      -1.2096508012810105,
      -1.2727114881540211,
      -1.3420599760634797,
      -1.4035521225240966
    ],
    [
      -1.3377598701099365,
      -1.1560943778940642,
      -1.1420532179555667,
      -1.0690080246533837,
      -1.1930663247505842,
      -1.2873412922620728,
      -1.281904874955658,
      -1.2604306575603679,
      0.0,
      -1.2255178192316052,
      -1.2152445858884655,
      -1.332721400474846,
      -1.2104164884951247,
      -1.1030458789289384,
      -1.1580279592354086,
      -1.1957189393327827,
      -1.2684473630788242,
      -1.1062417206528148,
      -1.1950325707052505,
      -1.146292269178476,
      -1.25832970776759,
      -1.2845059023072185,
      -1.2678967913712427,
      -1.2458082857317836,
      -1.2191089584192392,
      -1.2443894420980495,
      -1.1450006686511955,
      -1.2867675704785877,
      -1.3722866311979756
    ],
    [
      -1.526001799609597,
      -1.4655346710176016,
      -1.479264094330421,
      -1.4422890218783735,
      -1.4460955161585434,
      -1.5355702965248104,
      -1.5419941669068522,
      -1.4069694867835363,
      -1.4826950866500654,
      0.0,
      -1.4802509513573046,
      -1.6176021562983425,
      -1.4863365464034792,
      -1.4623853214723028,
      -1.4615724750911785,
      -1.4780351208418672,
      -1.5246263999198508,
      -1.4828716339227253,
      -1.4390615732630536,
      -1.4487128540158003,
      -1.5104586157457118,
      -1.4571467830888911,
      -1.4820125278885397,
      -1.5173353648141692,
      -1.4385223749103004,
      -1.4225098324562548,
      -1.4635685721619505,
      -1.5149872802826623,
      -1.575288293430001
    ],
    [
      -1.258591838065983,
      -1.1928428583410062,
      -1.2105505557560863,
      -1.1811284861544051,
      -1.2804324151834312,
      -1.3235807736289933,
      -1.2784105801118335,
      -1.2421068084772975,
      -1.2689859118611264,
      -1.232588995324999,
      0.0,
      -1.378708157337383,
      -1.1979215941022858,
      -1.2424159584831926,
      -1.270009805424286,
      -1.2685581335314742,
      -1.253693268078439,
      -1.2230316651652278,
      -1.203425308470375,
      -1.2629357915291648,
      -1.242755005929371,
      -1.2500743926802191,
      -1.2593984946690238,
      -1.2335509011198562,
      -1.2925768432125133,
      -1.2483623848733156,
      -1.2258170262989982,
      -1.2370031050047006,
      -1.3269791277269325
    ],
    [
      -1.109352443358971,
      -1.0652036721978508,
      -1.0571124148533275,
      -1.0665817521758538,
      -1.061238531227562,
      -1.0911026990644468,
      -1.080800230596519,
      -1.07156916837075,
      -1.01736211477717,
      -1.0497685949809654,
      -1.0419767252344343,
      0.0,
      -1.0595727273380027,
      -1.0802449791241906,
      -1.0858324263921781,
      -1.0987739831149574,
      -1.0529150348693808,
      -1.0449416108586462,
      -1.0917829185193704,
      -1.067228508967742,
      -1.10614007016759,
      -1.0232494227111877,
      -1.0961659841617573,
      -1.0524766241069832,
      -1.0590434858063664,
      -1.0775889111720003,
      -1.0432081735407253,
      -1.0749219174433782,
      -1.0334515314248434
    ],
    [
      -1.3307512337789043,
      -1.178857058451803,
      -1.1949680182790128,
      -1.1685360078304066,
      -1.2745114143798786,
      -1.2691164940168203,
      -1.2564418840812615,
      -1.209251534471587,
      -1.2189895195148737,
      -1.210725082011445,
      -1.1337444217470676,
      -1.3554337561161236,
      0.0,
      -1.2194146076376904,
      -1.260202373780918,
      -1.1860160904383914,
      -1.2554400195789928,
      -1.2047303904091164,
      -1.263390138382832,
      -1.2347304371666643,
      -1.1916628846104913,
      -1.2797810253154314,
      -1.2144738703716484,
      -1.2151179730917823,
      -1.268399529146275,
      -1.2038044602853915,
      -1.2726302618251686,
      -1.2472131330553102,
      -1.294818598693597
    ],
    [
      -1.1382440222751817,
      -0.9653792852421881,
      -1.0339185320732838,
      -0.9617482974737288,
      -1.0740322466223715,
      -1.1631172804162193,
      -1.1168046168441728,
      -1.0478657960480242,
      -1.0093519382233591,
      -1.0660445561026266,
      -1.0451975834597496,
      -1.2443062366807558,
      -1.0704319029636455,
      0.0,
      -1.0028169215513043,
      -1.0172950639938572,
      -1.170745559677811,
      -1.0299023011531756,
      -1.0920872264699693,
      -1.039994088438812,
      -1.115692828849776,
      -1.1486358583182414,
      -1.1574328691380174,
      -1.130266977948948,
      -1.114963986954736,
      -1.0711368775490144,
      -1.068137926405963,
      -1.1700775221919024,
      -1.2442506540837046
    ],
    [
      -1.2763763145277187,
      -1.1050507628501036,
      -1.150601398545158,
      -1.0868389952441755,
      -1.1632070846052638,
      -1.2295859939622447,
      -1.2540473390026192,
      -1.137536651380192,
      -1.188107612752964,
      -1.1713424187626007,
      -1.2574607674716825,
      -1.3812469515203711,
      -1.2117815774291765,
      -1.1113931085508573,
      0.0,
      -1.1382035250119522,
      -1.2929746992125701,
      -1.1116988626753312,
      -1.1672199636514435,
      -1.1566082684855417,
      -1.2463469236272324,
      -1.2121783449213035,
      -1.2765783425152724,
      -1.2693139173389758,
      -1.2668363249527603,
      -1.144075107768375,
      -1.2162325350014138,
      -1.2430046704539335,
      -1.3528666883775273
    ],
    [
      -1.3270913987128776,
      -1.138965560023903,
      -1.1761238071972735,
      -1.1858761916100673,
      -1.2436184290695482,
      -1.3344077689224214,
      -1.3299066189541473,
      -1.159610088551933,
      -1.197219482119157,
      -1.2533193585606452,
      -1.276688604275809,
      -1.4388067481461035,
      -1.2222023068913923,
      -1.2131276582547805,
      -1.1863677301820816,
      0.0,
      -1.3578589838486779,
      -1.2485959173085954,
      -1.282711757371332,
      -1.1737833982900372,
      -1.2345423327978386,
      -1.3143237161886168,
      -1.3046766806600523,
      -1.2557504215386233,
      -1.2982622455689432,
      -1.2128208675217875,
      -1.2845577384400904,
      -1.3411828226949207,
      -1.3863471017605533
    ],
    [
      -1.5480031827737315,
      -1.5020138563697798,
      -1.4898416078848995,
      -1.5001733369688741,
      -1.5840365440254411,
      -1.5525728817467155,
      -1.5414005925897019,
      -1.516066958018699,
      -1.548506738021826,
      -1.4995492466084008,
      -1.4828802717501832,
      -1.6392904601697749,
      -1.541678568863889,
      -1.504218021543369,
      -1.5194553690238943,
      -1.5117215101620696,
      0.0,
      -1.5829573482349932,
      -1.449885132343923,
      -1.5128078994306249,
      -1.5001662320494789,
      -1.5358257916376985,
      -1.5491931025161823,
      -1.5615105913578788,
      -1.572552688446713,
      -1.5148660219740486,
      -1.5420241098392335,
      -1.5621485050490158,
      -1.5708176402459866
    ],
    [
      -1.3739688591458066,
      -1.1889447410976002,
      -1.2111708175512215,
      -1.163188780917613,
      -1.3353643298732505,
      -1.3781048717401883,
      -1.3356731971006706,
      -1.2655534861781288,
      -1.1891195548728175,
      -1.292915085099623,
      -1.3000816603485383,
      -1.4243581405028412,
      -1.2728625220514542,
      -1.2629287978729695,
      -1.2856106767024345,
      -1.2709175528206795,
      -1.4111959985147917,
      0.0,
      -1.3353644570520948,
      -1.314616014074349,
      -1.324024815533111,
      -1.3557744263561395,
      -1.31981662979256,
      -1.3459871482889787,
      -1.2855029160766225,
      -1.2508967858582782,
      -1.2879849305935418,
      -1.3440198079354289,
      -1.4379718098986602
    ],
    [
      -1.4562273248317816,
      -1.3079212060203145,
      -1.330748272837901,
      -1.2439053420717063,
      -1.341832460055759,
      -1.3611957388522913,
      -1.4292404539872217,
      -1.329626766895894,
      -1.3295480313294614,
      -1.2486099654323235,
      -1.3041347861027663,
      -1.5073049763254882,
      -1.3255484530550181,
      -1.3057761808452613,
      -1.2760945890111257,
      -1.3042371505897266,
      -1.3502016566110875,
      -1.3630358708275034,
      0.0,
      -1.2726940365021886,
      -1.3286890240025595,
      -1.3397223229827764,
      -1.3748102839701342,
      -1.4068026930730224,
      -1.3791184584300085,
      -1.336678313078589,
      -1.3183882355745276,
      -1.3814085560704952,
      -1.446609357058468
    ],
    [
      -1.3514271525969037,
      -1.298402035901473,
      -1.3024068527540498,
      -1.2775730927141096,
      -1.3209074195475352,
      -1.3922775702139218,
      -1.3751665073462416,
      -1.301868103419837,
      -1.3113750375523394,
      -1.3523405472712517,
      -1.3514744173533875,
      -1.5137787884524425,
      -1.3605691283101162,
      -1.2886837744264839,
      -1.2760329559976127,
      -1.2602900695549837,
      -1.376586368632035,
      -1.3175794118423714,
      -1.3504391463063894,
      0.0,
      -1.3771297532981712,
      -1.363410493302871,
      -1.4047935085007726,
      -1.3792773713801472,
      -1.3856269521904447,
      -1.3133323126352128,
      -1.3394995619154921,
      -1.4321903024470295,
      -1.4636835390346594
    ],
    [
      -1.6620595406040797,
      -1.4850514321377721,
      -1.5153014555426667,
      -1.4847896744373945,
      -1.6232670990987066,
      -1.5983130155625136,
      -1.6065040285904943,
      -1.5236131294476438,
      -1.5644214080275667,
      -1.5747763009132605,
      -1.5385146412362156,
      -1.7085438966371587,
      -1.534751741983063,
      -1.5265663329670496,
      -1.5608510867922958,
      -1.5259236647204153,
      -1.5803873873734446,
      -1.5502958066195611,
      -1.5469154276414927,
      -1.5603008983164663,
      0.0,
      -1.6130752740605452,
      -1.5751090752809256,
      -1.5262785326988066,
      -1.580662726608922,
      -1.5220150976452547,
      -1.5258789813987441,
      -1.573510889553105,
      -1.6719180095251298
    ],
    [
      -1.3705699362672583,
      -1.3424520754029894,
      -1.3029713701460253,
      -1.3038288837710492,
      -1.3246983934731487,
      -1.2649618735234778,
      -1.340030898157237,
      -1.2306213778670434,
      -1.3204055174236975,
      -1.2407059371385163,
      -1.27734837481954,
      -1.4089487218726073,
      -1.3060740810624571,
      -1.2934698256202914,
      -1.2542918993447807,
      -1.2904063075120529,
      -1.3495126751375772,
      -1.3222672022143582,
      -1.2817472526608988,
      -1.247170488881639,
      -1.2965517564316973,
      0.0,
      -1.2823587512898136,
      -1.2923076245358427,
      -1.3219370728132702,
      -1.2376591962588395,
      -1.307309928379366,
      -1.2961157262998506,
      -1.339320558301972
    ],
    [
      -1.5717002310009323,
      -1.452365632044941,
      -1.4507199595051576,
      -1.4364653815463353,
      -1.4981075182649897,
      -1.4813925617204604,
      -1.514826394526003,
      -1.4263245584455604,
      -1.4928200691353013,
      -1.477463003120499,
      -1.45646625656615,
      -1.6059735529177748,
      -1.4347093456104163,
      -1.4544659666529567,
      -1.466454485448997,
      -1.4900177909022325,
      -1.4898227464839517,
      -1.4493918142071858,
      -1.508811633134222,
      -1.470650648235261,
      -1.4206785433740476,
      -1.486734040881406,
      0.0,
      -1.4462368693857617,
      -1.4268198468395863,
      -1.5196072990783345,
      -1.4993697302722186,
      -1.4808169270764917,
      -1.5505724454400092
    ],
    [
      -1.5955840368028087,
      -1.4829911560731017,
      -1.4946471065143503,
      -1.5235816675594553,
      -1.5565514200155257,
      -1.5549672049295078,
      -1.6024341488871476,
      -1.4901028391496036,
      -1.5598547001653034,
      -1.5224175936731332,
      -1.5525179653587413,
      -1.704306632338291,
      -1.5243587606187328,
      -1.5537271131433306,
      -1.5643312440243606,
      -1.5204833487078178,
      -1.5399034571095065,
      -1.5537477450860235,
      -1.5711485285744837,
      -1.4773970455684882,
      -1.4170055535280324,
      -1.546069685295083,
      -1.5585098510037263,
      0.0,
      -1.5844080858387406,
      -1.447650185114482,
      -1.505405362417487,
      -1.5318192012778757,
      -1.599857672913313
    ],
    [
      -1.429537099965943,
      -1.3193342246230224,
      -1.3222659080741097,
      -1.2996449965802914,
      -1.3002159956785757,
      -1.4216383350360406,
      -1.3648798783927796,
      -1.3357910380053093,
      -1.3038337142663727,
      -1.2830755756928418,
      -1.3705178845283208,
      -1.4706354188671138,
      -1.3487676541916627,
      -1.3223218744754006,
      -1.3412259463442862,
      -1.3843003333113466,
      -1.3942035392417895,
      -1.28379405809673,
      -1.3481673686505724,
      -1.3816550333907682,
      -1.3583846683627032,
      -1.4067797790960859,
      -1.3155002670872318,
      -1.3941813552695912,
      0.0,
      -1.366830060705824,
      -1.3102628969766534,
      -1.414177630021983,
      -1.4655978236641873
    ],
    [
      -1.6186646591321576,
      -1.4780739566568264,
      -1.4455674591680523,
      -1.4831785813782064,
      -1.4770723350833252,
      -1.5205576780146604,
      -1.5377792781633874,
      -1.4192907793221343,
      -1.5181393935509937,
      -1.44228509206749,
      -1.55453008795615,
      -1.6759717121369215,
      -1.525167729596434,
      -1.4649368501765667,
      -1.4255200602196356,
      -1.4626855229426357,
      -1.5843835238296184,
      -1.4723648184666323,
      -1.4855990344404122,
      -1.483918734252498,
      -1.495635059724297,
      -1.4499148710734218,
      -1.5456538805871203,
      -1.5035301775542962,
      -1.5347936393717716,
      0.0,
      -1.5345910128352085,
      -1.556700959312592,
      -1.629660048536735
    ],
    [
      -1.4298719681763405,
      -1.3856575690220758,
      -1.4324223584943343,
      -1.3299810338029487,
      -1.3282526120807086,
      -1.5137017648192095,
      -1.4740613822454585,
      -1.4280224906168149,
      -1.3583596541909984,
      -1.3907965171138084,
      -1.368755844456017,
      -1.5508504337839069,
      -1.448773478669339,
      -1.4046443422149306,
      -1.4377751680468227,
      -1.439901679263734,
      -1.4283145197127298,
      -1.38770610060435,
      -1.4311347658600604,
      -1.3574913582040877,
      -1.3993956669918604,
      -1.4764785775044214,
      -1.4604721370151557,
      -1.434430684224429,
      -1.348351396783758,
      -1.415835845219931,
      0.0,
      -1.4948609571669407,
      -1.5254262424416356
    ],
    [
      -1.4210153986288032,
      -1.3460833993449879,
      -1.3516402822189282,
      -1.3336124288451727,
      -1.4248056783520675,
      -1.376162205455195,
      -1.4013079051044992,
      -1.3764393804223307,
      -1.3861636959049295,
      -1.3718213442704479,
      -1.2775936309778089,
      -1.4818962200450891,
      -1.3734728299656127,
      -1.4046233202612608,
      -1.4020910435575746,
      -1.398307480427328,
      -1.3954287090460165,
      -1.3266512740049237,
      -1.399035852317446,
      -1.3790780204355544,
      -1.3770464960942035,
      -1.3706560499493647,
      -1.3744417385679142,
      -1.3204699510424336,
      -1.4155704860342484,
      -1.3812428581404372,
      -1.366871784556821,
      0.0,
      -1.4299447184500413
    ],
    [
      -1.4750666132767767,
      -1.4607724675991962,
      -1.41901318845807,
      -1.4363092180862027,
      -1.4777016534752048,
      -1.42883712001407,
      -1.3976659899233919,
      -1.4410154149877197,
      -1.4326238994750442,
      -1.417128669815667,
      -1.433041422073229,
      -1.4401660180468379,
      -1.4382083847525058,
      -1.4643278561951223,
      -1.452822220327706,
      -1.466740218300684,
      -1.3789870243156357,
      -1.4138701641750457,
      -1.4520937002227563,
      -1.43999736346124,
      -1.4310259793861762,
      -1.366513560027253,
      -1.4432885185432391,
      -1.418345402233695,
      -1.4315911556277852,
      -1.4582250502949836,
      -1.4163430160620711,
      -1.4110200627061233,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.08265849444927253,
      0.07324978306685281,
      0.08494385564236095,
      0.06409323913254239,
      0.04398363771627656,
      0.06685828374323677,
      0.0591742145142069,
      0.06337187326275684,
      0.06661960294130642,
      0.07984785997241195,
      0.034413390017521195,
      0.05894522679344938,
      0.0747158063390061,
      0.06992112735089284,
      0.06705788242215327,
      0.05512901422057093,
      0.0746322486416271,
      0.04932536307453317,
      0.08084017480610362,
      0.06466511355376214,
      0.05154264751431281,
      0.053974064572151814,
      0.06538313595342515,
      0.06368797947246296,
      0.05196352349054445,
      0.07509758039436987,
      0.05716914307336474,
      0.046695765841847425
    ],
    [
      0.413041475117484,
      0.0,
      0.544038657951166,
      0.585404864094796,
      0.4421378620066072,
      0.35711907417028077,
      0.40029630531097626,
      0.4902674490122718,
      0.48438200099135886,
      0.41058776196457525,
      0.4743503707227501,
      0.24031094747055382,
      0.45988718318341326,
      0.5498096447698169,
      0.47720402024003006,
      0.5339880176655329,
      0.3511508883232026,
      0.513741054465465,
      0.37941837706361237,
      0.4354856534539011,
      0.42220023796824124,
      0.3610579188981926,
      0.35766212916097295,
      0.41163854358751184,
      0.346640513952722,
      0.46049282484239806,
      0.4033594073735651,
      0.40355392329421136,
      0.2795447720174802
    ],
    [
      0.48185368061168,
      0.5774020031861455,
      0.0,
      0.5979181904690711,
      0.5385692768014259,
      0.4392062724437402,
      0.4752486640198581,
      0.48210925105409475,
      0.568110641460174,
      0.5378802109252963,
      0.49723852247640776,
      0.259107740208643,
      0.5099816955487468,
      0.5910638856959929,
      0.5199312746542453,
      0.5694797346131637,
      0.3914228841581249,
      0.545038547304376,
      0.44466383179831115,
      0.4910247733846145,
      0.4664596631612634,
      0.4048514504367495,
      0.4275237849547249,
      0.4645880423028359,
      0.4565829646201902,
      0.5338328725806329,
      0.48135695355670904,
      0.39810336420082204,
      0.32339302094243094
    ],
    [
      0.45163377720947384,
      0.5967437727585423,
      0.5025406335147697,
      0.0,
      0.43470463226962397,
      0.3491870009756797,
      0.43607817442068364,
      0.47467943422602477,
      0.530518893416587,
      0.45719445583683993,
      0.4951211251217742,
      0.2588078666856295,
      0.5191257967719218,
      0.5472574472866409,
      0.5460023672222576,
      0.5388043931587474,
      0.4071411081409595,
      0.5131496370701054,
      0.4821133309439203,
      0.5058294036172553,
      0.441853908210603,
      0.3968813307921242,
      0.3738803462816167,
      0.424431339947132,
      0.37232799276440165,
      0.4414881559202237,
      0.4885229026738893,
      0.39247874718746734,
      0.3196929004662863
    ],
    [
      0.3788977601624275,
      0.4068918403337338,
      0.4226335678323041,
      0.4228616895019415,
      0.0,
      0.3702512650360823,
      0.4046262181736222,
      0.45575055398639996,
      0.435499716357612,
      0.4343286615778499,
      0.3796214570318659,
      0.25477952451487607,
      0.36359048735,
      0.42809292974920443,
      0.4059121081845949,
      0.4029116056182185,
      0.3537832202253959,
      0.3956908980250007,
      0.3789156070137285,
      0.43926009305168345,
      0.3151764451108954,
      0.3501566248173038,
      0.3472220721043331,
      0.37138856657575103,
      0.43058128727919454,
      0.40359782406383715,
      0.4700255173029495,
      0.33681137750935863,
      0.2806318553522522
    ],
    [
      0.3109477410635999,
      0.4045763093835173,
      0.4710869612345656,
      0.41194806075087453,
      0.3983600012358144,
      0.0,
      0.3792023966782412,
      0.4442698051194902,
      0.4290663523458016,
      0.5107495424626767,
      0.4444230416040864,
      0.27219272859227206,
      0.4333571552792852,
      0.4339185040689384,
      0.4506979320423572,
      0.4237087215375419,
      0.38204823186194137,
      0.38634389521938917,
      0.4502500847976536,
      0.48464055190642297,
      0.41501784312653855,
      0.43794991323794963,
      0.3692473816031554,
      0.4265028883650319,
      0.3846247771944158,
      0.4791031950080138,
      0.3603139887688078,
      0.37771580753506595,
      0.32308192207356745
    ],
    [
      0.37733095789470106,
      0.4742262990311352,
      0.4600012414957735,
      0.45755296587838123,
      0.4672014519844305,
      0.34457418502581105,
      0.0,
      0.4096312008073488,
      0.3695054587653963,
      0.41375205905158974,
      0.4031020844921551,
      0.30058894910456635,
      0.37487037125199363,
      0.4151434621830401,
      0.4465209245481283,
      0.4221154993551679,
      0.4076824636078846,
      0.4182615299149861,
      0.35117314595346705,
      0.40756487019793064,
      0.3424581070033521,
      0.38637783152564675,
      0.32238290198589525,
      0.36421664706056034,
      0.33807733832195264,
      0.4365123757080722,
      0.3803113576969934,
      0.3538673912564996,
      0.35839327543076904
    ],
    [
      0.32146354245160524,
      0.5233769153630392,
      0.4846007093844025,
      0.48012170323927283,
      0.4816192080570536,
      0.4115154055518706,
      0.40911743148874735,
      0.0,
      0.44660184201695174,
      0.45724060516535836,
      0.43323043374464465,
      0.27134946649331004,
      0.443348305514742,
      0.48825211401956925,
      0.491371783145756,
      0.5079487681937054,
      0.40847656838535684,
      0.45118712366085445,
      0.428086501740256,
      0.4637975361646791,
      0.44309376937022016,
      0.4306668790189745,
      0.3972016131721048,
      0.44342516968576406,
      0.42627858716791556,
      0.5251790343337124,
      0.4621183474607018,
      0.3927698595512432,
      0.33127771309062637
    ],
    [
      0.28862391694477196,
      0.4702894091606442,
      0.48433056909914174,
      0.5573757624013247,
      0.43331746230412427,
      0.3390424947926356,
      0.34447891209905035,
      0.36595312949434056,
      0.0,
      0.40086596782310324,
      0.4111392011662429,
      0.29366238657986243,
      0.4159672985595837,
      0.5233379081257701,
      0.46835582781929985,
      0.4306648477219257,
      0.3579364239758842,
      0.5201420664018936,
      0.43135121634945794,
      0.4800915178762324,
      0.36805407928711853,
      0.3418778847474899,
      0.35848699568346576,
      0.38057550132292484,
      0.40727482863546927,
      0.38199434495665896,
      0.4813831184035129,
      0.33961621657612073,
      0.25409715585673287
    ],
    [
      0.3416006002694978,
      0.4020677288614931,
      0.38833830554867377,
      0.4253133780007212,
      0.42150688372055134,
      0.33203210335428435,
      0.3256082329722425,
      0.4606329130955584,
      0.38490731322902927,
      0.0,
      0.38735144852179015,
      0.25000024358075223,
      0.3812658534756155,
      0.4052170784067919,
      0.4060299247879162,
      0.38956727903722754,
      0.3429759999592439,
      0.38473076595636946,
      0.4285408266160411,
      0.4188895458632944,
      0.3571437841333829,
      0.4104556167902036,
      0.38558987199055506,
      0.3502670350649255,
      0.42908002496879427,
      0.44509256742283987,
      0.40403382771714424,
      0.35261511959643244,
      0.2923141064490937
    ],
    [
      0.3552405580648432,
      0.42098953778982007,
      0.40328184037474,
      0.43270390997642116,
      0.3333999809473951,
      0.290251622501833,
      0.3354218160189928,
      0.3717255876535288,
      0.3448464842696999,
      0.3812434008058272,
      0.0,
      0.23512423879344335,
      0.41591080202854047,
      0.37141643764763366,
      0.3438225907065402,
      0.3452742625993521,
      0.36013912805238735,
      0.39080073096559853,
      0.4104070876604513,
      0.3508966046016615,
      0.37107739020145525,
      0.36375800345060716,
      0.35443390146180254,
      0.3802814950109701,
      0.321255552918313,
      0.36547001125751066,
      0.38801536983182805,
      0.37682929112612573,
      0.2868532684038938
    ],
    [
      0.24119949699932763,
      0.2853482681604478,
      0.2934395255049711,
      0.2839701881824448,
      0.28931340913073655,
      0.25944924129385183,
      0.26975170976177965,
      0.2789827719875486,
      0.33318982558112853,
      0.3007833453773332,
      0.3085752151238643,
      0.0,
      0.29097921302029595,
      0.270306961234108,
      0.2647195139661205,
      0.2517779572433412,
      0.29763690548891786,
      0.3056103294996524,
      0.25876902183892825,
      0.2833234313905566,
      0.24441187019070854,
      0.32730251764711094,
      0.25438595619654136,
      0.29807531625131545,
      0.2915084545519322,
      0.27296302918629833,
      0.30734376681757336,
      0.27563002291492045,
      0.31710040893345526
    ],
    [
      0.29349633325446134,
      0.4453905085815626,
      0.4292795487543528,
      0.455711559202959,
      0.349736152653487,
      0.35513107301654534,
      0.36780568295210414,
      0.4149960325617785,
      0.4052580475184919,
      0.41352248502192057,
      0.490503145286298,
      0.268813810917242,
      0.0,
      0.4048329593956752,
      0.36404519325244755,
      0.4382314765949742,
      0.36880754745437283,
      0.41951717662424914,
      0.3608574286505335,
      0.3895171298667013,
      0.4325846824228743,
      0.3444665417179342,
      0.40977369666171715,
      0.4091295939415833,
      0.3558480378870905,
      0.42044310674797414,
      0.351617305208197,
      0.37703443397805536,
      0.32942896833976865
    ],
    [
      0.38211516325022377,
      0.5549799002832174,
      0.48644065345212173,
      0.5586108880516767,
      0.446326938903034,
      0.3572419051091862,
      0.40355456868123274,
      0.4724933894773813,
      0.5110072473020464,
      0.4543146294227789,
      0.47516160206565594,
      0.2760529488446497,
      0.44992728256175996,
      0.0,
      0.5175422639741012,
      0.5030641215315483,
      0.3496136258475946,
      0.4904568843722299,
      0.42827195905543625,
      0.4803650970865936,
      0.40466635667562945,
      0.37172332720716406,
      0.36292631638738815,
      0.39009220757645746,
      0.4053951985706694,
      0.44922230797639107,
      0.45222125911944255,
      0.3502816633335031,
      0.2761085314417009
    ],
    [
      0.43304982611692044,
      0.6043753777945355,
      0.5588247420994812,
      0.6225871454004637,
      0.5462190560393754,
      0.4798401466823945,
      0.45537880164201994,
      0.571889489264447,
      0.5213185278916752,
      0.5380837218820385,
      0.4519653731729567,
      0.328179189124268,
      0.4976445632154627,
      0.5980330320937819,
      0.0,
      0.571222615632687,
      0.416451441432069,
      0.5977272779693079,
      0.5422061769931956,
      0.5528178721590975,
      0.46307921701740673,
      0.49724779572333566,
      0.4328477981293668,
      0.44011222330566335,
      0.44258981569187883,
      0.5653510328762641,
      0.49319360564322534,
      0.4664214701907057,
      0.35655945226711183
    ],
    [
      0.40086336328252403,
      0.5889892019714986,
      0.5518309547981282,
      0.5420785703853344,
      0.48433633292585343,
      0.39354699307298024,
      0.39804814304125435,
      0.5683446734434687,
      0.5307352798762446,
      0.4746354034347564,
      0.45126615771959266,
      0.2891480138492981,
      0.5057524551040093,
      0.5148271037406211,
      0.54158703181332,
      0.0,
      0.3700957781467238,
      0.47935884468680623,
      0.4452430046240696,
      0.5541713637053645,
      0.493412429197563,
      0.4136310458067849,
      0.4232780813353494,
      0.4722043404567784,
      0.42969251642645845,
      0.5151338944736141,
      0.4433970235553113,
      0.3867719393004809,
      0.34160766023484834
    ],
    [
      0.3802527460490317,
      0.4262420724529834,
      0.4384143209378637,
      0.4280825918538891,
      0.34421938479732206,
      0.3756830470760477,
      0.38685533623306134,
      0.41218897080406425,
      0.37974919080093716,
      0.42870668221436237,
      0.44537565707258,
      0.28896546865298833,
      0.3865773599588742,
      0.4240379072793943,
      0.40880055979886887,
      0.41653441866069363,
      0.0,
      0.34529858058776997,
      0.4783707964788402,
      0.4154480293921383,
      0.42808969677328434,
      0.3924301371850647,
      0.37906282630658095,
      0.36674533746488436,
      0.3557032403760503,
      0.4133899068487146,
      0.38623181898352965,
      0.3661074237737474,
      0.3574382885767766
    ],
    [
      0.3308789947894364,
      0.5159031128376428,
      0.4936770363840215,
      0.54165907301763,
      0.3694835240619925,
      0.3267429821950547,
      0.3691746568345724,
      0.43929436775711417,
      0.5157282990624255,
      0.41193276883562,
      0.4047661935867046,
      0.2804897134324018,
      0.43198533188378874,
      0.44191905606227344,
      0.4192371772328085,
      0.43393030111456343,
      0.29365185542045125,
      0.0,
      0.36948339688314813,
      0.39023183986089394,
      0.38082303840213205,
      0.3490734275791034,
      0.385031224142683,
      0.35886070564626427,
      0.4193449378586205,
      0.4539510680769647,
      0.41686292334170116,
      0.3608280459998141,
      0.2668760440365827
    ],
    [
      0.3030902144905667,
      0.4513963333020339,
      0.4285692664844474,
      0.515412197250642,
      0.4174850792665894,
      0.3981218004700571,
      0.3300770853351267,
      0.42969077242645426,
      0.42976950799288693,
      0.5107075738900249,
      0.455182753219582,
      0.25201256299686015,
      0.43376908626733024,
      0.4535413584770871,
      0.48322295031122264,
      0.4550803887326218,
      0.40911588271126087,
      0.3962816684948449,
      0.0,
      0.48662350282015976,
      0.43062851531978885,
      0.419595216339572,
      0.3845072553522142,
      0.35251484624932594,
      0.3801990808923399,
      0.42263922624375927,
      0.4409293037478208,
      0.3779089832518532,
      0.3127081822638804
    ],
    [
      0.3947668409516587,
      0.44779195764708946,
      0.44378714079451265,
      0.46862090083445285,
      0.4252865740010272,
      0.3539164233346406,
      0.37102748620232084,
      0.4443258901287255,
      0.434818955996223,
      0.39385344627731067,
      0.39471957619517495,
      0.2324152050961199,
      0.3856248652384462,
      0.45751021912207857,
      0.47016103755094973,
      0.48590392399357873,
      0.36960762491652743,
      0.42861458170619104,
      0.395754847242173,
      0.0,
      0.3690642402503912,
      0.3827835002456914,
      0.34140048504778986,
      0.36691662216841525,
      0.3605670413581177,
      0.43286168091334964,
      0.4066944316330703,
      0.3140036911015329,
      0.282510454513903
    ],
    [
      0.3183475796867188,
      0.49535568815302633,
      0.46510566474813175,
      0.49561744585340395,
      0.35714002119209187,
      0.38209410472828487,
      0.3739030917003041,
      0.45679399084315464,
      0.4159857122632318,
      0.405630819377538,
      0.44189247905458284,
      0.2718632236536398,
      0.4456553783077355,
      0.4538407873237489,
      0.4195560334985027,
      0.45448345557038317,
      0.4000197329173538,
      0.43011131367123734,
      0.43349169264930576,
      0.4201062219743321,
      0.0,
      0.3673318462302533,
      0.4052980450098729,
      0.4541285875919918,
      0.39974439368187653,
      0.4583920226455438,
      0.45452813889205435,
      0.40689623073769354,
      0.30848911076566865
    ],
    [
      0.3096567622123043,
      0.33777462307657324,
      0.37725532833353737,
      0.3763978147085134,
      0.35552830500641397,
      0.41526482495608485,
      0.34019580032232555,
      0.44960532061251923,
      0.35982118105586514,
      0.43952076134104634,
      0.40287832366002263,
      0.2712779766069553,
      0.3741526174171055,
      0.3867568728592712,
      0.42593479913478194,
      0.38982039096750976,
      0.3307140233419854,
      0.3579594962652044,
      0.3984794458186638,
      0.43305620959792357,
      0.38367494204786534,
      0.0,
      0.39786794718974905,
      0.38791907394371994,
      0.3582896256662924,
      0.44256750222072316,
      0.3729167701001965,
      0.384110972179712,
      0.3409061401775906
    ],
    [
      0.33414242532689475,
      0.45347702428288605,
      0.45512269682266937,
      0.4693772747814917,
      0.40773513806283734,
      0.4244500946073666,
      0.391016261801824,
      0.4795180978822666,
      0.4130225871925257,
      0.42837965320732807,
      0.4493763997616771,
      0.29986910341005224,
      0.4711333107174107,
      0.4513766896748703,
      0.4393881708788301,
      0.4158248654255945,
      0.41601990984387527,
      0.45645084212064124,
      0.39703102319360495,
      0.435192008092566,
      0.48516411295377937,
      0.419108615446421,
      0.0,
      0.45960578694206533,
      0.4790228094882407,
      0.38623535724949254,
      0.40647292605560836,
      0.4250257292513353,
      0.35527021088781785
    ],
    [
      0.39437111396538693,
      0.5069639946950939,
      0.49530804425384534,
      0.4663734832087403,
      0.4334037307526699,
      0.43498794583868783,
      0.387521001881048,
      0.499852311618592,
      0.43010045060289226,
      0.4675375570950624,
      0.4374371854094543,
      0.2856485184299047,
      0.46559639014946286,
      0.43622803762486506,
      0.425623906743835,
      0.46947180206037786,
      0.45005169365868913,
      0.4362074056821721,
      0.4188066221937119,
      0.5125581051997075,
      0.5729495972401633,
      0.4438854654731126,
      0.4314452997644693,
      0.0,
      0.405547064929455,
      0.5423049656537136,
      0.4845497883507086,
      0.4581359494903199,
      0.3900974778548827
    ],
    [
      0.30489400729857885,
      0.41509688264149935,
      0.41216519919041206,
      0.43478611068423034,
      0.43421511158594606,
      0.3127927722284811,
      0.3695512288717422,
      0.39864006925921247,
      0.43059739299814903,
      0.45135553157167996,
      0.36391322273620097,
      0.26379568839740797,
      0.3856634530728591,
      0.41210923278912115,
      0.3932051609202356,
      0.35013077395317516,
      0.34022756802273224,
      0.4506370491677918,
      0.38626373861394936,
      0.35277607387375354,
      0.37604643890181855,
      0.3276513281684359,
      0.41893084017729,
      0.34024975199493057,
      0.0,
      0.36760104655869785,
      0.42416821028786833,
      0.3202534772425387,
      0.2688332836003344
    ],
    [
      0.3478207889510556,
      0.48841149142638685,
      0.520917988915161,
      0.4833068667050069,
      0.489413112999888,
      0.4459277700685529,
      0.4287061699198258,
      0.5471946687610789,
      0.44834605453221954,
      0.5242003560157233,
      0.41195536012706313,
      0.2905137359462917,
      0.44131771848677914,
      0.5015485979066465,
      0.5409653878635776,
      0.5037999251405776,
      0.3821019242535948,
      0.494120629616581,
      0.48088641364280105,
      0.48256671383071525,
      0.47085038835891635,
      0.5165705770097915,
      0.42083156749609296,
      0.46295527052891705,
      0.43169180871144164,
      0.0,
      0.43189443524800475,
      0.40978448877062124,
      0.3368253995464783
    ],
    [
      0.4297621071794846,
      0.4739765063337493,
      0.42721171686149084,
      0.5296530415528764,
      0.5313814632751166,
      0.34593231053661566,
      0.3855726931103667,
      0.4316115847390103,
      0.5012744211648268,
      0.4688375582420168,
      0.4908782308998081,
      0.3087836415719183,
      0.4108605966864862,
      0.4549897331408945,
      0.42185890730900244,
      0.4197323960920911,
      0.4313195556430953,
      0.4719279747514751,
      0.42849930949576476,
      0.5021427171517374,
      0.4602384083639648,
      0.3831554978514038,
      0.3991619383406695,
      0.42520339113139616,
      0.5112826785720672,
      0.44379823013589403,
      0.0,
      0.36477311818888447,
      0.33420783291418954
    ],
    [
      0.36192864483684595,
      0.4368606441206613,
      0.431303761246721,
      0.44933161462047644,
      0.35813836511358166,
      0.4067818380104542,
      0.38163613836114996,
      0.40650466304331845,
      0.39678034756071967,
      0.4111226991952013,
      0.5053504124878403,
      0.30104782342056,
      0.40947121350003646,
      0.37832072320438836,
      0.38085299990807453,
      0.38463656303832106,
      0.3875153344196327,
      0.4562927694607255,
      0.3839081911482032,
      0.4038660230300948,
      0.4058975473714457,
      0.4122879935162844,
      0.408502304897735,
      0.4624740924232156,
      0.36737355743140077,
      0.40170118532521193,
      0.41607225890882815,
      0.0,
      0.3529993250156078
    ],
    [
      0.3178895217318858,
      0.33218366740946625,
      0.37394294655059257,
      0.35664691692245976,
      0.3152544815334577,
      0.36411901499459254,
      0.3952901450852706,
      0.3519407200209428,
      0.36033223553361826,
      0.37582746519299537,
      0.3599147129354334,
      0.3527901169618246,
      0.3547477502561567,
      0.3286282788135402,
      0.34013391468095655,
      0.32621591670797856,
      0.4139691106930268,
      0.3790859708336167,
      0.34086243478590617,
      0.3529587715474225,
      0.3619301556224863,
      0.42644257498140936,
      0.3496676164654233,
      0.3746107327749675,
      0.3613649793808773,
      0.33473108471367885,
      0.37661311894659133,
      0.3819360723025391,
      0.0
    ]
  ],
  "row_avgs": [
    0.06357000114190439,
    0.42817042425261026,
    0.48121225705608817,
    0.4535068169603993,
    0.38713895620865774,
    0.41054806193203636,
    0.3929784409476296,
    0.43766846202258713,
    0.4046530872915994,
    0.38225601354965955,
    0.3607453894686149,
    0.28413741690982897,
    0.38806355923090546,
    0.43072065134145765,
    0.501614885266112,
    0.46442812858603705,
    0.3948214927639409,
    0.3989936105834432,
    0.41288502123572685,
    0.39483248730219506,
    0.4139929575972023,
    0.37858228038644487,
    0.4251360401914991,
    0.44939160392217936,
    0.3752339516003241,
    0.4548366289564211,
    0.435286698615582,
    0.40210567980774053,
    0.35928680101353994
  ],
  "col_avgs": [
    0.3571128550058354,
    0.45070498448170354,
    0.43988210020124463,
    0.461941716541852,
    0.40605450642003543,
    0.36104240534958476,
    0.3707858013093922,
    0.43100219012836943,
    0.42409449432290985,
    0.42747909736247003,
    0.41951919804887927,
    0.2690001508340647,
    0.4113253129143319,
    0.43560831317981324,
    0.42437874605498765,
    0.42504936801366977,
    0.3658859087543876,
    0.4281920461834344,
    0.39933681701141677,
    0.42878720841084067,
    0.3989539992227519,
    0.383223696762801,
    0.36973300935256104,
    0.389446294473883,
    0.3832706103132372,
    0.42314333490824035,
    0.4092944805721501,
    0.36455121260410595,
    0.3079979474034135
  ],
  "combined_avgs": [
    0.2103414280738699,
    0.4394377043671569,
    0.4605471786286664,
    0.45772426675112565,
    0.3965967313143466,
    0.3857952336408106,
    0.38188212112851094,
    0.4343353260754783,
    0.4143737908072546,
    0.4048675554560648,
    0.3901322937587471,
    0.27656878387194683,
    0.3996944360726187,
    0.4331644822606354,
    0.46299681566054984,
    0.4447387482998534,
    0.38035370075916425,
    0.4135928283834388,
    0.4061109191235718,
    0.41180984785651786,
    0.40647347840997705,
    0.3809029885746229,
    0.39743452477203006,
    0.4194189491980312,
    0.37925228095678065,
    0.43898998193233074,
    0.42229058959386606,
    0.3833284462059232,
    0.33364237420847676
  ],
  "gppm": [
    595.6004319407838,
    675.0756272582012,
    680.240071245944,
    671.1000956682782,
    693.2874505941892,
    717.7928052009214,
    709.9208761283286,
    685.3204041822427,
    690.0285964804683,
    684.707793201338,
    690.6373462272061,
    764.0589795689356,
    696.5058525041452,
    684.7322712954102,
    688.919567284532,
    688.3502633654,
    712.3307202518417,
    686.9338486970245,
    698.1988326040555,
    683.1195102358979,
    696.8146459154366,
    710.1904036540558,
    713.8841939653734,
    702.413422451916,
    707.2770782971103,
    687.9231309234294,
    693.099701947515,
    714.9854816618324,
    741.0367522974733
  ],
  "gppm_normalized": [
    1.3955222675782364,
    1.5116458061394458,
    1.5213035504702188,
    1.5044942594399966,
    1.543616507263261,
    1.6029416981221138,
    1.588503120111188,
    1.5263679713814322,
    1.5460423159076566,
    1.5298816593298312,
    1.541073692282764,
    1.7081037592919734,
    1.5590766007877555,
    1.5327924664297883,
    1.5433259815526024,
    1.539670503130083,
    1.5883679509397004,
    1.5333118117843605,
    1.5618387383048717,
    1.5276802985582754,
    1.5526272857869419,
    1.589614191340681,
    1.5941940640313315,
    1.5650819715893625,
    1.5804975850308967,
    1.5363233052572396,
    1.540825687633268,
    1.5979581829283924,
    1.6564972155984417
  ],
  "token_counts": [
    548,
    467,
    449,
    492,
    391,
    412,
    438,
    397,
    480,
    439,
    411,
    388,
    456,
    452,
    464,
    438,
    403,
    409,
    446,
    444,
    401,
    457,
    413,
    395,
    434,
    435,
    372,
    423,
    413,
    819,
    418,
    459,
    508,
    546,
    421,
    391,
    416,
    442,
    444,
    460,
    526,
    464,
    390,
    413,
    420,
    381,
    431,
    456,
    441,
    417,
    437,
    387,
    449,
    412,
    382,
    431,
    441,
    393,
    1833,
    487,
    412,
    448,
    441,
    422,
    437,
    449,
    434,
    443,
    455,
    399,
    423,
    469,
    417,
    417,
    413,
    427,
    462,
    486,
    420,
    386,
    363,
    387,
    417,
    398,
    419,
    415,
    390
  ],
  "response_lengths": [
    8873,
    2777,
    2363,
    2587,
    2527,
    2480,
    2567,
    2610,
    2509,
    2619,
    2694,
    2298,
    2432,
    2716,
    2430,
    2469,
    2395,
    2489,
    2812,
    2783,
    2392,
    2283,
    2118,
    2205,
    2338,
    2234,
    2477,
    2500,
    2284
  ]
}