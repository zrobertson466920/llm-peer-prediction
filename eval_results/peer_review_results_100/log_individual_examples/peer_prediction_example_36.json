{
  "example_idx": 36,
  "reference": "Under review as a conference paper at ICLR 2023\n\nLASER: LATENT SET REPRESENTATIONS FOR 3D GENERATIVE MODELING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNeural Radiance Field (NeRF) provides unparalleled fidelity of novel view synthesis—rendering a 3D scene from an arbitrary viewpoint. NeRF requires training on a large number of views that fully cover a scene, which limits its applicability. While these issues can be addressed by learning a prior over scenes in various forms, previous approaches have been either applied to overly simple scenes or struggling to render unobserved parts. We introduce Latent Set Representations for NeRF-VAE (LASER-NV)—a generative model which achieves high modelling capacity, and which is based on a set-valued latent representation modelled by normalizing flows. Similarly to previous amortized approaches, LASER-NV learns structure from multiple scenes and is capable of fast, feed-forward inference from few views. To encourage higher rendering fidelity and consistency with observed views, LASER-NV further incorporates a geometry-informed attention mechanism over the observed views. LASER-NV further produces diverse and plausible completions of occluded parts of a scene while remaining consistent with observations. LASER-NV shows state-of-the-art novel-view synthesis quality when evaluated on ShapeNet and on a novel simulated City dataset, which features high uncertainty in the unobserved regions of the scene.\n\n1\n\nINTRODUCTION\n\nProbabilistic scene modelling aims to learn stochastic models for the structure of 3D scenes, which are typically only partially observed (Eslami et al., 2018; Kosiorek et al., 2021; Burgess et al., 2019). Such models need to reason about unobserved parts of a scene in way that is consistent with the observations and the data distribution. Scenes are usually represented as latent variables, which are ideally compact and concise, yet expressive enough to describe complex data.\n\nSuch 3D scenes can be thought of as projections of light rays onto an image plane. Neural Radiance Field (NeRF, (Mildenhall et al., 2020)) exploits this structure explicitly. It represents a scene as a radiance field (a.k.a. a scene function), which maps points in space (with the corresponding camera viewing direction) to color and mass density values. We can use volumetric rendering to project these radiance fields onto any camera plane, thus obtaining an image. Unlike directly predicting images with a CNN, this rendering process respects 3D geometry principles. NeRF represents scenes as parameters of an MLP, and is trained to minimize the reconstruction error of observations from a single scene—resulting in unprecedented quality of novel view synthesis.\n\nFor generative modelling, perhaps the most valuable property of NeRF is the notion of 3D geometry embedded in the rendering process, which does not need to be learned, and which promises strong generalisation to camera poses outside the training distribution. However, since NeRF’s scene representations are high dimensional MLP parameters, they are not easily amenable to generative modelling (Dupont et al., 2022). NeRF-VAE (Kosiorek et al., 2021) embeds NeRF in a generative model by conditioning the scene function on a latent vector that is inferred from a set of ‘context views’. It then uses NeRF’s rendering mechanism to generate outputs. While NeRF-VAE admits efficient inference of a compact latent representation, its outputs lack visual fidelity. This is not surprising, given its simple latent structure, and the inability to directly incorporate observed features. In addition, NeRF-VAE does not produce varied samples of unobserved parts of a scene.\n\nA number of recent deterministic methods (Yu et al., 2021; Trevithick & Yang, 2021; Wang et al., 2021) uses local image features to directly condition radiance fields in 3D. This greatly improves\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Left: LASER-NV infers a set-valued latent Z from the context V that consists of N image and camera pairs (In, Cn). On querying the scene function at a point xi with direction di, the latents are combined with local features Hn that are back-projected from the context views—producing color and density. Right: Rendering a novel viewpoint may include observed (green, an example query point on the left) and unobserved parts (red) of the scene. Conditioned on the context V, LASER-NV allows sampling multiple scene completions that are consistent with the context views (green arrows) while providing varied explanations for the unobserved parts of the scene (red arrows). We show two such samples for the same target camera Ct. Also see the gif in supp. material showing a fly-through for four prior samples conditioned on the same views.\n\nreconstruction quality of observed parts of the scene. However, these methods are still unable to produce plausible multimodal predictions for the unobserved parts of a scene.\n\nIn this work, we address NeRF-VAE’s shortcomings by proposing Latent Set Representations for NeRFVAE (LASER-NV). To increase modelling capacity, LASER-NV uses an arbitrarily-sized set of latent variables (instead of just one vector) modelled with normalizing flows. To further enable producing samples which are consistent with observed parts, we make the generative model conditional on a set of context views (as opposed to conditioning only the approximate posterior). LASER-NV offers superior visual quality with the ability to synthesise multiple varied novel views compatible with observations. Figure 1 shows LASER-NV’s key components and abilities. We include a gif in supp. material showing fly-throughs of additional prior samples, see Section 4.5 for details.\n\nOur contributions are as follows:\n\n• We introduce a novel set-valued latent representation modelled by purpose-built permutationinvariant normalizing flows conditioned on context views. We show that increasing the number of latent set elements improves modelling performance, providing a simple way to trade off computation for quality without adding new model parameters. We also verify that increasing latent dimensionality in NeRF-VAE offers no such benefits. In contrast with deterministic scene models in the literature, our probabilistic treatment over the latent set allows covering multiple models when predicting novel views.\n\n• We develop a novel attention mechanism to condition the scene function on the set-valued latent as well as additional local features computed from context views. We show that including local features further improves visual quality.\n\n• We evaluate LASER-NV on three datasets: a category-agnostic ShapeNet dataset, MultiShapeNet, and on a novel “City” dataset that contains a large simulated urban area and poses significant challenges as a benchmark for novel view synthesis due to high uncertainty in the unobserved parts of the scene. Our model overcomes some of the main limitations of NeRF-VAE and also outperforms deterministic NeRF models on novel view synthesis in the face of uncertainty.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2 BACKGROUND: NERF & NERF-VAE\n\nNeural Radiance Field (NeRF, (Mildenhall et al., 2020)) represents a 3D scene as a scene function F (x, d) of a point coordinate x ∈ R3 and direction d ∈ R3, and outputs the colour c ∈ R3 and volume density σ ≥ 0. This function is parameterized using a neural network (typically a fully connected MLP). To obtain a pixel colour, the scene function is evaluated at many points along the corresponding ray in the volumetric rendering process.\n\nThis volumetric rendering integral is in practice approximated using numerical integration; see (Mildenhall et al., 2020; Blinn, 1982) for details. We use ˆI = render(F (·), C) to denote the image rendering process that outputs the image ˆI ∈ RH×W ×3 for the rays of a camera C and a scene function F . The camera C = (K, R, t) is specified by its intrinsic parameters K ∈ R3×3 and extrinsic parameters given by a rotation R ∈ R3×3 and translation t ∈ R3. In its original formulation, the parameters of NeRF are optimized by minimizing an error function (typically the mean-squared error) between rendered images and the ground-truth images across many views of a single scene.\n\nWhile NeRF learns high-fidelity scene representations when many views are available, it does not learn a prior over scenes, does not provide compact scene representations, and does not admit efficient inference (i.e. fast estimation of the scene parameters from a few input views). To address these issues, NeRF-VAE (Kosiorek et al., 2021) embeds NeRF as a decoder in a variational auto-encoder (VAE, (Kingma & Welling, 2014; Rezende et al., 2014)). Each scene is represented by a latent vector z (instead of the set of parameters of an MLP as in NeRF) which can be either sampled from a Gaussian prior p(z) or from the approximate posterior q(z | C) inferred from a set of context views and associated cameras C. The latent z is used to condition an MLP that parameterizes the scene function, Fθ(·, z) : (xi, di) (cid:55)→ ci, σi. Here, parameters θ are shared between scenes, while z is scene specific. NeRF-VAE uses a Gaussian likelihood p(I | z, C) with the image rendered from the sampled scene function as mean, and a fixed standard deviation.\n\n3 LATENT SET REPRESENTATIONS FOR NeRF-VAE\n\nLASER-NV is a conditional generative model, conditioned on a set of input views V := {I n, Cn}N n=1. Given these views the model infers a latent representation that can be used for novel view synthesis.\n\nIn order to represent large-scale and complex scenes we use Latent Set Representations (LASERs), a set of latents of the form Z := {zk}K k=1, where K is a hyperparameter. In the NeRF-VAE, the scene function MLP is directly conditioned on the inferred latent. In contrast, LASER-NV’s scene function is conditioned on the latent set Z and features H := {H n}N n=1 extracted from N input views, and integrates information from both. This results in the form Fθ(·, Z, H) : (xi, di) (cid:55)→ ci, σi, which we detail further in Section 3.2.\n\nWe obtain the input features H by separately encoding the input views using a convolutional neural net (CNN) that maintains spatial structure. These feature maps are subsequently used in the conditional prior, the posterior, and the resulting scene function, all of which we describe next. Further architectural details are provided in Appendix A.\n\n3.1 CONDITIONAL PRIOR AND POSTERIOR\n\nGiven a set of context views V := {I n, Cn}N n=1 consisting of images I n and corresponding cameras Cn, LASER-NV defines a conditional prior p(Z|V) over a latent set of size K, parameterized by a permutation-invariant normalizing flow— a distribution model that allows for both fast and exact sampling and efficient density evaluation (Rezende & Mohamed, 2015; Papamakarios et al., 2021).\n\nFlows are defined by an invertible mapping f of random variables, starting from a base distribution (cid:0)Z (0)(cid:1). When f is composed of multiple invertible mappings f1, . . . , fm, the resulting density is p0 given by\n\np(Z | V) = p0\n\n(cid:0)f −1(Z, V)(cid:1)\n\ndet\n\nδfm\n\n(cid:0)Z (m−1), V(cid:1) δZ (m−1)\n\n(cid:12) −1 (cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\n(1)\n\nM (cid:89)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) m=1\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nOur flow design closely follows that of Wirnsberger et al. (2020; 2021), who parameterize the transformations in each layer using transformers, preserving permutation invariance. Specifically, we propose a novel design to condition the flow distribution on the features H extracted from the context views. To achieve that, we use split coupling layers (Dinh et al., 2017), where for each layer, we first apply self-attention to the set of latents, followed by cross-attention to attend the context features. The output of the second transformer defines the parameters for an invertible affine transformation of the random variables. Implementation details of our normalizing flow is in Appendix A.1.\n\nSince LASER-NV is trained as a VAE, we define an approximate posterior q(Z|U ∪ V), where U = {I t, Ct}M t=1 are target views that the model is trained to reconstruct, denoted as posterior context. The posterior and conditional prior are both distributions over the latent space, where the posterior is conditioned on the union of prior and posterior context. We model the posterior distribution using the same architecture as the conditional prior, but with a separate set of learned parameters.\n\n3.2 DECODER & SCENE FUNCTION\n\nWhen evaluating the scene function, i. e. computing colour and density for a point xi, di, LASER-NV has access to both the latent set Z as well as the features H extracted from the context views V. We now describe how to integrate those two, which consists of querying the latent set for each point, and using 3D projection to extract relevant features.\n\nQuerying the Latent Set While the latent set representation Z contains information regarding the entire scene, when querying for a specific point xi, we would like to extract only relevant information from Z. We propose using a transformer cross-attention model to compute a single feature vector from Z. In the first layer, queries are computed from the positional encoding of xi, and keys and values are computed from the elements of Z. In subsequent layers, queries are computed from previous layers’ output. We denote the output of the latent querying as latent features ̃zi, each corresponding to a query point xi.\n\nLocal Features When synthesizing a novel view given a few input views, a number of existing NeRF-based methods uses 3D projection to determine which features from the context views are useful for explaining a particular 3D point (Yu et al., 2021; Trevithick & Yang, 2021; Wang et al., 2021). LASER-NV incorporates this idea, and in particular follows the design of pixelNeRF (Yu et al., 2021) as it is shown to achieve strong results in a wide range of domains.\n\nn=1. Given a query 3D point xi and direction di, we obtain the point pn\n\nWe use the encoded input views {H n}N n=1 described above, along with their corresponding camera i ∈ R2 in the matrices {Cn}N image space of the nth context view using the known intrinsic parameters. The corresponding local features are then bilinearly interpolated hn i = H n[pn i ]. We refer to hi as local features since they are spatially localized based on the projected coordinates, as opposed to being a global aggregation of all context features. We finally obtain ˆhn i for each context view by separately processing the features hn i using a residual MLP that has as an additional conditioning on the image-space projected points and directions.\n\nIntegrating Latents and Local Features LASER-NV integrates the both local and the latent features. It does so with a transformer:\n\nIn contrast to pixelNeRF, which does not have latents,\n\nfi = Mθ( ̃zi;\n\nN (cid:91)\n\n{hn\n\ni : is_visible(xi, Cn)} ∪ { ̃zi})\n\n(2)\n\nn=1\n\nThe attention queries are computed from latent features ̃zi (see above), and attend to the combined set of local features and to ̃zi. Note that, for a point xi, we use only those context views from which this point is visible. Using a transformer to process encoded features from multiple views with epipolar constraints has been proposed in prior work on Multi-view Stereo, see e.g. (He et al., 2020; Xi et al., 2022; Wang et al., 2022; Ding et al., 2022).\n\nThe final step in evaluating the scene function is to use fi to condition a residual MLP similar to NeRF-VAE resulting in LASER-NV’s scene function Fθ(·, Z, H) : (xi, di) (cid:55)→ ci, σi which is used to volume-render image outputs.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.3 LOSS\n\nSimilarly to NeRF-VAE, our objective function is given by the evidence lower-bound:\n\nL(U, V) = EZ∼q\n\nlog p(cid:0)I t (cid:12)\n\n(cid:12) Ct, Z, V(cid:1)\n\n(cid:35)\n\n(cid:34) M (cid:88)\n\nt=1\n\n− KL(q(Z | U ∪ V) || p(Z | V))\n\n(3)\n\nwhere I t and Ct are the images and cameras from the posterior context views U. We use a Gaussian likelihood term for p(I t | Ct, Z, V) with a fixed variance.\n\n4 EXPERIMENTS\n\n4.1 DATASETS\n\nWe first evaluate LASER-NV on a novel dataset based on a set of synthetic, procedurally generated, environments consisting of urban outdoor scenes. This dataset contains posed images of a virtual city with objects of multiple scales, such as house blocks, roads, buildings, street lights, etc (but no transient objects such as cars or people). As such, it is an extremely challenging scene representation benchmark, especially when representations are inferred from few views only—resulting in partial observability and high uncertainty. In this situation, when evaluated on unobserved parts of a scene, we want a model to produce predictions that are varied, self-consistent, and aligned with the data distribution. Each scene consists of a neighborhood block, and a randomly chosen designated point of interest (road intersection or park) is selected within. Images are then rendered by placing the camera at random nearby positions and viewpoints from the point of interest. The dataset has 100K training scenes, and 3200 test scenes. We provide further details and dataset samples in Appendix B.\n\nTo test LASER-NV’s ability to produce near-deterministic scene completions in a low-uncertainty setting, we compare it against a number of previous methods on the ShapeNet NMR dataset (Kato et al., 2018), which consists of several views of single objects from 13 different ShapeNet categories. Finally, we evaluate on the more challenging MultiShapeNet-Hard (MSN-Hard) (Sajjadi et al., 2021), which tests the models capabilities in the presence of a large number of cluttered objects of varying size with detailed textures and realistic backgrounds.\n\n4.2 MODELS & EVALUATION\n\nOn the City dataset, we compare LASER-NV with Multi-view Conditional NeRF (MVC-NeRF) and NeRF-VAE; see Figure 7 in Appendix A for model diagrams. MVC-NeRF (Figure 7c) is a variant of LASER-NV (Figure 7a) which does not use LASER-NV’s latent set representations but instead only relies on deterministically rendering new views using local features. It therefore closely resembles the design of pixelNeRF but uses the same architecture for the image encoder and scene function Fθ as LASER-NV to ensure they are comparable. On ShapeNet, we further show quantitative results of a number of non-NeRF novel view synthesis methods: Scene Representation Networks (SRN, Sitzmann et al. (2019)), Differentiable Volumetric Rendering (DVR, Niemeyer et al. (2020)), and Scene Representation Transformers (SRT, Sajjadi et al. (2021)). During training, LASER-NV uses a random subset of 4 of the 23 target views as posterior context views1. MVC-NeRF and NeRF-VAE are trained to directly predict the 23 views given a single context view. At test time, LASER-NV samples the latents from its conditional prior whereas NeRF-VAE uses its posterior (given that its prior is unconditional). Importantly, all models in this evaluation use one input view and predict 23 target views.\n\nAt evaluation time, we feed each model with a set of context views, and evaluate reconstructions of those same views, and predictions of novel views of the same scene. We evaluate reconstruction ability via likelihood estimates, and standard image similarity metrics PSNR, and SSIM (Wang et al., 2004). For novel view predictions we report the Fréchet Inception Distance (FID, Heusel et al. (2017)) between the marginal distribution of samples and the evaluation dataset2. This metric\n\n1While using the 23 views as posterior context would be the standard set up (Bayer et al., 2021), we find that\n\nusing 4 randomly-sampled views is much cheaper computationally and yet sufficient for training.\n\n2Image similarity metrics such as PSNR and SSIM are not useful when there exists a large multimodal space\n\nof possible predictions as is the case in the City.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nhighlights the incapability of deterministic models to generate varied and plausible samples in face of uncertainty. Note that since NeRF-VAE is unconditional, we use the learned approximate posterior as the conditional distribution. In the City, we thus train a separate model (one to use for reconstruction metrics that has 8 posterior context views and another one with 2 views for predictions). For ShapeNet results all models use one context view.\n\nOn MSN-Hard, we train LASER-NV with 5 prior context views and 3 target views. We report the test PSNR of 1 novel view given 5 input views, and compare with the results reported in Sajjadi et al. (2022). The methods of comparison include two amortized non-NeRF models based on light fields, Scene Representation Transformers (SRT) and Object SRT (OSRT Sajjadi et al. (2022)); and two amortized NeRF methods, pixelNeRF and Volumetric OSRT (VOSRT).\n\n4.3 ABLATIONS\n\nWe further ablate LASER-NV to isolate contributions of individual model components. To quantify the effects of conditioning the prior we compare LASER-NV to a conditional version of NeRF-VAE. Note that the original NeRF-VAE used an unconditional prior (Kosiorek et al., 2021). We also compare LASER-NV to a version that does not condition the scene function with local context features, cf. Equation (2) and Figure 7b in Appendix A, denoted by LASER-NV (NO GEOM.). LASER-NV, LASER-NV (NO GEOM.), NeRF-VAE variants, and MVC-NeRF share architectures for the image encoder and the scene function Fθ.\n\n4.4 TRAINING AND IMPLEMENTATION DETAILS\n\nWe use the hierarchical sampling technique of (Mildenhall et al., 2020) when rendering a pixel colour, which involves learning two separate sets of scene function parameters as well as optimizing a corresponding colour likelihood term for each step. For experiments with the City, we leverage ground-truth depth maps using the method proposed by Stelzner et al. (2021) in order to train all the models with fewer scene function evaluations per ray. Not using ground-truth depth significantly increases training time while only marginally lowering reconstruction PSNR, see Appendix D for a discussion and Fig. 16 in Appendix B for visualisations. Importantly, when evaluating our models we revert to volumetric rendering. All generative models are trained by optimizing the evidence lower bound; where we anneal the KL term in Eq. (3). All experiments use the Adam (Kingma & Ba, 2014) optimizer. We provide full details of model architectures in Appendix A and training procedures in Appendix E.\n\n4.5 RESULTS\n\nResults on the City dataset are shown in Table 1. In terms of reconstructing observed input views, both LASER-NV and MVC-NeRF, due to the use of explicit geometrical knowledge, result in excellent reconstructions. NeRF-VAE, however, needs to reconstruct input views purely from its latent representation, resulting in low fidelity reconstructions. For prediction of unseen views, the output distribution of LASER-NV is closer to the actual evaluation data (lower FID), in contrast to MVC-NeRF which outputs a single prediction, and NeRF-VAE which produces low quality outputs that do not vary much. We show representative example reconstructions and predictions in Fig. 2. We also include a gif3 in supp. material showing fly-throughs of model samples.\n\nResults on ShapeNet are shown in Table 2. Both MVC-NeRF and LASER-NV outperform existing published methods when predicting novel views of these single-object scenes. We further observe low quality predictions of NeRF-VAE in example predictions in Fig. 11. Finally, Fig. 12 shows a hand-selected example where the input view has ambiguity and how LASER-NV can sample plausible variations. As in the City experiment, LASER-NV produces plausibly varied predictions of the object in contrast to the other methods.\n\nResults on MSN-Hard are shown in Table 3. LASER-NV clearly outpeforms the NeRF-based models, and improves over OSRT and the original SRT. Only a modified SRT shows higher PSNR (25.93) over LASER-NV (24.45). Note that SRT does not learn to estimate densities nor depth maps and\n\n3To create this visualisation we trained the model on a modified City dataset with birds-eye views.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nReconstruction\n\nPrediction\n\nLog-Likelihood ↑\n\nPSNR↑\n\nSSIM↑\n\nMVC-NeRF Cond. NeRF-VAE NeRF-VAE LASER-NV (NO GEOM.) LASER-NV\n\n- 3.51 3.50 3.82 3.83\n\n28.63 23.94 23.92 27.13 29.61\n\n0.910 0.709 0.708 0.870 0.923\n\nFID↓\n\n91.92 50.98 52.12 24.19 22.54\n\nTable 1: Results in City. The color likelihood is computed via importance sampling (10 samples); note that MVC-NeRF is deterministic and thus we do not compare it on this metric. Reconstruction: We report performance of reconstruction of the 2 context views. Prediction: We evaluate the models on previously unobserved views using the FID score. MVC-NeRF provides a single deterministic prediction whereas for the other models we average over 10 independent samples.\n\nFigure 2: Representative reconstructions of observed views and predictions of partially unobserved novel views. The models are conditioned on two context views (top left). The first two columns show reconstructions of those views. Note how NeRF-VAE struggles to reconstruct both views accurately, while the other models do well. We show predictions of two novel views (top right), where the first view is partially observed in the context (highlighted in cyan color) and the second view is fully unobserved. LASER-NV’s predictions are consistent with the observed part of the context, diverse, and plausibly so. NeRF-VAE’s predictions lack quality and diversity. MVC-NeRF’s only produces a single prediction, which is not very plausible.\n\nis deterministic. Fig. 3 shows a prediction example using LASER-NV, and further examples are in Appendix C which demonstrate high fidelity colours and densities.\n\n4.6 TRAIN- AND TEST-TIME SCALING & DATA EFFICIENCY\n\nLatent set representations allow trading computation for increased model capacity. We train LASERNV on the City dataset with increasing latent set sizes K, and compare it to a conditional NeRF-VAE also trained with increasing latent dimensions4. Figure 4 shows reconstruction log-likelihoods as a\n\n4Note that this increases the number of NeRF-VAE’s parameters.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nDVR*\n\nSRN*\n\nSRT*\n\npixelNeRF*\n\nNeRF-VAE MVC-NeRF\n\nLASER-NV\n\nPSNR ↑ SSIM ↑\n\n22.70 0.860\n\n23.28 0.849\n\n27.87 0.912\n\n26.80 0.910\n\n25.36 0.875\n\n28.03 0.925\n\n27.92 0.923\n\nTable 2: Category-agnostic ShapeNet evaluation by reconstructing 23 views from one input view for all models. We further report numbers from the respective papers for DVR* (Niemeyer et al., 2020), SRN* (Sitzmann et al., 2019), SRT* (Sajjadi et al., 2021), and pixelNeRF* (Yu et al., 2021).\n\nNeRF\n\nLight field\n\npixelNeRF VOSRT\n\nLASER-NV OSRT\n\nSRT\n\nSRT++\n\nPSNR ↑\n\n21.97\n\n21.38\n\n24.45\n\n23.33\n\n23.54\n\n25.93\n\nTable 3: Novel view PSNR of one view from 5 input views on MSN-Hard. PixelNeRF results are from Sajjadi et al. (2021) and the remaining from Sajjadi et al. (2022). SRT++ is an improved SRT (Sajjadi et al., 2022).\n\nFigure 3: Novel view syntesis with LASER-NV on MSN-Hard.\n\nfunction of latent capacity. While LASER-NV consistently improves its reconstructions with larger latent sets, cond. NeRF-VAE quickly saturates at 256 latent dimensions, at worse reconstructions compared to LASER-NV. Further, in Appendix D, we provide an analysis of test-time scaling—we show that increasing the latent set size at test time does not result in clear change of performance. Finally, in Figure 14 we show that LASER-NV performs better at test time than NeRF-VAE and MVC-NeRF for different training dataset sizes.\n\n5 RELATED WORK\n\nNeural scene representations Inferring neural scene representations using a deep generative model of scenes is an active research area, and we mention some of the most closely related approaches below. NeRF-VAE learns a generative model of 3D scenes which is only shown to work in relatively simple scenes. When applied to more complicated data (see Fig. 2), it produces blurry reconstructions and novel views. We posit that this is caused by insufficient capacity of its latent representation z. Generative Query Network (Eslami et al., 2018) similarly learns a distribution over scenes, but its decoder is a CNN. Subsequent work (Rosenbaum et al., 2018) improves over its representational capacity by using an attentive mechanism when conditioning the decoder from context in an analogous way to how LASER-NV improves over NeRF-VAE. By implementing a NeRF in our decoder we gain the benefits of more robust generalization capabilities to novel viewpoints compared to the convolutional counterparts (Kosiorek et al., 2021). Simone (Kabra et al., 2021) is a convolutional generative model that performs high quality novel view synthesis and while also performing unsupervised segmentation of input videos. The authors in (Dupont et al., 2022) propose directly modelling the underlying radiance fields as functions, which they refer to as ‘functa’. Both our method and theirs learn a prior over scene functions with a model that has a large representational capacity. Bautista et al. (2022) propose a two-stage method to learn a generative model of 3D scenes. As LASER-NV, their model can conditionally sample radiance fields. However, they focus on scenes with dense views, whereas LASER-NV works with sparse views.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) LASER-NV\n\n(b) Conditional NeRF-VAE\n\nFigure 4: Training reconstruction performance in City of LASER-NV with an increasing latent set size (each with 128 dimensions) vs a cond. NeRF-VAE with increasing latent dimensionality.\n\nDeterministic novel view synthesis NeRF relies on optimizing the parameters of an MLP scene function, separately for every scene. A number of methods in the literature propose amortizing the mapping of input views to the scene function using multi-view geometry, see e.g. IBRNet (Wang et al., 2021), General Radiance Fields (GRF) (Trevithick & Yang, 2021) and pixelNeRF (Yu et al., 2021). These methods extract features of the context views by using projective geometry to select which features to attend to, and are shown to generalize well and carry out high-fidelity predictions. PixelNeRF is designed to work well in a solely view-centered coordinate system. MVSNerf (Chen et al., 2021) lifts context features to a 3D representation with strong novel-view synthesis performance. Scene Representation Transformers (SRT) (Sajjadi et al., 2021) use global features obtained from context views using a generalization of the Vision Transformer (Dosovitskiy et al., 2020), which bears similarities to LASER-NV. SRT bypasses the use of 3D geometry entirely, yet shows remarkable performance on real world data.\n\nNeRF for large-scale scenes A number of NeRF methods have been developed to scale up to city-wide scenes. They work by optimizing multiple separate scene functions for separate blocks of a scene, which are then dynamically used when rendering (Tancik et al., 2022; Turki et al., 2021). Alternatively they design a scene function that represents the scene at multiple levels of scale (Martel et al., 2021; Xiangli et al., 2021). Similarly, Mip-NeRF (Barron et al., 2021) allows prefiltering the scene function inputs to better capture coarse and fine details. These methods are orthogonal to ours since they do not learn a conditional NeRF model sparse views nor address the uncertainty in predictions.\n\n6 CONCLUSION\n\nWe propose LASER-NV, a conditional generative model of neural radiance fields capable of efficient inference of large and complex scenes under partial observability conditions. While recent advances in neural scene representation and neural rendering lead to unprecedented novel view synthesis capabilities, producing sharp and multi-modal completions of unobserved parts of the scene remains a challenging problem. We experimentally show that LASER-NV can model scenes of different scale and uncertainty structure, and isolate the usefulness of each contribution through ablation studies. While NeRF serves as a strong inductive bias for learning 3D structure from images, LASER-NV also inherits some of its drawbacks. Volumetric rendering with neural radiance fields is computationally costly and can limit the model from real-time rendering. It remains to be seen whether fast NeRF implementations (Müller et al., 2022) can be of help. Furthermore, accurate GT camera information is required for learning and novel view synthesis; (Moreau et al., 2022) recently made progress on localization methods using NeRF. While learning a generative scene model of real scenes remains an open problem, we believe this work is an important step in that direction. Finally, an interesting direction for future research is incorporating object-centric structure and dynamics into LASER-NV, which might be useful for for downstream reasoning tasks (Sajjadi et al., 2022; Ding et al., 2021).\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nJonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. In International Conference on Computer Vision, 2021.\n\nMiguel Angel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin Dehghan, and Josh Susskind. GAUDI: A Neural Architect for Immersive 3D Scene Generation. In Advances in Neural Information Processing Systems, 2022 arXiv/2207.13751.\n\nJustin Bayer, Maximilian Soelch, Atanas Mirchev, Baris Kayalibay, and Patrick van der Smagt. Mind the Gap when Conditioning Amortised Inference in Sequential Latent-Variable Models. In International Conference on Representation Learning, 2021 arXiv/2101.07046.\n\nChristopher M. Bender, Kevin O’Connor, Yang Li, Juan Jose Garcia, Junier Oliva, and Manzil Zaheer. Exchangeable Generative Models with Flow Scans. In AAAI Conference on Artificial Intelligence, 2020.\n\nJames F. Blinn. Light Reflection Functions for Simulation of Clouds and Dusty Surfaces.\n\nIn\n\nSIGGRAPH, 1982.\n\nChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. MONET: Unsupervised scene decomposition and representation. arXiv/1901.11390, 2019.\n\nAnpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In International Conference on Computer Vision, 2021 arXiv/2103.15595.\n\nDavid Ding, Felix Hill, Adam Santoro, Malcolm Reynolds, and Matt Botvinick. Attention over learned object embeddings enables complex visual reasoning. Advances in Neural Information Processing Systems, 34, 2021.\n\nYikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and Xiao Liu. Transmvsnet: Global context-aware multi-view stereo network with transformers. In Conference on Computer Vision and Pattern Recognition, 2022.\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In\n\nInternational Conference on Representation Learning, 2017 arXiv/1605.08803.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nEmilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Jimenez Rezende, and Dan Rosenbaum. From data to functa: Your data point is a function and you should treat it like one. ArXiv, arXiv/2201.12204, 2022.\n\nSM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Garnelo, Avraham Ruderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene representation and rendering. Science, (6394):1204–1210, 2018. 360.\n\nYihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu. Epipolar transformers. In Conference on\n\nComputer Vision and Pattern, 2020.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017.\n\nRishabh Kabra, Daniel Zoran, Goker Erdogan, Loic Matthey, Antonia Creswell, Matt Botvinick, Alexander Lerchner, and Chris Burgess. Simone: View-invariant, temporally-abstracted object representations via unsupervised video decomposition. Advances in Neural Information Processing Systems, 34, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nHiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In Conference on\n\nComputer Vision and Pattern, 2018.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv/1412.6980,\n\n2014.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference\n\non Learning Representations, 2014.\n\nAdam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosália G. Schneider, Sovna Mokr’a, and Danilo Jimenez Rezende. Nerf-vae: A geometry aware 3d scene generative model. In International Conference on Machine Learning, 2021 arXiv/2104.00587.\n\nChen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. BARF: bundle-adjusting neural radiance fields. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, 2021.\n\nJulien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric Chan, Marco Monteiro, and Gordon Wetzstein. Acorn: Adaptive coordinate networks for neural scene representation. ACM Trans. Graph., 40:58:1–58:13, 2021.\n\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, 2020.\n\nArthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan Stanciulescu, and Arnaud de La Fortelle.\n\nLens: Localization enhanced by nerf synthesis. In Conference on Robot Learning, 2022.\n\nThomas Müller, Alex Evans, Christoph Schied, and Alexander Keller.\n\nInstant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1–102:15, July 2022.\n\nMichael Niemeyer, Lars M. Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable In 2020 volumetric rendering: Learning implicit 3d representations without 3d supervision. IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, 2020.\n\nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):1–64, 2021.\n\nRazvan Pascanu, Tomás Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural\n\nnetworks. In International Conference on Machine Learning, 2013 28.\n\nDanilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In\n\nInternational Conference on Machine Learning, 2015.\n\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and In International Conference on Machine\n\napproximate inference in deep generative models. Learning, 2014.\n\nDan Rosenbaum, Frederic Besse, Fabio Viola, Danilo J. Rezende, and S. M. Ali Eslami. Learning\n\nmodels for visual 3d localization with implicit mapping. CoRR, arXiv/1807.03149, 2018.\n\nMehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs M. Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas A. Funkhouser, and Andrea Tagliasacchi. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. arXiv/2111.13152, 2021.\n\nMehdi S. M. Sajjadi, Daniel Duckworth, Aravindh Mahendran, Sjoerd van Steenkiste, Filip Paveti’c, Mario Luvci’c, Leonidas J. Guibas, Klaus Greff, and Thomas Kipf. Object scene representation transformer. 2022 arXiv/2206.06922.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nVincent Sitzmann, M. Zollhöfer, and G. Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. In Advances in Neural Information Processing Systems, 2019. arXiv/1906.01618.\n\nKarl Stelzner, Kristian Kersting, and Adam R. Kosiorek. Decomposing 3d scenes into objects via\n\nunsupervised volume segmentation. In arXiv, 2021 arXiv/2104.01148.\n\nMatthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P. Srinivasan, Jonathan T. Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. ArXiv, arXiv/2202.05263, 2022.\n\nAlex Trevithick and B. Yang. GRF: Learning a general radiance field for 3d scene representation and rendering. In International Conference on Learning Representations, 2021. arXiv/2010.04595.\n\nHaithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-nerf: Scalable construction of\n\nlarge-scale nerfs for virtual fly-throughs. CoRR, arXiv/2112.10703, 2021.\n\nArash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. arXiv/2007.03898,\n\n2020.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 30.\n\nQianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P. Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas A. Funkhouser. IBRNet: Learning multi-view image-based rendering. In Conference on Computer Vision and Pattern Recognition, 2021.\n\nXiaofeng Wang, Zheng Zhu, Fangbo Qin, Yun Ye, Guan Huang, Xu Chi, Yijia He, and Xingang Wang. Mvster: Epipolar transformer for efficient multi-view stereo. arXiv preprint arXiv:2204.07346, 2022.\n\nZhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Trans. Image Process., 13(4):600–612, 2004.\n\nPeter Wirnsberger, Andrew J. Ballard, George Papamakarios, Stuart Abercrombie, Sébastien Racanière, Alexander Pritzel, Danilo Jimenez Rezende, and Charles Blundell. Targeted free energy estimation via learned mappings. The Journal of Chemical Physics, 2020.\n\nPeter Wirnsberger, George Papamakarios, Borja Ibarz, Sébastien Racanière, Andrew J Ballard, Alexander Pritzel, and Charles Blundell. Normalizing flows for atomic solids. arXiv preprint arXiv:2111.08696, 2021.\n\nJunhua Xi, Yifei Shi, Yijie Wang, Yulan Guo, and Kai Xu. Raymvsnet: Learning ray-based 1d implicit fields for accurate multi-view stereo. In Conference on Computer Vision and Pattern Recognition, 2022.\n\nYuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai,\n\nand Dahua Lin. Citynerf: Building nerf at city scale. ArXiv, arXiv/2112.05504, 2021.\n\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA MODEL ARCHITECTURES\n\nA.1 NORMALIZING FLOWS FOR LASER\n\nWe wish to model our latent set representation Z = {z1, . . . , zK} using a flexible distribution that is efficient both to evaluate as well as sample from. Furthermore, the distribution should ideally be invariant to re-ordering of the elements of the set. Normalizing flows (Rezende & Mohamed, 2015) allow specifying a distribution with precisely these properties. Formally, it is given by:\n\nZ (0) ∼ p0(Z) , Z (m) = fm\n\n(cid:16)\n\nZ (m−1), V\n\n(cid:17)\n\n, m = 1, . . . , M , with Z := Z (M ) ,\n\n(4)\n\nwhere p0(·) is a base distribution, fm(·) are invertible transformations applied to an intermediate random variable, and Z is the final LASER after M flow layers. V is an additional set of context vectors that condition the flow transformation. The resulting probability density of Z is\n\np(Z | V) = p0\n\n(cid:16)\n\nZ (0)(cid:17) M\n\n(cid:89)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) m=1\n\ndet\n\nδfm\n\n(cid:0)Z (m−1), V(cid:1) δZ (m−1)\n\n−1\n\n.\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(5)\n\nThe need to compute the Jacobian of intermediate flow transformations puts constraints on the design of those transformations, as they need to be computationally efficient. We use a split-coupling flow similar to RealNVP (Dinh et al., 2017), and we follow a similar architecture as the ones used in Wirnsberger et al. (2020; 2021) in adapting it to sets. The method Flow Scans (Bender et al., 2020) first introduced the general idea of adapting split-coupling to work on sets, and Wirnsberger et al. (2020; 2021) use a transformer for increased flexibility. A split-coupling transformation partitions the set Z into two sets Z1, Z2 by splitting each element of the original set across channels (we split in half). At a high level, the transformation of one part is computed as a function of the other part. Finally, the original set is recovered by elementwise concatenation of both transformed parts. More m that maps Z (m−1) → Z (m) is formally, let g be an invertible function, a split-coupling transform f sc given by three operations:\n\nZ (m)\n\n1 = g\n\nZ (m)\n\n2 = g\n\n(cid:16)\n\n(cid:16)\n\nZ (m−1)\n\n1\n\n, tm\n\nZ (m−1)\n\n2\n\n, tm\n\n(cid:16)\n\n(cid:16)\n\nZ (m−1)\n\n2\n\n, V\n\n(cid:17)(cid:17)\n\n,\n\nZ (m)\n\n1\n\n, V\n\n(cid:17)(cid:17)\n\n,\n\n(6)\n\nZ (m) = {concat(z(m)\n\nk,1 , z(m)\n\nk,2 )}K\n\nk=1 ,\n\nwhere zk,1 indicates dimensions of split 1 of element k, and V is conditioning context. Note how the parameters of the transformation g of one partition are derived from the other partition. To do so in a permutation-equivariant manner, the functions tm are a composition of two transformers: a self-attention module is applied followed by a cross-attention conditioned on context features, defined in Appendix A.2. More specifically, when transforming split 1 as a function of split 2, we have:\n\nψ(m−1)\n\n2\n\n= CrossAttn\n\n(cid:16)\n\n(cid:16)\n\nSelfAttn\n\nZ (m−1)\n\n2\n\n(cid:17)\n\n(cid:17)\n\n.\n\n, V\n\n(7)\n\nWe finally apply the function g, in our case the affine function (Dinh et al., 2017), from the parameters obtained for every element in ψm−1\n\n:\n\n2\n\nk,1 = z(m−1) z(m)\n\nk,1\n\nexp(W m\n\nscaleψ(m−1)\n\nk,2\n\n) + W m\n\nscaleψ(m−1)\n\nk,2\n\nfor k = 1, . . . K ,\n\n(8)\n\nscale and W m\n\nwhere W m bias are linear weights. When composing our complete flow with M layers of split-coupling flows, we find that including an additional linear invertible transformation after each fm to be helpful. To do so, we use the Linear Permutation Equivariant (f lpe m ), proposed in the Flow Scans work, the purpose of which is to allow capturing interdependencies in the dimensions of the set elements. The only difference is that we parameterize each sub-block of the linear transformation using a full unconstrained matrix (instead of a diagonal matrix).\n\nIn summary, our flow chains together the sequence of mappings f sc M . Thus, sampling from this flow involves sampling from the base distribution followed by applying the chain of functions.\n\n1 , . . . , f sc\n\nM , f lpe\n\n1 , f lpe\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nOur normalizing flow differs in a number of ways compared with Wirnsberger et al. (2020; 2021). First, our flow layers are additionally parameterized by a cross-attention module in order to condition the distribution on the context set; second, we don’t use rational-quadratic splines, instead we found the simpler affine coupling layer to work well. Notice that the size of the resulting set depends only on the hyper-parameter K, that is the number of elements sampled from the base distribution. It does not depend on the context size, i.e. features extracted from input images.\n\nIn our experiments, we use a diagonal Gaussian base distribution, and set the number of flow layers to be M = 8 for the prior flow, and M = 4 for the posterior flows. Our self-attention and cross-attention are composed of L = 1 layers, and 256 hidden units. Furthermore, we invert the chain of mappings (which is possible due to invertibility) for the prior flow as it we found it to be more numerically stable during training.\n\nA.2 ATTENTION MODULES\n\nFigure 5: Multi-head self-attention and cross-attention blocks. When combining multiple blocks, the outputs yl for layer l become the inputs for layer l + 1.\n\nWe use transformer attention Vaswani et al. (2017) in various components of LASER-NV. We distinguish two types of attention modules: self-attention, where keys, queries and values are computed from the same input, and cross-attention, where some context is used to compute keys and values, and queries are computed from a different input). An attention module can be composed of multiple attention blocks, where each attention block is shown in Fig. 5. We use the standard multihead dot-product attention with softmax weights. We define SelfAttn(y, L) to be a self-attention module with input y and L blocks5 and, similarly, CrossAttn(y, c, L) to be a cross-attention module with context c. Note how SelfAttn and CrossAttn do not include positional encoding that are often used in transformers.\n\nA.3\n\nIMAGE ENCODER\n\nTo encode the context images, we use a convolutional neural network based on the ResNet architecture from Vahdat & Kautz (2020) (also described in Kosiorek et al. (2021)) to independently encode context elements into h × w feature maps. The residual blocks that compose the encoder are shown in Fig. 6. For all models and datasets we use L = 4. Each context view consists of an image in RGB space with concatenated information of the camera position and direction.\n\nFor N context views, our last layer outputs N hw vectors that form the conditioning set used to parameterize our distributions over Z. Furthermore, the feature maps at different layers are also combined into a feature stack to use as the context local features as described in Appendix A.4.\n\n5We often drop the L input argument for notational clarity.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: ResNet architecture based on the encoder of NouveauVAE Vahdat & Kautz (2020).\n\nA.4 SCENE FUNCTION ARCHITECTURES\n\nFig. 7 provides a high-level overview of the information flows going into the scene function of each model, and we describe implementation details of their components below.\n\n(a) LASER-NV scene function\n\n(b) LASER-NV (NO GEOM.) scene function\n\n(c) MVC-NeRF scene function\n\n(d) NeRF-VAE scene function\n\nFigure 7: Scene function architecture for LASER-NV (a), its ablations (b) and (c), and NeRF-VAE (d).\n\nLatent attention A query point xi attends to Z as per Fig. 7 (a) using a cross-attn(γ(xi), Z, L), where γ(xi) is the circular encoding function of positions (see Appendix A.4). We use the same architecture across datasets, setting L = 3 layers, 512 hidden units, and 4 attention heads.\n\nLocal features from multi-view geometry We follow PixelNERF’s design of stacking the features of the encoded images at different levels of resolution, which allows the conditioning features to\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\ncapture context structure at different scales. For input images of resolution W × H, we stack the features of the outputs from all layers after the first layer:\n\nHn = stack({Hn\n\nl ∈ R W (9) where i = 2l. When stacking, we upscale the feature maps of all the feature maps of layers l ∈ {2 . . . L} to the size of the feature map with largest resolution (l = 1) using bilinear interpolation. The resulting set of stacked feature maps (one for each context view) is then used to condition the scene function.\n\ni ×16i}L\n\nl=1),\n\ni × H\n\nAs we described in Section 3.2, our multi-view geometry module is in part based on pixelNeRF’s processing of context views. Particularly, we process local features with exactly the f1 ResNet detailed in Yu et al. (2021): positions and directions are projected to the view-space using the cameras intrinsic parameters, and then processed with 3 residual blocks of 512 hidden units and ReLu activation. When computing the projection, we also define the binary variable vn i based on whether the projected coordinates are within the image plane.\n\nThe output features are integrated with our latent features via Mθ which is a cross-attention module. Note that context features with vn i = 0 are not attended to (we modify the pre-softmax logits of those context features by adding a large negative number to the logits). In our experiments Mθ has 3 layers and 512 hidden units.\n\nScene function output MLP We now describe the final component of the scene functions of all models, as shown in Fig. 7. This component follows the residual MLP architecture of the scene function of NeRF-VAE. Namely, it takes as input the circular encoded positions γ(xi) and directions γ(di), and at every layer it linearly projects the conditioning features (i.e. z for NeRF-VAE, or fi for LASER-NV) and sums it to the activations. The MLP is composed of two parts: the first has 4 layer of 256 hidden units each and outputs the densities, and the second part has an additional 4 layers and outputs the colours. All layers use a swish activation function.\n\nThe circular encoding function γ is given by\n\nγ(p) = (sin(2Lminπp), cos(2Lminπp), . . . , sin(2Lmaxπp) , cos(2Lmax)) (10) where we use different constants Lmin and Lmax for positions and directions and for each dataset (listed in Appendix E).\n\nA.5 NeRF-VAE AND CONDITIONAL NeRF-VAE\n\nOur implementation NeRF-VAE follows the same overall architecture as that of Kosiorek et al. (2021), but we use the image encoder described in Fig. 6 instead. The approximate posterior is computed as a diagonal Gaussian distribution, the means and log-variances of which are given by an MLP e as a function of the pooled feature maps H (pooled with a simple averaging over feature maps). The MLP e has two layers of 256 hidden units and ReLu activation function. The scene function is composed of an MLP that processes the latent vector z (one linear layer of 256 units, followed by a ReLu non-linearity), followed by the output MLP described in Appendix A.4.\n\nA.6 BACKGROUND SCENE FUNCTION\n\nThe volumetric rendering integral of a ray r is given by\n\nc(r)NeRF =\n\n(cid:90) tf\n\ntn\n\nT (t)σ(r(t))c(r(t), d) dt , with T (t) = exp\n\n−\n\n(cid:18)\n\n(cid:19)\n\nσ(r(s)) ds\n\n.\n\n(11)\n\n(cid:90) tf\n\ntn\n\nIf the ray is infinite (camera’s far plane tf → ∞) the colour weights T (t)σ(r(t)) would always sum to one. For a truncated ray (finite tf ), this happens only if the ray passes through a solid surface. This heuristic allows to model distant backgrounds by putting them at the end of the integration range, but it doesn’t work if the camera position changes significantly compared to the ray range tf − tn. If not normalized, the final weight T (tf ) tell us about the probability of light hitting a particle beyond the ray.\n\nIn that case, we can model the background as an infinite dome around the scene and model it with a light-field that depends only on the camera direction. That is, we compute the colour as\n\nc(r) = c(r)NeRF + T (tf )fbg(d) .\n\n(12)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFor the City dataset, in order to model the sky and sun, fbg(Z, di) is a learned neural network. We use a cross-attention module with 2 layers to attend to Z. For NeRF-VAE and cond. NeRF-VAE we instead use a 2 layered MLP with ReLu activations. For the ShapeNet fbg is a constant colour (white).\n\nn=1 H n, a cross-attention module attends to the latents: ˆhbg\n\nWe address the challenging backgrounds of MSN-Hard with a more expressive attentive background scene function that conditions on both the latents Z as well as context image features – this is in a similar fashion to how the NeRF scene function of LASER-NV integrates its latents with attention to local context features. Given an encoded ray direction γ(di) and context feature maps (cid:83)N bg(γ(di), Z), and the output is used to attend to the set of context features (cid:83)N n=1 H n with a second cross-attention module hbg n=1 H n). Finally, an MLP f mlp computes the background colour by taking as bg input the obtained background features and additionally conditions on the encoded directions by adding a linear projection of the directions to the activations of every layer. This MLP is composed of 2 layers of 512 hidden units each with a ReLu nonlinearity, and outputs the RGB background colours using a sigmoid nonlinearity.\n\ni = f 1\n\ni = f 2\n\ni , (cid:83)N\n\nbg(ˆhbg\n\nB CITY DATASET\n\nFigure 8: Examples of generated cities.\n\nFigure 9: Examples of different viewpoints (column) for each scene (row).\n\nThe aim of the City dataset is to allow generating visually complex scenes with large variability in its structure and appearance. Each generated city consists of a large area spanning approximately 270m × 270m, which consists of a 2x2 grid of blocks, where each block is composed of 4 lots. A lot can be a building or a green patch (park). The city 2x2 grid is surrounded by an outer ring of buildings. Roads separate all blocks in the scene. Each building is randomly generated with variation in size, appearance (textures, materials, colours), architectural styles (residential, industrial, commercial). Fig. 8 shows examples of generated city scenes. Once a city is built, we specify a range of points of interest, i.e. road intersections and parks and use each of these points as the starting placement for a scene (data point) of our dataset. Each scene is comprised of posed images rendered from randomly\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nchosen viewpoints near the point of interest. More specifically, we render 8 randomly chosen nearby points, and 4 random viewpoints for each point, making a total of 32 generated views per scene. Each generated view is annotated with the ground-truth render, camera parameters and depth maps. Our generated training dataset contains 100,000 scenes generated in total from approximately 2,600 uniquely generated cities. We test on 3,200 scenes generated in total from 10 uniquely generated cities.\n\nC ADDITIONAL VISUALIZATIONS\n\nCapturing detail at different scale In Fig. 10 we can see at a higher resolution how our model is expressive enough to capture texture and geometry details at different scales (e.g. far-away buildings and nearby objects such as the lamp post and the colonnade).\n\nFigure 10: Higher resolution reconstructions of LASER-NV reveal detail captured at different scales (e.g. lamp posts, and nearby buildings vs buildings at a large distance).\n\nNovel view synthesis with ShapeNet NMR In Fig. 11 we compare predictions for a number of objects that reveal how NeRF-VAE produces blurry reconstructions compared to MVC-NeRF and LASER-NV. When an image does not reveal the full shape of an object, as shown in Fig. 12, LASERNV is able to generate multiple plausible variations (in contrast to a deterministic which generates a single blurry prediction).\n\nFigure 11: Samples of category-agnostic ShapeNet for ablations of LASER-NV. MVC-NeRF and LASER-NV both result in high-fidelity predictions, while the NeRF-VAE’s outputs are blurrier.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: When predicting novel view from an ambiguous input, LASER-NV generates plausible variations, whereas MVC-NeRF results in a single blurry prediction.\n\nMSN-Hard predictions Figure 13 shows LASER-NV’s predictions on MSN-Hard including depth maps estimated from NeRF’s densities.\n\nFigure 13: Representative examples of LASER-NV predictions on MSN-Hard.\n\nD ADDITIONAL RESULTS\n\nData efficiency We evaluate how data hungry LASER-NV is compared to NeRF-VAE and MVCNeRF. In Figure 14 we report the reconstruction colour log-probability estimated on a validation set of the City dataset. For LASER-NV, we observe little degradation between using 50K scenes and 12.5K unique scenes. For NeRF-VAE we notice a larger drop when reducing the size from 25K to 12.5 scenes, showing clearer signs of overfitting. Finally, since MVC-NeRF severely underfits the City dataset for all training set sizes, we do not see any clear performance drop.\n\nTest-Time Scaling of the Latent Set We further want to understand how different latent set sizes at test time impact performance. Figure 15 shows reconstruction performance for different values of K at test time, for various values of used K during training. As we can see, LASER-NV does not significantly leverage a larger latent set size than trained with. As expected, we do see performance degrading when using smaller sets at test time, and this effect becomes stronger for models trained with larger capacity.\n\nTraining LASER-NV without ground-truth depth We train all our models using ground-truth depth on the City dataset. This reduces memory and computation requirements during training. We\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 14: Validation reconstruction performance of different method as a function of number of scenes throughout training.\n\nFigure 15: Reconstruction performance of LASER-NV as a function of the latent set size used at test time for different models trained with increasing sizes K.\n\nnote, however, that it is also possible to train LASER-NV without using ground truth depths. Once trained, we find that both the model trained with depths and the model trained without achieves slightly higher average colour log-likelihood (3.87) compared to LASER-NV (of 3.83) (importanceweighted estimate using 10 samples). We hypothesize that this is because the model trained without depths is trained solely using the rendering method used at test time.\n\nAs any NeRF-related method, LASER-NV can output estimated depth maps easily. We show example reconstructions and inferred depth maps of a trained LASER-NV on the City in Fig. 16. Note how the model is capable of capturing fine detail in textures and shapes (e.g. lamp post).\n\nComputational efficiency of LASER-NV vs NeRF-VAE For a model conditioned on 10 input views, inference (inferring the latent distribution) and rendering of a single image for LASER-NV (with 24 slots) take 0.39 GFLOPs and 1.80 GFLOPs respectively. NeRF-VAE instead takes 0.37 GFLOPs and 0.15 GFLOPs. The main reason for more expensive rendering in LASER-NV is the introduction of PixelNeRF-like conditioning. A model with a flow posterior but without such conditioning uses 0.47 GFLOPs to render a single image. Attention, however expensive, is evaluated only once per scene (to compute the prior/posterior). The PixelNeRF features are cheap per point, but they are evaluated for every 3D point used in rendering, therefore dominating the overall cost. Note that the above numbers do not reflect training latency. At training time we can guide the rendering process by using depths, which allows us to evaluate the scene function only twice for every pixel. We also\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 16: Reconstructions of LASER-NV trained without the use of ground-truth depths.\n\nsubsample images and we never reconstruct the whole target view(s). This makes training faster, and means that LASER-NV is only about 4x slower per training iteration than NeRF-VAE. Having said that, we noticed that NeRF-VAE does not benefit from longer training (its performance saturates), while the performance increase from increasing the model size (to equalize the FLOPs) is negligible.\n\nRobustness to camera noise We carry out an experiment on MSN-Hard in which we train and test with camera poses with incremental levels of Gaussian noise. We replicate the set up in Sajjadi et al. (2021) and compare their reported results using SRT and PixelNeRF to LASER-NV. Results in Fig. 17 show that while LASER-NV outperforms the other models when there is no noise, its performance drops more quickly compared to SRT, in a similar fashion to PixelNeRF. However, LASER-NV is more robust than PixelNeRF for all levels of noise. We believe that the use of NeRF and local features makes these models less robust to noisy cameras. That said, there are recent methods that allow mitigating noise sensitivity with NeRF (Lin et al., 2021) that can be incorporated.\n\nFigure 17: PSNR of models trained and evaluated under cameras with different levels of noise. The PSNR values of LASER-NV* are preliminary training values models that have not trained to convergence (340K steps out of 1000K).\n\nE TRAINING DETAILS\n\nHierarchical rendering Instead of optimizing separate sets of parameters for the coarse and fine rendering passes, we find that it suffices to only keep separate parameters of Fθ. This allows sharing parameters (and reduce computation) when querying the scene function on the coarse sampled points with respect to latent attention and multi-view geometry components of models.\n\nTraining with depth (City) Because we have RGB-D data in the City dataset, we can use depth as a supervised signal following the method in Stelzner et al. (2021). This method involves replacing the\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\npixel colour likelihood, for a given ray r, with the ray colour and depth likelihood p(t(r), c(r) | V, Z), where t(r) is the ray’s ground-truth depth. This alternative likelihood requires only two scene function evaluations per ray. In practice, we find that we get the best quality renders when combining both loglikelihood terms to the loss (each weighted by 0.5). At every training set, similar to how NeRF-VAE is trained, the likelihood terms are estimated by only reconstructing a subset of rays (i.e. pixels) of all the target images. We find that using 64 rays for estimating the (expensive) image log-likelihood, and 512 to estimate the ray colour and depth likelihood term, works well in the City. Additionally, we restrict output densities to be between 0 and 10 by using a weighted sigmoid in the output of the scene function.\n\nAdditional regularization The loss function for ShapeNet experiments includes a density regularization term based on the L1-norm of the densities, with a weight of 0.01. For all experiments, we scale the model parameter updates (Pascanu et al., 2013, Sec. 3.2), such that loss gradient norms have a maximum value of 10 for improved stability.\n\nHyper-parameters We use the following hyper-parameters across all models. Some of which were selected for each dataset based on a preliminary exploration using the training dataset and a validation split.\n\nHyperparameter\n\nCity\n\nShapenet NMR MSN-Hard\n\nLearning rate Batch size # of rays per scene # of context views # of target views # of posterior context views # of coarse points # of fine points Camera tnear Camera tf ar Positions circular encoding Lmin Positions circular encoding Lmax Directions circular encoding Lmin Directions circular encoding Lmax LASER-NV set size K LASER-NV latent vector dimensionality\n\n0.0003 96 512 2\n8 10 256 64 0.05 270 -8 8\n0 4\n24 128\n\n0.0002 96 512 1\n23 4\n32 64 0.1 3.7 -2 8\n0 4\n8 128\n\nTable 4: Common hyper-parameters\n\n0.0001 32 384 5\n3 8\n64 32 0.01 19 -5 10 0\n8 32 128\n\n22",
  "translations": [
    "# Summary Of The Paper\n\nThis paper tackles the problem of scene generation, generating consistent images of scenes from multiple viewpoints conditioned on few images. The main technical contribution of the work is proposing a set-valued latent representation using normalizing flows, built on top of NeRF-VAE. The proposed model is tested on synthetic city dataset and ShapeNet dataset, while having competitive performance with respect to recent methods.\n\n# Strength And Weaknesses\n\n**Strength**\n- Performance\n\nBy looking at the figures and tables of the manuscript, the work clearly outperforms NeRF-VAE [1], producing less blurry images from multiple views. However, I would like to ask the authors why SRT [2] was not used as a baseline.\n\n- Good experimental design\n\nI enjoyed showing that increasing the latent vector size of NeRF-VAE does not result in a good performance, thereby the design choice of this work is better. The ablations by showing with performance with/without either latent set and local features make the paper stronger.\n\n**Weaknesses**\n- Novelty\n\nMy main concern lies in the novelty of the work. The work seems to be a mixture of NeRF-VAE and SRT. Please see the novelty section below for details.\n\n- Intuition behind the set data structure for the scene latents\n\nAlthough it is clear from Table 1, that the scene latents increase the performance of the method, I would like to ask the authors the intuition behind the sets as the choice for the latent data structure. Other works such as Neural Sparse Voxel Fields [3] either have an explicit latent data structure, where the latent code explicitly encodes local information or alot of other generative modeling works show the controlability of latent code by interpolating them. I do not see either of the intuition behind the set representation nor other attributes of set-valued latent representation and would like to hear from the authors the reason for choosing particularly this data structure besides the performance.\n\n- Proposed model requires depth information\n\nAlthough the authors have reported the log-likelihood score of the method in the appendix without depth information, reporting all the scores in Table 1 without depth information will strengthen the paper.\n\n[1] Kosiorek et al. NeRF-VAE: A Geometry Aware 3D Scene Generative Model. ICML, 2021\n\n[2] Sajjadi et al. Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. CVPR, 2022\n\n[3] Liu et al. Neural Sparse Voxel Fields. NeurIPS, 2020\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- Clarity\n\nThe paper is clearly written and easy to understand and the figures make the paper easier to understand.\n\n- Novelty\n\nThe work seems to be a mixture of NeRF-VAE and SRT which is my biggest concern. Although the taking advantages of other methods does not necessarily reduce the novelty of the work, using the latent code of NeRF-VAE, thereby having multimodality and using set-representation latent code of SRT is straightforward with no huge novelty in the problem setting.\n\n- Quality\n\nThe experimental design of the paper is well, although some more ablation studies could help.\n\n- Reproducibility\n\nCode is not uploaded. Wil the code and city datasets be uploaded? I think the city dataset will be used alot in the community.\n\n# Summary Of The Review\n\nMy biggest concern lies in the novelty of the method. I would like to hear from other reviewers before making the final decision.\n\n---------------------------\nAfter Rebuttal:\nI appreciate the authors for their response.\nI'm in favor of accepting the paper, due to it's multimodality and high performance.\nI have raised my score accordingly.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents LASER-NV, a generative model leveraging a set-valued latent representation to improve 3D scene modeling, particularly addressing the limitations seen in existing models like NeRF and NeRF-VAE. LASER-NV employs permutation-invariant normalizing flows to enhance modeling capacity by utilizing multiple latent variables and incorporates a geometry-informed attention mechanism to ensure high fidelity in rendering. The empirical results demonstrate that LASER-NV achieves state-of-the-art performance in novel view synthesis across multiple datasets, including a novel City dataset, effectively handling uncertainty in unobserved regions.\n\n# Strength And Weaknesses\nOne of the main strengths of the paper is the introduction of a set-valued latent representation that significantly enhances the model's ability to capture complex scenes, surpassing the performance of previous models like NeRF-VAE and MVC-NeRF. The geometry-informed attention mechanism is a notable innovation that improves rendering fidelity and allows for better integration of context views. However, the paper does inherit some limitations from NeRF, such as computational costs, which may restrict its applicability in real-time scenarios. Additionally, while the empirical results are promising, further exploration into scalability and efficiency would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodological innovations and empirical results. The descriptions of the architecture, experiments, and evaluations are thorough, allowing for a good understanding of the contributions made. The novelty of the approach lies in its unique handling of latent representations and the attention mechanism, which are well-articulated in the paper. However, reproducibility could be a concern if the implementation details and hyperparameter settings are not made readily accessible, as these are crucial for replicating the results.\n\n# Summary Of The Review\nOverall, LASER-NV introduces a significant advancement in generative modeling for 3D scenes, particularly in handling partial observability and uncertainty. The novel set-valued latent representation and attention mechanism represent important contributions to the field. While the paper demonstrates strong empirical results, attention to computational efficiency and reproducibility would further enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces LASER-NV (Latent Set Representations for NeRF-VAE), which enhances the generative modeling capabilities of Neural Radiance Fields (NeRF) by integrating a set-valued latent representation modeled through normalizing flows. This approach addresses the visual fidelity and diversity issues faced by NeRF-VAE by incorporating a geometry-informed attention mechanism. The authors evaluate LASER-NV across multiple datasets, demonstrating superior performance in generating varied predictions of unseen views and achieving state-of-the-art quality in novel view synthesis, while also revealing the model's computational demands and reliance on accurate camera parameters.\n\n# Strength And Weaknesses\nThe strengths of the paper include the high fidelity of the generated images, the innovative use of set-valued latent representations that allow for richer modeling of complex scenes, and the model's ability to synthesize multiple plausible completions of unobserved scene parts while maintaining consistency with observed views. However, the limitations are notable: LASER-NV is computationally intensive due to the complexity of added mechanisms, it requires precise camera information which may not be readily available, and its performance can degrade in the presence of noisy data. These weaknesses suggest that while the model is promising, practical applications might be hindered by these challenges.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making it accessible to readers with a background in machine learning and computer vision. The methodology is clearly detailed, allowing for reproducibility, though the computational demands of the model may pose challenges for implementation. The novelty is evident in the proposed set-valued latent representations and the use of normalizing flows, which are not commonly applied in the context of NeRF. Overall, the clarity and quality of the presentation support the paper's contributions to the field.\n\n# Summary Of The Review\nLASER-NV presents significant advancements in the generative modeling of 3D scenes by addressing key limitations of existing NeRF models. The methodology is innovative and the empirical results are compelling, though the model's computational burden and dependence on accurate camera parameters may limit its practical applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces LASER-NV, a novel generative model that leverages set-valued latent representations for 3D scene modeling, specifically addressing the limitations of existing models like NeRF and NeRF-VAE. LASER-NV utilizes a permutation-invariant normalizing flow to model a set of latent variables, enhancing scene representation and generative quality. The methodology incorporates geometry-informed attention mechanisms, improving rendering fidelity and consistency with observed views. The experimental results demonstrate LASER-NV's superiority in reconstruction and novel view predictions across several datasets, notably outperforming MVC-NeRF and NeRF-VAE in various metrics.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to scene representation using latent sets, which provides a more robust modeling capacity compared to traditional vector-based methods. The incorporation of geometry-informed attention mechanisms is a noteworthy advancement that enhances output quality. The extensive experimental evaluation on diverse datasets, including the new City dataset, supports the claims made by the authors about the model's effectiveness. However, a potential weakness lies in the complexity of the model, which may pose challenges for reproducibility and practical implementation. The paper could also benefit from a deeper exploration of the limitations of the current approach and possible avenues for future work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, with sufficient detail on the theoretical foundations and experimental setup. The clarity of the writing facilitates understanding of complex concepts such as conditional priors and normalizing flows. Novelty is a significant strength, as the use of set-valued latent representations is an innovative contribution to the field of generative modeling in 3D. However, while the methodology is described in detail, the reproducibility may be hindered by the model's complexity and the need for specific hyper-parameter settings that are not universally applicable.\n\n# Summary Of The Review\nOverall, LASER-NV represents a meaningful advancement in 3D generative modeling, effectively overcoming several limitations of its predecessors through innovative methodologies. The experimental results are compelling, validating the proposed approach, though some concerns regarding reproducibility and model complexity remain.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents LASER-NV, a novel framework for novel view synthesis that leverages a set-valued latent representation and a geometry-informed attention mechanism. The authors demonstrate that LASER-NV achieves state-of-the-art performance in rendering fidelity while effectively handling uncertainty in unobserved regions of scenes. Comprehensive evaluations across multiple datasets indicate the model's robustness and versatility, showcasing its ability to synthesize high-quality images with fewer input views.\n\n# Strength And Weaknesses\nThe paper's strengths include its high modeling capacity, which allows for the representation of complex scenes, and its improved rendering fidelity relative to existing methods like NeRF-VAE and MVC-NeRF. The geometry-informed attention mechanism enhances consistency with observed data, while the model's ability to deal with uncertainty results in diverse completions. However, these strengths also come with limitations: the increased computational demands, challenges in real-time rendering efficiency, and potential scalability issues due to the model's complexity. Additionally, while the evaluation across multiple datasets is robust, the reliance on synthetic data raises questions about the model's performance in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, making it accessible to the reader. The quality of the results is high, with substantial empirical evidence supporting the claims made. The novelty lies in the effective integration of normalizing flows and a geometry-informed attention mechanism, which enhances the model's capacity. However, aspects such as the complexity of hyperparameter tuning and model interpretability may hinder reproducibility.\n\n# Summary Of The Review\nOverall, LASER-NV represents a significant advancement in the field of novel view synthesis, demonstrating both high modeling capacity and improved rendering quality. Despite its strengths, the paper highlights certain limitations related to computational efficiency and the use of synthetic datasets, suggesting areas for future exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents LASER-NV, a novel generative model that enhances 3D scene modeling and novel view synthesis by utilizing a set-valued latent representation. This approach allows the model to capture complex scene structures better than traditional single latent vector models. Key innovations include the use of permutation-invariant normalizing flows for efficient sampling and a geometry-aware attention mechanism that improves rendering fidelity. The results demonstrate that LASER-NV achieves state-of-the-art performance on benchmark datasets, outperforming existing models in generating diverse and plausible scene completions.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to latent representation, which significantly increases the modeling capacity for 3D scenes and its comprehensive evaluation across various datasets that highlight its robustness. The geometry-informed attention mechanism is a compelling addition that contributes to enhanced rendering quality. However, the paper does have limitations, particularly regarding computational complexity, as the advanced techniques used may inhibit real-time applications. Additionally, the reliance on accurate camera information could restrict the model's applicability in situations where such data is not available.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the contributions, methodology, and findings. The quality of the writing is high, with a logical flow that aids comprehension. The novelty of the latent set representation and the attention mechanism is significant, marking a clear advancement in the field. However, details on the reproducibility of the results, such as hyperparameter settings and dataset splits, could be improved to facilitate further research.\n\n# Summary Of The Review\nOverall, LASER-NV represents a meaningful contribution to the field of 3D generative modeling, showcasing significant advancements through its innovative latent representation and attention mechanism. While it offers impressive performance and quality, the computational demands and reliance on precise camera parameters may limit its practical applications.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces LASER (Latent Set Representations), a novel framework designed to enhance the robustness and effectiveness of generative models against adversarial attacks. The key contributions include the development of a set-valued latent representation that enables better generalization across diverse data distributions, the integration of normalizing flows for efficient sampling from complex distributions, and the introduction of a geometry-informed attention mechanism that conditions the model on observed data. Empirical results demonstrate that LASER achieves state-of-the-art performance in adversarial robustness while generating high-quality samples across multiple benchmark datasets.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to adversarial training, particularly through the use of latent set representations and normalizing flows, which significantly enhance the model's ability to handle multimodal distributions. The incorporation of an attention mechanism is also a notable improvement, as it allows for better alignment with real data. However, the paper could benefit from a more thorough discussion of potential limitations and future research directions, particularly regarding the generalizability of the approach across various data types and adversarial scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with a comprehensive background that contextualizes the need for robust generative models in the face of adversarial attacks. The quality of the methodology is high, with detailed descriptions of the architecture and experimental setup. However, while the novelty of the approach is significant, the reproducibility aspect could be strengthened by providing more extensive details on the training process and hyperparameter settings, as well as making the code publicly available.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in adversarial training for generative models through the introduction of LASER. Its innovative use of latent set representations, normalizing flows, and attention mechanisms contributes significantly to the field, although the authors should address potential limitations and enhance reproducibility to further validate their findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents LASER-NV, a novel generative model designed for high-fidelity 3D scene synthesis, which employs a set-valued latent representation and normalizing flows. It claims to redefine probabilistic scene modeling by providing unprecedented modeling capacity and inference speed, thereby surpassing existing methods such as NeRF and NeRF-VAE. The extensive evaluation demonstrates that LASER-NV achieves superior performance across various datasets, suggesting a significant advancement in the field of 3D generative modeling.\n\n# Strength And Weaknesses\nOne of the main strengths of the paper is its introduction of the set-valued latent representation, which is claimed to dramatically enhance the model's capacity for 3D scene interpretation. Additionally, the novel attention mechanism allows for improved context conditioning, potentially opening new avenues for generative modeling. However, the paper's critique of prior works, particularly NeRF and NeRF-VAE, appears superficial and may undermine the authors' claims. The reported results, while impressive, could benefit from more comprehensive comparisons and discussions regarding the significance of the improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the overstated claims regarding the superiority of LASER-NV over existing models, which may mislead readers about the actual advancements made. While the methodology involving normalizing flows is presented as innovative, it could be elaborated further to enhance reproducibility. The paper does not provide sufficient details on experimental setups or hyperparameter tuning, which is critical for validating the claims made about the model's performance.\n\n# Summary Of The Review\nOverall, the paper positions LASER-NV as a potentially transformative model in the field of 3D generative modeling, with significant contributions in latent representation and attention mechanisms. However, the lack of a balanced discussion of prior works and insufficient methodological details may hinder the reader's ability to fully appreciate the advancements claimed.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces LASER-NV, a generative model that leverages latent set representations based on normalizing flows to enhance fidelity in novel view synthesis, outperforming traditional methods such as NeRF and NeRF-VAE. The methodology involves developing a novel attention mechanism that conditions the scene function on both latent sets and local features, aiming to address the limitations of requiring extensive training views for accurate scene representation. The experimental results demonstrate LASER-NV's superior performance across various metrics, including log-likelihood, PSNR, SSIM, and FID on datasets like ShapeNet and a new simulated City dataset.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel contributions to generative modeling through the introduction of a set-valued latent representation, which significantly enhances modeling performance. The attention mechanism for conditioning further improves visual quality, which is well-supported by empirical results showing state-of-the-art performance against existing models. However, a potential weakness is the reliance on synthetic datasets for evaluation, which may limit the generalizability of the findings to real-world applications. Additionally, while the improvements over previous models are substantial, the paper could benefit from a more exhaustive exploration of the model's limitations and potential failure cases.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, presenting its contributions and findings clearly. The methodology is described in sufficient detail, making it relatively easy for readers to understand the approach taken. The novelty of the set-valued latent representation and attention mechanism is significant, as it introduces new avenues for generative modeling of 3D scenes. Reproducibility is supported by detailed experimental setups and clear metrics, although the reliance on simulated datasets raises questions about the real-world applicability of the results.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of 3D generative modeling through the innovative LASER-NV model, which demonstrates superior performance in novel view synthesis. The clarity of presentation and empirical results contribute to a solid understanding of the model's capabilities, although further exploration of real-world applicability would strengthen the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework, LASER-NV, aimed at enhancing scene rendering quality through the use of high-dimensional latent representations, normalization flows, and a geometry-informed attention mechanism. The methodology hinges on conditioning the model on context views and utilizing a probabilistic approach to generate multimodal predictions for unobserved parts of the scene. Key findings suggest that the proposed model achieves improved rendering quality compared to baseline methods, although the authors acknowledge limitations related to generalization and performance metrics.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include its innovative use of geometry-informed attention and the introduction of a probabilistic approach for rendering, which may enhance diversity in predictions. However, the paper also has notable weaknesses. The assumption that increasing the number of latent variables will always enhance modeling capacity is questionable, as it may lead to overfitting. Furthermore, the reliance on normalizing flows and the efficacy of conditioning on context views are contingent on specific design choices and dataset characteristics, which may not generalize well across varied scenarios. The evaluation metrics used do not fully capture perceptual quality, and the paper underrepresents the inherent limitations of NeRF, raising concerns about the robustness and applicability of the proposed method in real-world settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear in its presentation of methodology and findings. However, certain assumptions and limitations could be articulated more explicitly. The novelty lies in the combination of techniques employed, although some aspects, such as the use of probabilistic outputs, are not entirely new to the field. Reproducibility may be a concern due to the reliance on specific datasets and the requirement for precise camera parameters, which may not always be available in practical applications.\n\n# Summary Of The Review\nOverall, the paper introduces interesting ideas to enhance scene rendering but suffers from several assumptions that may limit its applicability and generalization. While the proposed approach shows promise, the concerns about overfitting, generalization, and evaluation metrics suggest that further investigation is needed.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces LASER-NV, a generative model designed for enhanced 3D scene representation by utilizing latent set representations. This model addresses the limitations of existing methods, particularly NeRF-VAE, by improving the fidelity of rendering unobserved scene parts while maintaining consistency with observed views. The methodology involves a conditional generative model that integrates local context features through advanced attention mechanisms, and the loss function is crafted to balance reconstruction quality with KL divergence, facilitating efficient learning. Experimental results demonstrate superior performance across various datasets, particularly in high-uncertainty environments, confirming the model's effectiveness in reconstructing and predicting novel views.\n\n# Strength And Weaknesses\nThe main strengths of the paper include the innovative use of latent set representations, which significantly enhance the model's ability to generalize to unobserved areas. The integration of context views and the attention mechanisms are well-justified and contribute to the overall improvement in visual fidelity. The thorough ablation studies provide valuable insights into the individual contributions of model components. However, a potential weakness is the reliance on specific datasets for evaluation, which may limit the generalizability of the findings. Additionally, while the improvements in rendering quality are notable, the paper could benefit from a more extensive discussion on the computational efficiency of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem it addresses, the proposed solution, and the evaluation methodology. The quality of writing is high, with a logical flow that aids comprehension. The novelty of LASER-NV is significant, as it proposes a new approach to generative modeling in 3D scenes. However, reproducibility could be enhanced by providing more details on the implementation and hyperparameter settings, as well as making the code publicly available.\n\n# Summary Of The Review\nOverall, LASER-NV presents a compelling advancement in the field of generative modeling for 3D scenes, demonstrating significant improvements in rendering fidelity and generalization to unobserved areas. While the paper is clear and well-executed, further discussion on computational efficiency and the availability of code for reproducibility would strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to addressing the challenges of unsupervised representation learning by introducing a self-supervised framework that leverages contrastive learning techniques. The authors propose a method that combines multiple complementary loss functions to enhance feature extraction from unlabelled data. Through extensive experiments on various benchmark datasets, the findings demonstrate that the proposed method outperforms existing state-of-the-art techniques in terms of representation quality, achieving improved performance across downstream tasks such as classification and clustering.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of multiple loss functions, which significantly contributes to improved representation learning. The experimental results are compelling, showcasing clear advantages over baseline methods. However, a notable weakness is the lack of detailed explanations regarding the theoretical justifications for the chosen loss functions and their interactions. Additionally, the paper could benefit from a more comprehensive comparison with recent advancements in the area, as certain relevant works are not adequately discussed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, with a logical progression of ideas that facilitate understanding of the proposed framework. While the methodology is innovative, some technical terms and concepts may require clarification for readers who are less familiar with the domain. The reproducibility of the experiments is somewhat supported by the inclusion of hyperparameter settings; however, a more detailed account of the training process and stability could further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of unsupervised representation learning, presenting an innovative framework with promising empirical results. Addressing the theoretical foundations and providing a more exhaustive comparison with related work would strengthen the paper's impact and clarity.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces Latent Set Representations for NeRF-VAE (LASER-NV), a novel generative model aimed at enhancing 3D scene representation and view synthesis. The methodology incorporates a set-valued latent representation modeled by normalizing flows, which allows the model to learn and infer structures from multiple scenes with minimal input views. Additionally, a geometry-informed attention mechanism is employed to improve the fidelity of rendering, particularly in completing occluded areas while ensuring consistency with the observed data. The findings demonstrate that LASER-NV achieves state-of-the-art performance on both the ShapeNet and a newly simulated City dataset, showcasing its ability to generate sharp, multi-modal outputs for unobserved parts of scenes.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to representing complex 3D scenes through a set-valued latent space, which significantly enhances the model's capacity for conditional generation. The integration of a geometry-informed attention mechanism is a notable contribution that helps improve rendering quality. However, the model still inherits some computational challenges from its predecessor, NeRF, which may limit its scalability in practical applications. Additionally, while LASER-NV shows strong empirical performance, the potential trade-offs in computational efficiency are not thoroughly addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations behind the design choices, making the contributions understandable to the reader. The quality of the writing is high, with detailed explanations of the methodology and results. The novelty of the proposed latent representation and attention mechanism is significant, contributing to the advancement of generative modeling in 3D scenes. However, the reproducibility of the results could be enhanced by providing more details on the implementation and training procedures, as well as the datasets used.\n\n# Summary Of The Review\nOverall, LASER-NV presents a compelling advancement in generative modeling for 3D scenes, effectively addressing limitations of previous methods and demonstrating strong performance in generating unobserved scene parts. Despite some computational challenges, the innovative methodological contributions mark a significant step forward in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces LASER-NV, a novel generative model that utilizes latent set representations to enhance novel view synthesis in 3D scenes. It addresses significant limitations of existing models like Neural Radiance Fields (NeRF) by employing a set-valued latent representation modeled through normalizing flows, alongside a geometry-informed attention mechanism. The authors evaluate LASER-NV on various datasets, including ShapeNet and a novel simulated City dataset, demonstrating state-of-the-art performance in reconstruction quality and diversity of synthesized views.\n\n# Strength And Weaknesses\nThe contributions of LASER-NV are noteworthy, particularly in its innovative approach to handling partial observations in 3D environments through latent set representations. The incorporation of context from multiple views and a novel attention mechanism enhances the model's generative capabilities, leading to improved fidelity in reconstructed scenes. However, the paper does not sufficiently address the computational costs associated with volumetric rendering, which may limit practical applications. Additionally, while the results are promising, further exploration into the model's scalability and robustness in more complex scenarios would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and contributions, making it accessible to readers familiar with the domain. The quality of the experimental evaluation is high, with comprehensive comparisons and ablation studies that effectively demonstrate the model's advantages. The novelty of the approach, particularly in combining probabilistic modeling with attention mechanisms, is significant. However, details regarding the implementation and hyperparameter settings could be elaborated upon to enhance reproducibility.\n\n# Summary Of The Review\nOverall, LASER-NV presents a compelling advancement in generative modeling for 3D scenes, effectively addressing key challenges faced by prior models. The innovative use of latent set representations and attention mechanisms offers a promising direction for future research, although practical considerations regarding computational efficiency warrant further investigation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"LASER: Latent Set Representations for 3D Generative Modeling\" introduces a novel framework for 3D generative modeling that leverages set-valued latent representations combined with a geometry-informed attention mechanism. The proposed LASER-NV architecture incorporates normalizing flows to enhance the latent representation and effectively integrates local features through attention. The authors conduct extensive experiments on various datasets, including ShapeNet, MultiShapeNet, and a new City dataset, demonstrating significant performance improvements over baseline models such as MVC-NeRF and NeRF-VAE, as evidenced by quantitative metrics like PSNR, SSIM, and FID.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to 3D generative modeling, particularly through the introduction of set-valued latent representations and the integration of an attention mechanism that improves feature extraction. The comprehensive evaluation against established baselines further strengthens the paper's contributions, providing a clear demonstration of the advantages of LASER. However, the paper could benefit from a deeper exploration of computational efficiency and practical applications, as well as a more thorough discussion of the limitations related to the reliance on accurate camera information and computational costs.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, facilitating understanding of the proposed methodologies and results. Technical terms are adequately defined, and the logical flow from introduction to conclusion is maintained throughout the text. The experiments are well-organized, and the results are presented with appropriate visualizations that support the findings. While the approach is novel and the significance is evident, further elaboration on reproducibility aspects, such as code and data availability, would enhance the paper's impact.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of 3D generative modeling through the introduction of LASER, which combines set-valued latent representations with an attention mechanism. The methodology is robust, and the empirical results demonstrate clear benefits over existing approaches. However, considerations regarding computational efficiency and practical implementation could be further developed.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces LASER-NV (Latent Set Representations for 3D Generative Modeling), a novel approach that enhances Neural Radiance Fields (NeRF) by leveraging latent set representations through normalizing flows. The methodology incorporates a geometry-informed attention mechanism, allowing the model to synthesize high-fidelity 3D scenes even under sparse viewing conditions. Findings demonstrate that LASER-NV outperforms existing methods like MVC-NeRF and NeRF-VAE in terms of reconstruction quality and novel view synthesis, particularly in challenging contexts with high uncertainty and occlusion variability.\n\n# Strength And Weaknesses\nThe main strength of LASER-NV lies in its ability to effectively model 3D scenes with latent variables that capture complex data distributions. The integration of a geometry-informed attention mechanism significantly boosts rendering fidelity and contextual relevance, which is a notable advancement in the field. However, the paper could benefit from a more thorough exploration of the computational efficiency and scalability of the proposed model, as well as potential limitations in certain applications. The experimental design is solid, but the inclusion of more diverse datasets could enhance the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and results, making it accessible to readers familiar with generative modeling and computer vision. The novelty of the approach is evident in the integration of latent set representations and the unique attention mechanism. However, while the theoretical components are well-explained, details regarding the implementation and hyperparameter tuning could benefit from further elaboration to enhance reproducibility.\n\n# Summary Of The Review\nOverall, LASER-NV presents a significant contribution to 3D generative modeling by addressing limitations in existing NeRF frameworks. The proposed method demonstrates strong empirical results, although further work is needed to clarify its computational aspects and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces LASER-NV, a novel model aimed at enhancing neural scene representation using a set-valued latent representation. The methodology involves leveraging normalizing flows and a geometry-informed attention mechanism to improve the modeling capacity of the architecture. The findings suggest that LASER-NV outperforms NeRF-VAE in terms of certain metrics; however, the paper does not convincingly demonstrate the practical significance of this improvement, especially considering the model's computational demands and reliance on synthetic data for evaluation.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to latent representation and the introduction of a geometry-informed attention mechanism. However, these contributions appear to complicate the model without delivering substantial benefits over simpler architectures. The heavy computational requirements and extensive training data needed for LASER-NV limit its applicability, particularly in real-time scenarios. Additionally, the evaluation on the synthetic City dataset raises concerns about the generalizability of the findings, and the ablation studies fail to clarify the trade-offs associated with different model components.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel approach, it suffers from clarity issues, particularly regarding the implementation details of its components. The vagueness in describing the geometry-informed attention mechanism may lead to overfitting in noisy environments. Furthermore, the metrics employed for evaluation, such as PSNR and FID, do not adequately capture the quality of generated scenes in multimodal contexts. The reproducibility of the results is questionable, given the reliance on synthetic data and the lack of clear ablation analysis.\n\n# Summary Of The Review\nOverall, while LASER-NV proposes interesting ideas in the realm of neural scene representation, it falls short in addressing practical limitations and lacks convincing empirical validation. The complexities introduced do not seem justified by the reported improvements, leading to skepticism about the model's robustness and applicability in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents LASER-NV, an innovative model for 3D generative modeling that employs a set-valued latent representation using normalizing flows. Its main contributions include a high modeling capacity that allows for diverse and plausible scene completions, a geometry-informed attention mechanism for enhanced rendering fidelity, and rapid feed-forward inference capabilities. The experimental results demonstrate state-of-the-art performance on benchmark datasets such as ShapeNet and a novel simulated City dataset, showcasing robustness and adaptability across various applications.\n\n# Strength And Weaknesses\nThe strengths of LASER-NV lie in its groundbreaking approach to 3D scene representation, particularly its probabilistic treatment that allows for multiple model predictions and its ability to condition on context views to enhance visual quality and diversity. The rapid inference speed and the model’s effectiveness in data-scarce scenarios are also significant advantages. However, weaknesses may include a potential over-reliance on specific datasets which could limit generalizability and the need for extensive computational resources for training, which could hinder accessibility for some practitioners.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, making it accessible to readers with varying levels of expertise. The quality of the model is evidenced by its state-of-the-art results, and the novelty is marked by the introduction of a set-valued latent representation and the integration of geometry-informed attention. While the methodology appears reproducible, further details on implementation and hyperparameter tuning would enhance clarity and enable easier replication of results.\n\n# Summary Of The Review\nOverall, LASER-NV represents a significant advancement in 3D generative modeling, combining innovative methodologies with high performance across diverse applications. Its ability to synthesize realistic and varied scene completions positions it as a leading model in the field, although some limitations regarding generalizability and practical implementation may warrant further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents LASER-NV, a generative model that utilizes set-valued latent representations formulated through normalizing flows to enhance 3D scene modeling. By leveraging a framework of probabilistic modeling, the authors propose an architecture that captures complex data distributions while maintaining computational efficiency. Key contributions include the introduction of an arbitrarily-sized latent set, an attention-driven scene function, and a probabilistic treatment of generative outputs that allows for uncertainty quantification in complex 3D scenes.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative approach to latent representations, which enhances the model's expressiveness and ability to capture multimodal distributions. The incorporation of an attention mechanism is also a notable strength, as it facilitates a nuanced interaction between latent features and observed data. However, a potential weakness is the complexity of the model, which may hinder interpretability and practical applicability in real-world scenarios. Furthermore, while the theoretical framework is robust, empirical validation of the proposed model's effectiveness in various applications could be further explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, with a logical flow from theoretical foundations to methodological innovations. The quality of writing is high, making complex concepts accessible to the reader. The novelty of using set-valued latent representations in generative modeling is significant, offering a fresh perspective in the field. However, reproducibility may be challenging due to the intricate nature of the model and the potential need for specific configurations that are not fully detailed in the paper.\n\n# Summary Of The Review\nOverall, LASER-NV represents a significant theoretical advancement in generative modeling for 3D scenes, with a novel approach to latent representations that enhances model expressiveness and uncertainty quantification. While the paper is well-articulated and presents high-quality contributions, further empirical validation and clarity on reproducibility aspects could strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces LASER-NV, an advanced generative model that utilizes a set-valued latent representation implemented through normalizing flows, improving upon the existing NeRF-VAE architecture. Key contributions include the incorporation of a permutation-invariant normalizing flow to model latent sets, as well as a novel attention mechanism that effectively integrates local features from context views with latent representations. The model is evaluated on several datasets, including ShapeNet and a new City dataset, demonstrating significant improvements in novel view synthesis quality measured by metrics such as PSNR, SSIM, and FID.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to combining latent set representations with attention mechanisms, leading to enhanced generative modeling capabilities. The ablation studies conducted provide valuable insights into the significance of different model components, and the results are well-supported by performance metrics across diverse datasets. However, a notable weakness is the lack of explicit code availability, which may hinder reproducibility. Additionally, while the paper provides extensive implementation details, the broader implications and applications of the LASER-NV model are not fully explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written with a clear structure, effectively detailing the methodology and results. The quality of the experiments and metrics used to evaluate performance is high, showcasing the model's capabilities. The technical novelty is evident in the proposed combinations of existing concepts in new ways. However, the reproducibility is questionable due to the absence of a clear commitment to releasing the code, which could limit the community's ability to build upon this work.\n\n# Summary Of The Review\nOverall, LASER-NV presents a significant advancement in generative modeling with its unique approach to latent representations and attention mechanisms. While the methodology is robust and the experimental results are promising, the lack of code availability raises concerns regarding reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents LASER-NV, a novel architecture for generating high-fidelity visual reconstructions, claiming to outperform NeRF-VAE in terms of visual quality. The authors argue that LASER-NV addresses limitations of previous deterministic models, such as MVC-NeRF, by employing a probabilistic treatment over latent sets and utilizing attention mechanisms to improve visual fidelity. The results indicate that LASER-NV achieves state-of-the-art performance on the City dataset, although the dataset's synthetic nature raises questions about the generalizability of these findings.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative use of probabilistic modeling and attention mechanisms, which are presented as enhancements to visual quality. However, the reliance on comparisons with NeRF-VAE, which is criticized for producing blurry reconstructions, weakens the assessment of LASER-NV's unique contributions. The authors fail to adequately explore the merits of deterministic models, such as MVC-NeRF, and do not provide a thorough evaluation of how their architecture compares to existing methods in practical applications. Additionally, while LASER-NV's performance metrics, including PSNR and FID scores, are impressive, the lack of discussion on the applicability of these metrics in various contexts raises concerns about the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, but it lacks comprehensive comparative analyses that would enhance its contributions and contextual understanding. The novelty of LASER-NV is partially undermined by the absence of detailed discussions regarding existing models that share similar features. Reproducibility is not thoroughly addressed, as the computational trade-offs and efficiency implications of using larger latent sets are not explored, which is essential for practical implementation.\n\n# Summary Of The Review\nWhile LASER-NV presents interesting advancements in visual reconstruction through probabilistic modeling and attention mechanisms, its reliance on potentially biased comparisons and insufficient exploration of competing models diminish its overall impact. The findings, although promising within a synthetic dataset context, lack a broader applicability discussion, which is crucial for real-world deployment.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Latent Set Representations for 3D Generative Modeling\" introduces a novel generative modeling framework that leverages latent set representations to enhance 3D data synthesis. The authors propose a methodology that integrates techniques from neural radiance fields (NeRF) and variational autoencoders (VAE) to improve the fidelity and diversity of generated 3D objects. Key findings indicate that their approach achieves lower Fréchet Inception Distance (FID) scores compared to existing models, demonstrating significant improvements in both quality and diversity of generated outputs.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative combination of latent set representations with established generative modeling frameworks, which addresses notable gaps in the current methodologies. The empirical results showcase clear advantages over baseline models, emphasizing the model's ability to capture complex 3D structures. However, the paper also exhibits weaknesses, such as inconsistencies in terminology and grammatical errors that may hinder reader comprehension. Additionally, the lack of detailed explanations for specific technical terms may limit accessibility for a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper suffers from several formatting issues and typographical errors, which detract from the overall quality of the presentation. The novelty of the proposed approach is significant, as it successfully merges different techniques to advance the field of 3D generative modeling. However, reproducibility could be compromised due to incomplete methodological descriptions and inconsistencies in notation. More attention to detail in the methodology and clearer definitions of terms would enhance the overall quality of the paper.\n\n# Summary Of The Review\nOverall, the paper presents a noteworthy contribution to the field of 3D generative modeling through the introduction of latent set representations. While the empirical results are promising and demonstrate the potential of the approach, issues with clarity and reproducibility need to be addressed for the work to be fully impactful.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents LASER-NV, a novel generative model aimed at enhancing 3D scene generation. The authors propose a unique architecture that integrates geometry-informed attention mechanisms to improve rendering quality. Through a series of experiments on a simulated City dataset, the model demonstrates promising results, showcasing its capability to handle varying complexities of 3D scenes.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to integrating attention mechanisms which may improve the model's performance on complex scenes. The architecture appears to offer potential advantages over traditional methods in generating high-quality 3D representations. However, several weaknesses are noted, including the limited evaluation scope restricted to a single simulated dataset. The lack of robustness testing against various noise levels in camera poses and the absence of comparisons with recent generative models diminish the comprehensiveness of the findings. Additionally, the paper does not address critical aspects such as real-time applicability, interpretability of latent representations, and potential biases in training datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of the model architecture and methodology. However, the clarity could be improved by including a more detailed discussion of the trade-offs between computational efficiency and rendering quality. The novelty is evident in the proposed architecture, but the reproducibility could be hindered by the lack of a thorough exploration of dataset biases and limitations in the training methodology. The absence of broader evaluations restricts the reader's ability to fully assess the model's applicability in real-world scenarios.\n\n# Summary Of The Review\nOverall, while LASER-NV presents a promising approach to 3D scene generation, the paper’s contributions are somewhat limited by its evaluation scope and lack of robustness analysis. Addressing these gaps could significantly enhance the model's applicability and relevance to practical applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces LASER-NV, a generative model designed for 3D scene representation using a set-valued latent representation. The authors evaluate LASER-NV against several baseline models, including NeRF-VAE and MVC-NeRF, utilizing metrics such as PSNR, SSIM, and FID to demonstrate its effectiveness in reconstruction quality and novel view synthesis across various datasets (City, ShapeNet, MSN-Hard). The findings indicate that LASER-NV outperforms these models, showcasing superior robustness to dataset size and noise levels, although the authors do not provide formal statistical significance testing for their results.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to 3D scene representation and its thorough evaluation across multiple datasets, which enhances the generalizability of the findings. The use of a set-valued latent representation is a notable contribution that addresses limitations of existing models. However, a significant weakness is the lack of explicit statistical significance testing, which undermines the robustness of the claims made regarding performance improvements. While the paper discusses improvements in metrics, the absence of confidence intervals or p-values leaves the conclusions somewhat unsubstantiated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, effectively communicating the methodology and findings. The quality of the experimental design is commendable, particularly the comprehensive ablation studies and data efficiency analysis. However, the novelty primarily rests on the methodology rather than on groundbreaking empirical results, which may limit the perceived significance of the contributions. The reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setup and any additional hyperparameters used in training.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to 3D scene representation with LASER-NV, showing strong performance in various evaluations. However, the absence of formal statistical significance testing raises concerns about the robustness of the findings. Future work should address this gap to strengthen the claims made.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents the LASER-NV model, which aims to improve novel view synthesis in complex scenes. The methodology involves leveraging volumetric rendering techniques, building on previous work with NeRF-VAE, to enhance the fidelity of rendered outputs. The findings indicate that while LASER-NV shows some improvements over its predecessor, it still struggles with fidelity in rendering unobserved parts of scenes and is hindered by high computational costs, limiting its application for real-time systems.\n\n# Strength And Weaknesses\nThe paper demonstrates notable advancements in the area of novel view synthesis through the LASER-NV model, indicating potential for future research. However, it also reveals several weaknesses, such as reliance on accurate ground-truth camera information, which restricts its usability in realistic scenarios lacking such data. Additionally, the model's performance is tied to specific datasets, primarily simulated environments, which raises questions about its generalizability and robustness against noise in camera poses. The challenges in generating multimodal predictions for unobserved scene parts suggest that further refinement is necessary.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and articulates its contributions clearly, making it relatively easy to follow. However, the limitations surrounding the scalability and flexibility of the model due to its dependence on depth information might hinder reproducibility in diverse scenarios. The novelty of the LASER-NV model is apparent, but additional empirical validation in real-world contexts would bolster its significance and confidence in its findings.\n\n# Summary Of The Review\nOverall, while the LASER-NV model showcases promising advancements in novel view synthesis, its limitations regarding computational costs, dependence on accurate camera data, and performance on unobserved scene parts necessitate further exploration. Future work should address these challenges to improve the model's applicability and robustness in real-world scenarios.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents LASER (Latent Set Representations), a model aimed at addressing limitations in Neural Radiance Fields (NeRF) by proposing a novel \"Latent Set Representation.\" This approach leverages normalizing flows to enhance modeling capacity and aims to improve rendering of unobserved parts of a scene. The authors conduct experiments on well-known datasets such as ShapeNet and City, reporting superior performance metrics like FID and PSNR, while also performing ablation studies to analyze the contributions of various model components.\n\n# Strength And Weaknesses\nOne strength of the paper is its systematic exploration of the contributions of different components through ablation studies, which is a standard practice in the field. However, the paper suffers from a lack of novelty, as many of the ideas presented seem to rehash existing concepts without significant innovation. The experimental setup appears limited, relying on familiar datasets and benchmarks that do not contribute to a compelling argument for the proposed model’s significance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the repetitiveness of the concepts discussed detracts from its overall quality. The novelty of the approach is questionable, as many of the insights appear trivial and lack depth. While the methodology is reproducible, the reliance on conventional datasets and metrics limits the impact of the findings.\n\n# Summary Of The Review\nOverall, the LASER paper presents a familiar approach to 3D generative modeling that lacks significant innovation or depth. The contributions are overshadowed by a sense of redundancy and an absence of groundbreaking insights, making it less impactful within the current landscape of research.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces LASER-NV, a novel approach to 3D generative modeling that employs Latent Set Representations for NeRF-VAE, enhancing the expressiveness and flexibility of generative models. The methodology leverages normalizing flows to model latent sets and incorporates a geometry-informed attention mechanism to improve spatial relationship understanding. The findings indicate that increasing the size of the latent set leads to improved modeling performance, particularly in terms of visual quality when compared to existing methods such as NeRF-VAE and MVC-NeRF.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative use of latent set representations and the integration of a geometry-informed attention mechanism, which provides a significant improvement over previous methods. Additionally, the evaluation of LASER-NV on various datasets, including a novel City dataset, demonstrates its applicability in different contexts. However, weaknesses are evident in the reliance on ground-truth camera information, which may limit the model's utility in real-world applications. Furthermore, the exploration of alternative probabilistic models and advanced attention mechanisms is limited and could enhance the model's overall performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly. The quality of the methodology is high, although there are areas where further exploration could enhance clarity and depth, particularly regarding the model's limitations and the trade-offs involved in latent set size. The novelty of the approach is commendable, but the reproducibility could be improved by providing more comprehensive details on the experimental setup and data handling.\n\n# Summary Of The Review\nOverall, LASER-NV presents a compelling approach to 3D generative modeling that shows promise in enhancing the expressiveness of generative models through latent set representations. However, the paper would benefit from addressing its reliance on specific data structures and exploring more diverse datasets and advanced methodologies to strengthen its applicability and robustness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents LASER-NV, a novel approach for novel view synthesis, which demonstrates substantial improvements over existing methods such as MVC-NeRF and NeRF-VAE across various datasets. The methodology focuses on leveraging efficient latent space representations to generate high-fidelity reconstructions and plausible predictions of unseen views. Key findings indicate that LASER-NV achieves superior performance in terms of Fréchet Inception Distance (FID), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM), particularly in challenging environments such as the MSN-Hard dataset.\n\n# Strength And Weaknesses\nStrengths of the paper include its demonstrated superiority in benchmark performance across multiple datasets, showcasing LASER-NV's capacity to produce higher quality predictions and reconstructions compared to competing models. The method also exhibits resilience to variations in training data size, maintaining performance with fewer training scenes, which is a significant advantage in real-world applications. However, a potential weakness is the lack of detailed exploration of the underlying reasons for LASER-NV’s improvements, which could provide further insights into its operation and applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and results. The quality of the figures and results is high, aiding in the understanding of the model's performance. The novelty of LASER-NV lies in its effective use of latent space for novel view synthesis, which is a meaningful advancement in the field. However, the reproducibility of the results could be strengthened by providing more comprehensive details on implementation specifics and hyperparameter settings.\n\n# Summary Of The Review\nOverall, LASER-NV establishes a new benchmark for novel view synthesis tasks, demonstrating significant improvements in performance and data efficiency across multiple datasets. While the paper is clear and well-organized, further insights into the model's mechanisms would enhance the understanding of its advantages.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents a novel approach to enhancing neural rendering techniques by integrating Variational Autoencoders (VAEs) into Neural Radiance Fields (NeRFs), referred to as NeRF-VAE. The authors propose a framework called LASER-NV that combines the strengths of both methodologies to improve scene representation and rendering efficiency. Through extensive experiments on various datasets, the authors demonstrate that LASER-NV outperforms existing methods in terms of visual quality and computational efficiency, providing compelling evidence for its effectiveness in real-time applications.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative integration of VAEs with NeRFs, which addresses critical limitations of traditional rendering techniques, such as speed and quality. The empirical results are robust, showcasing a significant improvement over baseline models. However, the paper could be strengthened by providing clearer definitions and explanations of key terminologies and methodologies, as well as ensuring that all figures and tables are adequately described. The abstract and introduction could also benefit from improved clarity to make the contributions more accessible to a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper generally maintains high clarity and quality; however, several areas need improvement. The dense language in the abstract may deter readers, and the introduction lacks clear structural guidance. The paper's novelty is evident through its unique approach, but the complex jargon and inconsistent terminology usage may hinder reproducibility for those less familiar with the field. Additionally, the figures require better captions to facilitate understanding. Overall, while the methodology is sound, enhancing clarity and consistency would improve the paper's overall impact.\n\n# Summary Of The Review\nIn summary, the paper offers a significant contribution to the field of neural rendering by introducing LASER-NV, which combines VAEs with NeRFs for improved performance. Despite some clarity and structural issues, the innovative approach and strong empirical results make this work a valuable addition to the literature.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.1770257858956326,
    -1.7570114877146272,
    -1.8536800142548047,
    -1.8215300301962483,
    -1.8933924392634736,
    -1.7635181143290886,
    -1.4389124097338128,
    -1.7418823780975168,
    -1.7781858229247414,
    -1.8874429419957806,
    -1.654869817223187,
    -1.5006447197066026,
    -1.7867659951635042,
    -1.6999689566846705,
    -1.6849892219043745,
    -1.7702601697787825,
    -1.9101865143290067,
    -1.9188014706658658,
    -1.6243702134760611,
    -1.7929394935526197,
    -1.9043364478059517,
    -1.5183217386259942,
    -1.8295405964055318,
    -1.696276596258742,
    -1.9596465579120725,
    -1.922266404009639,
    -1.7813566575270938,
    -1.666941236139534,
    -1.6516537426481925
  ],
  "logp_cond": [
    [
      0.0,
      -2.016101176677722,
      -2.0379793699974313,
      -2.0190593453598464,
      -2.0311048684850004,
      -2.0348696096234646,
      -2.072899861599186,
      -2.0401023095740256,
      -1.9940346531241602,
      -2.0713587687185955,
      -2.0321474856500323,
      -2.103369291470411,
      -2.0045536786646068,
      -2.0289249437510746,
      -2.0083818887625036,
      -2.0448459905444754,
      -2.018128800389017,
      -2.017594993839297,
      -2.038433479723585,
      -2.0073423415845433,
      -2.0377472784684203,
      -2.069784090709843,
      -2.059347738803899,
      -2.0183206985588167,
      -2.05741746403503,
      -2.040435617410016,
      -2.029531635918017,
      -2.0796422160425463,
      -2.0929722011110767
    ],
    [
      -1.4261870054724297,
      0.0,
      -1.3167168697024063,
      -1.1419236053364032,
      -1.230359786094747,
      -1.24040681365387,
      -1.4121688262214378,
      -1.3485000758856525,
      -1.2872743483680915,
      -1.3092136358296074,
      -1.2924758434900587,
      -1.5210788754024174,
      -1.2274001129317629,
      -1.2619053740615085,
      -1.2919907169251164,
      -1.2782118404804177,
      -1.2983671765265201,
      -1.2806623818977163,
      -1.3342487181691982,
      -1.2672276416568418,
      -1.3368216873345566,
      -1.470119308560584,
      -1.3791836093497565,
      -1.335714712396477,
      -1.4242644395638588,
      -1.4226959587490218,
      -1.3030932321814506,
      -1.4155081272369456,
      -1.4630740993489875
    ],
    [
      -1.52043284229271,
      -1.3023703160334668,
      0.0,
      -1.3004784501344473,
      -1.2824143146915679,
      -1.3081767481998123,
      -1.4397857555783407,
      -1.393382761713565,
      -1.3617349698215988,
      -1.332924229648388,
      -1.330969702704884,
      -1.5353956321623692,
      -1.2378885388387113,
      -1.3243598754567218,
      -1.29142834602123,
      -1.3091723491610967,
      -1.36255404340524,
      -1.358046392849159,
      -1.3995261224598723,
      -1.3639301958957157,
      -1.4064724126882213,
      -1.503475964630828,
      -1.4226776814257907,
      -1.411156745362738,
      -1.3772292041534955,
      -1.4232646574398062,
      -1.284323025656636,
      -1.443850290837092,
      -1.4734454203201979
    ],
    [
      -1.4825146291345523,
      -1.2226938984009061,
      -1.3512164383809682,
      0.0,
      -1.338789235989698,
      -1.3571112995625194,
      -1.4619022338230496,
      -1.3955770011994924,
      -1.3157109643928782,
      -1.3931497738092489,
      -1.3651318173933384,
      -1.5348730414918832,
      -1.3187929772212705,
      -1.3197462915121085,
      -1.330566627516573,
      -1.313996764856421,
      -1.3516312109653361,
      -1.3564769811124249,
      -1.3606849783102049,
      -1.3375899740288555,
      -1.3870789845141163,
      -1.5190001701075417,
      -1.4206559325596935,
      -1.3734909815714074,
      -1.4935976724794129,
      -1.4577448585689796,
      -1.3105485485481483,
      -1.4263060432400674,
      -1.496266530437039
    ],
    [
      -1.490150669218299,
      -1.219227438909894,
      -1.3217718547978834,
      -1.3098390539716758,
      0.0,
      -1.3382786505293252,
      -1.4580632805828382,
      -1.3784316752784953,
      -1.3576377328535623,
      -1.3539135300752714,
      -1.3979582529744494,
      -1.6045806769868411,
      -1.2946030645788296,
      -1.3332748971269273,
      -1.3560554102490543,
      -1.312660809082448,
      -1.295572920567557,
      -1.3274047049067677,
      -1.414187088284333,
      -1.358703191330633,
      -1.3817992346969483,
      -1.5528280170605657,
      -1.4419600275155824,
      -1.4002485965554745,
      -1.4305964073870252,
      -1.48494891481563,
      -1.3222667440459956,
      -1.410418177504005,
      -1.52761656990895
    ],
    [
      -1.418368684282836,
      -1.1448110634662652,
      -1.1812951960415135,
      -1.1680920979825946,
      -1.2414329440605063,
      0.0,
      -1.276898082143151,
      -1.2874944523037357,
      -1.2477740511820037,
      -1.256290367710171,
      -1.2367639326538222,
      -1.4340473845262531,
      -1.1515652153615448,
      -1.1918381372617257,
      -1.1888642577334159,
      -1.2129814293659689,
      -1.2129265026733358,
      -1.1941315044654275,
      -1.264812134026261,
      -1.2201469975431245,
      -1.2958033551563588,
      -1.3649211857342376,
      -1.2945297209310436,
      -1.2939435540938578,
      -1.3307967099904932,
      -1.3181642229103514,
      -1.1729950735657182,
      -1.353718929994963,
      -1.4240668674867503
    ],
    [
      -1.1511000006602912,
      -1.0494983090130312,
      -1.03531231317951,
      -1.0217967518283393,
      -1.0330186661602536,
      -1.0302527172212308,
      0.0,
      -1.0598941942592341,
      -1.0544849156146434,
      -1.0681019600510886,
      -1.1059582944677644,
      -1.2009112531285848,
      -1.0014259487585664,
      -0.9992131527469624,
      -1.060515573197071,
      -1.0386630714238498,
      -1.0490207762985004,
      -1.053360321259499,
      -1.0800229855775476,
      -1.0593744265954828,
      -1.0921594202558405,
      -1.1226476623385797,
      -1.1238480635927255,
      -1.0822541377353785,
      -1.1223688419676867,
      -1.0731565967474193,
      -1.0417677459939907,
      -1.1569919017506696,
      -1.1426444527539412
    ],
    [
      -1.470591489796574,
      -1.2339170762125293,
      -1.2746610249533878,
      -1.2570894681590403,
      -1.2889643277159764,
      -1.2825784659660906,
      -1.3632673032482414,
      0.0,
      -1.2411103807398207,
      -1.3426018279123075,
      -1.3135722877202118,
      -1.4616535409427078,
      -1.260471349786618,
      -1.287931234425009,
      -1.2904751550960618,
      -1.3023422465219483,
      -1.2409290301805667,
      -1.2563711132276243,
      -1.3000417851141468,
      -1.2823673985445112,
      -1.2842852528254778,
      -1.4038590971151168,
      -1.3438986954110113,
      -1.3037751059424223,
      -1.3855103432149003,
      -1.3721117867830042,
      -1.2774555805745385,
      -1.377446455949738,
      -1.3810352949832347
    ],
    [
      -1.4402737526622638,
      -1.3290778479661172,
      -1.3677185952925215,
      -1.3231836690068934,
      -1.398231798330571,
      -1.3825454032369269,
      -1.47189843193753,
      -1.3304437810297345,
      0.0,
      -1.4291086655289071,
      -1.3704951685047215,
      -1.512665705495735,
      -1.2893805288308278,
      -1.3123118222484809,
      -1.3462649299561724,
      -1.3900153518311564,
      -1.3242914420566636,
      -1.352475403158281,
      -1.3792728075185936,
      -1.322896125990712,
      -1.3474132660393165,
      -1.4401938067781421,
      -1.4049288260439663,
      -1.3536779325906718,
      -1.3999457512486608,
      -1.3784704500789124,
      -1.3638391011078657,
      -1.4312638981590717,
      -1.4399814229574213
    ],
    [
      -1.573658623082876,
      -1.4516560301050287,
      -1.4550702066853016,
      -1.4742406317611483,
      -1.469189600203586,
      -1.4657237975939557,
      -1.6148306939072263,
      -1.5405617746941418,
      -1.5312325719987705,
      0.0,
      -1.5156234932940056,
      -1.6268189542245584,
      -1.4706513857368162,
      -1.509383688580859,
      -1.479063076202584,
      -1.499324076491772,
      -1.4420142895227763,
      -1.4919466712345193,
      -1.528801812346537,
      -1.5120883030292211,
      -1.5189364663296663,
      -1.6038164077094073,
      -1.4681350380834985,
      -1.5966720787974036,
      -1.4798637249211237,
      -1.5283263281127566,
      -1.4657575264936784,
      -1.5526380087658718,
      -1.600889925062441
    ],
    [
      -1.358231430340138,
      -1.1966015416962306,
      -1.2423256014884427,
      -1.211765370085207,
      -1.2900807274132997,
      -1.283341944833186,
      -1.335533197587591,
      -1.2914461321327178,
      -1.2394586455587109,
      -1.2586432126541311,
      0.0,
      -1.3909823266818309,
      -1.1814958493651575,
      -1.242249008793521,
      -1.2550347056175426,
      -1.2394147114947418,
      -1.2648115879631616,
      -1.3322011776315061,
      -1.2751251429563055,
      -1.1879910182082887,
      -1.2461995296440167,
      -1.3744396601000866,
      -1.3379937222135443,
      -1.2948378950699095,
      -1.3022369504722084,
      -1.2901845310014999,
      -1.2508269934630023,
      -1.3094793706289245,
      -1.3500651612449337
    ],
    [
      -1.2212476788709337,
      -1.135495443708056,
      -1.1413335014830246,
      -1.134708789447616,
      -1.1608183195335589,
      -1.157217479868513,
      -1.1662303050302811,
      -1.1211643080712712,
      -1.129052407045521,
      -1.1224636380002877,
      -1.140026662155385,
      0.0,
      -1.128585675739594,
      -1.145004923733596,
      -1.1363090533292488,
      -1.106382305424632,
      -1.1231054483010634,
      -1.1847680356303645,
      -1.179997788703451,
      -1.1504434301799473,
      -1.1385059913741806,
      -1.1278150472238186,
      -1.1102900928987531,
      -1.1774873852632042,
      -1.172027950941025,
      -1.1732146913052688,
      -1.1407843820087609,
      -1.1633015286891644,
      -1.1467204241988387
    ],
    [
      -1.428220566552508,
      -1.240321220202915,
      -1.2621761777780423,
      -1.2971593063565288,
      -1.3315207288260673,
      -1.3491899072472575,
      -1.4072088382281027,
      -1.3984799527972789,
      -1.308505189859322,
      -1.3687961578749113,
      -1.3154573111866403,
      -1.5159658802659821,
      0.0,
      -1.241004861208129,
      -1.3636066490215015,
      -1.3021612406115,
      -1.2895948746926569,
      -1.3240011944190408,
      -1.4052014586829995,
      -1.3446191078602894,
      -1.4027942878474025,
      -1.5065969064159483,
      -1.3812066093823905,
      -1.3967512722284485,
      -1.418733903616841,
      -1.393252047968329,
      -1.2914420681384562,
      -1.476230150497507,
      -1.5135625452330523
    ],
    [
      -1.3417467196382356,
      -1.134298909658864,
      -1.1718574469029166,
      -1.162554330501268,
      -1.1963366784555582,
      -1.2021021479886822,
      -1.3048410778312036,
      -1.2370365343878984,
      -1.1346067969046647,
      -1.2274122507839937,
      -1.2453836544889516,
      -1.3939085472318917,
      -1.0829292536221127,
      0.0,
      -1.198444912737562,
      -1.1911330245478695,
      -1.151190955111814,
      -1.1722256437060343,
      -1.2523969470876701,
      -1.1421454574680754,
      -1.2407762659319046,
      -1.3576072466193154,
      -1.2754531580914124,
      -1.2660155289412915,
      -1.2947743447417372,
      -1.2770970249753215,
      -1.1757379599093698,
      -1.3556138027858107,
      -1.3644981671230552
    ],
    [
      -1.295776695131524,
      -1.1695702838191109,
      -1.1476075385646225,
      -1.1707838072639283,
      -1.230817454749535,
      -1.2196810146437933,
      -1.2623527199697124,
      -1.2275858142823048,
      -1.1439009579592874,
      -1.2606981216362645,
      -1.2659555519617156,
      -1.364493479929536,
      -1.1802761333931857,
      -1.1869292106086684,
      0.0,
      -1.1883579833470634,
      -1.1857301778713618,
      -1.2648937835168477,
      -1.2712884726843292,
      -1.138748625016312,
      -1.2074066917210466,
      -1.2886973330996785,
      -1.2717126755086747,
      -1.2146824964246168,
      -1.272603842618993,
      -1.2401822417052395,
      -1.138361334881184,
      -1.3114508927144395,
      -1.3289604674042186
    ],
    [
      -1.446274955527541,
      -1.1699851511566126,
      -1.240998890791772,
      -1.1873899993769712,
      -1.2360384985224384,
      -1.3151553501193158,
      -1.3719409748306826,
      -1.3215757041550282,
      -1.2893080788191746,
      -1.286915074920458,
      -1.2583798869728728,
      -1.481578861271899,
      -1.2240663622614256,
      -1.2462211151321614,
      -1.2275919878982824,
      0.0,
      -1.2856468569130457,
      -1.291079633293123,
      -1.319557264866483,
      -1.2791251946843878,
      -1.3198154364390162,
      -1.4142017547708374,
      -1.3645222220840874,
      -1.316439740775297,
      -1.374698267629601,
      -1.3790761833528165,
      -1.2349021453294766,
      -1.3550932148073782,
      -1.4497286983182003
    ],
    [
      -1.5194495254771176,
      -1.3582579351814383,
      -1.4248803614740337,
      -1.384861398106054,
      -1.4058452809866577,
      -1.4644848687599303,
      -1.5714052607204128,
      -1.437564798348711,
      -1.380258381904496,
      -1.441883854425247,
      -1.4693593805839027,
      -1.6695792390881634,
      -1.3563631393753806,
      -1.40541506861498,
      -1.390006444882826,
      -1.4144367002641123,
      0.0,
      -1.4562627613789583,
      -1.497532983782563,
      -1.4138803214750115,
      -1.4130712460827353,
      -1.6295179653375647,
      -1.4778180141126445,
      -1.5153725293614275,
      -1.4791228149759073,
      -1.4830581264398357,
      -1.3788812186446349,
      -1.534934415570809,
      -1.5513204353035095
    ],
    [
      -1.572985766434566,
      -1.3634862659943072,
      -1.4063739139147173,
      -1.392058921585217,
      -1.3512675768589966,
      -1.3431572829348786,
      -1.4842151368897143,
      -1.398329985663147,
      -1.3598027593058912,
      -1.3959368010583224,
      -1.451569737347537,
      -1.6148274040886408,
      -1.3156622508090654,
      -1.3382839164299611,
      -1.4045808122735317,
      -1.4434828411829164,
      -1.3863085750153252,
      0.0,
      -1.4328632105133083,
      -1.4200370203629764,
      -1.4313105114512554,
      -1.591660809288993,
      -1.4469755950801744,
      -1.4718895486352856,
      -1.5554526786264085,
      -1.4872779022790337,
      -1.398028803384984,
      -1.4930894145986038,
      -1.5630675823752371
    ],
    [
      -1.336760237010081,
      -1.1724713253387742,
      -1.216491874630039,
      -1.176653913195106,
      -1.2117625579117164,
      -1.2064601470687528,
      -1.261735630410805,
      -1.1731624801654792,
      -1.1773887730648411,
      -1.238425136547638,
      -1.1982137502699164,
      -1.3679053100355898,
      -1.180368382552109,
      -1.1790477212856076,
      -1.251636405867253,
      -1.2138131484599846,
      -1.2036273520255096,
      -1.220558322596078,
      0.0,
      -1.2108043522963325,
      -1.2224277967192705,
      -1.3254565600449102,
      -1.2615190223353354,
      -1.259660770992947,
      -1.2913592103314389,
      -1.2967390611752008,
      -1.2068236897171774,
      -1.2899547799433004,
      -1.3248138947756414
    ],
    [
      -1.3359568663285128,
      -1.264630563911315,
      -1.271342376573469,
      -1.2441705907710847,
      -1.310676821342516,
      -1.3325163035240608,
      -1.3975369792705201,
      -1.2980613198921573,
      -1.2009705847820227,
      -1.3761527978272114,
      -1.295608412095649,
      -1.4869183964478025,
      -1.2947101254520623,
      -1.2730993107717579,
      -1.2352181971193301,
      -1.329512305475037,
      -1.2740241551413236,
      -1.3084530899160258,
      -1.3191828587553702,
      0.0,
      -1.321441532434773,
      -1.4378534520082682,
      -1.3756989379032123,
      -1.2874606185578883,
      -1.4104276934913254,
      -1.3369404010930832,
      -1.2882299898811556,
      -1.4071039302722606,
      -1.4195329039087365
    ],
    [
      -1.6230124383827698,
      -1.4547545132350332,
      -1.5148851860129708,
      -1.4816115056074561,
      -1.5011054721481956,
      -1.547700972201176,
      -1.5987448897231444,
      -1.5199926163063364,
      -1.4822099869519738,
      -1.5263116962902894,
      -1.5148177204729338,
      -1.6503326167207781,
      -1.496151563053083,
      -1.5423706796730268,
      -1.4756934270276183,
      -1.5282416251782807,
      -1.4475185286856815,
      -1.5248293120864156,
      -1.52794500683047,
      -1.504472430089058,
      0.0,
      -1.639760405211186,
      -1.5567620007132599,
      -1.5051044319710012,
      -1.5713331188350688,
      -1.5643934038577234,
      -1.4373598500515539,
      -1.5135810286931481,
      -1.5926291009409517
    ],
    [
      -1.2427355650909735,
      -1.1902931364276887,
      -1.1532386802108892,
      -1.1849943587797807,
      -1.202587576972338,
      -1.2019353348967046,
      -1.1377859737882523,
      -1.1929360632195505,
      -1.1598734545624934,
      -1.1971620730967818,
      -1.20711375765322,
      -1.2087703329246957,
      -1.1511893063868923,
      -1.1918429314607497,
      -1.1410840723459097,
      -1.1406900870623893,
      -1.174245700109691,
      -1.2305547160878463,
      -1.2220534635168931,
      -1.1554602644779814,
      -1.171560538282835,
      0.0,
      -1.2105561441457355,
      -1.16119215115285,
      -1.1545053073159484,
      -1.086531711795224,
      -1.1342450662444867,
      -1.1829875986265048,
      -1.1482178893783328
    ],
    [
      -1.5818012319736867,
      -1.3880626989725375,
      -1.3864721324013145,
      -1.3891340607433003,
      -1.4012848016238284,
      -1.4019431623046537,
      -1.5186500662309157,
      -1.4920690951701234,
      -1.4069154127433496,
      -1.3622523226054715,
      -1.4552184175200535,
      -1.5446515745602378,
      -1.3430760860150985,
      -1.3942182054456074,
      -1.3779838404045792,
      -1.42406995658383,
      -1.362448014433994,
      -1.3884430480051055,
      -1.4480578350537154,
      -1.4334175924322632,
      -1.3937354212673794,
      -1.569796422570992,
      0.0,
      -1.4930269219474024,
      -1.3728403847756985,
      -1.4813070592710593,
      -1.3674791974101088,
      -1.4780128848823972,
      -1.4573537070601228
    ],
    [
      -1.3631135163925858,
      -1.2386405628847867,
      -1.3016703580346056,
      -1.2223812546023893,
      -1.293140767656582,
      -1.3175906177166186,
      -1.3430486505647585,
      -1.324492011179473,
      -1.268988278898333,
      -1.3697591144444166,
      -1.253830421224323,
      -1.4612320327272568,
      -1.2693139462588205,
      -1.237683314850934,
      -1.2556552328683697,
      -1.2986734590534168,
      -1.2462334455645294,
      -1.3186716448169111,
      -1.3378510142431719,
      -1.2579037100955652,
      -1.2826745775033848,
      -1.4055233005055283,
      -1.363929737794898,
      0.0,
      -1.3303208346801774,
      -1.3024981005815175,
      -1.281040027629607,
      -1.292117260882514,
      -1.3528912175772447
    ],
    [
      -1.6538100658317967,
      -1.5906200039161569,
      -1.5812069667662407,
      -1.6186462762752574,
      -1.5636209570537152,
      -1.5883877395434218,
      -1.637475265204412,
      -1.646524459748938,
      -1.5569951686507992,
      -1.5597121846623567,
      -1.5801306849757188,
      -1.7132684883838067,
      -1.5673031781435611,
      -1.5777235860160035,
      -1.5836944174221275,
      -1.595657849854168,
      -1.54163086539327,
      -1.6255860688154897,
      -1.651126635761333,
      -1.6163021065597305,
      -1.5761219691863184,
      -1.6360228834617838,
      -1.576019217474008,
      -1.5622368673528977,
      0.0,
      -1.5961323489238206,
      -1.5282209675955554,
      -1.597445894089626,
      -1.5723723888080914
    ],
    [
      -1.5323648298541088,
      -1.4414633606313363,
      -1.460227192064496,
      -1.4462257629177748,
      -1.4951714472628452,
      -1.483396658333314,
      -1.5052032483353446,
      -1.4665540060951543,
      -1.4184563577713478,
      -1.5308716388014205,
      -1.4601249722148284,
      -1.645302531714294,
      -1.424434467940077,
      -1.4326297455963672,
      -1.4415884426810266,
      -1.450955811591325,
      -1.4415999682699918,
      -1.4311827654157807,
      -1.5216864092936622,
      -1.4541339549134704,
      -1.4809305487145676,
      -1.4821294568698624,
      -1.503451334807209,
      -1.4542113899024423,
      -1.5016199691941146,
      0.0,
      -1.4237358348227092,
      -1.5381964600330822,
      -1.5405705543982433
    ],
    [
      -1.4298082766402747,
      -1.2985804452904988,
      -1.3282213007421788,
      -1.2419255243038645,
      -1.3124827694625212,
      -1.3615133515158806,
      -1.3770006284657923,
      -1.3826543673266085,
      -1.314408341480577,
      -1.3446328260988698,
      -1.3766108589118746,
      -1.529681075304385,
      -1.273559073017637,
      -1.2898634214288167,
      -1.2467040043145845,
      -1.2571555936956254,
      -1.2759673776307192,
      -1.3406272018006848,
      -1.3774092870754155,
      -1.3005879524841708,
      -1.3015599080249336,
      -1.4814476406899137,
      -1.358030768166885,
      -1.3661713574264394,
      -1.3899199708161816,
      -1.3982199225121184,
      0.0,
      -1.4003958008477697,
      -1.460049159879386
    ],
    [
      -1.3603179593090988,
      -1.24095514657499,
      -1.2569957684286646,
      -1.2195368698385616,
      -1.2579662089507213,
      -1.27028552118787,
      -1.3464425578450574,
      -1.2668060942737969,
      -1.2157028616560885,
      -1.3014339999753826,
      -1.2510733223637982,
      -1.3860559742047627,
      -1.2543308923490841,
      -1.2949480527034833,
      -1.2595329600044778,
      -1.2534467819105546,
      -1.2426826595829858,
      -1.2869456988285002,
      -1.2921080105562017,
      -1.2611284830435265,
      -1.2223073544641678,
      -1.336052246868642,
      -1.2813361098561915,
      -1.1661531694938942,
      -1.2737182118571464,
      -1.2925066317217768,
      -1.206906088606855,
      0.0,
      -1.3222458841507858
    ],
    [
      -1.4120706184672227,
      -1.2959566573799854,
      -1.2872073532542436,
      -1.2828653297846004,
      -1.2851224873759095,
      -1.3118328022854493,
      -1.3058853230583753,
      -1.2537885697095599,
      -1.2739601978598531,
      -1.2915370178854717,
      -1.3013496292069404,
      -1.3464175071199629,
      -1.297518248695954,
      -1.3021294613882857,
      -1.2828135653715245,
      -1.3008526370010844,
      -1.253809702255258,
      -1.3230797938520518,
      -1.3034171830776633,
      -1.2992388005121616,
      -1.2876806599716542,
      -1.2652261137139,
      -1.2710542202548825,
      -1.2550740797732858,
      -1.2429262958793186,
      -1.2938877270462847,
      -1.26995399552608,
      -1.307657431612575,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.1609246092179104,
      0.13904641589820121,
      0.15796644053578612,
      0.14592091741063218,
      0.14215617627216792,
      0.10412592429644674,
      0.13692347632160695,
      0.1829911327714724,
      0.10566701717703708,
      0.14487830024560022,
      0.07365649442522137,
      0.1724721072310258,
      0.148100842144558,
      0.168643897133129,
      0.1321797953511572,
      0.15889698550661535,
      0.1594307920563356,
      0.13859230617204776,
      0.16968344431108928,
      0.13927850742721226,
      0.10724169518578952,
      0.11767804709173335,
      0.15870508733681588,
      0.11960832186060255,
      0.13659016848561656,
      0.14749414997761567,
      0.09738356985308627,
      0.08405358478455582
    ],
    [
      0.3308244822421975,
      0.0,
      0.4402946180122209,
      0.615087882378224,
      0.5266517016198802,
      0.5166046740607573,
      0.34484266149318943,
      0.40851141182897477,
      0.4697371393465357,
      0.44779785188501986,
      0.4645356442245685,
      0.23593261231220986,
      0.5296113747828644,
      0.49510611365311874,
      0.4650207707895109,
      0.47879964723420954,
      0.4586443111881071,
      0.4763491058169109,
      0.422762769545429,
      0.48978384605778547,
      0.4201898003800706,
      0.2868921791540433,
      0.37782787836487075,
      0.42129677531815024,
      0.3327470481507684,
      0.33431552896560546,
      0.45391825553317666,
      0.3415033604776816,
      0.29393738836563976
    ],
    [
      0.33324717196209463,
      0.5513096982213379,
      0.0,
      0.5532015641203574,
      0.5712656995632368,
      0.5455032660549923,
      0.413894258676464,
      0.46029725254123965,
      0.49194504443320586,
      0.5207557846064166,
      0.5227103115499208,
      0.3182843820924355,
      0.6157914754160934,
      0.5293201387980828,
      0.5622516682335748,
      0.544507665093708,
      0.4911259708495648,
      0.4956336214056456,
      0.4541538917949324,
      0.48974981835908893,
      0.44720760156658335,
      0.35020404962397667,
      0.431002332829014,
      0.4425232688920666,
      0.4764508101013092,
      0.43041535681499843,
      0.5693569885981686,
      0.4098297234177126,
      0.3802345939346068
    ],
    [
      0.3390154010616959,
      0.5988361317953421,
      0.4703135918152801,
      0.0,
      0.48274079420655025,
      0.46441873063372885,
      0.35962779637319864,
      0.4259530289967559,
      0.5058190658033701,
      0.4283802563869994,
      0.4563982128029098,
      0.28665698870436507,
      0.5027370529749777,
      0.5017837386841397,
      0.4909634026796752,
      0.5075332653398272,
      0.46989881923091215,
      0.4650530490838234,
      0.4608450518860434,
      0.4839400561673928,
      0.434451045682132,
      0.3025298600887065,
      0.4008740976365548,
      0.4480390486248409,
      0.3279323577168354,
      0.36378517162726864,
      0.5109814816481,
      0.3952239869561809,
      0.3252634997592092
    ],
    [
      0.4032417700451745,
      0.6741650003535795,
      0.5716205844655902,
      0.5835533852917978,
      0.0,
      0.5551137887341484,
      0.4353291586806354,
      0.5149607639849783,
      0.5357547064099113,
      0.5394789091882022,
      0.49543418628902414,
      0.2888117622766324,
      0.598789374684644,
      0.5601175421365463,
      0.5373370290144193,
      0.5807316301810255,
      0.5978195186959165,
      0.5659877343567059,
      0.4792053509791405,
      0.5346892479328405,
      0.5115932045665252,
      0.34056442220290783,
      0.45143241174789117,
      0.49314384270799905,
      0.46279603187644835,
      0.4084435244478435,
      0.5711256952174779,
      0.48297426175946856,
      0.36577586935452366
    ],
    [
      0.34514943004625254,
      0.6187070508628234,
      0.5822229182875751,
      0.595426016346494,
      0.5220851702685823,
      0.0,
      0.4866200321859375,
      0.47602366202535285,
      0.5157440631470849,
      0.5072277466189175,
      0.5267541816752663,
      0.3294707298028354,
      0.6119528989675438,
      0.5716799770673628,
      0.5746538565956727,
      0.5505366849631197,
      0.5505916116557528,
      0.5693866098636611,
      0.49870598030282753,
      0.5433711167859641,
      0.46771475917272975,
      0.39859692859485096,
      0.468988393398045,
      0.4695745602352308,
      0.4327214043385954,
      0.44535389141873716,
      0.5905230407633704,
      0.40979918433412554,
      0.3394512468423383
    ],
    [
      0.2878124090735217,
      0.38941410072078164,
      0.40360009655430296,
      0.41711565790547356,
      0.40589374357355923,
      0.40865969251258205,
      0.0,
      0.3790182154745787,
      0.38442749411916943,
      0.37081044968272425,
      0.33295411526604846,
      0.23800115660522803,
      0.43748646097524646,
      0.4396992569868504,
      0.37839683653674183,
      0.400249338309963,
      0.3898916334353124,
      0.38555208847431377,
      0.35888942415626524,
      0.3795379831383301,
      0.3467529894779724,
      0.31626474739523314,
      0.31506434614108736,
      0.35665827199843436,
      0.31654356776612613,
      0.3657558129863936,
      0.39714466373982216,
      0.2819205079831433,
      0.2962679569798716
    ],
    [
      0.27129088830094283,
      0.5079653018849875,
      0.46722135314412894,
      0.48479290993847646,
      0.4529180503815404,
      0.4593039121314262,
      0.37861507484927537,
      0.0,
      0.5007719973576961,
      0.39928055018520925,
      0.42831009037730494,
      0.28022883715480895,
      0.48141102831089877,
      0.4539511436725079,
      0.45140722300145497,
      0.4395401315755685,
      0.5009533479169501,
      0.4855112648698925,
      0.44184059298337,
      0.4595149795530056,
      0.457597125272039,
      0.3380232809824,
      0.3979836826865055,
      0.4381072721550945,
      0.3563720348826165,
      0.3697705913145126,
      0.46442679752297833,
      0.36443592214777887,
      0.36084708311428204
    ],
    [
      0.3379120702624776,
      0.4491079749586242,
      0.4104672276322199,
      0.45500215391784793,
      0.3799540245941704,
      0.3956404196878145,
      0.30628739098721147,
      0.4477420418950069,
      0.0,
      0.3490771573958342,
      0.40769065442001984,
      0.2655201174290063,
      0.4888052940939136,
      0.4658740006762605,
      0.43192089296856895,
      0.38817047109358493,
      0.45389438086807776,
      0.4257104197664603,
      0.39891301540614776,
      0.45528969693402943,
      0.4307725568854248,
      0.33799201614659924,
      0.37325699688077507,
      0.42450789033406955,
      0.37824007167608054,
      0.3997153728458289,
      0.4143467218168757,
      0.34692192476566963,
      0.3382043999673201
    ],
    [
      0.3137843189129046,
      0.4357869118907518,
      0.43237273531047893,
      0.4132023102346323,
      0.41825334179219453,
      0.4217191444018249,
      0.2726122480885542,
      0.34688116730163876,
      0.35621036999701006,
      0.0,
      0.37181944870177497,
      0.2606239877712222,
      0.41679155625896436,
      0.37805925341492164,
      0.4083798657931965,
      0.38811886550400865,
      0.4454286524730042,
      0.39549627076126126,
      0.35864112964924355,
      0.3753546389665594,
      0.3685064756661143,
      0.2836265342863733,
      0.419307903912282,
      0.29077086319837697,
      0.40757921707465683,
      0.35911661388302396,
      0.42168541550210215,
      0.3348049332299088,
      0.28655301693333946
    ],
    [
      0.2966383868830491,
      0.4582682755269565,
      0.4125442157347443,
      0.44310444713798014,
      0.36478908980988733,
      0.37152787239000107,
      0.319336619635596,
      0.3634236850904693,
      0.41541117166447616,
      0.3962266045690559,
      0.0,
      0.2638874905413562,
      0.47337396785802954,
      0.4126208084296661,
      0.39983511160564444,
      0.41545510572844524,
      0.3900582292600254,
      0.3226686395916809,
      0.3797446742668815,
      0.46687879901489837,
      0.40867028757917034,
      0.2804301571231005,
      0.31687609500964276,
      0.3600319221532775,
      0.3526328667509786,
      0.36468528622168717,
      0.40404282376018474,
      0.34539044659426255,
      0.30480465597825335
    ],
    [
      0.27939704083566896,
      0.36514927599854663,
      0.35931121822357803,
      0.36593593025898663,
      0.3398264001730438,
      0.34342723983808954,
      0.3344144146763215,
      0.3794804116353314,
      0.3715923126610816,
      0.3781810817063149,
      0.3606180575512177,
      0.0,
      0.3720590439670086,
      0.35563979597300666,
      0.3643356663773538,
      0.3942624142819706,
      0.3775392714055392,
      0.31587668407623815,
      0.32064693100315167,
      0.3502012895266553,
      0.362138728332422,
      0.372829672482784,
      0.3903546268078495,
      0.32315733444339845,
      0.32861676876557766,
      0.32743002840133384,
      0.35986033769784176,
      0.33734319101743826,
      0.35392429550776394
    ],
    [
      0.35854542861099614,
      0.5464447749605892,
      0.524589817385462,
      0.48960668880697544,
      0.4552452663374369,
      0.4375760879162467,
      0.3795571569354015,
      0.3882860423662253,
      0.47826080530418213,
      0.41796983728859294,
      0.4713086839768639,
      0.2708001148975221,
      0.0,
      0.5457611339553752,
      0.4231593461420027,
      0.48460475455200425,
      0.4971711204708473,
      0.46276480074446336,
      0.3815645364805047,
      0.44214688730321483,
      0.38397170731610175,
      0.28016908874755586,
      0.4055593857811137,
      0.3900147229350557,
      0.3680320915466633,
      0.3935139471951752,
      0.495323927025048,
      0.31053584466599715,
      0.27320344993045187
    ],
    [
      0.35822223704643497,
      0.5656700470258065,
      0.5281115097817539,
      0.5374146261834025,
      0.5036322782291123,
      0.4978668086959883,
      0.3951278788534669,
      0.4629324222967721,
      0.5653621597800058,
      0.4725567059006768,
      0.4545853021957189,
      0.3060604094527788,
      0.6170397030625578,
      0.0,
      0.5015240439471085,
      0.508835932136801,
      0.5487780015728565,
      0.5277433129786362,
      0.4475720095970004,
      0.5578234992165951,
      0.4591926907527659,
      0.3423617100653551,
      0.42451579859325816,
      0.43395342774337897,
      0.40519461194293327,
      0.422871931709349,
      0.5242309967753007,
      0.34435515389885984,
      0.33547078956161536
    ],
    [
      0.38921252677285056,
      0.5154189380852636,
      0.537381683339752,
      0.5142054146404462,
      0.45417176715483953,
      0.4653082072605812,
      0.42263650193466207,
      0.45740340762206966,
      0.5410882639450871,
      0.42429110026811,
      0.4190336699426589,
      0.32049574197483843,
      0.5047130885111888,
      0.4980600112957061,
      0.0,
      0.49663123855731106,
      0.4992590440330127,
      0.42009543838752683,
      0.4137007492200453,
      0.5462405968880626,
      0.47758253018332786,
      0.396291888804696,
      0.4132765463956998,
      0.4703067254797577,
      0.41238537928538155,
      0.444806980199135,
      0.5466278870231904,
      0.373538329189935,
      0.3560287545001559
    ],
    [
      0.3239852142512416,
      0.6002750186221699,
      0.5292612789870106,
      0.5828701704018113,
      0.5342216712563441,
      0.4551048196594667,
      0.3983191949481,
      0.4486844656237543,
      0.4809520909596079,
      0.48334509485832444,
      0.5118802828059097,
      0.28868130850688356,
      0.5461938075173569,
      0.5240390546466211,
      0.5426681818805001,
      0.0,
      0.48461331286573683,
      0.4791805364856596,
      0.4507029049122995,
      0.49113497509439474,
      0.4504447333397663,
      0.3560584150079451,
      0.4057379476946952,
      0.4538204290034855,
      0.39556190214918163,
      0.3911839864259661,
      0.5353580244493059,
      0.4151669549714043,
      0.32053147146058225
    ],
    [
      0.390736988851889,
      0.5519285791475683,
      0.4853061528549729,
      0.5253251162229526,
      0.5043412333423489,
      0.4457016455690763,
      0.3387812536085939,
      0.4726217159802957,
      0.5299281324245106,
      0.46830265990375963,
      0.44082713374510396,
      0.24060727524084324,
      0.5538233749536261,
      0.5047714457140267,
      0.5201800694461807,
      0.4957498140648944,
      0.0,
      0.4539237529500484,
      0.4126535305464436,
      0.49630619285399513,
      0.4971152682462714,
      0.280668548991442,
      0.4323685002163622,
      0.39481398496757913,
      0.4310636993530994,
      0.42712838788917096,
      0.5313052956843718,
      0.37525209875819776,
      0.35886607902549716
    ],
    [
      0.3458157042312999,
      0.5553152046715586,
      0.5124275567511485,
      0.5267425490806488,
      0.5675338938068692,
      0.5756441877309872,
      0.4345863337761515,
      0.5204714850027188,
      0.5589987113599746,
      0.5228646696075434,
      0.4672317333183289,
      0.30397406657722503,
      0.6031392198568004,
      0.5805175542359047,
      0.5142206583923341,
      0.47531862948294945,
      0.5324928956505406,
      0.0,
      0.4859382601525575,
      0.4987644503028894,
      0.4874909592146104,
      0.3271406613768728,
      0.47182587558569145,
      0.44691192203058017,
      0.36334879203945736,
      0.43152356838683215,
      0.5207726672808819,
      0.42571205606726203,
      0.3557338882906287
    ],
    [
      0.2876099764659801,
      0.45189888813728696,
      0.4078783388460221,
      0.4477163002809552,
      0.41260765556434476,
      0.4179100664073083,
      0.3626345830652562,
      0.45120773331058195,
      0.44698144041122,
      0.38594507692842317,
      0.42615646320614475,
      0.25646490344047135,
      0.4440018309239522,
      0.4453224921904535,
      0.37273380760880803,
      0.4105570650160766,
      0.4207428614505515,
      0.40381189087998304,
      0.0,
      0.4135658611797286,
      0.40194241675679065,
      0.29891365343115095,
      0.3628511911407257,
      0.3647094424831141,
      0.3330110031446223,
      0.3276311523008604,
      0.4175465237588838,
      0.3344154335327607,
      0.29955631870041977
    ],
    [
      0.4569826272241069,
      0.5283089296413048,
      0.5215971169791507,
      0.548768902781535,
      0.48226267221010377,
      0.4604231900285589,
      0.3954025142820996,
      0.4948781736604624,
      0.591968908770597,
      0.4167866957254083,
      0.4973310814569707,
      0.3060210971048172,
      0.4982293681005574,
      0.5198401827808619,
      0.5577212964332896,
      0.46342718807758265,
      0.5189153384112961,
      0.4844864036365939,
      0.4737566347972495,
      0.0,
      0.4714979611178467,
      0.3550860415443515,
      0.41724055564940743,
      0.5054788749947314,
      0.38251180006129437,
      0.4559990924595365,
      0.5047095036714642,
      0.3858355632803592,
      0.3734065896438832
    ],
    [
      0.2813240094231819,
      0.44958193457091844,
      0.38945126179298084,
      0.42272494219849555,
      0.4032309756577561,
      0.3566354756047756,
      0.30559155808280725,
      0.38434383149961526,
      0.42212646085397787,
      0.3780247515156623,
      0.38951872733301784,
      0.25400383108517355,
      0.4081848847528686,
      0.3619657681329249,
      0.4286430207783334,
      0.376094822627671,
      0.45681791912027014,
      0.3795071357195361,
      0.3763914409754816,
      0.3998640177168937,
      0.0,
      0.26457604259476564,
      0.3475744470926918,
      0.3992320158349505,
      0.3330033289708829,
      0.3399430439482283,
      0.4669765977543978,
      0.39075541911280354,
      0.311707346865
    ],
    [
      0.2755861735350207,
      0.32802860219830543,
      0.365083058415105,
      0.3333273798462135,
      0.3157341616536562,
      0.3163864037292896,
      0.3805357648377419,
      0.3253856754064437,
      0.3584482840635008,
      0.3211596655292124,
      0.31120798097277413,
      0.30955140570129847,
      0.36713243223910186,
      0.32647880716524447,
      0.3772376662800845,
      0.3776316515636049,
      0.3440760385163031,
      0.2877670225381479,
      0.29626827510910103,
      0.36286147414801273,
      0.34676120034315927,
      0.0,
      0.30776559448025864,
      0.35712958747314416,
      0.3638164313100458,
      0.43179002683077017,
      0.38407667238150744,
      0.33533413999948936,
      0.3701038492476614
    ],
    [
      0.24773936443184508,
      0.44147789743299426,
      0.4430684640042173,
      0.44040653566223154,
      0.4282557947817034,
      0.42759743410087814,
      0.3108905301746161,
      0.33747150123540837,
      0.42262518366218216,
      0.4672882738000603,
      0.37432217888547825,
      0.28488902184529397,
      0.48646451039043326,
      0.4353223909599244,
      0.4515567560009526,
      0.40547063982170184,
      0.4670925819715377,
      0.44109754840042625,
      0.38148276135181636,
      0.3961230039732686,
      0.4358051751381524,
      0.25974417383453985,
      0.0,
      0.3365136744581294,
      0.4567002116298333,
      0.34823353713447247,
      0.462061398995423,
      0.35152771152313456,
      0.372186889345409
    ],
    [
      0.33316307986615623,
      0.4576360333739553,
      0.39460623822413643,
      0.4738953416563527,
      0.40313582860216,
      0.3786859785421235,
      0.3532279456939835,
      0.37178458507926915,
      0.42728831736040895,
      0.3265174818143255,
      0.4424461750344191,
      0.23504456353148528,
      0.4269626499999215,
      0.458593281407808,
      0.4406213633903724,
      0.39760313720532525,
      0.45004315069421263,
      0.3776049514418309,
      0.3584255820155702,
      0.4383728861631768,
      0.4136020187553573,
      0.29075329575321374,
      0.33234685846384404,
      0.0,
      0.3659557615785647,
      0.39377849567722456,
      0.41523656862913505,
      0.40415933537622806,
      0.3433853786814973
    ],
    [
      0.30583649208027586,
      0.36902655399591566,
      0.3784395911458318,
      0.3410002816368152,
      0.39602560085835736,
      0.3712588183686507,
      0.32217129270766054,
      0.31312209816313463,
      0.4026513892612733,
      0.3999343732497158,
      0.3795158729363537,
      0.2463780695282658,
      0.3923433797685114,
      0.38192297189606905,
      0.3759521404899451,
      0.3639887080579045,
      0.41801569251880255,
      0.3340604890965828,
      0.30851992215073953,
      0.34334445135234204,
      0.3835245887257541,
      0.3236236744502887,
      0.38362734043806457,
      0.39740969055917486,
      0.0,
      0.36351420898825193,
      0.43142559031651717,
      0.36220066382244664,
      0.38727416910398116
    ],
    [
      0.3899015741555303,
      0.4808030433783028,
      0.4620392119451431,
      0.4760406410918643,
      0.4270949567467939,
      0.4388697456763251,
      0.41706315567429453,
      0.4557123979144848,
      0.5038100462382913,
      0.3913947652082186,
      0.4621414317948107,
      0.27696387229534514,
      0.497831936069562,
      0.48963665841327186,
      0.4806779613286125,
      0.4713105924183141,
      0.4806664357396473,
      0.4910836385938584,
      0.40057999471597694,
      0.4681324490961687,
      0.44133585529507147,
      0.4401369471397767,
      0.41881506920243017,
      0.46805501410719685,
      0.42064643481552455,
      0.0,
      0.49853056918692995,
      0.3840699439765569,
      0.3816958496113958
    ],
    [
      0.35154838088681917,
      0.48277621223659506,
      0.453135356784915,
      0.5394311332232293,
      0.46887388806457264,
      0.4198433060112132,
      0.4043560290613015,
      0.3987022902004853,
      0.4669483160465169,
      0.436723831428224,
      0.4047457986152192,
      0.2516755822227088,
      0.5077975845094569,
      0.4914932360982771,
      0.5346526532125093,
      0.5242010638314685,
      0.5053892798963746,
      0.440729455726409,
      0.4039473704516783,
      0.480768705042923,
      0.4797967495021602,
      0.2999090168371801,
      0.42332588936020876,
      0.41518530010065446,
      0.39143668671091225,
      0.38313673501497547,
      0.0,
      0.3809608566793241,
      0.3213074976477077
    ],
    [
      0.3066232768304351,
      0.425986089564544,
      0.40994546771086937,
      0.44740436630097236,
      0.4089750271888126,
      0.39665571495166385,
      0.32049867829447654,
      0.40013514186573707,
      0.4512383744834454,
      0.3655072361641514,
      0.41586791377573573,
      0.28088526193477126,
      0.4126103437904498,
      0.3719931834360506,
      0.40740827613505615,
      0.41349445422897935,
      0.4242585765565481,
      0.37999553731103375,
      0.3748332255833322,
      0.4058127530960074,
      0.4446338816753661,
      0.33088898927089194,
      0.3856051262833424,
      0.5007880666456397,
      0.39322302428238753,
      0.3744346044177571,
      0.46003514753267893,
      0.0,
      0.3446953519887481
    ],
    [
      0.23958312418096983,
      0.3556970852682071,
      0.3644463893939489,
      0.3687884128635921,
      0.36653125527228303,
      0.33982094036274324,
      0.3457684195898172,
      0.39786517293863266,
      0.3776935447883394,
      0.36011672476272083,
      0.3503041134412521,
      0.30523623552822965,
      0.35413549395223853,
      0.34952428125990687,
      0.36884017727666807,
      0.3508011056471081,
      0.39784404039293464,
      0.32857394879614077,
      0.34823655957052924,
      0.352414942136031,
      0.3639730826765384,
      0.3864276289342925,
      0.3805995223933101,
      0.39657966287490676,
      0.40872744676887396,
      0.3577660156019078,
      0.38169974712211263,
      0.34399631103561745,
      0.0
    ]
  ],
  "row_avgs": [
    0.13751036451718102,
    0.4242688154707757,
    0.47864905034110095,
    0.436071249441672,
    0.5049996681279285,
    0.49996546952025184,
    0.36356367921318133,
    0.42472830241670184,
    0.3984620484394972,
    0.37076739967536876,
    0.37511991913962145,
    0.352983909415197,
    0.41984598034207393,
    0.4660359285355818,
    0.4546497289605461,
    0.45999918745662594,
    0.44858564037689724,
    0.4790163626521875,
    0.38594015609153126,
    0.4667455108759079,
    0.3742069647005737,
    0.3420237651969356,
    0.3969076837480737,
    0.38945986728614496,
    0.36343243270241526,
    0.4433942925653463,
    0.4308142216215721,
    0.39480118183213875,
    0.35864254945820895
  ],
  "col_avgs": [
    0.327883198159679,
    0.4755681487051046,
    0.4427065524793122,
    0.46785919646230534,
    0.4336490307185991,
    0.42233441954762174,
    0.35867337040940217,
    0.40786511633078665,
    0.455599104550862,
    0.4100575840483879,
    0.41894741951930065,
    0.27067168999940255,
    0.4757816158542784,
    0.4498998237580517,
    0.4475337014097036,
    0.43699306471236726,
    0.45467567936954456,
    0.41982436227892167,
    0.39383981699199305,
    0.43898828793969086,
    0.41905513932312266,
    0.31956947571611016,
    0.38477437361704847,
    0.40562209568175833,
    0.37560211094822327,
    0.3783083236283022,
    0.4600294103344594,
    0.3594766724438156,
    0.3298025451830835
  ],
  "combined_avgs": [
    0.23269678133843,
    0.44991848208794016,
    0.46067780141020653,
    0.4519652229519887,
    0.46932434942326384,
    0.46114994453393676,
    0.36111852481129175,
    0.41629670937374424,
    0.4270305764951796,
    0.3904124918618783,
    0.39703366932946105,
    0.3118277997072998,
    0.4478137980981762,
    0.4579678761468168,
    0.45109171518512486,
    0.4484961260844966,
    0.45163065987322093,
    0.4494203624655546,
    0.38988998654176216,
    0.45286689940779934,
    0.3966310520118482,
    0.3307966204565229,
    0.3908410286825611,
    0.39754098148395167,
    0.36951727182531924,
    0.41085130809682424,
    0.4454218159780158,
    0.37713892713797714,
    0.3442225473206462
  ],
  "gppm": [
    615.9441169777605,
    587.257854555856,
    600.6031415361214,
    589.1672419034779,
    605.9202265882075,
    612.9251636983942,
    645.3032550564487,
    619.4287153561685,
    595.5221402857597,
    615.335075906849,
    613.4671820618709,
    685.0302981954468,
    586.8373384742564,
    601.7942356514311,
    601.1827845707757,
    606.6196804376921,
    596.152788957287,
    611.7046556527257,
    626.9681806159264,
    604.852854764227,
    610.0875272874957,
    661.4475959968371,
    629.2524382248582,
    619.5096382691236,
    630.9083448199062,
    632.2392074588613,
    595.9610067765667,
    641.943004819955,
    655.0151187201436
  ],
  "gppm_normalized": [
    1.4548934571453647,
    1.4407016069677472,
    1.4772478304438705,
    1.4517633968717942,
    1.4855738485017582,
    1.5070080959176722,
    1.584318984228012,
    1.527371683431046,
    1.4627605592993596,
    1.5139002036825764,
    1.505023568183274,
    1.693287589144909,
    1.4402638748356176,
    1.4766046699674014,
    1.4824528941994781,
    1.4916607517346374,
    1.4668353123917417,
    1.503630660674366,
    1.5392328286216963,
    1.4858533370864748,
    1.5026777424129458,
    1.6249777997586012,
    1.545576422326863,
    1.5259033647643954,
    1.5513089256156132,
    1.5531159774294863,
    1.4644815712792483,
    1.5782246162681832,
    1.6096907351121974
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360,
    852,
    466,
    462,
    474,
    415,
    413,
    436,
    422,
    465,
    448,
    470,
    377,
    462,
    415,
    455,
    415,
    425,
    413,
    422,
    411,
    459,
    427,
    407,
    447,
    412,
    368,
    423,
    422,
    431
  ],
  "response_lengths": [
    4050,
    2565,
    2509,
    2620,
    2315,
    2352,
    2485,
    2264,
    2596,
    2527,
    2620,
    2257,
    2547,
    2399,
    2551,
    2240,
    2352,
    2320,
    2368,
    2289,
    2498,
    2327,
    2264,
    2396,
    2223,
    1914,
    2302,
    2274,
    2355
  ]
}